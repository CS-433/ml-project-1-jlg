{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = Path(\"../data/\")\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "# -1 c'est pour 'b' et 1 c'est pour 's'\n",
    "print(np.shape(y))\n",
    "print(np.shape(tX))\n",
    "# On a donc 30 features et 25'000 data différentes\n",
    "print(tX.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)\n",
    "plt.show()\n",
    "# Plus de -1 que de 1 --> donc plus de 'b' que de 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.boxplot(tX[:,])\n",
    "plt.show()\n",
    "# Certains features ont pas mal d'outliers, peut etre enlever les données avec des outliers vraiment loin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(15,15))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].hist(tX[:,n])\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()\n",
    "\n",
    "print(np.argmax(tX[:, 8]))\n",
    "\n",
    "# Les features 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28 semblent etre très bizarres\n",
    "\n",
    "# 15, 18, 20 bizarres ? 0, 4, 5, 6, 23, 26 aussi \n",
    "# enlever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(15,15))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].scatter(tX[:,n], y)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()\n",
    "\n",
    "#meme constat comment faire pour se debarrasser de ces valeurs ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regarder ou il y a des -999 ou bcp de outlier\n",
    "sum = 0\n",
    "nb_outliers = []\n",
    "for col in range(tX.shape[1]) :\n",
    "    sum = np.where(tX[:,col] == -999)[0].shape\n",
    "    nb_outliers.append(sum)   \n",
    "print(nb_outliers)\n",
    "#Ceci confirme ce qu'on a vu dans les histogrammes\n",
    "\n",
    "print(np.where(tX==-999)[0].shape)\n",
    "#il y a 1580052 -999 dans les data\n",
    "#peut etre que toutes les valeurs NA ont ete remplacées par une constante globale -999 ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer par la valeur da la mean du feature en question sans les compter dedans\n",
    "def filtering_with_mean(tX, index=0):\n",
    "    if index == 0: \n",
    "        index = np.arange(tX.shape[1])\n",
    "    tX_filtered = np.copy(tX)\n",
    "    arr = []\n",
    "    for ind in index :\n",
    "        arr = np.delete(tX_filtered[:,ind], np.where(tX_filtered[:,ind]==-999))\n",
    "        mean = np.mean(arr)\n",
    "        tX_filtered[np.where(tX_filtered[:,ind]==-999), ind] = mean\n",
    "    return tX_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(tX, to_cut):\n",
    "    cut_index = 100*np.ones(tX.shape[1])\n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_cut)):\n",
    "            if index_full[i] == to_cut[j]:\n",
    "                cut_index[i] = to_cut[j]\n",
    "    index = index_full[~(index_full == cut_index)]\n",
    "    index = index.reshape(-1)\n",
    "    tX_cut = tX[:, index]\n",
    "    return tX_cut\n",
    "\n",
    "def keep(tX, to_keep):\n",
    "    keep_index = 100*np.ones(tX.shape[1])\n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_keep)):\n",
    "            if index_full[i] == to_keep[j]:\n",
    "                keep_index[i] = to_keep[j]\n",
    "    index = index_full[index_full == keep_index]\n",
    "    index = index.reshape(-1)\n",
    "    tX_kept = tX[:, index]\n",
    "    return tX_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_mean = filtering_with_mean(tX)\n",
    "tX_norm, _, _ = standardize(tX)\n",
    "tX_mean_norm, _, _ = standardize(tX_mean)\n",
    "\n",
    "to_cut = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "tX_cut = cut(tX_mean_norm, to_cut)\n",
    "\n",
    "to_keep = [13, 14, 17]\n",
    "tX_kept = keep(tX_mean_norm, to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose x and generate test and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tX_mean\n",
    "x2 = tX_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-9c9f45ad5d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdegree_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_degree_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfonction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/cross_validation.py\u001b[0m in \u001b[0;36mbest_degree_selection\u001b[0;34m(x, y, degrees, k_fold, lambdas, fonction, seed)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mrmse_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfonction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mbest_rmses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/cross_validation.py\u001b[0m in \u001b[0;36mk_fold_regression\u001b[0;34m(y, x, k_indices, k, par, degree, fonction)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/implementations.py\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Least squares regression using normal equations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DD->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x1, degrees, k_fold=4, lambdas=0, fonction=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y, x1)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to find the oprimal lambda and degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt, lambda_opt = best_degree_selection(y, x1, degrees=np.arange(1,8), k_fold=4, lambdas=np.logspace(-2, 2, 30))\n",
    "print('CV done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV done\n"
     ]
    }
   ],
   "source": [
    "degree_opt_2, lambda_opt_2 = best_degree_selection(y, x1, degrees=np.arange(1,8), k_fold=4, lambdas=np.logspace(-4, 0, 30))\n",
    "print('CV done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression loss 0.348717909143505 with optimal lambda 0.06723357536499334 and degree 7\n",
      "Ridge regression loss 0.34381954694688865 with optimal lambda 0.0004893900918477494 and degree 6\n"
     ]
    }
   ],
   "source": [
    "tX_tr = build_poly(x1, degree_opt)\n",
    "w_rr, loss_rr = ridge_regression(y, x1, lambda_opt)\n",
    "print(\"Ridge regression loss {loss} with optimal lambda {l} and degree {d}\".format(loss=loss_rr, l=lambda_opt, d=degree_opt))\n",
    "\n",
    "tX_tr = build_poly(x1, degree_opt_2)\n",
    "w_rr, loss_rr = ridge_regression(y, x1, lambda_opt_2)\n",
    "print(\"Ridge regression loss {loss} with optimal lambda {l} and degree {d}\".format(loss=loss_rr, l=lambda_opt_2, d=degree_opt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV to find best gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4634160843093332\n",
      "Gradient Descent(2/49): loss=0.4542701053866668\n",
      "Gradient Descent(3/49): loss=0.4519836106560001\n",
      "Gradient Descent(4/49): loss=0.4514119869733334\n",
      "Gradient Descent(5/49): loss=0.4512690810526668\n",
      "Gradient Descent(6/49): loss=0.4512333545725\n",
      "Gradient Descent(7/49): loss=0.4512244229524584\n",
      "Gradient Descent(8/49): loss=0.4512221900474479\n",
      "Gradient Descent(9/49): loss=0.45122163182119535\n",
      "Gradient Descent(10/49): loss=0.45122149226463204\n",
      "Gradient Descent(11/49): loss=0.4512214573754914\n",
      "Gradient Descent(12/49): loss=0.4512214486532063\n",
      "Gradient Descent(13/49): loss=0.4512214464726348\n",
      "Gradient Descent(14/49): loss=0.45122144592749225\n",
      "Gradient Descent(15/49): loss=0.45122144579120627\n",
      "Gradient Descent(16/49): loss=0.4512214457571348\n",
      "Gradient Descent(17/49): loss=0.45122144574861717\n",
      "Gradient Descent(18/49): loss=0.45122144574648737\n",
      "Gradient Descent(19/49): loss=0.45122144574595524\n",
      "Gradient Descent(20/49): loss=0.4512214457458222\n",
      "Gradient Descent(21/49): loss=0.451221445745789\n",
      "Gradient Descent(22/49): loss=0.45122144574578044\n",
      "Gradient Descent(23/49): loss=0.4512214457457786\n",
      "Gradient Descent(24/49): loss=0.45122144574577805\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.45122144574577755\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.45122144574577766\n",
      "Gradient Descent(44/49): loss=0.45122144574577794\n",
      "Gradient Descent(45/49): loss=0.45122144574577794\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.45122144574577794\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4624983633493333\n",
      "Gradient Descent(2/49): loss=0.4531229541866668\n",
      "Gradient Descent(3/49): loss=0.4507791018960002\n",
      "Gradient Descent(4/49): loss=0.45019313882333334\n",
      "Gradient Descent(5/49): loss=0.45004664805516664\n",
      "Gradient Descent(6/49): loss=0.45001002536312507\n",
      "Gradient Descent(7/49): loss=0.4500008696901143\n",
      "Gradient Descent(8/49): loss=0.44999858077186217\n",
      "Gradient Descent(9/49): loss=0.4499980085422989\n",
      "Gradient Descent(10/49): loss=0.449997865484908\n",
      "Gradient Descent(11/49): loss=0.44999782972056046\n",
      "Gradient Descent(12/49): loss=0.44999782077947353\n",
      "Gradient Descent(13/49): loss=0.44999781854420173\n",
      "Gradient Descent(14/49): loss=0.4499978179853838\n",
      "Gradient Descent(15/49): loss=0.44999781784567927\n",
      "Gradient Descent(16/49): loss=0.4499978178107532\n",
      "Gradient Descent(17/49): loss=0.4499978178020218\n",
      "Gradient Descent(18/49): loss=0.44999781779983855\n",
      "Gradient Descent(19/49): loss=0.44999781779929304\n",
      "Gradient Descent(20/49): loss=0.44999781779915654\n",
      "Gradient Descent(21/49): loss=0.4499978177991226\n",
      "Gradient Descent(22/49): loss=0.449997817799114\n",
      "Gradient Descent(23/49): loss=0.44999781779911185\n",
      "Gradient Descent(24/49): loss=0.4499978177991113\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.44999781779911097\n",
      "Gradient Descent(28/49): loss=0.44999781779911097\n",
      "Gradient Descent(29/49): loss=0.44999781779911097\n",
      "Gradient Descent(30/49): loss=0.4499978177991109\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991113\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.4499978177991112\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991108\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.4499978177991113\n",
      "Gradient Descent(41/49): loss=0.44999781779911097\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.44999781779911097\n",
      "Gradient Descent(44/49): loss=0.4499978177991108\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911097\n",
      "Gradient Descent(48/49): loss=0.4499978177991112\n",
      "Gradient Descent(49/49): loss=0.4499978177991112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46293223999999983\n",
      "Gradient Descent(2/49): loss=0.4536652999999999\n",
      "Gradient Descent(3/49): loss=0.4513485650000001\n",
      "Gradient Descent(4/49): loss=0.4507693812499999\n",
      "Gradient Descent(5/49): loss=0.4506245853125\n",
      "Gradient Descent(6/49): loss=0.4505883863281248\n",
      "Gradient Descent(7/49): loss=0.4505793365820311\n",
      "Gradient Descent(8/49): loss=0.45057707414550774\n",
      "Gradient Descent(9/49): loss=0.45057650853637715\n",
      "Gradient Descent(10/49): loss=0.4505763671340944\n",
      "Gradient Descent(11/49): loss=0.45057633178352363\n",
      "Gradient Descent(12/49): loss=0.45057632294588096\n",
      "Gradient Descent(13/49): loss=0.4505763207364703\n",
      "Gradient Descent(14/49): loss=0.4505763201841176\n",
      "Gradient Descent(15/49): loss=0.45057632004602927\n",
      "Gradient Descent(16/49): loss=0.4505763200115073\n",
      "Gradient Descent(17/49): loss=0.45057632000287684\n",
      "Gradient Descent(18/49): loss=0.4505763200007193\n",
      "Gradient Descent(19/49): loss=0.45057632000017983\n",
      "Gradient Descent(20/49): loss=0.45057632000004494\n",
      "Gradient Descent(21/49): loss=0.4505763200000113\n",
      "Gradient Descent(22/49): loss=0.45057632000000286\n",
      "Gradient Descent(23/49): loss=0.45057632000000075\n",
      "Gradient Descent(24/49): loss=0.45057632000000036\n",
      "Gradient Descent(25/49): loss=0.45057632\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.45057632\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763199999998\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.45057631999999975\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.4505763199999998\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.45057631999999975\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763200000002\n",
      "Gradient Descent(46/49): loss=0.4505763199999997\n",
      "Gradient Descent(47/49): loss=0.45057631999999975\n",
      "Gradient Descent(48/49): loss=0.4505763200000001\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46262980160000006\n",
      "Gradient Descent(2/49): loss=0.4532872519999998\n",
      "Gradient Descent(3/49): loss=0.4509516145999999\n",
      "Gradient Descent(4/49): loss=0.45036770524999997\n",
      "Gradient Descent(5/49): loss=0.4502217279125\n",
      "Gradient Descent(6/49): loss=0.45018523357812507\n",
      "Gradient Descent(7/49): loss=0.45017610999453145\n",
      "Gradient Descent(8/49): loss=0.45017382909863274\n",
      "Gradient Descent(9/49): loss=0.45017325887465837\n",
      "Gradient Descent(10/49): loss=0.4501731163186645\n",
      "Gradient Descent(11/49): loss=0.4501730806796661\n",
      "Gradient Descent(12/49): loss=0.45017307176991656\n",
      "Gradient Descent(13/49): loss=0.4501730695424791\n",
      "Gradient Descent(14/49): loss=0.45017306898561976\n",
      "Gradient Descent(15/49): loss=0.4501730688464051\n",
      "Gradient Descent(16/49): loss=0.45017306881160135\n",
      "Gradient Descent(17/49): loss=0.4501730688029003\n",
      "Gradient Descent(18/49): loss=0.45017306880072505\n",
      "Gradient Descent(19/49): loss=0.4501730688001814\n",
      "Gradient Descent(20/49): loss=0.4501730688000451\n",
      "Gradient Descent(21/49): loss=0.4501730688000115\n",
      "Gradient Descent(22/49): loss=0.45017306880000263\n",
      "Gradient Descent(23/49): loss=0.4501730688000008\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.45017306879999985\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730687999999\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730687999998\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730687999999\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000001\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730687999999\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46199662838053535\n",
      "Gradient Descent(2/49): loss=0.45360168358979597\n",
      "Gradient Descent(3/49): loss=0.4517472402855214\n",
      "Gradient Descent(4/49): loss=0.45133759375960714\n",
      "Gradient Descent(5/49): loss=0.45124710284203273\n",
      "Gradient Descent(6/49): loss=0.4512271133983406\n",
      "Gradient Descent(7/49): loss=0.45122269773022883\n",
      "Gradient Descent(8/49): loss=0.45122172230914304\n",
      "Gradient Descent(9/49): loss=0.45122150683862516\n",
      "Gradient Descent(10/49): loss=0.4512214592411879\n",
      "Gradient Descent(11/49): loss=0.4512214487269137\n",
      "Gradient Descent(12/49): loss=0.4512214464043107\n",
      "Gradient Descent(13/49): loss=0.4512214458912475\n",
      "Gradient Descent(14/49): loss=0.451221445777912\n",
      "Gradient Descent(15/49): loss=0.45122144575287626\n",
      "Gradient Descent(16/49): loss=0.4512214457473459\n",
      "Gradient Descent(17/49): loss=0.45122144574612433\n",
      "Gradient Descent(18/49): loss=0.45122144574585427\n",
      "Gradient Descent(19/49): loss=0.4512214457457947\n",
      "Gradient Descent(20/49): loss=0.45122144574578155\n",
      "Gradient Descent(21/49): loss=0.4512214457457785\n",
      "Gradient Descent(22/49): loss=0.4512214457457781\n",
      "Gradient Descent(23/49): loss=0.45122144574577794\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.45122144574577794\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.4512214457457775\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.45122144574577766\n",
      "Gradient Descent(38/49): loss=0.45122144574577766\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4610432998472876\n",
      "Gradient Descent(2/49): loss=0.4524377647835535\n",
      "Gradient Descent(3/49): loss=0.45053680208797436\n",
      "Gradient Descent(4/49): loss=0.45011687942852097\n",
      "Gradient Descent(5/49): loss=0.4500241185130478\n",
      "Gradient Descent(6/49): loss=0.4500036276268196\n",
      "Gradient Descent(7/49): loss=0.449999101190052\n",
      "Gradient Descent(8/49): loss=0.4499981013001699\n",
      "Gradient Descent(9/49): loss=0.44999788042449496\n",
      "Gradient Descent(10/49): loss=0.4499978316330585\n",
      "Gradient Descent(11/49): loss=0.44999782085503004\n",
      "Gradient Descent(12/49): loss=0.4499978184741636\n",
      "Gradient Descent(13/49): loss=0.44999781794823024\n",
      "Gradient Descent(14/49): loss=0.44999781783205156\n",
      "Gradient Descent(15/49): loss=0.44999781780638765\n",
      "Gradient Descent(16/49): loss=0.4499978178007185\n",
      "Gradient Descent(17/49): loss=0.449997817799466\n",
      "Gradient Descent(18/49): loss=0.4499978177991897\n",
      "Gradient Descent(19/49): loss=0.44999781779912834\n",
      "Gradient Descent(20/49): loss=0.4499978177991149\n",
      "Gradient Descent(21/49): loss=0.4499978177991119\n",
      "Gradient Descent(22/49): loss=0.44999781779911135\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.4499978177991113\n",
      "Gradient Descent(33/49): loss=0.4499978177991113\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.44999781779911097\n",
      "Gradient Descent(40/49): loss=0.44999781779911074\n",
      "Gradient Descent(41/49): loss=0.44999781779911147\n",
      "Gradient Descent(42/49): loss=0.44999781779911135\n",
      "Gradient Descent(43/49): loss=0.4499978177991112\n",
      "Gradient Descent(44/49): loss=0.44999781779911097\n",
      "Gradient Descent(45/49): loss=0.4499978177991112\n",
      "Gradient Descent(46/49): loss=0.4499978177991112\n",
      "Gradient Descent(47/49): loss=0.44999781779911135\n",
      "Gradient Descent(48/49): loss=0.44999781779911135\n",
      "Gradient Descent(49/49): loss=0.44999781779911135\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4614940109119998\n",
      "Gradient Descent(2/49): loss=0.45298803792246084\n",
      "Gradient Descent(3/49): loss=0.4511090684890715\n",
      "Gradient Descent(4/49): loss=0.4506940041412359\n",
      "Gradient Descent(5/49): loss=0.4506023164267991\n",
      "Gradient Descent(6/49): loss=0.45058206261068\n",
      "Gradient Descent(7/49): loss=0.45057758854269914\n",
      "Gradient Descent(8/49): loss=0.4505766002210823\n",
      "Gradient Descent(9/49): loss=0.45057638190083704\n",
      "Gradient Descent(10/49): loss=0.45057633367389494\n",
      "Gradient Descent(11/49): loss=0.4505763230205634\n",
      "Gradient Descent(12/49): loss=0.45057632066724235\n",
      "Gradient Descent(13/49): loss=0.45057632014739396\n",
      "Gradient Descent(14/49): loss=0.4505763200325591\n",
      "Gradient Descent(15/49): loss=0.4505763200071922\n",
      "Gradient Descent(16/49): loss=0.45057632000158865\n",
      "Gradient Descent(17/49): loss=0.4505763200003509\n",
      "Gradient Descent(18/49): loss=0.4505763200000775\n",
      "Gradient Descent(19/49): loss=0.45057632000001696\n",
      "Gradient Descent(20/49): loss=0.4505763200000038\n",
      "Gradient Descent(21/49): loss=0.4505763200000007\n",
      "Gradient Descent(22/49): loss=0.4505763200000003\n",
      "Gradient Descent(23/49): loss=0.45057632000000014\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.45057632000000014\n",
      "Gradient Descent(26/49): loss=0.45057632000000014\n",
      "Gradient Descent(27/49): loss=0.4505763199999998\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.45057631999999975\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.45057632000000014\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763199999998\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.4505763199999998\n",
      "Gradient Descent(37/49): loss=0.45057632000000014\n",
      "Gradient Descent(38/49): loss=0.45057632\n",
      "Gradient Descent(39/49): loss=0.4505763199999998\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.45057632000000014\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.4505763200000001\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.4505763200000001\n",
      "Gradient Descent(48/49): loss=0.4505763200000001\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4611798379020798\n",
      "Gradient Descent(2/49): loss=0.45260446409464955\n",
      "Gradient Descent(3/49): loss=0.4507101640205882\n",
      "Gradient Descent(4/49): loss=0.4502917131342279\n",
      "Gradient Descent(5/49): loss=0.4501992773334308\n",
      "Gradient Descent(6/49): loss=0.45017885826503484\n",
      "Gradient Descent(7/49): loss=0.4501743476928262\n",
      "Gradient Descent(8/49): loss=0.45017335130742525\n",
      "Gradient Descent(9/49): loss=0.4501731312058901\n",
      "Gradient Descent(10/49): loss=0.45017308258546135\n",
      "Gradient Descent(11/49): loss=0.4501730718452084\n",
      "Gradient Descent(12/49): loss=0.45017306947268654\n",
      "Gradient Descent(13/49): loss=0.45017306894859643\n",
      "Gradient Descent(14/49): loss=0.4501730688328249\n",
      "Gradient Descent(15/49): loss=0.4501730688072511\n",
      "Gradient Descent(16/49): loss=0.4501730688016018\n",
      "Gradient Descent(17/49): loss=0.4501730688003537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=0.45017306880007824\n",
      "Gradient Descent(19/49): loss=0.4501730688000174\n",
      "Gradient Descent(20/49): loss=0.4501730688000039\n",
      "Gradient Descent(21/49): loss=0.4501730688000008\n",
      "Gradient Descent(22/49): loss=0.4501730687999999\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.45017306879999985\n",
      "Gradient Descent(29/49): loss=0.4501730688000001\n",
      "Gradient Descent(30/49): loss=0.45017306880000024\n",
      "Gradient Descent(31/49): loss=0.4501730687999999\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.45017306879999985\n",
      "Gradient Descent(34/49): loss=0.4501730687999999\n",
      "Gradient Descent(35/49): loss=0.4501730688000001\n",
      "Gradient Descent(36/49): loss=0.4501730687999999\n",
      "Gradient Descent(37/49): loss=0.45017306879999985\n",
      "Gradient Descent(38/49): loss=0.45017306879999985\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688000001\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730687999999\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4606649738493953\n",
      "Gradient Descent(2/49): loss=0.4530497127866382\n",
      "Gradient Descent(3/49): loss=0.45157539824488835\n",
      "Gradient Descent(4/49): loss=0.45128997094960566\n",
      "Gradient Descent(5/49): loss=0.4512347122252389\n",
      "Gradient Descent(6/49): loss=0.45122401413620145\n",
      "Gradient Descent(7/49): loss=0.45122194298616375\n",
      "Gradient Descent(8/49): loss=0.4512215420115165\n",
      "Gradient Descent(9/49): loss=0.4512214643828246\n",
      "Gradient Descent(10/49): loss=0.45122144935390995\n",
      "Gradient Descent(11/49): loss=0.4512214464443121\n",
      "Gradient Descent(12/49): loss=0.4512214458810138\n",
      "Gradient Descent(13/49): loss=0.4512214457719596\n",
      "Gradient Descent(14/49): loss=0.4512214457508468\n",
      "Gradient Descent(15/49): loss=0.4512214457467592\n",
      "Gradient Descent(16/49): loss=0.45122144574596773\n",
      "Gradient Descent(17/49): loss=0.4512214457458146\n",
      "Gradient Descent(18/49): loss=0.45122144574578493\n",
      "Gradient Descent(19/49): loss=0.45122144574577905\n",
      "Gradient Descent(20/49): loss=0.45122144574577805\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.45122144574577766\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.45122144574577766\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.45122144574577766\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457778\n",
      "Gradient Descent(38/49): loss=0.45122144574577794\n",
      "Gradient Descent(39/49): loss=0.45122144574577794\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45967824027320314\n",
      "Gradient Descent(2/49): loss=0.45187194759009536\n",
      "Gradient Descent(3/49): loss=0.4503606493266458\n",
      "Gradient Descent(4/49): loss=0.4500680619828418\n",
      "Gradient Descent(5/49): loss=0.45001141707308123\n",
      "Gradient Descent(6/49): loss=0.45000045061855165\n",
      "Gradient Descent(7/49): loss=0.449998327512955\n",
      "Gradient Descent(8/49): loss=0.4499979164797113\n",
      "Gradient Descent(9/49): loss=0.44999783690367523\n",
      "Gradient Descent(10/49): loss=0.4499978214977548\n",
      "Gradient Descent(11/49): loss=0.44999781851516857\n",
      "Gradient Descent(12/49): loss=0.4499978179377397\n",
      "Gradient Descent(13/49): loss=0.44999781782594966\n",
      "Gradient Descent(14/49): loss=0.449997817804307\n",
      "Gradient Descent(15/49): loss=0.4499978178001171\n",
      "Gradient Descent(16/49): loss=0.4499978177993059\n",
      "Gradient Descent(17/49): loss=0.44999781779914877\n",
      "Gradient Descent(18/49): loss=0.4499978177991184\n",
      "Gradient Descent(19/49): loss=0.44999781779911247\n",
      "Gradient Descent(20/49): loss=0.44999781779911147\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.4499978177991113\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991113\n",
      "Gradient Descent(29/49): loss=0.44999781779911097\n",
      "Gradient Descent(30/49): loss=0.4499978177991113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.4499978177991112\n",
      "Gradient Descent(33/49): loss=0.44999781779911135\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911097\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.44999781779911135\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.44999781779911135\n",
      "Gradient Descent(44/49): loss=0.44999781779911135\n",
      "Gradient Descent(45/49): loss=0.44999781779911135\n",
      "Gradient Descent(46/49): loss=0.44999781779911135\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46014474444799996\n",
      "Gradient Descent(2/49): loss=0.4524287669731327\n",
      "Gradient Descent(3/49): loss=0.45093495373399844\n",
      "Gradient Descent(4/49): loss=0.450645751490902\n",
      "Gradient Descent(5/49): loss=0.45058976193663874\n",
      "Gradient Descent(6/49): loss=0.4505789223589333\n",
      "Gradient Descent(7/49): loss=0.45057682381668945\n",
      "Gradient Descent(8/49): loss=0.4505764175389111\n",
      "Gradient Descent(9/49): loss=0.45057633888353316\n",
      "Gradient Descent(10/49): loss=0.4505763236558519\n",
      "Gradient Descent(11/49): loss=0.45057632070777304\n",
      "Gradient Descent(12/49): loss=0.45057632013702487\n",
      "Gradient Descent(13/49): loss=0.4505763200265279\n",
      "Gradient Descent(14/49): loss=0.45057632000513576\n",
      "Gradient Descent(15/49): loss=0.4505763200009944\n",
      "Gradient Descent(16/49): loss=0.4505763200001925\n",
      "Gradient Descent(17/49): loss=0.45057632000003733\n",
      "Gradient Descent(18/49): loss=0.4505763200000074\n",
      "Gradient Descent(19/49): loss=0.45057632000000136\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.45057632000000014\n",
      "Gradient Descent(22/49): loss=0.4505763199999998\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.45057632\n",
      "Gradient Descent(26/49): loss=0.45057632\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.45057632\n",
      "Gradient Descent(29/49): loss=0.4505763200000002\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.4505763200000002\n",
      "Gradient Descent(32/49): loss=0.4505763200000001\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763200000002\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.45057631999999975\n",
      "Gradient Descent(41/49): loss=0.4505763200000002\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.4505763200000001\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45981956268032026\n",
      "Gradient Descent(2/49): loss=0.45204063001523004\n",
      "Gradient Descent(3/49): loss=0.4505346286512685\n",
      "Gradient Descent(4/49): loss=0.4502430667872055\n",
      "Gradient Descent(5/49): loss=0.45018662041032276\n",
      "Gradient Descent(6/49): loss=0.45017569239175864\n",
      "Gradient Descent(7/49): loss=0.4501735767273645\n",
      "Gradient Descent(8/49): loss=0.4501731671347378\n",
      "Gradient Descent(9/49): loss=0.4501730878376052\n",
      "Gradient Descent(10/49): loss=0.4501730724856806\n",
      "Gradient Descent(11/49): loss=0.4501730695135477\n",
      "Gradient Descent(12/49): loss=0.45017306893814285\n",
      "Gradient Descent(13/49): loss=0.4501730688267445\n",
      "Gradient Descent(14/49): loss=0.4501730688051777\n",
      "Gradient Descent(15/49): loss=0.45017306880100244\n",
      "Gradient Descent(16/49): loss=0.45017306880019403\n",
      "Gradient Descent(17/49): loss=0.4501730688000377\n",
      "Gradient Descent(18/49): loss=0.45017306880000724\n",
      "Gradient Descent(19/49): loss=0.4501730688000016\n",
      "Gradient Descent(20/49): loss=0.45017306880000024\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=0.45017306879999985\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730687999999\n",
      "Gradient Descent(29/49): loss=0.4501730687999999\n",
      "Gradient Descent(30/49): loss=0.45017306879999985\n",
      "Gradient Descent(31/49): loss=0.45017306879999985\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.45017306879999985\n",
      "Gradient Descent(35/49): loss=0.4501730688000001\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688000001\n",
      "Gradient Descent(38/49): loss=0.4501730688000001\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4594211207159123\n",
      "Gradient Descent(2/49): loss=0.45259981110825764\n",
      "Gradient Descent(3/49): loss=0.4514531489632104\n",
      "Gradient Descent(4/49): loss=0.45126039505662824\n",
      "Gradient Descent(5/49): loss=0.45122799312493167\n",
      "Gradient Descent(6/49): loss=0.45122254636021364\n",
      "Gradient Descent(7/49): loss=0.4512216307590644\n",
      "Gradient Descent(8/49): loss=0.45122147684651126\n",
      "Gradient Descent(9/49): loss=0.451221450973811\n",
      "Gradient Descent(10/49): loss=0.45122144662461017\n",
      "Gradient Descent(11/49): loss=0.45122144589350954\n",
      "Gradient Descent(12/49): loss=0.4512214457706116\n",
      "Gradient Descent(13/49): loss=0.4512214457499524\n",
      "Gradient Descent(14/49): loss=0.45122144574647977\n",
      "Gradient Descent(15/49): loss=0.4512214457458956\n",
      "Gradient Descent(16/49): loss=0.45122144574579753\n",
      "Gradient Descent(17/49): loss=0.45122144574578116\n",
      "Gradient Descent(18/49): loss=0.4512214457457783\n",
      "Gradient Descent(19/49): loss=0.45122144574577794\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.45122144574577805\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45840318462708074\n",
      "Gradient Descent(2/49): loss=0.45141075996289276\n",
      "Gradient Descent(3/49): loss=0.4502353333768427\n",
      "Gradient Descent(4/49): loss=0.4500377441677278\n",
      "Gradient Descent(5/49): loss=0.45000452942167546\n",
      "Gradient Descent(6/49): loss=0.44999894602286405\n",
      "Gradient Descent(7/49): loss=0.44999800745352403\n",
      "Gradient Descent(8/49): loss=0.449997849680018\n",
      "Gradient Descent(9/49): loss=0.44999782315829157\n",
      "Gradient Descent(10/49): loss=0.4499978186999892\n",
      "Gradient Descent(11/49): loss=0.44999781795054866\n",
      "Gradient Descent(12/49): loss=0.4499978178245678\n",
      "Gradient Descent(13/49): loss=0.4499978178033903\n",
      "Gradient Descent(14/49): loss=0.4499978177998305\n",
      "Gradient Descent(15/49): loss=0.44999781779923215\n",
      "Gradient Descent(16/49): loss=0.44999781779913145\n",
      "Gradient Descent(17/49): loss=0.44999781779911446\n",
      "Gradient Descent(18/49): loss=0.4499978177991117\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991113\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991113\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911097\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991113\n",
      "Gradient Descent(37/49): loss=0.44999781779911135\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.44999781779911135\n",
      "Gradient Descent(41/49): loss=0.44999781779911135\n",
      "Gradient Descent(42/49): loss=0.44999781779911135\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4588844406080001\n",
      "Gradient Descent(2/49): loss=0.45197291507420473\n",
      "Gradient Descent(3/49): loss=0.4508110876319737\n",
      "Gradient Descent(4/49): loss=0.45061578443893474\n",
      "Gradient Descent(5/49): loss=0.4505829539721849\n",
      "Gradient Descent(6/49): loss=0.4505774351707243\n",
      "Gradient Descent(7/49): loss=0.45057650746019884\n",
      "Gradient Descent(8/49): loss=0.45057635151205944\n",
      "Gradient Descent(9/49): loss=0.450576325297177\n",
      "Gradient Descent(10/49): loss=0.4505763208904556\n",
      "Gradient Descent(11/49): loss=0.45057632014968557\n",
      "Gradient Descent(12/49): loss=0.45057632002516224\n",
      "Gradient Descent(13/49): loss=0.45057632000422976\n",
      "Gradient Descent(14/49): loss=0.4505763200007111\n",
      "Gradient Descent(15/49): loss=0.4505763200001195\n",
      "Gradient Descent(16/49): loss=0.45057632000002007\n",
      "Gradient Descent(17/49): loss=0.45057632000000325\n",
      "Gradient Descent(18/49): loss=0.4505763200000006\n",
      "Gradient Descent(19/49): loss=0.4505763200000002\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.45057632000000014\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057631999999975\n",
      "Gradient Descent(35/49): loss=0.45057631999999975\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.4505763200000002\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45854897593472016\n",
      "Gradient Descent(2/49): loss=0.45158105878934623\n",
      "Gradient Descent(3/49): loss=0.4504097519172092\n",
      "Gradient Descent(4/49): loss=0.45021285523200283\n",
      "Gradient Descent(5/49): loss=0.4501797568992198\n",
      "Gradient Descent(6/49): loss=0.45017419306947903\n",
      "Gradient Descent(7/49): loss=0.45017325778969935\n",
      "Gradient Descent(8/49): loss=0.4501731005691686\n",
      "Gradient Descent(9/49): loss=0.4501730741403972\n",
      "Gradient Descent(10/49): loss=0.45017306969772086\n",
      "Gradient Descent(11/49): loss=0.4501730689509068\n",
      "Gradient Descent(12/49): loss=0.4501730688253676\n",
      "Gradient Descent(13/49): loss=0.4501730688042643\n",
      "Gradient Descent(14/49): loss=0.4501730688007167\n",
      "Gradient Descent(15/49): loss=0.4501730688001205\n",
      "Gradient Descent(16/49): loss=0.45017306880002034\n",
      "Gradient Descent(17/49): loss=0.4501730688000035\n",
      "Gradient Descent(18/49): loss=0.45017306880000046\n",
      "Gradient Descent(19/49): loss=0.45017306880000024\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730687999999\n",
      "Gradient Descent(24/49): loss=0.45017306879999985\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730687999999\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.4501730687999998\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45826506898008745\n",
      "Gradient Descent(2/49): loss=0.4522385449408123\n",
      "Gradient Descent(3/49): loss=0.45136831486954077\n",
      "Gradient Descent(4/49): loss=0.4512426536472491\n",
      "Gradient Descent(5/49): loss=0.45122450816675036\n",
      "Gradient Descent(6/49): loss=0.451221887959366\n",
      "Gradient Descent(7/49): loss=0.4512215096014199\n",
      "Gradient Descent(8/49): loss=0.4512214549665326\n",
      "Gradient Descent(9/49): loss=0.4512214470772549\n",
      "Gradient Descent(10/49): loss=0.4512214459380431\n",
      "Gradient Descent(11/49): loss=0.4512214457735409\n",
      "Gradient Descent(12/49): loss=0.45122144574978684\n",
      "Gradient Descent(13/49): loss=0.45122144574635653\n",
      "Gradient Descent(14/49): loss=0.4512214457458615\n",
      "Gradient Descent(15/49): loss=0.45122144574578976\n",
      "Gradient Descent(16/49): loss=0.4512214457457795\n",
      "Gradient Descent(17/49): loss=0.45122144574577805\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.45122144574577766\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.45122144574577805\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.45122144574577794\n",
      "Gradient Descent(31/49): loss=0.45122144574577794\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4572181329089196\n",
      "Gradient Descent(2/49): loss=0.4510404313009674\n",
      "Gradient Descent(3/49): loss=0.4501483711887792\n",
      "Gradient Descent(4/49): loss=0.4500195577085792\n",
      "Gradient Descent(5/49): loss=0.45000095704203846\n",
      "Gradient Descent(6/49): loss=0.4499982711057899\n",
      "Gradient Descent(7/49): loss=0.44999788325659573\n",
      "Gradient Descent(8/49): loss=0.4499978272511718\n",
      "Gradient Descent(9/49): loss=0.44999781916398857\n",
      "Gradient Descent(10/49): loss=0.4499978179961995\n",
      "Gradient Descent(11/49): loss=0.4499978178275708\n",
      "Gradient Descent(12/49): loss=0.4499978178032207\n",
      "Gradient Descent(13/49): loss=0.44999781779970455\n",
      "Gradient Descent(14/49): loss=0.4499978177991968\n",
      "Gradient Descent(15/49): loss=0.44999781779912346\n",
      "Gradient Descent(16/49): loss=0.44999781779911296\n",
      "Gradient Descent(17/49): loss=0.44999781779911135\n",
      "Gradient Descent(18/49): loss=0.4499978177991113\n",
      "Gradient Descent(19/49): loss=0.4499978177991111\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.4499978177991113\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991109\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.44999781779911097\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.44999781779911097\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.4499978177991113\n",
      "Gradient Descent(34/49): loss=0.44999781779911135\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991112\n",
      "Gradient Descent(37/49): loss=0.44999781779911135\n",
      "Gradient Descent(38/49): loss=0.44999781779911135\n",
      "Gradient Descent(39/49): loss=0.44999781779911135\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45771309939200006\n",
      "Gradient Descent(2/49): loss=0.4516068709442049\n",
      "Gradient Descent(3/49): loss=0.45072513155634314\n",
      "Gradient Descent(4/49): loss=0.4505978083887358\n",
      "Gradient Descent(5/49): loss=0.45057942292333353\n",
      "Gradient Descent(6/49): loss=0.4505767680621293\n",
      "Gradient Descent(7/49): loss=0.4505763847001712\n",
      "Gradient Descent(8/49): loss=0.45057632934270475\n",
      "Gradient Descent(9/49): loss=0.4505763213490864\n",
      "Gradient Descent(10/49): loss=0.4505763201948081\n",
      "Gradient Descent(11/49): loss=0.45057632002813053\n",
      "Gradient Descent(12/49): loss=0.45057632000406195\n",
      "Gradient Descent(13/49): loss=0.4505763200005867\n",
      "Gradient Descent(14/49): loss=0.4505763200000847\n",
      "Gradient Descent(15/49): loss=0.45057632000001235\n",
      "Gradient Descent(16/49): loss=0.45057632000000175\n",
      "Gradient Descent(17/49): loss=0.45057632000000014\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.45057632\n",
      "Gradient Descent(27/49): loss=0.45057632\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763200000001\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.4505763200000001\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4573680776652799\n",
      "Gradient Descent(2/49): loss=0.4512120280801466\n",
      "Gradient Descent(3/49): loss=0.45032309452005326\n",
      "Gradient Descent(4/49): loss=0.4501947325139754\n",
      "Gradient Descent(5/49): loss=0.4501761970402979\n",
      "Gradient Descent(6/49): loss=0.45017352051789894\n",
      "Gradient Descent(7/49): loss=0.4501731340280647\n",
      "Gradient Descent(8/49): loss=0.4501730782189326\n",
      "Gradient Descent(9/49): loss=0.4501730701600939\n",
      "Gradient Descent(10/49): loss=0.45017306899639753\n",
      "Gradient Descent(11/49): loss=0.4501730688283597\n",
      "Gradient Descent(12/49): loss=0.4501730688040952\n",
      "Gradient Descent(13/49): loss=0.4501730688005915\n",
      "Gradient Descent(14/49): loss=0.4501730688000854\n",
      "Gradient Descent(15/49): loss=0.45017306880001234\n",
      "Gradient Descent(16/49): loss=0.45017306880000213\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.45017306879999985\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.45017306880000024\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.45017306879999985\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45719681864192\n",
      "Gradient Descent(2/49): loss=0.4519534289255552\n",
      "Gradient Descent(3/49): loss=0.45131111368530047\n",
      "Gradient Descent(4/49): loss=0.4512324300683693\n",
      "Gradient Descent(5/49): loss=0.45122279132529514\n",
      "Gradient Descent(6/49): loss=0.45122161057926885\n",
      "Gradient Descent(7/49): loss=0.4512214659378803\n",
      "Gradient Descent(8/49): loss=0.45122144821931043\n",
      "Gradient Descent(9/49): loss=0.4512214460487856\n",
      "Gradient Descent(10/49): loss=0.4512214457828963\n",
      "Gradient Descent(11/49): loss=0.4512214457503247\n",
      "Gradient Descent(12/49): loss=0.451221445746335\n",
      "Gradient Descent(13/49): loss=0.45122144574584605\n",
      "Gradient Descent(14/49): loss=0.45122144574578626\n",
      "Gradient Descent(15/49): loss=0.45122144574577866\n",
      "Gradient Descent(16/49): loss=0.4512214457457781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457779\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4561230851187201\n",
      "Gradient Descent(2/49): loss=0.45074816304576326\n",
      "Gradient Descent(3/49): loss=0.4500897350918259\n",
      "Gradient Descent(4/49): loss=0.45000907766746867\n",
      "Gradient Descent(5/49): loss=0.449999197132985\n",
      "Gradient Descent(6/49): loss=0.44999798676751074\n",
      "Gradient Descent(7/49): loss=0.44999783849774017\n",
      "Gradient Descent(8/49): loss=0.4499978203346931\n",
      "Gradient Descent(9/49): loss=0.4499978181097197\n",
      "Gradient Descent(10/49): loss=0.44999781783716064\n",
      "Gradient Descent(11/49): loss=0.4499978178037724\n",
      "Gradient Descent(12/49): loss=0.4499978177996821\n",
      "Gradient Descent(13/49): loss=0.4499978177991812\n",
      "Gradient Descent(14/49): loss=0.4499978177991195\n",
      "Gradient Descent(15/49): loss=0.44999781779911213\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.4499978177991111\n",
      "Gradient Descent(20/49): loss=0.4499978177991111\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991113\n",
      "Gradient Descent(30/49): loss=0.44999781779911097\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.44999781779911097\n",
      "Gradient Descent(33/49): loss=0.4499978177991112\n",
      "Gradient Descent(34/49): loss=0.44999781779911135\n",
      "Gradient Descent(35/49): loss=0.44999781779911135\n",
      "Gradient Descent(36/49): loss=0.44999781779911135\n",
      "Gradient Descent(37/49): loss=0.44999781779911135\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4566307208\n",
      "Gradient Descent(2/49): loss=0.4513179840979999\n",
      "Gradient Descent(3/49): loss=0.4506671738520051\n",
      "Gradient Descent(4/49): loss=0.4505874495968707\n",
      "Gradient Descent(5/49): loss=0.45057768337561654\n",
      "Gradient Descent(6/49): loss=0.45057648701351305\n",
      "Gradient Descent(7/49): loss=0.4505763404591554\n",
      "Gradient Descent(8/49): loss=0.4505763225062465\n",
      "Gradient Descent(9/49): loss=0.4505763203070152\n",
      "Gradient Descent(10/49): loss=0.45057632003760945\n",
      "Gradient Descent(11/49): loss=0.4505763200046072\n",
      "Gradient Descent(12/49): loss=0.45057632000056436\n",
      "Gradient Descent(13/49): loss=0.4505763200000692\n",
      "Gradient Descent(14/49): loss=0.45057632000000836\n",
      "Gradient Descent(15/49): loss=0.4505763200000009\n",
      "Gradient Descent(16/49): loss=0.45057632000000014\n",
      "Gradient Descent(17/49): loss=0.45057632000000014\n",
      "Gradient Descent(18/49): loss=0.4505763199999998\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999998\n",
      "Gradient Descent(28/49): loss=0.4505763199999998\n",
      "Gradient Descent(29/49): loss=0.45057631999999975\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763200000002\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4562768678719999\n",
      "Gradient Descent(2/49): loss=0.4509207841863201\n",
      "Gradient Descent(3/49): loss=0.4502646639348242\n",
      "Gradient Descent(4/49): loss=0.45018428920401593\n",
      "Gradient Descent(5/49): loss=0.45017444329949213\n",
      "Gradient Descent(6/49): loss=0.4501732371761878\n",
      "Gradient Descent(7/49): loss=0.45017308942608303\n",
      "Gradient Descent(8/49): loss=0.45017307132669515\n",
      "Gradient Descent(9/49): loss=0.4501730691095203\n",
      "Gradient Descent(10/49): loss=0.4501730688379161\n",
      "Gradient Descent(11/49): loss=0.4501730688046446\n",
      "Gradient Descent(12/49): loss=0.45017306880056895\n",
      "Gradient Descent(13/49): loss=0.45017306880006963\n",
      "Gradient Descent(14/49): loss=0.4501730688000086\n",
      "Gradient Descent(15/49): loss=0.45017306880000096\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730687999998\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.45017306879999985\n",
      "Gradient Descent(20/49): loss=0.4501730687999999\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.45017306880000024\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.45017306879999985\n",
      "Gradient Descent(26/49): loss=0.45017306879999985\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000001\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45621636970141016\n",
      "Gradient Descent(2/49): loss=0.4517329259588347\n",
      "Gradient Descent(3/49): loss=0.45127382131959465\n",
      "Gradient Descent(4/49): loss=0.45122680900453654\n",
      "Gradient Descent(5/49): loss=0.4512219949434746\n",
      "Gradient Descent(6/49): loss=0.451221501983622\n",
      "Gradient Descent(7/49): loss=0.4512214515045332\n",
      "Gradient Descent(8/49): loss=0.45122144633547434\n",
      "Gradient Descent(9/49): loss=0.4512214458061626\n",
      "Gradient Descent(10/49): loss=0.45122144575196116\n",
      "Gradient Descent(11/49): loss=0.4512214457464109\n",
      "Gradient Descent(12/49): loss=0.45122144574584255\n",
      "Gradient Descent(13/49): loss=0.4512214457457844\n",
      "Gradient Descent(14/49): loss=0.4512214457457785\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.45122144574577794\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45511804125648214\n",
      "Gradient Descent(2/49): loss=0.450522128681146\n",
      "Gradient Descent(3/49): loss=0.45005150723343157\n",
      "Gradient Descent(4/49): loss=0.4500033155971855\n",
      "Gradient Descent(5/49): loss=0.44999838077363385\n",
      "Gradient Descent(6/49): loss=0.4499978754477024\n",
      "Gradient Descent(7/49): loss=0.4499978237023268\n",
      "Gradient Descent(8/49): loss=0.4499978184036004\n",
      "Gradient Descent(9/49): loss=0.44999781786101095\n",
      "Gradient Descent(10/49): loss=0.44999781780544956\n",
      "Gradient Descent(11/49): loss=0.4499978177997601\n",
      "Gradient Descent(12/49): loss=0.44999781779917764\n",
      "Gradient Descent(13/49): loss=0.4499978177991179\n",
      "Gradient Descent(14/49): loss=0.44999781779911185\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.4499978177991111\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.4499978177991111\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.4499978177991109\n",
      "Gradient Descent(26/49): loss=0.44999781779911097\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.44999781779911135\n",
      "Gradient Descent(29/49): loss=0.44999781779911097\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.44999781779911135\n",
      "Gradient Descent(32/49): loss=0.44999781779911135\n",
      "Gradient Descent(33/49): loss=0.44999781779911135\n",
      "Gradient Descent(34/49): loss=0.44999781779911135\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4556373048319998\n",
      "Gradient Descent(2/49): loss=0.4510945648467968\n",
      "Gradient Descent(3/49): loss=0.4506293882723121\n",
      "Gradient Descent(4/49): loss=0.45058175419108476\n",
      "Gradient Descent(5/49): loss=0.4505768764611671\n",
      "Gradient Descent(6/49): loss=0.4505763769816234\n",
      "Gradient Descent(7/49): loss=0.45057632583491813\n",
      "Gradient Descent(8/49): loss=0.4505763205974958\n",
      "Gradient Descent(9/49): loss=0.45057632006118353\n",
      "Gradient Descent(10/49): loss=0.4505763200062653\n",
      "Gradient Descent(11/49): loss=0.45057632000064146\n",
      "Gradient Descent(12/49): loss=0.4505763200000657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=0.45057632000000664\n",
      "Gradient Descent(14/49): loss=0.4505763200000006\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.4505763199999998\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.4505763199999998\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.45057632\n",
      "Gradient Descent(26/49): loss=0.4505763200000002\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763200000001\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.4505763200000002\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4552753465548801\n",
      "Gradient Descent(2/49): loss=0.4506955420420997\n",
      "Gradient Descent(3/49): loss=0.45022657005999084\n",
      "Gradient Descent(4/49): loss=0.450178547329023\n",
      "Gradient Descent(5/49): loss=0.450173629801372\n",
      "Gradient Descent(6/49): loss=0.45017312624654043\n",
      "Gradient Descent(7/49): loss=0.4501730746825258\n",
      "Gradient Descent(8/49): loss=0.4501730694023706\n",
      "Gradient Descent(9/49): loss=0.45017306886168257\n",
      "Gradient Descent(10/49): loss=0.4501730688063164\n",
      "Gradient Descent(11/49): loss=0.4501730688006469\n",
      "Gradient Descent(12/49): loss=0.45017306880006636\n",
      "Gradient Descent(13/49): loss=0.45017306880000674\n",
      "Gradient Descent(14/49): loss=0.4501730688000008\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730687999998\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999998\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730687999999\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4553237221585579\n",
      "Gradient Descent(2/49): loss=0.45156644719209266\n",
      "Gradient Descent(3/49): loss=0.45125046036741295\n",
      "Gradient Descent(4/49): loss=0.4512238858754574\n",
      "Gradient Descent(5/49): loss=0.45122165096068395\n",
      "Gradient Descent(6/49): loss=0.45122146300435145\n",
      "Gradient Descent(7/49): loss=0.45122144719722385\n",
      "Gradient Descent(8/49): loss=0.45122144586784446\n",
      "Gradient Descent(9/49): loss=0.45122144575604345\n",
      "Gradient Descent(10/49): loss=0.4512214457466411\n",
      "Gradient Descent(11/49): loss=0.4512214457458504\n",
      "Gradient Descent(12/49): loss=0.4512214457457839\n",
      "Gradient Descent(13/49): loss=0.4512214457457782\n",
      "Gradient Descent(14/49): loss=0.45122144574577794\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.45122144574577794\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.45122144574577766\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45420300132220565\n",
      "Gradient Descent(2/49): loss=0.4503514737334035\n",
      "Gradient Descent(3/49): loss=0.45002756026318497\n",
      "Gradient Descent(4/49): loss=0.45000031914033967\n",
      "Gradient Descent(5/49): loss=0.44999802816190837\n",
      "Gradient Descent(6/49): loss=0.4499978354906222\n",
      "Gradient Descent(7/49): loss=0.44999781928696714\n",
      "Gradient Descent(8/49): loss=0.44999781792423993\n",
      "Gradient Descent(9/49): loss=0.44999781780963444\n",
      "Gradient Descent(10/49): loss=0.4499978177999961\n",
      "Gradient Descent(11/49): loss=0.44999781779918546\n",
      "Gradient Descent(12/49): loss=0.4499978177991175\n",
      "Gradient Descent(13/49): loss=0.44999781779911174\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911097\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.4499978177991111\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991109\n",
      "Gradient Descent(25/49): loss=0.4499978177991108\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.44999781779911135\n",
      "Gradient Descent(30/49): loss=0.44999781779911135\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4547328514879999\n",
      "Gradient Descent(2/49): loss=0.4509258842981408\n",
      "Gradient Descent(3/49): loss=0.45060571835747376\n",
      "Gradient Descent(4/49): loss=0.4505787924018635\n",
      "Gradient Descent(5/49): loss=0.4505765279289968\n",
      "Gradient Descent(6/49): loss=0.4505763374868285\n",
      "Gradient Descent(7/49): loss=0.45057632147064236\n",
      "Gradient Descent(8/49): loss=0.45057632012368104\n",
      "Gradient Descent(9/49): loss=0.4505763200104014\n",
      "Gradient Descent(10/49): loss=0.45057632000087494\n",
      "Gradient Descent(11/49): loss=0.45057632000007364\n",
      "Gradient Descent(12/49): loss=0.4505763200000062\n",
      "Gradient Descent(13/49): loss=0.45057632000000053\n",
      "Gradient Descent(14/49): loss=0.45057632000000014\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.4505763200000003\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763199999998\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.45057632000000014\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763200000001\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45436351371392003\n",
      "Gradient Descent(2/49): loss=0.4505254852172608\n",
      "Gradient Descent(3/49): loss=0.4502027070206918\n",
      "Gradient Descent(4/49): loss=0.45017556137435993\n",
      "Gradient Descent(5/49): loss=0.45017327842550375\n",
      "Gradient Descent(6/49): loss=0.4501730864295048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=0.4501730702826412\n",
      "Gradient Descent(8/49): loss=0.4501730689246901\n",
      "Gradient Descent(9/49): loss=0.45017306881048647\n",
      "Gradient Descent(10/49): loss=0.45017306880088215\n",
      "Gradient Descent(11/49): loss=0.4501730688000742\n",
      "Gradient Descent(12/49): loss=0.4501730688000063\n",
      "Gradient Descent(13/49): loss=0.4501730688000004\n",
      "Gradient Descent(14/49): loss=0.4501730687999999\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730687999998\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45451887601336316\n",
      "Gradient Descent(2/49): loss=0.45144435203186656\n",
      "Gradient Descent(3/49): loss=0.45123651421071737\n",
      "Gradient Descent(4/49): loss=0.45122246437400765\n",
      "Gradient Descent(5/49): loss=0.451221514605046\n",
      "Gradient Descent(6/49): loss=0.4512214504006644\n",
      "Gradient Descent(7/49): loss=0.4512214460604481\n",
      "Gradient Descent(8/49): loss=0.45122144576704953\n",
      "Gradient Descent(9/49): loss=0.4512214457472158\n",
      "Gradient Descent(10/49): loss=0.45122144574587514\n",
      "Gradient Descent(11/49): loss=0.4512214457457844\n",
      "Gradient Descent(12/49): loss=0.4512214457457781\n",
      "Gradient Descent(13/49): loss=0.45122144574577766\n",
      "Gradient Descent(14/49): loss=0.4512214457457778\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.45122144574577794\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.45122144574577794\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.45122144574577794\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45337796531589125\n",
      "Gradient Descent(2/49): loss=0.4502263157712452\n",
      "Gradient Descent(3/49): loss=0.4500132642620273\n",
      "Gradient Descent(4/49): loss=0.44999886198000416\n",
      "Gradient Descent(5/49): loss=0.4499978883857395\n",
      "Gradient Descent(6/49): loss=0.44999782257076726\n",
      "Gradient Descent(7/49): loss=0.44999781812167516\n",
      "Gradient Descent(8/49): loss=0.44999781782091647\n",
      "Gradient Descent(9/49): loss=0.449997817800585\n",
      "Gradient Descent(10/49): loss=0.4499978177992108\n",
      "Gradient Descent(11/49): loss=0.44999781779911774\n",
      "Gradient Descent(12/49): loss=0.4499978177991116\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.4499978177991111\n",
      "Gradient Descent(18/49): loss=0.4499978177991109\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911097\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.44999781779911135\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991111\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.453917360768\n",
      "Gradient Descent(2/49): loss=0.4508021743559169\n",
      "Gradient Descent(3/49): loss=0.45059158775445995\n",
      "Gradient Descent(4/49): loss=0.4505773521002015\n",
      "Gradient Descent(5/49): loss=0.4505763897699737\n",
      "Gradient Descent(6/49): loss=0.4505763247164501\n",
      "Gradient Descent(7/49): loss=0.450576320318832\n",
      "Gradient Descent(8/49): loss=0.45057632002155307\n",
      "Gradient Descent(9/49): loss=0.450576320001457\n",
      "Gradient Descent(10/49): loss=0.45057632000009834\n",
      "Gradient Descent(11/49): loss=0.4505763200000066\n",
      "Gradient Descent(12/49): loss=0.4505763200000005\n",
      "Gradient Descent(13/49): loss=0.45057632000000014\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.45057632\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45354136934911987\n",
      "Gradient Descent(2/49): loss=0.4504007659171205\n",
      "Gradient Descent(3/49): loss=0.45018846112511735\n",
      "Gradient Descent(4/49): loss=0.4501741093211781\n",
      "Gradient Descent(5/49): loss=0.4501731391392317\n",
      "Gradient Descent(6/49): loss=0.45017307355493197\n",
      "Gradient Descent(7/49): loss=0.45017306912143334\n",
      "Gradient Descent(8/49): loss=0.45017306882172875\n",
      "Gradient Descent(9/49): loss=0.45017306880146896\n",
      "Gradient Descent(10/49): loss=0.45017306880009944\n",
      "Gradient Descent(11/49): loss=0.45017306880000685\n",
      "Gradient Descent(12/49): loss=0.45017306880000046\n",
      "Gradient Descent(13/49): loss=0.4501730688000001\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45380183126582624\n",
      "Gradient Descent(2/49): loss=0.45135794813978825\n",
      "Gradient Descent(3/49): loss=0.451228666722421\n",
      "Gradient Descent(4/49): loss=0.4512218277354423\n",
      "Gradient Descent(5/49): loss=0.4512214659530311\n",
      "Gradient Descent(6/49): loss=0.4512214468147413\n",
      "Gradient Descent(7/49): loss=0.4512214458023261\n",
      "Gradient Descent(8/49): loss=0.4512214457487694\n",
      "Gradient Descent(9/49): loss=0.45122144574593614\n",
      "Gradient Descent(10/49): loss=0.45122144574578626\n",
      "Gradient Descent(11/49): loss=0.4512214457457781\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.45122144574577766\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45264293323753796\n",
      "Gradient Descent(2/49): loss=0.45013774440580373\n",
      "Gradient Descent(3/49): loss=0.4500052199166051\n",
      "Gradient Descent(4/49): loss=0.4499982093711264\n",
      "Gradient Descent(5/49): loss=0.4499978385132708\n",
      "Gradient Descent(6/49): loss=0.4499978188948902\n",
      "Gradient Descent(7/49): loss=0.4499978178570777\n",
      "Gradient Descent(8/49): loss=0.4499978178021775\n",
      "Gradient Descent(9/49): loss=0.4499978177992733\n",
      "Gradient Descent(10/49): loss=0.44999781779911957\n",
      "Gradient Descent(11/49): loss=0.44999781779911135\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991113\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.4499978177991108\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.4499978177991109\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.4499978177991112\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.44999781779911135\n",
      "Gradient Descent(25/49): loss=0.44999781779911135\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4531908326720001\n",
      "Gradient Descent(2/49): loss=0.45071462772034876\n",
      "Gradient Descent(3/49): loss=0.4505836364784063\n",
      "Gradient Descent(4/49): loss=0.4505767070417078\n",
      "Gradient Descent(5/49): loss=0.4505763404745063\n",
      "Gradient Descent(6/49): loss=0.4505763210831013\n",
      "Gradient Descent(7/49): loss=0.45057632005729603\n",
      "Gradient Descent(8/49): loss=0.45057632000303105\n",
      "Gradient Descent(9/49): loss=0.45057632000016035\n",
      "Gradient Descent(10/49): loss=0.45057632000000847\n",
      "Gradient Descent(11/49): loss=0.45057632000000053\n",
      "Gradient Descent(12/49): loss=0.45057632\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.45057632000000014\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763199999998\n",
      "Gradient Descent(21/49): loss=0.45057631999999975\n",
      "Gradient Descent(22/49): loss=0.4505763200000002\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763200000002\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45280891346048\n",
      "Gradient Descent(2/49): loss=0.4503125049825394\n",
      "Gradient Descent(3/49): loss=0.45018044497405635\n",
      "Gradient Descent(4/49): loss=0.45017345899960753\n",
      "Gradient Descent(5/49): loss=0.45017308944155937\n",
      "Gradient Descent(6/49): loss=0.45017306989193834\n",
      "Gradient Descent(7/49): loss=0.45017306885776376\n",
      "Gradient Descent(8/49): loss=0.4501730688030555\n",
      "Gradient Descent(9/49): loss=0.4501730688001616\n",
      "Gradient Descent(10/49): loss=0.45017306880000857\n",
      "Gradient Descent(11/49): loss=0.4501730688000004\n",
      "Gradient Descent(12/49): loss=0.4501730687999999\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688000001\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.45017306879999985\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.45017306879999985\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45317258791594667\n",
      "Gradient Descent(2/49): loss=0.4512994914325846\n",
      "Gradient Descent(3/49): loss=0.45122456757324997\n",
      "Gradient Descent(4/49): loss=0.45122157061887663\n",
      "Gradient Descent(5/49): loss=0.45122145074070186\n",
      "Gradient Descent(6/49): loss=0.4512214459455747\n",
      "Gradient Descent(7/49): loss=0.4512214457537697\n",
      "Gradient Descent(8/49): loss=0.45122144574609757\n",
      "Gradient Descent(9/49): loss=0.45122144574579054\n",
      "Gradient Descent(10/49): loss=0.45122144574577827\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.45122144574577755\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.45122144574577766\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45199790508714655\n",
      "Gradient Descent(2/49): loss=0.45007782129063245\n",
      "Gradient Descent(3/49): loss=0.4500010179387719\n",
      "Gradient Descent(4/49): loss=0.44999794580469754\n",
      "Gradient Descent(5/49): loss=0.4499978229193344\n",
      "Gradient Descent(6/49): loss=0.4499978180039201\n",
      "Gradient Descent(7/49): loss=0.4499978178073036\n",
      "Gradient Descent(8/49): loss=0.4499978177994388\n",
      "Gradient Descent(9/49): loss=0.44999781779912407\n",
      "Gradient Descent(10/49): loss=0.44999781779911174\n",
      "Gradient Descent(11/49): loss=0.4499978177991113\n",
      "Gradient Descent(12/49): loss=0.44999781779911113\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911097\n",
      "Gradient Descent(18/49): loss=0.4499978177991109\n",
      "Gradient Descent(19/49): loss=0.44999781779911097\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991112\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45255326720000005\n",
      "Gradient Descent(2/49): loss=0.450655397888\n",
      "Gradient Descent(3/49): loss=0.45057948311551993\n",
      "Gradient Descent(4/49): loss=0.4505764465246208\n",
      "Gradient Descent(5/49): loss=0.4505763250609849\n",
      "Gradient Descent(6/49): loss=0.45057632020243943\n",
      "Gradient Descent(7/49): loss=0.4505763200080976\n",
      "Gradient Descent(8/49): loss=0.4505763200003239\n",
      "Gradient Descent(9/49): loss=0.4505763200000128\n",
      "Gradient Descent(10/49): loss=0.4505763200000006\n",
      "Gradient Descent(11/49): loss=0.45057632000000014\n",
      "Gradient Descent(12/49): loss=0.45057632\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.45057632000000014\n",
      "Gradient Descent(17/49): loss=0.45057632000000014\n",
      "Gradient Descent(18/49): loss=0.45057632\n",
      "Gradient Descent(19/49): loss=0.45057632000000014\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45216614604799993\n",
      "Gradient Descent(2/49): loss=0.45025279188991996\n",
      "Gradient Descent(3/49): loss=0.4501762577235969\n",
      "Gradient Descent(4/49): loss=0.45017319635694386\n",
      "Gradient Descent(5/49): loss=0.45017307390227773\n",
      "Gradient Descent(6/49): loss=0.450173069004091\n",
      "Gradient Descent(7/49): loss=0.4501730688081635\n",
      "Gradient Descent(8/49): loss=0.4501730688003264\n",
      "Gradient Descent(9/49): loss=0.45017306880001307\n",
      "Gradient Descent(10/49): loss=0.4501730688000006\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688000001\n",
      "Gradient Descent(13/49): loss=0.45017306879999985\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730687999998\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730687999998\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4526311459637247\n",
      "Gradient Descent(2/49): loss=0.4512621860820764\n",
      "Gradient Descent(3/49): loss=0.4512226231414968\n",
      "Gradient Descent(4/49): loss=0.45122147977251414\n",
      "Gradient Descent(5/49): loss=0.4512214467291504\n",
      "Gradient Descent(6/49): loss=0.45122144577419737\n",
      "Gradient Descent(7/49): loss=0.451221445746599\n",
      "Gradient Descent(8/49): loss=0.45122144574580153\n",
      "Gradient Descent(9/49): loss=0.4512214457457785\n",
      "Gradient Descent(10/49): loss=0.45122144574577766\n",
      "Gradient Descent(11/49): loss=0.45122144574577755\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.45122144574577794\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45144288086471684\n",
      "Gradient Descent(2/49): loss=0.4500395801217072\n",
      "Gradient Descent(3/49): loss=0.4499990247302342\n",
      "Gradient Descent(4/49): loss=0.44999785267942066\n",
      "Gradient Descent(5/49): loss=0.4499978188071519\n",
      "Gradient Descent(6/49): loss=0.44999781782824344\n",
      "Gradient Descent(7/49): loss=0.44999781779995307\n",
      "Gradient Descent(8/49): loss=0.44999781779913545\n",
      "Gradient Descent(9/49): loss=0.44999781779911185\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.44999781779911113\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.4499978177991113\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.4499978177991109\n",
      "Gradient Descent(16/49): loss=0.44999781779911097\n",
      "Gradient Descent(17/49): loss=0.44999781779911135\n",
      "Gradient Descent(18/49): loss=0.44999781779911147\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911135\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4520046643519998\n",
      "Gradient Descent(2/49): loss=0.45061759915177274\n",
      "Gradient Descent(3/49): loss=0.45057751296748627\n",
      "Gradient Descent(4/49): loss=0.45057635447676037\n",
      "Gradient Descent(5/49): loss=0.4505763209963786\n",
      "Gradient Descent(6/49): loss=0.45057632002879533\n",
      "Gradient Descent(7/49): loss=0.45057632000083214\n",
      "Gradient Descent(8/49): loss=0.45057632000002407\n",
      "Gradient Descent(9/49): loss=0.4505763200000007\n",
      "Gradient Descent(10/49): loss=0.4505763200000002\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763199999998\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999998\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4516130671116798\n",
      "Gradient Descent(2/49): loss=0.45021468475120746\n",
      "Gradient Descent(3/49): loss=0.4501742715009899\n",
      "Gradient Descent(4/49): loss=0.45017310355805856\n",
      "Gradient Descent(5/49): loss=0.4501730698045078\n",
      "Gradient Descent(6/49): loss=0.4501730688290306\n",
      "Gradient Descent(7/49): loss=0.45017306880083896\n",
      "Gradient Descent(8/49): loss=0.4501730688000242\n",
      "Gradient Descent(9/49): loss=0.4501730688000007\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730687999999\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4521775054091604\n",
      "Gradient Descent(2/49): loss=0.4512401845151802\n",
      "Gradient Descent(3/49): loss=0.4512218130256581\n",
      "Gradient Descent(4/49): loss=0.4512214529444634\n",
      "Gradient Descent(5/49): loss=0.4512214458868719\n",
      "Gradient Descent(6/49): loss=0.4512214457485434\n",
      "Gradient Descent(7/49): loss=0.45122144574583195\n",
      "Gradient Descent(8/49): loss=0.4512214457457788\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457777\n",
      "Gradient Descent(11/49): loss=0.45122144574577766\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.45122144574577794\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4509778605702485\n",
      "Gradient Descent(2/49): loss=0.4500170266374255\n",
      "Gradient Descent(3/49): loss=0.44999819429234206\n",
      "Gradient Descent(4/49): loss=0.44999782517837844\n",
      "Gradient Descent(5/49): loss=0.44999781794374477\n",
      "Gradient Descent(6/49): loss=0.44999781780194587\n",
      "Gradient Descent(7/49): loss=0.44999781779916664\n",
      "Gradient Descent(8/49): loss=0.44999781779911247\n",
      "Gradient Descent(9/49): loss=0.4499978177991113\n",
      "Gradient Descent(10/49): loss=0.4499978177991113\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.44999781779911113\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.4499978177991113\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.44999781779911135\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.44999781779911135\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.451545024128\n",
      "Gradient Descent(2/49): loss=0.45059530660090874\n",
      "Gradient Descent(3/49): loss=0.45057669213737794\n",
      "Gradient Descent(4/49): loss=0.45057632729389246\n",
      "Gradient Descent(5/49): loss=0.45057632014296034\n",
      "Gradient Descent(6/49): loss=0.4505763200028022\n",
      "Gradient Descent(7/49): loss=0.45057632000005493\n",
      "Gradient Descent(8/49): loss=0.45057632000000114\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.45057632\n",
      "Gradient Descent(11/49): loss=0.4505763200000001\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999998\n",
      "Gradient Descent(14/49): loss=0.45057632000000014\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4511496766515201\n",
      "Gradient Descent(2/49): loss=0.45019221031388984\n",
      "Gradient Descent(3/49): loss=0.4501734439736721\n",
      "Gradient Descent(4/49): loss=0.4501730761534039\n",
      "Gradient Descent(5/49): loss=0.4501730689441267\n",
      "Gradient Descent(6/49): loss=0.45017306880282504\n",
      "Gradient Descent(7/49): loss=0.4501730688000554\n",
      "Gradient Descent(8/49): loss=0.450173068800001\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.4501730687999999\n",
      "Gradient Descent(11/49): loss=0.4501730688000001\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730687999999\n",
      "Gradient Descent(14/49): loss=0.4501730688000001\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45181166625225355\n",
      "Gradient Descent(2/49): loss=0.45122858741390626\n",
      "Gradient Descent(3/49): loss=0.4512215321599621\n",
      "Gradient Descent(4/49): loss=0.4512214467913895\n",
      "Gradient Descent(5/49): loss=0.45122144575842976\n",
      "Gradient Descent(6/49): loss=0.45122144574593076\n",
      "Gradient Descent(7/49): loss=0.45122144574577977\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.4512214457457778\n",
      "Gradient Descent(10/49): loss=0.4512214457457779\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.45122144574577794\n",
      "Gradient Descent(13/49): loss=0.45122144574577766\n",
      "Gradient Descent(14/49): loss=0.45122144574577794\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.45122144574577805\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4506028442037417\n",
      "Gradient Descent(2/49): loss=0.45000513861860714\n",
      "Gradient Descent(3/49): loss=0.449997906381027\n",
      "Gradient Descent(4/49): loss=0.4499978188709522\n",
      "Gradient Descent(5/49): loss=0.4499978178120805\n",
      "Gradient Descent(6/49): loss=0.44999781779926823\n",
      "Gradient Descent(7/49): loss=0.44999781779911313\n",
      "Gradient Descent(8/49): loss=0.44999781779911097\n",
      "Gradient Descent(9/49): loss=0.4499978177991112\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.44999781779911097\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.44999781779911135\n",
      "Gradient Descent(16/49): loss=0.44999781779911135\n",
      "Gradient Descent(17/49): loss=0.4499978177991111\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.4499978177991111\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.451174346528\n",
      "Gradient Descent(2/49): loss=0.4505835561209888\n",
      "Gradient Descent(3/49): loss=0.4505764075570639\n",
      "Gradient Descent(4/49): loss=0.4505763210594405\n",
      "Gradient Descent(5/49): loss=0.450576320012819\n",
      "Gradient Descent(6/49): loss=0.4505763200001552\n",
      "Gradient Descent(7/49): loss=0.45057632000000175\n",
      "Gradient Descent(8/49): loss=0.4505763200000001\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.45057632\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.45057632\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.4505763200000002\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45077597466752\n",
      "Gradient Descent(2/49): loss=0.4501803639609968\n",
      "Gradient Descent(3/49): loss=0.45017315707144806\n",
      "Gradient Descent(4/49): loss=0.4501730698680846\n",
      "Gradient Descent(5/49): loss=0.4501730688129238\n",
      "Gradient Descent(6/49): loss=0.4501730688001564\n",
      "Gradient Descent(7/49): loss=0.45017306880000185\n",
      "Gradient Descent(8/49): loss=0.4501730688000002\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.45017306879999985\n",
      "Gradient Descent(11/49): loss=0.4501730688000001\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4515336284930047\n",
      "Gradient Descent(2/49): loss=0.4512234437153599\n",
      "Gradient Descent(3/49): loss=0.45122145853278295\n",
      "Gradient Descent(4/49): loss=0.4512214458276147\n",
      "Gradient Descent(5/49): loss=0.4512214457463015\n",
      "Gradient Descent(6/49): loss=0.45122144574578116\n",
      "Gradient Descent(7/49): loss=0.4512214457457779\n",
      "Gradient Descent(8/49): loss=0.45122144574577794\n",
      "Gradient Descent(9/49): loss=0.4512214457457779\n",
      "Gradient Descent(10/49): loss=0.45122144574577766\n",
      "Gradient Descent(11/49): loss=0.4512214457457779\n",
      "Gradient Descent(12/49): loss=0.45122144574577794\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457779\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4503178317651969\n",
      "Gradient Descent(2/49): loss=0.44999986588849405\n",
      "Gradient Descent(3/49): loss=0.4499978309068833\n",
      "Gradient Descent(4/49): loss=0.4499978178830008\n",
      "Gradient Descent(5/49): loss=0.44999781779964804\n",
      "Gradient Descent(6/49): loss=0.44999781779911463\n",
      "Gradient Descent(7/49): loss=0.4499978177991111\n",
      "Gradient Descent(8/49): loss=0.4499978177991111\n",
      "Gradient Descent(9/49): loss=0.4499978177991111\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991109\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4508926315519998\n",
      "Gradient Descent(2/49): loss=0.4505783443939329\n",
      "Gradient Descent(3/49): loss=0.4505763329561215\n",
      "Gradient Descent(4/49): loss=0.45057632008291926\n",
      "Gradient Descent(5/49): loss=0.4505763200005306\n",
      "Gradient Descent(6/49): loss=0.4505763200000034\n",
      "Gradient Descent(7/49): loss=0.45057632\n",
      "Gradient Descent(8/49): loss=0.45057632\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.4505763200000001\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763200000002\n",
      "Gradient Descent(14/49): loss=0.4505763200000002\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4504919611596801\n",
      "Gradient Descent(2/49): loss=0.4501751097111019\n",
      "Gradient Descent(3/49): loss=0.45017308186183125\n",
      "Gradient Descent(4/49): loss=0.45017306888359554\n",
      "Gradient Descent(5/49): loss=0.450173068800535\n",
      "Gradient Descent(6/49): loss=0.4501730688000036\n",
      "Gradient Descent(7/49): loss=0.4501730688000002\n",
      "Gradient Descent(8/49): loss=0.4501730688000001\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730687999999\n",
      "Gradient Descent(12/49): loss=0.4501730688000001\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4513433921314133\n",
      "Gradient Descent(2/49): loss=0.4512217506117421\n",
      "Gradient Descent(3/49): loss=0.4512214465079426\n",
      "Gradient Descent(4/49): loss=0.4512214457476832\n",
      "Gradient Descent(5/49): loss=0.45122144574578255\n",
      "Gradient Descent(6/49): loss=0.4512214457457778\n",
      "Gradient Descent(7/49): loss=0.4512214457457778\n",
      "Gradient Descent(8/49): loss=0.4512214457457777\n",
      "Gradient Descent(9/49): loss=0.4512214457457779\n",
      "Gradient Descent(10/49): loss=0.4512214457457777\n",
      "Gradient Descent(11/49): loss=0.4512214457457779\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45012282325461334\n",
      "Gradient Descent(2/49): loss=0.4499981303127499\n",
      "Gradient Descent(3/49): loss=0.44999781858039517\n",
      "Gradient Descent(4/49): loss=0.44999781780106435\n",
      "Gradient Descent(5/49): loss=0.449997817799116\n",
      "Gradient Descent(6/49): loss=0.44999781779911113\n",
      "Gradient Descent(7/49): loss=0.44999781779911113\n",
      "Gradient Descent(8/49): loss=0.4499978177991108\n",
      "Gradient Descent(9/49): loss=0.4499978177991111\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.44999781779911135\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45069987919999993\n",
      "Gradient Descent(2/49): loss=0.45057662889799993\n",
      "Gradient Descent(3/49): loss=0.4505763207722449\n",
      "Gradient Descent(4/49): loss=0.4505763200019305\n",
      "Gradient Descent(5/49): loss=0.45057632000000464\n",
      "Gradient Descent(6/49): loss=0.45057632000000014\n",
      "Gradient Descent(7/49): loss=0.4505763200000001\n",
      "Gradient Descent(8/49): loss=0.4505763199999999\n",
      "Gradient Descent(9/49): loss=0.4505763200000001\n",
      "Gradient Descent(10/49): loss=0.4505763199999999\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4502976361280001\n",
      "Gradient Descent(2/49): loss=0.45017338021832015\n",
      "Gradient Descent(3/49): loss=0.45017306957854597\n",
      "Gradient Descent(4/49): loss=0.4501730688019463\n",
      "Gradient Descent(5/49): loss=0.4501730688000049\n",
      "Gradient Descent(6/49): loss=0.4501730688000001\n",
      "Gradient Descent(7/49): loss=0.4501730688\n",
      "Gradient Descent(8/49): loss=0.4501730688000001\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688000001\n",
      "Gradient Descent(11/49): loss=0.4501730688000002\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45124095716747936\n",
      "Gradient Descent(2/49): loss=0.4512214535503466\n",
      "Gradient Descent(3/49): loss=0.4512214457488995\n",
      "Gradient Descent(4/49): loss=0.4512214457457791\n",
      "Gradient Descent(5/49): loss=0.45122144574577766\n",
      "Gradient Descent(6/49): loss=0.4512214457457778\n",
      "Gradient Descent(7/49): loss=0.4512214457457779\n",
      "Gradient Descent(8/49): loss=0.4512214457457779\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457777\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4500178186719914\n",
      "Gradient Descent(2/49): loss=0.44999782579946024\n",
      "Gradient Descent(3/49): loss=0.44999781780231124\n",
      "Gradient Descent(4/49): loss=0.44999781779911235\n",
      "Gradient Descent(5/49): loss=0.44999781779911097\n",
      "Gradient Descent(6/49): loss=0.4499978177991113\n",
      "Gradient Descent(7/49): loss=0.4499978177991111\n",
      "Gradient Descent(8/49): loss=0.44999781779911113\n",
      "Gradient Descent(9/49): loss=0.4499978177991112\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4505960894719999\n",
      "Gradient Descent(2/49): loss=0.4505763279077889\n",
      "Gradient Descent(3/49): loss=0.4505763200031632\n",
      "Gradient Descent(4/49): loss=0.45057632000000125\n",
      "Gradient Descent(5/49): loss=0.45057632\n",
      "Gradient Descent(6/49): loss=0.4505763199999999\n",
      "Gradient Descent(7/49): loss=0.4505763200000001\n",
      "Gradient Descent(8/49): loss=0.45057631999999975\n",
      "Gradient Descent(9/49): loss=0.4505763200000001\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45019299957248005\n",
      "Gradient Descent(2/49): loss=0.450173076772309\n",
      "Gradient Descent(3/49): loss=0.450173068803189\n",
      "Gradient Descent(4/49): loss=0.45017306880000124\n",
      "Gradient Descent(5/49): loss=0.45017306879999985\n",
      "Gradient Descent(6/49): loss=0.4501730688\n",
      "Gradient Descent(7/49): loss=0.4501730688000001\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688000002\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4512263236012033\n",
      "Gradient Descent(2/49): loss=0.4512214462335633\n",
      "Gradient Descent(3/49): loss=0.4512214457458266\n",
      "Gradient Descent(4/49): loss=0.4512214457457778\n",
      "Gradient Descent(5/49): loss=0.4512214457457778\n",
      "Gradient Descent(6/49): loss=0.45122144574577766\n",
      "Gradient Descent(7/49): loss=0.4512214457457777\n",
      "Gradient Descent(8/49): loss=0.4512214457457779\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457777\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4500028180173313\n",
      "Gradient Descent(2/49): loss=0.44999781829913305\n",
      "Gradient Descent(3/49): loss=0.44999781779916104\n",
      "Gradient Descent(4/49): loss=0.4499978177991113\n",
      "Gradient Descent(5/49): loss=0.4499978177991109\n",
      "Gradient Descent(6/49): loss=0.44999781779911113\n",
      "Gradient Descent(7/49): loss=0.4499978177991112\n",
      "Gradient Descent(8/49): loss=0.4499978177991112\n",
      "Gradient Descent(9/49): loss=0.44999781779911135\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45058126236800006\n",
      "Gradient Descent(2/49): loss=0.45057632049423657\n",
      "Gradient Descent(3/49): loss=0.45057632000004927\n",
      "Gradient Descent(4/49): loss=0.45057632000000014\n",
      "Gradient Descent(5/49): loss=0.4505763199999998\n",
      "Gradient Descent(6/49): loss=0.4505763200000001\n",
      "Gradient Descent(7/49): loss=0.4505763200000002\n",
      "Gradient Descent(8/49): loss=0.4505763199999999\n",
      "Gradient Descent(9/49): loss=0.4505763200000001\n",
      "Gradient Descent(10/49): loss=0.4505763199999999\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45017805149312007\n",
      "Gradient Descent(2/49): loss=0.4501730692982693\n",
      "Gradient Descent(3/49): loss=0.45017306880004976\n",
      "Gradient Descent(4/49): loss=0.4501730688\n",
      "Gradient Descent(5/49): loss=0.4501730687999998\n",
      "Gradient Descent(6/49): loss=0.4501730688000002\n",
      "Gradient Descent(7/49): loss=0.4501730688\n",
      "Gradient Descent(8/49): loss=0.4501730687999999\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688000002\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688000002\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688000002\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4512994914325845\n",
      "Gradient Descent(2/49): loss=0.45122157061887663\n",
      "Gradient Descent(3/49): loss=0.45122144594557484\n",
      "Gradient Descent(4/49): loss=0.45122144574609735\n",
      "Gradient Descent(5/49): loss=0.4512214457457782\n",
      "Gradient Descent(6/49): loss=0.4512214457457778\n",
      "Gradient Descent(7/49): loss=0.4512214457457778\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.4512214457457778\n",
      "Gradient Descent(10/49): loss=0.4512214457457777\n",
      "Gradient Descent(11/49): loss=0.45122144574577766\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45007782129063273\n",
      "Gradient Descent(2/49): loss=0.4499979458046976\n",
      "Gradient Descent(3/49): loss=0.44999781800392014\n",
      "Gradient Descent(4/49): loss=0.4499978177994388\n",
      "Gradient Descent(5/49): loss=0.4499978177991117\n",
      "Gradient Descent(6/49): loss=0.4499978177991112\n",
      "Gradient Descent(7/49): loss=0.4499978177991112\n",
      "Gradient Descent(8/49): loss=0.44999781779911113\n",
      "Gradient Descent(9/49): loss=0.4499978177991113\n",
      "Gradient Descent(10/49): loss=0.4499978177991112\n",
      "Gradient Descent(11/49): loss=0.44999781779911135\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.450655397888\n",
      "Gradient Descent(2/49): loss=0.45057644652462076\n",
      "Gradient Descent(3/49): loss=0.45057632020243926\n",
      "Gradient Descent(4/49): loss=0.45057632000032394\n",
      "Gradient Descent(5/49): loss=0.4505763200000005\n",
      "Gradient Descent(6/49): loss=0.45057632\n",
      "Gradient Descent(7/49): loss=0.45057632000000014\n",
      "Gradient Descent(8/49): loss=0.4505763199999998\n",
      "Gradient Descent(9/49): loss=0.4505763200000001\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.4505763200000002\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45025279188992007\n",
      "Gradient Descent(2/49): loss=0.4501731963569438\n",
      "Gradient Descent(3/49): loss=0.4501730690040912\n",
      "Gradient Descent(4/49): loss=0.45017306880032654\n",
      "Gradient Descent(5/49): loss=0.45017306880000063\n",
      "Gradient Descent(6/49): loss=0.4501730688000002\n",
      "Gradient Descent(7/49): loss=0.4501730688000001\n",
      "Gradient Descent(8/49): loss=0.4501730687999999\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688000002\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688000002\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4514604606616235\n",
      "Gradient Descent(2/49): loss=0.4512226169188652\n",
      "Gradient Descent(3/49): loss=0.45122145148452597\n",
      "Gradient Descent(4/49): loss=0.45122144577389783\n",
      "Gradient Descent(5/49): loss=0.45122144574591566\n",
      "Gradient Descent(6/49): loss=0.45122144574577844\n",
      "Gradient Descent(7/49): loss=0.45122144574577755\n",
      "Gradient Descent(8/49): loss=0.4512214457457777\n",
      "Gradient Descent(9/49): loss=0.4512214457457778\n",
      "Gradient Descent(10/49): loss=0.45122144574577794\n",
      "Gradient Descent(11/49): loss=0.45122144574577766\n",
      "Gradient Descent(12/49): loss=0.45122144574577805\n",
      "Gradient Descent(13/49): loss=0.45122144574577766\n",
      "Gradient Descent(14/49): loss=0.4512214457457779\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4502428284918953\n",
      "Gradient Descent(2/49): loss=0.44999901835150574\n",
      "Gradient Descent(3/49): loss=0.44999782368181795\n",
      "Gradient Descent(4/49): loss=0.4499978178279366\n",
      "Gradient Descent(5/49): loss=0.4499978177992523\n",
      "Gradient Descent(6/49): loss=0.4499978177991117\n",
      "Gradient Descent(7/49): loss=0.4499978177991113\n",
      "Gradient Descent(8/49): loss=0.44999781779911113\n",
      "Gradient Descent(9/49): loss=0.44999781779911113\n",
      "Gradient Descent(10/49): loss=0.4499978177991112\n",
      "Gradient Descent(11/49): loss=0.44999781779911113\n",
      "Gradient Descent(12/49): loss=0.44999781779911113\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45081849603199997\n",
      "Gradient Descent(2/49): loss=0.4505775066625568\n",
      "Gradient Descent(3/49): loss=0.4505763258146465\n",
      "Gradient Descent(4/49): loss=0.4505763200284917\n",
      "Gradient Descent(5/49): loss=0.4505763200001396\n",
      "Gradient Descent(6/49): loss=0.4505763200000007\n",
      "Gradient Descent(7/49): loss=0.4505763200000001\n",
      "Gradient Descent(8/49): loss=0.45057632\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.4505763199999998\n",
      "Gradient Descent(11/49): loss=0.4505763200000001\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.4505763199999997\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4504172207628799\n",
      "Gradient Descent(2/49): loss=0.45017426514461806\n",
      "Gradient Descent(3/49): loss=0.45017307466208856\n",
      "Gradient Descent(4/49): loss=0.4501730688287244\n",
      "Gradient Descent(5/49): loss=0.4501730688001408\n",
      "Gradient Descent(6/49): loss=0.4501730688000007\n",
      "Gradient Descent(7/49): loss=0.4501730688000002\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.4501730688000001\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4517092312883201\n",
      "Gradient Descent(2/49): loss=0.4512263236012032\n",
      "Gradient Descent(3/49): loss=0.4512214945243321\n",
      "Gradient Descent(4/49): loss=0.4512214462335633\n",
      "Gradient Descent(5/49): loss=0.45122144575065537\n",
      "Gradient Descent(6/49): loss=0.45122144574582645\n",
      "Gradient Descent(7/49): loss=0.4512214457457783\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.45122144574577766\n",
      "Gradient Descent(10/49): loss=0.45122144574577766\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.45122144574577766\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.4512214457457778\n",
      "Gradient Descent(15/49): loss=0.45122144574577766\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45049783962112\n",
      "Gradient Descent(2/49): loss=0.4500028180173313\n",
      "Gradient Descent(3/49): loss=0.4499978678012932\n",
      "Gradient Descent(4/49): loss=0.4499978182991329\n",
      "Gradient Descent(5/49): loss=0.44999781780411124\n",
      "Gradient Descent(6/49): loss=0.44999781779916104\n",
      "Gradient Descent(7/49): loss=0.44999781779911174\n",
      "Gradient Descent(8/49): loss=0.44999781779911113\n",
      "Gradient Descent(9/49): loss=0.4499978177991111\n",
      "Gradient Descent(10/49): loss=0.4499978177991112\n",
      "Gradient Descent(11/49): loss=0.4499978177991113\n",
      "Gradient Descent(12/49): loss=0.44999781779911113\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4510705568000001\n",
      "Gradient Descent(2/49): loss=0.4505812623679999\n",
      "Gradient Descent(3/49): loss=0.45057636942368\n",
      "Gradient Descent(4/49): loss=0.45057632049423657\n",
      "Gradient Descent(5/49): loss=0.45057632000494235\n",
      "Gradient Descent(6/49): loss=0.4505763200000493\n",
      "Gradient Descent(7/49): loss=0.4505763200000006\n",
      "Gradient Descent(8/49): loss=0.4505763200000002\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.45057632000000014\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.45057632000000014\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45067133811200005\n",
      "Gradient Descent(2/49): loss=0.45017805149312\n",
      "Gradient Descent(3/49): loss=0.45017311862693105\n",
      "Gradient Descent(4/49): loss=0.4501730692982692\n",
      "Gradient Descent(5/49): loss=0.4501730688049826\n",
      "Gradient Descent(6/49): loss=0.45017306880004976\n",
      "Gradient Descent(7/49): loss=0.4501730688000004\n",
      "Gradient Descent(8/49): loss=0.4501730687999999\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730687999999\n",
      "Gradient Descent(12/49): loss=0.4501730688000002\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45204580331267424\n",
      "Gradient Descent(2/49): loss=0.45123537738865854\n",
      "Gradient Descent(3/49): loss=0.45122168119054257\n",
      "Gradient Descent(4/49): loss=0.4512214497247942\n",
      "Gradient Descent(5/49): loss=0.45122144581302304\n",
      "Gradient Descent(6/49): loss=0.45122144574691425\n",
      "Gradient Descent(7/49): loss=0.45122144574579715\n",
      "Gradient Descent(8/49): loss=0.4512214457457781\n",
      "Gradient Descent(9/49): loss=0.4512214457457778\n",
      "Gradient Descent(10/49): loss=0.4512214457457779\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.45122144574577755\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.4512214457457778\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4508428546783061\n",
      "Gradient Descent(2/49): loss=0.4500120989223695\n",
      "Gradient Descent(3/49): loss=0.44999805915009417\n",
      "Gradient Descent(4/49): loss=0.44999782187794257\n",
      "Gradient Descent(5/49): loss=0.4499978178680435\n",
      "Gradient Descent(6/49): loss=0.44999781780027603\n",
      "Gradient Descent(7/49): loss=0.44999781779913084\n",
      "Gradient Descent(8/49): loss=0.4499978177991113\n",
      "Gradient Descent(9/49): loss=0.44999781779911113\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4514115801919999\n",
      "Gradient Descent(2/49): loss=0.45059043589724473\n",
      "Gradient Descent(3/49): loss=0.4505765585586636\n",
      "Gradient Descent(4/49): loss=0.45057632403164155\n",
      "Gradient Descent(5/49): loss=0.45057632006813464\n",
      "Gradient Descent(6/49): loss=0.45057632000115133\n",
      "Gradient Descent(7/49): loss=0.4505763200000193\n",
      "Gradient Descent(8/49): loss=0.4505763200000003\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763199999997\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4510151439372801\n",
      "Gradient Descent(2/49): loss=0.45018729986981987\n",
      "Gradient Descent(3/49): loss=0.45017330930508\n",
      "Gradient Descent(4/49): loss=0.45017307286453573\n",
      "Gradient Descent(5/49): loss=0.45017306886869063\n",
      "Gradient Descent(6/49): loss=0.4501730688011609\n",
      "Gradient Descent(7/49): loss=0.4501730688000197\n",
      "Gradient Descent(8/49): loss=0.4501730688000002\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688000001\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730687999999\n",
      "Gradient Descent(13/49): loss=0.4501730688000001\n",
      "Gradient Descent(14/49): loss=0.4501730687999999\n",
      "Gradient Descent(15/49): loss=0.4501730688000001\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4524701767346858\n",
      "Gradient Descent(2/49): loss=0.4512534132590936\n",
      "Gradient Descent(3/49): loss=0.45122226411411864\n",
      "Gradient Descent(4/49): loss=0.45122146669600716\n",
      "Gradient Descent(5/49): loss=0.45122144628210364\n",
      "Gradient Descent(6/49): loss=0.45122144575950784\n",
      "Gradient Descent(7/49): loss=0.4512214457461292\n",
      "Gradient Descent(8/49): loss=0.4512214457457867\n",
      "Gradient Descent(9/49): loss=0.45122144574577805\n",
      "Gradient Descent(10/49): loss=0.4512214457457779\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.45122144574577766\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.45122144574577766\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45127787366345384\n",
      "Gradient Descent(2/49): loss=0.4500305872292383\n",
      "Gradient Descent(3/49): loss=0.44999865669652245\n",
      "Gradient Descent(4/49): loss=0.44999783927488485\n",
      "Gradient Descent(5/49): loss=0.44999781834889085\n",
      "Gradient Descent(6/49): loss=0.44999781781318543\n",
      "Gradient Descent(7/49): loss=0.44999781779947157\n",
      "Gradient Descent(8/49): loss=0.44999781779912035\n",
      "Gradient Descent(9/49): loss=0.4499978177991113\n",
      "Gradient Descent(10/49): loss=0.4499978177991113\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.44999781779911097\n",
      "Gradient Descent(13/49): loss=0.4499978177991109\n",
      "Gradient Descent(14/49): loss=0.4499978177991113\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.4499978177991111\n",
      "Gradient Descent(18/49): loss=0.4499978177991112\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.4499978177991111\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4518415662080001\n",
      "Gradient Descent(2/49): loss=0.45060871030292476\n",
      "Gradient Descent(3/49): loss=0.45057714919175484\n",
      "Gradient Descent(4/49): loss=0.45057634122730894\n",
      "Gradient Descent(5/49): loss=0.4505763205434193\n",
      "Gradient Descent(6/49): loss=0.4505763200139118\n",
      "Gradient Descent(7/49): loss=0.45057632000035613\n",
      "Gradient Descent(8/49): loss=0.45057632000000913\n",
      "Gradient Descent(9/49): loss=0.4505763200000005\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763199999998\n",
      "Gradient Descent(13/49): loss=0.45057632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.45057631999999975\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4514486382387199\n",
      "Gradient Descent(2/49): loss=0.45020572337763115\n",
      "Gradient Descent(3/49): loss=0.4501739047571872\n",
      "Gradient Descent(4/49): loss=0.4501730902005041\n",
      "Gradient Descent(5/49): loss=0.4501730693478528\n",
      "Gradient Descent(6/49): loss=0.4501730688140251\n",
      "Gradient Descent(7/49): loss=0.4501730688003592\n",
      "Gradient Descent(8/49): loss=0.45017306880000907\n",
      "Gradient Descent(9/49): loss=0.45017306880000024\n",
      "Gradient Descent(10/49): loss=0.4501730688000002\n",
      "Gradient Descent(11/49): loss=0.4501730688000001\n",
      "Gradient Descent(12/49): loss=0.45017306879999985\n",
      "Gradient Descent(13/49): loss=0.4501730688000001\n",
      "Gradient Descent(14/49): loss=0.4501730687999999\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4529823515543552\n",
      "Gradient Descent(2/49): loss=0.45128501444546737\n",
      "Gradient Descent(3/49): loss=0.45122374057583653\n",
      "Gradient Descent(4/49): loss=0.4512215285891428\n",
      "Gradient Descent(5/49): loss=0.4512214487364232\n",
      "Gradient Descent(6/49): loss=0.4512214458537402\n",
      "Gradient Descent(7/49): loss=0.4512214457496753\n",
      "Gradient Descent(8/49): loss=0.4512214457459186\n",
      "Gradient Descent(9/49): loss=0.4512214457457828\n",
      "Gradient Descent(10/49): loss=0.45122144574577805\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.45122144574577766\n",
      "Gradient Descent(14/49): loss=0.4512214457457779\n",
      "Gradient Descent(15/49): loss=0.45122144574577766\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45180289657656314\n",
      "Gradient Descent(2/49): loss=0.45006298114297716\n",
      "Gradient Descent(3/49): loss=0.4500001701958247\n",
      "Gradient Descent(4/49): loss=0.44999790272063234\n",
      "Gradient Descent(5/49): loss=0.4499978208647779\n",
      "Gradient Descent(6/49): loss=0.44999781790978166\n",
      "Gradient Descent(7/49): loss=0.44999781780310627\n",
      "Gradient Descent(8/49): loss=0.44999781779925524\n",
      "Gradient Descent(9/49): loss=0.44999781779911635\n",
      "Gradient Descent(10/49): loss=0.4499978177991115\n",
      "Gradient Descent(11/49): loss=0.4499978177991109\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.4499978177991113\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911135\n",
      "Gradient Descent(21/49): loss=0.44999781779911135\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4523605148480002\n",
      "Gradient Descent(2/49): loss=0.45064072943401295\n",
      "Gradient Descent(3/49): loss=0.4505786451805678\n",
      "Gradient Descent(4/49): loss=0.45057640393901854\n",
      "Gradient Descent(5/49): loss=0.4505763230301985\n",
      "Gradient Descent(6/49): loss=0.45057632010939025\n",
      "Gradient Descent(7/49): loss=0.4505763200039491\n",
      "Gradient Descent(8/49): loss=0.45057632000014264\n",
      "Gradient Descent(9/49): loss=0.450576320000005\n",
      "Gradient Descent(10/49): loss=0.45057632000000014\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.45057632\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.45057632000000014\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.4505763199999998\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4519718210163203\n",
      "Gradient Descent(2/49): loss=0.45023800375500905\n",
      "Gradient Descent(3/49): loss=0.45017541295187585\n",
      "Gradient Descent(4/49): loss=0.45017315342388264\n",
      "Gradient Descent(5/49): loss=0.45017307185492206\n",
      "Gradient Descent(6/49): loss=0.45017306891028275\n",
      "Gradient Descent(7/49): loss=0.45017306880398117\n",
      "Gradient Descent(8/49): loss=0.4501730688001435\n",
      "Gradient Descent(9/49): loss=0.4501730688000052\n",
      "Gradient Descent(10/49): loss=0.45017306880000024\n",
      "Gradient Descent(11/49): loss=0.4501730687999998\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688000001\n",
      "Gradient Descent(14/49): loss=0.4501730688000002\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730687999998\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4535823277716822\n",
      "Gradient Descent(2/49): loss=0.4513357124358315\n",
      "Gradient Descent(3/49): loss=0.45122697625357633\n",
      "Gradient Descent(4/49): loss=0.45122171342235506\n",
      "Gradient Descent(5/49): loss=0.45122145870132413\n",
      "Gradient Descent(6/49): loss=0.4512214463728263\n",
      "Gradient Descent(7/49): loss=0.4512214457761269\n",
      "Gradient Descent(8/49): loss=0.4512214457472467\n",
      "Gradient Descent(9/49): loss=0.4512214457458487\n",
      "Gradient Descent(10/49): loss=0.4512214457457812\n",
      "Gradient Descent(11/49): loss=0.45122144574577794\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.45122144574577766\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.45122144574577755\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45241792341763426\n",
      "Gradient Descent(2/49): loss=0.4501149509110476\n",
      "Gradient Descent(3/49): loss=0.4500034870417288\n",
      "Gradient Descent(4/49): loss=0.4499980921904538\n",
      "Gradient Descent(5/49): loss=0.44999783107965186\n",
      "Gradient Descent(6/49): loss=0.4499978184418894\n",
      "Gradient Descent(7/49): loss=0.44999781783022186\n",
      "Gradient Descent(8/49): loss=0.4499978178006169\n",
      "Gradient Descent(9/49): loss=0.4499978177991839\n",
      "Gradient Descent(10/49): loss=0.4499978177991145\n",
      "Gradient Descent(11/49): loss=0.4499978177991112\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.4499978177991113\n",
      "Gradient Descent(17/49): loss=0.4499978177991113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911097\n",
      "Gradient Descent(23/49): loss=0.44999781779911135\n",
      "Gradient Descent(24/49): loss=0.44999781779911135\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45296842611200006\n",
      "Gradient Descent(2/49): loss=0.4506920979358207\n",
      "Gradient Descent(3/49): loss=0.45058192365209365\n",
      "Gradient Descent(4/49): loss=0.4505765912167613\n",
      "Gradient Descent(5/49): loss=0.450576333126891\n",
      "Gradient Descent(6/49): loss=0.4505763206353414\n",
      "Gradient Descent(7/49): loss=0.45057632003075054\n",
      "Gradient Descent(8/49): loss=0.45057632000148823\n",
      "Gradient Descent(9/49): loss=0.45057632000007186\n",
      "Gradient Descent(10/49): loss=0.45057632000000347\n",
      "Gradient Descent(11/49): loss=0.4505763200000002\n",
      "Gradient Descent(12/49): loss=0.4505763199999998\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.45057631999999975\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45258469227008\n",
      "Gradient Descent(2/49): loss=0.45028979137595193\n",
      "Gradient Descent(3/49): loss=0.4501787181726761\n",
      "Gradient Descent(4/49): loss=0.45017334222963734\n",
      "Gradient Descent(5/49): loss=0.4501730820339946\n",
      "Gradient Descent(6/49): loss=0.4501730694405253\n",
      "Gradient Descent(7/49): loss=0.4501730688310015\n",
      "Gradient Descent(8/49): loss=0.45017306880150043\n",
      "Gradient Descent(9/49): loss=0.45017306880007263\n",
      "Gradient Descent(10/49): loss=0.4501730688000036\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.45017306880000024\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730687999999\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688000001\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45427010538666673\n",
      "Gradient Descent(2/49): loss=0.4514119869733334\n",
      "Gradient Descent(3/49): loss=0.45123335457249997\n",
      "Gradient Descent(4/49): loss=0.451222190047448\n",
      "Gradient Descent(5/49): loss=0.45122149226463215\n",
      "Gradient Descent(6/49): loss=0.45122144865320624\n",
      "Gradient Descent(7/49): loss=0.451221445927492\n",
      "Gradient Descent(8/49): loss=0.4512214457571348\n",
      "Gradient Descent(9/49): loss=0.45122144574648765\n",
      "Gradient Descent(10/49): loss=0.4512214457458222\n",
      "Gradient Descent(11/49): loss=0.4512214457457806\n",
      "Gradient Descent(12/49): loss=0.45122144574577794\n",
      "Gradient Descent(13/49): loss=0.45122144574577794\n",
      "Gradient Descent(14/49): loss=0.4512214457457779\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457778\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.45122144574577794\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457779\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45312295418666676\n",
      "Gradient Descent(2/49): loss=0.4501931388233335\n",
      "Gradient Descent(3/49): loss=0.450010025363125\n",
      "Gradient Descent(4/49): loss=0.449998580771862\n",
      "Gradient Descent(5/49): loss=0.4499978654849079\n",
      "Gradient Descent(6/49): loss=0.44999782077947353\n",
      "Gradient Descent(7/49): loss=0.44999781798538363\n",
      "Gradient Descent(8/49): loss=0.44999781781075326\n",
      "Gradient Descent(9/49): loss=0.44999781779983883\n",
      "Gradient Descent(10/49): loss=0.4499978177991567\n",
      "Gradient Descent(11/49): loss=0.44999781779911385\n",
      "Gradient Descent(12/49): loss=0.4499978177991113\n",
      "Gradient Descent(13/49): loss=0.4499978177991113\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.4499978177991113\n",
      "Gradient Descent(18/49): loss=0.4499978177991109\n",
      "Gradient Descent(19/49): loss=0.44999781779911097\n",
      "Gradient Descent(20/49): loss=0.4499978177991111\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991112\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.44999781779911135\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45366530000000005\n",
      "Gradient Descent(2/49): loss=0.4507693812499999\n",
      "Gradient Descent(3/49): loss=0.45058838632812515\n",
      "Gradient Descent(4/49): loss=0.45057707414550774\n",
      "Gradient Descent(5/49): loss=0.45057636713409416\n",
      "Gradient Descent(6/49): loss=0.45057632294588096\n",
      "Gradient Descent(7/49): loss=0.4505763201841175\n",
      "Gradient Descent(8/49): loss=0.45057632001150727\n",
      "Gradient Descent(9/49): loss=0.45057632000071934\n",
      "Gradient Descent(10/49): loss=0.4505763200000451\n",
      "Gradient Descent(11/49): loss=0.45057632000000264\n",
      "Gradient Descent(12/49): loss=0.45057632000000036\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763199999998\n",
      "Gradient Descent(18/49): loss=0.4505763199999998\n",
      "Gradient Descent(19/49): loss=0.45057631999999975\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.45057632000000014\n",
      "Gradient Descent(22/49): loss=0.45057632\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763200000002\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4532872520000002\n",
      "Gradient Descent(2/49): loss=0.45036770525\n",
      "Gradient Descent(3/49): loss=0.45018523357812484\n",
      "Gradient Descent(4/49): loss=0.45017382909863296\n",
      "Gradient Descent(5/49): loss=0.4501731163186645\n",
      "Gradient Descent(6/49): loss=0.45017307176991656\n",
      "Gradient Descent(7/49): loss=0.4501730689856199\n",
      "Gradient Descent(8/49): loss=0.45017306881160135\n",
      "Gradient Descent(9/49): loss=0.45017306880072494\n",
      "Gradient Descent(10/49): loss=0.45017306880004526\n",
      "Gradient Descent(11/49): loss=0.4501730688000029\n",
      "Gradient Descent(12/49): loss=0.4501730688000002\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688000001\n",
      "Gradient Descent(15/49): loss=0.45017306879999985\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688000001\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4550456843993088\n",
      "Gradient Descent(2/49): loss=0.4515212660562144\n",
      "Gradient Descent(3/49): loss=0.4512449516581159\n",
      "Gradient Descent(4/49): loss=0.451223288609305\n",
      "Gradient Descent(5/49): loss=0.4512215902262784\n",
      "Gradient Descent(6/49): loss=0.451221457073049\n",
      "Gradient Descent(7/49): loss=0.45122144663383584\n",
      "Gradient Descent(8/49): loss=0.45122144581540147\n",
      "Gradient Descent(9/49): loss=0.4512214457512363\n",
      "Gradient Descent(10/49): loss=0.45122144574620565\n",
      "Gradient Descent(11/49): loss=0.45122144574581147\n",
      "Gradient Descent(12/49): loss=0.4512214457457804\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.45122144574577794\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.45122144574577766\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.45122144574577766\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.45122144574577766\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45391798888366097\n",
      "Gradient Descent(2/49): loss=0.45030515921213965\n",
      "Gradient Descent(3/49): loss=0.4500219133658926\n",
      "Gradient Descent(4/49): loss=0.4499997068915468\n",
      "Gradient Descent(5/49): loss=0.44999796590395824\n",
      "Gradient Descent(6/49): loss=0.44999782941053124\n",
      "Gradient Descent(7/49): loss=0.44999781870944633\n",
      "Gradient Descent(8/49): loss=0.4499978178704815\n",
      "Gradient Descent(9/49): loss=0.44999781780470643\n",
      "Gradient Descent(10/49): loss=0.44999781779955\n",
      "Gradient Descent(11/49): loss=0.4499978177991454\n",
      "Gradient Descent(12/49): loss=0.4499978177991137\n",
      "Gradient Descent(13/49): loss=0.44999781779911135\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911097\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.44999781779911135\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991113\n",
      "Gradient Descent(26/49): loss=0.44999781779911135\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45445113651199986\n",
      "Gradient Descent(2/49): loss=0.45088010561454084\n",
      "Gradient Descent(3/49): loss=0.4506001367921802\n",
      "Gradient Descent(4/49): loss=0.45057818723650694\n",
      "Gradient Descent(5/49): loss=0.45057646639134213\n",
      "Gradient Descent(6/49): loss=0.45057633147708126\n",
      "Gradient Descent(7/49): loss=0.4505763208998032\n",
      "Gradient Descent(8/49): loss=0.4505763200705445\n",
      "Gradient Descent(9/49): loss=0.45057632000553055\n",
      "Gradient Descent(10/49): loss=0.4505763200004337\n",
      "Gradient Descent(11/49): loss=0.45057632000003384\n",
      "Gradient Descent(12/49): loss=0.45057632000000253\n",
      "Gradient Descent(13/49): loss=0.4505763200000002\n",
      "Gradient Descent(14/49): loss=0.45057632000000014\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.45057632\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4540795002060799\n",
      "Gradient Descent(2/49): loss=0.4504793330222366\n",
      "Gradient Descent(3/49): loss=0.4501970799150235\n",
      "Gradient Descent(4/49): loss=0.45017495127141777\n",
      "Gradient Descent(5/49): loss=0.45017321638575925\n",
      "Gradient Descent(6/49): loss=0.4501730803707237\n",
      "Gradient Descent(7/49): loss=0.4501730697071445\n",
      "Gradient Descent(8/49): loss=0.45017306887112024\n",
      "Gradient Descent(9/49): loss=0.4501730688055759\n",
      "Gradient Descent(10/49): loss=0.450173068800437\n",
      "Gradient Descent(11/49): loss=0.4501730688000342\n",
      "Gradient Descent(12/49): loss=0.45017306880000263\n",
      "Gradient Descent(13/49): loss=0.4501730688000003\n",
      "Gradient Descent(14/49): loss=0.4501730687999999\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.45017306879999985\n",
      "Gradient Descent(22/49): loss=0.45017306879999985\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000001\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4559090648096085\n",
      "Gradient Descent(2/49): loss=0.451671925937812\n",
      "Gradient Descent(3/49): loss=0.4512647368922323\n",
      "Gradient Descent(4/49): loss=0.45122560602495215\n",
      "Gradient Descent(5/49): loss=0.4512218455486066\n",
      "Gradient Descent(6/49): loss=0.45122148416682956\n",
      "Gradient Descent(7/49): loss=0.45122144943804093\n",
      "Gradient Descent(8/49): loss=0.4512214461006043\n",
      "Gradient Descent(9/49): loss=0.4512214457798767\n",
      "Gradient Descent(10/49): loss=0.45122144574905476\n",
      "Gradient Descent(11/49): loss=0.4512214457460927\n",
      "Gradient Descent(12/49): loss=0.45122144574580797\n",
      "Gradient Descent(13/49): loss=0.4512214457457808\n",
      "Gradient Descent(14/49): loss=0.45122144574577805\n",
      "Gradient Descent(15/49): loss=0.45122144574577766\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.45122144574577766\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457775\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.45122144574577794\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45480302750861645\n",
      "Gradient Descent(2/49): loss=0.4504595984521947\n",
      "Gradient Descent(3/49): loss=0.4500421949198726\n",
      "Gradient Descent(4/49): loss=0.45000208244041623\n",
      "Gradient Descent(5/49): loss=0.44999822763114067\n",
      "Gradient Descent(6/49): loss=0.4499978571839692\n",
      "Gradient Descent(7/49): loss=0.4499978215839959\n",
      "Gradient Descent(8/49): loss=0.4499978181628383\n",
      "Gradient Descent(9/49): loss=0.4499978178340653\n",
      "Gradient Descent(10/49): loss=0.4499978178024703\n",
      "Gradient Descent(11/49): loss=0.44999781779943404\n",
      "Gradient Descent(12/49): loss=0.4499978177991421\n",
      "Gradient Descent(13/49): loss=0.44999781779911413\n",
      "Gradient Descent(14/49): loss=0.4499978177991113\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.4499978177991112\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.44999781779911097\n",
      "Gradient Descent(22/49): loss=0.4499978177991113\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.4499978177991108\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.44999781779911097\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45532593564799995\n",
      "Gradient Descent(2/49): loss=0.45103275806377285\n",
      "Gradient Descent(3/49): loss=0.45062018369792867\n",
      "Gradient Descent(4/49): loss=0.4505805353013711\n",
      "Gradient Descent(5/49): loss=0.4505767250904617\n",
      "Gradient Descent(6/49): loss=0.4505763589291934\n",
      "Gradient Descent(7/49): loss=0.4505763237410955\n",
      "Gradient Descent(8/49): loss=0.4505763203595192\n",
      "Gradient Descent(9/49): loss=0.4505763200345498\n",
      "Gradient Descent(10/49): loss=0.4505763200033202\n",
      "Gradient Descent(11/49): loss=0.45057632000031894\n",
      "Gradient Descent(12/49): loss=0.4505763200000306\n",
      "Gradient Descent(13/49): loss=0.450576320000003\n",
      "Gradient Descent(14/49): loss=0.45057632000000036\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.45057632000000014\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763200000002\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.4505763199999998\n",
      "Gradient Descent(26/49): loss=0.45057631999999975\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763200000002\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4549614368883201\n",
      "Gradient Descent(2/49): loss=0.45063323097328756\n",
      "Gradient Descent(3/49): loss=0.45021729038485286\n",
      "Gradient Descent(4/49): loss=0.4501773184943043\n",
      "Gradient Descent(5/49): loss=0.4501734771956226\n",
      "Gradient Descent(6/49): loss=0.45017310804681937\n",
      "Gradient Descent(7/49): loss=0.4501730725716196\n",
      "Gradient Descent(8/49): loss=0.45017306916245264\n",
      "Gradient Descent(9/49): loss=0.4501730688348315\n",
      "Gradient Descent(10/49): loss=0.4501730688033474\n",
      "Gradient Descent(11/49): loss=0.45017306880032165\n",
      "Gradient Descent(12/49): loss=0.45017306880003083\n",
      "Gradient Descent(13/49): loss=0.450173068800003\n",
      "Gradient Descent(14/49): loss=0.4501730688000003\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000001\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730687999999\n",
      "Gradient Descent(28/49): loss=0.4501730687999999\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4568602466175657\n",
      "Gradient Descent(2/49): loss=0.4518732911265564\n",
      "Gradient Descent(3/49): loss=0.4512967990717959\n",
      "Gradient Descent(4/49): loss=0.45123015659026555\n",
      "Gradient Descent(5/49): loss=0.4512224527194004\n",
      "Gradient Descent(6/49): loss=0.45122156215192866\n",
      "Gradient Descent(7/49): loss=0.4512214592023287\n",
      "Gradient Descent(8/49): loss=0.45122144730135494\n",
      "Gradient Descent(9/49): loss=0.4512214459256023\n",
      "Gradient Descent(10/49): loss=0.45122144576656564\n",
      "Gradient Descent(11/49): loss=0.45122144574818085\n",
      "Gradient Descent(12/49): loss=0.4512214457460555\n",
      "Gradient Descent(13/49): loss=0.45122144574580986\n",
      "Gradient Descent(14/49): loss=0.45122144574578144\n",
      "Gradient Descent(15/49): loss=0.45122144574577827\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.45122144574577794\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.45122144574577794\n",
      "Gradient Descent(26/49): loss=0.45122144574577766\n",
      "Gradient Descent(27/49): loss=0.4512214457457779\n",
      "Gradient Descent(28/49): loss=0.45122144574577794\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4557780700615338\n",
      "Gradient Descent(2/49): loss=0.45066601496064723\n",
      "Gradient Descent(3/49): loss=0.45007506139098463\n",
      "Gradient Descent(4/49): loss=0.4500067471583319\n",
      "Gradient Descent(5/49): loss=0.44999885003303697\n",
      "Gradient Descent(6/49): loss=0.44999793712535296\n",
      "Gradient Descent(7/49): loss=0.4499978315932245\n",
      "Gradient Descent(8/49): loss=0.4499978193937105\n",
      "Gradient Descent(9/49): loss=0.4499978179834469\n",
      "Gradient Descent(10/49): loss=0.4499978178204204\n",
      "Gradient Descent(11/49): loss=0.4499978178015745\n",
      "Gradient Descent(12/49): loss=0.4499978177993959\n",
      "Gradient Descent(13/49): loss=0.44999781779914394\n",
      "Gradient Descent(14/49): loss=0.44999781779911485\n",
      "Gradient Descent(15/49): loss=0.4499978177991117\n",
      "Gradient Descent(16/49): loss=0.4499978177991113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.4499978177991112\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911097\n",
      "Gradient Descent(21/49): loss=0.4499978177991113\n",
      "Gradient Descent(22/49): loss=0.4499978177991113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.4499978177991109\n",
      "Gradient Descent(25/49): loss=0.4499978177991113\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991109\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45628969740800024\n",
      "Gradient Descent(2/49): loss=0.4512367864283647\n",
      "Gradient Descent(3/49): loss=0.45065266991911906\n",
      "Gradient Descent(4/49): loss=0.45058514605065025\n",
      "Gradient Descent(5/49): loss=0.4505773402914551\n",
      "Gradient Descent(6/49): loss=0.45057643794569235\n",
      "Gradient Descent(7/49): loss=0.450576333634522\n",
      "Gradient Descent(8/49): loss=0.45057632157615074\n",
      "Gradient Descent(9/49): loss=0.45057632018220295\n",
      "Gradient Descent(10/49): loss=0.45057632002106285\n",
      "Gradient Descent(11/49): loss=0.45057632000243464\n",
      "Gradient Descent(12/49): loss=0.4505763200002816\n",
      "Gradient Descent(13/49): loss=0.45057632000003256\n",
      "Gradient Descent(14/49): loss=0.45057632000000386\n",
      "Gradient Descent(15/49): loss=0.45057632000000053\n",
      "Gradient Descent(16/49): loss=0.45057632000000014\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.45057632000000014\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999998\n",
      "Gradient Descent(22/49): loss=0.4505763199999998\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999998\n",
      "Gradient Descent(27/49): loss=0.45057632000000014\n",
      "Gradient Descent(28/49): loss=0.45057632\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45593306204671985\n",
      "Gradient Descent(2/49): loss=0.45083892401932085\n",
      "Gradient Descent(3/49): loss=0.4502500416633535\n",
      "Gradient Descent(4/49): loss=0.45018196686300366\n",
      "Gradient Descent(5/49): loss=0.45017409741608333\n",
      "Gradient Descent(6/49): loss=0.4501731877080191\n",
      "Gradient Descent(7/49): loss=0.45017308254576693\n",
      "Gradient Descent(8/49): loss=0.4501730703890107\n",
      "Gradient Descent(9/49): loss=0.45017306898368964\n",
      "Gradient Descent(10/49): loss=0.45017306882123465\n",
      "Gradient Descent(11/49): loss=0.450173068802455\n",
      "Gradient Descent(12/49): loss=0.45017306880028385\n",
      "Gradient Descent(13/49): loss=0.4501730688000328\n",
      "Gradient Descent(14/49): loss=0.4501730688000039\n",
      "Gradient Descent(15/49): loss=0.45017306880000046\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.45017306879999985\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999998\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4578992298231807\n",
      "Gradient Descent(2/49): loss=0.45213563438597415\n",
      "Gradient Descent(3/49): loss=0.45134659817062067\n",
      "Gradient Descent(4/49): loss=0.45123857911273874\n",
      "Gradient Descent(5/49): loss=0.45122379130371476\n",
      "Gradient Descent(6/49): loss=0.45122176685265925\n",
      "Gradient Descent(7/49): loss=0.45122148970530995\n",
      "Gradient Descent(8/49): loss=0.45122145176383777\n",
      "Gradient Descent(9/49): loss=0.4512214465696502\n",
      "Gradient Descent(10/49): loss=0.4512214458585659\n",
      "Gradient Descent(11/49): loss=0.45122144576121864\n",
      "Gradient Descent(12/49): loss=0.4512214457478915\n",
      "Gradient Descent(13/49): loss=0.4512214457460673\n",
      "Gradient Descent(14/49): loss=0.4512214457458174\n",
      "Gradient Descent(15/49): loss=0.4512214457457832\n",
      "Gradient Descent(16/49): loss=0.4512214457457787\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.45122144574577805\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45684311654241283\n",
      "Gradient Descent(2/49): loss=0.450934939197069\n",
      "Gradient Descent(3/49): loss=0.45012610971849154\n",
      "Gradient Descent(4/49): loss=0.4500153809628743\n",
      "Gradient Descent(5/49): loss=0.45000022219623026\n",
      "Gradient Descent(6/49): loss=0.4499981469610768\n",
      "Gradient Descent(7/49): loss=0.44999786286138427\n",
      "Gradient Descent(8/49): loss=0.44999782396813637\n",
      "Gradient Descent(9/49): loss=0.4499978186436507\n",
      "Gradient Descent(10/49): loss=0.44999781791472837\n",
      "Gradient Descent(11/49): loss=0.44999781781493914\n",
      "Gradient Descent(12/49): loss=0.44999781780127807\n",
      "Gradient Descent(13/49): loss=0.44999781779940784\n",
      "Gradient Descent(14/49): loss=0.4499978177991517\n",
      "Gradient Descent(15/49): loss=0.44999781779911663\n",
      "Gradient Descent(16/49): loss=0.44999781779911185\n",
      "Gradient Descent(17/49): loss=0.44999781779911135\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911135\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991109\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.4499978177991113\n",
      "Gradient Descent(33/49): loss=0.4499978177991112\n",
      "Gradient Descent(34/49): loss=0.4499978177991112\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45734242179199996\n",
      "Gradient Descent(2/49): loss=0.4515025993353247\n",
      "Gradient Descent(3/49): loss=0.45070312764100606\n",
      "Gradient Descent(4/49): loss=0.4505936799660536\n",
      "Gradient Descent(5/49): loss=0.4505786965793526\n",
      "Gradient Descent(6/49): loss=0.45057664535371345\n",
      "Gradient Descent(7/49): loss=0.4505763645409233\n",
      "Gradient Descent(8/49): loss=0.4505763260976524\n",
      "Gradient Descent(9/49): loss=0.45057632083476845\n",
      "Gradient Descent(10/49): loss=0.45057632011428\n",
      "Gradient Descent(11/49): loss=0.4505763200156449\n",
      "Gradient Descent(12/49): loss=0.4505763200021419\n",
      "Gradient Descent(13/49): loss=0.4505763200002933\n",
      "Gradient Descent(14/49): loss=0.4505763200000401\n",
      "Gradient Descent(15/49): loss=0.4505763200000056\n",
      "Gradient Descent(16/49): loss=0.4505763200000007\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.45057632\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.45057632\n",
      "Gradient Descent(23/49): loss=0.45057631999999975\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.45057632\n",
      "Gradient Descent(26/49): loss=0.4505763199999998\n",
      "Gradient Descent(27/49): loss=0.4505763200000002\n",
      "Gradient Descent(28/49): loss=0.4505763199999998\n",
      "Gradient Descent(29/49): loss=0.4505763200000001\n",
      "Gradient Descent(30/49): loss=0.45057631999999975\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.4505763199999997\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763200000001\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45699437568128\n",
      "Gradient Descent(2/49): loss=0.4511069057120472\n",
      "Gradient Descent(3/49): loss=0.45030091107325937\n",
      "Gradient Descent(4/49): loss=0.4501905704072093\n",
      "Gradient Descent(5/49): loss=0.45017546477002707\n",
      "Gradient Descent(6/49): loss=0.4501733968082968\n",
      "Gradient Descent(7/49): loss=0.4501731137043357\n",
      "Gradient Descent(8/49): loss=0.45017307494740366\n",
      "Gradient Descent(9/49): loss=0.4501730696415795\n",
      "Gradient Descent(10/49): loss=0.4501730689152122\n",
      "Gradient Descent(11/49): loss=0.4501730688157724\n",
      "Gradient Descent(12/49): loss=0.45017306880215935\n",
      "Gradient Descent(13/49): loss=0.45017306880029573\n",
      "Gradient Descent(14/49): loss=0.45017306880004027\n",
      "Gradient Descent(15/49): loss=0.45017306880000557\n",
      "Gradient Descent(16/49): loss=0.4501730688000007\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000001\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.45017306879999985\n",
      "Gradient Descent(30/49): loss=0.4501730687999999\n",
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4590260144264536\n",
      "Gradient Descent(2/49): loss=0.45247017673468604\n",
      "Gradient Descent(3/49): loss=0.4514212427040033\n",
      "Gradient Descent(4/49): loss=0.45125341325909396\n",
      "Gradient Descent(5/49): loss=0.45122656054790855\n",
      "Gradient Descent(6/49): loss=0.45122226411411887\n",
      "Gradient Descent(7/49): loss=0.4512215766847125\n",
      "Gradient Descent(8/49): loss=0.45122146669600716\n",
      "Gradient Descent(9/49): loss=0.45122144909781453\n",
      "Gradient Descent(10/49): loss=0.4512214462821036\n",
      "Gradient Descent(11/49): loss=0.45122144583159\n",
      "Gradient Descent(12/49): loss=0.4512214457595077\n",
      "Gradient Descent(13/49): loss=0.45122144574797457\n",
      "Gradient Descent(14/49): loss=0.4512214457461292\n",
      "Gradient Descent(15/49): loss=0.45122144574583384\n",
      "Gradient Descent(16/49): loss=0.4512214457457867\n",
      "Gradient Descent(17/49): loss=0.4512214457457791\n",
      "Gradient Descent(18/49): loss=0.4512214457457782\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.45122144574577766\n",
      "Gradient Descent(23/49): loss=0.45122144574577766\n",
      "Gradient Descent(24/49): loss=0.4512214457457775\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.45122144574577766\n",
      "Gradient Descent(31/49): loss=0.45122144574577766\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457778\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45799816695125334\n",
      "Gradient Descent(2/49): loss=0.45127787366345395\n",
      "Gradient Descent(3/49): loss=0.4502026267374059\n",
      "Gradient Descent(4/49): loss=0.4500305872292381\n",
      "Gradient Descent(5/49): loss=0.4500030609079313\n",
      "Gradient Descent(6/49): loss=0.4499986566965224\n",
      "Gradient Descent(7/49): loss=0.4499979520226967\n",
      "Gradient Descent(8/49): loss=0.44999783927488485\n",
      "Gradient Descent(9/49): loss=0.4499978212352349\n",
      "Gradient Descent(10/49): loss=0.44999781834889085\n",
      "Gradient Descent(11/49): loss=0.44999781788707577\n",
      "Gradient Descent(12/49): loss=0.44999781781318543\n",
      "Gradient Descent(13/49): loss=0.44999781780136316\n",
      "Gradient Descent(14/49): loss=0.4499978177994714\n",
      "Gradient Descent(15/49): loss=0.449997817799169\n",
      "Gradient Descent(16/49): loss=0.44999781779912046\n",
      "Gradient Descent(17/49): loss=0.44999781779911274\n",
      "Gradient Descent(18/49): loss=0.4499978177991112\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.4499978177991109\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991112\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45848410879999996\n",
      "Gradient Descent(2/49): loss=0.4518415662080001\n",
      "Gradient Descent(3/49): loss=0.45077875939328016\n",
      "Gradient Descent(4/49): loss=0.4506087103029249\n",
      "Gradient Descent(5/49): loss=0.45058150244846806\n",
      "Gradient Descent(6/49): loss=0.45057714919175496\n",
      "Gradient Descent(7/49): loss=0.4505764526706808\n",
      "Gradient Descent(8/49): loss=0.450576341227309\n",
      "Gradient Descent(9/49): loss=0.4505763233963693\n",
      "Gradient Descent(10/49): loss=0.4505763205434193\n",
      "Gradient Descent(11/49): loss=0.45057632008694704\n",
      "Gradient Descent(12/49): loss=0.45057632001391146\n",
      "Gradient Descent(13/49): loss=0.4505763200022259\n",
      "Gradient Descent(14/49): loss=0.45057632000035625\n",
      "Gradient Descent(15/49): loss=0.4505763200000572\n",
      "Gradient Descent(16/49): loss=0.4505763200000093\n",
      "Gradient Descent(17/49): loss=0.45057632000000136\n",
      "Gradient Descent(18/49): loss=0.4505763200000002\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.45057632000000014\n",
      "Gradient Descent(21/49): loss=0.4505763199999998\n",
      "Gradient Descent(22/49): loss=0.45057632\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.45057632\n",
      "Gradient Descent(27/49): loss=0.4505763199999998\n",
      "Gradient Descent(28/49): loss=0.4505763199999998\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.45057632000000014\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763200000002\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.458145377792\n",
      "Gradient Descent(2/49): loss=0.45144863823871967\n",
      "Gradient Descent(3/49): loss=0.4503771599101952\n",
      "Gradient Descent(4/49): loss=0.4502057233776314\n",
      "Gradient Descent(5/49): loss=0.4501782935324209\n",
      "Gradient Descent(6/49): loss=0.4501739047571874\n",
      "Gradient Descent(7/49): loss=0.4501732025531499\n",
      "Gradient Descent(8/49): loss=0.4501730902005041\n",
      "Gradient Descent(9/49): loss=0.4501730722240809\n",
      "Gradient Descent(10/49): loss=0.45017306934785284\n",
      "Gradient Descent(11/49): loss=0.45017306888765646\n",
      "Gradient Descent(12/49): loss=0.45017306881402497\n",
      "Gradient Descent(13/49): loss=0.45017306880224395\n",
      "Gradient Descent(14/49): loss=0.4501730688003592\n",
      "Gradient Descent(15/49): loss=0.45017306880005736\n",
      "Gradient Descent(16/49): loss=0.45017306880000924\n",
      "Gradient Descent(17/49): loss=0.45017306880000146\n",
      "Gradient Descent(18/49): loss=0.45017306880000024\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.45017306880000024\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730687999999\n",
      "Gradient Descent(29/49): loss=0.4501730687999999\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730688000001\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46024060042738346\n",
      "Gradient Descent(2/49): loss=0.45288908744640655\n",
      "Gradient Descent(3/49): loss=0.451529792696224\n",
      "Gradient Descent(4/49): loss=0.4512784590969155\n",
      "Gradient Descent(5/49): loss=0.4512319875144032\n",
      "Gradient Descent(6/49): loss=0.45122339491879654\n",
      "Gradient Descent(7/49): loss=0.45122180614786905\n",
      "Gradient Descent(8/49): loss=0.4512215123841245\n",
      "Gradient Descent(9/49): loss=0.4512214580672083\n",
      "Gradient Descent(10/49): loss=0.45122144802401015\n",
      "Gradient Descent(11/49): loss=0.4512214461670229\n",
      "Gradient Descent(12/49): loss=0.451221445823666\n",
      "Gradient Descent(13/49): loss=0.4512214457601793\n",
      "Gradient Descent(14/49): loss=0.4512214457484406\n",
      "Gradient Descent(15/49): loss=0.45122144574627027\n",
      "Gradient Descent(16/49): loss=0.4512214457458686\n",
      "Gradient Descent(17/49): loss=0.45122144574579465\n",
      "Gradient Descent(18/49): loss=0.45122144574578077\n",
      "Gradient Descent(19/49): loss=0.4512214457457783\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.45122144574577766\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.45122144574577755\n",
      "Gradient Descent(32/49): loss=0.45122144574577794\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.45122144574577766\n",
      "Gradient Descent(36/49): loss=0.45122144574577794\n",
      "Gradient Descent(37/49): loss=0.45122144574577805\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4592432212880555\n",
      "Gradient Descent(2/49): loss=0.45170729290421646\n",
      "Gradient Descent(3/49): loss=0.450313899746045\n",
      "Gradient Descent(4/49): loss=0.4500562613510994\n",
      "Gradient Descent(5/49): loss=0.45000862401187364\n",
      "Gradient Descent(6/49): loss=0.4499998158678509\n",
      "Gradient Descent(7/49): loss=0.44999818724202084\n",
      "Gradient Descent(8/49): loss=0.4499978861091052\n",
      "Gradient Descent(9/49): loss=0.44999783042962893\n",
      "Gradient Descent(10/49): loss=0.44999782013449385\n",
      "Gradient Descent(11/49): loss=0.44999781823092333\n",
      "Gradient Descent(12/49): loss=0.4499978178789535\n",
      "Gradient Descent(13/49): loss=0.449997817813874\n",
      "Gradient Descent(14/49): loss=0.44999781780184067\n",
      "Gradient Descent(15/49): loss=0.44999781779961573\n",
      "Gradient Descent(16/49): loss=0.4499978177992044\n",
      "Gradient Descent(17/49): loss=0.44999781779912834\n",
      "Gradient Descent(18/49): loss=0.4499978177991144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=0.44999781779911174\n",
      "Gradient Descent(20/49): loss=0.4499978177991111\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991109\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.4499978177991113\n",
      "Gradient Descent(36/49): loss=0.44999781779911097\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.44999781779911135\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45971475843199994\n",
      "Gradient Descent(2/49): loss=0.4522660172660765\n",
      "Gradient Descent(3/49): loss=0.4508887450244976\n",
      "Gradient Descent(4/49): loss=0.45063408738702937\n",
      "Gradient Descent(5/49): loss=0.45058700118986167\n",
      "Gradient Descent(6/49): loss=0.4505782949520055\n",
      "Gradient Descent(7/49): loss=0.4505766851686258\n",
      "Gradient Descent(8/49): loss=0.4505763875196789\n",
      "Gradient Descent(9/49): loss=0.45057633248438855\n",
      "Gradient Descent(10/49): loss=0.4505763223083634\n",
      "Gradient Descent(11/49): loss=0.45057632042681645\n",
      "Gradient Descent(12/49): loss=0.45057632007891835\n",
      "Gradient Descent(13/49): loss=0.45057632001459186\n",
      "Gradient Descent(14/49): loss=0.45057632000269787\n",
      "Gradient Descent(15/49): loss=0.45057632000049863\n",
      "Gradient Descent(16/49): loss=0.45057632000009235\n",
      "Gradient Descent(17/49): loss=0.4505763200000171\n",
      "Gradient Descent(18/49): loss=0.4505763200000031\n",
      "Gradient Descent(19/49): loss=0.45057632000000053\n",
      "Gradient Descent(20/49): loss=0.45057632000000014\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.45057632000000014\n",
      "Gradient Descent(23/49): loss=0.4505763199999998\n",
      "Gradient Descent(24/49): loss=0.4505763200000002\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.45057632\n",
      "Gradient Descent(28/49): loss=0.45057632000000014\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.45057632000000014\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763200000002\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.4505763200000002\n",
      "Gradient Descent(40/49): loss=0.4505763200000001\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4593860683788799\n",
      "Gradient Descent(2/49): loss=0.45187655242213515\n",
      "Gradient Descent(3/49): loss=0.45048804292173295\n",
      "Gradient Descent(4/49): loss=0.4502313075151083\n",
      "Gradient Descent(5/49): loss=0.4501838371384235\n",
      "Gradient Descent(6/49): loss=0.45017505986577444\n",
      "Gradient Descent(7/49): loss=0.4501734369480618\n",
      "Gradient Descent(8/49): loss=0.45017313687057653\n",
      "Gradient Descent(9/49): loss=0.4501730813862495\n",
      "Gradient Descent(10/49): loss=0.45017307112719745\n",
      "Gradient Descent(11/49): loss=0.45017306923029876\n",
      "Gradient Descent(12/49): loss=0.4501730688795621\n",
      "Gradient Descent(13/49): loss=0.45017306881471103\n",
      "Gradient Descent(14/49): loss=0.4501730688027202\n",
      "Gradient Descent(15/49): loss=0.45017306880050284\n",
      "Gradient Descent(16/49): loss=0.450173068800093\n",
      "Gradient Descent(17/49): loss=0.45017306880001734\n",
      "Gradient Descent(18/49): loss=0.4501730688000032\n",
      "Gradient Descent(19/49): loss=0.45017306880000063\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730687999999\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.4501730687999998\n",
      "Gradient Descent(26/49): loss=0.4501730687999999\n",
      "Gradient Descent(27/49): loss=0.4501730687999998\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.4501730687999999\n",
      "Gradient Descent(30/49): loss=0.4501730687999999\n",
      "Gradient Descent(31/49): loss=0.45017306879999985\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46154298782597125\n",
      "Gradient Descent(2/49): loss=0.453405484049947\n",
      "Gradient Descent(3/49): loss=0.4516835882509401\n",
      "Gradient Descent(4/49): loss=0.4513192350998701\n",
      "Gradient Descent(5/49): loss=0.4512421379731038\n",
      "Gradient Descent(6/49): loss=0.45122582422107993\n",
      "Gradient Descent(7/49): loss=0.4512223722311519\n",
      "Gradient Descent(8/49): loss=0.45122164179008273\n",
      "Gradient Descent(9/49): loss=0.4512214872287529\n",
      "Gradient Descent(10/49): loss=0.4512214545235752\n",
      "Gradient Descent(11/49): loss=0.4512214476031596\n",
      "Gradient Descent(12/49): loss=0.45122144613879983\n",
      "Gradient Descent(13/49): loss=0.4512214458289411\n",
      "Gradient Descent(14/49): loss=0.45122144576337514\n",
      "Gradient Descent(15/49): loss=0.4512214457495014\n",
      "Gradient Descent(16/49): loss=0.45122144574656586\n",
      "Gradient Descent(17/49): loss=0.4512214457459444\n",
      "Gradient Descent(18/49): loss=0.45122144574581313\n",
      "Gradient Descent(19/49): loss=0.45122144574578527\n",
      "Gradient Descent(20/49): loss=0.45122144574577927\n",
      "Gradient Descent(21/49): loss=0.4512214457457781\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.45122144574577755\n",
      "Gradient Descent(33/49): loss=0.45122144574577766\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.45122144574577794\n",
      "Gradient Descent(36/49): loss=0.45122144574577766\n",
      "Gradient Descent(37/49): loss=0.4512214457457778\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.45122144574577766\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4605782795528192\n",
      "Gradient Descent(2/49): loss=0.4522366435061955\n",
      "Gradient Descent(3/49): loss=0.4504715533187302\n",
      "Gradient Descent(4/49): loss=0.45009806023506255\n",
      "Gradient Descent(5/49): loss=0.45001902909855845\n",
      "Gradient Descent(6/49): loss=0.4500023061100742\n",
      "Gradient Descent(7/49): loss=0.4499987675257111\n",
      "Gradient Descent(8/49): loss=0.44999801876125956\n",
      "Gradient Descent(9/49): loss=0.44999786032270167\n",
      "Gradient Descent(10/49): loss=0.4499978267971028\n",
      "Gradient Descent(11/49): loss=0.4499978197030861\n",
      "Gradient Descent(12/49): loss=0.44999781820199214\n",
      "Gradient Descent(13/49): loss=0.44999781788436066\n",
      "Gradient Descent(14/49): loss=0.4499978178171498\n",
      "Gradient Descent(15/49): loss=0.4499978178029284\n",
      "Gradient Descent(16/49): loss=0.44999781779991865\n",
      "Gradient Descent(17/49): loss=0.44999781779928205\n",
      "Gradient Descent(18/49): loss=0.44999781779914727\n",
      "Gradient Descent(19/49): loss=0.44999781779911896\n",
      "Gradient Descent(20/49): loss=0.44999781779911263\n",
      "Gradient Descent(21/49): loss=0.44999781779911174\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911097\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.44999781779911097\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.4499978177991109\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.4499978177991109\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.44999781779911135\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46103437068800013\n",
      "Gradient Descent(2/49): loss=0.45278924352558075\n",
      "Gradient Descent(3/49): loss=0.4510445746180129\n",
      "Gradient Descent(4/49): loss=0.4506754026771715\n",
      "Gradient Descent(5/49): loss=0.45059728589448944\n",
      "Gradient Descent(6/49): loss=0.45058075638327405\n",
      "Gradient Descent(7/49): loss=0.45057725873870075\n",
      "Gradient Descent(8/49): loss=0.4505765186371089\n",
      "Gradient Descent(9/49): loss=0.4505763620316122\n",
      "Gradient Descent(10/49): loss=0.45057632889388916\n",
      "Gradient Descent(11/49): loss=0.45057632188194685\n",
      "Gradient Descent(12/49): loss=0.45057632039821993\n",
      "Gradient Descent(13/49): loss=0.4505763200842634\n",
      "Gradient Descent(14/49): loss=0.45057632001783016\n",
      "Gradient Descent(15/49): loss=0.4505763200037728\n",
      "Gradient Descent(16/49): loss=0.4505763200007982\n",
      "Gradient Descent(17/49): loss=0.4505763200001689\n",
      "Gradient Descent(18/49): loss=0.45057632000003556\n",
      "Gradient Descent(19/49): loss=0.4505763200000077\n",
      "Gradient Descent(20/49): loss=0.45057632000000153\n",
      "Gradient Descent(21/49): loss=0.4505763200000005\n",
      "Gradient Descent(22/49): loss=0.45057632000000014\n",
      "Gradient Descent(23/49): loss=0.4505763199999998\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.45057632000000014\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999998\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.45057632000000014\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.4505763200000001\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.45057631999999975\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4607164474419201\n",
      "Gradient Descent(2/49): loss=0.4524040477206305\n",
      "Gradient Descent(3/49): loss=0.45064514393960553\n",
      "Gradient Descent(4/49): loss=0.4502729598995405\n",
      "Gradient Descent(5/49): loss=0.4501942057566628\n",
      "Gradient Descent(6/49): loss=0.45017754138002997\n",
      "Gradient Descent(7/49): loss=0.4501740151979344\n",
      "Gradient Descent(8/49): loss=0.450173269057803\n",
      "Gradient Descent(9/49): loss=0.45017311117455117\n",
      "Gradient Descent(10/49): loss=0.4501730777664549\n",
      "Gradient Descent(11/49): loss=0.450173070697302\n",
      "Gradient Descent(12/49): loss=0.4501730692014691\n",
      "Gradient Descent(13/49): loss=0.450173068884951\n",
      "Gradient Descent(14/49): loss=0.45017306881797564\n",
      "Gradient Descent(15/49): loss=0.4501730688038035\n",
      "Gradient Descent(16/49): loss=0.4501730688008048\n",
      "Gradient Descent(17/49): loss=0.4501730688001704\n",
      "Gradient Descent(18/49): loss=0.4501730688000361\n",
      "Gradient Descent(19/49): loss=0.4501730688000077\n",
      "Gradient Descent(20/49): loss=0.45017306880000163\n",
      "Gradient Descent(21/49): loss=0.45017306880000046\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.45017306879999985\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730687999999\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.45017306880000024\n",
      "Gradient Descent(36/49): loss=0.4501730687999999\n",
      "Gradient Descent(37/49): loss=0.45017306879999985\n",
      "Gradient Descent(38/49): loss=0.4501730687999999\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4629331766222165\n",
      "Gradient Descent(2/49): loss=0.45403343232921106\n",
      "Gradient Descent(3/49): loss=0.4518966037244602\n",
      "Gradient Descent(4/49): loss=0.45138355117645945\n",
      "Gradient Descent(5/49): loss=0.4512603672596844\n",
      "Gradient Descent(6/49): loss=0.4512307908012668\n",
      "Gradient Descent(7/49): loss=0.45122368949360053\n",
      "Gradient Descent(8/49): loss=0.4512219844696299\n",
      "Gradient Descent(9/49): loss=0.45122157509337457\n",
      "Gradient Descent(10/49): loss=0.4512214768021358\n",
      "Gradient Descent(11/49): loss=0.4512214532024093\n",
      "Gradient Descent(12/49): loss=0.451221447536115\n",
      "Gradient Descent(13/49): loss=0.45122144617563775\n",
      "Gradient Descent(14/49): loss=0.4512214458489871\n",
      "Gradient Descent(15/49): loss=0.4512214457705585\n",
      "Gradient Descent(16/49): loss=0.4512214457517276\n",
      "Gradient Descent(17/49): loss=0.45122144574720646\n",
      "Gradient Descent(18/49): loss=0.45122144574612066\n",
      "Gradient Descent(19/49): loss=0.4512214457458602\n",
      "Gradient Descent(20/49): loss=0.4512214457457976\n",
      "Gradient Descent(21/49): loss=0.45122144574578243\n",
      "Gradient Descent(22/49): loss=0.45122144574577905\n",
      "Gradient Descent(23/49): loss=0.45122144574577805\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.45122144574577794\n",
      "Gradient Descent(39/49): loss=0.45122144574577794\n",
      "Gradient Descent(40/49): loss=0.45122144574577794\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.45122144574577766\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4620033417455445\n",
      "Gradient Descent(2/49): loss=0.4528803440986503\n",
      "Gradient Descent(3/49): loss=0.4506899123636305\n",
      "Gradient Descent(4/49): loss=0.4501639897040522\n",
      "Gradient Descent(5/49): loss=0.4500377156734875\n",
      "Gradient Descent(6/49): loss=0.45000739727874894\n",
      "Gradient Descent(7/49): loss=0.45000011783217214\n",
      "Gradient Descent(8/49): loss=0.44999837003704896\n",
      "Gradient Descent(9/49): loss=0.44999795039144\n",
      "Gradient Descent(10/49): loss=0.4499978496345293\n",
      "Gradient Descent(11/49): loss=0.44999782544279526\n",
      "Gradient Descent(12/49): loss=0.4499978196343596\n",
      "Gradient Descent(13/49): loss=0.4499978182397544\n",
      "Gradient Descent(14/49): loss=0.4499978179049095\n",
      "Gradient Descent(15/49): loss=0.44999781782451326\n",
      "Gradient Descent(16/49): loss=0.4499978178052103\n",
      "Gradient Descent(17/49): loss=0.44999781780057535\n",
      "Gradient Descent(18/49): loss=0.4499978177994626\n",
      "Gradient Descent(19/49): loss=0.4499978177991956\n",
      "Gradient Descent(20/49): loss=0.44999781779913123\n",
      "Gradient Descent(21/49): loss=0.4499978177991161\n",
      "Gradient Descent(22/49): loss=0.4499978177991121\n",
      "Gradient Descent(23/49): loss=0.44999781779911147\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911097\n",
      "Gradient Descent(34/49): loss=0.4499978177991109\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991113\n",
      "Gradient Descent(40/49): loss=0.4499978177991112\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.4499978177991113\n",
      "Gradient Descent(43/49): loss=0.4499978177991113\n",
      "Gradient Descent(44/49): loss=0.44999781779911135\n",
      "Gradient Descent(45/49): loss=0.4499978177991112\n",
      "Gradient Descent(46/49): loss=0.44999781779911097\n",
      "Gradient Descent(47/49): loss=0.44999781779911135\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46244294556799986\n",
      "Gradient Descent(2/49): loss=0.45342549679887706\n",
      "Gradient Descent(3/49): loss=0.4512604073494103\n",
      "Gradient Descent(4/49): loss=0.4507405693725933\n",
      "Gradient Descent(5/49): loss=0.4506157562743597\n",
      "Gradient Descent(6/49): loss=0.45058578864947363\n",
      "Gradient Descent(7/49): loss=0.45057859342273876\n",
      "Gradient Descent(8/49): loss=0.45057686584879963\n",
      "Gradient Descent(9/49): loss=0.4505764510582968\n",
      "Gradient Descent(10/49): loss=0.45057635146709696\n",
      "Gradient Descent(11/49): loss=0.45057632755525\n",
      "Gradient Descent(12/49): loss=0.4505763218140156\n",
      "Gradient Descent(13/49): loss=0.4505763204355451\n",
      "Gradient Descent(14/49): loss=0.4505763201045743\n",
      "Gradient Descent(15/49): loss=0.4505763200251084\n",
      "Gradient Descent(16/49): loss=0.45057632000602854\n",
      "Gradient Descent(17/49): loss=0.45057632000144765\n",
      "Gradient Descent(18/49): loss=0.4505763200003476\n",
      "Gradient Descent(19/49): loss=0.45057632000008335\n",
      "Gradient Descent(20/49): loss=0.45057632000002007\n",
      "Gradient Descent(21/49): loss=0.45057632000000486\n",
      "Gradient Descent(22/49): loss=0.45057632000000136\n",
      "Gradient Descent(23/49): loss=0.45057632000000036\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.45057632000000014\n",
      "Gradient Descent(26/49): loss=0.45057632\n",
      "Gradient Descent(27/49): loss=0.4505763199999998\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057631999999975\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763200000001\n",
      "Gradient Descent(33/49): loss=0.45057632000000014\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.4505763199999998\n",
      "Gradient Descent(38/49): loss=0.45057632000000014\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.45057632000000014\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.45057632000000014\n",
      "Gradient Descent(44/49): loss=0.45057631999999975\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.45057631999999975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=0.4505763199999997\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4621365149811201\n",
      "Gradient Descent(2/49): loss=0.45304549222808704\n",
      "Gradient Descent(3/49): loss=0.4508627376650839\n",
      "Gradient Descent(4/49): loss=0.4503386582945066\n",
      "Gradient Descent(5/49): loss=0.4502128268376311\n",
      "Gradient Descent(6/49): loss=0.450182614704835\n",
      "Gradient Descent(7/49): loss=0.450175360771751\n",
      "Gradient Descent(8/49): loss=0.45017361910241743\n",
      "Gradient Descent(9/49): loss=0.4501732009276104\n",
      "Gradient Descent(10/49): loss=0.4501731005238394\n",
      "Gradient Descent(11/49): loss=0.4501730764168937\n",
      "Gradient Descent(12/49): loss=0.4501730706288163\n",
      "Gradient Descent(13/49): loss=0.4501730692390987\n",
      "Gradient Descent(14/49): loss=0.4501730689054277\n",
      "Gradient Descent(15/49): loss=0.4501730688253132\n",
      "Gradient Descent(16/49): loss=0.4501730688060778\n",
      "Gradient Descent(17/49): loss=0.45017306880145913\n",
      "Gradient Descent(18/49): loss=0.4501730688003505\n",
      "Gradient Descent(19/49): loss=0.4501730688000841\n",
      "Gradient Descent(20/49): loss=0.45017306880002017\n",
      "Gradient Descent(21/49): loss=0.45017306880000474\n",
      "Gradient Descent(22/49): loss=0.45017306880000124\n",
      "Gradient Descent(23/49): loss=0.4501730688000004\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.45017306879999985\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.4501730688000001\n",
      "Gradient Descent(30/49): loss=0.4501730687999999\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730687999999\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730687999999\n",
      "Gradient Descent(41/49): loss=0.45017306879999985\n",
      "Gradient Descent(42/49): loss=0.4501730688000001\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4644111668161197\n",
      "Gradient Descent(2/49): loss=0.4547879463231978\n",
      "Gradient Descent(3/49): loss=0.45218582750191205\n",
      "Gradient Descent(4/49): loss=0.45148221457263654\n",
      "Gradient Descent(5/49): loss=0.4512919576365605\n",
      "Gradient Descent(6/49): loss=0.45124051216104566\n",
      "Gradient Descent(7/49): loss=0.45122660130446607\n",
      "Gradient Descent(8/49): loss=0.45122283980884714\n",
      "Gradient Descent(9/49): loss=0.45122182270043165\n",
      "Gradient Descent(10/49): loss=0.45122154767431616\n",
      "Gradient Descent(11/49): loss=0.4512214733072548\n",
      "Gradient Descent(12/49): loss=0.4512214531984011\n",
      "Gradient Descent(13/49): loss=0.4512214477609673\n",
      "Gradient Descent(14/49): loss=0.4512214462906849\n",
      "Gradient Descent(15/49): loss=0.45122144589312063\n",
      "Gradient Descent(16/49): loss=0.45122144578561935\n",
      "Gradient Descent(17/49): loss=0.4512214457565509\n",
      "Gradient Descent(18/49): loss=0.45122144574869116\n",
      "Gradient Descent(19/49): loss=0.45122144574656553\n",
      "Gradient Descent(20/49): loss=0.45122144574599093\n",
      "Gradient Descent(21/49): loss=0.45122144574583545\n",
      "Gradient Descent(22/49): loss=0.4512214457457934\n",
      "Gradient Descent(23/49): loss=0.45122144574578216\n",
      "Gradient Descent(24/49): loss=0.4512214457457788\n",
      "Gradient Descent(25/49): loss=0.45122144574577805\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.45122144574577766\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.45122144574577766\n",
      "Gradient Descent(37/49): loss=0.4512214457457775\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.45122144574577766\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.45122144574577766\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.45122144574577794\n",
      "Gradient Descent(47/49): loss=0.45122144574577766\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.45122144574577794\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4635184078662314\n",
      "Gradient Descent(2/49): loss=0.4536537853532601\n",
      "Gradient Descent(3/49): loss=0.45098639142575314\n",
      "Gradient Descent(4/49): loss=0.4502651281077549\n",
      "Gradient Descent(5/49): loss=0.4500700985065685\n",
      "Gradient Descent(6/49): loss=0.4500173625024076\n",
      "Gradient Descent(7/49): loss=0.4500031026868825\n",
      "Gradient Descent(8/49): loss=0.4499992468327643\n",
      "Gradient Descent(9/49): loss=0.44999820420981096\n",
      "Gradient Descent(10/49): loss=0.44999792228456437\n",
      "Gradient Descent(11/49): loss=0.44999784605197785\n",
      "Gradient Descent(12/49): loss=0.44999782543868616\n",
      "Gradient Descent(13/49): loss=0.4499978198648522\n",
      "Gradient Descent(14/49): loss=0.44999781835768754\n",
      "Gradient Descent(15/49): loss=0.4499978179501501\n",
      "Gradient Descent(16/49): loss=0.449997817839952\n",
      "Gradient Descent(17/49): loss=0.44999781781015447\n",
      "Gradient Descent(18/49): loss=0.4499978178020972\n",
      "Gradient Descent(19/49): loss=0.4499978177999185\n",
      "Gradient Descent(20/49): loss=0.44999781779932935\n",
      "Gradient Descent(21/49): loss=0.44999781779917036\n",
      "Gradient Descent(22/49): loss=0.4499978177991271\n",
      "Gradient Descent(23/49): loss=0.4499978177991153\n",
      "Gradient Descent(24/49): loss=0.44999781779911247\n",
      "Gradient Descent(25/49): loss=0.44999781779911147\n",
      "Gradient Descent(26/49): loss=0.4499978177991113\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.44999781779911135\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.4499978177991109\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.44999781779911097\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.44999781779911135\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.4499978177991113\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.4499978177991113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911097\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46394048307200003\n",
      "Gradient Descent(2/49): loss=0.45418998969466945\n",
      "Gradient Descent(3/49): loss=0.4515534562854385\n",
      "Gradient Descent(4/49): loss=0.4508405376515826\n",
      "Gradient Descent(5/49): loss=0.4506477644529879\n",
      "Gradient Descent(6/49): loss=0.45059563858008794\n",
      "Gradient Descent(7/49): loss=0.4505815437440558\n",
      "Gradient Descent(8/49): loss=0.4505777325003928\n",
      "Gradient Descent(9/49): loss=0.4505767019401063\n",
      "Gradient Descent(10/49): loss=0.45057642327660485\n",
      "Gradient Descent(11/49): loss=0.45057634792599377\n",
      "Gradient Descent(12/49): loss=0.45057632755118887\n",
      "Gradient Descent(13/49): loss=0.4505763220418416\n",
      "Gradient Descent(14/49): loss=0.45057632055211383\n",
      "Gradient Descent(15/49): loss=0.4505763201492914\n",
      "Gradient Descent(16/49): loss=0.45057632004036846\n",
      "Gradient Descent(17/49): loss=0.4505763200109156\n",
      "Gradient Descent(18/49): loss=0.45057632000295156\n",
      "Gradient Descent(19/49): loss=0.4505763200007984\n",
      "Gradient Descent(20/49): loss=0.45057632000021575\n",
      "Gradient Descent(21/49): loss=0.45057632000005826\n",
      "Gradient Descent(22/49): loss=0.45057632000001574\n",
      "Gradient Descent(23/49): loss=0.4505763200000044\n",
      "Gradient Descent(24/49): loss=0.4505763200000011\n",
      "Gradient Descent(25/49): loss=0.4505763200000002\n",
      "Gradient Descent(26/49): loss=0.45057632\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.4505763200000001\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.45057632\n",
      "Gradient Descent(45/49): loss=0.45057632000000014\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763200000001\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46364627099648004\n",
      "Gradient Descent(2/49): loss=0.45381622267392885\n",
      "Gradient Descent(3/49): loss=0.45115817760751026\n",
      "Gradient Descent(4/49): loss=0.4504394422215508\n",
      "Gradient Descent(5/49): loss=0.4502450961731874\n",
      "Gradient Descent(6/49): loss=0.4501925450017101\n",
      "Gradient Descent(7/49): loss=0.4501783351649423\n",
      "Gradient Descent(8/49): loss=0.45017449282508043\n",
      "Gradient Descent(9/49): loss=0.45017345385638174\n",
      "Gradient Descent(10/49): loss=0.4501731729192456\n",
      "Gradient Descent(11/49): loss=0.45017309695384394\n",
      "Gradient Descent(12/49): loss=0.45017307641279947\n",
      "Gradient Descent(13/49): loss=0.45017307085850083\n",
      "Gradient Descent(14/49): loss=0.4501730693566188\n",
      "Gradient Descent(15/49): loss=0.4501730689505096\n",
      "Gradient Descent(16/49): loss=0.45017306884069785\n",
      "Gradient Descent(17/49): loss=0.4501730688110048\n",
      "Gradient Descent(18/49): loss=0.45017306880297564\n",
      "Gradient Descent(19/49): loss=0.45017306880080477\n",
      "Gradient Descent(20/49): loss=0.4501730688002176\n",
      "Gradient Descent(21/49): loss=0.45017306880005875\n",
      "Gradient Descent(22/49): loss=0.4501730688000158\n",
      "Gradient Descent(23/49): loss=0.45017306880000435\n",
      "Gradient Descent(24/49): loss=0.4501730688000012\n",
      "Gradient Descent(25/49): loss=0.4501730688000004\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.45017306879999985\n",
      "Gradient Descent(29/49): loss=0.4501730687999999\n",
      "Gradient Descent(30/49): loss=0.45017306879999985\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.45017306879999985\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730687999999\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.45017306879999985\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730688000001\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730687999999\n",
      "Gradient Descent(41/49): loss=0.45017306879999985\n",
      "Gradient Descent(42/49): loss=0.4501730688000001\n",
      "Gradient Descent(43/49): loss=0.45017306879999985\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4659769584076802\n",
      "Gradient Descent(2/49): loss=0.45568498832600324\n",
      "Gradient Descent(3/49): loss=0.45257166737629584\n",
      "Gradient Descent(4/49): loss=0.4516298877890095\n",
      "Gradient Descent(5/49): loss=0.4513449994638554\n",
      "Gradient Descent(6/49): loss=0.45125882074549617\n",
      "Gradient Descent(7/49): loss=0.45123275168319255\n",
      "Gradient Descent(8/49): loss=0.451224865791846\n",
      "Gradient Descent(9/49): loss=0.45122248030971335\n",
      "Gradient Descent(10/49): loss=0.4512217587013683\n",
      "Gradient Descent(11/49): loss=0.45122154041484386\n",
      "Gradient Descent(12/49): loss=0.45122147438317034\n",
      "Gradient Descent(13/49): loss=0.45122145440858896\n",
      "Gradient Descent(14/49): loss=0.45122144836627825\n",
      "Gradient Descent(15/49): loss=0.45122144653847907\n",
      "Gradient Descent(16/49): loss=0.4512214459855699\n",
      "Gradient Descent(17/49): loss=0.45122144581831486\n",
      "Gradient Descent(18/49): loss=0.45122144576772033\n",
      "Gradient Descent(19/49): loss=0.4512214457524155\n",
      "Gradient Descent(20/49): loss=0.4512214457477857\n",
      "Gradient Descent(21/49): loss=0.4512214457463852\n",
      "Gradient Descent(22/49): loss=0.4512214457459614\n",
      "Gradient Descent(23/49): loss=0.4512214457458333\n",
      "Gradient Descent(24/49): loss=0.45122144574579465\n",
      "Gradient Descent(25/49): loss=0.45122144574578293\n",
      "Gradient Descent(26/49): loss=0.4512214457457792\n",
      "Gradient Descent(27/49): loss=0.45122144574577827\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.45122144574577766\n",
      "Gradient Descent(32/49): loss=0.45122144574577794\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.45122144574577794\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.45122144574577794\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.45122144574577755\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.45122144574577766\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46512347791488\n",
      "Gradient Descent(2/49): loss=0.4545733299841314\n",
      "Gradient Descent(3/49): loss=0.45138191023507956\n",
      "Gradient Descent(4/49): loss=0.4504165057609916\n",
      "Gradient Descent(5/49): loss=0.4501244709075801\n",
      "Gradient Descent(6/49): loss=0.45003613036442297\n",
      "Gradient Descent(7/49): loss=0.450009407350118\n",
      "Gradient Descent(8/49): loss=0.45000132363829093\n",
      "Gradient Descent(9/49): loss=0.4499988783154628\n",
      "Gradient Descent(10/49): loss=0.44999813860530746\n",
      "Gradient Descent(11/49): loss=0.44999791484298574\n",
      "Gradient Descent(12/49): loss=0.44999784715488306\n",
      "Gradient Descent(13/49): loss=0.44999782667923205\n",
      "Gradient Descent(14/49): loss=0.4499978204853477\n",
      "Gradient Descent(15/49): loss=0.44999781861169763\n",
      "Gradient Descent(16/49): loss=0.44999781804491873\n",
      "Gradient Descent(17/49): loss=0.44999781787346793\n",
      "Gradient Descent(18/49): loss=0.449997817821604\n",
      "Gradient Descent(19/49): loss=0.4499978178059152\n",
      "Gradient Descent(20/49): loss=0.4499978178011695\n",
      "Gradient Descent(21/49): loss=0.4499978177997338\n",
      "Gradient Descent(22/49): loss=0.4499978177992994\n",
      "Gradient Descent(23/49): loss=0.4499978177991681\n",
      "Gradient Descent(24/49): loss=0.4499978177991285\n",
      "Gradient Descent(25/49): loss=0.4499978177991162\n",
      "Gradient Descent(26/49): loss=0.44999781779911263\n",
      "Gradient Descent(27/49): loss=0.4499978177991116\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991113\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.44999781779911097\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.4499978177991113\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.44999781779911097\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.4499978177991109\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.4499978177991113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46552698320000013\n",
      "Gradient Descent(2/49): loss=0.45509889561800054\n",
      "Gradient Descent(3/49): loss=0.45194439912444545\n",
      "Gradient Descent(4/49): loss=0.45099016393514496\n",
      "Gradient Descent(5/49): loss=0.4507015077903814\n",
      "Gradient Descent(6/49): loss=0.4506141893065903\n",
      "Gradient Descent(7/49): loss=0.45058777546524376\n",
      "Gradient Descent(8/49): loss=0.45057978527823617\n",
      "Gradient Descent(9/49): loss=0.45057736824666644\n",
      "Gradient Descent(10/49): loss=0.4505766370946166\n",
      "Gradient Descent(11/49): loss=0.4505764159211215\n",
      "Gradient Descent(12/49): loss=0.4505763490161393\n",
      "Gradient Descent(13/49): loss=0.4505763287773821\n",
      "Gradient Descent(14/49): loss=0.4505763226551581\n",
      "Gradient Descent(15/49): loss=0.4505763208031852\n",
      "Gradient Descent(16/49): loss=0.45057632024296357\n",
      "Gradient Descent(17/49): loss=0.4505763200734966\n",
      "Gradient Descent(18/49): loss=0.45057632002223275\n",
      "Gradient Descent(19/49): loss=0.4505763200067255\n",
      "Gradient Descent(20/49): loss=0.4505763200020345\n",
      "Gradient Descent(21/49): loss=0.45057632000061537\n",
      "Gradient Descent(22/49): loss=0.450576320000186\n",
      "Gradient Descent(23/49): loss=0.45057632000005643\n",
      "Gradient Descent(24/49): loss=0.45057632000001696\n",
      "Gradient Descent(25/49): loss=0.45057632000000525\n",
      "Gradient Descent(26/49): loss=0.4505763200000015\n",
      "Gradient Descent(27/49): loss=0.45057632000000053\n",
      "Gradient Descent(28/49): loss=0.4505763200000003\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.4505763199999998\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999998\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.45057632\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.45057631999999975\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763200000003\n",
      "Gradient Descent(44/49): loss=0.45057632\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.45057632\n",
      "Gradient Descent(47/49): loss=0.4505763199999998\n",
      "Gradient Descent(48/49): loss=0.45057632\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46524571548800026\n",
      "Gradient Descent(2/49): loss=0.4547325444231204\n",
      "Gradient Descent(3/49): loss=0.4515523101759942\n",
      "Gradient Descent(4/49): loss=0.45059028931623823\n",
      "Gradient Descent(5/49): loss=0.4502992780061621\n",
      "Gradient Descent(6/49): loss=0.45021124708486393\n",
      "Gradient Descent(7/49): loss=0.4501846177311714\n",
      "Gradient Descent(8/49): loss=0.4501765623516792\n",
      "Gradient Descent(9/49): loss=0.45017412559938286\n",
      "Gradient Descent(10/49): loss=0.45017338848181326\n",
      "Gradient Descent(11/49): loss=0.45017316550374836\n",
      "Gradient Descent(12/49): loss=0.4501730980528841\n",
      "Gradient Descent(13/49): loss=0.45017307764899755\n",
      "Gradient Descent(14/49): loss=0.45017307147682184\n",
      "Gradient Descent(15/49): loss=0.4501730696097385\n",
      "Gradient Descent(16/49): loss=0.45017306904494575\n",
      "Gradient Descent(17/49): loss=0.4501730688740962\n",
      "Gradient Descent(18/49): loss=0.4501730688224139\n",
      "Gradient Descent(19/49): loss=0.45017306880678026\n",
      "Gradient Descent(20/49): loss=0.45017306880205094\n",
      "Gradient Descent(21/49): loss=0.45017306880062014\n",
      "Gradient Descent(22/49): loss=0.45017306880018765\n",
      "Gradient Descent(23/49): loss=0.4501730688000568\n",
      "Gradient Descent(24/49): loss=0.45017306880001706\n",
      "Gradient Descent(25/49): loss=0.4501730688000052\n",
      "Gradient Descent(26/49): loss=0.45017306880000146\n",
      "Gradient Descent(27/49): loss=0.45017306880000046\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730687999999\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.45017306879999985\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=0.45017306879999985\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.45017306879999985\n",
      "Gradient Descent(38/49): loss=0.4501730688000001\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.45017306879999985\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.45017306879999985\n",
      "Gradient Descent(44/49): loss=0.4501730687999999\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730688000001\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46763055139689813\n",
      "Gradient Descent(2/49): loss=0.45674146888681494\n",
      "Gradient Descent(3/49): loss=0.45307838153042257\n",
      "Gradient Descent(4/49): loss=0.45184611894373233\n",
      "Gradient Descent(5/49): loss=0.4514315858095699\n",
      "Gradient Descent(6/49): loss=0.4512921368632375\n",
      "Gradient Descent(7/49): loss=0.45124522623769125\n",
      "Gradient Descent(8/49): loss=0.45122944550325744\n",
      "Gradient Descent(9/49): loss=0.4512241368641939\n",
      "Gradient Descent(10/49): loss=0.45122235103801306\n",
      "Gradient Descent(11/49): loss=0.45122175028608574\n",
      "Gradient Descent(12/49): loss=0.45122154819313753\n",
      "Gradient Descent(13/49): loss=0.4512214802090695\n",
      "Gradient Descent(14/49): loss=0.4512214573392291\n",
      "Gradient Descent(15/49): loss=0.4512214496458146\n",
      "Gradient Descent(16/49): loss=0.45122144705775025\n",
      "Gradient Descent(17/49): loss=0.45122144618712545\n",
      "Gradient Descent(18/49): loss=0.4512214458942469\n",
      "Gradient Descent(19/49): loss=0.45122144579572276\n",
      "Gradient Descent(20/49): loss=0.45122144576257917\n",
      "Gradient Descent(21/49): loss=0.45122144575142986\n",
      "Gradient Descent(22/49): loss=0.4512214457476792\n",
      "Gradient Descent(23/49): loss=0.45122144574641715\n",
      "Gradient Descent(24/49): loss=0.45122144574599293\n",
      "Gradient Descent(25/49): loss=0.45122144574585005\n",
      "Gradient Descent(26/49): loss=0.4512214457458021\n",
      "Gradient Descent(27/49): loss=0.4512214457457861\n",
      "Gradient Descent(28/49): loss=0.45122144574578066\n",
      "Gradient Descent(29/49): loss=0.45122144574577866\n",
      "Gradient Descent(30/49): loss=0.45122144574577794\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.45122144574577766\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.45122144574577755\n",
      "Gradient Descent(35/49): loss=0.45122144574577794\n",
      "Gradient Descent(36/49): loss=0.45122144574577794\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.45122144574577794\n",
      "Gradient Descent(42/49): loss=0.45122144574577766\n",
      "Gradient Descent(43/49): loss=0.45122144574577766\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.45122144574577794\n",
      "Gradient Descent(46/49): loss=0.45122144574577794\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.45122144574577766\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4668185518914905\n",
      "Gradient Descent(2/49): loss=0.4556563127477875\n",
      "Gradient Descent(3/49): loss=0.4519013354998459\n",
      "Gradient Descent(4/49): loss=0.45063816115363825\n",
      "Gradient Descent(5/49): loss=0.4502132293035742\n",
      "Gradient Descent(6/49): loss=0.4500702822292125\n",
      "Gradient Descent(7/49): loss=0.4500221948333971\n",
      "Gradient Descent(8/49): loss=0.4500060182334451\n",
      "Gradient Descent(9/49): loss=0.45000057642522096\n",
      "Gradient Descent(10/49): loss=0.4499987458009345\n",
      "Gradient Descent(11/49): loss=0.4499981299789246\n",
      "Gradient Descent(12/49): loss=0.44999792281640033\n",
      "Gradient Descent(13/49): loss=0.44999785312692714\n",
      "Gradient Descent(14/49): loss=0.4499978296833884\n",
      "Gradient Descent(15/49): loss=0.44999782179698183\n",
      "Gradient Descent(16/49): loss=0.4499978191439949\n",
      "Gradient Descent(17/49): loss=0.44999781825153\n",
      "Gradient Descent(18/49): loss=0.44999781795130483\n",
      "Gradient Descent(19/49): loss=0.4499978178503091\n",
      "Gradient Descent(20/49): loss=0.44999781781633413\n",
      "Gradient Descent(21/49): loss=0.44999781780490494\n",
      "Gradient Descent(22/49): loss=0.44999781780106013\n",
      "Gradient Descent(23/49): loss=0.4499978177997669\n",
      "Gradient Descent(24/49): loss=0.4499978177993317\n",
      "Gradient Descent(25/49): loss=0.44999781779918524\n",
      "Gradient Descent(26/49): loss=0.44999781779913606\n",
      "Gradient Descent(27/49): loss=0.44999781779911957\n",
      "Gradient Descent(28/49): loss=0.4499978177991137\n",
      "Gradient Descent(29/49): loss=0.44999781779911213\n",
      "Gradient Descent(30/49): loss=0.4499978177991116\n",
      "Gradient Descent(31/49): loss=0.44999781779911135\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911097\n",
      "Gradient Descent(36/49): loss=0.4499978177991113\n",
      "Gradient Descent(37/49): loss=0.44999781779911097\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911097\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.4499978177991112\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.4499978177991112\n",
      "Gradient Descent(46/49): loss=0.44999781779911097\n",
      "Gradient Descent(47/49): loss=0.4499978177991112\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911097\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46720244595200017\n",
      "Gradient Descent(2/49): loss=0.45616934877025206\n",
      "Gradient Descent(3/49): loss=0.45245781487831255\n",
      "Gradient Descent(4/49): loss=0.45120925487706454\n",
      "Gradient Descent(5/49): loss=0.45078923929264453\n",
      "Gradient Descent(6/49): loss=0.4506479460500457\n",
      "Gradient Descent(7/49): loss=0.4506004150032354\n",
      "Gradient Descent(8/49): loss=0.4505844255590883\n",
      "Gradient Descent(9/49): loss=0.45057904671007726\n",
      "Gradient Descent(10/49): loss=0.4505772372652701\n",
      "Gradient Descent(11/49): loss=0.4505766285680369\n",
      "Gradient Descent(12/49): loss=0.4505764238022877\n",
      "Gradient Descent(13/49): loss=0.4505763549190895\n",
      "Gradient Descent(14/49): loss=0.4505763317467816\n",
      "Gradient Descent(15/49): loss=0.45057632395161745\n",
      "Gradient Descent(16/49): loss=0.4505763213293243\n",
      "Gradient Descent(17/49): loss=0.45057632044718454\n",
      "Gradient Descent(18/49): loss=0.45057632015043286\n",
      "Gradient Descent(19/49): loss=0.45057632005060566\n",
      "Gradient Descent(20/49): loss=0.4505763200170238\n",
      "Gradient Descent(21/49): loss=0.4505763200057268\n",
      "Gradient Descent(22/49): loss=0.4505763200019265\n",
      "Gradient Descent(23/49): loss=0.450576320000648\n",
      "Gradient Descent(24/49): loss=0.45057632000021786\n",
      "Gradient Descent(25/49): loss=0.45057632000007325\n",
      "Gradient Descent(26/49): loss=0.45057632000002473\n",
      "Gradient Descent(27/49): loss=0.4505763200000083\n",
      "Gradient Descent(28/49): loss=0.45057632000000264\n",
      "Gradient Descent(29/49): loss=0.450576320000001\n",
      "Gradient Descent(30/49): loss=0.4505763200000005\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.45057632000000014\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763199999998\n",
      "Gradient Descent(36/49): loss=0.4505763199999998\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.45057632\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.45057632000000014\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.45057631999999975\n",
      "Gradient Descent(44/49): loss=0.4505763200000001\n",
      "Gradient Descent(45/49): loss=0.4505763199999998\n",
      "Gradient Descent(46/49): loss=0.45057632\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.45057632\n",
      "Gradient Descent(49/49): loss=0.4505763199999998\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46693484845568006\n",
      "Gradient Descent(2/49): loss=0.4558117314761711\n",
      "Gradient Descent(3/49): loss=0.45206991492426435\n",
      "Gradient Descent(4/49): loss=0.4508111678362024\n",
      "Gradient Descent(5/49): loss=0.4503877253157784\n",
      "Gradient Descent(6/49): loss=0.45024527925190777\n",
      "Gradient Descent(7/49): loss=0.4501973603960218\n",
      "Gradient Descent(8/49): loss=0.4501812404929015\n",
      "Gradient Descent(9/49): loss=0.45017581775749194\n",
      "Gradient Descent(10/49): loss=0.4501739935493004\n",
      "Gradient Descent(11/49): loss=0.4501733798856644\n",
      "Gradient Descent(12/49): loss=0.45017317344921776\n",
      "Gradient Descent(13/49): loss=0.45017310400399674\n",
      "Gradient Descent(14/49): loss=0.4501730806426243\n",
      "Gradient Descent(15/49): loss=0.45017307278385893\n",
      "Gradient Descent(16/49): loss=0.4501730701401701\n",
      "Gradient Descent(17/49): loss=0.4501730692508332\n",
      "Gradient Descent(18/49): loss=0.4501730689516603\n",
      "Gradient Descent(19/49): loss=0.45017306885101865\n",
      "Gradient Descent(20/49): loss=0.4501730688171627\n",
      "Gradient Descent(21/49): loss=0.4501730688057735\n",
      "Gradient Descent(22/49): loss=0.4501730688019422\n",
      "Gradient Descent(23/49): loss=0.45017306880065316\n",
      "Gradient Descent(24/49): loss=0.4501730688002198\n",
      "Gradient Descent(25/49): loss=0.45017306880007396\n",
      "Gradient Descent(26/49): loss=0.450173068800025\n",
      "Gradient Descent(27/49): loss=0.4501730688000083\n",
      "Gradient Descent(28/49): loss=0.4501730688000028\n",
      "Gradient Descent(29/49): loss=0.45017306880000085\n",
      "Gradient Descent(30/49): loss=0.4501730688000004\n",
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.4501730688000003\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.4501730688000001\n",
      "Gradient Descent(38/49): loss=0.45017306879999985\n",
      "Gradient Descent(39/49): loss=0.45017306879999985\n",
      "Gradient Descent(40/49): loss=0.4501730687999999\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730687999999\n",
      "Gradient Descent(43/49): loss=0.45017306879999985\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730687999999\n",
      "Gradient Descent(46/49): loss=0.4501730687999999\n",
      "Gradient Descent(47/49): loss=0.45017306879999985\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4693719457837741\n",
      "Gradient Descent(2/49): loss=0.45797524680991564\n",
      "Gradient Descent(3/49): loss=0.4537345351217438\n",
      "Gradient Descent(4/49): loss=0.45215656630257456\n",
      "Gradient Descent(5/49): loss=0.45156940410496177\n",
      "Gradient Descent(6/49): loss=0.4513509210512302\n",
      "Gradient Descent(7/49): loss=0.4512696235069366\n",
      "Gradient Descent(8/49): loss=0.451239372690705\n",
      "Gradient Descent(9/49): loss=0.4512281163619851\n",
      "Gradient Descent(10/49): loss=0.4512239278820687\n",
      "Gradient Descent(11/49): loss=0.45122236934869153\n",
      "Gradient Descent(12/49): loss=0.45122178941842206\n",
      "Gradient Descent(13/49): loss=0.4512215736263688\n",
      "Gradient Descent(14/49): loss=0.45122149333014566\n",
      "Gradient Descent(15/49): loss=0.451221463451921\n",
      "Gradient Descent(16/49): loss=0.4512214523342338\n",
      "Gradient Descent(17/49): loss=0.4512214481973424\n",
      "Gradient Descent(18/49): loss=0.45122144665800484\n",
      "Gradient Descent(19/49): loss=0.45122144608521747\n",
      "Gradient Descent(20/49): loss=0.45122144587208324\n",
      "Gradient Descent(21/49): loss=0.45122144579277623\n",
      "Gradient Descent(22/49): loss=0.4512214457632657\n",
      "Gradient Descent(23/49): loss=0.451221445752285\n",
      "Gradient Descent(24/49): loss=0.4512214457481991\n",
      "Gradient Descent(25/49): loss=0.4512214457466788\n",
      "Gradient Descent(26/49): loss=0.45122144574611284\n",
      "Gradient Descent(27/49): loss=0.45122144574590245\n",
      "Gradient Descent(28/49): loss=0.4512214457458243\n",
      "Gradient Descent(29/49): loss=0.4512214457457951\n",
      "Gradient Descent(30/49): loss=0.45122144574578427\n",
      "Gradient Descent(31/49): loss=0.4512214457457802\n",
      "Gradient Descent(32/49): loss=0.45122144574577866\n",
      "Gradient Descent(33/49): loss=0.4512214457457781\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.45122144574577794\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457778\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.45122144574577766\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.45122144574577766\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4686036297960618\n",
      "Gradient Descent(2/49): loss=0.45692104044317594\n",
      "Gradient Descent(3/49): loss=0.4525739489449672\n",
      "Gradient Descent(4/49): loss=0.45095639619848393\n",
      "Gradient Descent(5/49): loss=0.4503545048215179\n",
      "Gradient Descent(6/49): loss=0.45013054104014866\n",
      "Gradient Descent(7/49): loss=0.4500472041171013\n",
      "Gradient Descent(8/49): loss=0.45001619444803515\n",
      "Gradient Descent(9/49): loss=0.4500046557501756\n",
      "Gradient Descent(10/49): loss=0.45000036220070244\n",
      "Gradient Descent(11/49): loss=0.4499987645709432\n",
      "Gradient Descent(12/49): loss=0.44999817009290993\n",
      "Gradient Descent(13/49): loss=0.44999794888763384\n",
      "Gradient Descent(14/49): loss=0.44999786657715035\n",
      "Gradient Descent(15/49): loss=0.44999783594941956\n",
      "Gradient Descent(16/49): loss=0.44999782455284065\n",
      "Gradient Descent(17/49): loss=0.44999782031217395\n",
      "Gradient Descent(18/49): loss=0.4499978187342218\n",
      "Gradient Descent(19/49): loss=0.4499978181470656\n",
      "Gradient Descent(20/49): loss=0.4499978179285851\n",
      "Gradient Descent(21/49): loss=0.4499978178472883\n",
      "Gradient Descent(22/49): loss=0.44999781781703785\n",
      "Gradient Descent(23/49): loss=0.4499978178057816\n",
      "Gradient Descent(24/49): loss=0.4499978178015935\n",
      "Gradient Descent(25/49): loss=0.44999781780003467\n",
      "Gradient Descent(26/49): loss=0.4499978177994548\n",
      "Gradient Descent(27/49): loss=0.449997817799239\n",
      "Gradient Descent(28/49): loss=0.4499978177991587\n",
      "Gradient Descent(29/49): loss=0.4499978177991289\n",
      "Gradient Descent(30/49): loss=0.44999781779911774\n",
      "Gradient Descent(31/49): loss=0.44999781779911335\n",
      "Gradient Descent(32/49): loss=0.4499978177991121\n",
      "Gradient Descent(33/49): loss=0.4499978177991115\n",
      "Gradient Descent(34/49): loss=0.44999781779911135\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911097\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.4499978177991113\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.4499978177991111\n",
      "Gradient Descent(47/49): loss=0.4499978177991112\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4689668713279998\n",
      "Gradient Descent(2/49): loss=0.4574194441491486\n",
      "Gradient Descent(3/49): loss=0.4531226464958982\n",
      "Gradient Descent(4/49): loss=0.45152380808912357\n",
      "Gradient Descent(5/49): loss=0.4509288803179629\n",
      "Gradient Descent(6/49): loss=0.45070750769431395\n",
      "Gradient Descent(7/49): loss=0.4506251349410542\n",
      "Gradient Descent(8/49): loss=0.4505944840395663\n",
      "Gradient Descent(9/49): loss=0.45058307883912246\n",
      "Gradient Descent(10/49): loss=0.4505788349640375\n",
      "Gradient Descent(11/49): loss=0.4505772558181182\n",
      "Gradient Descent(12/49): loss=0.4505766682179219\n",
      "Gradient Descent(13/49): loss=0.4505764495718886\n",
      "Gradient Descent(14/49): loss=0.45057636821369973\n",
      "Gradient Descent(15/49): loss=0.45057633794031804\n",
      "Gradient Descent(16/49): loss=0.450576326675592\n",
      "Gradient Descent(17/49): loss=0.45057632248398816\n",
      "Gradient Descent(18/49): loss=0.450576320924292\n",
      "Gradient Descent(19/49): loss=0.450576320343929\n",
      "Gradient Descent(20/49): loss=0.45057632012797605\n",
      "Gradient Descent(21/49): loss=0.45057632004761977\n",
      "Gradient Descent(22/49): loss=0.45057632001771947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=0.4505763200065934\n",
      "Gradient Descent(24/49): loss=0.45057632000245335\n",
      "Gradient Descent(25/49): loss=0.45057632000091297\n",
      "Gradient Descent(26/49): loss=0.4505763200003396\n",
      "Gradient Descent(27/49): loss=0.4505763200001265\n",
      "Gradient Descent(28/49): loss=0.4505763200000471\n",
      "Gradient Descent(29/49): loss=0.4505763200000176\n",
      "Gradient Descent(30/49): loss=0.4505763200000065\n",
      "Gradient Descent(31/49): loss=0.45057632000000253\n",
      "Gradient Descent(32/49): loss=0.45057632000000075\n",
      "Gradient Descent(33/49): loss=0.45057632000000036\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.45057632000000014\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.45057632\n",
      "Gradient Descent(49/49): loss=0.4505763200000003\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4687136698995202\n",
      "Gradient Descent(2/49): loss=0.45707202646913137\n",
      "Gradient Descent(3/49): loss=0.4527401709486841\n",
      "Gradient Descent(4/49): loss=0.4511282875095252\n",
      "Gradient Descent(5/49): loss=0.4505285056818142\n",
      "Gradient Descent(6/49): loss=0.4503053268637232\n",
      "Gradient Descent(7/49): loss=0.45022228202551146\n",
      "Gradient Descent(8/49): loss=0.45019138104121276\n",
      "Gradient Descent(9/49): loss=0.45017988278495535\n",
      "Gradient Descent(10/49): loss=0.4501756042838019\n",
      "Gradient Descent(11/49): loss=0.45017401225352266\n",
      "Gradient Descent(12/49): loss=0.4501734198590559\n",
      "Gradient Descent(13/49): loss=0.4501731994290747\n",
      "Gradient Descent(14/49): loss=0.4501731174070787\n",
      "Gradient Descent(15/49): loss=0.45017308688669394\n",
      "Gradient Descent(16/49): loss=0.4501730755300589\n",
      "Gradient Descent(17/49): loss=0.4501730713042549\n",
      "Gradient Descent(18/49): loss=0.45017306973183335\n",
      "Gradient Descent(19/49): loss=0.45017306914673527\n",
      "Gradient Descent(20/49): loss=0.45017306892902026\n",
      "Gradient Descent(21/49): loss=0.4501730688480084\n",
      "Gradient Descent(22/49): loss=0.4501730688178641\n",
      "Gradient Descent(23/49): loss=0.4501730688066471\n",
      "Gradient Descent(24/49): loss=0.45017306880247354\n",
      "Gradient Descent(25/49): loss=0.45017306880092023\n",
      "Gradient Descent(26/49): loss=0.4501730688003425\n",
      "Gradient Descent(27/49): loss=0.4501730688001276\n",
      "Gradient Descent(28/49): loss=0.45017306880004726\n",
      "Gradient Descent(29/49): loss=0.4501730688000176\n",
      "Gradient Descent(30/49): loss=0.4501730688000066\n",
      "Gradient Descent(31/49): loss=0.45017306880000235\n",
      "Gradient Descent(32/49): loss=0.45017306880000085\n",
      "Gradient Descent(33/49): loss=0.4501730688000003\n",
      "Gradient Descent(34/49): loss=0.45017306880000024\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730687999999\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730687999999\n",
      "Gradient Descent(42/49): loss=0.45017306879999985\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730687999999\n",
      "Gradient Descent(45/49): loss=0.4501730688000001\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47120114156830706\n",
      "Gradient Descent(2/49): loss=0.4594051291546862\n",
      "Gradient Descent(3/49): loss=0.45457348247006707\n",
      "Gradient Descent(4/49): loss=0.45259443998804644\n",
      "Gradient Descent(5/49): loss=0.45178382418741114\n",
      "Gradient Descent(6/49): loss=0.45145179595547086\n",
      "Gradient Descent(7/49): loss=0.451315797191668\n",
      "Gradient Descent(8/49): loss=0.45126009209801443\n",
      "Gradient Descent(9/49): loss=0.4512372752916539\n",
      "Gradient Descent(10/49): loss=0.4512279295277686\n",
      "Gradient Descent(11/49): loss=0.4512241015028812\n",
      "Gradient Descent(12/49): loss=0.45122253354388747\n",
      "Gradient Descent(13/49): loss=0.4512218913078834\n",
      "Gradient Descent(14/49): loss=0.45122162824801626\n",
      "Gradient Descent(15/49): loss=0.4512215204986945\n",
      "Gradient Descent(16/49): loss=0.45122147636457255\n",
      "Gradient Descent(17/49): loss=0.45122145828723614\n",
      "Gradient Descent(18/49): loss=0.451221450882759\n",
      "Gradient Descent(19/49): loss=0.4512214478498853\n",
      "Gradient Descent(20/49): loss=0.45122144660762015\n",
      "Gradient Descent(21/49): loss=0.4512214460987884\n",
      "Gradient Descent(22/49): loss=0.45122144589037105\n",
      "Gradient Descent(23/49): loss=0.451221445805003\n",
      "Gradient Descent(24/49): loss=0.45122144577003676\n",
      "Gradient Descent(25/49): loss=0.45122144575571416\n",
      "Gradient Descent(26/49): loss=0.4512214457498477\n",
      "Gradient Descent(27/49): loss=0.45122144574744477\n",
      "Gradient Descent(28/49): loss=0.45122144574646067\n",
      "Gradient Descent(29/49): loss=0.45122144574605727\n",
      "Gradient Descent(30/49): loss=0.4512214457458922\n",
      "Gradient Descent(31/49): loss=0.45122144574582473\n",
      "Gradient Descent(32/49): loss=0.4512214457457972\n",
      "Gradient Descent(33/49): loss=0.45122144574578554\n",
      "Gradient Descent(34/49): loss=0.45122144574578105\n",
      "Gradient Descent(35/49): loss=0.4512214457457791\n",
      "Gradient Descent(36/49): loss=0.45122144574577844\n",
      "Gradient Descent(37/49): loss=0.45122144574577805\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.45122144574577766\n",
      "Gradient Descent(44/49): loss=0.45122144574577755\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.45122144574577805\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4704787116285955\n",
      "Gradient Descent(2/49): loss=0.4583867919116683\n",
      "Gradient Descent(3/49): loss=0.4534339415956149\n",
      "Gradient Descent(4/49): loss=0.4514052541061591\n",
      "Gradient Descent(5/49): loss=0.4505743037104779\n",
      "Gradient Descent(6/49): loss=0.4502339464284068\n",
      "Gradient Descent(7/49): loss=0.4500945360856707\n",
      "Gradient Descent(8/49): loss=0.4500374336092858\n",
      "Gradient Descent(9/49): loss=0.4500140444349587\n",
      "Gradient Descent(10/49): loss=0.45000446422915424\n",
      "Gradient Descent(11/49): loss=0.45000054017685687\n",
      "Gradient Descent(12/49): loss=0.4499989328850358\n",
      "Gradient Descent(13/49): loss=0.4499982745383059\n",
      "Gradient Descent(14/49): loss=0.4499980048794853\n",
      "Gradient Descent(15/49): loss=0.44999789442723237\n",
      "Gradient Descent(16/49): loss=0.4499978491859896\n",
      "Gradient Descent(17/49): loss=0.4499978306551765\n",
      "Gradient Descent(18/49): loss=0.4499978230649555\n",
      "Gradient Descent(19/49): loss=0.4499978199560009\n",
      "Gradient Descent(20/49): loss=0.4499978186825731\n",
      "Gradient Descent(21/49): loss=0.4499978181609772\n",
      "Gradient Descent(22/49): loss=0.4499978179473315\n",
      "Gradient Descent(23/49): loss=0.44999781785982224\n",
      "Gradient Descent(24/49): loss=0.4499978178239785\n",
      "Gradient Descent(25/49): loss=0.44999781780929693\n",
      "Gradient Descent(26/49): loss=0.44999781780328313\n",
      "Gradient Descent(27/49): loss=0.4499978178008199\n",
      "Gradient Descent(28/49): loss=0.4499978177998111\n",
      "Gradient Descent(29/49): loss=0.4499978177993977\n",
      "Gradient Descent(30/49): loss=0.4499978177992285\n",
      "Gradient Descent(31/49): loss=0.44999781779915926\n",
      "Gradient Descent(32/49): loss=0.4499978177991308\n",
      "Gradient Descent(33/49): loss=0.4499978177991191\n",
      "Gradient Descent(34/49): loss=0.44999781779911446\n",
      "Gradient Descent(35/49): loss=0.44999781779911247\n",
      "Gradient Descent(36/49): loss=0.4499978177991117\n",
      "Gradient Descent(37/49): loss=0.44999781779911135\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911097\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911097\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.4499978177991113\n",
      "Gradient Descent(47/49): loss=0.4499978177991112\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47082025932800003\n",
      "Gradient Descent(2/49): loss=0.4588682375487487\n",
      "Gradient Descent(3/49): loss=0.4539726894279672\n",
      "Gradient Descent(4/49): loss=0.4519674729176955\n",
      "Gradient Descent(5/49): loss=0.4511461362350881\n",
      "Gradient Descent(6/49): loss=0.450809716729892\n",
      "Gradient Descent(7/49): loss=0.4506719193005636\n",
      "Gradient Descent(8/49): loss=0.4506154774735109\n",
      "Gradient Descent(9/49): loss=0.45059235890114985\n",
      "Gradient Descent(10/49): loss=0.4505828895339111\n",
      "Gradient Descent(11/49): loss=0.45057901088109\n",
      "Gradient Descent(12/49): loss=0.45057742218489455\n",
      "Gradient Descent(13/49): loss=0.4505767714549328\n",
      "Gradient Descent(14/49): loss=0.4505765049159405\n",
      "Gradient Descent(15/49): loss=0.45057639574156927\n",
      "Gradient Descent(16/49): loss=0.45057635102374705\n",
      "Gradient Descent(17/49): loss=0.4505763327073267\n",
      "Gradient Descent(18/49): loss=0.450576325204921\n",
      "Gradient Descent(19/49): loss=0.4505763221319356\n",
      "Gradient Descent(20/49): loss=0.4505763208732408\n",
      "Gradient Descent(21/49): loss=0.45057632035767947\n",
      "Gradient Descent(22/49): loss=0.45057632014650556\n",
      "Gradient Descent(23/49): loss=0.4505763200600089\n",
      "Gradient Descent(24/49): loss=0.45057632002457954\n",
      "Gradient Descent(25/49): loss=0.4505763200100678\n",
      "Gradient Descent(26/49): loss=0.45057632000412373\n",
      "Gradient Descent(27/49): loss=0.45057632000168923\n",
      "Gradient Descent(28/49): loss=0.45057632000069175\n",
      "Gradient Descent(29/49): loss=0.4505763200002836\n",
      "Gradient Descent(30/49): loss=0.45057632000011627\n",
      "Gradient Descent(31/49): loss=0.4505763200000475\n",
      "Gradient Descent(32/49): loss=0.4505763200000196\n",
      "Gradient Descent(33/49): loss=0.45057632000000797\n",
      "Gradient Descent(34/49): loss=0.4505763200000031\n",
      "Gradient Descent(35/49): loss=0.4505763200000013\n",
      "Gradient Descent(36/49): loss=0.4505763200000006\n",
      "Gradient Descent(37/49): loss=0.45057632000000036\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.45057632000000014\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.45057632000000014\n",
      "Gradient Descent(47/49): loss=0.4505763200000001\n",
      "Gradient Descent(48/49): loss=0.4505763199999998\n",
      "Gradient Descent(49/49): loss=0.4505763200000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4705821798195203\n",
      "Gradient Descent(2/49): loss=0.45853264067359484\n",
      "Gradient Descent(3/49): loss=0.45359714943942425\n",
      "Gradient Descent(4/49): loss=0.4515755722299082\n",
      "Gradient Descent(5/49): loss=0.4507475342048905\n",
      "Gradient Descent(6/49): loss=0.4504083698298432\n",
      "Gradient Descent(7/49): loss=0.4502694481018238\n",
      "Gradient Descent(8/49): loss=0.450212545762027\n",
      "Gradient Descent(9/49): loss=0.4501892385636465\n",
      "Gradient Descent(10/49): loss=0.4501796919351895\n",
      "Gradient Descent(11/49): loss=0.45017578163617367\n",
      "Gradient Descent(12/49): loss=0.4501741799776966\n",
      "Gradient Descent(13/49): loss=0.4501735239383846\n",
      "Gradient Descent(14/49): loss=0.4501732552246824\n",
      "Gradient Descent(15/49): loss=0.45017314515954987\n",
      "Gradient Descent(16/49): loss=0.45017310007687156\n",
      "Gradient Descent(17/49): loss=0.45017308161100666\n",
      "Gradient Descent(18/49): loss=0.4501730740473884\n",
      "Gradient Descent(19/49): loss=0.45017307094933023\n",
      "Gradient Descent(20/49): loss=0.45017306968036574\n",
      "Gradient Descent(21/49): loss=0.4501730691605977\n",
      "Gradient Descent(22/49): loss=0.45017306894770076\n",
      "Gradient Descent(23/49): loss=0.45017306886049835\n",
      "Gradient Descent(24/49): loss=0.4501730688247802\n",
      "Gradient Descent(25/49): loss=0.45017306881014996\n",
      "Gradient Descent(26/49): loss=0.45017306880415736\n",
      "Gradient Descent(27/49): loss=0.45017306880170294\n",
      "Gradient Descent(28/49): loss=0.4501730688006976\n",
      "Gradient Descent(29/49): loss=0.45017306880028585\n",
      "Gradient Descent(30/49): loss=0.450173068800117\n",
      "Gradient Descent(31/49): loss=0.4501730688000478\n",
      "Gradient Descent(32/49): loss=0.4501730688000198\n",
      "Gradient Descent(33/49): loss=0.45017306880000807\n",
      "Gradient Descent(34/49): loss=0.4501730688000034\n",
      "Gradient Descent(35/49): loss=0.45017306880000135\n",
      "Gradient Descent(36/49): loss=0.45017306880000046\n",
      "Gradient Descent(37/49): loss=0.4501730688000003\n",
      "Gradient Descent(38/49): loss=0.45017306879999985\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730687999999\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730687999999\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000001\n",
      "Gradient Descent(49/49): loss=0.4501730688000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47311813875049824\n",
      "Gradient Descent(2/49): loss=0.46105087123559835\n",
      "Gradient Descent(3/49): loss=0.45563387484815887\n",
      "Gradient Descent(4/49): loss=0.4532021851698367\n",
      "Gradient Descent(5/49): loss=0.4521105996732378\n",
      "Gradient Descent(6/49): loss=0.45162058694381463\n",
      "Gradient Descent(7/49): loss=0.4514006202295764\n",
      "Gradient Descent(8/49): loss=0.45130187717155507\n",
      "Gradient Descent(9/49): loss=0.45125755141280915\n",
      "Gradient Descent(10/49): loss=0.451237653579708\n",
      "Gradient Descent(11/49): loss=0.4512287214424292\n",
      "Gradient Descent(12/49): loss=0.45122471180600454\n",
      "Gradient Descent(13/49): loss=0.4512229118802138\n",
      "Gradient Descent(14/49): loss=0.4512221038935262\n",
      "Gradient Descent(15/49): loss=0.451221741188302\n",
      "Gradient Descent(16/49): loss=0.4512215783699267\n",
      "Gradient Descent(17/49): loss=0.45122150528075833\n",
      "Gradient Descent(18/49): loss=0.45122147247103084\n",
      "Gradient Descent(19/49): loss=0.45122145774274364\n",
      "Gradient Descent(20/49): loss=0.45122145113121587\n",
      "Gradient Descent(21/49): loss=0.45122144816330073\n",
      "Gradient Descent(22/49): loss=0.45122144683100407\n",
      "Gradient Descent(23/49): loss=0.4512214462329358\n",
      "Gradient Descent(24/49): loss=0.45122144596446306\n",
      "Gradient Descent(25/49): loss=0.45122144584394575\n",
      "Gradient Descent(26/49): loss=0.4512214457898452\n",
      "Gradient Descent(27/49): loss=0.4512214457655597\n",
      "Gradient Descent(28/49): loss=0.4512214457546581\n",
      "Gradient Descent(29/49): loss=0.4512214457497642\n",
      "Gradient Descent(30/49): loss=0.4512214457475671\n",
      "Gradient Descent(31/49): loss=0.451221445746581\n",
      "Gradient Descent(32/49): loss=0.45122144574613854\n",
      "Gradient Descent(33/49): loss=0.45122144574593936\n",
      "Gradient Descent(34/49): loss=0.45122144574585027\n",
      "Gradient Descent(35/49): loss=0.4512214457458104\n",
      "Gradient Descent(36/49): loss=0.45122144574579237\n",
      "Gradient Descent(37/49): loss=0.4512214457457844\n",
      "Gradient Descent(38/49): loss=0.45122144574578066\n",
      "Gradient Descent(39/49): loss=0.4512214457457792\n",
      "Gradient Descent(40/49): loss=0.45122144574577844\n",
      "Gradient Descent(41/49): loss=0.4512214457457781\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.45122144574577794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=0.45122144574577794\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47244379738909004\n",
      "Gradient Descent(2/49): loss=0.46007381803705427\n",
      "Gradient Descent(3/49): loss=0.4545209343059244\n",
      "Gradient Descent(4/49): loss=0.4520282447990197\n",
      "Gradient Descent(5/49): loss=0.4509092764793701\n",
      "Gradient Descent(6/49): loss=0.45040697160067944\n",
      "Gradient Descent(7/49): loss=0.4501814869406352\n",
      "Gradient Descent(8/49): loss=0.45008026687674124\n",
      "Gradient Descent(9/49): loss=0.45003482919005927\n",
      "Gradient Descent(10/49): loss=0.45001443221250775\n",
      "Gradient Descent(11/49): loss=0.45000527600928475\n",
      "Gradient Descent(12/49): loss=0.45000116578965804\n",
      "Gradient Descent(13/49): loss=0.44999932071206744\n",
      "Gradient Descent(14/49): loss=0.44999849245673734\n",
      "Gradient Descent(15/49): loss=0.44999812065291955\n",
      "Gradient Descent(16/49): loss=0.44999795375018575\n",
      "Gradient Descent(17/49): loss=0.44999787882754844\n",
      "Gradient Descent(18/49): loss=0.44999784519477665\n",
      "Gradient Descent(19/49): loss=0.44999783009702526\n",
      "Gradient Descent(20/49): loss=0.44999782331964483\n",
      "Gradient Descent(21/49): loss=0.44999782027727886\n",
      "Gradient Descent(22/49): loss=0.44999781891156065\n",
      "Gradient Descent(23/49): loss=0.4499978182984897\n",
      "Gradient Descent(24/49): loss=0.4499978180232821\n",
      "Gradient Descent(25/49): loss=0.4499978178997415\n",
      "Gradient Descent(26/49): loss=0.44999781784428416\n",
      "Gradient Descent(27/49): loss=0.44999781781938936\n",
      "Gradient Descent(28/49): loss=0.44999781780821396\n",
      "Gradient Descent(29/49): loss=0.4499978178031975\n",
      "Gradient Descent(30/49): loss=0.44999781780094544\n",
      "Gradient Descent(31/49): loss=0.44999781779993464\n",
      "Gradient Descent(32/49): loss=0.44999781779948095\n",
      "Gradient Descent(33/49): loss=0.4499978177992771\n",
      "Gradient Descent(34/49): loss=0.44999781779918563\n",
      "Gradient Descent(35/49): loss=0.44999781779914444\n",
      "Gradient Descent(36/49): loss=0.4499978177991261\n",
      "Gradient Descent(37/49): loss=0.4499978177991179\n",
      "Gradient Descent(38/49): loss=0.449997817799114\n",
      "Gradient Descent(39/49): loss=0.44999781779911247\n",
      "Gradient Descent(40/49): loss=0.4499978177991119\n",
      "Gradient Descent(41/49): loss=0.44999781779911135\n",
      "Gradient Descent(42/49): loss=0.4499978177991113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.4499978177991112\n",
      "Gradient Descent(46/49): loss=0.4499978177991112\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.4499978177991113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47276260995200003\n",
      "Gradient Descent(2/49): loss=0.4605357455594535\n",
      "Gradient Descent(3/49): loss=0.45504710613363836\n",
      "Gradient Descent(4/49): loss=0.4525832558953903\n",
      "Gradient Descent(5/49): loss=0.4514772335234407\n",
      "Gradient Descent(6/49): loss=0.45098074008067246\n",
      "Gradient Descent(7/49): loss=0.4507578641742139\n",
      "Gradient Descent(8/49): loss=0.4506578151798046\n",
      "Gradient Descent(9/49): loss=0.4506129031862142\n",
      "Gradient Descent(10/49): loss=0.45059274219229156\n",
      "Gradient Descent(11/49): loss=0.45058369192211967\n",
      "Gradient Descent(12/49): loss=0.45057962925583933\n",
      "Gradient Descent(13/49): loss=0.4505778055249464\n",
      "Gradient Descent(14/49): loss=0.45057698685214853\n",
      "Gradient Descent(15/49): loss=0.4505766193499295\n",
      "Gradient Descent(16/49): loss=0.45057645437818317\n",
      "Gradient Descent(17/49): loss=0.4505763803223664\n",
      "Gradient Descent(18/49): loss=0.45057634707871047\n",
      "Gradient Descent(19/49): loss=0.45057633215563303\n",
      "Gradient Descent(20/49): loss=0.4505763254566638\n",
      "Gradient Descent(21/49): loss=0.4505763224494962\n",
      "Gradient Descent(22/49): loss=0.45057632109957885\n",
      "Gradient Descent(23/49): loss=0.45057632049360097\n",
      "Gradient Descent(24/49): loss=0.4505763202215772\n",
      "Gradient Descent(25/49): loss=0.45057632009946624\n",
      "Gradient Descent(26/49): loss=0.4505763200446503\n",
      "Gradient Descent(27/49): loss=0.4505763200200435\n",
      "Gradient Descent(28/49): loss=0.45057632000899767\n",
      "Gradient Descent(29/49): loss=0.45057632000403897\n",
      "Gradient Descent(30/49): loss=0.45057632000181314\n",
      "Gradient Descent(31/49): loss=0.450576320000814\n",
      "Gradient Descent(32/49): loss=0.45057632000036535\n",
      "Gradient Descent(33/49): loss=0.45057632000016407\n",
      "Gradient Descent(34/49): loss=0.4505763200000737\n",
      "Gradient Descent(35/49): loss=0.45057632000003306\n",
      "Gradient Descent(36/49): loss=0.4505763200000148\n",
      "Gradient Descent(37/49): loss=0.45057632000000675\n",
      "Gradient Descent(38/49): loss=0.45057632000000286\n",
      "Gradient Descent(39/49): loss=0.45057632000000125\n",
      "Gradient Descent(40/49): loss=0.4505763200000007\n",
      "Gradient Descent(41/49): loss=0.4505763200000003\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.45057632\n",
      "Gradient Descent(44/49): loss=0.4505763200000001\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.4505763199999998\n",
      "Gradient Descent(48/49): loss=0.45057632000000014\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47254037821567996\n",
      "Gradient Descent(2/49): loss=0.46021375399669817\n",
      "Gradient Descent(3/49): loss=0.4546803323847975\n",
      "Gradient Descent(4/49): loss=0.4521963794232158\n",
      "Gradient Descent(5/49): loss=0.4510813329387615\n",
      "Gradient Descent(6/49): loss=0.45058078857188977\n",
      "Gradient Descent(7/49): loss=0.4503560942056015\n",
      "Gradient Descent(8/49): loss=0.45025522890457464\n",
      "Gradient Descent(9/49): loss=0.4502099504709435\n",
      "Gradient Descent(10/49): loss=0.4501896249820865\n",
      "Gradient Descent(11/49): loss=0.4501805008701388\n",
      "Gradient Descent(12/49): loss=0.4501764050562852\n",
      "Gradient Descent(13/49): loss=0.4501745664454464\n",
      "Gradient Descent(14/49): loss=0.450173741093041\n",
      "Gradient Descent(15/49): loss=0.4501733705923461\n",
      "Gradient Descent(16/49): loss=0.45017320427458424\n",
      "Gradient Descent(17/49): loss=0.45017312961454076\n",
      "Gradient Descent(18/49): loss=0.4501730960996474\n",
      "Gradient Descent(19/49): loss=0.4501730810548115\n",
      "Gradient Descent(20/49): loss=0.4501730743011849\n",
      "Gradient Descent(21/49): loss=0.4501730712694818\n",
      "Gradient Descent(22/49): loss=0.4501730699085505\n",
      "Gradient Descent(23/49): loss=0.45017306929762835\n",
      "Gradient Descent(24/49): loss=0.45017306902338516\n",
      "Gradient Descent(25/49): loss=0.45017306890027786\n",
      "Gradient Descent(26/49): loss=0.45017306884501473\n",
      "Gradient Descent(27/49): loss=0.45017306882020713\n",
      "Gradient Descent(28/49): loss=0.45017306880907093\n",
      "Gradient Descent(29/49): loss=0.45017306880407204\n",
      "Gradient Descent(30/49): loss=0.4501730688018278\n",
      "Gradient Descent(31/49): loss=0.4501730688008206\n",
      "Gradient Descent(32/49): loss=0.4501730688003685\n",
      "Gradient Descent(33/49): loss=0.45017306880016533\n",
      "Gradient Descent(34/49): loss=0.4501730688000742\n",
      "Gradient Descent(35/49): loss=0.4501730688000333\n",
      "Gradient Descent(36/49): loss=0.45017306880001484\n",
      "Gradient Descent(37/49): loss=0.45017306880000674\n",
      "Gradient Descent(38/49): loss=0.4501730688000032\n",
      "Gradient Descent(39/49): loss=0.4501730688000014\n",
      "Gradient Descent(40/49): loss=0.45017306880000063\n",
      "Gradient Descent(41/49): loss=0.45017306880000046\n",
      "Gradient Descent(42/49): loss=0.4501730688000001\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.45017306879999985\n",
      "Gradient Descent(46/49): loss=0.45017306880000024\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4751229373303466\n",
      "Gradient Descent(2/49): loss=0.4629331766222172\n",
      "Gradient Descent(3/49): loss=0.4569601938752331\n",
      "Gradient Descent(4/49): loss=0.454033432329211\n",
      "Gradient Descent(5/49): loss=0.45259931917166\n",
      "Gradient Descent(6/49): loss=0.45189660372445994\n",
      "Gradient Descent(7/49): loss=0.45155227315533214\n",
      "Gradient Descent(8/49): loss=0.45138355117645923\n",
      "Gradient Descent(9/49): loss=0.4513008774068118\n",
      "Gradient Descent(10/49): loss=0.4512603672596843\n",
      "Gradient Descent(11/49): loss=0.45124051728759196\n",
      "Gradient Descent(12/49): loss=0.4512307908012668\n",
      "Gradient Descent(13/49): loss=0.45122602482296736\n",
      "Gradient Descent(14/49): loss=0.45122368949360075\n",
      "Gradient Descent(15/49): loss=0.4512225451822111\n",
      "Gradient Descent(16/49): loss=0.4512219844696301\n",
      "Gradient Descent(17/49): loss=0.4512217097204656\n",
      "Gradient Descent(18/49): loss=0.4512215750933745\n",
      "Gradient Descent(19/49): loss=0.4512215091261005\n",
      "Gradient Descent(20/49): loss=0.45122147680213576\n",
      "Gradient Descent(21/49): loss=0.45122146096339333\n",
      "Gradient Descent(22/49): loss=0.4512214532024093\n",
      "Gradient Descent(23/49): loss=0.451221449399527\n",
      "Gradient Descent(24/49): loss=0.451221447536115\n",
      "Gradient Descent(25/49): loss=0.45122144662304314\n",
      "Gradient Descent(26/49): loss=0.4512214461756378\n",
      "Gradient Descent(27/49): loss=0.45122144595640934\n",
      "Gradient Descent(28/49): loss=0.4512214458489872\n",
      "Gradient Descent(29/49): loss=0.45122144579635054\n",
      "Gradient Descent(30/49): loss=0.45122144577055806\n",
      "Gradient Descent(31/49): loss=0.4512214457579202\n",
      "Gradient Descent(32/49): loss=0.4512214457517276\n",
      "Gradient Descent(33/49): loss=0.45122144574869305\n",
      "Gradient Descent(34/49): loss=0.45122144574720646\n",
      "Gradient Descent(35/49): loss=0.4512214457464777\n",
      "Gradient Descent(36/49): loss=0.4512214457461208\n",
      "Gradient Descent(37/49): loss=0.45122144574594575\n",
      "Gradient Descent(38/49): loss=0.4512214457458601\n",
      "Gradient Descent(39/49): loss=0.4512214457458181\n",
      "Gradient Descent(40/49): loss=0.4512214457457976\n",
      "Gradient Descent(41/49): loss=0.45122144574578743\n",
      "Gradient Descent(42/49): loss=0.4512214457457824\n",
      "Gradient Descent(43/49): loss=0.45122144574578016\n",
      "Gradient Descent(44/49): loss=0.451221445745779\n",
      "Gradient Descent(45/49): loss=0.4512214457457782\n",
      "Gradient Descent(46/49): loss=0.45122144574577794\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.45122144574577794\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47449888707754684\n",
      "Gradient Descent(2/49): loss=0.4620033417455451\n",
      "Gradient Descent(3/49): loss=0.45588052453286304\n",
      "Gradient Descent(4/49): loss=0.45288034409865\n",
      "Gradient Descent(5/49): loss=0.4514102556858849\n",
      "Gradient Descent(6/49): loss=0.45068991236363026\n",
      "Gradient Descent(7/49): loss=0.45033694413572545\n",
      "Gradient Descent(8/49): loss=0.4501639897040522\n",
      "Gradient Descent(9/49): loss=0.4500792420325322\n",
      "Gradient Descent(10/49): loss=0.4500377156734874\n",
      "Gradient Descent(11/49): loss=0.45001736775755546\n",
      "Gradient Descent(12/49): loss=0.45000739727874894\n",
      "Gradient Descent(13/49): loss=0.4500025117441336\n",
      "Gradient Descent(14/49): loss=0.4500001178321722\n",
      "Gradient Descent(15/49): loss=0.4499989448153111\n",
      "Gradient Descent(16/49): loss=0.4499983700370491\n",
      "Gradient Descent(17/49): loss=0.44999808839570077\n",
      "Gradient Descent(18/49): loss=0.44999795039143986\n",
      "Gradient Descent(19/49): loss=0.44999788276935226\n",
      "Gradient Descent(20/49): loss=0.4499978496345293\n",
      "Gradient Descent(21/49): loss=0.44999783339846616\n",
      "Gradient Descent(22/49): loss=0.4499978254427951\n",
      "Gradient Descent(23/49): loss=0.44999782154451623\n",
      "Gradient Descent(24/49): loss=0.4499978196343598\n",
      "Gradient Descent(25/49): loss=0.44999781869838285\n",
      "Gradient Descent(26/49): loss=0.4499978182397543\n",
      "Gradient Descent(27/49): loss=0.44999781801502614\n",
      "Gradient Descent(28/49): loss=0.4499978179049095\n",
      "Gradient Descent(29/49): loss=0.44999781785095233\n",
      "Gradient Descent(30/49): loss=0.44999781782451326\n",
      "Gradient Descent(31/49): loss=0.4499978178115584\n",
      "Gradient Descent(32/49): loss=0.44999781780521037\n",
      "Gradient Descent(33/49): loss=0.4499978178020997\n",
      "Gradient Descent(34/49): loss=0.44999781780057535\n",
      "Gradient Descent(35/49): loss=0.44999781779982856\n",
      "Gradient Descent(36/49): loss=0.44999781779946263\n",
      "Gradient Descent(37/49): loss=0.44999781779928333\n",
      "Gradient Descent(38/49): loss=0.44999781779919557\n",
      "Gradient Descent(39/49): loss=0.4499978177991526\n",
      "Gradient Descent(40/49): loss=0.4499978177991313\n",
      "Gradient Descent(41/49): loss=0.44999781779912124\n",
      "Gradient Descent(42/49): loss=0.449997817799116\n",
      "Gradient Descent(43/49): loss=0.44999781779911363\n",
      "Gradient Descent(44/49): loss=0.44999781779911224\n",
      "Gradient Descent(45/49): loss=0.44999781779911174\n",
      "Gradient Descent(46/49): loss=0.4499978177991113\n",
      "Gradient Descent(47/49): loss=0.4499978177991112\n",
      "Gradient Descent(48/49): loss=0.4499978177991112\n",
      "Gradient Descent(49/49): loss=0.4499978177991113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4747939231999999\n",
      "Gradient Descent(2/49): loss=0.46244294556799903\n",
      "Gradient Descent(3/49): loss=0.45639096652831923\n",
      "Gradient Descent(4/49): loss=0.45342549679887606\n",
      "Gradient Descent(5/49): loss=0.4519724166314495\n",
      "Gradient Descent(6/49): loss=0.45126040734941014\n",
      "Gradient Descent(7/49): loss=0.45091152280121116\n",
      "Gradient Descent(8/49): loss=0.4507405693725935\n",
      "Gradient Descent(9/49): loss=0.4506568021925707\n",
      "Gradient Descent(10/49): loss=0.4506157562743598\n",
      "Gradient Descent(11/49): loss=0.45059564377443617\n",
      "Gradient Descent(12/49): loss=0.4505857886494738\n",
      "Gradient Descent(13/49): loss=0.45058095963824224\n",
      "Gradient Descent(14/49): loss=0.4505785934227389\n",
      "Gradient Descent(15/49): loss=0.45057743397714195\n",
      "Gradient Descent(16/49): loss=0.4505768658487997\n",
      "Gradient Descent(17/49): loss=0.4505765874659119\n",
      "Gradient Descent(18/49): loss=0.4505764510582968\n",
      "Gradient Descent(19/49): loss=0.45057638421856544\n",
      "Gradient Descent(20/49): loss=0.45057635146709696\n",
      "Gradient Descent(21/49): loss=0.4505763354188774\n",
      "Gradient Descent(22/49): loss=0.45057632755524984\n",
      "Gradient Descent(23/49): loss=0.4505763237020724\n",
      "Gradient Descent(24/49): loss=0.4505763218140156\n",
      "Gradient Descent(25/49): loss=0.4505763208888675\n",
      "Gradient Descent(26/49): loss=0.450576320435545\n",
      "Gradient Descent(27/49): loss=0.4505763202134172\n",
      "Gradient Descent(28/49): loss=0.4505763201045744\n",
      "Gradient Descent(29/49): loss=0.4505763200512414\n",
      "Gradient Descent(30/49): loss=0.4505763200251083\n",
      "Gradient Descent(31/49): loss=0.45057632001230274\n",
      "Gradient Descent(32/49): loss=0.4505763200060286\n",
      "Gradient Descent(33/49): loss=0.4505763200029539\n",
      "Gradient Descent(34/49): loss=0.4505763200014475\n",
      "Gradient Descent(35/49): loss=0.45057632000070924\n",
      "Gradient Descent(36/49): loss=0.4505763200003476\n",
      "Gradient Descent(37/49): loss=0.4505763200001703\n",
      "Gradient Descent(38/49): loss=0.45057632000008335\n",
      "Gradient Descent(39/49): loss=0.45057632000004083\n",
      "Gradient Descent(40/49): loss=0.45057632000002\n",
      "Gradient Descent(41/49): loss=0.45057632000000986\n",
      "Gradient Descent(42/49): loss=0.45057632000000475\n",
      "Gradient Descent(43/49): loss=0.4505763200000023\n",
      "Gradient Descent(44/49): loss=0.450576320000001\n",
      "Gradient Descent(45/49): loss=0.4505763200000007\n",
      "Gradient Descent(46/49): loss=0.45057632000000014\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.4505763200000001\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47458826508799995\n",
      "Gradient Descent(2/49): loss=0.46213651498111874\n",
      "Gradient Descent(3/49): loss=0.45603515742874734\n",
      "Gradient Descent(4/49): loss=0.4530454922280867\n",
      "Gradient Descent(5/49): loss=0.4515805562797624\n",
      "Gradient Descent(6/49): loss=0.45086273766508367\n",
      "Gradient Descent(7/49): loss=0.450511006543891\n",
      "Gradient Descent(8/49): loss=0.4503386582945067\n",
      "Gradient Descent(9/49): loss=0.45025420765230845\n",
      "Gradient Descent(10/49): loss=0.4502128268376309\n",
      "Gradient Descent(11/49): loss=0.4501925502384391\n",
      "Gradient Descent(12/49): loss=0.45018261470483517\n",
      "Gradient Descent(13/49): loss=0.4501777462933692\n",
      "Gradient Descent(14/49): loss=0.4501753607717508\n",
      "Gradient Descent(15/49): loss=0.45017419186615815\n",
      "Gradient Descent(16/49): loss=0.45017361910241743\n",
      "Gradient Descent(17/49): loss=0.4501733384481847\n",
      "Gradient Descent(18/49): loss=0.45017320092761026\n",
      "Gradient Descent(19/49): loss=0.4501731335425291\n",
      "Gradient Descent(20/49): loss=0.45017310052383924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=0.45017308434468134\n",
      "Gradient Descent(22/49): loss=0.4501730764168937\n",
      "Gradient Descent(23/49): loss=0.450173072532278\n",
      "Gradient Descent(24/49): loss=0.4501730706288161\n",
      "Gradient Descent(25/49): loss=0.4501730696961199\n",
      "Gradient Descent(26/49): loss=0.4501730692390987\n",
      "Gradient Descent(27/49): loss=0.4501730690151585\n",
      "Gradient Descent(28/49): loss=0.4501730689054276\n",
      "Gradient Descent(29/49): loss=0.45017306885165936\n",
      "Gradient Descent(30/49): loss=0.45017306882531327\n",
      "Gradient Descent(31/49): loss=0.4501730688124034\n",
      "Gradient Descent(32/49): loss=0.4501730688060778\n",
      "Gradient Descent(33/49): loss=0.4501730688029779\n",
      "Gradient Descent(34/49): loss=0.4501730688014591\n",
      "Gradient Descent(35/49): loss=0.4501730688007151\n",
      "Gradient Descent(36/49): loss=0.4501730688003506\n",
      "Gradient Descent(37/49): loss=0.4501730688001716\n",
      "Gradient Descent(38/49): loss=0.45017306880008406\n",
      "Gradient Descent(39/49): loss=0.4501730688000412\n",
      "Gradient Descent(40/49): loss=0.45017306880002\n",
      "Gradient Descent(41/49): loss=0.45017306880000996\n",
      "Gradient Descent(42/49): loss=0.45017306880000496\n",
      "Gradient Descent(43/49): loss=0.4501730688000024\n",
      "Gradient Descent(44/49): loss=0.45017306880000124\n",
      "Gradient Descent(45/49): loss=0.4501730688000006\n",
      "Gradient Descent(46/49): loss=0.45017306880000024\n",
      "Gradient Descent(47/49): loss=0.4501730688000001\n",
      "Gradient Descent(48/49): loss=0.4501730688000001\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47721553730785293\n",
      "Gradient Descent(2/49): loss=0.4650736971392066\n",
      "Gradient Descent(3/49): loss=0.45860331051333697\n",
      "Gradient Descent(4/49): loss=0.45515524148041026\n",
      "Gradient Descent(5/49): loss=0.4533177654927633\n",
      "Gradient Descent(6/49): loss=0.4523385745389463\n",
      "Gradient Descent(7/49): loss=0.4518167636796573\n",
      "Gradient Descent(8/49): loss=0.45153869067274244\n",
      "Gradient Descent(9/49): loss=0.45139050556735727\n",
      "Gradient Descent(10/49): loss=0.4513115377246975\n",
      "Gradient Descent(11/49): loss=0.45126945576134403\n",
      "Gradient Descent(12/49): loss=0.45124703028307306\n",
      "Gradient Descent(13/49): loss=0.4512350797457026\n",
      "Gradient Descent(14/49): loss=0.45122871130433756\n",
      "Gradient Descent(15/49): loss=0.4512253175619342\n",
      "Gradient Descent(16/49): loss=0.45122350903660763\n",
      "Gradient Descent(17/49): loss=0.451222545273461\n",
      "Gradient Descent(18/49): loss=0.4512220316840802\n",
      "Gradient Descent(19/49): loss=0.45122175799229913\n",
      "Gradient Descent(20/49): loss=0.45122161214194906\n",
      "Gradient Descent(21/49): loss=0.4512215344182974\n",
      "Gradient Descent(22/49): loss=0.4512214929993635\n",
      "Gradient Descent(23/49): loss=0.45122147092721365\n",
      "Gradient Descent(24/49): loss=0.45122145916496476\n",
      "Gradient Descent(25/49): loss=0.4512214528968626\n",
      "Gradient Descent(26/49): loss=0.45122144955659077\n",
      "Gradient Descent(27/49): loss=0.4512214477765602\n",
      "Gradient Descent(28/49): loss=0.4512214468279816\n",
      "Gradient Descent(29/49): loss=0.45122144632248423\n",
      "Gradient Descent(30/49): loss=0.45122144605310466\n",
      "Gradient Descent(31/49): loss=0.4512214459095522\n",
      "Gradient Descent(32/49): loss=0.4512214458330532\n",
      "Gradient Descent(33/49): loss=0.4512214457922868\n",
      "Gradient Descent(34/49): loss=0.45122144577056233\n",
      "Gradient Descent(35/49): loss=0.4512214457589854\n",
      "Gradient Descent(36/49): loss=0.45122144575281614\n",
      "Gradient Descent(37/49): loss=0.4512214457495286\n",
      "Gradient Descent(38/49): loss=0.4512214457477767\n",
      "Gradient Descent(39/49): loss=0.4512214457468431\n",
      "Gradient Descent(40/49): loss=0.45122144574634554\n",
      "Gradient Descent(41/49): loss=0.4512214457460802\n",
      "Gradient Descent(42/49): loss=0.4512214457459391\n",
      "Gradient Descent(43/49): loss=0.45122144574586354\n",
      "Gradient Descent(44/49): loss=0.4512214457458235\n",
      "Gradient Descent(45/49): loss=0.4512214457458022\n",
      "Gradient Descent(46/49): loss=0.45122144574579076\n",
      "Gradient Descent(47/49): loss=0.4512214457457847\n",
      "Gradient Descent(48/49): loss=0.45122144574578144\n",
      "Gradient Descent(49/49): loss=0.45122144574577977\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47664398069396485\n",
      "Gradient Descent(2/49): loss=0.4641975580057779\n",
      "Gradient Descent(3/49): loss=0.45756485935524455\n",
      "Gradient Descent(4/49): loss=0.4540302942443745\n",
      "Gradient Descent(5/49): loss=0.452146724496792\n",
      "Gradient Descent(6/49): loss=0.45114297017830524\n",
      "Gradient Descent(7/49): loss=0.45060806950198384\n",
      "Gradient Descent(8/49): loss=0.45032302093157195\n",
      "Gradient Descent(9/49): loss=0.4501711185483993\n",
      "Gradient Descent(10/49): loss=0.45009016976840693\n",
      "Gradient Descent(11/49): loss=0.45004703216354874\n",
      "Gradient Descent(12/49): loss=0.45002404413392\n",
      "Gradient Descent(13/49): loss=0.4500117938129308\n",
      "Gradient Descent(14/49): loss=0.45000526561687554\n",
      "Gradient Descent(15/49): loss=0.4500017867411977\n",
      "Gradient Descent(16/49): loss=0.44999993284834905\n",
      "Gradient Descent(17/49): loss=0.44999894490885006\n",
      "Gradient Descent(18/49): loss=0.44999841843589095\n",
      "Gradient Descent(19/49): loss=0.4499981378784511\n",
      "Gradient Descent(20/49): loss=0.44999798836939137\n",
      "Gradient Descent(21/49): loss=0.44999790869601364\n",
      "Gradient Descent(22/49): loss=0.44999786623807025\n",
      "Gradient Descent(23/49): loss=0.4499978436122325\n",
      "Gradient Descent(24/49): loss=0.44999783155492373\n",
      "Gradient Descent(25/49): loss=0.44999782512958364\n",
      "Gradient Descent(26/49): loss=0.44999782170551983\n",
      "Gradient Descent(27/49): loss=0.4499978198808362\n",
      "Gradient Descent(28/49): loss=0.44999781890846247\n",
      "Gradient Descent(29/49): loss=0.4499978183902843\n",
      "Gradient Descent(30/49): loss=0.4499978181141473\n",
      "Gradient Descent(31/49): loss=0.44999781796699384\n",
      "Gradient Descent(32/49): loss=0.44999781788857585\n",
      "Gradient Descent(33/49): loss=0.449997817846787\n",
      "Gradient Descent(34/49): loss=0.44999781782451753\n",
      "Gradient Descent(35/49): loss=0.4499978178126501\n",
      "Gradient Descent(36/49): loss=0.44999781780632603\n",
      "Gradient Descent(37/49): loss=0.449997817802956\n",
      "Gradient Descent(38/49): loss=0.4499978178011599\n",
      "Gradient Descent(39/49): loss=0.4499978178002029\n",
      "Gradient Descent(40/49): loss=0.449997817799693\n",
      "Gradient Descent(41/49): loss=0.44999781779942105\n",
      "Gradient Descent(42/49): loss=0.4499978177992763\n",
      "Gradient Descent(43/49): loss=0.4499978177991993\n",
      "Gradient Descent(44/49): loss=0.44999781779915815\n",
      "Gradient Descent(45/49): loss=0.4499978177991363\n",
      "Gradient Descent(46/49): loss=0.4499978177991244\n",
      "Gradient Descent(47/49): loss=0.4499978177991183\n",
      "Gradient Descent(48/49): loss=0.449997817799115\n",
      "Gradient Descent(49/49): loss=0.44999781779911324\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4769141990720001\n",
      "Gradient Descent(2/49): loss=0.4646117757574682\n",
      "Gradient Descent(3/49): loss=0.45805581437315473\n",
      "Gradient Descent(4/49): loss=0.4545621425514537\n",
      "Gradient Descent(5/49): loss=0.4527003648376703\n",
      "Gradient Descent(6/49): loss=0.4517082234939947\n",
      "Gradient Descent(7/49): loss=0.45117951137194967\n",
      "Gradient Descent(8/49): loss=0.45089776068211207\n",
      "Gradient Descent(9/49): loss=0.45074761573949745\n",
      "Gradient Descent(10/49): loss=0.45066760349957824\n",
      "Gradient Descent(11/49): loss=0.4506249649769252\n",
      "Gradient Descent(12/49): loss=0.4506022429082035\n",
      "Gradient Descent(13/49): loss=0.4505901343177816\n",
      "Gradient Descent(14/49): loss=0.4505836816499459\n",
      "Gradient Descent(15/49): loss=0.4505802430232562\n",
      "Gradient Descent(16/49): loss=0.45057841057909326\n",
      "Gradient Descent(17/49): loss=0.4505774340695987\n",
      "Gradient Descent(18/49): loss=0.450576913687689\n",
      "Gradient Descent(19/49): loss=0.45057663637616957\n",
      "Gradient Descent(20/49): loss=0.4505764885968607\n",
      "Gradient Descent(21/49): loss=0.4505764098452671\n",
      "Gradient Descent(22/49): loss=0.450576367878543\n",
      "Gradient Descent(23/49): loss=0.4505763455144753\n",
      "Gradient Descent(24/49): loss=0.450576333596664\n",
      "Gradient Descent(25/49): loss=0.45057632724566216\n",
      "Gradient Descent(26/49): loss=0.4505763238612134\n",
      "Gradient Descent(27/49): loss=0.4505763220576408\n",
      "Gradient Descent(28/49): loss=0.45057632109651663\n",
      "Gradient Descent(29/49): loss=0.4505763205843339\n",
      "Gradient Descent(30/49): loss=0.4505763203113915\n",
      "Gradient Descent(31/49): loss=0.45057632016594074\n",
      "Gradient Descent(32/49): loss=0.45057632008842974\n",
      "Gradient Descent(33/49): loss=0.4505763200471241\n",
      "Gradient Descent(34/49): loss=0.45057632002511255\n",
      "Gradient Descent(35/49): loss=0.4505763200133824\n",
      "Gradient Descent(36/49): loss=0.45057632000713127\n",
      "Gradient Descent(37/49): loss=0.4505763200038003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=0.45057632000202524\n",
      "Gradient Descent(39/49): loss=0.45057632000107917\n",
      "Gradient Descent(40/49): loss=0.4505763200005753\n",
      "Gradient Descent(41/49): loss=0.4505763200003066\n",
      "Gradient Descent(42/49): loss=0.4505763200001633\n",
      "Gradient Descent(43/49): loss=0.4505763200000869\n",
      "Gradient Descent(44/49): loss=0.4505763200000464\n",
      "Gradient Descent(45/49): loss=0.4505763200000247\n",
      "Gradient Descent(46/49): loss=0.450576320000013\n",
      "Gradient Descent(47/49): loss=0.45057632000000697\n",
      "Gradient Descent(48/49): loss=0.4505763200000037\n",
      "Gradient Descent(49/49): loss=0.4505763200000019\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4767258404364801\n",
      "Gradient Descent(2/49): loss=0.4643230408050789\n",
      "Gradient Descent(3/49): loss=0.4577135888815056\n",
      "Gradient Descent(4/49): loss=0.45419141195143464\n",
      "Gradient Descent(5/49): loss=0.45231444386539943\n",
      "Gradient Descent(6/49): loss=0.45131420757235136\n",
      "Gradient Descent(7/49): loss=0.4507811816517859\n",
      "Gradient Descent(8/49): loss=0.45049713213871667\n",
      "Gradient Descent(9/49): loss=0.45034576215320204\n",
      "Gradient Descent(10/49): loss=0.4502650970879214\n",
      "Gradient Descent(11/49): loss=0.45022211067463325\n",
      "Gradient Descent(12/49): loss=0.45019920321499207\n",
      "Gradient Descent(13/49): loss=0.4501869958297493\n",
      "Gradient Descent(14/49): loss=0.4501804905141533\n",
      "Gradient Descent(15/49): loss=0.4501770238314723\n",
      "Gradient Descent(16/49): loss=0.45017517643627153\n",
      "Gradient Descent(17/49): loss=0.4501741919593689\n",
      "Gradient Descent(18/49): loss=0.45017366733162806\n",
      "Gradient Descent(19/49): loss=0.4501733877575046\n",
      "Gradient Descent(20/49): loss=0.4501732387724541\n",
      "Gradient Descent(21/49): loss=0.4501731593783209\n",
      "Gradient Descent(22/49): loss=0.4501731170691871\n",
      "Gradient Descent(23/49): loss=0.4501730945226498\n",
      "Gradient Descent(24/49): loss=0.4501730825075999\n",
      "Gradient Descent(25/49): loss=0.45017307610478025\n",
      "Gradient Descent(26/49): loss=0.4501730726927173\n",
      "Gradient Descent(27/49): loss=0.4501730708744291\n",
      "Gradient Descent(28/49): loss=0.45017306990546324\n",
      "Gradient Descent(29/49): loss=0.45017306938910145\n",
      "Gradient Descent(30/49): loss=0.450173069113932\n",
      "Gradient Descent(31/49): loss=0.4501730689672944\n",
      "Gradient Descent(32/49): loss=0.4501730688891513\n",
      "Gradient Descent(33/49): loss=0.45017306884750863\n",
      "Gradient Descent(34/49): loss=0.45017306882531755\n",
      "Gradient Descent(35/49): loss=0.4501730688134918\n",
      "Gradient Descent(36/49): loss=0.4501730688071896\n",
      "Gradient Descent(37/49): loss=0.4501730688038313\n",
      "Gradient Descent(38/49): loss=0.4501730688020418\n",
      "Gradient Descent(39/49): loss=0.45017306880108793\n",
      "Gradient Descent(40/49): loss=0.45017306880057967\n",
      "Gradient Descent(41/49): loss=0.45017306880030905\n",
      "Gradient Descent(42/49): loss=0.4501730688001647\n",
      "Gradient Descent(43/49): loss=0.45017306880008784\n",
      "Gradient Descent(44/49): loss=0.4501730688000467\n",
      "Gradient Descent(45/49): loss=0.45017306880002467\n",
      "Gradient Descent(46/49): loss=0.4501730688000132\n",
      "Gradient Descent(47/49): loss=0.4501730688000071\n",
      "Gradient Descent(48/49): loss=0.4501730688000039\n",
      "Gradient Descent(49/49): loss=0.45017306880000213\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47939593868301655\n",
      "Gradient Descent(2/49): loss=0.46749503286632726\n",
      "Gradient Descent(3/49): loss=0.46062106966660693\n",
      "Gradient Descent(4/49): loss=0.4566506685224479\n",
      "Gradient Descent(5/49): loss=0.45435736482158295\n",
      "Gradient Descent(6/49): loss=0.4530327526039627\n",
      "Gradient Descent(7/49): loss=0.45226765658706536\n",
      "Gradient Descent(8/49): loss=0.4518257371277056\n",
      "Gradient Descent(9/49): loss=0.45157048444797915\n",
      "Gradient Descent(10/49): loss=0.45142305050016945\n",
      "Gradient Descent(11/49): loss=0.4513378926519143\n",
      "Gradient Descent(12/49): loss=0.4512887054787626\n",
      "Gradient Descent(13/49): loss=0.4512602949675497\n",
      "Gradient Descent(14/49): loss=0.45124388505627305\n",
      "Gradient Descent(15/49): loss=0.45123440669151976\n",
      "Gradient Descent(16/49): loss=0.4512289319880384\n",
      "Gradient Descent(17/49): loss=0.45122576979930745\n",
      "Gradient Descent(18/49): loss=0.45122394331909654\n",
      "Gradient Descent(19/49): loss=0.45122288834412677\n",
      "Gradient Descent(20/49): loss=0.451222278990584\n",
      "Gradient Descent(21/49): loss=0.45122192702797786\n",
      "Gradient Descent(22/49): loss=0.4512217237343768\n",
      "Gradient Descent(23/49): loss=0.4512216063119924\n",
      "Gradient Descent(24/49): loss=0.4512215384888234\n",
      "Gradient Descent(25/49): loss=0.4512214993141611\n",
      "Gradient Descent(26/49): loss=0.4512214766868759\n",
      "Gradient Descent(27/49): loss=0.45122146361735604\n",
      "Gradient Descent(28/49): loss=0.4512214560684015\n",
      "Gradient Descent(29/49): loss=0.45122145170812517\n",
      "Gradient Descent(30/49): loss=0.4512214491896297\n",
      "Gradient Descent(31/49): loss=0.4512214477349467\n",
      "Gradient Descent(32/49): loss=0.4512214468947216\n",
      "Gradient Descent(33/49): loss=0.45122144640940776\n",
      "Gradient Descent(34/49): loss=0.45122144612909054\n",
      "Gradient Descent(35/49): loss=0.4512214459671791\n",
      "Gradient Descent(36/49): loss=0.4512214458736592\n",
      "Gradient Descent(37/49): loss=0.4512214458196421\n",
      "Gradient Descent(38/49): loss=0.45122144578844153\n",
      "Gradient Descent(39/49): loss=0.4512214457704206\n",
      "Gradient Descent(40/49): loss=0.4512214457600113\n",
      "Gradient Descent(41/49): loss=0.4512214457539992\n",
      "Gradient Descent(42/49): loss=0.4512214457505265\n",
      "Gradient Descent(43/49): loss=0.45122144574852063\n",
      "Gradient Descent(44/49): loss=0.4512214457473622\n",
      "Gradient Descent(45/49): loss=0.45122144574669293\n",
      "Gradient Descent(46/49): loss=0.45122144574630635\n",
      "Gradient Descent(47/49): loss=0.4512214457460832\n",
      "Gradient Descent(48/49): loss=0.4512214457459542\n",
      "Gradient Descent(49/49): loss=0.4512214457458797\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4788790782383448\n",
      "Gradient Descent(2/49): loss=0.4666796338288096\n",
      "Gradient Descent(3/49): loss=0.45963323473786455\n",
      "Gradient Descent(4/49): loss=0.45556323462293435\n",
      "Gradient Descent(5/49): loss=0.45321240255655154\n",
      "Gradient Descent(6/49): loss=0.4518545619550087\n",
      "Gradient Descent(7/49): loss=0.45107027322355764\n",
      "Gradient Descent(8/49): loss=0.4506172680522716\n",
      "Gradient Descent(9/49): loss=0.45035561226533655\n",
      "Gradient Descent(10/49): loss=0.450204479882803\n",
      "Gradient Descent(11/49): loss=0.4501171858186515\n",
      "Gradient Descent(12/49): loss=0.45006676476719765\n",
      "Gradient Descent(13/49): loss=0.45003764156787796\n",
      "Gradient Descent(14/49): loss=0.4500208200079511\n",
      "Gradient Descent(15/49): loss=0.45001110387493676\n",
      "Gradient Descent(16/49): loss=0.4500054918365079\n",
      "Gradient Descent(17/49): loss=0.45000225032311136\n",
      "Gradient Descent(18/49): loss=0.45000037802497383\n",
      "Gradient Descent(19/49): loss=0.4499992965855695\n",
      "Gradient Descent(20/49): loss=0.44999867194616955\n",
      "Gradient Descent(21/49): loss=0.449998311154452\n",
      "Gradient Descent(22/49): loss=0.4499981027611561\n",
      "Gradient Descent(23/49): loss=0.4499979823931882\n",
      "Gradient Descent(24/49): loss=0.44999791286865\n",
      "Gradient Descent(25/49): loss=0.4499978727112767\n",
      "Gradient Descent(26/49): loss=0.44999784951637817\n",
      "Gradient Descent(27/49): loss=0.44999783611900446\n",
      "Gradient Descent(28/49): loss=0.4499978283806814\n",
      "Gradient Descent(29/49): loss=0.44999782391102633\n",
      "Gradient Descent(30/49): loss=0.4499978213293531\n",
      "Gradient Descent(31/49): loss=0.449997819838179\n",
      "Gradient Descent(32/49): loss=0.44999781897687674\n",
      "Gradient Descent(33/49): loss=0.4499978184793886\n",
      "Gradient Descent(34/49): loss=0.44999781819203916\n",
      "Gradient Descent(35/49): loss=0.4499978180260664\n",
      "Gradient Descent(36/49): loss=0.4499978179302006\n",
      "Gradient Descent(37/49): loss=0.4499978178748285\n",
      "Gradient Descent(38/49): loss=0.4499978178428453\n",
      "Gradient Descent(39/49): loss=0.44999781782437215\n",
      "Gradient Descent(40/49): loss=0.4499978178137018\n",
      "Gradient Descent(41/49): loss=0.4499978178075387\n",
      "Gradient Descent(42/49): loss=0.449997817803979\n",
      "Gradient Descent(43/49): loss=0.4499978178019227\n",
      "Gradient Descent(44/49): loss=0.4499978178007351\n",
      "Gradient Descent(45/49): loss=0.449997817800049\n",
      "Gradient Descent(46/49): loss=0.4499978177996529\n",
      "Gradient Descent(47/49): loss=0.4499978177994244\n",
      "Gradient Descent(48/49): loss=0.449997817799292\n",
      "Gradient Descent(49/49): loss=0.4499978177992154\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4791234375680001\n",
      "Gradient Descent(2/49): loss=0.46706513510727643\n",
      "Gradient Descent(3/49): loss=0.4601002596059613\n",
      "Gradient Descent(4/49): loss=0.45607734751640244\n",
      "Gradient Descent(5/49): loss=0.45375371349347393\n",
      "Gradient Descent(6/49): loss=0.4524115824818304\n",
      "Gradient Descent(7/49): loss=0.45163636760950515\n",
      "Gradient Descent(8/49): loss=0.45118860349925016\n",
      "Gradient Descent(9/49): loss=0.4509299749491668\n",
      "Gradient Descent(10/49): loss=0.4507805910986387\n",
      "Gradient Descent(11/49): loss=0.4506943069865737\n",
      "Gradient Descent(12/49): loss=0.4506444692834448\n",
      "Gradient Descent(13/49): loss=0.45061568302611776\n",
      "Gradient Descent(14/49): loss=0.45059905608388556\n",
      "Gradient Descent(15/49): loss=0.45058945236205233\n",
      "Gradient Descent(16/49): loss=0.4505839052523216\n",
      "Gradient Descent(17/49): loss=0.4505807012417409\n",
      "Gradient Descent(18/49): loss=0.45057885060522956\n",
      "Gradient Descent(19/49): loss=0.45057778167758045\n",
      "Gradient Descent(20/49): loss=0.4505771642649705\n",
      "Gradient Descent(21/49): loss=0.4505768076474471\n",
      "Gradient Descent(22/49): loss=0.45057660166516533\n",
      "Gradient Descent(23/49): loss=0.45057648268979933\n",
      "Gradient Descent(24/49): loss=0.4505764139696283\n",
      "Gradient Descent(25/49): loss=0.45057637427685726\n",
      "Gradient Descent(26/49): loss=0.45057635135031265\n",
      "Gradient Descent(27/49): loss=0.4505763381079407\n",
      "Gradient Descent(28/49): loss=0.4505763304591463\n",
      "Gradient Descent(29/49): loss=0.45057632604120285\n",
      "Gradient Descent(30/49): loss=0.4505763234893988\n",
      "Gradient Descent(31/49): loss=0.4505763220154767\n",
      "Gradient Descent(32/49): loss=0.45057632116413965\n",
      "Gradient Descent(33/49): loss=0.4505763206724068\n",
      "Gradient Descent(34/49): loss=0.45057632038838247\n",
      "Gradient Descent(35/49): loss=0.45057632022432936\n",
      "Gradient Descent(36/49): loss=0.4505763201295727\n",
      "Gradient Descent(37/49): loss=0.45057632007484133\n",
      "Gradient Descent(38/49): loss=0.45057632004322823\n",
      "Gradient Descent(39/49): loss=0.4505763200249687\n",
      "Gradient Descent(40/49): loss=0.4505763200144218\n",
      "Gradient Descent(41/49): loss=0.45057632000833003\n",
      "Gradient Descent(42/49): loss=0.45057632000481135\n",
      "Gradient Descent(43/49): loss=0.450576320002779\n",
      "Gradient Descent(44/49): loss=0.45057632000160514\n",
      "Gradient Descent(45/49): loss=0.4505763200009271\n",
      "Gradient Descent(46/49): loss=0.4505763200005356\n",
      "Gradient Descent(47/49): loss=0.45057632000030934\n",
      "Gradient Descent(48/49): loss=0.4505763200001785\n",
      "Gradient Descent(49/49): loss=0.45057632000010306\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4789531042611203\n",
      "Gradient Descent(2/49): loss=0.46679641728234106\n",
      "Gradient Descent(3/49): loss=0.4597747148833992\n",
      "Gradient Descent(4/49): loss=0.45571897957777124\n",
      "Gradient Descent(5/49): loss=0.4533763868652407\n",
      "Gradient Descent(6/49): loss=0.4520233053144828\n",
      "Gradient Descent(7/49): loss=0.4512417654107653\n",
      "Gradient Descent(8/49): loss=0.45079034796237805\n",
      "Gradient Descent(9/49): loss=0.45052960924418944\n",
      "Gradient Descent(10/49): loss=0.45037900656056407\n",
      "Gradient Descent(11/49): loss=0.45029201845050176\n",
      "Gradient Descent(12/49): loss=0.45024177411812977\n",
      "Gradient Descent(13/49): loss=0.45021275299175206\n",
      "Gradient Descent(14/49): loss=0.45019599038915575\n",
      "Gradient Descent(15/49): loss=0.45018630830989637\n",
      "Gradient Descent(16/49): loss=0.4501807159409161\n",
      "Gradient Descent(17/49): loss=0.45017748578859323\n",
      "Gradient Descent(18/49): loss=0.4501756200526115\n",
      "Gradient Descent(19/49): loss=0.4501745424035083\n",
      "Gradient Descent(20/49): loss=0.4501739199533864\n",
      "Gradient Descent(21/49): loss=0.45017356042619594\n",
      "Gradient Descent(22/49): loss=0.45017335276329085\n",
      "Gradient Descent(23/49): loss=0.4501732328171966\n",
      "Gradient Descent(24/49): loss=0.45017316353633285\n",
      "Gradient Descent(25/49): loss=0.4501731235197058\n",
      "Gradient Descent(26/49): loss=0.45017310040610203\n",
      "Gradient Descent(27/49): loss=0.45017308705568443\n",
      "Gradient Descent(28/49): loss=0.4501730793444834\n",
      "Gradient Descent(29/49): loss=0.45017307489049363\n",
      "Gradient Descent(30/49): loss=0.45017307231786924\n",
      "Gradient Descent(31/49): loss=0.45017307083192143\n",
      "Gradient Descent(32/49): loss=0.45017306997363776\n",
      "Gradient Descent(33/49): loss=0.4501730694778931\n",
      "Gradient Descent(34/49): loss=0.4501730691915511\n",
      "Gradient Descent(35/49): loss=0.4501730690261599\n",
      "Gradient Descent(36/49): loss=0.45017306893062997\n",
      "Gradient Descent(37/49): loss=0.45017306887545194\n",
      "Gradient Descent(38/49): loss=0.45017306884358094\n",
      "Gradient Descent(39/49): loss=0.45017306882517244\n",
      "Gradient Descent(40/49): loss=0.4501730688145395\n",
      "Gradient Descent(41/49): loss=0.4501730688083981\n",
      "Gradient Descent(42/49): loss=0.4501730688048506\n",
      "Gradient Descent(43/49): loss=0.4501730688028015\n",
      "Gradient Descent(44/49): loss=0.4501730688016181\n",
      "Gradient Descent(45/49): loss=0.45017306880093466\n",
      "Gradient Descent(46/49): loss=0.45017306880053987\n",
      "Gradient Descent(47/49): loss=0.45017306880031177\n",
      "Gradient Descent(48/49): loss=0.45017306880018015\n",
      "Gradient Descent(49/49): loss=0.450173068800104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4816641414558379\n",
      "Gradient Descent(2/49): loss=0.47022073213842475\n",
      "Gradient Descent(3/49): loss=0.4630789003834282\n",
      "Gradient Descent(4/49): loss=0.45862168318513463\n",
      "Gradient Descent(5/49): loss=0.45583993393168\n",
      "Gradient Descent(6/49): loss=0.45410384422259925\n",
      "Gradient Descent(7/49): loss=0.4530203506351622\n",
      "Gradient Descent(8/49): loss=0.45234414228724246\n",
      "Gradient Descent(9/49): loss=0.4519221206573059\n",
      "Gradient Descent(10/49): loss=0.45165873695806236\n",
      "Gradient Descent(11/49): loss=0.4514943591913647\n",
      "Gradient Descent(12/49): loss=0.45139177102716854\n",
      "Gradient Descent(13/49): loss=0.4513277457538938\n",
      "Gradient Descent(14/49): loss=0.4512877875808431\n",
      "Gradient Descent(15/49): loss=0.451262849685042\n",
      "Gradient Descent(16/49): loss=0.4512472859442727\n",
      "Gradient Descent(17/49): loss=0.4512375726136584\n",
      "Gradient Descent(18/49): loss=0.45123151052402216\n",
      "Gradient Descent(19/49): loss=0.45122772717388004\n",
      "Gradient Descent(20/49): loss=0.45122536598505636\n",
      "Gradient Descent(21/49): loss=0.45122389236711147\n",
      "Gradient Descent(22/49): loss=0.45122297268215233\n",
      "Gradient Descent(23/49): loss=0.4512223987067691\n",
      "Gradient Descent(24/49): loss=0.4512220404887324\n",
      "Gradient Descent(25/49): loss=0.45122181692485563\n",
      "Gradient Descent(26/49): loss=0.45122167739864033\n",
      "Gradient Descent(27/49): loss=0.4512215903203295\n",
      "Gradient Descent(28/49): loss=0.4512215359747554\n",
      "Gradient Descent(29/49): loss=0.4512215020576826\n",
      "Gradient Descent(30/49): loss=0.45122148089003783\n",
      "Gradient Descent(31/49): loss=0.4512214676793102\n",
      "Gradient Descent(32/49): loss=0.4512214594344955\n",
      "Gradient Descent(33/49): loss=0.4512214542889065\n",
      "Gradient Descent(34/49): loss=0.4512214510775443\n",
      "Gradient Descent(35/49): loss=0.4512214490733333\n",
      "Gradient Descent(36/49): loss=0.45122144782250523\n",
      "Gradient Descent(37/49): loss=0.45122144704186334\n",
      "Gradient Descent(38/49): loss=0.45122144655466473\n",
      "Gradient Descent(39/49): loss=0.451221446250604\n",
      "Gradient Descent(40/49): loss=0.45122144606083997\n",
      "Gradient Descent(41/49): loss=0.4512214459424081\n",
      "Gradient Descent(42/49): loss=0.4512214458684948\n",
      "Gradient Descent(43/49): loss=0.45122144582236534\n",
      "Gradient Descent(44/49): loss=0.45122144579357615\n",
      "Gradient Descent(45/49): loss=0.45122144577560874\n",
      "Gradient Descent(46/49): loss=0.45122144576439543\n",
      "Gradient Descent(47/49): loss=0.4512214457573971\n",
      "Gradient Descent(48/49): loss=0.45122144575302925\n",
      "Gradient Descent(49/49): loss=0.45122144575030343\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48120417971068596\n",
      "Gradient Descent(2/49): loss=0.4694737082681241\n",
      "Gradient Descent(3/49): loss=0.46215272104082217\n",
      "Gradient Descent(4/49): loss=0.4575836929122638\n",
      "Gradient Descent(5/49): loss=0.45473216245723\n",
      "Gradient Descent(6/49): loss=0.45295252230024297\n",
      "Gradient Descent(7/49): loss=0.4518418488782676\n",
      "Gradient Descent(8/49): loss=0.45114867759561283\n",
      "Gradient Descent(9/49): loss=0.4507160693981079\n",
      "Gradient Descent(10/49): loss=0.45044607862204494\n",
      "Gradient Descent(11/49): loss=0.45027757737870405\n",
      "Gradient Descent(12/49): loss=0.4501724157527351\n",
      "Gradient Descent(13/49): loss=0.4501067843819679\n",
      "Gradient Descent(14/49): loss=0.4500658238434721\n",
      "Gradient Descent(15/49): loss=0.4500402603713969\n",
      "Gradient Descent(16/49): loss=0.4500243062084746\n",
      "Gradient Descent(17/49): loss=0.45001434921539474\n",
      "Gradient Descent(18/49): loss=0.4500081350560138\n",
      "Gradient Descent(19/49): loss=0.45000425679914413\n",
      "Gradient Descent(20/49): loss=0.4500018363790315\n",
      "Gradient Descent(21/49): loss=0.4500003257948394\n",
      "Gradient Descent(22/49): loss=0.4499993830392453\n",
      "Gradient Descent(23/49): loss=0.44999879466547904\n",
      "Gradient Descent(24/49): loss=0.44999842746141105\n",
      "Gradient Descent(25/49): loss=0.4499981982893526\n",
      "Gradient Descent(26/49): loss=0.4499980552630707\n",
      "Gradient Descent(27/49): loss=0.44999796600036857\n",
      "Gradient Descent(28/49): loss=0.44999791029151576\n",
      "Gradient Descent(29/49): loss=0.4499978755236209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=0.4499978538249775\n",
      "Gradient Descent(31/49): loss=0.44999784028285456\n",
      "Gradient Descent(32/49): loss=0.4499978318312154\n",
      "Gradient Descent(33/49): loss=0.4499978265565474\n",
      "Gradient Descent(34/49): loss=0.4499978232646271\n",
      "Gradient Descent(35/49): loss=0.4499978212101394\n",
      "Gradient Descent(36/49): loss=0.4499978199279341\n",
      "Gradient Descent(37/49): loss=0.4499978191277096\n",
      "Gradient Descent(38/49): loss=0.44999781862828936\n",
      "Gradient Descent(39/49): loss=0.44999781831660124\n",
      "Gradient Descent(40/49): loss=0.4499978181220766\n",
      "Gradient Descent(41/49): loss=0.44999781800067395\n",
      "Gradient Descent(42/49): loss=0.4499978179249063\n",
      "Gradient Descent(43/49): loss=0.4499978178776199\n",
      "Gradient Descent(44/49): loss=0.4499978178481086\n",
      "Gradient Descent(45/49): loss=0.4499978178296902\n",
      "Gradient Descent(46/49): loss=0.44999781781819564\n",
      "Gradient Descent(47/49): loss=0.44999781781102166\n",
      "Gradient Descent(48/49): loss=0.44999781780654446\n",
      "Gradient Descent(49/49): loss=0.4499978178037502\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.481421638688\n",
      "Gradient Descent(2/49): loss=0.46982688339318074\n",
      "Gradient Descent(3/49): loss=0.46259059661368235\n",
      "Gradient Descent(4/49): loss=0.45807443003459825\n",
      "Gradient Descent(5/49): loss=0.45525589047259224\n",
      "Gradient Descent(6/49): loss=0.45349683993194523\n",
      "Gradient Descent(7/49): loss=0.45239901648952713\n",
      "Gradient Descent(8/49): loss=0.45171386487911364\n",
      "Gradient Descent(9/49): loss=0.45128626175905495\n",
      "Gradient Descent(10/49): loss=0.45101939465182617\n",
      "Gradient Descent(11/49): loss=0.4508528428902047\n",
      "Gradient Descent(12/49): loss=0.4507488979357766\n",
      "Gradient Descent(13/49): loss=0.4506840258897183\n",
      "Gradient Descent(14/49): loss=0.45064353924577294\n",
      "Gradient Descent(15/49): loss=0.450618271531287\n",
      "Gradient Descent(16/49): loss=0.45060250195067614\n",
      "Gradient Descent(17/49): loss=0.4505926601554171\n",
      "Gradient Descent(18/49): loss=0.4505865178909957\n",
      "Gradient Descent(19/49): loss=0.4505826845037704\n",
      "Gradient Descent(20/49): loss=0.45058029208680306\n",
      "Gradient Descent(21/49): loss=0.4505787989793738\n",
      "Gradient Descent(22/49): loss=0.4505778671310273\n",
      "Gradient Descent(23/49): loss=0.4505772855644741\n",
      "Gradient Descent(24/49): loss=0.4505769226087882\n",
      "Gradient Descent(25/49): loss=0.4505766960881447\n",
      "Gradient Descent(26/49): loss=0.4505765547166111\n",
      "Gradient Descent(27/49): loss=0.4505764664866368\n",
      "Gradient Descent(28/49): loss=0.45057641142231025\n",
      "Gradient Descent(29/49): loss=0.4505763770566637\n",
      "Gradient Descent(30/49): loss=0.4505763556090639\n",
      "Gradient Descent(31/49): loss=0.4505763422236168\n",
      "Gradient Descent(32/49): loss=0.4505763338697592\n",
      "Gradient Descent(33/49): loss=0.45057632865611663\n",
      "Gradient Descent(34/49): loss=0.45057632540228243\n",
      "Gradient Descent(35/49): loss=0.4505763233715645\n",
      "Gradient Descent(36/49): loss=0.4505763221041934\n",
      "Gradient Descent(37/49): loss=0.45057632131322706\n",
      "Gradient Descent(38/49): loss=0.45057632081958504\n",
      "Gradient Descent(39/49): loss=0.4505763205115031\n",
      "Gradient Descent(40/49): loss=0.45057632031922906\n",
      "Gradient Descent(41/49): loss=0.45057632019923094\n",
      "Gradient Descent(42/49): loss=0.45057632012434\n",
      "Gradient Descent(43/49): loss=0.45057632007760073\n",
      "Gradient Descent(44/49): loss=0.45057632004843046\n",
      "Gradient Descent(45/49): loss=0.45057632003022546\n",
      "Gradient Descent(46/49): loss=0.4505763200188638\n",
      "Gradient Descent(47/49): loss=0.45057632001177284\n",
      "Gradient Descent(48/49): loss=0.4505763200073475\n",
      "Gradient Descent(49/49): loss=0.45057632000458536\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48127005656192023\n",
      "Gradient Descent(2/49): loss=0.4695806988622117\n",
      "Gradient Descent(3/49): loss=0.46228537072182607\n",
      "Gradient Descent(4/49): loss=0.45773235642941174\n",
      "Gradient Descent(5/49): loss=0.45489082020951666\n",
      "Gradient Descent(6/49): loss=0.4531174174546795\n",
      "Gradient Descent(7/49): loss=0.4520106367953853\n",
      "Gradient Descent(8/49): loss=0.4513198949859201\n",
      "Gradient Descent(9/49): loss=0.4508888030226326\n",
      "Gradient Descent(10/49): loss=0.4506197585283452\n",
      "Gradient Descent(11/49): loss=0.45045184785946024\n",
      "Gradient Descent(12/49): loss=0.4503470548110091\n",
      "Gradient Descent(13/49): loss=0.45028165346947063\n",
      "Gradient Descent(14/49): loss=0.4502408364922167\n",
      "Gradient Descent(15/49): loss=0.45021536261671236\n",
      "Gradient Descent(16/49): loss=0.4501994643710102\n",
      "Gradient Descent(17/49): loss=0.45018954227586744\n",
      "Gradient Descent(18/49): loss=0.45018334989628894\n",
      "Gradient Descent(19/49): loss=0.45017948523219387\n",
      "Gradient Descent(20/49): loss=0.450177073295332\n",
      "Gradient Descent(21/49): loss=0.4501755680055368\n",
      "Gradient Descent(22/49): loss=0.45017462855417545\n",
      "Gradient Descent(23/49): loss=0.4501740422425808\n",
      "Gradient Descent(24/49): loss=0.4501736763255147\n",
      "Gradient Descent(25/49): loss=0.45017344795667374\n",
      "Gradient Descent(26/49): loss=0.45017330543168027\n",
      "Gradient Descent(27/49): loss=0.4501732164818316\n",
      "Gradient Descent(28/49): loss=0.4501731609682312\n",
      "Gradient Descent(29/49): loss=0.450173126322193\n",
      "Gradient Descent(30/49): loss=0.4501731046996007\n",
      "Gradient Descent(31/49): loss=0.4501730912049405\n",
      "Gradient Descent(32/49): loss=0.4501730827829234\n",
      "Gradient Descent(33/49): loss=0.4501730775267427\n",
      "Gradient Descent(34/49): loss=0.45017307424636\n",
      "Gradient Descent(35/49): loss=0.4501730721990732\n",
      "Gradient Descent(36/49): loss=0.45017307092136155\n",
      "Gradient Descent(37/49): loss=0.45017307012394187\n",
      "Gradient Descent(38/49): loss=0.45017306962627196\n",
      "Gradient Descent(39/49): loss=0.4501730693156764\n",
      "Gradient Descent(40/49): loss=0.45017306912183364\n",
      "Gradient Descent(41/49): loss=0.45017306900085624\n",
      "Gradient Descent(42/49): loss=0.4501730689253544\n",
      "Gradient Descent(43/49): loss=0.4501730688782339\n",
      "Gradient Descent(44/49): loss=0.45017306884882535\n",
      "Gradient Descent(45/49): loss=0.4501730688304724\n",
      "Gradient Descent(46/49): loss=0.45017306881901764\n",
      "Gradient Descent(47/49): loss=0.4501730688118689\n",
      "Gradient Descent(48/49): loss=0.4501730688074074\n",
      "Gradient Descent(49/49): loss=0.45017306880462293\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4840201456263168\n",
      "Gradient Descent(2/49): loss=0.47327529154545356\n",
      "Gradient Descent(3/49): loss=0.46605045166148085\n",
      "Gradient Descent(4/49): loss=0.4611924693234963\n",
      "Gradient Descent(5/49): loss=0.4579259619994365\n",
      "Gradient Descent(6/49): loss=0.4557295624747378\n",
      "Gradient Descent(7/49): loss=0.4542527034343311\n",
      "Gradient Descent(8/49): loss=0.45325966341556095\n",
      "Gradient Descent(9/49): loss=0.4525919433069399\n",
      "Gradient Descent(10/49): loss=0.4521429683059033\n",
      "Gradient Descent(11/49): loss=0.45184107751520647\n",
      "Gradient Descent(12/49): loss=0.4516380861475415\n",
      "Gradient Descent(13/49): loss=0.45150159475192375\n",
      "Gradient Descent(14/49): loss=0.45140981793751034\n",
      "Gradient Descent(15/49): loss=0.45134810720749874\n",
      "Gradient Descent(16/49): loss=0.45130661291263896\n",
      "Gradient Descent(17/49): loss=0.4512787121487753\n",
      "Gradient Descent(18/49): loss=0.4512599516751532\n",
      "Gradient Descent(19/49): loss=0.4512473371326897\n",
      "Gradient Descent(20/49): loss=0.45123885511433753\n",
      "Gradient Descent(21/49): loss=0.4512331518051972\n",
      "Gradient Descent(22/49): loss=0.45122931690013146\n",
      "Gradient Descent(23/49): loss=0.4512267383099653\n",
      "Gradient Descent(24/49): loss=0.4512250044659374\n",
      "Gradient Descent(25/49): loss=0.45122383862921317\n",
      "Gradient Descent(26/49): loss=0.45122305472059965\n",
      "Gradient Descent(27/49): loss=0.45122252762044796\n",
      "Gradient Descent(28/49): loss=0.4512221731983062\n",
      "Gradient Descent(29/49): loss=0.4512219348848576\n",
      "Gradient Descent(30/49): loss=0.4512217746428953\n",
      "Gradient Descent(31/49): loss=0.4512216668961995\n",
      "Gradient Descent(32/49): loss=0.45122159444732124\n",
      "Gradient Descent(33/49): loss=0.45122154573269574\n",
      "Gradient Descent(34/49): loss=0.4512215129769815\n",
      "Gradient Descent(35/49): loss=0.451221490952039\n",
      "Gradient Descent(36/49): loss=0.45122147614246777\n",
      "Gradient Descent(37/49): loss=0.4512214661845119\n",
      "Gradient Descent(38/49): loss=0.4512214594887829\n",
      "Gradient Descent(39/49): loss=0.4512214549865743\n",
      "Gradient Descent(40/49): loss=0.45122145195928937\n",
      "Gradient Descent(41/49): loss=0.451221449923743\n",
      "Gradient Descent(42/49): loss=0.45122144855504165\n",
      "Gradient Descent(43/49): loss=0.45122144763472677\n",
      "Gradient Descent(44/49): loss=0.45122144701590716\n",
      "Gradient Descent(45/49): loss=0.4512214465998126\n",
      "Gradient Descent(46/49): loss=0.4512214463200309\n",
      "Gradient Descent(47/49): loss=0.4512214461319056\n",
      "Gradient Descent(48/49): loss=0.45122144600541036\n",
      "Gradient Descent(49/49): loss=0.45122144592035446\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4836192851109888\n",
      "Gradient Descent(2/49): loss=0.4726048924196194\n",
      "Gradient Descent(3/49): loss=0.46519881477394043\n",
      "Gradient Descent(4/49): loss=0.4602189681649874\n",
      "Gradient Descent(5/49): loss=0.4568705193051257\n",
      "Gradient Descent(6/49): loss=0.45461902229175516\n",
      "Gradient Descent(7/49): loss=0.45310511569996487\n",
      "Gradient Descent(8/49): loss=0.45208716490764544\n",
      "Gradient Descent(9/49): loss=0.4514026947948892\n",
      "Gradient Descent(10/49): loss=0.45094245709107234\n",
      "Gradient Descent(11/49): loss=0.4506329932590257\n",
      "Gradient Descent(12/49): loss=0.450424909778358\n",
      "Gradient Descent(13/49): loss=0.4502849944459565\n",
      "Gradient Descent(14/49): loss=0.45019091537645\n",
      "Gradient Descent(15/49): loss=0.4501276566101137\n",
      "Gradient Descent(16/49): loss=0.45008512141562945\n",
      "Gradient Descent(17/49): loss=0.45005652075085795\n",
      "Gradient Descent(18/49): loss=0.4500372896638657\n",
      "Gradient Descent(19/49): loss=0.45002435868097196\n",
      "Gradient Descent(20/49): loss=0.4500156638880743\n",
      "Gradient Descent(21/49): loss=0.45000981750933017\n",
      "Gradient Descent(22/49): loss=0.45000588640426253\n",
      "Gradient Descent(23/49): loss=0.45000324312921497\n",
      "Gradient Descent(24/49): loss=0.4500014657910727\n",
      "Gradient Descent(25/49): loss=0.45000027070890625\n",
      "Gradient Descent(26/49): loss=0.4499994671356574\n",
      "Gradient Descent(27/49): loss=0.4499989268130049\n",
      "Gradient Descent(28/49): loss=0.4499985635000532\n",
      "Gradient Descent(29/49): loss=0.4499983192084247\n",
      "Gradient Descent(30/49): loss=0.44999815494673356\n",
      "Gradient Descent(31/49): loss=0.4499980444971725\n",
      "Gradient Descent(32/49): loss=0.4499979702308874\n",
      "Gradient Descent(33/49): loss=0.4499979202942376\n",
      "Gradient Descent(34/49): loss=0.4499978867168341\n",
      "Gradient Descent(35/49): loss=0.4499978641393879\n",
      "Gradient Descent(36/49): loss=0.4499978489583133\n",
      "Gradient Descent(37/49): loss=0.44999783875055877\n",
      "Gradient Descent(38/49): loss=0.4499978318868643\n",
      "Gradient Descent(39/49): loss=0.44999782727171633\n",
      "Gradient Descent(40/49): loss=0.4499978241684909\n",
      "Gradient Descent(41/49): loss=0.4499978220818821\n",
      "Gradient Descent(42/49): loss=0.4499978206788466\n",
      "Gradient Descent(43/49): loss=0.44999781973544506\n",
      "Gradient Descent(44/49): loss=0.4499978191011019\n",
      "Gradient Descent(45/49): loss=0.4499978186745697\n",
      "Gradient Descent(46/49): loss=0.4499978183877694\n",
      "Gradient Descent(47/49): loss=0.4499978181949252\n",
      "Gradient Descent(48/49): loss=0.4499978180652564\n",
      "Gradient Descent(49/49): loss=0.44999781797806726\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48380880243200003\n",
      "Gradient Descent(2/49): loss=0.4729218411872743\n",
      "Gradient Descent(3/49): loss=0.46560144844632106\n",
      "Gradient Descent(4/49): loss=0.4606792163673074\n",
      "Gradient Descent(5/49): loss=0.45736950751737826\n",
      "Gradient Descent(6/49): loss=0.4551440592866851\n",
      "Gradient Descent(7/49): loss=0.45364766789636674\n",
      "Gradient Descent(8/49): loss=0.45264149432551715\n",
      "Gradient Descent(9/49): loss=0.4519649432164775\n",
      "Gradient Descent(10/49): loss=0.45151003025075925\n",
      "Gradient Descent(11/49): loss=0.45120414677261056\n",
      "Gradient Descent(12/49): loss=0.45099847072190347\n",
      "Gradient Descent(13/49): loss=0.4508601741454078\n",
      "Gradient Descent(14/49): loss=0.4507671835273722\n",
      "Gradient Descent(15/49): loss=0.4507046566358051\n",
      "Gradient Descent(16/49): loss=0.450662613553915\n",
      "Gradient Descent(17/49): loss=0.4506343437856526\n",
      "Gradient Descent(18/49): loss=0.45061533519347285\n",
      "Gradient Descent(19/49): loss=0.4506025538160911\n",
      "Gradient Descent(20/49): loss=0.4505939596179397\n",
      "Gradient Descent(21/49): loss=0.4505881808791027\n",
      "Gradient Descent(22/49): loss=0.4505842952551089\n",
      "Gradient Descent(23/49): loss=0.450581682561535\n",
      "Gradient Descent(24/49): loss=0.4505799257863763\n",
      "Gradient Descent(25/49): loss=0.45057874453075936\n",
      "Gradient Descent(26/49): loss=0.4505779502544822\n",
      "Gradient Descent(27/49): loss=0.4505774161831142\n",
      "Gradient Descent(28/49): loss=0.4505770570735257\n",
      "Gradient Descent(29/49): loss=0.45057681560823865\n",
      "Gradient Descent(30/49): loss=0.45057665324697976\n",
      "Gradient Descent(31/49): loss=0.45057654407526915\n",
      "Gradient Descent(32/49): loss=0.4505764706682111\n",
      "Gradient Descent(33/49): loss=0.4505764213093052\n",
      "Gradient Descent(34/49): loss=0.45057638812037665\n",
      "Gradient Descent(35/49): loss=0.45057636580414145\n",
      "Gradient Descent(36/49): loss=0.45057635079870484\n",
      "Gradient Descent(37/49): loss=0.450576340709049\n",
      "Gradient Descent(38/49): loss=0.4505763339247645\n",
      "Gradient Descent(39/49): loss=0.4505763293630116\n",
      "Gradient Descent(40/49): loss=0.4505763262956891\n",
      "Gradient Descent(41/49): loss=0.4505763242332213\n",
      "Gradient Descent(42/49): loss=0.450576322846418\n",
      "Gradient Descent(43/49): loss=0.45057632191393154\n",
      "Gradient Descent(44/49): loss=0.45057632128692765\n",
      "Gradient Descent(45/49): loss=0.45057632086533006\n",
      "Gradient Descent(46/49): loss=0.45057632058184793\n",
      "Gradient Descent(47/49): loss=0.4505763203912343\n",
      "Gradient Descent(48/49): loss=0.45057632026306604\n",
      "Gradient Descent(49/49): loss=0.45057632017688554\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48367669733888\n",
      "Gradient Descent(2/49): loss=0.47270090862954284\n",
      "Gradient Descent(3/49): loss=0.46532078830138673\n",
      "Gradient Descent(4/49): loss=0.4603583953927324\n",
      "Gradient Descent(5/49): loss=0.4570216824009525\n",
      "Gradient Descent(6/49): loss=0.4547780765852802\n",
      "Gradient Descent(7/49): loss=0.4532694760348229\n",
      "Gradient Descent(8/49): loss=0.4522550930246951\n",
      "Gradient Descent(9/49): loss=0.45157302188868487\n",
      "Gradient Descent(10/49): loss=0.4511143972568315\n",
      "Gradient Descent(11/49): loss=0.4508060180543737\n",
      "Gradient Descent(12/49): loss=0.45059866387864095\n",
      "Gradient Descent(13/49): loss=0.45045923893087814\n",
      "Gradient Descent(14/49): loss=0.4503654895960024\n",
      "Gradient Descent(15/49): loss=0.45030245254323203\n",
      "Gradient Descent(16/49): loss=0.4502600664289493\n",
      "Gradient Descent(17/49): loss=0.4502315660057053\n",
      "Gradient Descent(18/49): loss=0.45021240232111637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=0.45019951665959856\n",
      "Gradient Descent(20/49): loss=0.4501908523407941\n",
      "Gradient Descent(21/49): loss=0.45018502645282993\n",
      "Gradient Descent(22/49): loss=0.45018110912576265\n",
      "Gradient Descent(23/49): loss=0.4501784751150431\n",
      "Gradient Descent(24/49): loss=0.45017670400623505\n",
      "Gradient Descent(25/49): loss=0.4501755131126724\n",
      "Gradient Descent(26/49): loss=0.45017471235584094\n",
      "Gradient Descent(27/49): loss=0.45017417392694725\n",
      "Gradient Descent(28/49): loss=0.45017381188735955\n",
      "Gradient Descent(29/49): loss=0.45017356845194034\n",
      "Gradient Descent(30/49): loss=0.4501734047659648\n",
      "Gradient Descent(31/49): loss=0.4501732947035147\n",
      "Gradient Descent(32/49): loss=0.4501732206975232\n",
      "Gradient Descent(33/49): loss=0.45017317093589476\n",
      "Gradient Descent(34/49): loss=0.4501731374761755\n",
      "Gradient Descent(35/49): loss=0.4501731149778605\n",
      "Gradient Descent(36/49): loss=0.45017309984999326\n",
      "Gradient Descent(37/49): loss=0.45017308967801556\n",
      "Gradient Descent(38/49): loss=0.4501730828383776\n",
      "Gradient Descent(39/49): loss=0.45017307823940517\n",
      "Gradient Descent(40/49): loss=0.45017307514705596\n",
      "Gradient Descent(41/49): loss=0.4501730730677604\n",
      "Gradient Descent(42/49): loss=0.45017307166964243\n",
      "Gradient Descent(43/49): loss=0.4501730707295473\n",
      "Gradient Descent(44/49): loss=0.4501730700974276\n",
      "Gradient Descent(45/49): loss=0.4501730696723903\n",
      "Gradient Descent(46/49): loss=0.45017306938659524\n",
      "Gradient Descent(47/49): loss=0.45017306919442646\n",
      "Gradient Descent(48/49): loss=0.4501730690652125\n",
      "Gradient Descent(49/49): loss=0.45017306897832904\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48646395119445346\n",
      "Gradient Descent(2/49): loss=0.47668415593244945\n",
      "Gradient Descent(3/49): loss=0.46961825385564837\n",
      "Gradient Descent(4/49): loss=0.4645131396051599\n",
      "Gradient Descent(5/49): loss=0.4608246945591811\n",
      "Gradient Descent(6/49): loss=0.45815979301346127\n",
      "Gradient Descent(7/49): loss=0.45623440164667906\n",
      "Gradient Descent(8/49): loss=0.45484330638417875\n",
      "Gradient Descent(9/49): loss=0.45383824005702234\n",
      "Gradient Descent(10/49): loss=0.45311207963565214\n",
      "Gradient Descent(11/49): loss=0.452587428731212\n",
      "Gradient Descent(12/49): loss=0.45220836845275403\n",
      "Gradient Descent(13/49): loss=0.45193449740156805\n",
      "Gradient Descent(14/49): loss=0.4517366255670863\n",
      "Gradient Descent(15/49): loss=0.4515936631666732\n",
      "Gradient Descent(16/49): loss=0.4514903728323748\n",
      "Gradient Descent(17/49): loss=0.4514157455658441\n",
      "Gradient Descent(18/49): loss=0.45136182736577557\n",
      "Gradient Descent(19/49): loss=0.4513228714662262\n",
      "Gradient Descent(20/49): loss=0.45129472582880176\n",
      "Gradient Descent(21/49): loss=0.4512743906057626\n",
      "Gradient Descent(22/49): loss=0.4512596984071167\n",
      "Gradient Descent(23/49): loss=0.45124908329359537\n",
      "Gradient Descent(24/49): loss=0.45124141387407585\n",
      "Gradient Descent(25/49): loss=0.4512358727184732\n",
      "Gradient Descent(26/49): loss=0.4512318692335503\n",
      "Gradient Descent(27/49): loss=0.45122897671569323\n",
      "Gradient Descent(28/49): loss=0.45122688687154183\n",
      "Gradient Descent(29/49): loss=0.45122537695914233\n",
      "Gradient Descent(30/49): loss=0.4512242860474336\n",
      "Gradient Descent(31/49): loss=0.45122349786372395\n",
      "Gradient Descent(32/49): loss=0.451222928400994\n",
      "Gradient Descent(33/49): loss=0.4512225169641713\n",
      "Gradient Descent(34/49): loss=0.45122221970106724\n",
      "Gradient Descent(35/49): loss=0.4512220049284745\n",
      "Gradient Descent(36/49): loss=0.45122184975527624\n",
      "Gradient Descent(37/49): loss=0.45122173764264023\n",
      "Gradient Descent(38/49): loss=0.4512216566412611\n",
      "Gradient Descent(39/49): loss=0.4512215981177644\n",
      "Gradient Descent(40/49): loss=0.4512215558345382\n",
      "Gradient Descent(41/49): loss=0.45122152528490705\n",
      "Gradient Descent(42/49): loss=0.4512215032127988\n",
      "Gradient Descent(43/49): loss=0.4512214872657004\n",
      "Gradient Descent(44/49): loss=0.4512214757439217\n",
      "Gradient Descent(45/49): loss=0.451221467419437\n",
      "Gradient Descent(46/49): loss=0.45122146140499636\n",
      "Gradient Descent(47/49): loss=0.45122145705956324\n",
      "Gradient Descent(48/49): loss=0.4512214539199878\n",
      "Gradient Descent(49/49): loss=0.45122145165164457\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4861243944392536\n",
      "Gradient Descent(2/49): loss=0.47609926942161535\n",
      "Gradient Descent(3/49): loss=0.4688561165963696\n",
      "Gradient Descent(4/49): loss=0.4636229386801294\n",
      "Gradient Descent(5/49): loss=0.45984196763564633\n",
      "Gradient Descent(6/49): loss=0.4571102160560082\n",
      "Gradient Descent(7/49): loss=0.45513652553971934\n",
      "Gradient Descent(8/49): loss=0.45371053414170026\n",
      "Gradient Descent(9/49): loss=0.45268025535663153\n",
      "Gradient Descent(10/49): loss=0.4519358789344199\n",
      "Gradient Descent(11/49): loss=0.4513980669693716\n",
      "Gradient Descent(12/49): loss=0.45100949782462463\n",
      "Gradient Descent(13/49): loss=0.4507287566175446\n",
      "Gradient Descent(14/49): loss=0.4505259210954294\n",
      "Gradient Descent(15/49): loss=0.45037937243070125\n",
      "Gradient Descent(16/49): loss=0.45027349102043485\n",
      "Gradient Descent(17/49): loss=0.4501969917015175\n",
      "Gradient Descent(18/49): loss=0.45014172094359994\n",
      "Gradient Descent(19/49): loss=0.45010178782100424\n",
      "Gradient Descent(20/49): loss=0.45007293613992894\n",
      "Gradient Descent(21/49): loss=0.45005209080035186\n",
      "Gradient Descent(22/49): loss=0.45003703004250756\n",
      "Gradient Descent(23/49): loss=0.450026148644965\n",
      "Gradient Descent(24/49): loss=0.45001828683524053\n",
      "Gradient Descent(25/49): loss=0.4500126066777148\n",
      "Gradient Descent(26/49): loss=0.45000850276390225\n",
      "Gradient Descent(27/49): loss=0.45000553768617274\n",
      "Gradient Descent(28/49): loss=0.45000339541751316\n",
      "Gradient Descent(29/49): loss=0.4500018476284065\n",
      "Gradient Descent(30/49): loss=0.450000729350777\n",
      "Gradient Descent(31/49): loss=0.44999992139518985\n",
      "Gradient Descent(32/49): loss=0.44999933764727784\n",
      "Gradient Descent(33/49): loss=0.4499989158894117\n",
      "Gradient Descent(34/49): loss=0.4499986111693533\n",
      "Gradient Descent(35/49): loss=0.44999839100911104\n",
      "Gradient Descent(36/49): loss=0.4499982319433361\n",
      "Gradient Descent(37/49): loss=0.44999811701831355\n",
      "Gradient Descent(38/49): loss=0.44999803398498484\n",
      "Gradient Descent(39/49): loss=0.449997973993405\n",
      "Gradient Descent(40/49): loss=0.44999793064948845\n",
      "Gradient Descent(41/49): loss=0.4499978993335087\n",
      "Gradient Descent(42/49): loss=0.44999787670771335\n",
      "Gradient Descent(43/49): loss=0.4499978603605763\n",
      "Gradient Descent(44/49): loss=0.4499978485497697\n",
      "Gradient Descent(45/49): loss=0.4499978400164621\n",
      "Gradient Descent(46/49): loss=0.449997833851147\n",
      "Gradient Descent(47/49): loss=0.44999782939670707\n",
      "Gradient Descent(48/49): loss=0.4499978261783744\n",
      "Gradient Descent(49/49): loss=0.4499978238531288\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4862849288000001\n",
      "Gradient Descent(2/49): loss=0.47637578985799867\n",
      "Gradient Descent(3/49): loss=0.46921643697240667\n",
      "Gradient Descent(4/49): loss=0.46404380451256527\n",
      "Gradient Descent(5/49): loss=0.4603065775603276\n",
      "Gradient Descent(6/49): loss=0.45760643108733706\n",
      "Gradient Descent(7/49): loss=0.4556555752606012\n",
      "Gradient Descent(8/49): loss=0.4542460819257844\n",
      "Gradient Descent(9/49): loss=0.4532277229913796\n",
      "Gradient Descent(10/49): loss=0.45249195866127173\n",
      "Gradient Descent(11/49): loss=0.45196036893276853\n",
      "Gradient Descent(12/49): loss=0.45157629535392546\n",
      "Gradient Descent(13/49): loss=0.45129880219321106\n",
      "Gradient Descent(14/49): loss=0.4510983133845953\n",
      "Gradient Descent(15/49): loss=0.4509534602203699\n",
      "Gradient Descent(16/49): loss=0.45084880380921716\n",
      "Gradient Descent(17/49): loss=0.45077318955215945\n",
      "Gradient Descent(18/49): loss=0.4507185582514352\n",
      "Gradient Descent(19/49): loss=0.45067908713666205\n",
      "Gradient Descent(20/49): loss=0.4506505692562382\n",
      "Gradient Descent(21/49): loss=0.4506299650876321\n",
      "Gradient Descent(22/49): loss=0.4506150785758143\n",
      "Gradient Descent(23/49): loss=0.45060432307102577\n",
      "Gradient Descent(24/49): loss=0.45059655221881617\n",
      "Gradient Descent(25/49): loss=0.4505909377780947\n",
      "Gradient Descent(26/49): loss=0.4505868813446734\n",
      "Gradient Descent(27/49): loss=0.4505839505715266\n",
      "Gradient Descent(28/49): loss=0.4505818330879278\n",
      "Gradient Descent(29/49): loss=0.4505803032060279\n",
      "Gradient Descent(30/49): loss=0.4505791978663552\n",
      "Gradient Descent(31/49): loss=0.4505783992584417\n",
      "Gradient Descent(32/49): loss=0.45057782226422416\n",
      "Gradient Descent(33/49): loss=0.45057740538590185\n",
      "Gradient Descent(34/49): loss=0.450577104191314\n",
      "Gradient Descent(35/49): loss=0.4505768865782243\n",
      "Gradient Descent(36/49): loss=0.4505767293527671\n",
      "Gradient Descent(37/49): loss=0.45057661575737434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=0.45057653368470296\n",
      "Gradient Descent(39/49): loss=0.45057647438719783\n",
      "Gradient Descent(40/49): loss=0.4505764315447505\n",
      "Gradient Descent(41/49): loss=0.4505764005910821\n",
      "Gradient Descent(42/49): loss=0.45057637822705676\n",
      "Gradient Descent(43/49): loss=0.45057636206904844\n",
      "Gradient Descent(44/49): loss=0.45057635039488736\n",
      "Gradient Descent(45/49): loss=0.4505763419603062\n",
      "Gradient Descent(46/49): loss=0.4505763358663213\n",
      "Gradient Descent(47/49): loss=0.45057633146341725\n",
      "Gradient Descent(48/49): loss=0.4505763282823189\n",
      "Gradient Descent(49/49): loss=0.4505763259839753\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.486173026592\n",
      "Gradient Descent(2/49): loss=0.4761830383047188\n",
      "Gradient Descent(3/49): loss=0.4689652717671614\n",
      "Gradient Descent(4/49): loss=0.4637504354437741\n",
      "Gradient Descent(5/49): loss=0.4599827162001265\n",
      "Gradient Descent(6/49): loss=0.45726053904659225\n",
      "Gradient Descent(7/49): loss=0.45529376605316335\n",
      "Gradient Descent(8/49): loss=0.4538727725654103\n",
      "Gradient Descent(9/49): loss=0.4528461047705089\n",
      "Gradient Descent(10/49): loss=0.45210433728869237\n",
      "Gradient Descent(11/49): loss=0.4515684102830803\n",
      "Gradient Descent(12/49): loss=0.45118120302152553\n",
      "Gradient Descent(13/49): loss=0.4509014457750525\n",
      "Gradient Descent(14/49): loss=0.4506993211644756\n",
      "Gradient Descent(15/49): loss=0.45055328613333356\n",
      "Gradient Descent(16/49): loss=0.4504477758233334\n",
      "Gradient Descent(17/49): loss=0.45037154462435836\n",
      "Gradient Descent(18/49): loss=0.45031646758309885\n",
      "Gradient Descent(19/49): loss=0.45027667442078906\n",
      "Gradient Descent(20/49): loss=0.4502479238610202\n",
      "Gradient Descent(21/49): loss=0.45022715158158705\n",
      "Gradient Descent(22/49): loss=0.4502121436096967\n",
      "Gradient Descent(23/49): loss=0.4502013003500057\n",
      "Gradient Descent(24/49): loss=0.4501934660948792\n",
      "Gradient Descent(25/49): loss=0.4501878058455502\n",
      "Gradient Descent(26/49): loss=0.45018371631541004\n",
      "Gradient Descent(27/49): loss=0.45018076162988374\n",
      "Gradient Descent(28/49): loss=0.4501786268695911\n",
      "Gradient Descent(29/49): loss=0.4501770845052794\n",
      "Gradient Descent(30/49): loss=0.4501759701470644\n",
      "Gradient Descent(31/49): loss=0.45017516502325416\n",
      "Gradient Descent(32/49): loss=0.4501745833213011\n",
      "Gradient Descent(33/49): loss=0.45017416304164\n",
      "Gradient Descent(34/49): loss=0.4501738593895849\n",
      "Gradient Descent(35/49): loss=0.45017364000097515\n",
      "Gradient Descent(36/49): loss=0.4501734814927045\n",
      "Gradient Descent(37/49): loss=0.45017336697047916\n",
      "Gradient Descent(38/49): loss=0.45017328422817116\n",
      "Gradient Descent(39/49): loss=0.45017322444685365\n",
      "Gradient Descent(40/49): loss=0.4501731812548518\n",
      "Gradient Descent(41/49): loss=0.45017315004863045\n",
      "Gradient Descent(42/49): loss=0.4501731275021354\n",
      "Gradient Descent(43/49): loss=0.45017311121229286\n",
      "Gradient Descent(44/49): loss=0.45017309944288136\n",
      "Gradient Descent(45/49): loss=0.45017309093948205\n",
      "Gradient Descent(46/49): loss=0.4501730847957757\n",
      "Gradient Descent(47/49): loss=0.4501730803569479\n",
      "Gradient Descent(48/49): loss=0.450173077149895\n",
      "Gradient Descent(49/49): loss=0.45017307483279906\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48899555816024765\n",
      "Gradient Descent(2/49): loss=0.4804737183995459\n",
      "Gradient Descent(3/49): loss=0.47387440568885897\n",
      "Gradient Descent(4/49): loss=0.46876389792569784\n",
      "Gradient Descent(5/49): loss=0.4648063207139071\n",
      "Gradient Descent(6/49): loss=0.4617415729210974\n",
      "Gradient Descent(7/49): loss=0.4593682322303454\n",
      "Gradient Descent(8/49): loss=0.457530317199426\n",
      "Gradient Descent(9/49): loss=0.45610703579948253\n",
      "Gradient Descent(10/49): loss=0.4550048466833663\n",
      "Gradient Descent(11/49): loss=0.45415131143184645\n",
      "Gradient Descent(12/49): loss=0.4534903337330692\n",
      "Gradient Descent(13/49): loss=0.45297847260313623\n",
      "Gradient Descent(14/49): loss=0.452582087344116\n",
      "Gradient Descent(15/49): loss=0.4522751265995312\n",
      "Gradient Descent(16/49): loss=0.4520374161989244\n",
      "Gradient Descent(17/49): loss=0.45185333326469446\n",
      "Gradient Descent(18/49): loss=0.451710779440427\n",
      "Gradient Descent(19/49): loss=0.451600385758914\n",
      "Gradient Descent(20/49): loss=0.45151489689195046\n",
      "Gradient Descent(21/49): loss=0.45144869431337387\n",
      "Gradient Descent(22/49): loss=0.4513974270365244\n",
      "Gradient Descent(23/49): loss=0.45135772565733184\n",
      "Gradient Descent(24/49): loss=0.4513269809092853\n",
      "Gradient Descent(25/49): loss=0.4513031721763981\n",
      "Gradient Descent(26/49): loss=0.45128473469365\n",
      "Gradient Descent(27/49): loss=0.45127045670701005\n",
      "Gradient Descent(28/49): loss=0.45125939983415586\n",
      "Gradient Descent(29/49): loss=0.45125083739181804\n",
      "Gradient Descent(30/49): loss=0.45124420663647147\n",
      "Gradient Descent(31/49): loss=0.4512390717795308\n",
      "Gradient Descent(32/49): loss=0.45123509534631606\n",
      "Gradient Descent(33/49): loss=0.4512320159964348\n",
      "Gradient Descent(34/49): loss=0.4512296313478865\n",
      "Gradient Descent(35/49): loss=0.4512277846760508\n",
      "Gradient Descent(36/49): loss=0.4512263546133813\n",
      "Gradient Descent(37/49): loss=0.4512252471728499\n",
      "Gradient Descent(38/49): loss=0.4512243895709025\n",
      "Gradient Descent(39/49): loss=0.45122372544395434\n",
      "Gradient Descent(40/49): loss=0.45122321114404546\n",
      "Gradient Descent(41/49): loss=0.4512228128701964\n",
      "Gradient Descent(42/49): loss=0.45122250444692763\n",
      "Gradient Descent(43/49): loss=0.4512222656039481\n",
      "Gradient Descent(44/49): loss=0.45122208064394514\n",
      "Gradient Descent(45/49): loss=0.45122193741091854\n",
      "Gradient Descent(46/49): loss=0.45122182649126275\n",
      "Gradient Descent(47/49): loss=0.4512217405950814\n",
      "Gradient Descent(48/49): loss=0.45122167407707847\n",
      "Gradient Descent(49/49): loss=0.45122162256553705\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4887195076954794\n",
      "Gradient Descent(2/49): loss=0.47998389445485423\n",
      "Gradient Descent(3/49): loss=0.47321903556131845\n",
      "Gradient Descent(4/49): loss=0.4679803288341635\n",
      "Gradient Descent(5/49): loss=0.4639234743446556\n",
      "Gradient Descent(6/49): loss=0.46078184622797935\n",
      "Gradient Descent(7/49): loss=0.45834896941442804\n",
      "Gradient Descent(8/49): loss=0.45646494961001227\n",
      "Gradient Descent(9/49): loss=0.4550059646734731\n",
      "Gradient Descent(10/49): loss=0.4538761267386166\n",
      "Gradient Descent(11/49): loss=0.4530011802418643\n",
      "Gradient Descent(12/49): loss=0.45232362167477896\n",
      "Gradient Descent(13/49): loss=0.4517989203204283\n",
      "Gradient Descent(14/49): loss=0.45139259159161926\n",
      "Gradient Descent(15/49): loss=0.4510779306240293\n",
      "Gradient Descent(16/49): loss=0.45083425717072795\n",
      "Gradient Descent(17/49): loss=0.4506455564484912\n",
      "Gradient Descent(18/49): loss=0.4504994266091911\n",
      "Gradient Descent(19/49): loss=0.4503862636616368\n",
      "Gradient Descent(20/49): loss=0.450298630275051\n",
      "Gradient Descent(21/49): loss=0.4502307669804791\n",
      "Gradient Descent(22/49): loss=0.4501782136451625\n",
      "Gradient Descent(23/49): loss=0.45013751634229326\n",
      "Gradient Descent(24/49): loss=0.4501060003509514\n",
      "Gradient Descent(25/49): loss=0.45008159436725614\n",
      "Gradient Descent(26/49): loss=0.4500626943734825\n",
      "Gradient Descent(27/49): loss=0.4500480582183045\n",
      "Gradient Descent(28/49): loss=0.45003672397973443\n",
      "Gradient Descent(29/49): loss=0.4500279467453859\n",
      "Gradient Descent(30/49): loss=0.45002114965510626\n",
      "Gradient Descent(31/49): loss=0.4500158859883938\n",
      "Gradient Descent(32/49): loss=0.4500118098048917\n",
      "Gradient Descent(33/49): loss=0.45000865320838757\n",
      "Gradient Descent(34/49): loss=0.45000620874005476\n",
      "Gradient Descent(35/49): loss=0.4500043157437781\n",
      "Gradient Descent(36/49): loss=0.4500028498074612\n",
      "Gradient Descent(37/49): loss=0.4500017145863772\n",
      "Gradient Descent(38/49): loss=0.45000083547117004\n",
      "Gradient Descent(39/49): loss=0.45000015468435345\n",
      "Gradient Descent(40/49): loss=0.4499996274830429\n",
      "Gradient Descent(41/49): loss=0.44999921921834785\n",
      "Gradient Descent(42/49): loss=0.44999890305816803\n",
      "Gradient Descent(43/49): loss=0.44999865822372487\n",
      "Gradient Descent(44/49): loss=0.4499984686239319\n",
      "Gradient Descent(45/49): loss=0.4499983217978525\n",
      "Gradient Descent(46/49): loss=0.4499982080957363\n",
      "Gradient Descent(47/49): loss=0.44999812004481754\n",
      "Gradient Descent(48/49): loss=0.4499980518581864\n",
      "Gradient Descent(49/49): loss=0.449997999054459\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4888500177920002\n",
      "Gradient Descent(2/49): loss=0.4802154715701236\n",
      "Gradient Descent(3/49): loss=0.4735288789759069\n",
      "Gradient Descent(4/49): loss=0.4683507816709417\n",
      "Gradient Descent(5/49): loss=0.46434086311797595\n",
      "Gradient Descent(6/49): loss=0.46123558219056005\n",
      "Gradient Descent(7/49): loss=0.4588308526403692\n",
      "Gradient Descent(8/49): loss=0.45696863007670246\n",
      "Gradient Descent(9/49): loss=0.45552652492339857\n",
      "Gradient Descent(10/49): loss=0.45440975869267974\n",
      "Gradient Descent(11/49): loss=0.45354493492361136\n",
      "Gradient Descent(12/49): loss=0.4528752153968446\n",
      "Gradient Descent(13/49): loss=0.45235658459531647\n",
      "Gradient Descent(14/49): loss=0.4519549569026131\n",
      "Gradient Descent(15/49): loss=0.45164393641738343\n",
      "Gradient Descent(16/49): loss=0.45140308215362174\n",
      "Gradient Descent(17/49): loss=0.4512165646117648\n",
      "Gradient Descent(18/49): loss=0.45107212542735053\n",
      "Gradient Descent(19/49): loss=0.4509602717229404\n",
      "Gradient Descent(20/49): loss=0.4508736522142451\n",
      "Gradient Descent(21/49): loss=0.4508065740667114\n",
      "Gradient Descent(22/49): loss=0.45075462874926114\n",
      "Gradient Descent(23/49): loss=0.45071440229542803\n",
      "Gradient Descent(24/49): loss=0.4506832509295792\n",
      "Gradient Descent(25/49): loss=0.45065912731186614\n",
      "Gradient Descent(26/49): loss=0.4506404459823093\n",
      "Gradient Descent(27/49): loss=0.45062597916070024\n",
      "Gradient Descent(28/49): loss=0.4506147760540464\n",
      "Gradient Descent(29/49): loss=0.45060610036825327\n",
      "Gradient Descent(30/49): loss=0.45059938191717536\n",
      "Gradient Descent(31/49): loss=0.4505941791486607\n",
      "Gradient Descent(32/49): loss=0.45059015012472303\n",
      "Gradient Descent(33/49): loss=0.45058703004858525\n",
      "Gradient Descent(34/49): loss=0.4505846138616246\n",
      "Gradient Descent(35/49): loss=0.450582742766442\n",
      "Gradient Descent(36/49): loss=0.45058129379033274\n",
      "Gradient Descent(37/49): loss=0.45058017170323356\n",
      "Gradient Descent(38/49): loss=0.4505793027589841\n",
      "Gradient Descent(39/49): loss=0.4505786298485573\n",
      "Gradient Descent(40/49): loss=0.45057810874672266\n",
      "Gradient Descent(41/49): loss=0.4505777052054621\n",
      "Gradient Descent(42/49): loss=0.4505773927031098\n",
      "Gradient Descent(43/49): loss=0.45057715070128823\n",
      "Gradient Descent(44/49): loss=0.45057696329507785\n",
      "Gradient Descent(45/49): loss=0.45057681816770795\n",
      "Gradient Descent(46/49): loss=0.45057670578107306\n",
      "Gradient Descent(47/49): loss=0.45057661874886307\n",
      "Gradient Descent(48/49): loss=0.45057655135111946\n",
      "Gradient Descent(49/49): loss=0.45057649915830705\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48875904432128014\n",
      "Gradient Descent(2/49): loss=0.48005404824367726\n",
      "Gradient Descent(3/49): loss=0.47331289928118697\n",
      "Gradient Descent(4/49): loss=0.4680925535246327\n",
      "Gradient Descent(5/49): loss=0.4640499177707546\n",
      "Gradient Descent(6/49): loss=0.4609193006429539\n",
      "Gradient Descent(7/49): loss=0.458494950739183\n",
      "Gradient Descent(8/49): loss=0.45661753417370315\n",
      "Gradient Descent(9/49): loss=0.4551636627853955\n",
      "Gradient Descent(10/49): loss=0.45403778478229\n",
      "Gradient Descent(11/49): loss=0.45316590485668584\n",
      "Gradient Descent(12/49): loss=0.4524907210422977\n",
      "Gradient Descent(13/49): loss=0.45196785869643524\n",
      "Gradient Descent(14/49): loss=0.4515629540957998\n",
      "Gradient Descent(15/49): loss=0.45124939597306696\n",
      "Gradient Descent(16/49): loss=0.451006576562823\n",
      "Gradient Descent(17/49): loss=0.4508185372115303\n",
      "Gradient Descent(18/49): loss=0.450672919537889\n",
      "Gradient Descent(19/49): loss=0.45056015321142123\n",
      "Gradient Descent(20/49): loss=0.4504728269682044\n",
      "Gradient Descent(21/49): loss=0.45040520152545777\n",
      "Gradient Descent(22/49): loss=0.4503528323825944\n",
      "Gradient Descent(23/49): loss=0.45031227771836113\n",
      "Gradient Descent(24/49): loss=0.4502808721863789\n",
      "Gradient Descent(25/49): loss=0.4502565517424116\n",
      "Gradient Descent(26/49): loss=0.4502377179906035\n",
      "Gradient Descent(27/49): loss=0.4502231331332034\n",
      "Gradient Descent(28/49): loss=0.4502118386196328\n",
      "Gradient Descent(29/49): loss=0.45020309214832355\n",
      "Gradient Descent(30/49): loss=0.45019631888094186\n",
      "Gradient Descent(31/49): loss=0.45019107366268135\n",
      "Gradient Descent(32/49): loss=0.4501870117656604\n",
      "Gradient Descent(33/49): loss=0.4501838662326074\n",
      "Gradient Descent(34/49): loss=0.45018143033181107\n",
      "Gradient Descent(35/49): loss=0.4501795439702345\n",
      "Gradient Descent(36/49): loss=0.45017808317182967\n",
      "Gradient Descent(37/49): loss=0.45017695192954493\n",
      "Gradient Descent(38/49): loss=0.4501760758955196\n",
      "Gradient Descent(39/49): loss=0.4501753974947704\n",
      "Gradient Descent(40/49): loss=0.45017487214123014\n",
      "Gradient Descent(41/49): loss=0.4501744653074487\n",
      "Gradient Descent(42/49): loss=0.450174150255368\n",
      "Gradient Descent(43/49): loss=0.4501739062790371\n",
      "Gradient Descent(44/49): loss=0.4501737173437665\n",
      "Gradient Descent(45/49): loss=0.4501735710322926\n",
      "Gradient Descent(46/49): loss=0.4501734577286876\n",
      "Gradient Descent(47/49): loss=0.4501733699863754\n",
      "Gradient Descent(48/49): loss=0.4501733020387293\n",
      "Gradient Descent(49/49): loss=0.4501732494200718\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49161496652369946\n",
      "Gradient Descent(2/49): loss=0.4846713203019712\n",
      "Gradient Descent(3/49): loss=0.4789212868657582\n",
      "Gradient Descent(4/49): loss=0.474159684177234\n",
      "Gradient Descent(5/49): loss=0.4702166009908677\n",
      "Gradient Descent(6/49): loss=0.46695133380423626\n",
      "Gradient Descent(7/49): loss=0.4642473660469868\n",
      "Gradient Descent(8/49): loss=0.4620082103472078\n",
      "Gradient Descent(9/49): loss=0.4601539655122216\n",
      "Gradient Descent(10/49): loss=0.45861846536436895\n",
      "Gradient Descent(11/49): loss=0.4573469176919327\n",
      "Gradient Descent(12/49): loss=0.4562939490643888\n",
      "Gradient Descent(13/49): loss=0.45542198574391984\n",
      "Gradient Descent(14/49): loss=0.45469991291823914\n",
      "Gradient Descent(15/49): loss=0.454101964411293\n",
      "Gradient Descent(16/49): loss=0.4536068032526909\n",
      "Gradient Descent(17/49): loss=0.4531967602972525\n",
      "Gradient Descent(18/49): loss=0.4528572037258539\n",
      "Gradient Descent(19/49): loss=0.452576016929079\n",
      "Gradient Descent(20/49): loss=0.45234316614266956\n",
      "Gradient Descent(21/49): loss=0.45215034240644353\n",
      "Gradient Descent(22/49): loss=0.4519906650704751\n",
      "Gradient Descent(23/49): loss=0.4518584362685595\n",
      "Gradient Descent(24/49): loss=0.4517489375976932\n",
      "Gradient Descent(25/49): loss=0.451658261748349\n",
      "Gradient Descent(26/49): loss=0.45158317307750717\n",
      "Gradient Descent(27/49): loss=0.45152099214918273\n",
      "Gradient Descent(28/49): loss=0.4514695001224377\n",
      "Gradient Descent(29/49): loss=0.4514268595750897\n",
      "Gradient Descent(30/49): loss=0.451391548937831\n",
      "Gradient Descent(31/49): loss=0.4513623081991169\n",
      "Gradient Descent(32/49): loss=0.4513380939433878\n",
      "Gradient Descent(33/49): loss=0.45131804211821896\n",
      "Gradient Descent(34/49): loss=0.45130143720179616\n",
      "Gradient Descent(35/49): loss=0.45128768667050667\n",
      "Gradient Descent(36/49): loss=0.4512762998555458\n",
      "Gradient Descent(37/49): loss=0.45126687043407654\n",
      "Gradient Descent(38/49): loss=0.451259061930158\n",
      "Gradient Descent(39/49): loss=0.45125259570806303\n",
      "Gradient Descent(40/49): loss=0.4512472410295461\n",
      "Gradient Descent(41/49): loss=0.4512428068202665\n",
      "Gradient Descent(42/49): loss=0.45123913485156186\n",
      "Gradient Descent(43/49): loss=0.4512360940942775\n",
      "Gradient Descent(44/49): loss=0.4512335760431705\n",
      "Gradient Descent(45/49): loss=0.45123149084504866\n",
      "Gradient Descent(46/49): loss=0.45122976409248394\n",
      "Gradient Descent(47/49): loss=0.4512283341686851\n",
      "Gradient Descent(48/49): loss=0.4512271500487874\n",
      "Gradient Descent(49/49): loss=0.45122616947910005\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4914046248796673\n",
      "Gradient Descent(2/49): loss=0.48428679474251773\n",
      "Gradient Descent(3/49): loss=0.4783925196059437\n",
      "Gradient Descent(4/49): loss=0.4735114703653489\n",
      "Gradient Descent(5/49): loss=0.46946947348921186\n",
      "Gradient Descent(6/49): loss=0.4661222958760845\n",
      "Gradient Descent(7/49): loss=0.4633504980946522\n",
      "Gradient Descent(8/49): loss=0.4610551723518481\n",
      "Gradient Descent(9/49): loss=0.45915441310423327\n",
      "Gradient Descent(10/49): loss=0.45758039437128184\n",
      "Gradient Descent(11/49): loss=0.45627694945852526\n",
      "Gradient Descent(12/49): loss=0.4551975667262723\n",
      "Gradient Descent(13/49): loss=0.4543037298856931\n",
      "Gradient Descent(14/49): loss=0.45356354359800977\n",
      "Gradient Descent(15/49): loss=0.4529505953331792\n",
      "Gradient Descent(16/49): loss=0.45244301287507305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=0.45202268384151495\n",
      "Gradient Descent(18/49): loss=0.4516746093688257\n",
      "Gradient Descent(19/49): loss=0.4513863688979918\n",
      "Gradient Descent(20/49): loss=0.4511476769640942\n",
      "Gradient Descent(21/49): loss=0.45095001617363367\n",
      "Gradient Descent(22/49): loss=0.45078633327305323\n",
      "Gradient Descent(23/49): loss=0.45065078746308246\n",
      "Gradient Descent(24/49): loss=0.45053854197784576\n",
      "Gradient Descent(25/49): loss=0.45044559149152136\n",
      "Gradient Descent(26/49): loss=0.4503686191937962\n",
      "Gradient Descent(27/49): loss=0.45030487843404987\n",
      "Gradient Descent(28/49): loss=0.4502520947109037\n",
      "Gradient Descent(29/49): loss=0.4502083845097663\n",
      "Gradient Descent(30/49): loss=0.4501721880922049\n",
      "Gradient Descent(31/49): loss=0.4501422138388221\n",
      "Gradient Descent(32/49): loss=0.45011739215959556\n",
      "Gradient Descent(33/49): loss=0.4500968373270284\n",
      "Gradient Descent(34/49): loss=0.4500798158701795\n",
      "Gradient Descent(35/49): loss=0.45006572040176285\n",
      "Gradient Descent(36/49): loss=0.4500540479443671\n",
      "Gradient Descent(37/49): loss=0.45004438198239727\n",
      "Gradient Descent(38/49): loss=0.45003637759929055\n",
      "Gradient Descent(39/49): loss=0.4500297491696398\n",
      "Gradient Descent(40/49): loss=0.450024260167046\n",
      "Gradient Descent(41/49): loss=0.4500197147239978\n",
      "Gradient Descent(42/49): loss=0.45001595064260996\n",
      "Gradient Descent(43/49): loss=0.4500128336068123\n",
      "Gradient Descent(44/49): loss=0.4500102523894686\n",
      "Gradient Descent(45/49): loss=0.45000811488338616\n",
      "Gradient Descent(46/49): loss=0.4500063448145993\n",
      "Gradient Descent(47/49): loss=0.45000487902063685\n",
      "Gradient Descent(48/49): loss=0.45000366519665624\n",
      "Gradient Descent(49/49): loss=0.4500026600290185\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4915040694080001\n",
      "Gradient Descent(2/49): loss=0.4844685892847652\n",
      "Gradient Descent(3/49): loss=0.4786425081947167\n",
      "Gradient Descent(4/49): loss=0.47381793044404624\n",
      "Gradient Descent(5/49): loss=0.46982269760871476\n",
      "Gradient Descent(6/49): loss=0.46651424529777563\n",
      "Gradient Descent(7/49): loss=0.4637745159390881\n",
      "Gradient Descent(8/49): loss=0.46150574605715844\n",
      "Gradient Descent(9/49): loss=0.4596269777179343\n",
      "Gradient Descent(10/49): loss=0.4580711696562216\n",
      "Gradient Descent(11/49): loss=0.45678280500031687\n",
      "Gradient Descent(12/49): loss=0.45571591022876295\n",
      "Gradient Descent(13/49): loss=0.45483241466843816\n",
      "Gradient Descent(14/49): loss=0.4541007919949339\n",
      "Gradient Descent(15/49): loss=0.453494935259005\n",
      "Gradient Descent(16/49): loss=0.45299322529598207\n",
      "Gradient Descent(17/49): loss=0.45257775927560256\n",
      "Gradient Descent(18/49): loss=0.4522337118641266\n",
      "Gradient Descent(19/49): loss=0.4519488062026833\n",
      "Gradient Descent(20/49): loss=0.4517128758244419\n",
      "Gradient Descent(21/49): loss=0.4515175018782204\n",
      "Gradient Descent(22/49): loss=0.4513557127133541\n",
      "Gradient Descent(23/49): loss=0.45122173510592883\n",
      "Gradient Descent(24/49): loss=0.45111078824921974\n",
      "Gradient Descent(25/49): loss=0.45101891315717885\n",
      "Gradient Descent(26/49): loss=0.45094283139345975\n",
      "Gradient Descent(27/49): loss=0.45087982808492394\n",
      "Gradient Descent(28/49): loss=0.4508276550451256\n",
      "Gradient Descent(29/49): loss=0.45078445055086847\n",
      "Gradient Descent(30/49): loss=0.45074867290917425\n",
      "Gradient Descent(31/49): loss=0.45071904544408714\n",
      "Gradient Descent(32/49): loss=0.45069451094024876\n",
      "Gradient Descent(33/49): loss=0.4506741939176198\n",
      "Gradient Descent(34/49): loss=0.45065736939118095\n",
      "Gradient Descent(35/49): loss=0.4506434370008371\n",
      "Gradient Descent(36/49): loss=0.4506318995883932\n",
      "Gradient Descent(37/49): loss=0.4506223454571485\n",
      "Gradient Descent(38/49): loss=0.4506144336810645\n",
      "Gradient Descent(39/49): loss=0.4506078819392895\n",
      "Gradient Descent(40/49): loss=0.4506024564419257\n",
      "Gradient Descent(41/49): loss=0.45059796358755844\n",
      "Gradient Descent(42/49): loss=0.4505942430548574\n",
      "Gradient Descent(43/49): loss=0.4505911620817273\n",
      "Gradient Descent(44/49): loss=0.4505886107278784\n",
      "Gradient Descent(45/49): loss=0.45058649795175637\n",
      "Gradient Descent(46/49): loss=0.45058474836184914\n",
      "Gradient Descent(47/49): loss=0.4505832995264475\n",
      "Gradient Descent(48/49): loss=0.45058209974585084\n",
      "Gradient Descent(49/49): loss=0.4505811062075393\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4914347505267201\n",
      "Gradient Descent(2/49): loss=0.4843418674378945\n",
      "Gradient Descent(3/49): loss=0.4784682509520419\n",
      "Gradient Descent(4/49): loss=0.47360430914010854\n",
      "Gradient Descent(5/49): loss=0.46957647892564436\n",
      "Gradient Descent(6/49): loss=0.4662410327250463\n",
      "Gradient Descent(7/49): loss=0.4634789497263308\n",
      "Gradient Descent(8/49): loss=0.46119166879509477\n",
      "Gradient Descent(9/49): loss=0.45929757145593714\n",
      "Gradient Descent(10/49): loss=0.4577290694493812\n",
      "Gradient Descent(11/49): loss=0.4564301929377522\n",
      "Gradient Descent(12/49): loss=0.4553545932984718\n",
      "Gradient Descent(13/49): loss=0.4544638892371842\n",
      "Gradient Descent(14/49): loss=0.4537262972040326\n",
      "Gradient Descent(15/49): loss=0.4531154972413794\n",
      "Gradient Descent(16/49): loss=0.4526096937923062\n",
      "Gradient Descent(17/49): loss=0.45219083795612874\n",
      "Gradient Descent(18/49): loss=0.4518439834381902\n",
      "Gradient Descent(19/49): loss=0.4515567532118853\n",
      "Gradient Descent(20/49): loss=0.45131889786148194\n",
      "Gradient Descent(21/49): loss=0.4511219298458133\n",
      "Gradient Descent(22/49): loss=0.4509588206320382\n",
      "Gradient Descent(23/49): loss=0.4508237498921109\n",
      "Gradient Descent(24/49): loss=0.45071189781237697\n",
      "Gradient Descent(25/49): loss=0.4506192731051493\n",
      "Gradient Descent(26/49): loss=0.45054257058509406\n",
      "Gradient Descent(27/49): loss=0.4504790532282365\n",
      "Gradient Descent(28/49): loss=0.45042645450502283\n",
      "Gradient Descent(29/49): loss=0.45038289750232935\n",
      "Gradient Descent(30/49): loss=0.45034682794839886\n",
      "Gradient Descent(31/49): loss=0.45031695875078903\n",
      "Gradient Descent(32/49): loss=0.4502922240682485\n",
      "Gradient Descent(33/49): loss=0.4502717412776365\n",
      "Gradient Descent(34/49): loss=0.4502547794787307\n",
      "Gradient Descent(35/49): loss=0.4502407334130571\n",
      "Gradient Descent(36/49): loss=0.4502291018660725\n",
      "Gradient Descent(37/49): loss=0.45021946978201444\n",
      "Gradient Descent(38/49): loss=0.45021149345320627\n",
      "Gradient Descent(39/49): loss=0.4502048882553202\n",
      "Gradient Descent(40/49): loss=0.45019941849095063\n",
      "Gradient Descent(41/49): loss=0.45019488897907617\n",
      "Gradient Descent(42/49): loss=0.45019113809029315\n",
      "Gradient Descent(43/49): loss=0.4501880319792918\n",
      "Gradient Descent(44/49): loss=0.4501854598087715\n",
      "Gradient Descent(45/49): loss=0.45018332979436343\n",
      "Gradient Descent(46/49): loss=0.4501815659294325\n",
      "Gradient Descent(47/49): loss=0.4501801052728833\n",
      "Gradient Descent(48/49): loss=0.4501788957031944\n",
      "Gradient Descent(49/49): loss=0.4501778940585354\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49432217628480873\n",
      "Gradient Descent(2/49): loss=0.4893052512500648\n",
      "Gradient Descent(3/49): loss=0.48487229628936723\n",
      "Gradient Descent(4/49): loss=0.48095533728609235\n",
      "Gradient Descent(5/49): loss=0.4774943123107984\n",
      "Gradient Descent(6/49): loss=0.4744361506426296\n",
      "Gradient Descent(7/49): loss=0.4717339589926383\n",
      "Gradient Descent(8/49): loss=0.46934630245070214\n",
      "Gradient Descent(9/49): loss=0.4672365691302492\n",
      "Gradient Descent(10/49): loss=0.4653724087682978\n",
      "Gradient Descent(11/49): loss=0.46372523667247545\n",
      "Gradient Descent(12/49): loss=0.4622697954086075\n",
      "Gradient Descent(13/49): loss=0.4609837675078535\n",
      "Gradient Descent(14/49): loss=0.4598474332547488\n",
      "Gradient Descent(15/49): loss=0.4588433683087059\n",
      "Gradient Descent(16/49): loss=0.457956176522382\n",
      "Gradient Descent(17/49): loss=0.457172253859985\n",
      "Gradient Descent(18/49): loss=0.4564795797954907\n",
      "Gradient Descent(19/49): loss=0.45586753299210414\n",
      "Gradient Descent(20/49): loss=0.45532672843663186\n",
      "Gradient Descent(21/49): loss=0.4548488735314159\n",
      "Gradient Descent(22/49): loss=0.4544266409371675\n",
      "Gradient Descent(23/49): loss=0.4540535562168897\n",
      "Gradient Descent(24/49): loss=0.45372389855805273\n",
      "Gradient Descent(25/49): loss=0.45343261305070387\n",
      "Gradient Descent(26/49): loss=0.4531752331764103\n",
      "Gradient Descent(27/49): loss=0.45294781231948494\n",
      "Gradient Descent(28/49): loss=0.45274686325030533\n",
      "Gradient Descent(29/49): loss=0.45256930465277867\n",
      "Gradient Descent(30/49): loss=0.45241241387600384\n",
      "Gradient Descent(31/49): loss=0.4522737851856452\n",
      "Gradient Descent(32/49): loss=0.4521512928748447\n",
      "Gradient Descent(33/49): loss=0.45204305866902106\n",
      "Gradient Descent(34/49): loss=0.4519474229247557\n",
      "Gradient Descent(35/49): loss=0.45186291918112254\n",
      "Gradient Descent(36/49): loss=0.4517882516732484\n",
      "Gradient Descent(37/49): loss=0.45172227546329097\n",
      "Gradient Descent(38/49): loss=0.45166397888417253\n",
      "Gradient Descent(39/49): loss=0.45161246802686317\n",
      "Gradient Descent(40/49): loss=0.45156695303334476\n",
      "Gradient Descent(41/49): loss=0.4515267359850721\n",
      "Gradient Descent(42/49): loss=0.4514912002012182\n",
      "Gradient Descent(43/49): loss=0.451459800782605\n",
      "Gradient Descent(44/49): loss=0.4514320562563182\n",
      "Gradient Descent(45/49): loss=0.4514075411928912\n",
      "Gradient Descent(46/49): loss=0.45138587968284755\n",
      "Gradient Descent(47/49): loss=0.45136673957257245\n",
      "Gradient Descent(48/49): loss=0.4513498273711335\n",
      "Gradient Descent(49/49): loss=0.4513348837499421\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4941797459918167\n",
      "Gradient Descent(2/49): loss=0.4890369695501832\n",
      "Gradient Descent(3/49): loss=0.4844928122863534\n",
      "Gradient Descent(4/49): loss=0.48047759492803965\n",
      "Gradient Descent(5/49): loss=0.4769297488702355\n",
      "Gradient Descent(6/49): loss=0.4737948720935546\n",
      "Gradient Descent(7/49): loss=0.47102489497368427\n",
      "Gradient Descent(8/49): loss=0.4685773431905654\n",
      "Gradient Descent(9/49): loss=0.4664146864350003\n",
      "Gradient Descent(10/49): loss=0.4645037629257817\n",
      "Gradient Descent(11/49): loss=0.4628152709130361\n",
      "Gradient Descent(12/49): loss=0.4613233193705749\n",
      "Gradient Descent(13/49): loss=0.4600050309876569\n",
      "Gradient Descent(14/49): loss=0.45884019137250975\n",
      "Gradient Descent(15/49): loss=0.4578109390885651\n",
      "Gradient Descent(16/49): loss=0.45690149177047396\n",
      "Gradient Descent(17/49): loss=0.4560979041202078\n",
      "Gradient Descent(18/49): loss=0.45538785407243226\n",
      "Gradient Descent(19/49): loss=0.45476045385021757\n",
      "Gradient Descent(20/49): loss=0.45420608301386856\n",
      "Gradient Descent(21/49): loss=0.4537162409428701\n",
      "Gradient Descent(22/49): loss=0.4532834164889368\n",
      "Gradient Descent(23/49): loss=0.4529009728014411\n",
      "Gradient Descent(24/49): loss=0.4525630455591702\n",
      "Gradient Descent(25/49): loss=0.4522644530478991\n",
      "Gradient Descent(26/49): loss=0.4520006167049401\n",
      "Gradient Descent(27/49): loss=0.4517674909123016\n",
      "Gradient Descent(28/49): loss=0.4515615009619262\n",
      "Gradient Descent(29/49): loss=0.4513794882417747\n",
      "Gradient Descent(30/49): loss=0.45121866180224857\n",
      "Gradient Descent(31/49): loss=0.4510765555602833\n",
      "Gradient Descent(32/49): loss=0.450950990484883\n",
      "Gradient Descent(33/49): loss=0.4508400411842592\n",
      "Gradient Descent(34/49): loss=0.45074200638222794\n",
      "Gradient Descent(35/49): loss=0.45065538283115314\n",
      "Gradient Descent(36/49): loss=0.45057884226142336\n",
      "Gradient Descent(37/49): loss=0.45051121101401015\n",
      "Gradient Descent(38/49): loss=0.45045145204379605\n",
      "Gradient Descent(39/49): loss=0.4503986490177148\n",
      "Gradient Descent(40/49): loss=0.4503519922638693\n",
      "Gradient Descent(41/49): loss=0.4503107663561714\n",
      "Gradient Descent(42/49): loss=0.45027433914412945\n",
      "Gradient Descent(43/49): loss=0.45024215205956947\n",
      "Gradient Descent(44/49): loss=0.4502137115516522\n",
      "Gradient Descent(45/49): loss=0.4501885815188562\n",
      "Gradient Descent(46/49): loss=0.45016637662187786\n",
      "Gradient Descent(47/49): loss=0.4501467563749079\n",
      "Gradient Descent(48/49): loss=0.450129419924685\n",
      "Gradient Descent(49/49): loss=0.4501141014372682\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49424708364800024\n",
      "Gradient Descent(2/49): loss=0.4891638067593748\n",
      "Gradient Descent(3/49): loss=0.4846722233005857\n",
      "Gradient Descent(4/49): loss=0.4807034601563998\n",
      "Gradient Descent(5/49): loss=0.477196661042197\n",
      "Gradient Descent(6/49): loss=0.47409805334488436\n",
      "Gradient Descent(7/49): loss=0.47136012358354124\n",
      "Gradient Descent(8/49): loss=0.46894088884641616\n",
      "Gradient Descent(9/49): loss=0.4668032530326929\n",
      "Gradient Descent(10/49): loss=0.46491443802768856\n",
      "Gradient Descent(11/49): loss=0.46324548108926594\n",
      "Gradient Descent(12/49): loss=0.461770790738477\n",
      "Gradient Descent(13/49): loss=0.46046775434451753\n",
      "Gradient Descent(14/49): loss=0.45931639138681674\n",
      "Gradient Descent(15/49): loss=0.4582990470773908\n",
      "Gradient Descent(16/49): loss=0.4574001216455825\n",
      "Gradient Descent(17/49): loss=0.4566058311340368\n",
      "Gradient Descent(18/49): loss=0.4559039960380347\n",
      "Gradient Descent(19/49): loss=0.45528385454720716\n",
      "Gradient Descent(20/49): loss=0.45473589752591237\n",
      "Gradient Descent(21/49): loss=0.45425172270189607\n",
      "Gradient Descent(22/49): loss=0.4538239058273957\n",
      "Gradient Descent(23/49): loss=0.45344588683708703\n",
      "Gradient Descent(24/49): loss=0.4531118692572501\n",
      "Gradient Descent(25/49): loss=0.4528167313237058\n",
      "Gradient Descent(26/49): loss=0.45255594744562627\n",
      "Gradient Descent(27/49): loss=0.45232551881095534\n",
      "Gradient Descent(28/49): loss=0.4521219120693602\n",
      "Gradient Descent(29/49): loss=0.45194200515248667\n",
      "Gradient Descent(30/49): loss=0.4517830394007373\n",
      "Gradient Descent(31/49): loss=0.45164257726249135\n",
      "Gradient Descent(32/49): loss=0.4515184649171374\n",
      "Gradient Descent(33/49): loss=0.45140879924878263\n",
      "Gradient Descent(34/49): loss=0.45131189866422444\n",
      "Gradient Descent(35/49): loss=0.4512262773077088\n",
      "Gradient Descent(36/49): loss=0.4511506222770915\n",
      "Gradient Descent(37/49): loss=0.4510837734920381\n",
      "Gradient Descent(38/49): loss=0.45102470590556476\n",
      "Gradient Descent(39/49): loss=0.450972513786157\n",
      "Gradient Descent(40/49): loss=0.45092639682944835\n",
      "Gradient Descent(41/49): loss=0.4508856478865006\n",
      "Gradient Descent(42/49): loss=0.4508496421205119\n",
      "Gradient Descent(43/49): loss=0.4508178274256844\n",
      "Gradient Descent(44/49): loss=0.45078971596133466\n",
      "Gradient Descent(45/49): loss=0.45076487667143533\n",
      "Gradient Descent(46/49): loss=0.45074292867488003\n",
      "Gradient Descent(47/49): loss=0.4507235354251244\n",
      "Gradient Descent(48/49): loss=0.45070639954963976\n",
      "Gradient Descent(49/49): loss=0.45069125829006174\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4942001452083202\n",
      "Gradient Descent(2/49): loss=0.4890753935143876\n",
      "Gradient Descent(3/49): loss=0.4845471629176299\n",
      "Gradient Descent(4/49): loss=0.4805460183623335\n",
      "Gradient Descent(5/49): loss=0.47701060703328063\n",
      "Gradient Descent(6/49): loss=0.4738867175829236\n",
      "Gradient Descent(7/49): loss=0.47112644886459015\n",
      "Gradient Descent(8/49): loss=0.46868747542507094\n",
      "Gradient Descent(9/49): loss=0.46653239849391315\n",
      "Gradient Descent(10/49): loss=0.4646281725175407\n",
      "Gradient Descent(11/49): loss=0.4629455984448191\n",
      "Gradient Descent(12/49): loss=0.4614588759941626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=0.46014520803676245\n",
      "Gradient Descent(14/49): loss=0.4589844510296031\n",
      "Gradient Descent(15/49): loss=0.45795880613807766\n",
      "Gradient Descent(16/49): loss=0.4570525463119261\n",
      "Gradient Descent(17/49): loss=0.4562517751295374\n",
      "Gradient Descent(18/49): loss=0.4555442137127794\n",
      "Gradient Descent(19/49): loss=0.4549190124449317\n",
      "Gradient Descent(20/49): loss=0.4543665846046611\n",
      "Gradient Descent(21/49): loss=0.453878459364999\n",
      "Gradient Descent(22/49): loss=0.4534471519032335\n",
      "Gradient Descent(23/49): loss=0.45306604863001676\n",
      "Gradient Descent(24/49): loss=0.45272930577780285\n",
      "Gradient Descent(25/49): loss=0.4524317597935866\n",
      "Gradient Descent(26/49): loss=0.45216884816193287\n",
      "Gradient Descent(27/49): loss=0.451936539444204\n",
      "Gradient Descent(28/49): loss=0.45173127146121844\n",
      "Gradient Descent(29/49): loss=0.45154989667145273\n",
      "Gradient Descent(30/49): loss=0.4513896339072156\n",
      "Gradient Descent(31/49): loss=0.45124802572873574\n",
      "Gradient Descent(32/49): loss=0.45112290074223094\n",
      "Gradient Descent(33/49): loss=0.4510123403041552\n",
      "Gradient Descent(34/49): loss=0.4509146491010716\n",
      "Gradient Descent(35/49): loss=0.450828329154027\n",
      "Gradient Descent(36/49): loss=0.4507520568488182\n",
      "Gradient Descent(37/49): loss=0.4506846626399358\n",
      "Gradient Descent(38/49): loss=0.45062511311696724\n",
      "Gradient Descent(39/49): loss=0.45057249515847225\n",
      "Gradient Descent(40/49): loss=0.45052600193034603\n",
      "Gradient Descent(41/49): loss=0.45048492051397404\n",
      "Gradient Descent(42/49): loss=0.45044862097446714\n",
      "Gradient Descent(43/49): loss=0.4504165467013593\n",
      "Gradient Descent(44/49): loss=0.4503882058736412\n",
      "Gradient Descent(45/49): loss=0.45036316391826936\n",
      "Gradient Descent(46/49): loss=0.4503410368465029\n",
      "Gradient Descent(47/49): loss=0.4503214853658897\n",
      "Gradient Descent(48/49): loss=0.4503042096776203\n",
      "Gradient Descent(49/49): loss=0.45028894487946525\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4971171874435758\n",
      "Gradient Descent(2/49): loss=0.49440474910923365\n",
      "Gradient Descent(3/49): loss=0.49185261588045215\n",
      "Gradient Descent(4/49): loss=0.48945131372549616\n",
      "Gradient Descent(5/49): loss=0.48719192852790016\n",
      "Gradient Descent(6/49): loss=0.48506607299547627\n",
      "Gradient Descent(7/49): loss=0.48306585552501735\n",
      "Gradient Descent(8/49): loss=0.481183850907066\n",
      "Gradient Descent(9/49): loss=0.47941307276203166\n",
      "Gradient Descent(10/49): loss=0.47774694760537295\n",
      "Gradient Descent(11/49): loss=0.47617929044547314\n",
      "Gradient Descent(12/49): loss=0.4747042818237219\n",
      "Gradient Descent(13/49): loss=0.47331644621151686\n",
      "Gradient Descent(14/49): loss=0.4720106316839898\n",
      "Gradient Descent(15/49): loss=0.47078199079504335\n",
      "Gradient Descent(16/49): loss=0.4696259625826343\n",
      "Gradient Descent(17/49): loss=0.4685382556375762\n",
      "Gradient Descent(18/49): loss=0.4675148321729701\n",
      "Gradient Descent(19/49): loss=0.46655189303512135\n",
      "Gradient Descent(20/49): loss=0.4656458636003231\n",
      "Gradient Descent(21/49): loss=0.4647933805051196\n",
      "Gradient Descent(22/49): loss=0.46399127916084115\n",
      "Gradient Descent(23/49): loss=0.46323658200601137\n",
      "Gradient Descent(24/49): loss=0.4625264874530326\n",
      "Gradient Descent(25/49): loss=0.46185835948813364\n",
      "Gradient Descent(26/49): loss=0.4612297178859605\n",
      "Gradient Descent(27/49): loss=0.4606382290024763\n",
      "Gradient Descent(28/49): loss=0.460081697112005\n",
      "Gradient Descent(29/49): loss=0.45955805625626134\n",
      "Gradient Descent(30/49): loss=0.45906536257509134\n",
      "Gradient Descent(31/49): loss=0.4586017870904797\n",
      "Gradient Descent(32/49): loss=0.4581656089170083\n",
      "Gradient Descent(33/49): loss=0.4577552088735886\n",
      "Gradient Descent(34/49): loss=0.45736906347273537\n",
      "Gradient Descent(35/49): loss=0.4570057392650719\n",
      "Gradient Descent(36/49): loss=0.4566638875180819\n",
      "Gradient Descent(37/49): loss=0.4563422392093392\n",
      "Gradient Descent(38/49): loss=0.45603960031564217\n",
      "Gradient Descent(39/49): loss=0.4557548473805632\n",
      "Gradient Descent(40/49): loss=0.45548692334394747\n",
      "Gradient Descent(41/49): loss=0.45523483361789563\n",
      "Gradient Descent(42/49): loss=0.45499764239465285\n",
      "Gradient Descent(43/49): loss=0.45477446917270475\n",
      "Gradient Descent(44/49): loss=0.4545644854881732\n",
      "Gradient Descent(45/49): loss=0.45436691183939804\n",
      "Gradient Descent(46/49): loss=0.4541810147932648\n",
      "Gradient Descent(47/49): loss=0.4540061042625583\n",
      "Gradient Descent(48/49): loss=0.45384153094421653\n",
      "Gradient Descent(49/49): loss=0.4536866839089887\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49704487103192757\n",
      "Gradient Descent(2/49): loss=0.4942643901858701\n",
      "Gradient Descent(3/49): loss=0.49164823575780836\n",
      "Gradient Descent(4/49): loss=0.48918669605645165\n",
      "Gradient Descent(5/49): loss=0.4868706333514419\n",
      "Gradient Descent(6/49): loss=0.4846914499523035\n",
      "Gradient Descent(7/49): loss=0.48264105629204734\n",
      "Gradient Descent(8/49): loss=0.4807118408971143\n",
      "Gradient Descent(9/49): loss=0.4788966421320196\n",
      "Gradient Descent(10/49): loss=0.4771887216139442\n",
      "Gradient Descent(11/49): loss=0.4755817391984883\n",
      "Gradient Descent(12/49): loss=0.47406972944378567\n",
      "Gradient Descent(13/49): loss=0.4726470794655869\n",
      "Gradient Descent(14/49): loss=0.47130850810110125\n",
      "Gradient Descent(15/49): loss=0.47004904630425254\n",
      "Gradient Descent(16/49): loss=0.4688640186996009\n",
      "Gradient Descent(17/49): loss=0.46774902622638137\n",
      "Gradient Descent(18/49): loss=0.4666999298083291\n",
      "Gradient Descent(19/49): loss=0.4657128349885845\n",
      "Gradient Descent(20/49): loss=0.4647840774726863\n",
      "Gradient Descent(21/49): loss=0.4639102095259771\n",
      "Gradient Descent(22/49): loss=0.4630879871749194\n",
      "Gradient Descent(23/49): loss=0.4623143581648091\n",
      "Gradient Descent(24/49): loss=0.4615864506291971\n",
      "Gradient Descent(25/49): loss=0.46090156242894026\n",
      "Gradient Descent(26/49): loss=0.4602571511213163\n",
      "Gradient Descent(27/49): loss=0.45965082452197464\n",
      "Gradient Descent(28/49): loss=0.4590803318246531\n",
      "Gradient Descent(29/49): loss=0.45854355524574353\n",
      "Gradient Descent(30/49): loss=0.45803850216264674\n",
      "Gradient Descent(31/49): loss=0.45756329771676224\n",
      "Gradient Descent(32/49): loss=0.4571161778536291\n",
      "Gradient Descent(33/49): loss=0.4566954827744076\n",
      "Gradient Descent(34/49): loss=0.4562996507743667\n",
      "Gradient Descent(35/49): loss=0.4559272124455295\n",
      "Gradient Descent(36/49): loss=0.45557678522192563\n",
      "Gradient Descent(37/49): loss=0.45524706824723726\n",
      "Gradient Descent(38/49): loss=0.4549368375457529\n",
      "Gradient Descent(39/49): loss=0.4546449414787265\n",
      "Gradient Descent(40/49): loss=0.4543702964692605\n",
      "Gradient Descent(41/49): loss=0.454111882979855\n",
      "Gradient Descent(42/49): loss=0.45386874172767283\n",
      "Gradient Descent(43/49): loss=0.45363997012349466\n",
      "Gradient Descent(44/49): loss=0.4534247189211235\n",
      "Gradient Descent(45/49): loss=0.4532221890648124\n",
      "Gradient Descent(46/49): loss=0.45303162872300967\n",
      "Gradient Descent(47/49): loss=0.4528523304974076\n",
      "Gradient Descent(48/49): loss=0.45268362879693835\n",
      "Gradient Descent(49/49): loss=0.4525248973669665\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4970790605120003\n",
      "Gradient Descent(2/49): loss=0.4943307485477436\n",
      "Gradient Descent(3/49): loss=0.4917448618205673\n",
      "Gradient Descent(4/49): loss=0.4893118009989687\n",
      "Gradient Descent(5/49): loss=0.48702253407193247\n",
      "Gradient Descent(6/49): loss=0.4848685628202852\n",
      "Gradient Descent(7/49): loss=0.4828418912696047\n",
      "Gradient Descent(8/49): loss=0.48093499600757317\n",
      "Gradient Descent(9/49): loss=0.47914079825552186\n",
      "Gradient Descent(10/49): loss=0.477452637590618\n",
      "Gradient Descent(11/49): loss=0.47586424722101417\n",
      "Gradient Descent(12/49): loss=0.4743697307222494\n",
      "Gradient Descent(13/49): loss=0.4729635401485648\n",
      "Gradient Descent(14/49): loss=0.4716404554377832\n",
      "Gradient Descent(15/49): loss=0.47039556503340857\n",
      "Gradient Descent(16/49): loss=0.46922424765193516\n",
      "Gradient Descent(17/49): loss=0.468122155127705\n",
      "Gradient Descent(18/49): loss=0.467085196271657\n",
      "Gradient Descent(19/49): loss=0.4661095216840029\n",
      "Gradient Descent(20/49): loss=0.4651915094644777\n",
      "Gradient Descent(21/49): loss=0.46432775176712576\n",
      "Gradient Descent(22/49): loss=0.46351504214968825\n",
      "Gradient Descent(23/49): loss=0.4627503636706411\n",
      "Gradient Descent(24/49): loss=0.4620308776897066\n",
      "Gradient Descent(25/49): loss=0.4613539133302441\n",
      "Gradient Descent(26/49): loss=0.46071695756442776\n",
      "Gradient Descent(27/49): loss=0.46011764588436926\n",
      "Gradient Descent(28/49): loss=0.45955375352460404\n",
      "Gradient Descent(29/49): loss=0.45902318720329943\n",
      "Gradient Descent(30/49): loss=0.45852397735158384\n",
      "Gradient Descent(31/49): loss=0.45805427080210454\n",
      "Gradient Descent(32/49): loss=0.4576123239097007\n",
      "Gradient Descent(33/49): loss=0.45719649607863805\n",
      "Gradient Descent(34/49): loss=0.4568052436723906\n",
      "Gradient Descent(35/49): loss=0.4564371142833519\n",
      "Gradient Descent(36/49): loss=0.4560907413412053\n",
      "Gradient Descent(37/49): loss=0.45576483903993986\n",
      "Gradient Descent(38/49): loss=0.4554581975646798\n",
      "Gradient Descent(39/49): loss=0.45516967860060736\n",
      "Gradient Descent(40/49): loss=0.45489821110731105\n",
      "Gradient Descent(41/49): loss=0.45464278734286867\n",
      "Gradient Descent(42/49): loss=0.45440245912290533\n",
      "Gradient Descent(43/49): loss=0.4541763343007417\n",
      "Gradient Descent(44/49): loss=0.45396357345556765\n",
      "Gradient Descent(45/49): loss=0.45376338677634376\n",
      "Gradient Descent(46/49): loss=0.45357503112986264\n",
      "Gradient Descent(47/49): loss=0.45339780730208756\n",
      "Gradient Descent(48/49): loss=0.4532310574025341\n",
      "Gradient Descent(49/49): loss=0.4530741624220447\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4970552283660803\n",
      "Gradient Descent(2/49): loss=0.4942844927357194\n",
      "Gradient Descent(3/49): loss=0.4916775075811168\n",
      "Gradient Descent(4/49): loss=0.48922459524914746\n",
      "Gradient Descent(5/49): loss=0.48691665003600826\n",
      "Gradient Descent(6/49): loss=0.4847451043849568\n",
      "Gradient Descent(7/49): loss=0.4827018970818892\n",
      "Gradient Descent(8/49): loss=0.480779443330433\n",
      "Gradient Descent(9/49): loss=0.4789706065956847\n",
      "Gradient Descent(10/49): loss=0.4772686721119574\n",
      "Gradient Descent(11/49): loss=0.47566732195622086\n",
      "Gradient Descent(12/49): loss=0.4741606115946873\n",
      "Gradient Descent(13/49): loss=0.47274294781551973\n",
      "Gradient Descent(14/49): loss=0.47140906796570226\n",
      "Gradient Descent(15/49): loss=0.4701540204150114\n",
      "Gradient Descent(16/49): loss=0.4689731461745639\n",
      "Gradient Descent(17/49): loss=0.46786206160172666\n",
      "Gradient Descent(18/49): loss=0.46681664212714324\n",
      "Gradient Descent(19/49): loss=0.46583300694350976\n",
      "Gradient Descent(20/49): loss=0.4649075045992275\n",
      "Gradient Descent(21/49): loss=0.46403669944349374\n",
      "Gradient Descent(22/49): loss=0.46321735887246285\n",
      "Gradient Descent(23/49): loss=0.4624464413291815\n",
      "Gradient Descent(24/49): loss=0.46172108501270814\n",
      "Gradient Descent(25/49): loss=0.4610385972545371\n",
      "Gradient Descent(26/49): loss=0.46039644452287515\n",
      "Gradient Descent(27/49): loss=0.45979224301765215\n",
      "Gradient Descent(28/49): loss=0.4592237498213885\n",
      "Gradient Descent(29/49): loss=0.45868885457302366\n",
      "Gradient Descent(30/49): loss=0.45818557163383766\n",
      "Gradient Descent(31/49): loss=0.4577120327163571\n",
      "Gradient Descent(32/49): loss=0.45726647994890074\n",
      "Gradient Descent(33/49): loss=0.45684725935000065\n",
      "Gradient Descent(34/49): loss=0.4564528146884963\n",
      "Gradient Descent(35/49): loss=0.4560816817064857\n",
      "Gradient Descent(36/49): loss=0.45573248268371264\n",
      "Gradient Descent(37/49): loss=0.4554039213231848\n",
      "Gradient Descent(38/49): loss=0.4550947779390655\n",
      "Gradient Descent(39/49): loss=0.45480390492894607\n",
      "Gradient Descent(40/49): loss=0.4545302225137252\n",
      "Gradient Descent(41/49): loss=0.45427271472924413\n",
      "Gradient Descent(42/49): loss=0.45403042565482526\n",
      "Gradient Descent(43/49): loss=0.45380245586470486\n",
      "Gradient Descent(44/49): loss=0.453587959089181\n",
      "Gradient Descent(45/49): loss=0.45338613907309017\n",
      "Gradient Descent(46/49): loss=0.45319624661995045\n",
      "Gradient Descent(47/49): loss=0.4530175768107914\n",
      "Gradient Descent(48/49): loss=0.45284946638735346\n",
      "Gradient Descent(49/49): loss=0.452691291289941\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5000000000000001\n",
      "Gradient Descent(2/49): loss=0.5000000000000029\n",
      "Gradient Descent(3/49): loss=0.49999999999999956\n",
      "Gradient Descent(4/49): loss=0.500000000000002\n",
      "Gradient Descent(5/49): loss=0.4999999999999998\n",
      "Gradient Descent(6/49): loss=0.5000000000000032\n",
      "Gradient Descent(7/49): loss=0.4999999999999992\n",
      "Gradient Descent(8/49): loss=0.5000000000000023\n",
      "Gradient Descent(9/49): loss=0.4999999999999996\n",
      "Gradient Descent(10/49): loss=0.5000000000000034\n",
      "Gradient Descent(11/49): loss=0.499999999999999\n",
      "Gradient Descent(12/49): loss=0.5000000000000023\n",
      "Gradient Descent(13/49): loss=0.49999999999999967\n",
      "Gradient Descent(14/49): loss=0.5000000000000034\n",
      "Gradient Descent(15/49): loss=0.4999999999999994\n",
      "Gradient Descent(16/49): loss=0.500000000000002\n",
      "Gradient Descent(17/49): loss=0.4999999999999996\n",
      "Gradient Descent(18/49): loss=0.5000000000000034\n",
      "Gradient Descent(19/49): loss=0.499999999999999\n",
      "Gradient Descent(20/49): loss=0.5000000000000023\n",
      "Gradient Descent(21/49): loss=0.49999999999999956\n",
      "Gradient Descent(22/49): loss=0.5000000000000034\n",
      "Gradient Descent(23/49): loss=0.499999999999999\n",
      "Gradient Descent(24/49): loss=0.5000000000000023\n",
      "Gradient Descent(25/49): loss=0.49999999999999956\n",
      "Gradient Descent(26/49): loss=0.5000000000000034\n",
      "Gradient Descent(27/49): loss=0.499999999999999\n",
      "Gradient Descent(28/49): loss=0.5000000000000023\n",
      "Gradient Descent(29/49): loss=0.49999999999999956\n",
      "Gradient Descent(30/49): loss=0.5000000000000034\n",
      "Gradient Descent(31/49): loss=0.499999999999999\n",
      "Gradient Descent(32/49): loss=0.5000000000000023\n",
      "Gradient Descent(33/49): loss=0.49999999999999956\n",
      "Gradient Descent(34/49): loss=0.5000000000000034\n",
      "Gradient Descent(35/49): loss=0.499999999999999\n",
      "Gradient Descent(36/49): loss=0.5000000000000023\n",
      "Gradient Descent(37/49): loss=0.49999999999999956\n",
      "Gradient Descent(38/49): loss=0.5000000000000034\n",
      "Gradient Descent(39/49): loss=0.499999999999999\n",
      "Gradient Descent(40/49): loss=0.5000000000000023\n",
      "Gradient Descent(41/49): loss=0.49999999999999956\n",
      "Gradient Descent(42/49): loss=0.5000000000000034\n",
      "Gradient Descent(43/49): loss=0.499999999999999\n",
      "Gradient Descent(44/49): loss=0.5000000000000023\n",
      "Gradient Descent(45/49): loss=0.49999999999999956\n",
      "Gradient Descent(46/49): loss=0.5000000000000034\n",
      "Gradient Descent(47/49): loss=0.499999999999999\n",
      "Gradient Descent(48/49): loss=0.5000000000000023\n",
      "Gradient Descent(49/49): loss=0.49999999999999956\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5000000000000001\n",
      "Gradient Descent(2/49): loss=0.5000000000000061\n",
      "Gradient Descent(3/49): loss=0.5000000000000019\n",
      "Gradient Descent(4/49): loss=0.5000000000000053\n",
      "Gradient Descent(5/49): loss=0.5000000000000024\n",
      "Gradient Descent(6/49): loss=0.5000000000000058\n",
      "Gradient Descent(7/49): loss=0.5000000000000022\n",
      "Gradient Descent(8/49): loss=0.5000000000000059\n",
      "Gradient Descent(9/49): loss=0.5000000000000021\n",
      "Gradient Descent(10/49): loss=0.5000000000000052\n",
      "Gradient Descent(11/49): loss=0.5000000000000024\n",
      "Gradient Descent(12/49): loss=0.5000000000000056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=0.5000000000000022\n",
      "Gradient Descent(14/49): loss=0.5000000000000061\n",
      "Gradient Descent(15/49): loss=0.5000000000000019\n",
      "Gradient Descent(16/49): loss=0.5000000000000052\n",
      "Gradient Descent(17/49): loss=0.5000000000000024\n",
      "Gradient Descent(18/49): loss=0.5000000000000056\n",
      "Gradient Descent(19/49): loss=0.5000000000000022\n",
      "Gradient Descent(20/49): loss=0.5000000000000061\n",
      "Gradient Descent(21/49): loss=0.5000000000000019\n",
      "Gradient Descent(22/49): loss=0.5000000000000052\n",
      "Gradient Descent(23/49): loss=0.5000000000000024\n",
      "Gradient Descent(24/49): loss=0.5000000000000056\n",
      "Gradient Descent(25/49): loss=0.5000000000000022\n",
      "Gradient Descent(26/49): loss=0.5000000000000061\n",
      "Gradient Descent(27/49): loss=0.5000000000000019\n",
      "Gradient Descent(28/49): loss=0.5000000000000052\n",
      "Gradient Descent(29/49): loss=0.5000000000000024\n",
      "Gradient Descent(30/49): loss=0.5000000000000056\n",
      "Gradient Descent(31/49): loss=0.5000000000000022\n",
      "Gradient Descent(32/49): loss=0.5000000000000061\n",
      "Gradient Descent(33/49): loss=0.5000000000000019\n",
      "Gradient Descent(34/49): loss=0.5000000000000052\n",
      "Gradient Descent(35/49): loss=0.5000000000000024\n",
      "Gradient Descent(36/49): loss=0.5000000000000056\n",
      "Gradient Descent(37/49): loss=0.5000000000000022\n",
      "Gradient Descent(38/49): loss=0.5000000000000061\n",
      "Gradient Descent(39/49): loss=0.5000000000000019\n",
      "Gradient Descent(40/49): loss=0.5000000000000052\n",
      "Gradient Descent(41/49): loss=0.5000000000000024\n",
      "Gradient Descent(42/49): loss=0.5000000000000056\n",
      "Gradient Descent(43/49): loss=0.5000000000000022\n",
      "Gradient Descent(44/49): loss=0.5000000000000061\n",
      "Gradient Descent(45/49): loss=0.5000000000000019\n",
      "Gradient Descent(46/49): loss=0.5000000000000052\n",
      "Gradient Descent(47/49): loss=0.5000000000000024\n",
      "Gradient Descent(48/49): loss=0.5000000000000056\n",
      "Gradient Descent(49/49): loss=0.5000000000000022\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5000000000000001\n",
      "Gradient Descent(2/49): loss=0.5000000000000056\n",
      "Gradient Descent(3/49): loss=0.5000000000000024\n",
      "Gradient Descent(4/49): loss=0.5000000000000053\n",
      "Gradient Descent(5/49): loss=0.5000000000000024\n",
      "Gradient Descent(6/49): loss=0.5000000000000053\n",
      "Gradient Descent(7/49): loss=0.5000000000000026\n",
      "Gradient Descent(8/49): loss=0.5000000000000052\n",
      "Gradient Descent(9/49): loss=0.5000000000000026\n",
      "Gradient Descent(10/49): loss=0.5000000000000052\n",
      "Gradient Descent(11/49): loss=0.5000000000000026\n",
      "Gradient Descent(12/49): loss=0.5000000000000052\n",
      "Gradient Descent(13/49): loss=0.5000000000000026\n",
      "Gradient Descent(14/49): loss=0.5000000000000052\n",
      "Gradient Descent(15/49): loss=0.5000000000000026\n",
      "Gradient Descent(16/49): loss=0.5000000000000052\n",
      "Gradient Descent(17/49): loss=0.5000000000000026\n",
      "Gradient Descent(18/49): loss=0.5000000000000052\n",
      "Gradient Descent(19/49): loss=0.5000000000000026\n",
      "Gradient Descent(20/49): loss=0.5000000000000052\n",
      "Gradient Descent(21/49): loss=0.5000000000000026\n",
      "Gradient Descent(22/49): loss=0.5000000000000052\n",
      "Gradient Descent(23/49): loss=0.5000000000000026\n",
      "Gradient Descent(24/49): loss=0.5000000000000052\n",
      "Gradient Descent(25/49): loss=0.5000000000000026\n",
      "Gradient Descent(26/49): loss=0.5000000000000052\n",
      "Gradient Descent(27/49): loss=0.5000000000000026\n",
      "Gradient Descent(28/49): loss=0.5000000000000052\n",
      "Gradient Descent(29/49): loss=0.5000000000000026\n",
      "Gradient Descent(30/49): loss=0.5000000000000052\n",
      "Gradient Descent(31/49): loss=0.5000000000000026\n",
      "Gradient Descent(32/49): loss=0.5000000000000052\n",
      "Gradient Descent(33/49): loss=0.5000000000000026\n",
      "Gradient Descent(34/49): loss=0.5000000000000052\n",
      "Gradient Descent(35/49): loss=0.5000000000000026\n",
      "Gradient Descent(36/49): loss=0.5000000000000052\n",
      "Gradient Descent(37/49): loss=0.5000000000000026\n",
      "Gradient Descent(38/49): loss=0.5000000000000052\n",
      "Gradient Descent(39/49): loss=0.5000000000000026\n",
      "Gradient Descent(40/49): loss=0.5000000000000052\n",
      "Gradient Descent(41/49): loss=0.5000000000000026\n",
      "Gradient Descent(42/49): loss=0.5000000000000052\n",
      "Gradient Descent(43/49): loss=0.5000000000000026\n",
      "Gradient Descent(44/49): loss=0.5000000000000052\n",
      "Gradient Descent(45/49): loss=0.5000000000000026\n",
      "Gradient Descent(46/49): loss=0.5000000000000052\n",
      "Gradient Descent(47/49): loss=0.5000000000000026\n",
      "Gradient Descent(48/49): loss=0.5000000000000052\n",
      "Gradient Descent(49/49): loss=0.5000000000000026\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5000000000000001\n",
      "Gradient Descent(2/49): loss=0.49999999999999273\n",
      "Gradient Descent(3/49): loss=0.4999999999999982\n",
      "Gradient Descent(4/49): loss=0.4999999999999943\n",
      "Gradient Descent(5/49): loss=0.499999999999998\n",
      "Gradient Descent(6/49): loss=0.49999999999999417\n",
      "Gradient Descent(7/49): loss=0.499999999999998\n",
      "Gradient Descent(8/49): loss=0.4999999999999944\n",
      "Gradient Descent(9/49): loss=0.499999999999998\n",
      "Gradient Descent(10/49): loss=0.49999999999999317\n",
      "Gradient Descent(11/49): loss=0.4999999999999976\n",
      "Gradient Descent(12/49): loss=0.4999999999999932\n",
      "Gradient Descent(13/49): loss=0.4999999999999985\n",
      "Gradient Descent(14/49): loss=0.4999999999999939\n",
      "Gradient Descent(15/49): loss=0.4999999999999983\n",
      "Gradient Descent(16/49): loss=0.49999999999999395\n",
      "Gradient Descent(17/49): loss=0.4999999999999984\n",
      "Gradient Descent(18/49): loss=0.4999999999999943\n",
      "Gradient Descent(19/49): loss=0.4999999999999982\n",
      "Gradient Descent(20/49): loss=0.49999999999999417\n",
      "Gradient Descent(21/49): loss=0.4999999999999982\n",
      "Gradient Descent(22/49): loss=0.4999999999999943\n",
      "Gradient Descent(23/49): loss=0.4999999999999982\n",
      "Gradient Descent(24/49): loss=0.4999999999999943\n",
      "Gradient Descent(25/49): loss=0.499999999999998\n",
      "Gradient Descent(26/49): loss=0.49999999999999417\n",
      "Gradient Descent(27/49): loss=0.499999999999998\n",
      "Gradient Descent(28/49): loss=0.4999999999999944\n",
      "Gradient Descent(29/49): loss=0.499999999999998\n",
      "Gradient Descent(30/49): loss=0.49999999999999317\n",
      "Gradient Descent(31/49): loss=0.4999999999999976\n",
      "Gradient Descent(32/49): loss=0.4999999999999932\n",
      "Gradient Descent(33/49): loss=0.4999999999999985\n",
      "Gradient Descent(34/49): loss=0.4999999999999939\n",
      "Gradient Descent(35/49): loss=0.4999999999999983\n",
      "Gradient Descent(36/49): loss=0.49999999999999395\n",
      "Gradient Descent(37/49): loss=0.4999999999999984\n",
      "Gradient Descent(38/49): loss=0.4999999999999943\n",
      "Gradient Descent(39/49): loss=0.4999999999999982\n",
      "Gradient Descent(40/49): loss=0.49999999999999417\n",
      "Gradient Descent(41/49): loss=0.4999999999999982\n",
      "Gradient Descent(42/49): loss=0.4999999999999943\n",
      "Gradient Descent(43/49): loss=0.4999999999999982\n",
      "Gradient Descent(44/49): loss=0.4999999999999943\n",
      "Gradient Descent(45/49): loss=0.499999999999998\n",
      "Gradient Descent(46/49): loss=0.49999999999999417\n",
      "Gradient Descent(47/49): loss=0.499999999999998\n",
      "Gradient Descent(48/49): loss=0.4999999999999944\n",
      "Gradient Descent(49/49): loss=0.499999999999998\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5029706139540823\n",
      "Gradient Descent(2/49): loss=0.5061221382979697\n",
      "Gradient Descent(3/49): loss=0.5094655904744044\n",
      "Gradient Descent(4/49): loss=0.5130126588883742\n",
      "Gradient Descent(5/49): loss=0.5167757437687588\n",
      "Gradient Descent(6/49): loss=0.5207680005183537\n",
      "Gradient Descent(7/49): loss=0.5250033857040114\n",
      "Gradient Descent(8/49): loss=0.5294967058474728\n",
      "Gradient Descent(9/49): loss=0.5342636691876751\n",
      "Gradient Descent(10/49): loss=0.5393209405952922\n",
      "Gradient Descent(11/49): loss=0.5446861998316269\n",
      "Gradient Descent(12/49): loss=0.5503782033554551\n",
      "Gradient Descent(13/49): loss=0.55641684989389\n",
      "Gradient Descent(14/49): loss=0.5628232500065204\n",
      "Gradient Descent(15/49): loss=0.5696197998859966\n",
      "Gradient Descent(16/49): loss=0.576830259653153\n",
      "Gradient Descent(17/49): loss=0.584479836420112\n",
      "Gradient Descent(18/49): loss=0.5925952724121853\n",
      "Gradient Descent(19/49): loss=0.6012049384561716\n",
      "Gradient Descent(20/49): loss=0.6103389331622369\n",
      "Gradient Descent(21/49): loss=0.6200291881459071\n",
      "Gradient Descent(22/49): loss=0.6303095796580493\n",
      "Gradient Descent(23/49): loss=0.6412160470133133\n",
      "Gradient Descent(24/49): loss=0.6527867182304875\n",
      "Gradient Descent(25/49): loss=0.6650620433248213\n",
      "Gradient Descent(26/49): loss=0.6780849357173583\n",
      "Gradient Descent(27/49): loss=0.691900922256606\n",
      "Gradient Descent(28/49): loss=0.7065583023761531\n",
      "Gradient Descent(29/49): loss=0.7221083169449689\n",
      "Gradient Descent(30/49): loss=0.7386053274009844\n",
      "Gradient Descent(31/49): loss=0.7561070057937552\n",
      "Gradient Descent(32/49): loss=0.7746745364006565\n",
      "Gradient Descent(33/49): loss=0.7943728296215506\n",
      "Gradient Descent(34/49): loss=0.8152707488996053\n",
      "Gradient Descent(35/49): loss=0.8374413514616904\n",
      "Gradient Descent(36/49): loss=0.8609621437198014\n",
      "Gradient Descent(37/49): loss=0.8859153522263624\n",
      "Gradient Descent(38/49): loss=0.9123882111310874\n",
      "Gradient Descent(39/49): loss=0.9404732671430358\n",
      "Gradient Descent(40/49): loss=0.9702687030661603\n",
      "Gradient Descent(41/49): loss=1.001878681037017\n",
      "Gradient Descent(42/49): loss=1.03541370666626\n",
      "Gradient Descent(43/49): loss=1.0709910153563047\n",
      "Gradient Descent(44/49): loss=1.1087349821455386\n",
      "Gradient Descent(45/49): loss=1.1487775565123162\n",
      "Gradient Descent(46/49): loss=1.1912587236579626\n",
      "Gradient Descent(47/49): loss=1.2363269938828256\n",
      "Gradient Descent(48/49): loss=1.2841399217643865\n",
      "Gradient Descent(49/49): loss=1.3348646569538454\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5030451328960344\n",
      "Gradient Descent(2/49): loss=0.5062757143854432\n",
      "Gradient Descent(3/49): loss=0.5097030382875546\n",
      "Gradient Descent(4/49): loss=0.5133390862152951\n",
      "Gradient Descent(5/49): loss=0.5171965694618339\n",
      "Gradient Descent(6/49): loss=0.5212889734381015\n",
      "Gradient Descent(7/49): loss=0.5256306048165109\n",
      "Gradient Descent(8/49): loss=0.5302366415458762\n",
      "Gradient Descent(9/49): loss=0.5351231859120457\n",
      "Gradient Descent(10/49): loss=0.5403073208301209\n",
      "Gradient Descent(11/49): loss=0.5458071695647085\n",
      "Gradient Descent(12/49): loss=0.551641959087243\n",
      "Gradient Descent(13/49): loss=0.5578320872916919\n",
      "Gradient Descent(14/49): loss=0.5643991943037878\n",
      "Gradient Descent(15/49): loss=0.571366238132918\n",
      "Gradient Descent(16/49): loss=0.5787575749312618\n",
      "Gradient Descent(17/49): loss=0.5865990441406176\n",
      "Gradient Descent(18/49): loss=0.5949180588248324\n",
      "Gradient Descent(19/49): loss=0.6037437015033071\n",
      "Gradient Descent(20/49): loss=0.6131068258208903\n",
      "Gradient Descent(21/49): loss=0.6230401644094026\n",
      "Gradient Descent(22/49): loss=0.633578443317958\n",
      "Gradient Descent(23/49): loss=0.6447585034120691\n",
      "Gradient Descent(24/49): loss=0.6566194291659143\n",
      "Gradient Descent(25/49): loss=0.6692026852981482\n",
      "Gradient Descent(26/49): loss=0.6825522617288747\n",
      "Gradient Descent(27/49): loss=0.6967148273641753\n",
      "Gradient Descent(28/49): loss=0.7117398932467179\n",
      "Gradient Descent(29/49): loss=0.7276799856414458\n",
      "Gradient Descent(30/49): loss=0.7445908296630421\n",
      "Gradient Descent(31/49): loss=0.7625315440855762\n",
      "Gradient Descent(32/49): loss=0.7815648480164334\n",
      "Gradient Descent(33/49): loss=0.8017572801566359\n",
      "Gradient Descent(34/49): loss=0.8231794314142189\n",
      "Gradient Descent(35/49): loss=0.8459061916833608\n",
      "Gradient Descent(36/49): loss=0.8700170116529542\n",
      "Gradient Descent(37/49): loss=0.8955961805586357\n",
      "Gradient Descent(38/49): loss=0.9227331208506971\n",
      "Gradient Descent(39/49): loss=0.9515227008064945\n",
      "Gradient Descent(40/49): loss=0.9820655661817171\n",
      "Gradient Descent(41/49): loss=1.0144684920582547\n",
      "Gradient Descent(42/49): loss=1.0488447561205703\n",
      "Gradient Descent(43/49): loss=1.0853145346643858\n",
      "Gradient Descent(44/49): loss=1.1240053227215108\n",
      "Gradient Descent(45/49): loss=1.1650523797712717\n",
      "Gradient Descent(46/49): loss=1.2085992025954027\n",
      "Gradient Descent(47/49): loss=1.2547980269294663\n",
      "Gradient Descent(48/49): loss=1.3038103596655244\n",
      "Gradient Descent(49/49): loss=1.3558075434652768\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.503009902112\n",
      "Gradient Descent(2/49): loss=0.5062031072626236\n",
      "Gradient Descent(3/49): loss=0.5095907786069143\n",
      "Gradient Descent(4/49): loss=0.5131847591360797\n",
      "Gradient Descent(5/49): loss=0.5169976130794711\n",
      "Gradient Descent(6/49): loss=0.5210426698280076\n",
      "Gradient Descent(7/49): loss=0.5253340705325322\n",
      "Gradient Descent(8/49): loss=0.5298868175399617\n",
      "Gradient Descent(9/49): loss=0.5347168268401546\n",
      "Gradient Descent(10/49): loss=0.5398409837067298\n",
      "Gradient Descent(11/49): loss=0.5452772017264783\n",
      "Gradient Descent(12/49): loss=0.5510444854236333\n",
      "Gradient Descent(13/49): loss=0.5571629966979217\n",
      "Gradient Descent(14/49): loss=0.5636541253088134\n",
      "Gradient Descent(15/49): loss=0.5705405636521055\n",
      "Gradient Descent(16/49): loss=0.5778463860905186\n",
      "Gradient Descent(17/49): loss=0.5855971331154386\n",
      "Gradient Descent(18/49): loss=0.5938199006341527\n",
      "Gradient Descent(19/49): loss=0.6025434346947602\n",
      "Gradient Descent(20/49): loss=0.6117982319796815\n",
      "Gradient Descent(21/49): loss=0.6216166464192364\n",
      "Gradient Descent(22/49): loss=0.6320330022981709\n",
      "Gradient Descent(23/49): loss=0.6430837142501454\n",
      "Gradient Descent(24/49): loss=0.6548074145599738\n",
      "Gradient Descent(25/49): loss=0.6672450882186844\n",
      "Gradient Descent(26/49): loss=0.6804402162032033\n",
      "Gradient Descent(27/49): loss=0.6944389274819551\n",
      "Gradient Descent(28/49): loss=0.7092901602776077\n",
      "Gradient Descent(29/49): loss=0.7250458331504787\n",
      "Gradient Descent(30/49): loss=0.7417610265013386\n",
      "Gradient Descent(31/49): loss=0.7594941751272598\n",
      "Gradient Descent(32/49): loss=0.7783072725045018\n",
      "Gradient Descent(33/49): loss=0.7982660875119997\n",
      "Gradient Descent(34/49): loss=0.8194403943534846\n",
      "Gradient Descent(35/49): loss=0.841904216481605\n",
      "Gradient Descent(36/49): loss=0.8657360853773329\n",
      "Gradient Descent(37/49): loss=0.8910193150888341\n",
      "Gradient Descent(38/49): loss=0.9178422934897801\n",
      "Gradient Descent(39/49): loss=0.9462987912753558\n",
      "Gradient Descent(40/49): loss=0.9764882897759979\n",
      "Gradient Descent(41/49): loss=1.008516328735416\n",
      "Gradient Descent(42/49): loss=1.0424948752674625\n",
      "Gradient Descent(43/49): loss=1.0785427152832923\n",
      "Gradient Descent(44/49): loss=1.1167858687559888\n",
      "Gradient Descent(45/49): loss=1.1573580302752924\n",
      "Gradient Descent(46/49): loss=1.200401036431084\n",
      "Gradient Descent(47/49): loss=1.2460653616618567\n",
      "Gradient Descent(48/49): loss=1.2945106442989807\n",
      "Gradient Descent(49/49): loss=1.3459062446487393\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.50303446011008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/49): loss=0.5062537188408719\n",
      "Gradient Descent(3/49): loss=0.509669030428363\n",
      "Gradient Descent(4/49): loss=0.5132923344915259\n",
      "Gradient Descent(5/49): loss=0.5171362977721452\n",
      "Gradient Descent(6/49): loss=0.5212143584165592\n",
      "Gradient Descent(7/49): loss=0.5255407729542158\n",
      "Gradient Descent(8/49): loss=0.530130666137219\n",
      "Gradient Descent(9/49): loss=0.5350000838150555\n",
      "Gradient Descent(10/49): loss=0.5401660490294599\n",
      "Gradient Descent(11/49): loss=0.5456466215254441\n",
      "Gradient Descent(12/49): loss=0.5514609608864297\n",
      "Gradient Descent(13/49): loss=0.5576293935145045\n",
      "Gradient Descent(14/49): loss=0.564173483689618\n",
      "Gradient Descent(15/49): loss=0.5711161089564124\n",
      "Gradient Descent(16/49): loss=0.5784815401019305\n",
      "Gradient Descent(17/49): loss=0.5862955260042335\n",
      "Gradient Descent(18/49): loss=0.5945853836479722\n",
      "Gradient Descent(19/49): loss=0.6033800936221984\n",
      "Gradient Descent(20/49): loss=0.6127104014338624\n",
      "Gradient Descent(21/49): loss=0.6226089249912654\n",
      "Gradient Descent(22/49): loss=0.6331102686332962\n",
      "Gradient Descent(23/49): loss=0.6442511441031374\n",
      "Gradient Descent(24/49): loss=0.6560704988890862\n",
      "Gradient Descent(25/49): loss=0.668609652381532\n",
      "Gradient Descent(26/49): loss=0.6819124403216196\n",
      "Gradient Descent(27/49): loss=0.696025368047276\n",
      "Gradient Descent(28/49): loss=0.7109977730714276\n",
      "Gradient Descent(29/49): loss=0.726881997561546\n",
      "Gradient Descent(30/49): loss=0.743733571323116\n",
      "Gradient Descent(31/49): loss=0.7616114059267675\n",
      "Gradient Descent(32/49): loss=0.7805780006578081\n",
      "Gradient Descent(33/49): loss=0.8006996610079384\n",
      "Gradient Descent(34/49): loss=0.8220467304734167\n",
      "Gradient Descent(35/49): loss=0.8446938364693356\n",
      "Gradient Descent(36/49): loss=0.8687201512204171\n",
      "Gradient Descent(37/49): loss=0.8942096685397983\n",
      "Gradient Descent(38/49): loss=0.9212514974639782\n",
      "Gradient Descent(39/49): loss=0.9499401737695443\n",
      "Gradient Descent(40/49): loss=0.9803759904621535\n",
      "Gradient Descent(41/49): loss=1.0126653483913366\n",
      "Gradient Descent(42/49): loss=1.0469211282184256\n",
      "Gradient Descent(43/49): loss=1.0832630850370855\n",
      "Gradient Descent(44/49): loss=1.1218182670258623\n",
      "Gradient Descent(45/49): loss=1.1627214595978241\n",
      "Gradient Descent(46/49): loss=1.206115656597332\n",
      "Gradient Descent(47/49): loss=1.2521525601941574\n",
      "Gradient Descent(48/49): loss=1.3009931112199906\n",
      "Gradient Descent(49/49): loss=1.3528080518034644\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.506029029305822\n",
      "Gradient Descent(2/49): loss=0.5128032466338508\n",
      "Gradient Descent(3/49): loss=0.5204147572236157\n",
      "Gradient Descent(4/49): loss=0.5289670505222809\n",
      "Gradient Descent(5/49): loss=0.5385764072726628\n",
      "Gradient Descent(6/49): loss=0.5493734805173782\n",
      "Gradient Descent(7/49): loss=0.561505072015146\n",
      "Gradient Descent(8/49): loss=0.5751361282220373\n",
      "Gradient Descent(9/49): loss=0.5904519829761015\n",
      "Gradient Descent(10/49): loss=0.6076608773777554\n",
      "Gradient Descent(11/49): loss=0.6269967911274772\n",
      "Gradient Descent(12/49): loss=0.6487226238166717\n",
      "Gradient Descent(13/49): loss=0.6731337694262242\n",
      "Gradient Descent(14/49): loss=0.700562132633157\n",
      "Gradient Descent(15/49): loss=0.7313806415324602\n",
      "Gradient Descent(16/49): loss=0.7660083181317098\n",
      "Gradient Descent(17/49): loss=0.804915975558625\n",
      "Gradient Descent(18/49): loss=0.8486326194434858\n",
      "Gradient Descent(19/49): loss=0.8977526405125821\n",
      "Gradient Descent(20/49): loss=0.9529438961857111\n",
      "Gradient Descent(21/49): loss=1.0149567910600907\n",
      "Gradient Descent(22/49): loss=1.084634479740956\n",
      "Gradient Descent(23/49): loss=1.1629243307427704\n",
      "Gradient Descent(24/49): loss=1.250890807328373\n",
      "Gradient Descent(25/49): loss=1.3497299404198948\n",
      "Gradient Descent(26/49): loss=1.4607855903616753\n",
      "Gradient Descent(27/49): loss=1.5855677186362958\n",
      "Gradient Descent(28/49): loss=1.7257729179656418\n",
      "Gradient Descent(29/49): loss=1.883307479932017\n",
      "Gradient Descent(30/49): loss=2.0603133137573573\n",
      "Gradient Descent(31/49): loss=2.2591970686433487\n",
      "Gradient Descent(32/49): loss=2.4826628556336283\n",
      "Gradient Descent(33/49): loss=2.7337490138960603\n",
      "Gradient Descent(34/49): loss=3.0158694213190937\n",
      "Gradient Descent(35/49): loss=3.332859911099748\n",
      "Gradient Descent(36/49): loss=3.68903042541777\n",
      "Gradient Descent(37/49): loss=4.089223615305451\n",
      "Gradient Descent(38/49): loss=4.538880683462846\n",
      "Gradient Descent(39/49): loss=5.044115365244449\n",
      "Gradient Descent(40/49): loss=5.611797053693981\n",
      "Gradient Descent(41/49): loss=6.2496441988368305\n",
      "Gradient Descent(42/49): loss=6.966329251118769\n",
      "Gradient Descent(43/49): loss=7.771596575862771\n",
      "Gradient Descent(44/49): loss=8.676394941944878\n",
      "Gradient Descent(45/49): loss=9.693026386074088\n",
      "Gradient Descent(46/49): loss=10.83531347669731\n",
      "Gradient Descent(47/49): loss=12.118787251722596\n",
      "Gradient Descent(48/49): loss=13.560898385342504\n",
      "Gradient Descent(49/49): loss=15.181254455078145\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5061802697200302\n",
      "Gradient Descent(2/49): loss=0.5131244207774501\n",
      "Gradient Descent(3/49): loss=0.5209268689055767\n",
      "Gradient Descent(4/49): loss=0.5296936996223409\n",
      "Gradient Descent(5/49): loss=0.5395441106156907\n",
      "Gradient Descent(6/49): loss=0.5506120324078204\n",
      "Gradient Descent(7/49): loss=0.5630479493334496\n",
      "Gradient Descent(8/49): loss=0.5770209455910866\n",
      "Gradient Descent(9/49): loss=0.5927210041861563\n",
      "Gradient Descent(10/49): loss=0.6103615900236136\n",
      "Gradient Descent(11/49): loss=0.6301825522705398\n",
      "Gradient Descent(12/49): loss=0.6524533854511798\n",
      "Gradient Descent(13/49): loss=0.677476893613005\n",
      "Gradient Descent(14/49): loss=0.7055933073835741\n",
      "Gradient Descent(15/49): loss=0.7371849098962462\n",
      "Gradient Descent(16/49): loss=0.7726812344794354\n",
      "Gradient Descent(17/49): loss=0.8125649047811171\n",
      "Gradient Descent(18/49): loss=0.8573781967321135\n",
      "Gradient Descent(19/49): loss=0.9077304115682693\n",
      "Gradient Descent(20/49): loss=0.964306160158165\n",
      "Gradient Descent(21/49): loss=1.0278746712736853\n",
      "Gradient Descent(22/49): loss=1.099300250363196\n",
      "Gradient Descent(23/49): loss=1.1795540310281012\n",
      "Gradient Descent(24/49): loss=1.2697271789831295\n",
      "Gradient Descent(25/49): loss=1.3710457280255532\n",
      "Gradient Descent(26/49): loss=1.4848872497295824\n",
      "Gradient Descent(27/49): loss=1.6127995835161228\n",
      "Gradient Descent(28/49): loss=1.7565218817587505\n",
      "Gradient Descent(29/49): loss=1.918008256064226\n",
      "Gradient Descent(30/49): loss=2.0994543462339745\n",
      "Gradient Descent(31/49): loss=2.3033271731485843\n",
      "Gradient Descent(32/49): loss=2.5323986814698904\n",
      "Gradient Descent(33/49): loss=2.789783428219727\n",
      "Gradient Descent(34/49): loss=3.078980929667626\n",
      "Gradient Descent(35/49): loss=3.403923242294348\n",
      "Gradient Descent(36/49): loss=3.7690284247616876\n",
      "Gradient Descent(37/49): loss=4.179260607782655\n",
      "Gradient Descent(38/49): loss=4.640197488624527\n",
      "Gradient Descent(39/49): loss=5.158106167938793\n",
      "Gradient Descent(40/49): loss=5.740028360016156\n",
      "Gradient Descent(41/49): loss=6.393876135033597\n",
      "Gradient Descent(42/49): loss=7.128539495044603\n",
      "Gradient Descent(43/49): loss=7.95400724635177\n",
      "Gradient Descent(44/49): loss=8.881502811721408\n",
      "Gradient Descent(45/49): loss=9.92363682897046\n",
      "Gradient Descent(46/49): loss=11.094578610750238\n",
      "Gradient Descent(47/49): loss=12.410248796757722\n",
      "Gradient Descent(48/49): loss=13.888535817758411\n",
      "Gradient Descent(49/49): loss=15.549539114551726\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5061087668480001\n",
      "Gradient Descent(2/49): loss=0.5129725772784182\n",
      "Gradient Descent(3/49): loss=0.5206847546780401\n",
      "Gradient Descent(4/49): loss=0.5293501572042536\n",
      "Gradient Descent(5/49): loss=0.5390866034827057\n",
      "Gradient Descent(6/49): loss=0.5500264745211628\n",
      "Gradient Descent(7/49): loss=0.5623185136199657\n",
      "Gradient Descent(8/49): loss=0.5761298487513904\n",
      "Gradient Descent(9/49): loss=0.5916482649050718\n",
      "Gradient Descent(10/49): loss=0.6090847572953283\n",
      "Gradient Descent(11/49): loss=0.6286764001450149\n",
      "Gradient Descent(12/49): loss=0.6506895700509572\n",
      "Gradient Descent(13/49): loss=0.6754235677572679\n",
      "Gradient Descent(14/49): loss=0.7032146875800949\n",
      "Gradient Descent(15/49): loss=0.7344407898129809\n",
      "Gradient Descent(16/49): loss=0.7695264382818751\n",
      "Gradient Descent(17/49): loss=0.8089486729014969\n",
      "Gradient Descent(18/49): loss=0.8532434957200927\n",
      "Gradient Descent(19/49): loss=0.9030131586391206\n",
      "Gradient Descent(20/49): loss=0.9589343518949679\n",
      "Gradient Descent(21/49): loss=1.0217674046371443\n",
      "Gradient Descent(22/49): loss=1.0923666226983575\n",
      "Gradient Descent(23/49): loss=1.1716919041118352\n",
      "Gradient Descent(24/49): loss=1.2608217903081333\n",
      "Gradient Descent(25/49): loss=1.3609681304383299\n",
      "Gradient Descent(26/49): loss=1.473492558208465\n",
      "Gradient Descent(27/49): loss=1.599925005250949\n",
      "Gradient Descent(28/49): loss=1.741984502747976\n",
      "Gradient Descent(29/49): loss=1.9016025541358412\n",
      "Gradient Descent(30/49): loss=2.0809493966749666\n",
      "Gradient Descent(31/49): loss=2.282463508952211\n",
      "Gradient Descent(32/49): loss=2.508884765506605\n",
      "Gradient Descent(33/49): loss=2.7632916893709636\n",
      "Gradient Descent(34/49): loss=3.049143309024954\n",
      "Gradient Descent(35/49): loss=3.3703261888680327\n",
      "Gradient Descent(36/49): loss=3.7312072726599252\n",
      "Gradient Descent(37/49): loss=4.136693258408244\n",
      "Gradient Descent(38/49): loss=4.592297311995211\n",
      "Gradient Descent(39/49): loss=5.104214026605417\n",
      "Gradient Descent(40/49): loss=5.679403647141288\n",
      "Gradient Descent(41/49): loss=6.325686704775721\n",
      "Gradient Descent(42/49): loss=7.05185034833374\n",
      "Gradient Descent(43/49): loss=7.867767818235329\n",
      "Gradient Descent(44/49): loss=8.784532687417899\n",
      "Gradient Descent(45/49): loss=9.814609694431436\n",
      "Gradient Descent(46/49): loss=10.972004219510623\n",
      "Gradient Descent(47/49): loss=12.272452707889201\n",
      "Gradient Descent(48/49): loss=13.73363662943388\n",
      "Gradient Descent(49/49): loss=15.37542288368127\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5061586086963202\n",
      "Gradient Descent(2/49): loss=0.5130784214275124\n",
      "Gradient Descent(3/49): loss=0.5208535230122681\n",
      "Gradient Descent(4/49): loss=0.5295896271529069\n",
      "Gradient Descent(5/49): loss=0.5394055137653275\n",
      "Gradient Descent(6/49): loss=0.5504346439630354\n",
      "Gradient Descent(7/49): loss=0.5628269746531886\n",
      "Gradient Descent(8/49): loss=0.5767509974166383\n",
      "Gradient Descent(9/49): loss=0.5923960293936485\n",
      "Gradient Descent(10/49): loss=0.609974787323034\n",
      "Gradient Descent(11/49): loss=0.6297262797324635\n",
      "Gradient Descent(12/49): loss=0.6519190566037313\n",
      "Gradient Descent(13/49): loss=0.6768548606962949\n",
      "Gradient Descent(14/49): loss=0.7048727301747075\n",
      "Gradient Descent(15/49): loss=0.7363536083206351\n",
      "Gradient Descent(16/49): loss=0.7717255230054195\n",
      "Gradient Descent(17/49): loss=0.8114694063452119\n",
      "Gradient Descent(18/49): loss=0.856125633665799\n",
      "Gradient Descent(19/49): loss=0.9063013706832469\n",
      "Gradient Descent(20/49): loss=0.962678828795971\n",
      "Gradient Descent(21/49): loss=1.0260245407314288\n",
      "Gradient Descent(22/49): loss=1.0971997826622195\n",
      "Gradient Descent(23/49): loss=1.1771722844955688\n",
      "Gradient Descent(24/49): loss=1.2670293875555858\n",
      "Gradient Descent(25/49): loss=1.3679928285536913\n",
      "Gradient Descent(26/49): loss=1.4814353508591924\n",
      "Gradient Descent(27/49): loss=1.608899368921726\n",
      "Gradient Descent(28/49): loss=1.7521179396169133\n",
      "Gradient Descent(29/49): loss=1.913038325649976\n",
      "Gradient Descent(30/49): loss=2.0938484713966363\n",
      "Gradient Descent(31/49): loss=2.297006751157757\n",
      "Gradient Descent(32/49): loss=2.5252753942971014\n",
      "Gradient Descent(33/49): loss=2.7817580417285392\n",
      "Gradient Descent(34/49): loss=3.069941944382816\n",
      "Gradient Descent(35/49): loss=3.3937453774046142\n",
      "Gradient Descent(36/49): loss=3.7575709147485528\n",
      "Gradient Descent(37/49): loss=4.166365288508233\n",
      "Gradient Descent(38/49): loss=4.625686646864611\n",
      "Gradient Descent(39/49): loss=5.141780125113522\n",
      "Gradient Descent(40/49): loss=5.721662757273443\n",
      "Gradient Descent(41/49): loss=6.373218882768429\n",
      "Gradient Descent(42/49): loss=7.105307345374083\n",
      "Gradient Descent(43/49): loss=7.927881941958603\n",
      "Gradient Descent(44/49): loss=8.852126758680997\n",
      "Gradient Descent(45/49): loss=9.890608234751245\n",
      "Gradient Descent(46/49): loss=11.057446021261619\n",
      "Gradient Descent(47/49): loss=12.368504958187101\n",
      "Gradient Descent(48/49): loss=13.841610779717039\n",
      "Gradient Descent(49/49): loss=15.496792480786894\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5091752460552194\n",
      "Gradient Descent(2/49): loss=0.5200763558934186\n",
      "Gradient Descent(3/49): loss=0.5330279644921867\n",
      "Gradient Descent(4/49): loss=0.5484157706683819\n",
      "Gradient Descent(5/49): loss=0.5666980231863373\n",
      "Gradient Descent(6/49): loss=0.5884191674029103\n",
      "Gradient Descent(7/49): loss=0.6142260588466283\n",
      "Gradient Descent(8/49): loss=0.6448872265709032\n",
      "Gradient Descent(9/49): loss=0.6813157599441164\n",
      "Gradient Descent(10/49): loss=0.7245965004448442\n",
      "Gradient Descent(11/49): loss=0.7760183482337081\n",
      "Gradient Descent(12/49): loss=0.8371126455917224\n",
      "Gradient Descent(13/49): loss=0.9096987802827833\n",
      "Gradient Descent(14/49): loss=0.9959383669092695\n",
      "Gradient Descent(15/49): loss=1.0983996197802142\n",
      "Gradient Descent(16/49): loss=1.2201338343161996\n",
      "Gradient Descent(17/49): loss=1.3647662546062556\n",
      "Gradient Descent(18/49): loss=1.536604033152751\n",
      "Gradient Descent(19/49): loss=1.7407644978441152\n",
      "Gradient Descent(20/49): loss=1.9833275459438928\n",
      "Gradient Descent(21/49): loss=2.2715167033912365\n",
      "Gradient Descent(22/49): loss=2.613914241354444\n",
      "Gradient Descent(23/49): loss=3.0207167562083033\n",
      "Gradient Descent(24/49): loss=3.504038824106184\n",
      "Gradient Descent(25/49): loss=4.078273772975438\n",
      "Gradient Descent(26/49): loss=4.760522315727541\n",
      "Gradient Descent(27/49): loss=5.571101809370496\n",
      "Gradient Descent(28/49): loss=6.5341513057683\n",
      "Gradient Descent(29/49): loss=7.678350412437526\n",
      "Gradient Descent(30/49): loss=9.037773371071236\n",
      "Gradient Descent(31/49): loss=10.652903788225691\n",
      "Gradient Descent(32/49): loss=12.571840236847176\n",
      "Gradient Descent(33/49): loss=14.851728631453604\n",
      "Gradient Descent(34/49): loss=17.560464033086433\n",
      "Gradient Descent(35/49): loss=20.77871256376768\n",
      "Gradient Descent(36/49): loss=24.60231364306749\n",
      "Gradient Descent(37/49): loss=29.145134085384154\n",
      "Gradient Descent(38/49): loss=34.54245905289827\n",
      "Gradient Descent(39/49): loss=40.95502084680622\n",
      "Gradient Descent(40/49): loss=48.57378551415145\n",
      "Gradient Descent(41/49): loss=57.62563981541166\n",
      "Gradient Descent(42/49): loss=68.38014791074947\n",
      "Gradient Descent(43/49): loss=81.15757897880896\n",
      "Gradient Descent(44/49): loss=96.33844483078181\n",
      "Gradient Descent(45/49): loss=114.37483154951894\n",
      "Gradient Descent(46/49): loss=135.8038626100491\n",
      "Gradient Descent(47/49): loss=161.26369441305513\n",
      "Gradient Descent(48/49): loss=191.51252057818644\n",
      "Gradient Descent(49/49): loss=227.45115094498226\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5094054104719874\n",
      "Gradient Descent(2/49): loss=0.5205799786537487\n",
      "Gradient Descent(3/49): loss=0.5338564831105053\n",
      "Gradient Descent(4/49): loss=0.5496302980555792\n",
      "Gradient Descent(5/49): loss=0.5683711675918103\n",
      "Gradient Descent(6/49): loss=0.5906371946878268\n",
      "Gradient Descent(7/49): loss=0.6170914614805783\n",
      "Gradient Descent(8/49): loss=0.6485217758570513\n",
      "Gradient Descent(9/49): loss=0.6858641323677457\n",
      "Gradient Descent(10/49): loss=0.7302305861380869\n",
      "Gradient Descent(11/49): loss=0.7829423698626738\n",
      "Gradient Descent(12/49): loss=0.8455692401058374\n",
      "Gradient Descent(13/49): loss=0.9199762246417618\n",
      "Gradient Descent(14/49): loss=1.0083791629688454\n",
      "Gradient Descent(15/49): loss=1.1134106939952952\n",
      "Gradient Descent(16/49): loss=1.2381986560078955\n",
      "Gradient Descent(17/49): loss=1.386459233675098\n",
      "Gradient Descent(18/49): loss=1.5626076260012507\n",
      "Gradient Descent(19/49): loss=1.771889530924075\n",
      "Gradient Descent(20/49): loss=2.020537362163032\n",
      "Gradient Descent(21/49): loss=2.315955850457917\n",
      "Gradient Descent(22/49): loss=2.666942556400801\n",
      "Gradient Descent(23/49): loss=3.083949861731664\n",
      "Gradient Descent(24/49): loss=3.5793962411955342\n",
      "Gradient Descent(25/49): loss=4.16803608463616\n",
      "Gradient Descent(26/49): loss=4.867399082627761\n",
      "Gradient Descent(27/49): loss=5.698312260542583\n",
      "Gradient Descent(28/49): loss=6.685520207222593\n",
      "Gradient Descent(29/49): loss=7.858421968672875\n",
      "Gradient Descent(30/49): loss=9.251946551452319\n",
      "Gradient Descent(31/49): loss=10.907593108252518\n",
      "Gradient Descent(32/49): loss=12.874666782387242\n",
      "Gradient Descent(33/49): loss=15.211747014625823\n",
      "Gradient Descent(34/49): loss=17.988432038547945\n",
      "Gradient Descent(35/49): loss=21.287411515470417\n",
      "Gradient Descent(36/49): loss=25.206929032002\n",
      "Gradient Descent(37/49): loss=29.86370779339416\n",
      "Gradient Descent(38/49): loss=35.396426639807956\n",
      "Gradient Descent(39/49): loss=41.96984990122731\n",
      "Gradient Descent(40/49): loss=49.77973407811713\n",
      "Gradient Descent(41/49): loss=59.058657468692125\n",
      "Gradient Descent(42/49): loss=70.08294634903146\n",
      "Gradient Descent(43/49): loss=83.1809039677621\n",
      "Gradient Descent(44/49): loss=98.74258741456588\n",
      "Gradient Descent(45/49): loss=117.23142351772348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=139.1980096918901\n",
      "Gradient Descent(47/49): loss=165.2965107254136\n",
      "Gradient Descent(48/49): loss=196.30413980334006\n",
      "Gradient Descent(49/49): loss=233.14430391081672\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5092965942080001\n",
      "Gradient Descent(2/49): loss=0.5203418777865326\n",
      "Gradient Descent(3/49): loss=0.5334647792061746\n",
      "Gradient Descent(4/49): loss=0.5490560983828591\n",
      "Gradient Descent(5/49): loss=0.5675801446966735\n",
      "Gradient Descent(6/49): loss=0.5895885641221073\n",
      "Gradient Descent(7/49): loss=0.6157367672414916\n",
      "Gradient Descent(8/49): loss=0.6468034473675934\n",
      "Gradient Descent(9/49): loss=0.6837137700254526\n",
      "Gradient Descent(10/49): loss=0.7275669243752271\n",
      "Gradient Descent(11/49): loss=0.7796688570581757\n",
      "Gradient Descent(12/49): loss=0.841571163278839\n",
      "Gradient Descent(13/49): loss=0.9151172932996249\n",
      "Gradient Descent(14/49): loss=1.0024974503773398\n",
      "Gradient Descent(15/49): loss=1.106313815001401\n",
      "Gradient Descent(16/49): loss=1.2296580378110633\n",
      "Gradient Descent(17/49): loss=1.3762033089312626\n",
      "Gradient Descent(18/49): loss=1.5503137455493476\n",
      "Gradient Descent(19/49): loss=1.7571743552950916\n",
      "Gradient Descent(20/49): loss=2.002945445733983\n",
      "Gradient Descent(21/49): loss=2.294946078284598\n",
      "Gradient Descent(22/49): loss=2.6418720298176863\n",
      "Gradient Descent(23/49): loss=3.0540547528341437\n",
      "Gradient Descent(24/49): loss=3.5437690460503997\n",
      "Gradient Descent(25/49): loss=4.1255985978206375\n",
      "Gradient Descent(26/49): loss=4.81687028827823\n",
      "Gradient Descent(27/49): loss=5.638170183711714\n",
      "Gradient Descent(28/49): loss=6.613956589475613\n",
      "Gradient Descent(29/49): loss=7.773288418164269\n",
      "Gradient Descent(30/49): loss=9.150690563828329\n",
      "Gradient Descent(31/49): loss=10.787182053093634\n",
      "Gradient Descent(32/49): loss=12.731497591488779\n",
      "Gradient Descent(33/49): loss=15.041538882656901\n",
      "Gradient Descent(34/49): loss=17.786098940694277\n",
      "Gradient Descent(35/49): loss=21.046910745647452\n",
      "Gradient Descent(36/49): loss=24.921081251110007\n",
      "Gradient Descent(37/49): loss=29.523983228655805\n",
      "Gradient Descent(38/49): loss=34.992691068173876\n",
      "Gradient Descent(39/49): loss=41.49006285230285\n",
      "Gradient Descent(40/49): loss=49.20959026903182\n",
      "Gradient Descent(41/49): loss=58.38116079284131\n",
      "Gradient Descent(42/49): loss=69.27790373217881\n",
      "Gradient Descent(43/49): loss=82.22432401841715\n",
      "Gradient Descent(44/49): loss=97.60596596049041\n",
      "Gradient Descent(45/49): loss=115.88089475186578\n",
      "Gradient Descent(46/49): loss=137.5933376488817\n",
      "Gradient Descent(47/49): loss=163.3898910548378\n",
      "Gradient Descent(48/49): loss=194.03877615647488\n",
      "Gradient Descent(49/49): loss=230.45271654571008\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5093724457587201\n",
      "Gradient Descent(2/49): loss=0.5205078485646623\n",
      "Gradient Descent(3/49): loss=0.5337378206384032\n",
      "Gradient Descent(4/49): loss=0.5494563504591936\n",
      "Gradient Descent(5/49): loss=0.5681315357392962\n",
      "Gradient Descent(6/49): loss=0.5903195233705908\n",
      "Gradient Descent(7/49): loss=0.6166810714753382\n",
      "Gradient Descent(8/49): loss=0.6480012267785853\n",
      "Gradient Descent(9/49): loss=0.685212703294348\n",
      "Gradient Descent(10/49): loss=0.729423658542753\n",
      "Gradient Descent(11/49): loss=0.7819506944733461\n",
      "Gradient Descent(12/49): loss=0.8443580658625522\n",
      "Gradient Descent(13/49): loss=0.9185042638099938\n",
      "Gradient Descent(14/49): loss=1.006597361591376\n",
      "Gradient Descent(15/49): loss=1.1112607710654892\n",
      "Gradient Descent(16/49): loss=1.23561136786156\n",
      "Gradient Descent(17/49): loss=1.3833523119150544\n",
      "Gradient Descent(18/49): loss=1.5588833275449376\n",
      "Gradient Descent(19/49): loss=1.767431727214874\n",
      "Gradient Descent(20/49): loss=2.0152080808626858\n",
      "Gradient Descent(21/49): loss=2.3095911666317512\n",
      "Gradient Descent(22/49): loss=2.6593477108340395\n",
      "Gradient Descent(23/49): loss=3.0748934610005785\n",
      "Gradient Descent(24/49): loss=3.5686033667736248\n",
      "Gradient Descent(25/49): loss=4.155180105822285\n",
      "Gradient Descent(26/49): loss=4.852091929486605\n",
      "Gradient Descent(27/49): loss=5.680092867181766\n",
      "Gradient Descent(28/49): loss=6.663840781257937\n",
      "Gradient Descent(29/49): loss=7.832631677971567\n",
      "Gradient Descent(30/49): loss=9.22127214235645\n",
      "Gradient Descent(31/49): loss=10.871115878091059\n",
      "Gradient Descent(32/49): loss=12.831295220518546\n",
      "Gradient Descent(33/49): loss=15.16018429725663\n",
      "Gradient Descent(34/49): loss=17.927137409327017\n",
      "Gradient Descent(35/49): loss=21.214554401778692\n",
      "Gradient Descent(36/49): loss=25.120334530512284\n",
      "Gradient Descent(37/49): loss=29.76079190145841\n",
      "Gradient Descent(38/49): loss=35.27411930387878\n",
      "Gradient Descent(39/49): loss=41.824503590692004\n",
      "Gradient Descent(40/49): loss=49.60701516186318\n",
      "Gradient Descent(41/49): loss=58.853417159573254\n",
      "Gradient Descent(42/49): loss=69.83906737305175\n",
      "Gradient Descent(43/49): loss=82.89111839167809\n",
      "Gradient Descent(44/49): loss=98.39826020690442\n",
      "Gradient Descent(45/49): loss=116.82229539758305\n",
      "Gradient Descent(46/49): loss=138.7118916076362\n",
      "Gradient Descent(47/49): loss=164.71892086478525\n",
      "Gradient Descent(48/49): loss=195.61787232522755\n",
      "Gradient Descent(49/49): loss=232.32891655534362\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5124092642022745\n",
      "Gradient Descent(2/49): loss=0.5279754452176008\n",
      "Gradient Descent(3/49): loss=0.5475016626832452\n",
      "Gradient Descent(4/49): loss=0.5719953498721229\n",
      "Gradient Descent(5/49): loss=0.6027202310818518\n",
      "Gradient Descent(6/49): loss=0.641261522071365\n",
      "Gradient Descent(7/49): loss=0.6896077174886265\n",
      "Gradient Descent(8/49): loss=0.7502531850199945\n",
      "Gradient Descent(9/49): loss=0.8263268594913558\n",
      "Gradient Descent(10/49): loss=0.9217536767482373\n",
      "Gradient Descent(11/49): loss=1.041457076315195\n",
      "Gradient Descent(12/49): loss=1.1916130207319846\n",
      "Gradient Descent(13/49): loss=1.3799686374085574\n",
      "Gradient Descent(14/49): loss=1.6162419229674492\n",
      "Gradient Descent(15/49): loss=1.9126231323727012\n",
      "Gradient Descent(16/49): loss=2.2844037214505453\n",
      "Gradient Descent(17/49): loss=2.7507652923896377\n",
      "Gradient Descent(18/49): loss=3.3357692469760227\n",
      "Gradient Descent(19/49): loss=4.069598207609246\n",
      "Gradient Descent(20/49): loss=4.990113255827599\n",
      "Gradient Descent(21/49): loss=6.144807332312465\n",
      "Gradient Descent(22/49): loss=7.593255581855366\n",
      "Gradient Descent(23/49): loss=9.410189066082328\n",
      "Gradient Descent(24/49): loss=11.689350428694608\n",
      "Gradient Descent(25/49): loss=14.548330441958228\n",
      "Gradient Descent(26/49): loss=18.134634970594288\n",
      "Gradient Descent(27/49): loss=22.63329537131809\n",
      "Gradient Descent(28/49): loss=28.27641497798233\n",
      "Gradient Descent(29/49): loss=35.35514421258341\n",
      "Gradient Descent(30/49): loss=44.23470216446534\n",
      "Gradient Descent(31/49): loss=55.373219659301625\n",
      "Gradient Descent(32/49): loss=69.34537600483725\n",
      "Gradient Descent(33/49): loss=86.87204892466548\n",
      "Gradient Descent(34/49): loss=108.85750743530075\n",
      "Gradient Descent(35/49): loss=136.43606659104285\n",
      "Gradient Descent(36/49): loss=171.0306111960004\n",
      "Gradient Descent(37/49): loss=214.42600794847505\n",
      "Gradient Descent(38/49): loss=268.86119363474694\n",
      "Gradient Descent(39/49): loss=337.14469055957693\n",
      "Gradient Descent(40/49): loss=422.79950910211187\n",
      "Gradient Descent(41/49): loss=530.2449134818783\n",
      "Gradient Descent(42/49): loss=665.0244287358859\n",
      "Gradient Descent(43/49): loss=834.0918526705995\n",
      "Gradient Descent(44/49): loss=1046.170029254229\n",
      "Gradient Descent(45/49): loss=1312.2008939608168\n",
      "Gradient Descent(46/49): loss=1645.9100106487267\n",
      "Gradient Descent(47/49): loss=2064.5147266219656\n",
      "Gradient Descent(48/49): loss=2589.612482338688\n",
      "Gradient Descent(49/49): loss=3248.29510710966\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5127205551519063\n",
      "Gradient Descent(2/49): loss=0.5286772195344578\n",
      "Gradient Descent(3/49): loss=0.5486932593359246\n",
      "Gradient Descent(4/49): loss=0.5738013796628968\n",
      "Gradient Descent(5/49): loss=0.6052970058010457\n",
      "Gradient Descent(6/49): loss=0.6448051192287232\n",
      "Gradient Descent(7/49): loss=0.6943640967124342\n",
      "Gradient Descent(8/49): loss=0.7565308780680212\n",
      "Gradient Descent(9/49): loss=0.8345128886004161\n",
      "Gradient Descent(10/49): loss=0.9323335226122325\n",
      "Gradient Descent(11/49): loss=1.0550397259167112\n",
      "Gradient Descent(12/49): loss=1.2089623873417708\n",
      "Gradient Descent(13/49): loss=1.4020429738334237\n",
      "Gradient Descent(14/49): loss=1.6442432615285998\n",
      "Gradient Descent(15/49): loss=1.9480593024135258\n",
      "Gradient Descent(16/49): loss=2.3291661440995197\n",
      "Gradient Descent(17/49): loss=2.8072265663104212\n",
      "Gradient Descent(18/49): loss=3.4069055599318023\n",
      "Gradient Descent(19/49): loss=4.159142889529995\n",
      "Gradient Descent(20/49): loss=5.102749395777651\n",
      "Gradient Descent(21/49): loss=6.286409397215457\n",
      "Gradient Descent(22/49): loss=7.77119250301987\n",
      "Gradient Descent(23/49): loss=9.633704430940446\n",
      "Gradient Descent(24/49): loss=11.970039393324265\n",
      "Gradient Descent(25/49): loss=14.900737970139462\n",
      "Gradient Descent(26/49): loss=18.577006264893623\n",
      "Gradient Descent(27/49): loss=23.188517213837166\n",
      "Gradient Descent(28/49): loss=28.973196548189758\n",
      "Gradient Descent(29/49): loss=36.229498305201865\n",
      "Gradient Descent(30/49): loss=45.33180322919761\n",
      "Gradient Descent(31/49): loss=56.74973452586112\n",
      "Gradient Descent(32/49): loss=71.07238754438696\n",
      "Gradient Descent(33/49): loss=89.03872349083024\n",
      "Gradient Descent(34/49): loss=111.57569530204668\n",
      "Gradient Descent(35/49): loss=139.84607274203782\n",
      "Gradient Descent(36/49): loss=175.3084342027442\n",
      "Gradient Descent(37/49): loss=219.79242041909734\n",
      "Gradient Descent(38/49): loss=275.5931327288476\n",
      "Gradient Descent(39/49): loss=345.5895462501687\n",
      "Gradient Descent(40/49): loss=433.3930473713261\n",
      "Gradient Descent(41/49): loss=543.53375917774\n",
      "Gradient Descent(42/49): loss=681.6942680677226\n",
      "Gradient Descent(43/49): loss=855.002810419322\n",
      "Gradient Descent(44/49): loss=1072.4010459450762\n",
      "Gradient Descent(45/49): loss=1345.105392588752\n",
      "Gradient Descent(46/49): loss=1687.1857250184569\n",
      "Gradient Descent(47/49): loss=2116.291294018269\n",
      "Gradient Descent(48/49): loss=2654.561319771547\n",
      "Gradient Descent(49/49): loss=3329.767240076388\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5125733841920002\n",
      "Gradient Descent(2/49): loss=0.5283454373224548\n",
      "Gradient Descent(3/49): loss=0.54812990076929\n",
      "Gradient Descent(4/49): loss=0.5729475317170017\n",
      "Gradient Descent(5/49): loss=0.6040787679777939\n",
      "Gradient Descent(6/49): loss=0.6431297907433505\n",
      "Gradient Descent(7/49): loss=0.6921153937004311\n",
      "Gradient Descent(8/49): loss=0.7535629340498095\n",
      "Gradient Descent(9/49): loss=0.8306427286641158\n",
      "Gradient Descent(10/49): loss=0.9273316230282393\n",
      "Gradient Descent(11/49): loss=1.0486181721186576\n",
      "Gradient Descent(12/49): loss=1.2007600192975882\n",
      "Gradient Descent(13/49): loss=1.3916067523987938\n",
      "Gradient Descent(14/49): loss=1.631004894400942\n",
      "Gradient Descent(15/49): loss=1.931305923728693\n",
      "Gradient Descent(16/49): loss=2.30800353491736\n",
      "Gradient Descent(17/49): loss=2.7805330183925445\n",
      "Gradient Descent(18/49): loss=3.373274002463794\n",
      "Gradient Descent(19/49): loss=4.116808292882225\n",
      "Gradient Descent(20/49): loss=5.049497706783439\n",
      "Gradient Descent(21/49): loss=6.219463307581642\n",
      "Gradient Descent(22/49): loss=7.687068157222316\n",
      "Gradient Descent(23/49): loss=9.528031680610535\n",
      "Gradient Descent(24/49): loss=11.837336324348618\n",
      "Gradient Descent(25/49): loss=14.734128069454599\n",
      "Gradient Descent(26/49): loss=18.36786363451369\n",
      "Gradient Descent(27/49): loss=22.926021527323957\n",
      "Gradient Descent(28/49): loss=28.643774788063393\n",
      "Gradient Descent(29/49): loss=35.81612447834218\n",
      "Gradient Descent(30/49): loss=44.813119929828964\n",
      "Gradient Descent(31/49): loss=56.098951024174404\n",
      "Gradient Descent(32/49): loss=70.25589754891173\n",
      "Gradient Descent(33/49): loss=88.01437126953616\n",
      "Gradient Descent(34/49): loss=110.29060070471142\n",
      "Gradient Descent(35/49): loss=138.23390290818065\n",
      "Gradient Descent(36/49): loss=173.28598119222764\n",
      "Gradient Descent(37/49): loss=217.2553081917375\n",
      "Gradient Descent(38/49): loss=272.4104319798809\n",
      "Gradient Descent(39/49): loss=341.59701925976873\n",
      "Gradient Descent(40/49): loss=428.3846743436215\n",
      "Gradient Descent(41/49): loss=537.2511088808253\n",
      "Gradient Descent(42/49): loss=673.8131643643412\n",
      "Gradient Descent(43/49): loss=845.1166067628687\n",
      "Gradient Descent(44/49): loss=1059.9996449075556\n",
      "Gradient Descent(45/49): loss=1329.548927956174\n",
      "Gradient Descent(46/49): loss=1667.6715486125365\n",
      "Gradient Descent(47/49): loss=2091.8125639635955\n",
      "Gradient Descent(48/49): loss=2623.855053619974\n",
      "Gradient Descent(49/49): loss=3291.249152645241\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5126759712972802\n",
      "Gradient Descent(2/49): loss=0.5285767096925946\n",
      "Gradient Descent(3/49): loss=0.5485225959356776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=0.5735427156389978\n",
      "Gradient Descent(5/49): loss=0.6049279537948306\n",
      "Gradient Descent(6/49): loss=0.6442975965375302\n",
      "Gradient Descent(7/49): loss=0.693682876393967\n",
      "Gradient Descent(8/49): loss=0.7556317714459062\n",
      "Gradient Descent(9/49): loss=0.8333404653990267\n",
      "Gradient Descent(10/49): loss=0.930818251093821\n",
      "Gradient Descent(11/49): loss=1.0530943854694\n",
      "Gradient Descent(12/49): loss=1.2064775684301412\n",
      "Gradient Descent(13/49): loss=1.3988814331361148\n",
      "Gradient Descent(14/49): loss=1.6402328410232379\n",
      "Gradient Descent(15/49): loss=1.9429840470767348\n",
      "Gradient Descent(16/49): loss=2.3227551599504013\n",
      "Gradient Descent(17/49): loss=2.799140043939003\n",
      "Gradient Descent(18/49): loss=3.3967172424145327\n",
      "Gradient Descent(19/49): loss=4.14631808018182\n",
      "Gradient Descent(20/49): loss=5.086617371077702\n",
      "Gradient Descent(21/49): loss=6.26612880157668\n",
      "Gradient Descent(22/49): loss=7.745707939994704\n",
      "Gradient Descent(23/49): loss=9.60169201122796\n",
      "Gradient Descent(24/49): loss=11.929838430181183\n",
      "Gradient Descent(25/49): loss=14.85026529811861\n",
      "Gradient Descent(26/49): loss=18.51364876125662\n",
      "Gradient Descent(27/49): loss=23.108996977414975\n",
      "Gradient Descent(28/49): loss=28.87340177976323\n",
      "Gradient Descent(29/49): loss=36.10427116382977\n",
      "Gradient Descent(30/49): loss=45.17467371920965\n",
      "Gradient Descent(31/49): loss=56.552586684669635\n",
      "Gradient Descent(32/49): loss=70.82504070854299\n",
      "Gradient Descent(33/49): loss=88.72840703609174\n",
      "Gradient Descent(34/49): loss=111.18638975736806\n",
      "Gradient Descent(35/49): loss=139.35768328294225\n",
      "Gradient Descent(36/49): loss=174.69575388141575\n",
      "Gradient Descent(37/49): loss=219.02382964012963\n",
      "Gradient Descent(38/49): loss=274.6289678718516\n",
      "Gradient Descent(39/49): loss=344.3800532697746\n",
      "Gradient Descent(40/49): loss=431.87581479288747\n",
      "Gradient Descent(41/49): loss=541.6304980475172\n",
      "Gradient Descent(42/49): loss=679.3067727221594\n",
      "Gradient Descent(43/49): loss=852.0078916739758\n",
      "Gradient Descent(44/49): loss=1068.6441752870821\n",
      "Gradient Descent(45/49): loss=1340.392729451264\n",
      "Gradient Descent(46/49): loss=1681.2741157949738\n",
      "Gradient Descent(47/49): loss=2108.875726824294\n",
      "Gradient Descent(48/49): loss=2645.25918769951\n",
      "Gradient Descent(49/49): loss=3318.098601021801\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5157310837469866\n",
      "Gradient Descent(2/49): loss=0.5365354420023748\n",
      "Gradient Descent(3/49): loss=0.5640492057951257\n",
      "Gradient Descent(4/49): loss=0.6004361584110509\n",
      "Gradient Descent(5/49): loss=0.6485579032455882\n",
      "Gradient Descent(6/49): loss=0.7121989107892603\n",
      "Gradient Descent(7/49): loss=0.7963641432657923\n",
      "Gradient Descent(8/49): loss=0.9076726632160146\n",
      "Gradient Descent(9/49): loss=1.0548781808502121\n",
      "Gradient Descent(10/49): loss=1.2495574779213587\n",
      "Gradient Descent(11/49): loss=1.5070208482979235\n",
      "Gradient Descent(12/49): loss=1.8475161556210251\n",
      "Gradient Descent(13/49): loss=2.2978211995557007\n",
      "Gradient Descent(14/49): loss=2.8933496201594853\n",
      "Gradient Descent(15/49): loss=3.6809359564075903\n",
      "Gradient Descent(16/49): loss=4.722518886095605\n",
      "Gradient Descent(17/49): loss=6.100012310608517\n",
      "Gradient Descent(18/49): loss=7.921747364526268\n",
      "Gradient Descent(19/49): loss=10.330991973332955\n",
      "Gradient Descent(20/49): loss=13.51721796847861\n",
      "Gradient Descent(21/49): loss=17.731001847058376\n",
      "Gradient Descent(22/49): loss=23.303731026479884\n",
      "Gradient Descent(23/49): loss=30.673665366268278\n",
      "Gradient Descent(24/49): loss=40.42040353063585\n",
      "Gradient Descent(25/49): loss=53.310464753015744\n",
      "Gradient Descent(26/49): loss=70.35757071960701\n",
      "Gradient Descent(27/49): loss=92.90236836042747\n",
      "Gradient Descent(28/49): loss=122.7178632404017\n",
      "Gradient Descent(29/49): loss=162.1488552191932\n",
      "Gradient Descent(30/49): loss=214.29634211115817\n",
      "Gradient Descent(31/49): loss=283.26139352577394\n",
      "Gradient Descent(32/49): loss=374.46767402159173\n",
      "Gradient Descent(33/49): loss=495.087979977297\n",
      "Gradient Descent(34/49): loss=654.6083346036904\n",
      "Gradient Descent(35/49): loss=865.574003597125\n",
      "Gradient Descent(36/49): loss=1144.576100840967\n",
      "Gradient Descent(37/49): loss=1513.5563744460321\n",
      "Gradient Descent(38/49): loss=2001.5327862886063\n",
      "Gradient Descent(39/49): loss=2646.8815909506784\n",
      "Gradient Descent(40/49): loss=3500.355385116307\n",
      "Gradient Descent(41/49): loss=4629.0744779005145\n",
      "Gradient Descent(42/49): loss=6121.805478107257\n",
      "Gradient Descent(43/49): loss=8095.942225880234\n",
      "Gradient Descent(44/49): loss=10706.738074809475\n",
      "Gradient Descent(45/49): loss=14159.515585018113\n",
      "Gradient Descent(46/49): loss=18725.813842268613\n",
      "Gradient Descent(47/49): loss=24764.74328748534\n",
      "Gradient Descent(48/49): loss=32751.227478785077\n",
      "Gradient Descent(49/49): loss=43313.35282177582\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5161257037597866\n",
      "Gradient Descent(2/49): loss=0.5374519469821104\n",
      "Gradient Descent(3/49): loss=0.5656559036436337\n",
      "Gradient Descent(4/49): loss=0.602955636328492\n",
      "Gradient Descent(5/49): loss=0.6522845328042087\n",
      "Gradient Descent(6/49): loss=0.7175219983933288\n",
      "Gradient Descent(7/49): loss=0.8037985466349946\n",
      "Gradient Descent(8/49): loss=0.9178992816845737\n",
      "Gradient Descent(9/49): loss=1.068797503787585\n",
      "Gradient Descent(10/49): loss=1.268360402518865\n",
      "Gradient Descent(11/49): loss=1.5322823360909237\n",
      "Gradient Descent(12/49): loss=1.8813190932400106\n",
      "Gradient Descent(13/49): loss=2.342920204569716\n",
      "Gradient Descent(14/49): loss=2.953387674303051\n",
      "Gradient Descent(15/49): loss=3.760730903026007\n",
      "Gradient Descent(16/49): loss=4.828442323011423\n",
      "Gradient Descent(17/49): loss=6.240490675942929\n",
      "Gradient Descent(18/49): loss=8.107924622693707\n",
      "Gradient Descent(19/49): loss=10.577606017273032\n",
      "Gradient Descent(20/49): loss=13.843759661601899\n",
      "Gradient Descent(21/49): loss=18.16324785623054\n",
      "Gradient Descent(22/49): loss=23.875770993624876\n",
      "Gradient Descent(23/49): loss=31.430582842829065\n",
      "Gradient Descent(24/49): loss=41.42182151340427\n",
      "Gradient Descent(25/49): loss=54.63523465523853\n",
      "Gradient Descent(26/49): loss=72.10997353530644\n",
      "Gradient Descent(27/49): loss=95.22031570420039\n",
      "Gradient Descent(28/49): loss=125.78374322256131\n",
      "Gradient Descent(29/49): loss=166.20387611559758\n",
      "Gradient Descent(30/49): loss=219.65950186662857\n",
      "Gradient Descent(31/49): loss=290.3545669223755\n",
      "Gradient Descent(32/49): loss=383.8487904585801\n",
      "Gradient Descent(33/49): loss=507.49490108523923\n",
      "Gradient Descent(34/49): loss=671.0168823890382\n",
      "Gradient Descent(35/49): loss=887.274702663335\n",
      "Gradient Descent(36/49): loss=1173.2756699761353\n",
      "Gradient Descent(37/49): loss=1551.5119492470392\n",
      "Gradient Descent(38/49): loss=2051.729428582957\n",
      "Gradient Descent(39/49): loss=2713.2670450043884\n",
      "Gradient Descent(40/49): loss=3588.150542721984\n",
      "Gradient Descent(41/49): loss=4745.183968454069\n",
      "Gradient Descent(42/49): loss=6275.360673983744\n",
      "Gradient Descent(43/49): loss=8299.019367046216\n",
      "Gradient Descent(44/49): loss=10975.307988621797\n",
      "Gradient Descent(45/49): loss=14514.699690654841\n",
      "Gradient Descent(46/49): loss=19195.545216593986\n",
      "Gradient Descent(47/49): loss=25385.96342464715\n",
      "Gradient Descent(48/49): loss=33572.791504799636\n",
      "Gradient Descent(49/49): loss=44399.871640804035\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5159391368\n",
      "Gradient Descent(2/49): loss=0.5370186452179925\n",
      "Gradient Descent(3/49): loss=0.5648962951007956\n",
      "Gradient Descent(4/49): loss=0.6017644870707931\n",
      "Gradient Descent(5/49): loss=0.6505226709511358\n",
      "Gradient Descent(6/49): loss=0.7150053691328879\n",
      "Gradient Descent(7/49): loss=0.800283737478218\n",
      "Gradient Descent(8/49): loss=0.913064379614946\n",
      "Gradient Descent(9/49): loss=1.0622167788407684\n",
      "Gradient Descent(10/49): loss=1.2594708268168757\n",
      "Gradient Descent(11/49): loss=1.5203393052651888\n",
      "Gradient Descent(12/49): loss=1.8653378680132995\n",
      "Gradient Descent(13/49): loss=2.321598467247648\n",
      "Gradient Descent(14/49): loss=2.9250031097348916\n",
      "Gradient Descent(15/49): loss=3.7230057494239843\n",
      "Gradient Descent(16/49): loss=4.778364240413074\n",
      "Gradient Descent(17/49): loss=6.174075844746863\n",
      "Gradient Descent(18/49): loss=8.01990444147705\n",
      "Gradient Descent(19/49): loss=10.461012760652427\n",
      "Gradient Descent(20/49): loss=13.689378512763739\n",
      "Gradient Descent(21/49): loss=17.958892219928643\n",
      "Gradient Descent(22/49): loss=23.6053240976547\n",
      "Gradient Descent(23/49): loss=31.072730255950564\n",
      "Gradient Descent(24/49): loss=40.94837490029125\n",
      "Gradient Descent(25/49): loss=54.00891494243418\n",
      "Gradient Descent(26/49): loss=71.28147914816577\n",
      "Gradient Descent(27/49): loss=94.12444531025925\n",
      "Gradient Descent(28/49): loss=124.33426805963488\n",
      "Gradient Descent(29/49): loss=164.286758645674\n",
      "Gradient Descent(30/49): loss=217.12392744569658\n",
      "Gradient Descent(31/49): loss=287.0010831837476\n",
      "Gradient Descent(32/49): loss=379.41362164732044\n",
      "Gradient Descent(33/49): loss=501.62920376533157\n",
      "Gradient Descent(34/49): loss=663.259311116398\n",
      "Gradient Descent(35/49): loss=877.0151280882654\n",
      "Gradient Descent(36/49): loss=1159.7071960336611\n",
      "Gradient Descent(37/49): loss=1533.5674558914068\n",
      "Gradient Descent(38/49): loss=2027.9976495533165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=2681.881580671051\n",
      "Gradient Descent(40/49): loss=3546.643079574504\n",
      "Gradient Descent(41/49): loss=4690.290161873541\n",
      "Gradient Descent(42/49): loss=6202.763428214226\n",
      "Gradient Descent(43/49): loss=8203.009322951102\n",
      "Gradient Descent(44/49): loss=10848.334518738933\n",
      "Gradient Descent(45/49): loss=14346.77709017009\n",
      "Gradient Descent(46/49): loss=18973.467390886144\n",
      "Gradient Descent(47/49): loss=25092.265313581665\n",
      "Gradient Descent(48/49): loss=33184.37556634936\n",
      "Gradient Descent(49/49): loss=43886.19137563064\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5160691853120001\n",
      "Gradient Descent(2/49): loss=0.5373206828871274\n",
      "Gradient Descent(3/49): loss=0.5654257884302183\n",
      "Gradient Descent(4/49): loss=0.602594790510949\n",
      "Gradient Descent(5/49): loss=0.6517507957627222\n",
      "Gradient Descent(6/49): loss=0.7167596127081769\n",
      "Gradient Descent(7/49): loss=0.8027337731185871\n",
      "Gradient Descent(8/49): loss=0.9164346002613399\n",
      "Gradient Descent(9/49): loss=1.0668039441576083\n",
      "Gradient Descent(10/49): loss=1.265667401460349\n",
      "Gradient Descent(11/49): loss=1.5286643237434259\n",
      "Gradient Descent(12/49): loss=1.876477753462824\n",
      "Gradient Descent(13/49): loss=2.336461014266735\n",
      "Gradient Descent(14/49): loss=2.9447888766800037\n",
      "Gradient Descent(15/49): loss=3.7493024747217243\n",
      "Gradient Descent(16/49): loss=4.813271708132044\n",
      "Gradient Descent(17/49): loss=6.2203710193174455\n",
      "Gradient Descent(18/49): loss=8.081259858359438\n",
      "Gradient Descent(19/49): loss=10.54228534799347\n",
      "Gradient Descent(20/49): loss=13.796991558032339\n",
      "Gradient Descent(21/49): loss=18.10134052080967\n",
      "Gradient Descent(22/49): loss=23.79384202408495\n",
      "Gradient Descent(23/49): loss=31.322175262163356\n",
      "Gradient Descent(24/49): loss=41.27839596952174\n",
      "Gradient Descent(25/49): loss=54.445497855008036\n",
      "Gradient Descent(26/49): loss=71.85899009856905\n",
      "Gradient Descent(27/49): loss=94.88833359067273\n",
      "Gradient Descent(28/49): loss=125.3446403589764\n",
      "Gradient Descent(29/49): loss=165.623106060048\n",
      "Gradient Descent(30/49): loss=218.8913769497201\n",
      "Gradient Descent(31/49): loss=289.33866520130795\n",
      "Gradient Descent(32/49): loss=382.50520391406917\n",
      "Gradient Descent(33/49): loss=505.717951361703\n",
      "Gradient Descent(34/49): loss=668.6668098611177\n",
      "Gradient Descent(35/49): loss=884.1666752266211\n",
      "Gradient Descent(36/49): loss=1169.1652471725993\n",
      "Gradient Descent(37/49): loss=1546.0758585710637\n",
      "Gradient Descent(38/49): loss=2044.540142145693\n",
      "Gradient Descent(39/49): loss=2703.759157173026\n",
      "Gradient Descent(40/49): loss=3575.5763045465087\n",
      "Gradient Descent(41/49): loss=4728.554481947441\n",
      "Gradient Descent(42/49): loss=6253.36812156053\n",
      "Gradient Descent(43/49): loss=8269.934159949824\n",
      "Gradient Descent(44/49): loss=10936.84274572001\n",
      "Gradient Descent(45/49): loss=14463.82935040081\n",
      "Gradient Descent(46/49): loss=19128.269135090577\n",
      "Gradient Descent(47/49): loss=25296.990750341058\n",
      "Gradient Descent(48/49): loss=33455.1250865112\n",
      "Gradient Descent(49/49): loss=44244.25774610098\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5191407046893569\n",
      "Gradient Descent(2/49): loss=0.5457922218988194\n",
      "Gradient Descent(3/49): loss=0.5829017944612804\n",
      "Gradient Descent(4/49): loss=0.6345731632972255\n",
      "Gradient Descent(5/49): loss=0.706520377264405\n",
      "Gradient Descent(6/49): loss=0.8066996779922962\n",
      "Gradient Descent(7/49): loss=0.9461893363258991\n",
      "Gradient Descent(8/49): loss=1.1404147365895683\n",
      "Gradient Descent(9/49): loss=1.410854183916772\n",
      "Gradient Descent(10/49): loss=1.787414070375042\n",
      "Gradient Descent(11/49): loss=2.3117360562795874\n",
      "Gradient Descent(12/49): loss=3.041801989452944\n",
      "Gradient Descent(13/49): loss=4.0583457948040795\n",
      "Gradient Descent(14/49): loss=5.47378138937384\n",
      "Gradient Descent(15/49): loss=7.444633911253718\n",
      "Gradient Descent(16/49): loss=10.188848962719389\n",
      "Gradient Descent(17/49): loss=14.009894000378607\n",
      "Gradient Descent(18/49): loss=19.330317110817205\n",
      "Gradient Descent(19/49): loss=26.73847424979112\n",
      "Gradient Descent(20/49): loss=37.053592250102106\n",
      "Gradient Descent(21/49): loss=51.416362553732206\n",
      "Gradient Descent(22/49): loss=71.41508392451223\n",
      "Gradient Descent(23/49): loss=99.26130356117967\n",
      "Gradient Descent(24/49): loss=138.03437978326946\n",
      "Gradient Descent(25/49): loss=192.0220111149131\n",
      "Gradient Descent(26/49): loss=267.1943889810824\n",
      "Gradient Descent(27/49): loss=371.864407921969\n",
      "Gradient Descent(28/49): loss=517.6069422952647\n",
      "Gradient Descent(29/49): loss=720.5388471565453\n",
      "Gradient Descent(30/49): loss=1003.1012314855544\n",
      "Gradient Descent(31/49): loss=1396.5410954250135\n",
      "Gradient Descent(32/49): loss=1944.3667619747696\n",
      "Gradient Descent(33/49): loss=2707.1592200785685\n",
      "Gradient Descent(34/49): loss=3769.2714387419283\n",
      "Gradient Descent(35/49): loss=5248.156492008572\n",
      "Gradient Descent(36/49): loss=7307.35604017742\n",
      "Gradient Descent(37/49): loss=10174.585491046792\n",
      "Gradient Descent(38/49): loss=14166.915778436645\n",
      "Gradient Descent(39/49): loss=19725.836470597427\n",
      "Gradient Descent(40/49): loss=27466.07764236221\n",
      "Gradient Descent(41/49): loss=38243.58944992648\n",
      "Gradient Descent(42/49): loss=53250.19689078656\n",
      "Gradient Descent(43/49): loss=74145.39709143028\n",
      "Gradient Descent(44/49): loss=103239.87385081116\n",
      "Gradient Descent(45/49): loss=143751.02329058165\n",
      "Gradient Descent(46/49): loss=200158.7477705145\n",
      "Gradient Descent(47/49): loss=278700.863336398\n",
      "Gradient Descent(48/49): loss=388062.9050503332\n",
      "Gradient Descent(49/49): loss=540338.6119327882\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5196208562956289\n",
      "Gradient Descent(2/49): loss=0.5469409366016668\n",
      "Gradient Descent(3/49): loss=0.584981416419786\n",
      "Gradient Descent(4/49): loss=0.6379489805185468\n",
      "Gradient Descent(5/49): loss=0.7117010167696315\n",
      "Gradient Descent(6/49): loss=0.8143933520457088\n",
      "Gradient Descent(7/49): loss=0.9573821596840949\n",
      "Gradient Descent(8/49): loss=1.1564797754397949\n",
      "Gradient Descent(9/49): loss=1.4337032956178963\n",
      "Gradient Descent(10/49): loss=1.8197093251139338\n",
      "Gradient Descent(11/49): loss=2.3571841205841895\n",
      "Gradient Descent(12/49): loss=3.1055640257968946\n",
      "Gradient Descent(13/49): loss=4.1476082058149375\n",
      "Gradient Descent(14/49): loss=5.5985505220725225\n",
      "Gradient Descent(15/49): loss=7.618842603228738\n",
      "Gradient Descent(16/49): loss=10.431897297031822\n",
      "Gradient Descent(17/49): loss=14.348794652683907\n",
      "Gradient Descent(18/49): loss=19.802682530689854\n",
      "Gradient Descent(19/49): loss=27.396676012028383\n",
      "Gradient Descent(20/49): loss=37.97055253544564\n",
      "Gradient Descent(21/49): loss=52.693618206648004\n",
      "Gradient Descent(22/49): loss=73.19401484722705\n",
      "Gradient Descent(23/49): loss=101.73876712957164\n",
      "Gradient Descent(24/49): loss=141.48448020749774\n",
      "Gradient Descent(25/49): loss=196.82641109719134\n",
      "Gradient Descent(26/49): loss=273.8845156680541\n",
      "Gradient Descent(27/49): loss=381.1802204725079\n",
      "Gradient Descent(28/49): loss=530.5787598422213\n",
      "Gradient Descent(29/49): loss=738.6012860606518\n",
      "Gradient Descent(30/49): loss=1028.2518515671993\n",
      "Gradient Descent(31/49): loss=1431.561298978567\n",
      "Gradient Descent(32/49): loss=1993.1293735539919\n",
      "Gradient Descent(33/49): loss=2775.056760593042\n",
      "Gradient Descent(34/49): loss=3863.8124543059103\n",
      "Gradient Descent(35/49): loss=5379.795882231893\n",
      "Gradient Descent(36/49): loss=7490.651207276188\n",
      "Gradient Descent(37/49): loss=10429.806161867702\n",
      "Gradient Descent(38/49): loss=14522.28552064109\n",
      "Gradient Descent(39/49): loss=20220.653779798413\n",
      "Gradient Descent(40/49): loss=28155.0617438504\n",
      "Gradient Descent(41/49): loss=39202.9313929948\n",
      "Gradient Descent(42/49): loss=54585.985092469266\n",
      "Gradient Descent(43/49): loss=76005.34906361425\n",
      "Gradient Descent(44/49): loss=105829.67145704065\n",
      "Gradient Descent(45/49): loss=147357.0579576293\n",
      "Gradient Descent(46/49): loss=205179.79092103848\n",
      "Gradient Descent(47/49): loss=285692.1642993411\n",
      "Gradient Descent(48/49): loss=397797.5929912232\n",
      "Gradient Descent(49/49): loss=553893.1919018419\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5193938520320002\n",
      "Gradient Descent(2/49): loss=0.5463978516013489\n",
      "Gradient Descent(3/49): loss=0.5839982206017147\n",
      "Gradient Descent(4/49): loss=0.6363529743978317\n",
      "Gradient Descent(5/49): loss=0.7092517335835172\n",
      "Gradient Descent(6/49): loss=0.8107559658737278\n",
      "Gradient Descent(7/49): loss=0.952090458914603\n",
      "Gradient Descent(8/49): loss=1.1488846070247354\n",
      "Gradient Descent(9/49): loss=1.4229007788531802\n",
      "Gradient Descent(10/49): loss=1.804440896507281\n",
      "Gradient Descent(11/49): loss=2.3356973563289234\n",
      "Gradient Descent(12/49): loss=3.075418850984489\n",
      "Gradient Descent(13/49): loss=4.105407060143331\n",
      "Gradient Descent(14/49): loss=5.539562642575008\n",
      "Gradient Descent(15/49): loss=7.536480875553058\n",
      "Gradient Descent(16/49): loss=10.316989823152825\n",
      "Gradient Descent(17/49): loss=14.188570481788695\n",
      "Gradient Descent(18/49): loss=19.57935939087545\n",
      "Gradient Descent(19/49): loss=27.08549386788368\n",
      "Gradient Descent(20/49): loss=37.537035513676194\n",
      "Gradient Descent(21/49): loss=52.08976210126992\n",
      "Gradient Descent(22/49): loss=72.35297860183923\n",
      "Gradient Descent(23/49): loss=100.56748125722643\n",
      "Gradient Descent(24/49): loss=139.85335475460167\n",
      "Gradient Descent(25/49): loss=194.5550050123257\n",
      "Gradient Descent(26/49): loss=270.7215828311736\n",
      "Gradient Descent(27/49): loss=376.775925786184\n",
      "Gradient Descent(28/49): loss=524.4459929166768\n",
      "Gradient Descent(29/49): loss=730.0617943891879\n",
      "Gradient Descent(30/49): loss=1016.3612363595023\n",
      "Gradient Descent(31/49): loss=1415.004579359079\n",
      "Gradient Descent(32/49): loss=1970.0755701515181\n",
      "Gradient Descent(33/49): loss=2742.956417731309\n",
      "Gradient Descent(34/49): loss=3819.1157099007783\n",
      "Gradient Descent(35/49): loss=5317.559908318315\n",
      "Gradient Descent(36/49): loss=7403.99361019501\n",
      "Gradient Descent(37/49): loss=10309.14389668735\n",
      "Gradient Descent(38/49): loss=14354.275155597908\n",
      "Gradient Descent(39/49): loss=19986.715920504783\n",
      "Gradient Descent(40/49): loss=27829.326441563146\n",
      "Gradient Descent(41/49): loss=38749.37733108674\n",
      "Gradient Descent(42/49): loss=53954.45618965188\n",
      "Gradient Descent(43/49): loss=75126.00799233143\n",
      "Gradient Descent(44/49): loss=104605.27672238597\n",
      "Gradient Descent(45/49): loss=145652.21050210093\n",
      "Gradient Descent(46/49): loss=202805.96109696286\n",
      "Gradient Descent(47/49): loss=282386.8434252476\n",
      "Gradient Descent(48/49): loss=393195.26397918956\n",
      "Gradient Descent(49/49): loss=547484.9087584843\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5195520878028802\n",
      "Gradient Descent(2/49): loss=0.546776414859612\n",
      "Gradient Descent(3/49): loss=0.5846835678534027\n",
      "Gradient Descent(4/49): loss=0.6374654876819511\n",
      "Gradient Descent(5/49): loss=0.7109590328512437\n",
      "Gradient Descent(6/49): loss=0.8132914451449128\n",
      "Gradient Descent(7/49): loss=0.9557790960226316\n",
      "Gradient Descent(8/49): loss=1.1541789011047539\n",
      "Gradient Descent(9/49): loss=1.4304307897010713\n",
      "Gradient Descent(10/49): loss=1.8150839193824908\n",
      "Gradient Descent(11/49): loss=2.350674937150955\n",
      "Gradient Descent(12/49): loss=3.096431870292191\n",
      "Gradient Descent(13/49): loss=4.134823823997343\n",
      "Gradient Descent(14/49): loss=5.580680780337265\n",
      "Gradient Descent(15/49): loss=7.593892006344883\n",
      "Gradient Descent(16/49): loss=10.397087317437547\n",
      "Gradient Descent(17/49): loss=14.300256468602349\n",
      "Gradient Descent(18/49): loss=19.735029194684603\n",
      "Gradient Descent(19/49): loss=27.3024067384813\n",
      "Gradient Descent(20/49): loss=37.83922323046343\n",
      "Gradient Descent(21/49): loss=52.51068651390476\n",
      "Gradient Descent(22/49): loss=72.93923198976941\n",
      "Gradient Descent(23/49): loss=101.38393871034694\n",
      "Gradient Descent(24/49): loss=140.99034834808228\n",
      "Gradient Descent(25/49): loss=196.13831312767735\n",
      "Gradient Descent(26/49): loss=272.9263392867574\n",
      "Gradient Descent(27/49): loss=379.8459869106748\n",
      "Gradient Descent(28/49): loss=528.7209042622334\n",
      "Gradient Descent(29/49): loss=736.0143391826072\n",
      "Gradient Descent(30/49): loss=1024.649717965576\n",
      "Gradient Descent(31/49): loss=1426.5456193831842\n",
      "Gradient Descent(32/49): loss=1986.1454725169888\n",
      "Gradient Descent(33/49): loss=2765.332308020317\n",
      "Gradient Descent(34/49): loss=3850.2720577754762\n",
      "Gradient Descent(35/49): loss=5360.942165334379\n",
      "Gradient Descent(36/49): loss=7464.399223099507\n",
      "Gradient Descent(37/49): loss=10393.252830331821\n",
      "Gradient Descent(38/49): loss=14471.388593040034\n",
      "Gradient Descent(39/49): loss=20149.784829038832\n",
      "Gradient Descent(40/49): loss=28056.383748040684\n",
      "Gradient Descent(41/49): loss=39065.53208285595\n",
      "Gradient Descent(42/49): loss=54394.670224253896\n",
      "Gradient Descent(43/49): loss=75738.96217233094\n",
      "Gradient Descent(44/49): loss=105458.7542808541\n",
      "Gradient Descent(45/49): loss=146840.59281276847\n",
      "Gradient Descent(46/49): loss=204460.6647846036\n",
      "Gradient Descent(47/49): loss=284690.8529981545\n",
      "Gradient Descent(48/49): loss=396403.3670667653\n",
      "Gradient Descent(49/49): loss=551951.8716558021\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5226381270293846\n",
      "Gradient Descent(2/49): loss=0.5557826088131133\n",
      "Gradient Descent(3/49): loss=0.6043094445926543\n",
      "Gradient Descent(4/49): loss=0.675357584857495\n",
      "Gradient Descent(5/49): loss=0.7793791670192489\n",
      "Gradient Descent(6/49): loss=0.9316771654622926\n",
      "Gradient Descent(7/49): loss=1.1546566649827616\n",
      "Gradient Descent(8/49): loss=1.4811209502306801\n",
      "Gradient Descent(9/49): loss=1.959097310262317\n",
      "Gradient Descent(10/49): loss=2.658902498984409\n",
      "Gradient Descent(11/49): loss=3.6834872757928165\n",
      "Gradient Descent(12/49): loss=5.183581847517992\n",
      "Gradient Descent(13/49): loss=7.379870309981118\n",
      "Gradient Descent(14/49): loss=10.595456247873024\n",
      "Gradient Descent(15/49): loss=15.303395619542306\n",
      "Gradient Descent(16/49): loss=22.196289653601696\n",
      "Gradient Descent(17/49): loss=32.28817580886392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=47.063706328792385\n",
      "Gradient Descent(19/49): loss=68.69656056301973\n",
      "Gradient Descent(20/49): loss=100.3692224473394\n",
      "Gradient Descent(21/49): loss=146.74116671218368\n",
      "Gradient Descent(22/49): loss=214.63433031031295\n",
      "Gradient Descent(23/49): loss=314.0367111343715\n",
      "Gradient Descent(24/49): loss=459.5717368988312\n",
      "Gradient Descent(25/49): loss=672.6495681205912\n",
      "Gradient Descent(26/49): loss=984.6168208124323\n",
      "Gradient Descent(27/49): loss=1441.3680754785853\n",
      "Gradient Descent(28/49): loss=2110.0975874351093\n",
      "Gradient Descent(29/49): loss=3089.1844658907758\n",
      "Gradient Descent(30/49): loss=4522.66556463787\n",
      "Gradient Descent(31/49): loss=6621.425241313374\n",
      "Gradient Descent(32/49): loss=9694.21928393509\n",
      "Gradient Descent(33/49): loss=14193.097041736364\n",
      "Gradient Descent(34/49): loss=20779.903966934457\n",
      "Gradient Descent(35/49): loss=30423.647986114836\n",
      "Gradient Descent(36/49): loss=44543.05360459435\n",
      "Gradient Descent(37/49): loss=65215.275370608324\n",
      "Gradient Descent(38/49): loss=95481.47525823578\n",
      "Gradient Descent(39/49): loss=139794.21851371968\n",
      "Gradient Descent(40/49): loss=204672.50591404346\n",
      "Gradient Descent(41/49): loss=299660.80649690673\n",
      "Gradient Descent(42/49): loss=438733.1773802199\n",
      "Gradient Descent(43/49): loss=642349.0355904471\n",
      "Gradient Descent(44/49): loss=940463.0135961529\n",
      "Gradient Descent(45/49): loss=1376931.6887941803\n",
      "Gradient Descent(46/49): loss=2015965.4761517146\n",
      "Gradient Descent(47/49): loss=2951574.844221746\n",
      "Gradient Descent(48/49): loss=4321400.520013583\n",
      "Gradient Descent(49/49): loss=6326962.2919403\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.523206012759433\n",
      "Gradient Descent(2/49): loss=0.5571819360405089\n",
      "Gradient Descent(3/49): loss=0.6069260853163407\n",
      "Gradient Descent(4/49): loss=0.6797564942710753\n",
      "Gradient Descent(5/49): loss=0.7863874960217268\n",
      "Gradient Descent(6/49): loss=0.9425059456848371\n",
      "Gradient Descent(7/49): loss=1.1710789678366462\n",
      "Gradient Descent(8/49): loss=1.505732729569168\n",
      "Gradient Descent(9/49): loss=1.9956993021216396\n",
      "Gradient Descent(10/49): loss=2.7130593609959286\n",
      "Gradient Descent(11/49): loss=3.7633462231937975\n",
      "Gradient Descent(12/49): loss=5.301071218137027\n",
      "Gradient Descent(13/49): loss=7.552454383233376\n",
      "Gradient Descent(14/49): loss=10.848704475251076\n",
      "Gradient Descent(15/49): loss=15.674744234972746\n",
      "Gradient Descent(16/49): loss=22.740549047180828\n",
      "Gradient Descent(17/49): loss=33.08559387273696\n",
      "Gradient Descent(18/49): loss=48.23177400182971\n",
      "Gradient Descent(19/49): loss=70.4072963288457\n",
      "Gradient Descent(20/49): loss=102.8744785678146\n",
      "Gradient Descent(21/49): loss=150.40968008389885\n",
      "Gradient Descent(22/49): loss=220.0059686236077\n",
      "Gradient Descent(23/49): loss=321.9018946745684\n",
      "Gradient Descent(24/49): loss=471.0877200058172\n",
      "Gradient Descent(25/49): loss=689.5106868733576\n",
      "Gradient Descent(26/49): loss=1009.3037526640672\n",
      "Gradient Descent(27/49): loss=1477.512780288139\n",
      "Gradient Descent(28/49): loss=2163.017617632725\n",
      "Gradient Descent(29/49): loss=3166.6652499887246\n",
      "Gradient Descent(30/49): loss=4636.105748520945\n",
      "Gradient Descent(31/49): loss=6787.513582422624\n",
      "Gradient Descent(32/49): loss=9937.389792036769\n",
      "Gradient Descent(33/49): loss=14549.123550534818\n",
      "Gradient Descent(34/49): loss=21301.162946352946\n",
      "Gradient Descent(35/49): loss=31186.823825768533\n",
      "Gradient Descent(36/49): loss=45660.41991931726\n",
      "Gradient Descent(37/49): loss=66851.211959888\n",
      "Gradient Descent(38/49): loss=97876.65058648387\n",
      "Gradient Descent(39/49): loss=143300.99527968178\n",
      "Gradient Descent(40/49): loss=209806.77834498096\n",
      "Gradient Descent(41/49): loss=307177.8953309336\n",
      "Gradient Descent(42/49): loss=449738.9477100942\n",
      "Gradient Descent(43/49): loss=658462.5844984109\n",
      "Gradient Descent(44/49): loss=964054.8611202042\n",
      "Gradient Descent(45/49): loss=1411472.5133222686\n",
      "Gradient Descent(46/49): loss=2066536.6979113007\n",
      "Gradient Descent(47/49): loss=3025616.170567663\n",
      "Gradient Descent(48/49): loss=4429804.426483692\n",
      "Gradient Descent(49/49): loss=6485676.451970483\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.522937529888\n",
      "Gradient Descent(2/49): loss=0.5565203673970186\n",
      "Gradient Descent(3/49): loss=0.6056889997939624\n",
      "Gradient Descent(4/49): loss=0.6776767944863284\n",
      "Gradient Descent(5/49): loss=0.7830741246954375\n",
      "Gradient Descent(6/49): loss=0.9373863558545602\n",
      "Gradient Descent(7/49): loss=1.1633148934946635\n",
      "Gradient Descent(8/49): loss=1.4940968654534217\n",
      "Gradient Descent(9/49): loss=1.9783947505985064\n",
      "Gradient Descent(10/49): loss=2.687455284239556\n",
      "Gradient Descent(11/49): loss=3.725590811543083\n",
      "Gradient Descent(12/49): loss=5.245525037067636\n",
      "Gradient Descent(13/49): loss=7.470860736658418\n",
      "Gradient Descent(14/49): loss=10.728974734429345\n",
      "Gradient Descent(15/49): loss=15.499179438566959\n",
      "Gradient Descent(16/49): loss=22.483236145895475\n",
      "Gradient Descent(17/49): loss=32.708593571093665\n",
      "Gradient Descent(18/49): loss=47.679539377330244\n",
      "Gradient Descent(19/49): loss=69.59850113222814\n",
      "Gradient Descent(20/49): loss=101.69005303757517\n",
      "Gradient Descent(21/49): loss=148.67529418218794\n",
      "Gradient Descent(22/49): loss=217.46638574201828\n",
      "Gradient Descent(23/49): loss=318.18342289475595\n",
      "Gradient Descent(24/49): loss=465.6432369901142\n",
      "Gradient Descent(25/49): loss=681.5391508071731\n",
      "Gradient Descent(26/49): loss=997.632358226637\n",
      "Gradient Descent(27/49): loss=1460.4244232094607\n",
      "Gradient Descent(28/49): loss=2137.9982855509365\n",
      "Gradient Descent(29/49): loss=3130.034177404803\n",
      "Gradient Descent(30/49): loss=4582.473926668446\n",
      "Gradient Descent(31/49): loss=6708.990963565748\n",
      "Gradient Descent(32/49): loss=9822.424557287051\n",
      "Gradient Descent(33/49): loss=14380.802681854082\n",
      "Gradient Descent(34/49): loss=21054.72409403121\n",
      "Gradient Descent(35/49): loss=30826.012433597858\n",
      "Gradient Descent(36/49): loss=45132.15569156374\n",
      "Gradient Descent(37/49): loss=66077.78003555104\n",
      "Gradient Descent(38/49): loss=96744.26863758928\n",
      "Gradient Descent(39/49): loss=141643.07459981827\n",
      "Gradient Descent(40/49): loss=207379.41640913152\n",
      "Gradient Descent(41/49): loss=303623.99445213645\n",
      "Gradient Descent(42/49): loss=444535.6811649157\n",
      "Gradient Descent(43/49): loss=650844.4816811142\n",
      "Gradient Descent(44/49): loss=952901.1965168015\n",
      "Gradient Descent(45/49): loss=1395142.4327076343\n",
      "Gradient Descent(46/49): loss=2042627.8266148216\n",
      "Gradient Descent(47/49): loss=2990611.1918342076\n",
      "Gradient Descent(48/49): loss=4378553.636852032\n",
      "Gradient Descent(49/49): loss=6410640.170602185\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5231246787699201\n",
      "Gradient Descent(2/49): loss=0.5569815209569707\n",
      "Gradient Descent(3/49): loss=0.6065513236030189\n",
      "Gradient Descent(4/49): loss=0.6791264716571136\n",
      "Gradient Descent(5/49): loss=0.7853837459230752\n",
      "Gradient Descent(6/49): loss=0.9409550211759663\n",
      "Gradient Descent(7/49): loss=1.1687269252736836\n",
      "Gradient Descent(8/49): loss=1.5022077700630259\n",
      "Gradient Descent(9/49): loss=1.9904570749194075\n",
      "Gradient Descent(10/49): loss=2.7053028821592333\n",
      "Gradient Descent(11/49): loss=3.7519086285389016\n",
      "Gradient Descent(12/49): loss=5.284244101813612\n",
      "Gradient Descent(13/49): loss=7.52773646823492\n",
      "Gradient Descent(14/49): loss=10.812433641913458\n",
      "Gradient Descent(15/49): loss=15.621558773896302\n",
      "Gradient Descent(16/49): loss=22.662598879632736\n",
      "Gradient Descent(17/49): loss=32.971385698438645\n",
      "Gradient Descent(18/49): loss=48.06448047985764\n",
      "Gradient Descent(19/49): loss=70.16228054932775\n",
      "Gradient Descent(20/49): loss=102.51566963104705\n",
      "Gradient Descent(21/49): loss=149.88426658560073\n",
      "Gradient Descent(22/49): loss=219.23662938673178\n",
      "Gradient Descent(23/49): loss=320.77542376389505\n",
      "Gradient Descent(24/49): loss=469.43837261145035\n",
      "Gradient Descent(25/49): loss=687.0957960192536\n",
      "Gradient Descent(26/49): loss=1005.7680296304796\n",
      "Gradient Descent(27/49): loss=1472.3360468605883\n",
      "Gradient Descent(28/49): loss=2155.4382808875534\n",
      "Gradient Descent(29/49): loss=3155.5682617260527\n",
      "Gradient Descent(30/49): loss=4619.858566671437\n",
      "Gradient Descent(31/49): loss=6763.726002142697\n",
      "Gradient Descent(32/49): loss=9902.562314417295\n",
      "Gradient Descent(33/49): loss=14498.132559215981\n",
      "Gradient Descent(34/49): loss=21226.50695462876\n",
      "Gradient Descent(35/49): loss=31077.519906949077\n",
      "Gradient Descent(36/49): loss=45500.38797044582\n",
      "Gradient Descent(37/49): loss=66616.90910221495\n",
      "Gradient Descent(38/49): loss=97533.60769123417\n",
      "Gradient Descent(39/49): loss=142798.7460953977\n",
      "Gradient Descent(40/49): loss=209071.43523294962\n",
      "Gradient Descent(41/49): loss=306101.27939923835\n",
      "Gradient Descent(42/49): loss=448162.67424313625\n",
      "Gradient Descent(43/49): loss=656154.7624339724\n",
      "Gradient Descent(44/49): loss=960675.9787543191\n",
      "Gradient Descent(45/49): loss=1406525.4915688843\n",
      "Gradient Descent(46/49): loss=2059293.7632809551\n",
      "Gradient Descent(47/49): loss=3015011.7898939643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=4414278.552658237\n",
      "Gradient Descent(49/49): loss=6462945.0200211145\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5262233507670702\n",
      "Gradient Descent(2/49): loss=0.5665443749065251\n",
      "Gradient Descent(3/49): loss=0.6285419816233269\n",
      "Gradient Descent(4/49): loss=0.7238695017110761\n",
      "Gradient Descent(5/49): loss=0.8704450965979936\n",
      "Gradient Descent(6/49): loss=1.0958197312960976\n",
      "Gradient Descent(7/49): loss=1.4423557696080191\n",
      "Gradient Descent(8/49): loss=1.9751895821165666\n",
      "Gradient Descent(9/49): loss=2.794474852229426\n",
      "Gradient Descent(10/49): loss=4.054207883554725\n",
      "Gradient Descent(11/49): loss=5.991173392521067\n",
      "Gradient Descent(12/49): loss=8.969451559107794\n",
      "Gradient Descent(13/49): loss=13.54885206805005\n",
      "Gradient Descent(14/49): loss=20.59013829060168\n",
      "Gradient Descent(15/49): loss=31.41681998639536\n",
      "Gradient Descent(16/49): loss=48.06392576185237\n",
      "Gradient Descent(17/49): loss=73.66051560219437\n",
      "Gradient Descent(18/49): loss=113.0178321407102\n",
      "Gradient Descent(19/49): loss=173.53364205031653\n",
      "Gradient Descent(20/49): loss=266.5827513673459\n",
      "Gradient Descent(21/49): loss=409.6550618531968\n",
      "Gradient Descent(22/49): loss=629.6430464562919\n",
      "Gradient Descent(23/49): loss=967.8965715819675\n",
      "Gradient Descent(24/49): loss=1487.995191815248\n",
      "Gradient Descent(25/49): loss=2287.6988302858704\n",
      "Gradient Descent(26/49): loss=3517.323144798603\n",
      "Gradient Descent(27/49): loss=5407.993490793506\n",
      "Gradient Descent(28/49): loss=8315.088214795676\n",
      "Gradient Descent(29/49): loss=12785.03706241901\n",
      "Gradient Descent(30/49): loss=19658.030410523625\n",
      "Gradient Descent(31/49): loss=30225.944982571356\n",
      "Gradient Descent(32/49): loss=46475.17042854787\n",
      "Gradient Descent(33/49): loss=71459.97947429103\n",
      "Gradient Descent(34/49): loss=109876.62186303255\n",
      "Gradient Descent(35/49): loss=168946.05119996102\n",
      "Gradient Descent(36/49): loss=259771.20574837975\n",
      "Gradient Descent(37/49): loss=399423.96338205086\n",
      "Gradient Descent(38/49): loss=614154.0435196189\n",
      "Gradient Descent(39/49): loss=944323.0147391573\n",
      "Gradient Descent(40/49): loss=1451990.8248864042\n",
      "Gradient Descent(41/49): loss=2232580.8497686624\n",
      "Gradient Descent(42/49): loss=3432816.0720274546\n",
      "Gradient Descent(43/49): loss=5278297.7497722125\n",
      "Gradient Descent(44/49): loss=8115910.3774724705\n",
      "Gradient Descent(45/49): loss=12479023.553823784\n",
      "Gradient Descent(46/49): loss=19187746.373781078\n",
      "Gradient Descent(47/49): loss=29503078.58174925\n",
      "Gradient Descent(48/49): loss=45363933.384724066\n",
      "Gradient Descent(49/49): loss=69751583.72977883\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5268811731511981\n",
      "Gradient Descent(2/49): loss=0.5682136649884731\n",
      "Gradient Descent(3/49): loss=0.631766504437485\n",
      "Gradient Descent(4/49): loss=0.7294853503742502\n",
      "Gradient Descent(5/49): loss=0.8797378478865913\n",
      "Gradient Descent(6/49): loss=1.1107660880616717\n",
      "Gradient Descent(7/49): loss=1.4659951101548\n",
      "Gradient Descent(8/49): loss=2.012195254525116\n",
      "Gradient Descent(9/49): loss=2.852032596508804\n",
      "Gradient Descent(10/49): loss=4.143366493543018\n",
      "Gradient Descent(11/49): loss=6.128921493622531\n",
      "Gradient Descent(12/49): loss=9.18191086174607\n",
      "Gradient Descent(13/49): loss=13.876187314171133\n",
      "Gradient Descent(14/49): loss=21.09410678742037\n",
      "Gradient Descent(15/49): loss=32.192379769487765\n",
      "Gradient Descent(16/49): loss=49.25708430671214\n",
      "Gradient Descent(17/49): loss=75.49577400314993\n",
      "Gradient Descent(18/49): loss=115.840383280404\n",
      "Gradient Descent(19/49): loss=177.87425450512282\n",
      "Gradient Descent(20/49): loss=273.25753490019355\n",
      "Gradient Descent(21/49): loss=419.9188668357172\n",
      "Gradient Descent(22/49): loss=645.4253308197591\n",
      "Gradient Descent(23/49): loss=992.1640698416817\n",
      "Gradient Descent(24/49): loss=1525.3095549616241\n",
      "Gradient Descent(25/49): loss=2345.0740528819715\n",
      "Gradient Descent(26/49): loss=3605.5439448843003\n",
      "Gradient Descent(27/49): loss=5543.642450827617\n",
      "Gradient Descent(28/49): loss=8523.662713566033\n",
      "Gradient Descent(29/49): loss=13105.741869552216\n",
      "Gradient Descent(30/49): loss=20151.14677979399\n",
      "Gradient Descent(31/49): loss=30984.16136978301\n",
      "Gradient Descent(32/49): loss=47641.0046033572\n",
      "Gradient Descent(33/49): loss=73252.56675929717\n",
      "Gradient Descent(34/49): loss=112632.90473026036\n",
      "Gradient Descent(35/49): loss=173184.11239442843\n",
      "Gradient Descent(36/49): loss=266287.64929886995\n",
      "Gradient Descent(37/49): loss=409443.6476430778\n",
      "Gradient Descent(38/49): loss=629560.3106971986\n",
      "Gradient Descent(39/49): loss=968011.6918091221\n",
      "Gradient Descent(40/49): loss=1488414.535407021\n",
      "Gradient Descent(41/49): loss=2288585.947723226\n",
      "Gradient Descent(42/49): loss=3518929.5113005512\n",
      "Gradient Descent(43/49): loss=5410705.77465614\n",
      "Gradient Descent(44/49): loss=8319500.957192569\n",
      "Gradient Descent(45/49): loss=12792064.429861214\n",
      "Gradient Descent(46/49): loss=19669078.025435466\n",
      "Gradient Descent(47/49): loss=30243174.12999262\n",
      "Gradient Descent(48/49): loss=46501904.30036037\n",
      "Gradient Descent(49/49): loss=71501327.81032147\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5265701703680001\n",
      "Gradient Descent(2/49): loss=0.5674244643258265\n",
      "Gradient Descent(3/49): loss=0.6302420267153812\n",
      "Gradient Descent(4/49): loss=0.7268303106455555\n",
      "Gradient Descent(5/49): loss=0.8753444560165804\n",
      "Gradient Descent(6/49): loss=1.1036998059390655\n",
      "Gradient Descent(7/49): loss=1.454818991979992\n",
      "Gradient Descent(8/49): loss=1.9946998524364692\n",
      "Gradient Descent(9/49): loss=2.824820663474118\n",
      "Gradient Descent(10/49): loss=4.101214422525673\n",
      "Gradient Descent(11/49): loss=6.063797466443222\n",
      "Gradient Descent(12/49): loss=9.081465154771434\n",
      "Gradient Descent(13/49): loss=13.7214309923443\n",
      "Gradient Descent(14/49): loss=20.855842464195746\n",
      "Gradient Descent(15/49): loss=31.82571354331684\n",
      "Gradient Descent(16/49): loss=48.6929873145743\n",
      "Gradient Descent(17/49): loss=74.62810746525537\n",
      "Gradient Descent(18/49): loss=114.50594820893822\n",
      "Gradient Descent(19/49): loss=175.82211613644483\n",
      "Gradient Descent(20/49): loss=270.1018559417916\n",
      "Gradient Descent(21/49): loss=415.066383866454\n",
      "Gradient Descent(22/49): loss=637.9638420034352\n",
      "Gradient Descent(23/49): loss=980.690973634933\n",
      "Gradient Descent(24/49): loss=1507.6682112315173\n",
      "Gradient Descent(25/49): loss=2317.9484117601196\n",
      "Gradient Descent(26/49): loss=3563.835248092302\n",
      "Gradient Descent(27/49): loss=5479.510847636991\n",
      "Gradient Descent(28/49): loss=8425.053649497058\n",
      "Gradient Descent(29/49): loss=12954.120261636368\n",
      "Gradient Descent(30/49): loss=19918.0130844625\n",
      "Gradient Descent(31/49): loss=30625.69468884136\n",
      "Gradient Descent(32/49): loss=47089.82592373355\n",
      "Gradient Descent(33/49): loss=72405.07411050689\n",
      "Gradient Descent(34/49): loss=111329.79972247517\n",
      "Gradient Descent(35/49): loss=171180.45782344713\n",
      "Gradient Descent(36/49): loss=263206.8297194809\n",
      "Gradient Descent(37/49): loss=404706.57914684963\n",
      "Gradient Descent(38/49): loss=622276.5938663778\n",
      "Gradient Descent(39/49): loss=956812.2484990612\n",
      "Gradient Descent(40/49): loss=1471194.2710623464\n",
      "Gradient Descent(41/49): loss=2262108.068955713\n",
      "Gradient Descent(42/49): loss=3478217.1245968146\n",
      "Gradient Descent(43/49): loss=5348106.408550649\n",
      "Gradient Descent(44/49): loss=8223248.171558407\n",
      "Gradient Descent(45/49): loss=12644066.146357492\n",
      "Gradient Descent(46/49): loss=19441515.864408046\n",
      "Gradient Descent(47/49): loss=29893274.550885603\n",
      "Gradient Descent(48/49): loss=45963898.70721579\n",
      "Gradient Descent(49/49): loss=70674090.40998185\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.52678695821312\n",
      "Gradient Descent(2/49): loss=0.5679745851616169\n",
      "Gradient Descent(3/49): loss=0.6313046803576399\n",
      "Gradient Descent(4/49): loss=0.7286810347310163\n",
      "Gradient Descent(5/49): loss=0.8784069172155169\n",
      "Gradient Descent(6/49): loss=1.108625434123643\n",
      "Gradient Descent(7/49): loss=1.4626094257216808\n",
      "Gradient Descent(8/49): loss=2.006895211202714\n",
      "Gradient Descent(9/49): loss=2.8437890349587036\n",
      "Gradient Descent(10/49): loss=4.130596978365273\n",
      "Gradient Descent(11/49): loss=6.109192872147796\n",
      "Gradient Descent(12/49): loss=9.151481918427622\n",
      "Gradient Descent(13/49): loss=13.829305555986494\n",
      "Gradient Descent(14/49): loss=21.021927181096466\n",
      "Gradient Descent(15/49): loss=32.081302191867444\n",
      "Gradient Descent(16/49): loss=49.086197208429326\n",
      "Gradient Descent(17/49): loss=75.23292378590484\n",
      "Gradient Descent(18/49): loss=115.4361305714228\n",
      "Gradient Descent(19/49): loss=177.25258132481687\n",
      "Gradient Descent(20/49): loss=272.30155600323155\n",
      "Gradient Descent(21/49): loss=418.44885946880174\n",
      "Gradient Descent(22/49): loss=643.1649532774697\n",
      "Gradient Descent(23/49): loss=988.688419117692\n",
      "Gradient Descent(24/49): loss=1519.9653001936888\n",
      "Gradient Descent(25/49): loss=2336.8566325362513\n",
      "Gradient Descent(26/49): loss=3592.9087451460045\n",
      "Gradient Descent(27/49): loss=5524.214473494285\n",
      "Gradient Descent(28/49): loss=8493.790161403389\n",
      "Gradient Descent(29/49): loss=13059.809739131419\n",
      "Gradient Descent(30/49): loss=20080.521441849614\n",
      "Gradient Descent(31/49): loss=30875.56775594759\n",
      "Gradient Descent(32/49): loss=47474.030968499836\n",
      "Gradient Descent(33/49): loss=72995.82800412491\n",
      "Gradient Descent(34/49): loss=112238.14312610586\n",
      "Gradient Descent(35/49): loss=172577.12685766767\n",
      "Gradient Descent(36/49): loss=265354.3482433143\n",
      "Gradient Descent(37/49): loss=408008.6038458719\n",
      "Gradient Descent(38/49): loss=627353.7872603281\n",
      "Gradient Descent(39/49): loss=964618.941278501\n",
      "Gradient Descent(40/49): loss=1483197.842096692\n",
      "Gradient Descent(41/49): loss=2280564.7599946917\n",
      "Gradient Descent(42/49): loss=3506596.1329548704\n",
      "Gradient Descent(43/49): loss=5391741.972018722\n",
      "Gradient Descent(44/49): loss=8290342.214162839\n",
      "Gradient Descent(45/49): loss=12747229.946485236\n",
      "Gradient Descent(46/49): loss=19600140.523703635\n",
      "Gradient Descent(47/49): loss=30137175.827237125\n",
      "Gradient Descent(48/49): loss=46338921.30994629\n",
      "Gradient Descent(49/49): loss=71250725.16415383\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5298963759024129\n",
      "Gradient Descent(2/49): loss=0.5781162405954103\n",
      "Gradient Descent(3/49): loss=0.6558900603587708\n",
      "Gradient Descent(4/49): loss=0.78133145425505\n",
      "Gradient Descent(5/49): loss=0.9836558784703923\n",
      "Gradient Descent(6/49): loss=1.3099849422873917\n",
      "Gradient Descent(7/49): loss=1.8363210893176247\n",
      "Gradient Descent(8/49): loss=2.685248660862787\n",
      "Gradient Descent(9/49): loss=4.054483941008039\n",
      "Gradient Descent(10/49): loss=6.262923524354747\n",
      "Gradient Descent(11/49): loss=9.824915728334204\n",
      "Gradient Descent(12/49): loss=15.570052954131901\n",
      "Gradient Descent(13/49): loss=24.836384785619973\n",
      "Gradient Descent(14/49): loss=39.782051396623466\n",
      "Gradient Descent(15/49): loss=63.887917073514146\n",
      "Gradient Descent(16/49): loss=102.76826782378282\n",
      "Gradient Descent(17/49): loss=165.478385548895\n",
      "Gradient Descent(18/49): loss=266.6235344277163\n",
      "Gradient Descent(19/49): loss=429.76054505435667\n",
      "Gradient Descent(20/49): loss=692.8842294940587\n",
      "Gradient Descent(21/49): loss=1117.2764201269674\n",
      "Gradient Descent(22/49): loss=1801.7785843988122\n",
      "Gradient Descent(23/49): loss=2905.812125152975\n",
      "Gradient Descent(24/49): loss=4686.5078230353\n",
      "Gradient Descent(25/49): loss=7558.591914149987\n",
      "Gradient Descent(26/49): loss=12190.976344709656\n",
      "Gradient Descent(27/49): loss=19662.549192758277\n",
      "Gradient Descent(28/49): loss=31713.44903937277\n",
      "Gradient Descent(29/49): loss=51150.34540198456\n",
      "Gradient Descent(30/49): loss=82500.11554522991\n",
      "Gradient Descent(31/49): loss=133064.1598092735\n",
      "Gradient Descent(32/49): loss=214618.90680274283\n",
      "Gradient Descent(33/49): loss=346158.5582285505\n",
      "Gradient Descent(34/49): loss=558318.8620131835\n",
      "Gradient Descent(35/49): loss=900512.2159874154\n",
      "Gradient Descent(36/49): loss=1452435.8766124272\n",
      "Gradient Descent(37/49): loss=2342633.5488345395\n",
      "Gradient Descent(38/49): loss=3778433.3743617637\n",
      "Gradient Descent(39/49): loss=6094234.912953885\n",
      "Gradient Descent(40/49): loss=9829391.214549955\n",
      "Gradient Descent(41/49): loss=15853824.813392863\n",
      "Gradient Descent(42/49): loss=25570633.76496802\n",
      "Gradient Descent(43/49): loss=41242874.92296132\n",
      "Gradient Descent(44/49): loss=66520632.68669878\n",
      "Gradient Descent(45/49): loss=107291128.18381046\n",
      "Gradient Descent(46/49): loss=173049860.37111846\n",
      "Gradient Descent(47/49): loss=279112119.5160472\n",
      "Gradient Descent(48/49): loss=450179937.2908627\n",
      "Gradient Descent(49/49): loss=726095220.579817\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5306463374709252\n",
      "Gradient Descent(2/49): loss=0.5800758151777816\n",
      "Gradient Descent(3/49): loss=0.6598006197711714\n",
      "Gradient Descent(4/49): loss=0.788388757099814\n",
      "Gradient Descent(5/49): loss=0.9957885637972128\n",
      "Gradient Descent(6/49): loss=1.3303037120193548\n",
      "Gradient Descent(7/49): loss=1.869843194586958\n",
      "Gradient Descent(8/49): loss=2.7400664260202663\n",
      "Gradient Descent(9/49): loss=4.143649475998893\n",
      "Gradient Descent(10/49): loss=6.40748857731028\n",
      "Gradient Descent(11/49): loss=10.058834663814892\n",
      "Gradient Descent(12/49): loss=15.948090766738469\n",
      "Gradient Descent(13/49): loss=25.446871935145236\n",
      "Gradient Descent(14/49): loss=40.76745608166444\n",
      "Gradient Descent(15/49): loss=65.47802625158344\n",
      "Gradient Descent(16/49): loss=105.33370487866388\n",
      "Gradient Descent(17/49): loss=169.61692893627622\n",
      "Gradient Descent(18/49): loss=273.2993410188097\n",
      "Gradient Descent(19/49): loss=440.52870346668044\n",
      "Gradient Descent(20/49): loss=710.2529421589375\n",
      "Gradient Descent(21/49): loss=1145.2911667455433\n",
      "Gradient Descent(22/49): loss=1846.9643191813507\n",
      "Gradient Descent(23/49): loss=2978.6929467449863\n",
      "Gradient Descent(24/49): loss=4804.058050143027\n",
      "Gradient Descent(25/49): loss=7748.189425413421\n",
      "Gradient Descent(26/49): loss=12496.778920586172\n",
      "Gradient Descent(27/49): loss=20155.77891735217\n",
      "Gradient Descent(28/49): loss=32508.980012133805\n",
      "Gradient Descent(29/49): loss=52433.45805790514\n",
      "Gradient Descent(30/49): loss=84569.64869794143\n",
      "Gradient Descent(31/49): loss=136402.11058125083\n",
      "Gradient Descent(32/49): loss=220002.68835283187\n",
      "Gradient Descent(33/49): loss=354842.06024063955\n",
      "Gradient Descent(34/49): loss=572324.483158462\n",
      "Gradient Descent(35/49): loss=923101.8830825632\n",
      "Gradient Descent(36/49): loss=1488870.7514203691\n",
      "Gradient Descent(37/49): loss=2401399.359162259\n",
      "Gradient Descent(38/49): loss=3873216.750589474\n",
      "Gradient Descent(39/49): loss=6247111.021222729\n",
      "Gradient Descent(40/49): loss=10075965.090326214\n",
      "Gradient Descent(41/49): loss=16251523.81838512\n",
      "Gradient Descent(42/49): loss=26212082.490870625\n",
      "Gradient Descent(43/49): loss=42277467.57371806\n",
      "Gradient Descent(44/49): loss=68189327.17384794\n",
      "Gradient Descent(45/49): loss=109982565.5229093\n",
      "Gradient Descent(46/49): loss=177390879.65611303\n",
      "Gradient Descent(47/49): loss=286113749.5215179\n",
      "Gradient Descent(48/49): loss=461472866.3274106\n",
      "Gradient Descent(49/49): loss=744309585.8235968\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5302917734720002\n",
      "Gradient Descent(2/49): loss=0.5791493749049889\n",
      "Gradient Descent(3/49): loss=0.657951800256249\n",
      "Gradient Descent(4/49): loss=0.7850522321053208\n",
      "Gradient Descent(5/49): loss=0.9900525186347457\n",
      "Gradient Descent(6/49): loss=1.3206974807779133\n",
      "Gradient Descent(7/49): loss=1.8539947402185724\n",
      "Gradient Descent(8/49): loss=2.7141498899703986\n",
      "Gradient Descent(9/49): loss=4.1014941310051345\n",
      "Gradient Descent(10/49): loss=6.339141657369591\n",
      "Gradient Descent(11/49): loss=9.94824335264214\n",
      "Gradient Descent(12/49): loss=15.769363476949021\n",
      "Gradient Descent(13/49): loss=25.158248125445986\n",
      "Gradient Descent(14/49): loss=40.30158017500479\n",
      "Gradient Descent(15/49): loss=64.72626043774137\n",
      "Gradient Descent(16/49): loss=104.12082723350775\n",
      "Gradient Descent(17/49): loss=167.66032401839644\n",
      "Gradient Descent(18/49): loss=270.14317838275207\n",
      "Gradient Descent(19/49): loss=435.43777418705275\n",
      "Gradient Descent(20/49): loss=702.0414277597622\n",
      "Gradient Descent(21/49): loss=1132.0464606072076\n",
      "Gradient Descent(22/49): loss=1825.6015780866783\n",
      "Gradient Descent(23/49): loss=2944.236627069519\n",
      "Gradient Descent(24/49): loss=4748.4830975741515\n",
      "Gradient Descent(25/49): loss=7658.5522298503\n",
      "Gradient Descent(26/49): loss=12352.202733299617\n",
      "Gradient Descent(27/49): loss=19922.591630310682\n",
      "Gradient Descent(28/49): loss=32132.87188229926\n",
      "Gradient Descent(29/49): loss=51826.83290073056\n",
      "Gradient Descent(30/49): loss=83591.22262735214\n",
      "Gradient Descent(31/49): loss=134824.00681742994\n",
      "Gradient Descent(32/49): loss=217457.364437591\n",
      "Gradient Descent(33/49): loss=350736.7069432023\n",
      "Gradient Descent(34/49): loss=565702.9584705482\n",
      "Gradient Descent(35/49): loss=912422.0255588866\n",
      "Gradient Descent(36/49): loss=1471645.2088656423\n",
      "Gradient Descent(37/49): loss=2373616.2812210815\n",
      "Gradient Descent(38/49): loss=3828405.423823741\n",
      "Gradient Descent(39/49): loss=6174834.831927374\n",
      "Gradient Descent(40/49): loss=9959390.824256618\n",
      "Gradient Descent(41/49): loss=16063501.184285946\n",
      "Gradient Descent(42/49): loss=25908820.78397718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=41788336.76631877\n",
      "Gradient Descent(44/49): loss=67400408.09424166\n",
      "Gradient Descent(45/49): loss=108710117.93903142\n",
      "Gradient Descent(46/49): loss=175338548.94771641\n",
      "Gradient Descent(47/49): loss=282803545.32162267\n",
      "Gradient Descent(48/49): loss=456133837.97307736\n",
      "Gradient Descent(49/49): loss=735698266.9906627\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5305389261324803\n",
      "Gradient Descent(2/49): loss=0.5797951600915657\n",
      "Gradient Descent(3/49): loss=0.6592405398441673\n",
      "Gradient Descent(4/49): loss=0.787377992847104\n",
      "Gradient Descent(5/49): loss=0.9940508907956123\n",
      "Gradient Descent(6/49): loss=1.327393607896649\n",
      "Gradient Descent(7/49): loss=1.8650420763090672\n",
      "Gradient Descent(8/49): loss=2.7322152910114372\n",
      "Gradient Descent(9/49): loss=4.130878969004595\n",
      "Gradient Descent(10/49): loss=6.386783615240582\n",
      "Gradient Descent(11/49): loss=10.025332219154864\n",
      "Gradient Descent(12/49): loss=15.89394726240617\n",
      "Gradient Descent(13/49): loss=25.35943646567049\n",
      "Gradient Descent(14/49): loss=40.626324001611295\n",
      "Gradient Descent(15/49): loss=65.25028690833734\n",
      "Gradient Descent(16/49): loss=104.96627668059621\n",
      "Gradient Descent(17/49): loss=169.02419658426362\n",
      "Gradient Descent(18/49): loss=272.34321559691614\n",
      "Gradient Descent(19/49): loss=438.98646136244537\n",
      "Gradient Descent(20/49): loss=707.7653524575982\n",
      "Gradient Descent(21/49): loss=1141.2788259050178\n",
      "Gradient Descent(22/49): loss=1840.492707228453\n",
      "Gradient Descent(23/49): loss=2968.254776414826\n",
      "Gradient Descent(24/49): loss=4787.22221780586\n",
      "Gradient Descent(25/49): loss=7721.034804024328\n",
      "Gradient Descent(26/49): loss=12452.981124336327\n",
      "Gradient Descent(27/49): loss=20085.137344367082\n",
      "Gradient Descent(28/49): loss=32395.042111654526\n",
      "Gradient Descent(29/49): loss=52249.687510818105\n",
      "Gradient Descent(30/49): loss=84273.24507512711\n",
      "Gradient Descent(31/49): loss=135924.0410705908\n",
      "Gradient Descent(32/49): loss=219231.60993167068\n",
      "Gradient Descent(33/49): loss=353598.3877477211\n",
      "Gradient Descent(34/49): loss=570318.5636872358\n",
      "Gradient Descent(35/49): loss=919866.5354601298\n",
      "Gradient Descent(36/49): loss=1483652.4591325293\n",
      "Gradient Descent(37/49): loss=2392982.7754234513\n",
      "Gradient Descent(38/49): loss=3859641.642569427\n",
      "Gradient Descent(39/49): loss=6225215.729389565\n",
      "Gradient Descent(40/49): loss=10040650.174021\n",
      "Gradient Descent(41/49): loss=16194564.389766555\n",
      "Gradient Descent(42/49): loss=26120212.62834656\n",
      "Gradient Descent(43/49): loss=42129290.672351114\n",
      "Gradient Descent(44/49): loss=67950332.64952034\n",
      "Gradient Descent(45/49): loss=109597091.25449592\n",
      "Gradient Descent(46/49): loss=176769148.2084718\n",
      "Gradient Descent(47/49): loss=285110958.869516\n",
      "Gradient Descent(48/49): loss=459855465.2847523\n",
      "Gradient Descent(49/49): loss=741700879.681899\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5336572024354138\n",
      "Gradient Descent(2/49): loss=0.5905378745512517\n",
      "Gradient Descent(3/49): loss=0.6866662104270127\n",
      "Gradient Descent(4/49): loss=0.8491230980570543\n",
      "Gradient Descent(5/49): loss=1.1236752381518458\n",
      "Gradient Descent(6/49): loss=1.58766835491194\n",
      "Gradient Descent(7/49): loss=2.3718167222364466\n",
      "Gradient Descent(8/49): loss=3.697027463014872\n",
      "Gradient Descent(9/49): loss=5.936633614930127\n",
      "Gradient Descent(10/49): loss=9.721568011667394\n",
      "Gradient Descent(11/49): loss=16.11810714215412\n",
      "Gradient Descent(12/49): loss=26.928258272674736\n",
      "Gradient Descent(13/49): loss=45.19741368325358\n",
      "Gradient Descent(14/49): loss=76.07228632713803\n",
      "Gradient Descent(15/49): loss=128.25082109528248\n",
      "Gradient Descent(16/49): loss=216.43254485345267\n",
      "Gradient Descent(17/49): loss=365.4596580047904\n",
      "Gradient Descent(18/49): loss=617.3154792305412\n",
      "Gradient Descent(19/49): loss=1042.9518171020911\n",
      "Gradient Descent(20/49): loss=1762.2772281051232\n",
      "Gradient Descent(21/49): loss=2977.9371727000225\n",
      "Gradient Descent(22/49): loss=5032.4024790660105\n",
      "Gradient Descent(23/49): loss=8504.44884682373\n",
      "Gradient Descent(24/49): loss=14372.207208333779\n",
      "Gradient Descent(25/49): loss=24288.718839284913\n",
      "Gradient Descent(26/49): loss=41047.62349559942\n",
      "Gradient Descent(27/49): loss=69370.17236477113\n",
      "Gradient Descent(28/49): loss=117235.2799536724\n",
      "Gradient Descent(29/49): loss=198127.31177890577\n",
      "Gradient Descent(30/49): loss=334834.84556357074\n",
      "Gradient Descent(31/49): loss=565870.5776596138\n",
      "Gradient Descent(32/49): loss=956320.9649019486\n",
      "Gradient Descent(33/49): loss=1616182.1193413187\n",
      "Gradient Descent(34/49): loss=2731347.4703439167\n",
      "Gradient Descent(35/49): loss=4615976.913538829\n",
      "Gradient Descent(36/49): loss=7801000.672537657\n",
      "Gradient Descent(37/49): loss=13183690.825245455\n",
      "Gradient Descent(38/49): loss=22280437.183324892\n",
      "Gradient Descent(39/49): loss=37653938.52847853\n",
      "Gradient Descent(40/49): loss=63635155.80179327\n",
      "Gradient Descent(41/49): loss=107543412.99369629\n",
      "Gradient Descent(42/49): loss=181748367.64799312\n",
      "Gradient Descent(43/49): loss=307154741.01374567\n",
      "Gradient Descent(44/49): loss=519091512.00189507\n",
      "Gradient Descent(45/49): loss=877264654.971947\n",
      "Gradient Descent(46/49): loss=1482577266.5913002\n",
      "Gradient Descent(47/49): loss=2505555580.227997\n",
      "Gradient Descent(48/49): loss=4234388930.273483\n",
      "Gradient Descent(49/49): loss=7156117291.850079\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5345015057186135\n",
      "Gradient Descent(2/49): loss=0.5928090503830752\n",
      "Gradient Descent(3/49): loss=0.6913488008659847\n",
      "Gradient Descent(4/49): loss=0.8578809791820836\n",
      "Gradient Descent(5/49): loss=1.1393203605363804\n",
      "Gradient Descent(6/49): loss=1.6149529150249664\n",
      "Gradient Descent(7/49): loss=2.418771932110649\n",
      "Gradient Descent(8/49): loss=3.7772260709854724\n",
      "Gradient Descent(9/49): loss=6.073013565684771\n",
      "Gradient Descent(10/49): loss=9.952894431725685\n",
      "Gradient Descent(11/49): loss=16.50989309533649\n",
      "Gradient Descent(12/49): loss=27.591220836838595\n",
      "Gradient Descent(13/49): loss=46.31866471997673\n",
      "Gradient Descent(14/49): loss=77.96804488248284\n",
      "Gradient Descent(15/49): loss=131.45549735710486\n",
      "Gradient Descent(16/49): loss=221.8492920392493\n",
      "Gradient Descent(17/49): loss=374.61480505206777\n",
      "Gradient Descent(18/49): loss=632.788522043715\n",
      "Gradient Descent(19/49): loss=1069.1021037596138\n",
      "Gradient Descent(20/49): loss=1806.472056859632\n",
      "Gradient Descent(21/49): loss=3052.6272775988027\n",
      "Gradient Descent(22/49): loss=5158.629600648135\n",
      "Gradient Descent(23/49): loss=8717.77352660159\n",
      "Gradient Descent(24/49): loss=14732.72676146086\n",
      "Gradient Descent(25/49): loss=24897.99772837784\n",
      "Gradient Descent(26/49): loss=42077.30566246193\n",
      "Gradient Descent(27/49): loss=71110.33607106886\n",
      "Gradient Descent(28/49): loss=120176.15746159837\n",
      "Gradient Descent(29/49): loss=203097.3956115949\n",
      "Gradient Descent(30/49): loss=343234.2880850789\n",
      "Gradient Descent(31/49): loss=580065.6363652826\n",
      "Gradient Descent(32/49): loss=980310.6149589036\n",
      "Gradient Descent(33/49): loss=1656724.6287821957\n",
      "Gradient Descent(34/49): loss=2799864.312143656\n",
      "Gradient Descent(35/49): loss=4731770.37702426\n",
      "Gradient Descent(36/49): loss=7996691.626672023\n",
      "Gradient Descent(37/49): loss=13514408.538578417\n",
      "Gradient Descent(38/49): loss=22839350.119697634\n",
      "Gradient Descent(39/49): loss=38598501.391791195\n",
      "Gradient Descent(40/49): loss=65231467.04163389\n",
      "Gradient Descent(41/49): loss=110241178.989872\n",
      "Gradient Descent(42/49): loss=186307592.18238905\n",
      "Gradient Descent(43/49): loss=314859830.4777033\n",
      "Gradient Descent(44/49): loss=532113113.1968955\n",
      "Gradient Descent(45/49): loss=899271160.9921677\n",
      "Gradient Descent(46/49): loss=1519768261.7661476\n",
      "Gradient Descent(47/49): loss=2568408362.074271\n",
      "Gradient Descent(48/49): loss=4340610131.594742\n",
      "Gradient Descent(49/49): loss=7335631122.084824\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5341023392000005\n",
      "Gradient Descent(2/49): loss=0.5917352924479945\n",
      "Gradient Descent(3/49): loss=0.6891349834371245\n",
      "Gradient Descent(4/49): loss=0.8537404612087897\n",
      "Gradient Descent(5/49): loss=1.1319237186428703\n",
      "Gradient Descent(6/49): loss=1.6020534237063795\n",
      "Gradient Descent(7/49): loss=2.3965726252639703\n",
      "Gradient Descent(8/49): loss=3.7393100758964706\n",
      "Gradient Descent(9/49): loss=6.008536367465784\n",
      "Gradient Descent(10/49): loss=9.843528800217467\n",
      "Gradient Descent(11/49): loss=16.324666011566098\n",
      "Gradient Descent(12/49): loss=27.277787898748173\n",
      "Gradient Descent(13/49): loss=45.788563888086635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=77.0717753100757\n",
      "Gradient Descent(15/49): loss=129.94040261323002\n",
      "Gradient Descent(16/49): loss=219.2883827555684\n",
      "Gradient Descent(17/49): loss=370.28646919616415\n",
      "Gradient Descent(18/49): loss=625.4732352807441\n",
      "Gradient Descent(19/49): loss=1056.7388699637343\n",
      "Gradient Descent(20/49): loss=1785.577792577957\n",
      "Gradient Descent(21/49): loss=3017.3155717960003\n",
      "Gradient Descent(22/49): loss=5098.952418674285\n",
      "Gradient Descent(23/49): loss=8616.918689898035\n",
      "Gradient Descent(24/49): loss=14562.281688265935\n",
      "Gradient Descent(25/49): loss=24609.94515551037\n",
      "Gradient Descent(26/49): loss=41590.496415157235\n",
      "Gradient Descent(27/49): loss=70287.62804395992\n",
      "Gradient Descent(28/49): loss=118785.7804966374\n",
      "Gradient Descent(29/49): loss=200747.65814167372\n",
      "Gradient Descent(30/49): loss=339263.23136180826\n",
      "Gradient Descent(31/49): loss=573354.5501037451\n",
      "Gradient Descent(32/49): loss=968968.8787776002\n",
      "Gradient Descent(33/49): loss=1637557.094236658\n",
      "Gradient Descent(34/49): loss=2767471.1783623155\n",
      "Gradient Descent(35/49): loss=4677025.98053431\n",
      "Gradient Descent(36/49): loss=7904173.596206057\n",
      "Gradient Descent(37/49): loss=13358053.066689933\n",
      "Gradient Descent(38/49): loss=22575109.37180914\n",
      "Gradient Descent(39/49): loss=38151934.527457796\n",
      "Gradient Descent(40/49): loss=64476769.04050849\n",
      "Gradient Descent(41/49): loss=108965739.3675744\n",
      "Gradient Descent(42/49): loss=184152099.22031683\n",
      "Gradient Descent(43/49): loss=311217047.37147427\n",
      "Gradient Descent(44/49): loss=525956809.7468345\n",
      "Gradient Descent(45/49): loss=888867008.1612906\n",
      "Gradient Descent(46/49): loss=1502185243.4818523\n",
      "Gradient Descent(47/49): loss=2538693061.173586\n",
      "Gradient Descent(48/49): loss=4290391273.0727873\n",
      "Gradient Descent(49/49): loss=7250761251.182529\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5343805825280002\n",
      "Gradient Descent(2/49): loss=0.5924837670003171\n",
      "Gradient Descent(3/49): loss=0.6906781487585535\n",
      "Gradient Descent(4/49): loss=0.8566266539299189\n",
      "Gradient Descent(5/49): loss=1.137079627669647\n",
      "Gradient Descent(6/49): loss=1.6110451532897458\n",
      "Gradient Descent(7/49): loss=2.4120468915876967\n",
      "Gradient Descent(8/49): loss=3.765739829311314\n",
      "Gradient Descent(9/49): loss=6.05348089406471\n",
      "Gradient Descent(10/49): loss=9.919763293497185\n",
      "Gradient Descent(11/49): loss=16.453780548539473\n",
      "Gradient Descent(12/49): loss=27.496269709560835\n",
      "Gradient Descent(13/49): loss=46.15807639168276\n",
      "Gradient Descent(14/49): loss=77.69652968446529\n",
      "Gradient Descent(15/49): loss=130.99651574927557\n",
      "Gradient Descent(16/49): loss=221.07349219880103\n",
      "Gradient Descent(17/49): loss=373.30358239852023\n",
      "Gradient Descent(18/49): loss=630.572434835956\n",
      "Gradient Descent(19/49): loss=1065.3567954553243\n",
      "Gradient Descent(20/49): loss=1800.142364901826\n",
      "Gradient Descent(21/49): loss=3041.9299772666473\n",
      "Gradient Descent(22/49): loss=5140.551042162909\n",
      "Gradient Descent(23/49): loss=8687.220641838523\n",
      "Gradient Descent(24/49): loss=14681.092265288638\n",
      "Gradient Descent(25/49): loss=24810.735308919568\n",
      "Gradient Descent(26/49): loss=41929.83205265392\n",
      "Gradient Descent(27/49): loss=70861.10554956352\n",
      "Gradient Descent(28/49): loss=119754.95775933837\n",
      "Gradient Descent(29/49): loss=202385.567993856\n",
      "Gradient Descent(30/49): loss=342031.29929015617\n",
      "Gradient Descent(31/49): loss=578032.5851809394\n",
      "Gradient Descent(32/49): loss=976874.7583364584\n",
      "Gradient Descent(33/49): loss=1650918.0309690372\n",
      "Gradient Descent(34/49): loss=2790051.1617182908\n",
      "Gradient Descent(35/49): loss=4715186.152684482\n",
      "Gradient Descent(36/49): loss=7968664.287417438\n",
      "Gradient Descent(37/49): loss=13467042.335116304\n",
      "Gradient Descent(38/49): loss=22759301.235729493\n",
      "Gradient Descent(39/49): loss=38463218.77776562\n",
      "Gradient Descent(40/49): loss=65002839.42380601\n",
      "Gradient Descent(41/49): loss=109854798.31561862\n",
      "Gradient Descent(42/49): loss=185654608.8427899\n",
      "Gradient Descent(43/49): loss=313756288.6336761\n",
      "Gradient Descent(44/49): loss=530248127.48026764\n",
      "Gradient Descent(45/49): loss=896119335.1310095\n",
      "Gradient Descent(46/49): loss=1514441676.0608416\n",
      "Gradient Descent(47/49): loss=2559406432.232045\n",
      "Gradient Descent(48/49): loss=4325396870.161924\n",
      "Gradient Descent(49/49): loss=7309920710.262664\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5375058303660716\n",
      "Gradient Descent(2/49): loss=0.6038498937006257\n",
      "Gradient Descent(3/49): loss=0.7212059073330854\n",
      "Gradient Descent(4/49): loss=0.9287969598475527\n",
      "Gradient Descent(5/49): loss=1.296004772640501\n",
      "Gradient Descent(6/49): loss=1.945558672690029\n",
      "Gradient Descent(7/49): loss=3.0945545664872975\n",
      "Gradient Descent(8/49): loss=5.127013403024981\n",
      "Gradient Descent(9/49): loss=8.722229838976176\n",
      "Gradient Descent(10/49): loss=15.081808192530993\n",
      "Gradient Descent(11/49): loss=26.331266342131613\n",
      "Gradient Descent(12/49): loss=46.230432862965074\n",
      "Gradient Descent(13/49): loss=81.43006852167152\n",
      "Gradient Descent(14/49): loss=143.69470403834603\n",
      "Gradient Descent(15/49): loss=253.83461780376658\n",
      "Gradient Descent(16/49): loss=448.66111126346607\n",
      "Gradient Descent(17/49): loss=793.2896955442894\n",
      "Gradient Descent(18/49): loss=1402.9031982788022\n",
      "Gradient Descent(19/49): loss=2481.2485232656045\n",
      "Gradient Descent(20/49): loss=4388.733568634812\n",
      "Gradient Descent(21/49): loss=7762.883865387971\n",
      "Gradient Descent(22/49): loss=13731.418325313865\n",
      "Gradient Descent(23/49): loss=24289.15893147615\n",
      "Gradient Descent(24/49): loss=42964.74628972079\n",
      "Gradient Descent(25/49): loss=75999.99276771586\n",
      "Gradient Descent(26/49): loss=134436.04026264633\n",
      "Gradient Descent(27/49): loss=237803.5646764051\n",
      "Gradient Descent(28/49): loss=420650.37861197593\n",
      "Gradient Descent(29/49): loss=744088.1077825838\n",
      "Gradient Descent(30/49): loss=1316217.10691261\n",
      "Gradient Descent(31/49): loss=2328256.0934735285\n",
      "Gradient Descent(32/49): loss=4118451.856800664\n",
      "Gradient Descent(33/49): loss=7285129.142551154\n",
      "Gradient Descent(34/49): loss=12886664.593314353\n",
      "Gradient Descent(35/49): loss=22795220.652170736\n",
      "Gradient Descent(36/49): loss=40322465.46467813\n",
      "Gradient Descent(37/49): loss=71326408.81352827\n",
      "Gradient Descent(38/49): loss=126169284.20329766\n",
      "Gradient Descent(39/49): loss=223180846.4802527\n",
      "Gradient Descent(40/49): loss=394784598.9919397\n",
      "Gradient Descent(41/49): loss=698334476.8098571\n",
      "Gradient Descent(42/49): loss=1235283855.6819432\n",
      "Gradient Descent(43/49): loss=2185093611.968645\n",
      "Gradient Descent(44/49): loss=3865212089.864391\n",
      "Gradient Descent(45/49): loss=6837173665.415132\n",
      "Gradient Descent(46/49): loss=12094276496.40567\n",
      "Gradient Descent(47/49): loss=21393565694.143925\n",
      "Gradient Descent(48/49): loss=37843078356.02609\n",
      "Gradient Descent(49/49): loss=66940621303.619606\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5384466778942637\n",
      "Gradient Descent(2/49): loss=0.6064550064214229\n",
      "Gradient Descent(3/49): loss=0.726754938753152\n",
      "Gradient Descent(4/49): loss=0.9395534890546929\n",
      "Gradient Descent(5/49): loss=1.315972844683032\n",
      "Gradient Descent(6/49): loss=1.9818210428542007\n",
      "Gradient Descent(7/49): loss=3.159639920599172\n",
      "Gradient Descent(8/49): loss=5.243083733442624\n",
      "Gradient Descent(9/49): loss=8.928487493980755\n",
      "Gradient Descent(10/49): loss=15.447598205996112\n",
      "Gradient Descent(11/49): loss=26.979253144481678\n",
      "Gradient Descent(12/49): loss=47.377597565169275\n",
      "Gradient Descent(13/49): loss=83.46022901091885\n",
      "Gradient Descent(14/49): loss=147.28679577530806\n",
      "Gradient Descent(15/49): loss=260.189609724831\n",
      "Gradient Descent(16/49): loss=459.90339732010534\n",
      "Gradient Descent(17/49): loss=813.1771161974624\n",
      "Gradient Descent(18/49): loss=1438.082997519496\n",
      "Gradient Descent(19/49): loss=2543.4790109902406\n",
      "Gradient Descent(20/49): loss=4498.814019218772\n",
      "Gradient Descent(21/49): loss=7957.606115273501\n",
      "Gradient Descent(22/49): loss=14075.863453986534\n",
      "Gradient Descent(23/49): loss=24898.448860436594\n",
      "Gradient Descent(24/49): loss=44042.52018590223\n",
      "Gradient Descent(25/49): loss=77906.46795352252\n",
      "Gradient Descent(26/49): loss=137808.40515966422\n",
      "Gradient Descent(27/49): loss=243768.94188361274\n",
      "Gradient Descent(28/49): loss=431202.53529457684\n",
      "Gradient Descent(29/49): loss=762753.8186792218\n",
      "Gradient Descent(30/49): loss=1349234.8838582186\n",
      "Gradient Descent(31/49): loss=2386661.240053458\n",
      "Gradient Descent(32/49): loss=4221764.721527697\n",
      "Gradient Descent(33/49): loss=7467879.269906783\n",
      "Gradient Descent(34/49): loss=13209931.294535667\n",
      "Gradient Descent(35/49): loss=23367047.120899215\n",
      "Gradient Descent(36/49): loss=41333969.30615506\n",
      "Gradient Descent(37/49): loss=73115657.95965426\n",
      "Gradient Descent(38/49): loss=129334287.01882704\n",
      "Gradient Descent(39/49): loss=228779419.96157792\n",
      "Gradient Descent(40/49): loss=404687915.6240053\n",
      "Gradient Descent(41/49): loss=715852453.6012853\n",
      "Gradient Descent(42/49): loss=1266271404.8294535\n",
      "Gradient Descent(43/49): loss=2239907487.6569486\n",
      "Gradient Descent(44/49): loss=3962172354.5700107\n",
      "Gradient Descent(45/49): loss=7008686677.652076\n",
      "Gradient Descent(46/49): loss=12397665863.753405\n",
      "Gradient Descent(47/49): loss=21930231146.04552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=38792385873.89322\n",
      "Gradient Descent(49/49): loss=68619851371.98748\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.538001867552\n",
      "Gradient Descent(2/49): loss=0.6052233710647423\n",
      "Gradient Descent(3/49): loss=0.7241314886284447\n",
      "Gradient Descent(4/49): loss=0.9344680577868973\n",
      "Gradient Descent(5/49): loss=1.3065324149713187\n",
      "Gradient Descent(6/49): loss=1.9646770563948193\n",
      "Gradient Descent(7/49): loss=3.1288691126087618\n",
      "Gradient Descent(8/49): loss=5.188208440845446\n",
      "Gradient Descent(9/49): loss=8.830973778563662\n",
      "Gradient Descent(10/49): loss=15.274661384454815\n",
      "Gradient Descent(11/49): loss=26.67290039051434\n",
      "Gradient Descent(12/49): loss=46.83524536832678\n",
      "Gradient Descent(13/49): loss=82.50041739959076\n",
      "Gradient Descent(14/49): loss=145.58854020569083\n",
      "Gradient Descent(15/49): loss=257.18512063737137\n",
      "Gradient Descent(16/49): loss=454.5883117629803\n",
      "Gradient Descent(17/49): loss=803.774816545115\n",
      "Gradient Descent(18/49): loss=1421.4508248541508\n",
      "Gradient Descent(19/49): loss=2514.0579159518347\n",
      "Gradient Descent(20/49): loss=4446.770599395187\n",
      "Gradient Descent(21/49): loss=7865.546065137612\n",
      "Gradient Descent(22/49): loss=13913.017986490337\n",
      "Gradient Descent(23/49): loss=24610.39106816955\n",
      "Gradient Descent(24/49): loss=43532.97431235602\n",
      "Gradient Descent(25/49): loss=77005.1318129982\n",
      "Gradient Descent(26/49): loss=136214.03121589846\n",
      "Gradient Descent(27/49): loss=240948.6533696939\n",
      "Gradient Descent(28/49): loss=426213.7264974764\n",
      "Gradient Descent(29/49): loss=753929.1143532023\n",
      "Gradient Descent(30/49): loss=1333624.8639313097\n",
      "Gradient Descent(31/49): loss=2359048.675360087\n",
      "Gradient Descent(32/49): loss=4172920.855396735\n",
      "Gradient Descent(33/49): loss=7381479.354663439\n",
      "Gradient Descent(34/49): loss=13057098.484017216\n",
      "Gradient Descent(35/49): loss=23096701.161931988\n",
      "Gradient Descent(36/49): loss=40855754.33889694\n",
      "Gradient Descent(37/49): loss=72269743.50363216\n",
      "Gradient Descent(38/49): loss=127837948.9371326\n",
      "Gradient Descent(39/49): loss=226132547.52845737\n",
      "Gradient Descent(40/49): loss=400005862.97666156\n",
      "Gradient Descent(41/49): loss=707570370.6729594\n",
      "Gradient Descent(42/49): loss=1251621228.336999\n",
      "Gradient Descent(43/49): loss=2213992790.45903\n",
      "Gradient Descent(44/49): loss=3916331846.6962233\n",
      "Gradient Descent(45/49): loss=6927599403.275193\n",
      "Gradient Descent(46/49): loss=12254230584.107431\n",
      "Gradient Descent(47/49): loss=21676508479.879696\n",
      "Gradient Descent(48/49): loss=38343575849.713646\n",
      "Gradient Descent(49/49): loss=67825951320.21554\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5383119273996801\n",
      "Gradient Descent(2/49): loss=0.6060818957769813\n",
      "Gradient Descent(3/49): loss=0.7259601928395701\n",
      "Gradient Descent(4/49): loss=0.9380129125135873\n",
      "Gradient Descent(5/49): loss=1.3131129683449296\n",
      "Gradient Descent(6/49): loss=1.9766274571048574\n",
      "Gradient Descent(7/49): loss=3.1503182362724154\n",
      "Gradient Descent(8/49): loss=5.226459855541575\n",
      "Gradient Descent(9/49): loss=8.898946765867706\n",
      "Gradient Descent(10/49): loss=15.39520886154161\n",
      "Gradient Descent(11/49): loss=26.886446882582128\n",
      "Gradient Descent(12/49): loss=47.213297817999006\n",
      "Gradient Descent(13/49): loss=83.16946443765708\n",
      "Gradient Descent(14/49): loss=146.77232757117767\n",
      "Gradient Descent(15/49): loss=259.27943216808075\n",
      "Gradient Descent(16/49): loss=458.29324948955997\n",
      "Gradient Descent(17/49): loss=810.3287909493966\n",
      "Gradient Descent(18/49): loss=1433.0444602379125\n",
      "Gradient Descent(19/49): loss=2534.566207642231\n",
      "Gradient Descent(20/49): loss=4483.048026626043\n",
      "Gradient Descent(21/49): loss=7929.71751622637\n",
      "Gradient Descent(22/49): loss=14026.531176381113\n",
      "Gradient Descent(23/49): loss=24811.18485982497\n",
      "Gradient Descent(24/49): loss=43888.15876047315\n",
      "Gradient Descent(25/49): loss=77633.41789332648\n",
      "Gradient Descent(26/49): loss=137325.40677342517\n",
      "Gradient Descent(27/49): loss=242914.5659034297\n",
      "Gradient Descent(28/49): loss=429691.2294885258\n",
      "Gradient Descent(29/49): loss=760080.4697042069\n",
      "Gradient Descent(30/49): loss=1344505.9967218312\n",
      "Gradient Descent(31/49): loss=2378296.3114631814\n",
      "Gradient Descent(32/49): loss=4206967.999209382\n",
      "Gradient Descent(33/49): loss=7441705.347663642\n",
      "Gradient Descent(34/49): loss=13163632.243344823\n",
      "Gradient Descent(35/49): loss=23285148.729112554\n",
      "Gradient Descent(36/49): loss=41189099.2407882\n",
      "Gradient Descent(37/49): loss=72859397.30089606\n",
      "Gradient Descent(38/49): loss=128880987.53941108\n",
      "Gradient Descent(39/49): loss=227977578.51233158\n",
      "Gradient Descent(40/49): loss=403269538.2843001\n",
      "Gradient Descent(41/49): loss=713343485.9249815\n",
      "Gradient Descent(42/49): loss=1261833291.9066446\n",
      "Gradient Descent(43/49): loss=2232056909.7076645\n",
      "Gradient Descent(44/49): loss=3948285467.2353973\n",
      "Gradient Descent(45/49): loss=6984122162.6470175\n",
      "Gradient Descent(46/49): loss=12354213693.160402\n",
      "Gradient Descent(47/49): loss=21853368601.483833\n",
      "Gradient Descent(48/49): loss=38656423718.81684\n",
      "Gradient Descent(49/49): loss=68379347915.86908\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5414422596943873\n",
      "Gradient Descent(2/49): loss=0.618093863225134\n",
      "Gradient Descent(3/49): loss=0.7598686691155615\n",
      "Gradient Descent(4/49): loss=1.022095350090475\n",
      "Gradient Descent(5/49): loss=1.5071098192217294\n",
      "Gradient Descent(6/49): loss=2.404192581326933\n",
      "Gradient Descent(7/49): loss=4.063436858116993\n",
      "Gradient Descent(8/49): loss=7.132375072467085\n",
      "Gradient Descent(9/49): loss=12.808683193729852\n",
      "Gradient Descent(10/49): loss=23.30758269481506\n",
      "Gradient Descent(11/49): loss=42.72634721202372\n",
      "Gradient Descent(12/49): loss=78.64329406305684\n",
      "Gradient Descent(13/49): loss=145.07527895872443\n",
      "Gradient Descent(14/49): loss=267.9478782217834\n",
      "Gradient Descent(15/49): loss=495.21303781867573\n",
      "Gradient Descent(16/49): loss=915.5626770091654\n",
      "Gradient Descent(17/49): loss=1693.0413696558653\n",
      "Gradient Descent(18/49): loss=3131.0659595750917\n",
      "Gradient Descent(19/49): loss=5790.836241089952\n",
      "Gradient Descent(20/49): loss=10710.347353778985\n",
      "Gradient Descent(21/49): loss=19809.475107807262\n",
      "Gradient Descent(22/49): loss=36639.221801661144\n",
      "Gradient Descent(23/49): loss=67767.52128661658\n",
      "Gradient Descent(24/49): loss=125342.42401397406\n",
      "Gradient Descent(25/49): loss=231832.96409848367\n",
      "Gradient Descent(26/49): loss=428797.86703883496\n",
      "Gradient Descent(27/49): loss=793104.151517245\n",
      "Gradient Descent(28/49): loss=1466925.055288459\n",
      "Gradient Descent(29/49): loss=2713224.1989037623\n",
      "Gradient Descent(30/49): loss=5018379.094934994\n",
      "Gradient Descent(31/49): loss=9281993.590634111\n",
      "Gradient Descent(32/49): loss=17167974.961878896\n",
      "Gradient Descent(33/49): loss=31753886.106134426\n",
      "Gradient Descent(34/49): loss=58731987.35854939\n",
      "Gradient Descent(35/49): loss=108630683.43502733\n",
      "Gradient Descent(36/49): loss=200923311.69807193\n",
      "Gradient Descent(37/49): loss=371627756.9333712\n",
      "Gradient Descent(38/49): loss=687362698.8405745\n",
      "Gradient Descent(39/49): loss=1271346047.392254\n",
      "Gradient Descent(40/49): loss=2351481648.87352\n",
      "Gradient Descent(41/49): loss=4349300457.3735895\n",
      "Gradient Descent(42/49): loss=8044466125.574661\n",
      "Gradient Descent(43/49): loss=14879044545.47875\n",
      "Gradient Descent(44/49): loss=27520280790.930443\n",
      "Gradient Descent(45/49): loss=50901511350.517624\n",
      "Gradient Descent(46/49): loss=94147435393.543\n",
      "Gradient Descent(47/49): loss=174135096503.512\n",
      "Gradient Descent(48/49): loss=322080274492.5395\n",
      "Gradient Descent(49/49): loss=595719675701.0448\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5424818539978755\n",
      "Gradient Descent(2/49): loss=0.6210562911523645\n",
      "Gradient Descent(3/49): loss=0.7663875701132506\n",
      "Gradient Descent(4/49): loss=1.03519230367932\n",
      "Gradient Descent(5/49): loss=1.532373538883198\n",
      "Gradient Descent(6/49): loss=2.4519599515161863\n",
      "Gradient Descent(7/49): loss=4.152826980321928\n",
      "Gradient Descent(8/49): loss=7.298750636801989\n",
      "Gradient Descent(9/49): loss=13.117451031826832\n",
      "Gradient Descent(10/49): loss=23.879719282466773\n",
      "Gradient Descent(11/49): loss=43.78561063885147\n",
      "Gradient Descent(12/49): loss=80.60354729162427\n",
      "Gradient Descent(13/49): loss=148.70200292458549\n",
      "Gradient Descent(14/49): loss=274.6569064632856\n",
      "Gradient Descent(15/49): loss=507.6230960484624\n",
      "Gradient Descent(16/49): loss=938.517360305296\n",
      "Gradient Descent(17/49): loss=1735.4993914745023\n",
      "Gradient Descent(18/49): loss=3209.597356325549\n",
      "Gradient Descent(19/49): loss=5936.088952113345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=10979.007807683907\n",
      "Gradient Descent(21/49): loss=20306.390522947197\n",
      "Gradient Descent(22/49): loss=37558.31759309725\n",
      "Gradient Descent(23/49): loss=69467.48190204107\n",
      "Gradient Descent(24/49): loss=128486.67220787465\n",
      "Gradient Descent(25/49): loss=237648.56659753292\n",
      "Gradient Descent(26/49): loss=439554.40646061464\n",
      "Gradient Descent(27/49): loss=812999.4478714188\n",
      "Gradient Descent(28/49): loss=1503723.3964647425\n",
      "Gradient Descent(29/49): loss=2781286.4117831425\n",
      "Gradient Descent(30/49): loss=5144266.964916074\n",
      "Gradient Descent(31/49): loss=9514835.795990707\n",
      "Gradient Descent(32/49): loss=17598639.90594681\n",
      "Gradient Descent(33/49): loss=32550443.98772059\n",
      "Gradient Descent(34/49): loss=60205300.81736524\n",
      "Gradient Descent(35/49): loss=111355724.00948732\n",
      "Gradient Descent(36/49): loss=205963546.74562693\n",
      "Gradient Descent(37/49): loss=380950175.6784108\n",
      "Gradient Descent(38/49): loss=704605444.5523932\n",
      "Gradient Descent(39/49): loss=1303238229.86171\n",
      "Gradient Descent(40/49): loss=2410469429.569903\n",
      "Gradient Descent(41/49): loss=4458404256.549983\n",
      "Gradient Descent(42/49): loss=8246264512.533072\n",
      "Gradient Descent(43/49): loss=15252290841.998379\n",
      "Gradient Descent(44/49): loss=28210637140.9761\n",
      "Gradient Descent(45/49): loss=52178394455.57173\n",
      "Gradient Descent(46/49): loss=96509158384.65009\n",
      "Gradient Descent(47/49): loss=178503339347.87103\n",
      "Gradient Descent(48/49): loss=330159776457.4714\n",
      "Gradient Descent(49/49): loss=610663522535.3313\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5419903585280001\n",
      "Gradient Descent(2/49): loss=0.6196557256613919\n",
      "Gradient Descent(3/49): loss=0.7633055887112794\n",
      "Gradient Descent(4/49): loss=1.029000375408393\n",
      "Gradient Descent(5/49): loss=1.5204294528832731\n",
      "Gradient Descent(6/49): loss=2.429376674580782\n",
      "Gradient Descent(7/49): loss=4.110565455833058\n",
      "Gradient Descent(8/49): loss=7.220092225636485\n",
      "Gradient Descent(9/49): loss=12.971472939066118\n",
      "Gradient Descent(10/49): loss=23.60922670662413\n",
      "Gradient Descent(11/49): loss=43.28481607509992\n",
      "Gradient Descent(12/49): loss=79.67678617102851\n",
      "Gradient Descent(13/49): loss=146.98737406045913\n",
      "Gradient Descent(14/49): loss=271.48503742073433\n",
      "Gradient Descent(15/49): loss=501.75591557188096\n",
      "Gradient Descent(16/49): loss=927.6649318002338\n",
      "Gradient Descent(17/49): loss=1715.4262482161928\n",
      "Gradient Descent(18/49): loss=3172.469579058816\n",
      "Gradient Descent(19/49): loss=5867.416923785541\n",
      "Gradient Descent(20/49): loss=10851.99153259189\n",
      "Gradient Descent(21/49): loss=20071.46072903983\n",
      "Gradient Descent(22/49): loss=37123.79095479114\n",
      "Gradient Descent(23/49): loss=68663.78094033968\n",
      "Gradient Descent(24/49): loss=127000.14641760789\n",
      "Gradient Descent(25/49): loss=234899.08800434062\n",
      "Gradient Descent(26/49): loss=434468.9703632157\n",
      "Gradient Descent(27/49): loss=803593.4247741764\n",
      "Gradient Descent(28/49): loss=1486326.0156526032\n",
      "Gradient Descent(29/49): loss=2749108.2157416157\n",
      "Gradient Descent(30/49): loss=5084750.173026183\n",
      "Gradient Descent(31/49): loss=9404753.537219577\n",
      "Gradient Descent(32/49): loss=17395031.75963306\n",
      "Gradient Descent(33/49): loss=32173850.359803863\n",
      "Gradient Descent(34/49): loss=59508753.24268105\n",
      "Gradient Descent(35/49): loss=110067389.6148592\n",
      "Gradient Descent(36/49): loss=203580643.44883838\n",
      "Gradient Descent(37/49): loss=376542757.740186\n",
      "Gradient Descent(38/49): loss=696453484.3334491\n",
      "Gradient Descent(39/49): loss=1288160364.2403903\n",
      "Gradient Descent(40/49): loss=2382581409.315963\n",
      "Gradient Descent(41/49): loss=4406822574.287762\n",
      "Gradient Descent(42/49): loss=8150859033.020884\n",
      "Gradient Descent(43/49): loss=15075828867.093811\n",
      "Gradient Descent(44/49): loss=27884253072.192154\n",
      "Gradient Descent(45/49): loss=51574714481.94561\n",
      "Gradient Descent(46/49): loss=95392591905.41391\n",
      "Gradient Descent(47/49): loss=176438137987.86496\n",
      "Gradient Descent(48/49): loss=326339980022.001\n",
      "Gradient Descent(49/49): loss=603598427048.3033\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5423329607475202\n",
      "Gradient Descent(2/49): loss=0.6206320049461235\n",
      "Gradient Descent(3/49): loss=0.7654539170959019\n",
      "Gradient Descent(4/49): loss=1.033316525808058\n",
      "Gradient Descent(5/49): loss=1.5287552068820613\n",
      "Gradient Descent(6/49): loss=2.4451185913966325\n",
      "Gradient Descent(7/49): loss=4.140024307394378\n",
      "Gradient Descent(8/49): loss=7.274921919703938\n",
      "Gradient Descent(9/49): loss=13.07322854343245\n",
      "Gradient Descent(10/49): loss=23.797776474678237\n",
      "Gradient Descent(11/49): loss=43.633900328313885\n",
      "Gradient Descent(12/49): loss=80.32279500800297\n",
      "Gradient Descent(13/49): loss=148.18257460754802\n",
      "Gradient Descent(14/49): loss=273.69602295489227\n",
      "Gradient Descent(15/49): loss=505.8456970180884\n",
      "Gradient Descent(16/49): loss=935.2297341654922\n",
      "Gradient Descent(17/49): loss=1729.4184492734057\n",
      "Gradient Descent(18/49): loss=3198.349896736482\n",
      "Gradient Descent(19/49): loss=5915.28550196495\n",
      "Gradient Descent(20/49): loss=10940.529597395076\n",
      "Gradient Descent(21/49): loss=20235.221076303045\n",
      "Gradient Descent(22/49): loss=37426.68243568925\n",
      "Gradient Descent(23/49): loss=69224.00936601585\n",
      "Gradient Descent(24/49): loss=128036.34525633765\n",
      "Gradient Descent(25/49): loss=236815.64171906837\n",
      "Gradient Descent(26/49): loss=438013.8284565438\n",
      "Gradient Descent(27/49): loss=810149.9946461634\n",
      "Gradient Descent(28/49): loss=1498453.0476304898\n",
      "Gradient Descent(29/49): loss=2771538.37443043\n",
      "Gradient Descent(30/49): loss=5126236.994879941\n",
      "Gradient Descent(31/49): loss=9481487.56326325\n",
      "Gradient Descent(32/49): loss=17536959.014544874\n",
      "Gradient Descent(33/49): loss=32436359.010835636\n",
      "Gradient Descent(34/49): loss=59994289.243972436\n",
      "Gradient Descent(35/49): loss=110965437.00317715\n",
      "Gradient Descent(36/49): loss=205241671.89862263\n",
      "Gradient Descent(37/49): loss=379614995.96119714\n",
      "Gradient Descent(38/49): loss=702135896.147313\n",
      "Gradient Descent(39/49): loss=1298670553.131566\n",
      "Gradient Descent(40/49): loss=2402021054.68987\n",
      "Gradient Descent(41/49): loss=4442778142.372062\n",
      "Gradient Descent(42/49): loss=8217362451.7484665\n",
      "Gradient Descent(43/49): loss=15198833590.369661\n",
      "Gradient Descent(44/49): loss=28111762608.366543\n",
      "Gradient Descent(45/49): loss=51995516120.05073\n",
      "Gradient Descent(46/49): loss=96170906615.2546\n",
      "Gradient Descent(47/49): loss=177877708875.18036\n",
      "Gradient Descent(48/49): loss=329002610335.17377\n",
      "Gradient Descent(49/49): loss=608523228075.6085\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5454664904203608\n",
      "Gradient Descent(2/49): loss=0.6333122965615369\n",
      "Gradient Descent(3/49): loss=0.803039178606872\n",
      "Gradient Descent(4/49): loss=1.1309684874067474\n",
      "Gradient Descent(5/49): loss=1.7645607049387637\n",
      "Gradient Descent(6/49): loss=2.988724228432503\n",
      "Gradient Descent(7/49): loss=5.353930572174779\n",
      "Gradient Descent(8/49): loss=9.92374574891935\n",
      "Gradient Descent(9/49): loss=18.753085651906098\n",
      "Gradient Descent(10/49): loss=35.812253278466116\n",
      "Gradient Descent(11/49): loss=68.77227104973964\n",
      "Gradient Descent(12/49): loss=132.45432138561551\n",
      "Gradient Descent(13/49): loss=255.4944108395977\n",
      "Gradient Descent(14/49): loss=493.2201676736091\n",
      "Gradient Descent(15/49): loss=952.5301024526084\n",
      "Gradient Descent(16/49): loss=1839.962827439258\n",
      "Gradient Descent(17/49): loss=3554.571595385957\n",
      "Gradient Descent(18/49): loss=6867.367195936416\n",
      "Gradient Descent(19/49): loss=13268.019575758544\n",
      "Gradient Descent(20/49): loss=25634.72003881283\n",
      "Gradient Descent(21/49): loss=49528.42200348501\n",
      "Gradient Descent(22/49): loss=95693.44356943242\n",
      "Gradient Descent(23/49): loss=184888.88173699364\n",
      "Gradient Descent(24/49): loss=357223.387820579\n",
      "Gradient Descent(25/49): loss=690190.8870246155\n",
      "Gradient Descent(26/49): loss=1333517.3922368803\n",
      "Gradient Descent(27/49): loss=2576488.5329575073\n",
      "Gradient Descent(28/49): loss=4978033.073944028\n",
      "Gradient Descent(29/49): loss=9618057.281584218\n",
      "Gradient Descent(30/49): loss=18583048.053163845\n",
      "Gradient Descent(31/49): loss=35904306.72293286\n",
      "Gradient Descent(32/49): loss=69370710.59879908\n",
      "Gradient Descent(33/49): loss=134031149.52737488\n",
      "Gradient Descent(34/49): loss=258961583.58125755\n",
      "Gradient Descent(35/49): loss=500339675.2167855\n",
      "Gradient Descent(36/49): loss=966706286.0657442\n",
      "Gradient Descent(37/49): loss=1867773214.8872383\n",
      "Gradient Descent(38/49): loss=3608724628.0632825\n",
      "Gradient Descent(39/49): loss=6972416853.461243\n",
      "Gradient Descent(40/49): loss=13471406602.151285\n",
      "Gradient Descent(41/49): loss=26028104695.595142\n",
      "Gradient Descent(42/49): loss=50288901081.94247\n",
      "Gradient Descent(43/49): loss=97163185780.00194\n",
      "Gradient Descent(44/49): loss=187728991245.11075\n",
      "Gradient Descent(45/49): loss=362711183984.23114\n",
      "Gradient Descent(46/49): loss=700794278575.484\n",
      "Gradient Descent(47/49): loss=1354004625635.2224\n",
      "Gradient Descent(48/49): loss=2616072337189.488\n",
      "Gradient Descent(49/49): loss=5054513362683.527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5466070340294489\n",
      "Gradient Descent(2/49): loss=0.6366564844777317\n",
      "Gradient Descent(3/49): loss=0.8106410276889079\n",
      "Gradient Descent(4/49): loss=1.146796563627199\n",
      "Gradient Descent(5/49): loss=1.7962826746134333\n",
      "Gradient Descent(6/49): loss=3.051154789650003\n",
      "Gradient Descent(7/49): loss=5.475693203111943\n",
      "Gradient Descent(8/49): loss=10.160143871761706\n",
      "Gradient Descent(9/49): loss=19.21097100866194\n",
      "Gradient Descent(10/49): loss=36.69807411986625\n",
      "Gradient Descent(11/49): loss=70.48490604101488\n",
      "Gradient Descent(12/49): loss=135.76444399587675\n",
      "Gradient Descent(13/49): loss=261.8910392784692\n",
      "Gradient Descent(14/49): loss=505.58023402399357\n",
      "Gradient Descent(15/49): loss=976.4121271918763\n",
      "Gradient Descent(16/49): loss=1886.1064279813786\n",
      "Gradient Descent(17/49): loss=3643.7267865367958\n",
      "Gradient Descent(18/49): loss=7039.625081302115\n",
      "Gradient Descent(19/49): loss=13600.840176617081\n",
      "Gradient Descent(20/49): loss=26277.763862274955\n",
      "Gradient Descent(21/49): loss=50770.84811533383\n",
      "Gradient Descent(22/49): loss=98093.93620066352\n",
      "Gradient Descent(23/49): loss=189526.87469033568\n",
      "Gradient Descent(24/49): loss=366184.4551462581\n",
      "Gradient Descent(25/49): loss=707504.5663451519\n",
      "Gradient Descent(26/49): loss=1366969.1531926866\n",
      "Gradient Descent(27/49): loss=2641120.681440958\n",
      "Gradient Descent(28/49): loss=5102908.849169574\n",
      "Gradient Descent(29/49): loss=9859329.768037133\n",
      "Gradient Descent(30/49): loss=19049210.625383817\n",
      "Gradient Descent(31/49): loss=36804979.42985784\n",
      "Gradient Descent(32/49): loss=71110900.33698748\n",
      "Gradient Descent(33/49): loss=137393370.1216406\n",
      "Gradient Descent(34/49): loss=265457729.99258512\n",
      "Gradient Descent(35/49): loss=512890879.6992267\n",
      "Gradient Descent(36/49): loss=990956468.2474487\n",
      "Gradient Descent(37/49): loss=1914626991.8816078\n",
      "Gradient Descent(38/49): loss=3699250810.594794\n",
      "Gradient Descent(39/49): loss=7147322490.731398\n",
      "Gradient Descent(40/49): loss=13809341783.922752\n",
      "Gradient Descent(41/49): loss=26681029260.2956\n",
      "Gradient Descent(42/49): loss=51550416633.40146\n",
      "Gradient Descent(43/49): loss=99600559976.97005\n",
      "Gradient Descent(44/49): loss=192438241931.0757\n",
      "Gradient Descent(45/49): loss=371809927234.5801\n",
      "Gradient Descent(46/49): loss=718373960409.502\n",
      "Gradient Descent(47/49): loss=1387970328906.6265\n",
      "Gradient Descent(48/49): loss=2681697472480.284\n",
      "Gradient Descent(49/49): loss=5181307686578.979\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5460678121280002\n",
      "Gradient Descent(2/49): loss=0.6350754319405001\n",
      "Gradient Descent(3/49): loss=0.8070470541802232\n",
      "Gradient Descent(4/49): loss=1.1393134255096053\n",
      "Gradient Descent(5/49): loss=1.7812852815551559\n",
      "Gradient Descent(6/49): loss=3.0216391046206685\n",
      "Gradient Descent(7/49): loss=5.4181267261662684\n",
      "Gradient Descent(8/49): loss=10.04838045975432\n",
      "Gradient Descent(9/49): loss=18.994493698419003\n",
      "Gradient Descent(10/49): loss=36.27927908684207\n",
      "Gradient Descent(11/49): loss=69.67521293582006\n",
      "Gradient Descent(12/49): loss=134.1994967254126\n",
      "Gradient Descent(13/49): loss=258.8668654353122\n",
      "Gradient Descent(14/49): loss=499.736688519667\n",
      "Gradient Descent(15/49): loss=965.1212737010445\n",
      "Gradient Descent(16/49): loss=1864.290830730107\n",
      "Gradient Descent(17/49): loss=3601.576331865806\n",
      "Gradient Descent(18/49): loss=6958.185648609834\n",
      "Gradient Descent(19/49): loss=13443.49050949131\n",
      "Gradient Descent(20/49): loss=25973.748031199153\n",
      "Gradient Descent(21/49): loss=50183.45858889414\n",
      "Gradient Descent(22/49): loss=96959.04035740218\n",
      "Gradient Descent(23/49): loss=187334.1418923598\n",
      "Gradient Descent(24/49): loss=361947.8755680585\n",
      "Gradient Descent(25/49): loss=699319.0704028448\n",
      "Gradient Descent(26/49): loss=1351153.9559432792\n",
      "Gradient Descent(27/49): loss=2610564.1382957837\n",
      "Gradient Descent(28/49): loss=5043870.551619348\n",
      "Gradient Descent(29/49): loss=9745261.87280084\n",
      "Gradient Descent(30/49): loss=18828820.04445716\n",
      "Gradient Descent(31/49): loss=36379162.78791643\n",
      "Gradient Descent(32/49): loss=70288180.00255333\n",
      "Gradient Descent(33/49): loss=135803792.16294384\n",
      "Gradient Descent(34/49): loss=262386506.41806245\n",
      "Gradient Descent(35/49): loss=506956968.63032925\n",
      "Gradient Descent(36/49): loss=979491558.6706595\n",
      "Gradient Descent(37/49): loss=1892475640.0876544\n",
      "Gradient Descent(38/49): loss=3656452183.7930307\n",
      "Gradient Descent(39/49): loss=7064631263.887241\n",
      "Gradient Descent(40/49): loss=13649574064.5354\n",
      "Gradient Descent(41/49): loss=26372342049.668076\n",
      "Gradient Descent(42/49): loss=50954002073.74359\n",
      "Gradient Descent(43/49): loss=98448227406.25401\n",
      "Gradient Descent(44/49): loss=190211820171.19934\n",
      "Gradient Descent(45/49): loss=367508257752.35974\n",
      "Gradient Descent(46/49): loss=710062704802.9147\n",
      "Gradient Descent(47/49): loss=1371912151949.1257\n",
      "Gradient Descent(48/49): loss=2650671468780.628\n",
      "Gradient Descent(49/49): loss=5121362344830.214\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5464436825715202\n",
      "Gradient Descent(2/49): loss=0.6361775216679559\n",
      "Gradient Descent(3/49): loss=0.809552272186159\n",
      "Gradient Descent(4/49): loss=1.1445296276624453\n",
      "Gradient Descent(5/49): loss=1.7917393761780496\n",
      "Gradient Descent(6/49): loss=3.0422133312850534\n",
      "Gradient Descent(7/49): loss=5.458254059947016\n",
      "Gradient Descent(8/49): loss=10.126286351795795\n",
      "Gradient Descent(9/49): loss=19.145391542878006\n",
      "Gradient Descent(10/49): loss=36.57120468256458\n",
      "Gradient Descent(11/49): loss=70.23961824976313\n",
      "Gradient Descent(12/49): loss=135.29036010292657\n",
      "Gradient Descent(13/49): loss=260.9748984374488\n",
      "Gradient Descent(14/49): loss=503.8099949535506\n",
      "Gradient Descent(15/49): loss=972.9916849323624\n",
      "Gradient Descent(16/49): loss=1879.4976281401944\n",
      "Gradient Descent(17/49): loss=3630.957761011828\n",
      "Gradient Descent(18/49): loss=7014.953883734262\n",
      "Gradient Descent(19/49): loss=13553.172792447393\n",
      "Gradient Descent(20/49): loss=26185.66554596835\n",
      "Gradient Descent(21/49): loss=50592.9047950424\n",
      "Gradient Descent(22/49): loss=97750.1317481786\n",
      "Gradient Descent(23/49): loss=188862.6099443287\n",
      "Gradient Descent(24/49): loss=364901.0290670909\n",
      "Gradient Descent(25/49): loss=705024.8586542578\n",
      "Gradient Descent(26/49): loss=1362178.109799671\n",
      "Gradient Descent(27/49): loss=2631863.9063377166\n",
      "Gradient Descent(28/49): loss=5085023.833828327\n",
      "Gradient Descent(29/49): loss=9824774.129733045\n",
      "Gradient Descent(30/49): loss=18982445.676452428\n",
      "Gradient Descent(31/49): loss=36675982.87186634\n",
      "Gradient Descent(32/49): loss=70861666.08712041\n",
      "Gradient Descent(33/49): loss=136911824.62732986\n",
      "Gradient Descent(34/49): loss=264527335.94284308\n",
      "Gradient Descent(35/49): loss=511093265.35559726\n",
      "Gradient Descent(36/49): loss=987483297.573852\n",
      "Gradient Descent(37/49): loss=1907916478.8230407\n",
      "Gradient Descent(38/49): loss=3686285428.3144236\n",
      "Gradient Descent(39/49): loss=7122272075.626979\n",
      "Gradient Descent(40/49): loss=13760941876.900393\n",
      "Gradient Descent(41/49): loss=26587515799.939697\n",
      "Gradient Descent(42/49): loss=51369739276.63928\n",
      "Gradient Descent(43/49): loss=99251473255.97986\n",
      "Gradient Descent(44/49): loss=191763771477.47278\n",
      "Gradient Descent(45/49): loss=370506782871.2402\n",
      "Gradient Descent(46/49): loss=715856155185.0487\n",
      "Gradient Descent(47/49): loss=1383105677432.5322\n",
      "Gradient Descent(48/49): loss=2672298479366.8013\n",
      "Gradient Descent(49/49): loss=5163147891983.673\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5495785225439915\n",
      "Gradient Descent(2/49): loss=0.6495486554017208\n",
      "Gradient Descent(3/49): loss=0.8511284312960598\n",
      "Gradient Descent(4/49): loss=1.2575938914093996\n",
      "Gradient Descent(5/49): loss=2.0771908451818675\n",
      "Gradient Descent(6/49): loss=3.729826142768595\n",
      "Gradient Descent(7/49): loss=7.062199956822822\n",
      "Gradient Descent(8/49): loss=13.781598515481805\n",
      "Gradient Descent(9/49): loss=27.330593769161144\n",
      "Gradient Descent(10/49): loss=54.65078779868438\n",
      "Gradient Descent(11/49): loss=109.7392270398159\n",
      "Gradient Descent(12/49): loss=220.8195559256499\n",
      "Gradient Descent(13/49): loss=444.80193109099866\n",
      "Gradient Descent(14/49): loss=896.4399923744226\n",
      "Gradient Descent(15/49): loss=1807.1229791464166\n",
      "Gradient Descent(16/49): loss=3643.42415367328\n",
      "Gradient Descent(17/49): loss=7346.1418419902175\n",
      "Gradient Descent(18/49): loss=14812.301788710321\n",
      "Gradient Descent(19/49): loss=29867.066705281246\n",
      "Gradient Descent(20/49): loss=60223.494683046345\n",
      "Gradient Descent(21/49): loss=121434.19605742543\n",
      "Gradient Descent(22/49): loss=244859.45430868663\n",
      "Gradient Descent(23/49): loss=493734.1450465087\n",
      "Gradient Descent(24/49): loss=995565.0714502496\n",
      "Gradient Descent(25/49): loss=2007456.9514509304\n",
      "Gradient Descent(26/49): loss=4047835.7382842335\n",
      "Gradient Descent(27/49): loss=8162055.524055444\n",
      "Gradient Descent(28/49): loss=16457968.300085338\n",
      "Gradient Descent(29/49): loss=33185846.821668923\n",
      "Gradient Descent(30/49): loss=66915941.07259683\n",
      "Gradient Descent(31/49): loss=134929303.12016204\n",
      "Gradient Descent(32/49): loss=272071446.3528707\n",
      "Gradient Descent(33/49): loss=548604863.9672501\n",
      "Gradient Descent(34/49): loss=1106206847.2448673\n",
      "Gradient Descent(35/49): loss=2230555486.325885\n",
      "Gradient Descent(36/49): loss=4497692082.169115\n",
      "Gradient Descent(37/49): loss=9069146314.026577\n",
      "Gradient Descent(38/49): loss=18287026627.142593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=36873960490.5089\n",
      "Gradient Descent(40/49): loss=74352653932.59921\n",
      "Gradient Descent(41/49): loss=149924691389.224\n",
      "Gradient Descent(42/49): loss=302308147716.7655\n",
      "Gradient Descent(43/49): loss=609574149055.6559\n",
      "Gradient Descent(44/49): loss=1229145314155.4915\n",
      "Gradient Descent(45/49): loss=2478448611462.555\n",
      "Gradient Descent(46/49): loss=4997543780152.881\n",
      "Gradient Descent(47/49): loss=10077047278301.197\n",
      "Gradient Descent(48/49): loss=20319358131964.36\n",
      "Gradient Descent(49/49): loss=40971953737291.26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5508222179889837\n",
      "Gradient Descent(2/49): loss=0.6533001383419761\n",
      "Gradient Descent(3/49): loss=0.8599366169417961\n",
      "Gradient Descent(4/49): loss=1.276598412390459\n",
      "Gradient Descent(5/49): loss=2.1167552567330064\n",
      "Gradient Descent(6/49): loss=3.810847517665375\n",
      "Gradient Descent(7/49): loss=7.226815152610057\n",
      "Gradient Descent(8/49): loss=14.114772291710945\n",
      "Gradient Descent(9/49): loss=28.00364906699238\n",
      "Gradient Descent(10/49): loss=56.00918019667147\n",
      "Gradient Descent(11/49): loss=112.47953316656857\n",
      "Gradient Descent(12/49): loss=226.34635289506323\n",
      "Gradient Descent(13/49): loss=455.9474081955507\n",
      "Gradient Descent(14/49): loss=918.9149761034147\n",
      "Gradient Descent(15/49): loss=1852.4427800327956\n",
      "Gradient Descent(16/49): loss=3734.8082438761908\n",
      "Gradient Descent(17/49): loss=7530.40996517038\n",
      "Gradient Descent(18/49): loss=15183.861275986512\n",
      "Gradient Descent(19/49): loss=30616.28049911883\n",
      "Gradient Descent(20/49): loss=61734.21062064223\n",
      "Gradient Descent(21/49): loss=124480.40491769617\n",
      "Gradient Descent(22/49): loss=251001.83109824252\n",
      "Gradient Descent(23/49): loss=506119.6348487458\n",
      "Gradient Descent(24/49): loss=1020539.1743312223\n",
      "Gradient Descent(25/49): loss=2057814.7337435137\n",
      "Gradient Descent(26/49): loss=4149377.1717428328\n",
      "Gradient Descent(27/49): loss=8366803.671724127\n",
      "Gradient Descent(28/49): loss=16870822.466285832\n",
      "Gradient Descent(29/49): loss=34018325.963639714\n",
      "Gradient Descent(30/49): loss=68594552.01571278\n",
      "Gradient Descent(31/49): loss=138314054.2271076\n",
      "Gradient Descent(32/49): loss=278896458.48614615\n",
      "Gradient Descent(33/49): loss=562366818.4340721\n",
      "Gradient Descent(34/49): loss=1133956452.2330866\n",
      "Gradient Descent(35/49): loss=2286509789.825318\n",
      "Gradient Descent(36/49): loss=4610518339.746295\n",
      "Gradient Descent(37/49): loss=9296649179.807009\n",
      "Gradient Descent(38/49): loss=18745763405.704052\n",
      "Gradient Descent(39/49): loss=37798957330.80709\n",
      "Gradient Descent(40/49): loss=76217817561.38567\n",
      "Gradient Descent(41/49): loss=153685607330.32788\n",
      "Gradient Descent(42/49): loss=309891658620.4444\n",
      "Gradient Descent(43/49): loss=624865540441.8418\n",
      "Gradient Descent(44/49): loss=1259978875746.349\n",
      "Gradient Descent(45/49): loss=2540621405054.6895\n",
      "Gradient Descent(46/49): loss=5122909001151.435\n",
      "Gradient Descent(47/49): loss=10329833709921.92\n",
      "Gradient Descent(48/49): loss=20829076692687.523\n",
      "Gradient Descent(49/49): loss=41999750243135.99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5502342283520001\n",
      "Gradient Descent(2/49): loss=0.6515265264009844\n",
      "Gradient Descent(3/49): loss=0.8557723161869588\n",
      "Gradient Descent(4/49): loss=1.2676135267113353\n",
      "Gradient Descent(5/49): loss=2.0980501436128\n",
      "Gradient Descent(6/49): loss=3.7725425379326176\n",
      "Gradient Descent(7/49): loss=7.148989001839546\n",
      "Gradient Descent(8/49): loss=13.95725565166065\n",
      "Gradient Descent(9/49): loss=27.6854445243588\n",
      "Gradient Descent(10/49): loss=55.36696456726717\n",
      "Gradient Descent(11/49): loss=111.18398158178258\n",
      "Gradient Descent(12/49): loss=223.7334146898548\n",
      "Gradient Descent(13/49): loss=450.67809160895547\n",
      "Gradient Descent(14/49): loss=908.2893381486322\n",
      "Gradient Descent(15/49): loss=1831.0166556712595\n",
      "Gradient Descent(16/49): loss=3691.6040187234903\n",
      "Gradient Descent(17/49): loss=7443.292377582984\n",
      "Gradient Descent(18/49): loss=15008.196784386246\n",
      "Gradient Descent(19/49): loss=30262.070030263698\n",
      "Gradient Descent(20/49): loss=61019.98004325109\n",
      "Gradient Descent(21/49): loss=123040.2297934492\n",
      "Gradient Descent(22/49): loss=248097.8613897428\n",
      "Gradient Descent(23/49): loss=500264.069740452\n",
      "Gradient Descent(24/49): loss=1008732.0122587698\n",
      "Gradient Descent(25/49): loss=2034006.7715526973\n",
      "Gradient Descent(26/49): loss=4101370.7961929045\n",
      "Gradient Descent(27/49): loss=8270003.615478375\n",
      "Gradient Descent(28/49): loss=16675634.832285674\n",
      "Gradient Descent(29/49): loss=33624749.617853776\n",
      "Gradient Descent(30/49): loss=67800944.6714662\n",
      "Gradient Descent(31/49): loss=136713824.37758315\n",
      "Gradient Descent(32/49): loss=275669755.0169875\n",
      "Gradient Descent(33/49): loss=555860493.558233\n",
      "Gradient Descent(34/49): loss=1120837098.752895\n",
      "Gradient Descent(35/49): loss=2260055925.467492\n",
      "Gradient Descent(36/49): loss=4557176767.655272\n",
      "Gradient Descent(37/49): loss=9189091233.842798\n",
      "Gradient Descent(38/49): loss=18528883563.464108\n",
      "Gradient Descent(39/49): loss=37361640816.9139\n",
      "Gradient Descent(40/49): loss=75336012542.77359\n",
      "Gradient Descent(41/49): loss=151907535690.79623\n",
      "Gradient Descent(42/49): loss=306306354966.438\n",
      "Gradient Descent(43/49): loss=617636134153.8207\n",
      "Gradient Descent(44/49): loss=1245401500907.4348\n",
      "Gradient Descent(45/49): loss=2511227586429.1445\n",
      "Gradient Descent(46/49): loss=5063639305275.515\n",
      "Gradient Descent(47/49): loss=10210322295156.46\n",
      "Gradient Descent(48/49): loss=20588093875955.406\n",
      "Gradient Descent(49/49): loss=41513832491474.586\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5506440928716803\n",
      "Gradient Descent(2/49): loss=0.6527628417381537\n",
      "Gradient Descent(3/49): loss=0.8586750869524988\n",
      "Gradient Descent(4/49): loss=1.2738765382027049\n",
      "Gradient Descent(5/49): loss=2.1110887445034456\n",
      "Gradient Descent(6/49): loss=3.7992434372882067\n",
      "Gradient Descent(7/49): loss=7.203238559819712\n",
      "Gradient Descent(8/49): loss=14.067054324891997\n",
      "Gradient Descent(9/49): loss=27.907252433583547\n",
      "Gradient Descent(10/49): loss=55.81462789994942\n",
      "Gradient Descent(11/49): loss=112.08705979033093\n",
      "Gradient Descent(12/49): loss=225.55479145409834\n",
      "Gradient Descent(13/49): loss=454.3511255808927\n",
      "Gradient Descent(14/49): loss=915.6960537142347\n",
      "Gradient Descent(15/49): loss=1845.9519668023777\n",
      "Gradient Descent(16/49): loss=3721.7199899533703\n",
      "Gradient Descent(17/49): loss=7504.018631835639\n",
      "Gradient Descent(18/49): loss=15130.645613325778\n",
      "Gradient Descent(19/49): loss=30508.976258805556\n",
      "Gradient Descent(20/49): loss=61517.84217234582\n",
      "Gradient Descent(21/49): loss=124044.1194004186\n",
      "Gradient Descent(22/49): loss=250122.10480307674\n",
      "Gradient Descent(23/49): loss=504345.7545690404\n",
      "Gradient Descent(24/49): loss=1016962.321957033\n",
      "Gradient Descent(25/49): loss=2050602.3684381566\n",
      "Gradient Descent(26/49): loss=4134834.1581625747\n",
      "Gradient Descent(27/49): loss=8337479.138963655\n",
      "Gradient Descent(28/49): loss=16811692.478250757\n",
      "Gradient Descent(29/49): loss=33899096.25559119\n",
      "Gradient Descent(30/49): loss=68354137.23222405\n",
      "Gradient Descent(31/49): loss=137829281.85751045\n",
      "Gradient Descent(32/49): loss=277918963.4799585\n",
      "Gradient Descent(33/49): loss=560395797.5034084\n",
      "Gradient Descent(34/49): loss=1129982085.6284304\n",
      "Gradient Descent(35/49): loss=2278495877.0038257\n",
      "Gradient Descent(36/49): loss=4594359085.9332\n",
      "Gradient Descent(37/49): loss=9264065660.417032\n",
      "Gradient Descent(38/49): loss=18680061997.208282\n",
      "Gradient Descent(39/49): loss=37666477010.7112\n",
      "Gradient Descent(40/49): loss=75950684243.94849\n",
      "Gradient Descent(41/49): loss=153146959709.06088\n",
      "Gradient Descent(42/49): loss=308805529556.8932\n",
      "Gradient Descent(43/49): loss=622675469798.0208\n",
      "Gradient Descent(44/49): loss=1255562817300.1858\n",
      "Gradient Descent(45/49): loss=2531716864803.723\n",
      "Gradient Descent(46/49): loss=5104953886189.904\n",
      "Gradient Descent(47/49): loss=10293629016113.08\n",
      "Gradient Descent(48/49): loss=20756073548089.414\n",
      "Gradient Descent(49/49): loss=41852546702367.02\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5537783560652804\n",
      "Gradient Descent(2/49): loss=0.6668473496925151\n",
      "Gradient Descent(3/49): loss=0.9045749087937921\n",
      "Gradient Descent(4/49): loss=1.404397101804241\n",
      "Gradient Descent(5/49): loss=2.4552732626085922\n",
      "Gradient Descent(6/49): loss=4.664740390700124\n",
      "Gradient Descent(7/49): loss=9.31014502751325\n",
      "Gradient Descent(8/49): loss=19.07710827641276\n",
      "Gradient Descent(9/49): loss=39.61214850721988\n",
      "Gradient Descent(10/49): loss=82.78707059248579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=173.5623442767511\n",
      "Gradient Descent(12/49): loss=364.417357197939\n",
      "Gradient Descent(13/49): loss=765.6900218647671\n",
      "Gradient Descent(14/49): loss=1609.3657993266181\n",
      "Gradient Descent(15/49): loss=3383.1941214406784\n",
      "Gradient Descent(16/49): loss=7112.668168685909\n",
      "Gradient Descent(17/49): loss=14953.887353017863\n",
      "Gradient Descent(18/49): loss=31440.050688077587\n",
      "Gradient Descent(19/49): loss=66102.20910003434\n",
      "Gradient Descent(20/49): loss=138979.39716116711\n",
      "Gradient Descent(21/49): loss=292203.685059738\n",
      "Gradient Descent(22/49): loss=614357.7503665107\n",
      "Gradient Descent(23/49): loss=1291686.67267406\n",
      "Gradient Descent(24/49): loss=2715770.7318257154\n",
      "Gradient Descent(25/49): loss=5709907.466191766\n",
      "Gradient Descent(26/49): loss=12005079.95019624\n",
      "Gradient Descent(27/49): loss=25240680.097818006\n",
      "Gradient Descent(28/49): loss=53068529.40819222\n",
      "Gradient Descent(29/49): loss=111576582.58324319\n",
      "Gradient Descent(30/49): loss=234589764.3837722\n",
      "Gradient Descent(31/49): loss=493224979.11936474\n",
      "Gradient Descent(32/49): loss=1037005518.1010847\n",
      "Gradient Descent(33/49): loss=2180304101.3100667\n",
      "Gradient Descent(34/49): loss=4584089372.506766\n",
      "Gradient Descent(35/49): loss=9638047905.198343\n",
      "Gradient Descent(36/49): loss=20263995720.181618\n",
      "Gradient Descent(37/49): loss=42605051001.18617\n",
      "Gradient Descent(38/49): loss=89577119729.49176\n",
      "Gradient Descent(39/49): loss=188335894230.73938\n",
      "Gradient Descent(40/49): loss=395976217619.6011\n",
      "Gradient Descent(41/49): loss=832539997544.6892\n",
      "Gradient Descent(42/49): loss=1750415344837.1272\n",
      "Gradient Descent(43/49): loss=3680248262519.3228\n",
      "Gradient Descent(44/49): loss=7737721971945.767\n",
      "Gradient Descent(45/49): loss=16268560446014.205\n",
      "Gradient Descent(46/49): loss=34204648337746.887\n",
      "Gradient Descent(47/49): loss=71915273130119.56\n",
      "Gradient Descent(48/49): loss=151201861756073.12\n",
      "Gradient Descent(49/49): loss=317901914342169.6\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5551274058764802\n",
      "Gradient Descent(2/49): loss=0.6710327767317799\n",
      "Gradient Descent(3/49): loss=0.9147238189550965\n",
      "Gradient Descent(4/49): loss=1.4270842352294817\n",
      "Gradient Descent(5/49): loss=2.504322010446563\n",
      "Gradient Descent(6/49): loss=4.769214432840211\n",
      "Gradient Descent(7/49): loss=9.531150750923892\n",
      "Gradient Descent(8/49): loss=19.54312185969598\n",
      "Gradient Descent(9/49): loss=40.59329111589096\n",
      "Gradient Descent(10/49): loss=84.85127197703537\n",
      "Gradient Descent(11/49): loss=177.90367673759218\n",
      "Gradient Descent(12/49): loss=373.54635774664825\n",
      "Gradient Descent(13/49): loss=784.8850945682418\n",
      "Gradient Descent(14/49): loss=1649.7247887357407\n",
      "Gradient Descent(15/49): loss=3468.0502457226467\n",
      "Gradient Descent(16/49): loss=7291.079519038344\n",
      "Gradient Descent(17/49): loss=15328.998566184791\n",
      "Gradient Descent(18/49): loss=32228.723362813453\n",
      "Gradient Descent(19/49): loss=67760.39474771405\n",
      "Gradient Descent(20/49): loss=142465.73383447403\n",
      "Gradient Descent(21/49): loss=299533.70926439663\n",
      "Gradient Descent(22/49): loss=629769.1276058161\n",
      "Gradient Descent(23/49): loss=1324089.0946687262\n",
      "Gradient Descent(24/49): loss=2783896.825418387\n",
      "Gradient Descent(25/49): loss=5853142.579319139\n",
      "Gradient Descent(26/49): loss=12306231.77689606\n",
      "Gradient Descent(27/49): loss=25873851.814798933\n",
      "Gradient Descent(28/49): loss=54399772.94449225\n",
      "Gradient Descent(29/49): loss=114375522.11968552\n",
      "Gradient Descent(30/49): loss=240474534.7605372\n",
      "Gradient Descent(31/49): loss=505597708.8378908\n",
      "Gradient Descent(32/49): loss=1063019182.3354715\n",
      "Gradient Descent(33/49): loss=2234997830.3643064\n",
      "Gradient Descent(34/49): loss=4699082937.844947\n",
      "Gradient Descent(35/49): loss=9879821876.322231\n",
      "Gradient Descent(36/49): loss=20772325494.47209\n",
      "Gradient Descent(37/49): loss=43673814351.6306\n",
      "Gradient Descent(38/49): loss=91824194673.79807\n",
      "Gradient Descent(39/49): loss=193060369301.16068\n",
      "Gradient Descent(40/49): loss=405909426455.164\n",
      "Gradient Descent(41/49): loss=853424569121.4323\n",
      "Gradient Descent(42/49): loss=1794325156577.2637\n",
      "Gradient Descent(43/49): loss=3772568641702.9365\n",
      "Gradient Descent(44/49): loss=7931825569179.352\n",
      "Gradient Descent(45/49): loss=16676663259200.543\n",
      "Gradient Descent(46/49): loss=35062684502467.54\n",
      "Gradient Descent(47/49): loss=73719294166437.11\n",
      "Gradient Descent(48/49): loss=154994815984921.06\n",
      "Gradient Descent(49/49): loss=325876600608294.2\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5544896072000002\n",
      "Gradient Descent(2/49): loss=0.6690540063379976\n",
      "Gradient Descent(3/49): loss=0.9099256555256708\n",
      "Gradient Descent(4/49): loss=1.4163582979427094\n",
      "Gradient Descent(5/49): loss=2.4811329286247092\n",
      "Gradient Descent(6/49): loss=4.719821589633147\n",
      "Gradient Descent(7/49): loss=9.426664499402882\n",
      "Gradient Descent(8/49): loss=19.322801717193\n",
      "Gradient Descent(9/49): loss=40.12943021759848\n",
      "Gradient Descent(10/49): loss=83.87536663970052\n",
      "Gradient Descent(11/49): loss=175.8511979671614\n",
      "Gradient Descent(12/49): loss=369.2303833331292\n",
      "Gradient Descent(13/49): loss=775.8101205650562\n",
      "Gradient Descent(14/49): loss=1630.6440180953225\n",
      "Gradient Descent(15/49): loss=3427.9322876526217\n",
      "Gradient Descent(16/49): loss=7206.730874397451\n",
      "Gradient Descent(17/49): loss=15151.654903026176\n",
      "Gradient Descent(18/49): loss=31855.857673219696\n",
      "Gradient Descent(19/49): loss=66976.4439975466\n",
      "Gradient Descent(20/49): loss=140817.4767444545\n",
      "Gradient Descent(21/49): loss=296068.2480948247\n",
      "Gradient Descent(22/49): loss=622482.9948590343\n",
      "Gradient Descent(23/49): loss=1308769.999930579\n",
      "Gradient Descent(24/49): loss=2751688.428093573\n",
      "Gradient Descent(25/49): loss=5785424.423306751\n",
      "Gradient Descent(26/49): loss=12163854.353242675\n",
      "Gradient Descent(27/49): loss=25574503.280930556\n",
      "Gradient Descent(28/49): loss=53770392.651396595\n",
      "Gradient Descent(29/49): loss=113052250.05281144\n",
      "Gradient Descent(30/49): loss=237692355.23926997\n",
      "Gradient Descent(31/49): loss=499748176.39384574\n",
      "Gradient Descent(32/49): loss=1050720540.3711782\n",
      "Gradient Descent(33/49): loss=2209139935.633747\n",
      "Gradient Descent(34/49): loss=4644716714.17361\n",
      "Gradient Descent(35/49): loss=9765516891.052223\n",
      "Gradient Descent(36/49): loss=20531999262.940407\n",
      "Gradient Descent(37/49): loss=43168528449.830086\n",
      "Gradient Descent(38/49): loss=90761831065.2781\n",
      "Gradient Descent(39/49): loss=190826749814.2347\n",
      "Gradient Descent(40/49): loss=401213241483.9203\n",
      "Gradient Descent(41/49): loss=843550840219.5189\n",
      "Gradient Descent(42/49): loss=1773565641560.8696\n",
      "Gradient Descent(43/49): loss=3728921761381.202\n",
      "Gradient Descent(44/49): loss=7840058003302.942\n",
      "Gradient Descent(45/49): loss=16483721951944.254\n",
      "Gradient Descent(46/49): loss=34657025403959.395\n",
      "Gradient Descent(47/49): loss=72866395911830.31\n",
      "Gradient Descent(48/49): loss=153201597404614.28\n",
      "Gradient Descent(49/49): loss=322106358543172.3\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5549341916480003\n",
      "Gradient Descent(2/49): loss=0.6704333295879161\n",
      "Gradient Descent(3/49): loss=0.913270267106641\n",
      "Gradient Descent(4/49): loss=1.4238349282396665\n",
      "Gradient Descent(5/49): loss=2.4972971282721077\n",
      "Gradient Descent(6/49): loss=4.754251403840352\n",
      "Gradient Descent(7/49): loss=9.499497768221502\n",
      "Gradient Descent(8/49): loss=19.4763782493359\n",
      "Gradient Descent(9/49): loss=40.452769460877654\n",
      "Gradient Descent(10/49): loss=84.55563198315097\n",
      "Gradient Descent(11/49): loss=177.2819004362162\n",
      "Gradient Descent(12/49): loss=372.2388798588017\n",
      "Gradient Descent(13/49): loss=782.1359290947105\n",
      "Gradient Descent(14/49): loss=1643.9444751132155\n",
      "Gradient Descent(15/49): loss=3455.896943117324\n",
      "Gradient Descent(16/49): loss=7265.527007095373\n",
      "Gradient Descent(17/49): loss=15275.27421661121\n",
      "Gradient Descent(18/49): loss=32115.767724613957\n",
      "Gradient Descent(19/49): loss=67522.90532518624\n",
      "Gradient Descent(20/49): loss=141966.4121303808\n",
      "Gradient Descent(21/49): loss=298483.88518830336\n",
      "Gradient Descent(22/49): loss=627561.8722926526\n",
      "Gradient Descent(23/49): loss=1319448.3401794117\n",
      "Gradient Descent(24/49): loss=2774139.638911491\n",
      "Gradient Descent(25/49): loss=5832628.094495798\n",
      "Gradient Descent(26/49): loss=12263100.072361583\n",
      "Gradient Descent(27/49): loss=25783167.405822948\n",
      "Gradient Descent(28/49): loss=54209108.97442579\n",
      "Gradient Descent(29/49): loss=113974651.12242275\n",
      "Gradient Descent(30/49): loss=239631703.48856235\n",
      "Gradient Descent(31/49): loss=503825656.08836323\n",
      "Gradient Descent(32/49): loss=1059293441.429571\n",
      "Gradient Descent(33/49): loss=2227164460.1095467\n",
      "Gradient Descent(34/49): loss=4682613276.884546\n",
      "Gradient Descent(35/49): loss=9845194414.154501\n",
      "Gradient Descent(36/49): loss=20699521255.265236\n",
      "Gradient Descent(37/49): loss=43520743438.7001\n",
      "Gradient Descent(38/49): loss=91502363079.37605\n",
      "Gradient Descent(39/49): loss=192383718373.89368\n",
      "Gradient Descent(40/49): loss=404486767880.6369\n",
      "Gradient Descent(41/49): loss=850433429468.519\n",
      "Gradient Descent(42/49): loss=1788036285457.1165\n",
      "Gradient Descent(43/49): loss=3759346290172.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=7904025575087.801\n",
      "Gradient Descent(45/49): loss=16618213771622.574\n",
      "Gradient Descent(46/49): loss=34939794454834.375\n",
      "Gradient Descent(47/49): loss=73460917841291.52\n",
      "Gradient Descent(48/49): loss=154451579761328.75\n",
      "Gradient Descent(49/49): loss=324734446448173.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5580659909842264\n",
      "Gradient Descent(2/49): loss=0.6852537376360857\n",
      "Gradient Descent(3/49): loss=0.9638457779023132\n",
      "Gradient Descent(4/49): loss=1.5740737829013614\n",
      "Gradient Descent(5/49): loss=2.910717205051671\n",
      "Gradient Descent(6/49): loss=5.838500956929521\n",
      "Gradient Descent(7/49): loss=12.25151848704341\n",
      "Gradient Descent(8/49): loss=26.298592085005982\n",
      "Gradient Descent(9/49): loss=57.06730209398147\n",
      "Gradient Descent(10/49): loss=124.46308449764354\n",
      "Gradient Descent(11/49): loss=272.0868062746499\n",
      "Gradient Descent(12/49): loss=595.4418064549927\n",
      "Gradient Descent(13/49): loss=1303.7185988501344\n",
      "Gradient Descent(14/49): loss=2855.1280849121795\n",
      "Gradient Descent(15/49): loss=6253.335423182907\n",
      "Gradient Descent(16/49): loss=13696.768776930974\n",
      "Gradient Descent(17/49): loss=30000.865194983322\n",
      "Gradient Descent(18/49): loss=65713.35798908422\n",
      "Gradient Descent(19/49): loss=143938.00220527768\n",
      "Gradient Descent(20/49): loss=315281.2628964126\n",
      "Gradient Descent(21/49): loss=690591.541114236\n",
      "Gradient Descent(22/49): loss=1512671.1745227317\n",
      "Gradient Descent(23/49): loss=3313354.4035405996\n",
      "Gradient Descent(24/49): loss=7257570.948380752\n",
      "Gradient Descent(25/49): loss=15896982.8681995\n",
      "Gradient Descent(26/49): loss=34820750.7373734\n",
      "Gradient Descent(27/49): loss=76271371.87800844\n",
      "Gradient Descent(28/49): loss=167064812.4244497\n",
      "Gradient Descent(29/49): loss=365938764.59736955\n",
      "Gradient Descent(30/49): loss=801552269.436927\n",
      "Gradient Descent(31/49): loss=1755720090.4375024\n",
      "Gradient Descent(32/49): loss=3845729285.557112\n",
      "Gradient Descent(33/49): loss=8423685426.546048\n",
      "Gradient Descent(34/49): loss=18451240557.77064\n",
      "Gradient Descent(35/49): loss=40415597317.20179\n",
      "Gradient Descent(36/49): loss=88526324363.06364\n",
      "Gradient Descent(37/49): loss=193908060884.3203\n",
      "Gradient Descent(38/49): loss=424736216560.52155\n",
      "Gradient Descent(39/49): loss=930342208753.6968\n",
      "Gradient Descent(40/49): loss=2037821574053.6665\n",
      "Gradient Descent(41/49): loss=4463644375806.451\n",
      "Gradient Descent(42/49): loss=9777166640765.049\n",
      "Gradient Descent(43/49): loss=21415905809929.293\n",
      "Gradient Descent(44/49): loss=46909400086068.26\n",
      "Gradient Descent(45/49): loss=102750349948521.17\n",
      "Gradient Descent(46/49): loss=225064366527239.22\n",
      "Gradient Descent(47/49): loss=492980988441281.2\n",
      "Gradient Descent(48/49): loss=1079825557081703.1\n",
      "Gradient Descent(49/49): loss=2365249900231970.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5595225976919382\n",
      "Gradient Descent(2/49): loss=0.6899008956763488\n",
      "Gradient Descent(3/49): loss=0.9754815195814543\n",
      "Gradient Descent(4/49): loss=1.6010173181832814\n",
      "Gradient Descent(5/49): loss=2.9711909314406273\n",
      "Gradient Descent(6/49): loss=5.972419213919774\n",
      "Gradient Descent(7/49): loss=12.546309643862223\n",
      "Gradient Descent(8/49): loss=26.945759241607586\n",
      "Gradient Descent(9/49): loss=58.48631364050741\n",
      "Gradient Descent(10/49): loss=127.57274399584995\n",
      "Gradient Descent(11/49): loss=278.89966104621067\n",
      "Gradient Descent(12/49): loss=610.3661401532637\n",
      "Gradient Descent(13/49): loss=1336.410315989534\n",
      "Gradient Descent(14/49): loss=2926.737478741223\n",
      "Gradient Descent(15/49): loss=6410.19009603221\n",
      "Gradient Descent(16/49): loss=14040.344708946666\n",
      "Gradient Descent(17/49): loss=30753.435373077926\n",
      "Gradient Descent(18/49): loss=67361.78916378909\n",
      "Gradient Descent(19/49): loss=147548.72730695328\n",
      "Gradient Descent(20/49): loss=323190.1966157508\n",
      "Gradient Descent(21/49): loss=707915.2709897278\n",
      "Gradient Descent(22/49): loss=1550617.073898394\n",
      "Gradient Descent(23/49): loss=3396471.102989957\n",
      "Gradient Descent(24/49): loss=7439629.768310953\n",
      "Gradient Descent(25/49): loss=16295764.508831523\n",
      "Gradient Descent(26/49): loss=35694242.0444703\n",
      "Gradient Descent(27/49): loss=78184667.23853758\n",
      "Gradient Descent(28/49): loss=171255694.583617\n",
      "Gradient Descent(29/49): loss=375118472.88031685\n",
      "Gradient Descent(30/49): loss=821659502.4613281\n",
      "Gradient Descent(31/49): loss=1799762973.6557083\n",
      "Gradient Descent(32/49): loss=3942200816.9595666\n",
      "Gradient Descent(33/49): loss=8634996668.932224\n",
      "Gradient Descent(34/49): loss=18914096703.092\n",
      "Gradient Descent(35/49): loss=41429437417.91481\n",
      "Gradient Descent(36/49): loss=90747039719.66019\n",
      "Gradient Descent(37/49): loss=198772315801.41034\n",
      "Gradient Descent(38/49): loss=435390880530.89667\n",
      "Gradient Descent(39/49): loss=953680184714.3582\n",
      "Gradient Descent(40/49): loss=2088941076597.6152\n",
      "Gradient Descent(41/49): loss=4575616534178.607\n",
      "Gradient Descent(42/49): loss=10022430456465.41\n",
      "Gradient Descent(43/49): loss=21953131671842.293\n",
      "Gradient Descent(44/49): loss=48086139614002.24\n",
      "Gradient Descent(45/49): loss=105327880210518.47\n",
      "Gradient Descent(46/49): loss=230710188813110.34\n",
      "Gradient Descent(47/49): loss=505347597576185.1\n",
      "Gradient Descent(48/49): loss=1106913377730842.6\n",
      "Gradient Descent(49/49): loss=2424583062581795.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5588339486720003\n",
      "Gradient Descent(2/49): loss=0.6877038298431649\n",
      "Gradient Descent(3/49): loss=0.9699804175604034\n",
      "Gradient Descent(4/49): loss=1.5882790552962354\n",
      "Gradient Descent(5/49): loss=2.94260039139285\n",
      "Gradient Descent(6/49): loss=5.909105845978453\n",
      "Gradient Descent(7/49): loss=12.406939393703896\n",
      "Gradient Descent(8/49): loss=26.639793996641277\n",
      "Gradient Descent(9/49): loss=57.81543871891896\n",
      "Gradient Descent(10/49): loss=126.10257091860292\n",
      "Gradient Descent(11/49): loss=275.67870528879416\n",
      "Gradient Descent(12/49): loss=603.3102700132443\n",
      "Gradient Descent(13/49): loss=1320.9544493855626\n",
      "Gradient Descent(14/49): loss=2892.882259882686\n",
      "Gradient Descent(15/49): loss=6336.032935996301\n",
      "Gradient Descent(16/49): loss=13877.910176955887\n",
      "Gradient Descent(17/49): loss=30397.638085555893\n",
      "Gradient Descent(18/49): loss=66582.45009655143\n",
      "Gradient Descent(19/49): loss=145841.662325428\n",
      "Gradient Descent(20/49): loss=319451.0407915785\n",
      "Gradient Descent(21/49): loss=699725.0233838764\n",
      "Gradient Descent(22/49): loss=1532677.154853863\n",
      "Gradient Descent(23/49): loss=3357175.503625505\n",
      "Gradient Descent(24/49): loss=7353556.686776075\n",
      "Gradient Descent(25/49): loss=16107230.030349378\n",
      "Gradient Descent(26/49): loss=35281276.12211416\n",
      "Gradient Descent(27/49): loss=77280106.68151301\n",
      "Gradient Descent(28/49): loss=169274345.13881192\n",
      "Gradient Descent(29/49): loss=370778525.05565834\n",
      "Gradient Descent(30/49): loss=812153280.7454789\n",
      "Gradient Descent(31/49): loss=1778940545.608594\n",
      "Gradient Descent(32/49): loss=3896591370.5648794\n",
      "Gradient Descent(33/49): loss=8535093737.5494375\n",
      "Gradient Descent(34/49): loss=18695269322.19018\n",
      "Gradient Descent(35/49): loss=40950117922.78835\n",
      "Gradient Descent(36/49): loss=89697138297.52806\n",
      "Gradient Descent(37/49): loss=196472611726.38132\n",
      "Gradient Descent(38/49): loss=430353608724.9212\n",
      "Gradient Descent(39/49): loss=942646544550.5497\n",
      "Gradient Descent(40/49): loss=2064772991182.724\n",
      "Gradient Descent(41/49): loss=4522678759885.67\n",
      "Gradient Descent(42/49): loss=9906475555653.61\n",
      "Gradient Descent(43/49): loss=21699144057102.117\n",
      "Gradient Descent(44/49): loss=47529805142676.68\n",
      "Gradient Descent(45/49): loss=104109285184514.45\n",
      "Gradient Descent(46/49): loss=228040978268146.47\n",
      "Gradient Descent(47/49): loss=499500958798582.75\n",
      "Gradient Descent(48/49): loss=1094106900152487.2\n",
      "Gradient Descent(49/49): loss=2396531754093891.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5593139789004803\n",
      "Gradient Descent(2/49): loss=0.6892353182841089\n",
      "Gradient Descent(3/49): loss=0.9738150200700464\n",
      "Gradient Descent(4/49): loss=1.5971583988619775\n",
      "Gradient Descent(5/49): loss=2.9625297357677356\n",
      "Gradient Descent(6/49): loss=5.953239112125824\n",
      "Gradient Descent(7/49): loss=12.50408893010057\n",
      "Gradient Descent(8/49): loss=26.853070371390412\n",
      "Gradient Descent(9/49): loss=58.28307932039882\n",
      "Gradient Descent(10/49): loss=127.12737092231504\n",
      "Gradient Descent(11/49): loss=277.9239072471704\n",
      "Gradient Descent(12/49): loss=608.2286404130753\n",
      "Gradient Descent(13/49): loss=1331.7281279398242\n",
      "Gradient Descent(14/49): loss=2916.4814054182852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=6387.724984406314\n",
      "Gradient Descent(16/49): loss=13991.136919822915\n",
      "Gradient Descent(17/49): loss=30645.6504231621\n",
      "Gradient Descent(18/49): loss=67125.69680087263\n",
      "Gradient Descent(19/49): loss=147031.59038660175\n",
      "Gradient Descent(20/49): loss=322057.4596968036\n",
      "Gradient Descent(21/49): loss=705434.1238338193\n",
      "Gradient Descent(22/49): loss=1545182.3689594525\n",
      "Gradient Descent(23/49): loss=3384566.9250823134\n",
      "Gradient Descent(24/49): loss=7413554.856813828\n",
      "Gradient Descent(25/49): loss=16238650.022480091\n",
      "Gradient Descent(26/49): loss=35569138.473351955\n",
      "Gradient Descent(27/49): loss=77910640.37614912\n",
      "Gradient Descent(28/49): loss=170655466.1440108\n",
      "Gradient Descent(29/49): loss=373803732.5059617\n",
      "Gradient Descent(30/49): loss=818779695.145108\n",
      "Gradient Descent(31/49): loss=1793455043.709899\n",
      "Gradient Descent(32/49): loss=3928383927.2058334\n",
      "Gradient Descent(33/49): loss=8604732153.614998\n",
      "Gradient Descent(34/49): loss=18847805308.74309\n",
      "Gradient Descent(35/49): loss=41284232747.732216\n",
      "Gradient Descent(36/49): loss=90428983410.10541\n",
      "Gradient Descent(37/49): loss=198075645260.96906\n",
      "Gradient Descent(38/49): loss=433864893379.0621\n",
      "Gradient Descent(39/49): loss=950337662456.9603\n",
      "Gradient Descent(40/49): loss=2081619615845.306\n",
      "Gradient Descent(41/49): loss=4559579606547.301\n",
      "Gradient Descent(42/49): loss=9987303170180.232\n",
      "Gradient Descent(43/49): loss=21876188863960.402\n",
      "Gradient Descent(44/49): loss=47917604087616.18\n",
      "Gradient Descent(45/49): loss=104958719993522.14\n",
      "Gradient Descent(46/49): loss=229901580273820.72\n",
      "Gradient Descent(47/49): loss=503576421431725.2\n",
      "Gradient Descent(48/49): loss=1103033793504059.0\n",
      "Gradient Descent(49/49): loss=2416085221291150.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5624414273008301\n",
      "Gradient Descent(2/49): loss=0.7048141256894531\n",
      "Gradient Descent(3/49): loss=1.029438115285326\n",
      "Gradient Descent(4/49): loss=1.769613273962929\n",
      "Gradient Descent(5/49): loss=3.457286653263621\n",
      "Gradient Descent(6/49): loss=7.305350725406602\n",
      "Gradient Descent(7/49): loss=16.079321616300522\n",
      "Gradient Descent(8/49): loss=36.08485264462387\n",
      "Gradient Descent(9/49): loss=81.69946394231224\n",
      "Gradient Descent(10/49): loss=185.70533916217263\n",
      "Gradient Descent(11/49): loss=422.8491352509988\n",
      "Gradient Descent(12/49): loss=963.560704713061\n",
      "Gradient Descent(13/49): loss=2196.4371542437675\n",
      "Gradient Descent(14/49): loss=5007.518746818296\n",
      "Gradient Descent(15/49): loss=11417.065886046608\n",
      "Gradient Descent(16/49): loss=26031.47431820377\n",
      "Gradient Descent(17/49): loss=59353.78698436409\n",
      "Gradient Descent(18/49): loss=135331.99209448686\n",
      "Gradient Descent(19/49): loss=308569.8975660955\n",
      "Gradient Descent(20/49): loss=703569.6458318518\n",
      "Gradient Descent(21/49): loss=1604208.5718528004\n",
      "Gradient Descent(22/49): loss=3657755.3870731373\n",
      "Gradient Descent(23/49): loss=8340047.480457365\n",
      "Gradient Descent(24/49): loss=19016141.68258413\n",
      "Gradient Descent(25/49): loss=43358704.07285196\n",
      "Gradient Descent(26/49): loss=98862180.57890642\n",
      "Gradient Descent(27/49): loss=225415657.360383\n",
      "Gradient Descent(28/49): loss=513970239.7697661\n",
      "Gradient Descent(29/49): loss=1171903543.1215284\n",
      "Gradient Descent(30/49): loss=2672057268.0936885\n",
      "Gradient Descent(31/49): loss=6092557776.40234\n",
      "Gradient Descent(32/49): loss=13891640985.397402\n",
      "Gradient Descent(33/49): loss=31674330610.2284\n",
      "Gradient Descent(34/49): loss=72220641223.8048\n",
      "Gradient Descent(35/49): loss=164670284053.82312\n",
      "Gradient Descent(36/49): loss=375464714670.5207\n",
      "Gradient Descent(37/49): loss=856097095919.6951\n",
      "Gradient Descent(38/49): loss=1951986988406.0625\n",
      "Gradient Descent(39/49): loss=4450725532263.926\n",
      "Gradient Descent(40/49): loss=10148099286114.354\n",
      "Gradient Descent(41/49): loss=23138681182270.184\n",
      "Gradient Descent(42/49): loss=52758506963699.06\n",
      "Gradient Descent(43/49): loss=120294671727921.42\n",
      "Gradient Descent(44/49): loss=274283881006823.28\n",
      "Gradient Descent(45/49): loss=625394677083636.4\n",
      "Gradient Descent(46/49): loss=1425962403218489.5\n",
      "Gradient Descent(47/49): loss=3251336875578598.5\n",
      "Gradient Descent(48/49): loss=7413373210007265.0\n",
      "Gradient Descent(49/49): loss=1.6903232256135792e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5640077934353578\n",
      "Gradient Descent(2/49): loss=0.7099519632473397\n",
      "Gradient Descent(3/49): loss=1.0427192648356414\n",
      "Gradient Descent(4/49): loss=1.8014619891871562\n",
      "Gradient Descent(5/49): loss=3.531471274981227\n",
      "Gradient Descent(6/49): loss=7.4760654475195585\n",
      "Gradient Descent(7/49): loss=16.470134620325304\n",
      "Gradient Descent(8/49): loss=36.97751174123774\n",
      "Gradient Descent(9/49): loss=83.73638231462984\n",
      "Gradient Descent(10/49): loss=190.35128310901368\n",
      "Gradient Descent(11/49): loss=433.44391841025265\n",
      "Gradient Descent(12/49): loss=987.719436160639\n",
      "Gradient Descent(13/49): loss=2251.5230441830486\n",
      "Gradient Descent(14/49): loss=5133.121650835673\n",
      "Gradient Descent(15/49): loss=11703.45463386355\n",
      "Gradient Descent(16/49): loss=26684.470868468394\n",
      "Gradient Descent(17/49): loss=60842.68598499115\n",
      "Gradient Descent(18/49): loss=138726.8322721567\n",
      "Gradient Descent(19/49): loss=316310.47422151273\n",
      "Gradient Descent(20/49): loss=721218.9362302736\n",
      "Gradient Descent(21/49): loss=1644450.7204565362\n",
      "Gradient Descent(22/49): loss=3749511.511670817\n",
      "Gradient Descent(23/49): loss=8549260.621718898\n",
      "Gradient Descent(24/49): loss=19493168.567540083\n",
      "Gradient Descent(25/49): loss=44446373.074804276\n",
      "Gradient Descent(26/49): loss=101342174.67182375\n",
      "Gradient Descent(27/49): loss=231070291.8931775\n",
      "Gradient Descent(28/49): loss=526863371.9696115\n",
      "Gradient Descent(29/49): loss=1201301173.8519256\n",
      "Gradient Descent(30/49): loss=2739086805.92392\n",
      "Gradient Descent(31/49): loss=6245391825.610529\n",
      "Gradient Descent(32/49): loss=14240117900.998009\n",
      "Gradient Descent(33/49): loss=32468892825.49243\n",
      "Gradient Descent(34/49): loss=74032322530.82718\n",
      "Gradient Descent(35/49): loss=168801098601.97363\n",
      "Gradient Descent(36/49): loss=384883384921.8158\n",
      "Gradient Descent(37/49): loss=877572605959.6578\n",
      "Gradient Descent(38/49): loss=2000953298847.8716\n",
      "Gradient Descent(39/49): loss=4562373616702.44\n",
      "Gradient Descent(40/49): loss=10402668083442.836\n",
      "Gradient Descent(41/49): loss=23719123497055.33\n",
      "Gradient Descent(42/49): loss=54081973485633.5\n",
      "Gradient Descent(43/49): loss=123312307744585.89\n",
      "Gradient Descent(44/49): loss=281164392888403.28\n",
      "Gradient Descent(45/49): loss=641082932224824.0\n",
      "Gradient Descent(46/49): loss=1461733193765973.8\n",
      "Gradient Descent(47/49): loss=3332897855105814.0\n",
      "Gradient Descent(48/49): loss=7599340399427647.0\n",
      "Gradient Descent(49/49): loss=1.73272560447349e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5632672527680002\n",
      "Gradient Descent(2/49): loss=0.7075229158043139\n",
      "Gradient Descent(3/49): loss=1.0364402530934649\n",
      "Gradient Descent(4/49): loss=1.7864046738465142\n",
      "Gradient Descent(5/49): loss=3.4963985496052565\n",
      "Gradient Descent(6/49): loss=7.395355585723356\n",
      "Gradient Descent(7/49): loss=16.285367523776138\n",
      "Gradient Descent(8/49): loss=36.55548374373122\n",
      "Gradient Descent(9/49): loss=82.77337573685722\n",
      "Gradient Descent(10/49): loss=188.1547912703596\n",
      "Gradient Descent(11/49): loss=428.4349568283407\n",
      "Gradient Descent(12/49): loss=976.2977623171404\n",
      "Gradient Descent(13/49): loss=2225.479745111857\n",
      "Gradient Descent(14/49): loss=5073.739584082164\n",
      "Gradient Descent(15/49): loss=11568.056842918737\n",
      "Gradient Descent(16/49): loss=26375.749624790787\n",
      "Gradient Descent(17/49): loss=60138.76993673604\n",
      "Gradient Descent(18/49): loss=137121.83254998844\n",
      "Gradient Descent(19/49): loss=312650.91361446807\n",
      "Gradient Descent(20/49): loss=712874.7713495715\n",
      "Gradient Descent(21/49): loss=1625425.1893713148\n",
      "Gradient Descent(22/49): loss=3706131.3975032335\n",
      "Gradient Descent(23/49): loss=8450349.622663852\n",
      "Gradient Descent(24/49): loss=19267641.59785496\n",
      "Gradient Descent(25/49): loss=43932149.03048588\n",
      "Gradient Descent(26/49): loss=100169692.42762029\n",
      "Gradient Descent(27/49): loss=228396915.1274377\n",
      "Gradient Descent(28/49): loss=520767805.605264\n",
      "Gradient Descent(29/49): loss=1187402672.9837396\n",
      "Gradient Descent(30/49): loss=2707396834.093575\n",
      "Gradient Descent(31/49): loss=6173135520.840289\n",
      "Gradient Descent(32/49): loss=14075366300.490108\n",
      "Gradient Descent(33/49): loss=32093242701.170715\n",
      "Gradient Descent(34/49): loss=73175802682.35938\n",
      "Gradient Descent(35/49): loss=166848147695.4632\n",
      "Gradient Descent(36/49): loss=380430461559.8297\n",
      "Gradient Descent(37/49): loss=867419495402.0111\n",
      "Gradient Descent(38/49): loss=1977803191465.6948\n",
      "Gradient Descent(39/49): loss=4509589056860.594\n",
      "Gradient Descent(40/49): loss=10282314008548.096\n",
      "Gradient Descent(41/49): loss=23444704170892.117\n",
      "Gradient Descent(42/49): loss=53456269980048.75\n",
      "Gradient Descent(43/49): loss=121885641181518.5\n",
      "Gradient Descent(44/49): loss=277911450458007.8\n",
      "Gradient Descent(45/49): loss=633665898189270.6\n",
      "Gradient Descent(46/49): loss=1444821614461257.0\n",
      "Gradient Descent(47/49): loss=3294337763133344.0\n",
      "Gradient Descent(48/49): loss=7511419533720154.0\n",
      "Gradient Descent(49/49): loss=1.712678767883619e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5637834546291204\n",
      "Gradient Descent(2/49): loss=0.7092161095289691\n",
      "Gradient Descent(3/49): loss=1.040817105966083\n",
      "Gradient Descent(4/49): loss=1.7969005379422565\n",
      "Gradient Descent(5/49): loss=3.520846371191101\n",
      "Gradient Descent(6/49): loss=7.4516152655827215\n",
      "Gradient Descent(7/49): loss=16.41416142168486\n",
      "Gradient Descent(8/49): loss=36.84966291221125\n",
      "Gradient Descent(9/49): loss=83.4446498607617\n",
      "Gradient Descent(10/49): loss=189.68587960214603\n",
      "Gradient Descent(11/49): loss=431.9265075355177\n",
      "Gradient Descent(12/49): loss=984.2593632864807\n",
      "Gradient Descent(13/49): loss=2243.6335076843593\n",
      "Gradient Descent(14/49): loss=5115.132494325747\n",
      "Gradient Descent(15/49): loss=11662.43733376624\n",
      "Gradient Descent(16/49): loss=26590.947098177825\n",
      "Gradient Descent(17/49): loss=60629.44221201076\n",
      "Gradient Descent(18/49): loss=138240.61492106784\n",
      "Gradient Descent(19/49): loss=315201.849815001\n",
      "Gradient Descent(20/49): loss=718691.1614966956\n",
      "Gradient Descent(21/49): loss=1638687.141061938\n",
      "Gradient Descent(22/49): loss=3736369.974068786\n",
      "Gradient Descent(23/49): loss=8519296.601608438\n",
      "Gradient Descent(24/49): loss=19424847.605063234\n",
      "Gradient Descent(25/49): loss=44290594.44803813\n",
      "Gradient Descent(26/49): loss=100986983.8247003\n",
      "Gradient Descent(27/49): loss=230260421.2424416\n",
      "Gradient Descent(28/49): loss=525016785.89858586\n",
      "Gradient Descent(29/49): loss=1197090772.951043\n",
      "Gradient Descent(30/49): loss=2729486670.829276\n",
      "Gradient Descent(31/49): loss=6223502557.582112\n",
      "Gradient Descent(32/49): loss=14190208180.966461\n",
      "Gradient Descent(33/49): loss=32355093672.841446\n",
      "Gradient Descent(34/49): loss=73772849082.86487\n",
      "Gradient Descent(35/49): loss=168209473193.26495\n",
      "Gradient Descent(36/49): loss=383534419827.3881\n",
      "Gradient Descent(37/49): loss=874496830647.9247\n",
      "Gradient Descent(38/49): loss=1993940223559.9834\n",
      "Gradient Descent(39/49): loss=4546383103738.562\n",
      "Gradient Descent(40/49): loss=10366208114835.045\n",
      "Gradient Descent(41/49): loss=23635991122632.145\n",
      "Gradient Descent(42/49): loss=53892423358710.35\n",
      "Gradient Descent(43/49): loss=122880114500189.53\n",
      "Gradient Descent(44/49): loss=280178949071895.12\n",
      "Gradient Descent(45/49): loss=638836021778826.4\n",
      "Gradient Descent(46/49): loss=1456610013258044.5\n",
      "Gradient Descent(47/49): loss=3321216491229931.5\n",
      "Gradient Descent(48/49): loss=7572705721654314.0\n",
      "Gradient Descent(49/49): loss=1.7266526315942718e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5669046650150913\n",
      "Gradient Descent(2/49): loss=0.7255757685648694\n",
      "Gradient Descent(3/49): loss=1.1018801577434814\n",
      "Gradient Descent(4/49): loss=1.9943236471194992\n",
      "Gradient Descent(5/49): loss=4.1108426265240015\n",
      "Gradient Descent(6/49): loss=9.130379038079955\n",
      "Gradient Descent(7/49): loss=21.034711591726065\n",
      "Gradient Descent(8/49): loss=49.26702667595684\n",
      "Gradient Descent(9/49): loss=116.22278512970672\n",
      "Gradient Descent(10/49): loss=275.0150618786408\n",
      "Gradient Descent(11/49): loss=651.6068254164455\n",
      "Gradient Descent(12/49): loss=1544.731851822779\n",
      "Gradient Descent(13/49): loss=3662.867164447848\n",
      "Gradient Descent(14/49): loss=8686.236871870058\n",
      "Gradient Descent(15/49): loss=20599.660469992938\n",
      "Gradient Descent(16/49): loss=48853.53587530253\n",
      "Gradient Descent(17/49): loss=115860.4267865372\n",
      "Gradient Descent(18/49): loss=274773.96927164437\n",
      "Gradient Descent(19/49): loss=651653.3266293254\n",
      "Gradient Descent(20/49): loss=1545460.410538688\n",
      "Gradient Descent(21/49): loss=3665213.290738337\n",
      "Gradient Descent(22/49): loss=8692419.221418705\n",
      "Gradient Descent(23/49): loss=20614940.806622025\n",
      "Gradient Descent(24/49): loss=48890392.99809349\n",
      "Gradient Descent(25/49): loss=115948455.41537377\n",
      "Gradient Descent(26/49): loss=274983356.24423313\n",
      "Gradient Descent(27/49): loss=652150527.0499884\n",
      "Gradient Descent(28/49): loss=1546640189.33295\n",
      "Gradient Descent(29/49): loss=3668011872.403012\n",
      "Gradient Descent(30/49): loss=8699056955.971676\n",
      "Gradient Descent(31/49): loss=20630683476.164845\n",
      "Gradient Descent(32/49): loss=48927728931.45837\n",
      "Gradient Descent(33/49): loss=116037001933.23445\n",
      "Gradient Descent(34/49): loss=275193353784.2595\n",
      "Gradient Descent(35/49): loss=652648557834.1891\n",
      "Gradient Descent(36/49): loss=1547821319758.906\n",
      "Gradient Descent(37/49): loss=3670813041939.309\n",
      "Gradient Descent(38/49): loss=8705700210261.754\n",
      "Gradient Descent(39/49): loss=20646438618654.18\n",
      "Gradient Descent(40/49): loss=48965093828000.04\n",
      "Gradient Descent(41/49): loss=116125616522488.7\n",
      "Gradient Descent(42/49): loss=275403512144737.06\n",
      "Gradient Descent(43/49): loss=653146969402418.2\n",
      "Gradient Descent(44/49): loss=1549003352634909.8\n",
      "Gradient Descent(45/49): loss=3673616351109447.5\n",
      "Gradient Descent(46/49): loss=8712348538290374.0\n",
      "Gradient Descent(47/49): loss=2.066220579340963e+16\n",
      "Gradient Descent(48/49): loss=4.900248725965185e+16\n",
      "Gradient Descent(49/49): loss=1.1621429878497664e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5685829931067395\n",
      "Gradient Descent(2/49): loss=0.7312344195586514\n",
      "Gradient Descent(3/49): loss=1.116978542532077\n",
      "Gradient Descent(4/49): loss=2.031809304575967\n",
      "Gradient Descent(5/49): loss=4.201421939839134\n",
      "Gradient Descent(6/49): loss=9.346875265630102\n",
      "Gradient Descent(7/49): loss=21.549832373077077\n",
      "Gradient Descent(8/49): loss=50.490365449094185\n",
      "Gradient Descent(9/49): loss=119.12573369216719\n",
      "Gradient Descent(10/49): loss=281.901373017452\n",
      "Gradient Descent(11/49): loss=667.9400792413707\n",
      "Gradient Descent(12/49): loss=1583.4694749219284\n",
      "Gradient Descent(13/49): loss=3754.738989718095\n",
      "Gradient Descent(14/49): loss=8904.121771007714\n",
      "Gradient Descent(15/49): loss=21116.397975116164\n",
      "Gradient Descent(16/49): loss=50079.032220778085\n",
      "Gradient Descent(17/49): loss=118766.81559778654\n",
      "Gradient Descent(18/49): loss=281666.76265467104\n",
      "Gradient Descent(19/49): loss=668000.2770948051\n",
      "Gradient Descent(20/49): loss=1584228.8399409316\n",
      "Gradient Descent(21/49): loss=3757156.4995867526\n",
      "Gradient Descent(22/49): loss=8910471.737203054\n",
      "Gradient Descent(23/49): loss=21132074.154735778\n",
      "Gradient Descent(24/49): loss=50116826.44815667\n",
      "Gradient Descent(25/49): loss=118857064.98722368\n",
      "Gradient Descent(26/49): loss=281881414.7064843\n",
      "Gradient Descent(27/49): loss=668509962.5006835\n",
      "Gradient Descent(28/49): loss=1585438226.449468\n",
      "Gradient Descent(29/49): loss=3760025297.230758\n",
      "Gradient Descent(30/49): loss=8917275994.295769\n",
      "Gradient Descent(31/49): loss=21148211747.455456\n",
      "Gradient Descent(32/49): loss=50155098979.65063\n",
      "Gradient Descent(33/49): loss=118947832739.51128\n",
      "Gradient Descent(34/49): loss=282096680124.4183\n",
      "Gradient Descent(35/49): loss=669020486582.4131\n",
      "Gradient Descent(36/49): loss=1586648985978.2954\n",
      "Gradient Descent(37/49): loss=3762896735145.5894\n",
      "Gradient Descent(38/49): loss=8924085897071.117\n",
      "Gradient Descent(39/49): loss=21164362113492.64\n",
      "Gradient Descent(40/49): loss=50193401188359.58\n",
      "Gradient Descent(41/49): loss=119038670258322.73\n",
      "Gradient Descent(42/49): loss=282312110384608.5\n",
      "Gradient Descent(43/49): loss=669531400988112.5\n",
      "Gradient Descent(44/49): loss=1587860670583528.5\n",
      "Gradient Descent(45/49): loss=3765770366355877.5\n",
      "Gradient Descent(46/49): loss=8930901000850132.0\n",
      "Gradient Descent(47/49): loss=2.118052481361583e+16\n",
      "Gradient Descent(48/49): loss=5.023173264797545e+16\n",
      "Gradient Descent(49/49): loss=1.1912957714795062e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5677895194880004\n",
      "Gradient Descent(2/49): loss=0.7285591439057691\n",
      "Gradient Descent(3/49): loss=1.1098403851749603\n",
      "Gradient Descent(4/49): loss=2.0140869769687564\n",
      "Gradient Descent(5/49): loss=4.1585981940673165\n",
      "Gradient Descent(6/49): loss=9.24452099653754\n",
      "Gradient Descent(7/49): loss=21.306295514877718\n",
      "Gradient Descent(8/49): loss=49.91199996257282\n",
      "Gradient Descent(9/49): loss=117.7532886307358\n",
      "Gradient Descent(10/49): loss=278.64568883615675\n",
      "Gradient Descent(11/49): loss=660.2181051632674\n",
      "Gradient Descent(12/49): loss=1565.1552477248008\n",
      "Gradient Descent(13/49): loss=3711.3041750234306\n",
      "Gradient Descent(14/49): loss=8801.11097100559\n",
      "Gradient Descent(15/49): loss=20872.096768357336\n",
      "Gradient Descent(16/49): loss=49499.64668535983\n",
      "Gradient Descent(17/49): loss=117392.74406852423\n",
      "Gradient Descent(18/49): loss=278408.0138224488\n",
      "Gradient Descent(19/49): loss=660271.8275708947\n",
      "Gradient Descent(20/49): loss=1565900.048256688\n",
      "Gradient Descent(21/49): loss=3713687.9364353684\n",
      "Gradient Descent(22/49): loss=8807381.692038951\n",
      "Gradient Descent(23/49): loss=20887585.802831035\n",
      "Gradient Descent(24/49): loss=49536997.871982664\n",
      "Gradient Descent(25/49): loss=117481943.53519\n",
      "Gradient Descent(26/49): loss=278620176.67003053\n",
      "Gradient Descent(27/49): loss=660775610.3727088\n",
      "Gradient Descent(28/49): loss=1567095436.9419427\n",
      "Gradient Descent(29/49): loss=3716523537.6334467\n",
      "Gradient Descent(30/49): loss=8814107221.232594\n",
      "Gradient Descent(31/49): loss=20903536685.25558\n",
      "Gradient Descent(32/49): loss=49574827602.12933\n",
      "Gradient Descent(33/49): loss=117571661140.5798\n",
      "Gradient Descent(34/49): loss=278832951560.36316\n",
      "Gradient Descent(35/49): loss=661280227919.9448\n",
      "Gradient Descent(36/49): loss=1568292188534.258\n",
      "Gradient Descent(37/49): loss=3719361754327.3115\n",
      "Gradient Descent(38/49): loss=8820838336562.312\n",
      "Gradient Descent(39/49): loss=20919500198991.508\n",
      "Gradient Descent(40/49): loss=49612686671931.42\n",
      "Gradient Descent(41/49): loss=117661447711160.02\n",
      "Gradient Descent(42/49): loss=279045889391782.4\n",
      "Gradient Descent(43/49): loss=661785231281583.8\n",
      "Gradient Descent(44/49): loss=1569489854507476.0\n",
      "Gradient Descent(45/49): loss=3722202138949874.0\n",
      "Gradient Descent(46/49): loss=8827574592734702.0\n",
      "Gradient Descent(47/49): loss=2.093547590413145e+16\n",
      "Gradient Descent(48/49): loss=4.965057465423439e+16\n",
      "Gradient Descent(49/49): loss=1.1775130284997477e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5683426188339203\n",
      "Gradient Descent(2/49): loss=0.7304239736604629\n",
      "Gradient Descent(3/49): loss=1.114816114767085\n",
      "Gradient Descent(4/49): loss=2.0264405166154558\n",
      "Gradient Descent(5/49): loss=4.18844894803924\n",
      "Gradient Descent(6/49): loss=9.315868144003982\n",
      "Gradient Descent(7/49): loss=21.4760555091535\n",
      "Gradient Descent(8/49): loss=50.31515586434312\n",
      "Gradient Descent(9/49): loss=118.70996626669604\n",
      "Gradient Descent(10/49): loss=280.91509861695033\n",
      "Gradient Descent(11/49): loss=665.6007904988429\n",
      "Gradient Descent(12/49): loss=1577.9213773660663\n",
      "Gradient Descent(13/49): loss=3741.5808811801917\n",
      "Gradient Descent(14/49): loss=8872.915760426475\n",
      "Gradient Descent(15/49): loss=21042.38956004518\n",
      "Gradient Descent(16/49): loss=49903.51362322704\n",
      "Gradient Descent(17/49): loss=118350.55545147172\n",
      "Gradient Descent(18/49): loss=280679.559851353\n",
      "Gradient Descent(19/49): loss=665659.0266861066\n",
      "Gradient Descent(20/49): loss=1578676.3302313455\n",
      "Gradient Descent(21/49): loss=3743988.1673190584\n",
      "Gradient Descent(22/49): loss=8879241.720156146\n",
      "Gradient Descent(23/49): loss=21058009.046066567\n",
      "Gradient Descent(24/49): loss=49941173.6361965\n",
      "Gradient Descent(25/49): loss=118440486.77816081\n",
      "Gradient Descent(26/49): loss=280893457.82563525\n",
      "Gradient Descent(27/49): loss=666166923.9618111\n",
      "Gradient Descent(28/49): loss=1579881476.2502697\n",
      "Gradient Descent(29/49): loss=3746846908.4579067\n",
      "Gradient Descent(30/49): loss=8886022127.482101\n",
      "Gradient Descent(31/49): loss=21074090076.917953\n",
      "Gradient Descent(32/49): loss=49979312025.7997\n",
      "Gradient Descent(33/49): loss=118530936399.78279\n",
      "Gradient Descent(34/49): loss=281107968765.0951\n",
      "Gradient Descent(35/49): loss=666675658722.7428\n",
      "Gradient Descent(36/49): loss=1581087992226.1323\n",
      "Gradient Descent(37/49): loss=3749708282362.783\n",
      "Gradient Descent(38/49): loss=8892808162449.844\n",
      "Gradient Descent(39/49): loss=21090183838066.56\n",
      "Gradient Descent(40/49): loss=50017479990356.18\n",
      "Gradient Descent(41/49): loss=118621455545127.73\n",
      "Gradient Descent(42/49): loss=281322643970829.06\n",
      "Gradient Descent(43/49): loss=667184782441244.6\n",
      "Gradient Descent(44/49): loss=1582295430037566.2\n",
      "Gradient Descent(45/49): loss=3752571841877160.5\n",
      "Gradient Descent(46/49): loss=8899599380196636.0\n",
      "Gradient Descent(47/49): loss=2.110628989007295e+16\n",
      "Gradient Descent(48/49): loss=5.005567710330095e+16\n",
      "Gradient Descent(49/49): loss=1.1871204381818502e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5714557041270105\n",
      "Gradient Descent(2/49): loss=0.7475868692297009\n",
      "Gradient Descent(3/49): loss=1.1817325780912982\n",
      "Gradient Descent(4/49): loss=2.251858335864205\n",
      "Gradient Descent(5/49): loss=4.889611316199193\n",
      "Gradient Descent(6/49): loss=11.39140863742611\n",
      "Gradient Descent(7/49): loss=27.417688854520144\n",
      "Gradient Descent(8/49): loss=66.92086696163126\n",
      "Gradient Descent(9/49): loss=164.29225067785717\n",
      "Gradient Descent(10/49): loss=404.3029743999506\n",
      "Gradient Descent(11/49): loss=995.9054073026722\n",
      "Gradient Descent(12/49): loss=2454.1462441643716\n",
      "Gradient Descent(13/49): loss=6048.56408294453\n",
      "Gradient Descent(14/49): loss=14908.444613754511\n",
      "Gradient Descent(15/49): loss=36747.16413414964\n",
      "Gradient Descent(16/49): loss=90577.42387996876\n",
      "Gradient Descent(17/49): loss=223263.63112745841\n",
      "Gradient Descent(18/49): loss=550321.8633717931\n",
      "Gradient Descent(19/49): loss=1356487.700030995\n",
      "Gradient Descent(20/49): loss=3343605.870812183\n",
      "Gradient Descent(21/49): loss=8241653.449970985\n",
      "Gradient Descent(22/49): loss=20314850.927841634\n",
      "Gradient Descent(23/49): loss=50074075.39104252\n",
      "Gradient Descent(24/49): loss=123427587.7703835\n",
      "Gradient Descent(25/49): loss=304236660.4342037\n",
      "Gradient Descent(26/49): loss=749912943.643253\n",
      "Gradient Descent(27/49): loss=1848460414.125455\n",
      "Gradient Descent(28/49): loss=4556270074.116858\n",
      "Gradient Descent(29/49): loss=11230750105.03094\n",
      "Gradient Descent(30/49): loss=27682675933.232166\n",
      "Gradient Descent(31/49): loss=68235027907.15559\n",
      "Gradient Descent(32/49): loss=168192520287.70068\n",
      "Gradient Descent(33/49): loss=414577743256.5164\n",
      "Gradient Descent(34/49): loss=1021892679352.3452\n",
      "Gradient Descent(35/49): loss=2518863265335.191\n",
      "Gradient Descent(36/49): loss=6208746062723.789\n",
      "Gradient Descent(37/49): loss=15303938170007.94\n",
      "Gradient Descent(38/49): loss=37722677195253.32\n",
      "Gradient Descent(39/49): loss=92982627018573.75\n",
      "Gradient Descent(40/49): loss=229192877338094.44\n",
      "Gradient Descent(41/49): loss=564937523350656.8\n",
      "Gradient Descent(42/49): loss=1392514501307112.0\n",
      "Gradient Descent(43/49): loss=3432408994271942.5\n",
      "Gradient Descent(44/49): loss=8460544929981555.0\n",
      "Gradient Descent(45/49): loss=2.0854397197911924e+16\n",
      "Gradient Descent(46/49): loss=5.1404003653135384e+16\n",
      "Gradient Descent(47/49): loss=1.2670572860461984e+17\n",
      "Gradient Descent(48/49): loss=3.123169504375176e+17\n",
      "Gradient Descent(49/49): loss=7.698300511334545e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5732481967060827\n",
      "Gradient Descent(2/49): loss=0.7537976767669222\n",
      "Gradient Descent(3/49): loss=1.1988340901687786\n",
      "Gradient Descent(4/49): loss=2.2958043455628903\n",
      "Gradient Descent(5/49): loss=4.999726328084193\n",
      "Gradient Descent(6/49): loss=11.664623622801338\n",
      "Gradient Descent(7/49): loss=28.0929289645465\n",
      "Gradient Descent(8/49): loss=68.5870588014112\n",
      "Gradient Descent(9/49): loss=168.40103943631095\n",
      "Gradient Descent(10/49): loss=414.43252030330365\n",
      "Gradient Descent(11/49): loss=1020.8755174922554\n",
      "Gradient Descent(12/49): loss=2515.6968612636724\n",
      "Gradient Descent(13/49): loss=6200.281991525641\n",
      "Gradient Descent(14/49): loss=15282.415879107228\n",
      "Gradient Descent(15/49): loss=37668.967698610046\n",
      "Gradient Descent(16/49): loss=92849.57927849446\n",
      "Gradient Descent(17/49): loss=228864.2687617569\n",
      "Gradient Descent(18/49): loss=564126.8768690382\n",
      "Gradient Descent(19/49): loss=1390515.679592661\n",
      "Gradient Descent(20/49): loss=3427481.439426133\n",
      "Gradient Descent(21/49): loss=8448398.34083924\n",
      "Gradient Descent(22/49): loss=20824456.4111337\n",
      "Gradient Descent(23/49): loss=51330201.94860351\n",
      "Gradient Descent(24/49): loss=126523814.1239038\n",
      "Gradient Descent(25/49): loss=311868548.774821\n",
      "Gradient Descent(26/49): loss=768724785.2158006\n",
      "Gradient Descent(27/49): loss=1894829722.419421\n",
      "Gradient Descent(28/49): loss=4670565782.13272\n",
      "Gradient Descent(29/49): loss=11512477595.720478\n",
      "Gradient Descent(30/49): loss=28377106025.033295\n",
      "Gradient Descent(31/49): loss=69946728640.4443\n",
      "Gradient Descent(32/49): loss=172411691425.19125\n",
      "Gradient Descent(33/49): loss=424977578193.28827\n",
      "Gradient Descent(34/49): loss=1047527232488.0183\n",
      "Gradient Descent(35/49): loss=2582049875359.0835\n",
      "Gradient Descent(36/49): loss=6364494737771.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=15687843079133.105\n",
      "Gradient Descent(38/49): loss=38668964405750.8\n",
      "Gradient Descent(39/49): loss=95315130363738.5\n",
      "Gradient Descent(40/49): loss=234942264833578.06\n",
      "Gradient Descent(41/49): loss=579109188588338.4\n",
      "Gradient Descent(42/49): loss=1427446238951474.2\n",
      "Gradient Descent(43/49): loss=3518512234391581.5\n",
      "Gradient Descent(44/49): loss=8672780806552214.0\n",
      "Gradient Descent(45/49): loss=2.1377537410072364e+16\n",
      "Gradient Descent(46/49): loss=5.269349196208308e+16\n",
      "Gradient Descent(47/49): loss=1.2988418833735037e+17\n",
      "Gradient Descent(48/49): loss=3.201515358327656e+17\n",
      "Gradient Descent(49/49): loss=7.89141520674275e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5724007488320001\n",
      "Gradient Descent(2/49): loss=0.7508613546279626\n",
      "Gradient Descent(3/49): loss=1.190748901854493\n",
      "Gradient Descent(4/49): loss=2.2750277170129953\n",
      "Gradient Descent(5/49): loss=4.947666568497275\n",
      "Gradient Descent(6/49): loss=11.535454073521148\n",
      "Gradient Descent(7/49): loss=27.773691494655367\n",
      "Gradient Descent(8/49): loss=67.79932291401025\n",
      "Gradient Descent(9/49): loss=166.45850179958884\n",
      "Gradient Descent(10/49): loss=409.6435118346419\n",
      "Gradient Descent(11/49): loss=1009.0702430701173\n",
      "Gradient Descent(12/49): loss=2486.5971928925733\n",
      "Gradient Descent(13/49): loss=6128.553371509557\n",
      "Gradient Descent(14/49): loss=15105.611156182773\n",
      "Gradient Descent(15/49): loss=37233.16088962096\n",
      "Gradient Descent(16/49): loss=91775.35822757777\n",
      "Gradient Descent(17/49): loss=226216.4204459116\n",
      "Gradient Descent(18/49): loss=557600.1947078279\n",
      "Gradient Descent(19/49): loss=1374428.0598860232\n",
      "Gradient Descent(20/49): loss=3387827.064763984\n",
      "Gradient Descent(21/49): loss=8350654.271888256\n",
      "Gradient Descent(22/49): loss=20583527.054730065\n",
      "Gradient Descent(23/49): loss=50736335.177153476\n",
      "Gradient Descent(24/49): loss=125059991.9181256\n",
      "Gradient Descent(25/49): loss=308260373.4189651\n",
      "Gradient Descent(26/49): loss=759830993.7803915\n",
      "Gradient Descent(27/49): loss=1872907415.9090617\n",
      "Gradient Descent(28/49): loss=4616529488.814373\n",
      "Gradient Descent(29/49): loss=11379283536.317669\n",
      "Gradient Descent(30/49): loss=28048795988.00764\n",
      "Gradient Descent(31/49): loss=69137477230.17769\n",
      "Gradient Descent(32/49): loss=170416967623.9992\n",
      "Gradient Descent(33/49): loss=420060783495.68964\n",
      "Gradient Descent(34/49): loss=1035407825237.9977\n",
      "Gradient Descent(35/49): loss=2552176748428.617\n",
      "Gradient Descent(36/49): loss=6290860467200.748\n",
      "Gradient Descent(37/49): loss=15506341965602.85\n",
      "Gradient Descent(38/49): loss=38221582311016.195\n",
      "Gradient Descent(39/49): loss=94212378238427.55\n",
      "Gradient Descent(40/49): loss=232224091119921.66\n",
      "Gradient Descent(41/49): loss=572409162201474.8\n",
      "Gradient Descent(42/49): loss=1410931343910572.0\n",
      "Gradient Descent(43/49): loss=3477804669605066.5\n",
      "Gradient Descent(44/49): loss=8572440730110129.0\n",
      "Gradient Descent(45/49): loss=2.113020915565024e+16\n",
      "Gradient Descent(46/49): loss=5.208385254776272e+16\n",
      "Gradient Descent(47/49): loss=1.2838148814497066e+17\n",
      "Gradient Descent(48/49): loss=3.164475301285444e+17\n",
      "Gradient Descent(49/49): loss=7.800115170137664e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5729914715148802\n",
      "Gradient Descent(2/49): loss=0.7529081496518903\n",
      "Gradient Descent(3/49): loss=1.1963847695917853\n",
      "Gradient Descent(4/49): loss=2.289510290081492\n",
      "Gradient Descent(5/49): loss=4.983955385536217\n",
      "Gradient Descent(6/49): loss=11.625493101322819\n",
      "Gradient Descent(7/49): loss=27.99621941696651\n",
      "Gradient Descent(8/49): loss=68.3484227123979\n",
      "Gradient Descent(9/49): loss=167.81256861532435\n",
      "Gradient Descent(10/49): loss=412.98174185140147\n",
      "Gradient Descent(11/49): loss=1017.2992369609809\n",
      "Gradient Descent(12/49): loss=2506.88143065685\n",
      "Gradient Descent(13/49): loss=6178.552579897973\n",
      "Gradient Descent(14/49): loss=15228.854795661106\n",
      "Gradient Descent(15/49): loss=37536.94472729192\n",
      "Gradient Descent(16/49): loss=92524.15559977182\n",
      "Gradient Descent(17/49): loss=228062.1316793721\n",
      "Gradient Descent(18/49): loss=562149.6889178953\n",
      "Gradient Descent(19/49): loss=1385642.1087553499\n",
      "Gradient Descent(20/49): loss=3415468.5744123184\n",
      "Gradient Descent(21/49): loss=8418787.829609593\n",
      "Gradient Descent(22/49): loss=20751469.461747907\n",
      "Gradient Descent(23/49): loss=51150296.416802414\n",
      "Gradient Descent(24/49): loss=126080364.97832215\n",
      "Gradient Descent(25/49): loss=310775490.97562724\n",
      "Gradient Descent(26/49): loss=766030507.0464033\n",
      "Gradient Descent(27/49): loss=1888188596.159273\n",
      "Gradient Descent(28/49): loss=4654196070.013506\n",
      "Gradient Descent(29/49): loss=11472127892.315847\n",
      "Gradient Descent(30/49): loss=28277648041.10893\n",
      "Gradient Descent(31/49): loss=69701574655.87628\n",
      "Gradient Descent(32/49): loss=171807411368.59445\n",
      "Gradient Descent(33/49): loss=423488088281.7684\n",
      "Gradient Descent(34/49): loss=1043855788804.9889\n",
      "Gradient Descent(35/49): loss=2573000133824.782\n",
      "Gradient Descent(36/49): loss=6342188029864.321\n",
      "Gradient Descent(37/49): loss=15632859274811.01\n",
      "Gradient Descent(38/49): loss=38533434826478.445\n",
      "Gradient Descent(39/49): loss=94981063503780.45\n",
      "Gradient Descent(40/49): loss=234118823430488.9\n",
      "Gradient Descent(41/49): loss=577079487873791.0\n",
      "Gradient Descent(42/49): loss=1422443229660065.0\n",
      "Gradient Descent(43/49): loss=3506180316789089.5\n",
      "Gradient Descent(44/49): loss=8642383862852946.0\n",
      "Gradient Descent(45/49): loss=2.1302611983544868e+16\n",
      "Gradient Descent(46/49): loss=5.25088082782441e+16\n",
      "Gradient Descent(47/49): loss=1.2942896152503502e+17\n",
      "Gradient Descent(48/49): loss=3.1902944726305696e+17\n",
      "Gradient Descent(49/49): loss=7.863756845586993e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5760945446365872\n",
      "Gradient Descent(2/49): loss=0.7708965789062135\n",
      "Gradient Descent(3/49): loss=1.2695897866365007\n",
      "Gradient Descent(4/49): loss=2.5462443984262397\n",
      "Gradient Descent(5/49): loss=5.814480204608161\n",
      "Gradient Descent(6/49): loss=14.181163868434114\n",
      "Gradient Descent(7/49): loss=35.59987404782347\n",
      "Gradient Descent(8/49): loss=90.43177210705541\n",
      "Gradient Descent(9/49): loss=230.80143113869607\n",
      "Gradient Descent(10/49): loss=590.147758259695\n",
      "Gradient Descent(11/49): loss=1510.074355689363\n",
      "Gradient Descent(12/49): loss=3865.0864451094785\n",
      "Gradient Descent(13/49): loss=9893.917394024103\n",
      "Gradient Descent(14/49): loss=25327.724623244492\n",
      "Gradient Descent(15/49): loss=64838.271130046436\n",
      "Gradient Descent(16/49): loss=165985.27018745872\n",
      "Gradient Descent(17/49): loss=424921.5877744157\n",
      "Gradient Descent(18/49): loss=1087798.5607971628\n",
      "Gradient Descent(19/49): loss=2784763.611735053\n",
      "Gradient Descent(20/49): loss=7128994.142136637\n",
      "Gradient Descent(21/49): loss=18250224.299963783\n",
      "Gradient Descent(22/49): loss=46720573.50400544\n",
      "Gradient Descent(23/49): loss=119604667.46635807\n",
      "Gradient Descent(24/49): loss=306187948.0099373\n",
      "Gradient Descent(25/49): loss=783841146.2015995\n",
      "Gradient Descent(26/49): loss=2006633333.5723987\n",
      "Gradient Descent(27/49): loss=5136981333.241845\n",
      "Gradient Descent(28/49): loss=13150672212.396032\n",
      "Gradient Descent(29/49): loss=33665720863.030838\n",
      "Gradient Descent(30/49): loss=86184245408.64818\n",
      "Gradient Descent(31/49): loss=220631668245.4533\n",
      "Gradient Descent(32/49): loss=564817070707.6761\n",
      "Gradient Descent(33/49): loss=1445931701011.0579\n",
      "Gradient Descent(34/49): loss=3701585154587.516\n",
      "Gradient Descent(35/49): loss=9476057995743.664\n",
      "Gradient Descent(36/49): loss=24258708469103.883\n",
      "Gradient Descent(37/49): loss=62102293680904.086\n",
      "Gradient Descent(38/49): loss=158981871823127.34\n",
      "Gradient Descent(39/49): loss=406993591867175.94\n",
      "Gradient Descent(40/49): loss=1041903595179965.9\n",
      "Gradient Descent(41/49): loss=2667273203660553.0\n",
      "Gradient Descent(42/49): loss=6828219401370578.0\n",
      "Gradient Descent(43/49): loss=1.7480241667508376e+16\n",
      "Gradient Descent(44/49): loss=4.474941866881851e+16\n",
      "Gradient Descent(45/49): loss=1.145585117921866e+17\n",
      "Gradient Descent(46/49): loss=2.932697901879875e+17\n",
      "Gradient Descent(47/49): loss=7.507706628812115e+17\n",
      "Gradient Descent(48/49): loss=1.9219728969758195e+18\n",
      "Gradient Descent(49/49): loss=4.920250616257943e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5780034042333869\n",
      "Gradient Descent(2/49): loss=0.7776921190708491\n",
      "Gradient Descent(3/49): loss=1.2888952290547733\n",
      "Gradient Descent(4/49): loss=2.597575190613633\n",
      "Gradient Descent(5/49): loss=5.947795892204927\n",
      "Gradient Descent(6/49): loss=14.524360888278679\n",
      "Gradient Descent(7/49): loss=36.48036727822256\n",
      "Gradient Descent(8/49): loss=92.68774363648639\n",
      "Gradient Descent(9/49): loss=236.57862711363973\n",
      "Gradient Descent(10/49): loss=604.9392888150994\n",
      "Gradient Descent(11/49): loss=1547.9425827710477\n",
      "Gradient Descent(12/49): loss=3962.031015298322\n",
      "Gradient Descent(13/49): loss=10142.097402567277\n",
      "Gradient Descent(14/49): loss=25963.067353977276\n",
      "Gradient Descent(15/49): loss=66464.75042959224\n",
      "Gradient Descent(16/49): loss=170149.0591031501\n",
      "Gradient Descent(17/49): loss=435580.88930743776\n",
      "Gradient Descent(18/49): loss=1115086.374630363\n",
      "Gradient Descent(19/49): loss=2854620.4170568376\n",
      "Gradient Descent(20/49): loss=7307827.565668249\n",
      "Gradient Descent(21/49): loss=18708037.866114467\n",
      "Gradient Descent(22/49): loss=47892576.235259034\n",
      "Gradient Descent(23/49): loss=122604994.46027455\n",
      "Gradient Descent(24/49): loss=313868785.1162989\n",
      "Gradient Descent(25/49): loss=803504089.1958187\n",
      "Gradient Descent(26/49): loss=2056970467.6393263\n",
      "Gradient Descent(27/49): loss=5265844396.454467\n",
      "Gradient Descent(28/49): loss=13480561654.222282\n",
      "Gradient Descent(29/49): loss=34510237834.10826\n",
      "Gradient Descent(30/49): loss=88346208854.62218\n",
      "Gradient Descent(31/49): loss=226166294667.11914\n",
      "Gradient Descent(32/49): loss=578985714347.1841\n",
      "Gradient Descent(33/49): loss=1482203428728.192\n",
      "Gradient Descent(34/49): loss=3794440777543.688\n",
      "Gradient Descent(35/49): loss=9713768390512.383\n",
      "Gradient Descent(36/49): loss=24867247079713.707\n",
      "Gradient Descent(37/49): loss=63660152524062.13\n",
      "Gradient Descent(38/49): loss=162969990461579.22\n",
      "Gradient Descent(39/49): loss=417203175581667.3\n",
      "Gradient Descent(40/49): loss=1068040129489038.2\n",
      "Gradient Descent(41/49): loss=2734182731491707.5\n",
      "Gradient Descent(42/49): loss=6999507792618121.0\n",
      "Gradient Descent(43/49): loss=1.7918739949100742e+16\n",
      "Gradient Descent(44/49): loss=4.587197426970111e+16\n",
      "Gradient Descent(45/49): loss=1.1743225413042814e+17\n",
      "Gradient Descent(46/49): loss=3.0062657057391597e+17\n",
      "Gradient Descent(47/49): loss=7.696040206691944e+17\n",
      "Gradient Descent(48/49): loss=1.9701862929130074e+18\n",
      "Gradient Descent(49/49): loss=5.043676909857758e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5771009408000002\n",
      "Gradient Descent(2/49): loss=0.7744793492480074\n",
      "Gradient Descent(3/49): loss=1.2797680748748463\n",
      "Gradient Descent(4/49): loss=2.5733072124794956\n",
      "Gradient Descent(5/49): loss=5.884767404747175\n",
      "Gradient Descent(6/49): loss=14.362105496952577\n",
      "Gradient Descent(7/49): loss=36.064091012998354\n",
      "Gradient Descent(8/49): loss=91.62117393407026\n",
      "Gradient Descent(9/49): loss=233.84730621203343\n",
      "Gradient Descent(10/49): loss=597.9462048436764\n",
      "Gradient Descent(11/49): loss=1530.0393853407857\n",
      "Gradient Descent(12/49): loss=3916.197927413258\n",
      "Gradient Descent(13/49): loss=10024.763795119365\n",
      "Gradient Descent(14/49): loss=25662.692416449147\n",
      "Gradient Descent(15/49): loss=65695.78968704253\n",
      "Gradient Descent(16/49): loss=168180.51869976983\n",
      "Gradient Descent(17/49): loss=430541.42497240455\n",
      "Gradient Descent(18/49): loss=1102185.3450303741\n",
      "Gradient Descent(19/49): loss=2821593.7803785363\n",
      "Gradient Descent(20/49): loss=7223279.374869838\n",
      "Gradient Descent(21/49): loss=18491594.496767454\n",
      "Gradient Descent(22/49): loss=47338481.20883071\n",
      "Gradient Descent(23/49): loss=121186511.19171615\n",
      "Gradient Descent(24/49): loss=310237467.9479016\n",
      "Gradient Descent(25/49): loss=794207917.2436378\n",
      "Gradient Descent(26/49): loss=2033172267.4409692\n",
      "Gradient Descent(27/49): loss=5204921003.946145\n",
      "Gradient Descent(28/49): loss=13324597769.399874\n",
      "Gradient Descent(29/49): loss=34110970288.962204\n",
      "Gradient Descent(30/49): loss=87324083939.03705\n",
      "Gradient Descent(31/49): loss=223549654883.2244\n",
      "Gradient Descent(32/49): loss=572287116500.3595\n",
      "Gradient Descent(33/49): loss=1465055018240.1438\n",
      "Gradient Descent(34/49): loss=3750540846694.124\n",
      "Gradient Descent(35/49): loss=9601384567536.104\n",
      "Gradient Descent(36/49): loss=24579544492893.785\n",
      "Gradient Descent(37/49): loss=62923633901806.445\n",
      "Gradient Descent(38/49): loss=161084502788624.25\n",
      "Gradient Descent(39/49): loss=412376327138911.4\n",
      "Gradient Descent(40/49): loss=1055683397475678.0\n",
      "Gradient Descent(41/49): loss=2702549497537849.0\n",
      "Gradient Descent(42/49): loss=6918526713696326.0\n",
      "Gradient Descent(43/49): loss=1.7711428387062298e+16\n",
      "Gradient Descent(44/49): loss=4.5341256670875464e+16\n",
      "Gradient Descent(45/49): loss=1.1607361707744536e+17\n",
      "Gradient Descent(46/49): loss=2.971484597182631e+17\n",
      "Gradient Descent(47/49): loss=7.607000568787862e+17\n",
      "Gradient Descent(48/49): loss=1.947392145609516e+18\n",
      "Gradient Descent(49/49): loss=4.985323892760491e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5777300126720003\n",
      "Gradient Descent(2/49): loss=0.7767188451123336\n",
      "Gradient Descent(3/49): loss=1.286130256159588\n",
      "Gradient Descent(4/49): loss=2.59022346844077\n",
      "Gradient Descent(5/49): loss=5.928702091880144\n",
      "Gradient Descent(6/49): loss=14.475207367884853\n",
      "Gradient Descent(7/49): loss=36.35426087446039\n",
      "Gradient Descent(8/49): loss=92.36463785130107\n",
      "Gradient Descent(9/49): loss=235.75120291198246\n",
      "Gradient Descent(10/49): loss=602.820809467381\n",
      "Gradient Descent(11/49): loss=1542.5190022492204\n",
      "Gradient Descent(12/49): loss=3948.1463757705264\n",
      "Gradient Descent(13/49): loss=10106.55245198552\n",
      "Gradient Descent(14/49): loss=25872.07200709728\n",
      "Gradient Descent(15/49): loss=66231.80206818813\n",
      "Gradient Descent(16/49): loss=169552.7110245638\n",
      "Gradient Descent(17/49): loss=434054.2379529321\n",
      "Gradient Descent(18/49): loss=1111178.1468894682\n",
      "Gradient Descent(19/49): loss=2844615.3537669126\n",
      "Gradient Descent(20/49): loss=7282214.603373859\n",
      "Gradient Descent(21/49): loss=18642468.68236804\n",
      "Gradient Descent(22/49): loss=47724719.12459616\n",
      "Gradient Descent(23/49): loss=122175280.25668812\n",
      "Gradient Descent(24/49): loss=312768716.7548579\n",
      "Gradient Descent(25/49): loss=800687914.1901399\n",
      "Gradient Descent(26/49): loss=2049761059.6246\n",
      "Gradient Descent(27/49): loss=5247388311.936345\n",
      "Gradient Descent(28/49): loss=13433314077.853376\n",
      "Gradient Descent(29/49): loss=34389284038.6049\n",
      "Gradient Descent(30/49): loss=88036567138.12933\n",
      "Gradient Descent(31/49): loss=225373611872.88968\n",
      "Gradient Descent(32/49): loss=576956446393.9308\n",
      "Gradient Descent(33/49): loss=1477008502767.6997\n",
      "Gradient Descent(34/49): loss=3781141767084.4287\n",
      "Gradient Descent(35/49): loss=9679722923735.78\n",
      "Gradient Descent(36/49): loss=24780090684764.28\n",
      "Gradient Descent(37/49): loss=63437032152995.484\n",
      "Gradient Descent(38/49): loss=162398802311656.12\n",
      "Gradient Descent(39/49): loss=415740933917846.6\n",
      "Gradient Descent(40/49): loss=1064296790829607.4\n",
      "Gradient Descent(41/49): loss=2724599784523721.5\n",
      "Gradient Descent(42/49): loss=6974975448381264.0\n",
      "Gradient Descent(43/49): loss=1.785593714785689e+16\n",
      "Gradient Descent(44/49): loss=4.571119909851849e+16\n",
      "Gradient Descent(45/49): loss=1.1702066969219674e+17\n",
      "Gradient Descent(46/49): loss=2.995729144120226e+17\n",
      "Gradient Descent(47/49): loss=7.669066608947267e+17\n",
      "Gradient Descent(48/49): loss=1.9632810518906488e+18\n",
      "Gradient Descent(49/49): loss=5.025999492840292e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.580821186543821\n",
      "Gradient Descent(2/49): loss=0.795554997072116\n",
      "Gradient Descent(3/49): loss=1.3660812582646822\n",
      "Gradient Descent(4/49): loss=2.8819124816273893\n",
      "Gradient Descent(5/49): loss=6.909324458978924\n",
      "Gradient Descent(6/49): loss=17.609755341605737\n",
      "Gradient Descent(7/49): loss=46.0397301536536\n",
      "Gradient Descent(8/49): loss=121.57533023179849\n",
      "Gradient Descent(9/49): loss=322.2658660794061\n",
      "Gradient Descent(10/49): loss=855.4805507728685\n",
      "Gradient Descent(11/49): loss=2272.178646534778\n",
      "Gradient Descent(12/49): loss=6036.203817164408\n",
      "Gradient Descent(13/49): loss=16036.84229300978\n",
      "Gradient Descent(14/49): loss=42607.53865948988\n",
      "Gradient Descent(15/49): loss=113203.2218355941\n",
      "Gradient Descent(16/49): loss=300768.8924661509\n",
      "Gradient Descent(17/49): loss=799112.1227644644\n",
      "Gradient Descent(18/49): loss=2123160.2513442393\n",
      "Gradient Descent(19/49): loss=5641023.7241677195\n",
      "Gradient Descent(20/49): loss=14987635.185111625\n",
      "Gradient Descent(21/49): loss=39820647.17569533\n",
      "Gradient Descent(22/49): loss=105799476.73347819\n",
      "Gradient Descent(23/49): loss=281098628.9855194\n",
      "Gradient Descent(24/49): loss=746850946.6039393\n",
      "Gradient Descent(25/49): loss=1984308279.2843573\n",
      "Gradient Descent(26/49): loss=5272108666.48356\n",
      "Gradient Descent(27/49): loss=14007465515.232702\n",
      "Gradient Descent(28/49): loss=37216435126.67131\n",
      "Gradient Descent(29/49): loss=98880346487.30464\n",
      "Gradient Descent(30/49): loss=262715192581.36435\n",
      "Gradient Descent(31/49): loss=698007995168.6399\n",
      "Gradient Descent(32/49): loss=1854537442363.0217\n",
      "Gradient Descent(33/49): loss=4927320530613.64\n",
      "Gradient Descent(34/49): loss=13091397917786.557\n",
      "Gradient Descent(35/49): loss=34782535127765.633\n",
      "Gradient Descent(36/49): loss=92413717580966.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=245534006240873.62\n",
      "Gradient Descent(38/49): loss=652359301181324.9\n",
      "Gradient Descent(39/49): loss=1733253427308661.5\n",
      "Gradient Descent(40/49): loss=4605081031016078.0\n",
      "Gradient Descent(41/49): loss=1.2235239791305404e+16\n",
      "Gradient Descent(42/49): loss=3.2507808601517616e+16\n",
      "Gradient Descent(43/49): loss=8.636999667337134e+16\n",
      "Gradient Descent(44/49): loss=2.2947644416145274e+17\n",
      "Gradient Descent(45/49): loss=6.096959644926208e+17\n",
      "Gradient Descent(46/49): loss=1.6199012080606093e+18\n",
      "Gradient Descent(47/49): loss=4.3039155196961956e+18\n",
      "Gradient Descent(48/49): loss=1.1435073144281516e+19\n",
      "Gradient Descent(49/49): loss=3.0381845837039903e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5828486156886531\n",
      "Gradient Descent(2/49): loss=0.8029691027118054\n",
      "Gradient Descent(3/49): loss=1.38780722468367\n",
      "Gradient Descent(4/49): loss=2.9416636309507935\n",
      "Gradient Descent(5/49): loss=7.070104716762466\n",
      "Gradient Descent(6/49): loss=18.038959837654172\n",
      "Gradient Descent(7/49): loss=47.18211100834861\n",
      "Gradient Descent(8/49): loss=124.61254935378271\n",
      "Gradient Descent(9/49): loss=330.3374809937338\n",
      "Gradient Descent(10/49): loss=876.928051868011\n",
      "Gradient Descent(11/49): loss=2329.1645396239433\n",
      "Gradient Descent(12/49): loss=6187.611663943265\n",
      "Gradient Descent(13/49): loss=16439.119828547202\n",
      "Gradient Descent(14/49): loss=43676.351871084815\n",
      "Gradient Descent(15/49): loss=116042.95368489182\n",
      "Gradient Descent(16/49): loss=308313.7780439877\n",
      "Gradient Descent(17/49): loss=819158.1312837176\n",
      "Gradient Descent(18/49): loss=2176420.4934064746\n",
      "Gradient Descent(19/49): loss=5782530.863330037\n",
      "Gradient Descent(20/49): loss=15363605.505178783\n",
      "Gradient Descent(21/49): loss=40819562.72110305\n",
      "Gradient Descent(22/49): loss=108453495.44810323\n",
      "Gradient Descent(23/49): loss=288150091.3104873\n",
      "Gradient Descent(24/49): loss=765585976.857189\n",
      "Gradient Descent(25/49): loss=2034085381.166134\n",
      "Gradient Descent(26/49): loss=5404361448.4751835\n",
      "Gradient Descent(27/49): loss=14358847931.708872\n",
      "Gradient Descent(28/49): loss=38150023069.015366\n",
      "Gradient Descent(29/49): loss=101360796291.3315\n",
      "Gradient Descent(30/49): loss=269305499665.6867\n",
      "Gradient Descent(31/49): loss=715517782060.9645\n",
      "Gradient Descent(32/49): loss=1901059195157.1582\n",
      "Gradient Descent(33/49): loss=5050924175612.051\n",
      "Gradient Descent(34/49): loss=13419800442182.746\n",
      "Gradient Descent(35/49): loss=35655067794838.36\n",
      "Gradient Descent(36/49): loss=94731949624096.36\n",
      "Gradient Descent(37/49): loss=251693316956266.56\n",
      "Gradient Descent(38/49): loss=668723973821059.8\n",
      "Gradient Descent(39/49): loss=1776732726045081.8\n",
      "Gradient Descent(40/49): loss=4720601179828915.0\n",
      "Gradient Descent(41/49): loss=1.2542165274688202e+16\n",
      "Gradient Descent(42/49): loss=3.3323278918317864e+16\n",
      "Gradient Descent(43/49): loss=8.853661975807253e+16\n",
      "Gradient Descent(44/49): loss=2.3523294503521194e+17\n",
      "Gradient Descent(45/49): loss=6.249904116640196e+17\n",
      "Gradient Descent(46/49): loss=1.6605370247502292e+18\n",
      "Gradient Descent(47/49): loss=4.4118808210586803e+18\n",
      "Gradient Descent(48/49): loss=1.172192615347012e+19\n",
      "Gradient Descent(49/49): loss=3.1143985597155803e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5818900953920002\n",
      "Gradient Descent(2/49): loss=0.7994638898389786\n",
      "Gradient Descent(3/49): loss=1.377535704305246\n",
      "Gradient Descent(4/49): loss=2.9134147081604316\n",
      "Gradient Descent(5/49): loss=6.994091633502955\n",
      "Gradient Descent(6/49): loss=17.83604215644745\n",
      "Gradient Descent(7/49): loss=46.642020500853526\n",
      "Gradient Descent(8/49): loss=123.17662436410379\n",
      "Gradient Descent(9/49): loss=326.5214133683561\n",
      "Gradient Descent(10/49): loss=866.7881832737745\n",
      "Gradient Descent(11/49): loss=2302.222964235561\n",
      "Gradient Descent(12/49): loss=6116.029633772134\n",
      "Gradient Descent(13/49): loss=16248.93257406516\n",
      "Gradient Descent(14/49): loss=43171.042396130775\n",
      "Gradient Descent(15/49): loss=114700.39598237\n",
      "Gradient Descent(16/49): loss=304746.735525652\n",
      "Gradient Descent(17/49): loss=809680.8550582626\n",
      "Gradient Descent(18/49): loss=2151240.3172442736\n",
      "Gradient Descent(19/49): loss=5715629.65232595\n",
      "Gradient Descent(20/49): loss=15185855.67670554\n",
      "Gradient Descent(21/49): loss=40347299.20088234\n",
      "Gradient Descent(22/49): loss=107198738.50025894\n",
      "Gradient Descent(23/49): loss=284816327.5747869\n",
      "Gradient Descent(24/49): loss=756728499.9869828\n",
      "Gradient Descent(25/49): loss=2010551950.868999\n",
      "Gradient Descent(26/49): loss=5341835477.516865\n",
      "Gradient Descent(27/49): loss=14192722679.46912\n",
      "Gradient Descent(28/49): loss=37708644886.33714\n",
      "Gradient Descent(29/49): loss=100188098597.76357\n",
      "Gradient Descent(30/49): loss=266189759163.6518\n",
      "Gradient Descent(31/49): loss=707239571121.2401\n",
      "Gradient Descent(32/49): loss=1879064816511.1865\n",
      "Gradient Descent(33/49): loss=4992487310987.301\n",
      "Gradient Descent(34/49): loss=13264539536561.906\n",
      "Gradient Descent(35/49): loss=35242555094689.81\n",
      "Gradient Descent(36/49): loss=93635944631087.77\n",
      "Gradient Descent(37/49): loss=248781341290335.9\n",
      "Gradient Descent(38/49): loss=660987145674326.0\n",
      "Gradient Descent(39/49): loss=1756176747342070.5\n",
      "Gradient Descent(40/49): loss=4665986000012964.0\n",
      "Gradient Descent(41/49): loss=1.2397058203434718e+16\n",
      "Gradient Descent(42/49): loss=3.2937743940708996e+16\n",
      "Gradient Descent(43/49): loss=8.75122918760733e+16\n",
      "Gradient Descent(44/49): loss=2.3251140828553818e+17\n",
      "Gradient Descent(45/49): loss=6.177595606739136e+17\n",
      "Gradient Descent(46/49): loss=1.6413253767546552e+18\n",
      "Gradient Descent(47/49): loss=4.360837393499182e+18\n",
      "Gradient Descent(48/49): loss=1.1586308870786828e+19\n",
      "Gradient Descent(49/49): loss=3.0783664038795022e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5825582423052803\n",
      "Gradient Descent(2/49): loss=0.8019072362861609\n",
      "Gradient Descent(3/49): loss=1.3846955783940795\n",
      "Gradient Descent(4/49): loss=2.933105924540427\n",
      "Gradient Descent(5/49): loss=7.047077373216575\n",
      "Gradient Descent(6/49): loss=17.977488115203585\n",
      "Gradient Descent(7/49): loss=47.01849641559273\n",
      "Gradient Descent(8/49): loss=124.17755136888607\n",
      "Gradient Descent(9/49): loss=329.1814444742898\n",
      "Gradient Descent(10/49): loss=873.8562880660278\n",
      "Gradient Descent(11/49): loss=2321.0028800047307\n",
      "Gradient Descent(12/49): loss=6165.926660127215\n",
      "Gradient Descent(13/49): loss=16381.504651533589\n",
      "Gradient Descent(14/49): loss=43523.27381690591\n",
      "Gradient Descent(15/49): loss=115636.24031237414\n",
      "Gradient Descent(16/49): loss=307233.1809941987\n",
      "Gradient Descent(17/49): loss=816287.0926918201\n",
      "Gradient Descent(18/49): loss=2168792.4306813665\n",
      "Gradient Descent(19/49): loss=5762263.863185142\n",
      "Gradient Descent(20/49): loss=15309758.112205282\n",
      "Gradient Descent(21/49): loss=40676495.58242459\n",
      "Gradient Descent(22/49): loss=108073380.36705747\n",
      "Gradient Descent(23/49): loss=287140163.55136335\n",
      "Gradient Descent(24/49): loss=762902699.7937615\n",
      "Gradient Descent(25/49): loss=2026956182.3360274\n",
      "Gradient Descent(26/49): loss=5385419880.102926\n",
      "Gradient Descent(27/49): loss=14308522078.69912\n",
      "Gradient Descent(28/49): loss=38016312310.14814\n",
      "Gradient Descent(29/49): loss=101005540176.09047\n",
      "Gradient Descent(30/49): loss=268361619693.13058\n",
      "Gradient Descent(31/49): loss=713009987361.8682\n",
      "Gradient Descent(32/49): loss=1894396235421.1672\n",
      "Gradient Descent(33/49): loss=5033221357890.143\n",
      "Gradient Descent(34/49): loss=13372765825777.207\n",
      "Gradient Descent(35/49): loss=35530101522503.34\n",
      "Gradient Descent(36/49): loss=94399926735133.45\n",
      "Gradient Descent(37/49): loss=250811165342584.1\n",
      "Gradient Descent(38/49): loss=666380185198742.1\n",
      "Gradient Descent(39/49): loss=1770505514054391.8\n",
      "Gradient Descent(40/49): loss=4704056100291173.0\n",
      "Gradient Descent(41/49): loss=1.2498206652864114e+16\n",
      "Gradient Descent(42/49): loss=3.3206485255996724e+16\n",
      "Gradient Descent(43/49): loss=8.822631067665701e+16\n",
      "Gradient Descent(44/49): loss=2.3440848483682038e+17\n",
      "Gradient Descent(45/49): loss=6.227999033628832e+17\n",
      "Gradient Descent(46/49): loss=1.6547170632449357e+18\n",
      "Gradient Descent(47/49): loss=4.3964177653353375e+18\n",
      "Gradient Descent(48/49): loss=1.1680842360719217e+19\n",
      "Gradient Descent(49/49): loss=3.103483006819207e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5856356298487131\n",
      "Gradient Descent(2/49): loss=0.8216131714598127\n",
      "Gradient Descent(3/49): loss=1.4718728851234304\n",
      "Gradient Descent(4/49): loss=3.2637285520947152\n",
      "Gradient Descent(5/49): loss=8.201366028001646\n",
      "Gradient Descent(6/49): loss=21.807519856609005\n",
      "Gradient Descent(7/49): loss=59.300637346720706\n",
      "Gradient Descent(8/49): loss=162.61667190248218\n",
      "Gradient Descent(9/49): loss=447.31433672430325\n",
      "Gradient Descent(10/49): loss=1231.8272219073174\n",
      "Gradient Descent(11/49): loss=3393.6309283179316\n",
      "Gradient Descent(12/49): loss=9350.697221703664\n",
      "Gradient Descent(13/49): loss=25765.989099758826\n",
      "Gradient Descent(14/49): loss=70999.96739891903\n",
      "Gradient Descent(15/49): loss=195646.7180001068\n",
      "Gradient Descent(16/49): loss=539123.3039567712\n",
      "Gradient Descent(17/49): loss=1485607.3842187568\n",
      "Gradient Descent(18/49): loss=4093738.915788394\n",
      "Gradient Descent(19/49): loss=11280706.164180892\n",
      "Gradient Descent(20/49): loss=31085113.113854166\n",
      "Gradient Descent(21/49): loss=85658136.9043771\n",
      "Gradient Descent(22/49): loss=236039561.26153496\n",
      "Gradient Descent(23/49): loss=650430614.2201319\n",
      "Gradient Descent(24/49): loss=1792326599.7529564\n",
      "Gradient Descent(25/49): loss=4938935177.486543\n",
      "Gradient Descent(26/49): loss=13609729774.291183\n",
      "Gradient Descent(27/49): loss=37502971365.24129\n",
      "Gradient Descent(28/49): loss=103343187893.2601\n",
      "Gradient Descent(29/49): loss=284772488557.90497\n",
      "Gradient Descent(30/49): loss=784719069469.2997\n",
      "Gradient Descent(31/49): loss=2162371867828.6636\n",
      "Gradient Descent(32/49): loss=5958631918987.643\n",
      "Gradient Descent(33/49): loss=16419606115960.516\n",
      "Gradient Descent(34/49): loss=45245866613137.02\n",
      "Gradient Descent(35/49): loss=124679510039168.42\n",
      "Gradient Descent(36/49): loss=343566857863953.44\n",
      "Gradient Descent(37/49): loss=946732833529904.2\n",
      "Gradient Descent(38/49): loss=2608816996074781.5\n",
      "Gradient Descent(39/49): loss=7188856114384202.0\n",
      "Gradient Descent(40/49): loss=1.9809611908798452e+16\n",
      "Gradient Descent(41/49): loss=5.458736657588669e+16\n",
      "Gradient Descent(42/49): loss=1.504209473365204e+17\n",
      "Gradient Descent(43/49): loss=4.144999624804682e+17\n",
      "Gradient Descent(44/49): loss=1.1421960966111729e+18\n",
      "Gradient Descent(45/49): loss=3.147435563821865e+18\n",
      "Gradient Descent(46/49): loss=8.673073439667943e+18\n",
      "Gradient Descent(47/49): loss=2.3899521170349502e+19\n",
      "Gradient Descent(48/49): loss=6.585752053700968e+19\n",
      "Gradient Descent(49/49): loss=1.8147698359176652e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5877838310718807\n",
      "Gradient Descent(2/49): loss=0.8296809559735759\n",
      "Gradient Descent(3/49): loss=1.4962526733527082\n",
      "Gradient Descent(4/49): loss=3.333057697762411\n",
      "Gradient Descent(5/49): loss=8.394557623026838\n",
      "Gradient Descent(6/49): loss=22.34202681708609\n",
      "Gradient Descent(7/49): loss=60.775672928233305\n",
      "Gradient Descent(8/49): loss=166.6834281521074\n",
      "Gradient Descent(9/49): loss=458.522838447037\n",
      "Gradient Descent(10/49): loss=1262.715517455725\n",
      "Gradient Descent(11/49): loss=3478.7488637318656\n",
      "Gradient Descent(12/49): loss=9585.250352731306\n",
      "Gradient Descent(13/49): loss=26412.325855815805\n",
      "Gradient Descent(14/49): loss=72781.01511211395\n",
      "Gradient Descent(15/49): loss=200554.57522677808\n",
      "Gradient Descent(16/49): loss=552647.3974787609\n",
      "Gradient Descent(17/49): loss=1522874.3784762253\n",
      "Gradient Descent(18/49): loss=4196431.847313389\n",
      "Gradient Descent(19/49): loss=11563686.80844124\n",
      "Gradient Descent(20/49): loss=31864894.57932351\n",
      "Gradient Descent(21/49): loss=87806902.71277164\n",
      "Gradient Descent(22/49): loss=241960700.3253088\n",
      "Gradient Descent(23/49): loss=666746905.026357\n",
      "Gradient Descent(24/49): loss=1837287770.7008443\n",
      "Gradient Descent(25/49): loss=5062830180.153417\n",
      "Gradient Descent(26/49): loss=13951134843.641733\n",
      "Gradient Descent(27/49): loss=38443747174.34766\n",
      "Gradient Descent(28/49): loss=105935589712.84879\n",
      "Gradient Descent(29/49): loss=291916111011.9184\n",
      "Gradient Descent(30/49): loss=804404035503.6865\n",
      "Gradient Descent(31/49): loss=2216615760233.0503\n",
      "Gradient Descent(32/49): loss=6108106388896.97\n",
      "Gradient Descent(33/49): loss=16831497965244.736\n",
      "Gradient Descent(34/49): loss=46380875793032.445\n",
      "Gradient Descent(35/49): loss=127807141335290.83\n",
      "Gradient Descent(36/49): loss=352185358663517.4\n",
      "Gradient Descent(37/49): loss=970481974333194.4\n",
      "Gradient Descent(38/49): loss=2674260128472315.5\n",
      "Gradient Descent(39/49): loss=7369191210017964.0\n",
      "Gradient Descent(40/49): loss=2.030654329832646e+16\n",
      "Gradient Descent(41/49): loss=5.595671071286762e+16\n",
      "Gradient Descent(42/49): loss=1.541943120403918e+17\n",
      "Gradient Descent(43/49): loss=4.248978462585351e+17\n",
      "Gradient Descent(44/49): loss=1.170848505149973e+18\n",
      "Gradient Descent(45/49): loss=3.2263901407910605e+18\n",
      "Gradient Descent(46/49): loss=8.890640671963019e+18\n",
      "Gradient Descent(47/49): loss=2.4499049435659133e+19\n",
      "Gradient Descent(48/49): loss=6.750958062489964e+19\n",
      "Gradient Descent(49/49): loss=1.8602940036996606e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5867682126080004\n",
      "Gradient Descent(2/49): loss=0.8258666992706287\n",
      "Gradient Descent(3/49): loss=1.4847264891182104\n",
      "Gradient Descent(4/49): loss=3.3002805260223553\n",
      "Gradient Descent(5/49): loss=8.30322123011485\n",
      "Gradient Descent(6/49): loss=22.089324634314004\n",
      "Gradient Descent(7/49): loss=60.07831117492387\n",
      "Gradient Descent(8/49): loss=164.76076248622223\n",
      "Gradient Descent(9/49): loss=453.2237253196874\n",
      "Gradient Descent(10/49): loss=1248.112265703585\n",
      "Gradient Descent(11/49): loss=3438.5071275852547\n",
      "Gradient Descent(12/49): loss=9474.359208985406\n",
      "Gradient Descent(13/49): loss=26106.753204491535\n",
      "Gradient Descent(14/49): loss=71938.97809851161\n",
      "Gradient Descent(15/49): loss=198234.2570164826\n",
      "Gradient Descent(16/49): loss=546253.527602783\n",
      "Gradient Descent(17/49): loss=1505255.4296303862\n",
      "Gradient Descent(18/49): loss=4147881.070858032\n",
      "Gradient Descent(19/49): loss=11429900.287824165\n",
      "Gradient Descent(20/49): loss=31496232.44209751\n",
      "Gradient Descent(21/49): loss=86791017.3264139\n",
      "Gradient Descent(22/49): loss=239161326.55363408\n",
      "Gradient Descent(23/49): loss=659032950.660124\n",
      "Gradient Descent(24/49): loss=1816031198.0478187\n",
      "Gradient Descent(25/49): loss=5004255568.549512\n",
      "Gradient Descent(26/49): loss=13789726643.903736\n",
      "Gradient Descent(27/49): loss=37998970739.152885\n",
      "Gradient Descent(28/49): loss=104709963768.00867\n",
      "Gradient Descent(29/49): loss=288538776158.32715\n",
      "Gradient Descent(30/49): loss=795097451581.1224\n",
      "Gradient Descent(31/49): loss=2190970537575.9407\n",
      "Gradient Descent(32/49): loss=6037438413343.896\n",
      "Gradient Descent(33/49): loss=16636765291811.56\n",
      "Gradient Descent(34/49): loss=45844270438118.55\n",
      "Gradient Descent(35/49): loss=126328471619274.84\n",
      "Gradient Descent(36/49): loss=348110736394072.44\n",
      "Gradient Descent(37/49): loss=959253945207420.1\n",
      "Gradient Descent(38/49): loss=2643320171413490.5\n",
      "Gradient Descent(39/49): loss=7283933064347337.0\n",
      "Gradient Descent(40/49): loss=2.007160595211665e+16\n",
      "Gradient Descent(41/49): loss=5.530931736165049e+16\n",
      "Gradient Descent(42/49): loss=1.5241035492177962e+17\n",
      "Gradient Descent(43/49): loss=4.199819740224362e+17\n",
      "Gradient Descent(44/49): loss=1.1573023276163732e+18\n",
      "Gradient Descent(45/49): loss=3.189062293979471e+18\n",
      "Gradient Descent(46/49): loss=8.787780057290513e+18\n",
      "Gradient Descent(47/49): loss=2.4215606725871518e+19\n",
      "Gradient Descent(48/49): loss=6.672852589381276e+19\n",
      "Gradient Descent(49/49): loss=1.838771259530044e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5874761604147205\n",
      "Gradient Descent(2/49): loss=0.8285254680535438\n",
      "Gradient Descent(3/49): loss=1.4927609401829973\n",
      "Gradient Descent(4/49): loss=3.323128207183027\n",
      "Gradient Descent(5/49): loss=8.366888248128896\n",
      "Gradient Descent(6/49): loss=22.26547341695903\n",
      "Gradient Descent(7/49): loss=60.564414708186604\n",
      "Gradient Descent(8/49): loss=166.100977330274\n",
      "Gradient Descent(9/49): loss=456.9175292917459\n",
      "Gradient Descent(10/49): loss=1258.2916198768798\n",
      "Gradient Descent(11/49): loss=3466.5580638933284\n",
      "Gradient Descent(12/49): loss=9551.657077024836\n",
      "Gradient Descent(13/49): loss=26319.755917608163\n",
      "Gradient Descent(14/49): loss=72525.92908272015\n",
      "Gradient Descent(15/49): loss=199851.65985651515\n",
      "Gradient Descent(16/49): loss=550710.443576818\n",
      "Gradient Descent(17/49): loss=1517536.907996511\n",
      "Gradient Descent(18/49): loss=4181723.913351083\n",
      "Gradient Descent(19/49): loss=11523157.625307007\n",
      "Gradient Descent(20/49): loss=31753212.361971304\n",
      "Gradient Descent(21/49): loss=87499151.19432849\n",
      "Gradient Descent(22/49): loss=241112660.2407553\n",
      "Gradient Descent(23/49): loss=664410045.7690946\n",
      "Gradient Descent(24/49): loss=1830848321.3308063\n",
      "Gradient Descent(25/49): loss=5045085633.468885\n",
      "Gradient Descent(26/49): loss=13902237970.797472\n",
      "Gradient Descent(27/49): loss=38309006951.54124\n",
      "Gradient Descent(28/49): loss=105564299554.88454\n",
      "Gradient Descent(29/49): loss=290892983852.6323\n",
      "Gradient Descent(30/49): loss=801584706303.4646\n",
      "Gradient Descent(31/49): loss=2208846816689.1836\n",
      "Gradient Descent(32/49): loss=6086698288067.352\n",
      "Gradient Descent(33/49): loss=16772505802596.715\n",
      "Gradient Descent(34/49): loss=46218316989630.38\n",
      "Gradient Descent(35/49): loss=127359194296631.31\n",
      "Gradient Descent(36/49): loss=350950995803787.2\n",
      "Gradient Descent(37/49): loss=967080564036889.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=2664887202259873.5\n",
      "Gradient Descent(39/49): loss=7343363174547602.0\n",
      "Gradient Descent(40/49): loss=2.023537156378556e+16\n",
      "Gradient Descent(41/49): loss=5.576058988116861e+16\n",
      "Gradient Descent(42/49): loss=1.5365388147653914e+17\n",
      "Gradient Descent(43/49): loss=4.234086357967679e+17\n",
      "Gradient Descent(44/49): loss=1.1667448368014943e+18\n",
      "Gradient Descent(45/49): loss=3.215082072290068e+18\n",
      "Gradient Descent(46/49): loss=8.859480158401893e+18\n",
      "Gradient Descent(47/49): loss=2.4413183524490973e+19\n",
      "Gradient Descent(48/49): loss=6.727296852008391e+19\n",
      "Gradient Descent(49/49): loss=1.8537739205392063e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5905378745512624\n",
      "Gradient Descent(2/49): loss=0.8491230980570802\n",
      "Gradient Descent(3/49): loss=1.5876683549120554\n",
      "Gradient Descent(4/49): loss=3.697027463015437\n",
      "Gradient Descent(5/49): loss=9.721568011670088\n",
      "Gradient Descent(6/49): loss=26.928258272683284\n",
      "Gradient Descent(7/49): loss=76.07228632716134\n",
      "Gradient Descent(8/49): loss=216.4325448535696\n",
      "Gradient Descent(9/49): loss=617.3154792308144\n",
      "Gradient Descent(10/49): loss=1762.277228105868\n",
      "Gradient Descent(11/49): loss=5032.402479068047\n",
      "Gradient Descent(12/49): loss=14372.207208340818\n",
      "Gradient Descent(13/49): loss=41047.623495620384\n",
      "Gradient Descent(14/49): loss=117235.27995371805\n",
      "Gradient Descent(15/49): loss=334834.8455636751\n",
      "Gradient Descent(16/49): loss=956320.9649022263\n",
      "Gradient Descent(17/49): loss=2731347.4703449863\n",
      "Gradient Descent(18/49): loss=7801000.672539405\n",
      "Gradient Descent(19/49): loss=22280437.183329035\n",
      "Gradient Descent(20/49): loss=63635155.801799804\n",
      "Gradient Descent(21/49): loss=181748367.64800113\n",
      "Gradient Descent(22/49): loss=519091512.0019647\n",
      "Gradient Descent(23/49): loss=1482577266.5914226\n",
      "Gradient Descent(24/49): loss=4234388930.2743435\n",
      "Gradient Descent(25/49): loss=12093838222.918776\n",
      "Gradient Descent(26/49): loss=34541211347.64145\n",
      "Gradient Descent(27/49): loss=98653153729.16656\n",
      "Gradient Descent(28/49): loss=281763272365.01294\n",
      "Gradient Descent(29/49): loss=804744082200.8943\n",
      "Gradient Descent(30/49): loss=2298429573173.2617\n",
      "Gradient Descent(31/49): loss=6564544703938.915\n",
      "Gradient Descent(32/49): loss=18748996128919.414\n",
      "Gradient Descent(33/49): loss=53549007843803.73\n",
      "Gradient Descent(34/49): loss=152941321302701.16\n",
      "Gradient Descent(35/49): loss=436815707772599.94\n",
      "Gradient Descent(36/49): loss=1247589342969211.0\n",
      "Gradient Descent(37/49): loss=3563239922454456.0\n",
      "Gradient Descent(38/49): loss=1.0176969542521996e+16\n",
      "Gradient Descent(39/49): loss=2.9066442710395056e+16\n",
      "Gradient Descent(40/49): loss=8.301666702516317e+16\n",
      "Gradient Descent(41/49): loss=2.371039026905667e+17\n",
      "Gradient Descent(42/49): loss=6.771924564745612e+17\n",
      "Gradient Descent(43/49): loss=1.9341293749368343e+18\n",
      "Gradient Descent(44/49): loss=5.524066907757217e+18\n",
      "Gradient Descent(45/49): loss=1.5777287495245867e+19\n",
      "Gradient Descent(46/49): loss=4.506151081517646e+19\n",
      "Gradient Descent(47/49): loss=1.2870018103923984e+20\n",
      "Gradient Descent(48/49): loss=3.675805870661408e+20\n",
      "Gradient Descent(49/49): loss=1.0498469147196166e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5928090503830703\n",
      "Gradient Descent(2/49): loss=0.8578809791821496\n",
      "Gradient Descent(3/49): loss=1.614952915025118\n",
      "Gradient Descent(4/49): loss=3.7772260709864782\n",
      "Gradient Descent(5/49): loss=9.952894431726849\n",
      "Gradient Descent(6/49): loss=27.591220836840023\n",
      "Gradient Descent(7/49): loss=77.96804488248102\n",
      "Gradient Descent(8/49): loss=221.849292039226\n",
      "Gradient Descent(9/49): loss=632.7885220436675\n",
      "Gradient Descent(10/49): loss=1806.4720568592759\n",
      "Gradient Descent(11/49): loss=5158.629600645839\n",
      "Gradient Descent(12/49): loss=14732.72676145595\n",
      "Gradient Descent(13/49): loss=42077.30566244944\n",
      "Gradient Descent(14/49): loss=120176.15746158473\n",
      "Gradient Descent(15/49): loss=343234.28808510315\n",
      "Gradient Descent(16/49): loss=980310.614958998\n",
      "Gradient Descent(17/49): loss=2799864.312143522\n",
      "Gradient Descent(18/49): loss=7996691.626671839\n",
      "Gradient Descent(19/49): loss=22839350.11969889\n",
      "Gradient Descent(20/49): loss=65231467.04162442\n",
      "Gradient Descent(21/49): loss=186307592.1823284\n",
      "Gradient Descent(22/49): loss=532113113.19668657\n",
      "Gradient Descent(23/49): loss=1519768261.7657483\n",
      "Gradient Descent(24/49): loss=4340610131.59389\n",
      "Gradient Descent(25/49): loss=12397216596.009325\n",
      "Gradient Descent(26/49): loss=35407690319.02475\n",
      "Gradient Descent(27/49): loss=101127904319.3419\n",
      "Gradient Descent(28/49): loss=288831407525.6679\n",
      "Gradient Descent(29/49): loss=824931383033.3254\n",
      "Gradient Descent(30/49): loss=2356086523080.53\n",
      "Gradient Descent(31/49): loss=6729218718568.784\n",
      "Gradient Descent(32/49): loss=19219321582103.34\n",
      "Gradient Descent(33/49): loss=54892304370643.516\n",
      "Gradient Descent(34/49): loss=156777910512988.9\n",
      "Gradient Descent(35/49): loss=447773390216183.1\n",
      "Gradient Descent(36/49): loss=1278885579796444.2\n",
      "Gradient Descent(37/49): loss=3652625104456314.5\n",
      "Gradient Descent(38/49): loss=1.0432262560837692e+16\n",
      "Gradient Descent(39/49): loss=2.979558510000749e+16\n",
      "Gradient Descent(40/49): loss=8.509917060412184e+16\n",
      "Gradient Descent(41/49): loss=2.4305174116244234e+17\n",
      "Gradient Descent(42/49): loss=6.941800779340052e+17\n",
      "Gradient Descent(43/49): loss=1.982647720587545e+18\n",
      "Gradient Descent(44/49): loss=5.662640154770292e+18\n",
      "Gradient Descent(45/49): loss=1.617306654603835e+19\n",
      "Gradient Descent(46/49): loss=4.619189536214476e+19\n",
      "Gradient Descent(47/49): loss=1.3192867234381624e+20\n",
      "Gradient Descent(48/49): loss=3.768014810811961e+20\n",
      "Gradient Descent(49/49): loss=1.0761827101160718e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5917352924480002\n",
      "Gradient Descent(2/49): loss=0.8537404612087092\n",
      "Gradient Descent(3/49): loss=1.602053423706218\n",
      "Gradient Descent(4/49): loss=3.7393100758950464\n",
      "Gradient Descent(5/49): loss=9.843528800211397\n",
      "Gradient Descent(6/49): loss=27.277787898729702\n",
      "Gradient Descent(7/49): loss=77.071775310015\n",
      "Gradient Descent(8/49): loss=219.28838275538968\n",
      "Gradient Descent(9/49): loss=625.4732352800813\n",
      "Gradient Descent(10/49): loss=1785.5777925756756\n",
      "Gradient Descent(11/49): loss=5098.952418667614\n",
      "Gradient Descent(12/49): loss=14562.281688248568\n",
      "Gradient Descent(13/49): loss=41590.49641509646\n",
      "Gradient Descent(14/49): loss=118785.78049644215\n",
      "Gradient Descent(15/49): loss=339263.2313611597\n",
      "Gradient Descent(16/49): loss=968968.8787759382\n",
      "Gradient Descent(17/49): loss=2767471.178357495\n",
      "Gradient Descent(18/49): loss=7904173.596192748\n",
      "Gradient Descent(19/49): loss=22575109.37176937\n",
      "Gradient Descent(20/49): loss=64476769.04039452\n",
      "Gradient Descent(21/49): loss=184152099.21994478\n",
      "Gradient Descent(22/49): loss=525956809.7457534\n",
      "Gradient Descent(23/49): loss=1502185243.478418\n",
      "Gradient Descent(24/49): loss=4290391273.0623846\n",
      "Gradient Descent(25/49): loss=12253786514.156277\n",
      "Gradient Descent(26/49): loss=34998039662.243744\n",
      "Gradient Descent(27/49): loss=99957901078.48885\n",
      "Gradient Descent(28/49): loss=285489761269.44135\n",
      "Gradient Descent(29/49): loss=815387307160.8822\n",
      "Gradient Descent(30/49): loss=2328827687981.2607\n",
      "Gradient Descent(31/49): loss=6651364759642.363\n",
      "Gradient Descent(32/49): loss=18996962890015.406\n",
      "Gradient Descent(33/49): loss=54257225710174.93\n",
      "Gradient Descent(34/49): loss=154964062350817.03\n",
      "Gradient Descent(35/49): loss=442592858480127.25\n",
      "Gradient Descent(36/49): loss=1264089463105083.0\n",
      "Gradient Descent(37/49): loss=3610365915574330.5\n",
      "Gradient Descent(38/49): loss=1.0311566091471826e+16\n",
      "Gradient Descent(39/49): loss=2.9450863913851324e+16\n",
      "Gradient Descent(40/49): loss=8.411461242434723e+16\n",
      "Gradient Descent(41/49): loss=2.4023974454517446e+17\n",
      "Gradient Descent(42/49): loss=6.861487343954054e+17\n",
      "Gradient Descent(43/49): loss=1.959709400306817e+18\n",
      "Gradient Descent(44/49): loss=5.597126018216385e+18\n",
      "Gradient Descent(45/49): loss=1.5985951620626889e+19\n",
      "Gradient Descent(46/49): loss=4.5657476423669e+19\n",
      "Gradient Descent(47/49): loss=1.3040231841365425e+20\n",
      "Gradient Descent(48/49): loss=3.72442061621226e+20\n",
      "Gradient Descent(49/49): loss=1.0637317721964963e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5924837670003205\n",
      "Gradient Descent(2/49): loss=0.8566266539299061\n",
      "Gradient Descent(3/49): loss=1.6110451532895254\n",
      "Gradient Descent(4/49): loss=3.765739829310595\n",
      "Gradient Descent(5/49): loss=9.919763293494489\n",
      "Gradient Descent(6/49): loss=27.496269709551573\n",
      "Gradient Descent(7/49): loss=77.69652968444575\n",
      "Gradient Descent(8/49): loss=221.07349219874772\n",
      "Gradient Descent(9/49): loss=630.572434835794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=1800.1423649013811\n",
      "Gradient Descent(11/49): loss=5140.551042161429\n",
      "Gradient Descent(12/49): loss=14681.092265284315\n",
      "Gradient Descent(13/49): loss=41929.83205264275\n",
      "Gradient Descent(14/49): loss=119754.95775930633\n",
      "Gradient Descent(15/49): loss=342031.29929015564\n",
      "Gradient Descent(16/49): loss=976874.758336354\n",
      "Gradient Descent(17/49): loss=2790051.1617181227\n",
      "Gradient Descent(18/49): loss=7968664.287417304\n",
      "Gradient Descent(19/49): loss=22759301.235727098\n",
      "Gradient Descent(20/49): loss=65002839.42379108\n",
      "Gradient Descent(21/49): loss=185654608.84271333\n",
      "Gradient Descent(22/49): loss=530248127.48016024\n",
      "Gradient Descent(23/49): loss=1514441676.060602\n",
      "Gradient Descent(24/49): loss=4325396870.160668\n",
      "Gradient Descent(25/49): loss=12353766000.029366\n",
      "Gradient Descent(26/49): loss=35283591071.8496\n",
      "Gradient Descent(27/49): loss=100773464459.4688\n",
      "Gradient Descent(28/49): loss=287819091841.8725\n",
      "Gradient Descent(29/49): loss=822040108208.7532\n",
      "Gradient Descent(30/49): loss=2347828753053.8994\n",
      "Gradient Descent(31/49): loss=6705633701596.703\n",
      "Gradient Descent(32/49): loss=19151960415129.53\n",
      "Gradient Descent(33/49): loss=54699914141654.47\n",
      "Gradient Descent(34/49): loss=156228424779992.72\n",
      "Gradient Descent(35/49): loss=446204004014163.94\n",
      "Gradient Descent(36/49): loss=1274403255864827.8\n",
      "Gradient Descent(37/49): loss=3639823139075495.5\n",
      "Gradient Descent(38/49): loss=1.0395698867512788e+16\n",
      "Gradient Descent(39/49): loss=2.969115553550158e+16\n",
      "Gradient Descent(40/49): loss=8.480090932494854e+16\n",
      "Gradient Descent(41/49): loss=2.421998771229879e+17\n",
      "Gradient Descent(42/49): loss=6.917470690509169e+17\n",
      "Gradient Descent(43/49): loss=1.9756988039163456e+18\n",
      "Gradient Descent(44/49): loss=5.642793353865884e+18\n",
      "Gradient Descent(45/49): loss=1.6116382097977836e+19\n",
      "Gradient Descent(46/49): loss=4.6029998910032536e+19\n",
      "Gradient Descent(47/49): loss=1.3146627988693434e+20\n",
      "Gradient Descent(48/49): loss=3.7548084198510566e+20\n",
      "Gradient Descent(49/49): loss=1.0724108327936553e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5955279206514691\n",
      "Gradient Descent(2/49): loss=0.8781377211067997\n",
      "Gradient Descent(3/49): loss=1.7142105547739093\n",
      "Gradient Descent(4/49): loss=4.187648425894137\n",
      "Gradient Descent(5/49): loss=11.505067023817325\n",
      "Gradient Descent(6/49): loss=33.15291820390943\n",
      "Gradient Descent(7/49): loss=97.19592113510096\n",
      "Gradient Descent(8/49): loss=286.66074100674604\n",
      "Gradient Descent(9/49): loss=847.1734641149999\n",
      "Gradient Descent(10/49): loss=2505.3943041582274\n",
      "Gradient Descent(11/49): loss=7411.074837342034\n",
      "Gradient Descent(12/49): loss=21924.040126714754\n",
      "Gradient Descent(13/49): loss=64859.19663879766\n",
      "Gradient Descent(14/49): loss=191878.56366415828\n",
      "Gradient Descent(15/49): loss=567652.6590719444\n",
      "Gradient Descent(16/49): loss=1679342.7429263545\n",
      "Gradient Descent(17/49): loss=4968166.687001167\n",
      "Gradient Descent(18/49): loss=14697823.443152225\n",
      "Gradient Descent(19/49): loss=43482039.990549535\n",
      "Gradient Descent(20/49): loss=128637266.22437866\n",
      "Gradient Descent(21/49): loss=380560487.5145463\n",
      "Gradient Descent(22/49): loss=1125850145.3794239\n",
      "Gradient Descent(23/49): loss=3330715069.20695\n",
      "Gradient Descent(24/49): loss=9853587459.857622\n",
      "Gradient Descent(25/49): loss=29150853140.35854\n",
      "Gradient Descent(26/49): loss=86239883929.55629\n",
      "Gradient Descent(27/49): loss=255132072616.29578\n",
      "Gradient Descent(28/49): loss=754782723627.0961\n",
      "Gradient Descent(29/49): loss=2232949209577.4575\n",
      "Gradient Descent(30/49): loss=6605956941613.591\n",
      "Gradient Descent(31/49): loss=19543063016068.152\n",
      "Gradient Descent(32/49): loss=57816197626730.25\n",
      "Gradient Descent(33/49): loss=171043439058936.9\n",
      "Gradient Descent(34/49): loss=506014910111926.56\n",
      "Gradient Descent(35/49): loss=1496994510075080.0\n",
      "Gradient Descent(36/49): loss=4428708558606112.5\n",
      "Gradient Descent(37/49): loss=1.3101891399781074e+16\n",
      "Gradient Descent(38/49): loss=3.8760635517109656e+16\n",
      "Gradient Descent(39/49): loss=1.1466946411380973e+17\n",
      "Gradient Descent(40/49): loss=3.3923814263427834e+17\n",
      "Gradient Descent(41/49): loss=1.0036021211692298e+18\n",
      "Gradient Descent(42/49): loss=2.9690565152672876e+18\n",
      "Gradient Descent(43/49): loss=8.783656794766052e+18\n",
      "Gradient Descent(44/49): loss=2.5985570261637267e+19\n",
      "Gradient Descent(45/49): loss=7.68757110620272e+19\n",
      "Gradient Descent(46/49): loss=2.2742910360592472e+20\n",
      "Gradient Descent(47/49): loss=6.728262601077407e+20\n",
      "Gradient Descent(48/49): loss=1.990489207902727e+21\n",
      "Gradient Descent(49/49): loss=5.888663272660142e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5979242736222209\n",
      "Gradient Descent(2/49): loss=0.8876234447062185\n",
      "Gradient Descent(3/49): loss=1.7446694724409773\n",
      "Gradient Descent(4/49): loss=4.2801544408916685\n",
      "Gradient Descent(5/49): loss=11.781133171557048\n",
      "Gradient Descent(6/49): loss=33.97202864835979\n",
      "Gradient Descent(7/49): loss=99.62157382693034\n",
      "Gradient Descent(8/49): loss=293.83918828322663\n",
      "Gradient Descent(9/49): loss=868.4125788906242\n",
      "Gradient Descent(10/49): loss=2568.2304976637497\n",
      "Gradient Descent(11/49): loss=7596.971828562213\n",
      "Gradient Descent(12/49): loss=22474.000181892727\n",
      "Gradient Descent(13/49): loss=66486.20086239086\n",
      "Gradient Descent(14/49): loss=196691.89535555313\n",
      "Gradient Descent(15/49): loss=581892.4219441715\n",
      "Gradient Descent(16/49): loss=1721469.6598039798\n",
      "Gradient Descent(17/49): loss=5092794.96028804\n",
      "Gradient Descent(18/49): loss=15066523.7292394\n",
      "Gradient Descent(19/49): loss=44572802.91930298\n",
      "Gradient Descent(20/49): loss=131864179.27519396\n",
      "Gradient Descent(21/49): loss=390106987.08643836\n",
      "Gradient Descent(22/49): loss=1154092509.7152123\n",
      "Gradient Descent(23/49): loss=3414267279.860154\n",
      "Gradient Descent(24/49): loss=10100768319.856052\n",
      "Gradient Descent(25/49): loss=29882112996.578342\n",
      "Gradient Descent(26/49): loss=88403243088.20062\n",
      "Gradient Descent(27/49): loss=261532154351.25833\n",
      "Gradient Descent(28/49): loss=773716725431.8484\n",
      "Gradient Descent(29/49): loss=2288963560516.5464\n",
      "Gradient Descent(30/49): loss=6771669797430.952\n",
      "Gradient Descent(31/49): loss=20033307928718.473\n",
      "Gradient Descent(32/49): loss=59266538176319.47\n",
      "Gradient Descent(33/49): loss=175334126540818.53\n",
      "Gradient Descent(34/49): loss=518708479958335.9\n",
      "Gradient Descent(35/49): loss=1534547167108582.5\n",
      "Gradient Descent(36/49): loss=4539804339174176.0\n",
      "Gradient Descent(37/49): loss=1.3430557157011422e+16\n",
      "Gradient Descent(38/49): loss=3.973296029330389e+16\n",
      "Gradient Descent(39/49): loss=1.1754598973171693e+17\n",
      "Gradient Descent(40/49): loss=3.477480560223061e+17\n",
      "Gradient Descent(41/49): loss=1.0287778489362721e+18\n",
      "Gradient Descent(42/49): loss=3.0435363882929423e+18\n",
      "Gradient Descent(43/49): loss=9.003998051125671e+18\n",
      "Gradient Descent(44/49): loss=2.6637427834452103e+19\n",
      "Gradient Descent(45/49): loss=7.880416650544618e+19\n",
      "Gradient Descent(46/49): loss=2.331342461897179e+20\n",
      "Gradient Descent(47/49): loss=6.897043539275923e+20\n",
      "Gradient Descent(48/49): loss=2.0404213606595667e+21\n",
      "Gradient Descent(49/49): loss=6.036382553375715e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5967913349120004\n",
      "Gradient Descent(2/49): loss=0.8831388201156951\n",
      "Gradient Descent(3/49): loss=1.7302692203424097\n",
      "Gradient Descent(4/49): loss=4.236419796373\n",
      "Gradient Descent(5/49): loss=11.650615660501922\n",
      "Gradient Descent(6/49): loss=33.584772704939425\n",
      "Gradient Descent(7/49): loss=98.4747829051947\n",
      "Gradient Descent(8/49): loss=290.4453890816754\n",
      "Gradient Descent(9/49): loss=858.3712303941421\n",
      "Gradient Descent(10/49): loss=2538.52303933269\n",
      "Gradient Descent(11/49): loss=7509.084150897221\n",
      "Gradient Descent(12/49): loss=22213.99214334808\n",
      "Gradient Descent(13/49): loss=65716.9919482168\n",
      "Gradient Descent(14/49): loss=194416.26657093025\n",
      "Gradient Descent(15/49): loss=575160.2006147142\n",
      "Gradient Descent(16/49): loss=1701553.0550897627\n",
      "Gradient Descent(17/49): loss=5033873.675768745\n",
      "Gradient Descent(18/49): loss=14892210.999984669\n",
      "Gradient Descent(19/49): loss=44057116.139948346\n",
      "Gradient Descent(20/49): loss=130338571.50602627\n",
      "Gradient Descent(21/49): loss=385593629.0610289\n",
      "Gradient Descent(22/49): loss=1140740191.3316145\n",
      "Gradient Descent(23/49): loss=3374765781.1533585\n",
      "Gradient Descent(24/49): loss=9983907086.082632\n",
      "Gradient Descent(25/49): loss=29536390722.585743\n",
      "Gradient Descent(26/49): loss=87380458312.82152\n",
      "Gradient Descent(27/49): loss=258506347871.774\n",
      "Gradient Descent(28/49): loss=764765179543.056\n",
      "Gradient Descent(29/49): loss=2262481307159.5767\n",
      "Gradient Descent(30/49): loss=6693324699099.958\n",
      "Gradient Descent(31/49): loss=19801531789816.81\n",
      "Gradient Descent(32/49): loss=58580851646998.055\n",
      "Gradient Descent(33/49): loss=173305591512486.8\n",
      "Gradient Descent(34/49): loss=512707261930531.1\n",
      "Gradient Descent(35/49): loss=1516793163695338.5\n",
      "Gradient Descent(36/49): loss=4487280895476599.5\n",
      "Gradient Descent(37/49): loss=1.3275171801178998e+16\n",
      "Gradient Descent(38/49): loss=3.927326825660915e+16\n",
      "Gradient Descent(39/49): loss=1.161860368103584e+17\n",
      "Gradient Descent(40/49): loss=3.437247712997384e+17\n",
      "Gradient Descent(41/49): loss=1.0168753634131091e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=3.0083240751216026e+18\n",
      "Gradient Descent(43/49): loss=8.899825943839022e+18\n",
      "Gradient Descent(44/49): loss=2.6329245072256225e+19\n",
      "Gradient Descent(45/49): loss=7.789243862176722e+19\n",
      "Gradient Descent(46/49): loss=2.3043699041861465e+20\n",
      "Gradient Descent(47/49): loss=6.817247924543945e+20\n",
      "Gradient Descent(48/49): loss=2.016814625996886e+21\n",
      "Gradient Descent(49/49): loss=5.96654438954919e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5975810620620802\n",
      "Gradient Descent(2/49): loss=0.8862648760665567\n",
      "Gradient Descent(3/49): loss=1.740307071417378\n",
      "Gradient Descent(4/49): loss=4.266905502143631\n",
      "Gradient Descent(5/49): loss=11.74159429960302\n",
      "Gradient Descent(6/49): loss=33.85471363800738\n",
      "Gradient Descent(7/49): loss=99.27416588875059\n",
      "Gradient Descent(8/49): loss=292.8110734273405\n",
      "Gradient Descent(9/49): loss=865.3706606894042\n",
      "Gradient Descent(10/49): loss=2559.2309436457986\n",
      "Gradient Descent(11/49): loss=7570.347204744493\n",
      "Gradient Descent(12/49): loss=22395.233551578574\n",
      "Gradient Descent(13/49): loss=66253.17732004638\n",
      "Gradient Descent(14/49): loss=196002.51816467955\n",
      "Gradient Descent(15/49): loss=579852.9681194907\n",
      "Gradient Descent(16/49): loss=1715436.139265735\n",
      "Gradient Descent(17/49): loss=5074945.392784628\n",
      "Gradient Descent(18/49): loss=15013717.56839572\n",
      "Gradient Descent(19/49): loss=44416581.17272617\n",
      "Gradient Descent(20/49): loss=131402012.8597608\n",
      "Gradient Descent(21/49): loss=388739713.96271926\n",
      "Gradient Descent(22/49): loss=1150047568.9057193\n",
      "Gradient Descent(23/49): loss=3402300726.969157\n",
      "Gradient Descent(24/49): loss=10065366469.78394\n",
      "Gradient Descent(25/49): loss=29777380163.32903\n",
      "Gradient Descent(26/49): loss=88093401474.30951\n",
      "Gradient Descent(27/49): loss=260615518920.69333\n",
      "Gradient Descent(28/49): loss=771004951174.1416\n",
      "Gradient Descent(29/49): loss=2280941047552.8154\n",
      "Gradient Descent(30/49): loss=6747935995079.374\n",
      "Gradient Descent(31/49): loss=19963093847844.395\n",
      "Gradient Descent(32/49): loss=59058816839459.305\n",
      "Gradient Descent(33/49): loss=174719603737845.62\n",
      "Gradient Descent(34/49): loss=516890475698091.75\n",
      "Gradient Descent(35/49): loss=1529168783305322.2\n",
      "Gradient Descent(36/49): loss=4523892928530506.0\n",
      "Gradient Descent(37/49): loss=1.3383484839763564e+16\n",
      "Gradient Descent(38/49): loss=3.959370154996051e+16\n",
      "Gradient Descent(39/49): loss=1.1713400666539586e+17\n",
      "Gradient Descent(40/49): loss=3.465292453189161e+17\n",
      "Gradient Descent(41/49): loss=1.0251721193514145e+18\n",
      "Gradient Descent(42/49): loss=3.0328691978889267e+18\n",
      "Gradient Descent(43/49): loss=8.972440235033957e+18\n",
      "Gradient Descent(44/49): loss=2.654406719132461e+19\n",
      "Gradient Descent(45/49): loss=7.852796837882135e+19\n",
      "Gradient Descent(46/49): loss=2.3231714165190512e+20\n",
      "Gradient Descent(47/49): loss=6.872870318629129e+20\n",
      "Gradient Descent(48/49): loss=2.0332699550633495e+21\n",
      "Gradient Descent(49/49): loss=6.015225835059112e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6006057681493336\n",
      "Gradient Descent(2/49): loss=0.9087109331066502\n",
      "Gradient Descent(3/49): loss=1.8522830007883553\n",
      "Gradient Descent(4/49): loss=4.741972458063784\n",
      "Gradient Descent(5/49): loss=13.591646420970116\n",
      "Gradient Descent(6/49): loss=40.69377293237369\n",
      "Gradient Descent(7/49): loss=123.694035373535\n",
      "Gradient Descent(8/49): loss=377.88233909958274\n",
      "Gradient Descent(9/49): loss=1156.334019260682\n",
      "Gradient Descent(10/49): loss=3540.3422897538603\n",
      "Gradient Descent(11/49): loss=10841.367618139006\n",
      "Gradient Descent(12/49): loss=33200.75768631997\n",
      "Gradient Descent(13/49): loss=101676.38977011279\n",
      "Gradient Descent(14/49): loss=311383.01302673516\n",
      "Gradient Descent(15/49): loss=953609.5467500265\n",
      "Gradient Descent(16/49): loss=2920428.306277464\n",
      "Gradient Descent(17/49): loss=8943810.757330837\n",
      "Gradient Descent(18/49): loss=27390419.513679907\n",
      "Gradient Descent(19/49): loss=83883158.83000635\n",
      "Gradient Descent(20/49): loss=256892172.9862652\n",
      "Gradient Descent(21/49): loss=786732278.8397585\n",
      "Gradient Descent(22/49): loss=2409367603.0159745\n",
      "Gradient Descent(23/49): loss=7378688283.306161\n",
      "Gradient Descent(24/49): loss=22597232866.696278\n",
      "Gradient Descent(25/49): loss=69204025653.32114\n",
      "Gradient Descent(26/49): loss=211937328562.35623\n",
      "Gradient Descent(27/49): loss=649058068721.3079\n",
      "Gradient Descent(28/49): loss=1987740335457.8838\n",
      "Gradient Descent(29/49): loss=6087454777338.484\n",
      "Gradient Descent(30/49): loss=18642830255596.94\n",
      "Gradient Descent(31/49): loss=57093667657759.08\n",
      "Gradient Descent(32/49): loss=174849357201879.34\n",
      "Gradient Descent(33/49): loss=535476156430701.2\n",
      "Gradient Descent(34/49): loss=1639895729069069.8\n",
      "Gradient Descent(35/49): loss=5022180670273892.0\n",
      "Gradient Descent(36/49): loss=1.538042830271367e+16\n",
      "Gradient Descent(37/49): loss=4.7102561677058104e+16\n",
      "Gradient Descent(38/49): loss=1.442515951359972e+17\n",
      "Gradient Descent(39/49): loss=4.417705101039681e+17\n",
      "Gradient Descent(40/49): loss=1.352922187193437e+18\n",
      "Gradient Descent(41/49): loss=4.1433241982800225e+18\n",
      "Gradient Descent(42/49): loss=1.2688930357233707e+19\n",
      "Gradient Descent(43/49): loss=3.885984921902629e+19\n",
      "Gradient Descent(44/49): loss=1.1900828823328059e+20\n",
      "Gradient Descent(45/49): loss=3.644628827144191e+20\n",
      "Gradient Descent(46/49): loss=1.1161675783128187e+21\n",
      "Gradient Descent(47/49): loss=3.4182632085828377e+21\n",
      "Gradient Descent(48/49): loss=1.0468431076285584e+22\n",
      "Gradient Descent(49/49): loss=3.20595701711229e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6031295007893338\n",
      "Gradient Descent(2/49): loss=0.9189635969566482\n",
      "Gradient Descent(3/49): loss=1.8862055164690374\n",
      "Gradient Descent(4/49): loss=4.8483838949755445\n",
      "Gradient Descent(5/49): loss=13.92005517915149\n",
      "Gradient Descent(6/49): loss=41.702048486937805\n",
      "Gradient Descent(7/49): loss=126.7844029920438\n",
      "Gradient Descent(8/49): loss=387.34911366392515\n",
      "Gradient Descent(9/49): loss=1185.3285400965713\n",
      "Gradient Descent(10/49): loss=3629.1405335466793\n",
      "Gradient Descent(11/49): loss=11113.31476348717\n",
      "Gradient Descent(12/49): loss=34033.59834268353\n",
      "Gradient Descent(13/49): loss=104226.96680397393\n",
      "Gradient Descent(14/49): loss=319194.15771667537\n",
      "Gradient Descent(15/49): loss=977531.1798868153\n",
      "Gradient Descent(16/49): loss=2993688.310283206\n",
      "Gradient Descent(17/49): loss=9168169.522122247\n",
      "Gradient Descent(18/49): loss=28077518.23337995\n",
      "Gradient Descent(19/49): loss=85987398.66160062\n",
      "Gradient Descent(20/49): loss=263336407.47305897\n",
      "Gradient Descent(21/49): loss=806467746.9581707\n",
      "Gradient Descent(22/49): loss=2469807474.1311407\n",
      "Gradient Descent(23/49): loss=7563785388.59769\n",
      "Gradient Descent(24/49): loss=23164092751.651417\n",
      "Gradient Descent(25/49): loss=70940034051.00317\n",
      "Gradient Descent(26/49): loss=217253854280.26245\n",
      "Gradient Descent(27/49): loss=665339928732.3167\n",
      "Gradient Descent(28/49): loss=2037603531741.8635\n",
      "Gradient Descent(29/49): loss=6240160815958.6455\n",
      "Gradient Descent(30/49): loss=19110492498870.883\n",
      "Gradient Descent(31/49): loss=58525883277786.46\n",
      "Gradient Descent(32/49): loss=179235517538231.38\n",
      "Gradient Descent(33/49): loss=548908772460805.75\n",
      "Gradient Descent(34/49): loss=1681033115661331.0\n",
      "Gradient Descent(35/49): loss=5148163916713109.0\n",
      "Gradient Descent(36/49): loss=1.5766251994932882e+16\n",
      "Gradient Descent(37/49): loss=4.828414673448592e+16\n",
      "Gradient Descent(38/49): loss=1.4787019937436906e+17\n",
      "Gradient Descent(39/49): loss=4.528524855840053e+17\n",
      "Gradient Descent(40/49): loss=1.3868607371009556e+18\n",
      "Gradient Descent(41/49): loss=4.2472610073716337e+18\n",
      "Gradient Descent(42/49): loss=1.3007236835076747e+19\n",
      "Gradient Descent(43/49): loss=3.983466280742567e+19\n",
      "Gradient Descent(44/49): loss=1.2199365484774679e+20\n",
      "Gradient Descent(45/49): loss=3.7360556797119785e+20\n",
      "Gradient Descent(46/49): loss=1.1441670519117302e+21\n",
      "Gradient Descent(47/49): loss=3.504011596479612e+21\n",
      "Gradient Descent(48/49): loss=1.0731035514218027e+22\n",
      "Gradient Descent(49/49): loss=3.2863796262293496e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6019363400000004\n",
      "Gradient Descent(2/49): loss=0.9141163812499922\n",
      "Gradient Descent(3/49): loss=1.870167757578044\n",
      "Gradient Descent(4/49): loss=4.798075097582965\n",
      "Gradient Descent(5/49): loss=13.764791326348814\n",
      "Gradient Descent(6/49): loss=41.22535977694141\n",
      "Gradient Descent(7/49): loss=125.32335065687136\n",
      "Gradient Descent(8/49): loss=382.8734477266736\n",
      "Gradient Descent(9/49): loss=1171.6206200028541\n",
      "Gradient Descent(10/49): loss=3587.158835098582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=10984.744618830422\n",
      "Gradient Descent(12/49): loss=33639.85108150683\n",
      "Gradient Descent(13/49): loss=103021.11462346354\n",
      "Gradient Descent(14/49): loss=315501.2342207089\n",
      "Gradient Descent(15/49): loss=966221.6004871855\n",
      "Gradient Descent(16/49): loss=2959052.7221784354\n",
      "Gradient Descent(17/49): loss=9062098.032357045\n",
      "Gradient Descent(18/49): loss=27752674.294782396\n",
      "Gradient Descent(19/49): loss=84992564.09845601\n",
      "Gradient Descent(20/49): loss=260289726.6222053\n",
      "Gradient Descent(21/49): loss=797137286.8512557\n",
      "Gradient Descent(22/49): loss=2441232940.052549\n",
      "Gradient Descent(23/49): loss=7476275877.981897\n",
      "Gradient Descent(24/49): loss=22896094875.390213\n",
      "Gradient Descent(25/49): loss=70119290554.955\n",
      "Gradient Descent(26/49): loss=214740327323.61664\n",
      "Gradient Descent(27/49): loss=657642252427.7216\n",
      "Gradient Descent(28/49): loss=2014029398059.112\n",
      "Gradient Descent(29/49): loss=6167965031555.621\n",
      "Gradient Descent(30/49): loss=18889392909137.64\n",
      "Gradient Descent(31/49): loss=57848765784230.766\n",
      "Gradient Descent(32/49): loss=177161845214219.03\n",
      "Gradient Descent(33/49): loss=542558150968563.44\n",
      "Gradient Descent(34/49): loss=1661584337341177.2\n",
      "Gradient Descent(35/49): loss=5088602033107219.0\n",
      "Gradient Descent(36/49): loss=1.5583843726391666e+16\n",
      "Gradient Descent(37/49): loss=4.772552141207938e+16\n",
      "Gradient Descent(38/49): loss=1.461594093244831e+17\n",
      "Gradient Descent(39/49): loss=4.4761319105623085e+17\n",
      "Gradient Descent(40/49): loss=1.370815397609829e+18\n",
      "Gradient Descent(41/49): loss=4.198122155180529e+18\n",
      "Gradient Descent(42/49): loss=1.285674910023973e+19\n",
      "Gradient Descent(43/49): loss=3.93737941194843e+19\n",
      "Gradient Descent(44/49): loss=1.2058224449093104e+20\n",
      "Gradient Descent(45/49): loss=3.692831237534814e+20\n",
      "Gradient Descent(46/49): loss=1.1309295664950905e+21\n",
      "Gradient Descent(47/49): loss=3.4634717973914195e+21\n",
      "Gradient Descent(48/49): loss=1.060688237951232e+22\n",
      "Gradient Descent(49/49): loss=3.2483577287253086e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6027680456000004\n",
      "Gradient Descent(2/49): loss=0.9174951852499593\n",
      "Gradient Descent(3/49): loss=1.8813470504281307\n",
      "Gradient Descent(4/49): loss=4.833143387535814\n",
      "Gradient Descent(5/49): loss=13.873019669928322\n",
      "Gradient Descent(6/49): loss=41.55764078475096\n",
      "Gradient Descent(7/49): loss=126.34179294888898\n",
      "Gradient Descent(8/49): loss=385.99325895158614\n",
      "Gradient Descent(9/49): loss=1181.1758735847454\n",
      "Gradient Descent(10/49): loss=3616.422630899027\n",
      "Gradient Descent(11/49): loss=11074.365825174438\n",
      "Gradient Descent(12/49): loss=33914.31685764533\n",
      "Gradient Descent(13/49): loss=103861.66689458118\n",
      "Gradient Descent(14/49): loss=318075.4263826924\n",
      "Gradient Descent(15/49): loss=974105.064814938\n",
      "Gradient Descent(16/49): loss=2983195.8325135936\n",
      "Gradient Descent(17/49): loss=9136036.308591403\n",
      "Gradient Descent(18/49): loss=27979110.266581275\n",
      "Gradient Descent(19/49): loss=85686024.26292512\n",
      "Gradient Descent(20/49): loss=262413448.37670347\n",
      "Gradient Descent(21/49): loss=803641184.7250735\n",
      "Gradient Descent(22/49): loss=2461151127.2918954\n",
      "Gradient Descent(23/49): loss=7537275326.402776\n",
      "Gradient Descent(24/49): loss=23082905686.18015\n",
      "Gradient Descent(25/49): loss=70691398663.00005\n",
      "Gradient Descent(26/49): loss=216492408404.48422\n",
      "Gradient Descent(27/49): loss=663008000737.8364\n",
      "Gradient Descent(28/49): loss=2030462002258.85\n",
      "Gradient Descent(29/49): loss=6218289881916.423\n",
      "Gradient Descent(30/49): loss=19043512763368.406\n",
      "Gradient Descent(31/49): loss=58320757837820.625\n",
      "Gradient Descent(32/49): loss=178607320878321.2\n",
      "Gradient Descent(33/49): loss=546984920189881.8\n",
      "Gradient Descent(34/49): loss=1675141318081422.0\n",
      "Gradient Descent(35/49): loss=5130120286624026.0\n",
      "Gradient Descent(36/49): loss=1.5710993377786106e+16\n",
      "Gradient Descent(37/49): loss=4.81149172194735e+16\n",
      "Gradient Descent(38/49): loss=1.473519339846263e+17\n",
      "Gradient Descent(39/49): loss=4.512652978278877e+17\n",
      "Gradient Descent(40/49): loss=1.3819999745979077e+18\n",
      "Gradient Descent(41/49): loss=4.2323749222062996e+18\n",
      "Gradient Descent(42/49): loss=1.296164819925749e+19\n",
      "Gradient Descent(43/49): loss=3.969504761022427e+19\n",
      "Gradient Descent(44/49): loss=1.2156608330631925e+20\n",
      "Gradient Descent(45/49): loss=3.722961301255854e+20\n",
      "Gradient Descent(46/49): loss=1.1401568985096568e+21\n",
      "Gradient Descent(47/49): loss=3.4917305016860064e+21\n",
      "Gradient Descent(48/49): loss=1.069342466141245e+22\n",
      "Gradient Descent(49/49): loss=3.2748613025574244e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6057714170448558\n",
      "Gradient Descent(2/49): loss=0.9408975748097888\n",
      "Gradient Descent(3/49): loss=2.0027112930720294\n",
      "Gradient Descent(4/49): loss=5.366961878013728\n",
      "Gradient Descent(5/49): loss=16.026253431342756\n",
      "Gradient Descent(6/49): loss=49.79915278890919\n",
      "Gradient Descent(7/49): loss=156.80520711341896\n",
      "Gradient Descent(8/49): loss=495.8431896351669\n",
      "Gradient Descent(9/49): loss=1570.0511334571622\n",
      "Gradient Descent(10/49): loss=4973.571582662874\n",
      "Gradient Descent(11/49): loss=15757.28577392773\n",
      "Gradient Descent(12/49): loss=49924.40581753605\n",
      "Gradient Descent(13/49): loss=158179.50896368382\n",
      "Gradient Descent(14/49): loss=501174.9777720007\n",
      "Gradient Descent(15/49): loss=1587921.821144262\n",
      "Gradient Descent(16/49): loss=5031170.519684905\n",
      "Gradient Descent(17/49): loss=15940759.696141563\n",
      "Gradient Descent(18/49): loss=50506702.042824395\n",
      "Gradient Descent(19/49): loss=160025433.77405247\n",
      "Gradient Descent(20/49): loss=507024583.3912605\n",
      "Gradient Descent(21/49): loss=1606456689.0385365\n",
      "Gradient Descent(22/49): loss=5089897372.57118\n",
      "Gradient Descent(23/49): loss=16126830834.27642\n",
      "Gradient Descent(24/49): loss=51096250814.345245\n",
      "Gradient Descent(25/49): loss=161893361079.1855\n",
      "Gradient Descent(26/49): loss=512942925242.28986\n",
      "Gradient Descent(27/49): loss=1625208364336.8137\n",
      "Gradient Descent(28/49): loss=5149310181564.003\n",
      "Gradient Descent(29/49): loss=16315074379268.443\n",
      "Gradient Descent(30/49): loss=51692681663267.4\n",
      "Gradient Descent(31/49): loss=163783092581905.2\n",
      "Gradient Descent(32/49): loss=518930350536505.06\n",
      "Gradient Descent(33/49): loss=1644178922639884.0\n",
      "Gradient Descent(34/49): loss=5209416498492000.0\n",
      "Gradient Descent(35/49): loss=1.6505515233824e+16\n",
      "Gradient Descent(36/49): loss=5.2296074466843656e+16\n",
      "Gradient Descent(37/49): loss=1.6569488234076195e+17\n",
      "Gradient Descent(38/49): loss=5.249876652084561e+17\n",
      "Gradient Descent(39/49): loss=1.6633709184465485e+18\n",
      "Gradient Descent(40/49): loss=5.270224418006067e+18\n",
      "Gradient Descent(41/49): loss=1.6698179046010604e+19\n",
      "Gradient Descent(42/49): loss=5.290651048937582e+19\n",
      "Gradient Descent(43/49): loss=1.6762898783453525e+20\n",
      "Gradient Descent(44/49): loss=5.3111568505491803e+20\n",
      "Gradient Descent(45/49): loss=1.6827869365281402e+21\n",
      "Gradient Descent(46/49): loss=5.331742129695758e+21\n",
      "Gradient Descent(47/49): loss=1.6893091763727412e+22\n",
      "Gradient Descent(48/49): loss=5.352407194419647e+22\n",
      "Gradient Descent(49/49): loss=1.6958566954797646e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6084247318844077\n",
      "Gradient Descent(2/49): loss=0.9519576523869073\n",
      "Gradient Descent(3/49): loss=2.0404073577072213\n",
      "Gradient Descent(4/49): loss=5.4890514040437\n",
      "Gradient Descent(5/49): loss=16.41573520045577\n",
      "Gradient Descent(6/49): loss=51.03584014101445\n",
      "Gradient Descent(7/49): loss=160.72618063466177\n",
      "Gradient Descent(8/49): loss=508.2690554547125\n",
      "Gradient Descent(9/49): loss=1609.4239000345792\n",
      "Gradient Descent(10/49): loss=5098.322909601282\n",
      "Gradient Descent(11/49): loss=16152.550531512772\n",
      "Gradient Descent(12/49): loss=51176.765328774214\n",
      "Gradient Descent(13/49): loss=162147.48749242217\n",
      "Gradient Descent(14/49): loss=513747.1235957675\n",
      "Gradient Descent(15/49): loss=1627755.4106255595\n",
      "Gradient Descent(16/49): loss=5157379.267250343\n",
      "Gradient Descent(17/49): loss=16340639.49458016\n",
      "Gradient Descent(18/49): loss=51773681.19885188\n",
      "Gradient Descent(19/49): loss=164039730.53468227\n",
      "Gradient Descent(20/49): loss=519743481.2503628\n",
      "Gradient Descent(21/49): loss=1646755245.017832\n",
      "Gradient Descent(22/49): loss=5217579317.338226\n",
      "Gradient Descent(23/49): loss=16531378308.07874\n",
      "Gradient Descent(24/49): loss=52378019030.34157\n",
      "Gradient Descent(25/49): loss=165954515494.77136\n",
      "Gradient Descent(26/49): loss=525810286892.7147\n",
      "Gradient Descent(27/49): loss=1665977312989.957\n",
      "Gradient Descent(28/49): loss=5278482518476.421\n",
      "Gradient Descent(29/49): loss=16724344011539.08\n",
      "Gradient Descent(30/49): loss=52989411566155.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=167891651606193.28\n",
      "Gradient Descent(32/49): loss=531947908949093.56\n",
      "Gradient Descent(33/49): loss=1685423754714192.0\n",
      "Gradient Descent(34/49): loss=5340096624436074.0\n",
      "Gradient Descent(35/49): loss=1.6919562144863988e+16\n",
      "Gradient Descent(36/49): loss=5.360794069978396e+16\n",
      "Gradient Descent(37/49): loss=1.6985139931319805e+17\n",
      "Gradient Descent(38/49): loss=5.3815717358399155e+17\n",
      "Gradient Descent(39/49): loss=1.7050971887834424e+18\n",
      "Gradient Descent(40/49): loss=5.402429932941612e+18\n",
      "Gradient Descent(41/49): loss=1.7117058999532618e+19\n",
      "Gradient Descent(42/49): loss=5.423368973412284e+19\n",
      "Gradient Descent(43/49): loss=1.7183402255358614e+20\n",
      "Gradient Descent(44/49): loss=5.444389170587528e+20\n",
      "Gradient Descent(45/49): loss=1.725000264808927e+21\n",
      "Gradient Descent(46/49): loss=5.465490839021017e+21\n",
      "Gradient Descent(47/49): loss=1.7316861174353079e+22\n",
      "Gradient Descent(48/49): loss=5.486674294482544e+22\n",
      "Gradient Descent(49/49): loss=1.7383978834638073e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6071703077120005\n",
      "Gradient Descent(2/49): loss=0.9467287106667583\n",
      "Gradient Descent(3/49): loss=2.022585554588566\n",
      "Gradient Descent(4/49): loss=5.4313303788704586\n",
      "Gradient Descent(5/49): loss=16.23159748012503\n",
      "Gradient Descent(6/49): loss=50.45116376374271\n",
      "Gradient Descent(7/49): loss=158.87243757675162\n",
      "Gradient Descent(8/49): loss=502.3944015258402\n",
      "Gradient Descent(9/49): loss=1590.80939210228\n",
      "Gradient Descent(10/49): loss=5039.343448244711\n",
      "Gradient Descent(11/49): loss=15965.678751727186\n",
      "Gradient Descent(12/49): loss=50584.679527275875\n",
      "Gradient Descent(13/49): loss=160271.5215845232\n",
      "Gradient Descent(14/49): loss=507803.3119587373\n",
      "Gradient Descent(15/49): loss=1608923.0365803798\n",
      "Gradient Descent(16/49): loss=5097710.772071344\n",
      "Gradient Descent(17/49): loss=16151585.83320069\n",
      "Gradient Descent(18/49): loss=51174683.576884285\n",
      "Gradient Descent(19/49): loss=162141866.4679684\n",
      "Gradient Descent(20/49): loss=513730288.7401328\n",
      "Gradient Descent(21/49): loss=1627703045.8672776\n",
      "Gradient Descent(22/49): loss=5157214329.548527\n",
      "Gradient Descent(23/49): loss=16340117880.764467\n",
      "Gradient Descent(24/49): loss=51772029492.44108\n",
      "Gradient Descent(25/49): loss=164034498242.8807\n",
      "Gradient Descent(26/49): loss=519726904231.77765\n",
      "Gradient Descent(27/49): loss=1646702723367.085\n",
      "Gradient Descent(28/49): loss=5217412908714.98\n",
      "Gradient Descent(29/49): loss=16530851059970.84\n",
      "Gradient Descent(30/49): loss=52376348498408.13\n",
      "Gradient Descent(31/49): loss=165949222582341.9\n",
      "Gradient Descent(32/49): loss=525793516829926.94\n",
      "Gradient Descent(33/49): loss=1665924178723912.0\n",
      "Gradient Descent(34/49): loss=5278314167869250.0\n",
      "Gradient Descent(35/49): loss=1.6723810609476488e+16\n",
      "Gradient Descent(36/49): loss=5.2987721535064104e+16\n",
      "Gradient Descent(37/49): loss=1.6788629691170083e+17\n",
      "Gradient Descent(38/49): loss=5.319309431350375e+17\n",
      "Gradient Descent(39/49): loss=1.685370000229199e+18\n",
      "Gradient Descent(40/49): loss=5.339926308726374e+18\n",
      "Gradient Descent(41/49): loss=1.6919022516567413e+19\n",
      "Gradient Descent(42/49): loss=5.360623094149458e+19\n",
      "Gradient Descent(43/49): loss=1.6984598211503863e+20\n",
      "Gradient Descent(44/49): loss=5.381400097332295e+20\n",
      "Gradient Descent(45/49): loss=1.7050428068387936e+21\n",
      "Gradient Descent(46/49): loss=5.402257629188e+21\n",
      "Gradient Descent(47/49): loss=1.7116513072318777e+22\n",
      "Gradient Descent(48/49): loss=5.423196001833721e+22\n",
      "Gradient Descent(49/49): loss=1.718285421221161e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6080447176140805\n",
      "Gradient Descent(2/49): loss=0.9503736009025425\n",
      "Gradient Descent(3/49): loss=2.035008434713859\n",
      "Gradient Descent(4/49): loss=5.471565442161574\n",
      "Gradient Descent(5/49): loss=16.35995266455925\n",
      "Gradient Descent(6/49): loss=50.85871874000111\n",
      "Gradient Descent(7/49): loss=160.16460917343738\n",
      "Gradient Descent(8/49): loss=506.48939242275344\n",
      "Gradient Descent(9/49): loss=1603.784835669715\n",
      "Gradient Descent(10/49): loss=5080.4557180536\n",
      "Gradient Descent(11/49): loss=16095.939741798105\n",
      "Gradient Descent(12/49): loss=50997.399322632264\n",
      "Gradient Descent(13/49): loss=161579.1838585534\n",
      "Gradient Descent(14/49): loss=511946.50998217636\n",
      "Gradient Descent(15/49): loss=1622050.3460723818\n",
      "Gradient Descent(16/49): loss=5139303.340340329\n",
      "Gradient Descent(17/49): loss=16283367.727378285\n",
      "Gradient Descent(18/49): loss=51592221.33126695\n",
      "Gradient Descent(19/49): loss=163464793.08983076\n",
      "Gradient Descent(20/49): loss=517921849.449694\n",
      "Gradient Descent(21/49): loss=1640983586.8201594\n",
      "Gradient Descent(22/49): loss=5199292395.505352\n",
      "Gradient Descent(23/49): loss=16473438024.941301\n",
      "Gradient Descent(24/49): loss=52194441037.24224\n",
      "Gradient Descent(25/49): loss=165372866981.43002\n",
      "Gradient Descent(26/49): loss=523967391742.97455\n",
      "Gradient Descent(27/49): loss=1660138283997.4233\n",
      "Gradient Descent(28/49): loss=5259982139017.003\n",
      "Gradient Descent(29/49): loss=16665727409261.287\n",
      "Gradient Descent(30/49): loss=52803690723509.19\n",
      "Gradient Descent(31/49): loss=167303213688353.03\n",
      "Gradient Descent(32/49): loss=530083502250117.1\n",
      "Gradient Descent(33/49): loss=1679516568529412.8\n",
      "Gradient Descent(34/49): loss=5321380295728354.0\n",
      "Gradient Descent(35/49): loss=1.686026132898776e+16\n",
      "Gradient Descent(36/49): loss=5.342005199476269e+16\n",
      "Gradient Descent(37/49): loss=1.692560927402131e+17\n",
      "Gradient Descent(38/49): loss=5.362710042380749e+17\n",
      "Gradient Descent(39/49): loss=1.6991210498278395e+18\n",
      "Gradient Descent(40/49): loss=5.383495134274786e+18\n",
      "Gradient Descent(41/49): loss=1.7057065983437445e+19\n",
      "Gradient Descent(42/49): loss=5.404360786192572e+19\n",
      "Gradient Descent(43/49): loss=1.7123176714971434e+20\n",
      "Gradient Descent(44/49): loss=5.42530731037125e+20\n",
      "Gradient Descent(45/49): loss=1.7189543682182147e+21\n",
      "Gradient Descent(46/49): loss=5.446335020262963e+21\n",
      "Gradient Descent(47/49): loss=1.7256167878201696e+22\n",
      "Gradient Descent(48/49): loss=5.467444230529784e+22\n",
      "Gradient Descent(49/49): loss=1.7323050300009362e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6110248673380357\n",
      "Gradient Descent(2/49): loss=0.9747534352241465\n",
      "Gradient Descent(3/49): loss=2.1663645964757627\n",
      "Gradient Descent(4/49): loss=6.070201921852\n",
      "Gradient Descent(5/49): loss=18.85956338351707\n",
      "Gradient Descent(6/49): loss=60.75879046807654\n",
      "Gradient Descent(7/49): loss=198.0248483198182\n",
      "Gradient Descent(8/49): loss=647.7221804479362\n",
      "Gradient Descent(9/49): loss=2120.9756102328834\n",
      "Gradient Descent(10/49): loss=6947.501171550901\n",
      "Gradient Descent(11/49): loss=22759.68156298372\n",
      "Gradient Descent(12/49): loss=74561.96574335509\n",
      "Gradient Descent(13/49): loss=244271.4289466836\n",
      "Gradient Descent(14/49): loss=800256.6013470573\n",
      "Gradient Descent(15/49): loss=2621719.624648089\n",
      "Gradient Descent(16/49): loss=8589014.63528357\n",
      "Gradient Descent(17/49): loss=28138469.81962605\n",
      "Gradient Descent(18/49): loss=92184439.9490416\n",
      "Gradient Descent(19/49): loss=302005442.69002473\n",
      "Gradient Descent(20/49): loss=989400029.7696843\n",
      "Gradient Descent(21/49): loss=3241373436.501816\n",
      "Gradient Descent(22/49): loss=10619063514.29579\n",
      "Gradient Descent(23/49): loss=34789113978.1577\n",
      "Gradient Descent(24/49): loss=113972616302.8147\n",
      "Gradient Descent(25/49): loss=373385688268.62634\n",
      "Gradient Descent(26/49): loss=1223248853335.8032\n",
      "Gradient Descent(27/49): loss=4007485568412.444\n",
      "Gradient Descent(28/49): loss=13128923470676.1\n",
      "Gradient Descent(29/49): loss=43011666182285.59\n",
      "Gradient Descent(30/49): loss=140910519579776.8\n",
      "Gradient Descent(31/49): loss=461636953195345.25\n",
      "Gradient Descent(32/49): loss=1512368822363390.0\n",
      "Gradient Descent(33/49): loss=4954671498944445.0\n",
      "Gradient Descent(34/49): loss=1.6231999297692234e+16\n",
      "Gradient Descent(35/49): loss=5.317765289916699e+16\n",
      "Gradient Descent(36/49): loss=1.7421530866296355e+17\n",
      "Gradient Descent(37/49): loss=5.707467727107171e+17\n",
      "Gradient Descent(38/49): loss=1.869823502077715e+18\n",
      "Gradient Descent(39/49): loss=6.125728775156114e+18\n",
      "Gradient Descent(40/49): loss=2.0068500040288862e+19\n",
      "Gradient Descent(41/49): loss=6.574641298198562e+19\n",
      "Gradient Descent(42/49): loss=2.1539182357029267e+20\n",
      "Gradient Descent(43/49): loss=7.056451531985802e+20\n",
      "Gradient Descent(44/49): loss=2.3117640863941127e+21\n",
      "Gradient Descent(45/49): loss=7.573570323435993e+21\n",
      "Gradient Descent(46/49): loss=2.481177373660901e+22\n",
      "Gradient Descent(47/49): loss=8.12858519385049e+22\n",
      "Gradient Descent(48/49): loss=2.663005795357154e+23\n",
      "Gradient Descent(49/49): loss=8.724273286170061e+23\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.6138099669074436\n",
      "Gradient Descent(2/49): loss=0.9866627994929459\n",
      "Gradient Descent(3/49): loss=2.208165964326161\n",
      "Gradient Descent(4/49): loss=6.2099324826364874\n",
      "Gradient Descent(5/49): loss=19.320119773272225\n",
      "Gradient Descent(6/49): loss=62.27040435612888\n",
      "Gradient Descent(7/49): loss=202.97983167799933\n",
      "Gradient Descent(8/49): loss=663.9579865272243\n",
      "Gradient Descent(9/49): loss=2174.1685196285644\n",
      "Gradient Descent(10/49): loss=7121.769247121624\n",
      "Gradient Descent(11/49): loss=23330.603990461124\n",
      "Gradient Descent(12/49): loss=76432.3674931153\n",
      "Gradient Descent(13/49): loss=250399.0549041865\n",
      "Gradient Descent(14/49): loss=820331.3195316619\n",
      "Gradient Descent(15/49): loss=2687486.4116778304\n",
      "Gradient Descent(16/49): loss=8804473.209057722\n",
      "Gradient Descent(17/49): loss=28844333.65595276\n",
      "Gradient Descent(18/49): loss=94496920.46603073\n",
      "Gradient Descent(19/49): loss=309581360.114525\n",
      "Gradient Descent(20/49): loss=1014219492.8470577\n",
      "Gradient Descent(21/49): loss=3322684479.491741\n",
      "Gradient Descent(22/49): loss=10885446622.23968\n",
      "Gradient Descent(23/49): loss=35661811678.091736\n",
      "Gradient Descent(24/49): loss=116831661237.58096\n",
      "Gradient Descent(25/49): loss=382752205379.3936\n",
      "Gradient Descent(26/49): loss=1253934500042.4395\n",
      "Gradient Descent(27/49): loss=4108014815588.0435\n",
      "Gradient Descent(28/49): loss=13458267337345.562\n",
      "Gradient Descent(29/49): loss=44090629623880.875\n",
      "Gradient Descent(30/49): loss=144445311710808.7\n",
      "Gradient Descent(31/49): loss=473217285695757.7\n",
      "Gradient Descent(32/49): loss=1550307149667826.0\n",
      "Gradient Descent(33/49): loss=5078961253026841.0\n",
      "Gradient Descent(34/49): loss=1.6639184961041168e+16\n",
      "Gradient Descent(35/49): loss=5.451163385086971e+16\n",
      "Gradient Descent(36/49): loss=1.7858556365882666e+17\n",
      "Gradient Descent(37/49): loss=5.850641651026742e+17\n",
      "Gradient Descent(38/49): loss=1.9167287112929152e+18\n",
      "Gradient Descent(39/49): loss=6.279394931067228e+18\n",
      "Gradient Descent(40/49): loss=2.0571925733668725e+19\n",
      "Gradient Descent(41/49): loss=6.739568589607078e+19\n",
      "Gradient Descent(42/49): loss=2.2079500656410516e+20\n",
      "Gradient Descent(43/49): loss=7.233465210047272e+20\n",
      "Gradient Descent(44/49): loss=2.369755537463617e+21\n",
      "Gradient Descent(45/49): loss=7.76355611628451e+21\n",
      "Gradient Descent(46/49): loss=2.5434186192557714e+22\n",
      "Gradient Descent(47/49): loss=8.332493738543993e+22\n",
      "Gradient Descent(48/49): loss=2.729808273684174e+23\n",
      "Gradient Descent(49/49): loss=8.943124885416605e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6124932380480004\n",
      "Gradient Descent(2/49): loss=0.9810323352170639\n",
      "Gradient Descent(3/49): loss=2.1884032714526827\n",
      "Gradient Descent(4/49): loss=6.143871195654248\n",
      "Gradient Descent(5/49): loss=19.102379662130705\n",
      "Gradient Descent(6/49): loss=61.55574924915451\n",
      "Gradient Descent(7/49): loss=200.6372333532288\n",
      "Gradient Descent(8/49): loss=656.2820834266068\n",
      "Gradient Descent(9/49): loss=2149.020176752106\n",
      "Gradient Descent(10/49): loss=7039.379444296392\n",
      "Gradient Descent(11/49): loss=23060.68544069947\n",
      "Gradient Descent(12/49): loss=75548.08601551237\n",
      "Gradient Descent(13/49): loss=247502.05903866357\n",
      "Gradient Descent(14/49): loss=810840.4700598483\n",
      "Gradient Descent(15/49): loss=2656393.4384064325\n",
      "Gradient Descent(16/49): loss=8702609.518005932\n",
      "Gradient Descent(17/49): loss=28510618.0163795\n",
      "Gradient Descent(18/49): loss=93403634.65790208\n",
      "Gradient Descent(19/49): loss=305999646.4772237\n",
      "Gradient Descent(20/49): loss=1002485440.7984991\n",
      "Gradient Descent(21/49): loss=3284242551.57419\n",
      "Gradient Descent(22/49): loss=10759507022.187006\n",
      "Gradient Descent(23/49): loss=35249220954.364204\n",
      "Gradient Descent(24/49): loss=115479972767.57104\n",
      "Gradient Descent(25/49): loss=378323938782.81964\n",
      "Gradient Descent(26/49): loss=1239427055845.2976\n",
      "Gradient Descent(27/49): loss=4060486977653.864\n",
      "Gradient Descent(28/49): loss=13302561387490.979\n",
      "Gradient Descent(29/49): loss=43580521361560.61\n",
      "Gradient Descent(30/49): loss=142774146032617.75\n",
      "Gradient Descent(31/49): loss=467742379817438.4\n",
      "Gradient Descent(32/49): loss=1532370810519895.2\n",
      "Gradient Descent(33/49): loss=5020200012344401.0\n",
      "Gradient Descent(34/49): loss=1.644667726044002e+16\n",
      "Gradient Descent(35/49): loss=5.388095937292683e+16\n",
      "Gradient Descent(36/49): loss=1.7651941100165648e+17\n",
      "Gradient Descent(37/49): loss=5.782952423825562e+17\n",
      "Gradient Descent(38/49): loss=1.8945530435696556e+18\n",
      "Gradient Descent(39/49): loss=6.206745226038692e+18\n",
      "Gradient Descent(40/49): loss=2.033391803502478e+19\n",
      "Gradient Descent(41/49): loss=6.661594887453763e+19\n",
      "Gradient Descent(42/49): loss=2.18240510107888e+20\n",
      "Gradient Descent(43/49): loss=7.149777351643984e+20\n",
      "Gradient Descent(44/49): loss=2.3423385581720342e+21\n",
      "Gradient Descent(45/49): loss=7.673735350428145e+21\n",
      "Gradient Descent(46/49): loss=2.513992438153833e+22\n",
      "Gradient Descent(47/49): loss=8.236090626635365e+22\n",
      "Gradient Descent(48/49): loss=2.6982256501921734e+23\n",
      "Gradient Descent(49/49): loss=8.83965705259451e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6134110781043205\n",
      "Gradient Descent(2/49): loss=0.984957111081878\n",
      "Gradient Descent(3/49): loss=2.2021790697195223\n",
      "Gradient Descent(4/49): loss=6.189919928411813\n",
      "Gradient Descent(5/49): loss=19.25415775557441\n",
      "Gradient Descent(6/49): loss=62.0539073011368\n",
      "Gradient Descent(7/49): loss=202.27016678734466\n",
      "Gradient Descent(8/49): loss=661.6326544900468\n",
      "Gradient Descent(9/49): loss=2166.5501004527996\n",
      "Gradient Descent(10/49): loss=7096.810145171575\n",
      "Gradient Descent(11/49): loss=23248.83507767399\n",
      "Gradient Descent(12/49): loss=76164.48395904311\n",
      "Gradient Descent(13/49): loss=249521.44125927685\n",
      "Gradient Descent(14/49): loss=817456.1690705355\n",
      "Gradient Descent(15/49): loss=2678067.1308528683\n",
      "Gradient Descent(16/49): loss=8773614.70274839\n",
      "Gradient Descent(17/49): loss=28743238.103036355\n",
      "Gradient Descent(18/49): loss=94165721.32472317\n",
      "Gradient Descent(19/49): loss=308496318.60727894\n",
      "Gradient Descent(20/49): loss=1010664788.3645927\n",
      "Gradient Descent(21/49): loss=3311038912.1363373\n",
      "Gradient Descent(22/49): loss=10847294579.025393\n",
      "Gradient Descent(23/49): loss=35536821769.32154\n",
      "Gradient Descent(24/49): loss=116422181797.46155\n",
      "Gradient Descent(25/49): loss=381410709785.6456\n",
      "Gradient Descent(26/49): loss=1249539626327.8027\n",
      "Gradient Descent(27/49): loss=4093616769811.374\n",
      "Gradient Descent(28/49): loss=13411097899577.9\n",
      "Gradient Descent(29/49): loss=43936097828803.945\n",
      "Gradient Descent(30/49): loss=143939050096935.0\n",
      "Gradient Descent(31/49): loss=471558722022565.75\n",
      "Gradient Descent(32/49): loss=1544873529218062.5\n",
      "Gradient Descent(33/49): loss=5061160169071366.0\n",
      "Gradient Descent(34/49): loss=1.6580866829893308e+16\n",
      "Gradient Descent(35/49): loss=5.432057782141642e+16\n",
      "Gradient Descent(36/49): loss=1.7795964500074787e+17\n",
      "Gradient Descent(37/49): loss=5.83013592986946e+17\n",
      "Gradient Descent(38/49): loss=1.9100108319846664e+18\n",
      "Gradient Descent(39/49): loss=6.257386486664723e+18\n",
      "Gradient Descent(40/49): loss=2.0499823868962443e+19\n",
      "Gradient Descent(41/49): loss=6.715947297711324e+19\n",
      "Gradient Descent(42/49): loss=2.2002114942034474e+20\n",
      "Gradient Descent(43/49): loss=7.208112876159557e+20\n",
      "Gradient Descent(44/49): loss=2.361449859358511e+21\n",
      "Gradient Descent(45/49): loss=7.736345884245131e+21\n",
      "Gradient Descent(46/49): loss=2.5345042751376213e+22\n",
      "Gradient Descent(47/49): loss=8.303289455778483e+22\n",
      "Gradient Descent(48/49): loss=2.720240658607432e+23\n",
      "Gradient Descent(49/49): loss=8.911780421664552e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.616366119028873\n",
      "Gradient Descent(2/49): loss=1.0103352516130744\n",
      "Gradient Descent(3/49): loss=2.3441571468899083\n",
      "Gradient Descent(4/49): loss=6.859944555539857\n",
      "Gradient Descent(5/49): loss=22.148594406263694\n",
      "Gradient Descent(6/49): loss=73.9098473408788\n",
      "Gradient Descent(7/49): loss=249.15274527629174\n",
      "Gradient Descent(8/49): loss=842.4551005265\n",
      "Gradient Descent(9/49): loss=2851.139554461326\n",
      "Gradient Descent(10/49): loss=9651.741641702176\n",
      "Gradient Descent(11/49): loss=32675.86006826441\n",
      "Gradient Descent(12/49): loss=110626.31541322701\n",
      "Gradient Descent(13/49): loss=374535.3770291136\n",
      "Gradient Descent(14/49): loss=1268025.896035915\n",
      "Gradient Descent(15/49): loss=4293027.397185377\n",
      "Gradient Descent(16/49): loss=14534472.479478223\n",
      "Gradient Descent(17/49): loss=49207908.95009029\n",
      "Gradient Descent(18/49): loss=166598295.46497747\n",
      "Gradient Descent(19/49): loss=564035188.0498352\n",
      "Gradient Descent(20/49): loss=1909597531.5849423\n",
      "Gradient Descent(21/49): loss=6465133401.8579855\n",
      "Gradient Descent(22/49): loss=21888355644.254463\n",
      "Gradient Descent(23/49): loss=74105216868.1062\n",
      "Gradient Descent(24/49): loss=250890622227.58856\n",
      "Gradient Descent(25/49): loss=849415290612.6646\n",
      "Gradient Descent(26/49): loss=2875780407897.4155\n",
      "Gradient Descent(27/49): loss=9736242148976.996\n",
      "Gradient Descent(28/49): loss=32963021419571.99\n",
      "Gradient Descent(29/49): loss=111599605318100.84\n",
      "Gradient Descent(30/49): loss=377831623764944.2\n",
      "Gradient Descent(31/49): loss=1279186745418585.8\n",
      "Gradient Descent(32/49): loss=4330814645289417.0\n",
      "Gradient Descent(33/49): loss=1.4662406063092594e+16\n",
      "Gradient Descent(34/49): loss=4.9641041967208344e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1.680647116841927e+17\n",
      "Gradient Descent(36/49): loss=5.689998878780673e+17\n",
      "Gradient Descent(37/49): loss=1.926406020399813e+18\n",
      "Gradient Descent(38/49): loss=6.522040222665804e+18\n",
      "Gradient Descent(39/49): loss=2.208101937785758e+19\n",
      "Gradient Descent(40/49): loss=7.475749920567638e+19\n",
      "Gradient Descent(41/49): loss=2.5309898931074987e+20\n",
      "Gradient Descent(42/49): loss=8.568919382105563e+20\n",
      "Gradient Descent(43/49): loss=2.9010933460058044e+21\n",
      "Gradient Descent(44/49): loss=9.821941632236847e+21\n",
      "Gradient Descent(45/49): loss=3.3253165590100566e+22\n",
      "Gradient Descent(46/49): loss=1.1258191742184145e+23\n",
      "Gradient Descent(47/49): loss=3.811573396233855e+23\n",
      "Gradient Descent(48/49): loss=1.290446289029018e+24\n",
      "Gradient Descent(49/49): loss=4.3689349561368357e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.619285205858441\n",
      "Gradient Descent(2/49): loss=1.023137198812752\n",
      "Gradient Descent(3/49): loss=2.3904185061588086\n",
      "Gradient Descent(4/49): loss=7.0194861003093525\n",
      "Gradient Descent(5/49): loss=22.691657347066943\n",
      "Gradient Descent(6/49): loss=75.75136032008554\n",
      "Gradient Descent(7/49): loss=255.39029070553218\n",
      "Gradient Descent(8/49): loss=863.5758534184502\n",
      "Gradient Descent(9/49): loss=2922.648894539228\n",
      "Gradient Descent(10/49): loss=9893.846582558677\n",
      "Gradient Descent(11/49): loss=33495.533475114964\n",
      "Gradient Descent(12/49): loss=113401.4046185643\n",
      "Gradient Descent(13/49): loss=383930.7219617899\n",
      "Gradient Descent(14/49): loss=1299834.7787590607\n",
      "Gradient Descent(15/49): loss=4400719.5534521565\n",
      "Gradient Descent(16/49): loss=14899075.046651373\n",
      "Gradient Descent(17/49): loss=50442307.40442775\n",
      "Gradient Descent(18/49): loss=170777474.87491333\n",
      "Gradient Descent(19/49): loss=578184217.8630095\n",
      "Gradient Descent(20/49): loss=1957500486.923612\n",
      "Gradient Descent(21/49): loss=6627313647.45569\n",
      "Gradient Descent(22/49): loss=22437433083.75162\n",
      "Gradient Descent(23/49): loss=75964173447.27505\n",
      "Gradient Descent(24/49): loss=257184305622.02466\n",
      "Gradient Descent(25/49): loss=870723185112.7881\n",
      "Gradient Descent(26/49): loss=2947920415516.624\n",
      "Gradient Descent(27/49): loss=9980479358771.398\n",
      "Gradient Descent(28/49): loss=33789910917052.027\n",
      "Gradient Descent(29/49): loss=114399122400765.67\n",
      "Gradient Descent(30/49): loss=387309668800015.2\n",
      "Gradient Descent(31/49): loss=1311275614689208.8\n",
      "Gradient Descent(32/49): loss=4439454721092129.0\n",
      "Gradient Descent(33/49): loss=1.5030217903730734e+16\n",
      "Gradient Descent(34/49): loss=5.088630573487699e+16\n",
      "Gradient Descent(35/49): loss=1.72280676695987e+17\n",
      "Gradient Descent(36/49): loss=5.832734590219197e+17\n",
      "Gradient Descent(37/49): loss=1.9747306228647388e+18\n",
      "Gradient Descent(38/49): loss=6.68564799677041e+18\n",
      "Gradient Descent(39/49): loss=2.263492985786584e+19\n",
      "Gradient Descent(40/49): loss=7.663281852679122e+19\n",
      "Gradient Descent(41/49): loss=2.5944807040430975e+20\n",
      "Gradient Descent(42/49): loss=8.783873871608933e+20\n",
      "Gradient Descent(43/49): loss=2.973868337972141e+21\n",
      "Gradient Descent(44/49): loss=1.0068328645038818e+22\n",
      "Gradient Descent(45/49): loss=3.4087333460646006e+22\n",
      "Gradient Descent(46/49): loss=1.1540607616436829e+23\n",
      "Gradient Descent(47/49): loss=3.9071881146205716e+23\n",
      "Gradient Descent(48/49): loss=1.3228176080858805e+24\n",
      "Gradient Descent(49/49): loss=4.4785312939351073e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6179051310080004\n",
      "Gradient Descent(2/49): loss=1.0170847425486378\n",
      "Gradient Descent(3/49): loss=2.3685472353807446\n",
      "Gradient Descent(4/49): loss=6.944058651112688\n",
      "Gradient Descent(5/49): loss=22.43491010021704\n",
      "Gradient Descent(6/49): loss=74.88073676630391\n",
      "Gradient Descent(7/49): loss=252.44132752699693\n",
      "Gradient Descent(8/49): loss=853.5904636064975\n",
      "Gradient Descent(9/49): loss=2888.840978716866\n",
      "Gradient Descent(10/49): loss=9779.385122675656\n",
      "Gradient Descent(11/49): loss=33108.01137646114\n",
      "Gradient Descent(12/49): loss=112089.40842126745\n",
      "Gradient Descent(13/49): loss=379488.8262561635\n",
      "Gradient Descent(14/49): loss=1284796.2952780263\n",
      "Gradient Descent(15/49): loss=4349805.262398755\n",
      "Gradient Descent(16/49): loss=14726699.621481378\n",
      "Gradient Descent(17/49): loss=49858713.16359507\n",
      "Gradient Descent(18/49): loss=168801658.21176454\n",
      "Gradient Descent(19/49): loss=571494892.966822\n",
      "Gradient Descent(20/49): loss=1934853108.5534773\n",
      "Gradient Descent(21/49): loss=6550638683.2438135\n",
      "Gradient Descent(22/49): loss=22177842324.915348\n",
      "Gradient Descent(23/49): loss=75085302974.15572\n",
      "Gradient Descent(24/49): loss=254208801748.2316\n",
      "Gradient Descent(25/49): loss=860649319197.6661\n",
      "Gradient Descent(26/49): loss=2913814335074.636\n",
      "Gradient Descent(27/49): loss=9865009812827.178\n",
      "Gradient Descent(28/49): loss=33398977222308.4\n",
      "Gradient Descent(29/49): loss=113075577283841.9\n",
      "Gradient Descent(30/49): loss=382828674452197.94\n",
      "Gradient Descent(31/49): loss=1296104760225376.5\n",
      "Gradient Descent(32/49): loss=4388092276218750.5\n",
      "Gradient Descent(33/49): loss=1.4856325210367812e+16\n",
      "Gradient Descent(34/49): loss=5.0297574632222824e+16\n",
      "Gradient Descent(35/49): loss=1.7028746867486957e+17\n",
      "Gradient Descent(36/49): loss=5.765252539456443e+17\n",
      "Gradient Descent(37/49): loss=1.951883899758478e+18\n",
      "Gradient Descent(38/49): loss=6.608298131022679e+18\n",
      "Gradient Descent(39/49): loss=2.237305415239135e+19\n",
      "Gradient Descent(40/49): loss=7.57462121383315e+19\n",
      "Gradient Descent(41/49): loss=2.5644637581552707e+20\n",
      "Gradient Descent(42/49): loss=8.682248499609964e+20\n",
      "Gradient Descent(43/49): loss=2.939462052027931e+21\n",
      "Gradient Descent(44/49): loss=9.95184272334589e+21\n",
      "Gradient Descent(45/49): loss=3.369295872416067e+22\n",
      "Gradient Descent(46/49): loss=1.1407088105651985e+23\n",
      "Gradient Descent(47/49): loss=3.861983749049915e+23\n",
      "Gradient Descent(48/49): loss=1.3075132180784203e+24\n",
      "Gradient Descent(49/49): loss=4.4267167511260555e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6188671270707203\n",
      "Gradient Descent(2/49): loss=1.0213036724814053\n",
      "Gradient Descent(3/49): loss=2.383792840623632\n",
      "Gradient Descent(4/49): loss=6.996636168286565\n",
      "Gradient Descent(5/49): loss=22.613878538423467\n",
      "Gradient Descent(6/49): loss=75.48761430674989\n",
      "Gradient Descent(7/49): loss=254.49693412401922\n",
      "Gradient Descent(8/49): loss=860.5508872974318\n",
      "Gradient Descent(9/49): loss=2912.4071511613693\n",
      "Gradient Descent(10/49): loss=9859.171718098303\n",
      "Gradient Descent(11/49): loss=33378.13783591972\n",
      "Gradient Descent(12/49): loss=113003.9495244115\n",
      "Gradient Descent(13/49): loss=382585.0975769966\n",
      "Gradient Descent(14/49): loss=1295279.0324238653\n",
      "Gradient Descent(15/49): loss=4385295.618241358\n",
      "Gradient Descent(16/49): loss=14846855.771185553\n",
      "Gradient Descent(17/49): loss=50265513.82498936\n",
      "Gradient Descent(18/49): loss=170178922.5319669\n",
      "Gradient Descent(19/49): loss=576157759.0502924\n",
      "Gradient Descent(20/49): loss=1950639707.966728\n",
      "Gradient Descent(21/49): loss=6604085794.21845\n",
      "Gradient Descent(22/49): loss=22358792863.832066\n",
      "Gradient Descent(23/49): loss=75697929118.71887\n",
      "Gradient Descent(24/49): loss=256282908823.28323\n",
      "Gradient Descent(25/49): loss=867671416110.9493\n",
      "Gradient Descent(26/49): loss=2937588346383.982\n",
      "Gradient Descent(27/49): loss=9945499105516.393\n",
      "Gradient Descent(28/49): loss=33671481771635.04\n",
      "Gradient Descent(29/49): loss=113998168686047.83\n",
      "Gradient Descent(30/49): loss=385952199903511.4\n",
      "Gradient Descent(31/49): loss=1306679767993288.8\n",
      "Gradient Descent(32/49): loss=4423895022518173.0\n",
      "Gradient Descent(33/49): loss=1.4977538988236364e+16\n",
      "Gradient Descent(34/49): loss=5.070795599857032e+16\n",
      "Gradient Descent(35/49): loss=1.716768558287713e+17\n",
      "Gradient Descent(36/49): loss=5.812291630939327e+17\n",
      "Gradient Descent(37/49): loss=1.9678094545709332e+18\n",
      "Gradient Descent(38/49): loss=6.662215689395405e+18\n",
      "Gradient Descent(39/49): loss=2.255559743801627e+19\n",
      "Gradient Descent(40/49): loss=7.636423068615111e+19\n",
      "Gradient Descent(41/49): loss=2.5853873941104237e+20\n",
      "Gradient Descent(42/49): loss=8.753087561499705e+20\n",
      "Gradient Descent(43/49): loss=2.963445324821542e+21\n",
      "Gradient Descent(44/49): loss=1.0033040491715062e+22\n",
      "Gradient Descent(45/49): loss=3.39678618887491e+22\n",
      "Gradient Descent(46/49): loss=1.1500159321054284e+23\n",
      "Gradient Descent(47/49): loss=3.8934939397362914e+23\n",
      "Gradient Descent(48/49): loss=1.318181308237071e+24\n",
      "Gradient Descent(49/49): loss=4.462834637167652e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6217951721173679\n",
      "Gradient Descent(2/49): loss=1.047700709494601\n",
      "Gradient Descent(3/49): loss=2.537049783148822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=7.745154558811092\n",
      "Gradient Descent(5/49): loss=25.957376148823847\n",
      "Gradient Descent(6/49): loss=89.6436938269302\n",
      "Gradient Descent(7/49): loss=312.34837811551404\n",
      "Gradient Descent(8/49): loss=1091.1243886043267\n",
      "Gradient Descent(9/49): loss=3814.4262196827126\n",
      "Gradient Descent(10/49): loss=13337.540392780598\n",
      "Gradient Descent(11/49): loss=46638.918344683916\n",
      "Gradient Descent(12/49): loss=163090.50690470342\n",
      "Gradient Descent(13/49): loss=570310.0669402297\n",
      "Gradient Descent(14/49): loss=1994316.1464283613\n",
      "Gradient Descent(15/49): loss=6973923.005790016\n",
      "Gradient Descent(16/49): loss=24387110.232290387\n",
      "Gradient Descent(17/49): loss=85279284.6446388\n",
      "Gradient Descent(18/49): loss=298213129.3472166\n",
      "Gradient Descent(19/49): loss=1042821490.8876497\n",
      "Gradient Descent(20/49): loss=3646642470.3586826\n",
      "Gradient Descent(21/49): loss=12751944053.469538\n",
      "Gradient Descent(22/49): loss=44592273159.44973\n",
      "Gradient Descent(23/49): loss=155934720010.14062\n",
      "Gradient Descent(24/49): loss=545288122402.3801\n",
      "Gradient Descent(25/49): loss=1906818035227.6104\n",
      "Gradient Descent(26/49): loss=6667951987386.304\n",
      "Gradient Descent(27/49): loss=23317161304688.97\n",
      "Gradient Descent(28/49): loss=81537781366361.97\n",
      "Gradient Descent(29/49): loss=285129467660051.0\n",
      "Gradient Descent(30/49): loss=997069235460525.9\n",
      "Gradient Descent(31/49): loss=3486651409482010.0\n",
      "Gradient Descent(32/49): loss=1.2192471313818842e+16\n",
      "Gradient Descent(33/49): loss=4.263585293729372e+16\n",
      "Gradient Descent(34/49): loss=1.490933141364094e+17\n",
      "Gradient Descent(35/49): loss=5.213644102035931e+17\n",
      "Gradient Descent(36/49): loss=1.8231592060411126e+18\n",
      "Gradient Descent(37/49): loss=6.375405427604773e+18\n",
      "Gradient Descent(38/49): loss=2.2294155239792132e+19\n",
      "Gradient Descent(39/49): loss=7.796043145802664e+19\n",
      "Gradient Descent(40/49): loss=2.7261983276560017e+20\n",
      "Gradient Descent(41/49): loss=9.533242931979802e+20\n",
      "Gradient Descent(42/49): loss=3.333679720883877e+21\n",
      "Gradient Descent(43/49): loss=1.1657544615959542e+22\n",
      "Gradient Descent(44/49): loss=4.076526776754753e+22\n",
      "Gradient Descent(45/49): loss=1.4255206485632946e+23\n",
      "Gradient Descent(46/49): loss=4.984903155961161e+23\n",
      "Gradient Descent(47/49): loss=1.743170784608089e+24\n",
      "Gradient Descent(48/49): loss=6.095693916696581e+24\n",
      "Gradient Descent(49/49): loss=2.13160320572971e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6248504487373999\n",
      "Gradient Descent(2/49): loss=1.061439982927234\n",
      "Gradient Descent(3/49): loss=2.5881499250355255\n",
      "Gradient Descent(4/49): loss=7.926901921594647\n",
      "Gradient Descent(5/49): loss=26.59598377835947\n",
      "Gradient Descent(6/49): loss=91.87989612328235\n",
      "Gradient Descent(7/49): loss=320.17120920224517\n",
      "Gradient Descent(8/49): loss=1118.4831019080634\n",
      "Gradient Descent(9/49): loss=3910.0999595112694\n",
      "Gradient Descent(10/49): loss=13672.10494886295\n",
      "Gradient Descent(11/49): loss=47808.86019612392\n",
      "Gradient Descent(12/49): loss=167181.6796202632\n",
      "Gradient Descent(13/49): loss=584616.491864517\n",
      "Gradient Descent(14/49): loss=2044344.286801428\n",
      "Gradient Descent(15/49): loss=7148866.412916902\n",
      "Gradient Descent(16/49): loss=24998869.835728183\n",
      "Gradient Descent(17/49): loss=87418546.80495538\n",
      "Gradient Descent(18/49): loss=305693915.1986595\n",
      "Gradient Descent(19/49): loss=1068981050.9344858\n",
      "Gradient Descent(20/49): loss=3738119835.889404\n",
      "Gradient Descent(21/49): loss=13071831252.997622\n",
      "Gradient Descent(22/49): loss=45710886707.48679\n",
      "Gradient Descent(23/49): loss=159846399726.27075\n",
      "Gradient Descent(24/49): loss=558966875201.7134\n",
      "Gradient Descent(25/49): loss=1954651265891.8213\n",
      "Gradient Descent(26/49): loss=6835220011695.997\n",
      "Gradient Descent(27/49): loss=23902080858898.46\n",
      "Gradient Descent(28/49): loss=83583186555479.45\n",
      "Gradient Descent(29/49): loss=292282045065851.94\n",
      "Gradient Descent(30/49): loss=1022081083390767.6\n",
      "Gradient Descent(31/49): loss=3574115340509471.0\n",
      "Gradient Descent(32/49): loss=1.2498323934227078e+16\n",
      "Gradient Descent(33/49): loss=4.370538896560141e+16\n",
      "Gradient Descent(34/49): loss=1.5283337467381536e+17\n",
      "Gradient Descent(35/49): loss=5.344430278968416e+17\n",
      "Gradient Descent(36/49): loss=1.8688938242524774e+18\n",
      "Gradient Descent(37/49): loss=6.535334814028615e+18\n",
      "Gradient Descent(38/49): loss=2.285341231117622e+19\n",
      "Gradient Descent(39/49): loss=7.991609751095057e+19\n",
      "Gradient Descent(40/49): loss=2.7945860138606025e+20\n",
      "Gradient Descent(41/49): loss=9.772387831868191e+20\n",
      "Gradient Descent(42/49): loss=3.4173063009261726e+21\n",
      "Gradient Descent(43/49): loss=1.1949978403708976e+22\n",
      "Gradient Descent(44/49): loss=4.178787947993286e+22\n",
      "Gradient Descent(45/49): loss=1.4612803575337056e+23\n",
      "Gradient Descent(46/49): loss=5.109951282260022e+23\n",
      "Gradient Descent(47/49): loss=1.7868988638935637e+24\n",
      "Gradient Descent(48/49): loss=6.248606637149371e+24\n",
      "Gradient Descent(49/49): loss=2.185075254944668e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6234059865920003\n",
      "Gradient Descent(2/49): loss=1.0549443811055952\n",
      "Gradient Descent(3/49): loss=2.5639909928801057\n",
      "Gradient Descent(4/49): loss=7.840976089594487\n",
      "Gradient Descent(5/49): loss=26.294065274294\n",
      "Gradient Descent(6/49): loss=90.82267284426398\n",
      "Gradient Descent(7/49): loss=316.47276065570406\n",
      "Gradient Descent(8/49): loss=1105.5485527235787\n",
      "Gradient Descent(9/49): loss=3864.867690005382\n",
      "Gradient Descent(10/49): loss=13513.930781167084\n",
      "Gradient Descent(11/49): loss=47255.739504652636\n",
      "Gradient Descent(12/49): loss=165247.47042980557\n",
      "Gradient Descent(13/49): loss=577852.7543019622\n",
      "Gradient Descent(14/49): loss=2020692.1714742985\n",
      "Gradient Descent(15/49): loss=7066157.329384664\n",
      "Gradient Descent(16/49): loss=24709644.44008363\n",
      "Gradient Descent(17/49): loss=86407154.51748064\n",
      "Gradient Descent(18/49): loss=302157177.5071652\n",
      "Gradient Descent(19/49): loss=1056613432.8998133\n",
      "Gradient Descent(20/49): loss=3694871512.382099\n",
      "Gradient Descent(21/49): loss=12920596190.524565\n",
      "Gradient Descent(22/49): loss=45182032817.516846\n",
      "Gradient Descent(23/49): loss=157997050558.4417\n",
      "Gradient Descent(24/49): loss=552499886096.7194\n",
      "Gradient Descent(25/49): loss=1932036851690.4194\n",
      "Gradient Descent(26/49): loss=6756139666674.368\n",
      "Gradient Descent(27/49): loss=23625544800392.055\n",
      "Gradient Descent(28/49): loss=82616167612485.62\n",
      "Gradient Descent(29/49): loss=288900476524122.2\n",
      "Gradient Descent(30/49): loss=1010256076357209.1\n",
      "Gradient Descent(31/49): loss=3532764473413873.5\n",
      "Gradient Descent(32/49): loss=1.2353724087080476e+16\n",
      "Gradient Descent(33/49): loss=4.319973776011126e+16\n",
      "Gradient Descent(34/49): loss=1.5106516297334288e+17\n",
      "Gradient Descent(35/49): loss=5.28259768401502e+17\n",
      "Gradient Descent(36/49): loss=1.847271584123096e+18\n",
      "Gradient Descent(37/49): loss=6.459724002520295e+18\n",
      "Gradient Descent(38/49): loss=2.2589008864411013e+19\n",
      "Gradient Descent(39/49): loss=7.899150509796249e+19\n",
      "Gradient Descent(40/49): loss=2.7622539417704563e+20\n",
      "Gradient Descent(41/49): loss=9.659325808977104e+20\n",
      "Gradient Descent(42/49): loss=3.377769642141511e+21\n",
      "Gradient Descent(43/49): loss=1.1811722661605761e+22\n",
      "Gradient Descent(44/49): loss=4.1304412975368e+22\n",
      "Gradient Descent(45/49): loss=1.4443740173355874e+23\n",
      "Gradient Descent(46/49): loss=5.0508315012202715e+23\n",
      "Gradient Descent(47/49): loss=1.7662252676616765e+24\n",
      "Gradient Descent(48/49): loss=6.176313138486612e+24\n",
      "Gradient Descent(49/49): loss=2.1597949413972884e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6244128645132805\n",
      "Gradient Descent(2/49): loss=1.0594722104297494\n",
      "Gradient Descent(3/49): loss=2.5808312371649946\n",
      "Gradient Descent(4/49): loss=7.900871617754963\n",
      "Gradient Descent(5/49): loss=26.504520824642295\n",
      "Gradient Descent(6/49): loss=91.55962173620145\n",
      "Gradient Descent(7/49): loss=319.0508041138592\n",
      "Gradient Descent(8/49): loss=1114.5647197702976\n",
      "Gradient Descent(9/49): loss=3896.397331429073\n",
      "Gradient Descent(10/49): loss=13624.187791139404\n",
      "Gradient Descent(11/49): loss=47641.29824969864\n",
      "Gradient Descent(12/49): loss=166595.731812225\n",
      "Gradient Descent(13/49): loss=582567.4905370068\n",
      "Gradient Descent(14/49): loss=2037179.133621622\n",
      "Gradient Descent(15/49): loss=7123810.588324809\n",
      "Gradient Descent(16/49): loss=24911252.12227675\n",
      "Gradient Descent(17/49): loss=87112156.42235222\n",
      "Gradient Descent(18/49): loss=304622498.6692664\n",
      "Gradient Descent(19/49): loss=1065234414.4725974\n",
      "Gradient Descent(20/49): loss=3725018222.8449507\n",
      "Gradient Descent(21/49): loss=13026016222.343254\n",
      "Gradient Descent(22/49): loss=45550676126.78598\n",
      "Gradient Descent(23/49): loss=159286159346.64832\n",
      "Gradient Descent(24/49): loss=557007770618.1116\n",
      "Gradient Descent(25/49): loss=1947800473073.3762\n",
      "Gradient Descent(26/49): loss=6811263474289.368\n",
      "Gradient Descent(27/49): loss=23818307243239.918\n",
      "Gradient Descent(28/49): loss=83290238598884.1\n",
      "Gradient Descent(29/49): loss=291257635356414.1\n",
      "Gradient Descent(30/49): loss=1018498825077918.2\n",
      "Gradient Descent(31/49): loss=3561588541414801.0\n",
      "Gradient Descent(32/49): loss=1.2454518970473052e+16\n",
      "Gradient Descent(33/49): loss=4.355220738784411e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(34/49): loss=1.5229771401456106e+17\n",
      "Gradient Descent(35/49): loss=5.325698761374756e+17\n",
      "Gradient Descent(36/49): loss=1.8623435998652616e+18\n",
      "Gradient Descent(37/49): loss=6.51242933436912e+18\n",
      "Gradient Descent(38/49): loss=2.27733141393548e+19\n",
      "Gradient Descent(39/49): loss=7.963600221391605e+19\n",
      "Gradient Descent(40/49): loss=2.784791361418288e+20\n",
      "Gradient Descent(41/49): loss=9.738136911743584e+20\n",
      "Gradient Descent(42/49): loss=3.4053290966677533e+21\n",
      "Gradient Descent(43/49): loss=1.1908095318137388e+22\n",
      "Gradient Descent(44/49): loss=4.164141851799072e+22\n",
      "Gradient Descent(45/49): loss=1.4561587641556028e+23\n",
      "Gradient Descent(46/49): loss=5.092041582376027e+23\n",
      "Gradient Descent(47/49): loss=1.780636020941152e+24\n",
      "Gradient Descent(48/49): loss=6.226706101629432e+24\n",
      "Gradient Descent(49/49): loss=2.177416856678929e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6273120266035203\n",
      "Gradient Descent(2/49): loss=1.0869084426421898\n",
      "Gradient Descent(3/49): loss=2.746051504541973\n",
      "Gradient Descent(4/49): loss=8.735557957999234\n",
      "Gradient Descent(5/49): loss=30.357676254980515\n",
      "Gradient Descent(6/49): loss=108.41352330708905\n",
      "Gradient Descent(7/49): loss=390.1951311651742\n",
      "Gradient Descent(8/49): loss=1407.4267355328127\n",
      "Gradient Descent(9/49): loss=5079.632827299488\n",
      "Gradient Descent(10/49): loss=18336.29681857708\n",
      "Gradient Descent(11/49): loss=66192.85382708827\n",
      "Gradient Descent(12/49): loss=238955.02462780266\n",
      "Gradient Descent(13/49): loss=862626.4612183443\n",
      "Gradient Descent(14/49): loss=3114080.3473101314\n",
      "Gradient Descent(15/49): loss=11241828.876101434\n",
      "Gradient Descent(16/49): loss=40583001.06503615\n",
      "Gradient Descent(17/49): loss=146504632.66709262\n",
      "Gradient Descent(18/49): loss=528881722.7504904\n",
      "Gradient Descent(19/49): loss=1909263017.951515\n",
      "Gradient Descent(20/49): loss=6892439493.627817\n",
      "Gradient Descent(21/49): loss=24881706570.81735\n",
      "Gradient Descent(22/49): loss=89822960719.46596\n",
      "Gradient Descent(23/49): loss=324260888196.08466\n",
      "Gradient Descent(24/49): loss=1170581806386.5774\n",
      "Gradient Descent(25/49): loss=4225800321054.4976\n",
      "Gradient Descent(26/49): loss=15255139159006.838\n",
      "Gradient Descent(27/49): loss=55071052364009.68\n",
      "Gradient Descent(28/49): loss=198806499034061.56\n",
      "Gradient Descent(29/49): loss=717691461512928.1\n",
      "Gradient Descent(30/49): loss=2590866176061860.5\n",
      "Gradient Descent(31/49): loss=9353026895582648.0\n",
      "Gradient Descent(32/49): loss=3.3764427093054584e+16\n",
      "Gradient Descent(33/49): loss=1.2188958180594109e+17\n",
      "Gradient Descent(34/49): loss=4.400213903194787e+17\n",
      "Gradient Descent(35/49): loss=1.5884772190532884e+18\n",
      "Gradient Descent(36/49): loss=5.734402760782445e+18\n",
      "Gradient Descent(37/49): loss=2.0701193966426567e+19\n",
      "Gradient Descent(38/49): loss=7.473131021879697e+19\n",
      "Gradient Descent(39/49): loss=2.697800298898299e+20\n",
      "Gradient Descent(40/49): loss=9.73905907902334e+20\n",
      "Gradient Descent(41/49): loss=3.515800327527191e+21\n",
      "Gradient Descent(42/49): loss=1.2692039182372548e+22\n",
      "Gradient Descent(43/49): loss=4.58182614483688e+22\n",
      "Gradient Descent(44/49): loss=1.654039238286206e+23\n",
      "Gradient Descent(45/49): loss=5.971081650212915e+23\n",
      "Gradient Descent(46/49): loss=2.155560475726978e+24\n",
      "Gradient Descent(47/49): loss=7.781573317373919e+24\n",
      "Gradient Descent(48/49): loss=2.809147967571915e+25\n",
      "Gradient Descent(49/49): loss=1.0141024162934855e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6305056955443202\n",
      "Gradient Descent(2/49): loss=1.1016312564592918\n",
      "Gradient Descent(3/49): loss=2.8023945313622227\n",
      "Gradient Descent(4/49): loss=8.942149953762662\n",
      "Gradient Descent(5/49): loss=31.106667028626337\n",
      "Gradient Descent(6/49): loss=111.12057366889296\n",
      "Gradient Descent(7/49): loss=399.9707766402329\n",
      "Gradient Descent(8/49): loss=1442.7200093667936\n",
      "Gradient Descent(9/49): loss=5207.044739509552\n",
      "Gradient Descent(10/49): loss=18796.257015324354\n",
      "Gradient Descent(11/49): loss=67853.31333101858\n",
      "Gradient Descent(12/49): loss=244949.28663066032\n",
      "Gradient Descent(13/49): loss=884265.7502423077\n",
      "Gradient Descent(14/49): loss=3192198.1838805755\n",
      "Gradient Descent(15/49): loss=11523834.269315688\n",
      "Gradient Descent(16/49): loss=41601040.53773328\n",
      "Gradient Descent(17/49): loss=150179755.16671476\n",
      "Gradient Descent(18/49): loss=542148914.9773989\n",
      "Gradient Descent(19/49): loss=1957157581.8939927\n",
      "Gradient Descent(20/49): loss=7065338869.463359\n",
      "Gradient Descent(21/49): loss=25505873317.590805\n",
      "Gradient Descent(22/49): loss=92076202675.31892\n",
      "Gradient Descent(23/49): loss=332395091656.69147\n",
      "Gradient Descent(24/49): loss=1199946280879.4878\n",
      "Gradient Descent(25/49): loss=4331806073974.0005\n",
      "Gradient Descent(26/49): loss=15637819927046.145\n",
      "Gradient Descent(27/49): loss=56452529936631.42\n",
      "Gradient Descent(28/49): loss=203793633071257.4\n",
      "Gradient Descent(29/49): loss=735695015387303.0\n",
      "Gradient Descent(30/49): loss=2655859005547906.5\n",
      "Gradient Descent(31/49): loss=9587651010027338.0\n",
      "Gradient Descent(32/49): loss=3.461142014619704e+16\n",
      "Gradient Descent(33/49): loss=1.249472267277581e+17\n",
      "Gradient Descent(34/49): loss=4.510594884871813e+17\n",
      "Gradient Descent(35/49): loss=1.628324753438736e+18\n",
      "Gradient Descent(36/49): loss=5.878252359913556e+18\n",
      "Gradient Descent(37/49): loss=2.122049101928856e+19\n",
      "Gradient Descent(38/49): loss=7.660597257963006e+19\n",
      "Gradient Descent(39/49): loss=2.7654756101247513e+20\n",
      "Gradient Descent(40/49): loss=9.983366952549835e+20\n",
      "Gradient Descent(41/49): loss=3.603995469870852e+21\n",
      "Gradient Descent(42/49): loss=1.3010423646233951e+22\n",
      "Gradient Descent(43/49): loss=4.6967629362907906e+22\n",
      "Gradient Descent(44/49): loss=1.6955314200009075e+23\n",
      "Gradient Descent(45/49): loss=6.120868426202856e+23\n",
      "Gradient Descent(46/49): loss=2.2096335018590055e+24\n",
      "Gradient Descent(47/49): loss=7.976776941710578e+24\n",
      "Gradient Descent(48/49): loss=2.879616475957583e+25\n",
      "Gradient Descent(49/49): loss=1.0395415478207988e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6289958048000003\n",
      "Gradient Descent(2/49): loss=1.0946706601279546\n",
      "Gradient Descent(3/49): loss=2.7757568878620926\n",
      "Gradient Descent(4/49): loss=8.844478169981333\n",
      "Gradient Descent(5/49): loss=30.75256199843114\n",
      "Gradient Descent(6/49): loss=109.8407446191421\n",
      "Gradient Descent(7/49): loss=395.34908387989014\n",
      "Gradient Descent(8/49): loss=1426.0341886113176\n",
      "Gradient Descent(9/49): loss=5146.807416691327\n",
      "Gradient Descent(10/49): loss=18578.798770059904\n",
      "Gradient Descent(11/49): loss=67068.28755571616\n",
      "Gradient Descent(12/49): loss=242115.34207196\n",
      "Gradient Descent(13/49): loss=874035.2088755474\n",
      "Gradient Descent(14/49): loss=3155265.9280363345\n",
      "Gradient Descent(15/49): loss=11390508.82420813\n",
      "Gradient Descent(16/49): loss=41119735.67938358\n",
      "Gradient Descent(17/49): loss=148442244.62658218\n",
      "Gradient Descent(18/49): loss=535876501.92592\n",
      "Gradient Descent(19/49): loss=1934514170.776377\n",
      "Gradient Descent(20/49): loss=6983596155.327263\n",
      "Gradient Descent(21/49): loss=25210782119.553913\n",
      "Gradient Descent(22/49): loss=91010923450.40666\n",
      "Gradient Descent(23/49): loss=328549433654.78815\n",
      "Gradient Descent(24/49): loss=1186063455492.5898\n",
      "Gradient Descent(25/49): loss=4281689074327.3247\n",
      "Gradient Descent(26/49): loss=15456897558319.28\n",
      "Gradient Descent(27/49): loss=55799400185533.07\n",
      "Gradient Descent(28/49): loss=201435834669782.44\n",
      "Gradient Descent(29/49): loss=727183363157888.4\n",
      "Gradient Descent(30/49): loss=2625131941000130.0\n",
      "Gradient Descent(31/49): loss=9476726307010282.0\n",
      "Gradient Descent(32/49): loss=3.42109819683086e+16\n",
      "Gradient Descent(33/49): loss=1.2350164490560229e+17\n",
      "Gradient Descent(34/49): loss=4.4584093810923923e+17\n",
      "Gradient Descent(35/49): loss=1.6094857865743493e+18\n",
      "Gradient Descent(36/49): loss=5.810243689533176e+18\n",
      "Gradient Descent(37/49): loss=2.09749797192164e+19\n",
      "Gradient Descent(38/49): loss=7.571967678636486e+19\n",
      "Gradient Descent(39/49): loss=2.7334803319875584e+20\n",
      "Gradient Descent(40/49): loss=9.867863998474157e+20\n",
      "Gradient Descent(41/49): loss=3.5622989034491106e+21\n",
      "Gradient Descent(42/49): loss=1.2859899041449786e+22\n",
      "Gradient Descent(43/49): loss=4.642423553963593e+22\n",
      "Gradient Descent(44/49): loss=1.675914902980793e+23\n",
      "Gradient Descent(45/49): loss=6.050052799761286e+23\n",
      "Gradient Descent(46/49): loss=2.1840690607139714e+24\n",
      "Gradient Descent(47/49): loss=7.884489309177578e+24\n",
      "Gradient Descent(48/49): loss=2.8463006406130573e+25\n",
      "Gradient Descent(49/49): loss=1.0275145312612697e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6300482904320004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/49): loss=1.0995226188915703\n",
      "Gradient Descent(3/49): loss=2.794324944630535\n",
      "Gradient Descent(4/49): loss=8.912561340547914\n",
      "Gradient Descent(5/49): loss=30.999394729808962\n",
      "Gradient Descent(6/49): loss=110.73286326504692\n",
      "Gradient Descent(7/49): loss=398.5706846772683\n",
      "Gradient Descent(8/49): loss=1437.6652199753926\n",
      "Gradient Descent(9/49): loss=5188.79649240193\n",
      "Gradient Descent(10/49): loss=18730.38038586173\n",
      "Gradient Descent(11/49): loss=67615.49824125464\n",
      "Gradient Descent(12/49): loss=244090.77369923596\n",
      "Gradient Descent(13/49): loss=881166.5181026418\n",
      "Gradient Descent(14/49): loss=3181009.9553990187\n",
      "Gradient Descent(15/49): loss=11483444.76403962\n",
      "Gradient Descent(16/49): loss=41455234.42323413\n",
      "Gradient Descent(17/49): loss=149653395.09291255\n",
      "Gradient Descent(18/49): loss=540248755.1104509\n",
      "Gradient Descent(19/49): loss=1950298004.773686\n",
      "Gradient Descent(20/49): loss=7040575796.05764\n",
      "Gradient Descent(21/49): loss=25416478622.59136\n",
      "Gradient Descent(22/49): loss=91753487826.37883\n",
      "Gradient Descent(23/49): loss=331230091052.0436\n",
      "Gradient Descent(24/49): loss=1195740628696.6304\n",
      "Gradient Descent(25/49): loss=4316623669593.4565\n",
      "Gradient Descent(26/49): loss=15583011447230.074\n",
      "Gradient Descent(27/49): loss=56254671324501.03\n",
      "Gradient Descent(28/49): loss=203079363481446.8\n",
      "Gradient Descent(29/49): loss=733116502167984.4\n",
      "Gradient Descent(30/49): loss=2646550572826311.0\n",
      "Gradient Descent(31/49): loss=9554047567903938.0\n",
      "Gradient Descent(32/49): loss=3.449011172013493e+16\n",
      "Gradient Descent(33/49): loss=1.2450930330968053e+17\n",
      "Gradient Descent(34/49): loss=4.494785849479278e+17\n",
      "Gradient Descent(35/49): loss=1.6226176916619108e+18\n",
      "Gradient Descent(36/49): loss=5.857649866899673e+18\n",
      "Gradient Descent(37/49): loss=2.1146116019507773e+19\n",
      "Gradient Descent(38/49): loss=7.633747883042677e+19\n",
      "Gradient Descent(39/49): loss=2.7557829857786384e+20\n",
      "Gradient Descent(40/49): loss=9.948376578661603e+20\n",
      "Gradient Descent(41/49): loss=3.5913639448971313e+21\n",
      "Gradient Descent(42/49): loss=1.2964823841079728e+22\n",
      "Gradient Descent(43/49): loss=4.68030140662958e+22\n",
      "Gradient Descent(44/49): loss=1.6895888077931793e+23\n",
      "Gradient Descent(45/49): loss=6.099415596133342e+23\n",
      "Gradient Descent(46/49): loss=2.2018890302043764e+24\n",
      "Gradient Descent(47/49): loss=7.948819399038414e+24\n",
      "Gradient Descent(48/49): loss=2.8695238030527903e+25\n",
      "Gradient Descent(49/49): loss=1.0358980929019946e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6329166824873307\n",
      "Gradient Descent(2/49): loss=1.128018033084438\n",
      "Gradient Descent(3/49): loss=2.972221053923347\n",
      "Gradient Descent(4/49): loss=9.841692886245808\n",
      "Gradient Descent(5/49): loss=35.42978851446229\n",
      "Gradient Descent(6/49): loss=130.74288592002017\n",
      "Gradient Descent(7/49): loss=485.77464244592664\n",
      "Gradient Descent(8/49): loss=1808.2324323292576\n",
      "Gradient Descent(9/49): loss=6734.255453866058\n",
      "Gradient Descent(10/49): loss=25083.198606788512\n",
      "Gradient Descent(11/49): loss=93431.1769571155\n",
      "Gradient Descent(12/49): loss=348020.56151427364\n",
      "Gradient Descent(13/49): loss=1296340.5600511495\n",
      "Gradient Descent(14/49): loss=4828737.722601293\n",
      "Gradient Descent(15/49): loss=17986563.913384628\n",
      "Gradient Descent(16/49): loss=66998150.69143575\n",
      "Gradient Descent(17/49): loss=249561410.28100017\n",
      "Gradient Descent(18/49): loss=929591295.9261494\n",
      "Gradient Descent(19/49): loss=3462634616.965421\n",
      "Gradient Descent(20/49): loss=12897967683.504602\n",
      "Gradient Descent(21/49): loss=48043639823.05449\n",
      "Gradient Descent(22/49): loss=178957753975.68277\n",
      "Gradient Descent(23/49): loss=666599737782.7457\n",
      "Gradient Descent(24/49): loss=2483017363265.81\n",
      "Gradient Descent(25/49): loss=9248991376427.98\n",
      "Gradient Descent(26/49): loss=34451567978058.035\n",
      "Gradient Descent(27/49): loss=128328645561459.52\n",
      "Gradient Descent(28/49): loss=478011371851869.0\n",
      "Gradient Descent(29/49): loss=1780544559011026.5\n",
      "Gradient Descent(30/49): loss=6632350427860392.0\n",
      "Gradient Descent(31/49): loss=2.470484210873988e+16\n",
      "Gradient Descent(32/49): loss=9.202306637085478e+16\n",
      "Gradient Descent(33/49): loss=3.427767199247802e+17\n",
      "Gradient Descent(34/49): loss=1.2768090040478103e+18\n",
      "Gradient Descent(35/49): loss=4.75598585917737e+18\n",
      "Gradient Descent(36/49): loss=1.7715571726849659e+19\n",
      "Gradient Descent(37/49): loss=6.5988733125337645e+19\n",
      "Gradient Descent(38/49): loss=2.45801432018549e+20\n",
      "Gradient Descent(39/49): loss=9.155857541259067e+20\n",
      "Gradient Descent(40/49): loss=3.4104653755433044e+21\n",
      "Gradient Descent(41/49): loss=1.270364247736258e+22\n",
      "Gradient Descent(42/49): loss=4.731979786393103e+22\n",
      "Gradient Descent(43/49): loss=1.7626151506335304e+23\n",
      "Gradient Descent(44/49): loss=6.565565174594641e+23\n",
      "Gradient Descent(45/49): loss=2.4456073718846653e+24\n",
      "Gradient Descent(46/49): loss=9.109642899533551e+24\n",
      "Gradient Descent(47/49): loss=3.3932508836472882e+25\n",
      "Gradient Descent(48/49): loss=1.2639520216497348e+26\n",
      "Gradient Descent(49/49): loss=4.7080948854430776e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6362509462792025\n",
      "Gradient Descent(2/49): loss=1.1437720960746742\n",
      "Gradient Descent(3/49): loss=3.0342376269477893\n",
      "Gradient Descent(4/49): loss=10.076032682896267\n",
      "Gradient Descent(5/49): loss=36.30601508679964\n",
      "Gradient Descent(6/49): loss=134.01007654309336\n",
      "Gradient Descent(7/49): loss=497.94793506161386\n",
      "Gradient Descent(8/49): loss=1853.5800642573977\n",
      "Gradient Descent(9/49): loss=6903.174182298975\n",
      "Gradient Descent(10/49): loss=25712.407312591175\n",
      "Gradient Descent(11/49): loss=95774.91979961396\n",
      "Gradient Descent(12/49): loss=356750.7725625332\n",
      "Gradient Descent(13/49): loss=1328859.726519196\n",
      "Gradient Descent(14/49): loss=4949868.369112317\n",
      "Gradient Descent(15/49): loss=18437763.46190831\n",
      "Gradient Descent(16/49): loss=68678823.89305678\n",
      "Gradient Descent(17/49): loss=255821749.893043\n",
      "Gradient Descent(18/49): loss=952910434.9504615\n",
      "Gradient Descent(19/49): loss=3549496077.9210386\n",
      "Gradient Descent(20/49): loss=13221517939.421326\n",
      "Gradient Descent(21/49): loss=49248832171.32855\n",
      "Gradient Descent(22/49): loss=183446974953.75803\n",
      "Gradient Descent(23/49): loss=683321637004.0662\n",
      "Gradient Descent(24/49): loss=2545304765675.321\n",
      "Gradient Descent(25/49): loss=9481005721662.191\n",
      "Gradient Descent(26/49): loss=35315798212614.86\n",
      "Gradient Descent(27/49): loss=131547816762171.88\n",
      "Gradient Descent(28/49): loss=490002462657453.75\n",
      "Gradient Descent(29/49): loss=1825210173152692.2\n",
      "Gradient Descent(30/49): loss=6798725373976608.0\n",
      "Gradient Descent(31/49): loss=2.5324572145525296e+16\n",
      "Gradient Descent(32/49): loss=9.433149878487109e+16\n",
      "Gradient Descent(33/49): loss=3.5137539982374586e+17\n",
      "Gradient Descent(34/49): loss=1.3088382268033585e+18\n",
      "Gradient Descent(35/49): loss=4.875291511020351e+18\n",
      "Gradient Descent(36/49): loss=1.8159973349399284e+19\n",
      "Gradient Descent(37/49): loss=6.764408472917445e+19\n",
      "Gradient Descent(38/49): loss=2.519674512076866e+20\n",
      "Gradient Descent(39/49): loss=9.385535590035785e+20\n",
      "Gradient Descent(40/49): loss=3.4960181519321305e+21\n",
      "Gradient Descent(41/49): loss=1.302231801413115e+22\n",
      "Gradient Descent(42/49): loss=4.850683237084285e+22\n",
      "Gradient Descent(43/49): loss=1.806830998981493e+23\n",
      "Gradient Descent(44/49): loss=6.730264788106132e+23\n",
      "Gradient Descent(45/49): loss=2.506956330921809e+24\n",
      "Gradient Descent(46/49): loss=9.338161637050631e+24\n",
      "Gradient Descent(47/49): loss=3.4783718281851217e+25\n",
      "Gradient Descent(48/49): loss=1.2956587222807589e+26\n",
      "Gradient Descent(49/49): loss=4.826199174623541e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6346745856320005\n",
      "Gradient Descent(2/49): loss=1.1363239496526796\n",
      "Gradient Descent(3/49): loss=3.00491766569316\n",
      "Gradient Descent(4/49): loss=9.965242398573515\n",
      "Gradient Descent(5/49): loss=35.89175599607932\n",
      "Gradient Descent(6/49): loss=132.46542649543608\n",
      "Gradient Descent(7/49): loss=492.1926917385013\n",
      "Gradient Descent(8/49): loss=1832.140782042533\n",
      "Gradient Descent(9/49): loss=6823.313423616391\n",
      "Gradient Descent(10/49): loss=25414.93239621635\n",
      "Gradient Descent(11/49): loss=94666.85390725067\n",
      "Gradient Descent(12/49): loss=352623.3363437089\n",
      "Gradient Descent(13/49): loss=1313485.4377711792\n",
      "Gradient Descent(14/49): loss=4892600.679378468\n",
      "Gradient Descent(15/49): loss=18224447.042840496\n",
      "Gradient Descent(16/49): loss=67884241.56210612\n",
      "Gradient Descent(17/49): loss=252862010.16692588\n",
      "Gradient Descent(18/49): loss=941885700.4429847\n",
      "Gradient Descent(19/49): loss=3508430044.3520894\n",
      "Gradient Descent(20/49): loss=13068551070.97812\n",
      "Gradient Descent(21/49): loss=48679045883.05985\n",
      "Gradient Descent(22/49): loss=181324578008.59097\n",
      "Gradient Descent(23/49): loss=675415920622.9219\n",
      "Gradient Descent(24/49): loss=2515856762727.2983\n",
      "Gradient Descent(25/49): loss=9371314855481.26\n",
      "Gradient Descent(26/49): loss=34907210705178.28\n",
      "Gradient Descent(27/49): loss=130025869155706.06\n",
      "Gradient Descent(28/49): loss=484333360018088.25\n",
      "Gradient Descent(29/49): loss=1804093332731404.0\n",
      "Gradient Descent(30/49): loss=6720067255091567.0\n",
      "Gradient Descent(31/49): loss=2.503157851849146e+16\n",
      "Gradient Descent(32/49): loss=9.324012682351998e+16\n",
      "Gradient Descent(33/49): loss=3.4731014840489645e+17\n",
      "Gradient Descent(34/49): loss=1.2936955717933642e+18\n",
      "Gradient Descent(35/49): loss=4.818886635372906e+18\n",
      "Gradient Descent(36/49): loss=1.7949870828100375e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=6.686147384758718e+19\n",
      "Gradient Descent(38/49): loss=2.4905230393489677e+20\n",
      "Gradient Descent(39/49): loss=9.276949269270158e+20\n",
      "Gradient Descent(40/49): loss=3.455570833310846e+21\n",
      "Gradient Descent(41/49): loss=1.287165579699927e+22\n",
      "Gradient Descent(42/49): loss=4.794563067824506e+22\n",
      "Gradient Descent(43/49): loss=1.7859267971341116e+23\n",
      "Gradient Descent(44/49): loss=6.652398726644485e+23\n",
      "Gradient Descent(45/49): loss=2.477952001687559e+24\n",
      "Gradient Descent(46/49): loss=9.230123411086321e+24\n",
      "Gradient Descent(47/49): loss=3.4381286693957358e+25\n",
      "Gradient Descent(48/49): loss=1.2806685480632027e+26\n",
      "Gradient Descent(49/49): loss=4.7703622746802827e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6357734048268806\n",
      "Gradient Descent(2/49): loss=1.1415157604664925\n",
      "Gradient Descent(3/49): loss=3.0253554609887257\n",
      "Gradient Descent(4/49): loss=10.042469961463254\n",
      "Gradient Descent(5/49): loss=36.18051976427863\n",
      "Gradient Descent(6/49): loss=133.54214147479433\n",
      "Gradient Descent(7/49): loss=496.20444618429025\n",
      "Gradient Descent(8/49): loss=1847.0852649965884\n",
      "Gradient Descent(9/49): loss=6878.981226990263\n",
      "Gradient Descent(10/49): loss=25622.2904958236\n",
      "Gradient Descent(11/49): loss=95439.24319129525\n",
      "Gradient Descent(12/49): loss=355500.41028664325\n",
      "Gradient Descent(13/49): loss=1324202.2515999926\n",
      "Gradient Descent(14/49): loss=4932519.740308502\n",
      "Gradient Descent(15/49): loss=18373141.553998902\n",
      "Gradient Descent(16/49): loss=68438113.74781752\n",
      "Gradient Descent(17/49): loss=254925128.67254817\n",
      "Gradient Descent(18/49): loss=949570610.5657805\n",
      "Gradient Descent(19/49): loss=3537055566.0697565\n",
      "Gradient Descent(20/49): loss=13175178276.826685\n",
      "Gradient Descent(21/49): loss=49076221562.12068\n",
      "Gradient Descent(22/49): loss=182804017695.5053\n",
      "Gradient Descent(23/49): loss=680926685512.8069\n",
      "Gradient Descent(24/49): loss=2536383810865.2417\n",
      "Gradient Descent(25/49): loss=9447776057090.184\n",
      "Gradient Descent(26/49): loss=35192021035051.43\n",
      "Gradient Descent(27/49): loss=131086759153449.62\n",
      "Gradient Descent(28/49): loss=488285069170702.3\n",
      "Gradient Descent(29/49): loss=1818813054154097.0\n",
      "Gradient Descent(30/49): loss=6774896745418843.0\n",
      "Gradient Descent(31/49): loss=2.523581288701063e+16\n",
      "Gradient Descent(32/49): loss=9.400087942281982e+16\n",
      "Gradient Descent(33/49): loss=3.50143875762042e+17\n",
      "Gradient Descent(34/49): loss=1.304250922826151e+18\n",
      "Gradient Descent(35/49): loss=4.858204262434587e+18\n",
      "Gradient Descent(36/49): loss=1.8096325057143525e+19\n",
      "Gradient Descent(37/49): loss=6.740700120535729e+19\n",
      "Gradient Descent(38/49): loss=2.5108433878985307e+20\n",
      "Gradient Descent(39/49): loss=9.35264053558415e+20\n",
      "Gradient Descent(40/49): loss=3.483765073099339e+21\n",
      "Gradient Descent(41/49): loss=1.2976676520788815e+22\n",
      "Gradient Descent(42/49): loss=4.833682237228861e+22\n",
      "Gradient Descent(43/49): loss=1.8004982965453574e+23\n",
      "Gradient Descent(44/49): loss=6.706676104801536e+23\n",
      "Gradient Descent(45/49): loss=2.4981697822777494e+24\n",
      "Gradient Descent(46/49): loss=9.305432622006379e+24\n",
      "Gradient Descent(47/49): loss=3.4661805973711325e+25\n",
      "Gradient Descent(48/49): loss=1.2911176107147046e+26\n",
      "Gradient Descent(49/49): loss=4.809283988151266e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6386091397687983\n",
      "Gradient Descent(2/49): loss=1.1710900111045552\n",
      "Gradient Descent(3/49): loss=3.2166685264278327\n",
      "Gradient Descent(4/49): loss=11.074962950894534\n",
      "Gradient Descent(5/49): loss=41.26338681192453\n",
      "Gradient Descent(6/49): loss=157.23523591644908\n",
      "Gradient Descent(7/49): loss=602.7526914364266\n",
      "Gradient Descent(8/49): loss=2314.252548561771\n",
      "Gradient Descent(9/49): loss=8889.150399694512\n",
      "Gradient Descent(10/49): loss=34147.27798460716\n",
      "Gradient Descent(11/49): loss=131178.9009148158\n",
      "Gradient Descent(12/49): loss=503935.5835634458\n",
      "Gradient Descent(13/49): loss=1935917.65562659\n",
      "Gradient Descent(14/49): loss=7437019.983664396\n",
      "Gradient Descent(15/49): loss=28570054.687052246\n",
      "Gradient Descent(16/49): loss=109754720.80359025\n",
      "Gradient Descent(17/49): loss=421633734.1569154\n",
      "Gradient Descent(18/49): loss=1619748151.8549223\n",
      "Gradient Descent(19/49): loss=6222424498.884163\n",
      "Gradient Descent(20/49): loss=23904065953.63059\n",
      "Gradient Descent(21/49): loss=91829859766.18707\n",
      "Gradient Descent(22/49): loss=352773589276.5052\n",
      "Gradient Descent(23/49): loss=1355215020563.276\n",
      "Gradient Descent(24/49): loss=5206194022994.413\n",
      "Gradient Descent(25/49): loss=20000114958734.45\n",
      "Gradient Descent(26/49): loss=76832441625476.7\n",
      "Gradient Descent(27/49): loss=295159507748435.9\n",
      "Gradient Descent(28/49): loss=1133884764966466.5\n",
      "Gradient Descent(29/49): loss=4355931713095045.0\n",
      "Gradient Descent(30/49): loss=1.6733747269024616e+16\n",
      "Gradient Descent(31/49): loss=6.4284363508690776e+16\n",
      "Gradient Descent(32/49): loss=2.4695481085500742e+17\n",
      "Gradient Descent(33/49): loss=9.487016013806461e+17\n",
      "Gradient Descent(34/49): loss=3.6445320718640824e+18\n",
      "Gradient Descent(35/49): loss=1.400083440727286e+19\n",
      "Gradient Descent(36/49): loss=5.3785605458974065e+19\n",
      "Gradient Descent(37/49): loss=2.0662278193119494e+20\n",
      "Gradient Descent(38/49): loss=7.937620790669484e+20\n",
      "Gradient Descent(39/49): loss=3.0493164029433626e+21\n",
      "Gradient Descent(40/49): loss=1.1714253893547078e+22\n",
      "Gradient Descent(41/49): loss=4.50014777574475e+22\n",
      "Gradient Descent(42/49): loss=1.7287767695299118e+23\n",
      "Gradient Descent(43/49): loss=6.641268837825894e+23\n",
      "Gradient Descent(44/49): loss=2.5513098367391196e+24\n",
      "Gradient Descent(45/49): loss=9.801111868817382e+24\n",
      "Gradient Descent(46/49): loss=3.7651951355251433e+25\n",
      "Gradient Descent(47/49): loss=1.4464373632632513e+26\n",
      "Gradient Descent(48/49): loss=5.5566337747122305e+26\n",
      "Gradient Descent(49/49): loss=2.134636430893457e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6420862009420463\n",
      "Gradient Descent(2/49): loss=1.1879245504810334\n",
      "Gradient Descent(3/49): loss=3.2848171540701605\n",
      "Gradient Descent(4/49): loss=11.340239780017436\n",
      "Gradient Descent(5/49): loss=42.28595133985286\n",
      "Gradient Descent(6/49): loss=161.16699686812467\n",
      "Gradient Descent(7/49): loss=617.8604213694872\n",
      "Gradient Descent(8/49): loss=2372.2938809338334\n",
      "Gradient Descent(9/49): loss=9112.125459196015\n",
      "Gradient Descent(10/49): loss=35003.86245025044\n",
      "Gradient Descent(11/49): loss=134469.5592750971\n",
      "Gradient Descent(12/49): loss=516576.9801974177\n",
      "Gradient Descent(13/49): loss=1984480.8484124646\n",
      "Gradient Descent(14/49): loss=7623580.348548212\n",
      "Gradient Descent(15/49): loss=29286744.988269135\n",
      "Gradient Descent(16/49): loss=112507958.26822051\n",
      "Gradient Descent(17/49): loss=432210571.2044909\n",
      "Gradient Descent(18/49): loss=1660380129.0604472\n",
      "Gradient Descent(19/49): loss=6378516302.520228\n",
      "Gradient Descent(20/49): loss=24503708226.482788\n",
      "Gradient Descent(21/49): loss=94133445521.57542\n",
      "Gradient Descent(22/49): loss=361623044314.41504\n",
      "Gradient Descent(23/49): loss=1389211087037.0093\n",
      "Gradient Descent(24/49): loss=5336793311960.222\n",
      "Gradient Descent(25/49): loss=20501825187225.24\n",
      "Gradient Descent(26/49): loss=78759811639245.34\n",
      "Gradient Descent(27/49): loss=302563692393344.2\n",
      "Gradient Descent(28/49): loss=1162328680698189.8\n",
      "Gradient Descent(29/49): loss=4465201859769799.0\n",
      "Gradient Descent(30/49): loss=1.7153519464491268e+16\n",
      "Gradient Descent(31/49): loss=6.589696037478533e+16\n",
      "Gradient Descent(32/49): loss=2.5314976297576262e+17\n",
      "Gradient Descent(33/49): loss=9.725001294476476e+17\n",
      "Gradient Descent(34/49): loss=3.735956497286343e+18\n",
      "Gradient Descent(35/49): loss=1.435205047997617e+19\n",
      "Gradient Descent(36/49): loss=5.513483712388094e+19\n",
      "Gradient Descent(37/49): loss=2.118059902950942e+20\n",
      "Gradient Descent(38/49): loss=8.136738923176726e+20\n",
      "Gradient Descent(39/49): loss=3.125809624727898e+21\n",
      "Gradient Descent(40/49): loss=1.2008110254355361e+22\n",
      "Gradient Descent(41/49): loss=4.6130356353130364e+22\n",
      "Gradient Descent(42/49): loss=1.7721437696618887e+23\n",
      "Gradient Descent(43/49): loss=6.80786750553355e+23\n",
      "Gradient Descent(44/49): loss=2.6153103809259456e+24\n",
      "Gradient Descent(45/49): loss=1.0046976359365392e+25\n",
      "Gradient Descent(46/49): loss=3.859646438213532e+25\n",
      "Gradient Descent(47/49): loss=1.4827217757040586e+26\n",
      "Gradient Descent(48/49): loss=5.6960239735447566e+26\n",
      "Gradient Descent(49/49): loss=2.188184569676892e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6404423290880006\n",
      "Gradient Descent(2/49): loss=1.1799655805124305\n",
      "Gradient Descent(3/49): loss=3.2525981031848197\n",
      "Gradient Descent(4/49): loss=11.214823202283187\n",
      "Gradient Descent(5/49): loss=41.80250714297674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=159.30815376955013\n",
      "Gradient Descent(7/49): loss=610.7178458501925\n",
      "Gradient Descent(8/49): loss=2344.85331894718\n",
      "Gradient Descent(9/49): loss=9006.708152397143\n",
      "Gradient Descent(10/49): loss=34598.88968057494\n",
      "Gradient Descent(11/49): loss=132913.81423923324\n",
      "Gradient Descent(12/49): loss=510600.42842374684\n",
      "Gradient Descent(13/49): loss=1961521.3254749414\n",
      "Gradient Descent(14/49): loss=7535379.04358685\n",
      "Gradient Descent(15/49): loss=28947910.85348521\n",
      "Gradient Descent(16/49): loss=111206293.05438988\n",
      "Gradient Descent(17/49): loss=427210094.117368\n",
      "Gradient Descent(18/49): loss=1641170296.2808027\n",
      "Gradient Descent(19/49): loss=6304719808.911851\n",
      "Gradient Descent(20/49): loss=24220211616.636677\n",
      "Gradient Descent(21/49): loss=93044364945.19864\n",
      "Gradient Descent(22/49): loss=357439232372.17487\n",
      "Gradient Descent(23/49): loss=1373138555079.698\n",
      "Gradient Descent(24/49): loss=5275049073192.818\n",
      "Gradient Descent(25/49): loss=20264628519577.66\n",
      "Gradient Descent(26/49): loss=77848596920808.8\n",
      "Gradient Descent(27/49): loss=299063169930982.1\n",
      "Gradient Descent(28/49): loss=1148881073606861.0\n",
      "Gradient Descent(29/49): loss=4413541532368413.5\n",
      "Gradient Descent(30/49): loss=1.6955061150747858e+16\n",
      "Gradient Descent(31/49): loss=6.513456291670929e+16\n",
      "Gradient Descent(32/49): loss=2.502209369008017e+17\n",
      "Gradient Descent(33/49): loss=9.612487511981869e+17\n",
      "Gradient Descent(34/49): loss=3.69273320260323e+18\n",
      "Gradient Descent(35/49): loss=1.4186003871119122e+19\n",
      "Gradient Descent(36/49): loss=5.449695247128929e+19\n",
      "Gradient Descent(37/49): loss=2.0935549261370956e+20\n",
      "Gradient Descent(38/49): loss=8.04260060424767e+20\n",
      "Gradient Descent(39/49): loss=3.0896454481277726e+21\n",
      "Gradient Descent(40/49): loss=1.1869181953527316e+22\n",
      "Gradient Descent(41/49): loss=4.559664939266879e+22\n",
      "Gradient Descent(42/49): loss=1.7516408830686394e+23\n",
      "Gradient Descent(43/49): loss=6.72910361639576e+23\n",
      "Gradient Descent(44/49): loss=2.58505244527445e+24\n",
      "Gradient Descent(45/49): loss=9.930737473766975e+24\n",
      "Gradient Descent(46/49): loss=3.8149921079219784e+25\n",
      "Gradient Descent(47/49): loss=1.4655673681793542e+26\n",
      "Gradient Descent(48/49): loss=5.630123601598146e+26\n",
      "Gradient Descent(49/49): loss=2.1628682827900262e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6415882076979205\n",
      "Gradient Descent(2/49): loss=1.1855134663902922\n",
      "Gradient Descent(3/49): loss=3.275056740182604\n",
      "Gradient Descent(4/49): loss=11.302246180783374\n",
      "Gradient Descent(5/49): loss=42.139497135798116\n",
      "Gradient Descent(6/49): loss=160.6038804045847\n",
      "Gradient Descent(7/49): loss=615.69665516996\n",
      "Gradient Descent(8/49): loss=2363.9810587086763\n",
      "Gradient Descent(9/49): loss=9080.190423343467\n",
      "Gradient Descent(10/49): loss=34881.18031852527\n",
      "Gradient Descent(11/49): loss=133998.26309986218\n",
      "Gradient Descent(12/49): loss=514766.4483126129\n",
      "Gradient Descent(13/49): loss=1977525.5086260664\n",
      "Gradient Descent(14/49): loss=7596860.71472651\n",
      "Gradient Descent(15/49): loss=29184098.842480626\n",
      "Gradient Descent(16/49): loss=112113632.83406362\n",
      "Gradient Descent(17/49): loss=430695730.61610407\n",
      "Gradient Descent(18/49): loss=1654560717.4556077\n",
      "Gradient Descent(19/49): loss=6356160450.898722\n",
      "Gradient Descent(20/49): loss=24417825986.89166\n",
      "Gradient Descent(21/49): loss=93803520309.97488\n",
      "Gradient Descent(22/49): loss=360355603621.50806\n",
      "Gradient Descent(23/49): loss=1384342086871.118\n",
      "Gradient Descent(24/49): loss=5318088560923.018\n",
      "Gradient Descent(25/49): loss=20429969015640.426\n",
      "Gradient Descent(26/49): loss=78483768970485.7\n",
      "Gradient Descent(27/49): loss=301503246876996.56\n",
      "Gradient Descent(28/49): loss=1158254873202576.0\n",
      "Gradient Descent(29/49): loss=4449551920894911.5\n",
      "Gradient Descent(30/49): loss=1.7093398659310804e+16\n",
      "Gradient Descent(31/49): loss=6.5666000289612776e+16\n",
      "Gradient Descent(32/49): loss=2.522625067125842e+17\n",
      "Gradient Descent(33/49): loss=9.690916457869743e+17\n",
      "Gradient Descent(34/49): loss=3.7228624664549673e+18\n",
      "Gradient Descent(35/49): loss=1.4301748451131922e+19\n",
      "Gradient Descent(36/49): loss=5.49415968498641e+19\n",
      "Gradient Descent(37/49): loss=2.1106363845844985e+20\n",
      "Gradient Descent(38/49): loss=8.108220735019343e+20\n",
      "Gradient Descent(39/49): loss=3.1148540775653237e+21\n",
      "Gradient Descent(40/49): loss=1.1966023424374475e+22\n",
      "Gradient Descent(41/49): loss=4.596867558707324e+22\n",
      "Gradient Descent(42/49): loss=1.7659326413528823e+23\n",
      "Gradient Descent(43/49): loss=6.784006835021282e+23\n",
      "Gradient Descent(44/49): loss=2.606144065741638e+24\n",
      "Gradient Descent(45/49): loss=1.0011763042952094e+25\n",
      "Gradient Descent(46/49): loss=3.8461188905807255e+25\n",
      "Gradient Descent(47/49): loss=1.477525033005357e+26\n",
      "Gradient Descent(48/49): loss=5.67606016679357e+26\n",
      "Gradient Descent(49/49): loss=2.180515273675362e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6443893984479235\n",
      "Gradient Descent(2/49): loss=1.2161858552416047\n",
      "Gradient Descent(3/49): loss=3.4805570037899125\n",
      "Gradient Descent(4/49): loss=12.447693189155718\n",
      "Gradient Descent(5/49): loss=47.9584491968214\n",
      "Gradient Descent(6/49): loss=188.58459406278777\n",
      "Gradient Descent(7/49): loss=745.478190346561\n",
      "Gradient Descent(8/49): loss=2950.8325209900663\n",
      "Gradient Descent(9/49): loss=11684.256205772435\n",
      "Gradient Descent(10/49): loss=46269.487339882915\n",
      "Gradient Descent(11/49): loss=183230.46115408072\n",
      "Gradient Descent(12/49): loss=725609.613555651\n",
      "Gradient Descent(13/49): loss=2873485.2949813176\n",
      "Gradient Descent(14/49): loss=11379287.780993996\n",
      "Gradient Descent(15/49): loss=45063116.205848664\n",
      "Gradient Descent(16/49): loss=178454445.15113363\n",
      "Gradient Descent(17/49): loss=706697446.9073949\n",
      "Gradient Descent(18/49): loss=2798592558.1625896\n",
      "Gradient Descent(19/49): loss=11082706388.243727\n",
      "Gradient Descent(20/49): loss=43888625566.74671\n",
      "Gradient Descent(21/49): loss=173803346105.5422\n",
      "Gradient Descent(22/49): loss=688278630911.1898\n",
      "Gradient Descent(23/49): loss=2725652206269.764\n",
      "Gradient Descent(24/49): loss=10793855302048.102\n",
      "Gradient Descent(25/49): loss=42744746381642.19\n",
      "Gradient Descent(26/49): loss=169273470145935.28\n",
      "Gradient Descent(27/49): loss=670339869124854.2\n",
      "Gradient Descent(28/49): loss=2654612915721245.0\n",
      "Gradient Descent(29/49): loss=1.0512532607548484e+16\n",
      "Gradient Descent(30/49): loss=4.163068037915187e+16\n",
      "Gradient Descent(31/49): loss=1.6486165736949456e+17\n",
      "Gradient Descent(32/49): loss=6.528686493489144e+17\n",
      "Gradient Descent(33/49): loss=2.585425138286417e+18\n",
      "Gradient Descent(34/49): loss=1.0238542090127833e+19\n",
      "Gradient Descent(35/49): loss=4.0545650531112894e+19\n",
      "Gradient Descent(36/49): loss=1.605648306682731e+20\n",
      "Gradient Descent(37/49): loss=6.358527859294527e+20\n",
      "Gradient Descent(38/49): loss=2.5180406175592984e+21\n",
      "Gradient Descent(39/49): loss=9.971692649595963e+21\n",
      "Gradient Descent(40/49): loss=3.948890006166293e+22\n",
      "Gradient Descent(41/49): loss=1.5637999313419943e+23\n",
      "Gradient Descent(42/49): loss=6.192804108107991e+23\n",
      "Gradient Descent(43/49): loss=2.4524123548520466e+24\n",
      "Gradient Descent(44/49): loss=9.71179816645004e+24\n",
      "Gradient Descent(45/49): loss=3.8459691918962284e+25\n",
      "Gradient Descent(46/49): loss=1.5230422596828841e+26\n",
      "Gradient Descent(47/49): loss=6.031399652570484e+26\n",
      "Gradient Descent(48/49): loss=2.3884945764146028e+27\n",
      "Gradient Descent(49/49): loss=9.458677372059131e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6480114595328518\n",
      "Gradient Descent(2/49): loss=1.2341516404288424\n",
      "Gradient Descent(3/49): loss=3.555325370795453\n",
      "Gradient Descent(4/49): loss=12.747405460420717\n",
      "Gradient Descent(5/49): loss=49.14896182334114\n",
      "Gradient Descent(6/49): loss=193.30276517615494\n",
      "Gradient Descent(7/49): loss=764.1662418336114\n",
      "Gradient Descent(8/49): loss=3024.8426957445945\n",
      "Gradient Descent(9/49): loss=11977.347520878751\n",
      "Gradient Descent(10/49): loss=47430.16187888951\n",
      "Gradient Descent(11/49): loss=187826.85201805647\n",
      "Gradient Descent(12/49): loss=743811.7846381348\n",
      "Gradient Descent(13/49): loss=2945567.7163067684\n",
      "Gradient Descent(14/49): loss=11664741.381308632\n",
      "Gradient Descent(15/49): loss=46193541.01207773\n",
      "Gradient Descent(16/49): loss=182931040.42988336\n",
      "Gradient Descent(17/49): loss=724425211.8743774\n",
      "Gradient Descent(18/49): loss=2868796280.211489\n",
      "Gradient Descent(19/49): loss=11360720147.934631\n",
      "Gradient Descent(20/49): loss=44989587856.505714\n",
      "Gradient Descent(21/49): loss=178163266869.2244\n",
      "Gradient Descent(22/49): loss=705544353127.5247\n",
      "Gradient Descent(23/49): loss=2794026192819.159\n",
      "Gradient Descent(24/49): loss=11064623126180.928\n",
      "Gradient Descent(25/49): loss=43817014041990.89\n",
      "Gradient Descent(26/49): loss=173519757307675.2\n",
      "Gradient Descent(27/49): loss=687155590914164.9\n",
      "Gradient Descent(28/49): loss=2721204855578908.0\n",
      "Gradient Descent(29/49): loss=1.077624334857769e+16\n",
      "Gradient Descent(30/49): loss=4.26750012847049e+16\n",
      "Gradient Descent(31/49): loss=1.6899727258754838e+17\n",
      "Gradient Descent(32/49): loss=6.692460991740076e+17\n",
      "Gradient Descent(33/49): loss=2.650281477339248e+18\n",
      "Gradient Descent(34/49): loss=1.0495379678411207e+19\n",
      "Gradient Descent(35/49): loss=4.156275306447549e+19\n",
      "Gradient Descent(36/49): loss=1.6459265841062258e+20\n",
      "Gradient Descent(37/49): loss=6.51803386571945e+20\n",
      "Gradient Descent(38/49): loss=2.5812065911633887e+21\n",
      "Gradient Descent(39/49): loss=1.022183622166585e+22\n",
      "Gradient Descent(40/49): loss=4.0479493621422865e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/49): loss=1.6030284269017955e+23\n",
      "Gradient Descent(42/49): loss=6.348152873374325e+23\n",
      "Gradient Descent(43/49): loss=2.5139320193850513e+24\n",
      "Gradient Descent(44/49): loss=9.955422189967363e+24\n",
      "Gradient Descent(45/49): loss=3.942446741449185e+25\n",
      "Gradient Descent(46/49): loss=1.561248334081288e+26\n",
      "Gradient Descent(47/49): loss=6.182699527795543e+26\n",
      "Gradient Descent(48/49): loss=2.4484108400022774e+27\n",
      "Gradient Descent(49/49): loss=9.695951767493764e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6462990351680001\n",
      "Gradient Descent(2/49): loss=1.225657844336867\n",
      "Gradient Descent(3/49): loss=3.519976664526195\n",
      "Gradient Descent(4/49): loss=12.605708624359469\n",
      "Gradient Descent(5/49): loss=48.58611575849568\n",
      "Gradient Descent(6/49): loss=191.07212605037736\n",
      "Gradient Descent(7/49): loss=755.3309754072304\n",
      "Gradient Descent(8/49): loss=2989.8524447454747\n",
      "Gradient Descent(9/49): loss=11838.780915472218\n",
      "Gradient Descent(10/49): loss=46881.422552393684\n",
      "Gradient Descent(11/49): loss=185653.78769875137\n",
      "Gradient Descent(12/49): loss=735206.2309148118\n",
      "Gradient Descent(13/49): loss=2911488.8612950025\n",
      "Gradient Descent(14/49): loss=11529785.705864185\n",
      "Gradient Descent(15/49): loss=45659103.04003922\n",
      "Gradient Descent(16/49): loss=180814612.61512193\n",
      "Gradient Descent(17/49): loss=716043946.0833318\n",
      "Gradient Descent(18/49): loss=2835605629.550918\n",
      "Gradient Descent(19/49): loss=11229281852.24999\n",
      "Gradient Descent(20/49): loss=44469079061.762764\n",
      "Gradient Descent(21/49): loss=176101999991.16202\n",
      "Gradient Descent(22/49): loss=697381530163.7229\n",
      "Gradient Descent(23/49): loss=2761700597599.9434\n",
      "Gradient Descent(24/49): loss=10936610536554.553\n",
      "Gradient Descent(25/49): loss=43310071385809.85\n",
      "Gradient Descent(26/49): loss=171512213694947.47\n",
      "Gradient Descent(27/49): loss=679205517453321.9\n",
      "Gradient Descent(28/49): loss=2689721769666884.0\n",
      "Gradient Descent(29/49): loss=1.0651567180058784e+16\n",
      "Gradient Descent(30/49): loss=4.218127118974935e+16\n",
      "Gradient Descent(31/49): loss=1.6704205203853482e+17\n",
      "Gradient Descent(32/49): loss=6.615032302777496e+17\n",
      "Gradient Descent(33/49): loss=2.6196189422229827e+18\n",
      "Gradient Descent(34/49): loss=1.0373952973096573e+19\n",
      "Gradient Descent(35/49): loss=4.1081891168764215e+19\n",
      "Gradient Descent(36/49): loss=1.6268839721741343e+20\n",
      "Gradient Descent(37/49): loss=6.442623218207145e+20\n",
      "Gradient Descent(38/49): loss=2.5513432206422064e+21\n",
      "Gradient Descent(39/49): loss=1.0103574288065647e+22\n",
      "Gradient Descent(40/49): loss=4.001116453817102e+22\n",
      "Gradient Descent(41/49): loss=1.5844821268761977e+23\n",
      "Gradient Descent(42/49): loss=6.274707670642099e+23\n",
      "Gradient Descent(43/49): loss=2.484846984651194e+24\n",
      "Gradient Descent(44/49): loss=9.840242543916493e+24\n",
      "Gradient Descent(45/49): loss=3.896834449816495e+25\n",
      "Gradient Descent(46/49): loss=1.5431854104716815e+26\n",
      "Gradient Descent(47/49): loss=6.111168544008696e+26\n",
      "Gradient Descent(48/49): loss=2.4200838551128717e+27\n",
      "Gradient Descent(49/49): loss=9.583774074632314e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6474926990451202\n",
      "Gradient Descent(2/49): loss=1.2315785365336436\n",
      "Gradient Descent(3/49): loss=3.544616861572024\n",
      "Gradient Descent(4/49): loss=12.704479932555811\n",
      "Gradient Descent(5/49): loss=48.978453679955194\n",
      "Gradient Descent(6/49): loss=192.62701711704767\n",
      "Gradient Descent(7/49): loss=761.4896931841843\n",
      "Gradient Descent(8/49): loss=3014.24277667747\n",
      "Gradient Descent(9/49): loss=11935.370262620238\n",
      "Gradient Descent(10/49): loss=47263.9272197042\n",
      "Gradient Descent(11/49): loss=187168.54562545946\n",
      "Gradient Descent(12/49): loss=741204.8249740556\n",
      "Gradient Descent(13/49): loss=2935243.8948225887\n",
      "Gradient Descent(14/49): loss=11623858.015328938\n",
      "Gradient Descent(15/49): loss=46031638.79394638\n",
      "Gradient Descent(16/49): loss=182289891.45533624\n",
      "Gradient Descent(17/49): loss=721886197.8196768\n",
      "Gradient Descent(18/49): loss=2858741530.653202\n",
      "Gradient Descent(19/49): loss=11320902334.207247\n",
      "Gradient Descent(20/49): loss=44831905332.35901\n",
      "Gradient Descent(21/49): loss=177538828305.3623\n",
      "Gradient Descent(22/49): loss=703071513970.7024\n",
      "Gradient Descent(23/49): loss=2784233502474.001\n",
      "Gradient Descent(24/49): loss=11025843093146.623\n",
      "Gradient Descent(25/49): loss=43663441233166.7\n",
      "Gradient Descent(26/49): loss=172911593627468.4\n",
      "Gradient Descent(27/49): loss=684747201924192.0\n",
      "Gradient Descent(28/49): loss=2711667394339960.5\n",
      "Gradient Descent(29/49): loss=1.0738474048326294e+16\n",
      "Gradient Descent(30/49): loss=4.252543107877629e+16\n",
      "Gradient Descent(31/49): loss=1.6840495961507085e+17\n",
      "Gradient Descent(32/49): loss=6.669004805716287e+17\n",
      "Gradient Descent(33/49): loss=2.6409925931119565e+18\n",
      "Gradient Descent(34/49): loss=1.045859476798337e+19\n",
      "Gradient Descent(35/49): loss=4.1417081140687995e+19\n",
      "Gradient Descent(36/49): loss=1.640157830252276e+20\n",
      "Gradient Descent(37/49): loss=6.495189023581718e+20\n",
      "Gradient Descent(38/49): loss=2.5721598052287916e+21\n",
      "Gradient Descent(39/49): loss=1.0186010044687595e+22\n",
      "Gradient Descent(40/49): loss=4.033761837796845e+22\n",
      "Gradient Descent(41/49): loss=1.5974100253860193e+23\n",
      "Gradient Descent(42/49): loss=6.325903441531551e+23\n",
      "Gradient Descent(43/49): loss=2.5051210218807413e+24\n",
      "Gradient Descent(44/49): loss=9.92052975874948e+24\n",
      "Gradient Descent(45/49): loss=3.928628989762211e+25\n",
      "Gradient Descent(46/49): loss=1.5557763662358452e+26\n",
      "Gradient Descent(47/49): loss=6.161029987930392e+26\n",
      "Gradient Descent(48/49): loss=2.4398294855202987e+27\n",
      "Gradient Descent(49/49): loss=9.661968745608565e+27\n"
     ]
    }
   ],
   "source": [
    "# Si tu fais ca avec tous les features, ca prend beaucoup de temps (>1h) et ne converge pas\n",
    "# en prenant x_kept on converge vers loss=0.42 et gamma_opt = 2.51 \n",
    "# je pense qu'il faut trouver un just milieu entre 30 et 3 features \n",
    "max_iters = 50\n",
    "k_fold = 4\n",
    "initial_w = np.zeros(x.shape[1])\n",
    "gammas = np.arange(0, 3, 0.03)\n",
    "gamma_opt = cross_validation(y, x2, k_fold, gammas, fonction=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8200000000000012\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44440890578566167\n",
      "Gradient Descent(2/49): loss=0.44024983391625105\n",
      "Gradient Descent(3/49): loss=0.4376256471028858\n",
      "Gradient Descent(4/49): loss=0.4354831764450516\n",
      "Gradient Descent(5/49): loss=0.4337218497498242\n",
      "Gradient Descent(6/49): loss=0.43227362758912535\n",
      "Gradient Descent(7/49): loss=0.4310828460324516\n",
      "Gradient Descent(8/49): loss=0.4301037415182902\n",
      "Gradient Descent(9/49): loss=0.4292986856635453\n",
      "Gradient Descent(10/49): loss=0.4286367390275766\n",
      "Gradient Descent(11/49): loss=0.42809246206268226\n",
      "Gradient Descent(12/49): loss=0.427644937435685\n",
      "Gradient Descent(13/49): loss=0.4272769661447635\n",
      "Gradient Descent(14/49): loss=0.42697440653710034\n",
      "Gradient Descent(15/49): loss=0.42672563082484527\n",
      "Gradient Descent(16/49): loss=0.42652107821252283\n",
      "Gradient Descent(17/49): loss=0.42635288746192346\n",
      "Gradient Descent(18/49): loss=0.4262145947734255\n",
      "Gradient Descent(19/49): loss=0.4261008853728879\n",
      "Gradient Descent(20/49): loss=0.4260073892572438\n",
      "Gradient Descent(21/49): loss=0.42593051324900405\n",
      "Gradient Descent(22/49): loss=0.4258673029052834\n",
      "Gradient Descent(23/49): loss=0.42581532897431584\n",
      "Gradient Descent(24/49): loss=0.42577259403581846\n",
      "Gradient Descent(25/49): loss=0.4257374557372602\n",
      "Gradient Descent(26/49): loss=0.42570856367589466\n",
      "Gradient Descent(27/49): loss=0.4256848075008423\n",
      "Gradient Descent(28/49): loss=0.4256652742407103\n",
      "Gradient Descent(29/49): loss=0.42564921321679033\n",
      "Gradient Descent(30/49): loss=0.4256360071933973\n",
      "Gradient Descent(31/49): loss=0.42562514865661577\n",
      "Gradient Descent(32/49): loss=0.42561622030981017\n",
      "Gradient Descent(33/49): loss=0.42560887903631534\n",
      "Gradient Descent(34/49): loss=0.4256028427129711\n",
      "Gradient Descent(35/49): loss=0.4255978793677227\n",
      "Gradient Descent(36/49): loss=0.4255937982646027\n",
      "Gradient Descent(37/49): loss=0.4255904425734726\n",
      "Gradient Descent(38/49): loss=0.4255876833428159\n",
      "Gradient Descent(39/49): loss=0.4255854145439454\n",
      "Gradient Descent(40/49): loss=0.42558354899616974\n",
      "Gradient Descent(41/49): loss=0.42558201501631243\n",
      "Gradient Descent(42/49): loss=0.425580753663826\n",
      "Gradient Descent(43/49): loss=0.42557971647562065\n",
      "Gradient Descent(44/49): loss=0.425578863603559\n",
      "Gradient Descent(45/49): loss=0.4255781622830352\n",
      "Gradient Descent(46/49): loss=0.42557758557378256\n",
      "Gradient Descent(47/49): loss=0.42557711132452236\n",
      "Gradient Descent(48/49): loss=0.4255767213216565\n",
      "Gradient Descent(49/49): loss=0.42557640058929463\n",
      "Gradient descent regression loss 0.42557640058929463\n"
     ]
    }
   ],
   "source": [
    "w_gd, loss_gd = least_squares_GD(y, x2, gamma_opt, max_iters=max_iters)\n",
    "print(\"Gradient descent regression loss {loss} with optimal gamma {g}\".format(loss=loss_gd, g=gamma_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv for gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4971171874435754\n",
      "Gradient Descent(2/49): loss=0.49440474910923576\n",
      "Gradient Descent(3/49): loss=0.49185261588045526\n",
      "Gradient Descent(4/49): loss=0.4894513137254959\n",
      "Gradient Descent(5/49): loss=0.48719192852789467\n",
      "Gradient Descent(6/49): loss=0.4850660729954716\n",
      "Gradient Descent(7/49): loss=0.48306585552501446\n",
      "Gradient Descent(8/49): loss=0.4811838509070617\n",
      "Gradient Descent(9/49): loss=0.4794130727620297\n",
      "Gradient Descent(10/49): loss=0.4777469476053693\n",
      "Gradient Descent(11/49): loss=0.4761792904454675\n",
      "Gradient Descent(12/49): loss=0.4747042818237156\n",
      "Gradient Descent(13/49): loss=0.47331644621150964\n",
      "Gradient Descent(14/49): loss=0.47201063168398477\n",
      "Gradient Descent(15/49): loss=0.47078199079503685\n",
      "Gradient Descent(16/49): loss=0.46962596258262573\n",
      "Gradient Descent(17/49): loss=0.46853825563756774\n",
      "Gradient Descent(18/49): loss=0.46751483217296325\n",
      "Gradient Descent(19/49): loss=0.46655189303511646\n",
      "Gradient Descent(20/49): loss=0.4656458636003165\n",
      "Gradient Descent(21/49): loss=0.46479338050511326\n",
      "Gradient Descent(22/49): loss=0.46399127916083666\n",
      "Gradient Descent(23/49): loss=0.4632365820060066\n",
      "Gradient Descent(24/49): loss=0.4625264874530274\n",
      "Gradient Descent(25/49): loss=0.46185835948812887\n",
      "Gradient Descent(26/49): loss=0.4612297178859556\n",
      "Gradient Descent(27/49): loss=0.46063822900247126\n",
      "Gradient Descent(28/49): loss=0.4600816971120005\n",
      "Gradient Descent(29/49): loss=0.4595580562562566\n",
      "Gradient Descent(30/49): loss=0.45906536257508745\n",
      "Gradient Descent(31/49): loss=0.4586017870904753\n",
      "Gradient Descent(32/49): loss=0.4581656089170034\n",
      "Gradient Descent(33/49): loss=0.4577552088735842\n",
      "Gradient Descent(34/49): loss=0.4573690634727306\n",
      "Gradient Descent(35/49): loss=0.45700573926506805\n",
      "Gradient Descent(36/49): loss=0.4566638875180778\n",
      "Gradient Descent(37/49): loss=0.4563422392093349\n",
      "Gradient Descent(38/49): loss=0.45603960031563856\n",
      "Gradient Descent(39/49): loss=0.4557548473805599\n",
      "Gradient Descent(40/49): loss=0.4554869233439443\n",
      "Gradient Descent(41/49): loss=0.4552348336178924\n",
      "Gradient Descent(42/49): loss=0.45499764239465074\n",
      "Gradient Descent(43/49): loss=0.45477446917270214\n",
      "Gradient Descent(44/49): loss=0.45456448548817097\n",
      "Gradient Descent(45/49): loss=0.4543669118393957\n",
      "Gradient Descent(46/49): loss=0.45418101479326284\n",
      "Gradient Descent(47/49): loss=0.45400610426255644\n",
      "Gradient Descent(48/49): loss=0.45384153094421464\n",
      "Gradient Descent(49/49): loss=0.4536866839089873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49704487103192735\n",
      "Gradient Descent(2/49): loss=0.4942643901858682\n",
      "Gradient Descent(3/49): loss=0.49164823575781075\n",
      "Gradient Descent(4/49): loss=0.48918669605645165\n",
      "Gradient Descent(5/49): loss=0.4868706333514427\n",
      "Gradient Descent(6/49): loss=0.4846914499523\n",
      "Gradient Descent(7/49): loss=0.48264105629204657\n",
      "Gradient Descent(8/49): loss=0.4807118408971141\n",
      "Gradient Descent(9/49): loss=0.4788966421320223\n",
      "Gradient Descent(10/49): loss=0.4771887216139473\n",
      "Gradient Descent(11/49): loss=0.47558173919849034\n",
      "Gradient Descent(12/49): loss=0.4740697294437869\n",
      "Gradient Descent(13/49): loss=0.47264707946558676\n",
      "Gradient Descent(14/49): loss=0.47130850810109803\n",
      "Gradient Descent(15/49): loss=0.4700490463042507\n",
      "Gradient Descent(16/49): loss=0.46886401869959693\n",
      "Gradient Descent(17/49): loss=0.4677490262263778\n",
      "Gradient Descent(18/49): loss=0.46669992980832653\n",
      "Gradient Descent(19/49): loss=0.4657128349885821\n",
      "Gradient Descent(20/49): loss=0.4647840774726842\n",
      "Gradient Descent(21/49): loss=0.463910209525976\n",
      "Gradient Descent(22/49): loss=0.4630879871749184\n",
      "Gradient Descent(23/49): loss=0.46231435816480815\n",
      "Gradient Descent(24/49): loss=0.4615864506291955\n",
      "Gradient Descent(25/49): loss=0.46090156242893743\n",
      "Gradient Descent(26/49): loss=0.4602571511213147\n",
      "Gradient Descent(27/49): loss=0.45965082452197265\n",
      "Gradient Descent(28/49): loss=0.45908033182465124\n",
      "Gradient Descent(29/49): loss=0.458543555245742\n",
      "Gradient Descent(30/49): loss=0.45803850216264613\n",
      "Gradient Descent(31/49): loss=0.4575632977167609\n",
      "Gradient Descent(32/49): loss=0.4571161778536279\n",
      "Gradient Descent(33/49): loss=0.45669548277440614\n",
      "Gradient Descent(34/49): loss=0.4562996507743659\n",
      "Gradient Descent(35/49): loss=0.4559272124455286\n",
      "Gradient Descent(36/49): loss=0.4555767852219254\n",
      "Gradient Descent(37/49): loss=0.455247068247237\n",
      "Gradient Descent(38/49): loss=0.454936837545753\n",
      "Gradient Descent(39/49): loss=0.4546449414787262\n",
      "Gradient Descent(40/49): loss=0.4543702964692608\n",
      "Gradient Descent(41/49): loss=0.4541118829798551\n",
      "Gradient Descent(42/49): loss=0.4538687417276732\n",
      "Gradient Descent(43/49): loss=0.4536399701234953\n",
      "Gradient Descent(44/49): loss=0.45342471892112407\n",
      "Gradient Descent(45/49): loss=0.4532221890648131\n",
      "Gradient Descent(46/49): loss=0.4530316287230103\n",
      "Gradient Descent(47/49): loss=0.4528523304974076\n",
      "Gradient Descent(48/49): loss=0.4526836287969384\n",
      "Gradient Descent(49/49): loss=0.45252489736696666\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49707906051200007\n",
      "Gradient Descent(2/49): loss=0.4943307485477406\n",
      "Gradient Descent(3/49): loss=0.4917448618205693\n",
      "Gradient Descent(4/49): loss=0.48931180099897376\n",
      "Gradient Descent(5/49): loss=0.4870225340719344\n",
      "Gradient Descent(6/49): loss=0.48486856282028307\n",
      "Gradient Descent(7/49): loss=0.4828418912696044\n",
      "Gradient Descent(8/49): loss=0.48093499600757084\n",
      "Gradient Descent(9/49): loss=0.47914079825552336\n",
      "Gradient Descent(10/49): loss=0.47745263759062184\n",
      "Gradient Descent(11/49): loss=0.47586424722101633\n",
      "Gradient Descent(12/49): loss=0.47436973072225413\n",
      "Gradient Descent(13/49): loss=0.4729635401485689\n",
      "Gradient Descent(14/49): loss=0.4716404554377884\n",
      "Gradient Descent(15/49): loss=0.4703955650334151\n",
      "Gradient Descent(16/49): loss=0.4692242476519404\n",
      "Gradient Descent(17/49): loss=0.4681221551277106\n",
      "Gradient Descent(18/49): loss=0.4670851962716629\n",
      "Gradient Descent(19/49): loss=0.46610952168400765\n",
      "Gradient Descent(20/49): loss=0.46519150946448296\n",
      "Gradient Descent(21/49): loss=0.46432775176713187\n",
      "Gradient Descent(22/49): loss=0.4635150421496944\n",
      "Gradient Descent(23/49): loss=0.46275036367064754\n",
      "Gradient Descent(24/49): loss=0.46203087768971207\n",
      "Gradient Descent(25/49): loss=0.46135391333025016\n",
      "Gradient Descent(26/49): loss=0.4607169575644324\n",
      "Gradient Descent(27/49): loss=0.4601176458843745\n",
      "Gradient Descent(28/49): loss=0.45955375352460803\n",
      "Gradient Descent(29/49): loss=0.45902318720330354\n",
      "Gradient Descent(30/49): loss=0.4585239773515883\n",
      "Gradient Descent(31/49): loss=0.45805427080210936\n",
      "Gradient Descent(32/49): loss=0.4576123239097048\n",
      "Gradient Descent(33/49): loss=0.45719649607864116\n",
      "Gradient Descent(34/49): loss=0.4568052436723935\n",
      "Gradient Descent(35/49): loss=0.4564371142833552\n",
      "Gradient Descent(36/49): loss=0.45609074134120886\n",
      "Gradient Descent(37/49): loss=0.45576483903994347\n",
      "Gradient Descent(38/49): loss=0.4554581975646826\n",
      "Gradient Descent(39/49): loss=0.45516967860060986\n",
      "Gradient Descent(40/49): loss=0.45489821110731393\n",
      "Gradient Descent(41/49): loss=0.45464278734287167\n",
      "Gradient Descent(42/49): loss=0.45440245912290805\n",
      "Gradient Descent(43/49): loss=0.45417633430074394\n",
      "Gradient Descent(44/49): loss=0.4539635734555701\n",
      "Gradient Descent(45/49): loss=0.4537633867763461\n",
      "Gradient Descent(46/49): loss=0.4535750311298639\n",
      "Gradient Descent(47/49): loss=0.45339780730208895\n",
      "Gradient Descent(48/49): loss=0.45323105740253544\n",
      "Gradient Descent(49/49): loss=0.4530741624220457\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49705522836608007\n",
      "Gradient Descent(2/49): loss=0.49428449273572483\n",
      "Gradient Descent(3/49): loss=0.4916775075811232\n",
      "Gradient Descent(4/49): loss=0.48922459524915896\n",
      "Gradient Descent(5/49): loss=0.4869166500360139\n",
      "Gradient Descent(6/49): loss=0.4847451043849656\n",
      "Gradient Descent(7/49): loss=0.4827018970818941\n",
      "Gradient Descent(8/49): loss=0.4807794433304341\n",
      "Gradient Descent(9/49): loss=0.4789706065956852\n",
      "Gradient Descent(10/49): loss=0.4772686721119602\n",
      "Gradient Descent(11/49): loss=0.47566732195622363\n",
      "Gradient Descent(12/49): loss=0.4741606115946908\n",
      "Gradient Descent(13/49): loss=0.4727429478155244\n",
      "Gradient Descent(14/49): loss=0.47140906796570703\n",
      "Gradient Descent(15/49): loss=0.4701540204150135\n",
      "Gradient Descent(16/49): loss=0.46897314617456626\n",
      "Gradient Descent(17/49): loss=0.46786206160172966\n",
      "Gradient Descent(18/49): loss=0.46681664212714735\n",
      "Gradient Descent(19/49): loss=0.46583300694351293\n",
      "Gradient Descent(20/49): loss=0.4649075045992312\n",
      "Gradient Descent(21/49): loss=0.4640366994434966\n",
      "Gradient Descent(22/49): loss=0.46321735887246596\n",
      "Gradient Descent(23/49): loss=0.4624464413291834\n",
      "Gradient Descent(24/49): loss=0.46172108501270864\n",
      "Gradient Descent(25/49): loss=0.46103859725453744\n",
      "Gradient Descent(26/49): loss=0.46039644452287426\n",
      "Gradient Descent(27/49): loss=0.4597922430176522\n",
      "Gradient Descent(28/49): loss=0.4592237498213892\n",
      "Gradient Descent(29/49): loss=0.45868885457302516\n",
      "Gradient Descent(30/49): loss=0.45818557163383944\n",
      "Gradient Descent(31/49): loss=0.45771203271635946\n",
      "Gradient Descent(32/49): loss=0.4572664799489027\n",
      "Gradient Descent(33/49): loss=0.4568472593500024\n",
      "Gradient Descent(34/49): loss=0.45645281468849747\n",
      "Gradient Descent(35/49): loss=0.4560816817064872\n",
      "Gradient Descent(36/49): loss=0.45573248268371347\n",
      "Gradient Descent(37/49): loss=0.4554039213231862\n",
      "Gradient Descent(38/49): loss=0.455094777939066\n",
      "Gradient Descent(39/49): loss=0.4548039049289472\n",
      "Gradient Descent(40/49): loss=0.4545302225137262\n",
      "Gradient Descent(41/49): loss=0.45427271472924513\n",
      "Gradient Descent(42/49): loss=0.45403042565482676\n",
      "Gradient Descent(43/49): loss=0.4538024558647065\n",
      "Gradient Descent(44/49): loss=0.45358795908918226\n",
      "Gradient Descent(45/49): loss=0.4533861390730916\n",
      "Gradient Descent(46/49): loss=0.453196246619952\n",
      "Gradient Descent(47/49): loss=0.45301757681079263\n",
      "Gradient Descent(48/49): loss=0.4528494663873549\n",
      "Gradient Descent(49/49): loss=0.4526912912899423\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49432217628480857\n",
      "Gradient Descent(2/49): loss=0.4893052512500657\n",
      "Gradient Descent(3/49): loss=0.4848722962893662\n",
      "Gradient Descent(4/49): loss=0.4809553372860926\n",
      "Gradient Descent(5/49): loss=0.4774943123108001\n",
      "Gradient Descent(6/49): loss=0.4744361506426315\n",
      "Gradient Descent(7/49): loss=0.47173395899263754\n",
      "Gradient Descent(8/49): loss=0.4693463024507033\n",
      "Gradient Descent(9/49): loss=0.4672365691302498\n",
      "Gradient Descent(10/49): loss=0.46537240876829755\n",
      "Gradient Descent(11/49): loss=0.46372523667247617\n",
      "Gradient Descent(12/49): loss=0.4622697954086084\n",
      "Gradient Descent(13/49): loss=0.460983767507855\n",
      "Gradient Descent(14/49): loss=0.45984743325474914\n",
      "Gradient Descent(15/49): loss=0.45884336830870476\n",
      "Gradient Descent(16/49): loss=0.4579561765223802\n",
      "Gradient Descent(17/49): loss=0.4571722538599836\n",
      "Gradient Descent(18/49): loss=0.45647957979549003\n",
      "Gradient Descent(19/49): loss=0.4558675329921035\n",
      "Gradient Descent(20/49): loss=0.45532672843663136\n",
      "Gradient Descent(21/49): loss=0.45484887353141595\n",
      "Gradient Descent(22/49): loss=0.4544266409371676\n",
      "Gradient Descent(23/49): loss=0.45405355621689\n",
      "Gradient Descent(24/49): loss=0.4537238985580525\n",
      "Gradient Descent(25/49): loss=0.4534326130507036\n",
      "Gradient Descent(26/49): loss=0.4531752331764101\n",
      "Gradient Descent(27/49): loss=0.45294781231948444\n",
      "Gradient Descent(28/49): loss=0.452746863250305\n",
      "Gradient Descent(29/49): loss=0.45256930465277806\n",
      "Gradient Descent(30/49): loss=0.45241241387600334\n",
      "Gradient Descent(31/49): loss=0.4522737851856452\n",
      "Gradient Descent(32/49): loss=0.4521512928748445\n",
      "Gradient Descent(33/49): loss=0.4520430586690209\n",
      "Gradient Descent(34/49): loss=0.4519474229247556\n",
      "Gradient Descent(35/49): loss=0.45186291918112254\n",
      "Gradient Descent(36/49): loss=0.45178825167324843\n",
      "Gradient Descent(37/49): loss=0.45172227546329097\n",
      "Gradient Descent(38/49): loss=0.45166397888417265\n",
      "Gradient Descent(39/49): loss=0.4516124680268633\n",
      "Gradient Descent(40/49): loss=0.4515669530333449\n",
      "Gradient Descent(41/49): loss=0.4515267359850719\n",
      "Gradient Descent(42/49): loss=0.4514912002012181\n",
      "Gradient Descent(43/49): loss=0.45145980078260506\n",
      "Gradient Descent(44/49): loss=0.4514320562563184\n",
      "Gradient Descent(45/49): loss=0.4514075411928914\n",
      "Gradient Descent(46/49): loss=0.4513858796828473\n",
      "Gradient Descent(47/49): loss=0.4513667395725723\n",
      "Gradient Descent(48/49): loss=0.4513498273711335\n",
      "Gradient Descent(49/49): loss=0.4513348837499419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4941797459918164\n",
      "Gradient Descent(2/49): loss=0.48903696955018555\n",
      "Gradient Descent(3/49): loss=0.4844928122863606\n",
      "Gradient Descent(4/49): loss=0.48047759492804476\n",
      "Gradient Descent(5/49): loss=0.4769297488702371\n",
      "Gradient Descent(6/49): loss=0.47379487209355775\n",
      "Gradient Descent(7/49): loss=0.4710248949736843\n",
      "Gradient Descent(8/49): loss=0.468577343190564\n",
      "Gradient Descent(9/49): loss=0.46641468643499895\n",
      "Gradient Descent(10/49): loss=0.46450376292578155\n",
      "Gradient Descent(11/49): loss=0.46281527091303715\n",
      "Gradient Descent(12/49): loss=0.4613233193705762\n",
      "Gradient Descent(13/49): loss=0.4600050309876576\n",
      "Gradient Descent(14/49): loss=0.4588401913725109\n",
      "Gradient Descent(15/49): loss=0.4578109390885672\n",
      "Gradient Descent(16/49): loss=0.4569014917704742\n",
      "Gradient Descent(17/49): loss=0.4560979041202077\n",
      "Gradient Descent(18/49): loss=0.45538785407243226\n",
      "Gradient Descent(19/49): loss=0.45476045385021757\n",
      "Gradient Descent(20/49): loss=0.4542060830138685\n",
      "Gradient Descent(21/49): loss=0.4537162409428709\n",
      "Gradient Descent(22/49): loss=0.4532834164889374\n",
      "Gradient Descent(23/49): loss=0.4529009728014414\n",
      "Gradient Descent(24/49): loss=0.4525630455591702\n",
      "Gradient Descent(25/49): loss=0.4522644530478995\n",
      "Gradient Descent(26/49): loss=0.45200061670494035\n",
      "Gradient Descent(27/49): loss=0.4517674909123016\n",
      "Gradient Descent(28/49): loss=0.4515615009619267\n",
      "Gradient Descent(29/49): loss=0.4513794882417747\n",
      "Gradient Descent(30/49): loss=0.45121866180224873\n",
      "Gradient Descent(31/49): loss=0.45107655556028337\n",
      "Gradient Descent(32/49): loss=0.45095099048488324\n",
      "Gradient Descent(33/49): loss=0.4508400411842592\n",
      "Gradient Descent(34/49): loss=0.45074200638222806\n",
      "Gradient Descent(35/49): loss=0.45065538283115325\n",
      "Gradient Descent(36/49): loss=0.45057884226142336\n",
      "Gradient Descent(37/49): loss=0.4505112110140103\n",
      "Gradient Descent(38/49): loss=0.45045145204379605\n",
      "Gradient Descent(39/49): loss=0.45039864901771465\n",
      "Gradient Descent(40/49): loss=0.4503519922638693\n",
      "Gradient Descent(41/49): loss=0.45031076635617134\n",
      "Gradient Descent(42/49): loss=0.45027433914412984\n",
      "Gradient Descent(43/49): loss=0.45024215205956936\n",
      "Gradient Descent(44/49): loss=0.45021371155165196\n",
      "Gradient Descent(45/49): loss=0.4501885815188562\n",
      "Gradient Descent(46/49): loss=0.45016637662187775\n",
      "Gradient Descent(47/49): loss=0.4501467563749081\n",
      "Gradient Descent(48/49): loss=0.450129419924685\n",
      "Gradient Descent(49/49): loss=0.4501141014372682\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4942470836479999\n",
      "Gradient Descent(2/49): loss=0.48916380675937277\n",
      "Gradient Descent(3/49): loss=0.48467222330058185\n",
      "Gradient Descent(4/49): loss=0.48070346015639404\n",
      "Gradient Descent(5/49): loss=0.4771966610421897\n",
      "Gradient Descent(6/49): loss=0.474098053344879\n",
      "Gradient Descent(7/49): loss=0.4713601235835349\n",
      "Gradient Descent(8/49): loss=0.4689408888464116\n",
      "Gradient Descent(9/49): loss=0.46680325303268927\n",
      "Gradient Descent(10/49): loss=0.4649144380276842\n",
      "Gradient Descent(11/49): loss=0.4632454810892617\n",
      "Gradient Descent(12/49): loss=0.4617707907384717\n",
      "Gradient Descent(13/49): loss=0.46046775434451354\n",
      "Gradient Descent(14/49): loss=0.4593163913868122\n",
      "Gradient Descent(15/49): loss=0.4582990470773873\n",
      "Gradient Descent(16/49): loss=0.4574001216455794\n",
      "Gradient Descent(17/49): loss=0.4566058311340338\n",
      "Gradient Descent(18/49): loss=0.4559039960380324\n",
      "Gradient Descent(19/49): loss=0.4552838545472054\n",
      "Gradient Descent(20/49): loss=0.4547358975259108\n",
      "Gradient Descent(21/49): loss=0.45425172270189457\n",
      "Gradient Descent(22/49): loss=0.4538239058273941\n",
      "Gradient Descent(23/49): loss=0.4534458868370854\n",
      "Gradient Descent(24/49): loss=0.45311186925724894\n",
      "Gradient Descent(25/49): loss=0.45281673132370515\n",
      "Gradient Descent(26/49): loss=0.4525559474456258\n",
      "Gradient Descent(27/49): loss=0.452325518810955\n",
      "Gradient Descent(28/49): loss=0.45212191206935987\n",
      "Gradient Descent(29/49): loss=0.4519420051524861\n",
      "Gradient Descent(30/49): loss=0.4517830394007369\n",
      "Gradient Descent(31/49): loss=0.45164257726249113\n",
      "Gradient Descent(32/49): loss=0.45151846491713715\n",
      "Gradient Descent(33/49): loss=0.45140879924878247\n",
      "Gradient Descent(34/49): loss=0.45131189866422394\n",
      "Gradient Descent(35/49): loss=0.45122627730770853\n",
      "Gradient Descent(36/49): loss=0.45115062227709124\n",
      "Gradient Descent(37/49): loss=0.4510837734920379\n",
      "Gradient Descent(38/49): loss=0.45102470590556437\n",
      "Gradient Descent(39/49): loss=0.45097251378615705\n",
      "Gradient Descent(40/49): loss=0.450926396829448\n",
      "Gradient Descent(41/49): loss=0.4508856478865004\n",
      "Gradient Descent(42/49): loss=0.45084964212051176\n",
      "Gradient Descent(43/49): loss=0.45081782742568416\n",
      "Gradient Descent(44/49): loss=0.45078971596133455\n",
      "Gradient Descent(45/49): loss=0.45076487667143533\n",
      "Gradient Descent(46/49): loss=0.45074292867488003\n",
      "Gradient Descent(47/49): loss=0.4507235354251242\n",
      "Gradient Descent(48/49): loss=0.45070639954963987\n",
      "Gradient Descent(49/49): loss=0.4506912582900616\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49420014520832006\n",
      "Gradient Descent(2/49): loss=0.48907539351439144\n",
      "Gradient Descent(3/49): loss=0.484547162917636\n",
      "Gradient Descent(4/49): loss=0.48054601836234356\n",
      "Gradient Descent(5/49): loss=0.4770106070332866\n",
      "Gradient Descent(6/49): loss=0.4738867175829322\n",
      "Gradient Descent(7/49): loss=0.4711264488645988\n",
      "Gradient Descent(8/49): loss=0.4686874754250799\n",
      "Gradient Descent(9/49): loss=0.46653239849392053\n",
      "Gradient Descent(10/49): loss=0.46462817251754807\n",
      "Gradient Descent(11/49): loss=0.4629455984448254\n",
      "Gradient Descent(12/49): loss=0.46145887599416774\n",
      "Gradient Descent(13/49): loss=0.46014520803676645\n",
      "Gradient Descent(14/49): loss=0.4589844510296069\n",
      "Gradient Descent(15/49): loss=0.4579588061380807\n",
      "Gradient Descent(16/49): loss=0.45705254631192793\n",
      "Gradient Descent(17/49): loss=0.4562517751295398\n",
      "Gradient Descent(18/49): loss=0.45554421371278137\n",
      "Gradient Descent(19/49): loss=0.4549190124449335\n",
      "Gradient Descent(20/49): loss=0.4543665846046632\n",
      "Gradient Descent(21/49): loss=0.45387845936500043\n",
      "Gradient Descent(22/49): loss=0.4534471519032342\n",
      "Gradient Descent(23/49): loss=0.45306604863001804\n",
      "Gradient Descent(24/49): loss=0.45272930577780374\n",
      "Gradient Descent(25/49): loss=0.45243175979358735\n",
      "Gradient Descent(26/49): loss=0.4521688481619339\n",
      "Gradient Descent(27/49): loss=0.45193653944420487\n",
      "Gradient Descent(28/49): loss=0.45173127146121944\n",
      "Gradient Descent(29/49): loss=0.4515498966714534\n",
      "Gradient Descent(30/49): loss=0.4513896339072163\n",
      "Gradient Descent(31/49): loss=0.4512480257287363\n",
      "Gradient Descent(32/49): loss=0.4511229007422314\n",
      "Gradient Descent(33/49): loss=0.45101234030415566\n",
      "Gradient Descent(34/49): loss=0.4509146491010721\n",
      "Gradient Descent(35/49): loss=0.45082832915402715\n",
      "Gradient Descent(36/49): loss=0.4507520568488184\n",
      "Gradient Descent(37/49): loss=0.4506846626399359\n",
      "Gradient Descent(38/49): loss=0.4506251131169672\n",
      "Gradient Descent(39/49): loss=0.4505724951584725\n",
      "Gradient Descent(40/49): loss=0.45052600193034614\n",
      "Gradient Descent(41/49): loss=0.4504849205139742\n",
      "Gradient Descent(42/49): loss=0.45044862097446736\n",
      "Gradient Descent(43/49): loss=0.4504165467013595\n",
      "Gradient Descent(44/49): loss=0.4503882058736411\n",
      "Gradient Descent(45/49): loss=0.45036316391826925\n",
      "Gradient Descent(46/49): loss=0.45034103684650284\n",
      "Gradient Descent(47/49): loss=0.45032148536588973\n",
      "Gradient Descent(48/49): loss=0.4503042096776202\n",
      "Gradient Descent(49/49): loss=0.45028894487946514\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49161496652369907\n",
      "Gradient Descent(2/49): loss=0.48467132030197413\n",
      "Gradient Descent(3/49): loss=0.47892128686576413\n",
      "Gradient Descent(4/49): loss=0.47415968417723836\n",
      "Gradient Descent(5/49): loss=0.4702166009908706\n",
      "Gradient Descent(6/49): loss=0.466951333804239\n",
      "Gradient Descent(7/49): loss=0.4642473660469896\n",
      "Gradient Descent(8/49): loss=0.4620082103472114\n",
      "Gradient Descent(9/49): loss=0.46015396551222476\n",
      "Gradient Descent(10/49): loss=0.4586184653643727\n",
      "Gradient Descent(11/49): loss=0.4573469176919359\n",
      "Gradient Descent(12/49): loss=0.4562939490643915\n",
      "Gradient Descent(13/49): loss=0.4554219857439219\n",
      "Gradient Descent(14/49): loss=0.4546999129182409\n",
      "Gradient Descent(15/49): loss=0.45410196441129447\n",
      "Gradient Descent(16/49): loss=0.45360680325269226\n",
      "Gradient Descent(17/49): loss=0.4531967602972535\n",
      "Gradient Descent(18/49): loss=0.4528572037258549\n",
      "Gradient Descent(19/49): loss=0.4525760169290796\n",
      "Gradient Descent(20/49): loss=0.45234316614267006\n",
      "Gradient Descent(21/49): loss=0.4521503424064443\n",
      "Gradient Descent(22/49): loss=0.45199066507047575\n",
      "Gradient Descent(23/49): loss=0.45185843626856015\n",
      "Gradient Descent(24/49): loss=0.45174893759769374\n",
      "Gradient Descent(25/49): loss=0.45165826174834955\n",
      "Gradient Descent(26/49): loss=0.4515831730775074\n",
      "Gradient Descent(27/49): loss=0.4515209921491831\n",
      "Gradient Descent(28/49): loss=0.4514695001224377\n",
      "Gradient Descent(29/49): loss=0.4514268595750898\n",
      "Gradient Descent(30/49): loss=0.4513915489378312\n",
      "Gradient Descent(31/49): loss=0.45136230819911727\n",
      "Gradient Descent(32/49): loss=0.451338093943388\n",
      "Gradient Descent(33/49): loss=0.45131804211821896\n",
      "Gradient Descent(34/49): loss=0.45130143720179616\n",
      "Gradient Descent(35/49): loss=0.45128768667050667\n",
      "Gradient Descent(36/49): loss=0.45127629985554596\n",
      "Gradient Descent(37/49): loss=0.4512668704340766\n",
      "Gradient Descent(38/49): loss=0.45125906193015786\n",
      "Gradient Descent(39/49): loss=0.4512525957080631\n",
      "Gradient Descent(40/49): loss=0.45124724102954644\n",
      "Gradient Descent(41/49): loss=0.4512428068202665\n",
      "Gradient Descent(42/49): loss=0.45123913485156186\n",
      "Gradient Descent(43/49): loss=0.4512360940942776\n",
      "Gradient Descent(44/49): loss=0.45123357604317027\n",
      "Gradient Descent(45/49): loss=0.4512314908450485\n",
      "Gradient Descent(46/49): loss=0.4512297640924839\n",
      "Gradient Descent(47/49): loss=0.4512283341686853\n",
      "Gradient Descent(48/49): loss=0.45122715004878755\n",
      "Gradient Descent(49/49): loss=0.4512261694791001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4914046248796671\n",
      "Gradient Descent(2/49): loss=0.4842867947425194\n",
      "Gradient Descent(3/49): loss=0.4783925196059476\n",
      "Gradient Descent(4/49): loss=0.4735114703653521\n",
      "Gradient Descent(5/49): loss=0.4694694734892153\n",
      "Gradient Descent(6/49): loss=0.4661222958760864\n",
      "Gradient Descent(7/49): loss=0.4633504980946542\n",
      "Gradient Descent(8/49): loss=0.46105517235185045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=0.4591544131042345\n",
      "Gradient Descent(10/49): loss=0.4575803943712837\n",
      "Gradient Descent(11/49): loss=0.45627694945852737\n",
      "Gradient Descent(12/49): loss=0.45519756672627376\n",
      "Gradient Descent(13/49): loss=0.4543037298856945\n",
      "Gradient Descent(14/49): loss=0.4535635435980106\n",
      "Gradient Descent(15/49): loss=0.4529505953331798\n",
      "Gradient Descent(16/49): loss=0.4524430128750734\n",
      "Gradient Descent(17/49): loss=0.4520226838415157\n",
      "Gradient Descent(18/49): loss=0.45167460936882625\n",
      "Gradient Descent(19/49): loss=0.4513863688979921\n",
      "Gradient Descent(20/49): loss=0.45114767696409447\n",
      "Gradient Descent(21/49): loss=0.4509500161736339\n",
      "Gradient Descent(22/49): loss=0.45078633327305323\n",
      "Gradient Descent(23/49): loss=0.4506507874630826\n",
      "Gradient Descent(24/49): loss=0.450538541977846\n",
      "Gradient Descent(25/49): loss=0.45044559149152164\n",
      "Gradient Descent(26/49): loss=0.45036861919379617\n",
      "Gradient Descent(27/49): loss=0.4503048784340498\n",
      "Gradient Descent(28/49): loss=0.45025209471090394\n",
      "Gradient Descent(29/49): loss=0.4502083845097666\n",
      "Gradient Descent(30/49): loss=0.450172188092205\n",
      "Gradient Descent(31/49): loss=0.45014221383882214\n",
      "Gradient Descent(32/49): loss=0.45011739215959584\n",
      "Gradient Descent(33/49): loss=0.4500968373270283\n",
      "Gradient Descent(34/49): loss=0.4500798158701795\n",
      "Gradient Descent(35/49): loss=0.4500657204017629\n",
      "Gradient Descent(36/49): loss=0.4500540479443671\n",
      "Gradient Descent(37/49): loss=0.45004438198239755\n",
      "Gradient Descent(38/49): loss=0.45003637759929055\n",
      "Gradient Descent(39/49): loss=0.45002974916963984\n",
      "Gradient Descent(40/49): loss=0.4500242601670459\n",
      "Gradient Descent(41/49): loss=0.45001971472399793\n",
      "Gradient Descent(42/49): loss=0.4500159506426099\n",
      "Gradient Descent(43/49): loss=0.4500128336068125\n",
      "Gradient Descent(44/49): loss=0.45001025238946857\n",
      "Gradient Descent(45/49): loss=0.45000811488338605\n",
      "Gradient Descent(46/49): loss=0.4500063448145993\n",
      "Gradient Descent(47/49): loss=0.45000487902063685\n",
      "Gradient Descent(48/49): loss=0.4500036651966564\n",
      "Gradient Descent(49/49): loss=0.45000266002901845\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49150406940799984\n",
      "Gradient Descent(2/49): loss=0.484468589284765\n",
      "Gradient Descent(3/49): loss=0.4786425081947139\n",
      "Gradient Descent(4/49): loss=0.4738179304440424\n",
      "Gradient Descent(5/49): loss=0.4698226976087115\n",
      "Gradient Descent(6/49): loss=0.46651424529777413\n",
      "Gradient Descent(7/49): loss=0.46377451593908653\n",
      "Gradient Descent(8/49): loss=0.46150574605715766\n",
      "Gradient Descent(9/49): loss=0.45962697771793226\n",
      "Gradient Descent(10/49): loss=0.4580711696562198\n",
      "Gradient Descent(11/49): loss=0.4567828050003153\n",
      "Gradient Descent(12/49): loss=0.4557159102287613\n",
      "Gradient Descent(13/49): loss=0.45483241466843716\n",
      "Gradient Descent(14/49): loss=0.45410079199493275\n",
      "Gradient Descent(15/49): loss=0.453494935259004\n",
      "Gradient Descent(16/49): loss=0.4529932252959812\n",
      "Gradient Descent(17/49): loss=0.4525777592756022\n",
      "Gradient Descent(18/49): loss=0.452233711864126\n",
      "Gradient Descent(19/49): loss=0.4519488062026827\n",
      "Gradient Descent(20/49): loss=0.4517128758244416\n",
      "Gradient Descent(21/49): loss=0.45151750187822004\n",
      "Gradient Descent(22/49): loss=0.4513557127133539\n",
      "Gradient Descent(23/49): loss=0.45122173510592867\n",
      "Gradient Descent(24/49): loss=0.45111078824921946\n",
      "Gradient Descent(25/49): loss=0.45101891315717857\n",
      "Gradient Descent(26/49): loss=0.4509428313934594\n",
      "Gradient Descent(27/49): loss=0.45087982808492405\n",
      "Gradient Descent(28/49): loss=0.4508276550451256\n",
      "Gradient Descent(29/49): loss=0.4507844505508682\n",
      "Gradient Descent(30/49): loss=0.45074867290917414\n",
      "Gradient Descent(31/49): loss=0.45071904544408703\n",
      "Gradient Descent(32/49): loss=0.4506945109402483\n",
      "Gradient Descent(33/49): loss=0.4506741939176198\n",
      "Gradient Descent(34/49): loss=0.4506573693911811\n",
      "Gradient Descent(35/49): loss=0.4506434370008369\n",
      "Gradient Descent(36/49): loss=0.45063189958839306\n",
      "Gradient Descent(37/49): loss=0.4506223454571484\n",
      "Gradient Descent(38/49): loss=0.45061443368106446\n",
      "Gradient Descent(39/49): loss=0.45060788193928963\n",
      "Gradient Descent(40/49): loss=0.4506024564419257\n",
      "Gradient Descent(41/49): loss=0.4505979635875588\n",
      "Gradient Descent(42/49): loss=0.4505942430548574\n",
      "Gradient Descent(43/49): loss=0.4505911620817273\n",
      "Gradient Descent(44/49): loss=0.4505886107278782\n",
      "Gradient Descent(45/49): loss=0.45058649795175615\n",
      "Gradient Descent(46/49): loss=0.45058474836184914\n",
      "Gradient Descent(47/49): loss=0.45058329952644743\n",
      "Gradient Descent(48/49): loss=0.45058209974585095\n",
      "Gradient Descent(49/49): loss=0.45058110620753933\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49143475052672\n",
      "Gradient Descent(2/49): loss=0.48434186743789664\n",
      "Gradient Descent(3/49): loss=0.4784682509520421\n",
      "Gradient Descent(4/49): loss=0.4736043091401062\n",
      "Gradient Descent(5/49): loss=0.4695764789256421\n",
      "Gradient Descent(6/49): loss=0.46624103272504414\n",
      "Gradient Descent(7/49): loss=0.463478949726329\n",
      "Gradient Descent(8/49): loss=0.4611916687950929\n",
      "Gradient Descent(9/49): loss=0.4592975714559366\n",
      "Gradient Descent(10/49): loss=0.45772906944938097\n",
      "Gradient Descent(11/49): loss=0.4564301929377522\n",
      "Gradient Descent(12/49): loss=0.455354593298473\n",
      "Gradient Descent(13/49): loss=0.4544638892371852\n",
      "Gradient Descent(14/49): loss=0.4537262972040333\n",
      "Gradient Descent(15/49): loss=0.45311549724137984\n",
      "Gradient Descent(16/49): loss=0.45260969379230653\n",
      "Gradient Descent(17/49): loss=0.4521908379561292\n",
      "Gradient Descent(18/49): loss=0.4518439834381906\n",
      "Gradient Descent(19/49): loss=0.45155675321188576\n",
      "Gradient Descent(20/49): loss=0.4513188978614825\n",
      "Gradient Descent(21/49): loss=0.45112192984581373\n",
      "Gradient Descent(22/49): loss=0.45095882063203835\n",
      "Gradient Descent(23/49): loss=0.4508237498921109\n",
      "Gradient Descent(24/49): loss=0.45071189781237686\n",
      "Gradient Descent(25/49): loss=0.4506192731051493\n",
      "Gradient Descent(26/49): loss=0.4505425705850942\n",
      "Gradient Descent(27/49): loss=0.45047905322823667\n",
      "Gradient Descent(28/49): loss=0.45042645450502256\n",
      "Gradient Descent(29/49): loss=0.4503828975023293\n",
      "Gradient Descent(30/49): loss=0.4503468279483988\n",
      "Gradient Descent(31/49): loss=0.450316958750789\n",
      "Gradient Descent(32/49): loss=0.4502922240682485\n",
      "Gradient Descent(33/49): loss=0.4502717412776366\n",
      "Gradient Descent(34/49): loss=0.4502547794787307\n",
      "Gradient Descent(35/49): loss=0.4502407334130569\n",
      "Gradient Descent(36/49): loss=0.45022910186607257\n",
      "Gradient Descent(37/49): loss=0.4502194697820148\n",
      "Gradient Descent(38/49): loss=0.45021149345320627\n",
      "Gradient Descent(39/49): loss=0.45020488825532\n",
      "Gradient Descent(40/49): loss=0.4501994184909506\n",
      "Gradient Descent(41/49): loss=0.450194888979076\n",
      "Gradient Descent(42/49): loss=0.450191138090293\n",
      "Gradient Descent(43/49): loss=0.4501880319792914\n",
      "Gradient Descent(44/49): loss=0.4501854598087713\n",
      "Gradient Descent(45/49): loss=0.4501833297943636\n",
      "Gradient Descent(46/49): loss=0.4501815659294325\n",
      "Gradient Descent(47/49): loss=0.4501801052728831\n",
      "Gradient Descent(48/49): loss=0.4501788957031945\n",
      "Gradient Descent(49/49): loss=0.45017789405853537\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48899555816024753\n",
      "Gradient Descent(2/49): loss=0.480473718399543\n",
      "Gradient Descent(3/49): loss=0.4738744056888534\n",
      "Gradient Descent(4/49): loss=0.46876389792569556\n",
      "Gradient Descent(5/49): loss=0.46480632071390593\n",
      "Gradient Descent(6/49): loss=0.4617415729210962\n",
      "Gradient Descent(7/49): loss=0.45936823223034434\n",
      "Gradient Descent(8/49): loss=0.45753031719942594\n",
      "Gradient Descent(9/49): loss=0.45610703579948325\n",
      "Gradient Descent(10/49): loss=0.4550048466833673\n",
      "Gradient Descent(11/49): loss=0.45415131143184706\n",
      "Gradient Descent(12/49): loss=0.45349033373306996\n",
      "Gradient Descent(13/49): loss=0.45297847260313684\n",
      "Gradient Descent(14/49): loss=0.4525820873441166\n",
      "Gradient Descent(15/49): loss=0.45227512659953145\n",
      "Gradient Descent(16/49): loss=0.45203741619892457\n",
      "Gradient Descent(17/49): loss=0.4518533332646946\n",
      "Gradient Descent(18/49): loss=0.451710779440427\n",
      "Gradient Descent(19/49): loss=0.451600385758914\n",
      "Gradient Descent(20/49): loss=0.45151489689195046\n",
      "Gradient Descent(21/49): loss=0.45144869431337387\n",
      "Gradient Descent(22/49): loss=0.4513974270365244\n",
      "Gradient Descent(23/49): loss=0.4513577256573319\n",
      "Gradient Descent(24/49): loss=0.45132698090928547\n",
      "Gradient Descent(25/49): loss=0.4513031721763981\n",
      "Gradient Descent(26/49): loss=0.45128473469364994\n",
      "Gradient Descent(27/49): loss=0.4512704567070102\n",
      "Gradient Descent(28/49): loss=0.45125939983415614\n",
      "Gradient Descent(29/49): loss=0.45125083739181804\n",
      "Gradient Descent(30/49): loss=0.45124420663647147\n",
      "Gradient Descent(31/49): loss=0.4512390717795307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=0.45123509534631606\n",
      "Gradient Descent(33/49): loss=0.45123201599643464\n",
      "Gradient Descent(34/49): loss=0.4512296313478865\n",
      "Gradient Descent(35/49): loss=0.4512277846760506\n",
      "Gradient Descent(36/49): loss=0.45122635461338123\n",
      "Gradient Descent(37/49): loss=0.45122524717284995\n",
      "Gradient Descent(38/49): loss=0.4512243895709025\n",
      "Gradient Descent(39/49): loss=0.45122372544395434\n",
      "Gradient Descent(40/49): loss=0.4512232111440456\n",
      "Gradient Descent(41/49): loss=0.4512228128701963\n",
      "Gradient Descent(42/49): loss=0.45122250444692763\n",
      "Gradient Descent(43/49): loss=0.4512222656039483\n",
      "Gradient Descent(44/49): loss=0.451222080643945\n",
      "Gradient Descent(45/49): loss=0.45122193741091843\n",
      "Gradient Descent(46/49): loss=0.45122182649126275\n",
      "Gradient Descent(47/49): loss=0.45122174059508136\n",
      "Gradient Descent(48/49): loss=0.45122167407707825\n",
      "Gradient Descent(49/49): loss=0.45122162256553705\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48871950769547934\n",
      "Gradient Descent(2/49): loss=0.47998389445485873\n",
      "Gradient Descent(3/49): loss=0.473219035561322\n",
      "Gradient Descent(4/49): loss=0.46798032883416707\n",
      "Gradient Descent(5/49): loss=0.46392347434465875\n",
      "Gradient Descent(6/49): loss=0.4607818462279832\n",
      "Gradient Descent(7/49): loss=0.4583489694144295\n",
      "Gradient Descent(8/49): loss=0.45646494961001377\n",
      "Gradient Descent(9/49): loss=0.4550059646734741\n",
      "Gradient Descent(10/49): loss=0.4538761267386178\n",
      "Gradient Descent(11/49): loss=0.4530011802418652\n",
      "Gradient Descent(12/49): loss=0.4523236216747798\n",
      "Gradient Descent(13/49): loss=0.45179892032042906\n",
      "Gradient Descent(14/49): loss=0.4513925915916196\n",
      "Gradient Descent(15/49): loss=0.4510779306240297\n",
      "Gradient Descent(16/49): loss=0.45083425717072795\n",
      "Gradient Descent(17/49): loss=0.4506455564484913\n",
      "Gradient Descent(18/49): loss=0.450499426609191\n",
      "Gradient Descent(19/49): loss=0.45038626366163703\n",
      "Gradient Descent(20/49): loss=0.45029863027505124\n",
      "Gradient Descent(21/49): loss=0.450230766980479\n",
      "Gradient Descent(22/49): loss=0.4501782136451625\n",
      "Gradient Descent(23/49): loss=0.4501375163422933\n",
      "Gradient Descent(24/49): loss=0.45010600035095155\n",
      "Gradient Descent(25/49): loss=0.4500815943672561\n",
      "Gradient Descent(26/49): loss=0.4500626943734827\n",
      "Gradient Descent(27/49): loss=0.4500480582183045\n",
      "Gradient Descent(28/49): loss=0.4500367239797345\n",
      "Gradient Descent(29/49): loss=0.45002794674538565\n",
      "Gradient Descent(30/49): loss=0.4500211496551065\n",
      "Gradient Descent(31/49): loss=0.4500158859883937\n",
      "Gradient Descent(32/49): loss=0.4500118098048918\n",
      "Gradient Descent(33/49): loss=0.4500086532083874\n",
      "Gradient Descent(34/49): loss=0.45000620874005476\n",
      "Gradient Descent(35/49): loss=0.4500043157437781\n",
      "Gradient Descent(36/49): loss=0.450002849807461\n",
      "Gradient Descent(37/49): loss=0.4500017145863773\n",
      "Gradient Descent(38/49): loss=0.45000083547117004\n",
      "Gradient Descent(39/49): loss=0.45000015468435345\n",
      "Gradient Descent(40/49): loss=0.4499996274830429\n",
      "Gradient Descent(41/49): loss=0.44999921921834785\n",
      "Gradient Descent(42/49): loss=0.4499989030581679\n",
      "Gradient Descent(43/49): loss=0.44999865822372476\n",
      "Gradient Descent(44/49): loss=0.449998468623932\n",
      "Gradient Descent(45/49): loss=0.44999832179785243\n",
      "Gradient Descent(46/49): loss=0.44999820809573626\n",
      "Gradient Descent(47/49): loss=0.4499981200448177\n",
      "Gradient Descent(48/49): loss=0.4499980518581866\n",
      "Gradient Descent(49/49): loss=0.44999799905445875\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4888500177920001\n",
      "Gradient Descent(2/49): loss=0.48021547157012495\n",
      "Gradient Descent(3/49): loss=0.4735288789759046\n",
      "Gradient Descent(4/49): loss=0.4683507816709404\n",
      "Gradient Descent(5/49): loss=0.4643408631179761\n",
      "Gradient Descent(6/49): loss=0.4612355821905605\n",
      "Gradient Descent(7/49): loss=0.45883085264037027\n",
      "Gradient Descent(8/49): loss=0.4569686300767027\n",
      "Gradient Descent(9/49): loss=0.4555265249233987\n",
      "Gradient Descent(10/49): loss=0.45440975869267985\n",
      "Gradient Descent(11/49): loss=0.4535449349236114\n",
      "Gradient Descent(12/49): loss=0.4528752153968446\n",
      "Gradient Descent(13/49): loss=0.4523565845953164\n",
      "Gradient Descent(14/49): loss=0.451954956902613\n",
      "Gradient Descent(15/49): loss=0.4516439364173836\n",
      "Gradient Descent(16/49): loss=0.45140308215362185\n",
      "Gradient Descent(17/49): loss=0.45121656461176485\n",
      "Gradient Descent(18/49): loss=0.45107212542735053\n",
      "Gradient Descent(19/49): loss=0.45096027172294034\n",
      "Gradient Descent(20/49): loss=0.4508736522142449\n",
      "Gradient Descent(21/49): loss=0.45080657406671126\n",
      "Gradient Descent(22/49): loss=0.4507546287492612\n",
      "Gradient Descent(23/49): loss=0.4507144022954279\n",
      "Gradient Descent(24/49): loss=0.4506832509295793\n",
      "Gradient Descent(25/49): loss=0.45065912731186614\n",
      "Gradient Descent(26/49): loss=0.4506404459823093\n",
      "Gradient Descent(27/49): loss=0.45062597916070013\n",
      "Gradient Descent(28/49): loss=0.4506147760540464\n",
      "Gradient Descent(29/49): loss=0.4506061003682535\n",
      "Gradient Descent(30/49): loss=0.4505993819171757\n",
      "Gradient Descent(31/49): loss=0.45059417914866084\n",
      "Gradient Descent(32/49): loss=0.45059015012472287\n",
      "Gradient Descent(33/49): loss=0.4505870300485852\n",
      "Gradient Descent(34/49): loss=0.4505846138616246\n",
      "Gradient Descent(35/49): loss=0.4505827427664421\n",
      "Gradient Descent(36/49): loss=0.45058129379033274\n",
      "Gradient Descent(37/49): loss=0.4505801717032336\n",
      "Gradient Descent(38/49): loss=0.4505793027589841\n",
      "Gradient Descent(39/49): loss=0.4505786298485573\n",
      "Gradient Descent(40/49): loss=0.4505781087467227\n",
      "Gradient Descent(41/49): loss=0.4505777052054621\n",
      "Gradient Descent(42/49): loss=0.45057739270311004\n",
      "Gradient Descent(43/49): loss=0.45057715070128845\n",
      "Gradient Descent(44/49): loss=0.4505769632950775\n",
      "Gradient Descent(45/49): loss=0.45057681816770834\n",
      "Gradient Descent(46/49): loss=0.4505767057810733\n",
      "Gradient Descent(47/49): loss=0.4505766187488633\n",
      "Gradient Descent(48/49): loss=0.45057655135111946\n",
      "Gradient Descent(49/49): loss=0.4505764991583069\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48875904432128014\n",
      "Gradient Descent(2/49): loss=0.4800540482436791\n",
      "Gradient Descent(3/49): loss=0.473312899281185\n",
      "Gradient Descent(4/49): loss=0.4680925535246298\n",
      "Gradient Descent(5/49): loss=0.4640499177707532\n",
      "Gradient Descent(6/49): loss=0.46091930064295156\n",
      "Gradient Descent(7/49): loss=0.4584949507391816\n",
      "Gradient Descent(8/49): loss=0.45661753417370243\n",
      "Gradient Descent(9/49): loss=0.45516366278539505\n",
      "Gradient Descent(10/49): loss=0.4540377847822899\n",
      "Gradient Descent(11/49): loss=0.45316590485668556\n",
      "Gradient Descent(12/49): loss=0.45249072104229693\n",
      "Gradient Descent(13/49): loss=0.45196785869643485\n",
      "Gradient Descent(14/49): loss=0.45156295409579916\n",
      "Gradient Descent(15/49): loss=0.45124939597306696\n",
      "Gradient Descent(16/49): loss=0.45100657656282295\n",
      "Gradient Descent(17/49): loss=0.45081853721153015\n",
      "Gradient Descent(18/49): loss=0.45067291953788885\n",
      "Gradient Descent(19/49): loss=0.4505601532114212\n",
      "Gradient Descent(20/49): loss=0.45047282696820445\n",
      "Gradient Descent(21/49): loss=0.4504052015254576\n",
      "Gradient Descent(22/49): loss=0.4503528323825944\n",
      "Gradient Descent(23/49): loss=0.45031227771836124\n",
      "Gradient Descent(24/49): loss=0.4502808721863788\n",
      "Gradient Descent(25/49): loss=0.4502565517424117\n",
      "Gradient Descent(26/49): loss=0.45023771799060375\n",
      "Gradient Descent(27/49): loss=0.45022313313320367\n",
      "Gradient Descent(28/49): loss=0.4502118386196327\n",
      "Gradient Descent(29/49): loss=0.45020309214832355\n",
      "Gradient Descent(30/49): loss=0.45019631888094186\n",
      "Gradient Descent(31/49): loss=0.45019107366268135\n",
      "Gradient Descent(32/49): loss=0.4501870117656604\n",
      "Gradient Descent(33/49): loss=0.4501838662326076\n",
      "Gradient Descent(34/49): loss=0.4501814303318111\n",
      "Gradient Descent(35/49): loss=0.4501795439702345\n",
      "Gradient Descent(36/49): loss=0.45017808317182967\n",
      "Gradient Descent(37/49): loss=0.4501769519295449\n",
      "Gradient Descent(38/49): loss=0.4501760758955196\n",
      "Gradient Descent(39/49): loss=0.45017539749477026\n",
      "Gradient Descent(40/49): loss=0.45017487214123014\n",
      "Gradient Descent(41/49): loss=0.45017446530744853\n",
      "Gradient Descent(42/49): loss=0.45017415025536833\n",
      "Gradient Descent(43/49): loss=0.45017390627903714\n",
      "Gradient Descent(44/49): loss=0.4501737173437664\n",
      "Gradient Descent(45/49): loss=0.45017357103229266\n",
      "Gradient Descent(46/49): loss=0.45017345772868766\n",
      "Gradient Descent(47/49): loss=0.45017336998637564\n",
      "Gradient Descent(48/49): loss=0.4501733020387293\n",
      "Gradient Descent(49/49): loss=0.45017324942007186\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48646395119445346\n",
      "Gradient Descent(2/49): loss=0.4766841559324458\n",
      "Gradient Descent(3/49): loss=0.4696182538556454\n",
      "Gradient Descent(4/49): loss=0.46451313960515694\n",
      "Gradient Descent(5/49): loss=0.4608246945591794\n",
      "Gradient Descent(6/49): loss=0.4581597930134605\n",
      "Gradient Descent(7/49): loss=0.45623440164667867\n",
      "Gradient Descent(8/49): loss=0.45484330638417875\n",
      "Gradient Descent(9/49): loss=0.4538382400570222\n",
      "Gradient Descent(10/49): loss=0.4531120796356521\n",
      "Gradient Descent(11/49): loss=0.4525874287312118\n",
      "Gradient Descent(12/49): loss=0.45220836845275403\n",
      "Gradient Descent(13/49): loss=0.45193449740156816\n",
      "Gradient Descent(14/49): loss=0.4517366255670862\n",
      "Gradient Descent(15/49): loss=0.4515936631666731\n",
      "Gradient Descent(16/49): loss=0.4514903728323748\n",
      "Gradient Descent(17/49): loss=0.451415745565844\n",
      "Gradient Descent(18/49): loss=0.4513618273657757\n",
      "Gradient Descent(19/49): loss=0.4513228714662264\n",
      "Gradient Descent(20/49): loss=0.4512947258288019\n",
      "Gradient Descent(21/49): loss=0.4512743906057627\n",
      "Gradient Descent(22/49): loss=0.451259698407117\n",
      "Gradient Descent(23/49): loss=0.45124908329359537\n",
      "Gradient Descent(24/49): loss=0.45124141387407607\n",
      "Gradient Descent(25/49): loss=0.4512358727184732\n",
      "Gradient Descent(26/49): loss=0.4512318692335503\n",
      "Gradient Descent(27/49): loss=0.45122897671569334\n",
      "Gradient Descent(28/49): loss=0.45122688687154183\n",
      "Gradient Descent(29/49): loss=0.45122537695914233\n",
      "Gradient Descent(30/49): loss=0.45122428604743375\n",
      "Gradient Descent(31/49): loss=0.45122349786372423\n",
      "Gradient Descent(32/49): loss=0.4512229284009939\n",
      "Gradient Descent(33/49): loss=0.4512225169641715\n",
      "Gradient Descent(34/49): loss=0.45122221970106724\n",
      "Gradient Descent(35/49): loss=0.45122200492847464\n",
      "Gradient Descent(36/49): loss=0.45122184975527607\n",
      "Gradient Descent(37/49): loss=0.45122173764264034\n",
      "Gradient Descent(38/49): loss=0.45122165664126085\n",
      "Gradient Descent(39/49): loss=0.45122159811776436\n",
      "Gradient Descent(40/49): loss=0.45122155583453816\n",
      "Gradient Descent(41/49): loss=0.45122152528490717\n",
      "Gradient Descent(42/49): loss=0.45122150321279864\n",
      "Gradient Descent(43/49): loss=0.4512214872657003\n",
      "Gradient Descent(44/49): loss=0.45122147574392185\n",
      "Gradient Descent(45/49): loss=0.451221467419437\n",
      "Gradient Descent(46/49): loss=0.45122146140499647\n",
      "Gradient Descent(47/49): loss=0.45122145705956324\n",
      "Gradient Descent(48/49): loss=0.45122145391998775\n",
      "Gradient Descent(49/49): loss=0.45122145165164457\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4861243944392534\n",
      "Gradient Descent(2/49): loss=0.47609926942161385\n",
      "Gradient Descent(3/49): loss=0.46885611659636933\n",
      "Gradient Descent(4/49): loss=0.46362293868013\n",
      "Gradient Descent(5/49): loss=0.4598419676356474\n",
      "Gradient Descent(6/49): loss=0.4571102160560086\n",
      "Gradient Descent(7/49): loss=0.45513652553971956\n",
      "Gradient Descent(8/49): loss=0.4537105341417008\n",
      "Gradient Descent(9/49): loss=0.4526802553566321\n",
      "Gradient Descent(10/49): loss=0.45193587893442\n",
      "Gradient Descent(11/49): loss=0.4513980669693716\n",
      "Gradient Descent(12/49): loss=0.4510094978246245\n",
      "Gradient Descent(13/49): loss=0.4507287566175446\n",
      "Gradient Descent(14/49): loss=0.4505259210954292\n",
      "Gradient Descent(15/49): loss=0.450379372430701\n",
      "Gradient Descent(16/49): loss=0.4502734910204348\n",
      "Gradient Descent(17/49): loss=0.45019699170151734\n",
      "Gradient Descent(18/49): loss=0.4501417209435999\n",
      "Gradient Descent(19/49): loss=0.4501017878210042\n",
      "Gradient Descent(20/49): loss=0.45007293613992894\n",
      "Gradient Descent(21/49): loss=0.45005209080035186\n",
      "Gradient Descent(22/49): loss=0.45003703004250756\n",
      "Gradient Descent(23/49): loss=0.45002614864496515\n",
      "Gradient Descent(24/49): loss=0.4500182868352406\n",
      "Gradient Descent(25/49): loss=0.45001260667771464\n",
      "Gradient Descent(26/49): loss=0.45000850276390225\n",
      "Gradient Descent(27/49): loss=0.4500055376861726\n",
      "Gradient Descent(28/49): loss=0.45000339541751316\n",
      "Gradient Descent(29/49): loss=0.4500018476284066\n",
      "Gradient Descent(30/49): loss=0.45000072935077695\n",
      "Gradient Descent(31/49): loss=0.4499999213951897\n",
      "Gradient Descent(32/49): loss=0.44999933764727795\n",
      "Gradient Descent(33/49): loss=0.4499989158894117\n",
      "Gradient Descent(34/49): loss=0.449998611169353\n",
      "Gradient Descent(35/49): loss=0.44999839100911093\n",
      "Gradient Descent(36/49): loss=0.4499982319433361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=0.4499981170183137\n",
      "Gradient Descent(38/49): loss=0.44999803398498484\n",
      "Gradient Descent(39/49): loss=0.4499979739934049\n",
      "Gradient Descent(40/49): loss=0.44999793064948856\n",
      "Gradient Descent(41/49): loss=0.4499978993335087\n",
      "Gradient Descent(42/49): loss=0.4499978767077135\n",
      "Gradient Descent(43/49): loss=0.4499978603605764\n",
      "Gradient Descent(44/49): loss=0.4499978485497697\n",
      "Gradient Descent(45/49): loss=0.4499978400164618\n",
      "Gradient Descent(46/49): loss=0.4499978338511471\n",
      "Gradient Descent(47/49): loss=0.44999782939670724\n",
      "Gradient Descent(48/49): loss=0.44999782617837414\n",
      "Gradient Descent(49/49): loss=0.4499978238531286\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4862849288000001\n",
      "Gradient Descent(2/49): loss=0.4763757898580002\n",
      "Gradient Descent(3/49): loss=0.4692164369724049\n",
      "Gradient Descent(4/49): loss=0.4640438045125626\n",
      "Gradient Descent(5/49): loss=0.46030657756032634\n",
      "Gradient Descent(6/49): loss=0.45760643108733573\n",
      "Gradient Descent(7/49): loss=0.4556555752606001\n",
      "Gradient Descent(8/49): loss=0.4542460819257835\n",
      "Gradient Descent(9/49): loss=0.45322772299137853\n",
      "Gradient Descent(10/49): loss=0.452491958661271\n",
      "Gradient Descent(11/49): loss=0.4519603689327683\n",
      "Gradient Descent(12/49): loss=0.4515762953539252\n",
      "Gradient Descent(13/49): loss=0.45129880219321067\n",
      "Gradient Descent(14/49): loss=0.45109831338459494\n",
      "Gradient Descent(15/49): loss=0.4509534602203699\n",
      "Gradient Descent(16/49): loss=0.4508488038092171\n",
      "Gradient Descent(17/49): loss=0.4507731895521593\n",
      "Gradient Descent(18/49): loss=0.4507185582514352\n",
      "Gradient Descent(19/49): loss=0.45067908713666194\n",
      "Gradient Descent(20/49): loss=0.4506505692562382\n",
      "Gradient Descent(21/49): loss=0.45062996508763203\n",
      "Gradient Descent(22/49): loss=0.45061507857581445\n",
      "Gradient Descent(23/49): loss=0.45060432307102577\n",
      "Gradient Descent(24/49): loss=0.4505965522188162\n",
      "Gradient Descent(25/49): loss=0.4505909377780947\n",
      "Gradient Descent(26/49): loss=0.45058688134467345\n",
      "Gradient Descent(27/49): loss=0.4505839505715266\n",
      "Gradient Descent(28/49): loss=0.4505818330879279\n",
      "Gradient Descent(29/49): loss=0.45058030320602804\n",
      "Gradient Descent(30/49): loss=0.4505791978663552\n",
      "Gradient Descent(31/49): loss=0.45057839925844145\n",
      "Gradient Descent(32/49): loss=0.45057782226422427\n",
      "Gradient Descent(33/49): loss=0.4505774053859019\n",
      "Gradient Descent(34/49): loss=0.45057710419131414\n",
      "Gradient Descent(35/49): loss=0.45057688657822453\n",
      "Gradient Descent(36/49): loss=0.4505767293527672\n",
      "Gradient Descent(37/49): loss=0.45057661575737434\n",
      "Gradient Descent(38/49): loss=0.4505765336847031\n",
      "Gradient Descent(39/49): loss=0.45057647438719767\n",
      "Gradient Descent(40/49): loss=0.45057643154475047\n",
      "Gradient Descent(41/49): loss=0.4505764005910821\n",
      "Gradient Descent(42/49): loss=0.45057637822705704\n",
      "Gradient Descent(43/49): loss=0.4505763620690486\n",
      "Gradient Descent(44/49): loss=0.4505763503948875\n",
      "Gradient Descent(45/49): loss=0.4505763419603062\n",
      "Gradient Descent(46/49): loss=0.4505763358663213\n",
      "Gradient Descent(47/49): loss=0.450576331463417\n",
      "Gradient Descent(48/49): loss=0.4505763282823187\n",
      "Gradient Descent(49/49): loss=0.4505763259839754\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4861730265920002\n",
      "Gradient Descent(2/49): loss=0.4761830383047197\n",
      "Gradient Descent(3/49): loss=0.4689652717671601\n",
      "Gradient Descent(4/49): loss=0.46375043544377303\n",
      "Gradient Descent(5/49): loss=0.4599827162001259\n",
      "Gradient Descent(6/49): loss=0.4572605390465909\n",
      "Gradient Descent(7/49): loss=0.45529376605316196\n",
      "Gradient Descent(8/49): loss=0.4538727725654097\n",
      "Gradient Descent(9/49): loss=0.45284610477050846\n",
      "Gradient Descent(10/49): loss=0.4521043372886922\n",
      "Gradient Descent(11/49): loss=0.45156841028308015\n",
      "Gradient Descent(12/49): loss=0.4511812030215253\n",
      "Gradient Descent(13/49): loss=0.4509014457750522\n",
      "Gradient Descent(14/49): loss=0.450699321164475\n",
      "Gradient Descent(15/49): loss=0.4505532861333333\n",
      "Gradient Descent(16/49): loss=0.4504477758233334\n",
      "Gradient Descent(17/49): loss=0.4503715446243582\n",
      "Gradient Descent(18/49): loss=0.4503164675830987\n",
      "Gradient Descent(19/49): loss=0.45027667442078906\n",
      "Gradient Descent(20/49): loss=0.4502479238610201\n",
      "Gradient Descent(21/49): loss=0.45022715158158694\n",
      "Gradient Descent(22/49): loss=0.4502121436096965\n",
      "Gradient Descent(23/49): loss=0.4502013003500057\n",
      "Gradient Descent(24/49): loss=0.4501934660948792\n",
      "Gradient Descent(25/49): loss=0.45018780584555035\n",
      "Gradient Descent(26/49): loss=0.4501837163154099\n",
      "Gradient Descent(27/49): loss=0.4501807616298836\n",
      "Gradient Descent(28/49): loss=0.4501786268695911\n",
      "Gradient Descent(29/49): loss=0.4501770845052795\n",
      "Gradient Descent(30/49): loss=0.4501759701470644\n",
      "Gradient Descent(31/49): loss=0.45017516502325405\n",
      "Gradient Descent(32/49): loss=0.4501745833213011\n",
      "Gradient Descent(33/49): loss=0.45017416304164\n",
      "Gradient Descent(34/49): loss=0.4501738593895849\n",
      "Gradient Descent(35/49): loss=0.4501736400009751\n",
      "Gradient Descent(36/49): loss=0.45017348149270464\n",
      "Gradient Descent(37/49): loss=0.45017336697047905\n",
      "Gradient Descent(38/49): loss=0.45017328422817104\n",
      "Gradient Descent(39/49): loss=0.45017322444685376\n",
      "Gradient Descent(40/49): loss=0.4501731812548518\n",
      "Gradient Descent(41/49): loss=0.45017315004863045\n",
      "Gradient Descent(42/49): loss=0.4501731275021354\n",
      "Gradient Descent(43/49): loss=0.45017311121229286\n",
      "Gradient Descent(44/49): loss=0.45017309944288153\n",
      "Gradient Descent(45/49): loss=0.4501730909394819\n",
      "Gradient Descent(46/49): loss=0.45017308479577567\n",
      "Gradient Descent(47/49): loss=0.450173080356948\n",
      "Gradient Descent(48/49): loss=0.4501730771498948\n",
      "Gradient Descent(49/49): loss=0.45017307483279906\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4840201456263168\n",
      "Gradient Descent(2/49): loss=0.47327529154545217\n",
      "Gradient Descent(3/49): loss=0.4660504516614786\n",
      "Gradient Descent(4/49): loss=0.4611924693234951\n",
      "Gradient Descent(5/49): loss=0.45792596199943475\n",
      "Gradient Descent(6/49): loss=0.4557295624747368\n",
      "Gradient Descent(7/49): loss=0.45425270343432955\n",
      "Gradient Descent(8/49): loss=0.45325966341556\n",
      "Gradient Descent(9/49): loss=0.45259194330693947\n",
      "Gradient Descent(10/49): loss=0.45214296830590295\n",
      "Gradient Descent(11/49): loss=0.45184107751520597\n",
      "Gradient Descent(12/49): loss=0.4516380861475414\n",
      "Gradient Descent(13/49): loss=0.4515015947519235\n",
      "Gradient Descent(14/49): loss=0.45140981793751017\n",
      "Gradient Descent(15/49): loss=0.4513481072074986\n",
      "Gradient Descent(16/49): loss=0.4513066129126388\n",
      "Gradient Descent(17/49): loss=0.4512787121487753\n",
      "Gradient Descent(18/49): loss=0.4512599516751531\n",
      "Gradient Descent(19/49): loss=0.45124733713268983\n",
      "Gradient Descent(20/49): loss=0.45123885511433737\n",
      "Gradient Descent(21/49): loss=0.4512331518051972\n",
      "Gradient Descent(22/49): loss=0.4512293169001315\n",
      "Gradient Descent(23/49): loss=0.45122673830996507\n",
      "Gradient Descent(24/49): loss=0.4512250044659374\n",
      "Gradient Descent(25/49): loss=0.45122383862921306\n",
      "Gradient Descent(26/49): loss=0.4512230547205996\n",
      "Gradient Descent(27/49): loss=0.45122252762044796\n",
      "Gradient Descent(28/49): loss=0.451222173198306\n",
      "Gradient Descent(29/49): loss=0.4512219348848578\n",
      "Gradient Descent(30/49): loss=0.4512217746428952\n",
      "Gradient Descent(31/49): loss=0.4512216668961994\n",
      "Gradient Descent(32/49): loss=0.45122159444732135\n",
      "Gradient Descent(33/49): loss=0.4512215457326955\n",
      "Gradient Descent(34/49): loss=0.4512215129769815\n",
      "Gradient Descent(35/49): loss=0.4512214909520393\n",
      "Gradient Descent(36/49): loss=0.45122147614246777\n",
      "Gradient Descent(37/49): loss=0.4512214661845123\n",
      "Gradient Descent(38/49): loss=0.4512214594887829\n",
      "Gradient Descent(39/49): loss=0.45122145498657434\n",
      "Gradient Descent(40/49): loss=0.4512214519592893\n",
      "Gradient Descent(41/49): loss=0.45122144992374325\n",
      "Gradient Descent(42/49): loss=0.4512214485550416\n",
      "Gradient Descent(43/49): loss=0.45122144763472666\n",
      "Gradient Descent(44/49): loss=0.45122144701590716\n",
      "Gradient Descent(45/49): loss=0.4512214465998127\n",
      "Gradient Descent(46/49): loss=0.45122144632003086\n",
      "Gradient Descent(47/49): loss=0.4512214461319057\n",
      "Gradient Descent(48/49): loss=0.4512214460054103\n",
      "Gradient Descent(49/49): loss=0.4512214459203545\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4836192851109887\n",
      "Gradient Descent(2/49): loss=0.472604892419618\n",
      "Gradient Descent(3/49): loss=0.46519881477393976\n",
      "Gradient Descent(4/49): loss=0.4602189681649859\n",
      "Gradient Descent(5/49): loss=0.4568705193051254\n",
      "Gradient Descent(6/49): loss=0.45461902229175505\n",
      "Gradient Descent(7/49): loss=0.45310511569996503\n",
      "Gradient Descent(8/49): loss=0.4520871649076452\n",
      "Gradient Descent(9/49): loss=0.45140269479488937\n",
      "Gradient Descent(10/49): loss=0.4509424570910725\n",
      "Gradient Descent(11/49): loss=0.4506329932590259\n",
      "Gradient Descent(12/49): loss=0.450424909778358\n",
      "Gradient Descent(13/49): loss=0.45028499444595654\n",
      "Gradient Descent(14/49): loss=0.45019091537645\n",
      "Gradient Descent(15/49): loss=0.4501276566101139\n",
      "Gradient Descent(16/49): loss=0.45008512141562934\n",
      "Gradient Descent(17/49): loss=0.4500565207508579\n",
      "Gradient Descent(18/49): loss=0.45003728966386575\n",
      "Gradient Descent(19/49): loss=0.45002435868097207\n",
      "Gradient Descent(20/49): loss=0.4500156638880745\n",
      "Gradient Descent(21/49): loss=0.45000981750932995\n",
      "Gradient Descent(22/49): loss=0.45000588640426253\n",
      "Gradient Descent(23/49): loss=0.45000324312921475\n",
      "Gradient Descent(24/49): loss=0.4500014657910729\n",
      "Gradient Descent(25/49): loss=0.4500002707089061\n",
      "Gradient Descent(26/49): loss=0.4499994671356573\n",
      "Gradient Descent(27/49): loss=0.4499989268130047\n",
      "Gradient Descent(28/49): loss=0.4499985635000532\n",
      "Gradient Descent(29/49): loss=0.4499983192084246\n",
      "Gradient Descent(30/49): loss=0.4499981549467335\n",
      "Gradient Descent(31/49): loss=0.44999804449717234\n",
      "Gradient Descent(32/49): loss=0.44999797023088733\n",
      "Gradient Descent(33/49): loss=0.4499979202942376\n",
      "Gradient Descent(34/49): loss=0.4499978867168341\n",
      "Gradient Descent(35/49): loss=0.4499978641393881\n",
      "Gradient Descent(36/49): loss=0.4499978489583133\n",
      "Gradient Descent(37/49): loss=0.44999783875055865\n",
      "Gradient Descent(38/49): loss=0.4499978318868643\n",
      "Gradient Descent(39/49): loss=0.4499978272717165\n",
      "Gradient Descent(40/49): loss=0.4499978241684909\n",
      "Gradient Descent(41/49): loss=0.4499978220818822\n",
      "Gradient Descent(42/49): loss=0.4499978206788462\n",
      "Gradient Descent(43/49): loss=0.449997819735445\n",
      "Gradient Descent(44/49): loss=0.4499978191011022\n",
      "Gradient Descent(45/49): loss=0.44999781867456995\n",
      "Gradient Descent(46/49): loss=0.4499978183877694\n",
      "Gradient Descent(47/49): loss=0.449997818194925\n",
      "Gradient Descent(48/49): loss=0.4499978180652565\n",
      "Gradient Descent(49/49): loss=0.44999781797806715\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4838088024320001\n",
      "Gradient Descent(2/49): loss=0.47292184118727704\n",
      "Gradient Descent(3/49): loss=0.465601448446325\n",
      "Gradient Descent(4/49): loss=0.46067921636730896\n",
      "Gradient Descent(5/49): loss=0.45736950751737865\n",
      "Gradient Descent(6/49): loss=0.45514405928668544\n",
      "Gradient Descent(7/49): loss=0.4536476678963675\n",
      "Gradient Descent(8/49): loss=0.45264149432551737\n",
      "Gradient Descent(9/49): loss=0.4519649432164779\n",
      "Gradient Descent(10/49): loss=0.4515100302507598\n",
      "Gradient Descent(11/49): loss=0.45120414677261067\n",
      "Gradient Descent(12/49): loss=0.4509984707219036\n",
      "Gradient Descent(13/49): loss=0.4508601741454081\n",
      "Gradient Descent(14/49): loss=0.45076718352737233\n",
      "Gradient Descent(15/49): loss=0.45070465663580506\n",
      "Gradient Descent(16/49): loss=0.45066261355391524\n",
      "Gradient Descent(17/49): loss=0.45063434378565276\n",
      "Gradient Descent(18/49): loss=0.45061533519347285\n",
      "Gradient Descent(19/49): loss=0.45060255381609127\n",
      "Gradient Descent(20/49): loss=0.4505939596179397\n",
      "Gradient Descent(21/49): loss=0.4505881808791026\n",
      "Gradient Descent(22/49): loss=0.45058429525510857\n",
      "Gradient Descent(23/49): loss=0.45058168256153525\n",
      "Gradient Descent(24/49): loss=0.4505799257863763\n",
      "Gradient Descent(25/49): loss=0.4505787445307592\n",
      "Gradient Descent(26/49): loss=0.45057795025448266\n",
      "Gradient Descent(27/49): loss=0.45057741618311403\n",
      "Gradient Descent(28/49): loss=0.4505770570735258\n",
      "Gradient Descent(29/49): loss=0.4505768156082388\n",
      "Gradient Descent(30/49): loss=0.45057665324697976\n",
      "Gradient Descent(31/49): loss=0.4505765440752691\n",
      "Gradient Descent(32/49): loss=0.45057647066821105\n",
      "Gradient Descent(33/49): loss=0.45057642130930514\n",
      "Gradient Descent(34/49): loss=0.4505763881203766\n",
      "Gradient Descent(35/49): loss=0.4505763658041414\n",
      "Gradient Descent(36/49): loss=0.45057635079870456\n",
      "Gradient Descent(37/49): loss=0.450576340709049\n",
      "Gradient Descent(38/49): loss=0.4505763339247645\n",
      "Gradient Descent(39/49): loss=0.45057632936301184\n",
      "Gradient Descent(40/49): loss=0.450576326295689\n",
      "Gradient Descent(41/49): loss=0.4505763242332214\n",
      "Gradient Descent(42/49): loss=0.450576322846418\n",
      "Gradient Descent(43/49): loss=0.4505763219139314\n",
      "Gradient Descent(44/49): loss=0.4505763212869276\n",
      "Gradient Descent(45/49): loss=0.4505763208653301\n",
      "Gradient Descent(46/49): loss=0.45057632058184793\n",
      "Gradient Descent(47/49): loss=0.4505763203912347\n",
      "Gradient Descent(48/49): loss=0.45057632026306604\n",
      "Gradient Descent(49/49): loss=0.4505763201768859\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48367669733887986\n",
      "Gradient Descent(2/49): loss=0.4727009086295425\n",
      "Gradient Descent(3/49): loss=0.46532078830138424\n",
      "Gradient Descent(4/49): loss=0.46035839539273077\n",
      "Gradient Descent(5/49): loss=0.4570216824009521\n",
      "Gradient Descent(6/49): loss=0.45477807658528\n",
      "Gradient Descent(7/49): loss=0.45326947603482226\n",
      "Gradient Descent(8/49): loss=0.45225509302469447\n",
      "Gradient Descent(9/49): loss=0.45157302188868453\n",
      "Gradient Descent(10/49): loss=0.4511143972568316\n",
      "Gradient Descent(11/49): loss=0.4508060180543734\n",
      "Gradient Descent(12/49): loss=0.4505986638786407\n",
      "Gradient Descent(13/49): loss=0.45045923893087797\n",
      "Gradient Descent(14/49): loss=0.4503654895960024\n",
      "Gradient Descent(15/49): loss=0.450302452543232\n",
      "Gradient Descent(16/49): loss=0.4502600664289493\n",
      "Gradient Descent(17/49): loss=0.45023156600570546\n",
      "Gradient Descent(18/49): loss=0.45021240232111637\n",
      "Gradient Descent(19/49): loss=0.4501995166595985\n",
      "Gradient Descent(20/49): loss=0.4501908523407939\n",
      "Gradient Descent(21/49): loss=0.45018502645282993\n",
      "Gradient Descent(22/49): loss=0.45018110912576303\n",
      "Gradient Descent(23/49): loss=0.45017847511504294\n",
      "Gradient Descent(24/49): loss=0.4501767040062349\n",
      "Gradient Descent(25/49): loss=0.4501755131126724\n",
      "Gradient Descent(26/49): loss=0.45017471235584106\n",
      "Gradient Descent(27/49): loss=0.4501741739269475\n",
      "Gradient Descent(28/49): loss=0.4501738118873592\n",
      "Gradient Descent(29/49): loss=0.45017356845194045\n",
      "Gradient Descent(30/49): loss=0.4501734047659648\n",
      "Gradient Descent(31/49): loss=0.4501732947035147\n",
      "Gradient Descent(32/49): loss=0.4501732206975233\n",
      "Gradient Descent(33/49): loss=0.45017317093589465\n",
      "Gradient Descent(34/49): loss=0.4501731374761754\n",
      "Gradient Descent(35/49): loss=0.45017311497786033\n",
      "Gradient Descent(36/49): loss=0.45017309984999326\n",
      "Gradient Descent(37/49): loss=0.45017308967801556\n",
      "Gradient Descent(38/49): loss=0.4501730828383776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=0.45017307823940506\n",
      "Gradient Descent(40/49): loss=0.45017307514705596\n",
      "Gradient Descent(41/49): loss=0.4501730730677605\n",
      "Gradient Descent(42/49): loss=0.4501730716696422\n",
      "Gradient Descent(43/49): loss=0.4501730707295475\n",
      "Gradient Descent(44/49): loss=0.45017307009742774\n",
      "Gradient Descent(45/49): loss=0.45017306967239035\n",
      "Gradient Descent(46/49): loss=0.45017306938659524\n",
      "Gradient Descent(47/49): loss=0.4501730691944266\n",
      "Gradient Descent(48/49): loss=0.45017306906521243\n",
      "Gradient Descent(49/49): loss=0.450173068978329\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48166414145583786\n",
      "Gradient Descent(2/49): loss=0.4702207321384267\n",
      "Gradient Descent(3/49): loss=0.4630789003834301\n",
      "Gradient Descent(4/49): loss=0.4586216831851362\n",
      "Gradient Descent(5/49): loss=0.4558399339316813\n",
      "Gradient Descent(6/49): loss=0.4541038442226004\n",
      "Gradient Descent(7/49): loss=0.4530203506351626\n",
      "Gradient Descent(8/49): loss=0.4523441422872429\n",
      "Gradient Descent(9/49): loss=0.4519221206573061\n",
      "Gradient Descent(10/49): loss=0.4516587369580627\n",
      "Gradient Descent(11/49): loss=0.4514943591913647\n",
      "Gradient Descent(12/49): loss=0.4513917710271687\n",
      "Gradient Descent(13/49): loss=0.45132774575389384\n",
      "Gradient Descent(14/49): loss=0.4512877875808431\n",
      "Gradient Descent(15/49): loss=0.4512628496850421\n",
      "Gradient Descent(16/49): loss=0.45124728594427255\n",
      "Gradient Descent(17/49): loss=0.4512375726136582\n",
      "Gradient Descent(18/49): loss=0.4512315105240222\n",
      "Gradient Descent(19/49): loss=0.45122772717388004\n",
      "Gradient Descent(20/49): loss=0.4512253659850563\n",
      "Gradient Descent(21/49): loss=0.45122389236711175\n",
      "Gradient Descent(22/49): loss=0.4512229726821523\n",
      "Gradient Descent(23/49): loss=0.451222398706769\n",
      "Gradient Descent(24/49): loss=0.45122204048873243\n",
      "Gradient Descent(25/49): loss=0.4512218169248558\n",
      "Gradient Descent(26/49): loss=0.4512216773986402\n",
      "Gradient Descent(27/49): loss=0.45122159032032944\n",
      "Gradient Descent(28/49): loss=0.4512215359747554\n",
      "Gradient Descent(29/49): loss=0.45122150205768274\n",
      "Gradient Descent(30/49): loss=0.45122148089003766\n",
      "Gradient Descent(31/49): loss=0.45122146767931015\n",
      "Gradient Descent(32/49): loss=0.4512214594344953\n",
      "Gradient Descent(33/49): loss=0.4512214542889064\n",
      "Gradient Descent(34/49): loss=0.45122145107754436\n",
      "Gradient Descent(35/49): loss=0.4512214490733331\n",
      "Gradient Descent(36/49): loss=0.45122144782250523\n",
      "Gradient Descent(37/49): loss=0.45122144704186334\n",
      "Gradient Descent(38/49): loss=0.45122144655466473\n",
      "Gradient Descent(39/49): loss=0.4512214462506042\n",
      "Gradient Descent(40/49): loss=0.45122144606083986\n",
      "Gradient Descent(41/49): loss=0.45122144594240815\n",
      "Gradient Descent(42/49): loss=0.4512214458684948\n",
      "Gradient Descent(43/49): loss=0.45122144582236545\n",
      "Gradient Descent(44/49): loss=0.45122144579357615\n",
      "Gradient Descent(45/49): loss=0.45122144577560874\n",
      "Gradient Descent(46/49): loss=0.45122144576439543\n",
      "Gradient Descent(47/49): loss=0.45122144575739714\n",
      "Gradient Descent(48/49): loss=0.45122144575302925\n",
      "Gradient Descent(49/49): loss=0.45122144575030354\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4812041797106857\n",
      "Gradient Descent(2/49): loss=0.469473708268125\n",
      "Gradient Descent(3/49): loss=0.4621527210408228\n",
      "Gradient Descent(4/49): loss=0.4575836929122633\n",
      "Gradient Descent(5/49): loss=0.4547321624572294\n",
      "Gradient Descent(6/49): loss=0.45295252230024274\n",
      "Gradient Descent(7/49): loss=0.4518418488782673\n",
      "Gradient Descent(8/49): loss=0.45114867759561267\n",
      "Gradient Descent(9/49): loss=0.4507160693981077\n",
      "Gradient Descent(10/49): loss=0.4504460786220448\n",
      "Gradient Descent(11/49): loss=0.45027757737870405\n",
      "Gradient Descent(12/49): loss=0.4501724157527351\n",
      "Gradient Descent(13/49): loss=0.45010678438196783\n",
      "Gradient Descent(14/49): loss=0.4500658238434719\n",
      "Gradient Descent(15/49): loss=0.4500402603713967\n",
      "Gradient Descent(16/49): loss=0.4500243062084748\n",
      "Gradient Descent(17/49): loss=0.4500143492153948\n",
      "Gradient Descent(18/49): loss=0.4500081350560138\n",
      "Gradient Descent(19/49): loss=0.4500042567991443\n",
      "Gradient Descent(20/49): loss=0.4500018363790315\n",
      "Gradient Descent(21/49): loss=0.4500003257948396\n",
      "Gradient Descent(22/49): loss=0.4499993830392453\n",
      "Gradient Descent(23/49): loss=0.44999879466547893\n",
      "Gradient Descent(24/49): loss=0.44999842746141117\n",
      "Gradient Descent(25/49): loss=0.4499981982893526\n",
      "Gradient Descent(26/49): loss=0.44999805526307085\n",
      "Gradient Descent(27/49): loss=0.44999796600036834\n",
      "Gradient Descent(28/49): loss=0.44999791029151576\n",
      "Gradient Descent(29/49): loss=0.4499978755236209\n",
      "Gradient Descent(30/49): loss=0.4499978538249776\n",
      "Gradient Descent(31/49): loss=0.4499978402828542\n",
      "Gradient Descent(32/49): loss=0.4499978318312153\n",
      "Gradient Descent(33/49): loss=0.44999782655654724\n",
      "Gradient Descent(34/49): loss=0.44999782326462695\n",
      "Gradient Descent(35/49): loss=0.44999782121013965\n",
      "Gradient Descent(36/49): loss=0.4499978199279341\n",
      "Gradient Descent(37/49): loss=0.44999781912770953\n",
      "Gradient Descent(38/49): loss=0.44999781862828925\n",
      "Gradient Descent(39/49): loss=0.44999781831660124\n",
      "Gradient Descent(40/49): loss=0.44999781812207656\n",
      "Gradient Descent(41/49): loss=0.449997818000674\n",
      "Gradient Descent(42/49): loss=0.4499978179249065\n",
      "Gradient Descent(43/49): loss=0.44999781787762\n",
      "Gradient Descent(44/49): loss=0.4499978178481086\n",
      "Gradient Descent(45/49): loss=0.44999781782969034\n",
      "Gradient Descent(46/49): loss=0.44999781781819553\n",
      "Gradient Descent(47/49): loss=0.4499978178110217\n",
      "Gradient Descent(48/49): loss=0.4499978178065444\n",
      "Gradient Descent(49/49): loss=0.44999781780375026\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.481421638688\n",
      "Gradient Descent(2/49): loss=0.469826883393181\n",
      "Gradient Descent(3/49): loss=0.4625905966136842\n",
      "Gradient Descent(4/49): loss=0.4580744300346003\n",
      "Gradient Descent(5/49): loss=0.455255890472594\n",
      "Gradient Descent(6/49): loss=0.45349683993194617\n",
      "Gradient Descent(7/49): loss=0.4523990164895274\n",
      "Gradient Descent(8/49): loss=0.4517138648791141\n",
      "Gradient Descent(9/49): loss=0.4512862617590552\n",
      "Gradient Descent(10/49): loss=0.45101939465182644\n",
      "Gradient Descent(11/49): loss=0.4508528428902048\n",
      "Gradient Descent(12/49): loss=0.4507488979357768\n",
      "Gradient Descent(13/49): loss=0.45068402588971845\n",
      "Gradient Descent(14/49): loss=0.45064353924577305\n",
      "Gradient Descent(15/49): loss=0.45061827153128714\n",
      "Gradient Descent(16/49): loss=0.45060250195067614\n",
      "Gradient Descent(17/49): loss=0.45059266015541694\n",
      "Gradient Descent(18/49): loss=0.45058651789099574\n",
      "Gradient Descent(19/49): loss=0.45058268450377037\n",
      "Gradient Descent(20/49): loss=0.4505802920868032\n",
      "Gradient Descent(21/49): loss=0.45057879897937386\n",
      "Gradient Descent(22/49): loss=0.45057786713102715\n",
      "Gradient Descent(23/49): loss=0.45057728556447413\n",
      "Gradient Descent(24/49): loss=0.4505769226087882\n",
      "Gradient Descent(25/49): loss=0.45057669608814477\n",
      "Gradient Descent(26/49): loss=0.4505765547166112\n",
      "Gradient Descent(27/49): loss=0.45057646648663696\n",
      "Gradient Descent(28/49): loss=0.45057641142231025\n",
      "Gradient Descent(29/49): loss=0.45057637705666387\n",
      "Gradient Descent(30/49): loss=0.4505763556090639\n",
      "Gradient Descent(31/49): loss=0.4505763422236167\n",
      "Gradient Descent(32/49): loss=0.45057633386975915\n",
      "Gradient Descent(33/49): loss=0.45057632865611663\n",
      "Gradient Descent(34/49): loss=0.4505763254022823\n",
      "Gradient Descent(35/49): loss=0.4505763233715645\n",
      "Gradient Descent(36/49): loss=0.4505763221041934\n",
      "Gradient Descent(37/49): loss=0.4505763213132272\n",
      "Gradient Descent(38/49): loss=0.45057632081958504\n",
      "Gradient Descent(39/49): loss=0.4505763205115032\n",
      "Gradient Descent(40/49): loss=0.45057632031922906\n",
      "Gradient Descent(41/49): loss=0.450576320199231\n",
      "Gradient Descent(42/49): loss=0.45057632012433985\n",
      "Gradient Descent(43/49): loss=0.4505763200776005\n",
      "Gradient Descent(44/49): loss=0.45057632004843046\n",
      "Gradient Descent(45/49): loss=0.4505763200302255\n",
      "Gradient Descent(46/49): loss=0.4505763200188637\n",
      "Gradient Descent(47/49): loss=0.4505763200117729\n",
      "Gradient Descent(48/49): loss=0.4505763200073475\n",
      "Gradient Descent(49/49): loss=0.45057632000458564\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48127005656192007\n",
      "Gradient Descent(2/49): loss=0.4695806988622141\n",
      "Gradient Descent(3/49): loss=0.4622853707218279\n",
      "Gradient Descent(4/49): loss=0.45773235642941296\n",
      "Gradient Descent(5/49): loss=0.45489082020951677\n",
      "Gradient Descent(6/49): loss=0.45311741745467926\n",
      "Gradient Descent(7/49): loss=0.45201063679538545\n",
      "Gradient Descent(8/49): loss=0.45131989498592007\n",
      "Gradient Descent(9/49): loss=0.4508888030226325\n",
      "Gradient Descent(10/49): loss=0.45061975852834524\n",
      "Gradient Descent(11/49): loss=0.4504518478594604\n",
      "Gradient Descent(12/49): loss=0.4503470548110091\n",
      "Gradient Descent(13/49): loss=0.45028165346947074\n",
      "Gradient Descent(14/49): loss=0.4502408364922165\n",
      "Gradient Descent(15/49): loss=0.45021536261671236\n",
      "Gradient Descent(16/49): loss=0.4501994643710102\n",
      "Gradient Descent(17/49): loss=0.4501895422758676\n",
      "Gradient Descent(18/49): loss=0.45018334989628894\n",
      "Gradient Descent(19/49): loss=0.450179485232194\n",
      "Gradient Descent(20/49): loss=0.45017707329533235\n",
      "Gradient Descent(21/49): loss=0.4501755680055367\n",
      "Gradient Descent(22/49): loss=0.4501746285541756\n",
      "Gradient Descent(23/49): loss=0.4501740422425809\n",
      "Gradient Descent(24/49): loss=0.4501736763255146\n",
      "Gradient Descent(25/49): loss=0.4501734479566736\n",
      "Gradient Descent(26/49): loss=0.45017330543168\n",
      "Gradient Descent(27/49): loss=0.4501732164818316\n",
      "Gradient Descent(28/49): loss=0.4501731609682312\n",
      "Gradient Descent(29/49): loss=0.4501731263221929\n",
      "Gradient Descent(30/49): loss=0.4501731046996007\n",
      "Gradient Descent(31/49): loss=0.4501730912049408\n",
      "Gradient Descent(32/49): loss=0.45017308278292345\n",
      "Gradient Descent(33/49): loss=0.45017307752674246\n",
      "Gradient Descent(34/49): loss=0.45017307424636\n",
      "Gradient Descent(35/49): loss=0.4501730721990734\n",
      "Gradient Descent(36/49): loss=0.45017307092136166\n",
      "Gradient Descent(37/49): loss=0.4501730701239417\n",
      "Gradient Descent(38/49): loss=0.4501730696262719\n",
      "Gradient Descent(39/49): loss=0.45017306931567636\n",
      "Gradient Descent(40/49): loss=0.45017306912183364\n",
      "Gradient Descent(41/49): loss=0.4501730690008564\n",
      "Gradient Descent(42/49): loss=0.4501730689253544\n",
      "Gradient Descent(43/49): loss=0.45017306887823366\n",
      "Gradient Descent(44/49): loss=0.45017306884882535\n",
      "Gradient Descent(45/49): loss=0.4501730688304722\n",
      "Gradient Descent(46/49): loss=0.45017306881901775\n",
      "Gradient Descent(47/49): loss=0.45017306881186886\n",
      "Gradient Descent(48/49): loss=0.4501730688074074\n",
      "Gradient Descent(49/49): loss=0.450173068804623\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4793959386830166\n",
      "Gradient Descent(2/49): loss=0.4674950328663271\n",
      "Gradient Descent(3/49): loss=0.46062106966660676\n",
      "Gradient Descent(4/49): loss=0.45665066852244895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=0.4543573648215831\n",
      "Gradient Descent(6/49): loss=0.45303275260396286\n",
      "Gradient Descent(7/49): loss=0.4522676565870654\n",
      "Gradient Descent(8/49): loss=0.45182573712770546\n",
      "Gradient Descent(9/49): loss=0.45157048444797937\n",
      "Gradient Descent(10/49): loss=0.45142305050016923\n",
      "Gradient Descent(11/49): loss=0.4513378926519143\n",
      "Gradient Descent(12/49): loss=0.45128870547876226\n",
      "Gradient Descent(13/49): loss=0.4512602949675499\n",
      "Gradient Descent(14/49): loss=0.45124388505627305\n",
      "Gradient Descent(15/49): loss=0.45123440669152\n",
      "Gradient Descent(16/49): loss=0.4512289319880384\n",
      "Gradient Descent(17/49): loss=0.4512257697993077\n",
      "Gradient Descent(18/49): loss=0.4512239433190966\n",
      "Gradient Descent(19/49): loss=0.4512228883441268\n",
      "Gradient Descent(20/49): loss=0.4512222789905841\n",
      "Gradient Descent(21/49): loss=0.451221927027978\n",
      "Gradient Descent(22/49): loss=0.4512217237343768\n",
      "Gradient Descent(23/49): loss=0.4512216063119926\n",
      "Gradient Descent(24/49): loss=0.4512215384888234\n",
      "Gradient Descent(25/49): loss=0.45122149931416095\n",
      "Gradient Descent(26/49): loss=0.4512214766868759\n",
      "Gradient Descent(27/49): loss=0.45122146361735593\n",
      "Gradient Descent(28/49): loss=0.4512214560684015\n",
      "Gradient Descent(29/49): loss=0.4512214517081251\n",
      "Gradient Descent(30/49): loss=0.4512214491896297\n",
      "Gradient Descent(31/49): loss=0.4512214477349467\n",
      "Gradient Descent(32/49): loss=0.4512214468947219\n",
      "Gradient Descent(33/49): loss=0.4512214464094076\n",
      "Gradient Descent(34/49): loss=0.4512214461290904\n",
      "Gradient Descent(35/49): loss=0.45122144596717917\n",
      "Gradient Descent(36/49): loss=0.4512214458736591\n",
      "Gradient Descent(37/49): loss=0.4512214458196421\n",
      "Gradient Descent(38/49): loss=0.45122144578844176\n",
      "Gradient Descent(39/49): loss=0.4512214457704202\n",
      "Gradient Descent(40/49): loss=0.4512214457600114\n",
      "Gradient Descent(41/49): loss=0.4512214457539992\n",
      "Gradient Descent(42/49): loss=0.4512214457505265\n",
      "Gradient Descent(43/49): loss=0.45122144574852063\n",
      "Gradient Descent(44/49): loss=0.451221445747362\n",
      "Gradient Descent(45/49): loss=0.451221445746693\n",
      "Gradient Descent(46/49): loss=0.45122144574630635\n",
      "Gradient Descent(47/49): loss=0.45122144574608314\n",
      "Gradient Descent(48/49): loss=0.45122144574595413\n",
      "Gradient Descent(49/49): loss=0.4512214457458798\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4788790782383446\n",
      "Gradient Descent(2/49): loss=0.4666796338288125\n",
      "Gradient Descent(3/49): loss=0.4596332347378667\n",
      "Gradient Descent(4/49): loss=0.45556323462293646\n",
      "Gradient Descent(5/49): loss=0.45321240255655265\n",
      "Gradient Descent(6/49): loss=0.4518545619550093\n",
      "Gradient Descent(7/49): loss=0.45107027322355814\n",
      "Gradient Descent(8/49): loss=0.45061726805227165\n",
      "Gradient Descent(9/49): loss=0.45035561226533677\n",
      "Gradient Descent(10/49): loss=0.45020447988280304\n",
      "Gradient Descent(11/49): loss=0.4501171858186514\n",
      "Gradient Descent(12/49): loss=0.4500667647671975\n",
      "Gradient Descent(13/49): loss=0.4500376415678778\n",
      "Gradient Descent(14/49): loss=0.4500208200079511\n",
      "Gradient Descent(15/49): loss=0.45001110387493687\n",
      "Gradient Descent(16/49): loss=0.4500054918365082\n",
      "Gradient Descent(17/49): loss=0.4500022503231116\n",
      "Gradient Descent(18/49): loss=0.45000037802497367\n",
      "Gradient Descent(19/49): loss=0.44999929658556936\n",
      "Gradient Descent(20/49): loss=0.44999867194616944\n",
      "Gradient Descent(21/49): loss=0.44999831115445194\n",
      "Gradient Descent(22/49): loss=0.449998102761156\n",
      "Gradient Descent(23/49): loss=0.44999798239318817\n",
      "Gradient Descent(24/49): loss=0.4499979128686501\n",
      "Gradient Descent(25/49): loss=0.44999787271127656\n",
      "Gradient Descent(26/49): loss=0.44999784951637817\n",
      "Gradient Descent(27/49): loss=0.4499978361190046\n",
      "Gradient Descent(28/49): loss=0.4499978283806814\n",
      "Gradient Descent(29/49): loss=0.4499978239110263\n",
      "Gradient Descent(30/49): loss=0.4499978213293533\n",
      "Gradient Descent(31/49): loss=0.44999781983817905\n",
      "Gradient Descent(32/49): loss=0.44999781897687674\n",
      "Gradient Descent(33/49): loss=0.4499978184793886\n",
      "Gradient Descent(34/49): loss=0.44999781819203943\n",
      "Gradient Descent(35/49): loss=0.4499978180260664\n",
      "Gradient Descent(36/49): loss=0.4499978179302006\n",
      "Gradient Descent(37/49): loss=0.4499978178748285\n",
      "Gradient Descent(38/49): loss=0.4499978178428455\n",
      "Gradient Descent(39/49): loss=0.4499978178243721\n",
      "Gradient Descent(40/49): loss=0.4499978178137017\n",
      "Gradient Descent(41/49): loss=0.4499978178075387\n",
      "Gradient Descent(42/49): loss=0.4499978178039789\n",
      "Gradient Descent(43/49): loss=0.4499978178019228\n",
      "Gradient Descent(44/49): loss=0.44999781780073494\n",
      "Gradient Descent(45/49): loss=0.4499978178000492\n",
      "Gradient Descent(46/49): loss=0.4499978177996529\n",
      "Gradient Descent(47/49): loss=0.4499978177994241\n",
      "Gradient Descent(48/49): loss=0.4499978177992918\n",
      "Gradient Descent(49/49): loss=0.4499978177992156\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4791234375679999\n",
      "Gradient Descent(2/49): loss=0.4670651351072765\n",
      "Gradient Descent(3/49): loss=0.4601002596059628\n",
      "Gradient Descent(4/49): loss=0.456077347516404\n",
      "Gradient Descent(5/49): loss=0.453753713493475\n",
      "Gradient Descent(6/49): loss=0.45241158248183133\n",
      "Gradient Descent(7/49): loss=0.4516363676095056\n",
      "Gradient Descent(8/49): loss=0.4511886034992505\n",
      "Gradient Descent(9/49): loss=0.4509299749491671\n",
      "Gradient Descent(10/49): loss=0.4507805910986387\n",
      "Gradient Descent(11/49): loss=0.4506943069865739\n",
      "Gradient Descent(12/49): loss=0.4506444692834452\n",
      "Gradient Descent(13/49): loss=0.4506156830261179\n",
      "Gradient Descent(14/49): loss=0.45059905608388573\n",
      "Gradient Descent(15/49): loss=0.4505894523620521\n",
      "Gradient Descent(16/49): loss=0.45058390525232156\n",
      "Gradient Descent(17/49): loss=0.4505807012417409\n",
      "Gradient Descent(18/49): loss=0.4505788506052297\n",
      "Gradient Descent(19/49): loss=0.4505777816775807\n",
      "Gradient Descent(20/49): loss=0.4505771642649705\n",
      "Gradient Descent(21/49): loss=0.45057680764744684\n",
      "Gradient Descent(22/49): loss=0.45057660166516544\n",
      "Gradient Descent(23/49): loss=0.45057648268979933\n",
      "Gradient Descent(24/49): loss=0.4505764139696282\n",
      "Gradient Descent(25/49): loss=0.45057637427685737\n",
      "Gradient Descent(26/49): loss=0.45057635135031265\n",
      "Gradient Descent(27/49): loss=0.45057633810794057\n",
      "Gradient Descent(28/49): loss=0.4505763304591466\n",
      "Gradient Descent(29/49): loss=0.4505763260412031\n",
      "Gradient Descent(30/49): loss=0.4505763234893989\n",
      "Gradient Descent(31/49): loss=0.45057632201547665\n",
      "Gradient Descent(32/49): loss=0.4505763211641393\n",
      "Gradient Descent(33/49): loss=0.4505763206724068\n",
      "Gradient Descent(34/49): loss=0.45057632038838247\n",
      "Gradient Descent(35/49): loss=0.4505763202243296\n",
      "Gradient Descent(36/49): loss=0.4505763201295727\n",
      "Gradient Descent(37/49): loss=0.45057632007484116\n",
      "Gradient Descent(38/49): loss=0.4505763200432282\n",
      "Gradient Descent(39/49): loss=0.4505763200249686\n",
      "Gradient Descent(40/49): loss=0.450576320014422\n",
      "Gradient Descent(41/49): loss=0.45057632000833014\n",
      "Gradient Descent(42/49): loss=0.4505763200048115\n",
      "Gradient Descent(43/49): loss=0.45057632000277914\n",
      "Gradient Descent(44/49): loss=0.4505763200016053\n",
      "Gradient Descent(45/49): loss=0.45057632000092707\n",
      "Gradient Descent(46/49): loss=0.4505763200005356\n",
      "Gradient Descent(47/49): loss=0.45057632000030917\n",
      "Gradient Descent(48/49): loss=0.45057632000017867\n",
      "Gradient Descent(49/49): loss=0.45057632000010306\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47895310426112003\n",
      "Gradient Descent(2/49): loss=0.4667964172823429\n",
      "Gradient Descent(3/49): loss=0.4597747148834012\n",
      "Gradient Descent(4/49): loss=0.4557189795777724\n",
      "Gradient Descent(5/49): loss=0.45337638686524123\n",
      "Gradient Descent(6/49): loss=0.45202330531448337\n",
      "Gradient Descent(7/49): loss=0.45124176541076544\n",
      "Gradient Descent(8/49): loss=0.450790347962378\n",
      "Gradient Descent(9/49): loss=0.45052960924418956\n",
      "Gradient Descent(10/49): loss=0.45037900656056407\n",
      "Gradient Descent(11/49): loss=0.4502920184505018\n",
      "Gradient Descent(12/49): loss=0.45024177411812977\n",
      "Gradient Descent(13/49): loss=0.4502127529917516\n",
      "Gradient Descent(14/49): loss=0.450195990389156\n",
      "Gradient Descent(15/49): loss=0.4501863083098965\n",
      "Gradient Descent(16/49): loss=0.4501807159409161\n",
      "Gradient Descent(17/49): loss=0.4501774857885931\n",
      "Gradient Descent(18/49): loss=0.45017562005261147\n",
      "Gradient Descent(19/49): loss=0.45017454240350824\n",
      "Gradient Descent(20/49): loss=0.4501739199533863\n",
      "Gradient Descent(21/49): loss=0.45017356042619583\n",
      "Gradient Descent(22/49): loss=0.45017335276329074\n",
      "Gradient Descent(23/49): loss=0.4501732328171966\n",
      "Gradient Descent(24/49): loss=0.450173163536333\n",
      "Gradient Descent(25/49): loss=0.450173123519706\n",
      "Gradient Descent(26/49): loss=0.450173100406102\n",
      "Gradient Descent(27/49): loss=0.4501730870556846\n",
      "Gradient Descent(28/49): loss=0.4501730793444835\n",
      "Gradient Descent(29/49): loss=0.45017307489049374\n",
      "Gradient Descent(30/49): loss=0.450173072317869\n",
      "Gradient Descent(31/49): loss=0.4501730708319213\n",
      "Gradient Descent(32/49): loss=0.4501730699736375\n",
      "Gradient Descent(33/49): loss=0.4501730694778932\n",
      "Gradient Descent(34/49): loss=0.450173069191551\n",
      "Gradient Descent(35/49): loss=0.45017306902616\n",
      "Gradient Descent(36/49): loss=0.45017306893062997\n",
      "Gradient Descent(37/49): loss=0.4501730688754519\n",
      "Gradient Descent(38/49): loss=0.4501730688435809\n",
      "Gradient Descent(39/49): loss=0.4501730688251721\n",
      "Gradient Descent(40/49): loss=0.4501730688145395\n",
      "Gradient Descent(41/49): loss=0.4501730688083982\n",
      "Gradient Descent(42/49): loss=0.4501730688048506\n",
      "Gradient Descent(43/49): loss=0.45017306880280183\n",
      "Gradient Descent(44/49): loss=0.45017306880161834\n",
      "Gradient Descent(45/49): loss=0.4501730688009345\n",
      "Gradient Descent(46/49): loss=0.45017306880053987\n",
      "Gradient Descent(47/49): loss=0.45017306880031177\n",
      "Gradient Descent(48/49): loss=0.45017306880018\n",
      "Gradient Descent(49/49): loss=0.450173068800104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4772155373078527\n",
      "Gradient Descent(2/49): loss=0.46507369713920726\n",
      "Gradient Descent(3/49): loss=0.4586033105133362\n",
      "Gradient Descent(4/49): loss=0.45515524148040953\n",
      "Gradient Descent(5/49): loss=0.45331776549276304\n",
      "Gradient Descent(6/49): loss=0.4523385745389463\n",
      "Gradient Descent(7/49): loss=0.4518167636796573\n",
      "Gradient Descent(8/49): loss=0.451538690672742\n",
      "Gradient Descent(9/49): loss=0.45139050556735694\n",
      "Gradient Descent(10/49): loss=0.4513115377246974\n",
      "Gradient Descent(11/49): loss=0.4512694557613442\n",
      "Gradient Descent(12/49): loss=0.45124703028307306\n",
      "Gradient Descent(13/49): loss=0.4512350797457024\n",
      "Gradient Descent(14/49): loss=0.45122871130433756\n",
      "Gradient Descent(15/49): loss=0.4512253175619344\n",
      "Gradient Descent(16/49): loss=0.45122350903660774\n",
      "Gradient Descent(17/49): loss=0.45122254527346123\n",
      "Gradient Descent(18/49): loss=0.4512220316840802\n",
      "Gradient Descent(19/49): loss=0.4512217579922992\n",
      "Gradient Descent(20/49): loss=0.45122161214194906\n",
      "Gradient Descent(21/49): loss=0.4512215344182974\n",
      "Gradient Descent(22/49): loss=0.4512214929993635\n",
      "Gradient Descent(23/49): loss=0.45122147092721354\n",
      "Gradient Descent(24/49): loss=0.45122145916496476\n",
      "Gradient Descent(25/49): loss=0.4512214528968625\n",
      "Gradient Descent(26/49): loss=0.4512214495565908\n",
      "Gradient Descent(27/49): loss=0.45122144777656\n",
      "Gradient Descent(28/49): loss=0.45122144682798176\n",
      "Gradient Descent(29/49): loss=0.45122144632248423\n",
      "Gradient Descent(30/49): loss=0.4512214460531047\n",
      "Gradient Descent(31/49): loss=0.4512214459095524\n",
      "Gradient Descent(32/49): loss=0.4512214458330532\n",
      "Gradient Descent(33/49): loss=0.4512214457922868\n",
      "Gradient Descent(34/49): loss=0.45122144577056256\n",
      "Gradient Descent(35/49): loss=0.45122144575898565\n",
      "Gradient Descent(36/49): loss=0.4512214457528162\n",
      "Gradient Descent(37/49): loss=0.4512214457495285\n",
      "Gradient Descent(38/49): loss=0.4512214457477767\n",
      "Gradient Descent(39/49): loss=0.45122144574684286\n",
      "Gradient Descent(40/49): loss=0.45122144574634554\n",
      "Gradient Descent(41/49): loss=0.45122144574608025\n",
      "Gradient Descent(42/49): loss=0.451221445745939\n",
      "Gradient Descent(43/49): loss=0.4512214457458637\n",
      "Gradient Descent(44/49): loss=0.4512214457458237\n",
      "Gradient Descent(45/49): loss=0.45122144574580225\n",
      "Gradient Descent(46/49): loss=0.4512214457457908\n",
      "Gradient Descent(47/49): loss=0.45122144574578454\n",
      "Gradient Descent(48/49): loss=0.45122144574578155\n",
      "Gradient Descent(49/49): loss=0.4512214457457798\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47664398069396474\n",
      "Gradient Descent(2/49): loss=0.4641975580057786\n",
      "Gradient Descent(3/49): loss=0.4575648593552441\n",
      "Gradient Descent(4/49): loss=0.4540302942443745\n",
      "Gradient Descent(5/49): loss=0.4521467244967918\n",
      "Gradient Descent(6/49): loss=0.451142970178305\n",
      "Gradient Descent(7/49): loss=0.4506080695019836\n",
      "Gradient Descent(8/49): loss=0.45032302093157195\n",
      "Gradient Descent(9/49): loss=0.45017111854839953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=0.45009016976840677\n",
      "Gradient Descent(11/49): loss=0.45004703216354874\n",
      "Gradient Descent(12/49): loss=0.45002404413392005\n",
      "Gradient Descent(13/49): loss=0.4500117938129308\n",
      "Gradient Descent(14/49): loss=0.4500052656168756\n",
      "Gradient Descent(15/49): loss=0.45000178674119773\n",
      "Gradient Descent(16/49): loss=0.44999993284834916\n",
      "Gradient Descent(17/49): loss=0.4499989449088501\n",
      "Gradient Descent(18/49): loss=0.44999841843589095\n",
      "Gradient Descent(19/49): loss=0.44999813787845117\n",
      "Gradient Descent(20/49): loss=0.4499979883693913\n",
      "Gradient Descent(21/49): loss=0.4499979086960135\n",
      "Gradient Descent(22/49): loss=0.4499978662380702\n",
      "Gradient Descent(23/49): loss=0.4499978436122326\n",
      "Gradient Descent(24/49): loss=0.4499978315549235\n",
      "Gradient Descent(25/49): loss=0.4499978251295835\n",
      "Gradient Descent(26/49): loss=0.44999782170552005\n",
      "Gradient Descent(27/49): loss=0.44999781988083626\n",
      "Gradient Descent(28/49): loss=0.44999781890846247\n",
      "Gradient Descent(29/49): loss=0.4499978183902846\n",
      "Gradient Descent(30/49): loss=0.44999781811414746\n",
      "Gradient Descent(31/49): loss=0.44999781796699395\n",
      "Gradient Descent(32/49): loss=0.44999781788857585\n",
      "Gradient Descent(33/49): loss=0.44999781784678694\n",
      "Gradient Descent(34/49): loss=0.4499978178245176\n",
      "Gradient Descent(35/49): loss=0.4499978178126501\n",
      "Gradient Descent(36/49): loss=0.44999781780632625\n",
      "Gradient Descent(37/49): loss=0.4499978178029561\n",
      "Gradient Descent(38/49): loss=0.4499978178011601\n",
      "Gradient Descent(39/49): loss=0.4499978178002029\n",
      "Gradient Descent(40/49): loss=0.449997817799693\n",
      "Gradient Descent(41/49): loss=0.4499978177994213\n",
      "Gradient Descent(42/49): loss=0.4499978177992763\n",
      "Gradient Descent(43/49): loss=0.44999781779919906\n",
      "Gradient Descent(44/49): loss=0.4499978177991581\n",
      "Gradient Descent(45/49): loss=0.4499978177991362\n",
      "Gradient Descent(46/49): loss=0.44999781779912446\n",
      "Gradient Descent(47/49): loss=0.4499978177991182\n",
      "Gradient Descent(48/49): loss=0.449997817799115\n",
      "Gradient Descent(49/49): loss=0.4499978177991129\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.476914199072\n",
      "Gradient Descent(2/49): loss=0.4646117757574686\n",
      "Gradient Descent(3/49): loss=0.4580558143731551\n",
      "Gradient Descent(4/49): loss=0.4545621425514543\n",
      "Gradient Descent(5/49): loss=0.4527003648376702\n",
      "Gradient Descent(6/49): loss=0.4517082234939942\n",
      "Gradient Descent(7/49): loss=0.45117951137194967\n",
      "Gradient Descent(8/49): loss=0.4508977606821119\n",
      "Gradient Descent(9/49): loss=0.4507476157394975\n",
      "Gradient Descent(10/49): loss=0.45066760349957824\n",
      "Gradient Descent(11/49): loss=0.4506249649769254\n",
      "Gradient Descent(12/49): loss=0.4506022429082035\n",
      "Gradient Descent(13/49): loss=0.4505901343177816\n",
      "Gradient Descent(14/49): loss=0.450583681649946\n",
      "Gradient Descent(15/49): loss=0.450580243023256\n",
      "Gradient Descent(16/49): loss=0.45057841057909337\n",
      "Gradient Descent(17/49): loss=0.4505774340695989\n",
      "Gradient Descent(18/49): loss=0.45057691368768915\n",
      "Gradient Descent(19/49): loss=0.45057663637616935\n",
      "Gradient Descent(20/49): loss=0.4505764885968607\n",
      "Gradient Descent(21/49): loss=0.45057640984526703\n",
      "Gradient Descent(22/49): loss=0.450576367878543\n",
      "Gradient Descent(23/49): loss=0.45057634551447545\n",
      "Gradient Descent(24/49): loss=0.450576333596664\n",
      "Gradient Descent(25/49): loss=0.45057632724566216\n",
      "Gradient Descent(26/49): loss=0.4505763238612133\n",
      "Gradient Descent(27/49): loss=0.4505763220576405\n",
      "Gradient Descent(28/49): loss=0.45057632109651663\n",
      "Gradient Descent(29/49): loss=0.45057632058433383\n",
      "Gradient Descent(30/49): loss=0.4505763203113915\n",
      "Gradient Descent(31/49): loss=0.45057632016594035\n",
      "Gradient Descent(32/49): loss=0.4505763200884297\n",
      "Gradient Descent(33/49): loss=0.4505763200471241\n",
      "Gradient Descent(34/49): loss=0.45057632002511255\n",
      "Gradient Descent(35/49): loss=0.4505763200133825\n",
      "Gradient Descent(36/49): loss=0.45057632000713144\n",
      "Gradient Descent(37/49): loss=0.45057632000380055\n",
      "Gradient Descent(38/49): loss=0.45057632000202513\n",
      "Gradient Descent(39/49): loss=0.45057632000107944\n",
      "Gradient Descent(40/49): loss=0.4505763200005753\n",
      "Gradient Descent(41/49): loss=0.45057632000030645\n",
      "Gradient Descent(42/49): loss=0.4505763200001633\n",
      "Gradient Descent(43/49): loss=0.4505763200000871\n",
      "Gradient Descent(44/49): loss=0.4505763200000464\n",
      "Gradient Descent(45/49): loss=0.45057632000002457\n",
      "Gradient Descent(46/49): loss=0.450576320000013\n",
      "Gradient Descent(47/49): loss=0.4505763200000073\n",
      "Gradient Descent(48/49): loss=0.45057632000000364\n",
      "Gradient Descent(49/49): loss=0.4505763200000019\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47672584043647986\n",
      "Gradient Descent(2/49): loss=0.4643230408050801\n",
      "Gradient Descent(3/49): loss=0.45771358888150704\n",
      "Gradient Descent(4/49): loss=0.4541914119514351\n",
      "Gradient Descent(5/49): loss=0.45231444386539965\n",
      "Gradient Descent(6/49): loss=0.45131420757235147\n",
      "Gradient Descent(7/49): loss=0.450781181651786\n",
      "Gradient Descent(8/49): loss=0.45049713213871684\n",
      "Gradient Descent(9/49): loss=0.45034576215320204\n",
      "Gradient Descent(10/49): loss=0.4502650970879214\n",
      "Gradient Descent(11/49): loss=0.45022211067463347\n",
      "Gradient Descent(12/49): loss=0.45019920321499207\n",
      "Gradient Descent(13/49): loss=0.4501869958297493\n",
      "Gradient Descent(14/49): loss=0.4501804905141535\n",
      "Gradient Descent(15/49): loss=0.4501770238314723\n",
      "Gradient Descent(16/49): loss=0.45017517643627153\n",
      "Gradient Descent(17/49): loss=0.4501741919593691\n",
      "Gradient Descent(18/49): loss=0.4501736673316278\n",
      "Gradient Descent(19/49): loss=0.45017338775750426\n",
      "Gradient Descent(20/49): loss=0.45017323877245413\n",
      "Gradient Descent(21/49): loss=0.4501731593783208\n",
      "Gradient Descent(22/49): loss=0.4501731170691871\n",
      "Gradient Descent(23/49): loss=0.45017309452265\n",
      "Gradient Descent(24/49): loss=0.45017308250760013\n",
      "Gradient Descent(25/49): loss=0.45017307610478013\n",
      "Gradient Descent(26/49): loss=0.45017307269271745\n",
      "Gradient Descent(27/49): loss=0.45017307087442915\n",
      "Gradient Descent(28/49): loss=0.4501730699054633\n",
      "Gradient Descent(29/49): loss=0.45017306938910134\n",
      "Gradient Descent(30/49): loss=0.4501730691139322\n",
      "Gradient Descent(31/49): loss=0.4501730689672944\n",
      "Gradient Descent(32/49): loss=0.45017306888915115\n",
      "Gradient Descent(33/49): loss=0.45017306884750874\n",
      "Gradient Descent(34/49): loss=0.4501730688253173\n",
      "Gradient Descent(35/49): loss=0.4501730688134918\n",
      "Gradient Descent(36/49): loss=0.4501730688071896\n",
      "Gradient Descent(37/49): loss=0.45017306880383157\n",
      "Gradient Descent(38/49): loss=0.4501730688020418\n",
      "Gradient Descent(39/49): loss=0.4501730688010879\n",
      "Gradient Descent(40/49): loss=0.45017306880057983\n",
      "Gradient Descent(41/49): loss=0.4501730688003091\n",
      "Gradient Descent(42/49): loss=0.4501730688001647\n",
      "Gradient Descent(43/49): loss=0.4501730688000878\n",
      "Gradient Descent(44/49): loss=0.4501730688000467\n",
      "Gradient Descent(45/49): loss=0.4501730688000248\n",
      "Gradient Descent(46/49): loss=0.4501730688000133\n",
      "Gradient Descent(47/49): loss=0.450173068800007\n",
      "Gradient Descent(48/49): loss=0.45017306880000396\n",
      "Gradient Descent(49/49): loss=0.45017306880000196\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4751229373303467\n",
      "Gradient Descent(2/49): loss=0.4629331766222166\n",
      "Gradient Descent(3/49): loss=0.4569601938752328\n",
      "Gradient Descent(4/49): loss=0.4540334323292107\n",
      "Gradient Descent(5/49): loss=0.4525993191716598\n",
      "Gradient Descent(6/49): loss=0.45189660372445983\n",
      "Gradient Descent(7/49): loss=0.45155227315533225\n",
      "Gradient Descent(8/49): loss=0.45138355117645923\n",
      "Gradient Descent(9/49): loss=0.45130087740681174\n",
      "Gradient Descent(10/49): loss=0.4512603672596843\n",
      "Gradient Descent(11/49): loss=0.45124051728759207\n",
      "Gradient Descent(12/49): loss=0.4512307908012666\n",
      "Gradient Descent(13/49): loss=0.45122602482296736\n",
      "Gradient Descent(14/49): loss=0.45122368949360075\n",
      "Gradient Descent(15/49): loss=0.45122254518221083\n",
      "Gradient Descent(16/49): loss=0.4512219844696302\n",
      "Gradient Descent(17/49): loss=0.45122170972046544\n",
      "Gradient Descent(18/49): loss=0.45122157509337457\n",
      "Gradient Descent(19/49): loss=0.4512215091261002\n",
      "Gradient Descent(20/49): loss=0.4512214768021358\n",
      "Gradient Descent(21/49): loss=0.4512214609633931\n",
      "Gradient Descent(22/49): loss=0.4512214532024093\n",
      "Gradient Descent(23/49): loss=0.4512214493995272\n",
      "Gradient Descent(24/49): loss=0.4512214475361151\n",
      "Gradient Descent(25/49): loss=0.45122144662304314\n",
      "Gradient Descent(26/49): loss=0.45122144617563775\n",
      "Gradient Descent(27/49): loss=0.45122144595640934\n",
      "Gradient Descent(28/49): loss=0.45122144584898705\n",
      "Gradient Descent(29/49): loss=0.4512214457963503\n",
      "Gradient Descent(30/49): loss=0.4512214457705581\n",
      "Gradient Descent(31/49): loss=0.45122144575792045\n",
      "Gradient Descent(32/49): loss=0.45122144575172757\n",
      "Gradient Descent(33/49): loss=0.45122144574869333\n",
      "Gradient Descent(34/49): loss=0.45122144574720624\n",
      "Gradient Descent(35/49): loss=0.4512214457464777\n",
      "Gradient Descent(36/49): loss=0.45122144574612066\n",
      "Gradient Descent(37/49): loss=0.45122144574594575\n",
      "Gradient Descent(38/49): loss=0.45122144574586\n",
      "Gradient Descent(39/49): loss=0.45122144574581824\n",
      "Gradient Descent(40/49): loss=0.4512214457457974\n",
      "Gradient Descent(41/49): loss=0.45122144574578743\n",
      "Gradient Descent(42/49): loss=0.4512214457457824\n",
      "Gradient Descent(43/49): loss=0.45122144574578016\n",
      "Gradient Descent(44/49): loss=0.45122144574577905\n",
      "Gradient Descent(45/49): loss=0.4512214457457783\n",
      "Gradient Descent(46/49): loss=0.4512214457457781\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.45122144574577794\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4744988870775466\n",
      "Gradient Descent(2/49): loss=0.4620033417455443\n",
      "Gradient Descent(3/49): loss=0.4558805245328633\n",
      "Gradient Descent(4/49): loss=0.45288034409865\n",
      "Gradient Descent(5/49): loss=0.4514102556858852\n",
      "Gradient Descent(6/49): loss=0.45068991236363043\n",
      "Gradient Descent(7/49): loss=0.45033694413572567\n",
      "Gradient Descent(8/49): loss=0.4501639897040522\n",
      "Gradient Descent(9/49): loss=0.45007924203253225\n",
      "Gradient Descent(10/49): loss=0.45003771567348755\n",
      "Gradient Descent(11/49): loss=0.45001736775755574\n",
      "Gradient Descent(12/49): loss=0.45000739727874883\n",
      "Gradient Descent(13/49): loss=0.45000251174413364\n",
      "Gradient Descent(14/49): loss=0.4500001178321722\n",
      "Gradient Descent(15/49): loss=0.4499989448153112\n",
      "Gradient Descent(16/49): loss=0.4499983700370491\n",
      "Gradient Descent(17/49): loss=0.44999808839570066\n",
      "Gradient Descent(18/49): loss=0.44999795039144\n",
      "Gradient Descent(19/49): loss=0.44999788276935243\n",
      "Gradient Descent(20/49): loss=0.4499978496345293\n",
      "Gradient Descent(21/49): loss=0.449997833398466\n",
      "Gradient Descent(22/49): loss=0.44999782544279504\n",
      "Gradient Descent(23/49): loss=0.44999782154451623\n",
      "Gradient Descent(24/49): loss=0.4499978196343594\n",
      "Gradient Descent(25/49): loss=0.4499978186983828\n",
      "Gradient Descent(26/49): loss=0.4499978182397543\n",
      "Gradient Descent(27/49): loss=0.4499978180150263\n",
      "Gradient Descent(28/49): loss=0.4499978179049095\n",
      "Gradient Descent(29/49): loss=0.44999781785095216\n",
      "Gradient Descent(30/49): loss=0.44999781782451337\n",
      "Gradient Descent(31/49): loss=0.44999781781155823\n",
      "Gradient Descent(32/49): loss=0.4499978178052103\n",
      "Gradient Descent(33/49): loss=0.4499978178020995\n",
      "Gradient Descent(34/49): loss=0.44999781780057574\n",
      "Gradient Descent(35/49): loss=0.4499978177998286\n",
      "Gradient Descent(36/49): loss=0.4499978177994626\n",
      "Gradient Descent(37/49): loss=0.4499978177992835\n",
      "Gradient Descent(38/49): loss=0.44999781779919557\n",
      "Gradient Descent(39/49): loss=0.4499978177991523\n",
      "Gradient Descent(40/49): loss=0.44999781779913145\n",
      "Gradient Descent(41/49): loss=0.4499978177991211\n",
      "Gradient Descent(42/49): loss=0.44999781779911585\n",
      "Gradient Descent(43/49): loss=0.4499978177991133\n",
      "Gradient Descent(44/49): loss=0.44999781779911224\n",
      "Gradient Descent(45/49): loss=0.44999781779911174\n",
      "Gradient Descent(46/49): loss=0.44999781779911147\n",
      "Gradient Descent(47/49): loss=0.44999781779911135\n",
      "Gradient Descent(48/49): loss=0.4499978177991112\n",
      "Gradient Descent(49/49): loss=0.4499978177991109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4747939232000001\n",
      "Gradient Descent(2/49): loss=0.462442945568\n",
      "Gradient Descent(3/49): loss=0.45639096652831984\n",
      "Gradient Descent(4/49): loss=0.45342549679887706\n",
      "Gradient Descent(5/49): loss=0.4519724166314497\n",
      "Gradient Descent(6/49): loss=0.45126040734941025\n",
      "Gradient Descent(7/49): loss=0.45091152280121116\n",
      "Gradient Descent(8/49): loss=0.45074056937259327\n",
      "Gradient Descent(9/49): loss=0.4506568021925707\n",
      "Gradient Descent(10/49): loss=0.4506157562743598\n",
      "Gradient Descent(11/49): loss=0.45059564377443634\n",
      "Gradient Descent(12/49): loss=0.4505857886494738\n",
      "Gradient Descent(13/49): loss=0.4505809596382423\n",
      "Gradient Descent(14/49): loss=0.45057859342273876\n",
      "Gradient Descent(15/49): loss=0.45057743397714206\n",
      "Gradient Descent(16/49): loss=0.4505768658487997\n",
      "Gradient Descent(17/49): loss=0.4505765874659116\n",
      "Gradient Descent(18/49): loss=0.4505764510582968\n",
      "Gradient Descent(19/49): loss=0.45057638421856533\n",
      "Gradient Descent(20/49): loss=0.45057635146709696\n",
      "Gradient Descent(21/49): loss=0.45057633541887765\n",
      "Gradient Descent(22/49): loss=0.4505763275552499\n",
      "Gradient Descent(23/49): loss=0.45057632370207257\n",
      "Gradient Descent(24/49): loss=0.4505763218140155\n",
      "Gradient Descent(25/49): loss=0.45057632088886734\n",
      "Gradient Descent(26/49): loss=0.4505763204355452\n",
      "Gradient Descent(27/49): loss=0.45057632021341715\n",
      "Gradient Descent(28/49): loss=0.4505763201045744\n",
      "Gradient Descent(29/49): loss=0.45057632005124143\n",
      "Gradient Descent(30/49): loss=0.45057632002510845\n",
      "Gradient Descent(31/49): loss=0.45057632001230324\n",
      "Gradient Descent(32/49): loss=0.45057632000602854\n",
      "Gradient Descent(33/49): loss=0.450576320002954\n",
      "Gradient Descent(34/49): loss=0.45057632000144743\n",
      "Gradient Descent(35/49): loss=0.4505763200007092\n",
      "Gradient Descent(36/49): loss=0.4505763200003476\n",
      "Gradient Descent(37/49): loss=0.4505763200001703\n",
      "Gradient Descent(38/49): loss=0.45057632000008335\n",
      "Gradient Descent(39/49): loss=0.4505763200000409\n",
      "Gradient Descent(40/49): loss=0.45057632000002\n",
      "Gradient Descent(41/49): loss=0.45057632000000986\n",
      "Gradient Descent(42/49): loss=0.45057632000000486\n",
      "Gradient Descent(43/49): loss=0.45057632000000214\n",
      "Gradient Descent(44/49): loss=0.4505763200000011\n",
      "Gradient Descent(45/49): loss=0.45057632000000053\n",
      "Gradient Descent(46/49): loss=0.45057632000000036\n",
      "Gradient Descent(47/49): loss=0.45057632000000014\n",
      "Gradient Descent(48/49): loss=0.45057632000000014\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47458826508799995\n",
      "Gradient Descent(2/49): loss=0.4621365149811199\n",
      "Gradient Descent(3/49): loss=0.4560351574287486\n",
      "Gradient Descent(4/49): loss=0.4530454922280869\n",
      "Gradient Descent(5/49): loss=0.45158055627976257\n",
      "Gradient Descent(6/49): loss=0.45086273766508356\n",
      "Gradient Descent(7/49): loss=0.4505110065438908\n",
      "Gradient Descent(8/49): loss=0.4503386582945067\n",
      "Gradient Descent(9/49): loss=0.45025420765230845\n",
      "Gradient Descent(10/49): loss=0.4502128268376309\n",
      "Gradient Descent(11/49): loss=0.4501925502384392\n",
      "Gradient Descent(12/49): loss=0.4501826147048353\n",
      "Gradient Descent(13/49): loss=0.4501777462933692\n",
      "Gradient Descent(14/49): loss=0.4501753607717508\n",
      "Gradient Descent(15/49): loss=0.4501741918661578\n",
      "Gradient Descent(16/49): loss=0.4501736191024173\n",
      "Gradient Descent(17/49): loss=0.45017333844818463\n",
      "Gradient Descent(18/49): loss=0.45017320092761043\n",
      "Gradient Descent(19/49): loss=0.45017313354252897\n",
      "Gradient Descent(20/49): loss=0.45017310052383946\n",
      "Gradient Descent(21/49): loss=0.4501730843446811\n",
      "Gradient Descent(22/49): loss=0.4501730764168938\n",
      "Gradient Descent(23/49): loss=0.450173072532278\n",
      "Gradient Descent(24/49): loss=0.4501730706288163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=0.4501730696961201\n",
      "Gradient Descent(26/49): loss=0.4501730692390987\n",
      "Gradient Descent(27/49): loss=0.45017306901515836\n",
      "Gradient Descent(28/49): loss=0.4501730689054276\n",
      "Gradient Descent(29/49): loss=0.4501730688516593\n",
      "Gradient Descent(30/49): loss=0.4501730688253132\n",
      "Gradient Descent(31/49): loss=0.45017306881240327\n",
      "Gradient Descent(32/49): loss=0.4501730688060779\n",
      "Gradient Descent(33/49): loss=0.4501730688029781\n",
      "Gradient Descent(34/49): loss=0.45017306880145924\n",
      "Gradient Descent(35/49): loss=0.4501730688007151\n",
      "Gradient Descent(36/49): loss=0.4501730688003505\n",
      "Gradient Descent(37/49): loss=0.45017306880017166\n",
      "Gradient Descent(38/49): loss=0.45017306880008423\n",
      "Gradient Descent(39/49): loss=0.4501730688000412\n",
      "Gradient Descent(40/49): loss=0.45017306880002\n",
      "Gradient Descent(41/49): loss=0.45017306880001\n",
      "Gradient Descent(42/49): loss=0.45017306880000474\n",
      "Gradient Descent(43/49): loss=0.4501730688000026\n",
      "Gradient Descent(44/49): loss=0.45017306880000124\n",
      "Gradient Descent(45/49): loss=0.45017306880000063\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688000001\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4731181387504982\n",
      "Gradient Descent(2/49): loss=0.46105087123559657\n",
      "Gradient Descent(3/49): loss=0.4556338748481574\n",
      "Gradient Descent(4/49): loss=0.45320218516983596\n",
      "Gradient Descent(5/49): loss=0.4521105996732374\n",
      "Gradient Descent(6/49): loss=0.45162058694381435\n",
      "Gradient Descent(7/49): loss=0.4514006202295765\n",
      "Gradient Descent(8/49): loss=0.4513018771715551\n",
      "Gradient Descent(9/49): loss=0.45125755141280927\n",
      "Gradient Descent(10/49): loss=0.45123765357970835\n",
      "Gradient Descent(11/49): loss=0.4512287214424291\n",
      "Gradient Descent(12/49): loss=0.45122471180600454\n",
      "Gradient Descent(13/49): loss=0.45122291188021346\n",
      "Gradient Descent(14/49): loss=0.45122210389352596\n",
      "Gradient Descent(15/49): loss=0.45122174118830194\n",
      "Gradient Descent(16/49): loss=0.4512215783699267\n",
      "Gradient Descent(17/49): loss=0.4512215052807584\n",
      "Gradient Descent(18/49): loss=0.45122147247103045\n",
      "Gradient Descent(19/49): loss=0.45122145774274375\n",
      "Gradient Descent(20/49): loss=0.4512214511312157\n",
      "Gradient Descent(21/49): loss=0.45122144816330073\n",
      "Gradient Descent(22/49): loss=0.45122144683100374\n",
      "Gradient Descent(23/49): loss=0.451221446232936\n",
      "Gradient Descent(24/49): loss=0.45122144596446306\n",
      "Gradient Descent(25/49): loss=0.45122144584394563\n",
      "Gradient Descent(26/49): loss=0.4512214457898452\n",
      "Gradient Descent(27/49): loss=0.4512214457655597\n",
      "Gradient Descent(28/49): loss=0.45122144575465784\n",
      "Gradient Descent(29/49): loss=0.45122144574976425\n",
      "Gradient Descent(30/49): loss=0.4512214457475673\n",
      "Gradient Descent(31/49): loss=0.4512214457465811\n",
      "Gradient Descent(32/49): loss=0.45122144574613854\n",
      "Gradient Descent(33/49): loss=0.45122144574593953\n",
      "Gradient Descent(34/49): loss=0.45122144574585044\n",
      "Gradient Descent(35/49): loss=0.4512214457458104\n",
      "Gradient Descent(36/49): loss=0.4512214457457925\n",
      "Gradient Descent(37/49): loss=0.4512214457457843\n",
      "Gradient Descent(38/49): loss=0.45122144574578066\n",
      "Gradient Descent(39/49): loss=0.4512214457457792\n",
      "Gradient Descent(40/49): loss=0.45122144574577844\n",
      "Gradient Descent(41/49): loss=0.4512214457457781\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.45122144574577766\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.45122144574577755\n",
      "Gradient Descent(48/49): loss=0.45122144574577766\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47244379738909004\n",
      "Gradient Descent(2/49): loss=0.4600738180370528\n",
      "Gradient Descent(3/49): loss=0.45452093430592316\n",
      "Gradient Descent(4/49): loss=0.45202824479901915\n",
      "Gradient Descent(5/49): loss=0.45090927647936957\n",
      "Gradient Descent(6/49): loss=0.45040697160067944\n",
      "Gradient Descent(7/49): loss=0.45018148694063503\n",
      "Gradient Descent(8/49): loss=0.45008026687674124\n",
      "Gradient Descent(9/49): loss=0.4500348291900593\n",
      "Gradient Descent(10/49): loss=0.4500144322125079\n",
      "Gradient Descent(11/49): loss=0.45000527600928475\n",
      "Gradient Descent(12/49): loss=0.45000116578965804\n",
      "Gradient Descent(13/49): loss=0.4499993207120677\n",
      "Gradient Descent(14/49): loss=0.44999849245673723\n",
      "Gradient Descent(15/49): loss=0.44999812065291955\n",
      "Gradient Descent(16/49): loss=0.44999795375018575\n",
      "Gradient Descent(17/49): loss=0.44999787882754844\n",
      "Gradient Descent(18/49): loss=0.44999784519477676\n",
      "Gradient Descent(19/49): loss=0.4499978300970253\n",
      "Gradient Descent(20/49): loss=0.4499978233196447\n",
      "Gradient Descent(21/49): loss=0.4499978202772788\n",
      "Gradient Descent(22/49): loss=0.44999781891156065\n",
      "Gradient Descent(23/49): loss=0.4499978182984896\n",
      "Gradient Descent(24/49): loss=0.44999781802328204\n",
      "Gradient Descent(25/49): loss=0.44999781789974147\n",
      "Gradient Descent(26/49): loss=0.44999781784428416\n",
      "Gradient Descent(27/49): loss=0.44999781781938936\n",
      "Gradient Descent(28/49): loss=0.44999781780821396\n",
      "Gradient Descent(29/49): loss=0.44999781780319714\n",
      "Gradient Descent(30/49): loss=0.4499978178009452\n",
      "Gradient Descent(31/49): loss=0.4499978177999345\n",
      "Gradient Descent(32/49): loss=0.44999781779948095\n",
      "Gradient Descent(33/49): loss=0.449997817799277\n",
      "Gradient Descent(34/49): loss=0.44999781779918563\n",
      "Gradient Descent(35/49): loss=0.4499978177991446\n",
      "Gradient Descent(36/49): loss=0.4499978177991261\n",
      "Gradient Descent(37/49): loss=0.44999781779911796\n",
      "Gradient Descent(38/49): loss=0.449997817799114\n",
      "Gradient Descent(39/49): loss=0.4499978177991125\n",
      "Gradient Descent(40/49): loss=0.4499978177991117\n",
      "Gradient Descent(41/49): loss=0.44999781779911135\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911097\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.4499978177991111\n",
      "Gradient Descent(47/49): loss=0.4499978177991112\n",
      "Gradient Descent(48/49): loss=0.44999781779911097\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47276260995200015\n",
      "Gradient Descent(2/49): loss=0.4605357455594526\n",
      "Gradient Descent(3/49): loss=0.45504710613363797\n",
      "Gradient Descent(4/49): loss=0.45258325589539017\n",
      "Gradient Descent(5/49): loss=0.4514772335234407\n",
      "Gradient Descent(6/49): loss=0.45098074008067246\n",
      "Gradient Descent(7/49): loss=0.4507578641742139\n",
      "Gradient Descent(8/49): loss=0.45065781517980474\n",
      "Gradient Descent(9/49): loss=0.45061290318621433\n",
      "Gradient Descent(10/49): loss=0.45059274219229156\n",
      "Gradient Descent(11/49): loss=0.4505836919221196\n",
      "Gradient Descent(12/49): loss=0.45057962925583944\n",
      "Gradient Descent(13/49): loss=0.45057780552494636\n",
      "Gradient Descent(14/49): loss=0.4505769868521484\n",
      "Gradient Descent(15/49): loss=0.4505766193499295\n",
      "Gradient Descent(16/49): loss=0.45057645437818333\n",
      "Gradient Descent(17/49): loss=0.45057638032236647\n",
      "Gradient Descent(18/49): loss=0.4505763470787104\n",
      "Gradient Descent(19/49): loss=0.4505763321556331\n",
      "Gradient Descent(20/49): loss=0.4505763254566637\n",
      "Gradient Descent(21/49): loss=0.45057632244949647\n",
      "Gradient Descent(22/49): loss=0.450576321099579\n",
      "Gradient Descent(23/49): loss=0.45057632049360113\n",
      "Gradient Descent(24/49): loss=0.45057632022157756\n",
      "Gradient Descent(25/49): loss=0.450576320099466\n",
      "Gradient Descent(26/49): loss=0.45057632004465037\n",
      "Gradient Descent(27/49): loss=0.4505763200200435\n",
      "Gradient Descent(28/49): loss=0.45057632000899767\n",
      "Gradient Descent(29/49): loss=0.4505763200040391\n",
      "Gradient Descent(30/49): loss=0.45057632000181314\n",
      "Gradient Descent(31/49): loss=0.45057632000081405\n",
      "Gradient Descent(32/49): loss=0.45057632000036535\n",
      "Gradient Descent(33/49): loss=0.45057632000016407\n",
      "Gradient Descent(34/49): loss=0.4505763200000736\n",
      "Gradient Descent(35/49): loss=0.45057632000003295\n",
      "Gradient Descent(36/49): loss=0.450576320000015\n",
      "Gradient Descent(37/49): loss=0.45057632000000664\n",
      "Gradient Descent(38/49): loss=0.45057632000000286\n",
      "Gradient Descent(39/49): loss=0.45057632000000125\n",
      "Gradient Descent(40/49): loss=0.4505763200000006\n",
      "Gradient Descent(41/49): loss=0.4505763200000003\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.4505763199999998\n",
      "Gradient Descent(44/49): loss=0.4505763200000002\n",
      "Gradient Descent(45/49): loss=0.4505763200000001\n",
      "Gradient Descent(46/49): loss=0.45057632000000014\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.4505763200000001\n",
      "Gradient Descent(49/49): loss=0.45057632000000014\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4725403782156799\n",
      "Gradient Descent(2/49): loss=0.46021375399669867\n",
      "Gradient Descent(3/49): loss=0.4546803323847979\n",
      "Gradient Descent(4/49): loss=0.4521963794232159\n",
      "Gradient Descent(5/49): loss=0.4510813329387617\n",
      "Gradient Descent(6/49): loss=0.45058078857189\n",
      "Gradient Descent(7/49): loss=0.45035609420560135\n",
      "Gradient Descent(8/49): loss=0.45025522890457426\n",
      "Gradient Descent(9/49): loss=0.4502099504709437\n",
      "Gradient Descent(10/49): loss=0.4501896249820865\n",
      "Gradient Descent(11/49): loss=0.4501805008701388\n",
      "Gradient Descent(12/49): loss=0.45017640505628514\n",
      "Gradient Descent(13/49): loss=0.4501745664454463\n",
      "Gradient Descent(14/49): loss=0.4501737410930408\n",
      "Gradient Descent(15/49): loss=0.4501733705923462\n",
      "Gradient Descent(16/49): loss=0.45017320427458424\n",
      "Gradient Descent(17/49): loss=0.4501731296145409\n",
      "Gradient Descent(18/49): loss=0.45017309609964723\n",
      "Gradient Descent(19/49): loss=0.4501730810548117\n",
      "Gradient Descent(20/49): loss=0.45017307430118503\n",
      "Gradient Descent(21/49): loss=0.4501730712694819\n",
      "Gradient Descent(22/49): loss=0.45017306990855055\n",
      "Gradient Descent(23/49): loss=0.45017306929762835\n",
      "Gradient Descent(24/49): loss=0.4501730690233854\n",
      "Gradient Descent(25/49): loss=0.45017306890027786\n",
      "Gradient Descent(26/49): loss=0.45017306884501485\n",
      "Gradient Descent(27/49): loss=0.4501730688202071\n",
      "Gradient Descent(28/49): loss=0.450173068809071\n",
      "Gradient Descent(29/49): loss=0.45017306880407204\n",
      "Gradient Descent(30/49): loss=0.4501730688018279\n",
      "Gradient Descent(31/49): loss=0.4501730688008204\n",
      "Gradient Descent(32/49): loss=0.45017306880036845\n",
      "Gradient Descent(33/49): loss=0.4501730688001652\n",
      "Gradient Descent(34/49): loss=0.4501730688000742\n",
      "Gradient Descent(35/49): loss=0.4501730688000333\n",
      "Gradient Descent(36/49): loss=0.450173068800015\n",
      "Gradient Descent(37/49): loss=0.4501730688000066\n",
      "Gradient Descent(38/49): loss=0.4501730688000032\n",
      "Gradient Descent(39/49): loss=0.45017306880000124\n",
      "Gradient Descent(40/49): loss=0.4501730688000007\n",
      "Gradient Descent(41/49): loss=0.4501730688000001\n",
      "Gradient Descent(42/49): loss=0.45017306880000024\n",
      "Gradient Descent(43/49): loss=0.45017306879999985\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.45017306879999985\n",
      "Gradient Descent(46/49): loss=0.45017306879999985\n",
      "Gradient Descent(47/49): loss=0.45017306879999985\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.45017306879999985\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47120114156830717\n",
      "Gradient Descent(2/49): loss=0.4594051291546859\n",
      "Gradient Descent(3/49): loss=0.4545734824700665\n",
      "Gradient Descent(4/49): loss=0.4525944399880463\n",
      "Gradient Descent(5/49): loss=0.451783824187411\n",
      "Gradient Descent(6/49): loss=0.45145179595547075\n",
      "Gradient Descent(7/49): loss=0.451315797191668\n",
      "Gradient Descent(8/49): loss=0.45126009209801443\n",
      "Gradient Descent(9/49): loss=0.4512372752916538\n",
      "Gradient Descent(10/49): loss=0.4512279295277686\n",
      "Gradient Descent(11/49): loss=0.4512241015028812\n",
      "Gradient Descent(12/49): loss=0.45122253354388747\n",
      "Gradient Descent(13/49): loss=0.4512218913078835\n",
      "Gradient Descent(14/49): loss=0.45122162824801626\n",
      "Gradient Descent(15/49): loss=0.4512215204986945\n",
      "Gradient Descent(16/49): loss=0.45122147636457244\n",
      "Gradient Descent(17/49): loss=0.4512214582872362\n",
      "Gradient Descent(18/49): loss=0.4512214508827591\n",
      "Gradient Descent(19/49): loss=0.4512214478498854\n",
      "Gradient Descent(20/49): loss=0.4512214466076202\n",
      "Gradient Descent(21/49): loss=0.4512214460987884\n",
      "Gradient Descent(22/49): loss=0.45122144589037105\n",
      "Gradient Descent(23/49): loss=0.4512214458050032\n",
      "Gradient Descent(24/49): loss=0.4512214457700366\n",
      "Gradient Descent(25/49): loss=0.451221445755714\n",
      "Gradient Descent(26/49): loss=0.4512214457498477\n",
      "Gradient Descent(27/49): loss=0.4512214457474449\n",
      "Gradient Descent(28/49): loss=0.45122144574646067\n",
      "Gradient Descent(29/49): loss=0.45122144574605744\n",
      "Gradient Descent(30/49): loss=0.4512214457458922\n",
      "Gradient Descent(31/49): loss=0.45122144574582473\n",
      "Gradient Descent(32/49): loss=0.451221445745797\n",
      "Gradient Descent(33/49): loss=0.45122144574578565\n",
      "Gradient Descent(34/49): loss=0.45122144574578105\n",
      "Gradient Descent(35/49): loss=0.4512214457457791\n",
      "Gradient Descent(36/49): loss=0.4512214457457783\n",
      "Gradient Descent(37/49): loss=0.45122144574577794\n",
      "Gradient Descent(38/49): loss=0.45122144574577794\n",
      "Gradient Descent(39/49): loss=0.45122144574577766\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4704787116285951\n",
      "Gradient Descent(2/49): loss=0.4583867919116682\n",
      "Gradient Descent(3/49): loss=0.4534339415956143\n",
      "Gradient Descent(4/49): loss=0.4514052541061588\n",
      "Gradient Descent(5/49): loss=0.4505743037104778\n",
      "Gradient Descent(6/49): loss=0.4502339464284071\n",
      "Gradient Descent(7/49): loss=0.4500945360856707\n",
      "Gradient Descent(8/49): loss=0.45003743360928594\n",
      "Gradient Descent(9/49): loss=0.4500140444349585\n",
      "Gradient Descent(10/49): loss=0.4500044642291543\n",
      "Gradient Descent(11/49): loss=0.45000054017685687\n",
      "Gradient Descent(12/49): loss=0.4499989328850358\n",
      "Gradient Descent(13/49): loss=0.4499982745383058\n",
      "Gradient Descent(14/49): loss=0.4499980048794855\n",
      "Gradient Descent(15/49): loss=0.4499978944272324\n",
      "Gradient Descent(16/49): loss=0.44999784918598945\n",
      "Gradient Descent(17/49): loss=0.4499978306551767\n",
      "Gradient Descent(18/49): loss=0.4499978230649555\n",
      "Gradient Descent(19/49): loss=0.44999781995600097\n",
      "Gradient Descent(20/49): loss=0.44999781868257327\n",
      "Gradient Descent(21/49): loss=0.4499978181609773\n",
      "Gradient Descent(22/49): loss=0.4499978179473313\n",
      "Gradient Descent(23/49): loss=0.4499978178598222\n",
      "Gradient Descent(24/49): loss=0.4499978178239785\n",
      "Gradient Descent(25/49): loss=0.4499978178092969\n",
      "Gradient Descent(26/49): loss=0.44999781780328313\n",
      "Gradient Descent(27/49): loss=0.44999781780081993\n",
      "Gradient Descent(28/49): loss=0.4499978177998112\n",
      "Gradient Descent(29/49): loss=0.44999781779939774\n",
      "Gradient Descent(30/49): loss=0.4499978177992284\n",
      "Gradient Descent(31/49): loss=0.44999781779915915\n",
      "Gradient Descent(32/49): loss=0.4499978177991309\n",
      "Gradient Descent(33/49): loss=0.4499978177991191\n",
      "Gradient Descent(34/49): loss=0.4499978177991143\n",
      "Gradient Descent(35/49): loss=0.4499978177991125\n",
      "Gradient Descent(36/49): loss=0.4499978177991117\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911097\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.4499978177991111\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4708202593279999\n",
      "Gradient Descent(2/49): loss=0.4588682375487485\n",
      "Gradient Descent(3/49): loss=0.4539726894279675\n",
      "Gradient Descent(4/49): loss=0.4519674729176954\n",
      "Gradient Descent(5/49): loss=0.45114613623508804\n",
      "Gradient Descent(6/49): loss=0.450809716729892\n",
      "Gradient Descent(7/49): loss=0.4506719193005637\n",
      "Gradient Descent(8/49): loss=0.45061547747351083\n",
      "Gradient Descent(9/49): loss=0.4505923589011501\n",
      "Gradient Descent(10/49): loss=0.45058288953391107\n",
      "Gradient Descent(11/49): loss=0.45057901088109015\n",
      "Gradient Descent(12/49): loss=0.4505774221848945\n",
      "Gradient Descent(13/49): loss=0.4505767714549327\n",
      "Gradient Descent(14/49): loss=0.45057650491594037\n",
      "Gradient Descent(15/49): loss=0.4505763957415693\n",
      "Gradient Descent(16/49): loss=0.4505763510237467\n",
      "Gradient Descent(17/49): loss=0.45057633270732655\n",
      "Gradient Descent(18/49): loss=0.450576325204921\n",
      "Gradient Descent(19/49): loss=0.4505763221319355\n",
      "Gradient Descent(20/49): loss=0.4505763208732408\n",
      "Gradient Descent(21/49): loss=0.4505763203576794\n",
      "Gradient Descent(22/49): loss=0.4505763201465054\n",
      "Gradient Descent(23/49): loss=0.4505763200600087\n",
      "Gradient Descent(24/49): loss=0.45057632002457954\n",
      "Gradient Descent(25/49): loss=0.4505763200100679\n",
      "Gradient Descent(26/49): loss=0.45057632000412373\n",
      "Gradient Descent(27/49): loss=0.4505763200016892\n",
      "Gradient Descent(28/49): loss=0.45057632000069175\n",
      "Gradient Descent(29/49): loss=0.45057632000028347\n",
      "Gradient Descent(30/49): loss=0.45057632000011616\n",
      "Gradient Descent(31/49): loss=0.45057632000004755\n",
      "Gradient Descent(32/49): loss=0.45057632000001946\n",
      "Gradient Descent(33/49): loss=0.4505763200000081\n",
      "Gradient Descent(34/49): loss=0.4505763200000031\n",
      "Gradient Descent(35/49): loss=0.45057632000000136\n",
      "Gradient Descent(36/49): loss=0.4505763200000007\n",
      "Gradient Descent(37/49): loss=0.4505763200000003\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.45057632\n",
      "Gradient Descent(42/49): loss=0.45057632000000014\n",
      "Gradient Descent(43/49): loss=0.45057632\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.45057632\n",
      "Gradient Descent(47/49): loss=0.4505763200000001\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999998\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47058217981952\n",
      "Gradient Descent(2/49): loss=0.4585326406735952\n",
      "Gradient Descent(3/49): loss=0.45359714943942475\n",
      "Gradient Descent(4/49): loss=0.45157557222990846\n",
      "Gradient Descent(5/49): loss=0.45074753420489033\n",
      "Gradient Descent(6/49): loss=0.45040836982984306\n",
      "Gradient Descent(7/49): loss=0.4502694481018238\n",
      "Gradient Descent(8/49): loss=0.450212545762027\n",
      "Gradient Descent(9/49): loss=0.45018923856364623\n",
      "Gradient Descent(10/49): loss=0.4501796919351895\n",
      "Gradient Descent(11/49): loss=0.45017578163617344\n",
      "Gradient Descent(12/49): loss=0.45017417997769654\n",
      "Gradient Descent(13/49): loss=0.45017352393838445\n",
      "Gradient Descent(14/49): loss=0.45017325522468216\n",
      "Gradient Descent(15/49): loss=0.45017314515955004\n",
      "Gradient Descent(16/49): loss=0.45017310007687167\n",
      "Gradient Descent(17/49): loss=0.45017308161100666\n",
      "Gradient Descent(18/49): loss=0.4501730740473882\n",
      "Gradient Descent(19/49): loss=0.45017307094933023\n",
      "Gradient Descent(20/49): loss=0.4501730696803658\n",
      "Gradient Descent(21/49): loss=0.4501730691605978\n",
      "Gradient Descent(22/49): loss=0.4501730689477011\n",
      "Gradient Descent(23/49): loss=0.45017306886049824\n",
      "Gradient Descent(24/49): loss=0.4501730688247802\n",
      "Gradient Descent(25/49): loss=0.45017306881014996\n",
      "Gradient Descent(26/49): loss=0.4501730688041574\n",
      "Gradient Descent(27/49): loss=0.45017306880170277\n",
      "Gradient Descent(28/49): loss=0.4501730688006974\n",
      "Gradient Descent(29/49): loss=0.4501730688002858\n",
      "Gradient Descent(30/49): loss=0.45017306880011715\n",
      "Gradient Descent(31/49): loss=0.45017306880004787\n",
      "Gradient Descent(32/49): loss=0.4501730688000198\n",
      "Gradient Descent(33/49): loss=0.45017306880000807\n",
      "Gradient Descent(34/49): loss=0.45017306880000313\n",
      "Gradient Descent(35/49): loss=0.45017306880000124\n",
      "Gradient Descent(36/49): loss=0.45017306880000046\n",
      "Gradient Descent(37/49): loss=0.45017306880000024\n",
      "Gradient Descent(38/49): loss=0.45017306880000024\n",
      "Gradient Descent(39/49): loss=0.4501730687999999\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730687999999\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730687999999\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4693719457837737\n",
      "Gradient Descent(2/49): loss=0.45797524680991636\n",
      "Gradient Descent(3/49): loss=0.4537345351217437\n",
      "Gradient Descent(4/49): loss=0.4521565663025746\n",
      "Gradient Descent(5/49): loss=0.4515694041049619\n",
      "Gradient Descent(6/49): loss=0.4513509210512302\n",
      "Gradient Descent(7/49): loss=0.4512696235069365\n",
      "Gradient Descent(8/49): loss=0.451239372690705\n",
      "Gradient Descent(9/49): loss=0.45122811636198523\n",
      "Gradient Descent(10/49): loss=0.4512239278820685\n",
      "Gradient Descent(11/49): loss=0.4512223693486917\n",
      "Gradient Descent(12/49): loss=0.451221789418422\n",
      "Gradient Descent(13/49): loss=0.4512215736263688\n",
      "Gradient Descent(14/49): loss=0.45122149333014555\n",
      "Gradient Descent(15/49): loss=0.45122146345192116\n",
      "Gradient Descent(16/49): loss=0.45122145233423394\n",
      "Gradient Descent(17/49): loss=0.4512214481973422\n",
      "Gradient Descent(18/49): loss=0.45122144665800484\n",
      "Gradient Descent(19/49): loss=0.4512214460852176\n",
      "Gradient Descent(20/49): loss=0.45122144587208307\n",
      "Gradient Descent(21/49): loss=0.4512214457927759\n",
      "Gradient Descent(22/49): loss=0.4512214457632657\n",
      "Gradient Descent(23/49): loss=0.451221445752285\n",
      "Gradient Descent(24/49): loss=0.4512214457481991\n",
      "Gradient Descent(25/49): loss=0.4512214457466788\n",
      "Gradient Descent(26/49): loss=0.45122144574611295\n",
      "Gradient Descent(27/49): loss=0.45122144574590245\n",
      "Gradient Descent(28/49): loss=0.4512214457458241\n",
      "Gradient Descent(29/49): loss=0.451221445745795\n",
      "Gradient Descent(30/49): loss=0.45122144574578427\n",
      "Gradient Descent(31/49): loss=0.45122144574578016\n",
      "Gradient Descent(32/49): loss=0.4512214457457788\n",
      "Gradient Descent(33/49): loss=0.45122144574577805\n",
      "Gradient Descent(34/49): loss=0.45122144574577794\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.45122144574577766\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.45122144574577766\n",
      "Gradient Descent(42/49): loss=0.45122144574577755\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.45122144574577766\n",
      "Gradient Descent(46/49): loss=0.45122144574577766\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4686036297960618\n",
      "Gradient Descent(2/49): loss=0.4569210404431765\n",
      "Gradient Descent(3/49): loss=0.45257394894496794\n",
      "Gradient Descent(4/49): loss=0.4509563961984843\n",
      "Gradient Descent(5/49): loss=0.45035450482151806\n",
      "Gradient Descent(6/49): loss=0.4501305410401488\n",
      "Gradient Descent(7/49): loss=0.45004720411710114\n",
      "Gradient Descent(8/49): loss=0.4500161944480352\n",
      "Gradient Descent(9/49): loss=0.4500046557501756\n",
      "Gradient Descent(10/49): loss=0.4500003622007024\n",
      "Gradient Descent(11/49): loss=0.44999876457094323\n",
      "Gradient Descent(12/49): loss=0.44999817009290977\n",
      "Gradient Descent(13/49): loss=0.44999794888763345\n",
      "Gradient Descent(14/49): loss=0.44999786657715035\n",
      "Gradient Descent(15/49): loss=0.44999783594941944\n",
      "Gradient Descent(16/49): loss=0.44999782455284104\n",
      "Gradient Descent(17/49): loss=0.44999782031217395\n",
      "Gradient Descent(18/49): loss=0.44999781873422157\n",
      "Gradient Descent(19/49): loss=0.4499978181470658\n",
      "Gradient Descent(20/49): loss=0.4499978179285851\n",
      "Gradient Descent(21/49): loss=0.4499978178472884\n",
      "Gradient Descent(22/49): loss=0.44999781781703785\n",
      "Gradient Descent(23/49): loss=0.4499978178057819\n",
      "Gradient Descent(24/49): loss=0.4499978178015933\n",
      "Gradient Descent(25/49): loss=0.4499978178000348\n",
      "Gradient Descent(26/49): loss=0.4499978177994548\n",
      "Gradient Descent(27/49): loss=0.4499978177992391\n",
      "Gradient Descent(28/49): loss=0.44999781779915865\n",
      "Gradient Descent(29/49): loss=0.4499978177991287\n",
      "Gradient Descent(30/49): loss=0.4499978177991178\n",
      "Gradient Descent(31/49): loss=0.44999781779911346\n",
      "Gradient Descent(32/49): loss=0.4499978177991121\n",
      "Gradient Descent(33/49): loss=0.4499978177991116\n",
      "Gradient Descent(34/49): loss=0.44999781779911135\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.4499978177991112\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911097\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.4499978177991113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911135\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.4499978177991113\n",
      "Gradient Descent(49/49): loss=0.44999781779911097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4689668713280001\n",
      "Gradient Descent(2/49): loss=0.4574194441491484\n",
      "Gradient Descent(3/49): loss=0.4531226464958984\n",
      "Gradient Descent(4/49): loss=0.4515238080891236\n",
      "Gradient Descent(5/49): loss=0.4509288803179629\n",
      "Gradient Descent(6/49): loss=0.45070750769431395\n",
      "Gradient Descent(7/49): loss=0.4506251349410541\n",
      "Gradient Descent(8/49): loss=0.45059448403956626\n",
      "Gradient Descent(9/49): loss=0.4505830788391227\n",
      "Gradient Descent(10/49): loss=0.4505788349640374\n",
      "Gradient Descent(11/49): loss=0.4505772558181186\n",
      "Gradient Descent(12/49): loss=0.4505766682179219\n",
      "Gradient Descent(13/49): loss=0.45057644957188875\n",
      "Gradient Descent(14/49): loss=0.4505763682136997\n",
      "Gradient Descent(15/49): loss=0.4505763379403176\n",
      "Gradient Descent(16/49): loss=0.45057632667559216\n",
      "Gradient Descent(17/49): loss=0.4505763224839879\n",
      "Gradient Descent(18/49): loss=0.450576320924292\n",
      "Gradient Descent(19/49): loss=0.45057632034392886\n",
      "Gradient Descent(20/49): loss=0.45057632012797605\n",
      "Gradient Descent(21/49): loss=0.45057632004761977\n",
      "Gradient Descent(22/49): loss=0.45057632001771947\n",
      "Gradient Descent(23/49): loss=0.4505763200065934\n",
      "Gradient Descent(24/49): loss=0.4505763200024534\n",
      "Gradient Descent(25/49): loss=0.45057632000091297\n",
      "Gradient Descent(26/49): loss=0.4505763200003396\n",
      "Gradient Descent(27/49): loss=0.4505763200001265\n",
      "Gradient Descent(28/49): loss=0.45057632000004694\n",
      "Gradient Descent(29/49): loss=0.45057632000001746\n",
      "Gradient Descent(30/49): loss=0.45057632000000636\n",
      "Gradient Descent(31/49): loss=0.4505763200000023\n",
      "Gradient Descent(32/49): loss=0.4505763200000009\n",
      "Gradient Descent(33/49): loss=0.4505763200000003\n",
      "Gradient Descent(34/49): loss=0.45057632000000014\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999998\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.45057632000000014\n",
      "Gradient Descent(43/49): loss=0.4505763200000003\n",
      "Gradient Descent(44/49): loss=0.45057632000000014\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.4505763200000001\n",
      "Gradient Descent(49/49): loss=0.4505763200000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46871366989951985\n",
      "Gradient Descent(2/49): loss=0.4570720264691313\n",
      "Gradient Descent(3/49): loss=0.4527401709486837\n",
      "Gradient Descent(4/49): loss=0.4511282875095251\n",
      "Gradient Descent(5/49): loss=0.4505285056818142\n",
      "Gradient Descent(6/49): loss=0.45030532686372293\n",
      "Gradient Descent(7/49): loss=0.4502222820255114\n",
      "Gradient Descent(8/49): loss=0.45019138104121265\n",
      "Gradient Descent(9/49): loss=0.4501798827849552\n",
      "Gradient Descent(10/49): loss=0.450175604283802\n",
      "Gradient Descent(11/49): loss=0.4501740122535229\n",
      "Gradient Descent(12/49): loss=0.4501734198590559\n",
      "Gradient Descent(13/49): loss=0.4501731994290747\n",
      "Gradient Descent(14/49): loss=0.45017311740707877\n",
      "Gradient Descent(15/49): loss=0.4501730868866941\n",
      "Gradient Descent(16/49): loss=0.45017307553005886\n",
      "Gradient Descent(17/49): loss=0.450173071304255\n",
      "Gradient Descent(18/49): loss=0.45017306973183324\n",
      "Gradient Descent(19/49): loss=0.4501730691467351\n",
      "Gradient Descent(20/49): loss=0.4501730689290201\n",
      "Gradient Descent(21/49): loss=0.4501730688480084\n",
      "Gradient Descent(22/49): loss=0.450173068817864\n",
      "Gradient Descent(23/49): loss=0.4501730688066471\n",
      "Gradient Descent(24/49): loss=0.45017306880247326\n",
      "Gradient Descent(25/49): loss=0.4501730688009203\n",
      "Gradient Descent(26/49): loss=0.4501730688003425\n",
      "Gradient Descent(27/49): loss=0.4501730688001277\n",
      "Gradient Descent(28/49): loss=0.45017306880004726\n",
      "Gradient Descent(29/49): loss=0.4501730688000176\n",
      "Gradient Descent(30/49): loss=0.4501730688000066\n",
      "Gradient Descent(31/49): loss=0.4501730688000026\n",
      "Gradient Descent(32/49): loss=0.45017306880000096\n",
      "Gradient Descent(33/49): loss=0.4501730688000004\n",
      "Gradient Descent(34/49): loss=0.4501730688000001\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688000001\n",
      "Gradient Descent(41/49): loss=0.4501730688000001\n",
      "Gradient Descent(42/49): loss=0.4501730687999999\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.4501730687999999\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000001\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46763055139689813\n",
      "Gradient Descent(2/49): loss=0.45674146888681455\n",
      "Gradient Descent(3/49): loss=0.4530783815304224\n",
      "Gradient Descent(4/49): loss=0.45184611894373256\n",
      "Gradient Descent(5/49): loss=0.4514315858095698\n",
      "Gradient Descent(6/49): loss=0.4512921368632374\n",
      "Gradient Descent(7/49): loss=0.4512452262376911\n",
      "Gradient Descent(8/49): loss=0.45122944550325744\n",
      "Gradient Descent(9/49): loss=0.4512241368641941\n",
      "Gradient Descent(10/49): loss=0.45122235103801306\n",
      "Gradient Descent(11/49): loss=0.4512217502860857\n",
      "Gradient Descent(12/49): loss=0.45122154819313737\n",
      "Gradient Descent(13/49): loss=0.45122148020906955\n",
      "Gradient Descent(14/49): loss=0.45122145733922914\n",
      "Gradient Descent(15/49): loss=0.4512214496458149\n",
      "Gradient Descent(16/49): loss=0.45122144705775014\n",
      "Gradient Descent(17/49): loss=0.45122144618712545\n",
      "Gradient Descent(18/49): loss=0.45122144589424706\n",
      "Gradient Descent(19/49): loss=0.45122144579572293\n",
      "Gradient Descent(20/49): loss=0.45122144576257917\n",
      "Gradient Descent(21/49): loss=0.4512214457514299\n",
      "Gradient Descent(22/49): loss=0.4512214457476791\n",
      "Gradient Descent(23/49): loss=0.45122144574641715\n",
      "Gradient Descent(24/49): loss=0.45122144574599293\n",
      "Gradient Descent(25/49): loss=0.45122144574585005\n",
      "Gradient Descent(26/49): loss=0.4512214457458021\n",
      "Gradient Descent(27/49): loss=0.4512214457457858\n",
      "Gradient Descent(28/49): loss=0.45122144574578055\n",
      "Gradient Descent(29/49): loss=0.45122144574577866\n",
      "Gradient Descent(30/49): loss=0.4512214457457781\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.45122144574577766\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.45122144574577766\n",
      "Gradient Descent(41/49): loss=0.45122144574577766\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.45122144574577805\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.45122144574577794\n",
      "Gradient Descent(46/49): loss=0.45122144574577794\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.45122144574577766\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46681855189149\n",
      "Gradient Descent(2/49): loss=0.45565631274778745\n",
      "Gradient Descent(3/49): loss=0.4519013354998457\n",
      "Gradient Descent(4/49): loss=0.45063816115363825\n",
      "Gradient Descent(5/49): loss=0.4502132293035741\n",
      "Gradient Descent(6/49): loss=0.4500702822292125\n",
      "Gradient Descent(7/49): loss=0.45002219483339717\n",
      "Gradient Descent(8/49): loss=0.4500060182334448\n",
      "Gradient Descent(9/49): loss=0.45000057642522096\n",
      "Gradient Descent(10/49): loss=0.4499987458009345\n",
      "Gradient Descent(11/49): loss=0.44999812997892435\n",
      "Gradient Descent(12/49): loss=0.44999792281640033\n",
      "Gradient Descent(13/49): loss=0.4499978531269274\n",
      "Gradient Descent(14/49): loss=0.4499978296833884\n",
      "Gradient Descent(15/49): loss=0.4499978217969822\n",
      "Gradient Descent(16/49): loss=0.44999781914399495\n",
      "Gradient Descent(17/49): loss=0.44999781825152985\n",
      "Gradient Descent(18/49): loss=0.449997817951305\n",
      "Gradient Descent(19/49): loss=0.449997817850309\n",
      "Gradient Descent(20/49): loss=0.44999781781633413\n",
      "Gradient Descent(21/49): loss=0.44999781780490483\n",
      "Gradient Descent(22/49): loss=0.44999781780106013\n",
      "Gradient Descent(23/49): loss=0.4499978177997669\n",
      "Gradient Descent(24/49): loss=0.44999781779933157\n",
      "Gradient Descent(25/49): loss=0.44999781779918535\n",
      "Gradient Descent(26/49): loss=0.44999781779913606\n",
      "Gradient Descent(27/49): loss=0.44999781779911946\n",
      "Gradient Descent(28/49): loss=0.4499978177991139\n",
      "Gradient Descent(29/49): loss=0.4499978177991121\n",
      "Gradient Descent(30/49): loss=0.4499978177991116\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.44999781779911135\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.4499978177991113\n",
      "Gradient Descent(46/49): loss=0.44999781779911097\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.44999781779911097\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.467202445952\n",
      "Gradient Descent(2/49): loss=0.4561693487702527\n",
      "Gradient Descent(3/49): loss=0.4524578148783129\n",
      "Gradient Descent(4/49): loss=0.45120925487706454\n",
      "Gradient Descent(5/49): loss=0.4507892392926446\n",
      "Gradient Descent(6/49): loss=0.4506479460500457\n",
      "Gradient Descent(7/49): loss=0.4506004150032353\n",
      "Gradient Descent(8/49): loss=0.45058442555908845\n",
      "Gradient Descent(9/49): loss=0.45057904671007726\n",
      "Gradient Descent(10/49): loss=0.4505772372652701\n",
      "Gradient Descent(11/49): loss=0.45057662856803693\n",
      "Gradient Descent(12/49): loss=0.45057642380228746\n",
      "Gradient Descent(13/49): loss=0.4505763549190897\n",
      "Gradient Descent(14/49): loss=0.45057633174678197\n",
      "Gradient Descent(15/49): loss=0.4505763239516173\n",
      "Gradient Descent(16/49): loss=0.450576321329324\n",
      "Gradient Descent(17/49): loss=0.45057632044718454\n",
      "Gradient Descent(18/49): loss=0.45057632015043286\n",
      "Gradient Descent(19/49): loss=0.4505763200506055\n",
      "Gradient Descent(20/49): loss=0.4505763200170238\n",
      "Gradient Descent(21/49): loss=0.45057632000572667\n",
      "Gradient Descent(22/49): loss=0.4505763200019265\n",
      "Gradient Descent(23/49): loss=0.450576320000648\n",
      "Gradient Descent(24/49): loss=0.45057632000021786\n",
      "Gradient Descent(25/49): loss=0.4505763200000733\n",
      "Gradient Descent(26/49): loss=0.45057632000002457\n",
      "Gradient Descent(27/49): loss=0.45057632000000847\n",
      "Gradient Descent(28/49): loss=0.45057632000000286\n",
      "Gradient Descent(29/49): loss=0.4505763200000009\n",
      "Gradient Descent(30/49): loss=0.4505763200000003\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.45057632\n",
      "Gradient Descent(34/49): loss=0.45057632000000014\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.4505763199999998\n",
      "Gradient Descent(37/49): loss=0.4505763199999998\n",
      "Gradient Descent(38/49): loss=0.45057632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=0.4505763200000002\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.45057632\n",
      "Gradient Descent(44/49): loss=0.45057632\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.45057632\n",
      "Gradient Descent(49/49): loss=0.4505763200000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46693484845568006\n",
      "Gradient Descent(2/49): loss=0.45581173147617104\n",
      "Gradient Descent(3/49): loss=0.45206991492426357\n",
      "Gradient Descent(4/49): loss=0.4508111678362023\n",
      "Gradient Descent(5/49): loss=0.4503877253157786\n",
      "Gradient Descent(6/49): loss=0.4502452792519081\n",
      "Gradient Descent(7/49): loss=0.4501973603960218\n",
      "Gradient Descent(8/49): loss=0.45018124049290176\n",
      "Gradient Descent(9/49): loss=0.45017581775749227\n",
      "Gradient Descent(10/49): loss=0.4501739935493006\n",
      "Gradient Descent(11/49): loss=0.45017337988566475\n",
      "Gradient Descent(12/49): loss=0.4501731734492178\n",
      "Gradient Descent(13/49): loss=0.4501731040039967\n",
      "Gradient Descent(14/49): loss=0.45017308064262435\n",
      "Gradient Descent(15/49): loss=0.4501730727838588\n",
      "Gradient Descent(16/49): loss=0.4501730701401702\n",
      "Gradient Descent(17/49): loss=0.45017306925083317\n",
      "Gradient Descent(18/49): loss=0.45017306895166015\n",
      "Gradient Descent(19/49): loss=0.4501730688510185\n",
      "Gradient Descent(20/49): loss=0.4501730688171627\n",
      "Gradient Descent(21/49): loss=0.45017306880577335\n",
      "Gradient Descent(22/49): loss=0.4501730688019422\n",
      "Gradient Descent(23/49): loss=0.4501730688006533\n",
      "Gradient Descent(24/49): loss=0.4501730688002198\n",
      "Gradient Descent(25/49): loss=0.45017306880007396\n",
      "Gradient Descent(26/49): loss=0.450173068800025\n",
      "Gradient Descent(27/49): loss=0.4501730688000084\n",
      "Gradient Descent(28/49): loss=0.4501730688000028\n",
      "Gradient Descent(29/49): loss=0.45017306880000096\n",
      "Gradient Descent(30/49): loss=0.4501730688000004\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.45017306879999985\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.45017306880000024\n",
      "Gradient Descent(39/49): loss=0.45017306880000024\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730687999999\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730687999998\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4659769584076802\n",
      "Gradient Descent(2/49): loss=0.4556849883260031\n",
      "Gradient Descent(3/49): loss=0.45257166737629595\n",
      "Gradient Descent(4/49): loss=0.4516298877890097\n",
      "Gradient Descent(5/49): loss=0.4513449994638554\n",
      "Gradient Descent(6/49): loss=0.45125882074549645\n",
      "Gradient Descent(7/49): loss=0.45123275168319255\n",
      "Gradient Descent(8/49): loss=0.45122486579184573\n",
      "Gradient Descent(9/49): loss=0.45122248030971335\n",
      "Gradient Descent(10/49): loss=0.4512217587013683\n",
      "Gradient Descent(11/49): loss=0.4512215404148439\n",
      "Gradient Descent(12/49): loss=0.4512214743831702\n",
      "Gradient Descent(13/49): loss=0.4512214544085889\n",
      "Gradient Descent(14/49): loss=0.45122144836627814\n",
      "Gradient Descent(15/49): loss=0.45122144653847923\n",
      "Gradient Descent(16/49): loss=0.4512214459855699\n",
      "Gradient Descent(17/49): loss=0.4512214458183152\n",
      "Gradient Descent(18/49): loss=0.4512214457677202\n",
      "Gradient Descent(19/49): loss=0.45122144575241535\n",
      "Gradient Descent(20/49): loss=0.4512214457477857\n",
      "Gradient Descent(21/49): loss=0.45122144574638534\n",
      "Gradient Descent(22/49): loss=0.45122144574596157\n",
      "Gradient Descent(23/49): loss=0.45122144574583345\n",
      "Gradient Descent(24/49): loss=0.45122144574579465\n",
      "Gradient Descent(25/49): loss=0.4512214457457827\n",
      "Gradient Descent(26/49): loss=0.4512214457457794\n",
      "Gradient Descent(27/49): loss=0.45122144574577827\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.45122144574577755\n",
      "Gradient Descent(37/49): loss=0.45122144574577766\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.45122144574577755\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.45122144574577794\n",
      "Gradient Descent(46/49): loss=0.45122144574577766\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.45122144574577766\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46512347791488\n",
      "Gradient Descent(2/49): loss=0.45457332998413086\n",
      "Gradient Descent(3/49): loss=0.45138191023507973\n",
      "Gradient Descent(4/49): loss=0.45041650576099157\n",
      "Gradient Descent(5/49): loss=0.4501244709075798\n",
      "Gradient Descent(6/49): loss=0.4500361303644229\n",
      "Gradient Descent(7/49): loss=0.450009407350118\n",
      "Gradient Descent(8/49): loss=0.45000132363829065\n",
      "Gradient Descent(9/49): loss=0.44999887831546287\n",
      "Gradient Descent(10/49): loss=0.4499981386053076\n",
      "Gradient Descent(11/49): loss=0.4499979148429856\n",
      "Gradient Descent(12/49): loss=0.44999784715488306\n",
      "Gradient Descent(13/49): loss=0.44999782667923205\n",
      "Gradient Descent(14/49): loss=0.4499978204853477\n",
      "Gradient Descent(15/49): loss=0.4499978186116977\n",
      "Gradient Descent(16/49): loss=0.4499978180449185\n",
      "Gradient Descent(17/49): loss=0.4499978178734681\n",
      "Gradient Descent(18/49): loss=0.44999781782160386\n",
      "Gradient Descent(19/49): loss=0.4499978178059152\n",
      "Gradient Descent(20/49): loss=0.44999781780116943\n",
      "Gradient Descent(21/49): loss=0.44999781779973386\n",
      "Gradient Descent(22/49): loss=0.4499978177992994\n",
      "Gradient Descent(23/49): loss=0.4499978177991681\n",
      "Gradient Descent(24/49): loss=0.4499978177991285\n",
      "Gradient Descent(25/49): loss=0.44999781779911624\n",
      "Gradient Descent(26/49): loss=0.44999781779911274\n",
      "Gradient Descent(27/49): loss=0.44999781779911174\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.44999781779911135\n",
      "Gradient Descent(30/49): loss=0.44999781779911097\n",
      "Gradient Descent(31/49): loss=0.4499978177991113\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.4499978177991112\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.4499978177991113\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.44999781779911097\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.4499978177991113\n",
      "Gradient Descent(46/49): loss=0.4499978177991112\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4655269832000001\n",
      "Gradient Descent(2/49): loss=0.455098895618\n",
      "Gradient Descent(3/49): loss=0.45194439912444506\n",
      "Gradient Descent(4/49): loss=0.4509901639351444\n",
      "Gradient Descent(5/49): loss=0.4507015077903811\n",
      "Gradient Descent(6/49): loss=0.4506141893065903\n",
      "Gradient Descent(7/49): loss=0.45058777546524365\n",
      "Gradient Descent(8/49): loss=0.4505797852782363\n",
      "Gradient Descent(9/49): loss=0.4505773682466664\n",
      "Gradient Descent(10/49): loss=0.45057663709461665\n",
      "Gradient Descent(11/49): loss=0.45057641592112146\n",
      "Gradient Descent(12/49): loss=0.4505763490161393\n",
      "Gradient Descent(13/49): loss=0.45057632877738224\n",
      "Gradient Descent(14/49): loss=0.450576322655158\n",
      "Gradient Descent(15/49): loss=0.45057632080318544\n",
      "Gradient Descent(16/49): loss=0.45057632024296373\n",
      "Gradient Descent(17/49): loss=0.45057632007349663\n",
      "Gradient Descent(18/49): loss=0.45057632002223275\n",
      "Gradient Descent(19/49): loss=0.4505763200067253\n",
      "Gradient Descent(20/49): loss=0.45057632000203446\n",
      "Gradient Descent(21/49): loss=0.4505763200006154\n",
      "Gradient Descent(22/49): loss=0.450576320000186\n",
      "Gradient Descent(23/49): loss=0.4505763200000563\n",
      "Gradient Descent(24/49): loss=0.45057632000001696\n",
      "Gradient Descent(25/49): loss=0.4505763200000052\n",
      "Gradient Descent(26/49): loss=0.45057632000000164\n",
      "Gradient Descent(27/49): loss=0.45057632000000036\n",
      "Gradient Descent(28/49): loss=0.45057632\n",
      "Gradient Descent(29/49): loss=0.45057632000000014\n",
      "Gradient Descent(30/49): loss=0.4505763200000002\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.45057632\n",
      "Gradient Descent(34/49): loss=0.45057632000000014\n",
      "Gradient Descent(35/49): loss=0.45057632000000014\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.45057632\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.4505763200000001\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.45057632000000014\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.45057632\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999998\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46524571548800014\n",
      "Gradient Descent(2/49): loss=0.4547325444231199\n",
      "Gradient Descent(3/49): loss=0.45155231017599395\n",
      "Gradient Descent(4/49): loss=0.4505902893162381\n",
      "Gradient Descent(5/49): loss=0.45029927800616204\n",
      "Gradient Descent(6/49): loss=0.45021124708486393\n",
      "Gradient Descent(7/49): loss=0.4501846177311715\n",
      "Gradient Descent(8/49): loss=0.45017656235167935\n",
      "Gradient Descent(9/49): loss=0.4501741255993831\n",
      "Gradient Descent(10/49): loss=0.45017338848181326\n",
      "Gradient Descent(11/49): loss=0.45017316550374853\n",
      "Gradient Descent(12/49): loss=0.4501730980528841\n",
      "Gradient Descent(13/49): loss=0.4501730776489974\n",
      "Gradient Descent(14/49): loss=0.45017307147682173\n",
      "Gradient Descent(15/49): loss=0.45017306960973863\n",
      "Gradient Descent(16/49): loss=0.450173069044946\n",
      "Gradient Descent(17/49): loss=0.4501730688740961\n",
      "Gradient Descent(18/49): loss=0.450173068822414\n",
      "Gradient Descent(19/49): loss=0.45017306880678026\n",
      "Gradient Descent(20/49): loss=0.45017306880205116\n",
      "Gradient Descent(21/49): loss=0.4501730688006205\n",
      "Gradient Descent(22/49): loss=0.45017306880018754\n",
      "Gradient Descent(23/49): loss=0.45017306880005664\n",
      "Gradient Descent(24/49): loss=0.45017306880001734\n",
      "Gradient Descent(25/49): loss=0.4501730688000052\n",
      "Gradient Descent(26/49): loss=0.45017306880000146\n",
      "Gradient Descent(27/49): loss=0.45017306880000046\n",
      "Gradient Descent(28/49): loss=0.45017306880000024\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730687999999\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730687999999\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730687999998\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730687999999\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.45017306879999985\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4644111668161195\n",
      "Gradient Descent(2/49): loss=0.45478794632319836\n",
      "Gradient Descent(3/49): loss=0.4521858275019122\n",
      "Gradient Descent(4/49): loss=0.45148221457263643\n",
      "Gradient Descent(5/49): loss=0.4512919576365605\n",
      "Gradient Descent(6/49): loss=0.45124051216104566\n",
      "Gradient Descent(7/49): loss=0.4512266013044661\n",
      "Gradient Descent(8/49): loss=0.451222839808847\n",
      "Gradient Descent(9/49): loss=0.4512218227004317\n",
      "Gradient Descent(10/49): loss=0.45122154767431605\n",
      "Gradient Descent(11/49): loss=0.45122147330725465\n",
      "Gradient Descent(12/49): loss=0.4512214531984011\n",
      "Gradient Descent(13/49): loss=0.451221447760967\n",
      "Gradient Descent(14/49): loss=0.45122144629068495\n",
      "Gradient Descent(15/49): loss=0.45122144589312085\n",
      "Gradient Descent(16/49): loss=0.45122144578561935\n",
      "Gradient Descent(17/49): loss=0.45122144575655093\n",
      "Gradient Descent(18/49): loss=0.45122144574869083\n",
      "Gradient Descent(19/49): loss=0.4512214457465654\n",
      "Gradient Descent(20/49): loss=0.4512214457459908\n",
      "Gradient Descent(21/49): loss=0.45122144574583517\n",
      "Gradient Descent(22/49): loss=0.4512214457457934\n",
      "Gradient Descent(23/49): loss=0.45122144574578193\n",
      "Gradient Descent(24/49): loss=0.4512214457457788\n",
      "Gradient Descent(25/49): loss=0.4512214457457781\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.45122144574577766\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.45122144574577766\n",
      "Gradient Descent(32/49): loss=0.45122144574577766\n",
      "Gradient Descent(33/49): loss=0.45122144574577794\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.45122144574577766\n",
      "Gradient Descent(37/49): loss=0.45122144574577766\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.45122144574577794\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.45122144574577766\n",
      "Gradient Descent(43/49): loss=0.45122144574577755\n",
      "Gradient Descent(44/49): loss=0.45122144574577766\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.45122144574577794\n",
      "Gradient Descent(47/49): loss=0.45122144574577805\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.45122144574577794\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46351840786623155\n",
      "Gradient Descent(2/49): loss=0.4536537853532605\n",
      "Gradient Descent(3/49): loss=0.45098639142575303\n",
      "Gradient Descent(4/49): loss=0.45026512810775504\n",
      "Gradient Descent(5/49): loss=0.45007009850656854\n",
      "Gradient Descent(6/49): loss=0.4500173625024076\n",
      "Gradient Descent(7/49): loss=0.4500031026868823\n",
      "Gradient Descent(8/49): loss=0.4499992468327643\n",
      "Gradient Descent(9/49): loss=0.44999820420981107\n",
      "Gradient Descent(10/49): loss=0.44999792228456437\n",
      "Gradient Descent(11/49): loss=0.44999784605197785\n",
      "Gradient Descent(12/49): loss=0.44999782543868616\n",
      "Gradient Descent(13/49): loss=0.44999781986485227\n",
      "Gradient Descent(14/49): loss=0.44999781835768765\n",
      "Gradient Descent(15/49): loss=0.44999781795015026\n",
      "Gradient Descent(16/49): loss=0.44999781783995224\n",
      "Gradient Descent(17/49): loss=0.4499978178101545\n",
      "Gradient Descent(18/49): loss=0.4499978178020972\n",
      "Gradient Descent(19/49): loss=0.44999781779991843\n",
      "Gradient Descent(20/49): loss=0.4499978177993295\n",
      "Gradient Descent(21/49): loss=0.44999781779917003\n",
      "Gradient Descent(22/49): loss=0.4499978177991271\n",
      "Gradient Descent(23/49): loss=0.44999781779911546\n",
      "Gradient Descent(24/49): loss=0.44999781779911247\n",
      "Gradient Descent(25/49): loss=0.44999781779911147\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991113\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.44999781779911097\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.4499978177991113\n",
      "Gradient Descent(40/49): loss=0.4499978177991109\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911097\n",
      "Gradient Descent(45/49): loss=0.44999781779911097\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4639404830720001\n",
      "Gradient Descent(2/49): loss=0.45418998969466884\n",
      "Gradient Descent(3/49): loss=0.4515534562854385\n",
      "Gradient Descent(4/49): loss=0.4508405376515826\n",
      "Gradient Descent(5/49): loss=0.45064776445298804\n",
      "Gradient Descent(6/49): loss=0.4505956385800879\n",
      "Gradient Descent(7/49): loss=0.4505815437440556\n",
      "Gradient Descent(8/49): loss=0.4505777325003928\n",
      "Gradient Descent(9/49): loss=0.4505767019401061\n",
      "Gradient Descent(10/49): loss=0.45057642327660463\n",
      "Gradient Descent(11/49): loss=0.4505763479259938\n",
      "Gradient Descent(12/49): loss=0.45057632755118865\n",
      "Gradient Descent(13/49): loss=0.4505763220418413\n",
      "Gradient Descent(14/49): loss=0.45057632055211383\n",
      "Gradient Descent(15/49): loss=0.45057632014929155\n",
      "Gradient Descent(16/49): loss=0.45057632004036846\n",
      "Gradient Descent(17/49): loss=0.45057632001091547\n",
      "Gradient Descent(18/49): loss=0.45057632000295145\n",
      "Gradient Descent(19/49): loss=0.4505763200007981\n",
      "Gradient Descent(20/49): loss=0.45057632000021597\n",
      "Gradient Descent(21/49): loss=0.4505763200000584\n",
      "Gradient Descent(22/49): loss=0.45057632000001574\n",
      "Gradient Descent(23/49): loss=0.4505763200000042\n",
      "Gradient Descent(24/49): loss=0.4505763200000011\n",
      "Gradient Descent(25/49): loss=0.4505763200000003\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763200000001\n",
      "Gradient Descent(38/49): loss=0.45057632\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.45057632000000014\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.45057632\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.45057632000000014\n",
      "Gradient Descent(47/49): loss=0.4505763200000001\n",
      "Gradient Descent(48/49): loss=0.45057631999999975\n",
      "Gradient Descent(49/49): loss=0.45057631999999975\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4636462709964801\n",
      "Gradient Descent(2/49): loss=0.45381622267392807\n",
      "Gradient Descent(3/49): loss=0.45115817760751026\n",
      "Gradient Descent(4/49): loss=0.45043944222155063\n",
      "Gradient Descent(5/49): loss=0.45024509617318725\n",
      "Gradient Descent(6/49): loss=0.45019254500170985\n",
      "Gradient Descent(7/49): loss=0.4501783351649424\n",
      "Gradient Descent(8/49): loss=0.45017449282508043\n",
      "Gradient Descent(9/49): loss=0.4501734538563816\n",
      "Gradient Descent(10/49): loss=0.4501731729192456\n",
      "Gradient Descent(11/49): loss=0.4501730969538441\n",
      "Gradient Descent(12/49): loss=0.45017307641279947\n",
      "Gradient Descent(13/49): loss=0.4501730708585009\n",
      "Gradient Descent(14/49): loss=0.4501730693566188\n",
      "Gradient Descent(15/49): loss=0.45017306895050974\n",
      "Gradient Descent(16/49): loss=0.45017306884069785\n",
      "Gradient Descent(17/49): loss=0.4501730688110048\n",
      "Gradient Descent(18/49): loss=0.45017306880297564\n",
      "Gradient Descent(19/49): loss=0.45017306880080465\n",
      "Gradient Descent(20/49): loss=0.4501730688002174\n",
      "Gradient Descent(21/49): loss=0.450173068800059\n",
      "Gradient Descent(22/49): loss=0.4501730688000158\n",
      "Gradient Descent(23/49): loss=0.4501730688000042\n",
      "Gradient Descent(24/49): loss=0.4501730688000012\n",
      "Gradient Descent(25/49): loss=0.4501730688000003\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730687999999\n",
      "Gradient Descent(28/49): loss=0.4501730687999999\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.4501730687999999\n",
      "Gradient Descent(35/49): loss=0.4501730688000001\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688000001\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688000001\n",
      "Gradient Descent(41/49): loss=0.45017306880000024\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4629331766222166\n",
      "Gradient Descent(2/49): loss=0.4540334323292105\n",
      "Gradient Descent(3/49): loss=0.45189660372445994\n",
      "Gradient Descent(4/49): loss=0.45138355117645923\n",
      "Gradient Descent(5/49): loss=0.4512603672596843\n",
      "Gradient Descent(6/49): loss=0.4512307908012666\n",
      "Gradient Descent(7/49): loss=0.45122368949360075\n",
      "Gradient Descent(8/49): loss=0.4512219844696302\n",
      "Gradient Descent(9/49): loss=0.4512215750933747\n",
      "Gradient Descent(10/49): loss=0.45122147680213587\n",
      "Gradient Descent(11/49): loss=0.4512214532024093\n",
      "Gradient Descent(12/49): loss=0.4512214475361151\n",
      "Gradient Descent(13/49): loss=0.45122144617563786\n",
      "Gradient Descent(14/49): loss=0.45122144584898727\n",
      "Gradient Descent(15/49): loss=0.4512214457705582\n",
      "Gradient Descent(16/49): loss=0.4512214457517276\n",
      "Gradient Descent(17/49): loss=0.45122144574720646\n",
      "Gradient Descent(18/49): loss=0.45122144574612066\n",
      "Gradient Descent(19/49): loss=0.4512214457458602\n",
      "Gradient Descent(20/49): loss=0.45122144574579753\n",
      "Gradient Descent(21/49): loss=0.4512214457457824\n",
      "Gradient Descent(22/49): loss=0.45122144574577905\n",
      "Gradient Descent(23/49): loss=0.45122144574577794\n",
      "Gradient Descent(24/49): loss=0.45122144574577794\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457779\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.45122144574577794\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.45122144574577794\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.45122144574577794\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46200334174554436\n",
      "Gradient Descent(2/49): loss=0.4528803440986501\n",
      "Gradient Descent(3/49): loss=0.45068991236363026\n",
      "Gradient Descent(4/49): loss=0.4501639897040522\n",
      "Gradient Descent(5/49): loss=0.4500377156734875\n",
      "Gradient Descent(6/49): loss=0.45000739727874883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=0.4500001178321722\n",
      "Gradient Descent(8/49): loss=0.4499983700370491\n",
      "Gradient Descent(9/49): loss=0.44999795039144\n",
      "Gradient Descent(10/49): loss=0.4499978496345293\n",
      "Gradient Descent(11/49): loss=0.4499978254427951\n",
      "Gradient Descent(12/49): loss=0.4499978196343595\n",
      "Gradient Descent(13/49): loss=0.4499978182397543\n",
      "Gradient Descent(14/49): loss=0.4499978179049095\n",
      "Gradient Descent(15/49): loss=0.44999781782451337\n",
      "Gradient Descent(16/49): loss=0.44999781780521014\n",
      "Gradient Descent(17/49): loss=0.44999781780057546\n",
      "Gradient Descent(18/49): loss=0.4499978177994627\n",
      "Gradient Descent(19/49): loss=0.44999781779919557\n",
      "Gradient Descent(20/49): loss=0.44999781779913145\n",
      "Gradient Descent(21/49): loss=0.44999781779911585\n",
      "Gradient Descent(22/49): loss=0.44999781779911235\n",
      "Gradient Descent(23/49): loss=0.44999781779911147\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991109\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.4499978177991108\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.44999781779911135\n",
      "Gradient Descent(47/49): loss=0.44999781779911097\n",
      "Gradient Descent(48/49): loss=0.4499978177991112\n",
      "Gradient Descent(49/49): loss=0.4499978177991112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.462442945568\n",
      "Gradient Descent(2/49): loss=0.453425496798877\n",
      "Gradient Descent(3/49): loss=0.45126040734941025\n",
      "Gradient Descent(4/49): loss=0.45074056937259344\n",
      "Gradient Descent(5/49): loss=0.4506157562743598\n",
      "Gradient Descent(6/49): loss=0.4505857886494739\n",
      "Gradient Descent(7/49): loss=0.45057859342273876\n",
      "Gradient Descent(8/49): loss=0.4505768658487997\n",
      "Gradient Descent(9/49): loss=0.4505764510582968\n",
      "Gradient Descent(10/49): loss=0.450576351467097\n",
      "Gradient Descent(11/49): loss=0.45057632755525007\n",
      "Gradient Descent(12/49): loss=0.4505763218140156\n",
      "Gradient Descent(13/49): loss=0.45057632043554535\n",
      "Gradient Descent(14/49): loss=0.4505763201045744\n",
      "Gradient Descent(15/49): loss=0.4505763200251083\n",
      "Gradient Descent(16/49): loss=0.45057632000602854\n",
      "Gradient Descent(17/49): loss=0.45057632000144743\n",
      "Gradient Descent(18/49): loss=0.4505763200003476\n",
      "Gradient Descent(19/49): loss=0.45057632000008335\n",
      "Gradient Descent(20/49): loss=0.45057632000002\n",
      "Gradient Descent(21/49): loss=0.45057632000000486\n",
      "Gradient Descent(22/49): loss=0.45057632000000125\n",
      "Gradient Descent(23/49): loss=0.45057632000000036\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.45057632\n",
      "Gradient Descent(26/49): loss=0.4505763199999998\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.4505763200000001\n",
      "Gradient Descent(33/49): loss=0.45057632\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763199999998\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.4505763200000001\n",
      "Gradient Descent(38/49): loss=0.45057632\n",
      "Gradient Descent(39/49): loss=0.4505763199999998\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763200000002\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.45057631999999975\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763200000002\n",
      "Gradient Descent(48/49): loss=0.4505763200000001\n",
      "Gradient Descent(49/49): loss=0.4505763200000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4621365149811201\n",
      "Gradient Descent(2/49): loss=0.4530454922280869\n",
      "Gradient Descent(3/49): loss=0.45086273766508367\n",
      "Gradient Descent(4/49): loss=0.4503386582945068\n",
      "Gradient Descent(5/49): loss=0.4502128268376309\n",
      "Gradient Descent(6/49): loss=0.45018261470483517\n",
      "Gradient Descent(7/49): loss=0.45017536077175085\n",
      "Gradient Descent(8/49): loss=0.4501736191024173\n",
      "Gradient Descent(9/49): loss=0.45017320092761026\n",
      "Gradient Descent(10/49): loss=0.45017310052383946\n",
      "Gradient Descent(11/49): loss=0.4501730764168937\n",
      "Gradient Descent(12/49): loss=0.4501730706288163\n",
      "Gradient Descent(13/49): loss=0.4501730692390987\n",
      "Gradient Descent(14/49): loss=0.4501730689054276\n",
      "Gradient Descent(15/49): loss=0.45017306882531327\n",
      "Gradient Descent(16/49): loss=0.4501730688060778\n",
      "Gradient Descent(17/49): loss=0.45017306880145924\n",
      "Gradient Descent(18/49): loss=0.4501730688003505\n",
      "Gradient Descent(19/49): loss=0.45017306880008423\n",
      "Gradient Descent(20/49): loss=0.45017306880002017\n",
      "Gradient Descent(21/49): loss=0.45017306880000474\n",
      "Gradient Descent(22/49): loss=0.45017306880000124\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688000001\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.45017306879999985\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.45017306879999985\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730687999999\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46154298782597103\n",
      "Gradient Descent(2/49): loss=0.453405484049947\n",
      "Gradient Descent(3/49): loss=0.4516835882509398\n",
      "Gradient Descent(4/49): loss=0.4513192350998701\n",
      "Gradient Descent(5/49): loss=0.4512421379731037\n",
      "Gradient Descent(6/49): loss=0.45122582422108004\n",
      "Gradient Descent(7/49): loss=0.45122237223115175\n",
      "Gradient Descent(8/49): loss=0.45122164179008273\n",
      "Gradient Descent(9/49): loss=0.4512214872287526\n",
      "Gradient Descent(10/49): loss=0.4512214545235753\n",
      "Gradient Descent(11/49): loss=0.45122144760315974\n",
      "Gradient Descent(12/49): loss=0.45122144613879983\n",
      "Gradient Descent(13/49): loss=0.4512214458289411\n",
      "Gradient Descent(14/49): loss=0.45122144576337514\n",
      "Gradient Descent(15/49): loss=0.4512214457495015\n",
      "Gradient Descent(16/49): loss=0.4512214457465658\n",
      "Gradient Descent(17/49): loss=0.4512214457459446\n",
      "Gradient Descent(18/49): loss=0.4512214457458133\n",
      "Gradient Descent(19/49): loss=0.45122144574578527\n",
      "Gradient Descent(20/49): loss=0.4512214457457791\n",
      "Gradient Descent(21/49): loss=0.45122144574577805\n",
      "Gradient Descent(22/49): loss=0.45122144574577805\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.45122144574577755\n",
      "Gradient Descent(26/49): loss=0.45122144574577755\n",
      "Gradient Descent(27/49): loss=0.45122144574577766\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.45122144574577755\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457778\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46057827955281927\n",
      "Gradient Descent(2/49): loss=0.45223664350619547\n",
      "Gradient Descent(3/49): loss=0.4504715533187302\n",
      "Gradient Descent(4/49): loss=0.45009806023506255\n",
      "Gradient Descent(5/49): loss=0.4500190290985584\n",
      "Gradient Descent(6/49): loss=0.4500023061100742\n",
      "Gradient Descent(7/49): loss=0.44999876752571094\n",
      "Gradient Descent(8/49): loss=0.4499980187612595\n",
      "Gradient Descent(9/49): loss=0.4499978603227017\n",
      "Gradient Descent(10/49): loss=0.4499978267971027\n",
      "Gradient Descent(11/49): loss=0.4499978197030863\n",
      "Gradient Descent(12/49): loss=0.44999781820199225\n",
      "Gradient Descent(13/49): loss=0.4499978178843608\n",
      "Gradient Descent(14/49): loss=0.44999781781714976\n",
      "Gradient Descent(15/49): loss=0.44999781780292825\n",
      "Gradient Descent(16/49): loss=0.44999781779991865\n",
      "Gradient Descent(17/49): loss=0.44999781779928194\n",
      "Gradient Descent(18/49): loss=0.4499978177991472\n",
      "Gradient Descent(19/49): loss=0.44999781779911874\n",
      "Gradient Descent(20/49): loss=0.4499978177991127\n",
      "Gradient Descent(21/49): loss=0.4499978177991116\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.44999781779911097\n",
      "Gradient Descent(27/49): loss=0.44999781779911097\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991113\n",
      "Gradient Descent(31/49): loss=0.4499978177991109\n",
      "Gradient Descent(32/49): loss=0.4499978177991113\n",
      "Gradient Descent(33/49): loss=0.44999781779911097\n",
      "Gradient Descent(34/49): loss=0.4499978177991109\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.44999781779911097\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.4499978177991112\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.4499978177991112\n",
      "Gradient Descent(46/49): loss=0.44999781779911135\n",
      "Gradient Descent(47/49): loss=0.44999781779911135\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4610343706880001\n",
      "Gradient Descent(2/49): loss=0.452789243525581\n",
      "Gradient Descent(3/49): loss=0.451044574618013\n",
      "Gradient Descent(4/49): loss=0.4506754026771715\n",
      "Gradient Descent(5/49): loss=0.4505972858944894\n",
      "Gradient Descent(6/49): loss=0.4505807563832739\n",
      "Gradient Descent(7/49): loss=0.45057725873870086\n",
      "Gradient Descent(8/49): loss=0.45057651863710907\n",
      "Gradient Descent(9/49): loss=0.45057636203161233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=0.4505763288938894\n",
      "Gradient Descent(11/49): loss=0.45057632188194696\n",
      "Gradient Descent(12/49): loss=0.45057632039822004\n",
      "Gradient Descent(13/49): loss=0.45057632008426335\n",
      "Gradient Descent(14/49): loss=0.45057632001783016\n",
      "Gradient Descent(15/49): loss=0.45057632000377285\n",
      "Gradient Descent(16/49): loss=0.4505763200007982\n",
      "Gradient Descent(17/49): loss=0.4505763200001689\n",
      "Gradient Descent(18/49): loss=0.45057632000003567\n",
      "Gradient Descent(19/49): loss=0.4505763200000076\n",
      "Gradient Descent(20/49): loss=0.45057632000000164\n",
      "Gradient Descent(21/49): loss=0.45057632000000014\n",
      "Gradient Descent(22/49): loss=0.45057632000000014\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.45057632\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.45057632000000014\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763200000001\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632000000014\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.4505763199999998\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763200000001\n",
      "Gradient Descent(45/49): loss=0.4505763200000002\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4607164474419201\n",
      "Gradient Descent(2/49): loss=0.45240404772063003\n",
      "Gradient Descent(3/49): loss=0.45064514393960553\n",
      "Gradient Descent(4/49): loss=0.4502729598995405\n",
      "Gradient Descent(5/49): loss=0.45019420575666275\n",
      "Gradient Descent(6/49): loss=0.4501775413800297\n",
      "Gradient Descent(7/49): loss=0.45017401519793426\n",
      "Gradient Descent(8/49): loss=0.45017326905780286\n",
      "Gradient Descent(9/49): loss=0.45017311117455105\n",
      "Gradient Descent(10/49): loss=0.45017307776645493\n",
      "Gradient Descent(11/49): loss=0.45017307069730206\n",
      "Gradient Descent(12/49): loss=0.4501730692014691\n",
      "Gradient Descent(13/49): loss=0.45017306888495084\n",
      "Gradient Descent(14/49): loss=0.45017306881797564\n",
      "Gradient Descent(15/49): loss=0.45017306880380353\n",
      "Gradient Descent(16/49): loss=0.4501730688008048\n",
      "Gradient Descent(17/49): loss=0.4501730688001703\n",
      "Gradient Descent(18/49): loss=0.45017306880003605\n",
      "Gradient Descent(19/49): loss=0.4501730688000078\n",
      "Gradient Descent(20/49): loss=0.45017306880000174\n",
      "Gradient Descent(21/49): loss=0.4501730688000004\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730687999999\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.45017306879999985\n",
      "Gradient Descent(29/49): loss=0.4501730687999999\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730687999999\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4602406004273834\n",
      "Gradient Descent(2/49): loss=0.4528890874464067\n",
      "Gradient Descent(3/49): loss=0.451529792696224\n",
      "Gradient Descent(4/49): loss=0.4512784590969155\n",
      "Gradient Descent(5/49): loss=0.45123198751440313\n",
      "Gradient Descent(6/49): loss=0.45122339491879637\n",
      "Gradient Descent(7/49): loss=0.45122180614786905\n",
      "Gradient Descent(8/49): loss=0.4512215123841245\n",
      "Gradient Descent(9/49): loss=0.45122145806720804\n",
      "Gradient Descent(10/49): loss=0.4512214480240102\n",
      "Gradient Descent(11/49): loss=0.45122144616702275\n",
      "Gradient Descent(12/49): loss=0.4512214458236657\n",
      "Gradient Descent(13/49): loss=0.45122144576017936\n",
      "Gradient Descent(14/49): loss=0.4512214457484406\n",
      "Gradient Descent(15/49): loss=0.4512214457462703\n",
      "Gradient Descent(16/49): loss=0.45122144574586887\n",
      "Gradient Descent(17/49): loss=0.45122144574579465\n",
      "Gradient Descent(18/49): loss=0.451221445745781\n",
      "Gradient Descent(19/49): loss=0.45122144574577844\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.45122144574577794\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.45122144574577766\n",
      "Gradient Descent(26/49): loss=0.45122144574577766\n",
      "Gradient Descent(27/49): loss=0.4512214457457779\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.45122144574577794\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457778\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4592432212880554\n",
      "Gradient Descent(2/49): loss=0.4517072929042168\n",
      "Gradient Descent(3/49): loss=0.45031389974604524\n",
      "Gradient Descent(4/49): loss=0.45005626135109933\n",
      "Gradient Descent(5/49): loss=0.4500086240118738\n",
      "Gradient Descent(6/49): loss=0.44999981586785104\n",
      "Gradient Descent(7/49): loss=0.44999818724202123\n",
      "Gradient Descent(8/49): loss=0.44999788610910507\n",
      "Gradient Descent(9/49): loss=0.4499978304296291\n",
      "Gradient Descent(10/49): loss=0.44999782013449396\n",
      "Gradient Descent(11/49): loss=0.44999781823092333\n",
      "Gradient Descent(12/49): loss=0.44999781787895315\n",
      "Gradient Descent(13/49): loss=0.4499978178138738\n",
      "Gradient Descent(14/49): loss=0.44999781780184067\n",
      "Gradient Descent(15/49): loss=0.44999781779961573\n",
      "Gradient Descent(16/49): loss=0.4499978177992044\n",
      "Gradient Descent(17/49): loss=0.4499978177991286\n",
      "Gradient Descent(18/49): loss=0.4499978177991144\n",
      "Gradient Descent(19/49): loss=0.4499978177991117\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.44999781779911135\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.4499978177991108\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.44999781779911097\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991111\n",
      "Gradient Descent(30/49): loss=0.4499978177991113\n",
      "Gradient Descent(31/49): loss=0.44999781779911097\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.4499978177991109\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991112\n",
      "Gradient Descent(37/49): loss=0.4499978177991113\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.44999781779911135\n",
      "Gradient Descent(40/49): loss=0.4499978177991112\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.44999781779911135\n",
      "Gradient Descent(43/49): loss=0.44999781779911135\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.4499978177991111\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4597147584320001\n",
      "Gradient Descent(2/49): loss=0.4522660172660767\n",
      "Gradient Descent(3/49): loss=0.45088874502449744\n",
      "Gradient Descent(4/49): loss=0.45063408738702954\n",
      "Gradient Descent(5/49): loss=0.45058700118986167\n",
      "Gradient Descent(6/49): loss=0.45057829495200535\n",
      "Gradient Descent(7/49): loss=0.4505766851686258\n",
      "Gradient Descent(8/49): loss=0.45057638751967893\n",
      "Gradient Descent(9/49): loss=0.45057633248438866\n",
      "Gradient Descent(10/49): loss=0.4505763223083634\n",
      "Gradient Descent(11/49): loss=0.45057632042681645\n",
      "Gradient Descent(12/49): loss=0.45057632007891835\n",
      "Gradient Descent(13/49): loss=0.4505763200145921\n",
      "Gradient Descent(14/49): loss=0.45057632000269804\n",
      "Gradient Descent(15/49): loss=0.4505763200004988\n",
      "Gradient Descent(16/49): loss=0.45057632000009235\n",
      "Gradient Descent(17/49): loss=0.45057632000001685\n",
      "Gradient Descent(18/49): loss=0.4505763200000032\n",
      "Gradient Descent(19/49): loss=0.4505763200000005\n",
      "Gradient Descent(20/49): loss=0.45057632000000014\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.45057632\n",
      "Gradient Descent(25/49): loss=0.4505763199999998\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999998\n",
      "Gradient Descent(34/49): loss=0.4505763199999998\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.45057631999999975\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763200000002\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763200000002\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.4505763200000002\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4593860683788801\n",
      "Gradient Descent(2/49): loss=0.451876552422135\n",
      "Gradient Descent(3/49): loss=0.45048804292173295\n",
      "Gradient Descent(4/49): loss=0.4502313075151084\n",
      "Gradient Descent(5/49): loss=0.4501838371384235\n",
      "Gradient Descent(6/49): loss=0.45017505986577444\n",
      "Gradient Descent(7/49): loss=0.45017343694806183\n",
      "Gradient Descent(8/49): loss=0.4501731368705766\n",
      "Gradient Descent(9/49): loss=0.45017308138624984\n",
      "Gradient Descent(10/49): loss=0.45017307112719757\n",
      "Gradient Descent(11/49): loss=0.45017306923029915\n",
      "Gradient Descent(12/49): loss=0.4501730688795621\n",
      "Gradient Descent(13/49): loss=0.4501730688147111\n",
      "Gradient Descent(14/49): loss=0.45017306880271996\n",
      "Gradient Descent(15/49): loss=0.450173068800503\n",
      "Gradient Descent(16/49): loss=0.45017306880009306\n",
      "Gradient Descent(17/49): loss=0.45017306880001723\n",
      "Gradient Descent(18/49): loss=0.45017306880000335\n",
      "Gradient Descent(19/49): loss=0.4501730688000007\n",
      "Gradient Descent(20/49): loss=0.4501730688000001\n",
      "Gradient Descent(21/49): loss=0.4501730688000001\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.45017306879999985\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730687999999\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730687999999\n",
      "Gradient Descent(36/49): loss=0.4501730688000001\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730687999999\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45902601442645313\n",
      "Gradient Descent(2/49): loss=0.4524701767346859\n",
      "Gradient Descent(3/49): loss=0.451421242704003\n",
      "Gradient Descent(4/49): loss=0.4512534132590936\n",
      "Gradient Descent(5/49): loss=0.45122656054790833\n",
      "Gradient Descent(6/49): loss=0.4512222641141185\n",
      "Gradient Descent(7/49): loss=0.4512215766847124\n",
      "Gradient Descent(8/49): loss=0.4512214666960073\n",
      "Gradient Descent(9/49): loss=0.4512214490978146\n",
      "Gradient Descent(10/49): loss=0.45122144628210376\n",
      "Gradient Descent(11/49): loss=0.45122144583158985\n",
      "Gradient Descent(12/49): loss=0.4512214457595076\n",
      "Gradient Descent(13/49): loss=0.45122144574797457\n",
      "Gradient Descent(14/49): loss=0.4512214457461292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=0.451221445745834\n",
      "Gradient Descent(16/49): loss=0.4512214457457867\n",
      "Gradient Descent(17/49): loss=0.4512214457457792\n",
      "Gradient Descent(18/49): loss=0.4512214457457781\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.45122144574577766\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.45122144574577766\n",
      "Gradient Descent(32/49): loss=0.45122144574577766\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45799816695125334\n",
      "Gradient Descent(2/49): loss=0.45127787366345395\n",
      "Gradient Descent(3/49): loss=0.45020262673740585\n",
      "Gradient Descent(4/49): loss=0.4500305872292382\n",
      "Gradient Descent(5/49): loss=0.4500030609079313\n",
      "Gradient Descent(6/49): loss=0.4499986566965224\n",
      "Gradient Descent(7/49): loss=0.4499979520226968\n",
      "Gradient Descent(8/49): loss=0.4499978392748849\n",
      "Gradient Descent(9/49): loss=0.44999782123523485\n",
      "Gradient Descent(10/49): loss=0.44999781834889097\n",
      "Gradient Descent(11/49): loss=0.44999781788707593\n",
      "Gradient Descent(12/49): loss=0.4499978178131855\n",
      "Gradient Descent(13/49): loss=0.4499978178013628\n",
      "Gradient Descent(14/49): loss=0.44999781779947146\n",
      "Gradient Descent(15/49): loss=0.4499978177991688\n",
      "Gradient Descent(16/49): loss=0.44999781779912046\n",
      "Gradient Descent(17/49): loss=0.44999781779911263\n",
      "Gradient Descent(18/49): loss=0.4499978177991112\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.44999781779911097\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.44999781779911097\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911097\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911074\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.44999781779911135\n",
      "Gradient Descent(37/49): loss=0.4499978177991112\n",
      "Gradient Descent(38/49): loss=0.4499978177991112\n",
      "Gradient Descent(39/49): loss=0.44999781779911135\n",
      "Gradient Descent(40/49): loss=0.44999781779911135\n",
      "Gradient Descent(41/49): loss=0.44999781779911135\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4584841087999999\n",
      "Gradient Descent(2/49): loss=0.45184156620800014\n",
      "Gradient Descent(3/49): loss=0.45077875939328\n",
      "Gradient Descent(4/49): loss=0.45060871030292476\n",
      "Gradient Descent(5/49): loss=0.450581502448468\n",
      "Gradient Descent(6/49): loss=0.45057714919175484\n",
      "Gradient Descent(7/49): loss=0.4505764526706808\n",
      "Gradient Descent(8/49): loss=0.4505763412273091\n",
      "Gradient Descent(9/49): loss=0.45057632339636927\n",
      "Gradient Descent(10/49): loss=0.450576320543419\n",
      "Gradient Descent(11/49): loss=0.4505763200869471\n",
      "Gradient Descent(12/49): loss=0.4505763200139115\n",
      "Gradient Descent(13/49): loss=0.450576320002226\n",
      "Gradient Descent(14/49): loss=0.4505763200003563\n",
      "Gradient Descent(15/49): loss=0.4505763200000571\n",
      "Gradient Descent(16/49): loss=0.4505763200000093\n",
      "Gradient Descent(17/49): loss=0.45057632000000153\n",
      "Gradient Descent(18/49): loss=0.4505763200000003\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.45057632000000014\n",
      "Gradient Descent(22/49): loss=0.45057632000000014\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.45057632000000014\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763200000001\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999998\n",
      "Gradient Descent(32/49): loss=0.4505763200000002\n",
      "Gradient Descent(33/49): loss=0.45057632\n",
      "Gradient Descent(34/49): loss=0.45057631999999975\n",
      "Gradient Descent(35/49): loss=0.45057631999999975\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.4505763200000002\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45814537779199993\n",
      "Gradient Descent(2/49): loss=0.45144863823871983\n",
      "Gradient Descent(3/49): loss=0.45037715991019533\n",
      "Gradient Descent(4/49): loss=0.4502057233776312\n",
      "Gradient Descent(5/49): loss=0.4501782935324211\n",
      "Gradient Descent(6/49): loss=0.4501739047571873\n",
      "Gradient Descent(7/49): loss=0.45017320255314985\n",
      "Gradient Descent(8/49): loss=0.45017309020050383\n",
      "Gradient Descent(9/49): loss=0.4501730722240807\n",
      "Gradient Descent(10/49): loss=0.4501730693478529\n",
      "Gradient Descent(11/49): loss=0.45017306888765646\n",
      "Gradient Descent(12/49): loss=0.4501730688140251\n",
      "Gradient Descent(13/49): loss=0.450173068802244\n",
      "Gradient Descent(14/49): loss=0.4501730688003592\n",
      "Gradient Descent(15/49): loss=0.4501730688000576\n",
      "Gradient Descent(16/49): loss=0.45017306880000935\n",
      "Gradient Descent(17/49): loss=0.45017306880000135\n",
      "Gradient Descent(18/49): loss=0.45017306880000024\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730687999999\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000001\n",
      "Gradient Descent(25/49): loss=0.4501730688000001\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730687999999\n",
      "Gradient Descent(29/49): loss=0.4501730688000001\n",
      "Gradient Descent(30/49): loss=0.4501730687999999\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45789922982318093\n",
      "Gradient Descent(2/49): loss=0.4521356343859743\n",
      "Gradient Descent(3/49): loss=0.45134659817062056\n",
      "Gradient Descent(4/49): loss=0.45123857911273874\n",
      "Gradient Descent(5/49): loss=0.4512237913037148\n",
      "Gradient Descent(6/49): loss=0.45122176685265936\n",
      "Gradient Descent(7/49): loss=0.45122148970530984\n",
      "Gradient Descent(8/49): loss=0.45122145176383766\n",
      "Gradient Descent(9/49): loss=0.4512214465696503\n",
      "Gradient Descent(10/49): loss=0.451221445858566\n",
      "Gradient Descent(11/49): loss=0.45122144576121853\n",
      "Gradient Descent(12/49): loss=0.45122144574789175\n",
      "Gradient Descent(13/49): loss=0.4512214457460672\n",
      "Gradient Descent(14/49): loss=0.4512214457458174\n",
      "Gradient Descent(15/49): loss=0.4512214457457831\n",
      "Gradient Descent(16/49): loss=0.45122144574577844\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.45122144574577766\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.45122144574577766\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457779\n",
      "Gradient Descent(30/49): loss=0.45122144574577794\n",
      "Gradient Descent(31/49): loss=0.45122144574577794\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.4512214457457778\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45684311654241283\n",
      "Gradient Descent(2/49): loss=0.4509349391970693\n",
      "Gradient Descent(3/49): loss=0.4501261097184917\n",
      "Gradient Descent(4/49): loss=0.4500153809628744\n",
      "Gradient Descent(5/49): loss=0.4500002221962302\n",
      "Gradient Descent(6/49): loss=0.4499981469610767\n",
      "Gradient Descent(7/49): loss=0.4499978628613841\n",
      "Gradient Descent(8/49): loss=0.44999782396813637\n",
      "Gradient Descent(9/49): loss=0.44999781864365057\n",
      "Gradient Descent(10/49): loss=0.44999781791472854\n",
      "Gradient Descent(11/49): loss=0.4499978178149392\n",
      "Gradient Descent(12/49): loss=0.44999781780127796\n",
      "Gradient Descent(13/49): loss=0.4499978177994076\n",
      "Gradient Descent(14/49): loss=0.44999781779915166\n",
      "Gradient Descent(15/49): loss=0.4499978177991166\n",
      "Gradient Descent(16/49): loss=0.44999781779911185\n",
      "Gradient Descent(17/49): loss=0.4499978177991113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.4499978177991111\n",
      "Gradient Descent(20/49): loss=0.4499978177991113\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991109\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991109\n",
      "Gradient Descent(29/49): loss=0.4499978177991113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911135\n",
      "Gradient Descent(34/49): loss=0.4499978177991112\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.44999781779911135\n",
      "Gradient Descent(37/49): loss=0.44999781779911135\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45734242179199996\n",
      "Gradient Descent(2/49): loss=0.4515025993353247\n",
      "Gradient Descent(3/49): loss=0.4507031276410061\n",
      "Gradient Descent(4/49): loss=0.45059367996605376\n",
      "Gradient Descent(5/49): loss=0.45057869657935296\n",
      "Gradient Descent(6/49): loss=0.4505766453537133\n",
      "Gradient Descent(7/49): loss=0.4505763645409233\n",
      "Gradient Descent(8/49): loss=0.45057632609765247\n",
      "Gradient Descent(9/49): loss=0.45057632083476845\n",
      "Gradient Descent(10/49): loss=0.4505763201142798\n",
      "Gradient Descent(11/49): loss=0.4505763200156448\n",
      "Gradient Descent(12/49): loss=0.4505763200021419\n",
      "Gradient Descent(13/49): loss=0.4505763200002932\n",
      "Gradient Descent(14/49): loss=0.4505763200000401\n",
      "Gradient Descent(15/49): loss=0.45057632000000536\n",
      "Gradient Descent(16/49): loss=0.4505763200000007\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.4505763199999998\n",
      "Gradient Descent(20/49): loss=0.45057632000000014\n",
      "Gradient Descent(21/49): loss=0.45057632000000014\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763199999998\n",
      "Gradient Descent(24/49): loss=0.45057632\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.4505763200000002\n",
      "Gradient Descent(31/49): loss=0.45057631999999975\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45699437568128\n",
      "Gradient Descent(2/49): loss=0.45110690571204726\n",
      "Gradient Descent(3/49): loss=0.4503009110732591\n",
      "Gradient Descent(4/49): loss=0.45019057040720906\n",
      "Gradient Descent(5/49): loss=0.45017546477002696\n",
      "Gradient Descent(6/49): loss=0.45017339680829666\n",
      "Gradient Descent(7/49): loss=0.45017311370433594\n",
      "Gradient Descent(8/49): loss=0.4501730749474035\n",
      "Gradient Descent(9/49): loss=0.4501730696415795\n",
      "Gradient Descent(10/49): loss=0.4501730689152123\n",
      "Gradient Descent(11/49): loss=0.4501730688157727\n",
      "Gradient Descent(12/49): loss=0.45017306880215935\n",
      "Gradient Descent(13/49): loss=0.45017306880029584\n",
      "Gradient Descent(14/49): loss=0.45017306880004043\n",
      "Gradient Descent(15/49): loss=0.45017306880000557\n",
      "Gradient Descent(16/49): loss=0.45017306880000063\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730687999999\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730687999999\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000001\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4568602466175661\n",
      "Gradient Descent(2/49): loss=0.4518732911265564\n",
      "Gradient Descent(3/49): loss=0.451296799071796\n",
      "Gradient Descent(4/49): loss=0.4512301565902653\n",
      "Gradient Descent(5/49): loss=0.4512224527194005\n",
      "Gradient Descent(6/49): loss=0.45122156215192866\n",
      "Gradient Descent(7/49): loss=0.45122145920232876\n",
      "Gradient Descent(8/49): loss=0.4512214473013552\n",
      "Gradient Descent(9/49): loss=0.4512214459256026\n",
      "Gradient Descent(10/49): loss=0.45122144576656564\n",
      "Gradient Descent(11/49): loss=0.4512214457481808\n",
      "Gradient Descent(12/49): loss=0.4512214457460557\n",
      "Gradient Descent(13/49): loss=0.4512214457458099\n",
      "Gradient Descent(14/49): loss=0.4512214457457814\n",
      "Gradient Descent(15/49): loss=0.4512214457457782\n",
      "Gradient Descent(16/49): loss=0.4512214457457778\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.45122144574577766\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.45122144574577766\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.45122144574577766\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457779\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4557780700615339\n",
      "Gradient Descent(2/49): loss=0.4506660149606472\n",
      "Gradient Descent(3/49): loss=0.4500750613909846\n",
      "Gradient Descent(4/49): loss=0.4500067471583319\n",
      "Gradient Descent(5/49): loss=0.44999885003303713\n",
      "Gradient Descent(6/49): loss=0.4499979371253531\n",
      "Gradient Descent(7/49): loss=0.44999783159322465\n",
      "Gradient Descent(8/49): loss=0.44999781939371053\n",
      "Gradient Descent(9/49): loss=0.4499978179834468\n",
      "Gradient Descent(10/49): loss=0.4499978178204204\n",
      "Gradient Descent(11/49): loss=0.4499978178015745\n",
      "Gradient Descent(12/49): loss=0.44999781779939596\n",
      "Gradient Descent(13/49): loss=0.44999781779914383\n",
      "Gradient Descent(14/49): loss=0.4499978177991149\n",
      "Gradient Descent(15/49): loss=0.44999781779911135\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911097\n",
      "Gradient Descent(21/49): loss=0.44999781779911097\n",
      "Gradient Descent(22/49): loss=0.4499978177991109\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.4499978177991112\n",
      "Gradient Descent(33/49): loss=0.44999781779911135\n",
      "Gradient Descent(34/49): loss=0.44999781779911135\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45628969740799985\n",
      "Gradient Descent(2/49): loss=0.4512367864283647\n",
      "Gradient Descent(3/49): loss=0.45065266991911906\n",
      "Gradient Descent(4/49): loss=0.4505851460506503\n",
      "Gradient Descent(5/49): loss=0.450577340291455\n",
      "Gradient Descent(6/49): loss=0.4505764379456921\n",
      "Gradient Descent(7/49): loss=0.450576333634522\n",
      "Gradient Descent(8/49): loss=0.4505763215761507\n",
      "Gradient Descent(9/49): loss=0.45057632018220317\n",
      "Gradient Descent(10/49): loss=0.45057632002106246\n",
      "Gradient Descent(11/49): loss=0.45057632000243486\n",
      "Gradient Descent(12/49): loss=0.45057632000028147\n",
      "Gradient Descent(13/49): loss=0.45057632000003267\n",
      "Gradient Descent(14/49): loss=0.45057632000000386\n",
      "Gradient Descent(15/49): loss=0.4505763200000003\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.4505763199999998\n",
      "Gradient Descent(22/49): loss=0.45057632000000014\n",
      "Gradient Descent(23/49): loss=0.45057632000000014\n",
      "Gradient Descent(24/49): loss=0.45057632000000014\n",
      "Gradient Descent(25/49): loss=0.4505763199999998\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.45057632\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45593306204672013\n",
      "Gradient Descent(2/49): loss=0.4508389240193207\n",
      "Gradient Descent(3/49): loss=0.4502500416633535\n",
      "Gradient Descent(4/49): loss=0.4501819668630037\n",
      "Gradient Descent(5/49): loss=0.4501740974160835\n",
      "Gradient Descent(6/49): loss=0.4501731877080191\n",
      "Gradient Descent(7/49): loss=0.45017308254576693\n",
      "Gradient Descent(8/49): loss=0.45017307038901055\n",
      "Gradient Descent(9/49): loss=0.45017306898368975\n",
      "Gradient Descent(10/49): loss=0.45017306882123453\n",
      "Gradient Descent(11/49): loss=0.4501730688024549\n",
      "Gradient Descent(12/49): loss=0.4501730688002837\n",
      "Gradient Descent(13/49): loss=0.4501730688000328\n",
      "Gradient Descent(14/49): loss=0.4501730688000039\n",
      "Gradient Descent(15/49): loss=0.4501730688000004\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.4501730688000001\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45590906480960874\n",
      "Gradient Descent(2/49): loss=0.451671925937812\n",
      "Gradient Descent(3/49): loss=0.45126473689223234\n",
      "Gradient Descent(4/49): loss=0.451225606024952\n",
      "Gradient Descent(5/49): loss=0.4512218455486064\n",
      "Gradient Descent(6/49): loss=0.45122148416682945\n",
      "Gradient Descent(7/49): loss=0.451221449438041\n",
      "Gradient Descent(8/49): loss=0.4512214461006043\n",
      "Gradient Descent(9/49): loss=0.4512214457798766\n",
      "Gradient Descent(10/49): loss=0.45122144574905476\n",
      "Gradient Descent(11/49): loss=0.4512214457460927\n",
      "Gradient Descent(12/49): loss=0.45122144574580814\n",
      "Gradient Descent(13/49): loss=0.45122144574578066\n",
      "Gradient Descent(14/49): loss=0.45122144574577827\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457778\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.45122144574577766\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.45122144574577794\n",
      "Gradient Descent(23/49): loss=0.45122144574577794\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45480302750861645\n",
      "Gradient Descent(2/49): loss=0.4504595984521947\n",
      "Gradient Descent(3/49): loss=0.4500421949198724\n",
      "Gradient Descent(4/49): loss=0.45000208244041623\n",
      "Gradient Descent(5/49): loss=0.44999822763114067\n",
      "Gradient Descent(6/49): loss=0.4499978571839692\n",
      "Gradient Descent(7/49): loss=0.449997821583996\n",
      "Gradient Descent(8/49): loss=0.4499978181628387\n",
      "Gradient Descent(9/49): loss=0.44999781783406545\n",
      "Gradient Descent(10/49): loss=0.44999781780247033\n",
      "Gradient Descent(11/49): loss=0.44999781779943393\n",
      "Gradient Descent(12/49): loss=0.44999781779914194\n",
      "Gradient Descent(13/49): loss=0.4499978177991141\n",
      "Gradient Descent(14/49): loss=0.44999781779911135\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.4499978177991113\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991109\n",
      "Gradient Descent(26/49): loss=0.4499978177991113\n",
      "Gradient Descent(27/49): loss=0.44999781779911135\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.44999781779911135\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.4499978177991111\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.455325935648\n",
      "Gradient Descent(2/49): loss=0.451032758063773\n",
      "Gradient Descent(3/49): loss=0.4506201836979286\n",
      "Gradient Descent(4/49): loss=0.4505805353013709\n",
      "Gradient Descent(5/49): loss=0.4505767250904617\n",
      "Gradient Descent(6/49): loss=0.4505763589291932\n",
      "Gradient Descent(7/49): loss=0.45057632374109546\n",
      "Gradient Descent(8/49): loss=0.4505763203595192\n",
      "Gradient Descent(9/49): loss=0.45057632003454967\n",
      "Gradient Descent(10/49): loss=0.4505763200033202\n",
      "Gradient Descent(11/49): loss=0.4505763200003191\n",
      "Gradient Descent(12/49): loss=0.4505763200000306\n",
      "Gradient Descent(13/49): loss=0.4505763200000028\n",
      "Gradient Descent(14/49): loss=0.4505763200000002\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763199999998\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.45057632000000014\n",
      "Gradient Descent(26/49): loss=0.45057631999999975\n",
      "Gradient Descent(27/49): loss=0.45057632\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763200000002\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45496143688832014\n",
      "Gradient Descent(2/49): loss=0.4506332309732874\n",
      "Gradient Descent(3/49): loss=0.450217290384853\n",
      "Gradient Descent(4/49): loss=0.45017731849430437\n",
      "Gradient Descent(5/49): loss=0.4501734771956228\n",
      "Gradient Descent(6/49): loss=0.4501731080468193\n",
      "Gradient Descent(7/49): loss=0.4501730725716192\n",
      "Gradient Descent(8/49): loss=0.4501730691624526\n",
      "Gradient Descent(9/49): loss=0.45017306883483155\n",
      "Gradient Descent(10/49): loss=0.4501730688033475\n",
      "Gradient Descent(11/49): loss=0.45017306880032176\n",
      "Gradient Descent(12/49): loss=0.45017306880003083\n",
      "Gradient Descent(13/49): loss=0.45017306880000313\n",
      "Gradient Descent(14/49): loss=0.4501730688000003\n",
      "Gradient Descent(15/49): loss=0.45017306879999985\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730687999999\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000001\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45504568439930887\n",
      "Gradient Descent(2/49): loss=0.4515212660562147\n",
      "Gradient Descent(3/49): loss=0.4512449516581162\n",
      "Gradient Descent(4/49): loss=0.451223288609305\n",
      "Gradient Descent(5/49): loss=0.4512215902262783\n",
      "Gradient Descent(6/49): loss=0.451221457073049\n",
      "Gradient Descent(7/49): loss=0.4512214466338358\n",
      "Gradient Descent(8/49): loss=0.45122144581540147\n",
      "Gradient Descent(9/49): loss=0.4512214457512361\n",
      "Gradient Descent(10/49): loss=0.45122144574620565\n",
      "Gradient Descent(11/49): loss=0.4512214457458111\n",
      "Gradient Descent(12/49): loss=0.4512214457457804\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.45122144574577766\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.45122144574577766\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.45122144574577805\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457779\n",
      "Gradient Descent(22/49): loss=0.45122144574577766\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.45122144574577794\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.45122144574577805\n",
      "Gradient Descent(27/49): loss=0.4512214457457779\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45391798888366097\n",
      "Gradient Descent(2/49): loss=0.4503051592121399\n",
      "Gradient Descent(3/49): loss=0.4500219133658926\n",
      "Gradient Descent(4/49): loss=0.44999970689154695\n",
      "Gradient Descent(5/49): loss=0.4499979659039581\n",
      "Gradient Descent(6/49): loss=0.44999782941053124\n",
      "Gradient Descent(7/49): loss=0.4499978187094465\n",
      "Gradient Descent(8/49): loss=0.4499978178704815\n",
      "Gradient Descent(9/49): loss=0.4499978178047064\n",
      "Gradient Descent(10/49): loss=0.44999781779954995\n",
      "Gradient Descent(11/49): loss=0.44999781779914544\n",
      "Gradient Descent(12/49): loss=0.44999781779911385\n",
      "Gradient Descent(13/49): loss=0.44999781779911135\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.4499978177991109\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.4499978177991111\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.44999781779911135\n",
      "Gradient Descent(29/49): loss=0.44999781779911135\n",
      "Gradient Descent(30/49): loss=0.44999781779911135\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45445113651200003\n",
      "Gradient Descent(2/49): loss=0.45088010561454084\n",
      "Gradient Descent(3/49): loss=0.45060013679218003\n",
      "Gradient Descent(4/49): loss=0.4505781872365067\n",
      "Gradient Descent(5/49): loss=0.4505764663913423\n",
      "Gradient Descent(6/49): loss=0.4505763314770813\n",
      "Gradient Descent(7/49): loss=0.45057632089980304\n",
      "Gradient Descent(8/49): loss=0.45057632007054466\n",
      "Gradient Descent(9/49): loss=0.4505763200055305\n",
      "Gradient Descent(10/49): loss=0.4505763200004335\n",
      "Gradient Descent(11/49): loss=0.45057632000003406\n",
      "Gradient Descent(12/49): loss=0.45057632000000264\n",
      "Gradient Descent(13/49): loss=0.4505763200000002\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763199999998\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763200000002\n",
      "Gradient Descent(25/49): loss=0.4505763199999997\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45407950020607996\n",
      "Gradient Descent(2/49): loss=0.4504793330222366\n",
      "Gradient Descent(3/49): loss=0.4501970799150233\n",
      "Gradient Descent(4/49): loss=0.45017495127141766\n",
      "Gradient Descent(5/49): loss=0.45017321638575897\n",
      "Gradient Descent(6/49): loss=0.45017308037072334\n",
      "Gradient Descent(7/49): loss=0.4501730697071448\n",
      "Gradient Descent(8/49): loss=0.45017306887112024\n",
      "Gradient Descent(9/49): loss=0.45017306880557584\n",
      "Gradient Descent(10/49): loss=0.450173068800437\n",
      "Gradient Descent(11/49): loss=0.4501730688000344\n",
      "Gradient Descent(12/49): loss=0.45017306880000263\n",
      "Gradient Descent(13/49): loss=0.4501730688000003\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688000001\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.45017306879999985\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730687999998\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4542701053866665\n",
      "Gradient Descent(2/49): loss=0.4514119869733334\n",
      "Gradient Descent(3/49): loss=0.4512333545725\n",
      "Gradient Descent(4/49): loss=0.4512221900474479\n",
      "Gradient Descent(5/49): loss=0.45122149226463215\n",
      "Gradient Descent(6/49): loss=0.45122144865320624\n",
      "Gradient Descent(7/49): loss=0.45122144592749214\n",
      "Gradient Descent(8/49): loss=0.4512214457571348\n",
      "Gradient Descent(9/49): loss=0.45122144574648737\n",
      "Gradient Descent(10/49): loss=0.4512214457458222\n",
      "Gradient Descent(11/49): loss=0.45122144574578044\n",
      "Gradient Descent(12/49): loss=0.45122144574577805\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.45122144574577755\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457779\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.45122144574577805\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457779\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4531229541866666\n",
      "Gradient Descent(2/49): loss=0.45019313882333334\n",
      "Gradient Descent(3/49): loss=0.4500100253631249\n",
      "Gradient Descent(4/49): loss=0.44999858077186217\n",
      "Gradient Descent(5/49): loss=0.449997865484908\n",
      "Gradient Descent(6/49): loss=0.44999782077947353\n",
      "Gradient Descent(7/49): loss=0.44999781798538396\n",
      "Gradient Descent(8/49): loss=0.44999781781075343\n",
      "Gradient Descent(9/49): loss=0.44999781779983855\n",
      "Gradient Descent(10/49): loss=0.4499978177991567\n",
      "Gradient Descent(11/49): loss=0.449997817799114\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.44999781779911097\n",
      "Gradient Descent(15/49): loss=0.4499978177991109\n",
      "Gradient Descent(16/49): loss=0.44999781779911097\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.4499978177991113\n",
      "Gradient Descent(21/49): loss=0.4499978177991113\n",
      "Gradient Descent(22/49): loss=0.4499978177991108\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.44999781779911135\n",
      "Gradient Descent(26/49): loss=0.44999781779911135\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.4499978177991111\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4536653000000001\n",
      "Gradient Descent(2/49): loss=0.4507693812499999\n",
      "Gradient Descent(3/49): loss=0.4505883863281248\n",
      "Gradient Descent(4/49): loss=0.45057707414550774\n",
      "Gradient Descent(5/49): loss=0.450576367134094\n",
      "Gradient Descent(6/49): loss=0.45057632294588096\n",
      "Gradient Descent(7/49): loss=0.45057632018411764\n",
      "Gradient Descent(8/49): loss=0.45057632001150744\n",
      "Gradient Descent(9/49): loss=0.45057632000071907\n",
      "Gradient Descent(10/49): loss=0.4505763200000451\n",
      "Gradient Descent(11/49): loss=0.45057632000000286\n",
      "Gradient Descent(12/49): loss=0.45057632000000036\n",
      "Gradient Descent(13/49): loss=0.45057632\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.4505763199999998\n",
      "Gradient Descent(16/49): loss=0.45057631999999975\n",
      "Gradient Descent(17/49): loss=0.45057632\n",
      "Gradient Descent(18/49): loss=0.45057632\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.45057631999999975\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45328725200000003\n",
      "Gradient Descent(2/49): loss=0.45036770525\n",
      "Gradient Descent(3/49): loss=0.45018523357812507\n",
      "Gradient Descent(4/49): loss=0.45017382909863285\n",
      "Gradient Descent(5/49): loss=0.4501731163186646\n",
      "Gradient Descent(6/49): loss=0.45017307176991656\n",
      "Gradient Descent(7/49): loss=0.45017306898561976\n",
      "Gradient Descent(8/49): loss=0.45017306881160135\n",
      "Gradient Descent(9/49): loss=0.45017306880072505\n",
      "Gradient Descent(10/49): loss=0.45017306880004515\n",
      "Gradient Descent(11/49): loss=0.4501730688000029\n",
      "Gradient Descent(12/49): loss=0.45017306880000024\n",
      "Gradient Descent(13/49): loss=0.45017306879999985\n",
      "Gradient Descent(14/49): loss=0.4501730688000002\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730687999998\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000001\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4535823277716821\n",
      "Gradient Descent(2/49): loss=0.4513357124358315\n",
      "Gradient Descent(3/49): loss=0.45122697625357644\n",
      "Gradient Descent(4/49): loss=0.45122171342235534\n",
      "Gradient Descent(5/49): loss=0.45122145870132424\n",
      "Gradient Descent(6/49): loss=0.4512214463728263\n",
      "Gradient Descent(7/49): loss=0.451221445776127\n",
      "Gradient Descent(8/49): loss=0.4512214457472467\n",
      "Gradient Descent(9/49): loss=0.45122144574584905\n",
      "Gradient Descent(10/49): loss=0.4512214457457814\n",
      "Gradient Descent(11/49): loss=0.45122144574577805\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.45122144574577766\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457779\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45241792341763415\n",
      "Gradient Descent(2/49): loss=0.4501149509110476\n",
      "Gradient Descent(3/49): loss=0.4500034870417288\n",
      "Gradient Descent(4/49): loss=0.4499980921904538\n",
      "Gradient Descent(5/49): loss=0.44999783107965224\n",
      "Gradient Descent(6/49): loss=0.44999781844188935\n",
      "Gradient Descent(7/49): loss=0.44999781783022136\n",
      "Gradient Descent(8/49): loss=0.4499978178006169\n",
      "Gradient Descent(9/49): loss=0.449997817799184\n",
      "Gradient Descent(10/49): loss=0.4499978177991145\n",
      "Gradient Descent(11/49): loss=0.4499978177991115\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991113\n",
      "Gradient Descent(17/49): loss=0.4499978177991109\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.44999781779911097\n",
      "Gradient Descent(20/49): loss=0.4499978177991113\n",
      "Gradient Descent(21/49): loss=0.4499978177991113\n",
      "Gradient Descent(22/49): loss=0.4499978177991112\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.44999781779911135\n",
      "Gradient Descent(25/49): loss=0.44999781779911135\n",
      "Gradient Descent(26/49): loss=0.4499978177991111\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.4499978177991111\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45296842611200006\n",
      "Gradient Descent(2/49): loss=0.45069209793582077\n",
      "Gradient Descent(3/49): loss=0.45058192365209365\n",
      "Gradient Descent(4/49): loss=0.45057659121676147\n",
      "Gradient Descent(5/49): loss=0.45057633312689116\n",
      "Gradient Descent(6/49): loss=0.4505763206353416\n",
      "Gradient Descent(7/49): loss=0.45057632003075054\n",
      "Gradient Descent(8/49): loss=0.45057632000148823\n",
      "Gradient Descent(9/49): loss=0.4505763200000719\n",
      "Gradient Descent(10/49): loss=0.4505763200000034\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.4505763199999998\n",
      "Gradient Descent(16/49): loss=0.4505763200000002\n",
      "Gradient Descent(17/49): loss=0.45057632000000014\n",
      "Gradient Descent(18/49): loss=0.4505763199999998\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4525846922700802\n",
      "Gradient Descent(2/49): loss=0.4502897913759518\n",
      "Gradient Descent(3/49): loss=0.4501787181726762\n",
      "Gradient Descent(4/49): loss=0.4501733422296378\n",
      "Gradient Descent(5/49): loss=0.4501730820339946\n",
      "Gradient Descent(6/49): loss=0.4501730694405253\n",
      "Gradient Descent(7/49): loss=0.4501730688310015\n",
      "Gradient Descent(8/49): loss=0.45017306880150054\n",
      "Gradient Descent(9/49): loss=0.45017306880007274\n",
      "Gradient Descent(10/49): loss=0.4501730688000036\n",
      "Gradient Descent(11/49): loss=0.4501730688000002\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.45017306879999985\n",
      "Gradient Descent(14/49): loss=0.4501730688000001\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730688000001\n",
      "Gradient Descent(18/49): loss=0.4501730688000002\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730688000001\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4529823515543552\n",
      "Gradient Descent(2/49): loss=0.45128501444546754\n",
      "Gradient Descent(3/49): loss=0.45122374057583675\n",
      "Gradient Descent(4/49): loss=0.4512215285891428\n",
      "Gradient Descent(5/49): loss=0.4512214487364232\n",
      "Gradient Descent(6/49): loss=0.45122144585374013\n",
      "Gradient Descent(7/49): loss=0.4512214457496751\n",
      "Gradient Descent(8/49): loss=0.4512214457459186\n",
      "Gradient Descent(9/49): loss=0.45122144574578293\n",
      "Gradient Descent(10/49): loss=0.45122144574577805\n",
      "Gradient Descent(11/49): loss=0.45122144574577794\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457779\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457778\n",
      "Gradient Descent(17/49): loss=0.45122144574577755\n",
      "Gradient Descent(18/49): loss=0.4512214457457778\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45180289657656336\n",
      "Gradient Descent(2/49): loss=0.4500629811429771\n",
      "Gradient Descent(3/49): loss=0.45000017019582456\n",
      "Gradient Descent(4/49): loss=0.4499979027206325\n",
      "Gradient Descent(5/49): loss=0.4499978208647779\n",
      "Gradient Descent(6/49): loss=0.44999781790978166\n",
      "Gradient Descent(7/49): loss=0.4499978178031062\n",
      "Gradient Descent(8/49): loss=0.44999781779925524\n",
      "Gradient Descent(9/49): loss=0.44999781779911624\n",
      "Gradient Descent(10/49): loss=0.4499978177991113\n",
      "Gradient Descent(11/49): loss=0.44999781779911097\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991113\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911097\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.44999781779911135\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45236051484799983\n",
      "Gradient Descent(2/49): loss=0.45064072943401295\n",
      "Gradient Descent(3/49): loss=0.45057864518056795\n",
      "Gradient Descent(4/49): loss=0.45057640393901854\n",
      "Gradient Descent(5/49): loss=0.4505763230301986\n",
      "Gradient Descent(6/49): loss=0.45057632010939025\n",
      "Gradient Descent(7/49): loss=0.4505763200039489\n",
      "Gradient Descent(8/49): loss=0.45057632000014247\n",
      "Gradient Descent(9/49): loss=0.450576320000005\n",
      "Gradient Descent(10/49): loss=0.45057632000000014\n",
      "Gradient Descent(11/49): loss=0.4505763200000002\n",
      "Gradient Descent(12/49): loss=0.45057632\n",
      "Gradient Descent(13/49): loss=0.4505763199999998\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.45057632000000014\n",
      "Gradient Descent(17/49): loss=0.4505763200000003\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45197182101631994\n",
      "Gradient Descent(2/49): loss=0.45023800375500905\n",
      "Gradient Descent(3/49): loss=0.4501754129518757\n",
      "Gradient Descent(4/49): loss=0.4501731534238827\n",
      "Gradient Descent(5/49): loss=0.45017307185492217\n",
      "Gradient Descent(6/49): loss=0.45017306891028264\n",
      "Gradient Descent(7/49): loss=0.45017306880398095\n",
      "Gradient Descent(8/49): loss=0.4501730688001439\n",
      "Gradient Descent(9/49): loss=0.4501730688000053\n",
      "Gradient Descent(10/49): loss=0.4501730688000001\n",
      "Gradient Descent(11/49): loss=0.4501730687999999\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688000002\n",
      "Gradient Descent(14/49): loss=0.45017306879999985\n",
      "Gradient Descent(15/49): loss=0.45017306879999985\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4524701767346859\n",
      "Gradient Descent(2/49): loss=0.4512534132590936\n",
      "Gradient Descent(3/49): loss=0.4512222641141185\n",
      "Gradient Descent(4/49): loss=0.45122146669600716\n",
      "Gradient Descent(5/49): loss=0.4512214462821036\n",
      "Gradient Descent(6/49): loss=0.4512214457595076\n",
      "Gradient Descent(7/49): loss=0.4512214457461292\n",
      "Gradient Descent(8/49): loss=0.4512214457457867\n",
      "Gradient Descent(9/49): loss=0.4512214457457781\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.45122144574577766\n",
      "Gradient Descent(12/49): loss=0.45122144574577766\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.45122144574577766\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457779\n",
      "Gradient Descent(20/49): loss=0.45122144574577805\n",
      "Gradient Descent(21/49): loss=0.4512214457457779\n",
      "Gradient Descent(22/49): loss=0.4512214457457779\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45127787366345395\n",
      "Gradient Descent(2/49): loss=0.4500305872292382\n",
      "Gradient Descent(3/49): loss=0.4499986566965222\n",
      "Gradient Descent(4/49): loss=0.4499978392748849\n",
      "Gradient Descent(5/49): loss=0.4499978183488911\n",
      "Gradient Descent(6/49): loss=0.4499978178131855\n",
      "Gradient Descent(7/49): loss=0.4499978177994714\n",
      "Gradient Descent(8/49): loss=0.44999781779912035\n",
      "Gradient Descent(9/49): loss=0.44999781779911147\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.44999781779911097\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911135\n",
      "Gradient Descent(19/49): loss=0.4499978177991112\n",
      "Gradient Descent(20/49): loss=0.44999781779911135\n",
      "Gradient Descent(21/49): loss=0.44999781779911135\n",
      "Gradient Descent(22/49): loss=0.44999781779911135\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991111\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4518415662080001\n",
      "Gradient Descent(2/49): loss=0.45060871030292476\n",
      "Gradient Descent(3/49): loss=0.4505771491917548\n",
      "Gradient Descent(4/49): loss=0.4505763412273091\n",
      "Gradient Descent(5/49): loss=0.4505763205434192\n",
      "Gradient Descent(6/49): loss=0.4505763200139118\n",
      "Gradient Descent(7/49): loss=0.45057632000035625\n",
      "Gradient Descent(8/49): loss=0.45057632000000913\n",
      "Gradient Descent(9/49): loss=0.4505763200000002\n",
      "Gradient Descent(10/49): loss=0.4505763199999999\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.45057632000000014\n",
      "Gradient Descent(13/49): loss=0.45057632000000014\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45144863823872\n",
      "Gradient Descent(2/49): loss=0.45020572337763115\n",
      "Gradient Descent(3/49): loss=0.4501739047571874\n",
      "Gradient Descent(4/49): loss=0.45017309020050406\n",
      "Gradient Descent(5/49): loss=0.4501730693478529\n",
      "Gradient Descent(6/49): loss=0.4501730688140251\n",
      "Gradient Descent(7/49): loss=0.4501730688003593\n",
      "Gradient Descent(8/49): loss=0.45017306880000924\n",
      "Gradient Descent(9/49): loss=0.4501730688000003\n",
      "Gradient Descent(10/49): loss=0.4501730688000001\n",
      "Gradient Descent(11/49): loss=0.4501730687999999\n",
      "Gradient Descent(12/49): loss=0.4501730687999999\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688000001\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4520458033126742\n",
      "Gradient Descent(2/49): loss=0.45123537738865854\n",
      "Gradient Descent(3/49): loss=0.45122168119054257\n",
      "Gradient Descent(4/49): loss=0.45122144972479444\n",
      "Gradient Descent(5/49): loss=0.45122144581302315\n",
      "Gradient Descent(6/49): loss=0.45122144574691403\n",
      "Gradient Descent(7/49): loss=0.451221445745797\n",
      "Gradient Descent(8/49): loss=0.45122144574577805\n",
      "Gradient Descent(9/49): loss=0.4512214457457778\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.45122144574577755\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.45122144574577766\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45084285467830626\n",
      "Gradient Descent(2/49): loss=0.4500120989223695\n",
      "Gradient Descent(3/49): loss=0.44999805915009394\n",
      "Gradient Descent(4/49): loss=0.44999782187794274\n",
      "Gradient Descent(5/49): loss=0.4499978178680434\n",
      "Gradient Descent(6/49): loss=0.449997817800276\n",
      "Gradient Descent(7/49): loss=0.44999781779913106\n",
      "Gradient Descent(8/49): loss=0.4499978177991113\n",
      "Gradient Descent(9/49): loss=0.44999781779911113\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.44999781779911113\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.44999781779911097\n",
      "Gradient Descent(15/49): loss=0.44999781779911097\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911135\n",
      "Gradient Descent(18/49): loss=0.44999781779911135\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.44999781779911135\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.451411580192\n",
      "Gradient Descent(2/49): loss=0.45059043589724473\n",
      "Gradient Descent(3/49): loss=0.4505765585586635\n",
      "Gradient Descent(4/49): loss=0.45057632403164155\n",
      "Gradient Descent(5/49): loss=0.45057632006813475\n",
      "Gradient Descent(6/49): loss=0.45057632000115133\n",
      "Gradient Descent(7/49): loss=0.45057632000001924\n",
      "Gradient Descent(8/49): loss=0.45057632000000036\n",
      "Gradient Descent(9/49): loss=0.4505763199999998\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.45057632\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.45057632000000014\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4510151439372801\n",
      "Gradient Descent(2/49): loss=0.4501872998698199\n",
      "Gradient Descent(3/49): loss=0.4501733093050799\n",
      "Gradient Descent(4/49): loss=0.45017307286453573\n",
      "Gradient Descent(5/49): loss=0.4501730688686908\n",
      "Gradient Descent(6/49): loss=0.45017306880116076\n",
      "Gradient Descent(7/49): loss=0.45017306880001956\n",
      "Gradient Descent(8/49): loss=0.4501730688000004\n",
      "Gradient Descent(9/49): loss=0.45017306879999985\n",
      "Gradient Descent(10/49): loss=0.4501730687999999\n",
      "Gradient Descent(11/49): loss=0.4501730687999999\n",
      "Gradient Descent(12/49): loss=0.45017306879999985\n",
      "Gradient Descent(13/49): loss=0.4501730687999999\n",
      "Gradient Descent(14/49): loss=0.4501730688000002\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45170923128832\n",
      "Gradient Descent(2/49): loss=0.4512263236012033\n",
      "Gradient Descent(3/49): loss=0.4512214945243319\n",
      "Gradient Descent(4/49): loss=0.45122144623356336\n",
      "Gradient Descent(5/49): loss=0.4512214457506557\n",
      "Gradient Descent(6/49): loss=0.4512214457458264\n",
      "Gradient Descent(7/49): loss=0.4512214457457781\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.45122144574577794\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.4512214457457778\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45049783962112006\n",
      "Gradient Descent(2/49): loss=0.45000281801733133\n",
      "Gradient Descent(3/49): loss=0.44999786780129336\n",
      "Gradient Descent(4/49): loss=0.4499978182991329\n",
      "Gradient Descent(5/49): loss=0.4499978178041114\n",
      "Gradient Descent(6/49): loss=0.44999781779916087\n",
      "Gradient Descent(7/49): loss=0.4499978177991116\n",
      "Gradient Descent(8/49): loss=0.4499978177991112\n",
      "Gradient Descent(9/49): loss=0.4499978177991112\n",
      "Gradient Descent(10/49): loss=0.4499978177991113\n",
      "Gradient Descent(11/49): loss=0.44999781779911113\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.44999781779911135\n",
      "Gradient Descent(17/49): loss=0.44999781779911135\n",
      "Gradient Descent(18/49): loss=0.44999781779911135\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.4499978177991111\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4510705568000001\n",
      "Gradient Descent(2/49): loss=0.4505812623680001\n",
      "Gradient Descent(3/49): loss=0.4505763694236802\n",
      "Gradient Descent(4/49): loss=0.4505763204942369\n",
      "Gradient Descent(5/49): loss=0.45057632000494235\n",
      "Gradient Descent(6/49): loss=0.4505763200000493\n",
      "Gradient Descent(7/49): loss=0.45057632000000053\n",
      "Gradient Descent(8/49): loss=0.45057632000000014\n",
      "Gradient Descent(9/49): loss=0.4505763200000001\n",
      "Gradient Descent(10/49): loss=0.4505763199999998\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.450671338112\n",
      "Gradient Descent(2/49): loss=0.45017805149312\n",
      "Gradient Descent(3/49): loss=0.45017311862693116\n",
      "Gradient Descent(4/49): loss=0.45017306929826945\n",
      "Gradient Descent(5/49): loss=0.45017306880498265\n",
      "Gradient Descent(6/49): loss=0.4501730688000498\n",
      "Gradient Descent(7/49): loss=0.4501730688000004\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.4501730688000001\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730687999998\n",
      "Gradient Descent(12/49): loss=0.4501730688000002\n",
      "Gradient Descent(13/49): loss=0.4501730688000001\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4514604606616235\n",
      "Gradient Descent(2/49): loss=0.4512226169188655\n",
      "Gradient Descent(3/49): loss=0.45122145148452586\n",
      "Gradient Descent(4/49): loss=0.4512214457738978\n",
      "Gradient Descent(5/49): loss=0.45122144574591566\n",
      "Gradient Descent(6/49): loss=0.45122144574577844\n",
      "Gradient Descent(7/49): loss=0.4512214457457778\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.45122144574577766\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4502428284918954\n",
      "Gradient Descent(2/49): loss=0.4499990183515058\n",
      "Gradient Descent(3/49): loss=0.4499978236818177\n",
      "Gradient Descent(4/49): loss=0.4499978178279365\n",
      "Gradient Descent(5/49): loss=0.44999781779925224\n",
      "Gradient Descent(6/49): loss=0.44999781779911174\n",
      "Gradient Descent(7/49): loss=0.4499978177991112\n",
      "Gradient Descent(8/49): loss=0.4499978177991111\n",
      "Gradient Descent(9/49): loss=0.4499978177991111\n",
      "Gradient Descent(10/49): loss=0.4499978177991112\n",
      "Gradient Descent(11/49): loss=0.4499978177991108\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45081849603199997\n",
      "Gradient Descent(2/49): loss=0.4505775066625568\n",
      "Gradient Descent(3/49): loss=0.45057632581464674\n",
      "Gradient Descent(4/49): loss=0.45057632002849196\n",
      "Gradient Descent(5/49): loss=0.4505763200001396\n",
      "Gradient Descent(6/49): loss=0.4505763200000007\n",
      "Gradient Descent(7/49): loss=0.45057632\n",
      "Gradient Descent(8/49): loss=0.4505763199999998\n",
      "Gradient Descent(9/49): loss=0.45057632000000014\n",
      "Gradient Descent(10/49): loss=0.4505763199999998\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763200000002\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45041722076288\n",
      "Gradient Descent(2/49): loss=0.45017426514461806\n",
      "Gradient Descent(3/49): loss=0.4501730746620887\n",
      "Gradient Descent(4/49): loss=0.45017306882872427\n",
      "Gradient Descent(5/49): loss=0.45017306880014074\n",
      "Gradient Descent(6/49): loss=0.45017306880000085\n",
      "Gradient Descent(7/49): loss=0.4501730688\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.45017306879999985\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4512994914325845\n",
      "Gradient Descent(2/49): loss=0.4512215706188766\n",
      "Gradient Descent(3/49): loss=0.4512214459455747\n",
      "Gradient Descent(4/49): loss=0.4512214457460974\n",
      "Gradient Descent(5/49): loss=0.4512214457457785\n",
      "Gradient Descent(6/49): loss=0.4512214457457778\n",
      "Gradient Descent(7/49): loss=0.45122144574577755\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.4512214457457779\n",
      "Gradient Descent(12/49): loss=0.4512214457457779\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45007782129063245\n",
      "Gradient Descent(2/49): loss=0.4499979458046976\n",
      "Gradient Descent(3/49): loss=0.4499978180039201\n",
      "Gradient Descent(4/49): loss=0.44999781779943887\n",
      "Gradient Descent(5/49): loss=0.44999781779911174\n",
      "Gradient Descent(6/49): loss=0.4499978177991112\n",
      "Gradient Descent(7/49): loss=0.44999781779911097\n",
      "Gradient Descent(8/49): loss=0.44999781779911113\n",
      "Gradient Descent(9/49): loss=0.44999781779911097\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.44999781779911135\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45065539788799974\n",
      "Gradient Descent(2/49): loss=0.45057644652462076\n",
      "Gradient Descent(3/49): loss=0.45057632020243915\n",
      "Gradient Descent(4/49): loss=0.4505763200003239\n",
      "Gradient Descent(5/49): loss=0.4505763200000007\n",
      "Gradient Descent(6/49): loss=0.45057632\n",
      "Gradient Descent(7/49): loss=0.45057632000000014\n",
      "Gradient Descent(8/49): loss=0.45057632\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.45057631999999975\n",
      "Gradient Descent(11/49): loss=0.4505763200000001\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45025279188992023\n",
      "Gradient Descent(2/49): loss=0.45017319635694386\n",
      "Gradient Descent(3/49): loss=0.4501730690040911\n",
      "Gradient Descent(4/49): loss=0.4501730688003264\n",
      "Gradient Descent(5/49): loss=0.4501730688000006\n",
      "Gradient Descent(6/49): loss=0.4501730688000001\n",
      "Gradient Descent(7/49): loss=0.45017306879999985\n",
      "Gradient Descent(8/49): loss=0.4501730688000002\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688000002\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45122632360120335\n",
      "Gradient Descent(2/49): loss=0.4512214462335633\n",
      "Gradient Descent(3/49): loss=0.45122144574582645\n",
      "Gradient Descent(4/49): loss=0.45122144574577766\n",
      "Gradient Descent(5/49): loss=0.4512214457457777\n",
      "Gradient Descent(6/49): loss=0.45122144574577794\n",
      "Gradient Descent(7/49): loss=0.4512214457457778\n",
      "Gradient Descent(8/49): loss=0.4512214457457779\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457777\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4500028180173313\n",
      "Gradient Descent(2/49): loss=0.4499978182991329\n",
      "Gradient Descent(3/49): loss=0.4499978177991612\n",
      "Gradient Descent(4/49): loss=0.44999781779911097\n",
      "Gradient Descent(5/49): loss=0.4499978177991112\n",
      "Gradient Descent(6/49): loss=0.44999781779911113\n",
      "Gradient Descent(7/49): loss=0.4499978177991112\n",
      "Gradient Descent(8/49): loss=0.44999781779911135\n",
      "Gradient Descent(9/49): loss=0.44999781779911135\n",
      "Gradient Descent(10/49): loss=0.44999781779911135\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4505812623679999\n",
      "Gradient Descent(2/49): loss=0.45057632049423657\n",
      "Gradient Descent(3/49): loss=0.4505763200000493\n",
      "Gradient Descent(4/49): loss=0.4505763200000002\n",
      "Gradient Descent(5/49): loss=0.4505763199999998\n",
      "Gradient Descent(6/49): loss=0.4505763199999999\n",
      "Gradient Descent(7/49): loss=0.4505763199999999\n",
      "Gradient Descent(8/49): loss=0.4505763200000001\n",
      "Gradient Descent(9/49): loss=0.4505763199999999\n",
      "Gradient Descent(10/49): loss=0.4505763199999999\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45017805149312\n",
      "Gradient Descent(2/49): loss=0.45017306929826945\n",
      "Gradient Descent(3/49): loss=0.45017306880004976\n",
      "Gradient Descent(4/49): loss=0.4501730688\n",
      "Gradient Descent(5/49): loss=0.4501730688\n",
      "Gradient Descent(6/49): loss=0.4501730688000001\n",
      "Gradient Descent(7/49): loss=0.4501730688\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45124095716747953\n",
      "Gradient Descent(2/49): loss=0.45122145355034665\n",
      "Gradient Descent(3/49): loss=0.4512214457488995\n",
      "Gradient Descent(4/49): loss=0.45122144574577905\n",
      "Gradient Descent(5/49): loss=0.4512214457457778\n",
      "Gradient Descent(6/49): loss=0.45122144574577794\n",
      "Gradient Descent(7/49): loss=0.4512214457457778\n",
      "Gradient Descent(8/49): loss=0.4512214457457777\n",
      "Gradient Descent(9/49): loss=0.4512214457457777\n",
      "Gradient Descent(10/49): loss=0.4512214457457779\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45001781867199137\n",
      "Gradient Descent(2/49): loss=0.4499978257994601\n",
      "Gradient Descent(3/49): loss=0.44999781780231124\n",
      "Gradient Descent(4/49): loss=0.4499978177991125\n",
      "Gradient Descent(5/49): loss=0.4499978177991112\n",
      "Gradient Descent(6/49): loss=0.44999781779911113\n",
      "Gradient Descent(7/49): loss=0.44999781779911113\n",
      "Gradient Descent(8/49): loss=0.4499978177991112\n",
      "Gradient Descent(9/49): loss=0.44999781779911135\n",
      "Gradient Descent(10/49): loss=0.44999781779911135\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45059608947199986\n",
      "Gradient Descent(2/49): loss=0.45057632790778895\n",
      "Gradient Descent(3/49): loss=0.4505763200031632\n",
      "Gradient Descent(4/49): loss=0.4505763200000013\n",
      "Gradient Descent(5/49): loss=0.45057632\n",
      "Gradient Descent(6/49): loss=0.45057632000000014\n",
      "Gradient Descent(7/49): loss=0.45057632\n",
      "Gradient Descent(8/49): loss=0.4505763200000002\n",
      "Gradient Descent(9/49): loss=0.4505763200000001\n",
      "Gradient Descent(10/49): loss=0.4505763199999999\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4501929995724799\n",
      "Gradient Descent(2/49): loss=0.4501730767723089\n",
      "Gradient Descent(3/49): loss=0.450173068803189\n",
      "Gradient Descent(4/49): loss=0.4501730688000011\n",
      "Gradient Descent(5/49): loss=0.4501730687999999\n",
      "Gradient Descent(6/49): loss=0.4501730688\n",
      "Gradient Descent(7/49): loss=0.45017306879999985\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.4501730688\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688000002\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4513433921314133\n",
      "Gradient Descent(2/49): loss=0.45122175061174186\n",
      "Gradient Descent(3/49): loss=0.45122144650794277\n",
      "Gradient Descent(4/49): loss=0.4512214457476832\n",
      "Gradient Descent(5/49): loss=0.45122144574578255\n",
      "Gradient Descent(6/49): loss=0.4512214457457778\n",
      "Gradient Descent(7/49): loss=0.4512214457457778\n",
      "Gradient Descent(8/49): loss=0.45122144574577766\n",
      "Gradient Descent(9/49): loss=0.4512214457457779\n",
      "Gradient Descent(10/49): loss=0.45122144574577794\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45012282325461334\n",
      "Gradient Descent(2/49): loss=0.4499981303127501\n",
      "Gradient Descent(3/49): loss=0.4499978185803954\n",
      "Gradient Descent(4/49): loss=0.4499978178010643\n",
      "Gradient Descent(5/49): loss=0.44999781779911596\n",
      "Gradient Descent(6/49): loss=0.44999781779911113\n",
      "Gradient Descent(7/49): loss=0.44999781779911097\n",
      "Gradient Descent(8/49): loss=0.4499978177991108\n",
      "Gradient Descent(9/49): loss=0.44999781779911097\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991112\n",
      "Gradient Descent(13/49): loss=0.44999781779911097\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4506998791999999\n",
      "Gradient Descent(2/49): loss=0.45057662889800026\n",
      "Gradient Descent(3/49): loss=0.45057632077224496\n",
      "Gradient Descent(4/49): loss=0.4505763200019306\n",
      "Gradient Descent(5/49): loss=0.45057632000000486\n",
      "Gradient Descent(6/49): loss=0.4505763200000001\n",
      "Gradient Descent(7/49): loss=0.45057632\n",
      "Gradient Descent(8/49): loss=0.4505763199999999\n",
      "Gradient Descent(9/49): loss=0.45057632000000014\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.4505763199999999\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45029763612800006\n",
      "Gradient Descent(2/49): loss=0.45017338021832015\n",
      "Gradient Descent(3/49): loss=0.45017306957854597\n",
      "Gradient Descent(4/49): loss=0.4501730688019463\n",
      "Gradient Descent(5/49): loss=0.45017306880000474\n",
      "Gradient Descent(6/49): loss=0.4501730688000001\n",
      "Gradient Descent(7/49): loss=0.4501730688000001\n",
      "Gradient Descent(8/49): loss=0.4501730688000001\n",
      "Gradient Descent(9/49): loss=0.4501730688000002\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688000002\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45153362849300477\n",
      "Gradient Descent(2/49): loss=0.45122344371536\n",
      "Gradient Descent(3/49): loss=0.45122145853278306\n",
      "Gradient Descent(4/49): loss=0.4512214458276147\n",
      "Gradient Descent(5/49): loss=0.4512214457463015\n",
      "Gradient Descent(6/49): loss=0.45122144574578116\n",
      "Gradient Descent(7/49): loss=0.45122144574577755\n",
      "Gradient Descent(8/49): loss=0.4512214457457778\n",
      "Gradient Descent(9/49): loss=0.45122144574577766\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.4512214457457778\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4503178317651967\n",
      "Gradient Descent(2/49): loss=0.44999986588849405\n",
      "Gradient Descent(3/49): loss=0.4499978309068831\n",
      "Gradient Descent(4/49): loss=0.44999781788300086\n",
      "Gradient Descent(5/49): loss=0.4499978177996481\n",
      "Gradient Descent(6/49): loss=0.44999781779911463\n",
      "Gradient Descent(7/49): loss=0.44999781779911113\n",
      "Gradient Descent(8/49): loss=0.4499978177991111\n",
      "Gradient Descent(9/49): loss=0.4499978177991111\n",
      "Gradient Descent(10/49): loss=0.4499978177991111\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.44999781779911097\n",
      "Gradient Descent(13/49): loss=0.4499978177991111\n",
      "Gradient Descent(14/49): loss=0.4499978177991112\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.450892631552\n",
      "Gradient Descent(2/49): loss=0.45057834439393274\n",
      "Gradient Descent(3/49): loss=0.4505763329561211\n",
      "Gradient Descent(4/49): loss=0.45057632008291926\n",
      "Gradient Descent(5/49): loss=0.45057632000053055\n",
      "Gradient Descent(6/49): loss=0.45057632000000325\n",
      "Gradient Descent(7/49): loss=0.45057632\n",
      "Gradient Descent(8/49): loss=0.45057631999999975\n",
      "Gradient Descent(9/49): loss=0.45057632\n",
      "Gradient Descent(10/49): loss=0.45057632\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763199999999\n",
      "Gradient Descent(13/49): loss=0.4505763200000001\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763199999999\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45049196115968004\n",
      "Gradient Descent(2/49): loss=0.45017510971110175\n",
      "Gradient Descent(3/49): loss=0.4501730818618311\n",
      "Gradient Descent(4/49): loss=0.4501730688835958\n",
      "Gradient Descent(5/49): loss=0.45017306880053487\n",
      "Gradient Descent(6/49): loss=0.4501730688000036\n",
      "Gradient Descent(7/49): loss=0.4501730688000002\n",
      "Gradient Descent(8/49): loss=0.4501730688\n",
      "Gradient Descent(9/49): loss=0.45017306879999985\n",
      "Gradient Descent(10/49): loss=0.4501730688000001\n",
      "Gradient Descent(11/49): loss=0.45017306880000024\n",
      "Gradient Descent(12/49): loss=0.4501730688000002\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688000002\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45181166625225383\n",
      "Gradient Descent(2/49): loss=0.4512285874139062\n",
      "Gradient Descent(3/49): loss=0.45122153215996225\n",
      "Gradient Descent(4/49): loss=0.4512214467913893\n",
      "Gradient Descent(5/49): loss=0.4512214457584298\n",
      "Gradient Descent(6/49): loss=0.4512214457459309\n",
      "Gradient Descent(7/49): loss=0.4512214457457795\n",
      "Gradient Descent(8/49): loss=0.4512214457457779\n",
      "Gradient Descent(9/49): loss=0.4512214457457778\n",
      "Gradient Descent(10/49): loss=0.4512214457457779\n",
      "Gradient Descent(11/49): loss=0.4512214457457779\n",
      "Gradient Descent(12/49): loss=0.45122144574577766\n",
      "Gradient Descent(13/49): loss=0.45122144574577755\n",
      "Gradient Descent(14/49): loss=0.4512214457457778\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457779\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45060284420374186\n",
      "Gradient Descent(2/49): loss=0.4500051386186071\n",
      "Gradient Descent(3/49): loss=0.44999790638102694\n",
      "Gradient Descent(4/49): loss=0.4499978188709522\n",
      "Gradient Descent(5/49): loss=0.44999781781208026\n",
      "Gradient Descent(6/49): loss=0.44999781779926823\n",
      "Gradient Descent(7/49): loss=0.4499978177991131\n",
      "Gradient Descent(8/49): loss=0.4499978177991112\n",
      "Gradient Descent(9/49): loss=0.44999781779911113\n",
      "Gradient Descent(10/49): loss=0.44999781779911097\n",
      "Gradient Descent(11/49): loss=0.4499978177991112\n",
      "Gradient Descent(12/49): loss=0.4499978177991113\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.4499978177991108\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.451174346528\n",
      "Gradient Descent(2/49): loss=0.45058355612098866\n",
      "Gradient Descent(3/49): loss=0.4505764075570641\n",
      "Gradient Descent(4/49): loss=0.45057632105944057\n",
      "Gradient Descent(5/49): loss=0.45057632001281916\n",
      "Gradient Descent(6/49): loss=0.4505763200001552\n",
      "Gradient Descent(7/49): loss=0.4505763200000019\n",
      "Gradient Descent(8/49): loss=0.4505763200000001\n",
      "Gradient Descent(9/49): loss=0.4505763199999999\n",
      "Gradient Descent(10/49): loss=0.45057632\n",
      "Gradient Descent(11/49): loss=0.45057632\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999998\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.4505763200000002\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4507759746675201\n",
      "Gradient Descent(2/49): loss=0.45018036396099703\n",
      "Gradient Descent(3/49): loss=0.45017315707144795\n",
      "Gradient Descent(4/49): loss=0.45017306986808464\n",
      "Gradient Descent(5/49): loss=0.4501730688129239\n",
      "Gradient Descent(6/49): loss=0.4501730688001564\n",
      "Gradient Descent(7/49): loss=0.45017306880000185\n",
      "Gradient Descent(8/49): loss=0.4501730688000001\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.45017306879999985\n",
      "Gradient Descent(11/49): loss=0.45017306879999985\n",
      "Gradient Descent(12/49): loss=0.4501730688\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4521775054091604\n",
      "Gradient Descent(2/49): loss=0.45124018451518\n",
      "Gradient Descent(3/49): loss=0.4512218130256581\n",
      "Gradient Descent(4/49): loss=0.45122145294446325\n",
      "Gradient Descent(5/49): loss=0.4512214458868722\n",
      "Gradient Descent(6/49): loss=0.4512214457485434\n",
      "Gradient Descent(7/49): loss=0.4512214457458321\n",
      "Gradient Descent(8/49): loss=0.451221445745779\n",
      "Gradient Descent(9/49): loss=0.4512214457457779\n",
      "Gradient Descent(10/49): loss=0.4512214457457778\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457777\n",
      "Gradient Descent(14/49): loss=0.45122144574577794\n",
      "Gradient Descent(15/49): loss=0.4512214457457779\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4509778605702486\n",
      "Gradient Descent(2/49): loss=0.4500170266374255\n",
      "Gradient Descent(3/49): loss=0.449998194292342\n",
      "Gradient Descent(4/49): loss=0.44999782517837844\n",
      "Gradient Descent(5/49): loss=0.44999781794374477\n",
      "Gradient Descent(6/49): loss=0.44999781780194587\n",
      "Gradient Descent(7/49): loss=0.44999781779916664\n",
      "Gradient Descent(8/49): loss=0.4499978177991121\n",
      "Gradient Descent(9/49): loss=0.44999781779911097\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.44999781779911097\n",
      "Gradient Descent(12/49): loss=0.4499978177991111\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991112\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45154502412800007\n",
      "Gradient Descent(2/49): loss=0.45059530660090896\n",
      "Gradient Descent(3/49): loss=0.45057669213737755\n",
      "Gradient Descent(4/49): loss=0.4505763272938927\n",
      "Gradient Descent(5/49): loss=0.4505763201429601\n",
      "Gradient Descent(6/49): loss=0.4505763200028018\n",
      "Gradient Descent(7/49): loss=0.450576320000055\n",
      "Gradient Descent(8/49): loss=0.45057632000000125\n",
      "Gradient Descent(9/49): loss=0.45057632000000014\n",
      "Gradient Descent(10/49): loss=0.4505763200000001\n",
      "Gradient Descent(11/49): loss=0.4505763200000001\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.45057632000000014\n",
      "Gradient Descent(14/49): loss=0.45057632\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763200000002\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45114967665151995\n",
      "Gradient Descent(2/49): loss=0.45019221031388984\n",
      "Gradient Descent(3/49): loss=0.45017344397367226\n",
      "Gradient Descent(4/49): loss=0.4501730761534039\n",
      "Gradient Descent(5/49): loss=0.45017306894412673\n",
      "Gradient Descent(6/49): loss=0.45017306880282504\n",
      "Gradient Descent(7/49): loss=0.4501730688000552\n",
      "Gradient Descent(8/49): loss=0.450173068800001\n",
      "Gradient Descent(9/49): loss=0.4501730687999999\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.4501730688000001\n",
      "Gradient Descent(12/49): loss=0.4501730688000001\n",
      "Gradient Descent(13/49): loss=0.4501730688000002\n",
      "Gradient Descent(14/49): loss=0.4501730688000001\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730688000002\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4526311459637247\n",
      "Gradient Descent(2/49): loss=0.4512621860820763\n",
      "Gradient Descent(3/49): loss=0.45122262314149675\n",
      "Gradient Descent(4/49): loss=0.45122147977251403\n",
      "Gradient Descent(5/49): loss=0.45122144672915043\n",
      "Gradient Descent(6/49): loss=0.45122144577419737\n",
      "Gradient Descent(7/49): loss=0.4512214457465993\n",
      "Gradient Descent(8/49): loss=0.4512214457458015\n",
      "Gradient Descent(9/49): loss=0.45122144574577827\n",
      "Gradient Descent(10/49): loss=0.45122144574577766\n",
      "Gradient Descent(11/49): loss=0.4512214457457777\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457777\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.45122144574577805\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.4512214457457779\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4514428808647165\n",
      "Gradient Descent(2/49): loss=0.4500395801217072\n",
      "Gradient Descent(3/49): loss=0.4499990247302341\n",
      "Gradient Descent(4/49): loss=0.4499978526794205\n",
      "Gradient Descent(5/49): loss=0.4499978188071518\n",
      "Gradient Descent(6/49): loss=0.4499978178282434\n",
      "Gradient Descent(7/49): loss=0.44999781779995307\n",
      "Gradient Descent(8/49): loss=0.4499978177991355\n",
      "Gradient Descent(9/49): loss=0.4499978177991119\n",
      "Gradient Descent(10/49): loss=0.44999781779911113\n",
      "Gradient Descent(11/49): loss=0.4499978177991111\n",
      "Gradient Descent(12/49): loss=0.4499978177991113\n",
      "Gradient Descent(13/49): loss=0.4499978177991109\n",
      "Gradient Descent(14/49): loss=0.44999781779911097\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.4499978177991113\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4520046643520001\n",
      "Gradient Descent(2/49): loss=0.45061759915177285\n",
      "Gradient Descent(3/49): loss=0.45057751296748627\n",
      "Gradient Descent(4/49): loss=0.45057635447676037\n",
      "Gradient Descent(5/49): loss=0.45057632099637845\n",
      "Gradient Descent(6/49): loss=0.4505763200287955\n",
      "Gradient Descent(7/49): loss=0.45057632000083225\n",
      "Gradient Descent(8/49): loss=0.45057632000002396\n",
      "Gradient Descent(9/49): loss=0.45057632000000086\n",
      "Gradient Descent(10/49): loss=0.4505763200000002\n",
      "Gradient Descent(11/49): loss=0.4505763199999998\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763199999998\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.4505763199999999\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45161306711168\n",
      "Gradient Descent(2/49): loss=0.45021468475120746\n",
      "Gradient Descent(3/49): loss=0.4501742715009898\n",
      "Gradient Descent(4/49): loss=0.45017310355805856\n",
      "Gradient Descent(5/49): loss=0.4501730698045078\n",
      "Gradient Descent(6/49): loss=0.45017306882903024\n",
      "Gradient Descent(7/49): loss=0.4501730688008389\n",
      "Gradient Descent(8/49): loss=0.45017306880002417\n",
      "Gradient Descent(9/49): loss=0.4501730688000007\n",
      "Gradient Descent(10/49): loss=0.4501730688\n",
      "Gradient Descent(11/49): loss=0.45017306879999985\n",
      "Gradient Descent(12/49): loss=0.4501730688000002\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730688000002\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000002\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45317258791594667\n",
      "Gradient Descent(2/49): loss=0.4512994914325845\n",
      "Gradient Descent(3/49): loss=0.45122456757325013\n",
      "Gradient Descent(4/49): loss=0.45122157061887663\n",
      "Gradient Descent(5/49): loss=0.4512214507407017\n",
      "Gradient Descent(6/49): loss=0.45122144594557484\n",
      "Gradient Descent(7/49): loss=0.4512214457537697\n",
      "Gradient Descent(8/49): loss=0.45122144574609757\n",
      "Gradient Descent(9/49): loss=0.4512214457457904\n",
      "Gradient Descent(10/49): loss=0.45122144574577827\n",
      "Gradient Descent(11/49): loss=0.4512214457457778\n",
      "Gradient Descent(12/49): loss=0.4512214457457777\n",
      "Gradient Descent(13/49): loss=0.4512214457457779\n",
      "Gradient Descent(14/49): loss=0.45122144574577755\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.4512214457457777\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457778\n",
      "Gradient Descent(20/49): loss=0.4512214457457777\n",
      "Gradient Descent(21/49): loss=0.45122144574577794\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4519979050871468\n",
      "Gradient Descent(2/49): loss=0.45007782129063273\n",
      "Gradient Descent(3/49): loss=0.45000101793877206\n",
      "Gradient Descent(4/49): loss=0.4499979458046976\n",
      "Gradient Descent(5/49): loss=0.4499978229193344\n",
      "Gradient Descent(6/49): loss=0.4499978180039201\n",
      "Gradient Descent(7/49): loss=0.4499978178073035\n",
      "Gradient Descent(8/49): loss=0.4499978177994388\n",
      "Gradient Descent(9/49): loss=0.44999781779912423\n",
      "Gradient Descent(10/49): loss=0.44999781779911174\n",
      "Gradient Descent(11/49): loss=0.44999781779911097\n",
      "Gradient Descent(12/49): loss=0.44999781779911113\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.44999781779911097\n",
      "Gradient Descent(15/49): loss=0.4499978177991109\n",
      "Gradient Descent(16/49): loss=0.4499978177991111\n",
      "Gradient Descent(17/49): loss=0.4499978177991112\n",
      "Gradient Descent(18/49): loss=0.44999781779911097\n",
      "Gradient Descent(19/49): loss=0.4499978177991113\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45255326719999994\n",
      "Gradient Descent(2/49): loss=0.450655397888\n",
      "Gradient Descent(3/49): loss=0.45057948311552004\n",
      "Gradient Descent(4/49): loss=0.45057644652462076\n",
      "Gradient Descent(5/49): loss=0.450576325060985\n",
      "Gradient Descent(6/49): loss=0.4505763202024395\n",
      "Gradient Descent(7/49): loss=0.45057632000809755\n",
      "Gradient Descent(8/49): loss=0.4505763200003239\n",
      "Gradient Descent(9/49): loss=0.4505763200000128\n",
      "Gradient Descent(10/49): loss=0.4505763200000007\n",
      "Gradient Descent(11/49): loss=0.4505763200000003\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.45057632\n",
      "Gradient Descent(14/49): loss=0.4505763199999999\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763199999998\n",
      "Gradient Descent(17/49): loss=0.45057631999999975\n",
      "Gradient Descent(18/49): loss=0.4505763199999999\n",
      "Gradient Descent(19/49): loss=0.4505763200000001\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45216614604799976\n",
      "Gradient Descent(2/49): loss=0.45025279188991996\n",
      "Gradient Descent(3/49): loss=0.45017625772359693\n",
      "Gradient Descent(4/49): loss=0.4501731963569438\n",
      "Gradient Descent(5/49): loss=0.45017307390227773\n",
      "Gradient Descent(6/49): loss=0.4501730690040911\n",
      "Gradient Descent(7/49): loss=0.4501730688081636\n",
      "Gradient Descent(8/49): loss=0.4501730688003266\n",
      "Gradient Descent(9/49): loss=0.4501730688000132\n",
      "Gradient Descent(10/49): loss=0.4501730688000003\n",
      "Gradient Descent(11/49): loss=0.45017306880000024\n",
      "Gradient Descent(12/49): loss=0.4501730688000002\n",
      "Gradient Descent(13/49): loss=0.4501730688\n",
      "Gradient Descent(14/49): loss=0.4501730687999999\n",
      "Gradient Descent(15/49): loss=0.4501730688\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730688000002\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688000001\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45380183126582596\n",
      "Gradient Descent(2/49): loss=0.45135794813978825\n",
      "Gradient Descent(3/49): loss=0.451228666722421\n",
      "Gradient Descent(4/49): loss=0.4512218277354422\n",
      "Gradient Descent(5/49): loss=0.45122146595303114\n",
      "Gradient Descent(6/49): loss=0.4512214468147414\n",
      "Gradient Descent(7/49): loss=0.4512214458023259\n",
      "Gradient Descent(8/49): loss=0.4512214457487694\n",
      "Gradient Descent(9/49): loss=0.451221445745936\n",
      "Gradient Descent(10/49): loss=0.4512214457457861\n",
      "Gradient Descent(11/49): loss=0.4512214457457782\n",
      "Gradient Descent(12/49): loss=0.4512214457457778\n",
      "Gradient Descent(13/49): loss=0.45122144574577755\n",
      "Gradient Descent(14/49): loss=0.45122144574577766\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.45122144574577805\n",
      "Gradient Descent(17/49): loss=0.45122144574577755\n",
      "Gradient Descent(18/49): loss=0.45122144574577766\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457778\n",
      "Gradient Descent(22/49): loss=0.45122144574577794\n",
      "Gradient Descent(23/49): loss=0.4512214457457779\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4526429332375383\n",
      "Gradient Descent(2/49): loss=0.4501377444058043\n",
      "Gradient Descent(3/49): loss=0.45000521991660525\n",
      "Gradient Descent(4/49): loss=0.4499982093711264\n",
      "Gradient Descent(5/49): loss=0.4499978385132706\n",
      "Gradient Descent(6/49): loss=0.44999781889489016\n",
      "Gradient Descent(7/49): loss=0.4499978178570779\n",
      "Gradient Descent(8/49): loss=0.44999781780217785\n",
      "Gradient Descent(9/49): loss=0.44999781779927317\n",
      "Gradient Descent(10/49): loss=0.4499978177991197\n",
      "Gradient Descent(11/49): loss=0.4499978177991116\n",
      "Gradient Descent(12/49): loss=0.44999781779911097\n",
      "Gradient Descent(13/49): loss=0.44999781779911113\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.44999781779911113\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.4499978177991113\n",
      "Gradient Descent(18/49): loss=0.4499978177991111\n",
      "Gradient Descent(19/49): loss=0.44999781779911097\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991113\n",
      "Gradient Descent(22/49): loss=0.4499978177991112\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.44999781779911113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4531908326720001\n",
      "Gradient Descent(2/49): loss=0.45071462772034887\n",
      "Gradient Descent(3/49): loss=0.45058363647840644\n",
      "Gradient Descent(4/49): loss=0.4505767070417078\n",
      "Gradient Descent(5/49): loss=0.4505763404745063\n",
      "Gradient Descent(6/49): loss=0.45057632108310136\n",
      "Gradient Descent(7/49): loss=0.45057632005729603\n",
      "Gradient Descent(8/49): loss=0.45057632000303105\n",
      "Gradient Descent(9/49): loss=0.4505763200001604\n",
      "Gradient Descent(10/49): loss=0.4505763200000085\n",
      "Gradient Descent(11/49): loss=0.45057632000000036\n",
      "Gradient Descent(12/49): loss=0.4505763200000001\n",
      "Gradient Descent(13/49): loss=0.4505763199999998\n",
      "Gradient Descent(14/49): loss=0.4505763200000001\n",
      "Gradient Descent(15/49): loss=0.45057632000000014\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.45057632000000014\n",
      "Gradient Descent(19/49): loss=0.45057632000000014\n",
      "Gradient Descent(20/49): loss=0.45057632000000014\n",
      "Gradient Descent(21/49): loss=0.45057631999999975\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763199999999\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45280891346048013\n",
      "Gradient Descent(2/49): loss=0.45031250498253955\n",
      "Gradient Descent(3/49): loss=0.4501804449740563\n",
      "Gradient Descent(4/49): loss=0.45017345899960753\n",
      "Gradient Descent(5/49): loss=0.45017308944155926\n",
      "Gradient Descent(6/49): loss=0.45017306989193856\n",
      "Gradient Descent(7/49): loss=0.4501730688577636\n",
      "Gradient Descent(8/49): loss=0.4501730688030559\n",
      "Gradient Descent(9/49): loss=0.45017306880016184\n",
      "Gradient Descent(10/49): loss=0.45017306880000846\n",
      "Gradient Descent(11/49): loss=0.4501730688000003\n",
      "Gradient Descent(12/49): loss=0.4501730687999999\n",
      "Gradient Descent(13/49): loss=0.4501730687999999\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730687999999\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000002\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000002\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4545188760133631\n",
      "Gradient Descent(2/49): loss=0.4514443520318667\n",
      "Gradient Descent(3/49): loss=0.45123651421071737\n",
      "Gradient Descent(4/49): loss=0.4512224643740076\n",
      "Gradient Descent(5/49): loss=0.45122151460504617\n",
      "Gradient Descent(6/49): loss=0.4512214504006643\n",
      "Gradient Descent(7/49): loss=0.451221446060448\n",
      "Gradient Descent(8/49): loss=0.4512214457670496\n",
      "Gradient Descent(9/49): loss=0.4512214457472157\n",
      "Gradient Descent(10/49): loss=0.45122144574587514\n",
      "Gradient Descent(11/49): loss=0.4512214457457844\n",
      "Gradient Descent(12/49): loss=0.4512214457457781\n",
      "Gradient Descent(13/49): loss=0.4512214457457778\n",
      "Gradient Descent(14/49): loss=0.4512214457457777\n",
      "Gradient Descent(15/49): loss=0.45122144574577766\n",
      "Gradient Descent(16/49): loss=0.4512214457457778\n",
      "Gradient Descent(17/49): loss=0.4512214457457778\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457778\n",
      "Gradient Descent(26/49): loss=0.4512214457457779\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45337796531589125\n",
      "Gradient Descent(2/49): loss=0.4502263157712456\n",
      "Gradient Descent(3/49): loss=0.4500132642620273\n",
      "Gradient Descent(4/49): loss=0.44999886198000416\n",
      "Gradient Descent(5/49): loss=0.4499978883857394\n",
      "Gradient Descent(6/49): loss=0.4499978225707672\n",
      "Gradient Descent(7/49): loss=0.4499978181216749\n",
      "Gradient Descent(8/49): loss=0.44999781782091647\n",
      "Gradient Descent(9/49): loss=0.4499978178005851\n",
      "Gradient Descent(10/49): loss=0.4499978177992106\n",
      "Gradient Descent(11/49): loss=0.44999781779911774\n",
      "Gradient Descent(12/49): loss=0.4499978177991115\n",
      "Gradient Descent(13/49): loss=0.4499978177991112\n",
      "Gradient Descent(14/49): loss=0.44999781779911113\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.44999781779911135\n",
      "Gradient Descent(17/49): loss=0.44999781779911097\n",
      "Gradient Descent(18/49): loss=0.4499978177991113\n",
      "Gradient Descent(19/49): loss=0.44999781779911097\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.4499978177991113\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.44999781779911097\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45391736076800016\n",
      "Gradient Descent(2/49): loss=0.4508021743559169\n",
      "Gradient Descent(3/49): loss=0.45059158775445995\n",
      "Gradient Descent(4/49): loss=0.4505773521002015\n",
      "Gradient Descent(5/49): loss=0.4505763897699738\n",
      "Gradient Descent(6/49): loss=0.4505763247164503\n",
      "Gradient Descent(7/49): loss=0.4505763203188319\n",
      "Gradient Descent(8/49): loss=0.45057632002155307\n",
      "Gradient Descent(9/49): loss=0.45057632000145686\n",
      "Gradient Descent(10/49): loss=0.4505763200000984\n",
      "Gradient Descent(11/49): loss=0.4505763200000068\n",
      "Gradient Descent(12/49): loss=0.45057632000000053\n",
      "Gradient Descent(13/49): loss=0.4505763199999999\n",
      "Gradient Descent(14/49): loss=0.4505763200000002\n",
      "Gradient Descent(15/49): loss=0.4505763200000002\n",
      "Gradient Descent(16/49): loss=0.45057632\n",
      "Gradient Descent(17/49): loss=0.4505763199999999\n",
      "Gradient Descent(18/49): loss=0.45057632\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.45057632000000014\n",
      "Gradient Descent(21/49): loss=0.45057632000000014\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.4505763200000002\n",
      "Gradient Descent(24/49): loss=0.4505763200000002\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4535413693491199\n",
      "Gradient Descent(2/49): loss=0.4504007659171204\n",
      "Gradient Descent(3/49): loss=0.45018846112511735\n",
      "Gradient Descent(4/49): loss=0.4501741093211778\n",
      "Gradient Descent(5/49): loss=0.4501731391392317\n",
      "Gradient Descent(6/49): loss=0.45017307355493197\n",
      "Gradient Descent(7/49): loss=0.4501730691214335\n",
      "Gradient Descent(8/49): loss=0.45017306882172886\n",
      "Gradient Descent(9/49): loss=0.45017306880146885\n",
      "Gradient Descent(10/49): loss=0.45017306880009944\n",
      "Gradient Descent(11/49): loss=0.45017306880000674\n",
      "Gradient Descent(12/49): loss=0.45017306880000046\n",
      "Gradient Descent(13/49): loss=0.4501730687999999\n",
      "Gradient Descent(14/49): loss=0.4501730688\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688000001\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730687999998\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730687999999\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4553237221585579\n",
      "Gradient Descent(2/49): loss=0.4515664471920927\n",
      "Gradient Descent(3/49): loss=0.45125046036741295\n",
      "Gradient Descent(4/49): loss=0.45122388587545736\n",
      "Gradient Descent(5/49): loss=0.45122165096068384\n",
      "Gradient Descent(6/49): loss=0.4512214630043516\n",
      "Gradient Descent(7/49): loss=0.451221447197224\n",
      "Gradient Descent(8/49): loss=0.4512214458678446\n",
      "Gradient Descent(9/49): loss=0.45122144575604367\n",
      "Gradient Descent(10/49): loss=0.4512214457466412\n",
      "Gradient Descent(11/49): loss=0.4512214457458506\n",
      "Gradient Descent(12/49): loss=0.45122144574578377\n",
      "Gradient Descent(13/49): loss=0.4512214457457783\n",
      "Gradient Descent(14/49): loss=0.45122144574577805\n",
      "Gradient Descent(15/49): loss=0.4512214457457777\n",
      "Gradient Descent(16/49): loss=0.45122144574577794\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457778\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.45122144574577766\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457777\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457777\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45420300132220603\n",
      "Gradient Descent(2/49): loss=0.45035147373340334\n",
      "Gradient Descent(3/49): loss=0.45002756026318497\n",
      "Gradient Descent(4/49): loss=0.45000031914033967\n",
      "Gradient Descent(5/49): loss=0.4499980281619085\n",
      "Gradient Descent(6/49): loss=0.4499978354906224\n",
      "Gradient Descent(7/49): loss=0.44999781928696714\n",
      "Gradient Descent(8/49): loss=0.4499978179242398\n",
      "Gradient Descent(9/49): loss=0.44999781780963466\n",
      "Gradient Descent(10/49): loss=0.4499978177999961\n",
      "Gradient Descent(11/49): loss=0.4499978177991856\n",
      "Gradient Descent(12/49): loss=0.4499978177991175\n",
      "Gradient Descent(13/49): loss=0.4499978177991117\n",
      "Gradient Descent(14/49): loss=0.4499978177991111\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911097\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.44999781779911135\n",
      "Gradient Descent(22/49): loss=0.4499978177991112\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.4499978177991109\n",
      "Gradient Descent(25/49): loss=0.4499978177991113\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911113\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.454732851488\n",
      "Gradient Descent(2/49): loss=0.45092588429814073\n",
      "Gradient Descent(3/49): loss=0.45060571835747376\n",
      "Gradient Descent(4/49): loss=0.4505787924018635\n",
      "Gradient Descent(5/49): loss=0.4505765279289965\n",
      "Gradient Descent(6/49): loss=0.4505763374868285\n",
      "Gradient Descent(7/49): loss=0.45057632147064214\n",
      "Gradient Descent(8/49): loss=0.45057632012368093\n",
      "Gradient Descent(9/49): loss=0.4505763200104016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=0.4505763200008747\n",
      "Gradient Descent(11/49): loss=0.45057632000007386\n",
      "Gradient Descent(12/49): loss=0.4505763200000062\n",
      "Gradient Descent(13/49): loss=0.45057632000000036\n",
      "Gradient Descent(14/49): loss=0.45057632000000014\n",
      "Gradient Descent(15/49): loss=0.45057632\n",
      "Gradient Descent(16/49): loss=0.4505763199999999\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763199999998\n",
      "Gradient Descent(19/49): loss=0.45057632000000014\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.4505763199999999\n",
      "Gradient Descent(22/49): loss=0.45057632\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.45057632\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4543635137139198\n",
      "Gradient Descent(2/49): loss=0.4505254852172608\n",
      "Gradient Descent(3/49): loss=0.4502027070206916\n",
      "Gradient Descent(4/49): loss=0.45017556137436\n",
      "Gradient Descent(5/49): loss=0.45017327842550375\n",
      "Gradient Descent(6/49): loss=0.4501730864295048\n",
      "Gradient Descent(7/49): loss=0.45017307028264125\n",
      "Gradient Descent(8/49): loss=0.4501730689246902\n",
      "Gradient Descent(9/49): loss=0.4501730688104864\n",
      "Gradient Descent(10/49): loss=0.450173068800882\n",
      "Gradient Descent(11/49): loss=0.45017306880007407\n",
      "Gradient Descent(12/49): loss=0.4501730688000063\n",
      "Gradient Descent(13/49): loss=0.4501730688000006\n",
      "Gradient Descent(14/49): loss=0.4501730688000001\n",
      "Gradient Descent(15/49): loss=0.4501730687999999\n",
      "Gradient Descent(16/49): loss=0.45017306880000024\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730687999999\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.45017306879999985\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730687999998\n",
      "Gradient Descent(25/49): loss=0.4501730688000001\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4562163697014101\n",
      "Gradient Descent(2/49): loss=0.4517329259588346\n",
      "Gradient Descent(3/49): loss=0.4512738213195949\n",
      "Gradient Descent(4/49): loss=0.45122680900453666\n",
      "Gradient Descent(5/49): loss=0.45122199494347487\n",
      "Gradient Descent(6/49): loss=0.45122150198362176\n",
      "Gradient Descent(7/49): loss=0.4512214515045331\n",
      "Gradient Descent(8/49): loss=0.4512214463354744\n",
      "Gradient Descent(9/49): loss=0.4512214458061625\n",
      "Gradient Descent(10/49): loss=0.45122144575196127\n",
      "Gradient Descent(11/49): loss=0.45122144574641104\n",
      "Gradient Descent(12/49): loss=0.45122144574584255\n",
      "Gradient Descent(13/49): loss=0.4512214457457844\n",
      "Gradient Descent(14/49): loss=0.4512214457457783\n",
      "Gradient Descent(15/49): loss=0.4512214457457778\n",
      "Gradient Descent(16/49): loss=0.4512214457457779\n",
      "Gradient Descent(17/49): loss=0.45122144574577766\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.4512214457457777\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457777\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457777\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.45122144574577766\n",
      "Gradient Descent(28/49): loss=0.45122144574577794\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45511804125648214\n",
      "Gradient Descent(2/49): loss=0.45052212868114605\n",
      "Gradient Descent(3/49): loss=0.45005150723343146\n",
      "Gradient Descent(4/49): loss=0.4500033155971855\n",
      "Gradient Descent(5/49): loss=0.44999838077363385\n",
      "Gradient Descent(6/49): loss=0.4499978754477023\n",
      "Gradient Descent(7/49): loss=0.449997823702327\n",
      "Gradient Descent(8/49): loss=0.44999781840360054\n",
      "Gradient Descent(9/49): loss=0.4499978178610108\n",
      "Gradient Descent(10/49): loss=0.44999781780544945\n",
      "Gradient Descent(11/49): loss=0.44999781779976\n",
      "Gradient Descent(12/49): loss=0.44999781779917764\n",
      "Gradient Descent(13/49): loss=0.4499978177991179\n",
      "Gradient Descent(14/49): loss=0.4499978177991119\n",
      "Gradient Descent(15/49): loss=0.4499978177991111\n",
      "Gradient Descent(16/49): loss=0.44999781779911113\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991111\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.4499978177991111\n",
      "Gradient Descent(26/49): loss=0.44999781779911097\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991111\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4556373048319999\n",
      "Gradient Descent(2/49): loss=0.45109456484679694\n",
      "Gradient Descent(3/49): loss=0.45062938827231186\n",
      "Gradient Descent(4/49): loss=0.45058175419108476\n",
      "Gradient Descent(5/49): loss=0.45057687646116706\n",
      "Gradient Descent(6/49): loss=0.45057637698162356\n",
      "Gradient Descent(7/49): loss=0.45057632583491825\n",
      "Gradient Descent(8/49): loss=0.4505763205974958\n",
      "Gradient Descent(9/49): loss=0.45057632006118353\n",
      "Gradient Descent(10/49): loss=0.45057632000626513\n",
      "Gradient Descent(11/49): loss=0.45057632000064146\n",
      "Gradient Descent(12/49): loss=0.4505763200000657\n",
      "Gradient Descent(13/49): loss=0.45057632000000664\n",
      "Gradient Descent(14/49): loss=0.4505763200000006\n",
      "Gradient Descent(15/49): loss=0.4505763200000001\n",
      "Gradient Descent(16/49): loss=0.4505763200000001\n",
      "Gradient Descent(17/49): loss=0.45057632000000014\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.4505763199999999\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.45057632000000014\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763200000001\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763199999999\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4552753465548799\n",
      "Gradient Descent(2/49): loss=0.45069554204209994\n",
      "Gradient Descent(3/49): loss=0.4502265700599911\n",
      "Gradient Descent(4/49): loss=0.4501785473290232\n",
      "Gradient Descent(5/49): loss=0.450173629801372\n",
      "Gradient Descent(6/49): loss=0.4501731262465405\n",
      "Gradient Descent(7/49): loss=0.45017307468252565\n",
      "Gradient Descent(8/49): loss=0.4501730694023706\n",
      "Gradient Descent(9/49): loss=0.45017306886168285\n",
      "Gradient Descent(10/49): loss=0.4501730688063164\n",
      "Gradient Descent(11/49): loss=0.4501730688006469\n",
      "Gradient Descent(12/49): loss=0.4501730688000662\n",
      "Gradient Descent(13/49): loss=0.45017306880000674\n",
      "Gradient Descent(14/49): loss=0.4501730688000007\n",
      "Gradient Descent(15/49): loss=0.45017306880000024\n",
      "Gradient Descent(16/49): loss=0.4501730687999998\n",
      "Gradient Descent(17/49): loss=0.4501730688\n",
      "Gradient Descent(18/49): loss=0.4501730688\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730688000002\n",
      "Gradient Descent(21/49): loss=0.4501730688000001\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.4501730688000002\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688000002\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000002\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688000002\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000002\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45719681864191997\n",
      "Gradient Descent(2/49): loss=0.4519534289255553\n",
      "Gradient Descent(3/49): loss=0.4513111136853006\n",
      "Gradient Descent(4/49): loss=0.4512324300683693\n",
      "Gradient Descent(5/49): loss=0.45122279132529514\n",
      "Gradient Descent(6/49): loss=0.4512216105792688\n",
      "Gradient Descent(7/49): loss=0.4512214659378803\n",
      "Gradient Descent(8/49): loss=0.45122144821931043\n",
      "Gradient Descent(9/49): loss=0.4512214460487856\n",
      "Gradient Descent(10/49): loss=0.45122144578289636\n",
      "Gradient Descent(11/49): loss=0.4512214457503247\n",
      "Gradient Descent(12/49): loss=0.4512214457463349\n",
      "Gradient Descent(13/49): loss=0.45122144574584605\n",
      "Gradient Descent(14/49): loss=0.45122144574578643\n",
      "Gradient Descent(15/49): loss=0.4512214457457787\n",
      "Gradient Descent(16/49): loss=0.45122144574577794\n",
      "Gradient Descent(17/49): loss=0.4512214457457779\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.4512214457457779\n",
      "Gradient Descent(21/49): loss=0.4512214457457779\n",
      "Gradient Descent(22/49): loss=0.4512214457457778\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457779\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457777\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457779\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457778\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.45612308511872013\n",
      "Gradient Descent(2/49): loss=0.4507481630457631\n",
      "Gradient Descent(3/49): loss=0.45008973509182604\n",
      "Gradient Descent(4/49): loss=0.45000907766746867\n",
      "Gradient Descent(5/49): loss=0.44999919713298503\n",
      "Gradient Descent(6/49): loss=0.44999798676751057\n",
      "Gradient Descent(7/49): loss=0.4499978384977399\n",
      "Gradient Descent(8/49): loss=0.4499978203346932\n",
      "Gradient Descent(9/49): loss=0.4499978181097201\n",
      "Gradient Descent(10/49): loss=0.44999781783716064\n",
      "Gradient Descent(11/49): loss=0.4499978178037722\n",
      "Gradient Descent(12/49): loss=0.4499978177996821\n",
      "Gradient Descent(13/49): loss=0.4499978177991809\n",
      "Gradient Descent(14/49): loss=0.4499978177991197\n",
      "Gradient Descent(15/49): loss=0.4499978177991121\n",
      "Gradient Descent(16/49): loss=0.4499978177991112\n",
      "Gradient Descent(17/49): loss=0.44999781779911113\n",
      "Gradient Descent(18/49): loss=0.44999781779911097\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.44999781779911113\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991112\n",
      "Gradient Descent(24/49): loss=0.44999781779911097\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911097\n",
      "Gradient Descent(28/49): loss=0.4499978177991109\n",
      "Gradient Descent(29/49): loss=0.4499978177991113\n",
      "Gradient Descent(30/49): loss=0.44999781779911147\n",
      "Gradient Descent(31/49): loss=0.4499978177991113\n",
      "Gradient Descent(32/49): loss=0.4499978177991112\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45663072079999995\n",
      "Gradient Descent(2/49): loss=0.4513179840979999\n",
      "Gradient Descent(3/49): loss=0.4506671738520051\n",
      "Gradient Descent(4/49): loss=0.4505874495968705\n",
      "Gradient Descent(5/49): loss=0.45057768337561654\n",
      "Gradient Descent(6/49): loss=0.45057648701351305\n",
      "Gradient Descent(7/49): loss=0.45057634045915523\n",
      "Gradient Descent(8/49): loss=0.4505763225062467\n",
      "Gradient Descent(9/49): loss=0.45057632030701544\n",
      "Gradient Descent(10/49): loss=0.45057632003760945\n",
      "Gradient Descent(11/49): loss=0.45057632000460723\n",
      "Gradient Descent(12/49): loss=0.45057632000056436\n",
      "Gradient Descent(13/49): loss=0.4505763200000692\n",
      "Gradient Descent(14/49): loss=0.4505763200000085\n",
      "Gradient Descent(15/49): loss=0.45057632000000114\n",
      "Gradient Descent(16/49): loss=0.4505763200000003\n",
      "Gradient Descent(17/49): loss=0.4505763200000001\n",
      "Gradient Descent(18/49): loss=0.4505763200000001\n",
      "Gradient Descent(19/49): loss=0.4505763199999998\n",
      "Gradient Descent(20/49): loss=0.4505763199999998\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.45057632\n",
      "Gradient Descent(23/49): loss=0.4505763199999999\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763199999998\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.45057632\n",
      "Gradient Descent(29/49): loss=0.45057632000000014\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45627686787200006\n",
      "Gradient Descent(2/49): loss=0.45092078418632\n",
      "Gradient Descent(3/49): loss=0.4502646639348241\n",
      "Gradient Descent(4/49): loss=0.45018428920401604\n",
      "Gradient Descent(5/49): loss=0.45017444329949197\n",
      "Gradient Descent(6/49): loss=0.4501732371761878\n",
      "Gradient Descent(7/49): loss=0.4501730894260831\n",
      "Gradient Descent(8/49): loss=0.4501730713266952\n",
      "Gradient Descent(9/49): loss=0.45017306910952026\n",
      "Gradient Descent(10/49): loss=0.45017306883791625\n",
      "Gradient Descent(11/49): loss=0.4501730688046447\n",
      "Gradient Descent(12/49): loss=0.4501730688005691\n",
      "Gradient Descent(13/49): loss=0.4501730688000695\n",
      "Gradient Descent(14/49): loss=0.4501730688000086\n",
      "Gradient Descent(15/49): loss=0.45017306880000096\n",
      "Gradient Descent(16/49): loss=0.4501730688000001\n",
      "Gradient Descent(17/49): loss=0.4501730687999999\n",
      "Gradient Descent(18/49): loss=0.4501730688000002\n",
      "Gradient Descent(19/49): loss=0.4501730687999999\n",
      "Gradient Descent(20/49): loss=0.4501730687999999\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730688\n",
      "Gradient Descent(24/49): loss=0.45017306879999985\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730687999999\n",
      "Gradient Descent(28/49): loss=0.4501730687999999\n",
      "Gradient Descent(29/49): loss=0.4501730688000002\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688000002\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45826506898008745\n",
      "Gradient Descent(2/49): loss=0.45223854494081184\n",
      "Gradient Descent(3/49): loss=0.4513683148695406\n",
      "Gradient Descent(4/49): loss=0.45124265364724897\n",
      "Gradient Descent(5/49): loss=0.45122450816675025\n",
      "Gradient Descent(6/49): loss=0.4512218879593662\n",
      "Gradient Descent(7/49): loss=0.45122150960142005\n",
      "Gradient Descent(8/49): loss=0.45122145496653243\n",
      "Gradient Descent(9/49): loss=0.45122144707725487\n",
      "Gradient Descent(10/49): loss=0.4512214459380431\n",
      "Gradient Descent(11/49): loss=0.4512214457735409\n",
      "Gradient Descent(12/49): loss=0.45122144574978684\n",
      "Gradient Descent(13/49): loss=0.45122144574635664\n",
      "Gradient Descent(14/49): loss=0.45122144574586137\n",
      "Gradient Descent(15/49): loss=0.45122144574578965\n",
      "Gradient Descent(16/49): loss=0.4512214457457795\n",
      "Gradient Descent(17/49): loss=0.45122144574577805\n",
      "Gradient Descent(18/49): loss=0.4512214457457777\n",
      "Gradient Descent(19/49): loss=0.45122144574577766\n",
      "Gradient Descent(20/49): loss=0.45122144574577755\n",
      "Gradient Descent(21/49): loss=0.45122144574577766\n",
      "Gradient Descent(22/49): loss=0.45122144574577766\n",
      "Gradient Descent(23/49): loss=0.45122144574577766\n",
      "Gradient Descent(24/49): loss=0.4512214457457778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.45122144574577766\n",
      "Gradient Descent(27/49): loss=0.45122144574577766\n",
      "Gradient Descent(28/49): loss=0.45122144574577794\n",
      "Gradient Descent(29/49): loss=0.45122144574577766\n",
      "Gradient Descent(30/49): loss=0.45122144574577794\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457779\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45721813290891966\n",
      "Gradient Descent(2/49): loss=0.4510404313009671\n",
      "Gradient Descent(3/49): loss=0.4501483711887789\n",
      "Gradient Descent(4/49): loss=0.4500195577085791\n",
      "Gradient Descent(5/49): loss=0.45000095704203813\n",
      "Gradient Descent(6/49): loss=0.4499982711057899\n",
      "Gradient Descent(7/49): loss=0.44999788325659534\n",
      "Gradient Descent(8/49): loss=0.4499978272511718\n",
      "Gradient Descent(9/49): loss=0.4499978191639887\n",
      "Gradient Descent(10/49): loss=0.44999781799619926\n",
      "Gradient Descent(11/49): loss=0.4499978178275705\n",
      "Gradient Descent(12/49): loss=0.44999781780322073\n",
      "Gradient Descent(13/49): loss=0.44999781779970444\n",
      "Gradient Descent(14/49): loss=0.4499978177991969\n",
      "Gradient Descent(15/49): loss=0.44999781779912346\n",
      "Gradient Descent(16/49): loss=0.4499978177991131\n",
      "Gradient Descent(17/49): loss=0.44999781779911147\n",
      "Gradient Descent(18/49): loss=0.4499978177991113\n",
      "Gradient Descent(19/49): loss=0.44999781779911113\n",
      "Gradient Descent(20/49): loss=0.4499978177991112\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.44999781779911097\n",
      "Gradient Descent(23/49): loss=0.4499978177991113\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991111\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.4499978177991111\n",
      "Gradient Descent(30/49): loss=0.4499978177991112\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.4499978177991112\n",
      "Gradient Descent(33/49): loss=0.4499978177991112\n",
      "Gradient Descent(34/49): loss=0.4499978177991112\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45771309939200006\n",
      "Gradient Descent(2/49): loss=0.4516068709442048\n",
      "Gradient Descent(3/49): loss=0.45072513155634303\n",
      "Gradient Descent(4/49): loss=0.45059780838873587\n",
      "Gradient Descent(5/49): loss=0.45057942292333325\n",
      "Gradient Descent(6/49): loss=0.4505767680621292\n",
      "Gradient Descent(7/49): loss=0.4505763847001715\n",
      "Gradient Descent(8/49): loss=0.45057632934270464\n",
      "Gradient Descent(9/49): loss=0.45057632134908665\n",
      "Gradient Descent(10/49): loss=0.450576320194808\n",
      "Gradient Descent(11/49): loss=0.4505763200281304\n",
      "Gradient Descent(12/49): loss=0.450576320004062\n",
      "Gradient Descent(13/49): loss=0.45057632000058656\n",
      "Gradient Descent(14/49): loss=0.4505763200000847\n",
      "Gradient Descent(15/49): loss=0.4505763200000124\n",
      "Gradient Descent(16/49): loss=0.45057632000000175\n",
      "Gradient Descent(17/49): loss=0.4505763200000002\n",
      "Gradient Descent(18/49): loss=0.45057632\n",
      "Gradient Descent(19/49): loss=0.45057632000000014\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.4505763199999998\n",
      "Gradient Descent(22/49): loss=0.4505763200000001\n",
      "Gradient Descent(23/49): loss=0.4505763200000002\n",
      "Gradient Descent(24/49): loss=0.4505763200000001\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.45057632\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057632000000014\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.45057631999999975\n",
      "Gradient Descent(33/49): loss=0.45057632\n",
      "Gradient Descent(34/49): loss=0.4505763200000001\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4573680776652799\n",
      "Gradient Descent(2/49): loss=0.45121202808014654\n",
      "Gradient Descent(3/49): loss=0.450323094520053\n",
      "Gradient Descent(4/49): loss=0.4501947325139757\n",
      "Gradient Descent(5/49): loss=0.45017619704029793\n",
      "Gradient Descent(6/49): loss=0.4501735205178991\n",
      "Gradient Descent(7/49): loss=0.45017313402806447\n",
      "Gradient Descent(8/49): loss=0.4501730782189325\n",
      "Gradient Descent(9/49): loss=0.4501730701600939\n",
      "Gradient Descent(10/49): loss=0.4501730689963975\n",
      "Gradient Descent(11/49): loss=0.4501730688283597\n",
      "Gradient Descent(12/49): loss=0.4501730688040952\n",
      "Gradient Descent(13/49): loss=0.4501730688005913\n",
      "Gradient Descent(14/49): loss=0.4501730688000854\n",
      "Gradient Descent(15/49): loss=0.45017306880001234\n",
      "Gradient Descent(16/49): loss=0.45017306880000185\n",
      "Gradient Descent(17/49): loss=0.4501730688000006\n",
      "Gradient Descent(18/49): loss=0.45017306879999985\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.4501730688\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.45017306879999985\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.45017306879999985\n",
      "Gradient Descent(25/49): loss=0.4501730687999999\n",
      "Gradient Descent(26/49): loss=0.4501730687999998\n",
      "Gradient Descent(27/49): loss=0.45017306879999985\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730687999998\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730688000002\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688000002\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4594211207159127\n",
      "Gradient Descent(2/49): loss=0.45259981110825764\n",
      "Gradient Descent(3/49): loss=0.45145314896321054\n",
      "Gradient Descent(4/49): loss=0.45126039505662824\n",
      "Gradient Descent(5/49): loss=0.4512279931249317\n",
      "Gradient Descent(6/49): loss=0.45122254636021364\n",
      "Gradient Descent(7/49): loss=0.4512216307590646\n",
      "Gradient Descent(8/49): loss=0.4512214768465113\n",
      "Gradient Descent(9/49): loss=0.4512214509738111\n",
      "Gradient Descent(10/49): loss=0.4512214466246103\n",
      "Gradient Descent(11/49): loss=0.4512214458935095\n",
      "Gradient Descent(12/49): loss=0.4512214457706116\n",
      "Gradient Descent(13/49): loss=0.4512214457499523\n",
      "Gradient Descent(14/49): loss=0.4512214457464794\n",
      "Gradient Descent(15/49): loss=0.4512214457458958\n",
      "Gradient Descent(16/49): loss=0.45122144574579753\n",
      "Gradient Descent(17/49): loss=0.45122144574578116\n",
      "Gradient Descent(18/49): loss=0.4512214457457783\n",
      "Gradient Descent(19/49): loss=0.45122144574577794\n",
      "Gradient Descent(20/49): loss=0.45122144574577766\n",
      "Gradient Descent(21/49): loss=0.4512214457457779\n",
      "Gradient Descent(22/49): loss=0.4512214457457777\n",
      "Gradient Descent(23/49): loss=0.45122144574577766\n",
      "Gradient Descent(24/49): loss=0.45122144574577805\n",
      "Gradient Descent(25/49): loss=0.4512214457457779\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457778\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.45122144574577766\n",
      "Gradient Descent(32/49): loss=0.45122144574577766\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.45122144574577766\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.45122144574577794\n",
      "Gradient Descent(37/49): loss=0.45122144574577766\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4584031846270805\n",
      "Gradient Descent(2/49): loss=0.45141075996289276\n",
      "Gradient Descent(3/49): loss=0.45023533337684285\n",
      "Gradient Descent(4/49): loss=0.4500377441677278\n",
      "Gradient Descent(5/49): loss=0.4500045294216756\n",
      "Gradient Descent(6/49): loss=0.44999894602286405\n",
      "Gradient Descent(7/49): loss=0.44999800745352403\n",
      "Gradient Descent(8/49): loss=0.449997849680018\n",
      "Gradient Descent(9/49): loss=0.4499978231582914\n",
      "Gradient Descent(10/49): loss=0.44999781869998934\n",
      "Gradient Descent(11/49): loss=0.44999781795054883\n",
      "Gradient Descent(12/49): loss=0.44999781782456777\n",
      "Gradient Descent(13/49): loss=0.44999781780339027\n",
      "Gradient Descent(14/49): loss=0.44999781779983034\n",
      "Gradient Descent(15/49): loss=0.44999781779923204\n",
      "Gradient Descent(16/49): loss=0.44999781779913145\n",
      "Gradient Descent(17/49): loss=0.4499978177991145\n",
      "Gradient Descent(18/49): loss=0.4499978177991117\n",
      "Gradient Descent(19/49): loss=0.44999781779911135\n",
      "Gradient Descent(20/49): loss=0.44999781779911113\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.44999781779911113\n",
      "Gradient Descent(25/49): loss=0.44999781779911097\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.4499978177991112\n",
      "Gradient Descent(28/49): loss=0.4499978177991113\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.44999781779911135\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.4499978177991112\n",
      "Gradient Descent(34/49): loss=0.4499978177991113\n",
      "Gradient Descent(35/49): loss=0.4499978177991111\n",
      "Gradient Descent(36/49): loss=0.4499978177991112\n",
      "Gradient Descent(37/49): loss=0.44999781779911135\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911113\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.458884440608\n",
      "Gradient Descent(2/49): loss=0.451972915074205\n",
      "Gradient Descent(3/49): loss=0.450811087631974\n",
      "Gradient Descent(4/49): loss=0.45061578443893485\n",
      "Gradient Descent(5/49): loss=0.4505829539721849\n",
      "Gradient Descent(6/49): loss=0.4505774351707241\n",
      "Gradient Descent(7/49): loss=0.45057650746019884\n",
      "Gradient Descent(8/49): loss=0.45057635151205927\n",
      "Gradient Descent(9/49): loss=0.45057632529717717\n",
      "Gradient Descent(10/49): loss=0.45057632089045535\n",
      "Gradient Descent(11/49): loss=0.45057632014968557\n",
      "Gradient Descent(12/49): loss=0.450576320025162\n",
      "Gradient Descent(13/49): loss=0.4505763200042298\n",
      "Gradient Descent(14/49): loss=0.4505763200007111\n",
      "Gradient Descent(15/49): loss=0.4505763200001194\n",
      "Gradient Descent(16/49): loss=0.45057632000002007\n",
      "Gradient Descent(17/49): loss=0.4505763200000032\n",
      "Gradient Descent(18/49): loss=0.45057632000000053\n",
      "Gradient Descent(19/49): loss=0.45057632\n",
      "Gradient Descent(20/49): loss=0.45057632\n",
      "Gradient Descent(21/49): loss=0.4505763200000001\n",
      "Gradient Descent(22/49): loss=0.4505763199999999\n",
      "Gradient Descent(23/49): loss=0.45057632\n",
      "Gradient Descent(24/49): loss=0.45057632\n",
      "Gradient Descent(25/49): loss=0.4505763199999998\n",
      "Gradient Descent(26/49): loss=0.45057632000000014\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763199999999\n",
      "Gradient Descent(29/49): loss=0.4505763200000003\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.45057632000000014\n",
      "Gradient Descent(32/49): loss=0.4505763199999999\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763200000002\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763200000001\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.4585489759347199\n",
      "Gradient Descent(2/49): loss=0.45158105878934685\n",
      "Gradient Descent(3/49): loss=0.4504097519172091\n",
      "Gradient Descent(4/49): loss=0.45021285523200283\n",
      "Gradient Descent(5/49): loss=0.4501797568992197\n",
      "Gradient Descent(6/49): loss=0.45017419306947903\n",
      "Gradient Descent(7/49): loss=0.4501732577896995\n",
      "Gradient Descent(8/49): loss=0.4501731005691684\n",
      "Gradient Descent(9/49): loss=0.45017307414039737\n",
      "Gradient Descent(10/49): loss=0.45017306969772075\n",
      "Gradient Descent(11/49): loss=0.45017306895090686\n",
      "Gradient Descent(12/49): loss=0.4501730688253677\n",
      "Gradient Descent(13/49): loss=0.4501730688042642\n",
      "Gradient Descent(14/49): loss=0.4501730688007167\n",
      "Gradient Descent(15/49): loss=0.4501730688001205\n",
      "Gradient Descent(16/49): loss=0.45017306880002034\n",
      "Gradient Descent(17/49): loss=0.45017306880000335\n",
      "Gradient Descent(18/49): loss=0.45017306880000063\n",
      "Gradient Descent(19/49): loss=0.4501730688000001\n",
      "Gradient Descent(20/49): loss=0.45017306879999985\n",
      "Gradient Descent(21/49): loss=0.4501730687999999\n",
      "Gradient Descent(22/49): loss=0.4501730688\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.45017306879999985\n",
      "Gradient Descent(25/49): loss=0.45017306879999985\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730687999999\n",
      "Gradient Descent(28/49): loss=0.4501730688000002\n",
      "Gradient Descent(29/49): loss=0.45017306880000024\n",
      "Gradient Descent(30/49): loss=0.45017306879999985\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730687999999\n",
      "Gradient Descent(33/49): loss=0.4501730688\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000002\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4606649738493953\n",
      "Gradient Descent(2/49): loss=0.4530497127866384\n",
      "Gradient Descent(3/49): loss=0.45157539824488835\n",
      "Gradient Descent(4/49): loss=0.45128997094960566\n",
      "Gradient Descent(5/49): loss=0.4512347122252388\n",
      "Gradient Descent(6/49): loss=0.45122401413620133\n",
      "Gradient Descent(7/49): loss=0.45122194298616364\n",
      "Gradient Descent(8/49): loss=0.45122154201151665\n",
      "Gradient Descent(9/49): loss=0.4512214643828246\n",
      "Gradient Descent(10/49): loss=0.45122144935391006\n",
      "Gradient Descent(11/49): loss=0.4512214464443123\n",
      "Gradient Descent(12/49): loss=0.4512214458810141\n",
      "Gradient Descent(13/49): loss=0.4512214457719593\n",
      "Gradient Descent(14/49): loss=0.45122144575084666\n",
      "Gradient Descent(15/49): loss=0.4512214457467591\n",
      "Gradient Descent(16/49): loss=0.45122144574596773\n",
      "Gradient Descent(17/49): loss=0.4512214457458145\n",
      "Gradient Descent(18/49): loss=0.45122144574578477\n",
      "Gradient Descent(19/49): loss=0.451221445745779\n",
      "Gradient Descent(20/49): loss=0.45122144574577805\n",
      "Gradient Descent(21/49): loss=0.4512214457457779\n",
      "Gradient Descent(22/49): loss=0.4512214457457779\n",
      "Gradient Descent(23/49): loss=0.45122144574577766\n",
      "Gradient Descent(24/49): loss=0.45122144574577766\n",
      "Gradient Descent(25/49): loss=0.45122144574577766\n",
      "Gradient Descent(26/49): loss=0.4512214457457778\n",
      "Gradient Descent(27/49): loss=0.4512214457457779\n",
      "Gradient Descent(28/49): loss=0.4512214457457777\n",
      "Gradient Descent(29/49): loss=0.45122144574577794\n",
      "Gradient Descent(30/49): loss=0.45122144574577766\n",
      "Gradient Descent(31/49): loss=0.45122144574577755\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457775\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457777\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457779\n",
      "Gradient Descent(39/49): loss=0.4512214457457777\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457777\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457777\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4596782402732033\n",
      "Gradient Descent(2/49): loss=0.45187194759009497\n",
      "Gradient Descent(3/49): loss=0.45036064932664543\n",
      "Gradient Descent(4/49): loss=0.4500680619828417\n",
      "Gradient Descent(5/49): loss=0.4500114170730814\n",
      "Gradient Descent(6/49): loss=0.45000045061855165\n",
      "Gradient Descent(7/49): loss=0.449998327512955\n",
      "Gradient Descent(8/49): loss=0.44999791647971144\n",
      "Gradient Descent(9/49): loss=0.4499978369036753\n",
      "Gradient Descent(10/49): loss=0.4499978214977548\n",
      "Gradient Descent(11/49): loss=0.4499978185151685\n",
      "Gradient Descent(12/49): loss=0.4499978179377397\n",
      "Gradient Descent(13/49): loss=0.44999781782594966\n",
      "Gradient Descent(14/49): loss=0.449997817804307\n",
      "Gradient Descent(15/49): loss=0.44999781780011705\n",
      "Gradient Descent(16/49): loss=0.4499978177993059\n",
      "Gradient Descent(17/49): loss=0.44999781779914877\n",
      "Gradient Descent(18/49): loss=0.4499978177991187\n",
      "Gradient Descent(19/49): loss=0.4499978177991127\n",
      "Gradient Descent(20/49): loss=0.4499978177991113\n",
      "Gradient Descent(21/49): loss=0.4499978177991112\n",
      "Gradient Descent(22/49): loss=0.4499978177991111\n",
      "Gradient Descent(23/49): loss=0.4499978177991111\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.4499978177991112\n",
      "Gradient Descent(27/49): loss=0.44999781779911135\n",
      "Gradient Descent(28/49): loss=0.4499978177991112\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991113\n",
      "Gradient Descent(32/49): loss=0.4499978177991112\n",
      "Gradient Descent(33/49): loss=0.4499978177991113\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.4499978177991112\n",
      "Gradient Descent(41/49): loss=0.44999781779911135\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46014474444799996\n",
      "Gradient Descent(2/49): loss=0.4524287669731325\n",
      "Gradient Descent(3/49): loss=0.45093495373399833\n",
      "Gradient Descent(4/49): loss=0.450645751490902\n",
      "Gradient Descent(5/49): loss=0.4505897619366386\n",
      "Gradient Descent(6/49): loss=0.4505789223589334\n",
      "Gradient Descent(7/49): loss=0.4505768238166895\n",
      "Gradient Descent(8/49): loss=0.4505764175389111\n",
      "Gradient Descent(9/49): loss=0.45057633888353316\n",
      "Gradient Descent(10/49): loss=0.45057632365585204\n",
      "Gradient Descent(11/49): loss=0.45057632070777304\n",
      "Gradient Descent(12/49): loss=0.4505763201370249\n",
      "Gradient Descent(13/49): loss=0.4505763200265281\n",
      "Gradient Descent(14/49): loss=0.4505763200051359\n",
      "Gradient Descent(15/49): loss=0.45057632000099435\n",
      "Gradient Descent(16/49): loss=0.4505763200001924\n",
      "Gradient Descent(17/49): loss=0.4505763200000371\n",
      "Gradient Descent(18/49): loss=0.45057632000000736\n",
      "Gradient Descent(19/49): loss=0.45057632000000136\n",
      "Gradient Descent(20/49): loss=0.4505763200000001\n",
      "Gradient Descent(21/49): loss=0.45057632\n",
      "Gradient Descent(22/49): loss=0.4505763199999998\n",
      "Gradient Descent(23/49): loss=0.45057632000000014\n",
      "Gradient Descent(24/49): loss=0.4505763199999998\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.45057632\n",
      "Gradient Descent(29/49): loss=0.4505763199999999\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.45057631999999975\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999999\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45981956268031987\n",
      "Gradient Descent(2/49): loss=0.4520406300152301\n",
      "Gradient Descent(3/49): loss=0.45053462865126864\n",
      "Gradient Descent(4/49): loss=0.45024306678720555\n",
      "Gradient Descent(5/49): loss=0.4501866204103231\n",
      "Gradient Descent(6/49): loss=0.4501756923917587\n",
      "Gradient Descent(7/49): loss=0.45017357672736447\n",
      "Gradient Descent(8/49): loss=0.45017316713473765\n",
      "Gradient Descent(9/49): loss=0.4501730878376052\n",
      "Gradient Descent(10/49): loss=0.45017307248568056\n",
      "Gradient Descent(11/49): loss=0.45017306951354763\n",
      "Gradient Descent(12/49): loss=0.4501730689381428\n",
      "Gradient Descent(13/49): loss=0.4501730688267444\n",
      "Gradient Descent(14/49): loss=0.4501730688051777\n",
      "Gradient Descent(15/49): loss=0.45017306880100216\n",
      "Gradient Descent(16/49): loss=0.4501730688001942\n",
      "Gradient Descent(17/49): loss=0.45017306880003766\n",
      "Gradient Descent(18/49): loss=0.45017306880000724\n",
      "Gradient Descent(19/49): loss=0.45017306880000146\n",
      "Gradient Descent(20/49): loss=0.45017306880000024\n",
      "Gradient Descent(21/49): loss=0.4501730688\n",
      "Gradient Descent(22/49): loss=0.4501730688000001\n",
      "Gradient Descent(23/49): loss=0.4501730687999999\n",
      "Gradient Descent(24/49): loss=0.4501730687999999\n",
      "Gradient Descent(25/49): loss=0.4501730688000002\n",
      "Gradient Descent(26/49): loss=0.4501730688\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.4501730688000001\n",
      "Gradient Descent(29/49): loss=0.45017306879999985\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.45017306879999985\n",
      "Gradient Descent(36/49): loss=0.4501730688000002\n",
      "Gradient Descent(37/49): loss=0.4501730688\n",
      "Gradient Descent(38/49): loss=0.4501730688000001\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46199662838053535\n",
      "Gradient Descent(2/49): loss=0.4536016835897957\n",
      "Gradient Descent(3/49): loss=0.4517472402855213\n",
      "Gradient Descent(4/49): loss=0.45133759375960714\n",
      "Gradient Descent(5/49): loss=0.4512471028420326\n",
      "Gradient Descent(6/49): loss=0.4512271133983406\n",
      "Gradient Descent(7/49): loss=0.4512226977302291\n",
      "Gradient Descent(8/49): loss=0.45122172230914304\n",
      "Gradient Descent(9/49): loss=0.4512215068386252\n",
      "Gradient Descent(10/49): loss=0.4512214592411877\n",
      "Gradient Descent(11/49): loss=0.45122144872691394\n",
      "Gradient Descent(12/49): loss=0.4512214464043108\n",
      "Gradient Descent(13/49): loss=0.4512214458912477\n",
      "Gradient Descent(14/49): loss=0.451221445777912\n",
      "Gradient Descent(15/49): loss=0.45122144575287626\n",
      "Gradient Descent(16/49): loss=0.45122144574734585\n",
      "Gradient Descent(17/49): loss=0.45122144574612416\n",
      "Gradient Descent(18/49): loss=0.4512214457458543\n",
      "Gradient Descent(19/49): loss=0.4512214457457946\n",
      "Gradient Descent(20/49): loss=0.45122144574578155\n",
      "Gradient Descent(21/49): loss=0.4512214457457788\n",
      "Gradient Descent(22/49): loss=0.4512214457457781\n",
      "Gradient Descent(23/49): loss=0.4512214457457778\n",
      "Gradient Descent(24/49): loss=0.4512214457457777\n",
      "Gradient Descent(25/49): loss=0.4512214457457777\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457777\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457778\n",
      "Gradient Descent(32/49): loss=0.4512214457457779\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.45122144574577766\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.45122144574577794\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4610432998472872\n",
      "Gradient Descent(2/49): loss=0.4524377647835534\n",
      "Gradient Descent(3/49): loss=0.4505368020879745\n",
      "Gradient Descent(4/49): loss=0.45011687942852097\n",
      "Gradient Descent(5/49): loss=0.45002411851304785\n",
      "Gradient Descent(6/49): loss=0.4500036276268198\n",
      "Gradient Descent(7/49): loss=0.449999101190052\n",
      "Gradient Descent(8/49): loss=0.4499981013001699\n",
      "Gradient Descent(9/49): loss=0.44999788042449523\n",
      "Gradient Descent(10/49): loss=0.4499978316330583\n",
      "Gradient Descent(11/49): loss=0.44999782085503026\n",
      "Gradient Descent(12/49): loss=0.44999781847416387\n",
      "Gradient Descent(13/49): loss=0.4499978179482301\n",
      "Gradient Descent(14/49): loss=0.44999781783205156\n",
      "Gradient Descent(15/49): loss=0.4499978178063877\n",
      "Gradient Descent(16/49): loss=0.44999781780071846\n",
      "Gradient Descent(17/49): loss=0.4499978177994663\n",
      "Gradient Descent(18/49): loss=0.4499978177991897\n",
      "Gradient Descent(19/49): loss=0.44999781779912845\n",
      "Gradient Descent(20/49): loss=0.4499978177991149\n",
      "Gradient Descent(21/49): loss=0.4499978177991121\n",
      "Gradient Descent(22/49): loss=0.44999781779911113\n",
      "Gradient Descent(23/49): loss=0.44999781779911113\n",
      "Gradient Descent(24/49): loss=0.4499978177991112\n",
      "Gradient Descent(25/49): loss=0.4499978177991113\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.4499978177991113\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.4499978177991111\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.4499978177991112\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991112\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911097\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.44999781779911074\n",
      "Gradient Descent(41/49): loss=0.4499978177991113\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.44999781779911135\n",
      "Gradient Descent(46/49): loss=0.4499978177991111\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46149401091200026\n",
      "Gradient Descent(2/49): loss=0.45298803792246084\n",
      "Gradient Descent(3/49): loss=0.4511090684890717\n",
      "Gradient Descent(4/49): loss=0.45069400414123606\n",
      "Gradient Descent(5/49): loss=0.4506023164267991\n",
      "Gradient Descent(6/49): loss=0.4505820626106799\n",
      "Gradient Descent(7/49): loss=0.45057758854269914\n",
      "Gradient Descent(8/49): loss=0.45057660022108226\n",
      "Gradient Descent(9/49): loss=0.45057638190083715\n",
      "Gradient Descent(10/49): loss=0.4505763336738949\n",
      "Gradient Descent(11/49): loss=0.45057632302056333\n",
      "Gradient Descent(12/49): loss=0.4505763206672426\n",
      "Gradient Descent(13/49): loss=0.4505763201473939\n",
      "Gradient Descent(14/49): loss=0.4505763200325592\n",
      "Gradient Descent(15/49): loss=0.45057632000719244\n",
      "Gradient Descent(16/49): loss=0.4505763200015888\n",
      "Gradient Descent(17/49): loss=0.45057632000035086\n",
      "Gradient Descent(18/49): loss=0.4505763200000775\n",
      "Gradient Descent(19/49): loss=0.45057632000001724\n",
      "Gradient Descent(20/49): loss=0.45057632000000397\n",
      "Gradient Descent(21/49): loss=0.4505763200000009\n",
      "Gradient Descent(22/49): loss=0.4505763200000002\n",
      "Gradient Descent(23/49): loss=0.4505763200000001\n",
      "Gradient Descent(24/49): loss=0.45057632\n",
      "Gradient Descent(25/49): loss=0.4505763200000001\n",
      "Gradient Descent(26/49): loss=0.4505763199999999\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.4505763200000001\n",
      "Gradient Descent(31/49): loss=0.45057632000000014\n",
      "Gradient Descent(32/49): loss=0.45057632\n",
      "Gradient Descent(33/49): loss=0.45057632000000014\n",
      "Gradient Descent(34/49): loss=0.4505763199999998\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.4505763200000001\n",
      "Gradient Descent(37/49): loss=0.45057632000000014\n",
      "Gradient Descent(38/49): loss=0.45057632000000014\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763200000002\n",
      "Gradient Descent(44/49): loss=0.45057631999999975\n",
      "Gradient Descent(45/49): loss=0.4505763199999999\n",
      "Gradient Descent(46/49): loss=0.4505763199999999\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4611798379020801\n",
      "Gradient Descent(2/49): loss=0.45260446409464994\n",
      "Gradient Descent(3/49): loss=0.450710164020588\n",
      "Gradient Descent(4/49): loss=0.45029171313422794\n",
      "Gradient Descent(5/49): loss=0.45019927733343085\n",
      "Gradient Descent(6/49): loss=0.45017885826503495\n",
      "Gradient Descent(7/49): loss=0.45017434769282627\n",
      "Gradient Descent(8/49): loss=0.45017335130742525\n",
      "Gradient Descent(9/49): loss=0.4501731312058902\n",
      "Gradient Descent(10/49): loss=0.45017308258546107\n",
      "Gradient Descent(11/49): loss=0.45017307184520855\n",
      "Gradient Descent(12/49): loss=0.4501730694726865\n",
      "Gradient Descent(13/49): loss=0.45017306894859654\n",
      "Gradient Descent(14/49): loss=0.45017306883282476\n",
      "Gradient Descent(15/49): loss=0.450173068807251\n",
      "Gradient Descent(16/49): loss=0.4501730688016018\n",
      "Gradient Descent(17/49): loss=0.45017306880035385\n",
      "Gradient Descent(18/49): loss=0.45017306880007824\n",
      "Gradient Descent(19/49): loss=0.4501730688000174\n",
      "Gradient Descent(20/49): loss=0.4501730688000038\n",
      "Gradient Descent(21/49): loss=0.4501730688000008\n",
      "Gradient Descent(22/49): loss=0.4501730688000002\n",
      "Gradient Descent(23/49): loss=0.4501730688000001\n",
      "Gradient Descent(24/49): loss=0.4501730688\n",
      "Gradient Descent(25/49): loss=0.4501730687999998\n",
      "Gradient Descent(26/49): loss=0.4501730688000001\n",
      "Gradient Descent(27/49): loss=0.4501730687999999\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730688\n",
      "Gradient Descent(30/49): loss=0.4501730688000001\n",
      "Gradient Descent(31/49): loss=0.4501730688\n",
      "Gradient Descent(32/49): loss=0.4501730688000001\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.4501730687999999\n",
      "Gradient Descent(35/49): loss=0.4501730687999999\n",
      "Gradient Descent(36/49): loss=0.4501730687999999\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000002\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000002\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4634160843093334\n",
      "Gradient Descent(2/49): loss=0.4542701053866668\n",
      "Gradient Descent(3/49): loss=0.45198361065599996\n",
      "Gradient Descent(4/49): loss=0.4514119869733334\n",
      "Gradient Descent(5/49): loss=0.4512690810526668\n",
      "Gradient Descent(6/49): loss=0.4512333545725001\n",
      "Gradient Descent(7/49): loss=0.45122442295245835\n",
      "Gradient Descent(8/49): loss=0.45122219004744796\n",
      "Gradient Descent(9/49): loss=0.4512216318211952\n",
      "Gradient Descent(10/49): loss=0.45122149226463215\n",
      "Gradient Descent(11/49): loss=0.4512214573754913\n",
      "Gradient Descent(12/49): loss=0.4512214486532063\n",
      "Gradient Descent(13/49): loss=0.4512214464726348\n",
      "Gradient Descent(14/49): loss=0.45122144592749236\n",
      "Gradient Descent(15/49): loss=0.4512214457912064\n",
      "Gradient Descent(16/49): loss=0.4512214457571349\n",
      "Gradient Descent(17/49): loss=0.4512214457486171\n",
      "Gradient Descent(18/49): loss=0.45122144574648737\n",
      "Gradient Descent(19/49): loss=0.45122144574595513\n",
      "Gradient Descent(20/49): loss=0.4512214457458222\n",
      "Gradient Descent(21/49): loss=0.45122144574578893\n",
      "Gradient Descent(22/49): loss=0.45122144574578044\n",
      "Gradient Descent(23/49): loss=0.4512214457457783\n",
      "Gradient Descent(24/49): loss=0.45122144574577794\n",
      "Gradient Descent(25/49): loss=0.45122144574577755\n",
      "Gradient Descent(26/49): loss=0.4512214457457777\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.4512214457457779\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.45122144574577766\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457777\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457777\n",
      "Gradient Descent(35/49): loss=0.4512214457457778\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457777\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.45122144574577794\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457779\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46249836334933325\n",
      "Gradient Descent(2/49): loss=0.453122954186667\n",
      "Gradient Descent(3/49): loss=0.4507791018960002\n",
      "Gradient Descent(4/49): loss=0.45019313882333334\n",
      "Gradient Descent(5/49): loss=0.45004664805516653\n",
      "Gradient Descent(6/49): loss=0.45001002536312473\n",
      "Gradient Descent(7/49): loss=0.45000086969011466\n",
      "Gradient Descent(8/49): loss=0.44999858077186217\n",
      "Gradient Descent(9/49): loss=0.4499980085422989\n",
      "Gradient Descent(10/49): loss=0.4499978654849079\n",
      "Gradient Descent(11/49): loss=0.4499978297205603\n",
      "Gradient Descent(12/49): loss=0.44999782077947337\n",
      "Gradient Descent(13/49): loss=0.44999781854420157\n",
      "Gradient Descent(14/49): loss=0.4499978179853838\n",
      "Gradient Descent(15/49): loss=0.44999781784567944\n",
      "Gradient Descent(16/49): loss=0.4499978178107532\n",
      "Gradient Descent(17/49): loss=0.4499978178020217\n",
      "Gradient Descent(18/49): loss=0.44999781779983855\n",
      "Gradient Descent(19/49): loss=0.4499978177992931\n",
      "Gradient Descent(20/49): loss=0.4499978177991567\n",
      "Gradient Descent(21/49): loss=0.4499978177991225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=0.449997817799114\n",
      "Gradient Descent(23/49): loss=0.44999781779911185\n",
      "Gradient Descent(24/49): loss=0.4499978177991113\n",
      "Gradient Descent(25/49): loss=0.4499978177991112\n",
      "Gradient Descent(26/49): loss=0.44999781779911113\n",
      "Gradient Descent(27/49): loss=0.4499978177991113\n",
      "Gradient Descent(28/49): loss=0.4499978177991113\n",
      "Gradient Descent(29/49): loss=0.4499978177991112\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991112\n",
      "Gradient Descent(32/49): loss=0.4499978177991111\n",
      "Gradient Descent(33/49): loss=0.4499978177991113\n",
      "Gradient Descent(34/49): loss=0.4499978177991112\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991109\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.4499978177991113\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.4499978177991112\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.4499978177991112\n",
      "Gradient Descent(44/49): loss=0.4499978177991112\n",
      "Gradient Descent(45/49): loss=0.4499978177991113\n",
      "Gradient Descent(46/49): loss=0.4499978177991112\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46293224000000016\n",
      "Gradient Descent(2/49): loss=0.4536652999999995\n",
      "Gradient Descent(3/49): loss=0.4513485650000001\n",
      "Gradient Descent(4/49): loss=0.45076938124999993\n",
      "Gradient Descent(5/49): loss=0.4506245853124998\n",
      "Gradient Descent(6/49): loss=0.45058838632812503\n",
      "Gradient Descent(7/49): loss=0.4505793365820314\n",
      "Gradient Descent(8/49): loss=0.45057707414550774\n",
      "Gradient Descent(9/49): loss=0.4505765085363768\n",
      "Gradient Descent(10/49): loss=0.4505763671340944\n",
      "Gradient Descent(11/49): loss=0.45057633178352363\n",
      "Gradient Descent(12/49): loss=0.45057632294588085\n",
      "Gradient Descent(13/49): loss=0.45057632073647025\n",
      "Gradient Descent(14/49): loss=0.45057632018411764\n",
      "Gradient Descent(15/49): loss=0.45057632004602943\n",
      "Gradient Descent(16/49): loss=0.4505763200115073\n",
      "Gradient Descent(17/49): loss=0.4505763200028766\n",
      "Gradient Descent(18/49): loss=0.45057632000071896\n",
      "Gradient Descent(19/49): loss=0.4505763200001797\n",
      "Gradient Descent(20/49): loss=0.4505763200000451\n",
      "Gradient Descent(21/49): loss=0.4505763200000112\n",
      "Gradient Descent(22/49): loss=0.45057632000000286\n",
      "Gradient Descent(23/49): loss=0.4505763200000007\n",
      "Gradient Descent(24/49): loss=0.4505763200000002\n",
      "Gradient Descent(25/49): loss=0.4505763199999999\n",
      "Gradient Descent(26/49): loss=0.4505763200000001\n",
      "Gradient Descent(27/49): loss=0.4505763199999999\n",
      "Gradient Descent(28/49): loss=0.4505763200000001\n",
      "Gradient Descent(29/49): loss=0.4505763200000001\n",
      "Gradient Descent(30/49): loss=0.4505763199999999\n",
      "Gradient Descent(31/49): loss=0.4505763200000001\n",
      "Gradient Descent(32/49): loss=0.4505763200000002\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057631999999975\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.45057632000000014\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.4505763200000001\n",
      "Gradient Descent(41/49): loss=0.4505763200000001\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763200000002\n",
      "Gradient Descent(46/49): loss=0.4505763200000002\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763199999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4626298016000002\n",
      "Gradient Descent(2/49): loss=0.45328725200000025\n",
      "Gradient Descent(3/49): loss=0.45095161459999983\n",
      "Gradient Descent(4/49): loss=0.45036770525\n",
      "Gradient Descent(5/49): loss=0.4502217279125001\n",
      "Gradient Descent(6/49): loss=0.45018523357812507\n",
      "Gradient Descent(7/49): loss=0.4501761099945312\n",
      "Gradient Descent(8/49): loss=0.45017382909863285\n",
      "Gradient Descent(9/49): loss=0.45017325887465837\n",
      "Gradient Descent(10/49): loss=0.4501731163186646\n",
      "Gradient Descent(11/49): loss=0.4501730806796661\n",
      "Gradient Descent(12/49): loss=0.45017307176991656\n",
      "Gradient Descent(13/49): loss=0.45017306954247893\n",
      "Gradient Descent(14/49): loss=0.45017306898561976\n",
      "Gradient Descent(15/49): loss=0.4501730688464048\n",
      "Gradient Descent(16/49): loss=0.45017306881160135\n",
      "Gradient Descent(17/49): loss=0.45017306880290037\n",
      "Gradient Descent(18/49): loss=0.45017306880072505\n",
      "Gradient Descent(19/49): loss=0.4501730688001813\n",
      "Gradient Descent(20/49): loss=0.45017306880004515\n",
      "Gradient Descent(21/49): loss=0.45017306880001134\n",
      "Gradient Descent(22/49): loss=0.4501730688000029\n",
      "Gradient Descent(23/49): loss=0.45017306880000063\n",
      "Gradient Descent(24/49): loss=0.4501730688000003\n",
      "Gradient Descent(25/49): loss=0.4501730688\n",
      "Gradient Descent(26/49): loss=0.45017306879999985\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.45017306879999985\n",
      "Gradient Descent(29/49): loss=0.4501730688000001\n",
      "Gradient Descent(30/49): loss=0.4501730688000002\n",
      "Gradient Descent(31/49): loss=0.4501730688000001\n",
      "Gradient Descent(32/49): loss=0.4501730687999998\n",
      "Gradient Descent(33/49): loss=0.45017306879999985\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730688\n",
      "Gradient Descent(39/49): loss=0.4501730688\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688000002\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4649233416357888\n",
      "Gradient Descent(2/49): loss=0.4550703083012826\n",
      "Gradient Descent(3/49): loss=0.45230259123761907\n",
      "Gradient Descent(4/49): loss=0.45152513951443585\n",
      "Gradient Descent(5/49): loss=0.45130675332539394\n",
      "Gradient Descent(6/49): loss=0.451245408644892\n",
      "Gradient Descent(7/49): loss=0.45122817692413897\n",
      "Gradient Descent(8/49): loss=0.45122333653377933\n",
      "Gradient Descent(9/49): loss=0.4512219768681273\n",
      "Gradient Descent(10/49): loss=0.4512215949380456\n",
      "Gradient Descent(11/49): loss=0.45122148765388587\n",
      "Gradient Descent(12/49): loss=0.4512214575177654\n",
      "Gradient Descent(13/49): loss=0.4512214490525289\n",
      "Gradient Descent(14/49): loss=0.45122144667464414\n",
      "Gradient Descent(15/49): loss=0.45122144600669634\n",
      "Gradient Descent(16/49): loss=0.4512214458190698\n",
      "Gradient Descent(17/49): loss=0.4512214457663658\n",
      "Gradient Descent(18/49): loss=0.45122144575156076\n",
      "Gradient Descent(19/49): loss=0.45122144574740225\n",
      "Gradient Descent(20/49): loss=0.451221445746234\n",
      "Gradient Descent(21/49): loss=0.45122144574590617\n",
      "Gradient Descent(22/49): loss=0.45122144574581374\n",
      "Gradient Descent(23/49): loss=0.4512214457457879\n",
      "Gradient Descent(24/49): loss=0.45122144574578077\n",
      "Gradient Descent(25/49): loss=0.4512214457457786\n",
      "Gradient Descent(26/49): loss=0.45122144574577805\n",
      "Gradient Descent(27/49): loss=0.4512214457457778\n",
      "Gradient Descent(28/49): loss=0.45122144574577766\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457777\n",
      "Gradient Descent(32/49): loss=0.45122144574577805\n",
      "Gradient Descent(33/49): loss=0.4512214457457778\n",
      "Gradient Descent(34/49): loss=0.45122144574577766\n",
      "Gradient Descent(35/49): loss=0.4512214457457777\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.45122144574577755\n",
      "Gradient Descent(38/49): loss=0.45122144574577766\n",
      "Gradient Descent(39/49): loss=0.45122144574577794\n",
      "Gradient Descent(40/49): loss=0.4512214457457777\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.45122144574577766\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4640434307793405\n",
      "Gradient Descent(2/49): loss=0.45394323048525764\n",
      "Gradient Descent(3/49): loss=0.45110608422264936\n",
      "Gradient Descent(4/49): loss=0.4503091298374831\n",
      "Gradient Descent(5/49): loss=0.4500852653506897\n",
      "Gradient Descent(6/49): loss=0.4500223818163495\n",
      "Gradient Descent(7/49): loss=0.4500047178315533\n",
      "Gradient Descent(8/49): loss=0.4499997560182241\n",
      "Gradient Descent(9/49): loss=0.44999836224485995\n",
      "Gradient Descent(10/49): loss=0.44999797073392195\n",
      "Gradient Descent(11/49): loss=0.44999786075849946\n",
      "Gradient Descent(12/49): loss=0.4499978298664032\n",
      "Gradient Descent(13/49): loss=0.4499978211888135\n",
      "Gradient Descent(14/49): loss=0.4499978187512786\n",
      "Gradient Descent(15/49): loss=0.449997818066575\n",
      "Gradient Descent(16/49): loss=0.44999781787424176\n",
      "Gradient Descent(17/49): loss=0.4499978178202153\n",
      "Gradient Descent(18/49): loss=0.4499978178050392\n",
      "Gradient Descent(19/49): loss=0.44999781780077647\n",
      "Gradient Descent(20/49): loss=0.4499978177995788\n",
      "Gradient Descent(21/49): loss=0.4499978177992425\n",
      "Gradient Descent(22/49): loss=0.4499978177991478\n",
      "Gradient Descent(23/49): loss=0.44999781779912135\n",
      "Gradient Descent(24/49): loss=0.4499978177991139\n",
      "Gradient Descent(25/49): loss=0.4499978177991121\n",
      "Gradient Descent(26/49): loss=0.44999781779911135\n",
      "Gradient Descent(27/49): loss=0.44999781779911113\n",
      "Gradient Descent(28/49): loss=0.44999781779911113\n",
      "Gradient Descent(29/49): loss=0.4499978177991111\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.44999781779911097\n",
      "Gradient Descent(32/49): loss=0.4499978177991113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.4499978177991109\n",
      "Gradient Descent(36/49): loss=0.4499978177991111\n",
      "Gradient Descent(37/49): loss=0.44999781779911097\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.44999781779911113\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991109\n",
      "Gradient Descent(42/49): loss=0.44999781779911097\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.44999781779911097\n",
      "Gradient Descent(45/49): loss=0.44999781779911113\n",
      "Gradient Descent(46/49): loss=0.4499978177991112\n",
      "Gradient Descent(47/49): loss=0.4499978177991112\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.4499978177991112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.464459431712\n",
      "Gradient Descent(2/49): loss=0.4544760860799007\n",
      "Gradient Descent(3/49): loss=0.4516717642918442\n",
      "Gradient Descent(4/49): loss=0.450884030301579\n",
      "Gradient Descent(5/49): loss=0.45066275582371335\n",
      "Gradient Descent(6/49): loss=0.45060059982288114\n",
      "Gradient Descent(7/49): loss=0.4505831402022473\n",
      "Gradient Descent(8/49): loss=0.45057823579481127\n",
      "Gradient Descent(9/49): loss=0.4505768581467623\n",
      "Gradient Descent(10/49): loss=0.4505764711654257\n",
      "Gradient Descent(11/49): loss=0.45057636246236804\n",
      "Gradient Descent(12/49): loss=0.4505763319276793\n",
      "Gradient Descent(13/49): loss=0.45057632335048503\n",
      "Gradient Descent(14/49): loss=0.4505763209411512\n",
      "Gradient Descent(15/49): loss=0.4505763202643694\n",
      "Gradient Descent(16/49): loss=0.4505763200742613\n",
      "Gradient Descent(17/49): loss=0.4505763200208599\n",
      "Gradient Descent(18/49): loss=0.4505763200058594\n",
      "Gradient Descent(19/49): loss=0.4505763200016459\n",
      "Gradient Descent(20/49): loss=0.4505763200004624\n",
      "Gradient Descent(21/49): loss=0.4505763200001298\n",
      "Gradient Descent(22/49): loss=0.45057632000003656\n",
      "Gradient Descent(23/49): loss=0.4505763200000104\n",
      "Gradient Descent(24/49): loss=0.45057632000000286\n",
      "Gradient Descent(25/49): loss=0.4505763200000009\n",
      "Gradient Descent(26/49): loss=0.4505763200000002\n",
      "Gradient Descent(27/49): loss=0.4505763200000001\n",
      "Gradient Descent(28/49): loss=0.4505763200000002\n",
      "Gradient Descent(29/49): loss=0.45057632\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.45057632\n",
      "Gradient Descent(32/49): loss=0.45057631999999975\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.45057632\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.45057632000000014\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.4505763200000001\n",
      "Gradient Descent(41/49): loss=0.4505763200000002\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.45057632\n",
      "Gradient Descent(44/49): loss=0.45057632\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.4505763199999999\n",
      "Gradient Descent(49/49): loss=0.4505763200000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4641694537740799\n",
      "Gradient Descent(2/49): loss=0.45410465333921946\n",
      "Gradient Descent(3/49): loss=0.45127745089706695\n",
      "Gradient Descent(4/49): loss=0.45048328973106605\n",
      "Gradient Descent(5/49): loss=0.4502602098595364\n",
      "Gradient Descent(6/49): loss=0.45019754672362383\n",
      "Gradient Descent(7/49): loss=0.45017994464874583\n",
      "Gradient Descent(8/49): loss=0.4501750002259127\n",
      "Gradient Descent(9/49): loss=0.45017361133753897\n",
      "Gradient Descent(10/49): loss=0.4501732211987946\n",
      "Gradient Descent(11/49): loss=0.4501731116088215\n",
      "Gradient Descent(12/49): loss=0.4501730808249978\n",
      "Gradient Descent(13/49): loss=0.45017307217782193\n",
      "Gradient Descent(14/49): loss=0.45017306974883026\n",
      "Gradient Descent(15/49): loss=0.4501730690665264\n",
      "Gradient Descent(16/49): loss=0.4501730688748673\n",
      "Gradient Descent(17/49): loss=0.4501730688210302\n",
      "Gradient Descent(18/49): loss=0.4501730688059074\n",
      "Gradient Descent(19/49): loss=0.45017306880165947\n",
      "Gradient Descent(20/49): loss=0.45017306880046604\n",
      "Gradient Descent(21/49): loss=0.4501730688001309\n",
      "Gradient Descent(22/49): loss=0.45017306880003677\n",
      "Gradient Descent(23/49): loss=0.45017306880001023\n",
      "Gradient Descent(24/49): loss=0.4501730688000028\n",
      "Gradient Descent(25/49): loss=0.45017306880000096\n",
      "Gradient Descent(26/49): loss=0.4501730688000003\n",
      "Gradient Descent(27/49): loss=0.4501730688000001\n",
      "Gradient Descent(28/49): loss=0.4501730688\n",
      "Gradient Descent(29/49): loss=0.4501730687999999\n",
      "Gradient Descent(30/49): loss=0.4501730687999999\n",
      "Gradient Descent(31/49): loss=0.4501730687999999\n",
      "Gradient Descent(32/49): loss=0.45017306879999985\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730688\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000001\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.45017306879999985\n",
      "Gradient Descent(42/49): loss=0.45017306880000024\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000001\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688000001\n",
      "Gradient Descent(49/49): loss=0.4501730688000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46651840035990183\n",
      "Gradient Descent(2/49): loss=0.45601857071276664\n",
      "Gradient Descent(3/49): loss=0.45272582413542556\n",
      "Gradient Descent(4/49): loss=0.45169321880877134\n",
      "Gradient Descent(5/49): loss=0.4513693937783325\n",
      "Gradient Descent(6/49): loss=0.45126784224878685\n",
      "Gradient Descent(7/49): loss=0.45123599568912137\n",
      "Gradient Descent(8/49): loss=0.45122600860801027\n",
      "Gradient Descent(9/49): loss=0.45122287665937405\n",
      "Gradient Descent(10/49): loss=0.45122189448028144\n",
      "Gradient Descent(11/49): loss=0.4512215864689182\n",
      "Gradient Descent(12/49): loss=0.4512214898765546\n",
      "Gradient Descent(13/49): loss=0.4512214595851895\n",
      "Gradient Descent(14/49): loss=0.4512214500858174\n",
      "Gradient Descent(15/49): loss=0.4512214471068142\n",
      "Gradient Descent(16/49): loss=0.4512214461725989\n",
      "Gradient Descent(17/49): loss=0.45122144587962887\n",
      "Gradient Descent(18/49): loss=0.4512214457877534\n",
      "Gradient Descent(19/49): loss=0.4512214457589413\n",
      "Gradient Descent(20/49): loss=0.4512214457499058\n",
      "Gradient Descent(21/49): loss=0.4512214457470724\n",
      "Gradient Descent(22/49): loss=0.45122144574618384\n",
      "Gradient Descent(23/49): loss=0.451221445745905\n",
      "Gradient Descent(24/49): loss=0.4512214457458178\n",
      "Gradient Descent(25/49): loss=0.4512214457457904\n",
      "Gradient Descent(26/49): loss=0.45122144574578166\n",
      "Gradient Descent(27/49): loss=0.4512214457457791\n",
      "Gradient Descent(28/49): loss=0.4512214457457782\n",
      "Gradient Descent(29/49): loss=0.4512214457457778\n",
      "Gradient Descent(30/49): loss=0.4512214457457778\n",
      "Gradient Descent(31/49): loss=0.4512214457457779\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.4512214457457779\n",
      "Gradient Descent(35/49): loss=0.45122144574577766\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.45122144574577766\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457779\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457779\n",
      "Gradient Descent(46/49): loss=0.4512214457457778\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.45122144574577766\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4656785021373099\n",
      "Gradient Descent(2/49): loss=0.4549152804075701\n",
      "Gradient Descent(3/49): loss=0.45153993407312415\n",
      "Gradient Descent(4/49): loss=0.45048142546264136\n",
      "Gradient Descent(5/49): loss=0.45014947716239423\n",
      "Gradient Descent(6/49): loss=0.45004537817543666\n",
      "Gradient Descent(7/49): loss=0.45001273273312675\n",
      "Gradient Descent(8/49): loss=0.4500024951224183\n",
      "Gradient Descent(9/49): loss=0.44999928460770017\n",
      "Gradient Descent(10/49): loss=0.44999827779028473\n",
      "Gradient Descent(11/49): loss=0.44999796205234316\n",
      "Gradient Descent(12/49): loss=0.4499978630369247\n",
      "Gradient Descent(13/49): loss=0.4499978319856894\n",
      "Gradient Descent(14/49): loss=0.4499978222480221\n",
      "Gradient Descent(15/49): loss=0.4499978191942895\n",
      "Gradient Descent(16/49): loss=0.4499978182366392\n",
      "Gradient Descent(17/49): loss=0.4499978179363199\n",
      "Gradient Descent(18/49): loss=0.4499978178421397\n",
      "Gradient Descent(19/49): loss=0.44999781781260484\n",
      "Gradient Descent(20/49): loss=0.4499978178033427\n",
      "Gradient Descent(21/49): loss=0.4499978178004382\n",
      "Gradient Descent(22/49): loss=0.4499978177995272\n",
      "Gradient Descent(23/49): loss=0.44999781779924153\n",
      "Gradient Descent(24/49): loss=0.4499978177991522\n",
      "Gradient Descent(25/49): loss=0.44999781779912396\n",
      "Gradient Descent(26/49): loss=0.44999781779911524\n",
      "Gradient Descent(27/49): loss=0.44999781779911247\n",
      "Gradient Descent(28/49): loss=0.4499978177991115\n",
      "Gradient Descent(29/49): loss=0.44999781779911113\n",
      "Gradient Descent(30/49): loss=0.44999781779911113\n",
      "Gradient Descent(31/49): loss=0.4499978177991111\n",
      "Gradient Descent(32/49): loss=0.44999781779911097\n",
      "Gradient Descent(33/49): loss=0.4499978177991111\n",
      "Gradient Descent(34/49): loss=0.44999781779911097\n",
      "Gradient Descent(35/49): loss=0.44999781779911113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.4499978177991111\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.4499978177991112\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.44999781779911097\n",
      "Gradient Descent(46/49): loss=0.44999781779911097\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.44999781779911097\n",
      "Gradient Descent(49/49): loss=0.44999781779911097\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4660755860479999\n",
      "Gradient Descent(2/49): loss=0.45543688983265285\n",
      "Gradient Descent(3/49): loss=0.45210059469951963\n",
      "Gradient Descent(4/49): loss=0.4510543325457693\n",
      "Gradient Descent(5/49): loss=0.4507262247343533\n",
      "Gradient Descent(6/49): loss=0.45062333012469324\n",
      "Gradient Descent(7/49): loss=0.45059106237510366\n",
      "Gradient Descent(8/49): loss=0.45058094320883246\n",
      "Gradient Descent(9/49): loss=0.45057776983828984\n",
      "Gradient Descent(10/49): loss=0.45057677466928775\n",
      "Gradient Descent(11/49): loss=0.45057646258428874\n",
      "Gradient Descent(12/49): loss=0.4505763647144328\n",
      "Gradient Descent(13/49): loss=0.4505763340224461\n",
      "Gradient Descent(14/49): loss=0.4505763243974392\n",
      "Gradient Descent(15/49): loss=0.450576321379037\n",
      "Gradient Descent(16/49): loss=0.4505763204324657\n",
      "Gradient Descent(17/49): loss=0.45057632013562143\n",
      "Gradient Descent(18/49): loss=0.4505763200425308\n",
      "Gradient Descent(19/49): loss=0.45057632001333775\n",
      "Gradient Descent(20/49): loss=0.45057632000418296\n",
      "Gradient Descent(21/49): loss=0.4505763200013116\n",
      "Gradient Descent(22/49): loss=0.4505763200004113\n",
      "Gradient Descent(23/49): loss=0.450576320000129\n",
      "Gradient Descent(24/49): loss=0.45057632000004033\n",
      "Gradient Descent(25/49): loss=0.4505763200000126\n",
      "Gradient Descent(26/49): loss=0.450576320000004\n",
      "Gradient Descent(27/49): loss=0.45057632000000136\n",
      "Gradient Descent(28/49): loss=0.45057632000000036\n",
      "Gradient Descent(29/49): loss=0.45057632000000014\n",
      "Gradient Descent(30/49): loss=0.45057632\n",
      "Gradient Descent(31/49): loss=0.4505763199999999\n",
      "Gradient Descent(32/49): loss=0.4505763199999998\n",
      "Gradient Descent(33/49): loss=0.45057632\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763199999999\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.4505763199999999\n",
      "Gradient Descent(38/49): loss=0.45057632000000014\n",
      "Gradient Descent(39/49): loss=0.4505763199999999\n",
      "Gradient Descent(40/49): loss=0.45057632\n",
      "Gradient Descent(41/49): loss=0.45057632\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763199999999\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.4505763199999998\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.45057632000000014\n",
      "Gradient Descent(48/49): loss=0.4505763199999998\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46579879442432004\n",
      "Gradient Descent(2/49): loss=0.45507329635578714\n",
      "Gradient Descent(3/49): loss=0.45170978016149455\n",
      "Gradient Descent(4/49): loss=0.4506549814829646\n",
      "Gradient Descent(5/49): loss=0.45032419661737777\n",
      "Gradient Descent(6/49): loss=0.4502204624835297\n",
      "Gradient Descent(7/49): loss=0.4501879314591549\n",
      "Gradient Descent(8/49): loss=0.45017772972991094\n",
      "Gradient Descent(9/49): loss=0.4501745304676202\n",
      "Gradient Descent(10/49): loss=0.4501735271789656\n",
      "Gradient Descent(11/49): loss=0.4501732125476436\n",
      "Gradient Descent(12/49): loss=0.45017311387926107\n",
      "Gradient Descent(13/49): loss=0.4501730829368562\n",
      "Gradient Descent(14/49): loss=0.4501730732333181\n",
      "Gradient Descent(15/49): loss=0.4501730701902885\n",
      "Gradient Descent(16/49): loss=0.4501730692359947\n",
      "Gradient Descent(17/49): loss=0.45017306893672804\n",
      "Gradient Descent(18/49): loss=0.4501730688428779\n",
      "Gradient Descent(19/49): loss=0.45017306881344654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=0.4501730688042168\n",
      "Gradient Descent(21/49): loss=0.45017306880132235\n",
      "Gradient Descent(22/49): loss=0.4501730688004147\n",
      "Gradient Descent(23/49): loss=0.45017306880013014\n",
      "Gradient Descent(24/49): loss=0.45017306880004065\n",
      "Gradient Descent(25/49): loss=0.45017306880001273\n",
      "Gradient Descent(26/49): loss=0.450173068800004\n",
      "Gradient Descent(27/49): loss=0.45017306880000135\n",
      "Gradient Descent(28/49): loss=0.45017306880000046\n",
      "Gradient Descent(29/49): loss=0.45017306880000024\n",
      "Gradient Descent(30/49): loss=0.4501730688\n",
      "Gradient Descent(31/49): loss=0.4501730687999999\n",
      "Gradient Descent(32/49): loss=0.4501730688\n",
      "Gradient Descent(33/49): loss=0.4501730688000001\n",
      "Gradient Descent(34/49): loss=0.4501730688000001\n",
      "Gradient Descent(35/49): loss=0.4501730687999999\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730688000001\n",
      "Gradient Descent(38/49): loss=0.4501730688000002\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688000001\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730687999999\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.4501730687999999\n",
      "Gradient Descent(46/49): loss=0.4501730687999999\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46820126048167254\n",
      "Gradient Descent(2/49): loss=0.45713211925534347\n",
      "Gradient Descent(3/49): loss=0.45327895119445777\n",
      "Gradient Descent(4/49): loss=0.45193766339246316\n",
      "Gradient Descent(5/49): loss=0.4514707611085889\n",
      "Gradient Descent(6/49): loss=0.45130823242357243\n",
      "Gradient Descent(7/49): loss=0.45125165618831814\n",
      "Gradient Descent(8/49): loss=0.451231962000826\n",
      "Gradient Descent(9/49): loss=0.45122510645416014\n",
      "Gradient Descent(10/49): loss=0.45122272003836555\n",
      "Gradient Descent(11/49): loss=0.45122188932702767\n",
      "Gradient Descent(12/49): loss=0.45122160015641083\n",
      "Gradient Descent(13/49): loss=0.4512214994961191\n",
      "Gradient Descent(14/49): loss=0.4512214644562717\n",
      "Gradient Descent(15/49): loss=0.4512214522589008\n",
      "Gradient Descent(16/49): loss=0.4512214480129959\n",
      "Gradient Descent(17/49): loss=0.4512214465349963\n",
      "Gradient Descent(18/49): loss=0.4512214460205047\n",
      "Gradient Descent(19/49): loss=0.4512214458414103\n",
      "Gradient Descent(20/49): loss=0.45122144577906736\n",
      "Gradient Descent(21/49): loss=0.451221445757366\n",
      "Gradient Descent(22/49): loss=0.45122144574981143\n",
      "Gradient Descent(23/49): loss=0.45122144574718176\n",
      "Gradient Descent(24/49): loss=0.4512214457462666\n",
      "Gradient Descent(25/49): loss=0.4512214457459481\n",
      "Gradient Descent(26/49): loss=0.45122144574583695\n",
      "Gradient Descent(27/49): loss=0.45122144574579837\n",
      "Gradient Descent(28/49): loss=0.45122144574578504\n",
      "Gradient Descent(29/49): loss=0.45122144574578027\n",
      "Gradient Descent(30/49): loss=0.45122144574577866\n",
      "Gradient Descent(31/49): loss=0.4512214457457781\n",
      "Gradient Descent(32/49): loss=0.4512214457457778\n",
      "Gradient Descent(33/49): loss=0.4512214457457777\n",
      "Gradient Descent(34/49): loss=0.45122144574577794\n",
      "Gradient Descent(35/49): loss=0.4512214457457779\n",
      "Gradient Descent(36/49): loss=0.4512214457457779\n",
      "Gradient Descent(37/49): loss=0.4512214457457779\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.4512214457457779\n",
      "Gradient Descent(41/49): loss=0.4512214457457779\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457778\n",
      "Gradient Descent(45/49): loss=0.4512214457457777\n",
      "Gradient Descent(46/49): loss=0.4512214457457778\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.4512214457457778\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46740357742324035\n",
      "Gradient Descent(2/49): loss=0.4560567627242714\n",
      "Gradient Descent(3/49): loss=0.4521069365275591\n",
      "Gradient Descent(4/49): loss=0.4507320020284839\n",
      "Gradient Descent(5/49): loss=0.4502533873293558\n",
      "Gradient Descent(6/49): loss=0.4500867815525894\n",
      "Gradient Descent(7/49): loss=0.4500287860816967\n",
      "Gradient Descent(8/49): loss=0.45000859785827924\n",
      "Gradient Descent(9/49): loss=0.45000157033770755\n",
      "Gradient Descent(10/49): loss=0.44999912405779646\n",
      "Gradient Descent(11/49): loss=0.4499982725077594\n",
      "Gradient Descent(12/49): loss=0.44999797608319164\n",
      "Gradient Descent(13/49): loss=0.4499978728977994\n",
      "Gradient Descent(14/49): loss=0.44999783697896467\n",
      "Gradient Descent(15/49): loss=0.44999782447561826\n",
      "Gradient Descent(16/49): loss=0.4499978201232033\n",
      "Gradient Descent(17/49): loss=0.4499978186081276\n",
      "Gradient Descent(18/49): loss=0.44999781808072975\n",
      "Gradient Descent(19/49): loss=0.44999781789714255\n",
      "Gradient Descent(20/49): loss=0.4499978178332359\n",
      "Gradient Descent(21/49): loss=0.44999781781098985\n",
      "Gradient Descent(22/49): loss=0.4499978178032461\n",
      "Gradient Descent(23/49): loss=0.4499978178005504\n",
      "Gradient Descent(24/49): loss=0.4499978177996122\n",
      "Gradient Descent(25/49): loss=0.4499978177992855\n",
      "Gradient Descent(26/49): loss=0.4499978177991719\n",
      "Gradient Descent(27/49): loss=0.44999781779913234\n",
      "Gradient Descent(28/49): loss=0.4499978177991185\n",
      "Gradient Descent(29/49): loss=0.4499978177991137\n",
      "Gradient Descent(30/49): loss=0.44999781779911197\n",
      "Gradient Descent(31/49): loss=0.4499978177991115\n",
      "Gradient Descent(32/49): loss=0.44999781779911113\n",
      "Gradient Descent(33/49): loss=0.44999781779911113\n",
      "Gradient Descent(34/49): loss=0.44999781779911113\n",
      "Gradient Descent(35/49): loss=0.4499978177991112\n",
      "Gradient Descent(36/49): loss=0.4499978177991109\n",
      "Gradient Descent(37/49): loss=0.4499978177991111\n",
      "Gradient Descent(38/49): loss=0.44999781779911113\n",
      "Gradient Descent(39/49): loss=0.4499978177991112\n",
      "Gradient Descent(40/49): loss=0.4499978177991108\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.4499978177991111\n",
      "Gradient Descent(43/49): loss=0.44999781779911113\n",
      "Gradient Descent(44/49): loss=0.4499978177991113\n",
      "Gradient Descent(45/49): loss=0.4499978177991112\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.4499978177991109\n",
      "Gradient Descent(48/49): loss=0.44999781779911113\n",
      "Gradient Descent(49/49): loss=0.4499978177991112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46778070300799995\n",
      "Gradient Descent(2/49): loss=0.4565651657250852\n",
      "Gradient Descent(3/49): loss=0.4526610371969019\n",
      "Gradient Descent(4/49): loss=0.4513020100562417\n",
      "Gradient Descent(5/49): loss=0.45082893270857766\n",
      "Gradient Descent(6/49): loss=0.45066425448385616\n",
      "Gradient Descent(7/49): loss=0.4506069299938302\n",
      "Gradient Descent(8/49): loss=0.45058697533885217\n",
      "Gradient Descent(9/49): loss=0.4505800291234545\n",
      "Gradient Descent(10/49): loss=0.45057761114587447\n",
      "Gradient Descent(11/49): loss=0.45057676944787894\n",
      "Gradient Descent(12/49): loss=0.45057647645280674\n",
      "Gradient Descent(13/49): loss=0.45057637446122195\n",
      "Gradient Descent(14/49): loss=0.4505763389579514\n",
      "Gradient Descent(15/49): loss=0.45057632659926294\n",
      "Gradient Descent(16/49): loss=0.4505763222972033\n",
      "Gradient Descent(17/49): loss=0.45057632079965637\n",
      "Gradient Descent(18/49): loss=0.4505763202783604\n",
      "Gradient Descent(19/49): loss=0.45057632009689735\n",
      "Gradient Descent(20/49): loss=0.45057632003372994\n",
      "Gradient Descent(21/49): loss=0.4505763200117413\n",
      "Gradient Descent(22/49): loss=0.4505763200040871\n",
      "Gradient Descent(23/49): loss=0.45057632000142267\n",
      "Gradient Descent(24/49): loss=0.45057632000049513\n",
      "Gradient Descent(25/49): loss=0.4505763200001725\n",
      "Gradient Descent(26/49): loss=0.45057632000006\n",
      "Gradient Descent(27/49): loss=0.45057632000002096\n",
      "Gradient Descent(28/49): loss=0.4505763200000072\n",
      "Gradient Descent(29/49): loss=0.45057632000000264\n",
      "Gradient Descent(30/49): loss=0.45057632000000075\n",
      "Gradient Descent(31/49): loss=0.45057632000000053\n",
      "Gradient Descent(32/49): loss=0.45057632000000014\n",
      "Gradient Descent(33/49): loss=0.4505763200000001\n",
      "Gradient Descent(34/49): loss=0.4505763199999999\n",
      "Gradient Descent(35/49): loss=0.4505763200000001\n",
      "Gradient Descent(36/49): loss=0.45057632\n",
      "Gradient Descent(37/49): loss=0.45057632\n",
      "Gradient Descent(38/49): loss=0.4505763199999999\n",
      "Gradient Descent(39/49): loss=0.45057632\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.45057632\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763199999998\n",
      "Gradient Descent(44/49): loss=0.4505763199999999\n",
      "Gradient Descent(45/49): loss=0.45057632000000014\n",
      "Gradient Descent(46/49): loss=0.4505763200000001\n",
      "Gradient Descent(47/49): loss=0.45057632\n",
      "Gradient Descent(48/49): loss=0.45057632000000014\n",
      "Gradient Descent(49/49): loss=0.45057632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46751782355071986\n",
      "Gradient Descent(2/49): loss=0.45621077792872583\n",
      "Gradient Descent(3/49): loss=0.45227479534770954\n",
      "Gradient Descent(4/49): loss=0.4509046798112577\n",
      "Gradient Descent(5/49): loss=0.4504277425930188\n",
      "Gradient Descent(6/49): loss=0.4502617207473501\n",
      "Gradient Descent(7/49): loss=0.4502039285428725\n",
      "Gradient Descent(8/49): loss=0.45018381107649386\n",
      "Gradient Descent(9/49): loss=0.45017680818644784\n",
      "Gradient Descent(10/49): loss=0.4501743704804224\n",
      "Gradient Descent(11/49): loss=0.450173521914955\n",
      "Gradient Descent(12/49): loss=0.45017322652931613\n",
      "Gradient Descent(13/49): loss=0.4501731237055748\n",
      "Gradient Descent(14/49): loss=0.45017308791263067\n",
      "Gradient Descent(15/49): loss=0.45017307545310653\n",
      "Gradient Descent(16/49): loss=0.45017307111594657\n",
      "Gradient Descent(17/49): loss=0.450173069606181\n",
      "Gradient Descent(18/49): loss=0.4501730690806317\n",
      "Gradient Descent(19/49): loss=0.45017306889768793\n",
      "Gradient Descent(20/49): loss=0.45017306883400515\n",
      "Gradient Descent(21/49): loss=0.45017306881183705\n",
      "Gradient Descent(22/49): loss=0.45017306880412067\n",
      "Gradient Descent(23/49): loss=0.4501730688014344\n",
      "Gradient Descent(24/49): loss=0.4501730688004992\n",
      "Gradient Descent(25/49): loss=0.45017306880017377\n",
      "Gradient Descent(26/49): loss=0.4501730688000604\n",
      "Gradient Descent(27/49): loss=0.45017306880002117\n",
      "Gradient Descent(28/49): loss=0.45017306880000724\n",
      "Gradient Descent(29/49): loss=0.45017306880000235\n",
      "Gradient Descent(30/49): loss=0.45017306880000096\n",
      "Gradient Descent(31/49): loss=0.45017306880000024\n",
      "Gradient Descent(32/49): loss=0.4501730688000002\n",
      "Gradient Descent(33/49): loss=0.4501730687999999\n",
      "Gradient Descent(34/49): loss=0.45017306879999985\n",
      "Gradient Descent(35/49): loss=0.4501730688\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.45017306879999985\n",
      "Gradient Descent(38/49): loss=0.4501730687999999\n",
      "Gradient Descent(39/49): loss=0.45017306880000024\n",
      "Gradient Descent(40/49): loss=0.4501730687999999\n",
      "Gradient Descent(41/49): loss=0.4501730687999999\n",
      "Gradient Descent(42/49): loss=0.4501730688000002\n",
      "Gradient Descent(43/49): loss=0.4501730688\n",
      "Gradient Descent(44/49): loss=0.4501730688000002\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.45017306879999985\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730687999998\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4699719220011008\n",
      "Gradient Descent(2/49): loss=0.4584291288183234\n",
      "Gradient Descent(3/49): loss=0.45399207911886413\n",
      "Gradient Descent(4/49): loss=0.452286477214392\n",
      "Gradient Descent(5/49): loss=0.45163084384231306\n",
      "Gradient Descent(6/49): loss=0.45137881837408594\n",
      "Gradient Descent(7/49): loss=0.45128193978409936\n",
      "Gradient Descent(8/49): loss=0.4512446996541085\n",
      "Gradient Descent(9/49): loss=0.4512303845481401\n",
      "Gradient Descent(10/49): loss=0.4512248818214061\n",
      "Gradient Descent(11/49): loss=0.451222766573249\n",
      "Gradient Descent(12/49): loss=0.4512219534718577\n",
      "Gradient Descent(13/49): loss=0.45122164091568295\n",
      "Gradient Descent(14/49): loss=0.4512215207690894\n",
      "Gradient Descent(15/49): loss=0.4512214745847389\n",
      "Gradient Descent(16/49): loss=0.45122145683147447\n",
      "Gradient Descent(17/49): loss=0.45122145000711966\n",
      "Gradient Descent(18/49): loss=0.4512214473838375\n",
      "Gradient Descent(19/49): loss=0.4512214463754479\n",
      "Gradient Descent(20/49): loss=0.4512214459878229\n",
      "Gradient Descent(21/49): loss=0.45122144583882\n",
      "Gradient Descent(22/49): loss=0.4512214457815432\n",
      "Gradient Descent(23/49): loss=0.451221445759526\n",
      "Gradient Descent(24/49): loss=0.4512214457510626\n",
      "Gradient Descent(25/49): loss=0.4512214457478094\n",
      "Gradient Descent(26/49): loss=0.4512214457465587\n",
      "Gradient Descent(27/49): loss=0.4512214457460781\n",
      "Gradient Descent(28/49): loss=0.4512214457458931\n",
      "Gradient Descent(29/49): loss=0.45122144574582207\n",
      "Gradient Descent(30/49): loss=0.451221445745795\n",
      "Gradient Descent(31/49): loss=0.45122144574578427\n",
      "Gradient Descent(32/49): loss=0.45122144574578027\n",
      "Gradient Descent(33/49): loss=0.45122144574577866\n",
      "Gradient Descent(34/49): loss=0.45122144574577827\n",
      "Gradient Descent(35/49): loss=0.45122144574577805\n",
      "Gradient Descent(36/49): loss=0.4512214457457778\n",
      "Gradient Descent(37/49): loss=0.4512214457457777\n",
      "Gradient Descent(38/49): loss=0.4512214457457778\n",
      "Gradient Descent(39/49): loss=0.4512214457457778\n",
      "Gradient Descent(40/49): loss=0.45122144574577766\n",
      "Gradient Descent(41/49): loss=0.4512214457457778\n",
      "Gradient Descent(42/49): loss=0.4512214457457777\n",
      "Gradient Descent(43/49): loss=0.4512214457457778\n",
      "Gradient Descent(44/49): loss=0.4512214457457779\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457778\n",
      "Gradient Descent(47/49): loss=0.4512214457457779\n",
      "Gradient Descent(48/49): loss=0.45122144574577794\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46921865663713286\n",
      "Gradient Descent(2/49): loss=0.457386308248446\n",
      "Gradient Descent(3/49): loss=0.45283795352783557\n",
      "Gradient Descent(4/49): loss=0.45108956597323296\n",
      "Gradient Descent(5/49): loss=0.45041748579724333\n",
      "Gradient Descent(6/49): loss=0.4501591381775932\n",
      "Gradient Descent(7/49): loss=0.4500598293525998\n",
      "Gradient Descent(8/49): loss=0.4500216550402719\n",
      "Gradient Descent(9/49): loss=0.4500069808346133\n",
      "Gradient Descent(10/49): loss=0.45000134006995823\n",
      "Gradient Descent(11/49): loss=0.4499991717600249\n",
      "Gradient Descent(12/49): loss=0.4499983382616863\n",
      "Gradient Descent(13/49): loss=0.44999801786492494\n",
      "Gradient Descent(14/49): loss=0.4499978947044101\n",
      "Gradient Descent(15/49): loss=0.4499978473615079\n",
      "Gradient Descent(16/49): loss=0.4499978291628966\n",
      "Gradient Descent(17/49): loss=0.4499978221673503\n",
      "Gradient Descent(18/49): loss=0.4499978194782623\n",
      "Gradient Descent(19/49): loss=0.4499978184445768\n",
      "Gradient Descent(20/49): loss=0.44999781804722805\n",
      "Gradient Descent(21/49): loss=0.44999781789448723\n",
      "Gradient Descent(22/49): loss=0.44999781783577386\n",
      "Gradient Descent(23/49): loss=0.4499978178132042\n",
      "Gradient Descent(24/49): loss=0.44999781780452847\n",
      "Gradient Descent(25/49): loss=0.44999781780119363\n",
      "Gradient Descent(26/49): loss=0.4499978177999116\n",
      "Gradient Descent(27/49): loss=0.4499978177994188\n",
      "Gradient Descent(28/49): loss=0.44999781779922915\n",
      "Gradient Descent(29/49): loss=0.4499978177991566\n",
      "Gradient Descent(30/49): loss=0.4499978177991286\n",
      "Gradient Descent(31/49): loss=0.44999781779911774\n",
      "Gradient Descent(32/49): loss=0.4499978177991135\n",
      "Gradient Descent(33/49): loss=0.44999781779911213\n",
      "Gradient Descent(34/49): loss=0.4499978177991115\n",
      "Gradient Descent(35/49): loss=0.4499978177991113\n",
      "Gradient Descent(36/49): loss=0.44999781779911113\n",
      "Gradient Descent(37/49): loss=0.44999781779911113\n",
      "Gradient Descent(38/49): loss=0.44999781779911097\n",
      "Gradient Descent(39/49): loss=0.44999781779911097\n",
      "Gradient Descent(40/49): loss=0.4499978177991111\n",
      "Gradient Descent(41/49): loss=0.4499978177991111\n",
      "Gradient Descent(42/49): loss=0.4499978177991112\n",
      "Gradient Descent(43/49): loss=0.4499978177991111\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.44999781779911097\n",
      "Gradient Descent(46/49): loss=0.44999781779911097\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.4499978177991112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46957478259200014\n",
      "Gradient Descent(2/49): loss=0.457879329020365\n",
      "Gradient Descent(3/49): loss=0.45338359666742817\n",
      "Gradient Descent(4/49): loss=0.4516554371509594\n",
      "Gradient Descent(5/49): loss=0.4509911326328288\n",
      "Gradient Descent(6/49): loss=0.4507357739760596\n",
      "Gradient Descent(7/49): loss=0.45063761410839726\n",
      "Gradient Descent(8/49): loss=0.450599881455268\n",
      "Gradient Descent(9/49): loss=0.450585377023405\n",
      "Gradient Descent(10/49): loss=0.4505798015197967\n",
      "Gradient Descent(11/49): loss=0.45057765829621\n",
      "Gradient Descent(12/49): loss=0.45057683444106306\n",
      "Gradient Descent(13/49): loss=0.4505765177511449\n",
      "Gradient Descent(14/49): loss=0.45057639601554006\n",
      "Gradient Descent(15/49): loss=0.45057634922037343\n",
      "Gradient Descent(16/49): loss=0.4505763312323116\n",
      "Gradient Descent(17/49): loss=0.45057632431770067\n",
      "Gradient Descent(18/49): loss=0.45057632165972406\n",
      "Gradient Descent(19/49): loss=0.450576320637998\n",
      "Gradient Descent(20/49): loss=0.45057632024524624\n",
      "Gradient Descent(21/49): loss=0.4505763200942727\n",
      "Gradient Descent(22/49): loss=0.45057632003623854\n",
      "Gradient Descent(23/49): loss=0.4505763200139302\n",
      "Gradient Descent(24/49): loss=0.4505763200053547\n",
      "Gradient Descent(25/49): loss=0.4505763200020582\n",
      "Gradient Descent(26/49): loss=0.45057632000079123\n",
      "Gradient Descent(27/49): loss=0.4505763200003041\n",
      "Gradient Descent(28/49): loss=0.45057632000011716\n",
      "Gradient Descent(29/49): loss=0.4505763200000448\n",
      "Gradient Descent(30/49): loss=0.45057632000001735\n",
      "Gradient Descent(31/49): loss=0.45057632000000664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=0.45057632000000264\n",
      "Gradient Descent(33/49): loss=0.4505763200000011\n",
      "Gradient Descent(34/49): loss=0.4505763200000002\n",
      "Gradient Descent(35/49): loss=0.45057632\n",
      "Gradient Descent(36/49): loss=0.4505763199999999\n",
      "Gradient Descent(37/49): loss=0.4505763200000002\n",
      "Gradient Descent(38/49): loss=0.4505763200000001\n",
      "Gradient Descent(39/49): loss=0.4505763200000001\n",
      "Gradient Descent(40/49): loss=0.4505763199999999\n",
      "Gradient Descent(41/49): loss=0.4505763199999998\n",
      "Gradient Descent(42/49): loss=0.4505763199999999\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.45057632000000014\n",
      "Gradient Descent(45/49): loss=0.45057632\n",
      "Gradient Descent(46/49): loss=0.45057632\n",
      "Gradient Descent(47/49): loss=0.45057632000000014\n",
      "Gradient Descent(48/49): loss=0.45057632\n",
      "Gradient Descent(49/49): loss=0.4505763200000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46932654115328004\n",
      "Gradient Descent(2/49): loss=0.4575356635726005\n",
      "Gradient Descent(3/49): loss=0.4530032502305881\n",
      "Gradient Descent(4/49): loss=0.45126099054191826\n",
      "Gradient Descent(5/49): loss=0.45059126591759296\n",
      "Gradient Descent(6/49): loss=0.45033382377200293\n",
      "Gradient Descent(7/49): loss=0.45023486301123794\n",
      "Gradient Descent(8/49): loss=0.4501968224948\n",
      "Gradient Descent(9/49): loss=0.45018219972028134\n",
      "Gradient Descent(10/49): loss=0.450176578725756\n",
      "Gradient Descent(11/49): loss=0.4501744180154607\n",
      "Gradient Descent(12/49): loss=0.450173587438423\n",
      "Gradient Descent(13/49): loss=0.4501732681646099\n",
      "Gradient Descent(14/49): loss=0.4501731454357561\n",
      "Gradient Descent(15/49): loss=0.4501730982587848\n",
      "Gradient Descent(16/49): loss=0.4501730801239568\n",
      "Gradient Descent(17/49): loss=0.45017307315292915\n",
      "Gradient Descent(18/49): loss=0.45017307047326566\n",
      "Gradient Descent(19/49): loss=0.45017306944320346\n",
      "Gradient Descent(20/49): loss=0.4501730690472472\n",
      "Gradient Descent(21/49): loss=0.45017306889504194\n",
      "Gradient Descent(22/49): loss=0.45017306883653424\n",
      "Gradient Descent(23/49): loss=0.45017306881404395\n",
      "Gradient Descent(24/49): loss=0.45017306880539837\n",
      "Gradient Descent(25/49): loss=0.45017306880207514\n",
      "Gradient Descent(26/49): loss=0.45017306880079755\n",
      "Gradient Descent(27/49): loss=0.4501730688003068\n",
      "Gradient Descent(28/49): loss=0.45017306880011787\n",
      "Gradient Descent(29/49): loss=0.4501730688000453\n",
      "Gradient Descent(30/49): loss=0.45017306880001745\n",
      "Gradient Descent(31/49): loss=0.45017306880000674\n",
      "Gradient Descent(32/49): loss=0.45017306880000263\n",
      "Gradient Descent(33/49): loss=0.45017306880000085\n",
      "Gradient Descent(34/49): loss=0.4501730688000003\n",
      "Gradient Descent(35/49): loss=0.4501730688000002\n",
      "Gradient Descent(36/49): loss=0.4501730688\n",
      "Gradient Descent(37/49): loss=0.4501730687999999\n",
      "Gradient Descent(38/49): loss=0.4501730688000001\n",
      "Gradient Descent(39/49): loss=0.4501730687999999\n",
      "Gradient Descent(40/49): loss=0.4501730688\n",
      "Gradient Descent(41/49): loss=0.4501730688\n",
      "Gradient Descent(42/49): loss=0.4501730688\n",
      "Gradient Descent(43/49): loss=0.4501730688000001\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.4501730688\n",
      "Gradient Descent(46/49): loss=0.4501730688000001\n",
      "Gradient Descent(47/49): loss=0.4501730687999999\n",
      "Gradient Descent(48/49): loss=0.4501730687999998\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4718303849181865\n",
      "Gradient Descent(2/49): loss=0.45992872254612055\n",
      "Gradient Descent(3/49): loss=0.45490027019392243\n",
      "Gradient Descent(4/49): loss=0.4527757490751191\n",
      "Gradient Descent(5/49): loss=0.45187813890242445\n",
      "Gradient Descent(6/49): loss=0.45149889860446085\n",
      "Gradient Descent(7/49): loss=0.45133866957857166\n",
      "Gradient Descent(8/49): loss=0.451270972815133\n",
      "Gradient Descent(9/49): loss=0.45124237093258046\n",
      "Gradient Descent(10/49): loss=0.45123028663720205\n",
      "Gradient Descent(11/49): loss=0.4512251810224044\n",
      "Gradient Descent(12/49): loss=0.4512230239001525\n",
      "Gradient Descent(13/49): loss=0.45122211251600103\n",
      "Gradient Descent(14/49): loss=0.45122172745619704\n",
      "Gradient Descent(15/49): loss=0.4512215647684302\n",
      "Gradient Descent(16/49): loss=0.45122149603284833\n",
      "Gradient Descent(17/49): loss=0.451221466992065\n",
      "Gradient Descent(18/49): loss=0.4512214547223343\n",
      "Gradient Descent(19/49): loss=0.4512214495383729\n",
      "Gradient Descent(20/49): loss=0.4512214473481492\n",
      "Gradient Descent(21/49): loss=0.4512214464227797\n",
      "Gradient Descent(22/49): loss=0.45122144603181125\n",
      "Gradient Descent(23/49): loss=0.4512214458666268\n",
      "Gradient Descent(24/49): loss=0.45122144579683654\n",
      "Gradient Descent(25/49): loss=0.4512214457673501\n",
      "Gradient Descent(26/49): loss=0.4512214457548921\n",
      "Gradient Descent(27/49): loss=0.4512214457496287\n",
      "Gradient Descent(28/49): loss=0.4512214457474048\n",
      "Gradient Descent(29/49): loss=0.4512214457464651\n",
      "Gradient Descent(30/49): loss=0.4512214457460683\n",
      "Gradient Descent(31/49): loss=0.45122144574590056\n",
      "Gradient Descent(32/49): loss=0.45122144574582956\n",
      "Gradient Descent(33/49): loss=0.4512214457457996\n",
      "Gradient Descent(34/49): loss=0.45122144574578704\n",
      "Gradient Descent(35/49): loss=0.45122144574578166\n",
      "Gradient Descent(36/49): loss=0.4512214457457794\n",
      "Gradient Descent(37/49): loss=0.4512214457457786\n",
      "Gradient Descent(38/49): loss=0.45122144574577794\n",
      "Gradient Descent(39/49): loss=0.4512214457457779\n",
      "Gradient Descent(40/49): loss=0.4512214457457778\n",
      "Gradient Descent(41/49): loss=0.45122144574577766\n",
      "Gradient Descent(42/49): loss=0.4512214457457778\n",
      "Gradient Descent(43/49): loss=0.45122144574577794\n",
      "Gradient Descent(44/49): loss=0.45122144574577794\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457779\n",
      "Gradient Descent(47/49): loss=0.4512214457457778\n",
      "Gradient Descent(48/49): loss=0.4512214457457777\n",
      "Gradient Descent(49/49): loss=0.4512214457457777\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4711237397789866\n",
      "Gradient Descent(2/49): loss=0.4589235198356094\n",
      "Gradient Descent(3/49): loss=0.4537689269095321\n",
      "Gradient Descent(4/49): loss=0.45159111139826397\n",
      "Gradient Descent(5/49): loss=0.45067098434475344\n",
      "Gradient Descent(6/49): loss=0.45028223066464496\n",
      "Gradient Descent(7/49): loss=0.4501179822347991\n",
      "Gradient Descent(8/49): loss=0.4500485872731894\n",
      "Gradient Descent(9/49): loss=0.45001926790190905\n",
      "Gradient Descent(10/49): loss=0.45000688046754317\n",
      "Gradient Descent(11/49): loss=0.45000164677652377\n",
      "Gradient Descent(12/49): loss=0.449999435542068\n",
      "Gradient Descent(13/49): loss=0.4499985012955105\n",
      "Gradient Descent(14/49): loss=0.44999810657633965\n",
      "Gradient Descent(15/49): loss=0.4499979398074902\n",
      "Gradient Descent(16/49): loss=0.4499978693476512\n",
      "Gradient Descent(17/49): loss=0.44999783957836925\n",
      "Gradient Descent(18/49): loss=0.4499978270008477\n",
      "Gradient Descent(19/49): loss=0.4499978216868447\n",
      "Gradient Descent(20/49): loss=0.44999781944167855\n",
      "Gradient Descent(21/49): loss=0.4499978184930958\n",
      "Gradient Descent(22/49): loss=0.4499978180923196\n",
      "Gradient Descent(23/49): loss=0.44999781792299165\n",
      "Gradient Descent(24/49): loss=0.4499978178514507\n",
      "Gradient Descent(25/49): loss=0.44999781782122455\n",
      "Gradient Descent(26/49): loss=0.4499978178084541\n",
      "Gradient Descent(27/49): loss=0.44999781780305853\n",
      "Gradient Descent(28/49): loss=0.4499978178007789\n",
      "Gradient Descent(29/49): loss=0.44999781779981585\n",
      "Gradient Descent(30/49): loss=0.44999781779940884\n",
      "Gradient Descent(31/49): loss=0.4499978177992369\n",
      "Gradient Descent(32/49): loss=0.44999781779916437\n",
      "Gradient Descent(33/49): loss=0.44999781779913356\n",
      "Gradient Descent(34/49): loss=0.4499978177991207\n",
      "Gradient Descent(35/49): loss=0.44999781779911524\n",
      "Gradient Descent(36/49): loss=0.44999781779911285\n",
      "Gradient Descent(37/49): loss=0.44999781779911174\n",
      "Gradient Descent(38/49): loss=0.44999781779911135\n",
      "Gradient Descent(39/49): loss=0.4499978177991111\n",
      "Gradient Descent(40/49): loss=0.44999781779911113\n",
      "Gradient Descent(41/49): loss=0.44999781779911113\n",
      "Gradient Descent(42/49): loss=0.44999781779911097\n",
      "Gradient Descent(43/49): loss=0.44999781779911097\n",
      "Gradient Descent(44/49): loss=0.44999781779911113\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.44999781779911113\n",
      "Gradient Descent(47/49): loss=0.4499978177991111\n",
      "Gradient Descent(48/49): loss=0.4499978177991112\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4714578248\n",
      "Gradient Descent(2/49): loss=0.4593987557779985\n",
      "Gradient Descent(3/49): loss=0.45430379911620455\n",
      "Gradient Descent(4/49): loss=0.4521511799265962\n",
      "Gradient Descent(5/49): loss=0.45124169831898714\n",
      "Gradient Descent(6/49): loss=0.450857442339772\n",
      "Gradient Descent(7/49): loss=0.4506950941885535\n",
      "Gradient Descent(8/49): loss=0.4506265020946638\n",
      "Gradient Descent(9/49): loss=0.45059752193499564\n",
      "Gradient Descent(10/49): loss=0.4505852778175356\n",
      "Gradient Descent(11/49): loss=0.45058010467790877\n",
      "Gradient Descent(12/49): loss=0.4505779190264165\n",
      "Gradient Descent(13/49): loss=0.45057699558866093\n",
      "Gradient Descent(14/49): loss=0.45057660543620914\n",
      "Gradient Descent(15/49): loss=0.45057644059679836\n",
      "Gradient Descent(16/49): loss=0.4505763709521472\n",
      "Gradient Descent(17/49): loss=0.4505763415272821\n",
      "Gradient Descent(18/49): loss=0.4505763290952768\n",
      "Gradient Descent(19/49): loss=0.4505763238427544\n",
      "Gradient Descent(20/49): loss=0.4505763216235637\n",
      "Gradient Descent(21/49): loss=0.45057632068595554\n",
      "Gradient Descent(22/49): loss=0.45057632028981626\n",
      "Gradient Descent(23/49): loss=0.4505763201224473\n",
      "Gradient Descent(24/49): loss=0.45057632005173404\n",
      "Gradient Descent(25/49): loss=0.45057632002185777\n",
      "Gradient Descent(26/49): loss=0.45057632000923475\n",
      "Gradient Descent(27/49): loss=0.45057632000390174\n",
      "Gradient Descent(28/49): loss=0.45057632000164843\n",
      "Gradient Descent(29/49): loss=0.45057632000069664\n",
      "Gradient Descent(30/49): loss=0.4505763200002942\n",
      "Gradient Descent(31/49): loss=0.45057632000012415\n",
      "Gradient Descent(32/49): loss=0.4505763200000527\n",
      "Gradient Descent(33/49): loss=0.450576320000022\n",
      "Gradient Descent(34/49): loss=0.45057632000000936\n",
      "Gradient Descent(35/49): loss=0.4505763200000038\n",
      "Gradient Descent(36/49): loss=0.45057632000000153\n",
      "Gradient Descent(37/49): loss=0.4505763200000006\n",
      "Gradient Descent(38/49): loss=0.4505763200000005\n",
      "Gradient Descent(39/49): loss=0.4505763200000002\n",
      "Gradient Descent(40/49): loss=0.45057632000000014\n",
      "Gradient Descent(41/49): loss=0.45057632000000014\n",
      "Gradient Descent(42/49): loss=0.4505763200000001\n",
      "Gradient Descent(43/49): loss=0.4505763200000001\n",
      "Gradient Descent(44/49): loss=0.4505763200000001\n",
      "Gradient Descent(45/49): loss=0.45057631999999975\n",
      "Gradient Descent(46/49): loss=0.45057632000000014\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.45057632\n",
      "Gradient Descent(49/49): loss=0.45057632\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.471224947232\n",
      "Gradient Descent(2/49): loss=0.4590674874375194\n",
      "Gradient Descent(3/49): loss=0.45393096067435224\n",
      "Gradient Descent(4/49): loss=0.45176077811691384\n",
      "Gradient Descent(5/49): loss=0.4508438759863962\n",
      "Gradient Descent(6/49): loss=0.4504564848362522\n",
      "Gradient Descent(7/49): loss=0.4502928120753166\n",
      "Gradient Descent(8/49): loss=0.4502236603338212\n",
      "Gradient Descent(9/49): loss=0.4501944437230396\n",
      "Gradient Descent(10/49): loss=0.45018209970498424\n",
      "Gradient Descent(11/49): loss=0.45017688435735576\n",
      "Gradient Descent(12/49): loss=0.450174680872983\n",
      "Gradient Descent(13/49): loss=0.4501737499008354\n",
      "Gradient Descent(14/49): loss=0.45017335656510277\n",
      "Gradient Descent(15/49): loss=0.450173190380756\n",
      "Gradient Descent(16/49): loss=0.45017312016786953\n",
      "Gradient Descent(17/49): loss=0.4501730905029249\n",
      "Gradient Descent(18/49): loss=0.45017307796948575\n",
      "Gradient Descent(19/49): loss=0.45017307267410767\n",
      "Gradient Descent(20/49): loss=0.4501730704368106\n",
      "Gradient Descent(21/49): loss=0.45017306949155245\n",
      "Gradient Descent(22/49): loss=0.45017306909218097\n",
      "Gradient Descent(23/49): loss=0.4501730689234463\n",
      "Gradient Descent(24/49): loss=0.4501730688521561\n",
      "Gradient Descent(25/49): loss=0.4501730688220359\n",
      "Gradient Descent(26/49): loss=0.4501730688093103\n",
      "Gradient Descent(27/49): loss=0.4501730688039336\n",
      "Gradient Descent(28/49): loss=0.45017306880166214\n",
      "Gradient Descent(29/49): loss=0.4501730688007023\n",
      "Gradient Descent(30/49): loss=0.45017306880029656\n",
      "Gradient Descent(31/49): loss=0.4501730688001254\n",
      "Gradient Descent(32/49): loss=0.45017306880005287\n",
      "Gradient Descent(33/49): loss=0.45017306880002245\n",
      "Gradient Descent(34/49): loss=0.45017306880000957\n",
      "Gradient Descent(35/49): loss=0.45017306880000396\n",
      "Gradient Descent(36/49): loss=0.45017306880000163\n",
      "Gradient Descent(37/49): loss=0.4501730688000007\n",
      "Gradient Descent(38/49): loss=0.4501730688000003\n",
      "Gradient Descent(39/49): loss=0.4501730688000001\n",
      "Gradient Descent(40/49): loss=0.4501730688000001\n",
      "Gradient Descent(41/49): loss=0.45017306879999985\n",
      "Gradient Descent(42/49): loss=0.4501730688000001\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688\n",
      "Gradient Descent(45/49): loss=0.4501730688000002\n",
      "Gradient Descent(46/49): loss=0.4501730688\n",
      "Gradient Descent(47/49): loss=0.45017306879999985\n",
      "Gradient Descent(48/49): loss=0.4501730687999999\n",
      "Gradient Descent(49/49): loss=0.4501730687999999\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4737766492329303\n",
      "Gradient Descent(2/49): loss=0.461650971838237\n",
      "Gradient Descent(3/49): loss=0.4560440586109311\n",
      "Gradient Descent(4/49): loss=0.45345142193462473\n",
      "Gradient Descent(5/49): loss=0.4522525867355006\n",
      "Gradient Descent(6/49): loss=0.4516982453394256\n",
      "Gradient Descent(7/49): loss=0.45144191787788046\n",
      "Gradient Descent(8/49): loss=0.451323392059662\n",
      "Gradient Descent(9/49): loss=0.45126858572131806\n",
      "Gradient Descent(10/49): loss=0.45124324327046733\n",
      "Gradient Descent(11/49): loss=0.4512315249211944\n",
      "Gradient Descent(12/49): loss=0.45122610635649035\n",
      "Gradient Descent(13/49): loss=0.45122360081217133\n",
      "Gradient Descent(14/49): loss=0.4512224422484781\n",
      "Gradient Descent(15/49): loss=0.4512219065286264\n",
      "Gradient Descent(16/49): loss=0.45122165881176696\n",
      "Gradient Descent(17/49): loss=0.4512215442674911\n",
      "Gradient Descent(18/49): loss=0.451221491302218\n",
      "Gradient Descent(19/49): loss=0.45122146681107606\n",
      "Gradient Descent(20/49): loss=0.45122145548637166\n",
      "Gradient Descent(21/49): loss=0.4512214502498283\n",
      "Gradient Descent(22/49): loss=0.4512214478284507\n",
      "Gradient Descent(23/49): loss=0.4512214467088059\n",
      "Gradient Descent(24/49): loss=0.45122144619108195\n",
      "Gradient Descent(25/49): loss=0.45122144595168645\n",
      "Gradient Descent(26/49): loss=0.45122144584098994\n",
      "Gradient Descent(27/49): loss=0.4512214457898038\n",
      "Gradient Descent(28/49): loss=0.45122144576613543\n",
      "Gradient Descent(29/49): loss=0.4512214457551913\n",
      "Gradient Descent(30/49): loss=0.4512214457501305\n",
      "Gradient Descent(31/49): loss=0.4512214457477908\n",
      "Gradient Descent(32/49): loss=0.4512214457467085\n",
      "Gradient Descent(33/49): loss=0.45122144574620815\n",
      "Gradient Descent(34/49): loss=0.4512214457459767\n",
      "Gradient Descent(35/49): loss=0.45122144574586975\n",
      "Gradient Descent(36/49): loss=0.45122144574582035\n",
      "Gradient Descent(37/49): loss=0.45122144574579737\n",
      "Gradient Descent(38/49): loss=0.4512214457457867\n",
      "Gradient Descent(39/49): loss=0.451221445745782\n",
      "Gradient Descent(40/49): loss=0.4512214457457799\n",
      "Gradient Descent(41/49): loss=0.4512214457457789\n",
      "Gradient Descent(42/49): loss=0.45122144574577827\n",
      "Gradient Descent(43/49): loss=0.4512214457457779\n",
      "Gradient Descent(44/49): loss=0.4512214457457777\n",
      "Gradient Descent(45/49): loss=0.4512214457457778\n",
      "Gradient Descent(46/49): loss=0.4512214457457778\n",
      "Gradient Descent(47/49): loss=0.4512214457457777\n",
      "Gradient Descent(48/49): loss=0.4512214457457779\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47311882684880213\n",
      "Gradient Descent(2/49): loss=0.4606889723836879\n",
      "Gradient Descent(3/49): loss=0.4549414076790194\n",
      "Gradient Descent(4/49): loss=0.45228373375958103\n",
      "Gradient Descent(5/49): loss=0.45105482533923247\n",
      "Gradient Descent(6/49): loss=0.450486578085663\n",
      "Gradient Descent(7/49): loss=0.450223820555613\n",
      "Gradient Descent(8/49): loss=0.45010232147371754\n",
      "Gradient Descent(9/49): loss=0.45004614029824913\n",
      "Gradient Descent(10/49): loss=0.4500201621227128\n",
      "Gradient Descent(11/49): loss=0.4500081498143446\n",
      "Gradient Descent(12/49): loss=0.4500025953229551\n",
      "Gradient Descent(13/49): loss=0.45000002692613633\n",
      "Gradient Descent(14/49): loss=0.4499988392994477\n",
      "Gradient Descent(15/49): loss=0.4499982901408667\n",
      "Gradient Descent(16/49): loss=0.44999803620993895\n",
      "Gradient Descent(17/49): loss=0.44999791879227796\n",
      "Gradient Descent(18/49): loss=0.44999786449835144\n",
      "Gradient Descent(19/49): loss=0.44999783939283977\n",
      "Gradient Descent(20/49): loss=0.4499978277840512\n",
      "Gradient Descent(21/49): loss=0.4499978224161474\n",
      "Gradient Descent(22/49): loss=0.4499978199340286\n",
      "Gradient Descent(23/49): loss=0.44999781878629713\n",
      "Gradient Descent(24/49): loss=0.4499978182555858\n",
      "Gradient Descent(25/49): loss=0.4499978180101851\n",
      "Gradient Descent(26/49): loss=0.4499978178967116\n",
      "Gradient Descent(27/49): loss=0.44999781784424175\n",
      "Gradient Descent(28/49): loss=0.4499978178199796\n",
      "Gradient Descent(29/49): loss=0.44999781780876047\n",
      "Gradient Descent(30/49): loss=0.44999781780357295\n",
      "Gradient Descent(31/49): loss=0.44999781780117415\n",
      "Gradient Descent(32/49): loss=0.4499978178000652\n",
      "Gradient Descent(33/49): loss=0.4499978177995525\n",
      "Gradient Descent(34/49): loss=0.4499978177993151\n",
      "Gradient Descent(35/49): loss=0.44999781779920556\n",
      "Gradient Descent(36/49): loss=0.4499978177991548\n",
      "Gradient Descent(37/49): loss=0.4499978177991314\n",
      "Gradient Descent(38/49): loss=0.44999781779912035\n",
      "Gradient Descent(39/49): loss=0.4499978177991154\n",
      "Gradient Descent(40/49): loss=0.4499978177991131\n",
      "Gradient Descent(41/49): loss=0.4499978177991119\n",
      "Gradient Descent(42/49): loss=0.4499978177991115\n",
      "Gradient Descent(43/49): loss=0.4499978177991113\n",
      "Gradient Descent(44/49): loss=0.4499978177991111\n",
      "Gradient Descent(45/49): loss=0.4499978177991111\n",
      "Gradient Descent(46/49): loss=0.4499978177991112\n",
      "Gradient Descent(47/49): loss=0.44999781779911113\n",
      "Gradient Descent(48/49): loss=0.4499978177991111\n",
      "Gradient Descent(49/49): loss=0.44999781779911113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.473429829632\n",
      "Gradient Descent(2/49): loss=0.4611437828538354\n",
      "Gradient Descent(3/49): loss=0.455462714823613\n",
      "Gradient Descent(4/49): loss=0.45283578896643895\n",
      "Gradient Descent(5/49): loss=0.4516210984500816\n",
      "Gradient Descent(6/49): loss=0.45105942555531753\n",
      "Gradient Descent(7/49): loss=0.4507997080087791\n",
      "Gradient Descent(8/49): loss=0.4506796146152593\n",
      "Gradient Descent(9/49): loss=0.4506240834300958\n",
      "Gradient Descent(10/49): loss=0.4505984058100764\n",
      "Gradient Descent(11/49): loss=0.45058653247857916\n",
      "Gradient Descent(12/49): loss=0.45058104225009515\n",
      "Gradient Descent(13/49): loss=0.45057850356844376\n",
      "Gradient Descent(14/49): loss=0.4505773296820484\n",
      "Gradient Descent(15/49): loss=0.4505767868769794\n",
      "Gradient Descent(16/49): loss=0.4505765358839152\n",
      "Gradient Descent(17/49): loss=0.45057641982472235\n",
      "Gradient Descent(18/49): loss=0.45057636615895175\n",
      "Gradient Descent(19/49): loss=0.4505763413438995\n",
      "Gradient Descent(20/49): loss=0.4505763298694192\n",
      "Gradient Descent(21/49): loss=0.45057632456361935\n",
      "Gradient Descent(22/49): loss=0.4505763221102175\n",
      "Gradient Descent(23/49): loss=0.4505763209757645\n",
      "Gradient Descent(24/49): loss=0.45057632045119356\n",
      "Gradient Descent(25/49): loss=0.4505763202086319\n",
      "Gradient Descent(26/49): loss=0.4505763200964714\n",
      "Gradient Descent(27/49): loss=0.45057632004460835\n",
      "Gradient Descent(28/49): loss=0.450576320020627\n",
      "Gradient Descent(29/49): loss=0.4505763200095378\n",
      "Gradient Descent(30/49): loss=0.4505763200044103\n",
      "Gradient Descent(31/49): loss=0.4505763200020395\n",
      "Gradient Descent(32/49): loss=0.4505763200009431\n",
      "Gradient Descent(33/49): loss=0.450576320000436\n",
      "Gradient Descent(34/49): loss=0.45057632000020165\n",
      "Gradient Descent(35/49): loss=0.45057632000009334\n",
      "Gradient Descent(36/49): loss=0.4505763200000432\n",
      "Gradient Descent(37/49): loss=0.45057632000002\n",
      "Gradient Descent(38/49): loss=0.4505763200000093\n",
      "Gradient Descent(39/49): loss=0.4505763200000042\n",
      "Gradient Descent(40/49): loss=0.4505763200000017\n",
      "Gradient Descent(41/49): loss=0.45057632000000075\n",
      "Gradient Descent(42/49): loss=0.4505763200000005\n",
      "Gradient Descent(43/49): loss=0.45057632000000014\n",
      "Gradient Descent(44/49): loss=0.45057632000000014\n",
      "Gradient Descent(45/49): loss=0.45057632000000014\n",
      "Gradient Descent(46/49): loss=0.4505763199999998\n",
      "Gradient Descent(47/49): loss=0.4505763199999999\n",
      "Gradient Descent(48/49): loss=0.4505763199999998\n",
      "Gradient Descent(49/49): loss=0.4505763199999998\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47321304178687984\n",
      "Gradient Descent(2/49): loss=0.4608267523091325\n",
      "Gradient Descent(3/49): loss=0.45509933205462333\n",
      "Gradient Descent(4/49): loss=0.45245097292893743\n",
      "Gradient Descent(5/49): loss=0.4512263716692205\n",
      "Gradient Descent(6/49): loss=0.45066011604672757\n",
      "Gradient Descent(7/49): loss=0.4503982794468869\n",
      "Gradient Descent(8/49): loss=0.4502772062031208\n",
      "Gradient Descent(9/49): loss=0.4502212219352029\n",
      "Gradient Descent(10/49): loss=0.45019533480971785\n",
      "Gradient Descent(11/49): loss=0.45018336460289343\n",
      "Gradient Descent(12/49): loss=0.4501778295792579\n",
      "Gradient Descent(13/49): loss=0.450175270184329\n",
      "Gradient Descent(14/49): loss=0.4501740867201138\n",
      "Gradient Descent(15/49): loss=0.4501735394862606\n",
      "Gradient Descent(16/49): loss=0.4501732864453268\n",
      "Gradient Descent(17/49): loss=0.45017316943919916\n",
      "Gradient Descent(18/49): loss=0.4501731153355657\n",
      "Gradient Descent(19/49): loss=0.4501730903180456\n",
      "Gradient Descent(20/49): loss=0.4501730787499442\n",
      "Gradient Descent(21/49): loss=0.4501730734008542\n",
      "Gradient Descent(22/49): loss=0.45017307092743497\n",
      "Gradient Descent(23/49): loss=0.45017306978372607\n",
      "Gradient Descent(24/49): loss=0.4501730692548747\n",
      "Gradient Descent(25/49): loss=0.4501730690103341\n",
      "Gradient Descent(26/49): loss=0.45017306889725867\n",
      "Gradient Descent(27/49): loss=0.4501730688449723\n",
      "Gradient Descent(28/49): loss=0.4501730688207952\n",
      "Gradient Descent(29/49): loss=0.45017306880961583\n",
      "Gradient Descent(30/49): loss=0.45017306880444613\n",
      "Gradient Descent(31/49): loss=0.45017306880205615\n",
      "Gradient Descent(32/49): loss=0.45017306880095087\n",
      "Gradient Descent(33/49): loss=0.45017306880043945\n",
      "Gradient Descent(34/49): loss=0.45017306880020314\n",
      "Gradient Descent(35/49): loss=0.450173068800094\n",
      "Gradient Descent(36/49): loss=0.4501730688000434\n",
      "Gradient Descent(37/49): loss=0.4501730688000201\n",
      "Gradient Descent(38/49): loss=0.4501730688000094\n",
      "Gradient Descent(39/49): loss=0.4501730688000042\n",
      "Gradient Descent(40/49): loss=0.45017306880000185\n",
      "Gradient Descent(41/49): loss=0.45017306880000096\n",
      "Gradient Descent(42/49): loss=0.4501730688000004\n",
      "Gradient Descent(43/49): loss=0.4501730688000002\n",
      "Gradient Descent(44/49): loss=0.4501730688000001\n",
      "Gradient Descent(45/49): loss=0.4501730688000001\n",
      "Gradient Descent(46/49): loss=0.45017306880000024\n",
      "Gradient Descent(47/49): loss=0.4501730688\n",
      "Gradient Descent(48/49): loss=0.4501730688\n",
      "Gradient Descent(49/49): loss=0.4501730688000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4758107149453313\n",
      "Gradient Descent(2/49): loss=0.4636168963492706\n",
      "Gradient Descent(3/49): loss=0.4574699923949992\n",
      "Gradient Descent(4/49): loss=0.4543713381116508\n",
      "Gradient Descent(5/49): loss=0.4528093064874142\n",
      "Gradient Descent(6/49): loss=0.4520218863456368\n",
      "Gradient Descent(7/49): loss=0.45162494785216684\n",
      "Gradient Descent(8/49): loss=0.45142485115760844\n",
      "Gradient Descent(9/49): loss=0.45132398241388166\n",
      "Gradient Descent(10/49): loss=0.45127313448016915\n",
      "Gradient Descent(11/49): loss=0.4512475020367843\n",
      "Gradient Descent(12/49): loss=0.4512345807220744\n",
      "Gradient Descent(13/49): loss=0.45122806708732865\n",
      "Gradient Descent(14/49): loss=0.45122478356405377\n",
      "Gradient Descent(15/49): loss=0.4512231283399707\n",
      "Gradient Descent(16/49): loss=0.4512222939415105\n",
      "Gradient Descent(17/49): loss=0.4512218733212466\n",
      "Gradient Descent(18/49): loss=0.4512216612865717\n",
      "Gradient Descent(19/49): loss=0.4512215543998918\n",
      "Gradient Descent(20/49): loss=0.4512215005183168\n",
      "Gradient Descent(21/49): loss=0.45122147335661483\n",
      "Gradient Descent(22/49): loss=0.4512214596644007\n",
      "Gradient Descent(23/49): loss=0.4512214527621555\n",
      "Gradient Descent(24/49): loss=0.45122144928273394\n",
      "Gradient Descent(25/49): loss=0.45122144752875715\n",
      "Gradient Descent(26/49): loss=0.45122144664457775\n",
      "Gradient Descent(27/49): loss=0.45122144619886284\n",
      "Gradient Descent(28/49): loss=0.45122144597417785\n",
      "Gradient Descent(29/49): loss=0.45122144586091434\n",
      "Gradient Descent(30/49): loss=0.4512214458038181\n",
      "Gradient Descent(31/49): loss=0.45122144577503576\n",
      "Gradient Descent(32/49): loss=0.45122144576052675\n",
      "Gradient Descent(33/49): loss=0.4512214457532127\n",
      "Gradient Descent(34/49): loss=0.45122144574952566\n",
      "Gradient Descent(35/49): loss=0.4512214457476671\n",
      "Gradient Descent(36/49): loss=0.45122144574673023\n",
      "Gradient Descent(37/49): loss=0.4512214457462579\n",
      "Gradient Descent(38/49): loss=0.4512214457460197\n",
      "Gradient Descent(39/49): loss=0.45122144574589973\n",
      "Gradient Descent(40/49): loss=0.45122144574583917\n",
      "Gradient Descent(41/49): loss=0.45122144574580886\n",
      "Gradient Descent(42/49): loss=0.45122144574579326\n",
      "Gradient Descent(43/49): loss=0.45122144574578565\n",
      "Gradient Descent(44/49): loss=0.4512214457457816\n",
      "Gradient Descent(45/49): loss=0.45122144574577966\n",
      "Gradient Descent(46/49): loss=0.45122144574577866\n",
      "Gradient Descent(47/49): loss=0.4512214457457782\n",
      "Gradient Descent(48/49): loss=0.4512214457457781\n",
      "Gradient Descent(49/49): loss=0.4512214457457778\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4752039178465791\n",
      "Gradient Descent(2/49): loss=0.4627042128330409\n",
      "Gradient Descent(3/49): loss=0.45640311153571556\n",
      "Gradient Descent(4/49): loss=0.4532267263717334\n",
      "Gradient Descent(5/49): loss=0.45162551061057005\n",
      "Gradient Descent(6/49): loss=0.45081833774536745\n",
      "Gradient Descent(7/49): loss=0.45041144190401894\n",
      "Gradient Descent(8/49): loss=0.4502063257103951\n",
      "Gradient Descent(9/49): loss=0.4501029266371895\n",
      "Gradient Descent(10/49): loss=0.45005080316438617\n",
      "Gradient Descent(11/49): loss=0.4500245277217463\n",
      "Gradient Descent(12/49): loss=0.4500112822711116\n",
      "Gradient Descent(13/49): loss=0.4500046052394466\n",
      "Gradient Descent(14/49): loss=0.4500012393477843\n",
      "Gradient Descent(15/49): loss=0.44999954260179725\n",
      "Gradient Descent(16/49): loss=0.44999868727214526\n",
      "Gradient Descent(17/49): loss=0.44999825610046756\n",
      "Gradient Descent(18/49): loss=0.44999803874682487\n",
      "Gradient Descent(19/49): loss=0.44999792917885373\n",
      "Gradient Descent(20/49): loss=0.4499978739456392\n",
      "Gradient Descent(21/49): loss=0.449997846102576\n",
      "Gradient Descent(22/49): loss=0.44999783206688776\n",
      "Gradient Descent(23/49): loss=0.4499978249914974\n",
      "Gradient Descent(24/49): loss=0.44999782142479305\n",
      "Gradient Descent(25/49): loss=0.4499978196268174\n",
      "Gradient Descent(26/49): loss=0.4499978187204577\n",
      "Gradient Descent(27/49): loss=0.449997818263562\n",
      "Gradient Descent(28/49): loss=0.4499978180332408\n",
      "Gradient Descent(29/49): loss=0.44999781791713617\n",
      "Gradient Descent(30/49): loss=0.44999781785860743\n",
      "Gradient Descent(31/49): loss=0.449997817829103\n",
      "Gradient Descent(32/49): loss=0.44999781781423004\n",
      "Gradient Descent(33/49): loss=0.4499978178067324\n",
      "Gradient Descent(34/49): loss=0.449997817802953\n",
      "Gradient Descent(35/49): loss=0.4499978178010479\n",
      "Gradient Descent(36/49): loss=0.4499978178000873\n",
      "Gradient Descent(37/49): loss=0.4499978177996032\n",
      "Gradient Descent(38/49): loss=0.44999781779935916\n",
      "Gradient Descent(39/49): loss=0.44999781779923625\n",
      "Gradient Descent(40/49): loss=0.44999781779917425\n",
      "Gradient Descent(41/49): loss=0.44999781779914305\n",
      "Gradient Descent(42/49): loss=0.4499978177991271\n",
      "Gradient Descent(43/49): loss=0.4499978177991191\n",
      "Gradient Descent(44/49): loss=0.4499978177991151\n",
      "Gradient Descent(45/49): loss=0.44999781779911313\n",
      "Gradient Descent(46/49): loss=0.44999781779911224\n",
      "Gradient Descent(47/49): loss=0.44999781779911174\n",
      "Gradient Descent(48/49): loss=0.4499978177991112\n",
      "Gradient Descent(49/49): loss=0.4499978177991113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47549079708800013\n",
      "Gradient Descent(2/49): loss=0.4631357079000595\n",
      "Gradient Descent(3/49): loss=0.45690750744042036\n",
      "Gradient Descent(4/49): loss=0.4537678715887161\n",
      "Gradient Descent(5/49): loss=0.4521851811558717\n",
      "Gradient Descent(6/49): loss=0.45138734690867494\n",
      "Gradient Descent(7/49): loss=0.45098515866466293\n",
      "Gradient Descent(8/49): loss=0.4507824155708565\n",
      "Gradient Descent(9/49): loss=0.45068021277726883\n",
      "Gradient Descent(10/49): loss=0.45062869234902114\n",
      "Gradient Descent(11/49): loss=0.45060272090114173\n",
      "Gradient Descent(12/49): loss=0.45058962869426544\n",
      "Gradient Descent(13/49): loss=0.45058302891277924\n",
      "Gradient Descent(14/49): loss=0.45057970196293184\n",
      "Gradient Descent(15/49): loss=0.4505780248475139\n",
      "Gradient Descent(16/49): loss=0.45057717941363196\n",
      "Gradient Descent(17/49): loss=0.45057675323041185\n",
      "Gradient Descent(18/49): loss=0.4505765383914507\n",
      "Gradient Descent(19/49): loss=0.45057643009113024\n",
      "Gradient Descent(20/49): loss=0.45057637549693885\n",
      "Gradient Descent(21/49): loss=0.4505763479760069\n",
      "Gradient Descent(22/49): loss=0.450576334102705\n",
      "Gradient Descent(23/49): loss=0.4505763271091736\n",
      "Gradient Descent(24/49): loss=0.4505763235837345\n",
      "Gradient Descent(25/49): loss=0.45057632180656026\n",
      "Gradient Descent(26/49): loss=0.4505763209106872\n",
      "Gradient Descent(27/49): loss=0.4505763204590775\n",
      "Gradient Descent(28/49): loss=0.45057632023142086\n",
      "Gradient Descent(29/49): loss=0.4505763201166593\n",
      "Gradient Descent(30/49): loss=0.4505763200588078\n",
      "Gradient Descent(31/49): loss=0.4505763200296451\n",
      "Gradient Descent(32/49): loss=0.45057632001494397\n",
      "Gradient Descent(33/49): loss=0.4505763200075333\n",
      "Gradient Descent(34/49): loss=0.45057632000379744\n",
      "Gradient Descent(35/49): loss=0.4505763200019145\n",
      "Gradient Descent(36/49): loss=0.4505763200009651\n",
      "Gradient Descent(37/49): loss=0.45057632000048653\n",
      "Gradient Descent(38/49): loss=0.4505763200002452\n",
      "Gradient Descent(39/49): loss=0.4505763200001235\n",
      "Gradient Descent(40/49): loss=0.45057632000006226\n",
      "Gradient Descent(41/49): loss=0.4505763200000315\n",
      "Gradient Descent(42/49): loss=0.45057632000001574\n",
      "Gradient Descent(43/49): loss=0.45057632000000797\n",
      "Gradient Descent(44/49): loss=0.45057632000000397\n",
      "Gradient Descent(45/49): loss=0.4505763200000021\n",
      "Gradient Descent(46/49): loss=0.4505763200000009\n",
      "Gradient Descent(47/49): loss=0.4505763200000007\n",
      "Gradient Descent(48/49): loss=0.4505763200000002\n",
      "Gradient Descent(49/49): loss=0.4505763200000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4752908248179201\n",
      "Gradient Descent(2/49): loss=0.4628349296086318\n",
      "Gradient Descent(3/49): loss=0.4565559128336325\n",
      "Gradient Descent(4/49): loss=0.45339066047735405\n",
      "Gradient Descent(5/49): loss=0.451795056764554\n",
      "Gradient Descent(6/49): loss=0.4509907129329316\n",
      "Gradient Descent(7/49): loss=0.45058524320741067\n",
      "Gradient Descent(8/49): loss=0.4503808459187757\n",
      "Gradient Descent(9/49): loss=0.4502778092455751\n",
      "Gradient Descent(10/49): loss=0.4502258684586141\n",
      "Gradient Descent(11/49): loss=0.4501996851079074\n",
      "Gradient Descent(12/49): loss=0.4501864860808161\n",
      "Gradient Descent(13/49): loss=0.4501798324512595\n",
      "Gradient Descent(14/49): loss=0.45017647835659974\n",
      "Gradient Descent(15/49): loss=0.4501747875574821\n",
      "Gradient Descent(16/49): loss=0.4501739352256468\n",
      "Gradient Descent(17/49): loss=0.4501735055651685\n",
      "Gradient Descent(18/49): loss=0.45017328897332165\n",
      "Gradient Descent(19/49): loss=0.45017317978937116\n",
      "Gradient Descent(20/49): loss=0.4501731247497421\n",
      "Gradient Descent(21/49): loss=0.4501730970042651\n",
      "Gradient Descent(22/49): loss=0.45017308301776987\n",
      "Gradient Descent(23/49): loss=0.4501730759671779\n",
      "Gradient Descent(24/49): loss=0.45017307241297444\n",
      "Gradient Descent(25/49): loss=0.45017307062130035\n",
      "Gradient Descent(26/49): loss=0.45017306971811766\n",
      "Gradient Descent(27/49): loss=0.45017306926282286\n",
      "Gradient Descent(28/49): loss=0.45017306903330895\n",
      "Gradient Descent(29/49): loss=0.4501730689176112\n",
      "Gradient Descent(30/49): loss=0.45017306885928776\n",
      "Gradient Descent(31/49): loss=0.45017306882988706\n",
      "Gradient Descent(32/49): loss=0.4501730688150659\n",
      "Gradient Descent(33/49): loss=0.4501730688075947\n",
      "Gradient Descent(34/49): loss=0.45017306880382835\n",
      "Gradient Descent(35/49): loss=0.4501730688019298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=0.4501730688009728\n",
      "Gradient Descent(37/49): loss=0.4501730688004905\n",
      "Gradient Descent(38/49): loss=0.4501730688002474\n",
      "Gradient Descent(39/49): loss=0.45017306880012475\n",
      "Gradient Descent(40/49): loss=0.45017306880006297\n",
      "Gradient Descent(41/49): loss=0.4501730688000316\n",
      "Gradient Descent(42/49): loss=0.4501730688000159\n",
      "Gradient Descent(43/49): loss=0.4501730688000079\n",
      "Gradient Descent(44/49): loss=0.45017306880000413\n",
      "Gradient Descent(45/49): loss=0.45017306880000196\n",
      "Gradient Descent(46/49): loss=0.450173068800001\n",
      "Gradient Descent(47/49): loss=0.4501730688000006\n",
      "Gradient Descent(48/49): loss=0.4501730688000003\n",
      "Gradient Descent(49/49): loss=0.4501730688000001\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4779325820553898\n",
      "Gradient Descent(2/49): loss=0.46584846398892044\n",
      "Gradient Descent(3/49): loss=0.4592312009357221\n",
      "Gradient Descent(4/49): loss=0.4556075876877912\n",
      "Gradient Descent(5/49): loss=0.45362329707322485\n",
      "Gradient Descent(6/49): loss=0.452536699532688\n",
      "Gradient Descent(7/49): loss=0.4519416787194896\n",
      "Gradient Descent(8/49): loss=0.4516158453221824\n",
      "Gradient Descent(9/49): loss=0.4514374189538168\n",
      "Gradient Descent(10/49): loss=0.4513397126745002\n",
      "Gradient Descent(11/49): loss=0.4512862087159461\n",
      "Gradient Descent(12/49): loss=0.4512569099482418\n",
      "Gradient Descent(13/49): loss=0.45124086594304713\n",
      "Gradient Descent(14/49): loss=0.45123208024580247\n",
      "Gradient Descent(15/49): loss=0.4512272691979911\n",
      "Gradient Descent(16/49): loss=0.45122463466821006\n",
      "Gradient Descent(17/49): loss=0.4512231919997017\n",
      "Gradient Descent(18/49): loss=0.45122240199442637\n",
      "Gradient Descent(19/49): loss=0.4512219693875379\n",
      "Gradient Descent(20/49): loss=0.4512217324920057\n",
      "Gradient Descent(21/49): loss=0.45122160276801215\n",
      "Gradient Descent(22/49): loss=0.45122153173115337\n",
      "Gradient Descent(23/49): loss=0.4512214928313694\n",
      "Gradient Descent(24/49): loss=0.45122147152984765\n",
      "Gradient Descent(25/49): loss=0.4512214598651345\n",
      "Gradient Descent(26/49): loss=0.4512214534775374\n",
      "Gradient Descent(27/49): loss=0.4512214499796894\n",
      "Gradient Descent(28/49): loss=0.4512214480642677\n",
      "Gradient Descent(29/49): loss=0.4512214470153829\n",
      "Gradient Descent(30/49): loss=0.45122144644101353\n",
      "Gradient Descent(31/49): loss=0.45122144612648907\n",
      "Gradient Descent(32/49): loss=0.45122144595425506\n",
      "Gradient Descent(33/49): loss=0.4512214458599401\n",
      "Gradient Descent(34/49): loss=0.4512214458082933\n",
      "Gradient Descent(35/49): loss=0.4512214457800113\n",
      "Gradient Descent(36/49): loss=0.45122144576452394\n",
      "Gradient Descent(37/49): loss=0.4512214457560433\n",
      "Gradient Descent(38/49): loss=0.45122144575139894\n",
      "Gradient Descent(39/49): loss=0.451221445748856\n",
      "Gradient Descent(40/49): loss=0.45122144574746337\n",
      "Gradient Descent(41/49): loss=0.45122144574670064\n",
      "Gradient Descent(42/49): loss=0.45122144574628337\n",
      "Gradient Descent(43/49): loss=0.45122144574605455\n",
      "Gradient Descent(44/49): loss=0.4512214457459293\n",
      "Gradient Descent(45/49): loss=0.4512214457458607\n",
      "Gradient Descent(46/49): loss=0.4512214457458233\n",
      "Gradient Descent(47/49): loss=0.4512214457458028\n",
      "Gradient Descent(48/49): loss=0.4512214457457913\n",
      "Gradient Descent(49/49): loss=0.45122144574578515\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4773790127723179\n",
      "Gradient Descent(2/49): loss=0.4649917601664384\n",
      "Gradient Descent(3/49): loss=0.45820850063946006\n",
      "Gradient Descent(4/49): loss=0.454493987722486\n",
      "Gradient Descent(5/49): loss=0.45245992044915095\n",
      "Gradient Descent(6/49): loss=0.4513460652102729\n",
      "Gradient Descent(7/49): loss=0.4507361180814636\n",
      "Gradient Descent(8/49): loss=0.4504021110337275\n",
      "Gradient Descent(9/49): loss=0.4502192087743869\n",
      "Gradient Descent(10/49): loss=0.450119051497172\n",
      "Gradient Descent(11/49): loss=0.4500642053721694\n",
      "Gradient Descent(12/49): loss=0.45003417163411774\n",
      "Gradient Descent(13/49): loss=0.4500177251591607\n",
      "Gradient Descent(14/49): loss=0.4500087190694742\n",
      "Gradient Descent(15/49): loss=0.45000378733476204\n",
      "Gradient Descent(16/49): loss=0.45000108671683353\n",
      "Gradient Descent(17/49): loss=0.4499996078584557\n",
      "Gradient Descent(18/49): loss=0.4499987980356084\n",
      "Gradient Descent(19/49): loss=0.44999835457661685\n",
      "Gradient Descent(20/49): loss=0.4499981117384734\n",
      "Gradient Descent(21/49): loss=0.4499979787603058\n",
      "Gradient Descent(22/49): loss=0.4499979059414612\n",
      "Gradient Descent(23/49): loss=0.44999786606586206\n",
      "Gradient Descent(24/49): loss=0.44999784422998396\n",
      "Gradient Descent(25/49): loss=0.4499978322726571\n",
      "Gradient Descent(26/49): loss=0.4499978257248249\n",
      "Gradient Descent(27/49): loss=0.44999782213923195\n",
      "Gradient Descent(28/49): loss=0.4499978201757611\n",
      "Gradient Descent(29/49): loss=0.44999781910056463\n",
      "Gradient Descent(30/49): loss=0.4499978185117873\n",
      "Gradient Descent(31/49): loss=0.4499978181893726\n",
      "Gradient Descent(32/49): loss=0.44999781801281813\n",
      "Gradient Descent(33/49): loss=0.4499978179161374\n",
      "Gradient Descent(34/49): loss=0.44999781786319465\n",
      "Gradient Descent(35/49): loss=0.44999781783420306\n",
      "Gradient Descent(36/49): loss=0.4499978178183276\n",
      "Gradient Descent(37/49): loss=0.44999781780963405\n",
      "Gradient Descent(38/49): loss=0.44999781780487363\n",
      "Gradient Descent(39/49): loss=0.44999781780226655\n",
      "Gradient Descent(40/49): loss=0.44999781780083903\n",
      "Gradient Descent(41/49): loss=0.4499978178000574\n",
      "Gradient Descent(42/49): loss=0.44999781779962933\n",
      "Gradient Descent(43/49): loss=0.44999781779939496\n",
      "Gradient Descent(44/49): loss=0.4499978177992665\n",
      "Gradient Descent(45/49): loss=0.44999781779919595\n",
      "Gradient Descent(46/49): loss=0.44999781779915793\n",
      "Gradient Descent(47/49): loss=0.44999781779913667\n",
      "Gradient Descent(48/49): loss=0.44999781779912496\n",
      "Gradient Descent(49/49): loss=0.44999781779911874\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.477640727168\n",
      "Gradient Descent(2/49): loss=0.4653967893651963\n",
      "Gradient Descent(3/49): loss=0.4586920090243823\n",
      "Gradient Descent(4/49): loss=0.4550204713097515\n",
      "Gradient Descent(5/49): loss=0.45300993725721994\n",
      "Gradient Descent(6/49): loss=0.45190896881005393\n",
      "Gradient Descent(7/49): loss=0.4513060784883854\n",
      "Gradient Descent(8/49): loss=0.45097593574823974\n",
      "Gradient Descent(9/49): loss=0.45079514958373607\n",
      "Gradient Descent(10/49): loss=0.450696151080054\n",
      "Gradient Descent(11/49): loss=0.4506419394994376\n",
      "Gradient Descent(12/49): loss=0.450612253237892\n",
      "Gradient Descent(13/49): loss=0.4505959970410696\n",
      "Gradient Descent(14/49): loss=0.4505870951476898\n",
      "Gradient Descent(15/49): loss=0.4505822204708751\n",
      "Gradient Descent(16/49): loss=0.4505795510978511\n",
      "Gradient Descent(17/49): loss=0.45057808934918325\n",
      "Gradient Descent(18/49): loss=0.4505772888956127\n",
      "Gradient Descent(19/49): loss=0.45057685056723756\n",
      "Gradient Descent(20/49): loss=0.45057661053861936\n",
      "Gradient Descent(21/49): loss=0.4505764790989481\n",
      "Gradient Descent(22/49): loss=0.45057640712258384\n",
      "Gradient Descent(23/49): loss=0.4505763677083269\n",
      "Gradient Descent(24/49): loss=0.4505763461250798\n",
      "Gradient Descent(25/49): loss=0.4505763343060938\n",
      "Gradient Descent(26/49): loss=0.45057632783401697\n",
      "Gradient Descent(27/49): loss=0.4505763242899077\n",
      "Gradient Descent(28/49): loss=0.4505763223491535\n",
      "Gradient Descent(29/49): loss=0.45057632128639635\n",
      "Gradient Descent(30/49): loss=0.4505763207044308\n",
      "Gradient Descent(31/49): loss=0.45057632038574624\n",
      "Gradient Descent(32/49): loss=0.4505763202112347\n",
      "Gradient Descent(33/49): loss=0.45057632011567206\n",
      "Gradient Descent(34/49): loss=0.4505763200633418\n",
      "Gradient Descent(35/49): loss=0.45057632003468595\n",
      "Gradient Descent(36/49): loss=0.45057632001899406\n",
      "Gradient Descent(37/49): loss=0.45057632001040115\n",
      "Gradient Descent(38/49): loss=0.4505763200056958\n",
      "Gradient Descent(39/49): loss=0.4505763200031191\n",
      "Gradient Descent(40/49): loss=0.45057632000170794\n",
      "Gradient Descent(41/49): loss=0.45057632000093534\n",
      "Gradient Descent(42/49): loss=0.4505763200005121\n",
      "Gradient Descent(43/49): loss=0.45057632000028053\n",
      "Gradient Descent(44/49): loss=0.4505763200001534\n",
      "Gradient Descent(45/49): loss=0.45057632000008396\n",
      "Gradient Descent(46/49): loss=0.45057632000004616\n",
      "Gradient Descent(47/49): loss=0.45057632000002534\n",
      "Gradient Descent(48/49): loss=0.45057632000001374\n",
      "Gradient Descent(49/49): loss=0.4505763200000074\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47745829632512\n",
      "Gradient Descent(2/49): loss=0.4651144593927542\n",
      "Gradient Descent(3/49): loss=0.4583549742885915\n",
      "Gradient Descent(4/49): loss=0.45465348024555197\n",
      "Gradient Descent(5/49): loss=0.4526265421075845\n",
      "Gradient Descent(6/49): loss=0.4515165907832332\n",
      "Gradient Descent(7/49): loss=0.45090878143801827\n",
      "Gradient Descent(8/49): loss=0.45057594504057874\n",
      "Gradient Descent(9/49): loss=0.45039368382934114\n",
      "Gradient Descent(10/49): loss=0.4502938775900672\n",
      "Gradient Descent(11/49): loss=0.4502392236934406\n",
      "Gradient Descent(12/49): loss=0.45020929521964803\n",
      "Gradient Descent(13/49): loss=0.4501929063873994\n",
      "Gradient Descent(14/49): loss=0.45018393186285993\n",
      "Gradient Descent(15/49): loss=0.45017901741322225\n",
      "Gradient Descent(16/49): loss=0.4501763262606004\n",
      "Gradient Descent(17/49): loss=0.45017485258542467\n",
      "Gradient Descent(18/49): loss=0.45017404560089874\n",
      "Gradient Descent(19/49): loss=0.45017360369617204\n",
      "Gradient Descent(20/49): loss=0.4501733617091439\n",
      "Gradient Descent(21/49): loss=0.4501732291970471\n",
      "Gradient Descent(22/49): loss=0.45017315663342317\n",
      "Gradient Descent(23/49): loss=0.45017311689758244\n",
      "Gradient Descent(24/49): loss=0.4501730951382363\n",
      "Gradient Descent(25/49): loss=0.450173083222818\n",
      "Gradient Descent(26/49): loss=0.4501730766979351\n",
      "Gradient Descent(27/49): loss=0.45017307312490923\n",
      "Gradient Descent(28/49): loss=0.45017307116832034\n",
      "Gradient Descent(29/49): loss=0.4501730700968922\n",
      "Gradient Descent(30/49): loss=0.45017306951017827\n",
      "Gradient Descent(31/49): loss=0.4501730691888936\n",
      "Gradient Descent(32/49): loss=0.4501730690129582\n",
      "Gradient Descent(33/49): loss=0.45017306891661585\n",
      "Gradient Descent(34/49): loss=0.4501730688638589\n",
      "Gradient Descent(35/49): loss=0.450173068834969\n",
      "Gradient Descent(36/49): loss=0.4501730688191489\n",
      "Gradient Descent(37/49): loss=0.450173068810486\n",
      "Gradient Descent(38/49): loss=0.45017306880574226\n",
      "Gradient Descent(39/49): loss=0.4501730688031443\n",
      "Gradient Descent(40/49): loss=0.4501730688017218\n",
      "Gradient Descent(41/49): loss=0.45017306880094277\n",
      "Gradient Descent(42/49): loss=0.45017306880051633\n",
      "Gradient Descent(43/49): loss=0.4501730688002828\n",
      "Gradient Descent(44/49): loss=0.4501730688001549\n",
      "Gradient Descent(45/49): loss=0.4501730688000848\n",
      "Gradient Descent(46/49): loss=0.4501730688000465\n",
      "Gradient Descent(47/49): loss=0.4501730688000256\n",
      "Gradient Descent(48/49): loss=0.45017306880001384\n",
      "Gradient Descent(49/49): loss=0.4501730688000076\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48014225056310617\n",
      "Gradient Descent(2/49): loss=0.4683685909219736\n",
      "Gradient Descent(3/49): loss=0.4613879881207432\n",
      "Gradient Descent(4/49): loss=0.45724918871989456\n",
      "Gradient Descent(5/49): loss=0.45479529455513157\n",
      "Gradient Descent(6/49): loss=0.4533403807048432\n",
      "Gradient Descent(7/49): loss=0.45247776228300784\n",
      "Gradient Descent(8/49): loss=0.45196631582070135\n",
      "Gradient Descent(9/49): loss=0.4516630792131999\n",
      "Gradient Descent(10/49): loss=0.4514832902286124\n",
      "Gradient Descent(11/49): loss=0.4513766933396506\n",
      "Gradient Descent(12/49): loss=0.45131349204418475\n",
      "Gradient Descent(13/49): loss=0.45127601999610345\n",
      "Gradient Descent(14/49): loss=0.4512538028187959\n",
      "Gradient Descent(15/49): loss=0.45124063025437017\n",
      "Gradient Descent(16/49): loss=0.4512328202409222\n",
      "Gradient Descent(17/49): loss=0.451228189683949\n",
      "Gradient Descent(18/49): loss=0.4512254442267195\n",
      "Gradient Descent(19/49): loss=0.451223816445128\n",
      "Gradient Descent(20/49): loss=0.45122285133342244\n",
      "Gradient Descent(21/49): loss=0.45122227911869245\n",
      "Gradient Descent(22/49): loss=0.4512219398525788\n",
      "Gradient Descent(23/49): loss=0.4512217387017002\n",
      "Gradient Descent(24/49): loss=0.45122161943934413\n",
      "Gradient Descent(25/49): loss=0.4512215487286933\n",
      "Gradient Descent(26/49): loss=0.45122150680434836\n",
      "Gradient Descent(27/49): loss=0.45122148194740447\n",
      "Gradient Descent(28/49): loss=0.45122146720972217\n",
      "Gradient Descent(29/49): loss=0.4512214584717504\n",
      "Gradient Descent(30/49): loss=0.45122145329100694\n",
      "Gradient Descent(31/49): loss=0.4512214502193443\n",
      "Gradient Descent(32/49): loss=0.451221448398155\n",
      "Gradient Descent(33/49): loss=0.4512214473183725\n",
      "Gradient Descent(34/49): loss=0.45122144667816916\n",
      "Gradient Descent(35/49): loss=0.45122144629859257\n",
      "Gradient Descent(36/49): loss=0.45122144607354187\n",
      "Gradient Descent(37/49): loss=0.4512214459401092\n",
      "Gradient Descent(38/49): loss=0.45122144586099694\n",
      "Gradient Descent(39/49): loss=0.4512214458140912\n",
      "Gradient Descent(40/49): loss=0.4512214457862809\n",
      "Gradient Descent(41/49): loss=0.45122144576979206\n",
      "Gradient Descent(42/49): loss=0.4512214457600159\n",
      "Gradient Descent(43/49): loss=0.45122144575421946\n",
      "Gradient Descent(44/49): loss=0.451221445750783\n",
      "Gradient Descent(45/49): loss=0.4512214457487454\n",
      "Gradient Descent(46/49): loss=0.45122144574753736\n",
      "Gradient Descent(47/49): loss=0.45122144574682094\n",
      "Gradient Descent(48/49): loss=0.4512214457463962\n",
      "Gradient Descent(49/49): loss=0.45122144574614453\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.479644111626018\n",
      "Gradient Descent(2/49): loss=0.4675751054090826\n",
      "Gradient Descent(3/49): loss=0.4604193916230631\n",
      "Gradient Descent(4/49): loss=0.4561767689193325\n",
      "Gradient Descent(5/49): loss=0.45366131791829095\n",
      "Gradient Descent(6/49): loss=0.45216990701977294\n",
      "Gradient Descent(7/49): loss=0.4512856494980416\n",
      "Gradient Descent(8/49): loss=0.4507613732134068\n",
      "Gradient Descent(9/49): loss=0.45045052980424705\n",
      "Gradient Descent(10/49): loss=0.45026623074695615\n",
      "Gradient Descent(11/49): loss=0.45015695983588844\n",
      "Gradient Descent(12/49): loss=0.45009217311271654\n",
      "Gradient Descent(13/49): loss=0.4500537610645478\n",
      "Gradient Descent(14/49): loss=0.4500309865611884\n",
      "Gradient Descent(15/49): loss=0.4500174835581468\n",
      "Gradient Descent(16/49): loss=0.4500094776276432\n",
      "Gradient Descent(17/49): loss=0.450004730911448\n",
      "Gradient Descent(18/49): loss=0.45000191658341565\n",
      "Gradient Descent(19/49): loss=0.4500002479683252\n",
      "Gradient Descent(20/49): loss=0.4499992586464383\n",
      "Gradient Descent(21/49): loss=0.44999867207749134\n",
      "Gradient Descent(22/49): loss=0.44999832430076286\n",
      "Gradient Descent(23/49): loss=0.44999811810394036\n",
      "Gradient Descent(24/49): loss=0.4499979958498444\n",
      "Gradient Descent(25/49): loss=0.44999792336539085\n",
      "Gradient Descent(26/49): loss=0.44999788038935856\n",
      "Gradient Descent(27/49): loss=0.4499978549088687\n",
      "Gradient Descent(28/49): loss=0.44999783980148644\n",
      "Gradient Descent(29/49): loss=0.4499978308443193\n",
      "Gradient Descent(30/49): loss=0.44999782553361517\n",
      "Gradient Descent(31/49): loss=0.44999782238489855\n",
      "Gradient Descent(32/49): loss=0.4499978205180245\n",
      "Gradient Descent(33/49): loss=0.44999781941115496\n",
      "Gradient Descent(34/49): loss=0.44999781875489187\n",
      "Gradient Descent(35/49): loss=0.4499978183657934\n",
      "Gradient Descent(36/49): loss=0.44999781813509715\n",
      "Gradient Descent(37/49): loss=0.4499978179983172\n",
      "Gradient Descent(38/49): loss=0.4499978179172203\n",
      "Gradient Descent(39/49): loss=0.4499978178691381\n",
      "Gradient Descent(40/49): loss=0.44999781784063003\n",
      "Gradient Descent(41/49): loss=0.4499978178237278\n",
      "Gradient Descent(42/49): loss=0.4499978178137063\n",
      "Gradient Descent(43/49): loss=0.4499978178077646\n",
      "Gradient Descent(44/49): loss=0.4499978178042419\n",
      "Gradient Descent(45/49): loss=0.4499978178021531\n",
      "Gradient Descent(46/49): loss=0.4499978178009148\n",
      "Gradient Descent(47/49): loss=0.4499978178001806\n",
      "Gradient Descent(48/49): loss=0.4499978177997451\n",
      "Gradient Descent(49/49): loss=0.449997817799487\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4798796198719999\n",
      "Gradient Descent(2/49): loss=0.4679502464941094\n",
      "Gradient Descent(3/49): loss=0.46087732101835793\n",
      "Gradient Descent(4/49): loss=0.45668378350378436\n",
      "Gradient Descent(5/49): loss=0.45419743511139354\n",
      "Gradient Descent(6/49): loss=0.4527232791495454\n",
      "Gradient Descent(7/49): loss=0.45184925207976556\n",
      "Gradient Descent(8/49): loss=0.4513310414300929\n",
      "Gradient Descent(9/49): loss=0.45102379433590223\n",
      "Gradient Descent(10/49): loss=0.45084162753375645\n",
      "Gradient Descent(11/49): loss=0.4507336208367641\n",
      "Gradient Descent(12/49): loss=0.4506695836661175\n",
      "Gradient Descent(13/49): loss=0.450631616027641\n",
      "Gradient Descent(14/49): loss=0.45060910501478846\n",
      "Gradient Descent(15/49): loss=0.450595758235268\n",
      "Gradient Descent(16/49): loss=0.4505878449296904\n",
      "Gradient Descent(17/49): loss=0.45058315313081343\n",
      "Gradient Descent(18/49): loss=0.4505803713632593\n",
      "Gradient Descent(19/49): loss=0.45057872205327654\n",
      "Gradient Descent(20/49): loss=0.45057774417738755\n",
      "Gradient Descent(21/49): loss=0.4505771643947731\n",
      "Gradient Descent(22/49): loss=0.4505768206416609\n",
      "Gradient Descent(23/49): loss=0.4505766168304407\n",
      "Gradient Descent(24/49): loss=0.4505764959907685\n",
      "Gradient Descent(25/49): loss=0.45057642434492645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=0.450576381866107\n",
      "Gradient Descent(27/49): loss=0.4505763566804148\n",
      "Gradient Descent(28/49): loss=0.4505763417478179\n",
      "Gradient Descent(29/49): loss=0.4505763328942812\n",
      "Gradient Descent(30/49): loss=0.45057632764501915\n",
      "Gradient Descent(31/49): loss=0.4505763245327319\n",
      "Gradient Descent(32/49): loss=0.45057632268745684\n",
      "Gradient Descent(33/49): loss=0.4505763215933931\n",
      "Gradient Descent(34/49): loss=0.45057632094472266\n",
      "Gradient Descent(35/49): loss=0.45057632056012603\n",
      "Gradient Descent(36/49): loss=0.4505763203320988\n",
      "Gradient Descent(37/49): loss=0.4505763201969012\n",
      "Gradient Descent(38/49): loss=0.45057632011674265\n",
      "Gradient Descent(39/49): loss=0.45057632006921694\n",
      "Gradient Descent(40/49): loss=0.4505763200410388\n",
      "Gradient Descent(41/49): loss=0.4505763200243318\n",
      "Gradient Descent(42/49): loss=0.4505763200144263\n",
      "Gradient Descent(43/49): loss=0.45057632000855335\n",
      "Gradient Descent(44/49): loss=0.45057632000507114\n",
      "Gradient Descent(45/49): loss=0.45057632000300674\n",
      "Gradient Descent(46/49): loss=0.45057632000178294\n",
      "Gradient Descent(47/49): loss=0.450576320001057\n",
      "Gradient Descent(48/49): loss=0.45057632000062664\n",
      "Gradient Descent(49/49): loss=0.4505763200003715\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47971545630848006\n",
      "Gradient Descent(2/49): loss=0.46768875035377555\n",
      "Gradient Descent(3/49): loss=0.46055811639323546\n",
      "Gradient Descent(4/49): loss=0.4563303635180286\n",
      "Gradient Descent(5/49): loss=0.4538237288383185\n",
      "Gradient Descent(6/49): loss=0.45233754513671903\n",
      "Gradient Descent(7/49): loss=0.451456386820041\n",
      "Gradient Descent(8/49): loss=0.4509339480540824\n",
      "Gradient Descent(9/49): loss=0.4506241941097457\n",
      "Gradient Descent(10/49): loss=0.4504405409961479\n",
      "Gradient Descent(11/49): loss=0.4503316530650962\n",
      "Gradient Descent(12/49): loss=0.45026709341077553\n",
      "Gradient Descent(13/49): loss=0.4502288159917288\n",
      "Gradient Descent(14/49): loss=0.45020612130997595\n",
      "Gradient Descent(15/49): loss=0.4501926656331648\n",
      "Gradient Descent(16/49): loss=0.45018468776238335\n",
      "Gradient Descent(17/49): loss=0.4501799576827971\n",
      "Gradient Descent(18/49): loss=0.4501771532186105\n",
      "Gradient Descent(19/49): loss=0.45017549045179395\n",
      "Gradient Descent(20/49): loss=0.45017450459734887\n",
      "Gradient Descent(21/49): loss=0.45017392008424806\n",
      "Gradient Descent(22/49): loss=0.45017357352643056\n",
      "Gradient Descent(23/49): loss=0.4501733680523006\n",
      "Gradient Descent(24/49): loss=0.45017324622668925\n",
      "Gradient Descent(25/49): loss=0.45017317399628404\n",
      "Gradient Descent(26/49): loss=0.4501731311708767\n",
      "Gradient Descent(27/49): loss=0.45017310577969294\n",
      "Gradient Descent(28/49): loss=0.4501730907252599\n",
      "Gradient Descent(29/49): loss=0.4501730817994867\n",
      "Gradient Descent(30/49): loss=0.45017307650739546\n",
      "Gradient Descent(31/49): loss=0.45017307336971496\n",
      "Gradient Descent(32/49): loss=0.450173071509384\n",
      "Gradient Descent(33/49): loss=0.4501730704063937\n",
      "Gradient Descent(34/49): loss=0.45017306975243093\n",
      "Gradient Descent(35/49): loss=0.4501730693646962\n",
      "Gradient Descent(36/49): loss=0.4501730691348083\n",
      "Gradient Descent(37/49): loss=0.4501730689985079\n",
      "Gradient Descent(38/49): loss=0.45017306891769543\n",
      "Gradient Descent(39/49): loss=0.45017306886978153\n",
      "Gradient Descent(40/49): loss=0.4501730688413735\n",
      "Gradient Descent(41/49): loss=0.4501730688245304\n",
      "Gradient Descent(42/49): loss=0.45017306881454383\n",
      "Gradient Descent(43/49): loss=0.45017306880862307\n",
      "Gradient Descent(44/49): loss=0.4501730688051128\n",
      "Gradient Descent(45/49): loss=0.4501730688030312\n",
      "Gradient Descent(46/49): loss=0.45017306880179714\n",
      "Gradient Descent(47/49): loss=0.4501730688010655\n",
      "Gradient Descent(48/49): loss=0.4501730688006316\n",
      "Gradient Descent(49/49): loss=0.4501730688003745\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48243972046848016\n",
      "Gradient Descent(2/49): loss=0.47120114156830756\n",
      "Gradient Descent(3/49): loss=0.4640084510721957\n",
      "Gradient Descent(4/49): loss=0.45940512915468457\n",
      "Gradient Descent(5/49): loss=0.45645900312747745\n",
      "Gradient Descent(6/49): loss=0.45457348247006546\n",
      "Gradient Descent(7/49): loss=0.45336674924932174\n",
      "Gradient Descent(8/49): loss=0.45259443998804577\n",
      "Gradient Descent(9/49): loss=0.45210016206082926\n",
      "Gradient Descent(10/49): loss=0.45178382418741075\n",
      "Gradient Descent(11/49): loss=0.4515813679484231\n",
      "Gradient Descent(12/49): loss=0.4514517959554707\n",
      "Gradient Descent(13/49): loss=0.4513688698799811\n",
      "Gradient Descent(14/49): loss=0.4513157971916679\n",
      "Gradient Descent(15/49): loss=0.4512818306711476\n",
      "Gradient Descent(16/49): loss=0.45126009209801443\n",
      "Gradient Descent(17/49): loss=0.45124617941120937\n",
      "Gradient Descent(18/49): loss=0.45123727529165397\n",
      "Gradient Descent(19/49): loss=0.4512315766551385\n",
      "Gradient Descent(20/49): loss=0.45122792952776847\n",
      "Gradient Descent(21/49): loss=0.45122559536625195\n",
      "Gradient Descent(22/49): loss=0.4512241015028812\n",
      "Gradient Descent(23/49): loss=0.45122314543032405\n",
      "Gradient Descent(24/49): loss=0.4512225335438874\n",
      "Gradient Descent(25/49): loss=0.451222141936568\n",
      "Gradient Descent(26/49): loss=0.4512218913078834\n",
      "Gradient Descent(27/49): loss=0.4512217309055253\n",
      "Gradient Descent(28/49): loss=0.45122162824801626\n",
      "Gradient Descent(29/49): loss=0.4512215625472103\n",
      "Gradient Descent(30/49): loss=0.4512215204986947\n",
      "Gradient Descent(31/49): loss=0.4512214935876445\n",
      "Gradient Descent(32/49): loss=0.45122147636457244\n",
      "Gradient Descent(33/49): loss=0.4512214653418064\n",
      "Gradient Descent(34/49): loss=0.45122145828723603\n",
      "Gradient Descent(35/49): loss=0.45122145377231104\n",
      "Gradient Descent(36/49): loss=0.4512214508827591\n",
      "Gradient Descent(37/49): loss=0.4512214490334459\n",
      "Gradient Descent(38/49): loss=0.4512214478498854\n",
      "Gradient Descent(39/49): loss=0.4512214470924065\n",
      "Gradient Descent(40/49): loss=0.45122144660762015\n",
      "Gradient Descent(41/49): loss=0.45122144629735694\n",
      "Gradient Descent(42/49): loss=0.45122144609878834\n",
      "Gradient Descent(43/49): loss=0.4512214459717047\n",
      "Gradient Descent(44/49): loss=0.4512214458903709\n",
      "Gradient Descent(45/49): loss=0.4512214458383174\n",
      "Gradient Descent(46/49): loss=0.45122144580500334\n",
      "Gradient Descent(47/49): loss=0.45122144578368184\n",
      "Gradient Descent(48/49): loss=0.45122144577003653\n",
      "Gradient Descent(49/49): loss=0.45122144576130335\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48199921440768\n",
      "Gradient Descent(2/49): loss=0.4704787116285956\n",
      "Gradient Descent(3/49): loss=0.46310558984997907\n",
      "Gradient Descent(4/49): loss=0.45838679191166737\n",
      "Gradient Descent(5/49): loss=0.45536676123114683\n",
      "Gradient Descent(6/49): loss=0.4534339415956139\n",
      "Gradient Descent(7/49): loss=0.45219693702887276\n",
      "Gradient Descent(8/49): loss=0.45140525410615856\n",
      "Gradient Descent(9/49): loss=0.45089857703562153\n",
      "Gradient Descent(10/49): loss=0.4505743037104779\n",
      "Gradient Descent(11/49): loss=0.4503667687823857\n",
      "Gradient Descent(12/49): loss=0.4502339464284071\n",
      "Gradient Descent(13/49): loss=0.45014894012186046\n",
      "Gradient Descent(14/49): loss=0.4500945360856707\n",
      "Gradient Descent(15/49): loss=0.45005971750250917\n",
      "Gradient Descent(16/49): loss=0.45003743360928583\n",
      "Gradient Descent(17/49): loss=0.450023171917623\n",
      "Gradient Descent(18/49): loss=0.4500140444349587\n",
      "Gradient Descent(19/49): loss=0.4500082028460537\n",
      "Gradient Descent(20/49): loss=0.4500044642291543\n",
      "Gradient Descent(21/49): loss=0.4500020715143388\n",
      "Gradient Descent(22/49): loss=0.45000054017685687\n",
      "Gradient Descent(23/49): loss=0.4499995601208685\n",
      "Gradient Descent(24/49): loss=0.44999893288503573\n",
      "Gradient Descent(25/49): loss=0.44999853145410285\n",
      "Gradient Descent(26/49): loss=0.44999827453830565\n",
      "Gradient Descent(27/49): loss=0.4499981101121959\n",
      "Gradient Descent(28/49): loss=0.4499980048794851\n",
      "Gradient Descent(29/49): loss=0.4499979375305506\n",
      "Gradient Descent(30/49): loss=0.44999789442723254\n",
      "Gradient Descent(31/49): loss=0.4499978668411089\n",
      "Gradient Descent(32/49): loss=0.4499978491859896\n",
      "Gradient Descent(33/49): loss=0.44999783788671327\n",
      "Gradient Descent(34/49): loss=0.4499978306551765\n",
      "Gradient Descent(35/49): loss=0.4499978260269929\n",
      "Gradient Descent(36/49): loss=0.4499978230649554\n",
      "Gradient Descent(37/49): loss=0.4499978211692515\n",
      "Gradient Descent(38/49): loss=0.44999781995600097\n",
      "Gradient Descent(39/49): loss=0.44999781917952064\n",
      "Gradient Descent(40/49): loss=0.4499978186825732\n",
      "Gradient Descent(41/49): loss=0.4499978183645268\n",
      "Gradient Descent(42/49): loss=0.4499978181609772\n",
      "Gradient Descent(43/49): loss=0.4499978180307052\n",
      "Gradient Descent(44/49): loss=0.4499978179473313\n",
      "Gradient Descent(45/49): loss=0.44999781789397214\n",
      "Gradient Descent(46/49): loss=0.44999781785982224\n",
      "Gradient Descent(47/49): loss=0.4499978178379662\n",
      "Gradient Descent(48/49): loss=0.44999781782397846\n",
      "Gradient Descent(49/49): loss=0.4499978178150261\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48220747520000007\n",
      "Gradient Descent(2/49): loss=0.47082025932800164\n",
      "Gradient Descent(3/49): loss=0.4635324411699216\n",
      "Gradient Descent(4/49): loss=0.4588682375487495\n",
      "Gradient Descent(5/49): loss=0.4558831472311994\n",
      "Gradient Descent(6/49): loss=0.45397268942796803\n",
      "Gradient Descent(7/49): loss=0.4527499964338994\n",
      "Gradient Descent(8/49): loss=0.4519674729176957\n",
      "Gradient Descent(9/49): loss=0.45146665786732515\n",
      "Gradient Descent(10/49): loss=0.45114613623508826\n",
      "Gradient Descent(11/49): loss=0.4509410023904565\n",
      "Gradient Descent(12/49): loss=0.450809716729892\n",
      "Gradient Descent(13/49): loss=0.45072569390713096\n",
      "Gradient Descent(14/49): loss=0.4506719193005637\n",
      "Gradient Descent(15/49): loss=0.45063750355236093\n",
      "Gradient Descent(16/49): loss=0.45061547747351083\n",
      "Gradient Descent(17/49): loss=0.4506013807830471\n",
      "Gradient Descent(18/49): loss=0.45059235890115007\n",
      "Gradient Descent(19/49): loss=0.45058658489673614\n",
      "Gradient Descent(20/49): loss=0.45058288953391107\n",
      "Gradient Descent(21/49): loss=0.450580524501703\n",
      "Gradient Descent(22/49): loss=0.45057901088109015\n",
      "Gradient Descent(23/49): loss=0.45057804216389763\n",
      "Gradient Descent(24/49): loss=0.45057742218489466\n",
      "Gradient Descent(25/49): loss=0.4505770253983324\n",
      "Gradient Descent(26/49): loss=0.4505767714549327\n",
      "Gradient Descent(27/49): loss=0.45057660893115714\n",
      "Gradient Descent(28/49): loss=0.4505765049159406\n",
      "Gradient Descent(29/49): loss=0.4505764383462017\n",
      "Gradient Descent(30/49): loss=0.4505763957415693\n",
      "Gradient Descent(31/49): loss=0.45057636847460447\n",
      "Gradient Descent(32/49): loss=0.4505763510237467\n",
      "Gradient Descent(33/49): loss=0.45057633985519796\n",
      "Gradient Descent(34/49): loss=0.4505763327073267\n",
      "Gradient Descent(35/49): loss=0.4505763281326892\n",
      "Gradient Descent(36/49): loss=0.45057632520492114\n",
      "Gradient Descent(37/49): loss=0.4505763233311493\n",
      "Gradient Descent(38/49): loss=0.4505763221319356\n",
      "Gradient Descent(39/49): loss=0.4505763213644389\n",
      "Gradient Descent(40/49): loss=0.4505763208732408\n",
      "Gradient Descent(41/49): loss=0.4505763205588741\n",
      "Gradient Descent(42/49): loss=0.4505763203576795\n",
      "Gradient Descent(43/49): loss=0.45057632022891475\n",
      "Gradient Descent(44/49): loss=0.4505763201465054\n",
      "Gradient Descent(45/49): loss=0.4505763200937635\n",
      "Gradient Descent(46/49): loss=0.4505763200600087\n",
      "Gradient Descent(47/49): loss=0.4505763200384057\n",
      "Gradient Descent(48/49): loss=0.45057632002457954\n",
      "Gradient Descent(49/49): loss=0.4505763200157309\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48206230476799955\n",
      "Gradient Descent(2/49): loss=0.470582179819517\n",
      "Gradient Descent(3/49): loss=0.4632348998524918\n",
      "Gradient Descent(4/49): loss=0.4585326406735933\n",
      "Gradient Descent(5/49): loss=0.4555231947991003\n",
      "Gradient Descent(6/49): loss=0.4535971494394238\n",
      "Gradient Descent(7/49): loss=0.4523644804092311\n",
      "Gradient Descent(8/49): loss=0.4515755722299078\n",
      "Gradient Descent(9/49): loss=0.451070670995141\n",
      "Gradient Descent(10/49): loss=0.4507475342048901\n",
      "Gradient Descent(11/49): loss=0.45054072665912986\n",
      "Gradient Descent(12/49): loss=0.45040836982984306\n",
      "Gradient Descent(13/49): loss=0.4503236614590996\n",
      "Gradient Descent(14/49): loss=0.4502694481018238\n",
      "Gradient Descent(15/49): loss=0.4502347515531671\n",
      "Gradient Descent(16/49): loss=0.4502125457620271\n",
      "Gradient Descent(17/49): loss=0.45019833405569737\n",
      "Gradient Descent(18/49): loss=0.45018923856364623\n",
      "Gradient Descent(19/49): loss=0.45018341744873364\n",
      "Gradient Descent(20/49): loss=0.4501796919351895\n",
      "Gradient Descent(21/49): loss=0.4501773076065212\n",
      "Gradient Descent(22/49): loss=0.4501757816361738\n",
      "Gradient Descent(23/49): loss=0.450174805015151\n",
      "Gradient Descent(24/49): loss=0.4501741799776966\n",
      "Gradient Descent(25/49): loss=0.4501737799537258\n",
      "Gradient Descent(26/49): loss=0.4501735239383846\n",
      "Gradient Descent(27/49): loss=0.45017336008856645\n",
      "Gradient Descent(28/49): loss=0.4501732552246824\n",
      "Gradient Descent(29/49): loss=0.4501731881117968\n",
      "Gradient Descent(30/49): loss=0.4501731451595499\n",
      "Gradient Descent(31/49): loss=0.45017311767011176\n",
      "Gradient Descent(32/49): loss=0.45017310007687167\n",
      "Gradient Descent(33/49): loss=0.4501730888171979\n",
      "Gradient Descent(34/49): loss=0.45017308161100666\n",
      "Gradient Descent(35/49): loss=0.4501730769990442\n",
      "Gradient Descent(36/49): loss=0.4501730740473884\n",
      "Gradient Descent(37/49): loss=0.45017307215832864\n",
      "Gradient Descent(38/49): loss=0.45017307094933023\n",
      "Gradient Descent(39/49): loss=0.4501730701755715\n",
      "Gradient Descent(40/49): loss=0.45017306968036574\n",
      "Gradient Descent(41/49): loss=0.45017306936343404\n",
      "Gradient Descent(42/49): loss=0.4501730691605977\n",
      "Gradient Descent(43/49): loss=0.4501730690307826\n",
      "Gradient Descent(44/49): loss=0.45017306894770087\n",
      "Gradient Descent(45/49): loss=0.4501730688945284\n",
      "Gradient Descent(46/49): loss=0.4501730688604981\n",
      "Gradient Descent(47/49): loss=0.45017306883871905\n",
      "Gradient Descent(48/49): loss=0.4501730688247802\n",
      "Gradient Descent(49/49): loss=0.45017306881585933\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48482499177151134\n",
      "Gradient Descent(2/49): loss=0.4743709286029085\n",
      "Gradient Descent(3/49): loss=0.46716912448605447\n",
      "Gradient Descent(4/49): loss=0.4622078016299542\n",
      "Gradient Descent(5/49): loss=0.458789946314386\n",
      "Gradient Descent(6/49): loss=0.4564353857874922\n",
      "Gradient Descent(7/49): loss=0.4548133290405151\n",
      "Gradient Descent(8/49): loss=0.4536958941475226\n",
      "Gradient Descent(9/49): loss=0.4529260932497396\n",
      "Gradient Descent(10/49): loss=0.45239577741125725\n",
      "Gradient Descent(11/49): loss=0.45203044283012656\n",
      "Gradient Descent(12/49): loss=0.4517787638371856\n",
      "Gradient Descent(13/49): loss=0.45160538217894886\n",
      "Gradient Descent(14/49): loss=0.45148593955458916\n",
      "Gradient Descent(15/49): loss=0.45140365553066797\n",
      "Gradient Descent(16/49): loss=0.4513469700665886\n",
      "Gradient Descent(17/49): loss=0.4513079194503845\n",
      "Gradient Descent(18/49): loss=0.4512810174808813\n",
      "Gradient Descent(19/49): loss=0.45126248471409053\n",
      "Gradient Descent(20/49): loss=0.45124971749104853\n",
      "Gradient Descent(21/49): loss=0.4512409221510946\n",
      "Gradient Descent(22/49): loss=0.4512348630414006\n",
      "Gradient Descent(23/49): loss=0.4512306889207322\n",
      "Gradient Descent(24/49): loss=0.4512278133690041\n",
      "Gradient Descent(25/49): loss=0.45122583240141845\n",
      "Gradient Descent(26/49): loss=0.4512244677128485\n",
      "Gradient Descent(27/49): loss=0.4512235275788929\n",
      "Gradient Descent(28/49): loss=0.45122287992061083\n",
      "Gradient Descent(29/49): loss=0.4512224337488201\n",
      "Gradient Descent(30/49): loss=0.4512221263810737\n",
      "Gradient Descent(31/49): loss=0.4512219146354331\n",
      "Gradient Descent(32/49): loss=0.45122176876386144\n",
      "Gradient Descent(33/49): loss=0.4512216682729357\n",
      "Gradient Descent(34/49): loss=0.4512215990447367\n",
      "Gradient Descent(35/49): loss=0.45122155135343084\n",
      "Gradient Descent(36/49): loss=0.45122151849889\n",
      "Gradient Descent(37/49): loss=0.45122149586539656\n",
      "Gradient Descent(38/49): loss=0.45122148027318326\n",
      "Gradient Descent(39/49): loss=0.4512214695317074\n",
      "Gradient Descent(40/49): loss=0.4512214621319047\n",
      "Gradient Descent(41/49): loss=0.4512214570341807\n",
      "Gradient Descent(42/49): loss=0.45122145352235854\n",
      "Gradient Descent(43/49): loss=0.4512214511030642\n",
      "Gradient Descent(44/49): loss=0.4512214494364124\n",
      "Gradient Descent(45/49): loss=0.45122144828825617\n",
      "Gradient Descent(46/49): loss=0.45122144749729093\n",
      "Gradient Descent(47/49): loss=0.45122144695239497\n",
      "Gradient Descent(48/49): loss=0.45122144657701657\n",
      "Gradient Descent(49/49): loss=0.45122144631841815\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.4844443211173034\n",
      "Gradient Descent(2/49): loss=0.47372801393501646\n",
      "Gradient Descent(3/49): loss=0.46634554991713517\n",
      "Gradient Descent(4/49): loss=0.46125977045521666\n",
      "Gradient Descent(5/49): loss=0.457756176983902\n",
      "Gradient Descent(6/49): loss=0.455342551441513\n",
      "Gradient Descent(7/49): loss=0.4536798048053621\n",
      "Gradient Descent(8/49): loss=0.45253433864771747\n",
      "Gradient Descent(9/49): loss=0.451745227011716\n",
      "Gradient Descent(10/49): loss=0.45120160800567466\n",
      "Gradient Descent(11/49): loss=0.45082710887241284\n",
      "Gradient Descent(12/49): loss=0.45056911641950864\n",
      "Gradient Descent(13/49): loss=0.4503913854187031\n",
      "Gradient Descent(14/49): loss=0.45026894653224814\n",
      "Gradient Descent(15/49): loss=0.45018459838336883\n",
      "Gradient Descent(16/49): loss=0.45012649094360657\n",
      "Gradient Descent(17/49): loss=0.4500864607283539\n",
      "Gradient Descent(18/49): loss=0.4500588839130663\n",
      "Gradient Descent(19/49): loss=0.450039886245015\n",
      "Gradient Descent(20/49): loss=0.4500267987514943\n",
      "Gradient Descent(21/49): loss=0.45001778277720783\n",
      "Gradient Descent(22/49): loss=0.4500115716725219\n",
      "Gradient Descent(23/49): loss=0.4500072928425038\n",
      "Gradient Descent(24/49): loss=0.45000434515650445\n",
      "Gradient Descent(25/49): loss=0.4500023144956192\n",
      "Gradient Descent(26/49): loss=0.45000091557333555\n",
      "Gradient Descent(27/49): loss=0.4499999518557745\n",
      "Gradient Descent(28/49): loss=0.4499992879507464\n",
      "Gradient Descent(29/49): loss=0.44999883058657264\n",
      "Gradient Descent(30/49): loss=0.44999851550839354\n",
      "Gradient Descent(31/49): loss=0.44999829845103567\n",
      "Gradient Descent(32/49): loss=0.4499981489202218\n",
      "Gradient Descent(33/49): loss=0.4499980459084445\n",
      "Gradient Descent(34/49): loss=0.4499979749436308\n",
      "Gradient Descent(35/49): loss=0.4499979260559708\n",
      "Gradient Descent(36/49): loss=0.4499978923772617\n",
      "Gradient Descent(37/49): loss=0.44999786917599927\n",
      "Gradient Descent(38/49): loss=0.44999785319264946\n",
      "Gradient Descent(39/49): loss=0.4499978421817196\n",
      "Gradient Descent(40/49): loss=0.4499978345962901\n",
      "Gradient Descent(41/49): loss=0.4499978293706877\n",
      "Gradient Descent(42/49): loss=0.4499978257707703\n",
      "Gradient Descent(43/49): loss=0.449997823290787\n",
      "Gradient Descent(44/49): loss=0.4499978215823265\n",
      "Gradient Descent(45/49): loss=0.44999782040536856\n",
      "Gradient Descent(46/49): loss=0.4499978195945619\n",
      "Gradient Descent(47/49): loss=0.44999781903599717\n",
      "Gradient Descent(48/49): loss=0.4499978186512019\n",
      "Gradient Descent(49/49): loss=0.44999781838611635\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4846242931520001\n",
      "Gradient Descent(2/49): loss=0.4740319687044149\n",
      "Gradient Descent(3/49): loss=0.46673491639247155\n",
      "Gradient Descent(4/49): loss=0.4617079770547729\n",
      "Gradient Descent(5/49): loss=0.45824491854503385\n",
      "Gradient Descent(6/49): loss=0.45585921753767306\n",
      "Gradient Descent(7/49): loss=0.4542157081137033\n",
      "Gradient Descent(8/49): loss=0.45308349447153035\n",
      "Gradient Descent(9/49): loss=0.45230351249343737\n",
      "Gradient Descent(10/49): loss=0.4517661829087288\n",
      "Gradient Descent(11/49): loss=0.451396016557823\n",
      "Gradient Descent(12/49): loss=0.45114100895868425\n",
      "Gradient Descent(13/49): loss=0.4509653342236376\n",
      "Gradient Descent(14/49): loss=0.4508443118986641\n",
      "Gradient Descent(15/49): loss=0.45076093961898983\n",
      "Gradient Descent(16/49): loss=0.450703504455522\n",
      "Gradient Descent(17/49): loss=0.4506639373714092\n",
      "Gradient Descent(18/49): loss=0.4506366796071636\n",
      "Gradient Descent(19/49): loss=0.450617901733375\n",
      "Gradient Descent(20/49): loss=0.45060496565612196\n",
      "Gradient Descent(21/49): loss=0.45059605399250263\n",
      "Gradient Descent(22/49): loss=0.4505899147474349\n",
      "Gradient Descent(23/49): loss=0.45058568542150806\n",
      "Gradient Descent(24/49): loss=0.45058277183887674\n",
      "Gradient Descent(25/49): loss=0.4505807646718022\n",
      "Gradient Descent(26/49): loss=0.45057938193440455\n",
      "Gradient Descent(27/49): loss=0.45057842936661135\n",
      "Gradient Descent(28/49): loss=0.4505777731426586\n",
      "Gradient Descent(29/49): loss=0.45057732106997755\n",
      "Gradient Descent(30/49): loss=0.45057700963710745\n",
      "Gradient Descent(31/49): loss=0.4505767950910032\n",
      "Gradient Descent(32/49): loss=0.45057664729019214\n",
      "Gradient Descent(33/49): loss=0.45057654547021353\n",
      "Gradient Descent(34/49): loss=0.4505764753264298\n",
      "Gradient Descent(35/49): loss=0.45057642700437733\n",
      "Gradient Descent(36/49): loss=0.45057639371531605\n",
      "Gradient Descent(37/49): loss=0.450576370782481\n",
      "Gradient Descent(38/49): loss=0.4505763549840511\n",
      "Gradient Descent(39/49): loss=0.4505763441005129\n",
      "Gradient Descent(40/49): loss=0.4505763366028432\n",
      "Gradient Descent(41/49): loss=0.4505763314376987\n",
      "Gradient Descent(42/49): loss=0.45057632787943075\n",
      "Gradient Descent(43/49): loss=0.45057632542813975\n",
      "Gradient Descent(44/49): loss=0.45057632373944556\n",
      "Gradient Descent(45/49): loss=0.450576322576104\n",
      "Gradient Descent(46/49): loss=0.45057632177467816\n",
      "Gradient Descent(47/49): loss=0.4505763212225757\n",
      "Gradient Descent(48/49): loss=0.4505763208422325\n",
      "Gradient Descent(49/49): loss=0.45057632058021396\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48449884170368\n",
      "Gradient Descent(2/49): loss=0.47382009375334455\n",
      "Gradient Descent(3/49): loss=0.46646350429035865\n",
      "Gradient Descent(4/49): loss=0.4613955498093082\n",
      "Gradient Descent(5/49): loss=0.45790423596731317\n",
      "Gradient Descent(6/49): loss=0.4554990698615617\n",
      "Gradient Descent(7/49): loss=0.45384215093131014\n",
      "Gradient Descent(8/49): loss=0.4527006994802595\n",
      "Gradient Descent(9/49): loss=0.45191435357563087\n",
      "Gradient Descent(10/49): loss=0.45137263988193216\n",
      "Gradient Descent(11/49): loss=0.4509994533183431\n",
      "Gradient Descent(12/49): loss=0.4507423650946867\n",
      "Gradient Descent(13/49): loss=0.4505652570174095\n",
      "Gradient Descent(14/49): loss=0.45044324726297347\n",
      "Gradient Descent(15/49): loss=0.4503591947431423\n",
      "Gradient Descent(16/49): loss=0.45030129096223087\n",
      "Gradient Descent(17/49): loss=0.4502614010475608\n",
      "Gradient Descent(18/49): loss=0.45023392088534475\n",
      "Gradient Descent(19/49): loss=0.4502149898015938\n",
      "Gradient Descent(20/49): loss=0.4502019481779982\n",
      "Gradient Descent(21/49): loss=0.450192963803503\n",
      "Gradient Descent(22/49): loss=0.45018677446791305\n",
      "Gradient Descent(23/49): loss=0.45018251063462517\n",
      "Gradient Descent(24/49): loss=0.4501795732798734\n",
      "Gradient Descent(25/49): loss=0.45017754973618485\n",
      "Gradient Descent(26/49): loss=0.45017615571693775\n",
      "Gradient Descent(27/49): loss=0.45017519537707823\n",
      "Gradient Descent(28/49): loss=0.45017453379894923\n",
      "Gradient Descent(29/49): loss=0.45017407803777604\n",
      "Gradient Descent(30/49): loss=0.45017376406390397\n",
      "Gradient Descent(31/49): loss=0.4501735477673035\n",
      "Gradient Descent(32/49): loss=0.4501733987605755\n",
      "Gradient Descent(33/49): loss=0.4501732961098405\n",
      "Gradient Descent(34/49): loss=0.4501732253937488\n",
      "Gradient Descent(35/49): loss=0.4501731766774338\n",
      "Gradient Descent(36/49): loss=0.45017314311676404\n",
      "Gradient Descent(37/49): loss=0.4501731199968187\n",
      "Gradient Descent(38/49): loss=0.45017310406948835\n",
      "Gradient Descent(39/49): loss=0.45017309309715053\n",
      "Gradient Descent(40/49): loss=0.45017308553830715\n",
      "Gradient Descent(41/49): loss=0.4501730803310195\n",
      "Gradient Descent(42/49): loss=0.4501730767437195\n",
      "Gradient Descent(43/49): loss=0.4501730742724285\n",
      "Gradient Descent(44/49): loss=0.4501730725699558\n",
      "Gradient Descent(45/49): loss=0.45017307139712276\n",
      "Gradient Descent(46/49): loss=0.45017307058915784\n",
      "Gradient Descent(47/49): loss=0.4501730700325508\n",
      "Gradient Descent(48/49): loss=0.45017306964910425\n",
      "Gradient Descent(49/49): loss=0.45017306938494783\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4872980644722006\n",
      "Gradient Descent(2/49): loss=0.4779037129558376\n",
      "Gradient Descent(3/49): loss=0.47095565057434025\n",
      "Gradient Descent(4/49): loss=0.46581686363698405\n",
      "Gradient Descent(5/49): loss=0.46201621681811234\n",
      "Gradient Descent(6/49): loss=0.45920525843087706\n",
      "Gradient Descent(7/49): loss=0.4571262736076764\n",
      "Gradient Descent(8/49): loss=0.45558865643243834\n",
      "Gradient Descent(9/49): loss=0.45445143476963223\n",
      "Gradient Descent(10/49): loss=0.4536103456278205\n",
      "Gradient Descent(11/49): loss=0.45298827609853654\n",
      "Gradient Descent(12/49): loss=0.45252819347467815\n",
      "Gradient Descent(13/49): loss=0.45218791636607253\n",
      "Gradient Descent(14/49): loss=0.45193624741654814\n",
      "Gradient Descent(15/49): loss=0.4517501130614794\n",
      "Gradient Descent(16/49): loss=0.4516124480924708\n",
      "Gradient Descent(17/49): loss=0.4515106310813917\n",
      "Gradient Descent(18/49): loss=0.45143532721999785\n",
      "Gradient Descent(19/49): loss=0.45137963248411084\n",
      "Gradient Descent(20/49): loss=0.4513384406574491\n",
      "Gradient Descent(21/49): loss=0.45130797518244975\n",
      "Gradient Descent(22/49): loss=0.45128544291714046\n",
      "Gradient Descent(23/49): loss=0.4512687780537175\n",
      "Gradient Descent(24/49): loss=0.45125645272073023\n",
      "Gradient Descent(25/49): loss=0.45124733690445257\n",
      "Gradient Descent(26/49): loss=0.45124059484673346\n",
      "Gradient Descent(27/49): loss=0.45123560842084465\n",
      "Gradient Descent(28/49): loss=0.4512319204602573\n",
      "Gradient Descent(29/49): loss=0.4512291928446068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=0.4512271755000717\n",
      "Gradient Descent(31/49): loss=0.45122568347205344\n",
      "Gradient Descent(32/49): loss=0.45122457996813137\n",
      "Gradient Descent(33/49): loss=0.4512237638166305\n",
      "Gradient Descent(34/49): loss=0.4512231601909804\n",
      "Gradient Descent(35/49): loss=0.4512227137494497\n",
      "Gradient Descent(36/49): loss=0.45122238356129346\n",
      "Gradient Descent(37/49): loss=0.4512221393541333\n",
      "Gradient Descent(38/49): loss=0.4512219587385176\n",
      "Gradient Descent(39/49): loss=0.4512218251552082\n",
      "Gradient Descent(40/49): loss=0.45122172635699237\n",
      "Gradient Descent(41/49): loss=0.4512216532858323\n",
      "Gradient Descent(42/49): loss=0.45122159924240207\n",
      "Gradient Descent(43/49): loss=0.45122155927188107\n",
      "Gradient Descent(44/49): loss=0.4512215297096838\n",
      "Gradient Descent(45/49): loss=0.4512215078454827\n",
      "Gradient Descent(46/49): loss=0.45122149167471953\n",
      "Gradient Descent(47/49): loss=0.4512214797148232\n",
      "Gradient Descent(48/49): loss=0.4512214708692836\n",
      "Gradient Descent(49/49): loss=0.4512214643271227\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48697943175488834\n",
      "Gradient Descent(2/49): loss=0.4773494194808062\n",
      "Gradient Descent(3/49): loss=0.4702270624028942\n",
      "Gradient Descent(4/49): loss=0.46495936710806846\n",
      "Gradient Descent(5/49): loss=0.4610633796680157\n",
      "Gradient Descent(6/49): loss=0.4581819073573541\n",
      "Gradient Descent(7/49): loss=0.45605077043638725\n",
      "Gradient Descent(8/49): loss=0.45447458156964116\n",
      "Gradient Descent(9/49): loss=0.45330883228379526\n",
      "Gradient Descent(10/49): loss=0.4524466441119837\n",
      "Gradient Descent(11/49): loss=0.45180896974011187\n",
      "Gradient Descent(12/49): loss=0.45133734577467494\n",
      "Gradient Descent(13/49): loss=0.4509885326898382\n",
      "Gradient Descent(14/49): loss=0.45073055053229283\n",
      "Gradient Descent(15/49): loss=0.4505397469285721\n",
      "Gradient Descent(16/49): loss=0.45039862858326063\n",
      "Gradient Descent(17/49): loss=0.45029425745506807\n",
      "Gradient Descent(18/49): loss=0.45021706456865696\n",
      "Gradient Descent(19/49): loss=0.4501599727098671\n",
      "Gradient Descent(20/49): loss=0.45011774757110623\n",
      "Gradient Descent(21/49): loss=0.4500865178584788\n",
      "Gradient Descent(22/49): loss=0.4500634203630194\n",
      "Gradient Descent(23/49): loss=0.4500463374553776\n",
      "Gradient Descent(24/49): loss=0.45003370293688605\n",
      "Gradient Descent(25/49): loss=0.4500243584470092\n",
      "Gradient Descent(26/49): loss=0.45001744726229675\n",
      "Gradient Descent(27/49): loss=0.45001233575008315\n",
      "Gradient Descent(28/49): loss=0.4500085552756499\n",
      "Gradient Descent(29/49): loss=0.4500057592367592\n",
      "Gradient Descent(30/49): loss=0.4500036912863956\n",
      "Gradient Descent(31/49): loss=0.45000216183030667\n",
      "Gradient Descent(32/49): loss=0.4500010306445833\n",
      "Gradient Descent(33/49): loss=0.45000019401962255\n",
      "Gradient Descent(34/49): loss=0.4499995752518012\n",
      "Gradient Descent(35/49): loss=0.449999117611121\n",
      "Gradient Descent(36/49): loss=0.4499987791400736\n",
      "Gradient Descent(37/49): loss=0.4499985288068868\n",
      "Gradient Descent(38/49): loss=0.44999834366046204\n",
      "Gradient Descent(39/49): loss=0.4499982067261664\n",
      "Gradient Descent(40/49): loss=0.44999810544956104\n",
      "Gradient Descent(41/49): loss=0.44999803054538384\n",
      "Gradient Descent(42/49): loss=0.4499979751462545\n",
      "Gradient Descent(43/49): loss=0.4499979341730585\n",
      "Gradient Descent(44/49): loss=0.44999790386928246\n",
      "Gradient Descent(45/49): loss=0.4499978814566099\n",
      "Gradient Descent(46/49): loss=0.44999786488019705\n",
      "Gradient Descent(47/49): loss=0.4499978526202825\n",
      "Gradient Descent(48/49): loss=0.44999784355284916\n",
      "Gradient Descent(49/49): loss=0.449997836846576\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48713007372800005\n",
      "Gradient Descent(2/49): loss=0.47761147625723255\n",
      "Gradient Descent(3/49): loss=0.47057152156785054\n",
      "Gradient Descent(4/49): loss=0.4653647710795832\n",
      "Gradient Descent(5/49): loss=0.4615138584184597\n",
      "Gradient Descent(6/49): loss=0.45866572341429235\n",
      "Gradient Descent(7/49): loss=0.45655924276520965\n",
      "Gradient Descent(8/49): loss=0.45500128967714876\n",
      "Gradient Descent(9/49): loss=0.4538490275732189\n",
      "Gradient Descent(10/49): loss=0.452996814521153\n",
      "Gradient Descent(11/49): loss=0.45236651774784487\n",
      "Gradient Descent(12/49): loss=0.4519003502543062\n",
      "Gradient Descent(13/49): loss=0.4515555727760848\n",
      "Gradient Descent(14/49): loss=0.45130057535319257\n",
      "Gradient Descent(15/49): loss=0.45111197925922114\n",
      "Gradient Descent(16/49): loss=0.4509724935881199\n",
      "Gradient Descent(17/49): loss=0.45086932998577345\n",
      "Gradient Descent(18/49): loss=0.4507930301854783\n",
      "Gradient Descent(19/49): loss=0.45073659885317946\n",
      "Gradient Descent(20/49): loss=0.4506948622398114\n",
      "Gradient Descent(21/49): loss=0.45066399384056466\n",
      "Gradient Descent(22/49): loss=0.45064116357248146\n",
      "Gradient Descent(23/49): loss=0.4506242783062074\n",
      "Gradient Descent(24/49): loss=0.45061178996327095\n",
      "Gradient Descent(25/49): loss=0.4506025535848354\n",
      "Gradient Descent(26/49): loss=0.45059572235934414\n",
      "Gradient Descent(27/49): loss=0.450590669984971\n",
      "Gradient Descent(28/49): loss=0.45058693324888444\n",
      "Gradient Descent(29/49): loss=0.4505841695588751\n",
      "Gradient Descent(30/49): loss=0.4505821255337438\n",
      "Gradient Descent(31/49): loss=0.45058061377275704\n",
      "Gradient Descent(32/49): loss=0.45057949567433114\n",
      "Gradient Descent(33/49): loss=0.45057866872873514\n",
      "Gradient Descent(34/49): loss=0.4505780571197726\n",
      "Gradient Descent(35/49): loss=0.45057760477378384\n",
      "Gradient Descent(36/49): loss=0.45057727021869054\n",
      "Gradient Descent(37/49): loss=0.45057702278174344\n",
      "Gradient Descent(38/49): loss=0.4505768397773776\n",
      "Gradient Descent(39/49): loss=0.4505767044273484\n",
      "Gradient Descent(40/49): loss=0.4505766043224668\n",
      "Gradient Descent(41/49): loss=0.45057653028489647\n",
      "Gradient Descent(42/49): loss=0.4505764755267095\n",
      "Gradient Descent(43/49): loss=0.45057643502755423\n",
      "Gradient Descent(44/49): loss=0.45057640507437907\n",
      "Gradient Descent(45/49): loss=0.45057638292101054\n",
      "Gradient Descent(46/49): loss=0.4505763665363796\n",
      "Gradient Descent(47/49): loss=0.45057635441830646\n",
      "Gradient Descent(48/49): loss=0.45057634545577946\n",
      "Gradient Descent(49/49): loss=0.45057633882709436\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4870250671155198\n",
      "Gradient Descent(2/49): loss=0.4774288067541569\n",
      "Gradient Descent(3/49): loss=0.47033141259089345\n",
      "Gradient Descent(4/49): loss=0.46508217986774353\n",
      "Gradient Descent(5/49): loss=0.46119984734570213\n",
      "Gradient Descent(6/49): loss=0.4583284742124003\n",
      "Gradient Descent(7/49): loss=0.4562048066430107\n",
      "Gradient Descent(8/49): loss=0.454634142108691\n",
      "Gradient Descent(9/49): loss=0.4534724786191081\n",
      "Gradient Descent(10/49): loss=0.45261331230221263\n",
      "Gradient Descent(11/49): loss=0.4519778728942362\n",
      "Gradient Descent(12/49): loss=0.4515079019080971\n",
      "Gradient Descent(13/49): loss=0.4511603113667486\n",
      "Gradient Descent(14/49): loss=0.4509032334023673\n",
      "Gradient Descent(15/49): loss=0.450713098539911\n",
      "Gradient Descent(16/49): loss=0.4505724747956382\n",
      "Gradient Descent(17/49): loss=0.45046846947437386\n",
      "Gradient Descent(18/49): loss=0.45039154713876706\n",
      "Gradient Descent(19/49): loss=0.45033465537935213\n",
      "Gradient Descent(20/49): loss=0.4502925782340887\n",
      "Gradient Descent(21/49): loss=0.450261457977452\n",
      "Gradient Descent(22/49): loss=0.45023844143564345\n",
      "Gradient Descent(23/49): loss=0.45022141840132185\n",
      "Gradient Descent(24/49): loss=0.4502088281651377\n",
      "Gradient Descent(25/49): loss=0.45019951642645584\n",
      "Gradient Descent(26/49): loss=0.45019262946452687\n",
      "Gradient Descent(27/49): loss=0.450187535867484\n",
      "Gradient Descent(28/49): loss=0.45018376864311116\n",
      "Gradient Descent(29/49): loss=0.4501809824039652\n",
      "Gradient Descent(30/49): loss=0.4501789217014926\n",
      "Gradient Descent(31/49): loss=0.45017739760594383\n",
      "Gradient Descent(32/49): loss=0.45017627038487595\n",
      "Gradient Descent(33/49): loss=0.45017543669217447\n",
      "Gradient Descent(34/49): loss=0.4501748200930522\n",
      "Gradient Descent(35/49): loss=0.45017436405634126\n",
      "Gradient Descent(36/49): loss=0.4501740267715899\n",
      "Gradient Descent(37/49): loss=0.4501737773157881\n",
      "Gradient Descent(38/49): loss=0.4501735928182769\n",
      "Gradient Descent(39/49): loss=0.4501734563639177\n",
      "Gradient Descent(40/49): loss=0.4501733554422733\n",
      "Gradient Descent(41/49): loss=0.45017328080062535\n",
      "Gradient Descent(42/49): loss=0.45017322559566253\n",
      "Gradient Descent(43/49): loss=0.450173184766072\n",
      "Gradient Descent(44/49): loss=0.4501731545685068\n",
      "Gradient Descent(45/49): loss=0.45017313223438765\n",
      "Gradient Descent(46/49): loss=0.4501731157160732\n",
      "Gradient Descent(47/49): loss=0.45017310349912776\n",
      "Gradient Descent(48/49): loss=0.45017309446347475\n",
      "Gradient Descent(49/49): loss=0.450173087780706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48985893857054724\n",
      "Gradient Descent(2/49): loss=0.4818262038122743\n",
      "Gradient Descent(3/49): loss=0.47546347461024663\n",
      "Gradient Descent(4/49): loss=0.47042355680932646\n",
      "Gradient Descent(5/49): loss=0.46643143791921443\n",
      "Gradient Descent(6/49): loss=0.4632692805463586\n",
      "Gradient Descent(7/49): loss=0.4607645356913188\n",
      "Gradient Descent(8/49): loss=0.4587805272916401\n",
      "Gradient Descent(9/49): loss=0.4572089942382552\n",
      "Gradient Descent(10/49): loss=0.45596418290666857\n",
      "Gradient Descent(11/49): loss=0.4549781678509193\n",
      "Gradient Descent(12/49): loss=0.45419714532526023\n",
      "Gradient Descent(13/49): loss=0.453578497382686\n",
      "Gradient Descent(14/49): loss=0.4530884663473727\n",
      "Gradient Descent(15/49): loss=0.4527003127643011\n",
      "Gradient Descent(16/49): loss=0.4523928563111501\n",
      "Gradient Descent(17/49): loss=0.45214932005460934\n",
      "Gradient Descent(18/49): loss=0.45195641498580336\n",
      "Gradient Descent(19/49): loss=0.4518036148808022\n",
      "Gradient Descent(20/49): loss=0.4516825819176306\n",
      "Gradient Descent(21/49): loss=0.45158671170750236\n",
      "Gradient Descent(22/49): loss=0.45151077291405994\n",
      "Gradient Descent(23/49): loss=0.45145062179577383\n",
      "Gradient Descent(24/49): loss=0.45140297609497954\n",
      "Gradient Descent(25/49): loss=0.45136523593538075\n",
      "Gradient Descent(26/49): loss=0.4513353419549623\n",
      "Gradient Descent(27/49): loss=0.4513116629330727\n",
      "Gradient Descent(28/49): loss=0.4512929067798341\n",
      "Gradient Descent(29/49): loss=0.4512780500308538\n",
      "Gradient Descent(30/49): loss=0.4512662819999865\n",
      "Gradient Descent(31/49): loss=0.45125696054273656\n",
      "Gradient Descent(32/49): loss=0.45124957701644886\n",
      "Gradient Descent(33/49): loss=0.45124372852527617\n",
      "Gradient Descent(34/49): loss=0.4512390959354185\n",
      "Gradient Descent(35/49): loss=0.45123542646099224\n",
      "Gradient Descent(36/49): loss=0.45123251987029916\n",
      "Gradient Descent(37/49): loss=0.4512302175598115\n",
      "Gradient Descent(38/49): loss=0.4512283938996736\n",
      "Gradient Descent(39/49): loss=0.4512269493784786\n",
      "Gradient Descent(40/49): loss=0.45122580517324007\n",
      "Gradient Descent(41/49): loss=0.4512248988482706\n",
      "Gradient Descent(42/49): loss=0.45122418094826255\n",
      "Gradient Descent(43/49): loss=0.4512236122996658\n",
      "Gradient Descent(44/49): loss=0.4512231618731127\n",
      "Gradient Descent(45/49): loss=0.45122280509023976\n",
      "Gradient Descent(46/49): loss=0.45122252248252603\n",
      "Gradient Descent(47/49): loss=0.45122229862895574\n",
      "Gradient Descent(48/49): loss=0.4512221213145432\n",
      "Gradient Descent(49/49): loss=0.4512219808637968\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48960454632043526\n",
      "Gradient Descent(2/49): loss=0.48137030746084825\n",
      "Gradient Descent(3/49): loss=0.4748479668601715\n",
      "Gradient Descent(4/49): loss=0.469681620870378\n",
      "Gradient Descent(5/49): loss=0.46558935821186115\n",
      "Gradient Descent(6/49): loss=0.4623478769600499\n",
      "Gradient Descent(7/49): loss=0.4597802996604914\n",
      "Gradient Descent(8/49): loss=0.4577465216815099\n",
      "Gradient Descent(9/49): loss=0.4561355661443597\n",
      "Gradient Descent(10/49): loss=0.45485952826338205\n",
      "Gradient Descent(11/49): loss=0.45384877865786005\n",
      "Gradient Descent(12/49): loss=0.45304816389532615\n",
      "Gradient Descent(13/49): loss=0.45241399694192286\n",
      "Gradient Descent(14/49): loss=0.4519116732981321\n",
      "Gradient Descent(15/49): loss=0.4515137827398856\n",
      "Gradient Descent(16/49): loss=0.45119861362869884\n",
      "Gradient Descent(17/49): loss=0.4509489681757274\n",
      "Gradient Descent(18/49): loss=0.4507512240124287\n",
      "Gradient Descent(19/49): loss=0.45059459086068016\n",
      "Gradient Descent(20/49): loss=0.4504705217411797\n",
      "Gradient Descent(21/49): loss=0.4503722465916239\n",
      "Gradient Descent(22/49): loss=0.45029440284566047\n",
      "Gradient Descent(23/49): loss=0.4502327428144829\n",
      "Gradient Descent(24/49): loss=0.450183901903787\n",
      "Gradient Descent(25/49): loss=0.4501452150184251\n",
      "Gradient Descent(26/49): loss=0.4501145711365298\n",
      "Gradient Descent(27/49): loss=0.45009029811768053\n",
      "Gradient Descent(28/49): loss=0.4500710714594495\n",
      "Gradient Descent(29/49): loss=0.4500558420234654\n",
      "Gradient Descent(30/49): loss=0.45004377878722224\n",
      "Gradient Descent(31/49): loss=0.45003422349779393\n",
      "Gradient Descent(32/49): loss=0.4500266547530379\n",
      "Gradient Descent(33/49): loss=0.4500206595503162\n",
      "Gradient Descent(34/49): loss=0.4500159107502408\n",
      "Gradient Descent(35/49): loss=0.4500121492257007\n",
      "Gradient Descent(36/49): loss=0.4500091697221129\n",
      "Gradient Descent(37/49): loss=0.45000680965732076\n",
      "Gradient Descent(38/49): loss=0.45000494024999904\n",
      "Gradient Descent(39/49): loss=0.4500034594924595\n",
      "Gradient Descent(40/49): loss=0.4500022865844122\n",
      "Gradient Descent(41/49): loss=0.45000135752394793\n",
      "Gradient Descent(42/49): loss=0.45000062161515475\n",
      "Gradient Descent(43/49): loss=0.45000003870179933\n",
      "Gradient Descent(44/49): loss=0.44999957697613036\n",
      "Gradient Descent(45/49): loss=0.449999211243228\n",
      "Gradient Descent(46/49): loss=0.44999892154619603\n",
      "Gradient Descent(47/49): loss=0.44999869207717696\n",
      "Gradient Descent(48/49): loss=0.4499985103147671\n",
      "Gradient Descent(49/49): loss=0.44999836634076246\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.489724816928\n",
      "Gradient Descent(2/49): loss=0.4815858444166704\n",
      "Gradient Descent(3/49): loss=0.4751389642904472\n",
      "Gradient Descent(4/49): loss=0.4700323905424647\n",
      "Gradient Descent(5/49): loss=0.46598747347668606\n",
      "Gradient Descent(6/49): loss=0.4627834946688824\n",
      "Gradient Descent(7/49): loss=0.4602456230552226\n",
      "Gradient Descent(8/49): loss=0.45823537495004135\n",
      "Gradient Descent(9/49): loss=0.4566430574259276\n",
      "Gradient Descent(10/49): loss=0.4553817827150769\n",
      "Gradient Descent(11/49): loss=0.45438272701661264\n",
      "Gradient Descent(12/49): loss=0.4535913749978592\n",
      "Gradient Descent(13/49): loss=0.45296454506380385\n",
      "Gradient Descent(14/49): loss=0.4524680330730392\n",
      "Gradient Descent(15/49): loss=0.4520747459251546\n",
      "Gradient Descent(16/49): loss=0.45176322317531487\n",
      "Gradient Descent(17/49): loss=0.4515164660051669\n",
      "Gradient Descent(18/49): loss=0.45132100965069283\n",
      "Gradient Descent(19/49): loss=0.45116618867231384\n",
      "Gradient Descent(20/49): loss=0.45104355497533977\n",
      "Gradient Descent(21/49): loss=0.4509464168239666\n",
      "Gradient Descent(22/49): loss=0.4508694736942641\n",
      "Gradient Descent(23/49): loss=0.4508085270412265\n",
      "Gradient Descent(24/49): loss=0.4507602511973556\n",
      "Gradient Descent(25/49): loss=0.45072201190142536\n",
      "Gradient Descent(26/49): loss=0.4506917225551189\n",
      "Gradient Descent(27/49): loss=0.4506677303639098\n",
      "Gradient Descent(28/49): loss=0.450648726149253\n",
      "Gradient Descent(29/49): loss=0.4506336729108232\n",
      "Gradient Descent(30/49): loss=0.4506217492406631\n",
      "Gradient Descent(31/49): loss=0.45061230450152945\n",
      "Gradient Descent(32/49): loss=0.45060482332366136\n",
      "Gradient Descent(33/49): loss=0.45059889748267234\n",
      "Gradient Descent(34/49): loss=0.45059420362402475\n",
      "Gradient Descent(35/49): loss=0.4505904856185898\n",
      "Gradient Descent(36/49): loss=0.45058754058648504\n",
      "Gradient Descent(37/49): loss=0.45058520782655487\n",
      "Gradient Descent(38/49): loss=0.45058336004741406\n",
      "Gradient Descent(39/49): loss=0.4505818964215566\n",
      "Gradient Descent(40/49): loss=0.450580737083515\n",
      "Gradient Descent(41/49): loss=0.4505798187718523\n",
      "Gradient Descent(42/49): loss=0.45057909137718416\n",
      "Gradient Descent(43/49): loss=0.4505785152078677\n",
      "Gradient Descent(44/49): loss=0.4505780588241521\n",
      "Gradient Descent(45/49): loss=0.45057769732261055\n",
      "Gradient Descent(46/49): loss=0.45057741097724\n",
      "Gradient Descent(47/49): loss=0.45057718416307185\n",
      "Gradient Descent(48/49): loss=0.4505770045035692\n",
      "Gradient Descent(49/49): loss=0.45057686219527726\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4896409810035201\n",
      "Gradient Descent(2/49): loss=0.48143560205640556\n",
      "Gradient Descent(3/49): loss=0.47493612139240055\n",
      "Gradient Descent(4/49): loss=0.46978788275844147\n",
      "Gradient Descent(5/49): loss=0.46570996293647926\n",
      "Gradient Descent(6/49): loss=0.4624798426455065\n",
      "Gradient Descent(7/49): loss=0.45992126436302516\n",
      "Gradient Descent(8/49): loss=0.4578946145054731\n",
      "Gradient Descent(9/49): loss=0.4562893051533044\n",
      "Gradient Descent(10/49): loss=0.45501773961545217\n",
      "Gradient Descent(11/49): loss=0.4540105325529199\n",
      "Gradient Descent(12/49): loss=0.4532127238386875\n",
      "Gradient Descent(13/49): loss=0.4525807795561443\n",
      "Gradient Descent(14/49): loss=0.4520802164899419\n",
      "Gradient Descent(15/49): loss=0.45168372048520294\n",
      "Gradient Descent(16/49): loss=0.45136965599984924\n",
      "Gradient Descent(17/49): loss=0.45112088552100055\n",
      "Gradient Descent(18/49): loss=0.4509238344247046\n",
      "Gradient Descent(19/49): loss=0.4507677502513286\n",
      "Gradient Descent(20/49): loss=0.4506441159775972\n",
      "Gradient Descent(21/49): loss=0.45054618526937484\n",
      "Gradient Descent(22/49): loss=0.45046861435539165\n",
      "Gradient Descent(23/49): loss=0.4504071704344259\n",
      "Gradient Descent(24/49): loss=0.4503585007046288\n",
      "Gradient Descent(25/49): loss=0.4503199494116564\n",
      "Gradient Descent(26/49): loss=0.4502894129324931\n",
      "Gradient Descent(27/49): loss=0.45026522498734783\n",
      "Gradient Descent(28/49): loss=0.450246065715998\n",
      "Gradient Descent(29/49): loss=0.4502308896571622\n",
      "Gradient Descent(30/49): loss=0.45021886870095806\n",
      "Gradient Descent(31/49): loss=0.45020934690154885\n",
      "Gradient Descent(32/49): loss=0.4502018046842368\n",
      "Gradient Descent(33/49): loss=0.4501958304939041\n",
      "Gradient Descent(34/49): loss=0.4501910983377414\n",
      "Gradient Descent(35/49): loss=0.45018734999684507\n",
      "Gradient Descent(36/49): loss=0.450184380936021\n",
      "Gradient Descent(37/49): loss=0.45018202914294225\n",
      "Gradient Descent(38/49): loss=0.4501801662876446\n",
      "Gradient Descent(39/49): loss=0.4501786907199633\n",
      "Gradient Descent(40/49): loss=0.4501775219228028\n",
      "Gradient Descent(41/49): loss=0.4501765961185722\n",
      "Gradient Descent(42/49): loss=0.45017586278904087\n",
      "Gradient Descent(43/49): loss=0.45017528191871947\n",
      "Gradient Descent(44/49): loss=0.45017482181133756\n",
      "Gradient Descent(45/49): loss=0.4501744573602806\n",
      "Gradient Descent(46/49): loss=0.4501741686785984\n",
      "Gradient Descent(47/49): loss=0.4501739400138377\n",
      "Gradient Descent(48/49): loss=0.4501737588884808\n",
      "Gradient Descent(49/49): loss=0.45017361541908546\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4925076140665516\n",
      "Gradient Descent(2/49): loss=0.4861660586124797\n",
      "Gradient Descent(3/49): loss=0.48079856607615007\n",
      "Gradient Descent(4/49): loss=0.47625552039340446\n",
      "Gradient Descent(5/49): loss=0.4724102865275294\n",
      "Gradient Descent(6/49): loss=0.4691556805834522\n",
      "Gradient Descent(7/49): loss=0.46640098211238556\n",
      "Gradient Descent(8/49): loss=0.4640694053264731\n",
      "Gradient Descent(9/49): loss=0.4620959587348789\n",
      "Gradient Descent(10/49): loss=0.46042563353975224\n",
      "Gradient Descent(11/49): loss=0.4590118702945985\n",
      "Gradient Descent(12/49): loss=0.45781526108390014\n",
      "Gradient Descent(13/49): loss=0.4568024510479642\n",
      "Gradient Descent(14/49): loss=0.45594520863354787\n",
      "Gradient Descent(15/49): loss=0.4552196386539864\n",
      "Gradient Descent(16/49): loss=0.4546055162232852\n",
      "Gradient Descent(17/49): loss=0.4540857229979401\n",
      "Gradient Descent(18/49): loss=0.453645770012008\n",
      "Gradient Descent(19/49): loss=0.45327339380471515\n",
      "Gradient Descent(20/49): loss=0.4529582145828625\n",
      "Gradient Descent(21/49): loss=0.4526914468894865\n",
      "Gradient Descent(22/49): loss=0.4524656547138129\n",
      "Gradient Descent(23/49): loss=0.4522745442163226\n",
      "Gradient Descent(24/49): loss=0.452112788291247\n",
      "Gradient Descent(25/49): loss=0.451975878076263\n",
      "Gradient Descent(26/49): loss=0.4518599972703003\n",
      "Gradient Descent(27/49): loss=0.4517619157561338\n",
      "Gradient Descent(28/49): loss=0.45167889956254303\n",
      "Gradient Descent(29/49): loss=0.4516086346562878\n",
      "Gradient Descent(30/49): loss=0.4515491624396334\n",
      "Gradient Descent(31/49): loss=0.4514988251554574\n",
      "Gradient Descent(32/49): loss=0.4514562196781306\n",
      "Gradient Descent(33/49): loss=0.4514201584021211\n",
      "Gradient Descent(34/49): loss=0.45138963613810684\n",
      "Gradient Descent(35/49): loss=0.4513638020938452\n",
      "Gradient Descent(36/49): loss=0.4513419361587818\n",
      "Gradient Descent(37/49): loss=0.4513234288313446\n",
      "Gradient Descent(38/49): loss=0.4513077642294014\n",
      "Gradient Descent(39/49): loss=0.451294505710317\n",
      "Gradient Descent(40/49): loss=0.4512832836997637\n",
      "Gradient Descent(41/49): loss=0.4512737853900314\n",
      "Gradient Descent(42/49): loss=0.45126574602067415\n",
      "Gradient Descent(43/49): loss=0.4512589414984499\n",
      "Gradient Descent(44/49): loss=0.45125318215083954\n",
      "Gradient Descent(45/49): loss=0.45124830743902206\n",
      "Gradient Descent(46/49): loss=0.45124418148293977\n",
      "Gradient Descent(47/49): loss=0.4512406892737117\n",
      "Gradient Descent(48/49): loss=0.451237733467821\n",
      "Gradient Descent(49/49): loss=0.45123523167371515\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49231966481394335\n",
      "Gradient Descent(2/49): loss=0.4858190291124601\n",
      "Gradient Descent(3/49): loss=0.4803168910547316\n",
      "Gradient Descent(4/49): loss=0.4756598814026645\n",
      "Gradient Descent(5/49): loss=0.47171818843315727\n",
      "Gradient Descent(6/49): loss=0.46838193950376705\n",
      "Gradient Descent(7/49): loss=0.4655581384099332\n",
      "Gradient Descent(8/49): loss=0.4631680731641097\n",
      "Gradient Descent(9/49): loss=0.4611451219400447\n",
      "Gradient Descent(10/49): loss=0.4594328960239977\n",
      "Gradient Descent(11/49): loss=0.45798366800865564\n",
      "Gradient Descent(12/49): loss=0.4567570414164703\n",
      "Gradient Descent(13/49): loss=0.4557188246688442\n",
      "Gradient Descent(14/49): loss=0.4548400780136535\n",
      "Gradient Descent(15/49): loss=0.45409630684470004\n",
      "Gradient Descent(16/49): loss=0.4534667789272977\n",
      "Gradient Descent(17/49): loss=0.4529339464980085\n",
      "Gradient Descent(18/49): loss=0.45248295712985787\n",
      "Gradient Descent(19/49): loss=0.4521012397286549\n",
      "Gradient Descent(20/49): loss=0.45177815412027683\n",
      "Gradient Descent(21/49): loss=0.45150469446134583\n",
      "Gradient Descent(22/49): loss=0.45127323820602666\n",
      "Gradient Descent(23/49): loss=0.4510773336315242\n",
      "Gradient Descent(24/49): loss=0.4509115199996656\n",
      "Gradient Descent(25/49): loss=0.45077117534166056\n",
      "Gradient Descent(26/49): loss=0.4506523876231248\n",
      "Gradient Descent(27/49): loss=0.4505518456981564\n",
      "Gradient Descent(28/49): loss=0.450466747012863\n",
      "Gradient Descent(29/49): loss=0.45039471948563076\n",
      "Gradient Descent(30/49): loss=0.45033375538658116\n",
      "Gradient Descent(31/49): loss=0.4502821553731459\n",
      "Gradient Descent(32/49): loss=0.4502384811217742\n",
      "Gradient Descent(33/49): loss=0.45020151523541335\n",
      "Gradient Descent(34/49): loss=0.4501702273091972\n",
      "Gradient Descent(35/49): loss=0.450143745208448\n",
      "Gradient Descent(36/49): loss=0.4501213307583738\n",
      "Gradient Descent(37/49): loss=0.45010235916783103\n",
      "Gradient Descent(38/49): loss=0.45008630161359586\n",
      "Gradient Descent(39/49): loss=0.45007271049969094\n",
      "Gradient Descent(40/49): loss=0.450061206980882\n",
      "Gradient Descent(41/49): loss=0.4500514704025618\n",
      "Gradient Descent(42/49): loss=0.4500432293626717\n",
      "Gradient Descent(43/49): loss=0.45003625414650894\n",
      "Gradient Descent(44/49): loss=0.45003035032354877\n",
      "Gradient Descent(45/49): loss=0.45002535332779503\n",
      "Gradient Descent(46/49): loss=0.450021123870589\n",
      "Gradient Descent(47/49): loss=0.45001754405801014\n",
      "Gradient Descent(48/49): loss=0.45001451410464316\n",
      "Gradient Descent(49/49): loss=0.4500119495521135\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49240852275199987\n",
      "Gradient Descent(2/49): loss=0.48598309640929654\n",
      "Gradient Descent(3/49): loss=0.48054461555282707\n",
      "Gradient Descent(4/49): loss=0.4759414853559111\n",
      "Gradient Descent(5/49): loss=0.4720453959572421\n",
      "Gradient Descent(6/49): loss=0.4687477458902106\n",
      "Gradient Descent(7/49): loss=0.46595661487347584\n",
      "Gradient Descent(8/49): loss=0.4635942015809118\n",
      "Gradient Descent(9/49): loss=0.46159465497008517\n",
      "Gradient Descent(10/49): loss=0.459902238718681\n",
      "Gradient Descent(11/49): loss=0.45846977760349106\n",
      "Gradient Descent(12/49): loss=0.45725734251559447\n",
      "Gradient Descent(13/49): loss=0.4562311374571988\n",
      "Gradient Descent(14/49): loss=0.45536255749577237\n",
      "Gradient Descent(15/49): loss=0.4546273914164216\n",
      "Gradient Descent(16/49): loss=0.454005146846859\n",
      "Gradient Descent(17/49): loss=0.4534784790431816\n",
      "Gradient Descent(18/49): loss=0.45303270741414875\n",
      "Gradient Descent(19/49): loss=0.4526554063073354\n",
      "Gradient Descent(20/49): loss=0.4523360586505286\n",
      "Gradient Descent(21/49): loss=0.4520657627938072\n",
      "Gradient Descent(22/49): loss=0.4518369843806786\n",
      "Gradient Descent(23/49): loss=0.45164334633180614\n",
      "Gradient Descent(24/49): loss=0.4514794510872409\n",
      "Gradient Descent(25/49): loss=0.45134073015224063\n",
      "Gradient Descent(26/49): loss=0.45122331675285654\n",
      "Gradient Descent(27/49): loss=0.4511239380516177\n",
      "Gradient Descent(28/49): loss=0.4510398239188894\n",
      "Gradient Descent(29/49): loss=0.4509686297169479\n",
      "Gradient Descent(30/49): loss=0.4509083709444246\n",
      "Gradient Descent(31/49): loss=0.450857367919361\n",
      "Gradient Descent(32/49): loss=0.45081419895894703\n",
      "Gradient Descent(33/49): loss=0.45077766075085296\n",
      "Gradient Descent(34/49): loss=0.450746734811522\n",
      "Gradient Descent(35/49): loss=0.45072055909647213\n",
      "Gradient Descent(36/49): loss=0.4506984039712541\n",
      "Gradient Descent(37/49): loss=0.4506796518732694\n",
      "Gradient Descent(38/49): loss=0.4506637800975351\n",
      "Gradient Descent(39/49): loss=0.45065034622655376\n",
      "Gradient Descent(40/49): loss=0.4506389757981552\n",
      "Gradient Descent(41/49): loss=0.4506293518675586\n",
      "Gradient Descent(42/49): loss=0.4506212061727017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=0.45061431165657456\n",
      "Gradient Descent(44/49): loss=0.45060847613812477\n",
      "Gradient Descent(45/49): loss=0.45060353695530864\n",
      "Gradient Descent(46/49): loss=0.4505993564309734\n",
      "Gradient Descent(47/49): loss=0.45059581803517595\n",
      "Gradient Descent(48/49): loss=0.4505928231369729\n",
      "Gradient Descent(49/49): loss=0.45059028825513375\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4923465833676799\n",
      "Gradient Descent(2/49): loss=0.48586873153008125\n",
      "Gradient Descent(3/49): loss=0.48038587773474367\n",
      "Gradient Descent(4/49): loss=0.4757451902823649\n",
      "Gradient Descent(5/49): loss=0.4718173124226747\n",
      "Gradient Descent(6/49): loss=0.4684927566022329\n",
      "Gradient Descent(7/49): loss=0.4656788525558096\n",
      "Gradient Descent(8/49): loss=0.463297164170919\n",
      "Gradient Descent(9/49): loss=0.461281303121946\n",
      "Gradient Descent(10/49): loss=0.45957507833009437\n",
      "Gradient Descent(11/49): loss=0.4581309296662716\n",
      "Gradient Descent(12/49): loss=0.4569086022372128\n",
      "Gradient Descent(13/49): loss=0.4558740243012566\n",
      "Gradient Descent(14/49): loss=0.454998357536264\n",
      "Gradient Descent(15/49): loss=0.454257193186374\n",
      "Gradient Descent(16/49): loss=0.45362987168062724\n",
      "Gradient Descent(17/49): loss=0.45309890675816283\n",
      "Gradient Descent(18/49): loss=0.4526494980477889\n",
      "Gradient Descent(19/49): loss=0.45226911851532814\n",
      "Gradient Descent(20/49): loss=0.4519471652790537\n",
      "Gradient Descent(21/49): loss=0.45167466405987095\n",
      "Gradient Descent(22/49): loss=0.4514440190279547\n",
      "Gradient Descent(23/49): loss=0.4512488010729411\n",
      "Gradient Descent(24/49): loss=0.4510835685958174\n",
      "Gradient Descent(25/49): loss=0.4509437158271799\n",
      "Gradient Descent(26/49): loss=0.4508253444438049\n",
      "Gradient Descent(27/49): loss=0.4507251549049165\n",
      "Gradient Descent(28/49): loss=0.4506403544792016\n",
      "Gradient Descent(29/49): loss=0.4505685793988759\n",
      "Gradient Descent(30/49): loss=0.45050782897088887\n",
      "Gradient Descent(31/49): loss=0.45045640980864005\n",
      "Gradient Descent(32/49): loss=0.45041288862971296\n",
      "Gradient Descent(33/49): loss=0.45037605230386907\n",
      "Gradient Descent(34/49): loss=0.45034487403767487\n",
      "Gradient Descent(35/49): loss=0.45031848475316794\n",
      "Gradient Descent(36/49): loss=0.4502961488627614\n",
      "Gradient Descent(37/49): loss=0.4502772437651212\n",
      "Gradient Descent(38/49): loss=0.4502612424904787\n",
      "Gradient Descent(39/49): loss=0.45024769901162104\n",
      "Gradient Descent(40/49): loss=0.45023623581111594\n",
      "Gradient Descent(41/49): loss=0.4502265333582088\n",
      "Gradient Descent(42/49): loss=0.45021832120206784\n",
      "Gradient Descent(43/49): loss=0.4502113704331102\n",
      "Gradient Descent(44/49): loss=0.45020548730226445\n",
      "Gradient Descent(45/49): loss=0.45020050782031656\n",
      "Gradient Descent(46/49): loss=0.4501962931867959\n",
      "Gradient Descent(47/49): loss=0.45019272592098414\n",
      "Gradient Descent(48/49): loss=0.4501897065872009\n",
      "Gradient Descent(49/49): loss=0.4501871510230871\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49524409096021327\n",
      "Gradient Descent(2/49): loss=0.49095188305180815\n",
      "Gradient Descent(3/49): loss=0.4870781654144657\n",
      "Gradient Descent(4/49): loss=0.4835821352467665\n",
      "Gradient Descent(5/49): loss=0.4804269680204195\n",
      "Gradient Descent(6/49): loss=0.47757942959864313\n",
      "Gradient Descent(7/49): loss=0.47500952617299175\n",
      "Gradient Descent(8/49): loss=0.4726901883313397\n",
      "Gradient Descent(9/49): loss=0.4705969859292476\n",
      "Gradient Descent(10/49): loss=0.46870787076135717\n",
      "Gradient Descent(11/49): loss=0.46700294432233913\n",
      "Gradient Descent(12/49): loss=0.4654642482111255\n",
      "Gradient Descent(13/49): loss=0.4640755749707528\n",
      "Gradient Descent(14/49): loss=0.4628222973713169\n",
      "Gradient Descent(15/49): loss=0.461691214337826\n",
      "Gradient Descent(16/49): loss=0.46067041190010066\n",
      "Gradient Descent(17/49): loss=0.45974913770005366\n",
      "Gradient Descent(18/49): loss=0.45891768773451136\n",
      "Gradient Descent(19/49): loss=0.45816730414060935\n",
      "Gradient Descent(20/49): loss=0.457490082947113\n",
      "Gradient Descent(21/49): loss=0.45687889081998273\n",
      "Gradient Descent(22/49): loss=0.45632728992524757\n",
      "Gradient Descent(23/49): loss=0.45582947011774966\n",
      "Gradient Descent(24/49): loss=0.4553801877414821\n",
      "Gradient Descent(25/49): loss=0.45497471039690024\n",
      "Gradient Descent(26/49): loss=0.4546087670934161\n",
      "Gradient Descent(27/49): loss=0.45427850326202096\n",
      "Gradient Descent(28/49): loss=0.45398044015418754\n",
      "Gradient Descent(29/49): loss=0.4537114381993674\n",
      "Gradient Descent(30/49): loss=0.4534686639351422\n",
      "Gradient Descent(31/49): loss=0.45324956016167944\n",
      "Gradient Descent(32/49): loss=0.45305181900612895\n",
      "Gradient Descent(33/49): loss=0.45287335761324493\n",
      "Gradient Descent(34/49): loss=0.4527122962061669\n",
      "Gradient Descent(35/49): loss=0.45256693828627903\n",
      "Gradient Descent(36/49): loss=0.45243575276357967\n",
      "Gradient Descent(37/49): loss=0.45231735782934424\n",
      "Gradient Descent(38/49): loss=0.45221050640119653\n",
      "Gradient Descent(39/49): loss=0.4521140729872933\n",
      "Gradient Descent(40/49): loss=0.4520270418312454\n",
      "Gradient Descent(41/49): loss=0.45194849621291217\n",
      "Gradient Descent(42/49): loss=0.4518776087923668\n",
      "Gradient Descent(43/49): loss=0.4518136328953244\n",
      "Gradient Descent(44/49): loss=0.45175589464824334\n",
      "Gradient Descent(45/49): loss=0.4517037858802532\n",
      "Gradient Descent(46/49): loss=0.4516567577171418\n",
      "Gradient Descent(47/49): loss=0.4516143147999337\n",
      "Gradient Descent(48/49): loss=0.4515760100671534\n",
      "Gradient Descent(49/49): loss=0.45154144004581953\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4951247872354134\n",
      "Gradient Descent(2/49): loss=0.49072490771537214\n",
      "Gradient Descent(3/49): loss=0.4867540164485375\n",
      "Gradient Descent(4/49): loss=0.4831702870802157\n",
      "Gradient Descent(5/49): loss=0.47993597132531\n",
      "Gradient Descent(6/49): loss=0.4770170013565086\n",
      "Gradient Descent(7/49): loss=0.47438263095966116\n",
      "Gradient Descent(8/49): loss=0.4720051116765067\n",
      "Gradient Descent(9/49): loss=0.4698594005234603\n",
      "Gradient Descent(10/49): loss=0.4679228962078354\n",
      "Gradient Descent(11/49): loss=0.4661752010629852\n",
      "Gradient Descent(12/49): loss=0.4645979061947575\n",
      "Gradient Descent(13/49): loss=0.4631743975761819\n",
      "Gradient Descent(14/49): loss=0.4618896810479185\n",
      "Gradient Descent(15/49): loss=0.4607302243811606\n",
      "Gradient Descent(16/49): loss=0.4596838147394098\n",
      "Gradient Descent(17/49): loss=0.4587394300377294\n",
      "Gradient Descent(18/49): loss=0.4578871228444639\n",
      "Gradient Descent(19/49): loss=0.4571179156025416\n",
      "Gradient Descent(20/49): loss=0.45642370606670746\n",
      "Gradient Descent(21/49): loss=0.45579718196061636\n",
      "Gradient Descent(22/49): loss=0.4552317439548701\n",
      "Gradient Descent(23/49): loss=0.45472143615468374\n",
      "Gradient Descent(24/49): loss=0.4542608833650156\n",
      "Gradient Descent(25/49): loss=0.4538452344723394\n",
      "Gradient Descent(26/49): loss=0.4534701113466993\n",
      "Gradient Descent(27/49): loss=0.45313156272580996\n",
      "Gradient Descent(28/49): loss=0.4528260225954564\n",
      "Gradient Descent(29/49): loss=0.4525502726278127\n",
      "Gradient Descent(30/49): loss=0.4523014082820142\n",
      "Gradient Descent(31/49): loss=0.45207680820993157\n",
      "Gradient Descent(32/49): loss=0.4518741066448765\n",
      "Gradient Descent(33/49): loss=0.45169116848241453\n",
      "Gradient Descent(34/49): loss=0.4515260667907923\n",
      "Gradient Descent(35/49): loss=0.4513770625141038\n",
      "Gradient Descent(36/49): loss=0.45124258615439183\n",
      "Gradient Descent(37/49): loss=0.4511212212397521\n",
      "Gradient Descent(38/49): loss=0.4510116894042895\n",
      "Gradient Descent(39/49): loss=0.4509128369227847\n",
      "Gradient Descent(40/49): loss=0.45082362255822656\n",
      "Gradient Descent(41/49): loss=0.4507431065942128\n",
      "Gradient Descent(42/49): loss=0.45067044093669045\n",
      "Gradient Descent(43/49): loss=0.4506048601807764\n",
      "Gradient Descent(44/49): loss=0.45054567354856406\n",
      "Gradient Descent(45/49): loss=0.45049225761299244\n",
      "Gradient Descent(46/49): loss=0.4504440497311389\n",
      "Gradient Descent(47/49): loss=0.4504005421177662\n",
      "Gradient Descent(48/49): loss=0.45036127649669727\n",
      "Gradient Descent(49/49): loss=0.45032583927368264\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4951811911999999\n",
      "Gradient Descent(2/49): loss=0.4908322162580047\n",
      "Gradient Descent(3/49): loss=0.4869072663728538\n",
      "Gradient Descent(4/49): loss=0.48336499910150305\n",
      "Gradient Descent(5/49): loss=0.4801681028891076\n",
      "Gradient Descent(6/49): loss=0.4772829040574201\n",
      "Gradient Descent(7/49): loss=0.47467901211182456\n",
      "Gradient Descent(8/49): loss=0.472328999630924\n",
      "Gradient Descent(9/49): loss=0.47020811336690865\n",
      "Gradient Descent(10/49): loss=0.46829401351363786\n",
      "Gradient Descent(11/49): loss=0.4665665383960572\n",
      "Gradient Descent(12/49): loss=0.4650074921024431\n",
      "Gradient Descent(13/49): loss=0.4636004528224559\n",
      "Gradient Descent(14/49): loss=0.4623305998722655\n",
      "Gradient Descent(15/49): loss=0.4611845575847204\n",
      "Gradient Descent(16/49): loss=0.46015025442020985\n",
      "Gradient Descent(17/49): loss=0.45921679581423924\n",
      "Gradient Descent(18/49): loss=0.45837434942235183\n",
      "Gradient Descent(19/49): loss=0.4576140415536731\n",
      "Gradient Descent(20/49): loss=0.45692786370218935\n",
      "Gradient Descent(21/49): loss=0.456308588191226\n",
      "Gradient Descent(22/49): loss=0.4557496920425818\n",
      "Gradient Descent(23/49): loss=0.45524528826843\n",
      "Gradient Descent(24/49): loss=0.4547900638622581\n",
      "Gradient Descent(25/49): loss=0.4543792238356884\n",
      "Gradient Descent(26/49): loss=0.4540084407117088\n",
      "Gradient Descent(27/49): loss=0.4536738089423171\n",
      "Gradient Descent(28/49): loss=0.4533718037704411\n",
      "Gradient Descent(29/49): loss=0.45309924410282293\n",
      "Gradient Descent(30/49): loss=0.4528532590027976\n",
      "Gradient Descent(31/49): loss=0.4526312574500249\n",
      "Gradient Descent(32/49): loss=0.45243090104864764\n",
      "Gradient Descent(33/49): loss=0.4522500793964043\n",
      "Gradient Descent(34/49): loss=0.45208688785525497\n",
      "Gradient Descent(35/49): loss=0.4519396074893676\n",
      "Gradient Descent(36/49): loss=0.45180668695915444\n",
      "Gradient Descent(37/49): loss=0.45168672618063704\n",
      "Gradient Descent(38/49): loss=0.451578461578025\n",
      "Gradient Descent(39/49): loss=0.4514807527741673\n",
      "Gradient Descent(40/49): loss=0.45139257057868604\n",
      "Gradient Descent(41/49): loss=0.45131298614726434\n",
      "Gradient Descent(42/49): loss=0.451241161197906\n",
      "Gradient Descent(43/49): loss=0.4511763391811103\n",
      "Gradient Descent(44/49): loss=0.45111783731095195\n",
      "Gradient Descent(45/49): loss=0.45106503937313397\n",
      "Gradient Descent(46/49): loss=0.4510173892342536\n",
      "Gradient Descent(47/49): loss=0.45097438498391385\n",
      "Gradient Descent(48/49): loss=0.450935573647982\n",
      "Gradient Descent(49/49): loss=0.450900546417304\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4951418742079999\n",
      "Gradient Descent(2/49): loss=0.49075741568071574\n",
      "Gradient Descent(3/49): loss=0.48680044185984833\n",
      "Gradient Descent(4/49): loss=0.48322927298651536\n",
      "Gradient Descent(5/49): loss=0.4800062930783279\n",
      "Gradient Descent(6/49): loss=0.4770975537111924\n",
      "Gradient Descent(7/49): loss=0.47447241643235133\n",
      "Gradient Descent(8/49): loss=0.47210323003819593\n",
      "Gradient Descent(9/49): loss=0.4699650393174717\n",
      "Gradient Descent(10/49): loss=0.46803532219201816\n",
      "Gradient Descent(11/49): loss=0.4662937524862985\n",
      "Gradient Descent(12/49): loss=0.4647219858268854\n",
      "Gradient Descent(13/49): loss=0.4633034664167654\n",
      "Gradient Descent(14/49): loss=0.46202325264913147\n",
      "Gradient Descent(15/49): loss=0.46086785972384176\n",
      "Gradient Descent(16/49): loss=0.459825117608768\n",
      "Gradient Descent(17/49): loss=0.458884042849914\n",
      "Gradient Descent(18/49): loss=0.458034722880048\n",
      "Gradient Descent(19/49): loss=0.45726821160724423\n",
      "Gradient Descent(20/49): loss=0.45657643518353763\n",
      "Gradient Descent(21/49): loss=0.45595210696114297\n",
      "Gradient Descent(22/49): loss=0.4553886507404314\n",
      "Gradient Descent(23/49): loss=0.4548801315012394\n",
      "Gradient Descent(24/49): loss=0.4544211928878685\n",
      "Gradient Descent(25/49): loss=0.4540070007893016\n",
      "Gradient Descent(26/49): loss=0.4536331924203443\n",
      "Gradient Descent(27/49): loss=0.4532958303673609\n",
      "Gradient Descent(28/49): loss=0.452991361114543\n",
      "Gradient Descent(29/49): loss=0.4527165776138746\n",
      "Gradient Descent(30/49): loss=0.45246858550452207\n",
      "Gradient Descent(31/49): loss=0.4522447726258313\n",
      "Gradient Descent(32/49): loss=0.4520427815028124\n",
      "Gradient Descent(33/49): loss=0.45186048451428806\n",
      "Gradient Descent(34/49): loss=0.4516959614821453\n",
      "Gradient Descent(35/49): loss=0.4515474794456361\n",
      "Gradient Descent(36/49): loss=0.4514134744076867\n",
      "Gradient Descent(37/49): loss=0.4512925348609374\n",
      "Gradient Descent(38/49): loss=0.45118338691999593\n",
      "Gradient Descent(39/49): loss=0.45108488090329624\n",
      "Gradient Descent(40/49): loss=0.4509959792232249\n",
      "Gradient Descent(41/49): loss=0.4509157454569602\n",
      "Gradient Descent(42/49): loss=0.45084333448290653\n",
      "Gradient Descent(43/49): loss=0.45077798357882326\n",
      "Gradient Descent(44/49): loss=0.4507190043878884\n",
      "Gradient Descent(45/49): loss=0.4506657756680689\n",
      "Gradient Descent(46/49): loss=0.45061773674843214\n",
      "Gradient Descent(47/49): loss=0.4505743816234603\n",
      "Gradient Descent(48/49): loss=0.4505352536231727\n",
      "Gradient Descent(49/49): loss=0.45049994060291343\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4980683692515328\n",
      "Gradient Descent(2/49): loss=0.4962132310807078\n",
      "Gradient Descent(3/49): loss=0.49443155638144803\n",
      "Gradient Descent(4/49): loss=0.4927204360002735\n",
      "Gradient Descent(5/49): loss=0.4910770759861987\n",
      "Gradient Descent(6/49): loss=0.48949879302867644\n",
      "Gradient Descent(7/49): loss=0.48798301007626826\n",
      "Gradient Descent(8/49): loss=0.4865272521287792\n",
      "Gradient Descent(9/49): loss=0.4851291421960103\n",
      "Gradient Descent(10/49): loss=0.48378639741657975\n",
      "Gradient Descent(11/49): loss=0.48249682533041427\n",
      "Gradient Descent(12/49): loss=0.4812583202988656\n",
      "Gradient Descent(13/49): loss=0.4800688600665623\n",
      "Gradient Descent(14/49): loss=0.47892650245945845\n",
      "Gradient Descent(15/49): loss=0.477829382213595\n",
      "Gradient Descent(16/49): loss=0.4767757079294677\n",
      "Gradient Descent(17/49): loss=0.4757637591469952\n",
      "Gradient Descent(18/49): loss=0.4747918835363106\n",
      "Gradient Descent(19/49): loss=0.47385849419980347\n",
      "Gradient Descent(20/49): loss=0.47296206708102145\n",
      "Gradient Descent(21/49): loss=0.47210113847614443\n",
      "Gradient Descent(22/49): loss=0.47127430264402353\n",
      "Gradient Descent(23/49): loss=0.4704802095108519\n",
      "Gradient Descent(24/49): loss=0.46971756246575364\n",
      "Gradient Descent(25/49): loss=0.4689851162436424\n",
      "Gradient Descent(26/49): loss=0.4682816748919263\n",
      "Gradient Descent(27/49): loss=0.4676060898177395\n",
      "Gradient Descent(28/49): loss=0.46695725791249\n",
      "Gradient Descent(29/49): loss=0.4663341197506874\n",
      "Gradient Descent(30/49): loss=0.46573565786009113\n",
      "Gradient Descent(31/49): loss=0.46516089506036346\n",
      "Gradient Descent(32/49): loss=0.46460889286750795\n",
      "Gradient Descent(33/49): loss=0.4640787499614867\n",
      "Gradient Descent(34/49): loss=0.463569600714546\n",
      "Gradient Descent(35/49): loss=0.4630806137777817\n",
      "Gradient Descent(36/49): loss=0.46261099072371564\n",
      "Gradient Descent(37/49): loss=0.4621599647425902\n",
      "Gradient Descent(38/49): loss=0.46172679939031747\n",
      "Gradient Descent(39/49): loss=0.4613107873859945\n",
      "Gradient Descent(40/49): loss=0.4609112494570408\n",
      "Gradient Descent(41/49): loss=0.46052753323007595\n",
      "Gradient Descent(42/49): loss=0.460159012165697\n",
      "Gradient Descent(43/49): loss=0.45980508453546715\n",
      "Gradient Descent(44/49): loss=0.45946517243939616\n",
      "Gradient Descent(45/49): loss=0.45913872086232776\n",
      "Gradient Descent(46/49): loss=0.45882519676771294\n",
      "Gradient Descent(47/49): loss=0.4585240882272445\n",
      "Gradient Descent(48/49): loss=0.458234903584978\n",
      "Gradient Descent(49/49): loss=0.4579571706545465\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.498019913584845\n",
      "Gradient Descent(2/49): loss=0.4961182385917334\n",
      "Gradient Descent(3/49): loss=0.4942918699283421\n",
      "Gradient Descent(4/49): loss=0.49253782546402336\n",
      "Gradient Descent(5/49): loss=0.49085324116049134\n",
      "Gradient Descent(6/49): loss=0.48923536639538384\n",
      "Gradient Descent(7/49): loss=0.48768155947097347\n",
      "Gradient Descent(8/49): loss=0.48618928330076355\n",
      "Gradient Descent(9/49): loss=0.4847561012669017\n",
      "Gradient Descent(10/49): loss=0.48337967324157227\n",
      "Gradient Descent(11/49): loss=0.48205775176605514\n",
      "Gradient Descent(12/49): loss=0.48078817838096627\n",
      "Gradient Descent(13/49): loss=0.4795688801019281\n",
      "Gradient Descent(14/49): loss=0.47839786603473616\n",
      "Gradient Descent(15/49): loss=0.47727322412460377\n",
      "Gradient Descent(16/49): loss=0.4761931180341145\n",
      "Gradient Descent(17/49): loss=0.4751557841448073\n",
      "Gradient Descent(18/49): loss=0.4741595286775177\n",
      "Gradient Descent(19/49): loss=0.473202724926731\n",
      "Gradient Descent(20/49): loss=0.4722838106044784\n",
      "Gradient Descent(21/49): loss=0.47140128528938746\n",
      "Gradient Descent(22/49): loss=0.47055370797677204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=0.4697396947257352\n",
      "Gradient Descent(24/49): loss=0.46895791639943807\n",
      "Gradient Descent(25/49): loss=0.468207096494867\n",
      "Gradient Descent(26/49): loss=0.4674860090585143\n",
      "Gradient Descent(27/49): loss=0.4667934766846404\n",
      "Gradient Descent(28/49): loss=0.4661283685927731\n",
      "Gradient Descent(29/49): loss=0.4654895987813437\n",
      "Gradient Descent(30/49): loss=0.4648761242544464\n",
      "Gradient Descent(31/49): loss=0.4642869433188142\n",
      "Gradient Descent(32/49): loss=0.46372109394823396\n",
      "Gradient Descent(33/49): loss=0.46317765221272744\n",
      "Gradient Descent(34/49): loss=0.4626557307699473\n",
      "Gradient Descent(35/49): loss=0.4621544774163012\n",
      "Gradient Descent(36/49): loss=0.46167307369546\n",
      "Gradient Descent(37/49): loss=0.46121073356196324\n",
      "Gradient Descent(38/49): loss=0.46076670209775483\n",
      "Gradient Descent(39/49): loss=0.46034025427952974\n",
      "Gradient Descent(40/49): loss=0.45993069379490636\n",
      "Gradient Descent(41/49): loss=0.4595373519054732\n",
      "Gradient Descent(42/49): loss=0.4591595863548606\n",
      "Gradient Descent(43/49): loss=0.45879678032005344\n",
      "Gradient Descent(44/49): loss=0.4584483414042243\n",
      "Gradient Descent(45/49): loss=0.4581137006694623\n",
      "Gradient Descent(46/49): loss=0.45779231170779605\n",
      "Gradient Descent(47/49): loss=0.4574836497490133\n",
      "Gradient Descent(48/49): loss=0.4571872108037966\n",
      "Gradient Descent(49/49): loss=0.45690251084081024\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4980428222720001\n",
      "Gradient Descent(2/49): loss=0.49616314878202183\n",
      "Gradient Descent(3/49): loss=0.49435791036225474\n",
      "Gradient Descent(4/49): loss=0.4926241593839073\n",
      "Gradient Descent(5/49): loss=0.4909590649443036\n",
      "Gradient Descent(6/49): loss=0.48935990824450637\n",
      "Gradient Descent(7/49): loss=0.48782407815002377\n",
      "Gradient Descent(8/49): loss=0.486349066927278\n",
      "Gradient Descent(9/49): loss=0.4849324661489598\n",
      "Gradient Descent(10/49): loss=0.48357196276146275\n",
      "Gradient Descent(11/49): loss=0.4822653353081049\n",
      "Gradient Descent(12/49): loss=0.4810104503019055\n",
      "Gradient Descent(13/49): loss=0.47980525874195235\n",
      "Gradient Descent(14/49): loss=0.47864779276777375\n",
      "Gradient Descent(15/49): loss=0.4775361624461712\n",
      "Gradient Descent(16/49): loss=0.4764685526853016\n",
      "Gradient Descent(17/49): loss=0.47544322027096314\n",
      "Gradient Descent(18/49): loss=0.47445849102023513\n",
      "Gradient Descent(19/49): loss=0.4735127570478367\n",
      "Gradient Descent(20/49): loss=0.47260447414074575\n",
      "Gradient Descent(21/49): loss=0.47173215923677286\n",
      "Gradient Descent(22/49): loss=0.4708943880029976\n",
      "Gradient Descent(23/49): loss=0.47008979251007826\n",
      "Gradient Descent(24/49): loss=0.46931705899867837\n",
      "Gradient Descent(25/49): loss=0.4685749257343293\n",
      "Gradient Descent(26/49): loss=0.4678621809472522\n",
      "Gradient Descent(27/49): loss=0.46717766085374085\n",
      "Gradient Descent(28/49): loss=0.46652024775593287\n",
      "Gradient Descent(29/49): loss=0.4658888682167967\n",
      "Gradient Descent(30/49): loss=0.46528249130741095\n",
      "Gradient Descent(31/49): loss=0.4647001269236393\n",
      "Gradient Descent(32/49): loss=0.46414082416946234\n",
      "Gradient Descent(33/49): loss=0.4636036698043513\n",
      "Gradient Descent(34/49): loss=0.46308778675210016\n",
      "Gradient Descent(35/49): loss=0.462592332668718\n",
      "Gradient Descent(36/49): loss=0.46211649856703774\n",
      "Gradient Descent(37/49): loss=0.46165950749578205\n",
      "Gradient Descent(38/49): loss=0.46122061327094843\n",
      "Gradient Descent(39/49): loss=0.46079909925741763\n",
      "Gradient Descent(40/49): loss=0.4603942771988234\n",
      "Gradient Descent(41/49): loss=0.4600054860937503\n",
      "Gradient Descent(42/49): loss=0.4596320911164368\n",
      "Gradient Descent(43/49): loss=0.45927348258022654\n",
      "Gradient Descent(44/49): loss=0.45892907494205004\n",
      "Gradient Descent(45/49): loss=0.4585983058463448\n",
      "Gradient Descent(46/49): loss=0.4582806352068302\n",
      "Gradient Descent(47/49): loss=0.4579755443246405\n",
      "Gradient Descent(48/49): loss=0.45768253504138406\n",
      "Gradient Descent(49/49): loss=0.4574011289257451\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49802685352448\n",
      "Gradient Descent(2/49): loss=0.49613184364938473\n",
      "Gradient Descent(3/49): loss=0.4943118761653533\n",
      "Gradient Descent(4/49): loss=0.4925639793936892\n",
      "Gradient Descent(5/49): loss=0.490885299334176\n",
      "Gradient Descent(6/49): loss=0.48927309500502514\n",
      "Gradient Descent(7/49): loss=0.4877247339673102\n",
      "Gradient Descent(8/49): loss=0.4862376880266817\n",
      "Gradient Descent(9/49): loss=0.4848095291053072\n",
      "Gradient Descent(10/49): loss=0.4834379252772191\n",
      "Gradient Descent(11/49): loss=0.4821206369607229\n",
      "Gradient Descent(12/49): loss=0.4808555132615607\n",
      "Gradient Descent(13/49): loss=0.47964048846088503\n",
      "Gradient Descent(14/49): loss=0.478473578642312\n",
      "Gradient Descent(15/49): loss=0.47735287845255403\n",
      "Gradient Descent(16/49): loss=0.4762765579903122\n",
      "Gradient Descent(17/49): loss=0.47524285981837316\n",
      "Gradient Descent(18/49): loss=0.4742500960940458\n",
      "Gradient Descent(19/49): loss=0.4732966458132036\n",
      "Gradient Descent(20/49): loss=0.4723809521634801\n",
      "Gradient Descent(21/49): loss=0.47150151998228784\n",
      "Gradient Descent(22/49): loss=0.4706569133154687\n",
      "Gradient Descent(23/49): loss=0.4698457530726556\n",
      "Gradient Descent(24/49): loss=0.4690667147754586\n",
      "Gradient Descent(25/49): loss=0.46831852639483046\n",
      "Gradient Descent(26/49): loss=0.46759996627407435\n",
      "Gradient Descent(27/49): loss=0.46690986113409916\n",
      "Gradient Descent(28/49): loss=0.46624708415766775\n",
      "Gradient Descent(29/49): loss=0.46561055314950467\n",
      "Gradient Descent(30/49): loss=0.4649992287692641\n",
      "Gradient Descent(31/49): loss=0.4644121128344819\n",
      "Gradient Descent(32/49): loss=0.46384824669071506\n",
      "Gradient Descent(33/49): loss=0.4633067096462434\n",
      "Gradient Descent(34/49): loss=0.4627866174687324\n",
      "Gradient Descent(35/49): loss=0.4622871209414512\n",
      "Gradient Descent(36/49): loss=0.4618074044766515\n",
      "Gradient Descent(37/49): loss=0.4613466847838558\n",
      "Gradient Descent(38/49): loss=0.46090420959089484\n",
      "Gradient Descent(39/49): loss=0.4604792564155766\n",
      "Gradient Descent(40/49): loss=0.46007113138599987\n",
      "Gradient Descent(41/49): loss=0.4596791681075938\n",
      "Gradient Descent(42/49): loss=0.45930272657501264\n",
      "Gradient Descent(43/49): loss=0.45894119212712137\n",
      "Gradient Descent(44/49): loss=0.45859397444336863\n",
      "Gradient Descent(45/49): loss=0.4582605065798903\n",
      "Gradient Descent(46/49): loss=0.4579402440438075\n",
      "Gradient Descent(47/49): loss=0.45763266390415375\n",
      "Gradient Descent(48/49): loss=0.45733726393803\n",
      "Gradient Descent(49/49): loss=0.4570535618105642\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.50098044894051\n",
      "Gradient Descent(2/49): loss=0.5019806049047312\n",
      "Gradient Descent(3/49): loss=0.5030008640038225\n",
      "Gradient Descent(4/49): loss=0.5040416303108045\n",
      "Gradient Descent(5/49): loss=0.5051033160205661\n",
      "Gradient Descent(6/49): loss=0.5061863416130948\n",
      "Gradient Descent(7/49): loss=0.5072911360200199\n",
      "Gradient Descent(8/49): loss=0.5084181367945318\n",
      "Gradient Descent(9/49): loss=0.5095677902846103\n",
      "Gradient Descent(10/49): loss=0.510740551809849\n",
      "Gradient Descent(11/49): loss=0.5119368858417351\n",
      "Gradient Descent(12/49): loss=0.5131572661876723\n",
      "Gradient Descent(13/49): loss=0.5144021761785603\n",
      "Gradient Descent(14/49): loss=0.5156721088602602\n",
      "Gradient Descent(15/49): loss=0.5169675671888686\n",
      "Gradient Descent(16/49): loss=0.5182890642298781\n",
      "Gradient Descent(17/49): loss=0.5196371233614137\n",
      "Gradient Descent(18/49): loss=0.5210122784814887\n",
      "Gradient Descent(19/49): loss=0.522415074219472\n",
      "Gradient Descent(20/49): loss=0.5238460661517998\n",
      "Gradient Descent(21/49): loss=0.5253058210219621\n",
      "Gradient Descent(22/49): loss=0.5267949169650076\n",
      "Gradient Descent(23/49): loss=0.5283139437365136\n",
      "Gradient Descent(24/49): loss=0.529863502946132\n",
      "Gradient Descent(25/49): loss=0.531444208295851\n",
      "Gradient Descent(26/49): loss=0.5330566858231182\n",
      "Gradient Descent(27/49): loss=0.534701574148675\n",
      "Gradient Descent(28/49): loss=0.5363795247295758\n",
      "Gradient Descent(29/49): loss=0.5380912021171569\n",
      "Gradient Descent(30/49): loss=0.5398372842202251\n",
      "Gradient Descent(31/49): loss=0.5416184625735674\n",
      "Gradient Descent(32/49): loss=0.5434354426117917\n",
      "Gradient Descent(33/49): loss=0.5452889439488016\n",
      "Gradient Descent(34/49): loss=0.5471797006626858\n",
      "Gradient Descent(35/49): loss=0.5491084615865073\n",
      "Gradient Descent(36/49): loss=0.5510759906049097\n",
      "Gradient Descent(37/49): loss=0.5530830669565665\n",
      "Gradient Descent(38/49): loss=0.555130485542889\n",
      "Gradient Descent(39/49): loss=0.5572190572428056\n",
      "Gradient Descent(40/49): loss=0.5593496092338839\n",
      "Gradient Descent(41/49): loss=0.5615229853200006\n",
      "Gradient Descent(42/49): loss=0.5637400462654455\n",
      "Gradient Descent(43/49): loss=0.566001670135895\n",
      "Gradient Descent(44/49): loss=0.5683087526461232\n",
      "Gradient Descent(45/49): loss=0.5706622075148109\n",
      "Gradient Descent(46/49): loss=0.5730629668263695\n",
      "Gradient Descent(47/49): loss=0.575511981400104\n",
      "Gradient Descent(48/49): loss=0.5780102211667484\n",
      "Gradient Descent(49/49): loss=0.5805586755526942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5010050438622378\n",
      "Gradient Descent(2/49): loss=0.5020302891061099\n",
      "Gradient Descent(3/49): loss=0.5030761417793796\n",
      "Gradient Descent(4/49): loss=0.5041430160913866\n",
      "Gradient Descent(5/49): loss=0.5052313345770651\n",
      "Gradient Descent(6/49): loss=0.5063415282643055\n",
      "Gradient Descent(7/49): loss=0.5074740368446544\n",
      "Gradient Descent(8/49): loss=0.5086293088474704\n",
      "Gradient Descent(9/49): loss=0.5098078018175375\n",
      "Gradient Descent(10/49): loss=0.5110099824963111\n",
      "Gradient Descent(11/49): loss=0.5122363270067326\n",
      "Gradient Descent(12/49): loss=0.513487321041798\n",
      "Gradient Descent(13/49): loss=0.514763460056975\n",
      "Gradient Descent(14/49): loss=0.5160652494663488\n",
      "Gradient Descent(15/49): loss=0.5173932048428567\n",
      "Gradient Descent(16/49): loss=0.518747852122429\n",
      "Gradient Descent(17/49): loss=0.5201297278123186\n",
      "Gradient Descent(18/49): loss=0.521539379203582\n",
      "Gradient Descent(19/49): loss=0.522977364587806\n",
      "Gradient Descent(20/49): loss=0.5244442534782525\n",
      "Gradient Descent(21/49): loss=0.5259406268354032\n",
      "Gradient Descent(22/49): loss=0.5274670772970397\n",
      "Gradient Descent(23/49): loss=0.5290242094129395\n",
      "Gradient Descent(24/49): loss=0.5306126398843714\n",
      "Gradient Descent(25/49): loss=0.5322329978082864\n",
      "Gradient Descent(26/49): loss=0.5338859249264738\n",
      "Gradient Descent(27/49): loss=0.5355720758797259\n",
      "Gradient Descent(28/49): loss=0.5372921184671429\n",
      "Gradient Descent(29/49): loss=0.5390467339105631\n",
      "Gradient Descent(30/49): loss=0.5408366171244102\n",
      "Gradient Descent(31/49): loss=0.5426624769908585\n",
      "Gradient Descent(32/49): loss=0.5445250366406118\n",
      "Gradient Descent(33/49): loss=0.5464250337393209\n",
      "Gradient Descent(34/49): loss=0.5483632207797218\n",
      "Gradient Descent(35/49): loss=0.5503403653796344\n",
      "Gradient Descent(36/49): loss=0.5523572505859959\n",
      "Gradient Descent(37/49): loss=0.5544146751850174\n",
      "Gradient Descent(38/49): loss=0.5565134540184711\n",
      "Gradient Descent(39/49): loss=0.5586544183064849\n",
      "Gradient Descent(40/49): loss=0.5608384159766691\n",
      "Gradient Descent(41/49): loss=0.5630663120000482\n",
      "Gradient Descent(42/49): loss=0.5653389887334964\n",
      "Gradient Descent(43/49): loss=0.5676573462692708\n",
      "Gradient Descent(44/49): loss=0.5700223027915129\n",
      "Gradient Descent(45/49): loss=0.5724347949398624\n",
      "Gradient Descent(46/49): loss=0.5748957781803848\n",
      "Gradient Descent(47/49): loss=0.5774062271840451\n",
      "Gradient Descent(48/49): loss=0.5799671362126889\n",
      "Gradient Descent(49/49): loss=0.5825795195128094\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5009934159679997\n",
      "Gradient Descent(2/49): loss=0.5020067995969509\n",
      "Gradient Descent(3/49): loss=0.5030405522368446\n",
      "Gradient Descent(4/49): loss=0.5040950833048015\n",
      "Gradient Descent(5/49): loss=0.5051708104472232\n",
      "Gradient Descent(6/49): loss=0.5062681597052044\n",
      "Gradient Descent(7/49): loss=0.5073875656832739\n",
      "Gradient Descent(8/49): loss=0.5085294717215115\n",
      "Gradient Descent(9/49): loss=0.509694330071119\n",
      "Gradient Descent(10/49): loss=0.510882602073543\n",
      "Gradient Descent(11/49): loss=0.5120947583432155\n",
      "Gradient Descent(12/49): loss=0.5133312789539106\n",
      "Gradient Descent(13/49): loss=0.5145926536288793\n",
      "Gradient Descent(14/49): loss=0.5158793819348229\n",
      "Gradient Descent(15/49): loss=0.5171919734797078\n",
      "Gradient Descent(16/49): loss=0.5185309481146503\n",
      "Gradient Descent(17/49): loss=0.5198968361397523\n",
      "Gradient Descent(18/49): loss=0.5212901785141695\n",
      "Gradient Descent(19/49): loss=0.5227115270703019\n",
      "Gradient Descent(20/49): loss=0.5241614447324124\n",
      "Gradient Descent(21/49): loss=0.525640505739538\n",
      "Gradient Descent(22/49): loss=0.5271492958728965\n",
      "Gradient Descent(23/49): loss=0.528688412687936\n",
      "Gradient Descent(24/49): loss=0.5302584657509624\n",
      "Gradient Descent(25/49): loss=0.5318600768805581\n",
      "Gradient Descent(26/49): loss=0.5334938803938585\n",
      "Gradient Descent(27/49): loss=0.535160523357774\n",
      "Gradient Descent(28/49): loss=0.5368606658452764\n",
      "Gradient Descent(29/49): loss=0.5385949811967689\n",
      "Gradient Descent(30/49): loss=0.5403641562868252\n",
      "Gradient Descent(31/49): loss=0.5421688917961908\n",
      "Gradient Descent(32/49): loss=0.544009902489289\n",
      "Gradient Descent(33/49): loss=0.5458879174973125\n",
      "Gradient Descent(34/49): loss=0.5478036806070012\n",
      "Gradient Descent(35/49): loss=0.5497579505552007\n",
      "Gradient Descent(36/49): loss=0.5517515013293516\n",
      "Gradient Descent(37/49): loss=0.5537851224740814\n",
      "Gradient Descent(38/49): loss=0.555859619403799\n",
      "Gradient Descent(39/49): loss=0.5579758137218085\n",
      "Gradient Descent(40/49): loss=0.5601345435456193\n",
      "Gradient Descent(41/49): loss=0.5623366638388805\n",
      "Gradient Descent(42/49): loss=0.5645830467500352\n",
      "Gradient Descent(43/49): loss=0.5668745819577113\n",
      "Gradient Descent(44/49): loss=0.5692121770230458\n",
      "Gradient Descent(45/49): loss=0.5715967577492079\n",
      "Gradient Descent(46/49): loss=0.574029268547969\n",
      "Gradient Descent(47/49): loss=0.5765106728137953\n",
      "Gradient Descent(48/49): loss=0.5790419533053511\n",
      "Gradient Descent(49/49): loss=0.5816241125347917\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.50100152131712\n",
      "Gradient Descent(2/49): loss=0.5020231732127093\n",
      "Gradient Descent(3/49): loss=0.5030653603114094\n",
      "Gradient Descent(4/49): loss=0.5041284953707905\n",
      "Gradient Descent(5/49): loss=0.5052129994448687\n",
      "Gradient Descent(6/49): loss=0.5063193020508331\n",
      "Gradient Descent(7/49): loss=0.5074478413391752\n",
      "Gradient Descent(8/49): loss=0.5085990642672107\n",
      "Gradient Descent(9/49): loss=0.5097734267761065\n",
      "Gradient Descent(10/49): loss=0.5109713939714288\n",
      "Gradient Descent(11/49): loss=0.5121934403073757\n",
      "Gradient Descent(12/49): loss=0.5134400497746688\n",
      "Gradient Descent(13/49): loss=0.5147117160922531\n",
      "Gradient Descent(14/49): loss=0.5160089429028325\n",
      "Gradient Descent(15/49): loss=0.517332243972293\n",
      "Gradient Descent(16/49): loss=0.518682143393259\n",
      "Gradient Descent(17/49): loss=0.5200591757925814\n",
      "Gradient Descent(18/49): loss=0.5214638865431431\n",
      "Gradient Descent(19/49): loss=0.5228968319797789\n",
      "Gradient Descent(20/49): loss=0.5243585796196882\n",
      "Gradient Descent(21/49): loss=0.525849708387164\n",
      "Gradient Descent(22/49): loss=0.527370808842867\n",
      "Gradient Descent(23/49): loss=0.5289224834177413\n",
      "Gradient Descent(24/49): loss=0.5305053466515565\n",
      "Gradient Descent(25/49): loss=0.5321200254363708\n",
      "Gradient Descent(26/49): loss=0.5337671592647637\n",
      "Gradient Descent(27/49): loss=0.5354474004831057\n",
      "Gradient Descent(28/49): loss=0.5371614145499347\n",
      "Gradient Descent(29/49): loss=0.5389098802995015\n",
      "Gradient Descent(30/49): loss=0.5406934902106436\n",
      "Gradient Descent(31/49): loss=0.5425129506810028\n",
      "Gradient Descent(32/49): loss=0.5443689823068072\n",
      "Gradient Descent(33/49): loss=0.5462623201682879\n",
      "Gradient Descent(34/49): loss=0.5481937141208001\n",
      "Gradient Descent(35/49): loss=0.5501639290917469\n",
      "Gradient Descent(36/49): loss=0.5521737453836194\n",
      "Gradient Descent(37/49): loss=0.5542239589829615\n",
      "Gradient Descent(38/49): loss=0.5563153818756356\n",
      "Gradient Descent(39/49): loss=0.5584488423684664\n",
      "Gradient Descent(40/49): loss=0.5606251854171839\n",
      "Gradient Descent(41/49): loss=0.5628452729611904\n",
      "Gradient Descent(42/49): loss=0.5651099842648297\n",
      "Gradient Descent(43/49): loss=0.5674202162656785\n",
      "Gradient Descent(44/49): loss=0.5697768839297218\n",
      "Gradient Descent(45/49): loss=0.5721809206138344\n",
      "Gradient Descent(46/49): loss=0.5746332784352882\n",
      "Gradient Descent(47/49): loss=0.5771349286489619\n",
      "Gradient Descent(48/49): loss=0.5796868620319353\n",
      "Gradient Descent(49/49): loss=0.5822900892758857\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5039803300271445\n",
      "Gradient Descent(2/49): loss=0.5082854549844993\n",
      "Gradient Descent(3/49): loss=0.5129418781383736\n",
      "Gradient Descent(4/49): loss=0.5179782654216177\n",
      "Gradient Descent(5/49): loss=0.5234256219071718\n",
      "Gradient Descent(6/49): loss=0.5293174826819463\n",
      "Gradient Descent(7/49): loss=0.5356901192959428\n",
      "Gradient Descent(8/49): loss=0.5425827630576321\n",
      "Gradient Descent(9/49): loss=0.5500378465502751\n",
      "Gradient Descent(10/49): loss=0.5581012648559235\n",
      "Gradient Descent(11/49): loss=0.5668226580953194\n",
      "Gradient Descent(12/49): loss=0.5762557170230374\n",
      "Gradient Descent(13/49): loss=0.5864585135592667\n",
      "Gradient Descent(14/49): loss=0.5974938582928614\n",
      "Gradient Descent(15/49): loss=0.6094296871567019\n",
      "Gradient Descent(16/49): loss=0.6223394796558488\n",
      "Gradient Descent(17/49): loss=0.636302711222923\n",
      "Gradient Descent(18/49): loss=0.6514053424858667\n",
      "Gradient Descent(19/49): loss=0.6677403484598543\n",
      "Gradient Descent(20/49): loss=0.6854082909213359\n",
      "Gradient Descent(21/49): loss=0.7045179374876214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=0.7251869312137698\n",
      "Gradient Descent(23/49): loss=0.7475425148279313\n",
      "Gradient Descent(24/49): loss=0.7717223140650024\n",
      "Gradient Descent(25/49): loss=0.7978751849198975\n",
      "Gradient Descent(26/49): loss=0.8261621300364859\n",
      "Gradient Descent(27/49): loss=0.856757289874601\n",
      "Gradient Descent(28/49): loss=0.8898490147555065\n",
      "Gradient Descent(29/49): loss=0.9256410243866766\n",
      "Gradient Descent(30/49): loss=0.9643536620038068\n",
      "Gradient Descent(31/49): loss=1.0062252508504825\n",
      "Gradient Descent(32/49): loss=1.0515135613471014\n",
      "Gradient Descent(33/49): loss=1.1004973979801167\n",
      "Gradient Descent(34/49): loss=1.1534783156823756\n",
      "Gradient Descent(35/49): loss=1.2107824762691066\n",
      "Gradient Descent(36/49): loss=1.2727626563597374\n",
      "Gradient Descent(37/49): loss=1.3398004191458062\n",
      "Gradient Descent(38/49): loss=1.4123084633753786\n",
      "Gradient Descent(39/49): loss=1.4907331640140238\n",
      "Gradient Descent(40/49): loss=1.5755573202247672\n",
      "Gradient Descent(41/49): loss=1.6673031275823367\n",
      "Gradient Descent(42/49): loss=1.7665353928201246\n",
      "Gradient Descent(43/49): loss=1.8738650109012285\n",
      "Gradient Descent(44/49): loss=1.989952725818005\n",
      "Gradient Descent(45/49): loss=2.115513198271824\n",
      "Gradient Descent(46/49): loss=2.2513194052776826\n",
      "Gradient Descent(47/49): loss=2.398207398775673\n",
      "Gradient Descent(48/49): loss=2.5570814525430308\n",
      "Gradient Descent(49/49): loss=2.7289196290978768\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5040801780675924\n",
      "Gradient Descent(2/49): loss=0.5084932986655094\n",
      "Gradient Descent(3/49): loss=0.5132665299041992\n",
      "Gradient Descent(4/49): loss=0.5184292568119694\n",
      "Gradient Descent(5/49): loss=0.5240132622354146\n",
      "Gradient Descent(6/49): loss=0.5300529225014213\n",
      "Gradient Descent(7/49): loss=0.5365854190451217\n",
      "Gradient Descent(8/49): loss=0.5436509673067905\n",
      "Gradient Descent(9/49): loss=0.5512930643066095\n",
      "Gradient Descent(10/49): loss=0.5595587564216267\n",
      "Gradient Descent(11/49): loss=0.5684989290132255\n",
      "Gradient Descent(12/49): loss=0.5781686196883032\n",
      "Gradient Descent(13/49): loss=0.5886273571224505\n",
      "Gradient Descent(14/49): loss=0.5999395275312461\n",
      "Gradient Descent(15/49): loss=0.6121747710453731\n",
      "Gradient Descent(16/49): loss=0.6254084104302935\n",
      "Gradient Descent(17/49): loss=0.6397219147890076\n",
      "Gradient Descent(18/49): loss=0.655203401103375\n",
      "Gradient Descent(19/49): loss=0.6719481767010342\n",
      "Gradient Descent(20/49): loss=0.6900593259874029\n",
      "Gradient Descent(21/49): loss=0.7096483450555371\n",
      "Gradient Descent(22/49): loss=0.7308358280796418\n",
      "Gradient Descent(23/49): loss=0.7537522097184981\n",
      "Gradient Descent(24/49): loss=0.7785385680991088\n",
      "Gradient Descent(25/49): loss=0.8053474933236264\n",
      "Gradient Descent(26/49): loss=0.8343440268464818\n",
      "Gradient Descent(27/49): loss=0.8657066775047784\n",
      "Gradient Descent(28/49): loss=0.8996285204567795\n",
      "Gradient Descent(29/49): loss=0.9363183857936297\n",
      "Gradient Descent(30/49): loss=0.9760021441420127\n",
      "Gradient Descent(31/49): loss=1.0189240971715146\n",
      "Gradient Descent(32/49): loss=1.0653484815682552\n",
      "Gradient Descent(33/49): loss=1.115561095731765\n",
      "Gradient Descent(34/49): loss=1.1698710592111368\n",
      "Gradient Descent(35/49): loss=1.228612715710355\n",
      "Gradient Descent(36/49): loss=1.2921476913798111\n",
      "Gradient Descent(37/49): loss=1.3608671210639822\n",
      "Gradient Descent(38/49): loss=1.4351940562103387\n",
      "Gradient Descent(39/49): loss=1.515586069264729\n",
      "Gradient Descent(40/49): loss=1.6025380705843337\n",
      "Gradient Descent(41/49): loss=1.6965853552115722\n",
      "Gradient Descent(42/49): loss=1.7983068982645158\n",
      "Gradient Descent(43/49): loss=1.9083289192304118\n",
      "Gradient Descent(44/49): loss=2.027328737107128\n",
      "Gradient Descent(45/49): loss=2.1560389401226034\n",
      "Gradient Descent(46/49): loss=2.2952518957041463\n",
      "Gradient Descent(47/49): loss=2.445824628461225\n",
      "Gradient Descent(48/49): loss=2.608684096211273\n",
      "Gradient Descent(49/49): loss=2.784832896529918\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5040329722879999\n",
      "Gradient Descent(2/49): loss=0.5083950351146954\n",
      "Gradient Descent(3/49): loss=0.5131130422680596\n",
      "Gradient Descent(4/49): loss=0.5182160388051416\n",
      "Gradient Descent(5/49): loss=0.523735439859635\n",
      "Gradient Descent(6/49): loss=0.5297052240401928\n",
      "Gradient Descent(7/49): loss=0.53616214260987\n",
      "Gradient Descent(8/49): loss=0.5431459457348425\n",
      "Gradient Descent(9/49): loss=0.5506996271948148\n",
      "Gradient Descent(10/49): loss=0.5588696890619181\n",
      "Gradient Descent(11/49): loss=0.5677064279773585\n",
      "Gradient Descent(12/49): loss=0.577264244788304\n",
      "Gradient Descent(13/49): loss=0.5876019794510295\n",
      "Gradient Descent(14/49): loss=0.598783273262244\n",
      "Gradient Descent(15/49): loss=0.61087696064842\n",
      "Gradient Descent(16/49): loss=0.6239574929253314\n",
      "Gradient Descent(17/49): loss=0.6381053966360557\n",
      "Gradient Descent(18/49): loss=0.653407769289569\n",
      "Gradient Descent(19/49): loss=0.6699588155516105\n",
      "Gradient Descent(20/49): loss=0.6878604271886527\n",
      "Gradient Descent(21/49): loss=0.707222810335271\n",
      "Gradient Descent(22/49): loss=0.7281651639466282\n",
      "Gradient Descent(23/49): loss=0.7508164136126892\n",
      "Gradient Descent(24/49): loss=0.7753160052514776\n",
      "Gradient Descent(25/49): loss=0.8018147635679835\n",
      "Gradient Descent(26/49): loss=0.8304758205631609\n",
      "Gradient Descent(27/49): loss=0.8614756198090717\n",
      "Gradient Descent(28/49): loss=0.8950050026734648\n",
      "Gradient Descent(29/49): loss=0.9312703831796514\n",
      "Gradient Descent(30/49): loss=0.9704950187350497\n",
      "Gradient Descent(31/49): loss=1.0129203845517605\n",
      "Gradient Descent(32/49): loss=1.058807660219176\n",
      "Gradient Descent(33/49): loss=1.108439337581136\n",
      "Gradient Descent(34/49): loss=1.16212095981581\n",
      "Gradient Descent(35/49): loss=1.220183002424759\n",
      "Gradient Descent(36/49): loss=1.2829829077106951\n",
      "Gradient Descent(37/49): loss=1.35090728526779\n",
      "Gradient Descent(38/49): loss=1.4243742920336895\n",
      "Gradient Descent(39/49): loss=1.503836206551587\n",
      "Gradient Descent(40/49): loss=1.5897822132941106\n",
      "Gradient Descent(41/49): loss=1.6827414141870203\n",
      "Gradient Descent(42/49): loss=1.783286085872548\n",
      "Gradient Descent(43/49): loss=1.8920352027678757\n",
      "Gradient Descent(44/49): loss=2.009658247601668\n",
      "Gradient Descent(45/49): loss=2.1368793328938045\n",
      "Gradient Descent(46/49): loss=2.274481658745784\n",
      "Gradient Descent(47/49): loss=2.4233123343876195\n",
      "Gradient Descent(48/49): loss=2.584287593161829\n",
      "Gradient Descent(49/49): loss=2.758398433051533\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5040658775859201\n",
      "Gradient Descent(2/49): loss=0.5084635307828586\n",
      "Gradient Descent(3/49): loss=0.5132200324806567\n",
      "Gradient Descent(4/49): loss=0.5183646647170028\n",
      "Gradient Descent(5/49): loss=0.5239290989438282\n",
      "Gradient Descent(6/49): loss=0.5299475910035601\n",
      "Gradient Descent(7/49): loss=0.5364571920153656\n",
      "Gradient Descent(8/49): loss=0.5434979764697433\n",
      "Gradient Descent(9/49): loss=0.5511132889355925\n",
      "Gradient Descent(10/49): loss=0.5593500108986638\n",
      "Gradient Descent(11/49): loss=0.5682588493739044\n",
      "Gradient Descent(12/49): loss=0.5778946490687209\n",
      "Gradient Descent(13/49): loss=0.5883167300186504\n",
      "Gradient Descent(14/49): loss=0.5995892527740732\n",
      "Gradient Descent(15/49): loss=0.6117816133863374\n",
      "Gradient Descent(16/49): loss=0.6249688706245822\n",
      "Gradient Descent(17/49): loss=0.6392322080534905\n",
      "Gradient Descent(18/49): loss=0.6546594338165868\n",
      "Gradient Descent(19/49): loss=0.671345521201964\n",
      "Gradient Descent(20/49): loss=0.6893931933179895\n",
      "Gradient Descent(21/49): loss=0.7089135554786421\n",
      "Gradient Descent(22/49): loss=0.7300267791916376\n",
      "Gradient Descent(23/49): loss=0.7528628419595547\n",
      "Gradient Descent(24/49): loss=0.7775623274493592\n",
      "Gradient Descent(25/49): loss=0.8042772909551764\n",
      "Gradient Descent(26/49): loss=0.8331721954830391\n",
      "Gradient Descent(27/49): loss=0.8644249242204122\n",
      "Gradient Descent(28/49): loss=0.8982278756227193\n",
      "Gradient Descent(29/49): loss=0.9347891478594352\n",
      "Gradient Descent(30/49): loss=0.974333819910704\n",
      "Gradient Descent(31/49): loss=1.0171053372013008\n",
      "Gradient Descent(32/49): loss=1.063367010302811\n",
      "Gradient Descent(33/49): loss=1.1134036359294164\n",
      "Gradient Descent(34/49): loss=1.1675232502071118\n",
      "Gradient Descent(35/49): loss=1.2260590250099068\n",
      "Gradient Descent(36/49): loss=1.2893713190367013\n",
      "Gradient Descent(37/49): loss=1.3578498962559598\n",
      "Gradient Descent(38/49): loss=1.4319163253762917\n",
      "Gradient Descent(39/49): loss=1.512026575112974\n",
      "Gradient Descent(40/49): loss=1.5986738212281348\n",
      "Gradient Descent(41/49): loss=1.6923914826261295\n",
      "Gradient Descent(42/49): loss=1.7937565051942648\n",
      "Gradient Descent(43/49): loss=1.903392913603964\n",
      "Gradient Descent(44/49): loss=2.021975652939983\n",
      "Gradient Descent(45/49): loss=2.150234743805692\n",
      "Gradient Descent(46/49): loss=2.2889597764862044\n",
      "Gradient Descent(47/49): loss=2.439004771833543\n",
      "Gradient Descent(48/49): loss=2.6012934388008353\n",
      "Gradient Descent(49/49): loss=2.776824860992805\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5070680125114368\n",
      "Gradient Descent(2/49): loss=0.5151601800357763\n",
      "Gradient Descent(3/49): loss=0.5244249026344038\n",
      "Gradient Descent(4/49): loss=0.535032083537572\n",
      "Gradient Descent(5/49): loss=0.5471762449536026\n",
      "Gradient Descent(6/49): loss=0.5610800953588195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=0.5769986136877395\n",
      "Gradient Descent(8/49): loss=0.5952237253225452\n",
      "Gradient Descent(9/49): loss=0.6160896556332357\n",
      "Gradient Descent(10/49): loss=0.6399790592459068\n",
      "Gradient Descent(11/49): loss=0.6673300374420942\n",
      "Gradient Descent(12/49): loss=0.6986441723788718\n",
      "Gradient Descent(13/49): loss=0.7344957254679838\n",
      "Gradient Descent(14/49): loss=0.775542168599735\n",
      "Gradient Descent(15/49): loss=0.8225362413412418\n",
      "Gradient Descent(16/49): loss=0.8763397552229866\n",
      "Gradient Descent(17/49): loss=0.9379393982662348\n",
      "Gradient Descent(18/49): loss=1.0084648295864111\n",
      "Gradient Descent(19/49): loss=1.0892093959049993\n",
      "Gradient Descent(20/49): loss=1.1816538498831257\n",
      "Gradient Descent(21/49): loss=1.2874935052425265\n",
      "Gradient Descent(22/49): loss=1.4086693266636459\n",
      "Gradient Descent(23/49): loss=1.5474035246087414\n",
      "Gradient Descent(24/49): loss=1.706240307835898\n",
      "Gradient Descent(25/49): loss=1.8880925409527538\n",
      "Gradient Descent(26/49): loss=2.0962951626480466\n",
      "Gradient Descent(27/49): loss=2.3346663442273368\n",
      "Gradient Descent(28/49): loss=2.6075775100171934\n",
      "Gradient Descent(29/49): loss=2.920033503730312\n",
      "Gradient Descent(30/49): loss=3.2777643709325988\n",
      "Gradient Descent(31/49): loss=3.687330440791771\n",
      "Gradient Descent(32/49): loss=4.156242634173684\n",
      "Gradient Descent(33/49): loss=4.693100204377393\n",
      "Gradient Descent(34/49): loss=5.307748436502892\n",
      "Gradient Descent(35/49): loss=6.011459197462964\n",
      "Gradient Descent(36/49): loss=6.817137647686817\n",
      "Gradient Descent(37/49): loss=7.7395589053480816\n",
      "Gradient Descent(38/49): loss=8.795639003245174\n",
      "Gradient Descent(39/49): loss=10.004745107327222\n",
      "Gradient Descent(40/49): loss=11.389050685891807\n",
      "Gradient Descent(41/49): loss=12.97394214278815\n",
      "Gradient Descent(42/49): loss=14.788484371788758\n",
      "Gradient Descent(43/49): loss=16.865953769773782\n",
      "Gradient Descent(44/49): loss=19.24444848352644\n",
      "Gradient Descent(45/49): loss=21.967587081298188\n",
      "Gradient Descent(46/49): loss=25.085308461888747\n",
      "Gradient Descent(47/49): loss=28.6547876705278\n",
      "Gradient Descent(48/49): loss=32.74148441649683\n",
      "Gradient Descent(49/49): loss=37.420343520954845\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5072453162009091\n",
      "Gradient Descent(2/49): loss=0.5155404787193243\n",
      "Gradient Descent(3/49): loss=0.5250376102866556\n",
      "Gradient Descent(4/49): loss=0.5359108762181013\n",
      "Gradient Descent(5/49): loss=0.5483596783829998\n",
      "Gradient Descent(6/49): loss=0.5626123119816063\n",
      "Gradient Descent(7/49): loss=0.5789301521886392\n",
      "Gradient Descent(8/49): loss=0.5976124474416685\n",
      "Gradient Descent(9/49): loss=0.6190018072768644\n",
      "Gradient Descent(10/49): loss=0.6434904853522169\n",
      "Gradient Descent(11/49): loss=0.6715275728806425\n",
      "Gradient Descent(12/49): loss=0.703627234391996\n",
      "Gradient Descent(13/49): loss=0.7403781368562775\n",
      "Gradient Descent(14/49): loss=0.7824542450876272\n",
      "Gradient Descent(15/49): loss=0.8306271814017724\n",
      "Gradient Descent(16/49): loss=0.8857803761878364\n",
      "Gradient Descent(17/49): loss=0.9489252688983832\n",
      "Gradient Descent(18/49): loss=1.021219856562712\n",
      "Gradient Descent(19/49): loss=1.1039899299795775\n",
      "Gradient Descent(20/49): loss=1.198753387034506\n",
      "Gradient Descent(21/49): loss=1.3072480690166624\n",
      "Gradient Descent(22/49): loss=1.4314636304180512\n",
      "Gradient Descent(23/49): loss=1.5736780266665984\n",
      "Gradient Descent(24/49): loss=1.7364992889314192\n",
      "Gradient Descent(25/49): loss=1.9229133520985258\n",
      "Gradient Descent(26/49): loss=2.136338813018532\n",
      "Gradient Descent(27/49): loss=2.380689623225778\n",
      "Gradient Descent(28/49): loss=2.660446865832118\n",
      "Gradient Descent(29/49): loss=2.9807409328919197\n",
      "Gradient Descent(30/49): loss=3.3474456102686343\n",
      "Gradient Descent(31/49): loss=3.7672857953978167\n",
      "Gradient Descent(32/49): loss=4.247960823351677\n",
      "Gradient Descent(33/49): loss=4.798285662856721\n",
      "Gradient Descent(34/49): loss=5.428352571605948\n",
      "Gradient Descent(35/49): loss=6.1497161754325305\n",
      "Gradient Descent(36/49): loss=6.975605365453731\n",
      "Gradient Descent(37/49): loss=7.921165899109619\n",
      "Gradient Descent(38/49): loss=9.003738154090428\n",
      "Gradient Descent(39/49): loss=10.243175128818894\n",
      "Gradient Descent(40/49): loss=11.662206521184752\n",
      "Gradient Descent(41/49): loss=13.286855562306222\n",
      "Gradient Descent(42/49): loss=15.14691624948477\n",
      "Gradient Descent(43/49): loss=17.276499730234892\n",
      "Gradient Descent(44/49): loss=19.71465985734944\n",
      "Gradient Descent(45/49): loss=22.506109386878638\n",
      "Gradient Descent(46/49): loss=25.70203995324184\n",
      "Gradient Descent(47/49): loss=29.361060858666498\n",
      "Gradient Descent(48/49): loss=33.550273893289614\n",
      "Gradient Descent(49/49): loss=38.34650389662392\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5071614912319999\n",
      "Gradient Descent(2/49): loss=0.5153606825435131\n",
      "Gradient Descent(3/49): loss=0.5247479366760615\n",
      "Gradient Descent(4/49): loss=0.5354954039324281\n",
      "Gradient Descent(5/49): loss=0.5478001791942371\n",
      "Gradient Descent(6/49): loss=0.5618879163914859\n",
      "Gradient Descent(7/49): loss=0.5780169667085994\n",
      "Gradient Descent(8/49): loss=0.5964831164166682\n",
      "Gradient Descent(9/49): loss=0.6176250112174262\n",
      "Gradient Descent(10/49): loss=0.6418303665748242\n",
      "Gradient Descent(11/49): loss=0.6695430779235025\n",
      "Gradient Descent(12/49): loss=0.701271361146656\n",
      "Gradient Descent(13/49): loss=0.737597072608808\n",
      "Gradient Descent(14/49): loss=0.7791863796617955\n",
      "Gradient Descent(15/49): loss=0.8268019773068218\n",
      "Gradient Descent(16/49): loss=0.8813170750505698\n",
      "Gradient Descent(17/49): loss=0.9437314104573484\n",
      "Gradient Descent(18/49): loss=1.0151895830646112\n",
      "Gradient Descent(19/49): loss=1.0970020448826916\n",
      "Gradient Descent(20/49): loss=1.1906691324181393\n",
      "Gradient Descent(21/49): loss=1.2979085809375048\n",
      "Gradient Descent(22/49): loss=1.4206870255473982\n",
      "Gradient Descent(23/49): loss=1.5612560667813085\n",
      "Gradient Descent(24/49): loss=1.7221935620899425\n",
      "Gradient Descent(25/49): loss=1.906450900468663\n",
      "Gradient Descent(26/49): loss=2.117407127178656\n",
      "Gradient Descent(27/49): loss=2.3589309111385988\n",
      "Gradient Descent(28/49): loss=2.635451491394847\n",
      "Gradient Descent(29/49): loss=2.9520399037298555\n",
      "Gradient Descent(30/49): loss=3.314501977012104\n",
      "Gradient Descent(31/49): loss=3.729484804713474\n",
      "Gradient Descent(32/49): loss=4.20459864414818\n",
      "Gradient Descent(33/49): loss=4.748556478917298\n",
      "Gradient Descent(34/49): loss=5.371333803944127\n",
      "Gradient Descent(35/49): loss=6.084351563367347\n",
      "Gradient Descent(36/49): loss=6.900685596130944\n",
      "Gradient Descent(37/49): loss=7.835306430242106\n",
      "Gradient Descent(38/49): loss=8.905353823215766\n",
      "Gradient Descent(39/49): loss=10.130451083432533\n",
      "Gradient Descent(40/49): loss=11.53306493665507\n",
      "Gradient Descent(41/49): loss=13.138917537207666\n",
      "Gradient Descent(42/49): loss=14.977458179579628\n",
      "Gradient Descent(43/49): loss=17.082403361031748\n",
      "Gradient Descent(44/49): loss=19.49235509927838\n",
      "Gradient Descent(45/49): loss=22.251508844395573\n",
      "Gradient Descent(46/49): loss=25.410463967180952\n",
      "Gradient Descent(47/49): loss=29.027151687259067\n",
      "Gradient Descent(48/49): loss=33.16789745797221\n",
      "Gradient Descent(49/49): loss=37.908637290863304\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5072199223308799\n",
      "Gradient Descent(2/49): loss=0.5154860114075122\n",
      "Gradient Descent(3/49): loss=0.5249498567913405\n",
      "Gradient Descent(4/49): loss=0.5357850133712857\n",
      "Gradient Descent(5/49): loss=0.5481901841396624\n",
      "Gradient Descent(6/49): loss=0.5623928641523764\n",
      "Gradient Descent(7/49): loss=0.578653512498929\n",
      "Gradient Descent(8/49): loss=0.5972703287909114\n",
      "Gradient Descent(9/49): loss=0.6185847217636179\n",
      "Gradient Descent(10/49): loss=0.6429875702780499\n",
      "Gradient Descent(11/49): loss=0.6709263915421965\n",
      "Gradient Descent(12/49): loss=0.7029135480075713\n",
      "Gradient Descent(13/49): loss=0.7395356434447362\n",
      "Gradient Descent(14/49): loss=0.781464280510809\n",
      "Gradient Descent(15/49): loss=0.8294683770877066\n",
      "Gradient Descent(16/49): loss=0.8844282672585793\n",
      "Gradient Descent(17/49): loss=0.947351845515262\n",
      "Gradient Descent(18/49): loss=1.0193930502612665\n",
      "Gradient Descent(19/49): loss=1.1018730255749822\n",
      "Gradient Descent(20/49): loss=1.1963043493117165\n",
      "Gradient Descent(21/49): loss=1.3044187718578677\n",
      "Gradient Descent(22/49): loss=1.4281989742309193\n",
      "Gradient Descent(23/49): loss=1.5699149279279467\n",
      "Gradient Descent(24/49): loss=1.7321655233156312\n",
      "Gradient Descent(25/49): loss=1.917926229974888\n",
      "Gradient Descent(26/49): loss=2.1306036630288694\n",
      "Gradient Descent(27/49): loss=2.3740980561324307\n",
      "Gradient Descent(28/49): loss=2.652874786797036\n",
      "Gradient Descent(29/49): loss=2.9720462657346807\n",
      "Gradient Descent(30/49): loss=3.337465691970706\n",
      "Gradient Descent(31/49): loss=3.755834393068362\n",
      "Gradient Descent(32/49): loss=4.234824718954642\n",
      "Gradient Descent(33/49): loss=4.78322074306232\n",
      "Gradient Descent(34/49): loss=5.411079351063107\n",
      "Gradient Descent(35/49): loss=6.129914671363132\n",
      "Gradient Descent(36/49): loss=6.952909229574944\n",
      "Gradient Descent(37/49): loss=7.895155699271227\n",
      "Gradient Descent(38/49): loss=8.973933682427182\n",
      "Gradient Descent(39/49): loss=10.209026595342715\n",
      "Gradient Descent(40/49): loss=11.623084471337762\n",
      "Gradient Descent(41/49): loss=13.242039333565998\n",
      "Gradient Descent(42/49): loss=15.095580755331794\n",
      "Gradient Descent(43/49): loss=17.217700329111125\n",
      "Gradient Descent(44/49): loss=19.64731502912961\n",
      "Gradient Descent(45/49): loss=22.428980899182065\n",
      "Gradient Descent(46/49): loss=25.613710153807524\n",
      "Gradient Descent(47/49): loss=29.25990667742466\n",
      "Gradient Descent(48/49): loss=33.43443707731104\n",
      "Gradient Descent(49/49): loss=38.21385693214423\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5102434963933866\n",
      "Gradient Descent(2/49): loss=0.5226381270293847\n",
      "Gradient Descent(3/49): loss=0.5376356300989493\n",
      "Gradient Descent(4/49): loss=0.5557826088131148\n",
      "Gradient Descent(5/49): loss=0.5777404530572443\n",
      "Gradient Descent(6/49): loss=0.6043094445926619\n",
      "Gradient Descent(7/49): loss=0.6364579243505194\n",
      "Gradient Descent(8/49): loss=0.6753575848575422\n",
      "Gradient Descent(9/49): loss=0.7224261740710414\n",
      "Gradient Descent(10/49): loss=0.7793791670193595\n",
      "Gradient Descent(11/49): loss=0.8482922884867927\n",
      "Gradient Descent(12/49): loss=0.931677165462436\n",
      "Gradient Descent(13/49): loss=1.032572866602941\n",
      "Gradient Descent(14/49): loss=1.1546566649830408\n",
      "Gradient Descent(15/49): loss=1.3023780610229831\n",
      "Gradient Descent(16/49): loss=1.4811209502312785\n",
      "Gradient Descent(17/49): loss=1.6973998461731399\n",
      "Gradient Descent(18/49): loss=1.959097310262819\n",
      "Gradient Descent(19/49): loss=2.275751241811611\n",
      "Gradient Descent(20/49): loss=2.658902498985237\n",
      "Gradient Descent(21/49): loss=3.1225155201657007\n",
      "Gradient Descent(22/49): loss=3.683487275793981\n",
      "Gradient Descent(23/49): loss=4.3622631001036405\n",
      "Gradient Descent(24/49): loss=5.183581847519324\n",
      "Gradient Descent(25/49): loss=6.177377531891269\n",
      "Gradient Descent(26/49): loss=7.379870309981856\n",
      "Gradient Descent(27/49): loss=8.834886571471383\n",
      "Gradient Descent(28/49): loss=10.595456247873024\n",
      "Gradient Descent(29/49): loss=12.725745556321465\n",
      "Gradient Descent(30/49): loss=15.303395619541233\n",
      "Gradient Descent(31/49): loss=18.422352196040123\n",
      "Gradient Descent(32/49): loss=22.196289653604556\n",
      "Gradient Descent(33/49): loss=26.762753977253716\n",
      "Gradient Descent(34/49): loss=32.28817580887322\n",
      "Gradient Descent(35/49): loss=38.97393622512729\n",
      "Gradient Descent(36/49): loss=47.063706328800805\n",
      "Gradient Descent(37/49): loss=56.852328154239224\n",
      "Gradient Descent(38/49): loss=68.69656056302867\n",
      "Gradient Descent(39/49): loss=83.02808177766414\n",
      "Gradient Descent(40/49): loss=100.3692224473641\n",
      "Gradient Descent(41/49): loss=121.35200265769865\n",
      "Gradient Descent(42/49): loss=146.74116671222092\n",
      "Gradient Descent(43/49): loss=177.46205521817282\n",
      "Gradient Descent(44/49): loss=214.63433031035933\n",
      "Gradient Descent(45/49): loss=259.6127831719046\n",
      "Gradient Descent(46/49): loss=314.0367111343679\n",
      "Gradient Descent(47/49): loss=379.8896639689961\n",
      "Gradient Descent(48/49): loss=459.57173689889567\n",
      "Gradient Descent(49/49): loss=555.9870451440748\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5105004582621867\n",
      "Gradient Descent(2/49): loss=0.5232060127594291\n",
      "Gradient Descent(3/49): loss=0.5385797337011069\n",
      "Gradient Descent(4/49): loss=0.5571819360405236\n",
      "Gradient Descent(5/49): loss=0.5796906008712278\n",
      "Gradient Descent(6/49): loss=0.6069260853163835\n",
      "Gradient Descent(7/49): loss=0.6398810214949978\n",
      "Gradient Descent(8/49): loss=0.6797564942711463\n",
      "Gradient Descent(9/49): loss=0.7280058163302898\n",
      "Gradient Descent(10/49): loss=0.78638749602184\n",
      "Gradient Descent(11/49): loss=0.8570293284486014\n",
      "Gradient Descent(12/49): loss=0.9425059456850264\n",
      "Gradient Descent(13/49): loss=1.045932652541083\n",
      "Gradient Descent(14/49): loss=1.1710789678368307\n",
      "Gradient Descent(15/49): loss=1.322506009344692\n",
      "Gradient Descent(16/49): loss=1.5057327295692333\n",
      "Gradient Descent(17/49): loss=1.727437061040923\n",
      "Gradient Descent(18/49): loss=1.9956993021216465\n",
      "Gradient Descent(19/49): loss=2.3202966138293295\n",
      "Gradient Descent(20/49): loss=2.7130593609957523\n",
      "Gradient Descent(21/49): loss=3.1883022850667255\n",
      "Gradient Descent(22/49): loss=3.7633462231931656\n",
      "Gradient Descent(23/49): loss=4.459149388325536\n",
      "Gradient Descent(24/49): loss=5.301071218135908\n",
      "Gradient Descent(25/49): loss=6.319796632207496\n",
      "Gradient Descent(26/49): loss=7.552454383232876\n",
      "Gradient Descent(27/49): loss=9.043970261974856\n",
      "Gradient Descent(28/49): loss=10.848704475251528\n",
      "Gradient Descent(29/49): loss=13.032432873315559\n",
      "Gradient Descent(30/49): loss=15.674744234975543\n",
      "Gradient Descent(31/49): loss=18.871940982581997\n",
      "Gradient Descent(32/49): loss=22.740549047187532\n",
      "Gradient Descent(33/49): loss=27.421564805356965\n",
      "Gradient Descent(34/49): loss=33.085593872741086\n",
      "Gradient Descent(35/49): loss=39.93906904428156\n",
      "Gradient Descent(36/49): loss=48.23177400184096\n",
      "Gradient Descent(37/49): loss=58.265947000484076\n",
      "Gradient Descent(38/49): loss=70.40729632884722\n",
      "Gradient Descent(39/49): loss=85.09832901617659\n",
      "Gradient Descent(40/49): loss=102.87447856783052\n",
      "Gradient Descent(41/49): loss=124.38361952534174\n",
      "Gradient Descent(42/49): loss=150.40968008394296\n",
      "Gradient Descent(43/49): loss=181.90121335981314\n",
      "Gradient Descent(44/49): loss=220.00596862360462\n",
      "Gradient Descent(45/49): loss=266.1127224928159\n",
      "Gradient Descent(46/49): loss=321.90189467461295\n",
      "Gradient Descent(47/49): loss=389.40679301456134\n",
      "Gradient Descent(48/49): loss=471.0877200058908\n",
      "Gradient Descent(49/49): loss=569.9216416653463\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5103789728\n",
      "Gradient Descent(2/49): loss=0.522937529887994\n",
      "Gradient Descent(3/49): loss=0.5381333839644636\n",
      "Gradient Descent(4/49): loss=0.5565203673969994\n",
      "Gradient Descent(5/49): loss=0.5787686173503767\n",
      "Gradient Descent(6/49): loss=0.6056889997939768\n",
      "Gradient Descent(7/49): loss=0.6382626625507172\n",
      "Gradient Descent(8/49): loss=0.6776767944863883\n",
      "Gradient Descent(9/49): loss=0.7253678941285596\n",
      "Gradient Descent(10/49): loss=0.7830741246955564\n",
      "Gradient Descent(11/49): loss=0.8528986636816254\n",
      "Gradient Descent(12/49): loss=0.9373863558547978\n",
      "Gradient Descent(13/49): loss=1.0396164633842777\n",
      "Gradient Descent(14/49): loss=1.1633148934949242\n",
      "Gradient Descent(15/49): loss=1.31298999392886\n",
      "Gradient Descent(16/49): loss=1.4940968654539142\n",
      "Gradient Descent(17/49): loss=1.7132361799991926\n",
      "Gradient Descent(18/49): loss=1.9783947505991866\n",
      "Gradient Descent(19/49): loss=2.2992366210251514\n",
      "Gradient Descent(20/49): loss=2.6874552842404476\n",
      "Gradient Descent(21/49): loss=3.1571998667310326\n",
      "Gradient Descent(22/49): loss=3.725590811544591\n",
      "Gradient Descent(23/49): loss=4.4133438547692725\n",
      "Gradient Descent(24/49): loss=5.2455250370707285\n",
      "Gradient Descent(25/49): loss=6.25246426765511\n",
      "Gradient Descent(26/49): loss=7.470860736663022\n",
      "Gradient Descent(27/49): loss=8.945120464161457\n",
      "Gradient Descent(28/49): loss=10.728974734435662\n",
      "Gradient Descent(29/49): loss=12.887438401465841\n",
      "Gradient Descent(30/49): loss=15.499179438575862\n",
      "Gradient Descent(31/49): loss=18.659386093476158\n",
      "Gradient Descent(32/49): loss=22.48323614590615\n",
      "Gradient Descent(33/49): loss=27.110094709344526\n",
      "Gradient Descent(34/49): loss=32.708593571104515\n",
      "Gradient Descent(35/49): loss=39.48277719384045\n",
      "Gradient Descent(36/49): loss=47.67953937735004\n",
      "Gradient Descent(37/49): loss=57.59762161939868\n",
      "Gradient Descent(38/49): loss=69.59850113227483\n",
      "Gradient Descent(39/49): loss=84.11956534285605\n",
      "Gradient Descent(40/49): loss=101.69005303764146\n",
      "Gradient Descent(41/49): loss=122.95034314836113\n",
      "Gradient Descent(42/49): loss=148.67529418230527\n",
      "Gradient Descent(43/49): loss=179.80248493340616\n",
      "Gradient Descent(44/49): loss=217.46638574223462\n",
      "Gradient Descent(45/49): loss=263.03970572090105\n",
      "Gradient Descent(46/49): loss=318.18342289512316\n",
      "Gradient Descent(47/49): loss=384.90732067585634\n",
      "Gradient Descent(48/49): loss=465.6432369905539\n",
      "Gradient Descent(49/49): loss=563.3336957314222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.510463655552\n",
      "Gradient Descent(2/49): loss=0.5231246787699256\n",
      "Gradient Descent(3/49): loss=0.5384445168636226\n",
      "Gradient Descent(4/49): loss=0.5569815209569838\n",
      "Gradient Descent(5/49): loss=0.5794112959099529\n",
      "Gradient Descent(6/49): loss=0.6065513236030426\n",
      "Gradient Descent(7/49): loss=0.6393907571116999\n",
      "Gradient Descent(8/49): loss=0.6791264716571284\n",
      "Gradient Descent(9/49): loss=0.7272066862571639\n",
      "Gradient Descent(10/49): loss=0.7853837459231447\n",
      "Gradient Descent(11/49): loss=0.8557779881190363\n",
      "Gradient Descent(12/49): loss=0.9409550211759933\n",
      "Gradient Descent(13/49): loss=1.0440192311749492\n",
      "Gradient Descent(14/49): loss=1.1687269252736592\n",
      "Gradient Descent(15/49): loss=1.319623235133178\n",
      "Gradient Descent(16/49): loss=1.5022077700632528\n",
      "Gradient Descent(17/49): loss=1.7231350573284154\n",
      "Gradient Descent(18/49): loss=1.9904570749192831\n",
      "Gradient Descent(19/49): loss=2.3139167162043153\n",
      "Gradient Descent(20/49): loss=2.7053028821594407\n",
      "Gradient Descent(21/49): loss=3.1788801429646716\n",
      "Gradient Descent(22/49): loss=3.7519086285394128\n",
      "Gradient Descent(23/49): loss=4.445273096084375\n",
      "Gradient Descent(24/49): loss=5.2842441018140835\n",
      "Gradient Descent(25/49): loss=6.299399018747355\n",
      "Gradient Descent(26/49): loss=7.527736468236167\n",
      "Gradient Descent(27/49): loss=9.01402478211793\n",
      "Gradient Descent(28/49): loss=10.812433641915637\n",
      "Gradient Descent(29/49): loss=12.98850836226859\n",
      "Gradient Descent(30/49): loss=15.621558773897306\n",
      "Gradient Descent(31/49): loss=18.807549771965537\n",
      "Gradient Descent(32/49): loss=22.66259887963063\n",
      "Gradient Descent(33/49): loss=27.32720829990613\n",
      "Gradient Descent(34/49): loss=32.971385698437416\n",
      "Gradient Descent(35/49): loss=39.80084035065797\n",
      "Gradient Descent(36/49): loss=48.064480479843695\n",
      "Gradient Descent(37/49): loss=58.063485036155384\n",
      "Gradient Descent(38/49): loss=70.16228054930487\n",
      "Gradient Descent(39/49): loss=84.8018231202038\n",
      "Gradient Descent(40/49): loss=102.51566963099903\n",
      "Gradient Descent(41/49): loss=123.94942390906643\n",
      "Gradient Descent(42/49): loss=149.88426658551091\n",
      "Gradient Descent(43/49): loss=181.2654262240377\n",
      "Gradient Descent(44/49): loss=219.23662938663213\n",
      "Gradient Descent(45/49): loss=265.181785213345\n",
      "Gradient Descent(46/49): loss=320.77542376369263\n",
      "Gradient Descent(47/49): loss=388.0437264096258\n",
      "Gradient Descent(48/49): loss=469.4383726112152\n",
      "Gradient Descent(49/49): loss=567.9258945150596\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5135067816729944\n",
      "Gradient Descent(2/49): loss=0.5307535911912447\n",
      "Gradient Descent(3/49): loss=0.5527760422650982\n",
      "Gradient Descent(4/49): loss=0.580896510041293\n",
      "Gradient Descent(5/49): loss=0.6168035353447324\n",
      "Gradient Descent(6/49): loss=0.6626532159546905\n",
      "Gradient Descent(7/49): loss=0.7211986731255798\n",
      "Gradient Descent(8/49): loss=0.7959553673870441\n",
      "Gradient Descent(9/49): loss=0.8914121902894885\n",
      "Gradient Descent(10/49): loss=1.0133010074535782\n",
      "Gradient Descent(11/49): loss=1.1689408380904163\n",
      "Gradient Descent(12/49): loss=1.3676773378306004\n",
      "Gradient Descent(13/49): loss=1.621443974348935\n",
      "Gradient Descent(14/49): loss=1.9454785925193527\n",
      "Gradient Descent(15/49): loss=2.359238396461015\n",
      "Gradient Descent(16/49): loss=2.8875682901139044\n",
      "Gradient Descent(17/49): loss=3.5621927313193114\n",
      "Gradient Descent(18/49): loss=4.423620680294401\n",
      "Gradient Descent(19/49): loss=5.5235780283416265\n",
      "Gradient Descent(20/49): loss=6.928113566061461\n",
      "Gradient Descent(21/49): loss=8.721564994176942\n",
      "Gradient Descent(22/49): loss=11.011623122738147\n",
      "Gradient Descent(23/49): loss=13.935798347098755\n",
      "Gradient Descent(24/49): loss=17.6696776910847\n",
      "Gradient Descent(25/49): loss=22.437468225418687\n",
      "Gradient Descent(26/49): loss=28.525459958712492\n",
      "Gradient Descent(27/49): loss=36.29921660295219\n",
      "Gradient Descent(28/49): loss=46.22552646198059\n",
      "Gradient Descent(29/49): loss=58.900431520970734\n",
      "Gradient Descent(30/49): loss=75.08501779079084\n",
      "Gradient Descent(31/49): loss=95.75111599873928\n",
      "Gradient Descent(32/49): loss=122.13965680046144\n",
      "Gradient Descent(33/49): loss=155.8351845501952\n",
      "Gradient Descent(34/49): loss=198.86100393382645\n",
      "Gradient Descent(35/49): loss=253.80067270476678\n",
      "Gradient Descent(36/49): loss=323.9531357583751\n",
      "Gradient Descent(37/49): loss=413.5308158315151\n",
      "Gradient Descent(38/49): loss=527.9125555169272\n",
      "Gradient Descent(39/49): loss=673.9665989212956\n",
      "Gradient Descent(40/49): loss=860.4630069442535\n",
      "Gradient Descent(41/49): loss=1098.600270348718\n",
      "Gradient Descent(42/49): loss=1402.6777419898817\n",
      "Gradient Descent(43/49): loss=1790.9542655285934\n",
      "Gradient Descent(44/49): loss=2286.744558435228\n",
      "Gradient Descent(45/49): loss=2919.8191834474737\n",
      "Gradient Descent(46/49): loss=3728.192172125887\n",
      "Gradient Descent(47/49): loss=4760.403641368543\n",
      "Gradient Descent(48/49): loss=6078.434466445904\n",
      "Gradient Descent(49/49): loss=7761.428026986866\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5138456042514259\n",
      "Gradient Descent(2/49): loss=0.531525056320074\n",
      "Gradient Descent(3/49): loss=0.5540999486665211\n",
      "Gradient Descent(4/49): loss=0.5829258287037163\n",
      "Gradient Descent(5/49): loss=0.61973359492319\n",
      "Gradient Descent(6/49): loss=0.6667334316088372\n",
      "Gradient Descent(7/49): loss=0.7267475230727681\n",
      "Gradient Descent(8/49): loss=0.8033795164630209\n",
      "Gradient Descent(9/49): loss=0.9012309088230541\n",
      "Gradient Descent(10/49): loss=1.0261773517275536\n",
      "Gradient Descent(11/49): loss=1.1857214646723546\n",
      "Gradient Descent(12/49): loss=1.3894433424915935\n",
      "Gradient Descent(13/49): loss=1.6495758082790464\n",
      "Gradient Descent(14/49): loss=1.98173895384293\n",
      "Gradient Descent(15/49): loss=2.405878074413527\n",
      "Gradient Descent(16/49): loss=2.947461317470071\n",
      "Gradient Descent(17/49): loss=3.639008960528734\n",
      "Gradient Descent(18/49): loss=4.5220461459501795\n",
      "Gradient Descent(19/49): loss=5.6495963280156705\n",
      "Gradient Descent(20/49): loss=7.089365155494755\n",
      "Gradient Descent(21/49): loss=8.927805971302291\n",
      "Gradient Descent(22/49): loss=11.275311049006666\n",
      "Gradient Descent(23/49): loss=14.272840282727612\n",
      "Gradient Descent(24/49): loss=18.10038536126466\n",
      "Gradient Descent(25/49): loss=22.987777672051386\n",
      "Gradient Descent(26/49): loss=29.22848891369377\n",
      "Gradient Descent(27/49): loss=37.197253098150206\n",
      "Gradient Descent(28/49): loss=47.37256808527531\n",
      "Gradient Descent(29/49): loss=60.365427792335325\n",
      "Gradient Descent(30/49): loss=76.95601035229537\n",
      "Gradient Descent(31/49): loss=98.14052522310413\n",
      "Gradient Descent(32/49): loss=125.19103226161826\n",
      "Gradient Descent(33/49): loss=159.73182469912385\n",
      "Gradient Descent(34/49): loss=203.8369625625413\n",
      "Gradient Descent(35/49): loss=260.1548131003846\n",
      "Gradient Descent(36/49): loss=332.0670764521445\n",
      "Gradient Descent(37/49): loss=423.8918455260416\n",
      "Gradient Descent(38/49): loss=541.1428931564031\n",
      "Gradient Descent(39/49): loss=690.8607558757506\n",
      "Gradient Descent(40/49): loss=882.0354947821113\n",
      "Gradient Descent(41/49): loss=1126.146518891554\n",
      "Gradient Descent(42/49): loss=1437.8518855767413\n",
      "Gradient Descent(43/49): loss=1835.8684682972305\n",
      "Gradient Descent(44/49): loss=2344.095842772866\n",
      "Gradient Descent(45/49): loss=2993.0513772407653\n",
      "Gradient Descent(46/49): loss=3821.702699202972\n",
      "Gradient Descent(47/49): loss=4879.807572216861\n",
      "Gradient Descent(48/49): loss=6230.901684567725\n",
      "Gradient Descent(49/49): loss=7956.113756628405\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.513685416992\n",
      "Gradient Descent(2/49): loss=0.5311603259490797\n",
      "Gradient Descent(3/49): loss=0.5534740371963729\n",
      "Gradient Descent(4/49): loss=0.5819664150880495\n",
      "Gradient Descent(5/49): loss=0.6183483324179244\n",
      "Gradient Descent(6/49): loss=0.6648044026564358\n",
      "Gradient Descent(7/49): loss=0.7241241587440342\n",
      "Gradient Descent(8/49): loss=0.7998695552922748\n",
      "Gradient Descent(9/49): loss=0.896588852144721\n",
      "Gradient Descent(10/49): loss=1.0200897222955876\n",
      "Gradient Descent(11/49): loss=1.1777879833912153\n",
      "Gradient Descent(12/49): loss=1.3791528929842145\n",
      "Gradient Descent(13/49): loss=1.636275746043676\n",
      "Gradient Descent(14/49): loss=1.9645959171153133\n",
      "Gradient Descent(15/49): loss=2.3838279435565384\n",
      "Gradient Descent(16/49): loss=2.919145318119113\n",
      "Gradient Descent(17/49): loss=3.6026920736983445\n",
      "Gradient Descent(18/49): loss=4.475512925896949\n",
      "Gradient Descent(19/49): loss=5.590017872070214\n",
      "Gradient Descent(20/49): loss=7.013129237838596\n",
      "Gradient Descent(21/49): loss=8.830300140787356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=11.15064566676248\n",
      "Gradient Descent(23/49): loss=14.113494868882368\n",
      "Gradient Descent(24/49): loss=17.896757015065482\n",
      "Gradient Descent(25/49): loss=22.72760444953007\n",
      "Gradient Descent(26/49): loss=28.89611353859727\n",
      "Gradient Descent(27/49): loss=36.77268279442284\n",
      "Gradient Descent(28/49): loss=46.83027407719226\n",
      "Gradient Descent(29/49): loss=59.67281238615339\n",
      "Gradient Descent(30/49): loss=76.07144955287406\n",
      "Gradient Descent(31/49): loss=97.01086935104857\n",
      "Gradient Descent(32/49): loss=123.74841449134692\n",
      "Gradient Descent(33/49): loss=157.88958588100093\n",
      "Gradient Descent(34/49): loss=201.48444762846225\n",
      "Gradient Descent(35/49): loss=257.150726593762\n",
      "Gradient Descent(36/49): loss=328.2309982045398\n",
      "Gradient Descent(37/49): loss=418.9933970243803\n",
      "Gradient Descent(38/49): loss=534.8879040774068\n",
      "Gradient Descent(39/49): loss=682.8736001334507\n",
      "Gradient Descent(40/49): loss=871.8365354273059\n",
      "Gradient Descent(41/49): loss=1113.1233075040573\n",
      "Gradient Descent(42/49): loss=1421.2223867687762\n",
      "Gradient Descent(43/49): loss=1814.63410108221\n",
      "Gradient Descent(44/49): loss=2316.981519089056\n",
      "Gradient Descent(45/49): loss=2958.4289371416157\n",
      "Gradient Descent(46/49): loss=3777.4931452533588\n",
      "Gradient Descent(47/49): loss=4823.356232590667\n",
      "Gradient Descent(48/49): loss=6158.818808812256\n",
      "Gradient Descent(49/49): loss=7864.070972389368\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5137970772492801\n",
      "Gradient Descent(2/49): loss=0.5314145651888931\n",
      "Gradient Descent(3/49): loss=0.5539103355389714\n",
      "Gradient Descent(4/49): loss=0.5826351846989989\n",
      "Gradient Descent(5/49): loss=0.6193139445914171\n",
      "Gradient Descent(6/49): loss=0.6661490530980475\n",
      "Gradient Descent(7/49): loss=0.725952803150193\n",
      "Gradient Descent(8/49): loss=0.8023162115917438\n",
      "Gradient Descent(9/49): loss=0.899824647830759\n",
      "Gradient Descent(10/49): loss=1.0243331700643892\n",
      "Gradient Descent(11/49): loss=1.1833181021044554\n",
      "Gradient Descent(12/49): loss=1.386325961826498\n",
      "Gradient Descent(13/49): loss=1.6455466979055957\n",
      "Gradient Descent(14/49): loss=1.9765456558050127\n",
      "Gradient Descent(15/49): loss=2.3991982251466806\n",
      "Gradient Descent(16/49): loss=2.938883290938936\n",
      "Gradient Descent(17/49): loss=3.6280071514490975\n",
      "Gradient Descent(18/49): loss=4.50794940893444\n",
      "Gradient Descent(19/49): loss=5.631547677518144\n",
      "Gradient Descent(20/49): loss=7.066270306672105\n",
      "Gradient Descent(21/49): loss=8.898267631838396\n",
      "Gradient Descent(22/49): loss=11.23754501634336\n",
      "Gradient Descent(23/49): loss=14.224568308616476\n",
      "Gradient Descent(24/49): loss=18.038698350519436\n",
      "Gradient Descent(25/49): loss=22.90896100102894\n",
      "Gradient Descent(26/49): loss=29.1277993794654\n",
      "Gradient Descent(27/49): loss=37.06863410489173\n",
      "Gradient Descent(28/49): loss=47.208285965779574\n",
      "Gradient Descent(29/49): loss=60.15560742694813\n",
      "Gradient Descent(30/49): loss=76.68804220071496\n",
      "Gradient Descent(31/49): loss=97.79830816333416\n",
      "Gradient Descent(32/49): loss=124.7540067709961\n",
      "Gradient Descent(33/49): loss=159.17373832312933\n",
      "Gradient Descent(34/49): loss=203.1242935420348\n",
      "Gradient Descent(35/49): loss=259.2447575011056\n",
      "Gradient Descent(36/49): loss=330.9049779304296\n",
      "Gradient Descent(37/49): loss=422.4079133966539\n",
      "Gradient Descent(38/49): loss=539.248011693421\n",
      "Gradient Descent(39/49): loss=688.4411332085814\n",
      "Gradient Descent(40/49): loss=878.945830071339\n",
      "Gradient Descent(41/49): loss=1122.2012774954126\n",
      "Gradient Descent(42/49): loss=1432.8141583112752\n",
      "Gradient Descent(43/49): loss=1829.4357458246673\n",
      "Gradient Descent(44/49): loss=2335.881850920774\n",
      "Gradient Descent(45/49): loss=2982.562882518177\n",
      "Gradient Descent(46/49): loss=3808.309891764449\n",
      "Gradient Descent(47/49): loss=4862.706247871959\n",
      "Gradient Descent(48/49): loss=6209.0649549846285\n",
      "Gradient Descent(49/49): loss=7928.230388097859\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5168578683502594\n",
      "Gradient Descent(2/49): loss=0.5395418160023755\n",
      "Gradient Descent(3/49): loss=0.5700653359630625\n",
      "Gradient Descent(4/49): loss=0.6111377844221507\n",
      "Gradient Descent(5/49): loss=0.6664048710687255\n",
      "Gradient Descent(6/49): loss=0.7407722628603736\n",
      "Gradient Descent(7/49): loss=0.8408410252551629\n",
      "Gradient Descent(8/49): loss=0.975493551933593\n",
      "Gradient Descent(9/49): loss=1.1566819918320568\n",
      "Gradient Descent(10/49): loss=1.400489156559559\n",
      "Gradient Descent(11/49): loss=1.728556077416919\n",
      "Gradient Descent(12/49): loss=2.1700029261226144\n",
      "Gradient Descent(13/49): loss=2.764013805740687\n",
      "Gradient Descent(14/49): loss=3.5633148453547445\n",
      "Gradient Descent(15/49): loss=4.638854324259959\n",
      "Gradient Descent(16/49): loss=6.086100247074948\n",
      "Gradient Descent(17/49): loss=8.033514360815131\n",
      "Gradient Descent(18/49): loss=10.65395479226322\n",
      "Gradient Descent(19/49): loss=14.180019436819451\n",
      "Gradient Descent(20/49): loss=18.924692022532938\n",
      "Gradient Descent(21/49): loss=25.30912345387073\n",
      "Gradient Descent(22/49): loss=33.90001438787771\n",
      "Gradient Descent(23/49): loss=45.45991722867579\n",
      "Gradient Descent(24/49): loss=61.01492249126195\n",
      "Gradient Descent(25/49): loss=81.94573757258476\n",
      "Gradient Descent(26/49): loss=110.11024234602382\n",
      "Gradient Descent(27/49): loss=148.00839996917566\n",
      "Gradient Descent(28/49): loss=199.00416086690024\n",
      "Gradient Descent(29/49): loss=267.6240567308781\n",
      "Gradient Descent(30/49): loss=359.95898860538864\n",
      "Gradient Descent(31/49): loss=484.2048729358015\n",
      "Gradient Descent(32/49): loss=651.3901348907676\n",
      "Gradient Descent(33/49): loss=876.3546233773156\n",
      "Gradient Descent(34/49): loss=1179.066839084911\n",
      "Gradient Descent(35/49): loss=1586.3963965410057\n",
      "Gradient Descent(36/49): loss=2134.4990490540968\n",
      "Gradient Descent(37/49): loss=2872.025978275828\n",
      "Gradient Descent(38/49): loss=3864.442214235961\n",
      "Gradient Descent(39/49): loss=5199.837501344193\n",
      "Gradient Descent(40/49): loss=6996.745399676903\n",
      "Gradient Descent(41/49): loss=9414.664667674331\n",
      "Gradient Descent(42/49): loss=12668.21683469122\n",
      "Gradient Descent(43/49): loss=17046.19663063012\n",
      "Gradient Descent(44/49): loss=22937.20624404282\n",
      "Gradient Descent(45/49): loss=30864.1487798527\n",
      "Gradient Descent(46/49): loss=41530.64265603352\n",
      "Gradient Descent(47/49): loss=55883.476815828224\n",
      "Gradient Descent(48/49): loss=75196.65046125361\n",
      "Gradient Descent(49/49): loss=101184.4569185282\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5172807541686273\n",
      "Gradient Descent(2/49): loss=0.540533736977937\n",
      "Gradient Descent(3/49): loss=0.5718229506461262\n",
      "Gradient Descent(4/49): loss=0.6139257165580337\n",
      "Gradient Descent(5/49): loss=0.6705791983690975\n",
      "Gradient Descent(6/49): loss=0.7468121234940679\n",
      "Gradient Descent(7/49): loss=0.8493911475422291\n",
      "Gradient Descent(8/49): loss=0.9874214823014753\n",
      "Gradient Descent(9/49): loss=1.1731551007534384\n",
      "Gradient Descent(10/49): loss=1.4230782577424113\n",
      "Gradient Descent(11/49): loss=1.7593748577869568\n",
      "Gradient Descent(12/49): loss=2.2118955628066757\n",
      "Gradient Descent(13/49): loss=2.8208074234814124\n",
      "Gradient Descent(14/49): loss=3.6401592232055386\n",
      "Gradient Descent(15/49): loss=4.742679004913874\n",
      "Gradient Descent(16/49): loss=6.226229623180707\n",
      "Gradient Descent(17/49): loss=8.222495335120623\n",
      "Gradient Descent(18/49): loss=10.908670477108265\n",
      "Gradient Descent(19/49): loss=14.523187748165364\n",
      "Gradient Descent(20/49): loss=19.386882188101012\n",
      "Gradient Descent(21/49): loss=25.93146942647866\n",
      "Gradient Descent(22/49): loss=34.73786601443582\n",
      "Gradient Descent(23/49): loss=46.58775326319539\n",
      "Gradient Descent(24/49): loss=62.532961545128366\n",
      "Gradient Descent(25/49): loss=83.988833809305\n",
      "Gradient Descent(26/49): loss=112.85985552798087\n",
      "Gradient Descent(27/49): loss=151.70870235260972\n",
      "Gradient Descent(28/49): loss=203.98371063982134\n",
      "Gradient Descent(29/49): loss=274.3249617911319\n",
      "Gradient Descent(30/49): loss=368.97614934033635\n",
      "Gradient Descent(31/49): loss=496.3387873065769\n",
      "Gradient Descent(32/49): loss=667.7179529539826\n",
      "Gradient Descent(33/49): loss=898.3257582490016\n",
      "Gradient Descent(34/49): loss=1208.6316210540974\n",
      "Gradient Descent(35/49): loss=1626.1791900444705\n",
      "Gradient Descent(36/49): loss=2188.031198877903\n",
      "Gradient Descent(37/49): loss=2944.059261964497\n",
      "Gradient Descent(38/49): loss=3961.3706236534595\n",
      "Gradient Descent(39/49): loss=5330.2647919427445\n",
      "Gradient Descent(40/49): loss=7172.248784792681\n",
      "Gradient Descent(41/49): loss=9650.822445571335\n",
      "Gradient Descent(42/49): loss=12985.991163515502\n",
      "Gradient Descent(43/49): loss=17473.794190379154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=23512.58194333134\n",
      "Gradient Descent(45/49): loss=31638.37474370027\n",
      "Gradient Descent(46/49): loss=42572.44153587598\n",
      "Gradient Descent(47/49): loss=57285.32181142579\n",
      "Gradient Descent(48/49): loss=77082.97351020355\n",
      "Gradient Descent(49/49): loss=103722.6936360831\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.517080823808\n",
      "Gradient Descent(2/49): loss=0.5400647803240444\n",
      "Gradient Descent(3/49): loss=0.570991992212039\n",
      "Gradient Descent(4/49): loss=0.6126076485285027\n",
      "Gradient Descent(5/49): loss=0.6686056756679412\n",
      "Gradient Descent(6/49): loss=0.7439566209867655\n",
      "Gradient Descent(7/49): loss=0.8453488530078117\n",
      "Gradient Descent(8/49): loss=0.981782240415268\n",
      "Gradient Descent(9/49): loss=1.1653670065107393\n",
      "Gradient Descent(10/49): loss=1.4123986677689813\n",
      "Gradient Descent(11/49): loss=1.7448044711578308\n",
      "Gradient Descent(12/49): loss=2.1920897201982\n",
      "Gradient Descent(13/49): loss=2.7939567513066845\n",
      "Gradient Descent(14/49): loss=3.603829028366007\n",
      "Gradient Descent(15/49): loss=4.693593164377011\n",
      "Gradient Descent(16/49): loss=6.159979785793546\n",
      "Gradient Descent(17/49): loss=8.133149623572118\n",
      "Gradient Descent(18/49): loss=10.78824695728612\n",
      "Gradient Descent(19/49): loss=14.360945929532779\n",
      "Gradient Descent(20/49): loss=19.168369666588628\n",
      "Gradient Descent(21/49): loss=25.637239047168404\n",
      "Gradient Descent(22/49): loss=34.34174968567823\n",
      "Gradient Descent(23/49): loss=46.05453920085378\n",
      "Gradient Descent(24/49): loss=61.815268772472265\n",
      "Gradient Descent(25/49): loss=83.02290648405166\n",
      "Gradient Descent(26/49): loss=111.55990378873905\n",
      "Gradient Descent(27/49): loss=149.95928736192704\n",
      "Gradient Descent(28/49): loss=201.62949789800123\n",
      "Gradient Descent(29/49): loss=271.1569331953794\n",
      "Gradient Descent(30/49): loss=364.7130501315125\n",
      "Gradient Descent(31/49): loss=490.60216108080647\n",
      "Gradient Descent(32/49): loss=659.9985487741097\n",
      "Gradient Descent(33/49): loss=887.9383280543059\n",
      "Gradient Descent(34/49): loss=1194.6540950537512\n",
      "Gradient Descent(35/49): loss=1607.3708311281823\n",
      "Gradient Descent(36/49): loss=2162.7224711898516\n",
      "Gradient Descent(37/49): loss=2910.0036380569136\n",
      "Gradient Descent(38/49): loss=3915.545176193536\n",
      "Gradient Descent(39/49): loss=5268.601869909427\n",
      "Gradient Descent(40/49): loss=7089.274956973386\n",
      "Gradient Descent(41/49): loss=9539.172662927584\n",
      "Gradient Descent(42/49): loss=12835.75501606031\n",
      "Gradient Descent(43/49): loss=17271.636230436034\n",
      "Gradient Descent(44/49): loss=23240.557992501657\n",
      "Gradient Descent(45/49): loss=31272.33911553199\n",
      "Gradient Descent(46/49): loss=42079.90379468209\n",
      "Gradient Descent(47/49): loss=56622.56282694555\n",
      "Gradient Descent(48/49): loss=76191.16482075784\n",
      "Gradient Descent(49/49): loss=102522.67566364586\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5172201874227201\n",
      "Gradient Descent(2/49): loss=0.5403916716187357\n",
      "Gradient Descent(3/49): loss=0.5715712207528891\n",
      "Gradient Descent(4/49): loss=0.6135264220678243\n",
      "Gradient Descent(5/49): loss=0.669981340957199\n",
      "Gradient Descent(6/49): loss=0.7459470798147609\n",
      "Gradient Descent(7/49): loss=0.8481665780214639\n",
      "Gradient Descent(8/49): loss=0.9857131348084189\n",
      "Gradient Descent(9/49): loss=1.1707957816210188\n",
      "Gradient Descent(10/49): loss=1.419842991171848\n",
      "Gradient Descent(11/49): loss=1.754960916343583\n",
      "Gradient Descent(12/49): loss=2.2058955964548055\n",
      "Gradient Descent(13/49): loss=2.8126733020124477\n",
      "Gradient Descent(14/49): loss=3.629153382610809\n",
      "Gradient Descent(15/49): loss=4.727808979063564\n",
      "Gradient Descent(16/49): loss=6.206159949650936\n",
      "Gradient Descent(17/49): loss=8.195429015673433\n",
      "Gradient Descent(18/49): loss=10.872189470912078\n",
      "Gradient Descent(19/49): loss=14.474038339481675\n",
      "Gradient Descent(20/49): loss=19.320686177027895\n",
      "Gradient Descent(21/49): loss=25.84233550723297\n",
      "Gradient Descent(22/49): loss=34.61786684595508\n",
      "Gradient Descent(23/49): loss=46.42622181534172\n",
      "Gradient Descent(24/49): loss=62.31554426214902\n",
      "Gradient Descent(25/49): loss=83.69621654656716\n",
      "Gradient Descent(26/49): loss=112.46604917249192\n",
      "Gradient Descent(27/49): loss=151.17873595393564\n",
      "Gradient Descent(28/49): loss=203.27052728704047\n",
      "Gradient Descent(29/49): loss=273.3652417048836\n",
      "Gradient Descent(30/49): loss=367.68468942554705\n",
      "Gradient Descent(31/49): loss=494.6009382784105\n",
      "Gradient Descent(32/49): loss=665.3794427348438\n",
      "Gradient Descent(33/49): loss=895.178998331364\n",
      "Gradient Descent(34/49): loss=1204.3972803422257\n",
      "Gradient Descent(35/49): loss=1620.4814006159954\n",
      "Gradient Descent(36/49): loss=2180.364192856181\n",
      "Gradient Descent(37/49): loss=2933.742478094921\n",
      "Gradient Descent(38/49): loss=3947.488298712241\n",
      "Gradient Descent(39/49): loss=5311.584674934344\n",
      "Gradient Descent(40/49): loss=7147.112758778923\n",
      "Gradient Descent(41/49): loss=9616.999348401383\n",
      "Gradient Descent(42/49): loss=12940.47874339505\n",
      "Gradient Descent(43/49): loss=17412.552617300833\n",
      "Gradient Descent(44/49): loss=23430.175222027818\n",
      "Gradient Descent(45/49): loss=31527.488198949915\n",
      "Gradient Descent(46/49): loss=42423.23254068933\n",
      "Gradient Descent(47/49): loss=57084.54612693691\n",
      "Gradient Descent(48/49): loss=76812.80968859623\n",
      "Gradient Descent(49/49): loss=103359.16113716192\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5202967564251818\n",
      "Gradient Descent(2/49): loss=0.5490389931988678\n",
      "Gradient Descent(3/49): loss=0.5897408746940926\n",
      "Gradient Descent(4/49): loss=0.6473788090794994\n",
      "Gradient Descent(5/49): loss=0.7289998879626375\n",
      "Gradient Descent(6/49): loss=0.8445834977690835\n",
      "Gradient Descent(7/49): loss=1.00826144761597\n",
      "Gradient Descent(8/49): loss=1.24004579239407\n",
      "Gradient Descent(9/49): loss=1.5682756030343261\n",
      "Gradient Descent(10/49): loss=2.033081837882025\n",
      "Gradient Descent(11/49): loss=2.6912939470497625\n",
      "Gradient Descent(12/49): loss=3.623388114842215\n",
      "Gradient Descent(13/49): loss=4.943326665852961\n",
      "Gradient Descent(14/49): loss=6.812491647938981\n",
      "Gradient Descent(15/49): loss=9.459416179071845\n",
      "Gradient Descent(16/49): loss=13.207726007607425\n",
      "Gradient Descent(17/49): loss=18.515707555799548\n",
      "Gradient Descent(18/49): loss=26.032340226195704\n",
      "Gradient Descent(19/49): loss=36.676643750740205\n",
      "Gradient Descent(20/49): loss=51.75004197185277\n",
      "Gradient Descent(21/49): loss=73.0954811927641\n",
      "Gradient Descent(22/49): loss=103.32275767349988\n",
      "Gradient Descent(23/49): loss=146.12760389785404\n",
      "Gradient Descent(24/49): loss=206.7435466361604\n",
      "Gradient Descent(25/49): loss=292.58178314786767\n",
      "Gradient Descent(26/49): loss=414.1373098721334\n",
      "Gradient Descent(27/49): loss=586.2720912663484\n",
      "Gradient Descent(28/49): loss=830.0321551987195\n",
      "Gradient Descent(29/49): loss=1175.2207817334627\n",
      "Gradient Descent(30/49): loss=1664.0423957691526\n",
      "Gradient Descent(31/49): loss=2356.2626834050193\n",
      "Gradient Descent(32/49): loss=3336.5158327261875\n",
      "Gradient Descent(33/49): loss=4724.652317480383\n",
      "Gradient Descent(34/49): loss=6690.392393540466\n",
      "Gradient Descent(35/49): loss=9474.076915250404\n",
      "Gradient Descent(36/49): loss=13416.052566440829\n",
      "Gradient Descent(37/49): loss=18998.284286095157\n",
      "Gradient Descent(38/49): loss=26903.282624294483\n",
      "Gradient Descent(39/49): loss=38097.550771017544\n",
      "Gradient Descent(40/49): loss=53949.75389359556\n",
      "Gradient Descent(41/49): loss=76398.05873547324\n",
      "Gradient Descent(42/49): loss=108187.10322206478\n",
      "Gradient Descent(43/49): loss=153203.56911951493\n",
      "Gradient Descent(44/49): loss=216951.38647690727\n",
      "Gradient Descent(45/49): loss=307224.6706367199\n",
      "Gradient Descent(46/49): loss=435060.66833536304\n",
      "Gradient Descent(47/49): loss=616089.2246764523\n",
      "Gradient Descent(48/49): loss=872443.7633110516\n",
      "Gradient Descent(49/49): loss=1235467.4254715948\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5208059080137896\n",
      "Gradient Descent(2/49): loss=0.5502691543521279\n",
      "Gradient Descent(3/49): loss=0.5919920574918371\n",
      "Gradient Descent(4/49): loss=0.6510758606279953\n",
      "Gradient Descent(5/49): loss=0.7347444342491277\n",
      "Gradient Descent(6/49): loss=0.853227501353956\n",
      "Gradient Descent(7/49): loss=1.0210113726811254\n",
      "Gradient Descent(8/49): loss=1.2586101128675733\n",
      "Gradient Descent(9/49): loss=1.5950736888454053\n",
      "Gradient Descent(10/49): loss=2.0715397587876927\n",
      "Gradient Descent(11/49): loss=2.7462633604332387\n",
      "Gradient Descent(12/49): loss=3.701739452723107\n",
      "Gradient Descent(13/49): loss=5.054789147015129\n",
      "Gradient Descent(14/49): loss=6.970842819102466\n",
      "Gradient Descent(15/49): loss=9.684166424145628\n",
      "Gradient Descent(16/49): loss=13.526503981246824\n",
      "Gradient Descent(17/49): loss=18.967638195855898\n",
      "Gradient Descent(18/49): loss=26.672828357166583\n",
      "Gradient Descent(19/49): loss=37.584148144596654\n",
      "Gradient Descent(20/49): loss=53.035668095581364\n",
      "Gradient Descent(21/49): loss=74.91656549816116\n",
      "Gradient Descent(22/49): loss=105.9021043099666\n",
      "Gradient Descent(23/49): loss=149.78072582134507\n",
      "Gradient Descent(24/49): loss=211.9172417436293\n",
      "Gradient Descent(25/49): loss=299.908761941145\n",
      "Gradient Descent(26/49): loss=424.5135536928548\n",
      "Gradient Descent(27/49): loss=600.9663992924678\n",
      "Gradient Descent(28/49): loss=850.8412739460716\n",
      "Gradient Descent(29/49): loss=1204.689083943177\n",
      "Gradient Descent(30/49): loss=1705.7729676799522\n",
      "Gradient Descent(31/49): loss=2415.3578554395694\n",
      "Gradient Descent(32/49): loss=3420.2010149957346\n",
      "Gradient Descent(33/49): loss=4843.159413243133\n",
      "Gradient Descent(34/49): loss=6858.2108010019265\n",
      "Gradient Descent(35/49): loss=9711.725071206325\n",
      "Gradient Descent(36/49): loss=13752.58662924172\n",
      "Gradient Descent(37/49): loss=19474.850681578915\n",
      "Gradient Descent(38/49): loss=27578.148806091867\n",
      "Gradient Descent(39/49): loss=39053.22928021872\n",
      "Gradient Descent(40/49): loss=55303.09073961985\n",
      "Gradient Descent(41/49): loss=78314.51955227488\n",
      "Gradient Descent(42/49): loss=110901.00389388572\n",
      "Gradient Descent(43/49): loss=157046.72437002833\n",
      "Gradient Descent(44/49): loss=222393.67913631664\n",
      "Gradient Descent(45/49): loss=314931.50178086036\n",
      "Gradient Descent(46/49): loss=445974.31242781057\n",
      "Gradient Descent(47/49): loss=631544.0365849141\n",
      "Gradient Descent(48/49): loss=894329.3229636936\n",
      "Gradient Descent(49/49): loss=1266459.5670046147\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5205651932480003\n",
      "Gradient Descent(2/49): loss=0.549687563406492\n",
      "Gradient Descent(3/49): loss=0.5909277517879306\n",
      "Gradient Descent(4/49): loss=0.6493279825548983\n",
      "Gradient Descent(5/49): loss=0.7320285493439547\n",
      "Gradient Descent(6/49): loss=0.8491408219740149\n",
      "Gradient Descent(7/49): loss=1.0149835112454055\n",
      "Gradient Descent(8/49): loss=1.24983334352269\n",
      "Gradient Descent(9/49): loss=1.5824041910104074\n",
      "Gradient Descent(10/49): loss=2.053357768137877\n",
      "Gradient Descent(11/49): loss=2.7202751287083555\n",
      "Gradient Descent(12/49): loss=3.664696803011624\n",
      "Gradient Descent(13/49): loss=5.002092335992451\n",
      "Gradient Descent(14/49): loss=6.895978150247482\n",
      "Gradient Descent(15/49): loss=9.577909851812368\n",
      "Gradient Descent(16/49): loss=13.37579333439871\n",
      "Gradient Descent(17/49): loss=18.753976134091534\n",
      "Gradient Descent(18/49): loss=26.370020796734625\n",
      "Gradient Descent(19/49): loss=37.155101643502\n",
      "Gradient Descent(20/49): loss=52.42785463060736\n",
      "Gradient Descent(21/49): loss=74.0556001356568\n",
      "Gradient Descent(22/49): loss=104.68265054535792\n",
      "Gradient Descent(23/49): loss=148.05361663052025\n",
      "Gradient Descent(24/49): loss=209.47124170371512\n",
      "Gradient Descent(25/49): loss=296.44474056984353\n",
      "Gradient Descent(26/49): loss=419.6079123141859\n",
      "Gradient Descent(27/49): loss=594.0192798212981\n",
      "Gradient Descent(28/49): loss=841.003217348234\n",
      "Gradient Descent(29/49): loss=1190.7571712801412\n",
      "Gradient Descent(30/49): loss=1686.04374544328\n",
      "Gradient Descent(31/49): loss=2387.4190631156653\n",
      "Gradient Descent(32/49): loss=3380.6366504710973\n",
      "Gradient Descent(33/49): loss=4787.132075926055\n",
      "Gradient Descent(34/49): loss=6778.870247912484\n",
      "Gradient Descent(35/49): loss=9599.370673262803\n",
      "Gradient Descent(36/49): loss=13593.481325600891\n",
      "Gradient Descent(37/49): loss=19249.541420378308\n",
      "Gradient Descent(38/49): loss=27259.08812059375\n",
      "Gradient Descent(39/49): loss=38601.40720276718\n",
      "Gradient Descent(40/49): loss=54663.26525502672\n",
      "Gradient Descent(41/49): loss=77408.46244284499\n",
      "Gradient Descent(42/49): loss=109617.93618050234\n",
      "Gradient Descent(43/49): loss=155229.77194039186\n",
      "Gradient Descent(44/49): loss=219820.6925599954\n",
      "Gradient Descent(45/49): loss=311287.89524942735\n",
      "Gradient Descent(46/49): loss=440814.6009778717\n",
      "Gradient Descent(47/49): loss=624237.3689599807\n",
      "Gradient Descent(48/49): loss=883982.3506993587\n",
      "Gradient Descent(49/49): loss=1251807.2193404739\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.52073298607232\n",
      "Gradient Descent(2/49): loss=0.5500929676493383\n",
      "Gradient Descent(3/49): loss=0.5916696375605578\n",
      "Gradient Descent(4/49): loss=0.6505463598218447\n",
      "Gradient Descent(5/49): loss=0.7339216862160493\n",
      "Gradient Descent(6/49): loss=0.8519894859228749\n",
      "Gradient Descent(7/49): loss=1.0191852970877815\n",
      "Gradient Descent(8/49): loss=1.2559512852782644\n",
      "Gradient Descent(9/49): loss=1.5912356011547693\n",
      "Gradient Descent(10/49): loss=2.066031720867551\n",
      "Gradient Descent(11/49): loss=2.7383905059928315\n",
      "Gradient Descent(12/49): loss=3.690517781608661\n",
      "Gradient Descent(13/49): loss=5.03882521660831\n",
      "Gradient Descent(14/49): loss=6.9481633753108865\n",
      "Gradient Descent(15/49): loss=9.651977141850923\n",
      "Gradient Descent(16/49): loss=13.480847816646387\n",
      "Gradient Descent(17/49): loss=18.902911579227652\n",
      "Gradient Descent(18/49): loss=26.581096073415523\n",
      "Gradient Descent(19/49): loss=37.45417313563894\n",
      "Gradient Descent(20/49): loss=52.85153756344523\n",
      "Gradient Descent(21/49): loss=74.65574532965876\n",
      "Gradient Descent(22/49): loss=105.53268394741015\n",
      "Gradient Descent(23/49): loss=149.25751672399375\n",
      "Gradient Descent(24/49): loss=211.1762524189357\n",
      "Gradient Descent(25/49): loss=298.85937403651127\n",
      "Gradient Descent(26/49): loss=423.0274425591577\n",
      "Gradient Descent(27/49): loss=598.8618443941045\n",
      "Gradient Descent(28/49): loss=847.8609408326229\n",
      "Gradient Descent(29/49): loss=1200.468561299244\n",
      "Gradient Descent(30/49): loss=1699.7962126418527\n",
      "Gradient Descent(31/49): loss=2406.8940997079867\n",
      "Gradient Descent(32/49): loss=3408.215417582586\n",
      "Gradient Descent(33/49): loss=4826.186535825143\n",
      "Gradient Descent(34/49): loss=6834.175436368004\n",
      "Gradient Descent(35/49): loss=9677.68851842602\n",
      "Gradient Descent(36/49): loss=13704.387393929383\n",
      "Gradient Descent(37/49): loss=19406.595671527273\n",
      "Gradient Descent(38/49): loss=27481.492813437108\n",
      "Gradient Descent(39/49): loss=38916.35465609548\n",
      "Gradient Descent(40/49): loss=55109.262511476016\n",
      "Gradient Descent(41/49): loss=78040.03932549075\n",
      "Gradient Descent(42/49): loss=110512.31237181454\n",
      "Gradient Descent(43/49): loss=156496.2982327068\n",
      "Gradient Descent(44/49): loss=221614.22061034237\n",
      "Gradient Descent(45/49): loss=313827.7104892816\n",
      "Gradient Descent(46/49): loss=444411.23350683\n",
      "Gradient Descent(47/49): loss=629330.5604520601\n",
      "Gradient Descent(48/49): loss=891194.8193390905\n",
      "Gradient Descent(49/49): loss=1262020.7963489685\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5238234458977621\n",
      "Gradient Descent(2/49): loss=0.5592822627719888\n",
      "Gradient Descent(3/49): loss=0.6120591658075794\n",
      "Gradient Descent(4/49): loss=0.6906123082857661\n",
      "Gradient Descent(5/49): loss=0.807530805550255\n",
      "Gradient Descent(6/49): loss=0.9815522968787678\n",
      "Gradient Descent(7/49): loss=1.240565884572168\n",
      "Gradient Descent(8/49): loss=1.6260817084948955\n",
      "Gradient Descent(9/49): loss=2.1998834608217743\n",
      "Gradient Descent(10/49): loss=3.053929988985046\n",
      "Gradient Descent(11/49): loss=4.325092841502996\n",
      "Gradient Descent(12/49): loss=6.2170916311908435\n",
      "Gradient Descent(13/49): loss=9.033142629762743\n",
      "Gradient Descent(14/49): loss=13.224552936035032\n",
      "Gradient Descent(15/49): loss=19.463048035893262\n",
      "Gradient Descent(16/49): loss=28.748424142519866\n",
      "Gradient Descent(17/49): loss=42.56877793962428\n",
      "Gradient Descent(18/49): loss=63.13899253123968\n",
      "Gradient Descent(19/49): loss=93.75569992940719\n",
      "Gradient Descent(20/49): loss=139.32560722082937\n",
      "Gradient Descent(21/49): loss=207.15185723335696\n",
      "Gradient Descent(22/49): loss=308.10444775205474\n",
      "Gradient Descent(23/49): loss=458.36228348006136\n",
      "Gradient Descent(24/49): loss=682.0060461775385\n",
      "Gradient Descent(25/49): loss=1014.8774225766042\n",
      "Gradient Descent(26/49): loss=1510.3231792089755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=2247.7446433803884\n",
      "Gradient Descent(28/49): loss=3345.3227506530347\n",
      "Gradient Descent(29/49): loss=4978.958005518042\n",
      "Gradient Descent(30/49): loss=7410.460718858464\n",
      "Gradient Descent(31/49): loss=11029.509357395955\n",
      "Gradient Descent(32/49): loss=16416.1013509953\n",
      "Gradient Descent(33/49): loss=24433.50487426557\n",
      "Gradient Descent(34/49): loss=36366.6082783059\n",
      "Gradient Descent(35/49): loss=54127.839384871826\n",
      "Gradient Descent(36/49): loss=80563.65576389575\n",
      "Gradient Descent(37/49): loss=119910.72486242131\n",
      "Gradient Descent(38/49): loss=178474.9025086718\n",
      "Gradient Descent(39/49): loss=265641.8245173313\n",
      "Gradient Descent(40/49): loss=395381.0712350885\n",
      "Gradient Descent(41/49): loss=588484.9660496992\n",
      "Gradient Descent(42/49): loss=875900.8030918743\n",
      "Gradient Descent(43/49): loss=1303690.5349452633\n",
      "Gradient Descent(44/49): loss=1940412.771836118\n",
      "Gradient Descent(45/49): loss=2888110.149224111\n",
      "Gradient Descent(46/49): loss=4298662.9257287\n",
      "Gradient Descent(47/49): loss=6398129.67827781\n",
      "Gradient Descent(48/49): loss=9522975.992771674\n",
      "Gradient Descent(49/49): loss=14173997.247263635\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5244210657869142\n",
      "Gradient Descent(2/49): loss=0.5607693801041462\n",
      "Gradient Descent(3/49): loss=0.6148702111339194\n",
      "Gradient Descent(4/49): loss=0.6953938880386592\n",
      "Gradient Descent(5/49): loss=0.8152453287436258\n",
      "Gradient Descent(6/49): loss=0.9936322130889392\n",
      "Gradient Descent(7/49): loss=1.259143251748591\n",
      "Gradient Descent(8/49): loss=1.6543298816894951\n",
      "Gradient Descent(9/49): loss=2.242525661693541\n",
      "Gradient Descent(10/49): loss=3.1179962606513905\n",
      "Gradient Descent(11/49): loss=4.421046700140687\n",
      "Gradient Descent(12/49): loss=6.360506974276487\n",
      "Gradient Descent(13/49): loss=9.247199646299268\n",
      "Gradient Descent(14/49): loss=13.543753019337878\n",
      "Gradient Descent(15/49): loss=19.938743059769127\n",
      "Gradient Descent(16/49): loss=29.457046235948866\n",
      "Gradient Descent(17/49): loss=43.62408868337071\n",
      "Gradient Descent(18/49): loss=64.71031466210901\n",
      "Gradient Descent(19/49): loss=96.09505340886112\n",
      "Gradient Descent(20/49): loss=142.8080985595447\n",
      "Gradient Descent(21/49): loss=212.3357949618225\n",
      "Gradient Descent(22/49): loss=315.82081828698136\n",
      "Gradient Descent(23/49): loss=469.8479270041296\n",
      "Gradient Descent(24/49): loss=699.1018756187261\n",
      "Gradient Descent(25/49): loss=1040.323452736631\n",
      "Gradient Descent(26/49): loss=1548.1976481190436\n",
      "Gradient Descent(27/49): loss=2304.117600526044\n",
      "Gradient Descent(28/49): loss=3429.2288576885644\n",
      "Gradient Descent(29/49): loss=5103.844452849024\n",
      "Gradient Descent(30/49): loss=7596.342304686256\n",
      "Gradient Descent(31/49): loss=11306.17610736153\n",
      "Gradient Descent(32/49): loss=16827.89273926408\n",
      "Gradient Descent(33/49): loss=25046.415774185294\n",
      "Gradient Descent(34/49): loss=37278.86545936681\n",
      "Gradient Descent(35/49): loss=55485.6435707891\n",
      "Gradient Descent(36/49): loss=82584.61211182366\n",
      "Gradient Descent(37/49): loss=122918.71688829332\n",
      "Gradient Descent(38/49): loss=182951.9984376098\n",
      "Gradient Descent(39/49): loss=272305.5346956335\n",
      "Gradient Descent(40/49): loss=405299.33806201606\n",
      "Gradient Descent(41/49): loss=603247.3149925251\n",
      "Gradient Descent(42/49): loss=897873.0838559489\n",
      "Gradient Descent(43/49): loss=1336394.0782321808\n",
      "Gradient Descent(44/49): loss=1989088.7262617152\n",
      "Gradient Descent(45/49): loss=2960559.4403890064\n",
      "Gradient Descent(46/49): loss=4406496.451295757\n",
      "Gradient Descent(47/49): loss=6558629.098330388\n",
      "Gradient Descent(48/49): loss=9761863.33017527\n",
      "Gradient Descent(49/49): loss=14529557.160852935\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.524138525312\n",
      "Gradient Descent(2/49): loss=0.5600663063863789\n",
      "Gradient Descent(3/49): loss=0.6135412157374799\n",
      "Gradient Descent(4/49): loss=0.6931332708156757\n",
      "Gradient Descent(5/49): loss=0.8115980855940033\n",
      "Gradient Descent(6/49): loss=0.9879211159101559\n",
      "Gradient Descent(7/49): loss=1.250360314232759\n",
      "Gradient Descent(8/49): loss=1.6409748170161436\n",
      "Gradient Descent(9/49): loss=2.2223654429586963\n",
      "Gradient Descent(10/49): loss=3.0877072506115604\n",
      "Gradient Descent(11/49): loss=4.37568199712248\n",
      "Gradient Descent(12/49): loss=6.292703609828236\n",
      "Gradient Descent(13/49): loss=9.145998578181086\n",
      "Gradient Descent(14/49): loss=13.39284280907727\n",
      "Gradient Descent(15/49): loss=19.713845762343237\n",
      "Gradient Descent(16/49): loss=29.122026557984142\n",
      "Gradient Descent(17/49): loss=43.12516285421743\n",
      "Gradient Descent(18/49): loss=63.96743091752875\n",
      "Gradient Descent(19/49): loss=94.98906270297084\n",
      "Gradient Descent(20/49): loss=141.16165945242912\n",
      "Gradient Descent(21/49): loss=209.88495245429723\n",
      "Gradient Descent(22/49): loss=312.17270175831214\n",
      "Gradient Descent(23/49): loss=464.4177878223844\n",
      "Gradient Descent(24/49): loss=691.0193739201471\n",
      "Gradient Descent(25/49): loss=1028.2931746681334\n",
      "Gradient Descent(26/49): loss=1530.291499701505\n",
      "Gradient Descent(27/49): loss=2277.465806680939\n",
      "Gradient Descent(28/49): loss=3389.5600451890878\n",
      "Gradient Descent(29/49): loss=5044.801109784979\n",
      "Gradient Descent(30/49): loss=7508.461910329519\n",
      "Gradient Descent(31/49): loss=11175.37464586088\n",
      "Gradient Descent(32/49): loss=16633.207561426552\n",
      "Gradient Descent(33/49): loss=24756.646072950072\n",
      "Gradient Descent(34/49): loss=36847.571953499086\n",
      "Gradient Descent(35/49): loss=54843.7060341117\n",
      "Gradient Descent(36/49): loss=81629.15199969278\n",
      "Gradient Descent(37/49): loss=121496.60977486556\n",
      "Gradient Descent(38/49): loss=180835.33392742608\n",
      "Gradient Descent(39/49): loss=269155.0909560977\n",
      "Gradient Descent(40/49): loss=400610.2173176115\n",
      "Gradient Descent(41/49): loss=596268.0273940562\n",
      "Gradient Descent(42/49): loss=887485.1119118155\n",
      "Gradient Descent(43/49): loss=1320932.6205081476\n",
      "Gradient Descent(44/49): loss=1966075.8923027474\n",
      "Gradient Descent(45/49): loss=2926307.1380419806\n",
      "Gradient Descent(46/49): loss=4355515.324200649\n",
      "Gradient Descent(47/49): loss=6482748.788477978\n",
      "Gradient Descent(48/49): loss=9648923.076708682\n",
      "Gradient Descent(49/49): loss=14361456.887310792\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5243354731980799\n",
      "Gradient Descent(2/49): loss=0.5605563915061083\n",
      "Gradient Descent(3/49): loss=0.6144676063157875\n",
      "Gradient Descent(4/49): loss=0.6947090584384894\n",
      "Gradient Descent(5/49): loss=0.814140435777944\n",
      "Gradient Descent(6/49): loss=0.9919020978099378\n",
      "Gradient Descent(7/49): loss=1.2564825555784465\n",
      "Gradient Descent(8/49): loss=1.6502841089210742\n",
      "Gradient Descent(9/49): loss=2.2364183409159697\n",
      "Gradient Descent(10/49): loss=3.108820531817214\n",
      "Gradient Descent(11/49): loss=4.407303952754501\n",
      "Gradient Descent(12/49): loss=6.33996667647759\n",
      "Gradient Descent(13/49): loss=9.216541874466476\n",
      "Gradient Descent(14/49): loss=13.498036399152351\n",
      "Gradient Descent(15/49): loss=19.870612849698823\n",
      "Gradient Descent(16/49): loss=29.35555563868552\n",
      "Gradient Descent(17/49): loss=43.47294448581528\n",
      "Gradient Descent(18/49): loss=64.48526604587786\n",
      "Gradient Descent(19/49): loss=95.76000545587425\n",
      "Gradient Descent(20/49): loss=142.30932759371586\n",
      "Gradient Descent(21/49): loss=211.5933386636996\n",
      "Gradient Descent(22/49): loss=314.7156607402334\n",
      "Gradient Descent(23/49): loss=468.20292491892394\n",
      "Gradient Descent(24/49): loss=696.6533689226111\n",
      "Gradient Descent(25/49): loss=1036.679009777711\n",
      "Gradient Descent(26/49): loss=1542.7731736262567\n",
      "Gradient Descent(27/49): loss=2296.0437270988386\n",
      "Gradient Descent(28/49): loss=3417.2116188873547\n",
      "Gradient Descent(29/49): loss=5085.957909024594\n",
      "Gradient Descent(30/49): loss=7569.719887266229\n",
      "Gradient Descent(31/49): loss=11266.551215681086\n",
      "Gradient Descent(32/49): loss=16768.914964891934\n",
      "Gradient Descent(33/49): loss=24958.633169220357\n",
      "Gradient Descent(34/49): loss=37148.20974454401\n",
      "Gradient Descent(35/49): loss=55291.175519249104\n",
      "Gradient Descent(36/49): loss=82295.16577831833\n",
      "Gradient Descent(37/49): loss=122487.90487991364\n",
      "Gradient Descent(38/49): loss=182310.77775871614\n",
      "Gradient Descent(39/49): loss=271351.1417515234\n",
      "Gradient Descent(40/49): loss=403878.8195184507\n",
      "Gradient Descent(41/49): loss=601133.0151067466\n",
      "Gradient Descent(42/49): loss=894726.1598204257\n",
      "Gradient Descent(43/49): loss=1331710.1964121216\n",
      "Gradient Descent(44/49): loss=1982117.2364752183\n",
      "Gradient Descent(45/49): loss=2950183.0749055175\n",
      "Gradient Descent(46/49): loss=4391052.268825216\n",
      "Gradient Descent(47/49): loss=6535641.9770546025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=9727649.298783941\n",
      "Gradient Descent(49/49): loss=14478632.99644468\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5274379367680001\n",
      "Gradient Descent(2/49): loss=0.5703097129679933\n",
      "Gradient Descent(3/49): loss=0.6372968632805\n",
      "Gradient Descent(4/49): loss=0.741964285643768\n",
      "Gradient Descent(5/49): loss=0.9055071330864148\n",
      "Gradient Descent(6/49): loss=1.161042832215498\n",
      "Gradient Descent(7/49): loss=1.5603173621046909\n",
      "Gradient Descent(8/49): loss=2.184183815056696\n",
      "Gradient Descent(9/49): loss=3.1589751477941994\n",
      "Gradient Descent(10/49): loss=4.682086605196738\n",
      "Gradient Descent(11/49): loss=7.061948257387839\n",
      "Gradient Descent(12/49): loss=10.780482088936644\n",
      "Gradient Descent(13/49): loss=16.59069120073297\n",
      "Gradient Descent(14/49): loss=25.66914293791023\n",
      "Gradient Descent(15/49): loss=39.854223777251505\n",
      "Gradient Descent(16/49): loss=62.018412588720985\n",
      "Gradient Descent(17/49): loss=96.64995760664071\n",
      "Gradient Descent(18/49): loss=150.7617466971373\n",
      "Gradient Descent(19/49): loss=235.31141715102052\n",
      "Gradient Descent(20/49): loss=367.4202772352395\n",
      "Gradient Descent(21/49): loss=573.8403711168266\n",
      "Gradient Descent(22/49): loss=896.3717678067643\n",
      "Gradient Descent(23/49): loss=1400.3270751349487\n",
      "Gradient Descent(24/49): loss=2187.757242835247\n",
      "Gradient Descent(25/49): loss=3418.1168798670415\n",
      "Gradient Descent(26/49): loss=5340.553812729367\n",
      "Gradient Descent(27/49): loss=8344.361520325288\n",
      "Gradient Descent(28/49): loss=13037.811063444089\n",
      "Gradient Descent(29/49): loss=20371.325974566553\n",
      "Gradient Descent(30/49): loss=31829.94302319914\n",
      "Gradient Descent(31/49): loss=49734.03216168311\n",
      "Gradient Descent(32/49): loss=77709.17144056076\n",
      "Gradient Descent(33/49): loss=121420.32656381307\n",
      "Gradient Descent(34/49): loss=189719.00644390637\n",
      "Gradient Descent(35/49): loss=296435.69375652523\n",
      "Gradient Descent(36/49): loss=463180.5176825533\n",
      "Gradient Descent(37/49): loss=723719.3050669899\n",
      "Gradient Descent(38/49): loss=1130811.160355226\n",
      "Gradient Descent(39/49): loss=1766892.1842431305\n",
      "Gradient Descent(40/49): loss=2760768.784068158\n",
      "Gradient Descent(41/49): loss=4313700.971294698\n",
      "Gradient Descent(42/49): loss=6740157.513835424\n",
      "Gradient Descent(43/49): loss=10531495.861556794\n",
      "Gradient Descent(44/49): loss=16455462.029869262\n",
      "Gradient Descent(45/49): loss=25711659.16786169\n",
      "Gradient Descent(46/49): loss=40174467.195976436\n",
      "Gradient Descent(47/49): loss=62772604.73990439\n",
      "Gradient Descent(48/49): loss=98082194.6522843\n",
      "Gradient Descent(49/49): loss=153253428.89039618\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.528126227488\n",
      "Gradient Descent(2/49): loss=0.5720734579379962\n",
      "Gradient Descent(3/49): loss=0.6407410055161106\n",
      "Gradient Descent(4/49): loss=0.7480340486069371\n",
      "Gradient Descent(5/49): loss=0.9156794284363305\n",
      "Gradient Descent(6/49): loss=1.1776253344197136\n",
      "Gradient Descent(7/49): loss=1.5869158125188763\n",
      "Gradient Descent(8/49): loss=2.2264321845488264\n",
      "Gradient Descent(9/49): loss=3.225676515845696\n",
      "Gradient Descent(10/49): loss=4.786995783497437\n",
      "Gradient Descent(11/49): loss=7.226557139202361\n",
      "Gradient Descent(12/49): loss=11.038371757490966\n",
      "Gradient Descent(13/49): loss=16.994332098568464\n",
      "Gradient Descent(14/49): loss=26.300520131504804\n",
      "Gradient Descent(15/49): loss=40.841438932966575\n",
      "Gradient Descent(16/49): loss=63.56162456024471\n",
      "Gradient Descent(17/49): loss=99.06191460286595\n",
      "Gradient Descent(18/49): loss=154.53111779445356\n",
      "Gradient Descent(19/49): loss=241.20174778129527\n",
      "Gradient Descent(20/49): loss=376.6246071357371\n",
      "Gradient Descent(21/49): loss=588.2228248771426\n",
      "Gradient Descent(22/49): loss=918.8450400980055\n",
      "Gradient Descent(23/49): loss=1435.4422513805014\n",
      "Gradient Descent(24/49): loss=2242.625394009471\n",
      "Gradient Descent(25/49): loss=3503.8490543675507\n",
      "Gradient Descent(26/49): loss=5474.511023676377\n",
      "Gradient Descent(27/49): loss=8553.67035072205\n",
      "Gradient Descent(28/49): loss=13364.856799232199\n",
      "Gradient Descent(29/49): loss=20882.335625030406\n",
      "Gradient Descent(30/49): loss=32628.396290336073\n",
      "Gradient Descent(31/49): loss=50981.616079872394\n",
      "Gradient Descent(32/49): loss=79658.5220010368\n",
      "Gradient Descent(33/49): loss=124466.18750283905\n",
      "Gradient Descent(34/49): loss=194478.16484942296\n",
      "Gradient Descent(35/49): loss=303871.8794534645\n",
      "Gradient Descent(36/49): loss=474799.5585222543\n",
      "Gradient Descent(37/49): loss=741874.0570671704\n",
      "Gradient Descent(38/49): loss=1159177.9610436647\n",
      "Gradient Descent(39/49): loss=1811215.3110070517\n",
      "Gradient Descent(40/49): loss=2830023.6703247675\n",
      "Gradient Descent(41/49): loss=4421911.731759142\n",
      "Gradient Descent(42/49): loss=6909236.827750042\n",
      "Gradient Descent(43/49): loss=10795682.290236449\n",
      "Gradient Descent(44/49): loss=16868253.325369477\n",
      "Gradient Descent(45/49): loss=26356645.5677641\n",
      "Gradient Descent(46/49): loss=41182258.446506165\n",
      "Gradient Descent(47/49): loss=64347278.56954862\n",
      "Gradient Descent(48/49): loss=100542622.5117938\n",
      "Gradient Descent(49/49): loss=157097847.42155775\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.52780082\n",
      "Gradient Descent(2/49): loss=0.5712396012500064\n",
      "Gradient Descent(3/49): loss=0.6391126969531453\n",
      "Gradient Descent(4/49): loss=0.7451644089892756\n",
      "Gradient Descent(5/49): loss=0.9108702090457427\n",
      "Gradient Descent(6/49): loss=1.1697855216338902\n",
      "Gradient Descent(7/49): loss=1.5743406975530845\n",
      "Gradient Descent(8/49): loss=2.2064581599266484\n",
      "Gradient Descent(9/49): loss=3.19414169488563\n",
      "Gradient Descent(10/49): loss=4.7373972182590425\n",
      "Gradient Descent(11/49): loss=7.1487339735297954\n",
      "Gradient Descent(12/49): loss=10.916447653639608\n",
      "Gradient Descent(13/49): loss=16.803500278813846\n",
      "Gradient Descent(14/49): loss=26.002020005650014\n",
      "Gradient Descent(15/49): loss=40.374707078827036\n",
      "Gradient Descent(16/49): loss=62.8320306306602\n",
      "Gradient Descent(17/49): loss=97.92159868039948\n",
      "Gradient Descent(18/49): loss=152.7490487581103\n",
      "Gradient Descent(19/49): loss=238.41693950455291\n",
      "Gradient Descent(20/49): loss=372.2730187958709\n",
      "Gradient Descent(21/49): loss=581.4231426885015\n",
      "Gradient Descent(22/49): loss=908.2202112707142\n",
      "Gradient Descent(23/49): loss=1418.840630930364\n",
      "Gradient Descent(24/49): loss=2216.685036648732\n",
      "Gradient Descent(25/49): loss=3463.3169205832933\n",
      "Gradient Descent(26/49): loss=5411.1792392309835\n",
      "Gradient Descent(27/49): loss=8454.714112117643\n",
      "Gradient Descent(28/49): loss=13210.237351005111\n",
      "Gradient Descent(29/49): loss=20640.742411766834\n",
      "Gradient Descent(30/49): loss=32250.906569206127\n",
      "Gradient Descent(31/49): loss=50391.78806520619\n",
      "Gradient Descent(32/49): loss=78736.91540270274\n",
      "Gradient Descent(33/49): loss=123026.17686754474\n",
      "Gradient Descent(34/49): loss=192228.14790634805\n",
      "Gradient Descent(35/49): loss=300356.22765450994\n",
      "Gradient Descent(36/49): loss=469306.35226100596\n",
      "Gradient Descent(37/49): loss=733290.9219586789\n",
      "Gradient Descent(38/49): loss=1145766.812111216\n",
      "Gradient Descent(39/49): loss=1790260.390474791\n",
      "Gradient Descent(40/49): loss=2797281.606667819\n",
      "Gradient Descent(41/49): loss=4370752.256969687\n",
      "Gradient Descent(42/49): loss=6829300.14806543\n",
      "Gradient Descent(43/49): loss=10670781.227902727\n",
      "Gradient Descent(44/49): loss=16673095.415147103\n",
      "Gradient Descent(45/49): loss=26051711.332715504\n",
      "Gradient Descent(46/49): loss=40705798.70392052\n",
      "Gradient Descent(47/49): loss=63602810.221429646\n",
      "Gradient Descent(48/49): loss=99379390.71753983\n",
      "Gradient Descent(49/49): loss=155280297.74269822\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5280276488000001\n",
      "Gradient Descent(2/49): loss=0.5718208500500087\n",
      "Gradient Descent(3/49): loss=0.6402477270031514\n",
      "Gradient Descent(4/49): loss=0.7471647222424054\n",
      "Gradient Descent(5/49): loss=0.9142225273037496\n",
      "Gradient Descent(6/49): loss=1.1752503477120566\n",
      "Gradient Descent(7/49): loss=1.5831063171001782\n",
      "Gradient Descent(8/49): loss=2.2203812692692644\n",
      "Gradient Descent(9/49): loss=3.2161233820335973\n",
      "Gradient Descent(10/49): loss=4.771970433228063\n",
      "Gradient Descent(11/49): loss=7.2029814507190855\n",
      "Gradient Descent(12/49): loss=11.001436165548276\n",
      "Gradient Descent(13/49): loss=16.936521657470124\n",
      "Gradient Descent(14/49): loss=26.210092738597144\n",
      "Gradient Descent(15/49): loss=40.70004755286142\n",
      "Gradient Descent(16/49): loss=63.34060195014918\n",
      "Gradient Descent(17/49): loss=98.7164681959136\n",
      "Gradient Descent(18/49): loss=153.99125920490488\n",
      "Gradient Descent(19/49): loss=240.3581201564315\n",
      "Gradient Descent(20/49): loss=375.30634039320194\n",
      "Gradient Descent(21/49): loss=586.1629345132274\n",
      "Gradient Descent(22/49): loss=915.6263628256568\n",
      "Gradient Descent(23/49): loss=1430.412969564006\n",
      "Gradient Descent(24/49): loss=2234.7670425924694\n",
      "Gradient Descent(25/49): loss=3491.5702816994813\n",
      "Gradient Descent(26/49): loss=5455.325342804147\n",
      "Gradient Descent(27/49): loss=8523.692625780917\n",
      "Gradient Descent(28/49): loss=13318.01650542981\n",
      "Gradient Descent(29/49): loss=20809.1475673837\n",
      "Gradient Descent(30/49): loss=32514.039851686975\n",
      "Gradient Descent(31/49): loss=50802.93404591016\n",
      "Gradient Descent(32/49): loss=79379.33122438236\n",
      "Gradient Descent(33/49): loss=124029.95181573594\n",
      "Gradient Descent(34/49): loss=193796.54648973857\n",
      "Gradient Descent(35/49): loss=302806.8506678788\n",
      "Gradient Descent(36/49): loss=473135.4509462439\n",
      "Gradient Descent(37/49): loss=739273.8888811579\n",
      "Gradient Descent(38/49): loss=1155115.1981544292\n",
      "Gradient Descent(39/49): loss=1804867.243893776\n",
      "Gradient Descent(40/49): loss=2820104.815361683\n",
      "Gradient Descent(41/49): loss=4406413.520779947\n",
      "Gradient Descent(42/49): loss=6885020.872997285\n",
      "Gradient Descent(43/49): loss=10757844.860835813\n",
      "Gradient Descent(44/49): loss=16809132.341835037\n",
      "Gradient Descent(45/49): loss=26264269.030894395\n",
      "Gradient Descent(46/49): loss=41037920.10754474\n",
      "Gradient Descent(47/49): loss=64121749.91482013\n",
      "Gradient Descent(48/49): loss=100190233.98869392\n",
      "Gradient Descent(49/49): loss=156547240.3541105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5311402290358955\n",
      "Gradient Descent(2/49): loss=0.5821603802883\n",
      "Gradient Descent(3/49): loss=0.6657517961002324\n",
      "Gradient Descent(4/49): loss=0.8027079717664869\n",
      "Gradient Descent(5/49): loss=1.0270969699781796\n",
      "Gradient Descent(6/49): loss=1.3947359046480858\n",
      "Gradient Descent(7/49): loss=1.997075535211474\n",
      "Gradient Descent(8/49): loss=2.983948785926131\n",
      "Gradient Descent(9/49): loss=4.600841919897514\n",
      "Gradient Descent(10/49): loss=7.2499596305958205\n",
      "Gradient Descent(11/49): loss=11.590274087804744\n",
      "Gradient Descent(12/49): loss=18.701445294494\n",
      "Gradient Descent(13/49): loss=30.3523881995366\n",
      "Gradient Descent(14/49): loss=49.44129305516235\n",
      "Gradient Descent(15/49): loss=80.71655477062194\n",
      "Gradient Descent(16/49): loss=131.95794356521665\n",
      "Gradient Descent(17/49): loss=215.9118349663069\n",
      "Gradient Descent(18/49): loss=353.4618906378045\n",
      "Gradient Descent(19/49): loss=578.823901849983\n",
      "Gradient Descent(20/49): loss=948.0570210201163\n",
      "Gradient Descent(21/49): loss=1553.0085634683403\n",
      "Gradient Descent(22/49): loss=2544.161170615449\n",
      "Gradient Descent(23/49): loss=4168.065602165724\n",
      "Gradient Descent(24/49): loss=6828.6706228172\n",
      "Gradient Descent(25/49): loss=11187.805888652669\n",
      "Gradient Descent(26/49): loss=18329.813108197748\n",
      "Gradient Descent(27/49): loss=30031.277736701453\n",
      "Gradient Descent(28/49): loss=49202.95738403563\n",
      "Gradient Descent(29/49): loss=80613.83731822726\n",
      "Gradient Descent(30/49): loss=132077.42300243067\n",
      "Gradient Descent(31/49): loss=216395.36178740233\n",
      "Gradient Descent(32/49): loss=354541.87269273784\n",
      "Gradient Descent(33/49): loss=580881.116159993\n",
      "Gradient Descent(34/49): loss=951715.3326566942\n",
      "Gradient Descent(35/49): loss=1559290.1129650315\n",
      "Gradient Descent(36/49): loss=2554740.633022006\n",
      "Gradient Descent(37/49): loss=4185686.765083366\n",
      "Gradient Descent(38/49): loss=6857828.90785235\n",
      "Gradient Descent(39/49): loss=11235866.594567081\n",
      "Gradient Descent(40/49): loss=18408843.540481295\n",
      "Gradient Descent(41/49): loss=30161048.968667895\n",
      "Gradient Descent(42/49): loss=49415862.34220556\n",
      "Gradient Descent(43/49): loss=80962948.57340078\n",
      "Gradient Descent(44/49): loss=132649694.65461653\n",
      "Gradient Descent(45/49): loss=217333259.4340771\n",
      "Gradient Descent(46/49): loss=356078811.9687797\n",
      "Gradient Descent(47/49): loss=583399525.2415926\n",
      "Gradient Descent(48/49): loss=955841781.8677971\n",
      "Gradient Descent(49/49): loss=1566051175.124034\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5319213931170474\n",
      "Gradient Descent(2/49): loss=0.5842214036000158\n",
      "Gradient Descent(3/49): loss=0.6699097407752846\n",
      "Gradient Descent(4/49): loss=0.8103015124032423\n",
      "Gradient Descent(5/49): loss=1.0403193910385165\n",
      "Gradient Descent(6/49): loss=1.4171806833946217\n",
      "Gradient Descent(7/49): loss=2.034630224790825\n",
      "Gradient Descent(8/49): loss=3.0462595534146515\n",
      "Gradient Descent(9/49): loss=4.703713045431243\n",
      "Gradient Descent(10/49): loss=7.419284846751644\n",
      "Gradient Descent(11/49): loss=11.868477686036336\n",
      "Gradient Descent(12/49): loss=19.158035233919506\n",
      "Gradient Descent(13/49): loss=31.101246320369647\n",
      "Gradient Descent(14/49): loss=50.669003364407054\n",
      "Gradient Descent(15/49): loss=82.72881650535479\n",
      "Gradient Descent(16/49): loss=135.25561435548946\n",
      "Gradient Descent(17/49): loss=221.31551995316946\n",
      "Gradient Descent(18/49): loss=362.31606928441636\n",
      "Gradient Descent(19/49): loss=593.3313693087771\n",
      "Gradient Descent(20/49): loss=971.8268368685372\n",
      "Gradient Descent(21/49): loss=1591.9538109183884\n",
      "Gradient Descent(22/49): loss=2607.9698452017083\n",
      "Gradient Descent(23/49): loss=4272.610515771972\n",
      "Gradient Descent(24/49): loss=6999.957790434041\n",
      "Gradient Descent(25/49): loss=11468.443565240166\n",
      "Gradient Descent(26/49): loss=18789.61065868129\n",
      "Gradient Descent(27/49): loss=30784.61082457571\n",
      "Gradient Descent(28/49): loss=50437.21909637537\n",
      "Gradient Descent(29/49): loss=82636.05248889986\n",
      "Gradient Descent(30/49): loss=135390.62111920564\n",
      "Gradient Descent(31/49): loss=221823.7063631205\n",
      "Gradient Descent(32/49): loss=363435.6732267652\n",
      "Gradient Descent(33/49): loss=595452.7197361368\n",
      "Gradient Descent(34/49): loss=975589.4487370712\n",
      "Gradient Descent(35/49): loss=1598405.4655323054\n",
      "Gradient Descent(36/49): loss=2618827.227449418\n",
      "Gradient Descent(37/49): loss=4290686.242174173\n",
      "Gradient Descent(38/49): loss=7029860.051899331\n",
      "Gradient Descent(39/49): loss=11517722.421752311\n",
      "Gradient Descent(40/49): loss=18870636.128521882\n",
      "Gradient Descent(41/49): loss=30917649.945691314\n",
      "Gradient Descent(42/49): loss=50655477.38374879\n",
      "Gradient Descent(43/49): loss=82993933.85826084\n",
      "Gradient Descent(44/49): loss=135977260.946079\n",
      "Gradient Descent(45/49): loss=222785144.04678836\n",
      "Gradient Descent(46/49): loss=365011179.7189292\n",
      "Gradient Descent(47/49): loss=598034316.5642463\n",
      "Gradient Descent(48/49): loss=979819423.971534\n",
      "Gradient Descent(49/49): loss=1605336143.9475536\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.531552077312\n",
      "Gradient Descent(2/49): loss=0.5832470007799815\n",
      "Gradient Descent(3/49): loss=0.6679439633899255\n",
      "Gradient Descent(4/49): loss=0.8067114669300967\n",
      "Gradient Descent(5/49): loss=1.0340681447302584\n",
      "Gradient Descent(6/49): loss=1.4065693256381258\n",
      "Gradient Descent(7/49): loss=2.016875260437559\n",
      "Gradient Descent(8/49): loss=3.0168005040125747\n",
      "Gradient Descent(9/49): loss=4.655078023086478\n",
      "Gradient Descent(10/49): loss=7.33923191033642\n",
      "Gradient Descent(11/49): loss=11.736949639208213\n",
      "Gradient Descent(12/49): loss=18.942170366190688\n",
      "Gradient Descent(13/49): loss=30.74720400527691\n",
      "Gradient Descent(14/49): loss=50.08857111955587\n",
      "Gradient Descent(15/49): loss=81.77746699959499\n",
      "Gradient Descent(16/49): loss=133.6965540094476\n",
      "Gradient Descent(17/49): loss=218.7607861664027\n",
      "Gradient Descent(18/49): loss=358.1300241323277\n",
      "Gradient Descent(19/49): loss=586.4725836157244\n",
      "Gradient Descent(20/49): loss=960.5890330733487\n",
      "Gradient Descent(21/49): loss=1573.5414238647782\n",
      "Gradient Descent(22/49): loss=2577.8026209371483\n",
      "Gradient Descent(23/49): loss=4223.184166220224\n",
      "Gradient Descent(24/49): loss=6918.9772900129665\n",
      "Gradient Descent(25/49): loss=11335.764744033579\n",
      "Gradient Descent(26/49): loss=18572.229308704074\n",
      "Gradient Descent(27/49): loss=30428.452851455368\n",
      "Gradient Descent(28/49): loss=49853.689503904825\n",
      "Gradient Descent(29/49): loss=81679.99723526942\n",
      "Gradient Descent(30/49): loss=133824.2198223505\n",
      "Gradient Descent(31/49): loss=219257.3141090106\n",
      "Gradient Descent(32/49): loss=359230.8957882759\n",
      "Gradient Descent(33/49): loss=588563.6120115756\n",
      "Gradient Descent(34/49): loss=964302.3342718034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1579912.6568230789\n",
      "Gradient Descent(36/49): loss=2588528.6092911726\n",
      "Gradient Descent(37/49): loss=4241044.985814318\n",
      "Gradient Descent(38/49): loss=6948527.817109592\n",
      "Gradient Descent(39/49): loss=11384467.687904976\n",
      "Gradient Descent(40/49): loss=18652311.572217163\n",
      "Gradient Descent(41/49): loss=30559946.99227565\n",
      "Gradient Descent(42/49): loss=50069416.86449953\n",
      "Gradient Descent(43/49): loss=82033732.3031463\n",
      "Gradient Descent(44/49): loss=134404066.71783054\n",
      "Gradient Descent(45/49): loss=220207622.62282836\n",
      "Gradient Descent(46/49): loss=360788168.6175767\n",
      "Gradient Descent(47/49): loss=591115335.1754475\n",
      "Gradient Descent(48/49): loss=968483364.8638784\n",
      "Gradient Descent(49/49): loss=1586763144.7055078\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5318095128780801\n",
      "Gradient Descent(2/49): loss=0.5839262187775253\n",
      "Gradient Descent(3/49): loss=0.6693142297231799\n",
      "Gradient Descent(4/49): loss=0.8092139468565233\n",
      "Gradient Descent(5/49): loss=1.0384256434077976\n",
      "Gradient Descent(6/49): loss=1.4139660870375401\n",
      "Gradient Descent(7/49): loss=2.029251549880339\n",
      "Gradient Descent(8/49): loss=3.0373352522022343\n",
      "Gradient Descent(9/49): loss=4.688979590086149\n",
      "Gradient Descent(10/49): loss=7.395033673275905\n",
      "Gradient Descent(11/49): loss=11.82863268317455\n",
      "Gradient Descent(12/49): loss=19.092641300989417\n",
      "Gradient Descent(13/49): loss=30.993993020422106\n",
      "Gradient Descent(14/49): loss=50.49316767754064\n",
      "Gradient Descent(15/49): loss=82.44061543575037\n",
      "Gradient Descent(16/49): loss=134.78331384281145\n",
      "Gradient Descent(17/49): loss=220.5415909129539\n",
      "Gradient Descent(18/49): loss=361.04795206468145\n",
      "Gradient Descent(19/49): loss=591.2535741756627\n",
      "Gradient Descent(20/49): loss=968.4224654423736\n",
      "Gradient Descent(21/49): loss=1586.3759768935354\n",
      "Gradient Descent(22/49): loss=2598.8310100551057\n",
      "Gradient Descent(23/49): loss=4257.637336387207\n",
      "Gradient Descent(24/49): loss=6975.425621449469\n",
      "Gradient Descent(25/49): loss=11428.249947695156\n",
      "Gradient Descent(26/49): loss=18723.75732381575\n",
      "Gradient Descent(27/49): loss=30676.716608854247\n",
      "Gradient Descent(28/49): loss=50260.44510146172\n",
      "Gradient Descent(29/49): loss=82346.42586374866\n",
      "Gradient Descent(30/49): loss=134916.0967446816\n",
      "Gradient Descent(31/49): loss=221046.24551600436\n",
      "Gradient Descent(32/49): loss=362161.88126295584\n",
      "Gradient Descent(33/49): loss=593365.7388707448\n",
      "Gradient Descent(34/49): loss=972170.139175407\n",
      "Gradient Descent(35/49): loss=1592803.2686343975\n",
      "Gradient Descent(36/49): loss=2609648.587939815\n",
      "Gradient Descent(37/49): loss=4275647.959089774\n",
      "Gradient Descent(38/49): loss=7005221.328781354\n",
      "Gradient Descent(39/49): loss=11477354.337683687\n",
      "Gradient Descent(40/49): loss=18804497.059468266\n",
      "Gradient Descent(41/49): loss=30809287.694845032\n",
      "Gradient Descent(42/49): loss=50477936.67183933\n",
      "Gradient Descent(43/49): loss=82703051.15574506\n",
      "Gradient Descent(44/49): loss=135500678.7261833\n",
      "Gradient Descent(45/49): loss=222004311.73758888\n",
      "Gradient Descent(46/49): loss=363731864.06349677\n",
      "Gradient Descent(47/49): loss=595938285.7941864\n",
      "Gradient Descent(48/49): loss=976385287.1578175\n",
      "Gradient Descent(49/49): loss=1599709654.191759\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5349303227014486\n",
      "Gradient Descent(2/49): loss=0.5948742494894139\n",
      "Gradient Descent(3/49): loss=0.6977440222502475\n",
      "Gradient Descent(4/49): loss=0.8742788392850874\n",
      "Gradient Descent(5/49): loss=1.1772302387986506\n",
      "Gradient Descent(6/49): loss=1.6971251355037733\n",
      "Gradient Descent(7/49): loss=2.589316767739551\n",
      "Gradient Descent(8/49): loss=4.120406827818914\n",
      "Gradient Descent(9/49): loss=6.747910479920692\n",
      "Gradient Descent(10/49): loss=11.256969497292973\n",
      "Gradient Descent(11/49): loss=18.994965677006338\n",
      "Gradient Descent(12/49): loss=32.27414092101173\n",
      "Gradient Descent(13/49): loss=55.06253355724733\n",
      "Gradient Descent(14/49): loss=94.16969416028569\n",
      "Gradient Descent(15/49): loss=161.28149247117886\n",
      "Gradient Descent(16/49): loss=276.4520495524701\n",
      "Gradient Descent(17/49): loss=474.0962425596917\n",
      "Gradient Descent(18/49): loss=813.2734421794033\n",
      "Gradient Descent(19/49): loss=1395.3354344466368\n",
      "Gradient Descent(20/49): loss=2394.2120193765436\n",
      "Gradient Descent(21/49): loss=4108.3841267745665\n",
      "Gradient Descent(22/49): loss=7050.0748802808075\n",
      "Gradient Descent(23/49): loss=12098.310382371457\n",
      "Gradient Descent(24/49): loss=20761.587327511745\n",
      "Gradient Descent(25/49): loss=35628.636893067895\n",
      "Gradient Descent(26/49): loss=61141.98065251245\n",
      "Gradient Descent(27/49): loss=104925.42987810903\n",
      "Gradient Descent(28/49): loss=180062.20709415385\n",
      "Gradient Descent(29/49): loss=309004.4304746278\n",
      "Gradient Descent(30/49): loss=530282.1800178796\n",
      "Gradient Descent(31/49): loss=910016.926009086\n",
      "Gradient Descent(32/49): loss=1561679.7236043897\n",
      "Gradient Descent(33/49): loss=2679998.250557852\n",
      "Gradient Descent(34/49): loss=4599144.674662806\n",
      "Gradient Descent(35/49): loss=7892591.853069566\n",
      "Gradient Descent(36/49): loss=13544476.555933386\n",
      "Gradient Descent(37/49): loss=23243675.89451934\n",
      "Gradient Descent(38/49): loss=39888471.87946734\n",
      "Gradient Descent(39/49): loss=68452606.26924276\n",
      "Gradient Descent(40/49): loss=117471517.29552062\n",
      "Gradient Descent(41/49): loss=201592870.50772387\n",
      "Gradient Descent(42/49): loss=345953524.75520205\n",
      "Gradient Descent(43/49): loss=593690843.5092516\n",
      "Gradient Descent(44/49): loss=1018832856.2230415\n",
      "Gradient Descent(45/49): loss=1748419064.241127\n",
      "Gradient Descent(46/49): loss=3000461955.8213162\n",
      "Gradient Descent(47/49): loss=5149092762.061558\n",
      "Gradient Descent(48/49): loss=8836358088.650194\n",
      "Gradient Descent(49/49): loss=15164074115.609829\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5358065626740564\n",
      "Gradient Descent(2/49): loss=0.5972542048790008\n",
      "Gradient Descent(3/49): loss=0.7027045036669305\n",
      "Gradient Descent(4/49): loss=0.8836677614169146\n",
      "Gradient Descent(5/49): loss=1.1942188080416303\n",
      "Gradient Descent(6/49): loss=1.7271554591544125\n",
      "Gradient Descent(7/49): loss=2.6417280461290837\n",
      "Gradient Descent(8/49): loss=4.211226062636175\n",
      "Gradient Descent(9/49): loss=6.904641608764788\n",
      "Gradient Descent(10/49): loss=11.526812027475426\n",
      "Gradient Descent(11/49): loss=19.458918683022816\n",
      "Gradient Descent(12/49): loss=33.07120691461003\n",
      "Gradient Descent(13/49): loss=56.43125474883903\n",
      "Gradient Descent(14/49): loss=96.5194328371635\n",
      "Gradient Descent(15/49): loss=165.31475525453752\n",
      "Gradient Descent(16/49): loss=283.37440805497556\n",
      "Gradient Descent(17/49): loss=485.97657822585956\n",
      "Gradient Descent(18/49): loss=833.6621624561324\n",
      "Gradient Descent(19/49): loss=1430.3253935535793\n",
      "Gradient Descent(20/49): loss=2454.2591644397994\n",
      "Gradient Descent(21/49): loss=4211.431908658199\n",
      "Gradient Descent(22/49): loss=7226.916055011984\n",
      "Gradient Descent(23/49): loss=12401.788398567971\n",
      "Gradient Descent(24/49): loss=21282.386827344377\n",
      "Gradient Descent(25/49): loss=36522.38179096949\n",
      "Gradient Descent(26/49): loss=62675.7371480384\n",
      "Gradient Descent(27/49): loss=107557.51027632428\n",
      "Gradient Descent(28/49): loss=184579.12114176786\n",
      "Gradient Descent(29/49): loss=316755.907547968\n",
      "Gradient Descent(30/49): loss=543584.4906996088\n",
      "Gradient Descent(31/49): loss=932845.022246251\n",
      "Gradient Descent(32/49): loss=1600855.0204335088\n",
      "Gradient Descent(33/49): loss=2747226.97832249\n",
      "Gradient Descent(34/49): loss=4714515.8952558255\n",
      "Gradient Descent(35/49): loss=8090580.405605891\n",
      "Gradient Descent(36/49): loss=13884244.711816896\n",
      "Gradient Descent(37/49): loss=23826752.027704652\n",
      "Gradient Descent(38/49): loss=40889088.832496315\n",
      "Gradient Descent(39/49): loss=70169765.0231989\n",
      "Gradient Descent(40/49): loss=120418333.43407243\n",
      "Gradient Descent(41/49): loss=206649901.68394667\n",
      "Gradient Descent(42/49): loss=354631895.9576155\n",
      "Gradient Descent(43/49): loss=608583796.3306999\n",
      "Gradient Descent(44/49): loss=1044390652.5609276\n",
      "Gradient Descent(45/49): loss=1792278798.537419\n",
      "Gradient Descent(46/49): loss=3075729645.8477626\n",
      "Gradient Descent(47/49): loss=5278259644.916537\n",
      "Gradient Descent(48/49): loss=9058021376.318865\n",
      "Gradient Descent(49/49): loss=15544470483.578514\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5353922972479999\n",
      "Gradient Descent(2/49): loss=0.5961290185553004\n",
      "Gradient Descent(3/49): loss=0.7003593059907283\n",
      "Gradient Descent(4/49): loss=0.8792289022586416\n",
      "Gradient Descent(5/49): loss=1.1861870164140638\n",
      "Gradient Descent(6/49): loss=1.712957836116087\n",
      "Gradient Descent(7/49): loss=2.6169492398068788\n",
      "Gradient Descent(8/49): loss=4.168288887680874\n",
      "Gradient Descent(9/49): loss=6.830542857397667\n",
      "Gradient Descent(10/49): loss=11.399236894827112\n",
      "Gradient Descent(11/49): loss=19.23957273245952\n",
      "Gradient Descent(12/49): loss=32.694373063419775\n",
      "Gradient Descent(13/49): loss=55.78415591138633\n",
      "Gradient Descent(14/49): loss=95.408532256772\n",
      "Gradient Descent(15/49): loss=163.40792450307936\n",
      "Gradient Descent(16/49): loss=280.10168153700147\n",
      "Gradient Descent(17/49): loss=480.3598379828301\n",
      "Gradient Descent(18/49): loss=824.0228602595121\n",
      "Gradient Descent(19/49): loss=1413.7829727886026\n",
      "Gradient Descent(20/49): loss=2425.87030189984\n",
      "Gradient Descent(21/49): loss=4162.713367387325\n",
      "Gradient Descent(22/49): loss=7143.309752069651\n",
      "Gradient Descent(23/49): loss=12258.311207823728\n",
      "Gradient Descent(24/49): loss=21036.16520604286\n",
      "Gradient Descent(25/49): loss=36099.84045238946\n",
      "Gradient Descent(26/49): loss=61950.61354263925\n",
      "Gradient Descent(27/49): loss=106313.12524282452\n",
      "Gradient Descent(28/49): loss=182443.6315714992\n",
      "Gradient Descent(29/49): loss=313091.19348217046\n",
      "Gradient Descent(30/49): loss=537295.4744770784\n",
      "Gradient Descent(31/49): loss=922052.4410925236\n",
      "Gradient Descent(32/49): loss=1582333.8715012504\n",
      "Gradient Descent(33/49): loss=2715442.834225397\n",
      "Gradient Descent(34/49): loss=4659971.125156459\n",
      "Gradient Descent(35/49): loss=7996976.125223184\n",
      "Gradient Descent(36/49): loss=13723610.40583845\n",
      "Gradient Descent(37/49): loss=23551087.494799662\n",
      "Gradient Descent(38/49): loss=40416020.92716863\n",
      "Gradient Descent(39/49): loss=69357933.1904497\n",
      "Gradient Descent(40/49): loss=119025148.82547899\n",
      "Gradient Descent(41/49): loss=204259057.57673424\n",
      "Gradient Descent(42/49): loss=350528968.3847914\n",
      "Gradient Descent(43/49): loss=601542762.3225126\n",
      "Gradient Descent(44/49): loss=1032307534.0990118\n",
      "Gradient Descent(45/49): loss=1771542958.9447496\n",
      "Gradient Descent(46/49): loss=3040144871.522564\n",
      "Gradient Descent(47/49): loss=5217192613.697758\n",
      "Gradient Descent(48/49): loss=8953224244.043798\n",
      "Gradient Descent(49/49): loss=15364628124.880789\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5356810654323201\n",
      "Gradient Descent(2/49): loss=0.5969133418207307\n",
      "Gradient Descent(3/49): loss=0.7019940513308666\n",
      "Gradient Descent(4/49): loss=0.8823230569212616\n",
      "Gradient Descent(5/49): loss=1.1917856634149313\n",
      "Gradient Descent(6/49): loss=1.7228544424185686\n",
      "Gradient Descent(7/49): loss=2.6342215740666686\n",
      "Gradient Descent(8/49): loss=4.198218708687859\n",
      "Gradient Descent(9/49): loss=6.882194191411227\n",
      "Gradient Descent(10/49): loss=11.488164517313896\n",
      "Gradient Descent(11/49): loss=19.3924701935936\n",
      "Gradient Descent(12/49): loss=32.957049164658166\n",
      "Gradient Descent(13/49): loss=56.235223136908175\n",
      "Gradient Descent(14/49): loss=96.18289749068177\n",
      "Gradient Descent(15/49): loss=164.73710144920395\n",
      "Gradient Descent(16/49): loss=282.38297086240595\n",
      "Gradient Descent(17/49): loss=484.2750473624533\n",
      "Gradient Descent(18/49): loss=830.7420398441677\n",
      "Gradient Descent(19/49): loss=1425.3140456420574\n",
      "Gradient Descent(20/49): loss=2445.659064791859\n",
      "Gradient Descent(21/49): loss=4196.673152154842\n",
      "Gradient Descent(22/49): loss=7201.5884274788095\n",
      "Gradient Descent(23/49): loss=12358.323531462638\n",
      "Gradient Descent(24/49): loss=21207.79664341036\n",
      "Gradient Descent(25/49): loss=36394.37745082055\n",
      "Gradient Descent(26/49): loss=62456.06877441188\n",
      "Gradient Descent(27/49): loss=107180.53725482732\n",
      "Gradient Descent(28/49): loss=183932.19761407655\n",
      "Gradient Descent(29/49): loss=315645.7219565902\n",
      "Gradient Descent(30/49): loss=541679.3010807378\n",
      "Gradient Descent(31/49): loss=929575.526215832\n",
      "Gradient Descent(32/49): loss=1595244.2381700363\n",
      "Gradient Descent(33/49): loss=2737598.3147547753\n",
      "Gradient Descent(34/49): loss=4697992.145582125\n",
      "Gradient Descent(35/49): loss=8062223.998664592\n",
      "Gradient Descent(36/49): loss=13835582.281740114\n",
      "Gradient Descent(37/49): loss=23743242.431327436\n",
      "Gradient Descent(38/49): loss=40745778.01403525\n",
      "Gradient Descent(39/49): loss=69923829.32752019\n",
      "Gradient Descent(40/49): loss=119996283.1865895\n",
      "Gradient Descent(41/49): loss=205925621.2541342\n",
      "Gradient Descent(42/49): loss=353388958.31181276\n",
      "Gradient Descent(43/49): loss=606450791.0365733\n",
      "Gradient Descent(44/49): loss=1040730202.1754366\n",
      "Gradient Descent(45/49): loss=1785997099.6307294\n",
      "Gradient Descent(46/49): loss=3064949622.3539305\n",
      "Gradient Descent(47/49): loss=5259760046.59891\n",
      "Gradient Descent(48/49): loss=9026274215.64601\n",
      "Gradient Descent(49/49): loss=15489989181.148699\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5388082177646591\n",
      "Gradient Descent(2/49): loss=0.6084922535828841\n",
      "Gradient Descent(3/49): loss=0.7336169082981092\n",
      "Gradient Descent(4/49): loss=0.958290738304747\n",
      "Gradient Descent(5/49): loss=1.3617150674646525\n",
      "Gradient Descent(6/49): loss=2.086103792904245\n",
      "Gradient Descent(7/49): loss=3.386816188303753\n",
      "Gradient Descent(8/49): loss=5.722375365482505\n",
      "Gradient Descent(9/49): loss=9.916105424024659\n",
      "Gradient Descent(10/49): loss=17.446367117142056\n",
      "Gradient Descent(11/49): loss=30.9677050133066\n",
      "Gradient Descent(12/49): loss=55.246619339653144\n",
      "Gradient Descent(13/49): loss=98.84183790404013\n",
      "Gradient Descent(14/49): loss=177.12141235827337\n",
      "Gradient Descent(15/49): loss=317.6802162483076\n",
      "Gradient Descent(16/49): loss=570.0676045132643\n",
      "Gradient Descent(17/49): loss=1023.2543988818883\n",
      "Gradient Descent(18/49): loss=1836.996606850229\n",
      "Gradient Descent(19/49): loss=3298.1521154777283\n",
      "Gradient Descent(20/49): loss=5921.802946769309\n",
      "Gradient Descent(21/49): loss=10632.830379435774\n",
      "Gradient Descent(22/49): loss=19091.951237533252\n",
      "Gradient Descent(23/49): loss=34281.14865033051\n",
      "Gradient Descent(24/49): loss=61554.871524756956\n",
      "Gradient Descent(25/49): loss=110527.56831808474\n",
      "Gradient Descent(26/49): loss=198462.942680165\n",
      "Gradient Descent(27/49): loss=356359.70088476106\n",
      "Gradient Descent(28/49): loss=639879.1199168952\n",
      "Gradient Descent(29/49): loss=1148966.588731049\n",
      "Gradient Descent(30/49): loss=2063084.0477338096\n",
      "Gradient Descent(31/49): loss=3704473.357119069\n",
      "Gradient Descent(32/49): loss=6651752.001051714\n",
      "Gradient Descent(33/49): loss=11943885.534095747\n",
      "Gradient Descent(34/49): loss=21446440.506028295\n",
      "Gradient Descent(35/49): loss=38509228.21362923\n",
      "Gradient Descent(36/49): loss=69147169.8214058\n",
      "Gradient Descent(37/49): loss=124160657.77232565\n",
      "Gradient Descent(38/49): loss=222942876.73697084\n",
      "Gradient Descent(39/49): loss=400316229.10989183\n",
      "Gradient Descent(40/49): loss=718807820.6306705\n",
      "Gradient Descent(41/49): loss=1290691322.3655784\n",
      "Gradient Descent(42/49): loss=2317565338.080448\n",
      "Gradient Descent(43/49): loss=4161420320.6986246\n",
      "Gradient Descent(44/49): loss=7472246327.487146\n",
      "Gradient Descent(45/49): loss=13417165505.27757\n",
      "Gradient Descent(46/49): loss=24091862380.91557\n",
      "Gradient Descent(47/49): loss=43259348090.808914\n",
      "Gradient Descent(48/49): loss=77676485431.50635\n",
      "Gradient Descent(49/49): loss=139475897240.46643\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5397817361590271\n",
      "Gradient Descent(2/49): loss=0.6112138216061884\n",
      "Gradient Descent(3/49): loss=0.7394772742350758\n",
      "Gradient Descent(4/49): loss=0.9697871297755161\n",
      "Gradient Descent(5/49): loss=1.3833315063839076\n",
      "Gradient Descent(6/49): loss=2.1258917890221345\n",
      "Gradient Descent(7/49): loss=3.4592330325273584\n",
      "Gradient Descent(8/49): loss=5.85338056936473\n",
      "Gradient Descent(9/49): loss=10.152311886509827\n",
      "Gradient Descent(10/49): loss=17.87147295957622\n",
      "Gradient Descent(11/49): loss=31.731998582376818\n",
      "Gradient Descent(12/49): loss=56.61995839066975\n",
      "Gradient Descent(13/49): loss=101.30877902245031\n",
      "Gradient Descent(14/49): loss=181.55202534888014\n",
      "Gradient Descent(15/49): loss=325.6367984525798\n",
      "Gradient Descent(16/49): loss=584.3554170376087\n",
      "Gradient Descent(17/49): loss=1048.9105685689572\n",
      "Gradient Descent(18/49): loss=1883.0657986585031\n",
      "Gradient Descent(19/49): loss=3380.87492980742\n",
      "Gradient Descent(20/49): loss=6070.341005697878\n",
      "Gradient Descent(21/49): loss=10899.546291566716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=19570.86730287471\n",
      "Gradient Descent(23/49): loss=35141.091310778924\n",
      "Gradient Descent(24/49): loss=63098.985539363384\n",
      "Gradient Descent(25/49): loss=113300.18041622656\n",
      "Gradient Descent(26/49): loss=203441.44593713264\n",
      "Gradient Descent(27/49): loss=365299.10230644996\n",
      "Gradient Descent(28/49): loss=655930.7100831968\n",
      "Gradient Descent(29/49): loss=1177788.8250071818\n",
      "Gradient Descent(30/49): loss=2114837.256164827\n",
      "Gradient Descent(31/49): loss=3797401.419151594\n",
      "Gradient Descent(32/49): loss=6818613.630210366\n",
      "Gradient Descent(33/49): loss=12243502.276388125\n",
      "Gradient Descent(34/49): loss=21984432.329465564\n",
      "Gradient Descent(35/49): loss=39475246.33276673\n",
      "Gradient Descent(36/49): loss=70881751.95710197\n",
      "Gradient Descent(37/49): loss=127275273.45615876\n",
      "Gradient Descent(38/49): loss=228535480.6598537\n",
      "Gradient Descent(39/49): loss=410358308.7147881\n",
      "Gradient Descent(40/49): loss=736839378.7702991\n",
      "Gradient Descent(41/49): loss=1323068788.1618426\n",
      "Gradient Descent(42/49): loss=2375702315.665175\n",
      "Gradient Descent(43/49): loss=4265811077.6498885\n",
      "Gradient Descent(44/49): loss=7659690370.669551\n",
      "Gradient Descent(45/49): loss=13753740029.216501\n",
      "Gradient Descent(46/49): loss=24696215596.105045\n",
      "Gradient Descent(47/49): loss=44344524724.00649\n",
      "Gradient Descent(48/49): loss=79625028594.06892\n",
      "Gradient Descent(49/49): loss=142974701343.16132\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5393214798079998\n",
      "Gradient Descent(2/49): loss=0.609927128951244\n",
      "Gradient Descent(3/49): loss=0.7367066325528844\n",
      "Gradient Descent(4/49): loss=0.964351909219934\n",
      "Gradient Descent(5/49): loss=1.373111768003391\n",
      "Gradient Descent(6/49): loss=2.1070809704346916\n",
      "Gradient Descent(7/49): loss=3.424996070320729\n",
      "Gradient Descent(8/49): loss=5.791444423676293\n",
      "Gradient Descent(9/49): loss=10.040639086960539\n",
      "Gradient Descent(10/49): loss=17.670493024352883\n",
      "Gradient Descent(11/49): loss=31.37065875433265\n",
      "Gradient Descent(12/49): loss=55.970676339092684\n",
      "Gradient Descent(13/49): loss=100.14246791427627\n",
      "Gradient Descent(14/49): loss=179.45733686669504\n",
      "Gradient Descent(15/49): loss=321.87511555765724\n",
      "Gradient Descent(16/49): loss=577.6004789751213\n",
      "Gradient Descent(17/49): loss=1036.780941527443\n",
      "Gradient Descent(18/49): loss=1861.2853800866073\n",
      "Gradient Descent(19/49): loss=3341.7655499630177\n",
      "Gradient Descent(20/49): loss=6000.11574299302\n",
      "Gradient Descent(21/49): loss=10773.449349598228\n",
      "Gradient Descent(22/49): loss=19344.44717361736\n",
      "Gradient Descent(23/49): loss=34734.53086642908\n",
      "Gradient Descent(24/49): loss=62368.965145245595\n",
      "Gradient Descent(25/49): loss=111989.35533629025\n",
      "Gradient Descent(26/49): loss=201087.72796331882\n",
      "Gradient Descent(27/49): loss=361072.76585243276\n",
      "Gradient Descent(28/49): loss=648341.8998861018\n",
      "Gradient Descent(29/49): loss=1164162.3569570947\n",
      "Gradient Descent(30/49): loss=2090369.5696737636\n",
      "Gradient Descent(31/49): loss=3753467.2408279805\n",
      "Gradient Descent(32/49): loss=6739725.419151687\n",
      "Gradient Descent(33/49): loss=12101850.604149455\n",
      "Gradient Descent(34/49): loss=21730082.586331338\n",
      "Gradient Descent(35/49): loss=39018535.93353751\n",
      "Gradient Descent(36/49): loss=70061682.7637839\n",
      "Gradient Descent(37/49): loss=125802757.21216066\n",
      "Gradient Descent(38/49): loss=225891430.49168226\n",
      "Gradient Descent(39/49): loss=405610652.2324081\n",
      "Gradient Descent(40/49): loss=728314486.7900534\n",
      "Gradient Descent(41/49): loss=1307761492.1218362\n",
      "Gradient Descent(42/49): loss=2348216534.895646\n",
      "Gradient Descent(43/49): loss=4216457609.70036\n",
      "Gradient Descent(44/49): loss=7571071283.619035\n",
      "Gradient Descent(45/49): loss=13594615596.506922\n",
      "Gradient Descent(46/49): loss=24410491764.730484\n",
      "Gradient Descent(47/49): loss=43831479012.39558\n",
      "Gradient Descent(48/49): loss=78703803714.30876\n",
      "Gradient Descent(49/49): loss=141320549949.03412\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5396423064627199\n",
      "Gradient Descent(2/49): loss=0.6108240319471736\n",
      "Gradient Descent(3/49): loss=0.7386379382270533\n",
      "Gradient Descent(4/49): loss=0.9681405883431539\n",
      "Gradient Descent(5/49): loss=1.3802355468916236\n",
      "Gradient Descent(6/49): loss=2.1201932544614674\n",
      "Gradient Descent(7/49): loss=3.4488613141739215\n",
      "Gradient Descent(8/49): loss=5.834617682193177\n",
      "Gradient Descent(9/49): loss=10.118481816608623\n",
      "Gradient Descent(10/49): loss=17.810588256364063\n",
      "Gradient Descent(11/49): loss=31.622534579592276\n",
      "Gradient Descent(12/49): loss=56.42326539757238\n",
      "Gradient Descent(13/49): loss=100.95545765434254\n",
      "Gradient Descent(14/49): loss=180.91746207059109\n",
      "Gradient Descent(15/49): loss=324.4972372003911\n",
      "Gradient Descent(16/49): loss=582.3090814234976\n",
      "Gradient Descent(17/49): loss=1045.2360289104308\n",
      "Gradient Descent(18/49): loss=1876.4676558181911\n",
      "Gradient Descent(19/49): loss=3369.0271650935597\n",
      "Gradient Descent(20/49): loss=6049.06701994796\n",
      "Gradient Descent(21/49): loss=10861.34658332499\n",
      "Gradient Descent(22/49): loss=19502.275767325642\n",
      "Gradient Descent(23/49): loss=35017.928210116515\n",
      "Gradient Descent(24/49): loss=62877.833736389424\n",
      "Gradient Descent(25/49): loss=112903.08009935584\n",
      "Gradient Descent(26/49): loss=202728.41246869304\n",
      "Gradient Descent(27/49): loss=364018.77927113406\n",
      "Gradient Descent(28/49): loss=653631.7619015906\n",
      "Gradient Descent(29/49): loss=1173660.8335129202\n",
      "Gradient Descent(30/49): loss=2107425.0344980652\n",
      "Gradient Descent(31/49): loss=3784092.033786676\n",
      "Gradient Descent(32/49): loss=6794715.297709249\n",
      "Gradient Descent(33/49): loss=12200590.430407558\n",
      "Gradient Descent(34/49): loss=21907379.81867984\n",
      "Gradient Descent(35/49): loss=39336890.8442621\n",
      "Gradient Descent(36/49): loss=70633320.84179883\n",
      "Gradient Descent(37/49): loss=126829190.5453885\n",
      "Gradient Descent(38/49): loss=227734494.18515858\n",
      "Gradient Descent(39/49): loss=408920057.4007384\n",
      "Gradient Descent(40/49): loss=734256854.7105994\n",
      "Gradient Descent(41/49): loss=1318431607.960146\n",
      "Gradient Descent(42/49): loss=2367375794.8950076\n",
      "Gradient Descent(43/49): loss=4250859976.9549594\n",
      "Gradient Descent(44/49): loss=7632844174.262712\n",
      "Gradient Descent(45/49): loss=13705534998.948757\n",
      "Gradient Descent(46/49): loss=24609658643.756107\n",
      "Gradient Descent(47/49): loss=44189103060.3653\n",
      "Gradient Descent(48/49): loss=79345953454.83585\n",
      "Gradient Descent(49/49): loss=142473594023.15945\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5427739142255276\n",
      "Gradient Descent(2/49): loss=0.623056273835415\n",
      "Gradient Descent(3/49): loss=0.7737382345871849\n",
      "Gradient Descent(4/49): loss=1.0565532067222476\n",
      "Gradient Descent(5/49): loss=1.587368627922382\n",
      "Gradient Descent(6/49): loss=2.583656091972888\n",
      "Gradient Descent(7/49): loss=4.453588033249721\n",
      "Gradient Descent(8/49): loss=7.963263293831511\n",
      "Gradient Descent(9/49): loss=14.55057279041635\n",
      "Gradient Descent(10/49): loss=26.914293984560054\n",
      "Gradient Descent(11/49): loss=50.11976229384764\n",
      "Gradient Descent(12/49): loss=93.6741057635565\n",
      "Gradient Descent(13/49): loss=175.42125302185198\n",
      "Gradient Descent(14/49): loss=328.85247371093385\n",
      "Gradient Descent(15/49): loss=616.8275318222921\n",
      "Gradient Descent(16/49): loss=1157.3279183915276\n",
      "Gradient Descent(17/49): loss=2171.793093943126\n",
      "Gradient Descent(18/49): loss=4075.8427819360027\n",
      "Gradient Descent(19/49): loss=7649.553641329216\n",
      "Gradient Descent(20/49): loss=14357.051553326019\n",
      "Gradient Descent(21/49): loss=26946.354384353108\n",
      "Gradient Descent(22/49): loss=50575.216867900635\n",
      "Gradient Descent(23/49): loss=94924.22886327632\n",
      "Gradient Descent(24/49): loss=178162.8894774093\n",
      "Gradient Descent(25/49): loss=334393.5315841027\n",
      "Gradient Descent(26/49): loss=627622.8237541704\n",
      "Gradient Descent(27/49): loss=1177984.8822280825\n",
      "Gradient Descent(28/49): loss=2210959.429777961\n",
      "Gradient Descent(29/49): loss=4149749.358074015\n",
      "Gradient Descent(30/49): loss=7788664.174492261\n",
      "Gradient Descent(31/49): loss=14618543.39342849\n",
      "Gradient Descent(32/49): loss=27437543.69945275\n",
      "Gradient Descent(33/49): loss=51497525.373830624\n",
      "Gradient Descent(34/49): loss=96655704.97846214\n",
      "Gradient Descent(35/49): loss=181413092.2784067\n",
      "Gradient Descent(36/49): loss=340494232.50166845\n",
      "Gradient Descent(37/49): loss=639073624.5867424\n",
      "Gradient Descent(38/49): loss=1199477285.5910826\n",
      "Gradient Descent(39/49): loss=2251298916.9303765\n",
      "Gradient Descent(40/49): loss=4225462936.790725\n",
      "Gradient Descent(41/49): loss=7930771385.666675\n",
      "Gradient Descent(42/49): loss=14885264813.362467\n",
      "Gradient Descent(43/49): loss=27938153527.8023\n",
      "Gradient Descent(44/49): loss=52437120355.9317\n",
      "Gradient Descent(45/49): loss=98419231195.66078\n",
      "Gradient Descent(46/49): loss=184723055030.73798\n",
      "Gradient Descent(47/49): loss=346706701986.82367\n",
      "Gradient Descent(48/49): loss=650733808958.6649\n",
      "Gradient Descent(49/49): loss=1221362286034.0354\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5438469135719592\n",
      "Gradient Descent(2/49): loss=0.6261431856551465\n",
      "Gradient Descent(3/49): loss=0.7806050587281156\n",
      "Gradient Descent(4/49): loss=1.0705145482987422\n",
      "Gradient Descent(5/49): loss=1.614645669273806\n",
      "Gradient Descent(6/49): loss=2.635925370232236\n",
      "Gradient Descent(7/49): loss=4.552765240961067\n",
      "Gradient Descent(8/49): loss=8.15048199433252\n",
      "Gradient Descent(9/49): loss=14.903036568735493\n",
      "Gradient Descent(10/49): loss=27.57690624942865\n",
      "Gradient Descent(11/49): loss=51.364492253120524\n",
      "Gradient Descent(12/49): loss=96.01141242344964\n",
      "Gradient Descent(13/49): loss=179.80921689115038\n",
      "Gradient Descent(14/49): loss=337.08931609657844\n",
      "Gradient Descent(15/49): loss=632.2883342952209\n",
      "Gradient Descent(16/49): loss=1186.3473715522573\n",
      "Gradient Descent(17/49): loss=2226.2607785798937\n",
      "Gradient Descent(18/49): loss=4178.074252230429\n",
      "Gradient Descent(19/49): loss=7841.432960924794\n",
      "Gradient Descent(20/49): loss=14717.190921273997\n",
      "Gradient Descent(21/49): loss=27622.30103705161\n",
      "Gradient Descent(22/49): loss=51843.90221335718\n",
      "Gradient Descent(23/49): loss=97305.42546117117\n",
      "Gradient Descent(24/49): loss=182632.15844499134\n",
      "Gradient Descent(25/49): loss=342781.9035823052\n",
      "Gradient Descent(26/49): loss=643366.9602306003\n",
      "Gradient Descent(27/49): loss=1207535.053053845\n",
      "Gradient Descent(28/49): loss=2266422.1464737183\n",
      "Gradient Descent(29/49): loss=4253847.3321135165\n",
      "Gradient Descent(30/49): loss=7984045.663039952\n",
      "Gradient Descent(31/49): loss=14985254.910358159\n",
      "Gradient Descent(32/49): loss=28125824.54665118\n",
      "Gradient Descent(33/49): loss=52789359.69700035\n",
      "Gradient Descent(34/49): loss=99080348.82068698\n",
      "Gradient Descent(35/49): loss=185963906.30695328\n",
      "Gradient Descent(36/49): loss=349035655.3529037\n",
      "Gradient Descent(37/49): loss=655105021.1372029\n",
      "Gradient Descent(38/49): loss=1229566613.7778993\n",
      "Gradient Descent(39/49): loss=2307773577.0053515\n",
      "Gradient Descent(40/49): loss=4331460226.287052\n",
      "Gradient Descent(41/49): loss=8129717698.3239\n",
      "Gradient Descent(42/49): loss=15258667147.589088\n",
      "Gradient Descent(43/49): loss=28638992368.913876\n",
      "Gradient Descent(44/49): loss=53752524776.81984\n",
      "Gradient Descent(45/49): loss=100888113753.22565\n",
      "Gradient Descent(46/49): loss=189356900703.03473\n",
      "Gradient Descent(47/49): loss=355403966929.1148\n",
      "Gradient Descent(48/49): loss=667057705528.8887\n",
      "Gradient Descent(49/49): loss=1252000607506.644\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5433396249920001\n",
      "Gradient Descent(2/49): loss=0.624683767139501\n",
      "Gradient Descent(3/49): loss=0.7773585875361027\n",
      "Gradient Descent(4/49): loss=1.063913957938567\n",
      "Gradient Descent(5/49): loss=1.6017497326469157\n",
      "Gradient Descent(6/49): loss=2.611213698197185\n",
      "Gradient Descent(7/49): loss=4.505876615138028\n",
      "Gradient Descent(8/49): loss=8.06196944394493\n",
      "Gradient Descent(9/49): loss=14.73640007433118\n",
      "Gradient Descent(10/49): loss=27.263638924500878\n",
      "Gradient Descent(11/49): loss=50.77601352239435\n",
      "Gradient Descent(12/49): loss=94.90638940517586\n",
      "Gradient Descent(13/49): loss=177.73469189956097\n",
      "Gradient Descent(14/49): loss=333.1951328512673\n",
      "Gradient Descent(15/49): loss=624.978834473451\n",
      "Gradient Descent(16/49): loss=1172.6276640482008\n",
      "Gradient Descent(17/49): loss=2200.50975227677\n",
      "Gradient Descent(18/49): loss=4129.7416436734775\n",
      "Gradient Descent(19/49): loss=7750.716980636714\n",
      "Gradient Descent(20/49): loss=14546.92559058284\n",
      "Gradient Descent(21/49): loss=27302.7295305873\n",
      "Gradient Descent(22/49): loss=51244.09794558265\n",
      "Gradient Descent(23/49): loss=96179.65232369446\n",
      "Gradient Descent(24/49): loss=180519.19433596852\n",
      "Gradient Descent(25/49): loss=338816.08073878154\n",
      "Gradient Descent(26/49): loss=635923.5068282982\n",
      "Gradient Descent(27/49): loss=1193564.4348555824\n",
      "Gradient Descent(28/49): loss=2240200.692670147\n",
      "Gradient Descent(29/49): loss=4204632.284962363\n",
      "Gradient Descent(30/49): loss=7891673.940534973\n",
      "Gradient Descent(31/49): loss=14811882.423880639\n",
      "Gradient Descent(32/49): loss=27800421.726273306\n",
      "Gradient Descent(33/49): loss=52178611.14292781\n",
      "Gradient Descent(34/49): loss=97934034.85904527\n",
      "Gradient Descent(35/49): loss=183812389.63184232\n",
      "Gradient Descent(36/49): loss=344997473.70491344\n",
      "Gradient Descent(37/49): loss=647525758.0015956\n",
      "Gradient Descent(38/49): loss=1215341094.798007\n",
      "Gradient Descent(39/49): loss=2281073700.4313025\n",
      "Gradient Descent(40/49): loss=4281347227.944592\n",
      "Gradient Descent(41/49): loss=8035660611.733864\n",
      "Gradient Descent(42/49): loss=15082131401.766432\n",
      "Gradient Descent(43/49): loss=28307652427.583424\n",
      "Gradient Descent(44/49): loss=53130632840.9331\n",
      "Gradient Descent(45/49): loss=99720884778.74225\n",
      "Gradient Descent(46/49): loss=187166128640.81418\n",
      "Gradient Descent(47/49): loss=351292106845.5555\n",
      "Gradient Descent(48/49): loss=659340155338.0609\n",
      "Gradient Descent(49/49): loss=1237515537553.565\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.54369323596928\n",
      "Gradient Descent(2/49): loss=0.6257010705600263\n",
      "Gradient Descent(3/49): loss=0.7796215753033707\n",
      "Gradient Descent(4/49): loss=1.0685149706562371\n",
      "Gradient Descent(5/49): loss=1.6107389843939295\n",
      "Gradient Descent(6/49): loss=2.628439235778544\n",
      "Gradient Descent(7/49): loss=4.538560837601707\n",
      "Gradient Descent(8/49): loss=8.123668072064762\n",
      "Gradient Descent(9/49): loss=14.852555840426803\n",
      "Gradient Descent(10/49): loss=27.48200529286508\n",
      "Gradient Descent(11/49): loss=51.18621897014355\n",
      "Gradient Descent(12/49): loss=95.67665762102273\n",
      "Gradient Descent(13/49): loss=179.18076192487098\n",
      "Gradient Descent(14/49): loss=335.9096152927453\n",
      "Gradient Descent(15/49): loss=630.0740001788528\n",
      "Gradient Descent(16/49): loss=1182.1911341716834\n",
      "Gradient Descent(17/49): loss=2218.4597829629424\n",
      "Gradient Descent(18/49): loss=4163.432409879535\n",
      "Gradient Descent(19/49): loss=7813.951533338258\n",
      "Gradient Descent(20/49): loss=14665.61087616018\n",
      "Gradient Descent(21/49): loss=27525.490296699594\n",
      "Gradient Descent(22/49): loss=51662.19798110835\n",
      "Gradient Descent(23/49): loss=96964.38463398523\n",
      "Gradient Descent(24/49): loss=181992.05876274724\n",
      "Gradient Descent(25/49): loss=341580.50033506355\n",
      "Gradient Descent(26/49): loss=641112.0463220748\n",
      "Gradient Descent(27/49): loss=1203302.8049850853\n",
      "Gradient Descent(28/49): loss=2258478.639919832\n",
      "Gradient Descent(29/49): loss=4238938.164508548\n",
      "Gradient Descent(30/49): loss=7956062.646208779\n",
      "Gradient Descent(31/49): loss=14932733.58591358\n",
      "Gradient Descent(32/49): loss=28027247.272647195\n",
      "Gradient Descent(33/49): loss=52604340.01127404\n",
      "Gradient Descent(34/49): loss=98733085.37241417\n",
      "Gradient Descent(35/49): loss=185312127.5407246\n",
      "Gradient Descent(36/49): loss=347812331.7864457\n",
      "Gradient Descent(37/49): loss=652808965.1352062\n",
      "Gradient Descent(38/49): loss=1225257146.2676368\n",
      "Gradient Descent(39/49): loss=2299685137.43492\n",
      "Gradient Descent(40/49): loss=4316279034.057294\n",
      "Gradient Descent(41/49): loss=8101224118.6282\n",
      "Gradient Descent(42/49): loss=15205187547.858315\n",
      "Gradient Descent(43/49): loss=28538616508.17785\n",
      "Gradient Descent(44/49): loss=53564129323.80157\n",
      "Gradient Descent(45/49): loss=100534514327.45111\n",
      "Gradient Descent(46/49): loss=188693229940.8037\n",
      "Gradient Descent(47/49): loss=354158323275.4616\n",
      "Gradient Descent(48/49): loss=664719756955.2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=1247612511828.8118\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5468274120840536\n",
      "Gradient Descent(2/49): loss=0.6386091397687865\n",
      "Gradient Descent(3/49): loss=0.8185013260308572\n",
      "Gradient Descent(4/49): loss=1.1710900111045517\n",
      "Gradient Descent(5/49): loss=1.8621638338488542\n",
      "Gradient Descent(6/49): loss=3.2166685264276773\n",
      "Gradient Descent(7/49): loss=5.871497723882092\n",
      "Gradient Descent(8/49): loss=11.074962950893923\n",
      "Gradient Descent(9/49): loss=21.273754795834755\n",
      "Gradient Descent(10/49): loss=41.263386811920675\n",
      "Gradient Descent(11/49): loss=80.44306556345464\n",
      "Gradient Descent(12/49): loss=157.23523591644323\n",
      "Gradient Descent(13/49): loss=307.7478898083369\n",
      "Gradient Descent(14/49): loss=602.7526914363963\n",
      "Gradient Descent(15/49): loss=1180.9621026275313\n",
      "Gradient Descent(16/49): loss=2314.2525485621695\n",
      "Gradient Descent(17/49): loss=4535.501822594111\n",
      "Gradient Descent(18/49): loss=8889.150399696899\n",
      "Gradient Descent(19/49): loss=17422.301610819282\n",
      "Gradient Descent(20/49): loss=34147.27798462197\n",
      "Gradient Descent(21/49): loss=66928.23167727704\n",
      "Gradient Descent(22/49): loss=131178.9009148837\n",
      "Gradient Descent(23/49): loss=257110.2126205578\n",
      "Gradient Descent(24/49): loss=503935.58356368466\n",
      "Gradient Descent(25/49): loss=987713.3106122839\n",
      "Gradient Descent(26/49): loss=1935917.6556273845\n",
      "Gradient Descent(27/49): loss=3794398.1718571014\n",
      "Gradient Descent(28/49): loss=7437019.983667301\n",
      "Gradient Descent(29/49): loss=14576558.734815344\n",
      "Gradient Descent(30/49): loss=28570054.68706372\n",
      "Gradient Descent(31/49): loss=55997306.753476344\n",
      "Gradient Descent(32/49): loss=109754720.8036453\n",
      "Gradient Descent(33/49): loss=215119252.34199184\n",
      "Gradient Descent(34/49): loss=421633734.1571748\n",
      "Gradient Descent(35/49): loss=826402118.5148278\n",
      "Gradient Descent(36/49): loss=1619748151.8557274\n",
      "Gradient Descent(37/49): loss=3174706377.2044168\n",
      "Gradient Descent(38/49): loss=6222424498.887081\n",
      "Gradient Descent(39/49): loss=12195952017.383991\n",
      "Gradient Descent(40/49): loss=23904065953.641052\n",
      "Gradient Descent(41/49): loss=46851969268.70001\n",
      "Gradient Descent(42/49): loss=91829859766.2258\n",
      "Gradient Descent(43/49): loss=179986525141.35886\n",
      "Gradient Descent(44/49): loss=352773589276.59125\n",
      "Gradient Descent(45/49): loss=691436234981.7223\n",
      "Gradient Descent(46/49): loss=1355215020563.7146\n",
      "Gradient Descent(47/49): loss=2656221440304.2153\n",
      "Gradient Descent(48/49): loss=5206194022995.348\n",
      "Gradient Descent(49/49): loss=10204140285070.258\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5480020949128535\n",
      "Gradient Descent(2/49): loss=0.6420862009420497\n",
      "Gradient Descent(3/49): loss=0.8264910487592472\n",
      "Gradient Descent(4/49): loss=1.18792455048097\n",
      "Gradient Descent(5/49): loss=1.8963342138554404\n",
      "Gradient Descent(6/49): loss=3.2848171540697266\n",
      "Gradient Descent(7/49): loss=6.006243716889308\n",
      "Gradient Descent(8/49): loss=11.340239780015617\n",
      "Gradient Descent(9/49): loss=21.794872063742964\n",
      "Gradient Descent(10/49): loss=42.285951339849994\n",
      "Gradient Descent(11/49): loss=82.44846672102298\n",
      "Gradient Descent(12/49): loss=161.1669968681194\n",
      "Gradient Descent(13/49): loss=315.4553159564097\n",
      "Gradient Descent(14/49): loss=617.8604213694738\n",
      "Gradient Descent(15/49): loss=1210.5744279791713\n",
      "Gradient Descent(16/49): loss=2372.2938809339416\n",
      "Gradient Descent(17/49): loss=4649.264008725033\n",
      "Gradient Descent(18/49): loss=9112.125459195255\n",
      "Gradient Descent(19/49): loss=17859.333902117145\n",
      "Gradient Descent(20/49): loss=35003.86245024353\n",
      "Gradient Descent(21/49): loss=68607.13840456943\n",
      "Gradient Descent(22/49): loss=134469.55927505324\n",
      "Gradient Descent(23/49): loss=263559.9041811701\n",
      "Gradient Descent(24/49): loss=516576.98019717\n",
      "Gradient Descent(25/49): loss=1012490.4491884925\n",
      "Gradient Descent(26/49): loss=1984480.848411635\n",
      "Gradient Descent(27/49): loss=3889582.03088898\n",
      "Gradient Descent(28/49): loss=7623580.348544155\n",
      "Gradient Descent(29/49): loss=14942217.051149834\n",
      "Gradient Descent(30/49): loss=29286744.988253597\n",
      "Gradient Descent(31/49): loss=57402019.74497463\n",
      "Gradient Descent(32/49): loss=112507958.26815836\n",
      "Gradient Descent(33/49): loss=220515597.77360556\n",
      "Gradient Descent(34/49): loss=432210571.2042951\n",
      "Gradient Descent(35/49): loss=847132719.1284643\n",
      "Gradient Descent(36/49): loss=1660380129.0595663\n",
      "Gradient Descent(37/49): loss=3254345052.524997\n",
      "Gradient Descent(38/49): loss=6378516302.517346\n",
      "Gradient Descent(39/49): loss=12501891952.500328\n",
      "Gradient Descent(40/49): loss=24503708226.467648\n",
      "Gradient Descent(41/49): loss=48027268123.44616\n",
      "Gradient Descent(42/49): loss=94133445521.52182\n",
      "Gradient Descent(43/49): loss=184501553221.75693\n",
      "Gradient Descent(44/49): loss=361623044314.18866\n",
      "Gradient Descent(45/49): loss=708781166855.3618\n",
      "Gradient Descent(46/49): loss=1389211087035.9873\n",
      "Gradient Descent(47/49): loss=2722853730590.2407\n",
      "Gradient Descent(48/49): loss=5336793311956.889\n",
      "Gradient Descent(49/49): loss=10460114891435.623\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5474467328000001\n",
      "Gradient Descent(2/49): loss=0.6404423290880048\n",
      "Gradient Descent(3/49): loss=0.8227136978124949\n",
      "Gradient Descent(4/49): loss=1.1799655805124432\n",
      "Gradient Descent(5/49): loss=1.8801792706045077\n",
      "Gradient Descent(6/49): loss=3.2525981031848406\n",
      "Gradient Descent(7/49): loss=5.942539015042736\n",
      "Gradient Descent(8/49): loss=11.214823202284174\n",
      "Gradient Descent(9/49): loss=21.548500209278927\n",
      "Gradient Descent(10/49): loss=41.80250714298411\n",
      "Gradient Descent(11/49): loss=81.50036073305455\n",
      "Gradient Descent(12/49): loss=159.30815376957608\n",
      "Gradient Descent(13/49): loss=311.81142812114547\n",
      "Gradient Descent(14/49): loss=610.717845850313\n",
      "Gradient Descent(15/49): loss=1196.5744245995395\n",
      "Gradient Descent(16/49): loss=2344.853318947725\n",
      "Gradient Descent(17/49): loss=4595.479951870067\n",
      "Gradient Descent(18/49): loss=9006.708152397478\n",
      "Gradient Descent(19/49): loss=17652.71542543399\n",
      "Gradient Descent(20/49): loss=34598.889680583474\n",
      "Gradient Descent(21/49): loss=67813.39122067021\n",
      "Gradient Descent(22/49): loss=132913.8142392324\n",
      "Gradient Descent(23/49): loss=260510.643355639\n",
      "Gradient Descent(24/49): loss=510600.42842373956\n",
      "Gradient Descent(25/49): loss=1000776.4071572431\n",
      "Gradient Descent(26/49): loss=1961521.3254747253\n",
      "Gradient Descent(27/49): loss=3844581.3653774178\n",
      "Gradient Descent(28/49): loss=7535379.04358601\n",
      "Gradient Descent(29/49): loss=14769342.492874673\n",
      "Gradient Descent(30/49): loss=28947910.853482764\n",
      "Gradient Descent(31/49): loss=56737904.84027157\n",
      "Gradient Descent(32/49): loss=111206293.05437781\n",
      "Gradient Descent(33/49): loss=217964333.95403925\n",
      "Gradient Descent(34/49): loss=427210094.1173391\n",
      "Gradient Descent(35/49): loss=837331784.0374715\n",
      "Gradient Descent(36/49): loss=1641170296.280706\n",
      "Gradient Descent(37/49): loss=3216693780.2772293\n",
      "Gradient Descent(38/49): loss=6304719808.910231\n",
      "Gradient Descent(39/49): loss=12357250825.030323\n",
      "Gradient Descent(40/49): loss=24220211616.624153\n",
      "Gradient Descent(41/49): loss=47471614768.153366\n",
      "Gradient Descent(42/49): loss=93044364945.1551\n",
      "Gradient Descent(43/49): loss=182366955292.08008\n",
      "Gradient Descent(44/49): loss=357439232372.0277\n",
      "Gradient Descent(45/49): loss=700580895448.7405\n",
      "Gradient Descent(46/49): loss=1373138555079.0137\n",
      "Gradient Descent(47/49): loss=2691351567954.4604\n",
      "Gradient Descent(48/49): loss=5275049073190.628\n",
      "Gradient Descent(49/49): loss=10339096183452.773\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.547833853952\n",
      "Gradient Descent(2/49): loss=0.6415882076979047\n",
      "Gradient Descent(3/49): loss=0.8253467410398662\n",
      "Gradient Descent(4/49): loss=1.185513466390169\n",
      "Gradient Descent(5/49): loss=1.8914402480768602\n",
      "Gradient Descent(6/49): loss=3.275056740182934\n",
      "Gradient Descent(7/49): loss=5.986945064710528\n",
      "Gradient Descent(8/49): loss=11.302246180783998\n",
      "Gradient Descent(9/49): loss=21.72023636828895\n",
      "Gradient Descent(10/49): loss=42.13949713579933\n",
      "Gradient Descent(11/49): loss=82.1612482401226\n",
      "Gradient Descent(12/49): loss=160.6038804045852\n",
      "Gradient Descent(13/49): loss=314.351439446942\n",
      "Gradient Descent(14/49): loss=615.6966551699663\n",
      "Gradient Descent(15/49): loss=1206.333277987153\n",
      "Gradient Descent(16/49): loss=2363.9810587085867\n",
      "Gradient Descent(17/49): loss=4632.970708923178\n",
      "Gradient Descent(18/49): loss=9080.190423343747\n",
      "Gradient Descent(19/49): loss=17796.741063607937\n",
      "Gradient Descent(20/49): loss=34881.1803185271\n",
      "Gradient Descent(21/49): loss=68366.68125816637\n",
      "Gradient Descent(22/49): loss=133998.26309987225\n",
      "Gradient Descent(23/49): loss=262636.16350963124\n",
      "Gradient Descent(24/49): loss=514766.44831276906\n",
      "Gradient Descent(25/49): loss=1008941.8065267778\n",
      "Gradient Descent(26/49): loss=1977525.5086263334\n",
      "Gradient Descent(27/49): loss=3875949.5647411183\n",
      "Gradient Descent(28/49): loss=7596860.714725758\n",
      "Gradient Descent(29/49): loss=14889846.568695094\n",
      "Gradient Descent(30/49): loss=29184098.84247837\n",
      "Gradient Descent(31/49): loss=57200833.29909651\n",
      "Gradient Descent(32/49): loss=112113632.8340543\n",
      "Gradient Descent(33/49): loss=219742719.92258397\n",
      "Gradient Descent(34/49): loss=430695730.6161333\n",
      "Gradient Descent(35/49): loss=844163631.5754054\n",
      "Gradient Descent(36/49): loss=1654560717.4557142\n",
      "Gradient Descent(37/49): loss=3242939005.781152\n",
      "Gradient Descent(38/49): loss=6356160450.89924\n",
      "Gradient Descent(39/49): loss=12458074483.32976\n",
      "Gradient Descent(40/49): loss=24417825986.895077\n",
      "Gradient Descent(41/49): loss=47858938933.88165\n",
      "Gradient Descent(42/49): loss=93803520309.98006\n",
      "Gradient Descent(43/49): loss=183854899807.12558\n",
      "Gradient Descent(44/49): loss=360355603621.563\n",
      "Gradient Descent(45/49): loss=706296983097.8718\n",
      "Gradient Descent(46/49): loss=1384342086871.361\n",
      "Gradient Descent(47/49): loss=2713310490267.2925\n",
      "Gradient Descent(48/49): loss=5318088560923.532\n",
      "Gradient Descent(49/49): loss=10423453579410.531\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.550968711340237\n",
      "Gradient Descent(2/49): loss=0.6551946291598972\n",
      "Gradient Descent(3/49): loss=0.8683262085093161\n",
      "Gradient Descent(4/49): loss=1.3041589751208769\n",
      "Gradient Descent(5/49): loss=2.1953933995647836\n",
      "Gradient Descent(6/49): loss=4.017878674110084\n",
      "Gradient Descent(7/49): loss=7.744678812028254\n",
      "Gradient Descent(8/49): loss=15.365612414056644\n",
      "Gradient Descent(9/49): loss=30.949659536847513\n",
      "Gradient Descent(10/49): loss=62.8174774982346\n",
      "Gradient Descent(11/49): loss=127.98397844746546\n",
      "Gradient Descent(12/49): loss=261.24295623855005\n",
      "Gradient Descent(13/49): loss=533.7442399235294\n",
      "Gradient Descent(14/49): loss=1090.9821149309007\n",
      "Gradient Descent(15/49): loss=2230.477845533661\n",
      "Gradient Descent(16/49): loss=4560.632665043415\n",
      "Gradient Descent(17/49): loss=9325.566255457852\n",
      "Gradient Descent(18/49): loss=19069.378954496056\n",
      "Gradient Descent(19/49): loss=38994.50154276376\n",
      "Gradient Descent(20/49): loss=79739.38472350029\n",
      "Gradient Descent(21/49): loss=163058.5963397981\n",
      "Gradient Descent(22/49): loss=333438.05217399617\n",
      "Gradient Descent(23/49): loss=681847.0014093546\n",
      "Gradient Descent(24/49): loss=1394308.4617005852\n",
      "Gradient Descent(25/49): loss=2851220.901849993\n",
      "Gradient Descent(26/49): loss=5830461.150712032\n",
      "Gradient Descent(27/49): loss=11922709.535609253\n",
      "Gradient Descent(28/49): loss=24380748.25788557\n",
      "Gradient Descent(29/49): loss=49856191.6410721\n",
      "Gradient Descent(30/49): loss=101950925.81534454\n",
      "Gradient Descent(31/49): loss=208479447.72828892\n",
      "Gradient Descent(32/49): loss=426319622.188053\n",
      "Gradient Descent(33/49): loss=871780994.9409652\n",
      "Gradient Descent(34/49): loss=1782704956.0832024\n",
      "Gradient Descent(35/49): loss=3645453364.2232647\n",
      "Gradient Descent(36/49): loss=7454587584.028281\n",
      "Gradient Descent(37/49): loss=15243886150.106035\n",
      "Gradient Descent(38/49): loss=31172222787.87901\n",
      "Gradient Descent(39/49): loss=63744078378.465294\n",
      "Gradient Descent(40/49): loss=130350265875.6585\n",
      "Gradient Descent(41/49): loss=266553258688.65344\n",
      "Gradient Descent(42/49): loss=545074758691.9267\n",
      "Gradient Descent(43/49): loss=1114623374048.763\n",
      "Gradient Descent(44/49): loss=2279293337591.7524\n",
      "Gradient Descent(45/49): loss=4660926946040.447\n",
      "Gradient Descent(46/49): loss=9531129511957.61\n",
      "Gradient Descent(47/49): loss=19490206739002.65\n",
      "Gradient Descent(48/49): loss=39855523760588.61\n",
      "Gradient Descent(49/49): loss=81500560538035.78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5522472801817085\n",
      "Gradient Descent(2/49): loss=0.6590877434252758\n",
      "Gradient Descent(3/49): loss=0.8775658067120105\n",
      "Gradient Descent(4/49): loss=1.3243315983271025\n",
      "Gradient Descent(5/49): loss=2.237922965600959\n",
      "Gradient Descent(6/49): loss=4.106125952539224\n",
      "Gradient Descent(7/49): loss=7.926414240528573\n",
      "Gradient Descent(8/49): loss=15.738521760639744\n",
      "Gradient Descent(9/49): loss=31.71350042851228\n",
      "Gradient Descent(10/49): loss=64.38073430643959\n",
      "Gradient Descent(11/49): loss=131.1819608634312\n",
      "Gradient Descent(12/49): loss=267.783789049834\n",
      "Gradient Descent(13/49): loss=547.120867508166\n",
      "Gradient Descent(14/49): loss=1118.3372592476906\n",
      "Gradient Descent(15/49): loss=2286.417658715547\n",
      "Gradient Descent(16/49): loss=4675.02526758807\n",
      "Gradient Descent(17/49): loss=9559.48896697039\n",
      "Gradient Descent(18/49): loss=19547.72878583944\n",
      "Gradient Descent(19/49): loss=39972.68039144329\n",
      "Gradient Descent(20/49): loss=81739.66392974737\n",
      "Gradient Descent(21/49): loss=167148.96856723633\n",
      "Gradient Descent(22/49): loss=341802.4556204488\n",
      "Gradient Descent(23/49): loss=698951.3712955471\n",
      "Gradient Descent(24/49): loss=1429285.1889594153\n",
      "Gradient Descent(25/49): loss=2922744.8127005408\n",
      "Gradient Descent(26/49): loss=5976720.39728803\n",
      "Gradient Descent(27/49): loss=12221795.070212973\n",
      "Gradient Descent(28/49): loss=24992348.26887675\n",
      "Gradient Descent(29/49): loss=51106852.50482616\n",
      "Gradient Descent(30/49): loss=104508402.2169081\n",
      "Gradient Descent(31/49): loss=213709231.2231629\n",
      "Gradient Descent(32/49): loss=437014006.4580035\n",
      "Gradient Descent(33/49): loss=893649941.3358133\n",
      "Gradient Descent(34/49): loss=1827424764.5671942\n",
      "Gradient Descent(35/49): loss=3736900900.593268\n",
      "Gradient Descent(36/49): loss=7641588651.153028\n",
      "Gradient Descent(37/49): loss=15626284632.27188\n",
      "Gradient Descent(38/49): loss=31954189444.058453\n",
      "Gradient Descent(39/49): loss=65343121993.68989\n",
      "Gradient Descent(40/49): loss=133620150164.4286\n",
      "Gradient Descent(41/49): loss=273239845070.73456\n",
      "Gradient Descent(42/49): loss=558748159184.6208\n",
      "Gradient Descent(43/49): loss=1142584110716.2795\n",
      "Gradient Descent(44/49): loss=2336470248003.468\n",
      "Gradient Descent(45/49): loss=4777848010142.125\n",
      "Gradient Descent(46/49): loss=9770221395940.348\n",
      "Gradient Descent(47/49): loss=19979125732558.727\n",
      "Gradient Descent(48/49): loss=40855314210512.74\n",
      "Gradient Descent(49/49): loss=83545032029071.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5516428032319999\n",
      "Gradient Descent(2/49): loss=0.6572471715611119\n",
      "Gradient Descent(3/49): loss=0.8731975443573381\n",
      "Gradient Descent(4/49): loss=1.3147944616883775\n",
      "Gradient Descent(5/49): loss=2.2178159979387844\n",
      "Gradient Descent(6/49): loss=4.064404737417317\n",
      "Gradient Descent(7/49): loss=7.840494050776405\n",
      "Gradient Descent(8/49): loss=15.56221908766363\n",
      "Gradient Descent(9/49): loss=31.35237461559353\n",
      "Gradient Descent(10/49): loss=63.64166365466008\n",
      "Gradient Descent(11/49): loss=129.67003081065232\n",
      "Gradient Descent(12/49): loss=264.69143880792615\n",
      "Gradient Descent(13/49): loss=540.796716021589\n",
      "Gradient Descent(14/49): loss=1105.4043973957162\n",
      "Gradient Descent(15/49): loss=2259.970645037589\n",
      "Gradient Descent(16/49): loss=4620.94316484082\n",
      "Gradient Descent(17/49): loss=9448.895870587241\n",
      "Gradient Descent(18/49): loss=19321.576358568167\n",
      "Gradient Descent(19/49): loss=39510.22068844092\n",
      "Gradient Descent(20/49): loss=80793.97947859619\n",
      "Gradient Descent(21/49): loss=165215.13782858607\n",
      "Gradient Descent(22/49): loss=337847.96453850175\n",
      "Gradient Descent(23/49): loss=690864.8318775838\n",
      "Gradient Descent(24/49): loss=1412749.0238994304\n",
      "Gradient Descent(25/49): loss=2888930.00816475\n",
      "Gradient Descent(26/49): loss=5907572.502888744\n",
      "Gradient Descent(27/49): loss=12080394.540350568\n",
      "Gradient Descent(28/49): loss=24703198.324754994\n",
      "Gradient Descent(29/49): loss=50515569.783487916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=103299288.17945212\n",
      "Gradient Descent(31/49): loss=211236713.92734045\n",
      "Gradient Descent(32/49): loss=431957955.83918816\n",
      "Gradient Descent(33/49): loss=883310823.424671\n",
      "Gradient Descent(34/49): loss=1806282302.350118\n",
      "Gradient Descent(35/49): loss=3693666679.604692\n",
      "Gradient Descent(36/49): loss=7553178992.653441\n",
      "Gradient Descent(37/49): loss=15445495721.60631\n",
      "Gradient Descent(38/49): loss=31584494200.640297\n",
      "Gradient Descent(39/49): loss=64587132190.41989\n",
      "Gradient Descent(40/49): loss=132074226615.70248\n",
      "Gradient Descent(41/49): loss=270078586005.95786\n",
      "Gradient Descent(42/49): loss=552283700523.1837\n",
      "Gradient Descent(43/49): loss=1129364939199.515\n",
      "Gradient Descent(44/49): loss=2309438364168.632\n",
      "Gradient Descent(45/49): loss=4722570510887.861\n",
      "Gradient Descent(46/49): loss=9657184437713.883\n",
      "Gradient Descent(47/49): loss=19747976456679.434\n",
      "Gradient Descent(48/49): loss=40382637056259.35\n",
      "Gradient Descent(49/49): loss=82578454516351.12\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5520641604108799\n",
      "Gradient Descent(2/49): loss=0.6585301620350861\n",
      "Gradient Descent(3/49): loss=0.8762424887563943\n",
      "Gradient Descent(4/49): loss=1.3214424256688189\n",
      "Gradient Descent(5/49): loss=2.231831776660941\n",
      "Gradient Descent(6/49): loss=4.093486960504394\n",
      "Gradient Descent(7/49): loss=7.900385645947018\n",
      "Gradient Descent(8/49): loss=15.685112767806592\n",
      "Gradient Descent(9/49): loss=31.604101259301018\n",
      "Gradient Descent(10/49): loss=64.15684082556042\n",
      "Gradient Descent(11/49): loss=130.72393796460096\n",
      "Gradient Descent(12/49): loss=266.84699490422236\n",
      "Gradient Descent(13/49): loss=545.2050340401192\n",
      "Gradient Descent(14/49): loss=1114.419388268995\n",
      "Gradient Descent(15/49): loss=2278.405821231662\n",
      "Gradient Descent(16/49): loss=4658.641677997041\n",
      "Gradient Descent(17/49): loss=9525.985981495918\n",
      "Gradient Descent(18/49): loss=19479.21834771912\n",
      "Gradient Descent(19/49): loss=39832.58321341342\n",
      "Gradient Descent(20/49): loss=81453.17902727613\n",
      "Gradient Descent(21/49): loss=166563.1354070229\n",
      "Gradient Descent(22/49): loss=340604.48520798684\n",
      "Gradient Descent(23/49): loss=696501.6414159733\n",
      "Gradient Descent(24/49): loss=1424275.7361458081\n",
      "Gradient Descent(25/49): loss=2912500.982458758\n",
      "Gradient Descent(26/49): loss=5955772.788643564\n",
      "Gradient Descent(27/49): loss=12178959.30511074\n",
      "Gradient Descent(28/49): loss=24904753.41263808\n",
      "Gradient Descent(29/49): loss=50927729.78311588\n",
      "Gradient Descent(30/49): loss=104142114.16310056\n",
      "Gradient Descent(31/49): loss=212960208.7817153\n",
      "Gradient Descent(32/49): loss=435482330.46739185\n",
      "Gradient Descent(33/49): loss=890517817.102464\n",
      "Gradient Descent(34/49): loss=1821019883.7224607\n",
      "Gradient Descent(35/49): loss=3723803559.7537527\n",
      "Gradient Descent(36/49): loss=7614805898.8696575\n",
      "Gradient Descent(37/49): loss=15571516582.127563\n",
      "Gradient Descent(38/49): loss=31842194258.32041\n",
      "Gradient Descent(39/49): loss=65114103038.36689\n",
      "Gradient Descent(40/49): loss=133151829302.68004\n",
      "Gradient Descent(41/49): loss=272282175740.57507\n",
      "Gradient Descent(42/49): loss=556789821171.4158\n",
      "Gradient Descent(43/49): loss=1138579505312.8506\n",
      "Gradient Descent(44/49): loss=2328281230413.6133\n",
      "Gradient Descent(45/49): loss=4761102288072.629\n",
      "Gradient Descent(46/49): loss=9735978068879.836\n",
      "Gradient Descent(47/49): loss=19909101553049.484\n",
      "Gradient Descent(48/49): loss=40712121765830.57\n",
      "Gradient Descent(49/49): loss=83252217798948.33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5551978119940779\n",
      "Gradient Descent(2/49): loss=0.672857468040659\n",
      "Gradient Descent(3/49): loss=0.9236607908695219\n",
      "Gradient Descent(4/49): loss=1.4582731538115452\n",
      "Gradient Descent(5/49): loss=2.59785286665852\n",
      "Gradient Descent(6/49): loss=5.026980982562995\n",
      "Gradient Descent(7/49): loss=10.204910474424837\n",
      "Gradient Descent(8/49): loss=21.242184979277667\n",
      "Gradient Descent(9/49): loss=44.76923931382219\n",
      "Gradient Descent(10/49): loss=94.9195083333429\n",
      "Gradient Descent(11/49): loss=201.81982177535463\n",
      "Gradient Descent(12/49): loss=429.68852990830374\n",
      "Gradient Descent(13/49): loss=915.4134681645447\n",
      "Gradient Descent(14/49): loss=1950.7847465513853\n",
      "Gradient Descent(15/49): loss=4157.782163561084\n",
      "Gradient Descent(16/49): loss=8862.217857658101\n",
      "Gradient Descent(17/49): loss=18890.19298319486\n",
      "Gradient Descent(18/49): loss=40265.82476079003\n",
      "Gradient Descent(19/49): loss=85830.12145790092\n",
      "Gradient Descent(20/49): loss=182954.9762974926\n",
      "Gradient Descent(21/49): loss=389986.31687354954\n",
      "Gradient Descent(22/49): loss=831294.3224453988\n",
      "Gradient Descent(23/49): loss=1771986.4671223506\n",
      "Gradient Descent(24/49): loss=3777165.8427158217\n",
      "Gradient Descent(25/49): loss=8051406.199731293\n",
      "Gradient Descent(26/49): loss=17162376.94474545\n",
      "Gradient Descent(27/49): loss=36583322.18482142\n",
      "Gradient Descent(28/49): loss=77981009.0585682\n",
      "Gradient Descent(29/49): loss=166224318.39866382\n",
      "Gradient Descent(30/49): loss=354323756.5879964\n",
      "Gradient Descent(31/49): loss=755276519.0323876\n",
      "Gradient Descent(32/49): loss=1609947427.4587216\n",
      "Gradient Descent(33/49): loss=3431763935.8601294\n",
      "Gradient Descent(34/49): loss=7315148005.168276\n",
      "Gradient Descent(35/49): loss=15592969487.306349\n",
      "Gradient Descent(36/49): loss=33237973758.633926\n",
      "Gradient Descent(37/49): loss=70850064863.39287\n",
      "Gradient Descent(38/49): loss=151023998262.28568\n",
      "Gradient Descent(39/49): loss=321922754695.4154\n",
      "Gradient Descent(40/49): loss=686210543908.1877\n",
      "Gradient Descent(41/49): loss=1462726395394.3079\n",
      "Gradient Descent(42/49): loss=3117947584422.048\n",
      "Gradient Descent(43/49): loss=6646217070953.866\n",
      "Gradient Descent(44/49): loss=14167076308444.592\n",
      "Gradient Descent(45/49): loss=30198539859078.875\n",
      "Gradient Descent(46/49): loss=64371207563615.555\n",
      "Gradient Descent(47/49): loss=137213666042612.33\n",
      "Gradient Descent(48/49): loss=292484650536405.94\n",
      "Gradient Descent(49/49): loss=623460281083373.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5565824693785258\n",
      "Gradient Descent(2/49): loss=0.6771936611057738\n",
      "Gradient Descent(3/49): loss=0.9342884773916439\n",
      "Gradient Descent(4/49): loss=1.4823117877864902\n",
      "Gradient Descent(5/49): loss=2.650478276224125\n",
      "Gradient Descent(6/49): loss=5.14054196297805\n",
      "Gradient Descent(7/49): loss=10.448361717662724\n",
      "Gradient Descent(8/49): loss=21.76251030675026\n",
      "Gradient Descent(9/49): loss=45.87974943924934\n",
      "Gradient Descent(10/49): loss=97.28805637409062\n",
      "Gradient Descent(11/49): loss=206.8700034363751\n",
      "Gradient Descent(12/49): loss=440.45488179429657\n",
      "Gradient Descent(13/49): loss=938.3644085021791\n",
      "Gradient Descent(14/49): loss=1999.7083556325813\n",
      "Gradient Descent(15/49): loss=4262.069113335782\n",
      "Gradient Descent(16/49): loss=9084.51730445607\n",
      "Gradient Descent(17/49): loss=19364.047868649148\n",
      "Gradient Descent(18/49): loss=41275.89521927809\n",
      "Gradient Descent(19/49): loss=87983.18903188656\n",
      "Gradient Descent(20/49): loss=187544.4565228611\n",
      "Gradient Descent(21/49): loss=399769.254306574\n",
      "Gradient Descent(22/49): loss=852147.633262295\n",
      "Gradient Descent(23/49): loss=1816437.3858442758\n",
      "Gradient Descent(24/49): loss=3871917.422447877\n",
      "Gradient Descent(25/49): loss=8253378.668472146\n",
      "Gradient Descent(26/49): loss=17592901.46049908\n",
      "Gradient Descent(27/49): loss=37501028.24398247\n",
      "Gradient Descent(28/49): loss=79937191.29566091\n",
      "Gradient Descent(29/49): loss=170394116.4566291\n",
      "Gradient Descent(30/49): loss=363212098.1297204\n",
      "Gradient Descent(31/49): loss=774222907.864126\n",
      "Gradient Descent(32/49): loss=1650333549.893946\n",
      "Gradient Descent(33/49): loss=3517850994.444719\n",
      "Gradient Descent(34/49): loss=7498651179.249669\n",
      "Gradient Descent(35/49): loss=15984124853.178583\n",
      "Gradient Descent(36/49): loss=34071760536.52406\n",
      "Gradient Descent(37/49): loss=72627364759.1396\n",
      "Gradient Descent(38/49): loss=154812490720.08337\n",
      "Gradient Descent(39/49): loss=329998305218.4254\n",
      "Gradient Descent(40/49): loss=703424387403.0254\n",
      "Gradient Descent(41/49): loss=1499419424187.7515\n",
      "Gradient Descent(42/49): loss=3196162444597.7954\n",
      "Gradient Descent(43/49): loss=6812939866904.325\n",
      "Gradient Descent(44/49): loss=14522462620291.93\n",
      "Gradient Descent(45/49): loss=30956081321415.215\n",
      "Gradient Descent(46/49): loss=65985982944733.914\n",
      "Gradient Descent(47/49): loss=140655721244991.42\n",
      "Gradient Descent(48/49): loss=299821735405799.4\n",
      "Gradient Descent(49/49): loss=639100011190963.9\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5559278362879998\n",
      "Gradient Descent(2/49): loss=0.6751436121194815\n",
      "Gradient Descent(3/49): loss=0.9292639598819475\n",
      "Gradient Descent(4/49): loss=1.470946893172393\n",
      "Gradient Descent(5/49): loss=2.6255982337744483\n",
      "Gradient Descent(6/49): loss=5.086853031401236\n",
      "Gradient Descent(7/49): loss=10.333263758024206\n",
      "Gradient Descent(8/49): loss=21.51651286289375\n",
      "Gradient Descent(9/49): loss=45.35472665483408\n",
      "Gradient Descent(10/49): loss=96.16826317374128\n",
      "Gradient Descent(11/49): loss=204.48239761742866\n",
      "Gradient Descent(12/49): loss=435.36480659757996\n",
      "Gradient Descent(13/49): loss=927.5137495796442\n",
      "Gradient Descent(14/49): loss=1976.5784364403178\n",
      "Gradient Descent(15/49): loss=4212.764722952497\n",
      "Gradient Descent(16/49): loss=8979.419411282079\n",
      "Gradient Descent(17/49): loss=19140.020544923842\n",
      "Gradient Descent(18/49): loss=40798.35792139414\n",
      "Gradient Descent(19/49): loss=86965.26987307445\n",
      "Gradient Descent(20/49): loss=185374.65938927827\n",
      "Gradient Descent(21/49): loss=395144.11408202694\n",
      "Gradient Descent(22/49): loss=842288.6837051108\n",
      "Gradient Descent(23/49): loss=1795422.0483134668\n",
      "Gradient Descent(24/49): loss=3827121.128313163\n",
      "Gradient Descent(25/49): loss=8157890.887240723\n",
      "Gradient Descent(26/49): loss=17389359.70537105\n",
      "Gradient Descent(27/49): loss=37067158.63809479\n",
      "Gradient Descent(28/49): loss=79012354.84308745\n",
      "Gradient Descent(29/49): loss=168422735.07366395\n",
      "Gradient Descent(30/49): loss=359009901.57317567\n",
      "Gradient Descent(31/49): loss=765265505.6835002\n",
      "Gradient Descent(32/49): loss=1631239951.4051526\n",
      "Gradient Descent(33/49): loss=3477151079.905351\n",
      "Gradient Descent(34/49): loss=7411895241.415368\n",
      "Gradient Descent(35/49): loss=15799195896.089813\n",
      "Gradient Descent(36/49): loss=33677565971.59827\n",
      "Gradient Descent(37/49): loss=71787099624.55548\n",
      "Gradient Descent(38/49): loss=153021381559.17807\n",
      "Gradient Descent(39/49): loss=326180376931.0619\n",
      "Gradient Descent(40/49): loss=695286091465.7202\n",
      "Gradient Descent(41/49): loss=1482071832567.9011\n",
      "Gradient Descent(42/49): loss=3159184318301.501\n",
      "Gradient Descent(43/49): loss=6734117292891.41\n",
      "Gradient Descent(44/49): loss=14354444421528.695\n",
      "Gradient Descent(45/49): loss=30597933728933.21\n",
      "Gradient Descent(46/49): loss=65222555536597.27\n",
      "Gradient Descent(47/49): loss=139028399381822.55\n",
      "Gradient Descent(48/49): loss=296352936122271.25\n",
      "Gradient Descent(49/49): loss=631705918638301.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.55638415534592\n",
      "Gradient Descent(2/49): loss=0.6765726208813001\n",
      "Gradient Descent(3/49): loss=0.9327663540165317\n",
      "Gradient Descent(4/49): loss=1.478868915567471\n",
      "Gradient Descent(5/49): loss=2.642941135769298\n",
      "Gradient Descent(6/49): loss=5.124277480351586\n",
      "Gradient Descent(7/49): loss=10.413494032463694\n",
      "Gradient Descent(8/49): loss=21.687988034943935\n",
      "Gradient Descent(9/49): loss=45.72069945063639\n",
      "Gradient Descent(10/49): loss=96.94882710433023\n",
      "Gradient Descent(11/49): loss=206.14670401093395\n",
      "Gradient Descent(12/49): loss=438.91289842509906\n",
      "Gradient Descent(13/49): loss=935.0773184382971\n",
      "Gradient Descent(14/49): loss=1992.7013961383768\n",
      "Gradient Descent(15/49): loss=4247.132880164043\n",
      "Gradient Descent(16/49): loss=9052.6790315137\n",
      "Gradient Descent(17/49): loss=19296.181207729413\n",
      "Gradient Descent(18/49): loss=41131.230446556096\n",
      "Gradient Descent(19/49): loss=87674.82140404149\n",
      "Gradient Descent(20/49): loss=186887.13988901034\n",
      "Gradient Descent(21/49): loss=398368.1179715557\n",
      "Gradient Descent(22/49): loss=849160.9708522643\n",
      "Gradient Descent(23/49): loss=1810071.016052772\n",
      "Gradient Descent(24/49): loss=3858346.8684020718\n",
      "Gradient Descent(25/49): loss=8224451.675269259\n",
      "Gradient Descent(26/49): loss=17531240.681588512\n",
      "Gradient Descent(27/49): loss=37369592.12745417\n",
      "Gradient Descent(28/49): loss=79657022.06947355\n",
      "Gradient Descent(29/49): loss=169796907.73386955\n",
      "Gradient Descent(30/49): loss=361939088.0161193\n",
      "Gradient Descent(31/49): loss=771509359.5056983\n",
      "Gradient Descent(32/49): loss=1644549350.213133\n",
      "Gradient Descent(33/49): loss=3505521394.404629\n",
      "Gradient Descent(34/49): loss=7472369403.804465\n",
      "Gradient Descent(35/49): loss=15928102620.63983\n",
      "Gradient Descent(36/49): loss=33952343545.643497\n",
      "Gradient Descent(37/49): loss=72372815501.38469\n",
      "Gradient Descent(38/49): loss=154269893522.2475\n",
      "Gradient Descent(39/49): loss=328841705031.5314\n",
      "Gradient Descent(40/49): loss=700958978444.6755\n",
      "Gradient Descent(41/49): loss=1494164158452.2688\n",
      "Gradient Descent(42/49): loss=3184960320156.4146\n",
      "Gradient Descent(43/49): loss=6789061418444.21\n",
      "Gradient Descent(44/49): loss=14471563319555.428\n",
      "Gradient Descent(45/49): loss=30847584371961.44\n",
      "Gradient Descent(46/49): loss=65754710847273.89\n",
      "Gradient Descent(47/49): loss=140162741642049.05\n",
      "Gradient Descent(48/49): loss=298770900084208.5\n",
      "Gradient Descent(49/49): loss=636860050619476.8\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5595147140455766\n",
      "Gradient Descent(2/49): loss=0.6916433306981551\n",
      "Gradient Descent(3/49): loss=0.9849820725285108\n",
      "Gradient Descent(4/49): loss=1.6362234132659856\n",
      "Gradient Descent(5/49): loss=3.0820443138374856\n",
      "Gradient Descent(6/49): loss=6.291911295196083\n",
      "Gradient Descent(7/49): loss=13.41813698050982\n",
      "Gradient Descent(8/49): loss=29.23907062447787\n",
      "Gradient Descent(9/49): loss=64.36312540744811\n",
      "Gradient Descent(10/49): loss=142.34203943113002\n",
      "Gradient Descent(11/49): loss=315.4630264550764\n",
      "Gradient Descent(12/49): loss=699.8089297468791\n",
      "Gradient Descent(13/49): loss=1553.0952696450609\n",
      "Gradient Descent(14/49): loss=3447.4762728531578\n",
      "Gradient Descent(15/49): loss=7653.191538074772\n",
      "Gradient Descent(16/49): loss=16990.299998394432\n",
      "Gradient Descent(17/49): loss=37719.61449115207\n",
      "Gradient Descent(18/49): loss=83740.76559652429\n",
      "Gradient Descent(19/49): loss=185912.32316555636\n",
      "Gradient Descent(20/49): loss=412743.39812458033\n",
      "Gradient Descent(21/49): loss=916331.0676411106\n",
      "Gradient Descent(22/49): loss=2034346.0527345159\n",
      "Gradient Descent(23/49): loss=4516451.121140598\n",
      "Gradient Descent(24/49): loss=10026972.583507773\n",
      "Gradient Descent(25/49): loss=22260881.282108173\n",
      "Gradient Descent(26/49): loss=49421381.983874105\n",
      "Gradient Descent(27/49): loss=109720409.59185857\n",
      "Gradient Descent(28/49): loss=243590280.78434634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=540794781.8187375\n",
      "Gradient Descent(30/49): loss=1200618494.5651498\n",
      "Gradient Descent(31/49): loss=2665493119.2334743\n",
      "Gradient Descent(32/49): loss=5917661273.459622\n",
      "Gradient Descent(33/49): loss=13137799792.657347\n",
      "Gradient Descent(34/49): loss=29167229319.129845\n",
      "Gradient Descent(35/49): loss=64754165810.84527\n",
      "Gradient Descent(36/49): loss=143760723516.09476\n",
      "Gradient Descent(37/49): loss=319163182277.5518\n",
      "Gradient Descent(38/49): loss=708574180973.8236\n",
      "Gradient Descent(39/49): loss=1573105539179.3604\n",
      "Gradient Descent(40/49): loss=3492451607531.77\n",
      "Gradient Descent(41/49): loss=7753591813881.173\n",
      "Gradient Descent(42/49): loss=17213749185995.13\n",
      "Gradient Descent(43/49): loss=38216244567829.51\n",
      "Gradient Descent(44/49): loss=84843884565042.86\n",
      "Gradient Descent(45/49): loss=188361908122845.28\n",
      "Gradient Descent(46/49): loss=418182272223558.0\n",
      "Gradient Descent(47/49): loss=928406462563440.1\n",
      "Gradient Descent(48/49): loss=2061155187536832.5\n",
      "Gradient Descent(49/49): loss=4575970631850827.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5610076625033046\n",
      "Gradient Descent(2/49): loss=0.6964507740269106\n",
      "Gradient Descent(3/49): loss=0.9971480259204503\n",
      "Gradient Descent(4/49): loss=1.6647259948492221\n",
      "Gradient Descent(5/49): loss=3.1468158436683753\n",
      "Gradient Descent(6/49): loss=6.437203517031815\n",
      "Gradient Descent(7/49): loss=13.742193190666795\n",
      "Gradient Descent(8/49): loss=29.960000765099643\n",
      "Gradient Descent(9/49): loss=65.9651553611069\n",
      "Gradient Descent(10/49): loss=145.90019907969736\n",
      "Gradient Descent(11/49): loss=323.363989639355\n",
      "Gradient Descent(12/49): loss=717.3513510608392\n",
      "Gradient Descent(13/49): loss=1592.042692152586\n",
      "Gradient Descent(14/49): loss=3533.9449385107064\n",
      "Gradient Descent(15/49): loss=7845.162115651026\n",
      "Gradient Descent(16/49): loss=17416.495370619636\n",
      "Gradient Descent(17/49): loss=38665.81232997342\n",
      "Gradient Descent(18/49): loss=85841.42091143467\n",
      "Gradient Descent(19/49): loss=190575.98952312564\n",
      "Gradient Descent(20/49): loss=423097.20529799926\n",
      "Gradient Descent(21/49): loss=939317.5564398628\n",
      "Gradient Descent(22/49): loss=2085378.3580095684\n",
      "Gradient Descent(23/49): loss=4629747.9435752705\n",
      "Gradient Descent(24/49): loss=10278502.860488826\n",
      "Gradient Descent(25/49): loss=22819303.651529808\n",
      "Gradient Descent(26/49): loss=50661135.487720385\n",
      "Gradient Descent(27/49): loss=112472786.34725456\n",
      "Gradient Descent(28/49): loss=249700832.42047855\n",
      "Gradient Descent(29/49): loss=554360817.5077295\n",
      "Gradient Descent(30/49): loss=1230736450.3998141\n",
      "Gradient Descent(31/49): loss=2732357992.983716\n",
      "Gradient Descent(32/49): loss=6066107979.673997\n",
      "Gradient Descent(33/49): loss=13467366325.125393\n",
      "Gradient Descent(34/49): loss=29898899977.860325\n",
      "Gradient Descent(35/49): loss=66378547840.29091\n",
      "Gradient Descent(36/49): loss=147367014059.6882\n",
      "Gradient Descent(37/49): loss=327169507913.36914\n",
      "Gradient Descent(38/49): loss=726349024517.9222\n",
      "Gradient Descent(39/49): loss=1612567469331.7783\n",
      "Gradient Descent(40/49): loss=3580061038663.242\n",
      "Gradient Descent(41/49): loss=7948093511935.445\n",
      "Gradient Descent(42/49): loss=17645562405847.188\n",
      "Gradient Descent(43/49): loss=39174913097220.836\n",
      "Gradient Descent(44/49): loss=86972224567138.06\n",
      "Gradient Descent(45/49): loss=193087035761487.78\n",
      "Gradient Descent(46/49): loss=428672528094088.4\n",
      "Gradient Descent(47/49): loss=951695879621682.1\n",
      "Gradient Descent(48/49): loss=2112860022347921.0\n",
      "Gradient Descent(49/49): loss=4690760535614176.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5603018319679999\n",
      "Gradient Descent(2/49): loss=0.6941779291201634\n",
      "Gradient Descent(3/49): loss=0.9913962524076763\n",
      "Gradient Descent(4/49): loss=1.651250651938394\n",
      "Gradient Descent(5/49): loss=3.1161934043361934\n",
      "Gradient Descent(6/49): loss=6.368512808934123\n",
      "Gradient Descent(7/49): loss=13.58898711908341\n",
      "Gradient Descent(8/49): loss=29.619162135046878\n",
      "Gradient Descent(9/49): loss=65.20775368798084\n",
      "Gradient Descent(10/49): loss=144.21798579465806\n",
      "Gradient Descent(11/49): loss=319.6286020947105\n",
      "Gradient Descent(12/49): loss=709.0577113424525\n",
      "Gradient Descent(13/49): loss=1573.6292767831744\n",
      "Gradient Descent(14/49): loss=3493.0646092181096\n",
      "Gradient Descent(15/49): loss=7754.402990756218\n",
      "Gradient Descent(16/49): loss=17215.00033160839\n",
      "Gradient Descent(17/49): loss=38218.47248803715\n",
      "Gradient Descent(18/49): loss=84848.28102251905\n",
      "Gradient Descent(19/49): loss=188371.11894991007\n",
      "Gradient Descent(20/49): loss=418202.1714324965\n",
      "Gradient Descent(21/49): loss=928450.091049059\n",
      "Gradient Descent(22/49): loss=2061251.4973897766\n",
      "Gradient Descent(23/49): loss=4576183.899606966\n",
      "Gradient Descent(24/49): loss=10159585.325769885\n",
      "Gradient Descent(25/49): loss=22555294.831990894\n",
      "Gradient Descent(26/49): loss=50075009.506748445\n",
      "Gradient Descent(27/49): loss=111171528.0561983\n",
      "Gradient Descent(28/49): loss=246811908.88782272\n",
      "Gradient Descent(29/49): loss=547947118.3720917\n",
      "Gradient Descent(30/49): loss=1216497396.9480553\n",
      "Gradient Descent(31/49): loss=2700745870.414595\n",
      "Gradient Descent(32/49): loss=5995925906.357775\n",
      "Gradient Descent(33/49): loss=13311555104.155554\n",
      "Gradient Descent(34/49): loss=29552983486.18841\n",
      "Gradient Descent(35/49): loss=65610578637.14346\n",
      "Gradient Descent(36/49): loss=145662045631.78668\n",
      "Gradient Descent(37/49): loss=323384307506.54645\n",
      "Gradient Descent(38/49): loss=717945501094.7876\n",
      "Gradient Descent(39/49): loss=1593910806980.1172\n",
      "Gradient Descent(40/49): loss=3538641382575.742\n",
      "Gradient Descent(41/49): loss=7856137733454.94\n",
      "Gradient Descent(42/49): loss=17441411382042.959\n",
      "Gradient Descent(43/49): loss=38721677409276.35\n",
      "Gradient Descent(44/49): loss=85965996016339.69\n",
      "Gradient Descent(45/49): loss=190853107755887.4\n",
      "Gradient Descent(46/49): loss=423712984528828.25\n",
      "Gradient Descent(47/49): loss=940685196952465.1\n",
      "Gradient Descent(48/49): loss=2088415205754175.2\n",
      "Gradient Descent(49/49): loss=4636490598294663.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5607938387571201\n",
      "Gradient Descent(2/49): loss=0.6957622401817934\n",
      "Gradient Descent(3/49): loss=0.9954055881846832\n",
      "Gradient Descent(4/49): loss=1.660643785085864\n",
      "Gradient Descent(5/49): loss=3.137539106026386\n",
      "Gradient Descent(6/49): loss=6.416394408046824\n",
      "Gradient Descent(7/49): loss=13.695781064061805\n",
      "Gradient Descent(8/49): loss=29.85674737907898\n",
      "Gradient Descent(9/49): loss=65.73570869504589\n",
      "Gradient Descent(10/49): loss=145.39059071264097\n",
      "Gradient Descent(11/49): loss=322.23239427990706\n",
      "Gradient Descent(12/49): loss=714.8388823795528\n",
      "Gradient Descent(13/49): loss=1586.4645466094512\n",
      "Gradient Descent(14/49): loss=3521.560683766529\n",
      "Gradient Descent(15/49): loss=7817.667617868406\n",
      "Gradient Descent(16/49): loss=17355.454622268866\n",
      "Gradient Descent(17/49): loss=38530.295550738854\n",
      "Gradient Descent(18/49): loss=85540.559896038\n",
      "Gradient Descent(19/49): loss=189908.0477690238\n",
      "Gradient Descent(20/49): loss=421614.3075958024\n",
      "Gradient Descent(21/49): loss=936025.3750372459\n",
      "Gradient Descent(22/49): loss=2078069.385864151\n",
      "Gradient Descent(23/49): loss=4613521.294300753\n",
      "Gradient Descent(24/49): loss=10242478.07622116\n",
      "Gradient Descent(25/49): loss=22739325.027764175\n",
      "Gradient Descent(26/49): loss=50483574.94488532\n",
      "Gradient Descent(27/49): loss=112078584.18587804\n",
      "Gradient Descent(28/49): loss=248825664.20180184\n",
      "Gradient Descent(29/49): loss=552417856.5451897\n",
      "Gradient Descent(30/49): loss=1226422882.766783\n",
      "Gradient Descent(31/49): loss=2722781441.481279\n",
      "Gradient Descent(32/49): loss=6044847077.683919\n",
      "Gradient Descent(33/49): loss=13420164996.616152\n",
      "Gradient Descent(34/49): loss=29794108308.43616\n",
      "Gradient Descent(35/49): loss=66145899855.00554\n",
      "Gradient Descent(36/49): loss=146850512267.5486\n",
      "Gradient Descent(37/49): loss=326022822284.6459\n",
      "Gradient Descent(38/49): loss=723803267753.5951\n",
      "Gradient Descent(39/49): loss=1606915634739.3564\n",
      "Gradient Descent(40/49): loss=3567513400684.261\n",
      "Gradient Descent(41/49): loss=7920236500858.9795\n",
      "Gradient Descent(42/49): loss=17583717055556.59\n",
      "Gradient Descent(43/49): loss=39037610235037.72\n",
      "Gradient Descent(44/49): loss=86667398482816.64\n",
      "Gradient Descent(45/49): loss=192410291371714.25\n",
      "Gradient Descent(46/49): loss=427170087874364.0\n",
      "Gradient Descent(47/49): loss=948360312089912.8\n",
      "Gradient Descent(48/49): loss=2105454728870666.5\n",
      "Gradient Descent(49/49): loss=4674320043565477.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5639194174947327\n",
      "Gradient Descent(2/49): loss=0.7115988396745863\n",
      "Gradient Descent(3/49): loss=1.0527973766789398\n",
      "Gradient Descent(4/49): loss=1.8411024765737851\n",
      "Gradient Descent(5/49): loss=3.662402579370605\n",
      "Gradient Descent(6/49): loss=7.870334336872908\n",
      "Gradient Descent(7/49): loss=17.592339869406715\n",
      "Gradient Descent(8/49): loss=40.05406145177029\n",
      "Gradient Descent(9/49): loss=91.94962299567025\n",
      "Gradient Descent(10/49): loss=211.8491283866745\n",
      "Gradient Descent(11/49): loss=488.86494564202604\n",
      "Gradient Descent(12/49): loss=1128.8822898288245\n",
      "Gradient Descent(13/49): loss=2607.578361837751\n",
      "Gradient Descent(14/49): loss=6023.95776660715\n",
      "Gradient Descent(15/49): loss=13917.160743386203\n",
      "Gradient Descent(16/49): loss=32153.616900935256\n",
      "Gradient Descent(17/49): loss=74287.12520733404\n",
      "Gradient Descent(18/49): loss=171632.38279843508\n",
      "Gradient Descent(19/49): loss=396538.86593692383\n",
      "Gradient Descent(20/49): loss=916162.8045800647\n",
      "Gradient Descent(21/49): loss=2116701.9524211674\n",
      "Gradient Descent(22/49): loss=4890427.599593131\n",
      "Gradient Descent(23/49): loss=11298843.334819548\n",
      "Gradient Descent(24/49): loss=26104847.0494848\n",
      "Gradient Descent(25/49): loss=60312638.031849556\n",
      "Gradient Descent(26/49): loss=139346318.31750342\n",
      "Gradient Descent(27/49): loss=321945733.2495074\n",
      "Gradient Descent(28/49): loss=743823421.5084114\n",
      "Gradient Descent(29/49): loss=1718529632.4617963\n",
      "Gradient Descent(30/49): loss=3970490862.248646\n",
      "Gradient Descent(31/49): loss=9173422087.548645\n",
      "Gradient Descent(32/49): loss=21194274390.479935\n",
      "Gradient Descent(33/49): loss=48967251551.1741\n",
      "Gradient Descent(34/49): loss=113133937983.24983\n",
      "Gradient Descent(35/49): loss=261384650315.93204\n",
      "Gradient Descent(36/49): loss=603903096089.3016\n",
      "Gradient Descent(37/49): loss=1395257713204.1226\n",
      "Gradient Descent(38/49): loss=3223603420585.9937\n",
      "Gradient Descent(39/49): loss=7447813342920.44\n",
      "Gradient Descent(40/49): loss=17207427947483.809\n",
      "Gradient Descent(41/49): loss=39756041529868.914\n",
      "Gradient Descent(42/49): loss=91852358350604.16\n",
      "Gradient Descent(43/49): loss=212215688733243.34\n",
      "Gradient Descent(44/49): loss=490303127249232.06\n",
      "Gradient Descent(45/49): loss=1132796345196571.5\n",
      "Gradient Descent(46/49): loss=2617212675942064.0\n",
      "Gradient Descent(47/49): loss=6046808166497139.0\n",
      "Gradient Descent(48/49): loss=1.3970545587874488e+16\n",
      "Gradient Descent(49/49): loss=3.2277548526221204e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5655228595560448\n",
      "Gradient Descent(2/49): loss=0.7169068742743279\n",
      "Gradient Descent(3/49): loss=1.066664501879408\n",
      "Gradient Descent(4/49): loss=1.874744524698114\n",
      "Gradient Descent(5/49): loss=3.741732609418619\n",
      "Gradient Descent(6/49): loss=8.055221880357143\n",
      "Gradient Descent(7/49): loss=18.021107491932295\n",
      "Gradient Descent(8/49): loss=41.04628960891791\n",
      "Gradient Descent(9/49): loss=94.2436703719969\n",
      "Gradient Descent(10/49): loss=217.15089888700862\n",
      "Gradient Descent(11/49): loss=501.1157596481461\n",
      "Gradient Descent(12/49): loss=1157.1881739506823\n",
      "Gradient Descent(13/49): loss=2672.9778799550827\n",
      "Gradient Descent(14/49): loss=6175.058416708366\n",
      "Gradient Descent(15/49): loss=14266.265288823215\n",
      "Gradient Descent(16/49): loss=32960.18964615388\n",
      "Gradient Descent(17/49): loss=76150.63248132868\n",
      "Gradient Descent(18/49): loss=175937.8316077371\n",
      "Gradient Descent(19/49): loss=406486.17646936554\n",
      "Gradient Descent(20/49): loss=939145.0724376165\n",
      "Gradient Descent(21/49): loss=2169800.1856829645\n",
      "Gradient Descent(22/49): loss=5013105.759325129\n",
      "Gradient Descent(23/49): loss=11582278.956668202\n",
      "Gradient Descent(24/49): loss=26759696.71180736\n",
      "Gradient Descent(25/49): loss=61825602.69328122\n",
      "Gradient Descent(26/49): loss=142841871.87286296\n",
      "Gradient Descent(27/49): loss=330021860.1853751\n",
      "Gradient Descent(28/49): loss=762482505.1827052\n",
      "Gradient Descent(29/49): loss=1761639579.3842716\n",
      "Gradient Descent(30/49): loss=4070092083.619559\n",
      "Gradient Descent(31/49): loss=9403540749.404495\n",
      "Gradient Descent(32/49): loss=21725940546.83593\n",
      "Gradient Descent(33/49): loss=50195613038.8193\n",
      "Gradient Descent(34/49): loss=115971944364.30182\n",
      "Gradient Descent(35/49): loss=267941580258.67358\n",
      "Gradient Descent(36/49): loss=619052227029.1033\n",
      "Gradient Descent(37/49): loss=1430258265327.359\n",
      "Gradient Descent(38/49): loss=3304468696211.79\n",
      "Gradient Descent(39/49): loss=7634644475726.508\n",
      "Gradient Descent(40/49): loss=17639082596718.24\n",
      "Gradient Descent(41/49): loss=40753336431455.93\n",
      "Gradient Descent(42/49): loss=94156508491241.19\n",
      "Gradient Descent(43/49): loss=217539197218137.53\n",
      "Gradient Descent(44/49): loss=502602561252821.1\n",
      "Gradient Descent(45/49): loss=1161212957518606.8\n",
      "Gradient Descent(46/49): loss=2682866417050963.0\n",
      "Gradient Descent(47/49): loss=6198494569954823.0\n",
      "Gradient Descent(48/49): loss=1.4321001854423464e+16\n",
      "Gradient Descent(49/49): loss=3.3087242684457372e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.564764790272\n",
      "Gradient Descent(2/49): loss=0.7143973617164165\n",
      "Gradient Descent(3/49): loss=1.060108454781608\n",
      "Gradient Descent(4/49): loss=1.8588393641995227\n",
      "Gradient Descent(5/49): loss=3.7042272573188253\n",
      "Gradient Descent(6/49): loss=7.967811445582062\n",
      "Gradient Descent(7/49): loss=17.818396354144728\n",
      "Gradient Descent(8/49): loss=40.577187726889186\n",
      "Gradient Descent(9/49): loss=93.15909931446926\n",
      "Gradient Descent(10/49): loss=214.64434784642427\n",
      "Gradient Descent(11/49): loss=495.3238660546067\n",
      "Gradient Descent(12/49): loss=1143.8058249229464\n",
      "Gradient Descent(13/49): loss=2642.0585426921007\n",
      "Gradient Descent(14/49): loss=6103.6216218256895\n",
      "Gradient Descent(15/49): loss=14101.216959856745\n",
      "Gradient Descent(16/49): loss=32578.861228840215\n",
      "Gradient Descent(17/49): loss=75269.61054789896\n",
      "Gradient Descent(18/49): loss=173902.3177746572\n",
      "Gradient Descent(19/49): loss=401783.32455134473\n",
      "Gradient Descent(20/49): loss=928279.6026081926\n",
      "Gradient Descent(21/49): loss=2144696.603430603\n",
      "Gradient Descent(22/49): loss=4955106.4421311775\n",
      "Gradient Descent(23/49): loss=11448277.333465256\n",
      "Gradient Descent(24/49): loss=26450099.36080227\n",
      "Gradient Descent(25/49): loss=61110308.972757146\n",
      "Gradient Descent(26/49): loss=141189257.26020926\n",
      "Gradient Descent(27/49): loss=326203659.3835512\n",
      "Gradient Descent(28/49): loss=753660934.0493268\n",
      "Gradient Descent(29/49): loss=1741258221.437224\n",
      "Gradient Descent(30/49): loss=4023002994.218104\n",
      "Gradient Descent(31/49): loss=9294746117.2514\n",
      "Gradient Descent(32/49): loss=21474581428.706066\n",
      "Gradient Descent(33/49): loss=49614872932.29573\n",
      "Gradient Descent(34/49): loss=114630202422.18192\n",
      "Gradient Descent(35/49): loss=264841619675.6205\n",
      "Gradient Descent(36/49): loss=611890078097.9373\n",
      "Gradient Descent(37/49): loss=1413710836436.9404\n",
      "Gradient Descent(38/49): loss=3266237516503.386\n",
      "Gradient Descent(39/49): loss=7546315158128.494\n",
      "Gradient Descent(40/49): loss=17435006541341.605\n",
      "Gradient Descent(41/49): loss=40281839113111.97\n",
      "Gradient Descent(42/49): loss=93067161086934.47\n",
      "Gradient Descent(43/49): loss=215022368975280.4\n",
      "Gradient Descent(44/49): loss=496787681280435.9\n",
      "Gradient Descent(45/49): loss=1147778258830189.8\n",
      "Gradient Descent(46/49): loss=2651826889201386.0\n",
      "Gradient Descent(47/49): loss=6126780844810474.0\n",
      "Gradient Descent(48/49): loss=1.415531446385195e+16\n",
      "Gradient Descent(49/49): loss=3.2704438537279292e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5652932106444801\n",
      "Gradient Descent(2/49): loss=0.7161466445175043\n",
      "Gradient Descent(3/49): loss=1.0646784181377642\n",
      "Gradient Descent(4/49): loss=1.869926227910098\n",
      "Gradient Descent(5/49): loss=3.7303707676079036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=8.028741832125432\n",
      "Gradient Descent(7/49): loss=17.959698339587185\n",
      "Gradient Descent(8/49): loss=40.90418025442776\n",
      "Gradient Descent(9/49): loss=93.91511127046245\n",
      "Gradient Descent(10/49): loss=216.39156628991907\n",
      "Gradient Descent(11/49): loss=499.3611679668424\n",
      "Gradient Descent(12/49): loss=1153.1341356812197\n",
      "Gradient Descent(13/49): loss=2663.6112002885156\n",
      "Gradient Descent(14/49): loss=6153.417410357806\n",
      "Gradient Descent(15/49): loss=14216.265678101112\n",
      "Gradient Descent(16/49): loss=32844.670315894924\n",
      "Gradient Descent(17/49): loss=75883.7363910507\n",
      "Gradient Descent(18/49): loss=175321.19465110646\n",
      "Gradient Descent(19/49): loss=405061.49821510655\n",
      "Gradient Descent(20/49): loss=935853.4955693566\n",
      "Gradient Descent(21/49): loss=2162195.326256909\n",
      "Gradient Descent(22/49): loss=4995535.49187738\n",
      "Gradient Descent(23/49): loss=11541684.610526154\n",
      "Gradient Descent(24/49): loss=26665907.534254875\n",
      "Gradient Descent(25/49): loss=61608912.17723665\n",
      "Gradient Descent(26/49): loss=142341230.1043929\n",
      "Gradient Descent(27/49): loss=328865177.44325715\n",
      "Gradient Descent(28/49): loss=759810105.3750696\n",
      "Gradient Descent(29/49): loss=1755465266.8685997\n",
      "Gradient Descent(30/49): loss=4055826951.9830995\n",
      "Gradient Descent(31/49): loss=9370582589.271921\n",
      "Gradient Descent(32/49): loss=21649794013.6648\n",
      "Gradient Descent(33/49): loss=50019684088.57475\n",
      "Gradient Descent(34/49): loss=115565478117.64337\n",
      "Gradient Descent(35/49): loss=267002480642.42612\n",
      "Gradient Descent(36/49): loss=616882531275.6554\n",
      "Gradient Descent(37/49): loss=1425245400258.6997\n",
      "Gradient Descent(38/49): loss=3292886972756.9604\n",
      "Gradient Descent(39/49): loss=7607886061856.173\n",
      "Gradient Descent(40/49): loss=17577259957311.553\n",
      "Gradient Descent(41/49): loss=40610501405368.9\n",
      "Gradient Descent(42/49): loss=93826502446957.06\n",
      "Gradient Descent(43/49): loss=216776751253451.0\n",
      "Gradient Descent(44/49): loss=500841006095940.56\n",
      "Gradient Descent(45/49): loss=1157143060484127.2\n",
      "Gradient Descent(46/49): loss=2673463326942622.5\n",
      "Gradient Descent(47/49): loss=6176769670568591.0\n",
      "Gradient Descent(48/49): loss=1.4270808646880794e+16\n",
      "Gradient Descent(49/49): loss=3.2971276297753816e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5684119223415465\n",
      "Gradient Descent(2/49): loss=0.7327715657670855\n",
      "Gradient Descent(3/49): loss=1.1276456090969755\n",
      "Gradient Descent(4/49): loss=2.076330498196943\n",
      "Gradient Descent(5/49): loss=4.355545944259537\n",
      "Gradient Descent(6/49): loss=9.831361053424802\n",
      "Gradient Descent(7/49): loss=22.98700685319726\n",
      "Gradient Descent(8/49): loss=54.59344588714258\n",
      "Gradient Descent(9/49): loss=130.52791566620624\n",
      "Gradient Descent(10/49): loss=312.96047931042636\n",
      "Gradient Descent(11/49): loss=751.2547134656428\n",
      "Gradient Descent(12/49): loss=1804.256611023708\n",
      "Gradient Descent(13/49): loss=4334.09366990654\n",
      "Gradient Descent(14/49): loss=10412.02720387335\n",
      "Gradient Descent(15/49): loss=25014.262519225613\n",
      "Gradient Descent(16/49): loss=60096.13286435966\n",
      "Gradient Descent(17/49): loss=144380.32636855808\n",
      "Gradient Descent(18/49): loss=346873.10126235965\n",
      "Gradient Descent(19/49): loss=833361.9929448356\n",
      "Gradient Descent(20/49): loss=2002151.5552119464\n",
      "Gradient Descent(21/49): loss=4810168.478558277\n",
      "Gradient Descent(22/49): loss=11556429.136899088\n",
      "Gradient Descent(23/49): loss=27764320.368563447\n",
      "Gradient Descent(24/49): loss=66703779.052632295\n",
      "Gradient Descent(25/49): loss=160255828.54111487\n",
      "Gradient Descent(26/49): loss=385014627.4371647\n",
      "Gradient Descent(27/49): loss=924997641.7850127\n",
      "Gradient Descent(28/49): loss=2222306833.7558255\n",
      "Gradient Descent(29/49): loss=5339092167.465102\n",
      "Gradient Descent(30/49): loss=12827168931.701256\n",
      "Gradient Descent(31/49): loss=30817273357.78203\n",
      "Gradient Descent(32/49): loss=74038499241.43413\n",
      "Gradient Descent(33/49): loss=177877494426.925\n",
      "Gradient Descent(34/49): loss=427350680360.0471\n",
      "Gradient Descent(35/49): loss=1026710009564.4266\n",
      "Gradient Descent(36/49): loss=2466670797977.647\n",
      "Gradient Descent(37/49): loss=5926176592140.859\n",
      "Gradient Descent(38/49): loss=14237639262618.146\n",
      "Gradient Descent(39/49): loss=34205928328441.543\n",
      "Gradient Descent(40/49): loss=82179742809079.78\n",
      "Gradient Descent(41/49): loss=197436832098800.1\n",
      "Gradient Descent(42/49): loss=474341989117317.4\n",
      "Gradient Descent(43/49): loss=1139606628854388.0\n",
      "Gradient Descent(44/49): loss=2737904925822693.0\n",
      "Gradient Descent(45/49): loss=6577816584288537.0\n",
      "Gradient Descent(46/49): loss=1.5803204343751924e+16\n",
      "Gradient Descent(47/49): loss=3.7967198435868696e+16\n",
      "Gradient Descent(48/49): loss=9.121619424218232e+16\n",
      "Gradient Descent(49/49): loss=2.1914690666683344e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5701280605367466\n",
      "Gradient Descent(2/49): loss=0.7386107259762872\n",
      "Gradient Descent(3/49): loss=1.143390329694802\n",
      "Gradient Descent(4/49): loss=2.11587332762845\n",
      "Gradient Descent(5/49): loss=4.45226373016419\n",
      "Gradient Descent(6/49): loss=10.06544167225668\n",
      "Gradient Descent(7/49): loss=23.551101678132007\n",
      "Gradient Descent(8/49): loss=55.9503998422501\n",
      "Gradient Descent(9/49): loss=133.7897136815466\n",
      "Gradient Descent(10/49): loss=320.79866518042655\n",
      "Gradient Descent(11/49): loss=770.0876711564819\n",
      "Gradient Descent(12/49): loss=1849.504508013816\n",
      "Gradient Descent(13/49): loss=4442.8034585639\n",
      "Gradient Descent(14/49): loss=10673.204187259624\n",
      "Gradient Descent(15/49): loss=25641.74193795334\n",
      "Gradient Descent(16/49): loss=61603.653883991225\n",
      "Gradient Descent(17/49): loss=148002.1473343651\n",
      "Gradient Descent(18/49): loss=355574.5278488857\n",
      "Gradient Descent(19/49): loss=854267.1720349323\n",
      "Gradient Descent(20/49): loss=2052376.2496918642\n",
      "Gradient Descent(21/49): loss=4930833.308762512\n",
      "Gradient Descent(22/49): loss=11846326.39317977\n",
      "Gradient Descent(23/49): loss=28460798.52849513\n",
      "Gradient Descent(24/49): loss=68377067.83358829\n",
      "Gradient Descent(25/49): loss=164275904.83908516\n",
      "Gradient Descent(26/49): loss=394672860.7447863\n",
      "Gradient Descent(27/49): loss=948201547.3082905\n",
      "Gradient Descent(28/49): loss=2278054216.776789\n",
      "Gradient Descent(29/49): loss=5473025255.174471\n",
      "Gradient Descent(30/49): loss=13148943174.92666\n",
      "Gradient Descent(31/49): loss=31590335977.127583\n",
      "Gradient Descent(32/49): loss=75895782184.42389\n",
      "Gradient Descent(33/49): loss=182339616697.4336\n",
      "Gradient Descent(34/49): loss=438070929114.97986\n",
      "Gradient Descent(35/49): loss=1052465407198.0459\n",
      "Gradient Descent(36/49): loss=2528548140792.7363\n",
      "Gradient Descent(37/49): loss=6074836908254.412\n",
      "Gradient Descent(38/49): loss=14594795672081.842\n",
      "Gradient Descent(39/49): loss=35063996602177.96\n",
      "Gradient Descent(40/49): loss=84241251836741.08\n",
      "Gradient Descent(41/49): loss=202389607537769.5\n",
      "Gradient Descent(42/49): loss=486241032109463.4\n",
      "Gradient Descent(43/49): loss=1168194079642964.0\n",
      "Gradient Descent(44/49): loss=2806586276342032.0\n",
      "Gradient Descent(45/49): loss=6742823528911616.0\n",
      "Gradient Descent(46/49): loss=1.6199633528210806e+16\n",
      "Gradient Descent(47/49): loss=3.891961955152535e+16\n",
      "Gradient Descent(48/49): loss=9.350438597254326e+16\n",
      "Gradient Descent(49/49): loss=2.2464428729902125e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5693167111999998\n",
      "Gradient Descent(2/49): loss=0.7358501098580129\n",
      "Gradient Descent(3/49): loss=1.135946600133895\n",
      "Gradient Descent(4/49): loss=2.0971784180218007\n",
      "Gradient Descent(5/49): loss=4.4065378604972665\n",
      "Gradient Descent(6/49): loss=9.954773921043834\n",
      "Gradient Descent(7/49): loss=23.28441105650606\n",
      "Gradient Descent(8/49): loss=55.30886427445763\n",
      "Gradient Descent(9/49): loss=132.2476131305713\n",
      "Gradient Descent(10/49): loss=317.0929572573942\n",
      "Gradient Descent(11/49): loss=761.183896522068\n",
      "Gradient Descent(12/49): loss=1828.112378105315\n",
      "Gradient Descent(13/49): loss=4391.408055109258\n",
      "Gradient Descent(14/49): loss=10549.72591911184\n",
      "Gradient Descent(15/49): loss=25345.084587379897\n",
      "Gradient Descent(16/49): loss=60890.933787892856\n",
      "Gradient Descent(17/49): loss=146289.83649214022\n",
      "Gradient Descent(18/49): loss=351460.7002390841\n",
      "Gradient Descent(19/49): loss=844383.7003911489\n",
      "Gradient Descent(20/49): loss=2028631.208256631\n",
      "Gradient Descent(21/49): loss=4873785.845903034\n",
      "Gradient Descent(22/49): loss=11709269.862849599\n",
      "Gradient Descent(23/49): loss=28131520.213559426\n",
      "Gradient Descent(24/49): loss=67585976.68114331\n",
      "Gradient Descent(25/49): loss=162375308.34452805\n",
      "Gradient Descent(26/49): loss=390106677.6657637\n",
      "Gradient Descent(27/49): loss=937231292.4601194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=2251698179.503383\n",
      "Gradient Descent(29/49): loss=5409704875.624562\n",
      "Gradient Descent(30/49): loss=12996815963.055458\n",
      "Gradient Descent(31/49): loss=31224850350.606167\n",
      "Gradient Descent(32/49): loss=75017702966.69296\n",
      "Gradient Descent(33/49): loss=180230031376.8421\n",
      "Gradient Descent(34/49): loss=433002650382.2009\n",
      "Gradient Descent(35/49): loss=1040288867542.499\n",
      "Gradient Descent(36/49): loss=2499294004270.392\n",
      "Gradient Descent(37/49): loss=6004553845259.202\n",
      "Gradient Descent(38/49): loss=14425940613234.852\n",
      "Gradient Descent(39/49): loss=34658322323298.773\n",
      "Gradient Descent(40/49): loss=83266619381725.31\n",
      "Gradient Descent(41/49): loss=200048053064598.84\n",
      "Gradient Descent(42/49): loss=480615447487724.2\n",
      "Gradient Descent(43/49): loss=1154678612589224.8\n",
      "Gradient Descent(44/49): loss=2774115366745646.0\n",
      "Gradient Descent(45/49): loss=6664812168606115.0\n",
      "Gradient Descent(46/49): loss=1.601221123507627e+16\n",
      "Gradient Descent(47/49): loss=3.846933749227508e+16\n",
      "Gradient Descent(48/49): loss=9.24225833251935e+16\n",
      "Gradient Descent(49/49): loss=2.2204525643877504e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5698822710080003\n",
      "Gradient Descent(2/49): loss=0.7377744271047061\n",
      "Gradient Descent(3/49): loss=1.1411353321270974\n",
      "Gradient Descent(4/49): loss=2.11020990644345\n",
      "Gradient Descent(5/49): loss=4.438411571238352\n",
      "Gradient Descent(6/49): loss=10.031916070908913\n",
      "Gradient Descent(7/49): loss=23.470310631365102\n",
      "Gradient Descent(8/49): loss=55.756053562860174\n",
      "Gradient Descent(9/49): loss=133.32255095577327\n",
      "Gradient Descent(10/49): loss=319.67606094227074\n",
      "Gradient Descent(11/49): loss=767.3903686848062\n",
      "Gradient Descent(12/49): loss=1843.0239930364423\n",
      "Gradient Descent(13/49): loss=4427.233775540661\n",
      "Gradient Descent(14/49): loss=10635.797778007734\n",
      "Gradient Descent(15/49): loss=25551.872793934723\n",
      "Gradient Descent(16/49): loss=61387.74301970597\n",
      "Gradient Descent(17/49): loss=147483.42123712937\n",
      "Gradient Descent(18/49): loss=354328.2881545102\n",
      "Gradient Descent(19/49): loss=851273.0809235561\n",
      "Gradient Descent(20/49): loss=2045182.945551231\n",
      "Gradient Descent(21/49): loss=4913551.395318861\n",
      "Gradient Descent(22/49): loss=11804806.595886555\n",
      "Gradient Descent(23/49): loss=28361047.215247583\n",
      "Gradient Descent(24/49): loss=68137415.30326594\n",
      "Gradient Descent(25/49): loss=163700139.63472173\n",
      "Gradient Descent(26/49): loss=393289584.84104115\n",
      "Gradient Descent(27/49): loss=944878226.9493538\n",
      "Gradient Descent(28/49): loss=2270069939.614582\n",
      "Gradient Descent(29/49): loss=5453843029.292088\n",
      "Gradient Descent(30/49): loss=13102857877.243183\n",
      "Gradient Descent(31/49): loss=31479616049.44667\n",
      "Gradient Descent(32/49): loss=75629777558.16164\n",
      "Gradient Descent(33/49): loss=181700540582.8464\n",
      "Gradient Descent(34/49): loss=436535548749.6465\n",
      "Gradient Descent(35/49): loss=1048776655870.4401\n",
      "Gradient Descent(36/49): loss=2519685915727.901\n",
      "Gradient Descent(37/49): loss=6053545412535.775\n",
      "Gradient Descent(38/49): loss=14543642853616.822\n",
      "Gradient Descent(39/49): loss=34941101955816.652\n",
      "Gradient Descent(40/49): loss=83945997448847.1\n",
      "Gradient Descent(41/49): loss=201680258870847.94\n",
      "Gradient Descent(42/49): loss=484536821937229.44\n",
      "Gradient Descent(43/49): loss=1164099714704133.5\n",
      "Gradient Descent(44/49): loss=2796749564576336.5\n",
      "Gradient Descent(45/49): loss=6719190828894871.0\n",
      "Gradient Descent(46/49): loss=1.6142855966419366e+16\n",
      "Gradient Descent(47/49): loss=3.878321145932438e+16\n",
      "Gradient Descent(48/49): loss=9.317666553102554e+16\n",
      "Gradient Descent(49/49): loss=2.238569389382706e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5729922285860183\n",
      "Gradient Descent(2/49): loss=0.7552100280281718\n",
      "Gradient Descent(3/49): loss=1.210098542555547\n",
      "Gradient Descent(4/49): loss=2.345682230221501\n",
      "Gradient Descent(5/49): loss=5.180553348110959\n",
      "Gradient Descent(6/49): loss=12.257525606809141\n",
      "Gradient Descent(7/49): loss=29.924479153421785\n",
      "Gradient Descent(8/49): loss=74.02826198718385\n",
      "Gradient Descent(9/49): loss=184.12894545338543\n",
      "Gradient Descent(10/49): loss=458.98429165840787\n",
      "Gradient Descent(11/49): loss=1145.1331779246261\n",
      "Gradient Descent(12/49): loss=2858.0352575995253\n",
      "Gradient Descent(13/49): loss=7134.124009299656\n",
      "Gradient Descent(14/49): loss=17808.951969044167\n",
      "Gradient Descent(15/49): loss=44457.59248774739\n",
      "Gradient Descent(16/49): loss=110983.25867864715\n",
      "Gradient Descent(17/49): loss=277057.9317575862\n",
      "Gradient Descent(18/49): loss=691646.7456319305\n",
      "Gradient Descent(19/49): loss=1726626.2605876902\n",
      "Gradient Descent(20/49): loss=4310349.121723678\n",
      "Gradient Descent(21/49): loss=10760354.872262094\n",
      "Gradient Descent(22/49): loss=26862149.22790572\n",
      "Gradient Descent(23/49): loss=67058668.65734085\n",
      "Gradient Descent(24/49): loss=167405259.76097253\n",
      "Gradient Descent(25/49): loss=417910489.79204124\n",
      "Gradient Descent(26/49): loss=1043271746.041616\n",
      "Gradient Descent(27/49): loss=2604423586.1429996\n",
      "Gradient Descent(28/49): loss=6501683039.772517\n",
      "Gradient Descent(29/49): loss=16230801539.812817\n",
      "Gradient Descent(30/49): loss=40518572963.31226\n",
      "Gradient Descent(31/49): loss=101150565544.93044\n",
      "Gradient Descent(32/49): loss=252512271825.6863\n",
      "Gradient Descent(33/49): loss=630371635385.017\n",
      "Gradient Descent(34/49): loss=1573659750574.4883\n",
      "Gradient Descent(35/49): loss=3928484201333.388\n",
      "Gradient Descent(36/49): loss=9807067960206.889\n",
      "Gradient Descent(37/49): loss=24482364455858.61\n",
      "Gradient Descent(38/49): loss=61117774627602.56\n",
      "Gradient Descent(39/49): loss=152574412580343.5\n",
      "Gradient Descent(40/49): loss=380886763565573.3\n",
      "Gradient Descent(41/49): loss=950845716565041.4\n",
      "Gradient Descent(42/49): loss=2373691246832901.0\n",
      "Gradient Descent(43/49): loss=5925682828593840.0\n",
      "Gradient Descent(44/49): loss=1.4792874613302772e+16\n",
      "Gradient Descent(45/49): loss=3.692893218464616e+16\n",
      "Gradient Descent(46/49): loss=9.218938630575032e+16\n",
      "Gradient Descent(47/49): loss=2.3014158397367706e+17\n",
      "Gradient Descent(48/49): loss=5.745254502319449e+17\n",
      "Gradient Descent(49/49): loss=1.4342453339590456e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5748232654454101\n",
      "Gradient Descent(2/49): loss=0.7616120653033137\n",
      "Gradient Descent(3/49): loss=1.22791162526863\n",
      "Gradient Descent(4/49): loss=2.3919818467662095\n",
      "Gradient Descent(5/49): loss=5.297966747712689\n",
      "Gradient Descent(6/49): loss=12.552467454435686\n",
      "Gradient Descent(7/49): loss=30.662603018702377\n",
      "Gradient Descent(8/49): loss=75.87274544133832\n",
      "Gradient Descent(9/49): loss=188.73534498520442\n",
      "Gradient Descent(10/49): loss=470.48553848649493\n",
      "Gradient Descent(11/49): loss=1173.846721543265\n",
      "Gradient Descent(12/49): loss=2929.7175789260637\n",
      "Gradient Descent(13/49): loss=7313.07358729631\n",
      "Gradient Descent(14/49): loss=18255.683526591005\n",
      "Gradient Descent(15/49): loss=45572.81497904997\n",
      "Gradient Descent(16/49): loss=113767.30193697447\n",
      "Gradient Descent(17/49): loss=284008.0191787561\n",
      "Gradient Descent(18/49): loss=708996.9457011839\n",
      "Gradient Descent(19/49): loss=1769939.3018717216\n",
      "Gradient Descent(20/49): loss=4418475.799815953\n",
      "Gradient Descent(21/49): loss=11030282.313283876\n",
      "Gradient Descent(22/49): loss=27535996.09350789\n",
      "Gradient Descent(23/49): loss=68740859.97446184\n",
      "Gradient Descent(24/49): loss=171604682.1668554\n",
      "Gradient Descent(25/49): loss=428393927.8879217\n",
      "Gradient Descent(26/49): loss=1069442600.9060934\n",
      "Gradient Descent(27/49): loss=2669756508.2285967\n",
      "Gradient Descent(28/49): loss=6664780146.468246\n",
      "Gradient Descent(29/49): loss=16637957156.969221\n",
      "Gradient Descent(30/49): loss=41534996245.98075\n",
      "Gradient Descent(31/49): loss=103687964627.80228\n",
      "Gradient Descent(32/49): loss=258846634896.1522\n",
      "Gradient Descent(33/49): loss=646184739354.14\n",
      "Gradient Descent(34/49): loss=1613135583322.8398\n",
      "Gradient Descent(35/49): loss=4027031670206.6694\n",
      "Gradient Descent(36/49): loss=10053081861503.336\n",
      "Gradient Descent(37/49): loss=25096513559054.992\n",
      "Gradient Descent(38/49): loss=62650936448820.164\n",
      "Gradient Descent(39/49): loss=156401797750845.53\n",
      "Gradient Descent(40/49): loss=390441447905231.94\n",
      "Gradient Descent(41/49): loss=974698030550673.4\n",
      "Gradient Descent(42/49): loss=2433236163467003.0\n",
      "Gradient Descent(43/49): loss=6074330758479182.0\n",
      "Gradient Descent(44/49): loss=1.5163959305468882e+16\n",
      "Gradient Descent(45/49): loss=3.785530801017432e+16\n",
      "Gradient Descent(46/49): loss=9.450199091659773e+16\n",
      "Gradient Descent(47/49): loss=2.3591477012419363e+17\n",
      "Gradient Descent(48/49): loss=5.88937632138012e+17\n",
      "Gradient Descent(49/49): loss=1.470223904869378e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5739575947519999\n",
      "Gradient Descent(2/49): loss=0.7585853342908823\n",
      "Gradient Descent(3/49): loss=1.2194900232757477\n",
      "Gradient Descent(4/49): loss=2.3700924888575168\n",
      "Gradient Descent(5/49): loss=5.242456483935688\n",
      "Gradient Descent(6/49): loss=12.413025961249508\n",
      "Gradient Descent(7/49): loss=30.313635604414056\n",
      "Gradient Descent(8/49): loss=75.00071751760831\n",
      "Gradient Descent(9/49): loss=186.5575488057158\n",
      "Gradient Descent(10/49): loss=465.04802243330585\n",
      "Gradient Descent(11/49): loss=1160.2716407971766\n",
      "Gradient Descent(12/49): loss=2895.8278816806346\n",
      "Gradient Descent(13/49): loss=7228.470481421735\n",
      "Gradient Descent(14/49): loss=18044.47946741697\n",
      "Gradient Descent(15/49): loss=45045.56430005085\n",
      "Gradient Descent(16/49): loss=112451.07247623429\n",
      "Gradient Descent(17/49): loss=280722.1830872774\n",
      "Gradient Descent(18/49): loss=700794.1836166367\n",
      "Gradient Descent(19/49): loss=1749461.9257382082\n",
      "Gradient Descent(20/49): loss=4367356.077170421\n",
      "Gradient Descent(21/49): loss=10902667.036804747\n",
      "Gradient Descent(22/49): loss=27217417.316437606\n",
      "Gradient Descent(23/49): loss=67945559.91451466\n",
      "Gradient Descent(24/49): loss=169619295.09635317\n",
      "Gradient Descent(25/49): loss=423437607.60433143\n",
      "Gradient Descent(26/49): loss=1057069642.9491236\n",
      "Gradient Descent(27/49): loss=2638868655.984162\n",
      "Gradient Descent(28/49): loss=6587671712.1252575\n",
      "Gradient Descent(29/49): loss=16445463661.475258\n",
      "Gradient Descent(30/49): loss=41054455483.828735\n",
      "Gradient Descent(31/49): loss=102488342669.16162\n",
      "Gradient Descent(32/49): loss=255851898638.6013\n",
      "Gradient Descent(33/49): loss=638708679760.7887\n",
      "Gradient Descent(34/49): loss=1594472348154.1492\n",
      "Gradient Descent(35/49): loss=3980440769931.4565\n",
      "Gradient Descent(36/49): loss=9936772338057.123\n",
      "Gradient Descent(37/49): loss=24806158464724.47\n",
      "Gradient Descent(38/49): loss=61926093991337.05\n",
      "Gradient Descent(39/49): loss=154592301039961.22\n",
      "Gradient Descent(40/49): loss=385924220316140.4\n",
      "Gradient Descent(41/49): loss=963421223597249.9\n",
      "Gradient Descent(42/49): loss=2405084742588181.0\n",
      "Gradient Descent(43/49): loss=6004053551397396.0\n",
      "Gradient Descent(44/49): loss=1.4988519285707954e+16\n",
      "Gradient Descent(45/49): loss=3.741733954483886e+16\n",
      "Gradient Descent(46/49): loss=9.340864643972611e+16\n",
      "Gradient Descent(47/49): loss=2.331853449721629e+17\n",
      "Gradient Descent(48/49): loss=5.821238951885688e+17\n",
      "Gradient Descent(49/49): loss=1.4532140919485832e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.57456101984768\n",
      "Gradient Descent(2/49): loss=0.7606951497954441\n",
      "Gradient Descent(3/49): loss=1.225360391797015\n",
      "Gradient Descent(4/49): loss=2.3853507019296587\n",
      "Gradient Descent(5/49): loss=5.281150512145228\n",
      "Gradient Descent(6/49): loss=12.510225158368568\n",
      "Gradient Descent(7/49): loss=30.556887105197045\n",
      "Gradient Descent(8/49): loss=75.60857398926791\n",
      "Gradient Descent(9/49): loss=188.07560512665114\n",
      "Gradient Descent(10/49): loss=468.8383016579815\n",
      "Gradient Descent(11/49): loss=1169.7342972787847\n",
      "Gradient Descent(12/49): loss=2919.4510607467028\n",
      "Gradient Descent(13/49): loss=7287.443989067368\n",
      "Gradient Descent(14/49): loss=18191.70153532731\n",
      "Gradient Descent(15/49): loss=45413.090073815416\n",
      "Gradient Descent(16/49): loss=113368.56442128446\n",
      "Gradient Descent(17/49): loss=283012.61058231146\n",
      "Gradient Descent(18/49): loss=706512.0074187662\n",
      "Gradient Descent(19/49): loss=1763735.9016813792\n",
      "Gradient Descent(20/49): loss=4402989.631318608\n",
      "Gradient Descent(21/49): loss=10991622.641984047\n",
      "Gradient Descent(22/49): loss=27439486.089807093\n",
      "Gradient Descent(23/49): loss=68499932.40095702\n",
      "Gradient Descent(24/49): loss=171003230.57210073\n",
      "Gradient Descent(25/49): loss=426892464.1265341\n",
      "Gradient Descent(26/49): loss=1065694346.7717397\n",
      "Gradient Descent(27/49): loss=2660399366.607273\n",
      "Gradient Descent(28/49): loss=6641420978.125489\n",
      "Gradient Descent(29/49): loss=16579643329.120262\n",
      "Gradient Descent(30/49): loss=41389421606.13813\n",
      "Gradient Descent(31/49): loss=103324552096.88281\n",
      "Gradient Descent(32/49): loss=257939411853.97232\n",
      "Gradient Descent(33/49): loss=643919947751.5336\n",
      "Gradient Descent(34/49): loss=1607481757566.1814\n",
      "Gradient Descent(35/49): loss=4012917459587.37\n",
      "Gradient Descent(36/49): loss=10017847146114.25\n",
      "Gradient Descent(37/49): loss=25008553615557.945\n",
      "Gradient Descent(38/49): loss=62431353245878.64\n",
      "Gradient Descent(39/49): loss=155853630242992.62\n",
      "Gradient Descent(40/49): loss=389073002538642.0\n",
      "Gradient Descent(41/49): loss=971281843537537.1\n",
      "Gradient Descent(42/49): loss=2424707994207153.0\n",
      "Gradient Descent(43/49): loss=6053041036738946.0\n",
      "Gradient Descent(44/49): loss=1.5110811644114232e+16\n",
      "Gradient Descent(45/49): loss=3.772263018836972e+16\n",
      "Gradient Descent(46/49): loss=9.417077400224114e+16\n",
      "Gradient Descent(47/49): loss=2.350879202191926e+17\n",
      "Gradient Descent(48/49): loss=5.868734840351616e+17\n",
      "Gradient Descent(49/49): loss=1.4650709655452882e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5776603362281473\n",
      "Gradient Descent(2/49): loss=0.7789636937651185\n",
      "Gradient Descent(3/49): loss=1.3007621268366605\n",
      "Gradient Descent(4/49): loss=2.653315845201579\n",
      "Gradient Descent(5/49): loss=6.159270338575205\n",
      "Gradient Descent(6/49): loss=15.247054980849065\n",
      "Gradient Descent(7/49): loss=38.80350155208481\n",
      "Gradient Descent(8/49): loss=99.8641667093938\n",
      "Gradient Descent(9/49): loss=258.13951686366624\n",
      "Gradient Descent(10/49): loss=668.4050519984629\n",
      "Gradient Descent(11/49): loss=1731.85434562131\n",
      "Gradient Descent(12/49): loss=4488.4212596216485\n",
      "Gradient Descent(13/49): loss=11633.718357402438\n",
      "Gradient Descent(14/49): loss=30155.042964561115\n",
      "Gradient Descent(15/49): loss=78164.16847877433\n",
      "Gradient Descent(16/49): loss=202608.6227241643\n",
      "Gradient Descent(17/49): loss=525181.0925736007\n",
      "Gradient Descent(18/49): loss=1361321.1916704942\n",
      "Gradient Descent(19/49): loss=3528679.942539193\n",
      "Gradient Descent(20/49): loss=9146690.560666278\n",
      "Gradient Descent(21/49): loss=23709135.88391539\n",
      "Gradient Descent(22/49): loss=61456450.406303935\n",
      "Gradient Descent(23/49): loss=159301264.3797939\n",
      "Gradient Descent(24/49): loss=412924806.6804562\n",
      "Gradient Descent(25/49): loss=1070342390.6779456\n",
      "Gradient Descent(26/49): loss=2774434510.1579137\n",
      "Gradient Descent(27/49): loss=7191611693.062344\n",
      "Gradient Descent(28/49): loss=18641376668.86794\n",
      "Gradient Descent(29/49): loss=48320312462.65222\n",
      "Gradient Descent(30/49): loss=125251081933.73328\n",
      "Gradient Descent(31/49): loss=324663329479.6933\n",
      "Gradient Descent(32/49): loss=841559816343.5199\n",
      "Gradient Descent(33/49): loss=2181407199943.2803\n",
      "Gradient Descent(34/49): loss=5654425602972.117\n",
      "Gradient Descent(35/49): loss=14656836605464.926\n",
      "Gradient Descent(36/49): loss=37991986165029.78\n",
      "Gradient Descent(37/49): loss=98479027338365.45\n",
      "Gradient Descent(38/49): loss=255267486763798.16\n",
      "Gradient Descent(39/49): loss=661678852440488.6\n",
      "Gradient Descent(40/49): loss=1715137753410909.2\n",
      "Gradient Descent(41/49): loss=4445808570616326.0\n",
      "Gradient Descent(42/49): loss=1.1523980395895012e+16\n",
      "Gradient Descent(43/49): loss=2.987130958419847e+16\n",
      "Gradient Descent(44/49): loss=7.74294215732036e+16\n",
      "Gradient Descent(45/49): loss=2.007048036598897e+17\n",
      "Gradient Descent(46/49): loss=5.202469215668374e+17\n",
      "Gradient Descent(47/49): loss=1.3485320453934943e+18\n",
      "Gradient Descent(48/49): loss=3.495529914864376e+18\n",
      "Gradient Descent(49/49): loss=9.060763092319328e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5796084742820351\n",
      "Gradient Descent(2/49): loss=0.7859616004685333\n",
      "Gradient Descent(3/49): loss=1.3208495388565202\n",
      "Gradient Descent(4/49): loss=2.7073325639520722\n",
      "Gradient Descent(5/49): loss=6.3012352133020855\n",
      "Gradient Descent(6/49): loss=15.616990270682164\n",
      "Gradient Descent(7/49): loss=39.76435895491473\n",
      "Gradient Descent(8/49): loss=102.35675332132006\n",
      "Gradient Descent(9/49): loss=264.6024987584868\n",
      "Gradient Descent(10/49): loss=685.1596955061248\n",
      "Gradient Descent(11/49): loss=1775.2860051955804\n",
      "Gradient Descent(12/49): loss=4601.0024125415175\n",
      "Gradient Descent(13/49): loss=11925.54191202357\n",
      "Gradient Descent(14/49): loss=30911.48074862993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=80124.93280699733\n",
      "Gradient Descent(16/49): loss=207691.12188749044\n",
      "Gradient Descent(17/49): loss=538355.4406030066\n",
      "Gradient Descent(18/49): loss=1395470.4211455989\n",
      "Gradient Descent(19/49): loss=3617198.162209821\n",
      "Gradient Descent(20/49): loss=9376138.639822703\n",
      "Gradient Descent(21/49): loss=24303888.25184132\n",
      "Gradient Descent(22/49): loss=62998108.021158226\n",
      "Gradient Descent(23/49): loss=163297395.0851912\n",
      "Gradient Descent(24/49): loss=423283177.0838557\n",
      "Gradient Descent(25/49): loss=1097192322.602703\n",
      "Gradient Descent(26/49): loss=2844032218.7020245\n",
      "Gradient Descent(27/49): loss=7372015913.380404\n",
      "Gradient Descent(28/49): loss=19109002448.358547\n",
      "Gradient Descent(29/49): loss=49532445245.66998\n",
      "Gradient Descent(30/49): loss=128393051320.59328\n",
      "Gradient Descent(31/49): loss=332807628327.42334\n",
      "Gradient Descent(32/49): loss=862670653386.7644\n",
      "Gradient Descent(33/49): loss=2236128600643.3247\n",
      "Gradient Descent(34/49): loss=5796268945726.343\n",
      "Gradient Descent(35/49): loss=15024508734216.465\n",
      "Gradient Descent(36/49): loss=38945029089958.85\n",
      "Gradient Descent(37/49): loss=100949409904081.86\n",
      "Gradient Descent(38/49): loss=261670965412386.22\n",
      "Gradient Descent(39/49): loss=678277309445478.9\n",
      "Gradient Descent(40/49): loss=1758162613813498.0\n",
      "Gradient Descent(41/49): loss=4557333311265695.0\n",
      "Gradient Descent(42/49): loss=1.1813063676132324e+16\n",
      "Gradient Descent(43/49): loss=3.062064235490584e+16\n",
      "Gradient Descent(44/49): loss=7.937176704815155e+16\n",
      "Gradient Descent(45/49): loss=2.0573955736551616e+17\n",
      "Gradient Descent(46/49): loss=5.332975066471874e+17\n",
      "Gradient Descent(47/49): loss=1.38236046698012e+18\n",
      "Gradient Descent(48/49): loss=3.583216566459354e+18\n",
      "Gradient Descent(49/49): loss=9.288055661920221e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5786874409280001\n",
      "Gradient Descent(2/49): loss=0.7826531565574976\n",
      "Gradient Descent(3/49): loss=1.3113526880406396\n",
      "Gradient Descent(4/49): loss=2.6817947435983074\n",
      "Gradient Descent(5/49): loss=6.2341175958092805\n",
      "Gradient Descent(6/49): loss=15.442093661024343\n",
      "Gradient Descent(7/49): loss=39.310088419671835\n",
      "Gradient Descent(8/49): loss=101.17831763356432\n",
      "Gradient Descent(9/49): loss=261.54695457890244\n",
      "Gradient Descent(10/49): loss=677.2384984048301\n",
      "Gradient Descent(11/49): loss=1754.752549156231\n",
      "Gradient Descent(12/49): loss=4547.77672010875\n",
      "Gradient Descent(13/49): loss=11787.574673635847\n",
      "Gradient Descent(14/49): loss=30553.854948969532\n",
      "Gradient Descent(15/49): loss=79197.93005067226\n",
      "Gradient Descent(16/49): loss=205288.2371217888\n",
      "Gradient Descent(17/49): loss=532126.9220808067\n",
      "Gradient Descent(18/49): loss=1379325.477363166\n",
      "Gradient Descent(19/49): loss=3575348.8525102977\n",
      "Gradient Descent(20/49): loss=9267661.043230537\n",
      "Gradient Descent(21/49): loss=24022703.47279365\n",
      "Gradient Descent(22/49): loss=62269248.95446423\n",
      "Gradient Descent(23/49): loss=161408119.49751487\n",
      "Gradient Descent(24/49): loss=418385985.83213216\n",
      "Gradient Descent(25/49): loss=1084498313.15807\n",
      "Gradient Descent(26/49): loss=2811128076.819433\n",
      "Gradient Descent(27/49): loss=7286725087.20582\n",
      "Gradient Descent(28/49): loss=18887920097.828396\n",
      "Gradient Descent(29/49): loss=48959377684.86375\n",
      "Gradient Descent(30/49): loss=126907602896.22202\n",
      "Gradient Descent(31/49): loss=328957197466.57477\n",
      "Gradient Descent(32/49): loss=852689951552.4631\n",
      "Gradient Descent(33/49): loss=2210257623418.6973\n",
      "Gradient Descent(34/49): loss=5729208785662.383\n",
      "Gradient Descent(35/49): loss=14850682093315.023\n",
      "Gradient Descent(36/49): loss=38494453054077.15\n",
      "Gradient Descent(37/49): loss=99781471761471.97\n",
      "Gradient Descent(38/49): loss=258643552952927.44\n",
      "Gradient Descent(39/49): loss=670429953609348.0\n",
      "Gradient Descent(40/49): loss=1737821482750924.8\n",
      "Gradient Descent(41/49): loss=4504607065438653.0\n",
      "Gradient Descent(42/49): loss=1.1676391974324284e+16\n",
      "Gradient Descent(43/49): loss=3.026637563664338e+16\n",
      "Gradient Descent(44/49): loss=7.845347228774746e+16\n",
      "Gradient Descent(45/49): loss=2.0335924551706003e+17\n",
      "Gradient Descent(46/49): loss=5.2712750030480864e+17\n",
      "Gradient Descent(47/49): loss=1.3663671935400755e+18\n",
      "Gradient Descent(48/49): loss=3.5417604023750605e+18\n",
      "Gradient Descent(49/49): loss=9.180597138996981e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5793294571635198\n",
      "Gradient Descent(2/49): loss=0.7849593430770574\n",
      "Gradient Descent(3/49): loss=1.3179725703535758\n",
      "Gradient Descent(4/49): loss=2.699596156776803\n",
      "Gradient Descent(5/49): loss=6.280902655145287\n",
      "Gradient Descent(6/49): loss=15.564007229566268\n",
      "Gradient Descent(7/49): loss=39.626742596923094\n",
      "Gradient Descent(8/49): loss=101.99975894265675\n",
      "Gradient Descent(9/49): loss=263.67685461242166\n",
      "Gradient Descent(10/49): loss=682.7600542979799\n",
      "Gradient Descent(11/49): loss=1769.0656162030423\n",
      "Gradient Descent(12/49): loss=4584.878263217173\n",
      "Gradient Descent(13/49): loss=11883.746225543373\n",
      "Gradient Descent(14/49): loss=30803.141870689546\n",
      "Gradient Descent(15/49): loss=79844.10732247551\n",
      "Gradient Descent(16/49): loss=206963.1938700348\n",
      "Gradient Descent(17/49): loss=536468.5781100339\n",
      "Gradient Descent(18/49): loss=1390579.4845984036\n",
      "Gradient Descent(19/49): loss=3604520.365306706\n",
      "Gradient Descent(20/49): loss=9343276.522190215\n",
      "Gradient Descent(21/49): loss=24218706.35645141\n",
      "Gradient Descent(22/49): loss=62777308.0298315\n",
      "Gradient Descent(23/49): loss=162725059.4273927\n",
      "Gradient Descent(24/49): loss=421799625.82506406\n",
      "Gradient Descent(25/49): loss=1093346809.3844705\n",
      "Gradient Descent(26/49): loss=2834064263.8888946\n",
      "Gradient Descent(27/49): loss=7346177977.709944\n",
      "Gradient Descent(28/49): loss=19042027935.303455\n",
      "Gradient Descent(29/49): loss=49358840610.38262\n",
      "Gradient Descent(30/49): loss=127943050745.45013\n",
      "Gradient Descent(31/49): loss=331641181836.5351\n",
      "Gradient Descent(32/49): loss=859647107437.7018\n",
      "Gradient Descent(33/49): loss=2228291267188.303\n",
      "Gradient Descent(34/49): loss=5775953793678.202\n",
      "Gradient Descent(35/49): loss=14971849828591.82\n",
      "Gradient Descent(36/49): loss=38808531940690.85\n",
      "Gradient Descent(37/49): loss=100595595643463.47\n",
      "Gradient Descent(38/49): loss=260753843467430.3\n",
      "Gradient Descent(39/49): loss=675900037651940.6\n",
      "Gradient Descent(40/49): loss=1752000487597423.0\n",
      "Gradient Descent(41/49): loss=4541360463900987.0\n",
      "Gradient Descent(42/49): loss=1.1771660458477254e+16\n",
      "Gradient Descent(43/49): loss=3.0513321074417736e+16\n",
      "Gradient Descent(44/49): loss=7.909357955699184e+16\n",
      "Gradient Descent(45/49): loss=2.050184675696958e+17\n",
      "Gradient Descent(46/49): loss=5.3142836978736416e+17\n",
      "Gradient Descent(47/49): loss=1.3775154773257065e+18\n",
      "Gradient Descent(48/49): loss=3.570657868775792e+18\n",
      "Gradient Descent(49/49): loss=9.255502261653864e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5824162452679339\n",
      "Gradient Descent(2/49): loss=0.8040829785405383\n",
      "Gradient Descent(3/49): loss=1.4002778243505791\n",
      "Gradient Descent(4/49): loss=3.003803481641282\n",
      "Gradient Descent(5/49): loss=7.316646089489703\n",
      "Gradient Descent(6/49): loss=18.91646756755814\n",
      "Gradient Descent(7/49): loss=50.115347414972426\n",
      "Gradient Descent(8/49): loss=134.02785465258444\n",
      "Gradient Descent(9/49): loss=359.7189341188757\n",
      "Gradient Descent(10/49): loss=966.7376614513645\n",
      "Gradient Descent(11/49): loss=2599.37523048497\n",
      "Gradient Descent(12/49): loss=6990.517236157088\n",
      "Gradient Descent(13/49): loss=18800.932774613626\n",
      "Gradient Descent(14/49): loss=50566.22640684812\n",
      "Gradient Descent(15/49): loss=136002.16016011444\n",
      "Gradient Descent(16/49): loss=365790.6475828838\n",
      "Gradient Descent(17/49): loss=983829.7633551466\n",
      "Gradient Descent(18/49): loss=2646107.769136366\n",
      "Gradient Descent(19/49): loss=7116970.69348455\n",
      "Gradient Descent(20/49): loss=19141803.614813585\n",
      "Gradient Descent(21/49): loss=51483794.24001748\n",
      "Gradient Descent(22/49): loss=138470812.22555423\n",
      "Gradient Descent(23/49): loss=372431095.7994633\n",
      "Gradient Descent(24/49): loss=1001690674.4997559\n",
      "Gradient Descent(25/49): loss=2694147237.3720465\n",
      "Gradient Descent(26/49): loss=7246178408.873659\n",
      "Gradient Descent(27/49): loss=19489321447.74262\n",
      "Gradient Descent(28/49): loss=52418478965.087166\n",
      "Gradient Descent(29/49): loss=140984741023.73053\n",
      "Gradient Descent(30/49): loss=379192559456.67883\n",
      "Gradient Descent(31/49): loss=1019876307913.9194\n",
      "Gradient Descent(32/49): loss=2743059317764.8027\n",
      "Gradient Descent(33/49): loss=7377732341059.134\n",
      "Gradient Descent(34/49): loss=19843148904513.016\n",
      "Gradient Descent(35/49): loss=53370133293576.54\n",
      "Gradient Descent(36/49): loss=143544310506393.9\n",
      "Gradient Descent(37/49): loss=386076777538006.4\n",
      "Gradient Descent(38/49): loss=1038392100866202.1\n",
      "Gradient Descent(39/49): loss=2792859394489964.5\n",
      "Gradient Descent(40/49): loss=7511674627420520.0\n",
      "Gradient Descent(41/49): loss=2.020340007791021e+16\n",
      "Gradient Descent(42/49): loss=5.43390648495507e+16\n",
      "Gradient Descent(43/49): loss=1.4615034881936224e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=3.930859781845877e+17\n",
      "Gradient Descent(45/49): loss=1.0572440469252672e+18\n",
      "Gradient Descent(46/49): loss=2.843563588610195e+18\n",
      "Gradient Descent(47/49): loss=7.648048627925683e+18\n",
      "Gradient Descent(48/49): loss=2.0570191589667574e+19\n",
      "Gradient Descent(49/49): loss=5.53255872995697e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5844836870466217\n",
      "Gradient Descent(2/49): loss=0.8117110117272325\n",
      "Gradient Descent(3/49): loss=1.4228616241882879\n",
      "Gradient Descent(4/49): loss=3.066612311463657\n",
      "Gradient Descent(5/49): loss=7.487644159959794\n",
      "Gradient Descent(6/49): loss=19.37845141967495\n",
      "Gradient Descent(7/49): loss=51.35996662540687\n",
      "Gradient Descent(8/49): loss=137.3774499227386\n",
      "Gradient Descent(9/49): loss=368.7300729992239\n",
      "Gradient Descent(10/49): loss=990.9760880257755\n",
      "Gradient Descent(11/49): loss=2664.5689700410862\n",
      "Gradient Descent(12/49): loss=7165.864385509262\n",
      "Gradient Descent(13/49): loss=19272.54853495174\n",
      "Gradient Descent(14/49): loss=51834.686223293735\n",
      "Gradient Descent(15/49): loss=139413.81174984344\n",
      "Gradient Descent(16/49): loss=374966.6277660274\n",
      "Gradient Descent(17/49): loss=1008509.4817231992\n",
      "Gradient Descent(18/49): loss=2712486.3417262677\n",
      "Gradient Descent(19/49): loss=7295502.504389893\n",
      "Gradient Descent(20/49): loss=19621982.775488753\n",
      "Gradient Descent(21/49): loss=52775284.11264086\n",
      "Gradient Descent(22/49): loss=141944403.389049\n",
      "Gradient Descent(23/49): loss=381773666.5948887\n",
      "Gradient Descent(24/49): loss=1026818452.9133003\n",
      "Gradient Descent(25/49): loss=2761730910.195412\n",
      "Gradient Descent(26/49): loss=7427951455.30208\n",
      "Gradient Descent(27/49): loss=19978218233.4201\n",
      "Gradient Descent(28/49): loss=53733415759.84577\n",
      "Gradient Descent(29/49): loss=144521395026.9024\n",
      "Gradient Descent(30/49): loss=388704744063.5555\n",
      "Gradient Descent(31/49): loss=1045460279632.6079\n",
      "Gradient Descent(32/49): loss=2811869968098.9556\n",
      "Gradient Descent(33/49): loss=7562805466198.669\n",
      "Gradient Descent(34/49): loss=20340921581886.184\n",
      "Gradient Descent(35/49): loss=54708942686638.79\n",
      "Gradient Descent(36/49): loss=147145172249992.53\n",
      "Gradient Descent(37/49): loss=395761655283535.25\n",
      "Gradient Descent(38/49): loss=1064440548050532.9\n",
      "Gradient Descent(39/49): loss=2862919298036667.0\n",
      "Gradient Descent(40/49): loss=7700107743999648.0\n",
      "Gradient Descent(41/49): loss=2.0710209788260828e+16\n",
      "Gradient Descent(42/49): loss=5.570218024650948e+16\n",
      "Gradient Descent(43/49): loss=1.4981658399100704e+17\n",
      "Gradient Descent(44/49): loss=4.029466843022296e+17\n",
      "Gradient Descent(45/49): loss=1.0837654020991809e+18\n",
      "Gradient Descent(46/49): loss=2.9148954254861937e+18\n",
      "Gradient Descent(47/49): loss=7.839902736387265e+18\n",
      "Gradient Descent(48/49): loss=2.1086202399787713e+19\n",
      "Gradient Descent(49/49): loss=5.671344997447264e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5835062497279999\n",
      "Gradient Descent(2/49): loss=0.8081046589964291\n",
      "Gradient Descent(3/49): loss=1.412184540564766\n",
      "Gradient Descent(4/49): loss=3.036917790031082\n",
      "Gradient Descent(5/49): loss=7.406800337794935\n",
      "Gradient Descent(6/49): loss=19.16003643826225\n",
      "Gradient Descent(7/49): loss=50.77154025407795\n",
      "Gradient Descent(8/49): loss=135.79384091708522\n",
      "Gradient Descent(9/49): loss=364.4698207803479\n",
      "Gradient Descent(10/49): loss=979.5167362205337\n",
      "Gradient Descent(11/49): loss=2633.746919988661\n",
      "Gradient Descent(12/49): loss=7082.9644222509305\n",
      "Gradient Descent(13/49): loss=19049.579816336\n",
      "Gradient Descent(14/49): loss=51234.98858026568\n",
      "Gradient Descent(15/49): loss=137800.86399173358\n",
      "Gradient Descent(16/49): loss=370628.4424983823\n",
      "Gradient Descent(17/49): loss=996841.4976498672\n",
      "Gradient Descent(18/49): loss=2681104.130785444\n",
      "Gradient Descent(19/49): loss=7211096.908866568\n",
      "Gradient Descent(20/49): loss=19394965.484794647\n",
      "Gradient Descent(21/49): loss=52164698.406609274\n",
      "Gradient Descent(22/49): loss=140302172.07313836\n",
      "Gradient Descent(23/49): loss=377356721.2465871\n",
      "Gradient Descent(24/49): loss=1014938636.7034667\n",
      "Gradient Descent(25/49): loss=2729778956.5166655\n",
      "Gradient Descent(26/49): loss=7342013480.686442\n",
      "Gradient Descent(27/49): loss=19747079456.89453\n",
      "Gradient Descent(28/49): loss=53111744906.50476\n",
      "Gradient Descent(29/49): loss=142849349099.7776\n",
      "Gradient Descent(30/49): loss=384207609337.9822\n",
      "Gradient Descent(31/49): loss=1033364786074.7059\n",
      "Gradient Descent(32/49): loss=2779337928625.988\n",
      "Gradient Descent(33/49): loss=7475307292831.144\n",
      "Gradient Descent(34/49): loss=20105586494796.21\n",
      "Gradient Descent(35/49): loss=54075985436398.57\n",
      "Gradient Descent(36/49): loss=145442770429752.56\n",
      "Gradient Descent(37/49): loss=391182875347829.75\n",
      "Gradient Descent(38/49): loss=1052125461535598.6\n",
      "Gradient Descent(39/49): loss=2829796641345913.0\n",
      "Gradient Descent(40/49): loss=7611021046563309.0\n",
      "Gradient Descent(41/49): loss=2.0470602206834788e+16\n",
      "Gradient Descent(42/49): loss=5.505773169549978e+16\n",
      "Gradient Descent(43/49): loss=1.480832751682069e+17\n",
      "Gradient Descent(44/49): loss=3.982847768924315e+17\n",
      "Gradient Descent(45/49): loss=1.0712267359298232e+18\n",
      "Gradient Descent(46/49): loss=2.881171428957116e+18\n",
      "Gradient Descent(47/49): loss=7.749198675323414e+18\n",
      "Gradient Descent(48/49): loss=2.0842244757150056e+19\n",
      "Gradient Descent(49/49): loss=5.605730149883324e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.58418758295552\n",
      "Gradient Descent(2/49): loss=0.8106185060727068\n",
      "Gradient Descent(3/49): loss=1.419627116888743\n",
      "Gradient Descent(4/49): loss=3.0576166765396198\n",
      "Gradient Descent(5/49): loss=7.463153396175963\n",
      "Gradient Descent(6/49): loss=19.31228495730952\n",
      "Gradient Descent(7/49): loss=51.18170920413821\n",
      "Gradient Descent(8/49): loss=136.89771265841748\n",
      "Gradient Descent(9/49): loss=367.4394755490193\n",
      "Gradient Descent(10/49): loss=987.504601019628\n",
      "Gradient Descent(11/49): loss=2655.2317624850675\n",
      "Gradient Descent(12/49): loss=7140.750735963478\n",
      "Gradient Descent(13/49): loss=19205.00256703171\n",
      "Gradient Descent(14/49): loss=51653.014291874206\n",
      "Gradient Descent(15/49): loss=138925.1866270117\n",
      "Gradient Descent(16/49): loss=373652.4213395803\n",
      "Gradient Descent(17/49): loss=1004974.7918225091\n",
      "Gradient Descent(18/49): loss=2702979.4394732565\n",
      "Gradient Descent(19/49): loss=7269932.739794955\n",
      "Gradient Descent(20/49): loss=19553210.336340394\n",
      "Gradient Descent(21/49): loss=52590313.76001107\n",
      "Gradient Descent(22/49): loss=141446907.12830865\n",
      "Gradient Descent(23/49): loss=380435600.6516903\n",
      "Gradient Descent(24/49): loss=1023219590.7521093\n",
      "Gradient Descent(25/49): loss=2752051410.526016\n",
      "Gradient Descent(26/49): loss=7401917472.990401\n",
      "Gradient Descent(27/49): loss=19908197234.59524\n",
      "Gradient Descent(28/49): loss=53545087281.40789\n",
      "Gradient Descent(29/49): loss=144014866751.30936\n",
      "Gradient Descent(30/49): loss=387342385613.5702\n",
      "Gradient Descent(31/49): loss=1041796080345.4597\n",
      "Gradient Descent(32/49): loss=2802014737696.1445\n",
      "Gradient Descent(33/49): loss=7536298838507.084\n",
      "Gradient Descent(34/49): loss=20269629356046.81\n",
      "Gradient Descent(35/49): loss=54517195116020.87\n",
      "Gradient Descent(36/49): loss=146629447984061.62\n",
      "Gradient Descent(37/49): loss=394374563297968.0\n",
      "Gradient Descent(38/49): loss=1060709825446224.8\n",
      "Gradient Descent(39/49): loss=2852885146520301.5\n",
      "Gradient Descent(40/49): loss=7673119890081465.0\n",
      "Gradient Descent(41/49): loss=2.0637623256361116e+16\n",
      "Gradient Descent(42/49): loss=5.550695151030586e+16\n",
      "Gradient Descent(43/49): loss=1.4929149678212704e+17\n",
      "Gradient Descent(44/49): loss=4.015344097452379e+17\n",
      "Gradient Descent(45/49): loss=1.0799669484508364e+18\n",
      "Gradient Descent(46/49): loss=2.9046791045532334e+18\n",
      "Gradient Descent(47/49): loss=7.812424919605757e+18\n",
      "Gradient Descent(48/49): loss=2.1012298063771914e+19\n",
      "Gradient Descent(49/49): loss=5.651467687232389e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.587259955705378\n",
      "Gradient Descent(2/49): loss=0.830619246172125\n",
      "Gradient Descent(3/49): loss=1.5093239713548452\n",
      "Gradient Descent(4/49): loss=3.4021635794167104\n",
      "Gradient Descent(5/49): loss=8.681103962340387\n",
      "Gradient Descent(6/49): loss=23.403540796278367\n",
      "Gradient Descent(7/49): loss=64.46294488244867\n",
      "Gradient Descent(8/49): loss=178.97351693835836\n",
      "Gradient Descent(9/49): loss=498.3320513451383\n",
      "Gradient Descent(10/49): loss=1388.9910679520876\n",
      "Gradient Descent(11/49): loss=3872.9499993674876\n",
      "Gradient Descent(12/49): loss=10800.463063192372\n",
      "Gradient Descent(13/49): loss=30120.604246892315\n",
      "Gradient Descent(14/49): loss=84002.54599411594\n",
      "Gradient Descent(15/49): loss=234273.89333296532\n",
      "Gradient Descent(16/49): loss=653365.653926235\n",
      "Gradient Descent(17/49): loss=1822170.6650446749\n",
      "Gradient Descent(18/49): loss=5081850.960553297\n",
      "Gradient Descent(19/49): loss=14172773.336696502\n",
      "Gradient Descent(20/49): loss=39526446.75152105\n",
      "Gradient Descent(21/49): loss=110235306.53811818\n",
      "Gradient Descent(22/49): loss=307435245.5969556\n",
      "Gradient Descent(23/49): loss=857406155.6382073\n",
      "Gradient Descent(24/49): loss=2391220026.652102\n",
      "Gradient Descent(25/49): loss=6668873531.5223465\n",
      "Gradient Descent(26/49): loss=18598821391.253704\n",
      "Gradient Descent(27/49): loss=51870252977.258965\n",
      "Gradient Descent(28/49): loss=144660948527.45758\n",
      "Gradient Descent(29/49): loss=403444919347.4432\n",
      "Gradient Descent(30/49): loss=1125167535567.354\n",
      "Gradient Descent(31/49): loss=3137979739943.207\n",
      "Gradient Descent(32/49): loss=8751511696726.028\n",
      "Gradient Descent(33/49): loss=24407090970997.914\n",
      "Gradient Descent(34/49): loss=68068936009011.0\n",
      "Gradient Descent(35/49): loss=189837455635512.16\n",
      "Gradient Descent(36/49): loss=529437680021901.0\n",
      "Gradient Descent(37/49): loss=1476548745813171.8\n",
      "Gradient Descent(38/49): loss=4117946797198651.5\n",
      "Gradient Descent(39/49): loss=1.1484541822706528e+16\n",
      "Gradient Descent(40/49): loss=3.2029238689347976e+16\n",
      "Gradient Descent(41/49): loss=8.932634378072315e+16\n",
      "Gradient Descent(42/49): loss=2.4912224017004323e+17\n",
      "Gradient Descent(43/49): loss=6.947770156102624e+17\n",
      "Gradient Descent(44/49): loss=1.9376636188355804e+18\n",
      "Gradient Descent(45/49): loss=5.403950066570904e+18\n",
      "Gradient Descent(46/49): loss=1.5071076340659636e+19\n",
      "Gradient Descent(47/49): loss=4.2031724806467985e+19\n",
      "Gradient Descent(48/49): loss=1.1722227731274557e+20\n",
      "Gradient Descent(49/49): loss=3.269212091975042e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5894489037391699\n",
      "Gradient Descent(2/49): loss=0.8389129513773241\n",
      "Gradient Descent(3/49): loss=1.5346432338354237\n",
      "Gradient Descent(4/49): loss=3.474965418582698\n",
      "Gradient Descent(5/49): loss=8.886329959624117\n",
      "Gradient Descent(6/49): loss=23.978084528132435\n",
      "Gradient Descent(7/49): loss=66.06747884425413\n",
      "Gradient Descent(8/49): loss=183.45059065247654\n",
      "Gradient Descent(9/49): loss=510.8203511744002\n",
      "Gradient Descent(10/49): loss=1423.8218762940933\n",
      "Gradient Descent(11/49): loss=3970.091829700613\n",
      "Gradient Descent(12/49): loss=11071.384102755075\n",
      "Gradient Descent(13/49): loss=30876.17812307427\n",
      "Gradient Descent(14/49): loss=86109.76816634554\n",
      "Gradient Descent(15/49): loss=240150.7274380521\n",
      "Gradient Descent(16/49): loss=669755.5587509405\n",
      "Gradient Descent(17/49): loss=1867880.4727996185\n",
      "Gradient Descent(18/49): loss=5209331.045589811\n",
      "Gradient Descent(19/49): loss=14528302.548043922\n",
      "Gradient Descent(20/49): loss=40517982.171238385\n",
      "Gradient Descent(21/49): loss=113000599.67237361\n",
      "Gradient Descent(22/49): loss=315147371.6212854\n",
      "Gradient Descent(23/49): loss=878914503.9094913\n",
      "Gradient Descent(24/49): loss=2451204659.1483574\n",
      "Gradient Descent(25/49): loss=6836164673.093179\n",
      "Gradient Descent(26/49): loss=19065379655.984303\n",
      "Gradient Descent(27/49): loss=53171437321.774765\n",
      "Gradient Descent(28/49): loss=148289821545.87698\n",
      "Gradient Descent(29/49): loss=413565483308.49133\n",
      "Gradient Descent(30/49): loss=1153392776398.271\n",
      "Gradient Descent(31/49): loss=3216697114096.426\n",
      "Gradient Descent(32/49): loss=8971046581503.186\n",
      "Gradient Descent(33/49): loss=25019351811155.285\n",
      "Gradient Descent(34/49): loss=69776470266129.03\n",
      "Gradient Descent(35/49): loss=194599597925222.6\n",
      "Gradient Descent(36/49): loss=542718818653690.7\n",
      "Gradient Descent(37/49): loss=1513588513343120.8\n",
      "Gradient Descent(38/49): loss=4221247004862504.5\n",
      "Gradient Descent(39/49): loss=1.177263577185975e+16\n",
      "Gradient Descent(40/49): loss=3.283270390414188e+16\n",
      "Gradient Descent(41/49): loss=9.156712791826021e+16\n",
      "Gradient Descent(42/49): loss=2.5537156305121926e+17\n",
      "Gradient Descent(43/49): loss=7.122057521935985e+17\n",
      "Gradient Descent(44/49): loss=1.9862706222926817e+18\n",
      "Gradient Descent(45/49): loss=5.539510138512006e+18\n",
      "Gradient Descent(46/49): loss=1.5449139825296933e+19\n",
      "Gradient Descent(47/49): loss=4.308610605877337e+19\n",
      "Gradient Descent(48/49): loss=1.2016284118731694e+20\n",
      "Gradient Descent(49/49): loss=3.351221477873227e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.588414021152\n",
      "Gradient Descent(2/49): loss=0.8349918847427795\n",
      "Gradient Descent(3/49): loss=1.5226728885110499\n",
      "Gradient Descent(4/49): loss=3.440546439920615\n",
      "Gradient Descent(5/49): loss=8.789303987446633\n",
      "Gradient Descent(6/49): loss=23.706453911742276\n",
      "Gradient Descent(7/49): loss=65.30889333560964\n",
      "Gradient Descent(8/49): loss=181.33393664484575\n",
      "Gradient Descent(9/49): loss=504.916179929985\n",
      "Gradient Descent(10/49): loss=1407.3546982277865\n",
      "Gradient Descent(11/49): loss=3924.165481908349\n",
      "Gradient Descent(12/49): loss=10943.299076516118\n",
      "Gradient Descent(13/49): loss=30518.960758515794\n",
      "Gradient Descent(14/49): loss=85113.52362345002\n",
      "Gradient Descent(15/49): loss=237372.29999744706\n",
      "Gradient Descent(16/49): loss=662006.8014268823\n",
      "Gradient Descent(17/49): loss=1846269.9624633451\n",
      "Gradient Descent(18/49): loss=5149061.492278557\n",
      "Gradient Descent(19/49): loss=14360216.789780123\n",
      "Gradient Descent(20/49): loss=40049207.798976675\n",
      "Gradient Descent(21/49): loss=111693234.82453594\n",
      "Gradient Descent(22/49): loss=311501261.7961165\n",
      "Gradient Descent(23/49): loss=868745868.2171134\n",
      "Gradient Descent(24/49): loss=2422845351.0644355\n",
      "Gradient Descent(25/49): loss=6757073398.777685\n",
      "Gradient Descent(26/49): loss=18844802001.04359\n",
      "Gradient Descent(27/49): loss=52556268299.90966\n",
      "Gradient Descent(28/49): loss=146574176660.8058\n",
      "Gradient Descent(29/49): loss=408780721288.51105\n",
      "Gradient Descent(30/49): loss=1140048553600.7908\n",
      "Gradient Descent(31/49): loss=3179481411136.2007\n",
      "Gradient Descent(32/49): loss=8867255707516.918\n",
      "Gradient Descent(33/49): loss=24729889442693.234\n",
      "Gradient Descent(34/49): loss=68969188666721.414\n",
      "Gradient Descent(35/49): loss=192348170272611.22\n",
      "Gradient Descent(36/49): loss=536439812073325.25\n",
      "Gradient Descent(37/49): loss=1496076991891367.0\n",
      "Gradient Descent(38/49): loss=4172409122685670.5\n",
      "Gradient Descent(39/49): loss=1.163643180225887e+16\n",
      "Gradient Descent(40/49): loss=3.2452844653320876e+16\n",
      "Gradient Descent(41/49): loss=9.050773845364496e+16\n",
      "Gradient Descent(42/49): loss=2.5241703177339e+17\n",
      "Gradient Descent(43/49): loss=7.039658599127572e+17\n",
      "Gradient Descent(44/49): loss=1.9632903867105723e+18\n",
      "Gradient Descent(45/49): loss=5.475420559496786e+18\n",
      "Gradient Descent(46/49): loss=1.5270400398380155e+19\n",
      "Gradient Descent(47/49): loss=4.25876196710459e+19\n",
      "Gradient Descent(48/49): loss=1.1877261250058514e+20\n",
      "Gradient Descent(49/49): loss=3.312449390028806e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.58913539722368\n",
      "Gradient Descent(2/49): loss=0.837725106540776\n",
      "Gradient Descent(3/49): loss=1.531016946855257\n",
      "Gradient Descent(4/49): loss=3.4645385603086023\n",
      "Gradient Descent(5/49): loss=8.856936988067577\n",
      "Gradient Descent(6/49): loss=23.895796963242823\n",
      "Gradient Descent(7/49): loss=65.83767354800942\n",
      "Gradient Descent(8/49): loss=182.8093731552578\n",
      "Gradient Descent(9/49): loss=509.031746189893\n",
      "Gradient Descent(10/49): loss=1418.8333223462112\n",
      "Gradient Descent(11/49): loss=3956.178938088556\n",
      "Gradient Descent(12/49): loss=11032.58212583333\n",
      "Gradient Descent(13/49): loss=30767.96297613037\n",
      "Gradient Descent(14/49): loss=85807.96662952099\n",
      "Gradient Descent(15/49): loss=239309.03281848953\n",
      "Gradient Descent(16/49): loss=667408.1563129224\n",
      "Gradient Descent(17/49): loss=1861333.8018264333\n",
      "Gradient Descent(18/49): loss=5191073.034598592\n",
      "Gradient Descent(19/49): loss=14477382.780878937\n",
      "Gradient Descent(20/49): loss=40375972.03228273\n",
      "Gradient Descent(21/49): loss=112604547.59551212\n",
      "Gradient Descent(22/49): loss=314042821.9838145\n",
      "Gradient Descent(23/49): loss=875834025.4253901\n",
      "Gradient Descent(24/49): loss=2442613512.703606\n",
      "Gradient Descent(25/49): loss=6812204824.774151\n",
      "Gradient Descent(26/49): loss=18998558035.007866\n",
      "Gradient Descent(27/49): loss=52985078503.02576\n",
      "Gradient Descent(28/49): loss=147770085436.28607\n",
      "Gradient Descent(29/49): loss=412115991272.4137\n",
      "Gradient Descent(30/49): loss=1149350288058.7756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=3205423018366.4224\n",
      "Gradient Descent(32/49): loss=8939604255921.94\n",
      "Gradient Descent(33/49): loss=24931662309341.234\n",
      "Gradient Descent(34/49): loss=69531913014520.805\n",
      "Gradient Descent(35/49): loss=193917552206181.56\n",
      "Gradient Descent(36/49): loss=540816661347778.0\n",
      "Gradient Descent(37/49): loss=1508283586832910.0\n",
      "Gradient Descent(38/49): loss=4206452095318087.0\n",
      "Gradient Descent(39/49): loss=1.1731374248632146e+16\n",
      "Gradient Descent(40/49): loss=3.2717629642009332e+16\n",
      "Gradient Descent(41/49): loss=9.124619730859483e+16\n",
      "Gradient Descent(42/49): loss=2.5447651967393542e+17\n",
      "Gradient Descent(43/49): loss=7.097095657186525e+17\n",
      "Gradient Descent(44/49): loss=1.9793090078329288e+18\n",
      "Gradient Descent(45/49): loss=5.520094891945272e+18\n",
      "Gradient Descent(46/49): loss=1.5394992644145402e+19\n",
      "Gradient Descent(47/49): loss=4.293509498525508e+19\n",
      "Gradient Descent(48/49): loss=1.1974168640437736e+20\n",
      "Gradient Descent(49/49): loss=3.339475892131591e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5921914675404799\n",
      "Gradient Descent(2/49): loss=0.8586248087324567\n",
      "Gradient Descent(3/49): loss=1.628617164777279\n",
      "Gradient Descent(4/49): loss=3.8538950737468127\n",
      "Gradient Descent(5/49): loss=10.284948230668\n",
      "Gradient Descent(6/49): loss=28.87069185417269\n",
      "Gradient Descent(7/49): loss=82.58349092610872\n",
      "Gradient Descent(8/49): loss=237.81348024397204\n",
      "Gradient Descent(9/49): loss=686.4281493726552\n",
      "Gradient Descent(10/49): loss=1982.924543154343\n",
      "Gradient Descent(11/49): loss=5729.799121184094\n",
      "Gradient Descent(12/49): loss=16558.266651689937\n",
      "Gradient Descent(13/49): loss=47852.53781484996\n",
      "Gradient Descent(14/49): loss=138292.98147638523\n",
      "Gradient Descent(15/49): loss=399665.8636582033\n",
      "Gradient Descent(16/49): loss=1155033.4931636187\n",
      "Gradient Descent(17/49): loss=3338045.9424342355\n",
      "Gradient Descent(18/49): loss=9646951.92082581\n",
      "Gradient Descent(19/49): loss=27879690.19837669\n",
      "Gradient Descent(20/49): loss=80572303.82049195\n",
      "Gradient Descent(21/49): loss=232853957.18840188\n",
      "Gradient Descent(22/49): loss=672947935.4216166\n",
      "Gradient Descent(23/49): loss=1944819532.5154247\n",
      "Gradient Descent(24/49): loss=5620528448.11663\n",
      "Gradient Descent(25/49): loss=16243327214.205242\n",
      "Gradient Descent(26/49): loss=46943215648.19619\n",
      "Gradient Descent(27/49): loss=135665893222.44855\n",
      "Gradient Descent(28/49): loss=392074431412.03534\n",
      "Gradient Descent(29/49): loss=1133095106779.915\n",
      "Gradient Descent(30/49): loss=3274644858593.196\n",
      "Gradient Descent(31/49): loss=9463723641334.395\n",
      "Gradient Descent(32/49): loss=27350161323458.473\n",
      "Gradient Descent(33/49): loss=79041966224793.92\n",
      "Gradient Descent(34/49): loss=228431282389631.72\n",
      "Gradient Descent(35/49): loss=660166406106073.8\n",
      "Gradient Descent(36/49): loss=1907880913646370.5\n",
      "Gradient Descent(37/49): loss=5513775840437574.0\n",
      "Gradient Descent(38/49): loss=1.5934812178865946e+16\n",
      "Gradient Descent(39/49): loss=4.605160719691945e+16\n",
      "Gradient Descent(40/49): loss=1.3308914479908462e+17\n",
      "Gradient Descent(41/49): loss=3.846276284693894e+17\n",
      "Gradient Descent(42/49): loss=1.111573846276407e+18\n",
      "Gradient Descent(43/49): loss=3.212448415738641e+18\n",
      "Gradient Descent(44/49): loss=9.283975921484931e+18\n",
      "Gradient Descent(45/49): loss=2.6830690413091525e+19\n",
      "Gradient Descent(46/49): loss=7.754069529383567e+19\n",
      "Gradient Descent(47/49): loss=2.24092609399195e+20\n",
      "Gradient Descent(48/49): loss=6.476276411637085e+20\n",
      "Gradient Descent(49/49): loss=1.8716438829633189e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5945041243596797\n",
      "Gradient Descent(2/49): loss=0.8676210437591605\n",
      "Gradient Descent(3/49): loss=1.6569289408236345\n",
      "Gradient Descent(4/49): loss=3.9380287633400384\n",
      "Gradient Descent(5/49): loss=10.530407250411699\n",
      "Gradient Descent(6/49): loss=29.582381078050915\n",
      "Gradient Descent(7/49): loss=84.64258543993161\n",
      "Gradient Descent(8/49): loss=243.76657604575018\n",
      "Gradient Descent(9/49): loss=703.6349088966292\n",
      "Gradient Descent(10/49): loss=2032.6543908355197\n",
      "Gradient Descent(11/49): loss=5873.520693638846\n",
      "Gradient Descent(12/49): loss=16973.624308740946\n",
      "Gradient Descent(13/49): loss=49052.92375638297\n",
      "Gradient Descent(14/49): loss=141762.09916006145\n",
      "Gradient Descent(15/49): loss=409691.6160767264\n",
      "Gradient Descent(16/49): loss=1184007.9199658704\n",
      "Gradient Descent(17/49): loss=3421782.038205181\n",
      "Gradient Descent(18/49): loss=9888949.239916217\n",
      "Gradient Descent(19/49): loss=28579062.452863064\n",
      "Gradient Descent(20/49): loss=82593489.63827829\n",
      "Gradient Descent(21/49): loss=238695184.20414832\n",
      "Gradient Descent(22/49): loss=689829081.499546\n",
      "Gradient Descent(23/49): loss=1993606044.6830504\n",
      "Gradient Descent(24/49): loss=5761521468.283892\n",
      "Gradient Descent(25/49): loss=16650797042.489906\n",
      "Gradient Descent(26/49): loss=48120803451.94143\n",
      "Gradient Descent(27/49): loss=139069121975.25714\n",
      "Gradient Descent(28/49): loss=401909762507.6574\n",
      "Gradient Descent(29/49): loss=1161519213646.2275\n",
      "Gradient Descent(30/49): loss=3356790527437.029\n",
      "Gradient Descent(31/49): loss=9701124624291.717\n",
      "Gradient Descent(32/49): loss=28036250164200.35\n",
      "Gradient Descent(33/49): loss=81024762974542.81\n",
      "Gradient Descent(34/49): loss=234161564996431.25\n",
      "Gradient Descent(35/49): loss=676726922839633.9\n",
      "Gradient Descent(36/49): loss=1955740807006607.8\n",
      "Gradient Descent(37/49): loss=5652090932249485.0\n",
      "Gradient Descent(38/49): loss=1.6334542794200876e+16\n",
      "Gradient Descent(39/49): loss=4.720682867523829e+16\n",
      "Gradient Descent(40/49): loss=1.3642773487144926e+17\n",
      "Gradient Descent(41/49): loss=3.9427615377852186e+17\n",
      "Gradient Descent(42/49): loss=1.1394580844199044e+18\n",
      "Gradient Descent(43/49): loss=3.293033863973689e+18\n",
      "Gradient Descent(44/49): loss=9.51686786688327e+18\n",
      "Gradient Descent(45/49): loss=2.75037481352916e+19\n",
      "Gradient Descent(46/49): loss=7.948583211099193e+19\n",
      "Gradient Descent(47/49): loss=2.297140548007592e+20\n",
      "Gradient Descent(48/49): loss=6.638736183741439e+20\n",
      "Gradient Descent(49/49): loss=1.9185947571011212e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5934107551999999\n",
      "Gradient Descent(2/49): loss=0.863367837728015\n",
      "Gradient Descent(3/49): loss=1.643543806233823\n",
      "Gradient Descent(4/49): loss=3.8982523552156216\n",
      "Gradient Descent(5/49): loss=10.414360061773746\n",
      "Gradient Descent(6/49): loss=29.245911333725406\n",
      "Gradient Descent(7/49): loss=83.66909450967226\n",
      "Gradient Descent(8/49): loss=240.95209388815093\n",
      "Gradient Descent(9/49): loss=695.4999620918973\n",
      "Gradient Descent(10/49): loss=2009.1433012009948\n",
      "Gradient Descent(11/49): loss=5805.572551225689\n",
      "Gradient Descent(12/49): loss=16777.25308379673\n",
      "Gradient Descent(13/49): loss=48485.40982293286\n",
      "Gradient Descent(14/49): loss=140121.98279902348\n",
      "Gradient Descent(15/49): loss=404951.67869990185\n",
      "Gradient Descent(16/49): loss=1170309.4998534056\n",
      "Gradient Descent(17/49): loss=3382193.602987123\n",
      "Gradient Descent(18/49): loss=9774538.661043968\n",
      "Gradient Descent(19/49): loss=28248415.878827266\n",
      "Gradient Descent(20/49): loss=81637921.03822616\n",
      "Gradient Descent(21/49): loss=235933590.94887808\n",
      "Gradient Descent(22/49): loss=681848076.9905869\n",
      "Gradient Descent(23/49): loss=1970540941.651176\n",
      "Gradient Descent(24/49): loss=5694863320.520536\n",
      "Gradient Descent(25/49): loss=16458154995.452805\n",
      "Gradient Descent(26/49): loss=47564067936.003944\n",
      "Gradient Descent(27/49): loss=137460156334.19635\n",
      "Gradient Descent(28/49): loss=397259851804.9579\n",
      "Gradient Descent(29/49): loss=1148080971715.439\n",
      "Gradient Descent(30/49): loss=3317954008256.8423\n",
      "Gradient Descent(31/49): loss=9588887083862.143\n",
      "Gradient Descent(32/49): loss=27711883672361.27\n",
      "Gradient Descent(33/49): loss=80087343813127.45\n",
      "Gradient Descent(34/49): loss=231452423619922.4\n",
      "Gradient Descent(35/49): loss=668897504261529.4\n",
      "Gradient Descent(36/49): loss=1933113787315761.2\n",
      "Gradient Descent(37/49): loss=5586698845343085.0\n",
      "Gradient Descent(38/49): loss=1.6145559663043148e+16\n",
      "Gradient Descent(39/49): loss=4.666066742619176e+16\n",
      "Gradient Descent(40/49): loss=1.3484932886169962e+17\n",
      "Gradient Descent(41/49): loss=3.8971456041034714e+17\n",
      "Gradient Descent(42/49): loss=1.1262750795859466e+18\n",
      "Gradient Descent(43/49): loss=3.2549349800032036e+18\n",
      "Gradient Descent(44/49): loss=9.406762092209644e+18\n",
      "Gradient Descent(45/49): loss=2.718554244648802e+19\n",
      "Gradient Descent(46/49): loss=7.856621767034213e+19\n",
      "Gradient Descent(47/49): loss=2.2705636906730753e+20\n",
      "Gradient Descent(48/49): loss=6.561929066044656e+20\n",
      "Gradient Descent(49/49): loss=1.8963975000870153e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5941728999679999\n",
      "Gradient Descent(2/49): loss=0.8663325808755372\n",
      "Gradient Descent(3/49): loss=1.6528740586983584\n",
      "Gradient Descent(4/49): loss=3.9259789296065546\n",
      "Gradient Descent(5/49): loss=10.49525200653051\n",
      "Gradient Descent(6/49): loss=29.480451198842704\n",
      "Gradient Descent(7/49): loss=84.34767686462658\n",
      "Gradient Descent(8/49): loss=242.91395903873988\n",
      "Gradient Descent(9/49): loss=701.1705145219827\n",
      "Gradient Descent(10/49): loss=2025.531959868547\n",
      "Gradient Descent(11/49): loss=5852.936536919639\n",
      "Gradient Descent(12/49): loss=16914.13576459585\n",
      "Gradient Descent(13/49): loss=48881.00153258582\n",
      "Gradient Descent(14/49): loss=141265.24360206575\n",
      "Gradient Descent(15/49): loss=408255.7031828908\n",
      "Gradient Descent(16/49): loss=1179858.1313714676\n",
      "Gradient Descent(17/49): loss=3409789.1488361717\n",
      "Gradient Descent(18/49): loss=9854289.78931023\n",
      "Gradient Descent(19/49): loss=28478896.640277077\n",
      "Gradient Descent(20/49): loss=82304010.43957557\n",
      "Gradient Descent(21/49): loss=237858589.31953236\n",
      "Gradient Descent(22/49): loss=687411322.2825363\n",
      "Gradient Descent(23/49): loss=1986618720.5457547\n",
      "Gradient Descent(24/49): loss=5741328101.526912\n",
      "Gradient Descent(25/49): loss=16592438212.56192\n",
      "Gradient Descent(26/49): loss=47952146433.44819\n",
      "Gradient Descent(27/49): loss=138581703191.80563\n",
      "Gradient Descent(28/49): loss=400501122223.4599\n",
      "Gradient Descent(29/49): loss=1157448243224.9128\n",
      "Gradient Descent(30/49): loss=3345025422919.2188\n",
      "Gradient Descent(31/49): loss=9667123472235.072\n",
      "Gradient Descent(32/49): loss=27937986834758.14\n",
      "Gradient Descent(33/49): loss=80740781952447.73\n",
      "Gradient Descent(34/49): loss=233340859842600.16\n",
      "Gradient Descent(35/49): loss=674355084945090.4\n",
      "Gradient Descent(36/49): loss=1948886195491208.5\n",
      "Gradient Descent(37/49): loss=5632281104969312.0\n",
      "Gradient Descent(38/49): loss=1.6277292393361792e+16\n",
      "Gradient Descent(39/49): loss=4.704137501681833e+16\n",
      "Gradient Descent(40/49): loss=1.3594957379861046e+17\n",
      "Gradient Descent(41/49): loss=3.9289426827801325e+17\n",
      "Gradient Descent(42/49): loss=1.1354644353235141e+18\n",
      "Gradient Descent(43/49): loss=3.281492218085229e+18\n",
      "Gradient Descent(44/49): loss=9.483512510265647e+18\n",
      "Gradient Descent(45/49): loss=2.740735115466958e+19\n",
      "Gradient Descent(46/49): loss=7.92072448370018e+19\n",
      "Gradient Descent(47/49): loss=2.2890893757894512e+20\n",
      "Gradient Descent(48/49): loss=6.615468296031881e+20\n",
      "Gradient Descent(49/49): loss=1.91187033755334e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5972107807732395\n",
      "Gradient Descent(2/49): loss=0.8881529265494827\n",
      "Gradient Descent(3/49): loss=1.75891367464316\n",
      "Gradient Descent(4/49): loss=4.365013517612953\n",
      "Gradient Descent(5/49): loss=12.164809737636281\n",
      "Gradient Descent(6/49): loss=35.50881984454067\n",
      "Gradient Descent(7/49): loss=105.37510769349299\n",
      "Gradient Descent(8/49): loss=314.4779205966326\n",
      "Gradient Descent(9/49): loss=940.3017293343221\n",
      "Gradient Descent(10/49): loss=2813.3298065055897\n",
      "Gradient Descent(11/49): loss=8419.115538670927\n",
      "Gradient Descent(12/49): loss=25196.671656471044\n",
      "Gradient Descent(13/49): loss=75410.2193614335\n",
      "Gradient Descent(14/49): loss=225694.3462875982\n",
      "Gradient Descent(15/49): loss=675479.709764994\n",
      "Gradient Descent(16/49): loss=2021642.324116584\n",
      "Gradient Descent(17/49): loss=6050572.4126095725\n",
      "Gradient Descent(18/49): loss=18108757.274459418\n",
      "Gradient Descent(19/49): loss=54197698.747488335\n",
      "Gradient Descent(20/49): loss=162208291.68212226\n",
      "Gradient Descent(21/49): loss=485473195.2762249\n",
      "Gradient Descent(22/49): loss=1452972725.2428873\n",
      "Gradient Descent(23/49): loss=4348602068.479756\n",
      "Gradient Descent(24/49): loss=13014931129.852894\n",
      "Gradient Descent(25/49): loss=38952387377.64081\n",
      "Gradient Descent(26/49): loss=116580600181.63646\n",
      "Gradient Descent(27/49): loss=348914078282.7214\n",
      "Gradient Descent(28/49): loss=1044264944891.5507\n",
      "Gradient Descent(29/49): loss=3125380553564.881\n",
      "Gradient Descent(30/49): loss=9353951458764.303\n",
      "Gradient Descent(31/49): loss=27995441320934.746\n",
      "Gradient Descent(32/49): loss=83787556329424.31\n",
      "Gradient Descent(33/49): loss=250767777338339.8\n",
      "Gradient Descent(34/49): loss=750522880795914.8\n",
      "Gradient Descent(35/49): loss=2246239929934129.8\n",
      "Gradient Descent(36/49): loss=6722771486300317.0\n",
      "Gradient Descent(37/49): loss=2.0120582781350536e+16\n",
      "Gradient Descent(38/49): loss=6.021889220630448e+16\n",
      "Gradient Descent(39/49): loss=1.8022912248424813e+17\n",
      "Gradient Descent(40/49): loss=5.394077406831489e+17\n",
      "Gradient Descent(41/49): loss=1.614393427090664e+18\n",
      "Gradient Descent(42/49): loss=4.83171808793966e+18\n",
      "Gradient Descent(43/49): loss=1.4460849065395667e+19\n",
      "Gradient Descent(44/49): loss=4.327987516782546e+19\n",
      "Gradient Descent(45/49): loss=1.2953233838978325e+20\n",
      "Gradient Descent(46/49): loss=3.876773355667678e+20\n",
      "Gradient Descent(47/49): loss=1.1602794976177906e+21\n",
      "Gradient Descent(48/49): loss=3.472600508419996e+21\n",
      "Gradient Descent(49/49): loss=1.0393146061651112e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5996493489081515\n",
      "Gradient Descent(2/49): loss=0.8978898852553351\n",
      "Gradient Descent(3/49): loss=1.7904939864889224\n",
      "Gradient Descent(4/49): loss=4.461968801071191\n",
      "Gradient Descent(5/49): loss=12.457425773634707\n",
      "Gradient Descent(6/49): loss=36.387028946822824\n",
      "Gradient Descent(7/49): loss=108.00593828384766\n",
      "Gradient Descent(8/49): loss=322.35417203860356\n",
      "Gradient Descent(9/49): loss=963.8770008432558\n",
      "Gradient Descent(10/49): loss=2883.890675173039\n",
      "Gradient Descent(11/49): loss=8630.299601073772\n",
      "Gradient Descent(12/49): loss=25828.726875404485\n",
      "Gradient Descent(13/49): loss=77301.89986474533\n",
      "Gradient Descent(14/49): loss=231355.95930454944\n",
      "Gradient Descent(15/49): loss=692424.353801887\n",
      "Gradient Descent(16/49): loss=2072355.9516928736\n",
      "Gradient Descent(17/49): loss=6202353.231021426\n",
      "Gradient Descent(18/49): loss=18563022.08832196\n",
      "Gradient Descent(19/49): loss=55557267.911342874\n",
      "Gradient Descent(20/49): loss=166277346.23505774\n",
      "Gradient Descent(21/49): loss=497651468.650138\n",
      "Gradient Descent(22/49): loss=1489421079.626218\n",
      "Gradient Descent(23/49): loss=4457688348.316377\n",
      "Gradient Descent(24/49): loss=13341415456.780035\n",
      "Gradient Descent(25/49): loss=39929522319.7022\n",
      "Gradient Descent(26/49): loss=119505067349.72835\n",
      "Gradient Descent(27/49): loss=357666716070.1457\n",
      "Gradient Descent(28/49): loss=1070460714525.367\n",
      "Gradient Descent(29/49): loss=3203781872501.787\n",
      "Gradient Descent(30/49): loss=9588598766210.15\n",
      "Gradient Descent(31/49): loss=28697717247391.45\n",
      "Gradient Descent(32/49): loss=85889397949724.03\n",
      "Gradient Descent(33/49): loss=257058379123757.7\n",
      "Gradient Descent(34/49): loss=769350022879548.5\n",
      "Gradient Descent(35/49): loss=2302587683476045.5\n",
      "Gradient Descent(36/49): loss=6891414677876069.0\n",
      "Gradient Descent(37/49): loss=2.0625314989415356e+16\n",
      "Gradient Descent(38/49): loss=6.172950523181966e+16\n",
      "Gradient Descent(39/49): loss=1.8475023620830538e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=5.5293898194785274e+17\n",
      "Gradient Descent(41/49): loss=1.654891079071781e+18\n",
      "Gradient Descent(42/49): loss=4.952923510554183e+18\n",
      "Gradient Descent(43/49): loss=1.482360477473721e+19\n",
      "Gradient Descent(44/49): loss=4.436556673031228e+19\n",
      "Gradient Descent(45/49): loss=1.3278170466713996e+20\n",
      "Gradient Descent(46/49): loss=3.9740236389831705e+20\n",
      "Gradient Descent(47/49): loss=1.1893855349112455e+21\n",
      "Gradient Descent(48/49): loss=3.559711967435905e+21\n",
      "Gradient Descent(49/49): loss=1.0653861947339327e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5984964518720001\n",
      "Gradient Descent(2/49): loss=0.8932864826796837\n",
      "Gradient Descent(3/49): loss=1.77556356588396\n",
      "Gradient Descent(4/49): loss=4.416130648205623\n",
      "Gradient Descent(5/49): loss=12.319083868885855\n",
      "Gradient Descent(6/49): loss=35.97183256305825\n",
      "Gradient Descent(7/49): loss=106.76214412984963\n",
      "Gradient Descent(8/49): loss=318.63046761809073\n",
      "Gradient Descent(9/49): loss=952.7311729860251\n",
      "Gradient Descent(10/49): loss=2850.531174081858\n",
      "Gradient Descent(11/49): loss=8530.456797360823\n",
      "Gradient Descent(12/49): loss=25529.906195274143\n",
      "Gradient Descent(13/49): loss=76407.5582982816\n",
      "Gradient Descent(14/49): loss=228679.2832773896\n",
      "Gradient Descent(15/49): loss=684413.3289672727\n",
      "Gradient Descent(16/49): loss=2048379.7543126992\n",
      "Gradient Descent(17/49): loss=6130594.868728495\n",
      "Gradient Descent(18/49): loss=18348256.484664056\n",
      "Gradient Descent(19/49): loss=54914495.934998445\n",
      "Gradient Descent(20/49): loss=164353593.98590317\n",
      "Gradient Descent(21/49): loss=491893870.5424494\n",
      "Gradient Descent(22/49): loss=1472189164.2484825\n",
      "Gradient Descent(23/49): loss=4406114948.780967\n",
      "Gradient Descent(24/49): loss=13187061429.308144\n",
      "Gradient Descent(25/49): loss=39467556150.8759\n",
      "Gradient Descent(26/49): loss=118122448803.06956\n",
      "Gradient Descent(27/49): loss=353528677021.79663\n",
      "Gradient Descent(28/49): loss=1058075977457.7435\n",
      "Gradient Descent(29/49): loss=3166715592932.062\n",
      "Gradient Descent(30/49): loss=9477663098084.605\n",
      "Gradient Descent(31/49): loss=28365697886259.125\n",
      "Gradient Descent(32/49): loss=84895697203780.67\n",
      "Gradient Descent(33/49): loss=254084332161218.5\n",
      "Gradient Descent(34/49): loss=760448997725369.1\n",
      "Gradient Descent(35/49): loss=2275947805292045.5\n",
      "Gradient Descent(36/49): loss=6811684186459111.0\n",
      "Gradient Descent(37/49): loss=2.0386689601652052e+16\n",
      "Gradient Descent(38/49): loss=6.10153233087838e+16\n",
      "Gradient Descent(39/49): loss=1.8261276113084784e+17\n",
      "Gradient Descent(40/49): loss=5.465417327884953e+17\n",
      "Gradient Descent(41/49): loss=1.635744752062586e+18\n",
      "Gradient Descent(42/49): loss=4.895620468447581e+18\n",
      "Gradient Descent(43/49): loss=1.4652102500015616e+19\n",
      "Gradient Descent(44/49): loss=4.385227757229501e+19\n",
      "Gradient Descent(45/49): loss=1.3124548154612346e+20\n",
      "Gradient Descent(46/49): loss=3.928046017194161e+20\n",
      "Gradient Descent(47/49): loss=1.1756248924860311e+21\n",
      "Gradient Descent(48/49): loss=3.518527740721741e+21\n",
      "Gradient Descent(49/49): loss=1.0530601675206276e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5993000911884802\n",
      "Gradient Descent(2/49): loss=0.8964953341064464\n",
      "Gradient Descent(3/49): loss=1.785970976635775\n",
      "Gradient Descent(4/49): loss=4.448082627161616\n",
      "Gradient Descent(5/49): loss=12.41551658602\n",
      "Gradient Descent(6/49): loss=36.26124968148328\n",
      "Gradient Descent(7/49): loss=107.62914426289393\n",
      "Gradient Descent(8/49): loss=321.2261159556006\n",
      "Gradient Descent(9/49): loss=960.5004925346834\n",
      "Gradient Descent(10/49): loss=2873.7847741985\n",
      "Gradient Descent(11/49): loss=8600.053300789297\n",
      "Gradient Descent(12/49): loss=25738.202374023327\n",
      "Gradient Descent(13/49): loss=77030.96873530398\n",
      "Gradient Descent(14/49): loss=230545.08917795928\n",
      "Gradient Descent(15/49): loss=689997.5002508807\n",
      "Gradient Descent(16/49): loss=2065092.621350886\n",
      "Gradient Descent(17/49): loss=6180614.80929097\n",
      "Gradient Descent(18/49): loss=18497961.165576193\n",
      "Gradient Descent(19/49): loss=55362547.07529946\n",
      "Gradient Descent(20/49): loss=165694566.24449372\n",
      "Gradient Descent(21/49): loss=495907266.4159876\n",
      "Gradient Descent(22/49): loss=1484200856.7591681\n",
      "Gradient Descent(23/49): loss=4442064743.297356\n",
      "Gradient Descent(24/49): loss=13294655569.318176\n",
      "Gradient Descent(25/49): loss=39789574652.51241\n",
      "Gradient Descent(26/49): loss=119086217976.60474\n",
      "Gradient Descent(27/49): loss=356413141781.322\n",
      "Gradient Descent(28/49): loss=1066708892036.4156\n",
      "Gradient Descent(29/49): loss=3192553042974.7446\n",
      "Gradient Descent(30/49): loss=9554992002319.188\n",
      "Gradient Descent(31/49): loss=28597135563737.67\n",
      "Gradient Descent(32/49): loss=85588367028707.89\n",
      "Gradient Descent(33/49): loss=256157423680226.06\n",
      "Gradient Descent(34/49): loss=766653553332619.4\n",
      "Gradient Descent(35/49): loss=2294517419769286.5\n",
      "Gradient Descent(36/49): loss=6867261185627160.0\n",
      "Gradient Descent(37/49): loss=2.055302600246303e+16\n",
      "Gradient Descent(38/49): loss=6.151315152277233e+16\n",
      "Gradient Descent(39/49): loss=1.8410271119252125e+17\n",
      "Gradient Descent(40/49): loss=5.5100100432808045e+17\n",
      "Gradient Descent(41/49): loss=1.6490909058534628e+18\n",
      "Gradient Descent(42/49): loss=4.935564172128809e+18\n",
      "Gradient Descent(43/49): loss=1.4771650010764612e+19\n",
      "Gradient Descent(44/49): loss=4.4210071317222195e+19\n",
      "Gradient Descent(45/49): loss=1.323163224453109e+20\n",
      "Gradient Descent(46/49): loss=3.960095214465832e+20\n",
      "Gradient Descent(47/49): loss=1.1852168967375107e+21\n",
      "Gradient Descent(48/49): loss=3.547235650245913e+21\n",
      "Gradient Descent(49/49): loss=1.0616521577621638e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6023178954036564\n",
      "Gradient Descent(2/49): loss=0.9192578082059929\n",
      "Gradient Descent(3/49): loss=1.9010108821024612\n",
      "Gradient Descent(4/49): loss=4.942089203804134\n",
      "Gradient Descent(5/49): loss=14.362133413107756\n",
      "Gradient Descent(6/49): loss=43.54166235585019\n",
      "Gradient Descent(7/49): loss=133.92817120887662\n",
      "Gradient Descent(8/49): loss=413.90942103203565\n",
      "Gradient Descent(9/49): loss=1281.1793404842708\n",
      "Gradient Descent(10/49): loss=3967.634642979311\n",
      "Gradient Descent(11/49): loss=12289.19858798914\n",
      "Gradient Descent(12/49): loss=38066.07506404785\n",
      "Gradient Descent(13/49): loss=117912.5276362863\n",
      "Gradient Descent(14/49): loss=365244.89912409254\n",
      "Gradient Descent(15/49): loss=1131381.6530446357\n",
      "Gradient Descent(16/49): loss=3504566.8619891275\n",
      "Gradient Descent(17/49): loss=10855745.365216304\n",
      "Gradient Descent(18/49): loss=33626755.89681122\n",
      "Gradient Descent(19/49): loss=104162238.11947487\n",
      "Gradient Descent(20/49): loss=322652947.852403\n",
      "Gradient Descent(21/49): loss=999449770.3211539\n",
      "Gradient Descent(22/49): loss=3095895607.60006\n",
      "Gradient Descent(23/49): loss=9589846233.156519\n",
      "Gradient Descent(24/49): loss=29705507690.879684\n",
      "Gradient Descent(25/49): loss=92015780622.3231\n",
      "Gradient Descent(26/49): loss=285028082054.77814\n",
      "Gradient Descent(27/49): loss=882902986971.9934\n",
      "Gradient Descent(28/49): loss=2734880292443.195\n",
      "Gradient Descent(29/49): loss=8471565193871.272\n",
      "Gradient Descent(30/49): loss=26241520344535.79\n",
      "Gradient Descent(31/49): loss=81285733419239.7\n",
      "Gradient Descent(32/49): loss=251790687839443.28\n",
      "Gradient Descent(33/49): loss=779946834651421.1\n",
      "Gradient Descent(34/49): loss=2415963315016114.0\n",
      "Gradient Descent(35/49): loss=7483687964594728.0\n",
      "Gradient Descent(36/49): loss=2.318147183913018e+16\n",
      "Gradient Descent(37/49): loss=7.180692716889474e+16\n",
      "Gradient Descent(38/49): loss=2.2242913759837597e+17\n",
      "Gradient Descent(39/49): loss=6.889964966247716e+17\n",
      "Gradient Descent(40/49): loss=2.1342355479447462e+18\n",
      "Gradient Descent(41/49): loss=6.61100803331358e+18\n",
      "Gradient Descent(42/49): loss=2.0478258483992412e+19\n",
      "Gradient Descent(43/49): loss=6.3433453480014004e+19\n",
      "Gradient Descent(44/49): loss=1.9649146549968824e+20\n",
      "Gradient Descent(45/49): loss=6.086519635318038e+20\n",
      "Gradient Descent(46/49): loss=1.8853603222360212e+21\n",
      "Gradient Descent(47/49): loss=5.840092134157819e+21\n",
      "Gradient Descent(48/49): loss=1.809026939476768e+22\n",
      "Gradient Descent(49/49): loss=5.603641847722827e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6048845773845846\n",
      "Gradient Descent(2/49): loss=0.9297750442911148\n",
      "Gradient Descent(3/49): loss=1.9361557545808055\n",
      "Gradient Descent(4/49): loss=5.053520642774357\n",
      "Gradient Descent(5/49): loss=14.709870120441835\n",
      "Gradient Descent(6/49): loss=44.62137826246876\n",
      "Gradient Descent(7/49): loss=137.2752658832188\n",
      "Gradient Descent(8/49): loss=424.2799481772049\n",
      "Gradient Descent(9/49): loss=1313.3056520510515\n",
      "Gradient Descent(10/49): loss=4067.1516723708237\n",
      "Gradient Descent(11/49): loss=12597.465104912328\n",
      "Gradient Descent(12/49): loss=39020.96399355652\n",
      "Gradient Descent(13/49): loss=120870.39415102146\n",
      "Gradient Descent(14/49): loss=374407.18900674756\n",
      "Gradient Descent(15/49): loss=1159762.7647518248\n",
      "Gradient Descent(16/49): loss=3592480.196180028\n",
      "Gradient Descent(17/49): loss=11128065.71177125\n",
      "Gradient Descent(18/49): loss=34470295.40487038\n",
      "Gradient Descent(19/49): loss=106775186.10220373\n",
      "Gradient Descent(20/49): loss=330746815.5262723\n",
      "Gradient Descent(21/49): loss=1024521334.8302727\n",
      "Gradient Descent(22/49): loss=3173557285.826132\n",
      "Gradient Descent(23/49): loss=9830411047.631409\n",
      "Gradient Descent(24/49): loss=30450681260.200775\n",
      "Gradient Descent(25/49): loss=94324030270.65709\n",
      "Gradient Descent(26/49): loss=292178116165.45374\n",
      "Gradient Descent(27/49): loss=905050932633.1619\n",
      "Gradient Descent(28/49): loss=2803485768923.7383\n",
      "Gradient Descent(29/49): loss=8684077517817.232\n",
      "Gradient Descent(30/49): loss=26899798519187.004\n",
      "Gradient Descent(31/49): loss=83324815893026.56\n",
      "Gradient Descent(32/49): loss=258106949710250.56\n",
      "Gradient Descent(33/49): loss=799512087422489.5\n",
      "Gradient Descent(34/49): loss=2476568641999822.0\n",
      "Gradient Descent(35/49): loss=7671419025458316.0\n",
      "Gradient Descent(36/49): loss=2.3762987573261172e+16\n",
      "Gradient Descent(37/49): loss=7.360823030693318e+16\n",
      "Gradient Descent(38/49): loss=2.2800885419876666e+17\n",
      "Gradient Descent(39/49): loss=7.062802267660499e+17\n",
      "Gradient Descent(40/49): loss=2.1877736304306179e+18\n",
      "Gradient Descent(41/49): loss=6.776847597622391e+18\n",
      "Gradient Descent(42/49): loss=2.0991963118394925e+19\n",
      "Gradient Descent(43/49): loss=6.502470495553864e+19\n",
      "Gradient Descent(44/49): loss=2.0142052607029048e+20\n",
      "Gradient Descent(45/49): loss=6.239202215553022e+20\n",
      "Gradient Descent(46/49): loss=1.9326552782897052e+21\n",
      "Gradient Descent(47/49): loss=5.986592990030335e+21\n",
      "Gradient Descent(48/49): loss=1.8544070445918826e+22\n",
      "Gradient Descent(49/49): loss=5.744211261327175e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6036711111680001\n",
      "Gradient Descent(2/49): loss=0.9248027451220201\n",
      "Gradient Descent(3/49): loss=1.9195400944579746\n",
      "Gradient Descent(4/49): loss=5.000838507761431\n",
      "Gradient Descent(5/49): loss=14.545468472810702\n",
      "Gradient Descent(6/49): loss=44.110914252548774\n",
      "Gradient Descent(7/49): loss=135.69283909985265\n",
      "Gradient Descent(8/49): loss=419.377009506913\n",
      "Gradient Descent(9/49): loss=1298.1170957598526\n",
      "Gradient Descent(10/49): loss=4020.1023869369296\n",
      "Gradient Descent(11/49): loss=12451.72402488708\n",
      "Gradient Descent(12/49): loss=38569.515210599726\n",
      "Gradient Descent(13/49): loss=119471.98518747308\n",
      "Gradient Descent(14/49): loss=370075.47618784773\n",
      "Gradient Descent(15/49): loss=1146344.8499106562\n",
      "Gradient Descent(16/49): loss=3550916.861954453\n",
      "Gradient Descent(17/49): loss=10999319.126459934\n",
      "Gradient Descent(18/49): loss=34071489.98099558\n",
      "Gradient Descent(19/49): loss=105539846.41999938\n",
      "Gradient Descent(20/49): loss=326920227.3254249\n",
      "Gradient Descent(21/49): loss=1012668095.2181343\n",
      "Gradient Descent(22/49): loss=3136840690.8028893\n",
      "Gradient Descent(23/49): loss=9716677722.885183\n",
      "Gradient Descent(24/49): loss=30098380913.461308\n",
      "Gradient Descent(25/49): loss=93232744716.58604\n",
      "Gradient Descent(26/49): loss=288797750033.1505\n",
      "Gradient Descent(27/49): loss=894579910501.6532\n",
      "Gradient Descent(28/49): loss=2771050730769.2163\n",
      "Gradient Descent(29/49): loss=8583606743630.401\n",
      "Gradient Descent(30/49): loss=26588580249067.203\n",
      "Gradient Descent(31/49): loss=82360786179514.06\n",
      "Gradient Descent(32/49): loss=255120771269643.7\n",
      "Gradient Descent(33/49): loss=790262101084919.5\n",
      "Gradient Descent(34/49): loss=2447915884320786.0\n",
      "Gradient Descent(35/49): loss=7582664243271732.0\n",
      "Gradient Descent(36/49): loss=2.3488060759957884e+16\n",
      "Gradient Descent(37/49): loss=7.275661701004915e+16\n",
      "Gradient Descent(38/49): loss=2.2537089685030317e+17\n",
      "Gradient Descent(39/49): loss=6.981088900835452e+17\n",
      "Gradient Descent(40/49): loss=2.1624620979229448e+18\n",
      "Gradient Descent(41/49): loss=6.698442594526227e+18\n",
      "Gradient Descent(42/49): loss=2.0749095780803506e+19\n",
      "Gradient Descent(43/49): loss=6.42723990906241e+19\n",
      "Gradient Descent(44/49): loss=1.9909018342310887e+20\n",
      "Gradient Descent(45/49): loss=6.167017521714927e+20\n",
      "Gradient Descent(46/49): loss=1.9102953475264482e+21\n",
      "Gradient Descent(47/49): loss=5.917330868498005e+21\n",
      "Gradient Descent(48/49): loss=1.8329524098259627e+22\n",
      "Gradient Descent(49/49): loss=5.67775338467632e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6045169708851199\n",
      "Gradient Descent(2/49): loss=0.9282687398988817\n",
      "Gradient Descent(3/49): loss=1.9311222195959663\n",
      "Gradient Descent(4/49): loss=5.0375611583057385\n",
      "Gradient Descent(5/49): loss=14.66006641485285\n",
      "Gradient Descent(6/49): loss=44.46673869753652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=136.79588676037025\n",
      "Gradient Descent(8/49): loss=422.79465579983497\n",
      "Gradient Descent(9/49): loss=1308.7044427764563\n",
      "Gradient Descent(10/49): loss=4052.8985989149887\n",
      "Gradient Descent(11/49): loss=12553.314416970434\n",
      "Gradient Descent(12/49): loss=38884.20245497442\n",
      "Gradient Descent(13/49): loss=120446.76124150782\n",
      "Gradient Descent(14/49): loss=373094.9433386993\n",
      "Gradient Descent(15/49): loss=1155697.9522029455\n",
      "Gradient Descent(16/49): loss=3579889.0324610476\n",
      "Gradient Descent(17/49): loss=11089063.322669161\n",
      "Gradient Descent(18/49): loss=34349481.604015216\n",
      "Gradient Descent(19/49): loss=106400953.27231659\n",
      "Gradient Descent(20/49): loss=329587591.91207135\n",
      "Gradient Descent(21/49): loss=1020930523.7626036\n",
      "Gradient Descent(22/49): loss=3162434389.46275\n",
      "Gradient Descent(23/49): loss=9795956763.85574\n",
      "Gradient Descent(24/49): loss=30343955670.776108\n",
      "Gradient Descent(25/49): loss=93993437084.84457\n",
      "Gradient Descent(26/49): loss=291154070713.0831\n",
      "Gradient Descent(27/49): loss=901878849440.0029\n",
      "Gradient Descent(28/49): loss=2793659924024.1655\n",
      "Gradient Descent(29/49): loss=8653640980656.803\n",
      "Gradient Descent(30/49): loss=26805518301680.26\n",
      "Gradient Descent(31/49): loss=83032773491283.39\n",
      "Gradient Descent(32/49): loss=257202319166572.6\n",
      "Gradient Descent(33/49): loss=796709903850303.2\n",
      "Gradient Descent(34/49): loss=2467888598166439.0\n",
      "Gradient Descent(35/49): loss=7644531721681115.0\n",
      "Gradient Descent(36/49): loss=2.3679701461080456e+16\n",
      "Gradient Descent(37/49): loss=7.335024324584093e+16\n",
      "Gradient Descent(38/49): loss=2.2720971347830563e+17\n",
      "Gradient Descent(39/49): loss=7.038048084704187e+17\n",
      "Gradient Descent(40/49): loss=2.1801057747180278e+18\n",
      "Gradient Descent(41/49): loss=6.753095647766283e+18\n",
      "Gradient Descent(42/49): loss=2.091838907851996e+19\n",
      "Gradient Descent(43/49): loss=6.479680200962738e+19\n",
      "Gradient Descent(44/49): loss=2.0071457390503526e+20\n",
      "Gradient Descent(45/49): loss=6.217334641281886e+20\n",
      "Gradient Descent(46/49): loss=1.9258815784834092e+21\n",
      "Gradient Descent(47/49): loss=5.965610777509558e+21\n",
      "Gradient Descent(48/49): loss=1.8479075944412834e+22\n",
      "Gradient Descent(49/49): loss=5.7240785645413165e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6075128114317314\n",
      "Gradient Descent(2/49): loss=0.9519946105401915\n",
      "Gradient Descent(3/49): loss=2.0557487430633823\n",
      "Gradient Descent(4/49): loss=5.592287359081095\n",
      "Gradient Descent(5/49): loss=16.92371073866252\n",
      "Gradient Descent(6/49): loss=53.230724389178455\n",
      "Gradient Descent(7/49): loss=169.56202682680745\n",
      "Gradient Descent(8/49): loss=542.2991529671706\n",
      "Gradient Descent(9/49): loss=1736.5861788336076\n",
      "Gradient Descent(10/49): loss=5563.201238412502\n",
      "Gradient Descent(11/49): loss=17824.058550809277\n",
      "Gradient Descent(12/49): loss=57109.071465456815\n",
      "Gradient Descent(13/49): loss=182982.18134527327\n",
      "Gradient Descent(14/49): loss=586292.2127111604\n",
      "Gradient Descent(15/49): loss=1878537.8842106778\n",
      "Gradient Descent(16/49): loss=6019022.240262602\n",
      "Gradient Descent(17/49): loss=19285548.165489808\n",
      "Gradient Descent(18/49): loss=61792823.88250613\n",
      "Gradient Descent(19/49): loss=197990386.00740647\n",
      "Gradient Descent(20/49): loss=634380994.8118236\n",
      "Gradient Descent(21/49): loss=2032620144.4821312\n",
      "Gradient Descent(22/49): loss=6512718203.940897\n",
      "Gradient Descent(23/49): loss=20867400396.25283\n",
      "Gradient Descent(24/49): loss=66861237608.64207\n",
      "Gradient Descent(25/49): loss=214230091420.8672\n",
      "Gradient Descent(26/49): loss=686414635920.6813\n",
      "Gradient Descent(27/49): loss=2199341134952.6265\n",
      "Gradient Descent(28/49): loss=7046908930501.44\n",
      "Gradient Descent(29/49): loss=22579000904218.383\n",
      "Gradient Descent(30/49): loss=72345376797209.1\n",
      "Gradient Descent(31/49): loss=231801821795955.38\n",
      "Gradient Descent(32/49): loss=742716217216339.8\n",
      "Gradient Descent(33/49): loss=2379737031583034.0\n",
      "Gradient Descent(34/49): loss=7624915422894587.0\n",
      "Gradient Descent(35/49): loss=2.4430991506494584e+16\n",
      "Gradient Descent(36/49): loss=7.827933988595786e+16\n",
      "Gradient Descent(37/49): loss=2.5081483292860422e+17\n",
      "Gradient Descent(38/49): loss=8.036358061865123e+17\n",
      "Gradient Descent(39/49): loss=2.5749294866021396e+18\n",
      "Gradient Descent(40/49): loss=8.250331568022141e+18\n",
      "Gradient Descent(41/49): loss=2.6434887377098494e+19\n",
      "Gradient Descent(42/49): loss=8.470002264495995e+19\n",
      "Gradient Descent(43/49): loss=2.7138734255672037e+20\n",
      "Gradient Descent(44/49): loss=8.695521842859627e+20\n",
      "Gradient Descent(45/49): loss=2.7861321536707997e+21\n",
      "Gradient Descent(46/49): loss=8.927046033576944e+21\n",
      "Gradient Descent(47/49): loss=2.8603148196181073e+22\n",
      "Gradient Descent(48/49): loss=9.164734713538018e+22\n",
      "Gradient Descent(49/49): loss=2.9364726495645926e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6102098097889791\n",
      "Gradient Descent(2/49): loss=0.9633330613338588\n",
      "Gradient Descent(3/49): loss=2.094775271608866\n",
      "Gradient Descent(4/49): loss=5.720029257550684\n",
      "Gradient Descent(5/49): loss=17.335705553907026\n",
      "Gradient Descent(6/49): loss=54.55349397506011\n",
      "Gradient Descent(7/49): loss=173.80300985526893\n",
      "Gradient Descent(8/49): loss=555.8903836870801\n",
      "Gradient Descent(9/49): loss=1780.1365381816988\n",
      "Gradient Descent(10/49): loss=5702.74364179768\n",
      "Gradient Descent(11/49): loss=18271.169062491957\n",
      "Gradient Descent(12/49): loss=58541.66095293887\n",
      "Gradient Descent(13/49): loss=187572.34401911325\n",
      "Gradient Descent(14/49): loss=600999.5556314955\n",
      "Gradient Descent(15/49): loss=1925661.6843587395\n",
      "Gradient Descent(16/49): loss=6170011.611014246\n",
      "Gradient Descent(17/49): loss=19769333.211009413\n",
      "Gradient Descent(18/49): loss=63342919.549555436\n",
      "Gradient Descent(19/49): loss=202957047.53688192\n",
      "Gradient Descent(20/49): loss=650294675.0210458\n",
      "Gradient Descent(21/49): loss=2083609167.2431786\n",
      "Gradient Descent(22/49): loss=6676092131.772041\n",
      "Gradient Descent(23/49): loss=21390866798.418617\n",
      "Gradient Descent(24/49): loss=68538476307.81973\n",
      "Gradient Descent(25/49): loss=219604131936.88077\n",
      "Gradient Descent(26/49): loss=703633599137.9203\n",
      "Gradient Descent(27/49): loss=2254512414997.0483\n",
      "Gradient Descent(28/49): loss=7223683228891.038\n",
      "Gradient Descent(29/49): loss=23145403433691.266\n",
      "Gradient Descent(30/49): loss=74160187141893.45\n",
      "Gradient Descent(31/49): loss=237616655621350.66\n",
      "Gradient Descent(32/49): loss=761347526276417.9\n",
      "Gradient Descent(33/49): loss=2439433608942145.0\n",
      "Gradient Descent(34/49): loss=7816189226411049.0\n",
      "Gradient Descent(35/49): loss=2.5043851900344896e+16\n",
      "Gradient Descent(36/49): loss=8.024300587389808e+16\n",
      "Gradient Descent(37/49): loss=2.571066151205262e+17\n",
      "Gradient Descent(38/49): loss=8.237953055076475e+17\n",
      "Gradient Descent(39/49): loss=2.6395225383772826e+18\n",
      "Gradient Descent(40/49): loss=8.457294165213878e+18\n",
      "Gradient Descent(41/49): loss=2.7098016234764902e+19\n",
      "Gradient Descent(42/49): loss=8.682475381781478e+19\n",
      "Gradient Descent(43/49): loss=2.7819519370767198e+20\n",
      "Gradient Descent(44/49): loss=8.913652201587604e+20\n",
      "Gradient Descent(45/49): loss=2.8560233019104224e+21\n",
      "Gradient Descent(46/49): loss=9.150984261651189e+21\n",
      "Gradient Descent(47/49): loss=2.9320668672753563e+22\n",
      "Gradient Descent(48/49): loss=9.394635449436575e+22\n",
      "Gradient Descent(49/49): loss=3.0101351443539226e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6089347330880001\n",
      "Gradient Descent(2/49): loss=0.957972511375237\n",
      "Gradient Descent(3/49): loss=2.076324456785411\n",
      "Gradient Descent(4/49): loss=5.659635925073649\n",
      "Gradient Descent(5/49): loss=17.140924200616464\n",
      "Gradient Descent(6/49): loss=53.928119964283866\n",
      "Gradient Descent(7/49): loss=171.79797391063462\n",
      "Gradient Descent(8/49): loss=549.4647729401337\n",
      "Gradient Descent(9/49): loss=1759.5469637106069\n",
      "Gradient Descent(10/49): loss=5636.771311158816\n",
      "Gradient Descent(11/49): loss=18059.785842815832\n",
      "Gradient Descent(12/49): loss=57864.36670370416\n",
      "Gradient Descent(13/49): loss=185402.22424005932\n",
      "Gradient Descent(14/49): loss=594046.2735723741\n",
      "Gradient Descent(15/49): loss=1903382.6720378848\n",
      "Gradient Descent(16/49): loss=6098627.426361384\n",
      "Gradient Descent(17/49): loss=19540611.143688925\n",
      "Gradient Descent(18/49): loss=62610071.172380805\n",
      "Gradient Descent(19/49): loss=200608928.0503268\n",
      "Gradient Descent(20/49): loss=642771065.3729937\n",
      "Gradient Descent(21/49): loss=2059502769.5685873\n",
      "Gradient Descent(22/49): loss=6598852822.981932\n",
      "Gradient Descent(23/49): loss=21143384329.12468\n",
      "Gradient Descent(24/49): loss=67745517727.95657\n",
      "Gradient Descent(25/49): loss=217063413351.14755\n",
      "Gradient Descent(26/49): loss=695492882717.3977\n",
      "Gradient Descent(27/49): loss=2228428745513.8794\n",
      "Gradient Descent(28/49): loss=7140108543500.693\n",
      "Gradient Descent(29/49): loss=22877621784230.234\n",
      "Gradient Descent(30/49): loss=73302187958847.06\n",
      "Gradient Descent(31/49): loss=234867540438922.6\n",
      "Gradient Descent(32/49): loss=752539086320263.6\n",
      "Gradient Descent(33/49): loss=2411210486478695.5\n",
      "Gradient Descent(34/49): loss=7725759519726944.0\n",
      "Gradient Descent(35/49): loss=2.4754106077159004e+16\n",
      "Gradient Descent(36/49): loss=7.931463128182997e+16\n",
      "Gradient Descent(37/49): loss=2.541320100901206e+17\n",
      "Gradient Descent(38/49): loss=8.142643735297325e+17\n",
      "Gradient Descent(39/49): loss=2.6089844792268083e+18\n",
      "Gradient Descent(40/49): loss=8.359447169890072e+18\n",
      "Gradient Descent(41/49): loss=2.6784504677043937e+19\n",
      "Gradient Descent(42/49): loss=8.582023143571653e+19\n",
      "Gradient Descent(43/49): loss=2.7497660354316855e+20\n",
      "Gradient Descent(44/49): loss=8.810525354127116e+20\n",
      "Gradient Descent(45/49): loss=2.822980428715681e+21\n",
      "Gradient Descent(46/49): loss=9.045111591647302e+21\n",
      "Gradient Descent(47/49): loss=2.898144205079575e+22\n",
      "Gradient Descent(48/49): loss=9.28594384749476e+22\n",
      "Gradient Descent(49/49): loss=2.975309268175835e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.60982353905792\n",
      "Gradient Descent(2/49): loss=0.9617091405533524\n",
      "Gradient Descent(3/49): loss=2.0891857963047427\n",
      "Gradient Descent(4/49): loss=5.701733748998349\n",
      "Gradient Descent(5/49): loss=17.276698644224073\n",
      "Gradient Descent(6/49): loss=54.3640436650139\n",
      "Gradient Descent(7/49): loss=173.19560584611824\n",
      "Gradient Descent(8/49): loss=553.9438142305719\n",
      "Gradient Descent(9/49): loss=1773.8991487154008\n",
      "Gradient Descent(10/49): loss=5682.75803593815\n",
      "Gradient Descent(11/49): loss=18207.132796488913\n",
      "Gradient Descent(12/49): loss=58336.48196676888\n",
      "Gradient Descent(13/49): loss=186914.92964325543\n",
      "Gradient Descent(14/49): loss=598893.1338435528\n",
      "Gradient Descent(15/49): loss=1918912.4979217597\n",
      "Gradient Descent(16/49): loss=6148386.542364309\n",
      "Gradient Descent(17/49): loss=19700044.32816152\n",
      "Gradient Descent(18/49): loss=63120911.03963261\n",
      "Gradient Descent(19/49): loss=202245710.06988278\n",
      "Gradient Descent(20/49): loss=648015478.6426226\n",
      "Gradient Descent(21/49): loss=2076306394.1265604\n",
      "Gradient Descent(22/49): loss=6652693316.428979\n",
      "Gradient Descent(23/49): loss=21315894654.179405\n",
      "Gradient Descent(24/49): loss=68298258060.46568\n",
      "Gradient Descent(25/49): loss=218834448650.53964\n",
      "Gradient Descent(26/49): loss=701167456920.1619\n",
      "Gradient Descent(27/49): loss=2246610648716.806\n",
      "Gradient Descent(28/49): loss=7198365179552.746\n",
      "Gradient Descent(29/49): loss=23064281871802.0\n",
      "Gradient Descent(30/49): loss=73900265545443.3\n",
      "Gradient Descent(31/49): loss=236783840834151.03\n",
      "Gradient Descent(32/49): loss=758679104416714.2\n",
      "Gradient Descent(33/49): loss=2430883718461777.0\n",
      "Gradient Descent(34/49): loss=7788794522322919.0\n",
      "Gradient Descent(35/49): loss=2.4956076528975108e+16\n",
      "Gradient Descent(36/49): loss=7.996176480649266e+16\n",
      "Gradient Descent(37/49): loss=2.5620549061647776e+17\n",
      "Gradient Descent(38/49): loss=8.209080124841779e+17\n",
      "Gradient Descent(39/49): loss=2.6302713628003553e+18\n",
      "Gradient Descent(40/49): loss=8.427652473548299e+18\n",
      "Gradient Descent(41/49): loss=2.7003041290497565e+19\n",
      "Gradient Descent(42/49): loss=8.652044459887963e+19\n",
      "Gradient Descent(43/49): loss=2.7722015653926958e+20\n",
      "Gradient Descent(44/49): loss=8.882411035675663e+20\n",
      "Gradient Descent(45/49): loss=2.846013319940813e+21\n",
      "Gradient Descent(46/49): loss=9.118911278421328e+21\n",
      "Gradient Descent(47/49): loss=2.921790362719034e+22\n",
      "Gradient Descent(48/49): loss=9.361708501187456e+22\n",
      "Gradient Descent(49/49): loss=2.9995850208655593e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6127955288574635\n",
      "Gradient Descent(2/49): loss=0.9864194386449411\n",
      "Gradient Descent(3/49): loss=2.2240112774247804\n",
      "Gradient Descent(4/49): loss=6.323410484198895\n",
      "Gradient Descent(5/49): loss=19.902260416718708\n",
      "Gradient Descent(6/49): loss=64.8808429331984\n",
      "Gradient Descent(7/49): loss=213.86789966079377\n",
      "Gradient Descent(8/49): loss=707.3726263653252\n",
      "Gradient Descent(9/49): loss=2342.0576831015337\n",
      "Gradient Descent(10/49): loss=7756.788465033799\n",
      "Gradient Descent(11/49): loss=25692.542707105888\n",
      "Gradient Descent(12/49): loss=85102.9350585456\n",
      "Gradient Descent(13/49): loss=281893.9186834431\n",
      "Gradient Descent(14/49): loss=933744.3728426344\n",
      "Gradient Descent(15/49): loss=3092933.817199266\n",
      "Gradient Descent(16/49): loss=10245032.932685647\n",
      "Gradient Descent(17/49): loss=33935646.04282127\n",
      "Gradient Descent(18/49): loss=112408432.9088448\n",
      "Gradient Descent(19/49): loss=372341692.1238485\n",
      "Gradient Descent(20/49): loss=1233344619.9476874\n",
      "Gradient Descent(21/49): loss=4085330718.0715694\n",
      "Gradient Descent(22/49): loss=13532249469.496134\n",
      "Gradient Descent(23/49): loss=44824223141.71428\n",
      "Gradient Descent(24/49): loss=148475756733.56204\n",
      "Gradient Descent(25/49): loss=491811096603.23126\n",
      "Gradient Descent(26/49): loss=1629075076387.3982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=5396148283024.358\n",
      "Gradient Descent(28/49): loss=17874201572689.887\n",
      "Gradient Descent(29/49): loss=59206505289373.52\n",
      "Gradient Descent(30/49): loss=196115628120511.84\n",
      "Gradient Descent(31/49): loss=649613406586437.1\n",
      "Gradient Descent(32/49): loss=2151779447976736.0\n",
      "Gradient Descent(33/49): loss=7127554243478860.0\n",
      "Gradient Descent(34/49): loss=2.3609310676101364e+16\n",
      "Gradient Descent(35/49): loss=7.820348068351845e+16\n",
      "Gradient Descent(36/49): loss=2.5904120941608173e+17\n",
      "Gradient Descent(37/49): loss=8.580481020698701e+17\n",
      "Gradient Descent(38/49): loss=2.8421985332964465e+18\n",
      "Gradient Descent(39/49): loss=9.41449842169085e+18\n",
      "Gradient Descent(40/49): loss=3.1184584572006273e+19\n",
      "Gradient Descent(41/49): loss=1.032958179363178e+20\n",
      "Gradient Descent(42/49): loss=3.421570673322757e+20\n",
      "Gradient Descent(43/49): loss=1.1333610698313309e+21\n",
      "Gradient Descent(44/49): loss=3.7541452077094493e+21\n",
      "Gradient Descent(45/49): loss=1.243523058601694e+22\n",
      "Gradient Descent(46/49): loss=4.1190457793120016e+22\n",
      "Gradient Descent(47/49): loss=1.3643927239393842e+23\n",
      "Gradient Descent(48/49): loss=4.519414458776717e+23\n",
      "Gradient Descent(49/49): loss=1.497010845325175e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6156250461213356\n",
      "Gradient Descent(2/49): loss=0.9986214488936057\n",
      "Gradient Descent(3/49): loss=2.2672587334366052\n",
      "Gradient Descent(4/49): loss=6.469492874756574\n",
      "Gradient Descent(5/49): loss=20.38897324446573\n",
      "Gradient Descent(6/49): loss=66.49586002108946\n",
      "Gradient Descent(7/49): loss=219.22031178000077\n",
      "Gradient Descent(8/49): loss=725.1047857862609\n",
      "Gradient Descent(9/49): loss=2400.796517484489\n",
      "Gradient Descent(10/49): loss=7951.357809561325\n",
      "Gradient Descent(11/49): loss=26337.037033435918\n",
      "Gradient Descent(12/49): loss=87237.76089459768\n",
      "Gradient Descent(13/49): loss=288965.31861229596\n",
      "Gradient Descent(14/49): loss=957167.6807964848\n",
      "Gradient Descent(15/49): loss=3170521.1852952424\n",
      "Gradient Descent(16/49): loss=10502033.333596606\n",
      "Gradient Descent(17/49): loss=34786934.1736319\n",
      "Gradient Descent(18/49): loss=115228239.71617556\n",
      "Gradient Descent(19/49): loss=381682020.1953045\n",
      "Gradient Descent(20/49): loss=1264283522.6543713\n",
      "Gradient Descent(21/49): loss=4187812739.399664\n",
      "Gradient Descent(22/49): loss=13871710916.94552\n",
      "Gradient Descent(23/49): loss=45948655240.24501\n",
      "Gradient Descent(24/49): loss=152200325616.73505\n",
      "Gradient Descent(25/49): loss=504148358571.8407\n",
      "Gradient Descent(26/49): loss=1669941022932.3557\n",
      "Gradient Descent(27/49): loss=5531512644360.491\n",
      "Gradient Descent(28/49): loss=18322582483179.01\n",
      "Gradient Descent(29/49): loss=60691722217285.09\n",
      "Gradient Descent(30/49): loss=201035260672510.03\n",
      "Gradient Descent(31/49): loss=665909197451567.9\n",
      "Gradient Descent(32/49): loss=2205757625638432.0\n",
      "Gradient Descent(33/49): loss=7306351559164204.0\n",
      "Gradient Descent(34/49): loss=2.4201558904576772e+16\n",
      "Gradient Descent(35/49): loss=8.016524371551968e+16\n",
      "Gradient Descent(36/49): loss=2.655393532832988e+17\n",
      "Gradient Descent(37/49): loss=8.7957255381563e+17\n",
      "Gradient Descent(38/49): loss=2.9134961272591365e+18\n",
      "Gradient Descent(39/49): loss=9.650664571933364e+18\n",
      "Gradient Descent(40/49): loss=3.196686132807188e+19\n",
      "Gradient Descent(41/49): loss=1.058870314631016e+20\n",
      "Gradient Descent(42/49): loss=3.507402030184013e+20\n",
      "Gradient Descent(43/49): loss=1.1617918484780763e+21\n",
      "Gradient Descent(44/49): loss=3.848319318898814e+21\n",
      "Gradient Descent(45/49): loss=1.2747172911921077e+22\n",
      "Gradient Descent(46/49): loss=4.22237355534435e+22\n",
      "Gradient Descent(47/49): loss=1.3986190164722921e+23\n",
      "Gradient Descent(48/49): loss=4.632785630163097e+23\n",
      "Gradient Descent(49/49): loss=1.5345639121351864e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.614287317632\n",
      "Gradient Descent(2/49): loss=0.9928526285562833\n",
      "Gradient Descent(3/49): loss=2.2468123644617726\n",
      "Gradient Descent(4/49): loss=6.4004285936752545\n",
      "Gradient Descent(5/49): loss=20.15886699132114\n",
      "Gradient Descent(6/49): loss=65.73231833968835\n",
      "Gradient Descent(7/49): loss=216.689818585998\n",
      "Gradient Descent(8/49): loss=716.7214424018491\n",
      "Gradient Descent(9/49): loss=2373.02619312938\n",
      "Gradient Descent(10/49): loss=7859.370049439701\n",
      "Gradient Descent(11/49): loss=26032.335439081933\n",
      "Gradient Descent(12/49): loss=86228.46599573351\n",
      "Gradient Descent(13/49): loss=285622.1288515808\n",
      "Gradient Descent(14/49): loss=946093.6976952911\n",
      "Gradient Descent(15/49): loss=3133839.722333433\n",
      "Gradient Descent(16/49): loss=10380529.654345248\n",
      "Gradient Descent(17/49): loss=34384465.385142684\n",
      "Gradient Descent(18/49): loss=113895102.09982558\n",
      "Gradient Descent(19/49): loss=377266135.153557\n",
      "Gradient Descent(20/49): loss=1249656345.0406582\n",
      "Gradient Descent(21/49): loss=4139361676.270651\n",
      "Gradient Descent(22/49): loss=13711221615.438328\n",
      "Gradient Descent(23/49): loss=45417050477.93502\n",
      "Gradient Descent(24/49): loss=150439438002.07736\n",
      "Gradient Descent(25/49): loss=498315594437.072\n",
      "Gradient Descent(26/49): loss=1650620575012.4094\n",
      "Gradient Descent(27/49): loss=5467515592670.407\n",
      "Gradient Descent(28/49): loss=18110598649161.8\n",
      "Gradient Descent(29/49): loss=59989546965486.66\n",
      "Gradient Descent(30/49): loss=198709375368494.38\n",
      "Gradient Descent(31/49): loss=658204934970679.4\n",
      "Gradient Descent(32/49): loss=2180238026597054.2\n",
      "Gradient Descent(33/49): loss=7221820439300761.0\n",
      "Gradient Descent(34/49): loss=2.392155802313833e+16\n",
      "Gradient Descent(35/49): loss=7.923776879583848e+16\n",
      "Gradient Descent(36/49): loss=2.6246718535935722e+17\n",
      "Gradient Descent(37/49): loss=8.693963047842588e+17\n",
      "Gradient Descent(38/49): loss=2.8797883199676073e+18\n",
      "Gradient Descent(39/49): loss=9.53901083105996e+18\n",
      "Gradient Descent(40/49): loss=3.1597019476801556e+19\n",
      "Gradient Descent(41/49): loss=1.0466196731495465e+20\n",
      "Gradient Descent(42/49): loss=3.4668230053403355e+20\n",
      "Gradient Descent(43/49): loss=1.1483504522889753e+21\n",
      "Gradient Descent(44/49): loss=3.803796038162244e+21\n",
      "Gradient Descent(45/49): loss=1.2599693996809924e+22\n",
      "Gradient Descent(46/49): loss=4.173522639502817e+22\n",
      "Gradient Descent(47/49): loss=1.3824376391089633e+23\n",
      "Gradient Descent(48/49): loss=4.5791864357844836e+23\n",
      "Gradient Descent(49/49): loss=1.516809714989159e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.61521979570688\n",
      "Gradient Descent(2/49): loss=0.9968738470064059\n",
      "Gradient Descent(3/49): loss=2.2610647265308885\n",
      "Gradient Descent(4/49): loss=6.448570595867378\n",
      "Gradient Descent(5/49): loss=20.319265037457143\n",
      "Gradient Descent(6/49): loss=66.26455330577154\n",
      "Gradient Descent(7/49): loss=218.4537261657518\n",
      "Gradient Descent(8/49): loss=722.5651423471785\n",
      "Gradient Descent(9/49): loss=2392.383797306268\n",
      "Gradient Descent(10/49): loss=7923.491109992621\n",
      "Gradient Descent(11/49): loss=26244.730972534257\n",
      "Gradient Descent(12/49): loss=86932.00589321552\n",
      "Gradient Descent(13/49): loss=287952.5353404761\n",
      "Gradient Descent(14/49): loss=953812.9370816492\n",
      "Gradient Descent(15/49): loss=3159408.9318091315\n",
      "Gradient Descent(16/49): loss=10465225.104744215\n",
      "Gradient Descent(17/49): loss=34665010.59597726\n",
      "Gradient Descent(18/49): loss=114824380.05713865\n",
      "Gradient Descent(19/49): loss=380344275.4603149\n",
      "Gradient Descent(20/49): loss=1259852376.9937756\n",
      "Gradient Descent(21/49): loss=4173135012.5129094\n",
      "Gradient Descent(22/49): loss=13823092414.407843\n",
      "Gradient Descent(23/49): loss=45787611312.44562\n",
      "Gradient Descent(24/49): loss=151666883710.29556\n",
      "Gradient Descent(25/49): loss=502381385600.92365\n",
      "Gradient Descent(26/49): loss=1664088101663.5122\n",
      "Gradient Descent(27/49): loss=5512125427949.373\n",
      "Gradient Descent(28/49): loss=18258364267537.5\n",
      "Gradient Descent(29/49): loss=60479005799794.51\n",
      "Gradient Descent(30/49): loss=200330658811220.7\n",
      "Gradient Descent(31/49): loss=663575274246235.0\n",
      "Gradient Descent(32/49): loss=2198026738413461.8\n",
      "Gradient Descent(33/49): loss=7280743768320753.0\n",
      "Gradient Descent(34/49): loss=2.4116735658186268e+16\n",
      "Gradient Descent(35/49): loss=7.988427519417736e+16\n",
      "Gradient Descent(36/49): loss=2.646086731531881e+17\n",
      "Gradient Descent(37/49): loss=8.76489768952588e+17\n",
      "Gradient Descent(38/49): loss=2.9032847106783944e+18\n",
      "Gradient Descent(39/49): loss=9.616840275650404e+18\n",
      "Gradient Descent(40/49): loss=3.1854821729066746e+19\n",
      "Gradient Descent(41/49): loss=1.0551591149535596e+20\n",
      "Gradient Descent(42/49): loss=3.495109052372342e+20\n",
      "Gradient Descent(43/49): loss=1.157719922507884e+21\n",
      "Gradient Descent(44/49): loss=3.834831471315562e+21\n",
      "Gradient Descent(45/49): loss=1.2702495765585073e+22\n",
      "Gradient Descent(46/49): loss=4.2075746973927355e+22\n",
      "Gradient Descent(47/49): loss=1.393717042764245e+23\n",
      "Gradient Descent(48/49): loss=4.616548332452533e+23\n",
      "Gradient Descent(49/49): loss=1.5291854696415358e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6181660476808533\n",
      "Gradient Descent(2/49): loss=1.022589345868536\n",
      "Gradient Descent(3/49): loss=2.4067280839160676\n",
      "Gradient Descent(4/49): loss=7.14394291488307\n",
      "Gradient Descent(5/49): loss=23.357060673867256\n",
      "Gradient Descent(6/49): loss=78.84645620399039\n",
      "Gradient Descent(7/49): loss=268.75891240582087\n",
      "Gradient Descent(8/49): loss=918.7342937565568\n",
      "Gradient Descent(9/49): loss=3143.275036429271\n",
      "Gradient Descent(10/49): loss=10756.76572822737\n",
      "Gradient Descent(11/49): loss=36813.93762090909\n",
      "Gradient Descent(12/49): loss=125994.60842360859\n",
      "Gradient Descent(13/49): loss=431215.4542458841\n",
      "Gradient Descent(14/49): loss=1475833.7990725918\n",
      "Gradient Descent(15/49): loss=5051040.084241975\n",
      "Gradient Descent(16/49): loss=17287183.595235627\n",
      "Gradient Descent(17/49): loss=59165384.76160998\n",
      "Gradient Descent(18/49): loss=202493528.25354308\n",
      "Gradient Descent(19/49): loss=693034099.3546174\n",
      "Gradient Descent(20/49): loss=2371909203.9481606\n",
      "Gradient Descent(21/49): loss=8117859249.420003\n",
      "Gradient Descent(22/49): loss=27783373280.04666\n",
      "Gradient Descent(23/49): loss=95088595049.86752\n",
      "Gradient Descent(24/49): loss=325440716557.0926\n",
      "Gradient Descent(25/49): loss=1113820852415.589\n",
      "Gradient Descent(26/49): loss=3812051867391.705\n",
      "Gradient Descent(27/49): loss=13046747516147.24\n",
      "Gradient Descent(28/49): loss=44652493374007.336\n",
      "Gradient Descent(29/49): loss=152823158572537.53\n",
      "Gradient Descent(30/49): loss=523037260214449.2\n",
      "Gradient Descent(31/49): loss=1790095023083838.5\n",
      "Gradient Descent(32/49): loss=6126600216503966.0\n",
      "Gradient Descent(33/49): loss=2.0968289240985988e+16\n",
      "Gradient Descent(34/49): loss=7.176396992726617e+16\n",
      "Gradient Descent(35/49): loss=2.4561218707605066e+17\n",
      "Gradient Descent(36/49): loss=8.40607710267852e+17\n",
      "Gradient Descent(37/49): loss=2.8769798883919345e+18\n",
      "Gradient Descent(38/49): loss=9.846463668022454e+18\n",
      "Gradient Descent(39/49): loss=3.369952190380527e+19\n",
      "Gradient Descent(40/49): loss=1.1533661371577344e+20\n",
      "Gradient Descent(41/49): loss=3.947395604422377e+20\n",
      "Gradient Descent(42/49): loss=1.3509961456135727e+21\n",
      "Gradient Descent(43/49): loss=4.623784308362416e+21\n",
      "Gradient Descent(44/49): loss=1.582490179537224e+22\n",
      "Gradient Descent(45/49): loss=5.4160726394659316e+22\n",
      "Gradient Descent(46/49): loss=1.8536508608573265e+23\n",
      "Gradient Descent(47/49): loss=6.344120071284759e+23\n",
      "Gradient Descent(48/49): loss=2.171275094397087e+24\n",
      "Gradient Descent(49/49): loss=7.431189010573587e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6211302863816534\n",
      "Gradient Descent(2/49): loss=1.035698691522915\n",
      "Gradient Descent(3/49): loss=2.4545590581187544\n",
      "Gradient Descent(4/49): loss=7.310608662793133\n",
      "Gradient Descent(5/49): loss=23.93043843479228\n",
      "Gradient Descent(6/49): loss=80.81180582945674\n",
      "Gradient Descent(7/49): loss=275.4882857376872\n",
      "Gradient Descent(8/49): loss=941.7685382235945\n",
      "Gradient Descent(9/49): loss=3222.1127023564695\n",
      "Gradient Descent(10/49): loss=11026.590604100324\n",
      "Gradient Descent(11/49): loss=37737.41622282156\n",
      "Gradient Descent(12/49): loss=129155.21690290727\n",
      "Gradient Descent(13/49): loss=442032.63973045367\n",
      "Gradient Descent(14/49): loss=1512855.6193576152\n",
      "Gradient Descent(15/49): loss=5177747.2671317095\n",
      "Gradient Descent(16/49): loss=17720838.931638405\n",
      "Gradient Descent(17/49): loss=60649570.1534175\n",
      "Gradient Descent(18/49): loss=207573152.75995493\n",
      "Gradient Descent(19/49): loss=710419114.230873\n",
      "Gradient Descent(20/49): loss=2431409417.365193\n",
      "Gradient Descent(21/49): loss=8321498729.841605\n",
      "Gradient Descent(22/49): loss=28480329401.79064\n",
      "Gradient Descent(23/49): loss=97473927376.5431\n",
      "Gradient Descent(24/49): loss=333604516445.1118\n",
      "Gradient Descent(25/49): loss=1141761457532.2744\n",
      "Gradient Descent(26/49): loss=3907678588403.06\n",
      "Gradient Descent(27/49): loss=13374029968809.623\n",
      "Gradient Descent(28/49): loss=45772617568246.39\n",
      "Gradient Descent(29/49): loss=156656783627338.0\n",
      "Gradient Descent(30/49): loss=536157841964537.06\n",
      "Gradient Descent(31/49): loss=1835000214123571.0\n",
      "Gradient Descent(32/49): loss=6280288232837740.0\n",
      "Gradient Descent(33/49): loss=2.1494286476887372e+16\n",
      "Gradient Descent(34/49): loss=7.356419546715008e+16\n",
      "Gradient Descent(35/49): loss=2.5177345898630998e+17\n",
      "Gradient Descent(36/49): loss=8.616946633806595e+17\n",
      "Gradient Descent(37/49): loss=2.949149985420176e+18\n",
      "Gradient Descent(38/49): loss=1.0093465825100851e+19\n",
      "Gradient Descent(39/49): loss=3.454488678640964e+19\n",
      "Gradient Descent(40/49): loss=1.1822987502648413e+20\n",
      "Gradient Descent(41/49): loss=4.046417472781478e+20\n",
      "Gradient Descent(42/49): loss=1.3848863800595346e+21\n",
      "Gradient Descent(43/49): loss=4.739773635754189e+21\n",
      "Gradient Descent(44/49): loss=1.6221875268368124e+22\n",
      "Gradient Descent(45/49): loss=5.5519368105989736e+22\n",
      "Gradient Descent(46/49): loss=1.900150373427449e+23\n",
      "Gradient Descent(47/49): loss=6.503264653054958e+23\n",
      "Gradient Descent(48/49): loss=2.2257423275082642e+24\n",
      "Gradient Descent(49/49): loss=7.617603115896541e+24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6197288648\n",
      "Gradient Descent(2/49): loss=1.0295009045779895\n",
      "Gradient Descent(3/49): loss=2.4319457107180638\n",
      "Gradient Descent(4/49): loss=7.231813059732662\n",
      "Gradient Descent(5/49): loss=23.65935906173643\n",
      "Gradient Descent(6/49): loss=79.882635253597\n",
      "Gradient Descent(7/49): loss=272.3067980202014\n",
      "Gradient Descent(8/49): loss=930.878495088877\n",
      "Gradient Descent(9/49): loss=3184.840128306696\n",
      "Gradient Descent(10/49): loss=10899.023817993857\n",
      "Gradient Descent(11/49): loss=37300.817495950636\n",
      "Gradient Descent(12/49): loss=127660.95635874868\n",
      "Gradient Descent(13/49): loss=436918.5316166884\n",
      "Gradient Descent(14/49): loss=1495352.5829371074\n",
      "Gradient Descent(15/49): loss=5117843.123580833\n",
      "Gradient Descent(16/49): loss=17515816.998934437\n",
      "Gradient Descent(17/49): loss=59947882.587333374\n",
      "Gradient Descent(18/49): loss=205171627.06364113\n",
      "Gradient Descent(19/49): loss=702199892.533807\n",
      "Gradient Descent(20/49): loss=2403279131.105337\n",
      "Gradient Descent(21/49): loss=8225222825.116352\n",
      "Gradient Descent(22/49): loss=28150825117.87025\n",
      "Gradient Descent(23/49): loss=96346198964.82503\n",
      "Gradient Descent(24/49): loss=329744865956.063\n",
      "Gradient Descent(25/49): loss=1128551803733.581\n",
      "Gradient Descent(26/49): loss=3862468548277.445\n",
      "Gradient Descent(27/49): loss=13219298606479.582\n",
      "Gradient Descent(28/49): loss=45243049480673.98\n",
      "Gradient Descent(29/49): loss=154844336847606.94\n",
      "Gradient Descent(30/49): loss=529954742860929.5\n",
      "Gradient Descent(31/49): loss=1813770107441643.8\n",
      "Gradient Descent(32/49): loss=6207628192719012.0\n",
      "Gradient Descent(33/49): loss=2.124560748958208e+16\n",
      "Gradient Descent(34/49): loss=7.271309163309091e+16\n",
      "Gradient Descent(35/49): loss=2.488605561142809e+17\n",
      "Gradient Descent(36/49): loss=8.51725253301156e+17\n",
      "Gradient Descent(37/49): loss=2.9150296794230267e+18\n",
      "Gradient Descent(38/49): loss=9.976689077824627e+18\n",
      "Gradient Descent(39/49): loss=3.414521836885765e+19\n",
      "Gradient Descent(40/49): loss=1.168620098674266e+20\n",
      "Gradient Descent(41/49): loss=3.999602287712541e+20\n",
      "Gradient Descent(42/49): loss=1.3688638829697219e+21\n",
      "Gradient Descent(43/49): loss=4.6849366394635886e+21\n",
      "Gradient Descent(44/49): loss=1.6034195648562806e+22\n",
      "Gradient Descent(45/49): loss=5.4877034607210486e+22\n",
      "Gradient Descent(46/49): loss=1.8781665094317372e+23\n",
      "Gradient Descent(47/49): loss=6.428024878530583e+23\n",
      "Gradient Descent(48/49): loss=2.199991514676923e+24\n",
      "Gradient Descent(49/49): loss=7.529470958981528e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6207057408319998\n",
      "Gradient Descent(2/49): loss=1.033821138829503\n",
      "Gradient Descent(3/49): loss=2.4477085884759853\n",
      "Gradient Descent(4/49): loss=7.286738384890754\n",
      "Gradient Descent(5/49): loss=23.84831786312174\n",
      "Gradient Descent(6/49): loss=80.53032362737488\n",
      "Gradient Descent(7/49): loss=274.5244883555217\n",
      "Gradient Descent(8/49): loss=938.4695171375533\n",
      "Gradient Descent(9/49): loss=3210.82137814432\n",
      "Gradient Descent(10/49): loss=10987.945622439422\n",
      "Gradient Descent(11/49): loss=37605.153348541404\n",
      "Gradient Descent(12/49): loss=128702.54679111081\n",
      "Gradient Descent(13/49): loss=440483.3758483459\n",
      "Gradient Descent(14/49): loss=1507553.2632968142\n",
      "Gradient Descent(15/49): loss=5159599.95308855\n",
      "Gradient Descent(16/49): loss=17658729.748902775\n",
      "Gradient Descent(17/49): loss=60437001.47507452\n",
      "Gradient Descent(18/49): loss=206845636.45790815\n",
      "Gradient Descent(19/49): loss=707929189.6865971\n",
      "Gradient Descent(20/49): loss=2422887650.6119328\n",
      "Gradient Descent(21/49): loss=8292332983.1289215\n",
      "Gradient Descent(22/49): loss=28380509633.665398\n",
      "Gradient Descent(23/49): loss=97132294220.13364\n",
      "Gradient Descent(24/49): loss=332435276967.3146\n",
      "Gradient Descent(25/49): loss=1137759735419.5427\n",
      "Gradient Descent(26/49): loss=3893982694472.5654\n",
      "Gradient Descent(27/49): loss=13327155771832.143\n",
      "Gradient Descent(28/49): loss=45612190629089.87\n",
      "Gradient Descent(29/49): loss=156107722428065.28\n",
      "Gradient Descent(30/49): loss=534278680010078.6\n",
      "Gradient Descent(31/49): loss=1828568782334405.8\n",
      "Gradient Descent(32/49): loss=6258276657538814.0\n",
      "Gradient Descent(33/49): loss=2.141895186042666e+16\n",
      "Gradient Descent(34/49): loss=7.330636274231152e+16\n",
      "Gradient Descent(35/49): loss=2.5089102648557325e+17\n",
      "Gradient Descent(36/49): loss=8.586745381468973e+17\n",
      "Gradient Descent(37/49): loss=2.9388136068080195e+18\n",
      "Gradient Descent(38/49): loss=1.0058089569299894e+19\n",
      "Gradient Descent(39/49): loss=3.4423811550930883e+19\n",
      "Gradient Descent(40/49): loss=1.1781549503305862e+20\n",
      "Gradient Descent(41/49): loss=4.032235317506439e+20\n",
      "Gradient Descent(42/49): loss=1.380032537416656e+21\n",
      "Gradient Descent(43/49): loss=4.723161359308667e+21\n",
      "Gradient Descent(44/49): loss=1.6165019752233995e+22\n",
      "Gradient Descent(45/49): loss=5.532478010202552e+22\n",
      "Gradient Descent(46/49): loss=1.8934905989917413e+23\n",
      "Gradient Descent(47/49): loss=6.480471575048877e+23\n",
      "Gradient Descent(48/49): loss=2.217941396560489e+24\n",
      "Gradient Descent(49/49): loss=7.590904429727993e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6236243679019007\n",
      "Gradient Descent(2/49): loss=1.0605623338144328\n",
      "Gradient Descent(3/49): loss=2.6048758805357215\n",
      "Gradient Descent(4/49): loss=8.063097680067566\n",
      "Gradient Descent(5/49): loss=27.354636808332057\n",
      "Gradient Descent(6/49): loss=95.5386527032668\n",
      "Gradient Descent(7/49): loss=336.52823848232515\n",
      "Gradient Descent(8/49): loss=1188.281830459805\n",
      "Gradient Descent(9/49): loss=4198.719725945386\n",
      "Gradient Descent(10/49): loss=14838.811423748917\n",
      "Gradient Descent(11/49): loss=52445.15152046089\n",
      "Gradient Descent(12/49): loss=185360.9999582909\n",
      "Gradient Descent(13/49): loss=655138.7746769977\n",
      "Gradient Descent(14/49): loss=2315521.3416425926\n",
      "Gradient Descent(15/49): loss=8183977.486326418\n",
      "Gradient Descent(16/49): loss=28925448.88409891\n",
      "Gradient Descent(17/49): loss=102234105.39238173\n",
      "Gradient Descent(18/49): loss=361336220.9552535\n",
      "Gradient Descent(19/49): loss=1277106738.200591\n",
      "Gradient Descent(20/49): loss=4513806054.352516\n",
      "Gradient Descent(21/49): loss=15953596117.359943\n",
      "Gradient Descent(22/49): loss=56386390116.05578\n",
      "Gradient Descent(23/49): loss=199292057225.0277\n",
      "Gradient Descent(24/49): loss=704377847055.0109\n",
      "Gradient Descent(25/49): loss=2489553062629.907\n",
      "Gradient Descent(26/49): loss=8799076344558.723\n",
      "Gradient Descent(27/49): loss=31099455432208.875\n",
      "Gradient Descent(28/49): loss=109917915279601.17\n",
      "Gradient Descent(29/49): loss=388493879764188.8\n",
      "Gradient Descent(30/49): loss=1373092768638527.0\n",
      "Gradient Descent(31/49): loss=4853059081476069.0\n",
      "Gradient Descent(32/49): loss=1.7152652017569124e+16\n",
      "Gradient Descent(33/49): loss=6.062433329090003e+16\n",
      "Gradient Descent(34/49): loss=2.142706435833731e+17\n",
      "Gradient Descent(35/49): loss=7.57318162681123e+17\n",
      "Gradient Descent(36/49): loss=2.6766653141803187e+18\n",
      "Gradient Descent(37/49): loss=9.460405886439418e+18\n",
      "Gradient Descent(38/49): loss=3.343685856503264e+19\n",
      "Gradient Descent(39/49): loss=1.1817923291225847e+20\n",
      "Gradient Descent(40/49): loss=4.176926808050839e+20\n",
      "Gradient Descent(41/49): loss=1.4762930110374268e+21\n",
      "Gradient Descent(42/49): loss=5.217810018211085e+21\n",
      "Gradient Descent(43/49): loss=1.8441827728364157e+22\n",
      "Gradient Descent(44/49): loss=6.518079592313217e+22\n",
      "Gradient Descent(45/49): loss=2.3037500511072162e+23\n",
      "Gradient Descent(46/49): loss=8.142374180633834e+23\n",
      "Gradient Descent(47/49): loss=2.877840730403051e+24\n",
      "Gradient Descent(48/49): loss=1.017144027753738e+25\n",
      "Gradient Descent(49/49): loss=3.594993851693092e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6267255305699331\n",
      "Gradient Descent(2/49): loss=1.0746242458162558\n",
      "Gradient Descent(3/49): loss=2.657677464983025\n",
      "Gradient Descent(4/49): loss=8.252820762806026\n",
      "Gradient Descent(5/49): loss=28.02829523462984\n",
      "Gradient Descent(6/49): loss=97.92273220783754\n",
      "Gradient Descent(7/49): loss=344.9576302459316\n",
      "Gradient Descent(8/49): loss=1218.0777738718434\n",
      "Gradient Descent(9/49): loss=4304.033609503376\n",
      "Gradient Descent(10/49): loss=15211.035914958715\n",
      "Gradient Descent(11/49): loss=53760.74486336349\n",
      "Gradient Descent(12/49): loss=190010.83617059106\n",
      "Gradient Descent(13/49): loss=671573.1588869086\n",
      "Gradient Descent(14/49): loss=2373607.0322952694\n",
      "Gradient Descent(15/49): loss=8389275.554469159\n",
      "Gradient Descent(16/49): loss=29651054.379242312\n",
      "Gradient Descent(17/49): loss=104798685.45751622\n",
      "Gradient Descent(18/49): loss=370400472.74058217\n",
      "Gradient Descent(19/49): loss=1309143429.7139342\n",
      "Gradient Descent(20/49): loss=4627036536.839892\n",
      "Gradient Descent(21/49): loss=16353797934.666864\n",
      "Gradient Descent(22/49): loss=57800863419.151634\n",
      "Gradient Descent(23/49): loss=204291371667.50003\n",
      "Gradient Descent(24/49): loss=722047424020.5433\n",
      "Gradient Descent(25/49): loss=2552004415456.767\n",
      "Gradient Descent(26/49): loss=9019804405989.383\n",
      "Gradient Descent(27/49): loss=31879596692528.727\n",
      "Gradient Descent(28/49): loss=112675246550066.81\n",
      "Gradient Descent(29/49): loss=398239391406600.06\n",
      "Gradient Descent(30/49): loss=1407537304987522.5\n",
      "Gradient Descent(31/49): loss=4974799850748045.0\n",
      "Gradient Descent(32/49): loss=1.7582932592482194e+16\n",
      "Gradient Descent(33/49): loss=6.214511695486389e+16\n",
      "Gradient Descent(34/49): loss=2.1964570136526147e+17\n",
      "Gradient Descent(35/49): loss=7.763157669054524e+17\n",
      "Gradient Descent(36/49): loss=2.7438104465509043e+18\n",
      "Gradient Descent(37/49): loss=9.697723642290244e+18\n",
      "Gradient Descent(38/49): loss=3.4275634441307025e+19\n",
      "Gradient Descent(39/49): loss=1.2114380236936258e+20\n",
      "Gradient Descent(40/49): loss=4.281706550942606e+20\n",
      "Gradient Descent(41/49): loss=1.5133263633651364e+21\n",
      "Gradient Descent(42/49): loss=5.348700698677126e+21\n",
      "Gradient Descent(43/49): loss=1.890444774940529e+22\n",
      "Gradient Descent(44/49): loss=6.681588012550224e+22\n",
      "Gradient Descent(45/49): loss=2.3615404671558837e+23\n",
      "Gradient Descent(46/49): loss=8.346628627115114e+23\n",
      "Gradient Descent(47/49): loss=2.950032421967736e+24\n",
      "Gradient Descent(48/49): loss=1.0426594592201607e+25\n",
      "Gradient Descent(49/49): loss=3.6851755926675283e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.625259374592\n",
      "Gradient Descent(2/49): loss=1.0679761081499848\n",
      "Gradient Descent(3/49): loss=2.632714131237177\n",
      "Gradient Descent(4/49): loss=8.163124200036986\n",
      "Gradient Descent(5/49): loss=27.70980554720157\n",
      "Gradient Descent(6/49): loss=96.79559610063271\n",
      "Gradient Descent(7/49): loss=340.97241423269975\n",
      "Gradient Descent(8/49): loss=1203.9909602385912\n",
      "Gradient Descent(9/49): loss=4254.2437092418795\n",
      "Gradient Descent(10/49): loss=15035.05702531746\n",
      "Gradient Descent(11/49): loss=53138.76360965916\n",
      "Gradient Descent(12/49): loss=187812.50416133396\n",
      "Gradient Descent(13/49): loss=663803.3727671694\n",
      "Gradient Descent(14/49): loss=2346145.4987674546\n",
      "Gradient Descent(15/49): loss=8292215.5089025\n",
      "Gradient Descent(16/49): loss=29308005.352725558\n",
      "Gradient Descent(17/49): loss=103586212.97673126\n",
      "Gradient Descent(18/49): loss=366115110.00300705\n",
      "Gradient Descent(19/49): loss=1293997243.6525788\n",
      "Gradient Descent(20/49): loss=4573503856.824046\n",
      "Gradient Descent(21/49): loss=16164592030.415949\n",
      "Gradient Descent(22/49): loss=57132134071.155136\n",
      "Gradient Descent(23/49): loss=201927814659.94934\n",
      "Gradient Descent(24/49): loss=713693668132.9211\n",
      "Gradient Descent(25/49): loss=2522478900647.721\n",
      "Gradient Descent(26/49): loss=8915449426447.223\n",
      "Gradient Descent(27/49): loss=31510764452832.926\n",
      "Gradient Descent(28/49): loss=111371645882101.53\n",
      "Gradient Descent(29/49): loss=393631945205737.56\n",
      "Gradient Descent(30/49): loss=1391252747135225.5\n",
      "Gradient Descent(31/49): loss=4917243709474355.0\n",
      "Gradient Descent(32/49): loss=1.7379506166768342e+16\n",
      "Gradient Descent(33/49): loss=6.142612659582577e+16\n",
      "Gradient Descent(34/49): loss=2.171045018402779e+17\n",
      "Gradient Descent(35/49): loss=7.673341513043037e+17\n",
      "Gradient Descent(36/49): loss=2.712065824370101e+18\n",
      "Gradient Descent(37/49): loss=9.585525449653578e+18\n",
      "Gradient Descent(38/49): loss=3.387908114925705e+19\n",
      "Gradient Descent(39/49): loss=1.1974222441394661e+20\n",
      "Gradient Descent(40/49): loss=4.232169179686573e+20\n",
      "Gradient Descent(41/49): loss=1.4958178748685458e+21\n",
      "Gradient Descent(42/49): loss=5.286818696935313e+21\n",
      "Gradient Descent(43/49): loss=1.8685732002446707e+22\n",
      "Gradient Descent(44/49): loss=6.604285118945019e+22\n",
      "Gradient Descent(45/49): loss=2.3342185324398306e+23\n",
      "Gradient Descent(46/49): loss=8.250061981054698e+23\n",
      "Gradient Descent(47/49): loss=2.915901906584198e+24\n",
      "Gradient Descent(48/49): loss=1.030596369863193e+25\n",
      "Gradient Descent(49/49): loss=3.642539809644693e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6262813744332799\n",
      "Gradient Descent(2/49): loss=1.072610264230315\n",
      "Gradient Descent(3/49): loss=2.650115092329099\n",
      "Gradient Descent(4/49): loss=8.225648156761354\n",
      "Gradient Descent(5/49): loss=27.931812219692326\n",
      "Gradient Descent(6/49): loss=97.58127848370707\n",
      "Gradient Descent(7/49): loss=343.75035204726055\n",
      "Gradient Descent(8/49): loss=1213.8103256502795\n",
      "Gradient Descent(9/49): loss=4288.950296352515\n",
      "Gradient Descent(10/49): loss=15157.725008803734\n",
      "Gradient Descent(11/49): loss=53572.32235249411\n",
      "Gradient Descent(12/49): loss=189344.87520403948\n",
      "Gradient Descent(13/49): loss=669219.3860024934\n",
      "Gradient Descent(14/49): loss=2365287.8569688136\n",
      "Gradient Descent(15/49): loss=8359872.2607515445\n",
      "Gradient Descent(16/49): loss=29547131.377483223\n",
      "Gradient Descent(17/49): loss=104431379.99966566\n",
      "Gradient Descent(18/49): loss=369102268.3298796\n",
      "Gradient Descent(19/49): loss=1304555056.044282\n",
      "Gradient Descent(20/49): loss=4610819388.942061\n",
      "Gradient Descent(21/49): loss=16296480047.137814\n",
      "Gradient Descent(22/49): loss=57598279077.46763\n",
      "Gradient Descent(23/49): loss=203575357570.2837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=719516743795.2367\n",
      "Gradient Descent(25/49): loss=2543059979268.8213\n",
      "Gradient Descent(26/49): loss=8988191190727.098\n",
      "Gradient Descent(27/49): loss=31767862944505.95\n",
      "Gradient Descent(28/49): loss=112280334791052.02\n",
      "Gradient Descent(29/49): loss=396843615285501.06\n",
      "Gradient Descent(30/49): loss=1402604073865095.2\n",
      "Gradient Descent(31/49): loss=4957363838668897.0\n",
      "Gradient Descent(32/49): loss=1.7521306751392288e+16\n",
      "Gradient Descent(33/49): loss=6.192730658212122e+16\n",
      "Gradient Descent(34/49): loss=2.188758723838247e+17\n",
      "Gradient Descent(35/49): loss=7.735948833534373e+17\n",
      "Gradient Descent(36/49): loss=2.734193755724548e+18\n",
      "Gradient Descent(37/49): loss=9.663734410232318e+18\n",
      "Gradient Descent(38/49): loss=3.415550289952748e+19\n",
      "Gradient Descent(39/49): loss=1.2071920944809966e+20\n",
      "Gradient Descent(40/49): loss=4.266699738733706e+20\n",
      "Gradient Descent(41/49): loss=1.5080223556579416e+21\n",
      "Gradient Descent(42/49): loss=5.329954213837418e+21\n",
      "Gradient Descent(43/49): loss=1.8838190173387286e+22\n",
      "Gradient Descent(44/49): loss=6.658169934881573e+22\n",
      "Gradient Descent(45/49): loss=2.3532635817844613e+23\n",
      "Gradient Descent(46/49): loss=8.317374803458175e+23\n",
      "Gradient Descent(47/49): loss=2.939692950534482e+24\n",
      "Gradient Descent(48/49): loss=1.039005076436874e+25\n",
      "Gradient Descent(49/49): loss=3.672259542158348e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6291704895206056\n",
      "Gradient Descent(2/49): loss=1.1003973523406805\n",
      "Gradient Descent(3/49): loss=2.8194800705947602\n",
      "Gradient Descent(4/49): loss=9.090865735057049\n",
      "Gradient Descent(5/49): loss=31.969507777580862\n",
      "Gradient Descent(6/49): loss=115.43308181290271\n",
      "Gradient Descent(7/49): loss=419.9165462511593\n",
      "Gradient Descent(8/49): loss=1530.7026728682838\n",
      "Gradient Descent(9/49): loss=5582.961541380334\n",
      "Gradient Descent(10/49): loss=20366.00711959825\n",
      "Gradient Descent(11/49): loss=74296.03569349146\n",
      "Gradient Descent(12/49): loss=271038.17293392366\n",
      "Gradient Descent(13/49): loss=988773.1638008155\n",
      "Gradient Descent(14/49): loss=3607142.183982388\n",
      "Gradient Descent(15/49): loss=13159214.206506541\n",
      "Gradient Descent(16/49): loss=48006128.15187391\n",
      "Gradient Descent(17/49): loss=175131154.91598022\n",
      "Gradient Descent(18/49): loss=638895965.0540795\n",
      "Gradient Descent(19/49): loss=2330756368.918794\n",
      "Gradient Descent(20/49): loss=8502832308.2581\n",
      "Gradient Descent(21/49): loss=31019182542.56295\n",
      "Gradient Descent(22/49): loss=113161079832.33284\n",
      "Gradient Descent(23/49): loss=412822935335.1464\n",
      "Gradient Descent(24/49): loss=1506019350394.9595\n",
      "Gradient Descent(25/49): loss=5494109192174.632\n",
      "Gradient Descent(26/49): loss=20043059743971.223\n",
      "Gradient Descent(27/49): loss=73119086251976.45\n",
      "Gradient Descent(28/49): loss=266745738555861.2\n",
      "Gradient Descent(29/49): loss=973115128825575.9\n",
      "Gradient Descent(30/49): loss=3550021301468866.5\n",
      "Gradient Descent(31/49): loss=1.2950832709887656e+16\n",
      "Gradient Descent(32/49): loss=4.724593280894447e+16\n",
      "Gradient Descent(33/49): loss=1.7235788748031293e+17\n",
      "Gradient Descent(34/49): loss=6.287788093168695e+17\n",
      "Gradient Descent(35/49): loss=2.2938479742687424e+18\n",
      "Gradient Descent(36/49): loss=8.368186794929369e+18\n",
      "Gradient Descent(37/49): loss=3.0527982246583763e+19\n",
      "Gradient Descent(38/49): loss=1.1136913203376605e+20\n",
      "Gradient Descent(39/49): loss=4.062857305724233e+20\n",
      "Gradient Descent(40/49): loss=1.4821709737012473e+21\n",
      "Gradient Descent(41/49): loss=5.40710792915994e+21\n",
      "Gradient Descent(42/49): loss=1.9725670436369364e+22\n",
      "Gradient Descent(43/49): loss=7.196121831892092e+22\n",
      "Gradient Descent(44/49): loss=2.6252172054927386e+23\n",
      "Gradient Descent(45/49): loss=9.577054887357569e+23\n",
      "Gradient Descent(46/49): loss=3.493805393457214e+24\n",
      "Gradient Descent(47/49): loss=1.2745751455871722e+25\n",
      "Gradient Descent(48/49): loss=4.649777588616378e+25\n",
      "Gradient Descent(49/49): loss=1.696285362103072e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.632410778686174\n",
      "Gradient Descent(2/49): loss=1.1154585404112543\n",
      "Gradient Descent(3/49): loss=2.8776650799606776\n",
      "Gradient Descent(4/49): loss=9.306370756891125\n",
      "Gradient Descent(5/49): loss=32.75893193689747\n",
      "Gradient Descent(6/49): loss=118.31622037766995\n",
      "Gradient Descent(7/49): loss=430.43776433849973\n",
      "Gradient Descent(8/49): loss=1569.0883688618787\n",
      "Gradient Descent(9/49): loss=5722.999639223157\n",
      "Gradient Descent(10/49): loss=20876.883344628677\n",
      "Gradient Descent(11/49): loss=76159.76649032564\n",
      "Gradient Descent(12/49): loss=277837.2524941384\n",
      "Gradient Descent(13/49): loss=1013576.8891845697\n",
      "Gradient Descent(14/49): loss=3697628.6577948555\n",
      "Gradient Descent(15/49): loss=13489317.91486275\n",
      "Gradient Descent(16/49): loss=49210379.49357437\n",
      "Gradient Descent(17/49): loss=179524384.23888287\n",
      "Gradient Descent(18/49): loss=654922904.9501909\n",
      "Gradient Descent(19/49): loss=2389224248.357418\n",
      "Gradient Descent(20/49): loss=8716128979.24075\n",
      "Gradient Descent(21/49): loss=31797310127.97812\n",
      "Gradient Descent(22/49): loss=115999767076.67848\n",
      "Gradient Descent(23/49): loss=423178750271.2704\n",
      "Gradient Descent(24/49): loss=1543798398863.5396\n",
      "Gradient Descent(25/49): loss=5631930938892.811\n",
      "Gradient Descent(26/49): loss=20545847258174.152\n",
      "Gradient Descent(27/49): loss=74953305382541.66\n",
      "Gradient Descent(28/49): loss=273437153366031.25\n",
      "Gradient Descent(29/49): loss=997526079194535.5\n",
      "Gradient Descent(30/49): loss=3639074889509450.0\n",
      "Gradient Descent(31/49): loss=1.3275709104420906e+16\n",
      "Gradient Descent(32/49): loss=4.843111438383424e+16\n",
      "Gradient Descent(33/49): loss=1.7668154838365613e+17\n",
      "Gradient Descent(34/49): loss=6.445519566583859e+17\n",
      "Gradient Descent(35/49): loss=2.3513899930852275e+18\n",
      "Gradient Descent(36/49): loss=8.578105833773572e+18\n",
      "Gradient Descent(37/49): loss=3.1293787892192027e+19\n",
      "Gradient Descent(38/49): loss=1.1416286760950019e+20\n",
      "Gradient Descent(39/49): loss=4.1647755732621394e+20\n",
      "Gradient Descent(40/49): loss=1.5193517768816578e+21\n",
      "Gradient Descent(41/49): loss=5.542747217241382e+21\n",
      "Gradient Descent(42/49): loss=2.0220496123218765e+22\n",
      "Gradient Descent(43/49): loss=7.376639190711449e+22\n",
      "Gradient Descent(44/49): loss=2.6910717431632837e+23\n",
      "Gradient Descent(45/49): loss=9.817298826233353e+23\n",
      "Gradient Descent(46/49): loss=3.581448784798002e+24\n",
      "Gradient Descent(47/49): loss=1.3065483311821882e+25\n",
      "Gradient Descent(48/49): loss=4.766418966985549e+25\n",
      "Gradient Descent(49/49): loss=1.738837303345852e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6308788470080002\n",
      "Gradient Descent(2/49): loss=1.1083379687778383\n",
      "Gradient Descent(3/49): loss=2.850156590906271\n",
      "Gradient Descent(4/49): loss=9.204485106293133\n",
      "Gradient Descent(5/49): loss=32.38571096327298\n",
      "Gradient Descent(6/49): loss=116.95314101212054\n",
      "Gradient Descent(7/49): loss=425.46358257330934\n",
      "Gradient Descent(8/49): loss=1550.9405244327122\n",
      "Gradient Descent(9/49): loss=5656.79295602989\n",
      "Gradient Descent(10/49): loss=20635.353211739002\n",
      "Gradient Descent(11/49): loss=75278.63888059127\n",
      "Gradient Descent(12/49): loss=274622.80932912347\n",
      "Gradient Descent(13/49): loss=1001850.2775424554\n",
      "Gradient Descent(14/49): loss=3654848.80433168\n",
      "Gradient Descent(15/49): loss=13333252.729910206\n",
      "Gradient Descent(16/49): loss=48641038.090817936\n",
      "Gradient Descent(17/49): loss=177447369.86593503\n",
      "Gradient Descent(18/49): loss=647345748.814726\n",
      "Gradient Descent(19/49): loss=2361582025.057984\n",
      "Gradient Descent(20/49): loss=8615287384.420025\n",
      "Gradient Descent(21/49): loss=31429429905.912407\n",
      "Gradient Descent(22/49): loss=114657703238.56873\n",
      "Gradient Descent(23/49): loss=418282767183.40375\n",
      "Gradient Descent(24/49): loss=1525937362960.4927\n",
      "Gradient Descent(25/49): loss=5566772093814.779\n",
      "Gradient Descent(26/49): loss=20308141275443.53\n",
      "Gradient Descent(27/49): loss=74086130186946.62\n",
      "Gradient Descent(28/49): loss=270273611535029.8\n",
      "Gradient Descent(29/49): loss=985985162241033.0\n",
      "Gradient Descent(30/49): loss=3596972470371262.5\n",
      "Gradient Descent(31/49): loss=1.3122115269161538e+16\n",
      "Gradient Descent(32/49): loss=4.787078871342485e+16\n",
      "Gradient Descent(33/49): loss=1.7463742430545978e+17\n",
      "Gradient Descent(34/49): loss=6.37094787608705e+17\n",
      "Gradient Descent(35/49): loss=2.324185494675206e+18\n",
      "Gradient Descent(36/49): loss=8.478861103123966e+18\n",
      "Gradient Descent(37/49): loss=3.0931733190307e+19\n",
      "Gradient Descent(38/49): loss=1.1284205585155775e+20\n",
      "Gradient Descent(39/49): loss=4.116591039520806e+20\n",
      "Gradient Descent(40/49): loss=1.501773577127463e+21\n",
      "Gradient Descent(41/49): loss=5.478620186718774e+21\n",
      "Gradient Descent(42/49): loss=1.9986554303166795e+22\n",
      "Gradient Descent(43/49): loss=7.291294875339176e+22\n",
      "Gradient Descent(44/49): loss=2.659937283472213e+23\n",
      "Gradient Descent(45/49): loss=9.703717203835418e+23\n",
      "Gradient Descent(46/49): loss=3.5400130731311163e+24\n",
      "Gradient Descent(47/49): loss=1.2914321692088634e+25\n",
      "Gradient Descent(48/49): loss=4.711273696490565e+25\n",
      "Gradient Descent(49/49): loss=1.7187197572166924e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6319466965107201\n",
      "Gradient Descent(2/49): loss=1.1133014400514452\n",
      "Gradient Descent(3/49): loss=2.8693316799624426\n",
      "Gradient Descent(4/49): loss=9.275505598182185\n",
      "Gradient Descent(5/49): loss=32.645868669236776\n",
      "Gradient Descent(6/49): loss=117.90329018874571\n",
      "Gradient Descent(7/49): loss=428.9308896340868\n",
      "Gradient Descent(8/49): loss=1563.5906751706393\n",
      "Gradient Descent(9/49): loss=5702.943038786938\n",
      "Gradient Descent(10/49): loss=20803.714396495787\n",
      "Gradient Descent(11/49): loss=75892.83838655268\n",
      "Gradient Descent(12/49): loss=276863.4716146733\n",
      "Gradient Descent(13/49): loss=1010024.4386942075\n",
      "Gradient Descent(14/49): loss=3684668.9626967916\n",
      "Gradient Descent(15/49): loss=13442039.65070962\n",
      "Gradient Descent(16/49): loss=49037903.65765281\n",
      "Gradient Descent(17/49): loss=178895175.14136544\n",
      "Gradient Descent(18/49): loss=652627487.2411584\n",
      "Gradient Descent(19/49): loss=2380850335.0125165\n",
      "Gradient Descent(20/49): loss=8685580105.967203\n",
      "Gradient Descent(21/49): loss=31685864783.383247\n",
      "Gradient Descent(22/49): loss=115593203315.0621\n",
      "Gradient Descent(23/49): loss=421695565012.492\n",
      "Gradient Descent(24/49): loss=1538387590720.908\n",
      "Gradient Descent(25/49): loss=5612191769708.061\n",
      "Gradient Descent(26/49): loss=20473836795071.453\n",
      "Gradient Descent(27/49): loss=74690604012093.62\n",
      "Gradient Descent(28/49): loss=272478792496504.5\n",
      "Gradient Descent(29/49): loss=994029882906402.0\n",
      "Gradient Descent(30/49): loss=3626320415830711.5\n",
      "Gradient Descent(31/49): loss=1.3229179508993202e+16\n",
      "Gradient Descent(32/49): loss=4.826136976675979e+16\n",
      "Gradient Descent(33/49): loss=1.7606230304612368e+17\n",
      "Gradient Descent(34/49): loss=6.422928877425523e+17\n",
      "Gradient Descent(35/49): loss=2.3431486837734815e+18\n",
      "Gradient Descent(36/49): loss=8.548040713273295e+18\n",
      "Gradient Descent(37/49): loss=3.1184107326091457e+19\n",
      "Gradient Descent(38/49): loss=1.1376274193632667e+20\n",
      "Gradient Descent(39/49): loss=4.15017858857946e+20\n",
      "Gradient Descent(40/49): loss=1.5140266508995018e+21\n",
      "Gradient Descent(41/49): loss=5.523320625146439e+21\n",
      "Gradient Descent(42/49): loss=2.0149625972595465e+22\n",
      "Gradient Descent(43/49): loss=7.350785051063322e+22\n",
      "Gradient Descent(44/49): loss=2.6816398944782666e+23\n",
      "Gradient Descent(45/49): loss=9.782890499045751e+23\n",
      "Gradient Descent(46/49): loss=3.568896282956549e+24\n",
      "Gradient Descent(47/49): loss=1.3019690529854422e+25\n",
      "Gradient Descent(48/49): loss=4.7497133021963845e+25\n",
      "Gradient Descent(49/49): loss=1.7327429097740893e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6348044125369688\n",
      "Gradient Descent(2/49): loss=1.1421542995611544\n",
      "Gradient Descent(3/49): loss=3.0516163343652356\n",
      "Gradient Descent(4/49): loss=10.238067648553445\n",
      "Gradient Descent(5/49): loss=37.28499581463087\n",
      "Gradient Descent(6/49): loss=139.07881466047616\n",
      "Gradient Descent(7/49): loss=522.1900312687188\n",
      "Gradient Descent(8/49): loss=1964.0674060955089\n",
      "Gradient Descent(9/49): loss=7390.717093992897\n",
      "Gradient Descent(10/49): loss=27814.45585936432\n",
      "Gradient Descent(11/49): loss=104681.23907672738\n",
      "Gradient Descent(12/49): loss=393977.0643935582\n",
      "Gradient Descent(13/49): loss=1482770.8325560095\n",
      "Gradient Descent(14/49): loss=5580555.058412392\n",
      "Gradient Descent(15/49): loss=21002975.770845715\n",
      "Gradient Descent(16/49): loss=79046798.36415775\n",
      "Gradient Descent(17/49): loss=297500529.07632023\n",
      "Gradient Descent(18/49): loss=1119672989.9847534\n",
      "Gradient Descent(19/49): loss=4214001263.859948\n",
      "Gradient Descent(20/49): loss=15859815155.417109\n",
      "Gradient Descent(21/49): loss=59690000317.6859\n",
      "Gradient Descent(22/49): loss=224649285194.3792\n",
      "Gradient Descent(23/49): loss=845490049756.394\n",
      "Gradient Descent(24/49): loss=3182086351262.268\n",
      "Gradient Descent(25/49): loss=11976100191609.031\n",
      "Gradient Descent(26/49): loss=45073250681136.86\n",
      "Gradient Descent(27/49): loss=169637686263520.75\n",
      "Gradient Descent(28/49): loss=638448396021413.1\n",
      "Gradient Descent(29/49): loss=2402864383266184.5\n",
      "Gradient Descent(30/49): loss=9043420392860374.0\n",
      "Gradient Descent(31/49): loss=3.403581699056955e+16\n",
      "Gradient Descent(32/49): loss=1.2809720082570648e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=4.821066250276466e+17\n",
      "Gradient Descent(34/49): loss=1.8144564939538954e+18\n",
      "Gradient Descent(35/49): loss=6.828888460644255e+18\n",
      "Gradient Descent(36/49): loss=2.570120461048008e+19\n",
      "Gradient Descent(37/49): loss=9.672905367200729e+19\n",
      "Gradient Descent(38/49): loss=3.640494663999355e+20\n",
      "Gradient Descent(39/49): loss=1.3701365717428868e+21\n",
      "Gradient Descent(40/49): loss=5.156646001411695e+21\n",
      "Gradient Descent(41/49): loss=1.9407552890913914e+22\n",
      "Gradient Descent(42/49): loss=7.30422660602483e+22\n",
      "Gradient Descent(43/49): loss=2.7490187254432858e+23\n",
      "Gradient Descent(44/49): loss=1.0346206875077859e+24\n",
      "Gradient Descent(45/49): loss=3.893898419503885e+24\n",
      "Gradient Descent(46/49): loss=1.4655076091646254e+25\n",
      "Gradient Descent(47/49): loss=5.5155844378525495e+25\n",
      "Gradient Descent(48/49): loss=2.075845359030059e+26\n",
      "Gradient Descent(49/49): loss=7.812651593245695e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6381860307303765\n",
      "Gradient Descent(2/49): loss=1.158262975987222\n",
      "Gradient Descent(3/49): loss=3.1156245671562024\n",
      "Gradient Descent(4/49): loss=10.482350651678585\n",
      "Gradient Descent(5/49): loss=38.20776094338566\n",
      "Gradient Descent(6/49): loss=142.55511511726667\n",
      "Gradient Descent(7/49): loss=535.2768172860366\n",
      "Gradient Descent(8/49): loss=2013.3242155686617\n",
      "Gradient Descent(9/49): loss=7576.103403744862\n",
      "Gradient Descent(10/49): loss=28512.17915636455\n",
      "Gradient Descent(11/49): loss=107307.1938589179\n",
      "Gradient Descent(12/49): loss=403860.11119347217\n",
      "Gradient Descent(13/49): loss=1519966.6708737435\n",
      "Gradient Descent(14/49): loss=5720545.318886157\n",
      "Gradient Descent(15/49): loss=21529843.118548285\n",
      "Gradient Descent(16/49): loss=81029716.31735101\n",
      "Gradient Descent(17/49): loss=304963439.08835536\n",
      "Gradient Descent(18/49): loss=1147760398.1093926\n",
      "Gradient Descent(19/49): loss=4319711033.080941\n",
      "Gradient Descent(20/49): loss=16257664442.859138\n",
      "Gradient Descent(21/49): loss=61187345895.89861\n",
      "Gradient Descent(22/49): loss=230284695012.54156\n",
      "Gradient Descent(23/49): loss=866699478148.0524\n",
      "Gradient Descent(24/49): loss=3261910155956.5845\n",
      "Gradient Descent(25/49): loss=12276525062957.717\n",
      "Gradient Descent(26/49): loss=46203929726944.03\n",
      "Gradient Descent(27/49): loss=173893109920320.84\n",
      "Gradient Descent(28/49): loss=654464108496162.2\n",
      "Gradient Descent(29/49): loss=2463141118736082.0\n",
      "Gradient Descent(30/49): loss=9270277914475292.0\n",
      "Gradient Descent(31/49): loss=3.4889617958918516e+16\n",
      "Gradient Descent(32/49): loss=1.3131056615018318e+17\n",
      "Gradient Descent(33/49): loss=4.942004467628441e+17\n",
      "Gradient Descent(34/49): loss=1.8599728014366822e+18\n",
      "Gradient Descent(35/49): loss=7.000193635487506e+18\n",
      "Gradient Descent(36/49): loss=2.63459287665209e+19\n",
      "Gradient Descent(37/49): loss=9.91555375056849e+19\n",
      "Gradient Descent(38/49): loss=3.7318178095639075e+20\n",
      "Gradient Descent(39/49): loss=1.4045069508073716e+21\n",
      "Gradient Descent(40/49): loss=5.286002360058401e+21\n",
      "Gradient Descent(41/49): loss=1.9894398482314875e+22\n",
      "Gradient Descent(42/49): loss=7.487455812804397e+22\n",
      "Gradient Descent(43/49): loss=2.8179788697072383e+23\n",
      "Gradient Descent(44/49): loss=1.0605745274030623e+24\n",
      "Gradient Descent(45/49): loss=3.9915782913342814e+24\n",
      "Gradient Descent(46/49): loss=1.5022704057266709e+25\n",
      "Gradient Descent(47/49): loss=5.653944898993248e+25\n",
      "Gradient Descent(48/49): loss=2.127918702185197e+26\n",
      "Gradient Descent(49/49): loss=8.008634827544419e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.636587282048\n",
      "Gradient Descent(2/49): loss=1.1506471767638977\n",
      "Gradient Descent(3/49): loss=3.085362996516681\n",
      "Gradient Descent(4/49): loss=10.366859455738618\n",
      "Gradient Descent(5/49): loss=37.771499529668056\n",
      "Gradient Descent(6/49): loss=140.91160291189786\n",
      "Gradient Descent(7/49): loss=529.0896960012835\n",
      "Gradient Descent(8/49): loss=1990.0367671523295\n",
      "Gradient Descent(9/49): loss=7488.457164137076\n",
      "Gradient Descent(10/49): loss=28182.312170227233\n",
      "Gradient Descent(11/49): loss=106065.7048711463\n",
      "Gradient Descent(12/49): loss=399187.64164035744\n",
      "Gradient Descent(13/49): loss=1502381.3628650378\n",
      "Gradient Descent(14/49): loss=5654361.2520666635\n",
      "Gradient Descent(15/49): loss=21280752.76306379\n",
      "Gradient Descent(16/49): loss=80092239.85385436\n",
      "Gradient Descent(17/49): loss=301435152.6687604\n",
      "Gradient Descent(18/49): loss=1134481339.339056\n",
      "Gradient Descent(19/49): loss=4269733967.4907746\n",
      "Gradient Descent(20/49): loss=16069570758.802933\n",
      "Gradient Descent(21/49): loss=60479436506.590454\n",
      "Gradient Descent(22/49): loss=227620407234.9436\n",
      "Gradient Descent(23/49): loss=856672164668.2406\n",
      "Gradient Descent(24/49): loss=3224171358944.3066\n",
      "Gradient Descent(25/49): loss=12134491326522.08\n",
      "Gradient Descent(26/49): loss=45669371556499.55\n",
      "Gradient Descent(27/49): loss=171881246790030.72\n",
      "Gradient Descent(28/49): loss=646892260418908.1\n",
      "Gradient Descent(29/49): loss=2434643711312640.5\n",
      "Gradient Descent(30/49): loss=9163025071896752.0\n",
      "Gradient Descent(31/49): loss=3.448596116058897e+16\n",
      "Gradient Descent(32/49): loss=1.2979136342398291e+17\n",
      "Gradient Descent(33/49): loss=4.884827753825234e+17\n",
      "Gradient Descent(34/49): loss=1.8384537734296236e+18\n",
      "Gradient Descent(35/49): loss=6.919204621680241e+18\n",
      "Gradient Descent(36/49): loss=2.6041118514153607e+19\n",
      "Gradient Descent(37/49): loss=9.800835363986909e+19\n",
      "Gradient Descent(38/49): loss=3.688642397590196e+20\n",
      "Gradient Descent(39/49): loss=1.388257452756934e+21\n",
      "Gradient Descent(40/49): loss=5.224845749196142e+21\n",
      "Gradient Descent(41/49): loss=1.9664229461675383e+22\n",
      "Gradient Descent(42/49): loss=7.400829400196676e+22\n",
      "Gradient Descent(43/49): loss=2.7853761530578937e+23\n",
      "Gradient Descent(44/49): loss=1.0483041689648172e+24\n",
      "Gradient Descent(45/49): loss=3.945397570315771e+24\n",
      "Gradient Descent(46/49): loss=1.4848898295640082e+25\n",
      "Gradient Descent(47/49): loss=5.588531362547348e+25\n",
      "Gradient Descent(48/49): loss=2.103299663608196e+26\n",
      "Gradient Descent(49/49): loss=7.915978613955682e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6377017070643201\n",
      "Gradient Descent(2/49): loss=1.1559558517716406\n",
      "Gradient Descent(3/49): loss=3.106457150791773\n",
      "Gradient Descent(4/49): loss=10.44736383978355\n",
      "Gradient Descent(5/49): loss=38.07560025447543\n",
      "Gradient Descent(6/49): loss=142.05723082480483\n",
      "Gradient Descent(7/49): loss=533.402495639281\n",
      "Gradient Descent(8/49): loss=2006.269534295191\n",
      "Gradient Descent(9/49): loss=7549.551920980795\n",
      "Gradient Descent(10/49): loss=28412.249511507445\n",
      "Gradient Descent(11/49): loss=106931.09816320734\n",
      "Gradient Descent(12/49): loss=402444.6369487647\n",
      "Gradient Descent(13/49): loss=1514639.391521902\n",
      "Gradient Descent(14/49): loss=5700495.569833719\n",
      "Gradient Descent(15/49): loss=21454383.88252737\n",
      "Gradient Descent(16/49): loss=80745717.9361806\n",
      "Gradient Descent(17/49): loss=303894582.7805235\n",
      "Gradient Descent(18/49): loss=1143737650.5086346\n",
      "Gradient Descent(19/49): loss=4304571020.210386\n",
      "Gradient Descent(20/49): loss=16200683490.42075\n",
      "Gradient Descent(21/49): loss=60972892383.30125\n",
      "Gradient Descent(22/49): loss=229477577772.53595\n",
      "Gradient Descent(23/49): loss=863661811703.3981\n",
      "Gradient Descent(24/49): loss=3250477594525.409\n",
      "Gradient Descent(25/49): loss=12233497474753.564\n",
      "Gradient Descent(26/49): loss=46041991095979.72\n",
      "Gradient Descent(27/49): loss=173283637688838.47\n",
      "Gradient Descent(28/49): loss=652170298805689.9\n",
      "Gradient Descent(29/49): loss=2454508136585171.0\n",
      "Gradient Descent(30/49): loss=9237786822851088.0\n",
      "Gradient Descent(31/49): loss=3.4767334486484656e+16\n",
      "Gradient Descent(32/49): loss=1.308503400733468e+17\n",
      "Gradient Descent(33/49): loss=4.924683399000197e+17\n",
      "Gradient Descent(34/49): loss=1.8534538440477942e+18\n",
      "Gradient Descent(35/49): loss=6.975658887457717e+18\n",
      "Gradient Descent(36/49): loss=2.625358978883405e+19\n",
      "Gradient Descent(37/49): loss=9.880801052924872e+19\n",
      "Gradient Descent(38/49): loss=3.71873828427882e+20\n",
      "Gradient Descent(39/49): loss=1.3995843406712414e+21\n",
      "Gradient Descent(40/49): loss=5.267475624550074e+21\n",
      "Gradient Descent(41/49): loss=1.9824671260555705e+22\n",
      "Gradient Descent(42/49): loss=7.46121327562301e+22\n",
      "Gradient Descent(43/49): loss=2.8081022284132498e+23\n",
      "Gradient Descent(44/49): loss=1.0568573546855641e+24\n",
      "Gradient Descent(45/49): loss=3.9775883400947423e+24\n",
      "Gradient Descent(46/49): loss=1.4970051476780647e+25\n",
      "Gradient Descent(47/49): loss=5.634128573800871e+25\n",
      "Gradient Descent(48/49): loss=2.1204606300356418e+26\n",
      "Gradient Descent(49/49): loss=7.9805656272023e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6405261369509888\n",
      "Gradient Descent(2/49): loss=1.185894021844075\n",
      "Gradient Descent(3/49): loss=3.302412246325571\n",
      "Gradient Descent(4/49): loss=11.516407823714834\n",
      "Gradient Descent(5/49): loss=43.39410326000447\n",
      "Gradient Descent(6/49): loss=167.10825147869681\n",
      "Gradient Descent(7/49): loss=647.2304893006569\n",
      "Gradient Descent(8/49): loss=2510.536882064022\n",
      "Gradient Descent(9/49): loss=9741.842661740178\n",
      "Gradient Descent(10/49): loss=37805.8172620803\n",
      "Gradient Descent(11/49): loss=146719.2962885374\n",
      "Gradient Descent(12/49): loss=569401.6170423041\n",
      "Gradient Descent(13/49): loss=2209789.4356557303\n",
      "Gradient Descent(14/49): loss=8575970.520912103\n",
      "Gradient Descent(15/49): loss=33282482.694685865\n",
      "Gradient Descent(16/49): loss=129165985.78988688\n",
      "Gradient Descent(17/49): loss=501280272.95200866\n",
      "Gradient Descent(18/49): loss=1945418609.999485\n",
      "Gradient Descent(19/49): loss=7549975082.246353\n",
      "Gradient Descent(20/49): loss=29300698295.390846\n",
      "Gradient Descent(21/49): loss=113713080013.2729\n",
      "Gradient Descent(22/49): loss=441309092222.1976\n",
      "Gradient Descent(23/49): loss=1712676456003.8127\n",
      "Gradient Descent(24/49): loss=6646726058103.905\n",
      "Gradient Descent(25/49): loss=25795279158894.902\n",
      "Gradient Descent(26/49): loss=100108898887751.88\n",
      "Gradient Descent(27/49): loss=388512625693485.94\n",
      "Gradient Descent(28/49): loss=1507778649053852.0\n",
      "Gradient Descent(29/49): loss=5851538159113126.0\n",
      "Gradient Descent(30/49): loss=2.2709234441703364e+16\n",
      "Gradient Descent(31/49): loss=8.813226794480526e+16\n",
      "Gradient Descent(32/49): loss=3.420325186670012e+17\n",
      "Gradient Descent(33/49): loss=1.3273940016947548e+18\n",
      "Gradient Descent(34/49): loss=5.151483381177174e+18\n",
      "Gradient Descent(35/49): loss=1.9992391854010425e+19\n",
      "Gradient Descent(36/49): loss=7.758847354623299e+19\n",
      "Gradient Descent(37/49): loss=3.01113106985569e+20\n",
      "Gradient Descent(38/49): loss=1.1685898569003517e+21\n",
      "Gradient Descent(39/49): loss=4.535180375644935e+21\n",
      "Gradient Descent(40/49): loss=1.7600581519839685e+22\n",
      "Gradient Descent(41/49): loss=6.830609682034938e+22\n",
      "Gradient Descent(42/49): loss=2.6508913115007906e+23\n",
      "Gradient Descent(43/49): loss=1.0287844090802986e+24\n",
      "Gradient Descent(44/49): loss=3.9926094131993076e+24\n",
      "Gradient Descent(45/49): loss=1.5494917871683491e+25\n",
      "Gradient Descent(46/49): loss=6.013422676822002e+25\n",
      "Gradient Descent(47/49): loss=2.333749206648062e+26\n",
      "Gradient Descent(48/49): loss=9.057047296080333e+26\n",
      "Gradient Descent(49/49): loss=3.5149494851355014e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6440512867025407\n",
      "Gradient Descent(2/49): loss=1.2030999252663717\n",
      "Gradient Descent(3/49): loss=3.372711786669064\n",
      "Gradient Descent(4/49): loss=11.792758459587654\n",
      "Gradient Descent(5/49): loss=44.470117592515486\n",
      "Gradient Descent(6/49): loss=171.28768065150385\n",
      "Gradient Descent(7/49): loss=663.453961127085\n",
      "Gradient Descent(8/49): loss=2573.502079024771\n",
      "Gradient Descent(9/49): loss=9986.20781977346\n",
      "Gradient Descent(10/49): loss=38754.17752904791\n",
      "Gradient Descent(11/49): loss=150399.79117376162\n",
      "Gradient Descent(12/49): loss=583685.2531674979\n",
      "Gradient Descent(13/49): loss=2265222.8026189045\n",
      "Gradient Descent(14/49): loss=8791101.87828534\n",
      "Gradient Descent(15/49): loss=34117385.98304227\n",
      "Gradient Descent(16/49): loss=132406161.96519579\n",
      "Gradient Descent(17/49): loss=513855072.67435807\n",
      "Gradient Descent(18/49): loss=1994220150.2455134\n",
      "Gradient Descent(19/49): loss=7739368979.790623\n",
      "Gradient Descent(20/49): loss=30035717072.37183\n",
      "Gradient Descent(21/49): loss=116565614384.88014\n",
      "Gradient Descent(22/49): loss=452379492865.0078\n",
      "Gradient Descent(23/49): loss=1755639573858.4229\n",
      "Gradient Descent(24/49): loss=6813461622186.629\n",
      "Gradient Descent(25/49): loss=26442363209541.758\n",
      "Gradient Descent(26/49): loss=102620167379908.17\n",
      "Gradient Descent(27/49): loss=398258607584676.94\n",
      "Gradient Descent(28/49): loss=1545601830175357.2\n",
      "Gradient Descent(29/49): loss=5998326142727621.0\n",
      "Gradient Descent(30/49): loss=2.3278903927311344e+16\n",
      "Gradient Descent(31/49): loss=9.03430982514933e+16\n",
      "Gradient Descent(32/49): loss=3.506125300041865e+17\n",
      "Gradient Descent(33/49): loss=1.3606921676933128e+18\n",
      "Gradient Descent(34/49): loss=5.280710233601253e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=2.0493908345583387e+19\n",
      "Gradient Descent(36/49): loss=7.953480889837597e+19\n",
      "Gradient Descent(37/49): loss=3.086666398537024e+20\n",
      "Gradient Descent(38/49): loss=1.197904362608188e+21\n",
      "Gradient Descent(39/49): loss=4.6489470408457925e+21\n",
      "Gradient Descent(40/49): loss=1.8042098570817712e+22\n",
      "Gradient Descent(41/49): loss=7.00195803434803e+22\n",
      "Gradient Descent(42/49): loss=2.7173898935498213e+23\n",
      "Gradient Descent(43/49): loss=1.0545918437878621e+24\n",
      "Gradient Descent(44/49): loss=4.092765486556138e+24\n",
      "Gradient Descent(45/49): loss=1.588361357677405e+25\n",
      "Gradient Descent(46/49): loss=6.164271593010941e+25\n",
      "Gradient Descent(47/49): loss=2.3922921625313892e+26\n",
      "Gradient Descent(48/49): loss=9.284246653568e+26\n",
      "Gradient Descent(49/49): loss=3.6031232837832777e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.642384679712\n",
      "Gradient Descent(2/49): loss=1.1949653832062601\n",
      "Gradient Descent(3/49): loss=3.3394758353971814\n",
      "Gradient Descent(4/49): loss=11.66210644930437\n",
      "Gradient Descent(5/49): loss=43.96140359882145\n",
      "Gradient Descent(6/49): loss=169.31174590636664\n",
      "Gradient Descent(7/49): loss=655.7838893677737\n",
      "Gradient Descent(8/49): loss=2543.733630927222\n",
      "Gradient Descent(9/49): loss=9870.677782944846\n",
      "Gradient Descent(10/49): loss=38305.815342508475\n",
      "Gradient Descent(11/49): loss=148659.74069743138\n",
      "Gradient Descent(12/49): loss=576932.2896073225\n",
      "Gradient Descent(13/49): loss=2239015.224671931\n",
      "Gradient Descent(14/49): loss=8689392.887364086\n",
      "Gradient Descent(15/49): loss=33722663.558503866\n",
      "Gradient Descent(16/49): loss=130874283.70613492\n",
      "Gradient Descent(17/49): loss=507910006.337109\n",
      "Gradient Descent(18/49): loss=1971147942.2954853\n",
      "Gradient Descent(19/49): loss=7649828047.956085\n",
      "Gradient Descent(20/49): loss=29688217670.016373\n",
      "Gradient Descent(21/49): loss=115217003954.26633\n",
      "Gradient Descent(22/49): loss=447145670644.804\n",
      "Gradient Descent(23/49): loss=1735327633203.998\n",
      "Gradient Descent(24/49): loss=6734633011700.213\n",
      "Gradient Descent(25/49): loss=26136437255108.137\n",
      "Gradient Descent(26/49): loss=101432899343342.66\n",
      "Gradient Descent(27/49): loss=393650939061605.25\n",
      "Gradient Descent(28/49): loss=1527719929404006.5\n",
      "Gradient Descent(29/49): loss=5928928274023477.0\n",
      "Gradient Descent(30/49): loss=2.300957773865884e+16\n",
      "Gradient Descent(31/49): loss=8.92978702459632e+16\n",
      "Gradient Descent(32/49): loss=3.465561046375769e+17\n",
      "Gradient Descent(33/49): loss=1.3449495864880218e+18\n",
      "Gradient Descent(34/49): loss=5.219614850201182e+18\n",
      "Gradient Descent(35/49): loss=2.0256803272148034e+19\n",
      "Gradient Descent(36/49): loss=7.861462781888068e+19\n",
      "Gradient Descent(37/49): loss=3.050955091022764e+20\n",
      "Gradient Descent(38/49): loss=1.184045161274942e+21\n",
      "Gradient Descent(39/49): loss=4.595160866391517e+21\n",
      "Gradient Descent(40/49): loss=1.7833359806380003e+22\n",
      "Gradient Descent(41/49): loss=6.9209486072584685e+22\n",
      "Gradient Descent(42/49): loss=2.6859509449911054e+23\n",
      "Gradient Descent(43/49): loss=1.0423907022416131e+24\n",
      "Gradient Descent(44/49): loss=4.045414076329749e+24\n",
      "Gradient Descent(45/49): loss=1.5699847488827676e+25\n",
      "Gradient Descent(46/49): loss=6.092953811938744e+25\n",
      "Gradient Descent(47/49): loss=2.364614444875224e+26\n",
      "Gradient Descent(48/49): loss=9.176832199116318e+26\n",
      "Gradient Descent(49/49): loss=3.561436808155117e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6435464060940801\n",
      "Gradient Descent(2/49): loss=1.200635653504547\n",
      "Gradient Descent(3/49): loss=3.3626433137797545\n",
      "Gradient Descent(4/49): loss=11.75317884254146\n",
      "Gradient Descent(5/49): loss=44.316008176109555\n",
      "Gradient Descent(6/49): loss=170.6890925367481\n",
      "Gradient Descent(7/49): loss=661.130395632009\n",
      "Gradient Descent(8/49): loss=2564.4840488146033\n",
      "Gradient Descent(9/49): loss=9951.209241449746\n",
      "Gradient Descent(10/49): loss=38618.351041546826\n",
      "Gradient Descent(11/49): loss=149872.66165354662\n",
      "Gradient Descent(12/49): loss=581639.5157075938\n",
      "Gradient Descent(13/49): loss=2257283.4996061497\n",
      "Gradient Descent(14/49): loss=8760290.23671759\n",
      "Gradient Descent(15/49): loss=33997809.0827722\n",
      "Gradient Descent(16/49): loss=131942095.97242458\n",
      "Gradient Descent(17/49): loss=512054078.96243477\n",
      "Gradient Descent(18/49): loss=1987230673.7485094\n",
      "Gradient Descent(19/49): loss=7712243520.453017\n",
      "Gradient Descent(20/49): loss=29930445877.226772\n",
      "Gradient Descent(21/49): loss=116157067403.63875\n",
      "Gradient Descent(22/49): loss=450793962885.5104\n",
      "Gradient Descent(23/49): loss=1749486290561.0916\n",
      "Gradient Descent(24/49): loss=6789581345037.561\n",
      "Gradient Descent(25/49): loss=26349686241953.6\n",
      "Gradient Descent(26/49): loss=102260497336400.05\n",
      "Gradient Descent(27/49): loss=396862764112836.5\n",
      "Gradient Descent(28/49): loss=1540184701245438.5\n",
      "Gradient Descent(29/49): loss=5977302807063065.0\n",
      "Gradient Descent(30/49): loss=2.3197314463931516e+16\n",
      "Gradient Descent(31/49): loss=9.002645770307206e+16\n",
      "Gradient Descent(32/49): loss=3.493836796998651e+17\n",
      "Gradient Descent(33/49): loss=1.3559231225472594e+18\n",
      "Gradient Descent(34/49): loss=5.26220204629385e+18\n",
      "Gradient Descent(35/49): loss=2.04220799214628e+19\n",
      "Gradient Descent(36/49): loss=7.925604996720845e+19\n",
      "Gradient Descent(37/49): loss=3.075848043177262e+20\n",
      "Gradient Descent(38/49): loss=1.1937058670765697e+21\n",
      "Gradient Descent(39/49): loss=4.6326530995370727e+21\n",
      "Gradient Descent(40/49): loss=1.7978863413994254e+22\n",
      "Gradient Descent(41/49): loss=6.9774171023367465e+22\n",
      "Gradient Descent(42/49): loss=2.7078658032458062e+23\n",
      "Gradient Descent(43/49): loss=1.050895639581721e+24\n",
      "Gradient Descent(44/49): loss=4.0784208876528205e+24\n",
      "Gradient Descent(45/49): loss=1.582794362289228e+25\n",
      "Gradient Descent(46/49): loss=6.142666640608271e+25\n",
      "Gradient Descent(47/49): loss=2.3839074965534742e+26\n",
      "Gradient Descent(48/49): loss=9.251706603374365e+26\n",
      "Gradient Descent(49/49): loss=3.590494815703857e+27\n"
     ]
    }
   ],
   "source": [
    "max_iters = 50\n",
    "k_fold = 4\n",
    "initial_w = np.zeros(x.shape[1])\n",
    "gammas = np.arange(0, 3, 0.03)\n",
    "gamma_opt = cross_validation(y, x2, k_fold, gammas, fonction=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=0.5, w0=-0.037348050780443134, w1=-0.0892131248343261\n",
      "SGD(1/49): loss=0.5334786296601186, w0=-0.03688527826399847, w1=-0.1683343714900538\n",
      "SGD(2/49): loss=0.5666277427808751, w0=0.016073308438578675, w1=-0.06889369565858076\n",
      "SGD(3/49): loss=0.5205240833028542, w0=0.03710200199100139, w1=0.02180359228194688\n",
      "SGD(4/49): loss=0.48966832006871264, w0=0.06427849231935809, w1=0.10473294434706487\n",
      "SGD(5/49): loss=0.4679893792880534, w0=0.07211117833667495, w1=0.17941902723405947\n",
      "SGD(6/49): loss=0.45544922869280396, w0=0.10602379689792471, w1=0.24704105822688263\n",
      "SGD(7/49): loss=0.44727542482689847, w0=0.1413191089654544, w1=0.3113069704501027\n",
      "SGD(8/49): loss=0.44383260689406634, w0=0.12651207208113555, w1=0.19607840354468647\n",
      "SGD(9/49): loss=0.4503922329765159, w0=0.16456291735283687, w1=0.26390987755500905\n",
      "SGD(10/49): loss=0.4436091353985919, w0=0.11211471266205315, w1=0.15353581512444991\n",
      "SGD(11/49): loss=0.4566045380180913, w0=0.14204070343934774, w1=0.22608976004896775\n",
      "SGD(12/49): loss=0.4470054790641739, w0=0.10403757952570564, w1=0.1162476915582128\n",
      "SGD(13/49): loss=0.46357384108523, w0=0.12970743802887408, w1=0.019263020053010366\n",
      "SGD(14/49): loss=0.4844045147292205, w0=0.16276683067489922, w1=0.10098475397107477\n",
      "SGD(15/49): loss=0.4634381902909299, w0=0.2056754756767724, w1=0.008344767593884292\n",
      "SGD(16/49): loss=0.48256391750027877, w0=0.2506150734169586, w1=-0.07597461076879666\n",
      "SGD(17/49): loss=0.5059591115111722, w0=0.2302345803200213, w1=-0.1561902345603554\n",
      "SGD(18/49): loss=0.5391556574121619, w0=0.2216780516675966, w1=-0.053005669818947096\n",
      "SGD(19/49): loss=0.4999134146807389, w0=0.21977053856318668, w1=-0.13589081985636536\n",
      "SGD(20/49): loss=0.5304795782738095, w0=0.22005619575906607, w1=-0.036211065770498124\n",
      "SGD(21/49): loss=0.49448349839400624, w0=0.1965966618943588, w1=-0.12160983761773711\n",
      "SGD(22/49): loss=0.5261894553167055, w0=0.21086465153725079, w1=-0.19993783454227843\n",
      "SGD(23/49): loss=0.5585198039642223, w0=0.2412002508260491, w1=-0.1000094887911309\n",
      "SGD(24/49): loss=0.5142954257986909, w0=0.28382806880811884, w1=-0.012541457156343416\n",
      "SGD(25/49): loss=0.4822758148250859, w0=0.1793950506196303, w1=0.08914723977615932\n",
      "SGD(26/49): loss=0.4639685204555055, w0=0.19763673029012477, w1=0.16726883150288158\n",
      "SGD(27/49): loss=0.45010830489335346, w0=0.23409875292008023, w1=0.2361255264008602\n",
      "SGD(28/49): loss=0.4419336292474945, w0=0.2643921664442671, w1=0.29843724743208366\n",
      "SGD(29/49): loss=0.43885390040683514, w0=0.2874409272180442, w1=0.3552400247346145\n",
      "SGD(30/49): loss=0.43972762968076295, w0=0.29013809031255816, w1=0.4107628751817248\n",
      "SGD(31/49): loss=0.4443420203555638, w0=0.3118751731332271, w1=0.4568526862473279\n",
      "SGD(32/49): loss=0.44991820039129204, w0=0.3354420778096266, w1=0.49676410110083674\n",
      "SGD(33/49): loss=0.4565210720839092, w0=0.3736360868267483, w1=0.373873904266242\n",
      "SGD(34/49): loss=0.4387360579703763, w0=0.3830449185598034, w1=0.4246641005287339\n",
      "SGD(35/49): loss=0.44404084480269373, w0=0.3598684031532424, w1=0.4812721424913472\n",
      "SGD(36/49): loss=0.4532823136995343, w0=0.3714522073286849, w1=0.5228913561263304\n",
      "SGD(37/49): loss=0.46149465164531916, w0=0.386594732624959, w1=0.5591680115999658\n",
      "SGD(38/49): loss=0.46988689405916995, w0=0.401032423707535, w1=0.5922500560339021\n",
      "SGD(39/49): loss=0.47872063503960016, w0=0.3377512044251532, w1=0.4453574848793149\n",
      "SGD(40/49): loss=0.44773091795912145, w0=0.35984105810898104, w1=0.48722001323211134\n",
      "SGD(41/49): loss=0.4542303541425634, w0=0.41668000571195607, w1=0.361430237603071\n",
      "SGD(42/49): loss=0.43683460993278034, w0=0.3876971601169255, w1=0.23571133707877\n",
      "SGD(43/49): loss=0.43567827414098736, w0=0.4172248880442404, w1=0.29488649134305667\n",
      "SGD(44/49): loss=0.4338826136646935, w0=0.4443290754298619, w1=0.3463795681120521\n",
      "SGD(45/49): loss=0.43524760091901254, w0=0.4682558405440862, w1=0.39347600349113165\n",
      "SGD(46/49): loss=0.43894132647912554, w0=0.5196227270169745, w1=0.2833905369075726\n",
      "SGD(47/49): loss=0.43112624007437933, w0=0.5473000684218465, w1=0.3333439871675272\n",
      "SGD(48/49): loss=0.43238780091438517, w0=0.6199310190907305, w1=0.23616829015355284\n",
      "SGD(49/49): loss=0.4292108567647615, w0=0.6427199701373769, w1=0.29257477546864813\n",
      "Stochastic gradient descent regression loss 0.42557640058929463 with optimal gamma 0.18\n"
     ]
    }
   ],
   "source": [
    "w_sgd, loss_sgd = least_squares_SGD(y, x2, gamma_opt, max_iters=50)\n",
    "print(\"Stochastic gradient descent regression loss {loss} with optimal gamma {g}\".format(loss=loss_gd, g=gamma_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test1 = filtering_with_mean(tX_test)\n",
    "\n",
    "tX_test2 = keep(tX_test, to_keep)\n",
    "tX_test2 = standardize(filtering_with_mean(tX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_ls.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_ls, tX_test1)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_rr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_rr, tX_test1)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-2d8ff60821ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/pred_sg.csv'\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_gd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/pred_sg.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_gd, tX_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_sgd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_sgd, tX_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
