{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = Path(\"../data/\")\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(250000,)\n",
      "(250000, 30)\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(np.shape(y))\n",
    "print(np.shape(tX))\n",
    "print(tX.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 DER_mass_MMC\n",
      "1 DER_mass_transverse_met_lep\n",
      "2 DER_mass_vis\n",
      "3 DER_pt_h\n",
      "4 DER_deltaeta_jet_jet\n",
      "5 DER_mass_jet_jet\n",
      "6 DER_prodeta_jet_jet\n",
      "7 DER_deltar_tau_lep\n",
      "8 DER_pt_tot\n",
      "9 DER_sum_pt\n",
      "10 DER_pt_ratio_lep_tau\n",
      "11 DER_met_phi_centrality\n",
      "12 DER_lep_eta_centrality\n",
      "13 PRI_tau_pt\n",
      "14 PRI_tau_eta\n",
      "15 PRI_tau_phi\n",
      "16 PRI_lep_pt\n",
      "17 PRI_lep_eta\n",
      "18 PRI_lep_phi\n",
      "19 PRI_met\n",
      "20 PRI_met_phi\n",
      "21 PRI_met_sumet\n",
      "22 PRI_jet_num\n",
      "23 PRI_jet_leading_pt\n",
      "24 PRI_jet_leading_eta\n",
      "25 PRI_jet_leading_phi\n",
      "26 PRI_jet_subleading_pt\n",
      "27 PRI_jet_subleading_eta\n",
      "28 PRI_jet_subleading_phi\n",
      "29 PRI_jet_all_pt\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', \n",
    "                 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', \n",
    "                 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
    "                 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(i, feature_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set is composed of : \n",
    "* a y vector of length 250'000 and type float\n",
    "* a tX float matrix of 250'000 rows and 30 columns\n",
    "\n",
    "It means that our data set is composed of 250'000 different obsevations of 30 different features. In the rest of the notebook, we name the features by their index nummer. So, it means from the feature 0 from the feature  29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)\n",
    "plt.title('Histogram of y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more y = -1 than y = 1 in the data, so there is more y = 'b' than y = 's'. So, we have to pay attention to normalize the data in order to compare them in the next plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ind = np.arange(30)\n",
    "plt.boxplot(tX[:,], labels = ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many outliers depending on the feature. There are also feature that has a long interquantile range. Maybe we have to treat these feature in order to be more efficient in our futur predictions. Let's do more plots to be have a better idea :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "fig, axs = plt.subplots(5, 6, figsize=(20,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].hist(tX_2[:,n], alpha=0.4, density=True, label=['y=-1', 'y=1'])\n",
    "        axs[i,j].hist(tX_1[:,n], alpha=0.4, density=True)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the histograms of the features with a color for each y, we can see that there are useless features as they have almost the same distribution for y=1 than for y = -1. We can cut feature 15, 18, 20. \n",
    "\n",
    "\n",
    "There are also features that are very inequally distributed with value that are about -1000 and values around 0 ; it can be problematic for the prediction with such a large gap between values of a single distribution. Moreover, there is not a big difference in the distribution of y=1 and y=-1. Maybe it can be useful to put off these big negative values of these features. The features in question are : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].scatter(tX[:,n], y)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()\n",
    "\n",
    "#meme constat comment faire pour se debarrasser de ces valeurs ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots confirm the obsevations that we made in the previous plot ; no difference of the distribution of y for features 15, 18, 20 and very large gap in the distributions of features : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of -999 in the entire matrix :\n",
      "[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "(0,)\n",
      "number of -999 in the rows where y = 1 :\n",
      "[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "(0,)\n",
      "number of -999 in the rows where y = -1 :\n",
      "[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "def nb_outliers(tX, outlier) : \n",
    "    sum = 0\n",
    "    nb_outliers = []\n",
    "    for col in range(tX.shape[1]) :\n",
    "        sum = np.where(tX[:,col] == outlier)[0].shape\n",
    "        nb_outliers.append(sum)   \n",
    "    print(nb_outliers)\n",
    "    print(np.where(tX==outlier)[0].shape)\n",
    "\n",
    "out = -999\n",
    "\n",
    "print('number of -999 in the entire matrix :')\n",
    "nb_outliers(tX, out)\n",
    "\n",
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "print('number of -999 in the rows where y = 1 :')\n",
    "nb_outliers(tX_1, out)\n",
    "print('number of -999 in the rows where y = -1 :')\n",
    "nb_outliers(tX_2, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem with features  0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28. They are inequally distributed; they have a lot of -999 values and the rest is values around 0. So, here we can see how much of these -999 there are. We can see that the -999 appear only in the features that we identified with the histograms. It seems that there is a correlation between features as many features have the same number of -999. We can also see that there is more -999 in the obsevations where y=-1, so we have to take this into account when we filter the data. As there are many -999, we can't delete the rows where there is -999 because we will loose to much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changer par la valeur da la mean du feature en question sans les compter dedans\n",
    "def filtering_with_mean(tX):\n",
    "    index = [0,4,5,6,12,23,24,25,26,27,28]\n",
    "    tX_filtered = np.copy(tX)\n",
    "    arr = []\n",
    "    for ind in index :\n",
    "        arr = np.delete(tX_filtered[:,ind], np.where(tX_filtered[:,ind]==-999))\n",
    "        mean = np.mean(arr)\n",
    "        tX_filtered[np.where(tX_filtered[:,ind]==-999), ind] = mean\n",
    "    return tX_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_with_mean_bis(tX, y):\n",
    "    #index = [0,4,5,6,12,23,24,25,26,27,28]\n",
    "    index = np.arange(tX.shape[1])\n",
    "    tX_filtered = np.copy(tX)\n",
    "    \n",
    "    ind_1 = np.where(y == 1)[0]\n",
    "    ind_2 = np.where(y == -1)[0]\n",
    "    tX_1 = tX[ind_1,:]\n",
    "    tX_2 = tX[ind_2,:]\n",
    "    \n",
    "    ind_3 = np.where(tX[:,0]==-999)[0]\n",
    "    new_ind_1 = np.intersect1d(ind_3, ind_1)\n",
    "    new_ind_2 = np.intersect1d(ind_3, ind_2)\n",
    "    \n",
    "    arr_1 = []\n",
    "    arr_2 = []\n",
    "    for ind in index :\n",
    "        arr_1 = np.delete(tX_1[:,ind], np.where(tX_1[:,ind]==-999))\n",
    "        mean_1 = np.mean(arr_1)\n",
    "        arr_2 = np.delete(tX_2[:,ind], np.where(tX_2[:,ind]==-999))\n",
    "        mean_2 = np.mean(arr_2)\n",
    "        tX_filtered[new_ind_1, ind] = mean_1\n",
    "        tX_filtered[new_ind_2, ind] = mean_2\n",
    "    return tX_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(tX):\n",
    "    for i in range(tX.shape[1]):\n",
    "        tX[:,i] = (tX[:,i] - np.mean(tX[:,i])) / np.std(tX[:,i])\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to filter the data, we want to get rif of these -999, but we can't just delete the rows. So, we have the idea to replace the -999 by the mean of the rest of values of the feature. As there is a significant difference of amount of -999 in between y=1 and y=-1 in certain features, we calculate the mean for the rows where y = 1 and y = -1 separatly.\n",
    "\n",
    "Then, we can also standardize the data. It can be a good idea because the features are not all in the same range of values and it can create disproportionality between the importance of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_distribution(tX, to_log):\n",
    "    tX_log = tX\n",
    "    index = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_log)):\n",
    "            if index[i] == to_log[j]:\n",
    "                tX_log[:, i] = np.log(1+tX[:, to_log[j]])  \n",
    "    return tX_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sets(tX, y, ids):\n",
    "    index1 = np.where(tX[:, 22]==0)\n",
    "    index2 = np.where(tX[:, 22]==1)\n",
    "    index3 = np.where(tX[:, 22]>1)\n",
    "    \n",
    "    set1_x = tX[index1]\n",
    "    set1_y = y[index1]\n",
    "    set1_ids = ids[index1]\n",
    "    \n",
    "    set2_x = tX[index2]\n",
    "    set2_y = y[index2]\n",
    "    set2_ids = ids[index2]\n",
    "    \n",
    "    set3_x = tX[index3]\n",
    "    set3_y = y[index3]\n",
    "    set3_ids = ids[index3]\n",
    "    \n",
    "    return set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(tX, outlier):\n",
    "    outliers = []\n",
    "    M = np.squeeze(tX.shape[0])\n",
    "    for col in range(tX.shape[1]) :\n",
    "        out_col = np.nonzero(tX[:,col] == outlier)[0].shape\n",
    "        out_col = np.squeeze(out_col)\n",
    "        outliers.append(out_col/M)\n",
    "    print('outliers ratio for each feature', outliers)\n",
    "    \n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    index = index_full[~(outliers==np.ones(len(outliers)))]\n",
    "    index = index.reshape(-1)\n",
    "    X_without_outliers = tX[:, index]\n",
    "    return X_without_outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(tX, to_cut):\n",
    "    cut_index = 100*np.ones(tX.shape[1])\n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_cut)):\n",
    "            if index_full[i] == to_cut[j]:\n",
    "                cut_index[i] = to_cut[j]\n",
    "    index = index_full[~(index_full == cut_index)]\n",
    "    index = index.reshape(-1)\n",
    "    tX_cut = tX[:, index]\n",
    "    return tX_cut\n",
    "\n",
    "def keep(tX, to_keep):\n",
    "    keep_index = 100*np.ones(tX.shape[1])\n",
    "    index_full = np.arange(tX.shape[1])\n",
    "    for i in range(tX.shape[1]):\n",
    "        for j in range(len(to_keep)):\n",
    "            if index_full[i] == to_keep[j]:\n",
    "                keep_index[i] = to_keep[j]\n",
    "    index = index_full[index_full == keep_index]\n",
    "    index = index.reshape(-1)\n",
    "    tX_kept = tX[:, index]\n",
    "    return tX_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above in the histograms, some features seem to be useless as they have a similar distribution between the y = 1 and y = -1. So, it is useful to have function that cut or keep some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_mean = filtering_with_mean_bis(tX, y)\n",
    "tX_std = std(tX_mean)\n",
    "\n",
    "to_cut = [15,18,20]\n",
    "tX_mean_cut = cut(tX_std, to_cut)\n",
    "\n",
    "to_cut1 = [0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "tX_cut = cut(tX_std, to_cut1)\n",
    "\n",
    "to_keep = [13, 14, 17]\n",
    "tX_kept = keep(tX_std, to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-d9e33aca35d4>:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tX[:,i] = (tX[:,i] - np.mean(tX[:,i])) / np.std(tX[:,i])\n"
     ]
    }
   ],
   "source": [
    "to_log = [1, 2, 5, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "tX_log = log_distribution(tX, to_log)\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "print('Set 1')\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = std(filtering_with_mean_bis(set1_x, set1_y))\n",
    "set1_x = np.c_[np.ones(set1_x.shape[0]), set1_x]\n",
    "_ = outliers(set1_x, -999)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y)\n",
    "_ = outliers(set2_x, -999)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = std(filtering_with_mean_bis(set3_x, set3_y))\n",
    "set1_x = np.c_[np.ones(set1_x.shape[0]), set1_x]\n",
    "_ = outliers(set3_x, -999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose x and generate test and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tX_mean\n",
    "x2 = tX_kept\n",
    "x3 = tX_std\n",
    "x4 = tX_mean_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8231175887967075, 0.8979280274561516, 3.8156436509961016, 5.897661698062343, 510.1260411941516, 8994.2125337868, 675850.9495906193, 7011082.216144496, 537245683.3080721, 9413700696.528532]\n",
      "Cross validation finished: optimal degree 1\n",
      "Least square loss 0.3386372049426108\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=1)\n",
    "degrees = np.arange(1,11)\n",
    "degree_opt, _ = best_degree_selection(y, tX_log, degrees, k_fold=4, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "tX_poly = build_poly(x1, degree_opt)\n",
    "w_ls, loss_ls = least_squares(y, tX_poly)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))\n",
    "degree_ls = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-0ce6ba13d48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdegree_opt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_degree_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset1_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset1_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfonction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross validation finished: optimal degree {d}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegree_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtX_poly1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset1_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_opt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/cross_validation.py\u001b[0m in \u001b[0;36mbest_degree_selection\u001b[0;34m(y, x, degrees, k_fold, lambdas, fonction, seed)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mrmse_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfonction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mbest_rmses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/cross_validation.py\u001b[0m in \u001b[0;36mk_fold_regression\u001b[0;34m(y, x, k_indices, k, par, degree, fonction)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/implementations.py\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Least squares regression using normal equations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DD->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=1)\n",
    "degrees = np.arange(1,11)\n",
    "degree_opt, _ = best_degree_selection(set1_y, set1_x, degrees, k_fold=4, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "tX_poly = build_poly(set1_x, degree_opt)\n",
    "w_ls1, loss_ls1 = least_squares(set1_y, tX_poly)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls1))\n",
    "degree_ls1 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"w_ls, loss_ls = least_squares(y, x1)\\nprint(\"Least square loss {loss}\".format(loss=loss_ls))\\nw_ls, loss_ls = least_squares(y, build_poly(x1, 8))\\nprint(\"Least square mse loss {loss} with degree 8\".format(loss=loss_ls))\\nw_ls, loss_ls = least_squares(y, build_poly(x1, 8))\\nloss_ls = np.sqrt(2*loss_ls)\\nprint(\"Least square rmse loss {loss} with degree 8\".format(loss=loss_ls))'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#je commente sinon pas le bon w_ls pour la submission\n",
    "\"\"\"\"w_ls, loss_ls = least_squares(y, x1)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))\n",
    "w_ls, loss_ls = least_squares(y, build_poly(x1, 8))\n",
    "print(\"Least square mse loss {loss} with degree 8\".format(loss=loss_ls))\n",
    "w_ls, loss_ls = least_squares(y, build_poly(x1, 8))\n",
    "loss_ls = np.sqrt(2*loss_ls)\n",
    "print(\"Least square rmse loss {loss} with degree 8\".format(loss=loss_ls))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Cross validation finished: optimal degree 1\n",
      "0 1 1\n",
      "0 2 2\n",
      "0 3 3\n",
      "0 4 4\n",
      "0 5 5\n",
      "0 6 6\n",
      "0 7 7\n",
      "0 8 8\n",
      "0 9 9\n",
      "0 10 10\n",
      "0 11 11\n",
      "0 12 12\n",
      "0 13 13\n",
      "0 14 14\n",
      "0 15 15\n",
      "0 16 16\n",
      "0 17 17\n",
      "0 18 18\n",
      "0 19 19\n",
      "0 20 20\n",
      "0 21 21\n",
      "0 22 22\n",
      "0 23 23\n",
      "0 24 24\n",
      "0 25 25\n",
      "0 26 26\n",
      "0 27 27\n",
      "1 2 28\n",
      "1 3 29\n",
      "1 4 30\n",
      "1 5 31\n",
      "1 6 32\n",
      "1 7 33\n",
      "1 8 34\n",
      "1 9 35\n",
      "1 10 36\n",
      "1 11 37\n",
      "1 12 38\n",
      "1 13 39\n",
      "1 14 40\n",
      "1 15 41\n",
      "1 16 42\n",
      "1 17 43\n",
      "1 18 44\n",
      "1 19 45\n",
      "1 20 46\n",
      "1 21 47\n",
      "1 22 48\n",
      "1 23 49\n",
      "1 24 50\n",
      "1 25 51\n",
      "1 26 52\n",
      "1 27 53\n",
      "2 3 54\n",
      "2 4 55\n",
      "2 5 56\n",
      "2 6 57\n",
      "2 7 58\n",
      "2 8 59\n",
      "2 9 60\n",
      "2 10 61\n",
      "2 11 62\n",
      "2 12 63\n",
      "2 13 64\n",
      "2 14 65\n",
      "2 15 66\n",
      "2 16 67\n",
      "2 17 68\n",
      "2 18 69\n",
      "2 19 70\n",
      "2 20 71\n",
      "2 21 72\n",
      "2 22 73\n",
      "2 23 74\n",
      "2 24 75\n",
      "2 25 76\n",
      "2 26 77\n",
      "2 27 78\n",
      "3 4 79\n",
      "3 5 80\n",
      "3 6 81\n",
      "3 7 82\n",
      "3 8 83\n",
      "3 9 84\n",
      "3 10 85\n",
      "3 11 86\n",
      "3 12 87\n",
      "3 13 88\n",
      "3 14 89\n",
      "3 15 90\n",
      "3 16 91\n",
      "3 17 92\n",
      "3 18 93\n",
      "3 19 94\n",
      "3 20 95\n",
      "3 21 96\n",
      "3 22 97\n",
      "3 23 98\n",
      "3 24 99\n",
      "3 25 100\n",
      "3 26 101\n",
      "3 27 102\n",
      "4 5 103\n",
      "4 6 104\n",
      "4 7 105\n",
      "4 8 106\n",
      "4 9 107\n",
      "4 10 108\n",
      "4 11 109\n",
      "4 12 110\n",
      "4 13 111\n",
      "4 14 112\n",
      "4 15 113\n",
      "4 16 114\n",
      "4 17 115\n",
      "4 18 116\n",
      "4 19 117\n",
      "4 20 118\n",
      "4 21 119\n",
      "4 22 120\n",
      "4 23 121\n",
      "4 24 122\n",
      "4 25 123\n",
      "4 26 124\n",
      "4 27 125\n",
      "5 6 126\n",
      "5 7 127\n",
      "5 8 128\n",
      "5 9 129\n",
      "5 10 130\n",
      "5 11 131\n",
      "5 12 132\n",
      "5 13 133\n",
      "5 14 134\n",
      "5 15 135\n",
      "5 16 136\n",
      "5 17 137\n",
      "5 18 138\n",
      "5 19 139\n",
      "5 20 140\n",
      "5 21 141\n",
      "5 22 142\n",
      "5 23 143\n",
      "5 24 144\n",
      "5 25 145\n",
      "5 26 146\n",
      "5 27 147\n",
      "6 7 148\n",
      "6 8 149\n",
      "6 9 150\n",
      "6 10 151\n",
      "6 11 152\n",
      "6 12 153\n",
      "6 13 154\n",
      "6 14 155\n",
      "6 15 156\n",
      "6 16 157\n",
      "6 17 158\n",
      "6 18 159\n",
      "6 19 160\n",
      "6 20 161\n",
      "6 21 162\n",
      "6 22 163\n",
      "6 23 164\n",
      "6 24 165\n",
      "6 25 166\n",
      "6 26 167\n",
      "6 27 168\n",
      "7 8 169\n",
      "7 9 170\n",
      "7 10 171\n",
      "7 11 172\n",
      "7 12 173\n",
      "7 13 174\n",
      "7 14 175\n",
      "7 15 176\n",
      "7 16 177\n",
      "7 17 178\n",
      "7 18 179\n",
      "7 19 180\n",
      "7 20 181\n",
      "7 21 182\n",
      "7 22 183\n",
      "7 23 184\n",
      "7 24 185\n",
      "7 25 186\n",
      "7 26 187\n",
      "7 27 188\n",
      "8 9 189\n",
      "8 10 190\n",
      "8 11 191\n",
      "8 12 192\n",
      "8 13 193\n",
      "8 14 194\n",
      "8 15 195\n",
      "8 16 196\n",
      "8 17 197\n",
      "8 18 198\n",
      "8 19 199\n",
      "8 20 200\n",
      "8 21 201\n",
      "8 22 202\n",
      "8 23 203\n",
      "8 24 204\n",
      "8 25 205\n",
      "8 26 206\n",
      "8 27 207\n",
      "9 10 208\n",
      "9 11 209\n",
      "9 12 210\n",
      "9 13 211\n",
      "9 14 212\n",
      "9 15 213\n",
      "9 16 214\n",
      "9 17 215\n",
      "9 18 216\n",
      "9 19 217\n",
      "9 20 218\n",
      "9 21 219\n",
      "9 22 220\n",
      "9 23 221\n",
      "9 24 222\n",
      "9 25 223\n",
      "9 26 224\n",
      "9 27 225\n",
      "10 11 226\n",
      "10 12 227\n",
      "10 13 228\n",
      "10 14 229\n",
      "10 15 230\n",
      "10 16 231\n",
      "10 17 232\n",
      "10 18 233\n",
      "10 19 234\n",
      "10 20 235\n",
      "10 21 236\n",
      "10 22 237\n",
      "10 23 238\n",
      "10 24 239\n",
      "10 25 240\n",
      "10 26 241\n",
      "10 27 242\n",
      "11 12 243\n",
      "11 13 244\n",
      "11 14 245\n",
      "11 15 246\n",
      "11 16 247\n",
      "11 17 248\n",
      "11 18 249\n",
      "11 19 250\n",
      "11 20 251\n",
      "11 21 252\n",
      "11 22 253\n",
      "11 23 254\n",
      "11 24 255\n",
      "11 25 256\n",
      "11 26 257\n",
      "11 27 258\n",
      "12 13 259\n",
      "12 14 260\n",
      "12 15 261\n",
      "12 16 262\n",
      "12 17 263\n",
      "12 18 264\n",
      "12 19 265\n",
      "12 20 266\n",
      "12 21 267\n",
      "12 22 268\n",
      "12 23 269\n",
      "12 24 270\n",
      "12 25 271\n",
      "12 26 272\n",
      "12 27 273\n",
      "13 14 274\n",
      "13 15 275\n",
      "13 16 276\n",
      "13 17 277\n",
      "13 18 278\n",
      "13 19 279\n",
      "13 20 280\n",
      "13 21 281\n",
      "13 22 282\n",
      "13 23 283\n",
      "13 24 284\n",
      "13 25 285\n",
      "13 26 286\n",
      "13 27 287\n",
      "14 15 288\n",
      "14 16 289\n",
      "14 17 290\n",
      "14 18 291\n",
      "14 19 292\n",
      "14 20 293\n",
      "14 21 294\n",
      "14 22 295\n",
      "14 23 296\n",
      "14 24 297\n",
      "14 25 298\n",
      "14 26 299\n",
      "14 27 300\n",
      "15 16 301\n",
      "15 17 302\n",
      "15 18 303\n",
      "15 19 304\n",
      "15 20 305\n",
      "15 21 306\n",
      "15 22 307\n",
      "15 23 308\n",
      "15 24 309\n",
      "15 25 310\n",
      "15 26 311\n",
      "15 27 312\n",
      "16 17 313\n",
      "16 18 314\n",
      "16 19 315\n",
      "16 20 316\n",
      "16 21 317\n",
      "16 22 318\n",
      "16 23 319\n",
      "16 24 320\n",
      "16 25 321\n",
      "16 26 322\n",
      "16 27 323\n",
      "17 18 324\n",
      "17 19 325\n",
      "17 20 326\n",
      "17 21 327\n",
      "17 22 328\n",
      "17 23 329\n",
      "17 24 330\n",
      "17 25 331\n",
      "17 26 332\n",
      "17 27 333\n",
      "18 19 334\n",
      "18 20 335\n",
      "18 21 336\n",
      "18 22 337\n",
      "18 23 338\n",
      "18 24 339\n",
      "18 25 340\n",
      "18 26 341\n",
      "18 27 342\n",
      "19 20 343\n",
      "19 21 344\n",
      "19 22 345\n",
      "19 23 346\n",
      "19 24 347\n",
      "19 25 348\n",
      "19 26 349\n",
      "19 27 350\n",
      "20 21 351\n",
      "20 22 352\n",
      "20 23 353\n",
      "20 24 354\n",
      "20 25 355\n",
      "20 26 356\n",
      "20 27 357\n",
      "21 22 358\n",
      "21 23 359\n",
      "21 24 360\n",
      "21 25 361\n",
      "21 26 362\n",
      "21 27 363\n",
      "22 23 364\n",
      "22 24 365\n",
      "22 25 366\n",
      "22 26 367\n",
      "22 27 368\n",
      "23 24 369\n",
      "23 25 370\n",
      "23 26 371\n",
      "23 27 372\n",
      "24 25 373\n",
      "24 26 374\n",
      "24 27 375\n",
      "25 26 376\n",
      "25 27 377\n",
      "26 27 378\n",
      "Least square loss 0.3386372049426108\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=1)\n",
    "degrees = np.arange(1,11)\n",
    "degree_opt, _ = best_degree_selection(y, x4, degrees, k_fold=4, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "tX2 = build_poly(x4, degree_opt)\n",
    "w_ls2, loss_ls2 = least_squares(y, tX2)\n",
    "print(\"Least square loss {loss}\".format(loss=loss_ls))\n",
    "degree_ls2 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to find the oprimal lambda and degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Cross validation finished: optimal lambda 0.0001 and degree 1\n",
      "Ridge regression loss nan\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x4, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "tX = build_poly(x1, degree_opt)\n",
    "w_rr, loss_rr = ridge_regression(y, tX, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "degree_rr = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-ee963c2e1408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdegree_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_degree_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset1_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset1_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfonction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross validation finished: optimal lambda {l} and degree {d}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegree_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtX_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw_rr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_rr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset1_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/cross_validation.py\u001b[0m in \u001b[0;36mbest_degree_selection\u001b[0;34m(y, x, degrees, k_fold, lambdas, fonction, seed)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mrmse_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfonction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mrmse_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse_te_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/cross_validation.py\u001b[0m in \u001b[0;36mk_fold_regression\u001b[0;34m(y, x, k_indices, k, par, degree, fonction)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/implementations.py\u001b[0m in \u001b[0;36mridge_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlambda_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DD->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "degree_opt, lambda_opt = best_degree_selection(set1_y, set1_x, degrees, k_fold=10, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "tX_poly = build_poly(x1, degree_opt)\n",
    "w_rr1, loss_rr1 = ridge_regression(set1_y, tX_poly, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr1))\n",
    "degree_rr1 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV to find best gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Si tu fais ca avec tous les features, ca prend beaucoup de temps (>1h) et ne converge pas\n",
    "# en prenant x_kept on converge vers loss=0.42 et gamma_opt = 2.51 \n",
    "# je pense qu'il faut trouver un just milieu entre 30 et 3 features \n",
    "max_iters = 500\n",
    "k_fold = 10\n",
    "initial_w = np.zeros(x2.shape[1])\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "gamma_opt = cross_validation(y, x2, k_fold, gammas, fonction=2)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "w_gd, loss_gd = least_squares_GD(y, x2, gamma_opt, max_iters=max_iters)\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "gamma_opt = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=2)\n",
    "w_gd, loss_gd = least_squares_GD(set1_y, set1_x, gamma_opt, max_iters=max_iters)\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4941655003981128\n",
      "Gradient Descent(2/49): loss=0.4886613766342383\n",
      "Gradient Descent(3/49): loss=0.4834675536251354\n",
      "Gradient Descent(4/49): loss=0.47856521166821153\n",
      "Gradient Descent(5/49): loss=0.47393670626558704\n",
      "Gradient Descent(6/49): loss=0.4695654931910855\n",
      "Gradient Descent(7/49): loss=0.46543605844542885\n",
      "Gradient Descent(8/49): loss=0.4615338527702052\n",
      "Gradient Descent(9/49): loss=0.4578452304145177\n",
      "Gradient Descent(10/49): loss=0.45435739186978397\n",
      "Gradient Descent(11/49): loss=0.4510583303080921\n",
      "Gradient Descent(12/49): loss=0.4479367814779505\n",
      "Gradient Descent(13/49): loss=0.4449821768283364\n",
      "Gradient Descent(14/49): loss=0.4421845996477504\n",
      "Gradient Descent(15/49): loss=0.43953474401962667\n",
      "Gradient Descent(16/49): loss=0.43702387640902884\n",
      "Gradient Descent(17/49): loss=0.43464379970815625\n",
      "Gradient Descent(18/49): loss=0.43238681957987346\n",
      "Gradient Descent(19/49): loss=0.43024571294933556\n",
      "Gradient Descent(20/49): loss=0.4282136985038611\n",
      "Gradient Descent(21/49): loss=0.4262844090705844\n",
      "Gradient Descent(22/49): loss=0.42445186575013044\n",
      "Gradient Descent(23/49): loss=0.4227104536926621\n",
      "Gradient Descent(24/49): loss=0.4210548994101978\n",
      "Gradient Descent(25/49): loss=0.4194802495261185\n",
      "Gradient Descent(26/49): loss=0.41798185086932205\n",
      "Gradient Descent(27/49): loss=0.4165553318265793\n",
      "Gradient Descent(28/49): loss=0.41519658487231703\n",
      "Gradient Descent(29/49): loss=0.4139017502003542\n",
      "Gradient Descent(30/49): loss=0.4126672003870411\n",
      "Gradient Descent(31/49): loss=0.4114895260198615\n",
      "Gradient Descent(32/49): loss=0.4103655222298449\n",
      "Gradient Descent(33/49): loss=0.40929217607014584\n",
      "Gradient Descent(34/49): loss=0.40826665468687967\n",
      "Gradient Descent(35/49): loss=0.40728629423180224\n",
      "Gradient Descent(36/49): loss=0.4063485894696716\n",
      "Gradient Descent(37/49): loss=0.40545118403617747\n",
      "Gradient Descent(38/49): loss=0.4045918613051645\n",
      "Gradient Descent(39/49): loss=0.4037685358265335\n",
      "Gradient Descent(40/49): loss=0.4029792452986819\n",
      "Gradient Descent(41/49): loss=0.4022221430416668\n",
      "Gradient Descent(42/49): loss=0.4014954909394419\n",
      "Gradient Descent(43/49): loss=0.4007976528215396\n",
      "Gradient Descent(44/49): loss=0.4001270882564727\n",
      "Gradient Descent(45/49): loss=0.39948234673089406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=0.39886206219021036\n",
      "Gradient Descent(47/49): loss=0.3982649479178996\n",
      "Gradient Descent(48/49): loss=0.3976897917322227\n",
      "Gradient Descent(49/49): loss=0.3971354514803838\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49413642631629345\n",
      "Gradient Descent(2/49): loss=0.48860589924345954\n",
      "Gradient Descent(3/49): loss=0.48338808662401894\n",
      "Gradient Descent(4/49): loss=0.478463933367628\n",
      "Gradient Descent(5/49): loss=0.47381557965054294\n",
      "Gradient Descent(6/49): loss=0.4694262844643921\n",
      "Gradient Descent(7/49): loss=0.4652803541541558\n",
      "Gradient Descent(8/49): loss=0.461363075610596\n",
      "Gradient Descent(9/49): loss=0.45766065380581167\n",
      "Gradient Descent(10/49): loss=0.4541601533822859\n",
      "Gradient Descent(11/49): loss=0.45084944402586735\n",
      "Gradient Descent(12/49): loss=0.4477171493717443\n",
      "Gradient Descent(13/49): loss=0.4447525992097034\n",
      "Gradient Descent(14/49): loss=0.44194578477096985\n",
      "Gradient Descent(15/49): loss=0.43928731689376255\n",
      "Gradient Descent(16/49): loss=0.43676838687847597\n",
      "Gradient Descent(17/49): loss=0.4343807298561965\n",
      "Gradient Descent(18/49): loss=0.43211659050614804\n",
      "Gradient Descent(19/49): loss=0.42996869096871443\n",
      "Gradient Descent(20/49): loss=0.4279302008109606\n",
      "Gradient Descent(21/49): loss=0.42599470891113317\n",
      "Gradient Descent(22/49): loss=0.42415619713751457\n",
      "Gradient Descent(23/49): loss=0.4224090157052874\n",
      "Gradient Descent(24/49): loss=0.4207478601027725\n",
      "Gradient Descent(25/49): loss=0.4191677494855874\n",
      "Gradient Descent(26/49): loss=0.41766400644396134\n",
      "Gradient Descent(27/49): loss=0.41623223805468024\n",
      "Gradient Descent(28/49): loss=0.4148683181349464\n",
      "Gradient Descent(29/49): loss=0.41356837062086105\n",
      "Gradient Descent(30/49): loss=0.41232875399828894\n",
      "Gradient Descent(31/49): loss=0.4111460467185865\n",
      "Gradient Descent(32/49): loss=0.4100170335360703\n",
      "Gradient Descent(33/49): loss=0.408938692708213\n",
      "Gradient Descent(34/49): loss=0.40790818400338763\n",
      "Gradient Descent(35/49): loss=0.40692283746455904\n",
      "Gradient Descent(36/49): loss=0.40598014288066503\n",
      "Gradient Descent(37/49): loss=0.4050777399205522\n",
      "Gradient Descent(38/49): loss=0.4042134088872451\n",
      "Gradient Descent(39/49): loss=0.40338506205305386\n",
      "Gradient Descent(40/49): loss=0.4025907355385706\n",
      "Gradient Descent(41/49): loss=0.40182858170098257\n",
      "Gradient Descent(42/49): loss=0.40109686199935257\n",
      "Gradient Descent(43/49): loss=0.4003939403065998\n",
      "Gradient Descent(44/49): loss=0.39971827663985043\n",
      "Gradient Descent(45/49): loss=0.3990684212826471\n",
      "Gradient Descent(46/49): loss=0.3984430092742061\n",
      "Gradient Descent(47/49): loss=0.3978407552424958\n",
      "Gradient Descent(48/49): loss=0.3972604485593957\n",
      "Gradient Descent(49/49): loss=0.3967009487975875\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49402210880954417\n",
      "Gradient Descent(2/49): loss=0.48838436758743664\n",
      "Gradient Descent(3/49): loss=0.4830660330853122\n",
      "Gradient Descent(4/49): loss=0.4780476621382103\n",
      "Gradient Descent(5/49): loss=0.4733110286518961\n",
      "Gradient Descent(6/49): loss=0.46883904599372717\n",
      "Gradient Descent(7/49): loss=0.46461569442582096\n",
      "Gradient Descent(8/49): loss=0.4606259532444396\n",
      "Gradient Descent(9/49): loss=0.4568557373128088\n",
      "Gradient Descent(10/49): loss=0.45329183769617293\n",
      "Gradient Descent(11/49): loss=0.4499218661278931\n",
      "Gradient Descent(12/49): loss=0.4467342030539546\n",
      "Gradient Descent(13/49): loss=0.44371794902046063\n",
      "Gradient Descent(14/49): loss=0.4408628791846778\n",
      "Gradient Descent(15/49): loss=0.43815940074503534\n",
      "Gradient Descent(16/49): loss=0.43559851309927544\n",
      "Gradient Descent(17/49): loss=0.4331717705527686\n",
      "Gradient Descent(18/49): loss=0.4308712474109264\n",
      "Gradient Descent(19/49): loss=0.4286895053007359\n",
      "Gradient Descent(20/49): loss=0.4266195625767524\n",
      "Gradient Descent(21/49): loss=0.4246548656764969\n",
      "Gradient Descent(22/49): loss=0.4227892622991463\n",
      "Gradient Descent(23/49): loss=0.4210169762897333\n",
      "Gradient Descent(24/49): loss=0.41933258411883906\n",
      "Gradient Descent(25/49): loss=0.4177309928549935\n",
      "Gradient Descent(26/49): loss=0.416207419533742\n",
      "Gradient Descent(27/49): loss=0.41475737183362915\n",
      "Gradient Descent(28/49): loss=0.4133766299752127\n",
      "Gradient Descent(29/49): loss=0.41206122976469506\n",
      "Gradient Descent(30/49): loss=0.41080744670886715\n",
      "Gradient Descent(31/49): loss=0.4096117811328231\n",
      "Gradient Descent(32/49): loss=0.40847094423635577\n",
      "Gradient Descent(33/49): loss=0.40738184502909436\n",
      "Gradient Descent(34/49): loss=0.4063415780883292\n",
      "Gradient Descent(35/49): loss=0.4053474120870874\n",
      "Gradient Descent(36/49): loss=0.40439677904340926\n",
      "Gradient Descent(37/49): loss=0.40348726424493947\n",
      "Gradient Descent(38/49): loss=0.40261659680589973\n",
      "Gradient Descent(39/49): loss=0.40178264081627163\n",
      "Gradient Descent(40/49): loss=0.40098338704560005\n",
      "Gradient Descent(41/49): loss=0.40021694516623896\n",
      "Gradient Descent(42/49): loss=0.3994815364631202\n",
      "Gradient Descent(43/49): loss=0.398775486999229\n",
      "Gradient Descent(44/49): loss=0.3980972212079492\n",
      "Gradient Descent(45/49): loss=0.3974452558852801\n",
      "Gradient Descent(46/49): loss=0.396818194556653\n",
      "Gradient Descent(47/49): loss=0.39621472219469167\n",
      "Gradient Descent(48/49): loss=0.395633600265764\n",
      "Gradient Descent(49/49): loss=0.39507366208459344\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49421128060577485\n",
      "Gradient Descent(2/49): loss=0.48874981295609315\n",
      "Gradient Descent(3/49): loss=0.4835957380468211\n",
      "Gradient Descent(4/49): loss=0.47873043513867913\n",
      "Gradient Descent(5/49): loss=0.47413644323334403\n",
      "Gradient Descent(6/49): loss=0.4697973876075692\n",
      "Gradient Descent(7/49): loss=0.46569791107280456\n",
      "Gradient Descent(8/49): loss=0.4618236096503577\n",
      "Gradient Descent(9/49): loss=0.45816097237308695\n",
      "Gradient Descent(10/49): loss=0.4546973249440866\n",
      "Gradient Descent(11/49): loss=0.45142077700091454\n",
      "Gradient Descent(12/49): loss=0.44832017275074193\n",
      "Gradient Descent(13/49): loss=0.44538504475745283\n",
      "Gradient Descent(14/49): loss=0.44260557067629036\n",
      "Gradient Descent(15/49): loss=0.4399725327452032\n",
      "Gradient Descent(16/49): loss=0.43747727985467477\n",
      "Gradient Descent(17/49): loss=0.4351116920295762\n",
      "Gradient Descent(18/49): loss=0.43286814716754946\n",
      "Gradient Descent(19/49): loss=0.4307394898886347\n",
      "Gradient Descent(20/49): loss=0.42871900236038923\n",
      "Gradient Descent(21/49): loss=0.4268003769716171\n",
      "Gradient Descent(22/49): loss=0.4249776907361195\n",
      "Gradient Descent(23/49): loss=0.4232453813156048\n",
      "Gradient Descent(24/49): loss=0.4215982245581059\n",
      "Gradient Descent(25/49): loss=0.4200313134549912\n",
      "Gradient Descent(26/49): loss=0.4185400384259359\n",
      "Gradient Descent(27/49): loss=0.4171200688470932\n",
      "Gradient Descent(28/49): loss=0.41576733574318303\n",
      "Gradient Descent(29/49): loss=0.414478015569339\n",
      "Gradient Descent(30/49): loss=0.4132485150133344\n",
      "Gradient Descent(31/49): loss=0.4120754567532796\n",
      "Gradient Descent(32/49): loss=0.4109556661100583\n",
      "Gradient Descent(33/49): loss=0.409886158537673\n",
      "Gradient Descent(34/49): loss=0.408864127898322\n",
      "Gradient Descent(35/49): loss=0.4078869354724351\n",
      "Gradient Descent(36/49): loss=0.40695209965709056\n",
      "Gradient Descent(37/49): loss=0.40605728630921384\n",
      "Gradient Descent(38/49): loss=0.4052002996927493\n",
      "Gradient Descent(39/49): loss=0.40437907399160194\n",
      "Gradient Descent(40/49): loss=0.40359166535258867\n",
      "Gradient Descent(41/49): loss=0.4028362444249172\n",
      "Gradient Descent(42/49): loss=0.4021110893648483\n",
      "Gradient Descent(43/49): loss=0.40141457927619095\n",
      "Gradient Descent(44/49): loss=0.400745188059153\n",
      "Gradient Descent(45/49): loss=0.4001014786418123\n",
      "Gradient Descent(46/49): loss=0.39948209757011593\n",
      "Gradient Descent(47/49): loss=0.39888576993384056\n",
      "Gradient Descent(48/49): loss=0.3983112946073841\n",
      "Gradient Descent(49/49): loss=0.39775753978559725\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.488501412862874\n",
      "Gradient Descent(2/49): loss=0.4782832061104915\n",
      "Gradient Descent(3/49): loss=0.4691924343145119\n",
      "Gradient Descent(4/49): loss=0.46109494484470387\n",
      "Gradient Descent(5/49): loss=0.45387302188896084\n",
      "Gradient Descent(6/49): loss=0.4474233322912113\n",
      "Gradient Descent(7/49): loss=0.44165513335358936\n",
      "Gradient Descent(8/49): loss=0.43648870825971475\n",
      "Gradient Descent(9/49): loss=0.43185399947139924\n",
      "Gradient Descent(10/49): loss=0.4276894144631632\n",
      "Gradient Descent(11/49): loss=0.4239407815958188\n",
      "Gradient Descent(12/49): loss=0.42056043688133726\n",
      "Gradient Descent(13/49): loss=0.41750642493037327\n",
      "Gradient Descent(14/49): loss=0.41474179956280227\n",
      "Gradient Descent(15/49): loss=0.412234011452028\n",
      "Gradient Descent(16/49): loss=0.409954371808913\n",
      "Gradient Descent(17/49): loss=0.4078775825274375\n",
      "Gradient Descent(18/49): loss=0.4059813244424495\n",
      "Gradient Descent(19/49): loss=0.40424589641627634\n",
      "Gradient Descent(20/49): loss=0.4026538988977916\n",
      "Gradient Descent(21/49): loss=0.4011899564037648\n",
      "Gradient Descent(22/49): loss=0.39984047407421586\n",
      "Gradient Descent(23/49): loss=0.39859342406498766\n",
      "Gradient Descent(24/49): loss=0.39743815807383265\n",
      "Gradient Descent(25/49): loss=0.39636524276128443\n",
      "Gradient Descent(26/49): loss=0.3953663152333754\n",
      "Gradient Descent(27/49): loss=0.3944339561075466\n",
      "Gradient Descent(28/49): loss=0.3935615779925508\n",
      "Gradient Descent(29/49): loss=0.3927433274835341\n",
      "Gradient Descent(30/49): loss=0.39197399900981483\n",
      "Gradient Descent(31/49): loss=0.39124895907949936\n",
      "Gradient Descent(32/49): loss=0.39056407964578027\n",
      "Gradient Descent(33/49): loss=0.38991567947782724\n",
      "Gradient Descent(34/49): loss=0.3893004725574842\n",
      "Gradient Descent(35/49): loss=0.3887155226440148\n",
      "Gradient Descent(36/49): loss=0.3881582032550783\n",
      "Gradient Descent(37/49): loss=0.38762616240485653\n",
      "Gradient Descent(38/49): loss=0.38711729152145985\n",
      "Gradient Descent(39/49): loss=0.38662969803685154\n",
      "Gradient Descent(40/49): loss=0.38616168120481703\n",
      "Gradient Descent(41/49): loss=0.38571171075706434\n",
      "Gradient Descent(42/49): loss=0.3852784080553468\n",
      "Gradient Descent(43/49): loss=0.38486052943938637\n",
      "Gradient Descent(44/49): loss=0.384456951507081\n",
      "Gradient Descent(45/49): loss=0.3840666580956587\n",
      "Gradient Descent(46/49): loss=0.3836887287606398\n",
      "Gradient Descent(47/49): loss=0.38332232857420345\n",
      "Gradient Descent(48/49): loss=0.38296669908623093\n",
      "Gradient Descent(49/49): loss=0.38262115031031946\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4884446679295025\n",
      "Gradient Descent(2/49): loss=0.47817986828859377\n",
      "Gradient Descent(3/49): loss=0.469050731665049\n",
      "Gradient Descent(4/49): loss=0.46092150222219064\n",
      "Gradient Descent(5/49): loss=0.45367313398691567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=0.44720119524788743\n",
      "Gradient Descent(7/49): loss=0.4414140405361766\n",
      "Gradient Descent(8/49): loss=0.43623121520369174\n",
      "Gradient Descent(9/49): loss=0.43158206235447866\n",
      "Gradient Descent(10/49): loss=0.4274045059462808\n",
      "Gradient Descent(11/49): loss=0.42364398736889175\n",
      "Gradient Descent(12/49): loss=0.4202525358085233\n",
      "Gradient Descent(13/49): loss=0.4171879552960511\n",
      "Gradient Descent(14/49): loss=0.4144131135722714\n",
      "Gradient Descent(15/49): loss=0.4118953198362451\n",
      "Gradient Descent(16/49): loss=0.4096057801164972\n",
      "Gradient Descent(17/49): loss=0.4075191204557555\n",
      "Gradient Descent(18/49): loss=0.40561296935903746\n",
      "Gradient Descent(19/49): loss=0.40386759204860617\n",
      "Gradient Descent(20/49): loss=0.40226557002016616\n",
      "Gradient Descent(21/49): loss=0.40079152022192543\n",
      "Gradient Descent(22/49): loss=0.39943184889837907\n",
      "Gradient Descent(23/49): loss=0.39817453576809847\n",
      "Gradient Descent(24/49): loss=0.3970089447516754\n",
      "Gradient Descent(25/49): loss=0.3959256579428744\n",
      "Gradient Descent(26/49): loss=0.3949163299320922\n",
      "Gradient Descent(27/49): loss=0.39397355995434036\n",
      "Gradient Descent(28/49): loss=0.3930907796509879\n",
      "Gradient Descent(29/49): loss=0.3922621545113633\n",
      "Gradient Descent(30/49): loss=0.39148249730218904\n",
      "Gradient Descent(31/49): loss=0.3907471920041628\n",
      "Gradient Descent(32/49): loss=0.39005212695972413\n",
      "Gradient Descent(33/49): loss=0.38939363609752836\n",
      "Gradient Descent(34/49): loss=0.38876844724035\n",
      "Gradient Descent(35/49): loss=0.3881736366266211\n",
      "Gradient Descent(36/49): loss=0.3876065888838213\n",
      "Gradient Descent(37/49): loss=0.38706496178643013\n",
      "Gradient Descent(38/49): loss=0.38654665521382287\n",
      "Gradient Descent(39/49): loss=0.3860497837958501\n",
      "Gradient Descent(40/49): loss=0.385572652797155\n",
      "Gradient Descent(41/49): loss=0.38511373684671507\n",
      "Gradient Descent(42/49): loss=0.3846716611676182\n",
      "Gradient Descent(43/49): loss=0.38424518500457044\n",
      "Gradient Descent(44/49): loss=0.3838331869838367\n",
      "Gradient Descent(45/49): loss=0.3834346521729007\n",
      "Gradient Descent(46/49): loss=0.383048660635671\n",
      "Gradient Descent(47/49): loss=0.3826743773040635\n",
      "Gradient Descent(48/49): loss=0.38231104300869634\n",
      "Gradient Descent(49/49): loss=0.38195796653062947\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4882196911008455\n",
      "Gradient Descent(2/49): loss=0.47775748815064895\n",
      "Gradient Descent(3/49): loss=0.46845537295325007\n",
      "Gradient Descent(4/49): loss=0.4601747896454957\n",
      "Gradient Descent(5/49): loss=0.45279420436675083\n",
      "Gradient Descent(6/49): loss=0.446206976397188\n",
      "Gradient Descent(7/49): loss=0.4403195000532209\n",
      "Gradient Descent(8/49): loss=0.4350495821454107\n",
      "Gradient Descent(9/49): loss=0.4303250245317689\n",
      "Gradient Descent(10/49): loss=0.4260823853593635\n",
      "Gradient Descent(11/49): loss=0.42226589608067866\n",
      "Gradient Descent(12/49): loss=0.4188265143427495\n",
      "Gradient Descent(13/49): loss=0.41572109544741503\n",
      "Gradient Descent(14/49): loss=0.4129116673295833\n",
      "Gradient Descent(15/49): loss=0.410364795947363\n",
      "Gradient Descent(16/49): loss=0.40805102966574225\n",
      "Gradient Descent(17/49): loss=0.40594441268026193\n",
      "Gradient Descent(18/49): loss=0.40402205879953274\n",
      "Gradient Descent(19/49): loss=0.4022637780116997\n",
      "Gradient Descent(20/49): loss=0.40065174922249847\n",
      "Gradient Descent(21/49): loss=0.3991702333906268\n",
      "Gradient Descent(22/49): loss=0.39780532201630603\n",
      "Gradient Descent(23/49): loss=0.39654471657538604\n",
      "Gradient Descent(24/49): loss=0.3953775350464427\n",
      "Gradient Descent(25/49): loss=0.3942941421626386\n",
      "Gradient Descent(26/49): loss=0.39328600044287476\n",
      "Gradient Descent(27/49): loss=0.3923455394258868\n",
      "Gradient Descent(28/49): loss=0.39146604085335585\n",
      "Gradient Descent(29/49): loss=0.3906415378298019\n",
      "Gradient Descent(30/49): loss=0.38986672623319857\n",
      "Gradient Descent(31/49): loss=0.3891368868654446\n",
      "Gradient Descent(32/49): loss=0.38844781701997144\n",
      "Gradient Descent(33/49): loss=0.38779577030829465\n",
      "Gradient Descent(34/49): loss=0.3871774037312277\n",
      "Gradient Descent(35/49): loss=0.3865897311063631\n",
      "Gradient Descent(36/49): loss=0.3860300820735716\n",
      "Gradient Descent(37/49): loss=0.38549606599665637\n",
      "Gradient Descent(38/49): loss=0.3849855401636511\n",
      "Gradient Descent(39/49): loss=0.384496581762092\n",
      "Gradient Descent(40/49): loss=0.3840274631702272\n",
      "Gradient Descent(41/49): loss=0.3835766301617281\n",
      "Gradient Descent(42/49): loss=0.38314268267101786\n",
      "Gradient Descent(43/49): loss=0.3827243578097369\n",
      "Gradient Descent(44/49): loss=0.3823205148628745\n",
      "Gradient Descent(45/49): loss=0.381930122026399\n",
      "Gradient Descent(46/49): loss=0.3815522446773877\n",
      "Gradient Descent(47/49): loss=0.38118603499322357\n",
      "Gradient Descent(48/49): loss=0.38083072275882324\n",
      "Gradient Descent(49/49): loss=0.3804856072204965\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48859135433989337\n",
      "Gradient Descent(2/49): loss=0.478451038349089\n",
      "Gradient Descent(3/49): loss=0.4694277299624645\n",
      "Gradient Descent(4/49): loss=0.4613886528617513\n",
      "Gradient Descent(5/49): loss=0.4542172659719519\n",
      "Gradient Descent(6/49): loss=0.44781124503662895\n",
      "Gradient Descent(7/49): loss=0.4420807186693831\n",
      "Gradient Descent(8/49): loss=0.43694672625494707\n",
      "Gradient Descent(9/49): loss=0.43233986936195157\n",
      "Gradient Descent(10/49): loss=0.4281991320318027\n",
      "Gradient Descent(11/49): loss=0.4244708485092603\n",
      "Gradient Descent(12/49): loss=0.4211077997518965\n",
      "Gradient Descent(13/49): loss=0.41806842245822873\n",
      "Gradient Descent(14/49): loss=0.4153161164393919\n",
      "Gradient Descent(15/49): loss=0.4128186379704963\n",
      "Gradient Descent(16/49): loss=0.41054756833263956\n",
      "Gradient Descent(17/49): loss=0.4084778481268647\n",
      "Gradient Descent(18/49): loss=0.4065873691345546\n",
      "Gradient Descent(19/49): loss=0.40485661653838817\n",
      "Gradient Descent(20/49): loss=0.403268355224307\n",
      "Gradient Descent(21/49): loss=0.40180735467545375\n",
      "Gradient Descent(22/49): loss=0.4004601476588344\n",
      "Gradient Descent(23/49): loss=0.39921481850761037\n",
      "Gradient Descent(24/49): loss=0.39806081732775295\n",
      "Gradient Descent(25/49): loss=0.3969887969171194\n",
      "Gradient Descent(26/49): loss=0.39599046958635387\n",
      "Gradient Descent(27/49): loss=0.39505848142179484\n",
      "Gradient Descent(28/49): loss=0.3941863018372219\n",
      "Gradient Descent(29/49): loss=0.3933681265294002\n",
      "Gradient Descent(30/49): loss=0.39259879218688026\n",
      "Gradient Descent(31/49): loss=0.3918737015066239\n",
      "Gradient Descent(32/49): loss=0.3911887572524692\n",
      "Gradient Descent(33/49): loss=0.39054030424646424\n",
      "Gradient Descent(34/49): loss=0.38992507832149664\n",
      "Gradient Descent(35/49): loss=0.38934016138390826\n",
      "Gradient Descent(36/49): loss=0.38878294184005\n",
      "Gradient Descent(37/49): loss=0.38825107973288875\n",
      "Gradient Descent(38/49): loss=0.3877424760154647\n",
      "Gradient Descent(39/49): loss=0.387255245458655\n",
      "Gradient Descent(40/49): loss=0.38678769275257296\n",
      "Gradient Descent(41/49): loss=0.38633829141513226\n",
      "Gradient Descent(42/49): loss=0.3859056651687701\n",
      "Gradient Descent(43/49): loss=0.38548857148792726\n",
      "Gradient Descent(44/49): loss=0.3850858870563028\n",
      "Gradient Descent(45/49): loss=0.38469659490485275\n",
      "Gradient Descent(46/49): loss=0.38431977302946896\n",
      "Gradient Descent(47/49): loss=0.3839545843118125\n",
      "Gradient Descent(48/49): loss=0.38360026758827204\n",
      "Gradient Descent(49/49): loss=0.38325612973087037\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4830077373942836\n",
      "Gradient Descent(2/49): loss=0.468805311958913\n",
      "Gradient Descent(3/49): loss=0.45690151582102184\n",
      "Gradient Descent(4/49): loss=0.44689406194844017\n",
      "Gradient Descent(5/49): loss=0.4384531758135973\n",
      "Gradient Descent(6/49): loss=0.43130827527150883\n",
      "Gradient Descent(7/49): loss=0.4252371416507277\n",
      "Gradient Descent(8/49): loss=0.4200571051967361\n",
      "Gradient Descent(9/49): loss=0.41561786249878924\n",
      "Gradient Descent(10/49): loss=0.4117956182999001\n",
      "Gradient Descent(11/49): loss=0.40848830356139093\n",
      "Gradient Descent(12/49): loss=0.405611669169076\n",
      "Gradient Descent(13/49): loss=0.4030960927749412\n",
      "Gradient Descent(14/49): loss=0.40088396692659223\n",
      "Gradient Descent(15/49): loss=0.3989275613684246\n",
      "Gradient Descent(16/49): loss=0.39718727239357005\n",
      "Gradient Descent(17/49): loss=0.39563018832132785\n",
      "Gradient Descent(18/49): loss=0.3942289133131955\n",
      "Gradient Descent(19/49): loss=0.392960602412604\n",
      "Gradient Descent(20/49): loss=0.3918061693713495\n",
      "Gradient Descent(21/49): loss=0.39074963588845457\n",
      "Gradient Descent(22/49): loss=0.3897775966397879\n",
      "Gradient Descent(23/49): loss=0.38887877916527075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=0.3880436815038882\n",
      "Gradient Descent(25/49): loss=0.3872642735861371\n",
      "Gradient Descent(26/49): loss=0.3865337509397112\n",
      "Gradient Descent(27/49): loss=0.38584633134330565\n",
      "Gradient Descent(28/49): loss=0.3851970867617161\n",
      "Gradient Descent(29/49): loss=0.3845818042831328\n",
      "Gradient Descent(30/49): loss=0.38399687091386053\n",
      "Gradient Descent(31/49): loss=0.38343917801318733\n",
      "Gradient Descent(32/49): loss=0.38290604190976224\n",
      "Gradient Descent(33/49): loss=0.3823951378615506\n",
      "Gradient Descent(34/49): loss=0.3819044450294846\n",
      "Gradient Descent(35/49): loss=0.3814322005508979\n",
      "Gradient Descent(36/49): loss=0.38097686113955187\n",
      "Gradient Descent(37/49): loss=0.3805370709182549\n",
      "Gradient Descent(38/49): loss=0.38011163441893536\n",
      "Gradient Descent(39/49): loss=0.3796994938727396\n",
      "Gradient Descent(40/49): loss=0.3792997100667387\n",
      "Gradient Descent(41/49): loss=0.37891144617027095\n",
      "Gradient Descent(42/49): loss=0.37853395403781\n",
      "Gradient Descent(43/49): loss=0.37816656258061887\n",
      "Gradient Descent(44/49): loss=0.37780866786966194\n",
      "Gradient Descent(45/49): loss=0.377459724690035\n",
      "Gradient Descent(46/49): loss=0.3771192393147764\n",
      "Gradient Descent(47/49): loss=0.3767867633051578\n",
      "Gradient Descent(48/49): loss=0.3764618881769278\n",
      "Gradient Descent(49/49): loss=0.37614424079872266\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.482924724839627\n",
      "Gradient Descent(2/49): loss=0.46866096042989913\n",
      "Gradient Descent(3/49): loss=0.45671143572696526\n",
      "Gradient Descent(4/49): loss=0.4466692984203074\n",
      "Gradient Descent(5/49): loss=0.4382013748064984\n",
      "Gradient Descent(6/49): loss=0.43103458374537984\n",
      "Gradient Descent(7/49): loss=0.4249448966440418\n",
      "Gradient Descent(8/49): loss=0.41974835669753857\n",
      "Gradient Descent(9/49): loss=0.4152937664524242\n",
      "Gradient Descent(10/49): loss=0.41145672887341844\n",
      "Gradient Descent(11/49): loss=0.4081347878078277\n",
      "Gradient Descent(12/49): loss=0.4052434623531747\n",
      "Gradient Descent(13/49): loss=0.4027130086767269\n",
      "Gradient Descent(14/49): loss=0.40048577427798665\n",
      "Gradient Descent(15/49): loss=0.39851403506314853\n",
      "Gradient Descent(16/49): loss=0.39675822612218953\n",
      "Gradient Descent(17/49): loss=0.3951854937199683\n",
      "Gradient Descent(18/49): loss=0.39376850949171843\n",
      "Gradient Descent(19/49): loss=0.39248449877650593\n",
      "Gradient Descent(20/49): loss=0.3913144439148937\n",
      "Gradient Descent(21/49): loss=0.39024243056907915\n",
      "Gradient Descent(22/49): loss=0.3892551110091837\n",
      "Gradient Descent(23/49): loss=0.3883412631016491\n",
      "Gradient Descent(24/49): loss=0.3874914276398083\n",
      "Gradient Descent(25/49): loss=0.3866976098386472\n",
      "Gradient Descent(26/49): loss=0.38595303341015935\n",
      "Gradient Descent(27/49): loss=0.3852519377517672\n",
      "Gradient Descent(28/49): loss=0.3845894105068097\n",
      "Gradient Descent(29/49): loss=0.3839612491652341\n",
      "Gradient Descent(30/49): loss=0.3833638465230696\n",
      "Gradient Descent(31/49): loss=0.3827940957588118\n",
      "Gradient Descent(32/49): loss=0.38224931165236326\n",
      "Gradient Descent(33/49): loss=0.38172716509939986\n",
      "Gradient Descent(34/49): loss=0.381225628586744\n",
      "Gradient Descent(35/49): loss=0.380742930713592\n",
      "Gradient Descent(36/49): loss=0.3802775181864168\n",
      "Gradient Descent(37/49): loss=0.37982802399603977\n",
      "Gradient Descent(38/49): loss=0.3793932407151476\n",
      "Gradient Descent(39/49): loss=0.37897209804274107\n",
      "Gradient Descent(40/49): loss=0.37856364387622465\n",
      "Gradient Descent(41/49): loss=0.37816702831829957\n",
      "Gradient Descent(42/49): loss=0.3777814901295478\n",
      "Gradient Descent(43/49): loss=0.3774063452227488\n",
      "Gradient Descent(44/49): loss=0.3770409768648998\n",
      "Gradient Descent(45/49): loss=0.3766848273104088\n",
      "Gradient Descent(46/49): loss=0.37633739063620936\n",
      "Gradient Descent(47/49): loss=0.3759982065884844\n",
      "Gradient Descent(48/49): loss=0.37566685528276766\n",
      "Gradient Descent(49/49): loss=0.3753429526256521\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4825927468739042\n",
      "Gradient Descent(2/49): loss=0.46805718242950106\n",
      "Gradient Descent(3/49): loss=0.4558858680953344\n",
      "Gradient Descent(4/49): loss=0.44566345421239645\n",
      "Gradient Descent(5/49): loss=0.43704967797938\n",
      "Gradient Descent(6/49): loss=0.42976555221456497\n",
      "Gradient Descent(7/49): loss=0.42358213332026\n",
      "Gradient Descent(8/49): loss=0.41831137781405225\n",
      "Gradient Descent(9/49): loss=0.4137986926810011\n",
      "Gradient Descent(10/49): loss=0.4099168611623874\n",
      "Gradient Descent(11/49): loss=0.40656108665148405\n",
      "Gradient Descent(12/49): loss=0.40364494635018766\n",
      "Gradient Descent(13/49): loss=0.4010970857525145\n",
      "Gradient Descent(14/49): loss=0.39885851680917733\n",
      "Gradient Descent(15/49): loss=0.3968804083187148\n",
      "Gradient Descent(16/49): loss=0.39512227788973214\n",
      "Gradient Descent(17/49): loss=0.39355051168127225\n",
      "Gradient Descent(18/49): loss=0.3921371518155513\n",
      "Gradient Descent(19/49): loss=0.39085890247824956\n",
      "Gradient Descent(20/49): loss=0.3896963147650037\n",
      "Gradient Descent(21/49): loss=0.38863311769202036\n",
      "Gradient Descent(22/49): loss=0.3876556687811001\n",
      "Gradient Descent(23/49): loss=0.3867525025112714\n",
      "Gradient Descent(24/49): loss=0.3859139589082663\n",
      "Gradient Descent(25/49): loss=0.38513187778752667\n",
      "Gradient Descent(26/49): loss=0.38439934681288174\n",
      "Gradient Descent(27/49): loss=0.38371049369242766\n",
      "Gradient Descent(28/49): loss=0.3830603145956661\n",
      "Gradient Descent(29/49): loss=0.3824445323149905\n",
      "Gradient Descent(30/49): loss=0.38185947886987126\n",
      "Gradient Descent(31/49): loss=0.3813019982122027\n",
      "Gradient Descent(32/49): loss=0.38076936547587786\n",
      "Gradient Descent(33/49): loss=0.38025921985500877\n",
      "Gradient Descent(34/49): loss=0.37976950871964804\n",
      "Gradient Descent(35/49): loss=0.3792984410068476\n",
      "Gradient Descent(36/49): loss=0.3788444482759012\n",
      "Gradient Descent(37/49): loss=0.3784061521039614\n",
      "Gradient Descent(38/49): loss=0.37798233673352005\n",
      "Gradient Descent(39/49): loss=0.3775719260760233\n",
      "Gradient Descent(40/49): loss=0.3771739643339067\n",
      "Gradient Descent(41/49): loss=0.37678759963292013\n",
      "Gradient Descent(42/49): loss=0.3764120701629466\n",
      "Gradient Descent(43/49): loss=0.3760466924128156\n",
      "Gradient Descent(44/49): loss=0.37569085115633943\n",
      "Gradient Descent(45/49): loss=0.37534399090576687\n",
      "Gradient Descent(46/49): loss=0.3750056085973656\n",
      "Gradient Descent(47/49): loss=0.37467524731379503\n",
      "Gradient Descent(48/49): loss=0.3743524908808581\n",
      "Gradient Descent(49/49): loss=0.37403695920338526\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4831402212023558\n",
      "Gradient Descent(2/49): loss=0.4690441468800159\n",
      "Gradient Descent(3/49): loss=0.45722571097922665\n",
      "Gradient Descent(4/49): loss=0.44728664133095813\n",
      "Gradient Descent(5/49): loss=0.4389003474888245\n",
      "Gradient Descent(6/49): loss=0.431798805995414\n",
      "Gradient Descent(7/49): loss=0.4257618763014806\n",
      "Gradient Descent(8/49): loss=0.42060859032107534\n",
      "Gradient Descent(9/49): loss=0.41619004630764456\n",
      "Gradient Descent(10/49): loss=0.4123836080658471\n",
      "Gradient Descent(11/49): loss=0.4090881670834604\n",
      "Gradient Descent(12/49): loss=0.4062202707850912\n",
      "Gradient Descent(13/49): loss=0.40371095697306575\n",
      "Gradient Descent(14/49): loss=0.40150316436333655\n",
      "Gradient Descent(15/49): loss=0.3995496133184069\n",
      "Gradient Descent(16/49): loss=0.39781107051825476\n",
      "Gradient Descent(17/49): loss=0.3962549272679643\n",
      "Gradient Descent(18/49): loss=0.39485403411839864\n",
      "Gradient Descent(19/49): loss=0.39358574503787874\n",
      "Gradient Descent(20/49): loss=0.3924311329734827\n",
      "Gradient Descent(21/49): loss=0.3913743456480006\n",
      "Gradient Descent(22/49): loss=0.39040207615045736\n",
      "Gradient Descent(23/49): loss=0.38950312753582894\n",
      "Gradient Descent(24/49): loss=0.3886680544489999\n",
      "Gradient Descent(25/49): loss=0.38788886788830657\n",
      "Gradient Descent(26/49): loss=0.3871587917545674\n",
      "Gradient Descent(27/49): loss=0.38647206189760785\n",
      "Gradient Descent(28/49): loss=0.3858237600596489\n",
      "Gradient Descent(29/49): loss=0.38520967649333565\n",
      "Gradient Descent(30/49): loss=0.3846261961585256\n",
      "Gradient Descent(31/49): loss=0.3840702043225695\n",
      "Gradient Descent(32/49): loss=0.3835390081415067\n",
      "Gradient Descent(33/49): loss=0.38303027141517065\n",
      "Gradient Descent(34/49): loss=0.38254196021279724\n",
      "Gradient Descent(35/49): loss=0.3820722974778647\n",
      "Gradient Descent(36/49): loss=0.38161972505828967\n",
      "Gradient Descent(37/49): loss=0.3811828718844355\n",
      "Gradient Descent(38/49): loss=0.3807605272437915\n",
      "Gradient Descent(39/49): loss=0.3803516182867682\n",
      "Gradient Descent(40/49): loss=0.3799551910502511\n",
      "Gradient Descent(41/49): loss=0.3795703944104439\n",
      "Gradient Descent(42/49): loss=0.37919646647906496\n",
      "Gradient Descent(43/49): loss=0.37883272304118937\n",
      "Gradient Descent(44/49): loss=0.378478547702281\n",
      "Gradient Descent(45/49): loss=0.37813338346891956\n",
      "Gradient Descent(46/49): loss=0.3777967255346293\n",
      "Gradient Descent(47/49): loss=0.37746811508086436\n",
      "Gradient Descent(48/49): loss=0.37714713393507737\n",
      "Gradient Descent(49/49): loss=0.37683339995411175\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4776844739923415\n",
      "Gradient Descent(2/49): loss=0.4601695283511431\n",
      "Gradient Descent(3/49): loss=0.4463480491944753\n",
      "Gradient Descent(4/49): loss=0.43537531011090286\n",
      "Gradient Descent(5/49): loss=0.42660563802997653\n",
      "Gradient Descent(6/49): loss=0.4195446445243058\n",
      "Gradient Descent(7/49): loss=0.4138131060370636\n",
      "Gradient Descent(8/49): loss=0.4091195967584379\n",
      "Gradient Descent(9/49): loss=0.4052397184014255\n",
      "Gradient Descent(10/49): loss=0.4020003141780792\n",
      "Gradient Descent(11/49): loss=0.3992674557884067\n",
      "Gradient Descent(12/49): loss=0.3969372910148515\n",
      "Gradient Descent(13/49): loss=0.3949290629675995\n",
      "Gradient Descent(14/49): loss=0.39317977979047253\n",
      "Gradient Descent(15/49): loss=0.3916401399690274\n",
      "Gradient Descent(16/49): loss=0.39027141373395546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=0.3890430531503597\n",
      "Gradient Descent(18/49): loss=0.38793085807488703\n",
      "Gradient Descent(19/49): loss=0.38691556654528186\n",
      "Gradient Descent(20/49): loss=0.38598176956566344\n",
      "Gradient Descent(21/49): loss=0.385117074093056\n",
      "Gradient Descent(22/49): loss=0.38431145614744483\n",
      "Gradient Descent(23/49): loss=0.3835567597425155\n",
      "Gradient Descent(24/49): loss=0.3828463078141526\n",
      "Gradient Descent(25/49): loss=0.3821745993016471\n",
      "Gradient Descent(26/49): loss=0.38153707261349495\n",
      "Gradient Descent(27/49): loss=0.38092992034157364\n",
      "Gradient Descent(28/49): loss=0.3803499436204298\n",
      "Gradient Descent(29/49): loss=0.37979443722514233\n",
      "Gradient Descent(30/49): loss=0.3792610985613631\n",
      "Gradient Descent(31/49): loss=0.3787479552763772\n",
      "Gradient Descent(32/49): loss=0.3782533074257019\n",
      "Gradient Descent(33/49): loss=0.37777568105357917\n",
      "Gradient Descent(34/49): loss=0.37731379075446675\n",
      "Gradient Descent(35/49): loss=0.37686650932710614\n",
      "Gradient Descent(36/49): loss=0.3764328430516489\n",
      "Gradient Descent(37/49): loss=0.3760119114431485\n",
      "Gradient Descent(38/49): loss=0.37560293058396677\n",
      "Gradient Descent(39/49): loss=0.37520519933045227\n",
      "Gradient Descent(40/49): loss=0.3748180878387347\n",
      "Gradient Descent(41/49): loss=0.3744410279706574\n",
      "Gradient Descent(42/49): loss=0.3740735052313807\n",
      "Gradient Descent(43/49): loss=0.3737150519609309\n",
      "Gradient Descent(44/49): loss=0.3733652415573912\n",
      "Gradient Descent(45/49): loss=0.3730236835530266\n",
      "Gradient Descent(46/49): loss=0.3726900193990205\n",
      "Gradient Descent(47/49): loss=0.3723639188417347\n",
      "Gradient Descent(48/49): loss=0.3720450767950573\n",
      "Gradient Descent(49/49): loss=0.37173321063067805\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4775765970466673\n",
      "Gradient Descent(2/49): loss=0.45999027471567\n",
      "Gradient Descent(3/49): loss=0.44612052679120706\n",
      "Gradient Descent(4/49): loss=0.4351135906751922\n",
      "Gradient Descent(5/49): loss=0.4263178374741784\n",
      "Gradient Descent(6/49): loss=0.41923506574946623\n",
      "Gradient Descent(7/49): loss=0.413483704275065\n",
      "Gradient Descent(8/49): loss=0.40877096248258693\n",
      "Gradient Descent(9/49): loss=0.40487172139030503\n",
      "Gradient Descent(10/49): loss=0.4016125129011485\n",
      "Gradient Descent(11/49): loss=0.3988593467446546\n",
      "Gradient Descent(12/49): loss=0.3965084508782327\n",
      "Gradient Descent(13/49): loss=0.39447922054069795\n",
      "Gradient Descent(14/49): loss=0.3927088433527731\n",
      "Gradient Descent(15/49): loss=0.3911481974666087\n",
      "Gradient Descent(16/49): loss=0.3897587175086905\n",
      "Gradient Descent(17/49): loss=0.3885099968868253\n",
      "Gradient Descent(18/49): loss=0.38737795086296156\n",
      "Gradient Descent(19/49): loss=0.38634340705801534\n",
      "Gradient Descent(20/49): loss=0.3853910220757832\n",
      "Gradient Descent(21/49): loss=0.3845084472103858\n",
      "Gradient Descent(22/49): loss=0.3836856846197439\n",
      "Gradient Descent(23/49): loss=0.3829145893286898\n",
      "Gradient Descent(24/49): loss=0.38218848304433634\n",
      "Gradient Descent(25/49): loss=0.38150185383634555\n",
      "Gradient Descent(26/49): loss=0.380850121871208\n",
      "Gradient Descent(27/49): loss=0.38022945605873676\n",
      "Gradient Descent(28/49): loss=0.37963663002396014\n",
      "Gradient Descent(29/49): loss=0.3790689085262827\n",
      "Gradient Descent(30/49): loss=0.3785239575133109\n",
      "Gradient Descent(31/49): loss=0.37799977257321943\n",
      "Gradient Descent(32/49): loss=0.3774946217539591\n",
      "Gradient Descent(33/49): loss=0.3770069996387985\n",
      "Gradient Descent(34/49): loss=0.37653559027311123\n",
      "Gradient Descent(35/49): loss=0.37607923707827073\n",
      "Gradient Descent(36/49): loss=0.37563691830394985\n",
      "Gradient Descent(37/49): loss=0.37520772688972737\n",
      "Gradient Descent(38/49): loss=0.3747908538532247\n",
      "Gradient Descent(39/49): loss=0.37438557451224413\n",
      "Gradient Descent(40/49): loss=0.37399123699565034\n",
      "Gradient Descent(41/49): loss=0.3736072526120149\n",
      "Gradient Descent(42/49): loss=0.3732330877339748\n",
      "Gradient Descent(43/49): loss=0.37286825692565667\n",
      "Gradient Descent(44/49): loss=0.372512317094856\n",
      "Gradient Descent(45/49): loss=0.37216486249435077\n",
      "Gradient Descent(46/49): loss=0.37182552043038863\n",
      "Gradient Descent(47/49): loss=0.3714939475630326\n",
      "Gradient Descent(48/49): loss=0.37116982670423015\n",
      "Gradient Descent(49/49): loss=0.3708528640363762\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4771412761287201\n",
      "Gradient Descent(2/49): loss=0.4592233533325462\n",
      "Gradient Descent(3/49): loss=0.4451026843084033\n",
      "Gradient Descent(4/49): loss=0.4339074528629499\n",
      "Gradient Descent(5/49): loss=0.4249719266489613\n",
      "Gradient Descent(6/49): loss=0.41778691038243676\n",
      "Gradient Descent(7/49): loss=0.4119622764079237\n",
      "Gradient Descent(8/49): loss=0.4071985824099082\n",
      "Gradient Descent(9/49): loss=0.4032655428027407\n",
      "Gradient Descent(10/49): loss=0.3999856791169133\n",
      "Gradient Descent(11/49): loss=0.3972218899079014\n",
      "Gradient Descent(12/49): loss=0.3948679907563153\n",
      "Gradient Descent(13/49): loss=0.3928415073296821\n",
      "Gradient Descent(14/49): loss=0.391078179198056\n",
      "Gradient Descent(15/49): loss=0.389527763756629\n",
      "Gradient Descent(16/49): loss=0.3881508289995285\n",
      "Gradient Descent(17/49): loss=0.38691629902535785\n",
      "Gradient Descent(18/49): loss=0.3857995730196234\n",
      "Gradient Descent(19/49): loss=0.38478108153472423\n",
      "Gradient Descent(20/49): loss=0.38384517654332406\n",
      "Gradient Descent(21/49): loss=0.3829792765128311\n",
      "Gradient Descent(22/49): loss=0.38217320655163484\n",
      "Gradient Descent(23/49): loss=0.38141868795789874\n",
      "Gradient Descent(24/49): loss=0.3807089423528702\n",
      "Gradient Descent(25/49): loss=0.3800383838307179\n",
      "Gradient Descent(26/49): loss=0.3794023788329437\n",
      "Gradient Descent(27/49): loss=0.3787970582326498\n",
      "Gradient Descent(28/49): loss=0.3782191697526944\n",
      "Gradient Descent(29/49): loss=0.3776659616153502\n",
      "Gradient Descent(30/49): loss=0.3771350904368705\n",
      "Gradient Descent(31/49): loss=0.37662454799578404\n",
      "Gradient Descent(32/49): loss=0.37613260273833987\n",
      "Gradient Descent(33/49): loss=0.37565775282908087\n",
      "Gradient Descent(34/49): loss=0.37519868827809244\n",
      "Gradient Descent(35/49): loss=0.37475426023145403\n",
      "Gradient Descent(36/49): loss=0.37432345593778893\n",
      "Gradient Descent(37/49): loss=0.37390537823186226\n",
      "Gradient Descent(38/49): loss=0.37349922862908774\n",
      "Gradient Descent(39/49): loss=0.3731042933201678\n",
      "Gradient Descent(40/49): loss=0.37271993150633315\n",
      "Gradient Descent(41/49): loss=0.37234556563303417\n",
      "Gradient Descent(42/49): loss=0.37198067317128164\n",
      "Gradient Descent(43/49): loss=0.3716247796671168\n",
      "Gradient Descent(44/49): loss=0.371277452835508\n",
      "Gradient Descent(45/49): loss=0.3709382975188072\n",
      "Gradient Descent(46/49): loss=0.3706069513644664\n",
      "Gradient Descent(47/49): loss=0.37028308110406566\n",
      "Gradient Descent(48/49): loss=0.36996637933744053\n",
      "Gradient Descent(49/49): loss=0.3696565617430386\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.477857881193162\n",
      "Gradient Descent(2/49): loss=0.46047159146901656\n",
      "Gradient Descent(3/49): loss=0.44674544799746846\n",
      "Gradient Descent(4/49): loss=0.435842949482821\n",
      "Gradient Descent(5/49): loss=0.4271245093369753\n",
      "Gradient Descent(6/49): loss=0.42010033920920553\n",
      "Gradient Descent(7/49): loss=0.41439474286352923\n",
      "Gradient Descent(8/49): loss=0.4097190228027982\n",
      "Gradient Descent(9/49): loss=0.40585089939824154\n",
      "Gradient Descent(10/49): loss=0.4026188613910149\n",
      "Gradient Descent(11/49): loss=0.3998902548451935\n",
      "Gradient Descent(12/49): loss=0.3975622089975176\n",
      "Gradient Descent(13/49): loss=0.3955547167337044\n",
      "Gradient Descent(14/49): loss=0.3938053528090176\n",
      "Gradient Descent(15/49): loss=0.3922652378754568\n",
      "Gradient Descent(16/49): loss=0.39089595089143214\n",
      "Gradient Descent(17/49): loss=0.3896671640573953\n",
      "Gradient Descent(18/49): loss=0.3885548286586317\n",
      "Gradient Descent(19/49): loss=0.3875397813293066\n",
      "Gradient Descent(20/49): loss=0.38660667146509575\n",
      "Gradient Descent(21/49): loss=0.3857431342104204\n",
      "Gradient Descent(22/49): loss=0.38493915144873075\n",
      "Gradient Descent(23/49): loss=0.3841865569064478\n",
      "Gradient Descent(24/49): loss=0.3834786518850861\n",
      "Gradient Descent(25/49): loss=0.38280990605140186\n",
      "Gradient Descent(26/49): loss=0.38217572374075526\n",
      "Gradient Descent(27/49): loss=0.3815722608183351\n",
      "Gradient Descent(28/49): loss=0.38099628064094926\n",
      "Gradient Descent(29/49): loss=0.380445040330259\n",
      "Gradient Descent(30/49): loss=0.379916200605158\n",
      "Gradient Descent(31/49): loss=0.3794077539772685\n",
      "Gradient Descent(32/49): loss=0.37891796730383276\n",
      "Gradient Descent(33/49): loss=0.37844533560369237\n",
      "Gradient Descent(34/49): loss=0.37798854474079363\n",
      "Gradient Descent(35/49): loss=0.3775464411161068\n",
      "Gradient Descent(36/49): loss=0.3771180069213244\n",
      "Gradient Descent(37/49): loss=0.3767023398254142\n",
      "Gradient Descent(38/49): loss=0.3762986362102716\n",
      "Gradient Descent(39/49): loss=0.37590617726130465\n",
      "Gradient Descent(40/49): loss=0.37552431736573433\n",
      "Gradient Descent(41/49): loss=0.3751524743855667\n",
      "Gradient Descent(42/49): loss=0.374790121461155\n",
      "Gradient Descent(43/49): loss=0.37443678007078335\n",
      "Gradient Descent(44/49): loss=0.37409201412620313\n",
      "Gradient Descent(45/49): loss=0.37375542492690944\n",
      "Gradient Descent(46/49): loss=0.3734266468297971\n",
      "Gradient Descent(47/49): loss=0.37310534351764546\n",
      "Gradient Descent(48/49): loss=0.3727912047712282\n",
      "Gradient Descent(49/49): loss=0.37248394366689574\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.472531622657048\n",
      "Gradient Descent(2/49): loss=0.45231970010030786\n",
      "Gradient Descent(3/49): loss=0.4373098643962595\n",
      "Gradient Descent(4/49): loss=0.4260451832035256\n",
      "Gradient Descent(5/49): loss=0.41748935526246816\n",
      "Gradient Descent(6/49): loss=0.41090309594008323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=0.40575737894778785\n",
      "Gradient Descent(8/49): loss=0.4016723669894701\n",
      "Gradient Descent(9/49): loss=0.3983743237374851\n",
      "Gradient Descent(10/49): loss=0.3956651513029204\n",
      "Gradient Descent(11/49): loss=0.39340081363516105\n",
      "Gradient Descent(12/49): loss=0.3914760257942402\n",
      "Gradient Descent(13/49): loss=0.3898133687667562\n",
      "Gradient Descent(14/49): loss=0.3883555346975207\n",
      "Gradient Descent(15/49): loss=0.3870597896924038\n",
      "Gradient Descent(16/49): loss=0.38589400994508766\n",
      "Gradient Descent(17/49): loss=0.38483383595364906\n",
      "Gradient Descent(18/49): loss=0.38386062276815847\n",
      "Gradient Descent(19/49): loss=0.38295995814362616\n",
      "Gradient Descent(20/49): loss=0.38212058679221744\n",
      "Gradient Descent(21/49): loss=0.38133362579771696\n",
      "Gradient Descent(22/49): loss=0.3805919894117675\n",
      "Gradient Descent(23/49): loss=0.37988996493290594\n",
      "Gradient Descent(24/49): loss=0.37922289801919373\n",
      "Gradient Descent(25/49): loss=0.37858695760699024\n",
      "Gradient Descent(26/49): loss=0.3779789590151555\n",
      "Gradient Descent(27/49): loss=0.37739622980274196\n",
      "Gradient Descent(28/49): loss=0.37683650722310513\n",
      "Gradient Descent(29/49): loss=0.37629785917581193\n",
      "Gradient Descent(30/49): loss=0.37577862275161183\n",
      "Gradient Descent(31/49): loss=0.3752773560440995\n",
      "Gradient Descent(32/49): loss=0.37479280004102694\n",
      "Gradient Descent(33/49): loss=0.3743238482337119\n",
      "Gradient Descent(34/49): loss=0.3738695221835935\n",
      "Gradient Descent(35/49): loss=0.37342895172398965\n",
      "Gradient Descent(36/49): loss=0.37300135879762\n",
      "Gradient Descent(37/49): loss=0.37258604416871044\n",
      "Gradient Descent(38/49): loss=0.37218237642554025\n",
      "Gradient Descent(39/49): loss=0.37178978282170766\n",
      "Gradient Descent(40/49): loss=0.37140774160408196\n",
      "Gradient Descent(41/49): loss=0.37103577555100153\n",
      "Gradient Descent(42/49): loss=0.37067344650200573\n",
      "Gradient Descent(43/49): loss=0.370320350704806\n",
      "Gradient Descent(44/49): loss=0.36997611483962445\n",
      "Gradient Descent(45/49): loss=0.3696403926079158\n",
      "Gradient Descent(46/49): loss=0.3693128617936272\n",
      "Gradient Descent(47/49): loss=0.3689932217218999\n",
      "Gradient Descent(48/49): loss=0.36868119105347813\n",
      "Gradient Descent(49/49): loss=0.36837650586380344\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.472400284550623\n",
      "Gradient Descent(2/49): loss=0.45211095594799905\n",
      "Gradient Descent(3/49): loss=0.4370533253534374\n",
      "Gradient Descent(4/49): loss=0.42575590055649587\n",
      "Gradient Descent(5/49): loss=0.41717404067278135\n",
      "Gradient Descent(6/49): loss=0.4105639735531957\n",
      "Gradient Descent(7/49): loss=0.4053944953950161\n",
      "Gradient Descent(8/49): loss=0.40128492070879723\n",
      "Gradient Descent(9/49): loss=0.39796138556904154\n",
      "Gradient Descent(10/49): loss=0.39522601770688737\n",
      "Gradient Descent(11/49): loss=0.39293514612202424\n",
      "Gradient Descent(12/49): loss=0.39098387232324383\n",
      "Gradient Descent(13/49): loss=0.38929512538242567\n",
      "Gradient Descent(14/49): loss=0.38781188176518966\n",
      "Gradient Descent(15/49): loss=0.386491622108375\n",
      "Gradient Descent(16/49): loss=0.38530237149833596\n",
      "Gradient Descent(17/49): loss=0.38421986251924434\n",
      "Gradient Descent(18/49): loss=0.3832254958490894\n",
      "Gradient Descent(19/49): loss=0.3823048685586569\n",
      "Gradient Descent(20/49): loss=0.3814467074622427\n",
      "Gradient Descent(21/49): loss=0.38064209225023715\n",
      "Gradient Descent(22/49): loss=0.37988388657693845\n",
      "Gradient Descent(23/49): loss=0.3791663189071069\n",
      "Gradient Descent(24/49): loss=0.3784846716409416\n",
      "Gradient Descent(25/49): loss=0.3778350488781377\n",
      "Gradient Descent(26/49): loss=0.3772142015821393\n",
      "Gradient Descent(27/49): loss=0.376619394875785\n",
      "Gradient Descent(28/49): loss=0.3760483064509745\n",
      "Gradient Descent(29/49): loss=0.37549894810954043\n",
      "Gradient Descent(30/49): loss=0.3749696046242592\n",
      "Gradient Descent(31/49): loss=0.37445878566789614\n",
      "Gradient Descent(32/49): loss=0.3739651876811273\n",
      "Gradient Descent(33/49): loss=0.3734876633621971\n",
      "Gradient Descent(34/49): loss=0.37302519705091336\n",
      "Gradient Descent(35/49): loss=0.3725768847099785\n",
      "Gradient Descent(36/49): loss=0.3721419175224427\n",
      "Gradient Descent(37/49): loss=0.37171956835712344\n",
      "Gradient Descent(38/49): loss=0.3713091805269333\n",
      "Gradient Descent(39/49): loss=0.3709101583944903\n",
      "Gradient Descent(40/49): loss=0.37052195947686933\n",
      "Gradient Descent(41/49): loss=0.37014408777532437\n",
      "Gradient Descent(42/49): loss=0.36977608811237156\n",
      "Gradient Descent(43/49): loss=0.36941754130222015\n",
      "Gradient Descent(44/49): loss=0.36906806001439824\n",
      "Gradient Descent(45/49): loss=0.3687272852169345\n",
      "Gradient Descent(46/49): loss=0.3683948831063661\n",
      "Gradient Descent(47/49): loss=0.3680705424484665\n",
      "Gradient Descent(48/49): loss=0.3677539722668873\n",
      "Gradient Descent(49/49): loss=0.36744489982762163\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47186527886529317\n",
      "Gradient Descent(2/49): loss=0.45119798593702687\n",
      "Gradient Descent(3/49): loss=0.43587644256934116\n",
      "Gradient Descent(4/49): loss=0.4243976577829987\n",
      "Gradient Descent(5/49): loss=0.4156940144388304\n",
      "Gradient Descent(6/49): loss=0.4090050295382568\n",
      "Gradient Descent(7/49): loss=0.40378736871034776\n",
      "Gradient Descent(8/49): loss=0.3996515523067998\n",
      "Gradient Descent(9/49): loss=0.3963173471028129\n",
      "Gradient Descent(10/49): loss=0.3935822714689679\n",
      "Gradient Descent(11/49): loss=0.3912993214009894\n",
      "Gradient Descent(12/49): loss=0.3893611907147129\n",
      "Gradient Descent(13/49): loss=0.3876890715468587\n",
      "Gradient Descent(14/49): loss=0.3862246897068641\n",
      "Gradient Descent(15/49): loss=0.3849246277840529\n",
      "Gradient Descent(16/49): loss=0.3837562685602873\n",
      "Gradient Descent(17/49): loss=0.38269488784024863\n",
      "Gradient Descent(18/49): loss=0.38172156412034686\n",
      "Gradient Descent(19/49): loss=0.38082166992765115\n",
      "Gradient Descent(20/49): loss=0.37998377832657504\n",
      "Gradient Descent(21/49): loss=0.3791988665382426\n",
      "Gradient Descent(22/49): loss=0.37845973283135453\n",
      "Gradient Descent(23/49): loss=0.3777605670305031\n",
      "Gradient Descent(24/49): loss=0.37709663210653865\n",
      "Gradient Descent(25/49): loss=0.37646402644540605\n",
      "Gradient Descent(26/49): loss=0.375859505002523\n",
      "Gradient Descent(27/49): loss=0.3752803436718573\n",
      "Gradient Descent(28/49): loss=0.3747242355602372\n",
      "Gradient Descent(29/49): loss=0.3741892109715951\n",
      "Gradient Descent(30/49): loss=0.3736735751352736\n",
      "Gradient Descent(31/49): loss=0.3731758593133279\n",
      "Gradient Descent(32/49): loss=0.3726947820750903\n",
      "Gradient Descent(33/49): loss=0.3722292183613819\n",
      "Gradient Descent(34/49): loss=0.3717781745665976\n",
      "Gradient Descent(35/49): loss=0.3713407683090352\n",
      "Gradient Descent(36/49): loss=0.3709162118842165\n",
      "Gradient Descent(37/49): loss=0.3705037986353002\n",
      "Gradient Descent(38/49): loss=0.37010289165240234\n",
      "Gradient Descent(39/49): loss=0.36971291434547937\n",
      "Gradient Descent(40/49): loss=0.36933334253541905\n",
      "Gradient Descent(41/49): loss=0.3689636977838097\n",
      "Gradient Descent(42/49): loss=0.3686035417398044\n",
      "Gradient Descent(43/49): loss=0.3682524713271053\n",
      "Gradient Descent(44/49): loss=0.3679101146287277\n",
      "Gradient Descent(45/49): loss=0.36757612735427636\n",
      "Gradient Descent(46/49): loss=0.3672501897958114\n",
      "Gradient Descent(47/49): loss=0.3669320041953167\n",
      "Gradient Descent(48/49): loss=0.36662129246032615\n",
      "Gradient Descent(49/49): loss=0.36631779417515997\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4727443343123119\n",
      "Gradient Descent(2/49): loss=0.45267780725534823\n",
      "Gradient Descent(3/49): loss=0.43776697799206543\n",
      "Gradient Descent(4/49): loss=0.42656880260147295\n",
      "Gradient Descent(5/49): loss=0.4180566074280679\n",
      "Gradient Descent(6/49): loss=0.41149797145573824\n",
      "Gradient Descent(7/49): loss=0.40636883414854436\n",
      "Gradient Descent(8/49): loss=0.40229296751444993\n",
      "Gradient Descent(9/49): loss=0.3989992511556221\n",
      "Gradient Descent(10/49): loss=0.3962914667173192\n",
      "Gradient Descent(11/49): loss=0.3940269097504948\n",
      "Gradient Descent(12/49): loss=0.39210122000128206\n",
      "Gradient Descent(13/49): loss=0.3904376027117175\n",
      "Gradient Descent(14/49): loss=0.3889791544308282\n",
      "Gradient Descent(15/49): loss=0.38768338665980984\n",
      "Gradient Descent(16/49): loss=0.3865183076946539\n",
      "Gradient Descent(17/49): loss=0.3854596109636335\n",
      "Gradient Descent(18/49): loss=0.38448865053487746\n",
      "Gradient Descent(19/49): loss=0.38359097779040513\n",
      "Gradient Descent(20/49): loss=0.3827552791043994\n",
      "Gradient Descent(21/49): loss=0.3819726008567955\n",
      "Gradient Descent(22/49): loss=0.381235780974806\n",
      "Gradient Descent(23/49): loss=0.3805390294458738\n",
      "Gradient Descent(24/49): loss=0.3798776167160059\n",
      "Gradient Descent(25/49): loss=0.37924764057060856\n",
      "Gradient Descent(26/49): loss=0.37864585039520576\n",
      "Gradient Descent(27/49): loss=0.3780695136208667\n",
      "Gradient Descent(28/49): loss=0.3775163133722451\n",
      "Gradient Descent(29/49): loss=0.37698426934790413\n",
      "Gradient Descent(30/49): loss=0.3764716761214016\n",
      "Gradient Descent(31/49): loss=0.37597705460375636\n",
      "Gradient Descent(32/49): loss=0.3754991135277559\n",
      "Gradient Descent(33/49): loss=0.37503671862567145\n",
      "Gradient Descent(34/49): loss=0.3745888677619976\n",
      "Gradient Descent(35/49): loss=0.37415467071415737\n",
      "Gradient Descent(36/49): loss=0.37373333261110026\n",
      "Gradient Descent(37/49): loss=0.3733241402740279\n",
      "Gradient Descent(38/49): loss=0.37292645087777143\n",
      "Gradient Descent(39/49): loss=0.37253968248185537\n",
      "Gradient Descent(40/49): loss=0.3721633060787082\n",
      "Gradient Descent(41/49): loss=0.37179683888125165\n",
      "Gradient Descent(42/49): loss=0.3714398386293387\n",
      "Gradient Descent(43/49): loss=0.3710918987386626\n",
      "Gradient Descent(44/49): loss=0.3707526441500794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=0.37042172776417503\n",
      "Gradient Descent(46/49): loss=0.37009882736711813\n",
      "Gradient Descent(47/49): loss=0.36978364297071337\n",
      "Gradient Descent(48/49): loss=0.36947589450305884\n",
      "Gradient Descent(49/49): loss=0.3691753197970952\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46754918338840273\n",
      "Gradient Descent(2/49): loss=0.44520168266102034\n",
      "Gradient Descent(3/49): loss=0.42958763740389305\n",
      "Gradient Descent(4/49): loss=0.4184912583267776\n",
      "Gradient Descent(5/49): loss=0.4104489319694311\n",
      "Gradient Descent(6/49): loss=0.40448939287074726\n",
      "Gradient Descent(7/49): loss=0.3999648389657038\n",
      "Gradient Descent(8/49): loss=0.39644072914060263\n",
      "Gradient Descent(9/49): loss=0.3936236466690701\n",
      "Gradient Descent(10/49): loss=0.39131394623671223\n",
      "Gradient Descent(11/49): loss=0.389374581898206\n",
      "Gradient Descent(12/49): loss=0.38771052432278985\n",
      "Gradient Descent(13/49): loss=0.38625512346952906\n",
      "Gradient Descent(14/49): loss=0.38496103727590036\n",
      "Gradient Descent(15/49): loss=0.3837941697816781\n",
      "Gradient Descent(16/49): loss=0.3827295985561293\n",
      "Gradient Descent(17/49): loss=0.38174882156893986\n",
      "Gradient Descent(18/49): loss=0.3808378826871749\n",
      "Gradient Descent(19/49): loss=0.3799860849747247\n",
      "Gradient Descent(20/49): loss=0.37918509936052963\n",
      "Gradient Descent(21/49): loss=0.37842834090337374\n",
      "Gradient Descent(22/49): loss=0.3777105274699285\n",
      "Gradient Descent(23/49): loss=0.37702736376623175\n",
      "Gradient Descent(24/49): loss=0.3763753122907138\n",
      "Gradient Descent(25/49): loss=0.3757514251595081\n",
      "Gradient Descent(26/49): loss=0.37515321902050486\n",
      "Gradient Descent(27/49): loss=0.3745785808171007\n",
      "Gradient Descent(28/49): loss=0.3740256959027335\n",
      "Gradient Descent(29/49): loss=0.37349299254651863\n",
      "Gradient Descent(30/49): loss=0.37297909860680944\n",
      "Gradient Descent(31/49): loss=0.3724828073467195\n",
      "Gradient Descent(32/49): loss=0.3720030501984771\n",
      "Gradient Descent(33/49): loss=0.37153887486846887\n",
      "Gradient Descent(34/49): loss=0.3710894275900027\n",
      "Gradient Descent(35/49): loss=0.37065393862868184\n",
      "Gradient Descent(36/49): loss=0.37023171036136376\n",
      "Gradient Descent(37/49): loss=0.369822107408223\n",
      "Gradient Descent(38/49): loss=0.36942454841504424\n",
      "Gradient Descent(39/49): loss=0.36903849917109083\n",
      "Gradient Descent(40/49): loss=0.36866346681473416\n",
      "Gradient Descent(41/49): loss=0.36829899493019186\n",
      "Gradient Descent(42/49): loss=0.3679446593782318\n",
      "Gradient Descent(43/49): loss=0.36760006473447493\n",
      "Gradient Descent(44/49): loss=0.36726484123308667\n",
      "Gradient Descent(45/49): loss=0.36693864213274247\n",
      "Gradient Descent(46/49): loss=0.36662114143694896\n",
      "Gradient Descent(47/49): loss=0.36631203191295897\n",
      "Gradient Descent(48/49): loss=0.36601102336330443\n",
      "Gradient Descent(49/49): loss=0.36571784111188044\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46739578735149445\n",
      "Gradient Descent(2/49): loss=0.4449681946827772\n",
      "Gradient Descent(3/49): loss=0.4293083763376341\n",
      "Gradient Descent(4/49): loss=0.4181800838986904\n",
      "Gradient Descent(5/49): loss=0.41010978621833327\n",
      "Gradient Descent(6/49): loss=0.40412202164562216\n",
      "Gradient Descent(7/49): loss=0.3995677176191647\n",
      "Gradient Descent(8/49): loss=0.39601242645461665\n",
      "Gradient Descent(9/49): loss=0.39316335397510405\n",
      "Gradient Descent(10/49): loss=0.39082158710375275\n",
      "Gradient Descent(11/49): loss=0.3888507319536221\n",
      "Gradient Descent(12/49): loss=0.38715626273409864\n",
      "Gradient Descent(13/49): loss=0.38567187640318584\n",
      "Gradient Descent(14/49): loss=0.3843504400338584\n",
      "Gradient Descent(15/49): loss=0.3831579566322437\n",
      "Gradient Descent(16/49): loss=0.3820695205744148\n",
      "Gradient Descent(17/49): loss=0.3810665890150127\n",
      "Gradient Descent(18/49): loss=0.38013512723723497\n",
      "Gradient Descent(19/49): loss=0.3792643371667355\n",
      "Gradient Descent(20/49): loss=0.37844577720280964\n",
      "Gradient Descent(21/49): loss=0.377672746347664\n",
      "Gradient Descent(22/49): loss=0.3769398481868213\n",
      "Gradient Descent(23/49): loss=0.3762426783028725\n",
      "Gradient Descent(24/49): loss=0.3755775972153179\n",
      "Gradient Descent(25/49): loss=0.3749415632079868\n",
      "Gradient Descent(26/49): loss=0.37433200757203766\n",
      "Gradient Descent(27/49): loss=0.3737467402557976\n",
      "Gradient Descent(28/49): loss=0.3731838775888869\n",
      "Gradient Descent(29/49): loss=0.37264178623843924\n",
      "Gradient Descent(30/49): loss=0.3721190392552287\n",
      "Gradient Descent(31/49): loss=0.3716143812379889\n",
      "Gradient Descent(32/49): loss=0.37112670045780255\n",
      "Gradient Descent(33/49): loss=0.37065500635583204\n",
      "Gradient Descent(34/49): loss=0.37019841123340463\n",
      "Gradient Descent(35/49): loss=0.36975611524493585\n",
      "Gradient Descent(36/49): loss=0.3693273940160519\n",
      "Gradient Descent(37/49): loss=0.3689115883651361\n",
      "Gradient Descent(38/49): loss=0.3685080957225457\n",
      "Gradient Descent(39/49): loss=0.3681163629290897\n",
      "Gradient Descent(40/49): loss=0.3677358801618356\n",
      "Gradient Descent(41/49): loss=0.36736617578641556\n",
      "Gradient Descent(42/49): loss=0.36700681197465884\n",
      "Gradient Descent(43/49): loss=0.36665738095741507\n",
      "Gradient Descent(44/49): loss=0.3663175018069086\n",
      "Gradient Descent(45/49): loss=0.36598681766240376\n",
      "Gradient Descent(46/49): loss=0.3656649933284989\n",
      "Gradient Descent(47/49): loss=0.36535171318785226\n",
      "Gradient Descent(48/49): loss=0.36504667938022933\n",
      "Gradient Descent(49/49): loss=0.3647496102079475\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46676475508362375\n",
      "Gradient Descent(2/49): loss=0.4439251474888744\n",
      "Gradient Descent(3/49): loss=0.42800142431256566\n",
      "Gradient Descent(4/49): loss=0.4167088881649146\n",
      "Gradient Descent(5/49): loss=0.4085411065531708\n",
      "Gradient Descent(6/49): loss=0.402500354445021\n",
      "Gradient Descent(7/49): loss=0.397922526556857\n",
      "Gradient Descent(8/49): loss=0.39436303905603415\n",
      "Gradient Descent(9/49): loss=0.3915222710642346\n",
      "Gradient Descent(10/49): loss=0.38919671754345564\n",
      "Gradient Descent(11/49): loss=0.3872469013091658\n",
      "Gradient Descent(12/49): loss=0.3855762319234884\n",
      "Gradient Descent(13/49): loss=0.3841170297455914\n",
      "Gradient Descent(14/49): loss=0.38282125019212027\n",
      "Gradient Descent(15/49): loss=0.38165429889531094\n",
      "Gradient Descent(16/49): loss=0.38059088530442353\n",
      "Gradient Descent(17/49): loss=0.3796122251794737\n",
      "Gradient Descent(18/49): loss=0.37870413924166996\n",
      "Gradient Descent(19/49): loss=0.377855749996403\n",
      "Gradient Descent(20/49): loss=0.37705858002800663\n",
      "Gradient Descent(21/49): loss=0.3763059214746751\n",
      "Gradient Descent(22/49): loss=0.37559239002786193\n",
      "Gradient Descent(23/49): loss=0.3749136055453364\n",
      "Gradient Descent(24/49): loss=0.3742659603595574\n",
      "Gradient Descent(25/49): loss=0.3736464489566873\n",
      "Gradient Descent(26/49): loss=0.3730525410874656\n",
      "Gradient Descent(27/49): loss=0.37248208598279225\n",
      "Gradient Descent(28/49): loss=0.37193323912364695\n",
      "Gradient Descent(29/49): loss=0.37140440557367504\n",
      "Gradient Descent(30/49): loss=0.37089419562934134\n",
      "Gradient Descent(31/49): loss=0.37040138974486964\n",
      "Gradient Descent(32/49): loss=0.36992491052461396\n",
      "Gradient Descent(33/49): loss=0.36946380016193053\n",
      "Gradient Descent(34/49): loss=0.36901720211975914\n",
      "Gradient Descent(35/49): loss=0.3685843461468026\n",
      "Gradient Descent(36/49): loss=0.36816453594008847\n",
      "Gradient Descent(37/49): loss=0.36775713892408307\n",
      "Gradient Descent(38/49): loss=0.36736157773500294\n",
      "Gradient Descent(39/49): loss=0.36697732308806136\n",
      "Gradient Descent(40/49): loss=0.3666038877730758\n",
      "Gradient Descent(41/49): loss=0.36624082157584\n",
      "Gradient Descent(42/49): loss=0.36588770696292316\n",
      "Gradient Descent(43/49): loss=0.3655441553990332\n",
      "Gradient Descent(44/49): loss=0.3652098041908517\n",
      "Gradient Descent(45/49): loss=0.36488431377090824\n",
      "Gradient Descent(46/49): loss=0.3645673653507351\n",
      "Gradient Descent(47/49): loss=0.3642586588851345\n",
      "Gradient Descent(48/49): loss=0.3639579112995378\n",
      "Gradient Descent(49/49): loss=0.36366485494066797\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46779958055980564\n",
      "Gradient Descent(2/49): loss=0.44560921159738237\n",
      "Gradient Descent(3/49): loss=0.43009290939469896\n",
      "Gradient Descent(4/49): loss=0.4190556456115396\n",
      "Gradient Descent(5/49): loss=0.41104733276838606\n",
      "Gradient Descent(6/49): loss=0.4051058001817418\n",
      "Gradient Descent(7/49): loss=0.4005894204174439\n",
      "Gradient Descent(8/49): loss=0.3970678232272215\n",
      "Gradient Descent(9/49): loss=0.3942503649700131\n",
      "Gradient Descent(10/49): loss=0.3919391967363144\n",
      "Gradient Descent(11/49): loss=0.3899983914068858\n",
      "Gradient Descent(12/49): loss=0.3883335744837122\n",
      "Gradient Descent(13/49): loss=0.38687843874070216\n",
      "Gradient Descent(14/49): loss=0.3855857801512736\n",
      "Gradient Descent(15/49): loss=0.3844215108357333\n",
      "Gradient Descent(16/49): loss=0.38336063798270936\n",
      "Gradient Descent(17/49): loss=0.38238454557365525\n",
      "Gradient Descent(18/49): loss=0.38147914298295765\n",
      "Gradient Descent(19/49): loss=0.38063359317478235\n",
      "Gradient Descent(20/49): loss=0.3798394306089734\n",
      "Gradient Descent(21/49): loss=0.3790899428957053\n",
      "Gradient Descent(22/49): loss=0.3783797322929944\n",
      "Gradient Descent(23/49): loss=0.37770440087906787\n",
      "Gradient Descent(24/49): loss=0.37706032158366376\n",
      "Gradient Descent(25/49): loss=0.3764444694492555\n",
      "Gradient Descent(26/49): loss=0.37585429562116723\n",
      "Gradient Descent(27/49): loss=0.37528763201392384\n",
      "Gradient Descent(28/49): loss=0.37474261827483785\n",
      "Gradient Descent(29/49): loss=0.37421764515956973\n",
      "Gradient Descent(30/49): loss=0.373711310140153\n",
      "Gradient Descent(31/49): loss=0.37322238224273574\n",
      "Gradient Descent(32/49): loss=0.3727497739317808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.37229251843401634\n",
      "Gradient Descent(34/49): loss=0.3718497513054695\n",
      "Gradient Descent(35/49): loss=0.37142069533988087\n",
      "Gradient Descent(36/49): loss=0.3710046481314533\n",
      "Gradient Descent(37/49): loss=0.37060097176293366\n",
      "Gradient Descent(38/49): loss=0.3702090842077464\n",
      "Gradient Descent(39/49): loss=0.3698284521235501\n",
      "Gradient Descent(40/49): loss=0.36945858478207194\n",
      "Gradient Descent(41/49): loss=0.3690990289319488\n",
      "Gradient Descent(42/49): loss=0.36874936443154577\n",
      "Gradient Descent(43/49): loss=0.3684092005202115\n",
      "Gradient Descent(44/49): loss=0.36807817262124326\n",
      "Gradient Descent(45/49): loss=0.3677559395895416\n",
      "Gradient Descent(46/49): loss=0.3674421813326633\n",
      "Gradient Descent(47/49): loss=0.3671365967466236\n",
      "Gradient Descent(48/49): loss=0.36683890191799845\n",
      "Gradient Descent(49/49): loss=0.3665488285521545\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4627371561864058\n",
      "Gradient Descent(2/49): loss=0.4387633421293793\n",
      "Gradient Descent(3/49): loss=0.4230032228054702\n",
      "Gradient Descent(4/49): loss=0.4123708959703214\n",
      "Gradient Descent(5/49): loss=0.404977483816434\n",
      "Gradient Descent(6/49): loss=0.3996586858650931\n",
      "Gradient Descent(7/49): loss=0.3956910890684321\n",
      "Gradient Descent(8/49): loss=0.39262096690495585\n",
      "Gradient Descent(9/49): loss=0.39016053236034437\n",
      "Gradient Descent(10/49): loss=0.38812481055159337\n",
      "Gradient Descent(11/49): loss=0.3863930546659194\n",
      "Gradient Descent(12/49): loss=0.38488503923867134\n",
      "Gradient Descent(13/49): loss=0.3835464028793846\n",
      "Gradient Descent(14/49): loss=0.3823395176471143\n",
      "Gradient Descent(15/49): loss=0.38123774987607956\n",
      "Gradient Descent(16/49): loss=0.38022181426799545\n",
      "Gradient Descent(17/49): loss=0.3792774290389928\n",
      "Gradient Descent(18/49): loss=0.3783937865051998\n",
      "Gradient Descent(19/49): loss=0.3775625398084211\n",
      "Gradient Descent(20/49): loss=0.3767771201020435\n",
      "Gradient Descent(21/49): loss=0.376032268097407\n",
      "Gradient Descent(22/49): loss=0.3753237067015828\n",
      "Gradient Descent(23/49): loss=0.3746479080066267\n",
      "Gradient Descent(24/49): loss=0.37400192444492103\n",
      "Gradient Descent(25/49): loss=0.3733832643465287\n",
      "Gradient Descent(26/49): loss=0.3727897987621552\n",
      "Gradient Descent(27/49): loss=0.3722196906798353\n",
      "Gradient Descent(28/49): loss=0.37167134054353024\n",
      "Gradient Descent(29/49): loss=0.3711433438203065\n",
      "Gradient Descent(30/49): loss=0.3706344575973036\n",
      "Gradient Descent(31/49): loss=0.3701435740319826\n",
      "Gradient Descent(32/49): loss=0.36966969906313857\n",
      "Gradient Descent(33/49): loss=0.36921193520157164\n",
      "Gradient Descent(34/49): loss=0.3687694675135904\n",
      "Gradient Descent(35/49): loss=0.36834155212408887\n",
      "Gradient Descent(36/49): loss=0.3679275067229913\n",
      "Gradient Descent(37/49): loss=0.3675267026757538\n",
      "Gradient Descent(38/49): loss=0.3671385584265763\n",
      "Gradient Descent(39/49): loss=0.3667625339498001\n",
      "Gradient Descent(40/49): loss=0.366398126056185\n",
      "Gradient Descent(41/49): loss=0.36604486440030665\n",
      "Gradient Descent(42/49): loss=0.36570230806607273\n",
      "Gradient Descent(43/49): loss=0.36537004263142764\n",
      "Gradient Descent(44/49): loss=0.3650476776322591\n",
      "Gradient Descent(45/49): loss=0.36473484436051556\n",
      "Gradient Descent(46/49): loss=0.3644311939434654\n",
      "Gradient Descent(47/49): loss=0.3641363956605673\n",
      "Gradient Descent(48/49): loss=0.36385013546206985\n",
      "Gradient Descent(49/49): loss=0.36357211465963485\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4625631054492814\n",
      "Gradient Descent(2/49): loss=0.4385092272296935\n",
      "Gradient Descent(3/49): loss=0.4227057477876143\n",
      "Gradient Descent(4/49): loss=0.41204080510826563\n",
      "Gradient Descent(5/49): loss=0.4046153019695887\n",
      "Gradient Descent(6/49): loss=0.3992620486616662\n",
      "Gradient Descent(7/49): loss=0.39525778929837263\n",
      "Gradient Descent(8/49): loss=0.39214994803738223\n",
      "Gradient Descent(9/49): loss=0.3896519803364668\n",
      "Gradient Descent(10/49): loss=0.3875799096705741\n",
      "Gradient Descent(11/49): loss=0.3858136678298602\n",
      "Gradient Descent(12/49): loss=0.38427341940373394\n",
      "Gradient Descent(13/49): loss=0.3829049693619667\n",
      "Gradient Descent(14/49): loss=0.3816706987319566\n",
      "Gradient Descent(15/49): loss=0.38054388110172926\n",
      "Gradient Descent(16/49): loss=0.37950507900449154\n",
      "Gradient Descent(17/49): loss=0.3785398290876255\n",
      "Gradient Descent(18/49): loss=0.37763713283148465\n",
      "Gradient Descent(19/49): loss=0.3767884560036462\n",
      "Gradient Descent(20/49): loss=0.3759870533065717\n",
      "Gradient Descent(21/49): loss=0.37522750379500025\n",
      "Gradient Descent(22/49): loss=0.3745053850352094\n",
      "Gradient Descent(23/49): loss=0.37381704014855954\n",
      "Gradient Descent(24/49): loss=0.3731594081605808\n",
      "Gradient Descent(25/49): loss=0.3725298982958487\n",
      "Gradient Descent(26/49): loss=0.3719262953429684\n",
      "Gradient Descent(27/49): loss=0.371346687379337\n",
      "Gradient Descent(28/49): loss=0.3707894098585881\n",
      "Gradient Descent(29/49): loss=0.3702530018581935\n",
      "Gradient Descent(30/49): loss=0.3697361714911704\n",
      "Gradient Descent(31/49): loss=0.36923776831082816\n",
      "Gradient Descent(32/49): loss=0.3687567611113521\n",
      "Gradient Descent(33/49): loss=0.3682922199329483\n",
      "Gradient Descent(34/49): loss=0.3678433013720518\n",
      "Gradient Descent(35/49): loss=0.3674092365099659\n",
      "Gradient Descent(36/49): loss=0.3669893209307314\n",
      "Gradient Descent(37/49): loss=0.3665829064168676\n",
      "Gradient Descent(38/49): loss=0.36618939400080924\n",
      "Gradient Descent(39/49): loss=0.36580822811799046\n",
      "Gradient Descent(40/49): loss=0.36543889166000876\n",
      "Gradient Descent(41/49): loss=0.36508090176703306\n",
      "Gradient Descent(42/49): loss=0.36473380623043333\n",
      "Gradient Descent(43/49): loss=0.3643971804016154\n",
      "Gradient Descent(44/49): loss=0.36407062452279104\n",
      "Gradient Descent(45/49): loss=0.36375376141110255\n",
      "Gradient Descent(46/49): loss=0.3634462344400254\n",
      "Gradient Descent(47/49): loss=0.3631477057720028\n",
      "Gradient Descent(48/49): loss=0.36285785480433086\n",
      "Gradient Descent(49/49): loss=0.3625763767968327\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46183970478371134\n",
      "Gradient Descent(2/49): loss=0.4373509874027096\n",
      "Gradient Descent(3/49): loss=0.4212938441102717\n",
      "Gradient Descent(4/49): loss=0.4104881911760929\n",
      "Gradient Descent(5/49): loss=0.402991932275602\n",
      "Gradient Descent(6/49): loss=0.3976108859358841\n",
      "Gradient Descent(7/49): loss=0.3936048907436822\n",
      "Gradient Descent(8/49): loss=0.3905107993445434\n",
      "Gradient Descent(9/49): loss=0.38803546813329626\n",
      "Gradient Descent(10/49): loss=0.38599081244021594\n",
      "Gradient Descent(11/49): loss=0.38425421851968933\n",
      "Gradient Descent(12/49): loss=0.3827442863726061\n",
      "Gradient Descent(13/49): loss=0.38140587258220354\n",
      "Gradient Descent(14/49): loss=0.38020079695833175\n",
      "Gradient Descent(15/49): loss=0.37910201496873824\n",
      "Gradient Descent(16/49): loss=0.3780899233087157\n",
      "Gradient Descent(17/49): loss=0.37714998769937136\n",
      "Gradient Descent(18/49): loss=0.37627119728676456\n",
      "Gradient Descent(19/49): loss=0.37544504106364196\n",
      "Gradient Descent(20/49): loss=0.37466481789773054\n",
      "Gradient Descent(21/49): loss=0.37392516267512504\n",
      "Gradient Descent(22/49): loss=0.3732217145938379\n",
      "Gradient Descent(23/49): loss=0.37255088052231045\n",
      "Gradient Descent(24/49): loss=0.37190966306289747\n",
      "Gradient Descent(25/49): loss=0.37129553346112093\n",
      "Gradient Descent(26/49): loss=0.3707063361641989\n",
      "Gradient Descent(27/49): loss=0.3701402161114419\n",
      "Gradient Descent(28/49): loss=0.3695955626251564\n",
      "Gradient Descent(29/49): loss=0.3690709656121287\n",
      "Gradient Descent(30/49): loss=0.3685651810225707\n",
      "Gradient Descent(31/49): loss=0.36807710335816535\n",
      "Gradient Descent(32/49): loss=0.3676057436076514\n",
      "Gradient Descent(33/49): loss=0.36715021140284226\n",
      "Gradient Descent(34/49): loss=0.3667097004853906\n",
      "Gradient Descent(35/49): loss=0.3662834767912199\n",
      "Gradient Descent(36/49): loss=0.3658708686194548\n",
      "Gradient Descent(37/49): loss=0.3654712584721774\n",
      "Gradient Descent(38/49): loss=0.36508407624161104\n",
      "Gradient Descent(39/49): loss=0.3647087934901624\n",
      "Gradient Descent(40/49): loss=0.3643449186216979\n",
      "Gradient Descent(41/49): loss=0.36399199278345007\n",
      "Gradient Descent(42/49): loss=0.3636495863699261\n",
      "Gradient Descent(43/49): loss=0.3633172960252978\n",
      "Gradient Descent(44/49): loss=0.3629947420605375\n",
      "Gradient Descent(45/49): loss=0.3626815662172639\n",
      "Gradient Descent(46/49): loss=0.3623774297227535\n",
      "Gradient Descent(47/49): loss=0.36208201159058384\n",
      "Gradient Descent(48/49): loss=0.3617950071293985\n",
      "Gradient Descent(49/49): loss=0.3615161266287791\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4630236199356432\n",
      "Gradient Descent(2/49): loss=0.4392142040726052\n",
      "Gradient Descent(3/49): loss=0.42354678645198934\n",
      "Gradient Descent(4/49): loss=0.4129640101178784\n",
      "Gradient Descent(5/49): loss=0.4055945106397895\n",
      "Gradient Descent(6/49): loss=0.4002850540310548\n",
      "Gradient Descent(7/49): loss=0.3963192065630738\n",
      "Gradient Descent(8/49): loss=0.39324754847882076\n",
      "Gradient Descent(9/49): loss=0.3907848479134211\n",
      "Gradient Descent(10/49): loss=0.3887475513748815\n",
      "Gradient Descent(11/49): loss=0.3870156153602623\n",
      "Gradient Descent(12/49): loss=0.38550907664711875\n",
      "Gradient Descent(13/49): loss=0.3841735768234952\n",
      "Gradient Descent(14/49): loss=0.38297134870859034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=0.3818755507580326\n",
      "Gradient Descent(16/49): loss=0.38086666599376995\n",
      "Gradient Descent(17/49): loss=0.3799301832915191\n",
      "Gradient Descent(18/49): loss=0.3790550821704067\n",
      "Gradient Descent(19/49): loss=0.37823282627544963\n",
      "Gradient Descent(20/49): loss=0.37745668281317846\n",
      "Gradient Descent(21/49): loss=0.37672125373764115\n",
      "Gradient Descent(22/49): loss=0.37602214662041195\n",
      "Gradient Descent(23/49): loss=0.3753557392105426\n",
      "Gradient Descent(24/49): loss=0.37471900794800783\n",
      "Gradient Descent(25/49): loss=0.3741094009250964\n",
      "Gradient Descent(26/49): loss=0.3735247422977839\n",
      "Gradient Descent(27/49): loss=0.37296315933950225\n",
      "Gradient Descent(28/49): loss=0.3724230260653556\n",
      "Gradient Descent(29/49): loss=0.3719029191678483\n",
      "Gradient Descent(30/49): loss=0.3714015832261797\n",
      "Gradient Descent(31/49): loss=0.37091790298722466\n",
      "Gradient Descent(32/49): loss=0.3704508810984825\n",
      "Gradient Descent(33/49): loss=0.36999962008534587\n",
      "Gradient Descent(34/49): loss=0.3695633076613174\n",
      "Gradient Descent(35/49): loss=0.369141204675955\n",
      "Gradient Descent(36/49): loss=0.3687326351651334\n",
      "Gradient Descent(37/49): loss=0.3683369780877881\n",
      "Gradient Descent(38/49): loss=0.3679536604237429\n",
      "Gradient Descent(39/49): loss=0.3675821513762526\n",
      "Gradient Descent(40/49): loss=0.3672219574760399\n",
      "Gradient Descent(41/49): loss=0.3668726184248121\n",
      "Gradient Descent(42/49): loss=0.3665337035484093\n",
      "Gradient Descent(43/49): loss=0.36620480875499284\n",
      "Gradient Descent(44/49): loss=0.36588555391362093\n",
      "Gradient Descent(45/49): loss=0.365575580584377\n",
      "Gradient Descent(46/49): loss=0.36527455004382675\n",
      "Gradient Descent(47/49): loss=0.3649821415596789\n",
      "Gradient Descent(48/49): loss=0.36469805087664336\n",
      "Gradient Descent(49/49): loss=0.36442198888203703\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4580955410510573\n",
      "Gradient Descent(2/49): loss=0.4329545552429705\n",
      "Gradient Descent(3/49): loss=0.41739805188400136\n",
      "Gradient Descent(4/49): loss=0.4074011977290764\n",
      "Gradient Descent(5/49): loss=0.40068604007588116\n",
      "Gradient Descent(6/49): loss=0.3959499882023969\n",
      "Gradient Descent(7/49): loss=0.39243899541772737\n",
      "Gradient Descent(8/49): loss=0.389709946703056\n",
      "Gradient Descent(9/49): loss=0.38749763053855063\n",
      "Gradient Descent(10/49): loss=0.3856398041468398\n",
      "Gradient Descent(11/49): loss=0.38403467133354763\n",
      "Gradient Descent(12/49): loss=0.38261653264640955\n",
      "Gradient Descent(13/49): loss=0.3813416836898135\n",
      "Gradient Descent(14/49): loss=0.38018013498621817\n",
      "Gradient Descent(15/49): loss=0.3791106689726185\n",
      "Gradient Descent(16/49): loss=0.3781178315821795\n",
      "Gradient Descent(17/49): loss=0.3771900607702357\n",
      "Gradient Descent(18/49): loss=0.376318494176412\n",
      "Gradient Descent(19/49): loss=0.37549619017458724\n",
      "Gradient Descent(20/49): loss=0.3747176059260863\n",
      "Gradient Descent(21/49): loss=0.3739782389106455\n",
      "Gradient Descent(22/49): loss=0.37327437495533716\n",
      "Gradient Descent(23/49): loss=0.3726029073229632\n",
      "Gradient Descent(24/49): loss=0.37196120432499685\n",
      "Gradient Descent(25/49): loss=0.3713470107968698\n",
      "Gradient Descent(26/49): loss=0.37075837367512016\n",
      "Gradient Descent(27/49): loss=0.3701935850344878\n",
      "Gradient Descent(28/49): loss=0.3696511379716811\n",
      "Gradient Descent(29/49): loss=0.36912969207149265\n",
      "Gradient Descent(30/49): loss=0.3686280461069727\n",
      "Gradient Descent(31/49): loss=0.3681451162596488\n",
      "Gradient Descent(32/49): loss=0.3676799185927503\n",
      "Gradient Descent(33/49): loss=0.36723155483031755\n",
      "Gradient Descent(34/49): loss=0.3667992007271922\n",
      "Gradient Descent(35/49): loss=0.36638209648531594\n",
      "Gradient Descent(36/49): loss=0.3659795387982008\n",
      "Gradient Descent(37/49): loss=0.36559087420009473\n",
      "Gradient Descent(38/49): loss=0.3652154934678218\n",
      "Gradient Descent(39/49): loss=0.36485282687761256\n",
      "Gradient Descent(40/49): loss=0.3645023401608574\n",
      "Gradient Descent(41/49): loss=0.36416353103477367\n",
      "Gradient Descent(42/49): loss=0.3638359262088633\n",
      "Gradient Descent(43/49): loss=0.36351907878743034\n",
      "Gradient Descent(44/49): loss=0.3632125660036652\n",
      "Gradient Descent(45/49): loss=0.3629159872328183\n",
      "Gradient Descent(46/49): loss=0.36262896224153013\n",
      "Gradient Descent(47/49): loss=0.36235112963800253\n",
      "Gradient Descent(48/49): loss=0.3620821454938034\n",
      "Gradient Descent(49/49): loss=0.36182168211304\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.457902238843984\n",
      "Gradient Descent(2/49): loss=0.4326833356522351\n",
      "Gradient Descent(3/49): loss=0.4170853963359924\n",
      "Gradient Descent(4/49): loss=0.40705329652616945\n",
      "Gradient Descent(5/49): loss=0.40030008122624783\n",
      "Gradient Descent(6/49): loss=0.3955224038222378\n",
      "Gradient Descent(7/49): loss=0.39196798164398233\n",
      "Gradient Descent(8/49): loss=0.38919571786798135\n",
      "Gradient Descent(9/49): loss=0.3869419392312842\n",
      "Gradient Descent(10/49): loss=0.3850453483276956\n",
      "Gradient Descent(11/49): loss=0.3834046025744205\n",
      "Gradient Descent(12/49): loss=0.38195411039721183\n",
      "Gradient Descent(13/49): loss=0.3806500588469542\n",
      "Gradient Descent(14/49): loss=0.3794622301250853\n",
      "Gradient Descent(15/49): loss=0.37836912414493074\n",
      "Gradient Descent(16/49): loss=0.37735499131758027\n",
      "Gradient Descent(17/49): loss=0.37640798499381817\n",
      "Gradient Descent(18/49): loss=0.37551898155494207\n",
      "Gradient Descent(19/49): loss=0.3746808066667177\n",
      "Gradient Descent(20/49): loss=0.3738877142441745\n",
      "Gradient Descent(21/49): loss=0.3731350265245479\n",
      "Gradient Descent(22/49): loss=0.37241887947789704\n",
      "Gradient Descent(23/49): loss=0.37173603884543205\n",
      "Gradient Descent(24/49): loss=0.3710837646866821\n",
      "Gradient Descent(25/49): loss=0.37045970999282724\n",
      "Gradient Descent(26/49): loss=0.3698618437057215\n",
      "Gradient Descent(27/49): loss=0.36928839153094245\n",
      "Gradient Descent(28/49): loss=0.36873778992336753\n",
      "Gradient Descent(29/49): loss=0.36820864995342434\n",
      "Gradient Descent(30/49): loss=0.36769972867025197\n",
      "Gradient Descent(31/49): loss=0.3672099062107976\n",
      "Gradient Descent(32/49): loss=0.36673816735278786\n",
      "Gradient Descent(33/49): loss=0.36628358653298526\n",
      "Gradient Descent(34/49): loss=0.36584531558838285\n",
      "Gradient Descent(35/49): loss=0.3654225736525138\n",
      "Gradient Descent(36/49): loss=0.36501463876927775\n",
      "Gradient Descent(37/49): loss=0.36462084088468805\n",
      "Gradient Descent(38/49): loss=0.36424055595126725\n",
      "Gradient Descent(39/49): loss=0.3638732009365746\n",
      "Gradient Descent(40/49): loss=0.3635182295709629\n",
      "Gradient Descent(41/49): loss=0.36317512870338187\n",
      "Gradient Descent(42/49): loss=0.36284341516026153\n",
      "Gradient Descent(43/49): loss=0.36252263302300797\n",
      "Gradient Descent(44/49): loss=0.36221235125575846\n",
      "Gradient Descent(45/49): loss=0.36191216162777595\n",
      "Gradient Descent(46/49): loss=0.3616216768849868\n",
      "Gradient Descent(47/49): loss=0.361340529133242\n",
      "Gradient Descent(48/49): loss=0.36106836840237555\n",
      "Gradient Descent(49/49): loss=0.3608048613653679\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45709012796555626\n",
      "Gradient Descent(2/49): loss=0.4314237372618427\n",
      "Gradient Descent(3/49): loss=0.4155901897554321\n",
      "Gradient Descent(4/49): loss=0.4054444237816799\n",
      "Gradient Descent(5/49): loss=0.39864704141060797\n",
      "Gradient Descent(6/49): loss=0.3938642490180616\n",
      "Gradient Descent(7/49): loss=0.3903261282196958\n",
      "Gradient Descent(8/49): loss=0.3875813890603273\n",
      "Gradient Descent(9/49): loss=0.38536048726344624\n",
      "Gradient Descent(10/49): loss=0.38349876618917217\n",
      "Gradient Descent(11/49): loss=0.3818929853746942\n",
      "Gradient Descent(12/49): loss=0.3804765063814394\n",
      "Gradient Descent(13/49): loss=0.3792049649903007\n",
      "Gradient Descent(14/49): loss=0.37804788016543406\n",
      "Gradient Descent(15/49): loss=0.3769836550160081\n",
      "Gradient Descent(16/49): loss=0.37599653811691885\n",
      "Gradient Descent(17/49): loss=0.3750747338398432\n",
      "Gradient Descent(18/49): loss=0.3742091975993736\n",
      "Gradient Descent(19/49): loss=0.3733928474695226\n",
      "Gradient Descent(20/49): loss=0.37262003457648823\n",
      "Gradient Descent(21/49): loss=0.3718861782211053\n",
      "Gradient Descent(22/49): loss=0.3711875085093261\n",
      "Gradient Descent(23/49): loss=0.37052088091312363\n",
      "Gradient Descent(24/49): loss=0.36988364012057723\n",
      "Gradient Descent(25/49): loss=0.3692735184158782\n",
      "Gradient Descent(26/49): loss=0.36868855873586553\n",
      "Gradient Descent(27/49): loss=0.3681270556734438\n",
      "Gradient Descent(28/49): loss=0.3675875097342148\n",
      "Gradient Descent(29/49): loss=0.36706859151051546\n",
      "Gradient Descent(30/49): loss=0.3665691133626483\n",
      "Gradient Descent(31/49): loss=0.36608800684078685\n",
      "Gradient Descent(32/49): loss=0.36562430453675054\n",
      "Gradient Descent(33/49): loss=0.36517712538255637\n",
      "Gradient Descent(34/49): loss=0.3647456626515056\n",
      "Gradient Descent(35/49): loss=0.3643291740936693\n",
      "Gradient Descent(36/49): loss=0.36392697376878913\n",
      "Gradient Descent(37/49): loss=0.3635384252381271\n",
      "Gradient Descent(38/49): loss=0.3631629358513669\n",
      "Gradient Descent(39/49): loss=0.36279995192152054\n",
      "Gradient Descent(40/49): loss=0.36244895462439486\n",
      "Gradient Descent(41/49): loss=0.36210945649282555\n",
      "Gradient Descent(42/49): loss=0.3617809984020137\n",
      "Gradient Descent(43/49): loss=0.3614631469626781\n",
      "Gradient Descent(44/49): loss=0.3611554922547519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=0.36085764584696645\n",
      "Gradient Descent(46/49): loss=0.36056923905769017\n",
      "Gradient Descent(47/49): loss=0.3602899214203751\n",
      "Gradient Descent(48/49): loss=0.3600193593233682\n",
      "Gradient Descent(49/49): loss=0.3597572347990052\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4584164524398245\n",
      "Gradient Descent(2/49): loss=0.43344316647761705\n",
      "Gradient Descent(3/49): loss=0.4179715165594702\n",
      "Gradient Descent(4/49): loss=0.4080136053379354\n",
      "Gradient Descent(5/49): loss=0.4013128233269203\n",
      "Gradient Descent(6/49): loss=0.3965791613420868\n",
      "Gradient Descent(7/49): loss=0.39306583378930476\n",
      "Gradient Descent(8/49): loss=0.390333713783851\n",
      "Gradient Descent(9/49): loss=0.3881195961673771\n",
      "Gradient Descent(10/49): loss=0.38626208088993685\n",
      "Gradient Descent(11/49): loss=0.38465956140557733\n",
      "Gradient Descent(12/49): loss=0.3832461861444695\n",
      "Gradient Descent(13/49): loss=0.3819779412271898\n",
      "Gradient Descent(14/49): loss=0.38082447550618626\n",
      "Gradient Descent(15/49): loss=0.3797642143673284\n",
      "Gradient Descent(16/49): loss=0.3787813793033526\n",
      "Gradient Descent(17/49): loss=0.3778641277937199\n",
      "Gradient Descent(18/49): loss=0.3770033631185969\n",
      "Gradient Descent(19/49): loss=0.37619195280638595\n",
      "Gradient Descent(20/49): loss=0.3754242019236102\n",
      "Gradient Descent(21/49): loss=0.3746954891425681\n",
      "Gradient Descent(22/49): loss=0.37400200938796785\n",
      "Gradient Descent(23/49): loss=0.3733405880054668\n",
      "Gradient Descent(24/49): loss=0.37270854407018567\n",
      "Gradient Descent(25/49): loss=0.3721035882006712\n",
      "Gradient Descent(26/49): loss=0.3715237450812027\n",
      "Gradient Descent(27/49): loss=0.37096729398491635\n",
      "Gradient Descent(28/49): loss=0.3704327226096351\n",
      "Gradient Descent(29/49): loss=0.3699186908885347\n",
      "Gradient Descent(30/49): loss=0.36942400236020495\n",
      "Gradient Descent(31/49): loss=0.36894758132537486\n",
      "Gradient Descent(32/49): loss=0.36848845447331124\n",
      "Gradient Descent(33/49): loss=0.3680457359890776\n",
      "Gradient Descent(34/49): loss=0.3676186153923152\n",
      "Gradient Descent(35/49): loss=0.36720634753496784\n",
      "Gradient Descent(36/49): loss=0.36680824431713366\n",
      "Gradient Descent(37/49): loss=0.36642366777930496\n",
      "Gradient Descent(38/49): loss=0.366052024304322\n",
      "Gradient Descent(39/49): loss=0.3656927597196395\n",
      "Gradient Descent(40/49): loss=0.3653453551344836\n",
      "Gradient Descent(41/49): loss=0.3650093233804453\n",
      "Gradient Descent(42/49): loss=0.36468420595045226\n",
      "Gradient Descent(43/49): loss=0.3643695703516762\n",
      "Gradient Descent(44/49): loss=0.3640650078041308\n",
      "Gradient Descent(45/49): loss=0.3637701312295072\n",
      "Gradient Descent(46/49): loss=0.3634845734849416\n",
      "Gradient Descent(47/49): loss=0.36320798580451946\n",
      "Gradient Descent(48/49): loss=0.362940036417812\n",
      "Gradient Descent(49/49): loss=0.3626804093199851\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45362433798235724\n",
      "Gradient Descent(2/49): loss=0.42772720938086567\n",
      "Gradient Descent(3/49): loss=0.412631596191645\n",
      "Gradient Descent(4/49): loss=0.40335012013543076\n",
      "Gradient Descent(5/49): loss=0.39727850722959573\n",
      "Gradient Descent(6/49): loss=0.39303653129654453\n",
      "Gradient Descent(7/49): loss=0.3898790092597372\n",
      "Gradient Descent(8/49): loss=0.38739410202220803\n",
      "Gradient Descent(9/49): loss=0.38534755219383354\n",
      "Gradient Descent(10/49): loss=0.3836014976276167\n",
      "Gradient Descent(11/49): loss=0.3820716378270085\n",
      "Gradient Descent(12/49): loss=0.3807042802568874\n",
      "Gradient Descent(13/49): loss=0.3794637964343865\n",
      "Gradient Descent(14/49): loss=0.3783256009323092\n",
      "Gradient Descent(15/49): loss=0.3772721098361056\n",
      "Gradient Descent(16/49): loss=0.3762903400332125\n",
      "Gradient Descent(17/49): loss=0.3753704346860232\n",
      "Gradient Descent(18/49): loss=0.3745047264756153\n",
      "Gradient Descent(19/49): loss=0.37368712287860434\n",
      "Gradient Descent(20/49): loss=0.3729126905666403\n",
      "Gradient Descent(21/49): loss=0.3721773668843182\n",
      "Gradient Descent(22/49): loss=0.37147775487679213\n",
      "Gradient Descent(23/49): loss=0.3708109747464341\n",
      "Gradient Descent(24/49): loss=0.3701745543322005\n",
      "Gradient Descent(25/49): loss=0.36956634712991143\n",
      "Gradient Descent(26/49): loss=0.3689844700926673\n",
      "Gradient Descent(27/49): loss=0.3684272558534965\n",
      "Gradient Descent(28/49): loss=0.3678932156035788\n",
      "Gradient Descent(29/49): loss=0.36738100993673606\n",
      "Gradient Descent(30/49): loss=0.36688942571427197\n",
      "Gradient Descent(31/49): loss=0.3664173575255679\n",
      "Gradient Descent(32/49): loss=0.3659637926904938\n",
      "Gradient Descent(33/49): loss=0.3655277990163652\n",
      "Gradient Descent(34/49): loss=0.3651085147160496\n",
      "Gradient Descent(35/49): loss=0.3647051400360981\n",
      "Gradient Descent(36/49): loss=0.3643169302490875\n",
      "Gradient Descent(37/49): loss=0.3639431897429487\n",
      "Gradient Descent(38/49): loss=0.3635832669991633\n",
      "Gradient Descent(39/49): loss=0.3632365502965063\n",
      "Gradient Descent(40/49): loss=0.36290246401120685\n",
      "Gradient Descent(41/49): loss=0.36258046541070527\n",
      "Gradient Descent(42/49): loss=0.362270041858553\n",
      "Gradient Descent(43/49): loss=0.36197070836389655\n",
      "Gradient Descent(44/49): loss=0.36168200542146706\n",
      "Gradient Descent(45/49): loss=0.3614034970978712\n",
      "Gradient Descent(46/49): loss=0.3611347693278302\n",
      "Gradient Descent(47/49): loss=0.36087542839031106\n",
      "Gradient Descent(48/49): loss=0.3606250995395611\n",
      "Gradient Descent(49/49): loss=0.3603834257701658\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4534131875356022\n",
      "Gradient Descent(2/49): loss=0.42744184776768757\n",
      "Gradient Descent(3/49): loss=0.412305599450569\n",
      "Gradient Descent(4/49): loss=0.40298429966807287\n",
      "Gradient Descent(5/49): loss=0.39686740916505225\n",
      "Gradient Descent(6/49): loss=0.3925765575646106\n",
      "Gradient Descent(7/49): loss=0.389369760437195\n",
      "Gradient Descent(8/49): loss=0.3868376625162548\n",
      "Gradient Descent(9/49): loss=0.3847474507805845\n",
      "Gradient Descent(10/49): loss=0.38296187197570075\n",
      "Gradient Descent(11/49): loss=0.38139668847389235\n",
      "Gradient Descent(12/49): loss=0.37999796258066343\n",
      "Gradient Descent(13/49): loss=0.37872967558055737\n",
      "Gradient Descent(14/49): loss=0.37756680668782355\n",
      "Gradient Descent(15/49): loss=0.3764913471069266\n",
      "Gradient Descent(16/49): loss=0.37548992659878005\n",
      "Gradient Descent(17/49): loss=0.3745523494047948\n",
      "Gradient Descent(18/49): loss=0.37367065864759713\n",
      "Gradient Descent(19/49): loss=0.3728385181045739\n",
      "Gradient Descent(20/49): loss=0.37205079114623113\n",
      "Gradient Descent(21/49): loss=0.3713032462828359\n",
      "Gradient Descent(22/49): loss=0.37059234654761253\n",
      "Gradient Descent(23/49): loss=0.3699150959312717\n",
      "Gradient Descent(24/49): loss=0.36926892556514324\n",
      "Gradient Descent(25/49): loss=0.36865160815541\n",
      "Gradient Descent(26/49): loss=0.3680611928371404\n",
      "Gradient Descent(27/49): loss=0.3674959550003697\n",
      "Gradient Descent(28/49): loss=0.36695435723086756\n",
      "Gradient Descent(29/49): loss=0.36643501859341515\n",
      "Gradient Descent(30/49): loss=0.3659366902399976\n",
      "Gradient Descent(31/49): loss=0.36545823585834203\n",
      "Gradient Descent(32/49): loss=0.3649986158577505\n",
      "Gradient Descent(33/49): loss=0.3645568744653248\n",
      "Gradient Descent(34/49): loss=0.3641321291075023\n",
      "Gradient Descent(35/49): loss=0.36372356160060965\n",
      "Gradient Descent(36/49): loss=0.36333041078470796\n",
      "Gradient Descent(37/49): loss=0.36295196631777243\n",
      "Gradient Descent(38/49): loss=0.3625875634096724\n",
      "Gradient Descent(39/49): loss=0.36223657832281303\n",
      "Gradient Descent(40/49): loss=0.3618984245025434\n",
      "Gradient Descent(41/49): loss=0.3615725492283399\n",
      "Gradient Descent(42/49): loss=0.36125843069839636\n",
      "Gradient Descent(43/49): loss=0.3609555754771302\n",
      "Gradient Descent(44/49): loss=0.36066351624837056\n",
      "Gradient Descent(45/49): loss=0.3603818098274682\n",
      "Gradient Descent(46/49): loss=0.36011003539390757\n",
      "Gradient Descent(47/49): loss=0.3598477929126736\n",
      "Gradient Descent(48/49): loss=0.35959470171799857\n",
      "Gradient Descent(49/49): loss=0.3593503992374681\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4525160246291584\n",
      "Gradient Descent(2/49): loss=0.4260937108182725\n",
      "Gradient Descent(3/49): loss=0.410745629747557\n",
      "Gradient Descent(4/49): loss=0.4013390317750969\n",
      "Gradient Descent(5/49): loss=0.39520283583732363\n",
      "Gradient Descent(6/49): loss=0.39092626591226437\n",
      "Gradient Descent(7/49): loss=0.3877500369647884\n",
      "Gradient Descent(8/49): loss=0.3852555911917025\n",
      "Gradient Descent(9/49): loss=0.3832052581798076\n",
      "Gradient Descent(10/49): loss=0.3814592626335963\n",
      "Gradient Descent(11/49): loss=0.37993210189446786\n",
      "Gradient Descent(12/49): loss=0.37856924980496337\n",
      "Gradient Descent(13/49): loss=0.377334462023796\n",
      "Gradient Descent(14/49): loss=0.3762026846247056\n",
      "Gradient Descent(15/49): loss=0.37515597525806804\n",
      "Gradient Descent(16/49): loss=0.3741810789106278\n",
      "Gradient Descent(17/49): loss=0.3732679360571525\n",
      "Gradient Descent(18/49): loss=0.3724087319860756\n",
      "Gradient Descent(19/49): loss=0.3715972705454667\n",
      "Gradient Descent(20/49): loss=0.3708285489856792\n",
      "Gradient Descent(21/49): loss=0.37009846160730087\n",
      "Gradient Descent(22/49): loss=0.36940358846724486\n",
      "Gradient Descent(23/49): loss=0.36874104180447764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=0.3681083525668084\n",
      "Gradient Descent(25/49): loss=0.36750338536053345\n",
      "Gradient Descent(26/49): loss=0.3669242738887327\n",
      "Gradient Descent(27/49): loss=0.3663693713727845\n",
      "Gradient Descent(28/49): loss=0.36583721206848774\n",
      "Gradient Descent(29/49): loss=0.365326481088855\n",
      "Gradient Descent(30/49): loss=0.36483599050922916\n",
      "Gradient Descent(31/49): loss=0.3643646602685626\n",
      "Gradient Descent(32/49): loss=0.36391150276506506\n",
      "Gradient Descent(33/49): loss=0.3634756103220451\n",
      "Gradient Descent(34/49): loss=0.3630561449022526\n",
      "Gradient Descent(35/49): loss=0.3626523295979934\n",
      "Gradient Descent(36/49): loss=0.362263441534765\n",
      "Gradient Descent(37/49): loss=0.3618888059087112\n",
      "Gradient Descent(38/49): loss=0.3615277909403213\n",
      "Gradient Descent(39/49): loss=0.3611798035738894\n",
      "Gradient Descent(40/49): loss=0.3608442857881884\n",
      "Gradient Descent(41/49): loss=0.3605207114114391\n",
      "Gradient Descent(42/49): loss=0.3602085833550186\n",
      "Gradient Descent(43/49): loss=0.3599074311970092\n",
      "Gradient Descent(44/49): loss=0.35961680905973675\n",
      "Gradient Descent(45/49): loss=0.3593362937357586\n",
      "Gradient Descent(46/49): loss=0.35906548302494307\n",
      "Gradient Descent(47/49): loss=0.3588039942518284\n",
      "Gradient Descent(48/49): loss=0.3585514629377095\n",
      "Gradient Descent(49/49): loss=0.3583075416061551\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4539780780723496\n",
      "Gradient Descent(2/49): loss=0.4282484628281328\n",
      "Gradient Descent(3/49): loss=0.41322786057140515\n",
      "Gradient Descent(4/49): loss=0.40397453719142334\n",
      "Gradient Descent(5/49): loss=0.3979090056120876\n",
      "Gradient Descent(6/49): loss=0.3936645192848307\n",
      "Gradient Descent(7/49): loss=0.39050286383802923\n",
      "Gradient Descent(8/49): loss=0.38801545990726244\n",
      "Gradient Descent(9/49): loss=0.3859692756170685\n",
      "Gradient Descent(10/49): loss=0.3842266221507182\n",
      "Gradient Descent(11/49): loss=0.38270287805271097\n",
      "Gradient Descent(12/49): loss=0.3813438400825291\n",
      "Gradient Descent(13/49): loss=0.38011333674129744\n",
      "Gradient Descent(14/49): loss=0.3789862818507625\n",
      "Gradient Descent(15/49): loss=0.3779446625616772\n",
      "Gradient Descent(16/49): loss=0.37697514490235634\n",
      "Gradient Descent(17/49): loss=0.3760675943763717\n",
      "Gradient Descent(18/49): loss=0.3752141297847179\n",
      "Gradient Descent(19/49): loss=0.37440849794984676\n",
      "Gradient Descent(20/49): loss=0.3736456480787598\n",
      "Gradient Descent(21/49): loss=0.37292143441178416\n",
      "Gradient Descent(22/49): loss=0.37223240382168343\n",
      "Gradient Descent(23/49): loss=0.37157564119351066\n",
      "Gradient Descent(24/49): loss=0.37094865502589164\n",
      "Gradient Descent(25/49): loss=0.37034929158709595\n",
      "Gradient Descent(26/49): loss=0.3697756696838776\n",
      "Gradient Descent(27/49): loss=0.3692261305231348\n",
      "Gradient Descent(28/49): loss=0.3686991987620076\n",
      "Gradient Descent(29/49): loss=0.36819355194365316\n",
      "Gradient Descent(30/49): loss=0.36770799628123313\n",
      "Gradient Descent(31/49): loss=0.3672414472926704\n",
      "Gradient Descent(32/49): loss=0.36679291417485105\n",
      "Gradient Descent(33/49): loss=0.36636148708511646\n",
      "Gradient Descent(34/49): loss=0.3659463267017152\n",
      "Gradient Descent(35/49): loss=0.3655466555850087\n",
      "Gradient Descent(36/49): loss=0.36516175097266745\n",
      "Gradient Descent(37/49): loss=0.36479093872546153\n",
      "Gradient Descent(38/49): loss=0.3644335882030564\n",
      "Gradient Descent(39/49): loss=0.36408910789687504\n",
      "Gradient Descent(40/49): loss=0.3637569416834931\n",
      "Gradient Descent(41/49): loss=0.36343656559003495\n",
      "Gradient Descent(42/49): loss=0.3631274849847139\n",
      "Gradient Descent(43/49): loss=0.36282923212256646\n",
      "Gradient Descent(44/49): loss=0.3625413639896808\n",
      "Gradient Descent(45/49): loss=0.3622634603996956\n",
      "Gradient Descent(46/49): loss=0.3619951223046535\n",
      "Gradient Descent(47/49): loss=0.36173597028894966\n",
      "Gradient Descent(48/49): loss=0.3614856432204546\n",
      "Gradient Descent(49/49): loss=0.3612437970372141\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44932354698030547\n",
      "Gradient Descent(2/49): loss=0.42303520256362315\n",
      "Gradient Descent(3/49): loss=0.4085798966138327\n",
      "Gradient Descent(4/49): loss=0.4000286478432592\n",
      "Gradient Descent(5/49): loss=0.3945313291509817\n",
      "Gradient Descent(6/49): loss=0.3906885113617149\n",
      "Gradient Descent(7/49): loss=0.387793873416088\n",
      "Gradient Descent(8/49): loss=0.3854780868400006\n",
      "Gradient Descent(9/49): loss=0.3835394886363178\n",
      "Gradient Descent(10/49): loss=0.3818623539254343\n",
      "Gradient Descent(11/49): loss=0.38037665483253913\n",
      "Gradient Descent(12/49): loss=0.37903772560519733\n",
      "Gradient Descent(13/49): loss=0.37781564851398425\n",
      "Gradient Descent(14/49): loss=0.3766894971856729\n",
      "Gradient Descent(15/49): loss=0.3756440786497469\n",
      "Gradient Descent(16/49): loss=0.37466800631455704\n",
      "Gradient Descent(17/49): loss=0.37375251035168333\n",
      "Gradient Descent(18/49): loss=0.3728906740580267\n",
      "Gradient Descent(19/49): loss=0.3720769266746971\n",
      "Gradient Descent(20/49): loss=0.3713066966552054\n",
      "Gradient Descent(21/49): loss=0.37057616878531147\n",
      "Gradient Descent(22/49): loss=0.3698821104963249\n",
      "Gradient Descent(23/49): loss=0.36922174541207586\n",
      "Gradient Descent(24/49): loss=0.36859265980149425\n",
      "Gradient Descent(25/49): loss=0.36799273235558916\n",
      "Gradient Descent(26/49): loss=0.3674200807495561\n",
      "Gradient Descent(27/49): loss=0.36687302044969394\n",
      "Gradient Descent(28/49): loss=0.3663500325659812\n",
      "Gradient Descent(29/49): loss=0.3658497384666544\n",
      "Gradient Descent(30/49): loss=0.36537087950520625\n",
      "Gradient Descent(31/49): loss=0.36491230065495545\n",
      "Gradient Descent(32/49): loss=0.36447293716181645\n",
      "Gradient Descent(33/49): loss=0.3640518035520241\n",
      "Gradient Descent(34/49): loss=0.36364798449527014\n",
      "Gradient Descent(35/49): loss=0.36326062714335183\n",
      "Gradient Descent(36/49): loss=0.3628889346526975\n",
      "Gradient Descent(37/49): loss=0.36253216066485683\n",
      "Gradient Descent(38/49): loss=0.3621896045684106\n",
      "Gradient Descent(39/49): loss=0.36186060740318504\n",
      "Gradient Descent(40/49): loss=0.36154454829626526\n",
      "Gradient Descent(41/49): loss=0.3612408413413598\n",
      "Gradient Descent(42/49): loss=0.3609489328502201\n",
      "Gradient Descent(43/49): loss=0.36066829891823887\n",
      "Gradient Descent(44/49): loss=0.3603984432569565\n",
      "Gradient Descent(45/49): loss=0.36013889525462006\n",
      "Gradient Descent(46/49): loss=0.35988920823267523\n",
      "Gradient Descent(47/49): loss=0.35964895787149603\n",
      "Gradient Descent(48/49): loss=0.3594177407830426\n",
      "Gradient Descent(49/49): loss=0.35919517321171546\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44909595152413606\n",
      "Gradient Descent(2/49): loss=0.4227381371471345\n",
      "Gradient Descent(3/49): loss=0.4082414544581544\n",
      "Gradient Descent(4/49): loss=0.3996440898312004\n",
      "Gradient Descent(5/49): loss=0.3940936915079675\n",
      "Gradient Descent(6/49): loss=0.39019536067310473\n",
      "Gradient Descent(7/49): loss=0.3872469579936132\n",
      "Gradient Descent(8/49): loss=0.38488161435170587\n",
      "Gradient Descent(9/49): loss=0.3828986700963688\n",
      "Gradient Descent(10/49): loss=0.3811824888390278\n",
      "Gradient Descent(11/49): loss=0.3796626544049942\n",
      "Gradient Descent(12/49): loss=0.3782939173511666\n",
      "Gradient Descent(13/49): loss=0.377045738973433\n",
      "Gradient Descent(14/49): loss=0.3758966128363526\n",
      "Gradient Descent(15/49): loss=0.3748308385343389\n",
      "Gradient Descent(16/49): loss=0.3738366016384169\n",
      "Gradient Descent(17/49): loss=0.3729047791909071\n",
      "Gradient Descent(18/49): loss=0.37202816644160946\n",
      "Gradient Descent(19/49): loss=0.3712009589774655\n",
      "Gradient Descent(20/49): loss=0.37041839595973025\n",
      "Gradient Descent(21/49): loss=0.3696765085455498\n",
      "Gradient Descent(22/49): loss=0.3689719389819072\n",
      "Gradient Descent(23/49): loss=0.3683018083150654\n",
      "Gradient Descent(24/49): loss=0.3676636181971332\n",
      "Gradient Descent(25/49): loss=0.36705517699955764\n",
      "Gradient Descent(26/49): loss=0.3664745435004033\n",
      "Gradient Descent(27/49): loss=0.36591998343897664\n",
      "Gradient Descent(28/49): loss=0.3653899356024993\n",
      "Gradient Descent(29/49): loss=0.36488298505259503\n",
      "Gradient Descent(30/49): loss=0.36439784175689455\n",
      "Gradient Descent(31/49): loss=0.36393332335492695\n",
      "Gradient Descent(32/49): loss=0.36348834111810346\n",
      "Gradient Descent(33/49): loss=0.3630618884015472\n",
      "Gradient Descent(34/49): loss=0.3626530310583285\n",
      "Gradient Descent(35/49): loss=0.36226089941327033\n",
      "Gradient Descent(36/49): loss=0.36188468148705355\n",
      "Gradient Descent(37/49): loss=0.361523617231105\n",
      "Gradient Descent(38/49): loss=0.36117699358617744\n",
      "Gradient Descent(39/49): loss=0.36084414021729194\n",
      "Gradient Descent(40/49): loss=0.3605244258080951\n",
      "Gradient Descent(41/49): loss=0.3602172548211087\n",
      "Gradient Descent(42/49): loss=0.35992206464853793\n",
      "Gradient Descent(43/49): loss=0.3596383230925417\n",
      "Gradient Descent(44/49): loss=0.3593655261250971\n",
      "Gradient Descent(45/49): loss=0.3591031958865029\n",
      "Gradient Descent(46/49): loss=0.35885087888868605\n",
      "Gradient Descent(47/49): loss=0.3586081443952112\n",
      "Gradient Descent(48/49): loss=0.3583745829545231\n",
      "Gradient Descent(49/49): loss=0.3581498050667271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4481173947745179\n",
      "Gradient Descent(2/49): loss=0.42131330399268746\n",
      "Gradient Descent(3/49): loss=0.40663248818120956\n",
      "Gradient Descent(4/49): loss=0.3979779269930371\n",
      "Gradient Descent(5/49): loss=0.3924304823182664\n",
      "Gradient Descent(6/49): loss=0.38856240971404515\n",
      "Gradient Descent(7/49): loss=0.38565547141665285\n",
      "Gradient Descent(8/49): loss=0.3833349650898796\n",
      "Gradient Descent(9/49): loss=0.381396485980005\n",
      "Gradient Descent(10/49): loss=0.37972267948388594\n",
      "Gradient Descent(11/49): loss=0.37824241670669007\n",
      "Gradient Descent(12/49): loss=0.37691023198131274\n",
      "Gradient Descent(13/49): loss=0.37569561159805887\n",
      "Gradient Descent(14/49): loss=0.3745771865532307\n",
      "Gradient Descent(15/49): loss=0.37353944114128146\n",
      "Gradient Descent(16/49): loss=0.37257076003745837\n",
      "Gradient Descent(17/49): loss=0.37166221749904355\n",
      "Gradient Descent(18/49): loss=0.37080679637953623\n",
      "Gradient Descent(19/49): loss=0.36999886698769646\n",
      "Gradient Descent(20/49): loss=0.3692338293546959\n",
      "Gradient Descent(21/49): loss=0.3685078618497021\n",
      "Gradient Descent(22/49): loss=0.36781774102602405\n",
      "Gradient Descent(23/49): loss=0.36716071031742215\n",
      "Gradient Descent(24/49): loss=0.36653438289509027\n",
      "Gradient Descent(25/49): loss=0.3659366688072314\n",
      "Gradient Descent(26/49): loss=0.36536571962592246\n",
      "Gradient Descent(27/49): loss=0.36481988587767067\n",
      "Gradient Descent(28/49): loss=0.36429768391866046\n",
      "Gradient Descent(29/49): loss=0.3637977698656808\n",
      "Gradient Descent(30/49): loss=0.36331891885454687\n",
      "Gradient Descent(31/49): loss=0.3628600083629234\n",
      "Gradient Descent(32/49): loss=0.36242000466523633\n",
      "Gradient Descent(33/49): loss=0.3619979517248804\n",
      "Gradient Descent(34/49): loss=0.36159296200105245\n",
      "Gradient Descent(35/49): loss=0.3612042087733835\n",
      "Gradient Descent(36/49): loss=0.36083091968034775\n",
      "Gradient Descent(37/49): loss=0.36047237123648135\n",
      "Gradient Descent(38/49): loss=0.3601278841452398\n",
      "Gradient Descent(39/49): loss=0.3597968192635388\n",
      "Gradient Descent(40/49): loss=0.3594785741039319\n",
      "Gradient Descent(41/49): loss=0.3591725797833978\n",
      "Gradient Descent(42/49): loss=0.35887829834555984\n",
      "Gradient Descent(43/49): loss=0.3585952203971055\n",
      "Gradient Descent(44/49): loss=0.35832286301015587\n",
      "Gradient Descent(45/49): loss=0.35806076785103863\n",
      "Gradient Descent(46/49): loss=0.35780849950286603\n",
      "Gradient Descent(47/49): loss=0.3575656439548955\n",
      "Gradient Descent(48/49): loss=0.3573318072361616\n",
      "Gradient Descent(49/49): loss=0.3571066141745257\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44970849683321834\n",
      "Gradient Descent(2/49): loss=0.4235844393589818\n",
      "Gradient Descent(3/49): loss=0.40919298608644533\n",
      "Gradient Descent(4/49): loss=0.4006595639309745\n",
      "Gradient Descent(5/49): loss=0.39516167573063127\n",
      "Gradient Descent(6/49): loss=0.3913135418907661\n",
      "Gradient Descent(7/49): loss=0.3884149631753115\n",
      "Gradient Descent(8/49): loss=0.38609883270161327\n",
      "Gradient Descent(9/49): loss=0.3841638210544186\n",
      "Gradient Descent(10/49): loss=0.3824937212581869\n",
      "Gradient Descent(11/49): loss=0.3810177507962153\n",
      "Gradient Descent(12/49): loss=0.3796904730468257\n",
      "Gradient Descent(13/49): loss=0.3784812901513977\n",
      "Gradient Descent(14/49): loss=0.37736871923256304\n",
      "Gradient Descent(15/49): loss=0.37633713313456013\n",
      "Gradient Descent(16/49): loss=0.37537481830221014\n",
      "Gradient Descent(17/49): loss=0.3744727662282674\n",
      "Gradient Descent(18/49): loss=0.37362389153999365\n",
      "Gradient Descent(19/49): loss=0.37282250895547386\n",
      "Gradient Descent(20/49): loss=0.3720639735305644\n",
      "Gradient Descent(21/49): loss=0.37134442744004426\n",
      "Gradient Descent(22/49): loss=0.3706606182564241\n",
      "Gradient Descent(23/49): loss=0.37000976634351074\n",
      "Gradient Descent(24/49): loss=0.3693894666449057\n",
      "Gradient Descent(25/49): loss=0.36879761495302527\n",
      "Gradient Descent(26/49): loss=0.368232351848938\n",
      "Gradient Descent(27/49): loss=0.36769201955939096\n",
      "Gradient Descent(28/49): loss=0.36717512836672306\n",
      "Gradient Descent(29/49): loss=0.36668033016172225\n",
      "Gradient Descent(30/49): loss=0.36620639739410865\n",
      "Gradient Descent(31/49): loss=0.3657522061436372\n",
      "Gradient Descent(32/49): loss=0.3653167223682835\n",
      "Gradient Descent(33/49): loss=0.3648989906257107\n",
      "Gradient Descent(34/49): loss=0.364498124738161\n",
      "Gradient Descent(35/49): loss=0.36411329999822595\n",
      "Gradient Descent(36/49): loss=0.3637437466069534\n",
      "Gradient Descent(37/49): loss=0.3633887441057505\n",
      "Gradient Descent(38/49): loss=0.36304761661610174\n",
      "Gradient Descent(39/49): loss=0.36271972874093394\n",
      "Gradient Descent(40/49): loss=0.36240448201184383\n",
      "Gradient Descent(41/49): loss=0.36210131178979105\n",
      "Gradient Descent(42/49): loss=0.3618096845449993\n",
      "Gradient Descent(43/49): loss=0.3615290954559766\n",
      "Gradient Descent(44/49): loss=0.3612590662787214\n",
      "Gradient Descent(45/49): loss=0.36099914344601974\n",
      "Gradient Descent(46/49): loss=0.36074889636378604\n",
      "Gradient Descent(47/49): loss=0.3605079158770622\n",
      "Gradient Descent(48/49): loss=0.36027581288285626\n",
      "Gradient Descent(49/49): loss=0.36005221707070856\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4451931680449022\n",
      "Gradient Descent(2/49): loss=0.4188344434532873\n",
      "Gradient Descent(3/49): loss=0.40513415792328855\n",
      "Gradient Descent(4/49): loss=0.3972839340748113\n",
      "Gradient Descent(5/49): loss=0.39227709036709985\n",
      "Gradient Descent(6/49): loss=0.38874542426518516\n",
      "Gradient Descent(7/49): loss=0.3860407352610406\n",
      "Gradient Descent(8/49): loss=0.38383957214197073\n",
      "Gradient Descent(9/49): loss=0.3819702198542311\n",
      "Gradient Descent(10/49): loss=0.38033515924858086\n",
      "Gradient Descent(11/49): loss=0.3788751832746066\n",
      "Gradient Descent(12/49): loss=0.37755208852332567\n",
      "Gradient Descent(13/49): loss=0.37633989907635007\n",
      "Gradient Descent(14/49): loss=0.3752201630457835\n",
      "Gradient Descent(15/49): loss=0.3741792858695643\n",
      "Gradient Descent(16/49): loss=0.37320693680938677\n",
      "Gradient Descent(17/49): loss=0.372295052129642\n",
      "Gradient Descent(18/49): loss=0.3714371872814514\n",
      "Gradient Descent(19/49): loss=0.37062808253240714\n",
      "Gradient Descent(20/49): loss=0.3698633640909477\n",
      "Gradient Descent(21/49): loss=0.3691393338934362\n",
      "Gradient Descent(22/49): loss=0.36845281886435416\n",
      "Gradient Descent(23/49): loss=0.3678010609042299\n",
      "Gradient Descent(24/49): loss=0.3671816352726712\n",
      "Gradient Descent(25/49): loss=0.3665923890898101\n",
      "Gradient Descent(26/49): loss=0.3660313943065522\n",
      "Gradient Descent(27/49): loss=0.3654969112288333\n",
      "Gradient Descent(28/49): loss=0.3649873598453726\n",
      "Gradient Descent(29/49): loss=0.3645012970009481\n",
      "Gradient Descent(30/49): loss=0.36403739800368273\n",
      "Gradient Descent(31/49): loss=0.36359444163623966\n",
      "Gradient Descent(32/49): loss=0.3631712978101708\n",
      "Gradient Descent(33/49): loss=0.36276691729506383\n",
      "Gradient Descent(34/49): loss=0.3623803230931396\n",
      "Gradient Descent(35/49): loss=0.3620106031314887\n",
      "Gradient Descent(36/49): loss=0.3616569040191245\n",
      "Gradient Descent(37/49): loss=0.3613184256719855\n",
      "Gradient Descent(38/49): loss=0.36099441665120996\n",
      "Gradient Descent(39/49): loss=0.36068417009211184\n",
      "Gradient Descent(40/49): loss=0.36038702012596563\n",
      "Gradient Descent(41/49): loss=0.3601023387158157\n",
      "Gradient Descent(42/49): loss=0.35982953284247\n",
      "Gradient Descent(43/49): loss=0.3595680419885914\n",
      "Gradient Descent(44/49): loss=0.3593173358781257\n",
      "Gradient Descent(45/49): loss=0.35907691243574447\n",
      "Gradient Descent(46/49): loss=0.3588462959369624\n",
      "Gradient Descent(47/49): loss=0.3586250353244139\n",
      "Gradient Descent(48/49): loss=0.35841270266970054\n",
      "Gradient Descent(49/49): loss=0.3582088917634235\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4449505308095854\n",
      "Gradient Descent(2/49): loss=0.4185276231154578\n",
      "Gradient Descent(3/49): loss=0.4047834443458244\n",
      "Gradient Descent(4/49): loss=0.39687949022262\n",
      "Gradient Descent(5/49): loss=0.39181181345238186\n",
      "Gradient Descent(6/49): loss=0.38821907810135886\n",
      "Gradient Descent(7/49): loss=0.38545763164665203\n",
      "Gradient Descent(8/49): loss=0.3832059915976386\n",
      "Gradient Descent(9/49): loss=0.3812927650529243\n",
      "Gradient Descent(10/49): loss=0.37961993142830536\n",
      "Gradient Descent(11/49): loss=0.3781274710106249\n",
      "Gradient Descent(12/49): loss=0.3767763253870547\n",
      "Gradient Descent(13/49): loss=0.37553974152430575\n",
      "Gradient Descent(14/49): loss=0.3743986088527808\n",
      "Gradient Descent(15/49): loss=0.3733387948609119\n",
      "Gradient Descent(16/49): loss=0.3723495380897287\n",
      "Gradient Descent(17/49): loss=0.3714224331904249\n",
      "Gradient Descent(18/49): loss=0.37055076542601695\n",
      "Gradient Descent(19/49): loss=0.3697290609918669\n",
      "Gradient Descent(20/49): loss=0.36895277565359685\n",
      "Gradient Descent(21/49): loss=0.3682180746825456\n",
      "Gradient Descent(22/49): loss=0.36752167448923745\n",
      "Gradient Descent(23/49): loss=0.36686072676774706\n",
      "Gradient Descent(24/49): loss=0.36623273242218995\n",
      "Gradient Descent(25/49): loss=0.36563547667171786\n",
      "Gradient Descent(26/49): loss=0.36506697942652605\n",
      "Gradient Descent(27/49): loss=0.3645254568220209\n",
      "Gradient Descent(28/49): loss=0.36400929101097207\n",
      "Gradient Descent(29/49): loss=0.3635170061436416\n",
      "Gradient Descent(30/49): loss=0.3630472490409272\n",
      "Gradient Descent(31/49): loss=0.362598773468352\n",
      "Gradient Descent(32/49): loss=0.3621704272039301\n",
      "Gradient Descent(33/49): loss=0.361761141297045\n",
      "Gradient Descent(34/49): loss=0.3613699210630927\n",
      "Gradient Descent(35/49): loss=0.36099583846653205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=0.36063802562465813\n",
      "Gradient Descent(37/49): loss=0.36029566922384815\n",
      "Gradient Descent(38/49): loss=0.35996800568480825\n",
      "Gradient Descent(39/49): loss=0.3596543169474059\n",
      "Gradient Descent(40/49): loss=0.35935392677180966\n",
      "Gradient Descent(41/49): loss=0.3590661974728974\n",
      "Gradient Descent(42/49): loss=0.35879052702068776\n",
      "Gradient Descent(43/49): loss=0.3585263464519733\n",
      "Gradient Descent(44/49): loss=0.3582731175481787\n",
      "Gradient Descent(45/49): loss=0.35803033074231766\n",
      "Gradient Descent(46/49): loss=0.3577975032242242\n",
      "Gradient Descent(47/49): loss=0.3575741772183259\n",
      "Gradient Descent(48/49): loss=0.357359918412358\n",
      "Gradient Descent(49/49): loss=0.35715431451879454\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44389423840163456\n",
      "Gradient Descent(2/49): loss=0.4170369948744651\n",
      "Gradient Descent(3/49): loss=0.4031387870372789\n",
      "Gradient Descent(4/49): loss=0.39520436831470845\n",
      "Gradient Descent(5/49): loss=0.39015892563495674\n",
      "Gradient Descent(6/49): loss=0.3866092214900465\n",
      "Gradient Descent(7/49): loss=0.3838973206761286\n",
      "Gradient Descent(8/49): loss=0.3816954419519335\n",
      "Gradient Descent(9/49): loss=0.37982952453813035\n",
      "Gradient Descent(10/49): loss=0.37820053476277693\n",
      "Gradient Descent(11/49): loss=0.3767481910569124\n",
      "Gradient Descent(12/49): loss=0.3754335076465789\n",
      "Gradient Descent(13/49): loss=0.37422994484992866\n",
      "Gradient Descent(14/49): loss=0.37311865568490116\n",
      "Gradient Descent(15/49): loss=0.3720857795016803\n",
      "Gradient Descent(16/49): loss=0.37112081610528525\n",
      "Gradient Descent(17/49): loss=0.3702156030402631\n",
      "Gradient Descent(18/49): loss=0.3693636476606827\n",
      "Gradient Descent(19/49): loss=0.36855967754620356\n",
      "Gradient Descent(20/49): loss=0.3677993303683128\n",
      "Gradient Descent(21/49): loss=0.3670789354921278\n",
      "Gradient Descent(22/49): loss=0.3663953573714591\n",
      "Gradient Descent(23/49): loss=0.36574588138712166\n",
      "Gradient Descent(24/49): loss=0.36512812932939703\n",
      "Gradient Descent(25/49): loss=0.3645399958980124\n",
      "Gradient Descent(26/49): loss=0.36397960031262333\n",
      "Gradient Descent(27/49): loss=0.3634452489322913\n",
      "Gradient Descent(28/49): loss=0.36293540599934915\n",
      "Gradient Descent(29/49): loss=0.3624486704540223\n",
      "Gradient Descent(30/49): loss=0.3619837573403653\n",
      "Gradient Descent(31/49): loss=0.361539482725317\n",
      "Gradient Descent(32/49): loss=0.36111475133610743\n",
      "Gradient Descent(33/49): loss=0.36070854632363225\n",
      "Gradient Descent(34/49): loss=0.3603199207054606\n",
      "Gradient Descent(35/49): loss=0.3599479901486663\n",
      "Gradient Descent(36/49): loss=0.35959192683117186\n",
      "Gradient Descent(37/49): loss=0.35925095417875186\n",
      "Gradient Descent(38/49): loss=0.3589243423187928\n",
      "Gradient Descent(39/49): loss=0.3586114041252833\n",
      "Gradient Descent(40/49): loss=0.35831149175507976\n",
      "Gradient Descent(41/49): loss=0.35802399359525444\n",
      "Gradient Descent(42/49): loss=0.35774833155673874\n",
      "Gradient Descent(43/49): loss=0.35748395866156707\n",
      "Gradient Descent(44/49): loss=0.3572303568805961\n",
      "Gradient Descent(45/49): loss=0.3569870351861881\n",
      "Gradient Descent(46/49): loss=0.3567535277904586\n",
      "Gradient Descent(47/49): loss=0.35652939254460503\n",
      "Gradient Descent(48/49): loss=0.3563142094788227\n",
      "Gradient Descent(49/49): loss=0.35610757946556687\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.445607708722431\n",
      "Gradient Descent(2/49): loss=0.41940742452410795\n",
      "Gradient Descent(3/49): loss=0.40575908370912933\n",
      "Gradient Descent(4/49): loss=0.39791730081636856\n",
      "Gradient Descent(5/49): loss=0.39290506784683354\n",
      "Gradient Descent(6/49): loss=0.3893672235489787\n",
      "Gradient Descent(7/49): loss=0.3866603702692587\n",
      "Gradient Descent(8/49): loss=0.3844620594162983\n",
      "Gradient Descent(9/49): loss=0.3826000211944102\n",
      "Gradient Descent(10/49): loss=0.38097569024590966\n",
      "Gradient Descent(11/49): loss=0.3795287880483187\n",
      "Gradient Descent(12/49): loss=0.3782201902515432\n",
      "Gradient Descent(13/49): loss=0.377023193999286\n",
      "Gradient Descent(14/49): loss=0.3759188031166071\n",
      "Gradient Descent(15/49): loss=0.3748930315062112\n",
      "Gradient Descent(16/49): loss=0.37393527710710306\n",
      "Gradient Descent(17/49): loss=0.3730372960858155\n",
      "Gradient Descent(18/49): loss=0.37219253135609653\n",
      "Gradient Descent(19/49): loss=0.3713956597778636\n",
      "Gradient Descent(20/49): loss=0.3706422793269246\n",
      "Gradient Descent(21/49): loss=0.36992868850706523\n",
      "Gradient Descent(22/49): loss=0.3692517279916979\n",
      "Gradient Descent(23/49): loss=0.36860866506719864\n",
      "Gradient Descent(24/49): loss=0.36799710800874963\n",
      "Gradient Descent(25/49): loss=0.36741494170313294\n",
      "Gradient Descent(26/49): loss=0.3668602785633204\n",
      "Gradient Descent(27/49): loss=0.3663314205946098\n",
      "Gradient Descent(28/49): loss=0.36582682969682234\n",
      "Gradient Descent(29/49): loss=0.3653451041245148\n",
      "Gradient Descent(30/49): loss=0.3648849596066106\n",
      "Gradient Descent(31/49): loss=0.36444521403230423\n",
      "Gradient Descent(32/49): loss=0.36402477489689\n",
      "Gradient Descent(33/49): loss=0.36362262890618535\n",
      "Gradient Descent(34/49): loss=0.3632378332863491\n",
      "Gradient Descent(35/49): loss=0.36286950845401256\n",
      "Gradient Descent(36/49): loss=0.3625168317813854\n",
      "Gradient Descent(37/49): loss=0.36217903225039216\n",
      "Gradient Descent(38/49): loss=0.36185538583457205\n",
      "Gradient Descent(39/49): loss=0.3615452114813868\n",
      "Gradient Descent(40/49): loss=0.36124786759356603\n",
      "Gradient Descent(41/49): loss=0.3609627489281894\n",
      "Gradient Descent(42/49): loss=0.36068928384783516\n",
      "Gradient Descent(43/49): loss=0.360426931870396\n",
      "Gradient Descent(44/49): loss=0.3601751814738531\n",
      "Gradient Descent(45/49): loss=0.35993354812002104\n",
      "Gradient Descent(46/49): loss=0.3597015724674477\n",
      "Gradient Descent(47/49): loss=0.35947881874863624\n",
      "Gradient Descent(48/49): loss=0.359264873290784\n",
      "Gradient Descent(49/49): loss=0.35905934316252147\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4412332011761473\n",
      "Gradient Descent(2/49): loss=0.41508285135338924\n",
      "Gradient Descent(3/49): loss=0.4021994088239401\n",
      "Gradient Descent(4/49): loss=0.3949933209169512\n",
      "Gradient Descent(5/49): loss=0.39039140879524753\n",
      "Gradient Descent(6/49): loss=0.38709575102708377\n",
      "Gradient Descent(7/49): loss=0.3845255328486846\n",
      "Gradient Descent(8/49): loss=0.3824010143395344\n",
      "Gradient Descent(9/49): loss=0.38057571405841273\n",
      "Gradient Descent(10/49): loss=0.37896618152068406\n",
      "Gradient Descent(11/49): loss=0.37752110371625847\n",
      "Gradient Descent(12/49): loss=0.37620684030719553\n",
      "Gradient Descent(13/49): loss=0.37500015836285905\n",
      "Gradient Descent(14/49): loss=0.37388431612315265\n",
      "Gradient Descent(15/49): loss=0.37284681247447654\n",
      "Gradient Descent(16/49): loss=0.37187802253802643\n",
      "Gradient Descent(17/49): loss=0.37097033426052917\n",
      "Gradient Descent(18/49): loss=0.37011758305858933\n",
      "Gradient Descent(19/49): loss=0.3693146711152683\n",
      "Gradient Descent(20/49): loss=0.36855730479206816\n",
      "Gradient Descent(21/49): loss=0.3678418095744132\n",
      "Gradient Descent(22/49): loss=0.3671649970370546\n",
      "Gradient Descent(23/49): loss=0.3665240673957928\n",
      "Gradient Descent(24/49): loss=0.3659165368439625\n",
      "Gradient Descent(25/49): loss=0.3653401824462913\n",
      "Gradient Descent(26/49): loss=0.36479299967413503\n",
      "Gradient Descent(27/49): loss=0.36427316918563163\n",
      "Gradient Descent(28/49): loss=0.36377903046847154\n",
      "Gradient Descent(29/49): loss=0.36330906064963286\n",
      "Gradient Descent(30/49): loss=0.36286185724791065\n",
      "Gradient Descent(31/49): loss=0.362436123973277\n",
      "Gradient Descent(32/49): loss=0.36203065890872393\n",
      "Gradient Descent(33/49): loss=0.36164434457584244\n",
      "Gradient Descent(34/49): loss=0.3612761395053462\n",
      "Gradient Descent(35/49): loss=0.3609250710217036\n",
      "Gradient Descent(36/49): loss=0.36059022901629717\n",
      "Gradient Descent(37/49): loss=0.3602707605324819\n",
      "Gradient Descent(38/49): loss=0.35996586502301736\n",
      "Gradient Descent(39/49): loss=0.35967479016874143\n",
      "Gradient Descent(40/49): loss=0.35939682816927576\n",
      "Gradient Descent(41/49): loss=0.35913131243362534\n",
      "Gradient Descent(42/49): loss=0.35887761461192247\n",
      "Gradient Descent(43/49): loss=0.3586351419201501\n",
      "Gradient Descent(44/49): loss=0.35840333471809993\n",
      "Gradient Descent(45/49): loss=0.3581816643075571\n",
      "Gradient Descent(46/49): loss=0.35796963092313844\n",
      "Gradient Descent(47/49): loss=0.3577667618926027\n",
      "Gradient Descent(48/49): loss=0.35757260994704093\n",
      "Gradient Descent(49/49): loss=0.3573867516642834\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4409769253919503\n",
      "Gradient Descent(2/49): loss=0.4147677707513377\n",
      "Gradient Descent(3/49): loss=0.40183607033960833\n",
      "Gradient Descent(4/49): loss=0.3945677864300449\n",
      "Gradient Descent(5/49): loss=0.3898978586201518\n",
      "Gradient Descent(6/49): loss=0.3865369040511473\n",
      "Gradient Descent(7/49): loss=0.3839083610132875\n",
      "Gradient Descent(8/49): loss=0.3817336035397838\n",
      "Gradient Descent(9/49): loss=0.3798656783617387\n",
      "Gradient Descent(10/49): loss=0.3782200704930719\n",
      "Gradient Descent(11/49): loss=0.37674431077484927\n",
      "Gradient Descent(12/49): loss=0.3754037198928468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=0.3741742051395981\n",
      "Gradient Descent(14/49): loss=0.37303834179272466\n",
      "Gradient Descent(15/49): loss=0.37198309699170606\n",
      "Gradient Descent(16/49): loss=0.37099843512910335\n",
      "Gradient Descent(17/49): loss=0.37007642701653043\n",
      "Gradient Descent(18/49): loss=0.3692106620680157\n",
      "Gradient Descent(19/49): loss=0.3683958501233792\n",
      "Gradient Descent(20/49): loss=0.36762754563636224\n",
      "Gradient Descent(21/49): loss=0.36690195275486065\n",
      "Gradient Descent(22/49): loss=0.366215784972215\n",
      "Gradient Descent(23/49): loss=0.3655661622603789\n",
      "Gradient Descent(24/49): loss=0.36495053437989083\n",
      "Gradient Descent(25/49): loss=0.3643666227643532\n",
      "Gradient Descent(26/49): loss=0.3638123757891871\n",
      "Gradient Descent(27/49): loss=0.36328593382954627\n",
      "Gradient Descent(28/49): loss=0.3627856015817622\n",
      "Gradient Descent(29/49): loss=0.3623098258492911\n",
      "Gradient Descent(30/49): loss=0.3618571774942091\n",
      "Gradient Descent(31/49): loss=0.3614263366039171\n",
      "Gradient Descent(32/49): loss=0.361016080168884\n",
      "Gradient Descent(33/49): loss=0.3606252717432765\n",
      "Gradient Descent(34/49): loss=0.3602528526877571\n",
      "Gradient Descent(35/49): loss=0.3598978346871029\n",
      "Gradient Descent(36/49): loss=0.3595592933045005\n",
      "Gradient Descent(37/49): loss=0.3592363623862305\n",
      "Gradient Descent(38/49): loss=0.35892822916971423\n",
      "Gradient Descent(39/49): loss=0.35863412997791105\n",
      "Gradient Descent(40/49): loss=0.35835334640620936\n",
      "Gradient Descent(41/49): loss=0.35808520192596177\n",
      "Gradient Descent(42/49): loss=0.35782905884294125\n",
      "Gradient Descent(43/49): loss=0.35758431556014614\n",
      "Gradient Descent(44/49): loss=0.357350404103251\n",
      "Gradient Descent(45/49): loss=0.35712678787410096\n",
      "Gradient Descent(46/49): loss=0.3569129596033615\n",
      "Gradient Descent(47/49): loss=0.3567084394780643\n",
      "Gradient Descent(48/49): loss=0.35651277342356547\n",
      "Gradient Descent(49/49): loss=0.35632553152251056\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43984655551050855\n",
      "Gradient Descent(2/49): loss=0.41322134372167213\n",
      "Gradient Descent(3/49): loss=0.40016685587701134\n",
      "Gradient Descent(4/49): loss=0.39289275666820805\n",
      "Gradient Descent(5/49): loss=0.3882613231198892\n",
      "Gradient Descent(6/49): loss=0.38495335965133287\n",
      "Gradient Descent(7/49): loss=0.38238019410750146\n",
      "Gradient Descent(8/49): loss=0.38025843510642493\n",
      "Gradient Descent(9/49): loss=0.37843942557308163\n",
      "Gradient Descent(10/49): loss=0.37683822189155886\n",
      "Gradient Descent(11/49): loss=0.3754024480253542\n",
      "Gradient Descent(12/49): loss=0.37409771809256775\n",
      "Gradient Descent(13/49): loss=0.37290029504849614\n",
      "Gradient Descent(14/49): loss=0.3717931132207665\n",
      "Gradient Descent(15/49): loss=0.37076347820044236\n",
      "Gradient Descent(16/49): loss=0.3698016637729258\n",
      "Gradient Descent(17/49): loss=0.36890001952182977\n",
      "Gradient Descent(18/49): loss=0.36805238441121263\n",
      "Gradient Descent(19/49): loss=0.36725369114511675\n",
      "Gradient Descent(20/49): loss=0.3664996931854836\n",
      "Gradient Descent(21/49): loss=0.3657867725775206\n",
      "Gradient Descent(22/49): loss=0.3651118021072262\n",
      "Gradient Descent(23/49): loss=0.3644720446550655\n",
      "Gradient Descent(24/49): loss=0.3638650784437779\n",
      "Gradient Descent(25/49): loss=0.36328874060228306\n",
      "Gradient Descent(26/49): loss=0.3627410838867791\n",
      "Gradient Descent(27/49): loss=0.36222034299554934\n",
      "Gradient Descent(28/49): loss=0.3617249079808436\n",
      "Gradient Descent(29/49): loss=0.36125330298411884\n",
      "Gradient Descent(30/49): loss=0.3608041690172068\n",
      "Gradient Descent(31/49): loss=0.36037624985711425\n",
      "Gradient Descent(32/49): loss=0.359968380365298\n",
      "Gradient Descent(33/49): loss=0.35957947671572976\n",
      "Gradient Descent(34/49): loss=0.3592085281413918\n",
      "Gradient Descent(35/49): loss=0.358854589900489\n",
      "Gradient Descent(36/49): loss=0.3585167772314575\n",
      "Gradient Descent(37/49): loss=0.3581942601165603\n",
      "Gradient Descent(38/49): loss=0.35788625871218754\n",
      "Gradient Descent(39/49): loss=0.3575920393332258\n",
      "Gradient Descent(40/49): loss=0.3573109109013827\n",
      "Gradient Descent(41/49): loss=0.3570422217848466\n",
      "Gradient Descent(42/49): loss=0.35678535697034536\n",
      "Gradient Descent(43/49): loss=0.35653973551945956\n",
      "Gradient Descent(44/49): loss=0.3563048082696099\n",
      "Gradient Descent(45/49): loss=0.35608005574697177\n",
      "Gradient Descent(46/49): loss=0.3558649862640721\n",
      "Gradient Descent(47/49): loss=0.3556591341792542\n",
      "Gradient Descent(48/49): loss=0.3554620582988096\n",
      "Gradient Descent(49/49): loss=0.3552733404055142\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4416757137399874\n",
      "Gradient Descent(2/49): loss=0.4156757289965692\n",
      "Gradient Descent(3/49): loss=0.4028320462872243\n",
      "Gradient Descent(4/49): loss=0.39562629045388276\n",
      "Gradient Descent(5/49): loss=0.39101602591827633\n",
      "Gradient Descent(6/49): loss=0.3877150032006802\n",
      "Gradient Descent(7/49): loss=0.3851455464427392\n",
      "Gradient Descent(8/49): loss=0.3830276502546545\n",
      "Gradient Descent(9/49): loss=0.38121348392168897\n",
      "Gradient Descent(10/49): loss=0.3796181223196383\n",
      "Gradient Descent(11/49): loss=0.378188991566316\n",
      "Gradient Descent(12/49): loss=0.3768914812506892\n",
      "Gradient Descent(13/49): loss=0.37570165693867086\n",
      "Gradient Descent(14/49): loss=0.37460229260453703\n",
      "Gradient Descent(15/49): loss=0.37358056750530105\n",
      "Gradient Descent(16/49): loss=0.37262665724580935\n",
      "Gradient Descent(17/49): loss=0.3717328356895327\n",
      "Gradient Descent(18/49): loss=0.3708928837596998\n",
      "Gradient Descent(19/49): loss=0.3701016899761845\n",
      "Gradient Descent(20/49): loss=0.36935497447524873\n",
      "Gradient Descent(21/49): loss=0.36864909450251626\n",
      "Gradient Descent(22/49): loss=0.3679809047624531\n",
      "Gradient Descent(23/49): loss=0.36734765537192327\n",
      "Gradient Descent(24/49): loss=0.36674691602278997\n",
      "Gradient Descent(25/49): loss=0.3661765187023218\n",
      "Gradient Descent(26/49): loss=0.3656345137554946\n",
      "Gradient Descent(27/49): loss=0.3651191356817258\n",
      "Gradient Descent(28/49): loss=0.36462877613576816\n",
      "Gradient Descent(29/49): loss=0.3641619623335153\n",
      "Gradient Descent(30/49): loss=0.3637173395660428\n",
      "Gradient Descent(31/49): loss=0.36329365687515885\n",
      "Gradient Descent(32/49): loss=0.3628897551905405\n",
      "Gradient Descent(33/49): loss=0.3625045574047405\n",
      "Gradient Descent(34/49): loss=0.3621370599897351\n",
      "Gradient Descent(35/49): loss=0.36178632585183534\n",
      "Gradient Descent(36/49): loss=0.3614514781906959\n",
      "Gradient Descent(37/49): loss=0.36113169517968136\n",
      "Gradient Descent(38/49): loss=0.36082620532376486\n",
      "Gradient Descent(39/49): loss=0.3605342833808183\n",
      "Gradient Descent(40/49): loss=0.3602552467549821\n",
      "Gradient Descent(41/49): loss=0.3599884522885254\n",
      "Gradient Descent(42/49): loss=0.35973329339245746\n",
      "Gradient Descent(43/49): loss=0.35948919746706803\n",
      "Gradient Descent(44/49): loss=0.35925562357222346\n",
      "Gradient Descent(45/49): loss=0.35903206031416\n",
      "Gradient Descent(46/49): loss=0.3588180239210621\n",
      "Gradient Descent(47/49): loss=0.35861305648419795\n",
      "Gradient Descent(48/49): loss=0.3584167243450297\n",
      "Gradient Descent(49/49): loss=0.35822861661169075\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4374436463740407\n",
      "Gradient Descent(2/49): loss=0.4117403562089463\n",
      "Gradient Descent(3/49): loss=0.3996932274847236\n",
      "Gradient Descent(4/49): loss=0.39305915672863784\n",
      "Gradient Descent(5/49): loss=0.38878255376029514\n",
      "Gradient Descent(6/49): loss=0.38566222084647184\n",
      "Gradient Descent(7/49): loss=0.3831859179993409\n",
      "Gradient Descent(8/49): loss=0.38111226506112633\n",
      "Gradient Descent(9/49): loss=0.37931484220022144\n",
      "Gradient Descent(10/49): loss=0.3777206981767727\n",
      "Gradient Descent(11/49): loss=0.37628422272722034\n",
      "Gradient Descent(12/49): loss=0.3749750368673821\n",
      "Gradient Descent(13/49): loss=0.3737718623640761\n",
      "Gradient Descent(14/49): loss=0.37265916449890574\n",
      "Gradient Descent(15/49): loss=0.3716251931458269\n",
      "Gradient Descent(16/49): loss=0.37066078243300754\n",
      "Gradient Descent(17/49): loss=0.3697585867059093\n",
      "Gradient Descent(18/49): loss=0.3689125788880616\n",
      "Gradient Descent(19/49): loss=0.36811771217189815\n",
      "Gradient Descent(20/49): loss=0.36736968623791283\n",
      "Gradient Descent(21/49): loss=0.3666647819877816\n",
      "Gradient Descent(22/49): loss=0.3659997421712347\n",
      "Gradient Descent(23/49): loss=0.36537168338898224\n",
      "Gradient Descent(24/49): loss=0.3647780299690997\n",
      "Gradient Descent(25/49): loss=0.36421646337982766\n",
      "Gradient Descent(26/49): loss=0.3636848828758676\n",
      "Gradient Descent(27/49): loss=0.3631813744048484\n",
      "Gradient Descent(28/49): loss=0.36270418568433727\n",
      "Gradient Descent(29/49): loss=0.36225170595684836\n",
      "Gradient Descent(30/49): loss=0.3618224493402958\n",
      "Gradient Descent(31/49): loss=0.36141504097732446\n",
      "Gradient Descent(32/49): loss=0.36102820538947805\n",
      "Gradient Descent(33/49): loss=0.36066075658766006\n",
      "Gradient Descent(34/49): loss=0.3603115895962953\n",
      "Gradient Descent(35/49): loss=0.35997967312671975\n",
      "Gradient Descent(36/49): loss=0.3596640431936114\n",
      "Gradient Descent(37/49): loss=0.3593637975122182\n",
      "Gradient Descent(38/49): loss=0.3590780905476046\n",
      "Gradient Descent(39/49): loss=0.35880612911285387\n",
      "Gradient Descent(40/49): loss=0.35854716843308926\n",
      "Gradient Descent(41/49): loss=0.358300508607743\n",
      "Gradient Descent(42/49): loss=0.35806549141574384\n",
      "Gradient Descent(43/49): loss=0.3578414974179999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=0.3576279433192874\n",
      "Gradient Descent(45/49): loss=0.3574242795578691\n",
      "Gradient Descent(46/49): loss=0.35722998809618145\n",
      "Gradient Descent(47/49): loss=0.3570445803900069\n",
      "Gradient Descent(48/49): loss=0.3568675955168808\n",
      "Gradient Descent(49/49): loss=0.3566985984472191\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4371751352712309\n",
      "Gradient Descent(2/49): loss=0.4114180908872521\n",
      "Gradient Descent(3/49): loss=0.39931655126061\n",
      "Gradient Descent(4/49): loss=0.39261145672235176\n",
      "Gradient Descent(5/49): loss=0.3882606091008603\n",
      "Gradient Descent(6/49): loss=0.3850721422837372\n",
      "Gradient Descent(7/49): loss=0.38253717207325183\n",
      "Gradient Descent(8/49): loss=0.380414353692204\n",
      "Gradient Descent(9/49): loss=0.3785760040016384\n",
      "Gradient Descent(10/49): loss=0.3769476348349534\n",
      "Gradient Descent(11/49): loss=0.37548223893739385\n",
      "Gradient Descent(12/49): loss=0.3741482986431653\n",
      "Gradient Descent(13/49): loss=0.3729236526557401\n",
      "Gradient Descent(14/49): loss=0.37179209664113955\n",
      "Gradient Descent(15/49): loss=0.37074137649231603\n",
      "Gradient Descent(16/49): loss=0.3697619464164405\n",
      "Gradient Descent(17/49): loss=0.36884617225572797\n",
      "Gradient Descent(18/49): loss=0.3679878054438818\n",
      "Gradient Descent(19/49): loss=0.3671816268946458\n",
      "Gradient Descent(20/49): loss=0.36642320037851595\n",
      "Gradient Descent(21/49): loss=0.36570869801731587\n",
      "Gradient Descent(22/49): loss=0.3650347742467369\n",
      "Gradient Descent(23/49): loss=0.36439847298012784\n",
      "Gradient Descent(24/49): loss=0.3637971579386437\n",
      "Gradient Descent(25/49): loss=0.3632284594369623\n",
      "Gradient Descent(26/49): loss=0.36269023306048226\n",
      "Gradient Descent(27/49): loss=0.36218052707812104\n",
      "Gradient Descent(28/49): loss=0.3616975563729493\n",
      "Gradient Descent(29/49): loss=0.3612396813075453\n",
      "Gradient Descent(30/49): loss=0.3608053903768973\n",
      "Gradient Descent(31/49): loss=0.3603932858056855\n",
      "Gradient Descent(32/49): loss=0.36000207146188784\n",
      "Gradient Descent(33/49): loss=0.3596305426130297\n",
      "Gradient Descent(34/49): loss=0.35927757716367326\n",
      "Gradient Descent(35/49): loss=0.3589421280954271\n",
      "Gradient Descent(36/49): loss=0.3586232168923702\n",
      "Gradient Descent(37/49): loss=0.3583199277811935\n",
      "Gradient Descent(38/49): loss=0.35803140265066624\n",
      "Gradient Descent(39/49): loss=0.3577568365421534\n",
      "Gradient Descent(40/49): loss=0.3574954736239032\n",
      "Gradient Descent(41/49): loss=0.35724660357821764\n",
      "Gradient Descent(42/49): loss=0.3570095583435156\n",
      "Gradient Descent(43/49): loss=0.35678370916350616\n",
      "Gradient Descent(44/49): loss=0.35656846390383806\n",
      "Gradient Descent(45/49): loss=0.3563632646031231\n",
      "Gradient Descent(46/49): loss=0.3561675852305117\n",
      "Gradient Descent(47/49): loss=0.35598092962628386\n",
      "Gradient Descent(48/49): loss=0.3558028296054256\n",
      "Gradient Descent(49/49): loss=0.35563284320703586\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4359743461011397\n",
      "Gradient Descent(2/49): loss=0.40982499296106434\n",
      "Gradient Descent(3/49): loss=0.3976320089387983\n",
      "Gradient Descent(4/49): loss=0.39094325889001524\n",
      "Gradient Descent(5/49): loss=0.38664431339179517\n",
      "Gradient Descent(6/49): loss=0.383516433518378\n",
      "Gradient Descent(7/49): loss=0.38104092730021477\n",
      "Gradient Descent(8/49): loss=0.37897310597133826\n",
      "Gradient Descent(9/49): loss=0.3771844011562295\n",
      "Gradient Descent(10/49): loss=0.37560036487366044\n",
      "Gradient Descent(11/49): loss=0.37417435910442676\n",
      "Gradient Descent(12/49): loss=0.37287532968698406\n",
      "Gradient Descent(13/49): loss=0.3716815811030606\n",
      "Gradient Descent(14/49): loss=0.3705773428337702\n",
      "Gradient Descent(15/49): loss=0.3695507521523578\n",
      "Gradient Descent(16/49): loss=0.3685926114783374\n",
      "Gradient Descent(17/49): loss=0.3676955947463233\n",
      "Gradient Descent(18/49): loss=0.36685372562015334\n",
      "Gradient Descent(19/49): loss=0.3660620257501517\n",
      "Gradient Descent(20/49): loss=0.3653162721891131\n",
      "Gradient Descent(21/49): loss=0.364612826452941\n",
      "Gradient Descent(22/49): loss=0.3639485115631312\n",
      "Gradient Descent(23/49): loss=0.36332052184487135\n",
      "Gradient Descent(24/49): loss=0.3627263555036594\n",
      "Gradient Descent(25/49): loss=0.3621637633285349\n",
      "Gradient Descent(26/49): loss=0.3616307090111855\n",
      "Gradient Descent(27/49): loss=0.3611253379707769\n",
      "Gradient Descent(28/49): loss=0.3606459525048464\n",
      "Gradient Descent(29/49): loss=0.3601909917144331\n",
      "Gradient Descent(30/49): loss=0.35975901508180674\n",
      "Gradient Descent(31/49): loss=0.3593486888784477\n",
      "Gradient Descent(32/49): loss=0.35895877479223687\n",
      "Gradient Descent(33/49): loss=0.3585881203141539\n",
      "Gradient Descent(34/49): loss=0.35823565053461914\n",
      "Gradient Descent(35/49): loss=0.35790036108036316\n",
      "Gradient Descent(36/49): loss=0.35758131198276466\n",
      "Gradient Descent(37/49): loss=0.3572776223137359\n",
      "Gradient Descent(38/49): loss=0.356988465459535\n",
      "Gradient Descent(39/49): loss=0.35671306492914695\n",
      "Gradient Descent(40/49): loss=0.35645069061419116\n",
      "Gradient Descent(41/49): loss=0.3562006554331274\n",
      "Gradient Descent(42/49): loss=0.3559623123049452\n",
      "Gradient Descent(43/49): loss=0.355735051407332\n",
      "Gradient Descent(44/49): loss=0.3555182976821083\n",
      "Gradient Descent(45/49): loss=0.3553115085569667\n",
      "Gradient Descent(46/49): loss=0.35511417185756916\n",
      "Gradient Descent(47/49): loss=0.35492580388813216\n",
      "Gradient Descent(48/49): loss=0.3547459476619393\n",
      "Gradient Descent(49/49): loss=0.35457417126593715\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4379125118858876\n",
      "Gradient Descent(2/49): loss=0.41234964566853843\n",
      "Gradient Descent(3/49): loss=0.4003302111249082\n",
      "Gradient Descent(4/49): loss=0.3936898595412595\n",
      "Gradient Descent(5/49): loss=0.3894037036096123\n",
      "Gradient Descent(6/49): loss=0.38628016905525714\n",
      "Gradient Descent(7/49): loss=0.3838082949723883\n",
      "Gradient Descent(8/49): loss=0.38174523317592773\n",
      "Gradient Descent(9/49): loss=0.3799625721639989\n",
      "Gradient Descent(10/49): loss=0.378385608244772\n",
      "Gradient Descent(11/49): loss=0.3769673987274467\n",
      "Gradient Descent(12/49): loss=0.375676626670231\n",
      "Gradient Descent(13/49): loss=0.3744913887241001\n",
      "Gradient Descent(14/49): loss=0.3733957549186612\n",
      "Gradient Descent(15/49): loss=0.3723777417334924\n",
      "Gradient Descent(16/49): loss=0.37142806074717694\n",
      "Gradient Descent(17/49): loss=0.3705393180362302\n",
      "Gradient Descent(18/49): loss=0.3697054870054602\n",
      "Gradient Descent(19/49): loss=0.3689215525413995\n",
      "Gradient Descent(20/49): loss=0.3681832653173343\n",
      "Gradient Descent(21/49): loss=0.3674869685007593\n",
      "Gradient Descent(22/49): loss=0.36682947301552643\n",
      "Gradient Descent(23/49): loss=0.36620796598952393\n",
      "Gradient Descent(24/49): loss=0.36561994230164585\n",
      "Gradient Descent(25/49): loss=0.36506315249374016\n",
      "Gradient Descent(26/49): loss=0.3645355624752198\n",
      "Gradient Descent(27/49): loss=0.3640353218646294\n",
      "Gradient Descent(28/49): loss=0.3635607387550785\n",
      "Gradient Descent(29/49): loss=0.3631102593273431\n",
      "Gradient Descent(30/49): loss=0.36268245117132053\n",
      "Gradient Descent(31/49): loss=0.36227598948068723\n",
      "Gradient Descent(32/49): loss=0.3618896455004497\n",
      "Gradient Descent(33/49): loss=0.36152227676093107\n",
      "Gradient Descent(34/49): loss=0.3611728187433872\n",
      "Gradient Descent(35/49): loss=0.36084027770445454\n",
      "Gradient Descent(36/49): loss=0.36052372444758457\n",
      "Gradient Descent(37/49): loss=0.36022228887539615\n",
      "Gradient Descent(38/49): loss=0.3599351551916115\n",
      "Gradient Descent(39/49): loss=0.3596615576478263\n",
      "Gradient Descent(40/49): loss=0.3594007767509008\n",
      "Gradient Descent(41/49): loss=0.3591521358627431\n",
      "Gradient Descent(42/49): loss=0.3589149981367887\n",
      "Gradient Descent(43/49): loss=0.3586887637453892\n",
      "Gradient Descent(44/49): loss=0.3584728673601978\n",
      "Gradient Descent(45/49): loss=0.35826677585394184\n",
      "Gradient Descent(46/49): loss=0.35806998619706476\n",
      "Gradient Descent(47/49): loss=0.3578820235268283\n",
      "Gradient Descent(48/49): loss=0.357702439369835\n",
      "Gradient Descent(49/49): loss=0.3575308100016832\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4338245036385825\n",
      "Gradient Descent(2/49): loss=0.4087688986064621\n",
      "Gradient Descent(3/49): loss=0.3975445325632823\n",
      "Gradient Descent(4/49): loss=0.3914043325969469\n",
      "Gradient Descent(5/49): loss=0.3873833047749716\n",
      "Gradient Descent(6/49): loss=0.38439125808718255\n",
      "Gradient Descent(7/49): loss=0.3819800301412938\n",
      "Gradient Descent(8/49): loss=0.3799399895950996\n",
      "Gradient Descent(9/49): loss=0.3781600987116779\n",
      "Gradient Descent(10/49): loss=0.37657515785033757\n",
      "Gradient Descent(11/49): loss=0.3751437286995643\n",
      "Gradient Descent(12/49): loss=0.37383782093623946\n",
      "Gradient Descent(13/49): loss=0.3726375775376611\n",
      "Gradient Descent(14/49): loss=0.3715283163580772\n",
      "Gradient Descent(15/49): loss=0.3704987861417408\n",
      "Gradient Descent(16/49): loss=0.36954009290449297\n",
      "Gradient Descent(17/49): loss=0.3686450154669576\n",
      "Gradient Descent(18/49): loss=0.36780755561348927\n",
      "Gradient Descent(19/49): loss=0.3670226341054786\n",
      "Gradient Descent(20/49): loss=0.36628587981985444\n",
      "Gradient Descent(21/49): loss=0.3655934798324389\n",
      "Gradient Descent(22/49): loss=0.36494207033264753\n",
      "Gradient Descent(23/49): loss=0.36432865551465576\n",
      "Gradient Descent(24/49): loss=0.36375054605082013\n",
      "Gradient Descent(25/49): loss=0.3632053115503521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=0.36269074319533373\n",
      "Gradient Descent(27/49): loss=0.3622048239130153\n",
      "Gradient Descent(28/49): loss=0.36174570421908153\n",
      "Gradient Descent(29/49): loss=0.36131168239201045\n",
      "Gradient Descent(30/49): loss=0.360901188000937\n",
      "Gradient Descent(31/49): loss=0.3605127680634623\n",
      "Gradient Descent(32/49): loss=0.36014507529078443\n",
      "Gradient Descent(33/49): loss=0.35979685800826516\n",
      "Gradient Descent(34/49): loss=0.35946695143526713\n",
      "Gradient Descent(35/49): loss=0.35915427007903467\n",
      "Gradient Descent(36/49): loss=0.35885780105053416\n",
      "Gradient Descent(37/49): loss=0.358576598150392\n",
      "Gradient Descent(38/49): loss=0.3583097766037926\n",
      "Gradient Descent(39/49): loss=0.35805650834687214\n",
      "Gradient Descent(40/49): loss=0.35781601778553507\n",
      "Gradient Descent(41/49): loss=0.3575875779620185\n",
      "Gradient Descent(42/49): loss=0.3573705070758845\n",
      "Gradient Descent(43/49): loss=0.3571641653151437\n",
      "Gradient Descent(44/49): loss=0.3569679519604303\n",
      "Gradient Descent(45/49): loss=0.3567813027309641\n",
      "Gradient Descent(46/49): loss=0.3566036873457524\n",
      "Gradient Descent(47/49): loss=0.3564346072773315\n",
      "Gradient Descent(48/49): loss=0.35627359367851913\n",
      "Gradient Descent(49/49): loss=0.35612020546526096\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43354516044742714\n",
      "Gradient Descent(2/49): loss=0.4084401401094778\n",
      "Gradient Descent(3/49): loss=0.39715358965856257\n",
      "Gradient Descent(4/49): loss=0.3909336383331764\n",
      "Gradient Descent(5/49): loss=0.38683332927528263\n",
      "Gradient Descent(6/49): loss=0.38377162618425514\n",
      "Gradient Descent(7/49): loss=0.3813023540524136\n",
      "Gradient Descent(8/49): loss=0.37921475853772474\n",
      "Gradient Descent(9/49): loss=0.37739583497120055\n",
      "Gradient Descent(10/49): loss=0.37577849537626506\n",
      "Gradient Descent(11/49): loss=0.37431976428040425\n",
      "Gradient Descent(12/49): loss=0.37299048180801914\n",
      "Gradient Descent(13/49): loss=0.3717699260130432\n",
      "Gradient Descent(14/49): loss=0.37064278054354693\n",
      "Gradient Descent(15/49): loss=0.36959732776825294\n",
      "Gradient Descent(16/49): loss=0.3686243273721695\n",
      "Gradient Descent(17/49): loss=0.36771629738847483\n",
      "Gradient Descent(18/49): loss=0.3668670400327879\n",
      "Gradient Descent(19/49): loss=0.36607132071976506\n",
      "Gradient Descent(20/49): loss=0.3653246453240217\n",
      "Gradient Descent(21/49): loss=0.36462310191280045\n",
      "Gradient Descent(22/49): loss=0.3639632457304581\n",
      "Gradient Descent(23/49): loss=0.3633420138251829\n",
      "Gradient Descent(24/49): loss=0.36275666041291477\n",
      "Gradient Descent(25/49): loss=0.36220470703595903\n",
      "Gradient Descent(26/49): loss=0.3616839034736387\n",
      "Gradient Descent(27/49): loss=0.36119219660310226\n",
      "Gradient Descent(28/49): loss=0.36072770523355374\n",
      "Gradient Descent(29/49): loss=0.36028869949581416\n",
      "Gradient Descent(30/49): loss=0.359873583753915\n",
      "Gradient Descent(31/49): loss=0.3594808822748922\n",
      "Gradient Descent(32/49): loss=0.3591092270846036\n",
      "Gradient Descent(33/49): loss=0.35875734757568795\n",
      "Gradient Descent(34/49): loss=0.3584240615349147\n",
      "Gradient Descent(35/49): loss=0.3581082673320351\n",
      "Gradient Descent(36/49): loss=0.35780893706828665\n",
      "Gradient Descent(37/49): loss=0.3575251105250853\n",
      "Gradient Descent(38/49): loss=0.35725588978580214\n",
      "Gradient Descent(39/49): loss=0.3570004344284496\n",
      "Gradient Descent(40/49): loss=0.3567579572064556\n",
      "Gradient Descent(41/49): loss=0.35652772014986484\n",
      "Gradient Descent(42/49): loss=0.35630903103125133\n",
      "Gradient Descent(43/49): loss=0.3561012401501169\n",
      "Gradient Descent(44/49): loss=0.35590373739714853\n",
      "Gradient Descent(45/49): loss=0.3557159495658108\n",
      "Gradient Descent(46/49): loss=0.3555373378837139\n",
      "Gradient Descent(47/49): loss=0.3553673957402373\n",
      "Gradient Descent(48/49): loss=0.3552056465902103\n",
      "Gradient Descent(49/49): loss=0.3550516420161956\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43227761017352817\n",
      "Gradient Descent(2/49): loss=0.4068086671880869\n",
      "Gradient Descent(3/49): loss=0.39546128963772087\n",
      "Gradient Descent(4/49): loss=0.3892771799066981\n",
      "Gradient Descent(5/49): loss=0.3852396142504341\n",
      "Gradient Descent(6/49): loss=0.3822441689145275\n",
      "Gradient Descent(7/49): loss=0.37983711789740193\n",
      "Gradient Descent(8/49): loss=0.3778056127863191\n",
      "Gradient Descent(9/49): loss=0.3760364358155857\n",
      "Gradient Descent(10/49): loss=0.37446291034767437\n",
      "Gradient Descent(11/49): loss=0.37304264637735535\n",
      "Gradient Descent(12/49): loss=0.371747081499839\n",
      "Gradient Descent(13/49): loss=0.37055604747949505\n",
      "Gradient Descent(14/49): loss=0.36945472242071464\n",
      "Gradient Descent(15/49): loss=0.36843182363924426\n",
      "Gradient Descent(16/49): loss=0.36747849139469774\n",
      "Gradient Descent(17/49): loss=0.3665875764988956\n",
      "Gradient Descent(18/49): loss=0.3657531726085993\n",
      "Gradient Descent(19/49): loss=0.3649703010288215\n",
      "Gradient Descent(20/49): loss=0.3642346929568408\n",
      "Gradient Descent(21/49): loss=0.36354263543182463\n",
      "Gradient Descent(22/49): loss=0.362890859865985\n",
      "Gradient Descent(23/49): loss=0.3622764596541143\n",
      "Gradient Descent(24/49): loss=0.3616968280545028\n",
      "Gradient Descent(25/49): loss=0.36114961048229355\n",
      "Gradient Descent(26/49): loss=0.3606326672412452\n",
      "Gradient Descent(27/49): loss=0.3601440439473742\n",
      "Gradient Descent(28/49): loss=0.35968194771207396\n",
      "Gradient Descent(29/49): loss=0.3592447277021009\n",
      "Gradient Descent(30/49): loss=0.3588308590716253\n",
      "Gradient Descent(31/49): loss=0.3584389295255471\n",
      "Gradient Descent(32/49): loss=0.35806762796065383\n",
      "Gradient Descent(33/49): loss=0.35771573476613605\n",
      "Gradient Descent(34/49): loss=0.35738211346346205\n",
      "Gradient Descent(35/49): loss=0.3570657034383672\n",
      "Gradient Descent(36/49): loss=0.356765513572067\n",
      "Gradient Descent(37/49): loss=0.356480616619822\n",
      "Gradient Descent(38/49): loss=0.3562101442162245\n",
      "Gradient Descent(39/49): loss=0.35595328241058954\n",
      "Gradient Descent(40/49): loss=0.3557092676544196\n",
      "Gradient Descent(41/49): loss=0.3554773831774354\n",
      "Gradient Descent(42/49): loss=0.3552569557000661\n",
      "Gradient Descent(43/49): loss=0.3550473524393305\n",
      "Gradient Descent(44/49): loss=0.3548479783722371\n",
      "Gradient Descent(45/49): loss=0.35465827372661113\n",
      "Gradient Descent(46/49): loss=0.35447771167391867\n",
      "Gradient Descent(47/49): loss=0.3543057962024531\n",
      "Gradient Descent(48/49): loss=0.3541420601523508\n",
      "Gradient Descent(49/49): loss=0.35398606339645666\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43431810316013153\n",
      "Gradient Descent(2/49): loss=0.4093914496513025\n",
      "Gradient Descent(3/49): loss=0.398183165171795\n",
      "Gradient Descent(4/49): loss=0.3920316868777341\n",
      "Gradient Descent(5/49): loss=0.3880014912397015\n",
      "Gradient Descent(6/49): loss=0.38500941791378845\n",
      "Gradient Descent(7/49): loss=0.38260667710850016\n",
      "Gradient Descent(8/49): loss=0.38058111863623106\n",
      "Gradient Descent(9/49): loss=0.37881925031613645\n",
      "Gradient Descent(10/49): loss=0.377253986215558\n",
      "Gradient Descent(11/49): loss=0.37584258142634497\n",
      "Gradient Descent(12/49): loss=0.3745561984589468\n",
      "Gradient Descent(13/49): loss=0.37337446303899013\n",
      "Gradient Descent(14/49): loss=0.372282400922388\n",
      "Gradient Descent(15/49): loss=0.37126861760648305\n",
      "Gradient Descent(16/49): loss=0.37032417188649314\n",
      "Gradient Descent(17/49): loss=0.36944185582614053\n",
      "Gradient Descent(18/49): loss=0.3686157213771929\n",
      "Gradient Descent(19/49): loss=0.3678407609875723\n",
      "Gradient Descent(20/49): loss=0.3671126867479031\n",
      "Gradient Descent(21/49): loss=0.36642777405137883\n",
      "Gradient Descent(22/49): loss=0.36578274842476505\n",
      "Gradient Descent(23/49): loss=0.365174701865528\n",
      "Gradient Descent(24/49): loss=0.36460102975954556\n",
      "Gradient Descent(25/49): loss=0.3640593824347019\n",
      "Gradient Descent(26/49): loss=0.363547627314959\n",
      "Gradient Descent(27/49): loss=0.36306381888482436\n",
      "Gradient Descent(28/49): loss=0.3626061745010915\n",
      "Gradient Descent(29/49): loss=0.3621730546476278\n",
      "Gradient Descent(30/49): loss=0.36176294661319597\n",
      "Gradient Descent(31/49): loss=0.3613744508407394\n",
      "Gradient Descent(32/49): loss=0.36100626938699504\n",
      "Gradient Descent(33/49): loss=0.3606571960683447\n",
      "Gradient Descent(34/49): loss=0.36032610796871795\n",
      "Gradient Descent(35/49): loss=0.36001195805910025\n",
      "Gradient Descent(36/49): loss=0.35971376873321015\n",
      "Gradient Descent(37/49): loss=0.35943062610539717\n",
      "Gradient Descent(38/49): loss=0.35916167494837553\n",
      "Gradient Descent(39/49): loss=0.3589061141726565\n",
      "Gradient Descent(40/49): loss=0.358663192768314\n",
      "Gradient Descent(41/49): loss=0.35843220614437343\n",
      "Gradient Descent(42/49): loss=0.3582124928126412\n",
      "Gradient Descent(43/49): loss=0.3580034313719256\n",
      "Gradient Descent(44/49): loss=0.3578044377558914\n",
      "Gradient Descent(45/49): loss=0.35761496271364984\n",
      "Gradient Descent(46/49): loss=0.35743448949692785\n",
      "Gradient Descent(47/49): loss=0.3572625317315256\n",
      "Gradient Descent(48/49): loss=0.3570986314539401\n",
      "Gradient Descent(49/49): loss=0.356942357296649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43037577296977275\n",
      "Gradient Descent(2/49): loss=0.4061324297739267\n",
      "Gradient Descent(3/49): loss=0.39569243971955687\n",
      "Gradient Descent(4/49): loss=0.3899684644543507\n",
      "Gradient Descent(5/49): loss=0.38614463725239667\n",
      "Gradient Descent(6/49): loss=0.38324552822012325\n",
      "Gradient Descent(7/49): loss=0.38087921635979033\n",
      "Gradient Descent(8/49): loss=0.3788612878811512\n",
      "Gradient Descent(9/49): loss=0.37709231090285483\n",
      "Gradient Descent(10/49): loss=0.37551286515657734\n",
      "Gradient Descent(11/49): loss=0.37408463983380263\n",
      "Gradient Descent(12/49): loss=0.37278143092176963\n",
      "Gradient Descent(13/49): loss=0.37158441610450105\n",
      "Gradient Descent(14/49): loss=0.37047949625634446\n",
      "Gradient Descent(15/49): loss=0.36945572198623544\n",
      "Gradient Descent(16/49): loss=0.3685043242543639\n",
      "Gradient Descent(17/49): loss=0.36761809599890566\n",
      "Gradient Descent(18/49): loss=0.36679098485636874\n",
      "Gradient Descent(19/49): loss=0.36601781673534406\n",
      "Gradient Descent(20/49): loss=0.3652941028307781\n",
      "Gradient Descent(21/49): loss=0.3646159013002738\n",
      "Gradient Descent(22/49): loss=0.363979715682407\n",
      "Gradient Descent(23/49): loss=0.3633824186181383\n",
      "Gradient Descent(24/49): loss=0.3628211933957896\n",
      "Gradient Descent(25/49): loss=0.3622934883151693\n",
      "Gradient Descent(26/49): loss=0.36179698044916103\n",
      "Gradient Descent(27/49): loss=0.3613295464157821\n",
      "Gradient Descent(28/49): loss=0.36088923846455584\n",
      "Gradient Descent(29/49): loss=0.36047426465154697\n",
      "Gradient Descent(30/49): loss=0.3600829722037694\n",
      "Gradient Descent(31/49): loss=0.35971383340389645\n",
      "Gradient Descent(32/49): loss=0.3593654334910659\n",
      "Gradient Descent(33/49): loss=0.3590364601932866\n",
      "Gradient Descent(34/49): loss=0.3587256945949561\n",
      "Gradient Descent(35/49): loss=0.35843200310843404\n",
      "Gradient Descent(36/49): loss=0.3581543303677788\n",
      "Gradient Descent(37/49): loss=0.3578916929000541\n",
      "Gradient Descent(38/49): loss=0.3576431734581682\n",
      "Gradient Descent(39/49): loss=0.35740791592125926\n",
      "Gradient Descent(40/49): loss=0.35718512068582325\n",
      "Gradient Descent(41/49): loss=0.35697404048426157\n",
      "Gradient Descent(42/49): loss=0.3567739765782025\n",
      "Gradient Descent(43/49): loss=0.3565842752824681\n",
      "Gradient Descent(44/49): loss=0.35640432478240125\n",
      "Gradient Descent(45/49): loss=0.35623355221281505\n",
      "Gradient Descent(46/49): loss=0.3560714209713518\n",
      "Gradient Descent(47/49): loss=0.3559174282427627\n",
      "Gradient Descent(48/49): loss=0.3557711027136988\n",
      "Gradient Descent(49/49): loss=0.3556320024601776\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43008700092053886\n",
      "Gradient Descent(2/49): loss=0.4057975207580892\n",
      "Gradient Descent(3/49): loss=0.3952862047228146\n",
      "Gradient Descent(4/49): loss=0.3894742545471078\n",
      "Gradient Descent(5/49): loss=0.385567408708262\n",
      "Gradient Descent(6/49): loss=0.3825982706520986\n",
      "Gradient Descent(7/49): loss=0.380175236215956\n",
      "Gradient Descent(8/49): loss=0.37811165380562667\n",
      "Gradient Descent(9/49): loss=0.3763055591745429\n",
      "Gradient Descent(10/49): loss=0.3746954171657004\n",
      "Gradient Descent(11/49): loss=0.3732413247146194\n",
      "Gradient Descent(12/49): loss=0.37191592686098085\n",
      "Gradient Descent(13/49): loss=0.37069957942849746\n",
      "Gradient Descent(14/49): loss=0.3695775946193794\n",
      "Gradient Descent(15/49): loss=0.36853859636634617\n",
      "Gradient Descent(16/49): loss=0.36757350111649234\n",
      "Gradient Descent(17/49): loss=0.3666748654476551\n",
      "Gradient Descent(18/49): loss=0.3658364557594401\n",
      "Gradient Descent(19/49): loss=0.3650529562139339\n",
      "Gradient Descent(20/49): loss=0.36431976503991154\n",
      "Gradient Descent(21/49): loss=0.36363284877256136\n",
      "Gradient Descent(22/49): loss=0.36298863542551724\n",
      "Gradient Descent(23/49): loss=0.3623839344480572\n",
      "Gradient Descent(24/49): loss=0.36181587552344824\n",
      "Gradient Descent(25/49): loss=0.36128186089675485\n",
      "Gradient Descent(26/49): loss=0.36077952760462584\n",
      "Gradient Descent(27/49): loss=0.36030671708007916\n",
      "Gradient Descent(28/49): loss=0.35986145033929584\n",
      "Gradient Descent(29/49): loss=0.3594419074565723\n",
      "Gradient Descent(30/49): loss=0.35904641037928137\n",
      "Gradient Descent(31/49): loss=0.35867340837815304\n",
      "Gradient Descent(32/49): loss=0.3583214656023276\n",
      "Gradient Descent(33/49): loss=0.35798925033492157\n",
      "Gradient Descent(34/49): loss=0.35767552563761723\n",
      "Gradient Descent(35/49): loss=0.3573791411417173\n",
      "Gradient Descent(36/49): loss=0.35709902579487723\n",
      "Gradient Descent(37/49): loss=0.3568341814119921\n",
      "Gradient Descent(38/49): loss=0.35658367690876897\n",
      "Gradient Descent(39/49): loss=0.3563466431197272\n",
      "Gradient Descent(40/49): loss=0.35612226812043857\n",
      "Gradient Descent(41/49): loss=0.35590979298800707\n",
      "Gradient Descent(42/49): loss=0.3557085079450141\n",
      "Gradient Descent(43/49): loss=0.3555177488411008\n",
      "Gradient Descent(44/49): loss=0.35533689393355183\n",
      "Gradient Descent(45/49): loss=0.3551653609340616\n",
      "Gradient Descent(46/49): loss=0.3550026042936103\n",
      "Gradient Descent(47/49): loss=0.3548481127012679\n",
      "Gradient Descent(48/49): loss=0.35470140677597184\n",
      "Gradient Descent(49/49): loss=0.35456203693299976\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42875634772767385\n",
      "Gradient Descent(2/49): loss=0.4041351731668741\n",
      "Gradient Descent(3/49): loss=0.3935922824678538\n",
      "Gradient Descent(4/49): loss=0.38783300733103204\n",
      "Gradient Descent(5/49): loss=0.3839975179200771\n",
      "Gradient Descent(6/49): loss=0.38109877608117937\n",
      "Gradient Descent(7/49): loss=0.3787397167195694\n",
      "Gradient Descent(8/49): loss=0.37673265325330396\n",
      "Gradient Descent(9/49): loss=0.37497595082009366\n",
      "Gradient Descent(10/49): loss=0.3734087803998629\n",
      "Gradient Descent(11/49): loss=0.3719919970609594\n",
      "Gradient Descent(12/49): loss=0.3706989518670957\n",
      "Gradient Descent(13/49): loss=0.36951062677067065\n",
      "Gradient Descent(14/49): loss=0.36841287912003645\n",
      "Gradient Descent(15/49): loss=0.36739480467521424\n",
      "Gradient Descent(16/49): loss=0.36644772811847837\n",
      "Gradient Descent(17/49): loss=0.3655645600195634\n",
      "Gradient Descent(18/49): loss=0.36473937472133167\n",
      "Gradient Descent(19/49): loss=0.3639671251961494\n",
      "Gradient Descent(20/49): loss=0.3632434450992852\n",
      "Gradient Descent(21/49): loss=0.3625645077705435\n",
      "Gradient Descent(22/49): loss=0.3619269233595287\n",
      "Gradient Descent(23/49): loss=0.3613276620823697\n",
      "Gradient Descent(24/49): loss=0.3607639957925718\n",
      "Gradient Descent(25/49): loss=0.36023345265506945\n",
      "Gradient Descent(26/49): loss=0.3597337813752257\n",
      "Gradient Descent(27/49): loss=0.35926292251799197\n",
      "Gradient Descent(28/49): loss=0.3588189851732378\n",
      "Gradient Descent(29/49): loss=0.35840022771228885\n",
      "Gradient Descent(30/49): loss=0.3580050417186554\n",
      "Gradient Descent(31/49): loss=0.35763193841344887\n",
      "Gradient Descent(32/49): loss=0.3572795370655018\n",
      "Gradient Descent(33/49): loss=0.35694655499889966\n",
      "Gradient Descent(34/49): loss=0.3566317989005469\n",
      "Gradient Descent(35/49): loss=0.35633415719705513\n",
      "Gradient Descent(36/49): loss=0.35605259332017125\n",
      "Gradient Descent(37/49): loss=0.35578613971773604\n",
      "Gradient Descent(38/49): loss=0.35553389249598716\n",
      "Gradient Descent(39/49): loss=0.3552950066012047\n",
      "Gradient Descent(40/49): loss=0.3550686914659182\n",
      "Gradient Descent(41/49): loss=0.35485420705835397\n",
      "Gradient Descent(42/49): loss=0.35465086028442194\n",
      "Gradient Descent(43/49): loss=0.3544580016999585\n",
      "Gradient Descent(44/49): loss=0.35427502249769743\n",
      "Gradient Descent(45/49): loss=0.35410135173886365\n",
      "Gradient Descent(46/49): loss=0.3539364538037058\n",
      "Gradient Descent(47/49): loss=0.35377982603888575\n",
      "Gradient Descent(48/49): loss=0.3536309965826193\n",
      "Gradient Descent(49/49): loss=0.3534895223509247\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4308924875627193\n",
      "Gradient Descent(2/49): loss=0.40676539827526303\n",
      "Gradient Descent(3/49): loss=0.3963306131878003\n",
      "Gradient Descent(4/49): loss=0.390592011617512\n",
      "Gradient Descent(5/49): loss=0.38676075914363767\n",
      "Gradient Descent(6/49): loss=0.38386548916235136\n",
      "Gradient Descent(7/49): loss=0.3815118323198196\n",
      "Gradient Descent(8/49): loss=0.3795120110871186\n",
      "Gradient Descent(9/49): loss=0.3777638584337266\n",
      "Gradient Descent(10/49): loss=0.3762060582513579\n",
      "Gradient Descent(11/49): loss=0.3747990922394728\n",
      "Gradient Descent(12/49): loss=0.37351603781285875\n",
      "Gradient Descent(13/49): loss=0.3723376792321767\n",
      "Gradient Descent(14/49): loss=0.3712497321194989\n",
      "Gradient Descent(15/49): loss=0.37024119143575224\n",
      "Gradient Descent(16/49): loss=0.3693033109910234\n",
      "Gradient Descent(17/49): loss=0.3684289524373765\n",
      "Gradient Descent(18/49): loss=0.36761215740166997\n",
      "Gradient Descent(19/49): loss=0.36684785819671345\n",
      "Gradient Descent(20/49): loss=0.36613167688173465\n",
      "Gradient Descent(21/49): loss=0.36545978208941854\n",
      "Gradient Descent(22/49): loss=0.3648287845538604\n",
      "Gradient Descent(23/49): loss=0.3642356591759951\n",
      "Gradient Descent(24/49): loss=0.36367768568920117\n",
      "Gradient Descent(25/49): loss=0.3631524026310855\n",
      "Gradient Descent(26/49): loss=0.36265757101616336\n",
      "Gradient Descent(27/49): loss=0.3621911452056764\n",
      "Gradient Descent(28/49): loss=0.3617512492039373\n",
      "Gradient Descent(29/49): loss=0.36133615710795014\n",
      "Gradient Descent(30/49): loss=0.36094427678058205\n",
      "Gradient Descent(31/49): loss=0.36057413605876754\n",
      "Gradient Descent(32/49): loss=0.3602243709801861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=0.3598937156361572\n",
      "Gradient Descent(34/49): loss=0.3595809933494764\n",
      "Gradient Descent(35/49): loss=0.35928510894329874\n",
      "Gradient Descent(36/49): loss=0.3590050419176056\n",
      "Gradient Descent(37/49): loss=0.35873984038791923\n",
      "Gradient Descent(38/49): loss=0.35848861567001883\n",
      "Gradient Descent(39/49): loss=0.3582505374168246\n",
      "Gradient Descent(40/49): loss=0.3580248292310064\n",
      "Gradient Descent(41/49): loss=0.3578107646905095\n",
      "Gradient Descent(42/49): loss=0.35760766373494185\n",
      "Gradient Descent(43/49): loss=0.35741488936934096\n",
      "Gradient Descent(44/49): loss=0.35723184464869595\n",
      "Gradient Descent(45/49): loss=0.3570579699121633\n",
      "Gradient Descent(46/49): loss=0.3568927402404307\n",
      "Gradient Descent(47/49): loss=0.3567356631133913\n",
      "Gradient Descent(48/49): loss=0.35658627624836337\n",
      "Gradient Descent(49/49): loss=0.3564441456016298\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42709745436761143\n",
      "Gradient Descent(2/49): loss=0.4037969115808165\n",
      "Gradient Descent(3/49): loss=0.39408518361927064\n",
      "Gradient Descent(4/49): loss=0.38870465214537814\n",
      "Gradient Descent(5/49): loss=0.3850308837652922\n",
      "Gradient Descent(6/49): loss=0.38219874705328\n",
      "Gradient Descent(7/49): loss=0.3798633683522825\n",
      "Gradient Descent(8/49): loss=0.377859865970831\n",
      "Gradient Descent(9/49): loss=0.3760975911870963\n",
      "Gradient Descent(10/49): loss=0.37452153332091864\n",
      "Gradient Descent(11/49): loss=0.37309578068032156\n",
      "Gradient Descent(12/49): loss=0.3717954718799639\n",
      "Gradient Descent(13/49): loss=0.37060251842028286\n",
      "Gradient Descent(14/49): loss=0.369503188427649\n",
      "Gradient Descent(15/49): loss=0.36848667641743527\n",
      "Gradient Descent(16/49): loss=0.3675442220638439\n",
      "Gradient Descent(17/49): loss=0.36666854687008604\n",
      "Gradient Descent(18/49): loss=0.3658534813969823\n",
      "Gradient Descent(19/49): loss=0.36509371048847233\n",
      "Gradient Descent(20/49): loss=0.3643845938644383\n",
      "Gradient Descent(21/49): loss=0.3637220362894172\n",
      "Gradient Descent(22/49): loss=0.363102391260388\n",
      "Gradient Descent(23/49): loss=0.36252238793790387\n",
      "Gradient Descent(24/49): loss=0.36197907457076417\n",
      "Gradient Descent(25/49): loss=0.3614697738718702\n",
      "Gradient Descent(26/49): loss=0.36099204722007655\n",
      "Gradient Descent(27/49): loss=0.36054366549443356\n",
      "Gradient Descent(28/49): loss=0.36012258497310884\n",
      "Gradient Descent(29/49): loss=0.3597269271581899\n",
      "Gradient Descent(30/49): loss=0.3593549616867889\n",
      "Gradient Descent(31/49): loss=0.359005091700968\n",
      "Gradient Descent(32/49): loss=0.35867584120150564\n",
      "Gradient Descent(33/49): loss=0.3583658440216188\n",
      "Gradient Descent(34/49): loss=0.35807383413863175\n",
      "Gradient Descent(35/49): loss=0.3577986371026085\n",
      "Gradient Descent(36/49): loss=0.35753916240690137\n",
      "Gradient Descent(37/49): loss=0.3572943966605091\n",
      "Gradient Descent(38/49): loss=0.35706339744895177\n",
      "Gradient Descent(39/49): loss=0.3568452877911439\n",
      "Gradient Descent(40/49): loss=0.35663925111599015\n",
      "Gradient Descent(41/49): loss=0.3564445266952394\n",
      "Gradient Descent(42/49): loss=0.35626040547932675\n",
      "Gradient Descent(43/49): loss=0.3560862262911137\n",
      "Gradient Descent(44/49): loss=0.3559213723390614\n",
      "Gradient Descent(45/49): loss=0.3557652680167786\n",
      "Gradient Descent(46/49): loss=0.3556173759603322\n",
      "Gradient Descent(47/49): loss=0.3554771943384009\n",
      "Gradient Descent(48/49): loss=0.35534425435343575\n",
      "Gradient Descent(49/49): loss=0.3552181179345881\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4268006566905662\n",
      "Gradient Descent(2/49): loss=0.40345588092695944\n",
      "Gradient Descent(3/49): loss=0.39366263197075035\n",
      "Gradient Descent(4/49): loss=0.38818673207414933\n",
      "Gradient Descent(5/49): loss=0.38442750369641493\n",
      "Gradient Descent(6/49): loss=0.3815259043318664\n",
      "Gradient Descent(7/49): loss=0.3791355798948481\n",
      "Gradient Descent(8/49): loss=0.37708842951498894\n",
      "Gradient Descent(9/49): loss=0.3752908637376132\n",
      "Gradient Descent(10/49): loss=0.3736856420008194\n",
      "Gradient Descent(11/49): loss=0.3722352694439665\n",
      "Gradient Descent(12/49): loss=0.3709137836348503\n",
      "Gradient Descent(13/49): loss=0.36970232934464864\n",
      "Gradient Descent(14/49): loss=0.36858663369213446\n",
      "Gradient Descent(15/49): loss=0.3675555018844562\n",
      "Gradient Descent(16/49): loss=0.3665998870272046\n",
      "Gradient Descent(17/49): loss=0.36571229467840277\n",
      "Gradient Descent(18/49): loss=0.3648863889193737\n",
      "Gradient Descent(19/49): loss=0.3641167234724299\n",
      "Gradient Descent(20/49): loss=0.3633985527270829\n",
      "Gradient Descent(21/49): loss=0.36272769529516924\n",
      "Gradient Descent(22/49): loss=0.3621004330342759\n",
      "Gradient Descent(23/49): loss=0.3615134346250462\n",
      "Gradient Descent(24/49): loss=0.36096369654105964\n",
      "Gradient Descent(25/49): loss=0.3604484965993239\n",
      "Gradient Descent(26/49): loss=0.3599653567860931\n",
      "Gradient Descent(27/49): loss=0.3595120130415942\n",
      "Gradient Descent(28/49): loss=0.3590863903504385\n",
      "Gradient Descent(29/49): loss=0.3586865819382056\n",
      "Gradient Descent(30/49): loss=0.3583108316907229\n",
      "Gradient Descent(31/49): loss=0.35795751913630725\n",
      "Gradient Descent(32/49): loss=0.3576251464919732\n",
      "Gradient Descent(33/49): loss=0.3573123273916234\n",
      "Gradient Descent(34/49): loss=0.3570177770004456\n",
      "Gradient Descent(35/49): loss=0.35674030328397227\n",
      "Gradient Descent(36/49): loss=0.35647879924861414\n",
      "Gradient Descent(37/49): loss=0.35623223600723697\n",
      "Gradient Descent(38/49): loss=0.3559996565515675\n",
      "Gradient Descent(39/49): loss=0.35578017013506513\n",
      "Gradient Descent(40/49): loss=0.35557294718697047\n",
      "Gradient Descent(41/49): loss=0.35537721469170175\n",
      "Gradient Descent(42/49): loss=0.3551922519784741\n",
      "Gradient Descent(43/49): loss=0.3550173868745888\n",
      "Gradient Descent(44/49): loss=0.3548519921827834\n",
      "Gradient Descent(45/49): loss=0.35469548244867877\n",
      "Gradient Descent(46/49): loss=0.3545473109890033\n",
      "Gradient Descent(47/49): loss=0.3544069671551191\n",
      "Gradient Descent(48/49): loss=0.35427397380957715\n",
      "Gradient Descent(49/49): loss=0.35414788499611904\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42541055876357686\n",
      "Gradient Descent(2/49): loss=0.4017693998302492\n",
      "Gradient Descent(3/49): loss=0.3919719923073241\n",
      "Gradient Descent(4/49): loss=0.3865630571878352\n",
      "Gradient Descent(5/49): loss=0.3828819168192571\n",
      "Gradient Descent(6/49): loss=0.38005365097206123\n",
      "Gradient Descent(7/49): loss=0.3777282993241804\n",
      "Gradient Descent(8/49): loss=0.37573760226560804\n",
      "Gradient Descent(9/49): loss=0.3739887363571045\n",
      "Gradient Descent(10/49): loss=0.37242540492802895\n",
      "Gradient Descent(11/49): loss=0.3710110126237815\n",
      "Gradient Descent(12/49): loss=0.3697203934409843\n",
      "Gradient Descent(13/49): loss=0.3685353795115477\n",
      "Gradient Descent(14/49): loss=0.36744228651410804\n",
      "Gradient Descent(15/49): loss=0.3664304233838289\n",
      "Gradient Descent(16/49): loss=0.36549117551770444\n",
      "Gradient Descent(17/49): loss=0.36461742089855365\n",
      "Gradient Descent(18/49): loss=0.36380314576430445\n",
      "Gradient Descent(19/49): loss=0.3630431835743603\n",
      "Gradient Descent(20/49): loss=0.362333032440794\n",
      "Gradient Descent(21/49): loss=0.3616687239324583\n",
      "Gradient Descent(22/49): loss=0.3610467264310059\n",
      "Gradient Descent(23/49): loss=0.360463872314023\n",
      "Gradient Descent(24/49): loss=0.35991730195081334\n",
      "Gradient Descent(25/49): loss=0.35940441981185617\n",
      "Gradient Descent(26/49): loss=0.35892285947379987\n",
      "Gradient Descent(27/49): loss=0.35847045527128096\n",
      "Gradient Descent(28/49): loss=0.35804521899549857\n",
      "Gradient Descent(29/49): loss=0.35764532048223774\n",
      "Gradient Descent(30/49): loss=0.3572690712397706\n",
      "Gradient Descent(31/49): loss=0.35691491048446056\n",
      "Gradient Descent(32/49): loss=0.3565813931076939\n",
      "Gradient Descent(33/49): loss=0.356267179210908\n",
      "Gradient Descent(34/49): loss=0.355971024928617\n",
      "Gradient Descent(35/49): loss=0.35569177432108734\n",
      "Gradient Descent(36/49): loss=0.3554283521646538\n",
      "Gradient Descent(37/49): loss=0.3551797575027744\n",
      "Gradient Descent(38/49): loss=0.3549450578477583\n",
      "Gradient Descent(39/49): loss=0.35472338394381264\n",
      "Gradient Descent(40/49): loss=0.35451392501815815\n",
      "Gradient Descent(41/49): loss=0.35431592445961446\n",
      "Gradient Descent(42/49): loss=0.3541286758740598\n",
      "Gradient Descent(43/49): loss=0.35395151947415887\n",
      "Gradient Descent(44/49): loss=0.35378383876718333\n",
      "Gradient Descent(45/49): loss=0.35362505750996165\n",
      "Gradient Descent(46/49): loss=0.35347463690426784\n",
      "Gradient Descent(47/49): loss=0.35333207300947106\n",
      "Gradient Descent(48/49): loss=0.35319689435219725\n",
      "Gradient Descent(49/49): loss=0.3530686597151991\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42763566509365075\n",
      "Gradient Descent(2/49): loss=0.40443773108993575\n",
      "Gradient Descent(3/49): loss=0.39472130888384915\n",
      "Gradient Descent(4/49): loss=0.3893244148640344\n",
      "Gradient Descent(5/49): loss=0.3856460678480201\n",
      "Gradient Descent(6/49): loss=0.3828220416863132\n",
      "Gradient Descent(7/49): loss=0.3805033833395406\n",
      "Gradient Descent(8/49): loss=0.3785212327183355\n",
      "Gradient Descent(9/49): loss=0.37678209234055376\n",
      "Gradient Descent(10/49): loss=0.3752291449424777\n",
      "Gradient Descent(11/49): loss=0.3738254205879363\n",
      "Gradient Descent(12/49): loss=0.37254548864236325\n",
      "Gradient Descent(13/49): loss=0.3713709957957991\n",
      "Gradient Descent(14/49): loss=0.3702881288376972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=0.3692861081993977\n",
      "Gradient Descent(16/49): loss=0.3683562597169607\n",
      "Gradient Descent(17/49): loss=0.36749142269659824\n",
      "Gradient Descent(18/49): loss=0.3666855599195764\n",
      "Gradient Descent(19/49): loss=0.36593349261727337\n",
      "Gradient Descent(20/49): loss=0.3652307150673827\n",
      "Gradient Descent(21/49): loss=0.3645732613561738\n",
      "Gradient Descent(22/49): loss=0.36395760723535\n",
      "Gradient Descent(23/49): loss=0.36338059617876534\n",
      "Gradient Descent(24/49): loss=0.3628393825106622\n",
      "Gradient Descent(25/49): loss=0.3623313868307831\n",
      "Gradient Descent(26/49): loss=0.3618542604680862\n",
      "Gradient Descent(27/49): loss=0.3614058566810443\n",
      "Gradient Descent(28/49): loss=0.36098420698207856\n",
      "Gradient Descent(29/49): loss=0.36058750141347\n",
      "Gradient Descent(30/49): loss=0.3602140719143272\n",
      "Gradient Descent(31/49): loss=0.3598623781384381\n",
      "Gradient Descent(32/49): loss=0.3595309952404768\n",
      "Gradient Descent(33/49): loss=0.3592186032623727\n",
      "Gradient Descent(34/49): loss=0.35892397783557645\n",
      "Gradient Descent(35/49): loss=0.3586459819772887\n",
      "Gradient Descent(36/49): loss=0.3583835588054692\n",
      "Gradient Descent(37/49): loss=0.3581357250328975\n",
      "Gradient Descent(38/49): loss=0.35790156512769\n",
      "Gradient Descent(39/49): loss=0.35768022604863936\n",
      "Gradient Descent(40/49): loss=0.35747091248009405\n",
      "Gradient Descent(41/49): loss=0.3572728825039623\n",
      "Gradient Descent(42/49): loss=0.3570854436566351\n",
      "Gradient Descent(43/49): loss=0.3569079493268046\n",
      "Gradient Descent(44/49): loss=0.3567397954567577\n",
      "Gradient Descent(45/49): loss=0.3565804175150994\n",
      "Gradient Descent(46/49): loss=0.35642928771328014\n",
      "Gradient Descent(47/49): loss=0.35628591244194624\n",
      "Gradient Descent(48/49): loss=0.35614982990617755\n",
      "Gradient Descent(49/49): loss=0.35602060794122964\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4239895478320984\n",
      "Gradient Descent(2/49): loss=0.4017303165380944\n",
      "Gradient Descent(3/49): loss=0.3926791054273059\n",
      "Gradient Descent(4/49): loss=0.3875767514061933\n",
      "Gradient Descent(5/49): loss=0.3840160744163417\n",
      "Gradient Descent(6/49): loss=0.381232117597702\n",
      "Gradient Descent(7/49): loss=0.3789179645547326\n",
      "Gradient Descent(8/49): loss=0.37692373181400707\n",
      "Gradient Descent(9/49): loss=0.37516554232949245\n",
      "Gradient Descent(10/49): loss=0.37359183728031803\n",
      "Gradient Descent(11/49): loss=0.372168565501165\n",
      "Gradient Descent(12/49): loss=0.37087185769772624\n",
      "Gradient Descent(13/49): loss=0.3696841135279104\n",
      "Gradient Descent(14/49): loss=0.3685917920631679\n",
      "Gradient Descent(15/49): loss=0.3675841047008848\n",
      "Gradient Descent(16/49): loss=0.3666522083823399\n",
      "Gradient Descent(17/49): loss=0.36578868739963355\n",
      "Gradient Descent(18/49): loss=0.364987207997755\n",
      "Gradient Descent(19/49): loss=0.36424228019316107\n",
      "Gradient Descent(20/49): loss=0.3635490883941862\n",
      "Gradient Descent(21/49): loss=0.36290336756869135\n",
      "Gradient Descent(22/49): loss=0.3623013104312415\n",
      "Gradient Descent(23/49): loss=0.361739496302499\n",
      "Gradient Descent(24/49): loss=0.361214835462104\n",
      "Gradient Descent(25/49): loss=0.3607245248102323\n",
      "Gradient Descent(26/49): loss=0.36026601194122376\n",
      "Gradient Descent(27/49): loss=0.3598369655850245\n",
      "Gradient Descent(28/49): loss=0.3594352509483335\n",
      "Gradient Descent(29/49): loss=0.3590589088842215\n",
      "Gradient Descent(30/49): loss=0.35870613809701724\n",
      "Gradient Descent(31/49): loss=0.3583752797869783\n",
      "Gradient Descent(32/49): loss=0.358064804281792\n",
      "Gradient Descent(33/49): loss=0.35777329930599805\n",
      "Gradient Descent(34/49): loss=0.3574994596162661\n",
      "Gradient Descent(35/49): loss=0.35724207778785\n",
      "Gradient Descent(36/49): loss=0.3570000359808607\n",
      "Gradient Descent(37/49): loss=0.3567722985480455\n",
      "Gradient Descent(38/49): loss=0.35655790537122733\n",
      "Gradient Descent(39/49): loss=0.3563559658333819\n",
      "Gradient Descent(40/49): loss=0.3561656533489159\n",
      "Gradient Descent(41/49): loss=0.3559862003870824\n",
      "Gradient Descent(42/49): loss=0.35581689393339005\n",
      "Gradient Descent(43/49): loss=0.3556570713418839\n",
      "Gradient Descent(44/49): loss=0.35550611653773156\n",
      "Gradient Descent(45/49): loss=0.35536345653494117\n",
      "Gradient Descent(46/49): loss=0.3552285582385242\n",
      "Gradient Descent(47/49): loss=0.35510092550416894\n",
      "Gradient Descent(48/49): loss=0.35498009643166056\n",
      "Gradient Descent(49/49): loss=0.3548656408709756\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42368612775750913\n",
      "Gradient Descent(2/49): loss=0.4013829144637594\n",
      "Gradient Descent(3/49): loss=0.3922392897136409\n",
      "Gradient Descent(4/49): loss=0.38703524286427876\n",
      "Gradient Descent(5/49): loss=0.3833878737458048\n",
      "Gradient Descent(6/49): loss=0.38053573614393765\n",
      "Gradient Descent(7/49): loss=0.3781686668705295\n",
      "Gradient Descent(8/49): loss=0.37613276849456073\n",
      "Gradient Descent(9/49): loss=0.37434096726097715\n",
      "Gradient Descent(10/49): loss=0.37273944924765806\n",
      "Gradient Descent(11/49): loss=0.3712926326840517\n",
      "Gradient Descent(12/49): loss=0.36997561375394805\n",
      "Gradient Descent(13/49): loss=0.3687700836999977\n",
      "Gradient Descent(14/49): loss=0.3676620056791555\n",
      "Gradient Descent(15/49): loss=0.36664023474171964\n",
      "Gradient Descent(16/49): loss=0.3656956647346172\n",
      "Gradient Descent(17/49): loss=0.36482068054790867\n",
      "Gradient Descent(18/49): loss=0.3640087935718578\n",
      "Gradient Descent(19/49): loss=0.3632543908629195\n",
      "Gradient Descent(20/49): loss=0.3625525572089103\n",
      "Gradient Descent(21/49): loss=0.36189894537590656\n",
      "Gradient Descent(22/49): loss=0.3612896791067946\n",
      "Gradient Descent(23/49): loss=0.360721278958272\n",
      "Gradient Descent(24/49): loss=0.3601906044351612\n",
      "Gradient Descent(25/49): loss=0.35969480799961717\n",
      "Gradient Descent(26/49): loss=0.3592312978989985\n",
      "Gradient Descent(27/49): loss=0.3587977076583744\n",
      "Gradient Descent(28/49): loss=0.3583918706924062\n",
      "Gradient Descent(29/49): loss=0.3580117989101015\n",
      "Gradient Descent(30/49): loss=0.3576556644789907\n",
      "Gradient Descent(31/49): loss=0.3573217841235239\n",
      "Gradient Descent(32/49): loss=0.35700860548255264\n",
      "Gradient Descent(33/49): loss=0.3567146951602709\n",
      "Gradient Descent(34/49): loss=0.3564387281858633\n",
      "Gradient Descent(35/49): loss=0.356179478657501\n",
      "Gradient Descent(36/49): loss=0.35593581139190095\n",
      "Gradient Descent(37/49): loss=0.3557066744354267\n",
      "Gradient Descent(38/49): loss=0.35549109231947634\n",
      "Gradient Descent(39/49): loss=0.3552881599637333\n",
      "Gradient Descent(40/49): loss=0.35509703714720686\n",
      "Gradient Descent(41/49): loss=0.3549169434799634\n",
      "Gradient Descent(42/49): loss=0.3547471538188243\n",
      "Gradient Descent(43/49): loss=0.35458699407869104\n",
      "Gradient Descent(44/49): loss=0.3544358373979848\n",
      "Gradient Descent(45/49): loss=0.35429310062230296\n",
      "Gradient Descent(46/49): loss=0.3541582410750425\n",
      "Gradient Descent(47/49): loss=0.35403075358762504\n",
      "Gradient Descent(48/49): loss=0.3539101677652294\n",
      "Gradient Descent(49/49): loss=0.3537960454667059\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4222402432812371\n",
      "Gradient Descent(2/49): loss=0.399678318279725\n",
      "Gradient Descent(3/49): loss=0.39055579112612976\n",
      "Gradient Descent(4/49): loss=0.3854306541079313\n",
      "Gradient Descent(5/49): loss=0.38186655045006623\n",
      "Gradient Descent(6/49): loss=0.37908975013019675\n",
      "Gradient Descent(7/49): loss=0.37678807766630573\n",
      "Gradient Descent(8/49): loss=0.3748081936196483\n",
      "Gradient Descent(9/49): loss=0.37306414494658113\n",
      "Gradient Descent(10/49): loss=0.37150325756199165\n",
      "Gradient Descent(11/49): loss=0.37009096784560935\n",
      "Gradient Descent(12/49): loss=0.3688032454809254\n",
      "Gradient Descent(13/49): loss=0.3676225219674242\n",
      "Gradient Descent(14/49): loss=0.36653538692557336\n",
      "Gradient Descent(15/49): loss=0.3655312271604277\n",
      "Gradient Descent(16/49): loss=0.364601389936671\n",
      "Gradient Descent(17/49): loss=0.36373864861144084\n",
      "Gradient Descent(18/49): loss=0.3629368488644546\n",
      "Gradient Descent(19/49): loss=0.3621906665237525\n",
      "Gradient Descent(20/49): loss=0.36149543663266337\n",
      "Gradient Descent(21/49): loss=0.36084702940853064\n",
      "Gradient Descent(22/49): loss=0.3602417579465146\n",
      "Gradient Descent(23/49): loss=0.3596763079695637\n",
      "Gradient Descent(24/49): loss=0.35914768324501617\n",
      "Gradient Descent(25/49): loss=0.35865316236798805\n",
      "Gradient Descent(26/49): loss=0.3581902639492936\n",
      "Gradient Descent(27/49): loss=0.3577567181268516\n",
      "Gradient Descent(28/49): loss=0.3573504429127813\n",
      "Gradient Descent(29/49): loss=0.35696952429553397\n",
      "Gradient Descent(30/49): loss=0.35661219930065136\n",
      "Gradient Descent(31/49): loss=0.3562768414152105\n",
      "Gradient Descent(32/49): loss=0.35596194792577984\n",
      "Gradient Descent(33/49): loss=0.35566612882503035\n",
      "Gradient Descent(34/49): loss=0.35538809701965557\n",
      "Gradient Descent(35/49): loss=0.35512665962991147\n",
      "Gradient Descent(36/49): loss=0.3548807102144368\n",
      "Gradient Descent(37/49): loss=0.3546492217869189\n",
      "Gradient Descent(38/49): loss=0.3544312405164079\n",
      "Gradient Descent(39/49): loss=0.35422588002261923\n",
      "Gradient Descent(40/49): loss=0.35403231619283215\n",
      "Gradient Descent(41/49): loss=0.3538497824590485\n",
      "Gradient Descent(42/49): loss=0.3536775654836716\n",
      "Gradient Descent(43/49): loss=0.35351500120968166\n",
      "Gradient Descent(44/49): loss=0.3533614712375505\n",
      "Gradient Descent(45/49): loss=0.35321639949625666\n",
      "Gradient Descent(46/49): loss=0.353079249179999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=0.35294951992572976\n",
      "Gradient Descent(48/49): loss=0.3528267452095842\n",
      "Gradient Descent(49/49): loss=0.35271048994278864\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42454763575292603\n",
      "Gradient Descent(2/49): loss=0.4023766698639512\n",
      "Gradient Descent(3/49): loss=0.3933120490384255\n",
      "Gradient Descent(4/49): loss=0.3881931118213151\n",
      "Gradient Descent(5/49): loss=0.38463154963859536\n",
      "Gradient Descent(6/49): loss=0.38186014146973585\n",
      "Gradient Descent(7/49): loss=0.3795665224876613\n",
      "Gradient Descent(8/49): loss=0.3775964463466099\n",
      "Gradient Descent(9/49): loss=0.3758632157059184\n",
      "Gradient Descent(10/49): loss=0.37431362706800686\n",
      "Gradient Descent(11/49): loss=0.37291275066750784\n",
      "Gradient Descent(12/49): loss=0.3716363052353494\n",
      "Gradient Descent(13/49): loss=0.3704665516589403\n",
      "Gradient Descent(14/49): loss=0.36938996495204335\n",
      "Gradient Descent(15/49): loss=0.3683958564841815\n",
      "Gradient Descent(16/49): loss=0.3674755256804972\n",
      "Gradient Descent(17/49): loss=0.3666217177208253\n",
      "Gradient Descent(18/49): loss=0.3658282643201238\n",
      "Gradient Descent(19/49): loss=0.3650898377761692\n",
      "Gradient Descent(20/49): loss=0.36440177737329577\n",
      "Gradient Descent(21/49): loss=0.363759963419592\n",
      "Gradient Descent(22/49): loss=0.36316072352547407\n",
      "Gradient Descent(23/49): loss=0.3626007612655436\n",
      "Gradient Descent(24/49): loss=0.36207710074178245\n",
      "Gradient Descent(25/49): loss=0.3615870426824471\n",
      "Gradient Descent(26/49): loss=0.36112812907185293\n",
      "Gradient Descent(27/49): loss=0.3606981142019814\n",
      "Gradient Descent(28/49): loss=0.3602949406390926\n",
      "Gradient Descent(29/49): loss=0.35991671901122024\n",
      "Gradient Descent(30/49): loss=0.3595617108100807\n",
      "Gradient Descent(31/49): loss=0.35922831360454743\n",
      "Gradient Descent(32/49): loss=0.35891504820898085\n",
      "Gradient Descent(33/49): loss=0.358620547455969\n",
      "Gradient Descent(34/49): loss=0.35834354630123183\n",
      "Gradient Descent(35/49): loss=0.35808287304665387\n",
      "Gradient Descent(36/49): loss=0.3578374415112182\n",
      "Gradient Descent(37/49): loss=0.3576062440129436\n",
      "Gradient Descent(38/49): loss=0.3573883450505456\n",
      "Gradient Descent(39/49): loss=0.3571828755934315\n",
      "Gradient Descent(40/49): loss=0.3569890279042439\n",
      "Gradient Descent(41/49): loss=0.3568060508305241\n",
      "Gradient Descent(42/49): loss=0.3566332455119411\n",
      "Gradient Descent(43/49): loss=0.3564699614575104\n",
      "Gradient Descent(44/49): loss=0.3563155929537097\n",
      "Gradient Descent(45/49): loss=0.3561695757697319\n",
      "Gradient Descent(46/49): loss=0.3560313841305292\n",
      "Gradient Descent(47/49): loss=0.35590052793197796\n",
      "Gradient Descent(48/49): loss=0.3557765501755988\n",
      "Gradient Descent(49/49): loss=0.35565902460288146\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4210520533632338\n",
      "Gradient Descent(2/49): loss=0.3999026277982095\n",
      "Gradient Descent(3/49): loss=0.3914377057909754\n",
      "Gradient Descent(4/49): loss=0.3865570993960414\n",
      "Gradient Descent(5/49): loss=0.3830812080860004\n",
      "Gradient Descent(6/49): loss=0.3803319179395465\n",
      "Gradient Descent(7/49): loss=0.3780322039701409\n",
      "Gradient Descent(8/49): loss=0.3760437912966375\n",
      "Gradient Descent(9/49): loss=0.3742881646815691\n",
      "Gradient Descent(10/49): loss=0.3727165164000194\n",
      "Gradient Descent(11/49): loss=0.3712962286083162\n",
      "Gradient Descent(12/49): loss=0.3700041333706008\n",
      "Gradient Descent(13/49): loss=0.3688229140835841\n",
      "Gradient Descent(14/49): loss=0.36773907657168325\n",
      "Gradient Descent(15/49): loss=0.3667417476199552\n",
      "Gradient Descent(16/49): loss=0.3658219300693628\n",
      "Gradient Descent(17/49): loss=0.36497202086586694\n",
      "Gradient Descent(18/49): loss=0.36418548688750424\n",
      "Gradient Descent(19/49): loss=0.3634566391598581\n",
      "Gradient Descent(20/49): loss=0.3627804706355081\n",
      "Gradient Descent(21/49): loss=0.3621525363697206\n",
      "Gradient Descent(22/49): loss=0.36156886278954825\n",
      "Gradient Descent(23/49): loss=0.3610258774402093\n",
      "Gradient Descent(24/49): loss=0.36052035347637834\n",
      "Gradient Descent(25/49): loss=0.36004936499309675\n",
      "Gradient Descent(26/49): loss=0.3596102504792481\n",
      "Gradient Descent(27/49): loss=0.3592005824673089\n",
      "Gradient Descent(28/49): loss=0.3588181419900862\n",
      "Gradient Descent(29/49): loss=0.35846089682638177\n",
      "Gradient Descent(30/49): loss=0.3581269827782647\n",
      "Gradient Descent(31/49): loss=0.3578146874084526\n",
      "Gradient Descent(32/49): loss=0.35752243580049664\n",
      "Gradient Descent(33/49): loss=0.35724877800262306\n",
      "Gradient Descent(34/49): loss=0.3569923778887579\n",
      "Gradient Descent(35/49): loss=0.35675200322469547\n",
      "Gradient Descent(36/49): loss=0.35652651676863234\n",
      "Gradient Descent(37/49): loss=0.3563148682669077\n",
      "Gradient Descent(38/49): loss=0.3561160872303024\n",
      "Gradient Descent(39/49): loss=0.3559292763954573\n",
      "Gradient Descent(40/49): loss=0.35575360579118404\n",
      "Gradient Descent(41/49): loss=0.3555883073416269\n",
      "Gradient Descent(42/49): loss=0.3554326699480761\n",
      "Gradient Descent(43/49): loss=0.35528603499928396\n",
      "Gradient Descent(44/49): loss=0.35514779226676135\n",
      "Gradient Descent(45/49): loss=0.35501737614704854\n",
      "Gradient Descent(46/49): loss=0.354894262217581\n",
      "Gradient Descent(47/49): loss=0.35477796407668216\n",
      "Gradient Descent(48/49): loss=0.35466803044154693\n",
      "Gradient Descent(49/49): loss=0.3545640424809382\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42074341412136773\n",
      "Gradient Descent(2/49): loss=0.3995483609699585\n",
      "Gradient Descent(3/49): loss=0.39097981229993106\n",
      "Gradient Descent(4/49): loss=0.3859924091800902\n",
      "Gradient Descent(5/49): loss=0.38242965903839415\n",
      "Gradient Descent(6/49): loss=0.3796139730971334\n",
      "Gradient Descent(7/49): loss=0.37726346842579095\n",
      "Gradient Descent(8/49): loss=0.37523526822655967\n",
      "Gradient Descent(9/49): loss=0.373447537638682\n",
      "Gradient Descent(10/49): loss=0.3718492545197304\n",
      "Gradient Descent(11/49): loss=0.37040634881037415\n",
      "Gradient Descent(12/49): loss=0.36909469057262917\n",
      "Gradient Descent(13/49): loss=0.3678963108007257\n",
      "Gradient Descent(14/49): loss=0.3667972596059222\n",
      "Gradient Descent(15/49): loss=0.36578633537467\n",
      "Gradient Descent(16/49): loss=0.36485429705879713\n",
      "Gradient Descent(17/49): loss=0.3639933553727978\n",
      "Gradient Descent(18/49): loss=0.36319683137558517\n",
      "Gradient Descent(19/49): loss=0.3624589193111904\n",
      "Gradient Descent(20/49): loss=0.3617745166815551\n",
      "Gradient Descent(21/49): loss=0.3611390990686997\n",
      "Gradient Descent(22/49): loss=0.36054862560215406\n",
      "Gradient Descent(23/49): loss=0.35999946595474663\n",
      "Gradient Descent(24/49): loss=0.35948834281261977\n",
      "Gradient Descent(25/49): loss=0.3590122857015856\n",
      "Gradient Descent(26/49): loss=0.3585685933085547\n",
      "Gradient Descent(27/49): loss=0.35815480227159363\n",
      "Gradient Descent(28/49): loss=0.3577686609783484\n",
      "Gradient Descent(29/49): loss=0.35740810730361977\n",
      "Gradient Descent(30/49): loss=0.35707124949140095\n",
      "Gradient Descent(31/49): loss=0.3567563495822746\n",
      "Gradient Descent(32/49): loss=0.35646180892829477\n",
      "Gradient Descent(33/49): loss=0.35618615544078314\n",
      "Gradient Descent(34/49): loss=0.35592803229293074\n",
      "Gradient Descent(35/49): loss=0.35568618785636297\n",
      "Gradient Descent(36/49): loss=0.35545946669420275\n",
      "Gradient Descent(37/49): loss=0.3552468014663908\n",
      "Gradient Descent(38/49): loss=0.35504720562874187\n",
      "Gradient Descent(39/49): loss=0.35485976682734754\n",
      "Gradient Descent(40/49): loss=0.3546836409058539\n",
      "Gradient Descent(41/49): loss=0.3545180464558604\n",
      "Gradient Descent(42/49): loss=0.35436225985094205\n",
      "Gradient Descent(43/49): loss=0.3542156107131596\n",
      "Gradient Descent(44/49): loss=0.35407747776778825\n",
      "Gradient Descent(45/49): loss=0.35394728504769557\n",
      "Gradient Descent(46/49): loss=0.3538244984135636\n",
      "Gradient Descent(47/49): loss=0.35370862236016837\n",
      "Gradient Descent(48/49): loss=0.35359919708234033\n",
      "Gradient Descent(49/49): loss=0.35349579577714885\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4192454012806545\n",
      "Gradient Descent(2/49): loss=0.3978309817855033\n",
      "Gradient Descent(3/49): loss=0.389306432096713\n",
      "Gradient Descent(4/49): loss=0.3844077839517561\n",
      "Gradient Descent(5/49): loss=0.3809322145151318\n",
      "Gradient Descent(6/49): loss=0.3781931454662045\n",
      "Gradient Descent(7/49): loss=0.3759080211963753\n",
      "Gradient Descent(8/49): loss=0.37393510927879653\n",
      "Gradient Descent(9/49): loss=0.3721939887291846\n",
      "Gradient Descent(10/49): loss=0.3706349424163856\n",
      "Gradient Descent(11/49): loss=0.36922502013427383\n",
      "Gradient Descent(12/49): loss=0.36794103241986265\n",
      "Gradient Descent(13/49): loss=0.3667657970250852\n",
      "Gradient Descent(14/49): loss=0.3656860241049453\n",
      "Gradient Descent(15/49): loss=0.36469106865715495\n",
      "Gradient Descent(16/49): loss=0.3637721616375003\n",
      "Gradient Descent(17/49): loss=0.36292191607408547\n",
      "Gradient Descent(18/49): loss=0.36213399745227043\n",
      "Gradient Descent(19/49): loss=0.36140289596144753\n",
      "Gradient Descent(20/49): loss=0.3607237641433691\n",
      "Gradient Descent(21/49): loss=0.3600922978853054\n",
      "Gradient Descent(22/49): loss=0.35950464696887247\n",
      "Gradient Descent(23/49): loss=0.3589573462904699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=0.3584472618728681\n",
      "Gradient Descent(25/49): loss=0.35797154768138123\n",
      "Gradient Descent(26/49): loss=0.3575276104842043\n",
      "Gradient Descent(27/49): loss=0.35711308080914533\n",
      "Gradient Descent(28/49): loss=0.3567257885987876\n",
      "Gradient Descent(29/49): loss=0.3563637425448577\n",
      "Gradient Descent(30/49): loss=0.3560251123477138\n",
      "Gradient Descent(31/49): loss=0.3557082133351466\n",
      "Gradient Descent(32/49): loss=0.35541149301017605\n",
      "Gradient Descent(33/49): loss=0.3551335191962324\n",
      "Gradient Descent(34/49): loss=0.3548729695208655\n",
      "Gradient Descent(35/49): loss=0.3546286220333691\n",
      "Gradient Descent(36/49): loss=0.35439934679258894\n",
      "Gradient Descent(37/49): loss=0.3541840982923488\n",
      "Gradient Descent(38/49): loss=0.3539819086159249\n",
      "Gradient Descent(39/49): loss=0.35379188122969113\n",
      "Gradient Descent(40/49): loss=0.3536131853407631\n",
      "Gradient Descent(41/49): loss=0.35344505075515636\n",
      "Gradient Descent(42/49): loss=0.3532867631823734\n",
      "Gradient Descent(43/49): loss=0.35313765993994367\n",
      "Gradient Descent(44/49): loss=0.35299712601769084\n",
      "Gradient Descent(45/49): loss=0.3528645904666573\n",
      "Gradient Descent(46/49): loss=0.3527395230819218\n",
      "Gradient Descent(47/49): loss=0.35262143135217067\n",
      "Gradient Descent(48/49): loss=0.3525098576519534\n",
      "Gradient Descent(49/49): loss=0.35240437665518254\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42162839954054515\n",
      "Gradient Descent(2/49): loss=0.40055241858505425\n",
      "Gradient Descent(3/49): loss=0.3920667305899628\n",
      "Gradient Descent(4/49): loss=0.3871706958375832\n",
      "Gradient Descent(5/49): loss=0.38369821398878273\n",
      "Gradient Descent(6/49): loss=0.38096588636048345\n",
      "Gradient Descent(7/49): loss=0.37869017245947034\n",
      "Gradient Descent(8/49): loss=0.3767282627204869\n",
      "Gradient Descent(9/49): loss=0.3749989631000052\n",
      "Gradient Descent(10/49): loss=0.3734520320245512\n",
      "Gradient Descent(11/49): loss=0.372054167563163\n",
      "Gradient Descent(12/49): loss=0.3707819461888676\n",
      "Gradient Descent(13/49): loss=0.369618031639183\n",
      "Gradient Descent(14/49): loss=0.36854903464938676\n",
      "Gradient Descent(15/49): loss=0.36756424828904943\n",
      "Gradient Descent(16/49): loss=0.3666548676472488\n",
      "Gradient Descent(17/49): loss=0.36581348830192006\n",
      "Gradient Descent(18/49): loss=0.3650337715427428\n",
      "Gradient Descent(19/49): loss=0.3643102130658238\n",
      "Gradient Descent(20/49): loss=0.36363797811058396\n",
      "Gradient Descent(21/49): loss=0.3630127806188761\n",
      "Gradient Descent(22/49): loss=0.3624307923989773\n",
      "Gradient Descent(23/49): loss=0.3618885732682723\n",
      "Gradient Descent(24/49): loss=0.36138301620566204\n",
      "Gradient Descent(25/49): loss=0.36091130347140443\n",
      "Gradient Descent(26/49): loss=0.3604708708980031\n",
      "Gradient Descent(27/49): loss=0.36005937838018526\n",
      "Gradient Descent(28/49): loss=0.3596746851488101\n",
      "Gradient Descent(29/49): loss=0.3593148287964822\n",
      "Gradient Descent(30/49): loss=0.3589780072903438\n",
      "Gradient Descent(31/49): loss=0.35866256339746866\n",
      "Gradient Descent(32/49): loss=0.35836697108493015\n",
      "Gradient Descent(33/49): loss=0.35808982355621977\n",
      "Gradient Descent(34/49): loss=0.3578298226591953\n",
      "Gradient Descent(35/49): loss=0.3575857694556477\n",
      "Gradient Descent(36/49): loss=0.3573565557840703\n",
      "Gradient Descent(37/49): loss=0.357141156678943\n",
      "Gradient Descent(38/49): loss=0.3569386235343712\n",
      "Gradient Descent(39/49): loss=0.35674807791909385\n",
      "Gradient Descent(40/49): loss=0.3565687059650199\n",
      "Gradient Descent(41/49): loss=0.35639975326354634\n",
      "Gradient Descent(42/49): loss=0.35624052021365626\n",
      "Gradient Descent(43/49): loss=0.3560903577737291\n",
      "Gradient Descent(44/49): loss=0.355948663575512\n",
      "Gradient Descent(45/49): loss=0.3558148783641032\n",
      "Gradient Descent(46/49): loss=0.3556884827323033\n",
      "Gradient Descent(47/49): loss=0.3555689941214956\n",
      "Gradient Descent(48/49): loss=0.3554559640644312\n",
      "Gradient Descent(49/49): loss=0.35534897564805057\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41828497096101763\n",
      "Gradient Descent(2/49): loss=0.39828583915509735\n",
      "Gradient Descent(3/49): loss=0.3903307633131857\n",
      "Gradient Descent(4/49): loss=0.38562463909492\n",
      "Gradient Descent(5/49): loss=0.382212248524529\n",
      "Gradient Descent(6/49): loss=0.37948788850996584\n",
      "Gradient Descent(7/49): loss=0.3771978273836269\n",
      "Gradient Descent(8/49): loss=0.37521297282069893\n",
      "Gradient Descent(9/49): loss=0.3734591628066482\n",
      "Gradient Descent(10/49): loss=0.3718897915708001\n",
      "Gradient Descent(11/49): loss=0.37047331676951767\n",
      "Gradient Descent(12/49): loss=0.369187025557389\n",
      "Gradient Descent(13/49): loss=0.36801371488518125\n",
      "Gradient Descent(14/49): loss=0.36693982022151417\n",
      "Gradient Descent(15/49): loss=0.3659543032767771\n",
      "Gradient Descent(16/49): loss=0.3650479575085604\n",
      "Gradient Descent(17/49): loss=0.3642129546732036\n",
      "Gradient Descent(18/49): loss=0.36344253674496174\n",
      "Gradient Descent(19/49): loss=0.36273079911832634\n",
      "Gradient Descent(20/49): loss=0.36207253323791955\n",
      "Gradient Descent(21/49): loss=0.3614631091695207\n",
      "Gradient Descent(22/49): loss=0.360898385781502\n",
      "Gradient Descent(23/49): loss=0.36037464049815054\n",
      "Gradient Descent(24/49): loss=0.3598885132462705\n",
      "Gradient Descent(25/49): loss=0.3594369609128868\n",
      "Gradient Descent(26/49): loss=0.35901721974117334\n",
      "Gradient Descent(27/49): loss=0.35862677383305025\n",
      "Gradient Descent(28/49): loss=0.3582633284318197\n",
      "Gradient Descent(29/49): loss=0.3579247870080237\n",
      "Gradient Descent(30/49): loss=0.3576092314178508\n",
      "Gradient Descent(31/49): loss=0.3573149045791296\n",
      "Gradient Descent(32/49): loss=0.3570401952371338\n",
      "Gradient Descent(33/49): loss=0.3567836244856971\n",
      "Gradient Descent(34/49): loss=0.3565438337784622\n",
      "Gradient Descent(35/49): loss=0.3563195742172483\n",
      "Gradient Descent(36/49): loss=0.3561096969442871\n",
      "Gradient Descent(37/49): loss=0.3559131444957545\n",
      "Gradient Descent(38/49): loss=0.35572894299798313\n",
      "Gradient Descent(39/49): loss=0.3555561951066796\n",
      "Gradient Descent(40/49): loss=0.35539407360459746\n",
      "Gradient Descent(41/49): loss=0.3552418155853522\n",
      "Gradient Descent(42/49): loss=0.35509871716104346\n",
      "Gradient Descent(43/49): loss=0.354964128639591\n",
      "Gradient Descent(44/49): loss=0.35483745012453893\n",
      "Gradient Descent(45/49): loss=0.3547181274958443\n",
      "Gradient Descent(46/49): loss=0.35460564873503514\n",
      "Gradient Descent(47/49): loss=0.35449954056228006\n",
      "Gradient Descent(48/49): loss=0.3543993653564832\n",
      "Gradient Descent(49/49): loss=0.35430471833260313\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41797251578214184\n",
      "Gradient Descent(2/49): loss=0.39792400580082393\n",
      "Gradient Descent(3/49): loss=0.389854150135956\n",
      "Gradient Descent(4/49): loss=0.3850374154116571\n",
      "Gradient Descent(5/49): loss=0.3815388885241679\n",
      "Gradient Descent(6/49): loss=0.3787502342318228\n",
      "Gradient Descent(7/49): loss=0.37641149120522693\n",
      "Gradient Descent(8/49): loss=0.37438857888558363\n",
      "Gradient Descent(9/49): loss=0.37260399955473006\n",
      "Gradient Descent(10/49): loss=0.37100901758919663\n",
      "Gradient Descent(11/49): loss=0.3695707290585412\n",
      "Gradient Descent(12/49): loss=0.3682655306421956\n",
      "Gradient Descent(13/49): loss=0.3670756176717375\n",
      "Gradient Descent(14/49): loss=0.36598700443706894\n",
      "Gradient Descent(15/49): loss=0.3649883473877424\n",
      "Gradient Descent(16/49): loss=0.3640702115692227\n",
      "Gradient Descent(17/49): loss=0.3632245928805039\n",
      "Gradient Descent(18/49): loss=0.3624445944162184\n",
      "Gradient Descent(19/49): loss=0.3617241993676612\n",
      "Gradient Descent(20/49): loss=0.3610581066434319\n",
      "Gradient Descent(21/49): loss=0.36044160855186547\n",
      "Gradient Descent(22/49): loss=0.3598704975019754\n",
      "Gradient Descent(23/49): loss=0.3593409932367798\n",
      "Gradient Descent(24/49): loss=0.3588496849301495\n",
      "Gradient Descent(25/49): loss=0.3583934842711845\n",
      "Gradient Descent(26/49): loss=0.35796958683047664\n",
      "Gradient Descent(27/49): loss=0.3575754397837855\n",
      "Gradient Descent(28/49): loss=0.35720871460033826\n",
      "Gradient Descent(29/49): loss=0.3568672836711712\n",
      "Gradient Descent(30/49): loss=0.35654920011198277\n",
      "Gradient Descent(31/49): loss=0.3562526801598893\n",
      "Gradient Descent(32/49): loss=0.35597608771730915\n",
      "Gradient Descent(33/49): loss=0.35571792069434394\n",
      "Gradient Descent(34/49): loss=0.35547679887392464\n",
      "Gradient Descent(35/49): loss=0.355251453078812\n",
      "Gradient Descent(36/49): loss=0.3550407154612805\n",
      "Gradient Descent(37/49): loss=0.35484351076847587\n",
      "Gradient Descent(38/49): loss=0.3546588484615049\n",
      "Gradient Descent(39/49): loss=0.35448581558608877\n",
      "Gradient Descent(40/49): loss=0.3543235703083691\n",
      "Gradient Descent(41/49): loss=0.354171336042161\n",
      "Gradient Descent(42/49): loss=0.3540283961042877\n",
      "Gradient Descent(43/49): loss=0.3538940888431314\n",
      "Gradient Descent(44/49): loss=0.3537678031925895\n",
      "Gradient Descent(45/49): loss=0.3536489746095326\n",
      "Gradient Descent(46/49): loss=0.3535370813578411\n",
      "Gradient Descent(47/49): loss=0.3534316411063362\n",
      "Gradient Descent(48/49): loss=0.3533322078115476\n",
      "Gradient Descent(49/49): loss=0.35323836885939786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4164260327618294\n",
      "Gradient Descent(2/49): loss=0.3961985257864751\n",
      "Gradient Descent(3/49): loss=0.38819313110729375\n",
      "Gradient Descent(4/49): loss=0.3834731614472273\n",
      "Gradient Descent(5/49): loss=0.380064717656221\n",
      "Gradient Descent(6/49): loss=0.3773533956511891\n",
      "Gradient Descent(7/49): loss=0.37507967330070924\n",
      "Gradient Descent(8/49): loss=0.37311109903386913\n",
      "Gradient Descent(9/49): loss=0.37137183634709436\n",
      "Gradient Descent(10/49): loss=0.3698145966603987\n",
      "Gradient Descent(11/49): loss=0.36840768460180584\n",
      "Gradient Descent(12/49): loss=0.3671284964658836\n",
      "Gradient Descent(13/49): loss=0.36596005580067414\n",
      "Gradient Descent(14/49): loss=0.364889066983411\n",
      "Gradient Descent(15/49): loss=0.36390476518331627\n",
      "Gradient Descent(16/49): loss=0.362998203695612\n",
      "Gradient Descent(17/49): loss=0.36216179246173297\n",
      "Gradient Descent(18/49): loss=0.3613889871669273\n",
      "Gradient Descent(19/49): loss=0.36067407226931203\n",
      "Gradient Descent(20/49): loss=0.3600120047685669\n",
      "Gradient Descent(21/49): loss=0.3593982985246742\n",
      "Gradient Descent(22/49): loss=0.3588289364225874\n",
      "Gradient Descent(23/49): loss=0.35830030214522135\n",
      "Gradient Descent(24/49): loss=0.35780912607103893\n",
      "Gradient Descent(25/49): loss=0.35735244156065576\n",
      "Gradient Descent(26/49): loss=0.35692754903525503\n",
      "Gradient Descent(27/49): loss=0.3565319860074467\n",
      "Gradient Descent(28/49): loss=0.356163501739564\n",
      "Gradient Descent(29/49): loss=0.3558200355594772\n",
      "Gradient Descent(30/49): loss=0.3554996981129358\n",
      "Gradient Descent(31/49): loss=0.35520075500844994\n",
      "Gradient Descent(32/49): loss=0.35492161243826265\n",
      "Gradient Descent(33/49): loss=0.3546608044520475\n",
      "Gradient Descent(34/49): loss=0.3544169816287623\n",
      "Gradient Descent(35/49): loss=0.3541889009435447\n",
      "Gradient Descent(36/49): loss=0.35397541666551485\n",
      "Gradient Descent(37/49): loss=0.3537754721522038\n",
      "Gradient Descent(38/49): loss=0.3535880924294918\n",
      "Gradient Descent(39/49): loss=0.35341237746410603\n",
      "Gradient Descent(40/49): loss=0.35324749605014844\n",
      "Gradient Descent(41/49): loss=0.35309268024269147\n",
      "Gradient Descent(42/49): loss=0.3529472202808691\n",
      "Gradient Descent(43/49): loss=0.3528104599505798\n",
      "Gradient Descent(44/49): loss=0.3526817923432868\n",
      "Gradient Descent(45/49): loss=0.3525606559727204\n",
      "Gradient Descent(46/49): loss=0.35244653121577235\n",
      "Gradient Descent(47/49): loss=0.35233893704768404\n",
      "Gradient Descent(48/49): loss=0.35223742804489194\n",
      "Gradient Descent(49/49): loss=0.3521415916317155\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41887795645650794\n",
      "Gradient Descent(2/49): loss=0.3989371634601039\n",
      "Gradient Descent(3/49): loss=0.390955470705077\n",
      "Gradient Descent(4/49): loss=0.386236279795409\n",
      "Gradient Descent(5/49): loss=0.38283197146025016\n",
      "Gradient Descent(6/49): loss=0.38012881920441377\n",
      "Gradient Descent(7/49): loss=0.37786582249614353\n",
      "Gradient Descent(8/49): loss=0.37590936697739985\n",
      "Gradient Descent(9/49): loss=0.3741828374914697\n",
      "Gradient Descent(10/49): loss=0.37263843433330907\n",
      "Gradient Descent(11/49): loss=0.3712441290512672\n",
      "Gradient Descent(12/49): loss=0.3699771030860352\n",
      "Gradient Descent(13/49): loss=0.3688202437300476\n",
      "Gradient Descent(14/49): loss=0.367760171644842\n",
      "Gradient Descent(15/49): loss=0.3667860738313535\n",
      "Gradient Descent(16/49): loss=0.36588897990654057\n",
      "Gradient Descent(17/49): loss=0.36506129336789833\n",
      "Gradient Descent(18/49): loss=0.36429647583336966\n",
      "Gradient Descent(19/49): loss=0.3635888267173789\n",
      "Gradient Descent(20/49): loss=0.3629333245973574\n",
      "Gradient Descent(21/49): loss=0.3623255097440879\n",
      "Gradient Descent(22/49): loss=0.3617613949085685\n",
      "Gradient Descent(23/49): loss=0.36123739600524635\n",
      "Gradient Descent(24/49): loss=0.3607502771329104\n",
      "Gradient Descent(25/49): loss=0.3602971061503017\n",
      "Gradient Descent(26/49): loss=0.35987521817770335\n",
      "Gradient Descent(27/49): loss=0.3594821851627002\n",
      "Gradient Descent(28/49): loss=0.35911579016792466\n",
      "Gradient Descent(29/49): loss=0.3587740053968777\n",
      "Gradient Descent(30/49): loss=0.3584549732249278\n",
      "Gradient Descent(31/49): loss=0.3581569896810936\n",
      "Gradient Descent(32/49): loss=0.3578784899549671\n",
      "Gradient Descent(33/49): loss=0.3576180355972832\n",
      "Gradient Descent(34/49): loss=0.3573743031523925\n",
      "Gradient Descent(35/49): loss=0.3571460740132586\n",
      "Gradient Descent(36/49): loss=0.356932225329402\n",
      "Gradient Descent(37/49): loss=0.35673172182884016\n",
      "Gradient Descent(38/49): loss=0.3565436084389309\n",
      "Gradient Descent(39/49): loss=0.35636700360982015\n",
      "Gradient Descent(40/49): loss=0.35620109325916866\n",
      "Gradient Descent(41/49): loss=0.3560451252688904\n",
      "Gradient Descent(42/49): loss=0.355898404474441\n",
      "Gradient Descent(43/49): loss=0.3557602880952516\n",
      "Gradient Descent(44/49): loss=0.3556301815615778\n",
      "Gradient Descent(45/49): loss=0.35550753469861346\n",
      "Gradient Descent(46/49): loss=0.35539183823342346\n",
      "Gradient Descent(47/49): loss=0.35528262059423904\n",
      "Gradient Descent(48/49): loss=0.3551794449750742\n",
      "Gradient Descent(49/49): loss=0.3550819066415595\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41568830062544976\n",
      "Gradient Descent(2/49): loss=0.3968539550441798\n",
      "Gradient Descent(3/49): loss=0.3893335185154942\n",
      "Gradient Descent(4/49): loss=0.38476339255725023\n",
      "Gradient Descent(5/49): loss=0.38139867620104817\n",
      "Gradient Descent(6/49): loss=0.3786921640960878\n",
      "Gradient Descent(7/49): loss=0.3764083652938865\n",
      "Gradient Descent(8/49): loss=0.37442566323949666\n",
      "Gradient Descent(9/49): loss=0.3726734862330674\n",
      "Gradient Descent(10/49): loss=0.3711069714277524\n",
      "Gradient Descent(11/49): loss=0.36969534385140357\n",
      "Gradient Descent(12/49): loss=0.36841613676034385\n",
      "Gradient Descent(13/49): loss=0.36725211999482654\n",
      "Gradient Descent(14/49): loss=0.36618956429497057\n",
      "Gradient Descent(15/49): loss=0.36521720291897664\n",
      "Gradient Descent(16/49): loss=0.3643255774403595\n",
      "Gradient Descent(17/49): loss=0.3635066062807567\n",
      "Gradient Descent(18/49): loss=0.3627532885205113\n",
      "Gradient Descent(19/49): loss=0.3620594933235029\n",
      "Gradient Descent(20/49): loss=0.36141980552984077\n",
      "Gradient Descent(21/49): loss=0.3608294092752184\n",
      "Gradient Descent(22/49): loss=0.3602839980820007\n",
      "Gradient Descent(23/49): loss=0.35977970384711944\n",
      "Gradient Descent(24/49): loss=0.35931303963466105\n",
      "Gradient Descent(25/49): loss=0.3588808527727725\n",
      "Gradient Descent(26/49): loss=0.35848028579929686\n",
      "Gradient Descent(27/49): loss=0.35810874350053273\n",
      "Gradient Descent(28/49): loss=0.3577638647651629\n",
      "Gradient Descent(29/49): loss=0.3574434983068322\n",
      "Gradient Descent(30/49): loss=0.3571456815424819\n",
      "Gradient Descent(31/49): loss=0.356868622080721\n",
      "Gradient Descent(32/49): loss=0.35661068139590557\n",
      "Gradient Descent(33/49): loss=0.3563703603530139\n",
      "Gradient Descent(34/49): loss=0.3561462863151996\n",
      "Gradient Descent(35/49): loss=0.355937201616507\n",
      "Gradient Descent(36/49): loss=0.3557419532210806\n",
      "Gradient Descent(37/49): loss=0.3555594834204288\n",
      "Gradient Descent(38/49): loss=0.35538882144411904\n",
      "Gradient Descent(39/49): loss=0.3552290758782838\n",
      "Gradient Descent(40/49): loss=0.3550794278016522\n",
      "Gradient Descent(41/49): loss=0.3549391245613378\n",
      "Gradient Descent(42/49): loss=0.3548074741209277\n",
      "Gradient Descent(43/49): loss=0.3546838399220152\n",
      "Gradient Descent(44/49): loss=0.354567636207528\n",
      "Gradient Descent(45/49): loss=0.35445832376132497\n",
      "Gradient Descent(46/49): loss=0.3543554060237403\n",
      "Gradient Descent(47/49): loss=0.3542584255472498\n",
      "Gradient Descent(48/49): loss=0.3541669607603015\n",
      "Gradient Descent(49/49): loss=0.35408062301073145\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4153734327398317\n",
      "Gradient Descent(2/49): loss=0.39648368006542173\n",
      "Gradient Descent(3/49): loss=0.3888377364840925\n",
      "Gradient Descent(4/49): loss=0.3841544747839131\n",
      "Gradient Descent(5/49): loss=0.38070504634879243\n",
      "Gradient Descent(6/49): loss=0.37793650443041277\n",
      "Gradient Descent(7/49): loss=0.3756060409185776\n",
      "Gradient Descent(8/49): loss=0.37358684448265306\n",
      "Gradient Descent(9/49): loss=0.37180507102909033\n",
      "Gradient Descent(10/49): loss=0.37021383802804003\n",
      "Gradient Descent(11/49): loss=0.3687811021793823\n",
      "Gradient Descent(12/49): loss=0.36748357269671206\n",
      "Gradient Descent(13/49): loss=0.3663034621698275\n",
      "Gradient Descent(14/49): loss=0.3652266491484715\n",
      "Gradient Descent(15/49): loss=0.36424158052372296\n",
      "Gradient Descent(16/49): loss=0.3633385818216887\n",
      "Gradient Descent(17/49): loss=0.36250940370256207\n",
      "Gradient Descent(18/49): loss=0.3617469116019693\n",
      "Gradient Descent(19/49): loss=0.36104486574098826\n",
      "Gradient Descent(20/49): loss=0.36039776028511294\n",
      "Gradient Descent(21/49): loss=0.35980070246470247\n",
      "Gradient Descent(22/49): loss=0.35924931946149735\n",
      "Gradient Descent(23/49): loss=0.35873968507979404\n",
      "Gradient Descent(24/49): loss=0.35826826084361424\n",
      "Gradient Descent(25/49): loss=0.35783184783976646\n",
      "Gradient Descent(26/49): loss=0.35742754672722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=0.3570527240700909\n",
      "Gradient Descent(28/49): loss=0.3567049836542343\n",
      "Gradient Descent(29/49): loss=0.3563821417962413\n",
      "Gradient Descent(30/49): loss=0.3560822058995049\n",
      "Gradient Descent(31/49): loss=0.3558033556879383\n",
      "Gradient Descent(32/49): loss=0.3555439266756289\n",
      "Gradient Descent(33/49): loss=0.35530239552471965\n",
      "Gradient Descent(34/49): loss=0.3550773670139682\n",
      "Gradient Descent(35/49): loss=0.3548675623935042\n",
      "Gradient Descent(36/49): loss=0.3546718089419834\n",
      "Gradient Descent(37/49): loss=0.3544890305739115\n",
      "Gradient Descent(38/49): loss=0.3543182393697304\n",
      "Gradient Descent(39/49): loss=0.3541585279210031\n",
      "Gradient Descent(40/49): loss=0.3540090623989148\n",
      "Gradient Descent(41/49): loss=0.35386907626723\n",
      "Gradient Descent(42/49): loss=0.3537378645714591\n",
      "Gradient Descent(43/49): loss=0.35361477874480246\n",
      "Gradient Descent(44/49): loss=0.3534992218788086\n",
      "Gradient Descent(45/49): loss=0.35339064441291873\n",
      "Gradient Descent(46/49): loss=0.35328854020236244\n",
      "Gradient Descent(47/49): loss=0.35319244292841523\n",
      "Gradient Descent(48/49): loss=0.35310192281894465\n",
      "Gradient Descent(49/49): loss=0.35301658365057265\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4137821377247613\n",
      "Gradient Descent(2/49): loss=0.3947541678902204\n",
      "Gradient Descent(3/49): loss=0.38719071567795793\n",
      "Gradient Descent(4/49): loss=0.38261066004960614\n",
      "Gradient Descent(5/49): loss=0.37925340993705\n",
      "Gradient Descent(6/49): loss=0.3765624717579876\n",
      "Gradient Descent(7/49): loss=0.37429639726856706\n",
      "Gradient Descent(8/49): loss=0.3723304114423161\n",
      "Gradient Descent(9/49): loss=0.37059254419159526\n",
      "Gradient Descent(10/49): loss=0.36903748414752896\n",
      "Gradient Descent(11/49): loss=0.3676344750665765\n",
      "Gradient Descent(12/49): loss=0.3663612787132779\n",
      "Gradient Descent(13/49): loss=0.36520097324953876\n",
      "Gradient Descent(14/49): loss=0.36414015431489477\n",
      "Gradient Descent(15/49): loss=0.3631678671066956\n",
      "Gradient Descent(16/49): loss=0.3622749393875339\n",
      "Gradient Descent(17/49): loss=0.36145354548909386\n",
      "Gradient Descent(18/49): loss=0.36069690963157114\n",
      "Gradient Descent(19/49): loss=0.3599990967842026\n",
      "Gradient Descent(20/49): loss=0.3593548605539468\n",
      "Gradient Descent(21/49): loss=0.35875952941583034\n",
      "Gradient Descent(22/49): loss=0.35820891944959354\n",
      "Gradient Descent(23/49): loss=0.3576992658650062\n",
      "Gradient Descent(24/49): loss=0.3572271681544982\n",
      "Gradient Descent(25/49): loss=0.3567895453435756\n",
      "Gradient Descent(26/49): loss=0.3563835988764601\n",
      "Gradient Descent(27/49): loss=0.35600678138662933\n",
      "Gradient Descent(28/49): loss=0.35565677008615815\n",
      "Gradient Descent(29/49): loss=0.3553314438424335\n",
      "Gradient Descent(30/49): loss=0.3550288632456991\n",
      "Gradient Descent(31/49): loss=0.3547472531381364\n",
      "Gradient Descent(32/49): loss=0.35448498719596483\n",
      "Gradient Descent(33/49): loss=0.35424057424446614\n",
      "Gradient Descent(34/49): loss=0.3540126460514549\n",
      "Gradient Descent(35/49): loss=0.35379994639406454\n",
      "Gradient Descent(36/49): loss=0.3536013212313321\n",
      "Gradient Descent(37/49): loss=0.353415709844101\n",
      "Gradient Descent(38/49): loss=0.3532421368264765\n",
      "Gradient Descent(39/49): loss=0.3530797048310469\n",
      "Gradient Descent(40/49): loss=0.35292758798450286\n",
      "Gradient Descent(41/49): loss=0.35278502590196564\n",
      "Gradient Descent(42/49): loss=0.3526513182379195\n",
      "Gradient Descent(43/49): loss=0.35252581971957175\n",
      "Gradient Descent(44/49): loss=0.3524079356151014\n",
      "Gradient Descent(45/49): loss=0.35229711759485743\n",
      "Gradient Descent(46/49): loss=0.3521928599483288\n",
      "Gradient Descent(47/49): loss=0.3520946961237997\n",
      "Gradient Descent(48/49): loss=0.35200219556113094\n",
      "Gradient Descent(49/49): loss=0.35191496079117485\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41629630650081456\n",
      "Gradient Descent(2/49): loss=0.3975050729150738\n",
      "Gradient Descent(3/49): loss=0.3899537898226406\n",
      "Gradient Descent(4/49): loss=0.38537398442144355\n",
      "Gradient Descent(5/49): loss=0.3820222076273471\n",
      "Gradient Descent(6/49): loss=0.37934087716257003\n",
      "Gradient Descent(7/49): loss=0.3770867832428398\n",
      "Gradient Descent(8/49): loss=0.37513395133649596\n",
      "Gradient Descent(9/49): loss=0.37340964037189006\n",
      "Gradient Descent(10/49): loss=0.3718680470021093\n",
      "Gradient Descent(11/49): loss=0.3704781037714752\n",
      "Gradient Descent(12/49): loss=0.3692173784307521\n",
      "Gradient Descent(13/49): loss=0.36806883133357216\n",
      "Gradient Descent(14/49): loss=0.36701899038232816\n",
      "Gradient Descent(15/49): loss=0.36605686656019504\n",
      "Gradient Descent(16/49): loss=0.3651732764064552\n",
      "Gradient Descent(17/49): loss=0.3643603991848169\n",
      "Gradient Descent(18/49): loss=0.36361147561777446\n",
      "Gradient Descent(19/49): loss=0.3629205955396342\n",
      "Gradient Descent(20/49): loss=0.3622825434399977\n",
      "Gradient Descent(21/49): loss=0.36169268291073037\n",
      "Gradient Descent(22/49): loss=0.361146867985778\n",
      "Gradient Descent(23/49): loss=0.3606413735522675\n",
      "Gradient Descent(24/49): loss=0.3601728396074783\n",
      "Gradient Descent(25/49): loss=0.3597382257901444\n",
      "Gradient Descent(26/49): loss=0.359334773693662\n",
      "Gradient Descent(27/49): loss=0.3589599751878717\n",
      "Gradient Descent(28/49): loss=0.3586115454643784\n",
      "Gradient Descent(29/49): loss=0.3582873998577407\n",
      "Gradient Descent(30/49): loss=0.3579856337317502\n",
      "Gradient Descent(31/49): loss=0.3577045048889619\n",
      "Gradient Descent(32/49): loss=0.3574424180839268\n",
      "Gradient Descent(33/49): loss=0.35719791131040324\n",
      "Gradient Descent(34/49): loss=0.35696964359974676\n",
      "Gradient Descent(35/49): loss=0.3567563841182316\n",
      "Gradient Descent(36/49): loss=0.3565570023897648\n",
      "Gradient Descent(37/49): loss=0.35637045950047697\n",
      "Gradient Descent(38/49): loss=0.35619580016525504\n",
      "Gradient Descent(39/49): loss=0.3560321455550196\n",
      "Gradient Descent(40/49): loss=0.3558786867986235\n",
      "Gradient Descent(41/49): loss=0.35573467908548084\n",
      "Gradient Descent(42/49): loss=0.3555994363050888\n",
      "Gradient Descent(43/49): loss=0.35547232616792596\n",
      "Gradient Descent(44/49): loss=0.3553527657591725\n",
      "Gradient Descent(45/49): loss=0.35524021748255835\n",
      "Gradient Descent(46/49): loss=0.35513418535663316\n",
      "Gradient Descent(47/49): loss=0.35503421163000465\n",
      "Gradient Descent(48/49): loss=0.3549398736857682\n",
      "Gradient Descent(49/49): loss=0.35485078120852187\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41326204235653036\n",
      "Gradient Descent(2/49): loss=0.39558299054236506\n",
      "Gradient Descent(3/49): loss=0.38842592329105846\n",
      "Gradient Descent(4/49): loss=0.38396123768672913\n",
      "Gradient Descent(5/49): loss=0.38063245926082157\n",
      "Gradient Descent(6/49): loss=0.37793856994399033\n",
      "Gradient Descent(7/49): loss=0.3756586482946387\n",
      "Gradient Descent(8/49): loss=0.37367733014659354\n",
      "Gradient Descent(9/49): loss=0.3719270133945389\n",
      "Gradient Descent(10/49): loss=0.3703641783268452\n",
      "Gradient Descent(11/49): loss=0.36895855040801406\n",
      "Gradient Descent(12/49): loss=0.36768773289244544\n",
      "Gradient Descent(13/49): loss=0.3665343531923831\n",
      "Gradient Descent(14/49): loss=0.3654844417213282\n",
      "Gradient Descent(15/49): loss=0.36452645364369546\n",
      "Gradient Descent(16/49): loss=0.3636506463295293\n",
      "Gradient Descent(17/49): loss=0.3628486644876978\n",
      "Gradient Descent(18/49): loss=0.3621132524411565\n",
      "Gradient Descent(19/49): loss=0.36143804749864566\n",
      "Gradient Descent(20/49): loss=0.3608174269197113\n",
      "Gradient Descent(21/49): loss=0.3602463914152843\n",
      "Gradient Descent(22/49): loss=0.35972047425846665\n",
      "Gradient Descent(23/49): loss=0.35923566881083974\n",
      "Gradient Descent(24/49): loss=0.3587883696087336\n",
      "Gradient Descent(25/49): loss=0.3583753236587515\n",
      "Gradient Descent(26/49): loss=0.35799358958180405\n",
      "Gradient Descent(27/49): loss=0.35764050290920507\n",
      "Gradient Descent(28/49): loss=0.35731364628832973\n",
      "Gradient Descent(29/49): loss=0.35701082367090103\n",
      "Gradient Descent(30/49): loss=0.3567300377799933\n",
      "Gradient Descent(31/49): loss=0.3564694703120027\n",
      "Gradient Descent(32/49): loss=0.3562274644466908\n",
      "Gradient Descent(33/49): loss=0.3560025093250008\n",
      "Gradient Descent(34/49): loss=0.35579322621948806\n",
      "Gradient Descent(35/49): loss=0.35559835617195606\n",
      "Gradient Descent(36/49): loss=0.3554167489114216\n",
      "Gradient Descent(37/49): loss=0.3552473528957979\n",
      "Gradient Descent(38/49): loss=0.3550892063447628\n",
      "Gradient Descent(39/49): loss=0.35494142915068616\n",
      "Gradient Descent(40/49): loss=0.35480321557029393\n",
      "Gradient Descent(41/49): loss=0.35467382761278526\n",
      "Gradient Descent(42/49): loss=0.35455258905094844\n",
      "Gradient Descent(43/49): loss=0.35443887999093465\n",
      "Gradient Descent(44/49): loss=0.35433213194405105\n",
      "Gradient Descent(45/49): loss=0.35423182335051795\n",
      "Gradient Descent(46/49): loss=0.35413747551078634\n",
      "Gradient Descent(47/49): loss=0.3540486488848987\n",
      "Gradient Descent(48/49): loss=0.3539649397246279\n",
      "Gradient Descent(49/49): loss=0.35388597700684016\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.412946164994437\n",
      "Gradient Descent(2/49): loss=0.3952032606266154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3/49): loss=0.3879107210383426\n",
      "Gradient Descent(4/49): loss=0.3833316037730014\n",
      "Gradient Descent(5/49): loss=0.37992005759498537\n",
      "Gradient Descent(6/49): loss=0.3771664470669239\n",
      "Gradient Descent(7/49): loss=0.37484174169489276\n",
      "Gradient Descent(8/49): loss=0.3728253253143161\n",
      "Gradient Descent(9/49): loss=0.3710464409994984\n",
      "Gradient Descent(10/49): loss=0.3694596711216013\n",
      "Gradient Descent(11/49): loss=0.36803356247704305\n",
      "Gradient Descent(12/49): loss=0.36674495360531956\n",
      "Gradient Descent(13/49): loss=0.3655759525187159\n",
      "Gradient Descent(14/49): loss=0.3645122214457415\n",
      "Gradient Descent(15/49): loss=0.36354194449778376\n",
      "Gradient Descent(16/49): loss=0.36265517280952847\n",
      "Gradient Descent(17/49): loss=0.3618433895600421\n",
      "Gradient Descent(18/49): loss=0.3610992092475335\n",
      "Gradient Descent(19/49): loss=0.3604161623816724\n",
      "Gradient Descent(20/49): loss=0.35978853650065323\n",
      "Gradient Descent(21/49): loss=0.359211255510726\n",
      "Gradient Descent(22/49): loss=0.3586797858382068\n",
      "Gradient Descent(23/49): loss=0.3581900618238745\n",
      "Gradient Descent(24/49): loss=0.3577384252555936\n",
      "Gradient Descent(25/49): loss=0.3573215755197175\n",
      "Gradient Descent(26/49): loss=0.35693652789376873\n",
      "Gradient Descent(27/49): loss=0.35658057820197264\n",
      "Gradient Descent(28/49): loss=0.3562512725329453\n",
      "Gradient Descent(29/49): loss=0.35594638105095466\n",
      "Gradient Descent(30/49): loss=0.35566387516685133\n",
      "Gradient Descent(31/49): loss=0.3554019075032362\n",
      "Gradient Descent(32/49): loss=0.35515879421125723\n",
      "Gradient Descent(33/49): loss=0.3549329992873196\n",
      "Gradient Descent(34/49): loss=0.3547231206062696\n",
      "Gradient Descent(35/49): loss=0.35452787743963204\n",
      "Gradient Descent(36/49): loss=0.35434609926767036\n",
      "Gradient Descent(37/49): loss=0.3541767157255131\n",
      "Gradient Descent(38/49): loss=0.354018747548553\n",
      "Gradient Descent(39/49): loss=0.3538712984023638\n",
      "Gradient Descent(40/49): loss=0.35373354749865943\n",
      "Gradient Descent(41/49): loss=0.3536047429121819\n",
      "Gradient Descent(42/49): loss=0.3534841955244826\n",
      "Gradient Descent(43/49): loss=0.35337127352984\n",
      "Gradient Descent(44/49): loss=0.35326539744637986\n",
      "Gradient Descent(45/49): loss=0.3531660355821335\n",
      "Gradient Descent(46/49): loss=0.3530726999114691\n",
      "Gradient Descent(47/49): loss=0.35298494232226485\n",
      "Gradient Descent(48/49): loss=0.35290235119845725\n",
      "Gradient Descent(49/49): loss=0.3528245483063342\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4113137161694506\n",
      "Gradient Descent(2/49): loss=0.39347320787300893\n",
      "Gradient Descent(3/49): loss=0.3862788412795047\n",
      "Gradient Descent(4/49): loss=0.38180805585418226\n",
      "Gradient Descent(5/49): loss=0.37849014103613104\n",
      "Gradient Descent(6/49): loss=0.37581405162437187\n",
      "Gradient Descent(7/49): loss=0.37355288511800405\n",
      "Gradient Descent(8/49): loss=0.3715884107095235\n",
      "Gradient Descent(9/49): loss=0.36985193152790424\n",
      "Gradient Descent(10/49): loss=0.36829971099348185\n",
      "Gradient Descent(11/49): loss=0.366901653314455\n",
      "Gradient Descent(12/49): loss=0.365635697499301\n",
      "Gradient Descent(13/49): loss=0.36448485101161876\n",
      "Gradient Descent(14/49): loss=0.3634355174143151\n",
      "Gradient Descent(15/49): loss=0.36247649628773587\n",
      "Gradient Descent(16/49): loss=0.36159835274757834\n",
      "Gradient Descent(17/49): loss=0.3607930011702537\n",
      "Gradient Descent(18/49): loss=0.3600534190691613\n",
      "Gradient Descent(19/49): loss=0.35937344337267174\n",
      "Gradient Descent(20/49): loss=0.35874762076376665\n",
      "Gradient Descent(21/49): loss=0.3581710946084974\n",
      "Gradient Descent(22/49): loss=0.35763951734390076\n",
      "Gradient Descent(23/49): loss=0.3571489810354622\n",
      "Gradient Descent(24/49): loss=0.3566959612109198\n",
      "Gradient Descent(25/49): loss=0.3562772706130331\n",
      "Gradient Descent(26/49): loss=0.3558900205204799\n",
      "Gradient Descent(27/49): loss=0.3555315879588801\n",
      "Gradient Descent(28/49): loss=0.35519958758180153\n",
      "Gradient Descent(29/49): loss=0.3548918473183944\n",
      "Gradient Descent(30/49): loss=0.3546063871069788\n",
      "Gradient Descent(31/49): loss=0.3543414001928747\n",
      "Gradient Descent(32/49): loss=0.3540952365839564\n",
      "Gradient Descent(33/49): loss=0.3538663883421603\n",
      "Gradient Descent(34/49): loss=0.3536534764524373\n",
      "Gradient Descent(35/49): loss=0.3534552390585657\n",
      "Gradient Descent(36/49): loss=0.3532705208920662\n",
      "Gradient Descent(37/49): loss=0.3530982637491578\n",
      "Gradient Descent(38/49): loss=0.35293749789335166\n",
      "Gradient Descent(39/49): loss=0.3527873342794108\n",
      "Gradient Descent(40/49): loss=0.3526469575090906\n",
      "Gradient Descent(41/49): loss=0.35251561944110166\n",
      "Gradient Descent(42/49): loss=0.3523926333877007\n",
      "Gradient Descent(43/49): loss=0.35227736883865535\n",
      "Gradient Descent(44/49): loss=0.3521692466603612\n",
      "Gradient Descent(45/49): loss=0.35206773472388736\n",
      "Gradient Descent(46/49): loss=0.35197234392086735\n",
      "Gradient Descent(47/49): loss=0.3518826245305948\n",
      "Gradient Descent(48/49): loss=0.35179816290555044\n",
      "Gradient Descent(49/49): loss=0.3517185784459599\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41388344967346496\n",
      "Gradient Descent(2/49): loss=0.39623229759505213\n",
      "Gradient Descent(3/49): loss=0.38904185767369864\n",
      "Gradient Descent(4/49): loss=0.384571727207843\n",
      "Gradient Descent(5/49): loss=0.38126077086045024\n",
      "Gradient Descent(6/49): loss=0.37859569863899367\n",
      "Gradient Descent(7/49): loss=0.37634769894306064\n",
      "Gradient Descent(8/49): loss=0.3743973317302244\n",
      "Gradient Descent(9/49): loss=0.3726751453771742\n",
      "Gradient Descent(10/49): loss=0.3711369350788784\n",
      "Gradient Descent(11/49): loss=0.3697523181494326\n",
      "Gradient Descent(12/49): loss=0.36849906146658695\n",
      "Gradient Descent(13/49): loss=0.36736007348625005\n",
      "Gradient Descent(14/49): loss=0.3663217061815774\n",
      "Gradient Descent(15/49): loss=0.36537273910642915\n",
      "Gradient Descent(16/49): loss=0.3645037389721228\n",
      "Gradient Descent(17/49): loss=0.36370663692642824\n",
      "Gradient Descent(18/49): loss=0.36297443805173435\n",
      "Gradient Descent(19/49): loss=0.36230101451524754\n",
      "Gradient Descent(20/49): loss=0.3616809535696173\n",
      "Gradient Descent(21/49): loss=0.3611094426712154\n",
      "Gradient Descent(22/49): loss=0.36058218043781404\n",
      "Gradient Descent(23/49): loss=0.36009530606673157\n",
      "Gradient Descent(24/49): loss=0.3596453422630402\n",
      "Gradient Descent(25/49): loss=0.3592291482801341\n",
      "Gradient Descent(26/49): loss=0.35884388069069334\n",
      "Gradient Descent(27/49): loss=0.3584869601842848\n",
      "Gradient Descent(28/49): loss=0.3581560431492684\n",
      "Gradient Descent(29/49): loss=0.35784899711621343\n",
      "Gradient Descent(30/49): loss=0.35756387936508066\n",
      "Gradient Descent(31/49): loss=0.3572989181595617\n",
      "Gradient Descent(32/49): loss=0.35705249618919055\n",
      "Gradient Descent(33/49): loss=0.3568231358864688\n",
      "Gradient Descent(34/49): loss=0.35660948635123146\n",
      "Gradient Descent(35/49): loss=0.35641031166395176\n",
      "Gradient Descent(36/49): loss=0.3562244804078776\n",
      "Gradient Descent(37/49): loss=0.3560509562497786\n",
      "Gradient Descent(38/49): loss=0.3558887894527639\n",
      "Gradient Descent(39/49): loss=0.3557371092136262\n",
      "Gradient Descent(40/49): loss=0.3555951167325765\n",
      "Gradient Descent(41/49): loss=0.35546207893585885\n",
      "Gradient Descent(42/49): loss=0.3553373227821977\n",
      "Gradient Descent(43/49): loss=0.3552202300927557\n",
      "Gradient Descent(44/49): loss=0.3551102328516475\n",
      "Gradient Descent(45/49): loss=0.3550068089302953\n",
      "Gradient Descent(46/49): loss=0.3549094781942629\n",
      "Gradient Descent(47/49): loss=0.3548177989557975\n",
      "Gradient Descent(48/49): loss=0.35473136473929723\n",
      "Gradient Descent(49/49): loss=0.3546498013303884\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41100619615425926\n",
      "Gradient Descent(2/49): loss=0.39445097136804796\n",
      "Gradient Descent(3/49): loss=0.3875919558474806\n",
      "Gradient Descent(4/49): loss=0.38320894787295817\n",
      "Gradient Descent(5/49): loss=0.3799073356196846\n",
      "Gradient Descent(6/49): loss=0.37722215695325845\n",
      "Gradient Descent(7/49): loss=0.37494447875855413\n",
      "Gradient Descent(8/49): loss=0.37296425909769615\n",
      "Gradient Descent(9/49): loss=0.37121632745906935\n",
      "Gradient Descent(10/49): loss=0.36965815353708753\n",
      "Gradient Descent(11/49): loss=0.3682597333105609\n",
      "Gradient Descent(12/49): loss=0.3669985929579977\n",
      "Gradient Descent(13/49): loss=0.36585712350970145\n",
      "Gradient Descent(14/49): loss=0.3648210547480226\n",
      "Gradient Descent(15/49): loss=0.3638785251371437\n",
      "Gradient Descent(16/49): loss=0.36301948373733445\n",
      "Gradient Descent(17/49): loss=0.36223528752401934\n",
      "Gradient Descent(18/49): loss=0.36151841929509554\n",
      "Gradient Descent(19/49): loss=0.36086228306224005\n",
      "Gradient Descent(20/49): loss=0.3602610510004247\n",
      "Gradient Descent(21/49): loss=0.3597095457851573\n",
      "Gradient Descent(22/49): loss=0.3592031479141053\n",
      "Gradient Descent(23/49): loss=0.35873772113615815\n",
      "Gradient Descent(24/49): loss=0.35830955132938286\n",
      "Gradient Descent(25/49): loss=0.35791529559938\n",
      "Gradient Descent(26/49): loss=0.3575519393115689\n",
      "Gradient Descent(27/49): loss=0.3572167594039123\n",
      "Gradient Descent(28/49): loss=0.35690729275993327\n",
      "Gradient Descent(29/49): loss=0.35662130872398157\n",
      "Gradient Descent(30/49): loss=0.3563567850550396\n",
      "Gradient Descent(31/49): loss=0.3561118867701032\n",
      "Gradient Descent(32/49): loss=0.3558849474418009\n",
      "Gradient Descent(33/49): loss=0.3556744525997583\n",
      "Gradient Descent(34/49): loss=0.3554790249495899\n",
      "Gradient Descent(35/49): loss=0.35529741117301705\n",
      "Gradient Descent(36/49): loss=0.35512847011141463\n",
      "Gradient Descent(37/49): loss=0.3549711621658709\n",
      "Gradient Descent(38/49): loss=0.35482453977157746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=0.3546877388244925\n",
      "Gradient Descent(40/49): loss=0.35455997095476527\n",
      "Gradient Descent(41/49): loss=0.3544405165551746\n",
      "Gradient Descent(42/49): loss=0.35432871848437014\n",
      "Gradient Descent(43/49): loss=0.35422397637447206\n",
      "Gradient Descent(44/49): loss=0.35412574148091064\n",
      "Gradient Descent(45/49): loss=0.3540335120195344\n",
      "Gradient Descent(46/49): loss=0.3539468289421891\n",
      "Gradient Descent(47/49): loss=0.353865272107332\n",
      "Gradient Descent(48/49): loss=0.35378845680693016\n",
      "Gradient Descent(49/49): loss=0.3537160306149896\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4106907125459579\n",
      "Gradient Descent(2/49): loss=0.39406067010106727\n",
      "Gradient Descent(3/49): loss=0.38705727027734993\n",
      "Gradient Descent(4/49): loss=0.3825596617141981\n",
      "Gradient Descent(5/49): loss=0.3791775834798908\n",
      "Gradient Descent(6/49): loss=0.376434950742884\n",
      "Gradient Descent(7/49): loss=0.37411421188052735\n",
      "Gradient Descent(8/49): loss=0.3721001325699129\n",
      "Gradient Descent(9/49): loss=0.37032453833225654\n",
      "Gradient Descent(10/49): loss=0.36874312432085604\n",
      "Gradient Descent(11/49): loss=0.3673247903117412\n",
      "Gradient Descent(12/49): loss=0.3660463493827436\n",
      "Gradient Descent(13/49): loss=0.3648897049860476\n",
      "Gradient Descent(14/49): loss=0.36384023845779234\n",
      "Gradient Descent(15/49): loss=0.3628858295645049\n",
      "Gradient Descent(16/49): loss=0.3620162288446866\n",
      "Gradient Descent(17/49): loss=0.3612226364206552\n",
      "Gradient Descent(18/49): loss=0.3604974078785408\n",
      "Gradient Descent(19/49): loss=0.3598338416034135\n",
      "Gradient Descent(20/49): loss=0.3592260202044017\n",
      "Gradient Descent(21/49): loss=0.3586686889940761\n",
      "Gradient Descent(22/49): loss=0.35815716057654823\n",
      "Gradient Descent(23/49): loss=0.3576872383156418\n",
      "Gradient Descent(24/49): loss=0.3572551537901807\n",
      "Gradient Descent(25/49): loss=0.35685751484838046\n",
      "Gradient Descent(26/49): loss=0.35649126186462765\n",
      "Gradient Descent(27/49): loss=0.35615363046807064\n",
      "Gradient Descent(28/49): loss=0.35584211946854055\n",
      "Gradient Descent(29/49): loss=0.3555544630232114\n",
      "Gradient Descent(30/49): loss=0.35528860631285214\n",
      "Gradient Descent(31/49): loss=0.3550426841591132\n",
      "Gradient Descent(32/49): loss=0.35481500213351974\n",
      "Gradient Descent(33/49): loss=0.3546040197976801\n",
      "Gradient Descent(34/49): loss=0.354408335781466\n",
      "Gradient Descent(35/49): loss=0.35422667445758965\n",
      "Gradient Descent(36/49): loss=0.3540578740112886\n",
      "Gradient Descent(37/49): loss=0.3539008757356647\n",
      "Gradient Descent(38/49): loss=0.35375471440871437\n",
      "Gradient Descent(39/49): loss=0.35361850962874514\n",
      "Gradient Descent(40/49): loss=0.3534914580018051\n",
      "Gradient Descent(41/49): loss=0.35337282608877024\n",
      "Gradient Descent(42/49): loss=0.3532619440314639\n",
      "Gradient Descent(43/49): loss=0.3531581997870658\n",
      "Gradient Descent(44/49): loss=0.35306103390847915\n",
      "Gradient Descent(45/49): loss=0.35296993481552486\n",
      "Gradient Descent(46/49): loss=0.3528844345080357\n",
      "Gradient Descent(47/49): loss=0.3528041046773078\n",
      "Gradient Descent(48/49): loss=0.35272855317705526\n",
      "Gradient Descent(49/49): loss=0.352657420819121\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40902076809589705\n",
      "Gradient Descent(2/49): loss=0.3923330276797989\n",
      "Gradient Descent(3/49): loss=0.38544127505505016\n",
      "Gradient Descent(4/49): loss=0.38105604201571863\n",
      "Gradient Descent(5/49): loss=0.3777685357452239\n",
      "Gradient Descent(6/49): loss=0.375103054923173\n",
      "Gradient Descent(7/49): loss=0.37284482722004547\n",
      "Gradient Descent(8/49): loss=0.37088130833719724\n",
      "Gradient Descent(9/49): loss=0.36914654856991075\n",
      "Gradient Descent(10/49): loss=0.3675980225199434\n",
      "Gradient Descent(11/49): loss=0.3662060511558936\n",
      "Gradient Descent(12/49): loss=0.36494859176851313\n",
      "Gradient Descent(13/49): loss=0.3638084780515778\n",
      "Gradient Descent(14/49): loss=0.3627718542980275\n",
      "Gradient Descent(15/49): loss=0.3618272305047578\n",
      "Gradient Descent(16/49): loss=0.36096488068436094\n",
      "Gradient Descent(17/49): loss=0.3601764415784915\n",
      "Gradient Descent(18/49): loss=0.35945463407953776\n",
      "Gradient Descent(19/49): loss=0.35879306290199997\n",
      "Gradient Descent(20/49): loss=0.35818606793091873\n",
      "Gradient Descent(21/49): loss=0.35762861076913544\n",
      "Gradient Descent(22/49): loss=0.35711618594067424\n",
      "Gradient Descent(23/49): loss=0.35664474982020794\n",
      "Gradient Descent(24/49): loss=0.3562106626220323\n",
      "Gradient Descent(25/49): loss=0.3558106402354628\n",
      "Gradient Descent(26/49): loss=0.3554417136472152\n",
      "Gradient Descent(27/49): loss=0.3551011943292982\n",
      "Gradient Descent(28/49): loss=0.3547866444055135\n",
      "Gradient Descent(29/49): loss=0.35449585071085654\n",
      "Gradient Descent(30/49): loss=0.3542268020704176\n",
      "Gradient Descent(31/49): loss=0.3539776692765382\n",
      "Gradient Descent(32/49): loss=0.3537467873538384\n",
      "Gradient Descent(33/49): loss=0.35353263978381566\n",
      "Gradient Descent(34/49): loss=0.3533338444224781\n",
      "Gradient Descent(35/49): loss=0.3531491408916792\n",
      "Gradient Descent(36/49): loss=0.3529773792614325\n",
      "Gradient Descent(37/49): loss=0.352817509869319\n",
      "Gradient Descent(38/49): loss=0.35266857414609665\n",
      "Gradient Descent(39/49): loss=0.3525296963352332\n",
      "Gradient Descent(40/49): loss=0.35240007600929324\n",
      "Gradient Descent(41/49): loss=0.35227898129872315\n",
      "Gradient Descent(42/49): loss=0.35216574275910295\n",
      "Gradient Descent(43/49): loss=0.3520597478118364\n",
      "Gradient Descent(44/49): loss=0.3519604357008146\n",
      "Gradient Descent(45/49): loss=0.3518672929140918\n",
      "Gradient Descent(46/49): loss=0.3517798490252191\n",
      "Gradient Descent(47/49): loss=0.35169767291375764\n",
      "Gradient Descent(48/49): loss=0.35162036932876156\n",
      "Gradient Descent(49/49): loss=0.35154757576275336\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41163938597445915\n",
      "Gradient Descent(2/49): loss=0.3950969703642415\n",
      "Gradient Descent(3/49): loss=0.3882038022772257\n",
      "Gradient Descent(4/49): loss=0.38382026975640304\n",
      "Gradient Descent(5/49): loss=0.3805412659738525\n",
      "Gradient Descent(6/49): loss=0.377888164368388\n",
      "Gradient Descent(7/49): loss=0.3756442178483449\n",
      "Gradient Descent(8/49): loss=0.37369567857127883\n",
      "Gradient Descent(9/49): loss=0.37197586494047497\n",
      "Gradient Descent(10/49): loss=0.3704418108858887\n",
      "Gradient Descent(11/49): loss=0.3690635765431164\n",
      "Gradient Descent(12/49): loss=0.36781896952572174\n",
      "Gradient Descent(13/49): loss=0.3666907434823366\n",
      "Gradient Descent(14/49): loss=0.3656650074403863\n",
      "Gradient Descent(15/49): loss=0.36473026589059715\n",
      "Gradient Descent(16/49): loss=0.36387680773200237\n",
      "Gradient Descent(17/49): loss=0.3630962988509857\n",
      "Gradient Descent(18/49): loss=0.36238149928870234\n",
      "Gradient Descent(19/49): loss=0.36172605979921885\n",
      "Gradient Descent(20/49): loss=0.36112437082429644\n",
      "Gradient Descent(21/49): loss=0.3605714471853379\n",
      "Gradient Descent(22/49): loss=0.360062837822282\n",
      "Gradient Descent(23/49): loss=0.3595945535698325\n",
      "Gradient Descent(24/49): loss=0.3591630082492692\n",
      "Gradient Descent(25/49): loss=0.35876496982035083\n",
      "Gradient Descent(26/49): loss=0.35839751929874536\n",
      "Gradient Descent(27/49): loss=0.3580580157872158\n",
      "Gradient Descent(28/49): loss=0.35774406640714695\n",
      "Gradient Descent(29/49): loss=0.3574535002215397\n",
      "Gradient Descent(30/49): loss=0.35718434545601\n",
      "Gradient Descent(31/49): loss=0.3569348094793958\n",
      "Gradient Descent(32/49): loss=0.35670326111911665\n",
      "Gradient Descent(33/49): loss=0.3564882149709515\n",
      "Gradient Descent(34/49): loss=0.3562883174268245\n",
      "Gradient Descent(35/49): loss=0.3561023341932699\n",
      "Gradient Descent(36/49): loss=0.35592913911148727\n",
      "Gradient Descent(37/49): loss=0.35576770412007686\n",
      "Gradient Descent(38/49): loss=0.35561709022569665\n",
      "Gradient Descent(39/49): loss=0.3554764393664103\n",
      "Gradient Descent(40/49): loss=0.35534496706848323\n",
      "Gradient Descent(41/49): loss=0.3552219558105909\n",
      "Gradient Descent(42/49): loss=0.3551067490204312\n",
      "Gradient Descent(43/49): loss=0.35499874563800693\n",
      "Gradient Descent(44/49): loss=0.35489739518771546\n",
      "Gradient Descent(45/49): loss=0.3548021933081089\n",
      "Gradient Descent(46/49): loss=0.3547126776939713\n",
      "Gradient Descent(47/49): loss=0.3546284244103664\n",
      "Gradient Descent(48/49): loss=0.35454904454266145\n",
      "Gradient Descent(49/49): loss=0.3544741811503439\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4089207620186366\n",
      "Gradient Descent(2/49): loss=0.39343793388110937\n",
      "Gradient Descent(3/49): loss=0.3868190011395421\n",
      "Gradient Descent(4/49): loss=0.3824994585058567\n",
      "Gradient Descent(5/49): loss=0.37921832388838905\n",
      "Gradient Descent(6/49): loss=0.37653889255751183\n",
      "Gradient Descent(7/49): loss=0.37426240314003023\n",
      "Gradient Descent(8/49): loss=0.37228336481819735\n",
      "Gradient Descent(9/49): loss=0.37053855386899986\n",
      "Gradient Descent(10/49): loss=0.36898611653905455\n",
      "Gradient Descent(11/49): loss=0.36759612311786344\n",
      "Gradient Descent(12/49): loss=0.36634590075239376\n",
      "Gradient Descent(13/49): loss=0.3652175278252603\n",
      "Gradient Descent(14/49): loss=0.36419638557807865\n",
      "Gradient Descent(15/49): loss=0.3632702661170717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=0.36242879284483054\n",
      "Gradient Descent(17/49): loss=0.3616630263089954\n",
      "Gradient Descent(18/49): loss=0.36096518533894945\n",
      "Gradient Descent(19/49): loss=0.36032844276908055\n",
      "Gradient Descent(20/49): loss=0.359746771123616\n",
      "Gradient Descent(21/49): loss=0.359214822834747\n",
      "Gradient Descent(22/49): loss=0.35872783503161\n",
      "Gradient Descent(23/49): loss=0.35828155229091524\n",
      "Gradient Descent(24/49): loss=0.3578721628533918\n",
      "Gradient Descent(25/49): loss=0.3574962451741749\n",
      "Gradient Descent(26/49): loss=0.35715072257483044\n",
      "Gradient Descent(27/49): loss=0.35683282437029384\n",
      "Gradient Descent(28/49): loss=0.35654005225980295\n",
      "Gradient Descent(29/49): loss=0.3562701510619918\n",
      "Gradient Descent(30/49): loss=0.35602108308199387\n",
      "Gradient Descent(31/49): loss=0.3557910055493701\n",
      "Gradient Descent(32/49): loss=0.35557825067742416\n",
      "Gradient Descent(33/49): loss=0.3553813079786501\n",
      "Gradient Descent(34/49): loss=0.3551988085355416\n",
      "Gradient Descent(35/49): loss=0.3550295109761855\n",
      "Gradient Descent(36/49): loss=0.35487228894370776\n",
      "Gradient Descent(37/49): loss=0.35472611988040226\n",
      "Gradient Descent(38/49): loss=0.354590074973135\n",
      "Gradient Descent(39/49): loss=0.3544633101277648\n",
      "Gradient Descent(40/49): loss=0.35434505785786097\n",
      "Gradient Descent(41/49): loss=0.3542346199876952\n",
      "Gradient Descent(42/49): loss=0.3541313610818884\n",
      "Gradient Descent(43/49): loss=0.3540347025246666\n",
      "Gradient Descent(44/49): loss=0.3539441171807294\n",
      "Gradient Descent(45/49): loss=0.3538591245775497\n",
      "Gradient Descent(46/49): loss=0.3537792865556975\n",
      "Gradient Descent(47/49): loss=0.35370420333967517\n",
      "Gradient Descent(48/49): loss=0.35363350998692794\n",
      "Gradient Descent(49/49): loss=0.3535668731772193\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4086070753943944\n",
      "Gradient Descent(2/49): loss=0.3930358768592375\n",
      "Gradient Descent(3/49): loss=0.3862649345948489\n",
      "Gradient Descent(4/49): loss=0.38183161775017355\n",
      "Gradient Descent(5/49): loss=0.3784725419111469\n",
      "Gradient Descent(6/49): loss=0.37573782696609215\n",
      "Gradient Descent(7/49): loss=0.3734198370633792\n",
      "Gradient Descent(8/49): loss=0.371408035598673\n",
      "Gradient Descent(9/49): loss=0.3696363629509376\n",
      "Gradient Descent(10/49): loss=0.36806130933344766\n",
      "Gradient Descent(11/49): loss=0.3666519224495347\n",
      "Gradient Descent(12/49): loss=0.36538486050103364\n",
      "Gradient Descent(13/49): loss=0.3642417408375858\n",
      "Gradient Descent(14/49): loss=0.363207612431022\n",
      "Gradient Descent(15/49): loss=0.36227001860422914\n",
      "Gradient Descent(16/49): loss=0.36141839019797256\n",
      "Gradient Descent(17/49): loss=0.3606436342522699\n",
      "Gradient Descent(18/49): loss=0.35993784394125283\n",
      "Gradient Descent(19/49): loss=0.3592940867848179\n",
      "Gradient Descent(20/49): loss=0.3587062451908097\n",
      "Gradient Descent(21/49): loss=0.35816889309477623\n",
      "Gradient Descent(22/49): loss=0.3576771982252004\n",
      "Gradient Descent(23/49): loss=0.3572268430521821\n",
      "Gradient Descent(24/49): loss=0.3568139597013846\n",
      "Gradient Descent(25/49): loss=0.3564350755502672\n",
      "Gradient Descent(26/49): loss=0.35608706717036925\n",
      "Gradient Descent(27/49): loss=0.355767120916788\n",
      "Gradient Descent(28/49): loss=0.3554726989035745\n",
      "Gradient Descent(29/49): loss=0.3552015094099043\n",
      "Gradient Descent(30/49): loss=0.3549514809800752\n",
      "Gradient Descent(31/49): loss=0.3547207396387099\n",
      "Gradient Descent(32/49): loss=0.35450758875948485\n",
      "Gradient Descent(33/49): loss=0.3543104912135447\n",
      "Gradient Descent(34/49): loss=0.35412805349084564\n",
      "Gradient Descent(35/49): loss=0.35395901153967596\n",
      "Gradient Descent(36/49): loss=0.3538022181105477\n",
      "Gradient Descent(37/49): loss=0.35365663142330434\n",
      "Gradient Descent(38/49): loss=0.3535213050026859\n",
      "Gradient Descent(39/49): loss=0.35339537854916736\n",
      "Gradient Descent(40/49): loss=0.35327806972972575\n",
      "Gradient Descent(41/49): loss=0.3531686667880766\n",
      "Gradient Descent(42/49): loss=0.3530665218864619\n",
      "Gradient Descent(43/49): loss=0.35297104510171534\n",
      "Gradient Descent(44/49): loss=0.35288169900743976\n",
      "Gradient Descent(45/49): loss=0.35279799378196625\n",
      "Gradient Descent(46/49): loss=0.35271948278855203\n",
      "Gradient Descent(47/49): loss=0.3526457585801814\n",
      "Gradient Descent(48/49): loss=0.35257644928649484\n",
      "Gradient Descent(49/49): loss=0.35251121534490754\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4069032935041009\n",
      "Gradient Descent(2/49): loss=0.39131309142423826\n",
      "Gradient Descent(3/49): loss=0.38466524694438936\n",
      "Gradient Descent(4/49): loss=0.3803474747517019\n",
      "Gradient Descent(5/49): loss=0.377083500456985\n",
      "Gradient Descent(6/49): loss=0.37442533342970225\n",
      "Gradient Descent(7/49): loss=0.37216868171516326\n",
      "Gradient Descent(8/49): loss=0.3702059688947352\n",
      "Gradient Descent(9/49): loss=0.36847350748690444\n",
      "Gradient Descent(10/49): loss=0.36692965622021\n",
      "Gradient Descent(11/49): loss=0.3655449423884195\n",
      "Gradient Descent(12/49): loss=0.3642972085448965\n",
      "Gradient Descent(13/49): loss=0.3631690301923925\n",
      "Gradient Descent(14/49): loss=0.36214623830136605\n",
      "Gradient Descent(15/49): loss=0.36121701877219276\n",
      "Gradient Descent(16/49): loss=0.36037133315097436\n",
      "Gradient Descent(17/49): loss=0.35960052843026724\n",
      "Gradient Descent(18/49): loss=0.3588970634872542\n",
      "Gradient Descent(19/49): loss=0.3582543103942194\n",
      "Gradient Descent(20/49): loss=0.3576664054863169\n",
      "Gradient Descent(21/49): loss=0.3571281345414286\n",
      "Gradient Descent(22/49): loss=0.35663484202780676\n",
      "Gradient Descent(23/49): loss=0.3561823577968702\n",
      "Gradient Descent(24/49): loss=0.3557669367465568\n",
      "Gradient Descent(25/49): loss=0.3553852083612607\n",
      "Gradient Descent(26/49): loss=0.35503413394078576\n",
      "Gradient Descent(27/49): loss=0.3547109699376863\n",
      "Gradient Descent(28/49): loss=0.35441323623654114\n",
      "Gradient Descent(29/49): loss=0.35413868849662944\n",
      "Gradient Descent(30/49): loss=0.35388529388329487\n",
      "Gradient Descent(31/49): loss=0.3536512096602077\n",
      "Gradient Descent(32/49): loss=0.35343476422253295\n",
      "Gradient Descent(33/49): loss=0.35323444023149464\n",
      "Gradient Descent(34/49): loss=0.35304885957194715\n",
      "Gradient Descent(35/49): loss=0.35287676990173983\n",
      "Gradient Descent(36/49): loss=0.35271703259864445\n",
      "Gradient Descent(37/49): loss=0.35256861194003775\n",
      "Gradient Descent(38/49): loss=0.3524305653742665\n",
      "Gradient Descent(39/49): loss=0.35230203476200583\n",
      "Gradient Descent(40/49): loss=0.35218223848194646\n",
      "Gradient Descent(41/49): loss=0.35207046430852773\n",
      "Gradient Descent(42/49): loss=0.3519660629807204\n",
      "Gradient Descent(43/49): loss=0.3518684423904641\n",
      "Gradient Descent(44/49): loss=0.35177706232758993\n",
      "Gradient Descent(45/49): loss=0.3516914297251627\n",
      "Gradient Descent(46/49): loss=0.35161109435534094\n",
      "Gradient Descent(47/49): loss=0.3515356449312348\n",
      "Gradient Descent(48/49): loss=0.3514647055749582\n",
      "Gradient Descent(49/49): loss=0.3513979326162296\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4095641154037971\n",
      "Gradient Descent(2/49): loss=0.3940792063059588\n",
      "Gradient Descent(3/49): loss=0.38742708191172553\n",
      "Gradient Descent(4/49): loss=0.3831124854753714\n",
      "Gradient Descent(5/49): loss=0.3798585703463754\n",
      "Gradient Descent(6/49): loss=0.37721408989182237\n",
      "Gradient Descent(7/49): loss=0.37497276132355023\n",
      "Gradient Descent(8/49): loss=0.3730258215522558\n",
      "Gradient Descent(9/49): loss=0.37130888006742013\n",
      "Gradient Descent(10/49): loss=0.36977988555847585\n",
      "Gradient Descent(11/49): loss=0.368409131650026\n",
      "Gradient Descent(12/49): loss=0.3671743339111663\n",
      "Gradient Descent(13/49): loss=0.3660580068660871\n",
      "Gradient Descent(14/49): loss=0.36504596280849383\n",
      "Gradient Descent(15/49): loss=0.3641263971078438\n",
      "Gradient Descent(16/49): loss=0.36328930005045124\n",
      "Gradient Descent(17/49): loss=0.36252606071378035\n",
      "Gradient Descent(18/49): loss=0.36182918918884716\n",
      "Gradient Descent(19/49): loss=0.36119211473732626\n",
      "Gradient Descent(20/49): loss=0.36060903442555803\n",
      "Gradient Descent(21/49): loss=0.360074796401195\n",
      "Gradient Descent(22/49): loss=0.35958480765476786\n",
      "Gradient Descent(23/49): loss=0.3591349595670171\n",
      "Gradient Descent(24/49): loss=0.35872156670930416\n",
      "Gradient Descent(25/49): loss=0.3583413157551726\n",
      "Gradient Descent(26/49): loss=0.3579912222741437\n",
      "Gradient Descent(27/49): loss=0.3576685937910145\n",
      "Gradient Descent(28/49): loss=0.3573709979128065\n",
      "Gradient Descent(29/49): loss=0.35709623461784046\n",
      "Gradient Descent(30/49): loss=0.3568423120093668\n",
      "Gradient Descent(31/49): loss=0.35660742498690307\n",
      "Gradient Descent(32/49): loss=0.35638993639963407\n",
      "Gradient Descent(33/49): loss=0.35618836032971685\n",
      "Gradient Descent(34/49): loss=0.3560013472170254\n",
      "Gradient Descent(35/49): loss=0.35582767058622317\n",
      "Gradient Descent(36/49): loss=0.355666215175848\n",
      "Gradient Descent(37/49): loss=0.35551596630000076\n",
      "Gradient Descent(38/49): loss=0.35537600029816413\n",
      "Gradient Descent(39/49): loss=0.3552454759490259\n",
      "Gradient Descent(40/49): loss=0.35512362674096276\n",
      "Gradient Descent(41/49): loss=0.3550097539058156\n",
      "Gradient Descent(42/49): loss=0.3549032201343346\n",
      "Gradient Descent(43/49): loss=0.35480344390161356\n",
      "Gradient Descent(44/49): loss=0.35470989433932937\n",
      "Gradient Descent(45/49): loss=0.3546220865988875\n",
      "Gradient Descent(46/49): loss=0.3545395776558762\n",
      "Gradient Descent(47/49): loss=0.3544619625117104\n",
      "Gradient Descent(48/49): loss=0.354388870753119\n",
      "Gradient Descent(49/49): loss=0.3543199634343206\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4070057399496624\n",
      "Gradient Descent(2/49): loss=0.3925259250829168\n",
      "Gradient Descent(3/49): loss=0.3860972967918343\n",
      "Gradient Descent(4/49): loss=0.3818273290592744\n",
      "Gradient Descent(5/49): loss=0.3785614042165276\n",
      "Gradient Descent(6/49): loss=0.37588545571342713\n",
      "Gradient Descent(7/49): loss=0.37360955042147087\n",
      "Gradient Descent(8/49): loss=0.3716320529232757\n",
      "Gradient Descent(9/49): loss=0.3698912410369395\n",
      "Gradient Descent(10/49): loss=0.36834566213707887\n",
      "Gradient Descent(11/49): loss=0.3669652944916822\n",
      "Gradient Descent(12/49): loss=0.3657271654164548\n",
      "Gradient Descent(13/49): loss=0.36461297884243843\n",
      "Gradient Descent(14/49): loss=0.36360772964549304\n",
      "Gradient Descent(15/49): loss=0.3626988412960311\n",
      "Gradient Descent(16/49): loss=0.36187559987774737\n",
      "Gradient Descent(17/49): loss=0.36112876536898997\n",
      "Gradient Descent(18/49): loss=0.3604502939049406\n",
      "Gradient Descent(19/49): loss=0.3598331323093047\n",
      "Gradient Descent(20/49): loss=0.35927106136550035\n",
      "Gradient Descent(21/49): loss=0.35875857303172126\n",
      "Gradient Descent(22/49): loss=0.3582907720132758\n",
      "Gradient Descent(23/49): loss=0.35786329530702377\n",
      "Gradient Descent(24/49): loss=0.3574722453524851\n",
      "Gradient Descent(25/49): loss=0.3571141337291116\n",
      "Gradient Descent(26/49): loss=0.3567858332013783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=0.35648453649547207\n",
      "Gradient Descent(28/49): loss=0.3562077205927876\n",
      "Gradient Descent(29/49): loss=0.3559531156081057\n",
      "Gradient Descent(30/49): loss=0.3557186775234472\n",
      "Gradient Descent(31/49): loss=0.3555025641974774\n",
      "Gradient Descent(32/49): loss=0.3553031141815444\n",
      "Gradient Descent(33/49): loss=0.3551188279580408\n",
      "Gradient Descent(34/49): loss=0.35494835128222246\n",
      "Gradient Descent(35/49): loss=0.35479046036007944\n",
      "Gradient Descent(36/49): loss=0.3546440486358922\n",
      "Gradient Descent(37/49): loss=0.3545081149962883\n",
      "Gradient Descent(38/49): loss=0.35438175322476617\n",
      "Gradient Descent(39/49): loss=0.3542641425631172\n",
      "Gradient Descent(40/49): loss=0.3541545392549438\n",
      "Gradient Descent(41/49): loss=0.354052268962294\n",
      "Gradient Descent(42/49): loss=0.3539567199598692\n",
      "Gradient Descent(43/49): loss=0.3538673370227585\n",
      "Gradient Descent(44/49): loss=0.35378361593354507\n",
      "Gradient Descent(45/49): loss=0.3537050985431924\n",
      "Gradient Descent(46/49): loss=0.35363136832755837\n",
      "Gradient Descent(47/49): loss=0.3535620463878861\n",
      "Gradient Descent(48/49): loss=0.3534967878493116\n",
      "Gradient Descent(49/49): loss=0.35343527861643564\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40669525353974656\n",
      "Gradient Descent(2/49): loss=0.39211089502538465\n",
      "Gradient Descent(3/49): loss=0.385524082207546\n",
      "Gradient Descent(4/49): loss=0.3811420119345086\n",
      "Gradient Descent(5/49): loss=0.37780079236145603\n",
      "Gradient Descent(6/49): loss=0.37507160669099054\n",
      "Gradient Descent(7/49): loss=0.37275560637264155\n",
      "Gradient Descent(8/49): loss=0.37074631897544363\n",
      "Gradient Descent(9/49): loss=0.36897936080296156\n",
      "Gradient Descent(10/49): loss=0.36741173344606404\n",
      "Gradient Descent(11/49): loss=0.3660124571538694\n",
      "Gradient Descent(12/49): loss=0.36475792775906224\n",
      "Gradient Descent(13/49): loss=0.36362941024937706\n",
      "Gradient Descent(14/49): loss=0.3626115804610939\n",
      "Gradient Descent(15/49): loss=0.3616916209582863\n",
      "Gradient Descent(16/49): loss=0.3608586297216295\n",
      "Gradient Descent(17/49): loss=0.36010321539814005\n",
      "Gradient Descent(18/49): loss=0.3594172090820579\n",
      "Gradient Descent(19/49): loss=0.35879345182635025\n",
      "Gradient Descent(20/49): loss=0.3582256331245108\n",
      "Gradient Descent(21/49): loss=0.35770816480897116\n",
      "Gradient Descent(22/49): loss=0.3572360802958485\n",
      "Gradient Descent(23/49): loss=0.3568049524741625\n",
      "Gradient Descent(24/49): loss=0.3564108256628198\n",
      "Gradient Descent(25/49): loss=0.35605015843199794\n",
      "Gradient Descent(26/49): loss=0.35571977499307444\n",
      "Gradient Descent(27/49): loss=0.3554168234738807\n",
      "Gradient Descent(28/49): loss=0.35513873981827293\n",
      "Gradient Descent(29/49): loss=0.35488321634595016\n",
      "Gradient Descent(30/49): loss=0.3546481742214192\n",
      "Gradient Descent(31/49): loss=0.35443173923672794\n",
      "Gradient Descent(32/49): loss=0.3542322204285484\n",
      "Gradient Descent(33/49): loss=0.35404809113809715\n",
      "Gradient Descent(34/49): loss=0.35387797219011924\n",
      "Gradient Descent(35/49): loss=0.3537206169202056\n",
      "Gradient Descent(36/49): loss=0.3535748978218399\n",
      "Gradient Descent(37/49): loss=0.35343979461849434\n",
      "Gradient Descent(38/49): loss=0.3533143835937442\n",
      "Gradient Descent(39/49): loss=0.35319782803516303\n",
      "Gradient Descent(40/49): loss=0.353089369666744\n",
      "Gradient Descent(41/49): loss=0.3529883209605429\n",
      "Gradient Descent(42/49): loss=0.3528940582317609\n",
      "Gradient Descent(43/49): loss=0.3528060154330179\n",
      "Gradient Descent(44/49): loss=0.35272367857348796\n",
      "Gradient Descent(45/49): loss=0.35264658069712995\n",
      "Gradient Descent(46/49): loss=0.3525742973616903\n",
      "Gradient Descent(47/49): loss=0.352506442566646\n",
      "Gradient Descent(48/49): loss=0.35244266508394245\n",
      "Gradient Descent(49/49): loss=0.35238264515037904\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40496129239406187\n",
      "Gradient Descent(2/49): loss=0.3903949453886639\n",
      "Gradient Descent(3/49): loss=0.38394086821111423\n",
      "Gradient Descent(4/49): loss=0.37967681662954395\n",
      "Gradient Descent(5/49): loss=0.3764308975435804\n",
      "Gradient Descent(6/49): loss=0.3737774625806936\n",
      "Gradient Descent(7/49): loss=0.37152150860437094\n",
      "Gradient Descent(8/49): loss=0.36955976638389826\n",
      "Gradient Descent(9/49): loss=0.3678303576691109\n",
      "Gradient Descent(10/49): loss=0.3662922340996198\n",
      "Gradient Descent(11/49): loss=0.36491594942524996\n",
      "Gradient Descent(12/49): loss=0.36367912071901004\n",
      "Gradient Descent(13/49): loss=0.3625639962428286\n",
      "Gradient Descent(14/49): loss=0.3615560502830681\n",
      "Gradient Descent(15/49): loss=0.36064311789282855\n",
      "Gradient Descent(16/49): loss=0.35981483273043263\n",
      "Gradient Descent(17/49): loss=0.3590622446760185\n",
      "Gradient Descent(18/49): loss=0.3583775490959887\n",
      "Gradient Descent(19/49): loss=0.35775388822083465\n",
      "Gradient Descent(20/49): loss=0.3571852007486891\n",
      "Gradient Descent(21/49): loss=0.3566661047426055\n",
      "Gradient Descent(22/49): loss=0.3561918042072237\n",
      "Gradient Descent(23/49): loss=0.355758012985509\n",
      "Gradient Descent(24/49): loss=0.35536089166125673\n",
      "Gradient Descent(25/49): loss=0.35499699446806104\n",
      "Gradient Descent(26/49): loss=0.35466322406950324\n",
      "Gradient Descent(27/49): loss=0.35435679265493625\n",
      "Gradient Descent(28/49): loss=0.35407518819199957\n",
      "Gradient Descent(29/49): loss=0.35381614495406444\n",
      "Gradient Descent(30/49): loss=0.3535776176381364\n",
      "Gradient Descent(31/49): loss=0.3533577585320364\n",
      "Gradient Descent(32/49): loss=0.3531548972957313\n",
      "Gradient Descent(33/49): loss=0.3529675230016239\n",
      "Gradient Descent(34/49): loss=0.3527942681399387\n",
      "Gradient Descent(35/49): loss=0.35263389434318304\n",
      "Gradient Descent(36/49): loss=0.35248527962156884\n",
      "Gradient Descent(37/49): loss=0.35234740693175715\n",
      "Gradient Descent(38/49): loss=0.3522193539261185\n",
      "Gradient Descent(39/49): loss=0.35210028375017643\n",
      "Gradient Descent(40/49): loss=0.3519894367729747\n",
      "Gradient Descent(41/49): loss=0.3518861231494789\n",
      "Gradient Descent(42/49): loss=0.35178971612633086\n",
      "Gradient Descent(43/49): loss=0.35169964601271786\n",
      "Gradient Descent(44/49): loss=0.35161539474712\n",
      "Gradient Descent(45/49): loss=0.3515364909985004\n",
      "Gradient Descent(46/49): loss=0.3514625057472971\n",
      "Gradient Descent(47/49): loss=0.3513930482975272\n",
      "Gradient Descent(48/49): loss=0.3513277626765349\n",
      "Gradient Descent(49/49): loss=0.3512663243835315\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40765763796147886\n",
      "Gradient Descent(2/49): loss=0.3931611027226352\n",
      "Gradient Descent(3/49): loss=0.38670192006267184\n",
      "Gradient Descent(4/49): loss=0.38244281367785576\n",
      "Gradient Descent(5/49): loss=0.37920851070208944\n",
      "Gradient Descent(6/49): loss=0.37657001663442435\n",
      "Gradient Descent(7/49): loss=0.3743303570349428\n",
      "Gradient Descent(8/49): loss=0.37238510506531025\n",
      "Gradient Descent(9/49): loss=0.3706717145413803\n",
      "Gradient Descent(10/49): loss=0.36914876019269877\n",
      "Gradient Descent(11/49): loss=0.3677865898756728\n",
      "Gradient Descent(12/49): loss=0.3665627164394333\n",
      "Gradient Descent(13/49): loss=0.3654593463884604\n",
      "Gradient Descent(14/49): loss=0.36446195231675854\n",
      "Gradient Descent(15/49): loss=0.36355839429265485\n",
      "Gradient Descent(16/49): loss=0.36273834918839104\n",
      "Gradient Descent(17/49): loss=0.36199292246527\n",
      "Gradient Descent(18/49): loss=0.3613143731851857\n",
      "Gradient Descent(19/49): loss=0.3606959121573067\n",
      "Gradient Descent(20/49): loss=0.3601315490401843\n",
      "Gradient Descent(21/49): loss=0.35961597330078865\n",
      "Gradient Descent(22/49): loss=0.35914445930826866\n",
      "Gradient Descent(23/49): loss=0.35871278912309296\n",
      "Gradient Descent(24/49): loss=0.3583171886017245\n",
      "Gradient Descent(25/49): loss=0.3579542737611635\n",
      "Gradient Descent(26/49): loss=0.35762100521899615\n",
      "Gradient Descent(27/49): loss=0.35731464911077343\n",
      "Gradient Descent(28/49): loss=0.3570327432895215\n",
      "Gradient Descent(29/49): loss=0.3567730678950727\n",
      "Gradient Descent(30/49): loss=0.3565336195835749\n",
      "Gradient Descent(31/49): loss=0.35631258885559014\n",
      "Gradient Descent(32/49): loss=0.3561083400313758\n",
      "Gradient Descent(33/49): loss=0.3559193935053948\n",
      "Gradient Descent(34/49): loss=0.3557444099763612\n",
      "Gradient Descent(35/49): loss=0.35558217639936673\n",
      "Gradient Descent(36/49): loss=0.35543159344648645\n",
      "Gradient Descent(37/49): loss=0.35529166429427766\n",
      "Gradient Descent(38/49): loss=0.3551614845826377\n",
      "Gradient Descent(39/49): loss=0.3550402334109005\n",
      "Gradient Descent(40/49): loss=0.35492716525484896\n",
      "Gradient Descent(41/49): loss=0.3548216027032362\n",
      "Gradient Descent(42/49): loss=0.3547229299250191\n",
      "Gradient Descent(43/49): loss=0.35463058678924686\n",
      "Gradient Descent(44/49): loss=0.3545440635687568\n",
      "Gradient Descent(45/49): loss=0.3544628961667667\n",
      "Gradient Descent(46/49): loss=0.35438666181234607\n",
      "Gradient Descent(47/49): loss=0.35431497517674226\n",
      "Gradient Descent(48/49): loss=0.35424748486779195\n",
      "Gradient Descent(49/49): loss=0.3541838702642556\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40526112994733643\n",
      "Gradient Descent(2/49): loss=0.39169900261632373\n",
      "Gradient Descent(3/49): loss=0.3854194445112803\n",
      "Gradient Descent(4/49): loss=0.3811883741106376\n",
      "Gradient Descent(5/49): loss=0.37793333202249546\n",
      "Gradient Descent(6/49): loss=0.3752591110133788\n",
      "Gradient Descent(7/49): loss=0.37298352172279453\n",
      "Gradient Descent(8/49): loss=0.37100812058350024\n",
      "Gradient Descent(9/49): loss=0.36927227329160867\n",
      "Gradient Descent(10/49): loss=0.36773468490756456\n",
      "Gradient Descent(11/49): loss=0.36636509996953776\n",
      "Gradient Descent(12/49): loss=0.36514016217370465\n",
      "Gradient Descent(13/49): loss=0.3640411508027755\n",
      "Gradient Descent(14/49): loss=0.36305264478118265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=0.36216168285518435\n",
      "Gradient Descent(16/49): loss=0.3613572070033333\n",
      "Gradient Descent(17/49): loss=0.3606296764370153\n",
      "Gradient Descent(18/49): loss=0.3599707890850302\n",
      "Gradient Descent(19/49): loss=0.3593732735261141\n",
      "Gradient Descent(20/49): loss=0.35883072877739164\n",
      "Gradient Descent(21/49): loss=0.35833749768768475\n",
      "Gradient Descent(22/49): loss=0.35788856466786756\n",
      "Gradient Descent(23/49): loss=0.35747947155569015\n",
      "Gradient Descent(24/49): loss=0.3571062473479699\n",
      "Gradient Descent(25/49): loss=0.3567653487855174\n",
      "Gradient Descent(26/49): loss=0.35645360960606814\n",
      "Gradient Descent(27/49): loss=0.35616819684326906\n",
      "Gradient Descent(28/49): loss=0.35590657294018463\n",
      "Gradient Descent(29/49): loss=0.3556664627227028\n",
      "Gradient Descent(30/49): loss=0.35544582447891326\n",
      "Gradient Descent(31/49): loss=0.35524282453900724\n",
      "Gradient Descent(32/49): loss=0.3550558148622573\n",
      "Gradient Descent(33/49): loss=0.35488331322370315\n",
      "Gradient Descent(34/49): loss=0.35472398566041674\n",
      "Gradient Descent(35/49): loss=0.35457663089059943\n",
      "Gradient Descent(36/49): loss=0.3544401664617262\n",
      "Gradient Descent(37/49): loss=0.35431361641897696\n",
      "Gradient Descent(38/49): loss=0.35419610031407384\n",
      "Gradient Descent(39/49): loss=0.3540868233987027\n",
      "Gradient Descent(40/49): loss=0.353985067866912\n",
      "Gradient Descent(41/49): loss=0.3538901850280141\n",
      "Gradient Descent(42/49): loss=0.3538015883061263\n",
      "Gradient Descent(43/49): loss=0.3537187469750336\n",
      "Gradient Descent(44/49): loss=0.3536411805478807\n",
      "Gradient Descent(45/49): loss=0.35356845375058865\n",
      "Gradient Descent(46/49): loss=0.35350017201606215\n",
      "Gradient Descent(47/49): loss=0.3534359774433999\n",
      "Gradient Descent(48/49): loss=0.35337554517257525\n",
      "Gradient Descent(49/49): loss=0.3533185801305615\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40495524698201424\n",
      "Gradient Descent(2/49): loss=0.3912697844775654\n",
      "Gradient Descent(3/49): loss=0.38482739984043585\n",
      "Gradient Descent(4/49): loss=0.3804865819715385\n",
      "Gradient Descent(5/49): loss=0.37715894507579684\n",
      "Gradient Descent(6/49): loss=0.37443340832691097\n",
      "Gradient Descent(7/49): loss=0.3721189955831921\n",
      "Gradient Descent(8/49): loss=0.37011267695712624\n",
      "Gradient Descent(9/49): loss=0.36835133120790436\n",
      "Gradient Descent(10/49): loss=0.3667922191168244\n",
      "Gradient Descent(11/49): loss=0.36540418383650447\n",
      "Gradient Descent(12/49): loss=0.3641632695358511\n",
      "Gradient Descent(13/49): loss=0.36305033504681505\n",
      "Gradient Descent(14/49): loss=0.3620496510740119\n",
      "Gradient Descent(15/49): loss=0.36114802161240306\n",
      "Gradient Descent(16/49): loss=0.3603342036834076\n",
      "Gradient Descent(17/49): loss=0.3595985062975952\n",
      "Gradient Descent(18/49): loss=0.35893250210923205\n",
      "Gradient Descent(19/49): loss=0.35832881278492884\n",
      "Gradient Descent(20/49): loss=0.3577809443352757\n",
      "Gradient Descent(21/49): loss=0.3572831574378493\n",
      "Gradient Descent(22/49): loss=0.3568303630223788\n",
      "Gradient Descent(23/49): loss=0.35641803661370247\n",
      "Gradient Descent(24/49): loss=0.3560421469650347\n",
      "Gradient Descent(25/49): loss=0.3556990958324735\n",
      "Gradient Descent(26/49): loss=0.3553856666151764\n",
      "Gradient Descent(27/49): loss=0.35509898017760927\n",
      "Gradient Descent(28/49): loss=0.3548364565804085\n",
      "Gradient Descent(29/49): loss=0.35459578173672507\n",
      "Gradient Descent(30/49): loss=0.35437487822076924\n",
      "Gradient Descent(31/49): loss=0.35417187961001073\n",
      "Gradient Descent(32/49): loss=0.35398510785878506\n",
      "Gradient Descent(33/49): loss=0.35381305329003954\n",
      "Gradient Descent(34/49): loss=0.3536543568611891\n",
      "Gradient Descent(35/49): loss=0.35350779441476177\n",
      "Gradient Descent(36/49): loss=0.3533722626683681\n",
      "Gradient Descent(37/49): loss=0.3532467667341369\n",
      "Gradient Descent(38/49): loss=0.3531304089870134\n",
      "Gradient Descent(39/49): loss=0.35302237912560663\n",
      "Gradient Descent(40/49): loss=0.35292194528962945\n",
      "Gradient Descent(41/49): loss=0.35282844611518055\n",
      "Gradient Descent(42/49): loss=0.35274128362377\n",
      "Gradient Descent(43/49): loss=0.3526599168535427\n",
      "Gradient Descent(44/49): loss=0.35258385615198673\n",
      "Gradient Descent(45/49): loss=0.3525126580587819\n",
      "Gradient Descent(46/49): loss=0.35244592071561465\n",
      "Gradient Descent(47/49): loss=0.3523832797469114\n",
      "Gradient Descent(48/49): loss=0.35232440456169434\n",
      "Gradient Descent(49/49): loss=0.352268995032258\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4031947647657801\n",
      "Gradient Descent(2/49): loss=0.389562218024102\n",
      "Gradient Descent(3/49): loss=0.38326061737249006\n",
      "Gradient Descent(4/49): loss=0.37903974746099117\n",
      "Gradient Descent(5/49): loss=0.37580734554815554\n",
      "Gradient Descent(6/49): loss=0.37315660521920424\n",
      "Gradient Descent(7/49): loss=0.37090085104741877\n",
      "Gradient Descent(8/49): loss=0.3689404782667954\n",
      "Gradient Descent(9/49): loss=0.3672149935153045\n",
      "Gradient Descent(10/49): loss=0.3656836832776462\n",
      "Gradient Descent(11/49): loss=0.3643169743399654\n",
      "Gradient Descent(12/49): loss=0.36309216597092836\n",
      "Gradient Descent(13/49): loss=0.36199112262871724\n",
      "Gradient Descent(14/49): loss=0.360998927302072\n",
      "Gradient Descent(15/49): loss=0.3601030439393901\n",
      "Gradient Descent(16/49): loss=0.35929276797646525\n",
      "Gradient Descent(17/49): loss=0.35855884894292284\n",
      "Gradient Descent(18/49): loss=0.35789322059185713\n",
      "Gradient Descent(19/49): loss=0.3572888008984803\n",
      "Gradient Descent(20/49): loss=0.3567393390957358\n",
      "Gradient Descent(21/49): loss=0.35623929543524435\n",
      "Gradient Descent(22/49): loss=0.3557837444316756\n",
      "Gradient Descent(23/49): loss=0.35536829545403226\n",
      "Gradient Descent(24/49): loss=0.3549890264788613\n",
      "Gradient Descent(25/49): loss=0.3546424280761586\n",
      "Gradient Descent(26/49): loss=0.35432535552525307\n",
      "Gradient Descent(27/49): loss=0.35403498751411094\n",
      "Gradient Descent(28/49): loss=0.3537687902579732\n",
      "Gradient Descent(29/49): loss=0.3535244861419858\n",
      "Gradient Descent(30/49): loss=0.353300026185366\n",
      "Gradient Descent(31/49): loss=0.35309356576595335\n",
      "Gradient Descent(32/49): loss=0.35290344314960453\n",
      "Gradient Descent(33/49): loss=0.35272816044933375\n",
      "Gradient Descent(34/49): loss=0.3525663667014734\n",
      "Gradient Descent(35/49): loss=0.3524168427953083\n",
      "Gradient Descent(36/49): loss=0.35227848803201467\n",
      "Gradient Descent(37/49): loss=0.3521503081207013\n",
      "Gradient Descent(38/49): loss=0.35203140444563796\n",
      "Gradient Descent(39/49): loss=0.3519209644606098\n",
      "Gradient Descent(40/49): loss=0.3518182530846987\n",
      "Gradient Descent(41/49): loss=0.3517226049893479\n",
      "Gradient Descent(42/49): loss=0.35163341767984696\n",
      "Gradient Descent(43/49): loss=0.35155014528580114\n",
      "Gradient Descent(44/49): loss=0.3514722929850229\n",
      "Gradient Descent(45/49): loss=0.35139941199386954\n",
      "Gradient Descent(46/49): loss=0.35133109506455007\n",
      "Gradient Descent(47/49): loss=0.3512669724364927\n",
      "Gradient Descent(48/49): loss=0.3512067081946432\n",
      "Gradient Descent(49/49): loss=0.35114999699266086\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4059199536475044\n",
      "Gradient Descent(2/49): loss=0.39232673913581667\n",
      "Gradient Descent(3/49): loss=0.3860208033457903\n",
      "Gradient Descent(4/49): loss=0.3818068702496927\n",
      "Gradient Descent(5/49): loss=0.3785876578390737\n",
      "Gradient Descent(6/49): loss=0.3759530716964077\n",
      "Gradient Descent(7/49): loss=0.3737145181842306\n",
      "Gradient Descent(8/49): loss=0.37177128100502876\n",
      "Gradient Descent(9/49): loss=0.37006224168211616\n",
      "Gradient Descent(10/49): loss=0.3685463454199037\n",
      "Gradient Descent(11/49): loss=0.3671938413571193\n",
      "Gradient Descent(12/49): loss=0.36598194739553824\n",
      "Gradient Descent(13/49): loss=0.3648925057518961\n",
      "Gradient Descent(14/49): loss=0.36391061524888585\n",
      "Gradient Descent(15/49): loss=0.36302378105712035\n",
      "Gradient Descent(16/49): loss=0.3622213569348214\n",
      "Gradient Descent(17/49): loss=0.3614941619920565\n",
      "Gradient Descent(18/49): loss=0.3608342064729996\n",
      "Gradient Descent(19/49): loss=0.36023448842860056\n",
      "Gradient Descent(20/49): loss=0.35968883819381803\n",
      "Gradient Descent(21/49): loss=0.3591917962023939\n",
      "Gradient Descent(22/49): loss=0.35873851478609614\n",
      "Gradient Descent(23/49): loss=0.35832467773227195\n",
      "Gradient Descent(24/49): loss=0.35794643333795967\n",
      "Gradient Descent(25/49): loss=0.3576003379645396\n",
      "Gradient Descent(26/49): loss=0.35728330793252183\n",
      "Gradient Descent(27/49): loss=0.35699257816089824\n",
      "Gradient Descent(28/49): loss=0.35672566634612596\n",
      "Gradient Descent(29/49): loss=0.35648034175202026\n",
      "Gradient Descent(30/49): loss=0.35625459788133196\n",
      "Gradient Descent(31/49): loss=0.3560466284467829\n",
      "Gradient Descent(32/49): loss=0.3558548061697356\n",
      "Gradient Descent(33/49): loss=0.35567766401905593\n",
      "Gradient Descent(34/49): loss=0.3555138785682919\n",
      "Gradient Descent(35/49): loss=0.3553622552010114\n",
      "Gradient Descent(36/49): loss=0.35522171493550914\n",
      "Gradient Descent(37/49): loss=0.35509128267360573\n",
      "Gradient Descent(38/49): loss=0.3549700767057184\n",
      "Gradient Descent(39/49): loss=0.3548572993271279\n",
      "Gradient Descent(40/49): loss=0.354752228439375\n",
      "Gradient Descent(41/49): loss=0.35465421002674774\n",
      "Gradient Descent(42/49): loss=0.3545626514114373\n",
      "Gradient Descent(43/49): loss=0.35447701520258673\n",
      "Gradient Descent(44/49): loss=0.3543968138644802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=0.3543216048377857\n",
      "Gradient Descent(46/49): loss=0.3542509861553072\n",
      "Gradient Descent(47/49): loss=0.3541845925002719\n",
      "Gradient Descent(48/49): loss=0.35412209166094566\n",
      "Gradient Descent(49/49): loss=0.35406318134042963\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4036869320116589\n",
      "Gradient Descent(2/49): loss=0.39094323476567044\n",
      "Gradient Descent(3/49): loss=0.3847799869895513\n",
      "Gradient Descent(4/49): loss=0.38057944133886706\n",
      "Gradient Descent(5/49): loss=0.3773315686795229\n",
      "Gradient Descent(6/49): loss=0.37465766333085077\n",
      "Gradient Descent(7/49): loss=0.3723823373737381\n",
      "Gradient Descent(8/49): loss=0.3704096991084281\n",
      "Gradient Descent(9/49): loss=0.3686798147668556\n",
      "Gradient Descent(10/49): loss=0.3671513270476084\n",
      "Gradient Descent(11/49): loss=0.3657936221288059\n",
      "Gradient Descent(12/49): loss=0.3645828881873947\n",
      "Gradient Descent(13/49): loss=0.3634999381746148\n",
      "Gradient Descent(14/49): loss=0.3625289119189759\n",
      "Gradient Descent(15/49): loss=0.361656452464957\n",
      "Gradient Descent(16/49): loss=0.36087115506114326\n",
      "Gradient Descent(17/49): loss=0.360163181360026\n",
      "Gradient Descent(18/49): loss=0.359523978331828\n",
      "Gradient Descent(19/49): loss=0.35894606626782727\n",
      "Gradient Descent(20/49): loss=0.35842287408345075\n",
      "Gradient Descent(21/49): loss=0.35794860813334867\n",
      "Gradient Descent(22/49): loss=0.3575181455334175\n",
      "Gradient Descent(23/49): loss=0.35712694592770733\n",
      "Gradient Descent(24/49): loss=0.35677097749859404\n",
      "Gradient Descent(25/49): loss=0.3564466542255899\n",
      "Gradient Descent(26/49): loss=0.3561507822013005\n",
      "Gradient Descent(27/49): loss=0.35588051336086945\n",
      "Gradient Descent(28/49): loss=0.35563330536411114\n",
      "Gradient Descent(29/49): loss=0.35540688664343256\n",
      "Gradient Descent(30/49): loss=0.35519922583103536\n",
      "Gradient Descent(31/49): loss=0.3550085049286219\n",
      "Gradient Descent(32/49): loss=0.3548330956969341\n",
      "Gradient Descent(33/49): loss=0.35467153883099545\n",
      "Gradient Descent(34/49): loss=0.354522525556774\n",
      "Gradient Descent(35/49): loss=0.35438488134092105\n",
      "Gradient Descent(36/49): loss=0.354257551450619\n",
      "Gradient Descent(37/49): loss=0.35413958813785207\n",
      "Gradient Descent(38/49): loss=0.3540301392533412\n",
      "Gradient Descent(39/49): loss=0.35392843812129265\n",
      "Gradient Descent(40/49): loss=0.35383379452798946\n",
      "Gradient Descent(41/49): loss=0.35374558669585976\n",
      "Gradient Descent(42/49): loss=0.3536632541305799\n",
      "Gradient Descent(43/49): loss=0.3535862912424707\n",
      "Gradient Descent(44/49): loss=0.35351424165528417\n",
      "Gradient Descent(45/49): loss=0.35344669312576354\n",
      "Gradient Descent(46/49): loss=0.3533832730063071\n",
      "Gradient Descent(47/49): loss=0.35332364419089785\n",
      "Gradient Descent(48/49): loss=0.35326750149130554\n",
      "Gradient Descent(49/49): loss=0.3532145683965993\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4033870557211976\n",
      "Gradient Descent(2/49): loss=0.39049865084763463\n",
      "Gradient Descent(3/49): loss=0.38416946018954756\n",
      "Gradient Descent(4/49): loss=0.3798620317397556\n",
      "Gradient Descent(5/49): loss=0.3765442743773954\n",
      "Gradient Descent(6/49): loss=0.37382087258680957\n",
      "Gradient Descent(7/49): loss=0.37150789733376854\n",
      "Gradient Descent(8/49): loss=0.36950514362624676\n",
      "Gradient Descent(9/49): loss=0.36775036212396206\n",
      "Gradient Descent(10/49): loss=0.36620084591562607\n",
      "Gradient Descent(11/49): loss=0.36482513106634845\n",
      "Gradient Descent(12/49): loss=0.36359883457185077\n",
      "Gradient Descent(13/49): loss=0.3625023649732566\n",
      "Gradient Descent(14/49): loss=0.3615195629177353\n",
      "Gradient Descent(15/49): loss=0.360636841467369\n",
      "Gradient Descent(16/49): loss=0.3598426129344514\n",
      "Gradient Descent(17/49): loss=0.35912688897601003\n",
      "Gradient Descent(18/49): loss=0.35848099029962166\n",
      "Gradient Descent(19/49): loss=0.35789732854177475\n",
      "Gradient Descent(20/49): loss=0.35736923743254634\n",
      "Gradient Descent(21/49): loss=0.3568908387727296\n",
      "Gradient Descent(22/49): loss=0.35645693377859716\n",
      "Gradient Descent(23/49): loss=0.35606291344557156\n",
      "Gradient Descent(24/49): loss=0.3557046835401661\n",
      "Gradient Descent(25/49): loss=0.35537860109997393\n",
      "Gradient Descent(26/49): loss=0.35508142016637145\n",
      "Gradient Descent(27/49): loss=0.3548102450501834\n",
      "Gradient Descent(28/49): loss=0.3545624898319894\n",
      "Gradient Descent(29/49): loss=0.35433584308512683\n",
      "Gradient Descent(30/49): loss=0.3541282370182348\n",
      "Gradient Descent(31/49): loss=0.35393782038956945\n",
      "Gradient Descent(32/49): loss=0.35376293466320696\n",
      "Gradient Descent(33/49): loss=0.35360209296832684\n",
      "Gradient Descent(34/49): loss=0.3534539614942798\n",
      "Gradient Descent(35/49): loss=0.35331734301116274\n",
      "Gradient Descent(36/49): loss=0.35319116225170927\n",
      "Gradient Descent(37/49): loss=0.353074452928006\n",
      "Gradient Descent(38/49): loss=0.3529663461877346\n",
      "Gradient Descent(39/49): loss=0.3528660603406973\n",
      "Gradient Descent(40/49): loss=0.3527728917083296\n",
      "Gradient Descent(41/49): loss=0.3526862064675438\n",
      "Gradient Descent(42/49): loss=0.3526054333761717\n",
      "Gradient Descent(43/49): loss=0.3525300572809642\n",
      "Gradient Descent(44/49): loss=0.3524596133209325\n",
      "Gradient Descent(45/49): loss=0.35239368174907737\n",
      "Gradient Descent(46/49): loss=0.35233188330449605\n",
      "Gradient Descent(47/49): loss=0.35227387507466285\n",
      "Gradient Descent(48/49): loss=0.35221934679453215\n",
      "Gradient Descent(49/49): loss=0.35216801753512544\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40160371061925565\n",
      "Gradient Descent(2/49): loss=0.38880061995026793\n",
      "Gradient Descent(3/49): loss=0.3826188935320891\n",
      "Gradient Descent(4/49): loss=0.3784329177500997\n",
      "Gradient Descent(5/49): loss=0.3752101226049958\n",
      "Gradient Descent(6/49): loss=0.37256044071942257\n",
      "Gradient Descent(7/49): loss=0.3703046626933456\n",
      "Gradient Descent(8/49): loss=0.3683462145782054\n",
      "Gradient Descent(9/49): loss=0.3666255898046223\n",
      "Gradient Descent(10/49): loss=0.3651021786958936\n",
      "Gradient Descent(11/49): loss=0.3637461480323239\n",
      "Gradient Descent(12/49): loss=0.3625344009608014\n",
      "Gradient Descent(13/49): loss=0.3614483712650826\n",
      "Gradient Descent(14/49): loss=0.36047272308904377\n",
      "Gradient Descent(15/49): loss=0.35959453448109546\n",
      "Gradient Descent(16/49): loss=0.35880275671805373\n",
      "Gradient Descent(17/49): loss=0.3580878393463338\n",
      "Gradient Descent(18/49): loss=0.35744145935720234\n",
      "Gradient Descent(19/49): loss=0.3568563184463685\n",
      "Gradient Descent(20/49): loss=0.3563259864437689\n",
      "Gradient Descent(21/49): loss=0.3558447771426778\n",
      "Gradient Descent(22/49): loss=0.355407647605053\n",
      "Gradient Descent(23/49): loss=0.35501011498935686\n",
      "Gradient Descent(24/49): loss=0.3546481868137713\n",
      "Gradient Descent(25/49): loss=0.3543183017706105\n",
      "Gradient Descent(26/49): loss=0.3540172790017849\n",
      "Gradient Descent(27/49): loss=0.3537422742819353\n",
      "Gradient Descent(28/49): loss=0.3534907419273267\n",
      "Gradient Descent(29/49): loss=0.3532604015116361\n",
      "Gradient Descent(30/49): loss=0.35304920866027045\n",
      "Gradient Descent(31/49): loss=0.3528553293358157\n",
      "Gradient Descent(32/49): loss=0.352677117133679\n",
      "Gradient Descent(33/49): loss=0.3525130931889552\n",
      "Gradient Descent(34/49): loss=0.3523619283597902\n",
      "Gradient Descent(35/49): loss=0.3522224274036817\n",
      "Gradient Descent(36/49): loss=0.3520935149045322\n",
      "Gradient Descent(37/49): loss=0.35197422274214646\n",
      "Gradient Descent(38/49): loss=0.3518636789239581\n",
      "Gradient Descent(39/49): loss=0.35176109762228075\n",
      "Gradient Descent(40/49): loss=0.3516657702802527\n",
      "Gradient Descent(41/49): loss=0.3515770576665675\n",
      "Gradient Descent(42/49): loss=0.35149438277359285\n",
      "Gradient Descent(43/49): loss=0.35141722446599516\n",
      "Gradient Descent(44/49): loss=0.35134511179783995\n",
      "Gradient Descent(45/49): loss=0.35127761892558057\n",
      "Gradient Descent(46/49): loss=0.35121436055260924\n",
      "Gradient Descent(47/49): loss=0.3511549878482781\n",
      "Gradient Descent(48/49): loss=0.3510991847906658\n",
      "Gradient Descent(49/49): loss=0.35104666488797315\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40435106246187363\n",
      "Gradient Descent(2/49): loss=0.39156217728616355\n",
      "Gradient Descent(3/49): loss=0.3853780424061826\n",
      "Gradient Descent(4/49): loss=0.381201189173588\n",
      "Gradient Descent(5/49): loss=0.37799321376068296\n",
      "Gradient Descent(6/49): loss=0.37536088589337346\n",
      "Gradient Descent(7/49): loss=0.3731231649280741\n",
      "Gradient Descent(8/49): loss=0.3711824348992921\n",
      "Gradient Descent(9/49): loss=0.3694786180446933\n",
      "Gradient Descent(10/49): loss=0.367970802971422\n",
      "Gradient Descent(11/49): loss=0.3666290082192795\n",
      "Gradient Descent(12/49): loss=0.3654300789497147\n",
      "Gradient Descent(13/49): loss=0.3643554468051254\n",
      "Gradient Descent(14/49): loss=0.36338981000312065\n",
      "Gradient Descent(15/49): loss=0.36252030474618313\n",
      "Gradient Descent(16/49): loss=0.3617359563519945\n",
      "Gradient Descent(17/49): loss=0.36102729836072833\n",
      "Gradient Descent(18/49): loss=0.36038609723662457\n",
      "Gradient Descent(19/49): loss=0.3598051462119568\n",
      "Gradient Descent(20/49): loss=0.3592781061295025\n",
      "Gradient Descent(21/49): loss=0.3587993793565922\n",
      "Gradient Descent(22/49): loss=0.35836400772486665\n",
      "Gradient Descent(23/49): loss=0.35796758843756804\n",
      "Gradient Descent(24/49): loss=0.35760620376653385\n",
      "Gradient Descent(25/49): loss=0.35727636157641296\n",
      "Gradient Descent(26/49): loss=0.35697494451966394\n",
      "Gradient Descent(27/49): loss=0.35669916629403575\n",
      "Gradient Descent(28/49): loss=0.3564465337360882\n",
      "Gradient Descent(29/49): loss=0.3562148137965354\n",
      "Gradient Descent(30/49): loss=0.3560020046415409\n",
      "Gradient Descent(31/49): loss=0.35580631027158566\n",
      "Gradient Descent(32/49): loss=0.35562611816132367\n",
      "Gradient Descent(33/49): loss=0.35545997951005026\n",
      "Gradient Descent(34/49): loss=0.35530659175999435\n",
      "Gradient Descent(35/49): loss=0.3551647830933995\n",
      "Gradient Descent(36/49): loss=0.35503349866269013\n",
      "Gradient Descent(37/49): loss=0.35491178834338477\n",
      "Gradient Descent(38/49): loss=0.3547987958285835\n",
      "Gradient Descent(39/49): loss=0.3546937489081652\n",
      "Gradient Descent(40/49): loss=0.3545959507962473\n",
      "Gradient Descent(41/49): loss=0.3545047723877681\n",
      "Gradient Descent(42/49): loss=0.3544196453397996\n",
      "Gradient Descent(43/49): loss=0.3543400558858649\n",
      "Gradient Descent(44/49): loss=0.35426553930245486\n",
      "Gradient Descent(45/49): loss=0.35419567495640536\n",
      "Gradient Descent(46/49): loss=0.35413008187004197\n",
      "Gradient Descent(47/49): loss=0.35406841474819034\n",
      "Gradient Descent(48/49): loss=0.35401036041746486\n",
      "Gradient Descent(49/49): loss=0.35395563463378366\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40228314614262983\n",
      "Gradient Descent(2/49): loss=0.3902467004567835\n",
      "Gradient Descent(3/49): loss=0.38417505029537447\n",
      "Gradient Descent(4/49): loss=0.3799983192182211\n",
      "Gradient Descent(5/49): loss=0.37675433430504773\n",
      "Gradient Descent(6/49): loss=0.3740795264077978\n",
      "Gradient Descent(7/49): loss=0.37180448268151306\n",
      "Gradient Descent(8/49): loss=0.36983527310672903\n",
      "Gradient Descent(9/49): loss=0.3681123079965468\n",
      "Gradient Descent(10/49): loss=0.3665939636344117\n",
      "Gradient Descent(11/49): loss=0.3652491511888172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=0.36405353622176956\n",
      "Gradient Descent(13/49): loss=0.36298742751247093\n",
      "Gradient Descent(14/49): loss=0.3620345062253216\n",
      "Gradient Descent(15/49): loss=0.3611810121025898\n",
      "Gradient Descent(16/49): loss=0.3604151941105001\n",
      "Gradient Descent(17/49): loss=0.3597269222169767\n",
      "Gradient Descent(18/49): loss=0.35910740189036844\n",
      "Gradient Descent(19/49): loss=0.3585489568393953\n",
      "Gradient Descent(20/49): loss=0.3580448588555092\n",
      "Gradient Descent(21/49): loss=0.35758919133721934\n",
      "Gradient Descent(22/49): loss=0.35717673768782876\n",
      "Gradient Descent(23/49): loss=0.3568028886167482\n",
      "Gradient Descent(24/49): loss=0.356463564172168\n",
      "Gradient Descent(25/49): loss=0.35615514750316546\n",
      "Gradient Descent(26/49): loss=0.3558744281322116\n",
      "Gradient Descent(27/49): loss=0.3556185530568342\n",
      "Gradient Descent(28/49): loss=0.3553849843781907\n",
      "Gradient Descent(29/49): loss=0.35517146242799297\n",
      "Gradient Descent(30/49): loss=0.35497597356744554\n",
      "Gradient Descent(31/49): loss=0.354796721984482\n",
      "Gradient Descent(32/49): loss=0.3546321049330403\n",
      "Gradient Descent(33/49): loss=0.35448069095011653\n",
      "Gradient Descent(34/49): loss=0.3543412006595554\n",
      "Gradient Descent(35/49): loss=0.35421248983062287\n",
      "Gradient Descent(36/49): loss=0.3540935344077013\n",
      "Gradient Descent(37/49): loss=0.353983417267348\n",
      "Gradient Descent(38/49): loss=0.353881316492247\n",
      "Gradient Descent(39/49): loss=0.35378649497958586\n",
      "Gradient Descent(40/49): loss=0.3536982912251212\n",
      "Gradient Descent(41/49): loss=0.35361611114443164\n",
      "Gradient Descent(42/49): loss=0.3535394208102108\n",
      "Gradient Descent(43/49): loss=0.3534677399993977\n",
      "Gradient Descent(44/49): loss=0.3534006364568792\n",
      "Gradient Descent(45/49): loss=0.35333772079371917\n",
      "Gradient Descent(46/49): loss=0.3532786419476457\n",
      "Gradient Descent(47/49): loss=0.3532230831420624\n",
      "Gradient Descent(48/49): loss=0.35317075828731737\n",
      "Gradient Descent(49/49): loss=0.353121408774506\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40199067975729647\n",
      "Gradient Descent(2/49): loss=0.38978564552124517\n",
      "Gradient Descent(3/49): loss=0.3835463561621254\n",
      "Gradient Descent(4/49): loss=0.37926592241215323\n",
      "Gradient Descent(5/49): loss=0.37595473498869125\n",
      "Gradient Descent(6/49): loss=0.37323218535616765\n",
      "Gradient Descent(7/49): loss=0.37092062401567427\n",
      "Gradient Descent(8/49): loss=0.3689220781008823\n",
      "Gradient Descent(9/49): loss=0.36717480462330715\n",
      "Gradient Descent(10/49): loss=0.36563591979691407\n",
      "Gradient Descent(11/49): loss=0.36427353397866263\n",
      "Gradient Descent(12/49): loss=0.3630627691979703\n",
      "Gradient Descent(13/49): loss=0.36198354541439753\n",
      "Gradient Descent(14/49): loss=0.3610192531092525\n",
      "Gradient Descent(15/49): loss=0.3601559061585677\n",
      "Gradient Descent(16/49): loss=0.35938157191910797\n",
      "Gradient Descent(17/49): loss=0.3586859699068716\n",
      "Gradient Descent(18/49): loss=0.3580601777595583\n",
      "Gradient Descent(19/49): loss=0.35749640831588425\n",
      "Gradient Descent(20/49): loss=0.3569878356454298\n",
      "Gradient Descent(21/49): loss=0.3565284559635492\n",
      "Gradient Descent(22/49): loss=0.35611297420960464\n",
      "Gradient Descent(23/49): loss=0.3557367100504752\n",
      "Gradient Descent(24/49): loss=0.3553955189615229\n",
      "Gradient Descent(25/49): loss=0.3550857252675656\n",
      "Gradient Descent(26/49): loss=0.3548040648486638\n",
      "Gradient Descent(27/49): loss=0.35454763577928594\n",
      "Gradient Descent(28/49): loss=0.35431385556563477\n",
      "Gradient Descent(29/49): loss=0.3541004239310131\n",
      "Gradient Descent(30/49): loss=0.35390529030889933\n",
      "Gradient Descent(31/49): loss=0.3537266253610161\n",
      "Gradient Descent(32/49): loss=0.3535627959584042\n",
      "Gradient Descent(33/49): loss=0.3534123431576359\n",
      "Gradient Descent(34/49): loss=0.35327396277886886\n",
      "Gradient Descent(35/49): loss=0.3531464882523769\n",
      "Gradient Descent(36/49): loss=0.3530288754489956\n",
      "Gradient Descent(37/49): loss=0.3529201892501135\n",
      "Gradient Descent(38/49): loss=0.3528195916462823\n",
      "Gradient Descent(39/49): loss=0.35272633118159213\n",
      "Gradient Descent(40/49): loss=0.35263973358470885\n",
      "Gradient Descent(41/49): loss=0.35255919344769443\n",
      "Gradient Descent(42/49): loss=0.35248416683106826\n",
      "Gradient Descent(43/49): loss=0.3524141646884876\n",
      "Gradient Descent(44/49): loss=0.3523487470173339\n",
      "Gradient Descent(45/49): loss=0.35228751765269994\n",
      "Gradient Descent(46/49): loss=0.3522301196320346\n",
      "Gradient Descent(47/49): loss=0.35217623106622736\n",
      "Gradient Descent(48/49): loss=0.3521255614603832\n",
      "Gradient Descent(49/49): loss=0.3520778484340884\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4001881299544884\n",
      "Gradient Descent(2/49): loss=0.38809794395556596\n",
      "Gradient Descent(3/49): loss=0.38201163711518094\n",
      "Gradient Descent(4/49): loss=0.37785382426424186\n",
      "Gradient Descent(5/49): loss=0.37463716914377326\n",
      "Gradient Descent(6/49): loss=0.37198717720380486\n",
      "Gradient Descent(7/49): loss=0.3697313034398105\n",
      "Gradient Descent(8/49): loss=0.36777539910237855\n",
      "Gradient Descent(9/49): loss=0.36606057435284756\n",
      "Gradient Descent(10/49): loss=0.3645461119702774\n",
      "Gradient Descent(11/49): loss=0.36320179804786973\n",
      "Gradient Descent(12/49): loss=0.36200406946285124\n",
      "Gradient Descent(13/49): loss=0.3609338885017858\n",
      "Gradient Descent(14/49): loss=0.35997547784490863\n",
      "Gradient Descent(15/49): loss=0.35911551904642613\n",
      "Gradient Descent(16/49): loss=0.358342616887719\n",
      "Gradient Descent(17/49): loss=0.3576469243372615\n",
      "Gradient Descent(18/49): loss=0.35701986897723575\n",
      "Gradient Descent(19/49): loss=0.35645394619808657\n",
      "Gradient Descent(20/49): loss=0.3559425580289332\n",
      "Gradient Descent(21/49): loss=0.3554798842899256\n",
      "Gradient Descent(22/49): loss=0.3550607774041051\n",
      "Gradient Descent(23/49): loss=0.35468067505443746\n",
      "Gradient Descent(24/49): loss=0.35433552666354656\n",
      "Gradient Descent(25/49): loss=0.35402173083109983\n",
      "Gradient Descent(26/49): loss=0.3537360816310742\n",
      "Gradient Descent(27/49): loss=0.3534757221929468\n",
      "Gradient Descent(28/49): loss=0.3532381043547492\n",
      "Gradient Descent(29/49): loss=0.35302095343594825\n",
      "Gradient Descent(30/49): loss=0.3528222373683239\n",
      "Gradient Descent(31/49): loss=0.35264013956524126\n",
      "Gradient Descent(32/49): loss=0.3524730350183117\n",
      "Gradient Descent(33/49): loss=0.3523194691949294\n",
      "Gradient Descent(34/49): loss=0.3521781393770743\n",
      "Gradient Descent(35/49): loss=0.35204787813556465\n",
      "Gradient Descent(36/49): loss=0.3519276386778166\n",
      "Gradient Descent(37/49): loss=0.3518164818433726\n",
      "Gradient Descent(38/49): loss=0.35171356455167413\n",
      "Gradient Descent(39/49): loss=0.3516181295319925\n",
      "Gradient Descent(40/49): loss=0.3515294961870261\n",
      "Gradient Descent(41/49): loss=0.35144705246013397\n",
      "Gradient Descent(42/49): loss=0.3513702475920416\n",
      "Gradient Descent(43/49): loss=0.3512985856665777\n",
      "Gradient Descent(44/49): loss=0.35123161985690005\n",
      "Gradient Descent(45/49): loss=0.3511689472940427\n",
      "Gradient Descent(46/49): loss=0.35111020448867986\n",
      "Gradient Descent(47/49): loss=0.35105506324494423\n",
      "Gradient Descent(48/49): loss=0.3510032270121129\n",
      "Gradient Descent(49/49): loss=0.3509544276261139\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4029509644045868\n",
      "Gradient Descent(2/49): loss=0.3908554611334504\n",
      "Gradient Descent(3/49): loss=0.38476939579329256\n",
      "Gradient Descent(4/49): loss=0.3806230733152894\n",
      "Gradient Descent(5/49): loss=0.3774229814410219\n",
      "Gradient Descent(6/49): loss=0.3747915793152505\n",
      "Gradient Descent(7/49): loss=0.37255460278823793\n",
      "Gradient Descent(8/49): loss=0.3706169571029826\n",
      "Gradient Descent(9/49): loss=0.3689192507074716\n",
      "Gradient Descent(10/49): loss=0.36742051170255685\n",
      "Gradient Descent(11/49): loss=0.36609041085831917\n",
      "Gradient Descent(12/49): loss=0.3649053523807409\n",
      "Gradient Descent(13/49): loss=0.36384631786931054\n",
      "Gradient Descent(14/49): loss=0.3628975833976556\n",
      "Gradient Descent(15/49): loss=0.3620459064512387\n",
      "Gradient Descent(16/49): loss=0.3612799821568304\n",
      "Gradient Descent(17/49): loss=0.3605900622015169\n",
      "Gradient Descent(18/49): loss=0.3599676766704102\n",
      "Gradient Descent(19/49): loss=0.3594054237690843\n",
      "Gradient Descent(20/49): loss=0.3588968060832561\n",
      "Gradient Descent(21/49): loss=0.3584360998962358\n",
      "Gradient Descent(22/49): loss=0.35801824876116745\n",
      "Gradient Descent(23/49): loss=0.3576387753910575\n",
      "Gradient Descent(24/49): loss=0.3572937077379671\n",
      "Gradient Descent(25/49): loss=0.35697951630645236\n",
      "Gradient Descent(26/49): loss=0.3566930605292774\n",
      "Gradient Descent(27/49): loss=0.35643154256964926\n",
      "Gradient Descent(28/49): loss=0.3561924672908356\n",
      "Gradient Descent(29/49): loss=0.3559736074048716\n",
      "Gradient Descent(30/49): loss=0.35577297301120614\n",
      "Gradient Descent(31/49): loss=0.35558878488558543\n",
      "Gradient Descent(32/49): loss=0.35541945099376304\n",
      "Gradient Descent(33/49): loss=0.35526354579353997\n",
      "Gradient Descent(34/49): loss=0.35511979195891347\n",
      "Gradient Descent(35/49): loss=0.354987044216435\n",
      "Gradient Descent(36/49): loss=0.35486427502962004\n",
      "Gradient Descent(37/49): loss=0.354750561904803\n",
      "Gradient Descent(38/49): loss=0.3546450761230024\n",
      "Gradient Descent(39/49): loss=0.3545470727284501\n",
      "Gradient Descent(40/49): loss=0.3544558816264693\n",
      "Gradient Descent(41/49): loss=0.3543708996621041\n",
      "Gradient Descent(42/49): loss=0.3542915835669234\n",
      "Gradient Descent(43/49): loss=0.35421744367519215\n",
      "Gradient Descent(44/49): loss=0.3541480383225058\n",
      "Gradient Descent(45/49): loss=0.3540829688503118\n",
      "Gradient Descent(46/49): loss=0.35402187514872974\n",
      "Gradient Descent(47/49): loss=0.35396443167794034\n",
      "Gradient Descent(48/49): loss=0.35391034391528786\n",
      "Gradient Descent(49/49): loss=0.3538593451812793\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40104977234024913\n",
      "Gradient Descent(2/49): loss=0.3895994892569756\n",
      "Gradient Descent(3/49): loss=0.38360205175673584\n",
      "Gradient Descent(4/49): loss=0.3794437618011307\n",
      "Gradient Descent(5/49): loss=0.37620080959845836\n",
      "Gradient Descent(6/49): loss=0.37352398323324437\n",
      "Gradient Descent(7/49): loss=0.3712491606304512\n",
      "Gradient Descent(8/49): loss=0.3692838901504618\n",
      "Gradient Descent(9/49): loss=0.3675686291208004\n",
      "Gradient Descent(10/49): loss=0.36606130686010463\n",
      "Gradient Descent(11/49): loss=0.3647302482118274\n",
      "Gradient Descent(12/49): loss=0.3635505277496056\n",
      "Gradient Descent(13/49): loss=0.36250190983073527\n",
      "Gradient Descent(14/49): loss=0.3615675957644949\n",
      "Gradient Descent(15/49): loss=0.36073341384697777\n",
      "Gradient Descent(16/49): loss=0.3599872674716794\n",
      "Gradient Descent(17/49): loss=0.3593187414955545\n",
      "Gradient Descent(18/49): loss=0.35871881018137325\n",
      "Gradient Descent(19/49): loss=0.3581796131462443\n",
      "Gradient Descent(20/49): loss=0.35769427866096737\n",
      "Gradient Descent(21/49): loss=0.3572567811249327\n",
      "Gradient Descent(22/49): loss=0.35686182401528505\n",
      "Gradient Descent(23/49): loss=0.35650474236800017\n",
      "Gradient Descent(24/49): loss=0.3561814206012667\n",
      "Gradient Descent(25/49): loss=0.35588822263817493\n",
      "Gradient Descent(26/49): loss=0.35562193205766585\n",
      "Gradient Descent(27/49): loss=0.3553797005370525\n",
      "Gradient Descent(28/49): loss=0.35515900322928734\n",
      "Gradient Descent(29/49): loss=0.3549575999950012\n",
      "Gradient Descent(30/49): loss=0.35477350161583654\n",
      "Gradient Descent(31/49): loss=0.35460494027291395\n",
      "Gradient Descent(32/49): loss=0.35445034369641937\n",
      "Gradient Descent(33/49): loss=0.35430831248877714\n",
      "Gradient Descent(34/49): loss=0.3541776002012325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=0.3540570958065197\n",
      "Gradient Descent(36/49): loss=0.3539458082619642\n",
      "Gradient Descent(37/49): loss=0.35384285290026357\n",
      "Gradient Descent(38/49): loss=0.35374743942112585\n",
      "Gradient Descent(39/49): loss=0.3536588612872714\n",
      "Gradient Descent(40/49): loss=0.3535764863540624\n",
      "Gradient Descent(41/49): loss=0.3534997485840236\n",
      "Gradient Descent(42/49): loss=0.35342814071641193\n",
      "Gradient Descent(43/49): loss=0.3533612077782575\n",
      "Gradient Descent(44/49): loss=0.35329854133738575\n",
      "Gradient Descent(45/49): loss=0.3532397744101338\n",
      "Gradient Descent(46/49): loss=0.35318457694710476\n",
      "Gradient Descent(47/49): loss=0.3531326518295546\n",
      "Gradient Descent(48/49): loss=0.3530837313171041\n",
      "Gradient Descent(49/49): loss=0.3530375738945331\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.400766119090311\n",
      "Gradient Descent(2/49): loss=0.38912096563784865\n",
      "Gradient Descent(3/49): loss=0.38295540189424226\n",
      "Gradient Descent(4/49): loss=0.37869667165305276\n",
      "Gradient Descent(5/49): loss=0.3753890995322272\n",
      "Gradient Descent(6/49): loss=0.37266624430733364\n",
      "Gradient Descent(7/49): loss=0.3703560563534507\n",
      "Gradient Descent(8/49): loss=0.36836227634477725\n",
      "Gradient Descent(9/49): loss=0.36662334596547347\n",
      "Gradient Descent(10/49): loss=0.36509601373254713\n",
      "Gradient Descent(11/49): loss=0.3637478510422945\n",
      "Gradient Descent(12/49): loss=0.3625534179852341\n",
      "Gradient Descent(13/49): loss=0.361492107788778\n",
      "Gradient Descent(14/49): loss=0.3605468413300667\n",
      "Gradient Descent(15/49): loss=0.35970322631568497\n",
      "Gradient Descent(16/49): loss=0.3589489864988319\n",
      "Gradient Descent(17/49): loss=0.35827355611164824\n",
      "Gradient Descent(18/49): loss=0.35766778009308225\n",
      "Gradient Descent(19/49): loss=0.35712368495317776\n",
      "Gradient Descent(20/49): loss=0.3566342986589923\n",
      "Gradient Descent(21/49): loss=0.35619350577093023\n",
      "Gradient Descent(22/49): loss=0.3557959287508414\n",
      "Gradient Descent(23/49): loss=0.3554368292574714\n",
      "Gradient Descent(24/49): loss=0.35511202508312795\n",
      "Gradient Descent(25/49): loss=0.3548178195869532\n",
      "Gradient Descent(26/49): loss=0.3545509412877415\n",
      "Gradient Descent(27/49): loss=0.35430849183684393\n",
      "Gradient Descent(28/49): loss=0.35408790098676685\n",
      "Gradient Descent(29/49): loss=0.35388688745786645\n",
      "Gradient Descent(30/49): loss=0.3537034248185172\n",
      "Gradient Descent(31/49): loss=0.35353571165562825\n",
      "Gradient Descent(32/49): loss=0.3533821454371972\n",
      "Gradient Descent(33/49): loss=0.3532412995667416\n",
      "Gradient Descent(34/49): loss=0.3531119032078189\n",
      "Gradient Descent(35/49): loss=0.3529928235202992\n",
      "Gradient Descent(36/49): loss=0.3528830500020497\n",
      "Gradient Descent(37/49): loss=0.352781680672747\n",
      "Gradient Descent(38/49): loss=0.3526879098725326\n",
      "Gradient Descent(39/49): loss=0.35260101747855177\n",
      "Gradient Descent(40/49): loss=0.35252035936815806\n",
      "Gradient Descent(41/49): loss=0.35244535897952584\n",
      "Gradient Descent(42/49): loss=0.3523754998392742\n",
      "Gradient Descent(43/49): loss=0.3523103189429391\n",
      "Gradient Descent(44/49): loss=0.35224940088819723\n",
      "Gradient Descent(45/49): loss=0.3521923726729328\n",
      "Gradient Descent(46/49): loss=0.3521388990808658\n",
      "Gradient Descent(47/49): loss=0.3520886785867141\n",
      "Gradient Descent(48/49): loss=0.35204143972096563\n",
      "Gradient Descent(49/49): loss=0.351996937841427\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39894802277147845\n",
      "Gradient Descent(2/49): loss=0.38744406499708994\n",
      "Gradient Descent(3/49): loss=0.3814360180068805\n",
      "Gradient Descent(4/49): loss=0.37730079192071486\n",
      "Gradient Descent(5/49): loss=0.37408720439031207\n",
      "Gradient Descent(6/49): loss=0.3714356966502091\n",
      "Gradient Descent(7/49): loss=0.3691796695730717\n",
      "Gradient Descent(8/49): loss=0.3672268673413754\n",
      "Gradient Descent(9/49): loss=0.36551869170476387\n",
      "Gradient Descent(10/49): loss=0.36401412622393997\n",
      "Gradient Descent(11/49): loss=0.3626824621328612\n",
      "Gradient Descent(12/49): loss=0.3614996015605792\n",
      "Gradient Descent(13/49): loss=0.3604459952163901\n",
      "Gradient Descent(14/49): loss=0.35950540279542054\n",
      "Gradient Descent(15/49): loss=0.35866410034624685\n",
      "Gradient Descent(16/49): loss=0.3579103456588455\n",
      "Gradient Descent(17/49): loss=0.35723400037216474\n",
      "Gradient Descent(18/49): loss=0.356626251676963\n",
      "Gradient Descent(19/49): loss=0.3560794000206562\n",
      "Gradient Descent(20/49): loss=0.3555866923079895\n",
      "Gradient Descent(21/49): loss=0.3551421876339643\n",
      "Gradient Descent(22/49): loss=0.35474064707152136\n",
      "Gradient Descent(23/49): loss=0.3543774417842592\n",
      "Gradient Descent(24/49): loss=0.3540484754661752\n",
      "Gradient Descent(25/49): loss=0.35375011823289676\n",
      "Gradient Descent(26/49): loss=0.35347914983693435\n",
      "Gradient Descent(27/49): loss=0.3532327105918517\n",
      "Gradient Descent(28/49): loss=0.35300825875058045\n",
      "Gradient Descent(29/49): loss=0.3528035333430853\n",
      "Gradient Descent(30/49): loss=0.3526165216706972\n",
      "Gradient Descent(31/49): loss=0.3524454307996261\n",
      "Gradient Descent(32/49): loss=0.3522886625081715\n",
      "Gradient Descent(33/49): loss=0.3521447912301723\n",
      "Gradient Descent(34/49): loss=0.35201254460758813\n",
      "Gradient Descent(35/49): loss=0.35189078632214177\n",
      "Gradient Descent(36/49): loss=0.3517785009228148\n",
      "Gradient Descent(37/49): loss=0.3516747804049145\n",
      "Gradient Descent(38/49): loss=0.3515788123290717\n",
      "Gradient Descent(39/49): loss=0.3514898692961443\n",
      "Gradient Descent(40/49): loss=0.3514072996175129\n",
      "Gradient Descent(41/49): loss=0.351330519040399\n",
      "Gradient Descent(42/49): loss=0.3512590034051941\n",
      "Gradient Descent(43/49): loss=0.3511922821267883\n",
      "Gradient Descent(44/49): loss=0.3511299324049227\n",
      "Gradient Descent(45/49): loss=0.3510715740799305\n",
      "Gradient Descent(46/49): loss=0.35101686506014484\n",
      "Gradient Descent(47/49): loss=0.35096549725591625\n",
      "Gradient Descent(48/49): loss=0.3509171929627906\n",
      "Gradient Descent(49/49): loss=0.35087170164307163\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4017196594756436\n",
      "Gradient Descent(2/49): loss=0.3901966168565663\n",
      "Gradient Descent(3/49): loss=0.3841917568117129\n",
      "Gradient Descent(4/49): loss=0.38007053699649845\n",
      "Gradient Descent(5/49): loss=0.37687542238890404\n",
      "Gradient Descent(6/49): loss=0.3742438478185172\n",
      "Gradient Descent(7/49): loss=0.37200760558171786\n",
      "Gradient Descent(8/49): loss=0.3700736076886787\n",
      "Gradient Descent(9/49): loss=0.36838284004543037\n",
      "Gradient Descent(10/49): loss=0.36689409022703906\n",
      "Gradient Descent(11/49): loss=0.36557657485177375\n",
      "Gradient Descent(12/49): loss=0.3644061938554296\n",
      "Gradient Descent(13/49): loss=0.3633634421462874\n",
      "Gradient Descent(14/49): loss=0.3624321543907213\n",
      "Gradient Descent(15/49): loss=0.36159870176871617\n",
      "Gradient Descent(16/49): loss=0.3608514495216216\n",
      "Gradient Descent(17/49): loss=0.3601803730602666\n",
      "Gradient Descent(18/49): loss=0.35957677507810676\n",
      "Gradient Descent(19/49): loss=0.35903306982178723\n",
      "Gradient Descent(20/49): loss=0.35854261381701336\n",
      "Gradient Descent(21/49): loss=0.35809956991266956\n",
      "Gradient Descent(22/49): loss=0.3576987960080898\n",
      "Gradient Descent(23/49): loss=0.3573357525937144\n",
      "Gradient Descent(24/49): loss=0.3570064249868442\n",
      "Gradient Descent(25/49): loss=0.356707257287064\n",
      "Gradient Descent(26/49): loss=0.35643509584339655\n",
      "Gradient Descent(27/49): loss=0.356187140554986\n",
      "Gradient Descent(28/49): loss=0.35596090270230357\n",
      "Gradient Descent(29/49): loss=0.3557541682780921\n",
      "Gradient Descent(30/49): loss=0.3555649659891937\n",
      "Gradient Descent(31/49): loss=0.355391539253285\n",
      "Gradient Descent(32/49): loss=0.35523232163244456\n",
      "Gradient Descent(33/49): loss=0.3550859152379499\n",
      "Gradient Descent(34/49): loss=0.3549510717143295\n",
      "Gradient Descent(35/49): loss=0.3548266754701265\n",
      "Gradient Descent(36/49): loss=0.3547117288713893\n",
      "Gradient Descent(37/49): loss=0.35460533915399806\n",
      "Gradient Descent(38/49): loss=0.35450670684437213\n",
      "Gradient Descent(39/49): loss=0.3544151155062107\n",
      "Gradient Descent(40/49): loss=0.3543299226547165\n",
      "Gradient Descent(41/49): loss=0.35425055170004943\n",
      "Gradient Descent(42/49): loss=0.3541764847991342\n",
      "Gradient Descent(43/49): loss=0.3541072565099249\n",
      "Gradient Descent(44/49): loss=0.35404244815517216\n",
      "Gradient Descent(45/49): loss=0.3539816828139748\n",
      "Gradient Descent(46/49): loss=0.35392462086917514\n",
      "Gradient Descent(47/49): loss=0.3538709560471925\n",
      "Gradient Descent(48/49): loss=0.3538204118943542\n",
      "Gradient Descent(49/49): loss=0.35377273864032366\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3999868106045168\n",
      "Gradient Descent(2/49): loss=0.3889937013750459\n",
      "Gradient Descent(3/49): loss=0.383059473332975\n",
      "Gradient Descent(4/49): loss=0.3789156226585007\n",
      "Gradient Descent(5/49): loss=0.37567153693740557\n",
      "Gradient Descent(6/49): loss=0.37299178106799696\n",
      "Gradient Descent(7/49): loss=0.370716987899237\n",
      "Gradient Descent(8/49): loss=0.36875586451507275\n",
      "Gradient Descent(9/49): loss=0.3670487314673521\n",
      "Gradient Descent(10/49): loss=0.3655529531076452\n",
      "Gradient Descent(11/49): loss=0.3642361849696241\n",
      "Gradient Descent(12/49): loss=0.3630728512692497\n",
      "Gradient Descent(13/49): loss=0.36204213123711404\n",
      "Gradient Descent(14/49): loss=0.36112672088319864\n",
      "Gradient Descent(15/49): loss=0.36031202376112775\n",
      "Gradient Descent(16/49): loss=0.35958559364174214\n",
      "Gradient Descent(17/49): loss=0.3589367329173006\n",
      "Gradient Descent(18/49): loss=0.35835619187789736\n",
      "Gradient Descent(19/49): loss=0.35783593620695237\n",
      "Gradient Descent(20/49): loss=0.35736896247861155\n",
      "Gradient Descent(21/49): loss=0.35694914865983\n",
      "Gradient Descent(22/49): loss=0.3565711309539176\n",
      "Gradient Descent(23/49): loss=0.35623020100895325\n",
      "Gradient Descent(24/49): loss=0.35592221923312045\n",
      "Gradient Descent(25/49): loss=0.355643541092677\n",
      "Gradient Descent(26/49): loss=0.35539095403862264\n",
      "Gradient Descent(27/49): loss=0.35516162324662864\n",
      "Gradient Descent(28/49): loss=0.35495304474140904\n",
      "Gradient Descent(29/49): loss=0.35476300476120387\n",
      "Gradient Descent(30/49): loss=0.3545895444322051\n",
      "Gradient Descent(31/49): loss=0.35443092898726425\n",
      "Gradient Descent(32/49): loss=0.35428562089194987\n",
      "Gradient Descent(33/49): loss=0.3541522563433686\n",
      "Gradient Descent(34/49): loss=0.35402962468971794\n",
      "Gradient Descent(35/49): loss=0.3539166503859376\n",
      "Gradient Descent(36/49): loss=0.3538123771564666\n",
      "Gradient Descent(37/49): loss=0.35371595408245193\n",
      "Gradient Descent(38/49): loss=0.35362662336967154\n",
      "Gradient Descent(39/49): loss=0.3535437095863318\n",
      "Gradient Descent(40/49): loss=0.35346661018787656\n",
      "Gradient Descent(41/49): loss=0.3533947871698501\n",
      "Gradient Descent(42/49): loss=0.35332775971037966\n",
      "Gradient Descent(43/49): loss=0.35326509768151093\n",
      "Gradient Descent(44/49): loss=0.3532064159239083\n",
      "Gradient Descent(45/49): loss=0.3531513691926524\n",
      "Gradient Descent(46/49): loss=0.35309964769336255\n",
      "Gradient Descent(47/49): loss=0.35305097313785505\n",
      "Gradient Descent(48/49): loss=0.35300509525726564\n",
      "Gradient Descent(49/49): loss=0.35296178871815354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3997133737202411\n",
      "Gradient Descent(2/49): loss=0.3884968540906943\n",
      "Gradient Descent(3/49): loss=0.38239490054584546\n",
      "Gradient Descent(4/49): loss=0.37815365103710835\n",
      "Gradient Descent(5/49): loss=0.37484725542414593\n",
      "Gradient Descent(6/49): loss=0.3721230749299545\n",
      "Gradient Descent(7/49): loss=0.36981410192104175\n",
      "Gradient Descent(8/49): loss=0.3678254073136325\n",
      "Gradient Descent(9/49): loss=0.36609538397086516\n",
      "Gradient Descent(10/49): loss=0.3645802654267922\n",
      "Gradient Descent(11/49): loss=0.3632469869071587\n",
      "Gradient Descent(12/49): loss=0.3620694820805494\n",
      "Gradient Descent(13/49): loss=0.36102657648005804\n",
      "Gradient Descent(14/49): loss=0.36010069790465143\n",
      "Gradient Descent(15/49): loss=0.35927703731517047\n",
      "Gradient Descent(16/49): loss=0.3585429735301728\n",
      "Gradient Descent(17/49): loss=0.3578876605362981\n",
      "Gradient Descent(18/49): loss=0.3573017197769706\n",
      "Gradient Descent(19/49): loss=0.3567770031577786\n",
      "Gradient Descent(20/49): loss=0.3563064055928896\n",
      "Gradient Descent(21/49): loss=0.3558837135130991\n",
      "Gradient Descent(22/49): loss=0.35550348031413487\n",
      "Gradient Descent(23/49): loss=0.3551609225465295\n",
      "Gradient Descent(24/49): loss=0.3548518324508056\n",
      "Gradient Descent(25/49): loss=0.3545725036275385\n",
      "Gradient Descent(26/49): loss=0.35431966743485815\n",
      "Gradient Descent(27/49): loss=0.35409043826497777\n",
      "Gradient Descent(28/49): loss=0.3538822662508201\n",
      "Gradient Descent(29/49): loss=0.35369289624635547\n",
      "Gradient Descent(30/49): loss=0.3535203321433927\n",
      "Gradient Descent(31/49): loss=0.3533628057550957\n",
      "Gradient Descent(32/49): loss=0.35321874962701905\n",
      "Gradient Descent(33/49): loss=0.35308677323980836\n",
      "Gradient Descent(34/49): loss=0.35296564215080506\n",
      "Gradient Descent(35/49): loss=0.3528542596894375\n",
      "Gradient Descent(36/49): loss=0.35275165087699584\n",
      "Gradient Descent(37/49): loss=0.3526569482877139\n",
      "Gradient Descent(38/49): loss=0.3525693796069402\n",
      "Gradient Descent(39/49): loss=0.3524882566750003\n",
      "Gradient Descent(40/49): loss=0.35241296583326026\n",
      "Gradient Descent(41/49): loss=0.35234295941274285\n",
      "Gradient Descent(42/49): loss=0.35227774822611835\n",
      "Gradient Descent(43/49): loss=0.35221689494153563\n",
      "Gradient Descent(44/49): loss=0.3521600082320076\n",
      "Gradient Descent(45/49): loss=0.35210673760729166\n",
      "Gradient Descent(46/49): loss=0.3520567688466989\n",
      "Gradient Descent(47/49): loss=0.35200981996127273\n",
      "Gradient Descent(48/49): loss=0.3519656376225108\n",
      "Gradient Descent(49/49): loss=0.35192399400242824\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3978833890702258\n",
      "Gradient Descent(2/49): loss=0.38683094020062275\n",
      "Gradient Descent(3/49): loss=0.3808901910930543\n",
      "Gradient Descent(4/49): loss=0.3767730508046276\n",
      "Gradient Descent(5/49): loss=0.37355999012263297\n",
      "Gradient Descent(6/49): loss=0.3709059285264481\n",
      "Gradient Descent(7/49): loss=0.3686496075070539\n",
      "Gradient Descent(8/49): loss=0.3667002611565168\n",
      "Gradient Descent(9/49): loss=0.36499934243540316\n",
      "Gradient Descent(10/49): loss=0.3635053861493161\n",
      "Gradient Descent(11/49): loss=0.3621870904465004\n",
      "Gradient Descent(12/49): loss=0.3610197572953453\n",
      "Gradient Descent(13/49): loss=0.3599832837588508\n",
      "Gradient Descent(14/49): loss=0.3590609418212656\n",
      "Gradient Descent(15/49): loss=0.35823859015540743\n",
      "Gradient Descent(16/49): loss=0.35750413698697936\n",
      "Gradient Descent(17/49): loss=0.35684715653843724\n",
      "Gradient Descent(18/49): loss=0.35625860383869234\n",
      "Gradient Descent(19/49): loss=0.3557305953072186\n",
      "Gradient Descent(20/49): loss=0.3552562351273642\n",
      "Gradient Descent(21/49): loss=0.35482947469927456\n",
      "Gradient Descent(22/49): loss=0.3544449967983858\n",
      "Gradient Descent(23/49): loss=0.35409811872928904\n",
      "Gradient Descent(24/49): loss=0.3537847104513075\n",
      "Gradient Descent(25/49): loss=0.3535011247520314\n",
      "Gradient Descent(26/49): loss=0.3532441372835833\n",
      "Gradient Descent(27/49): loss=0.3530108947865742\n",
      "Gradient Descent(28/49): loss=0.3527988701889025\n",
      "Gradient Descent(29/49): loss=0.35260582353044123\n",
      "Gradient Descent(30/49): loss=0.3524297678616117\n",
      "Gradient Descent(31/49): loss=0.35226893941415155\n",
      "Gradient Descent(32/49): loss=0.35212177145940815\n",
      "Gradient Descent(33/49): loss=0.35198687136225176\n",
      "Gradient Descent(34/49): loss=0.35186300041340346\n",
      "Gradient Descent(35/49): loss=0.35174905608395424\n",
      "Gradient Descent(36/49): loss=0.35164405639623614\n",
      "Gradient Descent(37/49): loss=0.35154712614725614\n",
      "Gradient Descent(38/49): loss=0.35145748475630156\n",
      "Gradient Descent(39/49): loss=0.3513744355383491\n",
      "Gradient Descent(40/49): loss=0.35129735623052627\n",
      "Gradient Descent(41/49): loss=0.3512256906208551\n",
      "Gradient Descent(42/49): loss=0.3511589411474393\n",
      "Gradient Descent(43/49): loss=0.3510966623526368\n",
      "Gradient Descent(44/49): loss=0.3510384550909681\n",
      "Gradient Descent(45/49): loss=0.3509839614018688\n",
      "Gradient Descent(46/49): loss=0.3509328599691719\n",
      "Gradient Descent(47/49): loss=0.35088486209861436\n",
      "Gradient Descent(48/49): loss=0.3508397081529047\n",
      "Gradient Descent(49/49): loss=0.3507971643910907\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40065714767504423\n",
      "Gradient Descent(2/49): loss=0.3895776528535149\n",
      "Gradient Descent(3/49): loss=0.38364290334783424\n",
      "Gradient Descent(4/49): loss=0.3795423269981745\n",
      "Gradient Descent(5/49): loss=0.37634982180717985\n",
      "Gradient Descent(6/49): loss=0.37371721700332017\n",
      "Gradient Descent(7/49): loss=0.3714817115763541\n",
      "Gradient Descent(8/49): loss=0.3695518110086047\n",
      "Gradient Descent(9/49): loss=0.36786864249335727\n",
      "Gradient Descent(10/49): loss=0.3663906132716408\n",
      "Gradient Descent(11/49): loss=0.3650863982173434\n",
      "Gradient Descent(12/49): loss=0.36393133693054774\n",
      "Gradient Descent(13/49): loss=0.36290540286050005\n",
      "Gradient Descent(14/49): loss=0.3619919697823663\n",
      "Gradient Descent(15/49): loss=0.36117701418291537\n",
      "Gradient Descent(16/49): loss=0.3604485709510177\n",
      "Gradient Descent(17/49): loss=0.35979634422435497\n",
      "Gradient Descent(18/49): loss=0.35921141790970906\n",
      "Gradient Descent(19/49): loss=0.3586860331057057\n",
      "Gradient Descent(20/49): loss=0.3582134122730699\n",
      "Gradient Descent(21/49): loss=0.35778761727205904\n",
      "Gradient Descent(22/49): loss=0.35740343272913966\n",
      "Gradient Descent(23/49): loss=0.3570562688746608\n",
      "Gradient Descent(24/49): loss=0.35674207970103605\n",
      "Gradient Descent(25/49): loss=0.3564572934136452\n",
      "Gradient Descent(26/49): loss=0.35619875290692016\n",
      "Gradient Descent(27/49): loss=0.3559636645274539\n",
      "Gradient Descent(28/49): loss=0.3557495537642711\n",
      "Gradient Descent(29/49): loss=0.35555422678328324\n",
      "Gradient Descent(30/49): loss=0.35537573693010177\n",
      "Gradient Descent(31/49): loss=0.3552123554834761\n",
      "Gradient Descent(32/49): loss=0.35506254606448723\n",
      "Gradient Descent(33/49): loss=0.3549249422036615\n",
      "Gradient Descent(34/49): loss=0.35479832764594127\n",
      "Gradient Descent(35/49): loss=0.35468161903658313\n",
      "Gradient Descent(36/49): loss=0.354573850682898\n",
      "Gradient Descent(37/49): loss=0.35447416112975594\n",
      "Gradient Descent(38/49): loss=0.35438178132275777\n",
      "Gradient Descent(39/49): loss=0.354296024163325\n",
      "Gradient Descent(40/49): loss=0.3542162752857076\n",
      "Gradient Descent(41/49): loss=0.3541419849079001\n",
      "Gradient Descent(42/49): loss=0.3540726606273118\n",
      "Gradient Descent(43/49): loss=0.3540078610482847\n",
      "Gradient Descent(44/49): loss=0.35394719014259995\n",
      "Gradient Descent(45/49): loss=0.3538902922562916\n",
      "Gradient Descent(46/49): loss=0.3538368476866794\n",
      "Gradient Descent(47/49): loss=0.3537865687627608\n",
      "Gradient Descent(48/49): loss=0.35373919637016227\n",
      "Gradient Descent(49/49): loss=0.35369449686889454\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39909426093543277\n",
      "Gradient Descent(2/49): loss=0.3884234476612802\n",
      "Gradient Descent(3/49): loss=0.382546700476773\n",
      "Gradient Descent(4/49): loss=0.37841509472136564\n",
      "Gradient Descent(5/49): loss=0.375169096422334\n",
      "Gradient Descent(6/49): loss=0.3724862992199976\n",
      "Gradient Descent(7/49): loss=0.3702116048006509\n",
      "Gradient Descent(8/49): loss=0.3682547006487388\n",
      "Gradient Descent(9/49): loss=0.36655572755916715\n",
      "Gradient Descent(10/49): loss=0.36507148086407193\n",
      "Gradient Descent(11/49): loss=0.36376894714170566\n",
      "Gradient Descent(12/49): loss=0.3626218972062227\n",
      "Gradient Descent(13/49): loss=0.3616089194466946\n",
      "Gradient Descent(14/49): loss=0.3607121984767774\n",
      "Gradient Descent(15/49): loss=0.35991670839207024\n",
      "Gradient Descent(16/49): loss=0.3592096509188173\n",
      "Gradient Descent(17/49): loss=0.35858004609620825\n",
      "Gradient Descent(18/49): loss=0.3580184227064563\n",
      "Gradient Descent(19/49): loss=0.35751657691907357\n",
      "Gradient Descent(20/49): loss=0.35706737950693085\n",
      "Gradient Descent(21/49): loss=0.3566646188977271\n",
      "Gradient Descent(22/49): loss=0.35630287148076406\n",
      "Gradient Descent(23/49): loss=0.3559773931788008\n",
      "Gradient Descent(24/49): loss=0.3556840279643024\n",
      "Gradient Descent(25/49): loss=0.35541913011154425\n",
      "Gradient Descent(26/49): loss=0.35517949774032226\n",
      "Gradient Descent(27/49): loss=0.3549623157478145\n",
      "Gradient Descent(28/49): loss=0.35476510661818034\n",
      "Gradient Descent(29/49): loss=0.3545856878921951\n",
      "Gradient Descent(30/49): loss=0.35442213530200417\n",
      "Gradient Descent(31/49): loss=0.3542727507489594\n",
      "Gradient Descent(32/49): loss=0.3541360344389618\n",
      "Gradient Descent(33/49): loss=0.3540106605990409\n",
      "Gradient Descent(34/49): loss=0.35389545628760793\n",
      "Gradient Descent(35/49): loss=0.35378938288360084\n",
      "Gradient Descent(36/49): loss=0.3536915199000313\n",
      "Gradient Descent(37/49): loss=0.3536010508178041\n",
      "Gradient Descent(38/49): loss=0.3535172506780272\n",
      "Gradient Descent(39/49): loss=0.3534394752068728\n",
      "Gradient Descent(40/49): loss=0.35336715127752305\n",
      "Gradient Descent(41/49): loss=0.35329976853976586\n",
      "Gradient Descent(42/49): loss=0.35323687207012316\n",
      "Gradient Descent(43/49): loss=0.3531780559145893\n",
      "Gradient Descent(44/49): loss=0.3531229574126059\n",
      "Gradient Descent(45/49): loss=0.35307125220520663\n",
      "Gradient Descent(46/49): loss=0.3530226498426557\n",
      "Gradient Descent(47/49): loss=0.352976889917648\n",
      "Gradient Descent(48/49): loss=0.35293373865948124\n",
      "Gradient Descent(49/49): loss=0.35289298593272667\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39883244364708675\n",
      "Gradient Descent(2/49): loss=0.38790759952682985\n",
      "Gradient Descent(3/49): loss=0.38186397887323476\n",
      "Gradient Descent(4/49): loss=0.377637376502342\n",
      "Gradient Descent(5/49): loss=0.3743307209202432\n",
      "Gradient Descent(6/49): loss=0.37160467484541215\n",
      "Gradient Descent(7/49): loss=0.3692967961968196\n",
      "Gradient Descent(8/49): loss=0.3673132559895335\n",
      "Gradient Descent(9/49): loss=0.3655922974575314\n",
      "Gradient Descent(10/49): loss=0.36408958544269615\n",
      "Gradient Descent(11/49): loss=0.36277138067516257\n",
      "Gradient Descent(12/49): loss=0.3616109592462675\n",
      "Gradient Descent(13/49): loss=0.36058655422826924\n",
      "Gradient Descent(14/49): loss=0.3596800823974989\n",
      "Gradient Descent(15/49): loss=0.3588763067913276\n",
      "Gradient Descent(16/49): loss=0.35816225621012854\n",
      "Gradient Descent(17/49): loss=0.35752680437095513\n",
      "Gradient Descent(18/49): loss=0.3569603531121389\n",
      "Gradient Descent(19/49): loss=0.356454586448518\n",
      "Gradient Descent(20/49): loss=0.3560022748246391\n",
      "Gradient Descent(21/49): loss=0.3555971162084731\n",
      "Gradient Descent(22/49): loss=0.3552336050607913\n",
      "Gradient Descent(23/49): loss=0.3549069229510768\n",
      "Gradient Descent(24/49): loss=0.35461284635133133\n",
      "Gradient Descent(25/49): loss=0.354347668308297\n",
      "Gradient Descent(26/49): loss=0.35410813149469894\n",
      "Gradient Descent(27/49): loss=0.3538913707032908\n",
      "Gradient Descent(28/49): loss=0.3536948632544244\n",
      "Gradient Descent(29/49): loss=0.35351638608905755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=0.35335397854697564\n",
      "Gradient Descent(31/49): loss=0.3532059100057997\n",
      "Gradient Descent(32/49): loss=0.35307065169439195\n",
      "Gradient Descent(33/49): loss=0.35294685210435156\n",
      "Gradient Descent(34/49): loss=0.3528333155122818\n",
      "Gradient Descent(35/49): loss=0.35272898319831075\n",
      "Gradient Descent(36/49): loss=0.3526329170065215\n",
      "Gradient Descent(37/49): loss=0.3525442849431216\n",
      "Gradient Descent(38/49): loss=0.3524623485503435\n",
      "Gradient Descent(39/49): loss=0.35238645182972583\n",
      "Gradient Descent(40/49): loss=0.3523160115187466\n",
      "Gradient Descent(41/49): loss=0.3522505085506901\n",
      "Gradient Descent(42/49): loss=0.3521894805498558\n",
      "Gradient Descent(43/49): loss=0.352132515233347\n",
      "Gradient Descent(44/49): loss=0.3520792446071949\n",
      "Gradient Descent(45/49): loss=0.35202933985886253\n",
      "Gradient Descent(46/49): loss=0.3519825068605644\n",
      "Gradient Descent(47/49): loss=0.35193848220860535\n",
      "Gradient Descent(48/49): loss=0.3518970297333011\n",
      "Gradient Descent(49/49): loss=0.35185793742220384\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39699422885073027\n",
      "Gradient Descent(2/49): loss=0.3862526088606363\n",
      "Gradient Descent(3/49): loss=0.380373119203982\n",
      "Gradient Descent(4/49): loss=0.3762709017568454\n",
      "Gradient Descent(5/49): loss=0.3730567952485043\n",
      "Gradient Descent(6/49): loss=0.37039961442298613\n",
      "Gradient Descent(7/49): loss=0.3681429137741412\n",
      "Gradient Descent(8/49): loss=0.3661971595992979\n",
      "Gradient Descent(9/49): loss=0.364503740203114\n",
      "Gradient Descent(10/49): loss=0.3630206792402328\n",
      "Gradient Descent(11/49): loss=0.3617160382419602\n",
      "Gradient Descent(12/49): loss=0.3605644846811008\n",
      "Gradient Descent(13/49): loss=0.35954533524285237\n",
      "Gradient Descent(14/49): loss=0.3586413550238433\n",
      "Gradient Descent(15/49): loss=0.3578379733540677\n",
      "Gradient Descent(16/49): loss=0.3571227432930483\n",
      "Gradient Descent(17/49): loss=0.3564849512755863\n",
      "Gradient Descent(18/49): loss=0.35591532382705493\n",
      "Gradient Descent(19/49): loss=0.35540579990431825\n",
      "Gradient Descent(20/49): loss=0.35494934947071677\n",
      "Gradient Descent(21/49): loss=0.3545398258729719\n",
      "Gradient Descent(22/49): loss=0.3541718437448807\n",
      "Gradient Descent(23/49): loss=0.35384067672889363\n",
      "Gradient Descent(24/49): loss=0.3535421709429797\n",
      "Gradient Descent(25/49): loss=0.35327267119714045\n",
      "Gradient Descent(26/49): loss=0.35302895769488385\n",
      "Gradient Descent(27/49): loss=0.35280819146587533\n",
      "Gradient Descent(28/49): loss=0.3526078671431392\n",
      "Gradient Descent(29/49): loss=0.3524257719689011\n",
      "Gradient Descent(30/49): loss=0.3522599501175494\n",
      "Gradient Descent(31/49): loss=0.3521086715818004\n",
      "Gradient Descent(32/49): loss=0.35197040499201077\n",
      "Gradient Descent(33/49): loss=0.35184379383755476\n",
      "Gradient Descent(34/49): loss=0.35172763563940745\n",
      "Gradient Descent(35/49): loss=0.35162086368894035\n",
      "Gradient Descent(36/49): loss=0.35152253102256664\n",
      "Gradient Descent(37/49): loss=0.35143179634761684\n",
      "Gradient Descent(38/49): loss=0.35134791167341684\n",
      "Gradient Descent(39/49): loss=0.35127021143430404\n",
      "Gradient Descent(40/49): loss=0.3511981029192944\n",
      "Gradient Descent(41/49): loss=0.3511310578471078\n",
      "Gradient Descent(42/49): loss=0.35106860494591385\n",
      "Gradient Descent(43/49): loss=0.3510103234150082\n",
      "Gradient Descent(44/49): loss=0.3509558371610825\n",
      "Gradient Descent(45/49): loss=0.3509048097151688\n",
      "Gradient Descent(46/49): loss=0.3508569397480073\n",
      "Gradient Descent(47/49): loss=0.35081195711175545\n",
      "Gradient Descent(48/49): loss=0.35076961934482276\n",
      "Gradient Descent(49/49): loss=0.3507297085843674\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3997634290027887\n",
      "Gradient Descent(2/49): loss=0.38899255974141406\n",
      "Gradient Descent(3/49): loss=0.38312131067233596\n",
      "Gradient Descent(4/49): loss=0.37903801275683563\n",
      "Gradient Descent(5/49): loss=0.37584659602163145\n",
      "Gradient Descent(6/49): loss=0.37321257756765713\n",
      "Gradient Descent(7/49): loss=0.37097795307286996\n",
      "Gradient Descent(8/49): loss=0.3690525107001175\n",
      "Gradient Descent(9/49): loss=0.36737737488772454\n",
      "Gradient Descent(10/49): loss=0.3659105002965136\n",
      "Gradient Descent(11/49): loss=0.364619977822311\n",
      "Gradient Descent(12/49): loss=0.3634805591610059\n",
      "Gradient Descent(13/49): loss=0.36247167810496494\n",
      "Gradient Descent(14/49): loss=0.36157623658926613\n",
      "Gradient Descent(15/49): loss=0.3607798113973336\n",
      "Gradient Descent(16/49): loss=0.3600701068519731\n",
      "Gradient Descent(17/49): loss=0.3594365593264784\n",
      "Gradient Descent(18/49): loss=0.3588700401930307\n",
      "Gradient Descent(19/49): loss=0.3583626255458817\n",
      "Gradient Descent(20/49): loss=0.3579074131047902\n",
      "Gradient Descent(21/49): loss=0.35749837367225784\n",
      "Gradient Descent(22/49): loss=0.35713022869163163\n",
      "Gradient Descent(23/49): loss=0.3567983480430838\n",
      "Gradient Descent(24/49): loss=0.3564986638776195\n",
      "Gradient Descent(25/49): loss=0.35622759739256965\n",
      "Gradient Descent(26/49): loss=0.3559819962066388\n",
      "Gradient Descent(27/49): loss=0.3557590805234926\n",
      "Gradient Descent(28/49): loss=0.3555563966561983\n",
      "Gradient Descent(29/49): loss=0.35537177676821663\n",
      "Gradient Descent(30/49): loss=0.3552033039006513\n",
      "Gradient Descent(31/49): loss=0.355049281520225\n",
      "Gradient Descent(32/49): loss=0.3549082069515058\n",
      "Gradient Descent(33/49): loss=0.3547787481595526\n",
      "Gradient Descent(34/49): loss=0.3546597234319246\n",
      "Gradient Descent(35/49): loss=0.3545500835765416\n",
      "Gradient Descent(36/49): loss=0.35444889630759463\n",
      "Gradient Descent(37/49): loss=0.3543553325380717\n",
      "Gradient Descent(38/49): loss=0.35426865433636767\n",
      "Gradient Descent(39/49): loss=0.35418820433731074\n",
      "Gradient Descent(40/49): loss=0.35411339642586004\n",
      "Gradient Descent(41/49): loss=0.3540437075355859\n",
      "Gradient Descent(42/49): loss=0.353978670424494\n",
      "Gradient Descent(43/49): loss=0.35391786730837393\n",
      "Gradient Descent(44/49): loss=0.353860924247057\n",
      "Gradient Descent(45/49): loss=0.353807506192142\n",
      "Gradient Descent(46/49): loss=0.35375731261617815\n",
      "Gradient Descent(47/49): loss=0.3537100736532315\n",
      "Gradient Descent(48/49): loss=0.3536655466894208\n",
      "Gradient Descent(49/49): loss=0.3536235133495604\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3983721233329972\n",
      "Gradient Descent(2/49): loss=0.38788484960745023\n",
      "Gradient Descent(3/49): loss=0.38206392648603443\n",
      "Gradient Descent(4/49): loss=0.3779450574432001\n",
      "Gradient Descent(5/49): loss=0.37469916099948397\n",
      "Gradient Descent(6/49): loss=0.3720156646621696\n",
      "Gradient Descent(7/49): loss=0.36974307726369177\n",
      "Gradient Descent(8/49): loss=0.36779184000706777\n",
      "Gradient Descent(9/49): loss=0.36610189056086034\n",
      "Gradient Descent(10/49): loss=0.36462951091183177\n",
      "Gradient Descent(11/49): loss=0.363341099202258\n",
      "Gradient Descent(12/49): loss=0.36220985589492166\n",
      "Gradient Descent(13/49): loss=0.3612138552254721\n",
      "Gradient Descent(14/49): loss=0.36033483755509715\n",
      "Gradient Descent(15/49): loss=0.35955740447922946\n",
      "Gradient Descent(16/49): loss=0.3588684529700175\n",
      "Gradient Descent(17/49): loss=0.3582567593583442\n",
      "Gradient Descent(18/49): loss=0.35771266210417935\n",
      "Gradient Descent(19/49): loss=0.3572278127638356\n",
      "Gradient Descent(20/49): loss=0.356794975998394\n",
      "Gradient Descent(21/49): loss=0.35640786611145714\n",
      "Gradient Descent(22/49): loss=0.3560610116123832\n",
      "Gradient Descent(23/49): loss=0.3557496418098653\n",
      "Gradient Descent(24/49): loss=0.3554695910683175\n",
      "Gradient Descent(25/49): loss=0.35521721745228735\n",
      "Gradient Descent(26/49): loss=0.35498933324182574\n",
      "Gradient Descent(27/49): loss=0.35478314534296684\n",
      "Gradient Descent(28/49): loss=0.3545962040146357\n",
      "Gradient Descent(29/49): loss=0.35442635863188876\n",
      "Gradient Descent(30/49): loss=0.3542717194346887\n",
      "Gradient Descent(31/49): loss=0.35413062439084503\n",
      "Gradient Descent(32/49): loss=0.35400161044443756\n",
      "Gradient Descent(33/49): loss=0.35388338853610046\n",
      "Gradient Descent(34/49): loss=0.3537748218754516\n",
      "Gradient Descent(35/49): loss=0.35367490702337057\n",
      "Gradient Descent(36/49): loss=0.353582757406205\n",
      "Gradient Descent(37/49): loss=0.35349758893792693\n",
      "Gradient Descent(38/49): loss=0.3534187074717149\n",
      "Gradient Descent(39/49): loss=0.3533454978409636\n",
      "Gradient Descent(40/49): loss=0.35327741428250664\n",
      "Gradient Descent(41/49): loss=0.3532139720628456\n",
      "Gradient Descent(42/49): loss=0.3531547401521813\n",
      "Gradient Descent(43/49): loss=0.3530993348116659\n",
      "Gradient Descent(44/49): loss=0.3530474139770552\n",
      "Gradient Descent(45/49): loss=0.3529986723372665\n",
      "Gradient Descent(46/49): loss=0.35295283701959224\n",
      "Gradient Descent(47/49): loss=0.3529096638047794\n",
      "Gradient Descent(48/49): loss=0.352868933805123\n",
      "Gradient Descent(49/49): loss=0.3528304505473305\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3981233288708481\n",
      "Gradient Descent(2/49): loss=0.3873495363471013\n",
      "Gradient Descent(3/49): loss=0.38136248857897487\n",
      "Gradient Descent(4/49): loss=0.37714979131521553\n",
      "Gradient Descent(5/49): loss=0.3738434638217702\n",
      "Gradient Descent(6/49): loss=0.3711165709928613\n",
      "Gradient Descent(7/49): loss=0.36881067450221106\n",
      "Gradient Descent(8/49): loss=0.3668328559018649\n",
      "Gradient Descent(9/49): loss=0.36512119355312567\n",
      "Gradient Descent(10/49): loss=0.3636308280728618\n",
      "Gradient Descent(11/49): loss=0.36232740211291437\n",
      "Gradient Descent(12/49): loss=0.3611835859851904\n",
      "Gradient Descent(13/49): loss=0.3601770632015331\n",
      "Gradient Descent(14/49): loss=0.3592892735900867\n",
      "Gradient Descent(15/49): loss=0.35850457889182596\n",
      "Gradient Descent(16/49): loss=0.35780967864998875\n",
      "Gradient Descent(17/49): loss=0.3571931826172424\n",
      "Gradient Descent(18/49): loss=0.35664528601405154\n",
      "Gradient Descent(19/49): loss=0.3561575155003342\n",
      "Gradient Descent(20/49): loss=0.3557225257683551\n",
      "Gradient Descent(21/49): loss=0.35533393366875216\n",
      "Gradient Descent(22/49): loss=0.3549861810070877\n",
      "Gradient Descent(23/49): loss=0.3546744197906739\n",
      "Gradient Descent(24/49): loss=0.354394415416138\n",
      "Gradient Descent(25/49): loss=0.35414246443311764\n",
      "Gradient Descent(26/49): loss=0.35391532431007183\n",
      "Gradient Descent(27/49): loss=0.35371015319020777\n",
      "Gradient Descent(28/49): loss=0.3535244580358188\n",
      "Gradient Descent(29/49): loss=0.3533560498661698\n",
      "Gradient Descent(30/49): loss=0.3532030050285372\n",
      "Gradient Descent(31/49): loss=0.3530636316246315\n",
      "Gradient Descent(32/49): loss=0.3529364403592735\n",
      "Gradient Descent(33/49): loss=0.3528201191944364\n",
      "Gradient Descent(34/49): loss=0.3527135112863702\n",
      "Gradient Descent(35/49): loss=0.35261559576134666\n",
      "Gradient Descent(36/49): loss=0.3525254709501824\n",
      "Gradient Descent(37/49): loss=0.35244233975576933\n",
      "Gradient Descent(38/49): loss=0.35236549687339025\n",
      "Gradient Descent(39/49): loss=0.3522943176221827\n",
      "Gradient Descent(40/49): loss=0.35222824817895654\n",
      "Gradient Descent(41/49): loss=0.35216679703363046\n",
      "Gradient Descent(42/49): loss=0.3521095275096166\n",
      "Gradient Descent(43/49): loss=0.35205605121317063\n",
      "Gradient Descent(44/49): loss=0.35200602229354944\n",
      "Gradient Descent(45/49): loss=0.35195913241122523\n",
      "Gradient Descent(46/49): loss=0.35191510632471734\n",
      "Gradient Descent(47/49): loss=0.3518736980181465\n",
      "Gradient Descent(48/49): loss=0.3518346873016269\n",
      "Gradient Descent(49/49): loss=0.35179787682529795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39628054211299213\n",
      "Gradient Descent(2/49): loss=0.38570519244029194\n",
      "Gradient Descent(3/49): loss=0.3798844634607774\n",
      "Gradient Descent(4/49): loss=0.3757959685938834\n",
      "Gradient Descent(5/49): loss=0.37258113671114634\n",
      "Gradient Descent(6/49): loss=0.3699217224834668\n",
      "Gradient Descent(7/49): loss=0.3676654932975698\n",
      "Gradient Descent(8/49): loss=0.36572393224286737\n",
      "Gradient Descent(9/49): loss=0.36403832769984734\n",
      "Gradient Descent(10/49): loss=0.3625662203188329\n",
      "Gradient Descent(11/49): loss=0.36127507833798805\n",
      "Gradient Descent(12/49): loss=0.36013897639833825\n",
      "Gradient Descent(13/49): loss=0.3591366861646823\n",
      "Gradient Descent(14/49): loss=0.35825049452694785\n",
      "Gradient Descent(15/49): loss=0.35746542441162027\n",
      "Gradient Descent(16/49): loss=0.3567686920915388\n",
      "Gradient Descent(17/49): loss=0.35614931110819187\n",
      "Gradient Descent(18/49): loss=0.35559779176190787\n",
      "Gradient Descent(19/49): loss=0.355105905868624\n",
      "Gradient Descent(20/49): loss=0.3546664980134182\n",
      "Gradient Descent(21/49): loss=0.35427333118338217\n",
      "Gradient Descent(22/49): loss=0.3539209586400165\n",
      "Gradient Descent(23/49): loss=0.3536046163549611\n",
      "Gradient Descent(24/49): loss=0.3533201319126559\n",
      "Gradient Descent(25/49): loss=0.35306384683140346\n",
      "Gradient Descent(26/49): loss=0.3528325499722992\n",
      "Gradient Descent(27/49): loss=0.35262342021278953\n",
      "Gradient Descent(28/49): loss=0.3524339769303916\n",
      "Gradient Descent(29/49): loss=0.3522620371172343\n",
      "Gradient Descent(30/49): loss=0.35210567815618476\n",
      "Gradient Descent(31/49): loss=0.3519632054531077\n",
      "Gradient Descent(32/49): loss=0.3518331242497895\n",
      "Gradient Descent(33/49): loss=0.3517141150468355\n",
      "Gradient Descent(34/49): loss=0.35160501215141404\n",
      "Gradient Descent(35/49): loss=0.3515047849354013\n",
      "Gradient Descent(36/49): loss=0.35141252144839547\n",
      "Gradient Descent(37/49): loss=0.3513274140795849\n",
      "Gradient Descent(38/49): loss=0.35124874700433617\n",
      "Gradient Descent(39/49): loss=0.3511758851869841\n",
      "Gradient Descent(40/49): loss=0.35110826474174683\n",
      "Gradient Descent(41/49): loss=0.3510453844797899\n",
      "Gradient Descent(42/49): loss=0.35098679849292685\n",
      "Gradient Descent(43/49): loss=0.350932109643822\n",
      "Gradient Descent(44/49): loss=0.3508809638493157\n",
      "Gradient Descent(45/49): loss=0.35083304505801255\n",
      "Gradient Descent(46/49): loss=0.35078807083585933\n",
      "Gradient Descent(47/49): loss=0.3507457884843916\n",
      "Gradient Descent(48/49): loss=0.3507059716258361\n",
      "Gradient Descent(49/49): loss=0.350668417197549\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39903850345887687\n",
      "Gradient Descent(2/49): loss=0.3884373103564963\n",
      "Gradient Descent(3/49): loss=0.38262602721851763\n",
      "Gradient Descent(4/49): loss=0.37855814063540216\n",
      "Gradient Descent(5/49): loss=0.37536779252032293\n",
      "Gradient Descent(6/49): loss=0.3727331834743178\n",
      "Gradient Descent(7/49): loss=0.3705004340114039\n",
      "Gradient Descent(8/49): loss=0.36858032295427784\n",
      "Gradient Descent(9/49): loss=0.36691387231404066\n",
      "Gradient Descent(10/49): loss=0.3654585655814579\n",
      "Gradient Descent(11/49): loss=0.3641819226546549\n",
      "Gradient Descent(12/49): loss=0.3630581315077077\n",
      "Gradient Descent(13/49): loss=0.36206611332852173\n",
      "Gradient Descent(14/49): loss=0.36118832466780093\n",
      "Gradient Descent(15/49): loss=0.3604099670497329\n",
      "Gradient Descent(16/49): loss=0.3597184361767695\n",
      "Gradient Descent(17/49): loss=0.35910292005779637\n",
      "Gradient Descent(18/49): loss=0.35855409453956916\n",
      "Gradient Descent(19/49): loss=0.3580638855637864\n",
      "Gradient Descent(20/49): loss=0.3576252790511728\n",
      "Gradient Descent(21/49): loss=0.35723216600909685\n",
      "Gradient Descent(22/49): loss=0.3568792144830128\n",
      "Gradient Descent(23/49): loss=0.35656176248244337\n",
      "Gradient Descent(24/49): loss=0.3562757276355339\n",
      "Gradient Descent(25/49): loss=0.3560175304117838\n",
      "Gradient Descent(26/49): loss=0.35578402850154855\n",
      "Gradient Descent(27/49): loss=0.3555724604726333\n",
      "Gradient Descent(28/49): loss=0.35538039721171355\n",
      "Gradient Descent(29/49): loss=0.3552056999472866\n",
      "Gradient Descent(30/49): loss=0.35504648387097293\n",
      "Gradient Descent(31/49): loss=0.35490108654482844\n",
      "Gradient Descent(32/49): loss=0.3547680404171671\n",
      "Gradient Descent(33/49): loss=0.3546460488773819\n",
      "Gradient Descent(34/49): loss=0.3545339653678535\n",
      "Gradient Descent(35/49): loss=0.35443077514291715\n",
      "Gradient Descent(36/49): loss=0.3543355793243901\n",
      "Gradient Descent(37/49): loss=0.3542475809528976\n",
      "Gradient Descent(38/49): loss=0.35416607277607626\n",
      "Gradient Descent(39/49): loss=0.35409042655014317\n",
      "Gradient Descent(40/49): loss=0.35402008366145105\n",
      "Gradient Descent(41/49): loss=0.3539545469003955\n",
      "Gradient Descent(42/49): loss=0.3538933732421191\n",
      "Gradient Descent(43/49): loss=0.3538361675074581\n",
      "Gradient Descent(44/49): loss=0.3537825767939652\n",
      "Gradient Descent(45/49): loss=0.35373228558101105\n",
      "Gradient Descent(46/49): loss=0.35368501142524156\n",
      "Gradient Descent(47/49): loss=0.35364050117331547\n",
      "Gradient Descent(48/49): loss=0.3535985276280998\n",
      "Gradient Descent(49/49): loss=0.35355888661254875\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3978203977972101\n",
      "Gradient Descent(2/49): loss=0.3873760393468144\n",
      "Gradient Descent(3/49): loss=0.38161212234566155\n",
      "Gradient Descent(4/49): loss=0.37751053737759555\n",
      "Gradient Descent(5/49): loss=0.37427206694125403\n",
      "Gradient Descent(6/49): loss=0.37159638288007424\n",
      "Gradient Descent(7/49): loss=0.3693346471761816\n",
      "Gradient Descent(8/49): loss=0.36739755690788267\n",
      "Gradient Descent(9/49): loss=0.36572458418058873\n",
      "Gradient Descent(10/49): loss=0.3642713602485742\n",
      "Gradient Descent(11/49): loss=0.36300363115356243\n",
      "Gradient Descent(12/49): loss=0.3618940103775736\n",
      "Gradient Descent(13/49): loss=0.36092007220555283\n",
      "Gradient Descent(14/49): loss=0.3600631468251886\n",
      "Gradient Descent(15/49): loss=0.3593075095381591\n",
      "Gradient Descent(16/49): loss=0.3586398053811677\n",
      "Gradient Descent(17/49): loss=0.3580486225286404\n",
      "Gradient Descent(18/49): loss=0.3575241647327167\n",
      "Gradient Descent(19/49): loss=0.35705799282948775\n",
      "Gradient Descent(20/49): loss=0.3566428163998374\n",
      "Gradient Descent(21/49): loss=0.3562723231164354\n",
      "Gradient Descent(22/49): loss=0.35594103721525644\n",
      "Gradient Descent(23/49): loss=0.3556442009930477\n",
      "Gradient Descent(24/49): loss=0.35537767484418076\n",
      "Gradient Descent(25/49): loss=0.3551378524432186\n",
      "Gradient Descent(26/49): loss=0.3549215884450174\n",
      "Gradient Descent(27/49): loss=0.35472613662645713\n",
      "Gradient Descent(28/49): loss=0.3545490968030295\n",
      "Gradient Descent(29/49): loss=0.3543883691637718\n",
      "Gradient Descent(30/49): loss=0.3542421149081557\n",
      "Gradient Descent(31/49): loss=0.3541087222576872\n",
      "Gradient Descent(32/49): loss=0.3539867770662309\n",
      "Gradient Descent(33/49): loss=0.3538750373756016\n",
      "Gradient Descent(34/49): loss=0.35377241136331244\n",
      "Gradient Descent(35/49): loss=0.3536779382123131\n",
      "Gradient Descent(36/49): loss=0.353590771501644\n",
      "Gradient Descent(37/49): loss=0.35351016477486774\n",
      "Gradient Descent(38/49): loss=0.35343545899198264\n",
      "Gradient Descent(39/49): loss=0.3533660716118982\n",
      "Gradient Descent(40/49): loss=0.3533014870877284\n",
      "Gradient Descent(41/49): loss=0.35324124858717326\n",
      "Gradient Descent(42/49): loss=0.35318495077592965\n",
      "Gradient Descent(43/49): loss=0.35313223352408174\n",
      "Gradient Descent(44/49): loss=0.35308277641433455\n",
      "Gradient Descent(45/49): loss=0.35303629394722175\n",
      "Gradient Descent(46/49): loss=0.35299253135244296\n",
      "Gradient Descent(47/49): loss=0.3529512609275793\n",
      "Gradient Descent(48/49): loss=0.352912278835886\n",
      "Gradient Descent(49/49): loss=0.35287540230388914\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.397586029391525\n",
      "Gradient Descent(2/49): loss=0.38682104470615253\n",
      "Gradient Descent(3/49): loss=0.3808909744392393\n",
      "Gradient Descent(4/49): loss=0.3766946456919018\n",
      "Gradient Descent(5/49): loss=0.373393132995868\n",
      "Gradient Descent(6/49): loss=0.3706705234329252\n",
      "Gradient Descent(7/49): loss=0.36837153953002655\n",
      "Gradient Descent(8/49): loss=0.3664037837737451\n",
      "Gradient Descent(9/49): loss=0.36470501612724276\n",
      "Gradient Descent(10/49): loss=0.36322981595173237\n",
      "Gradient Descent(11/49): loss=0.36194323056247\n",
      "Gradient Descent(12/49): loss=0.3608173787831452\n",
      "Gradient Descent(13/49): loss=0.3598294647728797\n",
      "Gradient Descent(14/49): loss=0.3589605280962866\n",
      "Gradient Descent(15/49): loss=0.35819460646168283\n",
      "Gradient Descent(16/49): loss=0.3575181443618883\n",
      "Gradient Descent(17/49): loss=0.35691955661403557\n",
      "Gradient Descent(18/49): loss=0.35638889457147166\n",
      "Gradient Descent(19/49): loss=0.3559175835878581\n",
      "Gradient Descent(20/49): loss=0.35549821196098985\n",
      "Gradient Descent(21/49): loss=0.35512435837079404\n",
      "Gradient Descent(22/49): loss=0.3547904489375814\n",
      "Gradient Descent(23/49): loss=0.35449163761321045\n",
      "Gradient Descent(24/49): loss=0.3542237053049433\n",
      "Gradient Descent(25/49): loss=0.3539829742704496\n",
      "Gradient Descent(26/49): loss=0.35376623511575944\n",
      "Gradient Descent(27/49): loss=0.35357068429706\n",
      "Gradient Descent(28/49): loss=0.3533938704463877\n",
      "Gradient Descent(29/49): loss=0.3532336481573622\n",
      "Gradient Descent(30/49): loss=0.35308813811051215\n",
      "Gradient Descent(31/49): loss=0.35295569260864024\n",
      "Gradient Descent(32/49): loss=0.3528348657447666\n",
      "Gradient Descent(33/49): loss=0.3527243875480568\n",
      "Gradient Descent(34/49): loss=0.3526231415535356\n",
      "Gradient Descent(35/49): loss=0.35253014532425375\n",
      "Gradient Descent(36/49): loss=0.3524445335235484\n",
      "Gradient Descent(37/49): loss=0.35236554319283486\n",
      "Gradient Descent(38/49): loss=0.35229250093911696\n",
      "Gradient Descent(39/49): loss=0.3522248117776925\n",
      "Gradient Descent(40/49): loss=0.35216194941067575\n",
      "Gradient Descent(41/49): loss=0.35210344775195507\n",
      "Gradient Descent(42/49): loss=0.35204889353489865\n",
      "Gradient Descent(43/49): loss=0.3519979198611685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=0.35195020056797527\n",
      "Gradient Descent(45/49): loss=0.3519054453074436\n",
      "Gradient Descent(46/49): loss=0.3518633952458633\n",
      "Gradient Descent(47/49): loss=0.3518238193027803\n",
      "Gradient Descent(48/49): loss=0.35178651086041907\n",
      "Gradient Descent(49/49): loss=0.3517512848830447\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39574232885701116\n",
      "Gradient Descent(2/49): loss=0.3851868945714402\n",
      "Gradient Descent(3/49): loss=0.3794245410245657\n",
      "Gradient Descent(4/49): loss=0.3753515396447365\n",
      "Gradient Descent(5/49): loss=0.3721398966749676\n",
      "Gradient Descent(6/49): loss=0.36948290623400976\n",
      "Gradient Descent(7/49): loss=0.3672316999820238\n",
      "Gradient Descent(8/49): loss=0.3652983855501386\n",
      "Gradient Descent(9/49): loss=0.3636239923512435\n",
      "Gradient Descent(10/49): loss=0.3621655324892551\n",
      "Gradient Descent(11/49): loss=0.3608898934974408\n",
      "Gradient Descent(12/49): loss=0.359770600905877\n",
      "Gradient Descent(13/49): loss=0.3587859413869238\n",
      "Gradient Descent(14/49): loss=0.35791779097215204\n",
      "Gradient Descent(15/49): loss=0.35715083516491625\n",
      "Gradient Descent(16/49): loss=0.35647202048487087\n",
      "Gradient Descent(17/49): loss=0.35587015051403464\n",
      "Gradient Descent(18/49): loss=0.35533557698178625\n",
      "Gradient Descent(19/49): loss=0.3548599564114974\n",
      "Gradient Descent(20/49): loss=0.35443605395522265\n",
      "Gradient Descent(21/49): loss=0.3540575824558744\n",
      "Gradient Descent(22/49): loss=0.35371906862358715\n",
      "Gradient Descent(23/49): loss=0.3534157406090728\n",
      "Gradient Descent(24/49): loss=0.3531434328047062\n",
      "Gradient Descent(25/49): loss=0.35289850473998946\n",
      "Gradient Descent(26/49): loss=0.35267777165487557\n",
      "Gradient Descent(27/49): loss=0.35247844484612917\n",
      "Gradient Descent(28/49): loss=0.3522980802577195\n",
      "Gradient Descent(29/49): loss=0.3521345340693803\n",
      "Gradient Descent(30/49): loss=0.3519859242556655\n",
      "Gradient Descent(31/49): loss=0.3518505972592951\n",
      "Gradient Descent(32/49): loss=0.35172709905962485\n",
      "Gradient Descent(33/49): loss=0.35161415002817964\n",
      "Gradient Descent(34/49): loss=0.35151062305435876\n",
      "Gradient Descent(35/49): loss=0.35141552449999386\n",
      "Gradient Descent(36/49): loss=0.35132797760461304\n",
      "Gradient Descent(37/49): loss=0.3512472080164409\n",
      "Gradient Descent(38/49): loss=0.35117253116918945\n",
      "Gradient Descent(39/49): loss=0.3511033412629964\n",
      "Gradient Descent(40/49): loss=0.351039101640582\n",
      "Gradient Descent(41/49): loss=0.3509793363777343\n",
      "Gradient Descent(42/49): loss=0.35092362293131896\n",
      "Gradient Descent(43/49): loss=0.3508715857087578\n",
      "Gradient Descent(44/49): loss=0.35082289044082504\n",
      "Gradient Descent(45/49): loss=0.35077723925507603\n",
      "Gradient Descent(46/49): loss=0.35073436636061756\n",
      "Gradient Descent(47/49): loss=0.35069403426652457\n",
      "Gradient Descent(48/49): loss=0.3506560304662679\n",
      "Gradient Descent(49/49): loss=0.35062016452924866\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39848237104330886\n",
      "Gradient Descent(2/49): loss=0.38790985975410835\n",
      "Gradient Descent(3/49): loss=0.38215661333647394\n",
      "Gradient Descent(4/49): loss=0.378104451269083\n",
      "Gradient Descent(5/49): loss=0.37491784994503025\n",
      "Gradient Descent(6/49): loss=0.37228638905495204\n",
      "Gradient Descent(7/49): loss=0.370059494716205\n",
      "Gradient Descent(8/49): loss=0.3681485279501039\n",
      "Gradient Descent(9/49): loss=0.36649421175556823\n",
      "Gradient Descent(10/49): loss=0.36505346768548985\n",
      "Gradient Descent(11/49): loss=0.3637932111022802\n",
      "Gradient Descent(12/49): loss=0.3626870663305013\n",
      "Gradient Descent(13/49): loss=0.36171346245704983\n",
      "Gradient Descent(14/49): loss=0.36085444329010946\n",
      "Gradient Descent(15/49): loss=0.3600948743309041\n",
      "Gradient Descent(16/49): loss=0.35942188473748155\n",
      "Gradient Descent(17/49): loss=0.3588244565604364\n",
      "Gradient Descent(18/49): loss=0.358293111227856\n",
      "Gradient Descent(19/49): loss=0.35781966332530546\n",
      "Gradient Descent(20/49): loss=0.3573970228796537\n",
      "Gradient Descent(21/49): loss=0.3570190338316178\n",
      "Gradient Descent(22/49): loss=0.35668034029526785\n",
      "Gradient Descent(23/49): loss=0.3563762746618894\n",
      "Gradient Descent(24/49): loss=0.35610276320902545\n",
      "Gradient Descent(25/49): loss=0.3558562459574187\n",
      "Gradient Descent(26/49): loss=0.3556336082718145\n",
      "Gradient Descent(27/49): loss=0.3554321222411231\n",
      "Gradient Descent(28/49): loss=0.35524939626993673\n",
      "Gradient Descent(29/49): loss=0.3550833316115265\n",
      "Gradient Descent(30/49): loss=0.35493208480124855\n",
      "Gradient Descent(31/49): loss=0.35479403512808316\n",
      "Gradient Descent(32/49): loss=0.35466775642399406\n",
      "Gradient Descent(33/49): loss=0.35455199256508624\n",
      "Gradient Descent(34/49): loss=0.35444563617166575\n",
      "Gradient Descent(35/49): loss=0.3543477100709804\n",
      "Gradient Descent(36/49): loss=0.3542573511501081\n",
      "Gradient Descent(37/49): loss=0.35417379627975953\n",
      "Gradient Descent(38/49): loss=0.3540963700346633\n",
      "Gradient Descent(39/49): loss=0.35402447397422154\n",
      "Gradient Descent(40/49): loss=0.3539575772794699\n",
      "Gradient Descent(41/49): loss=0.35389520857000367\n",
      "Gradient Descent(42/49): loss=0.35383694874819205\n",
      "Gradient Descent(43/49): loss=0.3537824247383414\n",
      "Gradient Descent(44/49): loss=0.35373130400596753\n",
      "Gradient Descent(45/49): loss=0.35368328975744323\n",
      "Gradient Descent(46/49): loss=0.35363811673333145\n",
      "Gradient Descent(47/49): loss=0.3535955475200118\n",
      "Gradient Descent(48/49): loss=0.3535553693139815\n",
      "Gradient Descent(49/49): loss=0.3535173910816969\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39743908432807135\n",
      "Gradient Descent(2/49): loss=0.3868971596541175\n",
      "Gradient Descent(3/49): loss=0.3811930720592217\n",
      "Gradient Descent(4/49): loss=0.37711929294142427\n",
      "Gradient Descent(5/49): loss=0.3739050725663472\n",
      "Gradient Descent(6/49): loss=0.3712593148494937\n",
      "Gradient Descent(7/49): loss=0.36903547327766867\n",
      "Gradient Descent(8/49): loss=0.36714453963804355\n",
      "Gradient Descent(9/49): loss=0.3655257714184636\n",
      "Gradient Descent(10/49): loss=0.3641345380193713\n",
      "Gradient Descent(11/49): loss=0.3629364360261539\n",
      "Gradient Descent(12/49): loss=0.36190410240982795\n",
      "Gradient Descent(13/49): loss=0.36101533099959354\n",
      "Gradient Descent(14/49): loss=0.36025187530624675\n",
      "Gradient Descent(15/49): loss=0.3595986396328256\n",
      "Gradient Descent(16/49): loss=0.35904310451415933\n",
      "Gradient Descent(17/49): loss=0.35857490234708117\n",
      "Gradient Descent(18/49): loss=0.35818549478805606\n",
      "Gradient Descent(19/49): loss=0.3578679226511146\n",
      "Gradient Descent(20/49): loss=0.35761660978638116\n",
      "Gradient Descent(21/49): loss=0.3574272087228456\n",
      "Gradient Descent(22/49): loss=0.35729647972289275\n",
      "Gradient Descent(23/49): loss=0.3572221973704689\n",
      "Gradient Descent(24/49): loss=0.3572030804688545\n",
      "Gradient Descent(25/49): loss=0.3572387421771468\n",
      "Gradient Descent(26/49): loss=0.35732965815203543\n",
      "Gradient Descent(27/49): loss=0.3574771510945467\n",
      "Gradient Descent(28/49): loss=0.3576833905991878\n",
      "Gradient Descent(29/49): loss=0.35795140760973915\n",
      "Gradient Descent(30/49): loss=0.35828512313130406\n",
      "Gradient Descent(31/49): loss=0.35868939115246273\n",
      "Gradient Descent(32/49): loss=0.3591700560090669\n",
      "Gradient Descent(33/49): loss=0.35973402468319715\n",
      "Gradient Descent(34/49): loss=0.36038935478557355\n",
      "Gradient Descent(35/49): loss=0.3611453592241908\n",
      "Gradient Descent(36/49): loss=0.36201272882208746\n",
      "Gradient Descent(37/49): loss=0.3630036744183023\n",
      "Gradient Descent(38/49): loss=0.3641320902732539\n",
      "Gradient Descent(39/49): loss=0.3654137409079204\n",
      "Gradient Descent(40/49): loss=0.36686647384023136\n",
      "Gradient Descent(41/49): loss=0.36851046104720164\n",
      "Gradient Descent(42/49): loss=0.3703684723828853\n",
      "Gradient Descent(43/49): loss=0.3724661846261317\n",
      "Gradient Descent(44/49): loss=0.37483253032464936\n",
      "Gradient Descent(45/49): loss=0.3775000911499831\n",
      "Gradient Descent(46/49): loss=0.3805055410893459\n",
      "Gradient Descent(47/49): loss=0.3838901454833157\n",
      "Gradient Descent(48/49): loss=0.38770032268255783\n",
      "Gradient Descent(49/49): loss=0.39198827595261027\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3972205452091175\n",
      "Gradient Descent(2/49): loss=0.38632255051242603\n",
      "Gradient Descent(3/49): loss=0.3804507092085874\n",
      "Gradient Descent(4/49): loss=0.37627798188607425\n",
      "Gradient Descent(5/49): loss=0.37299284311411196\n",
      "Gradient Descent(6/49): loss=0.3702890149020761\n",
      "Gradient Descent(7/49): loss=0.3680135497558924\n",
      "Gradient Descent(8/49): loss=0.3660741291421131\n",
      "Gradient Descent(9/49): loss=0.36440794790080605\n",
      "Gradient Descent(10/49): loss=0.3629688821072751\n",
      "Gradient Descent(11/49): loss=0.361721308006736\n",
      "Gradient Descent(12/49): loss=0.3606367650451006\n",
      "Gradient Descent(13/49): loss=0.35969198707033745\n",
      "Gradient Descent(14/49): loss=0.3588676518429462\n",
      "Gradient Descent(15/49): loss=0.3581475354086398\n",
      "Gradient Descent(16/49): loss=0.35751790941143774\n",
      "Gradient Descent(17/49): loss=0.3569670927793412\n",
      "Gradient Descent(18/49): loss=0.35648510675247347\n",
      "Gradient Descent(19/49): loss=0.35606340236520617\n",
      "Gradient Descent(20/49): loss=0.35569464078608876\n",
      "Gradient Descent(21/49): loss=0.35537251352373794\n",
      "Gradient Descent(22/49): loss=0.3550915935312014\n",
      "Gradient Descent(23/49): loss=0.35484721079230597\n",
      "Gradient Descent(24/49): loss=0.35463534765219384\n",
      "Gradient Descent(25/49): loss=0.354452550298148\n",
      "Gradient Descent(26/49): loss=0.3542958536016667\n",
      "Gradient Descent(27/49): loss=0.3541627171157346\n",
      "Gradient Descent(28/49): loss=0.3540509704546151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=0.3539587666131339\n",
      "Gradient Descent(30/49): loss=0.35388454203823994\n",
      "Gradient Descent(31/49): loss=0.3538269824676401\n",
      "Gradient Descent(32/49): loss=0.35378499371222305\n",
      "Gradient Descent(33/49): loss=0.3537576766904369\n",
      "Gradient Descent(34/49): loss=0.3537443061306661\n",
      "Gradient Descent(35/49): loss=0.35374431244700155\n",
      "Gradient Descent(36/49): loss=0.35375726636837995\n",
      "Gradient Descent(37/49): loss=0.3537828659637329\n",
      "Gradient Descent(38/49): loss=0.3538209257587427\n",
      "Gradient Descent(39/49): loss=0.35387136768475524\n",
      "Gradient Descent(40/49): loss=0.35393421363874517\n",
      "Gradient Descent(41/49): loss=0.3540095794660395\n",
      "Gradient Descent(42/49): loss=0.3540976702056891\n",
      "Gradient Descent(43/49): loss=0.35419877646266507\n",
      "Gradient Descent(44/49): loss=0.354313271792047\n",
      "Gradient Descent(45/49): loss=0.35444161099857613\n",
      "Gradient Descent(46/49): loss=0.35458432927078165\n",
      "Gradient Descent(47/49): loss=0.35474204208272375\n",
      "Gradient Descent(48/49): loss=0.354915445808516\n",
      "Gradient Descent(49/49): loss=0.3551053190054665\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3953795890827875\n",
      "Gradient Descent(2/49): loss=0.38469800105462054\n",
      "Gradient Descent(3/49): loss=0.3789943502484178\n",
      "Gradient Descent(4/49): loss=0.3749430059127474\n",
      "Gradient Descent(5/49): loss=0.3717449435650566\n",
      "Gradient Descent(6/49): loss=0.3691035893485802\n",
      "Gradient Descent(7/49): loss=0.36687261261033005\n",
      "Gradient Descent(8/49): loss=0.36496431357853876\n",
      "Gradient Descent(9/49): loss=0.3633192238376664\n",
      "Gradient Descent(10/49): loss=0.3618936894502803\n",
      "Gradient Descent(11/49): loss=0.36065394324472516\n",
      "Gradient Descent(12/49): loss=0.3595729333525664\n",
      "Gradient Descent(13/49): loss=0.35862846847175417\n",
      "Gradient Descent(14/49): loss=0.35780204892614476\n",
      "Gradient Descent(15/49): loss=0.35707808079371733\n",
      "Gradient Descent(16/49): loss=0.356443317750489\n",
      "Gradient Descent(17/49): loss=0.35588644634703315\n",
      "Gradient Descent(18/49): loss=0.3553977666115668\n",
      "Gradient Descent(19/49): loss=0.35496893915025163\n",
      "Gradient Descent(20/49): loss=0.35459278063035843\n",
      "Gradient Descent(21/49): loss=0.3542630957374527\n",
      "Gradient Descent(22/49): loss=0.3539745374399073\n",
      "Gradient Descent(23/49): loss=0.35372248974199577\n",
      "Gradient Descent(24/49): loss=0.3535029686376249\n",
      "Gradient Descent(25/49): loss=0.35331253801192813\n",
      "Gradient Descent(26/49): loss=0.3531482379621949\n",
      "Gradient Descent(27/49): loss=0.35300752353241316\n",
      "Gradient Descent(28/49): loss=0.35288821224381606\n",
      "Gradient Descent(29/49): loss=0.3527884390991797\n",
      "Gradient Descent(30/49): loss=0.35270661796829583\n",
      "Gradient Descent(31/49): loss=0.3526414084439561\n",
      "Gradient Descent(32/49): loss=0.35259168740416985\n",
      "Gradient Descent(33/49): loss=0.35255652463568443\n",
      "Gradient Descent(34/49): loss=0.35253516197226253\n",
      "Gradient Descent(35/49): loss=0.3525269954830409\n",
      "Gradient Descent(36/49): loss=0.3525315603149319\n",
      "Gradient Descent(37/49): loss=0.3525485178509606\n",
      "Gradient Descent(38/49): loss=0.3525776448955851\n",
      "Gradient Descent(39/49): loss=0.3526188246399521\n",
      "Gradient Descent(40/49): loss=0.3526720391959131\n",
      "Gradient Descent(41/49): loss=0.352737363518441\n",
      "Gradient Descent(42/49): loss=0.35281496056264733\n",
      "Gradient Descent(43/49): loss=0.352905077544566\n",
      "Gradient Descent(44/49): loss=0.353008043194783\n",
      "Gradient Descent(45/49): loss=0.3531242659113182\n",
      "Gradient Descent(46/49): loss=0.3532542327332821\n",
      "Gradient Descent(47/49): loss=0.3533985090700784\n",
      "Gradient Descent(48/49): loss=0.35355773913255567\n",
      "Gradient Descent(49/49): loss=0.353732647022802\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3980950317560845\n",
      "Gradient Descent(2/49): loss=0.38741014520871153\n",
      "Gradient Descent(3/49): loss=0.3817131430231088\n",
      "Gradient Descent(4/49): loss=0.3776801631057474\n",
      "Gradient Descent(5/49): loss=0.374504704255849\n",
      "Gradient Descent(6/49): loss=0.37188653104978464\n",
      "Gradient Descent(7/49): loss=0.3696777240596305\n",
      "Gradient Descent(8/49): loss=0.36778994116605784\n",
      "Gradient Descent(9/49): loss=0.3661634986756432\n",
      "Gradient Descent(10/49): loss=0.36475474136924624\n",
      "Gradient Descent(11/49): loss=0.3635300241039511\n",
      "Gradient Descent(12/49): loss=0.3624624956991957\n",
      "Gradient Descent(13/49): loss=0.3615302178191546\n",
      "Gradient Descent(14/49): loss=0.3607149783292945\n",
      "Gradient Descent(15/49): loss=0.360001493052989\n",
      "Gradient Descent(16/49): loss=0.35937683921627506\n",
      "Gradient Descent(17/49): loss=0.3588300355133281\n",
      "Gradient Descent(18/49): loss=0.3583517200664119\n",
      "Gradient Descent(19/49): loss=0.35793389690880517\n",
      "Gradient Descent(20/49): loss=0.3575697324115116\n",
      "Gradient Descent(21/49): loss=0.3572533893661413\n",
      "Gradient Descent(22/49): loss=0.35697989026506094\n",
      "Gradient Descent(23/49): loss=0.35674500374676554\n",
      "Gradient Descent(24/49): loss=0.356545149772246\n",
      "Gradient Descent(25/49): loss=0.3563773201874646\n",
      "Gradient Descent(26/49): loss=0.3562390120937107\n",
      "Gradient Descent(27/49): loss=0.3561281720029356\n",
      "Gradient Descent(28/49): loss=0.3560431491680224\n",
      "Gradient Descent(29/49): loss=0.35598265679220364\n",
      "Gradient Descent(30/49): loss=0.35594574006617546\n",
      "Gradient Descent(31/49): loss=0.35593175017512974\n",
      "Gradient Descent(32/49): loss=0.35594032357414446\n",
      "Gradient Descent(33/49): loss=0.355971365958431\n",
      "Gradient Descent(34/49): loss=0.35602504046147376\n",
      "Gradient Descent(35/49): loss=0.356101759704\n",
      "Gradient Descent(36/49): loss=0.3562021813936207\n",
      "Gradient Descent(37/49): loss=0.35632720724165323\n",
      "Gradient Descent(38/49): loss=0.3564779850222279\n",
      "Gradient Descent(39/49): loss=0.3566559136510002\n",
      "Gradient Descent(40/49): loss=0.3568626512079953\n",
      "Gradient Descent(41/49): loss=0.35710012587244677\n",
      "Gradient Descent(42/49): loss=0.35737054977787963\n",
      "Gradient Descent(43/49): loss=0.3576764358339488\n",
      "Gradient Descent(44/49): loss=0.35802061759836484\n",
      "Gradient Descent(45/49): loss=0.3584062723182399\n",
      "Gradient Descent(46/49): loss=0.358836947295935\n",
      "Gradient Descent(47/49): loss=0.3593165897704808\n",
      "Gradient Descent(48/49): loss=0.35984958054238836\n",
      "Gradient Descent(49/49): loss=0.3604407716075975\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39722818292558093\n",
      "Gradient Descent(2/49): loss=0.3864503639455904\n",
      "Gradient Descent(3/49): loss=0.38080947347050864\n",
      "Gradient Descent(4/49): loss=0.37678253880902746\n",
      "Gradient Descent(5/49): loss=0.3736255198874488\n",
      "Gradient Descent(6/49): loss=0.37105918428394563\n",
      "Gradient Descent(7/49): loss=0.36894368611001105\n",
      "Gradient Descent(8/49): loss=0.3671964220306084\n",
      "Gradient Descent(9/49): loss=0.36576443686983257\n",
      "Gradient Descent(10/49): loss=0.36461313801517203\n",
      "Gradient Descent(11/49): loss=0.3637211448987108\n",
      "Gradient Descent(12/49): loss=0.3630779092219364\n",
      "Gradient Descent(13/49): loss=0.3626828050180173\n",
      "Gradient Descent(14/49): loss=0.3625451431263804\n",
      "Gradient Descent(15/49): loss=0.36268488641387325\n",
      "Gradient Descent(16/49): loss=0.3631339989202837\n",
      "Gradient Descent(17/49): loss=0.36393845135222425\n",
      "Gradient Descent(18/49): loss=0.36516096711967183\n",
      "Gradient Descent(19/49): loss=0.36688464592699904\n",
      "Gradient Descent(20/49): loss=0.36921765554166197\n",
      "Gradient Descent(21/49): loss=0.3722992429159037\n",
      "Gradient Descent(22/49): loss=0.37630738817750553\n",
      "Gradient Descent(23/49): loss=0.3814685138201584\n",
      "Gradient Descent(24/49): loss=0.38806977189169994\n",
      "Gradient Descent(25/49): loss=0.39647457024360166\n",
      "Gradient Descent(26/49): loss=0.4071421725009127\n",
      "Gradient Descent(27/49): loss=0.42065242471226244\n",
      "Gradient Descent(28/49): loss=0.43773693638666905\n",
      "Gradient Descent(29/49): loss=0.4593183895705023\n",
      "Gradient Descent(30/49): loss=0.4865600853260508\n",
      "Gradient Descent(31/49): loss=0.520928385813363\n",
      "Gradient Descent(32/49): loss=0.5642714015835472\n",
      "Gradient Descent(33/49): loss=0.6189181447432851\n",
      "Gradient Descent(34/49): loss=0.6878034660575733\n",
      "Gradient Descent(35/49): loss=0.7746254766707821\n",
      "Gradient Descent(36/49): loss=0.8840438970888477\n",
      "Gradient Descent(37/49): loss=1.0219299707920555\n",
      "Gradient Descent(38/49): loss=1.1956813450355412\n",
      "Gradient Descent(39/49): loss=1.4146188053275444\n",
      "Gradient Descent(40/49): loss=1.6904861395894246\n",
      "Gradient Descent(41/49): loss=2.038079938487586\n",
      "Gradient Descent(42/49): loss=2.4760431064715376\n",
      "Gradient Descent(43/49): loss=3.0278646373254996\n",
      "Gradient Descent(44/49): loss=3.723139269352099\n",
      "Gradient Descent(45/49): loss=4.599154571835221\n",
      "Gradient Descent(46/49): loss=5.702890573551356\n",
      "Gradient Descent(47/49): loss=7.0935391674573784\n",
      "Gradient Descent(48/49): loss=8.845678399679407\n",
      "Gradient Descent(49/49): loss=11.053271870371004\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39702687632362554\n",
      "Gradient Descent(2/49): loss=0.3858565254281623\n",
      "Gradient Descent(3/49): loss=0.3800437953021741\n",
      "Gradient Descent(4/49): loss=0.375908738219688\n",
      "Gradient Descent(5/49): loss=0.37266368756260065\n",
      "Gradient Descent(6/49): loss=0.3700124417567691\n",
      "Gradient Descent(7/49): loss=0.36780580156071985\n",
      "Gradient Descent(8/49): loss=0.3659536828766203\n",
      "Gradient Descent(9/49): loss=0.36439550783621577\n",
      "Gradient Descent(10/49): loss=0.3630879066016275\n",
      "Gradient Descent(11/49): loss=0.361998801679739\n",
      "Gradient Descent(12/49): loss=0.3611042659891176\n",
      "Gradient Descent(13/49): loss=0.3603867422099159\n",
      "Gradient Descent(14/49): loss=0.3598340006223989\n",
      "Gradient Descent(15/49): loss=0.3594385383546698\n",
      "Gradient Descent(16/49): loss=0.35919727121555795\n",
      "Gradient Descent(17/49): loss=0.35911144213408586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=0.3591867086858315\n",
      "Gradient Descent(19/49): loss=0.3594333942236091\n",
      "Gradient Descent(20/49): loss=0.35986690112417785\n",
      "Gradient Descent(21/49): loss=0.3605082947406366\n",
      "Gradient Descent(22/49): loss=0.3613850749659065\n",
      "Gradient Descent(23/49): loss=0.36253216007760064\n",
      "Gradient Descent(24/49): loss=0.3639931155032167\n",
      "Gradient Descent(25/49): loss=0.3658216688540687\n",
      "Gradient Descent(26/49): loss=0.3680835624705244\n",
      "Gradient Descent(27/49): loss=0.3708588062194836\n",
      "Gradient Descent(28/49): loss=0.37424440682539617\n",
      "Gradient Descent(29/49): loss=0.37835766608465576\n",
      "Gradient Descent(30/49): loss=0.3833401594692893\n",
      "Gradient Descent(31/49): loss=0.3893625295257731\n",
      "Gradient Descent(32/49): loss=0.39663025589664846\n",
      "Gradient Descent(33/49): loss=0.4053905966642269\n",
      "Gradient Descent(34/49): loss=0.41594093514684305\n",
      "Gradient Descent(35/49): loss=0.42863881359860867\n",
      "Gradient Descent(36/49): loss=0.4439139920683593\n",
      "Gradient Descent(37/49): loss=0.4622829388766647\n",
      "Gradient Descent(38/49): loss=0.4843662410690465\n",
      "Gradient Descent(39/49): loss=0.5109095215582719\n",
      "Gradient Descent(40/49): loss=0.5428085677922283\n",
      "Gradient Descent(41/49): loss=0.5811395186555618\n",
      "Gradient Descent(42/49): loss=0.627195126713047\n",
      "Gradient Descent(43/49): loss=0.6825283175705694\n",
      "Gradient Descent(44/49): loss=0.7490045139614835\n",
      "Gradient Descent(45/49): loss=0.8288644874430314\n",
      "Gradient Descent(46/49): loss=0.9247998552583596\n",
      "Gradient Descent(47/49): loss=1.0400437659324389\n",
      "Gradient Descent(48/49): loss=1.1784798288774962\n",
      "Gradient Descent(49/49): loss=1.3447729579257206\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.395192322790321\n",
      "Gradient Descent(2/49): loss=0.38424087985906175\n",
      "Gradient Descent(3/49): loss=0.3785956632320434\n",
      "Gradient Descent(4/49): loss=0.3745784077937143\n",
      "Gradient Descent(5/49): loss=0.3714154160044712\n",
      "Gradient Descent(6/49): loss=0.36882050951975004\n",
      "Gradient Descent(7/49): loss=0.36665113483901796\n",
      "Gradient Descent(8/49): loss=0.36482173617240915\n",
      "Gradient Descent(9/49): loss=0.3632748963866719\n",
      "Gradient Descent(10/49): loss=0.36196946126648627\n",
      "Gradient Descent(11/49): loss=0.3608748770694166\n",
      "Gradient Descent(12/49): loss=0.3599682095329755\n",
      "Gradient Descent(13/49): loss=0.35923246864159475\n",
      "Gradient Descent(14/49): loss=0.35865563529590355\n",
      "Gradient Descent(15/49): loss=0.35823010337705136\n",
      "Gradient Descent(16/49): loss=0.3579523946385281\n",
      "Gradient Descent(17/49): loss=0.35782307423213344\n",
      "Gradient Descent(18/49): loss=0.35784683157061836\n",
      "Gradient Descent(19/49): loss=0.3580327121841752\n",
      "Gradient Descent(20/49): loss=0.3583944994069484\n",
      "Gradient Descent(21/49): loss=0.3589512541421327\n",
      "Gradient Descent(22/49): loss=0.35972802868239667\n",
      "Gradient Descent(23/49): loss=0.36075677776912024\n",
      "Gradient Descent(24/49): loss=0.3620774974774437\n",
      "Gradient Descent(25/49): loss=0.363739630621424\n",
      "Gradient Descent(26/49): loss=0.3658037866013905\n",
      "Gradient Descent(27/49): loss=0.3683438343568776\n",
      "Gradient Descent(28/49): loss=0.3714494397535336\n",
      "Gradient Descent(29/49): loss=0.37522913377723766\n",
      "Gradient Descent(30/49): loss=0.37981401585886987\n",
      "Gradient Descent(31/49): loss=0.3853622181261116\n",
      "Gradient Descent(32/49): loss=0.39206428210708644\n",
      "Gradient Descent(33/49): loss=0.40014963026897826\n",
      "Gradient Descent(34/49): loss=0.4098943518097454\n",
      "Gradient Descent(35/49): loss=0.42163056658772696\n",
      "Gradient Descent(36/49): loss=0.4357576844781958\n",
      "Gradient Descent(37/49): loss=0.45275594159638743\n",
      "Gradient Descent(38/49): loss=0.4732026718950818\n",
      "Gradient Descent(39/49): loss=0.49779186524081565\n",
      "Gradient Descent(40/49): loss=0.527357674331361\n",
      "Gradient Descent(41/49): loss=0.562902666505036\n",
      "Gradient Descent(42/49): loss=0.6056317771361593\n",
      "Gradient Descent(43/49): loss=0.6569931143495886\n",
      "Gradient Descent(44/49): loss=0.7187269967564537\n",
      "Gradient Descent(45/49): loss=0.7929248846676198\n",
      "Gradient Descent(46/49): loss=0.882100200218808\n",
      "Gradient Descent(47/49): loss=0.9892734343837855\n",
      "Gradient Descent(48/49): loss=1.1180744225894597\n",
      "Gradient Descent(49/49): loss=1.2728652519574966\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3978764855972042\n",
      "Gradient Descent(2/49): loss=0.38694008621388154\n",
      "Gradient Descent(3/49): loss=0.38129626862799376\n",
      "Gradient Descent(4/49): loss=0.3772903293791738\n",
      "Gradient Descent(5/49): loss=0.37414134859294074\n",
      "Gradient Descent(6/49): loss=0.37155953593495267\n",
      "Gradient Descent(7/49): loss=0.36940088852762154\n",
      "Gradient Descent(8/49): loss=0.3675793962913861\n",
      "Gradient Descent(9/49): loss=0.36603770018736637\n",
      "Gradient Descent(10/49): loss=0.3647350549958728\n",
      "Gradient Descent(11/49): loss=0.36364163358952556\n",
      "Gradient Descent(12/49): loss=0.3627355701815317\n",
      "Gradient Descent(13/49): loss=0.36200134602982365\n",
      "Gradient Descent(14/49): loss=0.36142891100814595\n",
      "Gradient Descent(15/49): loss=0.36101325867711676\n",
      "Gradient Descent(16/49): loss=0.3607543198298182\n",
      "Gradient Descent(17/49): loss=0.360657112561792\n",
      "Gradient Descent(18/49): loss=0.36073212680807953\n",
      "Gradient Descent(19/49): loss=0.36099594593583784\n",
      "Gradient Descent(20/49): loss=0.3614721257755602\n",
      "Gradient Descent(21/49): loss=0.3621923666451858\n",
      "Gradient Descent(22/49): loss=0.3631980288191976\n",
      "Gradient Descent(23/49): loss=0.36454205802320844\n",
      "Gradient Descent(24/49): loss=0.36629140604413213\n",
      "Gradient Descent(25/49): loss=0.3685300534862843\n",
      "Gradient Descent(26/49): loss=0.3713627681674604\n",
      "Gradient Descent(27/49): loss=0.3749197648697324\n",
      "Gradient Descent(28/49): loss=0.3793624715915387\n",
      "Gradient Descent(29/49): loss=0.38489065584251225\n",
      "Gradient Descent(30/49): loss=0.391751224014839\n",
      "Gradient Descent(31/49): loss=0.4002490800689182\n",
      "Gradient Descent(32/49): loss=0.4107605198981732\n",
      "Gradient Descent(33/49): loss=0.423749748738467\n",
      "Gradient Descent(34/49): loss=0.4397892457252987\n",
      "Gradient Descent(35/49): loss=0.45958486816714367\n",
      "Gradient Descent(36/49): loss=0.4840067956755155\n",
      "Gradient Descent(37/49): loss=0.5141276700643922\n",
      "Gradient Descent(38/49): loss=0.5512696021071782\n",
      "Gradient Descent(39/49): loss=0.5970621046248309\n",
      "Gradient Descent(40/49): loss=0.6535134899871508\n",
      "Gradient Descent(41/49): loss=0.7230988599062478\n",
      "Gradient Descent(42/49): loss=0.8088685422228372\n",
      "Gradient Descent(43/49): loss=0.9145817250722952\n",
      "Gradient Descent(44/49): loss=1.0448711426026482\n",
      "Gradient Descent(45/49): loss=1.2054460266566833\n",
      "Gradient Descent(46/49): loss=1.4033422151074484\n",
      "Gradient Descent(47/49): loss=1.6472303732871825\n",
      "Gradient Descent(48/49): loss=1.9477958306620733\n",
      "Gradient Descent(49/49): loss=2.31820667210152\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39718769358973893\n",
      "Gradient Descent(2/49): loss=0.3860398162789507\n",
      "Gradient Descent(3/49): loss=0.38046510457499616\n",
      "Gradient Descent(4/49): loss=0.3765158300583716\n",
      "Gradient Descent(5/49): loss=0.3734751616281537\n",
      "Gradient Descent(6/49): loss=0.3710892636312568\n",
      "Gradient Descent(7/49): loss=0.36924660893325917\n",
      "Gradient Descent(8/49): loss=0.36790336991597805\n",
      "Gradient Descent(9/49): loss=0.3670619728883334\n",
      "Gradient Descent(10/49): loss=0.3667672543921687\n",
      "Gradient Descent(11/49): loss=0.36711147386854726\n",
      "Gradient Descent(12/49): loss=0.3682461737760619\n",
      "Gradient Descent(13/49): loss=0.3704013313549209\n",
      "Gradient Descent(14/49): loss=0.37391366685444555\n",
      "Gradient Descent(15/49): loss=0.37926725965677344\n",
      "Gradient Descent(16/49): loss=0.3871511462241703\n",
      "Gradient Descent(17/49): loss=0.3985405865296673\n",
      "Gradient Descent(18/49): loss=0.4148114528532962\n",
      "Gradient Descent(19/49): loss=0.43790105090784404\n",
      "Gradient Descent(20/49): loss=0.4705340819962477\n",
      "Gradient Descent(21/49): loss=0.5165400263519603\n",
      "Gradient Descent(22/49): loss=0.5812988532195584\n",
      "Gradient Descent(23/49): loss=0.6723668782054075\n",
      "Gradient Descent(24/49): loss=0.8003555268964726\n",
      "Gradient Descent(25/49): loss=0.9801651596596751\n",
      "Gradient Descent(26/49): loss=1.2327173828535298\n",
      "Gradient Descent(27/49): loss=1.5873872135928504\n",
      "Gradient Descent(28/49): loss=2.0854178137447894\n",
      "Gradient Descent(29/49): loss=2.784714719772293\n",
      "Gradient Descent(30/49): loss=3.766576844001717\n",
      "Gradient Descent(31/49): loss=5.145146648486326\n",
      "Gradient Descent(32/49): loss=7.080677962982323\n",
      "Gradient Descent(33/49): loss=9.798163672969912\n",
      "Gradient Descent(34/49): loss=13.61348852322859\n",
      "Gradient Descent(35/49): loss=18.970146985660435\n",
      "Gradient Descent(36/49): loss=26.49079420050335\n",
      "Gradient Descent(37/49): loss=37.049622164914695\n",
      "Gradient Descent(38/49): loss=51.87397402544125\n",
      "Gradient Descent(39/49): loss=72.68700790609704\n",
      "Gradient Descent(40/49): loss=101.907993214842\n",
      "Gradient Descent(41/49): loss=142.9335214486253\n",
      "Gradient Descent(42/49): loss=200.53231884730462\n",
      "Gradient Descent(43/49): loss=281.39955308539214\n",
      "Gradient Descent(44/49): loss=394.93506544046346\n",
      "Gradient Descent(45/49): loss=554.3359885008656\n",
      "Gradient Descent(46/49): loss=778.1307529860479\n",
      "Gradient Descent(47/49): loss=1092.3327933900323\n",
      "Gradient Descent(48/49): loss=1533.464294640333\n",
      "Gradient Descent(49/49): loss=2152.8014536922456\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3970050227350492\n",
      "Gradient Descent(2/49): loss=0.3854274868693996\n",
      "Gradient Descent(3/49): loss=0.3796733332553932\n",
      "Gradient Descent(4/49): loss=0.37559949019938266\n",
      "Gradient Descent(5/49): loss=0.3724381940288293\n",
      "Gradient Descent(6/49): loss=0.3699102867903976\n",
      "Gradient Descent(7/49): loss=0.36788146446745135\n",
      "Gradient Descent(8/49): loss=0.36627986262092216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=0.36506943337663283\n",
      "Gradient Descent(10/49): loss=0.36424036660019254\n",
      "Gradient Descent(11/49): loss=0.3638064596399314\n",
      "Gradient Descent(12/49): loss=0.36380627268410426\n",
      "Gradient Descent(13/49): loss=0.36430708284730723\n",
      "Gradient Descent(14/49): loss=0.3654115369042362\n",
      "Gradient Descent(15/49): loss=0.3672673902376677\n",
      "Gradient Descent(16/49): loss=0.37008109447599496\n",
      "Gradient Descent(17/49): loss=0.3741363767514638\n",
      "Gradient Descent(18/49): loss=0.37981940619201904\n",
      "Gradient Descent(19/49): loss=0.3876527216008013\n",
      "Gradient Descent(20/49): loss=0.3983408546030949\n",
      "Gradient Descent(21/49): loss=0.41283159343995696\n",
      "Gradient Descent(22/49): loss=0.43239818272277314\n",
      "Gradient Descent(23/49): loss=0.45874956105023934\n",
      "Gradient Descent(24/49): loss=0.49417815769390994\n",
      "Gradient Descent(25/49): loss=0.5417580105577252\n",
      "Gradient Descent(26/49): loss=0.605610310122381\n",
      "Gradient Descent(27/49): loss=0.6912592929350371\n",
      "Gradient Descent(28/49): loss=0.8061092056362475\n",
      "Gradient Descent(29/49): loss=0.9600835095261211\n",
      "Gradient Descent(30/49): loss=1.16648149810734\n",
      "Gradient Descent(31/49): loss=1.4431262644432938\n",
      "Gradient Descent(32/49): loss=1.813903101074724\n",
      "Gradient Descent(33/49): loss=2.3108211129747582\n",
      "Gradient Descent(34/49): loss=2.9767759820167647\n",
      "Gradient Descent(35/49): loss=3.869252337244253\n",
      "Gradient Descent(36/49): loss=5.065285281950215\n",
      "Gradient Descent(37/49): loss=6.668109305609866\n",
      "Gradient Descent(38/49): loss=8.816068445924396\n",
      "Gradient Descent(39/49): loss=11.694556733494888\n",
      "Gradient Descent(40/49): loss=15.552019493748162\n",
      "Gradient Descent(41/49): loss=20.72139657123671\n",
      "Gradient Descent(42/49): loss=27.648858231015357\n",
      "Gradient Descent(43/49): loss=36.93231391910773\n",
      "Gradient Descent(44/49): loss=49.37301755469906\n",
      "Gradient Descent(45/49): loss=66.04472338187229\n",
      "Gradient Descent(46/49): loss=88.38636118885773\n",
      "Gradient Descent(47/49): loss=118.32622964782651\n",
      "Gradient Descent(48/49): loss=158.44842684173798\n",
      "Gradient Descent(49/49): loss=212.21588251569008\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39518052997961184\n",
      "Gradient Descent(2/49): loss=0.38381998112268195\n",
      "Gradient Descent(3/49): loss=0.3782311857792394\n",
      "Gradient Descent(4/49): loss=0.37426910690454723\n",
      "Gradient Descent(5/49): loss=0.3711808646839767\n",
      "Gradient Descent(6/49): loss=0.3686968866017198\n",
      "Gradient Descent(7/49): loss=0.36668852558266124\n",
      "Gradient Descent(8/49): loss=0.36508695992008094\n",
      "Gradient Descent(9/49): loss=0.36385721091147527\n",
      "Gradient Descent(10/49): loss=0.3629888081451239\n",
      "Gradient Descent(11/49): loss=0.3624931638184727\n",
      "Gradient Descent(12/49): loss=0.36240455026553975\n",
      "Gradient Descent(13/49): loss=0.3627837018485049\n",
      "Gradient Descent(14/49): loss=0.3637239213273681\n",
      "Gradient Descent(15/49): loss=0.3653600369133078\n",
      "Gradient Descent(16/49): loss=0.36788090921902367\n",
      "Gradient Descent(17/49): loss=0.37154654188201797\n",
      "Gradient Descent(18/49): loss=0.3767112695644553\n",
      "Gradient Descent(19/49): loss=0.38385503274327215\n",
      "Gradient Descent(20/49): loss=0.3936254528535801\n",
      "Gradient Descent(21/49): loss=0.40689435780366895\n",
      "Gradient Descent(22/49): loss=0.42483365909640913\n",
      "Gradient Descent(23/49): loss=0.4490171567647265\n",
      "Gradient Descent(24/49): loss=0.4815570923992058\n",
      "Gradient Descent(25/49): loss=0.5252872781569538\n",
      "Gradient Descent(26/49): loss=0.584008661255041\n",
      "Gradient Descent(27/49): loss=0.6628185881414921\n",
      "Gradient Descent(28/49): loss=0.76855227822615\n",
      "Gradient Descent(29/49): loss=0.9103747310395107\n",
      "Gradient Descent(30/49): loss=1.1005743139521047\n",
      "Gradient Descent(31/49): loss=1.3556267376119504\n",
      "Gradient Descent(32/49): loss=1.6976215346308063\n",
      "Gradient Descent(33/49): loss=2.1561745403394754\n",
      "Gradient Descent(34/49): loss=2.7709919496280957\n",
      "Gradient Descent(35/49): loss=3.5953079336318874\n",
      "Gradient Descent(36/49): loss=4.7004934279603985\n",
      "Gradient Descent(37/49): loss=6.182235097766512\n",
      "Gradient Descent(38/49): loss=8.168819422307424\n",
      "Gradient Descent(39/49): loss=10.832239091503231\n",
      "Gradient Descent(40/49): loss=14.403083247557802\n",
      "Gradient Descent(41/49): loss=19.19050068965725\n",
      "Gradient Descent(42/49): loss=25.60896434967878\n",
      "Gradient Descent(43/49): loss=34.21415416440246\n",
      "Gradient Descent(44/49): loss=45.75106489162684\n",
      "Gradient Descent(45/49): loss=61.21850378772385\n",
      "Gradient Descent(46/49): loss=81.95556201063057\n",
      "Gradient Descent(47/49): loss=109.75754597971748\n",
      "Gradient Descent(48/49): loss=147.0314054093808\n",
      "Gradient Descent(49/49): loss=197.00411414363256\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39782673256666745\n",
      "Gradient Descent(2/49): loss=0.3865035844823084\n",
      "Gradient Descent(3/49): loss=0.3809073485350654\n",
      "Gradient Descent(4/49): loss=0.37694227987251533\n",
      "Gradient Descent(5/49): loss=0.3738479786303242\n",
      "Gradient Descent(6/49): loss=0.3713500624248794\n",
      "Gradient Descent(7/49): loss=0.36931706759985017\n",
      "Gradient Descent(8/49): loss=0.3676782068838467\n",
      "Gradient Descent(9/49): loss=0.3663970406819148\n",
      "Gradient Descent(10/49): loss=0.36546221955320457\n",
      "Gradient Descent(11/49): loss=0.3648852186901734\n",
      "Gradient Descent(12/49): loss=0.36470197917170677\n",
      "Gradient Descent(13/49): loss=0.3649775884251697\n",
      "Gradient Descent(14/49): loss=0.36581406841183967\n",
      "Gradient Descent(15/49): loss=0.3673618994491906\n",
      "Gradient Descent(16/49): loss=0.3698363890495748\n",
      "Gradient Descent(17/49): loss=0.37354053201098325\n",
      "Gradient Descent(18/49): loss=0.3788966877091774\n",
      "Gradient Descent(19/49): loss=0.3864903060218601\n",
      "Gradient Descent(20/49): loss=0.397130162746645\n",
      "Gradient Descent(21/49): loss=0.4119312466262302\n",
      "Gradient Descent(22/49): loss=0.43242874557422206\n",
      "Gradient Descent(23/49): loss=0.4607347446907793\n",
      "Gradient Descent(24/49): loss=0.4997535956399092\n",
      "Gradient Descent(25/49): loss=0.553477888574276\n",
      "Gradient Descent(26/49): loss=0.6273951619920936\n",
      "Gradient Descent(27/49): loss=0.7290467578814375\n",
      "Gradient Descent(28/49): loss=0.8687957167094595\n",
      "Gradient Descent(29/49): loss=1.0608818857953346\n",
      "Gradient Descent(30/49): loss=1.3248716514957355\n",
      "Gradient Descent(31/49): loss=1.6876498766500443\n",
      "Gradient Descent(32/49): loss=2.1861568191567198\n",
      "Gradient Descent(33/49): loss=2.871148644001688\n",
      "Gradient Descent(34/49): loss=3.8123643394449984\n",
      "Gradient Descent(35/49): loss=5.105625015468907\n",
      "Gradient Descent(36/49): loss=6.882588272993054\n",
      "Gradient Descent(37/49): loss=9.324150610292529\n",
      "Gradient Descent(38/49): loss=12.678862191982297\n",
      "Gradient Descent(39/49): loss=17.28822854904601\n",
      "Gradient Descent(40/49): loss=23.621474846913845\n",
      "Gradient Descent(41/49): loss=32.32331161913077\n",
      "Gradient Descent(42/49): loss=44.27956437375516\n",
      "Gradient Descent(43/49): loss=60.707347967484615\n",
      "Gradient Descent(44/49): loss=83.27896522516079\n",
      "Gradient Descent(45/49): loss=114.29214230794445\n",
      "Gradient Descent(46/49): loss=156.9039302735659\n",
      "Gradient Descent(47/49): loss=215.4520832967894\n",
      "Gradient Descent(48/49): loss=295.89662887645306\n",
      "Gradient Descent(49/49): loss=406.42658053402505\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39731761632054535\n",
      "Gradient Descent(2/49): loss=0.385671691353402\n",
      "Gradient Descent(3/49): loss=0.380165055321185\n",
      "Gradient Descent(4/49): loss=0.3763401308655313\n",
      "Gradient Descent(5/49): loss=0.3735159705003627\n",
      "Gradient Descent(6/49): loss=0.3715034872330969\n",
      "Gradient Descent(7/49): loss=0.3702887026456159\n",
      "Gradient Descent(8/49): loss=0.36998316647667717\n",
      "Gradient Descent(9/49): loss=0.37083829545276775\n",
      "Gradient Descent(10/49): loss=0.3732968293441584\n",
      "Gradient Descent(11/49): loss=0.378084687954645\n",
      "Gradient Descent(12/49): loss=0.386359450150458\n",
      "Gradient Descent(13/49): loss=0.39994429458382297\n",
      "Gradient Descent(14/49): loss=0.4216936640041201\n",
      "Gradient Descent(15/49): loss=0.45606323184870173\n",
      "Gradient Descent(16/49): loss=0.509997373648547\n",
      "Gradient Descent(17/49): loss=0.5943104072852949\n",
      "Gradient Descent(18/49): loss=0.7258359111232718\n",
      "Gradient Descent(19/49): loss=0.9307709350063598\n",
      "Gradient Descent(20/49): loss=1.249879171555742\n",
      "Gradient Descent(21/49): loss=1.7465862645181292\n",
      "Gradient Descent(22/49): loss=2.5195746879741896\n",
      "Gradient Descent(23/49): loss=3.722379059579385\n",
      "Gradient Descent(24/49): loss=5.593872751856118\n",
      "Gradient Descent(25/49): loss=8.505699236121984\n",
      "Gradient Descent(26/49): loss=13.036066133439915\n",
      "Gradient Descent(27/49): loss=20.08455451827037\n",
      "Gradient Descent(28/49): loss=31.050739998656134\n",
      "Gradient Descent(29/49): loss=48.11209254835602\n",
      "Gradient Descent(30/49): loss=74.65633483357604\n",
      "Gradient Descent(31/49): loss=115.95410802291275\n",
      "Gradient Descent(32/49): loss=180.2055094728519\n",
      "Gradient Descent(33/49): loss=280.16830260366214\n",
      "Gradient Descent(34/49): loss=435.69109599134856\n",
      "Gradient Descent(35/49): loss=677.654479203373\n",
      "Gradient Descent(36/49): loss=1054.1026665097463\n",
      "Gradient Descent(37/49): loss=1639.7831463972243\n",
      "Gradient Descent(38/49): loss=2550.988527751543\n",
      "Gradient Descent(39/49): loss=3968.6475766612853\n",
      "Gradient Descent(40/49): loss=6174.250395405262\n",
      "Gradient Descent(41/49): loss=9605.741039839146\n",
      "Gradient Descent(42/49): loss=14944.475601893177\n",
      "Gradient Descent(43/49): loss=23250.51213631622\n",
      "Gradient Descent(44/49): loss=36173.09557193228\n",
      "Gradient Descent(45/49): loss=56278.13144846594\n",
      "Gradient Descent(46/49): loss=87557.6715973664\n",
      "Gradient Descent(47/49): loss=136222.57513950835\n",
      "Gradient Descent(48/49): loss=211935.73540586498\n",
      "Gradient Descent(49/49): loss=329730.742067098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39715498444338854\n",
      "Gradient Descent(2/49): loss=0.3850419980059753\n",
      "Gradient Descent(3/49): loss=0.3793436569609522\n",
      "Gradient Descent(4/49): loss=0.3753673515272903\n",
      "Gradient Descent(5/49): loss=0.372364982524669\n",
      "Gradient Descent(6/49): loss=0.370098027362785\n",
      "Gradient Descent(7/49): loss=0.36848730787634987\n",
      "Gradient Descent(8/49): loss=0.36754355000022254\n",
      "Gradient Descent(9/49): loss=0.36735662130277613\n",
      "Gradient Descent(10/49): loss=0.3681085479963295\n",
      "Gradient Descent(11/49): loss=0.37010447197366136\n",
      "Gradient Descent(12/49): loss=0.3738240999374641\n",
      "Gradient Descent(13/49): loss=0.38000102969859\n",
      "Gradient Descent(14/49): loss=0.3897423078308251\n",
      "Gradient Descent(15/49): loss=0.40470717832918257\n",
      "Gradient Descent(16/49): loss=0.4273734866389518\n",
      "Gradient Descent(17/49): loss=0.4614341945057507\n",
      "Gradient Descent(18/49): loss=0.512387195231843\n",
      "Gradient Descent(19/49): loss=0.5884124102579098\n",
      "Gradient Descent(20/49): loss=0.7016759060192963\n",
      "Gradient Descent(21/49): loss=0.870268785469346\n",
      "Gradient Descent(22/49): loss=1.121089716346414\n",
      "Gradient Descent(23/49): loss=1.4941302636276579\n",
      "Gradient Descent(24/49): loss=2.0488456383914464\n",
      "Gradient Descent(25/49): loss=2.8736256512363014\n",
      "Gradient Descent(26/49): loss=4.09987447655753\n",
      "Gradient Descent(27/49): loss=5.922941952828118\n",
      "Gradient Descent(28/49): loss=8.633240498642884\n",
      "Gradient Descent(29/49): loss=12.662504152532152\n",
      "Gradient Descent(30/49): loss=18.652558177073132\n",
      "Gradient Descent(31/49): loss=27.55755329254262\n",
      "Gradient Descent(32/49): loss=40.79594906331861\n",
      "Gradient Descent(33/49): loss=60.47645532060267\n",
      "Gradient Descent(34/49): loss=89.73392101035702\n",
      "Gradient Descent(35/49): loss=133.22867297917958\n",
      "Gradient Descent(36/49): loss=197.88884255489828\n",
      "Gradient Descent(37/49): loss=294.01392241801136\n",
      "Gradient Descent(38/49): loss=436.91533531534543\n",
      "Gradient Descent(39/49): loss=649.3553348248569\n",
      "Gradient Descent(40/49): loss=965.172721781395\n",
      "Gradient Descent(41/49): loss=1434.6729035112348\n",
      "Gradient Descent(42/49): loss=2132.6408587546043\n",
      "Gradient Descent(43/49): loss=3170.2533642361927\n",
      "Gradient Descent(44/49): loss=4712.787938012639\n",
      "Gradient Descent(45/49): loss=7005.9492925984305\n",
      "Gradient Descent(46/49): loss=10415.006742577936\n",
      "Gradient Descent(47/49): loss=15482.976621630429\n",
      "Gradient Descent(48/49): loss=23017.117373984653\n",
      "Gradient Descent(49/49): loss=34217.514807988184\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3953442106506599\n",
      "Gradient Descent(2/49): loss=0.38344183715208824\n",
      "Gradient Descent(3/49): loss=0.37790478475809736\n",
      "Gradient Descent(4/49): loss=0.37403060419988815\n",
      "Gradient Descent(5/49): loss=0.3710854883816201\n",
      "Gradient Descent(6/49): loss=0.3688378105463368\n",
      "Gradient Descent(7/49): loss=0.3672094958631226\n",
      "Gradient Descent(8/49): loss=0.3662072136936033\n",
      "Gradient Descent(9/49): loss=0.36591103630200655\n",
      "Gradient Descent(10/49): loss=0.36648584189159633\n",
      "Gradient Descent(11/49): loss=0.3682094863261787\n",
      "Gradient Descent(12/49): loss=0.37151989998675633\n",
      "Gradient Descent(13/49): loss=0.3770878281052807\n",
      "Gradient Descent(14/49): loss=0.3859265460275854\n",
      "Gradient Descent(15/49): loss=0.3995559779752348\n",
      "Gradient Descent(16/49): loss=0.42024740878462763\n",
      "Gradient Descent(17/49): loss=0.4513878730262268\n",
      "Gradient Descent(18/49): loss=0.4980224209804153\n",
      "Gradient Descent(19/49): loss=0.567660859023195\n",
      "Gradient Descent(20/49): loss=0.6714777813582146\n",
      "Gradient Descent(21/49): loss=0.8260974927502365\n",
      "Gradient Descent(22/49): loss=1.0562487915717431\n",
      "Gradient Descent(23/49): loss=1.3987134450859389\n",
      "Gradient Descent(24/49): loss=1.9081987127859001\n",
      "Gradient Descent(25/49): loss=2.6660714285447527\n",
      "Gradient Descent(26/49): loss=3.7933479728667514\n",
      "Gradient Descent(27/49): loss=5.4700138800009634\n",
      "Gradient Descent(28/49): loss=7.963757293403093\n",
      "Gradient Descent(29/49): loss=11.672703319380854\n",
      "Gradient Descent(30/49): loss=17.18897144723235\n",
      "Gradient Descent(31/49): loss=25.393202421661925\n",
      "Gradient Descent(32/49): loss=37.595144951619474\n",
      "Gradient Descent(33/49): loss=55.74274568459686\n",
      "Gradient Descent(34/49): loss=82.73312181533136\n",
      "Gradient Descent(35/49): loss=122.87506035897707\n",
      "Gradient Descent(36/49): loss=182.5768780103427\n",
      "Gradient Descent(37/49): loss=271.3694523350815\n",
      "Gradient Descent(38/49): loss=403.4277421774245\n",
      "Gradient Descent(39/49): loss=599.8336945640433\n",
      "Gradient Descent(40/49): loss=891.9417911810804\n",
      "Gradient Descent(41/49): loss=1326.3845141580134\n",
      "Gradient Descent(42/49): loss=1972.5168095071392\n",
      "Gradient Descent(43/49): loss=2933.487990973677\n",
      "Gradient Descent(44/49): loss=4362.708615834762\n",
      "Gradient Descent(45/49): loss=6488.341123747643\n",
      "Gradient Descent(46/49): loss=9649.723952562617\n",
      "Gradient Descent(47/49): loss=14351.543919075775\n",
      "Gradient Descent(48/49): loss=21344.405006255958\n",
      "Gradient Descent(49/49): loss=31744.65565131991\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39794577266447456\n",
      "Gradient Descent(2/49): loss=0.386106523945797\n",
      "Gradient Descent(3/49): loss=0.38054863782016685\n",
      "Gradient Descent(4/49): loss=0.3766461629482689\n",
      "Gradient Descent(5/49): loss=0.3736548830003076\n",
      "Gradient Descent(6/49): loss=0.37133228602137575\n",
      "Gradient Descent(7/49): loss=0.3695890770170996\n",
      "Gradient Descent(8/49): loss=0.3684186946864223\n",
      "Gradient Descent(9/49): loss=0.36788512810542756\n",
      "Gradient Descent(10/49): loss=0.3681341378605162\n",
      "Gradient Descent(11/49): loss=0.36942216053513294\n",
      "Gradient Descent(12/49): loss=0.37216581257449327\n",
      "Gradient Descent(13/49): loss=0.3770201431155914\n",
      "Gradient Descent(14/49): loss=0.38499946021643444\n",
      "Gradient Descent(15/49): loss=0.3976623928794278\n",
      "Gradient Descent(16/49): loss=0.41739446846166506\n",
      "Gradient Descent(17/49): loss=0.44783904112960393\n",
      "Gradient Descent(18/49): loss=0.4945540851919255\n",
      "Gradient Descent(19/49): loss=0.5660129746286465\n",
      "Gradient Descent(20/49): loss=0.675129212149651\n",
      "Gradient Descent(21/49): loss=0.8415792680515026\n",
      "Gradient Descent(22/49): loss=1.0953411773918829\n",
      "Gradient Descent(23/49): loss=1.48208512158038\n",
      "Gradient Descent(24/49): loss=2.0713851855177166\n",
      "Gradient Descent(25/49): loss=2.969228697659762\n",
      "Gradient Descent(26/49): loss=4.337072220170103\n",
      "Gradient Descent(27/49): loss=6.420870276065222\n",
      "Gradient Descent(28/49): loss=9.595295896186729\n",
      "Gradient Descent(29/49): loss=14.431103403531772\n",
      "Gradient Descent(30/49): loss=21.797744592348817\n",
      "Gradient Descent(31/49): loss=33.0196876635452\n",
      "Gradient Descent(32/49): loss=50.11454349159684\n",
      "Gradient Descent(33/49): loss=76.15581193135833\n",
      "Gradient Descent(34/49): loss=115.82546630018298\n",
      "Gradient Descent(35/49): loss=176.25572516150726\n",
      "Gradient Descent(36/49): loss=268.3113534988294\n",
      "Gradient Descent(37/49): loss=408.5430381148303\n",
      "Gradient Descent(38/49): loss=622.1630344746551\n",
      "Gradient Descent(39/49): loss=947.5780763184669\n",
      "Gradient Descent(40/49): loss=1443.2945195251755\n",
      "Gradient Descent(41/49): loss=2198.437195526793\n",
      "Gradient Descent(42/49): loss=3348.7731581438525\n",
      "Gradient Descent(43/49): loss=5101.121232086035\n",
      "Gradient Descent(44/49): loss=7770.535954019423\n",
      "Gradient Descent(45/49): loss=11836.951204180206\n",
      "Gradient Descent(46/49): loss=18031.467482827604\n",
      "Gradient Descent(47/49): loss=27467.796414510998\n",
      "Gradient Descent(48/49): loss=41842.49487774396\n",
      "Gradient Descent(49/49): loss=63739.98883926197\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.397617951118\n",
      "Gradient Descent(2/49): loss=0.3853541745096346\n",
      "Gradient Descent(3/49): loss=0.37991602490184356\n",
      "Gradient Descent(4/49): loss=0.37628309721926967\n",
      "Gradient Descent(5/49): loss=0.3738378095252778\n",
      "Gradient Descent(6/49): loss=0.37254900347430603\n",
      "Gradient Descent(7/49): loss=0.37268310364255836\n",
      "Gradient Descent(8/49): loss=0.374852579882426\n",
      "Gradient Descent(9/49): loss=0.38020100110066324\n",
      "Gradient Descent(10/49): loss=0.39075272231724995\n",
      "Gradient Descent(11/49): loss=0.4100260036584897\n",
      "Gradient Descent(12/49): loss=0.4440911217281712\n",
      "Gradient Descent(13/49): loss=0.5033888864487386\n",
      "Gradient Descent(14/49): loss=0.6058521820511434\n",
      "Gradient Descent(15/49): loss=0.7822620337594456\n",
      "Gradient Descent(16/49): loss=1.0854364473313363\n",
      "Gradient Descent(17/49): loss=1.605993867834587\n",
      "Gradient Descent(18/49): loss=2.499394796859621\n",
      "Gradient Descent(19/49): loss=4.032330221730821\n",
      "Gradient Descent(20/49): loss=6.662298116951969\n",
      "Gradient Descent(21/49): loss=11.17411178375707\n",
      "Gradient Descent(22/49): loss=18.914070866242056\n",
      "Gradient Descent(23/49): loss=32.19166605326796\n",
      "Gradient Descent(24/49): loss=54.96867646566612\n",
      "Gradient Descent(25/49): loss=94.0412697445792\n",
      "Gradient Descent(26/49): loss=161.06781492683697\n",
      "Gradient Descent(27/49): loss=276.04745841813605\n",
      "Gradient Descent(28/49): loss=473.28738597030843\n",
      "Gradient Descent(29/49): loss=811.639257242016\n",
      "Gradient Descent(30/49): loss=1392.0591173026328\n",
      "Gradient Descent(31/49): loss=2387.7302377242113\n",
      "Gradient Descent(32/49): loss=4095.736814064279\n",
      "Gradient Descent(33/49): loss=7025.70670054465\n",
      "Gradient Descent(34/49): loss=12051.872028760916\n",
      "Gradient Descent(35/49): loss=20673.918919099942\n",
      "Gradient Descent(36/49): loss=35464.457526048485\n",
      "Gradient Descent(37/49): loss=60836.62664990601\n",
      "Gradient Descent(38/49): loss=104360.86755458894\n",
      "Gradient Descent(39/49): loss=179023.7604321056\n",
      "Gradient Descent(40/49): loss=307102.90564961126\n",
      "Gradient Descent(41/49): loss=526814.0205690244\n",
      "Gradient Descent(42/49): loss=903713.5847665671\n",
      "Gradient Descent(43/49): loss=1550259.3070359197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=2659364.7841540286\n",
      "Gradient Descent(45/49): loss=4561960.249545182\n",
      "Gradient Descent(46/49): loss=7825734.146242223\n",
      "Gradient Descent(47/49): loss=13424517.61991731\n",
      "Gradient Descent(48/49): loss=23028852.165502686\n",
      "Gradient Descent(49/49): loss=39504438.781264745\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3974767614486434\n",
      "Gradient Descent(2/49): loss=0.38470866776152435\n",
      "Gradient Descent(3/49): loss=0.3790606356833808\n",
      "Gradient Descent(4/49): loss=0.37523506248119326\n",
      "Gradient Descent(5/49): loss=0.37251493726163537\n",
      "Gradient Descent(6/49): loss=0.37076213590921137\n",
      "Gradient Descent(7/49): loss=0.37006545738175073\n",
      "Gradient Descent(8/49): loss=0.3707209133428075\n",
      "Gradient Descent(9/49): loss=0.3733028833507973\n",
      "Gradient Descent(10/49): loss=0.3788130126665998\n",
      "Gradient Descent(11/49): loss=0.38893758194588635\n",
      "Gradient Descent(12/49): loss=0.40647599547544894\n",
      "Gradient Descent(13/49): loss=0.43604712352727804\n",
      "Gradient Descent(14/49): loss=0.48525015764128476\n",
      "Gradient Descent(15/49): loss=0.5665704978978611\n",
      "Gradient Descent(16/49): loss=0.7005076547892937\n",
      "Gradient Descent(17/49): loss=0.920707932009445\n",
      "Gradient Descent(18/49): loss=1.2823862966843056\n",
      "Gradient Descent(19/49): loss=1.876144871416533\n",
      "Gradient Descent(20/49): loss=2.850645843359673\n",
      "Gradient Descent(21/49): loss=4.449812174722485\n",
      "Gradient Descent(22/49): loss=7.073864713135405\n",
      "Gradient Descent(23/49): loss=11.379468761283174\n",
      "Gradient Descent(24/49): loss=18.444049331392886\n",
      "Gradient Descent(25/49): loss=30.035390926696227\n",
      "Gradient Descent(26/49): loss=49.05398252865878\n",
      "Gradient Descent(27/49): loss=80.25879366719415\n",
      "Gradient Descent(28/49): loss=131.45808904461617\n",
      "Gradient Descent(29/49): loss=215.46325354190336\n",
      "Gradient Descent(30/49): loss=353.2945238511663\n",
      "Gradient Descent(31/49): loss=579.4407815491935\n",
      "Gradient Descent(32/49): loss=950.4895403387039\n",
      "Gradient Descent(33/49): loss=1559.2865676565384\n",
      "Gradient Descent(34/49): loss=2558.168234654233\n",
      "Gradient Descent(35/49): loss=4197.079913730401\n",
      "Gradient Descent(36/49): loss=6886.11860686246\n",
      "Gradient Descent(37/49): loss=11298.149545763652\n",
      "Gradient Descent(38/49): loss=18537.17402208108\n",
      "Gradient Descent(39/49): loss=30414.578319836244\n",
      "Gradient Descent(40/49): loss=49902.388191570404\n",
      "Gradient Descent(41/49): loss=81876.94456534006\n",
      "Gradient Descent(42/49): loss=134339.0868771986\n",
      "Gradient Descent(43/49): loss=220416.1646347041\n",
      "Gradient Descent(44/49): loss=361646.83093415445\n",
      "Gradient Descent(45/49): loss=593370.5560115657\n",
      "Gradient Descent(46/49): loss=973570.4528766655\n",
      "Gradient Descent(47/49): loss=1597382.1127492497\n",
      "Gradient Descent(48/49): loss=2620898.926392917\n",
      "Gradient Descent(49/49): loss=4300230.582386841\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3956833648034652\n",
      "Gradient Descent(2/49): loss=0.3831150624225771\n",
      "Gradient Descent(3/49): loss=0.3776217828639675\n",
      "Gradient Descent(4/49): loss=0.3738835301772137\n",
      "Gradient Descent(5/49): loss=0.3711937473962241\n",
      "Gradient Descent(6/49): loss=0.3694129938863059\n",
      "Gradient Descent(7/49): loss=0.36861664907367964\n",
      "Gradient Descent(8/49): loss=0.369071821942088\n",
      "Gradient Descent(9/49): loss=0.3713010186794024\n",
      "Gradient Descent(10/49): loss=0.3762178925615227\n",
      "Gradient Descent(11/49): loss=0.3853626583514439\n",
      "Gradient Descent(12/49): loss=0.4012944392145428\n",
      "Gradient Descent(13/49): loss=0.42823840275646935\n",
      "Gradient Descent(14/49): loss=0.4731498029598679\n",
      "Gradient Descent(15/49): loss=0.5474616790277529\n",
      "Gradient Descent(16/49): loss=0.6699543706207767\n",
      "Gradient Descent(17/49): loss=0.8714662157326576\n",
      "Gradient Descent(18/49): loss=1.202626312432209\n",
      "Gradient Descent(19/49): loss=1.7465477426490195\n",
      "Gradient Descent(20/49): loss=2.6396630611139438\n",
      "Gradient Descent(21/49): loss=4.105924824598699\n",
      "Gradient Descent(22/49): loss=6.512944076158364\n",
      "Gradient Descent(23/49): loss=10.464138768669908\n",
      "Gradient Descent(24/49): loss=16.949990535352757\n",
      "Gradient Descent(25/49): loss=27.59632459978076\n",
      "Gradient Descent(26/49): loss=45.071847912014576\n",
      "Gradient Descent(27/49): loss=73.7571010454833\n",
      "Gradient Descent(28/49): loss=120.84250664715768\n",
      "Gradient Descent(29/49): loss=198.1307566863267\n",
      "Gradient Descent(30/49): loss=324.9953337773563\n",
      "Gradient Descent(31/49): loss=533.2367641103027\n",
      "Gradient Descent(32/49): loss=875.0538942728216\n",
      "Gradient Descent(33/49): loss=1436.1283114736432\n",
      "Gradient Descent(34/49): loss=2357.101712383045\n",
      "Gradient Descent(35/49): loss=3868.829843636869\n",
      "Gradient Descent(36/49): loss=6350.2499403474085\n",
      "Gradient Descent(37/49): loss=10423.366999736447\n",
      "Gradient Descent(38/49): loss=17109.16861701615\n",
      "Gradient Descent(39/49): loss=28083.550764097483\n",
      "Gradient Descent(40/49): loss=46097.40612716845\n",
      "Gradient Descent(41/49): loss=75666.17641459584\n",
      "Gradient Descent(42/49): loss=124201.71521246187\n",
      "Gradient Descent(43/49): loss=203870.1797050642\n",
      "Gradient Descent(44/49): loss=334641.6595484728\n",
      "Gradient Descent(45/49): loss=549295.9778888698\n",
      "Gradient Descent(46/49): loss=901639.4432604815\n",
      "Gradient Descent(47/49): loss=1479992.203858357\n",
      "Gradient Descent(48/49): loss=2429327.01082802\n",
      "Gradient Descent(49/49): loss=3987608.802003154\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3982336058906254\n",
      "Gradient Descent(2/49): loss=0.38575677075526593\n",
      "Gradient Descent(3/49): loss=0.3802235418844287\n",
      "Gradient Descent(4/49): loss=0.376415607439982\n",
      "Gradient Descent(5/49): loss=0.37360627020827647\n",
      "Gradient Descent(6/49): loss=0.3716258109006902\n",
      "Gradient Descent(7/49): loss=0.3705078347901709\n",
      "Gradient Descent(8/49): loss=0.3704593512132211\n",
      "Gradient Descent(9/49): loss=0.37191641450814916\n",
      "Gradient Descent(10/49): loss=0.3756704514860492\n",
      "Gradient Descent(11/49): loss=0.3830937593965681\n",
      "Gradient Descent(12/49): loss=0.39652465068591547\n",
      "Gradient Descent(13/49): loss=0.41991792632858016\n",
      "Gradient Descent(14/49): loss=0.45993982634879116\n",
      "Gradient Descent(15/49): loss=0.5278091693750704\n",
      "Gradient Descent(16/49): loss=0.6423919880896566\n",
      "Gradient Descent(17/49): loss=0.83540228954475\n",
      "Gradient Descent(18/49): loss=1.1601417705300014\n",
      "Gradient Descent(19/49): loss=1.7061862439059996\n",
      "Gradient Descent(20/49): loss=2.6240647643339754\n",
      "Gradient Descent(21/49): loss=4.1667302942220426\n",
      "Gradient Descent(22/49): loss=6.759246599071659\n",
      "Gradient Descent(23/49): loss=11.115889268278577\n",
      "Gradient Descent(24/49): loss=18.43692074273285\n",
      "Gradient Descent(25/49): loss=30.739248405655736\n",
      "Gradient Descent(26/49): loss=51.41205787768074\n",
      "Gradient Descent(27/49): loss=86.15049147394798\n",
      "Gradient Descent(28/49): loss=144.5245884335368\n",
      "Gradient Descent(29/49): loss=242.61571010414787\n",
      "Gradient Descent(30/49): loss=407.446751852124\n",
      "Gradient Descent(31/49): loss=684.4266089972671\n",
      "Gradient Descent(32/49): loss=1149.8597528900036\n",
      "Gradient Descent(33/49): loss=1931.9671469601196\n",
      "Gradient Descent(34/49): loss=3246.2094989629586\n",
      "Gradient Descent(35/49): loss=5454.643958184483\n",
      "Gradient Descent(36/49): loss=9165.666276156282\n",
      "Gradient Descent(37/49): loss=15401.61613329692\n",
      "Gradient Descent(38/49): loss=25880.418776873412\n",
      "Gradient Descent(39/49): loss=43488.851675586826\n",
      "Gradient Descent(40/49): loss=73077.81516196362\n",
      "Gradient Descent(41/49): loss=122798.6940553705\n",
      "Gradient Descent(42/49): loss=206348.96097270338\n",
      "Gradient Descent(43/49): loss=346745.65660663013\n",
      "Gradient Descent(44/49): loss=582666.2930107783\n",
      "Gradient Descent(45/49): loss=979104.0184631033\n",
      "Gradient Descent(46/49): loss=1645272.4069186645\n",
      "Gradient Descent(47/49): loss=2764692.414848415\n",
      "Gradient Descent(48/49): loss=4645750.081133275\n",
      "Gradient Descent(49/49): loss=7806652.97620747\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3980886979821031\n",
      "Gradient Descent(2/49): loss=0.38509746172982495\n",
      "Gradient Descent(3/49): loss=0.3797266845351408\n",
      "Gradient Descent(4/49): loss=0.37638060780289406\n",
      "Gradient Descent(5/49): loss=0.3745684122623093\n",
      "Gradient Descent(6/49): loss=0.3746131361703824\n",
      "Gradient Descent(7/49): loss=0.37749008239636994\n",
      "Gradient Descent(8/49): loss=0.3852151448889691\n",
      "Gradient Descent(9/49): loss=0.4016931557082886\n",
      "Gradient Descent(10/49): loss=0.4343509854516379\n",
      "Gradient Descent(11/49): loss=0.4972272192686681\n",
      "Gradient Descent(12/49): loss=0.6167967270626168\n",
      "Gradient Descent(13/49): loss=0.8429413602237906\n",
      "Gradient Descent(14/49): loss=1.2696084787187867\n",
      "Gradient Descent(15/49): loss=2.073709415027067\n",
      "Gradient Descent(16/49): loss=3.5883605843176882\n",
      "Gradient Descent(17/49): loss=6.440786447296061\n",
      "Gradient Descent(18/49): loss=11.811971016378887\n",
      "Gradient Descent(19/49): loss=21.925543778163096\n",
      "Gradient Descent(20/49): loss=40.968280107486656\n",
      "Gradient Descent(21/49): loss=76.8232680179858\n",
      "Gradient Descent(22/49): loss=144.33320566687416\n",
      "Gradient Descent(23/49): loss=271.4447092659185\n",
      "Gradient Descent(24/49): loss=510.7771384766019\n",
      "Gradient Descent(25/49): loss=961.4049878431143\n",
      "Gradient Descent(26/49): loss=1809.870916096079\n",
      "Gradient Descent(27/49): loss=3407.407229172647\n",
      "Gradient Descent(28/49): loss=6415.33253920654\n",
      "Gradient Descent(29/49): loss=12078.812220980479\n",
      "Gradient Descent(30/49): loss=22742.308959603346\n",
      "Gradient Descent(31/49): loss=42820.100079882875\n",
      "Gradient Descent(32/49): loss=80623.61848298865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=151802.06601592214\n",
      "Gradient Descent(34/49): loss=285820.58080235944\n",
      "Gradient Descent(35/49): loss=538157.670507972\n",
      "Gradient Descent(36/49): loss=1013271.1795136987\n",
      "Gradient Descent(37/49): loss=1907839.816598516\n",
      "Gradient Descent(38/49): loss=3592180.5715122973\n",
      "Gradient Descent(39/49): loss=6763545.697681746\n",
      "Gradient Descent(40/49): loss=12734758.312504763\n",
      "Gradient Descent(41/49): loss=23977670.646078967\n",
      "Gradient Descent(42/49): loss=45146415.73922788\n",
      "Gradient Descent(43/49): loss=85004039.41561712\n",
      "Gradient Descent(44/49): loss=160050064.0211159\n",
      "Gradient Descent(45/49): loss=301350655.7175649\n",
      "Gradient Descent(46/49): loss=567398821.7425947\n",
      "Gradient Descent(47/49): loss=1068328264.3972883\n",
      "Gradient Descent(48/49): loss=2011504495.4813898\n",
      "Gradient Descent(49/49): loss=3787366178.052221\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39797035375081385\n",
      "Gradient Descent(2/49): loss=0.3844381508134799\n",
      "Gradient Descent(3/49): loss=0.37883204285097205\n",
      "Gradient Descent(4/49): loss=0.37523229780512174\n",
      "Gradient Descent(5/49): loss=0.3729892625546945\n",
      "Gradient Descent(6/49): loss=0.372196289762653\n",
      "Gradient Descent(7/49): loss=0.3733848580135739\n",
      "Gradient Descent(8/49): loss=0.37768711028954827\n",
      "Gradient Descent(9/49): loss=0.387248385138908\n",
      "Gradient Descent(10/49): loss=0.4060065678041721\n",
      "Gradient Descent(11/49): loss=0.44111146225824666\n",
      "Gradient Descent(12/49): loss=0.5054902035292362\n",
      "Gradient Descent(13/49): loss=0.6224751392020306\n",
      "Gradient Descent(14/49): loss=0.8341477151174622\n",
      "Gradient Descent(15/49): loss=1.2163795606944132\n",
      "Gradient Descent(16/49): loss=1.905944653427217\n",
      "Gradient Descent(17/49): loss=3.149389021738002\n",
      "Gradient Descent(18/49): loss=5.3911176788559025\n",
      "Gradient Descent(19/49): loss=9.432169494975483\n",
      "Gradient Descent(20/49): loss=16.71640519015279\n",
      "Gradient Descent(21/49): loss=29.84635416643555\n",
      "Gradient Descent(22/49): loss=53.51301549700928\n",
      "Gradient Descent(23/49): loss=96.17180069796488\n",
      "Gradient Descent(24/49): loss=173.06337802226665\n",
      "Gradient Descent(25/49): loss=311.6586674573253\n",
      "Gradient Descent(26/49): loss=561.4733055831227\n",
      "Gradient Descent(27/49): loss=1011.7579685120294\n",
      "Gradient Descent(28/49): loss=1823.3847283465761\n",
      "Gradient Descent(29/49): loss=3286.321398355287\n",
      "Gradient Descent(30/49): loss=5923.22757587357\n",
      "Gradient Descent(31/49): loss=10676.183870258616\n",
      "Gradient Descent(32/49): loss=19243.26657883408\n",
      "Gradient Descent(33/49): loss=34685.214965066465\n",
      "Gradient Descent(34/49): loss=62518.933570862304\n",
      "Gradient Descent(35/49): loss=112688.50227426823\n",
      "Gradient Descent(36/49): loss=203117.87170118134\n",
      "Gradient Descent(37/49): loss=366114.5061894788\n",
      "Gradient Descent(38/49): loss=659911.7870800395\n",
      "Gradient Descent(39/49): loss=1189473.9005718608\n",
      "Gradient Descent(40/49): loss=2143996.1180143235\n",
      "Gradient Descent(41/49): loss=3864498.0957035283\n",
      "Gradient Descent(42/49): loss=6965659.075624485\n",
      "Gradient Descent(43/49): loss=12555422.730675174\n",
      "Gradient Descent(44/49): loss=22630829.303096887\n",
      "Gradient Descent(45/49): loss=40791492.94899598\n",
      "Gradient Descent(46/49): loss=73525626.4738872\n",
      "Gradient Descent(47/49): loss=132528068.15321115\n",
      "Gradient Descent(48/49): loss=238878466.01769158\n",
      "Gradient Descent(49/49): loss=430572348.5760347\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3961979924380278\n",
      "Gradient Descent(2/49): loss=0.3828503535781339\n",
      "Gradient Descent(3/49): loss=0.3773893207851799\n",
      "Gradient Descent(4/49): loss=0.37385483759404686\n",
      "Gradient Descent(5/49): loss=0.37159769124137965\n",
      "Gradient Descent(6/49): loss=0.37068972490019875\n",
      "Gradient Descent(7/49): loss=0.3716101747511763\n",
      "Gradient Descent(8/49): loss=0.37538903538517027\n",
      "Gradient Descent(9/49): loss=0.3839827247473272\n",
      "Gradient Descent(10/49): loss=0.4009861799306682\n",
      "Gradient Descent(11/49): loss=0.43293096730651076\n",
      "Gradient Descent(12/49): loss=0.49163275853233007\n",
      "Gradient Descent(13/49): loss=0.5984279330763493\n",
      "Gradient Descent(14/49): loss=0.7918152325094879\n",
      "Gradient Descent(15/49): loss=1.141236739473455\n",
      "Gradient Descent(16/49): loss=1.7719290330412039\n",
      "Gradient Descent(17/49): loss=2.9097361364029637\n",
      "Gradient Descent(18/49): loss=4.961917953321601\n",
      "Gradient Descent(19/49): loss=8.662866646576248\n",
      "Gradient Descent(20/49): loss=15.336866718895038\n",
      "Gradient Descent(21/49): loss=27.37191145053068\n",
      "Gradient Descent(22/49): loss=49.0741019047543\n",
      "Gradient Descent(23/49): loss=88.20832282665567\n",
      "Gradient Descent(24/49): loss=158.77643183764505\n",
      "Gradient Descent(25/49): loss=286.0269665368304\n",
      "Gradient Descent(26/49): loss=515.4887859020183\n",
      "Gradient Descent(27/49): loss=929.260780413755\n",
      "Gradient Descent(28/49): loss=1675.3859243815457\n",
      "Gradient Descent(29/49): loss=3020.8193731292395\n",
      "Gradient Descent(30/49): loss=5446.9419892404\n",
      "Gradient Descent(31/49): loss=9821.792953847604\n",
      "Gradient Descent(32/49): loss=17710.64433710272\n",
      "Gradient Descent(33/49): loss=31936.03768529779\n",
      "Gradient Descent(34/49): loss=57587.65736119239\n",
      "Gradient Descent(35/49): loss=103843.3618604619\n",
      "Gradient Descent(36/49): loss=187252.91744738587\n",
      "Gradient Descent(37/49): loss=337659.31674660376\n",
      "Gradient Descent(38/49): loss=608876.2628982401\n",
      "Gradient Descent(39/49): loss=1097942.1019576686\n",
      "Gradient Descent(40/49): loss=1979839.0420927526\n",
      "Gradient Descent(41/49): loss=3570099.8022737545\n",
      "Gradient Descent(42/49): loss=6437701.639007039\n",
      "Gradient Descent(43/49): loss=11608639.952953305\n",
      "Gradient Descent(44/49): loss=20933017.80207964\n",
      "Gradient Descent(45/49): loss=37746991.78406378\n",
      "Gradient Descent(46/49): loss=68066411.01406617\n",
      "Gradient Descent(47/49): loss=122739219.68078353\n",
      "Gradient Descent(48/49): loss=221326728.39364618\n",
      "Gradient Descent(49/49): loss=399102429.1712733\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39869023224511996\n",
      "Gradient Descent(2/49): loss=0.3854641732807489\n",
      "Gradient Descent(3/49): loss=0.3799369330634934\n",
      "Gradient Descent(4/49): loss=0.37626852811987754\n",
      "Gradient Descent(5/49): loss=0.37376525990076237\n",
      "Gradient Descent(6/49): loss=0.37241867014578617\n",
      "Gradient Descent(7/49): loss=0.37257794528428173\n",
      "Gradient Descent(8/49): loss=0.37506083493939857\n",
      "Gradient Descent(9/49): loss=0.38147975498954617\n",
      "Gradient Descent(10/49): loss=0.39487981594543914\n",
      "Gradient Descent(11/49): loss=0.42093077505714566\n",
      "Gradient Descent(12/49): loss=0.47013335798844924\n",
      "Gradient Descent(13/49): loss=0.5618939775917422\n",
      "Gradient Descent(14/49): loss=0.732045368404831\n",
      "Gradient Descent(15/49): loss=1.0467246219851891\n",
      "Gradient Descent(16/49): loss=1.6279810678948705\n",
      "Gradient Descent(17/49): loss=2.701027086230253\n",
      "Gradient Descent(18/49): loss=4.681422972799469\n",
      "Gradient Descent(19/49): loss=8.335945581120082\n",
      "Gradient Descent(20/49): loss=15.079413722476549\n",
      "Gradient Descent(21/49): loss=27.522372680120327\n",
      "Gradient Descent(22/49): loss=50.48164872220787\n",
      "Gradient Descent(23/49): loss=92.84496327020821\n",
      "Gradient Descent(24/49): loss=171.01140619706544\n",
      "Gradient Descent(25/49): loss=315.2395889430469\n",
      "Gradient Descent(26/49): loss=581.3608723403252\n",
      "Gradient Descent(27/49): loss=1072.3919053977916\n",
      "Gradient Descent(28/49): loss=1978.4127732147394\n",
      "Gradient Descent(29/49): loss=3650.147740794558\n",
      "Gradient Descent(30/49): loss=6734.731985980552\n",
      "Gradient Descent(31/49): loss=12426.220153909911\n",
      "Gradient Descent(32/49): loss=22927.809572461672\n",
      "Gradient Descent(33/49): loss=42304.706540025334\n",
      "Gradient Descent(34/49): loss=78057.78355535422\n",
      "Gradient Descent(35/49): loss=144027.19634705305\n",
      "Gradient Descent(36/49): loss=265749.96219114354\n",
      "Gradient Descent(37/49): loss=490345.4390004553\n",
      "Gradient Descent(38/49): loss=904755.4127363717\n",
      "Gradient Descent(39/49): loss=1669399.6021926394\n",
      "Gradient Descent(40/49): loss=3080274.7584514725\n",
      "Gradient Descent(41/49): loss=5683536.16285796\n",
      "Gradient Descent(42/49): loss=10486916.468728965\n",
      "Gradient Descent(43/49): loss=19349822.945841745\n",
      "Gradient Descent(44/49): loss=35703121.29406287\n",
      "Gradient Descent(45/49): loss=65877237.15055502\n",
      "Gradient Descent(46/49): loss=121552688.56916365\n",
      "Gradient Descent(47/49): loss=224281660.16215998\n",
      "Gradient Descent(48/49): loss=413830937.90768117\n",
      "Gradient Descent(49/49): loss=763575787.2593228\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3987298569128548\n",
      "Gradient Descent(2/49): loss=0.3849137596376361\n",
      "Gradient Descent(3/49): loss=0.3796081057356729\n",
      "Gradient Descent(4/49): loss=0.3766785818659819\n",
      "Gradient Descent(5/49): loss=0.375886200827544\n",
      "Gradient Descent(6/49): loss=0.3782899143118606\n",
      "Gradient Descent(7/49): loss=0.3864960389856214\n",
      "Gradient Descent(8/49): loss=0.4060748762922403\n",
      "Gradient Descent(9/49): loss=0.4486173208985401\n",
      "Gradient Descent(10/49): loss=0.5380658656066901\n",
      "Gradient Descent(11/49): loss=0.7237606878045426\n",
      "Gradient Descent(12/49): loss=1.1072973940068191\n",
      "Gradient Descent(13/49): loss=1.8978064583872294\n",
      "Gradient Descent(14/49): loss=3.5257246561157345\n",
      "Gradient Descent(15/49): loss=6.876945989150603\n",
      "Gradient Descent(16/49): loss=13.774721075427058\n",
      "Gradient Descent(17/49): loss=27.971439002821178\n",
      "Gradient Descent(18/49): loss=57.18978094210639\n",
      "Gradient Descent(19/49): loss=117.32355038049207\n",
      "Gradient Descent(20/49): loss=241.08326116017187\n",
      "Gradient Descent(21/49): loss=495.78933218507535\n",
      "Gradient Descent(22/49): loss=1019.9916641041603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=2098.8351603633505\n",
      "Gradient Descent(24/49): loss=4319.166974951665\n",
      "Gradient Descent(25/49): loss=8888.757529518043\n",
      "Gradient Descent(26/49): loss=18293.278567068293\n",
      "Gradient Descent(27/49): loss=37648.40761829236\n",
      "Gradient Descent(28/49): loss=77482.54879031073\n",
      "Gradient Descent(29/49): loss=159463.85695808433\n",
      "Gradient Descent(30/49): loss=328186.8338977058\n",
      "Gradient Descent(31/49): loss=675429.9259088648\n",
      "Gradient Descent(32/49): loss=1390079.27073528\n",
      "Gradient Descent(33/49): loss=2860875.0842992957\n",
      "Gradient Descent(34/49): loss=5887870.548269958\n",
      "Gradient Descent(35/49): loss=12117628.244241828\n",
      "Gradient Descent(36/49): loss=24938883.31785691\n",
      "Gradient Descent(37/49): loss=51325877.75400696\n",
      "Gradient Descent(38/49): loss=105632064.73606722\n",
      "Gradient Descent(39/49): loss=217397804.1674659\n",
      "Gradient Descent(40/49): loss=447419118.5854855\n",
      "Gradient Descent(41/49): loss=920818260.0018295\n",
      "Gradient Descent(42/49): loss=1895105132.7672224\n",
      "Gradient Descent(43/49): loss=3900252221.9718523\n",
      "Gradient Descent(44/49): loss=8026978098.851978\n",
      "Gradient Descent(45/49): loss=16520054020.612862\n",
      "Gradient Descent(46/49): loss=33999368315.975826\n",
      "Gradient Descent(47/49): loss=69972957984.85704\n",
      "Gradient Descent(48/49): loss=144008994627.79956\n",
      "Gradient Descent(49/49): loss=296380074974.46344\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39863576134989986\n",
      "Gradient Descent(2/49): loss=0.3842431475930735\n",
      "Gradient Descent(3/49): loss=0.3786679916251563\n",
      "Gradient Descent(4/49): loss=0.3753972309176506\n",
      "Gradient Descent(5/49): loss=0.3739298594946259\n",
      "Gradient Descent(6/49): loss=0.37485285213602604\n",
      "Gradient Descent(7/49): loss=0.3797477275740677\n",
      "Gradient Descent(8/49): loss=0.39193369026312275\n",
      "Gradient Descent(9/49): loss=0.4180775572044771\n",
      "Gradient Descent(10/49): loss=0.4714086593010146\n",
      "Gradient Descent(11/49): loss=0.5780742962619683\n",
      "Gradient Descent(12/49): loss=0.7896793402869384\n",
      "Gradient Descent(13/49): loss=1.2080161782440917\n",
      "Gradient Descent(14/49): loss=2.033829882336213\n",
      "Gradient Descent(15/49): loss=3.6629752154493715\n",
      "Gradient Descent(16/49): loss=6.876020486606284\n",
      "Gradient Descent(17/49): loss=13.212109875675088\n",
      "Gradient Descent(18/49): loss=25.706146965759455\n",
      "Gradient Descent(19/49): loss=50.34237600482825\n",
      "Gradient Descent(20/49): loss=98.92055830627613\n",
      "Gradient Descent(21/49): loss=194.70750800586848\n",
      "Gradient Descent(22/49): loss=383.58080789444557\n",
      "Gradient Descent(23/49): loss=756.0020227497401\n",
      "Gradient Descent(24/49): loss=1490.3435490691993\n",
      "Gradient Descent(25/49): loss=2938.320630630449\n",
      "Gradient Descent(26/49): loss=5793.446469361078\n",
      "Gradient Descent(27/49): loss=11423.192530579792\n",
      "Gradient Descent(28/49): loss=22523.94325342037\n",
      "Gradient Descent(29/49): loss=44412.43776105486\n",
      "Gradient Descent(30/49): loss=87572.23859254103\n",
      "Gradient Descent(31/49): loss=172674.8665715431\n",
      "Gradient Descent(32/49): loss=340480.48996641947\n",
      "Gradient Descent(33/49): loss=671360.1337928595\n",
      "Gradient Descent(34/49): loss=1323789.6320928298\n",
      "Gradient Descent(35/49): loss=2610252.121298814\n",
      "Gradient Descent(36/49): loss=5146902.8098367145\n",
      "Gradient Descent(37/49): loss=10148678.430626066\n",
      "Gradient Descent(38/49): loss=20011194.966154158\n",
      "Gradient Descent(39/49): loss=39458135.37047347\n",
      "Gradient Descent(40/49): loss=77803672.20443027\n",
      "Gradient Descent(41/49): loss=153413519.53840536\n",
      "Gradient Descent(42/49): loss=302501248.7993193\n",
      "Gradient Descent(43/49): loss=596472891.3816644\n",
      "Gradient Descent(44/49): loss=1176127079.3619967\n",
      "Gradient Descent(45/49): loss=2319090988.028724\n",
      "Gradient Descent(46/49): loss=4572790734.538832\n",
      "Gradient Descent(47/49): loss=9016642818.498684\n",
      "Gradient Descent(48/49): loss=17779044010.004196\n",
      "Gradient Descent(49/49): loss=35056773599.16592\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39688809355434773\n",
      "Gradient Descent(2/49): loss=0.3826604894314335\n",
      "Gradient Descent(3/49): loss=0.377216786771524\n",
      "Gradient Descent(4/49): loss=0.3739792317440074\n",
      "Gradient Descent(5/49): loss=0.37242639823662593\n",
      "Gradient Descent(6/49): loss=0.37307971734424294\n",
      "Gradient Descent(7/49): loss=0.3773758109681764\n",
      "Gradient Descent(8/49): loss=0.38833945952144777\n",
      "Gradient Descent(9/49): loss=0.41205279314002385\n",
      "Gradient Descent(10/49): loss=0.46059258330066066\n",
      "Gradient Descent(11/49): loss=0.5578400016228017\n",
      "Gradient Descent(12/49): loss=0.7509485054844591\n",
      "Gradient Descent(13/49): loss=1.1329695658236454\n",
      "Gradient Descent(14/49): loss=1.8874868467880868\n",
      "Gradient Descent(15/49): loss=3.376663657524473\n",
      "Gradient Descent(16/49): loss=6.314927435717246\n",
      "Gradient Descent(17/49): loss=12.111582514348376\n",
      "Gradient Descent(18/49): loss=23.546652535306077\n",
      "Gradient Descent(19/49): loss=46.10405529207431\n",
      "Gradient Descent(20/49): loss=90.60143884245245\n",
      "Gradient Descent(21/49): loss=178.37782619941632\n",
      "Gradient Descent(22/49): loss=351.5268220244345\n",
      "Gradient Descent(23/49): loss=693.0827518059574\n",
      "Gradient Descent(24/49): loss=1366.8400679099352\n",
      "Gradient Descent(25/49): loss=2695.9015593319746\n",
      "Gradient Descent(26/49): loss=5317.623436537985\n",
      "Gradient Descent(27/49): loss=10489.261753978211\n",
      "Gradient Descent(28/49): loss=20690.893991462013\n",
      "Gradient Descent(29/49): loss=40814.748967679756\n",
      "Gradient Descent(30/49): loss=80511.29244544877\n",
      "Gradient Descent(31/49): loss=158817.14206470965\n",
      "Gradient Descent(32/49): loss=313284.1445505388\n",
      "Gradient Descent(33/49): loss=617987.4964060604\n",
      "Gradient Descent(34/49): loss=1219048.7659551026\n",
      "Gradient Descent(35/49): loss=2404709.007386665\n",
      "Gradient Descent(36/49): loss=4743555.776069566\n",
      "Gradient Descent(37/49): loss=9357191.29840898\n",
      "Gradient Descent(38/49): loss=18458100.779890597\n",
      "Gradient Descent(39/49): loss=36410657.49764688\n",
      "Gradient Descent(40/49): loss=71824073.41608293\n",
      "Gradient Descent(41/49): loss=141680977.18349805\n",
      "Gradient Descent(42/49): loss=279481493.66593134\n",
      "Gradient Descent(43/49): loss=551308347.1154568\n",
      "Gradient Descent(44/49): loss=1087517064.9229949\n",
      "Gradient Descent(45/49): loss=2145248431.063358\n",
      "Gradient Descent(46/49): loss=4231741257.013032\n",
      "Gradient Descent(47/49): loss=8347580544.851865\n",
      "Gradient Descent(48/49): loss=16466531558.069824\n",
      "Gradient Descent(49/49): loss=32482065923.02832\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39931565172795846\n",
      "Gradient Descent(2/49): loss=0.38524056211139374\n",
      "Gradient Descent(3/49): loss=0.37969553021257985\n",
      "Gradient Descent(4/49): loss=0.3762281025755281\n",
      "Gradient Descent(5/49): loss=0.3742203079397036\n",
      "Gradient Descent(6/49): loss=0.3739999662352475\n",
      "Gradient Descent(7/49): loss=0.3766517787116977\n",
      "Gradient Descent(8/49): loss=0.3845631715792733\n",
      "Gradient Descent(9/49): loss=0.4026711874195088\n",
      "Gradient Descent(10/49): loss=0.4410189954938214\n",
      "Gradient Descent(11/49): loss=0.5199302101661019\n",
      "Gradient Descent(12/49): loss=0.6804564056749942\n",
      "Gradient Descent(13/49): loss=1.005460227795245\n",
      "Gradient Descent(14/49): loss=1.6621551441375164\n",
      "Gradient Descent(15/49): loss=2.9879351413642836\n",
      "Gradient Descent(16/49): loss=5.6635455555258\n",
      "Gradient Descent(17/49): loss=11.062472742382386\n",
      "Gradient Descent(18/49): loss=21.955872030087903\n",
      "Gradient Descent(19/49): loss=43.934828266491344\n",
      "Gradient Descent(20/49): loss=88.27990387840633\n",
      "Gradient Descent(21/49): loss=177.75072883425327\n",
      "Gradient Descent(22/49): loss=358.26705638982384\n",
      "Gradient Descent(23/49): loss=722.4764185585882\n",
      "Gradient Descent(24/49): loss=1457.3041415824398\n",
      "Gradient Descent(25/49): loss=2939.8900830085972\n",
      "Gradient Descent(26/49): loss=5931.15024090619\n",
      "Gradient Descent(27/49): loss=11966.305947272822\n",
      "Gradient Descent(28/49): loss=24142.81413735494\n",
      "Gradient Descent(29/49): loss=48710.09257314294\n",
      "Gradient Descent(30/49): loss=98276.9436276318\n",
      "Gradient Descent(31/49): loss=198282.8407560225\n",
      "Gradient Descent(32/49): loss=400054.37236120895\n",
      "Gradient Descent(33/49): loss=807147.8750818897\n",
      "Gradient Descent(34/49): loss=1628498.2341623981\n",
      "Gradient Descent(35/49): loss=3285651.7082761014\n",
      "Gradient Descent(36/49): loss=6629118.483854454\n",
      "Gradient Descent(37/49): loss=13374884.795717195\n",
      "Gradient Descent(38/49): loss=26985118.18173635\n",
      "Gradient Descent(39/49): loss=54445075.17664164\n",
      "Gradient Descent(40/49): loss=109848183.76226076\n",
      "Gradient Descent(41/49): loss=221629292.57891795\n",
      "Gradient Descent(42/49): loss=447158448.02275974\n",
      "Gradient Descent(43/49): loss=902185245.4276024\n",
      "Gradient Descent(44/49): loss=1820245644.0888076\n",
      "Gradient Descent(45/49): loss=3672520939.5151415\n",
      "Gradient Descent(46/49): loss=7409664786.5320015\n",
      "Gradient Descent(47/49): loss=14949712514.740639\n",
      "Gradient Descent(48/49): loss=30162485175.08324\n",
      "Gradient Descent(49/49): loss=60855719536.11271\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3995414279102546\n",
      "Gradient Descent(2/49): loss=0.3848172854982171\n",
      "Gradient Descent(3/49): loss=0.3795742540753827\n",
      "Gradient Descent(4/49): loss=0.37723512758398164\n",
      "Gradient Descent(5/49): loss=0.3780365582838052\n",
      "Gradient Descent(6/49): loss=0.384472788215428\n",
      "Gradient Descent(7/49): loss=0.40264061500667064\n",
      "Gradient Descent(8/49): loss=0.4464405443214985\n",
      "Gradient Descent(9/49): loss=0.5471707190876024\n",
      "Gradient Descent(10/49): loss=0.7750743487707896\n",
      "Gradient Descent(11/49): loss=1.2876474899634323\n",
      "Gradient Descent(12/49): loss=2.4379093347749277\n",
      "Gradient Descent(13/49): loss=5.017048950544578\n",
      "Gradient Descent(14/49): loss=10.798217707333844\n",
      "Gradient Descent(15/49): loss=23.755210534407755\n",
      "Gradient Descent(16/49): loss=52.79362256957984\n",
      "Gradient Descent(17/49): loss=117.87157511729777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=263.71671791097424\n",
      "Gradient Descent(19/49): loss=590.5670505513756\n",
      "Gradient Descent(20/49): loss=1323.0633869027831\n",
      "Gradient Descent(21/49): loss=2964.642484681551\n",
      "Gradient Descent(22/49): loss=6643.543515576426\n",
      "Gradient Descent(23/49): loss=14888.234262063523\n",
      "Gradient Descent(24/49): loss=33365.1988135824\n",
      "Gradient Descent(25/49): loss=74773.44885482243\n",
      "Gradient Descent(26/49): loss=167572.41269815475\n",
      "Gradient Descent(27/49): loss=375541.78280303214\n",
      "Gradient Descent(28/49): loss=841616.5867260961\n",
      "Gradient Descent(29/49): loss=1886124.836637142\n",
      "Gradient Descent(30/49): loss=4226945.397738786\n",
      "Gradient Descent(31/49): loss=9472898.121947898\n",
      "Gradient Descent(32/49): loss=21229467.78027581\n",
      "Gradient Descent(33/49): loss=47576813.51453403\n",
      "Gradient Descent(34/49): loss=106623172.05391735\n",
      "Gradient Descent(35/49): loss=238950446.760184\n",
      "Gradient Descent(36/49): loss=535505696.9794079\n",
      "Gradient Descent(37/49): loss=1200108037.107592\n",
      "Gradient Descent(38/49): loss=2689531239.621578\n",
      "Gradient Descent(39/49): loss=6027439252.041532\n",
      "Gradient Descent(40/49): loss=13507939005.617735\n",
      "Gradient Descent(41/49): loss=30272294510.635696\n",
      "Gradient Descent(42/49): loss=67842460242.44726\n",
      "Gradient Descent(43/49): loss=152039991885.89124\n",
      "Gradient Descent(44/49): loss=340732913430.4561\n",
      "Gradient Descent(45/49): loss=763607764344.1658\n",
      "Gradient Descent(46/49): loss=1711301711056.6753\n",
      "Gradient Descent(47/49): loss=3835154228403.2017\n",
      "Gradient Descent(48/49): loss=8594865452777.068\n",
      "Gradient Descent(49/49): loss=19261731797967.16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39947298424590166\n",
      "Gradient Descent(2/49): loss=0.3841384042853349\n",
      "Gradient Descent(3/49): loss=0.37858143724730736\n",
      "Gradient Descent(4/49): loss=0.3757783959112958\n",
      "Gradient Descent(5/49): loss=0.3755325351968631\n",
      "Gradient Descent(6/49): loss=0.3794148483435195\n",
      "Gradient Descent(7/49): loss=0.39130691060621714\n",
      "Gradient Descent(8/49): loss=0.41978071632074176\n",
      "Gradient Descent(9/49): loss=0.4833960929847855\n",
      "Gradient Descent(10/49): loss=0.6221305815801437\n",
      "Gradient Descent(11/49): loss=0.9219574716560253\n",
      "Gradient Descent(12/49): loss=1.5676662503706102\n",
      "Gradient Descent(13/49): loss=2.956364474358171\n",
      "Gradient Descent(14/49): loss=5.94136586622281\n",
      "Gradient Descent(15/49): loss=12.356242479412897\n",
      "Gradient Descent(16/49): loss=26.140873828871634\n",
      "Gradient Descent(17/49): loss=55.76102590855382\n",
      "Gradient Descent(18/49): loss=119.4073792558135\n",
      "Gradient Descent(19/49): loss=256.166848445147\n",
      "Gradient Descent(20/49): loss=550.0267562176296\n",
      "Gradient Descent(21/49): loss=1181.4533751846834\n",
      "Gradient Descent(22/49): loss=2538.220400303594\n",
      "Gradient Descent(23/49): loss=5453.549653982251\n",
      "Gradient Descent(24/49): loss=11717.811681808053\n",
      "Gradient Descent(25/49): loss=25178.03328733536\n",
      "Gradient Descent(26/49): loss=54100.44475017451\n",
      "Gradient Descent(27/49): loss=116246.96105135669\n",
      "Gradient Descent(28/49): loss=249783.18042411766\n",
      "Gradient Descent(29/49): loss=536716.7518177152\n",
      "Gradient Descent(30/49): loss=1153260.1492239635\n",
      "Gradient Descent(31/49): loss=2478046.785354183\n",
      "Gradient Descent(32/49): loss=5324658.457077303\n",
      "Gradient Descent(33/49): loss=11441264.549276128\n",
      "Gradient Descent(34/49): loss=24584212.868651092\n",
      "Gradient Descent(35/49): loss=52824888.81236094\n",
      "Gradient Descent(36/49): loss=113506537.88958491\n",
      "Gradient Descent(37/49): loss=243895149.76517174\n",
      "Gradient Descent(38/49): loss=524065355.50894403\n",
      "Gradient Descent(39/49): loss=1126076091.3958755\n",
      "Gradient Descent(40/49): loss=2419635929.9983597\n",
      "Gradient Descent(41/49): loss=5199149577.009754\n",
      "Gradient Descent(42/49): loss=11171579984.457403\n",
      "Gradient Descent(43/49): loss=24004733370.898903\n",
      "Gradient Descent(44/49): loss=51579742974.12201\n",
      "Gradient Descent(45/49): loss=110831053366.86905\n",
      "Gradient Descent(46/49): loss=238146250488.2295\n",
      "Gradient Descent(47/49): loss=511712511058.80255\n",
      "Gradient Descent(48/49): loss=1099533137462.4058\n",
      "Gradient Descent(49/49): loss=2362602231234.523\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39775366815242474\n",
      "Gradient Descent(2/49): loss=0.3825603309638398\n",
      "Gradient Descent(3/49): loss=0.377116313605484\n",
      "Gradient Descent(4/49): loss=0.37430087796153794\n",
      "Gradient Descent(5/49): loss=0.3738579933020853\n",
      "Gradient Descent(6/49): loss=0.37720461014647616\n",
      "Gradient Descent(7/49): loss=0.3878737634065655\n",
      "Gradient Descent(8/49): loss=0.41368003332122494\n",
      "Gradient Descent(9/49): loss=0.47155486019154824\n",
      "Gradient Descent(10/49): loss=0.5979861491469672\n",
      "Gradient Descent(11/49): loss=0.8714767016692653\n",
      "Gradient Descent(12/49): loss=1.4608291557825885\n",
      "Gradient Descent(13/49): loss=2.728943731667796\n",
      "Gradient Descent(14/49): loss=5.455944364436062\n",
      "Gradient Descent(15/49): loss=11.318811896390283\n",
      "Gradient Descent(16/49): loss=23.922403663584117\n",
      "Gradient Descent(17/49): loss=51.015731562736704\n",
      "Gradient Descent(18/49): loss=109.25606856174205\n",
      "Gradient Descent(19/49): loss=234.44986070461076\n",
      "Gradient Descent(20/49): loss=503.5665663944555\n",
      "Gradient Descent(21/49): loss=1082.0595506190182\n",
      "Gradient Descent(22/49): loss=2325.586961112319\n",
      "Gradient Descent(23/49): loss=4998.67066955947\n",
      "Gradient Descent(24/49): loss=10744.724984873917\n",
      "Gradient Descent(25/49): loss=23096.429199719987\n",
      "Gradient Descent(26/49): loss=49647.621889370595\n",
      "Gradient Descent(27/49): loss=106721.99946397055\n",
      "Gradient Descent(28/49): loss=229408.93889855724\n",
      "Gradient Descent(29/49): loss=493136.47717047133\n",
      "Gradient Descent(30/49): loss=1060044.5338964998\n",
      "Gradient Descent(31/49): loss=2278668.6747182007\n",
      "Gradient Descent(32/49): loss=4898220.079731069\n",
      "Gradient Descent(33/49): loss=10529201.227630353\n",
      "Gradient Descent(34/49): loss=22633544.21819089\n",
      "Gradient Descent(35/49): loss=48653009.633587934\n",
      "Gradient Descent(36/49): loss=104584387.40681946\n",
      "Gradient Descent(37/49): loss=224814337.16429242\n",
      "Gradient Descent(38/49): loss=483260336.4258655\n",
      "Gradient Descent(39/49): loss=1038815209.9748139\n",
      "Gradient Descent(40/49): loss=2233034576.5187216\n",
      "Gradient Descent(41/49): loss=4800125539.6814165\n",
      "Gradient Descent(42/49): loss=10318337852.903624\n",
      "Gradient Descent(43/49): loss=22180273238.4273\n",
      "Gradient Descent(44/49): loss=47678659872.30147\n",
      "Gradient Descent(45/49): loss=102489928000.09286\n",
      "Gradient Descent(46/49): loss=220312092865.44278\n",
      "Gradient Descent(47/49): loss=473582323745.56793\n",
      "Gradient Descent(48/49): loss=1018011378528.0641\n",
      "Gradient Descent(49/49): loss=2188314712881.0598\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4001098643391406\n",
      "Gradient Descent(2/49): loss=0.38509975005546254\n",
      "Gradient Descent(3/49): loss=0.37950834126739064\n",
      "Gradient Descent(4/49): loss=0.3763239514476539\n",
      "Gradient Descent(5/49): loss=0.3750933820172454\n",
      "Gradient Descent(6/49): loss=0.37680542986949406\n",
      "Gradient Descent(7/49): loss=0.38413512187777166\n",
      "Gradient Descent(8/49): loss=0.40318987517548716\n",
      "Gradient Descent(9/49): loss=0.44753061990838294\n",
      "Gradient Descent(10/49): loss=0.5470564289646535\n",
      "Gradient Descent(11/49): loss=0.7675489614539838\n",
      "Gradient Descent(12/49): loss=1.2536351089322961\n",
      "Gradient Descent(13/49): loss=2.3232132125500007\n",
      "Gradient Descent(14/49): loss=4.6749818090788615\n",
      "Gradient Descent(15/49): loss=9.844539221021057\n",
      "Gradient Descent(16/49): loss=21.206780797205415\n",
      "Gradient Descent(17/49): loss=46.17892508061222\n",
      "Gradient Descent(18/49): loss=101.06222992236201\n",
      "Gradient Descent(19/49): loss=221.68290674738128\n",
      "Gradient Descent(20/49): loss=486.77826087088835\n",
      "Gradient Descent(21/49): loss=1069.3937367444103\n",
      "Gradient Descent(22/49): loss=2349.841115078255\n",
      "Gradient Descent(23/49): loss=5163.953139433049\n",
      "Gradient Descent(24/49): loss=11348.686767287376\n",
      "Gradient Descent(25/49): loss=24941.226331145244\n",
      "Gradient Descent(26/49): loss=54814.32045786835\n",
      "Gradient Descent(27/49): loss=120468.11124854733\n",
      "Gradient Descent(28/49): loss=264759.1652430935\n",
      "Gradient Descent(29/49): loss=581875.7855823997\n",
      "Gradient Descent(30/49): loss=1278820.9397148218\n",
      "Gradient Descent(31/49): loss=2810536.7712314427\n",
      "Gradient Descent(32/49): loss=6176875.391268941\n",
      "Gradient Descent(33/49): loss=13575268.403441167\n",
      "Gradient Descent(34/49): loss=29835135.677724533\n",
      "Gradient Descent(35/49): loss=65570366.739581734\n",
      "Gradient Descent(36/49): loss=144107707.63548177\n",
      "Gradient Descent(37/49): loss=316713669.08338773\n",
      "Gradient Descent(38/49): loss=696059564.7744483\n",
      "Gradient Descent(39/49): loss=1529769521.0847504\n",
      "Gradient Descent(40/49): loss=3362061102.8473415\n",
      "Gradient Descent(41/49): loss=7388992069.890508\n",
      "Gradient Descent(42/49): loss=16239206290.560461\n",
      "Gradient Descent(43/49): loss=35689823247.44035\n",
      "Gradient Descent(44/49): loss=78437545570.32611\n",
      "Gradient Descent(45/49): loss=172386635609.25397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=378863870875.117\n",
      "Gradient Descent(47/49): loss=832650583076.0372\n",
      "Gradient Descent(48/49): loss=1829963337215.6853\n",
      "Gradient Descent(49/49): loss=4021814052159.4585\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40052341097430294\n",
      "Gradient Descent(2/49): loss=0.3848242672182036\n",
      "Gradient Descent(3/49): loss=0.37964254843437306\n",
      "Gradient Descent(4/49): loss=0.37812306907910104\n",
      "Gradient Descent(5/49): loss=0.38135227111458436\n",
      "Gradient Descent(6/49): loss=0.394481923168748\n",
      "Gradient Descent(7/49): loss=0.43065817581746924\n",
      "Gradient Descent(8/49): loss=0.5221251199807307\n",
      "Gradient Descent(9/49): loss=0.7474637325454627\n",
      "Gradient Descent(10/49): loss=1.2978902098462863\n",
      "Gradient Descent(11/49): loss=2.638506162615362\n",
      "Gradient Descent(12/49): loss=5.900449107123296\n",
      "Gradient Descent(13/49): loss=13.834559438851274\n",
      "Gradient Descent(14/49): loss=33.13058376060653\n",
      "Gradient Descent(15/49): loss=80.05719218915092\n",
      "Gradient Descent(16/49): loss=194.17780287158638\n",
      "Gradient Descent(17/49): loss=471.7057679655084\n",
      "Gradient Descent(18/49): loss=1146.6200515094124\n",
      "Gradient Descent(19/49): loss=2787.9285019884333\n",
      "Gradient Descent(20/49): loss=6779.387674996773\n",
      "Gradient Descent(21/49): loss=16486.121966486196\n",
      "Gradient Descent(22/49): loss=40091.69670707523\n",
      "Gradient Descent(23/49): loss=97497.52818569788\n",
      "Gradient Descent(24/49): loss=237101.3934343606\n",
      "Gradient Descent(25/49): loss=576600.685815102\n",
      "Gradient Descent(26/49): loss=1402220.8737954944\n",
      "Gradient Descent(27/49): loss=3410026.81028638\n",
      "Gradient Descent(28/49): loss=8292761.919029385\n",
      "Gradient Descent(29/49): loss=20166968.339216802\n",
      "Gradient Descent(30/49): loss=49043566.18198339\n",
      "Gradient Descent(31/49): loss=119267871.99769275\n",
      "Gradient Descent(32/49): loss=290044677.2897978\n",
      "Gradient Descent(33/49): loss=705352694.7488828\n",
      "Gradient Descent(34/49): loss=1715330303.070711\n",
      "Gradient Descent(35/49): loss=4171470628.875857\n",
      "Gradient Descent(36/49): loss=10144499387.473248\n",
      "Gradient Descent(37/49): loss=24670164788.67076\n",
      "Gradient Descent(38/49): loss=59994782143.61769\n",
      "Gradient Descent(39/49): loss=145899871983.45987\n",
      "Gradient Descent(40/49): loss=354810399909.06024\n",
      "Gradient Descent(41/49): loss=862854902970.7283\n",
      "Gradient Descent(42/49): loss=2098356146752.5164\n",
      "Gradient Descent(43/49): loss=5102941993439.547\n",
      "Gradient Descent(44/49): loss=12409722262215.043\n",
      "Gradient Descent(45/49): loss=30178905976848.12\n",
      "Gradient Descent(46/49): loss=73391357736710.67\n",
      "Gradient Descent(47/49): loss=178478682910831.16\n",
      "Gradient Descent(48/49): loss=434038028944260.3\n",
      "Gradient Descent(49/49): loss=1055526674095527.8\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40048202243881886\n",
      "Gradient Descent(2/49): loss=0.3841407128290919\n",
      "Gradient Descent(3/49): loss=0.37858874616298266\n",
      "Gradient Descent(4/49): loss=0.3764368934825792\n",
      "Gradient Descent(5/49): loss=0.3780636407072688\n",
      "Gradient Descent(6/49): loss=0.386895076857654\n",
      "Gradient Descent(7/49): loss=0.41154429749073573\n",
      "Gradient Descent(8/49): loss=0.4723768530905109\n",
      "Gradient Descent(9/49): loss=0.6170727930301622\n",
      "Gradient Descent(10/49): loss=0.956994517534842\n",
      "Gradient Descent(11/49): loss=1.7520633561823957\n",
      "Gradient Descent(12/49): loss=3.6088087391658585\n",
      "Gradient Descent(13/49): loss=7.942474252909654\n",
      "Gradient Descent(14/49): loss=18.055235447196615\n",
      "Gradient Descent(15/49): loss=41.65196655723675\n",
      "Gradient Descent(16/49): loss=96.71018458780803\n",
      "Gradient Descent(17/49): loss=225.17616700612342\n",
      "Gradient Descent(18/49): loss=524.9215813427646\n",
      "Gradient Descent(19/49): loss=1224.3066700987051\n",
      "Gradient Descent(20/49): loss=2856.155684879462\n",
      "Gradient Descent(21/49): loss=6663.6871273715005\n",
      "Gradient Descent(22/49): loss=15547.655288244889\n",
      "Gradient Descent(23/49): loss=36276.27945156075\n",
      "Gradient Descent(24/49): loss=84641.58736347848\n",
      "Gradient Descent(25/49): loss=197490.51465089573\n",
      "Gradient Descent(26/49): loss=460796.6099159906\n",
      "Gradient Descent(27/49): loss=1075158.6728483222\n",
      "Gradient Descent(28/49): loss=2508626.2267099377\n",
      "Gradient Descent(29/49): loss=5853281.360544999\n",
      "Gradient Descent(30/49): loss=13657237.532299586\n",
      "Gradient Descent(31/49): loss=31865910.62206178\n",
      "Gradient Descent(32/49): loss=74351512.58999947\n",
      "Gradient Descent(33/49): loss=173481546.1557068\n",
      "Gradient Descent(34/49): loss=404777870.0761764\n",
      "Gradient Descent(35/49): loss=944452755.020564\n",
      "Gradient Descent(36/49): loss=2203655566.8208394\n",
      "Gradient Descent(37/49): loss=5141705428.839291\n",
      "Gradient Descent(38/49): loss=11996945038.227337\n",
      "Gradient Descent(39/49): loss=27992013981.624252\n",
      "Gradient Descent(40/49): loss=65312697879.18737\n",
      "Gradient Descent(41/49): loss=152391625235.5369\n",
      "Gradient Descent(42/49): loss=355569562980.3188\n",
      "Gradient Descent(43/49): loss=829636890627.7129\n",
      "Gradient Descent(44/49): loss=1935760092965.8972\n",
      "Gradient Descent(45/49): loss=4516635144665.404\n",
      "Gradient Descent(46/49): loss=10538492401075.791\n",
      "Gradient Descent(47/49): loss=24589062107154.285\n",
      "Gradient Descent(48/49): loss=57372720148071.54\n",
      "Gradient Descent(49/49): loss=133865578233308.62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3987947162322592\n",
      "Gradient Descent(2/49): loss=0.38256682132540587\n",
      "Gradient Descent(3/49): loss=0.37710334297623244\n",
      "Gradient Descent(4/49): loss=0.3748754306482461\n",
      "Gradient Descent(5/49): loss=0.3761347874836849\n",
      "Gradient Descent(6/49): loss=0.38398615849876694\n",
      "Gradient Descent(7/49): loss=0.40627424268709816\n",
      "Gradient Descent(8/49): loss=0.4615655934295699\n",
      "Gradient Descent(9/49): loss=0.5933510544023938\n",
      "Gradient Descent(10/49): loss=0.9032526179462038\n",
      "Gradient Descent(11/49): loss=1.6285543271395826\n",
      "Gradient Descent(12/49): loss=3.323183153302323\n",
      "Gradient Descent(13/49): loss=7.280157157585123\n",
      "Gradient Descent(14/49): loss=16.517661247564607\n",
      "Gradient Descent(15/49): loss=38.080733581246136\n",
      "Gradient Descent(16/49): loss=88.41383143790983\n",
      "Gradient Descent(17/49): loss=205.901400841272\n",
      "Gradient Descent(18/49): loss=480.13990057085357\n",
      "Gradient Descent(19/49): loss=1120.2641540711488\n",
      "Gradient Descent(20/49): loss=2614.4337701142126\n",
      "Gradient Descent(21/49): loss=6102.104127700321\n",
      "Gradient Descent(22/49): loss=14242.976130637226\n",
      "Gradient Descent(23/49): loss=33245.286636849705\n",
      "Gradient Descent(24/49): loss=77600.21576279498\n",
      "Gradient Descent(25/49): loss=181132.8745524102\n",
      "Gradient Descent(26/49): loss=422797.3666617839\n",
      "Gradient Descent(27/49): loss=986887.2625038424\n",
      "Gradient Descent(28/49): loss=2303578.0503977463\n",
      "Gradient Descent(29/49): loss=5376979.370930328\n",
      "Gradient Descent(30/49): loss=12550869.978806477\n",
      "Gradient Descent(31/49): loss=29296065.63852051\n",
      "Gradient Descent(32/49): loss=68382468.40202622\n",
      "Gradient Descent(33/49): loss=159617405.99452016\n",
      "Gradient Descent(34/49): loss=372576728.1369163\n",
      "Gradient Descent(35/49): loss=869663415.365227\n",
      "Gradient Descent(36/49): loss=2029956245.6286187\n",
      "Gradient Descent(37/49): loss=4738295628.981028\n",
      "Gradient Descent(38/49): loss=11060063741.391672\n",
      "Gradient Descent(39/49): loss=25816246926.094368\n",
      "Gradient Descent(40/49): loss=60259924439.830154\n",
      "Gradient Descent(41/49): loss=140657877340.01727\n",
      "Gradient Descent(42/49): loss=328321660568.83374\n",
      "Gradient Descent(43/49): loss=766363852755.8827\n",
      "Gradient Descent(44/49): loss=1788835844072.8325\n",
      "Gradient Descent(45/49): loss=4175475742407.7627\n",
      "Gradient Descent(46/49): loss=9746337392111.6\n",
      "Gradient Descent(47/49): loss=22749765157562.754\n",
      "Gradient Descent(48/49): loss=53102185354587.24\n",
      "Gradient Descent(49/49): loss=123950382340341.88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4010728700786666\n",
      "Gradient Descent(2/49): loss=0.3850575321403322\n",
      "Gradient Descent(3/49): loss=0.37938716878086054\n",
      "Gradient Descent(4/49): loss=0.3765935581000695\n",
      "Gradient Descent(5/49): loss=0.3765502580629737\n",
      "Gradient Descent(6/49): loss=0.3814800590674414\n",
      "Gradient Descent(7/49): loss=0.3972965805969652\n",
      "Gradient Descent(8/49): loss=0.43837156657813847\n",
      "Gradient Descent(9/49): loss=0.5391489533411119\n",
      "Gradient Descent(10/49): loss=0.7819086923753863\n",
      "Gradient Descent(11/49): loss=1.3630213087792966\n",
      "Gradient Descent(12/49): loss=2.7510158357691483\n",
      "Gradient Descent(13/49): loss=6.063675247247198\n",
      "Gradient Descent(14/49): loss=13.96764627988205\n",
      "Gradient Descent(15/49): loss=32.82456910717376\n",
      "Gradient Descent(16/49): loss=77.81092946728403\n",
      "Gradient Descent(17/49): loss=185.13208311061652\n",
      "Gradient Descent(18/49): loss=441.1602816429829\n",
      "Gradient Descent(19/49): loss=1051.9469398673848\n",
      "Gradient Descent(20/49): loss=2509.0525140647337\n",
      "Gradient Descent(21/49): loss=5985.153630370094\n",
      "Gradient Descent(22/49): loss=14277.811490856682\n",
      "Gradient Descent(23/49): loss=34060.944964412\n",
      "Gradient Descent(24/49): loss=81255.99055122674\n",
      "Gradient Descent(25/49): loss=193845.4507570311\n",
      "Gradient Descent(26/49): loss=462441.1555157251\n",
      "Gradient Descent(27/49): loss=1103208.5279047496\n",
      "Gradient Descent(28/49): loss=2631836.1945389668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=6278561.423862379\n",
      "Gradient Descent(30/49): loss=14978263.246659627\n",
      "Gradient Descent(31/49): loss=35732448.726871334\n",
      "Gradient Descent(32/49): loss=85244055.41395485\n",
      "Gradient Descent(33/49): loss=203359950.59422317\n",
      "Gradient Descent(34/49): loss=485139630.70844597\n",
      "Gradient Descent(35/49): loss=1157358962.4425788\n",
      "Gradient Descent(36/49): loss=2761019062.325771\n",
      "Gradient Descent(37/49): loss=6586743189.186133\n",
      "Gradient Descent(38/49): loss=15713468419.743456\n",
      "Gradient Descent(39/49): loss=37486369620.13774\n",
      "Gradient Descent(40/49): loss=89428245233.40463\n",
      "Gradient Descent(41/49): loss=213341839356.3372\n",
      "Gradient Descent(42/49): loss=508952627899.7746\n",
      "Gradient Descent(43/49): loss=1214167733004.167\n",
      "Gradient Descent(44/49): loss=2896543220441.2363\n",
      "Gradient Descent(45/49): loss=6910052375651.598\n",
      "Gradient Descent(46/49): loss=16484761386359.229\n",
      "Gradient Descent(47/49): loss=39326381797443.33\n",
      "Gradient Descent(48/49): loss=93817815680246.56\n",
      "Gradient Descent(49/49): loss=223813687827868.6\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4016758061049996\n",
      "Gradient Descent(2/49): loss=0.38495294334571795\n",
      "Gradient Descent(3/49): loss=0.3798344857416121\n",
      "Gradient Descent(4/49): loss=0.37943290495131465\n",
      "Gradient Descent(5/49): loss=0.38627896780119725\n",
      "Gradient Descent(6/49): loss=0.4102365975398882\n",
      "Gradient Descent(7/49): loss=0.47802456181941627\n",
      "Gradient Descent(8/49): loss=0.6602474051063405\n",
      "Gradient Descent(9/49): loss=1.1428198835691472\n",
      "Gradient Descent(10/49): loss=2.4149191131807295\n",
      "Gradient Descent(11/49): loss=5.763411785947229\n",
      "Gradient Descent(12/49): loss=14.573442225296882\n",
      "Gradient Descent(13/49): loss=37.749598704081805\n",
      "Gradient Descent(14/49): loss=98.71519846569458\n",
      "Gradient Descent(15/49): loss=259.0846546991716\n",
      "Gradient Descent(16/49): loss=680.9329691362275\n",
      "Gradient Descent(17/49): loss=1790.5938696061787\n",
      "Gradient Descent(18/49): loss=4709.5261713838945\n",
      "Gradient Descent(19/49): loss=12387.695534602672\n",
      "Gradient Descent(20/49): loss=32584.903067682764\n",
      "Gradient Descent(21/49): loss=85713.08575437528\n",
      "Gradient Descent(22/49): loss=225465.26554521587\n",
      "Gradient Descent(23/49): loss=593079.4154985533\n",
      "Gradient Descent(24/49): loss=1560078.0223004\n",
      "Gradient Descent(25/49): loss=4103740.461479676\n",
      "Gradient Descent(26/49): loss=10794772.442801258\n",
      "Gradient Descent(27/49): loss=28395342.504440363\n",
      "Gradient Descent(28/49): loss=74693143.40522341\n",
      "Gradient Descent(29/49): loss=196478196.9992665\n",
      "Gradient Descent(30/49): loss=516830330.15291756\n",
      "Gradient Descent(31/49): loss=1359507540.4299061\n",
      "Gradient Descent(32/49): loss=3576146067.945153\n",
      "Gradient Descent(33/49): loss=9406950914.382166\n",
      "Gradient Descent(34/49): loss=24744717868.808987\n",
      "Gradient Descent(35/49): loss=65090279303.95947\n",
      "Gradient Descent(36/49): loss=171218135617.99432\n",
      "Gradient Descent(37/49): loss=450384454914.75653\n",
      "Gradient Descent(38/49): loss=1184723548687.613\n",
      "Gradient Descent(39/49): loss=3116381730095.369\n",
      "Gradient Descent(40/49): loss=8197553849952.741\n",
      "Gradient Descent(41/49): loss=21563433155160.91\n",
      "Gradient Descent(42/49): loss=56721999995131.164\n",
      "Gradient Descent(43/49): loss=149205613980708.38\n",
      "Gradient Descent(44/49): loss=392481140391205.1\n",
      "Gradient Descent(45/49): loss=1032410520308596.0\n",
      "Gradient Descent(46/49): loss=2715726624167006.5\n",
      "Gradient Descent(47/49): loss=7143641944877706.0\n",
      "Gradient Descent(48/49): loss=1.8791147747527936e+16\n",
      "Gradient Descent(49/49): loss=4.942958177272826e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4016628759286517\n",
      "Gradient Descent(2/49): loss=0.38426891091697074\n",
      "Gradient Descent(3/49): loss=0.378710331923595\n",
      "Gradient Descent(4/49): loss=0.377448991598477\n",
      "Gradient Descent(5/49): loss=0.3818808278228919\n",
      "Gradient Descent(6/49): loss=0.3987707034881455\n",
      "Gradient Descent(7/49): loss=0.44597933342409135\n",
      "Gradient Descent(8/49): loss=0.5689289965602332\n",
      "Gradient Descent(9/49): loss=0.8825146184973278\n",
      "Gradient Descent(10/49): loss=1.6770195711718383\n",
      "Gradient Descent(11/49): loss=3.685619181020792\n",
      "Gradient Descent(12/49): loss=8.759943142482932\n",
      "Gradient Descent(13/49): loss=21.576141629400908\n",
      "Gradient Descent(14/49): loss=53.943374919150166\n",
      "Gradient Descent(15/49): loss=135.6844414111569\n",
      "Gradient Descent(16/49): loss=342.1136309337405\n",
      "Gradient Descent(17/49): loss=863.4290783025211\n",
      "Gradient Descent(18/49): loss=2179.9556620895505\n",
      "Gradient Descent(19/49): loss=5504.7020280619445\n",
      "Gradient Descent(20/49): loss=13900.990576572673\n",
      "Gradient Descent(21/49): loss=35104.91134319946\n",
      "Gradient Descent(22/49): loss=88653.12457846345\n",
      "Gradient Descent(23/49): loss=223883.35668573805\n",
      "Gradient Descent(24/49): loss=565392.709480352\n",
      "Gradient Descent(25/49): loss=1427837.712799928\n",
      "Gradient Descent(26/49): loss=3605849.4642031356\n",
      "Gradient Descent(27/49): loss=9106183.016669653\n",
      "Gradient Descent(28/49): loss=22996681.618502203\n",
      "Gradient Descent(29/49): loss=58075636.29740456\n",
      "Gradient Descent(30/49): loss=146663749.41224506\n",
      "Gradient Descent(31/49): loss=370383465.6088933\n",
      "Gradient Descent(32/49): loss=935363457.3398975\n",
      "Gradient Descent(33/49): loss=2362159434.3906193\n",
      "Gradient Descent(34/49): loss=5965378645.554124\n",
      "Gradient Descent(35/49): loss=15064919780.043497\n",
      "Gradient Descent(36/49): loss=38044828579.85569\n",
      "Gradient Descent(37/49): loss=96078107471.92268\n",
      "Gradient Descent(38/49): loss=242634888367.0667\n",
      "Gradient Descent(39/49): loss=612748217071.036\n",
      "Gradient Descent(40/49): loss=1547429473357.1677\n",
      "Gradient Descent(41/49): loss=3907866083170.185\n",
      "Gradient Descent(42/49): loss=9868893921777.236\n",
      "Gradient Descent(43/49): loss=24922826209102.152\n",
      "Gradient Descent(44/49): loss=62939907062779.445\n",
      "Gradient Descent(45/49): loss=158947940648266.88\n",
      "Gradient Descent(46/49): loss=401405864980452.9\n",
      "Gradient Descent(47/49): loss=1013707178485952.2\n",
      "Gradient Descent(48/49): loss=2560008045134126.0\n",
      "Gradient Descent(49/49): loss=6465023953899356.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4000112377938508\n",
      "Gradient Descent(2/49): loss=0.38269898583487405\n",
      "Gradient Descent(3/49): loss=0.37719725725638215\n",
      "Gradient Descent(4/49): loss=0.37577243273691396\n",
      "Gradient Descent(5/49): loss=0.37958216918009247\n",
      "Gradient Descent(6/49): loss=0.3947687127109525\n",
      "Gradient Descent(7/49): loss=0.4376024471733907\n",
      "Gradient Descent(8/49): loss=0.5494901157581021\n",
      "Gradient Descent(9/49): loss=0.8352165373056858\n",
      "Gradient Descent(10/49): loss=1.5596294096112635\n",
      "Gradient Descent(11/49): loss=3.3919250590617755\n",
      "Gradient Descent(12/49): loss=8.022816873682862\n",
      "Gradient Descent(13/49): loss=19.723741208483116\n",
      "Gradient Descent(14/49): loss=49.28600295303018\n",
      "Gradient Descent(15/49): loss=123.97254489248363\n",
      "Gradient Descent(16/49): loss=312.65987116821907\n",
      "Gradient Descent(17/49): loss=789.3560270458759\n",
      "Gradient Descent(18/49): loss=1993.6709664028904\n",
      "Gradient Descent(19/49): loss=5036.225206504968\n",
      "Gradient Descent(20/49): loss=12722.864950857342\n",
      "Gradient Descent(21/49): loss=32142.21567268554\n",
      "Gradient Descent(22/49): loss=81202.81811021\n",
      "Gradient Descent(23/49): loss=205148.39863307335\n",
      "Gradient Descent(24/49): loss=518281.66930863174\n",
      "Gradient Descent(25/49): loss=1309374.378923601\n",
      "Gradient Descent(26/49): loss=3307972.8469652086\n",
      "Gradient Descent(27/49): loss=8357186.15356678\n",
      "Gradient Descent(28/49): loss=21113402.783632956\n",
      "Gradient Descent(29/49): loss=53340415.7519385\n",
      "Gradient Descent(30/49): loss=134758001.77880225\n",
      "Gradient Descent(31/49): loss=340449522.769374\n",
      "Gradient Descent(32/49): loss=860103861.249016\n",
      "Gradient Descent(33/49): loss=2172946657.102756\n",
      "Gradient Descent(34/49): loss=5489682569.800927\n",
      "Gradient Descent(35/49): loss=13869008068.12044\n",
      "Gradient Descent(36/49): loss=35038343720.67268\n",
      "Gradient Descent(37/49): loss=88520067525.32848\n",
      "Gradient Descent(38/49): loss=223635067261.87778\n",
      "Gradient Descent(39/49): loss=564986502015.3369\n",
      "Gradient Descent(40/49): loss=1427368933539.9485\n",
      "Gradient Descent(41/49): loss=3606072118834.6543\n",
      "Gradient Descent(42/49): loss=9110297849895.934\n",
      "Gradient Descent(43/49): loss=23016047427427.926\n",
      "Gradient Descent(44/49): loss=58147214054879.266\n",
      "Gradient Descent(45/49): loss=146901787242358.44\n",
      "Gradient Descent(46/49): loss=371129304228254.25\n",
      "Gradient Descent(47/49): loss=937612557631535.0\n",
      "Gradient Descent(48/49): loss=2368762849531384.0\n",
      "Gradient Descent(49/49): loss=5984388105353559.0\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4022046689465363\n",
      "Gradient Descent(2/49): loss=0.38513168561249383\n",
      "Gradient Descent(3/49): loss=0.37934717843574634\n",
      "Gradient Descent(4/49): loss=0.37708396791175147\n",
      "Gradient Descent(5/49): loss=0.3788133679938334\n",
      "Gradient Descent(6/49): loss=0.3889630669167059\n",
      "Gradient Descent(7/49): loss=0.41972494433471436\n",
      "Gradient Descent(8/49): loss=0.5028784967702575\n",
      "Gradient Descent(9/49): loss=0.7206339118852078\n",
      "Gradient Descent(10/49): loss=1.2853165399689008\n",
      "Gradient Descent(11/49): loss=2.7450715889189463\n",
      "Gradient Descent(12/49): loss=6.514837425681487\n",
      "Gradient Descent(13/49): loss=16.24689318742648\n",
      "Gradient Descent(14/49): loss=41.36850276537583\n",
      "Gradient Descent(15/49): loss=106.21323948057885\n",
      "Gradient Descent(16/49): loss=273.59064830199134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=705.6239628998923\n",
      "Gradient Descent(18/49): loss=1820.7836568589087\n",
      "Gradient Descent(19/49): loss=4699.22047207631\n",
      "Gradient Descent(20/49): loss=12129.005934976063\n",
      "Gradient Descent(21/49): loss=31306.675220316232\n",
      "Gradient Descent(22/49): loss=80807.83443056222\n",
      "Gradient Descent(23/49): loss=208579.60444575473\n",
      "Gradient Descent(24/49): loss=538382.4907222535\n",
      "Gradient Descent(25/49): loss=1389665.553580312\n",
      "Gradient Descent(26/49): loss=3586986.690241453\n",
      "Gradient Descent(27/49): loss=9258684.375658896\n",
      "Gradient Descent(28/49): loss=23898399.116126116\n",
      "Gradient Descent(29/49): loss=61686246.700336374\n",
      "Gradient Descent(30/49): loss=159223763.6777693\n",
      "Gradient Descent(31/49): loss=410986375.9014744\n",
      "Gradient Descent(32/49): loss=1060832864.4925525\n",
      "Gradient Descent(33/49): loss=2738208448.595924\n",
      "Gradient Descent(34/49): loss=7067829212.179781\n",
      "Gradient Descent(35/49): loss=18243391879.305782\n",
      "Gradient Descent(36/49): loss=47089613695.623\n",
      "Gradient Descent(37/49): loss=121547118687.63829\n",
      "Gradient Descent(38/49): loss=313735894220.8027\n",
      "Gradient Descent(39/49): loss=809811144726.4238\n",
      "Gradient Descent(40/49): loss=2090274342858.1047\n",
      "Gradient Descent(41/49): loss=5395389847206.502\n",
      "Gradient Descent(42/49): loss=13926512423024.15\n",
      "Gradient Descent(43/49): loss=35946938731232.27\n",
      "Gradient Descent(44/49): loss=92785786196599.84\n",
      "Gradient Descent(45/49): loss=239497504488211.88\n",
      "Gradient Descent(46/49): loss=618187946745941.4\n",
      "Gradient Descent(47/49): loss=1595658954019539.5\n",
      "Gradient Descent(48/49): loss=4118694825651773.5\n",
      "Gradient Descent(49/49): loss=1.063112328866853e+16\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4029986133023447\n",
      "Gradient Descent(2/49): loss=0.385223563070368\n",
      "Gradient Descent(3/49): loss=0.3801763312055322\n",
      "Gradient Descent(4/49): loss=0.3812762558437261\n",
      "Gradient Descent(5/49): loss=0.3934065017532058\n",
      "Gradient Descent(6/49): loss=0.43448578475074984\n",
      "Gradient Descent(7/49): loss=0.5563321379092282\n",
      "Gradient Descent(8/49): loss=0.9063308636664497\n",
      "Gradient Descent(9/49): loss=1.9028137097867428\n",
      "Gradient Descent(10/49): loss=4.732681747812585\n",
      "Gradient Descent(11/49): loss=12.763122346529462\n",
      "Gradient Descent(12/49): loss=35.5464672682331\n",
      "Gradient Descent(13/49): loss=100.1814263686251\n",
      "Gradient Descent(14/49): loss=283.5433353037844\n",
      "Gradient Descent(15/49): loss=803.7168982749596\n",
      "Gradient Descent(16/49): loss=2279.3781919585053\n",
      "Gradient Descent(17/49): loss=6465.625414096949\n",
      "Gradient Descent(18/49): loss=18341.428596558973\n",
      "Gradient Descent(19/49): loss=52031.434736407035\n",
      "Gradient Descent(20/49): loss=147605.30709813978\n",
      "Gradient Descent(21/49): loss=418735.1193250001\n",
      "Gradient Descent(22/49): loss=1187892.7684204525\n",
      "Gradient Descent(23/49): loss=3369886.272056743\n",
      "Gradient Descent(24/49): loss=9559899.015103841\n",
      "Gradient Descent(25/49): loss=27120106.088928945\n",
      "Gradient Descent(26/49): loss=76935976.47114524\n",
      "Gradient Descent(27/49): loss=218256687.06955868\n",
      "Gradient Descent(28/49): loss=619163929.8976668\n",
      "Gradient Descent(29/49): loss=1756482138.0456355\n",
      "Gradient Descent(30/49): loss=4982896052.2967825\n",
      "Gradient Descent(31/49): loss=14135784550.428694\n",
      "Gradient Descent(32/49): loss=40101258939.56382\n",
      "Gradient Descent(33/49): loss=113761706173.2894\n",
      "Gradient Descent(34/49): loss=322726172038.8807\n",
      "Gradient Descent(35/49): loss=915529360647.5314\n",
      "Gradient Descent(36/49): loss=2597229734770.6025\n",
      "Gradient Descent(37/49): loss=7367980302025.946\n",
      "Gradient Descent(38/49): loss=20901937554569.445\n",
      "Gradient Descent(39/49): loss=59295895975047.37\n",
      "Gradient Descent(40/49): loss=168214227523371.94\n",
      "Gradient Descent(41/49): loss=477200417937745.0\n",
      "Gradient Descent(42/49): loss=1353751357615166.5\n",
      "Gradient Descent(43/49): loss=3840404721699123.5\n",
      "Gradient Descent(44/49): loss=1.089469520638637e+16\n",
      "Gradient Descent(45/49): loss=3.090673828448571e+16\n",
      "Gradient Descent(46/49): loss=8.767812713344938e+16\n",
      "Gradient Descent(47/49): loss=2.4873067830285907e+17\n",
      "Gradient Descent(48/49): loss=7.056144143548887e+17\n",
      "Gradient Descent(49/49): loss=2.0017301651030377e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4030155447154001\n",
      "Gradient Descent(2/49): loss=0.38454388199539535\n",
      "Gradient Descent(3/49): loss=0.37897135786551717\n",
      "Gradient Descent(4/49): loss=0.37890917637112836\n",
      "Gradient Descent(5/49): loss=0.38745871986890806\n",
      "Gradient Descent(6/49): loss=0.41716373286502356\n",
      "Gradient Descent(7/49): loss=0.503201290065664\n",
      "Gradient Descent(8/49): loss=0.7418147425130063\n",
      "Gradient Descent(9/49): loss=1.3954935394363523\n",
      "Gradient Descent(10/49): loss=3.1797003597921027\n",
      "Gradient Descent(11/49): loss=8.04426817997547\n",
      "Gradient Descent(12/49): loss=21.30281619360821\n",
      "Gradient Descent(13/49): loss=57.435676685909414\n",
      "Gradient Descent(14/49): loss=155.90358777415227\n",
      "Gradient Descent(15/49): loss=424.24195117814406\n",
      "Gradient Descent(16/49): loss=1155.4979792285292\n",
      "Gradient Descent(17/49): loss=3148.2613446454097\n",
      "Gradient Descent(18/49): loss=8578.787005725362\n",
      "Gradient Descent(19/49): loss=23377.637012285984\n",
      "Gradient Descent(20/49): loss=63706.32130557977\n",
      "Gradient Descent(21/49): loss=173606.93930189204\n",
      "Gradient Descent(22/49): loss=473099.62077985605\n",
      "Gradient Descent(23/49): loss=1289253.9591973182\n",
      "Gradient Descent(24/49): loss=3513374.7645039703\n",
      "Gradient Descent(25/49): loss=9574377.105922647\n",
      "Gradient Descent(26/49): loss=26091352.844766725\n",
      "Gradient Descent(27/49): loss=71102140.20091654\n",
      "Gradient Descent(28/49): loss=193762063.56988662\n",
      "Gradient Descent(29/49): loss=528025418.7477909\n",
      "Gradient Descent(30/49): loss=1438934112.8502479\n",
      "Gradient Descent(31/49): loss=3921272173.9571457\n",
      "Gradient Descent(32/49): loss=10685948249.084024\n",
      "Gradient Descent(33/49): loss=29120521330.957623\n",
      "Gradient Descent(34/49): loss=79356996948.8314\n",
      "Gradient Descent(35/49): loss=216257562603.20325\n",
      "Gradient Descent(36/49): loss=589328416917.2451\n",
      "Gradient Descent(37/49): loss=1605992312156.3003\n",
      "Gradient Descent(38/49): loss=4376526284270.311\n",
      "Gradient Descent(39/49): loss=11926571610541.143\n",
      "Gradient Descent(40/49): loss=32501372353826.223\n",
      "Gradient Descent(41/49): loss=88570231192707.36\n",
      "Gradient Descent(42/49): loss=241364757405592.94\n",
      "Gradient Descent(43/49): loss=657748606195990.8\n",
      "Gradient Descent(44/49): loss=1792445730698629.2\n",
      "Gradient Descent(45/49): loss=4884634748343881.0\n",
      "Gradient Descent(46/49): loss=1.3311229576489696e+16\n",
      "Gradient Descent(47/49): loss=3.627473536237125e+16\n",
      "Gradient Descent(48/49): loss=9.88531088017699e+16\n",
      "Gradient Descent(49/49): loss=2.6938686174155315e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4014032328371997\n",
      "Gradient Descent(2/49): loss=0.38297793197967583\n",
      "Gradient Descent(3/49): loss=0.3774220786814928\n",
      "Gradient Descent(4/49): loss=0.37707813913232185\n",
      "Gradient Descent(5/49): loss=0.3846319733814299\n",
      "Gradient Descent(6/49): loss=0.41148344317039387\n",
      "Gradient Descent(7/49): loss=0.4896782514524227\n",
      "Gradient Descent(8/49): loss=0.7069424966949753\n",
      "Gradient Descent(9/49): loss=1.3026355206861053\n",
      "Gradient Descent(10/49): loss=2.929430600764691\n",
      "Gradient Descent(11/49): loss=7.366730224244746\n",
      "Gradient Descent(12/49): loss=19.46556985940169\n",
      "Gradient Descent(13/49): loss=52.450763509010855\n",
      "Gradient Descent(14/49): loss=142.37545993018585\n",
      "Gradient Descent(15/49): loss=387.52672851650937\n",
      "Gradient Descent(16/49): loss=1055.8519026655704\n",
      "Gradient Descent(17/49): loss=2877.8210975578127\n",
      "Gradient Descent(18/49): loss=7844.820335339138\n",
      "Gradient Descent(19/49): loss=21385.707242725253\n",
      "Gradient Descent(20/49): loss=58300.472744657956\n",
      "Gradient Descent(21/49): loss=158936.4138727872\n",
      "Gradient Descent(22/49): loss=433287.13708460657\n",
      "Gradient Descent(23/49): loss=1181213.9552990978\n",
      "Gradient Descent(24/49): loss=3220189.9006259916\n",
      "Gradient Descent(25/49): loss=8778785.74705772\n",
      "Gradient Descent(26/49): loss=23932465.148561046\n",
      "Gradient Descent(27/49): loss=65243977.21834049\n",
      "Gradient Descent(28/49): loss=177866198.15009987\n",
      "Gradient Descent(29/49): loss=484893562.00358087\n",
      "Gradient Descent(30/49): loss=1321902469.9694824\n",
      "Gradient Descent(31/49): loss=3603731370.2380743\n",
      "Gradient Descent(32/49): loss=9824385751.030973\n",
      "Gradient Descent(33/49): loss=26782949524.46846\n",
      "Gradient Descent(34/49): loss=73014883924.4435\n",
      "Gradient Descent(35/49): loss=199051014514.19992\n",
      "Gradient Descent(36/49): loss=542646981678.5531\n",
      "Gradient Descent(37/49): loss=1479348133159.3193\n",
      "Gradient Descent(38/49): loss=4032955075716.1255\n",
      "Gradient Descent(39/49): loss=10994522707789.135\n",
      "Gradient Descent(40/49): loss=29972942247722.25\n",
      "Gradient Descent(41/49): loss=81711347628471.4\n",
      "Gradient Descent(42/49): loss=222759056354181.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=607279143325703.9\n",
      "Gradient Descent(44/49): loss=1655546418422766.0\n",
      "Gradient Descent(45/49): loss=4513301623603299.0\n",
      "Gradient Descent(46/49): loss=1.2304029243122466e+16\n",
      "Gradient Descent(47/49): loss=3.3542880188617244e+16\n",
      "Gradient Descent(48/49): loss=9.144360673368904e+16\n",
      "Gradient Descent(49/49): loss=2.492908529454007e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4035052609427499\n",
      "Gradient Descent(2/49): loss=0.3853419699375532\n",
      "Gradient Descent(3/49): loss=0.37940753053305876\n",
      "Gradient Descent(4/49): loss=0.37785381149994324\n",
      "Gradient Descent(5/49): loss=0.3821776969807922\n",
      "Gradient Descent(6/49): loss=0.4006016429737884\n",
      "Gradient Descent(7/49): loss=0.45699442584460415\n",
      "Gradient Descent(8/49): loss=0.6181893622777405\n",
      "Gradient Descent(9/49): loss=1.0704897260342463\n",
      "Gradient Descent(10/49): loss=2.332791932163514\n",
      "Gradient Descent(11/49): loss=5.850047733770357\n",
      "Gradient Descent(12/49): loss=15.645747329552435\n",
      "Gradient Descent(13/49): loss=42.92319582438613\n",
      "Gradient Descent(14/49): loss=118.87757816002168\n",
      "Gradient Descent(15/49): loss=330.3706177539329\n",
      "Gradient Descent(16/49): loss=919.265164660426\n",
      "Gradient Descent(17/49): loss=2559.0181787172833\n",
      "Gradient Descent(18/49): loss=7124.842255693832\n",
      "Gradient Descent(19/49): loss=19838.18799612818\n",
      "Gradient Descent(20/49): loss=55237.964941053215\n",
      "Gradient Descent(21/49): loss=153807.15505143223\n",
      "Gradient Descent(22/49): loss=428268.91843977314\n",
      "Gradient Descent(23/49): loss=1192496.152045421\n",
      "Gradient Descent(24/49): loss=3320454.7323806286\n",
      "Gradient Descent(25/49): loss=9245665.873767981\n",
      "Gradient Descent(26/49): loss=25744167.01622808\n",
      "Gradient Descent(27/49): loss=71683549.33516942\n",
      "Gradient Descent(28/49): loss=199599826.67547172\n",
      "Gradient Descent(29/49): loss=555777319.3326796\n",
      "Gradient Descent(30/49): loss=1547538562.806253\n",
      "Gradient Descent(31/49): loss=4309056020.628252\n",
      "Gradient Descent(32/49): loss=11998385201.45598\n",
      "Gradient Descent(33/49): loss=33408998805.817516\n",
      "Gradient Descent(34/49): loss=93025951615.99353\n",
      "Gradient Descent(35/49): loss=259026848556.49756\n",
      "Gradient Descent(36/49): loss=721249362222.8932\n",
      "Gradient Descent(37/49): loss=2008288505250.368\n",
      "Gradient Descent(38/49): loss=5591994851671.167\n",
      "Gradient Descent(39/49): loss=15570674402291.986\n",
      "Gradient Descent(40/49): loss=43355887795523.15\n",
      "Gradient Descent(41/49): loss=120722645530450.56\n",
      "Gradient Descent(42/49): loss=336147127527533.25\n",
      "Gradient Descent(43/49): loss=935987534472250.9\n",
      "Gradient Descent(44/49): loss=2606217911577109.5\n",
      "Gradient Descent(45/49): loss=7256904127954202.0\n",
      "Gradient Descent(46/49): loss=2.020654423729631e+16\n",
      "Gradient Descent(47/49): loss=5.6264272314277464e+16\n",
      "Gradient Descent(48/49): loss=1.5666549915112048e+17\n",
      "Gradient Descent(49/49): loss=4.362284912736773e+17\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4044918325663382\n",
      "Gradient Descent(2/49): loss=0.38565838622324883\n",
      "Gradient Descent(3/49): loss=0.38069987403452304\n",
      "Gradient Descent(4/49): loss=0.3837898632419403\n",
      "Gradient Descent(5/49): loss=0.4035073617433948\n",
      "Gradient Descent(6/49): loss=0.47111302640388836\n",
      "Gradient Descent(7/49): loss=0.6832575957943757\n",
      "Gradient Descent(8/49): loss=1.3352984564189012\n",
      "Gradient Descent(9/49): loss=3.32863995354152\n",
      "Gradient Descent(10/49): loss=9.413679768948477\n",
      "Gradient Descent(11/49): loss=27.982132590241612\n",
      "Gradient Descent(12/49): loss=84.63759448971233\n",
      "Gradient Descent(13/49): loss=257.4978597061905\n",
      "Gradient Descent(14/49): loss=784.9038707888589\n",
      "Gradient Descent(15/49): loss=2394.0445391802973\n",
      "Gradient Descent(16/49): loss=7303.60575152965\n",
      "Gradient Descent(17/49): loss=22282.897267554337\n",
      "Gradient Descent(18/49): loss=67985.38550561662\n",
      "Gradient Descent(19/49): loss=207425.7189025038\n",
      "Gradient Descent(20/49): loss=632864.4040535089\n",
      "Gradient Descent(21/49): loss=1930896.8328325788\n",
      "Gradient Descent(22/49): loss=5891251.742849636\n",
      "Gradient Descent(23/49): loss=17974471.440746464\n",
      "Gradient Descent(24/49): loss=54840914.368556194\n",
      "Gradient Descent(25/49): loss=167322078.17427778\n",
      "Gradient Descent(26/49): loss=510507132.28536266\n",
      "Gradient Descent(27/49): loss=1557580058.8118014\n",
      "Gradient Descent(28/49): loss=4752246319.276476\n",
      "Gradient Descent(29/49): loss=14499315751.783669\n",
      "Gradient Descent(30/49): loss=44238059888.484535\n",
      "Gradient Descent(31/49): loss=134972296363.56412\n",
      "Gradient Descent(32/49): loss=411806503984.18304\n",
      "Gradient Descent(33/49): loss=1256440034680.0068\n",
      "Gradient Descent(34/49): loss=3833454657646.547\n",
      "Gradient Descent(35/49): loss=11696041360204.533\n",
      "Gradient Descent(36/49): loss=35685144527990.12\n",
      "Gradient Descent(37/49): loss=108876969631485.2\n",
      "Gradient Descent(38/49): loss=332188496724101.9\n",
      "Gradient Descent(39/49): loss=1013521938839011.5\n",
      "Gradient Descent(40/49): loss=3092300698663794.5\n",
      "Gradient Descent(41/49): loss=9434747531869064.0\n",
      "Gradient Descent(42/49): loss=2.8785836069749756e+16\n",
      "Gradient Descent(43/49): loss=8.782687140652693e+16\n",
      "Gradient Descent(44/49): loss=2.679637069553279e+17\n",
      "Gradient Descent(45/49): loss=8.175692370149078e+17\n",
      "Gradient Descent(46/49): loss=2.4944402542713144e+18\n",
      "Gradient Descent(47/49): loss=7.610648615948296e+18\n",
      "Gradient Descent(48/49): loss=2.322042881414115e+19\n",
      "Gradient Descent(49/49): loss=7.084656532200251e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4045400287990642\n",
      "Gradient Descent(2/49): loss=0.3849885552645888\n",
      "Gradient Descent(3/49): loss=0.3794025065666214\n",
      "Gradient Descent(4/49): loss=0.38093371327883097\n",
      "Gradient Descent(5/49): loss=0.3954204075620776\n",
      "Gradient Descent(6/49): loss=0.44508018292231666\n",
      "Gradient Descent(7/49): loss=0.5963508215783191\n",
      "Gradient Descent(8/49): loss=1.0445286883756977\n",
      "Gradient Descent(9/49): loss=2.3625619928036707\n",
      "Gradient Descent(10/49): loss=6.230762399327396\n",
      "Gradient Descent(11/49): loss=17.576690595002535\n",
      "Gradient Descent(12/49): loss=50.850288176051095\n",
      "Gradient Descent(13/49): loss=148.42542210074083\n",
      "Gradient Descent(14/49): loss=434.5615746609026\n",
      "Gradient Descent(15/49): loss=1273.644027481446\n",
      "Gradient Descent(16/49): loss=3734.21594511192\n",
      "Gradient Descent(17/49): loss=10949.731177004456\n",
      "Gradient Descent(18/49): loss=32108.899433486553\n",
      "Gradient Descent(19/49): loss=94157.19047971543\n",
      "Gradient Descent(20/49): loss=276110.9584415461\n",
      "Gradient Descent(21/49): loss=809682.037356701\n",
      "Gradient Descent(22/49): loss=2374354.752015186\n",
      "Gradient Descent(23/49): loss=6962685.717172978\n",
      "Gradient Descent(24/49): loss=20417755.80775647\n",
      "Gradient Descent(25/49): loss=59874131.66948171\n",
      "Gradient Descent(26/49): loss=175578144.03451386\n",
      "Gradient Descent(27/49): loss=514874852.9938696\n",
      "Gradient Descent(28/49): loss=1509846888.599598\n",
      "Gradient Descent(29/49): loss=4427556744.004811\n",
      "Gradient Descent(30/49): loss=12983607060.680809\n",
      "Gradient Descent(31/49): loss=38073832151.388855\n",
      "Gradient Descent(32/49): loss=111649766350.30283\n",
      "Gradient Descent(33/49): loss=327407818487.02795\n",
      "Gradient Descent(34/49): loss=960108409633.8634\n",
      "Gradient Descent(35/49): loss=2815473871423.842\n",
      "Gradient Descent(36/49): loss=8256247983177.78\n",
      "Gradient Descent(37/49): loss=24211068499548.316\n",
      "Gradient Descent(38/49): loss=70997847821936.89\n",
      "Gradient Descent(39/49): loss=208197932092131.66\n",
      "Gradient Descent(40/49): loss=610530885896033.0\n",
      "Gradient Descent(41/49): loss=1790353818058327.5\n",
      "Gradient Descent(42/49): loss=5250130448571412.0\n",
      "Gradient Descent(43/49): loss=1.5395766718843418e+16\n",
      "Gradient Descent(44/49): loss=4.5147379704737544e+16\n",
      "Gradient Descent(45/49): loss=1.3239262009010656e+17\n",
      "Gradient Descent(46/49): loss=3.882352856124662e+17\n",
      "Gradient Descent(47/49): loss=1.1384821668458926e+18\n",
      "Gradient Descent(48/49): loss=3.3385467325087846e+18\n",
      "Gradient Descent(49/49): loss=9.790135155146619e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4029707013623058\n",
      "Gradient Descent(2/49): loss=0.38342684941593186\n",
      "Gradient Descent(3/49): loss=0.3778072359323373\n",
      "Gradient Descent(4/49): loss=0.378898822291146\n",
      "Gradient Descent(5/49): loss=0.3918511621355655\n",
      "Gradient Descent(6/49): loss=0.43686598295467366\n",
      "Gradient Descent(7/49): loss=0.5744648470454476\n",
      "Gradient Descent(8/49): loss=0.9826427803677764\n",
      "Gradient Descent(9/49): loss=2.1837976204936593\n",
      "Gradient Descent(10/49): loss=5.71057659226035\n",
      "Gradient Descent(11/49): loss=16.059219480188226\n",
      "Gradient Descent(12/49): loss=46.41983465399125\n",
      "Gradient Descent(13/49): loss=135.48655002019325\n",
      "Gradient Descent(14/49): loss=396.7712065335244\n",
      "Gradient Descent(15/49): loss=1163.2683408087216\n",
      "Gradient Descent(16/49): loss=3411.839476094898\n",
      "Gradient Descent(17/49): loss=10008.172492982578\n",
      "Gradient Descent(18/49): loss=29358.95223542002\n",
      "Gradient Descent(19/49): loss=86125.74438977113\n",
      "Gradient Descent(20/49): loss=252654.88202321666\n",
      "Gradient Descent(21/49): loss=741179.1152326756\n",
      "Gradient Descent(22/49): loss=2174297.2448388147\n",
      "Gradient Descent(23/49): loss=6378444.006293833\n",
      "Gradient Descent(24/49): loss=18711586.42178859\n",
      "Gradient Descent(25/49): loss=54891674.907846615\n",
      "Gradient Descent(26/49): loss=161028355.9540773\n",
      "Gradient Descent(27/49): loss=472387325.34492874\n",
      "Gradient Descent(28/49): loss=1385779442.6078014\n",
      "Gradient Descent(29/49): loss=4065275593.0161314\n",
      "Gradient Descent(30/49): loss=11925754662.583153\n",
      "Gradient Descent(31/49): loss=34984989583.927025\n",
      "Gradient Descent(32/49): loss=102630779420.88113\n",
      "Gradient Descent(33/49): loss=301074175234.9341\n",
      "Gradient Descent(34/49): loss=883220993789.7941\n",
      "Gradient Descent(35/49): loss=2590987165414.3545\n",
      "Gradient Descent(36/49): loss=7600832111720.553\n",
      "Gradient Descent(37/49): loss=22297543408064.793\n",
      "Gradient Descent(38/49): loss=65411317435608.76\n",
      "Gradient Descent(39/49): loss=191888423328036.0\n",
      "Gradient Descent(40/49): loss=562917373489195.75\n",
      "Gradient Descent(41/49): loss=1651355323474965.8\n",
      "Gradient Descent(42/49): loss=4844359994551643.0\n",
      "Gradient Descent(43/49): loss=1.4211250251961948e+16\n",
      "Gradient Descent(44/49): loss=4.168964196530002e+16\n",
      "Gradient Descent(45/49): loss=1.2229932035395206e+17\n",
      "Gradient Descent(46/49): loss=3.587731401360808e+17\n",
      "Gradient Descent(47/49): loss=1.0524847211788925e+18\n",
      "Gradient Descent(48/49): loss=3.087533497894758e+18\n",
      "Gradient Descent(49/49): loss=9.057483599328782e+18\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4049746460673072\n",
      "Gradient Descent(2/49): loss=0.38571012680023\n",
      "Gradient Descent(3/49): loss=0.3795920744563358\n",
      "Gradient Descent(4/49): loss=0.37897570030216776\n",
      "Gradient Descent(5/49): loss=0.3870303039002831\n",
      "Gradient Descent(6/49): loss=0.4183015504603579\n",
      "Gradient Descent(7/49): loss=0.5176179551029944\n",
      "Gradient Descent(8/49): loss=0.8197175097868371\n",
      "Gradient Descent(9/49): loss=1.728441863211317\n",
      "Gradient Descent(10/49): loss=4.453633719702443\n",
      "Gradient Descent(11/49): loss=12.619414926813125\n",
      "Gradient Descent(12/49): loss=37.081688963307876\n",
      "Gradient Descent(13/49): loss=110.35864903262186\n",
      "Gradient Descent(14/49): loss=329.8563862004111\n",
      "Gradient Descent(15/49): loss=987.3482554327676\n",
      "Gradient Descent(16/49): loss=2956.8214932216215\n",
      "Gradient Descent(17/49): loss=8856.245325699527\n",
      "Gradient Descent(18/49): loss=26527.568148387523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=79460.81187254605\n",
      "Gradient Descent(20/49): loss=238018.73600415303\n",
      "Gradient Descent(21/49): loss=712968.1905298986\n",
      "Gradient Descent(22/49): loss=2135646.9081207435\n",
      "Gradient Descent(23/49): loss=6397184.033200511\n",
      "Gradient Descent(24/49): loss=19162328.01307738\n",
      "Gradient Descent(25/49): loss=57399446.6960613\n",
      "Gradient Descent(26/49): loss=171936129.35882628\n",
      "Gradient Descent(27/49): loss=515022955.1244354\n",
      "Gradient Descent(28/49): loss=1542716155.9169793\n",
      "Gradient Descent(29/49): loss=4621101088.355615\n",
      "Gradient Descent(30/49): loss=13842193321.870947\n",
      "Gradient Descent(31/49): loss=41463346571.096176\n",
      "Gradient Descent(32/49): loss=124200628391.60309\n",
      "Gradient Descent(33/49): loss=372034516472.92377\n",
      "Gradient Descent(34/49): loss=1114404035147.202\n",
      "Gradient Descent(35/49): loss=3338121326286.2915\n",
      "Gradient Descent(36/49): loss=9999114896902.83\n",
      "Gradient Descent(37/49): loss=29951667105130.883\n",
      "Gradient Descent(38/49): loss=89718177221314.64\n",
      "Gradient Descent(39/49): loss=268744684416460.84\n",
      "Gradient Descent(40/49): loss=805006383755915.2\n",
      "Gradient Descent(41/49): loss=2411341750981548.5\n",
      "Gradient Descent(42/49): loss=7223009851049576.0\n",
      "Gradient Descent(43/49): loss=2.1636033667613196e+16\n",
      "Gradient Descent(44/49): loss=6.480926407681471e+16\n",
      "Gradient Descent(45/49): loss=1.9413173295554275e+17\n",
      "Gradient Descent(46/49): loss=5.815083734889128e+17\n",
      "Gradient Descent(47/49): loss=1.741868695496379e+18\n",
      "Gradient Descent(48/49): loss=5.217648946560204e+18\n",
      "Gradient Descent(49/49): loss=1.56291117694047e+19\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40615546389698015\n",
      "Gradient Descent(2/49): loss=0.38628168327694146\n",
      "Gradient Descent(3/49): loss=0.38144324864731594\n",
      "Gradient Descent(4/49): loss=0.3871402063824988\n",
      "Gradient Descent(5/49): loss=0.4175833413171713\n",
      "Gradient Descent(6/49): loss=0.5255352716011223\n",
      "Gradient Descent(7/49): loss=0.8853398291026249\n",
      "Gradient Descent(8/49): loss=2.0682207564157675\n",
      "Gradient Descent(9/49): loss=5.9441085815402595\n",
      "Gradient Descent(10/49): loss=18.6335244651855\n",
      "Gradient Descent(11/49): loss=60.16921369851935\n",
      "Gradient Descent(12/49): loss=196.11890424921407\n",
      "Gradient Descent(13/49): loss=641.0873008041386\n",
      "Gradient Descent(14/49): loss=2097.4803941902537\n",
      "Gradient Descent(15/49): loss=6864.288503119224\n",
      "Gradient Descent(16/49): loss=22466.1575667111\n",
      "Gradient Descent(17/49): loss=73531.41932925185\n",
      "Gradient Descent(18/49): loss=240669.14546169084\n",
      "Gradient Descent(19/49): loss=787714.6010255631\n",
      "Gradient Descent(20/49): loss=2578206.413151847\n",
      "Gradient Descent(21/49): loss=8438525.506921882\n",
      "Gradient Descent(22/49): loss=27619478.832534753\n",
      "Gradient Descent(23/49): loss=90399161.06234138\n",
      "Gradient Descent(24/49): loss=295878442.19859886\n",
      "Gradient Descent(25/49): loss=968416650.048353\n",
      "Gradient Descent(26/49): loss=3169649000.660309\n",
      "Gradient Descent(27/49): loss=10374330912.892529\n",
      "Gradient Descent(28/49): loss=33955413319.75906\n",
      "Gradient Descent(29/49): loss=111136814838.03227\n",
      "Gradient Descent(30/49): loss=363753240053.1277\n",
      "Gradient Descent(31/49): loss=1190572357523.3257\n",
      "Gradient Descent(32/49): loss=3896769519612.4604\n",
      "Gradient Descent(33/49): loss=12754212369394.42\n",
      "Gradient Descent(34/49): loss=41744817686779.086\n",
      "Gradient Descent(35/49): loss=136631706704532.17\n",
      "Gradient Descent(36/49): loss=447198582038741.1\n",
      "Gradient Descent(37/49): loss=1463690797699995.2\n",
      "Gradient Descent(38/49): loss=4790692183111754.0\n",
      "Gradient Descent(39/49): loss=1.5680040913963062e+16\n",
      "Gradient Descent(40/49): loss=5.132111888346419e+16\n",
      "Gradient Descent(41/49): loss=1.6797515120672534e+17\n",
      "Gradient Descent(42/49): loss=5.497863654725351e+17\n",
      "Gradient Descent(43/49): loss=1.7994628698830052e+18\n",
      "Gradient Descent(44/49): loss=5.889681562591411e+18\n",
      "Gradient Descent(45/49): loss=1.9277057331548905e+19\n",
      "Gradient Descent(46/49): loss=6.309423275514399e+19\n",
      "Gradient Descent(47/49): loss=2.065088119256202e+20\n",
      "Gradient Descent(48/49): loss=6.759078847734102e+20\n",
      "Gradient Descent(49/49): loss=2.2122613773180178e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4062363281796438\n",
      "Gradient Descent(2/49): loss=0.3856279056785712\n",
      "Gradient Descent(3/49): loss=0.3800408160802476\n",
      "Gradient Descent(4/49): loss=0.3836647835375148\n",
      "Gradient Descent(5/49): loss=0.40657580918668984\n",
      "Gradient Descent(6/49): loss=0.4867236582237948\n",
      "Gradient Descent(7/49): loss=0.7452170540761806\n",
      "Gradient Descent(8/49): loss=1.5638264082241726\n",
      "Gradient Descent(9/49): loss=4.144429006256021\n",
      "Gradient Descent(10/49): loss=12.269988597447796\n",
      "Gradient Descent(11/49): loss=37.847092282130305\n",
      "Gradient Descent(12/49): loss=118.35047343233482\n",
      "Gradient Descent(13/49): loss=371.72766853424196\n",
      "Gradient Descent(14/49): loss=1169.205171338968\n",
      "Gradient Descent(15/49): loss=3679.1761075781947\n",
      "Gradient Descent(16/49): loss=11579.024692591485\n",
      "Gradient Descent(17/49): loss=36442.89857174223\n",
      "Gradient Descent(18/49): loss=114699.1080761211\n",
      "Gradient Descent(19/49): loss=361001.6072964538\n",
      "Gradient Descent(20/49): loss=1136210.6466004949\n",
      "Gradient Descent(21/49): loss=3576092.7272206107\n",
      "Gradient Descent(22/49): loss=11255343.438357538\n",
      "Gradient Descent(23/49): loss=35424909.64413413\n",
      "Gradient Descent(24/49): loss=111495864.03046608\n",
      "Gradient Descent(25/49): loss=350920521.1368244\n",
      "Gradient Descent(26/49): loss=1104482335.7989573\n",
      "Gradient Descent(27/49): loss=3476232244.015123\n",
      "Gradient Descent(28/49): loss=10941044708.86248\n",
      "Gradient Descent(29/49): loss=34435690979.241516\n",
      "Gradient Descent(30/49): loss=108382411807.07458\n",
      "Gradient Descent(31/49): loss=341121285943.00085\n",
      "Gradient Descent(32/49): loss=1073640360861.4589\n",
      "Gradient Descent(33/49): loss=3379160644533.48\n",
      "Gradient Descent(34/49): loss=10635522916075.523\n",
      "Gradient Descent(35/49): loss=33474095965625.426\n",
      "Gradient Descent(36/49): loss=105355901120969.97\n",
      "Gradient Descent(37/49): loss=331595688570964.4\n",
      "Gradient Descent(38/49): loss=1043659629018778.5\n",
      "Gradient Descent(39/49): loss=3284799708758813.0\n",
      "Gradient Descent(40/49): loss=1.0338532627544756e+16\n",
      "Gradient Descent(41/49): loss=3.2539352888336684e+16\n",
      "Gradient Descent(42/49): loss=1.024139038426734e+17\n",
      "Gradient Descent(43/49): loss=3.2233608751500614e+17\n",
      "Gradient Descent(44/49): loss=1.0145160902575348e+18\n",
      "Gradient Descent(45/49): loss=3.1930737427702067e+18\n",
      "Gradient Descent(46/49): loss=1.0049835606037715e+19\n",
      "Gradient Descent(47/49): loss=3.1630711923603337e+19\n",
      "Gradient Descent(48/49): loss=9.955405998809265e+19\n",
      "Gradient Descent(49/49): loss=3.1333505499499817e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4047136433691692\n",
      "Gradient Descent(2/49): loss=0.384071009968452\n",
      "Gradient Descent(3/49): loss=0.3783883981199246\n",
      "Gradient Descent(4/49): loss=0.3813646227263004\n",
      "Gradient Descent(5/49): loss=0.40197676760217044\n",
      "Gradient Descent(6/49): loss=0.47474177247537763\n",
      "Gradient Descent(7/49): loss=0.7099776482470155\n",
      "Gradient Descent(8/49): loss=1.4556006106651973\n",
      "Gradient Descent(9/49): loss=3.807331149146773\n",
      "Gradient Descent(10/49): loss=11.215297885530527\n",
      "Gradient Descent(11/49): loss=34.54258559997507\n",
      "Gradient Descent(12/49): loss=107.99241720951053\n",
      "Gradient Descent(13/49): loss=339.25591738215417\n",
      "Gradient Descent(14/49): loss=1067.4055701352488\n",
      "Gradient Descent(15/49): loss=3360.032907619158\n",
      "Gradient Descent(16/49): loss=10578.518433359093\n",
      "Gradient Descent(17/49): loss=33306.38444045765\n",
      "Gradient Descent(18/49): loss=104866.53017520314\n",
      "Gradient Descent(19/49): loss=330178.24268578005\n",
      "Gradient Descent(20/49): loss=1039586.6312452751\n",
      "Gradient Descent(21/49): loss=3273204.4747904893\n",
      "Gradient Descent(22/49): loss=10305893.614852138\n",
      "Gradient Descent(23/49): loss=32448766.748310838\n",
      "Gradient Descent(24/49): loss=102167023.99425755\n",
      "Gradient Descent(25/49): loss=321679431.62523717\n",
      "Gradient Descent(26/49): loss=1012828335.9059663\n",
      "Gradient Descent(27/49): loss=3188955020.696115\n",
      "Gradient Descent(28/49): loss=10040629557.001015\n",
      "Gradient Descent(29/49): loss=31613566592.13306\n",
      "Gradient Descent(30/49): loss=99537343452.20018\n",
      "Gradient Descent(31/49): loss=313399714413.68335\n",
      "Gradient Descent(32/49): loss=986759115606.7571\n",
      "Gradient Descent(33/49): loss=3106874408150.5396\n",
      "Gradient Descent(34/49): loss=9782193481016.852\n",
      "Gradient Descent(35/49): loss=30799864020578.07\n",
      "Gradient Descent(36/49): loss=96975348680952.1\n",
      "Gradient Descent(37/49): loss=305333109441949.2\n",
      "Gradient Descent(38/49): loss=961360892119177.2\n",
      "Gradient Descent(39/49): loss=3026906471379132.5\n",
      "Gradient Descent(40/49): loss=9530409299550510.0\n",
      "Gradient Descent(41/49): loss=3.0007105365096068e+16\n",
      "Gradient Descent(42/49): loss=9.44792971729364e+16\n",
      "Gradient Descent(43/49): loss=2.974741310661339e+17\n",
      "Gradient Descent(44/49): loss=9.366163942940266e+17\n",
      "Gradient Descent(45/49): loss=2.948996831813027e+18\n",
      "Gradient Descent(46/49): loss=9.285105798941968e+18\n",
      "Gradient Descent(47/49): loss=2.9234751549237957e+19\n",
      "Gradient Descent(48/49): loss=9.204749161211298e+19\n",
      "Gradient Descent(49/49): loss=2.8981743517853647e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4066128243202083\n",
      "Gradient Descent(2/49): loss=0.38625988010435897\n",
      "Gradient Descent(3/49): loss=0.37993010611175776\n",
      "Gradient Descent(4/49): loss=0.3805390470639563\n",
      "Gradient Descent(5/49): loss=0.3938741225472278\n",
      "Gradient Descent(6/49): loss=0.4447243724228411\n",
      "Gradient Descent(7/49): loss=0.6143945962116253\n",
      "Gradient Descent(8/49): loss=1.1647838778461892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=2.9379533807012854\n",
      "Gradient Descent(10/49): loss=8.640565800044802\n",
      "Gradient Descent(11/49): loss=26.972266500734694\n",
      "Gradient Descent(12/49): loss=85.89477025433825\n",
      "Gradient Descent(13/49): loss=275.2801635322144\n",
      "Gradient Descent(14/49): loss=883.9872085841441\n",
      "Gradient Descent(15/49): loss=2840.4395502324924\n",
      "Gradient Descent(16/49): loss=9128.69216402595\n",
      "Gradient Descent(17/49): loss=29339.823485432782\n",
      "Gradient Descent(18/49): loss=94300.60649639984\n",
      "Gradient Descent(19/49): loss=303091.6543096612\n",
      "Gradient Descent(20/49): loss=974168.8720807185\n",
      "Gradient Descent(21/49): loss=3131084.2982716565\n",
      "Gradient Descent(22/49): loss=10063645.904619412\n",
      "Gradient Descent(23/49): loss=32345655.592910726\n",
      "Gradient Descent(24/49): loss=103962466.7997857\n",
      "Gradient Descent(25/49): loss=334146714.95203704\n",
      "Gradient Descent(26/49): loss=1073984012.9895985\n",
      "Gradient Descent(27/49): loss=3451901841.690489\n",
      "Gradient Descent(28/49): loss=11094789291.473825\n",
      "Gradient Descent(29/49): loss=35659863771.76607\n",
      "Gradient Descent(30/49): loss=114614694414.9179\n",
      "Gradient Descent(31/49): loss=368384137975.5428\n",
      "Gradient Descent(32/49): loss=1184026828365.557\n",
      "Gradient Descent(33/49): loss=3805591462201.008\n",
      "Gradient Descent(34/49): loss=12231586337592.621\n",
      "Gradient Descent(35/49): loss=39313653559508.36\n",
      "Gradient Descent(36/49): loss=126358373602519.11\n",
      "Gradient Descent(37/49): loss=406129604700969.0\n",
      "Gradient Descent(38/49): loss=1305344878317437.2\n",
      "Gradient Descent(39/49): loss=4195520916541201.5\n",
      "Gradient Descent(40/49): loss=1.3484862164414278e+16\n",
      "Gradient Descent(41/49): loss=4.3341818861235384e+16\n",
      "Gradient Descent(42/49): loss=1.3930533655415622e+17\n",
      "Gradient Descent(43/49): loss=4.4774255678096525e+17\n",
      "Gradient Descent(44/49): loss=1.4390934483319224e+18\n",
      "Gradient Descent(45/49): loss=4.625403419145891e+18\n",
      "Gradient Descent(46/49): loss=1.486655144920982e+19\n",
      "Gradient Descent(47/49): loss=4.778271903314373e+19\n",
      "Gradient Descent(48/49): loss=1.535788744283222e+20\n",
      "Gradient Descent(49/49): loss=4.9361926545683566e+20\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40798950729427036\n",
      "Gradient Descent(2/49): loss=0.3871197353455133\n",
      "Gradient Descent(3/49): loss=0.38245182137326283\n",
      "Gradient Descent(4/49): loss=0.3915288088208672\n",
      "Gradient Descent(5/49): loss=0.43692186112929793\n",
      "Gradient Descent(6/49): loss=0.6052195352414949\n",
      "Gradient Descent(7/49): loss=1.201852030485058\n",
      "Gradient Descent(8/49): loss=3.297437418551486\n",
      "Gradient Descent(9/49): loss=10.642496217974779\n",
      "Gradient Descent(10/49): loss=36.37455457580836\n",
      "Gradient Descent(11/49): loss=126.51181386220769\n",
      "Gradient Descent(12/49): loss=442.24664379352004\n",
      "Gradient Descent(13/49): loss=1548.2026856704706\n",
      "Gradient Descent(14/49): loss=5422.139891609716\n",
      "Gradient Descent(15/49): loss=18991.74251425144\n",
      "Gradient Descent(16/49): loss=66523.2561694676\n",
      "Gradient Descent(17/49): loss=233016.32198710466\n",
      "Gradient Descent(18/49): loss=816207.1082997886\n",
      "Gradient Descent(19/49): loss=2859003.8526871395\n",
      "Gradient Descent(20/49): loss=10014498.47905112\n",
      "Gradient Descent(21/49): loss=35078716.68125627\n",
      "Gradient Descent(22/49): loss=122873490.75061741\n",
      "Gradient Descent(23/49): loss=430400431.8134186\n",
      "Gradient Descent(24/49): loss=1507603721.8932214\n",
      "Gradient Descent(25/49): loss=5280824123.813965\n",
      "Gradient Descent(26/49): loss=18497635038.32153\n",
      "Gradient Descent(27/49): loss=64793390955.712875\n",
      "Gradient Descent(28/49): loss=226957851794.75723\n",
      "Gradient Descent(29/49): loss=794986428888.1194\n",
      "Gradient Descent(30/49): loss=2784673088500.6646\n",
      "Gradient Descent(31/49): loss=9754134068259.791\n",
      "Gradient Descent(32/49): loss=34166714870223.844\n",
      "Gradient Descent(33/49): loss=119678937858961.11\n",
      "Gradient Descent(34/49): loss=419210574427543.94\n",
      "Gradient Descent(35/49): loss=1468407965977930.5\n",
      "Gradient Descent(36/49): loss=5143529495867005.0\n",
      "Gradient Descent(37/49): loss=1.8016720344632244e+16\n",
      "Gradient Descent(38/49): loss=6.310884621882621e+16\n",
      "Gradient Descent(39/49): loss=2.2105723987985354e+17\n",
      "Gradient Descent(40/49): loss=7.743178053653231e+17\n",
      "Gradient Descent(41/49): loss=2.7122751737587174e+18\n",
      "Gradient Descent(42/49): loss=9.500539141957476e+18\n",
      "Gradient Descent(43/49): loss=3.3278424276833526e+19\n",
      "Gradient Descent(44/49): loss=1.165674395738354e+20\n",
      "Gradient Descent(45/49): loss=4.0831163927009087e+20\n",
      "Gradient Descent(46/49): loss=1.430231249592062e+21\n",
      "Gradient Descent(47/49): loss=5.009804351809248e+21\n",
      "Gradient Descent(48/49): loss=1.7548308814091797e+22\n",
      "Gradient Descent(49/49): loss=6.1468097476408465e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40810444285713904\n",
      "Gradient Descent(2/49): loss=0.38648895394516175\n",
      "Gradient Descent(3/49): loss=0.38093058294660914\n",
      "Gradient Descent(4/49): loss=0.38727526509302834\n",
      "Gradient Descent(5/49): loss=0.4219680751114788\n",
      "Gradient Descent(6/49): loss=0.5479023845931855\n",
      "Gradient Descent(7/49): loss=0.9791680405968477\n",
      "Gradient Descent(8/49): loss=2.437995218584324\n",
      "Gradient Descent(9/49): loss=7.3586049855817794\n",
      "Gradient Descent(10/49): loss=23.944347047674334\n",
      "Gradient Descent(11/49): loss=79.83999938644712\n",
      "Gradient Descent(12/49): loss=268.2063296120177\n",
      "Gradient Descent(13/49): loss=902.9876204140176\n",
      "Gradient Descent(14/49): loss=3042.1505719376974\n",
      "Gradient Descent(15/49): loss=10250.956726806504\n",
      "Gradient Descent(16/49): loss=34544.04671617332\n",
      "Gradient Descent(17/49): loss=116409.77947785497\n",
      "Gradient Descent(18/49): loss=392290.6220459506\n",
      "Gradient Descent(19/49): loss=1321986.5587949825\n",
      "Gradient Descent(20/49): loss=4454986.031388986\n",
      "Gradient Descent(21/49): loss=15012938.697103966\n",
      "Gradient Descent(22/49): loss=50592377.973107345\n",
      "Gradient Descent(23/49): loss=170492186.1330968\n",
      "Gradient Descent(24/49): loss=574544759.4534231\n",
      "Gradient Descent(25/49): loss=1936168973.1371124\n",
      "Gradient Descent(26/49): loss=6524731506.110837\n",
      "Gradient Descent(27/49): loss=21987812955.029015\n",
      "Gradient Descent(28/49): loss=74097136120.53665\n",
      "Gradient Descent(29/49): loss=249701304651.72614\n",
      "Gradient Descent(30/49): loss=841473028640.3103\n",
      "Gradient Descent(31/49): loss=2835695467899.889\n",
      "Gradient Descent(32/49): loss=9556062420280.93\n",
      "Gradient Descent(33/49): loss=32203150872170.004\n",
      "Gradient Descent(34/49): loss=108521991641123.31\n",
      "Gradient Descent(35/49): loss=365710259735281.44\n",
      "Gradient Descent(36/49): loss=1232413744468816.0\n",
      "Gradient Descent(37/49): loss=4153133791365457.5\n",
      "Gradient Descent(38/49): loss=1.3995722107445454e+16\n",
      "Gradient Descent(39/49): loss=4.716444187665802e+16\n",
      "Gradient Descent(40/49): loss=1.589403219397431e+17\n",
      "Gradient Descent(41/49): loss=5.35615920238667e+17\n",
      "Gradient Descent(42/49): loss=1.8049819612286725e+18\n",
      "Gradient Descent(43/49): loss=6.08264197768646e+18\n",
      "Gradient Descent(44/49): loss=2.049800730613946e+19\n",
      "Gradient Descent(45/49): loss=6.907661260746312e+19\n",
      "Gradient Descent(46/49): loss=2.327825499355932e+20\n",
      "Gradient Descent(47/49): loss=7.844582053038162e+20\n",
      "Gradient Descent(48/49): loss=2.6435601639329675e+21\n",
      "Gradient Descent(49/49): loss=8.908582118312891e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4066320588577898\n",
      "Gradient Descent(2/49): loss=0.3849377676307354\n",
      "Gradient Descent(3/49): loss=0.3792083761732804\n",
      "Gradient Descent(4/49): loss=0.3846340118441825\n",
      "Gradient Descent(5/49): loss=0.41595817910949856\n",
      "Gradient Descent(6/49): loss=0.5303964558234947\n",
      "Gradient Descent(7/49): loss=0.9229517220331949\n",
      "Gradient Descent(8/49): loss=2.2517523287548142\n",
      "Gradient Descent(9/49): loss=6.735805476528001\n",
      "Gradient Descent(10/49): loss=21.855962309823212\n",
      "Gradient Descent(11/49): loss=72.83155508151546\n",
      "Gradient Descent(12/49): loss=244.68122078952007\n",
      "Gradient Descent(13/49): loss=824.0168737312614\n",
      "Gradient Descent(14/49): loss=2777.05453805438\n",
      "Gradient Descent(15/49): loss=9361.067567670005\n",
      "Gradient Descent(16/49): loss=31556.86076183272\n",
      "Gradient Descent(17/49): loss=106382.53500901897\n",
      "Gradient Descent(18/49): loss=358632.2030662042\n",
      "Gradient Descent(19/49): loss=1209007.365063284\n",
      "Gradient Descent(20/49): loss=4075762.0419069515\n",
      "Gradient Descent(21/49): loss=13740064.038260331\n",
      "Gradient Descent(22/49): loss=46320017.1911681\n",
      "Gradient Descent(23/49): loss=156152403.20660675\n",
      "Gradient Descent(24/49): loss=526415459.1782119\n",
      "Gradient Descent(25/49): loss=1774633180.6620462\n",
      "Gradient Descent(26/49): loss=5982580625.317211\n",
      "Gradient Descent(27/49): loss=20168264254.235497\n",
      "Gradient Descent(28/49): loss=67990539286.53419\n",
      "Gradient Descent(29/49): loss=229207301840.20398\n",
      "Gradient Descent(30/49): loss=772695551003.0496\n",
      "Gradient Descent(31/49): loss=2604883918386.606\n",
      "Gradient Descent(32/49): loss=8781492554813.311\n",
      "Gradient Descent(33/49): loss=29603857179944.324\n",
      "Gradient Descent(34/49): loss=99799476508141.95\n",
      "Gradient Descent(35/49): loss=336440466212188.8\n",
      "Gradient Descent(36/49): loss=1134196202881292.8\n",
      "Gradient Descent(37/49): loss=3823562133037308.5\n",
      "Gradient Descent(38/49): loss=1.288985745857468e+16\n",
      "Gradient Descent(39/49): loss=4.34538316683235e+16\n",
      "Gradient Descent(40/49): loss=1.4649002075681392e+17\n",
      "Gradient Descent(41/49): loss=4.9384197796702656e+17\n",
      "Gradient Descent(42/49): loss=1.6648226134615288e+18\n",
      "Gradient Descent(43/49): loss=5.612391124996347e+18\n",
      "Gradient Descent(44/49): loss=1.892029450179428e+19\n",
      "Gradient Descent(45/49): loss=6.378342778718184e+19\n",
      "Gradient Descent(46/49): loss=2.150244363213736e+20\n",
      "Gradient Descent(47/49): loss=7.248827762846601e+20\n",
      "Gradient Descent(48/49): loss=2.4436991829561647e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=8.238112274356864e+21\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4084197957014531\n",
      "Gradient Descent(2/49): loss=0.38701693597288883\n",
      "Gradient Descent(3/49): loss=0.3804571883441038\n",
      "Gradient Descent(4/49): loss=0.3826533678980603\n",
      "Gradient Descent(5/49): loss=0.4033567940547298\n",
      "Gradient Descent(6/49): loss=0.4835433213681143\n",
      "Gradient Descent(7/49): loss=0.7662899153439922\n",
      "Gradient Descent(8/49): loss=1.744592833952678\n",
      "Gradient Descent(9/49): loss=5.11495269795723\n",
      "Gradient Descent(10/49): loss=16.71437880192482\n",
      "Gradient Descent(11/49): loss=56.625196439473136\n",
      "Gradient Descent(12/49): loss=193.94055983665555\n",
      "Gradient Descent(13/49): loss=666.3748585522811\n",
      "Gradient Descent(14/49): loss=2291.78211497212\n",
      "Gradient Descent(15/49): loss=7883.980864649393\n",
      "Gradient Descent(16/49): loss=27123.885311691236\n",
      "Gradient Descent(17/49): loss=93318.58270543294\n",
      "Gradient Descent(18/49): loss=321060.76921444165\n",
      "Gradient Descent(19/49): loss=1104605.4547120358\n",
      "Gradient Descent(20/49): loss=3800383.005174963\n",
      "Gradient Descent(21/49): loss=13075178.348236918\n",
      "Gradient Descent(22/49): loss=44985017.73264862\n",
      "Gradient Descent(23/49): loss=154770496.75001854\n",
      "Gradient Descent(24/49): loss=532486324.7062695\n",
      "Gradient Descent(25/49): loss=1832013802.8893466\n",
      "Gradient Descent(26/49): loss=6303024921.724649\n",
      "Gradient Descent(27/49): loss=21685493365.32829\n",
      "Gradient Descent(28/49): loss=74608720154.37285\n",
      "Gradient Descent(29/49): loss=256690545580.08258\n",
      "Gradient Descent(30/49): loss=883141220678.0204\n",
      "Gradient Descent(31/49): loss=3038438419688.0234\n",
      "Gradient Descent(32/49): loss=10453716590366.21\n",
      "Gradient Descent(33/49): loss=35965905987629.74\n",
      "Gradient Descent(34/49): loss=123740335059702.88\n",
      "Gradient Descent(35/49): loss=425727368746210.6\n",
      "Gradient Descent(36/49): loss=1464710697705246.5\n",
      "Gradient Descent(37/49): loss=5039322311577845.0\n",
      "Gradient Descent(38/49): loss=1.7337737342775764e+16\n",
      "Gradient Descent(39/49): loss=5.965030962128736e+16\n",
      "Gradient Descent(40/49): loss=2.0522628573549466e+17\n",
      "Gradient Descent(41/49): loss=7.060789562399072e+17\n",
      "Gradient Descent(42/49): loss=2.42925749329889e+18\n",
      "Gradient Descent(43/49): loss=8.357835786772188e+18\n",
      "Gradient Descent(44/49): loss=2.8755049323236274e+19\n",
      "Gradient Descent(45/49): loss=9.89314557831332e+19\n",
      "Gradient Descent(46/49): loss=3.403726710168035e+20\n",
      "Gradient Descent(47/49): loss=1.1710487251807595e+21\n",
      "Gradient Descent(48/49): loss=4.0289812711779085e+21\n",
      "Gradient Descent(49/49): loss=1.386166923241932e+22\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.409993962758209\n",
      "Gradient Descent(2/49): loss=0.38820083418451834\n",
      "Gradient Descent(3/49): loss=0.38377914264250795\n",
      "Gradient Descent(4/49): loss=0.3971983108848425\n",
      "Gradient Descent(5/49): loss=0.46316351555472424\n",
      "Gradient Descent(6/49): loss=0.7203460919791009\n",
      "Gradient Descent(7/49): loss=1.6901359059636956\n",
      "Gradient Descent(8/49): loss=5.323701909710058\n",
      "Gradient Descent(9/49): loss=18.919571426021207\n",
      "Gradient Descent(10/49): loss=69.77712129990405\n",
      "Gradient Descent(11/49): loss=260.0059905765907\n",
      "Gradient Descent(12/49): loss=971.5329627249688\n",
      "Gradient Descent(13/49): loss=3632.9011901526624\n",
      "Gradient Descent(14/49): loss=13587.374104163735\n",
      "Gradient Descent(15/49): loss=50820.671901272\n",
      "Gradient Descent(16/49): loss=190086.5505622085\n",
      "Gradient Descent(17/49): loss=710990.8474047987\n",
      "Gradient Descent(18/49): loss=2659359.5978782265\n",
      "Gradient Descent(19/49): loss=9946956.972878486\n",
      "Gradient Descent(20/49): loss=37205182.851489335\n",
      "Gradient Descent(21/49): loss=139160716.3147564\n",
      "Gradient Descent(22/49): loss=520510949.8193319\n",
      "Gradient Descent(23/49): loss=1946897489.6556535\n",
      "Gradient Descent(24/49): loss=7282094330.439208\n",
      "Gradient Descent(25/49): loss=27237642518.078503\n",
      "Gradient Descent(26/49): loss=101878544316.69016\n",
      "Gradient Descent(27/49): loss=381062266503.8364\n",
      "Gradient Descent(28/49): loss=1425309440051.7258\n",
      "Gradient Descent(29/49): loss=5331168101581.724\n",
      "Gradient Descent(30/49): loss=19940479259222.793\n",
      "Gradient Descent(31/49): loss=74584538605998.95\n",
      "Gradient Descent(32/49): loss=278972903647589.7\n",
      "Gradient Descent(33/49): loss=1043458636657804.6\n",
      "Gradient Descent(34/49): loss=3902909250968703.0\n",
      "Gradient Descent(35/49): loss=1.4598279305145954e+16\n",
      "Gradient Descent(36/49): loss=5.4602796264903656e+16\n",
      "Gradient Descent(37/49): loss=2.04234026327719e+17\n",
      "Gradient Descent(38/49): loss=7.639084509091519e+17\n",
      "Gradient Descent(39/49): loss=2.857291372369314e+18\n",
      "Gradient Descent(40/49): loss=1.0687293715496356e+19\n",
      "Gradient Descent(41/49): loss=3.997430855872983e+19\n",
      "Gradient Descent(42/49): loss=1.4951823981702444e+20\n",
      "Gradient Descent(43/49): loss=5.592518005692559e+20\n",
      "Gradient Descent(44/49): loss=2.0918021561964954e+21\n",
      "Gradient Descent(45/49): loss=7.824089714533685e+21\n",
      "Gradient Descent(46/49): loss=2.926489949335347e+22\n",
      "Gradient Descent(47/49): loss=1.0946121192414847e+23\n",
      "Gradient Descent(48/49): loss=4.0942416079796e+23\n",
      "Gradient Descent(49/49): loss=1.5313930889169685e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41014437283155\n",
      "Gradient Descent(2/49): loss=0.3876007665259773\n",
      "Gradient Descent(3/49): loss=0.3821243319816275\n",
      "Gradient Descent(4/49): loss=0.39197423237073853\n",
      "Gradient Descent(5/49): loss=0.44292937088595696\n",
      "Gradient Descent(6/49): loss=0.6365526935107109\n",
      "Gradient Descent(7/49): loss=1.3411971688912268\n",
      "Gradient Descent(8/49): loss=3.8839457689758303\n",
      "Gradient Descent(9/49): loss=13.042821709827487\n",
      "Gradient Descent(10/49): loss=46.019219330168276\n",
      "Gradient Descent(11/49): loss=164.73922407230376\n",
      "Gradient Descent(12/49): loss=592.1400627513688\n",
      "Gradient Descent(13/49): loss=2130.8073188412804\n",
      "Gradient Descent(14/49): loss=7670.090438433189\n",
      "Gradient Descent(15/49): loss=27611.79603942234\n",
      "Gradient Descent(16/49): loss=99402.96276937514\n",
      "Gradient Descent(17/49): loss=357854.8549991222\n",
      "Gradient Descent(18/49): loss=1288294.9553027723\n",
      "Gradient Descent(19/49): loss=4637927.152224577\n",
      "Gradient Descent(20/49): loss=16696775.270324204\n",
      "Gradient Descent(21/49): loss=60109248.455451965\n",
      "Gradient Descent(22/49): loss=216396383.8080009\n",
      "Gradient Descent(23/49): loss=779038105.9804109\n",
      "Gradient Descent(24/49): loss=2804577231.864424\n",
      "Gradient Descent(25/49): loss=10096622220.36111\n",
      "Gradient Descent(26/49): loss=36348359071.43366\n",
      "Gradient Descent(27/49): loss=130855961367.50705\n",
      "Gradient Descent(28/49): loss=471088188378.7285\n",
      "Gradient Descent(29/49): loss=1695941697352.287\n",
      "Gradient Descent(30/49): loss=6105477300795.417\n",
      "Gradient Descent(31/49): loss=21980032172527.01\n",
      "Gradient Descent(32/49): loss=79129245840023.98\n",
      "Gradient Descent(33/49): loss=284869353150323.56\n",
      "Gradient Descent(34/49): loss=1025544316804761.6\n",
      "Gradient Descent(35/49): loss=3692012264919088.5\n",
      "Gradient Descent(36/49): loss=1.3291433964338072e+16\n",
      "Gradient Descent(37/49): loss=4.784984559964241e+16\n",
      "Gradient Descent(38/49): loss=1.7226190417473366e+17\n",
      "Gradient Descent(39/49): loss=6.201517112131688e+17\n",
      "Gradient Descent(40/49): loss=2.232578043085655e+18\n",
      "Gradient Descent(41/49): loss=8.037395734532998e+18\n",
      "Gradient Descent(42/49): loss=2.8935037856148996e+19\n",
      "Gradient Descent(43/49): loss=1.0416762386596987e+20\n",
      "Gradient Descent(44/49): loss=3.750088012957634e+20\n",
      "Gradient Descent(45/49): loss=1.3500509643018202e+21\n",
      "Gradient Descent(46/49): loss=4.860252879171344e+21\n",
      "Gradient Descent(47/49): loss=1.7497160236249926e+22\n",
      "Gradient Descent(48/49): loss=6.299067639978576e+22\n",
      "Gradient Descent(49/49): loss=2.2676967346290744e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40872594782816773\n",
      "Gradient Descent(2/49): loss=0.3860565585649701\n",
      "Gradient Descent(3/49): loss=0.38031809162998303\n",
      "Gradient Descent(4/49): loss=0.3888989391463983\n",
      "Gradient Descent(5/49): loss=0.4350079982659533\n",
      "Gradient Descent(6/49): loss=0.6110522537277416\n",
      "Gradient Descent(7/49): loss=1.2525249348492202\n",
      "Gradient Descent(8/49): loss=3.5686295195702797\n",
      "Gradient Descent(9/49): loss=11.914590661051582\n",
      "Gradient Descent(10/49): loss=41.97542598556894\n",
      "Gradient Descent(11/49): loss=150.23882129232254\n",
      "Gradient Descent(12/49): loss=540.1378208468344\n",
      "Gradient Descent(13/49): loss=1944.3096743126634\n",
      "Gradient Descent(14/49): loss=7001.25042063205\n",
      "Gradient Descent(15/49): loss=25213.15392748517\n",
      "Gradient Descent(16/49): loss=90800.9121286574\n",
      "Gradient Descent(17/49): loss=327006.5320430985\n",
      "Gradient Descent(18/49): loss=1177669.76861708\n",
      "Gradient Descent(19/49): loss=4241220.676858964\n",
      "Gradient Descent(20/49): loss=15274193.258613387\n",
      "Gradient Descent(21/49): loss=55007981.80320512\n",
      "Gradient Descent(22/49): loss=198103955.2943786\n",
      "Gradient Descent(23/49): loss=713445139.192487\n",
      "Gradient Descent(24/49): loss=2569378114.3973885\n",
      "Gradient Descent(25/49): loss=9253274756.225662\n",
      "Gradient Descent(26/49): loss=33324442689.27662\n",
      "Gradient Descent(27/49): loss=120013563827.84505\n",
      "Gradient Descent(28/49): loss=432212944625.606\n",
      "Gradient Descent(29/49): loss=1556557638519.8513\n",
      "Gradient Descent(30/49): loss=5605736043223.278\n",
      "Gradient Descent(31/49): loss=20188315426712.3\n",
      "Gradient Descent(32/49): loss=72705542434723.92\n",
      "Gradient Descent(33/49): loss=261839375351407.6\n",
      "Gradient Descent(34/49): loss=942979808533434.4\n",
      "Gradient Descent(35/49): loss=3396016806518741.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=1.2230304451687312e+16\n",
      "Gradient Descent(37/49): loss=4.404582059012158e+16\n",
      "Gradient Descent(38/49): loss=1.5862518542533542e+17\n",
      "Gradient Descent(39/49): loss=5.712675825788757e+17\n",
      "Gradient Descent(40/49): loss=2.0573444880802609e+18\n",
      "Gradient Descent(41/49): loss=7.409253512210081e+18\n",
      "Gradient Descent(42/49): loss=2.668344456956746e+19\n",
      "Gradient Descent(43/49): loss=9.60968892377404e+19\n",
      "Gradient Descent(44/49): loss=3.460802107874386e+20\n",
      "Gradient Descent(45/49): loss=1.2463620128469106e+21\n",
      "Gradient Descent(46/49): loss=4.488607607852877e+21\n",
      "Gradient Descent(47/49): loss=1.6165125420706003e+22\n",
      "Gradient Descent(48/49): loss=5.821655682487895e+22\n",
      "Gradient Descent(49/49): loss=2.0965921391511126e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41039556021104173\n",
      "Gradient Descent(2/49): loss=0.3880089827478831\n",
      "Gradient Descent(3/49): loss=0.38121603432855056\n",
      "Gradient Descent(4/49): loss=0.38545212669985196\n",
      "Gradient Descent(5/49): loss=0.4163053833379592\n",
      "Gradient Descent(6/49): loss=0.5397719782837468\n",
      "Gradient Descent(7/49): loss=1.0010292175137587\n",
      "Gradient Descent(8/49): loss=2.7019628143881302\n",
      "Gradient Descent(9/49): loss=8.957058945109033\n",
      "Gradient Descent(10/49): loss=31.945898395780215\n",
      "Gradient Descent(11/49): loss=116.42342775680389\n",
      "Gradient Descent(12/49): loss=426.845141297677\n",
      "Gradient Descent(13/49): loss=1567.5150229862788\n",
      "Gradient Descent(14/49): loss=5758.992853796312\n",
      "Gradient Descent(15/49): loss=21160.88990486067\n",
      "Gradient Descent(16/49): loss=77756.30154113524\n",
      "Gradient Descent(17/49): loss=285720.3300493505\n",
      "Gradient Descent(18/49): loss=1049899.540215989\n",
      "Gradient Descent(19/49): loss=3857932.587294395\n",
      "Gradient Descent(20/49): loss=14176257.774244463\n",
      "Gradient Descent(21/49): loss=52091707.084388904\n",
      "Gradient Descent(22/49): loss=191414830.78175342\n",
      "Gradient Descent(23/49): loss=703367956.7477211\n",
      "Gradient Descent(24/49): loss=2584577595.396413\n",
      "Gradient Descent(25/49): loss=9497221595.502283\n",
      "Gradient Descent(26/49): loss=34898243411.720764\n",
      "Gradient Descent(27/49): loss=128236177391.56018\n",
      "Gradient Descent(28/49): loss=471213321486.6857\n",
      "Gradient Descent(29/49): loss=1731508212919.1697\n",
      "Gradient Descent(30/49): loss=6362555035474.42\n",
      "Gradient Descent(31/49): loss=23379679216880.367\n",
      "Gradient Descent(32/49): loss=85910361047823.52\n",
      "Gradient Descent(33/49): loss=315683977821153.3\n",
      "Gradient Descent(34/49): loss=1160004132650705.8\n",
      "Gradient Descent(35/49): loss=4262521009314704.0\n",
      "Gradient Descent(36/49): loss=1.5662948814959408e+16\n",
      "Gradient Descent(37/49): loss=5.755466425712128e+16\n",
      "Gradient Descent(38/49): loss=2.114888720434432e+17\n",
      "Gradient Descent(39/49): loss=7.771315075072152e+17\n",
      "Gradient Descent(40/49): loss=2.855627221068963e+18\n",
      "Gradient Descent(41/49): loss=1.0493213499820323e+19\n",
      "Gradient Descent(42/49): loss=3.855808935439183e+19\n",
      "Gradient Descent(43/49): loss=1.416845520856621e+20\n",
      "Gradient Descent(44/49): loss=5.2063036929053406e+20\n",
      "Gradient Descent(45/49): loss=1.913094811237538e+21\n",
      "Gradient Descent(46/49): loss=7.029808425834431e+21\n",
      "Gradient Descent(47/49): loss=2.5831551167067178e+22\n",
      "Gradient Descent(48/49): loss=9.491994593261288e+22\n",
      "Gradient Descent(49/49): loss=3.487903640620935e+23\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41216883028879603\n",
      "Gradient Descent(2/49): loss=0.3895552821909968\n",
      "Gradient Descent(3/49): loss=0.3854879646660521\n",
      "Gradient Descent(4/49): loss=0.4044393889146922\n",
      "Gradient Descent(5/49): loss=0.49838260797261513\n",
      "Gradient Descent(6/49): loss=0.8846525549710256\n",
      "Gradient Descent(7/49): loss=2.432869998379662\n",
      "Gradient Descent(8/49): loss=8.610374843170478\n",
      "Gradient Descent(9/49): loss=33.237584718771835\n",
      "Gradient Descent(10/49): loss=131.39910419748065\n",
      "Gradient Descent(11/49): loss=522.6468160489384\n",
      "Gradient Descent(12/49): loss=2082.052638917484\n",
      "Gradient Descent(13/49): loss=8297.406073729313\n",
      "Gradient Descent(14/49): loss=33070.0505473933\n",
      "Gradient Descent(15/49): loss=131806.81263662493\n",
      "Gradient Descent(16/49): loss=525343.6456493789\n",
      "Gradient Descent(17/49): loss=2093870.2531986318\n",
      "Gradient Descent(18/49): loss=8345574.059962767\n",
      "Gradient Descent(19/49): loss=33263098.757455777\n",
      "Gradient Descent(20/49): loss=132577310.7264586\n",
      "Gradient Descent(21/49): loss=528415694.30215096\n",
      "Gradient Descent(22/49): loss=2106115630.7473571\n",
      "Gradient Descent(23/49): loss=8394381733.124027\n",
      "Gradient Descent(24/49): loss=33457633407.831207\n",
      "Gradient Descent(25/49): loss=133352671926.04561\n",
      "Gradient Descent(26/49): loss=531506065989.6758\n",
      "Gradient Descent(27/49): loss=2118432980037.4866\n",
      "Gradient Descent(28/49): loss=8443475207673.497\n",
      "Gradient Descent(29/49): loss=33653306125051.57\n",
      "Gradient Descent(30/49): loss=134132568082531.17\n",
      "Gradient Descent(31/49): loss=534614511678610.94\n",
      "Gradient Descent(32/49): loss=2130822366134863.2\n",
      "Gradient Descent(33/49): loss=8492855799525980.0\n",
      "Gradient Descent(34/49): loss=3.3850123209649344e+16\n",
      "Gradient Descent(35/49): loss=1.3491702536294131e+17\n",
      "Gradient Descent(36/49): loss=5.377411367175018e+17\n",
      "Gradient Descent(37/49): loss=2.1432842099827604e+18\n",
      "Gradient Descent(38/49): loss=8.542525187495163e+18\n",
      "Gradient Descent(39/49): loss=3.404809135395767e+19\n",
      "Gradient Descent(40/49): loss=1.3570607044207654e+20\n",
      "Gradient Descent(41/49): loss=5.408860474256542e+20\n",
      "Gradient Descent(42/49): loss=2.1558189353410002e+21\n",
      "Gradient Descent(43/49): loss=8.592485060568286e+21\n",
      "Gradient Descent(44/49): loss=3.424721728980041e+22\n",
      "Gradient Descent(45/49): loss=1.3649973015108722e+23\n",
      "Gradient Descent(46/49): loss=5.4404935074445115e+23\n",
      "Gradient Descent(47/49): loss=2.1684269684476804e+24\n",
      "Gradient Descent(48/49): loss=8.642737117610935e+24\n",
      "Gradient Descent(49/49): loss=3.4447507788379255e+25\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4123561181028762\n",
      "Gradient Descent(2/49): loss=0.3889944556364336\n",
      "Gradient Descent(3/49): loss=0.3836838528432016\n",
      "Gradient Descent(4/49): loss=0.39801325358509604\n",
      "Gradient Descent(5/49): loss=0.47114754147240456\n",
      "Gradient Descent(6/49): loss=0.763406498303671\n",
      "Gradient Descent(7/49): loss=1.8934487336594972\n",
      "Gradient Descent(8/49): loss=6.236842282662376\n",
      "Gradient Descent(9/49): loss=22.911090440410682\n",
      "Gradient Descent(10/49): loss=86.90753342823737\n",
      "Gradient Descent(11/49): loss=332.51562134377957\n",
      "Gradient Descent(12/49): loss=1275.1095373930523\n",
      "Gradient Descent(13/49): loss=4892.584680630282\n",
      "Gradient Descent(14/49): loss=18775.678330066363\n",
      "Gradient Descent(15/49): loss=72056.00781573639\n",
      "Gradient Descent(16/49): loss=276534.4556310475\n",
      "Gradient Descent(17/49): loss=1061278.765577515\n",
      "Gradient Descent(18/49): loss=4072958.6655582334\n",
      "Gradient Descent(19/49): loss=15631138.44795849\n",
      "Gradient Descent(20/49): loss=59988946.81776563\n",
      "Gradient Descent(21/49): loss=230224676.00611693\n",
      "Gradient Descent(22/49): loss=883552794.7129713\n",
      "Gradient Descent(23/49): loss=3390885613.315892\n",
      "Gradient Descent(24/49): loss=13013489758.513115\n",
      "Gradient Descent(25/49): loss=49942975085.77746\n",
      "Gradient Descent(26/49): loss=191670397928.7192\n",
      "Gradient Descent(27/49): loss=735589767714.0682\n",
      "Gradient Descent(28/49): loss=2823035336773.2764\n",
      "Gradient Descent(29/49): loss=10834202516762.516\n",
      "Gradient Descent(30/49): loss=41579339317940.875\n",
      "Gradient Descent(31/49): loss=159572562488256.03\n",
      "Gradient Descent(32/49): loss=612405178070789.8\n",
      "Gradient Descent(33/49): loss=2350279373094035.5\n",
      "Gradient Descent(34/49): loss=9019866796346270.0\n",
      "Gradient Descent(35/49): loss=3.4616309003607364e+16\n",
      "Gradient Descent(36/49): loss=1.3284994957116669e+17\n",
      "Gradient Descent(37/49): loss=5.0984953650669894e+17\n",
      "Gradient Descent(38/49): loss=1.9566928757985068e+18\n",
      "Gradient Descent(39/49): loss=7.509366462178399e+18\n",
      "Gradient Descent(40/49): loss=2.8819333560601367e+19\n",
      "Gradient Descent(41/49): loss=1.1060240448516755e+20\n",
      "Gradient Descent(42/49): loss=4.2446824289595536e+20\n",
      "Gradient Descent(43/49): loss=1.6290178325313413e+21\n",
      "Gradient Descent(44/49): loss=6.251820114032568e+21\n",
      "Gradient Descent(45/49): loss=2.399314111711575e+22\n",
      "Gradient Descent(46/49): loss=9.20805157803096e+22\n",
      "Gradient Descent(47/49): loss=3.5338521725775615e+23\n",
      "Gradient Descent(48/49): loss=1.3562164668392456e+24\n",
      "Gradient Descent(49/49): loss=5.20486713960192e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4109953102803028\n",
      "Gradient Descent(2/49): loss=0.38745890110203357\n",
      "Gradient Descent(3/49): loss=0.38177761282946027\n",
      "Gradient Descent(4/49): loss=0.3943907404506632\n",
      "Gradient Descent(5/49): loss=0.46066284206980584\n",
      "Gradient Descent(6/49): loss=0.726475385571113\n",
      "Gradient Descent(7/49): loss=1.7552673856179755\n",
      "Gradient Descent(8/49): loss=5.711443856491814\n",
      "Gradient Descent(9/49): loss=20.90509436252142\n",
      "Gradient Descent(10/49): loss=79.24039792571769\n",
      "Gradient Descent(11/49): loss=303.2032094649678\n",
      "Gradient Descent(12/49): loss=1163.0380234343145\n",
      "Gradient Descent(13/49): loss=4464.094765795563\n",
      "Gradient Descent(14/49): loss=17137.42362620682\n",
      "Gradient Descent(15/49): loss=65792.52315595896\n",
      "Gradient Descent(16/49): loss=252587.85307832572\n",
      "Gradient Descent(17/49): loss=969727.3802677155\n",
      "Gradient Descent(18/49): loss=3722949.8564061155\n",
      "Gradient Descent(19/49): loss=14293046.348804312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=54873471.94814723\n",
      "Gradient Descent(21/49): loss=210668732.94546443\n",
      "Gradient Descent(22/49): loss=808793641.4613801\n",
      "Gradient Descent(23/49): loss=3105098444.9371443\n",
      "Gradient Descent(24/49): loss=11921009094.016928\n",
      "Gradient Descent(25/49): loss=45766812340.567566\n",
      "Gradient Descent(26/49): loss=175706695241.2418\n",
      "Gradient Descent(27/49): loss=674568342732.5371\n",
      "Gradient Descent(28/49): loss=2589784347105.634\n",
      "Gradient Descent(29/49): loss=9942629292899.828\n",
      "Gradient Descent(30/49): loss=38171470673425.03\n",
      "Gradient Descent(31/49): loss=146546867075962.62\n",
      "Gradient Descent(32/49): loss=562618727308598.25\n",
      "Gradient Descent(33/49): loss=2159990442881793.5\n",
      "Gradient Descent(34/49): loss=8292576281026340.0\n",
      "Gradient Descent(35/49): loss=3.183663224217621e+16\n",
      "Gradient Descent(36/49): loss=1.222263284864359e+17\n",
      "Gradient Descent(37/49): loss=4.692479801768467e+17\n",
      "Gradient Descent(38/49): loss=1.8015240220889833e+18\n",
      "Gradient Descent(39/49): loss=6.916361794334142e+18\n",
      "Gradient Descent(40/49): loss=2.6553107193462268e+19\n",
      "Gradient Descent(41/49): loss=1.0194196350530519e+20\n",
      "Gradient Descent(42/49): loss=3.9137280046365075e+20\n",
      "Gradient Descent(43/49): loss=1.502547760273315e+21\n",
      "Gradient Descent(44/49): loss=5.76854030026556e+21\n",
      "Gradient Descent(45/49): loss=2.2146422280603526e+22\n",
      "Gradient Descent(46/49): loss=8.502393921183829e+22\n",
      "Gradient Descent(47/49): loss=3.2642158392462956e+23\n",
      "Gradient Descent(48/49): loss=1.2531888246955068e+24\n",
      "Gradient Descent(49/49): loss=4.8112082891684843e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41254011784897426\n",
      "Gradient Descent(2/49): loss=0.38926569099051916\n",
      "Gradient Descent(3/49): loss=0.38225745393831123\n",
      "Gradient Descent(4/49): loss=0.38909718682256833\n",
      "Gradient Descent(5/49): loss=0.4337679448010333\n",
      "Gradient Descent(6/49): loss=0.620183171551513\n",
      "Gradient Descent(7/49): loss=1.3586346896919905\n",
      "Gradient Descent(8/49): loss=4.25722644914841\n",
      "Gradient Descent(9/49): loss=15.61443855077584\n",
      "Gradient Descent(10/49): loss=60.09770786772939\n",
      "Gradient Descent(11/49): loss=234.31383916561705\n",
      "Gradient Descent(12/49): loss=916.6101530396019\n",
      "Gradient Descent(13/49): loss=3588.731647630265\n",
      "Gradient Descent(14/49): loss=14053.72749153069\n",
      "Gradient Descent(15/49): loss=55038.436161381695\n",
      "Gradient Descent(16/49): loss=215549.3731689124\n",
      "Gradient Descent(17/49): loss=844168.2300489461\n",
      "Gradient Descent(18/49): loss=3306066.926650799\n",
      "Gradient Descent(19/49): loss=12947752.224409541\n",
      "Gradient Descent(20/49): loss=50708077.41668555\n",
      "Gradient Descent(21/49): loss=198591161.6047506\n",
      "Gradient Descent(22/49): loss=777754780.4966825\n",
      "Gradient Descent(23/49): loss=3045968885.506562\n",
      "Gradient Descent(24/49): loss=11929115302.762861\n",
      "Gradient Descent(25/49): loss=46718728018.82777\n",
      "Gradient Descent(26/49): loss=182967428207.14963\n",
      "Gradient Descent(27/49): loss=716566593409.5562\n",
      "Gradient Descent(28/49): loss=2806333825766.0625\n",
      "Gradient Descent(29/49): loss=10990617779385.574\n",
      "Gradient Descent(30/49): loss=43043232442091.29\n",
      "Gradient Descent(31/49): loss=168572858801351.16\n",
      "Gradient Descent(32/49): loss=660192255836999.9\n",
      "Gradient Descent(33/49): loss=2585551539947189.0\n",
      "Gradient Descent(34/49): loss=1.0125954533120062e+16\n",
      "Gradient Descent(35/49): loss=3.965689858532443e+16\n",
      "Gradient Descent(36/49): loss=1.553107512247684e+17\n",
      "Gradient Descent(37/49): loss=6.082530481828233e+17\n",
      "Gradient Descent(38/49): loss=2.3821388262315853e+18\n",
      "Gradient Descent(39/49): loss=9.329316810483806e+18\n",
      "Gradient Descent(40/49): loss=3.6536977270994244e+19\n",
      "Gradient Descent(41/49): loss=1.430920114751545e+20\n",
      "Gradient Descent(42/49): loss=5.604000461269782e+20\n",
      "Gradient Descent(43/49): loss=2.1947291708430095e+21\n",
      "Gradient Descent(44/49): loss=8.595352849520889e+21\n",
      "Gradient Descent(45/49): loss=3.3662509064564505e+22\n",
      "Gradient Descent(46/49): loss=1.3183455482982816e+23\n",
      "Gradient Descent(47/49): loss=5.1631177622096106e+23\n",
      "Gradient Descent(48/49): loss=2.0220635675414183e+24\n",
      "Gradient Descent(49/49): loss=7.919131926652906e+24\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41451410988603143\n",
      "Gradient Descent(2/49): loss=0.3912153924034748\n",
      "Gradient Descent(3/49): loss=0.38765132460571117\n",
      "Gradient Descent(4/49): loss=0.41359860686672817\n",
      "Gradient Descent(5/49): loss=0.54518264857946\n",
      "Gradient Descent(6/49): loss=1.116499678490947\n",
      "Gradient Descent(7/49): loss=3.54787253050394\n",
      "Gradient Descent(8/49): loss=13.861509927297817\n",
      "Gradient Descent(9/49): loss=57.58552402344748\n",
      "Gradient Descent(10/49): loss=242.93067708661636\n",
      "Gradient Descent(11/49): loss=1028.5886149387134\n",
      "Gradient Descent(12/49): loss=4358.893929634558\n",
      "Gradient Descent(13/49): loss=18475.62898807916\n",
      "Gradient Descent(14/49): loss=78314.64071344692\n",
      "Gradient Descent(15/49): loss=331964.454491106\n",
      "Gradient Descent(16/49): loss=1407153.128240529\n",
      "Gradient Descent(17/49): loss=5964738.507904325\n",
      "Gradient Descent(18/49): loss=25283751.993905254\n",
      "Gradient Descent(19/49): loss=107174545.24607016\n",
      "Gradient Descent(20/49): loss=454298999.82892555\n",
      "Gradient Descent(21/49): loss=1925714554.422275\n",
      "Gradient Descent(22/49): loss=8162854305.639475\n",
      "Gradient Descent(23/49): loss=34601280999.425224\n",
      "Gradient Descent(24/49): loss=146670343730.66122\n",
      "Gradient Descent(25/49): loss=621716569700.3182\n",
      "Gradient Descent(26/49): loss=2635375926780.917\n",
      "Gradient Descent(27/49): loss=11171016849054.879\n",
      "Gradient Descent(28/49): loss=47352491981780.67\n",
      "Gradient Descent(29/49): loss=200721073755649.12\n",
      "Gradient Descent(30/49): loss=850830606024335.2\n",
      "Gradient Descent(31/49): loss=3606560619683647.0\n",
      "Gradient Descent(32/49): loss=1.5287742837826702e+16\n",
      "Gradient Descent(33/49): loss=6.480275967079788e+16\n",
      "Gradient Descent(34/49): loss=2.7469049587625248e+17\n",
      "Gradient Descent(35/49): loss=1.1643773954698324e+18\n",
      "Gradient Descent(36/49): loss=4.935644805460914e+18\n",
      "Gradient Descent(37/49): loss=2.0921558371410633e+19\n",
      "Gradient Descent(38/49): loss=8.868377323345188e+19\n",
      "Gradient Descent(39/49): loss=3.7591901594048345e+20\n",
      "Gradient Descent(40/49): loss=1.5934719666659525e+21\n",
      "Gradient Descent(41/49): loss=6.754521056078076e+21\n",
      "Gradient Descent(42/49): loss=2.863153896109055e+22\n",
      "Gradient Descent(43/49): loss=1.2136538127197017e+23\n",
      "Gradient Descent(44/49): loss=5.144521148970608e+23\n",
      "Gradient Descent(45/49): loss=2.1806958108504847e+24\n",
      "Gradient Descent(46/49): loss=9.24368679174825e+24\n",
      "Gradient Descent(47/49): loss=3.918278976773732e+25\n",
      "Gradient Descent(48/49): loss=1.660907653592541e+26\n",
      "Gradient Descent(49/49): loss=7.040372189204833e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41473967867111833\n",
      "Gradient Descent(2/49): loss=0.3907031792457435\n",
      "Gradient Descent(3/49): loss=0.38568130337490986\n",
      "Gradient Descent(4/49): loss=0.4056935690779742\n",
      "Gradient Descent(5/49): loss=0.5087453422958748\n",
      "Gradient Descent(6/49): loss=0.9428355589929561\n",
      "Gradient Descent(7/49): loss=2.7246855016972398\n",
      "Gradient Descent(8/49): loss=10.007345665660715\n",
      "Gradient Descent(9/49): loss=39.74897286248668\n",
      "Gradient Descent(10/49): loss=161.19214541115508\n",
      "Gradient Descent(11/49): loss=657.0628055021763\n",
      "Gradient Descent(12/49): loss=2681.7648053180505\n",
      "Gradient Descent(13/49): loss=10948.866618296453\n",
      "Gradient Descent(14/49): loss=44704.42949894677\n",
      "Gradient Descent(15/49): loss=182532.41256838926\n",
      "Gradient Descent(16/49): loss=745300.4751570198\n",
      "Gradient Descent(17/49): loss=3043149.4645708515\n",
      "Gradient Descent(18/49): loss=12425540.411901597\n",
      "Gradient Descent(19/49): loss=50734959.47566721\n",
      "Gradient Descent(20/49): loss=207156877.64284706\n",
      "Gradient Descent(21/49): loss=845846189.0698327\n",
      "Gradient Descent(22/49): loss=3453690673.464715\n",
      "Gradient Descent(23/49): loss=14101830125.795763\n",
      "Gradient Descent(24/49): loss=57579451001.86898\n",
      "Gradient Descent(25/49): loss=235103752360.4382\n",
      "Gradient Descent(26/49): loss=959956606261.4463\n",
      "Gradient Descent(27/49): loss=3919617090981.059\n",
      "Gradient Descent(28/49): loss=16004263150755.748\n",
      "Gradient Descent(29/49): loss=65347311498360.73\n",
      "Gradient Descent(30/49): loss=266820851409348.7\n",
      "Gradient Descent(31/49): loss=1089461297097089.8\n",
      "Gradient Descent(32/49): loss=4448400159144711.5\n",
      "Gradient Descent(33/49): loss=1.8163347361311044e+16\n",
      "Gradient Descent(34/49): loss=7.416310933480987e+16\n",
      "Gradient Descent(35/49): loss=3.028168033565584e+17\n",
      "Gradient Descent(36/49): loss=1.2364370536450007e+18\n",
      "Gradient Descent(37/49): loss=5.048519668263591e+18\n",
      "Gradient Descent(38/49): loss=2.0613706751757484e+19\n",
      "Gradient Descent(39/49): loss=8.416821840244564e+19\n",
      "Gradient Descent(40/49): loss=3.4366885462935175e+20\n",
      "Gradient Descent(41/49): loss=1.4032408417809368e+21\n",
      "Gradient Descent(42/49): loss=5.729599390569442e+21\n",
      "Gradient Descent(43/49): loss=2.339463632967692e+22\n",
      "Gradient Descent(44/49): loss=9.552308489467358e+22\n",
      "Gradient Descent(45/49): loss=3.9003212613398516e+23\n",
      "Gradient Descent(46/49): loss=1.592547598146857e+24\n",
      "Gradient Descent(47/49): loss=6.50256141077996e+24\n",
      "Gradient Descent(48/49): loss=2.655073226707208e+25\n",
      "Gradient Descent(49/49): loss=1.0840980029024741e+26\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.4134401462141953\n",
      "Gradient Descent(2/49): loss=0.3891783957414928\n",
      "Gradient Descent(3/49): loss=0.3836572585090403\n",
      "Gradient Descent(4/49): loss=0.4013868884086309\n",
      "Gradient Descent(5/49): loss=0.4948556437774492\n",
      "Gradient Descent(6/49): loss=0.8897444001546297\n",
      "Gradient Descent(7/49): loss=2.5119783564254567\n",
      "Gradient Descent(8/49): loss=9.14520791290701\n",
      "Gradient Descent(9/49): loss=36.244794164236296\n",
      "Gradient Descent(10/49): loss=146.93979101844866\n",
      "Gradient Descent(11/49): loss=599.0861283645596\n",
      "Gradient Descent(12/49): loss=2445.917214423598\n",
      "Gradient Descent(13/49): loss=9989.449553402883\n",
      "Gradient Descent(14/49): loss=40801.61488850164\n",
      "Gradient Descent(15/49): loss=166656.37863376254\n",
      "Gradient Descent(16/49): loss=680720.2623760651\n",
      "Gradient Descent(17/49): loss=2780455.4499126277\n",
      "Gradient Descent(18/49): loss=11356992.326166252\n",
      "Gradient Descent(19/49): loss=46388545.45678561\n",
      "Gradient Descent(20/49): loss=189477735.46723583\n",
      "Gradient Descent(21/49): loss=773937014.8322475\n",
      "Gradient Descent(22/49): loss=3161207843.6809597\n",
      "Gradient Descent(23/49): loss=12912207120.247763\n",
      "Gradient Descent(24/49): loss=52740946173.47704\n",
      "Gradient Descent(25/49): loss=215424627053.4969\n",
      "Gradient Descent(26/49): loss=879919176813.1903\n",
      "Gradient Descent(27/49): loss=3594100490340.116\n",
      "Gradient Descent(28/49): loss=14680391875818.664\n",
      "Gradient Descent(29/49): loss=59963238703774.45\n",
      "Gradient Descent(30/49): loss=244924660476434.1\n",
      "Gradient Descent(31/49): loss=1000414430678876.9\n",
      "Gradient Descent(32/49): loss=4086273024381023.5\n",
      "Gradient Descent(33/49): loss=1.6690710087471302e+16\n",
      "Gradient Descent(34/49): loss=6.817454476532871e+16\n",
      "Gradient Descent(35/49): loss=2.7846439903409245e+17\n",
      "Gradient Descent(36/49): loss=1.1374101843486289e+18\n",
      "Gradient Descent(37/49): loss=4.645843174019571e+18\n",
      "Gradient Descent(38/49): loss=1.8976319268624155e+19\n",
      "Gradient Descent(39/49): loss=7.751029888363646e+19\n",
      "Gradient Descent(40/49): loss=3.1659703591539166e+20\n",
      "Gradient Descent(41/49): loss=1.2931659997968893e+21\n",
      "Gradient Descent(42/49): loss=5.282040301468902e+21\n",
      "Gradient Descent(43/49): loss=2.1574917489884286e+22\n",
      "Gradient Descent(44/49): loss=8.812448185331884e+22\n",
      "Gradient Descent(45/49): loss=3.599515180327938e+23\n",
      "Gradient Descent(46/49): loss=1.4702508611599274e+24\n",
      "Gradient Descent(47/49): loss=6.005357628592032e+24\n",
      "Gradient Descent(48/49): loss=2.452936515802174e+25\n",
      "Gradient Descent(49/49): loss=1.0019216044534599e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41485346861525046\n",
      "Gradient Descent(2/49): loss=0.3908187134810896\n",
      "Gradient Descent(3/49): loss=0.38364136308811786\n",
      "Gradient Descent(4/49): loss=0.39378393903500575\n",
      "Gradient Descent(5/49): loss=0.4570630255604811\n",
      "Gradient Descent(6/49): loss=0.7338384908052935\n",
      "Gradient Descent(7/49): loss=1.8962006053894396\n",
      "Gradient Descent(8/49): loss=6.745592018806447\n",
      "Gradient Descent(9/49): loss=26.953141143395346\n",
      "Gradient Descent(10/49): loss=111.13943742596906\n",
      "Gradient Descent(11/49): loss=461.8509258777925\n",
      "Gradient Descent(12/49): loss=1922.8663790140208\n",
      "Gradient Descent(13/49): loss=8009.243781504045\n",
      "Gradient Descent(14/49): loss=33364.196108273696\n",
      "Gradient Descent(15/49): loss=138989.18791088578\n",
      "Gradient Descent(16/49): loss=579007.3190798055\n",
      "Gradient Descent(17/49): loss=2412057.924911182\n",
      "Gradient Descent(18/49): loss=10048276.261412138\n",
      "Gradient Descent(19/49): loss=41859635.03701221\n",
      "Gradient Descent(20/49): loss=174381061.5510545\n",
      "Gradient Descent(21/49): loss=726445769.2899764\n",
      "Gradient Descent(22/49): loss=3026265877.9896383\n",
      "Gradient Descent(23/49): loss=12606977084.908094\n",
      "Gradient Descent(24/49): loss=52518806224.56244\n",
      "Gradient Descent(25/49): loss=218785596953.28607\n",
      "Gradient Descent(26/49): loss=911428512478.3379\n",
      "Gradient Descent(27/49): loss=3796876690820.925\n",
      "Gradient Descent(28/49): loss=15817228019455.092\n",
      "Gradient Descent(29/49): loss=65892237908140.4\n",
      "Gradient Descent(30/49): loss=274497339938616.97\n",
      "Gradient Descent(31/49): loss=1143515412823297.8\n",
      "Gradient Descent(32/49): loss=4763716470464914.0\n",
      "Gradient Descent(33/49): loss=1.9844939872695788e+16\n",
      "Gradient Descent(34/49): loss=8.267109115175368e+16\n",
      "Gradient Descent(35/49): loss=3.4439556663131616e+17\n",
      "Gradient Descent(36/49): loss=1.4347011109068956e+18\n",
      "Gradient Descent(37/49): loss=5.976753120753676e+18\n",
      "Gradient Descent(38/49): loss=2.4898271559752815e+19\n",
      "Gradient Descent(39/49): loss=1.0372252528058787e+20\n",
      "Gradient Descent(40/49): loss=4.320927348215063e+20\n",
      "Gradient Descent(41/49): loss=1.8000345728226277e+21\n",
      "Gradient Descent(42/49): loss=7.498678413778919e+21\n",
      "Gradient Descent(43/49): loss=3.1238387752242476e+22\n",
      "Gradient Descent(44/49): loss=1.3013451377862824e+23\n",
      "Gradient Descent(45/49): loss=5.421211814999882e+23\n",
      "Gradient Descent(46/49): loss=2.2583968456737791e+24\n",
      "Gradient Descent(47/49): loss=9.408148005649251e+24\n",
      "Gradient Descent(48/49): loss=3.919295630692898e+25\n",
      "Gradient Descent(49/49): loss=1.6327207258585607e+26\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41702980154991526\n",
      "Gradient Descent(2/49): loss=0.3932154885019658\n",
      "Gradient Descent(3/49): loss=0.39035369323396635\n",
      "Gradient Descent(4/49): loss=0.4250872905324313\n",
      "Gradient Descent(5/49): loss=0.6068090151960486\n",
      "Gradient Descent(6/49): loss=1.4402071666539944\n",
      "Gradient Descent(7/49): loss=5.2011964896471214\n",
      "Gradient Descent(8/49): loss=22.133058831202966\n",
      "Gradient Descent(9/49): loss=98.32966312709256\n",
      "Gradient Descent(10/49): loss=441.2055081296693\n",
      "Gradient Descent(11/49): loss=1984.0882279120344\n",
      "Gradient Descent(12/49): loss=8926.781743219319\n",
      "Gradient Descent(13/49): loss=40167.63598878015\n",
      "Gradient Descent(14/49): loss=180745.77055127078\n",
      "Gradient Descent(15/49): loss=813321.6757323433\n",
      "Gradient Descent(16/49): loss=3659797.5951236147\n",
      "Gradient Descent(17/49): loss=16468418.804966496\n",
      "Gradient Descent(18/49): loss=74104872.41605817\n",
      "Gradient Descent(19/49): loss=333458375.84050393\n",
      "Gradient Descent(20/49): loss=1500501722.9589844\n",
      "Gradient Descent(21/49): loss=6751983411.346673\n",
      "Gradient Descent(22/49): loss=30382690866.695587\n",
      "Gradient Descent(23/49): loss=136716553950.53215\n",
      "Gradient Descent(24/49): loss=615199496524.1633\n",
      "Gradient Descent(25/49): loss=2768285255793.5776\n",
      "Gradient Descent(26/49): loss=12456777518097.334\n",
      "Gradient Descent(27/49): loss=56053221325596.734\n",
      "Gradient Descent(28/49): loss=252229247605303.2\n",
      "Gradient Descent(29/49): loss=1134985498478122.5\n",
      "Gradient Descent(30/49): loss=5107227230727343.0\n",
      "Gradient Descent(31/49): loss=2.298158877030401e+16\n",
      "Gradient Descent(32/49): loss=1.0341294768123242e+17\n",
      "Gradient Descent(33/49): loss=4.653393573006606e+17\n",
      "Gradient Descent(34/49): loss=2.0939420286178378e+18\n",
      "Gradient Descent(35/49): loss=9.422356287777253e+18\n",
      "Gradient Descent(36/49): loss=4.239888058048128e+19\n",
      "Gradient Descent(37/49): loss=1.9078721071180716e+20\n",
      "Gradient Descent(38/49): loss=8.585075660687973e+20\n",
      "Gradient Descent(39/49): loss=3.863127084082809e+21\n",
      "Gradient Descent(40/49): loss=1.7383365572550139e+22\n",
      "Gradient Descent(41/49): loss=7.822196682940788e+22\n",
      "Gradient Descent(42/49): loss=3.51984549201619e+23\n",
      "Gradient Descent(43/49): loss=1.5838661171338473e+24\n",
      "Gradient Descent(44/49): loss=7.127107944638098e+24\n",
      "Gradient Descent(45/49): loss=3.207068268272697e+25\n",
      "Gradient Descent(46/49): loss=1.4431220850386716e+26\n",
      "Gradient Descent(47/49): loss=6.49378553281634e+26\n",
      "Gradient Descent(48/49): loss=2.922084762155354e+27\n",
      "Gradient Descent(49/49): loss=1.3148847177152322e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41729505453627597\n",
      "Gradient Descent(2/49): loss=0.39276214107691937\n",
      "Gradient Descent(3/49): loss=0.3882003797271456\n",
      "Gradient Descent(4/49): loss=0.4153742388207475\n",
      "Gradient Descent(5/49): loss=0.5583741214294625\n",
      "Gradient Descent(6/49): loss=1.1939113682010523\n",
      "Gradient Descent(7/49): loss=3.960283306258983\n",
      "Gradient Descent(8/49): loss=15.96332375529065\n",
      "Gradient Descent(9/49): loss=68.015292889268\n",
      "Gradient Descent(10/49): loss=293.72032476894066\n",
      "Gradient Descent(11/49): loss=1272.3934276725909\n",
      "Gradient Descent(12/49): loss=5515.975781022623\n",
      "Gradient Descent(13/49): loss=23916.37932067763\n",
      "Gradient Descent(14/49): loss=103701.51895238986\n",
      "Gradient Descent(15/49): loss=449654.168819267\n",
      "Gradient Descent(16/49): loss=1949723.4296734321\n",
      "Gradient Descent(17/49): loss=8454104.264324805\n",
      "Gradient Descent(18/49): loss=36657448.6960874\n",
      "Gradient Descent(19/49): loss=158948664.00656474\n",
      "Gradient Descent(20/49): loss=689209937.7432745\n",
      "Gradient Descent(21/49): loss=2988451283.1715894\n",
      "Gradient Descent(22/49): loss=12958085171.90013\n",
      "Gradient Descent(23/49): loss=56186952847.28054\n",
      "Gradient Descent(24/49): loss=243629643456.83124\n",
      "Gradient Descent(25/49): loss=1056391211184.8508\n",
      "Gradient Descent(26/49): loss=4580568994951.883\n",
      "Gradient Descent(27/49): loss=19861593030469.824\n",
      "Gradient Descent(28/49): loss=86120933478519.88\n",
      "Gradient Descent(29/49): loss=373424990222771.4\n",
      "Gradient Descent(30/49): loss=1619190801707367.8\n",
      "Gradient Descent(31/49): loss=7020898228502993.0\n",
      "Gradient Descent(32/49): loss=3.044299157518538e+16\n",
      "Gradient Descent(33/49): loss=1.3200244553956997e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(34/49): loss=5.723696892729229e+17\n",
      "Gradient Descent(35/49): loss=2.4818256954200745e+18\n",
      "Gradient Descent(36/49): loss=1.0761329430759016e+19\n",
      "Gradient Descent(37/49): loss=4.666170204097332e+19\n",
      "Gradient Descent(38/49): loss=2.0232764468088462e+20\n",
      "Gradient Descent(39/49): loss=8.773035275517708e+20\n",
      "Gradient Descent(40/49): loss=3.804035185941415e+21\n",
      "Gradient Descent(41/49): loss=1.6494500753078349e+22\n",
      "Gradient Descent(42/49): loss=7.1521040630429075e+22\n",
      "Gradient Descent(43/49): loss=3.101190711640598e+23\n",
      "Gradient Descent(44/49): loss=1.3446929386363002e+24\n",
      "Gradient Descent(45/49): loss=5.830660760175452e+24\n",
      "Gradient Descent(46/49): loss=2.528205802487989e+25\n",
      "Gradient Descent(47/49): loss=1.0962436064521722e+26\n",
      "Gradient Descent(48/49): loss=4.7533711199644096e+26\n",
      "Gradient Descent(49/49): loss=2.06108723199179e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41606045562984495\n",
      "Gradient Descent(2/49): loss=0.3912507251516028\n",
      "Gradient Descent(3/49): loss=0.3860387688027621\n",
      "Gradient Descent(4/49): loss=0.4102186712215795\n",
      "Gradient Descent(5/49): loss=0.5400011857056767\n",
      "Gradient Descent(6/49): loss=1.1182147745510385\n",
      "Gradient Descent(7/49): loss=3.636783235637838\n",
      "Gradient Descent(8/49): loss=14.569132090938522\n",
      "Gradient Descent(9/49): loss=61.995292085159186\n",
      "Gradient Descent(10/49): loss=267.7154343066136\n",
      "Gradient Descent(11/49): loss=1160.0490883062394\n",
      "Gradient Descent(12/49): loss=5030.630068484643\n",
      "Gradient Descent(13/49): loss=21819.626893173925\n",
      "Gradient Descent(14/49): loss=94643.41721353096\n",
      "Gradient Descent(15/49): loss=410523.1782278229\n",
      "Gradient Descent(16/49): loss=1780680.135206892\n",
      "Gradient Descent(17/49): loss=7723859.524081021\n",
      "Gradient Descent(18/49): loss=33502936.19269722\n",
      "Gradient Descent(19/49): loss=145322006.48816586\n",
      "Gradient Descent(20/49): loss=630347309.8528666\n",
      "Gradient Descent(21/49): loss=2734188311.947462\n",
      "Gradient Descent(22/49): loss=11859788422.679893\n",
      "Gradient Descent(23/49): loss=51442902022.08644\n",
      "Gradient Descent(24/49): loss=223138227612.87103\n",
      "Gradient Descent(25/49): loss=967882189092.5878\n",
      "Gradient Descent(26/49): loss=4198276297097.8604\n",
      "Gradient Descent(27/49): loss=18210402118569.875\n",
      "Gradient Descent(28/49): loss=78989261747558.89\n",
      "Gradient Descent(29/49): loss=342623047574656.4\n",
      "Gradient Descent(30/49): loss=1486158373077540.0\n",
      "Gradient Descent(31/49): loss=6446345993076174.0\n",
      "Gradient Descent(32/49): loss=2.7961607198293148e+16\n",
      "Gradient Descent(33/49): loss=1.2128599332884376e+17\n",
      "Gradient Descent(34/49): loss=5.260889359272014e+17\n",
      "Gradient Descent(35/49): loss=2.2819582122282255e+18\n",
      "Gradient Descent(36/49): loss=9.89819957566315e+18\n",
      "Gradient Descent(37/49): loss=4.2934333466164986e+19\n",
      "Gradient Descent(38/49): loss=1.8623154403919865e+20\n",
      "Gradient Descent(39/49): loss=8.077961201506484e+20\n",
      "Gradient Descent(40/49): loss=3.5038885334759406e+21\n",
      "Gradient Descent(41/49): loss=1.5198432560847534e+22\n",
      "Gradient Descent(42/49): loss=6.592457211459428e+22\n",
      "Gradient Descent(43/49): loss=2.8595377787102695e+23\n",
      "Gradient Descent(44/49): loss=1.2403503042321301e+24\n",
      "Gradient Descent(45/49): loss=5.380131322841359e+24\n",
      "Gradient Descent(46/49): loss=2.3336804894758114e+25\n",
      "Gradient Descent(47/49): loss=1.0122549618517366e+26\n",
      "Gradient Descent(48/49): loss=4.3907472013171555e+26\n",
      "Gradient Descent(49/49): loss=1.9045262026286643e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41733561250987045\n",
      "Gradient Descent(2/49): loss=0.39270168521900084\n",
      "Gradient Descent(3/49): loss=0.3854378560535445\n",
      "Gradient Descent(4/49): loss=0.3997471789032167\n",
      "Gradient Descent(5/49): loss=0.487838328591952\n",
      "Gradient Descent(6/49): loss=0.8927527057468855\n",
      "Gradient Descent(7/49): loss=2.6942781278846515\n",
      "Gradient Descent(8/49): loss=10.670408098288537\n",
      "Gradient Descent(9/49): loss=45.9553836518666\n",
      "Gradient Descent(10/49): loss=202.0274425908495\n",
      "Gradient Descent(11/49): loss=892.345503022766\n",
      "Gradient Descent(12/49): loss=3945.6578983602426\n",
      "Gradient Descent(13/49): loss=17450.604183684645\n",
      "Gradient Descent(14/49): loss=77183.61552819589\n",
      "Gradient Descent(15/49): loss=341385.52034627507\n",
      "Gradient Descent(16/49): loss=1509962.9037190604\n",
      "Gradient Descent(17/49): loss=6678635.326199621\n",
      "Gradient Descent(18/49): loss=29539915.191196892\n",
      "Gradient Descent(19/49): loss=130656425.2588706\n",
      "Gradient Descent(20/49): loss=577899478.5159049\n",
      "Gradient Descent(21/49): loss=2556076420.6639924\n",
      "Gradient Descent(22/49): loss=11305645555.271585\n",
      "Gradient Descent(23/49): loss=50005399055.65402\n",
      "Gradient Descent(24/49): loss=221176218778.25677\n",
      "Gradient Descent(25/49): loss=978272760083.5254\n",
      "Gradient Descent(26/49): loss=4326946171739.5356\n",
      "Gradient Descent(27/49): loss=19138285289203.25\n",
      "Gradient Descent(28/49): loss=84649530933198.56\n",
      "Gradient Descent(29/49): loss=374408834382519.56\n",
      "Gradient Descent(30/49): loss=1656027785603464.8\n",
      "Gradient Descent(31/49): loss=7324688348269362.0\n",
      "Gradient Descent(32/49): loss=3.239743914062506e+16\n",
      "Gradient Descent(33/49): loss=1.4329538854967232e+17\n",
      "Gradient Descent(34/49): loss=6.338022054913827e+17\n",
      "Gradient Descent(35/49): loss=2.8033367978657085e+18\n",
      "Gradient Descent(36/49): loss=1.239928976923463e+19\n",
      "Gradient Descent(37/49): loss=5.4842638565047575e+19\n",
      "Gradient Descent(38/49): loss=2.4257155536756923e+20\n",
      "Gradient Descent(39/49): loss=1.0729053344808269e+21\n",
      "Gradient Descent(40/49): loss=4.745510474272508e+21\n",
      "Gradient Descent(41/49): loss=2.098961477559235e+22\n",
      "Gradient Descent(42/49): loss=9.283804783832007e+22\n",
      "Gradient Descent(43/49): loss=4.1062702763143564e+23\n",
      "Gradient Descent(44/49): loss=1.8162225482710862e+24\n",
      "Gradient Descent(45/49): loss=8.03323727586961e+24\n",
      "Gradient Descent(46/49): loss=3.5531384186288155e+25\n",
      "Gradient Descent(47/49): loss=1.5715697406149037e+26\n",
      "Gradient Descent(48/49): loss=6.951126465175714e+26\n",
      "Gradient Descent(49/49): loss=3.074515746018474e+27\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4197159052804474\n",
      "Gradient Descent(2/49): loss=0.39559190480796896\n",
      "Gradient Descent(3/49): loss=0.3936921890837095\n",
      "Gradient Descent(4/49): loss=0.4393915193006775\n",
      "Gradient Descent(5/49): loss=0.6872812220405174\n",
      "Gradient Descent(6/49): loss=1.8877162696764866\n",
      "Gradient Descent(7/49): loss=7.624466114460954\n",
      "Gradient Descent(8/49): loss=34.989514627718826\n",
      "Gradient Descent(9/49): loss=165.4882296980369\n",
      "Gradient Descent(10/49): loss=787.7843440239283\n",
      "Gradient Descent(11/49): loss=3755.243760432346\n",
      "Gradient Descent(12/49): loss=17905.749336958077\n",
      "Gradient Descent(13/49): loss=85383.25730823197\n",
      "Gradient Descent(14/49): loss=407153.6629356008\n",
      "Gradient Descent(15/49): loss=1941534.4723221739\n",
      "Gradient Descent(16/49): loss=9258318.192943955\n",
      "Gradient Descent(17/49): loss=44148824.95418758\n",
      "Gradient Descent(18/49): loss=210526226.35048038\n",
      "Gradient Descent(19/49): loss=1003906496.9393717\n",
      "Gradient Descent(20/49): loss=4787186248.13425\n",
      "Gradient Descent(21/49): loss=22827974765.820232\n",
      "Gradient Descent(22/49): loss=108856519241.56584\n",
      "Gradient Descent(23/49): loss=519088613995.0844\n",
      "Gradient Descent(24/49): loss=2475304107252.6704\n",
      "Gradient Descent(25/49): loss=11803630937361.879\n",
      "Gradient Descent(26/49): loss=56286297468350.4\n",
      "Gradient Descent(27/49): loss=268404468041069.66\n",
      "Gradient Descent(28/49): loss=1279902244501336.0\n",
      "Gradient Descent(29/49): loss=6103287949844695.0\n",
      "Gradient Descent(30/49): loss=2.9103881924382784e+16\n",
      "Gradient Descent(31/49): loss=1.387835458574371e+17\n",
      "Gradient Descent(32/49): loss=6.6179737296925e+17\n",
      "Gradient Descent(33/49): loss=3.155819086211385e+18\n",
      "Gradient Descent(34/49): loss=1.5048706011347423e+19\n",
      "Gradient Descent(35/49): loss=7.1760625824667935e+19\n",
      "Gradient Descent(36/49): loss=3.421946986581441e+20\n",
      "Gradient Descent(37/49): loss=1.6317752311112624e+21\n",
      "Gradient Descent(38/49): loss=7.781214657355966e+21\n",
      "Gradient Descent(39/49): loss=3.71051725687834e+22\n",
      "Gradient Descent(40/49): loss=1.769381635112298e+23\n",
      "Gradient Descent(41/49): loss=8.437398761234756e+23\n",
      "Gradient Descent(42/49): loss=4.0234224456371203e+24\n",
      "Gradient Descent(43/49): loss=1.9185922858631472e+25\n",
      "Gradient Descent(44/49): loss=9.148918387541328e+25\n",
      "Gradient Descent(45/49): loss=4.362714698617328e+26\n",
      "Gradient Descent(46/49): loss=2.0803857609497434e+27\n",
      "Gradient Descent(47/49): loss=9.920439940145628e+27\n",
      "Gradient Descent(48/49): loss=4.730619217519918e+28\n",
      "Gradient Descent(49/49): loss=2.2558231606853758e+29\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4200222456983491\n",
      "Gradient Descent(2/49): loss=0.39520859060677077\n",
      "Gradient Descent(3/49): loss=0.3913375532556818\n",
      "Gradient Descent(4/49): loss=0.4274813518812098\n",
      "Gradient Descent(5/49): loss=0.6233240520762273\n",
      "Gradient Descent(6/49): loss=1.541726392191482\n",
      "Gradient Descent(7/49): loss=5.775486632066439\n",
      "Gradient Descent(8/49): loss=25.245198362572967\n",
      "Gradient Descent(9/49): loss=114.74619282678461\n",
      "Gradient Descent(10/49): loss=526.1507547219945\n",
      "Gradient Descent(11/49): loss=2417.2126207980496\n",
      "Gradient Descent(12/49): loss=11109.649988527586\n",
      "Gradient Descent(13/49): loss=51065.21368278252\n",
      "Gradient Descent(14/49): loss=234724.51547450467\n",
      "Gradient Descent(15/49): loss=1078930.820337937\n",
      "Gradient Descent(16/49): loss=4959400.128923314\n",
      "Gradient Descent(17/49): loss=22796322.17394696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=104785319.56574582\n",
      "Gradient Descent(19/49): loss=481655032.640016\n",
      "Gradient Descent(20/49): loss=2213970162.1159496\n",
      "Gradient Descent(21/49): loss=10176710609.817389\n",
      "Gradient Descent(22/49): loss=46778154745.81384\n",
      "Gradient Descent(23/49): loss=215019945576.4134\n",
      "Gradient Descent(24/49): loss=988358289186.9869\n",
      "Gradient Descent(25/49): loss=4543076714055.076\n",
      "Gradient Descent(26/49): loss=20882655870445.25\n",
      "Gradient Descent(27/49): loss=95988983600988.17\n",
      "Gradient Descent(28/49): loss=441221893896706.7\n",
      "Gradient Descent(29/49): loss=2028115647760538.2\n",
      "Gradient Descent(30/49): loss=9322413818508318.0\n",
      "Gradient Descent(31/49): loss=4.285130362239522e+16\n",
      "Gradient Descent(32/49): loss=1.9696982540006458e+17\n",
      "Gradient Descent(33/49): loss=9.053893076394989e+17\n",
      "Gradient Descent(34/49): loss=4.161702416717802e+18\n",
      "Gradient Descent(35/49): loss=1.912963501907298e+19\n",
      "Gradient Descent(36/49): loss=8.7931067462423e+19\n",
      "Gradient Descent(37/49): loss=4.041829662391492e+20\n",
      "Gradient Descent(38/49): loss=1.857862925042905e+21\n",
      "Gradient Descent(39/49): loss=8.539832047762881e+21\n",
      "Gradient Descent(40/49): loss=3.925409696321477e+22\n",
      "Gradient Descent(41/49): loss=1.8043494529861792e+23\n",
      "Gradient Descent(42/49): loss=8.293852617581576e+23\n",
      "Gradient Descent(43/49): loss=3.8123430651595553e+24\n",
      "Gradient Descent(44/49): loss=1.752377371121902e+25\n",
      "Gradient Descent(45/49): loss=8.054958324406956e+25\n",
      "Gradient Descent(46/49): loss=3.702533180190081e+26\n",
      "Gradient Descent(47/49): loss=1.701902281588477e+27\n",
      "Gradient Descent(48/49): loss=7.822945089522435e+27\n",
      "Gradient Descent(49/49): loss=3.59588623481727e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41885623852725185\n",
      "Gradient Descent(2/49): loss=0.39371365416930926\n",
      "Gradient Descent(3/49): loss=0.3890165436429412\n",
      "Gradient Descent(4/49): loss=0.4212798900779275\n",
      "Gradient Descent(5/49): loss=0.5990967978280656\n",
      "Gradient Descent(6/49): loss=1.434721430649322\n",
      "Gradient Descent(7/49): loss=5.289198408873454\n",
      "Gradient Descent(8/49): loss=23.021645609121308\n",
      "Gradient Descent(9/49): loss=104.56573183092216\n",
      "Gradient Descent(10/49): loss=479.5272383461339\n",
      "Gradient Descent(11/49): loss=2203.680432184557\n",
      "Gradient Descent(12/49): loss=10131.689457357264\n",
      "Gradient Descent(13/49): loss=46586.283395992534\n",
      "Gradient Descent(14/49): loss=214211.89153939215\n",
      "Gradient Descent(15/49): loss=984988.3876294213\n",
      "Gradient Descent(16/49): loss=4529174.817556296\n",
      "Gradient Descent(17/49): loss=20826061.799779356\n",
      "Gradient Descent(18/49): loss=95762448.62480666\n",
      "Gradient Descent(19/49): loss=440335131.52089673\n",
      "Gradient Descent(20/49): loss=2024750111.0576618\n",
      "Gradient Descent(21/49): loss=9310211066.000456\n",
      "Gradient Descent(22/49): loss=42810236003.60119\n",
      "Gradient Descent(23/49): loss=196850135161.73154\n",
      "Gradient Descent(24/49): loss=905156787973.0422\n",
      "Gradient Descent(25/49): loss=4162094225342.675\n",
      "Gradient Descent(26/49): loss=19138152164142.25\n",
      "Gradient Descent(27/49): loss=88001099549287.08\n",
      "Gradient Descent(28/49): loss=404646877894150.5\n",
      "Gradient Descent(29/49): loss=1860648294488393.0\n",
      "Gradient Descent(30/49): loss=8555637680437702.0\n",
      "Gradient Descent(31/49): loss=3.934055476027156e+16\n",
      "Gradient Descent(32/49): loss=1.8089583811907392e+17\n",
      "Gradient Descent(33/49): loss=8.31795699074586e+17\n",
      "Gradient Descent(34/49): loss=3.824765081347654e+18\n",
      "Gradient Descent(35/49): loss=1.758704444345188e+19\n",
      "Gradient Descent(36/49): loss=8.086879211597736e+19\n",
      "Gradient Descent(37/49): loss=3.7185108386599515e+20\n",
      "Gradient Descent(38/49): loss=1.7098465916742268e+21\n",
      "Gradient Descent(39/49): loss=7.862220910222272e+21\n",
      "Gradient Descent(40/49): loss=3.615208401860629e+22\n",
      "Gradient Descent(41/49): loss=1.662346039131353e+23\n",
      "Gradient Descent(42/49): loss=7.643803749719014e+23\n",
      "Gradient Descent(43/49): loss=3.5147757680313293e+24\n",
      "Gradient Descent(44/49): loss=1.6161650801139498e+25\n",
      "Gradient Descent(45/49): loss=7.431454347492543e+25\n",
      "Gradient Descent(46/49): loss=3.417133212343083e+26\n",
      "Gradient Descent(47/49): loss=1.5712670555310996e+27\n",
      "Gradient Descent(48/49): loss=7.225004137618717e+27\n",
      "Gradient Descent(49/49): loss=3.322203224770289e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4199865495328342\n",
      "Gradient Descent(2/49): loss=0.39495022342277375\n",
      "Gradient Descent(3/49): loss=0.38772834076617163\n",
      "Gradient Descent(4/49): loss=0.40726781085670194\n",
      "Gradient Descent(5/49): loss=0.5281399040424797\n",
      "Gradient Descent(6/49): loss=1.112721679685006\n",
      "Gradient Descent(7/49): loss=3.865335494361672\n",
      "Gradient Descent(8/49): loss=16.778267726888014\n",
      "Gradient Descent(9/49): loss=77.32014101437044\n",
      "Gradient Descent(10/49): loss=361.14243051230704\n",
      "Gradient Descent(11/49): loss=1691.6898852449983\n",
      "Gradient Descent(12/49): loss=7929.226037588522\n",
      "Gradient Descent(13/49): loss=37170.45227665003\n",
      "Gradient Descent(14/49): loss=174251.70058261263\n",
      "Gradient Descent(15/49): loss=816880.9874705567\n",
      "Gradient Descent(16/49): loss=3829491.4240450733\n",
      "Gradient Descent(17/49): loss=17952441.970445886\n",
      "Gradient Descent(18/49): loss=84160050.39471577\n",
      "Gradient Descent(19/49): loss=394537644.56628615\n",
      "Gradient Descent(20/49): loss=1849570581.9613485\n",
      "Gradient Descent(21/49): loss=8670684246.928905\n",
      "Gradient Descent(22/49): loss=40647686577.68229\n",
      "Gradient Descent(23/49): loss=190554098973.73608\n",
      "Gradient Descent(24/49): loss=893307041385.8306\n",
      "Gradient Descent(25/49): loss=4187773836869.29\n",
      "Gradient Descent(26/49): loss=19632051351083.953\n",
      "Gradient Descent(27/49): loss=92033967273586.25\n",
      "Gradient Descent(28/49): loss=431450131249166.94\n",
      "Gradient Descent(29/49): loss=2022614272419323.2\n",
      "Gradient Descent(30/49): loss=9481903466224098.0\n",
      "Gradient Descent(31/49): loss=4.445063726127587e+16\n",
      "Gradient Descent(32/49): loss=2.0838212073892774e+17\n",
      "Gradient Descent(33/49): loss=9.768838180748209e+17\n",
      "Gradient Descent(34/49): loss=4.579577127982166e+18\n",
      "Gradient Descent(35/49): loss=2.1468803437105017e+19\n",
      "Gradient Descent(36/49): loss=1.0064455912420907e+20\n",
      "Gradient Descent(37/49): loss=4.718161080090502e+20\n",
      "Gradient Descent(38/49): loss=2.2118477314017025e+21\n",
      "Gradient Descent(39/49): loss=1.0369019420619376e+22\n",
      "Gradient Descent(40/49): loss=4.860938762590444e+22\n",
      "Gradient Descent(41/49): loss=2.2787811166277068e+23\n",
      "Gradient Descent(42/49): loss=1.068279941616004e+24\n",
      "Gradient Descent(43/49): loss=5.008037083210392e+24\n",
      "Gradient Descent(44/49): loss=2.3477399930275416e+25\n",
      "Gradient Descent(45/49): loss=1.100607480192133e+26\n",
      "Gradient Descent(46/49): loss=5.159586790071983e+26\n",
      "Gradient Descent(47/49): loss=2.418785654594945e+27\n",
      "Gradient Descent(48/49): loss=1.1339132920744377e+28\n",
      "Gradient Descent(49/49): loss=5.315722587904831e+28\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.422572421077628\n",
      "Gradient Descent(2/49): loss=0.39838298628447005\n",
      "Gradient Descent(3/49): loss=0.3977778580878793\n",
      "Gradient Descent(4/49): loss=0.4570833350659668\n",
      "Gradient Descent(5/49): loss=0.7915475045923689\n",
      "Gradient Descent(6/49): loss=2.5006456143733646\n",
      "Gradient Descent(7/49): loss=11.137635480890186\n",
      "Gradient Descent(8/49): loss=54.72229726603629\n",
      "Gradient Descent(9/49): loss=274.6185971102343\n",
      "Gradient Descent(10/49): loss=1384.0216864311776\n",
      "Gradient Descent(11/49): loss=6981.067591179485\n",
      "Gradient Descent(12/49): loss=35218.68576321477\n",
      "Gradient Descent(13/49): loss=177680.0849253634\n",
      "Gradient Descent(14/49): loss=896411.0260856869\n",
      "Gradient Descent(15/49): loss=4522475.119949435\n",
      "Gradient Descent(16/49): loss=22816303.941682264\n",
      "Gradient Descent(17/49): loss=115110362.80803308\n",
      "Gradient Descent(18/49): loss=580742428.4034609\n",
      "Gradient Descent(19/49): loss=2929899277.4281363\n",
      "Gradient Descent(20/49): loss=14781612913.732101\n",
      "Gradient Descent(21/49): loss=74574604673.842\n",
      "Gradient Descent(22/49): loss=376235779870.75226\n",
      "Gradient Descent(23/49): loss=1898144317016.201\n",
      "Gradient Descent(24/49): loss=9576313686754.639\n",
      "Gradient Descent(25/49): loss=48313388505301.67\n",
      "Gradient Descent(26/49): loss=243745514737348.44\n",
      "Gradient Descent(27/49): loss=1229718672041676.5\n",
      "Gradient Descent(28/49): loss=6204044468253855.0\n",
      "Gradient Descent(29/49): loss=3.129997831143515e+16\n",
      "Gradient Descent(30/49): loss=1.5791128630837674e+17\n",
      "Gradient Descent(31/49): loss=7.966770486373306e+17\n",
      "Gradient Descent(32/49): loss=4.0193094151991337e+18\n",
      "Gradient Descent(33/49): loss=2.0277787847335297e+19\n",
      "Gradient Descent(34/49): loss=1.0230331569563099e+20\n",
      "Gradient Descent(35/49): loss=5.1612969230738974e+20\n",
      "Gradient Descent(36/49): loss=2.6039220475891276e+21\n",
      "Gradient Descent(37/49): loss=1.3137027632741397e+22\n",
      "Gradient Descent(38/49): loss=6.6277519783361595e+22\n",
      "Gradient Descent(39/49): loss=3.3437621899232836e+23\n",
      "Gradient Descent(40/49): loss=1.6869589597357387e+24\n",
      "Gradient Descent(41/49): loss=8.510864021397367e+24\n",
      "Gradient Descent(42/49): loss=4.293809637316903e+25\n",
      "Gradient Descent(43/49): loss=2.1662666863392482e+26\n",
      "Gradient Descent(44/49): loss=1.0929015845414845e+27\n",
      "Gradient Descent(45/49): loss=5.513789604140417e+27\n",
      "Gradient Descent(46/49): loss=2.781757866284105e+28\n",
      "Gradient Descent(47/49): loss=1.4034225790593471e+29\n",
      "Gradient Descent(48/49): loss=7.080396749428789e+29\n",
      "Gradient Descent(49/49): loss=3.5721256645966526e+30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.422921252157338\n",
      "Gradient Descent(2/49): loss=0.39808182306590556\n",
      "Gradient Descent(3/49): loss=0.39520337419767376\n",
      "Gradient Descent(4/49): loss=0.44251839532264586\n",
      "Gradient Descent(5/49): loss=0.707653246263068\n",
      "Gradient Descent(6/49): loss=2.0190302604988237\n",
      "Gradient Descent(7/49): loss=8.412839505161369\n",
      "Gradient Descent(8/49): loss=39.527085508470144\n",
      "Gradient Descent(9/49): loss=190.89684046194165\n",
      "Gradient Descent(10/49): loss=927.27479569329\n",
      "Gradient Descent(11/49): loss=4509.55532402724\n",
      "Gradient Descent(12/49): loss=21936.367270714145\n",
      "Gradient Descent(13/49): loss=106713.01140600088\n",
      "Gradient Descent(14/49): loss=519128.0419632962\n",
      "Gradient Descent(15/49): loss=2525413.597741004\n",
      "Gradient Descent(16/49): loss=12285440.342004528\n",
      "Gradient Descent(17/49): loss=59765282.76069162\n",
      "Gradient Descent(18/49): loss=290741641.23976475\n",
      "Gradient Descent(19/49): loss=1414378019.506847\n",
      "Gradient Descent(20/49): loss=6880559575.394107\n",
      "Gradient Descent(21/49): loss=33472027580.370724\n",
      "Gradient Descent(22/49): loss=162832196727.7205\n",
      "Gradient Descent(23/49): loss=792133796725.8303\n",
      "Gradient Descent(24/49): loss=3853512785099.741\n",
      "Gradient Descent(25/49): loss=18746278528084.008\n",
      "Gradient Descent(26/49): loss=91195482732369.89\n",
      "Gradient Descent(27/49): loss=443640910292177.4\n",
      "Gradient Descent(28/49): loss=2158190859765143.8\n",
      "Gradient Descent(29/49): loss=1.0499004215157898e+16\n",
      "Gradient Descent(30/49): loss=5.1074764315280536e+16\n",
      "Gradient Descent(31/49): loss=2.484646635435457e+17\n",
      "Gradient Descent(32/49): loss=1.20871216651584e+18\n",
      "Gradient Descent(33/49): loss=5.880051837742755e+18\n",
      "Gradient Descent(34/49): loss=2.8604832955562942e+19\n",
      "Gradient Descent(35/49): loss=1.391546351961723e+20\n",
      "Gradient Descent(36/49): loss=6.769489801482709e+20\n",
      "Gradient Descent(37/49): loss=3.29317037177922e+21\n",
      "Gradient Descent(38/49): loss=1.6020366993077026e+22\n",
      "Gradient Descent(39/49): loss=7.793467376976558e+22\n",
      "Gradient Descent(40/49): loss=3.7913072641996474e+23\n",
      "Gradient Descent(41/49): loss=1.8443665798918995e+24\n",
      "Gradient Descent(42/49): loss=8.972335513777598e+24\n",
      "Gradient Descent(43/49): loss=4.3647941493556725e+25\n",
      "Gradient Descent(44/49): loss=2.1233521569713235e+26\n",
      "Gradient Descent(45/49): loss=1.0329523519866828e+27\n",
      "Gradient Descent(46/49): loss=5.025028740389141e+27\n",
      "Gradient Descent(47/49): loss=2.444538104121801e+28\n",
      "Gradient Descent(48/49): loss=1.1892004705312107e+29\n",
      "Gradient Descent(49/49): loss=5.7851328098632125e+29\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.421827494906416\n",
      "Gradient Descent(2/49): loss=0.3966070298002461\n",
      "Gradient Descent(3/49): loss=0.392698948564496\n",
      "Gradient Descent(4/49): loss=0.43503667045968236\n",
      "Gradient Descent(5/49): loss=0.6758403716513338\n",
      "Gradient Descent(6/49): loss=1.8690679837528694\n",
      "Gradient Descent(7/49): loss=7.689996259426146\n",
      "Gradient Descent(8/49): loss=36.02710841656952\n",
      "Gradient Descent(9/49): loss=173.93480166677617\n",
      "Gradient Descent(10/49): loss=845.0569524189302\n",
      "Gradient Descent(11/49): loss=4111.02191595463\n",
      "Gradient Descent(12/49): loss=20004.573731943565\n",
      "Gradient Descent(13/49): loss=97349.23095428344\n",
      "Gradient Descent(14/49): loss=473740.6050938239\n",
      "Gradient Descent(15/49): loss=2305418.071175709\n",
      "Gradient Descent(16/49): loss=11219124.832706131\n",
      "Gradient Descent(17/49): loss=54596940.899817295\n",
      "Gradient Descent(18/49): loss=265691491.91440168\n",
      "Gradient Descent(19/49): loss=1292965649.7211142\n",
      "Gradient Descent(20/49): loss=6292110299.474355\n",
      "Gradient Descent(21/49): loss=30620033901.204346\n",
      "Gradient Descent(22/49): loss=149009860208.9586\n",
      "Gradient Descent(23/49): loss=725144149457.6622\n",
      "Gradient Descent(24/49): loss=3528853974871.6733\n",
      "Gradient Descent(25/49): loss=17172875745167.773\n",
      "Gradient Descent(26/49): loss=83570378218812.78\n",
      "Gradient Descent(27/49): loss=406688327527241.1\n",
      "Gradient Descent(28/49): loss=1979115079673902.8\n",
      "Gradient Descent(29/49): loss=9631199701275844.0\n",
      "Gradient Descent(30/49): loss=4.686943606186926e+16\n",
      "Gradient Descent(31/49): loss=2.280862306765971e+17\n",
      "Gradient Descent(32/49): loss=1.1099627602855007e+18\n",
      "Gradient Descent(33/49): loss=5.401541888635581e+18\n",
      "Gradient Descent(34/49): loss=2.628615645373567e+19\n",
      "Gradient Descent(35/49): loss=1.2791940437673624e+20\n",
      "Gradient Descent(36/49): loss=6.225091920494104e+20\n",
      "Gradient Descent(37/49): loss=3.02938945091339e+21\n",
      "Gradient Descent(38/49): loss=1.4742272985707955e+22\n",
      "Gradient Descent(39/49): loss=7.17420510986514e+22\n",
      "Gradient Descent(40/49): loss=3.491267527626969e+23\n",
      "Gradient Descent(41/49): loss=1.698996441111153e+24\n",
      "Gradient Descent(42/49): loss=8.268025535328665e+24\n",
      "Gradient Descent(43/49): loss=4.0235661828778615e+25\n",
      "Gradient Descent(44/49): loss=1.9580351752451424e+26\n",
      "Gradient Descent(45/49): loss=9.528616091397429e+26\n",
      "Gradient Descent(46/49): loss=4.6370221416411844e+27\n",
      "Gradient Descent(47/49): loss=2.2565684393018036e+28\n",
      "Gradient Descent(48/49): loss=1.0981403507922007e+29\n",
      "Gradient Descent(49/49): loss=5.3440091115122515e+29\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4228062796841417\n",
      "Gradient Descent(2/49): loss=0.39760192753004425\n",
      "Gradient Descent(3/49): loss=0.3906067370845921\n",
      "Gradient Descent(4/49): loss=0.41668046031854505\n",
      "Gradient Descent(5/49): loss=0.5804933950142975\n",
      "Gradient Descent(6/49): loss=1.4143472905283527\n",
      "Gradient Descent(7/49): loss=5.564873437298381\n",
      "Gradient Descent(8/49): loss=26.16381460757064\n",
      "Gradient Descent(9/49): loss=128.35332989585908\n",
      "Gradient Descent(10/49): loss=635.2749187115671\n",
      "Gradient Descent(11/49): loss=3149.887232668894\n",
      "Gradient Descent(12/49): loss=15623.740141263166\n",
      "Gradient Descent(13/49): loss=77500.85993587544\n",
      "Gradient Descent(14/49): loss=384445.1396422897\n",
      "Gradient Descent(15/49): loss=1907056.2519876454\n",
      "Gradient Descent(16/49): loss=9460038.177558413\n",
      "Gradient Descent(17/49): loss=46926949.78383393\n",
      "Gradient Descent(18/49): loss=232783275.02062157\n",
      "Gradient Descent(19/49): loss=1154732059.9464772\n",
      "Gradient Descent(20/49): loss=5728101090.796224\n",
      "Gradient Descent(21/49): loss=28414506924.08183\n",
      "Gradient Descent(22/49): loss=140951458602.54468\n",
      "Gradient Descent(23/49): loss=699196144259.1451\n",
      "Gradient Descent(24/49): loss=3468394389065.3315\n",
      "Gradient Descent(25/49): loss=17205128685098.627\n",
      "Gradient Descent(26/49): loss=85346826186799.17\n",
      "Gradient Descent(27/49): loss=423366827036195.4\n",
      "Gradient Descent(28/49): loss=2100130470492200.5\n",
      "Gradient Descent(29/49): loss=1.0417793061317834e+16\n",
      "Gradient Descent(30/49): loss=5.167793801068125e+16\n",
      "Gradient Descent(31/49): loss=2.5635077039032675e+17\n",
      "Gradient Descent(32/49): loss=1.2716396978945224e+18\n",
      "Gradient Descent(33/49): loss=6.308026766602187e+18\n",
      "Gradient Descent(34/49): loss=3.129125471157707e+19\n",
      "Gradient Descent(35/49): loss=1.552216973156892e+20\n",
      "Gradient Descent(36/49): loss=7.69984314775625e+20\n",
      "Gradient Descent(37/49): loss=3.8195423401065757e+21\n",
      "Gradient Descent(38/49): loss=1.8947014124720477e+22\n",
      "Gradient Descent(39/49): loss=9.398752842005377e+22\n",
      "Gradient Descent(40/49): loss=4.662294248772857e+23\n",
      "Gradient Descent(41/49): loss=2.3127523435867739e+24\n",
      "Gradient Descent(42/49): loss=1.147251356813009e+25\n",
      "Gradient Descent(43/49): loss=5.69099272284409e+25\n",
      "Gradient Descent(44/49): loss=2.8230429172412618e+26\n",
      "Gradient Descent(45/49): loss=1.4003833251438646e+27\n",
      "Gradient Descent(46/49): loss=6.946665406197087e+27\n",
      "Gradient Descent(47/49): loss=3.4459250834552106e+28\n",
      "Gradient Descent(48/49): loss=1.709366866898875e+29\n",
      "Gradient Descent(49/49): loss=8.479392368919111e+29\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42559934894145696\n",
      "Gradient Descent(2/49): loss=0.40162908853594104\n",
      "Gradient Descent(3/49): loss=0.4027370187089904\n",
      "Gradient Descent(4/49): loss=0.4788332725610645\n",
      "Gradient Descent(5/49): loss=0.9256647112182629\n",
      "Gradient Descent(6/49): loss=3.3328176646520316\n",
      "Gradient Descent(7/49): loss=16.178628297725975\n",
      "Gradient Descent(8/49): loss=84.65127562584242\n",
      "Gradient Descent(9/49): loss=449.57953220758634\n",
      "Gradient Descent(10/49): loss=2394.4424270560157\n",
      "Gradient Descent(11/49): loss=12759.440751408452\n",
      "Gradient Descent(12/49): loss=67998.88489748209\n",
      "Gradient Descent(13/49): loss=362393.1437660525\n",
      "Gradient Descent(14/49): loss=1931344.102307401\n",
      "Gradient Descent(15/49): loss=10292944.346293634\n",
      "Gradient Descent(16/49): loss=54855432.57714831\n",
      "Gradient Descent(17/49): loss=292347694.74131185\n",
      "Gradient Descent(18/49): loss=1558043952.4402924\n",
      "Gradient Descent(19/49): loss=8303472212.517485\n",
      "Gradient Descent(20/49): loss=44252699473.85358\n",
      "Gradient Descent(21/49): loss=235841267443.04523\n",
      "Gradient Descent(22/49): loss=1256897411700.251\n",
      "Gradient Descent(23/49): loss=6698535505122.688\n",
      "Gradient Descent(24/49): loss=35699316026675.805\n",
      "Gradient Descent(25/49): loss=190256685778242.66\n",
      "Gradient Descent(26/49): loss=1013957983292259.0\n",
      "Gradient Descent(27/49): loss=5403809005064042.0\n",
      "Gradient Descent(28/49): loss=2.8799173382309156e+16\n",
      "Gradient Descent(29/49): loss=1.534829204228111e+17\n",
      "Gradient Descent(30/49): loss=8.179751046599638e+17\n",
      "Gradient Descent(31/49): loss=4.359333729123249e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=2.3232724875863503e+19\n",
      "Gradient Descent(33/49): loss=1.2381697266066919e+20\n",
      "Gradient Descent(34/49): loss=6.598727786244232e+20\n",
      "Gradient Descent(35/49): loss=3.516739867020114e+21\n",
      "Gradient Descent(36/49): loss=1.8742187422960956e+22\n",
      "Gradient Descent(37/49): loss=9.988500789938818e+22\n",
      "Gradient Descent(38/49): loss=5.323292622097034e+23\n",
      "Gradient Descent(39/49): loss=2.8370067677239785e+24\n",
      "Gradient Descent(40/49): loss=1.5119603545184724e+25\n",
      "Gradient Descent(41/49): loss=8.057873317904292e+25\n",
      "Gradient Descent(42/49): loss=4.294379956018773e+26\n",
      "Gradient Descent(43/49): loss=2.288655887115973e+27\n",
      "Gradient Descent(44/49): loss=1.2197210827349921e+28\n",
      "Gradient Descent(45/49): loss=6.500407195521925e+28\n",
      "Gradient Descent(46/49): loss=3.4643406845803026e+29\n",
      "Gradient Descent(47/49): loss=1.846293011783411e+30\n",
      "Gradient Descent(48/49): loss=9.839672814318363e+30\n",
      "Gradient Descent(49/49): loss=5.243975927705738e+31\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4259920739132424\n",
      "Gradient Descent(2/49): loss=0.4014231794387305\n",
      "Gradient Descent(3/49): loss=0.39992384212509224\n",
      "Gradient Descent(4/49): loss=0.46107788466845034\n",
      "Gradient Descent(5/49): loss=0.8163383300258716\n",
      "Gradient Descent(6/49): loss=2.6682434127525165\n",
      "Gradient Descent(7/49): loss=12.204923136978532\n",
      "Gradient Descent(8/49): loss=61.23970804253952\n",
      "Gradient Descent(9/49): loss=313.3097130909824\n",
      "Gradient Descent(10/49): loss=1609.0721977658295\n",
      "Gradient Descent(11/49): loss=8269.893621164796\n",
      "Gradient Descent(12/49): loss=42509.589829524644\n",
      "Gradient Descent(13/49): loss=218517.41347136482\n",
      "Gradient Descent(14/49): loss=1123278.7561453127\n",
      "Gradient Descent(15/49): loss=5774169.087643118\n",
      "Gradient Descent(16/49): loss=29681889.1096643\n",
      "Gradient Descent(17/49): loss=152578589.71458504\n",
      "Gradient Descent(18/49): loss=784324277.1124146\n",
      "Gradient Descent(19/49): loss=4031788298.364826\n",
      "Gradient Descent(20/49): loss=20725250208.274475\n",
      "Gradient Descent(21/49): loss=106537338876.19781\n",
      "Gradient Descent(22/49): loss=547651027653.9755\n",
      "Gradient Descent(23/49): loss=2815178708749.5234\n",
      "Gradient Descent(24/49): loss=14471316152093.768\n",
      "Gradient Descent(25/49): loss=74389235227941.64\n",
      "Gradient Descent(26/49): loss=382394957005877.4\n",
      "Gradient Descent(27/49): loss=1965686334796503.2\n",
      "Gradient Descent(28/49): loss=1.010453379683652e+16\n",
      "Gradient Descent(29/49): loss=5.194196115830576e+16\n",
      "Gradient Descent(30/49): loss=2.6700562175521136e+17\n",
      "Gradient Descent(31/49): loss=1.3725319656608358e+18\n",
      "Gradient Descent(32/49): loss=7.055446939195254e+18\n",
      "Gradient Descent(33/49): loss=3.6268249306553782e+19\n",
      "Gradient Descent(34/49): loss=1.8643551841555392e+20\n",
      "Gradient Descent(35/49): loss=9.583644976377794e+20\n",
      "Gradient Descent(36/49): loss=4.926435252993418e+21\n",
      "Gradient Descent(37/49): loss=2.5324147922588144e+22\n",
      "Gradient Descent(38/49): loss=1.301777928808595e+23\n",
      "Gradient Descent(39/49): loss=6.691738577398156e+23\n",
      "Gradient Descent(40/49): loss=3.439862068426704e+24\n",
      "Gradient Descent(41/49): loss=1.7682476553652047e+25\n",
      "Gradient Descent(42/49): loss=9.08960797993422e+25\n",
      "Gradient Descent(43/49): loss=4.6724774653707076e+26\n",
      "Gradient Descent(44/49): loss=2.4018687838455848e+27\n",
      "Gradient Descent(45/49): loss=1.2346712632789493e+28\n",
      "Gradient Descent(46/49): loss=6.346779385367294e+28\n",
      "Gradient Descent(47/49): loss=3.262537143656039e+29\n",
      "Gradient Descent(48/49): loss=1.6770944706658754e+30\n",
      "Gradient Descent(49/49): loss=8.621038595705192e+30\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4249742247673374\n",
      "Gradient Descent(2/49): loss=0.39997278121873664\n",
      "Gradient Descent(3/49): loss=0.3972096879120263\n",
      "Gradient Descent(4/49): loss=0.4520384870880303\n",
      "Gradient Descent(5/49): loss=0.7747680710971365\n",
      "Gradient Descent(6/49): loss=2.4598596666822554\n",
      "Gradient Descent(7/49): loss=11.141900046475023\n",
      "Gradient Descent(8/49): loss=55.79876086114046\n",
      "Gradient Descent(9/49): loss=285.4433733791136\n",
      "Gradient Descent(10/49): loss=1466.3362405847247\n",
      "Gradient Descent(11/49): loss=7538.769048474621\n",
      "Gradient Descent(12/49): loss=38764.64521886284\n",
      "Gradient Descent(13/49): loss=199335.41954231978\n",
      "Gradient Descent(14/49): loss=1025027.9635817246\n",
      "Gradient Descent(15/49): loss=5270932.47733574\n",
      "Gradient Descent(16/49): loss=27104368.254892416\n",
      "Gradient Descent(17/49): loss=139377010.34470654\n",
      "Gradient Descent(18/49): loss=716709240.146344\n",
      "Gradient Descent(19/49): loss=3685486828.015905\n",
      "Gradient Descent(20/49): loss=18951636734.9111\n",
      "Gradient Descent(21/49): loss=97453756237.58029\n",
      "Gradient Descent(22/49): loss=501130046854.12787\n",
      "Gradient Descent(23/49): loss=2576928109865.512\n",
      "Gradient Descent(24/49): loss=13251168085219.719\n",
      "Gradient Descent(25/49): loss=68140610888803.266\n",
      "Gradient Descent(26/49): loss=350394985743044.9\n",
      "Gradient Descent(27/49): loss=1801813110161658.0\n",
      "Gradient Descent(28/49): loss=9265345156311978.0\n",
      "Gradient Descent(29/49): loss=4.764457555639388e+16\n",
      "Gradient Descent(30/49): loss=2.4499957008104118e+17\n",
      "Gradient Descent(31/49): loss=1.2598451899072376e+18\n",
      "Gradient Descent(32/49): loss=6.478419133582145e+18\n",
      "Gradient Descent(33/49): loss=3.3313549003154457e+19\n",
      "Gradient Descent(34/49): loss=1.713060739513932e+20\n",
      "Gradient Descent(35/49): loss=8.80895967279321e+20\n",
      "Gradient Descent(36/49): loss=4.529773447432604e+21\n",
      "Gradient Descent(37/49): loss=2.329315633994663e+22\n",
      "Gradient Descent(38/49): loss=1.1977886721569194e+23\n",
      "Gradient Descent(39/49): loss=6.159309980189549e+23\n",
      "Gradient Descent(40/49): loss=3.1672614972844603e+24\n",
      "Gradient Descent(41/49): loss=1.6286800671577623e+25\n",
      "Gradient Descent(42/49): loss=8.37505448612739e+25\n",
      "Gradient Descent(43/49): loss=4.306649234555128e+26\n",
      "Gradient Descent(44/49): loss=2.2145799361922064e+27\n",
      "Gradient Descent(45/49): loss=1.1387888882229194e+28\n",
      "Gradient Descent(46/49): loss=5.855919268237499e+28\n",
      "Gradient Descent(47/49): loss=3.0112508851072046e+29\n",
      "Gradient Descent(48/49): loss=1.5484557552289632e+30\n",
      "Gradient Descent(49/49): loss=7.962522278566068e+30\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42579480296379313\n",
      "Gradient Descent(2/49): loss=0.400696379197562\n",
      "Gradient Descent(3/49): loss=0.39418073804126097\n",
      "Gradient Descent(4/49): loss=0.4283820793979023\n",
      "Gradient Descent(5/49): loss=0.6479990349615696\n",
      "Gradient Descent(6/49): loss=1.8242984607656296\n",
      "Gradient Descent(7/49): loss=8.005912973839271\n",
      "Gradient Descent(8/49): loss=40.414577183574835\n",
      "Gradient Descent(9/49): loss=210.272118477743\n",
      "Gradient Descent(10/49): loss=1100.4765518534605\n",
      "Gradient Descent(11/49): loss=5765.909692409039\n",
      "Gradient Descent(12/49): loss=30216.749881861888\n",
      "Gradient Descent(13/49): loss=158359.94358438655\n",
      "Gradient Descent(14/49): loss=829939.2272830406\n",
      "Gradient Descent(15/49): loss=4349585.645916359\n",
      "Gradient Descent(16/49): loss=22795525.248855487\n",
      "Gradient Descent(17/49): loss=119467931.31942128\n",
      "Gradient Descent(18/49): loss=626113529.0236574\n",
      "Gradient Descent(19/49): loss=3281367205.912328\n",
      "Gradient Descent(20/49): loss=17197153942.39156\n",
      "Gradient Descent(21/49): loss=90127707501.42627\n",
      "Gradient Descent(22/49): loss=472345812963.21655\n",
      "Gradient Descent(23/49): loss=2475493643516.0464\n",
      "Gradient Descent(24/49): loss=12973691331457.82\n",
      "Gradient Descent(25/49): loss=67993172676822.72\n",
      "Gradient Descent(26/49): loss=356342031928143.9\n",
      "Gradient Descent(27/49): loss=1867535205662815.8\n",
      "Gradient Descent(28/49): loss=9787472237048250.0\n",
      "Gradient Descent(29/49): loss=5.1294675731156536e+16\n",
      "Gradient Descent(30/49): loss=2.6882771104114115e+17\n",
      "Gradient Descent(31/49): loss=1.4088857604323768e+18\n",
      "Gradient Descent(32/49): loss=7.383759204962825e+18\n",
      "Gradient Descent(33/49): loss=3.869717583073663e+19\n",
      "Gradient Descent(34/49): loss=2.0280610129708027e+20\n",
      "Gradient Descent(35/49): loss=1.0628763944745735e+21\n",
      "Gradient Descent(36/49): loss=5.570375953711721e+21\n",
      "Gradient Descent(37/49): loss=2.9193505874243926e+22\n",
      "Gradient Descent(38/49): loss=1.5299879080183443e+23\n",
      "Gradient Descent(39/49): loss=8.018437418123368e+23\n",
      "Gradient Descent(40/49): loss=4.202342926463832e+24\n",
      "Gradient Descent(41/49): loss=2.2023849723745124e+25\n",
      "Gradient Descent(42/49): loss=1.1542369700472704e+26\n",
      "Gradient Descent(43/49): loss=6.0491830435415657e+26\n",
      "Gradient Descent(44/49): loss=3.170286210185408e+27\n",
      "Gradient Descent(45/49): loss=1.6614995086357816e+28\n",
      "Gradient Descent(46/49): loss=8.707670015179667e+28\n",
      "Gradient Descent(47/49): loss=4.563559405173486e+29\n",
      "Gradient Descent(48/49): loss=2.3916931174748509e+30\n",
      "Gradient Descent(49/49): loss=1.2534505328651441e+31\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4287966888719343\n",
      "Gradient Descent(2/49): loss=0.4053725778083403\n",
      "Gradient Descent(3/49): loss=0.408712672558563\n",
      "Gradient Descent(4/49): loss=0.5054243200678121\n",
      "Gradient Descent(5/49): loss=1.0970067949145375\n",
      "Gradient Descent(6/49): loss=4.4533455660944545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=23.341650982750128\n",
      "Gradient Descent(8/49): loss=129.53727981504073\n",
      "Gradient Descent(9/49): loss=726.5308056229683\n",
      "Gradient Descent(10/49): loss=4082.5633872956832\n",
      "Gradient Descent(11/49): loss=22948.65210103227\n",
      "Gradient Descent(12/49): loss=129005.1940385998\n",
      "Gradient Descent(13/49): loss=725206.6565009998\n",
      "Gradient Descent(14/49): loss=4076779.0870821048\n",
      "Gradient Descent(15/49): loss=22917789.14249019\n",
      "Gradient Descent(16/49): loss=128833345.86201712\n",
      "Gradient Descent(17/49): loss=724242250.2702632\n",
      "Gradient Descent(18/49): loss=4071359278.475983\n",
      "Gradient Descent(19/49): loss=22887323093.521175\n",
      "Gradient Descent(20/49): loss=128662081282.62117\n",
      "Gradient Descent(21/49): loss=723279480632.452\n",
      "Gradient Descent(22/49): loss=4065947028758.159\n",
      "Gradient Descent(23/49): loss=22856897898191.062\n",
      "Gradient Descent(24/49): loss=128491044726652.23\n",
      "Gradient Descent(25/49): loss=722317991202704.0\n",
      "Gradient Descent(26/49): loss=4060541974151111.0\n",
      "Gradient Descent(27/49): loss=2.2826513148855784e+16\n",
      "Gradient Descent(28/49): loss=1.2832023553796554e+17\n",
      "Gradient Descent(29/49): loss=7.213577799263782e+17\n",
      "Gradient Descent(30/49): loss=4.0551441047376364e+18\n",
      "Gradient Descent(31/49): loss=2.2796168791396348e+19\n",
      "Gradient Descent(32/49): loss=1.2814965341396515e+20\n",
      "Gradient Descent(33/49): loss=7.203988451040605e+20\n",
      "Gradient Descent(34/49): loss=4.049753410965625e+21\n",
      "Gradient Descent(35/49): loss=2.2765864772116505e+22\n",
      "Gradient Descent(36/49): loss=1.2797929805279644e+23\n",
      "Gradient Descent(37/49): loss=7.194411850388753e+23\n",
      "Gradient Descent(38/49): loss=4.0443698832963827e+24\n",
      "Gradient Descent(39/49): loss=2.273560103739526e+25\n",
      "Gradient Descent(40/49): loss=1.2780916915302073e+26\n",
      "Gradient Descent(41/49): loss=7.184847980362474e+26\n",
      "Gradient Descent(42/49): loss=4.0389935122036185e+27\n",
      "Gradient Descent(43/49): loss=2.2705377533680615e+28\n",
      "Gradient Descent(44/49): loss=1.2763926641360061e+29\n",
      "Gradient Descent(45/49): loss=7.175296824038969e+29\n",
      "Gradient Descent(46/49): loss=4.0336242881741184e+30\n",
      "Gradient Descent(47/49): loss=2.2675194207492272e+31\n",
      "Gradient Descent(48/49): loss=1.2746958953389053e+32\n",
      "Gradient Descent(49/49): loss=7.165758364516971e+32\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4292347109660624\n",
      "Gradient Descent(2/49): loss=0.4052760464634497\n",
      "Gradient Descent(3/49): loss=0.4056418431755868\n",
      "Gradient Descent(4/49): loss=0.4838543627318987\n",
      "Gradient Descent(5/49): loss=0.9554493280398834\n",
      "Gradient Descent(6/49): loss=3.5439207868761238\n",
      "Gradient Descent(7/49): loss=17.603791753079935\n",
      "Gradient Descent(8/49): loss=93.87591685804915\n",
      "Gradient Descent(9/49): loss=507.57068438822296\n",
      "Gradient Descent(10/49): loss=2751.3749533975597\n",
      "Gradient Descent(11/49): loss=14921.321249794953\n",
      "Gradient Descent(12/49): loss=80928.65338484844\n",
      "Gradient Descent(13/49): loss=438939.07857116533\n",
      "Gradient Descent(14/49): loss=2380715.2319476525\n",
      "Gradient Descent(15/49): loss=12912516.431023529\n",
      "Gradient Descent(16/49): loss=70034876.43321706\n",
      "Gradient Descent(17/49): loss=379855006.0058888\n",
      "Gradient Descent(18/49): loss=2060256738.0831373\n",
      "Gradient Descent(19/49): loss=11174415928.001427\n",
      "Gradient Descent(20/49): loss=60607772341.220604\n",
      "Gradient Descent(21/49): loss=328724301288.76306\n",
      "Gradient Descent(22/49): loss=1782934136728.0989\n",
      "Gradient Descent(23/49): loss=9670274219003.445\n",
      "Gradient Descent(24/49): loss=52449611875366.96\n",
      "Gradient Descent(25/49): loss=284476088637772.5\n",
      "Gradient Descent(26/49): loss=1542940779027018.2\n",
      "Gradient Descent(27/49): loss=8368598777438454.0\n",
      "Gradient Descent(28/49): loss=4.5389587500505064e+16\n",
      "Gradient Descent(29/49): loss=2.4618394408156522e+17\n",
      "Gradient Descent(30/49): loss=1.3352519302556383e+18\n",
      "Gradient Descent(31/49): loss=7.24213645980377e+18\n",
      "Gradient Descent(32/49): loss=3.927988367886414e+19\n",
      "Gradient Descent(33/49): loss=2.1304614603559895e+20\n",
      "Gradient Descent(34/49): loss=1.1555192146621581e+21\n",
      "Gradient Descent(35/49): loss=6.267302555335965e+21\n",
      "Gradient Descent(36/49): loss=3.3992581708480367e+22\n",
      "Gradient Descent(37/49): loss=1.8436888932765688e+23\n",
      "Gradient Descent(38/49): loss=9.999795732912408e+23\n",
      "Gradient Descent(39/49): loss=5.423686993214016e+24\n",
      "Gradient Descent(40/49): loss=2.9416981492473554e+25\n",
      "Gradient Descent(41/49): loss=1.5955175901766132e+26\n",
      "Gradient Descent(42/49): loss=8.653764769217979e+26\n",
      "Gradient Descent(43/49): loss=4.693627017466224e+27\n",
      "Gradient Descent(44/49): loss=2.5457283814151817e+28\n",
      "Gradient Descent(45/49): loss=1.380751595264488e+29\n",
      "Gradient Descent(46/49): loss=7.488917442031055e+29\n",
      "Gradient Descent(47/49): loss=4.0618373823291175e+30\n",
      "Gradient Descent(48/49): loss=2.2030584591424476e+31\n",
      "Gradient Descent(49/49): loss=1.1948943587731708e+32\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42829642811001617\n",
      "Gradient Descent(2/49): loss=0.40385491976779336\n",
      "Gradient Descent(3/49): loss=0.40268924544965407\n",
      "Gradient Descent(4/49): loss=0.4729305069014199\n",
      "Gradient Descent(5/49): loss=0.9014143716393275\n",
      "Gradient Descent(6/49): loss=3.256746062190639\n",
      "Gradient Descent(7/49): loss=16.056375057257814\n",
      "Gradient Descent(8/49): loss=85.5168793384484\n",
      "Gradient Descent(9/49): loss=462.39566525435964\n",
      "Gradient Descent(10/49): loss=2507.2169491295417\n",
      "Gradient Descent(11/49): loss=13601.714861779152\n",
      "Gradient Descent(12/49): loss=73796.62326877644\n",
      "Gradient Descent(13/49): loss=400393.35616599646\n",
      "Gradient Descent(14/49): loss=2172394.1414901903\n",
      "Gradient Descent(15/49): loss=11786656.81681983\n",
      "Gradient Descent(16/49): loss=63950316.97839921\n",
      "Gradient Descent(17/49): loss=346972274.38262516\n",
      "Gradient Descent(18/49): loss=1882551413.6613634\n",
      "Gradient Descent(19/49): loss=10214072100.65398\n",
      "Gradient Descent(20/49): loss=55418018404.83583\n",
      "Gradient Descent(21/49): loss=300678978347.17896\n",
      "Gradient Descent(22/49): loss=1631380020841.677\n",
      "Gradient Descent(23/49): loss=8851303097519.375\n",
      "Gradient Descent(24/49): loss=48024105679402.91\n",
      "Gradient Descent(25/49): loss=260562168179836.44\n",
      "Gradient Descent(26/49): loss=1413720100064179.5\n",
      "Gradient Descent(27/49): loss=7670355736163661.0\n",
      "Gradient Descent(28/49): loss=4.161669422159993e+16\n",
      "Gradient Descent(29/49): loss=2.2579777229476986e+17\n",
      "Gradient Descent(30/49): loss=1.2251005258081572e+18\n",
      "Gradient Descent(31/49): loss=6.646971239273767e+18\n",
      "Gradient Descent(32/49): loss=3.606416430732254e+19\n",
      "Gradient Descent(33/49): loss=1.956716676462841e+20\n",
      "Gradient Descent(34/49): loss=1.061646713707534e+21\n",
      "Gradient Descent(35/49): loss=5.760127453727388e+21\n",
      "Gradient Descent(36/49): loss=3.1252457013044316e+22\n",
      "Gradient Descent(37/49): loss=1.6956501001035146e+23\n",
      "Gradient Descent(38/49): loss=9.200010292889653e+23\n",
      "Gradient Descent(39/49): loss=4.991607017515517e+24\n",
      "Gradient Descent(40/49): loss=2.7082731240602568e+25\n",
      "Gradient Descent(41/49): loss=1.469415218139046e+26\n",
      "Gradient Descent(42/49): loss=7.972538161371326e+26\n",
      "Gradient Descent(43/49): loss=4.3256231424511634e+27\n",
      "Gradient Descent(44/49): loss=2.346933334376192e+28\n",
      "Gradient Descent(45/49): loss=1.2733647603163902e+29\n",
      "Gradient Descent(46/49): loss=6.908836263329721e+29\n",
      "Gradient Descent(47/49): loss=3.7484953252233326e+30\n",
      "Gradient Descent(48/49): loss=2.033803764868603e+31\n",
      "Gradient Descent(49/49): loss=1.1034714985931783e+32\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4289521193717881\n",
      "Gradient Descent(2/49): loss=0.40427514230119105\n",
      "Gradient Descent(3/49): loss=0.3985731340651831\n",
      "Gradient Descent(4/49): loss=0.44284163576213786\n",
      "Gradient Descent(5/49): loss=0.734442277993821\n",
      "Gradient Descent(6/49): loss=2.3768537195913093\n",
      "Gradient Descent(7/49): loss=11.47773698582341\n",
      "Gradient Descent(8/49): loss=61.80928386544559\n",
      "Gradient Descent(9/49): loss=340.09537594856096\n",
      "Gradient Descent(10/49): loss=1878.707201190717\n",
      "Gradient Descent(11/49): loss=10385.479427303342\n",
      "Gradient Descent(12/49): loss=57418.22072861271\n",
      "Gradient Descent(13/49): loss=317455.578742132\n",
      "Gradient Descent(14/49): loss=1755165.244523965\n",
      "Gradient Descent(15/49): loss=9704058.032002134\n",
      "Gradient Descent(16/49): loss=53652358.606801584\n",
      "Gradient Descent(17/49): loss=296636277.8746279\n",
      "Gradient Descent(18/49): loss=1640059897.2233167\n",
      "Gradient Descent(19/49): loss=9067658507.29081\n",
      "Gradient Descent(20/49): loss=50133797525.31339\n",
      "Gradient Descent(21/49): loss=277182654414.49506\n",
      "Gradient Descent(22/49): loss=1532503574456.6628\n",
      "Gradient Descent(23/49): loss=8472994858518.362\n",
      "Gradient Descent(24/49): loss=46845986573273.88\n",
      "Gradient Descent(25/49): loss=259004814079060.72\n",
      "Gradient Descent(26/49): loss=1432000873995951.8\n",
      "Gradient Descent(27/49): loss=7917329685228197.0\n",
      "Gradient Descent(28/49): loss=4.377379265815332e+16\n",
      "Gradient Descent(29/49): loss=2.4201908975120134e+17\n",
      "Gradient Descent(30/49): loss=1.338089213823e+18\n",
      "Gradient Descent(31/49): loss=7.398105438666307e+18\n",
      "Gradient Descent(32/49): loss=4.090307545731719e+19\n",
      "Gradient Descent(33/49): loss=2.2614730159463026e+20\n",
      "Gradient Descent(34/49): loss=1.2503363487154074e+21\n",
      "Gradient Descent(35/49): loss=6.912932296319425e+21\n",
      "Gradient Descent(36/49): loss=3.822062198111308e+22\n",
      "Gradient Descent(37/49): loss=2.113163968640212e+23\n",
      "Gradient Descent(38/49): loss=1.1683383804077627e+24\n",
      "Gradient Descent(39/49): loss=6.459577161975697e+24\n",
      "Gradient Descent(40/49): loss=3.571408575737765e+25\n",
      "Gradient Descent(41/49): loss=1.9745811366625013e+26\n",
      "Gradient Descent(42/49): loss=1.091717898576777e+27\n",
      "Gradient Descent(43/49): loss=6.035953387498661e+27\n",
      "Gradient Descent(44/49): loss=3.337193000458349e+28\n",
      "Gradient Descent(45/49): loss=1.8450866677291677e+29\n",
      "Gradient Descent(46/49): loss=1.0201222437432669e+30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=5.640111168656388e+30\n",
      "Gradient Descent(48/49): loss=3.1183374531736647e+31\n",
      "Gradient Descent(49/49): loss=1.7240845403729934e+32\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43216444086906003\n",
      "Gradient Descent(2/49): loss=0.40965783098911285\n",
      "Gradient Descent(3/49): loss=0.4158659805064299\n",
      "Gradient Descent(4/49): loss=0.5377674241351986\n",
      "Gradient Descent(5/49): loss=1.3145055220919601\n",
      "Gradient Descent(6/49): loss=5.95038402994601\n",
      "Gradient Descent(7/49): loss=33.42636724327595\n",
      "Gradient Descent(8/49): loss=196.14159618197144\n",
      "Gradient Descent(9/49): loss=1159.6656246847883\n",
      "Gradient Descent(10/49): loss=6865.143409766792\n",
      "Gradient Descent(11/49): loss=40649.90729522075\n",
      "Gradient Descent(12/49): loss=240705.0325138099\n",
      "Gradient Descent(13/49): loss=1425323.4117074555\n",
      "Gradient Descent(14/49): loss=8439993.486804329\n",
      "Gradient Descent(15/49): loss=49977080.1196471\n",
      "Gradient Descent(16/49): loss=295937267.4674891\n",
      "Gradient Descent(17/49): loss=1752380621.1984584\n",
      "Gradient Descent(18/49): loss=10376651343.618614\n",
      "Gradient Descent(19/49): loss=61444923448.436\n",
      "Gradient Descent(20/49): loss=363843642100.86633\n",
      "Gradient Descent(21/49): loss=2154485488273.8293\n",
      "Gradient Descent(22/49): loss=12757699137968.488\n",
      "Gradient Descent(23/49): loss=75544202168352.6\n",
      "Gradient Descent(24/49): loss=447331953790058.5\n",
      "Gradient Descent(25/49): loss=2648858166980013.5\n",
      "Gradient Descent(26/49): loss=1.5685107064964172e+16\n",
      "Gradient Descent(27/49): loss=9.287873043043366e+16\n",
      "Gradient Descent(28/49): loss=5.499776654784965e+17\n",
      "Gradient Descent(29/49): loss=3.256670619025399e+18\n",
      "Gradient Descent(30/49): loss=1.9284244045794406e+19\n",
      "Gradient Descent(31/49): loss=1.1419087525929537e+20\n",
      "Gradient Descent(32/49): loss=6.761766736367488e+20\n",
      "Gradient Descent(33/49): loss=4.003952968503362e+21\n",
      "Gradient Descent(34/49): loss=2.3709246413015676e+22\n",
      "Gradient Descent(35/49): loss=1.4039334874685203e+23\n",
      "Gradient Descent(36/49): loss=8.313335661961268e+23\n",
      "Gradient Descent(37/49): loss=4.922708265407447e+24\n",
      "Gradient Descent(38/49): loss=2.9149618939594998e+25\n",
      "Gradient Descent(39/49): loss=1.726082957819312e+26\n",
      "Gradient Descent(40/49): loss=1.0220930789689473e+27\n",
      "Gradient Descent(41/49): loss=6.052283045514884e+27\n",
      "Gradient Descent(42/49): loss=3.5838350554118968e+28\n",
      "Gradient Descent(43/49): loss=2.1221535093136365e+29\n",
      "Gradient Descent(44/49): loss=1.2566246625361883e+30\n",
      "Gradient Descent(45/49): loss=7.441052381761366e+30\n",
      "Gradient Descent(46/49): loss=4.406189230471432e+31\n",
      "Gradient Descent(47/49): loss=2.6091072255194466e+32\n",
      "Gradient Descent(48/49): loss=1.5449723464394396e+33\n",
      "Gradient Descent(49/49): loss=9.148491590978553e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43264916331579795\n",
      "Gradient Descent(2/49): loss=0.4096858566320663\n",
      "Gradient Descent(3/49): loss=0.4125186540607872\n",
      "Gradient Descent(4/49): loss=0.5116588782768066\n",
      "Gradient Descent(5/49): loss=1.1323519923350376\n",
      "Gradient Descent(6/49): loss=4.715749480410189\n",
      "Gradient Descent(7/49): loss=25.21880770455705\n",
      "Gradient Descent(8/49): loss=142.40578539620438\n",
      "Gradient Descent(9/49): loss=812.1107905263151\n",
      "Gradient Descent(10/49): loss=4639.306642715998\n",
      "Gradient Descent(11/49): loss=26510.72423930088\n",
      "Gradient Descent(12/49): loss=151500.0892672255\n",
      "Gradient Descent(13/49): loss=865781.2165362054\n",
      "Gradient Descent(14/49): loss=4947708.71669922\n",
      "Gradient Descent(15/49): loss=28274843.467406053\n",
      "Gradient Descent(16/49): loss=161583241.4255937\n",
      "Gradient Descent(17/49): loss=923405435.0154653\n",
      "Gradient Descent(18/49): loss=5277017536.985688\n",
      "Gradient Descent(19/49): loss=30156757841.08705\n",
      "Gradient Descent(20/49): loss=172337885397.07318\n",
      "Gradient Descent(21/49): loss=984865379092.7437\n",
      "Gradient Descent(22/49): loss=5628244844144.443\n",
      "Gradient Descent(23/49): loss=32163928896382.008\n",
      "Gradient Descent(24/49): loss=183808336470621.56\n",
      "Gradient Descent(25/49): loss=1050415969545857.5\n",
      "Gradient Descent(26/49): loss=6002849110455708.0\n",
      "Gradient Descent(27/49): loss=3.4304693081234756e+16\n",
      "Gradient Descent(28/49): loss=1.960422369018016e+17\n",
      "Gradient Descent(29/49): loss=1.1203294709109307e+18\n",
      "Gradient Descent(30/49): loss=6.402386257305753e+18\n",
      "Gradient Descent(31/49): loss=3.6587942076010627e+19\n",
      "Gradient Descent(32/49): loss=2.0909040029097784e+20\n",
      "Gradient Descent(33/49): loss=1.1948962694599813e+21\n",
      "Gradient Descent(34/49): loss=6.828515765345644e+21\n",
      "Gradient Descent(35/49): loss=3.902315937319608e+22\n",
      "Gradient Descent(36/49): loss=2.230070222864525e+23\n",
      "Gradient Descent(37/49): loss=1.274426078971665e+24\n",
      "Gradient Descent(38/49): loss=7.283007566806098e+24\n",
      "Gradient Descent(39/49): loss=4.162045966679688e+25\n",
      "Gradient Descent(40/49): loss=2.3784990568602942e+26\n",
      "Gradient Descent(41/49): loss=1.3592492271291783e+27\n",
      "Gradient Descent(42/49): loss=7.767749396924529e+27\n",
      "Gradient Descent(43/49): loss=4.439063086381858e+28\n",
      "Gradient Descent(44/49): loss=2.53680700521576e+29\n",
      "Gradient Descent(45/49): loss=1.4497180275392067e+30\n",
      "Gradient Descent(46/49): loss=8.284754634668701e+30\n",
      "Gradient Descent(47/49): loss=4.734517889190557e+31\n",
      "Gradient Descent(48/49): loss=2.7056516012271165e+32\n",
      "Gradient Descent(49/49): loss=1.5462082430687844e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4317941049344521\n",
      "Gradient Descent(2/49): loss=0.40829953895911797\n",
      "Gradient Descent(3/49): loss=0.4092963923736218\n",
      "Gradient Descent(4/49): loss=0.4984673590824541\n",
      "Gradient Descent(5/49): loss=1.0624973264271655\n",
      "Gradient Descent(6/49): loss=4.32315012841747\n",
      "Gradient Descent(7/49): loss=22.988064286949246\n",
      "Gradient Descent(8/49): loss=129.70668491206257\n",
      "Gradient Descent(9/49): loss=739.7948271507472\n",
      "Gradient Descent(10/49): loss=4227.479858318806\n",
      "Gradient Descent(11/49): loss=24165.450353934644\n",
      "Gradient Descent(12/49): loss=138144.36389803488\n",
      "Gradient Descent(13/49): loss=789724.8359037513\n",
      "Gradient Descent(14/49): loss=4514598.990812773\n",
      "Gradient Descent(15/49): loss=25808495.993553262\n",
      "Gradient Descent(16/49): loss=147538796.3098929\n",
      "Gradient Descent(17/49): loss=843431427.5642068\n",
      "Gradient Descent(18/49): loss=4821623816.596411\n",
      "Gradient Descent(19/49): loss=27563658971.68112\n",
      "Gradient Descent(20/49): loss=157572495251.4369\n",
      "Gradient Descent(21/49): loss=900790830618.3201\n",
      "Gradient Descent(22/49): loss=5149528915134.924\n",
      "Gradient Descent(23/49): loss=29438186032173.164\n",
      "Gradient Descent(24/49): loss=168288558263624.28\n",
      "Gradient Descent(25/49): loss=962051086011089.9\n",
      "Gradient Descent(26/49): loss=5499733919196364.0\n",
      "Gradient Descent(27/49): loss=3.144019441563271e+16\n",
      "Gradient Descent(28/49): loss=1.7973339063596858e+17\n",
      "Gradient Descent(29/49): loss=1.027477479383464e+18\n",
      "Gradient Descent(30/49): loss=5.873755382373213e+18\n",
      "Gradient Descent(31/49): loss=3.3578353768553026e+19\n",
      "Gradient Descent(32/49): loss=1.919565539262444e+20\n",
      "Gradient Descent(33/49): loss=1.0973533380825961e+21\n",
      "Gradient Descent(34/49): loss=6.273212995184788e+21\n",
      "Gradient Descent(35/49): loss=3.5861923336121425e+22\n",
      "Gradient Descent(36/49): loss=2.0501098023501065e+23\n",
      "Gradient Descent(37/49): loss=1.1719812577533258e+24\n",
      "Gradient Descent(38/49): loss=6.699836598754348e+24\n",
      "Gradient Descent(39/49): loss=3.8300792058790055e+25\n",
      "Gradient Descent(40/49): loss=2.1895320142634727e+26\n",
      "Gradient Descent(41/49): loss=1.2516844127207175e+27\n",
      "Gradient Descent(42/49): loss=7.155473675844202e+27\n",
      "Gradient Descent(43/49): loss=4.090552139608933e+28\n",
      "Gradient Descent(44/49): loss=2.3384359393769372e+29\n",
      "Gradient Descent(45/49): loss=1.3368079554884577e+30\n",
      "Gradient Descent(46/49): loss=7.64209735133222e+30\n",
      "Gradient Descent(47/49): loss=4.368739106276348e+31\n",
      "Gradient Descent(48/49): loss=2.4974664023851595e+32\n",
      "Gradient Descent(49/49): loss=1.427720511412996e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43227822890812706\n",
      "Gradient Descent(2/49): loss=0.40838176293591116\n",
      "Gradient Descent(3/49): loss=0.4039232001804452\n",
      "Gradient Descent(4/49): loss=0.46061097842496657\n",
      "Gradient Descent(5/49): loss=0.8444221413944585\n",
      "Gradient Descent(6/49): loss=3.115777846294558\n",
      "Gradient Descent(7/49): loss=16.369962628967436\n",
      "Gradient Descent(8/49): loss=93.5870559894207\n",
      "Gradient Descent(9/49): loss=543.3556898709012\n",
      "Gradient Descent(10/49): loss=3163.0733718433985\n",
      "Gradient Descent(11/49): loss=18421.80780546735\n",
      "Gradient Descent(12/49): loss=107297.37199037752\n",
      "Gradient Descent(13/49): loss=624959.275500042\n",
      "Gradient Descent(14/49): loss=3640117.0076423767\n",
      "Gradient Descent(15/49): loss=21202112.742334843\n",
      "Gradient Descent(16/49): loss=123493177.27238862\n",
      "Gradient Descent(17/49): loss=719294590.766628\n",
      "Gradient Descent(18/49): loss=4189581325.5045342\n",
      "Gradient Descent(19/49): loss=24402507559.87725\n",
      "Gradient Descent(20/49): loss=142134100994.62326\n",
      "Gradient Descent(21/49): loss=827869948044.5015\n",
      "Gradient Descent(22/49): loss=4821986040509.199\n",
      "Gradient Descent(23/49): loss=28085992769509.715\n",
      "Gradient Descent(24/49): loss=163588816562748.97\n",
      "Gradient Descent(25/49): loss=952834429746544.5\n",
      "Gradient Descent(26/49): loss=5549850347882270.0\n",
      "Gradient Descent(27/49): loss=3.232548900660719e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=1.8828205699546656e+17\n",
      "Gradient Descent(29/49): loss=1.0966619245635456e+18\n",
      "Gradient Descent(30/49): loss=6.387583585919463e+18\n",
      "Gradient Descent(31/49): loss=3.720492446507325e+19\n",
      "Gradient Descent(32/49): loss=2.1670266789198378e+20\n",
      "Gradient Descent(33/49): loss=1.2621997476594175e+21\n",
      "Gradient Descent(34/49): loss=7.351770139653401e+21\n",
      "Gradient Descent(35/49): loss=4.282089604797162e+22\n",
      "Gradient Descent(36/49): loss=2.4941328462666743e+23\n",
      "Gradient Descent(37/49): loss=1.452725007869358e+24\n",
      "Gradient Descent(38/49): loss=8.461497757218311e+24\n",
      "Gradient Descent(39/49): loss=4.928458167070292e+25\n",
      "Gradient Descent(40/49): loss=2.870614706934256e+26\n",
      "Gradient Descent(41/49): loss=1.6720094837622865e+27\n",
      "Gradient Descent(42/49): loss=9.738735424987135e+27\n",
      "Gradient Descent(43/49): loss=5.672394122100975e+28\n",
      "Gradient Descent(44/49): loss=3.303925373502701e+29\n",
      "Gradient Descent(45/49): loss=1.924394292551766e+30\n",
      "Gradient Descent(46/49): loss=1.1208768281832372e+31\n",
      "Gradient Descent(47/49): loss=6.528624974729958e+31\n",
      "Gradient Descent(48/49): loss=3.802642983507114e+32\n",
      "Gradient Descent(49/49): loss=2.214875830053903e+33\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4357026049328342\n",
      "Gradient Descent(2/49): loss=0.4145312356071892\n",
      "Gradient Descent(3/49): loss=0.4243778042799557\n",
      "Gradient Descent(4/49): loss=0.5769186566094447\n",
      "Gradient Descent(5/49): loss=1.5889273605331502\n",
      "Gradient Descent(6/49): loss=7.935663497916548\n",
      "Gradient Descent(7/49): loss=47.50059144106188\n",
      "Gradient Descent(8/49): loss=293.9786267533926\n",
      "Gradient Descent(9/49): loss=1829.3476221071837\n",
      "Gradient Descent(10/49): loss=11393.433348284476\n",
      "Gradient Descent(11/49): loss=70969.75739224654\n",
      "Gradient Descent(12/49): loss=442080.8128893389\n",
      "Gradient Descent(13/49): loss=2753794.663558956\n",
      "Gradient Descent(14/49): loss=17153853.186481167\n",
      "Gradient Descent(15/49): loss=106854265.43980958\n",
      "Gradient Descent(16/49): loss=665613380.5767906\n",
      "Gradient Descent(17/49): loss=4146218886.221541\n",
      "Gradient Descent(18/49): loss=25827502211.593006\n",
      "Gradient Descent(19/49): loss=160883901413.58438\n",
      "Gradient Descent(20/49): loss=1002173168828.0348\n",
      "Gradient Descent(21/49): loss=6242707017270.117\n",
      "Gradient Descent(22/49): loss=38886883141222.77\n",
      "Gradient Descent(23/49): loss=242233005049847.22\n",
      "Gradient Descent(24/49): loss=1508910563039715.0\n",
      "Gradient Descent(25/49): loss=9399260380657990.0\n",
      "Gradient Descent(26/49): loss=5.854959059033685e+16\n",
      "Gradient Descent(27/49): loss=3.6471535200262464e+17\n",
      "Gradient Descent(28/49): loss=2.2718739216658834e+18\n",
      "Gradient Descent(29/49): loss=1.415188882947864e+19\n",
      "Gradient Descent(30/49): loss=8.815452104625336e+19\n",
      "Gradient Descent(31/49): loss=5.491294960363678e+20\n",
      "Gradient Descent(32/49): loss=3.4206209714296067e+21\n",
      "Gradient Descent(33/49): loss=2.1307629465617773e+22\n",
      "Gradient Descent(34/49): loss=1.3272884579617872e+23\n",
      "Gradient Descent(35/49): loss=8.267905416138976e+23\n",
      "Gradient Descent(36/49): loss=5.150218820948032e+24\n",
      "Gradient Descent(37/49): loss=3.2081588466010265e+25\n",
      "Gradient Descent(38/49): loss=1.9984166775908315e+26\n",
      "Gradient Descent(39/49): loss=1.244847717407881e+27\n",
      "Gradient Descent(40/49): loss=7.754368030013367e+27\n",
      "Gradient Descent(41/49): loss=4.8303276540604295e+28\n",
      "Gradient Descent(42/49): loss=3.0088932012607887e+29\n",
      "Gradient Descent(43/49): loss=1.874290720005924e+30\n",
      "Gradient Descent(44/49): loss=1.1675275485445393e+31\n",
      "Gradient Descent(45/49): loss=7.2727275553392305e+31\n",
      "Gradient Descent(46/49): loss=4.530305615497356e+32\n",
      "Gradient Descent(47/49): loss=2.8220043736877747e+33\n",
      "Gradient Descent(48/49): loss=1.7578744925884387e+34\n",
      "Gradient Descent(49/49): loss=1.0950099016518635e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43623543096244927\n",
      "Gradient Descent(2/49): loss=0.41470008819038034\n",
      "Gradient Descent(3/49): loss=0.4207355128520305\n",
      "Gradient Descent(4/49): loss=0.5454350606409426\n",
      "Gradient Descent(5/49): loss=1.3559410161334167\n",
      "Gradient Descent(6/49): loss=6.272177054930581\n",
      "Gradient Descent(7/49): loss=35.86494400497303\n",
      "Gradient Descent(8/49): loss=213.835924512756\n",
      "Gradient Descent(9/49): loss=1284.0397057794098\n",
      "Gradient Descent(10/49): loss=7719.4788701241405\n",
      "Gradient Descent(11/49): loss=46417.54100117896\n",
      "Gradient Descent(12/49): loss=279119.56577222125\n",
      "Gradient Descent(13/49): loss=1678420.4105046801\n",
      "Gradient Descent(14/49): loss=10092798.626781581\n",
      "Gradient Descent(15/49): loss=60690753.31646881\n",
      "Gradient Descent(16/49): loss=364950076.2095252\n",
      "Gradient Descent(17/49): loss=2194544496.270092\n",
      "Gradient Descent(18/49): loss=13196395516.264933\n",
      "Gradient Descent(19/49): loss=79353530966.06342\n",
      "Gradient Descent(20/49): loss=477174457914.5116\n",
      "Gradient Descent(21/49): loss=2869380360454.054\n",
      "Gradient Descent(22/49): loss=17254367907594.365\n",
      "Gradient Descent(23/49): loss=103755227432984.48\n",
      "Gradient Descent(24/49): loss=623908524341387.5\n",
      "Gradient Descent(25/49): loss=3751732383770924.5\n",
      "Gradient Descent(26/49): loss=2.2560191647154336e+16\n",
      "Gradient Descent(27/49): loss=1.3566059491822896e+17\n",
      "Gradient Descent(28/49): loss=8.157642143030652e+17\n",
      "Gradient Descent(29/49): loss=4.905413054827242e+18\n",
      "Gradient Descent(30/49): loss=2.9497588661727613e+19\n",
      "Gradient Descent(31/49): loss=1.7737705818682278e+20\n",
      "Gradient Descent(32/49): loss=1.0666167032098331e+21\n",
      "Gradient Descent(33/49): loss=6.413857593510957e+21\n",
      "Gradient Descent(34/49): loss=3.856827771967215e+22\n",
      "Gradient Descent(35/49): loss=2.319215892424379e+23\n",
      "Gradient Descent(36/49): loss=1.394607867836048e+24\n",
      "Gradient Descent(37/49): loss=8.386158060503087e+24\n",
      "Gradient Descent(38/49): loss=5.042825918146182e+25\n",
      "Gradient Descent(39/49): loss=3.0323889744574536e+26\n",
      "Gradient Descent(40/49): loss=1.8234583231045538e+27\n",
      "Gradient Descent(41/49): loss=1.0964952992859777e+28\n",
      "Gradient Descent(42/49): loss=6.593525753356843e+28\n",
      "Gradient Descent(43/49): loss=3.964867144299531e+29\n",
      "Gradient Descent(44/49): loss=2.384182918212288e+30\n",
      "Gradient Descent(45/49): loss=1.4336743150821412e+31\n",
      "Gradient Descent(46/49): loss=8.621075279187934e+31\n",
      "Gradient Descent(47/49): loss=5.184088058742141e+32\n",
      "Gradient Descent(48/49): loss=3.1173337583156896e+33\n",
      "Gradient Descent(49/49): loss=1.874537941990898e+34\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4354672552406453\n",
      "Gradient Descent(2/49): loss=0.41335481447310174\n",
      "Gradient Descent(3/49): loss=0.4172097627276405\n",
      "Gradient Descent(4/49): loss=0.5295284457732629\n",
      "Gradient Descent(5/49): loss=1.2661322442368539\n",
      "Gradient Descent(6/49): loss=5.7395716254310125\n",
      "Gradient Descent(7/49): loss=32.67875099018683\n",
      "Gradient Descent(8/49): loss=194.7475451193074\n",
      "Gradient Descent(9/49): loss=1169.6555411152406\n",
      "Gradient Descent(10/49): loss=7034.03081243071\n",
      "Gradient Descent(11/49): loss=42310.01435430222\n",
      "Gradient Descent(12/49): loss=254505.63529992188\n",
      "Gradient Descent(13/49): loss=1530925.9031684885\n",
      "Gradient Descent(14/49): loss=9208976.409272378\n",
      "Gradient Descent(15/49): loss=55394751.60659977\n",
      "Gradient Descent(16/49): loss=333216033.03264666\n",
      "Gradient Descent(17/49): loss=2004394314.2865226\n",
      "Gradient Descent(18/49): loss=12057032590.938261\n",
      "Gradient Descent(19/49): loss=72526664978.3147\n",
      "Gradient Descent(20/49): loss=436269628808.74384\n",
      "Gradient Descent(21/49): loss=2624292583678.946\n",
      "Gradient Descent(22/49): loss=15785906489895.447\n",
      "Gradient Descent(23/49): loss=94956959165886.72\n",
      "Gradient Descent(24/49): loss=571194571550503.1\n",
      "Gradient Descent(25/49): loss=3435906556346215.5\n",
      "Gradient Descent(26/49): loss=2.0668007806685964e+16\n",
      "Gradient Descent(27/49): loss=1.2432426193554206e+17\n",
      "Gradient Descent(28/49): loss=7.478477001937912e+17\n",
      "Gradient Descent(29/49): loss=4.4985280747140577e+18\n",
      "Gradient Descent(30/49): loss=2.705999474725531e+19\n",
      "Gradient Descent(31/49): loss=1.6277397930166895e+20\n",
      "Gradient Descent(32/49): loss=9.791342749756768e+20\n",
      "Gradient Descent(33/49): loss=5.8897861473018e+21\n",
      "Gradient Descent(34/49): loss=3.5428829066176754e+22\n",
      "Gradient Descent(35/49): loss=2.1311502618400376e+23\n",
      "Gradient Descent(36/49): loss=1.2819507610757238e+24\n",
      "Gradient Descent(37/49): loss=7.711318076669684e+24\n",
      "Gradient Descent(38/49): loss=4.638588960286975e+25\n",
      "Gradient Descent(39/49): loss=2.7902502955485415e+26\n",
      "Gradient Descent(40/49): loss=1.678419187055356e+27\n",
      "Gradient Descent(41/49): loss=1.0096194495417884e+28\n",
      "Gradient Descent(42/49): loss=6.073163609868961e+28\n",
      "Gradient Descent(43/49): loss=3.6531899468631414e+29\n",
      "Gradient Descent(44/49): loss=2.1975032528640096e+30\n",
      "Gradient Descent(45/49): loss=1.3218640740250193e+31\n",
      "Gradient Descent(46/49): loss=7.95140861757879e+31\n",
      "Gradient Descent(47/49): loss=4.7830106170592e+32\n",
      "Gradient Descent(48/49): loss=2.877124250956596e+33\n",
      "Gradient Descent(49/49): loss=1.730676475172059e+34\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4357731315728097\n",
      "Gradient Descent(2/49): loss=0.413061769415814\n",
      "Gradient Descent(3/49): loss=0.41038814618058933\n",
      "Gradient Descent(4/49): loss=0.4823369783057671\n",
      "Gradient Descent(5/49): loss=0.983499552082205\n",
      "Gradient Descent(6/49): loss=4.096593150476934\n",
      "Gradient Descent(7/49): loss=23.203253390637038\n",
      "Gradient Descent(8/49): loss=140.30981493466382\n",
      "Gradient Descent(9/49): loss=857.9523922208143\n",
      "Gradient Descent(10/49): loss=5255.666126762538\n",
      "Gradient Descent(11/49): loss=32204.795043182294\n",
      "Gradient Descent(12/49): loss=197348.63883075927\n",
      "Gradient Descent(13/49): loss=1209347.5165994328\n",
      "Gradient Descent(14/49): loss=7410860.699319863\n",
      "Gradient Descent(15/49): loss=45413635.781222686\n",
      "Gradient Descent(16/49): loss=278294042.7520917\n",
      "Gradient Descent(17/49): loss=1705381507.6425734\n",
      "Gradient Descent(18/49): loss=10450551008.741425\n",
      "Gradient Descent(19/49): loss=64040811931.40355\n",
      "Gradient Descent(20/49): loss=392441086551.35614\n",
      "Gradient Descent(21/49): loss=2404872795478.7837\n",
      "Gradient Descent(22/49): loss=14737022601940.574\n",
      "Gradient Descent(23/49): loss=90308242323019.61\n",
      "Gradient Descent(24/49): loss=553407486149840.3\n",
      "Gradient Descent(25/49): loss=3391272356195862.0\n",
      "Gradient Descent(26/49): loss=2.07816635693001e+16\n",
      "Gradient Descent(27/49): loss=1.2734970693773082e+17\n",
      "Gradient Descent(28/49): loss=7.803969977208018e+17\n",
      "Gradient Descent(29/49): loss=4.782260506884626e+18\n",
      "Gradient Descent(30/49): loss=2.9305617041713582e+19\n",
      "Gradient Descent(31/49): loss=1.79584359521859e+20\n",
      "Gradient Descent(32/49): loss=1.1004901257998046e+21\n",
      "Gradient Descent(33/49): loss=6.743786152688224e+21\n",
      "Gradient Descent(34/49): loss=4.132581529537724e+22\n",
      "Gradient Descent(35/49): loss=2.532439450421881e+23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=1.551874905362219e+24\n",
      "Gradient Descent(37/49): loss=9.509864970283183e+24\n",
      "Gradient Descent(38/49): loss=5.8276302710049465e+25\n",
      "Gradient Descent(39/49): loss=3.5711626486450305e+26\n",
      "Gradient Descent(40/49): loss=2.188402844725883e+27\n",
      "Gradient Descent(41/49): loss=1.3410498154211591e+28\n",
      "Gradient Descent(42/49): loss=8.21793214067204e+28\n",
      "Gradient Descent(43/49): loss=5.035935868458584e+29\n",
      "Gradient Descent(44/49): loss=3.086013566078535e+30\n",
      "Gradient Descent(45/49): loss=1.8911042512810854e+31\n",
      "Gradient Descent(46/49): loss=1.1588657057518401e+32\n",
      "Gradient Descent(47/49): loss=7.101510786927309e+32\n",
      "Gradient Descent(48/49): loss=4.351794621804697e+33\n",
      "Gradient Descent(49/49): loss=2.666772887992969e+34\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43941118106325666\n",
      "Gradient Descent(2/49): loss=0.4200411898329872\n",
      "Gradient Descent(3/49): loss=0.4344503135531333\n",
      "Gradient Descent(4/49): loss=0.624098166955954\n",
      "Gradient Descent(5/49): loss=1.9331908762352528\n",
      "Gradient Descent(6/49): loss=10.549944250732759\n",
      "Gradient Descent(7/49): loss=66.97971085414073\n",
      "Gradient Descent(8/49): loss=436.3205733093231\n",
      "Gradient Descent(9/49): loss=2853.555718613661\n",
      "Gradient Descent(10/49): loss=18673.58653009763\n",
      "Gradient Descent(11/49): loss=122210.54321437764\n",
      "Gradient Descent(12/49): loss=799826.1891185476\n",
      "Gradient Descent(13/49): loss=5234599.755190801\n",
      "Gradient Descent(14/49): loss=34258747.40657466\n",
      "Gradient Descent(15/49): loss=224212334.48702633\n",
      "Gradient Descent(16/49): loss=1467396653.8852372\n",
      "Gradient Descent(17/49): loss=9603632856.362556\n",
      "Gradient Descent(18/49): loss=62852647109.15438\n",
      "Gradient Descent(19/49): loss=411350090930.99646\n",
      "Gradient Descent(20/49): loss=2692152281442.705\n",
      "Gradient Descent(21/49): loss=17619259278824.902\n",
      "Gradient Descent(22/49): loss=115312309661817.47\n",
      "Gradient Descent(23/49): loss=754681485136167.5\n",
      "Gradient Descent(24/49): loss=4939144360889706.0\n",
      "Gradient Descent(25/49): loss=3.2325090118393664e+16\n",
      "Gradient Descent(26/49): loss=2.1155717970834928e+17\n",
      "Gradient Descent(27/49): loss=1.3845727922869742e+18\n",
      "Gradient Descent(28/49): loss=9.06157767741146e+18\n",
      "Gradient Descent(29/49): loss=5.9305072626864865e+19\n",
      "Gradient Descent(30/49): loss=3.881323721414523e+20\n",
      "Gradient Descent(31/49): loss=2.5401998788871014e+21\n",
      "Gradient Descent(32/49): loss=1.662478032712654e+22\n",
      "Gradient Descent(33/49): loss=1.0880376903501845e+23\n",
      "Gradient Descent(34/49): loss=7.12085207941615e+23\n",
      "Gradient Descent(35/49): loss=4.660365609265223e+24\n",
      "Gradient Descent(36/49): loss=3.0500574046193924e+25\n",
      "Gradient Descent(37/49): loss=1.996163166464554e+26\n",
      "Gradient Descent(38/49): loss=1.3064237352106774e+27\n",
      "Gradient Descent(39/49): loss=8.550117568518436e+27\n",
      "Gradient Descent(40/49): loss=5.595773290486183e+28\n",
      "Gradient Descent(41/49): loss=3.662251245972482e+29\n",
      "Gradient Descent(42/49): loss=2.3968240835328163e+30\n",
      "Gradient Descent(43/49): loss=1.5686432474343447e+31\n",
      "Gradient Descent(44/49): loss=1.0266258815684436e+32\n",
      "Gradient Descent(45/49): loss=6.71893180575045e+32\n",
      "Gradient Descent(46/49): loss=4.3973218891924056e+33\n",
      "Gradient Descent(47/49): loss=2.8779038627273327e+34\n",
      "Gradient Descent(48/49): loss=1.883494283977037e+35\n",
      "Gradient Descent(49/49): loss=1.2326856236303068e+36\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.439993513906016\n",
      "Gradient Descent(2/49): loss=0.4203682651379923\n",
      "Gradient Descent(3/49): loss=0.43049525654353415\n",
      "Gradient Descent(4/49): loss=0.5862769111203694\n",
      "Gradient Descent(5/49): loss=1.636907900655326\n",
      "Gradient Descent(6/49): loss=8.32478140840458\n",
      "Gradient Descent(7/49): loss=50.62405547572126\n",
      "Gradient Descent(8/49): loss=317.9572357227124\n",
      "Gradient Descent(9/49): loss=2007.3655790749483\n",
      "Gradient Descent(10/49): loss=12683.44736260437\n",
      "Gradient Descent(11/49): loss=80150.01457945415\n",
      "Gradient Descent(12/49): loss=506499.03598990536\n",
      "Gradient Descent(13/49): loss=3200774.027831117\n",
      "Gradient Descent(14/49): loss=20227006.88301397\n",
      "Gradient Descent(15/49): loss=127822781.6428897\n",
      "Gradient Descent(16/49): loss=807764777.2842374\n",
      "Gradient Descent(17/49): loss=5104598165.64631\n",
      "Gradient Descent(18/49): loss=32258057263.166904\n",
      "Gradient Descent(19/49): loss=203851943811.73096\n",
      "Gradient Descent(20/49): loss=1288224354527.1758\n",
      "Gradient Descent(21/49): loss=8140820031285.134\n",
      "Gradient Descent(22/49): loss=51445193183071.25\n",
      "Gradient Descent(23/49): loss=325103354634131.44\n",
      "Gradient Descent(24/49): loss=2054461936186241.5\n",
      "Gradient Descent(25/49): loss=1.2982990753781158e+16\n",
      "Gradient Descent(26/49): loss=8.20448633989595e+16\n",
      "Gradient Descent(27/49): loss=5.1847526797273734e+17\n",
      "Gradient Descent(28/49): loss=3.2764586637462605e+18\n",
      "Gradient Descent(29/49): loss=2.0705291145733407e+19\n",
      "Gradient Descent(30/49): loss=1.3084525868530401e+20\n",
      "Gradient Descent(31/49): loss=8.268650558894375e+20\n",
      "Gradient Descent(32/49): loss=5.225300691219012e+21\n",
      "Gradient Descent(33/49): loss=3.3020826214845544e+22\n",
      "Gradient Descent(34/49): loss=2.0867219483529445e+23\n",
      "Gradient Descent(35/49): loss=1.3186855051434922e+24\n",
      "Gradient Descent(36/49): loss=8.33331658225032e+24\n",
      "Gradient Descent(37/49): loss=5.266165813542579e+25\n",
      "Gradient Descent(38/49): loss=3.327906974612568e+26\n",
      "Gradient Descent(39/49): loss=2.103041420229141e+27\n",
      "Gradient Descent(40/49): loss=1.328998451260561e+28\n",
      "Gradient Descent(41/49): loss=8.398488334388557e+28\n",
      "Gradient Descent(42/49): loss=5.307350526703492e+29\n",
      "Gradient Descent(43/49): loss=3.3539332903474214e+30\n",
      "Gradient Descent(44/49): loss=2.119488520590954e+31\n",
      "Gradient Descent(45/49): loss=1.3393920510719242e+32\n",
      "Gradient Descent(46/49): loss=8.464169770424028e+32\n",
      "Gradient Descent(47/49): loss=5.348857330101804e+33\n",
      "Gradient Descent(48/49): loss=3.380163148163134e+34\n",
      "Gradient Descent(49/49): loss=2.1360642475731194e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4393158790285957\n",
      "Gradient Descent(2/49): loss=0.41907100415882415\n",
      "Gradient Descent(3/49): loss=0.4266294962210063\n",
      "Gradient Descent(4/49): loss=0.5671349117418292\n",
      "Gradient Descent(5/49): loss=1.5220772695022946\n",
      "Gradient Descent(6/49): loss=7.60756606123281\n",
      "Gradient Descent(7/49): loss=46.11312461889834\n",
      "Gradient Descent(8/49): loss=289.5537214840796\n",
      "Gradient Descent(9/49): loss=1828.4897733733033\n",
      "Gradient Descent(10/49): loss=11556.929440866243\n",
      "Gradient Descent(11/49): loss=73055.52793586135\n",
      "Gradient Descent(12/49): loss=461820.5443301194\n",
      "Gradient Descent(13/49): loss=2919408.8267513844\n",
      "Gradient Descent(14/49): loss=18455117.795084473\n",
      "Gradient Descent(15/49): loss=116664510.64026909\n",
      "Gradient Descent(16/49): loss=737497770.4768698\n",
      "Gradient Descent(17/49): loss=4662111550.803122\n",
      "Gradient Descent(18/49): loss=29471660782.70431\n",
      "Gradient Descent(19/49): loss=186305878757.3774\n",
      "Gradient Descent(20/49): loss=1177737512513.1316\n",
      "Gradient Descent(21/49): loss=7445098660513.933\n",
      "Gradient Descent(22/49): loss=47064386992751.81\n",
      "Gradient Descent(23/49): loss=297518760194737.7\n",
      "Gradient Descent(24/49): loss=1880772667483049.8\n",
      "Gradient Descent(25/49): loss=1.1889353882881126e+16\n",
      "Gradient Descent(26/49): loss=7.51588632673808e+16\n",
      "Gradient Descent(27/49): loss=4.751187308654557e+17\n",
      "Gradient Descent(28/49): loss=3.003475553057866e+18\n",
      "Gradient Descent(29/49): loss=1.8986549701764547e+19\n",
      "Gradient Descent(30/49): loss=1.2002397329672625e+20\n",
      "Gradient Descent(31/49): loss=7.587347038937777e+20\n",
      "Gradient Descent(32/49): loss=4.796361385817272e+21\n",
      "Gradient Descent(33/49): loss=3.0320324647467545e+22\n",
      "Gradient Descent(34/49): loss=1.9167072970069113e+23\n",
      "Gradient Descent(35/49): loss=1.2116515588517376e+24\n",
      "Gradient Descent(36/49): loss=7.659487196404044e+24\n",
      "Gradient Descent(37/49): loss=4.84196497609241e+25\n",
      "Gradient Descent(38/49): loss=3.0608608942791553e+26\n",
      "Gradient Descent(39/49): loss=1.934931264556261e+27\n",
      "Gradient Descent(40/49): loss=1.2231718878681365e+28\n",
      "Gradient Descent(41/49): loss=7.732313259272254e+28\n",
      "Gradient Descent(42/49): loss=4.888002163271176e+29\n",
      "Gradient Descent(43/49): loss=3.0899634232346605e+30\n",
      "Gradient Descent(44/49): loss=1.9533285047767683e+31\n",
      "Gradient Descent(45/49): loss=1.2348017516593183e+32\n",
      "Gradient Descent(46/49): loss=7.805831749100583e+32\n",
      "Gradient Descent(47/49): loss=4.93447706997388e+33\n",
      "Gradient Descent(48/49): loss=3.119342657738394e+34\n",
      "Gradient Descent(49/49): loss=1.9719006651373518e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4394368273658362\n",
      "Gradient Descent(2/49): loss=0.41836267227410867\n",
      "Gradient Descent(3/49): loss=0.4181446297788274\n",
      "Gradient Descent(4/49): loss=0.5087750455343125\n",
      "Gradient Descent(5/49): loss=1.1583682161152147\n",
      "Gradient Descent(6/49): loss=5.389314912627603\n",
      "Gradient Descent(7/49): loss=32.668252986869156\n",
      "Gradient Descent(8/49): loss=208.34639016414326\n",
      "Gradient Descent(9/49): loss=1339.57548690093\n",
      "Gradient Descent(10/49): loss=8623.690175025322\n",
      "Gradient Descent(11/49): loss=55526.86167345202\n",
      "Gradient Descent(12/49): loss=357541.1944913276\n",
      "Gradient Descent(13/49): loss=2302242.4176383726\n",
      "Gradient Descent(14/49): loss=14824372.74489261\n",
      "Gradient Descent(15/49): loss=95455652.25013879\n",
      "Gradient Descent(16/49): loss=614648718.0040431\n",
      "Gradient Descent(17/49): loss=3957786036.171046\n",
      "Gradient Descent(18/49): loss=25484589580.600044\n",
      "Gradient Descent(19/49): loss=164097882047.5896\n",
      "Gradient Descent(20/49): loss=1056643066885.865\n",
      "Gradient Descent(21/49): loss=6803832912821.272\n",
      "Gradient Descent(22/49): loss=43810576869665.555\n",
      "Gradient Descent(23/49): loss=282100790869813.9\n",
      "Gradient Descent(24/49): loss=1816475880838642.0\n",
      "Gradient Descent(25/49): loss=1.1696474212265248e+16\n",
      "Gradient Descent(26/49): loss=7.531479522592302e+16\n",
      "Gradient Descent(27/49): loss=4.849596790436536e+17\n",
      "Gradient Descent(28/49): loss=3.122705035479886e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=2.010741750291705e+19\n",
      "Gradient Descent(30/49): loss=1.2947372039398911e+20\n",
      "Gradient Descent(31/49): loss=8.33694544325693e+20\n",
      "Gradient Descent(32/49): loss=5.368244545096908e+21\n",
      "Gradient Descent(33/49): loss=3.456667635899038e+22\n",
      "Gradient Descent(34/49): loss=2.225783688633435e+23\n",
      "Gradient Descent(35/49): loss=1.433204910167162e+24\n",
      "Gradient Descent(36/49): loss=9.228553183388613e+24\n",
      "Gradient Descent(37/49): loss=5.942359899444071e+25\n",
      "Gradient Descent(38/49): loss=3.826346391770289e+26\n",
      "Gradient Descent(39/49): loss=2.463823625220622e+27\n",
      "Gradient Descent(40/49): loss=1.586481262974959e+28\n",
      "Gradient Descent(41/49): loss=1.0215515315326741e+29\n",
      "Gradient Descent(42/49): loss=6.577874923148361e+29\n",
      "Gradient Descent(43/49): loss=4.2355610235018425e+30\n",
      "Gradient Descent(44/49): loss=2.727321117139894e+31\n",
      "Gradient Descent(45/49): loss=1.7561499963580883e+32\n",
      "Gradient Descent(46/49): loss=1.1308029664445105e+33\n",
      "Gradient Descent(47/49): loss=7.281356100398625e+33\n",
      "Gradient Descent(48/49): loss=4.68853975750643e+34\n",
      "Gradient Descent(49/49): loss=3.0189987626775615e+35\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4432901692603276\n",
      "Gradient Descent(2/49): loss=0.4262381024784116\n",
      "Gradient Descent(3/49): loss=0.44630865852558566\n",
      "Gradient Descent(4/49): loss=0.6807110475286025\n",
      "Gradient Descent(5/49): loss=2.3627293595620875\n",
      "Gradient Descent(6/49): loss=13.969546539843108\n",
      "Gradient Descent(7/49): loss=93.7266956938973\n",
      "Gradient Descent(8/49): loss=641.5286911199393\n",
      "Gradient Descent(9/49): loss=4403.842241762915\n",
      "Gradient Descent(10/49): loss=30243.32582189162\n",
      "Gradient Descent(11/49): loss=207708.19280170716\n",
      "Gradient Descent(12/49): loss=1426531.958599821\n",
      "Gradient Descent(13/49): loss=9797379.580053007\n",
      "Gradient Descent(14/49): loss=67288127.46449059\n",
      "Gradient Descent(15/49): loss=462132979.67321306\n",
      "Gradient Descent(16/49): loss=3173916406.5852313\n",
      "Gradient Descent(17/49): loss=21798369310.87972\n",
      "Gradient Descent(18/49): loss=149710592146.16928\n",
      "Gradient Descent(19/49): loss=1028208169215.523\n",
      "Gradient Descent(20/49): loss=7061705014239.18\n",
      "Gradient Descent(21/49): loss=48499592982411.03\n",
      "Gradient Descent(22/49): loss=333093851232348.06\n",
      "Gradient Descent(23/49): loss=2287679275350436.5\n",
      "Gradient Descent(24/49): loss=1.5711717425902068e+16\n",
      "Gradient Descent(25/49): loss=1.0790763684895138e+17\n",
      "Gradient Descent(26/49): loss=7.411066387388768e+17\n",
      "Gradient Descent(27/49): loss=5.089899714434985e+18\n",
      "Gradient Descent(28/49): loss=3.495728920616713e+19\n",
      "Gradient Descent(29/49): loss=2.400856867922197e+20\n",
      "Gradient Descent(30/49): loss=1.6489017973488188e+21\n",
      "Gradient Descent(31/49): loss=1.1324611531936557e+22\n",
      "Gradient Descent(32/49): loss=7.777711599045726e+22\n",
      "Gradient Descent(33/49): loss=5.341710622685394e+23\n",
      "Gradient Descent(34/49): loss=3.66867194973021e+24\n",
      "Gradient Descent(35/49): loss=2.5196336577233856e+25\n",
      "Gradient Descent(36/49): loss=1.7304773651401594e+26\n",
      "Gradient Descent(37/49): loss=1.1884870255179063e+27\n",
      "Gradient Descent(38/49): loss=8.162495726778588e+27\n",
      "Gradient Descent(39/49): loss=5.605979287880446e+28\n",
      "Gradient Descent(40/49): loss=3.850170931550107e+29\n",
      "Gradient Descent(41/49): loss=2.644286651968324e+30\n",
      "Gradient Descent(42/49): loss=1.816088693746077e+31\n",
      "Gradient Descent(43/49): loss=1.2472846471078058e+32\n",
      "Gradient Descent(44/49): loss=8.566316151122982e+32\n",
      "Gradient Descent(45/49): loss=5.883322028467759e+33\n",
      "Gradient Descent(46/49): loss=4.0406491518663656e+34\n",
      "Gradient Descent(47/49): loss=2.7751065621561163e+35\n",
      "Gradient Descent(48/49): loss=1.9059354430127798e+36\n",
      "Gradient Descent(49/49): loss=1.3089911437887756e+37\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4439234121464984\n",
      "Gradient Descent(2/49): loss=0.42674195722829805\n",
      "Gradient Descent(3/49): loss=0.44202402539297736\n",
      "Gradient Descent(4/49): loss=0.6354484365787055\n",
      "Gradient Descent(5/49): loss=1.9880475906620816\n",
      "Gradient Descent(6/49): loss=11.013509045404865\n",
      "Gradient Descent(7/49): loss=70.92212834511302\n",
      "Gradient Descent(8/49): loss=468.3371750306522\n",
      "Gradient Descent(9/49): loss=3104.4761368190216\n",
      "Gradient Descent(10/49): loss=20590.40283822207\n",
      "Gradient Descent(11/49): loss=136577.20600605325\n",
      "Gradient Descent(12/49): loss=905935.1686785455\n",
      "Gradient Descent(13/49): loss=6009202.526741751\n",
      "Gradient Descent(14/49): loss=39859944.247596376\n",
      "Gradient Descent(15/49): loss=264397017.1522018\n",
      "Gradient Descent(16/49): loss=1753785271.1276927\n",
      "Gradient Descent(17/49): loss=11633122087.887726\n",
      "Gradient Descent(18/49): loss=77164252522.43867\n",
      "Gradient Descent(19/49): loss=511842119638.4667\n",
      "Gradient Descent(20/49): loss=3395125940741.485\n",
      "Gradient Descent(21/49): loss=22520382186685.414\n",
      "Gradient Descent(22/49): loss=149381090035104.94\n",
      "Gradient Descent(23/49): loss=990867289688745.9\n",
      "Gradient Descent(24/49): loss=6572572107650374.0\n",
      "Gradient Descent(25/49): loss=4.3596861617900296e+16\n",
      "Gradient Descent(26/49): loss=2.8918455542206976e+17\n",
      "Gradient Descent(27/49): loss=1.918204751241092e+18\n",
      "Gradient Descent(28/49): loss=1.272374128802847e+19\n",
      "Gradient Descent(29/49): loss=8.439849409190332e+19\n",
      "Gradient Descent(30/49): loss=5.598279345465017e+20\n",
      "Gradient Descent(31/49): loss=3.7134230849821905e+21\n",
      "Gradient Descent(32/49): loss=2.4631695128341783e+22\n",
      "Gradient Descent(33/49): loss=1.6338574706158825e+23\n",
      "Gradient Descent(34/49): loss=1.0837622909743631e+24\n",
      "Gradient Descent(35/49): loss=7.188758655277609e+24\n",
      "Gradient Descent(36/49): loss=4.768411987961664e+25\n",
      "Gradient Descent(37/49): loss=3.162959556340505e+26\n",
      "Gradient Descent(38/49): loss=2.0980387559428093e+27\n",
      "Gradient Descent(39/49): loss=1.3916607351536098e+28\n",
      "Gradient Descent(40/49): loss=9.231095451799682e+28\n",
      "Gradient Descent(41/49): loss=6.123124773713542e+29\n",
      "Gradient Descent(42/49): loss=4.061560969684846e+30\n",
      "Gradient Descent(43/49): loss=2.6940946200027695e+31\n",
      "Gradient Descent(44/49): loss=1.7870335754410997e+32\n",
      "Gradient Descent(45/49): loss=1.1853663104641273e+33\n",
      "Gradient Descent(46/49): loss=7.862713433554101e+33\n",
      "Gradient Descent(47/49): loss=5.215456352389913e+34\n",
      "Gradient Descent(48/49): loss=3.4594908225453194e+35\n",
      "Gradient Descent(49/49): loss=2.2947324150821913e+36\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4433399762983034\n",
      "Gradient Descent(2/49): loss=0.4255004480340548\n",
      "Gradient Descent(3/49): loss=0.43777894844946685\n",
      "Gradient Descent(4/49): loss=0.6124683958851485\n",
      "Gradient Descent(5/49): loss=1.8420146800616182\n",
      "Gradient Descent(6/49): loss=10.054514795523703\n",
      "Gradient Descent(7/49): loss=64.58908967959188\n",
      "Gradient Descent(8/49): loss=426.47770590877064\n",
      "Gradient Descent(9/49): loss=2827.762368647387\n",
      "Gradient Descent(10/49): loss=18761.158718885217\n",
      "Gradient Descent(11/49): loss=124484.91893325324\n",
      "Gradient Descent(12/49): loss=825999.6369157453\n",
      "Gradient Descent(13/49): loss=5480799.004033394\n",
      "Gradient Descent(14/49): loss=36367046.36863641\n",
      "Gradient Descent(15/49): loss=241308269.57252398\n",
      "Gradient Descent(16/49): loss=1601166088.213942\n",
      "Gradient Descent(17/49): loss=10624305786.54053\n",
      "Gradient Descent(18/49): loss=70496043036.85623\n",
      "Gradient Descent(19/49): loss=467766288341.32684\n",
      "Gradient Descent(20/49): loss=3103795490975.5713\n",
      "Gradient Descent(21/49): loss=20594785665220.453\n",
      "Gradient Descent(22/49): loss=136653718915962.98\n",
      "Gradient Descent(23/49): loss=906745969446964.0\n",
      "Gradient Descent(24/49): loss=6016581616881838.0\n",
      "Gradient Descent(25/49): loss=3.9922156339640504e+16\n",
      "Gradient Descent(26/49): loss=2.6489768913542614e+17\n",
      "Gradient Descent(27/49): loss=1.75769026883992e+18\n",
      "Gradient Descent(28/49): loss=1.1662899330144651e+19\n",
      "Gradient Descent(29/49): loss=7.738748014737854e+19\n",
      "Gradient Descent(30/49): loss=5.134934216641849e+20\n",
      "Gradient Descent(31/49): loss=3.407211264538464e+21\n",
      "Gradient Descent(32/49): loss=2.260805710728265e+22\n",
      "Gradient Descent(33/49): loss=1.5001249012229288e+23\n",
      "Gradient Descent(34/49): loss=9.953861619290923e+23\n",
      "Gradient Descent(35/49): loss=6.604740782265556e+24\n",
      "Gradient Descent(36/49): loss=4.382480133778335e+25\n",
      "Gradient Descent(37/49): loss=2.9079312506150156e+26\n",
      "Gradient Descent(38/49): loss=1.9295156852230182e+27\n",
      "Gradient Descent(39/49): loss=1.2803022006569972e+28\n",
      "Gradient Descent(40/49): loss=8.495259911907487e+28\n",
      "Gradient Descent(41/49): loss=5.636906734505887e+29\n",
      "Gradient Descent(42/49): loss=3.740287862055824e+30\n",
      "Gradient Descent(43/49): loss=2.481813865289806e+31\n",
      "Gradient Descent(44/49): loss=1.646771662799064e+32\n",
      "Gradient Descent(45/49): loss=1.092691497668519e+33\n",
      "Gradient Descent(46/49): loss=7.250396251339777e+33\n",
      "Gradient Descent(47/49): loss=4.810895473572038e+34\n",
      "Gradient Descent(48/49): loss=3.1922000474606427e+35\n",
      "Gradient Descent(49/49): loss=2.1181381302059122e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44326931628720634\n",
      "Gradient Descent(2/49): loss=0.424333964263117\n",
      "Gradient Descent(3/49): loss=0.4273903327340975\n",
      "Gradient Descent(4/49): loss=0.5408041295939938\n",
      "Gradient Descent(5/49): loss=1.37705077320416\n",
      "Gradient Descent(6/49): loss=7.081730524016558\n",
      "Gradient Descent(7/49): loss=45.67464431100962\n",
      "Gradient Descent(8/49): loss=306.51442799547794\n",
      "Gradient Descent(9/49): loss=2069.2745338392274\n",
      "Gradient Descent(10/49): loss=13981.892507639792\n",
      "Gradient Descent(11/49): loss=94486.47402255468\n",
      "Gradient Descent(12/49): loss=638530.326720806\n",
      "Gradient Descent(13/49): loss=4315137.327831878\n",
      "Gradient Descent(14/49): loss=29161367.909552738\n",
      "Gradient Descent(15/49): loss=197070305.082951\n",
      "Gradient Descent(16/49): loss=1331786136.0867524\n",
      "Gradient Descent(17/49): loss=9000109447.528143\n",
      "Gradient Descent(18/49): loss=60822055349.83551\n",
      "Gradient Descent(19/49): loss=411030825641.8478\n",
      "Gradient Descent(20/49): loss=2777715068272.487\n",
      "Gradient Descent(21/49): loss=18771587236709.21\n",
      "Gradient Descent(22/49): loss=126856959308129.1\n",
      "Gradient Descent(23/49): loss=857289685841482.4\n",
      "Gradient Descent(24/49): loss=5793498515639657.0\n",
      "Gradient Descent(25/49): loss=3.915202247857841e+16\n",
      "Gradient Descent(26/49): loss=2.6458639111153498e+17\n",
      "Gradient Descent(27/49): loss=1.7880547141524803e+18\n",
      "Gradient Descent(28/49): loss=1.2083537809225746e+19\n",
      "Gradient Descent(29/49): loss=8.165962978162844e+19\n",
      "Gradient Descent(30/49): loss=5.518495693356766e+20\n",
      "Gradient Descent(31/49): loss=3.7293574314549324e+21\n",
      "Gradient Descent(32/49): loss=2.5202713972017925e+22\n",
      "Gradient Descent(33/49): loss=1.7031802481521397e+23\n",
      "Gradient Descent(34/49): loss=1.1509962621153506e+24\n",
      "Gradient Descent(35/49): loss=7.778345227058879e+24\n",
      "Gradient Descent(36/49): loss=5.256546564288324e+25\n",
      "Gradient Descent(37/49): loss=3.5523342016768646e+26\n",
      "Gradient Descent(38/49): loss=2.4006404444572297e+27\n",
      "Gradient Descent(39/49): loss=1.6223345598630068e+28\n",
      "Gradient Descent(40/49): loss=1.0963613606539104e+29\n",
      "Gradient Descent(41/49): loss=7.409126716972493e+29\n",
      "Gradient Descent(42/49): loss=5.007031502406687e+30\n",
      "Gradient Descent(43/49): loss=3.3837138199651228e+31\n",
      "Gradient Descent(44/49): loss=2.2866880725475194e+32\n",
      "Gradient Descent(45/49): loss=1.5453264133268293e+33\n",
      "Gradient Descent(46/49): loss=1.0443198407315424e+34\n",
      "Gradient Descent(47/49): loss=7.057434082147541e+34\n",
      "Gradient Descent(48/49): loss=4.769360293774261e+35\n",
      "Gradient Descent(49/49): loss=3.2230974242282836e+36\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4473395695240469\n",
      "Gradient Descent(2/49): loss=0.43317439299685156\n",
      "Gradient Descent(3/49): loss=0.46020270799145\n",
      "Gradient Descent(4/49): loss=0.7483702441172683\n",
      "Gradient Descent(5/49): loss=2.895903815714405\n",
      "Gradient Descent(6/49): loss=18.41413440170417\n",
      "Gradient Descent(7/49): loss=130.1773139025294\n",
      "Gradient Descent(8/49): loss=934.8049550779722\n",
      "Gradient Descent(9/49): loss=6727.4009865841745\n",
      "Gradient Descent(10/49): loss=48428.694929608944\n",
      "Gradient Descent(11/49): loss=348638.9986539841\n",
      "Gradient Descent(12/49): loss=2509872.204458042\n",
      "Gradient Descent(13/49): loss=18068728.383314867\n",
      "Gradient Descent(14/49): loss=130077929.77982193\n",
      "Gradient Descent(15/49): loss=936439339.1404704\n",
      "Gradient Descent(16/49): loss=6741486731.613777\n",
      "Gradient Descent(17/49): loss=48532394428.38091\n",
      "Gradient Descent(18/49): loss=349387813521.74713\n",
      "Gradient Descent(19/49): loss=2515265230078.75\n",
      "Gradient Descent(20/49): loss=18107555366278.285\n",
      "Gradient Descent(21/49): loss=130357449950753.66\n",
      "Gradient Descent(22/49): loss=938451624967008.8\n",
      "Gradient Descent(23/49): loss=6755973308283449.0\n",
      "Gradient Descent(24/49): loss=4.863668422316807e+16\n",
      "Gradient Descent(25/49): loss=3.501386024311028e+17\n",
      "Gradient Descent(26/49): loss=2.520670207489242e+18\n",
      "Gradient Descent(27/49): loss=1.8146466144572475e+19\n",
      "Gradient Descent(28/49): loss=1.3063757113395516e+20\n",
      "Gradient Descent(29/49): loss=9.404682352923991e+20\n",
      "Gradient Descent(30/49): loss=6.770491015077558e+21\n",
      "Gradient Descent(31/49): loss=4.8741198123500376e+22\n",
      "Gradient Descent(32/49): loss=3.508910046884065e+23\n",
      "Gradient Descent(33/49): loss=2.526086799492755e+24\n",
      "Gradient Descent(34/49): loss=1.8185460537063876e+25\n",
      "Gradient Descent(35/49): loss=1.3091829426111167e+26\n",
      "Gradient Descent(36/49): loss=9.424891790508362e+26\n",
      "Gradient Descent(37/49): loss=6.785039918533385e+27\n",
      "Gradient Descent(38/49): loss=4.884593661059589e+28\n",
      "Gradient Descent(39/49): loss=3.516450237601721e+29\n",
      "Gradient Descent(40/49): loss=2.5315150310469702e+30\n",
      "Gradient Descent(41/49): loss=1.8224538723425025e+31\n",
      "Gradient Descent(42/49): loss=1.3119962062569758e+32\n",
      "Gradient Descent(43/49): loss=9.445144655541536e+32\n",
      "Gradient Descent(44/49): loss=6.79962008568721e+33\n",
      "Gradient Descent(45/49): loss=4.895090016705719e+34\n",
      "Gradient Descent(46/49): loss=3.524006631207252e+35\n",
      "Gradient Descent(47/49): loss=2.5369549271640003e+36\n",
      "Gradient Descent(48/49): loss=1.826370088371997e+37\n",
      "Gradient Descent(49/49): loss=1.3148155152400532e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44802512568389624\n",
      "Gradient Descent(2/49): loss=0.43387477996849466\n",
      "Gradient Descent(3/49): loss=0.4555730340395538\n",
      "Gradient Descent(4/49): loss=0.6944052554119448\n",
      "Gradient Descent(5/49): loss=2.4246083642397496\n",
      "Gradient Descent(6/49): loss=14.512926271733292\n",
      "Gradient Descent(7/49): loss=98.6261131541381\n",
      "Gradient Descent(8/49): loss=683.6272004368385\n",
      "Gradient Descent(9/49): loss=4752.041669626724\n",
      "Gradient Descent(10/49): loss=33045.81557824184\n",
      "Gradient Descent(11/49): loss=229814.61424022983\n",
      "Gradient Descent(12/49): loss=1598241.3480122397\n",
      "Gradient Descent(13/49): loss=11114951.915575337\n",
      "Gradient Descent(14/49): loss=77298824.00473374\n",
      "Gradient Descent(15/49): loss=537573925.6748614\n",
      "Gradient Descent(16/49): loss=3738552691.743779\n",
      "Gradient Descent(17/49): loss=25999728722.389107\n",
      "Gradient Descent(18/49): loss=180814863243.217\n",
      "Gradient Descent(19/49): loss=1257475226725.6465\n",
      "Gradient Descent(20/49): loss=8745099365554.013\n",
      "Gradient Descent(21/49): loss=60817709397397.42\n",
      "Gradient Descent(22/49): loss=422956174850987.94\n",
      "Gradient Descent(23/49): loss=2941444648558752.5\n",
      "Gradient Descent(24/49): loss=2.0456248507503268e+16\n",
      "Gradient Descent(25/49): loss=1.4226278342710595e+17\n",
      "Gradient Descent(26/49): loss=9.893651585726854e+17\n",
      "Gradient Descent(27/49): loss=6.880530476188085e+18\n",
      "Gradient Descent(28/49): loss=4.7850582996121985e+19\n",
      "Gradient Descent(29/49): loss=3.327764190555933e+20\n",
      "Gradient Descent(30/49): loss=2.3142904045377392e+21\n",
      "Gradient Descent(31/49): loss=1.6094710351579335e+22\n",
      "Gradient Descent(32/49): loss=1.1193050828595979e+23\n",
      "Gradient Descent(33/49): loss=7.784196429433855e+23\n",
      "Gradient Descent(34/49): loss=5.413511917341073e+24\n",
      "Gradient Descent(35/49): loss=3.7648216543432896e+25\n",
      "Gradient Descent(36/49): loss=2.6182415972168944e+26\n",
      "Gradient Descent(37/49): loss=1.8208535996621833e+27\n",
      "Gradient Descent(38/49): loss=1.266310883963865e+28\n",
      "Gradient Descent(39/49): loss=8.806546858807728e+28\n",
      "Gradient Descent(40/49): loss=6.124504539802212e+29\n",
      "Gradient Descent(41/49): loss=4.2592807895573243e+30\n",
      "Gradient Descent(42/49): loss=2.9621127270612115e+31\n",
      "Gradient Descent(43/49): loss=2.059998446059246e+32\n",
      "Gradient Descent(44/49): loss=1.4326239372991709e+33\n",
      "Gradient Descent(45/49): loss=9.963169388058903e+33\n",
      "Gradient Descent(46/49): loss=6.928876564933708e+34\n",
      "Gradient Descent(47/49): loss=4.8186805405143915e+35\n",
      "Gradient Descent(48/49): loss=3.351146745641858e+36\n",
      "Gradient Descent(49/49): loss=2.33055177997484e+37\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44753954704976834\n",
      "Gradient Descent(2/49): loss=0.43269756828525274\n",
      "Gradient Descent(3/49): loss=0.45090646851884175\n",
      "Gradient Descent(4/49): loss=0.6668916920779195\n",
      "Gradient Descent(5/49): loss=2.2398720642812644\n",
      "Gradient Descent(6/49): loss=13.239318098798845\n",
      "Gradient Descent(7/49): loss=89.80589867540422\n",
      "Gradient Descent(8/49): loss=622.5004799569929\n",
      "Gradient Descent(9/49): loss=4328.373333709283\n",
      "Gradient Descent(10/49): loss=30109.364563692558\n",
      "Gradient Descent(11/49): loss=209462.23784031594\n",
      "Gradient Descent(12/49): loss=1457181.910415085\n",
      "Gradient Descent(13/49): loss=10137301.35608831\n",
      "Gradient Descent(14/49): loss=70523039.54888418\n",
      "Gradient Descent(15/49): loss=490613730.6170146\n",
      "Gradient Descent(16/49): loss=3413094998.548371\n",
      "Gradient Descent(17/49): loss=23744173365.634182\n",
      "Gradient Descent(18/49): loss=165183145825.17847\n",
      "Gradient Descent(19/49): loss=1149143886579.428\n",
      "Gradient Descent(20/49): loss=7994348730121.36\n",
      "Gradient Descent(21/49): loss=55614977693560.49\n",
      "Gradient Descent(22/49): loss=386901528601242.56\n",
      "Gradient Descent(23/49): loss=2691591348984938.0\n",
      "Gradient Descent(24/49): loss=1.87248264852355e+16\n",
      "Gradient Descent(25/49): loss=1.3026462097762794e+17\n",
      "Gradient Descent(26/49): loss=9.062231626992297e+17\n",
      "Gradient Descent(27/49): loss=6.304401106373054e+18\n",
      "Gradient Descent(28/49): loss=4.385837280041726e+19\n",
      "Gradient Descent(29/49): loss=3.051133378483641e+20\n",
      "Gradient Descent(30/49): loss=2.1226083638946198e+21\n",
      "Gradient Descent(31/49): loss=1.476653330938442e+22\n",
      "Gradient Descent(32/49): loss=1.0272762026484652e+23\n",
      "Gradient Descent(33/49): loss=7.146541266102085e+23\n",
      "Gradient Descent(34/49): loss=4.971696213387162e+24\n",
      "Gradient Descent(35/49): loss=3.458702933046801e+25\n",
      "Gradient Descent(36/49): loss=2.4061458032885244e+26\n",
      "Gradient Descent(37/49): loss=1.673904275318283e+27\n",
      "Gradient Descent(38/49): loss=1.1644994742626656e+28\n",
      "Gradient Descent(39/49): loss=8.101174275931276e+28\n",
      "Gradient Descent(40/49): loss=5.635814021347519e+29\n",
      "Gradient Descent(41/49): loss=3.9207155162163827e+30\n",
      "Gradient Descent(42/49): loss=2.727558095578209e+31\n",
      "Gradient Descent(43/49): loss=1.89750394640565e+32\n",
      "Gradient Descent(44/49): loss=1.3200529926244302e+33\n",
      "Gradient Descent(45/49): loss=9.183326899728143e+33\n",
      "Gradient Descent(46/49): loss=6.3886445027939805e+34\n",
      "Gradient Descent(47/49): loss=4.444443612726838e+35\n",
      "Gradient Descent(48/49): loss=3.091904553159952e+36\n",
      "Gradient Descent(49/49): loss=2.1509719998418214e+37\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.4472705983369203\n",
      "Gradient Descent(2/49): loss=0.43102712035427376\n",
      "Gradient Descent(3/49): loss=0.4383455999529585\n",
      "Gradient Descent(4/49): loss=0.5794433125156373\n",
      "Gradient Descent(5/49): loss=1.6491232571070722\n",
      "Gradient Descent(6/49): loss=9.283313018910798\n",
      "Gradient Descent(7/49): loss=63.412612069193294\n",
      "Gradient Descent(8/49): loss=446.9255726842276\n",
      "Gradient Descent(9/49): loss=3163.933415475258\n",
      "Gradient Descent(10/49): loss=22412.46108513527\n",
      "Gradient Descent(11/49): loss=158777.692351357\n",
      "Gradient Descent(12/49): loss=1124850.2261661487\n",
      "Gradient Descent(13/49): loss=7968941.773552635\n",
      "Gradient Descent(14/49): loss=56455558.86336145\n",
      "Gradient Descent(15/49): loss=399956521.71064764\n",
      "Gradient Descent(16/49): loss=2833471552.377922\n",
      "Gradient Descent(17/49): loss=20073584521.436222\n",
      "Gradient Descent(18/49): loss=142210284496.6547\n",
      "Gradient Descent(19/49): loss=1007481498647.5446\n",
      "Gradient Descent(20/49): loss=7137451230841.582\n",
      "Gradient Descent(21/49): loss=50564908776034.24\n",
      "Gradient Descent(22/49): loss=358224514162777.9\n",
      "Gradient Descent(23/49): loss=2537823278106741.5\n",
      "Gradient Descent(24/49): loss=1.7979079421610556e+16\n",
      "Gradient Descent(25/49): loss=1.273718700735237e+17\n",
      "Gradient Descent(26/49): loss=9.023595093821225e+17\n",
      "Gradient Descent(27/49): loss=6.392719865872282e+18\n",
      "Gradient Descent(28/49): loss=4.5288897450089054e+19\n",
      "Gradient Descent(29/49): loss=3.208468813398934e+20\n",
      "Gradient Descent(30/49): loss=2.273023347035177e+21\n",
      "Gradient Descent(31/49): loss=1.610311783175364e+22\n",
      "Gradient Descent(32/49): loss=1.1408171598482226e+23\n",
      "Gradient Descent(33/49): loss=8.082060913929553e+23\n",
      "Gradient Descent(34/49): loss=5.725694784005422e+24\n",
      "Gradient Descent(35/49): loss=4.056339231876379e+25\n",
      "Gradient Descent(36/49): loss=2.8736928154156098e+26\n",
      "Gradient Descent(37/49): loss=2.0358530007736364e+27\n",
      "Gradient Descent(38/49): loss=1.4422896624598436e+28\n",
      "Gradient Descent(39/49): loss=1.0217827464203392e+29\n",
      "Gradient Descent(40/49): loss=7.238767690407303e+29\n",
      "Gradient Descent(41/49): loss=5.128268006017738e+30\n",
      "Gradient Descent(42/49): loss=3.633095281728172e+31\n",
      "Gradient Descent(43/49): loss=2.5738478001982252e+32\n",
      "Gradient Descent(44/49): loss=1.8234293308801288e+33\n",
      "Gradient Descent(45/49): loss=1.2917991982501058e+34\n",
      "Gradient Descent(46/49): loss=9.151685455197437e+34\n",
      "Gradient Descent(47/49): loss=6.483464828305202e+35\n",
      "Gradient Descent(48/49): loss=4.593177550261862e+36\n",
      "Gradient Descent(49/49): loss=3.254013180749162e+37\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4515593818544146\n",
      "Gradient Descent(2/49): loss=0.44090449148318456\n",
      "Gradient Descent(3/49): loss=0.4764088528981599\n",
      "Gradient Descent(4/49): loss=0.8289216487795902\n",
      "Gradient Descent(5/49): loss=3.5544718937290933\n",
      "Gradient Descent(6/49): loss=24.155954735650994\n",
      "Gradient Descent(7/49): loss=179.4960503364276\n",
      "Gradient Descent(8/49): loss=1350.4816002457424\n",
      "Gradient Descent(9/49): loss=10177.345523148892\n",
      "Gradient Descent(10/49): loss=76713.83128817136\n",
      "Gradient Descent(11/49): loss=578262.545338426\n",
      "Gradient Descent(12/49): loss=4358911.595322659\n",
      "Gradient Descent(13/49): loss=32857254.365463734\n",
      "Gradient Descent(14/49): loss=247676331.57993436\n",
      "Gradient Descent(15/49): loss=1866971751.8268867\n",
      "Gradient Descent(16/49): loss=14073139341.847734\n",
      "Gradient Descent(17/49): loss=106082617891.98956\n",
      "Gradient Descent(18/49): loss=799645448373.6016\n",
      "Gradient Descent(19/49): loss=6027687248040.12\n",
      "Gradient Descent(20/49): loss=45436403888869.02\n",
      "Gradient Descent(21/49): loss=342497331629764.94\n",
      "Gradient Descent(22/49): loss=2581727692632130.0\n",
      "Gradient Descent(23/49): loss=1.9460933745634476e+16\n",
      "Gradient Descent(24/49): loss=1.4669554164554474e+17\n",
      "Gradient Descent(25/49): loss=1.1057836288820312e+18\n",
      "Gradient Descent(26/49): loss=8.335341484733236e+18\n",
      "Gradient Descent(27/49): loss=6.283138568198829e+19\n",
      "Gradient Descent(28/49): loss=4.736198311669964e+20\n",
      "Gradient Descent(29/49): loss=3.5701225118605246e+21\n",
      "Gradient Descent(30/49): loss=2.6911404276058346e+22\n",
      "Gradient Descent(31/49): loss=2.0285681449403e+23\n",
      "Gradient Descent(32/49): loss=1.5291244843463748e+24\n",
      "Gradient Descent(33/49): loss=1.1526463601726389e+25\n",
      "Gradient Descent(34/49): loss=8.688590400716018e+25\n",
      "Gradient Descent(35/49): loss=6.549415827774753e+26\n",
      "Gradient Descent(36/49): loss=4.9369167732398395e+27\n",
      "Gradient Descent(37/49): loss=3.7214230805953767e+28\n",
      "Gradient Descent(38/49): loss=2.8051900367968315e+29\n",
      "Gradient Descent(39/49): loss=2.1145381678251152e+30\n",
      "Gradient Descent(40/49): loss=1.5939282560318005e+31\n",
      "Gradient Descent(41/49): loss=1.2014951179574131e+32\n",
      "Gradient Descent(42/49): loss=9.056809884714742e+32\n",
      "Gradient Descent(43/49): loss=6.826977826369612e+33\n",
      "Gradient Descent(44/49): loss=5.146141614433611e+34\n",
      "Gradient Descent(45/49): loss=3.87913571558913e+35\n",
      "Gradient Descent(46/49): loss=2.924073029345718e+36\n",
      "Gradient Descent(47/49): loss=2.2041515708218178e+37\n",
      "Gradient Descent(48/49): loss=1.661478389355935e+38\n",
      "Gradient Descent(49/49): loss=1.2524140693588952e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45229865451820983\n",
      "Gradient Descent(2/49): loss=0.44182239461957346\n",
      "Gradient Descent(3/49): loss=0.4714204093994062\n",
      "Gradient Descent(4/49): loss=0.7648183106650223\n",
      "Gradient Descent(5/49): loss=2.96468985457229\n",
      "Gradient Descent(6/49): loss=19.03964747751547\n",
      "Gradient Descent(7/49): loss=136.1646390844059\n",
      "Gradient Descent(8/49): loss=989.2739656893367\n",
      "Gradient Descent(9/49): loss=7202.868717184366\n",
      "Gradient Descent(10/49): loss=52459.18237639134\n",
      "Gradient Descent(11/49): loss=382080.42431904195\n",
      "Gradient Descent(12/49): loss=2782853.8717114707\n",
      "Gradient Descent(13/49): loss=20268720.582064103\n",
      "Gradient Descent(14/49): loss=147625816.54771227\n",
      "Gradient Descent(15/49): loss=1075222381.2297862\n",
      "Gradient Descent(16/49): loss=7831307546.917321\n",
      "Gradient Descent(17/49): loss=57038784702.24139\n",
      "Gradient Descent(18/49): loss=415438027549.07587\n",
      "Gradient Descent(19/49): loss=3025814025240.0547\n",
      "Gradient Descent(20/49): loss=22038306337433.465\n",
      "Gradient Descent(21/49): loss=160514473847762.6\n",
      "Gradient Descent(22/49): loss=1169096023992635.5\n",
      "Gradient Descent(23/49): loss=8515029707611427.0\n",
      "Gradient Descent(24/49): loss=6.20186275836336e+16\n",
      "Gradient Descent(25/49): loss=4.5170836737295366e+17\n",
      "Gradient Descent(26/49): loss=3.289986526702743e+18\n",
      "Gradient Descent(27/49): loss=2.3962388407448465e+19\n",
      "Gradient Descent(28/49): loss=1.745283919946322e+20\n",
      "Gradient Descent(29/49): loss=1.271165423675503e+21\n",
      "Gradient Descent(30/49): loss=9.258445092405704e+21\n",
      "Gradient Descent(31/49): loss=6.7433241915312025e+22\n",
      "Gradient Descent(32/49): loss=4.911453348617758e+23\n",
      "Gradient Descent(33/49): loss=3.577222940867093e+24\n",
      "Gradient Descent(34/49): loss=2.6054454884045745e+25\n",
      "Gradient Descent(35/49): loss=1.8976581290184575e+26\n",
      "Gradient Descent(36/49): loss=1.3821461207522346e+27\n",
      "Gradient Descent(37/49): loss=1.0066765292958872e+28\n",
      "Gradient Descent(38/49): loss=7.332058596551793e+28\n",
      "Gradient Descent(39/49): loss=5.340253964287035e+29\n",
      "Gradient Descent(40/49): loss=3.8895368916575574e+30\n",
      "Gradient Descent(41/49): loss=2.832917185724998e+31\n",
      "Gradient Descent(42/49): loss=2.0633355601767087e+32\n",
      "Gradient Descent(43/49): loss=1.5028161272565067e+33\n",
      "Gradient Descent(44/49): loss=1.0945656905893011e+34\n",
      "Gradient Descent(45/49): loss=7.972193199725817e+34\n",
      "Gradient Descent(46/49): loss=5.806491557353276e+35\n",
      "Gradient Descent(47/49): loss=4.2291178049692144e+36\n",
      "Gradient Descent(48/49): loss=3.0802485858535254e+37\n",
      "Gradient Descent(49/49): loss=2.2434776679675117e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4519145912829905\n",
      "Gradient Descent(2/49): loss=0.44071886926756526\n",
      "Gradient Descent(3/49): loss=0.46628724407142\n",
      "Gradient Descent(4/49): loss=0.7319714514989012\n",
      "Gradient Descent(5/49): loss=2.7321879065652834\n",
      "Gradient Descent(6/49): loss=17.359160900150986\n",
      "Gradient Descent(7/49): loss=123.97402139169769\n",
      "Gradient Descent(8/49): loss=900.7893153381787\n",
      "Gradient Descent(9/49): loss=6560.558915347237\n",
      "Gradient Descent(10/49): loss=47796.651150235586\n",
      "Gradient Descent(11/49): loss=348235.430649337\n",
      "Gradient Descent(12/49): loss=2537178.3892607866\n",
      "Gradient Descent(13/49): loss=18485423.2965593\n",
      "Gradient Descent(14/49): loss=134681468.68148762\n",
      "Gradient Descent(15/49): loss=981264966.7358847\n",
      "Gradient Descent(16/49): loss=7149320142.738659\n",
      "Gradient Descent(17/49): loss=52088661320.04706\n",
      "Gradient Descent(18/49): loss=379508622365.0266\n",
      "Gradient Descent(19/49): loss=2765031598051.7764\n",
      "Gradient Descent(20/49): loss=20145523151979.48\n",
      "Gradient Descent(21/49): loss=146776660112284.62\n",
      "Gradient Descent(22/49): loss=1069388359448024.4\n",
      "Gradient Descent(23/49): loss=7791371342338077.0\n",
      "Gradient Descent(24/49): loss=5.6766530940675704e+16\n",
      "Gradient Descent(25/49): loss=4.135907394797272e+17\n",
      "Gradient Descent(26/49): loss=3.013347776388719e+18\n",
      "Gradient Descent(27/49): loss=2.1954710187392193e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=1.5995807161364123e+20\n",
      "Gradient Descent(29/49): loss=1.1654257540164565e+21\n",
      "Gradient Descent(30/49): loss=8.491082534462268e+21\n",
      "Gradient Descent(31/49): loss=6.186450089898456e+22\n",
      "Gradient Descent(32/49): loss=4.507336321307735e+23\n",
      "Gradient Descent(33/49): loss=3.2839642150437337e+24\n",
      "Gradient Descent(34/49): loss=2.3926372910549727e+25\n",
      "Gradient Descent(35/49): loss=1.7432325176755295e+26\n",
      "Gradient Descent(36/49): loss=1.2700878741806372e+27\n",
      "Gradient Descent(37/49): loss=9.253631926804919e+27\n",
      "Gradient Descent(38/49): loss=6.742029868762062e+28\n",
      "Gradient Descent(39/49): loss=4.912121760496035e+29\n",
      "Gradient Descent(40/49): loss=3.5788836091836355e+30\n",
      "Gradient Descent(41/49): loss=2.6075102598413064e+31\n",
      "Gradient Descent(42/49): loss=1.8997850999487553e+32\n",
      "Gradient Descent(43/49): loss=1.3841492712695245e+33\n",
      "Gradient Descent(44/49): loss=1.0084662761107394e+34\n",
      "Gradient Descent(45/49): loss=7.347503995142653e+34\n",
      "Gradient Descent(46/49): loss=5.353259324331682e+35\n",
      "Gradient Descent(47/49): loss=3.900288507837308e+36\n",
      "Gradient Descent(48/49): loss=2.841680091084905e+37\n",
      "Gradient Descent(49/49): loss=2.0703970293074277e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45144067351497813\n",
      "Gradient Descent(2/49): loss=0.43849559773813185\n",
      "Gradient Descent(3/49): loss=0.451255141567337\n",
      "Gradient Descent(4/49): loss=0.6258701094529738\n",
      "Gradient Descent(5/49): loss=1.9859711582721145\n",
      "Gradient Descent(6/49): loss=12.129872081166804\n",
      "Gradient Descent(7/49): loss=87.42942505982994\n",
      "Gradient Descent(8/49): loss=646.0908679265256\n",
      "Gradient Descent(9/49): loss=4790.65256479222\n",
      "Gradient Descent(10/49): loss=35537.84643445222\n",
      "Gradient Descent(11/49): loss=263641.3800134569\n",
      "Gradient Descent(12/49): loss=1955867.9091358918\n",
      "Gradient Descent(13/49): loss=14509950.477419833\n",
      "Gradient Descent(14/49): loss=107644637.95479536\n",
      "Gradient Descent(15/49): loss=798580830.4671092\n",
      "Gradient Descent(16/49): loss=5924413482.571472\n",
      "Gradient Descent(17/49): loss=43951311859.08999\n",
      "Gradient Descent(18/49): loss=326060599906.9648\n",
      "Gradient Descent(19/49): loss=2418938373289.113\n",
      "Gradient Descent(20/49): loss=17945323217355.625\n",
      "Gradient Descent(21/49): loss=133130562122393.05\n",
      "Gradient Descent(22/49): loss=987652680107941.5\n",
      "Gradient Descent(23/49): loss=7327076525280489.0\n",
      "Gradient Descent(24/49): loss=5.435721634598196e+16\n",
      "Gradient Descent(25/49): loss=4.0325864738677037e+17\n",
      "Gradient Descent(26/49): loss=2.9916457762875515e+18\n",
      "Gradient Descent(27/49): loss=2.219405463162932e+19\n",
      "Gradient Descent(28/49): loss=1.6465052944971543e+20\n",
      "Gradient Descent(29/49): loss=1.221489146441801e+21\n",
      "Gradient Descent(30/49): loss=9.061833811660065e+21\n",
      "Gradient Descent(31/49): loss=6.722682085989127e+22\n",
      "Gradient Descent(32/49): loss=4.987340903463365e+23\n",
      "Gradient Descent(33/49): loss=3.699947278363562e+24\n",
      "Gradient Descent(34/49): loss=2.744871491171348e+25\n",
      "Gradient Descent(35/49): loss=2.0363315842645598e+26\n",
      "Gradient Descent(36/49): loss=1.5106886914053662e+27\n",
      "Gradient Descent(37/49): loss=1.1207311913124308e+28\n",
      "Gradient Descent(38/49): loss=8.314343056425181e+28\n",
      "Gradient Descent(39/49): loss=6.16814281567134e+29\n",
      "Gradient Descent(40/49): loss=4.57594611339941e+30\n",
      "Gradient Descent(41/49): loss=3.394746759029975e+31\n",
      "Gradient Descent(42/49): loss=2.5184530744798377e+32\n",
      "Gradient Descent(43/49): loss=1.8683590672811718e+33\n",
      "Gradient Descent(44/49): loss=1.3860753014080337e+34\n",
      "Gradient Descent(45/49): loss=1.0282845384581824e+35\n",
      "Gradient Descent(46/49): loss=7.62851117077173e+35\n",
      "Gradient Descent(47/49): loss=5.659346271008342e+36\n",
      "Gradient Descent(48/49): loss=4.1984863754137076e+37\n",
      "Gradient Descent(49/49): loss=3.114721559773641e+38\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4559496062514306\n",
      "Gradient Descent(2/49): loss=0.4494848386737725\n",
      "Gradient Descent(3/49): loss=0.49523187539512487\n",
      "Gradient Descent(4/49): loss=0.9244715166389494\n",
      "Gradient Descent(5/49): loss=4.364118792793575\n",
      "Gradient Descent(6/49): loss=31.53075967727088\n",
      "Gradient Descent(7/49): loss=245.7692535320425\n",
      "Gradient Descent(8/49): loss=1934.99476443364\n",
      "Gradient Descent(9/49): loss=15253.942231724988\n",
      "Gradient Descent(10/49): loss=120268.9350894858\n",
      "Gradient Descent(11/49): loss=948273.2695643498\n",
      "Gradient Descent(12/49): loss=7476780.69215664\n",
      "Gradient Descent(13/49): loss=58951642.92130431\n",
      "Gradient Descent(14/49): loss=464811860.1357709\n",
      "Gradient Descent(15/49): loss=3664869301.332609\n",
      "Gradient Descent(16/49): loss=28896136601.183727\n",
      "Gradient Descent(17/49): loss=227835331054.54358\n",
      "Gradient Descent(18/49): loss=1796397172178.6304\n",
      "Gradient Descent(19/49): loss=14163926135946.465\n",
      "Gradient Descent(20/49): loss=111677309835266.22\n",
      "Gradient Descent(21/49): loss=880534211512863.2\n",
      "Gradient Descent(22/49): loss=6942686019105293.0\n",
      "Gradient Descent(23/49): loss=5.474050698957436e+16\n",
      "Gradient Descent(24/49): loss=4.316086162084359e+17\n",
      "Gradient Descent(25/49): loss=3.4030740274444216e+18\n",
      "Gradient Descent(26/49): loss=2.6831977864579224e+19\n",
      "Gradient Descent(27/49): loss=2.1156020419159903e+20\n",
      "Gradient Descent(28/49): loss=1.6680738268153106e+21\n",
      "Gradient Descent(29/49): loss=1.3152144101667932e+22\n",
      "Gradient Descent(30/49): loss=1.0369978336108076e+23\n",
      "Gradient Descent(31/49): loss=8.176343709442229e+23\n",
      "Gradient Descent(32/49): loss=6.446744080665637e+24\n",
      "Gradient Descent(33/49): loss=5.083018855188609e+25\n",
      "Gradient Descent(34/49): loss=4.007772041035594e+26\n",
      "Gradient Descent(35/49): loss=3.159979766061761e+27\n",
      "Gradient Descent(36/49): loss=2.491526968020803e+28\n",
      "Gradient Descent(37/49): loss=1.9644767029984638e+29\n",
      "Gradient Descent(38/49): loss=1.5489170962854639e+30\n",
      "Gradient Descent(39/49): loss=1.2212637429110164e+31\n",
      "Gradient Descent(40/49): loss=9.629212133598381e+31\n",
      "Gradient Descent(41/49): loss=7.592277004214454e+32\n",
      "Gradient Descent(42/49): loss=5.986229123314672e+33\n",
      "Gradient Descent(43/49): loss=4.719919873435748e+34\n",
      "Gradient Descent(44/49): loss=3.721481946771615e+35\n",
      "Gradient Descent(45/49): loss=2.9342506338070024e+36\n",
      "Gradient Descent(46/49): loss=2.3135479105214447e+37\n",
      "Gradient Descent(47/49): loss=1.824146810299417e+38\n",
      "Gradient Descent(48/49): loss=1.4382721751267472e+39\n",
      "Gradient Descent(49/49): loss=1.1340243219811114e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.456743998649439\n",
      "Gradient Descent(2/49): loss=0.45064250819632823\n",
      "Gradient Descent(3/49): loss=0.4898730953385455\n",
      "Gradient Descent(4/49): loss=0.8485998297631836\n",
      "Gradient Descent(5/49): loss=3.629694496922613\n",
      "Gradient Descent(6/49): loss=24.86112640894218\n",
      "Gradient Descent(7/49): loss=186.67771253576797\n",
      "Gradient Descent(8/49): loss=1419.7446321002985\n",
      "Gradient Descent(9/49): loss=10815.704803012792\n",
      "Gradient Descent(10/49): loss=82412.67439199843\n",
      "Gradient Descent(11/49): loss=627979.5478686889\n",
      "Gradient Descent(12/49): loss=4785183.480992751\n",
      "Gradient Descent(13/49): loss=36462958.12906142\n",
      "Gradient Descent(14/49): loss=277846691.60034525\n",
      "Gradient Descent(15/49): loss=2117183811.3483484\n",
      "Gradient Descent(16/49): loss=16132879862.639523\n",
      "Gradient Descent(17/49): loss=122932081429.8456\n",
      "Gradient Descent(18/49): loss=936738931524.9557\n",
      "Gradient Descent(19/49): loss=7137923767583.097\n",
      "Gradient Descent(20/49): loss=54390774203118.27\n",
      "Gradient Descent(21/49): loss=414456138050979.6\n",
      "Gradient Descent(22/49): loss=3158143874302104.5\n",
      "Gradient Descent(23/49): loss=2.4064965662459412e+16\n",
      "Gradient Descent(24/49): loss=1.833743475234556e+17\n",
      "Gradient Descent(25/49): loss=1.3973072640659707e+18\n",
      "Gradient Descent(26/49): loss=1.0647441240175722e+19\n",
      "Gradient Descent(27/49): loss=8.113319659779688e+19\n",
      "Gradient Descent(28/49): loss=6.182326290131382e+20\n",
      "Gradient Descent(29/49): loss=4.710914885694027e+21\n",
      "Gradient Descent(30/49): loss=3.589703619441483e+22\n",
      "Gradient Descent(31/49): loss=2.7353438531786805e+23\n",
      "Gradient Descent(32/49): loss=2.084324163866973e+24\n",
      "Gradient Descent(33/49): loss=1.588249029470739e+25\n",
      "Gradient Descent(34/49): loss=1.21024120112621e+26\n",
      "Gradient Descent(35/49): loss=9.22200321061418e+26\n",
      "Gradient Descent(36/49): loss=7.027139973208472e+27\n",
      "Gradient Descent(37/49): loss=5.354660487021842e+28\n",
      "Gradient Descent(38/49): loss=4.080235919675398e+29\n",
      "Gradient Descent(39/49): loss=3.1091280578031924e+30\n",
      "Gradient Descent(40/49): loss=2.3691466547817256e+31\n",
      "Gradient Descent(41/49): loss=1.8052829499178273e+32\n",
      "Gradient Descent(42/49): loss=1.375620425475231e+33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=1.0482188152670387e+34\n",
      "Gradient Descent(44/49): loss=7.987397281486623e+34\n",
      "Gradient Descent(45/49): loss=6.086373799352987e+35\n",
      "Gradient Descent(46/49): loss=4.637799363168232e+36\n",
      "Gradient Descent(47/49): loss=3.5339898011670585e+37\n",
      "Gradient Descent(48/49): loss=2.69289008358938e+38\n",
      "Gradient Descent(49/49): loss=2.051974513310443e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4564651089979701\n",
      "Gradient Descent(2/49): loss=0.4496229375048283\n",
      "Gradient Descent(3/49): loss=0.4842252137150779\n",
      "Gradient Descent(4/49): loss=0.8095030631896875\n",
      "Gradient Descent(5/49): loss=3.3385264995858863\n",
      "Gradient Descent(6/49): loss=22.657520814256344\n",
      "Gradient Descent(7/49): loss=169.95139535607584\n",
      "Gradient Descent(8/49): loss=1292.7235861786141\n",
      "Gradient Descent(9/49): loss=9851.032927595457\n",
      "Gradient Descent(10/49): loss=75086.39618448091\n",
      "Gradient Descent(11/49): loss=572340.1228636164\n",
      "Gradient Descent(12/49): loss=4362634.937268549\n",
      "Gradient Descent(13/49): loss=33253991.487827167\n",
      "Gradient Descent(14/49): loss=253477093.84287268\n",
      "Gradient Descent(15/49): loss=1932118064.638692\n",
      "Gradient Descent(16/49): loss=14727485483.759777\n",
      "Gradient Descent(17/49): loss=112259614294.20396\n",
      "Gradient Descent(18/49): loss=855694002593.254\n",
      "Gradient Descent(19/49): loss=6522490128613.72\n",
      "Gradient Descent(20/49): loss=49717395878606.7\n",
      "Gradient Descent(21/49): loss=378968676718504.5\n",
      "Gradient Descent(22/49): loss=2888672171898085.5\n",
      "Gradient Descent(23/49): loss=2.201877735371924e+16\n",
      "Gradient Descent(24/49): loss=1.6783716784106016e+17\n",
      "Gradient Descent(25/49): loss=1.2793314749672166e+18\n",
      "Gradient Descent(26/49): loss=9.751648242728403e+18\n",
      "Gradient Descent(27/49): loss=7.433151244273751e+19\n",
      "Gradient Descent(28/49): loss=5.665887042372289e+20\n",
      "Gradient Descent(29/49): loss=4.3187976299623885e+21\n",
      "Gradient Descent(30/49): loss=3.2919846140736726e+22\n",
      "Gradient Descent(31/49): loss=2.509300881364097e+23\n",
      "Gradient Descent(32/49): loss=1.9127036275612662e+24\n",
      "Gradient Descent(33/49): loss=1.4579499788392576e+25\n",
      "Gradient Descent(34/49): loss=1.1113159980292516e+26\n",
      "Gradient Descent(35/49): loss=8.470957614464607e+26\n",
      "Gradient Descent(36/49): loss=6.45695041134203e+27\n",
      "Gradient Descent(37/49): loss=4.921782224873591e+28\n",
      "Gradient Descent(38/49): loss=3.751606985633826e+29\n",
      "Gradient Descent(39/49): loss=2.859646024874213e+30\n",
      "Gradient Descent(40/49): loss=2.1797526816891e+31\n",
      "Gradient Descent(41/49): loss=1.661506952959216e+32\n",
      "Gradient Descent(42/49): loss=1.2664764117150627e+33\n",
      "Gradient Descent(43/49): loss=9.653661085040292e+33\n",
      "Gradient Descent(44/49): loss=7.358460961670923e+34\n",
      "Gradient Descent(45/49): loss=5.60895470096241e+35\n",
      "Gradient Descent(46/49): loss=4.275401201599191e+36\n",
      "Gradient Descent(47/49): loss=3.258905876258711e+37\n",
      "Gradient Descent(48/49): loss=2.484086758066388e+38\n",
      "Gradient Descent(49/49): loss=1.893484272299617e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45577954182137953\n",
      "Gradient Descent(2/49): loss=0.44679483582435686\n",
      "Gradient Descent(3/49): loss=0.46638979798810126\n",
      "Gradient Descent(4/49): loss=0.6814405950896877\n",
      "Gradient Descent(5/49): loss=2.401080677714996\n",
      "Gradient Descent(6/49): loss=15.789059333189615\n",
      "Gradient Descent(7/49): loss=119.72436158218837\n",
      "Gradient Descent(8/49): loss=926.3573110210743\n",
      "Gradient Descent(9/49): loss=7186.348228610465\n",
      "Gradient Descent(10/49): loss=55767.715459296385\n",
      "Gradient Descent(11/49): loss=432788.7375656383\n",
      "Gradient Descent(12/49): loss=3358701.5262642843\n",
      "Gradient Descent(13/49): loss=26065566.907473467\n",
      "Gradient Descent(14/49): loss=202284672.63842392\n",
      "Gradient Descent(15/49): loss=1569852265.111451\n",
      "Gradient Descent(16/49): loss=12183009744.296627\n",
      "Gradient Descent(17/49): loss=94547576071.98816\n",
      "Gradient Descent(18/49): loss=733746777596.3735\n",
      "Gradient Descent(19/49): loss=5694321906516.255\n",
      "Gradient Descent(20/49): loss=44191406306768.8\n",
      "Gradient Descent(21/49): loss=342952229155716.8\n",
      "Gradient Descent(22/49): loss=2661518184472438.5\n",
      "Gradient Descent(23/49): loss=2.0655002195834304e+16\n",
      "Gradient Descent(24/49): loss=1.602953976414326e+17\n",
      "Gradient Descent(25/49): loss=1.243989918829803e+18\n",
      "Gradient Descent(26/49): loss=9.65411946269271e+18\n",
      "Gradient Descent(27/49): loss=7.492184718636183e+19\n",
      "Gradient Descent(28/49): loss=5.81439167757186e+20\n",
      "Gradient Descent(29/49): loss=4.5123220862567386e+21\n",
      "Gradient Descent(30/49): loss=3.5018367766072232e+22\n",
      "Gradient Descent(31/49): loss=2.7176386294205815e+23\n",
      "Gradient Descent(32/49): loss=2.1090531030616932e+24\n",
      "Gradient Descent(33/49): loss=1.63675366672366e+25\n",
      "Gradient Descent(34/49): loss=1.2702205371900468e+26\n",
      "Gradient Descent(35/49): loss=9.857685037780596e+26\n",
      "Gradient Descent(36/49): loss=7.650164003728716e+27\n",
      "Gradient Descent(37/49): loss=5.936993225046555e+28\n",
      "Gradient Descent(38/49): loss=4.607468354543706e+29\n",
      "Gradient Descent(39/49): loss=3.5756760759911933e+30\n",
      "Gradient Descent(40/49): loss=2.774942423165449e+31\n",
      "Gradient Descent(41/49): loss=2.153524337281851e+32\n",
      "Gradient Descent(42/49): loss=1.671266053143845e+33\n",
      "Gradient Descent(43/49): loss=1.2970042511414155e+34\n",
      "Gradient Descent(44/49): loss=1.0065542971536386e+35\n",
      "Gradient Descent(45/49): loss=7.811474420587605e+35\n",
      "Gradient Descent(46/49): loss=6.062179933665164e+36\n",
      "Gradient Descent(47/49): loss=4.704620865335758e+37\n",
      "Gradient Descent(48/49): loss=3.65107234175594e+38\n",
      "Gradient Descent(49/49): loss=2.833454517654035e+39\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4605102427150951\n",
      "Gradient Descent(2/49): loss=0.45897388594646543\n",
      "Gradient Descent(3/49): loss=0.5170068833722989\n",
      "Gradient Descent(4/49): loss=1.037416353005512\n",
      "Gradient Descent(5/49): loss=5.355056675347622\n",
      "Gradient Descent(6/49): loss=40.95066947699877\n",
      "Gradient Descent(7/49): loss=334.2432161789364\n",
      "Gradient Descent(8/49): loss=2750.7233285590296\n",
      "Gradient Descent(9/49): loss=22660.361664687607\n",
      "Gradient Descent(10/49): loss=186697.93178287786\n",
      "Gradient Descent(11/49): loss=1538220.3890021702\n",
      "Gradient Descent(12/49): loss=12673553.253620517\n",
      "Gradient Descent(13/49): loss=104418708.7269509\n",
      "Gradient Descent(14/49): loss=860316503.1525806\n",
      "Gradient Descent(15/49): loss=7088236360.806661\n",
      "Gradient Descent(16/49): loss=58400710136.21113\n",
      "Gradient Descent(17/49): loss=481169471637.7863\n",
      "Gradient Descent(18/49): loss=3964404883048.502\n",
      "Gradient Descent(19/49): loss=32663140542259.6\n",
      "Gradient Descent(20/49): loss=269114982338305.06\n",
      "Gradient Descent(21/49): loss=2217266083928631.0\n",
      "Gradient Descent(22/49): loss=1.826828385481723e+16\n",
      "Gradient Descent(23/49): loss=1.5051427405088813e+17\n",
      "Gradient Descent(24/49): loss=1.240102621193546e+18\n",
      "Gradient Descent(25/49): loss=1.0217333344551504e+19\n",
      "Gradient Descent(26/49): loss=8.418166278304385e+19\n",
      "Gradient Descent(27/49): loss=6.935813983887411e+20\n",
      "Gradient Descent(28/49): loss=5.714488646187306e+21\n",
      "Gradient Descent(29/49): loss=4.708226109187145e+22\n",
      "Gradient Descent(30/49): loss=3.879156030876416e+23\n",
      "Gradient Descent(31/49): loss=3.1960766460476443e+24\n",
      "Gradient Descent(32/49): loss=2.6332804986715295e+25\n",
      "Gradient Descent(33/49): loss=2.1695869506942944e+26\n",
      "Gradient Descent(34/49): loss=1.787545056061283e+27\n",
      "Gradient Descent(35/49): loss=1.4727768004074542e+28\n",
      "Gradient Descent(36/49): loss=1.2134359894668837e+29\n",
      "Gradient Descent(37/49): loss=9.997624216555379e+29\n",
      "Gradient Descent(38/49): loss=8.237145662654047e+30\n",
      "Gradient Descent(39/49): loss=6.786669232418842e+31\n",
      "Gradient Descent(40/49): loss=5.591606747842755e+32\n",
      "Gradient Descent(41/49): loss=4.6069824462885267e+33\n",
      "Gradient Descent(42/49): loss=3.7957403332413406e+34\n",
      "Gradient Descent(43/49): loss=3.1273495927908618e+35\n",
      "Gradient Descent(44/49): loss=2.576655570950906e+36\n",
      "Gradient Descent(45/49): loss=2.1229330889699026e+37\n",
      "Gradient Descent(46/49): loss=1.7491064584080283e+38\n",
      "Gradient Descent(47/49): loss=1.441106843517646e+39\n",
      "Gradient Descent(48/49): loss=1.1873427854835785e+40\n",
      "Gradient Descent(49/49): loss=9.782639618854821e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4613611580775839\n",
      "Gradient Descent(2/49): loss=0.4603948734673494\n",
      "Gradient Descent(3/49): loss=0.5112688241231663\n",
      "Gradient Descent(4/49): loss=0.9479316749866076\n",
      "Gradient Descent(5/49): loss=4.444838133405924\n",
      "Gradient Descent(6/49): loss=32.3060203293709\n",
      "Gradient Descent(7/49): loss=254.20143351906827\n",
      "Gradient Descent(8/49): loss=2021.4031315995958\n",
      "Gradient Descent(9/49): loss=16095.594871026175\n",
      "Gradient Descent(10/49): loss=128184.03836964615\n",
      "Gradient Descent(11/49): loss=1020869.0155254405\n",
      "Gradient Descent(12/49): loss=8130312.66588948\n",
      "Gradient Descent(13/49): loss=64750722.28031893\n",
      "Gradient Descent(14/49): loss=515682038.8493282\n",
      "Gradient Descent(15/49): loss=4106949810.5091467\n",
      "Gradient Descent(16/49): loss=32708210654.23663\n",
      "Gradient Descent(17/49): loss=260491871985.93015\n",
      "Gradient Descent(18/49): loss=2074586595053.189\n",
      "Gradient Descent(19/49): loss=16522241203029.195\n",
      "Gradient Descent(20/49): loss=131584989039286.03\n",
      "Gradient Descent(21/49): loss=1047957666741668.8\n",
      "Gradient Descent(22/49): loss=8346052838555491.0\n",
      "Gradient Descent(23/49): loss=6.646890441723422e+16\n",
      "Gradient Descent(24/49): loss=5.2936583794646074e+17\n",
      "Gradient Descent(25/49): loss=4.215929130195046e+18\n",
      "Gradient Descent(26/49): loss=3.3576134228412846e+19\n",
      "Gradient Descent(27/49): loss=2.6740411304595705e+20\n",
      "Gradient Descent(28/49): loss=2.1296364610486938e+21\n",
      "Gradient Descent(29/49): loss=1.696066453341562e+22\n",
      "Gradient Descent(30/49): loss=1.3507664180083564e+23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=1.0757655824300526e+24\n",
      "Gradient Descent(32/49): loss=8.5675182097541e+24\n",
      "Gradient Descent(33/49): loss=6.823267956635961e+25\n",
      "Gradient Descent(34/49): loss=5.434127418025305e+26\n",
      "Gradient Descent(35/49): loss=4.32780025392604e+27\n",
      "Gradient Descent(36/49): loss=3.446708845242393e+28\n",
      "Gradient Descent(37/49): loss=2.744997727909206e+29\n",
      "Gradient Descent(38/49): loss=2.1861470940974603e+30\n",
      "Gradient Descent(39/49): loss=1.7410721577066685e+31\n",
      "Gradient Descent(40/49): loss=1.386609467645594e+32\n",
      "Gradient Descent(41/49): loss=1.1043113906875273e+33\n",
      "Gradient Descent(42/49): loss=8.794860240445999e+33\n",
      "Gradient Descent(43/49): loss=7.004325709329064e+34\n",
      "Gradient Descent(44/49): loss=5.578323850645029e+35\n",
      "Gradient Descent(45/49): loss=4.442639916249025e+36\n",
      "Gradient Descent(46/49): loss=3.538168445198361e+37\n",
      "Gradient Descent(47/49): loss=2.8178371829799355e+38\n",
      "Gradient Descent(48/49): loss=2.244157256153175e+39\n",
      "Gradient Descent(49/49): loss=1.787272103854879e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4611911001947068\n",
      "Gradient Descent(2/49): loss=0.45947044168956847\n",
      "Gradient Descent(3/49): loss=0.5050550468551959\n",
      "Gradient Descent(4/49): loss=0.901537854224443\n",
      "Gradient Descent(5/49): loss=4.081947513567873\n",
      "Gradient Descent(6/49): loss=29.43360996905746\n",
      "Gradient Descent(7/49): loss=231.41154963485118\n",
      "Gradient Descent(8/49): loss=1840.5131505438042\n",
      "Gradient Descent(9/49): loss=14659.736369497348\n",
      "Gradient Descent(10/49): loss=116786.57391357998\n",
      "Gradient Descent(11/49): loss=930399.9087103854\n",
      "Gradient Descent(12/49): loss=7412208.959620893\n",
      "Gradient Descent(13/49): loss=59050802.86518732\n",
      "Gradient Descent(14/49): loss=470439716.0229244\n",
      "Gradient Descent(15/49): loss=3747849596.3263545\n",
      "Gradient Descent(16/49): loss=29857973568.992386\n",
      "Gradient Descent(17/49): loss=237869360232.9721\n",
      "Gradient Descent(18/49): loss=1895032575052.2402\n",
      "Gradient Descent(19/49): loss=15097146000632.162\n",
      "Gradient Descent(20/49): loss=120274353256523.06\n",
      "Gradient Descent(21/49): loss=958189054452347.8\n",
      "Gradient Descent(22/49): loss=7633599676184682.0\n",
      "Gradient Descent(23/49): loss=6.081455819754897e+16\n",
      "Gradient Descent(24/49): loss=4.8449101939433344e+17\n",
      "Gradient Descent(25/49): loss=3.859792043728577e+18\n",
      "Gradient Descent(26/49): loss=3.074978487620691e+19\n",
      "Gradient Descent(27/49): loss=2.4497414866413168e+20\n",
      "Gradient Descent(28/49): loss=1.951634255501864e+21\n",
      "Gradient Descent(29/49): loss=1.554807430914011e+22\n",
      "Gradient Descent(30/49): loss=1.2386676142881257e+23\n",
      "Gradient Descent(31/49): loss=9.868086736530695e+23\n",
      "Gradient Descent(32/49): loss=7.861603445219731e+24\n",
      "Gradient Descent(33/49): loss=6.2630994619345405e+25\n",
      "Gradient Descent(34/49): loss=4.9896201383621535e+26\n",
      "Gradient Descent(35/49): loss=3.97507803867102e+27\n",
      "Gradient Descent(36/49): loss=3.166823320284126e+28\n",
      "Gradient Descent(37/49): loss=2.5229114609403557e+29\n",
      "Gradient Descent(38/49): loss=2.0099265402571333e+30\n",
      "Gradient Descent(39/49): loss=1.6012471146032e+31\n",
      "Gradient Descent(40/49): loss=1.275664692549933e+32\n",
      "Gradient Descent(41/49): loss=1.0162831164393779e+33\n",
      "Gradient Descent(42/49): loss=8.096417332796145e+33\n",
      "Gradient Descent(43/49): loss=6.4501685176537025e+34\n",
      "Gradient Descent(44/49): loss=5.1386523441178525e+35\n",
      "Gradient Descent(45/49): loss=4.093807447268609e+36\n",
      "Gradient Descent(46/49): loss=3.2614114154845417e+37\n",
      "Gradient Descent(47/49): loss=2.5982669087549773e+38\n",
      "Gradient Descent(48/49): loss=2.069959925043165e+39\n",
      "Gradient Descent(49/49): loss=1.6490738795337054e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46028720325612493\n",
      "Gradient Descent(2/49): loss=0.4559822562417278\n",
      "Gradient Descent(3/49): loss=0.4840483679344828\n",
      "Gradient Descent(4/49): loss=0.7477114784469332\n",
      "Gradient Descent(5/49): loss=2.9103690714117656\n",
      "Gradient Descent(6/49): loss=20.466859885479504\n",
      "Gradient Descent(7/49): loss=162.86578837239864\n",
      "Gradient Descent(8/49): loss=1317.7634509561299\n",
      "Gradient Descent(9/49): loss=10684.270193231465\n",
      "Gradient Descent(10/49): loss=86648.93008313967\n",
      "Gradient Descent(11/49): loss=702740.8776365954\n",
      "Gradient Descent(12/49): loss=5699397.059295778\n",
      "Gradient Descent(13/49): loss=46223499.183463104\n",
      "Gradient Descent(14/49): loss=374883865.9023105\n",
      "Gradient Descent(15/49): loss=3040399719.197608\n",
      "Gradient Descent(16/49): loss=24658384373.410706\n",
      "Gradient Descent(17/49): loss=199985520369.28632\n",
      "Gradient Descent(18/49): loss=1621931419036.8591\n",
      "Gradient Descent(19/49): loss=13154259984451.336\n",
      "Gradient Descent(20/49): loss=106684261558562.95\n",
      "Gradient Descent(21/49): loss=865235420141391.1\n",
      "Gradient Descent(22/49): loss=7017270601402696.0\n",
      "Gradient Descent(23/49): loss=5.691177862929325e+16\n",
      "Gradient Descent(24/49): loss=4.615684260633067e+17\n",
      "Gradient Descent(25/49): loss=3.743432678958559e+18\n",
      "Gradient Descent(26/49): loss=3.0360153404368556e+19\n",
      "Gradient Descent(27/49): loss=2.4622825993847983e+20\n",
      "Gradient Descent(28/49): loss=1.9969713322860434e+21\n",
      "Gradient Descent(29/49): loss=1.6195925288870815e+22\n",
      "Gradient Descent(30/49): loss=1.3135291014038525e+23\n",
      "Gradient Descent(31/49): loss=1.0653041857512147e+24\n",
      "Gradient Descent(32/49): loss=8.639877159677339e+24\n",
      "Gradient Descent(33/49): loss=7.007151415787671e+25\n",
      "Gradient Descent(34/49): loss=5.682970956222362e+26\n",
      "Gradient Descent(35/49): loss=4.6090282588301495e+27\n",
      "Gradient Descent(36/49): loss=3.738034498915597e+28\n",
      "Gradient Descent(37/49): loss=3.0316372845651366e+29\n",
      "Gradient Descent(38/49): loss=2.4587318891335546e+30\n",
      "Gradient Descent(39/49): loss=1.994091619541957e+31\n",
      "Gradient Descent(40/49): loss=1.6172570115112336e+32\n",
      "Gradient Descent(41/49): loss=1.311634939764194e+33\n",
      "Gradient Descent(42/49): loss=1.063767974394292e+34\n",
      "Gradient Descent(43/49): loss=8.627418110334439e+34\n",
      "Gradient Descent(44/49): loss=6.99704682244369e+35\n",
      "Gradient Descent(45/49): loss=5.674775884203656e+36\n",
      "Gradient Descent(46/49): loss=4.602381855248571e+37\n",
      "Gradient Descent(47/49): loss=3.732644103264741e+38\n",
      "Gradient Descent(48/49): loss=3.0272655420253786e+39\n",
      "Gradient Descent(49/49): loss=2.4551862991488466e+40\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.465241291245408\n",
      "Gradient Descent(2/49): loss=0.4694320953205989\n",
      "Gradient Descent(3/49): loss=0.5421013104886352\n",
      "Gradient Descent(4/49): loss=1.1704754218527953\n",
      "Gradient Descent(5/49): loss=6.562699633986266\n",
      "Gradient Descent(6/49): loss=52.91926520616592\n",
      "Gradient Descent(7/49): loss=451.616257879601\n",
      "Gradient Descent(8/49): loss=3880.916125949892\n",
      "Gradient Descent(9/49): loss=33377.547571239236\n",
      "Gradient Descent(10/49): loss=287088.9518362099\n",
      "Gradient Descent(11/49): loss=2469354.7025068724\n",
      "Gradient Descent(12/49): loss=21239831.08067835\n",
      "Gradient Descent(13/49): loss=182691654.4591758\n",
      "Gradient Descent(14/49): loss=1571398619.4389474\n",
      "Gradient Descent(15/49): loss=13516181863.889692\n",
      "Gradient Descent(16/49): loss=116257689147.87277\n",
      "Gradient Descent(17/49): loss=999975468111.8894\n",
      "Gradient Descent(18/49): loss=8601159580571.648\n",
      "Gradient Descent(19/49): loss=73981761042776.19\n",
      "Gradient Descent(20/49): loss=636344543514104.6\n",
      "Gradient Descent(21/49): loss=5473435240694703.0\n",
      "Gradient Descent(22/49): loss=4.707904489702649e+16\n",
      "Gradient Descent(23/49): loss=4.049443121089836e+17\n",
      "Gradient Descent(24/49): loss=3.4830760961290496e+18\n",
      "Gradient Descent(25/49): loss=2.9959228290533118e+19\n",
      "Gradient Descent(26/49): loss=2.57690425070467e+20\n",
      "Gradient Descent(27/49): loss=2.2164908431230116e+21\n",
      "Gradient Descent(28/49): loss=1.9064859147577955e+22\n",
      "Gradient Descent(29/49): loss=1.6398391874467323e+23\n",
      "Gradient Descent(30/49): loss=1.4104864556670589e+24\n",
      "Gradient Descent(31/49): loss=1.2132116715163274e+25\n",
      "Gradient Descent(32/49): loss=1.0435283188929246e+26\n",
      "Gradient Descent(33/49): loss=8.975773790326685e+26\n",
      "Gradient Descent(34/49): loss=7.720395668858009e+27\n",
      "Gradient Descent(35/49): loss=6.640598423721207e+28\n",
      "Gradient Descent(36/49): loss=5.711824797141582e+29\n",
      "Gradient Descent(37/49): loss=4.9129521816437934e+30\n",
      "Gradient Descent(38/49): loss=4.225812239758805e+31\n",
      "Gradient Descent(39/49): loss=3.6347777111318395e+32\n",
      "Gradient Descent(40/49): loss=3.1264070100035964e+33\n",
      "Gradient Descent(41/49): loss=2.6891385303328006e+34\n",
      "Gradient Descent(42/49): loss=2.3130277063037575e+35\n",
      "Gradient Descent(43/49): loss=1.9895208483240796e+36\n",
      "Gradient Descent(44/49): loss=1.7112606109857168e+37\n",
      "Gradient Descent(45/49): loss=1.47191866884831e+38\n",
      "Gradient Descent(46/49): loss=1.2660517946803167e+39\n",
      "Gradient Descent(47/49): loss=1.0889780670201233e+40\n",
      "Gradient Descent(48/49): loss=9.366703917119984e+40\n",
      "Gradient Descent(49/49): loss=8.056649158331213e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46615013280264417\n",
      "Gradient Descent(2/49): loss=0.4711412889550238\n",
      "Gradient Descent(3/49): loss=0.5359781546474137\n",
      "Gradient Descent(4/49): loss=1.0652962334837215\n",
      "Gradient Descent(5/49): loss=5.439725972141945\n",
      "Gradient Descent(6/49): loss=41.77636341058917\n",
      "Gradient Descent(7/49): loss=343.8948362666861\n",
      "Gradient Descent(8/49): loss=2856.2061744205535\n",
      "Gradient Descent(9/49): loss=23748.14997763778\n",
      "Gradient Descent(10/49): loss=197482.43226162114\n",
      "Gradient Descent(11/49): loss=1642231.3415783679\n",
      "Gradient Descent(12/49): loss=13656552.84958937\n",
      "Gradient Descent(13/49): loss=113565901.3960579\n",
      "Gradient Descent(14/49): loss=944397499.3271832\n",
      "Gradient Descent(15/49): loss=7853472117.976982\n",
      "Gradient Descent(16/49): loss=65308330847.1498\n",
      "Gradient Descent(17/49): loss=543094571957.6355\n",
      "Gradient Descent(18/49): loss=4516295398550.78\n",
      "Gradient Descent(19/49): loss=37556855067557.914\n",
      "Gradient Descent(20/49): loss=312317339343731.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=2597185527895999.5\n",
      "Gradient Descent(22/49): loss=2.159781676062663e+16\n",
      "Gradient Descent(23/49): loss=1.796043000453193e+17\n",
      "Gradient Descent(24/49): loss=1.4935632129992077e+18\n",
      "Gradient Descent(25/49): loss=1.2420254251494525e+19\n",
      "Gradient Descent(26/49): loss=1.0328502625743924e+20\n",
      "Gradient Descent(27/49): loss=8.589032424771786e+20\n",
      "Gradient Descent(28/49): loss=7.142514328253453e+21\n",
      "Gradient Descent(29/49): loss=5.939610937102929e+22\n",
      "Gradient Descent(30/49): loss=4.939293988476946e+23\n",
      "Gradient Descent(31/49): loss=4.107444976270582e+24\n",
      "Gradient Descent(32/49): loss=3.415691447492126e+25\n",
      "Gradient Descent(33/49): loss=2.8404392832705536e+26\n",
      "Gradient Descent(34/49): loss=2.3620679578274104e+27\n",
      "Gradient Descent(35/49): loss=1.9642613275545327e+28\n",
      "Gradient Descent(36/49): loss=1.633451124952012e+29\n",
      "Gradient Descent(37/49): loss=1.358354176289138e+30\n",
      "Gradient Descent(38/49): loss=1.1295875585480922e+31\n",
      "Gradient Descent(39/49): loss=9.393485695405549e+31\n",
      "Gradient Descent(40/49): loss=7.811485957158042e+32\n",
      "Gradient Descent(41/49): loss=6.495918005040539e+33\n",
      "Gradient Descent(42/49): loss=5.401910847646579e+34\n",
      "Gradient Descent(43/49): loss=4.492150421738421e+35\n",
      "Gradient Descent(44/49): loss=3.735606895533403e+36\n",
      "Gradient Descent(45/49): loss=3.106476312642268e+37\n",
      "Gradient Descent(46/49): loss=2.583300478577213e+38\n",
      "Gradient Descent(47/49): loss=2.1482350711829076e+39\n",
      "Gradient Descent(48/49): loss=1.786441011926737e+40\n",
      "Gradient Descent(49/49): loss=1.4855783391230399e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4660925648732007\n",
      "Gradient Descent(2/49): loss=0.47032413268300155\n",
      "Gradient Descent(3/49): loss=0.5291441909293149\n",
      "Gradient Descent(4/49): loss=1.010412755491589\n",
      "Gradient Descent(5/49): loss=4.989535988275373\n",
      "Gradient Descent(6/49): loss=38.053466321151674\n",
      "Gradient Descent(7/49): loss=313.0500708569477\n",
      "Gradient Descent(8/49): loss=2600.562950902366\n",
      "Gradient Descent(9/49): loss=21629.25527494604\n",
      "Gradient Descent(10/49): loss=179920.03502505293\n",
      "Gradient Descent(11/49): loss=1496667.4269952234\n",
      "Gradient Descent(12/49): loss=12450077.285243098\n",
      "Gradient Descent(13/49): loss=103566406.13479804\n",
      "Gradient Descent(14/49): loss=861520823.8253156\n",
      "Gradient Descent(15/49): loss=7166591568.237715\n",
      "Gradient Descent(16/49): loss=59615546498.56091\n",
      "Gradient Descent(17/49): loss=495914040964.5835\n",
      "Gradient Descent(18/49): loss=4125278563599.032\n",
      "Gradient Descent(19/49): loss=34316276252645.34\n",
      "Gradient Descent(20/49): loss=285461162850692.1\n",
      "Gradient Descent(21/49): loss=2374618822162832.5\n",
      "Gradient Descent(22/49): loss=1.975335101370483e+16\n",
      "Gradient Descent(23/49): loss=1.6431895200562806e+17\n",
      "Gradient Descent(24/49): loss=1.3668930385277315e+18\n",
      "Gradient Descent(25/49): loss=1.137054829020308e+19\n",
      "Gradient Descent(26/49): loss=9.458630981038519e+19\n",
      "Gradient Descent(27/49): loss=7.86819577667529e+20\n",
      "Gradient Descent(28/49): loss=6.545186603029314e+21\n",
      "Gradient Descent(29/49): loss=5.444636722877364e+22\n",
      "Gradient Descent(30/49): loss=4.52914039614775e+23\n",
      "Gradient Descent(31/49): loss=3.767581525104326e+24\n",
      "Gradient Descent(32/49): loss=3.134076073327395e+25\n",
      "Gradient Descent(33/49): loss=2.6070923131865583e+26\n",
      "Gradient Descent(34/49): loss=2.1687190005762918e+27\n",
      "Gradient Descent(35/49): loss=1.8040566034701435e+28\n",
      "Gradient Descent(36/49): loss=1.500710893232104e+29\n",
      "Gradient Descent(37/49): loss=1.2483716867494729e+30\n",
      "Gradient Descent(38/49): loss=1.0384624215802592e+31\n",
      "Gradient Descent(39/49): loss=8.63848653795025e+31\n",
      "Gradient Descent(40/49): loss=7.185955708709116e+32\n",
      "Gradient Descent(41/49): loss=5.97766277931621e+33\n",
      "Gradient Descent(42/49): loss=4.97254001439456e+34\n",
      "Gradient Descent(43/49): loss=4.136425072406483e+35\n",
      "Gradient Descent(44/49): loss=3.44089988820663e+36\n",
      "Gradient Descent(45/49): loss=2.86232479336863e+37\n",
      "Gradient Descent(46/49): loss=2.381035045748672e+38\n",
      "Gradient Descent(47/49): loss=1.980672459749433e+39\n",
      "Gradient Descent(48/49): loss=1.647629420581015e+40\n",
      "Gradient Descent(49/49): loss=1.370586385548858e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.464963657819214\n",
      "Gradient Descent(2/49): loss=0.466117262838138\n",
      "Gradient Descent(3/49): loss=0.5045594994393515\n",
      "Gradient Descent(4/49): loss=0.8264642527793614\n",
      "Gradient Descent(5/49): loss=3.5325583130094365\n",
      "Gradient Descent(6/49): loss=26.415218852863177\n",
      "Gradient Descent(7/49): loss=220.1348798840585\n",
      "Gradient Descent(8/49): loss=1860.422395706868\n",
      "Gradient Descent(9/49): loss=15749.635139291206\n",
      "Gradient Descent(10/49): loss=133357.6294565041\n",
      "Gradient Descent(11/49): loss=1129212.9919391684\n",
      "Gradient Descent(12/49): loss=9561700.695502505\n",
      "Gradient Descent(13/49): loss=80964488.82883729\n",
      "Gradient Descent(14/49): loss=685573512.630276\n",
      "Gradient Descent(15/49): loss=5805150510.518064\n",
      "Gradient Descent(16/49): loss=49155592872.37885\n",
      "Gradient Descent(17/49): loss=416229054947.2068\n",
      "Gradient Descent(18/49): loss=3524454005338.4683\n",
      "Gradient Descent(19/49): loss=29843606274281.484\n",
      "Gradient Descent(20/49): loss=252703208526882.2\n",
      "Gradient Descent(21/49): loss=2139785353448212.8\n",
      "Gradient Descent(22/49): loss=1.811880975125912e+16\n",
      "Gradient Descent(23/49): loss=1.534225226251306e+17\n",
      "Gradient Descent(24/49): loss=1.2991179206472159e+18\n",
      "Gradient Descent(25/49): loss=1.1000388618758744e+19\n",
      "Gradient Descent(26/49): loss=9.314670195869103e+19\n",
      "Gradient Descent(27/49): loss=7.887274155919813e+20\n",
      "Gradient Descent(28/49): loss=6.678614733802109e+21\n",
      "Gradient Descent(29/49): loss=5.655172354961556e+22\n",
      "Gradient Descent(30/49): loss=4.7885640419498725e+23\n",
      "Gradient Descent(31/49): loss=4.054756273473736e+24\n",
      "Gradient Descent(32/49): loss=3.433398466271598e+25\n",
      "Gradient Descent(33/49): loss=2.9072585953723494e+26\n",
      "Gradient Descent(34/49): loss=2.4617453008723173e+27\n",
      "Gradient Descent(35/49): loss=2.0845032278907378e+28\n",
      "Gradient Descent(36/49): loss=1.7650703773243385e+29\n",
      "Gradient Descent(37/49): loss=1.4945879647595212e+30\n",
      "Gradient Descent(38/49): loss=1.265554741103361e+31\n",
      "Gradient Descent(39/49): loss=1.0716189615423012e+32\n",
      "Gradient Descent(40/49): loss=9.074022335342138e+32\n",
      "Gradient Descent(41/49): loss=7.683503586366585e+33\n",
      "Gradient Descent(42/49): loss=6.506070315891917e+34\n",
      "Gradient Descent(43/49): loss=5.5090689396485085e+35\n",
      "Gradient Descent(44/49): loss=4.664849764636835e+36\n",
      "Gradient Descent(45/49): loss=3.9500001842455557e+37\n",
      "Gradient Descent(46/49): loss=3.3446953798637445e+38\n",
      "Gradient Descent(47/49): loss=2.8321485220939626e+39\n",
      "Gradient Descent(48/49): loss=2.3981452240729976e+40\n",
      "Gradient Descent(49/49): loss=2.0306493359648465e+41\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47014275184236926\n",
      "Gradient Descent(2/49): loss=0.4809219394569955\n",
      "Gradient Descent(3/49): loss=0.5709169816904616\n",
      "Gradient Descent(4/49): loss=1.3267260313578195\n",
      "Gradient Descent(5/49): loss=8.02842180434561\n",
      "Gradient Descent(6/49): loss=68.04923587842983\n",
      "Gradient Descent(7/49): loss=606.3954429486903\n",
      "Gradient Descent(8/49): loss=5435.981246509637\n",
      "Gradient Descent(9/49): loss=48764.10555828832\n",
      "Gradient Descent(10/49): loss=437479.22156306717\n",
      "Gradient Descent(11/49): loss=3924810.209353759\n",
      "Gradient Descent(12/49): loss=35211162.68608135\n",
      "Gradient Descent(13/49): loss=315894545.49760365\n",
      "Gradient Descent(14/49): loss=2834026421.6991363\n",
      "Gradient Descent(15/49): loss=25425275262.99401\n",
      "Gradient Descent(16/49): loss=228101127564.66602\n",
      "Gradient Descent(17/49): loss=2046393750290.4512\n",
      "Gradient Descent(18/49): loss=18359082333135.812\n",
      "Gradient Descent(19/49): loss=164707258350029.7\n",
      "Gradient Descent(20/49): loss=1477659964747912.5\n",
      "Gradient Descent(21/49): loss=1.3256725861944074e+16\n",
      "Gradient Descent(22/49): loss=1.1893181433572642e+17\n",
      "Gradient Descent(23/49): loss=1.0669886824614533e+18\n",
      "Gradient Descent(24/49): loss=9.57241638715014e+18\n",
      "Gradient Descent(25/49): loss=8.587828249273836e+19\n",
      "Gradient Descent(26/49): loss=7.704511698637438e+20\n",
      "Gradient Descent(27/49): loss=6.912050263634376e+21\n",
      "Gradient Descent(28/49): loss=6.201098877617205e+22\n",
      "Gradient Descent(29/49): loss=5.563273677608535e+23\n",
      "Gradient Descent(30/49): loss=4.991053137966529e+24\n",
      "Gradient Descent(31/49): loss=4.47768937312356e+25\n",
      "Gradient Descent(32/49): loss=4.0171285634424044e+26\n",
      "Gradient Descent(33/49): loss=3.603939565813895e+27\n",
      "Gradient Descent(34/49): loss=3.2332498671410584e+28\n",
      "Gradient Descent(35/49): loss=2.9006881254422324e+29\n",
      "Gradient Descent(36/49): loss=2.602332620992747e+30\n",
      "Gradient Descent(37/49): loss=2.3346650096175437e+31\n",
      "Gradient Descent(38/49): loss=2.0945288327719768e+32\n",
      "Gradient Descent(39/49): loss=1.879092295143316e+33\n",
      "Gradient Descent(40/49): loss=1.6858148708290096e+34\n",
      "Gradient Descent(41/49): loss=1.5124173442962188e+35\n",
      "Gradient Descent(42/49): loss=1.356854932833286e+36\n",
      "Gradient Descent(43/49): loss=1.2172931735391402e+37\n",
      "Gradient Descent(44/49): loss=1.0920862905003638e+38\n",
      "Gradient Descent(45/49): loss=9.797577870508753e+38\n",
      "Gradient Descent(46/49): loss=8.789830342500064e+39\n",
      "Gradient Descent(47/49): loss=7.885736502538508e+40\n",
      "Gradient Descent(48/49): loss=7.074634863746368e+41\n",
      "Gradient Descent(49/49): loss=6.346960545692127e+42\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.47111092282462014\n",
      "Gradient Descent(2/49): loss=0.4829455989355385\n",
      "Gradient Descent(3/49): loss=0.5644065774385737\n",
      "Gradient Descent(4/49): loss=1.2035100002836214\n",
      "Gradient Descent(5/49): loss=6.649000586668186\n",
      "Gradient Descent(6/49): loss=53.76181475800652\n",
      "Gradient Descent(7/49): loss=462.31719307108045\n",
      "Gradient Descent(8/49): loss=4006.426229521877\n",
      "Gradient Descent(9/49): loss=34752.04553771073\n",
      "Gradient Descent(10/49): loss=301476.08128242363\n",
      "Gradient Descent(11/49): loss=2615359.2806697246\n",
      "Gradient Descent(12/49): loss=22688751.0947596\n",
      "Gradient Descent(13/49): loss=196829375.4964341\n",
      "Gradient Descent(14/49): loss=1707533565.9202716\n",
      "Gradient Descent(15/49): loss=14813189752.892954\n",
      "Gradient Descent(16/49): loss=128507336619.11188\n",
      "Gradient Descent(17/49): loss=1114826437874.7822\n",
      "Gradient Descent(18/49): loss=9671338767811.469\n",
      "Gradient Descent(19/49): loss=83900767315994.42\n",
      "Gradient Descent(20/49): loss=727855669748810.1\n",
      "Gradient Descent(21/49): loss=6314291191047429.0\n",
      "Gradient Descent(22/49): loss=5.477771885612879e+16\n",
      "Gradient Descent(23/49): loss=4.7520749238417664e+17\n",
      "Gradient Descent(24/49): loss=4.122518526395185e+18\n",
      "Gradient Descent(25/49): loss=3.57636596073091e+19\n",
      "Gradient Descent(26/49): loss=3.102567860685814e+20\n",
      "Gradient Descent(27/49): loss=2.6915386836399193e+21\n",
      "Gradient Descent(28/49): loss=2.33496278270894e+22\n",
      "Gradient Descent(29/49): loss=2.025626170552617e+23\n",
      "Gradient Descent(30/49): loss=1.7572705711683522e+24\n",
      "Gradient Descent(31/49): loss=1.5244668069487578e+25\n",
      "Gradient Descent(32/49): loss=1.322504959463003e+26\n",
      "Gradient Descent(33/49): loss=1.147299081772038e+27\n",
      "Gradient Descent(34/49): loss=9.953045344869386e+27\n",
      "Gradient Descent(35/49): loss=8.634462731724739e+28\n",
      "Gradient Descent(36/49): loss=7.490566362583068e+29\n",
      "Gradient Descent(37/49): loss=6.498213748274784e+30\n",
      "Gradient Descent(38/49): loss=5.637328323956764e+31\n",
      "Gradient Descent(39/49): loss=4.890493274482126e+32\n",
      "Gradient Descent(40/49): loss=4.2425991699145906e+33\n",
      "Gradient Descent(41/49): loss=3.680538282401773e+34\n",
      "Gradient Descent(42/49): loss=3.192939400046608e+35\n",
      "Gradient Descent(43/49): loss=2.769937772720894e+36\n",
      "Gradient Descent(44/49): loss=2.402975535531309e+37\n",
      "Gradient Descent(45/49): loss=2.084628572247619e+38\n",
      "Gradient Descent(46/49): loss=1.808456315919322e+39\n",
      "Gradient Descent(47/49): loss=1.5688714479540372e+40\n",
      "Gradient Descent(48/49): loss=1.3610268595037077e+41\n",
      "Gradient Descent(49/49): loss=1.1807175882423715e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47116950303345195\n",
      "Gradient Descent(2/49): loss=0.4822488435150321\n",
      "Gradient Descent(3/49): loss=0.5568949860445216\n",
      "Gradient Descent(4/49): loss=1.13878258371165\n",
      "Gradient Descent(5/49): loss=6.092998972681566\n",
      "Gradient Descent(6/49): loss=48.96293671493215\n",
      "Gradient Descent(7/49): loss=420.8370025864903\n",
      "Gradient Descent(8/49): loss=3647.7724688990843\n",
      "Gradient Descent(9/49): loss=31650.83785625053\n",
      "Gradient Descent(10/49): loss=274660.5952151045\n",
      "Gradient Descent(11/49): loss=2383493.804609097\n",
      "Gradient Descent(12/49): loss=20683903.13239805\n",
      "Gradient Descent(13/49): loss=179494464.98724693\n",
      "Gradient Descent(14/49): loss=1557649136.7106419\n",
      "Gradient Descent(15/49): loss=13517246014.518826\n",
      "Gradient Descent(16/49): loss=117302372908.95406\n",
      "Gradient Descent(17/49): loss=1017947492857.2393\n",
      "Gradient Descent(18/49): loss=8833726654665.674\n",
      "Gradient Descent(19/49): loss=76658891698223.5\n",
      "Gradient Descent(20/49): loss=665244228866497.5\n",
      "Gradient Descent(21/49): loss=5772975244441623.0\n",
      "Gradient Descent(22/49): loss=5.009775617252987e+16\n",
      "Gradient Descent(23/49): loss=4.347472606848076e+17\n",
      "Gradient Descent(24/49): loss=3.772727465518378e+18\n",
      "Gradient Descent(25/49): loss=3.273964856421812e+19\n",
      "Gradient Descent(26/49): loss=2.841139726908925e+20\n",
      "Gradient Descent(27/49): loss=2.4655350016928865e+21\n",
      "Gradient Descent(28/49): loss=2.1395860214119714e+22\n",
      "Gradient Descent(29/49): loss=1.856728190789587e+23\n",
      "Gradient Descent(30/49): loss=1.6112647680309663e+24\n",
      "Gradient Descent(31/49): loss=1.3982521327442217e+25\n",
      "Gradient Descent(32/49): loss=1.2134002216861177e+26\n",
      "Gradient Descent(33/49): loss=1.0529861271144875e+27\n",
      "Gradient Descent(34/49): loss=9.137791176227655e+27\n",
      "Gradient Descent(35/49): loss=7.929755713796317e+28\n",
      "Gradient Descent(36/49): loss=6.881425113332945e+29\n",
      "Gradient Descent(37/49): loss=5.971686051818962e+30\n",
      "Gradient Descent(38/49): loss=5.182216432522632e+31\n",
      "Gradient Descent(39/49): loss=4.497116378937518e+32\n",
      "Gradient Descent(40/49): loss=3.9025880121073374e+33\n",
      "Gradient Descent(41/49): loss=3.3866575620713576e+34\n",
      "Gradient Descent(42/49): loss=2.9389342167691363e+35\n",
      "Gradient Descent(43/49): loss=2.5504008516330082e+36\n",
      "Gradient Descent(44/49): loss=2.213232425175326e+37\n",
      "Gradient Descent(45/49): loss=1.9206383830647043e+38\n",
      "Gradient Descent(46/49): loss=1.6667258967206054e+39\n",
      "Gradient Descent(47/49): loss=1.4463811820559413e+40\n",
      "Gradient Descent(48/49): loss=1.2551665081353066e+41\n",
      "Gradient Descent(49/49): loss=1.0892308215080378e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46980890551064697\n",
      "Gradient Descent(2/49): loss=0.4772612416805974\n",
      "Gradient Descent(3/49): loss=0.5282836438303093\n",
      "Gradient Descent(4/49): loss=0.9197315513661645\n",
      "Gradient Descent(5/49): loss=4.289596649922993\n",
      "Gradient Descent(6/49): loss=33.94096994791405\n",
      "Gradient Descent(7/49): loss=295.70124178434673\n",
      "Gradient Descent(8/49): loss=2607.56520515757\n",
      "Gradient Descent(9/49): loss=23027.196592528588\n",
      "Gradient Descent(10/49): loss=203385.91183573345\n",
      "Gradient Descent(11/49): loss=1796426.5147093285\n",
      "Gradient Descent(12/49): loss=15867156.197943872\n",
      "Gradient Descent(13/49): loss=140148632.2576636\n",
      "Gradient Descent(14/49): loss=1237880283.5102034\n",
      "Gradient Descent(15/49): loss=10933732125.240143\n",
      "Gradient Descent(16/49): loss=96573553873.49332\n",
      "Gradient Descent(17/49): loss=852997970091.0105\n",
      "Gradient Descent(18/49): loss=7534211052626.217\n",
      "Gradient Descent(19/49): loss=66546859636152.16\n",
      "Gradient Descent(20/49): loss=587783444942193.8\n",
      "Gradient Descent(21/49): loss=5191670651884945.0\n",
      "Gradient Descent(22/49): loss=4.585607912161456e+16\n",
      "Gradient Descent(23/49): loss=4.0502954316724806e+17\n",
      "Gradient Descent(24/49): loss=3.5774740008450125e+18\n",
      "Gradient Descent(25/49): loss=3.159848569722066e+19\n",
      "Gradient Descent(26/49): loss=2.7909756943631132e+20\n",
      "Gradient Descent(27/49): loss=2.465164122472744e+21\n",
      "Gradient Descent(28/49): loss=2.177386984415766e+22\n",
      "Gradient Descent(29/49): loss=1.923204234835122e+23\n",
      "Gradient Descent(30/49): loss=1.6986941482430517e+24\n",
      "Gradient Descent(31/49): loss=1.5003928116467666e+25\n",
      "Gradient Descent(32/49): loss=1.3252406806544235e+26\n",
      "Gradient Descent(33/49): loss=1.170535374488895e+27\n",
      "Gradient Descent(34/49): loss=1.0338899815943328e+28\n",
      "Gradient Descent(35/49): loss=9.131962325426248e+28\n",
      "Gradient Descent(36/49): loss=8.065919720433467e+29\n",
      "Gradient Descent(37/49): loss=7.124324281905309e+30\n",
      "Gradient Descent(38/49): loss=6.292648356660846e+31\n",
      "Gradient Descent(39/49): loss=5.558060213676475e+32\n",
      "Gradient Descent(40/49): loss=4.9092260663435365e+33\n",
      "Gradient Descent(41/49): loss=4.336135206157741e+34\n",
      "Gradient Descent(42/49): loss=3.829945549866283e+35\n",
      "Gradient Descent(43/49): loss=3.382847217058666e+36\n",
      "Gradient Descent(44/49): loss=2.987942033369401e+37\n",
      "Gradient Descent(45/49): loss=2.639137100178691e+38\n",
      "Gradient Descent(46/49): loss=2.331050788721432e+39\n",
      "Gradient Descent(47/49): loss=2.0589297082106285e+40\n",
      "Gradient Descent(48/49): loss=1.818575366895982e+41\n",
      "Gradient Descent(49/49): loss=1.606279394528322e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4752146245059788\n",
      "Gradient Descent(2/49): loss=0.4935079016579635\n",
      "Gradient Descent(3/49): loss=0.6038922442196951\n",
      "Gradient Descent(4/49): loss=1.509641756887688\n",
      "Gradient Descent(5/49): loss=9.800406789644025\n",
      "Gradient Descent(6/49): loss=87.08294322196612\n",
      "Gradient Descent(7/49): loss=809.3303562315533\n",
      "Gradient Descent(8/49): loss=7561.471655675044\n",
      "Gradient Descent(9/49): loss=70688.74330449966\n",
      "Gradient Descent(10/49): loss=660883.208752188\n",
      "Gradient Descent(11/49): loss=6178780.165296192\n",
      "Gradient Descent(12/49): loss=57767183.14265894\n",
      "Gradient Descent(13/49): loss=540081981.2851404\n",
      "Gradient Descent(14/49): loss=5049381581.133844\n",
      "Gradient Descent(15/49): loss=47208118911.9576\n",
      "Gradient Descent(16/49): loss=441362265032.42596\n",
      "Gradient Descent(17/49): loss=4126422604610.9146\n",
      "Gradient Descent(18/49): loss=38579110315752.87\n",
      "Gradient Descent(19/49): loss=360687184849291.5\n",
      "Gradient Descent(20/49): loss=3372168104700773.0\n",
      "Gradient Descent(21/49): loss=3.152736832364275e+16\n",
      "Gradient Descent(22/49): loss=2.9475842323193197e+17\n",
      "Gradient Descent(23/49): loss=2.755781173178983e+18\n",
      "Gradient Descent(24/49): loss=2.5764589833187213e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=2.4088055166827476e+20\n",
      "Gradient Descent(26/49): loss=2.252061474592979e+21\n",
      "Gradient Descent(27/49): loss=2.105516966903391e+22\n",
      "Gradient Descent(28/49): loss=1.9685082969235052e+23\n",
      "Gradient Descent(29/49): loss=1.8404149555516222e+24\n",
      "Gradient Descent(30/49): loss=1.7206568110033367e+25\n",
      "Gradient Descent(31/49): loss=1.608691481408252e+26\n",
      "Gradient Descent(32/49): loss=1.5040118783747326e+27\n",
      "Gradient Descent(33/49): loss=1.4061439103986308e+28\n",
      "Gradient Descent(34/49): loss=1.3146443357134798e+29\n",
      "Gradient Descent(35/49): loss=1.2290987548589896e+30\n",
      "Gradient Descent(36/49): loss=1.1491197338754108e+31\n",
      "Gradient Descent(37/49): loss=1.0743450496241191e+32\n",
      "Gradient Descent(38/49): loss=1.0044360492872635e+33\n",
      "Gradient Descent(39/49): loss=9.390761166170969e+33\n",
      "Gradient Descent(40/49): loss=8.779692379882217e+34\n",
      "Gradient Descent(41/49): loss=8.20838661758773e+35\n",
      "Gradient Descent(42/49): loss=7.674256448686633e+36\n",
      "Gradient Descent(43/49): loss=7.17488280998096e+37\n",
      "Gradient Descent(44/49): loss=6.708004049795516e+38\n",
      "Gradient Descent(45/49): loss=6.271505685009546e+39\n",
      "Gradient Descent(46/49): loss=5.863410824611252e+40\n",
      "Gradient Descent(47/49): loss=5.481871216404101e+41\n",
      "Gradient Descent(48/49): loss=5.1251588763153775e+42\n",
      "Gradient Descent(49/49): loss=4.79165826239605e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47624352814351173\n",
      "Gradient Descent(2/49): loss=0.4958736934388782\n",
      "Gradient Descent(3/49): loss=0.5969966864396883\n",
      "Gradient Descent(4/49): loss=1.3657600124358107\n",
      "Gradient Descent(5/49): loss=8.113069156829276\n",
      "Gradient Descent(6/49): loss=68.85627835253996\n",
      "Gradient Descent(7/49): loss=617.7655321389495\n",
      "Gradient Descent(8/49): loss=5580.654735598491\n",
      "Gradient Descent(9/49): loss=50455.26478820539\n",
      "Gradient Descent(10/49): loss=456217.1087625016\n",
      "Gradient Descent(11/49): loss=4125171.611297492\n",
      "Gradient Descent(12/49): loss=37300370.04043181\n",
      "Gradient Descent(13/49): loss=337275149.88071525\n",
      "Gradient Descent(14/49): loss=3049689034.82423\n",
      "Gradient Descent(15/49): loss=27575714491.514763\n",
      "Gradient Descent(16/49): loss=249343464642.0426\n",
      "Gradient Descent(17/49): loss=2254598457714.8105\n",
      "Gradient Descent(18/49): loss=20386394377178.55\n",
      "Gradient Descent(19/49): loss=184336627340496.06\n",
      "Gradient Descent(20/49): loss=1666797548923651.0\n",
      "Gradient Descent(21/49): loss=1.5071416403676162e+16\n",
      "Gradient Descent(22/49): loss=1.3627785363596886e+17\n",
      "Gradient Descent(23/49): loss=1.2322433999697508e+18\n",
      "Gradient Descent(24/49): loss=1.1142117051719287e+19\n",
      "Gradient Descent(25/49): loss=1.0074857970208167e+20\n",
      "Gradient Descent(26/49): loss=9.109827391752456e+20\n",
      "Gradient Descent(27/49): loss=8.237233254595198e+21\n",
      "Gradient Descent(28/49): loss=7.448221439633413e+22\n",
      "Gradient Descent(29/49): loss=6.73478592862087e+23\n",
      "Gradient Descent(30/49): loss=6.089687567959972e+24\n",
      "Gradient Descent(31/49): loss=5.506380613787529e+25\n",
      "Gradient Descent(32/49): loss=4.978946313012823e+26\n",
      "Gradient Descent(33/49): loss=4.502032846365931e+27\n",
      "Gradient Descent(34/49): loss=4.070801024061127e+28\n",
      "Gradient Descent(35/49): loss=3.6808751830572845e+29\n",
      "Gradient Descent(36/49): loss=3.3282987876745354e+30\n",
      "Gradient Descent(37/49): loss=3.0094942830511417e+31\n",
      "Gradient Descent(38/49): loss=2.7212267940781286e+32\n",
      "Gradient Descent(39/49): loss=2.4605713014683666e+33\n",
      "Gradient Descent(40/49): loss=2.224882961899771e+34\n",
      "Gradient Descent(41/49): loss=2.0117702710740367e+35\n",
      "Gradient Descent(42/49): loss=1.8190707973787018e+36\n",
      "Gradient Descent(43/49): loss=1.6448292399257943e+37\n",
      "Gradient Descent(44/49): loss=1.487277588323377e+38\n",
      "Gradient Descent(45/49): loss=1.3448171828637436e+39\n",
      "Gradient Descent(46/49): loss=1.2160024930950557e+40\n",
      "Gradient Descent(47/49): loss=1.0995264501786e+41\n",
      "Gradient Descent(48/49): loss=9.942071842017653e+41\n",
      "Gradient Descent(49/49): loss=8.989760318706671e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4764219146754605\n",
      "Gradient Descent(2/49): loss=0.49531148938425384\n",
      "Gradient Descent(3/49): loss=0.5887468470176599\n",
      "Gradient Descent(4/49): loss=1.289655094939474\n",
      "Gradient Descent(5/49): loss=7.429335521305706\n",
      "Gradient Descent(6/49): loss=62.70282309123404\n",
      "Gradient Descent(7/49): loss=562.324054566993\n",
      "Gradient Descent(8/49): loss=5081.00056890298\n",
      "Gradient Descent(9/49): loss=45952.03311035563\n",
      "Gradient Descent(10/49): loss=415630.8911815325\n",
      "Gradient Descent(11/49): loss=3759384.3622679813\n",
      "Gradient Descent(12/49): loss=34003714.639698036\n",
      "Gradient Descent(13/49): loss=307564414.08682823\n",
      "Gradient Descent(14/49): loss=2781927571.2548504\n",
      "Gradient Descent(15/49): loss=25162602313.287514\n",
      "Gradient Descent(16/49): loss=227596347938.09296\n",
      "Gradient Descent(17/49): loss=2058614484808.081\n",
      "Gradient Descent(18/49): loss=18620217923025.137\n",
      "Gradient Descent(19/49): loss=168420322532412.4\n",
      "Gradient Descent(20/49): loss=1523365900398492.0\n",
      "Gradient Descent(21/49): loss=1.3778881500777808e+16\n",
      "Gradient Descent(22/49): loss=1.246303172224167e+17\n",
      "Gradient Descent(23/49): loss=1.1272842407479508e+18\n",
      "Gradient Descent(24/49): loss=1.019631328684537e+19\n",
      "Gradient Descent(25/49): loss=9.222590087351792e+19\n",
      "Gradient Descent(26/49): loss=8.341855092767397e+20\n",
      "Gradient Descent(27/49): loss=7.545228154958587e+21\n",
      "Gradient Descent(28/49): loss=6.824677158410322e+22\n",
      "Gradient Descent(29/49): loss=6.172937035166849e+23\n",
      "Gradient Descent(30/49): loss=5.58343651364916e+24\n",
      "Gradient Descent(31/49): loss=5.050231862782579e+25\n",
      "Gradient Descent(32/49): loss=4.567946963400632e+26\n",
      "Gradient Descent(33/49): loss=4.131719102683873e+27\n",
      "Gradient Descent(34/49): loss=3.7371499450978995e+28\n",
      "Gradient Descent(35/49): loss=3.3802611854888466e+29\n",
      "Gradient Descent(36/49): loss=3.057454437200306e+30\n",
      "Gradient Descent(37/49): loss=2.765474950777891e+31\n",
      "Gradient Descent(38/49): loss=2.501378797449148e+32\n",
      "Gradient Descent(39/49): loss=2.2625031865026824e+33\n",
      "Gradient Descent(40/49): loss=2.0464396172842022e+34\n",
      "Gradient Descent(41/49): loss=1.851009595113105e+35\n",
      "Gradient Descent(42/49): loss=1.6742426662692068e+36\n",
      "Gradient Descent(43/49): loss=1.5143565505855023e+37\n",
      "Gradient Descent(44/49): loss=1.369739171330149e+38\n",
      "Gradient Descent(45/49): loss=1.2389324011910155e+39\n",
      "Gradient Descent(46/49): loss=1.1206173604791588e+40\n",
      "Gradient Descent(47/49): loss=1.0136011193185942e+41\n",
      "Gradient Descent(48/49): loss=9.16804669744395e+41\n",
      "Gradient Descent(49/49): loss=8.292520464363635e+42\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47482294633042366\n",
      "Gradient Descent(2/49): loss=0.48947756105522916\n",
      "Gradient Descent(3/49): loss=0.5556150726866452\n",
      "Gradient Descent(4/49): loss=1.0298258441232782\n",
      "Gradient Descent(5/49): loss=5.207132994490825\n",
      "Gradient Descent(6/49): loss=43.4162534686225\n",
      "Gradient Descent(7/49): loss=394.8365924447406\n",
      "Gradient Descent(8/49): loss=3629.4071086394842\n",
      "Gradient Descent(9/49): loss=33404.35335040952\n",
      "Gradient Descent(10/49): loss=307493.1773502563\n",
      "Gradient Descent(11/49): loss=2830581.5684197964\n",
      "Gradient Descent(12/49): loss=26056543.451863244\n",
      "Gradient Descent(13/49): loss=239860120.33699533\n",
      "Gradient Descent(14/49): loss=2208001199.41534\n",
      "Gradient Descent(15/49): loss=20325468483.04992\n",
      "Gradient Descent(16/49): loss=187103462340.95163\n",
      "Gradient Descent(17/49): loss=1722356640949.7373\n",
      "Gradient Descent(18/49): loss=15854930536997.72\n",
      "Gradient Descent(19/49): loss=145950505462485.9\n",
      "Gradient Descent(20/49): loss=1343528437103579.2\n",
      "Gradient Descent(21/49): loss=1.2367676669471492e+16\n",
      "Gradient Descent(22/49): loss=1.1384904254825074e+17\n",
      "Gradient Descent(23/49): loss=1.0480225862588965e+18\n",
      "Gradient Descent(24/49): loss=9.64743590920679e+18\n",
      "Gradient Descent(25/49): loss=8.880821925269235e+19\n",
      "Gradient Descent(26/49): loss=8.175125371195743e+20\n",
      "Gradient Descent(27/49): loss=7.525505566619447e+21\n",
      "Gradient Descent(28/49): loss=6.927506486048659e+22\n",
      "Gradient Descent(29/49): loss=6.377026193046135e+23\n",
      "Gradient Descent(30/49): loss=5.870288703257748e+24\n",
      "Gradient Descent(31/49): loss=5.403818083289929e+25\n",
      "Gradient Descent(32/49): loss=4.9744146077662065e+26\n",
      "Gradient Descent(33/49): loss=4.5791328110166146e+27\n",
      "Gradient Descent(34/49): loss=4.215261282843645e+28\n",
      "Gradient Descent(35/49): loss=3.880304069777823e+29\n",
      "Gradient Descent(36/49): loss=3.571963554244155e+30\n",
      "Gradient Descent(37/49): loss=3.2881246942020163e+31\n",
      "Gradient Descent(38/49): loss=3.0268405151488093e+32\n",
      "Gradient Descent(39/49): loss=2.786318754973352e+33\n",
      "Gradient Descent(40/49): loss=2.5649095700487344e+34\n",
      "Gradient Descent(41/49): loss=2.3610942182350396e+35\n",
      "Gradient Descent(42/49): loss=2.1734746411652585e+36\n",
      "Gradient Descent(43/49): loss=2.0007638743529468e+37\n",
      "Gradient Descent(44/49): loss=1.8417772193420966e+38\n",
      "Gradient Descent(45/49): loss=1.695424117343438e+39\n",
      "Gradient Descent(46/49): loss=1.5607006686164764e+40\n",
      "Gradient Descent(47/49): loss=1.4366827462832856e+41\n",
      "Gradient Descent(48/49): loss=1.3225196573394458e+42\n",
      "Gradient Descent(49/49): loss=1.217428307379653e+43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4804569092362369\n",
      "Gradient Descent(2/49): loss=0.5072564758672972\n",
      "Gradient Descent(3/49): loss=0.641504164112008\n",
      "Gradient Descent(4/49): loss=1.723133766492221\n",
      "Gradient Descent(5/49): loss=11.934597165194178\n",
      "Gradient Descent(6/49): loss=110.91630960773674\n",
      "Gradient Descent(7/49): loss=1073.9384028456204\n",
      "Gradient Descent(8/49): loss=10448.173451893706\n",
      "Gradient Descent(9/49): loss=101704.82376709642\n",
      "Gradient Descent(10/49): loss=990081.3191532826\n",
      "Gradient Descent(11/49): loss=9638368.515396046\n",
      "Gradient Descent(12/49): loss=93828892.83271003\n",
      "Gradient Descent(13/49): loss=913418294.4473699\n",
      "Gradient Descent(14/49): loss=8892069032.993038\n",
      "Gradient Descent(15/49): loss=86563726944.22452\n",
      "Gradient Descent(16/49): loss=842692380854.299\n",
      "Gradient Descent(17/49): loss=8203556776470.661\n",
      "Gradient Descent(18/49): loss=79861103902226.94\n",
      "Gradient Descent(19/49): loss=777442771503438.4\n",
      "Gradient Descent(20/49): loss=7568355975932738.0\n",
      "Gradient Descent(21/49): loss=7.36774644745452e+16\n",
      "Gradient Descent(22/49): loss=7.172454346307131e+17\n",
      "Gradient Descent(23/49): loss=6.982338726875443e+18\n",
      "Gradient Descent(24/49): loss=6.797262379498423e+19\n",
      "Gradient Descent(25/49): loss=6.61709173144367e+20\n",
      "Gradient Descent(26/49): loss=6.44169675050428e+21\n",
      "Gradient Descent(27/49): loss=6.270950851153545e+22\n",
      "Gradient Descent(28/49): loss=6.104730803185434e+23\n",
      "Gradient Descent(29/49): loss=5.942916642777532e+24\n",
      "Gradient Descent(30/49): loss=5.785391585911219e+25\n",
      "Gradient Descent(31/49): loss=5.6320419440865915e+26\n",
      "Gradient Descent(32/49): loss=5.482757042271076e+27\n",
      "Gradient Descent(33/49): loss=5.337429139023766e+28\n",
      "Gradient Descent(34/49): loss=5.195953348737093e+29\n",
      "Gradient Descent(35/49): loss=5.058227565938221e+30\n",
      "Gradient Descent(36/49): loss=4.924152391598439e+31\n",
      "Gradient Descent(37/49): loss=4.7936310613948066e+32\n",
      "Gradient Descent(38/49): loss=4.6665693758738556e+33\n",
      "Gradient Descent(39/49): loss=4.542875632466301e+34\n",
      "Gradient Descent(40/49): loss=4.422460559303538e+35\n",
      "Gradient Descent(41/49): loss=4.305237250788833e+36\n",
      "Gradient Descent(42/49): loss=4.1911211048762386e+37\n",
      "Gradient Descent(43/49): loss=4.0800297620115974e+38\n",
      "Gradient Descent(44/49): loss=3.971883045692186e+39\n",
      "Gradient Descent(45/49): loss=3.8666029046019676e+40\n",
      "Gradient Descent(46/49): loss=3.764113356281078e+41\n",
      "Gradient Descent(47/49): loss=3.6643404322876566e+42\n",
      "Gradient Descent(48/49): loss=3.567212124813362e+43\n",
      "Gradient Descent(49/49): loss=3.4726583347147298e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4815479487593189\n",
      "Gradient Descent(2/49): loss=0.5099935082488273\n",
      "Gradient Descent(3/49): loss=0.6342304175696545\n",
      "Gradient Descent(4/49): loss=1.555643297070222\n",
      "Gradient Descent(5/49): loss=9.878917694391676\n",
      "Gradient Descent(6/49): loss=87.77722708902314\n",
      "Gradient Descent(7/49): loss=820.6837324333675\n",
      "Gradient Descent(8/49): loss=7721.393708231942\n",
      "Gradient Descent(9/49): loss=72702.09673783665\n",
      "Gradient Descent(10/49): loss=684603.3647202711\n",
      "Gradient Descent(11/49): loss=6446681.935423674\n",
      "Gradient Descent(12/49): loss=60706348.40097728\n",
      "Gradient Descent(13/49): loss=571652438.8121917\n",
      "Gradient Descent(14/49): loss=5383069937.30503\n",
      "Gradient Descent(15/49): loss=50690664605.78033\n",
      "Gradient Descent(16/49): loss=477337933418.71356\n",
      "Gradient Descent(17/49): loss=4494940132785.439\n",
      "Gradient Descent(18/49): loss=42327427558045.164\n",
      "Gradient Descent(19/49): loss=398583979042518.0\n",
      "Gradient Descent(20/49): loss=3753339088030530.5\n",
      "Gradient Descent(21/49): loss=3.534400540528414e+16\n",
      "Gradient Descent(22/49): loss=3.3282330447373734e+17\n",
      "Gradient Descent(23/49): loss=3.134091643847959e+18\n",
      "Gradient Descent(24/49): loss=2.951274835627584e+19\n",
      "Gradient Descent(25/49): loss=2.7791220376423626e+20\n",
      "Gradient Descent(26/49): loss=2.617011200336688e+21\n",
      "Gradient Descent(27/49): loss=2.4643565593462223e+22\n",
      "Gradient Descent(28/49): loss=2.320606518921862e+23\n",
      "Gradient Descent(29/49): loss=2.1852416588171296e+24\n",
      "Gradient Descent(30/49): loss=2.0577728574375183e+25\n",
      "Gradient Descent(31/49): loss=1.9377395244691256e+26\n",
      "Gradient Descent(32/49): loss=1.8247079366017458e+27\n",
      "Gradient Descent(33/49): loss=1.7182696703312648e+28\n",
      "Gradient Descent(34/49): loss=1.618040126179794e+29\n",
      "Gradient Descent(35/49): loss=1.52365713900029e+30\n",
      "Gradient Descent(36/49): loss=1.4347796693445786e+31\n",
      "Gradient Descent(37/49): loss=1.3510865711660408e+32\n",
      "Gradient Descent(38/49): loss=1.272275431404083e+33\n",
      "Gradient Descent(39/49): loss=1.198061477257842e+34\n",
      "Gradient Descent(40/49): loss=1.128176547200309e+35\n",
      "Gradient Descent(41/49): loss=1.0623681220149358e+36\n",
      "Gradient Descent(42/49): loss=1.0003984123534003e+37\n",
      "Gradient Descent(43/49): loss=9.420434995178845e+37\n",
      "Gradient Descent(44/49): loss=8.870925263628021e+38\n",
      "Gradient Descent(45/49): loss=8.35346935392557e+39\n",
      "Gradient Descent(46/49): loss=7.866197513023904e+40\n",
      "Gradient Descent(47/49): loss=7.407349053698903e+41\n",
      "Gradient Descent(48/49): loss=6.975265992557345e+42\n",
      "Gradient Descent(49/49): loss=6.568387059150474e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48184979979922626\n",
      "Gradient Descent(2/49): loss=0.5095810676579458\n",
      "Gradient Descent(3/49): loss=0.625178512818208\n",
      "Gradient Descent(4/49): loss=1.4664289694203478\n",
      "Gradient Descent(5/49): loss=9.041587265535739\n",
      "Gradient Descent(6/49): loss=79.92649516762835\n",
      "Gradient Descent(7/49): loss=747.0169662869699\n",
      "Gradient Descent(8/49): loss=7029.9762451210745\n",
      "Gradient Descent(9/49): loss=66212.33296656425\n",
      "Gradient Descent(10/49): loss=623689.3559040291\n",
      "Gradient Descent(11/49): loss=5874938.370573231\n",
      "Gradient Descent(12/49): loss=55339980.24648494\n",
      "Gradient Descent(13/49): loss=521284455.4234081\n",
      "Gradient Descent(14/49): loss=4910328655.384813\n",
      "Gradient Descent(15/49): loss=46253686136.96689\n",
      "Gradient Descent(16/49): loss=435694559856.71216\n",
      "Gradient Descent(17/49): loss=4104099918403.9067\n",
      "Gradient Descent(18/49): loss=38659275768610.35\n",
      "Gradient Descent(19/49): loss=364157703922660.06\n",
      "Gradient Descent(20/49): loss=3430246187744610.5\n",
      "Gradient Descent(21/49): loss=3.2311794537885104e+16\n",
      "Gradient Descent(22/49): loss=3.043665116482406e+17\n",
      "Gradient Descent(23/49): loss=2.8670327580939433e+18\n",
      "Gradient Descent(24/49): loss=2.700650867097815e+19\n",
      "Gradient Descent(25/49): loss=2.5439245803404152e+20\n",
      "Gradient Descent(26/49): loss=2.396293556232426e+21\n",
      "Gradient Descent(27/49): loss=2.257229971366839e+22\n",
      "Gradient Descent(28/49): loss=2.1262366333979085e+23\n",
      "Gradient Descent(29/49): loss=2.0028452034356152e+24\n",
      "Gradient Descent(30/49): loss=1.8866145215993632e+25\n",
      "Gradient Descent(31/49): loss=1.7771290297443236e+26\n",
      "Gradient Descent(32/49): loss=1.6739972857214956e+27\n",
      "Gradient Descent(33/49): loss=1.5768505638591837e+28\n",
      "Gradient Descent(34/49): loss=1.485341536662851e+29\n",
      "Gradient Descent(35/49): loss=1.3991430330191925e+30\n",
      "Gradient Descent(36/49): loss=1.3179468684652275e+31\n",
      "Gradient Descent(37/49): loss=1.241462743340197e+32\n",
      "Gradient Descent(38/49): loss=1.169417204880617e+33\n",
      "Gradient Descent(39/49): loss=1.1015526695480182e+34\n",
      "Gradient Descent(40/49): loss=1.0376265020936076e+35\n",
      "Gradient Descent(41/49): loss=9.774101480674227e+35\n",
      "Gradient Descent(42/49): loss=9.206883166704368e+36\n",
      "Gradient Descent(43/49): loss=8.672582110278771e+37\n",
      "Gradient Descent(44/49): loss=8.169288031320778e+38\n",
      "Gradient Descent(45/49): loss=7.695201508623923e+39\n",
      "Gradient Descent(46/49): loss=7.248627546402409e+40\n",
      "Gradient Descent(47/49): loss=6.8279695141940356e+41\n",
      "Gradient Descent(48/49): loss=6.43172343844627e+42\n",
      "Gradient Descent(49/49): loss=6.058472625377793e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4800057802785441\n",
      "Gradient Descent(2/49): loss=0.5028315714672691\n",
      "Gradient Descent(3/49): loss=0.5869839577721233\n",
      "Gradient Descent(4/49): loss=1.1593706140811382\n",
      "Gradient Descent(5/49): loss=6.315049478281917\n",
      "Gradient Descent(6/49): loss=55.29063313691633\n",
      "Gradient Descent(7/49): loss=524.1736723427144\n",
      "Gradient Descent(8/49): loss=5018.034117675302\n",
      "Gradient Descent(9/49): loss=48094.37106463552\n",
      "Gradient Descent(10/49): loss=461015.0528049732\n",
      "Gradient Descent(11/49): loss=4419196.313992899\n",
      "Gradient Descent(12/49): loss=42361603.38243697\n",
      "Gradient Descent(13/49): loss=406070647.9112974\n",
      "Gradient Descent(14/49): loss=3892519720.6749954\n",
      "Gradient Descent(15/49): loss=37312989545.571686\n",
      "Gradient Descent(16/49): loss=357675564801.57654\n",
      "Gradient Descent(17/49): loss=3428613231567.7915\n",
      "Gradient Descent(18/49): loss=32866065922936.188\n",
      "Gradient Descent(19/49): loss=315048159794386.0\n",
      "Gradient Descent(20/49): loss=3019994641968365.0\n",
      "Gradient Descent(21/49): loss=2.8949122075400652e+16\n",
      "Gradient Descent(22/49): loss=2.7750104496549933e+17\n",
      "Gradient Descent(23/49): loss=2.660074794543739e+18\n",
      "Gradient Descent(24/49): loss=2.5498995556744393e+19\n",
      "Gradient Descent(25/49): loss=2.4442875656598913e+20\n",
      "Gradient Descent(26/49): loss=2.3430498234112978e+21\n",
      "Gradient Descent(27/49): loss=2.246005155905506e+22\n",
      "Gradient Descent(28/49): loss=2.1529798939612738e+23\n",
      "Gradient Descent(29/49): loss=2.0638075614446803e+24\n",
      "Gradient Descent(30/49): loss=1.9783285773465777e+25\n",
      "Gradient Descent(31/49): loss=1.8963899701997845e+26\n",
      "Gradient Descent(32/49): loss=1.8178451043243205e+27\n",
      "Gradient Descent(33/49): loss=1.7425534174111212e+28\n",
      "Gradient Descent(34/49): loss=1.6703801689748003e+29\n",
      "Gradient Descent(35/49): loss=1.6011961992244583e+30\n",
      "Gradient Descent(36/49): loss=1.5348776979221309e+31\n",
      "Gradient Descent(37/49): loss=1.4713059828144802e+32\n",
      "Gradient Descent(38/49): loss=1.4103672872413554e+33\n",
      "Gradient Descent(39/49): loss=1.3519525565412631e+34\n",
      "Gradient Descent(40/49): loss=1.2959572528895061e+35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/49): loss=1.2422811682190766e+36\n",
      "Gradient Descent(42/49): loss=1.1908282448907185e+37\n",
      "Gradient Descent(43/49): loss=1.141506403789757e+38\n",
      "Gradient Descent(44/49): loss=1.0942273795433848e+39\n",
      "Gradient Descent(45/49): loss=1.048906562562678e+40\n",
      "Gradient Descent(46/49): loss=1.0054628476268929e+41\n",
      "Gradient Descent(47/49): loss=9.638184887394039e+41\n",
      "Gradient Descent(48/49): loss=9.238989599948247e+42\n",
      "Gradient Descent(49/49): loss=8.856328222089962e+43\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4858696060331432\n",
      "Gradient Descent(2/49): loss=0.5222361666702785\n",
      "Gradient Descent(3/49): loss=0.6842707881848641\n",
      "Gradient Descent(4/49): loss=1.9715954186355251\n",
      "Gradient Descent(5/49): loss=14.49575346361866\n",
      "Gradient Descent(6/49): loss=140.62648078162107\n",
      "Gradient Descent(7/49): loss=1417.1384224576354\n",
      "Gradient Descent(8/49): loss=14344.788705524434\n",
      "Gradient Descent(9/49): loss=145279.1080520783\n",
      "Gradient Descent(10/49): loss=1471429.110411295\n",
      "Gradient Descent(11/49): loss=14903176.301578285\n",
      "Gradient Descent(12/49): loss=150945005.13813972\n",
      "Gradient Descent(13/49): loss=1528828274.1025336\n",
      "Gradient Descent(14/49): loss=15484552968.81893\n",
      "Gradient Descent(15/49): loss=156833429351.68222\n",
      "Gradient Descent(16/49): loss=1588468496187.861\n",
      "Gradient Descent(17/49): loss=16088611808697.95\n",
      "Gradient Descent(18/49): loss=162951566589652.22\n",
      "Gradient Descent(19/49): loss=1650435312242545.2\n",
      "Gradient Descent(20/49): loss=1.671623524035694e+16\n",
      "Gradient Descent(21/49): loss=1.693083749106603e+17\n",
      "Gradient Descent(22/49): loss=1.714819479549132e+18\n",
      "Gradient Descent(23/49): loss=1.7368342522883275e+19\n",
      "Gradient Descent(24/49): loss=1.7591316496562668e+20\n",
      "Gradient Descent(25/49): loss=1.7817152999748188e+21\n",
      "Gradient Descent(26/49): loss=1.804588878146038e+22\n",
      "Gradient Descent(27/49): loss=1.8277561062501695e+23\n",
      "Gradient Descent(28/49): loss=1.8512207541514127e+24\n",
      "Gradient Descent(29/49): loss=1.874986640111329e+25\n",
      "Gradient Descent(30/49): loss=1.8990576314100726e+26\n",
      "Gradient Descent(31/49): loss=1.9234376449757921e+27\n",
      "Gradient Descent(32/49): loss=1.948130648021948e+28\n",
      "Gradient Descent(33/49): loss=1.9731406586930122e+29\n",
      "Gradient Descent(34/49): loss=1.9984717467180837e+30\n",
      "Gradient Descent(35/49): loss=2.024128034073356e+31\n",
      "Gradient Descent(36/49): loss=2.050113695652645e+32\n",
      "Gradient Descent(37/49): loss=2.076432959946933e+33\n",
      "Gradient Descent(38/49): loss=2.103090109732392e+34\n",
      "Gradient Descent(39/49): loss=2.130089482767295e+35\n",
      "Gradient Descent(40/49): loss=2.1574354724977636e+36\n",
      "Gradient Descent(41/49): loss=2.185132528772726e+37\n",
      "Gradient Descent(42/49): loss=2.213185158568267e+38\n",
      "Gradient Descent(43/49): loss=2.241597926720733e+39\n",
      "Gradient Descent(44/49): loss=2.27037545666952e+40\n",
      "Gradient Descent(45/49): loss=2.2995224312096305e+41\n",
      "Gradient Descent(46/49): loss=2.329043593253492e+42\n",
      "Gradient Descent(47/49): loss=2.3589437466029186e+43\n",
      "Gradient Descent(48/49): loss=2.389227756730762e+44\n",
      "Gradient Descent(49/49): loss=2.4199005515724156e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4870241846720416\n",
      "Gradient Descent(2/49): loss=0.525375024902964\n",
      "Gradient Descent(3/49): loss=0.676631354060663\n",
      "Gradient Descent(4/49): loss=1.7772095008366908\n",
      "Gradient Descent(5/49): loss=12.001020566078664\n",
      "Gradient Descent(6/49): loss=111.38810121796922\n",
      "Gradient Descent(7/49): loss=1084.1563942862115\n",
      "Gradient Descent(8/49): loss=10614.60915889566\n",
      "Gradient Descent(9/49): loss=103999.75066452568\n",
      "Gradient Descent(10/49): loss=1019061.708260091\n",
      "Gradient Descent(11/49): loss=9985591.81045178\n",
      "Gradient Descent(12/49): loss=97847066.50464334\n",
      "Gradient Descent(13/49): loss=958786478.3573922\n",
      "Gradient Descent(14/49): loss=9394983105.821863\n",
      "Gradient Descent(15/49): loss=92059817161.6059\n",
      "Gradient Descent(16/49): loss=902078252275.1366\n",
      "Gradient Descent(17/49): loss=8839309031626.0\n",
      "Gradient Descent(18/49): loss=86614862912727.94\n",
      "Gradient Descent(19/49): loss=848724085848701.4\n",
      "Gradient Descent(20/49): loss=8316500767606731.0\n",
      "Gradient Descent(21/49): loss=8.149195500731402e+16\n",
      "Gradient Descent(22/49): loss=7.985255958588718e+17\n",
      "Gradient Descent(23/49): loss=7.824614431995611e+18\n",
      "Gradient Descent(24/49): loss=7.66720457389282e+19\n",
      "Gradient Descent(25/49): loss=7.512961371941773e+20\n",
      "Gradient Descent(26/49): loss=7.361821121675126e+21\n",
      "Gradient Descent(27/49): loss=7.213721400185449e+22\n",
      "Gradient Descent(28/49): loss=7.06860104034346e+23\n",
      "Gradient Descent(29/49): loss=6.926400105534649e+24\n",
      "Gradient Descent(30/49): loss=6.787059864906384e+25\n",
      "Gradient Descent(31/49): loss=6.650522769109912e+26\n",
      "Gradient Descent(32/49): loss=6.516732426532217e+27\n",
      "Gradient Descent(33/49): loss=6.38563358000488e+28\n",
      "Gradient Descent(34/49): loss=6.257172083983264e+29\n",
      "Gradient Descent(35/49): loss=6.131294882183079e+30\n",
      "Gradient Descent(36/49): loss=6.007949985667089e+31\n",
      "Gradient Descent(37/49): loss=5.887086451373921e+32\n",
      "Gradient Descent(38/49): loss=5.76865436107679e+33\n",
      "Gradient Descent(39/49): loss=5.6526048007675835e+34\n",
      "Gradient Descent(40/49): loss=5.538889840454259e+35\n",
      "Gradient Descent(41/49): loss=5.427462514365504e+36\n",
      "Gradient Descent(42/49): loss=5.318276801552357e+37\n",
      "Gradient Descent(43/49): loss=5.211287606881953e+38\n",
      "Gradient Descent(44/49): loss=5.106450742412548e+39\n",
      "Gradient Descent(45/49): loss=5.003722909142508e+40\n",
      "Gradient Descent(46/49): loss=4.903061679127899e+41\n",
      "Gradient Descent(47/49): loss=4.804425477959161e+42\n",
      "Gradient Descent(48/49): loss=4.707773567590418e+43\n",
      "Gradient Descent(49/49): loss=4.613066029513636e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48745315840474923\n",
      "Gradient Descent(2/49): loss=0.5251286578720852\n",
      "Gradient Descent(3/49): loss=0.6667103634140181\n",
      "Gradient Descent(4/49): loss=1.6729348922945897\n",
      "Gradient Descent(5/49): loss=10.97967731364734\n",
      "Gradient Descent(6/49): loss=101.4203077908093\n",
      "Gradient Descent(7/49): loss=986.8250409248676\n",
      "Gradient Descent(8/49): loss=9663.995181765731\n",
      "Gradient Descent(9/49): loss=94714.88698881169\n",
      "Gradient Descent(10/49): loss=928374.3154501383\n",
      "Gradient Descent(11/49): loss=9099835.536069917\n",
      "Gradient Descent(12/49): loss=89195859.54833223\n",
      "Gradient Descent(13/49): loss=874290869.2359979\n",
      "Gradient Descent(14/49): loss=8569731269.809373\n",
      "Gradient Descent(15/49): loss=83999841375.37137\n",
      "Gradient Descent(16/49): loss=823359931928.2024\n",
      "Gradient Descent(17/49): loss=8070510211160.078\n",
      "Gradient Descent(18/49): loss=79106515320198.3\n",
      "Gradient Descent(19/49): loss=775395929424648.6\n",
      "Gradient Descent(20/49): loss=7600370777739664.0\n",
      "Gradient Descent(21/49): loss=7.449824504750966e+16\n",
      "Gradient Descent(22/49): loss=7.30226021527007e+17\n",
      "Gradient Descent(23/49): loss=7.157618842901612e+18\n",
      "Gradient Descent(24/49): loss=7.0158424912231514e+19\n",
      "Gradient Descent(25/49): loss=6.876874410610388e+20\n",
      "Gradient Descent(26/49): loss=6.74065897552141e+21\n",
      "Gradient Descent(27/49): loss=6.6071416622314185e+22\n",
      "Gradient Descent(28/49): loss=6.47626902700821e+23\n",
      "Gradient Descent(29/49): loss=6.347988684719632e+24\n",
      "Gradient Descent(30/49): loss=6.2222492878658836e+25\n",
      "Gradient Descent(31/49): loss=6.0990005060252774e+26\n",
      "Gradient Descent(32/49): loss=5.978193005708782e+27\n",
      "Gradient Descent(33/49): loss=5.859778430612848e+28\n",
      "Gradient Descent(34/49): loss=5.743709382264224e+29\n",
      "Gradient Descent(35/49): loss=5.62993940104658e+30\n",
      "Gradient Descent(36/49): loss=5.518422947604509e+31\n",
      "Gradient Descent(37/49): loss=5.409115384614348e+32\n",
      "Gradient Descent(38/49): loss=5.301972958917841e+33\n",
      "Gradient Descent(39/49): loss=5.19695278400893e+34\n",
      "Gradient Descent(40/49): loss=5.094012822866294e+35\n",
      "Gradient Descent(41/49): loss=4.9931118711279887e+36\n",
      "Gradient Descent(42/49): loss=4.89420954059772e+37\n",
      "Gradient Descent(43/49): loss=4.797266243078471e+38\n",
      "Gradient Descent(44/49): loss=4.702243174526916e+39\n",
      "Gradient Descent(45/49): loss=4.609102299520644e+40\n",
      "Gradient Descent(46/49): loss=4.517806336033198e+41\n",
      "Gradient Descent(47/49): loss=4.4283187405114666e+42\n",
      "Gradient Descent(48/49): loss=4.3406036932479327e+43\n",
      "Gradient Descent(49/49): loss=4.254626084043237e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4853574073550083\n",
      "Gradient Descent(2/49): loss=0.5173906056410721\n",
      "Gradient Descent(3/49): loss=0.6228585139436094\n",
      "Gradient Descent(4/49): loss=1.311334156891674\n",
      "Gradient Descent(5/49): loss=7.648057904501364\n",
      "Gradient Descent(6/49): loss=70.10514545566359\n",
      "Gradient Descent(7/49): loss=692.0187082886379\n",
      "Gradient Descent(8/49): loss=6893.549338281538\n",
      "Gradient Descent(9/49): loss=68745.49780412477\n",
      "Gradient Descent(10/49): loss=685652.4031236\n",
      "Gradient Descent(11/49): loss=6838660.508649912\n",
      "Gradient Descent(12/49): loss=68208581.33253706\n",
      "Gradient Descent(13/49): loss=680310398.5591255\n",
      "Gradient Descent(14/49): loss=6785396296.787784\n",
      "Gradient Descent(15/49): loss=67677347315.32571\n",
      "Gradient Descent(16/49): loss=675011914151.52\n",
      "Gradient Descent(17/49): loss=6732549403313.238\n",
      "Gradient Descent(18/49): loss=67150253970783.59\n",
      "Gradient Descent(19/49): loss=669754700370797.6\n",
      "Gradient Descent(20/49): loss=6680114104468496.0\n",
      "Gradient Descent(21/49): loss=6.662726580942978e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=6.645384315023854e+17\n",
      "Gradient Descent(23/49): loss=6.628087188910886e+18\n",
      "Gradient Descent(24/49): loss=6.610835085110932e+19\n",
      "Gradient Descent(25/49): loss=6.593627886436338e+20\n",
      "Gradient Descent(26/49): loss=6.576465476004411e+21\n",
      "Gradient Descent(27/49): loss=6.55934773723677e+22\n",
      "Gradient Descent(28/49): loss=6.542274553858647e+23\n",
      "Gradient Descent(29/49): loss=6.525245809897716e+24\n",
      "Gradient Descent(30/49): loss=6.508261389683616e+25\n",
      "Gradient Descent(31/49): loss=6.49132117784707e+26\n",
      "Gradient Descent(32/49): loss=6.47442505931899e+27\n",
      "Gradient Descent(33/49): loss=6.457572919329629e+28\n",
      "Gradient Descent(34/49): loss=6.440764643408478e+29\n",
      "Gradient Descent(35/49): loss=6.42400011738264e+30\n",
      "Gradient Descent(36/49): loss=6.407279227376568e+31\n",
      "Gradient Descent(37/49): loss=6.390601859810996e+32\n",
      "Gradient Descent(38/49): loss=6.37396790140219e+33\n",
      "Gradient Descent(39/49): loss=6.357377239161564e+34\n",
      "Gradient Descent(40/49): loss=6.3408297603942935e+35\n",
      "Gradient Descent(41/49): loss=6.324325352699154e+36\n",
      "Gradient Descent(42/49): loss=6.307863903967503e+37\n",
      "Gradient Descent(43/49): loss=6.29144530238221e+38\n",
      "Gradient Descent(44/49): loss=6.275069436417368e+39\n",
      "Gradient Descent(45/49): loss=6.258736194837448e+40\n",
      "Gradient Descent(46/49): loss=6.242445466696387e+41\n",
      "Gradient Descent(47/49): loss=6.22619714133691e+42\n",
      "Gradient Descent(48/49): loss=6.209991108389669e+43\n",
      "Gradient Descent(49/49): loss=6.193827257772654e+44\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.491452714896698\n",
      "Gradient Descent(2/49): loss=0.5385174892936745\n",
      "Gradient Descent(3/49): loss=0.7327534715154249\n",
      "Gradient Descent(4/49): loss=2.2599503065771276\n",
      "Gradient Descent(5/49): loss=17.558632704606197\n",
      "Gradient Descent(6/49): loss=177.50376532755016\n",
      "Gradient Descent(7/49): loss=1860.0120469415688\n",
      "Gradient Descent(8/49): loss=19573.806124715044\n",
      "Gradient Descent(9/49): loss=206089.8678210487\n",
      "Gradient Descent(10/49): loss=2170027.615835543\n",
      "Gradient Descent(11/49): loss=22849532.026874326\n",
      "Gradient Descent(12/49): loss=240596773.29436624\n",
      "Gradient Descent(13/49): loss=2533391706.9028444\n",
      "Gradient Descent(14/49): loss=26675643085.101143\n",
      "Gradient Descent(15/49): loss=280884291876.7689\n",
      "Gradient Descent(16/49): loss=2957603878400.7773\n",
      "Gradient Descent(17/49): loss=31142434642550.88\n",
      "Gradient Descent(18/49): loss=327917894128308.6\n",
      "Gradient Descent(19/49): loss=3452849673567446.0\n",
      "Gradient Descent(20/49): loss=3.635718294650588e+16\n",
      "Gradient Descent(21/49): loss=3.828271939913362e+17\n",
      "Gradient Descent(22/49): loss=4.0310235442310385e+18\n",
      "Gradient Descent(23/49): loss=4.244513208356929e+19\n",
      "Gradient Descent(24/49): loss=4.469309637672692e+20\n",
      "Gradient Descent(25/49): loss=4.706011657135577e+21\n",
      "Gradient Descent(26/49): loss=4.95524980646199e+22\n",
      "Gradient Descent(27/49): loss=5.217688019792815e+23\n",
      "Gradient Descent(28/49): loss=5.494025394317516e+24\n",
      "Gradient Descent(29/49): loss=5.784998052567281e+25\n",
      "Gradient Descent(30/49): loss=6.0913811033386674e+26\n",
      "Gradient Descent(31/49): loss=6.413990706469394e+27\n",
      "Gradient Descent(32/49): loss=6.753686246970829e+28\n",
      "Gradient Descent(33/49): loss=7.111372624303282e+29\n",
      "Gradient Descent(34/49): loss=7.48800266289726e+30\n",
      "Gradient Descent(35/49): loss=7.884579650338806e+31\n",
      "Gradient Descent(36/49): loss=8.30216000998097e+32\n",
      "Gradient Descent(37/49): loss=8.741856115102338e+33\n",
      "Gradient Descent(38/49): loss=9.204839252107681e+34\n",
      "Gradient Descent(39/49): loss=9.692342740664265e+35\n",
      "Gradient Descent(40/49): loss=1.020566521908525e+37\n",
      "Gradient Descent(41/49): loss=1.0746174103714098e+38\n",
      "Gradient Descent(42/49): loss=1.131530923152186e+39\n",
      "Gradient Descent(43/49): loss=1.1914586695623468e+40\n",
      "Gradient Descent(44/49): loss=1.2545602883928456e+41\n",
      "Gradient Descent(45/49): loss=1.3210038731687342e+42\n",
      "Gradient Descent(46/49): loss=1.3909664199257394e+43\n",
      "Gradient Descent(47/49): loss=1.4646342987019345e+44\n",
      "Gradient Descent(48/49): loss=1.5422037500004507e+45\n",
      "Gradient Descent(49/49): loss=1.6238814075454565e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49267223588167997\n",
      "Gradient Descent(2/49): loss=0.5420902706926688\n",
      "Gradient Descent(3/49): loss=0.7247670985731556\n",
      "Gradient Descent(4/49): loss=2.035006872850617\n",
      "Gradient Descent(5/49): loss=14.542354224312923\n",
      "Gradient Descent(6/49): loss=140.72419307625273\n",
      "Gradient Descent(7/49): loss=1424.5027693215593\n",
      "Gradient Descent(8/49): loss=14501.697001186836\n",
      "Gradient Descent(9/49): loss=147735.73718886834\n",
      "Gradient Descent(10/49): loss=1505194.682227036\n",
      "Gradient Descent(11/49): loss=15335754.583853645\n",
      "Gradient Descent(12/49): loss=156249397.1936907\n",
      "Gradient Descent(13/49): loss=1591958165.0226564\n",
      "Gradient Descent(14/49): loss=16219780206.626783\n",
      "Gradient Descent(15/49): loss=165256397374.78564\n",
      "Gradient Descent(16/49): loss=1683726693117.5015\n",
      "Gradient Descent(17/49): loss=17154770540820.068\n",
      "Gradient Descent(18/49): loss=174782613778600.7\n",
      "Gradient Descent(19/49): loss=1780785234442026.8\n",
      "Gradient Descent(20/49): loss=1.8143658471800724e+16\n",
      "Gradient Descent(21/49): loss=1.8485796960490646e+17\n",
      "Gradient Descent(22/49): loss=1.8834387221056102e+18\n",
      "Gradient Descent(23/49): loss=1.918955091581098e+19\n",
      "Gradient Descent(24/49): loss=1.955141200127927e+20\n",
      "Gradient Descent(25/49): loss=1.992009677145747e+21\n",
      "Gradient Descent(26/49): loss=2.029573390189297e+22\n",
      "Gradient Descent(27/49): loss=2.0678454494591432e+23\n",
      "Gradient Descent(28/49): loss=2.1068392123775494e+24\n",
      "Gradient Descent(29/49): loss=2.1465682882503047e+25\n",
      "Gradient Descent(30/49): loss=2.1870465430165275e+26\n",
      "Gradient Descent(31/49): loss=2.2282881040879148e+27\n",
      "Gradient Descent(32/49): loss=2.270307365279553e+28\n",
      "Gradient Descent(33/49): loss=2.3131189918336264e+29\n",
      "Gradient Descent(34/49): loss=2.356737925537417e+30\n",
      "Gradient Descent(35/49): loss=2.4011793899385926e+31\n",
      "Gradient Descent(36/49): loss=2.4464588956581176e+32\n",
      "Gradient Descent(37/49): loss=2.492592245803721e+33\n",
      "Gradient Descent(38/49): loss=2.539595541485562e+34\n",
      "Gradient Descent(39/49): loss=2.587485187435386e+35\n",
      "Gradient Descent(40/49): loss=2.6362778977320936e+36\n",
      "Gradient Descent(41/49): loss=2.685990701635439e+37\n",
      "Gradient Descent(42/49): loss=2.7366409495290918e+38\n",
      "Gradient Descent(43/49): loss=2.788246318976156e+39\n",
      "Gradient Descent(44/49): loss=2.8408248208893204e+40\n",
      "Gradient Descent(45/49): loss=2.8943948058161865e+41\n",
      "Gradient Descent(46/49): loss=2.948974970344359e+42\n",
      "Gradient Descent(47/49): loss=3.004584363626669e+43\n",
      "Gradient Descent(48/49): loss=3.0612423940294145e+44\n",
      "Gradient Descent(49/49): loss=3.118968835906343e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4932319904920295\n",
      "Gradient Descent(2/49): loss=0.5420274217313316\n",
      "Gradient Descent(3/49): loss=0.7139068040197118\n",
      "Gradient Descent(4/49): loss=1.9134798992669548\n",
      "Gradient Descent(5/49): loss=13.301345795263623\n",
      "Gradient Descent(6/49): loss=128.1271992051129\n",
      "Gradient Descent(7/49): loss=1296.601764093531\n",
      "Gradient Descent(8/49): loss=13202.812362233257\n",
      "Gradient Descent(9/49): loss=134544.44178091924\n",
      "Gradient Descent(10/49): loss=1371225.5619388656\n",
      "Gradient Descent(11/49): loss=13975193.530756589\n",
      "Gradient Descent(12/49): loss=142431989.401954\n",
      "Gradient Descent(13/49): loss=1451634750.5357225\n",
      "Gradient Descent(14/49): loss=14794734869.7853\n",
      "Gradient Descent(15/49): loss=150784610808.4947\n",
      "Gradient Descent(16/49): loss=1536762846492.977\n",
      "Gradient Descent(17/49): loss=15662341361734.6\n",
      "Gradient Descent(18/49): loss=159627061191964.5\n",
      "Gradient Descent(19/49): loss=1626883112577567.5\n",
      "Gradient Descent(20/49): loss=1.658082684869929e+16\n",
      "Gradient Descent(21/49): loss=1.6898805873704195e+17\n",
      "Gradient Descent(22/49): loss=1.7222882945643776e+18\n",
      "Gradient Descent(23/49): loss=1.7553175009893722e+19\n",
      "Gradient Descent(24/49): loss=1.7889801254550965e+20\n",
      "Gradient Descent(25/49): loss=1.823288315344335e+21\n",
      "Gradient Descent(26/49): loss=1.8582544509964447e+22\n",
      "Gradient Descent(27/49): loss=1.8938911501749806e+23\n",
      "Gradient Descent(28/49): loss=1.9302112726208617e+24\n",
      "Gradient Descent(29/49): loss=1.9672279246926574e+25\n",
      "Gradient Descent(30/49): loss=2.004954464096511e+26\n",
      "Gradient Descent(31/49): loss=2.043404504706099e+27\n",
      "Gradient Descent(32/49): loss=2.0825919214752473e+28\n",
      "Gradient Descent(33/49): loss=2.122530855444966e+29\n",
      "Gradient Descent(34/49): loss=2.1632357188463384e+30\n",
      "Gradient Descent(35/49): loss=2.2047212003010123e+31\n",
      "Gradient Descent(36/49): loss=2.2470022701219706e+32\n",
      "Gradient Descent(37/49): loss=2.2900941857156428e+33\n",
      "Gradient Descent(38/49): loss=2.3340124970874224e+34\n",
      "Gradient Descent(39/49): loss=2.378773052453292e+35\n",
      "Gradient Descent(40/49): loss=2.4243920039584638e+36\n",
      "Gradient Descent(41/49): loss=2.4708858135062934e+37\n",
      "Gradient Descent(42/49): loss=2.5182712586982954e+38\n",
      "Gradient Descent(43/49): loss=2.5665654388888204e+39\n",
      "Gradient Descent(44/49): loss=2.61578578135521e+40\n",
      "Gradient Descent(45/49): loss=2.665950047586768e+41\n",
      "Gradient Descent(46/49): loss=2.7170763396938457e+42\n",
      "Gradient Descent(47/49): loss=2.769183106940385e+43\n",
      "Gradient Descent(48/49): loss=2.8222891524012305e+44\n",
      "Gradient Descent(49/49): loss=2.876413639747484e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4908778275598163\n",
      "Gradient Descent(2/49): loss=0.5332239785201011\n",
      "Gradient Descent(3/49): loss=0.6637472050355449\n",
      "Gradient Descent(4/49): loss=1.4890661506475267\n",
      "Gradient Descent(5/49): loss=9.24636626121028\n",
      "Gradient Descent(6/49): loss=88.5085416780358\n",
      "Gradient Descent(7/49): loss=908.7270732006025\n",
      "Gradient Descent(8/49): loss=9411.767684952683\n",
      "Gradient Descent(9/49): loss=97583.24315165644\n",
      "Gradient Descent(10/49): loss=1011901.0121519128\n",
      "Gradient Descent(11/49): loss=10493210.504513545\n",
      "Gradient Descent(12/49): loss=108812737.28070423\n",
      "Gradient Descent(13/49): loss=1128369187.665519\n",
      "Gradient Descent(14/49): loss=11700992990.033106\n",
      "Gradient Descent(15/49): loss=121337270830.00711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=1258246485259.7102\n",
      "Gradient Descent(17/49): loss=13047798149779.666\n",
      "Gradient Descent(18/49): loss=135303407205401.05\n",
      "Gradient Descent(19/49): loss=1403072900981066.2\n",
      "Gradient Descent(20/49): loss=1.4549623000103e+16\n",
      "Gradient Descent(21/49): loss=1.5087707081870848e+17\n",
      "Gradient Descent(22/49): loss=1.5645690956165898e+18\n",
      "Gradient Descent(23/49): loss=1.6224310570688762e+19\n",
      "Gradient Descent(24/49): loss=1.682432909045972e+20\n",
      "Gradient Descent(25/49): loss=1.7446537904388233e+21\n",
      "Gradient Descent(26/49): loss=1.809175766906844e+22\n",
      "Gradient Descent(27/49): loss=1.8760839391176332e+23\n",
      "Gradient Descent(28/49): loss=1.9454665549897244e+24\n",
      "Gradient Descent(29/49): loss=2.0174151260863873e+25\n",
      "Gradient Descent(30/49): loss=2.0920245483138972e+26\n",
      "Gradient Descent(31/49): loss=2.1693932270836213e+27\n",
      "Gradient Descent(32/49): loss=2.2496232071030917e+28\n",
      "Gradient Descent(33/49): loss=2.3328203069667813e+29\n",
      "Gradient Descent(34/49): loss=2.4190942587245886e+30\n",
      "Gradient Descent(35/49): loss=2.5085588526117605e+31\n",
      "Gradient Descent(36/49): loss=2.6013320871319796e+32\n",
      "Gradient Descent(37/49): loss=2.697536324689758e+33\n",
      "Gradient Descent(38/49): loss=2.7972984529797245e+34\n",
      "Gradient Descent(39/49): loss=2.9007500523436773e+35\n",
      "Gradient Descent(40/49): loss=3.008027569317317e+36\n",
      "Gradient Descent(41/49): loss=3.119272496595311e+37\n",
      "Gradient Descent(42/49): loss=3.2346315596517423e+38\n",
      "Gradient Descent(43/49): loss=3.354256910261978e+39\n",
      "Gradient Descent(44/49): loss=3.4783063271822787e+40\n",
      "Gradient Descent(45/49): loss=3.606943424250581e+41\n",
      "Gradient Descent(46/49): loss=3.7403378661831265e+42\n",
      "Gradient Descent(47/49): loss=3.878665592352677e+43\n",
      "Gradient Descent(48/49): loss=4.022109048841699e+44\n",
      "Gradient Descent(49/49): loss=4.170857429078285e+45\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49720623582690116\n",
      "Gradient Descent(2/49): loss=0.5561729696057367\n",
      "Gradient Descent(3/49): loss=0.787559270408378\n",
      "Gradient Descent(4/49): loss=2.5937039284868377\n",
      "Gradient Descent(5/49): loss=21.209297227405443\n",
      "Gradient Descent(6/49): loss=223.08840747770745\n",
      "Gradient Descent(7/49): loss=2428.715216527271\n",
      "Gradient Descent(8/49): loss=26551.27223938698\n",
      "Gradient Descent(9/49): loss=290413.3592587749\n",
      "Gradient Descent(10/49): loss=3176698.5613677013\n",
      "Gradient Descent(11/49): loss=34748740.91518828\n",
      "Gradient Descent(12/49): loss=380104158.9838141\n",
      "Gradient Descent(13/49): loss=4157825271.0979786\n",
      "Gradient Descent(14/49): loss=45480984415.979546\n",
      "Gradient Descent(15/49): loss=497500451366.1041\n",
      "Gradient Descent(16/49): loss=5441982014671.129\n",
      "Gradient Descent(17/49): loss=59527922373106.3\n",
      "Gradient Descent(18/49): loss=651154952833377.0\n",
      "Gradient Descent(19/49): loss=7122754426776332.0\n",
      "Gradient Descent(20/49): loss=7.791329913626515e+16\n",
      "Gradient Descent(21/49): loss=8.522661064204047e+17\n",
      "Gradient Descent(22/49): loss=9.322638422519171e+18\n",
      "Gradient Descent(23/49): loss=1.0197705446960767e+20\n",
      "Gradient Descent(24/49): loss=1.1154910409457939e+21\n",
      "Gradient Descent(25/49): loss=1.2201963166147132e+22\n",
      "Gradient Descent(26/49): loss=1.3347297256799917e+23\n",
      "Gradient Descent(27/49): loss=1.4600137833200577e+24\n",
      "Gradient Descent(28/49): loss=1.5970575963598593e+25\n",
      "Gradient Descent(29/49): loss=1.7469649911734043e+26\n",
      "Gradient Descent(30/49): loss=1.9109434045093325e+27\n",
      "Gradient Descent(31/49): loss=2.090313608851951e+28\n",
      "Gradient Descent(32/49): loss=2.2865203506504608e+29\n",
      "Gradient Descent(33/49): loss=2.5011439871025173e+30\n",
      "Gradient Descent(34/49): loss=2.735913215222214e+31\n",
      "Gradient Descent(35/49): loss=2.992718995717956e+32\n",
      "Gradient Descent(36/49): loss=3.2736297838319345e+33\n",
      "Gradient Descent(37/49): loss=3.580908189818565e+34\n",
      "Gradient Descent(38/49): loss=3.9170292032533056e+35\n",
      "Gradient Descent(39/49): loss=4.284700127962907e+36\n",
      "Gradient Descent(40/49): loss=4.686882388141813e+37\n",
      "Gradient Descent(41/49): loss=5.126815381294403e+38\n",
      "Gradient Descent(42/49): loss=5.608042570126916e+39\n",
      "Gradient Descent(43/49): loss=6.134440023548399e+40\n",
      "Gradient Descent(44/49): loss=6.710247636665143e+41\n",
      "Gradient Descent(45/49): loss=7.34010328123215e+42\n",
      "Gradient Descent(46/49): loss=8.029080161627491e+43\n",
      "Gradient Descent(47/49): loss=8.782727677234974e+44\n",
      "Gradient Descent(48/49): loss=9.607116120364327e+45\n",
      "Gradient Descent(49/49): loss=1.0508885569729798e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49849210238823394\n",
      "Gradient Descent(2/49): loss=0.5602133186631221\n",
      "Gradient Descent(3/49): loss=0.779251712088186\n",
      "Gradient Descent(4/49): loss=2.334131777936146\n",
      "Gradient Descent(5/49): loss=17.575524682325867\n",
      "Gradient Descent(6/49): loss=177.02247525438398\n",
      "Gradient Descent(7/49): loss=1861.988394512191\n",
      "Gradient Descent(8/49): loss=19694.403281215822\n",
      "Gradient Descent(9/49): loss=208459.97904792783\n",
      "Gradient Descent(10/49): loss=2206706.164537847\n",
      "Gradient Descent(11/49): loss=23359958.48774586\n",
      "Gradient Descent(12/49): loss=247286509.37150365\n",
      "Gradient Descent(13/49): loss=2617754376.909659\n",
      "Gradient Descent(14/49): loss=27711330665.941013\n",
      "Gradient Descent(15/49): loss=293349849052.6079\n",
      "Gradient Descent(16/49): loss=3105377184575.7524\n",
      "Gradient Descent(17/49): loss=32873265456505.824\n",
      "Gradient Descent(18/49): loss=347993663107451.1\n",
      "Gradient Descent(19/49): loss=3683832070879626.0\n",
      "Gradient Descent(20/49): loss=3.899674093276824e+16\n",
      "Gradient Descent(21/49): loss=4.128162668974003e+17\n",
      "Gradient Descent(22/49): loss=4.3700387811616783e+18\n",
      "Gradient Descent(23/49): loss=4.626086828502547e+19\n",
      "Gradient Descent(24/49): loss=4.897137168919177e+20\n",
      "Gradient Descent(25/49): loss=5.184068812424856e+21\n",
      "Gradient Descent(26/49): loss=5.487812271733417e+22\n",
      "Gradient Descent(27/49): loss=5.809352579889945e+23\n",
      "Gradient Descent(28/49): loss=6.149732484710799e+24\n",
      "Gradient Descent(29/49): loss=6.510055830390806e+25\n",
      "Gradient Descent(30/49): loss=6.891491137243872e+26\n",
      "Gradient Descent(31/49): loss=7.295275391188137e+27\n",
      "Gradient Descent(32/49): loss=7.722718055262432e+28\n",
      "Gradient Descent(33/49): loss=8.175205316185141e+29\n",
      "Gradient Descent(34/49): loss=8.654204579725819e+30\n",
      "Gradient Descent(35/49): loss=9.161269229467627e+31\n",
      "Gradient Descent(36/49): loss=9.698043664395024e+32\n",
      "Gradient Descent(37/49): loss=1.0266268631642474e+34\n",
      "Gradient Descent(38/49): loss=1.0867786871695855e+35\n",
      "Gradient Descent(39/49): loss=1.1504549094358626e+36\n",
      "Gradient Descent(40/49): loss=1.2178620304857894e+37\n",
      "Gradient Descent(41/49): loss=1.2892186500610741e+38\n",
      "Gradient Descent(42/49): loss=1.3647561760360718e+39\n",
      "Gradient Descent(43/49): loss=1.4447195748683494e+40\n",
      "Gradient Descent(44/49): loss=1.5293681660192472e+41\n",
      "Gradient Descent(45/49): loss=1.618976462921064e+42\n",
      "Gradient Descent(46/49): loss=1.7138350632173984e+43\n",
      "Gradient Descent(47/49): loss=1.8142515911651017e+44\n",
      "Gradient Descent(48/49): loss=1.920551695252426e+45\n",
      "Gradient Descent(49/49): loss=2.03308010426942e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49918629606106707\n",
      "Gradient Descent(2/49): loss=0.5603526031090346\n",
      "Gradient Descent(3/49): loss=0.7673787167478963\n",
      "Gradient Descent(4/49): loss=2.1928951609806218\n",
      "Gradient Descent(5/49): loss=16.073190955437973\n",
      "Gradient Descent(6/49): loss=161.17388790387898\n",
      "Gradient Descent(7/49): loss=1694.7925720525236\n",
      "Gradient Descent(8/49): loss=17930.223944194797\n",
      "Gradient Descent(9/49): loss=189844.21847795768\n",
      "Gradient Descent(10/49): loss=2010270.8301994207\n",
      "Gradient Descent(11/49): loss=21287171.632558737\n",
      "Gradient Descent(12/49): loss=225414687.5153169\n",
      "Gradient Descent(13/49): loss=2386967901.912156\n",
      "Gradient Descent(14/49): loss=25276152367.83216\n",
      "Gradient Descent(15/49): loss=267654995092.8288\n",
      "Gradient Descent(16/49): loss=2834260349930.723\n",
      "Gradient Descent(17/49): loss=30012635218389.145\n",
      "Gradient Descent(18/49): loss=317810702460245.6\n",
      "Gradient Descent(19/49): loss=3365370680166924.0\n",
      "Gradient Descent(20/49): loss=3.5636684753708996e+16\n",
      "Gradient Descent(21/49): loss=3.7736505750158675e+17\n",
      "Gradient Descent(22/49): loss=3.996005453575504e+18\n",
      "Gradient Descent(23/49): loss=4.231462152517362e+19\n",
      "Gradient Descent(24/49): loss=4.4807926706322257e+20\n",
      "Gradient Descent(25/49): loss=4.744814495208733e+21\n",
      "Gradient Descent(26/49): loss=5.0243932823532895e+22\n",
      "Gradient Descent(27/49): loss=5.3204456952423574e+23\n",
      "Gradient Descent(28/49): loss=5.633942409612677e+24\n",
      "Gradient Descent(29/49): loss=5.965911296344131e+25\n",
      "Gradient Descent(30/49): loss=6.317440791570694e+26\n",
      "Gradient Descent(31/49): loss=6.689683465367137e+27\n",
      "Gradient Descent(32/49): loss=7.083859800715299e+28\n",
      "Gradient Descent(33/49): loss=7.501262195136971e+29\n",
      "Gradient Descent(34/49): loss=7.943259198115266e+30\n",
      "Gradient Descent(35/49): loss=8.411299998198628e+31\n",
      "Gradient Descent(36/49): loss=8.906919174497395e+32\n",
      "Gradient Descent(37/49): loss=9.431741728153559e+33\n",
      "Gradient Descent(38/49): loss=9.987488410279881e+34\n",
      "Gradient Descent(39/49): loss=1.0575981363837125e+36\n",
      "Gradient Descent(40/49): loss=1.1199150097946996e+37\n",
      "Gradient Descent(41/49): loss=1.1859037814230985e+38\n",
      "Gradient Descent(42/49): loss=1.2557808105915449e+39\n",
      "Gradient Descent(43/49): loss=1.329775205166764e+40\n",
      "Gradient Descent(44/49): loss=1.408129572742343e+41\n",
      "Gradient Descent(45/49): loss=1.4911008160833074e+42\n",
      "Gradient Descent(46/49): loss=1.5789609754408643e+43\n",
      "Gradient Descent(47/49): loss=1.671998120498532e+44\n",
      "Gradient Descent(48/49): loss=1.77051729487492e+45\n",
      "Gradient Descent(49/49): loss=1.8748415162790765e+46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49656704089296805\n",
      "Gradient Descent(2/49): loss=0.5504029872669397\n",
      "Gradient Descent(3/49): loss=0.7102010127202723\n",
      "Gradient Descent(4/49): loss=1.696337147414538\n",
      "Gradient Descent(5/49): loss=11.156421907349381\n",
      "Gradient Descent(6/49): loss=111.27601127599378\n",
      "Gradient Descent(7/49): loss=1187.1532701470242\n",
      "Gradient Descent(8/49): loss=12773.805965378217\n",
      "Gradient Descent(9/49): loss=137594.9199717422\n",
      "Gradient Descent(10/49): loss=1482331.274044002\n",
      "Gradient Descent(11/49): loss=15969680.181728391\n",
      "Gradient Descent(12/49): loss=172047457.3233907\n",
      "Gradient Descent(13/49): loss=1853533535.829623\n",
      "Gradient Descent(14/49): loss=19968831772.290806\n",
      "Gradient Descent(15/49): loss=215131928980.3173\n",
      "Gradient Descent(16/49): loss=2317699274418.913\n",
      "Gradient Descent(17/49): loss=24969468515431.6\n",
      "Gradient Descent(18/49): loss=269005718229258.1\n",
      "Gradient Descent(19/49): loss=2898102392347718.0\n",
      "Gradient Descent(20/49): loss=3.1222375241022028e+16\n",
      "Gradient Descent(21/49): loss=3.363706949296356e+17\n",
      "Gradient Descent(22/49): loss=3.623851277617965e+18\n",
      "Gradient Descent(23/49): loss=3.904114799608489e+19\n",
      "Gradient Descent(24/49): loss=4.206053505192507e+20\n",
      "Gradient Descent(25/49): loss=4.5313437223507397e+21\n",
      "Gradient Descent(26/49): loss=4.881791423893759e+22\n",
      "Gradient Descent(27/49): loss=5.259342254010083e+23\n",
      "Gradient Descent(28/49): loss=5.666092330252328e+24\n",
      "Gradient Descent(29/49): loss=6.1042998809338335e+25\n",
      "Gradient Descent(30/49): loss=6.576397782545633e+26\n",
      "Gradient Descent(31/49): loss=7.085007066797924e+27\n",
      "Gradient Descent(32/49): loss=7.632951472279343e+28\n",
      "Gradient Descent(33/49): loss=8.223273121519181e+29\n",
      "Gradient Descent(34/49): loss=8.859249410491338e+30\n",
      "Gradient Descent(35/49): loss=9.544411204329896e+31\n",
      "Gradient Descent(36/49): loss=1.028256244027376e+33\n",
      "Gradient Descent(37/49): loss=1.1077801246677728e+34\n",
      "Gradient Descent(38/49): loss=1.1934542695334522e+35\n",
      "Gradient Descent(39/49): loss=1.2857543313433527e+36\n",
      "Gradient Descent(40/49): loss=1.3851927491234423e+37\n",
      "Gradient Descent(41/49): loss=1.49232159320785e+38\n",
      "Gradient Descent(42/49): loss=1.6077356302678035e+39\n",
      "Gradient Descent(43/49): loss=1.7320756253860764e+40\n",
      "Gradient Descent(44/49): loss=1.8660318995087256e+41\n",
      "Gradient Descent(45/49): loss=2.010348162025593e+42\n",
      "Gradient Descent(46/49): loss=2.1658256397565548e+43\n",
      "Gradient Descent(47/49): loss=2.333327525268315e+44\n",
      "Gradient Descent(48/49): loss=2.5137837692172968e+45\n",
      "Gradient Descent(49/49): loss=2.7081962433258485e+46\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5031301688237527\n",
      "Gradient Descent(2/49): loss=0.575277144116213\n",
      "Gradient Descent(3/49): loss=0.8493434008537005\n",
      "Gradient Descent(4/49): loss=2.978999167054821\n",
      "Gradient Descent(5/49): loss=25.54656531077193\n",
      "Gradient Descent(6/49): loss=279.2128092724194\n",
      "Gradient Descent(7/49): loss=3155.565641370312\n",
      "Gradient Descent(8/49): loss=35811.316166411074\n",
      "Gradient Descent(9/49): loss=406622.3143538203\n",
      "Gradient Descent(10/49): loss=4617339.677619053\n",
      "Gradient Descent(11/49): loss=52431997.16532026\n",
      "Gradient Descent(12/49): loss=595389967.6841301\n",
      "Gradient Descent(13/49): loss=6760934023.516132\n",
      "Gradient Descent(14/49): loss=76773597838.43097\n",
      "Gradient Descent(15/49): loss=871800452953.5856\n",
      "Gradient Descent(16/49): loss=9899705777896.395\n",
      "Gradient Descent(17/49): loss=112415833420369.4\n",
      "Gradient Descent(18/49): loss=1276534867523272.8\n",
      "Gradient Descent(19/49): loss=1.4495656158248288e+16\n",
      "Gradient Descent(20/49): loss=1.646050200460904e+17\n",
      "Gradient Descent(21/49): loss=1.8691677236671393e+18\n",
      "Gradient Descent(22/49): loss=2.1225282061389623e+19\n",
      "Gradient Descent(23/49): loss=2.4102309968293226e+20\n",
      "Gradient Descent(24/49): loss=2.7369311000320153e+21\n",
      "Gradient Descent(25/49): loss=3.107914492916574e+22\n",
      "Gradient Descent(26/49): loss=3.529183652145118e+23\n",
      "Gradient Descent(27/49): loss=4.007554673384936e+24\n",
      "Gradient Descent(28/49): loss=4.550767555099461e+25\n",
      "Gradient Descent(29/49): loss=5.1676114310009896e+26\n",
      "Gradient Descent(30/49): loss=5.868066777414037e+27\n",
      "Gradient Descent(31/49): loss=6.6634668964497005e+28\n",
      "Gradient Descent(32/49): loss=7.566681287776245e+29\n",
      "Gradient Descent(33/49): loss=8.592323875922684e+30\n",
      "Gradient Descent(34/49): loss=9.756989462212157e+31\n",
      "Gradient Descent(35/49): loss=1.1079522227098958e+33\n",
      "Gradient Descent(36/49): loss=1.2581320627248682e+34\n",
      "Gradient Descent(37/49): loss=1.4286683620570229e+35\n",
      "Gradient Descent(38/49): loss=1.6223203821083956e+36\n",
      "Gradient Descent(39/49): loss=1.8422213944843414e+37\n",
      "Gradient Descent(40/49): loss=2.0919293770353662e+38\n",
      "Gradient Descent(41/49): loss=2.3754845816067483e+39\n",
      "Gradient Descent(42/49): loss=2.6974749049359727e+40\n",
      "Gradient Descent(43/49): loss=3.063110120394013e+41\n",
      "Gradient Descent(44/49): loss=3.478306171631691e+42\n",
      "Gradient Descent(45/49): loss=3.9497808919956047e+43\n",
      "Gradient Descent(46/49): loss=4.485162698445136e+44\n",
      "Gradient Descent(47/49): loss=5.093114018625819e+45\n",
      "Gradient Descent(48/49): loss=5.783471448140783e+46\n",
      "Gradient Descent(49/49): loss=6.567404905748566e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5044837841917034\n",
      "Gradient Descent(2/49): loss=0.5798202876132938\n",
      "Gradient Descent(3/49): loss=0.840748219577187\n",
      "Gradient Descent(4/49): loss=2.6802819216248803\n",
      "Gradient Descent(5/49): loss=21.184018926213135\n",
      "Gradient Descent(6/49): loss=221.75587853544215\n",
      "Gradient Descent(7/49): loss=2421.674742967794\n",
      "Gradient Descent(8/49): loss=26593.34777514207\n",
      "Gradient Descent(9/49): loss=292249.0590470436\n",
      "Gradient Descent(10/49): loss=3212015.4649496833\n",
      "Gradient Descent(11/49): loss=35302736.765998475\n",
      "Gradient Descent(12/49): loss=388007402.9680618\n",
      "Gradient Descent(13/49): loss=4264535926.8029966\n",
      "Gradient Descent(14/49): loss=46870929037.656044\n",
      "Gradient Descent(15/49): loss=515151950945.0433\n",
      "Gradient Descent(16/49): loss=5661964425564.296\n",
      "Gradient Descent(17/49): loss=62229874315371.91\n",
      "Gradient Descent(18/49): loss=683960012164258.1\n",
      "Gradient Descent(19/49): loss=7517310670921826.0\n",
      "Gradient Descent(20/49): loss=8.262173039087893e+16\n",
      "Gradient Descent(21/49): loss=9.080841050229373e+17\n",
      "Gradient Descent(22/49): loss=9.980627831129862e+18\n",
      "Gradient Descent(23/49): loss=1.0969571139119235e+20\n",
      "Gradient Descent(24/49): loss=1.2056505163019694e+21\n",
      "Gradient Descent(25/49): loss=1.3251139438583115e+22\n",
      "Gradient Descent(26/49): loss=1.4564145583361254e+23\n",
      "Gradient Descent(27/49): loss=1.600725262581863e+24\n",
      "Gradient Descent(28/49): loss=1.7593351780245236e+25\n",
      "Gradient Descent(29/49): loss=1.9336611603431004e+26\n",
      "Gradient Descent(30/49): loss=2.125260456178555e+27\n",
      "Gradient Descent(31/49): loss=2.3358446139523394e+28\n",
      "Gradient Descent(32/49): loss=2.567294773056185e+29\n",
      "Gradient Descent(33/49): loss=2.821678467990858e+30\n",
      "Gradient Descent(34/49): loss=3.101268097564478e+31\n",
      "Gradient Descent(35/49): loss=3.408561224135356e+32\n",
      "Gradient Descent(36/49): loss=3.746302884230767e+33\n",
      "Gradient Descent(37/49): loss=4.1175101098428643e+34\n",
      "Gradient Descent(38/49): loss=4.5254988794479255e+35\n",
      "Gradient Descent(39/49): loss=4.973913739501569e+36\n",
      "Gradient Descent(40/49): loss=5.466760361019078e+37\n",
      "Gradient Descent(41/49): loss=6.0084413220652854e+38\n",
      "Gradient Descent(42/49): loss=6.603795435798641e+39\n",
      "Gradient Descent(43/49): loss=7.258140975384301e+40\n",
      "Gradient Descent(44/49): loss=7.977323181904535e+41\n",
      "Gradient Descent(45/49): loss=8.767766479650368e+42\n",
      "Gradient Descent(46/49): loss=9.63653186523269e+43\n",
      "Gradient Descent(47/49): loss=1.0591379983166277e+45\n",
      "Gradient Descent(48/49): loss=1.1640840451380288e+46\n",
      "Gradient Descent(49/49): loss=1.2794288055934524e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5053160751118617\n",
      "Gradient Descent(2/49): loss=0.5801815280472319\n",
      "Gradient Descent(3/49): loss=0.8277859796631446\n",
      "Gradient Descent(4/49): loss=2.5165873844598092\n",
      "Gradient Descent(5/49): loss=19.371825320992784\n",
      "Gradient Descent(6/49): loss=201.90213075484976\n",
      "Gradient Descent(7/49): loss=2204.2082635087186\n",
      "Gradient Descent(8/49): loss=24210.92920331851\n",
      "Gradient Descent(9/49): loss=266147.5776604722\n",
      "Gradient Descent(10/49): loss=2926050.1098024156\n",
      "Gradient Descent(11/49): loss=32169753.748549696\n",
      "Gradient Descent(12/49): loss=353683394.035963\n",
      "Gradient Descent(13/49): loss=3888496742.998473\n",
      "Gradient Descent(14/49): loss=42751251156.20798\n",
      "Gradient Descent(15/49): loss=470019549522.2092\n",
      "Gradient Descent(16/49): loss=5167530099579.332\n",
      "Gradient Descent(17/49): loss=56813312043972.28\n",
      "Gradient Descent(18/49): loss=624621891555886.4\n",
      "Gradient Descent(19/49): loss=6867272710839997.0\n",
      "Gradient Descent(20/49): loss=7.550077114267731e+16\n",
      "Gradient Descent(21/49): loss=8.300771912175994e+17\n",
      "Gradient Descent(22/49): loss=9.126107362236145e+18\n",
      "Gradient Descent(23/49): loss=1.0033504891863402e+20\n",
      "Gradient Descent(24/49): loss=1.1031123831791257e+21\n",
      "Gradient Descent(25/49): loss=1.2127934784881981e+22\n",
      "Gradient Descent(26/49): loss=1.3333800289908052e+23\n",
      "Gradient Descent(27/49): loss=1.4659563505633056e+24\n",
      "Gradient Descent(28/49): loss=1.6117145712639497e+25\n",
      "Gradient Descent(29/49): loss=1.771965350964509e+26\n",
      "Gradient Descent(30/49): loss=1.948149666821222e+27\n",
      "Gradient Descent(31/49): loss=2.1418517705607997e+28\n",
      "Gradient Descent(32/49): loss=2.3548134340929872e+29\n",
      "Gradient Descent(33/49): loss=2.5889496115470894e+30\n",
      "Gradient Descent(34/49): loss=2.8463656585649843e+31\n",
      "Gradient Descent(35/49): loss=3.1293762636873702e+32\n",
      "Gradient Descent(36/49): loss=3.440526262064028e+33\n",
      "Gradient Descent(37/49): loss=3.7826135186455445e+34\n",
      "Gradient Descent(38/49): loss=4.158714086622389e+35\n",
      "Gradient Descent(39/49): loss=4.572209867336416e+36\n",
      "Gradient Descent(40/49): loss=5.026819020383029e+37\n",
      "Gradient Descent(41/49): loss=5.5266293973521556e+38\n",
      "Gradient Descent(42/49): loss=6.076135299844154e+39\n",
      "Gradient Descent(43/49): loss=6.680277892289949e+40\n",
      "Gradient Descent(44/49): loss=7.344489632968453e+41\n",
      "Gradient Descent(45/49): loss=8.074743122743119e+42\n",
      "Gradient Descent(46/49): loss=8.877604810769231e+43\n",
      "Gradient Descent(47/49): loss=9.760294040093224e+44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=1.0730747964081058e+46\n",
      "Gradient Descent(49/49): loss=1.1797692917408096e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5024250473544637\n",
      "Gradient Descent(2/49): loss=0.5690009112632827\n",
      "Gradient Descent(3/49): loss=0.7628157683441782\n",
      "Gradient Descent(4/49): loss=1.937381141998196\n",
      "Gradient Descent(5/49): loss=13.431738514848197\n",
      "Gradient Descent(6/49): loss=139.33070711186005\n",
      "Gradient Descent(7/49): loss=1543.188050452427\n",
      "Gradient Descent(8/49): loss=17237.983805273976\n",
      "Gradient Descent(9/49): loss=192767.78715118137\n",
      "Gradient Descent(10/49): loss=2155989.361151678\n",
      "Gradient Descent(11/49): loss=24113904.06195094\n",
      "Gradient Descent(12/49): loss=269705411.27191013\n",
      "Gradient Descent(13/49): loss=3016559958.2704487\n",
      "Gradient Descent(14/49): loss=33739161714.802044\n",
      "Gradient Descent(15/49): loss=377360655154.18854\n",
      "Gradient Descent(16/49): loss=4220646186211.0493\n",
      "Gradient Descent(17/49): loss=47206442931191.69\n",
      "Gradient Descent(18/49): loss=527987458779858.9\n",
      "Gradient Descent(19/49): loss=5905354000852208.0\n",
      "Gradient Descent(20/49): loss=6.604930722404968e+16\n",
      "Gradient Descent(21/49): loss=7.387382677054826e+17\n",
      "Gradient Descent(22/49): loss=8.262527664693058e+18\n",
      "Gradient Descent(23/49): loss=9.241346549145402e+19\n",
      "Gradient Descent(24/49): loss=1.0336121040336902e+21\n",
      "Gradient Descent(25/49): loss=1.1560587798796128e+22\n",
      "Gradient Descent(26/49): loss=1.2930110796121272e+23\n",
      "Gradient Descent(27/49): loss=1.446187409409887e+24\n",
      "Gradient Descent(28/49): loss=1.617509746137006e+25\n",
      "Gradient Descent(29/49): loss=1.8091277533080846e+26\n",
      "Gradient Descent(30/49): loss=2.0234457539474509e+27\n",
      "Gradient Descent(31/49): loss=2.26315289878301e+28\n",
      "Gradient Descent(32/49): loss=2.531256908309927e+29\n",
      "Gradient Descent(33/49): loss=2.8311218121021523e+30\n",
      "Gradient Descent(34/49): loss=3.166510158904512e+31\n",
      "Gradient Descent(35/49): loss=3.54163022713621e+32\n",
      "Gradient Descent(36/49): loss=3.9611888281779934e+33\n",
      "Gradient Descent(37/49): loss=4.430450364991985e+34\n",
      "Gradient Descent(38/49): loss=4.955302887109884e+35\n",
      "Gradient Descent(39/49): loss=5.54233197081384e+36\n",
      "Gradient Descent(40/49): loss=6.198903351520665e+37\n",
      "Gradient Descent(41/49): loss=6.933255345195671e+38\n",
      "Gradient Descent(42/49): loss=7.754602218454272e+39\n",
      "Gradient Descent(43/49): loss=8.673249804382964e+40\n",
      "Gradient Descent(44/49): loss=9.700724814769675e+41\n",
      "Gradient Descent(45/49): loss=1.0849919471284656e+43\n",
      "Gradient Descent(46/49): loss=1.213525327036636e+44\n",
      "Gradient Descent(47/49): loss=1.3572853911560572e+45\n",
      "Gradient Descent(48/49): loss=1.5180759659497747e+46\n",
      "Gradient Descent(49/49): loss=1.6979145678650449e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5092245138872526\n",
      "Gradient Descent(2/49): loss=0.5959065599763212\n",
      "Gradient Descent(3/49): loss=0.9188117624741623\n",
      "Gradient Descent(4/49): loss=3.4226757670302144\n",
      "Gradient Descent(5/49): loss=30.683615824363354\n",
      "Gradient Descent(6/49): loss=348.04988245414387\n",
      "Gradient Descent(7/49): loss=4080.335793516634\n",
      "Gradient Descent(8/49): loss=48036.445458989816\n",
      "Gradient Descent(9/49): loss=565825.7881438041\n",
      "Gradient Descent(10/49): loss=6665401.881192383\n",
      "Gradient Descent(11/49): loss=78518913.30009016\n",
      "Gradient Descent(12/49): loss=924959723.1269519\n",
      "Gradient Descent(13/49): loss=10896109216.238117\n",
      "Gradient Descent(14/49): loss=128357155785.6914\n",
      "Gradient Descent(15/49): loss=1512058954024.3384\n",
      "Gradient Descent(16/49): loss=17812191830563.24\n",
      "Gradient Descent(17/49): loss=209829237800937.0\n",
      "Gradient Descent(18/49): loss=2471807481943399.5\n",
      "Gradient Descent(19/49): loss=2.9118116673505624e+16\n",
      "Gradient Descent(20/49): loss=3.4301405947095904e+17\n",
      "Gradient Descent(21/49): loss=4.0407367795804897e+18\n",
      "Gradient Descent(22/49): loss=4.760024631945675e+19\n",
      "Gradient Descent(23/49): loss=5.607352255962269e+20\n",
      "Gradient Descent(24/49): loss=6.605511894082911e+21\n",
      "Gradient Descent(25/49): loss=7.781353014959298e+22\n",
      "Gradient Descent(26/49): loss=9.166504536560397e+23\n",
      "Gradient Descent(27/49): loss=1.0798225611567687e+25\n",
      "Gradient Descent(28/49): loss=1.2720407860297365e+26\n",
      "Gradient Descent(29/49): loss=1.4984756010189477e+27\n",
      "Gradient Descent(30/49): loss=1.7652178699846787e+28\n",
      "Gradient Descent(31/49): loss=2.079442685883192e+29\n",
      "Gradient Descent(32/49): loss=2.449602373394595e+30\n",
      "Gradient Descent(33/49): loss=2.885653847772183e+31\n",
      "Gradient Descent(34/49): loss=3.3993264456317195e+32\n",
      "Gradient Descent(35/49): loss=4.0044374320544593e+33\n",
      "Gradient Descent(36/49): loss=4.71726367082095e+34\n",
      "Gradient Descent(37/49): loss=5.55697945532136e+35\n",
      "Gradient Descent(38/49): loss=6.546172277347056e+36\n",
      "Gradient Descent(39/49): loss=7.711450407410138e+37\n",
      "Gradient Descent(40/49): loss=9.084158629879982e+38\n",
      "Gradient Descent(41/49): loss=1.0701221385477378e+40\n",
      "Gradient Descent(42/49): loss=1.2606136000788032e+41\n",
      "Gradient Descent(43/49): loss=1.4850142721654558e+42\n",
      "Gradient Descent(44/49): loss=1.7493603023140892e+43\n",
      "Gradient Descent(45/49): loss=2.0607623271188882e+44\n",
      "Gradient Descent(46/49): loss=2.4275967410799645e+45\n",
      "Gradient Descent(47/49): loss=2.8597310130088605e+46\n",
      "Gradient Descent(48/49): loss=3.368789110800363e+47\n",
      "Gradient Descent(49/49): loss=3.968464174225387e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5106472812920885\n",
      "Gradient Descent(2/49): loss=0.6009893420959611\n",
      "Gradient Descent(3/49): loss=0.9099711824492118\n",
      "Gradient Descent(4/49): loss=3.0798134730345077\n",
      "Gradient Descent(5/49): loss=25.463591142622043\n",
      "Gradient Descent(6/49): loss=276.67257930601767\n",
      "Gradient Descent(7/49): loss=3134.430210843391\n",
      "Gradient Descent(8/49): loss=35710.92566723381\n",
      "Gradient Descent(9/49): loss=407174.21306706156\n",
      "Gradient Descent(10/49): loss=4643088.447101579\n",
      "Gradient Descent(11/49): loss=52946894.410960466\n",
      "Gradient Descent(12/49): loss=603774858.2333561\n",
      "Gradient Descent(13/49): loss=6885091308.055555\n",
      "Gradient Descent(14/49): loss=78513512174.22389\n",
      "Gradient Descent(15/49): loss=895321697794.4144\n",
      "Gradient Descent(16/49): loss=10209719591186.574\n",
      "Gradient Descent(17/49): loss=116425609257949.11\n",
      "Gradient Descent(18/49): loss=1327648851699363.5\n",
      "Gradient Descent(19/49): loss=1.5139722992728004e+16\n",
      "Gradient Descent(20/49): loss=1.726444548972926e+17\n",
      "Gradient Descent(21/49): loss=1.968735347476455e+18\n",
      "Gradient Descent(22/49): loss=2.2450294570475626e+19\n",
      "Gradient Descent(23/49): loss=2.560098933293285e+20\n",
      "Gradient Descent(24/49): loss=2.9193855464456454e+21\n",
      "Gradient Descent(25/49): loss=3.3290947697211865e+22\n",
      "Gradient Descent(26/49): loss=3.7963029580928526e+23\n",
      "Gradient Descent(27/49): loss=4.329079568627445e+24\n",
      "Gradient Descent(28/49): loss=4.936626533337111e+25\n",
      "Gradient Descent(29/49): loss=5.629437191743733e+26\n",
      "Gradient Descent(30/49): loss=6.419477528182387e+27\n",
      "Gradient Descent(31/49): loss=7.320392844115719e+28\n",
      "Gradient Descent(32/49): loss=8.347743435025686e+29\n",
      "Gradient Descent(33/49): loss=9.519273342417508e+30\n",
      "Gradient Descent(34/49): loss=1.0855216822722234e+32\n",
      "Gradient Descent(35/49): loss=1.2378647826325295e+33\n",
      "Gradient Descent(36/49): loss=1.4115878522798581e+34\n",
      "Gradient Descent(37/49): loss=1.609691375552753e+35\n",
      "Gradient Descent(38/49): loss=1.8355969274913265e+36\n",
      "Gradient Descent(39/49): loss=2.093206270089166e+37\n",
      "Gradient Descent(40/49): loss=2.386968742167598e+38\n",
      "Gradient Descent(41/49): loss=2.7219581068053906e+39\n",
      "Gradient Descent(42/49): loss=3.103960183607351e+40\n",
      "Gradient Descent(43/49): loss=3.539572779364864e+41\n",
      "Gradient Descent(44/49): loss=4.036319643076241e+42\n",
      "Gradient Descent(45/49): loss=4.602780413518295e+43\n",
      "Gradient Descent(46/49): loss=5.248738803778387e+44\n",
      "Gradient Descent(47/49): loss=5.985351582138915e+45\n",
      "Gradient Descent(48/49): loss=6.825341267891656e+46\n",
      "Gradient Descent(49/49): loss=7.783215870259247e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5116213276444137\n",
      "Gradient Descent(2/49): loss=0.6015936047566548\n",
      "Gradient Descent(3/49): loss=0.8958400532386578\n",
      "Gradient Descent(4/49): loss=2.890594014604669\n",
      "Gradient Descent(5/49): loss=23.285157107895216\n",
      "Gradient Descent(6/49): loss=251.90455394899533\n",
      "Gradient Descent(7/49): loss=2852.945287411842\n",
      "Gradient Descent(8/49): loss=32511.37774172843\n",
      "Gradient Descent(9/49): loss=370804.14530885\n",
      "Gradient Descent(10/49): loss=4229659.202279275\n",
      "Gradient Descent(11/49): loss=48247360.565804444\n",
      "Gradient Descent(12/49): loss=550354870.670735\n",
      "Gradient Descent(13/49): loss=6277868660.21206\n",
      "Gradient Descent(14/49): loss=71611316807.25105\n",
      "Gradient Descent(15/49): loss=816866521485.2589\n",
      "Gradient Descent(16/49): loss=9317953424677.186\n",
      "Gradient Descent(17/49): loss=106289404395352.28\n",
      "Gradient Descent(18/49): loss=1212437642914360.5\n",
      "Gradient Descent(19/49): loss=1.3830212393451224e+16\n",
      "Gradient Descent(20/49): loss=1.5776050501723347e+17\n",
      "Gradient Descent(21/49): loss=1.7995657792702917e+18\n",
      "Gradient Descent(22/49): loss=2.0527552149804855e+19\n",
      "Gradient Descent(23/49): loss=2.3415670719956864e+20\n",
      "Gradient Descent(24/49): loss=2.671013237545892e+21\n",
      "Gradient Descent(25/49): loss=3.046810745021815e+22\n",
      "Gradient Descent(26/49): loss=3.4754809843283464e+23\n",
      "Gradient Descent(27/49): loss=3.9644628706145045e+24\n",
      "Gradient Descent(28/49): loss=4.5222419352462225e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=5.1584975792016005e+26\n",
      "Gradient Descent(30/49): loss=5.884271044242667e+27\n",
      "Gradient Descent(31/49): loss=6.712157016747468e+28\n",
      "Gradient Descent(32/49): loss=7.656522189193217e+29\n",
      "Gradient Descent(33/49): loss=8.733754572090579e+30\n",
      "Gradient Descent(34/49): loss=9.962547882794189e+31\n",
      "Gradient Descent(35/49): loss=1.1364225946323003e+33\n",
      "Gradient Descent(36/49): loss=1.2963112737668541e+34\n",
      "Gradient Descent(37/49): loss=1.4786954487108784e+35\n",
      "Gradient Descent(38/49): loss=1.686740117352087e+36\n",
      "Gradient Descent(39/49): loss=1.9240555761263948e+37\n",
      "Gradient Descent(40/49): loss=2.194760071180801e+38\n",
      "Gradient Descent(41/49): loss=2.5035512642244716e+39\n",
      "Gradient Descent(42/49): loss=2.8557877532499545e+40\n",
      "Gradient Descent(43/49): loss=3.257582062789731e+41\n",
      "Gradient Descent(44/49): loss=3.715906717413794e+42\n",
      "Gradient Descent(45/49): loss=4.2387152392089827e+43\n",
      "Gradient Descent(46/49): loss=4.835080168968099e+44\n",
      "Gradient Descent(47/49): loss=5.515350506232994e+45\n",
      "Gradient Descent(48/49): loss=6.291331300323844e+46\n",
      "Gradient Descent(49/49): loss=7.176488508881574e+47\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.508451846944303\n",
      "Gradient Descent(2/49): loss=0.5890930121099368\n",
      "Gradient Descent(3/49): loss=0.8222345477396812\n",
      "Gradient Descent(4/49): loss=2.2169413775837246\n",
      "Gradient Descent(5/49): loss=16.13381434398063\n",
      "Gradient Descent(6/49): loss=173.76842652357092\n",
      "Gradient Descent(7/49): loss=1996.3973703126094\n",
      "Gradient Descent(8/49): loss=23134.531156817506\n",
      "Gradient Descent(9/49): loss=268394.8611357971\n",
      "Gradient Descent(10/49): loss=3114270.111812193\n",
      "Gradient Descent(11/49): loss=36136656.12535129\n",
      "Gradient Descent(12/49): loss=419315591.0905459\n",
      "Gradient Descent(13/49): loss=4865575918.857426\n",
      "Gradient Descent(14/49): loss=56458264496.438774\n",
      "Gradient Descent(15/49): loss=655119910315.6401\n",
      "Gradient Descent(16/49): loss=7601758596309.669\n",
      "Gradient Descent(17/49): loss=88207872876205.55\n",
      "Gradient Descent(18/49): loss=1023530113326753.0\n",
      "Gradient Descent(19/49): loss=1.1876648407124016e+16\n",
      "Gradient Descent(20/49): loss=1.3781204436482462e+17\n",
      "Gradient Descent(21/49): loss=1.5991177747269617e+18\n",
      "Gradient Descent(22/49): loss=1.8555545483950277e+19\n",
      "Gradient Descent(23/49): loss=2.1531138834707015e+20\n",
      "Gradient Descent(24/49): loss=2.4983902516928546e+21\n",
      "Gradient Descent(25/49): loss=2.8990356235556563e+22\n",
      "Gradient Descent(26/49): loss=3.36392905029549e+23\n",
      "Gradient Descent(27/49): loss=3.903373440283234e+24\n",
      "Gradient Descent(28/49): loss=4.5293238907551575e+25\n",
      "Gradient Descent(29/49): loss=5.255652635141306e+26\n",
      "Gradient Descent(30/49): loss=6.098456477719817e+27\n",
      "Gradient Descent(31/49): loss=7.076413528924725e+28\n",
      "Gradient Descent(32/49): loss=8.211197147228418e+29\n",
      "Gradient Descent(33/49): loss=9.527956261326225e+30\n",
      "Gradient Descent(34/49): loss=1.1055872717462452e+32\n",
      "Gradient Descent(35/49): loss=1.2828808003755102e+33\n",
      "Gradient Descent(36/49): loss=1.488605368414414e+34\n",
      "Gradient Descent(37/49): loss=1.7273202172981302e+35\n",
      "Gradient Descent(38/49): loss=2.004315714825667e+36\n",
      "Gradient Descent(39/49): loss=2.3257305996110316e+37\n",
      "Gradient Descent(40/49): loss=2.6986880270195255e+38\n",
      "Gradient Descent(41/49): loss=3.1314534316212625e+39\n",
      "Gradient Descent(42/49): loss=3.633617704689725e+40\n",
      "Gradient Descent(43/49): loss=4.2163097463017884e+41\n",
      "Gradient Descent(44/49): loss=4.8924431025902995e+42\n",
      "Gradient Descent(45/49): loss=5.67700215409412e+43\n",
      "Gradient Descent(46/49): loss=6.587374197673797e+44\n",
      "Gradient Descent(47/49): loss=7.643734781548914e+45\n",
      "Gradient Descent(48/49): loss=8.86949483320589e+46\n",
      "Gradient Descent(49/49): loss=1.0291819489362903e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5154892710174009\n",
      "Gradient Descent(2/49): loss=0.6181397749787796\n",
      "Gradient Descent(3/49): loss=0.9967235279629242\n",
      "Gradient Descent(4/49): loss=3.9323340038027306\n",
      "Gradient Descent(5/49): loss=36.749759948555585\n",
      "Gradient Descent(6/49): loss=432.16828016970726\n",
      "Gradient Descent(7/49): loss=5251.785282575585\n",
      "Gradient Descent(8/49): loss=64094.82258937474\n",
      "Gradient Descent(9/49): loss=782686.5989597415\n",
      "Gradient Descent(10/49): loss=9558440.264626946\n",
      "Gradient Descent(11/49): loss=116732272.71169594\n",
      "Gradient Descent(12/49): loss=1425592910.6197019\n",
      "Gradient Descent(13/49): loss=17410057638.374443\n",
      "Gradient Descent(14/49): loss=212620387124.77725\n",
      "Gradient Descent(15/49): loss=2596627200449.7993\n",
      "Gradient Descent(16/49): loss=31711318531834.715\n",
      "Gradient Descent(17/49): loss=387274585641901.0\n",
      "Gradient Descent(18/49): loss=4729592197042505.0\n",
      "Gradient Descent(19/49): loss=5.776016082566394e+16\n",
      "Gradient Descent(20/49): loss=7.053961609404001e+17\n",
      "Gradient Descent(21/49): loss=8.614653019601337e+18\n",
      "Gradient Descent(22/49): loss=1.052064793621679e+20\n",
      "Gradient Descent(23/49): loss=1.2848344877730462e+21\n",
      "Gradient Descent(24/49): loss=1.5691045560875047e+22\n",
      "Gradient Descent(25/49): loss=1.9162694739008772e+23\n",
      "Gradient Descent(26/49): loss=2.3402447481005093e+24\n",
      "Gradient Descent(27/49): loss=2.8580246962151664e+25\n",
      "Gradient Descent(28/49): loss=3.4903636343190024e+26\n",
      "Gradient Descent(29/49): loss=4.262607777990745e+27\n",
      "Gradient Descent(30/49): loss=5.2057112016446e+28\n",
      "Gradient Descent(31/49): loss=6.357476579208488e+29\n",
      "Gradient Descent(32/49): loss=7.764070439100705e+30\n",
      "Gradient Descent(33/49): loss=9.481873669886594e+31\n",
      "Gradient Descent(34/49): loss=1.1579741450941898e+33\n",
      "Gradient Descent(35/49): loss=1.414176319354704e+34\n",
      "Gradient Descent(36/49): loss=1.7270633119886614e+35\n",
      "Gradient Descent(37/49): loss=2.1091766583803934e+36\n",
      "Gradient Descent(38/49): loss=2.575832712892321e+37\n",
      "Gradient Descent(39/49): loss=3.145736578509801e+38\n",
      "Gradient Descent(40/49): loss=3.8417318686286665e+39\n",
      "Gradient Descent(41/49): loss=4.691716353894089e+40\n",
      "Gradient Descent(42/49): loss=5.729760196214247e+41\n",
      "Gradient Descent(43/49): loss=6.997471592431904e+42\n",
      "Gradient Descent(44/49): loss=8.545664567121394e+43\n",
      "Gradient Descent(45/49): loss=1.043639576511288e+45\n",
      "Gradient Descent(46/49): loss=1.2745451885055154e+46\n",
      "Gradient Descent(47/49): loss=1.556538745850279e+47\n",
      "Gradient Descent(48/49): loss=1.90092347386602e+48\n",
      "Gradient Descent(49/49): loss=2.3215034403277753e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5169825936893891\n",
      "Gradient Descent(2/49): loss=0.623800692417697\n",
      "Gradient Descent(3/49): loss=0.9876893377755925\n",
      "Gradient Descent(4/49): loss=3.539802276417979\n",
      "Gradient Descent(5/49): loss=30.523795357841788\n",
      "Gradient Descent(6/49): loss=343.8409139466208\n",
      "Gradient Descent(7/49): loss=4038.1291408123748\n",
      "Gradient Descent(8/49): loss=47699.50888309051\n",
      "Gradient Descent(9/49): loss=563899.7787152503\n",
      "Gradient Descent(10/49): loss=6667164.676060857\n",
      "Gradient Descent(11/49): loss=78829355.49189243\n",
      "Gradient Descent(12/49): loss=932042870.7519015\n",
      "Gradient Descent(13/49): loss=11020060256.319391\n",
      "Gradient Descent(14/49): loss=130296297386.70378\n",
      "Gradient Descent(15/49): loss=1540565556361.095\n",
      "Gradient Descent(16/49): loss=18214963004643.566\n",
      "Gradient Descent(17/49): loss=215365633715259.56\n",
      "Gradient Descent(18/49): loss=2546387614134606.0\n",
      "Gradient Descent(19/49): loss=3.0107356357606564e+16\n",
      "Gradient Descent(20/49): loss=3.559760115910595e+17\n",
      "Gradient Descent(21/49): loss=4.208902280331752e+18\n",
      "Gradient Descent(22/49): loss=4.976419148639897e+19\n",
      "Gradient Descent(23/49): loss=5.8838970100769817e+20\n",
      "Gradient Descent(24/49): loss=6.956858534445402e+21\n",
      "Gradient Descent(25/49): loss=8.22548059311664e+22\n",
      "Gradient Descent(26/49): loss=9.725442978715294e+23\n",
      "Gradient Descent(27/49): loss=1.1498931893584004e+25\n",
      "Gradient Descent(28/49): loss=1.3595826429979695e+26\n",
      "Gradient Descent(29/49): loss=1.6075101411573694e+27\n",
      "Gradient Descent(30/49): loss=1.900648605093725e+28\n",
      "Gradient Descent(31/49): loss=2.2472425072502645e+29\n",
      "Gradient Descent(32/49): loss=2.65703974572586e+30\n",
      "Gradient Descent(33/49): loss=3.1415658023509595e+31\n",
      "Gradient Descent(34/49): loss=3.714447895021994e+32\n",
      "Gradient Descent(35/49): loss=4.3917982410263084e+33\n",
      "Gradient Descent(36/49): loss=5.192667210578128e+34\n",
      "Gradient Descent(37/49): loss=6.139579115436005e+35\n",
      "Gradient Descent(38/49): loss=7.259165701570433e+36\n",
      "Gradient Descent(39/49): loss=8.582915162762648e+37\n",
      "Gradient Descent(40/49): loss=1.0148057741021629e+39\n",
      "Gradient Descent(41/49): loss=1.1998612821189555e+40\n",
      "Gradient Descent(42/49): loss=1.418662696910529e+41\n",
      "Gradient Descent(43/49): loss=1.6773637732949122e+42\n",
      "Gradient Descent(44/49): loss=1.9832404376947477e+43\n",
      "Gradient Descent(45/49): loss=2.344895422405317e+44\n",
      "Gradient Descent(46/49): loss=2.772500216064953e+45\n",
      "Gradient Descent(47/49): loss=3.278081135147249e+46\n",
      "Gradient Descent(48/49): loss=3.875857562189854e+47\n",
      "Gradient Descent(49/49): loss=4.5826418636552386e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.518102053658723\n",
      "Gradient Descent(2/49): loss=0.624670323616721\n",
      "Gradient Descent(3/49): loss=0.9723066342157699\n",
      "Gradient Descent(4/49): loss=3.3216424233513173\n",
      "Gradient Descent(5/49): loss=27.913807714031705\n",
      "Gradient Descent(6/49): loss=313.0656212579497\n",
      "Gradient Descent(7/49): loss=3675.4772184108183\n",
      "Gradient Descent(8/49): loss=43425.44084038437\n",
      "Gradient Descent(9/49): loss=513524.7191837984\n",
      "Gradient Descent(10/49): loss=6073430.924686869\n",
      "Gradient Descent(11/49): loss=71831506.62601472\n",
      "Gradient Descent(12/49): loss=849565869.8396965\n",
      "Gradient Descent(13/49): loss=10047992837.657606\n",
      "Gradient Descent(14/49): loss=118839715485.8543\n",
      "Gradient Descent(15/49): loss=1405542214683.7175\n",
      "Gradient Descent(16/49): loss=16623642289015.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=196611300694376.0\n",
      "Gradient Descent(18/49): loss=2325363051601650.5\n",
      "Gradient Descent(19/49): loss=2.750255607221978e+16\n",
      "Gradient Descent(20/49): loss=3.2527849360344877e+17\n",
      "Gradient Descent(21/49): loss=3.847136903315538e+18\n",
      "Gradient Descent(22/49): loss=4.5500894291817595e+19\n",
      "Gradient Descent(23/49): loss=5.381486111323195e+20\n",
      "Gradient Descent(24/49): loss=6.364796388534473e+21\n",
      "Gradient Descent(25/49): loss=7.527778057860821e+22\n",
      "Gradient Descent(26/49): loss=8.903260847509885e+23\n",
      "Gradient Descent(27/49): loss=1.0530073164953088e+25\n",
      "Gradient Descent(28/49): loss=1.2454138181324205e+26\n",
      "Gradient Descent(29/49): loss=1.4729770193406937e+27\n",
      "Gradient Descent(30/49): loss=1.7421207858118804e+28\n",
      "Gradient Descent(31/49): loss=2.0604427581065023e+29\n",
      "Gradient Descent(32/49): loss=2.4369288249177628e+30\n",
      "Gradient Descent(33/49): loss=2.882206785095307e+31\n",
      "Gradient Descent(34/49): loss=3.4088463590354154e+32\n",
      "Gradient Descent(35/49): loss=4.031714018439129e+33\n",
      "Gradient Descent(36/49): loss=4.768392650902159e+34\n",
      "Gradient Descent(37/49): loss=5.6396779060189444e+35\n",
      "Gradient Descent(38/49): loss=6.670165234320473e+36\n",
      "Gradient Descent(39/49): loss=7.888944190528076e+37\n",
      "Gradient Descent(40/49): loss=9.33041960055848e+38\n",
      "Gradient Descent(41/49): loss=1.1035282772948572e+40\n",
      "Gradient Descent(42/49): loss=1.3051660170958027e+41\n",
      "Gradient Descent(43/49): loss=1.543647197113529e+42\n",
      "Gradient Descent(44/49): loss=1.825703885900004e+43\n",
      "Gradient Descent(45/49): loss=2.1592982419966383e+44\n",
      "Gradient Descent(46/49): loss=2.5538472771510113e+45\n",
      "Gradient Descent(47/49): loss=3.020488688482814e+46\n",
      "Gradient Descent(48/49): loss=3.5723952637569116e+47\n",
      "Gradient Descent(49/49): loss=4.225146735087721e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.514647439662486\n",
      "Gradient Descent(2/49): loss=0.6107565336268317\n",
      "Gradient Descent(3/49): loss=0.8891501290131084\n",
      "Gradient Descent(4/49): loss=2.5403195520077477\n",
      "Gradient Descent(5/49): loss=19.333149946120887\n",
      "Gradient Descent(6/49): loss=215.88583940187254\n",
      "Gradient Descent(7/49): loss=2570.7800208026592\n",
      "Gradient Descent(8/49): loss=30883.691917239452\n",
      "Gradient Descent(9/49): loss=371465.7786156583\n",
      "Gradient Descent(10/49): loss=4468711.317492785\n",
      "Gradient Descent(11/49): loss=53759641.42944019\n",
      "Gradient Descent(12/49): loss=646743326.8419896\n",
      "Gradient Descent(13/49): loss=7780504744.068924\n",
      "Gradient Descent(14/49): loss=93601674870.62476\n",
      "Gradient Descent(15/49): loss=1126054661330.9233\n",
      "Gradient Descent(16/49): loss=13546756552092.615\n",
      "Gradient Descent(17/49): loss=162971318735055.56\n",
      "Gradient Descent(18/49): loss=1960591129620395.8\n",
      "Gradient Descent(19/49): loss=2.3586466670339428e+16\n",
      "Gradient Descent(20/49): loss=2.8375187543522368e+17\n",
      "Gradient Descent(21/49): loss=3.4136154405129027e+18\n",
      "Gradient Descent(22/49): loss=4.106676073183805e+19\n",
      "Gradient Descent(23/49): loss=4.9404476467701226e+20\n",
      "Gradient Descent(24/49): loss=5.943498468227902e+21\n",
      "Gradient Descent(25/49): loss=7.15019702008642e+22\n",
      "Gradient Descent(26/49): loss=8.601889560391418e+23\n",
      "Gradient Descent(27/49): loss=1.0348316808797085e+25\n",
      "Gradient Descent(28/49): loss=1.2449318260063559e+26\n",
      "Gradient Descent(29/49): loss=1.4976882521474556e+27\n",
      "Gradient Descent(30/49): loss=1.8017613926829064e+28\n",
      "Gradient Descent(31/49): loss=2.1675699942948226e+29\n",
      "Gradient Descent(32/49): loss=2.607648104375889e+30\n",
      "Gradient Descent(33/49): loss=3.137074536994247e+31\n",
      "Gradient Descent(34/49): loss=3.77398953261498e+32\n",
      "Gradient Descent(35/49): loss=4.540216314379946e+33\n",
      "Gradient Descent(36/49): loss=5.462008837920233e+34\n",
      "Gradient Descent(37/49): loss=6.5709513555617525e+35\n",
      "Gradient Descent(38/49): loss=7.90504061754683e+36\n",
      "Gradient Descent(39/49): loss=9.509987790759564e+37\n",
      "Gradient Descent(40/49): loss=1.144078470383114e+39\n",
      "Gradient Descent(41/49): loss=1.3763588084371256e+40\n",
      "Gradient Descent(42/49): loss=1.6557986349731844e+41\n",
      "Gradient Descent(43/49): loss=1.991972662050397e+42\n",
      "Gradient Descent(44/49): loss=2.3963995395010217e+43\n",
      "Gradient Descent(45/49): loss=2.8829365293645864e+44\n",
      "Gradient Descent(46/49): loss=3.4682543104123475e+45\n",
      "Gradient Descent(47/49): loss=4.1724081814403295e+46\n",
      "Gradient Descent(48/49): loss=5.019525235010852e+47\n",
      "Gradient Descent(49/49): loss=6.038631046930148e+48\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5219244402141976\n",
      "Gradient Descent(2/49): loss=0.6420573575577868\n",
      "Gradient Descent(3/49): loss=1.0838937980108843\n",
      "Gradient Descent(4/49): loss=4.516402740811685\n",
      "Gradient Descent(5/49): loss=43.892393826827764\n",
      "Gradient Descent(6/49): loss=534.5953338752508\n",
      "Gradient Descent(7/49): loss=6729.471254654838\n",
      "Gradient Descent(8/49): loss=85085.95524084572\n",
      "Gradient Descent(9/49): loss=1076460.94180386\n",
      "Gradient Descent(10/49): loss=13619954.834825585\n",
      "Gradient Descent(11/49): loss=172328985.41061875\n",
      "Gradient Descent(12/49): loss=2180428012.446779\n",
      "Gradient Descent(13/49): loss=27588321865.775463\n",
      "Gradient Descent(14/49): loss=349067030337.19135\n",
      "Gradient Descent(15/49): loss=4416643857745.028\n",
      "Gradient Descent(16/49): loss=55882513345117.9\n",
      "Gradient Descent(17/49): loss=707065228417700.8\n",
      "Gradient Descent(18/49): loss=8946291197712608.0\n",
      "Gradient Descent(19/49): loss=1.1319482698020562e+17\n",
      "Gradient Descent(20/49): loss=1.4322213051100682e+18\n",
      "Gradient Descent(21/49): loss=1.8121480650083248e+19\n",
      "Gradient Descent(22/49): loss=2.2928583716753388e+20\n",
      "Gradient Descent(23/49): loss=2.9010871760843917e+21\n",
      "Gradient Descent(24/49): loss=3.670661436053597e+22\n",
      "Gradient Descent(25/49): loss=4.6443814199050716e+23\n",
      "Gradient Descent(26/49): loss=5.876401065402892e+24\n",
      "Gradient Descent(27/49): loss=7.435239778858329e+25\n",
      "Gradient Descent(28/49): loss=9.407593177156324e+26\n",
      "Gradient Descent(29/49): loss=1.190315470908284e+28\n",
      "Gradient Descent(30/49): loss=1.5060716312903302e+29\n",
      "Gradient Descent(31/49): loss=1.905588740140297e+30\n",
      "Gradient Descent(32/49): loss=2.4110861469704786e+31\n",
      "Gradient Descent(33/49): loss=3.0506773500797475e+32\n",
      "Gradient Descent(34/49): loss=3.859933543222172e+33\n",
      "Gradient Descent(35/49): loss=4.883861925844804e+34\n",
      "Gradient Descent(36/49): loss=6.179408801635767e+35\n",
      "Gradient Descent(37/49): loss=7.81862667649608e+36\n",
      "Gradient Descent(38/49): loss=9.892681495717907e+37\n",
      "Gradient Descent(39/49): loss=1.2516922884925045e+39\n",
      "Gradient Descent(40/49): loss=1.583729937883633e+40\n",
      "Gradient Descent(41/49): loss=2.003847542409667e+41\n",
      "Gradient Descent(42/49): loss=2.5354101587466396e+42\n",
      "Gradient Descent(43/49): loss=3.207980915227433e+43\n",
      "Gradient Descent(44/49): loss=4.058965180431002e+44\n",
      "Gradient Descent(45/49): loss=5.135690881996091e+45\n",
      "Gradient Descent(46/49): loss=6.498040673661695e+46\n",
      "Gradient Descent(47/49): loss=8.22178233985751e+47\n",
      "Gradient Descent(48/49): loss=1.04027826600077e+49\n",
      "Gradient Descent(49/49): loss=1.316233908878087e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5234897213836056\n",
      "Gradient Descent(2/49): loss=0.6483365946388748\n",
      "Gradient Descent(3/49): loss=1.0747283042920301\n",
      "Gradient Descent(4/49): loss=4.068109346839446\n",
      "Gradient Descent(5/49): loss=36.48967683268464\n",
      "Gradient Descent(6/49): loss=425.700600230683\n",
      "Gradient Descent(7/49): loss=5179.06937594835\n",
      "Gradient Descent(8/49): loss=63386.03976912988\n",
      "Gradient Descent(9/49): loss=776444.9861304587\n",
      "Gradient Descent(10/49): loss=9512248.70960439\n",
      "Gradient Descent(11/49): loss=116537047.23308969\n",
      "Gradient Descent(12/49): loss=1427729974.068227\n",
      "Gradient Descent(13/49): loss=17491551573.25314\n",
      "Gradient Descent(14/49): loss=214294301006.0309\n",
      "Gradient Descent(15/49): loss=2625384473010.7603\n",
      "Gradient Descent(16/49): loss=32164381457713.715\n",
      "Gradient Descent(17/49): loss=394055592785015.75\n",
      "Gradient Descent(18/49): loss=4827694585542259.0\n",
      "Gradient Descent(19/49): loss=5.9145550623149096e+16\n",
      "Gradient Descent(20/49): loss=7.246100797251329e+17\n",
      "Gradient Descent(21/49): loss=8.877417863345245e+18\n",
      "Gradient Descent(22/49): loss=1.0875993879403024e+20\n",
      "Gradient Descent(23/49): loss=1.332450997414713e+21\n",
      "Gradient Descent(24/49): loss=1.6324261306120654e+22\n",
      "Gradient Descent(25/49): loss=1.999934764637153e+23\n",
      "Gradient Descent(26/49): loss=2.4501807388396535e+24\n",
      "Gradient Descent(27/49): loss=3.0017907379443238e+25\n",
      "Gradient Descent(28/49): loss=3.6775848783611393e+26\n",
      "Gradient Descent(29/49): loss=4.505520776845591e+27\n",
      "Gradient Descent(30/49): loss=5.519850157648473e+28\n",
      "Gradient Descent(31/49): loss=6.762535847015433e+29\n",
      "Gradient Descent(32/49): loss=8.284987776126812e+30\n",
      "Gradient Descent(33/49): loss=1.0150189810951541e+32\n",
      "Gradient Descent(34/49): loss=1.2435305396009707e+33\n",
      "Gradient Descent(35/49): loss=1.5234869807575624e+34\n",
      "Gradient Descent(36/49): loss=1.866470107989898e+35\n",
      "Gradient Descent(37/49): loss=2.2866691399538562e+36\n",
      "Gradient Descent(38/49): loss=2.8014677187884974e+37\n",
      "Gradient Descent(39/49): loss=3.432163071729872e+38\n",
      "Gradient Descent(40/49): loss=4.2048470778169795e+39\n",
      "Gradient Descent(41/49): loss=5.151485689435615e+40\n",
      "Gradient Descent(42/49): loss=6.311241364391736e+41\n",
      "Gradient Descent(43/49): loss=7.732093217553744e+42\n",
      "Gradient Descent(44/49): loss=9.472821917769e+43\n",
      "Gradient Descent(45/49): loss=1.1605441445279695e+45\n",
      "Gradient Descent(46/49): loss=1.421817831148865e+46\n",
      "Gradient Descent(47/49): loss=1.7419121491454248e+47\n",
      "Gradient Descent(48/49): loss=2.1340694066894697e+48\n",
      "Gradient Descent(49/49): loss=2.614513157165961e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5247582531547896\n",
      "Gradient Descent(2/49): loss=0.6494952571755395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3/49): loss=1.0580083768662008\n",
      "Gradient Descent(4/49): loss=3.817213278726125\n",
      "Gradient Descent(5/49): loss=33.37267684727468\n",
      "Gradient Descent(6/49): loss=387.60836134911324\n",
      "Gradient Descent(7/49): loss=4713.945186574323\n",
      "Gradient Descent(8/49): loss=57705.90159669248\n",
      "Gradient Descent(9/49): loss=707074.8978603624\n",
      "Gradient Descent(10/49): loss=8665044.593380589\n",
      "Gradient Descent(11/49): loss=106190375.30327243\n",
      "Gradient Descent(12/49): loss=1301370197.8732185\n",
      "Gradient Descent(13/49): loss=15948386846.856258\n",
      "Gradient Descent(14/49): loss=195448659758.86642\n",
      "Gradient Descent(15/49): loss=2395237799150.0654\n",
      "Gradient Descent(16/49): loss=29353816653822.8\n",
      "Gradient Descent(17/49): loss=359733197540617.9\n",
      "Gradient Descent(18/49): loss=4408556984046451.0\n",
      "Gradient Descent(19/49): loss=5.40271924152894e+16\n",
      "Gradient Descent(20/49): loss=6.621072452605661e+17\n",
      "Gradient Descent(21/49): loss=8.114173338063982e+18\n",
      "Gradient Descent(22/49): loss=9.943979533744466e+19\n",
      "Gradient Descent(23/49): loss=1.2186420581338304e+21\n",
      "Gradient Descent(24/49): loss=1.493454869665651e+22\n",
      "Gradient Descent(25/49): loss=1.8302400059486187e+23\n",
      "Gradient Descent(26/49): loss=2.2429726852909355e+24\n",
      "Gradient Descent(27/49): loss=2.7487796412546255e+25\n",
      "Gradient Descent(28/49): loss=3.3686498126907144e+26\n",
      "Gradient Descent(29/49): loss=4.128305299642646e+27\n",
      "Gradient Descent(30/49): loss=5.0592687262571795e+28\n",
      "Gradient Descent(31/49): loss=6.2001713019379526e+29\n",
      "Gradient Descent(32/49): loss=7.598355859981292e+30\n",
      "Gradient Descent(33/49): loss=9.311841393295923e+31\n",
      "Gradient Descent(34/49): loss=1.1411730607483321e+33\n",
      "Gradient Descent(35/49): loss=1.3985160394970775e+34\n",
      "Gradient Descent(36/49): loss=1.7138917662918627e+35\n",
      "Gradient Descent(37/49): loss=2.1003870557103245e+36\n",
      "Gradient Descent(38/49): loss=2.574039895961679e+37\n",
      "Gradient Descent(39/49): loss=3.1545049604020135e+38\n",
      "Gradient Descent(40/49): loss=3.8658691968266186e+39\n",
      "Gradient Descent(41/49): loss=4.737651338189088e+40\n",
      "Gradient Descent(42/49): loss=5.80602681039219e+41\n",
      "Gradient Descent(43/49): loss=7.115328865859285e+42\n",
      "Gradient Descent(44/49): loss=8.719888233845303e+43\n",
      "Gradient Descent(45/49): loss=1.0686287625522568e+45\n",
      "Gradient Descent(46/49): loss=1.3096124646661718e+46\n",
      "Gradient Descent(47/49): loss=1.6049397767591088e+47\n",
      "Gradient Descent(48/49): loss=1.966865585446444e+48\n",
      "Gradient Descent(49/49): loss=2.4104083450567107e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.521011825509013\n",
      "Gradient Descent(2/49): loss=0.6340707018530026\n",
      "Gradient Descent(3/49): loss=0.9643075133083248\n",
      "Gradient Descent(4/49): loss=2.9134285925392884\n",
      "Gradient Descent(5/49): loss=23.110373928720524\n",
      "Gradient Descent(6/49): loss=267.2126942463798\n",
      "Gradient Descent(7/49): loss=3295.6631564161235\n",
      "Gradient Descent(8/49): loss=41017.92242634106\n",
      "Gradient Descent(9/49): loss=511164.1028064033\n",
      "Gradient Descent(10/49): loss=6371284.04715092\n",
      "Gradient Descent(11/49): loss=79415495.16112551\n",
      "Gradient Descent(12/49): loss=989886141.5375682\n",
      "Gradient Descent(13/49): loss=12338588876.311255\n",
      "Gradient Descent(14/49): loss=153796262254.69528\n",
      "Gradient Descent(15/49): loss=1917017482644.9504\n",
      "Gradient Descent(16/49): loss=23894963257837.883\n",
      "Gradient Descent(17/49): loss=297842494616829.1\n",
      "Gradient Descent(18/49): loss=3712504206266491.5\n",
      "Gradient Descent(19/49): loss=4.627508743963259e+16\n",
      "Gradient Descent(20/49): loss=5.768030414438674e+17\n",
      "Gradient Descent(21/49): loss=7.189651430761118e+18\n",
      "Gradient Descent(22/49): loss=8.961653108910686e+19\n",
      "Gradient Descent(23/49): loss=1.1170392225252223e+21\n",
      "Gradient Descent(24/49): loss=1.3923509529944772e+22\n",
      "Gradient Descent(25/49): loss=1.735517551408846e+23\n",
      "Gradient Descent(26/49): loss=2.1632629077966298e+24\n",
      "Gradient Descent(27/49): loss=2.6964327756004625e+25\n",
      "Gradient Descent(28/49): loss=3.361010669173771e+26\n",
      "Gradient Descent(29/49): loss=4.189384145052248e+27\n",
      "Gradient Descent(30/49): loss=5.221923177985715e+28\n",
      "Gradient Descent(31/49): loss=6.50894755234804e+29\n",
      "Gradient Descent(32/49): loss=8.113179147832683e+30\n",
      "Gradient Descent(33/49): loss=1.0112798629185539e+32\n",
      "Gradient Descent(34/49): loss=1.2605255504776582e+33\n",
      "Gradient Descent(35/49): loss=1.5712017233502509e+34\n",
      "Gradient Descent(36/49): loss=1.9584488823120797e+35\n",
      "Gradient Descent(37/49): loss=2.4411391405879106e+36\n",
      "Gradient Descent(38/49): loss=3.042795937913391e+37\n",
      "Gradient Descent(39/49): loss=3.79274043246573e+38\n",
      "Gradient Descent(40/49): loss=4.727520438956714e+39\n",
      "Gradient Descent(41/49): loss=5.892691550796001e+40\n",
      "Gradient Descent(42/49): loss=7.345037247577988e+41\n",
      "Gradient Descent(43/49): loss=9.15533618266838e+42\n",
      "Gradient Descent(44/49): loss=1.141181151195921e+44\n",
      "Gradient Descent(45/49): loss=1.4224430363465582e+45\n",
      "Gradient Descent(46/49): loss=1.7730262978234373e+46\n",
      "Gradient Descent(47/49): loss=2.2100162695074934e+47\n",
      "Gradient Descent(48/49): loss=2.7547092321662436e+48\n",
      "Gradient Descent(49/49): loss=3.433650266960671e+49\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5285300214776428\n",
      "Gradient Descent(2/49): loss=0.6677418867890262\n",
      "Gradient Descent(3/49): loss=1.181196321723945\n",
      "Gradient Descent(4/49): loss=5.184212078243992\n",
      "Gradient Descent(5/49): loss=52.27914687716972\n",
      "Gradient Descent(6/49): loss=658.8886021267275\n",
      "Gradient Descent(7/49): loss=8585.880809678725\n",
      "Gradient Descent(8/49): loss=112396.49324857313\n",
      "Gradient Descent(9/49): loss=1472314.8097238645\n",
      "Gradient Descent(10/49): loss=19288067.162153948\n",
      "Gradient Descent(11/49): loss=252686799.95012054\n",
      "Gradient Descent(12/49): loss=3310375435.5287776\n",
      "Gradient Descent(13/49): loss=43368267223.96142\n",
      "Gradient Descent(14/49): loss=568155098342.0707\n",
      "Gradient Descent(15/49): loss=7443235306134.626\n",
      "Gradient Descent(16/49): loss=97511668969437.61\n",
      "Gradient Descent(17/49): loss=1277472120038843.2\n",
      "Gradient Descent(18/49): loss=1.673579208268964e+16\n",
      "Gradient Descent(19/49): loss=2.192507626916532e+17\n",
      "Gradient Descent(20/49): loss=2.872340711654337e+18\n",
      "Gradient Descent(21/49): loss=3.762970337042928e+19\n",
      "Gradient Descent(22/49): loss=4.929758402271813e+20\n",
      "Gradient Descent(23/49): loss=6.458333637534706e+21\n",
      "Gradient Descent(24/49): loss=8.460875761069722e+22\n",
      "Gradient Descent(25/49): loss=1.1084348171207643e+24\n",
      "Gradient Descent(26/49): loss=1.4521283357672245e+25\n",
      "Gradient Descent(27/49): loss=1.9023912556405568e+26\n",
      "Gradient Descent(28/49): loss=2.492267660093289e+27\n",
      "Gradient Descent(29/49): loss=3.2650476452359597e+28\n",
      "Gradient Descent(30/49): loss=4.277444311604185e+29\n",
      "Gradient Descent(31/49): loss=5.603755848883697e+30\n",
      "Gradient Descent(32/49): loss=7.341318162508586e+31\n",
      "Gradient Descent(33/49): loss=9.61764820177052e+32\n",
      "Gradient Descent(34/49): loss=1.259980222699016e+34\n",
      "Gradient Descent(35/49): loss=1.6506635804171148e+35\n",
      "Gradient Descent(36/49): loss=2.1624865268749374e+36\n",
      "Gradient Descent(37/49): loss=2.83301093838511e+37\n",
      "Gradient Descent(38/49): loss=3.711445540707241e+38\n",
      "Gradient Descent(39/49): loss=4.8622572595811055e+39\n",
      "Gradient Descent(40/49): loss=6.369902346416965e+40\n",
      "Gradient Descent(41/49): loss=8.345024488972434e+41\n",
      "Gradient Descent(42/49): loss=1.0932574776553513e+43\n",
      "Gradient Descent(43/49): loss=1.4322449431140431e+44\n",
      "Gradient Descent(44/49): loss=1.8763425990692886e+45\n",
      "Gradient Descent(45/49): loss=2.4581420699083045e+46\n",
      "Gradient Descent(46/49): loss=3.2203406983618653e+47\n",
      "Gradient Descent(47/49): loss=4.218875035938425e+48\n",
      "Gradient Descent(48/49): loss=5.527025937944539e+49\n",
      "Gradient Descent(49/49): loss=7.240796529522197e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5301686643747375\n",
      "Gradient Descent(2/49): loss=0.6746813505736542\n",
      "Gradient Descent(3/49): loss=1.171973355178053\n",
      "Gradient Descent(4/49): loss=4.673450850098707\n",
      "Gradient Descent(5/49): loss=43.50363533854821\n",
      "Gradient Descent(6/49): loss=525.1210132842363\n",
      "Gradient Descent(7/49): loss=6613.6430848005775\n",
      "Gradient Descent(8/49): loss=83814.30861848735\n",
      "Gradient Descent(9/49): loss=1063149.4256929327\n",
      "Gradient Descent(10/49): loss=13487470.408269389\n",
      "Gradient Descent(11/49): loss=171110180.28491932\n",
      "Gradient Descent(12/49): loss=2170813906.503795\n",
      "Gradient Descent(13/49): loss=27540356443.35096\n",
      "Gradient Descent(14/49): loss=349394892279.35596\n",
      "Gradient Descent(15/49): loss=4432651132925.602\n",
      "Gradient Descent(16/49): loss=56235498961535.88\n",
      "Gradient Descent(17/49): loss=713440162445706.0\n",
      "Gradient Descent(18/49): loss=9051166519580852.0\n",
      "Gradient Descent(19/49): loss=1.1482899292477646e+17\n",
      "Gradient Descent(20/49): loss=1.4567953851701522e+18\n",
      "Gradient Descent(21/49): loss=1.8481854975804064e+19\n",
      "Gradient Descent(22/49): loss=2.3447284829692097e+20\n",
      "Gradient Descent(23/49): loss=2.9746752509662436e+21\n",
      "Gradient Descent(24/49): loss=3.7738667453323797e+22\n",
      "Gradient Descent(25/49): loss=4.7877731214187406e+23\n",
      "Gradient Descent(26/49): loss=6.074080779489871e+24\n",
      "Gradient Descent(27/49): loss=7.705974443675257e+25\n",
      "Gradient Descent(28/49): loss=9.776301021068018e+26\n",
      "Gradient Descent(29/49): loss=1.2402852144543463e+28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=1.5735065950598967e+29\n",
      "Gradient Descent(31/49): loss=1.996252939116286e+30\n",
      "Gradient Descent(32/49): loss=2.5325764820061854e+31\n",
      "Gradient Descent(33/49): loss=3.2129914559075008e+32\n",
      "Gradient Descent(34/49): loss=4.076210202961757e+33\n",
      "Gradient Descent(35/49): loss=5.171345721501851e+34\n",
      "Gradient Descent(36/49): loss=6.560705959635e+35\n",
      "Gradient Descent(37/49): loss=8.323338838055755e+36\n",
      "Gradient Descent(38/49): loss=1.0559529696853116e+38\n",
      "Gradient Descent(39/49): loss=1.3396507049420031e+39\n",
      "Gradient Descent(40/49): loss=1.6995681273441547e+40\n",
      "Gradient Descent(41/49): loss=2.1561828085696953e+41\n",
      "Gradient Descent(42/49): loss=2.7354739296249622e+42\n",
      "Gradient Descent(43/49): loss=3.4704003713958785e+43\n",
      "Gradient Descent(44/49): loss=4.402775916579723e+44\n",
      "Gradient Descent(45/49): loss=5.5856482529758986e+45\n",
      "Gradient Descent(46/49): loss=7.086317131990301e+46\n",
      "Gradient Descent(47/49): loss=8.990163401067168e+47\n",
      "Gradient Descent(48/49): loss=1.1405506763594022e+49\n",
      "Gradient Descent(49/49): loss=1.446976864947159e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5315899261326134\n",
      "Gradient Descent(2/49): loss=0.6761540601499013\n",
      "Gradient Descent(3/49): loss=1.1538276816569608\n",
      "Gradient Descent(4/49): loss=4.385608290650532\n",
      "Gradient Descent(5/49): loss=39.792667575092075\n",
      "Gradient Descent(6/49): loss=478.1475377647145\n",
      "Gradient Descent(7/49): loss=6019.678896157131\n",
      "Gradient Descent(8/49): loss=76302.93945927678\n",
      "Gradient Descent(9/49): loss=968154.1558282537\n",
      "Gradient Descent(10/49): loss=12286070.038311752\n",
      "Gradient Descent(11/49): loss=155916236.62888548\n",
      "Gradient Descent(12/49): loss=1978660151.2052555\n",
      "Gradient Descent(13/49): loss=25110265336.105465\n",
      "Gradient Descent(14/49): loss=318662846745.0113\n",
      "Gradient Descent(15/49): loss=4044003908469.452\n",
      "Gradient Descent(16/49): loss=51320597333671.76\n",
      "Gradient Descent(17/49): loss=651286143898629.9\n",
      "Gradient Descent(18/49): loss=8265173503254141.0\n",
      "Gradient Descent(19/49): loss=1.048895231064541e+17\n",
      "Gradient Descent(20/49): loss=1.3311047920747965e+18\n",
      "Gradient Descent(21/49): loss=1.68924399216347e+19\n",
      "Gradient Descent(22/49): loss=2.1437420119362906e+20\n",
      "Gradient Descent(23/49): loss=2.720524586773889e+21\n",
      "Gradient Descent(24/49): loss=3.4524928774224344e+22\n",
      "Gradient Descent(25/49): loss=4.381400236778402e+23\n",
      "Gradient Descent(26/49): loss=5.560233928468867e+24\n",
      "Gradient Descent(27/49): loss=7.056237656578397e+25\n",
      "Gradient Descent(28/49): loss=8.95474731938575e+26\n",
      "Gradient Descent(29/49): loss=1.1364058788367252e+28\n",
      "Gradient Descent(30/49): loss=1.4421605383090051e+29\n",
      "Gradient Descent(31/49): loss=1.8301797421047862e+30\n",
      "Gradient Descent(32/49): loss=2.322597103050826e+31\n",
      "Gradient Descent(33/49): loss=2.947501373223724e+32\n",
      "Gradient Descent(34/49): loss=3.740538698573135e+33\n",
      "Gradient Descent(35/49): loss=4.746945966719171e+34\n",
      "Gradient Descent(36/49): loss=6.02413123530753e+35\n",
      "Gradient Descent(37/49): loss=7.644948435191659e+36\n",
      "Gradient Descent(38/49): loss=9.70185314592594e+37\n",
      "Gradient Descent(39/49): loss=1.2312176499690275e+39\n",
      "Gradient Descent(40/49): loss=1.5624818050681683e+40\n",
      "Gradient Descent(41/49): loss=1.9828739388445653e+41\n",
      "Gradient Descent(42/49): loss=2.5163742992690753e+42\n",
      "Gradient Descent(43/49): loss=3.193415118316413e+43\n",
      "Gradient Descent(44/49): loss=4.052616544706464e+44\n",
      "Gradient Descent(45/49): loss=5.142989636463942e+45\n",
      "Gradient Descent(46/49): loss=6.52673207765606e+46\n",
      "Gradient Descent(47/49): loss=8.28277609417714e+47\n",
      "Gradient Descent(48/49): loss=1.0511290950816578e+49\n",
      "Gradient Descent(49/49): loss=1.3339396863618387e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5275450044838835\n",
      "Gradient Descent(2/49): loss=0.6591167250466012\n",
      "Gradient Descent(3/49): loss=1.0485065085463035\n",
      "Gradient Descent(4/49): loss=3.342849171166392\n",
      "Gradient Descent(5/49): loss=27.557485982192855\n",
      "Gradient Descent(6/49): loss=329.5484763233773\n",
      "Gradient Descent(7/49): loss=4206.757623296639\n",
      "Gradient Descent(8/49): loss=54209.010499706776\n",
      "Gradient Descent(9/49): loss=699497.3001964072\n",
      "Gradient Descent(10/49): loss=9027910.011100564\n",
      "Gradient Descent(11/49): loss=116520208.46975087\n",
      "Gradient Descent(12/49): loss=1503893921.9076388\n",
      "Gradient Descent(13/49): loss=19410353416.674675\n",
      "Gradient Descent(14/49): loss=250524223645.97464\n",
      "Gradient Descent(15/49): loss=3233448985918.6313\n",
      "Gradient Descent(16/49): loss=41733259226887.81\n",
      "Gradient Descent(17/49): loss=538639989024706.5\n",
      "Gradient Descent(18/49): loss=6952081940543925.0\n",
      "Gradient Descent(19/49): loss=8.972865790361763e+16\n",
      "Gradient Descent(20/49): loss=1.158103733248555e+18\n",
      "Gradient Descent(21/49): loss=1.4947334422466361e+19\n",
      "Gradient Descent(22/49): loss=1.9292123833357818e+20\n",
      "Gradient Descent(23/49): loss=2.4899827051585853e+21\n",
      "Gradient Descent(24/49): loss=3.213753926495393e+22\n",
      "Gradient Descent(25/49): loss=4.147906039133048e+23\n",
      "Gradient Descent(26/49): loss=5.353591128316045e+24\n",
      "Gradient Descent(27/49): loss=6.909736551114132e+25\n",
      "Gradient Descent(28/49): loss=8.918211731425222e+26\n",
      "Gradient Descent(29/49): loss=1.1510496803775676e+28\n",
      "Gradient Descent(30/49): loss=1.485628965309991e+29\n",
      "Gradient Descent(31/49): loss=1.917461479025057e+30\n",
      "Gradient Descent(32/49): loss=2.474816127981081e+31\n",
      "Gradient Descent(33/49): loss=3.1941788319154824e+32\n",
      "Gradient Descent(34/49): loss=4.122640989324728e+33\n",
      "Gradient Descent(35/49): loss=5.320982205829733e+34\n",
      "Gradient Descent(36/49): loss=6.867649089035691e+35\n",
      "Gradient Descent(37/49): loss=8.863890572394622e+36\n",
      "Gradient Descent(38/49): loss=1.1440385940047692e+38\n",
      "Gradient Descent(39/49): loss=1.4765799440807048e+39\n",
      "Gradient Descent(40/49): loss=1.9057821499091206e+40\n",
      "Gradient Descent(41/49): loss=2.4597419309886358e+41\n",
      "Gradient Descent(42/49): loss=3.174722969963888e+42\n",
      "Gradient Descent(43/49): loss=4.097529829873416e+43\n",
      "Gradient Descent(44/49): loss=5.288571905501962e+44\n",
      "Gradient Descent(45/49): loss=6.82581798325267e+45\n",
      "Gradient Descent(46/49): loss=8.809900285561788e+46\n",
      "Gradient Descent(47/49): loss=1.1370702124195356e+48\n",
      "Gradient Descent(48/49): loss=1.4675860407759235e+49\n",
      "Gradient Descent(49/49): loss=1.894173960020746e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5353060148077363\n",
      "Gradient Descent(2/49): loss=0.695277952389674\n",
      "Gradient Descent(3/49): loss=1.2895662825302694\n",
      "Gradient Descent(4/49): loss=5.9460708001614515\n",
      "Gradient Descent(5/49): loss=62.10024138721005\n",
      "Gradient Descent(6/49): loss=809.2170255490722\n",
      "Gradient Descent(7/49): loss=10908.935412573743\n",
      "Gradient Descent(8/49): loss=147768.1257144772\n",
      "Gradient Descent(9/49): loss=2002983.9342932736\n",
      "Gradient Descent(10/49): loss=27152990.472624067\n",
      "Gradient Descent(11/49): loss=368098693.27843714\n",
      "Gradient Descent(12/49): loss=4990129691.235693\n",
      "Gradient Descent(13/49): loss=67648711784.785286\n",
      "Gradient Descent(14/49): loss=917080058010.7556\n",
      "Gradient Descent(15/49): loss=12432399917772.756\n",
      "Gradient Descent(16/49): loss=168539885400600.75\n",
      "Gradient Descent(17/49): loss=2284811714812093.5\n",
      "Gradient Descent(18/49): loss=3.0974060293548656e+16\n",
      "Gradient Descent(19/49): loss=4.198999877548591e+17\n",
      "Gradient Descent(20/49): loss=5.692376073580145e+18\n",
      "Gradient Descent(21/49): loss=7.716872185761513e+19\n",
      "Gradient Descent(22/49): loss=1.0461381251278659e+21\n",
      "Gradient Descent(23/49): loss=1.4181976200996939e+22\n",
      "Gradient Descent(24/49): loss=1.9225802418878654e+23\n",
      "Gradient Descent(25/49): loss=2.6063467700910625e+24\n",
      "Gradient Descent(26/49): loss=3.5332951717497825e+25\n",
      "Gradient Descent(27/49): loss=4.789913189592298e+26\n",
      "Gradient Descent(28/49): loss=6.493447970968348e+27\n",
      "Gradient Descent(29/49): loss=8.802845663944413e+28\n",
      "Gradient Descent(30/49): loss=1.1933581685674144e+30\n",
      "Gradient Descent(31/49): loss=1.6177765382386403e+31\n",
      "Gradient Descent(32/49): loss=2.193139492075012e+32\n",
      "Gradient Descent(33/49): loss=2.97313053936094e+33\n",
      "Gradient Descent(34/49): loss=4.0305257536161004e+34\n",
      "Gradient Descent(35/49): loss=5.463984051657073e+35\n",
      "Gradient Descent(36/49): loss=7.407252438463406e+36\n",
      "Gradient Descent(37/49): loss=1.0041645101522803e+38\n",
      "Gradient Descent(38/49): loss=1.3612960700696118e+39\n",
      "Gradient Descent(39/49): loss=1.8454416299834384e+40\n",
      "Gradient Descent(40/49): loss=2.5017737761498906e+41\n",
      "Gradient Descent(41/49): loss=3.391530745454884e+42\n",
      "Gradient Descent(42/49): loss=4.597730181290519e+43\n",
      "Gradient Descent(43/49): loss=6.232914989280113e+44\n",
      "Gradient Descent(44/49): loss=8.449654010076657e+45\n",
      "Gradient Descent(45/49): loss=1.1454777261168986e+47\n",
      "Gradient Descent(46/49): loss=1.552867394884096e+48\n",
      "Gradient Descent(47/49): loss=2.1051453826768235e+49\n",
      "Gradient Descent(48/49): loss=2.8538412853573245e+50\n",
      "Gradient Descent(49/49): loss=3.868811222744981e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5370194226627849\n",
      "Gradient Descent(2/49): loss=0.7029213077900108\n",
      "Gradient Descent(3/49): loss=1.2803722576141103\n",
      "Gradient Descent(4/49): loss=5.365472771696791\n",
      "Gradient Descent(5/49): loss=51.72747425411025\n",
      "Gradient Descent(6/49): loss=645.4673364807365\n",
      "Gradient Descent(7/49): loss=8410.300343323013\n",
      "Gradient Descent(8/49): loss=110296.43689213198\n",
      "Gradient Descent(9/49): loss=1447891.4792489603\n",
      "Gradient Descent(10/49): loss=19009714.316923108\n",
      "Gradient Descent(11/49): loss=249588876.27136278\n",
      "Gradient Descent(12/49): loss=3276999866.3338456\n",
      "Gradient Descent(13/49): loss=43025692092.08764\n",
      "Gradient Descent(14/49): loss=564910105948.8207\n",
      "Gradient Descent(15/49): loss=7417043553133.677\n",
      "Gradient Descent(16/49): loss=97382812958571.27\n",
      "Gradient Descent(17/49): loss=1278597354179376.0\n",
      "Gradient Descent(18/49): loss=1.6787471469885252e+16\n",
      "Gradient Descent(19/49): loss=2.2041278079708854e+17\n",
      "Gradient Descent(20/49): loss=2.8939315861783055e+18\n",
      "Gradient Descent(21/49): loss=3.7996163358567105e+19\n",
      "Gradient Descent(22/49): loss=4.988744159904273e+20\n",
      "Gradient Descent(23/49): loss=6.550021395085904e+21\n",
      "Gradient Descent(24/49): loss=8.599915910882263e+22\n",
      "Gradient Descent(25/49): loss=1.1291345358006495e+24\n",
      "Gradient Descent(26/49): loss=1.4825084490935845e+25\n",
      "Gradient Descent(27/49): loss=1.9464742525791992e+26\n",
      "Gradient Descent(28/49): loss=2.555642781172775e+27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=3.355456675733686e+28\n",
      "Gradient Descent(30/49): loss=4.4055803047556824e+29\n",
      "Gradient Descent(31/49): loss=5.784350595856642e+30\n",
      "Gradient Descent(32/49): loss=7.5946207993688385e+31\n",
      "Gradient Descent(33/49): loss=9.971433116021537e+32\n",
      "Gradient Descent(34/49): loss=1.3092092549973799e+34\n",
      "Gradient Descent(35/49): loss=1.7189393474612646e+35\n",
      "Gradient Descent(36/49): loss=2.2568985584023146e+36\n",
      "Gradient Descent(37/49): loss=2.9632174692151114e+37\n",
      "Gradient Descent(38/49): loss=3.890585926944616e+38\n",
      "Gradient Descent(39/49): loss=5.1081835917189154e+39\n",
      "Gradient Descent(40/49): loss=6.706840588198576e+40\n",
      "Gradient Descent(41/49): loss=8.805813234361735e+41\n",
      "Gradient Descent(42/49): loss=1.1561680302183687e+43\n",
      "Gradient Descent(43/49): loss=1.5180023451813688e+44\n",
      "Gradient Descent(44/49): loss=1.993076317411119e+45\n",
      "Gradient Descent(45/49): loss=2.61682942693375e+46\n",
      "Gradient Descent(46/49): loss=3.4357922924705216e+47\n",
      "Gradient Descent(47/49): loss=4.5110577538986884e+48\n",
      "Gradient Descent(48/49): loss=5.922838264584516e+49\n",
      "Gradient Descent(49/49): loss=7.776449565095738e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5385970725921944\n",
      "Gradient Descent(2/49): loss=0.7047344694252969\n",
      "Gradient Descent(3/49): loss=1.260709551318184\n",
      "Gradient Descent(4/49): loss=5.036022534976359\n",
      "Gradient Descent(5/49): loss=47.3225843496353\n",
      "Gradient Descent(6/49): loss=587.7500118932921\n",
      "Gradient Descent(7/49): loss=7654.984190048435\n",
      "Gradient Descent(8/49): loss=100410.99445228405\n",
      "Gradient Descent(9/49): loss=1318505.2381651232\n",
      "Gradient Descent(10/49): loss=17316221.21690116\n",
      "Gradient Descent(11/49): loss=227423459.73096043\n",
      "Gradient Descent(12/49): loss=2986888967.712765\n",
      "Gradient Descent(13/49): loss=39228631304.05273\n",
      "Gradient Descent(14/49): loss=515213546149.9006\n",
      "Gradient Descent(15/49): loss=6766613902668.627\n",
      "Gradient Descent(16/49): loss=88870069803969.56\n",
      "Gradient Descent(17/49): loss=1167184861336975.2\n",
      "Gradient Descent(18/49): loss=1.5329351080884168e+16\n",
      "Gradient Descent(19/49): loss=2.0132972277741923e+17\n",
      "Gradient Descent(20/49): loss=2.6441861145858734e+18\n",
      "Gradient Descent(21/49): loss=3.4727709908485837e+19\n",
      "Gradient Descent(22/49): loss=4.5610020748363635e+20\n",
      "Gradient Descent(23/49): loss=5.990242368840471e+21\n",
      "Gradient Descent(24/49): loss=7.867350869104573e+22\n",
      "Gradient Descent(25/49): loss=1.0332672016671956e+24\n",
      "Gradient Descent(26/49): loss=1.3570528730755708e+25\n",
      "Gradient Descent(27/49): loss=1.7823003549819542e+26\n",
      "Gradient Descent(28/49): loss=2.3408038245182045e+27\n",
      "Gradient Descent(29/49): loss=3.074320514812803e+28\n",
      "Gradient Descent(30/49): loss=4.0376927484489254e+29\n",
      "Gradient Descent(31/49): loss=5.302948294533891e+30\n",
      "Gradient Descent(32/49): loss=6.964685617870134e+31\n",
      "Gradient Descent(33/49): loss=9.147146655336139e+32\n",
      "Gradient Descent(34/49): loss=1.2013505924739062e+34\n",
      "Gradient Descent(35/49): loss=1.5778070478354217e+35\n",
      "Gradient Descent(36/49): loss=2.0722302846438025e+36\n",
      "Gradient Descent(37/49): loss=2.721586494676867e+37\n",
      "Gradient Descent(38/49): loss=3.5744256335298186e+38\n",
      "Gradient Descent(39/49): loss=4.6945113207404066e+39\n",
      "Gradient Descent(40/49): loss=6.165588209145918e+40\n",
      "Gradient Descent(41/49): loss=8.097643261996517e+41\n",
      "Gradient Descent(42/49): loss=1.0635129070294985e+43\n",
      "Gradient Descent(43/49): loss=1.3967764037304555e+44\n",
      "Gradient Descent(44/49): loss=1.8344716919961934e+45\n",
      "Gradient Descent(45/49): loss=2.409323625275573e+46\n",
      "Gradient Descent(46/49): loss=3.1643117507005684e+47\n",
      "Gradient Descent(47/49): loss=4.155883730429231e+48\n",
      "Gradient Descent(48/49): loss=5.458175724001477e+49\n",
      "Gradient Descent(49/49): loss=7.168555274043431e+50\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5342469765870982\n",
      "Gradient Descent(2/49): loss=0.6859777936848984\n",
      "Gradient Descent(3/49): loss=1.1426043761404574\n",
      "Gradient Descent(4/49): loss=3.835890136503011\n",
      "Gradient Descent(5/49): loss=32.77922695801714\n",
      "Gradient Descent(6/49): loss=405.0040386059721\n",
      "Gradient Descent(7/49): loss=5347.397978114955\n",
      "Gradient Descent(8/49): loss=71301.08763530826\n",
      "Gradient Descent(9/49): loss=952091.4229204451\n",
      "Gradient Descent(10/49): loss=12716131.829548037\n",
      "Gradient Descent(11/49): loss=169842171.61779428\n",
      "Gradient Descent(12/49): loss=2268496890.553576\n",
      "Gradient Descent(13/49): loss=30299200404.806377\n",
      "Gradient Descent(14/49): loss=404691605909.2448\n",
      "Gradient Descent(15/49): loss=5405268011691.526\n",
      "Gradient Descent(16/49): loss=72195523525883.3\n",
      "Gradient Descent(17/49): loss=964280329484299.4\n",
      "Gradient Descent(18/49): loss=1.2879421167347718e+16\n",
      "Gradient Descent(19/49): loss=1.7202413503154186e+17\n",
      "Gradient Descent(20/49): loss=2.297642312405669e+18\n",
      "Gradient Descent(21/49): loss=3.0688485629007823e+19\n",
      "Gradient Descent(22/49): loss=4.098911066865829e+20\n",
      "Gradient Descent(23/49): loss=5.474715219637433e+21\n",
      "Gradient Descent(24/49): loss=7.31230959813125e+22\n",
      "Gradient Descent(25/49): loss=9.766694615845905e+23\n",
      "Gradient Descent(26/49): loss=1.3044896751030493e+25\n",
      "Gradient Descent(27/49): loss=1.7423431154380745e+26\n",
      "Gradient Descent(28/49): loss=2.3271625600828306e+27\n",
      "Gradient Descent(29/49): loss=3.108277315222997e+28\n",
      "Gradient Descent(30/49): loss=4.151574124665443e+29\n",
      "Gradient Descent(31/49): loss=5.545054692571737e+30\n",
      "Gradient Descent(32/49): loss=7.40625859500696e+31\n",
      "Gradient Descent(33/49): loss=9.892177700175863e+32\n",
      "Gradient Descent(34/49): loss=1.3212498375067273e+34\n",
      "Gradient Descent(35/49): loss=1.764728845378996e+35\n",
      "Gradient Descent(36/49): loss=2.3570620856912016e+36\n",
      "Gradient Descent(37/49): loss=3.1482126505445244e+37\n",
      "Gradient Descent(38/49): loss=4.204913800623098e+38\n",
      "Gradient Descent(39/49): loss=5.61629789131691e+39\n",
      "Gradient Descent(40/49): loss=7.5014146542877e+40\n",
      "Gradient Descent(41/49): loss=1.0019272998777632e+42\n",
      "Gradient Descent(42/49): loss=1.3382253354925873e+43\n",
      "Gradient Descent(43/49): loss=1.787402188534795e+44\n",
      "Gradient Descent(44/49): loss=2.3873457622165465e+45\n",
      "Gradient Descent(45/49): loss=3.188661077474288e+46\n",
      "Gradient Descent(46/49): loss=4.258938787969771e+47\n",
      "Gradient Descent(47/49): loss=5.6884564269969304e+48\n",
      "Gradient Descent(48/49): loss=7.597793284384942e+49\n",
      "Gradient Descent(49/49): loss=1.0148001225478463e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5422524202044783\n",
      "Gradient Descent(2/49): loss=0.7247521547183884\n",
      "Gradient Descent(3/49): loss=1.4100031495773\n",
      "Gradient Descent(4/49): loss=6.813348831866776\n",
      "Gradient Descent(5/49): loss=73.57107995231434\n",
      "Gradient Descent(6/49): loss=990.4527765662924\n",
      "Gradient Descent(7/49): loss=13804.923938569067\n",
      "Gradient Descent(8/49): loss=193379.9190923348\n",
      "Gradient Descent(9/49): loss=2710858.412864085\n",
      "Gradient Descent(10/49): loss=38005761.15848915\n",
      "Gradient Descent(11/49): loss=532842768.16279167\n",
      "Gradient Descent(12/49): loss=7470501769.252724\n",
      "Gradient Descent(13/49): loss=104737120236.50887\n",
      "Gradient Descent(14/49): loss=1468424116228.072\n",
      "Gradient Descent(15/49): loss=20587442141635.703\n",
      "Gradient Descent(16/49): loss=288637846367838.44\n",
      "Gradient Descent(17/49): loss=4046729350752186.0\n",
      "Gradient Descent(18/49): loss=5.673552046196954e+16\n",
      "Gradient Descent(19/49): loss=7.954372539161276e+17\n",
      "Gradient Descent(20/49): loss=1.1152104004089821e+19\n",
      "Gradient Descent(21/49): loss=1.5635353147686978e+20\n",
      "Gradient Descent(22/49): loss=2.1920909988216772e+21\n",
      "Gradient Descent(23/49): loss=3.073331891979548e+22\n",
      "Gradient Descent(24/49): loss=4.3088397896510725e+23\n",
      "Gradient Descent(25/49): loss=6.041033310242834e+24\n",
      "Gradient Descent(26/49): loss=8.469584676393517e+25\n",
      "Gradient Descent(27/49): loss=1.187443619437959e+27\n",
      "Gradient Descent(28/49): loss=1.6648069571509977e+28\n",
      "Gradient Descent(29/49): loss=2.3340747798115656e+29\n",
      "Gradient Descent(30/49): loss=3.2723944685307655e+30\n",
      "Gradient Descent(31/49): loss=4.587927366463918e+31\n",
      "Gradient Descent(32/49): loss=6.432316678923805e+32\n",
      "Gradient Descent(33/49): loss=9.018167584865289e+33\n",
      "Gradient Descent(34/49): loss=1.2643554515155491e+35\n",
      "Gradient Descent(35/49): loss=1.77263805837881e+36\n",
      "Gradient Descent(36/49): loss=2.4852549828819813e+37\n",
      "Gradient Descent(37/49): loss=3.484350514051666e+38\n",
      "Gradient Descent(38/49): loss=4.8850917062415634e+39\n",
      "Gradient Descent(39/49): loss=6.848943836778441e+40\n",
      "Gradient Descent(40/49): loss=9.602282720591173e+41\n",
      "Gradient Descent(41/49): loss=1.3462489347778293e+43\n",
      "Gradient Descent(42/49): loss=1.887453480726951e+44\n",
      "Gradient Descent(43/49): loss=2.6462272688789896e+45\n",
      "Gradient Descent(44/49): loss=3.710035150567891e+46\n",
      "Gradient Descent(45/49): loss=5.201503657801864e+47\n",
      "Gradient Descent(46/49): loss=7.292556324698238e+48\n",
      "Gradient Descent(47/49): loss=1.022423153911045e+50\n",
      "Gradient Descent(48/49): loss=1.433446735423984e+51\n",
      "Gradient Descent(49/49): loss=2.009705605196445e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5440419962477482\n",
      "Gradient Descent(2/49): loss=0.733144859609702\n",
      "Gradient Descent(3/49): loss=1.4009381791158102\n",
      "Gradient Descent(4/49): loss=6.1548304842916925\n",
      "Gradient Descent(5/49): loss=61.34465026970501\n",
      "Gradient Descent(6/49): loss=790.6754860703094\n",
      "Gradient Descent(7/49): loss=10651.850249877067\n",
      "Gradient Descent(8/49): loss=144475.354167459\n",
      "Gradient Descent(9/49): loss=1961619.72488414\n",
      "Gradient Descent(10/49): loss=26638284.726293005\n",
      "Gradient Descent(11/49): loss=361750153.821874\n",
      "Gradient Descent(12/49): loss=4912617233.7441\n",
      "Gradient Descent(13/49): loss=66714065005.63091\n",
      "Gradient Descent(14/49): loss=905986910771.7839\n",
      "Gradient Descent(15/49): loss=12303436993003.04\n",
      "Gradient Descent(16/49): loss=167082504631187.62\n",
      "Gradient Descent(17/49): loss=2269005269063618.0\n",
      "Gradient Descent(18/49): loss=3.081342910629405e+16\n",
      "Gradient Descent(19/49): loss=4.184509512794832e+17\n",
      "Gradient Descent(20/49): loss=5.682626170003308e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=7.717090877503277e+19\n",
      "Gradient Descent(22/49): loss=1.0479924216378946e+21\n",
      "Gradient Descent(23/49): loss=1.423189299237356e+22\n",
      "Gradient Descent(24/49): loss=1.9327122407031614e+23\n",
      "Gradient Descent(25/49): loss=2.6246519752259367e+24\n",
      "Gradient Descent(26/49): loss=3.5643164284773312e+25\n",
      "Gradient Descent(27/49): loss=4.8403947350846664e+26\n",
      "Gradient Descent(28/49): loss=6.573328059272572e+27\n",
      "Gradient Descent(29/49): loss=8.926677293822899e+28\n",
      "Gradient Descent(30/49): loss=1.212256056437704e+30\n",
      "Gradient Descent(31/49): loss=1.6462617589938332e+31\n",
      "Gradient Descent(32/49): loss=2.2356479596311356e+32\n",
      "Gradient Descent(33/49): loss=3.0360431882093885e+33\n",
      "Gradient Descent(34/49): loss=4.122991815846305e+34\n",
      "Gradient Descent(35/49): loss=5.599084222369588e+35\n",
      "Gradient Descent(36/49): loss=7.603639669789676e+36\n",
      "Gradient Descent(37/49): loss=1.0325855788526024e+38\n",
      "Gradient Descent(38/49): loss=1.402266577532128e+39\n",
      "Gradient Descent(39/49): loss=1.9042988733667392e+40\n",
      "Gradient Descent(40/49): loss=2.5860661996863513e+41\n",
      "Gradient Descent(41/49): loss=3.511916371266089e+42\n",
      "Gradient Descent(42/49): loss=4.76923467785264e+43\n",
      "Gradient Descent(43/49): loss=6.476691642925498e+44\n",
      "Gradient Descent(44/49): loss=8.795443602793695e+45\n",
      "Gradient Descent(45/49): loss=1.1944343259637158e+47\n",
      "Gradient Descent(46/49): loss=1.622059583881898e+48\n",
      "Gradient Descent(47/49): loss=2.2027810457810283e+49\n",
      "Gradient Descent(48/49): loss=2.991409430250225e+50\n",
      "Gradient Descent(49/49): loss=4.062378508535455e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5457796925335326\n",
      "Gradient Descent(2/49): loss=0.7353263040558973\n",
      "Gradient Descent(3/49): loss=1.3796645143135629\n",
      "Gradient Descent(4/49): loss=5.778621561851617\n",
      "Gradient Descent(5/49): loss=56.131217862039456\n",
      "Gradient Descent(6/49): loss=720.0031211353211\n",
      "Gradient Descent(7/49): loss=9695.237935687526\n",
      "Gradient Descent(8/49): loss=131525.63868330486\n",
      "Gradient Descent(9/49): loss=1786308.5112428335\n",
      "Gradient Descent(10/49): loss=24264928.316995922\n",
      "Gradient Descent(11/49): loss=329619947.1353965\n",
      "Gradient Descent(12/49): loss=4477646712.066393\n",
      "Gradient Descent(13/49): loss=60825607671.82399\n",
      "Gradient Descent(14/49): loss=826272186618.5027\n",
      "Gradient Descent(15/49): loss=11224314295188.285\n",
      "Gradient Descent(16/49): loss=152474249727329.97\n",
      "Gradient Descent(17/49): loss=2071253194449565.5\n",
      "Gradient Descent(18/49): loss=2.8136487331237852e+16\n",
      "Gradient Descent(19/49): loss=3.8221397628755437e+17\n",
      "Gradient Descent(20/49): loss=5.192102409584911e+18\n",
      "Gradient Descent(21/49): loss=7.05309829155556e+19\n",
      "Gradient Descent(22/49): loss=9.581127563761248e+20\n",
      "Gradient Descent(23/49): loss=1.3015273798604757e+22\n",
      "Gradient Descent(24/49): loss=1.7680314861202674e+23\n",
      "Gradient Descent(25/49): loss=2.4017438159832213e+24\n",
      "Gradient Descent(26/49): loss=3.2625965108072653e+25\n",
      "Gradient Descent(27/49): loss=4.4320030810506795e+26\n",
      "Gradient Descent(28/49): loss=6.020557934570396e+27\n",
      "Gradient Descent(29/49): loss=8.178495632933016e+28\n",
      "Gradient Descent(30/49): loss=1.1109899040058927e+30\n",
      "Gradient Descent(31/49): loss=1.5092000071906073e+31\n",
      "Gradient Descent(32/49): loss=2.050139837897181e+32\n",
      "Gradient Descent(33/49): loss=2.784967754378221e+33\n",
      "Gradient Descent(34/49): loss=3.7831787127660163e+34\n",
      "Gradient Descent(35/49): loss=5.139176620707922e+35\n",
      "Gradient Descent(36/49): loss=6.98120240783475e+36\n",
      "Gradient Descent(37/49): loss=9.483462168389985e+37\n",
      "Gradient Descent(38/49): loss=1.2882602372100394e+39\n",
      "Gradient Descent(39/49): loss=1.7500090255099074e+40\n",
      "Gradient Descent(40/49): loss=2.377261597391726e+41\n",
      "Gradient Descent(41/49): loss=3.2293391748574727e+42\n",
      "Gradient Descent(42/49): loss=4.3868253782887513e+43\n",
      "Gradient Descent(43/49): loss=5.959187269466023e+44\n",
      "Gradient Descent(44/49): loss=8.095127991262554e+45\n",
      "Gradient Descent(45/49): loss=1.099665008527137e+47\n",
      "Gradient Descent(46/49): loss=1.493815949895059e+48\n",
      "Gradient Descent(47/49): loss=2.0292417007518306e+49\n",
      "Gradient Descent(48/49): loss=2.7565791357090987e+50\n",
      "Gradient Descent(49/49): loss=3.7446148128196375e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5411177418186564\n",
      "Gradient Descent(2/49): loss=0.7147390804642703\n",
      "Gradient Descent(3/49): loss=1.2475185406878593\n",
      "Gradient Descent(4/49): loss=4.400653042551072\n",
      "Gradient Descent(5/49): loss=38.89458640230971\n",
      "Gradient Descent(6/49): loss=496.048776359528\n",
      "Gradient Descent(7/49): loss=6769.995422258868\n",
      "Gradient Descent(8/49): loss=93350.67683727945\n",
      "Gradient Descent(9/49): loss=1289189.4872685743\n",
      "Gradient Descent(10/49): loss=17808103.952383168\n",
      "Gradient Descent(11/49): loss=245999450.65050808\n",
      "Gradient Descent(12/49): loss=3398231598.013272\n",
      "Gradient Descent(13/49): loss=46943144352.918724\n",
      "Gradient Descent(14/49): loss=648472307925.7354\n",
      "Gradient Descent(15/49): loss=8957992659507.479\n",
      "Gradient Descent(16/49): loss=123745658454653.7\n",
      "Gradient Descent(17/49): loss=1709421806385458.2\n",
      "Gradient Descent(18/49): loss=2.3613942895855764e+16\n",
      "Gradient Descent(19/49): loss=3.2620286988970195e+17\n",
      "Gradient Descent(20/49): loss=4.506164548366656e+18\n",
      "Gradient Descent(21/49): loss=6.22481308758193e+19\n",
      "Gradient Descent(22/49): loss=8.598953180566221e+20\n",
      "Gradient Descent(23/49): loss=1.1878588924875514e+22\n",
      "Gradient Descent(24/49): loss=1.640907583554097e+23\n",
      "Gradient Descent(25/49): loss=2.2667487820263694e+24\n",
      "Gradient Descent(26/49): loss=3.1312854497808587e+25\n",
      "Gradient Descent(27/49): loss=4.325555900042901e+26\n",
      "Gradient Descent(28/49): loss=5.975320405779636e+27\n",
      "Gradient Descent(29/49): loss=8.2543041349604e+28\n",
      "Gradient Descent(30/49): loss=1.1402490933627955e+30\n",
      "Gradient Descent(31/49): loss=1.5751394347197791e+31\n",
      "Gradient Descent(32/49): loss=2.1758966994591072e+32\n",
      "Gradient Descent(33/49): loss=3.005782435736705e+33\n",
      "Gradient Descent(34/49): loss=4.1521861093999033e+34\n",
      "Gradient Descent(35/49): loss=5.735827477768872e+35\n",
      "Gradient Descent(36/49): loss=7.92346874342851e+36\n",
      "Gradient Descent(37/49): loss=1.094547511608758e+38\n",
      "Gradient Descent(38/49): loss=1.5120072962520256e+39\n",
      "Gradient Descent(39/49): loss=2.0886860000798542e+40\n",
      "Gradient Descent(40/49): loss=2.8853096263116948e+41\n",
      "Gradient Descent(41/49): loss=3.985765040493783e+42\n",
      "Gradient Descent(42/49): loss=5.505933509926084e+43\n",
      "Gradient Descent(43/49): loss=7.605893349893075e+44\n",
      "Gradient Descent(44/49): loss=1.0506776652071175e+46\n",
      "Gradient Descent(45/49): loss=1.451405515935355e+47\n",
      "Gradient Descent(46/49): loss=2.004970735979505e+48\n",
      "Gradient Descent(47/49): loss=2.7696654091489925e+49\n",
      "Gradient Descent(48/49): loss=3.826014186131644e+50\n",
      "Gradient Descent(49/49): loss=5.2852537725770175e+51\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5493692376678685\n",
      "Gradient Descent(2/49): loss=0.7562531047753063\n",
      "Gradient Descent(3/49): loss=1.543573594618643\n",
      "Gradient Descent(4/49): loss=7.798564923996801\n",
      "Gradient Descent(5/49): loss=86.93507828801513\n",
      "Gradient Descent(6/49): loss=1208.2749938289955\n",
      "Gradient Descent(7/49): loss=17401.928406244857\n",
      "Gradient Descent(8/49): loss=251947.83709674407\n",
      "Gradient Descent(9/49): loss=3650590.4414947564\n",
      "Gradient Descent(10/49): loss=52901330.65469584\n",
      "Gradient Descent(11/49): loss=766615857.0304658\n",
      "Gradient Descent(12/49): loss=11109388936.31152\n",
      "Gradient Descent(13/49): loss=160991417025.93698\n",
      "Gradient Descent(14/49): loss=2333003023005.9155\n",
      "Gradient Descent(15/49): loss=33808654259576.223\n",
      "Gradient Descent(16/49): loss=489937258212709.6\n",
      "Gradient Descent(17/49): loss=7099913388883649.0\n",
      "Gradient Descent(18/49): loss=1.0288821535073056e+17\n",
      "Gradient Descent(19/49): loss=1.4910019711911708e+18\n",
      "Gradient Descent(20/49): loss=2.160681736503507e+19\n",
      "Gradient Descent(21/49): loss=3.1311464750987084e+20\n",
      "Gradient Descent(22/49): loss=4.537492997181718e+21\n",
      "Gradient Descent(23/49): loss=6.5754965036645275e+22\n",
      "Gradient Descent(24/49): loss=9.528864131924704e+23\n",
      "Gradient Descent(25/49): loss=1.3808729362730286e+25\n",
      "Gradient Descent(26/49): loss=2.0010885240171403e+26\n",
      "Gradient Descent(27/49): loss=2.899872374760863e+27\n",
      "Gradient Descent(28/49): loss=4.2023427194612735e+28\n",
      "Gradient Descent(29/49): loss=6.0898143261446435e+29\n",
      "Gradient Descent(30/49): loss=8.825039032435502e+30\n",
      "Gradient Descent(31/49): loss=1.2788782999450702e+32\n",
      "Gradient Descent(32/49): loss=1.853283254679276e+33\n",
      "Gradient Descent(33/49): loss=2.685680742430375e+34\n",
      "Gradient Descent(34/49): loss=3.89194745706027e+35\n",
      "Gradient Descent(35/49): loss=5.640005816480788e+36\n",
      "Gradient Descent(36/49): loss=8.173200168011624e+37\n",
      "Gradient Descent(37/49): loss=1.184417235726669e+39\n",
      "Gradient Descent(38/49): loss=1.716395242315121e+40\n",
      "Gradient Descent(39/49): loss=2.4873098254387437e+41\n",
      "Gradient Descent(40/49): loss=3.6044787442892486e+42\n",
      "Gradient Descent(41/49): loss=5.223421258242973e+43\n",
      "Gradient Descent(42/49): loss=7.569507708789093e+44\n",
      "Gradient Descent(43/49): loss=1.0969332956440877e+46\n",
      "Gradient Descent(44/49): loss=1.5896181117504413e+47\n",
      "Gradient Descent(45/49): loss=2.3035910672410675e+48\n",
      "Gradient Descent(46/49): loss=3.3382431703860774e+49\n",
      "Gradient Descent(47/49): loss=4.837606649506555e+50\n",
      "Gradient Descent(48/49): loss=7.010405444083755e+51\n",
      "Gradient Descent(49/49): loss=1.0159111323251751e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5512363851296269\n",
      "Gradient Descent(2/49): loss=0.7654424451082928\n",
      "Gradient Descent(3/49): loss=1.534752660645824\n",
      "Gradient Descent(4/49): loss=7.053273427770402\n",
      "Gradient Descent(5/49): loss=72.56273936870475\n",
      "Gradient Descent(6/49): loss=965.3367926991775\n",
      "Gradient Descent(7/49): loss=13438.150237322843\n",
      "Gradient Descent(8/49): loss=188400.363655202\n",
      "Gradient Descent(9/49): loss=2644270.129894626\n",
      "Gradient Descent(10/49): loss=37119826.476829655\n",
      "Gradient Descent(11/49): loss=521096458.7260884\n",
      "Gradient Descent(12/49): loss=7315301432.895619\n",
      "Gradient Descent(13/49): loss=102694370405.59651\n",
      "Gradient Descent(14/49): loss=1441654181174.0266\n",
      "Gradient Descent(15/49): loss=20238371461885.152\n",
      "Gradient Descent(16/49): loss=284112296795984.1\n",
      "Gradient Descent(17/49): loss=3988453190488814.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=5.59910958915623e+16\n",
      "Gradient Descent(19/49): loss=7.860197097587864e+17\n",
      "Gradient Descent(20/49): loss=1.103437920427476e+19\n",
      "Gradient Descent(21/49): loss=1.5490390751287135e+20\n",
      "Gradient Descent(22/49): loss=2.1745872711590072e+21\n",
      "Gradient Descent(23/49): loss=3.0527504927489914e+22\n",
      "Gradient Descent(24/49): loss=4.285542224301058e+23\n",
      "Gradient Descent(25/49): loss=6.016172038917278e+24\n",
      "Gradient Descent(26/49): loss=8.445681808152752e+25\n",
      "Gradient Descent(27/49): loss=1.185630010962909e+27\n",
      "Gradient Descent(28/49): loss=1.6644227841249512e+28\n",
      "Gradient Descent(29/49): loss=2.3365663644633612e+29\n",
      "Gradient Descent(30/49): loss=3.2801415767759795e+30\n",
      "Gradient Descent(31/49): loss=4.604760612551917e+31\n",
      "Gradient Descent(32/49): loss=6.464300336618325e+32\n",
      "Gradient Descent(33/49): loss=9.074777682926557e+33\n",
      "Gradient Descent(34/49): loss=1.2739443668488616e+35\n",
      "Gradient Descent(35/49): loss=1.788401111885452e+36\n",
      "Gradient Descent(36/49): loss=2.5106108400199833e+37\n",
      "Gradient Descent(37/49): loss=3.524470404394119e+38\n",
      "Gradient Descent(38/49): loss=4.947756710614461e+39\n",
      "Gradient Descent(39/49): loss=6.945808492790793e+40\n",
      "Gradient Descent(40/49): loss=9.750733198951469e+41\n",
      "Gradient Descent(41/49): loss=1.3688370189851818e+43\n",
      "Gradient Descent(42/49): loss=1.921614248193904e+44\n",
      "Gradient Descent(43/49): loss=2.6976194153482207e+45\n",
      "Gradient Descent(44/49): loss=3.786998622071675e+46\n",
      "Gradient Descent(45/49): loss=5.316301655443705e+47\n",
      "Gradient Descent(46/49): loss=7.463182882335462e+48\n",
      "Gradient Descent(47/49): loss=1.0477038803498169e+50\n",
      "Gradient Descent(48/49): loss=1.470797966773855e+51\n",
      "Gradient Descent(49/49): loss=2.0647500688304002e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5531377859566283\n",
      "Gradient Descent(2/49): loss=0.7680214652645704\n",
      "Gradient Descent(3/49): loss=1.5117716157136825\n",
      "Gradient Descent(4/49): loss=6.624623499141735\n",
      "Gradient Descent(5/49): loss=66.40963141209289\n",
      "Gradient Descent(6/49): loss=879.0919717816333\n",
      "Gradient Descent(7/49): loss=12231.336371532563\n",
      "Gradient Descent(8/49): loss=171512.36201091926\n",
      "Gradient Descent(9/49): loss=2407927.563601246\n",
      "Gradient Descent(10/49): loss=33812250.19362117\n",
      "Gradient Descent(11/49): loss=474807730.5948864\n",
      "Gradient Descent(12/49): loss=6667508147.668711\n",
      "Gradient Descent(13/49): loss=93628842925.88934\n",
      "Gradient Descent(14/49): loss=1314788236046.732\n",
      "Gradient Descent(15/49): loss=18462987306402.355\n",
      "Gradient Descent(16/49): loss=259267532182827.38\n",
      "Gradient Descent(17/49): loss=3640778827431648.5\n",
      "Gradient Descent(18/49): loss=5.112584039999457e+16\n",
      "Gradient Descent(19/49): loss=7.179374745159084e+17\n",
      "Gradient Descent(20/49): loss=1.0081677157434468e+19\n",
      "Gradient Descent(21/49): loss=1.4157251559448045e+20\n",
      "Gradient Descent(22/49): loss=1.9880399717987301e+21\n",
      "Gradient Descent(23/49): loss=2.791716254297971e+22\n",
      "Gradient Descent(24/49): loss=3.920283170896123e+23\n",
      "Gradient Descent(25/49): loss=5.505079578324253e+24\n",
      "Gradient Descent(26/49): loss=7.730538801041513e+25\n",
      "Gradient Descent(27/49): loss=1.085565236689984e+27\n",
      "Gradient Descent(28/49): loss=1.524411057804927e+28\n",
      "Gradient Descent(29/49): loss=2.1406627576280575e+29\n",
      "Gradient Descent(30/49): loss=3.00603765528592e+30\n",
      "Gradient Descent(31/49): loss=4.221245197449621e+31\n",
      "Gradient Descent(32/49): loss=5.9277071881179584e+32\n",
      "Gradient Descent(33/49): loss=8.324015986869146e+33\n",
      "Gradient Descent(34/49): loss=1.1689046025846351e+35\n",
      "Gradient Descent(35/49): loss=1.641440828680455e+36\n",
      "Gradient Descent(36/49): loss=2.305002468209662e+37\n",
      "Gradient Descent(37/49): loss=3.236812613418172e+38\n",
      "Gradient Descent(38/49): loss=4.545312223687457e+39\n",
      "Gradient Descent(39/49): loss=6.382780123000589e+40\n",
      "Gradient Descent(40/49): loss=8.963054701997793e+41\n",
      "Gradient Descent(41/49): loss=1.2586419717249853e+43\n",
      "Gradient Descent(42/49): loss=1.7674550314131667e+44\n",
      "Gradient Descent(43/49): loss=2.481958617498201e+45\n",
      "Gradient Descent(44/49): loss=3.4853042761989324e+46\n",
      "Gradient Descent(45/49): loss=4.8942580315601734e+47\n",
      "Gradient Descent(46/49): loss=6.872789226200708e+48\n",
      "Gradient Descent(47/49): loss=9.65115272696884e+49\n",
      "Gradient Descent(48/49): loss=1.3552685219006577e+51\n",
      "Gradient Descent(49/49): loss=1.9031434051625707e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5481573001785583\n",
      "Gradient Descent(2/49): loss=0.7454877403002211\n",
      "Gradient Descent(3/49): loss=1.3642293626362847\n",
      "Gradient Descent(4/49): loss=5.046100958670856\n",
      "Gradient Descent(5/49): loss=46.03845858986989\n",
      "Gradient Descent(6/49): loss=605.5639702514859\n",
      "Gradient Descent(7/49): loss=8537.735586083783\n",
      "Gradient Descent(8/49): loss=121675.11417732447\n",
      "Gradient Descent(9/49): loss=1736900.8413041031\n",
      "Gradient Descent(10/49): loss=24800371.078572273\n",
      "Gradient Descent(11/49): loss=354126425.84516805\n",
      "Gradient Descent(12/49): loss=5056629475.253821\n",
      "Gradient Descent(13/49): loss=72204512070.27644\n",
      "Gradient Descent(14/49): loss=1031021227497.7621\n",
      "Gradient Descent(15/49): loss=14722138067824.984\n",
      "Gradient Descent(16/49): loss=210220065578824.5\n",
      "Gradient Descent(17/49): loss=3001770245108470.0\n",
      "Gradient Descent(18/49): loss=4.28628189244582e+16\n",
      "Gradient Descent(19/49): loss=6.120459249568794e+17\n",
      "Gradient Descent(20/49): loss=8.739514191012471e+18\n",
      "Gradient Descent(21/49): loss=1.24793099962746e+20\n",
      "Gradient Descent(22/49): loss=1.7819431901979264e+21\n",
      "Gradient Descent(23/49): loss=2.5444688320433404e+22\n",
      "Gradient Descent(24/49): loss=3.63329295392459e+23\n",
      "Gradient Descent(25/49): loss=5.188044562698365e+24\n",
      "Gradient Descent(26/49): loss=7.408102436515683e+25\n",
      "Gradient Descent(27/49): loss=1.0578163129995643e+27\n",
      "Gradient Descent(28/49): loss=1.510474999012425e+28\n",
      "Gradient Descent(29/49): loss=2.1568345038771675e+29\n",
      "Gradient Descent(30/49): loss=3.079782902832929e+30\n",
      "Gradient Descent(31/49): loss=4.397677574024098e+31\n",
      "Gradient Descent(32/49): loss=6.279523153169381e+32\n",
      "Gradient Descent(33/49): loss=8.966644408882073e+33\n",
      "Gradient Descent(34/49): loss=1.2803633332374607e+35\n",
      "Gradient Descent(35/49): loss=1.828253904520953e+36\n",
      "Gradient Descent(36/49): loss=2.610596736587518e+37\n",
      "Gradient Descent(37/49): loss=3.7277181819377464e+38\n",
      "Gradient Descent(38/49): loss=5.322876049448243e+39\n",
      "Gradient Descent(39/49): loss=7.600630749146852e+40\n",
      "Gradient Descent(40/49): loss=1.0853077781299132e+42\n",
      "Gradient Descent(41/49): loss=1.5497305580875585e+43\n",
      "Gradient Descent(42/49): loss=2.212888224950069e+44\n",
      "Gradient Descent(43/49): loss=3.1598230225037344e+45\n",
      "Gradient Descent(44/49): loss=4.511968305027999e+46\n",
      "Gradient Descent(45/49): loss=6.442720950063332e+47\n",
      "Gradient Descent(46/49): loss=9.199677487567712e+48\n",
      "Gradient Descent(47/49): loss=1.3136385469935276e+50\n",
      "Gradient Descent(48/49): loss=1.8757681826121767e+51\n",
      "Gradient Descent(49/49): loss=2.6784432315516374e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5566564671979071\n",
      "Gradient Descent(2/49): loss=0.7898714242020715\n",
      "Gradient Descent(3/49): loss=1.691414474391134\n",
      "Gradient Descent(4/49): loss=8.915479784512288\n",
      "Gradient Descent(5/49): loss=102.46676195832164\n",
      "Gradient Descent(6/49): loss=1469.286700083783\n",
      "Gradient Descent(7/49): loss=21853.814682888515\n",
      "Gradient Descent(8/49): loss=326844.64302782173\n",
      "Gradient Descent(9/49): loss=4892344.0956592355\n",
      "Gradient Descent(10/49): loss=73239893.73140784\n",
      "Gradient Descent(11/49): loss=1096445001.3905103\n",
      "Gradient Descent(12/49): loss=16414486206.721659\n",
      "Gradient Descent(13/49): loss=245735518445.42303\n",
      "Gradient Descent(14/49): loss=3678820556286.7197\n",
      "Gradient Descent(15/49): loss=55074337309937.81\n",
      "Gradient Descent(16/49): loss=824498664387225.8\n",
      "Gradient Descent(17/49): loss=1.2343281480169758e+16\n",
      "Gradient Descent(18/49): loss=1.8478695513464906e+17\n",
      "Gradient Descent(19/49): loss=2.766380953314538e+18\n",
      "Gradient Descent(20/49): loss=4.141452286657287e+19\n",
      "Gradient Descent(21/49): loss=6.200023544158511e+20\n",
      "Gradient Descent(22/49): loss=9.281838661274647e+21\n",
      "Gradient Descent(23/49): loss=1.389551641543439e+23\n",
      "Gradient Descent(24/49): loss=2.0802492210642164e+24\n",
      "Gradient Descent(25/49): loss=3.1142684390855367e+25\n",
      "Gradient Descent(26/49): loss=4.6622624887814314e+26\n",
      "Gradient Descent(27/49): loss=6.979710304189674e+27\n",
      "Gradient Descent(28/49): loss=1.0449080472760936e+29\n",
      "Gradient Descent(29/49): loss=1.5642953355914591e+30\n",
      "Gradient Descent(30/49): loss=2.3418519010664586e+31\n",
      "Gradient Descent(31/49): loss=3.5059046726973185e+32\n",
      "Gradient Descent(32/49): loss=5.248567413013338e+33\n",
      "Gradient Descent(33/49): loss=7.857446924747964e+34\n",
      "Gradient Descent(34/49): loss=1.1763109305246763e+36\n",
      "Gradient Descent(35/49): loss=1.7610140017792117e+37\n",
      "Gradient Descent(36/49): loss=2.6363525442029896e+38\n",
      "Gradient Descent(37/49): loss=3.946791297686068e+39\n",
      "Gradient Descent(38/49): loss=5.908603377701709e+40\n",
      "Gradient Descent(39/49): loss=8.845563710312005e+41\n",
      "Gradient Descent(40/49): loss=1.324238442682988e+43\n",
      "Gradient Descent(41/49): loss=1.9824711126495007e+44\n",
      "Gradient Descent(42/49): loss=2.967888248680538e+45\n",
      "Gradient Descent(43/49): loss=4.443121819255031e+46\n",
      "Gradient Descent(44/49): loss=6.651642463127252e+47\n",
      "Gradient Descent(45/49): loss=9.957941568367028e+48\n",
      "Gradient Descent(46/49): loss=1.4907686459201892e+50\n",
      "Gradient Descent(47/49): loss=2.2317776624824008e+51\n",
      "Gradient Descent(48/49): loss=3.341116375358958e+52\n",
      "Gradient Descent(49/49): loss=5.001868609651287e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5586025893084212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/49): loss=0.7999065491151479\n",
      "Gradient Descent(3/49): loss=1.6829686565031359\n",
      "Gradient Descent(4/49): loss=8.07373512072326\n",
      "Gradient Descent(5/49): loss=85.61613567246458\n",
      "Gradient Descent(6/49): loss=1174.7935135881673\n",
      "Gradient Descent(7/49): loss=16889.24078634599\n",
      "Gradient Descent(8/49): loss=244618.2438850279\n",
      "Gradient Descent(9/49): loss=3547157.0892145173\n",
      "Gradient Descent(10/49): loss=51446280.4495249\n",
      "Gradient Descent(11/49): loss=746175011.4110664\n",
      "Gradient Descent(12/49): loss=10822548436.649166\n",
      "Gradient Descent(13/49): loss=156970743883.85718\n",
      "Gradient Descent(14/49): loss=2276711228912.7256\n",
      "Gradient Descent(15/49): loss=33021529981995.707\n",
      "Gradient Descent(16/49): loss=478945872486401.06\n",
      "Gradient Descent(17/49): loss=6946654168300203.0\n",
      "Gradient Descent(18/49): loss=1.0075460904915875e+17\n",
      "Gradient Descent(19/49): loss=1.461349737419253e+18\n",
      "Gradient Descent(20/49): loss=2.1195487484032963e+19\n",
      "Gradient Descent(21/49): loss=3.074203786968452e+20\n",
      "Gradient Descent(22/49): loss=4.4588400860943137e+21\n",
      "Gradient Descent(23/49): loss=6.4671232914482314e+22\n",
      "Gradient Descent(24/49): loss=9.37994699501027e+23\n",
      "Gradient Descent(25/49): loss=1.3604720625250184e+25\n",
      "Gradient Descent(26/49): loss=1.973235279363178e+26\n",
      "Gradient Descent(27/49): loss=2.861990021681845e+27\n",
      "Gradient Descent(28/49): loss=4.151044211438341e+28\n",
      "Gradient Descent(29/49): loss=6.0206946616780416e+29\n",
      "Gradient Descent(30/49): loss=8.732444744691739e+30\n",
      "Gradient Descent(31/49): loss=1.2665580220246832e+32\n",
      "Gradient Descent(32/49): loss=1.8370218994288113e+33\n",
      "Gradient Descent(33/49): loss=2.6644254746311614e+34\n",
      "Gradient Descent(34/49): loss=3.864495633977426e+35\n",
      "Gradient Descent(35/49): loss=5.605083214833742e+36\n",
      "Gradient Descent(36/49): loss=8.129639885988502e+37\n",
      "Gradient Descent(37/49): loss=1.1791269128876853e+39\n",
      "Gradient Descent(38/49): loss=1.71021139459364e+40\n",
      "Gradient Descent(39/49): loss=2.480498903239396e+41\n",
      "Gradient Descent(40/49): loss=3.597727642572367e+42\n",
      "Gradient Descent(41/49): loss=5.218161625963992e+43\n",
      "Gradient Descent(42/49): loss=7.568446936470716e+44\n",
      "Gradient Descent(43/49): loss=1.0977312152455621e+46\n",
      "Gradient Descent(44/49): loss=1.5921546798693487e+47\n",
      "Gradient Descent(45/49): loss=2.3092688714904647e+48\n",
      "Gradient Descent(46/49): loss=3.349374773858194e+49\n",
      "Gradient Descent(47/49): loss=4.857949420379565e+50\n",
      "Gradient Descent(48/49): loss=7.045993406041343e+51\n",
      "Gradient Descent(49/49): loss=1.0219543017411638e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5606713528614812\n",
      "Gradient Descent(2/49): loss=0.8029139364428618\n",
      "Gradient Descent(3/49): loss=1.658181475471916\n",
      "Gradient Descent(4/49): loss=7.586386366254889\n",
      "Gradient Descent(5/49): loss=78.37366434551035\n",
      "Gradient Descent(6/49): loss=1069.886629195195\n",
      "Gradient Descent(7/49): loss=15372.549011038262\n",
      "Gradient Descent(8/49): loss=222689.4998606439\n",
      "Gradient Descent(9/49): loss=3230086.2283629063\n",
      "Gradient Descent(10/49): loss=46861657.94843177\n",
      "Gradient Descent(11/49): loss=679884945.1238484\n",
      "Gradient Descent(12/49): loss=9864054858.834879\n",
      "Gradient Descent(13/49): loss=143111949710.52634\n",
      "Gradient Descent(14/49): loss=2076329988355.2703\n",
      "Gradient Descent(15/49): loss=30124293061482.504\n",
      "Gradient Descent(16/49): loss=437056267879572.25\n",
      "Gradient Descent(17/49): loss=6341001297823477.0\n",
      "Gradient Descent(18/49): loss=9.199798840010525e+16\n",
      "Gradient Descent(19/49): loss=1.3347465916292995e+18\n",
      "Gradient Descent(20/49): loss=1.9365080637654938e+19\n",
      "Gradient Descent(21/49): loss=2.809569625087003e+20\n",
      "Gradient Descent(22/49): loss=4.0762450856326275e+21\n",
      "Gradient Descent(23/49): loss=5.913992609323626e+22\n",
      "Gradient Descent(24/49): loss=8.58027617289531e+23\n",
      "Gradient Descent(25/49): loss=1.2448635645416116e+25\n",
      "Gradient Descent(26/49): loss=1.8061018819168167e+26\n",
      "Gradient Descent(27/49): loss=2.620370698265735e+27\n",
      "Gradient Descent(28/49): loss=3.8017471024627534e+28\n",
      "Gradient Descent(29/49): loss=5.515739067243316e+29\n",
      "Gradient Descent(30/49): loss=8.002472715296015e+30\n",
      "Gradient Descent(31/49): loss=1.1610333407425501e+32\n",
      "Gradient Descent(32/49): loss=1.6844773687753505e+33\n",
      "Gradient Descent(33/49): loss=2.443912596085743e+34\n",
      "Gradient Descent(34/49): loss=3.545734058540065e+35\n",
      "Gradient Descent(35/49): loss=5.144304274230883e+36\n",
      "Gradient Descent(36/49): loss=7.463579058370327e+37\n",
      "Gradient Descent(37/49): loss=1.0828483190542565e+39\n",
      "Gradient Descent(38/49): loss=1.5710431589300587e+40\n",
      "Gradient Descent(39/49): loss=2.2793373400408726e+41\n",
      "Gradient Descent(40/49): loss=3.3069611615527487e+42\n",
      "Gradient Descent(41/49): loss=4.797882231781331e+43\n",
      "Gradient Descent(42/49): loss=6.96097498140402e+44\n",
      "Gradient Descent(43/49): loss=1.0099283465268187e+46\n",
      "Gradient Descent(44/49): loss=1.4652477100451283e+47\n",
      "Gradient Descent(45/49): loss=2.125844728664168e+48\n",
      "Gradient Descent(46/49): loss=3.084267444615233e+49\n",
      "Gradient Descent(47/49): loss=4.474788558941933e+50\n",
      "Gradient Descent(48/49): loss=6.492216711685038e+51\n",
      "Gradient Descent(49/49): loss=9.419188700493505e+52\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5553656516668042\n",
      "Gradient Descent(2/49): loss=0.7783129103273558\n",
      "Gradient Descent(3/49): loss=1.4937829739270918\n",
      "Gradient Descent(4/49): loss=5.782131749230689\n",
      "Gradient Descent(5/49): loss=54.36345877089004\n",
      "Gradient Descent(6/49): loss=736.9029809336218\n",
      "Gradient Descent(7/49): loss=10726.557219264543\n",
      "Gradient Descent(8/49): loss=157910.90714850303\n",
      "Gradient Descent(9/49): loss=2328758.694647172\n",
      "Gradient Descent(10/49): loss=34352269.42182828\n",
      "Gradient Descent(11/49): loss=506763052.6795548\n",
      "Gradient Descent(12/49): loss=7475794716.230155\n",
      "Gradient Descent(13/49): loss=110283425804.30908\n",
      "Gradient Descent(14/49): loss=1626908774552.3523\n",
      "Gradient Descent(15/49): loss=24000272119540.043\n",
      "Gradient Descent(16/49): loss=354053695681781.25\n",
      "Gradient Descent(17/49): loss=5223024925806517.0\n",
      "Gradient Descent(18/49): loss=7.705042966948816e+16\n",
      "Gradient Descent(19/49): loss=1.1366533372287665e+18\n",
      "Gradient Descent(20/49): loss=1.6767989673497594e+19\n",
      "Gradient Descent(21/49): loss=2.4736255855820816e+20\n",
      "Gradient Descent(22/49): loss=3.64910979598097e+21\n",
      "Gradient Descent(23/49): loss=5.383192339511122e+22\n",
      "Gradient Descent(24/49): loss=7.941323058047427e+23\n",
      "Gradient Descent(25/49): loss=1.1715095418270657e+25\n",
      "Gradient Descent(26/49): loss=1.728219084603431e+26\n",
      "Gradient Descent(27/49): loss=2.549480902843938e+27\n",
      "Gradient Descent(28/49): loss=3.761012091506473e+28\n",
      "Gradient Descent(29/49): loss=5.5482713899441205e+29\n",
      "Gradient Descent(30/49): loss=8.184848829917468e+30\n",
      "Gradient Descent(31/49): loss=1.2074346343262618e+32\n",
      "Gradient Descent(32/49): loss=1.7812160327771148e+33\n",
      "Gradient Descent(33/49): loss=2.6276623721272265e+34\n",
      "Gradient Descent(34/49): loss=3.876345942792968e+35\n",
      "Gradient Descent(35/49): loss=5.718412695480004e+36\n",
      "Gradient Descent(36/49): loss=8.435842475985543e+37\n",
      "Gradient Descent(37/49): loss=1.2444613928597844e+39\n",
      "Gradient Descent(38/49): loss=1.8358381664038483e+40\n",
      "Gradient Descent(39/49): loss=2.708241326378166e+41\n",
      "Gradient Descent(40/49): loss=3.9952165807023335e+42\n",
      "Gradient Descent(41/49): loss=5.893771493423261e+43\n",
      "Gradient Descent(42/49): loss=8.694533003410648e+44\n",
      "Gradient Descent(43/49): loss=1.2826236007240653e+46\n",
      "Gradient Descent(44/49): loss=1.8921353228391513e+47\n",
      "Gradient Descent(45/49): loss=2.791291286012909e+48\n",
      "Gradient Descent(46/49): loss=4.1177324630676064e+49\n",
      "Gradient Descent(47/49): loss=6.074507781529509e+50\n",
      "Gradient Descent(48/49): loss=8.961156442002668e+51\n",
      "Gradient Descent(49/49): loss=1.3219560772021323e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5641141087945941\n",
      "Gradient Descent(2/49): loss=0.8256997452817919\n",
      "Gradient Descent(3/49): loss=1.8547358784814123\n",
      "Gradient Descent(4/49): loss=10.179194884413302\n",
      "Gradient Descent(5/49): loss=120.47514661008059\n",
      "Gradient Descent(6/49): loss=1781.146318891502\n",
      "Gradient Descent(7/49): loss=27344.86956890289\n",
      "Gradient Descent(8/49): loss=422243.9121730318\n",
      "Gradient Descent(9/49): loss=6525830.444424607\n",
      "Gradient Descent(10/49): loss=100871282.94084318\n",
      "Gradient Descent(11/49): loss=1559223951.0514796\n",
      "Gradient Descent(12/49): loss=24101877590.95706\n",
      "Gradient Descent(13/49): loss=372557642662.95416\n",
      "Gradient Descent(14/49): loss=5758854577737.257\n",
      "Gradient Descent(15/49): loss=89018188483094.77\n",
      "Gradient Descent(16/49): loss=1376009376340683.2\n",
      "Gradient Descent(17/49): loss=2.1269830768336584e+16\n",
      "Gradient Descent(18/49): loss=3.28780972508811e+17\n",
      "Gradient Descent(19/49): loss=5.082171506767763e+18\n",
      "Gradient Descent(20/49): loss=7.85582785619955e+19\n",
      "Gradient Descent(21/49): loss=1.2143240586050284e+21\n",
      "Gradient Descent(22/49): loss=1.8770560484511893e+22\n",
      "Gradient Descent(23/49): loss=2.9014820089084446e+23\n",
      "Gradient Descent(24/49): loss=4.485000783522586e+24\n",
      "Gradient Descent(25/49): loss=6.932744013727416e+25\n",
      "Gradient Descent(26/49): loss=1.0716372611672894e+27\n",
      "Gradient Descent(27/49): loss=1.656496211670572e+28\n",
      "Gradient Descent(28/49): loss=2.5605489830487666e+29\n",
      "Gradient Descent(29/49): loss=3.957999449923219e+30\n",
      "Gradient Descent(30/49): loss=6.118125350970502e+31\n",
      "Gradient Descent(31/49): loss=9.457165996047398e+32\n",
      "Gradient Descent(32/49): loss=1.461852831482234e+34\n",
      "Gradient Descent(33/49): loss=2.2596766323080564e+35\n",
      "Gradient Descent(34/49): loss=3.492922387694633e+36\n",
      "Gradient Descent(35/49): loss=5.39922687698739e+37\n",
      "Gradient Descent(36/49): loss=8.345920015824863e+38\n",
      "Gradient Descent(37/49): loss=1.2900806448313226e+40\n",
      "Gradient Descent(38/49): loss=1.9941577046182202e+41\n",
      "Gradient Descent(39/49): loss=3.082493305221237e+42\n",
      "Gradient Descent(40/49): loss=4.764801176320581e+43\n",
      "Gradient Descent(41/49): loss=7.36524884300979e+44\n",
      "Gradient Descent(42/49): loss=1.1384922164023501e+46\n",
      "Gradient Descent(43/49): loss=1.759838064451699e+47\n",
      "Gradient Descent(44/49): loss=2.7202908974465494e+48\n",
      "Gradient Descent(45/49): loss=4.2049224392905193e+49\n",
      "Gradient Descent(46/49): loss=6.499809537665967e+50\n",
      "Gradient Descent(47/49): loss=1.0047158927635909e+52\n",
      "Gradient Descent(48/49): loss=1.5530516999336762e+53\n",
      "Gradient Descent(49/49): loss=2.4006483823327297e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5661406087841311\n",
      "Gradient Descent(2/49): loss=0.8366317022134295\n",
      "Gradient Descent(3/49): loss=1.8468136409896951\n",
      "Gradient Descent(4/49): loss=9.230428726771944\n",
      "Gradient Descent(5/49): loss=100.76900068839379\n",
      "Gradient Descent(6/49): loss=1425.2463463550575\n",
      "Gradient Descent(7/49): loss=21148.98999599255\n",
      "Gradient Descent(8/49): loss=316282.73878732463\n",
      "Gradient Descent(9/49): loss=4735944.45737612\n",
      "Gradient Descent(10/49): loss=70929339.05531935\n",
      "Gradient Descent(11/49): loss=1062330230.0632855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=15910927394.802164\n",
      "Gradient Descent(13/49): loss=238304271119.18936\n",
      "Gradient Descent(14/49): loss=3569178104810.25\n",
      "Gradient Descent(15/49): loss=53457005104853.88\n",
      "Gradient Descent(16/49): loss=800646905699662.2\n",
      "Gradient Descent(17/49): loss=1.1991608335345748e+16\n",
      "Gradient Descent(18/49): loss=1.796031052626945e+17\n",
      "Gradient Descent(19/49): loss=2.6899874077312527e+18\n",
      "Gradient Descent(20/49): loss=4.0289015288393286e+19\n",
      "Gradient Descent(21/49): loss=6.034246659459805e+20\n",
      "Gradient Descent(22/49): loss=9.037732117938898e+21\n",
      "Gradient Descent(23/49): loss=1.353617219269204e+23\n",
      "Gradient Descent(24/49): loss=2.0273665477041177e+24\n",
      "Gradient Descent(25/49): loss=3.0364678139723804e+25\n",
      "Gradient Descent(26/49): loss=4.547839065279882e+26\n",
      "Gradient Descent(27/49): loss=6.81148012454248e+27\n",
      "Gradient Descent(28/49): loss=1.0201825706904792e+29\n",
      "Gradient Descent(29/49): loss=1.5279681633226552e+30\n",
      "Gradient Descent(30/49): loss=2.2884989169609654e+31\n",
      "Gradient Descent(31/49): loss=3.427576188199417e+32\n",
      "Gradient Descent(32/49): loss=5.1336176909853974e+33\n",
      "Gradient Descent(33/49): loss=7.688824157412327e+34\n",
      "Gradient Descent(34/49): loss=1.1515858889807386e+36\n",
      "Gradient Descent(35/49): loss=1.724776158941152e+37\n",
      "Gradient Descent(36/49): loss=2.583266108865641e+38\n",
      "Gradient Descent(37/49): loss=3.8690607790579947e+39\n",
      "Gradient Descent(38/49): loss=5.794846787433076e+40\n",
      "Gradient Descent(39/49): loss=8.679173372406801e+41\n",
      "Gradient Descent(40/49): loss=1.299914444531226e+43\n",
      "Gradient Descent(41/49): loss=1.9469337580848172e+44\n",
      "Gradient Descent(42/49): loss=2.916000413963484e+45\n",
      "Gradient Descent(43/49): loss=4.367410231049533e+46\n",
      "Gradient Descent(44/49): loss=6.541244656529214e+47\n",
      "Gradient Descent(45/49): loss=9.797083258260866e+48\n",
      "Gradient Descent(46/49): loss=1.4673482709974577e+50\n",
      "Gradient Descent(47/49): loss=2.1977060841896092e+51\n",
      "Gradient Descent(48/49): loss=3.291592137973438e+52\n",
      "Gradient Descent(49/49): loss=4.92994895027721e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5683803932480912\n",
      "Gradient Descent(2/49): loss=0.8400997831510247\n",
      "Gradient Descent(3/49): loss=1.8201194141033372\n",
      "Gradient Descent(4/49): loss=8.677500818341906\n",
      "Gradient Descent(5/49): loss=92.26666901083459\n",
      "Gradient Descent(6/49): loss=1298.0402770244634\n",
      "Gradient Descent(7/49): loss=19249.836807476615\n",
      "Gradient Descent(8/49): loss=287927.8995053804\n",
      "Gradient Descent(9/49): loss=4312573.668807631\n",
      "Gradient Descent(10/49): loss=64607845.51892355\n",
      "Gradient Descent(11/49): loss=967942336.2394208\n",
      "Gradient Descent(12/49): loss=14501609904.589457\n",
      "Gradient Descent(13/49): loss=217261791674.03537\n",
      "Gradient Descent(14/49): loss=3254996760205.3105\n",
      "Gradient Descent(15/49): loss=48766071966114.57\n",
      "Gradient Descent(16/49): loss=730608955899052.2\n",
      "Gradient Descent(17/49): loss=1.0945918448246006e+16\n",
      "Gradient Descent(18/49): loss=1.639907774476505e+17\n",
      "Gradient Descent(19/49): loss=2.4568952541982444e+18\n",
      "Gradient Descent(20/49): loss=3.6808986359281304e+19\n",
      "Gradient Descent(21/49): loss=5.514689624977474e+20\n",
      "Gradient Descent(22/49): loss=8.262058988257937e+21\n",
      "Gradient Descent(23/49): loss=1.2378143353032545e+23\n",
      "Gradient Descent(24/49): loss=1.854482436956474e+24\n",
      "Gradient Descent(25/49): loss=2.7783691066540293e+25\n",
      "Gradient Descent(26/49): loss=4.162527904808978e+26\n",
      "Gradient Descent(27/49): loss=6.23626231547753e+27\n",
      "Gradient Descent(28/49): loss=9.343112780700915e+28\n",
      "Gradient Descent(29/49): loss=1.3997768537773652e+30\n",
      "Gradient Descent(30/49): loss=2.0971332428076824e+31\n",
      "Gradient Descent(31/49): loss=3.1419063875937578e+32\n",
      "Gradient Descent(32/49): loss=4.70717622843384e+33\n",
      "Gradient Descent(33/49): loss=7.052249593757444e+34\n",
      "Gradient Descent(34/49): loss=1.0565617669512844e+36\n",
      "Gradient Descent(35/49): loss=1.5829314498049106e+37\n",
      "Gradient Descent(36/49): loss=2.3715338309198105e+38\n",
      "Gradient Descent(37/49): loss=3.553010910163286e+39\n",
      "Gradient Descent(38/49): loss=5.32308937074818e+40\n",
      "Gradient Descent(39/49): loss=7.975005190082754e+41\n",
      "Gradient Descent(40/49): loss=1.1948081903593391e+43\n",
      "Gradient Descent(41/49): loss=1.7900510127880618e+44\n",
      "Gradient Descent(42/49): loss=2.6818385195533264e+45\n",
      "Gradient Descent(43/49): loss=4.017906637061529e+46\n",
      "Gradient Descent(44/49): loss=6.019592017356818e+47\n",
      "Gradient Descent(45/49): loss=9.018499265559484e+48\n",
      "Gradient Descent(46/49): loss=1.3511435454161078e+50\n",
      "Gradient Descent(47/49): loss=2.0242712524147323e+51\n",
      "Gradient Descent(48/49): loss=3.0327452010961534e+52\n",
      "Gradient Descent(49/49): loss=4.543631908915544e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5627427962833936\n",
      "Gradient Descent(2/49): loss=0.8133057098994055\n",
      "Gradient Descent(3/49): loss=1.6372941766140088\n",
      "Gradient Descent(4/49): loss=6.619656015528512\n",
      "Gradient Descent(5/49): loss=64.04191203656329\n",
      "Gradient Descent(6/49): loss=893.9590403926329\n",
      "Gradient Descent(7/49): loss=13427.452413194786\n",
      "Gradient Descent(8/49): loss=204083.850819071\n",
      "Gradient Descent(9/49): loss=3107654.7061119564\n",
      "Gradient Descent(10/49): loss=47335238.55298631\n",
      "Gradient Descent(11/49): loss=721035416.8132523\n",
      "Gradient Descent(12/49): loss=10983274307.939972\n",
      "Gradient Descent(13/49): loss=167304480454.767\n",
      "Gradient Descent(14/49): loss=2548492698855.373\n",
      "Gradient Descent(15/49): loss=38820330514245.53\n",
      "Gradient Descent(16/49): loss=591337016178871.0\n",
      "Gradient Descent(17/49): loss=9007637553087292.0\n",
      "Gradient Descent(18/49): loss=1.3721030829705765e+17\n",
      "Gradient Descent(19/49): loss=2.0900784020638756e+18\n",
      "Gradient Descent(20/49): loss=3.183746018068254e+19\n",
      "Gradient Descent(21/49): loss=4.849693053409432e+20\n",
      "Gradient Descent(22/49): loss=7.387374048938952e+21\n",
      "Gradient Descent(23/49): loss=1.12529380185349e+23\n",
      "Gradient Descent(24/49): loss=1.714122138802197e+24\n",
      "Gradient Descent(25/49): loss=2.611064507679885e+25\n",
      "Gradient Descent(26/49): loss=3.977346601467728e+26\n",
      "Gradient Descent(27/49): loss=6.0585580868179e+27\n",
      "Gradient Descent(28/49): loss=9.228797429371793e+28\n",
      "Gradient Descent(29/49): loss=1.4057916218991525e+30\n",
      "Gradient Descent(30/49): loss=2.141395018501759e+31\n",
      "Gradient Descent(31/49): loss=3.2619148911054266e+32\n",
      "Gradient Descent(32/49): loss=4.9687650643082933e+33\n",
      "Gradient Descent(33/49): loss=7.568752431773138e+34\n",
      "Gradient Descent(34/49): loss=1.152922559872474e+36\n",
      "Gradient Descent(35/49): loss=1.7562080951186312e+37\n",
      "Gradient Descent(36/49): loss=2.6751726271202042e+38\n",
      "Gradient Descent(37/49): loss=4.075000340099086e+39\n",
      "Gradient Descent(38/49): loss=6.207310737058359e+40\n",
      "Gradient Descent(39/49): loss=9.455387330216005e+41\n",
      "Gradient Descent(40/49): loss=1.440307298145317e+43\n",
      "Gradient Descent(41/49): loss=2.193971585343102e+44\n",
      "Gradient Descent(42/49): loss=3.342003004144523e+45\n",
      "Gradient Descent(43/49): loss=5.090760588845345e+46\n",
      "Gradient Descent(44/49): loss=7.754584104443081e+47\n",
      "Gradient Descent(45/49): loss=1.1812296725295778e+49\n",
      "Gradient Descent(46/49): loss=1.7993273662024394e+50\n",
      "Gradient Descent(47/49): loss=2.7408547601347348e+51\n",
      "Gradient Descent(48/49): loss=4.175051720581659e+52\n",
      "Gradient Descent(49/49): loss=6.359715634357571e+53\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5717421624579296\n",
      "Gradient Descent(2/49): loss=0.86383271093908\n",
      "Gradient Descent(3/49): loss=2.0348242426826313\n",
      "Gradient Descent(4/49): loss=11.606257167703601\n",
      "Gradient Descent(5/49): loss=141.30742239232558\n",
      "Gradient Descent(6/49): loss=2152.7153305534584\n",
      "Gradient Descent(7/49): loss=34095.17576089138\n",
      "Gradient Descent(8/49): loss=543292.4848314831\n",
      "Gradient Descent(9/49): loss=8665300.076513434\n",
      "Gradient Descent(10/49): loss=138228425.35148877\n",
      "Gradient Descent(11/49): loss=2205063529.5310526\n",
      "Gradient Descent(12/49): loss=35175997172.386826\n",
      "Gradient Descent(13/49): loss=561140964853.227\n",
      "Gradient Descent(14/49): loss=8951536151423.637\n",
      "Gradient Descent(15/49): loss=142798344094076.16\n",
      "Gradient Descent(16/49): loss=2277974056691661.0\n",
      "Gradient Descent(17/49): loss=3.633911749926559e+16\n",
      "Gradient Descent(18/49): loss=5.796955662498358e+17\n",
      "Gradient Descent(19/49): loss=9.247526430418379e+18\n",
      "Gradient Descent(20/49): loss=1.4752009513308242e+20\n",
      "Gradient Descent(21/49): loss=2.3532972446009034e+21\n",
      "Gradient Descent(22/49): loss=3.754070193928845e+22\n",
      "Gradient Descent(23/49): loss=5.9886370297155636e+23\n",
      "Gradient Descent(24/49): loss=9.553303913091665e+24\n",
      "Gradient Descent(25/49): loss=1.5239797503677966e+26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=2.4311110592308394e+27\n",
      "Gradient Descent(27/49): loss=3.878201781151011e+28\n",
      "Gradient Descent(28/49): loss=6.186656507614069e+29\n",
      "Gradient Descent(29/49): loss=9.869192193461588e+30\n",
      "Gradient Descent(30/49): loss=1.5743714627053818e+32\n",
      "Gradient Descent(31/49): loss=2.5114978551366955e+33\n",
      "Gradient Descent(32/49): loss=4.0064378869757707e+34\n",
      "Gradient Descent(33/49): loss=6.391223671310292e+35\n",
      "Gradient Descent(34/49): loss=1.0195525593821714e+37\n",
      "Gradient Descent(35/49): loss=1.6264294207209306e+38\n",
      "Gradient Descent(36/49): loss=2.594542710176298e+39\n",
      "Gradient Descent(37/49): loss=4.1389142308708985e+40\n",
      "Gradient Descent(38/49): loss=6.602555025714565e+41\n",
      "Gradient Descent(39/49): loss=1.0532649491124322e+43\n",
      "Gradient Descent(40/49): loss=1.680208720273046e+44\n",
      "Gradient Descent(41/49): loss=2.6803335153804645e+45\n",
      "Gradient Descent(42/49): loss=4.275771019986296e+46\n",
      "Gradient Descent(43/49): loss=6.820874234660294e+47\n",
      "Gradient Descent(44/49): loss=1.08809206825116e+49\n",
      "Gradient Descent(45/49): loss=1.7357662790128488e+50\n",
      "Gradient Descent(46/49): loss=2.7689610679733913e+51\n",
      "Gradient Descent(47/49): loss=4.417153097542911e+52\n",
      "Gradient Descent(48/49): loss=7.0464123576189015e+53\n",
      "Gradient Descent(49/49): loss=1.1240707762930599e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5738504435567566\n",
      "Gradient Descent(2/49): loss=0.875714480740085\n",
      "Gradient Descent(3/49): loss=2.0275927818545543\n",
      "Gradient Descent(4/49): loss=10.538948403874478\n",
      "Gradient Descent(5/49): loss=118.3184814904833\n",
      "Gradient Descent(6/49): loss=1723.8752196479272\n",
      "Gradient Descent(7/49): loss=26389.32049438073\n",
      "Gradient Descent(8/49): loss=407285.75144988956\n",
      "Gradient Descent(9/49): loss=6294324.131585874\n",
      "Gradient Descent(10/49): loss=97295707.0621604\n",
      "Gradient Descent(11/49): loss=1504020579.261942\n",
      "Gradient Descent(12/49): loss=23249650421.855198\n",
      "Gradient Descent(13/49): loss=359401176493.15137\n",
      "Gradient Descent(14/49): loss=5555749173335.098\n",
      "Gradient Descent(15/49): loss=85882717415753.98\n",
      "Gradient Descent(16/49): loss=1327605144167207.0\n",
      "Gradient Descent(17/49): loss=2.0522585603850456e+16\n",
      "Gradient Descent(18/49): loss=3.172453208443393e+17\n",
      "Gradient Descent(19/49): loss=4.904089355127411e+18\n",
      "Gradient Descent(20/49): loss=7.580913199646414e+19\n",
      "Gradient Descent(21/49): loss=1.1718841313630416e+21\n",
      "Gradient Descent(22/49): loss=1.8115395614933093e+22\n",
      "Gradient Descent(23/49): loss=2.8003413435067045e+23\n",
      "Gradient Descent(24/49): loss=4.3288657928556106e+24\n",
      "Gradient Descent(25/49): loss=6.691712457128431e+25\n",
      "Gradient Descent(26/49): loss=1.0344283641870372e+27\n",
      "Gradient Descent(27/49): loss=1.5990556191558624e+28\n",
      "Gradient Descent(28/49): loss=2.4718762184788423e+29\n",
      "Gradient Descent(29/49): loss=3.821112890811766e+30\n",
      "Gradient Descent(30/49): loss=5.906810225842542e+31\n",
      "Gradient Descent(31/49): loss=9.130954264140022e+32\n",
      "Gradient Descent(32/49): loss=1.4114949115691423e+34\n",
      "Gradient Descent(33/49): loss=2.181938303217655e+35\n",
      "Gradient Descent(34/49): loss=3.3729166998950327e+36\n",
      "Gradient Descent(35/49): loss=5.213972845911601e+37\n",
      "Gradient Descent(36/49): loss=8.059941960247388e+38\n",
      "Gradient Descent(37/49): loss=1.2459340760375329e+40\n",
      "Gradient Descent(38/49): loss=1.926008561212767e+41\n",
      "Gradient Descent(39/49): loss=2.9772915350885026e+42\n",
      "Gradient Descent(40/49): loss=4.602401600607651e+43\n",
      "Gradient Descent(41/49): loss=7.114553695409881e+44\n",
      "Gradient Descent(42/49): loss=1.0997926447398055e+46\n",
      "Gradient Descent(43/49): loss=1.7000980148679542e+47\n",
      "Gradient Descent(44/49): loss=2.6280710950214817e+48\n",
      "Gradient Descent(45/49): loss=4.0625644051610573e+49\n",
      "Gradient Descent(46/49): loss=6.2800544389178144e+50\n",
      "Gradient Descent(47/49): loss=9.707928249868132e+51\n",
      "Gradient Descent(48/49): loss=1.5006855724140707e+53\n",
      "Gradient Descent(49/49): loss=2.3198123526328807e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5762649071164585\n",
      "Gradient Descent(2/49): loss=0.8796771531179765\n",
      "Gradient Descent(3/49): loss=1.9988886457661046\n",
      "Gradient Descent(4/49): loss=9.912888545463852\n",
      "Gradient Descent(5/49): loss=108.36249862293855\n",
      "Gradient Descent(6/49): loss=1570.0995127184112\n",
      "Gradient Descent(7/49): loss=24019.70059588137\n",
      "Gradient Descent(8/49): loss=370770.3426403004\n",
      "Gradient Descent(9/49): loss=5731593.660302963\n",
      "Gradient Descent(10/49): loss=88623488.13281652\n",
      "Gradient Descent(11/49): loss=1370373956.177546\n",
      "Gradient Descent(12/49): loss=21190055735.639454\n",
      "Gradient Descent(13/49): loss=327661602131.201\n",
      "Gradient Descent(14/49): loss=5066628657371.372\n",
      "Gradient Descent(15/49): loss=78345239397376.7\n",
      "Gradient Descent(16/49): loss=1211451830991724.2\n",
      "Gradient Descent(17/49): loss=1.8732670308808404e+16\n",
      "Gradient Descent(18/49): loss=2.8966313638203494e+17\n",
      "Gradient Descent(19/49): loss=4.4790588419674706e+18\n",
      "Gradient Descent(20/49): loss=6.925965229973923e+19\n",
      "Gradient Descent(21/49): loss=1.0709614689000537e+21\n",
      "Gradient Descent(22/49): loss=1.656026892692061e+22\n",
      "Gradient Descent(23/49): loss=2.5607131058935204e+23\n",
      "Gradient Descent(24/49): loss=3.959628699045648e+24\n",
      "Gradient Descent(25/49): loss=6.122770800922889e+25\n",
      "Gradient Descent(26/49): loss=9.467635763340633e+26\n",
      "Gradient Descent(27/49): loss=1.4639797872847863e+28\n",
      "Gradient Descent(28/49): loss=2.263750815042152e+29\n",
      "Gradient Descent(29/49): loss=3.5004361379253973e+30\n",
      "Gradient Descent(30/49): loss=5.412721698110443e+31\n",
      "Gradient Descent(31/49): loss=8.369687383743814e+32\n",
      "Gradient Descent(32/49): loss=1.2942041140976382e+34\n",
      "Gradient Descent(33/49): loss=2.001226822641571e+35\n",
      "Gradient Descent(34/49): loss=3.094495491116932e+36\n",
      "Gradient Descent(35/49): loss=4.785015989293406e+37\n",
      "Gradient Descent(36/49): loss=7.399066530722023e+38\n",
      "Gradient Descent(37/49): loss=1.1441170865165021e+40\n",
      "Gradient Descent(38/49): loss=1.769147367744071e+41\n",
      "Gradient Descent(39/49): loss=2.7356312091494474e+42\n",
      "Gradient Descent(40/49): loss=4.2301044270920317e+43\n",
      "Gradient Descent(41/49): loss=6.5410072104225105e+44\n",
      "Gradient Descent(42/49): loss=1.0114354400515777e+46\n",
      "Gradient Descent(43/49): loss=1.5639818402313923e+47\n",
      "Gradient Descent(44/49): loss=2.4183839123224918e+48\n",
      "Gradient Descent(45/49): loss=3.739545176889699e+49\n",
      "Gradient Descent(46/49): loss=5.7824558204941595e+50\n",
      "Gradient Descent(47/49): loss=8.941406971790357e+51\n",
      "Gradient Descent(48/49): loss=1.3826090698666468e+53\n",
      "Gradient Descent(49/49): loss=2.1379273375079537e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.570288734028327\n",
      "Gradient Descent(2/49): loss=0.8505592405891982\n",
      "Gradient Descent(3/49): loss=1.7959494044575541\n",
      "Gradient Descent(4/49): loss=7.570679896693912\n",
      "Gradient Descent(5/49): loss=75.26802793146423\n",
      "Gradient Descent(6/49): loss=1081.2414522267038\n",
      "Gradient Descent(7/49): loss=16749.134039952856\n",
      "Gradient Descent(8/49): loss=262693.017933104\n",
      "Gradient Descent(9/49): loss=4128233.3938258765\n",
      "Gradient Descent(10/49): loss=64895885.00736738\n",
      "Gradient Descent(11/49): loss=1020215751.1127758\n",
      "Gradient Descent(12/49): loss=16038745799.738672\n",
      "Gradient Descent(13/49): loss=252144411769.8132\n",
      "Gradient Descent(14/49): loss=3963951944563.9033\n",
      "Gradient Descent(15/49): loss=62317127839539.79\n",
      "Gradient Descent(16/49): loss=979685045938200.4\n",
      "Gradient Descent(17/49): loss=1.5401588990554874e+16\n",
      "Gradient Descent(18/49): loss=2.4212775774213613e+17\n",
      "Gradient Descent(19/49): loss=3.806480688874125e+18\n",
      "Gradient Descent(20/49): loss=5.984152899253317e+19\n",
      "Gradient Descent(21/49): loss=9.407662575651717e+20\n",
      "Gradient Descent(22/49): loss=1.4789748294761415e+22\n",
      "Gradient Descent(23/49): loss=2.325090349100316e+23\n",
      "Gradient Descent(24/49): loss=3.655265136185173e+24\n",
      "Gradient Descent(25/49): loss=5.746427540323521e+25\n",
      "Gradient Descent(26/49): loss=9.033935500135121e+26\n",
      "Gradient Descent(27/49): loss=1.4202213470528957e+28\n",
      "Gradient Descent(28/49): loss=2.2327242369558946e+29\n",
      "Gradient Descent(29/49): loss=3.510056744770649e+30\n",
      "Gradient Descent(30/49): loss=5.518146015339239e+31\n",
      "Gradient Descent(31/49): loss=8.675055037776522e+32\n",
      "Gradient Descent(32/49): loss=1.3638018946808556e+34\n",
      "Gradient Descent(33/49): loss=2.1440274440170013e+35\n",
      "Gradient Descent(34/49): loss=3.370616875241825e+36\n",
      "Gradient Descent(35/49): loss=5.2989331602860075e+37\n",
      "Gradient Descent(36/49): loss=8.330431394747156e+38\n",
      "Gradient Descent(37/49): loss=1.3096237511107732e+40\n",
      "Gradient Descent(38/49): loss=2.0588542035828848e+41\n",
      "Gradient Descent(39/49): loss=3.2367163683582944e+42\n",
      "Gradient Descent(40/49): loss=5.088428714848771e+43\n",
      "Gradient Descent(41/49): loss=7.99949820726228e+44\n",
      "Gradient Descent(42/49): loss=1.2575978785210004e+46\n",
      "Gradient Descent(43/49): loss=1.9770645396541604e+47\n",
      "Gradient Descent(44/49): loss=3.1081351684171254e+48\n",
      "Gradient Descent(45/49): loss=4.886286730346748e+49\n",
      "Gradient Descent(46/49): loss=7.681711610799203e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=1.2076387761898637e+52\n",
      "Gradient Descent(48/49): loss=1.898524036891782e+53\n",
      "Gradient Descent(49/49): loss=2.9846619616073215e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5795406281879135\n",
      "Gradient Descent(2/49): loss=0.9043669747400092\n",
      "Gradient Descent(3/49): loss=2.233045527840924\n",
      "Gradient Descent(4/49): loss=13.21476990078913\n",
      "Gradient Descent(5/49): loss=165.35296436816114\n",
      "Gradient Descent(6/49): loss=2594.223741268148\n",
      "Gradient Descent(7/49): loss=42366.827340964\n",
      "Gradient Descent(8/49): loss=696316.3785786477\n",
      "Gradient Descent(9/49): loss=11455699.077523224\n",
      "Gradient Descent(10/49): loss=188497279.52243507\n",
      "Gradient Descent(11/49): loss=3101697247.6597605\n",
      "Gradient Descent(12/49): loss=51038210173.01805\n",
      "Gradient Descent(13/49): loss=839830685040.1084\n",
      "Gradient Descent(14/49): loss=13819364881137.756\n",
      "Gradient Descent(15/49): loss=227396845708176.72\n",
      "Gradient Descent(16/49): loss=3741801885319965.5\n",
      "Gradient Descent(17/49): loss=6.157113266401846e+16\n",
      "Gradient Descent(18/49): loss=1.0131494114209052e+18\n",
      "Gradient Descent(19/49): loss=1.6671314712917735e+19\n",
      "Gradient Descent(20/49): loss=2.7432551519502094e+20\n",
      "Gradient Descent(21/49): loss=4.5140104174701343e+21\n",
      "Gradient Descent(22/49): loss=7.4277779208923685e+22\n",
      "Gradient Descent(23/49): loss=1.222236542223495e+24\n",
      "Gradient Descent(24/49): loss=2.011183130481862e+25\n",
      "Gradient Descent(25/49): loss=3.30939015861563e+26\n",
      "Gradient Descent(26/49): loss=5.44558228236426e+27\n",
      "Gradient Descent(27/49): loss=8.960674013246232e+28\n",
      "Gradient Descent(28/49): loss=1.4744737037892237e+30\n",
      "Gradient Descent(29/49): loss=2.4262379146390185e+31\n",
      "Gradient Descent(30/49): loss=3.9923603949694715e+32\n",
      "Gradient Descent(31/49): loss=6.56940583903626e+33\n",
      "Gradient Descent(32/49): loss=1.08099191476661e+35\n",
      "Gradient Descent(33/49): loss=1.778765916465606e+36\n",
      "Gradient Descent(34/49): loss=2.9269489830206686e+37\n",
      "Gradient Descent(35/49): loss=4.816277549453125e+38\n",
      "Gradient Descent(36/49): loss=7.925156730756007e+39\n",
      "Gradient Descent(37/49): loss=1.304079936468354e+41\n",
      "Gradient Descent(38/49): loss=2.1458559602985953e+42\n",
      "Gradient Descent(39/49): loss=3.530993517789433e+43\n",
      "Gradient Descent(40/49): loss=5.810229322631629e+44\n",
      "Gradient Descent(41/49): loss=9.560698599838563e+45\n",
      "Gradient Descent(42/49): loss=1.5732074009697458e+47\n",
      "Gradient Descent(43/49): loss=2.5887036398238182e+48\n",
      "Gradient Descent(44/49): loss=4.259696802027622e+49\n",
      "Gradient Descent(45/49): loss=7.009306343942563e+50\n",
      "Gradient Descent(46/49): loss=1.1533772873188266e+52\n",
      "Gradient Descent(47/49): loss=1.8978756265269885e+53\n",
      "Gradient Descent(48/49): loss=3.122943319040358e+54\n",
      "Gradient Descent(49/49): loss=5.138785090878669e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5817320936262976\n",
      "Gradient Descent(2/49): loss=0.917253506785879\n",
      "Gradient Descent(3/49): loss=2.2266921805154296\n",
      "Gradient Descent(4/49): loss=12.016376669389624\n",
      "Gradient Descent(5/49): loss=138.59821738833855\n",
      "Gradient Descent(6/49): loss=2078.9747472660442\n",
      "Gradient Descent(7/49): loss=32815.10004268662\n",
      "Gradient Descent(8/49): loss=522414.0855601724\n",
      "Gradient Descent(9/49): loss=8328555.010038081\n",
      "Gradient Descent(10/49): loss=132808497.76135927\n",
      "Gradient Descent(11/49): loss=2117867871.2810397\n",
      "Gradient Descent(12/49): loss=33773388996.995995\n",
      "Gradient Descent(13/49): loss=538580820424.5018\n",
      "Gradient Descent(14/49): loss=8588695414079.838\n",
      "Gradient Descent(15/49): loss=136963085708696.61\n",
      "Gradient Descent(16/49): loss=2184136942016776.2\n",
      "Gradient Descent(17/49): loss=3.48302183805628e+16\n",
      "Gradient Descent(18/49): loss=5.5543408898111123e+17\n",
      "Gradient Descent(19/49): loss=8.857453141357355e+18\n",
      "Gradient Descent(20/49): loss=1.4124893971765877e+20\n",
      "Gradient Descent(21/49): loss=2.252483039195411e+21\n",
      "Gradient Descent(22/49): loss=3.592012691921848e+22\n",
      "Gradient Descent(23/49): loss=5.72814753958682e+23\n",
      "Gradient Descent(24/49): loss=9.134620907399725e+24\n",
      "Gradient Descent(25/49): loss=1.456689069987206e+26\n",
      "Gradient Descent(26/49): loss=2.3229678255189654e+27\n",
      "Gradient Descent(27/49): loss=3.7044140919130648e+28\n",
      "Gradient Descent(28/49): loss=5.907392953795333e+29\n",
      "Gradient Descent(29/49): loss=9.420461817897829e+30\n",
      "Gradient Descent(30/49): loss=1.5022718406679719e+32\n",
      "Gradient Descent(31/49): loss=2.3956582245005525e+33\n",
      "Gradient Descent(32/49): loss=3.8203327608571033e+34\n",
      "Gradient Descent(33/49): loss=6.092247322433134e+35\n",
      "Gradient Descent(34/49): loss=9.715247273215798e+36\n",
      "Gradient Descent(35/49): loss=1.5492809891707972e+38\n",
      "Gradient Descent(36/49): loss=2.470623254256663e+39\n",
      "Gradient Descent(37/49): loss=3.9398787612702634e+40\n",
      "Gradient Descent(38/49): loss=6.282886161119096e+41\n",
      "Gradient Descent(39/49): loss=1.0019257166394402e+43\n",
      "Gradient Descent(40/49): loss=1.597761149765355e+44\n",
      "Gradient Descent(41/49): loss=2.5479340926212175e+45\n",
      "Gradient Descent(42/49): loss=4.063165599748691e+46\n",
      "Gradient Descent(43/49): loss=6.479490477713476e+47\n",
      "Gradient Descent(44/49): loss=1.0332780148900788e+49\n",
      "Gradient Descent(45/49): loss=1.647758353419099e+50\n",
      "Gradient Descent(46/49): loss=2.6276641447280313e+51\n",
      "Gradient Descent(47/49): loss=4.190310334741845e+52\n",
      "Gradient Descent(48/49): loss=6.6822469441817486e+53\n",
      "Gradient Descent(49/49): loss=1.0656113904693858e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5843248944665831\n",
      "Gradient Descent(2/49): loss=0.9217462762413532\n",
      "Gradient Descent(3/49): loss=2.195873538745867\n",
      "Gradient Descent(4/49): loss=11.308906555947814\n",
      "Gradient Descent(5/49): loss=126.96876439016586\n",
      "Gradient Descent(6/49): loss=1893.6280488920027\n",
      "Gradient Descent(7/49): loss=29868.633909718003\n",
      "Gradient Descent(8/49): loss=475574.22338529286\n",
      "Gradient Descent(9/49): loss=7583897.204780709\n",
      "Gradient Descent(10/49): loss=120969802.60738356\n",
      "Gradient Descent(11/49): loss=1929655115.567946\n",
      "Gradient Descent(12/49): loss=30781190828.959797\n",
      "Gradient Descent(13/49): loss=491011466456.38904\n",
      "Gradient Descent(14/49): loss=7832455437867.701\n",
      "Gradient Descent(15/49): loss=124940789123673.25\n",
      "Gradient Descent(16/49): loss=1993014960698629.0\n",
      "Gradient Descent(17/49): loss=3.17919285189406e+16\n",
      "Gradient Descent(18/49): loss=5.0713453688266515e+17\n",
      "Gradient Descent(19/49): loss=8.089645720984784e+18\n",
      "Gradient Descent(20/49): loss=1.290434059048026e+20\n",
      "Gradient Descent(21/49): loss=2.0584585755499288e+21\n",
      "Gradient Descent(22/49): loss=3.2835863851745352e+22\n",
      "Gradient Descent(23/49): loss=5.237870548851362e+23\n",
      "Gradient Descent(24/49): loss=8.35528128950598e+24\n",
      "Gradient Descent(25/49): loss=1.3328073837578201e+26\n",
      "Gradient Descent(26/49): loss=2.1260511293981833e+27\n",
      "Gradient Descent(27/49): loss=3.39140783574522e+28\n",
      "Gradient Descent(28/49): loss=5.4098638312662144e+29\n",
      "Gradient Descent(29/49): loss=8.629639397649095e+30\n",
      "Gradient Descent(30/49): loss=1.3765720997089406e+32\n",
      "Gradient Descent(31/49): loss=2.1958631854459044e+33\n",
      "Gradient Descent(32/49): loss=3.502769764268927e+34\n",
      "Gradient Descent(33/49): loss=5.587504769330426e+35\n",
      "Gradient Descent(34/49): loss=8.913006462988776e+36\n",
      "Gradient Descent(35/49): loss=1.4217738953053227e+38\n",
      "Gradient Descent(36/49): loss=2.2679676243540385e+39\n",
      "Gradient Descent(37/49): loss=3.6177884276131782e+40\n",
      "Gradient Descent(38/49): loss=5.770978812230426e+41\n",
      "Gradient Descent(39/49): loss=9.20567830805569e+42\n",
      "Gradient Descent(40/49): loss=1.4684599591980097e+44\n",
      "Gradient Descent(41/49): loss=2.342439719928957e+45\n",
      "Gradient Descent(42/49): loss=3.73658390011375e+46\n",
      "Gradient Descent(43/49): loss=5.960477498653549e+47\n",
      "Gradient Descent(44/49): loss=9.507960469153228e+48\n",
      "Gradient Descent(45/49): loss=1.5166790295475648e+50\n",
      "Gradient Descent(46/49): loss=2.419357217704345e+51\n",
      "Gradient Descent(47/49): loss=3.8592801989253486e+52\n",
      "Gradient Descent(48/49): loss=6.156198656744759e+53\n",
      "Gradient Descent(49/49): loss=9.820168515325155e+54\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5780034649016041\n",
      "Gradient Descent(2/49): loss=0.8901685861886994\n",
      "Gradient Descent(3/49): loss=1.9710097474956871\n",
      "Gradient Descent(4/49): loss=8.648392930401346\n",
      "Gradient Descent(5/49): loss=88.26027469012439\n",
      "Gradient Descent(6/49): loss=1303.961084625316\n",
      "Gradient Descent(7/49): loss=20821.121682556997\n",
      "Gradient Descent(8/49): loss=336811.0760427456\n",
      "Gradient Descent(9/49): loss=5459845.495259749\n",
      "Gradient Descent(10/49): loss=88536313.71255861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=1435774652.9137464\n",
      "Gradient Descent(12/49): loss=23283859531.438393\n",
      "Gradient Descent(13/49): loss=377593300852.1661\n",
      "Gradient Descent(14/49): loss=6123414963106.108\n",
      "Gradient Descent(15/49): loss=99303171242765.6\n",
      "Gradient Descent(16/49): loss=1610395496308971.0\n",
      "Gradient Descent(17/49): loss=2.6115718406760004e+16\n",
      "Gradient Descent(18/49): loss=4.235175455796844e+17\n",
      "Gradient Descent(19/49): loss=6.868166849739486e+18\n",
      "Gradient Descent(20/49): loss=1.1138078308300231e+20\n",
      "Gradient Descent(21/49): loss=1.806257639279905e+21\n",
      "Gradient Descent(22/49): loss=2.9292006835919526e+22\n",
      "Gradient Descent(23/49): loss=4.750272861503948e+23\n",
      "Gradient Descent(24/49): loss=7.703498222276532e+24\n",
      "Gradient Descent(25/49): loss=1.2492731805268995e+26\n",
      "Gradient Descent(26/49): loss=2.0259412471475893e+27\n",
      "Gradient Descent(27/49): loss=3.2854606989664375e+28\n",
      "Gradient Descent(28/49): loss=5.3280182826874485e+29\n",
      "Gradient Descent(29/49): loss=8.640425627243806e+30\n",
      "Gradient Descent(30/49): loss=1.4012143175731015e+32\n",
      "Gradient Descent(31/49): loss=2.2723435724985503e+33\n",
      "Gradient Descent(32/49): loss=3.6850503500554445e+34\n",
      "Gradient Descent(33/49): loss=5.976031198271804e+35\n",
      "Gradient Descent(34/49): loss=9.691305542726054e+36\n",
      "Gradient Descent(35/49): loss=1.5716350870061742e+38\n",
      "Gradient Descent(36/49): loss=2.5487142426984737e+39\n",
      "Gradient Descent(37/49): loss=4.133239544370431e+40\n",
      "Gradient Descent(38/49): loss=6.702857795882266e+41\n",
      "Gradient Descent(39/49): loss=1.086999728651434e+43\n",
      "Gradient Descent(40/49): loss=1.7627830487678555e+44\n",
      "Gradient Descent(41/49): loss=2.858698116584134e+45\n",
      "Gradient Descent(42/49): loss=4.635939134696005e+46\n",
      "Gradient Descent(43/49): loss=7.51808368149304e+47\n",
      "Gradient Descent(44/49): loss=1.2192045796916847e+49\n",
      "Gradient Descent(45/49): loss=1.9771791191953494e+50\n",
      "Gradient Descent(46/49): loss=3.206383353949358e+51\n",
      "Gradient Descent(47/49): loss=5.199778873179314e+52\n",
      "Gradient Descent(48/49): loss=8.432460297256348e+53\n",
      "Gradient Descent(49/49): loss=1.3674886644041018e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5875095059845457\n",
      "Gradient Descent(2/49): loss=0.9474012008921743\n",
      "Gradient Descent(3/49): loss=2.450848464192018\n",
      "Gradient Descent(4/49): loss=15.024509901179139\n",
      "Gradient Descent(5/49): loss=193.04769189650176\n",
      "Gradient Descent(6/49): loss=3117.4551823733204\n",
      "Gradient Descent(7/49): loss=52471.10073351245\n",
      "Gradient Descent(8/49): loss=889065.962011098\n",
      "Gradient Descent(9/49): loss=15080234.447253015\n",
      "Gradient Descent(10/49): loss=255832398.29946068\n",
      "Gradient Descent(11/49): loss=4340249483.7473\n",
      "Gradient Descent(12/49): loss=73633545916.31004\n",
      "Gradient Descent(13/49): loss=1249214551288.805\n",
      "Gradient Descent(14/49): loss=21193291007657.574\n",
      "Gradient Descent(15/49): loss=359550400011297.1\n",
      "Gradient Descent(16/49): loss=6099878044611206.0\n",
      "Gradient Descent(17/49): loss=1.034862210544724e+17\n",
      "Gradient Descent(18/49): loss=1.755674108659044e+18\n",
      "Gradient Descent(19/49): loss=2.978552646374489e+19\n",
      "Gradient Descent(20/49): loss=5.053201971530287e+20\n",
      "Gradient Descent(21/49): loss=8.572905433169501e+21\n",
      "Gradient Descent(22/49): loss=1.4544185643110877e+23\n",
      "Gradient Descent(23/49): loss=2.467463774916184e+24\n",
      "Gradient Descent(24/49): loss=4.186124702971936e+25\n",
      "Gradient Descent(25/49): loss=7.101883402291115e+26\n",
      "Gradient Descent(26/49): loss=1.2048553599927176e+28\n",
      "Gradient Descent(27/49): loss=2.0440724752463828e+29\n",
      "Gradient Descent(28/49): loss=3.467828938475232e+30\n",
      "Gradient Descent(29/49): loss=5.8832735591124185e+31\n",
      "Gradient Descent(30/49): loss=9.981146240324489e+32\n",
      "Gradient Descent(31/49): loss=1.693330749790497e+34\n",
      "Gradient Descent(32/49): loss=2.872785308566784e+35\n",
      "Gradient Descent(33/49): loss=4.873764579151656e+36\n",
      "Gradient Descent(34/49): loss=8.268484631329231e+37\n",
      "Gradient Descent(35/49): loss=1.4027726819425038e+39\n",
      "Gradient Descent(36/49): loss=2.379845019906346e+40\n",
      "Gradient Descent(37/49): loss=4.037476913886282e+41\n",
      "Gradient Descent(38/49): loss=6.84969806597162e+42\n",
      "Gradient Descent(39/49): loss=1.162071377637009e+44\n",
      "Gradient Descent(40/49): loss=1.971488193664811e+45\n",
      "Gradient Descent(41/49): loss=3.344687574753975e+46\n",
      "Gradient Descent(42/49): loss=5.6743606219209944e+47\n",
      "Gradient Descent(43/49): loss=9.626719311735995e+48\n",
      "Gradient Descent(44/49): loss=1.6332011812738038e+50\n",
      "Gradient Descent(45/49): loss=2.770773731049108e+51\n",
      "Gradient Descent(46/49): loss=4.700698944317451e+52\n",
      "Gradient Descent(47/49): loss=7.974873703144604e+53\n",
      "Gradient Descent(48/49): loss=1.3529607263615816e+55\n",
      "Gradient Descent(49/49): loss=2.2953375755092793e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5897855589927543\n",
      "Gradient Descent(2/49): loss=0.961349448195363\n",
      "Gradient Descent(3/49): loss=2.445582179057644\n",
      "Gradient Descent(4/49): loss=13.681398018354383\n",
      "Gradient Descent(5/49): loss=161.98215570553197\n",
      "Gradient Descent(6/49): loss=2500.105851671446\n",
      "Gradient Descent(7/49): loss=40669.7869714231\n",
      "Gradient Descent(8/49): loss=667536.180969514\n",
      "Gradient Descent(9/49): loss=10973044.887782738\n",
      "Gradient Descent(10/49): loss=180421340.32432696\n",
      "Gradient Descent(11/49): loss=2966653114.5475345\n",
      "Gradient Descent(12/49): loss=48780770961.27233\n",
      "Gradient Descent(13/49): loss=802104694751.3812\n",
      "Gradient Descent(14/49): loss=13189050811794.518\n",
      "Gradient Descent(15/49): loss=216868281821727.12\n",
      "Gradient Descent(16/49): loss=3565977005330119.0\n",
      "Gradient Descent(17/49): loss=5.863555475855427e+16\n",
      "Gradient Descent(18/49): loss=9.641476311342926e+17\n",
      "Gradient Descent(19/49): loss=1.5853532186629337e+19\n",
      "Gradient Descent(20/49): loss=2.6068049609567968e+20\n",
      "Gradient Descent(21/49): loss=4.2863836427601455e+21\n",
      "Gradient Descent(22/49): loss=7.048124047677229e+22\n",
      "Gradient Descent(23/49): loss=1.158926888762081e+24\n",
      "Gradient Descent(24/49): loss=1.9056298164025432e+25\n",
      "Gradient Descent(25/49): loss=3.1334375208442386e+26\n",
      "Gradient Descent(26/49): loss=5.152328438883303e+27\n",
      "Gradient Descent(27/49): loss=8.472001808088585e+28\n",
      "Gradient Descent(28/49): loss=1.3930558870158403e+30\n",
      "Gradient Descent(29/49): loss=2.2906094076806504e+31\n",
      "Gradient Descent(30/49): loss=3.7664615665886006e+32\n",
      "Gradient Descent(31/49): loss=6.193213336599962e+33\n",
      "Gradient Descent(32/49): loss=1.0183534533548674e+35\n",
      "Gradient Descent(33/49): loss=1.6744841483679992e+36\n",
      "Gradient Descent(34/49): loss=2.7533634357486823e+37\n",
      "Gradient Descent(35/49): loss=4.52737054376202e+38\n",
      "Gradient Descent(36/49): loss=7.444380125920715e+39\n",
      "Gradient Descent(37/49): loss=1.2240834922505004e+41\n",
      "Gradient Descent(38/49): loss=2.0127671755811398e+42\n",
      "Gradient Descent(39/49): loss=3.3096040660172364e+43\n",
      "Gradient Descent(40/49): loss=5.442000051812081e+44\n",
      "Gradient Descent(41/49): loss=8.948310424201767e+45\n",
      "Gradient Descent(42/49): loss=1.4713755730526996e+47\n",
      "Gradient Descent(43/49): loss=2.4193908954262387e+48\n",
      "Gradient Descent(44/49): loss=3.9782176706434486e+49\n",
      "Gradient Descent(45/49): loss=6.541405055685061e+50\n",
      "Gradient Descent(46/49): loss=1.0756068080010453e+52\n",
      "Gradient Descent(47/49): loss=1.768626152286249e+53\n",
      "Gradient Descent(48/49): loss=2.9081616472511295e+54\n",
      "Gradient Descent(49/49): loss=4.7819060888644894e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5925603552984651\n",
      "Gradient Descent(2/49): loss=0.9664094645874521\n",
      "Gradient Descent(3/49): loss=2.412542943342783\n",
      "Gradient Descent(4/49): loss=12.883457577764823\n",
      "Gradient Descent(5/49): loss=148.43038126836635\n",
      "Gradient Descent(6/49): loss=2277.3451994653524\n",
      "Gradient Descent(7/49): loss=37018.26318595145\n",
      "Gradient Descent(8/49): loss=607681.5292381567\n",
      "Gradient Descent(9/49): loss=9991864.687909191\n",
      "Gradient Descent(10/49): loss=164336898.01839122\n",
      "Gradient Descent(11/49): loss=2702982621.203766\n",
      "Gradient Descent(12/49): loss=44458487880.46725\n",
      "Gradient Descent(13/49): loss=731251323123.1006\n",
      "Gradient Descent(14/49): loss=12027593254738.7\n",
      "Gradient Descent(15/49): loss=197829391814159.28\n",
      "Gradient Descent(16/49): loss=3253890255723590.5\n",
      "Gradient Descent(17/49): loss=5.351986228825165e+16\n",
      "Gradient Descent(18/49): loss=8.802926450186938e+17\n",
      "Gradient Descent(19/49): loss=1.4479019709000903e+19\n",
      "Gradient Descent(20/49): loss=2.3815036161083897e+20\n",
      "Gradient Descent(21/49): loss=3.9170880263501906e+21\n",
      "Gradient Descent(22/49): loss=6.442811382857026e+22\n",
      "Gradient Descent(23/49): loss=1.0597111485837777e+24\n",
      "Gradient Descent(24/49): loss=1.7430088383787746e+25\n",
      "Gradient Descent(25/49): loss=2.8668942614472757e+26\n",
      "Gradient Descent(26/49): loss=4.715456700703924e+27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=7.755965120593002e+28\n",
      "Gradient Descent(28/49): loss=1.2756981724140738e+30\n",
      "Gradient Descent(29/49): loss=2.098263467920498e+31\n",
      "Gradient Descent(30/49): loss=3.451215715452706e+32\n",
      "Gradient Descent(31/49): loss=5.676546390235761e+33\n",
      "Gradient Descent(32/49): loss=9.336761760854263e+34\n",
      "Gradient Descent(33/49): loss=1.5357069983415087e+36\n",
      "Gradient Descent(34/49): loss=2.525924988943189e+37\n",
      "Gradient Descent(35/49): loss=4.154631747239518e+38\n",
      "Gradient Descent(36/49): loss=6.833522385156876e+39\n",
      "Gradient Descent(37/49): loss=1.1239751445953702e+41\n",
      "Gradient Descent(38/49): loss=1.848710012880363e+42\n",
      "Gradient Descent(39/49): loss=3.0407511484202144e+43\n",
      "Gradient Descent(40/49): loss=5.001415842505833e+44\n",
      "Gradient Descent(41/49): loss=8.226309621773461e+45\n",
      "Gradient Descent(42/49): loss=1.353060255821046e+47\n",
      "Gradient Descent(43/49): loss=2.2255083264028974e+48\n",
      "Gradient Descent(44/49): loss=3.660507571322616e+49\n",
      "Gradient Descent(45/49): loss=6.020788833159498e+50\n",
      "Gradient Descent(46/49): loss=9.90297041248856e+51\n",
      "Gradient Descent(47/49): loss=1.6288367804980914e+53\n",
      "Gradient Descent(48/49): loss=2.6791044979368582e+54\n",
      "Gradient Descent(49/49): loss=4.4065808169376987e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5858869889032251\n",
      "Gradient Descent(2/49): loss=0.9322308127089725\n",
      "Gradient Descent(3/49): loss=2.1638140395898184\n",
      "Gradient Descent(4/49): loss=9.867261178339099\n",
      "Gradient Descent(5/49): loss=103.26396775338469\n",
      "Gradient Descent(6/49): loss=1568.126116457506\n",
      "Gradient Descent(7/49): loss=25797.303498808084\n",
      "Gradient Descent(8/49): loss=430203.7685848557\n",
      "Gradient Descent(9/49): loss=7190178.657869887\n",
      "Gradient Descent(10/49): loss=120216063.55600852\n",
      "Gradient Descent(11/49): loss=2010069087.8806694\n",
      "Gradient Descent(12/49): loss=33609623972.764084\n",
      "Gradient Descent(13/49): loss=561975012644.5975\n",
      "Gradient Descent(14/49): loss=9396594148453.484\n",
      "Gradient Descent(15/49): loss=157117279783046.8\n",
      "Gradient Descent(16/49): loss=2627105032459111.0\n",
      "Gradient Descent(17/49): loss=4.39269370550002e+16\n",
      "Gradient Descent(18/49): loss=7.344874969005981e+17\n",
      "Gradient Descent(19/49): loss=1.2281117676015354e+19\n",
      "Gradient Descent(20/49): loss=2.053484259559531e+20\n",
      "Gradient Descent(21/49): loss=3.4335617616444313e+21\n",
      "Gradient Descent(22/49): loss=5.741142799681008e+22\n",
      "Gradient Descent(23/49): loss=9.599571213346548e+23\n",
      "Gradient Descent(24/49): loss=1.6051119210139254e+25\n",
      "Gradient Descent(25/49): loss=2.6838534989969252e+26\n",
      "Gradient Descent(26/49): loss=4.487580903098471e+27\n",
      "Gradient Descent(27/49): loss=7.503532651607557e+28\n",
      "Gradient Descent(28/49): loss=1.2546403835274748e+30\n",
      "Gradient Descent(29/49): loss=2.097841863379822e+31\n",
      "Gradient Descent(30/49): loss=3.507730614708407e+32\n",
      "Gradient Descent(31/49): loss=5.86515803700263e+33\n",
      "Gradient Descent(32/49): loss=9.80693290835158e+34\n",
      "Gradient Descent(33/49): loss=1.6397841705568704e+36\n",
      "Gradient Descent(34/49): loss=2.741827797882655e+37\n",
      "Gradient Descent(35/49): loss=4.584517772658474e+38\n",
      "Gradient Descent(36/49): loss=7.665617521294402e+39\n",
      "Gradient Descent(37/49): loss=1.2817420478381207e+41\n",
      "Gradient Descent(38/49): loss=2.1431576420719346e+42\n",
      "Gradient Descent(39/49): loss=3.583501599653755e+43\n",
      "Gradient Descent(40/49): loss=5.991852144999655e+44\n",
      "Gradient Descent(41/49): loss=1.0018773852648684e+46\n",
      "Gradient Descent(42/49): loss=1.6752053802643474e+47\n",
      "Gradient Descent(43/49): loss=2.8010544077953125e+48\n",
      "Gradient Descent(44/49): loss=4.683548589243046e+49\n",
      "Gradient Descent(45/49): loss=7.831203609167153e+50\n",
      "Gradient Descent(46/49): loss=1.3094291390311972e+52\n",
      "Gradient Descent(47/49): loss=2.189452293306423e+53\n",
      "Gradient Descent(48/49): loss=3.660909324357447e+54\n",
      "Gradient Descent(49/49): loss=6.121282990335335e+55\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5956487958478263\n",
      "Gradient Descent(2/49): loss=0.9930360642446359\n",
      "Gradient Descent(3/49): loss=2.689767861187353\n",
      "Gradient Descent(4/49): loss=17.057051390024327\n",
      "Gradient Descent(5/49): loss=224.87880117269972\n",
      "Gradient Descent(6/49): loss=3735.953608999946\n",
      "Gradient Descent(7/49): loss=64776.70960666192\n",
      "Gradient Descent(8/49): loss=1131007.0827971888\n",
      "Gradient Descent(9/49): loss=19769641.81898162\n",
      "Gradient Descent(10/49): loss=345629355.71468925\n",
      "Gradient Descent(11/49): loss=6042756004.979523\n",
      "Gradient Descent(12/49): loss=105648060372.70341\n",
      "Gradient Descent(13/49): loss=1847091140174.6292\n",
      "Gradient Descent(14/49): loss=32293504327450.895\n",
      "Gradient Descent(15/49): loss=564601507470541.5\n",
      "Gradient Descent(16/49): loss=9871175949571842.0\n",
      "Gradient Descent(17/49): loss=1.7258210151356928e+17\n",
      "Gradient Descent(18/49): loss=3.0173286260361283e+18\n",
      "Gradient Descent(19/49): loss=5.275328065756352e+19\n",
      "Gradient Descent(20/49): loss=9.223087588565181e+20\n",
      "Gradient Descent(21/49): loss=1.6125128827257926e+22\n",
      "Gradient Descent(22/49): loss=2.819227045160666e+23\n",
      "Gradient Descent(23/49): loss=4.928978377357195e+24\n",
      "Gradient Descent(24/49): loss=8.61754922724585e+25\n",
      "Gradient Descent(25/49): loss=1.5066439533423517e+27\n",
      "Gradient Descent(26/49): loss=2.6341317493912836e+28\n",
      "Gradient Descent(27/49): loss=4.605368148034271e+29\n",
      "Gradient Descent(28/49): loss=8.051767260247958e+30\n",
      "Gradient Descent(29/49): loss=1.4077258088666234e+32\n",
      "Gradient Descent(30/49): loss=2.4611888159422065e+33\n",
      "Gradient Descent(31/49): loss=4.303004427116424e+34\n",
      "Gradient Descent(32/49): loss=7.523131496392278e+35\n",
      "Gradient Descent(33/49): loss=1.3153020981188407e+37\n",
      "Gradient Descent(34/49): loss=2.299600386016764e+38\n",
      "Gradient Descent(35/49): loss=4.020492283051709e+39\n",
      "Gradient Descent(36/49): loss=7.029203115623622e+40\n",
      "Gradient Descent(37/49): loss=1.2289464314849654e+42\n",
      "Gradient Descent(38/49): loss=2.14862098393865e+43\n",
      "Gradient Descent(39/49): loss=3.756528367996674e+44\n",
      "Gradient Descent(40/49): loss=6.567703417703759e+45\n",
      "Gradient Descent(41/49): loss=1.148260413801175e+47\n",
      "Gradient Descent(42/49): loss=2.0075540779578192e+48\n",
      "Gradient Descent(43/49): loss=3.509894904922559e+49\n",
      "Gradient Descent(44/49): loss=6.136503309606079e+50\n",
      "Gradient Descent(45/49): loss=1.0728718063892457e+52\n",
      "Gradient Descent(46/49): loss=1.875748866855575e+53\n",
      "Gradient Descent(47/49): loss=3.27945406949525e+54\n",
      "Gradient Descent(48/49): loss=5.73361348310895e+55\n",
      "Gradient Descent(49/49): loss=1.002432809761784e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5980108396561267\n",
      "Gradient Descent(2/49): loss=1.0081050185668878\n",
      "Gradient Descent(3/49): loss=2.68582073401061\n",
      "Gradient Descent(4/49): loss=15.554419037092567\n",
      "Gradient Descent(5/49): loss=188.88869839155666\n",
      "Gradient Descent(6/49): loss=2998.265190133258\n",
      "Gradient Descent(7/49): loss=50241.93236734098\n",
      "Gradient Descent(8/49): loss=849823.974777397\n",
      "Gradient Descent(9/49): loss=14397192.762580877\n",
      "Gradient Descent(10/49): loss=243973284.2552253\n",
      "Gradient Descent(11/49): loss=4134530654.6860805\n",
      "Gradient Descent(12/49): loss=70066987765.0196\n",
      "Gradient Descent(13/49): loss=1187411450704.365\n",
      "Gradient Descent(14/49): loss=20122832476772.89\n",
      "Gradient Descent(15/49): loss=341017767144991.9\n",
      "Gradient Descent(16/49): loss=5779162469083262.0\n",
      "Gradient Descent(17/49): loss=9.793835423360272e+16\n",
      "Gradient Descent(18/49): loss=1.6597424424341368e+18\n",
      "Gradient Descent(19/49): loss=2.812733578029841e+19\n",
      "Gradient Descent(20/49): loss=4.766685468025373e+20\n",
      "Gradient Descent(21/49): loss=8.078010135257263e+21\n",
      "Gradient Descent(22/49): loss=1.3689648327553859e+23\n",
      "Gradient Descent(23/49): loss=2.3199583584841856e+24\n",
      "Gradient Descent(24/49): loss=3.931588786154268e+25\n",
      "Gradient Descent(25/49): loss=6.662787858620922e+26\n",
      "Gradient Descent(26/49): loss=1.1291298369076752e+28\n",
      "Gradient Descent(27/49): loss=1.913514606270306e+29\n",
      "Gradient Descent(28/49): loss=3.2427963806514876e+30\n",
      "Gradient Descent(29/49): loss=5.495504623747248e+31\n",
      "Gradient Descent(30/49): loss=9.31312593347601e+32\n",
      "Gradient Descent(31/49): loss=1.578277530292469e+34\n",
      "Gradient Descent(32/49): loss=2.6746765591050064e+35\n",
      "Gradient Descent(33/49): loss=4.532722894749583e+36\n",
      "Gradient Descent(34/49): loss=7.681518264571988e+37\n",
      "Gradient Descent(35/49): loss=1.3017721184169853e+39\n",
      "Gradient Descent(36/49): loss=2.206088158513599e+40\n",
      "Gradient Descent(37/49): loss=3.7386151495180154e+41\n",
      "Gradient Descent(38/49): loss=6.335759150088975e+42\n",
      "Gradient Descent(39/49): loss=1.0737089109883777e+44\n",
      "Gradient Descent(40/49): loss=1.8195938296038945e+45\n",
      "Gradient Descent(41/49): loss=3.083630647793434e+46\n",
      "Gradient Descent(42/49): loss=5.2257695191683905e+47\n",
      "Gradient Descent(43/49): loss=8.856011042376297e+48\n",
      "Gradient Descent(44/49): loss=1.5008111493438574e+50\n",
      "Gradient Descent(45/49): loss=2.543395774030621e+51\n",
      "Gradient Descent(46/49): loss=4.3102438745773217e+52\n",
      "Gradient Descent(47/49): loss=7.3044873503465255e+53\n",
      "Gradient Descent(48/49): loss=1.237877414920179e+55\n",
      "Gradient Descent(49/49): loss=2.0978070340511378e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6009712896121041\n",
      "Gradient Descent(2/49): loss=1.013771112391282\n",
      "Gradient Descent(3/49): loss=2.6504535871612993\n",
      "Gradient Descent(4/49): loss=14.656106816401472\n",
      "Gradient Descent(5/49): loss=173.1334227483587\n",
      "Gradient Descent(6/49): loss=2731.280646338272\n",
      "Gradient Descent(7/49): loss=45731.26819385259\n",
      "Gradient Descent(8/49): loss=773620.7960295185\n",
      "Gradient Descent(9/49): loss=13109735.571557678\n",
      "Gradient Descent(10/49): loss=222221280.40168613\n",
      "Gradient Descent(11/49): loss=3767024846.206122\n",
      "Gradient Descent(12/49): loss=63857934845.23902\n",
      "Gradient Descent(13/49): loss=1082509831441.9442\n",
      "Gradient Descent(14/49): loss=18350543399421.24\n",
      "Gradient Descent(15/49): loss=311075656109492.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=5273307844088168.0\n",
      "Gradient Descent(17/49): loss=8.93923233871014e+16\n",
      "Gradient Descent(18/49): loss=1.5153652542707438e+18\n",
      "Gradient Descent(19/49): loss=2.568824443657777e+19\n",
      "Gradient Descent(20/49): loss=4.354632656232781e+20\n",
      "Gradient Descent(21/49): loss=7.381907945310522e+21\n",
      "Gradient Descent(22/49): loss=1.2513699596463541e+23\n",
      "Gradient Descent(23/49): loss=2.1213035810072628e+24\n",
      "Gradient Descent(24/49): loss=3.5960020041283497e+25\n",
      "Gradient Descent(25/49): loss=6.095888645770896e+26\n",
      "Gradient Descent(26/49): loss=1.0333658974320987e+28\n",
      "Gradient Descent(27/49): loss=1.7517463655056768e+29\n",
      "Gradient Descent(28/49): loss=2.96953415696019e+30\n",
      "Gradient Descent(29/49): loss=5.03390975028944e+31\n",
      "Gradient Descent(30/49): loss=8.53340828380946e+32\n",
      "Gradient Descent(31/49): loss=1.446570569406846e+34\n",
      "Gradient Descent(32/49): loss=2.452204726034622e+35\n",
      "Gradient Descent(33/49): loss=4.1569406605942786e+36\n",
      "Gradient Descent(34/49): loss=7.046783440322895e+37\n",
      "Gradient Descent(35/49): loss=1.1945601563556957e+39\n",
      "Gradient Descent(36/49): loss=2.0250004547992282e+40\n",
      "Gradient Descent(37/49): loss=3.4327503894379955e+41\n",
      "Gradient Descent(38/49): loss=5.819146957848343e+42\n",
      "Gradient Descent(39/49): loss=9.86453061697273e+43\n",
      "Gradient Descent(40/49): loss=1.6722204302118813e+45\n",
      "Gradient Descent(41/49): loss=2.8347229845956516e+46\n",
      "Gradient Descent(42/49): loss=4.805379873499577e+47\n",
      "Gradient Descent(43/49): loss=8.146007865360497e+48\n",
      "Gradient Descent(44/49): loss=1.380899031696919e+50\n",
      "Gradient Descent(45/49): loss=2.3408793205935513e+51\n",
      "Gradient Descent(46/49): loss=3.9682235035307986e+52\n",
      "Gradient Descent(47/49): loss=6.726872947034926e+53\n",
      "Gradient Descent(48/49): loss=1.1403294095024328e+55\n",
      "Gradient Descent(49/49): loss=1.9330693063697076e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5939393060331898\n",
      "Gradient Descent(2/49): loss=0.9768449683801983\n",
      "Gradient Descent(3/49): loss=2.375782008947121\n",
      "Gradient Descent(4/49): loss=11.243125825506304\n",
      "Gradient Descent(5/49): loss=120.55408802743595\n",
      "Gradient Descent(6/49): loss=1880.6490787735033\n",
      "Gradient Descent(7/49): loss=31860.03825511501\n",
      "Gradient Descent(8/49): loss=547471.8331530686\n",
      "Gradient Descent(9/49): loss=9429706.436690804\n",
      "Gradient Descent(10/49): loss=162480977.2979876\n",
      "Gradient Descent(11/49): loss=2799848356.7556705\n",
      "Gradient Descent(12/49): loss=48247081411.0936\n",
      "Gradient Descent(13/49): loss=831396769657.0946\n",
      "Gradient Descent(14/49): loss=14326685981097.832\n",
      "Gradient Descent(15/49): loss=246878443950062.12\n",
      "Gradient Descent(16/49): loss=4254226458013790.5\n",
      "Gradient Descent(17/49): loss=7.33091252902929e+16\n",
      "Gradient Descent(18/49): loss=1.2632679301371077e+18\n",
      "Gradient Descent(19/49): loss=2.176872056580226e+19\n",
      "Gradient Descent(20/49): loss=3.751201022123848e+20\n",
      "Gradient Descent(21/49): loss=6.464095611802671e+21\n",
      "Gradient Descent(22/49): loss=1.1138974379697523e+23\n",
      "Gradient Descent(23/49): loss=1.9194757887710286e+24\n",
      "Gradient Descent(24/49): loss=3.3076539886774123e+25\n",
      "Gradient Descent(25/49): loss=5.699772288255003e+26\n",
      "Gradient Descent(26/49): loss=9.821887128813669e+27\n",
      "Gradient Descent(27/49): loss=1.692514400442658e+29\n",
      "Gradient Descent(28/49): loss=2.916552550580647e+30\n",
      "Gradient Descent(29/49): loss=5.025823578265578e+31\n",
      "Gradient Descent(30/49): loss=8.6605340386618e+32\n",
      "Gradient Descent(31/49): loss=1.4923892306761729e+34\n",
      "Gradient Descent(32/49): loss=2.5716954703897174e+35\n",
      "Gradient Descent(33/49): loss=4.4315634664734587e+36\n",
      "Gradient Descent(34/49): loss=7.636500893477077e+37\n",
      "Gradient Descent(35/49): loss=1.3159271290428985e+39\n",
      "Gradient Descent(36/49): loss=2.267614753283422e+40\n",
      "Gradient Descent(37/49): loss=3.9075694662884295e+41\n",
      "Gradient Descent(38/49): loss=6.733550799032629e+42\n",
      "Gradient Descent(39/49): loss=1.1603301426709748e+44\n",
      "Gradient Descent(40/49): loss=1.9994889474722005e+45\n",
      "Gradient Descent(41/49): loss=3.445533218555129e+46\n",
      "Gradient Descent(42/49): loss=5.9373667332221126e+47\n",
      "Gradient Descent(43/49): loss=1.0231311523838937e+49\n",
      "Gradient Descent(44/49): loss=1.7630666960844693e+50\n",
      "Gradient Descent(45/49): loss=3.038128755633794e+51\n",
      "Gradient Descent(46/49): loss=5.235324537811271e+52\n",
      "Gradient Descent(47/49): loss=9.02154754481152e+53\n",
      "Gradient Descent(48/49): loss=1.5545993283793052e+55\n",
      "Gradient Descent(49/49): loss=2.6788963421107904e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6039584977777552\n",
      "Gradient Descent(2/49): loss=1.0413742502879362\n",
      "Gradient Descent(3/49): loss=2.951427982810432\n",
      "Gradient Descent(4/49): loss=19.33589671772019\n",
      "Gradient Descent(5/49): loss=261.38989637132704\n",
      "Gradient Descent(6/49): loss=4465.253730052936\n",
      "Gradient Descent(7/49): loss=79719.28707701183\n",
      "Gradient Descent(8/49): loss=1433665.8529821604\n",
      "Gradient Descent(9/49): loss=25813503.20067605\n",
      "Gradient Descent(10/49): loss=464867803.57412094\n",
      "Gradient Descent(11/49): loss=8371929358.971042\n",
      "Gradient Descent(12/49): loss=150773093830.25204\n",
      "Gradient Descent(13/49): loss=2715329239203.7695\n",
      "Gradient Descent(14/49): loss=48901390019853.46\n",
      "Gradient Descent(15/49): loss=880683625198870.0\n",
      "Gradient Descent(16/49): loss=1.5860564497777972e+16\n",
      "Gradient Descent(17/49): loss=2.8563890497702957e+17\n",
      "Gradient Descent(18/49): loss=5.144179077329947e+18\n",
      "Gradient Descent(19/49): loss=9.264346669502084e+19\n",
      "Gradient Descent(20/49): loss=1.668451232406845e+21\n",
      "Gradient Descent(21/49): loss=3.0047769305578704e+22\n",
      "Gradient Descent(22/49): loss=5.411416424433856e+23\n",
      "Gradient Descent(23/49): loss=9.745624515692971e+24\n",
      "Gradient Descent(24/49): loss=1.7551263800735896e+26\n",
      "Gradient Descent(25/49): loss=3.1608734823200544e+27\n",
      "Gradient Descent(26/49): loss=5.692536608569056e+28\n",
      "Gradient Descent(27/49): loss=1.0251904488158837e+30\n",
      "Gradient Descent(28/49): loss=1.8463042552263736e+31\n",
      "Gradient Descent(29/49): loss=3.32507916631919e+32\n",
      "Gradient Descent(30/49): loss=5.988260835662876e+33\n",
      "Gradient Descent(31/49): loss=1.0784485433960159e+35\n",
      "Gradient Descent(32/49): loss=1.9422187721457748e+36\n",
      "Gradient Descent(33/49): loss=3.4978152476304575e+37\n",
      "Gradient Descent(34/49): loss=6.299347777922741e+38\n",
      "Gradient Descent(35/49): loss=1.1344733674570613e+40\n",
      "Gradient Descent(36/49): loss=2.043115996833826e+41\n",
      "Gradient Descent(37/49): loss=3.679524875824135e+42\n",
      "Gradient Descent(38/49): loss=6.626595520171072e+43\n",
      "Gradient Descent(39/49): loss=1.1934086511132983e+45\n",
      "Gradient Descent(40/49): loss=2.14925477828971e+46\n",
      "Gradient Descent(41/49): loss=3.870674221853366e+47\n",
      "Gradient Descent(42/49): loss=6.970843607309454e+48\n",
      "Gradient Descent(43/49): loss=1.2554055911814994e+50\n",
      "Gradient Descent(44/49): loss=2.2609074125793704e+51\n",
      "Gradient Descent(45/49): loss=4.071753674002383e+52\n",
      "Gradient Descent(46/49): loss=7.332975198147349e+53\n",
      "Gradient Descent(47/49): loss=1.3206232390720045e+55\n",
      "Gradient Descent(48/49): loss=2.378360341403666e+56\n",
      "Gradient Descent(49/49): loss=4.283279095964226e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6064079356164144\n",
      "Gradient Descent(2/49): loss=1.057624977252605\n",
      "Gradient Descent(3/49): loss=2.9490568569016067\n",
      "Gradient Descent(4/49): loss=17.6576952589372\n",
      "Gradient Descent(5/49): loss=219.78520233430586\n",
      "Gradient Descent(6/49): loss=3586.074152606417\n",
      "Gradient Descent(7/49): loss=61872.65278485263\n",
      "Gradient Descent(8/49): loss=1078015.7965640356\n",
      "Gradient Descent(9/49): loss=18813749.90127149\n",
      "Gradient Descent(10/49): loss=328434411.5994172\n",
      "Gradient Descent(11/49): loss=5733804246.625152\n",
      "Gradient Descent(12/49): loss=100101509822.85527\n",
      "Gradient Descent(13/49): loss=1747587779515.9575\n",
      "Gradient Descent(14/49): loss=30509667400516.88\n",
      "Gradient Descent(15/49): loss=532642682289978.06\n",
      "Gradient Descent(16/49): loss=9298961710014628.0\n",
      "Gradient Descent(17/49): loss=1.6234277098002848e+17\n",
      "Gradient Descent(18/49): loss=2.834206238995518e+18\n",
      "Gradient Descent(19/49): loss=4.948002893470707e+19\n",
      "Gradient Descent(20/49): loss=8.638303133018304e+20\n",
      "Gradient Descent(21/49): loss=1.508088871905334e+22\n",
      "Gradient Descent(22/49): loss=2.6328458385210216e+23\n",
      "Gradient Descent(23/49): loss=4.596464663690443e+24\n",
      "Gradient Descent(24/49): loss=8.0245820303794e+25\n",
      "Gradient Descent(25/49): loss=1.4009444534832773e+27\n",
      "Gradient Descent(26/49): loss=2.4457913874085887e+28\n",
      "Gradient Descent(27/49): loss=4.2699019906526174e+29\n",
      "Gradient Descent(28/49): loss=7.454463656892912e+30\n",
      "Gradient Descent(29/49): loss=1.3014122697332584e+32\n",
      "Gradient Descent(30/49): loss=2.2720264981722683e+33\n",
      "Gradient Descent(31/49): loss=3.966540448750159e+34\n",
      "Gradient Descent(32/49): loss=6.924850191768481e+35\n",
      "Gradient Descent(33/49): loss=1.2089514981133182e+37\n",
      "Gradient Descent(34/49): loss=2.1106069941091728e+38\n",
      "Gradient Descent(35/49): loss=3.684731679090901e+39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=6.432863903507826e+40\n",
      "Gradient Descent(37/49): loss=1.1230597396243205e+42\n",
      "Gradient Descent(38/49): loss=1.9606557789560486e+43\n",
      "Gradient Descent(39/49): loss=3.422944432893353e+44\n",
      "Gradient Descent(40/49): loss=5.975831513328771e+45\n",
      "Gradient Descent(41/49): loss=1.0432702889514331e+47\n",
      "Gradient Descent(42/49): loss=1.821358071058623e+48\n",
      "Gradient Descent(43/49): loss=3.179756251224862e+49\n",
      "Gradient Descent(44/49): loss=5.551269669520201e+50\n",
      "Gradient Descent(45/49): loss=9.691495985537962e+51\n",
      "Gradient Descent(46/49): loss=1.6919569761382946e+53\n",
      "Gradient Descent(47/49): loss=2.9538457358647064e+54\n",
      "Gradient Descent(48/49): loss=5.156871453788508e+55\n",
      "Gradient Descent(49/49): loss=9.002949229206673e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6095576974075003\n",
      "Gradient Descent(2/49): loss=1.063937696056521\n",
      "Gradient Descent(3/49): loss=2.911253537802665\n",
      "Gradient Descent(4/49): loss=16.648205312304054\n",
      "Gradient Descent(5/49): loss=201.50930616350166\n",
      "Gradient Descent(6/49): loss=3266.9471070292298\n",
      "Gradient Descent(7/49): loss=56318.186325305694\n",
      "Gradient Descent(8/49): loss=981346.4154487525\n",
      "Gradient Descent(9/49): loss=17131220.829710256\n",
      "Gradient Descent(10/49): loss=299149536.86153495\n",
      "Gradient Descent(11/49): loss=5224094667.575205\n",
      "Gradient Descent(12/49): loss=91229980239.32887\n",
      "Gradient Descent(13/49): loss=1593179733002.9807\n",
      "Gradient Descent(14/49): loss=27822238894262.895\n",
      "Gradient Descent(15/49): loss=485869230095860.6\n",
      "Gradient Descent(16/49): loss=8484899844491384.0\n",
      "Gradient Descent(17/49): loss=1.4817469598826458e+17\n",
      "Gradient Descent(18/49): loss=2.587625185703966e+18\n",
      "Gradient Descent(19/49): loss=4.518857998841825e+19\n",
      "Gradient Descent(20/49): loss=7.891435640151414e+20\n",
      "Gradient Descent(21/49): loss=1.3781082848515389e+22\n",
      "Gradient Descent(22/49): loss=2.4066374375707143e+23\n",
      "Gradient Descent(23/49): loss=4.202792929686945e+24\n",
      "Gradient Descent(24/49): loss=7.339480444406426e+25\n",
      "Gradient Descent(25/49): loss=1.2817184690047456e+27\n",
      "Gradient Descent(26/49): loss=2.2383086190248104e+28\n",
      "Gradient Descent(27/49): loss=3.908834580413824e+29\n",
      "Gradient Descent(28/49): loss=6.826130966557865e+30\n",
      "Gradient Descent(29/49): loss=1.1920705011688262e+32\n",
      "Gradient Descent(30/49): loss=2.081753319294273e+33\n",
      "Gradient Descent(31/49): loss=3.6354367280656184e+34\n",
      "Gradient Descent(32/49): loss=6.348686984800374e+35\n",
      "Gradient Descent(33/49): loss=1.1086928324129765e+37\n",
      "Gradient Descent(34/49): loss=1.9361480564198932e+38\n",
      "Gradient Descent(35/49): loss=3.3811612980485176e+39\n",
      "Gradient Descent(36/49): loss=5.904637140488177e+40\n",
      "Gradient Descent(37/49): loss=1.0311468956229875e+42\n",
      "Gradient Descent(38/49): loss=1.800726945712041e+43\n",
      "Gradient Descent(39/49): loss=3.144670799842105e+44\n",
      "Gradient Descent(40/49): loss=5.49164572836944e+45\n",
      "Gradient Descent(41/49): loss=9.590247986349752e+46\n",
      "Gradient Descent(42/49): loss=1.6747776711917184e+48\n",
      "Gradient Descent(43/49): loss=2.9247212917899635e+49\n",
      "Gradient Descent(44/49): loss=5.107540410759732e+50\n",
      "Gradient Descent(45/49): loss=8.919471787199856e+51\n",
      "Gradient Descent(46/49): loss=1.5576377388039517e+53\n",
      "Gradient Descent(47/49): loss=2.7201558379590567e+54\n",
      "Gradient Descent(48/49): loss=4.750300791032583e+55\n",
      "Gradient Descent(49/49): loss=8.29561207133491e+56\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6021604162914982\n",
      "Gradient Descent(2/49): loss=1.0241120836516804\n",
      "Gradient Descent(3/49): loss=2.608417491618272\n",
      "Gradient Descent(4/49): loss=12.793307466516135\n",
      "Gradient Descent(5/49): loss=140.43834618529746\n",
      "Gradient Descent(6/49): loss=2249.466321464037\n",
      "Gradient Descent(7/49): loss=39224.86923946833\n",
      "Gradient Descent(8/49): loss=694219.1252461999\n",
      "Gradient Descent(9/49): loss=12317123.0267125\n",
      "Gradient Descent(10/49): loss=218625536.66380933\n",
      "Gradient Descent(11/49): loss=3880808306.368308\n",
      "Gradient Descent(12/49): loss=68888768820.4038\n",
      "Gradient Descent(13/49): loss=1222856435541.9836\n",
      "Gradient Descent(14/49): loss=21707142621847.36\n",
      "Gradient Descent(15/49): loss=385327379068560.44\n",
      "Gradient Descent(16/49): loss=6840015423824673.0\n",
      "Gradient Descent(17/49): loss=1.2141834088784622e+17\n",
      "Gradient Descent(18/49): loss=2.1553187520767585e+18\n",
      "Gradient Descent(19/49): loss=3.825944984315125e+19\n",
      "Gradient Descent(20/49): loss=6.791503581081696e+20\n",
      "Gradient Descent(21/49): loss=1.205571984986363e+22\n",
      "Gradient Descent(22/49): loss=2.1400324591344482e+23\n",
      "Gradient Descent(23/49): loss=3.798810011499101e+24\n",
      "Gradient Descent(24/49): loss=6.7433358040293524e+25\n",
      "Gradient Descent(25/49): loss=1.1970216364666011e+27\n",
      "Gradient Descent(26/49): loss=2.1248545820794484e+28\n",
      "Gradient Descent(27/49): loss=3.771867489639848e+29\n",
      "Gradient Descent(28/49): loss=6.69550965011423e+30\n",
      "Gradient Descent(29/49): loss=1.1885319300825279e+32\n",
      "Gradient Descent(30/49): loss=2.1097843519673383e+33\n",
      "Gradient Descent(31/49): loss=3.7451160538001897e+34\n",
      "Gradient Descent(32/49): loss=6.648022696420393e+35\n",
      "Gradient Descent(33/49): loss=1.1801024357383177e+37\n",
      "Gradient Descent(34/49): loss=2.0948210053273783e+38\n",
      "Gradient Descent(35/49): loss=3.7185543487293293e+39\n",
      "Gradient Descent(36/49): loss=6.600872537218688e+40\n",
      "Gradient Descent(37/49): loss=1.171732726388556e+42\n",
      "Gradient Descent(38/49): loss=2.079963784103756e+43\n",
      "Gradient Descent(39/49): loss=3.692181028789189e+44\n",
      "Gradient Descent(40/49): loss=6.554056783841855e+45\n",
      "Gradient Descent(41/49): loss=1.1634223780167534e+47\n",
      "Gradient Descent(42/49): loss=2.065211935617022e+48\n",
      "Gradient Descent(43/49): loss=3.66599475788456e+49\n",
      "Gradient Descent(44/49): loss=6.507573064563724e+50\n",
      "Gradient Descent(45/49): loss=1.1551709696134876e+52\n",
      "Gradient Descent(46/49): loss=2.0505647125258095e+53\n",
      "Gradient Descent(47/49): loss=3.6399942093965296e+54\n",
      "Gradient Descent(48/49): loss=6.46141902447926e+55\n",
      "Gradient Descent(49/49): loss=1.1469780831553616e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6124386117743326\n",
      "Gradient Descent(2/49): loss=1.0925204551541114\n",
      "Gradient Descent(3/49): loss=3.23754598838288\n",
      "Gradient Descent(4/49): loss=21.88661421645047\n",
      "Gradient Descent(5/49): loss=303.18654613141297\n",
      "Gradient Descent(6/49): loss=5323.137474625929\n",
      "Gradient Descent(7/49): loss=97812.25491241667\n",
      "Gradient Descent(8/49): loss=1811035.9379630734\n",
      "Gradient Descent(9/49): loss=33574026.50569228\n",
      "Gradient Descent(10/49): loss=622541966.5865563\n",
      "Gradient Descent(11/49): loss=11543789278.719631\n",
      "Gradient Descent(12/49): loss=214057541019.4207\n",
      "Gradient Descent(13/49): loss=3969292143477.0767\n",
      "Gradient Descent(14/49): loss=73603024532313.83\n",
      "Gradient Descent(15/49): loss=1364829081699873.0\n",
      "Gradient Descent(16/49): loss=2.530817777481823e+16\n",
      "Gradient Descent(17/49): loss=4.6929236142856576e+17\n",
      "Gradient Descent(18/49): loss=8.702140568087612e+18\n",
      "Gradient Descent(19/49): loss=1.6136476254919855e+20\n",
      "Gradient Descent(20/49): loss=2.9922047786866075e+21\n",
      "Gradient Descent(21/49): loss=5.548478674127709e+22\n",
      "Gradient Descent(22/49): loss=1.0288605852291989e+24\n",
      "Gradient Descent(23/49): loss=1.9078276515222343e+25\n",
      "Gradient Descent(24/49): loss=3.5377060800729086e+26\n",
      "Gradient Descent(25/49): loss=6.560007817791678e+27\n",
      "Gradient Descent(26/49): loss=1.2164295618533079e+29\n",
      "Gradient Descent(27/49): loss=2.2556388956389615e+30\n",
      "Gradient Descent(28/49): loss=4.182656346963021e+31\n",
      "Gradient Descent(29/49): loss=7.755946286710086e+32\n",
      "Gradient Descent(30/49): loss=1.4381937652136227e+34\n",
      "Gradient Descent(31/49): loss=2.666858729854217e+35\n",
      "Gradient Descent(32/49): loss=4.945185869265159e+36\n",
      "Gradient Descent(33/49): loss=9.169913279552024e+37\n",
      "Gradient Descent(34/49): loss=1.7003872407934518e+39\n",
      "Gradient Descent(35/49): loss=3.153047014196413e+40\n",
      "Gradient Descent(36/49): loss=5.846730224283484e+41\n",
      "Gradient Descent(37/49): loss=1.0841657026246838e+43\n",
      "Gradient Descent(38/49): loss=2.0103805471745673e+44\n",
      "Gradient Descent(39/49): loss=3.7278710576006977e+45\n",
      "Gradient Descent(40/49): loss=6.912632855320785e+46\n",
      "Gradient Descent(41/49): loss=1.2818172156205347e+48\n",
      "Gradient Descent(42/49): loss=2.376887950871662e+49\n",
      "Gradient Descent(43/49): loss=4.407489821599872e+50\n",
      "Gradient Descent(44/49): loss=8.172857504865683e+51\n",
      "Gradient Descent(45/49): loss=1.5155020771118675e+53\n",
      "Gradient Descent(46/49): loss=2.8102123943344924e+54\n",
      "Gradient Descent(47/49): loss=5.211008167221629e+55\n",
      "Gradient Descent(48/49): loss=9.662830529676498e+56\n",
      "Gradient Descent(49/49): loss=1.791789435920805e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6149768468736181\n",
      "Gradient Descent(2/49): loss=1.1100161293584563\n",
      "Gradient Descent(3/49): loss=3.237034121587109\n",
      "Gradient Descent(4/49): loss=20.015465013522444\n",
      "Gradient Descent(5/49): loss=255.19285742274369\n",
      "Gradient Descent(6/49): loss=4277.9893452525075\n",
      "Gradient Descent(7/49): loss=75964.20027575453\n",
      "Gradient Descent(8/49): loss=1362727.0865995209\n",
      "Gradient Descent(9/49): loss=24489005.76647279\n",
      "Gradient Descent(10/49): loss=440214266.1448602\n",
      "Gradient Descent(11/49): loss=7913698043.643781\n",
      "Gradient Descent(12/49): loss=142265199947.07068\n",
      "Gradient Descent(13/49): loss=2557517066507.8027\n",
      "Gradient Descent(14/49): loss=45976776109002.78\n",
      "Gradient Descent(15/49): loss=826529786758517.4\n",
      "Gradient Descent(16/49): loss=1.4858621056885228e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=2.6711514035968653e+17\n",
      "Gradient Descent(18/49): loss=4.801959613361779e+18\n",
      "Gradient Descent(19/49): loss=8.632538050144362e+19\n",
      "Gradient Descent(20/49): loss=1.55188129821673e+21\n",
      "Gradient Descent(21/49): loss=2.789834866372062e+22\n",
      "Gradient Descent(22/49): loss=5.015318240235391e+23\n",
      "Gradient Descent(23/49): loss=9.016095308733551e+24\n",
      "Gradient Descent(24/49): loss=1.6208338279318335e+26\n",
      "Gradient Descent(25/49): loss=2.913791622437008e+27\n",
      "Gradient Descent(26/49): loss=5.238156726909702e+28\n",
      "Gradient Descent(27/49): loss=9.416694620297063e+29\n",
      "Gradient Descent(28/49): loss=1.6928500271171795e+31\n",
      "Gradient Descent(29/49): loss=3.04325597235967e+32\n",
      "Gradient Descent(30/49): loss=5.470896278434516e+33\n",
      "Gradient Descent(31/49): loss=9.835093190067873e+34\n",
      "Gradient Descent(32/49): loss=1.7680660194310853e+36\n",
      "Gradient Descent(33/49): loss=3.178472627207785e+37\n",
      "Gradient Descent(34/49): loss=5.713976814711848e+38\n",
      "Gradient Descent(35/49): loss=1.0272081867115669e+40\n",
      "Gradient Descent(36/49): loss=1.8466239767206626e+41\n",
      "Gradient Descent(37/49): loss=3.3196971709465963e+42\n",
      "Gradient Descent(38/49): loss=5.967857801977471e+43\n",
      "Gradient Descent(39/49): loss=1.0728486639179407e+45\n",
      "Gradient Descent(40/49): loss=1.9286723877521901e+46\n",
      "Gradient Descent(41/49): loss=3.467196543539932e+47\n",
      "Gradient Descent(42/49): loss=6.233019121275321e+48\n",
      "Gradient Descent(43/49): loss=1.1205170193933952e+50\n",
      "Gradient Descent(44/49): loss=2.0143663388816446e+51\n",
      "Gradient Descent(45/49): loss=3.6212495455142823e+52\n",
      "Gradient Descent(46/49): loss=6.509961975520215e+53\n",
      "Gradient Descent(47/49): loss=1.170303354962529e+55\n",
      "Gradient Descent(48/49): loss=2.103867807195793e+56\n",
      "Gradient Descent(49/49): loss=3.782147364942486e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6183195786846539\n",
      "Gradient Descent(2/49): loss=1.1170177741555578\n",
      "Gradient Descent(3/49): loss=3.1966857329603084\n",
      "Gradient Descent(4/49): loss=18.883020145609997\n",
      "Gradient Descent(5/49): loss=234.03933112290417\n",
      "Gradient Descent(6/49): loss=3897.5326566151157\n",
      "Gradient Descent(7/49): loss=69145.21624858296\n",
      "Gradient Descent(8/49): loss=1240521.4749765093\n",
      "Gradient Descent(9/49): loss=22298776.807095774\n",
      "Gradient Descent(10/49): loss=400959226.58376074\n",
      "Gradient Descent(11/49): loss=7210139693.810063\n",
      "Gradient Descent(12/49): loss=129655603839.60931\n",
      "Gradient Descent(13/49): loss=2331522508848.2847\n",
      "Gradient Descent(14/49): loss=41926446363795.14\n",
      "Gradient Descent(15/49): loss=753939531527714.6\n",
      "Gradient Descent(16/49): loss=1.3557667560672166e+16\n",
      "Gradient Descent(17/49): loss=2.4379985695550506e+17\n",
      "Gradient Descent(18/49): loss=4.384114745367578e+18\n",
      "Gradient Descent(19/49): loss=7.883705241398093e+19\n",
      "Gradient Descent(20/49): loss=1.4176820622532372e+21\n",
      "Gradient Descent(21/49): loss=2.5493373586358575e+22\n",
      "Gradient Descent(22/49): loss=4.5843289840375204e+23\n",
      "Gradient Descent(23/49): loss=8.243739167237095e+24\n",
      "Gradient Descent(24/49): loss=1.4824249239980526e+26\n",
      "Gradient Descent(25/49): loss=2.6657607800407704e+27\n",
      "Gradient Descent(26/49): loss=4.793686628823123e+28\n",
      "Gradient Descent(27/49): loss=8.620215162369772e+29\n",
      "Gradient Descent(28/49): loss=1.5501244699383464e+31\n",
      "Gradient Descent(29/49): loss=2.78750103917471e+32\n",
      "Gradient Descent(30/49): loss=5.0126052417644983e+33\n",
      "Gradient Descent(31/49): loss=9.013884105027439e+34\n",
      "Gradient Descent(32/49): loss=1.620915726255449e+36\n",
      "Gradient Descent(33/49): loss=2.9148009459726996e+37\n",
      "Gradient Descent(34/49): loss=5.241521454215607e+38\n",
      "Gradient Descent(35/49): loss=9.425531164644957e+39\n",
      "Gradient Descent(36/49): loss=1.6949398855220062e+41\n",
      "Gradient Descent(37/49): loss=3.047914399041337e+42\n",
      "Gradient Descent(38/49): loss=5.480891837660894e+43\n",
      "Gradient Descent(39/49): loss=9.855977367863616e+44\n",
      "Gradient Descent(40/49): loss=1.7723445883087621e+46\n",
      "Gradient Descent(41/49): loss=3.1871068920568418e+47\n",
      "Gradient Descent(42/49): loss=5.731193814341258e+48\n",
      "Gradient Descent(43/49): loss=1.0306081236059434e+50\n",
      "Gradient Descent(44/49): loss=1.8532842176524046e+51\n",
      "Gradient Descent(45/49): loss=3.3326560432902866e+52\n",
      "Gradient Descent(46/49): loss=5.992926609469766e+53\n",
      "Gradient Descent(47/49): loss=1.0776740497658002e+55\n",
      "Gradient Descent(48/49): loss=1.9379202069710182e+56\n",
      "Gradient Descent(49/49): loss=3.484852149314384e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6105503196781504\n",
      "Gradient Descent(2/49): loss=1.0741351711918266\n",
      "Gradient Descent(3/49): loss=2.863311707971366\n",
      "Gradient Descent(4/49): loss=14.536716296218014\n",
      "Gradient Descent(5/49): loss=163.26051017773534\n",
      "Gradient Descent(6/49): loss=2683.6711280810473\n",
      "Gradient Descent(7/49): loss=48145.9299699021\n",
      "Gradient Descent(8/49): loss=877251.2762954923\n",
      "Gradient Descent(9/49): loss=16025962.047801293\n",
      "Gradient Descent(10/49): loss=292896639.58978575\n",
      "Gradient Descent(11/49): loss=5353483828.59647\n",
      "Gradient Descent(12/49): loss=97850698007.62048\n",
      "Gradient Descent(13/49): loss=1788513622867.3171\n",
      "Gradient Descent(14/49): loss=32690436997637.938\n",
      "Gradient Descent(15/49): loss=597515567621169.9\n",
      "Gradient Descent(16/49): loss=1.0921385266672834e+16\n",
      "Gradient Descent(17/49): loss=1.9962100202709754e+17\n",
      "Gradient Descent(18/49): loss=3.648671252597525e+18\n",
      "Gradient Descent(19/49): loss=6.669038715839152e+19\n",
      "Gradient Descent(20/49): loss=1.218966421324875e+21\n",
      "Gradient Descent(21/49): loss=2.2280259564101903e+22\n",
      "Gradient Descent(22/49): loss=4.072384255706868e+23\n",
      "Gradient Descent(23/49): loss=7.443501041096584e+24\n",
      "Gradient Descent(24/49): loss=1.3605225899585202e+26\n",
      "Gradient Descent(25/49): loss=2.4867622205837423e+27\n",
      "Gradient Descent(26/49): loss=4.545302215019501e+28\n",
      "Gradient Descent(27/49): loss=8.307900150184589e+29\n",
      "Gradient Descent(28/49): loss=1.5185173975311045e+31\n",
      "Gradient Descent(29/49): loss=2.7755450172970153e+32\n",
      "Gradient Descent(30/49): loss=5.0731392051005026e+33\n",
      "Gradient Descent(31/49): loss=9.272680224582296e+34\n",
      "Gradient Descent(32/49): loss=1.694859830791043e+36\n",
      "Gradient Descent(33/49): loss=3.0978635911694843e+37\n",
      "Gradient Descent(34/49): loss=5.662272864779709e+38\n",
      "Gradient Descent(35/49): loss=1.0349498307999419e+40\n",
      "Gradient Descent(36/49): loss=1.891680563357111e+41\n",
      "Gradient Descent(37/49): loss=3.457612385923298e+42\n",
      "Gradient Descent(38/49): loss=6.319821455517604e+43\n",
      "Gradient Descent(39/49): loss=1.155136515366104e+45\n",
      "Gradient Descent(40/49): loss=2.1113576997767395e+46\n",
      "Gradient Descent(41/49): loss=3.859138099355963e+47\n",
      "Gradient Descent(42/49): loss=7.053729868451551e+48\n",
      "Gradient Descent(43/49): loss=1.2892802427928072e+50\n",
      "Gradient Descent(44/49): loss=2.3565455091928174e+51\n",
      "Gradient Descent(45/49): loss=4.3072922027156723e+52\n",
      "Gradient Descent(46/49): loss=7.872865619272745e+53\n",
      "Gradient Descent(47/49): loss=1.4390018169663e+55\n",
      "Gradient Descent(48/49): loss=2.6302064957938365e+56\n",
      "Gradient Descent(49/49): loss=4.807489559047478e+57\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6210891378375583\n",
      "Gradient Descent(2/49): loss=1.1465813856166853\n",
      "Gradient Descent(3/49): loss=3.5499354388603694\n",
      "Gradient Descent(4/49): loss=24.73698343825224\n",
      "Gradient Descent(5/49): loss=350.94229346666066\n",
      "Gradient Descent(6/49): loss=6329.918984310238\n",
      "Gradient Descent(7/49): loss=119659.25733631736\n",
      "Gradient Descent(8/49): loss=2280058.4875364765\n",
      "Gradient Descent(9/49): loss=43502773.241148345\n",
      "Gradient Descent(10/49): loss=830199039.1232598\n",
      "Gradient Descent(11/49): loss=15843936211.85838\n",
      "Gradient Descent(12/49): loss=302375442234.57684\n",
      "Gradient Descent(13/49): loss=5770724919085.1045\n",
      "Gradient Descent(14/49): loss=110132196009962.92\n",
      "Gradient Descent(15/49): loss=2101833147787165.0\n",
      "Gradient Descent(16/49): loss=4.011272598443207e+16\n",
      "Gradient Descent(17/49): loss=7.655368784947369e+17\n",
      "Gradient Descent(18/49): loss=1.460999465938511e+19\n",
      "Gradient Descent(19/49): loss=2.78826467996172e+20\n",
      "Gradient Descent(20/49): loss=5.321302373340347e+21\n",
      "Gradient Descent(21/49): loss=1.0155513266739721e+23\n",
      "Gradient Descent(22/49): loss=1.9381430047583555e+24\n",
      "Gradient Descent(23/49): loss=3.698875879761117e+25\n",
      "Gradient Descent(24/49): loss=7.059170938516223e+26\n",
      "Gradient Descent(25/49): loss=1.3472172616511455e+28\n",
      "Gradient Descent(26/49): loss=2.5711154551986327e+29\n",
      "Gradient Descent(27/49): loss=4.906880925693626e+30\n",
      "Gradient Descent(28/49): loss=9.364604911169126e+31\n",
      "Gradient Descent(29/49): loss=1.7872010034540764e+33\n",
      "Gradient Descent(30/49): loss=3.4108085253417767e+34\n",
      "Gradient Descent(31/49): loss=6.509404803410365e+35\n",
      "Gradient Descent(32/49): loss=1.2422963816304104e+37\n",
      "Gradient Descent(33/49): loss=2.370877747537626e+38\n",
      "Gradient Descent(34/49): loss=4.5247344972476973e+39\n",
      "Gradient Descent(35/49): loss=8.63529226331752e+40\n",
      "Gradient Descent(36/49): loss=1.648014320359988e+42\n",
      "Gradient Descent(37/49): loss=3.1451757708871565e+43\n",
      "Gradient Descent(38/49): loss=6.002454291546925e+44\n",
      "Gradient Descent(39/49): loss=1.145546708569095e+46\n",
      "Gradient Descent(40/49): loss=2.1862344930498648e+47\n",
      "Gradient Descent(41/49): loss=4.172349519096588e+48\n",
      "Gradient Descent(42/49): loss=7.962778267769687e+49\n",
      "Gradient Descent(43/49): loss=1.5196674547868058e+51\n",
      "Gradient Descent(44/49): loss=2.900230416418494e+52\n",
      "Gradient Descent(45/49): loss=5.534984934910679e+53\n",
      "Gradient Descent(46/49): loss=1.0563318712973368e+55\n",
      "Gradient Descent(47/49): loss=2.0159712003561863e+56\n",
      "Gradient Descent(48/49): loss=3.84740817833532e+57\n",
      "Gradient Descent(49/49): loss=7.34263946236235e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.623717573427737\n",
      "Gradient Descent(2/49): loss=1.1653873257442104\n",
      "Gradient Descent(3/49): loss=3.551594238361575\n",
      "Gradient Descent(4/49): loss=22.65409052575151\n",
      "Gradient Descent(5/49): loss=295.691967632899\n",
      "Gradient Descent(6/49): loss=5090.53662769483\n",
      "Gradient Descent(7/49): loss=92989.77080922353\n",
      "Gradient Descent(8/49): loss=1716816.720777383\n",
      "Gradient Descent(9/49): loss=31755160.54673777\n",
      "Gradient Descent(10/49): loss=587547850.3104303\n",
      "Gradient Descent(11/49): loss=10871666161.00331\n",
      "Gradient Descent(12/49): loss=201165318955.99185\n",
      "Gradient Descent(13/49): loss=3722295326332.095\n",
      "Gradient Descent(14/49): loss=68876118944811.19\n",
      "Gradient Descent(15/49): loss=1274460938231179.0\n",
      "Gradient Descent(16/49): loss=2.3582204133301184e+16\n",
      "Gradient Descent(17/49): loss=4.363573146245859e+17\n",
      "Gradient Descent(18/49): loss=8.074211596097551e+18\n",
      "Gradient Descent(19/49): loss=1.4940254400799483e+20\n",
      "Gradient Descent(20/49): loss=2.76449531828202e+21\n",
      "Gradient Descent(21/49): loss=5.115330810166367e+22\n",
      "Gradient Descent(22/49): loss=9.465239143072468e+23\n",
      "Gradient Descent(23/49): loss=1.7514165820419033e+25\n",
      "Gradient Descent(24/49): loss=3.240763384300212e+26\n",
      "Gradient Descent(25/49): loss=5.996601505722938e+27\n",
      "Gradient Descent(26/49): loss=1.109591332481816e+29\n",
      "Gradient Descent(27/49): loss=2.0531511456009354e+30\n",
      "Gradient Descent(28/49): loss=3.7990830527253656e+31\n",
      "Gradient Descent(29/49): loss=7.029697775747801e+32\n",
      "Gradient Descent(30/49): loss=1.3007520533909776e+34\n",
      "Gradient Descent(31/49): loss=2.4068686284608493e+35\n",
      "Gradient Descent(32/49): loss=4.453590197737385e+36\n",
      "Gradient Descent(33/49): loss=8.240776174837129e+37\n",
      "Gradient Descent(34/49): loss=1.524845999487398e+39\n",
      "Gradient Descent(35/49): loss=2.8215246632380494e+40\n",
      "Gradient Descent(36/49): loss=5.220856026075389e+41\n",
      "Gradient Descent(37/49): loss=9.660499516500964e+42\n",
      "Gradient Descent(38/49): loss=1.7875469164866057e+44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=3.3076177615691127e+45\n",
      "Gradient Descent(40/49): loss=6.120306636846594e+46\n",
      "Gradient Descent(41/49): loss=1.1324813212775288e+48\n",
      "Gradient Descent(42/49): loss=2.0955060246838433e+49\n",
      "Gradient Descent(43/49): loss=3.877455121761127e+50\n",
      "Gradient Descent(44/49): loss=7.174714863222572e+51\n",
      "Gradient Descent(45/49): loss=1.3275855361845506e+53\n",
      "Gradient Descent(46/49): loss=2.4565204185616228e+54\n",
      "Gradient Descent(47/49): loss=4.5454642298628876e+55\n",
      "Gradient Descent(48/49): loss=8.410776848767654e+56\n",
      "Gradient Descent(49/49): loss=1.5563023625839835e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6272569334435647\n",
      "Gradient Descent(2/49): loss=1.1731219874294503\n",
      "Gradient Descent(3/49): loss=3.508591577918009\n",
      "Gradient Descent(4/49): loss=21.385871740497784\n",
      "Gradient Descent(5/49): loss=271.2595948331733\n",
      "Gradient Descent(6/49): loss=4638.114598844238\n",
      "Gradient Descent(7/49): loss=84643.14943865368\n",
      "Gradient Descent(8/49): loss=1562851.2157547702\n",
      "Gradient Descent(9/49): loss=28914869.837412264\n",
      "Gradient Descent(10/49): loss=535150411.66816837\n",
      "Gradient Descent(11/49): loss=9905045016.941578\n",
      "Gradient Descent(12/49): loss=183333364623.56558\n",
      "Gradient Descent(13/49): loss=3393339652305.482\n",
      "Gradient Descent(14/49): loss=62807757446927.945\n",
      "Gradient Descent(15/49): loss=1162516873049430.0\n",
      "Gradient Descent(16/49): loss=2.15171747442139e+16\n",
      "Gradient Descent(17/49): loss=3.9826416323281606e+17\n",
      "Gradient Descent(18/49): loss=7.371522778582722e+18\n",
      "Gradient Descent(19/49): loss=1.3644046613825816e+20\n",
      "Gradient Descent(20/49): loss=2.5253941905000633e+21\n",
      "Gradient Descent(21/49): loss=4.674284688354598e+22\n",
      "Gradient Descent(22/49): loss=8.651693834563289e+23\n",
      "Gradient Descent(23/49): loss=1.6013531737488136e+25\n",
      "Gradient Descent(24/49): loss=2.9639652490140468e+26\n",
      "Gradient Descent(25/49): loss=5.486041518746814e+27\n",
      "Gradient Descent(26/49): loss=1.0154185024748423e+29\n",
      "Gradient Descent(27/49): loss=1.8794512065664906e+30\n",
      "Gradient Descent(28/49): loss=3.478700485814419e+31\n",
      "Gradient Descent(29/49): loss=6.43877160935365e+32\n",
      "Gradient Descent(30/49): loss=1.1917605441019438e+34\n",
      "Gradient Descent(31/49): loss=2.205844966475964e+35\n",
      "Gradient Descent(32/49): loss=4.082826906972362e+36\n",
      "Gradient Descent(33/49): loss=7.556956996360713e+37\n",
      "Gradient Descent(34/49): loss=1.3987269200004543e+39\n",
      "Gradient Descent(35/49): loss=2.5889217017857037e+40\n",
      "Gradient Descent(36/49): loss=4.791868578589242e+41\n",
      "Gradient Descent(37/49): loss=8.8693313739975e+42\n",
      "Gradient Descent(38/49): loss=1.6416359867059445e+44\n",
      "Gradient Descent(39/49): loss=3.038525227221604e+45\n",
      "Gradient Descent(40/49): loss=5.624045544340176e+46\n",
      "Gradient Descent(41/49): loss=1.0409618456166357e+48\n",
      "Gradient Descent(42/49): loss=1.9267297099329531e+49\n",
      "Gradient Descent(43/49): loss=3.5662088776551054e+50\n",
      "Gradient Descent(44/49): loss=6.600742020793623e+51\n",
      "Gradient Descent(45/49): loss=1.2217398565200944e+53\n",
      "Gradient Descent(46/49): loss=2.2613340626063532e+54\n",
      "Gradient Descent(47/49): loss=4.185532390887998e+55\n",
      "Gradient Descent(48/49): loss=7.7470559015863075e+56\n",
      "Gradient Descent(49/49): loss=1.4339125716231457e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6191090161931464\n",
      "Gradient Descent(2/49): loss=1.1270192258881608\n",
      "Gradient Descent(3/49): loss=3.142146602141522\n",
      "Gradient Descent(4/49): loss=16.493968426056735\n",
      "Gradient Descent(5/49): loss=189.40401401839702\n",
      "Gradient Descent(6/49): loss=3193.6618011926894\n",
      "Gradient Descent(7/49): loss=58922.1306157906\n",
      "Gradient Descent(8/49): loss=1104809.848252385\n",
      "Gradient Descent(9/49): loss=20772633.687860712\n",
      "Gradient Descent(10/49): loss=390748533.522795\n",
      "Gradient Descent(11/49): loss=7350843690.456407\n",
      "Gradient Descent(12/49): loss=138287447581.56937\n",
      "Gradient Descent(13/49): loss=2601532778362.2686\n",
      "Gradient Descent(14/49): loss=48941357018941.57\n",
      "Gradient Descent(15/49): loss=920709744198595.5\n",
      "Gradient Descent(16/49): loss=1.7320861001287968e+16\n",
      "Gradient Descent(17/49): loss=3.258488663319616e+17\n",
      "Gradient Descent(18/49): loss=6.130034974259273e+18\n",
      "Gradient Descent(19/49): loss=1.1532134271503794e+20\n",
      "Gradient Descent(20/49): loss=2.1694838841145827e+21\n",
      "Gradient Descent(21/49): loss=4.0813436720643995e+22\n",
      "Gradient Descent(22/49): loss=7.678031762057844e+23\n",
      "Gradient Descent(23/49): loss=1.4444304737844103e+25\n",
      "Gradient Descent(24/49): loss=2.71733623701228e+26\n",
      "Gradient Descent(25/49): loss=5.111991445066987e+27\n",
      "Gradient Descent(26/49): loss=9.616938889819179e+28\n",
      "Gradient Descent(27/49): loss=1.8091875662226388e+30\n",
      "Gradient Descent(28/49): loss=3.4035358727709652e+31\n",
      "Gradient Descent(29/49): loss=6.40290517882845e+32\n",
      "Gradient Descent(30/49): loss=1.2045471609996341e+34\n",
      "Gradient Descent(31/49): loss=2.266055520968696e+35\n",
      "Gradient Descent(32/49): loss=4.263019158047163e+36\n",
      "Gradient Descent(33/49): loss=8.019808947182437e+37\n",
      "Gradient Descent(34/49): loss=1.5087273400565335e+39\n",
      "Gradient Descent(35/49): loss=2.8382947793710983e+40\n",
      "Gradient Descent(36/49): loss=5.339544820804547e+41\n",
      "Gradient Descent(37/49): loss=1.0045023899772163e+43\n",
      "Gradient Descent(38/49): loss=1.8897211004549227e+44\n",
      "Gradient Descent(39/49): loss=3.555039662559339e+45\n",
      "Gradient Descent(40/49): loss=6.687921831072009e+46\n",
      "Gradient Descent(41/49): loss=1.2581659464898744e+48\n",
      "Gradient Descent(42/49): loss=2.3669259134462123e+49\n",
      "Gradient Descent(43/49): loss=4.452781682236031e+50\n",
      "Gradient Descent(44/49): loss=8.376799880815968e+51\n",
      "Gradient Descent(45/49): loss=1.575886294250138e+53\n",
      "Gradient Descent(46/49): loss=2.9646376274223796e+54\n",
      "Gradient Descent(47/49): loss=5.577227426875178e+55\n",
      "Gradient Descent(48/49): loss=1.049216453416419e+57\n",
      "Gradient Descent(49/49): loss=1.9738394758925703e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6299100759674325\n",
      "Gradient Descent(2/49): loss=1.2036657590906839\n",
      "Gradient Descent(3/49): loss=3.89050986861882\n",
      "Gradient Descent(4/49): loss=27.917148041840047\n",
      "Gradient Descent(5/49): loss=405.40514856865036\n",
      "Gradient Descent(6/49): loss=7508.760816832889\n",
      "Gradient Descent(7/49): loss=145968.35664114435\n",
      "Gradient Descent(8/49): loss=2861186.3045215625\n",
      "Gradient Descent(9/49): loss=56160907.415420264\n",
      "Gradient Descent(10/49): loss=1102610322.5252903\n",
      "Gradient Descent(11/49): loss=21648441001.261047\n",
      "Gradient Descent(12/49): loss=425044075616.9082\n",
      "Gradient Descent(13/49): loss=8345296452690.475\n",
      "Gradient Descent(14/49): loss=163851207854958.97\n",
      "Gradient Descent(15/49): loss=3217047981073836.0\n",
      "Gradient Descent(16/49): loss=6.316339011867497e+16\n",
      "Gradient Descent(17/49): loss=1.2401474513291205e+18\n",
      "Gradient Descent(18/49): loss=2.434900499097475e+19\n",
      "Gradient Descent(19/49): loss=4.780673809622167e+20\n",
      "Gradient Descent(20/49): loss=9.386355657064923e+21\n",
      "Gradient Descent(21/49): loss=1.8429132802082985e+23\n",
      "Gradient Descent(22/49): loss=3.6183684940727636e+24\n",
      "Gradient Descent(23/49): loss=7.104290093030603e+25\n",
      "Gradient Descent(24/49): loss=1.394853448691344e+27\n",
      "Gradient Descent(25/49): loss=2.7386496297986725e+28\n",
      "Gradient Descent(26/49): loss=5.377053626553597e+29\n",
      "Gradient Descent(27/49): loss=1.055728538190518e+31\n",
      "Gradient Descent(28/49): loss=2.0728131496509034e+32\n",
      "Gradient Descent(29/49): loss=4.0697529695748773e+33\n",
      "Gradient Descent(30/49): loss=7.99053654988269e+34\n",
      "Gradient Descent(31/49): loss=1.568858720230429e+36\n",
      "Gradient Descent(32/49): loss=3.0802908774370107e+37\n",
      "Gradient Descent(33/49): loss=6.047830672878083e+38\n",
      "Gradient Descent(34/49): loss=1.1874286326568772e+40\n",
      "Gradient Descent(35/49): loss=2.3313925834209747e+41\n",
      "Gradient Descent(36/49): loss=4.577446785891077e+42\n",
      "Gradient Descent(37/49): loss=8.98734053915498e+43\n",
      "Gradient Descent(38/49): loss=1.7645708130502148e+45\n",
      "Gradient Descent(39/49): loss=3.4645512103422725e+46\n",
      "Gradient Descent(40/49): loss=6.802285859151962e+47\n",
      "Gradient Descent(41/49): loss=1.335558059338037e+49\n",
      "Gradient Descent(42/49): loss=2.622229301732253e+50\n",
      "Gradient Descent(43/49): loss=5.14847442444496e+51\n",
      "Gradient Descent(44/49): loss=1.0108493899314756e+53\n",
      "Gradient Descent(45/49): loss=1.9846976111471787e+54\n",
      "Gradient Descent(46/49): loss=3.8967472770204876e+55\n",
      "Gradient Descent(47/49): loss=7.650857871587506e+56\n",
      "Gradient Descent(48/49): loss=1.5021663456704775e+58\n",
      "Gradient Descent(49/49): loss=2.9493473384793155e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6326301152787718\n",
      "Gradient Descent(2/49): loss=1.2238494630233914\n",
      "Gradient Descent(3/49): loss=3.894680694843368\n",
      "Gradient Descent(4/49): loss=25.60220652523183\n",
      "Gradient Descent(5/49): loss=341.92766167519244\n",
      "Gradient Descent(6/49): loss=6042.570936128178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=113504.70779070118\n",
      "Gradient Descent(8/49): loss=2155817.8485452593\n",
      "Gradient Descent(9/49): loss=41025310.726569556\n",
      "Gradient Descent(10/49): loss=780977105.2194798\n",
      "Gradient Descent(11/49): loss=14867921117.11057\n",
      "Gradient Descent(12/49): loss=283052266662.877\n",
      "Gradient Descent(13/49): loss=5388697432701.299\n",
      "Gradient Descent(14/49): loss=102589070798992.44\n",
      "Gradient Descent(15/49): loss=1953072731754064.8\n",
      "Gradient Descent(16/49): loss=3.7182256371535016e+16\n",
      "Gradient Descent(17/49): loss=7.078692813956536e+17\n",
      "Gradient Descent(18/49): loss=1.3476291343898202e+19\n",
      "Gradient Descent(19/49): loss=2.565592732553047e+20\n",
      "Gradient Descent(20/49): loss=4.884330489312538e+21\n",
      "Gradient Descent(21/49): loss=9.298702801174184e+22\n",
      "Gradient Descent(22/49): loss=1.7702707458838782e+24\n",
      "Gradient Descent(23/49): loss=3.370210426917557e+25\n",
      "Gradient Descent(24/49): loss=6.416147557153833e+26\n",
      "Gradient Descent(25/49): loss=1.2214949294078116e+28\n",
      "Gradient Descent(26/49): loss=2.325460643288075e+29\n",
      "Gradient Descent(27/49): loss=4.42717122542908e+30\n",
      "Gradient Descent(28/49): loss=8.428371005047068e+31\n",
      "Gradient Descent(29/49): loss=1.604578503552971e+33\n",
      "Gradient Descent(30/49): loss=3.054768439266072e+34\n",
      "Gradient Descent(31/49): loss=5.815614628310952e+35\n",
      "Gradient Descent(32/49): loss=1.1071665226824902e+37\n",
      "Gradient Descent(33/49): loss=2.1078042258533882e+38\n",
      "Gradient Descent(34/49): loss=4.0128007517433413e+39\n",
      "Gradient Descent(35/49): loss=7.639499757940016e+40\n",
      "Gradient Descent(36/49): loss=1.4543945778073148e+42\n",
      "Gradient Descent(37/49): loss=2.768850912989213e+43\n",
      "Gradient Descent(38/49): loss=5.27128985169852e+44\n",
      "Gradient Descent(39/49): loss=1.0035389254895426e+46\n",
      "Gradient Descent(40/49): loss=1.910519822104274e+47\n",
      "Gradient Descent(41/49): loss=3.637214160749e+48\n",
      "Gradient Descent(42/49): loss=6.924464587120627e+49\n",
      "Gradient Descent(43/49): loss=1.3182674348879675e+51\n",
      "Gradient Descent(44/49): loss=2.509694443550789e+52\n",
      "Gradient Descent(45/49): loss=4.777912306181618e+53\n",
      "Gradient Descent(46/49): loss=9.096105728816618e+54\n",
      "Gradient Descent(47/49): loss=1.7317006702438873e+56\n",
      "Gradient Descent(48/49): loss=3.2967813927480455e+57\n",
      "Gradient Descent(49/49): loss=6.276354648542703e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6363697616842329\n",
      "Gradient Descent(2/49): loss=1.2323630587879513\n",
      "Gradient Descent(3/49): loss=3.84891461045046\n",
      "Gradient Descent(4/49): loss=24.184278526108596\n",
      "Gradient Descent(5/49): loss=313.7663092681134\n",
      "Gradient Descent(6/49): loss=5505.896931873134\n",
      "Gradient Descent(7/49): loss=103317.57234650372\n",
      "Gradient Descent(8/49): loss=1962475.2157895896\n",
      "Gradient Descent(9/49): loss=37355619.85458018\n",
      "Gradient Descent(10/49): loss=711324142.6778971\n",
      "Gradient Descent(11/49): loss=13545868193.704231\n",
      "Gradient Descent(12/49): loss=257959151745.44818\n",
      "Gradient Descent(13/49): loss=4912424246597.823\n",
      "Gradient Descent(14/49): loss=93549384930543.58\n",
      "Gradient Descent(15/49): loss=1781500840427287.8\n",
      "Gradient Descent(16/49): loss=3.3925880735656948e+16\n",
      "Gradient Descent(17/49): loss=6.460650253748173e+17\n",
      "Gradient Descent(18/49): loss=1.2303292002634537e+19\n",
      "Gradient Descent(19/49): loss=2.342968403561499e+20\n",
      "Gradient Descent(20/49): loss=4.4618147232150706e+21\n",
      "Gradient Descent(21/49): loss=8.496824197060215e+22\n",
      "Gradient Descent(22/49): loss=1.6180864942717133e+24\n",
      "Gradient Descent(23/49): loss=3.0813911671262906e+25\n",
      "Gradient Descent(24/49): loss=5.8680247060078135e+26\n",
      "Gradient Descent(25/49): loss=1.1174729880994139e+28\n",
      "Gradient Descent(26/49): loss=2.128051502328104e+29\n",
      "Gradient Descent(27/49): loss=4.0525392960621536e+30\n",
      "Gradient Descent(28/49): loss=7.717423534233794e+31\n",
      "Gradient Descent(29/49): loss=1.469661899753037e+33\n",
      "Gradient Descent(30/49): loss=2.7987398773755453e+34\n",
      "Gradient Descent(31/49): loss=5.32975979205032e+35\n",
      "Gradient Descent(32/49): loss=1.0149689033478238e+37\n",
      "Gradient Descent(33/49): loss=1.932848599105044e+38\n",
      "Gradient Descent(34/49): loss=3.680806076658745e+39\n",
      "Gradient Descent(35/49): loss=7.00951609983389e+40\n",
      "Gradient Descent(36/49): loss=1.3348520658396343e+42\n",
      "Gradient Descent(37/49): loss=2.5420157572910874e+43\n",
      "Gradient Descent(38/49): loss=4.840869093798459e+44\n",
      "Gradient Descent(39/49): loss=9.21867361210446e+45\n",
      "Gradient Descent(40/49): loss=1.755551359060377e+47\n",
      "Gradient Descent(41/49): loss=3.3431713758169216e+48\n",
      "Gradient Descent(42/49): loss=6.366543929574289e+49\n",
      "Gradient Descent(43/49): loss=1.2124081314047425e+51\n",
      "Gradient Descent(44/49): loss=2.3088405473307184e+52\n",
      "Gradient Descent(45/49): loss=4.396823590107282e+53\n",
      "Gradient Descent(46/49): loss=8.373058808618816e+54\n",
      "Gradient Descent(47/49): loss=1.5945173231496686e+56\n",
      "Gradient Descent(48/49): loss=3.0365073886824713e+57\n",
      "Gradient Descent(49/49): loss=5.782550611184383e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6278365058364862\n",
      "Gradient Descent(2/49): loss=1.18287122484733\n",
      "Gradient Descent(3/49): loss=3.4466982444563534\n",
      "Gradient Descent(4/49): loss=18.68750855171384\n",
      "Gradient Descent(5/49): loss=219.2958668371797\n",
      "Gradient Descent(6/49): loss=3791.3061464161406\n",
      "Gradient Descent(7/49): loss=71904.22390706137\n",
      "Gradient Descent(8/49): loss=1386847.661630832\n",
      "Gradient Descent(9/49): loss=26826156.34105\n",
      "Gradient Descent(10/49): loss=519160699.1214781\n",
      "Gradient Descent(11/49): loss=10048043073.518484\n",
      "Gradient Descent(12/49): loss=194476594803.05655\n",
      "Gradient Descent(13/49): loss=3764040126286.1274\n",
      "Gradient Descent(14/49): loss=72851974327282.77\n",
      "Gradient Descent(15/49): loss=1410030274886750.8\n",
      "Gradient Descent(16/49): loss=2.7290755234195504e+16\n",
      "Gradient Descent(17/49): loss=5.2820519957639354e+17\n",
      "Gradient Descent(18/49): loss=1.0223269036014959e+19\n",
      "Gradient Descent(19/49): loss=1.9786861218453576e+20\n",
      "Gradient Descent(20/49): loss=3.829693569668899e+21\n",
      "Gradient Descent(21/49): loss=7.412268512759044e+22\n",
      "Gradient Descent(22/49): loss=1.4346245595311603e+24\n",
      "Gradient Descent(23/49): loss=2.7766771040031057e+25\n",
      "Gradient Descent(24/49): loss=5.3741835720523544e+26\n",
      "Gradient Descent(25/49): loss=1.0401587215336846e+28\n",
      "Gradient Descent(26/49): loss=2.0131991240659905e+29\n",
      "Gradient Descent(27/49): loss=3.8964925537172394e+30\n",
      "Gradient Descent(28/49): loss=7.541556142996131e+31\n",
      "Gradient Descent(29/49): loss=1.4596478313221464e+33\n",
      "Gradient Descent(30/49): loss=2.8251089709941012e+34\n",
      "Gradient Descent(31/49): loss=5.467922143084386e+35\n",
      "Gradient Descent(32/49): loss=1.0583015688882335e+37\n",
      "Gradient Descent(33/49): loss=2.0483141153130695e+38\n",
      "Gradient Descent(34/49): loss=3.964456671266601e+39\n",
      "Gradient Descent(35/49): loss=7.673098857666248e+40\n",
      "Gradient Descent(36/49): loss=1.4851075686169476e+42\n",
      "Gradient Descent(37/49): loss=2.8743856051844605e+43\n",
      "Gradient Descent(38/49): loss=5.563295738224399e+44\n",
      "Gradient Descent(39/49): loss=1.0767608707447574e+46\n",
      "Gradient Descent(40/49): loss=2.0840415957053474e+47\n",
      "Gradient Descent(41/49): loss=4.033606245020756e+48\n",
      "Gradient Descent(42/49): loss=7.806935990816611e+49\n",
      "Gradient Descent(43/49): loss=1.51101138441422e+51\n",
      "Gradient Descent(44/49): loss=2.9245217413272577e+52\n",
      "Gradient Descent(45/49): loss=5.660332876189195e+53\n",
      "Gradient Descent(46/49): loss=1.0955421468238876e+55\n",
      "Gradient Descent(47/49): loss=2.120392248513045e+56\n",
      "Gradient Descent(48/49): loss=4.103961952161069e+57\n",
      "Gradient Descent(49/49): loss=7.943107562573518e+58\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.638901426163955\n",
      "Gradient Descent(2/49): loss=1.2638843036325658\n",
      "Gradient Descent(3/49): loss=4.2612864227296905\n",
      "Gradient Descent(4/49): loss=31.459776596101303\n",
      "Gradient Descent(5/49): loss=467.40459540342016\n",
      "Gradient Descent(6/49): loss=8886.024254676355\n",
      "Gradient Descent(7/49): loss=177568.20925127613\n",
      "Gradient Descent(8/49): loss=3579045.4868909055\n",
      "Gradient Descent(9/49): loss=72243639.24972099\n",
      "Gradient Descent(10/49): loss=1458605167.8734503\n",
      "Gradient Descent(11/49): loss=29450563506.17413\n",
      "Gradient Descent(12/49): loss=594637703496.7333\n",
      "Gradient Descent(13/49): loss=12006371428135.65\n",
      "Gradient Descent(14/49): loss=242421531232734.5\n",
      "Gradient Descent(15/49): loss=4894751178843322.0\n",
      "Gradient Descent(16/49): loss=9.8830285868518e+16\n",
      "Gradient Descent(17/49): loss=1.9954896698185667e+18\n",
      "Gradient Descent(18/49): loss=4.029108069419022e+19\n",
      "Gradient Descent(19/49): loss=8.13520214161171e+20\n",
      "Gradient Descent(20/49): loss=1.6425847295456223e+22\n",
      "Gradient Descent(21/49): loss=3.3165550735827176e+23\n",
      "Gradient Descent(22/49): loss=6.696481075378843e+24\n",
      "Gradient Descent(23/49): loss=1.3520914864370707e+26\n",
      "Gradient Descent(24/49): loss=2.730018000673882e+27\n",
      "Gradient Descent(25/49): loss=5.512199698589008e+28\n",
      "Gradient Descent(26/49): loss=1.1129723507180014e+30\n",
      "Gradient Descent(27/49): loss=2.247210771010076e+31\n",
      "Gradient Descent(28/49): loss=4.5373600216447254e+32\n",
      "Gradient Descent(29/49): loss=9.16141744762368e+33\n",
      "Gradient Descent(30/49): loss=1.8497886270704285e+35\n",
      "Gradient Descent(31/49): loss=3.734922007867434e+36\n",
      "Gradient Descent(32/49): loss=7.541208871494334e+37\n",
      "Gradient Descent(33/49): loss=1.5226511055307003e+39\n",
      "Gradient Descent(34/49): loss=3.074396199179234e+40\n",
      "Gradient Descent(35/49): loss=6.207536286675099e+41\n",
      "Gradient Descent(36/49): loss=1.2533682796210799e+43\n",
      "Gradient Descent(37/49): loss=2.530685237768771e+44\n",
      "Gradient Descent(38/49): loss=5.1097254308981706e+45\n",
      "Gradient Descent(39/49): loss=1.031708471267136e+47\n",
      "Gradient Descent(40/49): loss=2.0831302661545346e+48\n",
      "Gradient Descent(41/49): loss=4.206063851001945e+49\n",
      "Gradient Descent(42/49): loss=8.49249487952706e+50\n",
      "Gradient Descent(43/49): loss=1.7147259726362063e+52\n",
      "Gradient Descent(44/49): loss=3.462215995350649e+53\n",
      "Gradient Descent(45/49): loss=6.990586128483842e+54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=1.4114744569771255e+56\n",
      "Gradient Descent(47/49): loss=2.8499185992161694e+57\n",
      "Gradient Descent(48/49): loss=5.754291891015106e+58\n",
      "Gradient Descent(49/49): loss=1.1618533657806473e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6417144724267221\n",
      "Gradient Descent(2/49): loss=1.2855154835633535\n",
      "Gradient Descent(3/49): loss=4.268342463638571\n",
      "Gradient Descent(4/49): loss=28.890876631630174\n",
      "Gradient Descent(5/49): loss=394.61606104825734\n",
      "Gradient Descent(6/49): loss=7155.5642987244255\n",
      "Gradient Descent(7/49): loss=138159.27448912335\n",
      "Gradient Descent(8/49): loss=2698443.414329989\n",
      "Gradient Descent(9/49): loss=52811549.18007379\n",
      "Gradient Descent(10/49): loss=1033949571.1575518\n",
      "Gradient Descent(11/49): loss=20244029643.911804\n",
      "Gradient Descent(12/49): loss=396368683161.23944\n",
      "Gradient Descent(13/49): loss=7760729352748.844\n",
      "Gradient Descent(14/49): loss=151951813936556.38\n",
      "Gradient Descent(15/49): loss=2975152730404709.5\n",
      "Gradient Descent(16/49): loss=5.8252242152911336e+16\n",
      "Gradient Descent(17/49): loss=1.1405544620726391e+18\n",
      "Gradient Descent(18/49): loss=2.2331577863626154e+19\n",
      "Gradient Descent(19/49): loss=4.372429256914395e+20\n",
      "Gradient Descent(20/49): loss=8.561033046414677e+21\n",
      "Gradient Descent(21/49): loss=1.6762143539779566e+23\n",
      "Gradient Descent(22/49): loss=3.2819573820699747e+24\n",
      "Gradient Descent(23/49): loss=6.425934864572563e+25\n",
      "Gradient Descent(24/49): loss=1.2581710874528985e+27\n",
      "Gradient Descent(25/49): loss=2.4634462045822027e+28\n",
      "Gradient Descent(26/49): loss=4.8233243184407526e+29\n",
      "Gradient Descent(27/49): loss=9.443866660286121e+30\n",
      "Gradient Descent(28/49): loss=1.849069471780702e+32\n",
      "Gradient Descent(29/49): loss=3.6204004508549653e+33\n",
      "Gradient Descent(30/49): loss=7.088592194390673e+34\n",
      "Gradient Descent(31/49): loss=1.3879166125534236e+36\n",
      "Gradient Descent(32/49): loss=2.7174824994533136e+37\n",
      "Gradient Descent(33/49): loss=5.3207167260927965e+38\n",
      "Gradient Descent(34/49): loss=1.0417740127128076e+40\n",
      "Gradient Descent(35/49): loss=2.0397498108506104e+41\n",
      "Gradient Descent(36/49): loss=3.9937445550507365e+42\n",
      "Gradient Descent(37/49): loss=7.819584287322907e+43\n",
      "Gradient Descent(38/49): loss=1.5310417975836617e+45\n",
      "Gradient Descent(39/49): loss=2.99771560714345e+46\n",
      "Gradient Descent(40/49): loss=5.869401394197105e+47\n",
      "Gradient Descent(41/49): loss=1.1492041688047331e+49\n",
      "Gradient Descent(42/49): loss=2.250093549410106e+50\n",
      "Gradient Descent(43/49): loss=4.405588770499348e+51\n",
      "Gradient Descent(44/49): loss=8.625957982875144e+52\n",
      "Gradient Descent(45/49): loss=1.688926384154821e+54\n",
      "Gradient Descent(46/49): loss=3.306847003842577e+55\n",
      "Gradient Descent(47/49): loss=6.474667699797334e+56\n",
      "Gradient Descent(48/49): loss=1.2677127721387444e+58\n",
      "Gradient Descent(49/49): loss=2.4821284228903227e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6456580634066582\n",
      "Gradient Descent(2/49): loss=1.2948557933095122\n",
      "Gradient Descent(3/49): loss=4.219704233127036\n",
      "Gradient Descent(4/49): loss=27.308109215620654\n",
      "Gradient Descent(5/49): loss=362.22154638320023\n",
      "Gradient Descent(6/49): loss=6520.473614153681\n",
      "Gradient Descent(7/49): loss=125760.49755458653\n",
      "Gradient Descent(8/49): loss=2456427.556747784\n",
      "Gradient Descent(9/49): loss=48087279.553211704\n",
      "Gradient Descent(10/49): loss=941727657.5383753\n",
      "Gradient Descent(11/49): loss=18443779363.01695\n",
      "Gradient Descent(12/49): loss=361226543602.894\n",
      "Gradient Descent(13/49): loss=7074736853078.098\n",
      "Gradient Descent(14/49): loss=138561023509939.78\n",
      "Gradient Descent(15/49): loss=2713762906512993.0\n",
      "Gradient Descent(16/49): loss=5.31499335611134e+16\n",
      "Gradient Descent(17/49): loss=1.0409588237020177e+18\n",
      "Gradient Descent(18/49): loss=2.0387518862276653e+19\n",
      "Gradient Descent(19/49): loss=3.9929622182917964e+20\n",
      "Gradient Descent(20/49): loss=7.820347039074742e+21\n",
      "Gradient Descent(21/49): loss=1.5316405332238724e+23\n",
      "Gradient Descent(22/49): loss=2.9997680554246285e+24\n",
      "Gradient Descent(23/49): loss=5.875143802446707e+25\n",
      "Gradient Descent(24/49): loss=1.1506661202358196e+27\n",
      "Gradient Descent(25/49): loss=2.2536172130921805e+28\n",
      "Gradient Descent(26/49): loss=4.413782985202137e+29\n",
      "Gradient Descent(27/49): loss=8.644538268204507e+30\n",
      "Gradient Descent(28/49): loss=1.6930610798263162e+32\n",
      "Gradient Descent(29/49): loss=3.3159154729705674e+33\n",
      "Gradient Descent(30/49): loss=6.494328854936254e+34\n",
      "Gradient Descent(31/49): loss=1.2719355369537013e+36\n",
      "Gradient Descent(32/49): loss=2.4911273301721264e+37\n",
      "Gradient Descent(33/49): loss=4.8789543139844184e+38\n",
      "Gradient Descent(34/49): loss=9.555591522614851e+39\n",
      "Gradient Descent(35/49): loss=1.8714938380412918e+41\n",
      "Gradient Descent(36/49): loss=3.66538186310857e+42\n",
      "Gradient Descent(37/49): loss=7.178770204483243e+43\n",
      "Gradient Descent(38/49): loss=1.4059856127806555e+45\n",
      "Gradient Descent(39/49): loss=2.753668785931643e+46\n",
      "Gradient Descent(40/49): loss=5.393150337874294e+47\n",
      "Gradient Descent(41/49): loss=1.0562661245067825e+49\n",
      "Gradient Descent(42/49): loss=2.0687317354115315e+50\n",
      "Gradient Descent(43/49): loss=4.051678733043804e+51\n",
      "Gradient Descent(44/49): loss=7.935345252744336e+52\n",
      "Gradient Descent(45/49): loss=1.5541633093142106e+54\n",
      "Gradient Descent(46/49): loss=3.0438796486934923e+55\n",
      "Gradient Descent(47/49): loss=5.961537799923039e+56\n",
      "Gradient Descent(48/49): loss=1.1675866670735574e+58\n",
      "Gradient Descent(49/49): loss=2.286756657226177e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6367327886081697\n",
      "Gradient Descent(2/49): loss=1.2418001273950865\n",
      "Gradient Descent(3/49): loss=3.7788402968370893\n",
      "Gradient Descent(4/49): loss=21.141739201689866\n",
      "Gradient Descent(5/49): loss=253.41088215682532\n",
      "Gradient Descent(6/49): loss=4490.123895764752\n",
      "Gradient Descent(7/49): loss=87502.86008821275\n",
      "Gradient Descent(8/49): loss=1735351.7786081594\n",
      "Gradient Descent(9/49): loss=34519907.3929496\n",
      "Gradient Descent(10/49): loss=687032963.9786339\n",
      "Gradient Descent(11/49): loss=13674901582.367012\n",
      "Gradient Descent(12/49): loss=272193322798.12476\n",
      "Gradient Descent(13/49): loss=5417910908862.05\n",
      "Gradient Descent(14/49): loss=107841630503439.11\n",
      "Gradient Descent(15/49): loss=2146550276626685.5\n",
      "Gradient Descent(16/49): loss=4.272633980950061e+16\n",
      "Gradient Descent(17/49): loss=8.504529977673609e+17\n",
      "Gradient Descent(18/49): loss=1.6927972415009597e+19\n",
      "Gradient Descent(19/49): loss=3.3694542891197025e+20\n",
      "Gradient Descent(20/49): loss=6.706782081313179e+21\n",
      "Gradient Descent(21/49): loss=1.3349617483032276e+23\n",
      "Gradient Descent(22/49): loss=2.657195131475764e+24\n",
      "Gradient Descent(23/49): loss=5.289054892930947e+25\n",
      "Gradient Descent(24/49): loss=1.0527680609176753e+27\n",
      "Gradient Descent(25/49): loss=2.0954983688479304e+28\n",
      "Gradient Descent(26/49): loss=4.171016937972663e+29\n",
      "Gradient Descent(27/49): loss=8.30226477647878e+30\n",
      "Gradient Descent(28/49): loss=1.652537053763771e+32\n",
      "Gradient Descent(29/49): loss=3.2893177796486573e+33\n",
      "Gradient Descent(30/49): loss=6.547273134281759e+34\n",
      "Gradient Descent(31/49): loss=1.3032120447622783e+36\n",
      "Gradient Descent(32/49): loss=2.5939984460412283e+37\n",
      "Gradient Descent(33/49): loss=5.163264079017794e+38\n",
      "Gradient Descent(34/49): loss=1.0277298350105361e+40\n",
      "Gradient Descent(35/49): loss=2.0456606472309497e+41\n",
      "Gradient Descent(36/49): loss=4.0718166789294003e+42\n",
      "Gradient Descent(37/49): loss=8.104810096068877e+43\n",
      "Gradient Descent(38/49): loss=1.6132343833959308e+45\n",
      "Gradient Descent(39/49): loss=3.2110871752974583e+46\n",
      "Gradient Descent(40/49): loss=6.391557825376026e+47\n",
      "Gradient Descent(41/49): loss=1.272217451752656e+49\n",
      "Gradient Descent(42/49): loss=2.5323047819703793e+50\n",
      "Gradient Descent(43/49): loss=5.04046497708073e+51\n",
      "Gradient Descent(44/49): loss=1.0032870990121784e+53\n",
      "Gradient Descent(45/49): loss=1.9970082276560743e+54\n",
      "Gradient Descent(46/49): loss=3.9749757225551013e+55\n",
      "Gradient Descent(47/49): loss=7.912051525920508e+56\n",
      "Gradient Descent(48/49): loss=1.574866457513414e+58\n",
      "Gradient Descent(49/49): loss=3.134717147474943e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.648063188427126\n",
      "Gradient Descent(2/49): loss=1.3273497579403413\n",
      "Gradient Descent(3/49): loss=4.664389559726163\n",
      "Gradient Descent(4/49): loss=35.40023157286644\n",
      "Gradient Descent(5/49): loss=537.8591444796219\n",
      "Gradient Descent(6/49): loss=10491.656833307807\n",
      "Gradient Descent(7/49): loss=215426.46427651087\n",
      "Gradient Descent(8/49): loss=4463209.622945476\n",
      "Gradient Descent(9/49): loss=92609653.58705871\n",
      "Gradient Descent(10/49): loss=1922104014.3543258\n",
      "Gradient Descent(11/49): loss=39894806991.66017\n",
      "Gradient Descent(12/49): loss=828054716974.677\n",
      "Gradient Descent(13/49): loss=17187085616706.643\n",
      "Gradient Descent(14/49): loss=356734848218187.1\n",
      "Gradient Descent(15/49): loss=7404382526875952.0\n",
      "Gradient Descent(16/49): loss=1.5368524046694618e+17\n",
      "Gradient Descent(17/49): loss=3.1898882959446216e+18\n",
      "Gradient Descent(18/49): loss=6.620926844646871e+19\n",
      "Gradient Descent(19/49): loss=1.3742384753458495e+21\n",
      "Gradient Descent(20/49): loss=2.8523670951853713e+22\n",
      "Gradient Descent(21/49): loss=5.920368401601666e+23\n",
      "Gradient Descent(22/49): loss=1.2288306813611737e+25\n",
      "Gradient Descent(23/49): loss=2.5505589196883523e+26\n",
      "Gradient Descent(24/49): loss=5.293935854202487e+27\n",
      "Gradient Descent(25/49): loss=1.098808445947744e+29\n",
      "Gradient Descent(26/49): loss=2.280684983985364e+30\n",
      "Gradient Descent(27/49): loss=4.733785961837759e+31\n",
      "Gradient Descent(28/49): loss=9.825438273958848e+32\n",
      "Gradient Descent(29/49): loss=2.0393663349724754e+34\n",
      "Gradient Descent(30/49): loss=4.23290537506311e+35\n",
      "Gradient Descent(31/49): loss=8.785811360605922e+36\n",
      "Gradient Descent(32/49): loss=1.823581545642346e+38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=3.7850228250039704e+39\n",
      "Gradient Descent(34/49): loss=7.856187084167152e+40\n",
      "Gradient Descent(35/49): loss=1.6306288853454493e+42\n",
      "Gradient Descent(36/49): loss=3.3845306040148275e+43\n",
      "Gradient Descent(37/49): loss=7.024926095974683e+44\n",
      "Gradient Descent(38/49): loss=1.458092492807269e+46\n",
      "Gradient Descent(39/49): loss=3.0264143544502147e+47\n",
      "Gradient Descent(40/49): loss=6.281620603633833e+48\n",
      "Gradient Descent(41/49): loss=1.3038121283681918e+50\n",
      "Gradient Descent(42/49): loss=2.706190286463004e+51\n",
      "Gradient Descent(43/49): loss=5.616964060391701e+52\n",
      "Gradient Descent(44/49): loss=1.1658561267311262e+54\n",
      "Gradient Descent(45/49): loss=2.4198490387736e+55\n",
      "Gradient Descent(46/49): loss=5.022634642639868e+56\n",
      "Gradient Descent(47/49): loss=1.0424972115711361e+58\n",
      "Gradient Descent(48/49): loss=2.1638054795130263e+59\n",
      "Gradient Descent(49/49): loss=4.491191056630517e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6509706448715878\n",
      "Gradient Descent(2/49): loss=1.3505003754852183\n",
      "Gradient Descent(3/49): loss=4.674737776781837\n",
      "Gradient Descent(4/49): loss=32.553757786042794\n",
      "Gradient Descent(5/49): loss=454.55093469375356\n",
      "Gradient Descent(6/49): loss=8453.924634876088\n",
      "Gradient Descent(7/49): loss=167713.18785978053\n",
      "Gradient Descent(8/49): loss=3367177.9542964515\n",
      "Gradient Descent(9/49): loss=67746768.14191264\n",
      "Gradient Descent(10/49): loss=1363560432.7209442\n",
      "Gradient Descent(11/49): loss=27446630745.433\n",
      "Gradient Descent(12/49): loss=552470103608.8098\n",
      "Gradient Descent(13/49): loss=11120630808837.35\n",
      "Gradient Descent(14/49): loss=223846456132740.56\n",
      "Gradient Descent(15/49): loss=4505791085094839.0\n",
      "Gradient Descent(16/49): loss=9.069678334335832e+16\n",
      "Gradient Descent(17/49): loss=1.8256298106251428e+18\n",
      "Gradient Descent(18/49): loss=3.674798690501353e+19\n",
      "Gradient Descent(19/49): loss=7.396979024972098e+20\n",
      "Gradient Descent(20/49): loss=1.4889332261359867e+22\n",
      "Gradient Descent(21/49): loss=2.997064266928106e+23\n",
      "Gradient Descent(22/49): loss=6.032771693470228e+24\n",
      "Gradient Descent(23/49): loss=1.214332795834277e+26\n",
      "Gradient Descent(24/49): loss=2.4443227988136897e+27\n",
      "Gradient Descent(25/49): loss=4.920161890790008e+28\n",
      "Gradient Descent(26/49): loss=9.903762728609688e+29\n",
      "Gradient Descent(27/49): loss=1.9935221312169073e+31\n",
      "Gradient Descent(28/49): loss=4.0127480802536254e+32\n",
      "Gradient Descent(29/49): loss=8.077235212708764e+33\n",
      "Gradient Descent(30/49): loss=1.6258615636120072e+35\n",
      "Gradient Descent(31/49): loss=3.2726864507692523e+36\n",
      "Gradient Descent(32/49): loss=6.587569842819005e+37\n",
      "Gradient Descent(33/49): loss=1.3260077641662676e+39\n",
      "Gradient Descent(34/49): loss=2.6691126357406516e+40\n",
      "Gradient Descent(35/49): loss=5.372639930769778e+41\n",
      "Gradient Descent(36/49): loss=1.0814552911399187e+43\n",
      "Gradient Descent(37/49): loss=2.1768545106409037e+44\n",
      "Gradient Descent(38/49): loss=4.3817766664237666e+45\n",
      "Gradient Descent(39/49): loss=8.820050518104523e+46\n",
      "Gradient Descent(40/49): loss=1.7753823862823123e+48\n",
      "Gradient Descent(41/49): loss=3.5736559683547385e+49\n",
      "Gradient Descent(42/49): loss=7.193389479829274e+50\n",
      "Gradient Descent(43/49): loss=1.4479528154563896e+52\n",
      "Gradient Descent(44/49): loss=2.9145750576511664e+53\n",
      "Gradient Descent(45/49): loss=5.8667296862190106e+54\n",
      "Gradient Descent(46/49): loss=1.1809103052882544e+56\n",
      "Gradient Descent(47/49): loss=2.377046879135748e+57\n",
      "Gradient Descent(48/49): loss=4.7847426178821906e+58\n",
      "Gradient Descent(49/49): loss=9.63117813128807e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.655121838610841\n",
      "Gradient Descent(2/49): loss=1.3607170782412725\n",
      "Gradient Descent(3/49): loss=4.623119513018296\n",
      "Gradient Descent(4/49): loss=30.789742969687538\n",
      "Gradient Descent(5/49): loss=417.35943884929486\n",
      "Gradient Descent(6/49): loss=7704.1200059837665\n",
      "Gradient Descent(7/49): loss=152663.59928813914\n",
      "Gradient Descent(8/49): loss=3065175.526517746\n",
      "Gradient Descent(9/49): loss=61686084.740393184\n",
      "Gradient Descent(10/49): loss=1241930156.71733\n",
      "Gradient Descent(11/49): loss=25005667403.2153\n",
      "Gradient Descent(12/49): loss=503483487121.65173\n",
      "Gradient Descent(13/49): loss=10137549313980.943\n",
      "Gradient Descent(14/49): loss=204117809135701.8\n",
      "Gradient Descent(15/49): loss=4109877208279615.0\n",
      "Gradient Descent(16/49): loss=8.275167631356581e+16\n",
      "Gradient Descent(17/49): loss=1.6661908860604175e+18\n",
      "Gradient Descent(18/49): loss=3.3548469269775163e+19\n",
      "Gradient Descent(19/49): loss=6.754927060489145e+20\n",
      "Gradient Descent(20/49): loss=1.3600930410903466e+22\n",
      "Gradient Descent(21/49): loss=2.738524137804943e+23\n",
      "Gradient Descent(22/49): loss=5.513971637801341e+24\n",
      "Gradient Descent(23/49): loss=1.1102287835537986e+26\n",
      "Gradient Descent(24/49): loss=2.2354267174338263e+27\n",
      "Gradient Descent(25/49): loss=4.50099356370621e+28\n",
      "Gradient Descent(26/49): loss=9.06267376269962e+29\n",
      "Gradient Descent(27/49): loss=1.8247539030358698e+31\n",
      "Gradient Descent(28/49): loss=3.6741108571614685e+32\n",
      "Gradient Descent(29/49): loss=7.397759538013794e+33\n",
      "Gradient Descent(30/49): loss=1.4895262639014272e+35\n",
      "Gradient Descent(31/49): loss=2.9991357240679704e+36\n",
      "Gradient Descent(32/49): loss=6.03870862123713e+37\n",
      "Gradient Descent(33/49): loss=1.2158836800737193e+39\n",
      "Gradient Descent(34/49): loss=2.448161049318436e+40\n",
      "Gradient Descent(35/49): loss=4.929330512139626e+41\n",
      "Gradient Descent(36/49): loss=9.925122901810507e+42\n",
      "Gradient Descent(37/49): loss=1.9984065660325046e+44\n",
      "Gradient Descent(38/49): loss=4.023757531942949e+45\n",
      "Gradient Descent(39/49): loss=8.101767153423208e+46\n",
      "Gradient Descent(40/49): loss=1.631276996369896e+48\n",
      "Gradient Descent(41/49): loss=3.2845484059131364e+49\n",
      "Gradient Descent(42/49): loss=6.6133821875708695e+50\n",
      "Gradient Descent(43/49): loss=1.3315932223784748e+52\n",
      "Gradient Descent(44/49): loss=2.6811402389788746e+53\n",
      "Gradient Descent(45/49): loss=5.398430136368475e+54\n",
      "Gradient Descent(46/49): loss=1.0869646993307086e+56\n",
      "Gradient Descent(47/49): loss=2.188584880688791e+57\n",
      "Gradient Descent(48/49): loss=4.406678324446859e+58\n",
      "Gradient Descent(49/49): loss=8.87277163727772e+59\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6457978645081971\n",
      "Gradient Descent(2/49): loss=1.3039168750763028\n",
      "Gradient Descent(3/49): loss=4.140547541176112\n",
      "Gradient Descent(4/49): loss=23.88315680061589\n",
      "Gradient Descent(5/49): loss=292.2762483411626\n",
      "Gradient Descent(6/49): loss=5305.488730461945\n",
      "Gradient Descent(7/49): loss=106197.7522508436\n",
      "Gradient Descent(8/49): loss=2164721.5273805168\n",
      "Gradient Descent(9/49): loss=44265773.94993568\n",
      "Gradient Descent(10/49): loss=905675087.1636168\n",
      "Gradient Descent(11/49): loss=18531812342.419712\n",
      "Gradient Descent(12/49): loss=379201857668.55115\n",
      "Gradient Descent(13/49): loss=7759330336955.728\n",
      "Gradient Descent(14/49): loss=158773580069449.94\n",
      "Gradient Descent(15/49): loss=3248869521491167.0\n",
      "Gradient Descent(16/49): loss=6.647928021448197e+16\n",
      "Gradient Descent(17/49): loss=1.3603176980813527e+18\n",
      "Gradient Descent(18/49): loss=2.7835202705834177e+19\n",
      "Gradient Descent(19/49): loss=5.6957173373916224e+20\n",
      "Gradient Descent(20/49): loss=1.165473674854112e+22\n",
      "Gradient Descent(21/49): loss=2.3848249593843746e+23\n",
      "Gradient Descent(22/49): loss=4.879895796545224e+24\n",
      "Gradient Descent(23/49): loss=9.985379803844784e+25\n",
      "Gradient Descent(24/49): loss=2.0432364538935797e+27\n",
      "Gradient Descent(25/49): loss=4.180927804981507e+28\n",
      "Gradient Descent(26/49): loss=8.555131872846924e+29\n",
      "Gradient Descent(27/49): loss=1.7505751061904777e+31\n",
      "Gradient Descent(28/49): loss=3.582075937531949e+32\n",
      "Gradient Descent(29/49): loss=7.329744366220443e+33\n",
      "Gradient Descent(30/49): loss=1.4998328737597276e+35\n",
      "Gradient Descent(31/49): loss=3.069000140819708e+36\n",
      "Gradient Descent(32/49): loss=6.27987426408423e+37\n",
      "Gradient Descent(33/49): loss=1.2850055054795307e+39\n",
      "Gradient Descent(34/49): loss=2.629414347603203e+40\n",
      "Gradient Descent(35/49): loss=5.3803814706627314e+41\n",
      "Gradient Descent(36/49): loss=1.1009487643603041e+43\n",
      "Gradient Descent(37/49): loss=2.2527922757068062e+44\n",
      "Gradient Descent(38/49): loss=4.60972681179493e+45\n",
      "Gradient Descent(39/49): loss=9.432552441043352e+46\n",
      "Gradient Descent(40/49): loss=1.9301153666064372e+48\n",
      "Gradient Descent(41/49): loss=3.9494562597928344e+49\n",
      "Gradient Descent(42/49): loss=8.081488297480379e+50\n",
      "Gradient Descent(43/49): loss=1.6536568278322184e+52\n",
      "Gradient Descent(44/49): loss=3.3837590349400367e+53\n",
      "Gradient Descent(45/49): loss=6.923942751500689e+54\n",
      "Gradient Descent(46/49): loss=1.4167966078857834e+56\n",
      "Gradient Descent(47/49): loss=2.8990890019730167e+57\n",
      "Gradient Descent(48/49): loss=5.932197320759157e+58\n",
      "Gradient Descent(49/49): loss=1.2138628730774903e+60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6573953627569454\n",
      "Gradient Descent(2/49): loss=1.394176871353476\n",
      "Gradient Descent(3/49): loss=5.102054819858434\n",
      "Gradient Descent(4/49): loss=39.7767468062136\n",
      "Gradient Descent(5/49): loss=617.7844656917841\n",
      "Gradient Descent(6/49): loss=12359.62043798182\n",
      "Gradient Descent(7/49): loss=260670.65211491103\n",
      "Gradient Descent(8/49): loss=5549103.687575896\n",
      "Gradient Descent(9/49): loss=118316447.18279485\n",
      "Gradient Descent(10/49): loss=2523394210.268279\n",
      "Gradient Descent(11/49): loss=53820174662.69382\n",
      "Gradient Descent(12/49): loss=1147911767271.293\n",
      "Gradient Descent(13/49): loss=24483443065532.363\n",
      "Gradient Descent(14/49): loss=522199647517013.3\n",
      "Gradient Descent(15/49): loss=1.1137832275119834e+16\n",
      "Gradient Descent(16/49): loss=2.3755532810487654e+17\n",
      "Gradient Descent(17/49): loss=5.066743023236964e+18\n",
      "Gradient Descent(18/49): loss=1.0806697148487587e+20\n",
      "Gradient Descent(19/49): loss=2.3049265125012343e+21\n",
      "Gradient Descent(20/49): loss=4.916105406732607e+22\n",
      "Gradient Descent(21/49): loss=1.0485406905186704e+24\n",
      "Gradient Descent(22/49): loss=2.2363995250546595e+25\n",
      "Gradient Descent(23/49): loss=4.7699463462794546e+26\n",
      "Gradient Descent(24/49): loss=1.0173668833089439e+28\n",
      "Gradient Descent(25/49): loss=2.169909890204733e+29\n",
      "Gradient Descent(26/49): loss=4.628132691221467e+30\n",
      "Gradient Descent(27/49): loss=9.87119893975508e+31\n",
      "Gradient Descent(28/49): loss=2.1053970361101568e+33\n",
      "Gradient Descent(29/49): loss=4.490535249785318e+34\n",
      "Gradient Descent(30/49): loss=9.577721676107331e+35\n",
      "Gradient Descent(31/49): loss=2.0428021917735062e+37\n",
      "Gradient Descent(32/49): loss=4.357028671155394e+38\n",
      "Gradient Descent(33/49): loss=9.292969685326794e+39\n",
      "Gradient Descent(34/49): loss=1.9820683334980582e+41\n",
      "Gradient Descent(35/49): loss=4.2274913312790126e+42\n",
      "Gradient Descent(36/49): loss=9.01668355928918e+43\n",
      "Gradient Descent(37/49): loss=1.9231401329392712e+45\n",
      "Gradient Descent(38/49): loss=4.1018052220668304e+46\n",
      "Gradient Descent(39/49): loss=8.748611602244718e+47\n",
      "Gradient Descent(40/49): loss=1.865963906700708e+49\n",
      "Gradient Descent(41/49): loss=3.9798558438876655e+50\n",
      "Gradient Descent(42/49): loss=8.48850960152433e+51\n",
      "Gradient Descent(43/49): loss=1.8104875674286935e+53\n",
      "Gradient Descent(44/49): loss=3.861532101259838e+54\n",
      "Gradient Descent(45/49): loss=8.236140605062588e+55\n",
      "Gradient Descent(46/49): loss=1.7566605763611184e+57\n",
      "Gradient Descent(47/49): loss=3.746726201644176e+58\n",
      "Gradient Descent(48/49): loss=7.991274705535898e+59\n",
      "Gradient Descent(49/49): loss=1.704433897286505e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6603986326133694\n",
      "Gradient Descent(2/49): loss=1.4189211726639568\n",
      "Gradient Descent(3/49): loss=5.116137966955011\n",
      "Gradient Descent(4/49): loss=36.627273003189586\n",
      "Gradient Descent(5/49): loss=522.6108708409905\n",
      "Gradient Descent(6/49): loss=9965.348126213974\n",
      "Gradient Descent(7/49): loss=203052.12662517745\n",
      "Gradient Descent(8/49): loss=4188968.8555897367\n",
      "Gradient Descent(9/49): loss=86610853.75300433\n",
      "Gradient Descent(10/49): loss=1791469511.0884366\n",
      "Gradient Descent(11/49): loss=37057583400.33729\n",
      "Gradient Descent(12/49): loss=766567148908.1873\n",
      "Gradient Descent(13/49): loss=15857118773649.889\n",
      "Gradient Descent(14/49): loss=328018642133093.3\n",
      "Gradient Descent(15/49): loss=6785358594567373.0\n",
      "Gradient Descent(16/49): loss=1.4036120487945317e+17\n",
      "Gradient Descent(17/49): loss=2.903496934531531e+18\n",
      "Gradient Descent(18/49): loss=6.006142836553127e+19\n",
      "Gradient Descent(19/49): loss=1.2424243107195144e+21\n",
      "Gradient Descent(20/49): loss=2.5700656975584264e+22\n",
      "Gradient Descent(21/49): loss=5.316410531242626e+23\n",
      "Gradient Descent(22/49): loss=1.099747020613896e+25\n",
      "Gradient Descent(23/49): loss=2.274924974739573e+26\n",
      "Gradient Descent(24/49): loss=4.7058855752161234e+27\n",
      "Gradient Descent(25/49): loss=9.73454478408124e+28\n",
      "Gradient Descent(26/49): loss=2.0136775669249196e+30\n",
      "Gradient Descent(27/49): loss=4.165471969647398e+31\n",
      "Gradient Descent(28/49): loss=8.616650954906709e+32\n",
      "Gradient Descent(29/49): loss=1.7824312399581662e+34\n",
      "Gradient Descent(30/49): loss=3.687118280414449e+35\n",
      "Gradient Descent(31/49): loss=7.627133607737739e+36\n",
      "Gradient Descent(32/49): loss=1.5777407353404607e+38\n",
      "Gradient Descent(33/49): loss=3.263697682478379e+39\n",
      "Gradient Descent(34/49): loss=6.75125026819839e+40\n",
      "Gradient Descent(35/49): loss=1.396556440522868e+42\n",
      "Gradient Descent(36/49): loss=2.8889017797979205e+43\n",
      "Gradient Descent(37/49): loss=5.9759514554203734e+44\n",
      "Gradient Descent(38/49): loss=1.2361789537905373e+46\n",
      "Gradient Descent(39/49): loss=2.557146618733943e+47\n",
      "Gradient Descent(40/49): loss=5.28968626237472e+48\n",
      "Gradient Descent(41/49): loss=1.0942188668168159e+50\n",
      "Gradient Descent(42/49): loss=2.263489494668739e+51\n",
      "Gradient Descent(43/49): loss=4.6822302629275025e+52\n",
      "Gradient Descent(44/49): loss=9.685611657005923e+53\n",
      "Gradient Descent(45/49): loss=2.0035553123710476e+55\n",
      "Gradient Descent(46/49): loss=4.144533181677353e+56\n",
      "Gradient Descent(47/49): loss=8.573337201106254e+57\n",
      "Gradient Descent(48/49): loss=1.773471402975378e+59\n",
      "Gradient Descent(49/49): loss=3.668584057052573e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6647610872967807\n",
      "Gradient Descent(2/49): loss=1.4300658829990474\n",
      "Gradient Descent(3/49): loss=5.061433048804793\n",
      "Gradient Descent(4/49): loss=34.66423771503953\n",
      "Gradient Descent(5/49): loss=479.9928650987592\n",
      "Gradient Descent(6/49): loss=9082.115042595049\n",
      "Gradient Descent(7/49): loss=184833.24722114843\n",
      "Gradient Descent(8/49): loss=3813248.861968784\n",
      "Gradient Descent(9/49): loss=78862102.87251559\n",
      "Gradient Descent(10/49): loss=1631657896.8408742\n",
      "Gradient Descent(11/49): loss=33761603046.390774\n",
      "Gradient Descent(12/49): loss=698590839993.1494\n",
      "Gradient Descent(13/49): loss=14455188323682.838\n",
      "Gradient Descent(14/49): loss=299105779870022.4\n",
      "Gradient Descent(15/49): loss=6189077046708198.0\n",
      "Gradient Descent(16/49): loss=1.2806397527668608e+17\n",
      "Gradient Descent(17/49): loss=2.649891361573037e+18\n",
      "Gradient Descent(18/49): loss=5.483137795689118e+19\n",
      "Gradient Descent(19/49): loss=1.1345672703683364e+21\n",
      "Gradient Descent(20/49): loss=2.3476391419887835e+22\n",
      "Gradient Descent(21/49): loss=4.857719489141674e+23\n",
      "Gradient Descent(22/49): loss=1.0051561252810726e+25\n",
      "Gradient Descent(23/49): loss=2.0798624507827918e+26\n",
      "Gradient Descent(24/49): loss=4.3036377189330957e+27\n",
      "Gradient Descent(25/49): loss=8.905058894088486e+28\n",
      "Gradient Descent(26/49): loss=1.842628935942176e+30\n",
      "Gradient Descent(27/49): loss=3.812755688595578e+31\n",
      "Gradient Descent(28/49): loss=7.889329021898192e+32\n",
      "Gradient Descent(29/49): loss=1.6324547781001743e+34\n",
      "Gradient Descent(30/49): loss=3.3778647019856084e+35\n",
      "Gradient Descent(31/49): loss=6.989455449540215e+36\n",
      "Gradient Descent(32/49): loss=1.4462535297044484e+38\n",
      "Gradient Descent(33/49): loss=2.992578302105391e+39\n",
      "Gradient Descent(34/49): loss=6.1922233621528e+40\n",
      "Gradient Descent(35/49): loss=1.281290789945748e+42\n",
      "Gradient Descent(36/49): loss=2.651238484764518e+43\n",
      "Gradient Descent(37/49): loss=5.4859252546365455e+44\n",
      "Gradient Descent(38/49): loss=1.1351440495603745e+46\n",
      "Gradient Descent(39/49): loss=2.348832609710249e+47\n",
      "Gradient Descent(40/49): loss=4.8601890047126634e+48\n",
      "Gradient Descent(41/49): loss=1.0056671158207047e+50\n",
      "Gradient Descent(42/49): loss=2.08091978905031e+51\n",
      "Gradient Descent(43/49): loss=4.305825556329746e+52\n",
      "Gradient Descent(44/49): loss=8.909585952855767e+53\n",
      "Gradient Descent(45/49): loss=1.8435656719681007e+55\n",
      "Gradient Descent(46/49): loss=3.8146939766262444e+56\n",
      "Gradient Descent(47/49): loss=7.89333971475701e+57\n",
      "Gradient Descent(48/49): loss=1.6332846680315547e+59\n",
      "Gradient Descent(49/49): loss=3.3795819047794904e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6550317335365683\n",
      "Gradient Descent(2/49): loss=1.3693343916549496\n",
      "Gradient Descent(3/49): loss=4.5338994706894375\n",
      "Gradient Descent(4/49): loss=26.940494785178178\n",
      "Gradient Descent(5/49): loss=336.47646218958835\n",
      "Gradient Descent(6/49): loss=6254.851690251549\n",
      "Gradient Descent(7/49): loss=128548.08622597074\n",
      "Gradient Descent(8/49): loss=2692209.969133574\n",
      "Gradient Descent(9/49): loss=56571149.14957855\n",
      "Gradient Descent(10/49): loss=1189411574.7367048\n",
      "Gradient Descent(11/49): loss=25009956428.283497\n",
      "Gradient Descent(12/49): loss=525897714756.642\n",
      "Gradient Descent(13/49): loss=11058365684215.02\n",
      "Gradient Descent(14/49): loss=232530989355345.4\n",
      "Gradient Descent(15/49): loss=4889571161964331.0\n",
      "Gradient Descent(16/49): loss=1.0281600139958691e+17\n",
      "Gradient Descent(17/49): loss=2.1619749100164012e+18\n",
      "Gradient Descent(18/49): loss=4.546116801031406e+19\n",
      "Gradient Descent(19/49): loss=9.559397694480307e+20\n",
      "Gradient Descent(20/49): loss=2.010112988338117e+22\n",
      "Gradient Descent(21/49): loss=4.226787455687665e+23\n",
      "Gradient Descent(22/49): loss=8.887924360081126e+24\n",
      "Gradient Descent(23/49): loss=1.8689181857070033e+26\n",
      "Gradient Descent(24/49): loss=3.9298885131764937e+27\n",
      "Gradient Descent(25/49): loss=8.263616804688603e+28\n",
      "Gradient Descent(26/49): loss=1.7376412197387418e+30\n",
      "Gradient Descent(27/49): loss=3.653844412076263e+31\n",
      "Gradient Descent(28/49): loss=7.683161999154355e+32\n",
      "Gradient Descent(29/49): loss=1.6155854395481926e+34\n",
      "Gradient Descent(30/49): loss=3.397190261987706e+35\n",
      "Gradient Descent(31/49): loss=7.143479628890291e+36\n",
      "Gradient Descent(32/49): loss=1.5021031285575734e+38\n",
      "Gradient Descent(33/49): loss=3.1585640696688037e+39\n",
      "Gradient Descent(34/49): loss=6.641705747449557e+40\n",
      "Gradient Descent(35/49): loss=1.3965920672405615e+42\n",
      "Gradient Descent(36/49): loss=2.9366995113089683e+43\n",
      "Gradient Descent(37/49): loss=6.175177578347669e+44\n",
      "Gradient Descent(38/49): loss=1.2984923373086644e+46\n",
      "Gradient Descent(39/49): loss=2.7304192124956764e+47\n",
      "Gradient Descent(40/49): loss=5.741419384435965e+48\n",
      "Gradient Descent(41/49): loss=1.2072833503777902e+50\n",
      "Gradient Descent(42/49): loss=2.538628500211234e+51\n",
      "Gradient Descent(43/49): loss=5.338129329844593e+52\n",
      "Gradient Descent(44/49): loss=1.1224810853489186e+54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=2.3603095937032213e+55\n",
      "Gradient Descent(46/49): loss=4.963167264770068e+56\n",
      "Gradient Descent(47/49): loss=1.0436355198403051e+58\n",
      "Gradient Descent(48/49): loss=2.1945162034001924e+59\n",
      "Gradient Descent(49/49): loss=4.6145433682852924e+60\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.666897949153413\n",
      "Gradient Descent(2/49): loss=1.4644824038529112\n",
      "Gradient Descent(3/49): loss=5.576632658839202\n",
      "Gradient Descent(4/49): loss=44.63061370025484\n",
      "Gradient Descent(5/49): loss=708.3021367146567\n",
      "Gradient Descent(6/49): loss=14528.36356636502\n",
      "Gradient Descent(7/49): loss=314611.8584548621\n",
      "Gradient Descent(8/49): loss=6879057.104681318\n",
      "Gradient Descent(9/49): loss=150662653.79920503\n",
      "Gradient Descent(10/49): loss=3300701071.438239\n",
      "Gradient Descent(11/49): loss=72314933124.12486\n",
      "Gradient Descent(12/49): loss=1584358345605.8809\n",
      "Gradient Descent(13/49): loss=34711986215849.344\n",
      "Gradient Descent(14/49): loss=760511209987912.0\n",
      "Gradient Descent(15/49): loss=1.6662178914239676e+16\n",
      "Gradient Descent(16/49): loss=3.650547217706914e+17\n",
      "Gradient Descent(17/49): loss=7.99805059932402e+18\n",
      "Gradient Descent(18/49): loss=1.7523075196109567e+20\n",
      "Gradient Descent(19/49): loss=3.839162562506635e+21\n",
      "Gradient Descent(20/49): loss=8.411291406611017e+22\n",
      "Gradient Descent(21/49): loss=1.842845203220848e+24\n",
      "Gradient Descent(22/49): loss=4.037523227842822e+25\n",
      "Gradient Descent(23/49): loss=8.84588341271394e+26\n",
      "Gradient Descent(24/49): loss=1.9380607599163086e+28\n",
      "Gradient Descent(25/49): loss=4.2461327307670625e+29\n",
      "Gradient Descent(26/49): loss=9.302929784342645e+30\n",
      "Gradient Descent(27/49): loss=2.0381958845826257e+32\n",
      "Gradient Descent(28/49): loss=4.465520605048037e+33\n",
      "Gradient Descent(29/49): loss=9.783590686717113e+34\n",
      "Gradient Descent(30/49): loss=2.1435047599380715e+36\n",
      "Gradient Descent(31/49): loss=4.696243744247342e+37\n",
      "Gradient Descent(32/49): loss=1.0289086228116866e+39\n",
      "Gradient Descent(33/49): loss=2.254254701734902e+40\n",
      "Gradient Descent(34/49): loss=4.93888781533125e+41\n",
      "Gradient Descent(35/49): loss=1.0820699556999512e+43\n",
      "Gradient Descent(36/49): loss=2.370726837313239e+44\n",
      "Gradient Descent(37/49): loss=5.194068745325735e+45\n",
      "Gradient Descent(38/49): loss=1.1379780119140497e+47\n",
      "Gradient Descent(39/49): loss=2.4932168192137183e+48\n",
      "Gradient Descent(40/49): loss=5.462434284784415e+49\n",
      "Gradient Descent(41/49): loss=1.1967747083063045e+51\n",
      "Gradient Descent(42/49): loss=2.6220355756612466e+52\n",
      "Gradient Descent(43/49): loss=5.744665652036352e+53\n",
      "Gradient Descent(44/49): loss=1.2586092942451682e+55\n",
      "Gradient Descent(45/49): loss=2.7575100998240937e+56\n",
      "Gradient Descent(46/49): loss=6.0414792623887284e+57\n",
      "Gradient Descent(47/49): loss=1.3236387304692592e+59\n",
      "Gradient Descent(48/49): loss=2.899984279852588e+60\n",
      "Gradient Descent(49/49): loss=6.353628546673464e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6699984356520664\n",
      "Gradient Descent(2/49): loss=1.4908969547282875\n",
      "Gradient Descent(3/49): loss=5.594931375482791\n",
      "Gradient Descent(4/49): loss=41.15079272385615\n",
      "Gradient Descent(5/49): loss=599.7669980666531\n",
      "Gradient Descent(6/49): loss=11721.208155492604\n",
      "Gradient Descent(7/49): loss=245206.44867406972\n",
      "Gradient Descent(8/49): loss=5196032.052823337\n",
      "Gradient Descent(9/49): loss=110362076.54829253\n",
      "Gradient Descent(10/49): loss=2345031120.053334\n",
      "Gradient Descent(11/49): loss=49832159378.852875\n",
      "Gradient Descent(12/49): loss=1058952776138.7625\n",
      "Gradient Descent(13/49): loss=22503212218001.633\n",
      "Gradient Descent(14/49): loss=478203360709259.44\n",
      "Gradient Descent(15/49): loss=1.0162036844173992e+16\n",
      "Gradient Descent(16/49): loss=2.1594786385813843e+17\n",
      "Gradient Descent(17/49): loss=4.588989464823404e+18\n",
      "Gradient Descent(18/49): loss=9.751809506817709e+19\n",
      "Gradient Descent(19/49): loss=2.072303486283474e+21\n",
      "Gradient Descent(20/49): loss=4.403738338326428e+22\n",
      "Gradient Descent(21/49): loss=9.358142511874223e+23\n",
      "Gradient Descent(22/49): loss=1.9886474750420442e+25\n",
      "Gradient Descent(23/49): loss=4.2259655428131784e+26\n",
      "Gradient Descent(24/49): loss=8.980367306511512e+27\n",
      "Gradient Descent(25/49): loss=1.9083685406998475e+29\n",
      "Gradient Descent(26/49): loss=4.055369187953035e+30\n",
      "Gradient Descent(27/49): loss=8.617842361082048e+31\n",
      "Gradient Descent(28/49): loss=1.8313303553491302e+33\n",
      "Gradient Descent(29/49): loss=3.8916595708093246e+34\n",
      "Gradient Descent(30/49): loss=8.269952043788632e+35\n",
      "Gradient Descent(31/49): loss=1.7574020944576857e+37\n",
      "Gradient Descent(32/49): loss=3.7345586833528414e+38\n",
      "Gradient Descent(33/49): loss=7.936105575036268e+39\n",
      "Gradient Descent(34/49): loss=1.6864582146980163e+41\n",
      "Gradient Descent(35/49): loss=3.583799740352271e+42\n",
      "Gradient Descent(36/49): loss=7.615736024179611e+43\n",
      "Gradient Descent(37/49): loss=1.618378240751976e+45\n",
      "Gradient Descent(38/49): loss=3.4391267263253473e+46\n",
      "Gradient Descent(39/49): loss=7.30829934677653e+47\n",
      "Gradient Descent(40/49): loss=1.5530465607228615e+49\n",
      "Gradient Descent(41/49): loss=3.3002939607788433e+50\n",
      "Gradient Descent(42/49): loss=7.013273460702602e+51\n",
      "Gradient Descent(43/49): loss=1.4903522298052798e+53\n",
      "Gradient Descent(44/49): loss=3.167065680999541e+54\n",
      "Gradient Descent(45/49): loss=6.730157359562685e+55\n",
      "Gradient Descent(46/49): loss=1.4301887818815662e+57\n",
      "Gradient Descent(47/49): loss=3.0392156416871363e+58\n",
      "Gradient Descent(48/49): loss=6.458470261893706e+59\n",
      "Gradient Descent(49/49): loss=1.3724540487231918e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6745758094644779\n",
      "Gradient Descent(2/49): loss=1.503023259167349\n",
      "Gradient Descent(3/49): loss=5.5370349052889205\n",
      "Gradient Descent(4/49): loss=38.96950689372666\n",
      "Gradient Descent(5/49): loss=551.0206488393156\n",
      "Gradient Descent(6/49): loss=10683.096885892937\n",
      "Gradient Descent(7/49): loss=223207.5527484695\n",
      "Gradient Descent(8/49): loss=4729973.16371138\n",
      "Gradient Descent(9/49): loss=100487812.07597993\n",
      "Gradient Descent(10/49): loss=2135823131.1720574\n",
      "Gradient Descent(11/49): loss=45399630486.4875\n",
      "Gradient Descent(12/49): loss=965040656869.154\n",
      "Gradient Descent(13/49): loss=20513511913353.586\n",
      "Gradient Descent(14/49): loss=436048326589708.25\n",
      "Gradient Descent(15/49): loss=9268922816344208.0\n",
      "Gradient Descent(16/49): loss=1.97026169311482e+17\n",
      "Gradient Descent(17/49): loss=4.1881146561614633e+18\n",
      "Gradient Descent(18/49): loss=8.902525203901361e+19\n",
      "Gradient Descent(19/49): loss=1.8923778721292967e+21\n",
      "Gradient Descent(20/49): loss=4.022559811914476e+22\n",
      "Gradient Descent(21/49): loss=8.550611206558349e+23\n",
      "Gradient Descent(22/49): loss=1.817572775156773e+25\n",
      "Gradient Descent(23/49): loss=3.863549298625156e+26\n",
      "Gradient Descent(24/49): loss=8.212608258076132e+27\n",
      "Gradient Descent(25/49): loss=1.7457246999441703e+29\n",
      "Gradient Descent(26/49): loss=3.7108244204858204e+30\n",
      "Gradient Descent(27/49): loss=7.8879664589235975e+31\n",
      "Gradient Descent(28/49): loss=1.676716756352393e+33\n",
      "Gradient Descent(29/49): loss=3.5641367083304463e+34\n",
      "Gradient Descent(30/49): loss=7.576157647105095e+35\n",
      "Gradient Descent(31/49): loss=1.6104366748793286e+37\n",
      "Gradient Descent(32/49): loss=3.423247514902636e+38\n",
      "Gradient Descent(33/49): loss=7.276674538702604e+39\n",
      "Gradient Descent(34/49): loss=1.5467766240007942e+41\n",
      "Gradient Descent(35/49): loss=3.2879276265967513e+42\n",
      "Gradient Descent(36/49): loss=6.989029902570033e+43\n",
      "Gradient Descent(37/49): loss=1.4856330347385697e+45\n",
      "Gradient Descent(38/49): loss=3.157956890547844e+46\n",
      "Gradient Descent(39/49): loss=6.712755767654023e+47\n",
      "Gradient Descent(40/49): loss=1.4269064321633148e+49\n",
      "Gradient Descent(41/49): loss=3.0331238564643488e+50\n",
      "Gradient Descent(42/49): loss=6.447402661648557e+51\n",
      "Gradient Descent(43/49): loss=1.3705012735579415e+53\n",
      "Gradient Descent(44/49): loss=2.9132254326174015e+54\n",
      "Gradient Descent(45/49): loss=6.192538879745795e+55\n",
      "Gradient Descent(46/49): loss=1.3163257929788941e+57\n",
      "Gradient Descent(47/49): loss=2.7980665554297253e+58\n",
      "Gradient Descent(48/49): loss=5.947749782290999e+59\n",
      "Gradient Descent(49/49): loss=1.2642918519609697e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6644343956932831\n",
      "Gradient Descent(2/49): loss=1.438167583114132\n",
      "Gradient Descent(3/49): loss=4.961083944246069\n",
      "Gradient Descent(4/49): loss=30.34487401469491\n",
      "Gradient Descent(5/49): loss=386.65864871241365\n",
      "Gradient Descent(6/49): loss=7357.987890816158\n",
      "Gradient Descent(7/49): loss=155204.32321580517\n",
      "Gradient Descent(8/49): loss=3338438.346391832\n",
      "Gradient Descent(9/49): loss=72059294.41639486\n",
      "Gradient Descent(10/49): loss=1556326658.9066448\n",
      "Gradient Descent(11/49): loss=33616902520.982166\n",
      "Gradient Descent(12/49): loss=726143923329.9666\n",
      "Gradient Descent(13/49): loss=15685166436732.168\n",
      "Gradient Descent(14/49): loss=338809673983288.56\n",
      "Gradient Descent(15/49): loss=7318507396706797.0\n",
      "Gradient Descent(16/49): loss=1.5808448092962048e+17\n",
      "Gradient Descent(17/49): loss=3.414726778561826e+18\n",
      "Gradient Descent(18/49): loss=7.376030151628126e+19\n",
      "Gradient Descent(19/49): loss=1.5932701012667198e+21\n",
      "Gradient Descent(20/49): loss=3.441566213054717e+22\n",
      "Gradient Descent(21/49): loss=7.434005062579656e+23\n",
      "Gradient Descent(22/49): loss=1.6057930561049088e+25\n",
      "Gradient Descent(23/49): loss=3.468616603470904e+26\n",
      "Gradient Descent(24/49): loss=7.492435651115527e+27\n",
      "Gradient Descent(25/49): loss=1.618414440210347e+29\n",
      "Gradient Descent(26/49): loss=3.4958796074429495e+30\n",
      "Gradient Descent(27/49): loss=7.551325498644953e+31\n",
      "Gradient Descent(28/49): loss=1.6311350272212505e+33\n",
      "Gradient Descent(29/49): loss=3.5233568960912357e+34\n",
      "Gradient Descent(30/49): loss=7.610678214900374e+35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=1.6439555968632594e+37\n",
      "Gradient Descent(32/49): loss=3.551050153673275e+38\n",
      "Gradient Descent(33/49): loss=7.670497437986526e+39\n",
      "Gradient Descent(34/49): loss=1.656876934990515e+41\n",
      "Gradient Descent(35/49): loss=3.578961077684901e+42\n",
      "Gradient Descent(36/49): loss=7.730786834603676e+43\n",
      "Gradient Descent(37/49): loss=1.6698998336338853e+45\n",
      "Gradient Descent(38/49): loss=3.607091378963715e+46\n",
      "Gradient Descent(39/49): loss=7.79155010027212e+47\n",
      "Gradient Descent(40/49): loss=1.6830250910496892e+49\n",
      "Gradient Descent(41/49): loss=3.6354427817950627e+50\n",
      "Gradient Descent(42/49): loss=7.85279095955883e+51\n",
      "Gradient Descent(43/49): loss=1.6962535117683806e+53\n",
      "Gradient Descent(44/49): loss=3.664017024016392e+54\n",
      "Gradient Descent(45/49): loss=7.914513166304822e+55\n",
      "Gradient Descent(46/49): loss=1.709585906643774e+57\n",
      "Gradient Descent(47/49): loss=3.6928158571244256e+58\n",
      "Gradient Descent(48/49): loss=7.976720503856446e+59\n",
      "Gradient Descent(49/49): loss=1.7230230929032274e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6765709476165291\n",
      "Gradient Descent(2/49): loss=1.5383851260610883\n",
      "Gradient Descent(3/49): loss=6.0905923470790375\n",
      "Gradient Descent(4/49): loss=50.00637647202728\n",
      "Gradient Descent(5/49): loss=810.6490440467256\n",
      "Gradient Descent(6/49): loss=17041.341617107213\n",
      "Gradient Descent(7/49): loss=378771.5092592433\n",
      "Gradient Descent(8/49): loss=8503528.032168088\n",
      "Gradient Descent(9/49): loss=191238613.97353873\n",
      "Gradient Descent(10/49): loss=4302116999.724027\n",
      "Gradient Descent(11/49): loss=96785720628.18513\n",
      "Gradient Descent(12/49): loss=2177430121363.1738\n",
      "Gradient Descent(13/49): loss=48986660218050.66\n",
      "Gradient Descent(14/49): loss=1102076016374089.9\n",
      "Gradient Descent(15/49): loss=2.4793925444827788e+16\n",
      "Gradient Descent(16/49): loss=5.5780067314790835e+17\n",
      "Gradient Descent(17/49): loss=1.2549105710507145e+19\n",
      "Gradient Descent(18/49): loss=2.823231698188998e+20\n",
      "Gradient Descent(19/49): loss=6.351557956282154e+21\n",
      "Gradient Descent(20/49): loss=1.4289400511538047e+23\n",
      "Gradient Descent(21/49): loss=3.2147540553797936e+24\n",
      "Gradient Descent(22/49): loss=7.232384331475832e+25\n",
      "Gradient Descent(23/49): loss=1.6271037291529394e+27\n",
      "Gradient Descent(24/49): loss=3.660572259554079e+28\n",
      "Gradient Descent(25/49): loss=8.235362643039893e+29\n",
      "Gradient Descent(26/49): loss=1.8527485063397754e+31\n",
      "Gradient Descent(27/49): loss=4.168215993069113e+32\n",
      "Gradient Descent(28/49): loss=9.377432773755293e+33\n",
      "Gradient Descent(29/49): loss=2.1096854283108708e+35\n",
      "Gradient Descent(30/49): loss=4.7462591455559706e+36\n",
      "Gradient Descent(31/49): loss=1.067788380887161e+38\n",
      "Gradient Descent(32/49): loss=2.4022540518573014e+39\n",
      "Gradient Descent(33/49): loss=5.404464623289828e+40\n",
      "Gradient Descent(34/49): loss=1.2158679820649722e+42\n",
      "Gradient Descent(35/49): loss=2.735395738256979e+43\n",
      "Gradient Descent(36/49): loss=6.153949240580124e+44\n",
      "Gradient Descent(37/49): loss=1.384483083232743e+46\n",
      "Gradient Descent(38/49): loss=3.1147371107938067e+47\n",
      "Gradient Descent(39/49): loss=7.007371478099462e+48\n",
      "Gradient Descent(40/49): loss=1.576481522691582e+50\n",
      "Gradient Descent(41/49): loss=3.546685086063101e+51\n",
      "Gradient Descent(42/49): loss=7.979145279308889e+52\n",
      "Gradient Descent(43/49): loss=1.795106073513577e+54\n",
      "Gradient Descent(44/49): loss=4.038535084104111e+55\n",
      "Gradient Descent(45/49): loss=9.08568349591548e+56\n",
      "Gradient Descent(46/49): loss=2.0440492126185038e+58\n",
      "Gradient Descent(47/49): loss=4.5985942449839e+59\n",
      "Gradient Descent(48/49): loss=1.0345675094049517e+61\n",
      "Gradient Descent(49/49): loss=2.327515485159211e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.679770053987679\n",
      "Gradient Descent(2/49): loss=1.5665488470607503\n",
      "Gradient Descent(3/49): loss=6.113627327106351\n",
      "Gradient Descent(4/49): loss=46.16682505171499\n",
      "Gradient Descent(5/49): loss=687.0912890783754\n",
      "Gradient Descent(6/49): loss=13756.984029930727\n",
      "Gradient Descent(7/49): loss=295372.3769945766\n",
      "Gradient Descent(8/49): loss=6426789.136497288\n",
      "Gradient Descent(9/49): loss=140174615.3263567\n",
      "Gradient Descent(10/49): loss=3058682219.9699535\n",
      "Gradient Descent(11/49): loss=66747271456.48757\n",
      "Gradient Descent(12/49): loss=1456595054527.1917\n",
      "Gradient Descent(13/49): loss=31786686158136.438\n",
      "Gradient Descent(14/49): loss=693668345523567.0\n",
      "Gradient Descent(15/49): loss=1.5137652633742174e+16\n",
      "Gradient Descent(16/49): loss=3.303430697935806e+17\n",
      "Gradient Descent(17/49): loss=7.208947562364424e+18\n",
      "Gradient Descent(18/49): loss=1.5731804216311074e+20\n",
      "Gradient Descent(19/49): loss=3.433090083967229e+21\n",
      "Gradient Descent(20/49): loss=7.491898171856863e+22\n",
      "Gradient Descent(21/49): loss=1.6349276262733415e+24\n",
      "Gradient Descent(22/49): loss=3.5678385928871463e+25\n",
      "Gradient Descent(23/49): loss=7.785954570913734e+26\n",
      "Gradient Descent(24/49): loss=1.6990984037558863e+28\n",
      "Gradient Descent(25/49): loss=3.7078759699300406e+29\n",
      "Gradient Descent(26/49): loss=8.091552659924785e+30\n",
      "Gradient Descent(27/49): loss=1.7657878790797462e+32\n",
      "Gradient Descent(28/49): loss=3.853409802728801e+33\n",
      "Gradient Descent(29/49): loss=8.40914544928487e+34\n",
      "Gradient Descent(30/49): loss=1.8350949109319555e+36\n",
      "Gradient Descent(31/49): loss=4.004655826728202e+37\n",
      "Gradient Descent(32/49): loss=8.739203730015125e+38\n",
      "Gradient Descent(33/49): loss=1.9071222381951191e+40\n",
      "Gradient Descent(34/49): loss=4.1618382449725364e+41\n",
      "Gradient Descent(35/49): loss=9.082216771646679e+42\n",
      "Gradient Descent(36/49): loss=1.9819766322446843e+44\n",
      "Gradient Descent(37/49): loss=4.3251900604568576e+45\n",
      "Gradient Descent(38/49): loss=9.438693047499863e+46\n",
      "Gradient Descent(39/49): loss=2.0597690552241132e+48\n",
      "Gradient Descent(40/49): loss=4.494953421525599e+49\n",
      "Gradient Descent(41/49): loss=9.809160988432247e+50\n",
      "Gradient Descent(42/49): loss=2.1406148245318954e+52\n",
      "Gradient Descent(43/49): loss=4.6713799808255425e+53\n",
      "Gradient Descent(44/49): loss=1.0194169766169772e+55\n",
      "Gradient Descent(45/49): loss=2.224633783765175e+56\n",
      "Gradient Descent(46/49): loss=4.8547312683502244e+57\n",
      "Gradient Descent(47/49): loss=1.0594290107384856e+59\n",
      "Gradient Descent(48/49): loss=2.3119504803727474e+60\n",
      "Gradient Descent(49/49): loss=5.0452790791238415e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6845660051139324\n",
      "Gradient Descent(2/49): loss=1.579712340499374\n",
      "Gradient Descent(3/49): loss=6.052436615309967\n",
      "Gradient Descent(4/49): loss=43.74650492306047\n",
      "Gradient Descent(5/49): loss=631.4353045949878\n",
      "Gradient Descent(6/49): loss=12539.455004523134\n",
      "Gradient Descent(7/49): loss=268875.66391253954\n",
      "Gradient Descent(8/49): loss=5850322.935298407\n",
      "Gradient Descent(9/49): loss=127632264.09788191\n",
      "Gradient Descent(10/49): loss=2785788270.939591\n",
      "Gradient Descent(11/49): loss=60809700425.01824\n",
      "Gradient Descent(12/49): loss=1327407537021.8022\n",
      "Gradient Descent(13/49): loss=28975896931763.04\n",
      "Gradient Descent(14/49): loss=632513371603955.5\n",
      "Gradient Descent(15/49): loss=1.3807103246803058e+16\n",
      "Gradient Descent(16/49): loss=3.013945817531602e+17\n",
      "Gradient Descent(17/49): loss=6.579127608769653e+18\n",
      "Gradient Descent(18/49): loss=1.43615455403138e+20\n",
      "Gradient Descent(19/49): loss=3.1349747655056357e+21\n",
      "Gradient Descent(20/49): loss=6.84332111257112e+22\n",
      "Gradient Descent(21/49): loss=1.493825225171918e+24\n",
      "Gradient Descent(22/49): loss=3.2608637920875166e+25\n",
      "Gradient Descent(23/49): loss=7.118123654207916e+26\n",
      "Gradient Descent(24/49): loss=1.5538117378450598e+28\n",
      "Gradient Descent(25/49): loss=3.391808057784836e+29\n",
      "Gradient Descent(26/49): loss=7.403961252609011e+30\n",
      "Gradient Descent(27/49): loss=1.6162070876716419e+32\n",
      "Gradient Descent(28/49): loss=3.5280105623453e+33\n",
      "Gradient Descent(29/49): loss=7.701277034956619e+34\n",
      "Gradient Descent(30/49): loss=1.6811080046688717e+36\n",
      "Gradient Descent(31/49): loss=3.6696824572520324e+37\n",
      "Gradient Descent(32/49): loss=8.010531922793243e+38\n",
      "Gradient Descent(33/49): loss=1.748615103175466e+40\n",
      "Gradient Descent(34/49): loss=3.817043373053724e+41\n",
      "Gradient Descent(35/49): loss=8.332205346571197e+42\n",
      "Gradient Descent(36/49): loss=1.818833037830706e+44\n",
      "Gradient Descent(37/49): loss=3.970321759851688e+45\n",
      "Gradient Descent(38/49): loss=8.666795988901181e+46\n",
      "Gradient Descent(39/49): loss=1.8918706658182766e+48\n",
      "Gradient Descent(40/49): loss=4.1297552414608486e+49\n",
      "Gradient Descent(41/49): loss=9.01482255764927e+50\n",
      "Gradient Descent(42/49): loss=1.9678412156249972e+52\n",
      "Gradient Descent(43/49): loss=4.2955909837921945e+53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=9.376824590075819e+54\n",
      "Gradient Descent(45/49): loss=2.046862462576223e+56\n",
      "Gradient Descent(46/49): loss=4.4680860780288e+57\n",
      "Gradient Descent(47/49): loss=9.753363289269256e+58\n",
      "Gradient Descent(48/49): loss=2.129056911419988e+60\n",
      "Gradient Descent(49/49): loss=4.6475079391868344e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6740058509783418\n",
      "Gradient Descent(2/49): loss=1.5105333376560695\n",
      "Gradient Descent(3/49): loss=5.424400903672266\n",
      "Gradient Descent(4/49): loss=34.12996072246575\n",
      "Gradient Descent(5/49): loss=443.53829121541287\n",
      "Gradient Descent(6/49): loss=8637.268613528178\n",
      "Gradient Descent(7/49): loss=186921.5585786074\n",
      "Gradient Descent(8/49): loss=4127994.323830757\n",
      "Gradient Descent(9/49): loss=91493673.97985679\n",
      "Gradient Descent(10/49): loss=2029179308.664067\n",
      "Gradient Descent(11/49): loss=45008921826.36118\n",
      "Gradient Descent(12/49): loss=998355892812.5526\n",
      "Gradient Descent(13/49): loss=22144897447432.895\n",
      "Gradient Descent(14/49): loss=491204375939845.0\n",
      "Gradient Descent(15/49): loss=1.089559188589987e+16\n",
      "Gradient Descent(16/49): loss=2.416792899447001e+17\n",
      "Gradient Descent(17/49): loss=5.360780762962629e+18\n",
      "Gradient Descent(18/49): loss=1.189095284581196e+20\n",
      "Gradient Descent(19/49): loss=2.6375777307069765e+21\n",
      "Gradient Descent(20/49): loss=5.850512045464945e+22\n",
      "Gradient Descent(21/49): loss=1.297724453605584e+24\n",
      "Gradient Descent(22/49): loss=2.878532245382296e+25\n",
      "Gradient Descent(23/49): loss=6.384982470419562e+26\n",
      "Gradient Descent(24/49): loss=1.4162773827865179e+28\n",
      "Gradient Descent(25/49): loss=3.14149903196341e+29\n",
      "Gradient Descent(26/49): loss=6.96827915758247e+30\n",
      "Gradient Descent(27/49): loss=1.5456606519357175e+32\n",
      "Gradient Descent(28/49): loss=3.4284890098620955e+33\n",
      "Gradient Descent(29/49): loss=7.60486260423624e+34\n",
      "Gradient Descent(30/49): loss=1.686863661016572e+36\n",
      "Gradient Descent(31/49): loss=3.7416968049799744e+37\n",
      "Gradient Descent(32/49): loss=8.29960079403241e+38\n",
      "Gradient Descent(33/49): loss=1.840966196101794e+40\n",
      "Gradient Descent(34/49): loss=4.083517532103893e+41\n",
      "Gradient Descent(35/49): loss=9.057806422686838e+42\n",
      "Gradient Descent(36/49): loss=2.0091466865479063e+44\n",
      "Gradient Descent(37/49): loss=4.456565110461913e+45\n",
      "Gradient Descent(38/49): loss=9.885277524415882e+46\n",
      "Gradient Descent(39/49): loss=2.1926912165003966e+48\n",
      "Gradient Descent(40/49): loss=4.863692252486421e+49\n",
      "Gradient Descent(41/49): loss=1.0788341809774394e+51\n",
      "Gradient Descent(42/49): loss=2.3930033596394874e+52\n",
      "Gradient Descent(43/49): loss=5.308012278641178e+53\n",
      "Gradient Descent(44/49): loss=1.1773905053960925e+55\n",
      "Gradient Descent(45/49): loss=2.6116149123749435e+56\n",
      "Gradient Descent(46/49): loss=5.792922925129745e+57\n",
      "Gradient Descent(47/49): loss=1.284950390560337e+59\n",
      "Gradient Descent(48/49): loss=2.850197607564785e+60\n",
      "Gradient Descent(49/49): loss=6.322132326545007e+61\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6864143581462935\n",
      "Gradient Descent(2/49): loss=1.6160058192419466\n",
      "Gradient Descent(3/49): loss=6.646525934411869\n",
      "Gradient Descent(4/49): loss=55.95203672077834\n",
      "Gradient Descent(5/49): loss=926.1874754710245\n",
      "Gradient Descent(6/49): loss=19947.58934254829\n",
      "Gradient Descent(7/49): loss=454911.6251635631\n",
      "Gradient Descent(8/49): loss=10482523.819586001\n",
      "Gradient Descent(9/49): loss=241986650.57196578\n",
      "Gradient Descent(10/49): loss=5587963716.327201\n",
      "Gradient Descent(11/49): loss=129044484431.91446\n",
      "Gradient Descent(12/49): loss=2980090321128.7446\n",
      "Gradient Descent(13/49): loss=68820863916155.36\n",
      "Gradient Descent(14/49): loss=1589318493217687.8\n",
      "Gradient Descent(15/49): loss=3.670301786261493e+16\n",
      "Gradient Descent(16/49): loss=8.476032573154209e+17\n",
      "Gradient Descent(17/49): loss=1.957417467872384e+19\n",
      "Gradient Descent(18/49): loss=4.5203733131637044e+20\n",
      "Gradient Descent(19/49): loss=1.043915016937413e+22\n",
      "Gradient Descent(20/49): loss=2.410771162245413e+23\n",
      "Gradient Descent(21/49): loss=5.56732828096692e+24\n",
      "Gradient Descent(22/49): loss=1.2856941659777032e+26\n",
      "Gradient Descent(23/49): loss=2.9691252338761886e+27\n",
      "Gradient Descent(24/49): loss=6.856766475047977e+28\n",
      "Gradient Descent(25/49): loss=1.5834713186537918e+30\n",
      "Gradient Descent(26/49): loss=3.6567986180126944e+31\n",
      "Gradient Descent(27/49): loss=8.44484896895259e+32\n",
      "Gradient Descent(28/49): loss=1.9502160648698539e+34\n",
      "Gradient Descent(29/49): loss=4.503742711870077e+35\n",
      "Gradient Descent(30/49): loss=1.040074419450374e+37\n",
      "Gradient Descent(31/49): loss=2.401901856302593e+38\n",
      "Gradient Descent(32/49): loss=5.546845898160606e+39\n",
      "Gradient Descent(33/49): loss=1.2809640550968311e+41\n",
      "Gradient Descent(34/49): loss=2.9582017250457534e+42\n",
      "Gradient Descent(35/49): loss=6.831540206959391e+43\n",
      "Gradient Descent(36/49): loss=1.5776456758904888e+45\n",
      "Gradient Descent(37/49): loss=3.6433451363140463e+46\n",
      "Gradient Descent(38/49): loss=8.413780093436221e+47\n",
      "Gradient Descent(39/49): loss=1.9430411561921025e+49\n",
      "Gradient Descent(40/49): loss=4.487173295153731e+50\n",
      "Gradient Descent(41/49): loss=1.036247951649164e+52\n",
      "Gradient Descent(42/49): loss=2.3930651808273986e+53\n",
      "Gradient Descent(43/49): loss=5.5264388707109215e+54\n",
      "Gradient Descent(44/49): loss=1.2762513464486906e+56\n",
      "Gradient Descent(45/49): loss=2.947318404161702e+57\n",
      "Gradient Descent(46/49): loss=6.806406747136241e+58\n",
      "Gradient Descent(47/49): loss=1.5718414658574478e+60\n",
      "Gradient Descent(48/49): loss=3.629941150414544e+61\n",
      "Gradient Descent(49/49): loss=8.382825521328887e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6897134876202073\n",
      "Gradient Descent(2/49): loss=1.6460000207976715\n",
      "Gradient Descent(3/49): loss=6.6748601715336\n",
      "Gradient Descent(4/49): loss=51.721215163279965\n",
      "Gradient Descent(5/49): loss=785.7654822580894\n",
      "Gradient Descent(6/49): loss=16112.732938918998\n",
      "Gradient Descent(7/49): loss=354935.93961241824\n",
      "Gradient Descent(8/49): loss=7926955.082025412\n",
      "Gradient Descent(9/49): loss=177483304.6378049\n",
      "Gradient Descent(10/49): loss=3975643176.962534\n",
      "Gradient Descent(11/49): loss=89062182082.46342\n",
      "Gradient Descent(12/49): loss=1995197063707.8748\n",
      "Gradient Descent(13/49): loss=44697110301666.875\n",
      "Gradient Descent(14/49): loss=1001320968687659.2\n",
      "Gradient Descent(15/49): loss=2.243195959130411e+16\n",
      "Gradient Descent(16/49): loss=5.0252899420846406e+17\n",
      "Gradient Descent(17/49): loss=1.1257839054823436e+19\n",
      "Gradient Descent(18/49): loss=2.5220224447253106e+20\n",
      "Gradient Descent(19/49): loss=5.649927291852461e+21\n",
      "Gradient Descent(20/49): loss=1.265717459039338e+23\n",
      "Gradient Descent(21/49): loss=2.8355067302057375e+24\n",
      "Gradient Descent(22/49): loss=6.3522062997762585e+25\n",
      "Gradient Descent(23/49): loss=1.4230445812411753e+27\n",
      "Gradient Descent(24/49): loss=3.187956726580623e+28\n",
      "Gradient Descent(25/49): loss=7.141777723988498e+29\n",
      "Gradient Descent(26/49): loss=1.5999272710820346e+31\n",
      "Gradient Descent(27/49): loss=3.5842158236792676e+32\n",
      "Gradient Descent(28/49): loss=8.02949190435688e+33\n",
      "Gradient Descent(29/49): loss=1.7987962615473875e+35\n",
      "Gradient Descent(30/49): loss=4.029729438796984e+36\n",
      "Gradient Descent(31/49): loss=9.027547864669364e+37\n",
      "Gradient Descent(32/49): loss=2.022384422742375e+39\n",
      "Gradient Descent(33/49): loss=4.530619847896978e+40\n",
      "Gradient Descent(34/49): loss=1.0149660952354158e+42\n",
      "Gradient Descent(35/49): loss=2.273764317162045e+43\n",
      "Gradient Descent(36/49): loss=5.09377031830837e+44\n",
      "Gradient Descent(37/49): loss=1.1411251315643472e+46\n",
      "Gradient Descent(38/49): loss=2.5563904230378336e+47\n",
      "Gradient Descent(39/49): loss=5.726919699017285e+48\n",
      "Gradient Descent(40/49): loss=1.2829655808214753e+50\n",
      "Gradient Descent(41/49): loss=2.874146605993129e+51\n",
      "Gradient Descent(42/49): loss=6.438768768412883e+52\n",
      "Gradient Descent(43/49): loss=1.442436623331673e+54\n",
      "Gradient Descent(44/49): loss=3.2313994913677706e+55\n",
      "Gradient Descent(45/49): loss=7.2390998009283505e+56\n",
      "Gradient Descent(46/49): loss=1.6217297201349025e+58\n",
      "Gradient Descent(47/49): loss=3.633058470656211e+59\n",
      "Gradient Descent(48/49): loss=8.138910995667609e+60\n",
      "Gradient Descent(49/49): loss=1.8233087281811396e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.694731674245144\n",
      "Gradient Descent(2/49): loss=1.6602583429170192\n",
      "Gradient Descent(3/49): loss=6.610275249060807\n",
      "Gradient Descent(4/49): loss=49.03942165093891\n",
      "Gradient Descent(5/49): loss=722.3313622801709\n",
      "Gradient Descent(6/49): loss=14687.76184631839\n",
      "Gradient Descent(7/49): loss=323099.5691044839\n",
      "Gradient Descent(8/49): loss=7215911.73303044\n",
      "Gradient Descent(9/49): loss=161601823.7369169\n",
      "Gradient Descent(10/49): loss=3620914721.534398\n",
      "Gradient Descent(11/49): loss=81138971740.15051\n",
      "Gradient Descent(12/49): loss=1818225604173.7476\n",
      "Gradient Descent(13/49): loss=40744342477603.95\n",
      "Gradient Descent(14/49): loss=913034289808124.6\n",
      "Gradient Descent(15/49): loss=2.046006005586096e+16\n",
      "Gradient Descent(16/49): loss=4.5848668487011226e+17\n",
      "Gradient Descent(17/49): loss=1.0274165387743832e+19\n",
      "Gradient Descent(18/49): loss=2.3023236651651105e+20\n",
      "Gradient Descent(19/49): loss=5.159245602605543e+21\n",
      "Gradient Descent(20/49): loss=1.1561282886167503e+23\n",
      "Gradient Descent(21/49): loss=2.5907520647383903e+24\n",
      "Gradient Descent(22/49): loss=5.805580857277427e+25\n",
      "Gradient Descent(23/49): loss=1.3009646715766626e+27\n",
      "Gradient Descent(24/49): loss=2.915313933779831e+28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=6.532887109217607e+29\n",
      "Gradient Descent(26/49): loss=1.4639457345318374e+31\n",
      "Gradient Descent(27/49): loss=3.280535967979747e+32\n",
      "Gradient Descent(28/49): loss=7.351308168980872e+33\n",
      "Gradient Descent(29/49): loss=1.6473445901160476e+35\n",
      "Gradient Descent(30/49): loss=3.6915119543421104e+36\n",
      "Gradient Descent(31/49): loss=8.272258634176085e+37\n",
      "Gradient Descent(32/49): loss=1.8537191198909792e+39\n",
      "Gradient Descent(33/49): loss=4.153973814663747e+40\n",
      "Gradient Descent(34/49): loss=9.308583089938181e+41\n",
      "Gradient Descent(35/49): loss=2.0859476493665888e+43\n",
      "Gradient Descent(36/49): loss=4.674371549200958e+44\n",
      "Gradient Descent(37/49): loss=1.0474735253597701e+46\n",
      "Gradient Descent(38/49): loss=2.347269092285165e+47\n",
      "Gradient Descent(39/49): loss=5.259963195446507e+48\n",
      "Gradient Descent(40/49): loss=1.1786979562073896e+50\n",
      "Gradient Descent(41/49): loss=2.6413281240640146e+51\n",
      "Gradient Descent(42/49): loss=5.918916056679916e+52\n",
      "Gradient Descent(43/49): loss=1.3263618013546393e+54\n",
      "Gradient Descent(44/49): loss=2.9722260144360113e+55\n",
      "Gradient Descent(45/49): loss=6.660420612134616e+56\n",
      "Gradient Descent(46/49): loss=1.4925245427193603e+58\n",
      "Gradient Descent(47/49): loss=3.3445778282548184e+59\n",
      "Gradient Descent(48/49): loss=7.49481869750182e+60\n",
      "Gradient Descent(49/49): loss=1.679503668112679e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6837460993917444\n",
      "Gradient Descent(2/49): loss=1.5865505257021022\n",
      "Gradient Descent(3/49): loss=5.9262661540323185\n",
      "Gradient Descent(4/49): loss=38.33213225815621\n",
      "Gradient Descent(5/49): loss=507.90539694986103\n",
      "Gradient Descent(6/49): loss=10117.96098277524\n",
      "Gradient Descent(7/49): loss=224574.61673934708\n",
      "Gradient Descent(8/49): loss=5090126.25369479\n",
      "Gradient Descent(9/49): loss=115806966.64500928\n",
      "Gradient Descent(10/49): loss=2636523963.4332976\n",
      "Gradient Descent(11/49): loss=60031651697.39739\n",
      "Gradient Descent(12/49): loss=1366903914765.873\n",
      "Gradient Descent(13/49): loss=31124135526851.055\n",
      "Gradient Descent(14/49): loss=708690962060747.4\n",
      "Gradient Descent(15/49): loss=1.6136767488716896e+16\n",
      "Gradient Descent(16/49): loss=3.674313407427805e+17\n",
      "Gradient Descent(17/49): loss=8.366346652021449e+18\n",
      "Gradient Descent(18/49): loss=1.9050023388208087e+20\n",
      "Gradient Descent(19/49): loss=4.337656640669942e+21\n",
      "Gradient Descent(20/49): loss=9.876767471254647e+22\n",
      "Gradient Descent(21/49): loss=2.248922488860734e+24\n",
      "Gradient Descent(22/49): loss=5.12075674113681e+25\n",
      "Gradient Descent(23/49): loss=1.1659872553092986e+27\n",
      "Gradient Descent(24/49): loss=2.6549323630668667e+28\n",
      "Gradient Descent(25/49): loss=6.045234045538756e+29\n",
      "Gradient Descent(26/49): loss=1.3764891028382063e+31\n",
      "Gradient Descent(27/49): loss=3.1342413477449916e+32\n",
      "Gradient Descent(28/49): loss=7.136612128392068e+33\n",
      "Gradient Descent(29/49): loss=1.624993962502471e+35\n",
      "Gradient Descent(30/49): loss=3.7000825190768025e+36\n",
      "Gradient Descent(31/49): loss=8.425022470172428e+37\n",
      "Gradient Descent(32/49): loss=1.918362719127125e+39\n",
      "Gradient Descent(33/49): loss=4.368077990492794e+40\n",
      "Gradient Descent(34/49): loss=9.946036346926867e+41\n",
      "Gradient Descent(35/49): loss=2.2646948893700473e+43\n",
      "Gradient Descent(36/49): loss=5.156670218205607e+44\n",
      "Gradient Descent(37/49): loss=1.174164690534809e+46\n",
      "Gradient Descent(38/49): loss=2.6735522384799683e+47\n",
      "Gradient Descent(39/49): loss=6.087631172613229e+48\n",
      "Gradient Descent(40/49): loss=1.3861428537054443e+50\n",
      "Gradient Descent(41/49): loss=3.156222767769698e+51\n",
      "Gradient Descent(42/49): loss=7.186663433107274e+52\n",
      "Gradient Descent(43/49): loss=1.6363905560841868e+54\n",
      "Gradient Descent(44/49): loss=3.726032361144889e+55\n",
      "Gradient Descent(45/49): loss=8.4841098017098e+56\n",
      "Gradient Descent(46/49): loss=1.9318168000384616e+58\n",
      "Gradient Descent(47/49): loss=4.398712694829636e+59\n",
      "Gradient Descent(48/49): loss=1.0015791027011473e+61\n",
      "Gradient Descent(49/49): loss=2.2805779066833784e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6964281807427064\n",
      "Gradient Descent(2/49): loss=1.6974672753008924\n",
      "Gradient Descent(3/49): loss=7.247152280309862\n",
      "Gradient Descent(4/49): loss=62.5192676196125\n",
      "Gradient Descent(5/49): loss=1056.4159444223071\n",
      "Gradient Descent(6/49): loss=23302.349897685726\n",
      "Gradient Descent(7/49): loss=545068.9393725429\n",
      "Gradient Descent(8/49): loss=12887245.814855756\n",
      "Gradient Descent(9/49): loss=305272745.2292069\n",
      "Gradient Descent(10/49): loss=7233677063.397058\n",
      "Gradient Descent(11/49): loss=171417513494.88666\n",
      "Gradient Descent(12/49): loss=4062146923370.2627\n",
      "Gradient Descent(13/49): loss=96262430899013.98\n",
      "Gradient Descent(14/49): loss=2281172643863798.5\n",
      "Gradient Descent(15/49): loss=5.405794202675526e+16\n",
      "Gradient Descent(16/49): loss=1.281034616670808e+18\n",
      "Gradient Descent(17/49): loss=3.0357235777950368e+19\n",
      "Gradient Descent(18/49): loss=7.193886506631105e+20\n",
      "Gradient Descent(19/49): loss=1.7047666477737175e+22\n",
      "Gradient Descent(20/49): loss=4.039859845867471e+23\n",
      "Gradient Descent(21/49): loss=9.57343199762285e+24\n",
      "Gradient Descent(22/49): loss=2.268657916607765e+26\n",
      "Gradient Descent(23/49): loss=5.376137568915016e+27\n",
      "Gradient Descent(24/49): loss=1.2740067574011808e+29\n",
      "Gradient Descent(25/49): loss=3.0190693543422055e+30\n",
      "Gradient Descent(26/49): loss=7.154420267692564e+31\n",
      "Gradient Descent(27/49): loss=1.6954141610941008e+33\n",
      "Gradient Descent(28/49): loss=4.017696850461112e+34\n",
      "Gradient Descent(29/49): loss=9.520911381198315e+35\n",
      "Gradient Descent(30/49): loss=2.256211877166018e+37\n",
      "Gradient Descent(31/49): loss=5.346643646655248e+38\n",
      "Gradient Descent(32/49): loss=1.267017454062213e+40\n",
      "Gradient Descent(33/49): loss=3.0025064975156344e+41\n",
      "Gradient Descent(34/49): loss=7.115170543799851e+42\n",
      "Gradient Descent(35/49): loss=1.6861129829109566e+44\n",
      "Gradient Descent(36/49): loss=3.9956554430284297e+45\n",
      "Gradient Descent(37/49): loss=9.468678897092235e+46\n",
      "Gradient Descent(38/49): loss=2.2438341176957415e+48\n",
      "Gradient Descent(39/49): loss=5.317311530420327e+49\n",
      "Gradient Descent(40/49): loss=1.2600664946023548e+51\n",
      "Gradient Descent(41/49): loss=2.986034505851095e+52\n",
      "Gradient Descent(42/49): loss=7.076136147042867e+53\n",
      "Gradient Descent(43/49): loss=1.6768628317379503e+55\n",
      "Gradient Descent(44/49): loss=3.973734956526108e+56\n",
      "Gradient Descent(45/49): loss=9.416732964586799e+57\n",
      "Gradient Descent(46/49): loss=2.23152426360759e+59\n",
      "Gradient Descent(47/49): loss=5.288140332529724e+60\n",
      "Gradient Descent(48/49): loss=1.2531536686641088e+62\n",
      "Gradient Descent(49/49): loss=2.9696528808550806e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6998287365496513\n",
      "Gradient Descent(2/49): loss=1.7293756928292099\n",
      "Gradient Descent(3/49): loss=7.281393391767827\n",
      "Gradient Descent(4/49): loss=57.8633541844839\n",
      "Gradient Descent(5/49): loss=897.0906575772524\n",
      "Gradient Descent(6/49): loss=18833.608842796264\n",
      "Gradient Descent(7/49): loss=425499.9775110738\n",
      "Gradient Descent(8/49): loss=9750798.299741775\n",
      "Gradient Descent(9/49): loss=224036871.2453864\n",
      "Gradient Descent(10/49): loss=5149995884.494995\n",
      "Gradient Descent(11/49): loss=118394687909.78262\n",
      "Gradient Descent(12/49): loss=2721851787748.396\n",
      "Gradient Descent(13/49): loss=62574586862862.73\n",
      "Gradient Descent(14/49): loss=1438572450371076.0\n",
      "Gradient Descent(15/49): loss=3.3072386070413704e+16\n",
      "Gradient Descent(16/49): loss=7.603250982454223e+17\n",
      "Gradient Descent(17/49): loss=1.7479665835068006e+19\n",
      "Gradient Descent(18/49): loss=4.018527319296865e+20\n",
      "Gradient Descent(19/49): loss=9.238484287990753e+21\n",
      "Gradient Descent(20/49): loss=2.1239022447668435e+23\n",
      "Gradient Descent(21/49): loss=4.882793112724603e+24\n",
      "Gradient Descent(22/49): loss=1.1225407685514187e+26\n",
      "Gradient Descent(23/49): loss=2.5806904940869155e+27\n",
      "Gradient Descent(24/49): loss=5.932936792010746e+28\n",
      "Gradient Descent(25/49): loss=1.363965925346225e+30\n",
      "Gradient Descent(26/49): loss=3.135720319843679e+31\n",
      "Gradient Descent(27/49): loss=7.208935165872795e+32\n",
      "Gradient Descent(28/49): loss=1.6573144580810955e+34\n",
      "Gradient Descent(29/49): loss=3.810120565333375e+35\n",
      "Gradient Descent(30/49): loss=8.759362866588759e+36\n",
      "Gradient Descent(31/49): loss=2.0137535417297683e+38\n",
      "Gradient Descent(32/49): loss=4.629564260086907e+39\n",
      "Gradient Descent(33/49): loss=1.0643241486176747e+41\n",
      "Gradient Descent(34/49): loss=2.4468520787082866e+42\n",
      "Gradient Descent(35/49): loss=5.625245939270515e+43\n",
      "Gradient Descent(36/49): loss=1.2932286406943406e+45\n",
      "Gradient Descent(37/49): loss=2.9730972390674977e+46\n",
      "Gradient Descent(38/49): loss=6.835069155447254e+47\n",
      "Gradient Descent(39/49): loss=1.5713636858510515e+49\n",
      "Gradient Descent(40/49): loss=3.612522093128454e+50\n",
      "Gradient Descent(41/49): loss=8.305089388821626e+51\n",
      "Gradient Descent(42/49): loss=1.909317312896662e+53\n",
      "Gradient Descent(43/49): loss=4.3894682292444576e+54\n",
      "Gradient Descent(44/49): loss=1.0091267284596125e+56\n",
      "Gradient Descent(45/49): loss=2.319954720954588e+57\n",
      "Gradient Descent(46/49): loss=5.333512387978404e+58\n",
      "Gradient Descent(47/49): loss=1.226159895957538e+60\n",
      "Gradient Descent(48/49): loss=2.818908031119197e+61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=6.480592387751478e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.705072816858113\n",
      "Gradient Descent(2/49): loss=1.7447885645108956\n",
      "Gradient Descent(3/49): loss=7.213317550808807\n",
      "Gradient Descent(4/49): loss=54.89588609589713\n",
      "Gradient Descent(5/49): loss=824.9143053063895\n",
      "Gradient Descent(6/49): loss=17169.2474939704\n",
      "Gradient Descent(7/49): loss=387338.6956435278\n",
      "Gradient Descent(8/49): loss=8876139.180895088\n",
      "Gradient Descent(9/49): loss=203988636.72789004\n",
      "Gradient Descent(10/49): loss=4690455363.224425\n",
      "Gradient Descent(11/49): loss=107861210259.64981\n",
      "Gradient Descent(12/49): loss=2480407630751.84\n",
      "Gradient Descent(13/49): loss=57040350795381.76\n",
      "Gradient Descent(14/49): loss=1311721277960499.0\n",
      "Gradient Descent(15/49): loss=3.0164837075793788e+16\n",
      "Gradient Descent(16/49): loss=6.936819796476881e+17\n",
      "Gradient Descent(17/49): loss=1.5952172700746486e+19\n",
      "Gradient Descent(18/49): loss=3.668421863295838e+20\n",
      "Gradient Descent(19/49): loss=8.436041422735876e+21\n",
      "Gradient Descent(20/49): loss=1.939983936951647e+23\n",
      "Gradient Descent(21/49): loss=4.461260308064868e+24\n",
      "Gradient Descent(22/49): loss=1.0259282645196797e+26\n",
      "Gradient Descent(23/49): loss=2.3592633723653371e+27\n",
      "Gradient Descent(24/49): loss=5.425451128194301e+28\n",
      "Gradient Descent(25/49): loss=1.2476572259464892e+30\n",
      "Gradient Descent(26/49): loss=2.869159663732131e+31\n",
      "Gradient Descent(27/49): loss=6.598027891629003e+32\n",
      "Gradient Descent(28/49): loss=1.5173074056843268e+34\n",
      "Gradient Descent(29/49): loss=3.489257398055776e+35\n",
      "Gradient Descent(30/49): loss=8.024028054088116e+36\n",
      "Gradient Descent(31/49): loss=1.8452357870952906e+38\n",
      "Gradient Descent(32/49): loss=4.243373885317293e+39\n",
      "Gradient Descent(33/49): loss=9.758222800858398e+40\n",
      "Gradient Descent(34/49): loss=2.2440377587437868e+42\n",
      "Gradient Descent(35/49): loss=5.160473956615058e+43\n",
      "Gradient Descent(36/49): loss=1.1867220751138538e+45\n",
      "Gradient Descent(37/49): loss=2.7290308901902953e+46\n",
      "Gradient Descent(38/49): loss=6.275782473245294e+47\n",
      "Gradient Descent(39/49): loss=1.4432026325927018e+49\n",
      "Gradient Descent(40/49): loss=3.318843263930775e+50\n",
      "Gradient Descent(41/49): loss=7.632137277043868e+51\n",
      "Gradient Descent(42/49): loss=1.755115104370785e+53\n",
      "Gradient Descent(43/49): loss=4.0361289606986555e+54\n",
      "Gradient Descent(44/49): loss=9.281634547399631e+55\n",
      "Gradient Descent(45/49): loss=2.1344397245565566e+57\n",
      "Gradient Descent(46/49): loss=4.9084381791795996e+58\n",
      "Gradient Descent(47/49): loss=1.1287629761404236e+60\n",
      "Gradient Descent(48/49): loss=2.595745957868701e+61\n",
      "Gradient Descent(49/49): loss=5.96927541053007e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.6936551409334907\n",
      "Gradient Descent(2/49): loss=1.6663399998926252\n",
      "Gradient Descent(3/49): loss=6.469215206884545\n",
      "Gradient Descent(4/49): loss=42.99065087559233\n",
      "Gradient Descent(5/49): loss=580.6311247486004\n",
      "Gradient Descent(6/49): loss=11828.557605911614\n",
      "Gradient Descent(7/49): loss=269175.0801788938\n",
      "Gradient Descent(8/49): loss=6259547.2827706225\n",
      "Gradient Descent(9/49): loss=146135572.870936\n",
      "Gradient Descent(10/49): loss=3414079538.093339\n",
      "Gradient Descent(11/49): loss=79771108473.8442\n",
      "Gradient Descent(12/49): loss=1863920110042.6838\n",
      "Gradient Descent(13/49): loss=43552258843847.945\n",
      "Gradient Descent(14/49): loss=1017640499297568.5\n",
      "Gradient Descent(15/49): loss=2.3778153963374184e+16\n",
      "Gradient Descent(16/49): loss=5.555995648582646e+17\n",
      "Gradient Descent(17/49): loss=1.2982121243728036e+19\n",
      "Gradient Descent(18/49): loss=3.033398201992008e+20\n",
      "Gradient Descent(19/49): loss=7.087828314237193e+21\n",
      "Gradient Descent(20/49): loss=1.6561396449779287e+23\n",
      "Gradient Descent(21/49): loss=3.8697304760745046e+24\n",
      "Gradient Descent(22/49): loss=9.041999569830727e+25\n",
      "Gradient Descent(23/49): loss=2.112750661223448e+27\n",
      "Gradient Descent(24/49): loss=4.936646282746755e+28\n",
      "Gradient Descent(25/49): loss=1.15349515530837e+30\n",
      "Gradient Descent(26/49): loss=2.6952530060134593e+31\n",
      "Gradient Descent(27/49): loss=6.29771935581523e+32\n",
      "Gradient Descent(28/49): loss=1.4715230442604588e+34\n",
      "Gradient Descent(29/49): loss=3.4383559308499335e+35\n",
      "Gradient Descent(30/49): loss=8.034051218784971e+36\n",
      "Gradient Descent(31/49): loss=1.8772337792877444e+38\n",
      "Gradient Descent(32/49): loss=4.386338306954407e+39\n",
      "Gradient Descent(33/49): loss=1.024910373728516e+41\n",
      "Gradient Descent(34/49): loss=2.3948022260637272e+42\n",
      "Gradient Descent(35/49): loss=5.595687046367055e+43\n",
      "Gradient Descent(36/49): loss=1.3074864045180575e+45\n",
      "Gradient Descent(37/49): loss=3.0550684551050913e+46\n",
      "Gradient Descent(38/49): loss=7.1384629569576e+47\n",
      "Gradient Descent(39/49): loss=1.6679709190380044e+49\n",
      "Gradient Descent(40/49): loss=3.897375392338359e+50\n",
      "Gradient Descent(41/49): loss=9.106594590728838e+51\n",
      "Gradient Descent(42/49): loss=2.127843912673063e+53\n",
      "Gradient Descent(43/49): loss=4.971913124703406e+54\n",
      "Gradient Descent(44/49): loss=1.1617355940617197e+56\n",
      "Gradient Descent(45/49): loss=2.714507588244397e+57\n",
      "Gradient Descent(46/49): loss=6.342709549661134e+58\n",
      "Gradient Descent(47/49): loss=1.4820354382350938e+60\n",
      "Gradient Descent(48/49): loss=3.462919156217722e+61\n",
      "Gradient Descent(49/49): loss=8.091445570815915e+62\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7066124154057677\n",
      "Gradient Descent(2/49): loss=1.7828942967848185\n",
      "Gradient Descent(3/49): loss=7.895321149588351\n",
      "Gradient Descent(4/49): loss=69.7636380301601\n",
      "Gradient Descent(5/49): loss=1202.980788521311\n",
      "Gradient Descent(6/49): loss=27167.76522799685\n",
      "Gradient Descent(7/49): loss=651593.3117924964\n",
      "Gradient Descent(8/49): loss=15801990.28868011\n",
      "Gradient Descent(9/49): loss=383971578.3484431\n",
      "Gradient Descent(10/49): loss=9333320739.962711\n",
      "Gradient Descent(11/49): loss=226881776076.58734\n",
      "Gradient Descent(12/49): loss=5515281141720.678\n",
      "Gradient Descent(13/49): loss=134071512515896.16\n",
      "Gradient Descent(14/49): loss=3259158672122869.5\n",
      "Gradient Descent(15/49): loss=7.922723971891032e+16\n",
      "Gradient Descent(16/49): loss=1.9259435358612244e+18\n",
      "Gradient Descent(17/49): loss=4.6817969716830536e+19\n",
      "Gradient Descent(18/49): loss=1.1381030898752337e+21\n",
      "Gradient Descent(19/49): loss=2.766627111531467e+22\n",
      "Gradient Descent(20/49): loss=6.725423770858433e+23\n",
      "Gradient Descent(21/49): loss=1.6348905390675547e+25\n",
      "Gradient Descent(22/49): loss=3.974273095347443e+26\n",
      "Gradient Descent(23/49): loss=9.661103455533093e+27\n",
      "Gradient Descent(24/49): loss=2.3485280890178085e+29\n",
      "Gradient Descent(25/49): loss=5.709062334641463e+30\n",
      "Gradient Descent(26/49): loss=1.3878221381824057e+32\n",
      "Gradient Descent(27/49): loss=3.37367184720031e+33\n",
      "Gradient Descent(28/49): loss=8.201095384959202e+34\n",
      "Gradient Descent(29/49): loss=1.9936131479122538e+36\n",
      "Gradient Descent(30/49): loss=4.846295765341135e+37\n",
      "Gradient Descent(31/49): loss=1.1780912796326031e+39\n",
      "Gradient Descent(32/49): loss=2.8638348345805243e+40\n",
      "Gradient Descent(33/49): loss=6.961727076287569e+41\n",
      "Gradient Descent(34/49): loss=1.6923337651843218e+43\n",
      "Gradient Descent(35/49): loss=4.11391245505433e+44\n",
      "Gradient Descent(36/49): loss=1.0000554285465166e+46\n",
      "Gradient Descent(37/49): loss=2.4310455584358376e+47\n",
      "Gradient Descent(38/49): loss=5.90965494360649e+48\n",
      "Gradient Descent(39/49): loss=1.4365844124683485e+50\n",
      "Gradient Descent(40/49): loss=3.4922085872032515e+51\n",
      "Gradient Descent(41/49): loss=8.489247628394866e+52\n",
      "Gradient Descent(42/49): loss=2.0636603884512515e+54\n",
      "Gradient Descent(43/49): loss=5.016574360039114e+55\n",
      "Gradient Descent(44/49): loss=1.2194844874009932e+57\n",
      "Gradient Descent(45/49): loss=2.9644580310777455e+58\n",
      "Gradient Descent(46/49): loss=7.206333093051854e+59\n",
      "Gradient Descent(47/49): loss=1.7517953063796323e+61\n",
      "Gradient Descent(48/49): loss=4.258458158716861e+62\n",
      "Gradient Descent(49/49): loss=1.0351931999989295e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7101158007760106\n",
      "Gradient Descent(2/49): loss=1.8168031257992552\n",
      "Gradient Descent(3/49): loss=7.936123779213182\n",
      "Gradient Descent(4/49): loss=64.64639783193772\n",
      "Gradient Descent(5/49): loss=1022.4975051150241\n",
      "Gradient Descent(6/49): loss=21970.432250071153\n",
      "Gradient Descent(7/49): loss=508914.5654222829\n",
      "Gradient Descent(8/49): loss=11962597.481272325\n",
      "Gradient Descent(9/49): loss=281961125.2354623\n",
      "Gradient Descent(10/49): loss=6649216283.810048\n",
      "Gradient Descent(11/49): loss=156816439898.516\n",
      "Gradient Descent(12/49): loss=3698452458572.22\n",
      "Gradient Descent(13/49): loss=87226778554172.89\n",
      "Gradient Descent(14/49): loss=2057215902230622.8\n",
      "Gradient Descent(15/49): loss=4.8518789513109416e+16\n",
      "Gradient Descent(16/49): loss=1.1443004004465608e+18\n",
      "Gradient Descent(17/49): loss=2.6987965367320523e+19\n",
      "Gradient Descent(18/49): loss=6.365026830812237e+20\n",
      "Gradient Descent(19/49): loss=1.5011715781578715e+22\n",
      "Gradient Descent(20/49): loss=3.540465998051588e+23\n",
      "Gradient Descent(21/49): loss=8.350077809753526e+24\n",
      "Gradient Descent(22/49): loss=1.9693396142586486e+26\n",
      "Gradient Descent(23/49): loss=4.6446256006845467e+27\n",
      "Gradient Descent(24/49): loss=1.0954203538253595e+29\n",
      "Gradient Descent(25/49): loss=2.583514484780062e+30\n",
      "Gradient Descent(26/49): loss=6.093137734532559e+31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=1.4370473891554442e+33\n",
      "Gradient Descent(28/49): loss=3.389231113182729e+34\n",
      "Gradient Descent(29/49): loss=7.993395085819962e+35\n",
      "Gradient Descent(30/49): loss=1.8852171145688228e+37\n",
      "Gradient Descent(31/49): loss=4.446225328418865e+38\n",
      "Gradient Descent(32/49): loss=1.0486282730143162e+40\n",
      "Gradient Descent(33/49): loss=2.4731568324631315e+41\n",
      "Gradient Descent(34/49): loss=5.832862679142708e+42\n",
      "Gradient Descent(35/49): loss=1.3756623351642526e+44\n",
      "Gradient Descent(36/49): loss=3.244456392153016e+45\n",
      "Gradient Descent(37/49): loss=7.651948455305899e+46\n",
      "Gradient Descent(38/49): loss=1.8046879996375543e+48\n",
      "Gradient Descent(39/49): loss=4.2562999411964987e+49\n",
      "Gradient Descent(40/49): loss=1.0038349672114246e+51\n",
      "Gradient Descent(41/49): loss=2.3675132281985716e+52\n",
      "Gradient Descent(42/49): loss=5.5837055579621465e+53\n",
      "Gradient Descent(43/49): loss=1.316899411022103e+55\n",
      "Gradient Descent(44/49): loss=3.1058658819812266e+56\n",
      "Gradient Descent(45/49): loss=7.325087091783332e+57\n",
      "Gradient Descent(46/49): loss=1.7275987740971819e+59\n",
      "Gradient Descent(47/49): loss=4.0744874250163846e+60\n",
      "Gradient Descent(48/49): loss=9.609550565519464e+61\n",
      "Gradient Descent(49/49): loss=2.266382306257915e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.715589432952839\n",
      "Gradient Descent(2/49): loss=1.8334323855402008\n",
      "Gradient Descent(3/49): loss=7.864464143017895\n",
      "Gradient Descent(4/49): loss=61.367179765798625\n",
      "Gradient Descent(5/49): loss=940.5101582565607\n",
      "Gradient Descent(6/49): loss=20030.320934384945\n",
      "Gradient Descent(7/49): loss=463277.61751772877\n",
      "Gradient Descent(8/49): loss=10889517.131472975\n",
      "Gradient Descent(9/49): loss=256728160.42735234\n",
      "Gradient Descent(10/49): loss=6055860827.727646\n",
      "Gradient Descent(11/49): loss=142863618208.32138\n",
      "Gradient Descent(12/49): loss=3370352503459.2095\n",
      "Gradient Descent(13/49): loss=79511592758225.9\n",
      "Gradient Descent(14/49): loss=1875797033451268.0\n",
      "Gradient Descent(15/49): loss=4.425285394690286e+16\n",
      "Gradient Descent(16/49): loss=1.0439909474362787e+18\n",
      "Gradient Descent(17/49): loss=2.4629306477534376e+19\n",
      "Gradient Descent(18/49): loss=5.81042143577948e+20\n",
      "Gradient Descent(19/49): loss=1.3707652424694936e+22\n",
      "Gradient Descent(20/49): loss=3.233840042022276e+23\n",
      "Gradient Descent(21/49): loss=7.629111895622054e+24\n",
      "Gradient Descent(22/49): loss=1.7998214988880584e+26\n",
      "Gradient Descent(23/49): loss=4.246047865307728e+27\n",
      "Gradient Descent(24/49): loss=1.0017061406157877e+29\n",
      "Gradient Descent(25/49): loss=2.3631744718324882e+30\n",
      "Gradient Descent(26/49): loss=5.575081710977398e+31\n",
      "Gradient Descent(27/49): loss=1.315245084717422e+33\n",
      "Gradient Descent(28/49): loss=3.1028596934591885e+34\n",
      "Gradient Descent(29/49): loss=7.320109680822156e+35\n",
      "Gradient Descent(30/49): loss=1.726923258960794e+37\n",
      "Gradient Descent(31/49): loss=4.074070024050355e+38\n",
      "Gradient Descent(32/49): loss=9.611339979782113e+39\n",
      "Gradient Descent(33/49): loss=2.267458724607695e+41\n",
      "Gradient Descent(34/49): loss=5.349273960358084e+42\n",
      "Gradient Descent(35/49): loss=1.2619736620747331e+44\n",
      "Gradient Descent(36/49): loss=2.9771844470341766e+45\n",
      "Gradient Descent(37/49): loss=7.023622994706736e+46\n",
      "Gradient Descent(38/49): loss=1.6569776192709265e+48\n",
      "Gradient Descent(39/49): loss=3.909057807963239e+49\n",
      "Gradient Descent(40/49): loss=9.222051504064358e+50\n",
      "Gradient Descent(41/49): loss=2.1756197560026703e+52\n",
      "Gradient Descent(42/49): loss=5.132612109814168e+53\n",
      "Gradient Descent(43/49): loss=1.2108598939280708e+55\n",
      "Gradient Descent(44/49): loss=2.8565994299860267e+56\n",
      "Gradient Descent(45/49): loss=6.739144920329788e+57\n",
      "Gradient Descent(46/49): loss=1.589864990536318e+59\n",
      "Gradient Descent(47/49): loss=3.750729088059639e+60\n",
      "Gradient Descent(48/49): loss=8.84853039456586e+61\n",
      "Gradient Descent(49/49): loss=2.0875005447023766e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7037329756035807\n",
      "Gradient Descent(2/49): loss=1.7500245950872422\n",
      "Gradient Descent(3/49): loss=7.055907186514192\n",
      "Gradient Descent(4/49): loss=48.14784582446504\n",
      "Gradient Descent(5/49): loss=662.674902267106\n",
      "Gradient Descent(6/49): loss=13801.138719808156\n",
      "Gradient Descent(7/49): loss=321890.469953979\n",
      "Gradient Descent(8/49): loss=7677364.882680945\n",
      "Gradient Descent(9/49): loss=183860564.5306093\n",
      "Gradient Descent(10/49): loss=4406397303.211265\n",
      "Gradient Descent(11/49): loss=105617493007.98436\n",
      "Gradient Descent(12/49): loss=2531618712478.089\n",
      "Gradient Descent(13/49): loss=60682375676181.77\n",
      "Gradient Descent(14/49): loss=1454545065634371.2\n",
      "Gradient Descent(15/49): loss=3.486517479763335e+16\n",
      "Gradient Descent(16/49): loss=8.357117781721787e+17\n",
      "Gradient Descent(17/49): loss=2.0031856523693375e+19\n",
      "Gradient Descent(18/49): loss=4.80159890742348e+20\n",
      "Gradient Descent(19/49): loss=1.1509343651562223e+22\n",
      "Gradient Descent(20/49): loss=2.7587683571045494e+23\n",
      "Gradient Descent(21/49): loss=6.612716657533e+24\n",
      "Gradient Descent(22/49): loss=1.5850559355677168e+26\n",
      "Gradient Descent(23/49): loss=3.7993497211418594e+27\n",
      "Gradient Descent(24/49): loss=9.106970914796841e+28\n",
      "Gradient Descent(25/49): loss=2.1829240614898486e+30\n",
      "Gradient Descent(26/49): loss=5.232428546015317e+31\n",
      "Gradient Descent(27/49): loss=1.2542034316333413e+33\n",
      "Gradient Descent(28/49): loss=3.0063023968455164e+34\n",
      "Gradient Descent(29/49): loss=7.206051166284233e+35\n",
      "Gradient Descent(30/49): loss=1.7272771184160231e+37\n",
      "Gradient Descent(31/49): loss=4.140251262387268e+38\n",
      "Gradient Descent(32/49): loss=9.924105595411888e+39\n",
      "Gradient Descent(33/49): loss=2.378789731039111e+41\n",
      "Gradient Descent(34/49): loss=5.701914928347053e+42\n",
      "Gradient Descent(35/49): loss=1.3667384479545724e+44\n",
      "Gradient Descent(36/49): loss=3.276046746735401e+45\n",
      "Gradient Descent(37/49): loss=7.852623377104313e+46\n",
      "Gradient Descent(38/49): loss=1.8822592798498197e+48\n",
      "Gradient Descent(39/49): loss=4.511740632959338e+49\n",
      "Gradient Descent(40/49): loss=1.081455873641382e+51\n",
      "Gradient Descent(41/49): loss=2.592229699751939e+52\n",
      "Gradient Descent(42/49): loss=6.213526580284985e+53\n",
      "Gradient Descent(43/49): loss=1.489370813381324e+55\n",
      "Gradient Descent(44/49): loss=3.5699942554206857e+56\n",
      "Gradient Descent(45/49): loss=8.557210111296604e+57\n",
      "Gradient Descent(46/49): loss=2.051147415088715e+59\n",
      "Gradient Descent(47/49): loss=4.916562365193128e+60\n",
      "Gradient Descent(48/49): loss=1.1784908930979178e+62\n",
      "Gradient Descent(49/49): loss=2.8248208442285326e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7169670621354772\n",
      "Gradient Descent(2/49): loss=1.8724136968821266\n",
      "Gradient Descent(3/49): loss=8.594017373601273\n",
      "Gradient Descent(4/49): loss=77.74484684557213\n",
      "Gradient Descent(5/49): loss=1367.6885863592827\n",
      "Gradient Descent(6/49): loss=31613.632866416912\n",
      "Gradient Descent(7/49): loss=777190.9139978287\n",
      "Gradient Descent(8/49): loss=19326341.238932688\n",
      "Gradient Descent(9/49): loss=481567200.77828306\n",
      "Gradient Descent(10/49): loss=12003855161.080032\n",
      "Gradient Descent(11/49): loss=299234907050.39777\n",
      "Gradient Descent(12/49): loss=7459481608646.064\n",
      "Gradient Descent(13/49): loss=185954162078990.22\n",
      "Gradient Descent(14/49): loss=4635571791624971.0\n",
      "Gradient Descent(15/49): loss=1.155581941610855e+17\n",
      "Gradient Descent(16/49): loss=2.880701019019449e+18\n",
      "Gradient Descent(17/49): loss=7.181176927571063e+19\n",
      "Gradient Descent(18/49): loss=1.790165023797805e+21\n",
      "Gradient Descent(19/49): loss=4.462626175269624e+22\n",
      "Gradient Descent(20/49): loss=1.112469080541192e+24\n",
      "Gradient Descent(21/49): loss=2.7732268098565497e+25\n",
      "Gradient Descent(22/49): loss=6.913259050011596e+26\n",
      "Gradient Descent(23/49): loss=1.7233769168358333e+28\n",
      "Gradient Descent(24/49): loss=4.2961329468446314e+29\n",
      "Gradient Descent(25/49): loss=1.0709646924394784e+31\n",
      "Gradient Descent(26/49): loss=2.6697622877206317e+32\n",
      "Gradient Descent(27/49): loss=6.655336747563164e+33\n",
      "Gradient Descent(28/49): loss=1.659080564108256e+35\n",
      "Gradient Descent(29/49): loss=4.135851306411599e+36\n",
      "Gradient Descent(30/49): loss=1.0310087646611903e+38\n",
      "Gradient Descent(31/49): loss=2.5701578564013963e+39\n",
      "Gradient Descent(32/49): loss=6.407037101175904e+40\n",
      "Gradient Descent(33/49): loss=1.597183002343712e+42\n",
      "Gradient Descent(34/49): loss=3.9815495098468656e+43\n",
      "Gradient Descent(35/49): loss=9.925435267028893e+44\n",
      "Gradient Descent(36/49): loss=2.4742695022714723e+46\n",
      "Gradient Descent(37/49): loss=6.1680011054098045e+47\n",
      "Gradient Descent(38/49): loss=1.5375947366044973e+49\n",
      "Gradient Descent(39/49): loss=3.833004459030104e+50\n",
      "Gradient Descent(40/49): loss=9.555133633839869e+51\n",
      "Gradient Descent(41/49): loss=2.3819585846148505e+53\n",
      "Gradient Descent(42/49): loss=5.937883148726076e+54\n",
      "Gradient Descent(43/49): loss=1.480229610861448e+56\n",
      "Gradient Descent(44/49): loss=3.690001379264496e+57\n",
      "Gradient Descent(45/49): loss=9.198647344346312e+58\n",
      "Gradient Descent(46/49): loss=2.2930916351721943e+60\n",
      "Gradient Descent(47/49): loss=5.716350513783488e+61\n",
      "Gradient Descent(48/49): loss=1.4250046834250628e+63\n",
      "Gradient Descent(49/49): loss=3.552333508743086e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7205746802992854\n",
      "Gradient Descent(2/49): loss=1.9084116281055732\n",
      "Gradient Descent(3/49): loss=8.642085675557617\n",
      "Gradient Descent(4/49): loss=72.12749512169268\n",
      "Gradient Descent(5/49): loss=1163.5573260822218\n",
      "Gradient Descent(6/49): loss=25580.31511584606\n",
      "Gradient Descent(7/49): loss=607311.2238664472\n",
      "Gradient Descent(8/49): loss=14638322.803889358\n",
      "Gradient Descent(9/49): loss=353833800.22155434\n",
      "Gradient Descent(10/49): loss=8557252718.398057\n",
      "Gradient Descent(11/49): loss=206971858564.80328\n",
      "Gradient Descent(12/49): loss=5006059083321.555\n",
      "Gradient Descent(13/49): loss=121082692128125.12\n",
      "Gradient Descent(14/49): loss=2928656447916929.0\n",
      "Gradient Descent(15/49): loss=7.08361318822945e+16\n",
      "Gradient Descent(16/49): loss=1.713330932700004e+18\n",
      "Gradient Descent(17/49): loss=4.144075654759954e+19\n",
      "Gradient Descent(18/49): loss=1.0023377688836845e+21\n",
      "Gradient Descent(19/49): loss=2.424379009361821e+22\n",
      "Gradient Descent(20/49): loss=5.863905126233365e+23\n",
      "Gradient Descent(21/49): loss=1.4183171524269893e+25\n",
      "Gradient Descent(22/49): loss=3.4305185734858054e+26\n",
      "Gradient Descent(23/49): loss=8.297479631332544e+27\n",
      "Gradient Descent(24/49): loss=2.0069318022208784e+29\n",
      "Gradient Descent(25/49): loss=4.854215301181485e+30\n",
      "Gradient Descent(26/49): loss=1.1741009915807724e+32\n",
      "Gradient Descent(27/49): loss=2.839826939887479e+33\n",
      "Gradient Descent(28/49): loss=6.868759251836477e+34\n",
      "Gradient Descent(29/49): loss=1.661363690759175e+36\n",
      "Gradient Descent(30/49): loss=4.018381212349158e+37\n",
      "Gradient Descent(31/49): loss=9.719357451698356e+38\n",
      "Gradient Descent(32/49): loss=2.3508448870797437e+40\n",
      "Gradient Descent(33/49): loss=5.686046336471844e+41\n",
      "Gradient Descent(34/49): loss=1.3752980095878532e+43\n",
      "Gradient Descent(35/49): loss=3.326467114845786e+44\n",
      "Gradient Descent(36/49): loss=8.045807809658978e+45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=1.946059319842976e+47\n",
      "Gradient Descent(38/49): loss=4.706981531178549e+48\n",
      "Gradient Descent(39/49): loss=1.1384891975771996e+50\n",
      "Gradient Descent(40/49): loss=2.7536918180246422e+51\n",
      "Gradient Descent(41/49): loss=6.660422114494094e+52\n",
      "Gradient Descent(42/49): loss=1.6109726750419056e+54\n",
      "Gradient Descent(43/49): loss=3.896499223501359e+55\n",
      "Gradient Descent(44/49): loss=9.424558488151404e+56\n",
      "Gradient Descent(45/49): loss=2.2795411368457657e+58\n",
      "Gradient Descent(46/49): loss=5.5135822023970285e+59\n",
      "Gradient Descent(47/49): loss=1.3335836853838704e+61\n",
      "Gradient Descent(48/49): loss=3.2255716531238533e+62\n",
      "Gradient Descent(49/49): loss=7.801769475337688e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7262815225293225\n",
      "Gradient Descent(2/49): loss=1.92632126843296\n",
      "Gradient Descent(3/49): loss=8.566753797875096\n",
      "Gradient Descent(4/49): loss=68.50845985375364\n",
      "Gradient Descent(5/49): loss=1070.5757617445963\n",
      "Gradient Descent(6/49): loss=23323.141825249928\n",
      "Gradient Descent(7/49): loss=552857.2170980219\n",
      "Gradient Descent(8/49): loss=13325200.06325545\n",
      "Gradient Descent(9/49): loss=322167299.90709513\n",
      "Gradient Descent(10/49): loss=7793582833.402178\n",
      "Gradient Descent(11/49): loss=188555156460.2447\n",
      "Gradient Descent(12/49): loss=4561923927858.046\n",
      "Gradient Descent(13/49): loss=110372073332607.0\n",
      "Gradient Descent(14/49): loss=2670365109006714.0\n",
      "Gradient Descent(15/49): loss=6.460738165853766e+16\n",
      "Gradient Descent(16/49): loss=1.5631247735206584e+18\n",
      "Gradient Descent(17/49): loss=3.78185743594971e+19\n",
      "Gradient Descent(18/49): loss=9.149906596517667e+20\n",
      "Gradient Descent(19/49): loss=2.213747930858495e+22\n",
      "Gradient Descent(20/49): loss=5.355988992682946e+23\n",
      "Gradient Descent(21/49): loss=1.2958394083624228e+25\n",
      "Gradient Descent(22/49): loss=3.1351815221437684e+26\n",
      "Gradient Descent(23/49): loss=7.585325089946601e+27\n",
      "Gradient Descent(24/49): loss=1.8352097418854198e+29\n",
      "Gradient Descent(25/49): loss=4.44014561903895e+30\n",
      "Gradient Descent(26/49): loss=1.0742583078278517e+32\n",
      "Gradient Descent(27/49): loss=2.5990834782282875e+33\n",
      "Gradient Descent(28/49): loss=6.288278040370243e+34\n",
      "Gradient Descent(29/49): loss=1.5213994103782218e+36\n",
      "Gradient Descent(30/49): loss=3.6809062052270927e+37\n",
      "Gradient Descent(31/49): loss=8.905663035791037e+38\n",
      "Gradient Descent(32/49): loss=2.154655122546379e+40\n",
      "Gradient Descent(33/49): loss=5.213018591044145e+41\n",
      "Gradient Descent(34/49): loss=1.2612488442445519e+43\n",
      "Gradient Descent(35/49): loss=3.051492373039126e+44\n",
      "Gradient Descent(36/49): loss=7.382845776396367e+45\n",
      "Gradient Descent(37/49): loss=1.7862214646064737e+47\n",
      "Gradient Descent(38/49): loss=4.321622335416353e+48\n",
      "Gradient Descent(39/49): loss=1.0455825316198424e+50\n",
      "Gradient Descent(40/49): loss=2.5297046932335548e+51\n",
      "Gradient Descent(41/49): loss=6.120421527178546e+52\n",
      "Gradient Descent(42/49): loss=1.4807878473146334e+54\n",
      "Gradient Descent(43/49): loss=3.5826497227643326e+55\n",
      "Gradient Descent(44/49): loss=8.667939204998085e+56\n",
      "Gradient Descent(45/49): loss=2.0971397115421107e+58\n",
      "Gradient Descent(46/49): loss=5.073864578089055e+59\n",
      "Gradient Descent(47/49): loss=1.2275816253489469e+61\n",
      "Gradient Descent(48/49): loss=2.97003718507179e+62\n",
      "Gradient Descent(49/49): loss=7.185771356101697e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7139796034020145\n",
      "Gradient Descent(2/49): loss=1.8377291283646\n",
      "Gradient Descent(3/49): loss=7.689128799140532\n",
      "Gradient Descent(4/49): loss=53.84930400854344\n",
      "Gradient Descent(5/49): loss=755.0920616841496\n",
      "Gradient Descent(6/49): loss=16071.769566481184\n",
      "Gradient Descent(7/49): loss=384065.81632752327\n",
      "Gradient Descent(8/49): loss=9392153.347775955\n",
      "Gradient Descent(9/49): loss=230656172.3721889\n",
      "Gradient Descent(10/49): loss=5668887687.932486\n",
      "Gradient Descent(11/49): loss=139344765546.5231\n",
      "Gradient Descent(12/49): loss=3425265873463.0703\n",
      "Gradient Descent(13/49): loss=84197629922621.17\n",
      "Gradient Descent(14/49): loss=2069692364281348.5\n",
      "Gradient Descent(15/49): loss=5.0875863272934936e+16\n",
      "Gradient Descent(16/49): loss=1.2505981934876168e+18\n",
      "Gradient Descent(17/49): loss=3.0741411513328628e+19\n",
      "Gradient Descent(18/49): loss=7.556658785855632e+20\n",
      "Gradient Descent(19/49): loss=1.8575299312423065e+22\n",
      "Gradient Descent(20/49): loss=4.56606225494171e+23\n",
      "Gradient Descent(21/49): loss=1.1224004612492771e+25\n",
      "Gradient Descent(22/49): loss=2.7590136206538294e+26\n",
      "Gradient Descent(23/49): loss=6.782032279710607e+27\n",
      "Gradient Descent(24/49): loss=1.6671161569741554e+29\n",
      "Gradient Descent(25/49): loss=4.097999192895179e+30\n",
      "Gradient Descent(26/49): loss=1.0073441682341969e+32\n",
      "Gradient Descent(27/49): loss=2.4761895391163874e+33\n",
      "Gradient Descent(28/49): loss=6.08681206183743e+34\n",
      "Gradient Descent(29/49): loss=1.496221532756726e+36\n",
      "Gradient Descent(30/49): loss=3.677916867387409e+37\n",
      "Gradient Descent(31/49): loss=9.040821955349017e+38\n",
      "Gradient Descent(32/49): loss=2.2223575076720937e+40\n",
      "Gradient Descent(33/49): loss=5.4628582625547846e+41\n",
      "Gradient Descent(34/49): loss=1.3428451675186623e+43\n",
      "Gradient Descent(35/49): loss=3.3008968149302122e+44\n",
      "Gradient Descent(36/49): loss=8.11405517655459e+45\n",
      "Gradient Descent(37/49): loss=1.994545576534961e+47\n",
      "Gradient Descent(38/49): loss=4.9028654234075854e+48\n",
      "Gradient Descent(39/49): loss=1.205191279800489e+50\n",
      "Gradient Descent(40/49): loss=2.962524759444906e+51\n",
      "Gradient Descent(41/49): loss=7.282290452497309e+52\n",
      "Gradient Descent(42/49): loss=1.790086447900986e+54\n",
      "Gradient Descent(43/49): loss=4.4002769621195944e+55\n",
      "Gradient Descent(44/49): loss=1.0816481721351625e+57\n",
      "Gradient Descent(45/49): loss=2.658838928447304e+58\n",
      "Gradient Descent(46/49): loss=6.535789205349425e+59\n",
      "Gradient Descent(47/49): loss=1.6065862463397438e+61\n",
      "Gradient Descent(48/49): loss=3.949208405949541e+62\n",
      "Gradient Descent(49/49): loss=9.707693607582784e+63\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7274921209318354\n",
      "Gradient Descent(2/49): loss=1.9661542994226773\n",
      "Gradient Descent(3/49): loss=9.346365076925556\n",
      "Gradient Descent(4/49): loss=86.52696787184502\n",
      "Gradient Descent(5/49): loss=1552.5194384918839\n",
      "Gradient Descent(6/49): loss=36718.234555381496\n",
      "Gradient Descent(7/49): loss=924972.7049168664\n",
      "Gradient Descent(8/49): loss=23577695.271321084\n",
      "Gradient Descent(9/49): loss=602271953.3505173\n",
      "Gradient Descent(10/49): loss=15390310815.042719\n",
      "Gradient Descent(11/49): loss=393306552691.3646\n",
      "Gradient Descent(12/49): loss=10051251299178.979\n",
      "Gradient Descent(13/49): loss=256867995433428.94\n",
      "Gradient Descent(14/49): loss=6564475390420391.0\n",
      "Gradient Descent(15/49): loss=1.6776064276088707e+17\n",
      "Gradient Descent(16/49): loss=4.287263173107537e+18\n",
      "Gradient Descent(17/49): loss=1.095645870900645e+20\n",
      "Gradient Descent(18/49): loss=2.8000144297159284e+21\n",
      "Gradient Descent(19/49): loss=7.155670472880038e+22\n",
      "Gradient Descent(20/49): loss=1.8286912872175525e+24\n",
      "Gradient Descent(21/49): loss=4.6733731473892895e+25\n",
      "Gradient Descent(22/49): loss=1.1943194965388976e+27\n",
      "Gradient Descent(23/49): loss=3.0521831123410777e+28\n",
      "Gradient Descent(24/49): loss=7.800108579201107e+29\n",
      "Gradient Descent(25/49): loss=1.9933828216702777e+31\n",
      "Gradient Descent(26/49): loss=5.094256103467072e+32\n",
      "Gradient Descent(27/49): loss=1.301879647280529e+34\n",
      "Gradient Descent(28/49): loss=3.3270620510219474e+35\n",
      "Gradient Descent(29/49): loss=8.502584639427305e+36\n",
      "Gradient Descent(30/49): loss=2.172906439434144e+38\n",
      "Gradient Descent(31/49): loss=5.553043685845975e+39\n",
      "Gradient Descent(32/49): loss=1.419126641501629e+41\n",
      "Gradient Descent(33/49): loss=3.626696526362544e+42\n",
      "Gradient Descent(34/49): loss=9.268325538876894e+43\n",
      "Gradient Descent(35/49): loss=2.3685979146635038e+45\n",
      "Gradient Descent(36/49): loss=6.053149576820119e+46\n",
      "Gradient Descent(37/49): loss=1.546932874192035e+48\n",
      "Gradient Descent(38/49): loss=3.9533160165408443e+49\n",
      "Gradient Descent(39/49): loss=1.0103028895032945e+51\n",
      "Gradient Descent(40/49): loss=2.581913321039859e+52\n",
      "Gradient Descent(41/49): loss=6.598294894158378e+53\n",
      "Gradient Descent(42/49): loss=1.686249308041905e+55\n",
      "Gradient Descent(43/49): loss=4.309350786048119e+56\n",
      "Gradient Descent(44/49): loss=1.1012905451553463e+58\n",
      "Gradient Descent(45/49): loss=2.814439865919631e+59\n",
      "Gradient Descent(46/49): loss=7.192535878676974e+60\n",
      "Gradient Descent(47/49): loss=1.8381125492319712e+62\n",
      "Gradient Descent(48/49): loss=4.697449968460198e+63\n",
      "Gradient Descent(49/49): loss=1.2004725290301932e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.731205375119476\n",
      "Gradient Descent(2/49): loss=2.004332553899682\n",
      "Gradient Descent(3/49): loss=9.402455281433337\n",
      "Gradient Descent(4/49): loss=80.36802745292825\n",
      "Gradient Descent(5/49): loss=1321.9938079700694\n",
      "Gradient Descent(6/49): loss=29727.345384650776\n",
      "Gradient Descent(7/49): loss=723141.3370620512\n",
      "Gradient Descent(8/49): loss=17867572.481940333\n",
      "Gradient Descent(9/49): loss=442772997.8555427\n",
      "Gradient Descent(10/49): loss=10978258406.138107\n",
      "Gradient Descent(11/49): loss=272226064690.5488\n",
      "Gradient Descent(12/49): loss=6750471554155.323\n",
      "Gradient Descent(13/49): loss=167394054988189.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=4150937829384434.0\n",
      "Gradient Descent(15/49): loss=1.0293249021663941e+17\n",
      "Gradient Descent(16/49): loss=2.5524587968101033e+18\n",
      "Gradient Descent(17/49): loss=6.329435849220055e+19\n",
      "Gradient Descent(18/49): loss=1.5695359411908793e+21\n",
      "Gradient Descent(19/49): loss=3.892042086725397e+22\n",
      "Gradient Descent(20/49): loss=9.65125500327636e+23\n",
      "Gradient Descent(21/49): loss=2.393260942794443e+25\n",
      "Gradient Descent(22/49): loss=5.93466646396352e+26\n",
      "Gradient Descent(23/49): loss=1.471643371966613e+28\n",
      "Gradient Descent(24/49): loss=3.6492939028740474e+29\n",
      "Gradient Descent(25/49): loss=9.049302462292202e+30\n",
      "Gradient Descent(26/49): loss=2.2439923238178944e+32\n",
      "Gradient Descent(27/49): loss=5.564519000592715e+33\n",
      "Gradient Descent(28/49): loss=1.3798564005458358e+35\n",
      "Gradient Descent(29/49): loss=3.4216860180089354e+36\n",
      "Gradient Descent(30/49): loss=8.484893936214162e+37\n",
      "Gradient Descent(31/49): loss=2.1040336468597473e+39\n",
      "Gradient Descent(32/49): loss=5.217457778963417e+40\n",
      "Gradient Descent(33/49): loss=1.2937942183526265e+42\n",
      "Gradient Descent(34/49): loss=3.2082741257472246e+43\n",
      "Gradient Descent(35/49): loss=7.95568779016899e+44\n",
      "Gradient Descent(36/49): loss=1.9728042472026152e+46\n",
      "Gradient Descent(37/49): loss=4.8920429011682e+47\n",
      "Gradient Descent(38/49): loss=1.2130997680487104e+49\n",
      "Gradient Descent(39/49): loss=3.0081728164902337e+50\n",
      "Gradient Descent(40/49): loss=7.459488437976307e+51\n",
      "Gradient Descent(41/49): loss=1.849759676414577e+53\n",
      "Gradient Descent(42/49): loss=4.58692427629483e+54\n",
      "Gradient Descent(43/49): loss=1.1374382621013817e+56\n",
      "Gradient Descent(44/49): loss=2.8205519039815554e+57\n",
      "Gradient Descent(45/49): loss=6.99423723302259e+58\n",
      "Gradient Descent(46/49): loss=1.7343894435249654e+60\n",
      "Gradient Descent(47/49): loss=4.3008360191278787e+61\n",
      "Gradient Descent(48/49): loss=1.0664957938070866e+63\n",
      "Gradient Descent(49/49): loss=2.6446329810054815e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.737149085587563\n",
      "Gradient Descent(2/49): loss=2.0235887577857867\n",
      "Gradient Descent(3/49): loss=9.323367776218156\n",
      "Gradient Descent(4/49): loss=76.37899261442314\n",
      "Gradient Descent(5/49): loss=1216.709773705134\n",
      "Gradient Descent(6/49): loss=27106.246909375568\n",
      "Gradient Descent(7/49): loss=658309.678676039\n",
      "Gradient Descent(8/49): loss=16264747.925141247\n",
      "Gradient Descent(9/49): loss=403144929.4669475\n",
      "Gradient Descent(10/49): loss=9998473184.012318\n",
      "Gradient Descent(11/49): loss=248001296822.12753\n",
      "Gradient Descent(12/49): loss=6151528519298.787\n",
      "Gradient Descent(13/49): loss=152585674345569.38\n",
      "Gradient Descent(14/49): loss=3784816094921050.0\n",
      "Gradient Descent(15/49): loss=9.388060030395285e+16\n",
      "Gradient Descent(16/49): loss=2.3286646168930826e+18\n",
      "Gradient Descent(17/49): loss=5.7761442769594065e+19\n",
      "Gradient Descent(18/49): loss=1.4327457244422566e+21\n",
      "Gradient Descent(19/49): loss=3.5538591364506853e+22\n",
      "Gradient Descent(20/49): loss=8.815182308080575e+23\n",
      "Gradient Descent(21/49): loss=2.186564974623525e+25\n",
      "Gradient Descent(22/49): loss=5.4236727286652776e+26\n",
      "Gradient Descent(23/49): loss=1.345316796393747e+28\n",
      "Gradient Descent(24/49): loss=3.3369957466157055e+29\n",
      "Gradient Descent(25/49): loss=8.277262755346134e+30\n",
      "Gradient Descent(26/49): loss=2.05313653128041e+32\n",
      "Gradient Descent(27/49): loss=5.092709680329336e+33\n",
      "Gradient Descent(28/49): loss=1.2632229514686077e+35\n",
      "Gradient Descent(29/49): loss=3.1333657822290276e+36\n",
      "Gradient Descent(30/49): loss=7.772168098932619e+37\n",
      "Gradient Descent(31/49): loss=1.9278501508078607e+39\n",
      "Gradient Descent(32/49): loss=4.781942640278648e+40\n",
      "Gradient Descent(33/49): loss=1.1861386324727067e+42\n",
      "Gradient Descent(34/49): loss=2.9421617139309785e+43\n",
      "Gradient Descent(35/49): loss=7.297895299873948e+44\n",
      "Gradient Descent(36/49): loss=1.8102089887086325e+46\n",
      "Gradient Descent(37/49): loss=4.490139208845701e+47\n",
      "Gradient Descent(38/49): loss=1.1137581484001182e+49\n",
      "Gradient Descent(39/49): loss=2.762625289398458e+50\n",
      "Gradient Descent(40/49): loss=6.852563548547077e+51\n",
      "Gradient Descent(41/49): loss=1.69974651890269e+53\n",
      "Gradient Descent(42/49): loss=4.21614218978581e+54\n",
      "Gradient Descent(43/49): loss=1.045794462104151e+56\n",
      "Gradient Descent(44/49): loss=2.5940445263381048e+57\n",
      "Gradient Descent(45/49): loss=6.434406805985169e+58\n",
      "Gradient Descent(46/49): loss=1.5960246836377155e+60\n",
      "Gradient Descent(47/49): loss=3.958865001217192e+61\n",
      "Gradient Descent(48/49): loss=9.81978052002333e+62\n",
      "Gradient Descent(49/49): loss=2.435750888999364e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.724395024328792\n",
      "Gradient Descent(2/49): loss=1.9295803990224991\n",
      "Gradient Descent(3/49): loss=8.37179836510152\n",
      "Gradient Descent(4/49): loss=60.14406947714387\n",
      "Gradient Descent(5/49): loss=859.042023988453\n",
      "Gradient Descent(6/49): loss=18680.935908684474\n",
      "Gradient Descent(7/49): loss=457247.88095287717\n",
      "Gradient Descent(8/49): loss=11461188.984192962\n",
      "Gradient Descent(9/49): loss=288547074.48174995\n",
      "Gradient Descent(10/49): loss=7270277100.911363\n",
      "Gradient Descent(11/49): loss=183209606920.75558\n",
      "Gradient Descent(12/49): loss=4616968812381.636\n",
      "Gradient Descent(13/49): loss=116350353933221.66\n",
      "Gradient Descent(14/49): loss=2932100498269217.0\n",
      "Gradient Descent(15/49): loss=7.38907479655927e+16\n",
      "Gradient Descent(16/49): loss=1.8620926510218022e+18\n",
      "Gradient Descent(17/49): loss=4.692588929324494e+19\n",
      "Gradient Descent(18/49): loss=1.1825615050932504e+21\n",
      "Gradient Descent(19/49): loss=2.9801283142993266e+22\n",
      "Gradient Descent(20/49): loss=7.510108126900656e+23\n",
      "Gradient Descent(21/49): loss=1.892593812401455e+25\n",
      "Gradient Descent(22/49): loss=4.769453752488354e+26\n",
      "Gradient Descent(23/49): loss=1.2019319173544648e+28\n",
      "Gradient Descent(24/49): loss=3.028942954319934e+29\n",
      "Gradient Descent(25/49): loss=7.633124046425481e+30\n",
      "Gradient Descent(26/49): loss=1.923594586851479e+32\n",
      "Gradient Descent(27/49): loss=4.847577626223766e+33\n",
      "Gradient Descent(28/49): loss=1.2216196179221132e+35\n",
      "Gradient Descent(29/49): loss=3.078557180433926e+36\n",
      "Gradient Descent(30/49): loss=7.758154972430565e+37\n",
      "Gradient Descent(31/49): loss=1.955103155425762e+39\n",
      "Gradient Descent(32/49): loss=4.926981172635869e+40\n",
      "Gradient Descent(33/49): loss=1.2416298039385119e+42\n",
      "Gradient Descent(34/49): loss=3.1289840898734527e+43\n",
      "Gradient Descent(35/49): loss=7.885233910804352e+44\n",
      "Gradient Descent(36/49): loss=1.9871278358150367e+46\n",
      "Gradient Descent(37/49): loss=5.007685352821841e+47\n",
      "Gradient Descent(38/49): loss=1.2619677577301541e+49\n",
      "Gradient Descent(39/49): loss=3.180236994429119e+50\n",
      "Gradient Descent(40/49): loss=8.014394408084431e+51\n",
      "Gradient Descent(41/49): loss=2.0196770819548246e+53\n",
      "Gradient Descent(42/49): loss=5.08971147122338e+54\n",
      "Gradient Descent(43/49): loss=1.2826388481484488e+56\n",
      "Gradient Descent(44/49): loss=3.232329423939079e+57\n",
      "Gradient Descent(45/49): loss=8.145670560302001e+58\n",
      "Gradient Descent(46/49): loss=2.052759486256522e+60\n",
      "Gradient Descent(47/49): loss=5.173081181250015e+61\n",
      "Gradient Descent(48/49): loss=1.3036485319868013e+63\n",
      "Gradient Descent(49/49): loss=3.2852751298612414e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7381875917948417\n",
      "Gradient Descent(2/49): loss=2.0642469388778215\n",
      "Gradient Descent(3/49): loss=10.155631969535905\n",
      "Gradient Descent(4/49): loss=96.178705562145\n",
      "Gradient Descent(5/49): loss=1759.6411605288215\n",
      "Gradient Descent(6/49): loss=42569.24247416667\n",
      "Gradient Descent(7/49): loss=1098508.766067734\n",
      "Gradient Descent(8/49): loss=28694163.668807477\n",
      "Gradient Descent(9/49): loss=751166646.4342561\n",
      "Gradient Descent(10/49): loss=19672042558.449047\n",
      "Gradient Descent(11/49): loss=515220413136.7979\n",
      "Gradient Descent(12/49): loss=13494043490699.736\n",
      "Gradient Descent(13/49): loss=353420812253733.6\n",
      "Gradient Descent(14/49): loss=9256404168303576.0\n",
      "Gradient Descent(15/49): loss=2.4243344270307955e+17\n",
      "Gradient Descent(16/49): loss=6.349547169807387e+18\n",
      "Gradient Descent(17/49): loss=1.6630027979552614e+20\n",
      "Gradient Descent(18/49): loss=4.3555520309873434e+21\n",
      "Gradient Descent(19/49): loss=1.1407577617631914e+23\n",
      "Gradient Descent(20/49): loss=2.987745897148304e+24\n",
      "Gradient Descent(21/49): loss=7.825171868346435e+25\n",
      "Gradient Descent(22/49): loss=2.049482013434617e+27\n",
      "Gradient Descent(23/49): loss=5.3677754227779695e+28\n",
      "Gradient Descent(24/49): loss=1.4058680583926218e+30\n",
      "Gradient Descent(25/49): loss=3.6820933104273185e+31\n",
      "Gradient Descent(26/49): loss=9.643729413836033e+32\n",
      "Gradient Descent(27/49): loss=2.5257783865476207e+34\n",
      "Gradient Descent(28/49): loss=6.615237927350139e+35\n",
      "Gradient Descent(29/49): loss=1.7325895679734209e+37\n",
      "Gradient Descent(30/49): loss=4.5378059625629985e+38\n",
      "Gradient Descent(31/49): loss=1.1884916851922966e+40\n",
      "Gradient Descent(32/49): loss=3.1127652822188444e+41\n",
      "Gradient Descent(33/49): loss=8.152608741742519e+42\n",
      "Gradient Descent(34/49): loss=2.1352406387852903e+44\n",
      "Gradient Descent(35/49): loss=5.5923848794265935e+45\n",
      "Gradient Descent(36/49): loss=1.4646952700109699e+47\n",
      "Gradient Descent(37/49): loss=3.836167002533628e+48\n",
      "Gradient Descent(38/49): loss=1.004726209788194e+50\n",
      "Gradient Descent(39/49): loss=2.631467180570168e+51\n",
      "Gradient Descent(40/49): loss=6.892046265895491e+52\n",
      "Gradient Descent(41/49): loss=1.8050881303771488e+54\n",
      "Gradient Descent(42/49): loss=4.727686136629726e+55\n",
      "Gradient Descent(43/49): loss=1.238222989246058e+57\n",
      "Gradient Descent(44/49): loss=3.243015984539017e+58\n",
      "Gradient Descent(45/49): loss=8.493746899643182e+59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=2.2245877522387304e+61\n",
      "Gradient Descent(47/49): loss=5.826392905136547e+62\n",
      "Gradient Descent(48/49): loss=1.5259840503420073e+64\n",
      "Gradient Descent(49/49): loss=3.9966877617285905e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7420078852365823\n",
      "Gradient Descent(2/49): loss=2.1046993030868917\n",
      "Gradient Descent(3/49): loss=10.22055503185478\n",
      "Gradient Descent(4/49): loss=89.43385837867258\n",
      "Gradient Descent(5/49): loss=1499.6956172096066\n",
      "Gradient Descent(6/49): loss=34483.33600719747\n",
      "Gradient Descent(7/49): loss=859219.2305045497\n",
      "Gradient Descent(8/49): loss=21755799.45581468\n",
      "Gradient Descent(9/49): loss=552541488.6321485\n",
      "Gradient Descent(10/49): loss=14041105987.979559\n",
      "Gradient Descent(11/49): loss=356848397067.3679\n",
      "Gradient Descent(12/49): loss=9069320806513.932\n",
      "Gradient Descent(13/49): loss=230498115402459.97\n",
      "Gradient Descent(14/49): loss=5858147357780105.0\n",
      "Gradient Descent(15/49): loss=1.4888579372737126e+17\n",
      "Gradient Descent(16/49): loss=3.7839574100690427e+18\n",
      "Gradient Descent(17/49): loss=9.616991243149147e+19\n",
      "Gradient Descent(18/49): loss=2.444174461414534e+21\n",
      "Gradient Descent(19/49): loss=6.211910407023831e+22\n",
      "Gradient Descent(20/49): loss=1.578767453599585e+24\n",
      "Gradient Descent(21/49): loss=4.012463975237919e+25\n",
      "Gradient Descent(22/49): loss=1.0197744522722455e+27\n",
      "Gradient Descent(23/49): loss=2.591773882395269e+28\n",
      "Gradient Descent(24/49): loss=6.587036812404076e+29\n",
      "Gradient Descent(25/49): loss=1.6741064590043438e+31\n",
      "Gradient Descent(26/49): loss=4.254769657279723e+32\n",
      "Gradient Descent(27/49): loss=1.0813568479554466e+34\n",
      "Gradient Descent(28/49): loss=2.7482865743847688e+35\n",
      "Gradient Descent(29/49): loss=6.984816445398418e+36\n",
      "Gradient Descent(30/49): loss=1.775202820209203e+38\n",
      "Gradient Descent(31/49): loss=4.511707755691622e+39\n",
      "Gradient Descent(32/49): loss=1.1466580968122315e+41\n",
      "Gradient Descent(33/49): loss=2.9142507941175624e+42\n",
      "Gradient Descent(34/49): loss=7.406617294750221e+43\n",
      "Gradient Descent(35/49): loss=1.8824042138591263e+45\n",
      "Gradient Descent(36/49): loss=4.784161896505904e+46\n",
      "Gradient Descent(37/49): loss=1.2159027738817045e+48\n",
      "Gradient Descent(38/49): loss=3.090237302824106e+49\n",
      "Gradient Descent(39/49): loss=7.853889959704004e+50\n",
      "Gradient Descent(40/49): loss=1.99607931218643e+52\n",
      "Gradient Descent(41/49): loss=5.073069066387573e+53\n",
      "Gradient Descent(42/49): loss=1.2893290158970805e+55\n",
      "Gradient Descent(43/49): loss=3.276851329007906e+56\n",
      "Gradient Descent(44/49): loss=8.328172638657155e+57\n",
      "Gradient Descent(45/49): loss=2.116619050894754e+59\n",
      "Gradient Descent(46/49): loss=5.379422834986972e+60\n",
      "Gradient Descent(47/49): loss=1.3671893402521494e+62\n",
      "Gradient Descent(48/49): loss=3.47473464986261e+63\n",
      "Gradient Descent(49/49): loss=8.831096419116768e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.748192122127561\n",
      "Gradient Descent(2/49): loss=2.125370480364025\n",
      "Gradient Descent(3/49): loss=10.137634233867\n",
      "Gradient Descent(4/49): loss=85.04239722855391\n",
      "Gradient Descent(5/49): loss=1380.664438036478\n",
      "Gradient Descent(6/49): loss=31445.235509058097\n",
      "Gradient Descent(7/49): loss=782197.7273971582\n",
      "Gradient Descent(8/49): loss=19804153.099642586\n",
      "Gradient Descent(9/49): loss=503086849.75198835\n",
      "Gradient Descent(10/49): loss=12787894952.202858\n",
      "Gradient Descent(11/49): loss=325091190198.53143\n",
      "Gradient Descent(12/49): loss=8264577066909.5205\n",
      "Gradient Descent(13/49): loss=210105679635282.2\n",
      "Gradient Descent(14/49): loss=5341402085184456.0\n",
      "Gradient Descent(15/49): loss=1.3579157018832493e+17\n",
      "Gradient Descent(16/49): loss=3.4521555780747264e+18\n",
      "Gradient Descent(17/49): loss=8.7762282849898e+19\n",
      "Gradient Descent(18/49): loss=2.2311330192346097e+21\n",
      "Gradient Descent(19/49): loss=5.67208872499707e+22\n",
      "Gradient Descent(20/49): loss=1.4419844190306702e+24\n",
      "Gradient Descent(21/49): loss=3.6658789478650054e+25\n",
      "Gradient Descent(22/49): loss=9.319565650679623e+26\n",
      "Gradient Descent(23/49): loss=2.369262737601528e+28\n",
      "Gradient Descent(24/49): loss=6.023248432589717e+29\n",
      "Gradient Descent(25/49): loss=1.5312578510149296e+31\n",
      "Gradient Descent(26/49): loss=3.8928339624980746e+32\n",
      "Gradient Descent(27/49): loss=9.896541101509578e+33\n",
      "Gradient Descent(28/49): loss=2.5159440838575616e+35\n",
      "Gradient Descent(29/49): loss=6.39614848073781e+36\n",
      "Gradient Descent(30/49): loss=1.6260582121093162e+38\n",
      "Gradient Descent(31/49): loss=4.1338397898841694e+39\n",
      "Gradient Descent(32/49): loss=1.0509237173165193e+41\n",
      "Gradient Descent(33/49): loss=2.6717064902249774e+42\n",
      "Gradient Descent(34/49): loss=6.792134816537143e+43\n",
      "Gradient Descent(35/49): loss=1.7267276751696173e+45\n",
      "Gradient Descent(36/49): loss=4.389766317560734e+46\n",
      "Gradient Descent(37/49): loss=1.1159865333656535e+48\n",
      "Gradient Descent(38/49): loss=2.8371121662473557e+49\n",
      "Gradient Descent(39/49): loss=7.2126367148837985e+50\n",
      "Gradient Descent(40/49): loss=1.833629597017243e+52\n",
      "Gradient Descent(41/49): loss=4.661537287909663e+53\n",
      "Gradient Descent(42/49): loss=1.1850773963247511e+55\n",
      "Gradient Descent(43/49): loss=3.0127581279300445e+56\n",
      "Gradient Descent(44/49): loss=7.659171937257189e+57\n",
      "Gradient Descent(45/49): loss=1.9471498299392013e+59\n",
      "Gradient Descent(46/49): loss=4.95013363231787e+60\n",
      "Gradient Descent(47/49): loss=1.258445683071542e+62\n",
      "Gradient Descent(48/49): loss=3.1992783526125373e+63\n",
      "Gradient Descent(49/49): loss=8.133352209936701e+64\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7349792383839133\n",
      "Gradient Descent(2/49): loss=2.025707188577814\n",
      "Gradient Descent(3/49): loss=9.106969914013025\n",
      "Gradient Descent(4/49): loss=67.08485202069927\n",
      "Gradient Descent(5/49): loss=975.7970632875744\n",
      "Gradient Descent(6/49): loss=21674.02079515221\n",
      "Gradient Descent(7/49): loss=543212.3167769025\n",
      "Gradient Descent(8/49): loss=13951870.130433802\n",
      "Gradient Descent(9/49): loss=359975940.5213492\n",
      "Gradient Descent(10/49): loss=9295578763.361465\n",
      "Gradient Descent(11/49): loss=240074150053.16077\n",
      "Gradient Descent(12/49): loss=6200495490951.218\n",
      "Gradient Descent(13/49): loss=160143599138629.38\n",
      "Gradient Descent(14/49): loss=4136120408788466.5\n",
      "Gradient Descent(15/49): loss=1.0682596742633286e+17\n",
      "Gradient Descent(16/49): loss=2.7590559600578253e+18\n",
      "Gradient Descent(17/49): loss=7.125973222182367e+19\n",
      "Gradient Descent(18/49): loss=1.840466273607266e+21\n",
      "Gradient Descent(19/49): loss=4.753478576608032e+22\n",
      "Gradient Descent(20/49): loss=1.2277083749406158e+24\n",
      "Gradient Descent(21/49): loss=3.170873350152206e+25\n",
      "Gradient Descent(22/49): loss=8.189597796954601e+26\n",
      "Gradient Descent(23/49): loss=2.1151747379843324e+28\n",
      "Gradient Descent(24/49): loss=5.462983998885896e+29\n",
      "Gradient Descent(25/49): loss=1.410956439491296e+31\n",
      "Gradient Descent(26/49): loss=3.6441587135308035e+32\n",
      "Gradient Descent(27/49): loss=9.411979248764366e+33\n",
      "Gradient Descent(28/49): loss=2.430886257787067e+35\n",
      "Gradient Descent(29/49): loss=6.278390381145336e+36\n",
      "Gradient Descent(30/49): loss=1.6215561568043594e+38\n",
      "Gradient Descent(31/49): loss=4.188086770721807e+39\n",
      "Gradient Descent(32/49): loss=1.0816813667225213e+41\n",
      "Gradient Descent(33/49): loss=2.793720959399025e+42\n",
      "Gradient Descent(34/49): loss=7.215504527580156e+43\n",
      "Gradient Descent(35/49): loss=1.8635900415312096e+45\n",
      "Gradient Descent(36/49): loss=4.813201668184527e+46\n",
      "Gradient Descent(37/49): loss=1.2431334028582446e+48\n",
      "Gradient Descent(38/49): loss=3.2107124609320665e+49\n",
      "Gradient Descent(39/49): loss=8.292492570051319e+50\n",
      "Gradient Descent(40/49): loss=2.141749965501275e+52\n",
      "Gradient Descent(41/49): loss=5.531621374363575e+53\n",
      "Gradient Descent(42/49): loss=1.428683811004724e+55\n",
      "Gradient Descent(43/49): loss=3.689944220128011e+56\n",
      "Gradient Descent(44/49): loss=9.530232121886357e+57\n",
      "Gradient Descent(45/49): loss=2.461428110528011e+59\n",
      "Gradient Descent(46/49): loss=6.357272588758559e+60\n",
      "Gradient Descent(47/49): loss=1.6419295203023915e+62\n",
      "Gradient Descent(48/49): loss=4.240706233688328e+63\n",
      "Gradient Descent(49/49): loss=1.095271699429051e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7490534747244966\n",
      "Gradient Descent(2/49): loss=2.1668244603604183\n",
      "Gradient Descent(3/49): loss=11.025233704469416\n",
      "Gradient Descent(4/49): loss=106.77366192349214\n",
      "Gradient Descent(5/49): loss=1991.4244381888861\n",
      "Gradient Descent(6/49): loss=49264.70923545805\n",
      "Gradient Descent(7/49): loss=1301889.1180221795\n",
      "Gradient Descent(8/49): loss=34837902.20604963\n",
      "Gradient Descent(9/49): loss=934365461.3979759\n",
      "Gradient Descent(10/49): loss=25070272819.813904\n",
      "Gradient Descent(11/49): loss=672718273475.9894\n",
      "Gradient Descent(12/49): loss=18051492459712.953\n",
      "Gradient Descent(13/49): loss=484388731653684.8\n",
      "Gradient Descent(14/49): loss=1.2997958123687402e+16\n",
      "Gradient Descent(15/49): loss=3.487837706281777e+17\n",
      "Gradient Descent(16/49): loss=9.359171583921019e+18\n",
      "Gradient Descent(17/49): loss=2.5114153905435656e+20\n",
      "Gradient Descent(18/49): loss=6.739065749665666e+21\n",
      "Gradient Descent(19/49): loss=1.808343110141162e+23\n",
      "Gradient Descent(20/49): loss=4.852460156222317e+24\n",
      "Gradient Descent(21/49): loss=1.3020963464144705e+26\n",
      "Gradient Descent(22/49): loss=3.4940109568392414e+27\n",
      "Gradient Descent(23/49): loss=9.375736749535066e+28\n",
      "Gradient Descent(24/49): loss=2.5158604446994764e+30\n",
      "Gradient Descent(25/49): loss=6.750993491276812e+31\n",
      "Gradient Descent(26/49): loss=1.811543768863791e+33\n",
      "Gradient Descent(27/49): loss=4.861048719347126e+34\n",
      "Gradient Descent(28/49): loss=1.3044009787677824e+36\n",
      "Gradient Descent(29/49): loss=3.500195146447421e+37\n",
      "Gradient Descent(30/49): loss=9.392331240649465e+38\n",
      "Gradient Descent(31/49): loss=2.520313366630857e+40\n",
      "Gradient Descent(32/49): loss=6.762942344417267e+41\n",
      "Gradient Descent(33/49): loss=1.8147500925670396e+43\n",
      "Gradient Descent(34/49): loss=4.869652483716026e+44\n",
      "Gradient Descent(35/49): loss=1.3067096901820918e+46\n",
      "Gradient Descent(36/49): loss=3.506390281699972e+47\n",
      "Gradient Descent(37/49): loss=9.408955103016546e+48\n",
      "Gradient Descent(38/49): loss=2.5247741699666244e+50\n",
      "Gradient Descent(39/49): loss=6.774912346310403e+51\n",
      "Gradient Descent(40/49): loss=1.8179620912707105e+53\n",
      "Gradient Descent(41/49): loss=4.8782714762313206e+54\n",
      "Gradient Descent(42/49): loss=1.3090224878770162e+56\n",
      "Gradient Descent(43/49): loss=3.512596381969944e+57\n",
      "Gradient Descent(44/49): loss=9.425608388621714e+58\n",
      "Gradient Descent(45/49): loss=2.5292428686563962e+60\n",
      "Gradient Descent(46/49): loss=6.786903534388002e+61\n",
      "Gradient Descent(47/49): loss=1.8211797750192233e+63\n",
      "Gradient Descent(48/49): loss=4.8869057238459794e+64\n",
      "Gradient Descent(49/49): loss=1.3113393790849903e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7529822106506042\n",
      "Gradient Descent(2/49): loss=2.2096473213263184\n",
      "Gradient Descent(3/49): loss=11.099858038434098\n",
      "Gradient Descent(4/49): loss=99.39559438036572\n",
      "Gradient Descent(5/49): loss=1698.7298545457686\n",
      "Gradient Descent(6/49): loss=39928.64358334068\n",
      "Gradient Descent(7/49): loss=1018770.4043294721\n",
      "Gradient Descent(8/49): loss=26426867.219382994\n",
      "Gradient Descent(9/49): loss=687669458.1469821\n",
      "Gradient Descent(10/49): loss=17904835042.88117\n",
      "Gradient Descent(11/49): loss=466239465227.21027\n",
      "Gradient Descent(12/49): loss=12141064384094.805\n",
      "Gradient Descent(13/49): loss=316159462086400.44\n",
      "Gradient Descent(14/49): loss=8232958437473492.0\n",
      "Gradient Descent(15/49): loss=2.1439059095657856e+17\n",
      "Gradient Descent(16/49): loss=5.58284449260857e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=1.4538022699399335e+20\n",
      "Gradient Descent(18/49): loss=3.785778100729164e+21\n",
      "Gradient Descent(19/49): loss=9.858366661497614e+22\n",
      "Gradient Descent(20/49): loss=2.5671708866636846e+24\n",
      "Gradient Descent(21/49): loss=6.685048941333982e+25\n",
      "Gradient Descent(22/49): loss=1.740822147066299e+27\n",
      "Gradient Descent(23/49): loss=4.533193061579227e+28\n",
      "Gradient Descent(24/49): loss=1.18046748016057e+30\n",
      "Gradient Descent(25/49): loss=3.0739998336431e+31\n",
      "Gradient Descent(26/49): loss=8.004858359971368e+32\n",
      "Gradient Descent(27/49): loss=2.084507509138802e+34\n",
      "Gradient Descent(28/49): loss=5.428167945337156e+35\n",
      "Gradient Descent(29/49): loss=1.4135236795072918e+37\n",
      "Gradient Descent(30/49): loss=3.680890518953482e+38\n",
      "Gradient Descent(31/49): loss=9.58523384429228e+39\n",
      "Gradient Descent(32/49): loss=2.496045654622932e+41\n",
      "Gradient Descent(33/49): loss=6.49983507045258e+42\n",
      "Gradient Descent(34/49): loss=1.6925914742319637e+44\n",
      "Gradient Descent(35/49): loss=4.407597835314411e+45\n",
      "Gradient Descent(36/49): loss=1.1477618181128763e+47\n",
      "Gradient Descent(37/49): loss=2.9888325576414415e+48\n",
      "Gradient Descent(38/49): loss=7.783078262966714e+49\n",
      "Gradient Descent(39/49): loss=2.0267547973736694e+51\n",
      "Gradient Descent(40/49): loss=5.277776825427286e+52\n",
      "Gradient Descent(41/49): loss=1.3743610354401429e+54\n",
      "Gradient Descent(42/49): loss=3.578908919823692e+55\n",
      "Gradient Descent(43/49): loss=9.319668359407132e+56\n",
      "Gradient Descent(44/49): loss=2.4268909959747157e+58\n",
      "Gradient Descent(45/49): loss=6.319752677033777e+59\n",
      "Gradient Descent(46/49): loss=1.645697065303737e+61\n",
      "Gradient Descent(47/49): loss=4.2854823110269095e+62\n",
      "Gradient Descent(48/49): loss=1.1159622888878864e+64\n",
      "Gradient Descent(49/49): loss=2.9060248994971268e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7594106321493161\n",
      "Gradient Descent(2/49): loss=2.2318041451016932\n",
      "Gradient Descent(3/49): loss=11.013032695357843\n",
      "Gradient Descent(4/49): loss=94.56690046815466\n",
      "Gradient Descent(5/49): loss=1564.3581632381959\n",
      "Gradient Descent(6/49): loss=36413.51882935117\n",
      "Gradient Descent(7/49): loss=927458.5657358299\n",
      "Gradient Descent(8/49): loss=24056166.989451174\n",
      "Gradient Descent(9/49): loss=626117537.6143463\n",
      "Gradient Descent(10/49): loss=16306683249.41034\n",
      "Gradient Descent(11/49): loss=424744490157.4368\n",
      "Gradient Descent(12/49): loss=11063680992161.082\n",
      "Gradient Descent(13/49): loss=288186319183868.94\n",
      "Gradient Descent(14/49): loss=7506671579955481.0\n",
      "Gradient Descent(15/49): loss=1.955336624390098e+17\n",
      "Gradient Descent(16/49): loss=5.093257881846966e+18\n",
      "Gradient Descent(17/49): loss=1.3266910494929712e+20\n",
      "Gradient Descent(18/49): loss=3.4557628600599975e+21\n",
      "Gradient Descent(19/49): loss=9.001565927240676e+22\n",
      "Gradient Descent(20/49): loss=2.3447265459569253e+24\n",
      "Gradient Descent(21/49): loss=6.107540198879966e+25\n",
      "Gradient Descent(22/49): loss=1.5908911572353716e+27\n",
      "Gradient Descent(23/49): loss=4.143950906183666e+28\n",
      "Gradient Descent(24/49): loss=1.0794157120530444e+30\n",
      "Gradient Descent(25/49): loss=2.8116604318075587e+31\n",
      "Gradient Descent(26/49): loss=7.323808886157715e+32\n",
      "Gradient Descent(27/49): loss=1.907704642928071e+34\n",
      "Gradient Descent(28/49): loss=4.9691861997215955e+35\n",
      "Gradient Descent(29/49): loss=1.2943728778477288e+37\n",
      "Gradient Descent(30/49): loss=3.3715805356654327e+38\n",
      "Gradient Descent(31/49): loss=8.782287934972657e+39\n",
      "Gradient Descent(32/49): loss=2.287609047355644e+41\n",
      "Gradient Descent(33/49): loss=5.958760623987301e+42\n",
      "Gradient Descent(34/49): loss=1.5521370758271261e+44\n",
      "Gradient Descent(35/49): loss=4.043004332912919e+45\n",
      "Gradient Descent(36/49): loss=1.053121163750448e+47\n",
      "Gradient Descent(37/49): loss=2.743168431729167e+48\n",
      "Gradient Descent(38/49): loss=7.145401026826639e+49\n",
      "Gradient Descent(39/49): loss=1.8612329904216247e+51\n",
      "Gradient Descent(40/49): loss=4.848136908800515e+52\n",
      "Gradient Descent(41/49): loss=1.2628419766591877e+54\n",
      "Gradient Descent(42/49): loss=3.289448891423823e+55\n",
      "Gradient Descent(43/49): loss=8.568351550931844e+56\n",
      "Gradient Descent(44/49): loss=2.23188293004841e+58\n",
      "Gradient Descent(45/49): loss=5.813605316999047e+59\n",
      "Gradient Descent(46/49): loss=1.5143270431799676e+61\n",
      "Gradient Descent(47/49): loss=3.9445168164424646e+62\n",
      "Gradient Descent(48/49): loss=1.0274671501952424e+64\n",
      "Gradient Descent(49/49): loss=2.676344895602315e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7457322455673785\n",
      "Gradient Descent(2/49): loss=2.12624026076658\n",
      "Gradient Descent(3/49): loss=9.897837342904811\n",
      "Gradient Descent(4/49): loss=74.72824514539907\n",
      "Gradient Descent(5/49): loss=1106.751683921657\n",
      "Gradient Descent(6/49): loss=25101.825894732665\n",
      "Gradient Descent(7/49): loss=643994.0785272041\n",
      "Gradient Descent(8/49): loss=16943346.82642719\n",
      "Gradient Descent(9/49): loss=447882904.02364635\n",
      "Gradient Descent(10/49): loss=11849676578.190643\n",
      "Gradient Descent(11/49): loss=313557782610.975\n",
      "Gradient Descent(12/49): loss=8297386689915.552\n",
      "Gradient Descent(13/49): loss=219567165535626.5\n",
      "Gradient Descent(14/49): loss=5810237506988238.0\n",
      "Gradient Descent(15/49): loss=1.5375188661704653e+17\n",
      "Gradient Descent(16/49): loss=4.0686191203795036e+18\n",
      "Gradient Descent(17/49): loss=1.0766477088886519e+20\n",
      "Gradient Descent(18/49): loss=2.849050884432421e+21\n",
      "Gradient Descent(19/49): loss=7.539226505301533e+22\n",
      "Gradient Descent(20/49): loss=1.995048126761281e+24\n",
      "Gradient Descent(21/49): loss=5.279344008728062e+25\n",
      "Gradient Descent(22/49): loss=1.3970326223544565e+27\n",
      "Gradient Descent(23/49): loss=3.696861096182494e+28\n",
      "Gradient Descent(24/49): loss=9.78272213961259e+29\n",
      "Gradient Descent(25/49): loss=2.588727300565595e+31\n",
      "Gradient Descent(26/49): loss=6.850352019667221e+32\n",
      "Gradient Descent(27/49): loss=1.8127565148753298e+34\n",
      "Gradient Descent(28/49): loss=4.796959590964996e+35\n",
      "Gradient Descent(29/49): loss=1.2693829054550766e+37\n",
      "Gradient Descent(30/49): loss=3.359071366155629e+38\n",
      "Gradient Descent(31/49): loss=8.888854887234654e+39\n",
      "Gradient Descent(32/49): loss=2.352190013061366e+41\n",
      "Gradient Descent(33/49): loss=6.224421399309075e+42\n",
      "Gradient Descent(34/49): loss=1.6471212589561082e+44\n",
      "Gradient Descent(35/49): loss=4.3586516202234664e+45\n",
      "Gradient Descent(36/49): loss=1.1533968032515478e+47\n",
      "Gradient Descent(37/49): loss=3.052146171945403e+48\n",
      "Gradient Descent(38/49): loss=8.076662106795577e+49\n",
      "Gradient Descent(39/49): loss=2.137265619417264e+51\n",
      "Gradient Descent(40/49): loss=5.655683334950488e+52\n",
      "Gradient Descent(41/49): loss=1.4966204338213283e+54\n",
      "Gradient Descent(42/49): loss=3.9603927417395154e+55\n",
      "Gradient Descent(43/49): loss=1.0480085875064118e+57\n",
      "Gradient Descent(44/49): loss=2.7732653580330486e+58\n",
      "Gradient Descent(45/49): loss=7.338681035396777e+59\n",
      "Gradient Descent(46/49): loss=1.9419793054888227e+61\n",
      "Gradient Descent(47/49): loss=5.13891202623019e+62\n",
      "Gradient Descent(48/49): loss=1.3598711756964947e+64\n",
      "Gradient Descent(49/49): loss=3.598523588361031e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7600897697207997\n",
      "Gradient Descent(2/49): loss=2.274021719624797\n",
      "Gradient Descent(3/49): loss=11.958738300980425\n",
      "Gradient Descent(4/49): loss=118.39061491980283\n",
      "Gradient Descent(5/49): loss=2250.4589962249247\n",
      "Gradient Descent(6/49): loss=56914.14821896674\n",
      "Gradient Descent(7/49): loss=1539791.6967769854\n",
      "Gradient Descent(8/49): loss=42198925.285177074\n",
      "Gradient Descent(9/49): loss=1159209545.8478808\n",
      "Gradient Descent(10/49): loss=31857168345.69026\n",
      "Gradient Descent(11/49): loss=875559640359.1995\n",
      "Gradient Descent(12/49): loss=24064138376621.773\n",
      "Gradient Descent(13/49): loss=661387502983859.5\n",
      "Gradient Descent(14/49): loss=1.8177822107803332e+16\n",
      "Gradient Descent(15/49): loss=4.996061189813901e+17\n",
      "Gradient Descent(16/49): loss=1.3731363213616816e+19\n",
      "Gradient Descent(17/49): loss=3.7739797220501186e+20\n",
      "Gradient Descent(18/49): loss=1.0372548397194847e+22\n",
      "Gradient Descent(19/49): loss=2.8508303749711384e+23\n",
      "Gradient Descent(20/49): loss=7.835329868581551e+24\n",
      "Gradient Descent(21/49): loss=2.1534916524199967e+26\n",
      "Gradient Descent(22/49): loss=5.918737787468199e+27\n",
      "Gradient Descent(23/49): loss=1.6267282465404517e+29\n",
      "Gradient Descent(24/49): loss=4.470961348711054e+30\n",
      "Gradient Descent(25/49): loss=1.2288159023598434e+32\n",
      "Gradient Descent(26/49): loss=3.3773240341873087e+33\n",
      "Gradient Descent(27/49): loss=9.282364925449239e+34\n",
      "Gradient Descent(28/49): loss=2.5512002324036005e+36\n",
      "Gradient Descent(29/49): loss=7.011815068777897e+37\n",
      "Gradient Descent(30/49): loss=1.9271537347117308e+39\n",
      "Gradient Descent(31/49): loss=5.29666210643637e+40\n",
      "Gradient Descent(32/49): loss=1.4557546170002012e+42\n",
      "Gradient Descent(33/49): loss=4.0010509681978946e+43\n",
      "Gradient Descent(34/49): loss=1.0996639586900265e+45\n",
      "Gradient Descent(35/49): loss=3.0223579545813056e+46\n",
      "Gradient Descent(36/49): loss=8.306762746414583e+47\n",
      "Gradient Descent(37/49): loss=2.283062045004475e+49\n",
      "Gradient Descent(38/49): loss=6.274853947874949e+50\n",
      "Gradient Descent(39/49): loss=1.7246045569946329e+52\n",
      "Gradient Descent(40/49): loss=4.739968296814291e+53\n",
      "Gradient Descent(41/49): loss=1.302750787923029e+55\n",
      "Gradient Descent(42/49): loss=3.5805294659350033e+56\n",
      "Gradient Descent(43/49): loss=9.840862408433112e+57\n",
      "Gradient Descent(44/49): loss=2.70469978987939e+59\n",
      "Gradient Descent(45/49): loss=7.433699049693566e+60\n",
      "Gradient Descent(46/49): loss=2.0431059213370713e+62\n",
      "Gradient Descent(47/49): loss=5.615349475271714e+63\n",
      "Gradient Descent(48/49): loss=1.5433438570232838e+65\n",
      "Gradient Descent(49/49): loss=4.2417845434211506e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7641283513615416\n",
      "Gradient Descent(2/49): loss=2.31931410003093\n",
      "Gradient Descent(3/49): loss=12.043992598373752\n",
      "Gradient Descent(4/49): loss=110.32885696764542\n",
      "Gradient Descent(5/49): loss=1921.356420194011\n",
      "Gradient Descent(6/49): loss=46153.0621232942\n",
      "Gradient Descent(7/49): loss=1205485.4642303488\n",
      "Gradient Descent(8/49): loss=32025978.44443224\n",
      "Gradient Descent(9/49): loss=853598670.4350647\n",
      "Gradient Descent(10/49): loss=22765210132.399456\n",
      "Gradient Descent(11/49): loss=607211326936.8652\n",
      "Gradient Descent(12/49): loss=16196364379882.213\n",
      "Gradient Descent(13/49): loss=432013181959115.94\n",
      "Gradient Descent(14/49): loss=1.1523298043121106e+16\n",
      "Gradient Descent(15/49): loss=3.0736658676142675e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=8.198540114361953e+18\n",
      "Gradient Descent(17/49): loss=2.1868369318692196e+20\n",
      "Gradient Descent(18/49): loss=5.833057720705047e+21\n",
      "Gradient Descent(19/49): loss=1.5558801794268758e+23\n",
      "Gradient Descent(20/49): loss=4.150075738535231e+24\n",
      "Gradient Descent(21/49): loss=1.1069701165571469e+26\n",
      "Gradient Descent(22/49): loss=2.9526758453439026e+27\n",
      "Gradient Descent(23/49): loss=7.875817528656e+28\n",
      "Gradient Descent(24/49): loss=2.100755551697307e+30\n",
      "Gradient Descent(25/49): loss=5.603448622228716e+31\n",
      "Gradient Descent(26/49): loss=1.4946354151765895e+33\n",
      "Gradient Descent(27/49): loss=3.9867145661659078e+34\n",
      "Gradient Descent(28/49): loss=1.0633959874556807e+36\n",
      "Gradient Descent(29/49): loss=2.836448427318262e+37\n",
      "Gradient Descent(30/49): loss=7.565798419162949e+38\n",
      "Gradient Descent(31/49): loss=2.0180626295937598e+40\n",
      "Gradient Descent(32/49): loss=5.382877723318216e+41\n",
      "Gradient Descent(33/49): loss=1.4358014542902296e+43\n",
      "Gradient Descent(34/49): loss=3.829783848166583e+44\n",
      "Gradient Descent(35/49): loss=1.0215370850789541e+46\n",
      "Gradient Descent(36/49): loss=2.724796117909301e+47\n",
      "Gradient Descent(37/49): loss=7.267982721938795e+48\n",
      "Gradient Descent(38/49): loss=1.9386247836748786e+50\n",
      "Gradient Descent(39/49): loss=5.170989249236826e+51\n",
      "Gradient Descent(40/49): loss=1.37928339928861e+53\n",
      "Gradient Descent(41/49): loss=3.6790304598562966e+54\n",
      "Gradient Descent(42/49): loss=9.81325892237347e+55\n",
      "Gradient Descent(43/49): loss=2.6175388251964904e+57\n",
      "Gradient Descent(44/49): loss=6.981890068945294e+58\n",
      "Gradient Descent(45/49): loss=1.8623138830109876e+60\n",
      "Gradient Descent(46/49): loss=4.9674414300529905e+61\n",
      "Gradient Descent(47/49): loss=1.3249900881967743e+63\n",
      "Gradient Descent(48/49): loss=3.534211240415105e+64\n",
      "Gradient Descent(49/49): loss=9.426975494492366e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7708046156528288\n",
      "Gradient Descent(2/49): loss=2.3430295431015535\n",
      "Gradient Descent(3/49): loss=11.953198595079767\n",
      "Gradient Descent(4/49): loss=105.02560247935895\n",
      "Gradient Descent(5/49): loss=1769.8889554593336\n",
      "Gradient Descent(6/49): loss=42093.13811179125\n",
      "Gradient Descent(7/49): loss=1097453.0012898098\n",
      "Gradient Descent(8/49): loss=29152965.972364623\n",
      "Gradient Descent(9/49): loss=777191394.0500368\n",
      "Gradient Descent(10/49): loss=20733117272.751595\n",
      "Gradient Descent(11/49): loss=553166566859.603\n",
      "Gradient Descent(12/49): loss=14759019490703.89\n",
      "Gradient Descent(13/49): loss=393786673244027.7\n",
      "Gradient Descent(14/49): loss=1.0506664965811322e+16\n",
      "Gradient Descent(15/49): loss=2.8032951197090736e+17\n",
      "Gradient Descent(16/49): loss=7.479503518804803e+18\n",
      "Gradient Descent(17/49): loss=1.995614832052218e+20\n",
      "Gradient Descent(18/49): loss=5.324522613029315e+21\n",
      "Gradient Descent(19/49): loss=1.420641929835114e+23\n",
      "Gradient Descent(20/49): loss=3.7904308790804696e+24\n",
      "Gradient Descent(21/49): loss=1.0113291708102501e+26\n",
      "Gradient Descent(22/49): loss=2.6983388547667934e+27\n",
      "Gradient Descent(23/49): loss=7.199468566018396e+28\n",
      "Gradient Descent(24/49): loss=1.9208983905607603e+30\n",
      "Gradient Descent(25/49): loss=5.12517082757332e+31\n",
      "Gradient Descent(26/49): loss=1.3674526534503985e+33\n",
      "Gradient Descent(27/49): loss=3.648515966274417e+34\n",
      "Gradient Descent(28/49): loss=9.734646916345407e+35\n",
      "Gradient Descent(29/49): loss=2.597312207535088e+37\n",
      "Gradient Descent(30/49): loss=6.929918220334905e+38\n",
      "Gradient Descent(31/49): loss=1.8489793564750566e+40\n",
      "Gradient Descent(32/49): loss=4.93328283534292e+41\n",
      "Gradient Descent(33/49): loss=1.3162548001555827e+43\n",
      "Gradient Descent(34/49): loss=3.5119143920160474e+44\n",
      "Gradient Descent(35/49): loss=9.370178703539642e+45\n",
      "Gradient Descent(36/49): loss=2.500068029444867e+47\n",
      "Gradient Descent(37/49): loss=6.67045992355656e+48\n",
      "Gradient Descent(38/49): loss=1.7797529934276803e+50\n",
      "Gradient Descent(39/49): loss=4.748579189313197e+51\n",
      "Gradient Descent(40/49): loss=1.2669738104359136e+53\n",
      "Gradient Descent(41/49): loss=3.3804272232483834e+54\n",
      "Gradient Descent(42/49): loss=9.019356294150418e+55\n",
      "Gradient Descent(43/49): loss=2.406464703673121e+57\n",
      "Gradient Descent(44/49): loss=6.420715826228659e+58\n",
      "Gradient Descent(45/49): loss=1.7131184869762798e+60\n",
      "Gradient Descent(46/49): loss=4.5707909053243454e+61\n",
      "Gradient Descent(47/49): loss=1.2195379163219298e+63\n",
      "Gradient Descent(48/49): loss=3.253862975036389e+64\n",
      "Gradient Descent(49/49): loss=8.681668785046488e+65\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7566540458791874\n",
      "Gradient Descent(2/49): loss=2.231312361543913\n",
      "Gradient Descent(3/49): loss=10.7477386373317\n",
      "Gradient Descent(4/49): loss=83.13495370598017\n",
      "Gradient Descent(5/49): loss=1253.4326445506683\n",
      "Gradient Descent(6/49): loss=29021.140939417517\n",
      "Gradient Descent(7/49): loss=761921.4254605013\n",
      "Gradient Descent(8/49): loss=20528387.908607546\n",
      "Gradient Descent(9/49): loss=555798881.7086602\n",
      "Gradient Descent(10/49): loss=15061638538.723326\n",
      "Gradient Descent(11/49): loss=408224415014.4243\n",
      "Gradient Descent(12/49): loss=11064684368256.758\n",
      "Gradient Descent(13/49): loss=299903499654454.8\n",
      "Gradient Descent(14/49): loss=8128763509766376.0\n",
      "Gradient Descent(15/49): loss=2.2032690154080675e+17\n",
      "Gradient Descent(16/49): loss=5.97187323691429e+18\n",
      "Gradient Descent(17/49): loss=1.618652553992653e+20\n",
      "Gradient Descent(18/49): loss=4.3872935505116833e+21\n",
      "Gradient Descent(19/49): loss=1.1891585167625653e+23\n",
      "Gradient Descent(20/49): loss=3.2231669975248656e+24\n",
      "Gradient Descent(21/49): loss=8.73626631569204e+25\n",
      "Gradient Descent(22/49): loss=2.3679303367591795e+27\n",
      "Gradient Descent(23/49): loss=6.418181265461651e+28\n",
      "Gradient Descent(24/49): loss=1.739622577440491e+30\n",
      "Gradient Descent(25/49): loss=4.715178002569235e+31\n",
      "Gradient Descent(26/49): loss=1.2780302971593098e+33\n",
      "Gradient Descent(27/49): loss=3.4640504336571285e+34\n",
      "Gradient Descent(28/49): loss=9.389171315885267e+35\n",
      "Gradient Descent(29/49): loss=2.5448976476353363e+37\n",
      "Gradient Descent(30/49): loss=6.897844143052885e+38\n",
      "Gradient Descent(31/49): loss=1.8696332980644015e+40\n",
      "Gradient Descent(32/49): loss=5.0675669045837796e+41\n",
      "Gradient Descent(33/49): loss=1.3735439114728907e+43\n",
      "Gradient Descent(34/49): loss=3.7229362971759264e+44\n",
      "Gradient Descent(35/49): loss=1.0090871181516935e+46\n",
      "Gradient Descent(36/49): loss=2.735090613266994e+47\n",
      "Gradient Descent(37/49): loss=7.413354633327652e+48\n",
      "Gradient Descent(38/49): loss=2.0093603719342595e+50\n",
      "Gradient Descent(39/49): loss=5.4462915967202055e+51\n",
      "Gradient Descent(40/49): loss=1.4761957372510425e+53\n",
      "Gradient Descent(41/49): loss=4.0011699997671597e+54\n",
      "Gradient Descent(42/49): loss=1.0845012597617707e+56\n",
      "Gradient Descent(43/49): loss=2.939497653169741e+57\n",
      "Gradient Descent(44/49): loss=7.967391808183131e+58\n",
      "Gradient Descent(45/49): loss=2.1595299508625914e+60\n",
      "Gradient Descent(46/49): loss=5.853320284666808e+61\n",
      "Gradient Descent(47/49): loss=1.5865192488395983e+63\n",
      "Gradient Descent(48/49): loss=4.3001975024878797e+64\n",
      "Gradient Descent(49/49): loss=1.1655514784285206e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7712964767837512\n",
      "Gradient Descent(2/49): loss=2.3859755830667853\n",
      "Gradient Descent(3/49): loss=12.959870633184536\n",
      "Gradient Descent(4/49): loss=131.11380870000758\n",
      "Gradient Descent(5/49): loss=2539.570835216244\n",
      "Gradient Descent(6/49): loss=65639.71123480299\n",
      "Gradient Descent(7/49): loss=1817558.2301429373\n",
      "Gradient Descent(8/49): loss=50999467.618502356\n",
      "Gradient Descent(9/49): loss=1434493851.9055035\n",
      "Gradient Descent(10/49): loss=40366737674.83758\n",
      "Gradient Descent(11/49): loss=1136013368247.0762\n",
      "Gradient Descent(12/49): loss=31970508989797.57\n",
      "Gradient Descent(13/49): loss=899739537912130.2\n",
      "Gradient Descent(14/49): loss=2.5321199108606388e+16\n",
      "Gradient Descent(15/49): loss=7.126097642490936e+17\n",
      "Gradient Descent(16/49): loss=2.0054843450667323e+19\n",
      "Gradient Descent(17/49): loss=5.643997137823645e+20\n",
      "Gradient Descent(18/49): loss=1.5883795745722866e+22\n",
      "Gradient Descent(19/49): loss=4.470146974824034e+23\n",
      "Gradient Descent(20/49): loss=1.2580251154545814e+25\n",
      "Gradient Descent(21/49): loss=3.5404365897426e+26\n",
      "Gradient Descent(22/49): loss=9.963784579504042e+27\n",
      "Gradient Descent(23/49): loss=2.804089287586627e+29\n",
      "Gradient Descent(24/49): loss=7.89149611778312e+30\n",
      "Gradient Descent(25/49): loss=2.220889015648478e+32\n",
      "Gradient Descent(26/49): loss=6.250206483296754e+33\n",
      "Gradient Descent(27/49): loss=1.7589839388007032e+35\n",
      "Gradient Descent(28/49): loss=4.950275651256233e+36\n",
      "Gradient Descent(29/49): loss=1.3931468322631885e+38\n",
      "Gradient Descent(30/49): loss=3.9207071140623754e+39\n",
      "Gradient Descent(31/49): loss=1.1033972814830623e+41\n",
      "Gradient Descent(32/49): loss=3.105270364158199e+42\n",
      "Gradient Descent(33/49): loss=8.739104397247032e+43\n",
      "Gradient Descent(34/49): loss=2.4594298308928825e+45\n",
      "Gradient Descent(35/49): loss=6.921527445067813e+46\n",
      "Gradient Descent(36/49): loss=1.9479125434302075e+48\n",
      "Gradient Descent(37/49): loss=5.48197389516464e+49\n",
      "Gradient Descent(38/49): loss=1.5427816761396353e+51\n",
      "Gradient Descent(39/49): loss=4.3418216608649267e+52\n",
      "Gradient Descent(40/49): loss=1.2219107619897602e+54\n",
      "Gradient Descent(41/49): loss=3.438800639197477e+55\n",
      "Gradient Descent(42/49): loss=9.677752421861553e+56\n",
      "Gradient Descent(43/49): loss=2.7235917916050655e+58\n",
      "Gradient Descent(44/49): loss=7.664953518072752e+59\n",
      "Gradient Descent(45/49): loss=2.157133554863375e+61\n",
      "Gradient Descent(46/49): loss=6.070780680594928e+62\n",
      "Gradient Descent(47/49): loss=1.7084884702105856e+64\n",
      "Gradient Descent(48/49): loss=4.808167196968371e+65\n",
      "Gradient Descent(49/49): loss=1.3531535153499208e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7754463073693946\n",
      "Gradient Descent(2/49): loss=2.4338391763674103\n",
      "Gradient Descent(3/49): loss=13.056746770236737\n",
      "Gradient Descent(4/49): loss=122.31456642953775\n",
      "Gradient Descent(5/49): loss=2170.0433377705845\n",
      "Gradient Descent(6/49): loss=53256.7977768868\n",
      "Gradient Descent(7/49): loss=1423580.340918493\n",
      "Gradient Descent(8/49): loss=38723025.20638879\n",
      "Gradient Descent(9/49): loss=1056851453.1803697\n",
      "Gradient Descent(10/49): loss=28862597907.616238\n",
      "Gradient Descent(11/49): loss=788332334416.5476\n",
      "Gradient Descent(12/49): loss=21532437504979.367\n",
      "Gradient Descent(13/49): loss=588137582730998.1\n",
      "Gradient Descent(14/49): loss=1.6064419086240052e+16\n",
      "Gradient Descent(15/49): loss=4.387844073491089e+17\n",
      "Gradient Descent(16/49): loss=1.1984981266049794e+19\n",
      "Gradient Descent(17/49): loss=3.273584346716015e+20\n",
      "Gradient Descent(18/49): loss=8.941486222203434e+21\n",
      "Gradient Descent(19/49): loss=2.442282446702178e+23\n",
      "Gradient Descent(20/49): loss=6.670863658978699e+24\n",
      "Gradient Descent(21/49): loss=1.8220833555549906e+26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=4.976848462682083e+27\n",
      "Gradient Descent(23/49): loss=1.3593791164928814e+29\n",
      "Gradient Descent(24/49): loss=3.71301556841301e+30\n",
      "Gradient Descent(25/49): loss=1.0141751071508122e+32\n",
      "Gradient Descent(26/49): loss=2.770123445520567e+33\n",
      "Gradient Descent(27/49): loss=7.566330359833452e+34\n",
      "Gradient Descent(28/49): loss=2.066671620960202e+36\n",
      "Gradient Descent(29/49): loss=5.644918191195982e+37\n",
      "Gradient Descent(30/49): loss=1.5418560482526233e+39\n",
      "Gradient Descent(31/49): loss=4.211434059825618e+40\n",
      "Gradient Descent(32/49): loss=1.150313407036879e+42\n",
      "Gradient Descent(33/49): loss=3.141972343889953e+43\n",
      "Gradient Descent(34/49): loss=8.582000478633593e+44\n",
      "Gradient Descent(35/49): loss=2.344092313813398e+46\n",
      "Gradient Descent(36/49): loss=6.402666591967024e+47\n",
      "Gradient Descent(37/49): loss=1.7488278616980328e+49\n",
      "Gradient Descent(38/49): loss=4.77675800531057e+50\n",
      "Gradient Descent(39/49): loss=1.3047262993136174e+52\n",
      "Gradient Descent(40/49): loss=3.563736564063007e+53\n",
      "Gradient Descent(41/49): loss=9.73400958095272e+54\n",
      "Gradient Descent(42/49): loss=2.6587527113410614e+56\n",
      "Gradient Descent(43/49): loss=7.262131726165302e+57\n",
      "Gradient Descent(44/49): loss=1.9835826394541287e+59\n",
      "Gradient Descent(45/49): loss=5.41796849176872e+60\n",
      "Gradient Descent(46/49): loss=1.479866882978805e+62\n",
      "Gradient Descent(47/49): loss=4.042116514085678e+63\n",
      "Gradient Descent(48/49): loss=1.1040659198046414e+65\n",
      "Gradient Descent(49/49): loss=3.0156517038198323e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7823740726380982\n",
      "Gradient Descent(2/49): loss=2.459188547634925\n",
      "Gradient Descent(3/49): loss=12.96192788581372\n",
      "Gradient Descent(4/49): loss=116.496754004691\n",
      "Gradient Descent(5/49): loss=1999.5487521914188\n",
      "Gradient Descent(6/49): loss=48575.65700894275\n",
      "Gradient Descent(7/49): loss=1296020.3045398744\n",
      "Gradient Descent(8/49): loss=35249201.15425976\n",
      "Gradient Descent(9/49): loss=962246589.8821654\n",
      "Gradient Descent(10/49): loss=26286093519.567673\n",
      "Gradient Descent(11/49): loss=718162626258.1035\n",
      "Gradient Descent(12/49): loss=19621414690895.49\n",
      "Gradient Descent(13/49): loss=536092661650431.44\n",
      "Gradient Descent(14/49): loss=1.4647037416109168e+16\n",
      "Gradient Descent(15/49): loss=4.001840652975892e+17\n",
      "Gradient Descent(16/49): loss=1.0933766780665057e+19\n",
      "Gradient Descent(17/49): loss=2.987306769230509e+20\n",
      "Gradient Descent(18/49): loss=8.161873142575918e+21\n",
      "Gradient Descent(19/49): loss=2.229974299782206e+23\n",
      "Gradient Descent(20/49): loss=6.092701137372343e+24\n",
      "Gradient Descent(21/49): loss=1.6646383392508005e+26\n",
      "Gradient Descent(22/49): loss=4.5480990090096615e+27\n",
      "Gradient Descent(23/49): loss=1.2426245454054697e+29\n",
      "Gradient Descent(24/49): loss=3.3950794777893277e+30\n",
      "Gradient Descent(25/49): loss=9.275983403937058e+31\n",
      "Gradient Descent(26/49): loss=2.534369774640581e+33\n",
      "Gradient Descent(27/49): loss=6.924365724808901e+34\n",
      "Gradient Descent(28/49): loss=1.8918644457755744e+36\n",
      "Gradient Descent(29/49): loss=5.168922647118732e+37\n",
      "Gradient Descent(30/49): loss=1.4122450153104914e+39\n",
      "Gradient Descent(31/49): loss=3.858513890474015e+40\n",
      "Gradient Descent(32/49): loss=1.0542171706449797e+42\n",
      "Gradient Descent(33/49): loss=2.8803157755281012e+43\n",
      "Gradient Descent(34/49): loss=7.869554013885403e+44\n",
      "Gradient Descent(35/49): loss=2.1501073216914035e+46\n",
      "Gradient Descent(36/49): loss=5.874489820685893e+47\n",
      "Gradient Descent(37/49): loss=1.6050189823174181e+49\n",
      "Gradient Descent(38/49): loss=4.385207928232256e+50\n",
      "Gradient Descent(39/49): loss=1.19811969737991e+52\n",
      "Gradient Descent(40/49): loss=3.2734840234323712e+53\n",
      "Gradient Descent(41/49): loss=8.943762192625949e+54\n",
      "Gradient Descent(42/49): loss=2.4436008114184135e+56\n",
      "Gradient Descent(43/49): loss=6.676368173661818e+57\n",
      "Gradient Descent(44/49): loss=1.8241069401352306e+59\n",
      "Gradient Descent(45/49): loss=4.98379664287524e+60\n",
      "Gradient Descent(46/49): loss=1.3616651760391328e+62\n",
      "Gradient Descent(47/49): loss=3.720320439415088e+63\n",
      "Gradient Descent(48/49): loss=1.0164601706412542e+65\n",
      "Gradient Descent(49/49): loss=2.7771566867032936e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7677446393193402\n",
      "Gradient Descent(2/49): loss=2.3410582190840774\n",
      "Gradient Descent(3/49): loss=11.660160155460668\n",
      "Gradient Descent(4/49): loss=92.37003147990156\n",
      "Gradient Descent(5/49): loss=1417.5096648085507\n",
      "Gradient Descent(6/49): loss=33495.36504874719\n",
      "Gradient Descent(7/49): loss=899653.8891092378\n",
      "Gradient Descent(8/49): loss=24815516.57913018\n",
      "Gradient Descent(9/49): loss=687954938.1261353\n",
      "Gradient Descent(10/49): loss=19089896470.910202\n",
      "Gradient Descent(11/49): loss=529812909093.2017\n",
      "Gradient Descent(12/49): loss=14704675818261.129\n",
      "Gradient Descent(13/49): loss=408122892906047.94\n",
      "Gradient Descent(14/49): loss=1.1327313922506416e+16\n",
      "Gradient Descent(15/49): loss=3.143858604403378e+17\n",
      "Gradient Descent(16/49): loss=8.725676153250093e+18\n",
      "Gradient Descent(17/49): loss=2.421782734010696e+20\n",
      "Gradient Descent(18/49): loss=6.721578380089214e+21\n",
      "Gradient Descent(19/49): loss=1.865551987633236e+23\n",
      "Gradient Descent(20/49): loss=5.177778226950321e+24\n",
      "Gradient Descent(21/49): loss=1.4370753292012026e+26\n",
      "Gradient Descent(22/49): loss=3.98855534416785e+27\n",
      "Gradient Descent(23/49): loss=1.1070104266794117e+29\n",
      "Gradient Descent(24/49): loss=3.072471055388276e+30\n",
      "Gradient Descent(25/49): loss=8.527542432021557e+31\n",
      "Gradient Descent(26/49): loss=2.36679137472745e+33\n",
      "Gradient Descent(27/49): loss=6.568951671761328e+34\n",
      "Gradient Descent(28/49): loss=1.8231909464729678e+36\n",
      "Gradient Descent(29/49): loss=5.060206549532775e+37\n",
      "Gradient Descent(30/49): loss=1.404443696556848e+39\n",
      "Gradient Descent(31/49): loss=3.8979873202613588e+40\n",
      "Gradient Descent(32/49): loss=1.0818735693121105e+42\n",
      "Gradient Descent(33/49): loss=3.0027045339326884e+43\n",
      "Gradient Descent(34/49): loss=8.33390774472173e+44\n",
      "Gradient Descent(35/49): loss=2.3130487036820613e+46\n",
      "Gradient Descent(36/49): loss=6.41979065462287e+47\n",
      "Gradient Descent(37/49): loss=1.781791796410343e+49\n",
      "Gradient Descent(38/49): loss=4.945304569190455e+50\n",
      "Gradient Descent(39/49): loss=1.3725530295585592e+52\n",
      "Gradient Descent(40/49): loss=3.809475822150984e+53\n",
      "Gradient Descent(41/49): loss=1.057307493920293e+55\n",
      "Gradient Descent(42/49): loss=2.9345221990904136e+56\n",
      "Gradient Descent(43/49): loss=8.144669915300408e+57\n",
      "Gradient Descent(44/49): loss=2.2605263660899028e+59\n",
      "Gradient Descent(45/49): loss=6.2740166328754716e+60\n",
      "Gradient Descent(46/49): loss=1.741332695786569e+62\n",
      "Gradient Descent(47/49): loss=4.833011665169222e+63\n",
      "Gradient Descent(48/49): loss=1.3413865031179608e+65\n",
      "Gradient Descent(49/49): loss=3.7229741523582014e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7826735959133512\n",
      "Gradient Descent(2/49): loss=2.5028249277236525\n",
      "Gradient Descent(3/49): loss=14.032516984192878\n",
      "Gradient Descent(4/49): loss=145.03325598460313\n",
      "Gradient Descent(5/49): loss=2861.840592373266\n",
      "Gradient Descent(6/49): loss=75577.47095593504\n",
      "Gradient Descent(7/49): loss=2141278.820400763\n",
      "Gradient Descent(8/49): loss=61498964.01896064\n",
      "Gradient Descent(9/49): loss=1770732419.5135276\n",
      "Gradient Descent(10/49): loss=51007885821.521965\n",
      "Gradient Descent(11/49): loss=1469460944682.3162\n",
      "Gradient Descent(12/49): loss=42333619157124.08\n",
      "Gradient Descent(13/49): loss=1219590287504965.2\n",
      "Gradient Descent(14/49): loss=3.513522475322084e+16\n",
      "Gradient Descent(15/49): loss=1.0122121709639963e+18\n",
      "Gradient Descent(16/49): loss=2.9160863592468734e+19\n",
      "Gradient Descent(17/49): loss=8.400965651917172e+20\n",
      "Gradient Descent(18/49): loss=2.4202377854977267e+22\n",
      "Gradient Descent(19/49): loss=6.972473381810197e+23\n",
      "Gradient Descent(20/49): loss=2.0087028370627e+25\n",
      "Gradient Descent(21/49): loss=5.786880589840233e+26\n",
      "Gradient Descent(22/49): loss=1.6671449028298645e+28\n",
      "Gradient Descent(23/49): loss=4.8028848770641105e+29\n",
      "Gradient Descent(24/49): loss=1.3836651573103349e+31\n",
      "Gradient Descent(25/49): loss=3.9862068664135165e+32\n",
      "Gradient Descent(26/49): loss=1.1483880401187351e+34\n",
      "Gradient Descent(27/49): loss=3.3083960137630998e+35\n",
      "Gradient Descent(28/49): loss=9.531172218366019e+36\n",
      "Gradient Descent(29/49): loss=2.7458394786549218e+38\n",
      "Gradient Descent(30/49): loss=7.910500691626686e+39\n",
      "Gradient Descent(31/49): loss=2.278939525732261e+41\n",
      "Gradient Descent(32/49): loss=6.565406621406493e+42\n",
      "Gradient Descent(33/49): loss=1.891430800058585e+44\n",
      "Gradient Descent(34/49): loss=5.449031077139611e+45\n",
      "Gradient Descent(35/49): loss=1.5698136922965125e+47\n",
      "Gradient Descent(36/49): loss=4.522482976579606e+48\n",
      "Gradient Descent(37/49): loss=1.3028840539370287e+50\n",
      "Gradient Descent(38/49): loss=3.753484240392362e+51\n",
      "Gradient Descent(39/49): loss=1.0813428793069265e+53\n",
      "Gradient Descent(40/49): loss=3.1152453233841424e+54\n",
      "Gradient Descent(41/49): loss=8.974723568797081e+55\n",
      "Gradient Descent(42/49): loss=2.585531949336891e+57\n",
      "Gradient Descent(43/49): loss=7.448670045152133e+58\n",
      "Gradient Descent(44/49): loss=2.1458905373718066e+60\n",
      "Gradient Descent(45/49): loss=6.182105222097967e+61\n",
      "Gradient Descent(46/49): loss=1.7810053360829245e+63\n",
      "Gradient Descent(47/49): loss=5.1309058859392417e+64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=1.4781648699754858e+66\n",
      "Gradient Descent(49/49): loss=4.2584514925860363e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7869360786741632\n",
      "Gradient Descent(2/49): loss=2.5533641332562986\n",
      "Gradient Descent(3/49): loss=14.142073016494326\n",
      "Gradient Descent(4/49): loss=135.43923756776994\n",
      "Gradient Descent(5/49): loss=2447.483087952901\n",
      "Gradient Descent(6/49): loss=61351.53075658484\n",
      "Gradient Descent(7/49): loss=1677863.44211093\n",
      "Gradient Descent(8/49): loss=46716415.29369853\n",
      "Gradient Descent(9/49): loss=1305228399.4169214\n",
      "Gradient Descent(10/49): loss=36491407701.74869\n",
      "Gradient Descent(11/49): loss=1020350514237.5929\n",
      "Gradient Descent(12/49): loss=28531103099328.324\n",
      "Gradient Descent(13/49): loss=797792076609720.0\n",
      "Gradient Descent(14/49): loss=2.230803160891199e+16\n",
      "Gradient Descent(15/49): loss=6.237820242048547e+17\n",
      "Gradient Descent(16/49): loss=1.74423289679065e+19\n",
      "Gradient Descent(17/49): loss=4.8772620857741266e+20\n",
      "Gradient Descent(18/49): loss=1.3637906683318e+22\n",
      "Gradient Descent(19/49): loss=3.813461230342184e+23\n",
      "Gradient Descent(20/49): loss=1.066328351821832e+25\n",
      "Gradient Descent(21/49): loss=2.9816906092053226e+26\n",
      "Gradient Descent(22/49): loss=8.337468354702307e+27\n",
      "Gradient Descent(23/49): loss=2.331341097264543e+29\n",
      "Gradient Descent(24/49): loss=6.518946856007535e+30\n",
      "Gradient Descent(25/49): loss=1.822842147007737e+32\n",
      "Gradient Descent(26/49): loss=5.097070993677024e+33\n",
      "Gradient Descent(27/49): loss=1.4252541152414919e+35\n",
      "Gradient Descent(28/49): loss=3.9853266621805433e+36\n",
      "Gradient Descent(29/49): loss=1.1143857389666822e+38\n",
      "Gradient Descent(30/49): loss=3.116069723962986e+39\n",
      "Gradient Descent(31/49): loss=8.713222168117907e+40\n",
      "Gradient Descent(32/49): loss=2.4364101986275078e+42\n",
      "Gradient Descent(33/49): loss=6.812743370296154e+43\n",
      "Gradient Descent(34/49): loss=1.9049941695228233e+45\n",
      "Gradient Descent(35/49): loss=5.32678627194244e+46\n",
      "Gradient Descent(36/49): loss=1.4894876026870562e+48\n",
      "Gradient Descent(37/49): loss=4.1649377416253536e+49\n",
      "Gradient Descent(38/49): loss=1.164608980989253e+51\n",
      "Gradient Descent(39/49): loss=3.256505049392471e+52\n",
      "Gradient Descent(40/49): loss=9.105910489983115e+53\n",
      "Gradient Descent(41/49): loss=2.5462145642013876e+55\n",
      "Gradient Descent(42/49): loss=7.1197807337149785e+56\n",
      "Gradient Descent(43/49): loss=1.990848627168877e+58\n",
      "Gradient Descent(44/49): loss=5.566854380123761e+59\n",
      "Gradient Descent(45/49): loss=1.5566159710280543e+61\n",
      "Gradient Descent(46/49): loss=4.352643550208636e+62\n",
      "Gradient Descent(47/49): loss=1.2170956888397193e+64\n",
      "Gradient Descent(48/49): loss=3.403269527368479e+65\n",
      "Gradient Descent(49/49): loss=9.516296526328691e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7941190031051253\n",
      "Gradient Descent(2/49): loss=2.5804251141419914\n",
      "Gradient Descent(3/49): loss=14.043181714675608\n",
      "Gradient Descent(4/49): loss=129.06404537100082\n",
      "Gradient Descent(5/49): loss=2255.8387047092056\n",
      "Gradient Descent(6/49): loss=55963.133896093066\n",
      "Gradient Descent(7/49): loss=1527539.383549544\n",
      "Gradient Descent(8/49): loss=42525481.51921283\n",
      "Gradient Descent(9/49): loss=1188385054.7905533\n",
      "Gradient Descent(10/49): loss=33233722741.76624\n",
      "Gradient Descent(11/49): loss=929523363518.6522\n",
      "Gradient Descent(12/49): loss=25998776128096.625\n",
      "Gradient Descent(13/49): loss=727189559857403.6\n",
      "Gradient Descent(14/49): loss=2.0339616999393372e+16\n",
      "Gradient Descent(15/49): loss=5.689026845092492e+17\n",
      "Gradient Descent(16/49): loss=1.5912309227967992e+19\n",
      "Gradient Descent(17/49): loss=4.45070120894882e+20\n",
      "Gradient Descent(18/49): loss=1.244869050201728e+22\n",
      "Gradient Descent(19/49): loss=3.4819208923460894e+23\n",
      "Gradient Descent(20/49): loss=9.738994715238967e+24\n",
      "Gradient Descent(21/49): loss=2.7240141576083335e+26\n",
      "Gradient Descent(22/49): loss=7.619116087260331e+27\n",
      "Gradient Descent(23/49): loss=2.1310803319074186e+29\n",
      "Gradient Descent(24/49): loss=5.960669622341891e+30\n",
      "Gradient Descent(25/49): loss=1.667209903575491e+32\n",
      "Gradient Descent(26/49): loss=4.6632157772370076e+33\n",
      "Gradient Descent(27/49): loss=1.3043097535851068e+35\n",
      "Gradient Descent(28/49): loss=3.648177597960581e+36\n",
      "Gradient Descent(29/49): loss=1.0204017680370036e+38\n",
      "Gradient Descent(30/49): loss=2.854081908718243e+39\n",
      "Gradient Descent(31/49): loss=7.98291790237009e+40\n",
      "Gradient Descent(32/49): loss=2.2328363471741022e+42\n",
      "Gradient Descent(33/49): loss=6.245283008336486e+43\n",
      "Gradient Descent(34/49): loss=1.746816774260223e+45\n",
      "Gradient Descent(35/49): loss=4.885877611573107e+46\n",
      "Gradient Descent(36/49): loss=1.3665886649950076e+48\n",
      "Gradient Descent(37/49): loss=3.822372821761178e+49\n",
      "Gradient Descent(38/49): loss=1.06912448220781e+51\n",
      "Gradient Descent(39/49): loss=2.9903602075358713e+52\n",
      "Gradient Descent(40/49): loss=8.364090729966019e+53\n",
      "Gradient Descent(41/49): loss=2.3394510655540864e+55\n",
      "Gradient Descent(42/49): loss=6.543486273425799e+56\n",
      "Gradient Descent(43/49): loss=1.8302247583182814e+58\n",
      "Gradient Descent(44/49): loss=5.119171227675719e+59\n",
      "Gradient Descent(45/49): loss=1.4318413046899182e+61\n",
      "Gradient Descent(46/49): loss=4.0048856165082153e+62\n",
      "Gradient Descent(47/49): loss=1.1201736357778881e+64\n",
      "Gradient Descent(48/49): loss=3.13314559876473e+65\n",
      "Gradient Descent(49/49): loss=8.763464010864187e+66\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7790040258878366\n",
      "Gradient Descent(2/49): loss=2.4556145437804022\n",
      "Gradient Descent(3/49): loss=12.638740975134091\n",
      "Gradient Descent(4/49): loss=102.50312897019953\n",
      "Gradient Descent(5/49): loss=1600.8068515804055\n",
      "Gradient Descent(6/49): loss=38595.18395257582\n",
      "Gradient Descent(7/49): loss=1060224.6122149746\n",
      "Gradient Descent(8/49): loss=29931449.105923846\n",
      "Gradient Descent(9/49): loss=849410209.7599819\n",
      "Gradient Descent(10/49): loss=24128452413.187614\n",
      "Gradient Descent(11/49): loss=685519907641.2792\n",
      "Gradient Descent(12/49): loss=19477145167019.066\n",
      "Gradient Descent(13/49): loss=553392478118010.94\n",
      "Gradient Descent(14/49): loss=1.5723227870643132e+16\n",
      "Gradient Descent(15/49): loss=4.467352883195805e+17\n",
      "Gradient Descent(16/49): loss=1.2692840650783197e+19\n",
      "Gradient Descent(17/49): loss=3.606346066834052e+20\n",
      "Gradient Descent(18/49): loss=1.0246510083710525e+22\n",
      "Gradient Descent(19/49): loss=2.9112838029713764e+23\n",
      "Gradient Descent(20/49): loss=8.271668414523815e+24\n",
      "Gradient Descent(21/49): loss=2.3501830460710433e+26\n",
      "Gradient Descent(22/49): loss=6.67744410589617e+27\n",
      "Gradient Descent(23/49): loss=1.8972249783659405e+29\n",
      "Gradient Descent(24/49): loss=5.390479592869335e+30\n",
      "Gradient Descent(25/49): loss=1.5315669239273427e+32\n",
      "Gradient Descent(26/49): loss=4.3515557420367684e+33\n",
      "Gradient Descent(27/49): loss=1.2363832804313052e+35\n",
      "Gradient Descent(28/49): loss=3.512866907260444e+36\n",
      "Gradient Descent(29/49): loss=9.980912960761565e+37\n",
      "Gradient Descent(30/49): loss=2.835821172854788e+39\n",
      "Gradient Descent(31/49): loss=8.057260649428201e+40\n",
      "Gradient Descent(32/49): loss=2.2892645627394238e+42\n",
      "Gradient Descent(33/49): loss=6.504359814382254e+43\n",
      "Gradient Descent(34/49): loss=1.8480475032700038e+45\n",
      "Gradient Descent(35/49): loss=5.250754373690586e+46\n",
      "Gradient Descent(36/49): loss=1.4918675761335347e+48\n",
      "Gradient Descent(37/49): loss=4.2387601977165594e+49\n",
      "Gradient Descent(38/49): loss=1.2043353110677092e+51\n",
      "Gradient Descent(39/49): loss=3.4218107980392666e+52\n",
      "Gradient Descent(40/49): loss=9.722200312467602e+53\n",
      "Gradient Descent(41/49): loss=2.7623145899797745e+55\n",
      "Gradient Descent(42/49): loss=7.84841049225261e+56\n",
      "Gradient Descent(43/49): loss=2.2299251315670502e+58\n",
      "Gradient Descent(44/49): loss=6.3357619957604345e+59\n",
      "Gradient Descent(45/49): loss=1.8001447447122953e+61\n",
      "Gradient Descent(46/49): loss=5.1146509355680115e+62\n",
      "Gradient Descent(47/49): loss=1.4531972648059518e+64\n",
      "Gradient Descent(48/49): loss=4.128888397356364e+65\n",
      "Gradient Descent(49/49): loss=1.1731180487805878e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7942211271095996\n",
      "Gradient Descent(2/49): loss=2.6247106412742123\n",
      "Gradient Descent(3/49): loss=15.18072966573653\n",
      "Gradient Descent(4/49): loss=160.2450529487259\n",
      "Gradient Descent(5/49): loss=3220.6230847033576\n",
      "Gradient Descent(6/49): loss=86878.81602839583\n",
      "Gradient Descent(7/49): loss=2517886.1106179124\n",
      "Gradient Descent(8/49): loss=73999725.94130792\n",
      "Gradient Descent(9/49): loss=2180468043.549779\n",
      "Gradient Descent(10/49): loss=64280019655.758385\n",
      "Gradient Descent(11/49): loss=1895134996141.8464\n",
      "Gradient Descent(12/49): loss=55874190169396.086\n",
      "Gradient Descent(13/49): loss=1647341362263775.0\n",
      "Gradient Descent(14/49): loss=4.8568668485416936e+16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=1.4319532334070848e+18\n",
      "Gradient Descent(16/49): loss=4.2218371709950394e+19\n",
      "Gradient Descent(17/49): loss=1.2447270442460614e+21\n",
      "Gradient Descent(18/49): loss=3.6698369758500246e+22\n",
      "Gradient Descent(19/49): loss=1.0819804625490695e+24\n",
      "Gradient Descent(20/49): loss=3.1900101532180064e+25\n",
      "Gradient Descent(21/49): loss=9.40512803133015e+26\n",
      "Gradient Descent(22/49): loss=2.7729201174026576e+28\n",
      "Gradient Descent(23/49): loss=8.175418720387209e+29\n",
      "Gradient Descent(24/49): loss=2.4103641080098688e+31\n",
      "Gradient Descent(25/49): loss=7.10649244019042e+32\n",
      "Gradient Descent(26/49): loss=2.095211865902758e+34\n",
      "Gradient Descent(27/49): loss=6.17732700057859e+35\n",
      "Gradient Descent(28/49): loss=1.8212654048537488e+37\n",
      "Gradient Descent(29/49): loss=5.369648837768116e+38\n",
      "Gradient Descent(30/49): loss=1.5831371179677066e+40\n",
      "Gradient Descent(31/49): loss=4.667573634719964e+41\n",
      "Gradient Descent(32/49): loss=1.3761438215472315e+43\n",
      "Gradient Descent(33/49): loss=4.057293929967585e+44\n",
      "Gradient Descent(34/49): loss=1.196214652596652e+46\n",
      "Gradient Descent(35/49): loss=3.5268075712186416e+47\n",
      "Gradient Descent(36/49): loss=1.039811008618333e+49\n",
      "Gradient Descent(37/49): loss=3.0656816733276976e+50\n",
      "Gradient Descent(38/49): loss=9.03856955185123e+51\n",
      "Gradient Descent(39/49): loss=2.664847438480939e+53\n",
      "Gradient Descent(40/49): loss=7.856787326401682e+54\n",
      "Gradient Descent(41/49): loss=2.31642180339952e+56\n",
      "Gradient Descent(42/49): loss=6.829521722235674e+57\n",
      "Gradient Descent(43/49): loss=2.013552405958135e+59\n",
      "Gradient Descent(44/49): loss=5.9365698747825786e+60\n",
      "Gradient Descent(45/49): loss=1.750282822234592e+62\n",
      "Gradient Descent(46/49): loss=5.160370420000596e+63\n",
      "Gradient Descent(47/49): loss=1.5214354236544997e+65\n",
      "Gradient Descent(48/49): loss=4.4856581213223014e+66\n",
      "Gradient Descent(49/49): loss=1.322509550425374e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7985976652758474\n",
      "Gradient Descent(2/49): loss=2.6780325993718512\n",
      "Gradient Descent(3/49): loss=15.304092912850601\n",
      "Gradient Descent(4/49): loss=149.79528774767297\n",
      "Gradient Descent(5/49): loss=2756.610004852701\n",
      "Gradient Descent(6/49): loss=70561.57107535217\n",
      "Gradient Descent(7/49): loss=1973810.438065007\n",
      "Gradient Descent(8/49): loss=56237435.34132228\n",
      "Gradient Descent(9/49): loss=1608039233.4270496\n",
      "Gradient Descent(10/49): loss=46011381551.730095\n",
      "Gradient Descent(11/49): loss=1316712096777.4944\n",
      "Gradient Descent(12/49): loss=37681420120667.56\n",
      "Gradient Descent(13/49): loss=1078364989921897.5\n",
      "Gradient Descent(14/49): loss=3.086062344170336e+16\n",
      "Gradient Descent(15/49): loss=8.831687369153618e+17\n",
      "Gradient Descent(16/49): loss=2.527450702576239e+19\n",
      "Gradient Descent(17/49): loss=7.233053920349218e+20\n",
      "Gradient Descent(18/49): loss=2.069954085570773e+22\n",
      "Gradient Descent(19/49): loss=5.923790925047867e+23\n",
      "Gradient Descent(20/49): loss=1.6952694347119017e+25\n",
      "Gradient Descent(21/49): loss=4.851519057063572e+26\n",
      "Gradient Descent(22/49): loss=1.3884068620094902e+28\n",
      "Gradient Descent(23/49): loss=3.9733402915713385e+29\n",
      "Gradient Descent(24/49): loss=1.137089818886008e+31\n",
      "Gradient Descent(25/49): loss=3.254121623957173e+32\n",
      "Gradient Descent(26/49): loss=9.312639483378583e+33\n",
      "Gradient Descent(27/49): loss=2.6650895132161123e+35\n",
      "Gradient Descent(28/49): loss=7.626948435114983e+36\n",
      "Gradient Descent(29/49): loss=2.182678748441263e+38\n",
      "Gradient Descent(30/49): loss=6.246386165353876e+39\n",
      "Gradient Descent(31/49): loss=1.78758968330035e+41\n",
      "Gradient Descent(32/49): loss=5.115720980502041e+42\n",
      "Gradient Descent(33/49): loss=1.4640161215313546e+44\n",
      "Gradient Descent(34/49): loss=4.1897187361719646e+45\n",
      "Gradient Descent(35/49): loss=1.1990129637280665e+47\n",
      "Gradient Descent(36/49): loss=3.431333169876512e+48\n",
      "Gradient Descent(37/49): loss=9.819783170722559e+49\n",
      "Gradient Descent(38/49): loss=2.8102238035799977e+51\n",
      "Gradient Descent(39/49): loss=8.042293489486826e+52\n",
      "Gradient Descent(40/49): loss=2.3015421223265785e+54\n",
      "Gradient Descent(41/49): loss=6.586549157610443e+55\n",
      "Gradient Descent(42/49): loss=1.884937468003655e+57\n",
      "Gradient Descent(43/49): loss=5.394310697853877e+58\n",
      "Gradient Descent(44/49): loss=1.5437428773592202e+60\n",
      "Gradient Descent(45/49): loss=4.417880624387197e+61\n",
      "Gradient Descent(46/49): loss=1.2643082923707706e+63\n",
      "Gradient Descent(47/49): loss=3.618195225406809e+64\n",
      "Gradient Descent(48/49): loss=1.0354544669329215e+66\n",
      "Gradient Descent(49/49): loss=2.9632617542654065e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8060394070539094\n",
      "Gradient Descent(2/49): loss=2.7068852802314423\n",
      "Gradient Descent(3/49): loss=15.20109116645995\n",
      "Gradient Descent(4/49): loss=142.81690757401282\n",
      "Gradient Descent(5/49): loss=2541.4854592837296\n",
      "Gradient Descent(6/49): loss=64369.18020002332\n",
      "Gradient Descent(7/49): loss=1796996.9145804185\n",
      "Gradient Descent(8/49): loss=51192345.77336208\n",
      "Gradient Descent(9/49): loss=1464082658.803824\n",
      "Gradient Descent(10/49): loss=41903611019.337494\n",
      "Gradient Descent(11/49): loss=1199497285236.7495\n",
      "Gradient Descent(12/49): loss=34336726366126.855\n",
      "Gradient Descent(13/49): loss=982925838969906.8\n",
      "Gradient Descent(14/49): loss=2.813734034831372e+16\n",
      "Gradient Descent(15/49): loss=8.054626696075254e+17\n",
      "Gradient Descent(16/49): loss=2.305726579883031e+19\n",
      "Gradient Descent(17/49): loss=6.600399122019676e+20\n",
      "Gradient Descent(18/49): loss=1.8894377592903243e+22\n",
      "Gradient Descent(19/49): loss=5.408726019614974e+23\n",
      "Gradient Descent(20/49): loss=1.5483080621622979e+25\n",
      "Gradient Descent(21/49): loss=4.432204268961599e+26\n",
      "Gradient Descent(22/49): loss=1.2687678351555116e+28\n",
      "Gradient Descent(23/49): loss=3.631989235692228e+29\n",
      "Gradient Descent(24/49): loss=1.039697369579661e+31\n",
      "Gradient Descent(25/49): loss=2.976249515521681e+32\n",
      "Gradient Descent(26/49): loss=8.519845714550902e+33\n",
      "Gradient Descent(27/49): loss=2.4389007245928296e+35\n",
      "Gradient Descent(28/49): loss=6.981624953912896e+36\n",
      "Gradient Descent(29/49): loss=1.998567900103267e+38\n",
      "Gradient Descent(30/49): loss=5.721123202249043e+39\n",
      "Gradient Descent(31/49): loss=1.6377352349961275e+41\n",
      "Gradient Descent(32/49): loss=4.6881995110564923e+42\n",
      "Gradient Descent(33/49): loss=1.3420493243232696e+44\n",
      "Gradient Descent(34/49): loss=3.841765660076775e+45\n",
      "Gradient Descent(35/49): loss=1.0997482074205874e+47\n",
      "Gradient Descent(36/49): loss=3.148151726934457e+48\n",
      "Gradient Descent(37/49): loss=9.011934940131105e+49\n",
      "Gradient Descent(38/49): loss=2.5797667460024835e+51\n",
      "Gradient Descent(39/49): loss=7.384869628989275e+52\n",
      "Gradient Descent(40/49): loss=2.114001179435151e+54\n",
      "Gradient Descent(41/49): loss=6.051563820585379e+55\n",
      "Gradient Descent(42/49): loss=1.7323275422393065e+57\n",
      "Gradient Descent(43/49): loss=4.958980525649582e+58\n",
      "Gradient Descent(44/49): loss=1.419563405542969e+60\n",
      "Gradient Descent(45/49): loss=4.063658350609939e+61\n",
      "Gradient Descent(46/49): loss=1.1632674613900494e+63\n",
      "Gradient Descent(47/49): loss=3.3299826658058634e+64\n",
      "Gradient Descent(48/49): loss=9.532446253862617e+65\n",
      "Gradient Descent(49/49): loss=2.7287689066930586e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.7904322055846768\n",
      "Gradient Descent(2/49): loss=2.5751200282453683\n",
      "Gradient Descent(3/49): loss=13.687277303908918\n",
      "Gradient Descent(4/49): loss=113.60875172848903\n",
      "Gradient Descent(5/49): loss=1805.314883464443\n",
      "Gradient Descent(6/49): loss=44399.30738639032\n",
      "Gradient Descent(7/49): loss=1247087.501036485\n",
      "Gradient Descent(8/49): loss=36023875.28719342\n",
      "Gradient Descent(9/49): loss=1046201259.297184\n",
      "Gradient Descent(10/49): loss=30414299130.713\n",
      "Gradient Descent(11/49): loss=884346135846.1251\n",
      "Gradient Descent(12/49): loss=25714735508345.207\n",
      "Gradient Descent(13/49): loss=747729822724412.6\n",
      "Gradient Descent(14/49): loss=2.1742419874651104e+16\n",
      "Gradient Descent(15/49): loss=6.322242559600092e+17\n",
      "Gradient Descent(16/49): loss=1.8383764517807475e+19\n",
      "Gradient Descent(17/49): loss=5.3456158535248144e+20\n",
      "Gradient Descent(18/49): loss=1.554393762427321e+22\n",
      "Gradient Descent(19/49): loss=4.519853344416195e+23\n",
      "Gradient Descent(20/49): loss=1.3142792225440346e+25\n",
      "Gradient Descent(21/49): loss=3.821650268723449e+26\n",
      "Gradient Descent(22/49): loss=1.1112563088527398e+28\n",
      "Gradient Descent(23/49): loss=3.231301917058809e+29\n",
      "Gradient Descent(24/49): loss=9.395953027225576e+30\n",
      "Gradient Descent(25/49): loss=2.7321474611753266e+32\n",
      "Gradient Descent(26/49): loss=7.94451582290512e+33\n",
      "Gradient Descent(27/49): loss=2.3100997496394286e+35\n",
      "Gradient Descent(28/49): loss=6.717288972976874e+36\n",
      "Gradient Descent(29/49): loss=1.953247739779173e+38\n",
      "Gradient Descent(30/49): loss=5.679637645932183e+39\n",
      "Gradient Descent(31/49): loss=1.6515203438933548e+41\n",
      "Gradient Descent(32/49): loss=4.8022772161302233e+42\n",
      "Gradient Descent(33/49): loss=1.3964022027241155e+44\n",
      "Gradient Descent(34/49): loss=4.060446792249263e+45\n",
      "Gradient Descent(35/49): loss=1.1806933647429106e+47\n",
      "Gradient Descent(36/49): loss=3.433210414698547e+48\n",
      "Gradient Descent(37/49): loss=9.983060889108229e+49\n",
      "Gradient Descent(38/49): loss=2.902866200363453e+51\n",
      "Gradient Descent(39/49): loss=8.440930362756924e+52\n",
      "Gradient Descent(40/49): loss=2.4544467595506278e+54\n",
      "Gradient Descent(41/49): loss=7.137020016240162e+55\n",
      "Gradient Descent(42/49): loss=2.0752967858850447e+57\n",
      "Gradient Descent(43/49): loss=6.034530854200554e+58\n",
      "Gradient Descent(44/49): loss=1.7547158978886762e+60\n",
      "Gradient Descent(45/49): loss=5.1023483957497915e+61\n",
      "Gradient Descent(46/49): loss=1.4836566525062624e+63\n",
      "Gradient Descent(47/49): loss=4.314164560694694e+64\n",
      "Gradient Descent(48/49): loss=1.2544692079069294e+66\n",
      "Gradient Descent(49/49): loss=3.647735202139077e+67\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=0.8059390703724962\n",
      "Gradient Descent(2/49): loss=2.751775622038762\n",
      "Gradient Descent(3/49): loss=16.408731703280548\n",
      "Gradient Descent(4/49): loss=176.8517069443955\n",
      "Gradient Descent(5/49): loss=3619.568095149332\n",
      "Gradient Descent(6/49): loss=99711.96726049719\n",
      "Gradient Descent(7/49): loss=2955259.9884834327\n",
      "Gradient Descent(8/49): loss=88853402.31220156\n",
      "Gradient Descent(9/49): loss=2678633094.011645\n",
      "Gradient Descent(10/49): loss=80791663278.07607\n",
      "Gradient Descent(11/49): loss=2437021177915.7793\n",
      "Gradient Descent(12/49): loss=73512182506266.88\n",
      "Gradient Descent(13/49): loss=2217484875852898.8\n",
      "Gradient Descent(14/49): loss=6.689016421337748e+16\n",
      "Gradient Descent(15/49): loss=2.0177339578065902e+18\n",
      "Gradient Descent(16/49): loss=6.086470934085136e+19\n",
      "Gradient Descent(17/49): loss=1.8359768600306505e+21\n",
      "Gradient Descent(18/49): loss=5.538202793153213e+22\n",
      "Gradient Descent(19/49): loss=1.6705924159223088e+24\n",
      "Gradient Descent(20/49): loss=5.039322546384578e+25\n",
      "Gradient Descent(21/49): loss=1.520105771136404e+27\n",
      "Gradient Descent(22/49): loss=4.585381336826311e+28\n",
      "Gradient Descent(23/49): loss=1.3831749344915076e+30\n",
      "Gradient Descent(24/49): loss=4.172331064464478e+31\n",
      "Gradient Descent(25/49): loss=1.258578801378823e+33\n",
      "Gradient Descent(26/49): loss=3.7964882815056324e+34\n",
      "Gradient Descent(27/49): loss=1.1452062640670061e+36\n",
      "Gradient Descent(28/49): loss=3.4545013444323784e+37\n",
      "Gradient Descent(29/49): loss=1.0420463031965373e+39\n",
      "Gradient Descent(30/49): loss=3.1433205251335905e+40\n",
      "Gradient Descent(31/49): loss=9.481789718380977e+41\n",
      "Gradient Descent(32/49): loss=2.8601708144216954e+43\n",
      "Gradient Descent(33/49): loss=8.627671917054734e+44\n",
      "Gradient Descent(34/49): loss=2.60252717540528e+46\n",
      "Gradient Descent(35/49): loss=7.850492883641406e+47\n",
      "Gradient Descent(36/49): loss=2.3680920260325616e+49\n",
      "Gradient Descent(37/49): loss=7.1433219886672455e+50\n",
      "Gradient Descent(38/49): loss=2.1547747500027172e+52\n",
      "Gradient Descent(39/49): loss=6.499852912433795e+53\n",
      "Gradient Descent(40/49): loss=1.960673053330544e+55\n",
      "Gradient Descent(41/49): loss=5.914347407312689e+56\n",
      "Gradient Descent(42/49): loss=1.7840560003090292e+58\n",
      "Gradient Descent(43/49): loss=5.381584125922831e+59\n",
      "Gradient Descent(44/49): loss=1.6233485775877062e+61\n",
      "Gradient Descent(45/49): loss=4.896812058854715e+62\n",
      "Gradient Descent(46/49): loss=1.477117648717033e+64\n",
      "Gradient Descent(47/49): loss=4.455708166716067e+65\n",
      "Gradient Descent(48/49): loss=1.3440591739042877e+67\n",
      "Gradient Descent(49/49): loss=4.0543388286754414e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8104310671744472\n",
      "Gradient Descent(2/49): loss=2.807990249142279\n",
      "Gradient Descent(3/49): loss=16.54710192434655\n",
      "Gradient Descent(4/49): loss=165.48135760679955\n",
      "Gradient Descent(5/49): loss=3100.6187901607545\n",
      "Gradient Descent(6/49): loss=81025.11513555943\n",
      "Gradient Descent(7/49): loss=2317647.442983889\n",
      "Gradient Descent(8/49): loss=67555218.41799957\n",
      "Gradient Descent(9/49): loss=1976371912.142973\n",
      "Gradient Descent(10/49): loss=57861067540.33008\n",
      "Gradient Descent(11/49): loss=1694195064966.7544\n",
      "Gradient Descent(12/49): loss=49608004313547.7\n",
      "Gradient Descent(13/49): loss=1452587440153656.8\n",
      "Gradient Descent(14/49): loss=4.253370679957179e+16\n",
      "Gradient Descent(15/49): loss=1.2454441639404099e+18\n",
      "Gradient Descent(16/49): loss=3.646828215833172e+19\n",
      "Gradient Descent(17/49): loss=1.0678404148461632e+21\n",
      "Gradient Descent(18/49): loss=3.126780548994817e+22\n",
      "Gradient Descent(19/49): loss=9.155634557484187e+23\n",
      "Gradient Descent(20/49): loss=2.6808931052489587e+25\n",
      "Gradient Descent(21/49): loss=7.850016071210998e+26\n",
      "Gradient Descent(22/49): loss=2.298590428604907e+28\n",
      "Gradient Descent(23/49): loss=6.730582346006206e+29\n",
      "Gradient Descent(24/49): loss=1.9708051574837542e+31\n",
      "Gradient Descent(25/49): loss=5.770782926486875e+32\n",
      "Gradient Descent(26/49): loss=1.6897629609997155e+34\n",
      "Gradient Descent(27/49): loss=4.947853524798515e+35\n",
      "Gradient Descent(28/49): loss=1.4487981490834402e+37\n",
      "Gradient Descent(29/49): loss=4.242276102692507e+38\n",
      "Gradient Descent(30/49): loss=1.2421955772694194e+40\n",
      "Gradient Descent(31/49): loss=3.637315947465928e+41\n",
      "Gradient Descent(32/49): loss=1.0650550962975322e+43\n",
      "Gradient Descent(33/49): loss=3.1186247621397704e+44\n",
      "Gradient Descent(34/49): loss=9.131753315712583e+45\n",
      "Gradient Descent(35/49): loss=2.6739003560598435e+47\n",
      "Gradient Descent(36/49): loss=7.829540359828745e+48\n",
      "Gradient Descent(37/49): loss=2.2925948645490255e+50\n",
      "Gradient Descent(38/49): loss=6.713026526976867e+51\n",
      "Gradient Descent(39/49): loss=1.9656645772326027e+53\n",
      "Gradient Descent(40/49): loss=5.755730615185057e+54\n",
      "Gradient Descent(41/49): loss=1.6853554415280306e+56\n",
      "Gradient Descent(42/49): loss=4.934947714186604e+57\n",
      "Gradient Descent(43/49): loss=1.4450191539225089e+59\n",
      "Gradient Descent(44/49): loss=4.231210695911399e+60\n",
      "Gradient Descent(45/49): loss=1.2389554771365573e+62\n",
      "Gradient Descent(46/49): loss=3.627828497904682e+63\n",
      "Gradient Descent(47/49): loss=1.0622770432902756e+65\n",
      "Gradient Descent(48/49): loss=3.110490248790111e+66\n",
      "Gradient Descent(49/49): loss=9.107934365079432e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8181352844844509\n",
      "Gradient Descent(2/49): loss=2.8387171656808445\n",
      "Gradient Descent(3/49): loss=16.439962074389236\n",
      "Gradient Descent(4/49): loss=157.85082579503614\n",
      "Gradient Descent(5/49): loss=2859.4584891592253\n",
      "Gradient Descent(6/49): loss=73920.11120673695\n",
      "Gradient Descent(7/49): loss=2110063.123502445\n",
      "Gradient Descent(8/49): loss=61494784.45118031\n",
      "Gradient Descent(9/49): loss=1799434203.0774522\n",
      "Gradient Descent(10/49): loss=52695128993.02346\n",
      "Gradient Descent(11/49): loss=1543367798730.6394\n",
      "Gradient Descent(12/49): loss=45204401965298.305\n",
      "Gradient Descent(13/49): loss=1324019495149897.8\n",
      "Gradient Descent(14/49): loss=3.878006052599022e+16\n",
      "Gradient Descent(15/49): loss=1.1358544180416517e+18\n",
      "Gradient Descent(16/49): loss=3.326878015408621e+19\n",
      "Gradient Descent(17/49): loss=9.744309864038387e+20\n",
      "Gradient Descent(18/49): loss=2.8540744331046586e+22\n",
      "Gradient Descent(19/49): loss=8.359484648501053e+23\n",
      "Gradient Descent(20/49): loss=2.44846394969672e+25\n",
      "Gradient Descent(21/49): loss=7.17146566457034e+26\n",
      "Gradient Descent(22/49): loss=2.100497325459011e+28\n",
      "Gradient Descent(23/49): loss=6.1522835367660436e+29\n",
      "Gradient Descent(24/49): loss=1.8019824285419743e+31\n",
      "Gradient Descent(25/49): loss=5.277943796590512e+32\n",
      "Gradient Descent(26/49): loss=1.5458913626870325e+34\n",
      "Gradient Descent(27/49): loss=4.5278619805958136e+35\n",
      "Gradient Descent(28/49): loss=1.3261950102166245e+37\n",
      "Gradient Descent(29/49): loss=3.884379013010617e+38\n",
      "Gradient Descent(30/49): loss=1.137721089317969e+40\n",
      "Gradient Descent(31/49): loss=3.3323454604797374e+41\n",
      "Gradient Descent(32/49): loss=9.760323837045776e+42\n",
      "Gradient Descent(33/49): loss=2.858764870983439e+44\n",
      "Gradient Descent(34/49): loss=8.373222778274376e+45\n",
      "Gradient Descent(35/49): loss=2.452487800107043e+47\n",
      "Gradient Descent(36/49): loss=7.183251382346875e+48\n",
      "Gradient Descent(37/49): loss=2.103949321164271e+50\n",
      "Gradient Descent(38/49): loss=6.162394312004901e+51\n",
      "Gradient Descent(39/49): loss=1.8049438394084225e+53\n",
      "Gradient Descent(40/49): loss=5.286617665915995e+54\n",
      "Gradient Descent(41/49): loss=1.5484319088141133e+56\n",
      "Gradient Descent(42/49): loss=4.5353031517521196e+57\n",
      "Gradient Descent(43/49): loss=1.3283745033416374e+59\n",
      "Gradient Descent(44/49): loss=3.8907626724939234e+60\n",
      "Gradient Descent(45/49): loss=1.139590840955729e+62\n",
      "Gradient Descent(46/49): loss=3.337821897930717e+63\n",
      "Gradient Descent(47/49): loss=9.776364131675536e+64\n",
      "Gradient Descent(48/49): loss=2.863463017435675e+66\n",
      "Gradient Descent(49/49): loss=8.38698348566583e+67\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8020291784098609\n",
      "Gradient Descent(2/49): loss=2.6997153473105624\n",
      "Gradient Descent(3/49): loss=14.80972695207099\n",
      "Gradient Descent(4/49): loss=125.76652949366743\n",
      "Gradient Descent(5/49): loss=2033.203993522827\n",
      "Gradient Descent(6/49): loss=50995.27120355175\n",
      "Gradient Descent(7/49): loss=1464169.6719198872\n",
      "Gradient Descent(8/49): loss=43264623.69151328\n",
      "Gradient Descent(9/49): loss=1285516132.727757\n",
      "Gradient Descent(10/49): loss=38236273657.45964\n",
      "Gradient Descent(11/49): loss=1137519410521.052\n",
      "Gradient Descent(12/49): loss=33842160279578.51\n",
      "Gradient Descent(13/49): loss=1006839733027054.4\n",
      "Gradient Descent(14/49): loss=2.995457610435432e+16\n",
      "Gradient Descent(15/49): loss=8.911814057085537e+17\n",
      "Gradient Descent(16/49): loss=2.6513622877416845e+19\n",
      "Gradient Descent(17/49): loss=7.888093261750012e+20\n",
      "Gradient Descent(18/49): loss=2.3467941629220734e+22\n",
      "Gradient Descent(19/49): loss=6.981969738495315e+23\n",
      "Gradient Descent(20/49): loss=2.0772124885186558e+25\n",
      "Gradient Descent(21/49): loss=6.179934723455191e+26\n",
      "Gradient Descent(22/49): loss=1.8385982848331512e+28\n",
      "Gradient Descent(23/49): loss=5.470031325998664e+29\n",
      "Gradient Descent(24/49): loss=1.6273942467059106e+31\n",
      "Gradient Descent(25/49): loss=4.841676173999005e+32\n",
      "Gradient Descent(26/49): loss=1.4404517050074862e+34\n",
      "Gradient Descent(27/49): loss=4.2855016318557e+35\n",
      "Gradient Descent(28/49): loss=1.2749836855198725e+37\n",
      "Gradient Descent(29/49): loss=3.7932161459426495e+38\n",
      "Gradient Descent(30/49): loss=1.1285233602007e+40\n",
      "Gradient Descent(31/49): loss=3.3574806325786947e+41\n",
      "Gradient Descent(32/49): loss=9.988872712511788e+42\n",
      "Gradient Descent(33/49): loss=2.9717990656025683e+44\n",
      "Gradient Descent(34/49): loss=8.841427797207031e+45\n",
      "Gradient Descent(35/49): loss=2.630421632405163e+47\n",
      "Gradient Descent(36/49): loss=7.825792533656884e+48\n",
      "Gradient Descent(37/49): loss=2.3282590146524197e+50\n",
      "Gradient Descent(38/49): loss=6.926825642254968e+51\n",
      "Gradient Descent(39/49): loss=2.060806515780394e+53\n",
      "Gradient Descent(40/49): loss=6.131125157208952e+54\n",
      "Gradient Descent(41/49): loss=1.8240769041399282e+56\n",
      "Gradient Descent(42/49): loss=5.426828627539056e+57\n",
      "Gradient Descent(43/49): loss=1.6145409706047543e+59\n",
      "Gradient Descent(44/49): loss=4.8034362694505236e+60\n",
      "Gradient Descent(45/49): loss=1.4290749144650098e+62\n",
      "Gradient Descent(46/49): loss=4.2516544336015986e+63\n",
      "Gradient Descent(47/49): loss=1.264913773224473e+65\n",
      "Gradient Descent(48/49): loss=3.7632570536491385e+66\n",
      "Gradient Descent(49/49): loss=1.1196102020249484e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8178274257020415\n",
      "Gradient Descent(2/49): loss=2.884164778979026\n",
      "Gradient Descent(3/49): loss=17.720921586627796\n",
      "Gradient Descent(4/49): loss=194.96247740941257\n",
      "Gradient Descent(5/49): loss=4062.6424646309306\n",
      "Gradient Descent(6/49): loss=114263.62380955771\n",
      "Gradient Descent(7/49): loss=3462343.8636319735\n",
      "Gradient Descent(8/49): loss=106468321.96598601\n",
      "Gradient Descent(9/49): loss=3282969193.2963524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=101282618529.96408\n",
      "Gradient Descent(11/49): loss=3124957041716.5317\n",
      "Gradient Descent(12/49): loss=96418588092425.58\n",
      "Gradient Descent(13/49): loss=2974944625556491.5\n",
      "Gradient Descent(14/49): loss=9.179040044240029e+16\n",
      "Gradient Descent(15/49): loss=2.8321463436839813e+18\n",
      "Gradient Descent(16/49): loss=8.738444365860941e+19\n",
      "Gradient Descent(17/49): loss=2.6962028425518514e+21\n",
      "Gradient Descent(18/49): loss=8.318997602893683e+22\n",
      "Gradient Descent(19/49): loss=2.5667846659633413e+24\n",
      "Gradient Descent(20/49): loss=7.919684361118326e+25\n",
      "Gradient Descent(21/49): loss=2.4435785834211453e+27\n",
      "Gradient Descent(22/49): loss=7.539538220329216e+28\n",
      "Gradient Descent(23/49): loss=2.3262864129471802e+30\n",
      "Gradient Descent(24/49): loss=7.177639156296399e+31\n",
      "Gradient Descent(25/49): loss=2.2146242857830663e+33\n",
      "Gradient Descent(26/49): loss=6.833111306351656e+34\n",
      "Gradient Descent(27/49): loss=2.108321958931367e+36\n",
      "Gradient Descent(28/49): loss=6.505120849385587e+37\n",
      "Gradient Descent(29/49): loss=2.0071221611029353e+39\n",
      "Gradient Descent(30/49): loss=6.1928739878385e+40\n",
      "Gradient Descent(31/49): loss=1.9107799700727706e+42\n",
      "Gradient Descent(32/49): loss=5.89561502656307e+43\n",
      "Gradient Descent(33/49): loss=1.8190622199223126e+45\n",
      "Gradient Descent(34/49): loss=5.612624543902236e+46\n",
      "Gradient Descent(35/49): loss=1.7317469367353332e+48\n",
      "Gradient Descent(36/49): loss=5.343217650556052e+49\n",
      "Gradient Descent(37/49): loss=1.6486228013796858e+51\n",
      "Gradient Descent(38/49): loss=5.086742332021948e+52\n",
      "Gradient Descent(39/49): loss=1.569488638075936e+54\n",
      "Gradient Descent(40/49): loss=4.842577870600237e+55\n",
      "Gradient Descent(41/49): loss=1.4941529275150134e+57\n",
      "Gradient Descent(42/49): loss=4.610133343142232e+58\n",
      "Gradient Descent(43/49): loss=1.4224333433458273e+60\n",
      "Gradient Descent(44/49): loss=4.3888461909063597e+61\n",
      "Gradient Descent(45/49): loss=1.3541563109119387e+63\n",
      "Gradient Descent(46/49): loss=4.1781808580628025e+64\n",
      "Gradient Descent(47/49): loss=1.2891565871687274e+66\n",
      "Gradient Descent(48/49): loss=3.977627495548591e+67\n",
      "Gradient Descent(49/49): loss=1.227276860764573e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8224362843699627\n",
      "Gradient Descent(2/49): loss=2.9433848027494194\n",
      "Gradient Descent(3/49): loss=17.875574248237466\n",
      "Gradient Descent(4/49): loss=182.6026447659509\n",
      "Gradient Descent(5/49): loss=3482.984202248614\n",
      "Gradient Descent(6/49): loss=92895.6106408915\n",
      "Gradient Descent(7/49): loss=2716443.4204710736\n",
      "Gradient Descent(8/49): loss=80982391.27235475\n",
      "Gradient Descent(9/49): loss=2423405735.4642\n",
      "Gradient Descent(10/49): loss=72573865627.88322\n",
      "Gradient Descent(11/49): loss=2173681433154.4956\n",
      "Gradient Descent(12/49): loss=65106358709243.234\n",
      "Gradient Descent(13/49): loss=1950083502659800.5\n",
      "Gradient Descent(14/49): loss=5.840949506725718e+16\n",
      "Gradient Descent(15/49): loss=1.7494993313854618e+18\n",
      "Gradient Descent(16/49): loss=5.240154914856813e+19\n",
      "Gradient Descent(17/49): loss=1.5695475408240918e+21\n",
      "Gradient Descent(18/49): loss=4.701157745162431e+22\n",
      "Gradient Descent(19/49): loss=1.408105430136532e+24\n",
      "Gradient Descent(20/49): loss=4.217601301561106e+25\n",
      "Gradient Descent(21/49): loss=1.2632690960779459e+27\n",
      "Gradient Descent(22/49): loss=3.783782996553167e+28\n",
      "Gradient Descent(23/49): loss=1.133330484333045e+30\n",
      "Gradient Descent(24/49): loss=3.3945868140132003e+31\n",
      "Gradient Descent(25/49): loss=1.0167572298784524e+33\n",
      "Gradient Descent(26/49): loss=3.04542296647854e+34\n",
      "Gradient Descent(27/49): loss=9.12174585260985e+35\n",
      "Gradient Descent(28/49): loss=2.7321737675019865e+37\n",
      "Gradient Descent(29/49): loss=8.183492081935184e+38\n",
      "Gradient Descent(30/49): loss=2.451145071798485e+40\n",
      "Gradient Descent(31/49): loss=7.341746167586336e+41\n",
      "Gradient Descent(32/49): loss=2.1990227102192208e+43\n",
      "Gradient Descent(33/49): loss=6.586581406763302e+44\n",
      "Gradient Descent(34/49): loss=1.9728334057811532e+46\n",
      "Gradient Descent(35/49): loss=5.909092147513019e+47\n",
      "Gradient Descent(36/49): loss=1.7699097098355744e+49\n",
      "Gradient Descent(37/49): loss=5.301288764448721e+50\n",
      "Gradient Descent(38/49): loss=1.5878585448678568e+52\n",
      "Gradient Descent(39/49): loss=4.7560034371606114e+53\n",
      "Gradient Descent(40/49): loss=1.4245329829531838e+55\n",
      "Gradient Descent(41/49): loss=4.266805620167971e+56\n",
      "Gradient Descent(42/49): loss=1.2780069270530548e+58\n",
      "Gradient Descent(43/49): loss=3.827926207548466e+59\n",
      "Gradient Descent(44/49): loss=1.1465523965683575e+61\n",
      "Gradient Descent(45/49): loss=3.434189497917527e+62\n",
      "Gradient Descent(46/49): loss=1.0286191492779285e+64\n",
      "Gradient Descent(47/49): loss=3.0809521574241933e+65\n",
      "Gradient Descent(48/49): loss=9.228163993447047e+66\n",
      "Gradient Descent(49/49): loss=2.7640484609519996e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8304066353967496\n",
      "Gradient Descent(2/49): loss=2.976070972436291\n",
      "Gradient Descent(3/49): loss=17.76427989826287\n",
      "Gradient Descent(4/49): loss=174.26766568999562\n",
      "Gradient Descent(5/49): loss=3212.9885313110917\n",
      "Gradient Descent(6/49): loss=84756.19621101786\n",
      "Gradient Descent(7/49): loss=2473175.9729293007\n",
      "Gradient Descent(8/49): loss=73717380.7497003\n",
      "Gradient Descent(9/49): loss=2206438475.9608936\n",
      "Gradient Descent(10/49): loss=66094023630.4767\n",
      "Gradient Descent(11/49): loss=1980156665248.312\n",
      "Gradient Descent(12/49): loss=59326644431754.25\n",
      "Gradient Descent(13/49): loss=1777470866146651.8\n",
      "Gradient Descent(14/49): loss=5.325442163375643e+16\n",
      "Gradient Descent(15/49): loss=1.5955445873694188e+18\n",
      "Gradient Descent(16/49): loss=4.780377957350437e+19\n",
      "Gradient Descent(17/49): loss=1.4322391096638657e+21\n",
      "Gradient Descent(18/49): loss=4.291101851380673e+22\n",
      "Gradient Descent(19/49): loss=1.2856481141977693e+24\n",
      "Gradient Descent(20/49): loss=3.851903615616589e+25\n",
      "Gradient Descent(21/49): loss=1.1540608429558452e+27\n",
      "Gradient Descent(22/49): loss=3.457657724993451e+28\n",
      "Gradient Descent(23/49): loss=1.035941650406438e+30\n",
      "Gradient Descent(24/49): loss=3.103763265199773e+31\n",
      "Gradient Descent(25/49): loss=9.299120662466226e+32\n",
      "Gradient Descent(26/49): loss=2.786090230033721e+34\n",
      "Gradient Descent(27/49): loss=8.347347078977216e+35\n",
      "Gradient Descent(28/49): loss=2.5009313232495655e+37\n",
      "Gradient Descent(29/49): loss=7.492988400306465e+38\n",
      "Gradient Descent(30/49): loss=2.24495869379476e+40\n",
      "Gradient Descent(31/49): loss=6.7260741210256956e+41\n",
      "Gradient Descent(32/49): loss=2.0151850992438114e+43\n",
      "Gradient Descent(33/49): loss=6.037654226140367e+44\n",
      "Gradient Descent(34/49): loss=1.808929044190984e+46\n",
      "Gradient Descent(35/49): loss=5.419694742952403e+47\n",
      "Gradient Descent(36/49): loss=1.6237834867603675e+49\n",
      "Gradient Descent(37/49): loss=4.8649839832848124e+50\n",
      "Gradient Descent(38/49): loss=1.4575877480339567e+52\n",
      "Gradient Descent(39/49): loss=4.367048381902917e+53\n",
      "Gradient Descent(40/49): loss=1.3084022965756083e+55\n",
      "Gradient Descent(41/49): loss=3.920076948949175e+56\n",
      "Gradient Descent(42/49): loss=1.1744861137817555e+58\n",
      "Gradient Descent(43/49): loss=3.518853454741404e+59\n",
      "Gradient Descent(44/49): loss=1.0542763759100848e+61\n",
      "Gradient Descent(45/49): loss=3.1586955555209107e+62\n",
      "Gradient Descent(46/49): loss=9.463702156708906e+63\n",
      "Gradient Descent(47/49): loss=2.8354001497344695e+65\n",
      "Gradient Descent(48/49): loss=8.495083505364805e+66\n",
      "Gradient Descent(49/49): loss=2.5451943271526877e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8137949443633886\n",
      "Gradient Descent(2/49): loss=2.8295431580266954\n",
      "Gradient Descent(3/49): loss=16.01021386862619\n",
      "Gradient Descent(4/49): loss=139.06149644601015\n",
      "Gradient Descent(5/49): loss=2286.837792013362\n",
      "Gradient Descent(6/49): loss=58480.30903266448\n",
      "Gradient Descent(7/49): loss=1715929.7145936152\n",
      "Gradient Descent(8/49): loss=51853259.497930646\n",
      "Gradient Descent(9/49): loss=1575896844.4497843\n",
      "Gradient Descent(10/49): loss=47945598949.2828\n",
      "Gradient Descent(11/49): loss=1459010148200.1619\n",
      "Gradient Descent(12/49): loss=44400166037950.22\n",
      "Gradient Descent(13/49): loss=1351182554964849.8\n",
      "Gradient Descent(14/49): loss=4.11191434796879e+16\n",
      "Gradient Descent(15/49): loss=1.2513367577354844e+18\n",
      "Gradient Descent(16/49): loss=3.808065112951679e+19\n",
      "Gradient Descent(17/49): loss=1.1588695008114797e+21\n",
      "Gradient Descent(18/49): loss=3.5266690101312733e+22\n",
      "Gradient Descent(19/49): loss=1.0732351056250813e+24\n",
      "Gradient Descent(20/49): loss=3.2660666161419145e+25\n",
      "Gradient Descent(21/49): loss=9.93928644832037e+26\n",
      "Gradient Descent(22/49): loss=3.0247213762797783e+28\n",
      "Gradient Descent(23/49): loss=9.204825166975288e+29\n",
      "Gradient Descent(24/49): loss=2.8012102873025745e+31\n",
      "Gradient Descent(25/49): loss=8.52463673274543e+32\n",
      "Gradient Descent(26/49): loss=2.594215498731799e+34\n",
      "Gradient Descent(27/49): loss=7.894710666096615e+35\n",
      "Gradient Descent(28/49): loss=2.402516542355391e+37\n",
      "Gradient Descent(29/49): loss=7.311332840960525e+38\n",
      "Gradient Descent(30/49): loss=2.2249831361785534e+40\n",
      "Gradient Descent(31/49): loss=6.771063585758675e+41\n",
      "Gradient Descent(32/49): loss=2.0605685201340886e+43\n",
      "Gradient Descent(33/49): loss=6.270717402651137e+44\n",
      "Gradient Descent(34/49): loss=1.9083032842486223e+46\n",
      "Gradient Descent(35/49): loss=5.807344185426785e+47\n",
      "Gradient Descent(36/49): loss=1.7672896528755646e+49\n",
      "Gradient Descent(37/49): loss=5.378211825293414e+50\n",
      "Gradient Descent(38/49): loss=1.6366961912927427e+52\n",
      "Gradient Descent(39/49): loss=4.980790102007756e+53\n",
      "Gradient Descent(40/49): loss=1.51575290345504e+55\n",
      "Gradient Descent(41/49): loss=4.612735765368469e+56\n",
      "Gradient Descent(42/49): loss=1.4037466919976775e+58\n",
      "Gradient Descent(43/49): loss=4.2718787191075945e+59\n",
      "Gradient Descent(44/49): loss=1.300017153721267e+61\n",
      "Gradient Descent(45/49): loss=3.9562092257212196e+62\n",
      "Gradient Descent(46/49): loss=1.2039526857687368e+64\n",
      "Gradient Descent(47/49): loss=3.6638660567945607e+65\n",
      "Gradient Descent(48/49): loss=1.1149868795350532e+67\n",
      "Gradient Descent(49/49): loss=3.393125518957868e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8298861930982351\n",
      "Gradient Descent(2/49): loss=3.0220250316982677\n",
      "Gradient Descent(3/49): loss=19.12187808601308\n",
      "Gradient Descent(4/49): loss=214.69373031493296\n",
      "Gradient Descent(5/49): loss=4554.153555301034\n",
      "Gradient Descent(6/49): loss=130740.74882675952\n",
      "Gradient Descent(7/49): loss=4049273.6425247053\n",
      "Gradient Descent(8/49): loss=127317825.74093838\n",
      "Gradient Descent(9/49): loss=4014514505.285731\n",
      "Gradient Descent(10/49): loss=126650293043.33588\n",
      "Gradient Descent(11/49): loss=3995967870090.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=126079852984412.19\n",
      "Gradient Descent(13/49): loss=3978055799473716.5\n",
      "Gradient Descent(14/49): loss=1.2551519948370837e+17\n",
      "Gradient Descent(15/49): loss=3.9602428765066665e+18\n",
      "Gradient Descent(16/49): loss=1.2495318531802733e+20\n",
      "Gradient Descent(17/49): loss=3.9425103638691227e+21\n",
      "Gradient Descent(18/49): loss=1.2439369145568132e+23\n",
      "Gradient Descent(19/49): loss=3.9248572731558176e+24\n",
      "Gradient Descent(20/49): loss=1.2383670292902742e+26\n",
      "Gradient Descent(21/49): loss=3.9072832271617473e+27\n",
      "Gradient Descent(22/49): loss=1.2328220839360179e+29\n",
      "Gradient Descent(23/49): loss=3.8897878712125934e+30\n",
      "Gradient Descent(24/49): loss=1.227301966779059e+32\n",
      "Gradient Descent(25/49): loss=3.872370852938438e+33\n",
      "Gradient Descent(26/49): loss=1.2218065666464128e+35\n",
      "Gradient Descent(27/49): loss=3.855031821571383e+36\n",
      "Gradient Descent(28/49): loss=1.2163357728645231e+38\n",
      "Gradient Descent(29/49): loss=3.837770427915308e+39\n",
      "Gradient Descent(30/49): loss=1.2108894752552437e+41\n",
      "Gradient Descent(31/49): loss=3.820586324337381e+42\n",
      "Gradient Descent(32/49): loss=1.2054675641339427e+44\n",
      "Gradient Descent(33/49): loss=3.803479164761552e+45\n",
      "Gradient Descent(34/49): loss=1.2000699303069496e+47\n",
      "Gradient Descent(35/49): loss=3.7864486046612906e+48\n",
      "Gradient Descent(36/49): loss=1.1946964650696636e+50\n",
      "Gradient Descent(37/49): loss=3.769494301052637e+51\n",
      "Gradient Descent(38/49): loss=1.189347060204128e+53\n",
      "Gradient Descent(39/49): loss=3.752615912487718e+54\n",
      "Gradient Descent(40/49): loss=1.1840216079770133e+56\n",
      "Gradient Descent(41/49): loss=3.7358130990472194e+57\n",
      "Gradient Descent(42/49): loss=1.1787200011373218e+59\n",
      "Gradient Descent(43/49): loss=3.719085522333734e+60\n",
      "Gradient Descent(44/49): loss=1.173442132914252e+62\n",
      "Gradient Descent(45/49): loss=3.7024328454655036e+63\n",
      "Gradient Descent(46/49): loss=1.16818789701522e+65\n",
      "Gradient Descent(47/49): loss=3.685854733068879e+66\n",
      "Gradient Descent(48/49): loss=1.1629571876234678e+68\n",
      "Gradient Descent(49/49): loss=3.66935085127195e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8346133168623937\n",
      "Gradient Descent(2/49): loss=3.0843660261289885\n",
      "Gradient Descent(3/49): loss=19.294167723652645\n",
      "Gradient Descent(4/49): loss=201.27125089218808\n",
      "Gradient Descent(5/49): loss=3907.4819796051816\n",
      "Gradient Descent(6/49): loss=106343.23776054247\n",
      "Gradient Descent(7/49): loss=3178212.711886985\n",
      "Gradient Descent(8/49): loss=96881484.76362056\n",
      "Gradient Descent(9/49): loss=2964775028.305058\n",
      "Gradient Descent(10/49): loss=90797098764.09047\n",
      "Gradient Descent(11/49): loss=2781096492446.4185\n",
      "Gradient Descent(12/49): loss=85186842414580.62\n",
      "Gradient Descent(13/49): loss=2609344273404227.0\n",
      "Gradient Descent(14/49): loss=7.99264843420566e+16\n",
      "Gradient Descent(15/49): loss=2.4482182324042394e+18\n",
      "Gradient Descent(16/49): loss=7.499107215643091e+19\n",
      "Gradient Descent(17/49): loss=2.2970423439242004e+21\n",
      "Gradient Descent(18/49): loss=7.036042272797801e+22\n",
      "Gradient Descent(19/49): loss=2.1552014924773004e+24\n",
      "Gradient Descent(20/49): loss=6.6015713001009915e+25\n",
      "Gradient Descent(21/49): loss=2.02211922100615e+27\n",
      "Gradient Descent(22/49): loss=6.193928624097098e+28\n",
      "Gradient Descent(23/49): loss=1.89725469210146e+30\n",
      "Gradient Descent(24/49): loss=5.811457614634855e+31\n",
      "Gradient Descent(25/49): loss=1.780100465546288e+33\n",
      "Gradient Descent(26/49): loss=5.452603937880334e+34\n",
      "Gradient Descent(27/49): loss=1.6701804352521642e+36\n",
      "Gradient Descent(28/49): loss=5.115909239106673e+37\n",
      "Gradient Descent(29/49): loss=1.567047894368733e+39\n",
      "Gradient Descent(30/49): loss=4.800005216031333e+40\n",
      "Gradient Descent(31/49): loss=1.4702837198992596e+42\n",
      "Gradient Descent(32/49): loss=4.503608058134885e+43\n",
      "Gradient Descent(33/49): loss=1.3794946694157293e+45\n",
      "Gradient Descent(34/49): loss=4.22551322935162e+46\n",
      "Gradient Descent(35/49): loss=1.2943117829508233e+48\n",
      "Gradient Descent(36/49): loss=3.964590572923986e+49\n",
      "Gradient Descent(37/49): loss=1.2143888835720192e+51\n",
      "Gradient Descent(38/49): loss=3.7197797185287607e+52\n",
      "Gradient Descent(39/49): loss=1.1394011705441727e+54\n",
      "Gradient Descent(40/49): loss=3.4900857730115817e+55\n",
      "Gradient Descent(41/49): loss=1.0690438993634318e+57\n",
      "Gradient Descent(42/49): loss=3.2745752772144267e+58\n",
      "Gradient Descent(43/49): loss=1.003031143298125e+60\n",
      "Gradient Descent(44/49): loss=3.072372412466914e+61\n",
      "Gradient Descent(45/49): loss=9.410946314038885e+62\n",
      "Gradient Descent(46/49): loss=2.882655441324181e+64\n",
      "Gradient Descent(47/49): loss=8.829826582901472e+65\n",
      "Gradient Descent(48/49): loss=2.7046533680868426e+67\n",
      "Gradient Descent(49/49): loss=8.284590612084135e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8428534597908058\n",
      "Gradient Descent(2/49): loss=3.119098984612665\n",
      "Gradient Descent(3/49): loss=19.178714670011043\n",
      "Gradient Descent(4/49): loss=192.17601279557866\n",
      "Gradient Descent(5/49): loss=3605.5871840731447\n",
      "Gradient Descent(6/49): loss=97033.01529142183\n",
      "Gradient Descent(7/49): loss=2893634.5744565297\n",
      "Gradient Descent(8/49): loss=88190146.13174258\n",
      "Gradient Descent(9/49): loss=2699329349.0061736\n",
      "Gradient Descent(10/49): loss=82689784832.06207\n",
      "Gradient Descent(11/49): loss=2533479535873.191\n",
      "Gradient Descent(12/49): loss=77624061642131.2\n",
      "Gradient Descent(13/49): loss=2378361775487823.0\n",
      "Gradient Descent(14/49): loss=7.287187924111664e+16\n",
      "Gradient Descent(15/49): loss=2.2327603892748413e+18\n",
      "Gradient Descent(16/49): loss=6.841073980135123e+19\n",
      "Gradient Descent(17/49): loss=2.096073264896634e+21\n",
      "Gradient Descent(18/49): loss=6.422271052154993e+22\n",
      "Gradient Descent(19/49): loss=1.9677539984312983e+24\n",
      "Gradient Descent(20/49): loss=6.029106786567698e+25\n",
      "Gradient Descent(21/49): loss=1.847290295089336e+27\n",
      "Gradient Descent(22/49): loss=5.660011598967611e+28\n",
      "Gradient Descent(23/49): loss=1.734201245230508e+30\n",
      "Gradient Descent(24/49): loss=5.31351200677376e+31\n",
      "Gradient Descent(25/49): loss=1.6280353807713157e+33\n",
      "Gradient Descent(26/49): loss=4.988224732840098e+34\n",
      "Gradient Descent(27/49): loss=1.528368872028392e+36\n",
      "Gradient Descent(28/49): loss=4.6828511827199975e+37\n",
      "Gradient Descent(29/49): loss=1.4348038356996033e+39\n",
      "Gradient Descent(30/49): loss=4.396172260470014e+40\n",
      "Gradient Descent(31/49): loss=1.3469667464544352e+42\n",
      "Gradient Descent(32/49): loss=4.12704350183951e+43\n",
      "Gradient Descent(33/49): loss=1.2645069457661028e+45\n",
      "Gradient Descent(34/49): loss=3.8743905054018165e+46\n",
      "Gradient Descent(35/49): loss=1.1870952420315277e+48\n",
      "Gradient Descent(36/49): loss=3.6372046433861527e+49\n",
      "Gradient Descent(37/49): loss=1.1144225963900248e+51\n",
      "Gradient Descent(38/49): loss=3.4145390350884553e+52\n",
      "Gradient Descent(39/49): loss=1.0461988889950868e+54\n",
      "Gradient Descent(40/49): loss=3.2055047667837676e+55\n",
      "Gradient Descent(41/49): loss=9.82151760813216e+56\n",
      "Gradient Descent(42/49): loss=3.0092673430535464e+58\n",
      "Gradient Descent(43/49): loss=9.220255263270587e+59\n",
      "Gradient Descent(44/49): loss=2.8250433553572896e+61\n",
      "Gradient Descent(45/49): loss=8.655801528012629e+62\n",
      "Gradient Descent(46/49): loss=2.6520973545508826e+64\n",
      "Gradient Descent(47/49): loss=8.12590302036506e+65\n",
      "Gradient Descent(48/49): loss=2.4897389148657277e+67\n",
      "Gradient Descent(49/49): loss=7.62844430786482e+68\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8257295034452602\n",
      "Gradient Descent(2/49): loss=2.9647480996635935\n",
      "Gradient Descent(3/49): loss=17.29303274026616\n",
      "Gradient Descent(4/49): loss=153.58438288044633\n",
      "Gradient Descent(5/49): loss=2568.7879724192376\n",
      "Gradient Descent(6/49): loss=66962.29860611887\n",
      "Gradient Descent(7/49): loss=2007422.339270988\n",
      "Gradient Descent(8/49): loss=62021168.04682037\n",
      "Gradient Descent(9/49): loss=1927474524.2479749\n",
      "Gradient Descent(10/49): loss=59968410375.69725\n",
      "Gradient Descent(11/49): loss=1866158172945.9202\n",
      "Gradient Descent(12/49): loss=58075345985064.92\n",
      "Gradient Descent(13/49): loss=1807334193489659.8\n",
      "Gradient Descent(14/49): loss=5.624523696674225e+16\n",
      "Gradient Descent(15/49): loss=1.7503832756389985e+18\n",
      "Gradient Descent(16/49): loss=5.447291069401218e+19\n",
      "Gradient Descent(17/49): loss=1.695227593798184e+21\n",
      "Gradient Descent(18/49): loss=5.275643551084181e+22\n",
      "Gradient Descent(19/49): loss=1.6418099250932723e+24\n",
      "Gradient Descent(20/49): loss=5.109404765909106e+25\n",
      "Gradient Descent(21/49): loss=1.5900754809203737e+27\n",
      "Gradient Descent(22/49): loss=4.948404267948558e+28\n",
      "Gradient Descent(23/49): loss=1.5399712210448062e+30\n",
      "Gradient Descent(24/49): loss=4.79247699507282e+31\n",
      "Gradient Descent(25/49): loss=1.4914457773256485e+33\n",
      "Gradient Descent(26/49): loss=4.641463086811764e+34\n",
      "Gradient Descent(27/49): loss=1.4444494002903638e+36\n",
      "Gradient Descent(28/49): loss=4.495207720013048e+37\n",
      "Gradient Descent(29/49): loss=1.3989339081038726e+39\n",
      "Gradient Descent(30/49): loss=4.353560950098018e+40\n",
      "Gradient Descent(31/49): loss=1.3548526371705146e+42\n",
      "Gradient Descent(32/49): loss=4.2163775573341134e+43\n",
      "Gradient Descent(33/49): loss=1.3121603942933625e+45\n",
      "Gradient Descent(34/49): loss=4.083516897952274e+46\n",
      "Gradient Descent(35/49): loss=1.2708134103409331e+48\n",
      "Gradient Descent(36/49): loss=3.9548427599556836e+49\n",
      "Gradient Descent(37/49): loss=1.2307692953741965e+51\n",
      "Gradient Descent(38/49): loss=3.830223223471066e+52\n",
      "Gradient Descent(39/49): loss=1.1919869951871813e+54\n",
      "Gradient Descent(40/49): loss=3.709530525502298e+55\n",
      "Gradient Descent(41/49): loss=1.1544267492173717e+57\n",
      "Gradient Descent(42/49): loss=3.592640928943837e+58\n",
      "Gradient Descent(43/49): loss=1.1180500497820554e+60\n",
      "Gradient Descent(44/49): loss=3.4794345957226445e+61\n",
      "Gradient Descent(45/49): loss=1.0828196025993416e+63\n",
      "Gradient Descent(46/49): loss=3.3697954639377273e+64\n",
      "Gradient Descent(47/49): loss=1.0486992885533297e+66\n",
      "Gradient Descent(48/49): loss=3.2636111288698763e+67\n",
      "Gradient Descent(49/49): loss=1.0156541266635945e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8421153725610769\n",
      "Gradient Descent(2/49): loss=3.1655053104412803\n",
      "Gradient Descent(3/49): loss=20.616365133686756\n",
      "Gradient Descent(4/49): loss=236.16930650848963\n",
      "Gradient Descent(5/49): loss=5098.77415276538\n",
      "Gradient Descent(6/49): loss=149372.50458936702\n",
      "Gradient Descent(7/49): loss=4727520.61940988\n",
      "Gradient Descent(8/49): loss=151949708.06972203\n",
      "Gradient Descent(9/49): loss=4898168570.337028\n",
      "Gradient Descent(10/49): loss=157980918470.2836\n",
      "Gradient Descent(11/49): loss=5095886984514.214\n",
      "Gradient Descent(12/49): loss=164377812502338.9\n",
      "Gradient Descent(13/49): loss=5302347001205074.0\n",
      "Gradient Descent(14/49): loss=1.7103830435434986e+17\n",
      "Gradient Descent(15/49): loss=5.517199300806515e+18\n",
      "Gradient Descent(16/49): loss=1.779688411947707e+20\n",
      "Gradient Descent(17/49): loss=5.74075846193754e+21\n",
      "Gradient Descent(18/49): loss=1.8518021201620846e+23\n",
      "Gradient Descent(19/49): loss=5.973376367401258e+24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/49): loss=1.9268379077403732e+26\n",
      "Gradient Descent(21/49): loss=6.215420047841246e+27\n",
      "Gradient Descent(22/49): loss=2.004914176535353e+29\n",
      "Gradient Descent(23/49): loss=6.467271438346761e+30\n",
      "Gradient Descent(24/49): loss=2.086154128030309e+32\n",
      "Gradient Descent(25/49): loss=6.729327951341643e+33\n",
      "Gradient Descent(26/49): loss=2.1706859559538997e+35\n",
      "Gradient Descent(27/49): loss=7.002003102607348e+36\n",
      "Gradient Descent(28/49): loss=2.2586430485001772e+38\n",
      "Gradient Descent(29/49): loss=7.285727163757498e+39\n",
      "Gradient Descent(30/49): loss=2.350164198807912e+41\n",
      "Gradient Descent(31/49): loss=7.580947841189702e+42\n",
      "Gradient Descent(32/49): loss=2.4453938239715467e+44\n",
      "Gradient Descent(33/49): loss=7.888130982549529e+45\n",
      "Gradient Descent(34/49): loss=2.5444821929255938e+47\n",
      "Gradient Descent(35/49): loss=8.207761311821902e+48\n",
      "Gradient Descent(36/49): loss=2.647585663564153e+50\n",
      "Gradient Descent(37/49): loss=8.540343194208349e+51\n",
      "Gradient Descent(38/49): loss=2.7548669294678897e+53\n",
      "Gradient Descent(39/49): loss=8.886401431996636e+54\n",
      "Gradient Descent(40/49): loss=2.8664952766283196e+56\n",
      "Gradient Descent(41/49): loss=9.246482092680416e+57\n",
      "Gradient Descent(42/49): loss=2.982646850575829e+59\n",
      "Gradient Descent(43/49): loss=9.621153370634087e+60\n",
      "Gradient Descent(44/49): loss=3.103504934330115e+62\n",
      "Gradient Descent(45/49): loss=1.0011006483702612e+64\n",
      "Gradient Descent(46/49): loss=3.229260237614785e+65\n",
      "Gradient Descent(47/49): loss=1.041665660612249e+67\n",
      "Gradient Descent(48/49): loss=3.3601111977901353e+68\n",
      "Gradient Descent(49/49): loss=1.0838743839245454e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8469621646517402\n",
      "Gradient Descent(2/49): loss=3.2310857309705736\n",
      "Gradient Descent(3/49): loss=20.807728808029353\n",
      "Gradient Descent(4/49): loss=221.60654246778628\n",
      "Gradient Descent(5/49): loss=4378.211060217948\n",
      "Gradient Descent(6/49): loss=121556.51495422916\n",
      "Gradient Descent(7/49): loss=3712028.6622432014\n",
      "Gradient Descent(8/49): loss=115672200.13201545\n",
      "Gradient Descent(9/49): loss=3618990843.5281897\n",
      "Gradient Descent(10/49): loss=113314635175.49078\n",
      "Gradient Descent(11/49): loss=3548548580672.549\n",
      "Gradient Descent(12/49): loss=111129253825562.97\n",
      "Gradient Descent(13/49): loss=3480234844326298.0\n",
      "Gradient Descent(14/49): loss=1.0899063727383552e+17\n",
      "Gradient Descent(15/49): loss=3.413264375880256e+18\n",
      "Gradient Descent(16/49): loss=1.0689334870848586e+20\n",
      "Gradient Descent(17/49): loss=3.3475836719888476e+21\n",
      "Gradient Descent(18/49): loss=1.0483642429069726e+23\n",
      "Gradient Descent(19/49): loss=3.2831668864554064e+24\n",
      "Gradient Descent(20/49): loss=1.0281908103892592e+26\n",
      "Gradient Descent(21/49): loss=3.219989659809511e+27\n",
      "Gradient Descent(22/49): loss=1.0084055706915526e+29\n",
      "Gradient Descent(23/49): loss=3.1580281380853055e+30\n",
      "Gradient Descent(24/49): loss=9.89001053822027e+31\n",
      "Gradient Descent(25/49): loss=3.0972589276994735e+33\n",
      "Gradient Descent(26/49): loss=9.699699336154908e+34\n",
      "Gradient Descent(27/49): loss=3.037659085276697e+36\n",
      "Gradient Descent(28/49): loss=9.51305024885634e+37\n",
      "Gradient Descent(29/49): loss=2.979206108937738e+39\n",
      "Gradient Descent(30/49): loss=9.329992807090448e+40\n",
      "Gradient Descent(31/49): loss=2.921877929801759e+42\n",
      "Gradient Descent(32/49): loss=9.150457897646613e+43\n",
      "Gradient Descent(33/49): loss=2.865652903654542e+45\n",
      "Gradient Descent(34/49): loss=8.974377737244936e+46\n",
      "Gradient Descent(35/49): loss=2.8105098027763146e+48\n",
      "Gradient Descent(36/49): loss=8.801685846941906e+49\n",
      "Gradient Descent(37/49): loss=2.756427807927606e+51\n",
      "Gradient Descent(38/49): loss=8.632317027034386e+52\n",
      "Gradient Descent(39/49): loss=2.7033865004885584e+54\n",
      "Gradient Descent(40/49): loss=8.466207332441523e+55\n",
      "Gradient Descent(41/49): loss=2.651365854750452e+57\n",
      "Gradient Descent(42/49): loss=8.303294048563359e+58\n",
      "Gradient Descent(43/49): loss=2.600346230354428e+60\n",
      "Gradient Descent(44/49): loss=8.143515667602365e+61\n",
      "Gradient Descent(45/49): loss=2.5503083648766914e+63\n",
      "Gradient Descent(46/49): loss=7.98681186534169e+64\n",
      "Gradient Descent(47/49): loss=2.501233366555886e+66\n",
      "Gradient Descent(48/49): loss=7.833123478369245e+67\n",
      "Gradient Descent(49/49): loss=2.4531027071602875e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8554757576666189\n",
      "Gradient Descent(2/49): loss=3.2679555684935084\n",
      "Gradient Descent(3/49): loss=20.68812600665121\n",
      "Gradient Descent(4/49): loss=211.69152540197945\n",
      "Gradient Descent(5/49): loss=4041.0677238524318\n",
      "Gradient Descent(6/49): loss=110922.93043554787\n",
      "Gradient Descent(7/49): loss=3379702.7144962805\n",
      "Gradient Descent(8/49): loss=105295135.05088209\n",
      "Gradient Descent(9/49): loss=3294959694.471914\n",
      "Gradient Descent(10/49): loss=103196245749.92673\n",
      "Gradient Descent(11/49): loss=3232584136103.9443\n",
      "Gradient Descent(12/49): loss=101262760186643.28\n",
      "Gradient Descent(13/49): loss=3172140356716278.5\n",
      "Gradient Descent(14/49): loss=9.937006056972213e+16\n",
      "Gradient Descent(15/49): loss=3.112854458451384e+18\n",
      "Gradient Descent(16/49): loss=9.751290543792947e+19\n",
      "Gradient Descent(17/49): loss=3.054677607829611e+21\n",
      "Gradient Descent(18/49): loss=9.569046539831184e+22\n",
      "Gradient Descent(19/49): loss=2.997588075318916e+24\n",
      "Gradient Descent(20/49): loss=9.390208556795799e+25\n",
      "Gradient Descent(21/49): loss=2.941565502852947e+27\n",
      "Gradient Descent(22/49): loss=9.214712916395702e+28\n",
      "Gradient Descent(23/49): loss=2.8865899484228985e+30\n",
      "Gradient Descent(24/49): loss=9.042497152039374e+31\n",
      "Gradient Descent(25/49): loss=2.8326418440317586e+33\n",
      "Gradient Descent(26/49): loss=8.873499965383016e+34\n",
      "Gradient Descent(27/49): loss=2.779701987441507e+36\n",
      "Gradient Descent(28/49): loss=8.707661203729798e+37\n",
      "Gradient Descent(29/49): loss=2.727751535290647e+39\n",
      "Gradient Descent(30/49): loss=8.544921838591396e+40\n",
      "Gradient Descent(31/49): loss=2.6767719963855603e+42\n",
      "Gradient Descent(32/49): loss=8.385223944675409e+43\n",
      "Gradient Descent(33/49): loss=2.6267452251182977e+45\n",
      "Gradient Descent(34/49): loss=8.22851067926814e+46\n",
      "Gradient Descent(35/49): loss=2.5776534150083188e+48\n",
      "Gradient Descent(36/49): loss=8.074726262000756e+49\n",
      "Gradient Descent(37/49): loss=2.5294790923640468e+51\n",
      "Gradient Descent(38/49): loss=7.923815954996448e+52\n",
      "Gradient Descent(39/49): loss=2.4822051100637948e+54\n",
      "Gradient Descent(40/49): loss=7.775726043386396e+55\n",
      "Gradient Descent(41/49): loss=2.4358146414517915e+57\n",
      "Gradient Descent(42/49): loss=7.630403816190604e+58\n",
      "Gradient Descent(43/49): loss=2.3902911743495557e+60\n",
      "Gradient Descent(44/49): loss=7.48779754755623e+61\n",
      "Gradient Descent(45/49): loss=2.3456185051784093e+63\n",
      "Gradient Descent(46/49): loss=7.347856478345832e+64\n",
      "Gradient Descent(47/49): loss=2.3017807331913634e+66\n",
      "Gradient Descent(48/49): loss=7.21053079806999e+67\n",
      "Gradient Descent(49/49): loss=2.25876225481432e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8378328556554758\n",
      "Gradient Descent(2/49): loss=3.1054767937101233\n",
      "Gradient Descent(3/49): loss=18.66265365331103\n",
      "Gradient Descent(4/49): loss=169.43191860697453\n",
      "Gradient Descent(5/49): loss=2881.849945764903\n",
      "Gradient Descent(6/49): loss=76560.78819837779\n",
      "Gradient Descent(7/49): loss=2344370.0224978994\n",
      "Gradient Descent(8/49): loss=74036183.0119686\n",
      "Gradient Descent(9/49): loss=2352242031.1047144\n",
      "Gradient Descent(10/49): loss=74820611622.68137\n",
      "Gradient Descent(11/49): loss=2380433163380.8047\n",
      "Gradient Descent(12/49): loss=75737140847682.88\n",
      "Gradient Descent(13/49): loss=2409712769975830.0\n",
      "Gradient Descent(14/49): loss=7.666944342793008e+16\n",
      "Gradient Descent(15/49): loss=2.4393800474750264e+18\n",
      "Gradient Descent(16/49): loss=7.761338907182167e+19\n",
      "Gradient Descent(17/49): loss=2.4694135841534645e+21\n",
      "Gradient Descent(18/49): loss=7.856896258044967e+22\n",
      "Gradient Descent(19/49): loss=2.4998169293783615e+24\n",
      "Gradient Descent(20/49): loss=7.95363013029228e+25\n",
      "Gradient Descent(21/49): loss=2.530594601025027e+27\n",
      "Gradient Descent(22/49): loss=8.05155498790899e+28\n",
      "Gradient Descent(23/49): loss=2.561751206498839e+30\n",
      "Gradient Descent(24/49): loss=8.150685493490331e+31\n",
      "Gradient Descent(25/49): loss=2.5932914111745537e+33\n",
      "Gradient Descent(26/49): loss=8.251036490909922e+34\n",
      "Gradient Descent(27/49): loss=2.625219937912469e+36\n",
      "Gradient Descent(28/49): loss=8.352623006825822e+37\n",
      "Gradient Descent(29/49): loss=2.6575415677222125e+39\n",
      "Gradient Descent(30/49): loss=8.455460252904627e+40\n",
      "Gradient Descent(31/49): loss=2.6902611404769094e+42\n",
      "Gradient Descent(32/49): loss=8.559563628099074e+43\n",
      "Gradient Descent(33/49): loss=2.7233835556383233e+45\n",
      "Gradient Descent(34/49): loss=8.664948720952908e+46\n",
      "Gradient Descent(35/49): loss=2.7569137729903023e+48\n",
      "Gradient Descent(36/49): loss=8.771631311936607e+49\n",
      "Gradient Descent(37/49): loss=2.790856813381283e+51\n",
      "Gradient Descent(38/49): loss=8.879627375808016e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=2.8252177594761676e+54\n",
      "Gradient Descent(40/49): loss=8.988953084006117e+55\n",
      "Gradient Descent(41/49): loss=2.8600017565176082e+57\n",
      "Gradient Descent(42/49): loss=9.099624807072881e+58\n",
      "Gradient Descent(43/49): loss=2.895214013096178e+60\n",
      "Gradient Descent(44/49): loss=9.211659117102825e+61\n",
      "Gradient Descent(45/49): loss=2.9308598019306936e+63\n",
      "Gradient Descent(46/49): loss=9.325072790226295e+64\n",
      "Gradient Descent(47/49): loss=2.9669444606573467e+66\n",
      "Gradient Descent(48/49): loss=9.43988280912062e+67\n",
      "Gradient Descent(49/49): loss=3.0034733926292133e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8545149640905674\n",
      "Gradient Descent(2/49): loss=3.314756556094155\n",
      "Gradient Descent(3/49): loss=22.209336770987388\n",
      "Gradient Descent(4/49): loss=259.5209043138519\n",
      "Gradient Descent(5/49): loss=5701.568877511441\n",
      "Gradient Descent(6/49): loss=170412.34774484893\n",
      "Gradient Descent(7/49): loss=5510049.60305743\n",
      "Gradient Descent(8/49): loss=180996900.80424473\n",
      "Gradient Descent(9/49): loss=5963345942.390666\n",
      "Gradient Descent(10/49): loss=196586496495.25366\n",
      "Gradient Descent(11/49): loss=6481316855034.957\n",
      "Gradient Descent(12/49): loss=213688634329735.97\n",
      "Gradient Descent(13/49): loss=7045327805293360.0\n",
      "Gradient Descent(14/49): loss=2.3228506534386096e+17\n",
      "Gradient Descent(15/49): loss=7.658459634605302e+18\n",
      "Gradient Descent(16/49): loss=2.52500113683067e+20\n",
      "Gradient Descent(17/49): loss=8.324951849768216e+21\n",
      "Gradient Descent(18/49): loss=2.7447442438510138e+23\n",
      "Gradient Descent(19/49): loss=9.049446893284403e+24\n",
      "Gradient Descent(20/49): loss=2.9836109233236912e+26\n",
      "Gradient Descent(21/49): loss=9.836992522087405e+27\n",
      "Gradient Descent(22/49): loss=3.24326543797176e+29\n",
      "Gradient Descent(23/49): loss=1.0693075833417362e+31\n",
      "Gradient Descent(24/49): loss=3.525516889259732e+32\n",
      "Gradient Descent(25/49): loss=1.1623661451658933e+34\n",
      "Gradient Descent(26/49): loss=3.8323318193250765e+35\n",
      "Gradient Descent(27/49): loss=1.2635233084249409e+37\n",
      "Gradient Descent(28/49): loss=4.165847912444902e+38\n",
      "Gradient Descent(29/49): loss=1.3734838695816763e+40\n",
      "Gradient Descent(30/49): loss=4.528388889007356e+41\n",
      "Gradient Descent(31/49): loss=1.4930139613748447e+43\n",
      "Gradient Descent(32/49): loss=4.922480695664756e+44\n",
      "Gradient Descent(33/49): loss=1.6229463907276118e+46\n",
      "Gradient Descent(34/49): loss=5.350869104465131e+47\n",
      "Gradient Descent(35/49): loss=1.7641864411973644e+49\n",
      "Gradient Descent(36/49): loss=5.816538843582492e+50\n",
      "Gradient Descent(37/49): loss=1.9177181803948876e+52\n",
      "Gradient Descent(38/49): loss=6.322734392936812e+53\n",
      "Gradient Descent(39/49): loss=2.084611316319295e+55\n",
      "Gradient Descent(40/49): loss=6.872982589591207e+56\n",
      "Gradient Descent(41/49): loss=2.2660286503783967e+58\n",
      "Gradient Descent(42/49): loss=7.471117200431062e+59\n",
      "Gradient Descent(43/49): loss=2.4632341790230946e+61\n",
      "Gradient Descent(44/49): loss=8.121305633322759e+62\n",
      "Gradient Descent(45/49): loss=2.6776019004412444e+64\n",
      "Gradient Descent(46/49): loss=8.828077972867748e+65\n",
      "Gradient Descent(47/49): loss=2.910625387672162e+67\n",
      "Gradient Descent(48/49): loss=9.596358543047034e+68\n",
      "Gradient Descent(49/49): loss=3.163928194839454e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8594828277380023\n",
      "Gradient Descent(2/49): loss=3.3836977747173846\n",
      "Gradient Descent(3/49): loss=22.421297620324395\n",
      "Gradient Descent(4/49): loss=243.73552562401144\n",
      "Gradient Descent(5/49): loss=4899.617160811514\n",
      "Gradient Descent(6/49): loss=138744.03836909044\n",
      "Gradient Descent(7/49): loss=4328149.399477105\n",
      "Gradient Descent(8/49): loss=137839633.76996845\n",
      "Gradient Descent(9/49): loss=4407929128.802033\n",
      "Gradient Descent(10/49): loss=141073671594.34064\n",
      "Gradient Descent(11/49): loss=4515709181761.513\n",
      "Gradient Descent(12/49): loss=144550426136518.84\n",
      "Gradient Descent(13/49): loss=4627169514066436.0\n",
      "Gradient Descent(14/49): loss=1.4811940409002893e+17\n",
      "Gradient Descent(15/49): loss=4.74142191292089e+18\n",
      "Gradient Descent(16/49): loss=1.517767567863841e+20\n",
      "Gradient Descent(17/49): loss=4.858496992393863e+21\n",
      "Gradient Descent(18/49): loss=1.5552442656935217e+23\n",
      "Gradient Descent(19/49): loss=4.97846294970169e+24\n",
      "Gradient Descent(20/49): loss=1.5936463416008736e+26\n",
      "Gradient Descent(21/49): loss=5.1013911076365e+27\n",
      "Gradient Descent(22/49): loss=1.6329966413373457e+29\n",
      "Gradient Descent(23/49): loss=5.227354606527738e+30\n",
      "Gradient Descent(24/49): loss=1.673318578292482e+32\n",
      "Gradient Descent(25/49): loss=5.356428394894598e+33\n",
      "Gradient Descent(26/49): loss=1.7146361441173274e+35\n",
      "Gradient Descent(27/49): loss=5.488689271970422e+36\n",
      "Gradient Descent(28/49): loss=1.7569739228698533e+38\n",
      "Gradient Descent(29/49): loss=5.624215933317928e+39\n",
      "Gradient Descent(30/49): loss=1.8003571056376e+41\n",
      "Gradient Descent(31/49): loss=5.763089017650126e+42\n",
      "Gradient Descent(32/49): loss=1.8448115055261162e+44\n",
      "Gradient Descent(33/49): loss=5.905391154810278e+45\n",
      "Gradient Descent(34/49): loss=1.8903635730181258e+47\n",
      "Gradient Descent(35/49): loss=6.051207014937433e+48\n",
      "Gradient Descent(36/49): loss=1.9370404117111577e+50\n",
      "Gradient Descent(37/49): loss=6.20062335884408e+51\n",
      "Gradient Descent(38/49): loss=1.9848697944447227e+53\n",
      "Gradient Descent(39/49): loss=6.35372908964018e+54\n",
      "Gradient Descent(40/49): loss=2.033880179824772e+56\n",
      "Gradient Descent(41/49): loss=6.510615305630233e+57\n",
      "Gradient Descent(42/49): loss=2.084100729156944e+59\n",
      "Gradient Descent(43/49): loss=6.671375354517438e+60\n",
      "Gradient Descent(44/49): loss=2.135561323797737e+62\n",
      "Gradient Descent(45/49): loss=6.836104888945557e+63\n",
      "Gradient Descent(46/49): loss=2.1882925829336836e+65\n",
      "Gradient Descent(47/49): loss=7.004901923412746e+66\n",
      "Gradient Descent(48/49): loss=2.2423258817999316e+68\n",
      "Gradient Descent(49/49): loss=7.177866892589155e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8682735290241895\n",
      "Gradient Descent(2/49): loss=3.4227971725310034\n",
      "Gradient Descent(3/49): loss=22.297568190645507\n",
      "Gradient Descent(4/49): loss=232.93730124612424\n",
      "Gradient Descent(5/49): loss=4523.567201325799\n",
      "Gradient Descent(6/49): loss=126616.67920274156\n",
      "Gradient Descent(7/49): loss=3940723.4562530257\n",
      "Gradient Descent(8/49): loss=125473932.26083972\n",
      "Gradient Descent(9/49): loss=4013245810.6736274\n",
      "Gradient Descent(10/49): loss=128475972063.21793\n",
      "Gradient Descent(11/49): loss=4113607354598.006\n",
      "Gradient Descent(12/49): loss=131715933303960.17\n",
      "Gradient Descent(13/49): loss=4217514883591268.0\n",
      "Gradient Descent(14/49): loss=1.3504405978933269e+17\n",
      "Gradient Descent(15/49): loss=4.3240873104381076e+18\n",
      "Gradient Descent(16/49): loss=1.3845653038304553e+20\n",
      "Gradient Descent(17/49): loss=4.433354280061636e+21\n",
      "Gradient Descent(18/49): loss=1.4195524150401445e+23\n",
      "Gradient Descent(19/49): loss=4.5453824180559826e+24\n",
      "Gradient Descent(20/49): loss=1.4554236327520725e+26\n",
      "Gradient Descent(21/49): loss=4.660241440603887e+27\n",
      "Gradient Descent(22/49): loss=1.4922012942521594e+29\n",
      "Gradient Descent(23/49): loss=4.778002880212362e+30\n",
      "Gradient Descent(24/49): loss=1.5299083046809125e+32\n",
      "Gradient Descent(25/49): loss=4.8987400790926536e+33\n",
      "Gradient Descent(26/49): loss=1.5685681481096496e+35\n",
      "Gradient Descent(27/49): loss=5.022528232850936e+36\n",
      "Gradient Descent(28/49): loss=1.6082049020430325e+38\n",
      "Gradient Descent(29/49): loss=5.149444437243409e+39\n",
      "Gradient Descent(30/49): loss=1.6488432524096008e+41\n",
      "Gradient Descent(31/49): loss=5.2795677361885996e+42\n",
      "Gradient Descent(32/49): loss=1.6905085089361643e+44\n",
      "Gradient Descent(33/49): loss=5.412979170996862e+45\n",
      "Gradient Descent(34/49): loss=1.7332266209108928e+47\n",
      "Gradient Descent(35/49): loss=5.549761830842082e+48\n",
      "Gradient Descent(36/49): loss=1.7770241933444032e+50\n",
      "Gradient Descent(37/49): loss=5.6900009045112665e+51\n",
      "Gradient Descent(38/49): loss=1.8219285035397188e+53\n",
      "Gradient Descent(39/49): loss=5.8337837334589647e+54\n",
      "Gradient Descent(40/49): loss=1.8679675180804722e+56\n",
      "Gradient Descent(41/49): loss=5.981199866205425e+57\n",
      "Gradient Descent(42/49): loss=1.915169910248637e+59\n",
      "Gradient Descent(43/49): loss=6.1323411141063014e+60\n",
      "Gradient Descent(44/49): loss=1.9635650778826138e+62\n",
      "Gradient Descent(45/49): loss=6.287301608534358e+63\n",
      "Gradient Descent(46/49): loss=2.013183161685932e+65\n",
      "Gradient Descent(47/49): loss=6.4461778595039585e+66\n",
      "Gradient Descent(48/49): loss=2.0640550639992363e+68\n",
      "Gradient Descent(49/49): loss=6.609068815778405e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8501050009940349\n",
      "Gradient Descent(2/49): loss=3.2518778438743823\n",
      "Gradient Descent(3/49): loss=20.12372681862805\n",
      "Gradient Descent(4/49): loss=186.70714839018447\n",
      "Gradient Descent(5/49): loss=3229.0594499157182\n",
      "Gradient Descent(6/49): loss=87408.1089400643\n",
      "Gradient Descent(7/49): loss=2733242.3179566115\n",
      "Gradient Descent(8/49): loss=88207824.46044075\n",
      "Gradient Descent(9/49): loss=2864369479.0897617\n",
      "Gradient Descent(10/49): loss=93125459458.9\n",
      "Gradient Descent(11/49): loss=3028355231223.894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=98483648586849.78\n",
      "Gradient Descent(13/49): loss=3202764890662838.5\n",
      "Gradient Descent(14/49): loss=1.041565723648323e+17\n",
      "Gradient Descent(15/49): loss=3.3872592079022444e+18\n",
      "Gradient Descent(16/49): loss=1.1015652059197979e+20\n",
      "Gradient Descent(17/49): loss=3.582382811281253e+21\n",
      "Gradient Descent(18/49): loss=1.1650210595654972e+23\n",
      "Gradient Descent(19/49): loss=3.78874660046853e+24\n",
      "Gradient Descent(20/49): loss=1.2321323023146904e+26\n",
      "Gradient Descent(21/49): loss=4.0069980141771855e+27\n",
      "Gradient Descent(22/49): loss=1.3031095001326195e+29\n",
      "Gradient Descent(23/49): loss=4.237821839014526e+30\n",
      "Gradient Descent(24/49): loss=1.3781753519103575e+32\n",
      "Gradient Descent(25/49): loss=4.481942310852273e+33\n",
      "Gradient Descent(26/49): loss=1.4575653852728588e+35\n",
      "Gradient Descent(27/49): loss=4.740125385374739e+36\n",
      "Gradient Descent(28/49): loss=1.5415286954601015e+38\n",
      "Gradient Descent(29/49): loss=5.013181141281198e+39\n",
      "Gradient Descent(30/49): loss=1.6303287268873145e+41\n",
      "Gradient Descent(31/49): loss=5.3019663219962716e+42\n",
      "Gradient Descent(32/49): loss=1.7242440997315638e+44\n",
      "Gradient Descent(33/49): loss=5.607387023800873e+45\n",
      "Gradient Descent(34/49): loss=1.8235694841342373e+47\n",
      "Gradient Descent(35/49): loss=5.930401538810891e+48\n",
      "Gradient Descent(36/49): loss=1.9286165247620256e+50\n",
      "Gradient Descent(37/49): loss=6.272023361728314e+51\n",
      "Gradient Descent(38/49): loss=2.039714818627243e+53\n",
      "Gradient Descent(39/49): loss=6.633324369795041e+54\n",
      "Gradient Descent(40/49): loss=2.1572129492362138e+56\n",
      "Gradient Descent(41/49): loss=7.015438185930454e+57\n",
      "Gradient Descent(42/49): loss=2.2814795803092606e+59\n",
      "Gradient Descent(43/49): loss=7.419563735601255e+60\n",
      "Gradient Descent(44/49): loss=2.4129046125053057e+62\n",
      "Gradient Descent(45/49): loss=7.846969008586417e+63\n",
      "Gradient Descent(46/49): loss=2.5519004067791356e+65\n",
      "Gradient Descent(47/49): loss=8.298995037438792e+66\n",
      "Gradient Descent(48/49): loss=2.6989030782107045e+68\n",
      "Gradient Descent(49/49): loss=8.77706010512705e+69\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8670849676867058\n",
      "Gradient Descent(2/49): loss=3.4699317201847335\n",
      "Gradient Descent(3/49): loss=23.905942160908335\n",
      "Gradient Descent(4/49): loss=284.8884767539333\n",
      "Gradient Descent(5/49): loss=6368.02217836345\n",
      "Gradient Descent(6/49): loss=194140.2959153876\n",
      "Gradient Descent(7/49): loss=6411493.707276676\n",
      "Gradient Descent(8/49): loss=215189546.14411378\n",
      "Gradient Descent(9/49): loss=7244731364.624757\n",
      "Gradient Descent(10/49): loss=244048441874.728\n",
      "Gradient Descent(11/49): loss=8221997670671.621\n",
      "Gradient Descent(12/49): loss=277005000050095.25\n",
      "Gradient Descent(13/49): loss=9332533358986000.0\n",
      "Gradient Descent(14/49): loss=3.1442119244554323e+17\n",
      "Gradient Descent(15/49): loss=1.059312579172203e+19\n",
      "Gradient Descent(16/49): loss=3.5689170969731295e+20\n",
      "Gradient Descent(17/49): loss=1.2023995143062938e+22\n",
      "Gradient Descent(18/49): loss=4.0509895692221027e+23\n",
      "Gradient Descent(19/49): loss=1.3648139655343133e+25\n",
      "Gradient Descent(20/49): loss=4.5981781210448156e+26\n",
      "Gradient Descent(21/49): loss=1.5491665946359096e+28\n",
      "Gradient Descent(22/49): loss=5.219278320156315e+29\n",
      "Gradient Descent(23/49): loss=1.7584207068228179e+31\n",
      "Gradient Descent(24/49): loss=5.924273802840022e+32\n",
      "Gradient Descent(25/49): loss=1.9959398768928097e+34\n",
      "Gradient Descent(26/49): loss=6.724496748042155e+35\n",
      "Gradient Descent(27/49): loss=2.2655420154650984e+37\n",
      "Gradient Descent(28/49): loss=7.63281003196564e+38\n",
      "Gradient Descent(29/49): loss=2.5715607385067557e+40\n",
      "Gradient Descent(30/49): loss=8.663813987424249e+41\n",
      "Gradient Descent(31/49): loss=2.918915026377132e+43\n",
      "Gradient Descent(32/49): loss=9.834081091280629e+44\n",
      "Gradient Descent(33/49): loss=3.313188292086623e+46\n",
      "Gradient Descent(34/49): loss=1.11624223523567e+48\n",
      "Gradient Descent(35/49): loss=3.7607181297238765e+49\n",
      "Gradient Descent(36/49): loss=1.2670189681765392e+51\n",
      "Gradient Descent(37/49): loss=4.2686981856760166e+52\n",
      "Gradient Descent(38/49): loss=1.4381619106001517e+54\n",
      "Gradient Descent(39/49): loss=4.8452937901336463e+55\n",
      "Gradient Descent(40/49): loss=1.632422033963531e+57\n",
      "Gradient Descent(41/49): loss=5.499773207552341e+58\n",
      "Gradient Descent(42/49): loss=1.852921897964589e+60\n",
      "Gradient Descent(43/49): loss=6.242656615807667e+61\n",
      "Gradient Descent(44/49): loss=2.1032058429281977e+63\n",
      "Gradient Descent(45/49): loss=7.085885208752465e+64\n",
      "Gradient Descent(46/49): loss=2.387296961942286e+66\n",
      "Gradient Descent(47/49): loss=8.043013140347122e+67\n",
      "Gradient Descent(48/49): loss=2.7097617685218816e+69\n",
      "Gradient Descent(49/49): loss=9.129425395699462e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8721753061211802\n",
      "Gradient Descent(2/49): loss=3.542358060566552\n",
      "Gradient Descent(3/49): loss=24.140113051004708\n",
      "Gradient Descent(4/49): loss=267.7932354031164\n",
      "Gradient Descent(5/49): loss=5476.517782206975\n",
      "Gradient Descent(6/49): loss=158136.36424566948\n",
      "Gradient Descent(7/49): loss=5038156.909890142\n",
      "Gradient Descent(8/49): loss=163943574.11076662\n",
      "Gradient Descent(9/49): loss=5357394913.428358\n",
      "Gradient Descent(10/49): loss=175216383264.61322\n",
      "Gradient Descent(11/49): loss=5731480478164.659\n",
      "Gradient Descent(12/49): loss=187487738436322.7\n",
      "Gradient Descent(13/49): loss=6133122803266472.0\n",
      "Gradient Descent(14/49): loss=2.0062774324386762e+17\n",
      "Gradient Descent(15/49): loss=6.562970027134518e+18\n",
      "Gradient Descent(16/49): loss=2.1468904017334564e+20\n",
      "Gradient Descent(17/49): loss=7.022946045913774e+21\n",
      "Gradient Descent(18/49): loss=2.297358598953114e+23\n",
      "Gradient Descent(19/49): loss=7.515160303923399e+24\n",
      "Gradient Descent(20/49): loss=2.458372603362559e+26\n",
      "Gradient Descent(21/49): loss=8.041872179216325e+27\n",
      "Gradient Descent(22/49): loss=2.6306715287411884e+29\n",
      "Gradient Descent(23/49): loss=8.605499488064941e+30\n",
      "Gradient Descent(24/49): loss=2.8150462963550154e+32\n",
      "Gradient Descent(25/49): loss=9.208629506762184e+33\n",
      "Gradient Descent(26/49): loss=3.0123432606636593e+35\n",
      "Gradient Descent(27/49): loss=9.854030845091867e+36\n",
      "Gradient Descent(28/49): loss=3.2234680942247144e+38\n",
      "Gradient Descent(29/49): loss=1.054466615523157e+40\n",
      "Gradient Descent(30/49): loss=3.449389945087293e+41\n",
      "Gradient Descent(31/49): loss=1.1283705731514407e+43\n",
      "Gradient Descent(32/49): loss=3.691145885571648e+44\n",
      "Gradient Descent(33/49): loss=1.2074542063358145e+46\n",
      "Gradient Descent(34/49): loss=3.949845672849233e+47\n",
      "Gradient Descent(35/49): loss=1.2920805408157258e+49\n",
      "Gradient Descent(36/49): loss=4.2266768431748685e+50\n",
      "Gradient Descent(37/49): loss=1.3826380455627275e+52\n",
      "Gradient Descent(38/49): loss=4.5229101631567455e+53\n",
      "Gradient Descent(39/49): loss=1.4795424160096027e+55\n",
      "Gradient Descent(40/49): loss=4.839905463087453e+56\n",
      "Gradient Descent(41/49): loss=1.5832384822599736e+58\n",
      "Gradient Descent(42/49): loss=5.179117879112182e+59\n",
      "Gradient Descent(43/49): loss=1.6942022510374482e+61\n",
      "Gradient Descent(44/49): loss=5.542104532890855e+62\n",
      "Gradient Descent(45/49): loss=1.8129430907485235e+64\n",
      "Gradient Descent(46/49): loss=5.930531679412908e+65\n",
      "Gradient Descent(47/49): loss=1.9400060696885418e+67\n",
      "Gradient Descent(48/49): loss=6.346182355779892e+68\n",
      "Gradient Descent(49/49): loss=2.075974458125231e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8812467738635172\n",
      "Gradient Descent(2/49): loss=3.5837823273461353\n",
      "Gradient Descent(3/49): loss=24.01229531766391\n",
      "Gradient Descent(4/49): loss=256.04425838424794\n",
      "Gradient Descent(5/49): loss=5057.569879753722\n",
      "Gradient Descent(6/49): loss=144325.09959590892\n",
      "Gradient Descent(7/49): loss=4587245.859680319\n",
      "Gradient Descent(8/49): loss=149236116.14655173\n",
      "Gradient Descent(9/49): loss=4877681054.787673\n",
      "Gradient Descent(10/49): loss=159569082930.46722\n",
      "Gradient Descent(11/49): loss=5221094157673.853\n",
      "Gradient Descent(12/49): loss=170839949553681.47\n",
      "Gradient Descent(13/49): loss=5590109319193998.0\n",
      "Gradient Descent(14/49): loss=1.829160209050417e+17\n",
      "Gradient Descent(15/49): loss=5.985263881945468e+18\n",
      "Gradient Descent(16/49): loss=1.9584607941778293e+20\n",
      "Gradient Descent(17/49): loss=6.408353576507704e+21\n",
      "Gradient Descent(18/49): loss=2.09690159554477e+23\n",
      "Gradient Descent(19/49): loss=6.86135097037199e+24\n",
      "Gradient Descent(20/49): loss=2.245128586011736e+26\n",
      "Gradient Descent(21/49): loss=7.346370109277274e+27\n",
      "Gradient Descent(22/49): loss=2.403833531802572e+29\n",
      "Gradient Descent(23/49): loss=7.8656745612698e+30\n",
      "Gradient Descent(24/49): loss=2.5737571044456402e+32\n",
      "Gradient Descent(25/49): loss=8.421687906212094e+33\n",
      "Gradient Descent(26/49): loss=2.7556923327042104e+35\n",
      "Gradient Descent(27/49): loss=9.017005043518021e+36\n",
      "Gradient Descent(28/49): loss=2.9504883034254327e+38\n",
      "Gradient Descent(29/49): loss=9.654404302355413e+39\n",
      "Gradient Descent(30/49): loss=3.1590541241980086e+41\n",
      "Gradient Descent(31/49): loss=1.0336860408029156e+43\n",
      "Gradient Descent(32/49): loss=3.382363166132968e+44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=1.1067558364943027e+46\n",
      "Gradient Descent(34/49): loss=3.62145760656049e+47\n",
      "Gradient Descent(35/49): loss=1.1849908320932594e+49\n",
      "Gradient Descent(36/49): loss=3.877453292843389e+50\n",
      "Gradient Descent(37/49): loss=1.2687561482331363e+52\n",
      "Gradient Descent(38/49): loss=4.151544950007411e+53\n",
      "Gradient Descent(39/49): loss=1.3584427154054598e+55\n",
      "Gradient Descent(40/49): loss=4.445011756490575e+56\n",
      "Gradient Descent(41/49): loss=1.4544690984221638e+58\n",
      "Gradient Descent(42/49): loss=4.759223314035115e+59\n",
      "Gradient Descent(43/49): loss=1.5572834498461582e+61\n",
      "Gradient Descent(44/49): loss=5.095646039581533e+62\n",
      "Gradient Descent(45/49): loss=1.6673656015074548e+64\n",
      "Gradient Descent(46/49): loss=5.455850008998415e+65\n",
      "Gradient Descent(47/49): loss=1.7852293038656692e+67\n",
      "Gradient Descent(48/49): loss=5.841516284583037e+68\n",
      "Gradient Descent(49/49): loss=1.911424623669347e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8625459394609377\n",
      "Gradient Descent(2/49): loss=3.404101836083548\n",
      "Gradient Descent(3/49): loss=21.681087359525012\n",
      "Gradient Descent(4/49): loss=205.5197597441547\n",
      "Gradient Descent(5/49): loss=3613.710182320128\n",
      "Gradient Descent(6/49): loss=99650.57911740053\n",
      "Gradient Descent(7/49): loss=3181343.5533492966\n",
      "Gradient Descent(8/49): loss=104893219.02580187\n",
      "Gradient Descent(9/49): loss=3480568837.528108\n",
      "Gradient Descent(10/49): loss=115634339670.99078\n",
      "Gradient Descent(11/49): loss=3842606988286.8066\n",
      "Gradient Descent(12/49): loss=127698205722231.48\n",
      "Gradient Descent(13/49): loss=4243726623973055.0\n",
      "Gradient Descent(14/49): loss=1.410297475849455e+17\n",
      "Gradient Descent(15/49): loss=4.68677599604944e+18\n",
      "Gradient Descent(16/49): loss=1.557534560162486e+20\n",
      "Gradient Descent(17/49): loss=5.176082528296384e+21\n",
      "Gradient Descent(18/49): loss=1.7201435580084144e+23\n",
      "Gradient Descent(19/49): loss=5.716473501470965e+24\n",
      "Gradient Descent(20/49): loss=1.899729190834495e+26\n",
      "Gradient Descent(21/49): loss=6.31328212688784e+27\n",
      "Gradient Descent(22/49): loss=2.0980638401568743e+29\n",
      "Gradient Descent(23/49): loss=6.972398490840761e+30\n",
      "Gradient Descent(24/49): loss=2.3171049319189243e+32\n",
      "Gradient Descent(25/49): loss=7.700327616925506e+33\n",
      "Gradient Descent(26/49): loss=2.5590142505492893e+35\n",
      "Gradient Descent(27/49): loss=8.504253663338536e+36\n",
      "Gradient Descent(28/49): loss=2.8261792741045395e+38\n",
      "Gradient Descent(29/49): loss=9.392110825446338e+39\n",
      "Gradient Descent(30/49): loss=3.1212367370223962e+41\n",
      "Gradient Descent(31/49): loss=1.0372661640814143e+43\n",
      "Gradient Descent(32/49): loss=3.447098652871112e+44\n",
      "Gradient Descent(33/49): loss=1.1455583469406342e+46\n",
      "Gradient Descent(34/49): loss=3.8069810539143356e+47\n",
      "Gradient Descent(35/49): loss=1.265156400245208e+49\n",
      "Gradient Descent(36/49): loss=4.204435731130377e+50\n",
      "Gradient Descent(37/49): loss=1.3972406742581192e+52\n",
      "Gradient Descent(38/49): loss=4.64338528793829e+53\n",
      "Gradient Descent(39/49): loss=1.5431147496253936e+55\n",
      "Gradient Descent(40/49): loss=5.128161853587454e+56\n",
      "Gradient Descent(41/49): loss=1.7042183028172832e+58\n",
      "Gradient Descent(42/49): loss=5.663549838282875e+59\n",
      "Gradient Descent(43/49): loss=1.8821413147417648e+61\n",
      "Gradient Descent(44/49): loss=6.254833152014613e+62\n",
      "Gradient Descent(45/49): loss=2.0786397627591885e+64\n",
      "Gradient Descent(46/49): loss=6.907847353101505e+65\n",
      "Gradient Descent(47/49): loss=2.2956529509668162e+67\n",
      "Gradient Descent(48/49): loss=7.629037241126324e+68\n",
      "Gradient Descent(49/49): loss=2.535322649792503e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8798253833494931\n",
      "Gradient Descent(2/49): loss=3.631185764882123\n",
      "Gradient Descent(3/49): loss=25.711530666147034\n",
      "Gradient Descent(4/49): loss=312.4206437673273\n",
      "Gradient Descent(5/49): loss=7104.067983382216\n",
      "Gradient Descent(6/49): loss=220865.37756323334\n",
      "Gradient Descent(7/49): loss=7448347.349037562\n",
      "Gradient Descent(8/49): loss=255368620.98671716\n",
      "Gradient Descent(9/49): loss=8783150875.176527\n",
      "Gradient Descent(10/49): loss=302269042266.9218\n",
      "Gradient Descent(11/49): loss=10403662108910.932\n",
      "Gradient Descent(12/49): loss=358086634236818.56\n",
      "Gradient Descent(13/49): loss=1.23251365942948e+16\n",
      "Gradient Descent(14/49): loss=4.2422445769338784e+17\n",
      "Gradient Descent(15/49): loss=1.460157582220865e+19\n",
      "Gradient Descent(16/49): loss=5.025783366406065e+20\n",
      "Gradient Descent(17/49): loss=1.7298474413865946e+22\n",
      "Gradient Descent(18/49): loss=5.9540413010799507e+23\n",
      "Gradient Descent(19/49): loss=2.049348802289266e+25\n",
      "Gradient Descent(20/49): loss=7.053747700454705e+26\n",
      "Gradient Descent(21/49): loss=2.427861795240374e+28\n",
      "Gradient Descent(22/49): loss=8.356568943364989e+29\n",
      "Gradient Descent(23/49): loss=2.8762858183331704e+31\n",
      "Gradient Descent(24/49): loss=9.900020169538093e+32\n",
      "Gradient Descent(25/49): loss=3.407533379768872e+34\n",
      "Gradient Descent(26/49): loss=1.1728545533641069e+36\n",
      "Gradient Descent(27/49): loss=4.036901917128702e+37\n",
      "Gradient Descent(28/49): loss=1.3894797988184834e+39\n",
      "Gradient Descent(29/49): loss=4.782514291796006e+40\n",
      "Gradient Descent(30/49): loss=1.646115544154173e+42\n",
      "Gradient Descent(31/49): loss=5.665840642346258e+43\n",
      "Gradient Descent(32/49): loss=1.950151694907765e+45\n",
      "Gradient Descent(33/49): loss=6.712316623816608e+46\n",
      "Gradient Descent(34/49): loss=2.310343065927299e+48\n",
      "Gradient Descent(35/49): loss=7.952075835247569e+49\n",
      "Gradient Descent(36/49): loss=2.7370614789689943e+51\n",
      "Gradient Descent(37/49): loss=9.420817525972167e+52\n",
      "Gradient Descent(38/49): loss=3.242594422508128e+54\n",
      "Gradient Descent(39/49): loss=1.1160834566526438e+56\n",
      "Gradient Descent(40/49): loss=3.841498873763534e+57\n",
      "Gradient Descent(41/49): loss=1.3222231284913105e+59\n",
      "Gradient Descent(42/49): loss=4.551020471351955e+60\n",
      "Gradient Descent(43/49): loss=1.566436623620208e+62\n",
      "Gradient Descent(44/49): loss=5.391590108777856e+63\n",
      "Gradient Descent(45/49): loss=1.855756145045221e+65\n",
      "Gradient Descent(46/49): loss=6.3874122483204795e+66\n",
      "Gradient Descent(47/49): loss=2.198512737728336e+68\n",
      "Gradient Descent(48/49): loss=7.567161896000197e+69\n",
      "Gradient Descent(49/49): loss=2.6045761835997014e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8850395998012737\n",
      "Gradient Descent(2/49): loss=3.7072245374690187\n",
      "Gradient Descent(3/49): loss=25.969617938813908\n",
      "Gradient Descent(4/49): loss=293.92313981662414\n",
      "Gradient Descent(5/49): loss=6114.128709476614\n",
      "Gradient Descent(6/49): loss=179988.0443211059\n",
      "Gradient Descent(7/49): loss=5855110.645538206\n",
      "Gradient Descent(8/49): loss=194628996.23397127\n",
      "Gradient Descent(9/49): loss=6497773316.037516\n",
      "Gradient Descent(10/49): loss=217117256438.95245\n",
      "Gradient Descent(11/49): loss=7256006039980.352\n",
      "Gradient Descent(12/49): loss=242502050172567.44\n",
      "Gradient Descent(13/49): loss=8104683282137301.0\n",
      "Gradient Descent(14/49): loss=2.7086771482113942e+17\n",
      "Gradient Descent(15/49): loss=9.052708798145157e+18\n",
      "Gradient Descent(16/49): loss=3.0255189607443346e+20\n",
      "Gradient Descent(17/49): loss=1.0111630978768798e+22\n",
      "Gradient Descent(18/49): loss=3.379422921389723e+23\n",
      "Gradient Descent(19/49): loss=1.129441858587062e+25\n",
      "Gradient Descent(20/49): loss=3.7747240925403524e+26\n",
      "Gradient Descent(21/49): loss=1.2615560390901643e+28\n",
      "Gradient Descent(22/49): loss=4.216264820294344e+29\n",
      "Gradient Descent(23/49): loss=1.4091240090834994e+31\n",
      "Gradient Descent(24/49): loss=4.7094538830156256e+32\n",
      "Gradient Descent(25/49): loss=1.5739534443584357e+34\n",
      "Gradient Descent(26/49): loss=5.260332740367444e+35\n",
      "Gradient Descent(27/49): loss=1.7580634699561377e+37\n",
      "Gradient Descent(28/49): loss=5.875649539573193e+38\n",
      "Gradient Descent(29/49): loss=1.9637093939929373e+40\n",
      "Gradient Descent(30/49): loss=6.562941778750445e+41\n",
      "Gradient Descent(31/49): loss=2.193410334697631e+43\n",
      "Gradient Descent(32/49): loss=7.330628639637893e+44\n",
      "Gradient Descent(33/49): loss=2.449980079066557e+46\n",
      "Gradient Descent(34/49): loss=8.188114120754807e+47\n",
      "Gradient Descent(35/49): loss=2.7365615511473133e+49\n",
      "Gradient Descent(36/49): loss=9.145902234356777e+50\n",
      "Gradient Descent(37/49): loss=3.0566653121813716e+52\n",
      "Gradient Descent(38/49): loss=1.0215725678320423e+54\n",
      "Gradient Descent(39/49): loss=3.414212564221426e+55\n",
      "Gradient Descent(40/49): loss=1.1410689559162208e+57\n",
      "Gradient Descent(41/49): loss=3.8135831840119703e+58\n",
      "Gradient Descent(42/49): loss=1.2745431926768531e+60\n",
      "Gradient Descent(43/49): loss=4.259669375534548e+61\n",
      "Gradient Descent(44/49): loss=1.423630308734991e+63\n",
      "Gradient Descent(45/49): loss=4.7579355984513206e+64\n",
      "Gradient Descent(46/49): loss=1.590156589116648e+66\n",
      "Gradient Descent(47/49): loss=5.314485506559009e+67\n",
      "Gradient Descent(48/49): loss=1.7761619448507147e+69\n",
      "Gradient Descent(49/49): loss=5.93613671623011e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8943954921846021\n",
      "Gradient Descent(2/49): loss=3.751071645728427\n",
      "Gradient Descent(3/49): loss=25.837766511746278\n",
      "Gradient Descent(4/49): loss=281.15153060693467\n",
      "Gradient Descent(5/49): loss=5647.932080330864\n",
      "Gradient Descent(6/49): loss=164280.99532114153\n",
      "Gradient Descent(7/49): loss=5331164.946083731\n",
      "Gradient Descent(8/49): loss=177168812.4238938\n",
      "Gradient Descent(9/49): loss=5915928515.988077\n",
      "Gradient Descent(10/49): loss=197727246391.1429\n",
      "Gradient Descent(11/49): loss=6609829047698.084\n",
      "Gradient Descent(12/49): loss=220968138100308.4\n",
      "Gradient Descent(13/49): loss=7387069237966811.0\n",
      "Gradient Descent(14/49): loss=2.469535582535963e+17\n",
      "Gradient Descent(15/49): loss=8.25578786968293e+18\n",
      "Gradient Descent(16/49): loss=2.7599536320599074e+20\n",
      "Gradient Descent(17/49): loss=9.226671338371192e+21\n",
      "Gradient Descent(18/49): loss=3.084525159162366e+23\n",
      "Gradient Descent(19/49): loss=1.0311731189332766e+25\n",
      "Gradient Descent(20/49): loss=3.447266423282545e+26\n",
      "Gradient Descent(21/49): loss=1.152439447371377e+28\n",
      "Gradient Descent(22/49): loss=3.852666190497215e+29\n",
      "Gradient Descent(23/49): loss=1.2879667395336943e+31\n",
      "Gradient Descent(24/49): loss=4.30574111568992e+32\n",
      "Gradient Descent(25/49): loss=1.439432089842283e+34\n",
      "Gradient Descent(26/49): loss=4.812097814514694e+35\n",
      "Gradient Descent(27/49): loss=1.6087098196479662e+37\n",
      "Gradient Descent(28/49): loss=5.37800224265153e+38\n",
      "Gradient Descent(29/49): loss=1.7978946711653245e+40\n",
      "Gradient Descent(30/49): loss=6.010457234415458e+41\n",
      "Gradient Descent(31/49): loss=2.0093277290444262e+43\n",
      "Gradient Descent(32/49): loss=6.717289159947536e+44\n",
      "Gradient Descent(33/49): loss=2.2456253903293966e+46\n",
      "Gradient Descent(34/49): loss=7.507244773323647e+47\n",
      "Gradient Descent(35/49): loss=2.509711741294789e+49\n",
      "Gradient Descent(36/49): loss=8.390099450033477e+50\n",
      "Gradient Descent(37/49): loss=2.804854741809382e+52\n",
      "Gradient Descent(38/49): loss=9.376778153230423e+53\n",
      "Gradient Descent(39/49): loss=3.134706664994003e+55\n",
      "Gradient Descent(40/49): loss=1.0479490625650137e+57\n",
      "Gradient Descent(41/49): loss=3.5033492925979343e+58\n",
      "Gradient Descent(42/49): loss=1.1711882480151643e+60\n",
      "Gradient Descent(43/49): loss=3.9153444253675166e+61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=1.3089204058558516e+63\n",
      "Gradient Descent(45/49): loss=4.3757903334521767e+64\n",
      "Gradient Descent(46/49): loss=1.4628499148359575e+66\n",
      "Gradient Descent(47/49): loss=4.8903848453986e+67\n",
      "Gradient Descent(48/49): loss=1.6348815892562428e+69\n",
      "Gradient Descent(49/49): loss=5.465495856433375e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8751556710561845\n",
      "Gradient Descent(2/49): loss=3.562301338483845\n",
      "Gradient Descent(3/49): loss=23.339760162619196\n",
      "Gradient Descent(4/49): loss=225.98642340286617\n",
      "Gradient Descent(5/49): loss=4039.3725064429364\n",
      "Gradient Descent(6/49): loss=113449.80692577756\n",
      "Gradient Descent(7/49): loss=3696909.69321898\n",
      "Gradient Descent(8/49): loss=124503782.03085935\n",
      "Gradient Descent(9/49): loss=4220514569.4090414\n",
      "Gradient Descent(10/49): loss=143251268348.61023\n",
      "Gradient Descent(11/49): loss=4863374147416.174\n",
      "Gradient Descent(12/49): loss=165119111185861.75\n",
      "Gradient Descent(13/49): loss=5606101337314196.0\n",
      "Gradient Descent(14/49): loss=1.9033791386085776e+17\n",
      "Gradient Descent(15/49): loss=6.462340132596181e+18\n",
      "Gradient Descent(16/49): loss=2.1940895459804676e+20\n",
      "Gradient Descent(17/49): loss=7.449358741917098e+21\n",
      "Gradient Descent(18/49): loss=2.5292015041588754e+23\n",
      "Gradient Descent(19/49): loss=8.587128735813999e+24\n",
      "Gradient Descent(20/49): loss=2.9154964446600035e+26\n",
      "Gradient Descent(21/49): loss=9.898674842873641e+27\n",
      "Gradient Descent(22/49): loss=3.360791738392615e+29\n",
      "Gradient Descent(23/49): loss=1.14105385702075e+31\n",
      "Gradient Descent(24/49): loss=3.8740987421160895e+32\n",
      "Gradient Descent(25/49): loss=1.315331521936454e+34\n",
      "Gradient Descent(26/49): loss=4.46580515305772e+35\n",
      "Gradient Descent(27/49): loss=1.516227303342982e+37\n",
      "Gradient Descent(28/49): loss=5.1478852225081756e+38\n",
      "Gradient Descent(29/49): loss=1.747806691364175e+40\n",
      "Gradient Descent(30/49): loss=5.93414207647967e+41\n",
      "Gradient Descent(31/49): loss=2.0147561144969333e+43\n",
      "Gradient Descent(32/49): loss=6.840487046967992e+44\n",
      "Gradient Descent(33/49): loss=2.3224777779826427e+46\n",
      "Gradient Descent(34/49): loss=7.885261666585416e+47\n",
      "Gradient Descent(35/49): loss=2.677198987218363e+49\n",
      "Gradient Descent(36/49): loss=9.089608842703918e+50\n",
      "Gradient Descent(37/49): loss=3.0860981685641476e+52\n",
      "Gradient Descent(38/49): loss=1.0477900722493076e+54\n",
      "Gradient Descent(39/49): loss=3.557450137806308e+55\n",
      "Gradient Descent(40/49): loss=1.2078231907475808e+57\n",
      "Gradient Descent(41/49): loss=4.10079355604755e+58\n",
      "Gradient Descent(42/49): loss=1.3922987998692474e+60\n",
      "Gradient Descent(43/49): loss=4.727123961796591e+61\n",
      "Gradient Descent(44/49): loss=1.6049500978015866e+63\n",
      "Gradient Descent(45/49): loss=5.449116285612247e+64\n",
      "Gradient Descent(46/49): loss=1.8500804688442286e+66\n",
      "Gradient Descent(47/49): loss=6.281381350286749e+67\n",
      "Gradient Descent(48/49): loss=2.1326505701873135e+69\n",
      "Gradient Descent(49/49): loss=7.240761547319029e+70\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8927362110789285\n",
      "Gradient Descent(2/49): loss=3.7986756629970273\n",
      "Gradient Descent(3/49): loss=27.63165699265166\n",
      "Gradient Descent(4/49): loss=342.2751197941616\n",
      "Gradient Descent(5/49): loss=7916.121086342205\n",
      "Gradient Descent(6/49): loss=250928.27770089154\n",
      "Gradient Descent(7/49): loss=8639179.121856377\n",
      "Gradient Descent(8/49): loss=302501291.89621484\n",
      "Gradient Descent(9/49): loss=10626575082.886599\n",
      "Gradient Descent(10/49): loss=373532026459.491\n",
      "Gradient Descent(11/49): loss=13131469140786.877\n",
      "Gradient Descent(12/49): loss=461645338596529.3\n",
      "Gradient Descent(13/49): loss=1.6229510640001286e+16\n",
      "Gradient Descent(14/49): loss=5.705618674210239e+17\n",
      "Gradient Descent(15/49): loss=2.0058577316780397e+19\n",
      "Gradient Descent(16/49): loss=7.051760002463618e+20\n",
      "Gradient Descent(17/49): loss=2.479105004241276e+22\n",
      "Gradient Descent(18/49): loss=8.715500281170627e+23\n",
      "Gradient Descent(19/49): loss=3.0640067700264124e+25\n",
      "Gradient Descent(20/49): loss=1.0771771194133212e+27\n",
      "Gradient Descent(21/49): loss=3.7869059492509108e+28\n",
      "Gradient Descent(22/49): loss=1.331318351461217e+30\n",
      "Gradient Descent(23/49): loss=4.680360633958905e+31\n",
      "Gradient Descent(24/49): loss=1.645419793084829e+33\n",
      "Gradient Descent(25/49): loss=5.784610433288931e+34\n",
      "Gradient Descent(26/49): loss=2.0336280142941677e+36\n",
      "Gradient Descent(27/49): loss=7.149388793275525e+37\n",
      "Gradient Descent(28/49): loss=2.5134272225864222e+39\n",
      "Gradient Descent(29/49): loss=8.836162902737374e+40\n",
      "Gradient Descent(30/49): loss=3.1064267205384493e+42\n",
      "Gradient Descent(31/49): loss=1.0920902066083263e+44\n",
      "Gradient Descent(32/49): loss=3.8393341503418886e+45\n",
      "Gradient Descent(33/49): loss=1.3497499225600667e+47\n",
      "Gradient Descent(34/49): loss=4.7451583584841967e+48\n",
      "Gradient Descent(35/49): loss=1.6681999732502374e+50\n",
      "Gradient Descent(36/49): loss=5.8646960554571015e+51\n",
      "Gradient Descent(37/49): loss=2.0617827823052163e+53\n",
      "Gradient Descent(38/49): loss=7.248369227003188e+54\n",
      "Gradient Descent(39/49): loss=2.5482246190952033e+56\n",
      "Gradient Descent(40/49): loss=8.958496050631532e+57\n",
      "Gradient Descent(41/49): loss=3.149433958364212e+59\n",
      "Gradient Descent(42/49): loss=1.1072097595442351e+61\n",
      "Gradient Descent(43/49): loss=3.892488198948398e+62\n",
      "Gradient Descent(44/49): loss=1.3684366714026558e+64\n",
      "Gradient Descent(45/49): loss=4.810853181637187e+65\n",
      "Gradient Descent(46/49): loss=1.691295535915846e+67\n",
      "Gradient Descent(47/49): loss=5.945890431092646e+68\n",
      "Gradient Descent(48/49): loss=2.090327341839546e+70\n",
      "Gradient Descent(49/49): loss=7.348720005321134e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.8980757087782827\n",
      "Gradient Descent(2/49): loss=3.878457200129471\n",
      "Gradient Descent(3/49): loss=27.91546431431773\n",
      "Gradient Descent(4/49): loss=322.27755907279976\n",
      "Gradient Descent(5/49): loss=6818.092078040981\n",
      "Gradient Descent(6/49): loss=204579.82479406113\n",
      "Gradient Descent(7/49): loss=6793716.998805288\n",
      "Gradient Descent(8/49): loss=230637892.88104975\n",
      "Gradient Descent(9/49): loss=7864779564.647605\n",
      "Gradient Descent(10/49): loss=268427044565.2378\n",
      "Gradient Descent(11/49): loss=9163090328739.062\n",
      "Gradient Descent(12/49): loss=312804288312585.4\n",
      "Gradient Descent(13/49): loss=1.0678405345348586e+16\n",
      "Gradient Descent(14/49): loss=3.6453622734977555e+17\n",
      "Gradient Descent(15/49): loss=1.2444432698965295e+19\n",
      "Gradient Descent(16/49): loss=4.2482448049684906e+20\n",
      "Gradient Descent(17/49): loss=1.4502536635224756e+22\n",
      "Gradient Descent(18/49): loss=4.9508344963119764e+23\n",
      "Gradient Descent(19/49): loss=1.6901017274725426e+25\n",
      "Gradient Descent(20/49): loss=5.7696209630219476e+26\n",
      "Gradient Descent(21/49): loss=1.9696167110164173e+28\n",
      "Gradient Descent(22/49): loss=6.723821223596533e+29\n",
      "Gradient Descent(23/49): loss=2.2953588682533636e+31\n",
      "Gradient Descent(24/49): loss=7.835830488146021e+32\n",
      "Gradient Descent(25/49): loss=2.67497341213934e+34\n",
      "Gradient Descent(26/49): loss=9.131747766209628e+35\n",
      "Gradient Descent(27/49): loss=3.117369947949603e+37\n",
      "Gradient Descent(28/49): loss=1.0641988413586153e+39\n",
      "Gradient Descent(29/49): loss=3.6329315829001532e+40\n",
      "Gradient Descent(30/49): loss=1.2401997984872878e+42\n",
      "Gradient Descent(31/49): loss=4.233758619093186e+43\n",
      "Gradient Descent(32/49): loss=1.4453084145481028e+45\n",
      "Gradient Descent(33/49): loss=4.933952549262355e+46\n",
      "Gradient Descent(34/49): loss=1.6843386168192292e+48\n",
      "Gradient Descent(35/49): loss=5.749947020736304e+49\n",
      "Gradient Descent(36/49): loss=1.9629004768477156e+51\n",
      "Gradient Descent(37/49): loss=6.700893535390594e+52\n",
      "Gradient Descent(38/49): loss=2.2875318795962324e+54\n",
      "Gradient Descent(39/49): loss=7.809110938014951e+55\n",
      "Gradient Descent(40/49): loss=2.665851968497531e+57\n",
      "Gradient Descent(41/49): loss=9.100609242655594e+58\n",
      "Gradient Descent(42/49): loss=3.1067399677929725e+60\n",
      "Gradient Descent(43/49): loss=1.060570009120166e+62\n",
      "Gradient Descent(44/49): loss=3.620543579140237e+63\n",
      "Gradient Descent(45/49): loss=1.2359708171767368e+65\n",
      "Gradient Descent(46/49): loss=4.219321843587007e+66\n",
      "Gradient Descent(47/49): loss=1.4403800293955511e+68\n",
      "Gradient Descent(48/49): loss=4.9171281689138456e+69\n",
      "Gradient Descent(49/49): loss=1.6785951579509e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9077196839874444\n",
      "Gradient Descent(2/49): loss=3.9248278226363147\n",
      "Gradient Descent(3/49): loss=27.779651207872924\n",
      "Gradient Descent(4/49): loss=308.4068777646504\n",
      "Gradient Descent(5/49): loss=6299.908501841468\n",
      "Gradient Descent(6/49): loss=186741.15114454023\n",
      "Gradient Descent(7/49): loss=6185876.124726368\n",
      "Gradient Descent(8/49): loss=209947464.47667658\n",
      "Gradient Descent(9/49): loss=7160503827.752586\n",
      "Gradient Descent(10/49): loss=244453706249.67767\n",
      "Gradient Descent(11/49): loss=8347039886349.799\n",
      "Gradient Descent(12/49): loss=285026123856439.9\n",
      "Gradient Descent(13/49): loss=9732850633992764.0\n",
      "Gradient Descent(14/49): loss=3.323503061923157e+17\n",
      "Gradient Descent(15/49): loss=1.1348859845037554e+19\n",
      "Gradient Descent(16/49): loss=3.875327468214677e+20\n",
      "Gradient Descent(17/49): loss=1.3233191138355787e+22\n",
      "Gradient Descent(18/49): loss=4.5187754976776206e+23\n",
      "Gradient Descent(19/49): loss=1.5430391500997462e+25\n",
      "Gradient Descent(20/49): loss=5.2690597710968995e+26\n",
      "Gradient Descent(21/49): loss=1.7992408598357682e+28\n",
      "Gradient Descent(22/49): loss=6.14391905263844e+29\n",
      "Gradient Descent(23/49): loss=2.0979815525556216e+31\n",
      "Gradient Descent(24/49): loss=7.16403741187647e+32\n",
      "Gradient Descent(25/49): loss=2.4463242766004247e+34\n",
      "Gradient Descent(26/49): loss=8.353533241414668e+35\n",
      "Gradient Descent(27/49): loss=2.852504808250139e+37\n",
      "Gradient Descent(28/49): loss=9.740529481286159e+38\n",
      "Gradient Descent(29/49): loss=3.326126368004527e+40\n",
      "Gradient Descent(30/49): loss=1.135781852227818e+42\n",
      "Gradient Descent(31/49): loss=3.8783866670224267e+43\n",
      "Gradient Descent(32/49): loss=1.3243637507883631e+45\n",
      "Gradient Descent(33/49): loss=4.5223426516900316e+46\n",
      "Gradient Descent(34/49): loss=1.5442572365122942e+48\n",
      "Gradient Descent(35/49): loss=5.273219205602303e+49\n",
      "Gradient Descent(36/49): loss=1.8006611938005315e+51\n",
      "Gradient Descent(37/49): loss=6.148769107520479e+52\n",
      "Gradient Descent(38/49): loss=2.099637714621952e+54\n",
      "Gradient Descent(39/49): loss=7.169692755694583e+55\n",
      "Gradient Descent(40/49): loss=2.4482554229749603e+57\n",
      "Gradient Descent(41/49): loss=8.360127581988407e+58\n",
      "Gradient Descent(42/49): loss=2.8547565965235486e+60\n",
      "Gradient Descent(43/49): loss=9.748218726891815e+61\n",
      "Gradient Descent(44/49): loss=3.32875203662013e+63\n",
      "Gradient Descent(45/49): loss=1.136678446774626e+65\n",
      "Gradient Descent(46/49): loss=3.881448293979554e+66\n",
      "Gradient Descent(47/49): loss=1.325409213272728e+68\n",
      "Gradient Descent(48/49): loss=4.525912622237025e+69\n",
      "Gradient Descent(49/49): loss=1.545476284531418e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.887934195779775\n",
      "Gradient Descent(2/49): loss=3.7266309014406644\n",
      "Gradient Descent(3/49): loss=25.104964791684782\n",
      "Gradient Descent(4/49): loss=248.23114679067535\n",
      "Gradient Descent(5/49): loss=4509.913283993074\n",
      "Gradient Descent(6/49): loss=128984.0985229017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=4289215.210380032\n",
      "Gradient Descent(8/49): loss=147512749.71237418\n",
      "Gradient Descent(9/49): loss=5107328167.534354\n",
      "Gradient Descent(10/49): loss=177061734771.30493\n",
      "Gradient Descent(11/49): loss=6139958343357.165\n",
      "Gradient Descent(12/49): loss=212925350677887.3\n",
      "Gradient Descent(13/49): loss=7384029549023837.0\n",
      "Gradient Descent(14/49): loss=2.5607092805698445e+17\n",
      "Gradient Descent(15/49): loss=8.880292660282815e+18\n",
      "Gradient Descent(16/49): loss=3.079600004990767e+20\n",
      "Gradient Descent(17/49): loss=1.0679756489086548e+22\n",
      "Gradient Descent(18/49): loss=3.703636796129432e+23\n",
      "Gradient Descent(19/49): loss=1.2843856073366265e+25\n",
      "Gradient Descent(20/49): loss=4.454125712909179e+26\n",
      "Gradient Descent(21/49): loss=1.5446479432221209e+28\n",
      "Gradient Descent(22/49): loss=5.35669045351336e+29\n",
      "Gradient Descent(23/49): loss=1.8576487115196831e+31\n",
      "Gradient Descent(24/49): loss=6.442147003562502e+32\n",
      "Gradient Descent(25/49): loss=2.2340745996889166e+34\n",
      "Gradient Descent(26/49): loss=7.747555767068328e+35\n",
      "Gradient Descent(27/49): loss=2.686777799281728e+37\n",
      "Gradient Descent(28/49): loss=9.317486907802784e+38\n",
      "Gradient Descent(29/49): loss=3.2312148142762243e+40\n",
      "Gradient Descent(30/49): loss=1.12055420944618e+42\n",
      "Gradient Descent(31/49): loss=3.885974187664274e+43\n",
      "Gradient Descent(32/49): loss=1.34761846057015e+45\n",
      "Gradient Descent(33/49): loss=4.673411164269847e+46\n",
      "Gradient Descent(34/49): loss=1.6206940279730444e+48\n",
      "Gradient Descent(35/49): loss=5.620410958892859e+49\n",
      "Gradient Descent(36/49): loss=1.9491044454794298e+51\n",
      "Gradient Descent(37/49): loss=6.759306689801507e+52\n",
      "Gradient Descent(38/49): loss=2.3440625274217172e+54\n",
      "Gradient Descent(39/49): loss=8.128983318293611e+55\n",
      "Gradient Descent(40/49): loss=2.8190532042581733e+57\n",
      "Gradient Descent(41/49): loss=9.776205285787542e+58\n",
      "Gradient Descent(42/49): loss=3.3902939343427894e+60\n",
      "Gradient Descent(43/49): loss=1.1757213177542055e+62\n",
      "Gradient Descent(44/49): loss=4.07728841154183e+63\n",
      "Gradient Descent(45/49): loss=1.4139643927395993e+65\n",
      "Gradient Descent(46/49): loss=4.903492473762657e+66\n",
      "Gradient Descent(47/49): loss=1.7004840124481744e+68\n",
      "Gradient Descent(48/49): loss=5.8971149483033205e+69\n",
      "Gradient Descent(49/49): loss=2.0450627267841985e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9058174508750122\n",
      "Gradient Descent(2/49): loss=3.9725603979816713\n",
      "Gradient Descent(3/49): loss=29.672086398652795\n",
      "Gradient Descent(4/49): loss=374.61915711118974\n",
      "Gradient Descent(5/49): loss=8811.110349636223\n",
      "Gradient Descent(6/49): loss=284704.1927426606\n",
      "Gradient Descent(7/49): loss=10004866.344286757\n",
      "Gradient Descent(8/49): loss=357698198.30842215\n",
      "Gradient Descent(9/49): loss=12831272914.770964\n",
      "Gradient Descent(10/49): loss=460573728512.4058\n",
      "Gradient Descent(11/49): loss=16534126135010.457\n",
      "Gradient Descent(12/49): loss=593571937164065.8\n",
      "Gradient Descent(13/49): loss=2.1309211688057444e+16\n",
      "Gradient Descent(14/49): loss=7.650005914100067e+17\n",
      "Gradient Descent(15/49): loss=2.7463521726210294e+19\n",
      "Gradient Descent(16/49): loss=9.859404776585459e+20\n",
      "Gradient Descent(17/49): loss=3.5395265064536533e+22\n",
      "Gradient Descent(18/49): loss=1.2706900860212683e+24\n",
      "Gradient Descent(19/49): loss=4.561777661806413e+25\n",
      "Gradient Descent(20/49): loss=1.6376782714773161e+27\n",
      "Gradient Descent(21/49): loss=5.879265320938981e+28\n",
      "Gradient Descent(22/49): loss=2.110656367374548e+30\n",
      "Gradient Descent(23/49): loss=7.577256779471919e+31\n",
      "Gradient Descent(24/49): loss=2.720235334825056e+33\n",
      "Gradient Descent(25/49): loss=9.765645394092822e+34\n",
      "Gradient Descent(26/49): loss=3.505866891082756e+36\n",
      "Gradient Descent(27/49): loss=1.258606283761358e+38\n",
      "Gradient Descent(28/49): loss=4.518396809510204e+39\n",
      "Gradient Descent(29/49): loss=1.6221045446538265e+41\n",
      "Gradient Descent(30/49): loss=5.823355638549795e+42\n",
      "Gradient Descent(31/49): loss=2.0905847902834346e+44\n",
      "Gradient Descent(32/49): loss=7.505199813715791e+45\n",
      "Gradient Descent(33/49): loss=2.694366882682677e+47\n",
      "Gradient Descent(34/49): loss=9.67277764574688e+48\n",
      "Gradient Descent(35/49): loss=3.472527367575916e+50\n",
      "Gradient Descent(36/49): loss=1.246637394158023e+52\n",
      "Gradient Descent(37/49): loss=4.4754284934491885e+53\n",
      "Gradient Descent(38/49): loss=1.6066789183317094e+55\n",
      "Gradient Descent(39/49): loss=5.767977636979573e+56\n",
      "Gradient Descent(40/49): loss=2.0707040866162308e+58\n",
      "Gradient Descent(41/49): loss=7.433828083588739e+59\n",
      "Gradient Descent(42/49): loss=2.668744430144843e+61\n",
      "Gradient Descent(43/49): loss=9.58079303603019e+62\n",
      "Gradient Descent(44/49): loss=3.4395048908546165e+64\n",
      "Gradient Descent(45/49): loss=1.234782324357019e+66\n",
      "Gradient Descent(46/49): loss=4.4328687905011694e+67\n",
      "Gradient Descent(47/49): loss=1.5913999841252845e+69\n",
      "Gradient Descent(48/49): loss=5.713126260133765e+70\n",
      "Gradient Descent(49/49): loss=2.0510124412354927e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9112836330522073\n",
      "Gradient Descent(2/49): loss=4.056218089006367\n",
      "Gradient Descent(3/49): loss=29.98351871022426\n",
      "Gradient Descent(4/49): loss=353.01810035051955\n",
      "Gradient Descent(5/49): loss=7594.506079374614\n",
      "Gradient Descent(6/49): loss=232221.02001848957\n",
      "Gradient Descent(7/49): loss=7870516.085591181\n",
      "Gradient Descent(8/49): loss=272822594.8673452\n",
      "Gradient Descent(9/49): loss=9500321772.059353\n",
      "Gradient Descent(10/49): loss=331124432459.6403\n",
      "Gradient Descent(11/49): loss=11543104342961.754\n",
      "Gradient Descent(12/49): loss=402410776139532.3\n",
      "Gradient Descent(13/49): loss=1.402877267217162e+16\n",
      "Gradient Descent(14/49): loss=4.890692616726811e+17\n",
      "Gradient Descent(15/49): loss=1.7049874248315034e+19\n",
      "Gradient Descent(16/49): loss=5.943907149417198e+20\n",
      "Gradient Descent(17/49): loss=2.0721579571631427e+22\n",
      "Gradient Descent(18/49): loss=7.223932845755657e+23\n",
      "Gradient Descent(19/49): loss=2.5183990246681003e+25\n",
      "Gradient Descent(20/49): loss=8.779613243118182e+26\n",
      "Gradient Descent(21/49): loss=3.060738506687705e+28\n",
      "Gradient Descent(22/49): loss=1.0670310806340138e+30\n",
      "Gradient Descent(23/49): loss=3.719871281234948e+31\n",
      "Gradient Descent(24/49): loss=1.29681717806538e+33\n",
      "Gradient Descent(25/49): loss=4.520948888229175e+34\n",
      "Gradient Descent(26/49): loss=1.5760879170703363e+36\n",
      "Gradient Descent(27/49): loss=5.494539274271804e+37\n",
      "Gradient Descent(28/49): loss=1.9154998594642524e+39\n",
      "Gradient Descent(29/49): loss=6.677793220604265e+40\n",
      "Gradient Descent(30/49): loss=2.328004467179667e+42\n",
      "Gradient Descent(31/49): loss=8.115861962431358e+43\n",
      "Gradient Descent(32/49): loss=2.829342311058326e+45\n",
      "Gradient Descent(33/49): loss=9.863620093837092e+46\n",
      "Gradient Descent(34/49): loss=3.4386437079490897e+48\n",
      "Gradient Descent(35/49): loss=1.198775950181355e+50\n",
      "Gradient Descent(36/49): loss=4.179158705541883e+51\n",
      "Gradient Descent(37/49): loss=1.4569334230858007e+53\n",
      "Gradient Descent(38/49): loss=5.079144270089084e+54\n",
      "Gradient Descent(39/49): loss=1.7706853386435454e+56\n",
      "Gradient Descent(40/49): loss=6.172942530794089e+57\n",
      "Gradient Descent(41/49): loss=2.152004009796444e+59\n",
      "Gradient Descent(42/49): loss=7.502291225096138e+60\n",
      "Gradient Descent(43/49): loss=2.6154399977850374e+62\n",
      "Gradient Descent(44/49): loss=9.1179163495165e+63\n",
      "Gradient Descent(45/49): loss=3.178677340225264e+65\n",
      "Gradient Descent(46/49): loss=1.1081467789290949e+67\n",
      "Gradient Descent(47/49): loss=3.863208348048045e+68\n",
      "Gradient Descent(48/49): loss=1.346787178757175e+70\n",
      "Gradient Descent(49/49): loss=4.695153720562844e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9212193492720437\n",
      "Gradient Descent(2/49): loss=4.10521563519662\n",
      "Gradient Descent(3/49): loss=29.84383450192952\n",
      "Gradient Descent(4/49): loss=337.96711137620093\n",
      "Gradient Descent(5/49): loss=7019.180084285905\n",
      "Gradient Descent(6/49): loss=211988.50861061306\n",
      "Gradient Descent(7/49): loss=7166445.395666425\n",
      "Gradient Descent(8/49): loss=248347959.61287475\n",
      "Gradient Descent(9/49): loss=8649560630.594767\n",
      "Gradient Descent(10/49): loss=301550327774.9524\n",
      "Gradient Descent(11/49): loss=10515044513839.92\n",
      "Gradient Descent(12/49): loss=366673348885527.75\n",
      "Gradient Descent(13/49): loss=1.2786477534373952e+16\n",
      "Gradient Descent(14/49): loss=4.458853015839114e+17\n",
      "Gradient Descent(15/49): loss=1.5548751523352123e+19\n",
      "Gradient Descent(16/49): loss=5.422104910651327e+20\n",
      "Gradient Descent(17/49): loss=1.8907770160246625e+22\n",
      "Gradient Descent(18/49): loss=6.593449945290572e+23\n",
      "Gradient Descent(19/49): loss=2.2992442700866303e+25\n",
      "Gradient Descent(20/49): loss=8.017842340311025e+26\n",
      "Gradient Descent(21/49): loss=2.795953289157065e+28\n",
      "Gradient Descent(22/49): loss=9.74994825711758e+29\n",
      "Gradient Descent(23/49): loss=3.3999670661591944e+31\n",
      "Gradient Descent(24/49): loss=1.1856243485733748e+33\n",
      "Gradient Descent(25/49): loss=4.1344668009329486e+34\n",
      "Gradient Descent(26/49): loss=1.4417564676860363e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=5.027641561048308e+37\n",
      "Gradient Descent(28/49): loss=1.753221173819266e+39\n",
      "Gradient Descent(29/49): loss=6.113770138552361e+40\n",
      "Gradient Descent(30/49): loss=2.131972044669616e+42\n",
      "Gradient Descent(31/49): loss=7.4345366218315445e+43\n",
      "Gradient Descent(32/49): loss=2.5925450063731428e+45\n",
      "Gradient Descent(33/49): loss=9.040630172340696e+46\n",
      "Gradient Descent(34/49): loss=3.152616201921906e+48\n",
      "Gradient Descent(35/49): loss=1.0993690403384046e+50\n",
      "Gradient Descent(36/49): loss=3.8336803766909036e+51\n",
      "Gradient Descent(37/49): loss=1.3368673021845975e+53\n",
      "Gradient Descent(38/49): loss=4.6618758165568256e+54\n",
      "Gradient Descent(39/49): loss=1.6256726522881212e+56\n",
      "Gradient Descent(40/49): loss=5.668987498576183e+57\n",
      "Gradient Descent(41/49): loss=1.9768690341059877e+59\n",
      "Gradient Descent(42/49): loss=6.893666953734869e+60\n",
      "Gradient Descent(43/49): loss=2.4039348712094823e+62\n",
      "Gradient Descent(44/49): loss=8.382915658387015e+63\n",
      "Gradient Descent(45/49): loss=2.9232603502387784e+65\n",
      "Gradient Descent(46/49): loss=1.0193888884863958e+67\n",
      "Gradient Descent(47/49): loss=3.5547764532318685e+68\n",
      "Gradient Descent(48/49): loss=1.2396089240500137e+70\n",
      "Gradient Descent(49/49): loss=4.322719880704067e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9008815136317092\n",
      "Gradient Descent(2/49): loss=3.8972470575385243\n",
      "Gradient Descent(3/49): loss=26.98212046447379\n",
      "Gradient Descent(4/49): loss=272.3856408212541\n",
      "Gradient Descent(5/49): loss=5029.516886927299\n",
      "Gradient Descent(6/49): loss=146449.97862032914\n",
      "Gradient Descent(7/49): loss=4968690.875713263\n",
      "Gradient Descent(8/49): loss=174463658.77705488\n",
      "Gradient Descent(9/49): loss=6168135446.921397\n",
      "Gradient Descent(10/49): loss=218366595725.87894\n",
      "Gradient Descent(11/49): loss=7732713598092.22\n",
      "Gradient Descent(12/49): loss=273841759627708.1\n",
      "Gradient Descent(13/49): loss=9697766130698868.0\n",
      "Gradient Descent(14/49): loss=3.4343501137461485e+17\n",
      "Gradient Descent(15/49): loss=1.2162352573046618e+19\n",
      "Gradient Descent(16/49): loss=4.3071564017212064e+20\n",
      "Gradient Descent(17/49): loss=1.525329611795212e+22\n",
      "Gradient Descent(18/49): loss=5.401778375214042e+23\n",
      "Gradient Descent(19/49): loss=1.9129773266515476e+25\n",
      "Gradient Descent(20/49): loss=6.774587919877832e+26\n",
      "Gradient Descent(21/49): loss=2.3991419472494535e+28\n",
      "Gradient Descent(22/49): loss=8.496283687138351e+29\n",
      "Gradient Descent(23/49): loss=3.008860587642467e+31\n",
      "Gradient Descent(24/49): loss=1.0655531723327783e+33\n",
      "Gradient Descent(25/49): loss=3.773533302711433e+34\n",
      "Gradient Descent(26/49): loss=1.3363531690772717e+36\n",
      "Gradient Descent(27/49): loss=4.732540166585244e+37\n",
      "Gradient Descent(28/49): loss=1.6759743566745044e+39\n",
      "Gradient Descent(29/49): loss=5.935269317021564e+40\n",
      "Gradient Descent(30/49): loss=2.101906972817713e+42\n",
      "Gradient Descent(31/49): loss=7.443660407640515e+43\n",
      "Gradient Descent(32/49): loss=2.636086229353736e+45\n",
      "Gradient Descent(33/49): loss=9.33539445385747e+46\n",
      "Gradient Descent(34/49): loss=3.3060219593225575e+48\n",
      "Gradient Descent(35/49): loss=1.1707894347202947e+50\n",
      "Gradient Descent(36/49): loss=4.1462153528271154e+51\n",
      "Gradient Descent(37/49): loss=1.4683342061526554e+53\n",
      "Gradient Descent(38/49): loss=5.1999357425750575e+54\n",
      "Gradient Descent(39/49): loss=1.8414970933462698e+56\n",
      "Gradient Descent(40/49): loss=6.521448942219774e+57\n",
      "Gradient Descent(41/49): loss=2.3094957064905278e+59\n",
      "Gradient Descent(42/49): loss=8.178811895263526e+60\n",
      "Gradient Descent(43/49): loss=2.8964316248829476e+62\n",
      "Gradient Descent(44/49): loss=1.0257377556855625e+64\n",
      "Gradient Descent(45/49): loss=3.632531610275355e+65\n",
      "Gradient Descent(46/49): loss=1.2864190507280714e+67\n",
      "Gradient Descent(47/49): loss=4.555704262545028e+68\n",
      "Gradient Descent(48/49): loss=1.6133499667954265e+70\n",
      "Gradient Descent(49/49): loss=5.713492284296479e+71\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9190691027377444\n",
      "Gradient Descent(2/49): loss=4.153000963929632\n",
      "Gradient Descent(3/49): loss=31.838799969186454\n",
      "Gradient Descent(4/49): loss=409.6300053010592\n",
      "Gradient Descent(5/49): loss=9796.513807297284\n",
      "Gradient Descent(6/49): loss=322605.9085411022\n",
      "Gradient Descent(7/49): loss=11568853.224521391\n",
      "Gradient Descent(8/49): loss=422232881.6753248\n",
      "Gradient Descent(9/49): loss=15463136434.050274\n",
      "Gradient Descent(10/49): loss=566666558364.2999\n",
      "Gradient Descent(11/49): loss=20768827633601.395\n",
      "Gradient Descent(12/49): loss=761214022562594.0\n",
      "Gradient Descent(13/49): loss=2.7899959042842268e+16\n",
      "Gradient Descent(14/49): loss=1.0225880928706938e+18\n",
      "Gradient Descent(15/49): loss=3.747986084630337e+19\n",
      "Gradient Descent(16/49): loss=1.3737105133057435e+21\n",
      "Gradient Descent(17/49): loss=5.034918869187063e+22\n",
      "Gradient Descent(18/49): loss=1.8453966686106625e+24\n",
      "Gradient Descent(19/49): loss=6.763741291087357e+25\n",
      "Gradient Descent(20/49): loss=2.4790440468906727e+27\n",
      "Gradient Descent(21/49): loss=9.08618340351425e+28\n",
      "Gradient Descent(22/49): loss=3.3302647020723036e+30\n",
      "Gradient Descent(23/49): loss=1.2206074314527362e+32\n",
      "Gradient Descent(24/49): loss=4.4737660066199165e+33\n",
      "Gradient Descent(25/49): loss=1.6397231219678344e+35\n",
      "Gradient Descent(26/49): loss=6.00990734146023e+36\n",
      "Gradient Descent(27/49): loss=2.202749096419995e+38\n",
      "Gradient Descent(28/49): loss=8.073508135984572e+39\n",
      "Gradient Descent(29/49): loss=2.959099324010385e+41\n",
      "Gradient Descent(30/49): loss=1.0845680293961366e+43\n",
      "Gradient Descent(31/49): loss=3.9751548751464684e+44\n",
      "Gradient Descent(32/49): loss=1.4569723478018096e+46\n",
      "Gradient Descent(33/49): loss=5.3400898554447355e+47\n",
      "Gradient Descent(34/49): loss=1.9572478302177763e+49\n",
      "Gradient Descent(35/49): loss=7.173697770246876e+50\n",
      "Gradient Descent(36/49): loss=2.629301149518611e+52\n",
      "Gradient Descent(37/49): loss=9.636905200457106e+53\n",
      "Gradient Descent(38/49): loss=3.532115058771488e+55\n",
      "Gradient Descent(39/49): loss=1.2945895522359503e+57\n",
      "Gradient Descent(40/49): loss=4.744925011988157e+58\n",
      "Gradient Descent(41/49): loss=1.7391082239544398e+60\n",
      "Gradient Descent(42/49): loss=6.374173262979944e+61\n",
      "Gradient Descent(43/49): loss=2.3362597121243045e+63\n",
      "Gradient Descent(44/49): loss=8.56285076873417e+64\n",
      "Gradient Descent(45/49): loss=3.1384530113280975e+66\n",
      "Gradient Descent(46/49): loss=1.1503046789370182e+68\n",
      "Gradient Descent(47/49): loss=4.216092608710013e+69\n",
      "Gradient Descent(48/49): loss=1.5452807600196625e+71\n",
      "Gradient Descent(49/49): loss=5.663757533109581e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9246633726230472\n",
      "Gradient Descent(2/49): loss=4.240671290312075\n",
      "Gradient Descent(3/49): loss=32.179867538485404\n",
      "Gradient Descent(4/49): loss=386.31610850190214\n",
      "Gradient Descent(5/49): loss=8449.956382586839\n",
      "Gradient Descent(6/49): loss=263252.0727238053\n",
      "Gradient Descent(7/49): loss=9104087.391948279\n",
      "Gradient Descent(8/49): loss=322160749.46598667\n",
      "Gradient Descent(9/49): loss=11453491938.57606\n",
      "Gradient Descent(10/49): loss=407576655718.98413\n",
      "Gradient Descent(11/49): loss=14506468288222.076\n",
      "Gradient Descent(12/49): loss=516333414774709.9\n",
      "Gradient Descent(13/49): loss=1.837815831668337e+16\n",
      "Gradient Descent(14/49): loss=6.541455429469146e+17\n",
      "Gradient Descent(15/49): loss=2.3283427530610483e+19\n",
      "Gradient Descent(16/49): loss=8.28742219865055e+20\n",
      "Gradient Descent(17/49): loss=2.9497962622307963e+22\n",
      "Gradient Descent(18/49): loss=1.0499402347770169e+24\n",
      "Gradient Descent(19/49): loss=3.737120801291888e+25\n",
      "Gradient Descent(20/49): loss=1.3301777969969346e+27\n",
      "Gradient Descent(21/49): loss=4.734588646544618e+28\n",
      "Gradient Descent(22/49): loss=1.6852130371353096e+30\n",
      "Gradient Descent(23/49): loss=5.998288748074354e+31\n",
      "Gradient Descent(24/49): loss=2.1350100617807838e+33\n",
      "Gradient Descent(25/49): loss=7.599280653784298e+34\n",
      "Gradient Descent(26/49): loss=2.704861559613156e+36\n",
      "Gradient Descent(27/49): loss=9.627590281231804e+37\n",
      "Gradient Descent(28/49): loss=3.4268110430215985e+39\n",
      "Gradient Descent(29/49): loss=1.2197272195377115e+41\n",
      "Gradient Descent(30/49): loss=4.341454697686846e+42\n",
      "Gradient Descent(31/49): loss=1.5452823049411955e+44\n",
      "Gradient Descent(32/49): loss=5.500224160433324e+45\n",
      "Gradient Descent(33/49): loss=1.95773068249598e+47\n",
      "Gradient Descent(34/49): loss=6.968278589002544e+48\n",
      "Gradient Descent(35/49): loss=2.48026487647641e+50\n",
      "Gradient Descent(36/49): loss=8.828168648698217e+51\n",
      "Gradient Descent(37/49): loss=3.1422676839490956e+53\n",
      "Gradient Descent(38/49): loss=1.1184478446780157e+55\n",
      "Gradient Descent(39/49): loss=3.980964408776172e+56\n",
      "Gradient Descent(40/49): loss=1.4169706436785433e+58\n",
      "Gradient Descent(41/49): loss=5.0435160902733405e+59\n",
      "Gradient Descent(42/49): loss=1.7951715983903298e+61\n",
      "Gradient Descent(43/49): loss=6.389671431567988e+62\n",
      "Gradient Descent(44/49): loss=2.2743174546658417e+64\n",
      "Gradient Descent(45/49): loss=8.095126549141585e+65\n",
      "Gradient Descent(46/49): loss=2.88135122527326e+67\n",
      "Gradient Descent(47/49): loss=1.0255781466769051e+69\n",
      "Gradient Descent(48/49): loss=3.6504072315636424e+70\n",
      "Gradient Descent(49/49): loss=1.2993132702203147e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9348944880384007\n",
      "Gradient Descent(2/49): loss=4.292401942705094\n",
      "Gradient Descent(3/49): loss=32.03642256808369\n",
      "Gradient Descent(4/49): loss=369.9985358973841\n",
      "Gradient Descent(5/49): loss=7811.88348861038\n",
      "Gradient Descent(6/49): loss=240334.51296692915\n",
      "Gradient Descent(7/49): loss=8289796.746483469\n",
      "Gradient Descent(8/49): loss=293260264.71559757\n",
      "Gradient Descent(9/49): loss=10427792770.519497\n",
      "Gradient Descent(10/49): loss=371172797928.58856\n",
      "Gradient Descent(11/49): loss=13214422941612.135\n",
      "Gradient Descent(12/49): loss=470476347701290.06\n",
      "Gradient Descent(13/49): loss=1.6750619228068838e+16\n",
      "Gradient Descent(14/49): loss=5.9638213909126e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=2.1233350434076246e+19\n",
      "Gradient Descent(16/49): loss=7.559837381744063e+20\n",
      "Gradient Descent(17/49): loss=2.6915743761758524e+22\n",
      "Gradient Descent(18/49): loss=9.582974175940025e+23\n",
      "Gradient Descent(19/49): loss=3.411883948463072e+25\n",
      "Gradient Descent(20/49): loss=1.214753568692478e+27\n",
      "Gradient Descent(21/49): loss=4.324960212507377e+28\n",
      "Gradient Descent(22/49): loss=1.5398416042504387e+30\n",
      "Gradient Descent(23/49): loss=5.482390703445985e+31\n",
      "Gradient Descent(24/49): loss=1.95192854526526e+33\n",
      "Gradient Descent(25/49): loss=6.949568631485147e+34\n",
      "Gradient Descent(26/49): loss=2.4742967298098406e+36\n",
      "Gradient Descent(27/49): loss=8.809387505594681e+37\n",
      "Gradient Descent(28/49): loss=3.136459232587342e+39\n",
      "Gradient Descent(29/49): loss=1.116692450120375e+41\n",
      "Gradient Descent(30/49): loss=3.9758273125302624e+42\n",
      "Gradient Descent(31/49): loss=1.4155377174223563e+44\n",
      "Gradient Descent(32/49): loss=5.0398240968119264e+45\n",
      "Gradient Descent(33/49): loss=1.7943588937395236e+47\n",
      "Gradient Descent(34/49): loss=6.388563921464884e+48\n",
      "Gradient Descent(35/49): loss=2.274558847789064e+50\n",
      "Gradient Descent(36/49): loss=8.098248707620401e+51\n",
      "Gradient Descent(37/49): loss=2.8832682079966395e+53\n",
      "Gradient Descent(38/49): loss=1.0265473263894054e+55\n",
      "Gradient Descent(39/49): loss=3.6548782052068024e+56\n",
      "Gradient Descent(40/49): loss=1.3012682758503824e+58\n",
      "Gradient Descent(41/49): loss=4.6329837293137335e+59\n",
      "Gradient Descent(42/49): loss=1.6495090700692395e+61\n",
      "Gradient Descent(43/49): loss=5.87284637980748e+62\n",
      "Gradient Descent(44/49): loss=2.0909448287768125e+64\n",
      "Gradient Descent(45/49): loss=7.444516669158737e+65\n",
      "Gradient Descent(46/49): loss=2.6505160573654457e+67\n",
      "Gradient Descent(47/49): loss=9.43679177918486e+68\n",
      "Gradient Descent(48/49): loss=3.3598377506983984e+70\n",
      "Gradient Descent(49/49): loss=1.19622324781161e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9139976246119872\n",
      "Gradient Descent(2/49): loss=4.074308321581047\n",
      "Gradient Descent(3/49): loss=28.976851092512504\n",
      "Gradient Descent(4/49): loss=298.5897003576565\n",
      "Gradient Descent(5/49): loss=5602.707445151843\n",
      "Gradient Descent(6/49): loss=166063.83126421014\n",
      "Gradient Descent(7/49): loss=5747053.447723944\n",
      "Gradient Descent(8/49): loss=205979880.4177444\n",
      "Gradient Descent(9/49): loss=7434706565.174368\n",
      "Gradient Descent(10/49): loss=268721837629.08258\n",
      "Gradient Descent(11/49): loss=9715366858685.182\n",
      "Gradient Descent(12/49): loss=351267752552618.9\n",
      "Gradient Descent(13/49): loss=1.27005290052356e+16\n",
      "Gradient Descent(14/49): loss=4.5920457566600435e+17\n",
      "Gradient Descent(15/49): loss=1.6603160729685109e+19\n",
      "Gradient Descent(16/49): loss=6.003097132468632e+20\n",
      "Gradient Descent(17/49): loss=2.170500924598239e+22\n",
      "Gradient Descent(18/49): loss=7.847739547287167e+23\n",
      "Gradient Descent(19/49): loss=2.837456337323406e+25\n",
      "Gradient Descent(20/49): loss=1.0259207022080816e+27\n",
      "Gradient Descent(21/49): loss=3.709355007148619e+28\n",
      "Gradient Descent(22/49): loss=1.3411674547069563e+30\n",
      "Gradient Descent(23/49): loss=4.84917226337217e+31\n",
      "Gradient Descent(24/49): loss=1.7532837944534972e+33\n",
      "Gradient Descent(25/49): loss=6.339234609404497e+34\n",
      "Gradient Descent(26/49): loss=2.2920359818644864e+36\n",
      "Gradient Descent(27/49): loss=8.2871659843095965e+37\n",
      "Gradient Descent(28/49): loss=2.9963369072256746e+39\n",
      "Gradient Descent(29/49): loss=1.0833661204084756e+41\n",
      "Gradient Descent(30/49): loss=3.917056683507801e+42\n",
      "Gradient Descent(31/49): loss=1.4162648040006664e+44\n",
      "Gradient Descent(32/49): loss=5.12069688318833e+45\n",
      "Gradient Descent(33/49): loss=1.8514571918630106e+47\n",
      "Gradient Descent(34/49): loss=6.694193801150977e+48\n",
      "Gradient Descent(35/49): loss=2.42037627682203e+50\n",
      "Gradient Descent(36/49): loss=8.751197672818578e+51\n",
      "Gradient Descent(37/49): loss=3.1641138380888715e+53\n",
      "Gradient Descent(38/49): loss=1.1440281381692393e+55\n",
      "Gradient Descent(39/49): loss=4.136388410454584e+56\n",
      "Gradient Descent(40/49): loss=1.495567155325672e+58\n",
      "Gradient Descent(41/49): loss=5.407425256379873e+59\n",
      "Gradient Descent(42/49): loss=1.955127711865821e+61\n",
      "Gradient Descent(43/49): loss=7.069028582865065e+62\n",
      "Gradient Descent(44/49): loss=2.555902860057871e+64\n",
      "Gradient Descent(45/49): loss=9.24121235821129e+65\n",
      "Gradient Descent(46/49): loss=3.341285272775265e+67\n",
      "Gradient Descent(47/49): loss=1.2080868658045011e+69\n",
      "Gradient Descent(48/49): loss=4.368001401200536e+70\n",
      "Gradient Descent(49/49): loss=1.5793099636245667e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9324911666671253\n",
      "Gradient Descent(2/49): loss=4.340160365576078\n",
      "Gradient Descent(3/49): loss=34.13799995610829\n",
      "Gradient Descent(4/49): loss=447.49538724516003\n",
      "Gradient Descent(5/49): loss=10880.39575468663\n",
      "Gradient Descent(6/49): loss=365087.1164298831\n",
      "Gradient Descent(7/49): loss=13357434.732572535\n",
      "Gradient Descent(8/49): loss=497563600.1323392\n",
      "Gradient Descent(9/49): loss=18599199883.832005\n",
      "Gradient Descent(10/49): loss=695716743002.2626\n",
      "Gradient Descent(11/49): loss=26027161364397.945\n",
      "Gradient Descent(12/49): loss=973715154048125.0\n",
      "Gradient Descent(13/49): loss=3.642831813247487e+16\n",
      "Gradient Descent(14/49): loss=1.3628457648029806e+18\n",
      "Gradient Descent(15/49): loss=5.098640286656511e+19\n",
      "Gradient Descent(16/49): loss=1.9074890437277773e+21\n",
      "Gradient Descent(17/49): loss=7.136244706117896e+22\n",
      "Gradient Descent(18/49): loss=2.6697919307089704e+24\n",
      "Gradient Descent(19/49): loss=9.988150979268186e+25\n",
      "Gradient Descent(20/49): loss=3.736739138420567e+27\n",
      "Gradient Descent(21/49): loss=1.3979784063845751e+29\n",
      "Gradient Descent(22/49): loss=5.230077755834248e+30\n",
      "Gradient Descent(23/49): loss=1.9566620776941966e+32\n",
      "Gradient Descent(24/49): loss=7.320209497872151e+33\n",
      "Gradient Descent(25/49): loss=2.7386163254049025e+35\n",
      "Gradient Descent(26/49): loss=1.0245634882382099e+37\n",
      "Gradient Descent(27/49): loss=3.8330682969094575e+38\n",
      "Gradient Descent(28/49): loss=1.43401680202725e+40\n",
      "Gradient Descent(29/49): loss=5.364903594737575e+41\n",
      "Gradient Descent(30/49): loss=2.0071027438549202e+43\n",
      "Gradient Descent(31/49): loss=7.508916708850603e+44\n",
      "Gradient Descent(32/49): loss=2.809214939946871e+46\n",
      "Gradient Descent(33/49): loss=1.0509756446651011e+48\n",
      "Gradient Descent(34/49): loss=3.931880718604279e+49\n",
      "Gradient Descent(35/49): loss=1.4709842291596355e+51\n",
      "Gradient Descent(36/49): loss=5.503205100292107e+52\n",
      "Gradient Descent(37/49): loss=2.058843716712195e+54\n",
      "Gradient Descent(38/49): loss=7.702488590912944e+55\n",
      "Gradient Descent(39/49): loss=2.8816335116434776e+57\n",
      "Gradient Descent(40/49): loss=1.0780686783779148e+59\n",
      "Gradient Descent(41/49): loss=4.033240419378276e+60\n",
      "Gradient Descent(42/49): loss=1.5089046372242278e+62\n",
      "Gradient Descent(43/49): loss=5.6450718714848964e+63\n",
      "Gradient Descent(44/49): loss=2.111918516789263e+65\n",
      "Gradient Descent(45/49): loss=7.901050550103962e+66\n",
      "Gradient Descent(46/49): loss=2.9559189570535314e+68\n",
      "Gradient Descent(47/49): loss=1.1058601416685606e+70\n",
      "Gradient Descent(48/49): loss=4.13721306537531e+71\n",
      "Gradient Descent(49/49): loss=1.5478025930554173e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9382149274908032\n",
      "Gradient Descent(2/49): loss=4.43198293601264\n",
      "Gradient Descent(3/49): loss=34.510822534171965\n",
      "Gradient Descent(4/49): loss=422.3531330703032\n",
      "Gradient Descent(5/49): loss=9391.549350786994\n",
      "Gradient Descent(6/49): loss=298047.3132170657\n",
      "Gradient Descent(7/49): loss=10515275.959943559\n",
      "Gradient Descent(8/49): loss=379772142.32382476\n",
      "Gradient Descent(9/49): loss=13781702578.557314\n",
      "Gradient Descent(10/49): loss=500610507931.91766\n",
      "Gradient Descent(11/49): loss=18187817902824.223\n",
      "Gradient Descent(12/49): loss=660812041696784.0\n",
      "Gradient Descent(13/49): loss=2.4009252714371076e+16\n",
      "Gradient Descent(14/49): loss=8.72328388798985e+17\n",
      "Gradient Descent(15/49): loss=3.169432474087373e+19\n",
      "Gradient Descent(16/49): loss=1.1515506036212231e+21\n",
      "Gradient Descent(17/49): loss=4.183931413230819e+22\n",
      "Gradient Descent(18/49): loss=1.5201487506263492e+24\n",
      "Gradient Descent(19/49): loss=5.523159910946198e+25\n",
      "Gradient Descent(20/49): loss=2.0067309460553468e+27\n",
      "Gradient Descent(21/49): loss=7.291060108461921e+28\n",
      "Gradient Descent(22/49): loss=2.6490625267881357e+30\n",
      "Gradient Descent(23/49): loss=9.62484490107886e+31\n",
      "Gradient Descent(24/49): loss=3.496997086065714e+33\n",
      "Gradient Descent(25/49): loss=1.2705647463037268e+35\n",
      "Gradient Descent(26/49): loss=4.6163457813060396e+36\n",
      "Gradient Descent(27/49): loss=1.6772579622232943e+38\n",
      "Gradient Descent(28/49): loss=6.093985167301794e+39\n",
      "Gradient Descent(29/49): loss=2.21412901627053e+41\n",
      "Gradient Descent(30/49): loss=8.044599988519077e+42\n",
      "Gradient Descent(31/49): loss=2.922846342725242e+44\n",
      "Gradient Descent(32/49): loss=1.0619584261957825e+46\n",
      "Gradient Descent(33/49): loss=3.85841596420257e+47\n",
      "Gradient Descent(34/49): loss=1.401879149463869e+49\n",
      "Gradient Descent(35/49): loss=5.093450700844029e+50\n",
      "Gradient Descent(36/49): loss=1.8506046011062835e+52\n",
      "Gradient Descent(37/49): loss=6.723805904449518e+53\n",
      "Gradient Descent(38/49): loss=2.4429619278847932e+55\n",
      "Gradient Descent(39/49): loss=8.876019126526583e+56\n",
      "Gradient Descent(40/49): loss=3.2249260471561104e+58\n",
      "Gradient Descent(41/49): loss=1.1717131138828552e+60\n",
      "Gradient Descent(42/49): loss=4.257187920497428e+61\n",
      "Gradient Descent(43/49): loss=1.5467650550031024e+63\n",
      "Gradient Descent(44/49): loss=5.619864990830025e+64\n",
      "Gradient Descent(45/49): loss=2.0418668247644132e+66\n",
      "Gradient Descent(46/49): loss=7.418719376491034e+67\n",
      "Gradient Descent(47/49): loss=2.6954449976664916e+69\n",
      "Gradient Descent(48/49): loss=9.79336643797101e+70\n",
      "Gradient Descent(49/49): loss=3.558226054377301e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9487451002865147\n",
      "Gradient Descent(2/49): loss=4.486555686626114\n",
      "Gradient Descent(3/49): loss=34.36374814355974\n",
      "Gradient Descent(4/49): loss=404.67740603153015\n",
      "Gradient Descent(5/49): loss=8684.64226717815\n",
      "Gradient Descent(6/49): loss=272121.64274590544\n",
      "Gradient Descent(7/49): loss=9574918.270880537\n",
      "Gradient Descent(8/49): loss=345703740.22869515\n",
      "Gradient Descent(9/49): loss=12547469070.528263\n",
      "Gradient Descent(10/49): loss=455895284567.47974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=16567812239645.357\n",
      "Gradient Descent(12/49): loss=602120449367591.2\n",
      "Gradient Descent(13/49): loss=2.1882916604043028e+16\n",
      "Gradient Descent(14/49): loss=7.952940871246227e+17\n",
      "Gradient Descent(15/49): loss=2.8903500633017856e+19\n",
      "Gradient Descent(16/49): loss=1.0504446308964008e+21\n",
      "Gradient Descent(17/49): loss=3.817648080241425e+22\n",
      "Gradient Descent(18/49): loss=1.3874540812471993e+24\n",
      "Gradient Descent(19/49): loss=5.042447046215492e+25\n",
      "Gradient Descent(20/49): loss=1.8325847723669772e+27\n",
      "Gradient Descent(21/49): loss=6.660192793697729e+28\n",
      "Gradient Descent(22/49): loss=2.4205247537941043e+30\n",
      "Gradient Descent(23/49): loss=8.79695267872582e+31\n",
      "Gradient Descent(24/49): loss=3.19709089157028e+33\n",
      "Gradient Descent(25/49): loss=1.1619239687035019e+35\n",
      "Gradient Descent(26/49): loss=4.2227992723240897e+36\n",
      "Gradient Descent(27/49): loss=1.5346988421487056e+38\n",
      "Gradient Descent(28/49): loss=5.577581088281957e+39\n",
      "Gradient Descent(29/49): loss=2.0270694120551278e+41\n",
      "Gradient Descent(30/49): loss=7.367011498805634e+42\n",
      "Gradient Descent(31/49): loss=2.6774050311632382e+44\n",
      "Gradient Descent(32/49): loss=9.73053686974763e+45\n",
      "Gradient Descent(33/49): loss=3.5363849201545726e+47\n",
      "Gradient Descent(34/49): loss=1.285234152123535e+49\n",
      "Gradient Descent(35/49): loss=4.670947487561583e+50\n",
      "Gradient Descent(36/49): loss=1.6975700805576782e+52\n",
      "Gradient Descent(37/49): loss=6.169506692332791e+53\n",
      "Gradient Descent(38/49): loss=2.2421939018997596e+55\n",
      "Gradient Descent(39/49): loss=8.148841948683196e+56\n",
      "Gradient Descent(40/49): loss=2.9615469495461137e+58\n",
      "Gradient Descent(41/49): loss=1.0763198488324037e+60\n",
      "Gradient Descent(42/49): loss=3.911686820187494e+61\n",
      "Gradient Descent(43/49): loss=1.4216307351227631e+63\n",
      "Gradient Descent(44/49): loss=5.166655818700874e+64\n",
      "Gradient Descent(45/49): loss=1.877726169630867e+66\n",
      "Gradient Descent(46/49): loss=6.824250911691339e+67\n",
      "Gradient Descent(47/49): loss=2.480148663789207e+69\n",
      "Gradient Descent(48/49): loss=9.013644829438286e+70\n",
      "Gradient Descent(49/49): loss=3.275843674109839e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.927282528720609\n",
      "Gradient Descent(2/49): loss=4.257975190590964\n",
      "Gradient Descent(3/49): loss=31.09499038387597\n",
      "Gradient Descent(4/49): loss=326.991598670322\n",
      "Gradient Descent(5/49): loss=6234.372387823139\n",
      "Gradient Descent(6/49): loss=188063.66888625425\n",
      "Gradient Descent(7/49): loss=6637448.319664617\n",
      "Gradient Descent(8/49): loss=242775326.7038236\n",
      "Gradient Descent(9/49): loss=8944189981.476862\n",
      "Gradient Descent(10/49): loss=329985143972.57526\n",
      "Gradient Descent(11/49): loss=12177793493718.777\n",
      "Gradient Descent(12/49): loss=449434565024649.94\n",
      "Gradient Descent(13/49): loss=1.6587042657047588e+16\n",
      "Gradient Descent(14/49): loss=6.121704447363494e+17\n",
      "Gradient Descent(15/49): loss=2.259310559268357e+19\n",
      "Gradient Descent(16/49): loss=8.338338863511418e+20\n",
      "Gradient Descent(17/49): loss=3.077394376026732e+22\n",
      "Gradient Descent(18/49): loss=1.1357605309087264e+24\n",
      "Gradient Descent(19/49): loss=4.191701896869429e+25\n",
      "Gradient Descent(20/49): loss=1.5470131525185275e+27\n",
      "Gradient Descent(21/49): loss=5.709494026537058e+28\n",
      "Gradient Descent(22/49): loss=2.1071780796523477e+30\n",
      "Gradient Descent(23/49): loss=7.776870312384381e+31\n",
      "Gradient Descent(24/49): loss=2.87017563630096e+33\n",
      "Gradient Descent(25/49): loss=1.0592832144953101e+35\n",
      "Gradient Descent(26/49): loss=3.909450398504704e+36\n",
      "Gradient Descent(27/49): loss=1.4428438220510028e+38\n",
      "Gradient Descent(28/49): loss=5.325040818082718e+39\n",
      "Gradient Descent(29/49): loss=1.9652896093728904e+41\n",
      "Gradient Descent(30/49): loss=7.253208718313005e+42\n",
      "Gradient Descent(31/49): loss=2.6769101337792454e+44\n",
      "Gradient Descent(32/49): loss=9.879555576882022e+45\n",
      "Gradient Descent(33/49): loss=3.646204523829225e+47\n",
      "Gradient Descent(34/49): loss=1.34568881425216e+49\n",
      "Gradient Descent(35/49): loss=4.96647506460109e+50\n",
      "Gradient Descent(36/49): loss=1.832955309286146e+52\n",
      "Gradient Descent(37/49): loss=6.764808283820775e+53\n",
      "Gradient Descent(38/49): loss=2.4966583137627865e+55\n",
      "Gradient Descent(39/49): loss=9.21430803972528e+56\n",
      "Gradient Descent(40/49): loss=3.400684514293163e+58\n",
      "Gradient Descent(41/49): loss=1.2550758142549027e+60\n",
      "Gradient Descent(42/49): loss=4.632053614226514e+61\n",
      "Gradient Descent(43/49): loss=1.7095318419315424e+63\n",
      "Gradient Descent(44/49): loss=6.309294671378421e+64\n",
      "Gradient Descent(45/49): loss=2.3285438898468864e+66\n",
      "Gradient Descent(46/49): loss=8.593855461435885e+67\n",
      "Gradient Descent(47/49): loss=3.1716967850198496e+69\n",
      "Gradient Descent(48/49): loss=1.1705643108901364e+71\n",
      "Gradient Descent(49/49): loss=4.3201506915836885e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9460836426631543\n",
      "Gradient Descent(2/49): loss=4.534203618297644\n",
      "Gradient Descent(3/49): loss=36.57611518359521\n",
      "Gradient Descent(4/49): loss=488.41399203427216\n",
      "Gradient Descent(5/49): loss=12071.445914354532\n",
      "Gradient Descent(6/49): loss=412645.9829084913\n",
      "Gradient Descent(7/49): loss=15400068.431923168\n",
      "Gradient Descent(8/49): loss=585357792.0862826\n",
      "Gradient Descent(9/49): loss=22329378953.074932\n",
      "Gradient Descent(10/49): loss=852378590157.5306\n",
      "Gradient Descent(11/49): loss=32542157789768.74\n",
      "Gradient Descent(12/49): loss=1242428243277324.2\n",
      "Gradient Descent(13/49): loss=4.74349483589936e+16\n",
      "Gradient Descent(14/49): loss=1.8110313258083028e+18\n",
      "Gradient Descent(15/49): loss=6.914385232745568e+19\n",
      "Gradient Descent(16/49): loss=2.6398618374835e+21\n",
      "Gradient Descent(17/49): loss=1.0078799971078102e+23\n",
      "Gradient Descent(18/49): loss=3.848012329570271e+24\n",
      "Gradient Descent(19/49): loss=1.4691430462701801e+26\n",
      "Gradient Descent(20/49): loss=5.609081015470346e+27\n",
      "Gradient Descent(21/49): loss=2.1415062282925505e+29\n",
      "Gradient Descent(22/49): loss=8.176114613383672e+30\n",
      "Gradient Descent(23/49): loss=3.121580936260393e+32\n",
      "Gradient Descent(24/49): loss=1.1917968377882662e+34\n",
      "Gradient Descent(25/49): loss=4.550193416620976e+35\n",
      "Gradient Descent(26/49): loss=1.7372306648407744e+37\n",
      "Gradient Descent(27/49): loss=6.632619993337228e+38\n",
      "Gradient Descent(28/49): loss=2.5322859460374216e+40\n",
      "Gradient Descent(29/49): loss=9.668083078693463e+41\n",
      "Gradient Descent(30/49): loss=3.691203616352509e+43\n",
      "Gradient Descent(31/49): loss=1.4092746231567473e+45\n",
      "Gradient Descent(32/49): loss=5.380507741905926e+46\n",
      "Gradient Descent(33/49): loss=2.054238619287909e+48\n",
      "Gradient Descent(34/49): loss=7.842933246071198e+49\n",
      "Gradient Descent(35/49): loss=2.994374719897579e+51\n",
      "Gradient Descent(36/49): loss=1.1432304320138227e+53\n",
      "Gradient Descent(37/49): loss=4.364770421008725e+54\n",
      "Gradient Descent(38/49): loss=1.6664375172863405e+56\n",
      "Gradient Descent(39/49): loss=6.362336918461698e+57\n",
      "Gradient Descent(40/49): loss=2.4290938390500856e+59\n",
      "Gradient Descent(41/49): loss=9.27410313935057e+60\n",
      "Gradient Descent(42/49): loss=3.540784948552826e+62\n",
      "Gradient Descent(43/49): loss=1.3518458726971335e+64\n",
      "Gradient Descent(44/49): loss=5.161248960559399e+65\n",
      "Gradient Descent(45/49): loss=1.9705272154826008e+67\n",
      "Gradient Descent(46/49): loss=7.523329210875436e+68\n",
      "Gradient Descent(47/49): loss=2.872352229925796e+70\n",
      "Gradient Descent(48/49): loss=1.096643135173911e+72\n",
      "Gradient Descent(49/49): loss=4.18690351898495e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9519382976554744\n",
      "Gradient Descent(2/49): loss=4.63032120382794\n",
      "Gradient Descent(3/49): loss=36.98292626612865\n",
      "Gradient Descent(4/49): loss=461.32141201508716\n",
      "Gradient Descent(5/49): loss=10426.947133840738\n",
      "Gradient Descent(6/49): loss=337017.9307100125\n",
      "Gradient Descent(7/49): loss=12127440.917451585\n",
      "Gradient Descent(8/49): loss=446937566.9593761\n",
      "Gradient Descent(9/49): loss=16551988507.110468\n",
      "Gradient Descent(10/49): loss=613595378689.1981\n",
      "Gradient Descent(11/49): loss=22750979337208.074\n",
      "Gradient Descent(12/49): loss=843597727829106.2\n",
      "Gradient Descent(13/49): loss=3.1280535371284264e+16\n",
      "Gradient Descent(14/49): loss=1.1598815769797832e+18\n",
      "Gradient Descent(15/49): loss=4.300839771171735e+19\n",
      "Gradient Descent(16/49): loss=1.5947510763379887e+21\n",
      "Gradient Descent(17/49): loss=5.913335915311162e+22\n",
      "Gradient Descent(18/49): loss=2.1926645642216282e+24\n",
      "Gradient Descent(19/49): loss=8.130398750490577e+25\n",
      "Gradient Descent(20/49): loss=3.0147513179875625e+27\n",
      "Gradient Descent(21/49): loss=1.117869588985517e+29\n",
      "Gradient Descent(22/49): loss=4.145059695398427e+30\n",
      "Gradient Descent(23/49): loss=1.536987860455457e+32\n",
      "Gradient Descent(24/49): loss=5.69914996835969e+33\n",
      "Gradient Descent(25/49): loss=2.1132444307160035e+35\n",
      "Gradient Descent(26/49): loss=7.835908949133181e+36\n",
      "Gradient Descent(27/49): loss=2.9055545192328362e+38\n",
      "Gradient Descent(28/49): loss=1.0773794232471443e+40\n",
      "Gradient Descent(29/49): loss=3.994922187668531e+41\n",
      "Gradient Descent(30/49): loss=1.4813168825357034e+43\n",
      "Gradient Descent(31/49): loss=5.492722019113945e+44\n",
      "Gradient Descent(32/49): loss=2.03670096081087e+46\n",
      "Gradient Descent(33/49): loss=7.552085813432547e+47\n",
      "Gradient Descent(34/49): loss=2.800312919317511e+49\n",
      "Gradient Descent(35/49): loss=1.0383558449705062e+51\n",
      "Gradient Descent(36/49): loss=3.850222785270614e+52\n",
      "Gradient Descent(37/49): loss=1.427662353712468e+54\n",
      "Gradient Descent(38/49): loss=5.293771061781692e+55\n",
      "Gradient Descent(39/49): loss=1.9629299590119667e+57\n",
      "Gradient Descent(40/49): loss=7.27854298763344e+58\n",
      "Gradient Descent(41/49): loss=2.6988832576325397e+60\n",
      "Gradient Descent(42/49): loss=1.0007457331371432e+62\n",
      "Gradient Descent(43/49): loss=3.7107645155081374e+63\n",
      "Gradient Descent(44/49): loss=1.375951236523279e+65\n",
      "Gradient Descent(45/49): loss=5.102026273501351e+66\n",
      "Gradient Descent(46/49): loss=1.891831004220129e+68\n",
      "Gradient Descent(47/49): loss=7.014908110366368e+69\n",
      "Gradient Descent(48/49): loss=2.601127462607001e+71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=9.644978908176722e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9627711860163861\n",
      "Gradient Descent(2/49): loss=4.687847890592724\n",
      "Gradient Descent(3/49): loss=36.83237608081366\n",
      "Gradient Descent(4/49): loss=442.1904004683188\n",
      "Gradient Descent(5/49): loss=9644.599802201481\n",
      "Gradient Descent(6/49): loss=307726.1340875312\n",
      "Gradient Descent(7/49): loss=11043088.654734652\n",
      "Gradient Descent(8/49): loss=406844318.2514493\n",
      "Gradient Descent(9/49): loss=15069618460.487074\n",
      "Gradient Descent(10/49): loss=558786050683.0674\n",
      "Gradient Descent(11/49): loss=20724437876929.88\n",
      "Gradient Descent(12/49): loss=768667888751700.8\n",
      "Gradient Descent(13/49): loss=2.851008230526984e+16\n",
      "Gradient Descent(14/49): loss=1.0574478382992998e+18\n",
      "Gradient Descent(15/49): loss=3.922108343748733e+19\n",
      "Gradient Descent(16/49): loss=1.454722811373228e+21\n",
      "Gradient Descent(17/49): loss=5.395614556411125e+22\n",
      "Gradient Descent(18/49): loss=2.0012511176744135e+24\n",
      "Gradient Descent(19/49): loss=7.422705970431365e+25\n",
      "Gradient Descent(20/49): loss=2.753105966710318e+27\n",
      "Gradient Descent(21/49): loss=1.0211360242877305e+29\n",
      "Gradient Descent(22/49): loss=3.787426974155474e+30\n",
      "Gradient Descent(23/49): loss=1.4047690751659751e+32\n",
      "Gradient Descent(24/49): loss=5.210334530563492e+33\n",
      "Gradient Descent(25/49): loss=1.932530150350622e+35\n",
      "Gradient Descent(26/49): loss=7.167817651835473e+36\n",
      "Gradient Descent(27/49): loss=2.658567054213567e+38\n",
      "Gradient Descent(28/49): loss=9.860712318679025e+39\n",
      "Gradient Descent(29/49): loss=3.6573705100889595e+41\n",
      "Gradient Descent(30/49): loss=1.356530706481447e+43\n",
      "Gradient Descent(31/49): loss=5.031416840462168e+44\n",
      "Gradient Descent(32/49): loss=1.8661689928234223e+46\n",
      "Gradient Descent(33/49): loss=6.921681944078042e+47\n",
      "Gradient Descent(34/49): loss=2.5672745136810876e+49\n",
      "Gradient Descent(35/49): loss=9.522105294415433e+50\n",
      "Gradient Descent(36/49): loss=3.531780055258996e+52\n",
      "Gradient Descent(37/49): loss=1.3099487952565158e+54\n",
      "Gradient Descent(38/49): loss=4.858643005355886e+55\n",
      "Gradient Descent(39/49): loss=1.802086611245826e+57\n",
      "Gradient Descent(40/49): loss=6.683998290987032e+58\n",
      "Gradient Descent(41/49): loss=2.4791168679197268e+60\n",
      "Gradient Descent(42/49): loss=9.195125697580838e+61\n",
      "Gradient Descent(43/49): loss=3.4105022513625595e+63\n",
      "Gradient Descent(44/49): loss=1.2649664603942515e+65\n",
      "Gradient Descent(45/49): loss=4.6918020513930305e+66\n",
      "Gradient Descent(46/49): loss=1.7402047547248613e+68\n",
      "Gradient Descent(47/49): loss=6.454476457436899e+69\n",
      "Gradient Descent(48/49): loss=2.3939864677702943e+71\n",
      "Gradient Descent(49/49): loss=8.879374253915772e+72\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9407362259575748\n",
      "Gradient Descent(2/49): loss=4.448410143810064\n",
      "Gradient Descent(3/49): loss=33.34258700893511\n",
      "Gradient Descent(4/49): loss=357.7484962338032\n",
      "Gradient Descent(5/49): loss=6929.7873381710215\n",
      "Gradient Descent(6/49): loss=212711.03815545622\n",
      "Gradient Descent(7/49): loss=7654606.263497484\n",
      "Gradient Descent(8/49): loss=285666459.00017405\n",
      "Gradient Descent(9/49): loss=10739952942.225039\n",
      "Gradient Descent(10/49): loss=404370343160.438\n",
      "Gradient Descent(11/49): loss=15229330762946.791\n",
      "Gradient Descent(12/49): loss=573596923880628.6\n",
      "Gradient Descent(13/49): loss=2.1604171356872012e+16\n",
      "Gradient Descent(14/49): loss=8.137094439904795e+17\n",
      "Gradient Descent(15/49): loss=3.0647939730638193e+19\n",
      "Gradient Descent(16/49): loss=1.1543386835924284e+21\n",
      "Gradient Descent(17/49): loss=4.347756586778256e+22\n",
      "Gradient Descent(18/49): loss=1.6375599005381376e+24\n",
      "Gradient Descent(19/49): loss=6.167784180249097e+25\n",
      "Gradient Descent(20/49): loss=2.323063827244838e+27\n",
      "Gradient Descent(21/49): loss=8.749699061887954e+28\n",
      "Gradient Descent(22/49): loss=3.2955286366266475e+30\n",
      "Gradient Descent(23/49): loss=1.241243717986033e+32\n",
      "Gradient Descent(24/49): loss=4.6750798955797804e+33\n",
      "Gradient Descent(25/49): loss=1.760844523388069e+35\n",
      "Gradient Descent(26/49): loss=6.632129300030175e+36\n",
      "Gradient Descent(27/49): loss=2.497957001205739e+38\n",
      "Gradient Descent(28/49): loss=9.408425103901828e+39\n",
      "Gradient Descent(29/49): loss=3.543634373730306e+41\n",
      "Gradient Descent(30/49): loss=1.334691453246036e+43\n",
      "Gradient Descent(31/49): loss=5.0270459293823016e+44\n",
      "Gradient Descent(32/49): loss=1.893410699128858e+46\n",
      "Gradient Descent(33/49): loss=7.1314329050026826e+47\n",
      "Gradient Descent(34/49): loss=2.6860171066928797e+49\n",
      "Gradient Descent(35/49): loss=1.0116743708527067e+51\n",
      "Gradient Descent(36/49): loss=3.810418891562368e+52\n",
      "Gradient Descent(37/49): loss=1.4351744541019995e+54\n",
      "Gradient Descent(38/49): loss=5.405509925084427e+55\n",
      "Gradient Descent(39/49): loss=2.0359571943794984e+57\n",
      "Gradient Descent(40/49): loss=7.668326864242942e+58\n",
      "Gradient Descent(41/49): loss=2.8882354235739142e+60\n",
      "Gradient Descent(42/49): loss=1.0878388479871897e+62\n",
      "Gradient Descent(43/49): loss=4.09728843269207e+63\n",
      "Gradient Descent(44/49): loss=1.543222374502859e+65\n",
      "Gradient Descent(45/49): loss=5.812466796733229e+66\n",
      "Gradient Descent(46/49): loss=2.1892353831385215e+68\n",
      "Gradient Descent(47/49): loss=8.245641188830856e+69\n",
      "Gradient Descent(48/49): loss=3.105677860800531e+71\n",
      "Gradient Descent(49/49): loss=1.1697374108555237e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9598465307258313\n",
      "Gradient Descent(2/49): loss=4.735297748112433\n",
      "Gradient Descent(3/49): loss=39.159806519138606\n",
      "Gradient Descent(4/49): loss=532.5959851957243\n",
      "Gradient Descent(5/49): loss=13379.0207705704\n",
      "Gradient Descent(6/49): loss=465828.98945243773\n",
      "Gradient Descent(7/49): loss=17729716.69338063\n",
      "Gradient Descent(8/49): loss=687519478.0177896\n",
      "Gradient Descent(9/49): loss=26758459416.587982\n",
      "Gradient Descent(10/49): loss=1042187852870.3627\n",
      "Gradient Descent(11/49): loss=40596689284681.71\n",
      "Gradient Descent(12/49): loss=1581418328918125.2\n",
      "Gradient Descent(13/49): loss=6.160346610211734e+16\n",
      "Gradient Descent(14/49): loss=2.3997387230453053e+18\n",
      "Gradient Descent(15/49): loss=9.348089332321542e+19\n",
      "Gradient Descent(16/49): loss=3.641512159785877e+21\n",
      "Gradient Descent(17/49): loss=1.4185370329057114e+23\n",
      "Gradient Descent(18/49): loss=5.525856370138329e+24\n",
      "Gradient Descent(19/49): loss=2.1525760641590967e+26\n",
      "Gradient Descent(20/49): loss=8.385277144136114e+27\n",
      "Gradient Descent(21/49): loss=3.266452412781178e+29\n",
      "Gradient Descent(22/49): loss=1.272433955560535e+31\n",
      "Gradient Descent(23/49): loss=4.9567174618212916e+32\n",
      "Gradient Descent(24/49): loss=1.9308701947915266e+34\n",
      "Gradient Descent(25/49): loss=7.52163046986423e+35\n",
      "Gradient Descent(26/49): loss=2.9300221774514327e+37\n",
      "Gradient Descent(27/49): loss=1.1413788532624177e+39\n",
      "Gradient Descent(28/49): loss=4.446197358846496e+40\n",
      "Gradient Descent(29/49): loss=1.7319990551172785e+42\n",
      "Gradient Descent(30/49): loss=6.74693560545268e+43\n",
      "Gradient Descent(31/49): loss=2.628242777017159e+45\n",
      "Gradient Descent(32/49): loss=1.0238218502278638e+47\n",
      "Gradient Descent(33/49): loss=3.988258581627827e+48\n",
      "Gradient Descent(34/49): loss=1.5536107683566128e+50\n",
      "Gradient Descent(35/49): loss=6.052030905600079e+51\n",
      "Gradient Descent(36/49): loss=2.3575453278483184e+53\n",
      "Gradient Descent(37/49): loss=9.18372701586207e+54\n",
      "Gradient Descent(38/49): loss=3.5774854848221042e+56\n",
      "Gradient Descent(39/49): loss=1.3935956907263886e+58\n",
      "Gradient Descent(40/49): loss=5.428698334209429e+59\n",
      "Gradient Descent(41/49): loss=2.1147285256376698e+61\n",
      "Gradient Descent(42/49): loss=8.237843515754633e+62\n",
      "Gradient Descent(43/49): loss=3.209020210743146e+64\n",
      "Gradient Descent(44/49): loss=1.2500614624766058e+66\n",
      "Gradient Descent(45/49): loss=4.8695662767648305e+67\n",
      "Gradient Descent(46/49): loss=1.896920786344865e+69\n",
      "Gradient Descent(47/49): loss=7.389381856935746e+70\n",
      "Gradient Descent(48/49): loss=2.878505239684976e+72\n",
      "Gradient Descent(49/49): loss=1.1213106285902636e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9658334831170611\n",
      "Gradient Descent(2/49): loss=4.8358563172317455\n",
      "Gradient Descent(3/49): loss=39.602957714406664\n",
      "Gradient Descent(4/49): loss=503.42437253944223\n",
      "Gradient Descent(5/49): loss=11564.404721920544\n",
      "Gradient Descent(6/49): loss=380615.17063194845\n",
      "Gradient Descent(7/49): loss=13966728.293951312\n",
      "Gradient Descent(8/49): loss=525119966.0034777\n",
      "Gradient Descent(9/49): loss=19842495699.693924\n",
      "Gradient Descent(10/49): loss=750540204056.0513\n",
      "Gradient Descent(11/49): loss=28394898640563.42\n",
      "Gradient Descent(12/49): loss=1074297474569845.2\n",
      "Gradient Descent(13/49): loss=4.064549217822922e+16\n",
      "Gradient Descent(14/49): loss=1.5378038505414892e+18\n",
      "Gradient Descent(15/49): loss=5.81821341424124e+19\n",
      "Gradient Descent(16/49): loss=2.20129567340308e+21\n",
      "Gradient Descent(17/49): loss=8.328506300616571e+22\n",
      "Gradient Descent(18/49): loss=3.151054091833932e+24\n",
      "Gradient Descent(19/49): loss=1.1921875948425398e+26\n",
      "Gradient Descent(20/49): loss=4.5105898535299116e+27\n",
      "Gradient Descent(21/49): loss=1.7065620306099233e+29\n",
      "Gradient Descent(22/49): loss=6.45670313396695e+30\n",
      "Gradient Descent(23/49): loss=2.4428655163104564e+32\n",
      "Gradient Descent(24/49): loss=9.242475311255693e+33\n",
      "Gradient Descent(25/49): loss=3.496850289499145e+35\n",
      "Gradient Descent(26/49): loss=1.3230180806952094e+37\n",
      "Gradient Descent(27/49): loss=5.005581300127934e+38\n",
      "Gradient Descent(28/49): loss=1.8938398890984282e+40\n",
      "Gradient Descent(29/49): loss=7.165260756924929e+41\n",
      "Gradient Descent(30/49): loss=2.710945207684301e+43\n",
      "Gradient Descent(31/49): loss=1.0256743150574086e+45\n",
      "Gradient Descent(32/49): loss=3.8805941100783704e+46\n",
      "Gradient Descent(33/49): loss=1.4682058842754644e+48\n",
      "Gradient Descent(34/49): loss=5.554893033060676e+49\n",
      "Gradient Descent(35/49): loss=2.1016695913852136e+51\n",
      "Gradient Descent(36/49): loss=7.951575385997429e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=3.0084439237437524e+54\n",
      "Gradient Descent(38/49): loss=1.1382316588797245e+56\n",
      "Gradient Descent(39/49): loss=4.306449919345035e+57\n",
      "Gradient Descent(40/49): loss=1.629326575407362e+59\n",
      "Gradient Descent(41/49): loss=6.164486152279262e+60\n",
      "Gradient Descent(42/49): loss=2.3323064936900065e+62\n",
      "Gradient Descent(43/49): loss=8.824180063243819e+63\n",
      "Gradient Descent(44/49): loss=3.338590103796984e+65\n",
      "Gradient Descent(45/49): loss=1.263141028547189e+67\n",
      "Gradient Descent(46/49): loss=4.779039080552674e+68\n",
      "Gradient Descent(47/49): loss=1.8081286267550543e+70\n",
      "Gradient Descent(48/49): loss=6.840975927974606e+71\n",
      "Gradient Descent(49/49): loss=2.5882534546846145e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9769727452280141\n",
      "Gradient Descent(2/49): loss=4.896451660406641\n",
      "Gradient Descent(3/49): loss=39.44910896711759\n",
      "Gradient Descent(4/49): loss=482.73511244199324\n",
      "Gradient Descent(5/49): loss=10699.45409198419\n",
      "Gradient Descent(6/49): loss=347560.9125481142\n",
      "Gradient Descent(7/49): loss=12718125.8004998\n",
      "Gradient Descent(8/49): loss=478013748.827866\n",
      "Gradient Descent(9/49): loss=18065385414.160442\n",
      "Gradient Descent(10/49): loss=683495737054.6593\n",
      "Gradient Descent(11/49): loss=25865514541865.812\n",
      "Gradient Descent(12/49): loss=978871857879773.5\n",
      "Gradient Descent(13/49): loss=3.704541438942143e+16\n",
      "Gradient Descent(14/49): loss=1.4019865668513172e+18\n",
      "Gradient Descent(15/49): loss=5.305830897426291e+19\n",
      "Gradient Descent(16/49): loss=2.007996668091969e+21\n",
      "Gradient Descent(17/49): loss=7.599282290659962e+22\n",
      "Gradient Descent(18/49): loss=2.8759555440991927e+24\n",
      "Gradient Descent(19/49): loss=1.0884080864627085e+26\n",
      "Gradient Descent(20/49): loss=4.119090662742759e+27\n",
      "Gradient Descent(21/49): loss=1.5588737440784498e+29\n",
      "Gradient Descent(22/49): loss=5.899572378847646e+30\n",
      "Gradient Descent(23/49): loss=2.2326987278799534e+32\n",
      "Gradient Descent(24/49): loss=8.449669381717764e+33\n",
      "Gradient Descent(25/49): loss=3.197785342411003e+35\n",
      "Gradient Descent(26/49): loss=1.2102048771593378e+37\n",
      "Gradient Descent(27/49): loss=4.580031765346658e+38\n",
      "Gradient Descent(28/49): loss=1.73331733886433e+40\n",
      "Gradient Descent(29/49): loss=6.559755807676802e+41\n",
      "Gradient Descent(30/49): loss=2.4825457688286106e+43\n",
      "Gradient Descent(31/49): loss=9.395217863317924e+44\n",
      "Gradient Descent(32/49): loss=3.555629056573603e+46\n",
      "Gradient Descent(33/49): loss=1.3456311680979019e+48\n",
      "Gradient Descent(34/49): loss=5.09255383997061e+49\n",
      "Gradient Descent(35/49): loss=1.9272818011237543e+51\n",
      "Gradient Descent(36/49): loss=7.293816143462216e+52\n",
      "Gradient Descent(37/49): loss=2.7603515948528965e+54\n",
      "Gradient Descent(38/49): loss=1.044657663058403e+56\n",
      "Gradient Descent(39/49): loss=3.953516773086416e+57\n",
      "Gradient Descent(40/49): loss=1.4962121494724854e+59\n",
      "Gradient Descent(41/49): loss=5.662428983402059e+60\n",
      "Gradient Descent(42/49): loss=2.1429515863359895e+62\n",
      "Gradient Descent(43/49): loss=8.110020478563136e+63\n",
      "Gradient Descent(44/49): loss=3.0692448948494255e+65\n",
      "Gradient Descent(45/49): loss=1.1615586236137771e+67\n",
      "Gradient Descent(46/49): loss=4.395929560249971e+68\n",
      "Gradient Descent(47/49): loss=1.6636436858054803e+70\n",
      "Gradient Descent(48/49): loss=6.296075210911981e+71\n",
      "Gradient Descent(49/49): loss=2.382755598430126e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9543587163228838\n",
      "Gradient Descent(2/49): loss=4.645777642699395\n",
      "Gradient Descent(3/49): loss=35.72590982908454\n",
      "Gradient Descent(4/49): loss=391.02686420730004\n",
      "Gradient Descent(5/49): loss=7694.6424238479\n",
      "Gradient Descent(6/49): loss=240293.07163068716\n",
      "Gradient Descent(7/49): loss=8815015.496766016\n",
      "Gradient Descent(8/49): loss=335585740.83769685\n",
      "Gradient Descent(9/49): loss=12872542610.857882\n",
      "Gradient Descent(10/49): loss=494510967787.91626\n",
      "Gradient Descent(11/49): loss=19002726318752.23\n",
      "Gradient Descent(12/49): loss=730266207567236.0\n",
      "Gradient Descent(13/49): loss=2.8064123436668404e+16\n",
      "Gradient Descent(14/49): loss=1.0785064377245471e+18\n",
      "Gradient Descent(15/49): loss=4.144710442710403e+19\n",
      "Gradient Descent(16/49): loss=1.592816283884811e+21\n",
      "Gradient Descent(17/49): loss=6.121208690417377e+22\n",
      "Gradient Descent(18/49): loss=2.3523865456746173e+24\n",
      "Gradient Descent(19/49): loss=9.04024473573701e+25\n",
      "Gradient Descent(20/49): loss=3.474174983831291e+27\n",
      "Gradient Descent(21/49): loss=1.3351288788543813e+29\n",
      "Gradient Descent(22/49): loss=5.130913472856752e+30\n",
      "Gradient Descent(23/49): loss=1.9718151170963362e+32\n",
      "Gradient Descent(24/49): loss=7.577704977054581e+33\n",
      "Gradient Descent(25/49): loss=2.91211950965461e+35\n",
      "Gradient Descent(26/49): loss=1.1191304048112298e+37\n",
      "Gradient Descent(27/49): loss=4.300829202993307e+38\n",
      "Gradient Descent(28/49): loss=1.6528129120431393e+40\n",
      "Gradient Descent(29/49): loss=6.35177635120949e+41\n",
      "Gradient Descent(30/49): loss=2.44099392749249e+43\n",
      "Gradient Descent(31/49): loss=9.380763781017888e+44\n",
      "Gradient Descent(32/49): loss=3.60503678948739e+46\n",
      "Gradient Descent(33/49): loss=1.3854192000715141e+48\n",
      "Gradient Descent(34/49): loss=5.324179674182276e+49\n",
      "Gradient Descent(35/49): loss=2.0460875092183438e+51\n",
      "Gradient Descent(36/49): loss=7.863134513810741e+52\n",
      "Gradient Descent(37/49): loss=3.021810362641849e+54\n",
      "Gradient Descent(38/49): loss=1.1612847079916337e+56\n",
      "Gradient Descent(39/49): loss=4.4628286066113115e+57\n",
      "Gradient Descent(40/49): loss=1.7150694429132207e+59\n",
      "Gradient Descent(41/49): loss=6.591028814454368e+60\n",
      "Gradient Descent(42/49): loss=2.532938885505263e+62\n",
      "Gradient Descent(43/49): loss=9.7341091631016e+63\n",
      "Gradient Descent(44/49): loss=3.740827768936703e+65\n",
      "Gradient Descent(45/49): loss=1.4376038076389403e+67\n",
      "Gradient Descent(46/49): loss=5.524725636661622e+68\n",
      "Gradient Descent(47/49): loss=2.1231575207437902e+70\n",
      "Gradient Descent(48/49): loss=8.159315329575038e+71\n",
      "Gradient Descent(49/49): loss=3.1356329427746145e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9737798308551571\n",
      "Gradient Descent(2/49): loss=4.943611791680095\n",
      "Gradient Descent(3/49): loss=41.895972410027724\n",
      "Gradient Descent(4/49): loss=580.2635366408076\n",
      "Gradient Descent(5/49): loss=14813.187168132163\n",
      "Gradient Descent(6/49): loss=525235.0598184146\n",
      "Gradient Descent(7/49): loss=20383221.895694904\n",
      "Gradient Descent(8/49): loss=806219917.9268837\n",
      "Gradient Descent(9/49): loss=32008367800.28265\n",
      "Gradient Descent(10/49): loss=1271717142839.7942\n",
      "Gradient Descent(11/49): loss=50533459479122.055\n",
      "Gradient Descent(12/49): loss=2008072850558434.5\n",
      "Gradient Descent(13/49): loss=7.979620022748114e+16\n",
      "Gradient Descent(14/49): loss=3.1709208884271703e+18\n",
      "Gradient Descent(15/49): loss=1.2600526423295418e+20\n",
      "Gradient Descent(16/49): loss=5.007165990484368e+21\n",
      "Gradient Descent(17/49): loss=1.9897352382618265e+23\n",
      "Gradient Descent(18/49): loss=7.906760693946009e+24\n",
      "Gradient Descent(19/49): loss=3.141969016121246e+26\n",
      "Gradient Descent(20/49): loss=1.2485478795844869e+28\n",
      "Gradient Descent(21/49): loss=4.961448695493182e+29\n",
      "Gradient Descent(22/49): loss=1.9715682162108545e+31\n",
      "Gradient Descent(23/49): loss=7.834569033647912e+32\n",
      "Gradient Descent(24/49): loss=3.1132816728485616e+34\n",
      "Gradient Descent(25/49): loss=1.237148174056199e+36\n",
      "Gradient Descent(26/49): loss=4.916148827517529e+37\n",
      "Gradient Descent(27/49): loss=1.9535670666725094e+39\n",
      "Gradient Descent(28/49): loss=7.763036510663325e+40\n",
      "Gradient Descent(29/49): loss=3.084856255717882e+42\n",
      "Gradient Descent(30/49): loss=1.2258525520741804e+44\n",
      "Gradient Descent(31/49): loss=4.871262564151925e+45\n",
      "Gradient Descent(32/49): loss=1.9357302743105313e+47\n",
      "Gradient Descent(33/49): loss=7.692157106161904e+48\n",
      "Gradient Descent(34/49): loss=3.05669037319537e+50\n",
      "Gradient Descent(35/49): loss=1.21466006331316e+52\n",
      "Gradient Descent(36/49): loss=4.826786129030114e+53\n",
      "Gradient Descent(37/49): loss=1.918056338482813e+55\n",
      "Gradient Descent(38/49): loss=7.621924856930167e+56\n",
      "Gradient Descent(39/49): loss=3.028781655633845e+58\n",
      "Gradient Descent(40/49): loss=1.2035697661284315e+60\n",
      "Gradient Descent(41/49): loss=4.7827157802675444e+61\n",
      "Gradient Descent(42/49): loss=1.9005437722485442e+63\n",
      "Gradient Descent(43/49): loss=7.55233385419919e+64\n",
      "Gradient Descent(44/49): loss=3.0011277550215253e+66\n",
      "Gradient Descent(45/49): loss=1.1925807274730614e+68\n",
      "Gradient Descent(46/49): loss=4.739047810145564e+69\n",
      "Gradient Descent(47/49): loss=1.8831911022437056e+71\n",
      "Gradient Descent(48/49): loss=7.483378243151361e+72\n",
      "Gradient Descent(49/49): loss=2.9737263447851784e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9799004838755637\n",
      "Gradient Descent(2/49): loss=5.048760545451503\n",
      "Gradient Descent(3/49): loss=42.37793791447121\n",
      "Gradient Descent(4/49): loss=548.8771494218508\n",
      "Gradient Descent(5/49): loss=12812.809047064844\n",
      "Gradient Descent(6/49): loss=429333.77253768867\n",
      "Gradient Descent(7/49): loss=16062370.210621703\n",
      "Gradient Descent(8/49): loss=615988090.1965684\n",
      "Gradient Descent(9/49): loss=23744181773.853287\n",
      "Gradient Descent(10/49): loss=916206481487.8862\n",
      "Gradient Descent(11/49): loss=35360696371421.75\n",
      "Gradient Descent(12/49): loss=1364792778897313.0\n",
      "Gradient Descent(13/49): loss=5.267643237328992e+16\n",
      "Gradient Descent(14/49): loss=2.0331374761182075e+18\n",
      "Gradient Descent(15/49): loss=7.847246408727539e+19\n",
      "Gradient Descent(16/49): loss=3.028780950198857e+21\n",
      "Gradient Descent(17/49): loss=1.1690105927645203e+23\n",
      "Gradient Descent(18/49): loss=4.5119993569260314e+24\n",
      "Gradient Descent(19/49): loss=1.7414844942238337e+26\n",
      "Gradient Descent(20/49): loss=6.721561782418721e+27\n",
      "Gradient Descent(21/49): loss=2.5943034776252736e+29\n",
      "Gradient Descent(22/49): loss=1.0013164725578384e+31\n",
      "Gradient Descent(23/49): loss=3.864754786261262e+32\n",
      "Gradient Descent(24/49): loss=1.4916692142073964e+34\n",
      "Gradient Descent(25/49): loss=5.7573563335092256e+35\n",
      "Gradient Descent(26/49): loss=2.22215164295741e+37\n",
      "Gradient Descent(27/49): loss=8.576780102280913e+38\n",
      "Gradient Descent(28/49): loss=3.3103572006895825e+40\n",
      "Gradient Descent(29/49): loss=1.2776898399485713e+42\n",
      "Gradient Descent(30/49): loss=4.931465784924049e+43\n",
      "Gradient Descent(31/49): loss=1.90338484564111e+45\n",
      "Gradient Descent(32/49): loss=7.34644429997207e+46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=2.8354877352410845e+48\n",
      "Gradient Descent(34/49): loss=1.0944057245126414e+50\n",
      "Gradient Descent(35/49): loss=4.224048917440231e+51\n",
      "Gradient Descent(36/49): loss=1.6303450226261697e+53\n",
      "Gradient Descent(37/49): loss=6.292599694637965e+54\n",
      "Gradient Descent(38/49): loss=2.4287381117141878e+56\n",
      "Gradient Descent(39/49): loss=9.374136448437016e+57\n",
      "Gradient Descent(40/49): loss=3.618110727133757e+59\n",
      "Gradient Descent(41/49): loss=1.3964726570608814e+61\n",
      "Gradient Descent(42/49): loss=5.389928692048718e+62\n",
      "Gradient Descent(43/49): loss=2.0803365650218777e+64\n",
      "Gradient Descent(44/49): loss=8.029420185375444e+65\n",
      "Gradient Descent(45/49): loss=3.099094136848814e+67\n",
      "Gradient Descent(46/49): loss=1.1961491922597024e+69\n",
      "Gradient Descent(47/49): loss=4.616745497116068e+70\n",
      "Gradient Descent(48/49): loss=1.7819130860152093e+72\n",
      "Gradient Descent(49/49): loss=6.877602952330089e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9913497779213999\n",
      "Gradient Descent(2/49): loss=5.112542184038331\n",
      "Gradient Descent(3/49): loss=42.220992811541294\n",
      "Gradient Descent(4/49): loss=526.5205575045886\n",
      "Gradient Descent(5/49): loss=11857.49446752449\n",
      "Gradient Descent(6/49): loss=392078.7458297728\n",
      "Gradient Descent(7/49): loss=14626659.494229445\n",
      "Gradient Descent(8/49): loss=560731138.4130347\n",
      "Gradient Descent(9/49): loss=21617578042.31451\n",
      "Gradient Descent(10/49): loss=834360272831.6638\n",
      "Gradient Descent(11/49): loss=32210671750182.875\n",
      "Gradient Descent(12/49): loss=1243557856580945.8\n",
      "Gradient Descent(13/49): loss=4.801050177756672e+16\n",
      "Gradient Descent(14/49): loss=1.8535628079689485e+18\n",
      "Gradient Descent(15/49): loss=7.156135115175221e+19\n",
      "Gradient Descent(16/49): loss=2.7628021800823685e+21\n",
      "Gradient Descent(17/49): loss=1.0666478315264542e+23\n",
      "Gradient Descent(18/49): loss=4.1180566896752253e+24\n",
      "Gradient Descent(19/49): loss=1.5898772217622677e+26\n",
      "Gradient Descent(20/49): loss=6.138112636794104e+27\n",
      "Gradient Descent(21/49): loss=2.3697695788831652e+29\n",
      "Gradient Descent(22/49): loss=9.149079186597619e+30\n",
      "Gradient Descent(23/49): loss=3.532227382299619e+32\n",
      "Gradient Descent(24/49): loss=1.3637033876094058e+34\n",
      "Gradient Descent(25/49): loss=5.264912838557791e+35\n",
      "Gradient Descent(26/49): loss=2.0326492879220114e+37\n",
      "Gradient Descent(27/49): loss=7.847543263074456e+38\n",
      "Gradient Descent(28/49): loss=3.0297373792791877e+40\n",
      "Gradient Descent(29/49): loss=1.1697047444890564e+42\n",
      "Gradient Descent(30/49): loss=4.515933290580263e+43\n",
      "Gradient Descent(31/49): loss=1.743487284381265e+45\n",
      "Gradient Descent(32/49): loss=6.731162121326558e+46\n",
      "Gradient Descent(33/49): loss=2.5987309405391027e+48\n",
      "Gradient Descent(34/49): loss=1.0033040921593843e+50\n",
      "Gradient Descent(35/49): loss=3.873502584053446e+51\n",
      "Gradient Descent(36/49): loss=1.4954610856192503e+53\n",
      "Gradient Descent(37/49): loss=5.7735958865972515e+54\n",
      "Gradient Descent(38/49): loss=2.2290389086205548e+56\n",
      "Gradient Descent(39/49): loss=8.605753768943779e+57\n",
      "Gradient Descent(40/49): loss=3.322463221493117e+59\n",
      "Gradient Descent(41/49): loss=1.282718766368942e+61\n",
      "Gradient Descent(42/49): loss=4.952251759932745e+62\n",
      "Gradient Descent(43/49): loss=1.9119387769760433e+64\n",
      "Gradient Descent(44/49): loss=7.381510601864802e+65\n",
      "Gradient Descent(45/49): loss=2.849813990990798e+67\n",
      "Gradient Descent(46/49): loss=1.1002408885242513e+69\n",
      "Gradient Descent(47/49): loss=4.2477509641244276e+70\n",
      "Gradient Descent(48/49): loss=1.6399488913215088e+72\n",
      "Gradient Descent(49/49): loss=6.331426651093889e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9681499998165368\n",
      "Gradient Descent(2/49): loss=4.850244130939001\n",
      "Gradient Descent(3/49): loss=38.25145318844085\n",
      "Gradient Descent(4/49): loss=427.00292294800965\n",
      "Gradient Descent(5/49): loss=8535.07006692048\n",
      "Gradient Descent(6/49): loss=271124.69470394\n",
      "Gradient Descent(7/49): loss=10137110.390835503\n",
      "Gradient Descent(8/49): loss=393596691.532175\n",
      "Gradient Descent(9/49): loss=15400783656.123251\n",
      "Gradient Descent(10/49): loss=603534333052.287\n",
      "Gradient Descent(11/49): loss=23658835035251.62\n",
      "Gradient Descent(12/49): loss=927493511510887.8\n",
      "Gradient Descent(13/49): loss=3.636081210634375e+16\n",
      "Gradient Descent(14/49): loss=1.425467405193526e+18\n",
      "Gradient Descent(15/49): loss=5.5883192681177194e+19\n",
      "Gradient Descent(16/49): loss=2.1908122909332482e+21\n",
      "Gradient Descent(17/49): loss=8.58873366240156e+22\n",
      "Gradient Descent(18/49): loss=3.367077430335996e+24\n",
      "Gradient Descent(19/49): loss=1.3200095468756446e+26\n",
      "Gradient Descent(20/49): loss=5.17488902509142e+27\n",
      "Gradient Descent(21/49): loss=2.0287335410672632e+29\n",
      "Gradient Descent(22/49): loss=7.953329551064674e+30\n",
      "Gradient Descent(23/49): loss=3.11797728323803e+32\n",
      "Gradient Descent(24/49): loss=1.2223537672331278e+34\n",
      "Gradient Descent(25/49): loss=4.7920449590877545e+35\n",
      "Gradient Descent(26/49): loss=1.8786455693508271e+37\n",
      "Gradient Descent(27/49): loss=7.364933353866904e+38\n",
      "Gradient Descent(28/49): loss=2.8873058437331754e+40\n",
      "Gradient Descent(29/49): loss=1.1319226712185731e+42\n",
      "Gradient Descent(30/49): loss=4.437524124434225e+43\n",
      "Gradient Descent(31/49): loss=1.7396612733038578e+45\n",
      "Gradient Descent(32/49): loss=6.82006736407095e+46\n",
      "Gradient Descent(33/49): loss=2.6736997347840386e+48\n",
      "Gradient Descent(34/49): loss=1.048181768620711e+50\n",
      "Gradient Descent(35/49): loss=4.109231136822434e+51\n",
      "Gradient Descent(36/49): loss=1.6109591906040266e+53\n",
      "Gradient Descent(37/49): loss=6.315511168345696e+54\n",
      "Gradient Descent(38/49): loss=2.4758964441888034e+56\n",
      "Gradient Descent(39/49): loss=9.706361114634342e+57\n",
      "Gradient Descent(40/49): loss=3.805225630854379e+59\n",
      "Gradient Descent(41/49): loss=1.4917786316315905e+61\n",
      "Gradient Descent(42/49): loss=5.848282603134145e+62\n",
      "Gradient Descent(43/49): loss=2.292726861807437e+64\n",
      "Gradient Descent(44/49): loss=8.98827368574215e+65\n",
      "Gradient Descent(45/49): loss=3.523710791529629e+67\n",
      "Gradient Descent(46/49): loss=1.3814151834337615e+69\n",
      "Gradient Descent(47/49): loss=5.415620128668012e+70\n",
      "Gradient Descent(48/49): loss=2.1231083695729994e+72\n",
      "Gradient Descent(49/49): loss=8.323311166323597e+73\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9878835430511312\n",
      "Gradient Descent(2/49): loss=5.159316796301604\n",
      "Gradient Descent(3/49): loss=44.79175448532098\n",
      "Gradient Descent(4/49): loss=631.6513667402264\n",
      "Gradient Descent(5/49): loss=16384.768274155267\n",
      "Gradient Descent(6/49): loss=591519.9931359311\n",
      "Gradient Descent(7/49): loss=23401717.41093188\n",
      "Gradient Descent(8/49): loss=943931872.3923409\n",
      "Gradient Descent(9/49): loss=38220760600.24833\n",
      "Gradient Descent(10/49): loss=1548756758422.3308\n",
      "Gradient Descent(11/49): loss=62766863055982.59\n",
      "Gradient Descent(12/49): loss=2543840960065798.5\n",
      "Gradient Descent(13/49): loss=1.0309839046958795e+17\n",
      "Gradient Descent(14/49): loss=4.178441067030377e+18\n",
      "Gradient Descent(15/49): loss=1.6934671162566974e+20\n",
      "Gradient Descent(16/49): loss=6.86339939817174e+21\n",
      "Gradient Descent(17/49): loss=2.7816454904487483e+23\n",
      "Gradient Descent(18/49): loss=1.127364327436425e+25\n",
      "Gradient Descent(19/49): loss=4.569059326650132e+26\n",
      "Gradient Descent(20/49): loss=1.8517796442199969e+28\n",
      "Gradient Descent(21/49): loss=7.505019317183128e+29\n",
      "Gradient Descent(22/49): loss=3.0416856091504425e+31\n",
      "Gradient Descent(23/49): loss=1.2327551674296153e+33\n",
      "Gradient Descent(24/49): loss=4.9961945384914e+34\n",
      "Gradient Descent(25/49): loss=2.0248919271212177e+36\n",
      "Gradient Descent(26/49): loss=8.206620628825078e+37\n",
      "Gradient Descent(27/49): loss=3.3260353919831173e+39\n",
      "Gradient Descent(28/49): loss=1.3479983941097226e+41\n",
      "Gradient Descent(29/49): loss=5.4632601772736186e+42\n",
      "Gradient Descent(30/49): loss=2.214187486795612e+44\n",
      "Gradient Descent(31/49): loss=8.973810632480196e+45\n",
      "Gradient Descent(32/49): loss=3.6369674089414074e+47\n",
      "Gradient Descent(33/49): loss=1.4740150506213787e+49\n",
      "Gradient Descent(34/49): loss=5.9739891100392904e+50\n",
      "Gradient Descent(35/49): loss=2.421179205179951e+52\n",
      "Gradient Descent(36/49): loss=9.812720839655282e+53\n",
      "Gradient Descent(37/49): loss=3.9769666809876316e+55\n",
      "Gradient Descent(38/49): loss=1.61181228327301e+57\n",
      "Gradient Descent(39/49): loss=6.532463168297505e+58\n",
      "Gradient Descent(40/49): loss=2.6475213948928512e+60\n",
      "Gradient Descent(41/49): loss=1.0730055961788145e+62\n",
      "Gradient Descent(42/49): loss=4.348750539474572e+63\n",
      "Gradient Descent(43/49): loss=1.7624913907185948e+65\n",
      "Gradient Descent(44/49): loss=7.143145770629636e+66\n",
      "Gradient Descent(45/49): loss=2.8950230207741463e+68\n",
      "Gradient Descent(46/49): loss=1.1733147495425066e+70\n",
      "Gradient Descent(47/49): loss=4.755290343514817e+71\n",
      "Gradient Descent(48/49): loss=1.927256625721416e+73\n",
      "Gradient Descent(49/49): loss=7.810917595079655e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9941392999309819\n",
      "Gradient Descent(2/49): loss=5.269208203468475\n",
      "Gradient Descent(3/49): loss=45.31513566818883\n",
      "Gradient Descent(4/49): loss=597.9071212567018\n",
      "Gradient Descent(5/49): loss=14181.720222860986\n",
      "Gradient Descent(6/49): loss=483715.6640004565\n",
      "Gradient Descent(7/49): loss=18447012.688584488\n",
      "Gradient Descent(8/49): loss=721442944.88394\n",
      "Gradient Descent(9/49): loss=28362755564.0621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=1116239806949.4482\n",
      "Gradient Descent(11/49): loss=43940046095201.5\n",
      "Gradient Descent(12/49): loss=1729746899791867.8\n",
      "Gradient Descent(13/49): loss=6.809394212874565e+16\n",
      "Gradient Descent(14/49): loss=2.6806195294397117e+18\n",
      "Gradient Descent(15/49): loss=1.0552662199158499e+20\n",
      "Gradient Descent(16/49): loss=4.15421459592595e+21\n",
      "Gradient Descent(17/49): loss=1.635369240768348e+23\n",
      "Gradient Descent(18/49): loss=6.437877726482209e+24\n",
      "Gradient Descent(19/49): loss=2.534367689789576e+26\n",
      "Gradient Descent(20/49): loss=9.97692075050536e+27\n",
      "Gradient Descent(21/49): loss=3.927565367307215e+29\n",
      "Gradient Descent(22/49): loss=1.5461453589043408e+31\n",
      "Gradient Descent(23/49): loss=6.0866344600216966e+32\n",
      "Gradient Descent(24/49): loss=2.396095479417538e+34\n",
      "Gradient Descent(25/49): loss=9.432591334661779e+35\n",
      "Gradient Descent(26/49): loss=3.7132818809192086e+37\n",
      "Gradient Descent(27/49): loss=1.4617894317646105e+39\n",
      "Gradient Descent(28/49): loss=5.754554626727512e+40\n",
      "Gradient Descent(29/49): loss=2.2653672432161285e+42\n",
      "Gradient Descent(30/49): loss=8.917959910921985e+43\n",
      "Gradient Descent(31/49): loss=3.5106894571277166e+45\n",
      "Gradient Descent(32/49): loss=1.382035867787747e+47\n",
      "Gradient Descent(33/49): loss=5.4405926903445676e+48\n",
      "Gradient Descent(34/49): loss=2.1417713904640904e+50\n",
      "Gradient Descent(35/49): loss=8.431406190637042e+51\n",
      "Gradient Descent(36/49): loss=3.3191502448872074e+53\n",
      "Gradient Descent(37/49): loss=1.3066335672890206e+55\n",
      "Gradient Descent(38/49): loss=5.143760159083715e+56\n",
      "Gradient Descent(39/49): loss=2.0249187864561368e+58\n",
      "Gradient Descent(40/49): loss=7.9713982863334576e+59\n",
      "Gradient Descent(41/49): loss=3.1380611935833497e+61\n",
      "Gradient Descent(42/49): loss=1.2353451302962451e+63\n",
      "Gradient Descent(43/49): loss=4.863122472140205e+64\n",
      "Gradient Descent(44/49): loss=1.914441527232429e+66\n",
      "Gradient Descent(45/49): loss=7.536487888570777e+67\n",
      "Gradient Descent(46/49): loss=2.966852154355563e+69\n",
      "Gradient Descent(47/49): loss=1.1679461091091397e+71\n",
      "Gradient Descent(48/49): loss=4.597796057281045e+72\n",
      "Gradient Descent(49/49): loss=1.8099917812538253e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.005902284096543\n",
      "Gradient Descent(2/49): loss=5.336296731626912\n",
      "Gradient Descent(3/49): loss=45.155322799339366\n",
      "Gradient Descent(4/49): loss=573.7676989143131\n",
      "Gradient Descent(5/49): loss=13127.640324762706\n",
      "Gradient Descent(6/49): loss=441775.6315840978\n",
      "Gradient Descent(7/49): loss=16798430.161409322\n",
      "Gradient Descent(8/49): loss=656727026.1033354\n",
      "Gradient Descent(9/49): loss=25822433850.709377\n",
      "Gradient Descent(10/49): loss=1016520651986.557\n",
      "Gradient Descent(11/49): loss=40025585274316.15\n",
      "Gradient Descent(12/49): loss=1576085856313835.8\n",
      "Gradient Descent(13/49): loss=6.2062066495616376e+16\n",
      "Gradient Descent(14/49): loss=2.4438437597759544e+18\n",
      "Gradient Descent(15/49): loss=9.623228816070705e+19\n",
      "Gradient Descent(16/49): loss=3.789380553422092e+21\n",
      "Gradient Descent(17/49): loss=1.4921608415474068e+23\n",
      "Gradient Descent(18/49): loss=5.875746584062303e+24\n",
      "Gradient Descent(19/49): loss=2.3137182655769508e+26\n",
      "Gradient Descent(20/49): loss=9.110828958640254e+27\n",
      "Gradient Descent(21/49): loss=3.587610710896486e+29\n",
      "Gradient Descent(22/49): loss=1.4127090598932016e+31\n",
      "Gradient Descent(23/49): loss=5.5628858556055555e+32\n",
      "Gradient Descent(24/49): loss=2.190521737352076e+34\n",
      "Gradient Descent(25/49): loss=8.625712636143889e+35\n",
      "Gradient Descent(26/49): loss=3.396584348497388e+37\n",
      "Gradient Descent(27/49): loss=1.3374877790520651e+39\n",
      "Gradient Descent(28/49): loss=5.2666837492347605e+40\n",
      "Gradient Descent(29/49): loss=2.0738849467553088e+42\n",
      "Gradient Descent(30/49): loss=8.166426877260736e+43\n",
      "Gradient Descent(31/49): loss=3.215729399356912e+45\n",
      "Gradient Descent(32/49): loss=1.2662717398085585e+47\n",
      "Gradient Descent(33/49): loss=4.9862532567524515e+48\n",
      "Gradient Descent(34/49): loss=1.963458613096189e+50\n",
      "Gradient Descent(35/49): loss=7.73159630454165e+51\n",
      "Gradient Descent(36/49): loss=3.0445042751443494e+53\n",
      "Gradient Descent(37/49): loss=1.1988476785741265e+55\n",
      "Gradient Descent(38/49): loss=4.720754600860107e+56\n",
      "Gradient Descent(39/49): loss=1.8589120536186107e+58\n",
      "Gradient Descent(40/49): loss=7.319918774127818e+59\n",
      "Gradient Descent(41/49): loss=2.882396225013745e+61\n",
      "Gradient Descent(42/49): loss=1.1350136872199677e+63\n",
      "Gradient Descent(43/49): loss=4.469392719144742e+64\n",
      "Gradient Descent(44/49): loss=1.759932193141241e+66\n",
      "Gradient Descent(45/49): loss=6.930161476272602e+67\n",
      "Gradient Descent(46/49): loss=2.728919800114071e+69\n",
      "Gradient Descent(47/49): loss=1.0745786084424437e+71\n",
      "Gradient Descent(48/49): loss=4.231414883185008e+72\n",
      "Gradient Descent(49/49): loss=1.6662226265226234e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.982110076438534\n",
      "Gradient Descent(2/49): loss=5.061978034428011\n",
      "Gradient Descent(3/49): loss=40.925942268518625\n",
      "Gradient Descent(4/49): loss=465.8630959105741\n",
      "Gradient Descent(5/49): loss=9457.674319798436\n",
      "Gradient Descent(6/49): loss=305550.9978356058\n",
      "Gradient Descent(7/49): loss=11641478.237308692\n",
      "Gradient Descent(8/49): loss=460910711.91285396\n",
      "Gradient Descent(9/49): loss=18393029993.13502\n",
      "Gradient Descent(10/49): loss=735147741775.5901\n",
      "Gradient Descent(11/49): loss=29392196206082.69\n",
      "Gradient Descent(12/49): loss=1175212627718180.0\n",
      "Gradient Descent(13/49): loss=4.6990083578014536e+16\n",
      "Gradient Descent(14/49): loss=1.8788713684171174e+18\n",
      "Gradient Descent(15/49): loss=7.512562362229157e+19\n",
      "Gradient Descent(16/49): loss=3.0038564443970323e+21\n",
      "Gradient Descent(17/49): loss=1.2010753822582343e+23\n",
      "Gradient Descent(18/49): loss=4.802433491873085e+24\n",
      "Gradient Descent(19/49): loss=1.920226473865842e+26\n",
      "Gradient Descent(20/49): loss=7.677919366765044e+27\n",
      "Gradient Descent(21/49): loss=3.069973599812485e+29\n",
      "Gradient Descent(22/49): loss=1.2275119669004866e+31\n",
      "Gradient Descent(23/49): loss=4.908138718123408e+32\n",
      "Gradient Descent(24/49): loss=1.962492124388449e+34\n",
      "Gradient Descent(25/49): loss=7.846916233368708e+35\n",
      "Gradient Descent(26/49): loss=3.137546062392132e+37\n",
      "Gradient Descent(27/49): loss=1.2545304423883822e+39\n",
      "Gradient Descent(28/49): loss=5.016170598238952e+40\n",
      "Gradient Descent(29/49): loss=2.005688074235424e+42\n",
      "Gradient Descent(30/49): loss=8.019632850091788e+43\n",
      "Gradient Descent(31/49): loss=3.206605846464383e+45\n",
      "Gradient Descent(32/49): loss=1.2821436151484e+47\n",
      "Gradient Descent(33/49): loss=5.126580342508744e+48\n",
      "Gradient Descent(34/49): loss=2.049834799914816e+50\n",
      "Gradient Descent(35/49): loss=8.196151091403072e+51\n",
      "Gradient Descent(36/49): loss=3.277185689105226e+53\n",
      "Gradient Descent(37/49): loss=1.3103645749211799e+55\n",
      "Gradient Descent(38/49): loss=5.239420289538653e+56\n",
      "Gradient Descent(39/49): loss=2.0949532287287577e+58\n",
      "Gradient Descent(40/49): loss=8.376554633961982e+59\n",
      "Gradient Descent(41/49): loss=3.3493190479641436e+61\n",
      "Gradient Descent(42/49): loss=1.339206699562741e+63\n",
      "Gradient Descent(43/49): loss=5.354743929945794e+64\n",
      "Gradient Descent(44/49): loss=2.1410647486047147e+66\n",
      "Gradient Descent(45/49): loss=8.560928996214657e+67\n",
      "Gradient Descent(46/49): loss=3.4230401171189183e+69\n",
      "Gradient Descent(47/49): loss=1.3686836613860646e+71\n",
      "Gradient Descent(48/49): loss=5.472605931717464e+72\n",
      "Gradient Descent(49/49): loss=2.1881912182352004e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0021576673137538\n",
      "Gradient Descent(2/49): loss=5.382585819919631\n",
      "Gradient Descent(3/49): loss=47.854543223309086\n",
      "Gradient Descent(4/49): loss=687.0073109408956\n",
      "Gradient Descent(5/49): loss=18105.392004818656\n",
      "Gradient Descent(6/49): loss=665401.2220398046\n",
      "Gradient Descent(7/49): loss=26831077.378200937\n",
      "Gradient Descent(8/49): loss=1103467848.3434641\n",
      "Gradient Descent(9/49): loss=45559972875.788055\n",
      "Gradient Descent(10/49): loss=1882524765211.6213\n",
      "Gradient Descent(11/49): loss=77797042119047.14\n",
      "Gradient Descent(12/49): loss=3215127440263980.5\n",
      "Gradient Descent(13/49): loss=1.3287270676280059e+17\n",
      "Gradient Descent(14/49): loss=5.491283390492787e+18\n",
      "Gradient Descent(15/49): loss=2.2694051006345675e+20\n",
      "Gradient Descent(16/49): loss=9.378863489039529e+21\n",
      "Gradient Descent(17/49): loss=3.876041393053537e+23\n",
      "Gradient Descent(18/49): loss=1.6018675314534093e+25\n",
      "Gradient Descent(19/49): loss=6.620103680357086e+26\n",
      "Gradient Descent(20/49): loss=2.7359174139463194e+28\n",
      "Gradient Descent(21/49): loss=1.1306838166650955e+30\n",
      "Gradient Descent(22/49): loss=4.67282340744099e+31\n",
      "Gradient Descent(23/49): loss=1.931156904813603e+33\n",
      "Gradient Descent(24/49): loss=7.980971386744664e+34\n",
      "Gradient Descent(25/49): loss=3.2983287954111446e+36\n",
      "Gradient Descent(26/49): loss=1.3631138761764062e+38\n",
      "Gradient Descent(27/49): loss=5.633396652297798e+39\n",
      "Gradient Descent(28/49): loss=2.328136951487797e+41\n",
      "Gradient Descent(29/49): loss=9.621587115957544e+42\n",
      "Gradient Descent(30/49): loss=3.9763527901913555e+44\n",
      "Gradient Descent(31/49): loss=1.643323634812695e+46\n",
      "Gradient Descent(32/49): loss=6.79143102039518e+47\n",
      "Gradient Descent(33/49): loss=2.806722566857238e+49\n",
      "Gradient Descent(34/49): loss=1.1599457527652492e+51\n",
      "Gradient Descent(35/49): loss=4.79375541154641e+52\n",
      "Gradient Descent(36/49): loss=1.981134970402449e+54\n",
      "Gradient Descent(37/49): loss=8.187517789284829e+55\n",
      "Gradient Descent(38/49): loss=3.383689074764854e+57\n",
      "Gradient Descent(39/49): loss=1.3983910691061083e+59\n",
      "Gradient Descent(40/49): loss=5.7791881551397926e+60\n",
      "Gradient Descent(41/49): loss=2.3883888041316724e+62\n",
      "Gradient Descent(42/49): loss=9.870592419851073e+63\n",
      "Gradient Descent(43/49): loss=4.079260233940221e+65\n",
      "Gradient Descent(44/49): loss=1.6858526163779152e+67\n",
      "Gradient Descent(45/49): loss=6.967192287713008e+68\n",
      "Gradient Descent(46/49): loss=2.879360147048971e+70\n",
      "Gradient Descent(47/49): loss=1.1899649836039157e+72\n",
      "Gradient Descent(48/49): loss=4.917817118691128e+73\n",
      "Gradient Descent(49/49): loss=2.0324064612091406e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0085499312833157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/49): loss=5.4973756520177535\n",
      "Gradient Descent(3/49): loss=48.42207332159117\n",
      "Gradient Descent(4/49): loss=650.7544650141137\n",
      "Gradient Descent(5/49): loss=15681.41501533858\n",
      "Gradient Descent(6/49): loss=544353.9266940003\n",
      "Gradient Descent(7/49): loss=21157074.484574124\n",
      "Gradient Descent(8/49): loss=843647319.4912161\n",
      "Gradient Descent(9/49): loss=33820886495.21287\n",
      "Gradient Descent(10/49): loss=1357322737460.0637\n",
      "Gradient Descent(11/49): loss=54485108098343.16\n",
      "Gradient Descent(12/49): loss=2187218442027605.2\n",
      "Gradient Descent(13/49): loss=8.780323141785949e+16\n",
      "Gradient Descent(14/49): loss=3.5247608287880146e+18\n",
      "Gradient Descent(15/49): loss=1.4149756716969042e+20\n",
      "Gradient Descent(16/49): loss=5.680261453209156e+21\n",
      "Gradient Descent(17/49): loss=2.2802774161626914e+23\n",
      "Gradient Descent(18/49): loss=9.153918625376678e+24\n",
      "Gradient Descent(19/49): loss=3.6747382428479335e+26\n",
      "Gradient Descent(20/49): loss=1.475182564744287e+28\n",
      "Gradient Descent(21/49): loss=5.9219554035440685e+29\n",
      "Gradient Descent(22/49): loss=2.3773027583089473e+31\n",
      "Gradient Descent(23/49): loss=9.543416016417867e+32\n",
      "Gradient Descent(24/49): loss=3.8310976144758937e+34\n",
      "Gradient Descent(25/49): loss=1.5379512856186257e+36\n",
      "Gradient Descent(26/49): loss=6.173933412708468e+37\n",
      "Gradient Descent(27/49): loss=2.4784565116590583e+39\n",
      "Gradient Descent(28/49): loss=9.94948644496368e+40\n",
      "Gradient Descent(29/49): loss=3.9941100460241304e+42\n",
      "Gradient Descent(30/49): loss=1.6033908029319612e+44\n",
      "Gradient Descent(31/49): loss=6.436633035401648e+45\n",
      "Gradient Descent(32/49): loss=2.5839143368331094e+47\n",
      "Gradient Descent(33/49): loss=1.037283508842327e+49\n",
      "Gradient Descent(34/49): loss=4.164058623688547e+50\n",
      "Gradient Descent(35/49): loss=1.671614758521202e+52\n",
      "Gradient Descent(36/49): loss=6.710510474107459e+53\n",
      "Gradient Descent(37/49): loss=2.69385937121915e+55\n",
      "Gradient Descent(38/49): loss=1.0814197131359866e+57\n",
      "Gradient Descent(39/49): loss=4.3412384790889555e+58\n",
      "Gradient Descent(40/49): loss=1.74274162967405e+60\n",
      "Gradient Descent(41/49): loss=6.996041342645499e+61\n",
      "Gradient Descent(42/49): loss=2.808482544664922e+63\n",
      "Gradient Descent(43/49): loss=1.1274339040290662e+65\n",
      "Gradient Descent(44/49): loss=4.525957301635538e+66\n",
      "Gradient Descent(45/49): loss=1.8168949348626097e+68\n",
      "Gradient Descent(46/49): loss=7.293721492106394e+69\n",
      "Gradient Descent(47/49): loss=2.927982911044675e+71\n",
      "Gradient Descent(48/49): loss=1.1754059894730046e+73\n",
      "Gradient Descent(49/49): loss=4.718535872861607e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0206302637534432\n",
      "Gradient Descent(2/49): loss=5.567894655480133\n",
      "Gradient Descent(3/49): loss=48.25964911373866\n",
      "Gradient Descent(4/49): loss=624.7099910442225\n",
      "Gradient Descent(5/49): loss=14519.481960600955\n",
      "Gradient Descent(6/49): loss=497194.4351941977\n",
      "Gradient Descent(7/49): loss=19266615.908701204\n",
      "Gradient Descent(8/49): loss=767970266.6548294\n",
      "Gradient Descent(9/49): loss=30791631116.414078\n",
      "Gradient Descent(10/49): loss=1236062128138.4026\n",
      "Gradient Descent(11/49): loss=49631025089297.85\n",
      "Gradient Descent(12/49): loss=1992909326304084.2\n",
      "Gradient Descent(13/49): loss=8.002508790446002e+16\n",
      "Gradient Descent(14/49): loss=3.21340641814112e+18\n",
      "Gradient Descent(15/49): loss=1.2903434787297781e+20\n",
      "Gradient Descent(16/49): loss=5.18137519635267e+21\n",
      "Gradient Descent(17/49): loss=2.080581629520466e+23\n",
      "Gradient Descent(18/49): loss=8.354577245965944e+24\n",
      "Gradient Descent(19/49): loss=3.354781183135626e+26\n",
      "Gradient Descent(20/49): loss=1.347112661353097e+28\n",
      "Gradient Descent(21/49): loss=5.409332005174213e+29\n",
      "Gradient Descent(22/49): loss=2.1721177138205534e+31\n",
      "Gradient Descent(23/49): loss=8.72214047535899e+32\n",
      "Gradient Descent(24/49): loss=3.5023762288694625e+34\n",
      "Gradient Descent(25/49): loss=1.406379464215801e+36\n",
      "Gradient Descent(26/49): loss=5.647317901099368e+37\n",
      "Gradient Descent(27/49): loss=2.267680970004861e+39\n",
      "Gradient Descent(28/49): loss=9.105874809564179e+40\n",
      "Gradient Descent(29/49): loss=3.65646478248996e+42\n",
      "Gradient Descent(30/49): loss=1.4682537356593793e+44\n",
      "Gradient Descent(31/49): loss=5.895774089227671e+45\n",
      "Gradient Descent(32/49): loss=2.367448572885629e+47\n",
      "Gradient Descent(33/49): loss=9.50649170140154e+48\n",
      "Gradient Descent(34/49): loss=3.817332528523123e+50\n",
      "Gradient Descent(35/49): loss=1.532850192376641e+52\n",
      "Gradient Descent(36/49): loss=6.155161214572539e+53\n",
      "Gradient Descent(37/49): loss=2.471605494509302e+55\n",
      "Gradient Descent(38/49): loss=9.924733906279687e+56\n",
      "Gradient Descent(39/49): loss=3.9852777204645275e+58\n",
      "Gradient Descent(40/49): loss=1.6002885980833834e+60\n",
      "Gradient Descent(41/49): loss=6.425960188433888e+61\n",
      "Gradient Descent(42/49): loss=2.5803448448481708e+63\n",
      "Gradient Descent(43/49): loss=1.0361376857451412e+65\n",
      "Gradient Descent(44/49): loss=4.160611733601252e+66\n",
      "Gradient Descent(45/49): loss=1.670693985551888e+68\n",
      "Gradient Descent(46/49): loss=6.708673080012102e+69\n",
      "Gradient Descent(47/49): loss=2.693868229830967e+71\n",
      "Gradient Descent(48/49): loss=1.0817230103690826e+73\n",
      "Gradient Descent(49/49): loss=4.3436596423104417e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.9962389461888745\n",
      "Gradient Descent(2/49): loss=5.281149761284791\n",
      "Gradient Descent(3/49): loss=43.75633850588723\n",
      "Gradient Descent(4/49): loss=507.80447929013604\n",
      "Gradient Descent(5/49): loss=10469.561815612049\n",
      "Gradient Descent(6/49): loss=343949.7846159343\n",
      "Gradient Descent(7/49): loss=13351085.593947778\n",
      "Gradient Descent(8/49): loss=538905869.8620814\n",
      "Gradient Descent(9/49): loss=21928590452.74568\n",
      "Gradient Descent(10/49): loss=893738649204.1283\n",
      "Gradient Descent(11/49): loss=36437644760845.164\n",
      "Gradient Descent(12/49): loss=1485654823362651.5\n",
      "Gradient Descent(13/49): loss=6.057467365501822e+16\n",
      "Gradient Descent(14/49): loss=2.469820282270355e+18\n",
      "Gradient Descent(15/49): loss=1.0070240489783335e+20\n",
      "Gradient Descent(16/49): loss=4.105956796228742e+21\n",
      "Gradient Descent(17/49): loss=1.674128991013923e+23\n",
      "Gradient Descent(18/49): loss=6.82595562698005e+24\n",
      "Gradient Descent(19/49): loss=2.7831589147909698e+26\n",
      "Gradient Descent(20/49): loss=1.1347822882979393e+28\n",
      "Gradient Descent(21/49): loss=4.626867819266948e+29\n",
      "Gradient Descent(22/49): loss=1.8865209686354675e+31\n",
      "Gradient Descent(23/49): loss=7.691945186516282e+32\n",
      "Gradient Descent(24/49): loss=3.136250364350897e+34\n",
      "Gradient Descent(25/49): loss=1.2787488872299792e+36\n",
      "Gradient Descent(26/49): loss=5.213865369866085e+37\n",
      "Gradient Descent(27/49): loss=2.125858514252635e+39\n",
      "Gradient Descent(28/49): loss=8.667800378467624e+40\n",
      "Gradient Descent(29/49): loss=3.5341375212533524e+42\n",
      "Gradient Descent(30/49): loss=1.440980118803709e+44\n",
      "Gradient Descent(31/49): loss=5.87533362892789e+45\n",
      "Gradient Descent(32/49): loss=2.3955601330480637e+47\n",
      "Gradient Descent(33/49): loss=9.76745954101064e+48\n",
      "Gradient Descent(34/49): loss=3.9825034892315895e+50\n",
      "Gradient Descent(35/49): loss=1.6237931649625729e+52\n",
      "Gradient Descent(36/49): loss=6.620720483255515e+53\n",
      "Gradient Descent(37/49): loss=2.699478028558452e+55\n",
      "Gradient Descent(38/49): loss=1.1006629331505378e+57\n",
      "Gradient Descent(39/49): loss=4.4877523713667376e+58\n",
      "Gradient Descent(40/49): loss=1.829799181940227e+60\n",
      "Gradient Descent(41/49): loss=7.460672446170273e+61\n",
      "Gradient Descent(42/49): loss=3.041953122419856e+63\n",
      "Gradient Descent(43/49): loss=1.2403009066227138e+65\n",
      "Gradient Descent(44/49): loss=5.0571007410704496e+66\n",
      "Gradient Descent(45/49): loss=2.061940595929501e+68\n",
      "Gradient Descent(46/49): loss=8.407186723834587e+69\n",
      "Gradient Descent(47/49): loss=3.427877056640333e+71\n",
      "Gradient Descent(48/49): loss=1.397654352332676e+73\n",
      "Gradient Descent(49/49): loss=5.698680717881687e+74\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.016602203643025\n",
      "Gradient Descent(2/49): loss=5.613593931118189\n",
      "Gradient Descent(3/49): loss=51.09198368446822\n",
      "Gradient Descent(4/49): loss=746.592903341368\n",
      "Gradient Descent(5/49): loss=19987.54202227564\n",
      "Gradient Descent(6/49): loss=747662.9160994072\n",
      "Gradient Descent(7/49): loss=30722408.487787828\n",
      "Gradient Descent(8/49): loss=1288022746.5440264\n",
      "Gradient Descent(9/49): loss=54216371781.655235\n",
      "Gradient Descent(10/49): loss=2283910699672.8906\n",
      "Gradient Descent(11/49): loss=96226517987439.34\n",
      "Gradient Descent(12/49): loss=4054371535242833.0\n",
      "Gradient Descent(13/49): loss=1.7082636021369942e+17\n",
      "Gradient Descent(14/49): loss=7.197583870889162e+18\n",
      "Gradient Descent(15/49): loss=3.032624781970579e+20\n",
      "Gradient Descent(16/49): loss=1.2777639888816433e+22\n",
      "Gradient Descent(17/49): loss=5.383721834659968e+23\n",
      "Gradient Descent(18/49): loss=2.2683735881308707e+25\n",
      "Gradient Descent(19/49): loss=9.557549396260919e+26\n",
      "Gradient Descent(20/49): loss=4.026971172028652e+28\n",
      "Gradient Descent(21/49): loss=1.696721214635004e+30\n",
      "Gradient Descent(22/49): loss=7.1489532882589265e+31\n",
      "Gradient Descent(23/49): loss=3.012134974025515e+33\n",
      "Gradient Descent(24/49): loss=1.2691308413848113e+35\n",
      "Gradient Descent(25/49): loss=5.347347002852168e+36\n",
      "Gradient Descent(26/49): loss=2.253047442902853e+38\n",
      "Gradient Descent(27/49): loss=9.492974324021734e+39\n",
      "Gradient Descent(28/49): loss=3.9997631563598876e+41\n",
      "Gradient Descent(29/49): loss=1.6852574083647445e+43\n",
      "Gradient Descent(30/49): loss=7.100651767173554e+44\n",
      "Gradient Descent(31/49): loss=2.9917836449441186e+46\n",
      "Gradient Descent(32/49): loss=1.260556026636144e+48\n",
      "Gradient Descent(33/49): loss=5.311217938415897e+49\n",
      "Gradient Descent(34/49): loss=2.2378248481845214e+51\n",
      "Gradient Descent(35/49): loss=9.428835549997801e+52\n",
      "Gradient Descent(36/49): loss=3.972738970211417e+54\n",
      "Gradient Descent(37/49): loss=1.6738710566905575e+56\n",
      "Gradient Descent(38/49): loss=7.052676592736976e+57\n",
      "Gradient Descent(39/49): loss=2.971569818530881e+59\n",
      "Gradient Descent(40/49): loss=1.252039147165348e+61\n",
      "Gradient Descent(41/49): loss=5.275332978073961e+62\n",
      "Gradient Descent(42/49): loss=2.2227051041144164e+64\n",
      "Gradient Descent(43/49): loss=9.365130126174588e+65\n",
      "Gradient Descent(44/49): loss=3.945897371533255e+67\n",
      "Gradient Descent(45/49): loss=1.6625616362933075e+69\n",
      "Gradient Descent(46/49): loss=7.005025560003841e+70\n",
      "Gradient Descent(47/49): loss=2.9514925657559624e+72\n",
      "Gradient Descent(48/49): loss=1.243579811535761e+74\n",
      "Gradient Descent(49/49): loss=5.239690472549968e+75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.023132377932565\n",
      "Gradient Descent(2/49): loss=5.733441297588278\n",
      "Gradient Descent(3/49): loss=51.706532609417145\n",
      "Gradient Descent(4/49): loss=707.6727293338816\n",
      "Gradient Descent(5/49): loss=17322.93264117331\n",
      "Gradient Descent(6/49): loss=611897.0517159351\n",
      "Gradient Descent(7/49): loss=24233139.539066598\n",
      "Gradient Descent(8/49): loss=985058723.3651723\n",
      "Gradient Descent(9/49): loss=40260718037.5343\n",
      "Gradient Descent(10/49): loss=1647352172101.9434\n",
      "Gradient Descent(11/49): loss=67420287142760.99\n",
      "Gradient Descent(12/49): loss=2759402142314436.5\n",
      "Gradient Descent(13/49): loss=1.1293889411080333e+17\n",
      "Gradient Descent(14/49): loss=4.622457304476198e+18\n",
      "Gradient Descent(15/49): loss=1.8919186499002273e+20\n",
      "Gradient Descent(16/49): loss=7.74340622716376e+21\n",
      "Gradient Descent(17/49): loss=3.1692874845817315e+23\n",
      "Gradient Descent(18/49): loss=1.2971530743888789e+25\n",
      "Gradient Descent(19/49): loss=5.309098993829855e+26\n",
      "Gradient Descent(20/49): loss=2.172953422904645e+28\n",
      "Gradient Descent(21/49): loss=8.893649532125986e+29\n",
      "Gradient Descent(22/49): loss=3.6400689111419253e+31\n",
      "Gradient Descent(23/49): loss=1.4898385224215466e+33\n",
      "Gradient Descent(24/49): loss=6.097738468900272e+34\n",
      "Gradient Descent(25/49): loss=2.4957345293148156e+36\n",
      "Gradient Descent(26/49): loss=1.0214755638638535e+38\n",
      "Gradient Descent(27/49): loss=4.1807825123831234e+39\n",
      "Gradient Descent(28/49): loss=1.7111464076274454e+41\n",
      "Gradient Descent(29/49): loss=7.003526300790937e+42\n",
      "Gradient Descent(30/49): loss=2.8664631166119683e+44\n",
      "Gradient Descent(31/49): loss=1.1732105293827385e+46\n",
      "Gradient Descent(32/49): loss=4.801816350881157e+47\n",
      "Gradient Descent(33/49): loss=1.965328446184389e+49\n",
      "Gradient Descent(34/49): loss=8.04386427788482e+50\n",
      "Gradient Descent(35/49): loss=3.2922615375893853e+52\n",
      "Gradient Descent(36/49): loss=1.3474849472150085e+54\n",
      "Gradient Descent(37/49): loss=5.515101586675547e+55\n",
      "Gradient Descent(38/49): loss=2.2572679252719033e+57\n",
      "Gradient Descent(39/49): loss=9.238739135415937e+58\n",
      "Gradient Descent(40/49): loss=3.781310134107605e+60\n",
      "Gradient Descent(41/49): loss=1.547646937610031e+62\n",
      "Gradient Descent(42/49): loss=6.334341692549525e+63\n",
      "Gradient Descent(43/49): loss=2.5925735193798016e+65\n",
      "Gradient Descent(44/49): loss=1.0611106535814407e+67\n",
      "Gradient Descent(45/49): loss=4.343004395930899e+68\n",
      "Gradient Descent(46/49): loss=1.7775419669393714e+70\n",
      "Gradient Descent(47/49): loss=7.275275722011906e+71\n",
      "Gradient Descent(48/49): loss=2.97768704287928e+73\n",
      "Gradient Descent(49/49): loss=1.2187332087641132e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.035533716892101\n",
      "Gradient Descent(2/49): loss=5.8075173900745405\n",
      "Gradient Descent(3/49): loss=51.54178282513376\n",
      "Gradient Descent(4/49): loss=679.593941220491\n",
      "Gradient Descent(5/49): loss=16043.32360366545\n",
      "Gradient Descent(6/49): loss=558928.793219899\n",
      "Gradient Descent(7/49): loss=22068190.209031574\n",
      "Gradient Descent(8/49): loss=896698014.7192893\n",
      "Gradient Descent(9/49): loss=36654577092.47946\n",
      "Gradient Descent(10/49): loss=1500175735190.0947\n",
      "Gradient Descent(11/49): loss=61413564679197.93\n",
      "Gradient Descent(12/49): loss=2514250149223966.5\n",
      "Gradient Descent(13/49): loss=1.02933594050558e+17\n",
      "Gradient Descent(14/49): loss=4.21411806437126e+18\n",
      "Gradient Descent(15/49): loss=1.725267516784562e+20\n",
      "Gradient Descent(16/49): loss=7.063276862607754e+21\n",
      "Gradient Descent(17/49): loss=2.8917185555305188e+23\n",
      "Gradient Descent(18/49): loss=1.18387490635775e+25\n",
      "Gradient Descent(19/49): loss=4.846805690137835e+26\n",
      "Gradient Descent(20/49): loss=1.984291184420478e+28\n",
      "Gradient Descent(21/49): loss=8.12372468881665e+29\n",
      "Gradient Descent(22/49): loss=3.3258678634592183e+31\n",
      "Gradient Descent(23/49): loss=1.3616164344456196e+33\n",
      "Gradient Descent(24/49): loss=5.574482783644058e+34\n",
      "Gradient Descent(25/49): loss=2.2822035280293797e+36\n",
      "Gradient Descent(26/49): loss=9.343383315545699e+37\n",
      "Gradient Descent(27/49): loss=3.82519835365425e+39\n",
      "Gradient Descent(28/49): loss=1.5660432576338948e+41\n",
      "Gradient Descent(29/49): loss=6.411409966329505e+42\n",
      "Gradient Descent(30/49): loss=2.624843059473051e+44\n",
      "Gradient Descent(31/49): loss=1.0746155873741688e+46\n",
      "Gradient Descent(32/49): loss=4.399496024952359e+47\n",
      "Gradient Descent(33/49): loss=1.8011617829652526e+49\n",
      "Gradient Descent(34/49): loss=7.373989543381225e+50\n",
      "Gradient Descent(35/49): loss=3.0189249128069486e+52\n",
      "Gradient Descent(36/49): loss=1.2359534246081198e+54\n",
      "Gradient Descent(37/49): loss=5.060016104806781e+55\n",
      "Gradient Descent(38/49): loss=2.071579921308229e+57\n",
      "Gradient Descent(39/49): loss=8.481086386841527e+58\n",
      "Gradient Descent(40/49): loss=3.472172401422118e+60\n",
      "Gradient Descent(41/49): loss=1.42151378199643e+62\n",
      "Gradient Descent(42/49): loss=5.819703628708611e+63\n",
      "Gradient Descent(43/49): loss=2.382597394056733e+65\n",
      "Gradient Descent(44/49): loss=9.754397653795051e+66\n",
      "Gradient Descent(45/49): loss=3.9934683814271746e+68\n",
      "Gradient Descent(46/49): loss=1.634933317205269e+70\n",
      "Gradient Descent(47/49): loss=6.693447140183987e+71\n",
      "Gradient Descent(48/49): loss=2.7403095983768775e+73\n",
      "Gradient Descent(49/49): loss=1.1218878012607931e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.010536609067559\n",
      "Gradient Descent(2/49): loss=5.507931701846719\n",
      "Gradient Descent(3/49): loss=46.74984507279208\n",
      "Gradient Descent(4/49): loss=553.035327769974\n",
      "Gradient Descent(5/49): loss=11578.374403797312\n",
      "Gradient Descent(6/49): loss=386734.30674197874\n",
      "Gradient Descent(7/49): loss=15291525.842514595\n",
      "Gradient Descent(8/49): loss=629147851.0787754\n",
      "Gradient Descent(9/49): loss=26099350456.11546\n",
      "Gradient Descent(10/49): loss=1084490875375.2648\n",
      "Gradient Descent(11/49): loss=45078135024781.66\n",
      "Gradient Descent(12/49): loss=1873849505907111.5\n",
      "Gradient Descent(13/49): loss=7.789493415700304e+16\n",
      "Gradient Descent(14/49): loss=3.2380598098002417e+18\n",
      "Gradient Descent(15/49): loss=1.346048619074183e+20\n",
      "Gradient Descent(16/49): loss=5.595471306711946e+21\n",
      "Gradient Descent(17/49): loss=2.326015521154564e+23\n",
      "Gradient Descent(18/49): loss=9.669155520211381e+24\n",
      "Gradient Descent(19/49): loss=4.019430124264204e+26\n",
      "Gradient Descent(20/49): loss=1.670861379028875e+28\n",
      "Gradient Descent(21/49): loss=6.945705390349515e+29\n",
      "Gradient Descent(22/49): loss=2.887302559946203e+31\n",
      "Gradient Descent(23/49): loss=1.2002403793667927e+33\n",
      "Gradient Descent(24/49): loss=4.989352304975418e+34\n",
      "Gradient Descent(25/49): loss=2.074054235393885e+36\n",
      "Gradient Descent(26/49): loss=8.621762321866352e+37\n",
      "Gradient Descent(27/49): loss=3.5840328698365704e+39\n",
      "Gradient Descent(28/49): loss=1.4898684436580787e+41\n",
      "Gradient Descent(29/49): loss=6.193324838310908e+42\n",
      "Gradient Descent(30/49): loss=2.5745409076964697e+44\n",
      "Gradient Descent(31/49): loss=1.07022658401533e+46\n",
      "Gradient Descent(32/49): loss=4.448890043692996e+47\n",
      "Gradient Descent(33/49): loss=1.8493861876063415e+49\n",
      "Gradient Descent(34/49): loss=7.687826035974517e+50\n",
      "Gradient Descent(35/49): loss=3.1957992092448843e+52\n",
      "Gradient Descent(36/49): loss=1.3284812296764764e+54\n",
      "Gradient Descent(37/49): loss=5.52244450307542e+55\n",
      "Gradient Descent(38/49): loss=2.2956585767474507e+57\n",
      "Gradient Descent(39/49): loss=9.542962899960688e+58\n",
      "Gradient Descent(40/49): loss=3.966972346517421e+60\n",
      "Gradient Descent(41/49): loss=1.6490548860982264e+62\n",
      "Gradient Descent(42/49): loss=6.855056652340278e+63\n",
      "Gradient Descent(43/49): loss=2.84962023416815e+65\n",
      "Gradient Descent(44/49): loss=1.1845759839502658e+67\n",
      "Gradient Descent(45/49): loss=4.924236026002667e+68\n",
      "Gradient Descent(46/49): loss=2.046985652952477e+70\n",
      "Gradient Descent(47/49): loss=8.509239283549955e+71\n",
      "Gradient Descent(48/49): loss=3.537257482985897e+73\n",
      "Gradient Descent(49/49): loss=1.4704240983243118e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0312171520389442\n",
      "Gradient Descent(2/49): loss=5.852518209122826\n",
      "Gradient Descent(3/49): loss=54.51198130990032\n",
      "Gradient Descent(4/49): loss=810.6839796478843\n",
      "Gradient Descent(5/49): loss=22044.61141031296\n",
      "Gradient Descent(6/49): loss=839161.4518433408\n",
      "Gradient Descent(7/49): loss=35132587.22962509\n",
      "Gradient Descent(8/49): loss=1501221366.6656103\n",
      "Gradient Descent(9/49): loss=64410165846.15848\n",
      "Gradient Descent(10/49): loss=2765757866349.8394\n",
      "Gradient Descent(11/49): loss=118779837892450.89\n",
      "Gradient Descent(12/49): loss=5101346547422019.0\n",
      "Gradient Descent(13/49): loss=2.190935459608722e+17\n",
      "Gradient Descent(14/49): loss=9.409680201783622e+18\n",
      "Gradient Descent(15/49): loss=4.041292135490337e+20\n",
      "Gradient Descent(16/49): loss=1.7356639686453241e+22\n",
      "Gradient Descent(17/49): loss=7.454371865579803e+23\n",
      "Gradient Descent(18/49): loss=3.201521781507964e+25\n",
      "Gradient Descent(19/49): loss=1.3749973714630273e+27\n",
      "Gradient Descent(20/49): loss=5.905372197001349e+28\n",
      "Gradient Descent(21/49): loss=2.536253632869438e+30\n",
      "Gradient Descent(22/49): loss=1.089276386935775e+32\n",
      "Gradient Descent(23/49): loss=4.6782507544171624e+33\n",
      "Gradient Descent(24/49): loss=2.009226527233762e+35\n",
      "Gradient Descent(25/49): loss=8.62927502107134e+36\n",
      "Gradient Descent(26/49): loss=3.706122051444638e+38\n",
      "Gradient Descent(27/49): loss=1.5917143243974209e+40\n",
      "Gradient Descent(28/49): loss=6.8361334444028e+41\n",
      "Gradient Descent(29/49): loss=2.93599924015091e+43\n",
      "Gradient Descent(30/49): loss=1.2609601038763614e+45\n",
      "Gradient Descent(31/49): loss=5.415602163051476e+46\n",
      "Gradient Descent(32/49): loss=2.3259060059304983e+48\n",
      "Gradient Descent(33/49): loss=9.989357758464773e+49\n",
      "Gradient Descent(34/49): loss=4.290253697792067e+51\n",
      "Gradient Descent(35/49): loss=1.8425886064417808e+53\n",
      "Gradient Descent(36/49): loss=7.91359442061997e+54\n",
      "Gradient Descent(37/49): loss=3.3987498042225947e+56\n",
      "Gradient Descent(38/49): loss=1.4597033430982854e+58\n",
      "Gradient Descent(39/49): loss=6.269169466976198e+59\n",
      "Gradient Descent(40/49): loss=2.692498170364171e+61\n",
      "Gradient Descent(41/49): loss=1.1563806714115306e+63\n",
      "Gradient Descent(42/49): loss=4.9664518696154026e+64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=2.1330038440628058e+66\n",
      "Gradient Descent(44/49): loss=9.16087685581256e+67\n",
      "Gradient Descent(45/49): loss=3.934435702071294e+69\n",
      "Gradient Descent(46/49): loss=1.6897710270946207e+71\n",
      "Gradient Descent(47/49): loss=7.257269759180814e+72\n",
      "Gradient Descent(48/49): loss=3.116869890240611e+74\n",
      "Gradient Descent(49/49): loss=1.3386408711621073e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0378866398787299\n",
      "Gradient Descent(2/49): loss=5.977585592422708\n",
      "Gradient Descent(3/49): loss=55.17656056643112\n",
      "Gradient Descent(4/49): loss=768.9294269727084\n",
      "Gradient Descent(5/49): loss=19118.12299239186\n",
      "Gradient Descent(6/49): loss=687053.5020874079\n",
      "Gradient Descent(7/49): loss=27720385.808520664\n",
      "Gradient Descent(8/49): loss=1148466081.5835528\n",
      "Gradient Descent(9/49): loss=47846723477.81287\n",
      "Gradient Descent(10/49): loss=1995644883857.5854\n",
      "Gradient Descent(11/49): loss=83256126190921.86\n",
      "Gradient Descent(12/49): loss=3473521576587700.0\n",
      "Gradient Descent(13/49): loss=1.449199162482119e+17\n",
      "Gradient Descent(14/49): loss=6.04626283899637e+18\n",
      "Gradient Descent(15/49): loss=2.5225869806103577e+20\n",
      "Gradient Descent(16/49): loss=1.0524593094256878e+22\n",
      "Gradient Descent(17/49): loss=4.391010600087386e+23\n",
      "Gradient Descent(18/49): loss=1.8319923616683418e+25\n",
      "Gradient Descent(19/49): loss=7.643333895795062e+26\n",
      "Gradient Descent(20/49): loss=3.1889081130272235e+28\n",
      "Gradient Descent(21/49): loss=1.3304580294777487e+30\n",
      "Gradient Descent(22/49): loss=5.550861001549688e+31\n",
      "Gradient Descent(23/49): loss=2.3158985233573206e+33\n",
      "Gradient Descent(24/49): loss=9.662259546749274e+34\n",
      "Gradient Descent(25/49): loss=4.031232742158781e+36\n",
      "Gradient Descent(26/49): loss=1.6818879003224822e+38\n",
      "Gradient Descent(27/49): loss=7.017076636801649e+39\n",
      "Gradient Descent(28/49): loss=2.927624636416357e+41\n",
      "Gradient Descent(29/49): loss=1.221446829695516e+43\n",
      "Gradient Descent(30/49): loss=5.096050699995165e+44\n",
      "Gradient Descent(31/49): loss=2.126145166989785e+46\n",
      "Gradient Descent(32/49): loss=8.870581431064382e+47\n",
      "Gradient Descent(33/49): loss=3.7009333203975056e+49\n",
      "Gradient Descent(34/49): loss=1.544082262078389e+51\n",
      "Gradient Descent(35/49): loss=6.442131823680161e+52\n",
      "Gradient Descent(36/49): loss=2.6877494452796344e+54\n",
      "Gradient Descent(37/49): loss=1.1213674724951458e+56\n",
      "Gradient Descent(38/49): loss=4.6785053219100424e+57\n",
      "Gradient Descent(39/49): loss=1.9519392691530074e+59\n",
      "Gradient Descent(40/49): loss=8.14376953386885e+60\n",
      "Gradient Descent(41/49): loss=3.3976970118311783e+62\n",
      "Gradient Descent(42/49): loss=1.4175677413506316e+64\n",
      "Gradient Descent(43/49): loss=5.914295166168868e+65\n",
      "Gradient Descent(44/49): loss=2.4675284497685616e+67\n",
      "Gradient Descent(45/49): loss=1.0294881265388997e+69\n",
      "Gradient Descent(46/49): loss=4.2951715623946597e+70\n",
      "Gradient Descent(47/49): loss=1.7920069474163245e+72\n",
      "Gradient Descent(48/49): loss=7.476509035643871e+73\n",
      "Gradient Descent(49/49): loss=3.1193063978159414e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0506126435125158\n",
      "Gradient Descent(2/49): loss=6.055348452055376\n",
      "Gradient Descent(3/49): loss=55.009801847676954\n",
      "Gradient Descent(4/49): loss=738.6796904044779\n",
      "Gradient Descent(5/49): loss=17710.228733746084\n",
      "Gradient Descent(6/49): loss=627627.2990050047\n",
      "Gradient Descent(7/49): loss=25244312.7577281\n",
      "Gradient Descent(8/49): loss=1045449132.2430639\n",
      "Gradient Descent(9/49): loss=43561007848.87435\n",
      "Gradient Descent(10/49): loss=1817345441249.8345\n",
      "Gradient Descent(11/49): loss=75838235808732.1\n",
      "Gradient Descent(12/49): loss=3164911946363251.5\n",
      "Gradient Descent(13/49): loss=1.3208079232193882e+17\n",
      "Gradient Descent(14/49): loss=5.512119727439157e+18\n",
      "Gradient Descent(15/49): loss=2.3003706145727637e+20\n",
      "Gradient Descent(16/49): loss=9.600127906795722e+21\n",
      "Gradient Descent(17/49): loss=4.006417789357724e+23\n",
      "Gradient Descent(18/49): loss=1.6719968441016673e+25\n",
      "Gradient Descent(19/49): loss=6.977738208560452e+26\n",
      "Gradient Descent(20/49): loss=2.9120168908551373e+28\n",
      "Gradient Descent(21/49): loss=1.215270925826973e+30\n",
      "Gradient Descent(22/49): loss=5.071685634131912e+31\n",
      "Gradient Descent(23/49): loss=2.116564679105597e+33\n",
      "Gradient Descent(24/49): loss=8.833051502028823e+34\n",
      "Gradient Descent(25/49): loss=3.686294097587717e+36\n",
      "Gradient Descent(26/49): loss=1.5383997445038092e+38\n",
      "Gradient Descent(27/49): loss=6.420197931136672e+39\n",
      "Gradient Descent(28/49): loss=2.679338814390315e+41\n",
      "Gradient Descent(29/49): loss=1.118167470738309e+43\n",
      "Gradient Descent(30/49): loss=4.6664441462281025e+44\n",
      "Gradient Descent(31/49): loss=1.947445399702814e+46\n",
      "Gradient Descent(32/49): loss=8.127266642394259e+47\n",
      "Gradient Descent(33/49): loss=3.39174916465713e+49\n",
      "Gradient Descent(34/49): loss=1.4154774172099249e+51\n",
      "Gradient Descent(35/49): loss=5.907206639892652e+52\n",
      "Gradient Descent(36/49): loss=2.465252349639977e+54\n",
      "Gradient Descent(37/49): loss=1.0288228460407946e+56\n",
      "Gradient Descent(38/49): loss=4.293582556325708e+57\n",
      "Gradient Descent(39/49): loss=1.7918392110873935e+59\n",
      "Gradient Descent(40/49): loss=7.477875914275636e+60\n",
      "Gradient Descent(41/49): loss=3.1207391736544475e+62\n",
      "Gradient Descent(42/49): loss=1.3023769184761578e+64\n",
      "Gradient Descent(43/49): loss=5.435204749242678e+65\n",
      "Gradient Descent(44/49): loss=2.268271976192172e+67\n",
      "Gradient Descent(45/49): loss=9.466171000633879e+68\n",
      "Gradient Descent(46/49): loss=3.950513622430296e+70\n",
      "Gradient Descent(47/49): loss=1.6486663805209322e+72\n",
      "Gradient Descent(48/49): loss=6.8803732730527e+73\n",
      "Gradient Descent(49/49): loss=2.871383618654219e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0250030650745874\n",
      "Gradient Descent(2/49): loss=5.742498228670352\n",
      "Gradient Descent(3/49): loss=49.913912420766486\n",
      "Gradient Descent(4/49): loss=601.7755567400644\n",
      "Gradient Descent(5/49): loss=12792.323544010866\n",
      "Gradient Descent(6/49): loss=434356.1975797457\n",
      "Gradient Descent(7/49): loss=17491289.70899128\n",
      "Gradient Descent(8/49): loss=733413299.6636095\n",
      "Gradient Descent(9/49): loss=31011614313.85315\n",
      "Gradient Descent(10/49): loss=1313519239781.8157\n",
      "Gradient Descent(11/49): loss=55653984140314.914\n",
      "Gradient Descent(12/49): loss=2358227445323679.5\n",
      "Gradient Descent(13/49): loss=9.992658611547806e+16\n",
      "Gradient Descent(14/49): loss=4.234260759034172e+18\n",
      "Gradient Descent(15/49): loss=1.79421460284445e+20\n",
      "Gradient Descent(16/49): loss=7.602758964808529e+21\n",
      "Gradient Descent(17/49): loss=3.2215736655270804e+23\n",
      "Gradient Descent(18/49): loss=1.3651014028176154e+25\n",
      "Gradient Descent(19/49): loss=5.784445848810068e+26\n",
      "Gradient Descent(20/49): loss=2.451086323306736e+28\n",
      "Gradient Descent(21/49): loss=1.0386170640269977e+30\n",
      "Gradient Descent(22/49): loss=4.4010094440101e+31\n",
      "Gradient Descent(23/49): loss=1.864872511452396e+33\n",
      "Gradient Descent(24/49): loss=7.902163192821851e+34\n",
      "Gradient Descent(25/49): loss=3.348442466845165e+36\n",
      "Gradient Descent(26/49): loss=1.4188604664551495e+38\n",
      "Gradient Descent(27/49): loss=6.012243134540455e+39\n",
      "Gradient Descent(28/49): loss=2.5476125639851057e+41\n",
      "Gradient Descent(29/49): loss=1.079518846948777e+43\n",
      "Gradient Descent(30/49): loss=4.574325615252429e+44\n",
      "Gradient Descent(31/49): loss=1.938313063592787e+46\n",
      "Gradient Descent(32/49): loss=8.21335831443031e+47\n",
      "Gradient Descent(33/49): loss=3.4803074935780173e+49\n",
      "Gradient Descent(34/49): loss=1.4747366163941112e+51\n",
      "Gradient Descent(35/49): loss=6.249011306462607e+52\n",
      "Gradient Descent(36/49): loss=2.647940104978137e+54\n",
      "Gradient Descent(37/49): loss=1.1220313831566182e+56\n",
      "Gradient Descent(38/49): loss=4.754467151358662e+57\n",
      "Gradient Descent(39/49): loss=2.0146457784231575e+59\n",
      "Gradient Descent(40/49): loss=8.536808612419129e+60\n",
      "Gradient Descent(41/49): loss=3.617365497477872e+62\n",
      "Gradient Descent(42/49): loss=1.5328132252264857e+64\n",
      "Gradient Descent(43/49): loss=6.495103646748976e+65\n",
      "Gradient Descent(44/49): loss=2.752218645280672e+67\n",
      "Gradient Descent(45/49): loss=1.1662181057298207e+69\n",
      "Gradient Descent(46/49): loss=4.941702842048008e+70\n",
      "Gradient Descent(47/49): loss=2.093984552214004e+72\n",
      "Gradient Descent(48/49): loss=8.872996707939642e+73\n",
      "Gradient Descent(49/49): loss=3.759820983200011e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0460025125015118\n",
      "Gradient Descent(2/49): loss=6.0995377438006555\n",
      "Gradient Descent(3/49): loss=58.12270778526898\n",
      "Gradient Descent(4/49): loss=879.5712999386546\n",
      "Gradient Descent(5/49): loss=24290.95914078645\n",
      "Gradient Descent(6/49): loss=940831.271766389\n",
      "Gradient Descent(7/49): loss=40124846.30621524\n",
      "Gradient Descent(8/49): loss=1747171267.8989673\n",
      "Gradient Descent(9/49): loss=76395726585.95253\n",
      "Gradient Descent(10/49): loss=3343189874479.4663\n",
      "Gradient Descent(11/49): loss=146326745631930.25\n",
      "Gradient Descent(12/49): loss=6404722554263244.0\n",
      "Gradient Descent(13/49): loss=2.8033651874312406e+17\n",
      "Gradient Descent(14/49): loss=1.2270424059763513e+19\n",
      "Gradient Descent(15/49): loss=5.370807352665485e+20\n",
      "Gradient Descent(16/49): loss=2.350821200332731e+22\n",
      "Gradient Descent(17/49): loss=1.0289626876968979e+24\n",
      "Gradient Descent(18/49): loss=4.503805795673566e+25\n",
      "Gradient Descent(19/49): loss=1.9713316036822727e+27\n",
      "Gradient Descent(20/49): loss=8.628587617374096e+28\n",
      "Gradient Descent(21/49): loss=3.7767630840344297e+30\n",
      "Gradient Descent(22/49): loss=1.653102457260171e+32\n",
      "Gradient Descent(23/49): loss=7.235687474687707e+33\n",
      "Gradient Descent(24/49): loss=3.167085802904954e+35\n",
      "Gradient Descent(25/49): loss=1.3862445715146411e+37\n",
      "Gradient Descent(26/49): loss=6.0676411428172076e+38\n",
      "Gradient Descent(27/49): loss=2.655827823930212e+40\n",
      "Gradient Descent(28/49): loss=1.1624651597452478e+42\n",
      "Gradient Descent(29/49): loss=5.088150803472726e+43\n",
      "Gradient Descent(30/49): loss=2.2271014646626762e+45\n",
      "Gradient Descent(31/49): loss=9.748101275845388e+46\n",
      "Gradient Descent(32/49): loss=4.2667781415397277e+48\n",
      "Gradient Descent(33/49): loss=1.867583767746826e+50\n",
      "Gradient Descent(34/49): loss=8.17447969838167e+51\n",
      "Gradient Descent(35/49): loss=3.5779984541133665e+53\n",
      "Gradient Descent(36/49): loss=1.5661024811367673e+55\n",
      "Gradient Descent(37/49): loss=6.854885525741666e+56\n",
      "Gradient Descent(38/49): loss=3.000407453343294e+58\n",
      "Gradient Descent(39/49): loss=1.3132888729172288e+60\n",
      "Gradient Descent(40/49): loss=5.748311489515792e+61\n",
      "Gradient Descent(41/49): loss=2.5160561139225893e+63\n",
      "Gradient Descent(42/49): loss=1.1012865917153824e+65\n",
      "Gradient Descent(43/49): loss=4.8203700640095876e+66\n",
      "Gradient Descent(44/49): loss=2.1098928951643003e+68\n",
      "Gradient Descent(45/49): loss=9.235075253458645e+69\n",
      "Gradient Descent(46/49): loss=4.0422248509635354e+71\n",
      "Gradient Descent(47/49): loss=1.7692960043425478e+73\n",
      "Gradient Descent(48/49): loss=7.744270708336275e+74\n",
      "Gradient Descent(49/49): loss=3.389694469257472e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0528127171218105\n",
      "Gradient Descent(2/49): loss=6.22999103451753\n",
      "Gradient Descent(3/49): loss=58.840475505519215\n",
      "Gradient Descent(4/49): loss=834.8066468291196\n",
      "Gradient Descent(5/49): loss=21079.697389935896\n",
      "Gradient Descent(6/49): loss=770596.6012845676\n",
      "Gradient Descent(7/49): loss=31669053.451510277\n",
      "Gradient Descent(8/49): loss=1337030577.0455685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=56768946604.02301\n",
      "Gradient Descent(10/49): loss=2413175329201.6206\n",
      "Gradient Descent(11/49): loss=102605697523175.83\n",
      "Gradient Descent(12/49): loss=4362902996514069.5\n",
      "Gradient Descent(13/49): loss=1.8551714363484774e+17\n",
      "Gradient Descent(14/49): loss=7.888482145501751e+18\n",
      "Gradient Descent(15/49): loss=3.354308718155488e+20\n",
      "Gradient Descent(16/49): loss=1.4263058164330592e+22\n",
      "Gradient Descent(17/49): loss=6.064880992478787e+23\n",
      "Gradient Descent(18/49): loss=2.57888463790179e+25\n",
      "Gradient Descent(19/49): loss=1.0965830968364875e+27\n",
      "Gradient Descent(20/49): loss=4.662847150209105e+28\n",
      "Gradient Descent(21/49): loss=1.9827173708618392e+30\n",
      "Gradient Descent(22/49): loss=8.430832163492812e+31\n",
      "Gradient Descent(23/49): loss=3.584925013199609e+33\n",
      "Gradient Descent(24/49): loss=1.5243675951608287e+35\n",
      "Gradient Descent(25/49): loss=6.481855426879795e+36\n",
      "Gradient Descent(26/49): loss=2.7561888555194193e+38\n",
      "Gradient Descent(27/49): loss=1.171975693223122e+40\n",
      "Gradient Descent(28/49): loss=4.9834285584429765e+41\n",
      "Gradient Descent(29/49): loss=2.1190337257597923e+43\n",
      "Gradient Descent(30/49): loss=9.01047116106425e+44\n",
      "Gradient Descent(31/49): loss=3.831396808715715e+46\n",
      "Gradient Descent(32/49): loss=1.629171354464819e+48\n",
      "Gradient Descent(33/49): loss=6.927497815342201e+49\n",
      "Gradient Descent(34/49): loss=2.9456831443820088e+51\n",
      "Gradient Descent(35/49): loss=1.252551703138679e+53\n",
      "Gradient Descent(36/49): loss=5.326050671905337e+54\n",
      "Gradient Descent(37/49): loss=2.2647221418980596e+56\n",
      "Gradient Descent(38/49): loss=9.629961665701944e+57\n",
      "Gradient Descent(39/49): loss=4.094814104001576e+59\n",
      "Gradient Descent(40/49): loss=1.7411806119695657e+61\n",
      "Gradient Descent(41/49): loss=7.40377913746088e+62\n",
      "Gradient Descent(42/49): loss=3.148205599090245e+64\n",
      "Gradient Descent(43/49): loss=1.3386674980612963e+66\n",
      "Gradient Descent(44/49): loss=5.692228839449334e+67\n",
      "Gradient Descent(45/49): loss=2.4204269699221484e+69\n",
      "Gradient Descent(46/49): loss=1.0292043559677836e+71\n",
      "Gradient Descent(47/49): loss=4.37634194093084e+72\n",
      "Gradient Descent(48/49): loss=1.8608907621597102e+74\n",
      "Gradient Descent(49/49): loss=7.912805890014289e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.065867043614688\n",
      "Gradient Descent(2/49): loss=6.311573440236443\n",
      "Gradient Descent(3/49): loss=58.67205696327908\n",
      "Gradient Descent(4/49): loss=802.2416131375047\n",
      "Gradient Descent(5/49): loss=19532.067786843356\n",
      "Gradient Descent(6/49): loss=703997.9877940001\n",
      "Gradient Descent(7/49): loss=28840756.20819105\n",
      "Gradient Descent(8/49): loss=1217101370.7560928\n",
      "Gradient Descent(9/49): loss=51683938528.89335\n",
      "Gradient Descent(10/49): loss=2197564693625.7195\n",
      "Gradient Descent(11/49): loss=93463457940284.9\n",
      "Gradient Descent(12/49): loss=3975258422090756.0\n",
      "Gradient Descent(13/49): loss=1.6908055441220672e+17\n",
      "Gradient Descent(14/49): loss=7.191557162687913e+18\n",
      "Gradient Descent(15/49): loss=3.058809358458926e+20\n",
      "Gradient Descent(16/49): loss=1.301013863917354e+22\n",
      "Gradient Descent(17/49): loss=5.533646946655083e+23\n",
      "Gradient Descent(18/49): loss=2.353645068667924e+25\n",
      "Gradient Descent(19/49): loss=1.0010839446622596e+27\n",
      "Gradient Descent(20/49): loss=4.257944741824628e+28\n",
      "Gradient Descent(21/49): loss=1.811046268568375e+30\n",
      "Gradient Descent(22/49): loss=7.702985326936222e+31\n",
      "Gradient Descent(23/49): loss=3.276337218815616e+33\n",
      "Gradient Descent(24/49): loss=1.393535767731916e+35\n",
      "Gradient Descent(25/49): loss=5.927173566859826e+36\n",
      "Gradient Descent(26/49): loss=2.5210251006947813e+38\n",
      "Gradient Descent(27/49): loss=1.0722762690582664e+40\n",
      "Gradient Descent(28/49): loss=4.560749501734941e+41\n",
      "Gradient Descent(29/49): loss=1.939839257642372e+43\n",
      "Gradient Descent(30/49): loss=8.250784973081968e+44\n",
      "Gradient Descent(31/49): loss=3.509334724711884e+46\n",
      "Gradient Descent(32/49): loss=1.4926373975624645e+48\n",
      "Gradient Descent(33/49): loss=6.348685934440169e+49\n",
      "Gradient Descent(34/49): loss=2.7003084044376038e+51\n",
      "Gradient Descent(35/49): loss=1.1485314527090874e+53\n",
      "Gradient Descent(36/49): loss=4.885088294708166e+54\n",
      "Gradient Descent(37/49): loss=2.0777913909807042e+56\n",
      "Gradient Descent(38/49): loss=8.837541522248765e+57\n",
      "Gradient Descent(39/49): loss=3.7589019040361965e+59\n",
      "Gradient Descent(40/49): loss=1.59878666353032e+61\n",
      "Gradient Descent(41/49): loss=6.800174254980327e+62\n",
      "Gradient Descent(42/49): loss=2.892341483258852e+64\n",
      "Gradient Descent(43/49): loss=1.230209542005961e+66\n",
      "Gradient Descent(44/49): loss=5.2324925186128444e+67\n",
      "Gradient Descent(45/49): loss=2.2255540233165843e+69\n",
      "Gradient Descent(46/49): loss=9.466025403919457e+70\n",
      "Gradient Descent(47/49): loss=4.0262171130815304e+72\n",
      "Gradient Descent(48/49): loss=1.7124847599667943e+74\n",
      "Gradient Descent(49/49): loss=7.28377027555288e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0396383142099594\n",
      "Gradient Descent(2/49): loss=5.985025696531354\n",
      "Gradient Descent(3/49): loss=53.25624388720542\n",
      "Gradient Descent(4/49): loss=654.2572613554828\n",
      "Gradient Descent(5/49): loss=14120.226533804173\n",
      "Gradient Descent(6/49): loss=487308.61658199097\n",
      "Gradient Descent(7/49): loss=19982060.621781144\n",
      "Gradient Descent(8/49): loss=853715793.8794688\n",
      "Gradient Descent(9/49): loss=36788195574.812454\n",
      "Gradient Descent(10/49): loss=1588025316023.1047\n",
      "Gradient Descent(11/49): loss=68573775093009.05\n",
      "Gradient Descent(12/49): loss=2961346248067563.5\n",
      "Gradient Descent(13/49): loss=1.2788701406486725e+17\n",
      "Gradient Descent(14/49): loss=5.522871481028656e+18\n",
      "Gradient Descent(15/49): loss=2.385083974130221e+20\n",
      "Gradient Descent(16/49): loss=1.0300124191690957e+22\n",
      "Gradient Descent(17/49): loss=4.4481687838137395e+23\n",
      "Gradient Descent(18/49): loss=1.920967675003899e+25\n",
      "Gradient Descent(19/49): loss=8.295811133034484e+26\n",
      "Gradient Descent(20/49): loss=3.5825945050161715e+28\n",
      "Gradient Descent(21/49): loss=1.5471643678992795e+30\n",
      "Gradient Descent(22/49): loss=6.68151971417276e+31\n",
      "Gradient Descent(23/49): loss=2.885453324624e+33\n",
      "Gradient Descent(24/49): loss=1.2460998761891532e+35\n",
      "Gradient Descent(25/49): loss=5.38135511736642e+36\n",
      "Gradient Descent(26/49): loss=2.323969647422563e+38\n",
      "Gradient Descent(27/49): loss=1.0036198697818959e+40\n",
      "Gradient Descent(28/49): loss=4.334191043063484e+41\n",
      "Gradient Descent(29/49): loss=1.8717457239915076e+43\n",
      "Gradient Descent(30/49): loss=8.083243263785881e+44\n",
      "Gradient Descent(31/49): loss=3.490795828944473e+46\n",
      "Gradient Descent(32/49): loss=1.507520573328439e+48\n",
      "Gradient Descent(33/49): loss=6.510315671185289e+49\n",
      "Gradient Descent(34/49): loss=2.811517858419702e+51\n",
      "Gradient Descent(35/49): loss=1.2141704131489514e+53\n",
      "Gradient Descent(36/49): loss=5.2434658657828584e+54\n",
      "Gradient Descent(37/49): loss=2.264421368523135e+56\n",
      "Gradient Descent(38/49): loss=9.779035976347406e+57\n",
      "Gradient Descent(39/49): loss=4.2231338193504125e+59\n",
      "Gradient Descent(40/49): loss=1.823785013091134e+61\n",
      "Gradient Descent(41/49): loss=7.876122131709699e+62\n",
      "Gradient Descent(42/49): loss=3.401349357974381e+64\n",
      "Gradient Descent(43/49): loss=1.4688925922586347e+66\n",
      "Gradient Descent(44/49): loss=6.34349847813709e+67\n",
      "Gradient Descent(45/49): loss=2.739476878990404e+69\n",
      "Gradient Descent(46/49): loss=1.183059095290712e+71\n",
      "Gradient Descent(47/49): loss=5.109109821966856e+72\n",
      "Gradient Descent(48/49): loss=2.206398926040407e+74\n",
      "Gradient Descent(49/49): loss=9.52846266858743e+75\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.060958285030728\n",
      "Gradient Descent(2/49): loss=6.354833635660039\n",
      "Gradient Descent(3/49): loss=61.93260697021463\n",
      "Gradient Descent(4/49): loss=953.5611916668288\n",
      "Gradient Descent(5/49): loss=26741.96944629543\n",
      "Gradient Descent(6/49): loss=1053691.1558323912\n",
      "Gradient Descent(7/49): loss=45769414.17098149\n",
      "Gradient Descent(8/49): loss=2030521528.529567\n",
      "Gradient Descent(9/49): loss=90466485424.62997\n",
      "Gradient Descent(10/49): loss=4033987818984.142\n",
      "Gradient Descent(11/49): loss=179909464992128.03\n",
      "Gradient Descent(12/49): loss=8023942192101526.0\n",
      "Gradient Descent(13/49): loss=3.5786927008148384e+17\n",
      "Gradient Descent(14/49): loss=1.5961054657149819e+19\n",
      "Gradient Descent(15/49): loss=7.118670199629835e+20\n",
      "Gradient Descent(16/49): loss=3.174944830302553e+22\n",
      "Gradient Descent(17/49): loss=1.4160334013823105e+24\n",
      "Gradient Descent(18/49): loss=6.315544694353475e+25\n",
      "Gradient Descent(19/49): loss=2.816748867859156e+27\n",
      "Gradient Descent(20/49): loss=1.2562771018454649e+29\n",
      "Gradient Descent(21/49): loss=5.603027570735668e+30\n",
      "Gradient Descent(22/49): loss=2.49896443327681e+32\n",
      "Gradient Descent(23/49): loss=1.114544442258791e+34\n",
      "Gradient Descent(24/49): loss=4.970896333011203e+35\n",
      "Gradient Descent(25/49): loss=2.2170323063534958e+37\n",
      "Gradient Descent(26/49): loss=9.888020023217003e+38\n",
      "Gradient Descent(27/49): loss=4.410081878344656e+40\n",
      "Gradient Descent(28/49): loss=1.966907644608065e+42\n",
      "Gradient Descent(29/49): loss=8.772457721056954e+43\n",
      "Gradient Descent(30/49): loss=3.912538276959455e+45\n",
      "Gradient Descent(31/49): loss=1.7450019430619052e+47\n",
      "Gradient Descent(32/49): loss=7.782752693364381e+48\n",
      "Gradient Descent(33/49): loss=3.471127337531116e+50\n",
      "Gradient Descent(34/49): loss=1.5481315503740558e+52\n",
      "Gradient Descent(35/49): loss=6.904705774834165e+53\n",
      "Gradient Descent(36/49): loss=3.0795161965084917e+55\n",
      "Gradient Descent(37/49): loss=1.373471993422602e+57\n",
      "Gradient Descent(38/49): loss=6.125719744079056e+58\n",
      "Gradient Descent(39/49): loss=2.7320864613694458e+60\n",
      "Gradient Descent(40/49): loss=1.218517454967297e+62\n",
      "Gradient Descent(41/49): loss=5.4346185929845174e+63\n",
      "Gradient Descent(42/49): loss=2.423853604297021e+65\n",
      "Gradient Descent(43/49): loss=1.0810448230254155e+67\n",
      "Gradient Descent(44/49): loss=4.821487186017598e+68\n",
      "Gradient Descent(45/49): loss=2.150395449827289e+70\n",
      "Gradient Descent(46/49): loss=9.590817961827758e+71\n",
      "Gradient Descent(47/49): loss=4.27752900910878e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=1.90778873049111e+75\n",
      "Gradient Descent(49/49): loss=8.508785872494359e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0679106096618065\n",
      "Gradient Descent(2/49): loss=6.490842167622914\n",
      "Gradient Descent(3/49): loss=62.70687306256256\n",
      "Gradient Descent(4/49): loss=905.6016859745313\n",
      "Gradient Descent(5/49): loss=23221.281971646567\n",
      "Gradient Descent(6/49): loss=863369.7676134122\n",
      "Gradient Descent(7/49): loss=36134955.54860702\n",
      "Gradient Descent(8/49): loss=1554331060.521184\n",
      "Gradient Descent(9/49): loss=67246674710.45015\n",
      "Gradient Descent(10/49): loss=2912850418450.999\n",
      "Gradient Descent(11/49): loss=126203909413848.4\n",
      "Gradient Descent(12/49): loss=5468264751962204.0\n",
      "Gradient Descent(13/49): loss=2.3693586839397347e+17\n",
      "Gradient Descent(14/49): loss=1.0266278155205272e+19\n",
      "Gradient Descent(15/49): loss=4.448314000081327e+20\n",
      "Gradient Descent(16/49): loss=1.927426761448412e+22\n",
      "Gradient Descent(17/49): loss=8.351420315242823e+23\n",
      "Gradient Descent(18/49): loss=3.6186185098260277e+25\n",
      "Gradient Descent(19/49): loss=1.5679249080839002e+27\n",
      "Gradient Descent(20/49): loss=6.793721170441512e+28\n",
      "Gradient Descent(21/49): loss=2.943677156049433e+30\n",
      "Gradient Descent(22/49): loss=1.2754770149839604e+32\n",
      "Gradient Descent(23/49): loss=5.526562627329768e+33\n",
      "Gradient Descent(24/49): loss=2.3946252354999285e+35\n",
      "Gradient Descent(25/49): loss=1.0375762305011829e+37\n",
      "Gradient Descent(26/49): loss=4.495753315137291e+38\n",
      "Gradient Descent(27/49): loss=1.94798196762905e+40\n",
      "Gradient Descent(28/49): loss=8.440484786901755e+41\n",
      "Gradient Descent(29/49): loss=3.6572095954580175e+43\n",
      "Gradient Descent(30/49): loss=1.5846461859472688e+45\n",
      "Gradient Descent(31/49): loss=6.866173428386146e+46\n",
      "Gradient Descent(32/49): loss=2.975070269108344e+48\n",
      "Gradient Descent(33/49): loss=1.2890794557475562e+50\n",
      "Gradient Descent(34/49): loss=5.5855011576867184e+51\n",
      "Gradient Descent(35/49): loss=2.4201629343652215e+53\n",
      "Gradient Descent(36/49): loss=1.0486415566872917e+55\n",
      "Gradient Descent(37/49): loss=4.543698685725035e+56\n",
      "Gradient Descent(38/49): loss=1.9687563986953377e+58\n",
      "Gradient Descent(39/49): loss=8.530499105455785e+59\n",
      "Gradient Descent(40/49): loss=3.696212240194178e+61\n",
      "Gradient Descent(41/49): loss=1.6015457894864994e+63\n",
      "Gradient Descent(42/49): loss=6.939398360109247e+64\n",
      "Gradient Descent(43/49): loss=3.006798176886806e+66\n",
      "Gradient Descent(44/49): loss=1.3028269609798318e+68\n",
      "Gradient Descent(45/49): loss=5.645068243367743e+69\n",
      "Gradient Descent(46/49): loss=2.4459729823455055e+71\n",
      "Gradient Descent(47/49): loss=1.059824890052164e+73\n",
      "Gradient Descent(48/49): loss=4.592155374083424e+74\n",
      "Gradient Descent(49/49): loss=1.9897523805734426e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0812969171986173\n",
      "Gradient Descent(2/49): loss=6.576380035600387\n",
      "Gradient Descent(3/49): loss=62.53717791300896\n",
      "Gradient Descent(4/49): loss=870.5689371714535\n",
      "Gradient Descent(5/49): loss=21521.568345795746\n",
      "Gradient Descent(6/49): loss=788813.1395856044\n",
      "Gradient Descent(7/49): loss=32908371.68718391\n",
      "Gradient Descent(8/49): loss=1414912712.4777808\n",
      "Gradient Descent(9/49): loss=61223007178.90207\n",
      "Gradient Descent(10/49): loss=2652586619007.1685\n",
      "Gradient Descent(11/49): loss=114958623158190.75\n",
      "Gradient Descent(12/49): loss=4982388119072090.0\n",
      "Gradient Descent(13/49): loss=2.1594267930065978e+17\n",
      "Gradient Descent(14/49): loss=9.35923661546955e+18\n",
      "Gradient Descent(15/49): loss=4.056416830180976e+20\n",
      "Gradient Descent(16/49): loss=1.7581048314922826e+22\n",
      "Gradient Descent(17/49): loss=7.619859570803841e+23\n",
      "Gradient Descent(18/49): loss=3.3025482370690507e+25\n",
      "Gradient Descent(19/49): loss=1.4313682248408433e+27\n",
      "Gradient Descent(20/49): loss=6.203739804442719e+28\n",
      "Gradient Descent(21/49): loss=2.6887831443147856e+30\n",
      "Gradient Descent(22/49): loss=1.1653542903309137e+32\n",
      "Gradient Descent(23/49): loss=5.0508001170201525e+33\n",
      "Gradient Descent(24/49): loss=2.189083786258224e+35\n",
      "Gradient Descent(25/49): loss=9.48777958389325e+36\n",
      "Gradient Descent(26/49): loss=4.112129558385375e+38\n",
      "Gradient Descent(27/49): loss=1.7822515115815582e+40\n",
      "Gradient Descent(28/49): loss=7.724514525709696e+41\n",
      "Gradient Descent(29/49): loss=3.3479070866351194e+43\n",
      "Gradient Descent(30/49): loss=1.4510273523903455e+45\n",
      "Gradient Descent(31/49): loss=6.288945071952775e+46\n",
      "Gradient Descent(32/49): loss=2.7257122378075413e+48\n",
      "Gradient Descent(33/49): loss=1.181359849439247e+50\n",
      "Gradient Descent(34/49): loss=5.120170333863589e+51\n",
      "Gradient Descent(35/49): loss=2.2191497586633873e+53\n",
      "Gradient Descent(36/49): loss=9.618089497541728e+54\n",
      "Gradient Descent(37/49): loss=4.168607603951784e+56\n",
      "Gradient Descent(38/49): loss=1.8067298458977985e+58\n",
      "Gradient Descent(39/49): loss=7.8306068744953405e+59\n",
      "Gradient Descent(40/49): loss=3.393888918264145e+61\n",
      "Gradient Descent(41/49): loss=1.4709564883192062e+63\n",
      "Gradient Descent(42/49): loss=6.375320591326454e+64\n",
      "Gradient Descent(43/49): loss=2.76314853396056e+66\n",
      "Gradient Descent(44/49): loss=1.1975852369080754e+68\n",
      "Gradient Descent(45/49): loss=5.1904933159869665e+69\n",
      "Gradient Descent(46/49): loss=2.2496286721822355e+71\n",
      "Gradient Descent(47/49): loss=9.750189152766701e+72\n",
      "Gradient Descent(48/49): loss=4.2258613472644707e+74\n",
      "Gradient Descent(49/49): loss=1.8315443779094793e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0544423564736751\n",
      "Gradient Descent(2/49): loss=6.235692442424445\n",
      "Gradient Descent(3/49): loss=56.78480136492257\n",
      "Gradient Descent(4/49): loss=710.7252528090455\n",
      "Gradient Descent(5/49): loss=15571.544647958463\n",
      "Gradient Descent(6/49): loss=546129.6174619568\n",
      "Gradient Descent(7/49): loss=22799036.916415054\n",
      "Gradient Descent(8/49): loss=992334724.8921032\n",
      "Gradient Descent(9/49): loss=43570785943.36327\n",
      "Gradient Descent(10/49): loss=1916477367230.9216\n",
      "Gradient Descent(11/49): loss=84327196914225.34\n",
      "Gradient Descent(12/49): loss=3710761312547300.0\n",
      "Gradient Descent(13/49): loss=1.6329192255911014e+17\n",
      "Gradient Descent(14/49): loss=7.185676473927833e+18\n",
      "Gradient Descent(15/49): loss=3.1620655900184786e+20\n",
      "Gradient Descent(16/49): loss=1.391470938681973e+22\n",
      "Gradient Descent(17/49): loss=6.123185508417025e+23\n",
      "Gradient Descent(18/49): loss=2.694515559749388e+25\n",
      "Gradient Descent(19/49): loss=1.1857249954035181e+27\n",
      "Gradient Descent(20/49): loss=5.217797907733068e+28\n",
      "Gradient Descent(21/49): loss=2.296098598963885e+30\n",
      "Gradient Descent(22/49): loss=1.0104011058750265e+32\n",
      "Gradient Descent(23/49): loss=4.446282904470387e+33\n",
      "Gradient Descent(24/49): loss=1.9565924415205935e+35\n",
      "Gradient Descent(25/49): loss=8.61000989920538e+36\n",
      "Gradient Descent(26/49): loss=3.788845795949486e+38\n",
      "Gradient Descent(27/49): loss=1.6672864065822895e+40\n",
      "Gradient Descent(28/49): loss=7.336915016562305e+41\n",
      "Gradient Descent(29/49): loss=3.2286187752589717e+43\n",
      "Gradient Descent(30/49): loss=1.4207577943078127e+45\n",
      "Gradient Descent(31/49): loss=6.252062725877229e+46\n",
      "Gradient Descent(32/49): loss=2.7512281463391155e+48\n",
      "Gradient Descent(33/49): loss=1.2106814414832117e+50\n",
      "Gradient Descent(34/49): loss=5.327619066060569e+51\n",
      "Gradient Descent(35/49): loss=2.3444255392466673e+53\n",
      "Gradient Descent(36/49): loss=1.0316674373523207e+55\n",
      "Gradient Descent(37/49): loss=4.539865666346196e+56\n",
      "Gradient Descent(38/49): loss=1.997773654789705e+58\n",
      "Gradient Descent(39/49): loss=8.791228351441511e+59\n",
      "Gradient Descent(40/49): loss=3.86859120611048e+61\n",
      "Gradient Descent(41/49): loss=1.7023784756474416e+63\n",
      "Gradient Descent(42/49): loss=7.491338112360311e+64\n",
      "Gradient Descent(43/49): loss=3.296572854773606e+66\n",
      "Gradient Descent(44/49): loss=1.450661073340083e+68\n",
      "Gradient Descent(45/49): loss=6.383652485207091e+69\n",
      "Gradient Descent(46/49): loss=2.8091343871290566e+71\n",
      "Gradient Descent(47/49): loss=1.2361631563180143e+73\n",
      "Gradient Descent(48/49): loss=5.439751675959531e+74\n",
      "Gradient Descent(49/49): loss=2.3937696367073924e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0760844696265925\n",
      "Gradient Descent(2/49): loss=6.618588995851074\n",
      "Gradient Descent(3/49): loss=65.9504008932744\n",
      "Gradient Descent(4/49): loss=1032.97621333962\n",
      "Gradient Descent(5/49): loss=29414.1142182169\n",
      "Gradient Descent(6/49): loss=1178850.9301655674\n",
      "Gradient Descent(7/49): loss=52144211.92978331\n",
      "Gradient Descent(8/49): loss=2356527997.0530286\n",
      "Gradient Descent(9/49): loss=106960475905.83868\n",
      "Gradient Descent(10/49): loss=4859025363242.978\n",
      "Gradient Descent(11/49): loss=220774776046985.9\n",
      "Gradient Descent(12/49): loss=1.0031468317831074e+16\n",
      "Gradient Descent(13/49): loss=4.5580857085184365e+17\n",
      "Gradient Descent(14/49): loss=2.071099907791426e+19\n",
      "Gradient Descent(15/49): loss=9.410652277840882e+20\n",
      "Gradient Descent(16/49): loss=4.276007189155111e+22\n",
      "Gradient Descent(17/49): loss=1.9429298982226623e+24\n",
      "Gradient Descent(18/49): loss=8.828274673599891e+25\n",
      "Gradient Descent(19/49): loss=4.011386814099147e+27\n",
      "Gradient Descent(20/49): loss=1.822691836011444e+29\n",
      "Gradient Descent(21/49): loss=8.281937601827771e+30\n",
      "Gradient Descent(22/49): loss=3.7631424624627785e+32\n",
      "Gradient Descent(23/49): loss=1.7098946977907595e+34\n",
      "Gradient Descent(24/49): loss=7.769410556994619e+35\n",
      "Gradient Descent(25/49): loss=3.530260692727715e+37\n",
      "Gradient Descent(26/49): loss=1.604078001438372e+39\n",
      "Gradient Descent(27/49): loss=7.288601207267815e+40\n",
      "Gradient Descent(28/49): loss=3.31179079265155e+42\n",
      "Gradient Descent(29/49): loss=1.504809762860294e+44\n",
      "Gradient Descent(30/49): loss=6.837546705619612e+45\n",
      "Gradient Descent(31/49): loss=3.1068408848349004e+47\n",
      "Gradient Descent(32/49): loss=1.4116847312719037e+49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=6.4144056756615534e+50\n",
      "Gradient Descent(34/49): loss=2.914574285640156e+52\n",
      "Gradient Descent(35/49): loss=1.3243227348009118e+54\n",
      "Gradient Descent(36/49): loss=6.017450694434301e+55\n",
      "Gradient Descent(37/49): loss=2.734206089529387e+57\n",
      "Gradient Descent(38/49): loss=1.2423671284808674e+59\n",
      "Gradient Descent(39/49): loss=5.645061240410778e+60\n",
      "Gradient Descent(40/49): loss=2.564999964781262e+62\n",
      "Gradient Descent(41/49): loss=1.1654833382904297e+64\n",
      "Gradient Descent(42/49): loss=5.295717077908309e+65\n",
      "Gradient Descent(43/49): loss=2.4062651475040558e+67\n",
      "Gradient Descent(44/49): loss=1.0933574952949794e+69\n",
      "Gradient Descent(45/49): loss=4.967992050908032e+70\n",
      "Gradient Descent(46/49): loss=2.2573536216739504e+72\n",
      "Gradient Descent(47/49): loss=1.0256951543135578e+74\n",
      "Gradient Descent(48/49): loss=4.660548260941781e+75\n",
      "Gradient Descent(49/49): loss=2.1176574736871186e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0831803174987185\n",
      "Gradient Descent(2/49): loss=6.760325581243161\n",
      "Gradient Descent(3/49): loss=66.78463230809169\n",
      "Gradient Descent(4/49): loss=981.627702124255\n",
      "Gradient Descent(5/49): loss=25557.47382355067\n",
      "Gradient Descent(6/49): loss=966292.1152365468\n",
      "Gradient Descent(7/49): loss=41180034.75862414\n",
      "Gradient Descent(8/49): loss=1804414488.556297\n",
      "Gradient Descent(9/49): loss=79532596629.24689\n",
      "Gradient Descent(10/49): loss=3509826554423.832\n",
      "Gradient Descent(11/49): loss=154930211508259.47\n",
      "Gradient Descent(12/49): loss=6839262883494493.0\n",
      "Gradient Descent(13/49): loss=3.0191672791657805e+17\n",
      "Gradient Descent(14/49): loss=1.3328031948771617e+19\n",
      "Gradient Descent(15/49): loss=5.883626218328899e+20\n",
      "Gradient Descent(16/49): loss=2.5973122596824494e+22\n",
      "Gradient Descent(17/49): loss=1.1465771034518942e+24\n",
      "Gradient Descent(18/49): loss=5.0615364194684505e+25\n",
      "Gradient Descent(19/49): loss=2.2344028037906823e+27\n",
      "Gradient Descent(20/49): loss=9.863716225081562e+28\n",
      "Gradient Descent(21/49): loss=4.354313269181934e+30\n",
      "Gradient Descent(22/49): loss=1.9222008838923084e+32\n",
      "Gradient Descent(23/49): loss=8.485508528273962e+33\n",
      "Gradient Descent(24/49): loss=3.7459068709630056e+35\n",
      "Gradient Descent(25/49): loss=1.6536213756870647e+37\n",
      "Gradient Descent(26/49): loss=7.299870894618102e+38\n",
      "Gradient Descent(27/49): loss=3.2225100534849063e+40\n",
      "Gradient Descent(28/49): loss=1.422569137827856e+42\n",
      "Gradient Descent(29/49): loss=6.279896472973735e+43\n",
      "Gradient Descent(30/49): loss=2.7722448535250427e+45\n",
      "Gradient Descent(31/49): loss=1.22380067266572e+47\n",
      "Gradient Descent(32/49): loss=5.4024379719296794e+48\n",
      "Gradient Descent(33/49): loss=2.384892956217562e+50\n",
      "Gradient Descent(34/49): loss=1.0528051302335413e+52\n",
      "Gradient Descent(35/49): loss=4.647582355243274e+53\n",
      "Gradient Descent(36/49): loss=2.0516638006862426e+55\n",
      "Gradient Descent(37/49): loss=9.05701939051702e+56\n",
      "Gradient Descent(38/49): loss=3.998198935554702e+58\n",
      "Gradient Descent(39/49): loss=1.7649950871264135e+60\n",
      "Gradient Descent(40/49): loss=7.791527404696663e+61\n",
      "Gradient Descent(41/49): loss=3.439550610703204e+63\n",
      "Gradient Descent(42/49): loss=1.5183811580329893e+65\n",
      "Gradient Descent(43/49): loss=6.702856279815615e+66\n",
      "Gradient Descent(44/49): loss=2.9589594200487945e+68\n",
      "Gradient Descent(45/49): loss=1.306225358860991e+70\n",
      "Gradient Descent(46/49): loss=5.766299722026503e+71\n",
      "Gradient Descent(47/49): loss=2.5455188309340733e+73\n",
      "Gradient Descent(48/49): loss=1.1237130275917847e+75\n",
      "Gradient Descent(49/49): loss=4.9606035242571624e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0969022642643038\n",
      "Gradient Descent(2/49): loss=6.849958001298305\n",
      "Gradient Descent(3/49): loss=66.6140795558929\n",
      "Gradient Descent(4/49): loss=943.9663832133622\n",
      "Gradient Descent(5/49): loss=23692.36791962721\n",
      "Gradient Descent(6/49): loss=882914.4188704804\n",
      "Gradient Descent(7/49): loss=37503596.19316553\n",
      "Gradient Descent(8/49): loss=1642567288.9707801\n",
      "Gradient Descent(9/49): loss=72408260133.4533\n",
      "Gradient Descent(10/49): loss=3196212711626.175\n",
      "Gradient Descent(11/49): loss=141124776640417.12\n",
      "Gradient Descent(12/49): loss=6231542557827991.0\n",
      "Gradient Descent(13/49): loss=2.7516484438843885e+17\n",
      "Gradient Descent(14/49): loss=1.2150422348322114e+19\n",
      "Gradient Descent(15/49): loss=5.365250664207591e+20\n",
      "Gradient Descent(16/49): loss=2.3691289734795896e+22\n",
      "Gradient Descent(17/49): loss=1.046134199880935e+24\n",
      "Gradient Descent(18/49): loss=4.619405617578824e+25\n",
      "Gradient Descent(19/49): loss=2.0397868916920175e+27\n",
      "Gradient Descent(20/49): loss=9.00706910959685e+28\n",
      "Gradient Descent(21/49): loss=3.9772436168961986e+30\n",
      "Gradient Descent(22/49): loss=1.756227980021823e+32\n",
      "Gradient Descent(23/49): loss=7.754960507607297e+33\n",
      "Gradient Descent(24/49): loss=3.4243511183452785e+35\n",
      "Gradient Descent(25/49): loss=1.5120877237492845e+37\n",
      "Gradient Descent(26/49): loss=6.676912516547355e+38\n",
      "Gradient Descent(27/49): loss=2.9483184112550616e+40\n",
      "Gradient Descent(28/49): loss=1.3018863782627941e+42\n",
      "Gradient Descent(29/49): loss=5.748728276552646e+43\n",
      "Gradient Descent(30/49): loss=2.5384609094484403e+45\n",
      "Gradient Descent(31/49): loss=1.1209059602069012e+47\n",
      "Gradient Descent(32/49): loss=4.94957462985066e+48\n",
      "Gradient Descent(33/49): loss=2.1855793336970872e+50\n",
      "Gradient Descent(34/49): loss=9.650843519108561e+51\n",
      "Gradient Descent(35/49): loss=4.261514519025327e+53\n",
      "Gradient Descent(36/49): loss=1.8817532332698672e+55\n",
      "Gradient Descent(37/49): loss=8.309241268832725e+56\n",
      "Gradient Descent(38/49): loss=3.669104388554723e+58\n",
      "Gradient Descent(39/49): loss=1.6201632108827416e+60\n",
      "Gradient Descent(40/49): loss=7.154140498389691e+61\n",
      "Gradient Descent(41/49): loss=3.1590475531667664e+63\n",
      "Gradient Descent(42/49): loss=1.3949378608674654e+65\n",
      "Gradient Descent(43/49): loss=6.159614893200547e+66\n",
      "Gradient Descent(44/49): loss=2.719895752843331e+68\n",
      "Gradient Descent(45/49): loss=1.2010219850759718e+70\n",
      "Gradient Descent(46/49): loss=5.303342259084524e+71\n",
      "Gradient Descent(47/49): loss=2.341792195853295e+73\n",
      "Gradient Descent(48/49): loss=1.0340631286177248e+75\n",
      "Gradient Descent(49/49): loss=4.5661034991065894e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0694151918657349\n",
      "Gradient Descent(2/49): loss=6.494678785563558\n",
      "Gradient Descent(3/49): loss=60.507811034686284\n",
      "Gradient Descent(4/49): loss=771.437612195933\n",
      "Gradient Descent(5/49): loss=17156.42326982099\n",
      "Gradient Descent(6/49): loss=611405.7536753244\n",
      "Gradient Descent(7/49): loss=25981283.03607425\n",
      "Gradient Descent(8/49): loss=1151847370.5508335\n",
      "Gradient Descent(9/49): loss=51522636691.69433\n",
      "Gradient Descent(10/49): loss=2308817930814.316\n",
      "Gradient Descent(11/49): loss=103500142714330.8\n",
      "Gradient Descent(12/49): loss=4640069622011129.0\n",
      "Gradient Descent(13/49): loss=2.0802454112916906e+17\n",
      "Gradient Descent(14/49): loss=9.32622669983553e+18\n",
      "Gradient Descent(15/49): loss=4.181168116541106e+20\n",
      "Gradient Descent(16/49): loss=1.8745168386182288e+22\n",
      "Gradient Descent(17/49): loss=8.403903809963052e+23\n",
      "Gradient Descent(18/49): loss=3.7676695215806904e+25\n",
      "Gradient Descent(19/49): loss=1.6891356635450684e+27\n",
      "Gradient Descent(20/49): loss=7.572796058966767e+28\n",
      "Gradient Descent(21/49): loss=3.3950641970787486e+30\n",
      "Gradient Descent(22/49): loss=1.5220878540279318e+32\n",
      "Gradient Descent(23/49): loss=6.823881084124101e+33\n",
      "Gradient Descent(24/49): loss=3.0593078400209086e+35\n",
      "Gradient Descent(25/49): loss=1.3715603107136405e+37\n",
      "Gradient Descent(26/49): loss=6.149030383003403e+38\n",
      "Gradient Descent(27/49): loss=2.7567562545919293e+40\n",
      "Gradient Descent(28/49): loss=1.235919254560557e+42\n",
      "Gradient Descent(29/49): loss=5.540919336808158e+43\n",
      "Gradient Descent(30/49): loss=2.4841256403866663e+45\n",
      "Gradient Descent(31/49): loss=1.1136924799163558e+47\n",
      "Gradient Descent(32/49): loss=4.992947698205716e+48\n",
      "Gradient Descent(33/49): loss=2.238456949883507e+50\n",
      "Gradient Descent(34/49): loss=1.0035533755505868e+52\n",
      "Gradient Descent(35/49): loss=4.4991679542078304e+53\n",
      "Gradient Descent(36/49): loss=2.017083771858671e+55\n",
      "Gradient Descent(37/49): loss=9.043065260301074e+56\n",
      "Gradient Descent(38/49): loss=4.0542207737217356e+58\n",
      "Gradient Descent(39/49): loss=1.817603390991084e+60\n",
      "Gradient Descent(40/49): loss=8.148747370532403e+61\n",
      "Gradient Descent(41/49): loss=3.653276839044175e+63\n",
      "Gradient Descent(42/49): loss=1.6378507095409802e+65\n",
      "Gradient Descent(43/49): loss=7.342873439193538e+66\n",
      "Gradient Descent(44/49): loss=3.291984429956056e+68\n",
      "Gradient Descent(45/49): loss=1.4758747480554754e+70\n",
      "Gradient Descent(46/49): loss=6.616696762374662e+71\n",
      "Gradient Descent(47/49): loss=2.9664221915106514e+73\n",
      "Gradient Descent(48/49): loss=1.3299174700472423e+75\n",
      "Gradient Descent(49/49): loss=5.96233564527151e+76\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0913810662891055\n",
      "Gradient Descent(2/49): loss=6.890988946165258\n",
      "Gradient Descent(3/49): loss=70.1850958122797\n",
      "Gradient Descent(4/49): loss=1118.1558393135265\n",
      "Gradient Descent(5/49): loss=32325.018552766716\n",
      "Gradient Descent(6/49): loss=1317518.6388417275\n",
      "Gradient Descent(7/49): loss=59335612.13622828\n",
      "Gradient Descent(8/49): loss=2731125680.451515\n",
      "Gradient Descent(9/49): loss=126266598919.79393\n",
      "Gradient Descent(10/49): loss=5842769937133.186\n",
      "Gradient Descent(11/49): loss=270411668824293.66\n",
      "Gradient Descent(12/49): loss=1.2515472687712262e+16\n",
      "Gradient Descent(13/49): loss=5.792580871719946e+17\n",
      "Gradient Descent(14/49): loss=2.6810045781232935e+19\n",
      "Gradient Descent(15/49): loss=1.2408609710977025e+21\n",
      "Gradient Descent(16/49): loss=5.74313093839105e+22\n",
      "Gradient Descent(17/49): loss=2.658118363200352e+24\n",
      "Gradient Descent(18/49): loss=1.2302685284169301e+26\n",
      "Gradient Descent(19/49): loss=5.694105550825217e+27\n",
      "Gradient Descent(20/49): loss=2.6354277361249104e+29\n",
      "Gradient Descent(21/49): loss=1.2197665270620685e+31\n",
      "Gradient Descent(22/49): loss=5.6454986799742416e+32\n",
      "Gradient Descent(23/49): loss=2.612930805895985e+34\n",
      "Gradient Descent(24/49): loss=1.2093541746133679e+36\n",
      "Gradient Descent(25/49): loss=5.597306734470972e+37\n",
      "Gradient Descent(26/49): loss=2.590625917322405e+39\n",
      "Gradient Descent(27/49): loss=1.1990307056375656e+41\n",
      "Gradient Descent(28/49): loss=5.5495261722219147e+42\n",
      "Gradient Descent(29/49): loss=2.568511431056251e+44\n",
      "Gradient Descent(30/49): loss=1.1887953613930626e+46\n",
      "Gradient Descent(31/49): loss=5.502153481514662e+47\n",
      "Gradient Descent(32/49): loss=2.5465857217568786e+49\n",
      "Gradient Descent(33/49): loss=1.178647389616413e+51\n",
      "Gradient Descent(34/49): loss=5.45518518061576e+52\n",
      "Gradient Descent(35/49): loss=2.5248471779583653e+54\n",
      "Gradient Descent(36/49): loss=1.1685860444658128e+56\n",
      "Gradient Descent(37/49): loss=5.408617817512657e+57\n",
      "Gradient Descent(38/49): loss=2.5032942019505796e+59\n",
      "Gradient Descent(39/49): loss=1.1586105864660873e+61\n",
      "Gradient Descent(40/49): loss=5.362447969660651e+62\n",
      "Gradient Descent(41/49): loss=2.4819252096622668e+64\n",
      "Gradient Descent(42/49): loss=1.1487202824546628e+66\n",
      "Gradient Descent(43/49): loss=5.316672243730772e+67\n",
      "Gradient Descent(44/49): loss=2.4607386305441734e+69\n",
      "Gradient Descent(45/49): loss=1.1389144055273349e+71\n",
      "Gradient Descent(46/49): loss=5.271287275361159e+72\n",
      "Gradient Descent(47/49): loss=2.4397329074539357e+74\n",
      "Gradient Descent(48/49): loss=1.129192234985101e+76\n",
      "Gradient Descent(49/49): loss=5.226289728908628e+77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0986218406325454\n",
      "Gradient Descent(2/49): loss=7.038629910635972\n",
      "Gradient Descent(3/49): loss=71.08292192570715\n",
      "Gradient Descent(4/49): loss=1063.2143869859888\n",
      "Gradient Descent(5/49): loss=28103.899966634155\n",
      "Gradient Descent(6/49): loss=1080364.443693626\n",
      "Gradient Descent(7/49): loss=46872969.54976748\n",
      "Gradient Descent(8/49): loss=2091851890.3973072\n",
      "Gradient Descent(9/49): loss=93917504326.34561\n",
      "Gradient Descent(10/49): loss=4221874947623.8936\n",
      "Gradient Descent(11/49): loss=189835256346869.22\n",
      "Gradient Descent(12/49): loss=8536340611344943.0\n",
      "Gradient Descent(13/49): loss=3.838587438901592e+17\n",
      "Gradient Descent(14/49): loss=1.7261246030030416e+19\n",
      "Gradient Descent(15/49): loss=7.761989580917262e+20\n",
      "Gradient Descent(16/49): loss=3.4903904458748327e+22\n",
      "Gradient Descent(17/49): loss=1.5695493517361903e+24\n",
      "Gradient Descent(18/49): loss=7.0579071774983684e+25\n",
      "Gradient Descent(19/49): loss=3.1737806597169007e+27\n",
      "Gradient Descent(20/49): loss=1.427177125529212e+29\n",
      "Gradient Descent(21/49): loss=6.417691599055279e+30\n",
      "Gradient Descent(22/49): loss=2.885890246148294e+32\n",
      "Gradient Descent(23/49): loss=1.2977193410249622e+34\n",
      "Gradient Descent(24/49): loss=5.835549325961238e+35\n",
      "Gradient Descent(25/49): loss=2.624114079153042e+37\n",
      "Gradient Descent(26/49): loss=1.1800045404080221e+39\n",
      "Gradient Descent(27/49): loss=5.306212585974964e+40\n",
      "Gradient Descent(28/49): loss=2.386083361833733e+42\n",
      "Gradient Descent(29/49): loss=1.0729675295460569e+44\n",
      "Gradient Descent(30/49): loss=4.8248914429185225e+45\n",
      "Gradient Descent(31/49): loss=2.169644168616844e+47\n",
      "Gradient Descent(32/49): loss=9.756397369980235e+48\n",
      "Gradient Descent(33/49): loss=4.387230450864262e+50\n",
      "Gradient Descent(34/49): loss=1.972837954326735e+52\n",
      "Gradient Descent(35/49): loss=8.871404494526706e+53\n",
      "Gradient Descent(36/49): loss=3.989269241951865e+55\n",
      "Gradient Descent(37/49): loss=1.7938838314272755e+57\n",
      "Gradient Descent(38/49): loss=8.066688422067507e+58\n",
      "Gradient Descent(39/49): loss=3.627406689258285e+60\n",
      "Gradient Descent(40/49): loss=1.6311624548780479e+62\n",
      "Gradient Descent(41/49): loss=7.334967325507601e+63\n",
      "Gradient Descent(42/49): loss=3.298368320419002e+65\n",
      "Gradient Descent(43/49): loss=1.4832013687791546e+67\n",
      "Gradient Descent(44/49): loss=6.669619904877373e+68\n",
      "Gradient Descent(45/49): loss=2.9991766871246066e+70\n",
      "Gradient Descent(46/49): loss=1.3486616822067657e+72\n",
      "Gradient Descent(47/49): loss=6.064625471587568e+73\n",
      "Gradient Descent(48/49): loss=2.7271244223717176e+75\n",
      "Gradient Descent(49/49): loss=1.226325953670039e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1126830848117477\n",
      "Gradient Descent(2/49): loss=7.132499182650246\n",
      "Gradient Descent(3/49): loss=70.91196809512478\n",
      "Gradient Descent(4/49): loss=1022.7548252168084\n",
      "Gradient Descent(5/49): loss=26059.06941791597\n",
      "Gradient Descent(6/49): loss=987218.3713531735\n",
      "Gradient Descent(7/49): loss=42689005.196296744\n",
      "Gradient Descent(8/49): loss=1904226333.6449106\n",
      "Gradient Descent(9/49): loss=85504432246.84901\n",
      "Gradient Descent(10/49): loss=3844625480170.933\n",
      "Gradient Descent(11/49): loss=172918900391657.0\n",
      "Gradient Descent(12/49): loss=7777791241083139.0\n",
      "Gradient Descent(13/49): loss=3.4984473290407206e+17\n",
      "Gradient Descent(14/49): loss=1.5736041005124186e+19\n",
      "Gradient Descent(15/49): loss=7.078086089818115e+20\n",
      "Gradient Descent(16/49): loss=3.1837301384191154e+22\n",
      "Gradient Descent(17/49): loss=1.432045003050526e+24\n",
      "Gradient Descent(18/49): loss=6.441352787020528e+25\n",
      "Gradient Descent(19/49): loss=2.897326946928386e+27\n",
      "Gradient Descent(20/49): loss=1.3032205681897658e+29\n",
      "Gradient Descent(21/49): loss=5.861899193740469e+30\n",
      "Gradient Descent(22/49): loss=2.6366881398738763e+32\n",
      "Gradient Descent(23/49): loss=1.1859849712849354e+34\n",
      "Gradient Descent(24/49): loss=5.33457230243952e+35\n",
      "Gradient Descent(25/49): loss=2.399495974989063e+37\n",
      "Gradient Descent(26/49): loss=1.0792956974930848e+39\n",
      "Gradient Descent(27/49): loss=4.854682878275697e+40\n",
      "Gradient Descent(28/49): loss=2.1836412304213855e+42\n",
      "Gradient Descent(29/49): loss=9.82204016771924e+43\n",
      "Gradient Descent(30/49): loss=4.417963524057177e+45\n",
      "Gradient Descent(31/49): loss=1.98720442663713e+47\n",
      "Gradient Descent(32/49): loss=8.938465453014343e+48\n",
      "Gradient Descent(33/49): loss=4.020530730697749e+50\n",
      "Gradient Descent(34/49): loss=1.808438757352207e+52\n",
      "Gradient Descent(35/49): loss=8.13437567862085e+53\n",
      "Gradient Descent(36/49): loss=3.6588503432551483e+55\n",
      "Gradient Descent(37/49): loss=1.6457545561268933e+57\n",
      "Gradient Descent(38/49): loss=7.402620508940624e+58\n",
      "Gradient Descent(39/49): loss=3.329706133601771e+60\n",
      "Gradient Descent(40/49): loss=1.4977051603219193e+62\n",
      "Gradient Descent(41/49): loss=6.736692840903819e+63\n",
      "Gradient Descent(42/49): loss=3.0301712002468046e+65\n",
      "Gradient Descent(43/49): loss=1.362974046709479e+67\n",
      "Gradient Descent(44/49): loss=6.130670939820924e+68\n",
      "Gradient Descent(45/49): loss=2.757581940984469e+70\n",
      "Gradient Descent(46/49): loss=1.24036312434437e+72\n",
      "Gradient Descent(47/49): loss=5.5791657805970573e+73\n",
      "Gradient Descent(48/49): loss=2.509514366918893e+75\n",
      "Gradient Descent(49/49): loss=1.1287820805888095e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0845568203861384\n",
      "Gradient Descent(2/49): loss=6.762167027381558\n",
      "Gradient Descent(3/49): loss=64.43376916071904\n",
      "Gradient Descent(4/49): loss=836.6662623521647\n",
      "Gradient Descent(5/49): loss=18885.73409750694\n",
      "Gradient Descent(6/49): loss=683775.9354419137\n",
      "Gradient Descent(7/49): loss=29572112.026458364\n",
      "Gradient Descent(8/49): loss=1335164482.4573424\n",
      "Gradient Descent(9/49): loss=60831590243.8086\n",
      "Gradient Descent(10/49): loss=2776702978458.7314\n",
      "Gradient Descent(11/49): loss=126792435460048.5\n",
      "Gradient Descent(12/49): loss=5790158539762963.0\n",
      "Gradient Descent(13/49): loss=2.644199976126001e+17\n",
      "Gradient Descent(14/49): loss=1.2075343765993822e+19\n",
      "Gradient Descent(15/49): loss=5.5144853367975615e+20\n",
      "Gradient Descent(16/49): loss=2.518317742902177e+22\n",
      "Gradient Descent(17/49): loss=1.150048287911268e+24\n",
      "Gradient Descent(18/49): loss=5.251962645610479e+25\n",
      "Gradient Descent(19/49): loss=2.3984307398673064e+27\n",
      "Gradient Descent(20/49): loss=1.0952991107055597e+29\n",
      "Gradient Descent(21/49): loss=5.0019378171905814e+30\n",
      "Gradient Descent(22/49): loss=2.2842510947671147e+32\n",
      "Gradient Descent(23/49): loss=1.0431563235395854e+34\n",
      "Gradient Descent(24/49): loss=4.763815667348808e+35\n",
      "Gradient Descent(25/49): loss=2.1755070836819156e+37\n",
      "Gradient Descent(26/49): loss=9.934958448516854e+38\n",
      "Gradient Descent(27/49): loss=4.537029555734964e+40\n",
      "Gradient Descent(28/49): loss=2.0719399377744708e+42\n",
      "Gradient Descent(29/49): loss=9.461995019006539e+43\n",
      "Gradient Descent(30/49): loss=4.321039819130519e+45\n",
      "Gradient Descent(31/49): loss=1.9733032073052243e+47\n",
      "Gradient Descent(32/49): loss=9.011547476886397e+48\n",
      "Gradient Descent(33/49): loss=4.115332485527025e+50\n",
      "Gradient Descent(34/49): loss=1.879362175017374e+52\n",
      "Gradient Descent(35/49): loss=8.582543931280433e+53\n",
      "Gradient Descent(36/49): loss=3.9194180510564346e+55\n",
      "Gradient Descent(37/49): loss=1.7898932975989357e+57\n",
      "Gradient Descent(38/49): loss=8.173963519727128e+58\n",
      "Gradient Descent(39/49): loss=3.7328303151623086e+60\n",
      "Gradient Descent(40/49): loss=1.7046836737362614e+62\n",
      "Gradient Descent(41/49): loss=7.784833978923091e+63\n",
      "Gradient Descent(42/49): loss=3.55512527122208e+65\n",
      "Gradient Descent(43/49): loss=1.623530537491321e+67\n",
      "Gradient Descent(44/49): loss=7.414229331111059e+68\n",
      "Gradient Descent(45/49): loss=3.385880049983402e+70\n",
      "Gradient Descent(46/49): loss=1.546240775797225e+72\n",
      "Gradient Descent(47/49): loss=7.061267680612998e+73\n",
      "Gradient Descent(48/49): loss=3.224691913299307e+75\n",
      "Gradient Descent(49/49): loss=1.4726304689238284e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1068480750182665\n",
      "Gradient Descent(2/49): loss=7.172220619035585\n",
      "Gradient Descent(3/49): loss=74.64598834024686\n",
      "Gradient Descent(4/49): loss=1209.4571661515015\n",
      "Gradient Descent(5/49): loss=35493.52957153947\n",
      "Gradient Descent(6/49): loss=1471008.205962652\n",
      "Gradient Descent(7/49): loss=67439264.32169104\n",
      "Gradient Descent(8/49): loss=3161008972.4898176\n",
      "Gradient Descent(9/49): loss=148831697158.73343\n",
      "Gradient Descent(10/49): loss=7013859335053.631\n",
      "Gradient Descent(11/49): loss=330595478011149.94\n",
      "Gradient Descent(12/49): loss=1.5583046811257248e+16\n",
      "Gradient Descent(13/49): loss=7.345323971602906e+17\n",
      "Gradient Descent(14/49): loss=3.46234353685705e+19\n",
      "Gradient Descent(15/49): loss=1.6320350516201933e+21\n",
      "Gradient Descent(16/49): loss=7.692877765991554e+22\n",
      "Gradient Descent(17/49): loss=3.626170218312643e+24\n",
      "Gradient Descent(18/49): loss=1.709257690286011e+26\n",
      "Gradient Descent(19/49): loss=8.056880061413776e+27\n",
      "Gradient Descent(20/49): loss=3.797748970138879e+29\n",
      "Gradient Descent(21/49): loss=1.7901342865365854e+31\n",
      "Gradient Descent(22/49): loss=8.438105806999086e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=3.9774462813019555e+34\n",
      "Gradient Descent(24/49): loss=1.8748377043960775e+36\n",
      "Gradient Descent(25/49): loss=8.837369933440484e+37\n",
      "Gradient Descent(26/49): loss=4.1656462934019636e+39\n",
      "Gradient Descent(27/49): loss=1.9635490165543244e+41\n",
      "Gradient Descent(28/49): loss=9.25552595888497e+42\n",
      "Gradient Descent(29/49): loss=4.3627513269781704e+44\n",
      "Gradient Descent(30/49): loss=2.0564578637239014e+46\n",
      "Gradient Descent(31/49): loss=9.69346778745029e+47\n",
      "Gradient Descent(32/49): loss=4.5691827391096857e+49\n",
      "Gradient Descent(33/49): loss=2.153762859810312e+51\n",
      "Gradient Descent(34/49): loss=1.0152131619936635e+53\n",
      "Gradient Descent(35/49): loss=4.78538182414348e+54\n",
      "Gradient Descent(36/49): loss=2.2556720164928385e+56\n",
      "Gradient Descent(37/49): loss=1.0632497955164829e+58\n",
      "Gradient Descent(38/49): loss=5.01181075706006e+59\n",
      "Gradient Descent(39/49): loss=2.3624031878963335e+61\n",
      "Gradient Descent(40/49): loss=1.1135593686016776e+63\n",
      "Gradient Descent(41/49): loss=5.248953581478825e+64\n",
      "Gradient Descent(42/49): loss=2.474184536305104e+66\n",
      "Gradient Descent(43/49): loss=1.166249429465633e+68\n",
      "Gradient Descent(44/49): loss=5.497317244412771e+69\n",
      "Gradient Descent(45/49): loss=2.591255019911572e+71\n",
      "Gradient Descent(46/49): loss=1.2214326151617543e+73\n",
      "Gradient Descent(47/49): loss=5.757432679982876e+74\n",
      "Gradient Descent(48/49): loss=2.7138649036439214e+76\n",
      "Gradient Descent(49/49): loss=1.279226892367601e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1142351790632885\n",
      "Gradient Descent(2/49): loss=7.325945836813015\n",
      "Gradient Descent(3/49): loss=75.61120645729694\n",
      "Gradient Descent(4/49): loss=1150.7086609294438\n",
      "Gradient Descent(5/49): loss=30877.27931480759\n",
      "Gradient Descent(6/49): loss=1206675.6388373529\n",
      "Gradient Descent(7/49): loss=53289833.89424191\n",
      "Gradient Descent(8/49): loss=2421800409.6282125\n",
      "Gradient Descent(9/49): loss=110735603012.61153\n",
      "Gradient Descent(10/49): loss=5069802001378.718\n",
      "Gradient Descent(11/49): loss=232172159346467.9\n",
      "Gradient Descent(12/49): loss=1.0632937760966504e+16\n",
      "Gradient Descent(13/49): loss=4.86969081476502e+17\n",
      "Gradient Descent(14/49): loss=2.2302345528811418e+19\n",
      "Gradient Descent(15/49): loss=1.021409534049147e+21\n",
      "Gradient Descent(16/49): loss=4.677882612585704e+22\n",
      "Gradient Descent(17/49): loss=2.1423910269050636e+24\n",
      "Gradient Descent(18/49): loss=9.811788180029413e+25\n",
      "Gradient Descent(19/49): loss=4.493632869523297e+27\n",
      "Gradient Descent(20/49): loss=2.0580077758951552e+29\n",
      "Gradient Descent(21/49): loss=9.425327188029414e+30\n",
      "Gradient Descent(22/49): loss=4.316640279140853e+32\n",
      "Gradient Descent(23/49): loss=1.9769481661282257e+34\n",
      "Gradient Descent(24/49): loss=9.05408789897375e+35\n",
      "Gradient Descent(25/49): loss=4.1466189699295465e+37\n",
      "Gradient Descent(26/49): loss=1.8990812850104256e+39\n",
      "Gradient Descent(27/49): loss=8.69747076649788e+40\n",
      "Gradient Descent(28/49): loss=3.9832943608662525e+42\n",
      "Gradient Descent(29/49): loss=1.8242813791828268e+44\n",
      "Gradient Descent(30/49): loss=8.354899861604406e+45\n",
      "Gradient Descent(31/49): loss=3.826402686229598e+47\n",
      "Gradient Descent(32/49): loss=1.7524276484115108e+49\n",
      "Gradient Descent(33/49): loss=8.025821939674555e+50\n",
      "Gradient Descent(34/49): loss=3.675690569350901e+52\n",
      "Gradient Descent(35/49): loss=1.6834040504719863e+54\n",
      "Gradient Descent(36/49): loss=7.709705546966005e+55\n",
      "Gradient Descent(37/49): loss=3.530914613413993e+57\n",
      "Gradient Descent(38/49): loss=1.6170991137433306e+59\n",
      "Gradient Descent(39/49): loss=7.406040162327959e+60\n",
      "Gradient Descent(40/49): loss=3.391841008374963e+62\n",
      "Gradient Descent(41/49): loss=1.5534057571837325e+64\n",
      "Gradient Descent(42/49): loss=7.114335372717433e+65\n",
      "Gradient Descent(43/49): loss=3.258245153362752e+67\n",
      "Gradient Descent(44/49): loss=1.492221117396796e+69\n",
      "Gradient Descent(45/49): loss=6.834120081194074e+70\n",
      "Gradient Descent(46/49): loss=3.1299112939548112e+72\n",
      "Gradient Descent(47/49): loss=1.4334463825098658e+74\n",
      "Gradient Descent(48/49): loss=6.5649417461102916e+75\n",
      "Gradient Descent(49/49): loss=3.0066321737377476e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1286393788409486\n",
      "Gradient Descent(2/49): loss=7.424197507144887\n",
      "Gradient Descent(3/49): loss=75.44034737167422\n",
      "Gradient Descent(4/49): loss=1107.2719716569663\n",
      "Gradient Descent(5/49): loss=28637.299429761886\n",
      "Gradient Descent(6/49): loss=1102722.298751518\n",
      "Gradient Descent(7/49): loss=48533913.986854784\n",
      "Gradient Descent(8/49): loss=2204584664.9618993\n",
      "Gradient Descent(9/49): loss=100815781100.47977\n",
      "Gradient Descent(10/49): loss=4616771239181.446\n",
      "Gradient Descent(11/49): loss=211482385242744.94\n",
      "Gradient Descent(12/49): loss=9688044577191616.0\n",
      "Gradient Descent(13/49): loss=4.438165113153205e+17\n",
      "Gradient Descent(14/49): loss=2.0331616030068298e+19\n",
      "Gradient Descent(15/49): loss=9.314093097603021e+20\n",
      "Gradient Descent(16/49): loss=4.266868883439921e+22\n",
      "Gradient Descent(17/49): loss=1.9546906277183368e+24\n",
      "Gradient Descent(18/49): loss=8.954611853548466e+25\n",
      "Gradient Descent(19/49): loss=4.102187445366191e+27\n",
      "Gradient Descent(20/49): loss=1.8792486056949355e+29\n",
      "Gradient Descent(21/49): loss=8.609005242174989e+30\n",
      "Gradient Descent(22/49): loss=3.94386197950795e+32\n",
      "Gradient Descent(23/49): loss=1.806718299721587e+34\n",
      "Gradient Descent(24/49): loss=8.276737450526237e+35\n",
      "Gradient Descent(25/49): loss=3.7916471447433764e+37\n",
      "Gradient Descent(26/49): loss=1.736987328180432e+39\n",
      "Gradient Descent(27/49): loss=7.957293659148318e+40\n",
      "Gradient Descent(28/49): loss=3.645307098713888e+42\n",
      "Gradient Descent(29/49): loss=1.669947649683206e+44\n",
      "Gradient Descent(30/49): loss=7.650178920909975e+45\n",
      "Gradient Descent(31/49): loss=3.5046151017392346e+47\n",
      "Gradient Descent(32/49): loss=1.6054953927636594e+49\n",
      "Gradient Descent(33/49): loss=7.354917391373735e+50\n",
      "Gradient Descent(34/49): loss=3.369353165244296e+52\n",
      "Gradient Descent(35/49): loss=1.5435306949139354e+54\n",
      "Gradient Descent(36/49): loss=7.071051591496439e+55\n",
      "Gradient Descent(37/49): loss=3.239311714004618e+57\n",
      "Gradient Descent(38/49): loss=1.4839575478570337e+59\n",
      "Gradient Descent(39/49): loss=6.798141698810197e+60\n",
      "Gradient Descent(40/49): loss=3.1142892614307266e+62\n",
      "Gradient Descent(41/49): loss=1.4266836487918474e+64\n",
      "Gradient Descent(42/49): loss=6.535764865961368e+65\n",
      "Gradient Descent(43/49): loss=2.99409209738331e+67\n",
      "Gradient Descent(44/49): loss=1.3716202573781899e+69\n",
      "Gradient Descent(45/49): loss=6.283514565548649e+70\n",
      "Gradient Descent(46/49): loss=2.8785339880391134e+72\n",
      "Gradient Descent(47/49): loss=1.3186820582428377e+74\n",
      "Gradient Descent(48/49): loss=6.040999960247551e+75\n",
      "Gradient Descent(49/49): loss=2.7674358873389135e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.0998672420348856\n",
      "Gradient Descent(2/49): loss=7.038341451530733\n",
      "Gradient Descent(3/49): loss=68.57144794919017\n",
      "Gradient Descent(4/49): loss=906.6975580536141\n",
      "Gradient Descent(5/49): loss=20771.119510429307\n",
      "Gradient Descent(6/49): loss=763935.5532470322\n",
      "Gradient Descent(7/49): loss=33619501.78161244\n",
      "Gradient Descent(8/49): loss=1545569732.8337197\n",
      "Gradient Descent(9/49): loss=71713503731.56213\n",
      "Gradient Descent(10/49): loss=3333777089850.2695\n",
      "Gradient Descent(11/49): loss=155038607662285.28\n",
      "Gradient Descent(12/49): loss=7210697368312783.0\n",
      "Gradient Descent(13/49): loss=3.35367994193778e+17\n",
      "Gradient Descent(14/49): loss=1.5597944414916735e+19\n",
      "Gradient Descent(15/49): loss=7.254598972420865e+20\n",
      "Gradient Descent(16/49): loss=3.3741121211894117e+22\n",
      "Gradient Descent(17/49): loss=1.5692987249052693e+24\n",
      "Gradient Descent(18/49): loss=7.2988045838552555e+25\n",
      "Gradient Descent(19/49): loss=3.394672255063533e+27\n",
      "Gradient Descent(20/49): loss=1.578861249064313e+29\n",
      "Gradient Descent(21/49): loss=7.34327987096541e+30\n",
      "Gradient Descent(22/49): loss=3.415357701368334e+32\n",
      "Gradient Descent(23/49): loss=1.5884820452560052e+34\n",
      "Gradient Descent(24/49): loss=7.388026170994553e+35\n",
      "Gradient Descent(25/49): loss=3.4361691947551034e+37\n",
      "Gradient Descent(26/49): loss=1.598161465824257e+39\n",
      "Gradient Descent(27/49): loss=7.433045132771858e+40\n",
      "Gradient Descent(28/49): loss=3.457107503047451e+42\n",
      "Gradient Descent(29/49): loss=1.6078998679738089e+44\n",
      "Gradient Descent(30/49): loss=7.478338417741254e+45\n",
      "Gradient Descent(31/49): loss=3.478173398990322e+47\n",
      "Gradient Descent(32/49): loss=1.6176976111088823e+49\n",
      "Gradient Descent(33/49): loss=7.52390769749162e+50\n",
      "Gradient Descent(34/49): loss=3.499367660039329e+52\n",
      "Gradient Descent(35/49): loss=1.6275550568239446e+54\n",
      "Gradient Descent(36/49): loss=7.569754653797843e+55\n",
      "Gradient Descent(37/49): loss=3.5206910683877646e+57\n",
      "Gradient Descent(38/49): loss=1.6374725689169313e+59\n",
      "Gradient Descent(39/49): loss=7.615880978683225e+60\n",
      "Gradient Descent(40/49): loss=3.5421444109951423e+62\n",
      "Gradient Descent(41/49): loss=1.6474505134024553e+64\n",
      "Gradient Descent(42/49): loss=7.662288374481038e+65\n",
      "Gradient Descent(43/49): loss=3.563728479616198e+67\n",
      "Gradient Descent(44/49): loss=1.6574892585255858e+69\n",
      "Gradient Descent(45/49): loss=7.708978553898099e+70\n",
      "Gradient Descent(46/49): loss=3.585444070830547e+72\n",
      "Gradient Descent(47/49): loss=1.6675891747751488e+74\n",
      "Gradient Descent(48/49): loss=7.755953240076884e+75\n",
      "Gradient Descent(49/49): loss=3.607291986071344e+77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1224854958140762\n",
      "Gradient Descent(2/49): loss=7.462473157536487\n",
      "Gradient Descent(3/49): loss=79.34267163676328\n",
      "Gradient Descent(4/49): loss=1307.2556409926879\n",
      "Gradient Descent(5/49): loss=38939.78864674761\n",
      "Gradient Descent(6/49): loss=1640747.6165160302\n",
      "Gradient Descent(7/49): loss=76560992.42929403\n",
      "Gradient Descent(8/49): loss=3653720486.594729\n",
      "Gradient Descent(9/49): loss=175168534349.97342\n",
      "Gradient Descent(10/49): loss=8405764197580.835\n",
      "Gradient Descent(11/49): loss=403439538288280.0\n",
      "Gradient Descent(12/49): loss=1.9364030103730316e+16\n",
      "Gradient Descent(13/49): loss=9.294290617427684e+17\n",
      "Gradient Descent(14/49): loss=4.461053058688268e+19\n",
      "Gradient Descent(15/49): loss=2.1412070155749936e+21\n",
      "Gradient Descent(16/49): loss=1.027732173171561e+23\n",
      "Gradient Descent(17/49): loss=4.932887967311552e+24\n",
      "Gradient Descent(18/49): loss=2.3676775322242637e+26\n",
      "Gradient Descent(19/49): loss=1.1364330461995283e+28\n",
      "Gradient Descent(20/49): loss=5.454628221282299e+29\n",
      "Gradient Descent(21/49): loss=2.6181013595541465e+31\n",
      "Gradient Descent(22/49): loss=1.2566309656414838e+33\n",
      "Gradient Descent(23/49): loss=6.031551750460656e+34\n",
      "Gradient Descent(24/49): loss=2.895011941705612e+36\n",
      "Gradient Descent(25/49): loss=1.3895419436595345e+38\n",
      "Gradient Descent(26/49): loss=6.669495159497012e+39\n",
      "Gradient Descent(27/49): loss=3.201210721671676e+41\n",
      "Gradient Descent(28/49): loss=1.536510611294669e+43\n",
      "Gradient Descent(29/49): loss=7.37491238123884e+44\n",
      "Gradient Descent(30/49): loss=3.5397954450259824e+46\n",
      "Gradient Descent(31/49): loss=1.699023818167995e+48\n",
      "Gradient Descent(32/49): loss=8.154939966258502e+49\n",
      "Gradient Descent(33/49): loss=3.9141914987977025e+51\n",
      "Gradient Descent(34/49): loss=1.8787256745789474e+53\n",
      "Gradient Descent(35/49): loss=9.01746928173118e+54\n",
      "Gradient Descent(36/49): loss=4.328186565353112e+56\n",
      "Gradient Descent(37/49): loss=2.077434184606061e+58\n",
      "Gradient Descent(38/49): loss=9.97122634665771e+59\n",
      "Gradient Descent(39/49): loss=4.785968941544606e+61\n",
      "Gradient Descent(40/49): loss=2.297159638453904e+63\n",
      "Gradient Descent(41/49): loss=1.1025860111073987e+65\n",
      "Gradient Descent(42/49): loss=5.2921699107859296e+66\n",
      "Gradient Descent(43/49): loss=2.540124950115987e+68\n",
      "Gradient Descent(44/49): loss=1.2192040072355808e+70\n",
      "Gradient Descent(45/49): loss=5.851910596726275e+71\n",
      "Gradient Descent(46/49): loss=2.808788146105554e+73\n",
      "Gradient Descent(47/49): loss=1.3481564216167033e+75\n",
      "Gradient Descent(48/49): loss=6.470853772529406e+76\n",
      "Gradient Descent(49/49): loss=3.1058672327686336e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.130020332790947\n",
      "Gradient Descent(2/49): loss=7.622466086539862\n",
      "Gradient Descent(3/49): loss=80.37925261500942\n",
      "Gradient Descent(4/49): loss=1244.4753894233227\n",
      "Gradient Descent(5/49): loss=33895.48772314089\n",
      "Gradient Descent(6/49): loss=1346409.5092203561\n",
      "Gradient Descent(7/49): loss=60514814.57914678\n",
      "Gradient Descent(8/49): loss=2800072014.219961\n",
      "Gradient Descent(9/49): loss=130370501766.37122\n",
      "Gradient Descent(10/49): loss=6077932439354.114\n",
      "Gradient Descent(11/49): loss=283433096350534.2\n",
      "Gradient Descent(12/49): loss=1.3218125828264088e+16\n",
      "Gradient Descent(13/49): loss=6.164449997971882e+17\n",
      "Gradient Descent(14/49): loss=2.874880889990379e+19\n",
      "Gradient Descent(15/49): loss=1.3407431927935633e+21\n",
      "Gradient Descent(16/49): loss=6.252754705275009e+22\n",
      "Gradient Descent(17/49): loss=2.916064947069958e+24\n",
      "Gradient Descent(18/49): loss=1.3599501684957395e+26\n",
      "Gradient Descent(19/49): loss=6.342329461621423e+27\n",
      "Gradient Descent(20/49): loss=2.9578394811592066e+29\n",
      "Gradient Descent(21/49): loss=1.3794323441587472e+31\n",
      "Gradient Descent(22/49): loss=6.433187481094204e+32\n",
      "Gradient Descent(23/49): loss=3.000212467268415e+34\n",
      "Gradient Descent(24/49): loss=1.3991936151727202e+36\n",
      "Gradient Descent(25/49): loss=6.525347101576257e+37\n",
      "Gradient Descent(26/49): loss=3.0431924741733645e+39\n",
      "Gradient Descent(27/49): loss=1.4192379793295732e+41\n",
      "Gradient Descent(28/49): loss=6.618826968933916e+42\n",
      "Gradient Descent(29/49): loss=3.086788197803237e+44\n",
      "Gradient Descent(30/49): loss=1.4395694921198364e+46\n",
      "Gradient Descent(31/49): loss=6.713645996563552e+47\n",
      "Gradient Descent(32/49): loss=3.1310084587026906e+49\n",
      "Gradient Descent(33/49): loss=1.4601922671355438e+51\n",
      "Gradient Descent(34/49): loss=6.809823368812967e+52\n",
      "Gradient Descent(35/49): loss=3.175862203776923e+54\n",
      "Gradient Descent(36/49): loss=1.4811104768987377e+56\n",
      "Gradient Descent(37/49): loss=6.907378544857498e+57\n",
      "Gradient Descent(38/49): loss=3.2213585081013318e+59\n",
      "Gradient Descent(39/49): loss=1.5023283537055085e+61\n",
      "Gradient Descent(40/49): loss=7.00633126263793e+62\n",
      "Gradient Descent(41/49): loss=3.2675065767573554e+64\n",
      "Gradient Descent(42/49): loss=1.5238501904822625e+66\n",
      "Gradient Descent(43/49): loss=7.106701542854478e+67\n",
      "Gradient Descent(44/49): loss=3.314315746695881e+69\n",
      "Gradient Descent(45/49): loss=1.545680341654576e+71\n",
      "Gradient Descent(46/49): loss=7.208509693016497e+72\n",
      "Gradient Descent(47/49): loss=3.361795488625274e+74\n",
      "Gradient Descent(48/49): loss=1.5678232240278866e+76\n",
      "Gradient Descent(49/49): loss=7.311776311551876e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1447711463519068\n",
      "Gradient Descent(2/49): loss=7.725248984439472\n",
      "Gradient Descent(3/49): loss=80.2090252252947\n",
      "Gradient Descent(4/49): loss=1197.873068231535\n",
      "Gradient Descent(5/49): loss=31443.769420253786\n",
      "Gradient Descent(6/49): loss=1230510.533797589\n",
      "Gradient Descent(7/49): loss=55115031.560040265\n",
      "Gradient Descent(8/49): loss=2548933240.904501\n",
      "Gradient Descent(9/49): loss=118691540729.04343\n",
      "Gradient Descent(10/49): loss=5534800031017.053\n",
      "Gradient Descent(11/49): loss=258174364286075.44\n",
      "Gradient Descent(12/49): loss=1.2043454580100744e+16\n",
      "Gradient Descent(13/49): loss=5.618166425024089e+17\n",
      "Gradient Descent(14/49): loss=2.620832552792666e+19\n",
      "Gradient Descent(15/49): loss=1.2225994276591908e+21\n",
      "Gradient Descent(16/49): loss=5.703338543689275e+22\n",
      "Gradient Descent(17/49): loss=2.6605665420898473e+24\n",
      "Gradient Descent(18/49): loss=1.2411352237876892e+26\n",
      "Gradient Descent(19/49): loss=5.789806873581773e+27\n",
      "Gradient Descent(20/49): loss=2.700903414712911e+29\n",
      "Gradient Descent(21/49): loss=1.2599520873856715e+31\n",
      "Gradient Descent(22/49): loss=5.877586195324032e+32\n",
      "Gradient Descent(23/49): loss=2.7418518393988878e+34\n",
      "Gradient Descent(24/49): loss=1.2790542340661683e+36\n",
      "Gradient Descent(25/49): loss=5.966696340679516e+37\n",
      "Gradient Descent(26/49): loss=2.783421083615875e+39\n",
      "Gradient Descent(27/49): loss=1.298445988594599e+41\n",
      "Gradient Descent(28/49): loss=6.057157485877911e+42\n",
      "Gradient Descent(29/49): loss=2.825620559576607e+44\n",
      "Gradient Descent(30/49): loss=1.3181317417149404e+46\n",
      "Gradient Descent(31/49): loss=6.148990113438305e+47\n",
      "Gradient Descent(32/49): loss=2.8684598222306385e+49\n",
      "Gradient Descent(33/49): loss=1.3381159507427642e+51\n",
      "Gradient Descent(34/49): loss=6.242215016419025e+52\n",
      "Gradient Descent(35/49): loss=2.9119485713907633e+54\n",
      "Gradient Descent(36/49): loss=1.3584031405713146e+56\n",
      "Gradient Descent(37/49): loss=6.336853303122364e+57\n",
      "Gradient Descent(38/49): loss=2.956096653928758e+59\n",
      "Gradient Descent(39/49): loss=1.378997904695569e+61\n",
      "Gradient Descent(40/49): loss=6.432926401873386e+62\n",
      "Gradient Descent(41/49): loss=3.000914066004664e+64\n",
      "Gradient Descent(42/49): loss=1.3999049062526467e+66\n",
      "Gradient Descent(43/49): loss=6.530456065872585e+67\n",
      "Gradient Descent(44/49): loss=3.046410955330607e+69\n",
      "Gradient Descent(45/49): loss=1.4211288790774819e+71\n",
      "Gradient Descent(46/49): loss=6.6294643781204896e+72\n",
      "Gradient Descent(47/49): loss=3.092597623468008e+74\n",
      "Gradient Descent(48/49): loss=1.442674628774741e+76\n",
      "Gradient Descent(49/49): loss=6.729973756418961e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1153464568119764\n",
      "Gradient Descent(2/49): loss=7.323388323882081\n",
      "Gradient Descent(3/49): loss=72.92990146966653\n",
      "Gradient Descent(4/49): loss=981.8328949651732\n",
      "Gradient Descent(5/49): loss=22825.03918420532\n",
      "Gradient Descent(6/49): loss=852640.8834941331\n",
      "Gradient Descent(7/49): loss=38176547.66448193\n",
      "Gradient Descent(8/49): loss=1786763398.1607897\n",
      "Gradient Descent(9/49): loss=84416110852.47948\n",
      "Gradient Descent(10/49): loss=3995989652266.6494\n",
      "Gradient Descent(11/49): loss=189232225298390.9\n",
      "Gradient Descent(12/49): loss=8961915938189888.0\n",
      "Gradient Descent(13/49): loss=4.2443752962893574e+17\n",
      "Gradient Descent(14/49): loss=2.010148521107373e+19\n",
      "Gradient Descent(15/49): loss=9.520128541573129e+20\n",
      "Gradient Descent(16/49): loss=4.508764359106429e+22\n",
      "Gradient Descent(17/49): loss=2.135365771072931e+24\n",
      "Gradient Descent(18/49): loss=1.0113163251684453e+26\n",
      "Gradient Descent(19/49): loss=4.7896277284954236e+27\n",
      "Gradient Descent(20/49): loss=2.268383611752784e+29\n",
      "Gradient Descent(21/49): loss=1.074314018124715e+31\n",
      "Gradient Descent(22/49): loss=5.087986897679163e+32\n",
      "Gradient Descent(23/49): loss=2.4096875060976935e+34\n",
      "Gradient Descent(24/49): loss=1.1412360121631409e+36\n",
      "Gradient Descent(25/49): loss=5.404931685799141e+37\n",
      "Gradient Descent(26/49): loss=2.5597936112079112e+39\n",
      "Gradient Descent(27/49): loss=1.2123267624634299e+41\n",
      "Gradient Descent(28/49): loss=5.741619842122835e+42\n",
      "Gradient Descent(29/49): loss=2.719250241120798e+44\n",
      "Gradient Descent(30/49): loss=1.2878459523892086e+46\n",
      "Gradient Descent(31/49): loss=6.0992812357042295e+47\n",
      "Gradient Descent(32/49): loss=2.8886398659093314e+49\n",
      "Gradient Descent(33/49): loss=1.3680694417031846e+51\n",
      "Gradient Descent(34/49): loss=6.479222347549046e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=3.068581239320323e+54\n",
      "Gradient Descent(36/49): loss=1.4532902742364953e+56\n",
      "Gradient Descent(37/49): loss=6.882831042981288e+57\n",
      "Gradient Descent(38/49): loss=3.259731658984336e+59\n",
      "Gradient Descent(39/49): loss=1.5438197483316406e+61\n",
      "Gradient Descent(40/49): loss=7.311581641298122e+62\n",
      "Gradient Descent(41/49): loss=3.462789367420656e+64\n",
      "Gradient Descent(42/49): loss=1.6399885539666122e+66\n",
      "Gradient Descent(43/49): loss=7.767040301226607e+67\n",
      "Gradient Descent(44/49): loss=3.67849610260788e+69\n",
      "Gradient Descent(45/49): loss=1.7421479807133868e+71\n",
      "Gradient Descent(46/49): loss=8.250870741855698e+72\n",
      "Gradient Descent(47/49): loss=3.907639807436622e+74\n",
      "Gradient Descent(48/49): loss=1.850671200943919e+76\n",
      "Gradient Descent(49/49): loss=8.764840319942698e+77\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1382933286765347\n",
      "Gradient Descent(2/49): loss=7.761937715383838\n",
      "Gradient Descent(3/49): loss=84.28504166485287\n",
      "Gradient Descent(4/49): loss=1411.9458123893405\n",
      "Gradient Descent(5/49): loss=42685.30716523291\n",
      "Gradient Descent(6/49): loss=1828287.6458929186\n",
      "Gradient Descent(7/49): loss=86817769.6681302\n",
      "Gradient Descent(8/49): loss=4217749324.343509\n",
      "Gradient Descent(9/49): loss=205864785057.41675\n",
      "Gradient Descent(10/49): loss=10057548200843.055\n",
      "Gradient Descent(11/49): loss=491455554692381.4\n",
      "Gradient Descent(12/49): loss=2.401556666592933e+16\n",
      "Gradient Descent(13/49): loss=1.1735584697227075e+18\n",
      "Gradient Descent(14/49): loss=5.7347869496267416e+19\n",
      "Gradient Descent(15/49): loss=2.8023990513601977e+21\n",
      "Gradient Descent(16/49): loss=1.3694390034335089e+23\n",
      "Gradient Descent(17/49): loss=6.691992014163745e+24\n",
      "Gradient Descent(18/49): loss=3.270153480073239e+26\n",
      "Gradient Descent(19/49): loss=1.5980150254065932e+28\n",
      "Gradient Descent(20/49): loss=7.808966879505582e+29\n",
      "Gradient Descent(21/49): loss=3.815981874820807e+31\n",
      "Gradient Descent(22/49): loss=1.8647431720112043e+33\n",
      "Gradient Descent(23/49): loss=9.11237844317182e+34\n",
      "Gradient Descent(24/49): loss=4.452915668919666e+36\n",
      "Gradient Descent(25/49): loss=2.175991490933852e+38\n",
      "Gradient Descent(26/49): loss=1.0633345252112668e+40\n",
      "Gradient Descent(27/49): loss=5.196161461187698e+41\n",
      "Gradient Descent(28/49): loss=2.539190940439672e+43\n",
      "Gradient Descent(29/49): loss=1.2408179923141426e+45\n",
      "Gradient Descent(30/49): loss=6.063464017337409e+46\n",
      "Gradient Descent(31/49): loss=2.96301279617791e+48\n",
      "Gradient Descent(32/49): loss=1.4479256090595508e+50\n",
      "Gradient Descent(33/49): loss=7.075529920339062e+51\n",
      "Gradient Descent(34/49): loss=3.457575675184721e+53\n",
      "Gradient Descent(35/49): loss=1.689602006383226e+55\n",
      "Gradient Descent(36/49): loss=8.256521933743815e+56\n",
      "Gradient Descent(37/49): loss=4.034687114767278e+58\n",
      "Gradient Descent(38/49): loss=1.9716171342728315e+60\n",
      "Gradient Descent(39/49): loss=9.634635880265549e+61\n",
      "Gradient Descent(40/49): loss=4.708125473840336e+63\n",
      "Gradient Descent(41/49): loss=2.3007040175568424e+65\n",
      "Gradient Descent(42/49): loss=1.1242773808414994e+67\n",
      "Gradient Descent(43/49): loss=5.493968887028165e+68\n",
      "Gradient Descent(44/49): loss=2.6847195048113545e+70\n",
      "Gradient Descent(45/49): loss=1.3119329518834228e+72\n",
      "Gradient Descent(46/49): loss=6.410979125204444e+73\n",
      "Gradient Descent(47/49): loss=3.1328318482132163e+75\n",
      "Gradient Descent(48/49): loss=1.5309105204528787e+77\n",
      "Gradient Descent(49/49): loss=7.481049527027878e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.145977301815521\n",
      "Gradient Descent(2/49): loss=7.928385432335788\n",
      "Gradient Descent(3/49): loss=85.39713566002308\n",
      "Gradient Descent(4/49): loss=1344.8981216930629\n",
      "Gradient Descent(5/49): loss=37177.62624913279\n",
      "Gradient Descent(6/49): loss=1500852.0831348614\n",
      "Gradient Descent(7/49): loss=68640990.56642736\n",
      "Gradient Descent(8/49): loss=3233209520.4274096\n",
      "Gradient Descent(9/49): loss=153261964397.12656\n",
      "Gradient Descent(10/49): loss=7274663830852.61\n",
      "Gradient Descent(11/49): loss=345392088147939.0\n",
      "Gradient Descent(12/49): loss=1.639974560348647e+16\n",
      "Gradient Descent(13/49): loss=7.786945185923754e+17\n",
      "Gradient Descent(14/49): loss=3.697415242288912e+19\n",
      "Gradient Descent(15/49): loss=1.7556161503071917e+21\n",
      "Gradient Descent(16/49): loss=8.336062376153157e+22\n",
      "Gradient Descent(17/49): loss=3.958150962389068e+24\n",
      "Gradient Descent(18/49): loss=1.8794196120373587e+26\n",
      "Gradient Descent(19/49): loss=8.923909459864712e+27\n",
      "Gradient Descent(20/49): loss=4.237274079435767e+29\n",
      "Gradient Descent(21/49): loss=2.011953584446591e+31\n",
      "Gradient Descent(22/49): loss=9.553210743789772e+32\n",
      "Gradient Descent(23/49): loss=4.536080564728326e+34\n",
      "Gradient Descent(24/49): loss=2.1538336630007048e+36\n",
      "Gradient Descent(25/49): loss=1.022688945153943e+38\n",
      "Gradient Descent(26/49): loss=4.85595845448433e+39\n",
      "Gradient Descent(27/49): loss=2.305718921028169e+41\n",
      "Gradient Descent(28/49): loss=1.0948075014681931e+43\n",
      "Gradient Descent(29/49): loss=5.1983936738330345e+44\n",
      "Gradient Descent(30/49): loss=2.46831490941634e+46\n",
      "Gradient Descent(31/49): loss=1.1720117548455168e+48\n",
      "Gradient Descent(32/49): loss=5.56497693327512e+49\n",
      "Gradient Descent(33/49): loss=2.6423769334946723e+51\n",
      "Gradient Descent(34/49): loss=1.2546603413422616e+53\n",
      "Gradient Descent(35/49): loss=5.9574111179327234e+54\n",
      "Gradient Descent(36/49): loss=2.828713561640135e+56\n",
      "Gradient Descent(37/49): loss=1.343137187514361e+58\n",
      "Gradient Descent(38/49): loss=6.377519197942185e+59\n",
      "Gradient Descent(39/49): loss=3.028190381311014e+61\n",
      "Gradient Descent(40/49): loss=1.4378532938675487e+63\n",
      "Gradient Descent(41/49): loss=6.827252696677333e+64\n",
      "Gradient Descent(42/49): loss=3.2417340199508307e+66\n",
      "Gradient Descent(43/49): loss=1.5392486440732713e+68\n",
      "Gradient Descent(44/49): loss=7.308700756138575e+69\n",
      "Gradient Descent(45/49): loss=3.470336449439616e+71\n",
      "Gradient Descent(46/49): loss=1.6477942488197123e+73\n",
      "Gradient Descent(47/49): loss=7.824099841621163e+74\n",
      "Gradient Descent(48/49): loss=3.7150595940910076e+76\n",
      "Gradient Descent(49/49): loss=1.7639943337926644e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1610783873446224\n",
      "Gradient Descent(2/49): loss=8.035851706360088\n",
      "Gradient Descent(3/49): loss=85.22811992293781\n",
      "Gradient Descent(4/49): loss=1294.931622433728\n",
      "Gradient Descent(5/49): loss=34496.33996071335\n",
      "Gradient Descent(6/49): loss=1371761.138634055\n",
      "Gradient Descent(7/49): loss=62517171.08042529\n",
      "Gradient Descent(8/49): loss=2943228372.37854\n",
      "Gradient Descent(9/49): loss=139532067449.12534\n",
      "Gradient Descent(10/49): loss=6624570559513.456\n",
      "Gradient Descent(11/49): loss=314610681361259.06\n",
      "Gradient Descent(12/49): loss=1.4942273421058754e+16\n",
      "Gradient Descent(13/49): loss=7.096849806499771e+17\n",
      "Gradient Descent(14/49): loss=3.370666148370666e+19\n",
      "Gradient Descent(15/49): loss=1.6009070323652564e+21\n",
      "Gradient Descent(16/49): loss=7.60355228789306e+22\n",
      "Gradient Descent(17/49): loss=3.611328306615916e+24\n",
      "Gradient Descent(18/49): loss=1.7152104320962862e+26\n",
      "Gradient Descent(19/49): loss=8.146439702334158e+27\n",
      "Gradient Descent(20/49): loss=3.86917421889877e+29\n",
      "Gradient Descent(21/49): loss=1.8376750683608194e+31\n",
      "Gradient Descent(22/49): loss=8.728088904377904e+32\n",
      "Gradient Descent(23/49): loss=4.1454301271458634e+34\n",
      "Gradient Descent(24/49): loss=1.9688835811967595e+36\n",
      "Gradient Descent(25/49): loss=9.3512673894124e+37\n",
      "Gradient Descent(26/49): loss=4.441410483759253e+39\n",
      "Gradient Descent(27/49): loss=2.109460275682081e+41\n",
      "Gradient Descent(28/49): loss=1.0018940314010921e+43\n",
      "Gradient Descent(29/49): loss=4.7585235983294214e+44\n",
      "Gradient Descent(30/49): loss=2.260074032399631e+46\n",
      "Gradient Descent(31/49): loss=1.0734284545148143e+48\n",
      "Gradient Descent(32/49): loss=5.098278332673497e+49\n",
      "Gradient Descent(33/49): loss=2.4214414894707793e+51\n",
      "Gradient Descent(34/49): loss=1.1500703775534927e+53\n",
      "Gradient Descent(35/49): loss=5.462291280121384e+54\n",
      "Gradient Descent(36/49): loss=2.5943304523989076e+56\n",
      "Gradient Descent(37/49): loss=1.2321844718787939e+58\n",
      "Gradient Descent(38/49): loss=5.852294457459356e+59\n",
      "Gradient Descent(39/49): loss=2.779563547379165e+61\n",
      "Gradient Descent(40/49): loss=1.3201614460925818e+63\n",
      "Gradient Descent(41/49): loss=6.27014354607064e+64\n",
      "Gradient Descent(42/49): loss=2.9780221354512344e+66\n",
      "Gradient Descent(43/49): loss=1.4144199050746208e+68\n",
      "Gradient Descent(44/49): loss=6.7178267214867125e+69\n",
      "Gradient Descent(45/49): loss=3.1906505061197194e+71\n",
      "Gradient Descent(46/49): loss=1.5154083417544308e+73\n",
      "Gradient Descent(47/49): loss=7.19747411336428e+74\n",
      "Gradient Descent(48/49): loss=3.4184603703959085e+76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=1.6236072710937644e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.130994464717411\n",
      "Gradient Descent(2/49): loss=7.617495892526071\n",
      "Gradient Descent(3/49): loss=77.51847163955277\n",
      "Gradient Descent(4/49): loss=1062.3893377351874\n",
      "Gradient Descent(5/49): loss=25060.819044719083\n",
      "Gradient Descent(6/49): loss=950713.7927468044\n",
      "Gradient Descent(7/49): loss=43301954.30304183\n",
      "Gradient Descent(8/49): loss=2062910689.3923726\n",
      "Gradient Descent(9/49): loss=99223373331.15685\n",
      "Gradient Descent(10/49): loss=4781957739665.241\n",
      "Gradient Descent(11/49): loss=230554319378288.38\n",
      "Gradient Descent(12/49): loss=1.1116722040325674e+16\n",
      "Gradient Descent(13/49): loss=5.360281313073345e+17\n",
      "Gradient Descent(14/49): loss=2.584639168214892e+19\n",
      "Gradient Descent(15/49): loss=1.2462712248591327e+21\n",
      "Gradient Descent(16/49): loss=6.009319249514268e+22\n",
      "Gradient Descent(17/49): loss=2.8975971033170793e+24\n",
      "Gradient Descent(18/49): loss=1.39717473399669e+26\n",
      "Gradient Descent(19/49): loss=6.73695192992321e+27\n",
      "Gradient Descent(20/49): loss=3.248449904883645e+29\n",
      "Gradient Descent(21/49): loss=1.5663503161146297e+31\n",
      "Gradient Descent(22/49): loss=7.552689389268148e+32\n",
      "Gradient Descent(23/49): loss=3.641785392707128e+34\n",
      "Gradient Descent(24/49): loss=1.7560103643857377e+36\n",
      "Gradient Descent(25/49): loss=8.4671996488466325e+37\n",
      "Gradient Descent(26/49): loss=4.0827475365447294e+39\n",
      "Gradient Descent(27/49): loss=1.9686352204335907e+41\n",
      "Gradient Descent(28/49): loss=9.492442519263423e+42\n",
      "Gradient Descent(29/49): loss=4.5771031649873044e+44\n",
      "Gradient Descent(30/49): loss=2.2070055563067764e+46\n",
      "Gradient Descent(31/49): loss=1.064182595408581e+48\n",
      "Gradient Descent(32/49): loss=5.131317377676491e+49\n",
      "Gradient Descent(33/49): loss=2.4742387391079914e+51\n",
      "Gradient Descent(34/49): loss=1.1930381396277863e+53\n",
      "Gradient Descent(35/49): loss=5.7526380947362266e+54\n",
      "Gradient Descent(36/49): loss=2.77382959939034e+56\n",
      "Gradient Descent(37/49): loss=1.3374960356873897e+58\n",
      "Gradient Descent(38/49): loss=6.449190843852235e+59\n",
      "Gradient Descent(39/49): loss=3.1096961359629185e+61\n",
      "Gradient Descent(40/49): loss=1.499445479620293e+63\n",
      "Gradient Descent(41/49): loss=7.230085024553257e+64\n",
      "Gradient Descent(42/49): loss=3.486230754819253e+66\n",
      "Gradient Descent(43/49): loss=1.6810044189761946e+68\n",
      "Gradient Descent(44/49): loss=8.105533039404332e+69\n",
      "Gradient Descent(45/49): loss=3.9083577122830594e+71\n",
      "Gradient Descent(46/49): loss=1.8845472509831402e+73\n",
      "Gradient Descent(47/49): loss=9.08698385008747e+74\n",
      "Gradient Descent(48/49): loss=4.38159751360299e+76\n",
      "Gradient Descent(49/49): loss=2.1127358745143313e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1542715736056408\n",
      "Gradient Descent(2/49): loss=8.070807456935235\n",
      "Gradient Descent(3/49): loss=89.48330351334563\n",
      "Gradient Descent(4/49): loss=1523.942104070342\n",
      "Gradient Descent(5/49): loss=46753.0459692749\n",
      "Gradient Descent(6/49): loss=2035311.1693575731\n",
      "Gradient Descent(7/49): loss=98338776.67116313\n",
      "Gradient Descent(8/49): loss=4862639682.155338\n",
      "Gradient Descent(9/49): loss=241593152073.91373\n",
      "Gradient Descent(10/49): loss=12014739274849.484\n",
      "Gradient Descent(11/49): loss=597624058525207.5\n",
      "Gradient Descent(12/49): loss=2.972752079815362e+16\n",
      "Gradient Descent(13/49): loss=1.4787430269057661e+18\n",
      "Gradient Descent(14/49): loss=7.355757639215737e+19\n",
      "Gradient Descent(15/49): loss=3.65899866278369e+21\n",
      "Gradient Descent(16/49): loss=1.8201078289536445e+23\n",
      "Gradient Descent(17/49): loss=9.053822798999903e+24\n",
      "Gradient Descent(18/49): loss=4.503673144138204e+26\n",
      "Gradient Descent(19/49): loss=2.240277090687756e+28\n",
      "Gradient Descent(20/49): loss=1.1143884742426368e+30\n",
      "Gradient Descent(21/49): loss=5.543339601663369e+31\n",
      "Gradient Descent(22/49): loss=2.757441830182592e+33\n",
      "Gradient Descent(23/49): loss=1.3716434483944581e+35\n",
      "Gradient Descent(24/49): loss=6.823011564306081e+36\n",
      "Gradient Descent(25/49): loss=3.3939933049765466e+38\n",
      "Gradient Descent(26/49): loss=1.688285362798874e+40\n",
      "Gradient Descent(27/49): loss=8.398093956348022e+41\n",
      "Gradient Descent(28/49): loss=4.177491770865494e+43\n",
      "Gradient Descent(29/49): loss=2.0780236070659713e+45\n",
      "Gradient Descent(30/49): loss=1.0336781849910968e+47\n",
      "Gradient Descent(31/49): loss=5.141859729086969e+48\n",
      "Gradient Descent(32/49): loss=2.557732363659688e+50\n",
      "Gradient Descent(33/49): loss=1.2723013051298734e+52\n",
      "Gradient Descent(34/49): loss=6.328850641429064e+53\n",
      "Gradient Descent(35/49): loss=3.1481811957608084e+55\n",
      "Gradient Descent(36/49): loss=1.5660102288500217e+57\n",
      "Gradient Descent(37/49): loss=7.789856696193783e+58\n",
      "Gradient Descent(38/49): loss=3.874934290294942e+60\n",
      "Gradient Descent(39/49): loss=1.9275214345650508e+62\n",
      "Gradient Descent(40/49): loss=9.588133894329521e+63\n",
      "Gradient Descent(41/49): loss=4.769457289917725e+65\n",
      "Gradient Descent(42/49): loss=2.3724869814138514e+67\n",
      "Gradient Descent(43/49): loss=1.1801540793491831e+69\n",
      "Gradient Descent(44/49): loss=5.870479635570088e+70\n",
      "Gradient Descent(45/49): loss=2.9201721838430715e+72\n",
      "Gradient Descent(46/49): loss=1.45259094872278e+74\n",
      "Gradient Descent(47/49): loss=7.225671403850237e+75\n",
      "Gradient Descent(48/49): loss=3.594289726390434e+77\n",
      "Gradient Descent(49/49): loss=1.7879194769847784e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1621060861370107\n",
      "Gradient Descent(2/49): loss=8.243900692473781\n",
      "Gradient Descent(3/49): loss=90.67524584807605\n",
      "Gradient Descent(4/49): loss=1452.379852054953\n",
      "Gradient Descent(5/49): loss=40744.09275333418\n",
      "Gradient Descent(6/49): loss=1671399.392710093\n",
      "Gradient Descent(7/49): loss=77771179.1302307\n",
      "Gradient Descent(8/49): loss=3728570631.6593204\n",
      "Gradient Descent(9/49): loss=179913508761.5034\n",
      "Gradient Descent(10/49): loss=8693102267395.099\n",
      "Gradient Descent(11/49): loss=420154946855261.5\n",
      "Gradient Descent(12/49): loss=2.030813727296585e+16\n",
      "Gradient Descent(13/49): loss=9.816035699935903e+17\n",
      "Gradient Descent(14/49): loss=4.744640433683356e+19\n",
      "Gradient Descent(15/49): loss=2.293352009475835e+21\n",
      "Gradient Descent(16/49): loss=1.1085063488961836e+23\n",
      "Gradient Descent(17/49): loss=5.358036402864711e+24\n",
      "Gradient Descent(18/49): loss=2.58984121000763e+26\n",
      "Gradient Descent(19/49): loss=1.2518163364020625e+28\n",
      "Gradient Descent(20/49): loss=6.050734440299764e+29\n",
      "Gradient Descent(21/49): loss=2.924661246549578e+31\n",
      "Gradient Descent(22/49): loss=1.413653745929136e+33\n",
      "Gradient Descent(23/49): loss=6.832985925262967e+34\n",
      "Gradient Descent(24/49): loss=3.302767512151552e+36\n",
      "Gradient Descent(25/49): loss=1.59641383117663e+38\n",
      "Gradient Descent(26/49): loss=7.716368503067526e+39\n",
      "Gradient Descent(27/49): loss=3.729756139186492e+41\n",
      "Gradient Descent(28/49): loss=1.8028015189099908e+43\n",
      "Gradient Descent(29/49): loss=8.713956610828176e+44\n",
      "Gradient Descent(30/49): loss=4.2119467406099336e+46\n",
      "Gradient Descent(31/49): loss=2.035871434531781e+48\n",
      "Gradient Descent(32/49): loss=9.84051497608049e+49\n",
      "Gradient Descent(33/49): loss=4.756475942044698e+51\n",
      "Gradient Descent(34/49): loss=2.2990731117470015e+53\n",
      "Gradient Descent(35/49): loss=1.1112717140929423e+55\n",
      "Gradient Descent(36/49): loss=5.371403006860896e+56\n",
      "Gradient Descent(37/49): loss=2.596302047124906e+58\n",
      "Gradient Descent(38/49): loss=1.25493922375494e+60\n",
      "Gradient Descent(39/49): loss=6.065829116695553e+61\n",
      "Gradient Descent(40/49): loss=2.9319573551025606e+63\n",
      "Gradient Descent(41/49): loss=1.4171803667333481e+65\n",
      "Gradient Descent(42/49): loss=6.850032072803374e+66\n",
      "Gradient Descent(43/49): loss=3.311006876745993e+68\n",
      "Gradient Descent(44/49): loss=1.6003963808264162e+70\n",
      "Gradient Descent(45/49): loss=7.735618411881601e+71\n",
      "Gradient Descent(46/49): loss=3.7390607059071584e+73\n",
      "Gradient Descent(47/49): loss=1.8072989408300214e+75\n",
      "Gradient Descent(48/49): loss=8.73569518773784e+76\n",
      "Gradient Descent(49/49): loss=4.222454220994466e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1775611018190955\n",
      "Gradient Descent(2/49): loss=8.356205846901341\n",
      "Gradient Descent(3/49): loss=90.50806665457323\n",
      "Gradient Descent(4/49): loss=1398.8401504490694\n",
      "Gradient Descent(5/49): loss=37814.088112506346\n",
      "Gradient Descent(6/49): loss=1527753.050916191\n",
      "Gradient Descent(7/49): loss=70834021.23927903\n",
      "Gradient Descent(8/49): loss=3394168233.93651\n",
      "Gradient Descent(9/49): loss=163795758095.01962\n",
      "Gradient Descent(10/49): loss=7916229016313.382\n",
      "Gradient Descent(11/49): loss=382709382503904.1\n",
      "Gradient Descent(12/49): loss=1.8503251750643024e+16\n",
      "Gradient Descent(13/49): loss=8.946082274909573e+17\n",
      "Gradient Descent(14/49): loss=4.32532699197578e+19\n",
      "Gradient Descent(15/49): loss=2.091246653431097e+21\n",
      "Gradient Descent(16/49): loss=1.0110942159155638e+23\n",
      "Gradient Descent(17/49): loss=4.888526841267057e+24\n",
      "Gradient Descent(18/49): loss=2.3635477712773766e+26\n",
      "Gradient Descent(19/49): loss=1.1427487778517175e+28\n",
      "Gradient Descent(20/49): loss=5.5250618813614545e+29\n",
      "Gradient Descent(21/49): loss=2.671305311019023e+31\n",
      "Gradient Descent(22/49): loss=1.2915460890716148e+33\n",
      "Gradient Descent(23/49): loss=6.24448015477249e+34\n",
      "Gradient Descent(24/49): loss=3.0191359590887044e+36\n",
      "Gradient Descent(25/49): loss=1.4597182973664845e+38\n",
      "Gradient Descent(26/49): loss=7.057573877228581e+39\n",
      "Gradient Descent(27/49): loss=3.412257633709287e+41\n",
      "Gradient Descent(28/49): loss=1.6497882078677914e+43\n",
      "Gradient Descent(29/49): loss=7.976540528274342e+44\n",
      "Gradient Descent(30/49): loss=3.8565676791588685e+46\n",
      "Gradient Descent(31/49): loss=1.8646071202436494e+48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=9.01516582128731e+49\n",
      "Gradient Descent(33/49): loss=4.3587313328872306e+51\n",
      "Gradient Descent(34/49): loss=2.1073976018757494e+53\n",
      "Gradient Descent(35/49): loss=1.0189030507300919e+55\n",
      "Gradient Descent(36/49): loss=4.926281712871968e+56\n",
      "Gradient Descent(37/49): loss=2.381801830624357e+58\n",
      "Gradient Descent(38/49): loss=1.151574410684363e+60\n",
      "Gradient Descent(39/49): loss=5.567732824335661e+61\n",
      "Gradient Descent(40/49): loss=2.6919362323067632e+63\n",
      "Gradient Descent(41/49): loss=1.3015209076003806e+65\n",
      "Gradient Descent(42/49): loss=6.292707281068712e+66\n",
      "Gradient Descent(43/49): loss=3.042453232520243e+68\n",
      "Gradient Descent(44/49): loss=1.4709919369554837e+70\n",
      "Gradient Descent(45/49): loss=7.112080657343826e+71\n",
      "Gradient Descent(46/49): loss=3.4386110491707933e+73\n",
      "Gradient Descent(47/49): loss=1.6625297879981512e+75\n",
      "Gradient Descent(48/49): loss=8.03814463589217e+76\n",
      "Gradient Descent(49/49): loss=3.886352572685156e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1468112657511897\n",
      "Gradient Descent(2/49): loss=7.920854387772204\n",
      "Gradient Descent(3/49): loss=82.34679427150037\n",
      "Gradient Descent(4/49): loss=1148.7002676331879\n",
      "Gradient Descent(5/49): loss=27492.702654806857\n",
      "Gradient Descent(6/49): loss=1059046.7577875336\n",
      "Gradient Descent(7/49): loss=49060569.54974349\n",
      "Gradient Descent(8/49): loss=2378695173.886286\n",
      "Gradient Descent(9/49): loss=116460378737.83658\n",
      "Gradient Descent(10/49): loss=5713382041405.946\n",
      "Gradient Descent(11/49): loss=280406571717442.06\n",
      "Gradient Descent(12/49): loss=1.3763218263653696e+16\n",
      "Gradient Descent(13/49): loss=6.755529109452262e+17\n",
      "Gradient Descent(14/49): loss=3.3158913572368572e+19\n",
      "Gradient Descent(15/49): loss=1.627576961482733e+21\n",
      "Gradient Descent(16/49): loss=7.98882375110455e+22\n",
      "Gradient Descent(17/49): loss=3.9212466384142214e+24\n",
      "Gradient Descent(18/49): loss=1.924710792891726e+26\n",
      "Gradient Descent(19/49): loss=9.447280374494717e+27\n",
      "Gradient Descent(20/49): loss=4.637117785570522e+29\n",
      "Gradient Descent(21/49): loss=2.2760901027638726e+31\n",
      "Gradient Descent(22/49): loss=1.1171996044780447e+33\n",
      "Gradient Descent(23/49): loss=5.483679906753833e+34\n",
      "Gradient Descent(24/49): loss=2.6916179704340516e+36\n",
      "Gradient Descent(25/49): loss=1.3211579490337105e+38\n",
      "Gradient Descent(26/49): loss=6.484792215938213e+39\n",
      "Gradient Descent(27/49): loss=3.1830054926172183e+41\n",
      "Gradient Descent(28/49): loss=1.5623513643398994e+43\n",
      "Gradient Descent(29/49): loss=7.668669725252826e+44\n",
      "Gradient Descent(30/49): loss=3.7641017697613744e+46\n",
      "Gradient Descent(31/49): loss=1.847577564393511e+48\n",
      "Gradient Descent(32/49): loss=9.068678439761384e+49\n",
      "Gradient Descent(33/49): loss=4.451284223663211e+51\n",
      "Gradient Descent(34/49): loss=2.1848752683698836e+53\n",
      "Gradient Descent(35/49): loss=1.0724275733634915e+55\n",
      "Gradient Descent(36/49): loss=5.263920173202255e+56\n",
      "Gradient Descent(37/49): loss=2.583750761176455e+58\n",
      "Gradient Descent(38/49): loss=1.2682122403499029e+60\n",
      "Gradient Descent(39/49): loss=6.224912676333303e+61\n",
      "Gradient Descent(40/49): loss=3.0554458153853213e+63\n",
      "Gradient Descent(41/49): loss=1.4997397740613162e+65\n",
      "Gradient Descent(42/49): loss=7.36134602216102e+66\n",
      "Gradient Descent(43/49): loss=3.6132545255661773e+68\n",
      "Gradient Descent(44/49): loss=1.773535468543534e+70\n",
      "Gradient Descent(45/49): loss=8.705249065422774e+71\n",
      "Gradient Descent(46/49): loss=4.2728979845706184e+73\n",
      "Gradient Descent(47/49): loss=2.0973158894519525e+75\n",
      "Gradient Descent(48/49): loss=1.029449791694413e+77\n",
      "Gradient Descent(49/49): loss=5.052967361519394e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1704202306013956\n",
      "Gradient Descent(2/49): loss=8.389277557189482\n",
      "Gradient Descent(3/49): loss=94.94797778471748\n",
      "Gradient Descent(4/49): loss=1643.6796120959473\n",
      "Gradient Descent(5/49): loss=51167.49861624992\n",
      "Gradient Descent(6/49): loss=2263643.0842409865\n",
      "Gradient Descent(7/49): loss=111266549.22779636\n",
      "Gradient Descent(8/49): loss=5599110775.760045\n",
      "Gradient Descent(9/49): loss=283122740731.78754\n",
      "Gradient Descent(10/49): loss=14330326842497.05\n",
      "Gradient Descent(11/49): loss=725476519454841.9\n",
      "Gradient Descent(12/49): loss=3.672890308585283e+16\n",
      "Gradient Descent(13/49): loss=1.8594994388342602e+18\n",
      "Gradient Descent(14/49): loss=9.414230990050381e+19\n",
      "Gradient Descent(15/49): loss=4.766216773337166e+21\n",
      "Gradient Descent(16/49): loss=2.4130302150697176e+23\n",
      "Gradient Descent(17/49): loss=1.2216638842291628e+25\n",
      "Gradient Descent(18/49): loss=6.185014347182134e+26\n",
      "Gradient Descent(19/49): loss=3.131336121941046e+28\n",
      "Gradient Descent(20/49): loss=1.5853262998672675e+30\n",
      "Gradient Descent(21/49): loss=8.026156820074333e+31\n",
      "Gradient Descent(22/49): loss=4.063465881194544e+33\n",
      "Gradient Descent(23/49): loss=2.0572430040675515e+35\n",
      "Gradient Descent(24/49): loss=1.0415366835912959e+37\n",
      "Gradient Descent(25/49): loss=5.273070128912968e+38\n",
      "Gradient Descent(26/49): loss=2.669638911666531e+40\n",
      "Gradient Descent(27/49): loss=1.3515792023333601e+42\n",
      "Gradient Descent(28/49): loss=6.842746905572203e+43\n",
      "Gradient Descent(29/49): loss=3.464331585813291e+45\n",
      "Gradient Descent(30/49): loss=1.7539145466114324e+47\n",
      "Gradient Descent(31/49): loss=8.879681868250119e+48\n",
      "Gradient Descent(32/49): loss=4.49558675670174e+50\n",
      "Gradient Descent(33/49): loss=2.2760162567642527e+52\n",
      "Gradient Descent(34/49): loss=1.1522967482126246e+54\n",
      "Gradient Descent(35/49): loss=5.833823866570542e+55\n",
      "Gradient Descent(36/49): loss=2.9535361406651014e+57\n",
      "Gradient Descent(37/49): loss=1.495310097413479e+59\n",
      "Gradient Descent(38/49): loss=7.570424673805408e+60\n",
      "Gradient Descent(39/49): loss=3.832738763745214e+62\n",
      "Gradient Descent(40/49): loss=1.940430961811714e+64\n",
      "Gradient Descent(41/49): loss=9.82397327251754e+65\n",
      "Gradient Descent(42/49): loss=4.9736606330498324e+67\n",
      "Gradient Descent(43/49): loss=2.51805449857558e+69\n",
      "Gradient Descent(44/49): loss=1.2748353628439713e+71\n",
      "Gradient Descent(45/49): loss=6.454209800768213e+72\n",
      "Gradient Descent(46/49): loss=3.267623833355438e+74\n",
      "Gradient Descent(47/49): loss=1.654325757281915e+76\n",
      "Gradient Descent(48/49): loss=8.375485829395611e+77\n",
      "Gradient Descent(49/49): loss=4.2403234411137324e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1784066857554159\n",
      "Gradient Descent(2/49): loss=8.56921073098071\n",
      "Gradient Descent(3/49): loss=96.22429494178654\n",
      "Gradient Descent(4/49): loss=1567.3438043887913\n",
      "Gradient Descent(5/49): loss=44616.656969383715\n",
      "Gradient Descent(6/49): loss=1859565.7727269707\n",
      "Gradient Descent(7/49): loss=88018853.8126968\n",
      "Gradient Descent(8/49): loss=4294420753.391876\n",
      "Gradient Descent(9/49): loss=210900952036.7906\n",
      "Gradient Descent(10/49): loss=10371790169594.668\n",
      "Gradient Descent(11/49): loss=510217501696941.4\n",
      "Gradient Descent(12/49): loss=2.51005679662193e+16\n",
      "Gradient Descent(13/49): loss=1.2348588709649866e+18\n",
      "Gradient Descent(14/49): loss=6.075083834655468e+19\n",
      "Gradient Descent(15/49): loss=2.988735424679036e+21\n",
      "Gradient Descent(16/49): loss=1.4703567465945075e+23\n",
      "Gradient Descent(17/49): loss=7.233658083210457e+24\n",
      "Gradient Descent(18/49): loss=3.5587152343054344e+26\n",
      "Gradient Descent(19/49): loss=1.7507675905013548e+28\n",
      "Gradient Descent(20/49): loss=8.61318468843068e+29\n",
      "Gradient Descent(21/49): loss=4.237395693358845e+31\n",
      "Gradient Descent(22/49): loss=2.084655433709432e+33\n",
      "Gradient Descent(23/49): loss=1.0255800004976097e+35\n",
      "Gradient Descent(24/49): loss=5.0455068996680313e+36\n",
      "Gradient Descent(25/49): loss=2.4822188285895308e+38\n",
      "Gradient Descent(26/49): loss=1.22116775093696e+40\n",
      "Gradient Descent(27/49): loss=6.0077325123499566e+41\n",
      "Gradient Descent(28/49): loss=2.955601301479968e+43\n",
      "Gradient Descent(29/49): loss=1.4540559246525396e+45\n",
      "Gradient Descent(30/49): loss=7.153463597943656e+46\n",
      "Gradient Descent(31/49): loss=3.5192622635426955e+48\n",
      "Gradient Descent(32/49): loss=1.7313580631284601e+50\n",
      "Gradient Descent(33/49): loss=8.517696375780904e+51\n",
      "Gradient Descent(34/49): loss=4.1904186716232835e+53\n",
      "Gradient Descent(35/49): loss=2.061544327104437e+55\n",
      "Gradient Descent(36/49): loss=1.01421011733182e+57\n",
      "Gradient Descent(37/49): loss=4.9895709181426296e+58\n",
      "Gradient Descent(38/49): loss=2.4547002166247693e+60\n",
      "Gradient Descent(39/49): loss=1.2076295241316248e+62\n",
      "Gradient Descent(40/49): loss=5.941129013137396e+63\n",
      "Gradient Descent(41/49): loss=2.9228346314342223e+65\n",
      "Gradient Descent(42/49): loss=1.437935830684726e+67\n",
      "Gradient Descent(43/49): loss=7.074158185105402e+68\n",
      "Gradient Descent(44/49): loss=3.480246681387988e+70\n",
      "Gradient Descent(45/49): loss=1.712163715650895e+72\n",
      "Gradient Descent(46/49): loss=8.423266674942572e+73\n",
      "Gradient Descent(47/49): loss=4.143962451057206e+75\n",
      "Gradient Descent(48/49): loss=2.0386894370631959e+77\n",
      "Gradient Descent(49/49): loss=1.0029662840532464e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1942192897753257\n",
      "Gradient Descent(2/49): loss=8.686513662226659\n",
      "Gradient Descent(3/49): loss=96.0596240964031\n",
      "Gradient Descent(4/49): loss=1510.01094683067\n",
      "Gradient Descent(5/49): loss=41417.37808764081\n",
      "Gradient Descent(6/49): loss=1699873.7030773454\n",
      "Gradient Descent(7/49): loss=80168983.10296033\n",
      "Gradient Descent(8/49): loss=3909277364.7753706\n",
      "Gradient Descent(9/49): loss=192006829427.24698\n",
      "Gradient Descent(10/49): loss=9444871796673.516\n",
      "Gradient Descent(11/49): loss=464743748177696.25\n",
      "Gradient Descent(12/49): loss=2.286967241870798e+16\n",
      "Gradient Descent(13/49): loss=1.1254141659932146e+18\n",
      "Gradient Descent(14/49): loss=5.538167362151841e+19\n",
      "Gradient Descent(15/49): loss=2.7253359114535174e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=1.3411398195947867e+23\n",
      "Gradient Descent(17/49): loss=6.599759156675764e+24\n",
      "Gradient Descent(18/49): loss=3.247746471524302e+26\n",
      "Gradient Descent(19/49): loss=1.5982184963428795e+28\n",
      "Gradient Descent(20/49): loss=7.864845316812114e+29\n",
      "Gradient Descent(21/49): loss=3.870296333203543e+31\n",
      "Gradient Descent(22/49): loss=1.9045757549673572e+33\n",
      "Gradient Descent(23/49): loss=9.372431705804883e+34\n",
      "Gradient Descent(24/49): loss=4.61218073636151e+36\n",
      "Gradient Descent(25/49): loss=2.2696576312944476e+38\n",
      "Gradient Descent(26/49): loss=1.1169002382497965e+40\n",
      "Gradient Descent(27/49): loss=5.496274526175924e+41\n",
      "Gradient Descent(28/49): loss=2.704720854427328e+43\n",
      "Gradient Descent(29/49): loss=1.3309951796501401e+45\n",
      "Gradient Descent(30/49): loss=6.549837353278195e+46\n",
      "Gradient Descent(31/49): loss=3.223179919079529e+48\n",
      "Gradient Descent(32/49): loss=1.5861292777839757e+50\n",
      "Gradient Descent(33/49): loss=7.805354181289005e+51\n",
      "Gradient Descent(34/49): loss=3.8410207004365845e+53\n",
      "Gradient Descent(35/49): loss=1.8901691939295935e+55\n",
      "Gradient Descent(36/49): loss=9.301536909901111e+56\n",
      "Gradient Descent(37/49): loss=4.5772933536380116e+58\n",
      "Gradient Descent(38/49): loss=2.2524895238502787e+60\n",
      "Gradient Descent(39/49): loss=1.1084517995820855e+62\n",
      "Gradient Descent(40/49): loss=5.454699695546445e+63\n",
      "Gradient Descent(41/49): loss=2.6842618488067727e+65\n",
      "Gradient Descent(42/49): loss=1.3209272874989438e+67\n",
      "Gradient Descent(43/49): loss=6.500293179799143e+68\n",
      "Gradient Descent(44/49): loss=3.1987991938107345e+70\n",
      "Gradient Descent(45/49): loss=1.5741315044255434e+72\n",
      "Gradient Descent(46/49): loss=7.746313047781943e+73\n",
      "Gradient Descent(47/49): loss=3.8119665139498454e+75\n",
      "Gradient Descent(48/49): loss=1.8758716067685572e+77\n",
      "Gradient Descent(49/49): loss=9.231178375264152e+78\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1627968599133118\n",
      "Gradient Descent(2/49): loss=8.233656022148928\n",
      "Gradient Descent(3/49): loss=87.42480518379392\n",
      "Gradient Descent(4/49): loss=1241.1160501337572\n",
      "Gradient Descent(5/49): loss=30135.90512985588\n",
      "Gradient Descent(6/49): loss=1178608.2195469986\n",
      "Gradient Descent(7/49): loss=55523963.789904095\n",
      "Gradient Descent(8/49): loss=2739377772.2280803\n",
      "Gradient Descent(9/49): loss=136498847391.92151\n",
      "Gradient Descent(10/49): loss=6815523010316.806\n",
      "Gradient Descent(11/49): loss=340449995979365.4\n",
      "Gradient Descent(12/49): loss=1.7007688924004592e+16\n",
      "Gradient Descent(13/49): loss=8.496597597655091e+17\n",
      "Gradient Descent(14/49): loss=4.244694077498405e+19\n",
      "Gradient Descent(15/49): loss=2.120547811783312e+21\n",
      "Gradient Descent(16/49): loss=1.0593752198175855e+23\n",
      "Gradient Descent(17/49): loss=5.292386646535415e+24\n",
      "Gradient Descent(18/49): loss=2.6439505169204393e+26\n",
      "Gradient Descent(19/49): loss=1.3208548075268588e+28\n",
      "Gradient Descent(20/49): loss=6.598676533036621e+29\n",
      "Gradient Descent(21/49): loss=3.2965418865122466e+31\n",
      "Gradient Descent(22/49): loss=1.6468739383192934e+33\n",
      "Gradient Descent(23/49): loss=8.227390587144385e+34\n",
      "Gradient Descent(24/49): loss=4.110208698945441e+36\n",
      "Gradient Descent(25/49): loss=2.053362529704906e+38\n",
      "Gradient Descent(26/49): loss=1.0258110931150585e+40\n",
      "Gradient Descent(27/49): loss=5.124708294492591e+41\n",
      "Gradient Descent(28/49): loss=2.560182403944326e+43\n",
      "Gradient Descent(29/49): loss=1.2790062506602005e+45\n",
      "Gradient Descent(30/49): loss=6.389611094536009e+46\n",
      "Gradient Descent(31/49): loss=3.192097764834513e+48\n",
      "Gradient Descent(32/49): loss=1.5946961387015986e+50\n",
      "Gradient Descent(33/49): loss=7.966722707572354e+51\n",
      "Gradient Descent(34/49): loss=3.979985224710295e+53\n",
      "Gradient Descent(35/49): loss=1.9883059785494824e+55\n",
      "Gradient Descent(36/49): loss=9.933103871317636e+56\n",
      "Gradient Descent(37/49): loss=4.962342495714082e+58\n",
      "Gradient Descent(38/49): loss=2.479068311756604e+60\n",
      "Gradient Descent(39/49): loss=1.2384835790080378e+62\n",
      "Gradient Descent(40/49): loss=6.187169462811954e+63\n",
      "Gradient Descent(41/49): loss=3.0909627394668283e+65\n",
      "Gradient Descent(42/49): loss=1.5441714849087425e+67\n",
      "Gradient Descent(43/49): loss=7.714313551435781e+68\n",
      "Gradient Descent(44/49): loss=3.8538876123193506e+70\n",
      "Gradient Descent(45/49): loss=1.9253106098628948e+72\n",
      "Gradient Descent(46/49): loss=9.618393988972192e+73\n",
      "Gradient Descent(47/49): loss=4.8051209219527744e+75\n",
      "Gradient Descent(48/49): loss=2.400524152063237e+77\n",
      "Gradient Descent(49/49): loss=1.1992447845198537e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1867392996637989\n",
      "Gradient Descent(2/49): loss=8.717545201786997\n",
      "Gradient Descent(3/49): loss=100.68990704844344\n",
      "Gradient Descent(4/49): loss=1771.6149258729013\n",
      "Gradient Descent(5/49): loss=55954.778603286366\n",
      "Gradient Descent(6/49): loss=2515260.8791629705\n",
      "Gradient Descent(7/49): loss=125758222.27186944\n",
      "Gradient Descent(8/49): loss=6439189144.797987\n",
      "Gradient Descent(9/49): loss=331331832929.47595\n",
      "Gradient Descent(10/49): loss=17065901932981.791\n",
      "Gradient Descent(11/49): loss=879190911597026.4\n",
      "Gradient Descent(12/49): loss=4.529548400849007e+16\n",
      "Gradient Descent(13/49): loss=2.3336204017681203e+18\n",
      "Gradient Descent(14/49): loss=1.2022817309478204e+20\n",
      "Gradient Descent(15/49): loss=6.194160149491844e+21\n",
      "Gradient Descent(16/49): loss=3.191233934205582e+23\n",
      "Gradient Descent(17/49): loss=1.6441250981487883e+25\n",
      "Gradient Descent(18/49): loss=8.47053961463624e+26\n",
      "Gradient Descent(19/49): loss=4.3640256744760286e+28\n",
      "Gradient Descent(20/49): loss=2.2483479159647513e+30\n",
      "Gradient Descent(21/49): loss=1.1583498192748635e+32\n",
      "Gradient Descent(22/49): loss=5.967823281676686e+33\n",
      "Gradient Descent(23/49): loss=3.0746251373052758e+35\n",
      "Gradient Descent(24/49): loss=1.5840482012891163e+37\n",
      "Gradient Descent(25/49): loss=8.161023188038386e+38\n",
      "Gradient Descent(26/49): loss=4.204562678174815e+40\n",
      "Gradient Descent(27/49): loss=2.1661925113277025e+42\n",
      "Gradient Descent(28/49): loss=1.1160233192597277e+44\n",
      "Gradient Descent(29/49): loss=5.749756970436984e+45\n",
      "Gradient Descent(30/49): loss=2.962277279386801e+47\n",
      "Gradient Descent(31/49): loss=1.5261665362708734e+49\n",
      "Gradient Descent(32/49): loss=7.862816599380674e+50\n",
      "Gradient Descent(33/49): loss=4.0509265146489517e+52\n",
      "Gradient Descent(34/49): loss=2.0870390933928336e+54\n",
      "Gradient Descent(35/49): loss=1.075243444085914e+56\n",
      "Gradient Descent(36/49): loss=5.539658877066062e+57\n",
      "Gradient Descent(37/49): loss=2.8540346507617226e+59\n",
      "Gradient Descent(38/49): loss=1.4703998871610437e+61\n",
      "Gradient Descent(39/49): loss=7.5755065818355115e+62\n",
      "Gradient Descent(40/49): loss=3.9029042692756674e+64\n",
      "Gradient Descent(41/49): loss=2.0107779685196424e+66\n",
      "Gradient Descent(42/49): loss=1.03595367954909e+68\n",
      "Gradient Descent(43/49): loss=5.337237840145196e+69\n",
      "Gradient Descent(44/49): loss=2.7497472449421167e+71\n",
      "Gradient Descent(45/49): loss=1.4166709705522538e+73\n",
      "Gradient Descent(46/49): loss=7.298694970954293e+74\n",
      "Gradient Descent(47/49): loss=3.760290807559035e+76\n",
      "Gradient Descent(48/49): loss=1.937303451326987e+78\n",
      "Gradient Descent(49/49): loss=9.980995764952096e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.194879100670737\n",
      "Gradient Descent(2/49): loss=8.904516457637346\n",
      "Gradient Descent(3/49): loss=102.05532278975073\n",
      "Gradient Descent(4/49): loss=1690.234240214968\n",
      "Gradient Descent(5/49): loss=48818.539177300154\n",
      "Gradient Descent(6/49): loss=2066992.7031064031\n",
      "Gradient Descent(7/49): loss=99509139.56908776\n",
      "Gradient Descent(8/49): loss=4940035409.623946\n",
      "Gradient Descent(9/49): loss=246882009632.9794\n",
      "Gradient Descent(10/49): loss=12355538569581.996\n",
      "Gradient Descent(11/49): loss=618533383645906.0\n",
      "Gradient Descent(12/49): loss=3.0966479111708896e+16\n",
      "Gradient Descent(13/49): loss=1.5503374783460572e+18\n",
      "Gradient Descent(14/49): loss=7.761789673568579e+19\n",
      "Gradient Descent(15/49): loss=3.88595483525201e+21\n",
      "Gradient Descent(16/49): loss=1.9455109553533564e+23\n",
      "Gradient Descent(17/49): loss=9.740239236231862e+24\n",
      "Gradient Descent(18/49): loss=4.876470144596108e+26\n",
      "Gradient Descent(19/49): loss=2.4414144788204573e+28\n",
      "Gradient Descent(20/49): loss=1.2222990159068857e+30\n",
      "Gradient Descent(21/49): loss=6.119464340545028e+31\n",
      "Gradient Descent(22/49): loss=3.0637219966851424e+33\n",
      "Gradient Descent(23/49): loss=1.5338585128772076e+35\n",
      "Gradient Descent(24/49): loss=7.679293160647605e+36\n",
      "Gradient Descent(25/49): loss=3.844653398738671e+38\n",
      "Gradient Descent(26/49): loss=1.92483337296974e+40\n",
      "Gradient Descent(27/49): loss=9.636716576099258e+41\n",
      "Gradient Descent(28/49): loss=4.824641325954687e+43\n",
      "Gradient Descent(29/49): loss=2.4154662784045857e+45\n",
      "Gradient Descent(30/49): loss=1.2093079978240808e+47\n",
      "Gradient Descent(31/49): loss=6.054424550142052e+48\n",
      "Gradient Descent(32/49): loss=3.0311596962328332e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=1.517556132374416e+52\n",
      "Gradient Descent(34/49): loss=7.597674968327665e+53\n",
      "Gradient Descent(35/49): loss=3.803791088375409e+55\n",
      "Gradient Descent(36/49): loss=1.9043755759913465e+57\n",
      "Gradient Descent(37/49): loss=9.534294208521595e+58\n",
      "Gradient Descent(38/49): loss=4.773363363859e+60\n",
      "Gradient Descent(39/49): loss=2.389793864664481e+62\n",
      "Gradient Descent(40/49): loss=1.196455052810181e+64\n",
      "Gradient Descent(41/49): loss=5.990076025222155e+65\n",
      "Gradient Descent(42/49): loss=2.998943479211071e+67\n",
      "Gradient Descent(43/49): loss=1.5014270192286787e+69\n",
      "Gradient Descent(44/49): loss=7.516924242476809e+70\n",
      "Gradient Descent(45/49): loss=3.763363076825568e+72\n",
      "Gradient Descent(46/49): loss=1.8841352115779118e+74\n",
      "Gradient Descent(47/49): loss=9.432960421406061e+75\n",
      "Gradient Descent(48/49): loss=4.722630401737178e+77\n",
      "Gradient Descent(49/49): loss=2.3643943062455654e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2110529512133128\n",
      "Gradient Descent(2/49): loss=9.02697949066822\n",
      "Gradient Descent(3/49): loss=101.89388104149114\n",
      "Gradient Descent(4/49): loss=1628.8768774148425\n",
      "Gradient Descent(5/49): loss=45327.9353131131\n",
      "Gradient Descent(6/49): loss=1889627.14142396\n",
      "Gradient Descent(7/49): loss=90636077.35215941\n",
      "Gradient Descent(8/49): loss=4496999911.658439\n",
      "Gradient Descent(9/49): loss=224764056743.7575\n",
      "Gradient Descent(10/49): loss=11251303345623.268\n",
      "Gradient Descent(11/49): loss=563404031591473.9\n",
      "Gradient Descent(12/49): loss=2.8214131067657584e+16\n",
      "Gradient Descent(13/49): loss=1.4129266607555876e+18\n",
      "Gradient Descent(14/49): loss=7.075772535827493e+19\n",
      "Gradient Descent(15/49): loss=3.543466895341886e+21\n",
      "Gradient Descent(16/49): loss=1.7745283969708128e+23\n",
      "Gradient Descent(17/49): loss=8.886639022793263e+24\n",
      "Gradient Descent(18/49): loss=4.450329096716102e+26\n",
      "Gradient Descent(19/49): loss=2.2286748727379682e+28\n",
      "Gradient Descent(20/49): loss=1.116095367712732e+30\n",
      "Gradient Descent(21/49): loss=5.589280361770381e+31\n",
      "Gradient Descent(22/49): loss=2.799048886552146e+33\n",
      "Gradient Descent(23/49): loss=1.4017322736062707e+35\n",
      "Gradient Descent(24/49): loss=7.019717934582815e+36\n",
      "Gradient Descent(25/49): loss=3.515395971752444e+38\n",
      "Gradient Descent(26/49): loss=1.7604708555782987e+40\n",
      "Gradient Descent(27/49): loss=8.816240498209421e+41\n",
      "Gradient Descent(28/49): loss=4.4150743124195393e+43\n",
      "Gradient Descent(29/49): loss=2.211019673084625e+45\n",
      "Gradient Descent(30/49): loss=1.107253841915106e+47\n",
      "Gradient Descent(31/49): loss=5.545002992783633e+48\n",
      "Gradient Descent(32/49): loss=2.7768752770186286e+50\n",
      "Gradient Descent(33/49): loss=1.3906279787680194e+52\n",
      "Gradient Descent(34/49): loss=6.9641088720723815e+53\n",
      "Gradient Descent(35/49): loss=3.4875475772495243e+55\n",
      "Gradient Descent(36/49): loss=1.7465246921045407e+57\n",
      "Gradient Descent(37/49): loss=8.746399676464223e+58\n",
      "Gradient Descent(38/49): loss=4.3800988125896255e+60\n",
      "Gradient Descent(39/49): loss=2.1935043352379847e+62\n",
      "Gradient Descent(40/49): loss=1.0984823572651765e+64\n",
      "Gradient Descent(41/49): loss=5.501076381925517e+65\n",
      "Gradient Descent(42/49): loss=2.7548773232115575e+67\n",
      "Gradient Descent(43/49): loss=1.3796116503455526e+69\n",
      "Gradient Descent(44/49): loss=6.908940335500558e+70\n",
      "Gradient Descent(45/49): loss=3.4599197931932555e+72\n",
      "Gradient Descent(46/49): loss=1.7326890078669887e+74\n",
      "Gradient Descent(47/49): loss=8.677112122336841e+75\n",
      "Gradient Descent(48/49): loss=4.345400382974197e+77\n",
      "Gradient Descent(49/49): loss=2.1761277510457232e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.178951247203778\n",
      "Gradient Descent(2/49): loss=8.55609499040395\n",
      "Gradient Descent(3/49): loss=92.76274637371456\n",
      "Gradient Descent(4/49): loss=1340.0047228534636\n",
      "Gradient Descent(5/49): loss=33006.66968144185\n",
      "Gradient Descent(6/49): loss=1310448.2898098864\n",
      "Gradient Descent(7/49): loss=62771057.99335874\n",
      "Gradient Descent(8/49): loss=3150861854.010615\n",
      "Gradient Descent(9/49): loss=159763317618.402\n",
      "Gradient Descent(10/49): loss=8117745291132.32\n",
      "Gradient Descent(11/49): loss=412649962318450.1\n",
      "Gradient Descent(12/49): loss=2.097813947252304e+16\n",
      "Gradient Descent(13/49): loss=1.0664981783668868e+18\n",
      "Gradient Descent(14/49): loss=5.421942585437167e+19\n",
      "Gradient Descent(15/49): loss=2.75644956281908e+21\n",
      "Gradient Descent(16/49): loss=1.4013456056501816e+23\n",
      "Gradient Descent(17/49): loss=7.124271737648285e+24\n",
      "Gradient Descent(18/49): loss=3.621893695206601e+26\n",
      "Gradient Descent(19/49): loss=1.841327007299691e+28\n",
      "Gradient Descent(20/49): loss=9.361084101622478e+29\n",
      "Gradient Descent(21/49): loss=4.7590620903023825e+31\n",
      "Gradient Descent(22/49): loss=2.41944968486146e+33\n",
      "Gradient Descent(23/49): loss=1.2300189967093542e+35\n",
      "Gradient Descent(24/49): loss=6.253268012694002e+36\n",
      "Gradient Descent(25/49): loss=3.1790859282010476e+38\n",
      "Gradient Descent(26/49): loss=1.6162088876360322e+40\n",
      "Gradient Descent(27/49): loss=8.216610772618413e+41\n",
      "Gradient Descent(28/49): loss=4.177225673313606e+43\n",
      "Gradient Descent(29/49): loss=2.1236510781232926e+45\n",
      "Gradient Descent(30/49): loss=1.0796385578174236e+47\n",
      "Gradient Descent(31/49): loss=5.488752024914455e+48\n",
      "Gradient Descent(32/49): loss=2.790415234141514e+50\n",
      "Gradient Descent(33/49): loss=1.4186134012949898e+52\n",
      "Gradient Descent(34/49): loss=7.212059186427473e+53\n",
      "Gradient Descent(35/49): loss=3.6665237802669315e+55\n",
      "Gradient Descent(36/49): loss=1.864016404158453e+57\n",
      "Gradient Descent(37/49): loss=9.476434255443239e+58\n",
      "Gradient Descent(38/49): loss=4.817704715333795e+60\n",
      "Gradient Descent(39/49): loss=2.449262887126372e+62\n",
      "Gradient Descent(40/49): loss=1.2451756686459602e+64\n",
      "Gradient Descent(41/49): loss=6.33032270213744e+65\n",
      "Gradient Descent(42/49): loss=3.218259601616993e+67\n",
      "Gradient Descent(43/49): loss=1.636124310045498e+69\n",
      "Gradient Descent(44/49): loss=8.317858374684587e+70\n",
      "Gradient Descent(45/49): loss=4.228698731295451e+72\n",
      "Gradient Descent(46/49): loss=2.1498193590892835e+74\n",
      "Gradient Descent(47/49): loss=1.0929421957896627e+76\n",
      "Gradient Descent(48/49): loss=5.556386113499107e+77\n",
      "Gradient Descent(49/49): loss=2.8247995878665138e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2032287807928503\n",
      "Gradient Descent(2/49): loss=9.05580958700967\n",
      "Gradient Descent(3/49): loss=106.72026235982005\n",
      "Gradient Descent(4/49): loss=1908.2269735027533\n",
      "Gradient Descent(5/49): loss=61142.71070719617\n",
      "Gradient Descent(6/49): loss=2792305.8861698704\n",
      "Gradient Descent(7/49): loss=141986877.23743734\n",
      "Gradient Descent(8/49): loss=7396354488.813835\n",
      "Gradient Descent(9/49): loss=387222218405.41907\n",
      "Gradient Descent(10/49): loss=20292959096832.48\n",
      "Gradient Descent(11/49): loss=1063702788876951.9\n",
      "Gradient Descent(12/49): loss=5.575880100961779e+16\n",
      "Gradient Descent(13/49): loss=2.9228750306586194e+18\n",
      "Gradient Descent(14/49): loss=1.532173040881415e+20\n",
      "Gradient Descent(15/49): loss=8.031664107806258e+21\n",
      "Gradient Descent(16/49): loss=4.2102054526145124e+23\n",
      "Gradient Descent(17/49): loss=2.2069934660980054e+25\n",
      "Gradient Descent(18/49): loss=1.1569079534012247e+27\n",
      "Gradient Descent(19/49): loss=6.064521866480707e+28\n",
      "Gradient Descent(20/49): loss=3.1790278012437397e+30\n",
      "Gradient Descent(21/49): loss=1.6664492244943513e+32\n",
      "Gradient Descent(22/49): loss=8.735541780241796e+33\n",
      "Gradient Descent(23/49): loss=4.5791788356217235e+35\n",
      "Gradient Descent(24/49): loss=2.400409652442753e+37\n",
      "Gradient Descent(25/49): loss=1.2582968926039952e+39\n",
      "Gradient Descent(26/49): loss=6.596003595993088e+40\n",
      "Gradient Descent(27/49): loss=3.457631000607228e+42\n",
      "Gradient Descent(28/49): loss=1.8124932714746652e+44\n",
      "Gradient Descent(29/49): loss=9.501105984311265e+45\n",
      "Gradient Descent(30/49): loss=4.980488277987827e+47\n",
      "Gradient Descent(31/49): loss=2.6107764220433003e+49\n",
      "Gradient Descent(32/49): loss=1.3685713418948183e+51\n",
      "Gradient Descent(33/49): loss=7.174063248165397e+52\n",
      "Gradient Descent(34/49): loss=3.76065038870543e+54\n",
      "Gradient Descent(35/49): loss=1.9713363064768894e+56\n",
      "Gradient Descent(36/49): loss=1.0333762598367464e+58\n",
      "Gradient Descent(37/49): loss=5.416967621839306e+59\n",
      "Gradient Descent(38/49): loss=2.8395792855442206e+61\n",
      "Gradient Descent(39/49): loss=1.488510008142541e+63\n",
      "Gradient Descent(40/49): loss=7.802782812298987e+64\n",
      "Gradient Descent(41/49): loss=4.090225748087626e+66\n",
      "Gradient Descent(42/49): loss=2.1441000054428264e+68\n",
      "Gradient Descent(43/49): loss=1.1239391457768192e+70\n",
      "Gradient Descent(44/49): loss=5.891699082145189e+71\n",
      "Gradient Descent(45/49): loss=3.088433942796677e+73\n",
      "Gradient Descent(46/49): loss=1.6189598426581032e+75\n",
      "Gradient Descent(47/49): loss=8.486602014761456e+76\n",
      "Gradient Descent(48/49): loss=4.448684387297905e+78\n",
      "Gradient Descent(49/49): loss=2.3320043455984e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2115233308829734\n",
      "Gradient Descent(2/49): loss=9.250020827978066\n",
      "Gradient Descent(3/49): loss=108.17970397240421\n",
      "Gradient Descent(4/49): loss=1821.517290846641\n",
      "Gradient Descent(5/49): loss=53374.49261773596\n",
      "Gradient Descent(6/49): loss=2295458.2253802274\n",
      "Gradient Descent(7/49): loss=112379890.82189551\n",
      "Gradient Descent(8/49): loss=5675813155.643085\n",
      "Gradient Descent(9/49): loss=288607066557.14514\n",
      "Gradient Descent(10/49): loss=14696377736046.61\n",
      "Gradient Descent(11/49): loss=748592832014652.5\n",
      "Gradient Descent(12/49): loss=3.813369604818959e+16\n",
      "Gradient Descent(13/49): loss=1.942576017623624e+18\n",
      "Gradient Descent(14/49): loss=9.895742538653575e+19\n",
      "Gradient Descent(15/49): loss=5.041026897037595e+21\n",
      "Gradient Descent(16/49): loss=2.5679685297065272e+23\n",
      "Gradient Descent(17/49): loss=1.3081585720124717e+25\n",
      "Gradient Descent(18/49): loss=6.663940151369035e+26\n",
      "Gradient Descent(19/49): loss=3.394703008061109e+28\n",
      "Gradient Descent(20/49): loss=1.7293085253010516e+30\n",
      "Gradient Descent(21/49): loss=8.809336100205378e+31\n",
      "Gradient Descent(22/49): loss=4.4875972905845043e+33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=2.286043943997055e+35\n",
      "Gradient Descent(24/49): loss=1.164542309723987e+37\n",
      "Gradient Descent(25/49): loss=5.932339116658537e+38\n",
      "Gradient Descent(26/49): loss=3.0220153532574106e+40\n",
      "Gradient Descent(27/49): loss=1.5394562946812586e+42\n",
      "Gradient Descent(28/49): loss=7.842202656843545e+43\n",
      "Gradient Descent(29/49): loss=3.9949261777344615e+45\n",
      "Gradient Descent(30/49): loss=2.035070485155224e+47\n",
      "Gradient Descent(31/49): loss=1.0366929688544607e+49\n",
      "Gradient Descent(32/49): loss=5.281056943786002e+50\n",
      "Gradient Descent(33/49): loss=2.690243233184834e+52\n",
      "Gradient Descent(34/49): loss=1.3704470015633099e+54\n",
      "Gradient Descent(35/49): loss=6.981246011240632e+55\n",
      "Gradient Descent(36/49): loss=3.556342989832273e+57\n",
      "Gradient Descent(37/49): loss=1.8116501611553407e+59\n",
      "Gradient Descent(38/49): loss=9.228795748323048e+60\n",
      "Gradient Descent(39/49): loss=4.701275819717168e+62\n",
      "Gradient Descent(40/49): loss=2.3948947333755582e+64\n",
      "Gradient Descent(41/49): loss=1.2199924028909844e+66\n",
      "Gradient Descent(42/49): loss=6.214809537845056e+67\n",
      "Gradient Descent(43/49): loss=3.165909681090121e+69\n",
      "Gradient Descent(44/49): loss=1.6127580495886842e+71\n",
      "Gradient Descent(45/49): loss=8.215611904688115e+72\n",
      "Gradient Descent(46/49): loss=4.185145997917383e+74\n",
      "Gradient Descent(47/49): loss=2.131971084696548e+76\n",
      "Gradient Descent(48/49): loss=1.0860554705245697e+78\n",
      "Gradient Descent(49/49): loss=5.53251633440525e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2280620861330576\n",
      "Gradient Descent(2/49): loss=9.377809752726629\n",
      "Gradient Descent(3/49): loss=108.02226309777711\n",
      "Gradient Descent(4/49): loss=1755.8921959402762\n",
      "Gradient Descent(5/49): loss=49568.9240294609\n",
      "Gradient Descent(6/49): loss=2098642.6729575833\n",
      "Gradient Descent(7/49): loss=102360927.12922877\n",
      "Gradient Descent(8/49): loss=5166802428.447964\n",
      "Gradient Descent(9/49): loss=262750579857.4487\n",
      "Gradient Descent(10/49): loss=13382901760907.887\n",
      "Gradient Descent(11/49): loss=681869235637643.4\n",
      "Gradient Descent(12/49): loss=3.474419337045248e+16\n",
      "Gradient Descent(13/49): loss=1.7703932634762724e+18\n",
      "Gradient Descent(14/49): loss=9.021081459213907e+19\n",
      "Gradient Descent(15/49): loss=4.5967167417250323e+21\n",
      "Gradient Descent(16/49): loss=2.3422699134442227e+23\n",
      "Gradient Descent(17/49): loss=1.193510240719178e+25\n",
      "Gradient Descent(18/49): loss=6.0815651310063594e+26\n",
      "Gradient Descent(19/49): loss=3.098878688073562e+28\n",
      "Gradient Descent(20/49): loss=1.5790423877891657e+30\n",
      "Gradient Descent(21/49): loss=8.046055084927509e+31\n",
      "Gradient Descent(22/49): loss=4.0998900935835634e+33\n",
      "Gradient Descent(23/49): loss=2.089110576808547e+35\n",
      "Gradient Descent(24/49): loss=1.064512194842985e+37\n",
      "Gradient Descent(25/49): loss=5.4242519546317395e+38\n",
      "Gradient Descent(26/49): loss=2.763942903600657e+40\n",
      "Gradient Descent(27/49): loss=1.4083749129391582e+42\n",
      "Gradient Descent(28/49): loss=7.176414146661361e+43\n",
      "Gradient Descent(29/49): loss=3.656762097311292e+45\n",
      "Gradient Descent(30/49): loss=1.8633134547500294e+47\n",
      "Gradient Descent(31/49): loss=9.494566335626167e+48\n",
      "Gradient Descent(32/49): loss=4.8379830925279e+50\n",
      "Gradient Descent(33/49): loss=2.465207949072945e+52\n",
      "Gradient Descent(34/49): loss=1.256153673120217e+54\n",
      "Gradient Descent(35/49): loss=6.400766519866075e+55\n",
      "Gradient Descent(36/49): loss=3.261528658358549e+57\n",
      "Gradient Descent(37/49): loss=1.6619211396444541e+59\n",
      "Gradient Descent(38/49): loss=8.468366105932876e+60\n",
      "Gradient Descent(39/49): loss=4.31507986711408e+62\n",
      "Gradient Descent(40/49): loss=2.198761133688836e+64\n",
      "Gradient Descent(41/49): loss=1.120384945795708e+66\n",
      "Gradient Descent(42/49): loss=5.708953135167108e+67\n",
      "Gradient Descent(43/49): loss=2.909013194245247e+69\n",
      "Gradient Descent(44/49): loss=1.4822958892698826e+71\n",
      "Gradient Descent(45/49): loss=7.553080569359543e+72\n",
      "Gradient Descent(46/49): loss=3.8486935368441525e+74\n",
      "Gradient Descent(47/49): loss=1.9611126618502708e+76\n",
      "Gradient Descent(48/49): loss=9.992904957620869e+77\n",
      "Gradient Descent(49/49): loss=5.091912944860094e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.1952744276225877\n",
      "Gradient Descent(2/49): loss=8.88836746950408\n",
      "Gradient Descent(3/49): loss=98.37117225387766\n",
      "Gradient Descent(4/49): loss=1445.7527042513748\n",
      "Gradient Descent(5/49): loss=36122.32689101768\n",
      "Gradient Descent(6/49): loss=1455704.8304901673\n",
      "Gradient Descent(7/49): loss=70888804.12413985\n",
      "Gradient Descent(8/49): loss=3619765000.5677567\n",
      "Gradient Descent(9/49): loss=186738085779.54364\n",
      "Gradient Descent(10/49): loss=9654139482983.373\n",
      "Gradient Descent(11/49): loss=499328535577368.5\n",
      "Gradient Descent(12/49): loss=2.582848428014638e+16\n",
      "Gradient Descent(13/49): loss=1.3360406224803354e+18\n",
      "Gradient Descent(14/49): loss=6.911019228698149e+19\n",
      "Gradient Descent(15/49): loss=3.57490816608396e+21\n",
      "Gradient Descent(16/49): loss=1.849216461934688e+23\n",
      "Gradient Descent(17/49): loss=9.565564993592436e+24\n",
      "Gradient Descent(18/49): loss=4.948043464967578e+26\n",
      "Gradient Descent(19/49): loss=2.559507377098194e+28\n",
      "Gradient Descent(20/49): loss=1.323973417330037e+30\n",
      "Gradient Descent(21/49): loss=6.84860542146182e+31\n",
      "Gradient Descent(22/49): loss=3.542623711741274e+33\n",
      "Gradient Descent(23/49): loss=1.8325165476311644e+35\n",
      "Gradient Descent(24/49): loss=9.479180321110898e+36\n",
      "Gradient Descent(25/49): loss=4.903358699614376e+38\n",
      "Gradient Descent(26/49): loss=2.5363929920753637e+40\n",
      "Gradient Descent(27/49): loss=1.3120168856410371e+42\n",
      "Gradient Descent(28/49): loss=6.78675707426073e+43\n",
      "Gradient Descent(29/49): loss=3.5106310055241724e+45\n",
      "Gradient Descent(30/49): loss=1.8159674675390015e+47\n",
      "Gradient Descent(31/49): loss=9.393575792986516e+48\n",
      "Gradient Descent(32/49): loss=4.859077475554377e+50\n",
      "Gradient Descent(33/49): loss=2.513487348563081e+52\n",
      "Gradient Descent(34/49): loss=1.3001683309579047e+54\n",
      "Gradient Descent(35/49): loss=6.725467266792836e+55\n",
      "Gradient Descent(36/49): loss=3.478927218860721e+57\n",
      "Gradient Descent(37/49): loss=1.799567838786195e+59\n",
      "Gradient Descent(38/49): loss=9.308744341751903e+60\n",
      "Gradient Descent(39/49): loss=4.815196146122785e+62\n",
      "Gradient Descent(40/49): loss=2.4907885612069696e+64\n",
      "Gradient Descent(41/49): loss=1.2884267781355053e+66\n",
      "Gradient Descent(42/49): loss=6.664730954972018e+67\n",
      "Gradient Descent(43/49): loss=3.4475097425746344e+69\n",
      "Gradient Descent(44/49): loss=1.7833163117080347e+71\n",
      "Gradient Descent(45/49): loss=9.224678985907625e+72\n",
      "Gradient Descent(46/49): loss=4.771711099953e+74\n",
      "Gradient Descent(47/49): loss=2.4682947619314267e+76\n",
      "Gradient Descent(48/49): loss=1.2767912608619937e+78\n",
      "Gradient Descent(49/49): loss=6.604543140293012e+79\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2198886739885506\n",
      "Gradient Descent(2/49): loss=9.40427191978095\n",
      "Gradient Descent(3/49): loss=113.05054984429344\n",
      "Gradient Descent(4/49): loss=2054.0178919426266\n",
      "Gradient Descent(5/49): loss=66760.92659430634\n",
      "Gradient Descent(6/49): loss=3097095.2533321306\n",
      "Gradient Descent(7/49): loss=160143000.3513038\n",
      "Gradient Descent(8/49): loss=8485700281.424236\n",
      "Gradient Descent(9/49): loss=451935256895.2803\n",
      "Gradient Descent(10/49): loss=24094381356659.54\n",
      "Gradient Descent(11/49): loss=1284834216596981.2\n",
      "Gradient Descent(12/49): loss=6.8516798380165224e+16\n",
      "Gradient Descent(13/49): loss=3.6538508895643377e+18\n",
      "Gradient Descent(14/49): loss=1.9485221623171065e+20\n",
      "Gradient Descent(15/49): loss=1.0391064389741841e+22\n",
      "Gradient Descent(16/49): loss=5.541339439453577e+23\n",
      "Gradient Descent(17/49): loss=2.9550816056029287e+25\n",
      "Gradient Descent(18/49): loss=1.575883848595255e+27\n",
      "Gradient Descent(19/49): loss=8.403862353863202e+28\n",
      "Gradient Descent(20/49): loss=4.4816058324647293e+30\n",
      "Gradient Descent(21/49): loss=2.3899476207936634e+32\n",
      "Gradient Descent(22/49): loss=1.2745095940366233e+34\n",
      "Gradient Descent(23/49): loss=6.796695840361885e+35\n",
      "Gradient Descent(24/49): loss=3.6245371994488925e+37\n",
      "Gradient Descent(25/49): loss=1.9328906602196534e+39\n",
      "Gradient Descent(26/49): loss=1.0307705780844273e+41\n",
      "Gradient Descent(27/49): loss=5.496886122486373e+42\n",
      "Gradient Descent(28/49): loss=2.9313755830842873e+44\n",
      "Gradient Descent(29/49): loss=1.5632419187203154e+46\n",
      "Gradient Descent(30/49): loss=8.336445560050578e+47\n",
      "Gradient Descent(31/49): loss=4.4456538520010863e+49\n",
      "Gradient Descent(32/49): loss=2.3707751738370782e+51\n",
      "Gradient Descent(33/49): loss=1.2642853249477115e+53\n",
      "Gradient Descent(34/49): loss=6.742171929745242e+54\n",
      "Gradient Descent(35/49): loss=3.5954607265670307e+56\n",
      "Gradient Descent(36/49): loss=1.9173847791174116e+58\n",
      "Gradient Descent(37/49): loss=1.022501612665718e+60\n",
      "Gradient Descent(38/49): loss=5.45278944159167e+61\n",
      "Gradient Descent(39/49): loss=2.9078597359683285e+63\n",
      "Gradient Descent(40/49): loss=1.5507014042335372e+65\n",
      "Gradient Descent(41/49): loss=8.269569592190478e+66\n",
      "Gradient Descent(42/49): loss=4.409990282680031e+68\n",
      "Gradient Descent(43/49): loss=2.3517565305573413e+70\n",
      "Gradient Descent(44/49): loss=1.2541430761742562e+72\n",
      "Gradient Descent(45/49): loss=6.68808541649128e+73\n",
      "Gradient Descent(46/49): loss=3.5666175086444263e+75\n",
      "Gradient Descent(47/49): loss=1.902003288056506e+77\n",
      "Gradient Descent(48/49): loss=1.0142989818812072e+79\n",
      "Gradient Descent(49/49): loss=5.409046509569799e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2283393763921253\n",
      "Gradient Descent(2/49): loss=9.605928843291089\n",
      "Gradient Descent(3/49): loss=114.6091545146726\n",
      "Gradient Descent(4/49): loss=1961.681814092435\n",
      "Gradient Descent(5/49): loss=58310.889788777604\n",
      "Gradient Descent(6/49): loss=2546886.9648454026\n",
      "Gradient Descent(7/49): loss=126782858.51084279\n",
      "Gradient Descent(8/49): loss=6513399956.217693\n",
      "Gradient Descent(9/49): loss=336931252212.44495\n",
      "Gradient Descent(10/49): loss=17454641702241.23\n",
      "Gradient Descent(11/49): loss=904514194075725.8\n",
      "Gradient Descent(12/49): loss=4.687576549538916e+16\n",
      "Gradient Descent(13/49): loss=2.4293349582480077e+18\n",
      "Gradient Descent(14/49): loss=1.2590057247924378e+20\n",
      "Gradient Descent(15/49): loss=6.524816173054001e+21\n",
      "Gradient Descent(16/49): loss=3.381496276538482e+23\n",
      "Gradient Descent(17/49): loss=1.7524658295406035e+25\n",
      "Gradient Descent(18/49): loss=9.082182119059096e+26\n",
      "Gradient Descent(19/49): loss=4.706855378085926e+28\n",
      "Gradient Descent(20/49): loss=2.439335312334803e+30\n",
      "Gradient Descent(21/49): loss=1.2641894191268726e+32\n",
      "Gradient Descent(22/49): loss=6.551681842877133e+33\n",
      "Gradient Descent(23/49): loss=3.39541957249096e+35\n",
      "Gradient Descent(24/49): loss=1.7596816130189717e+37\n",
      "Gradient Descent(25/49): loss=9.119578046508138e+38\n",
      "Gradient Descent(26/49): loss=4.726235878755037e+40\n",
      "Gradient Descent(27/49): loss=2.4493792879138257e+42\n",
      "Gradient Descent(28/49): loss=1.269394725521357e+44\n",
      "Gradient Descent(29/49): loss=6.578658426371549e+45\n",
      "Gradient Descent(30/49): loss=3.409400229947717e+47\n",
      "Gradient Descent(31/49): loss=1.7669271110612557e+49\n",
      "Gradient Descent(32/49): loss=9.157127955761004e+50\n",
      "Gradient Descent(33/49): loss=4.7456961791602637e+52\n",
      "Gradient Descent(34/49): loss=2.4594646196603277e+54\n",
      "Gradient Descent(35/49): loss=1.2746214647965916e+56\n",
      "Gradient Descent(36/49): loss=6.605746086091617e+57\n",
      "Gradient Descent(37/49): loss=3.4234384528334233e+59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=1.7742024424787594e+61\n",
      "Gradient Descent(39/49): loss=9.1948324769571e+62\n",
      "Gradient Descent(40/49): loss=4.765236607452044e+64\n",
      "Gradient Descent(41/49): loss=2.4695914778118364e+66\n",
      "Gradient Descent(42/49): loss=1.279869725197529e+68\n",
      "Gradient Descent(43/49): loss=6.6329452793892386e+69\n",
      "Gradient Descent(44/49): loss=3.43753447817371e+71\n",
      "Gradient Descent(45/49): loss=1.7815077301106836e+73\n",
      "Gradient Descent(46/49): loss=9.232692246712346e+74\n",
      "Gradient Descent(47/49): loss=4.7848574935573284e+76\n",
      "Gradient Descent(48/49): loss=2.47976003335363e+78\n",
      "Gradient Descent(49/49): loss=1.285139595337548e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2452466945345595\n",
      "Gradient Descent(2/49): loss=9.73921295107143\n",
      "Gradient Descent(3/49): loss=114.45653945352004\n",
      "Gradient Descent(4/49): loss=1891.5333848422574\n",
      "Gradient Descent(5/49): loss=54165.02855788775\n",
      "Gradient Descent(6/49): loss=2328684.0691222595\n",
      "Gradient Descent(7/49): loss=115481822.04698096\n",
      "Gradient Descent(8/49): loss=5929287114.6791525\n",
      "Gradient Descent(9/49): loss=306744895683.20374\n",
      "Gradient Descent(10/49): loss=15894606321016.732\n",
      "Gradient Descent(11/49): loss=823890450420774.9\n",
      "Gradient Descent(12/49): loss=4.270907978086125e+16\n",
      "Gradient Descent(13/49): loss=2.2139994321751905e+18\n",
      "Gradient Descent(14/49): loss=1.1477205983255033e+20\n",
      "Gradient Descent(15/49): loss=5.949700957081167e+21\n",
      "Gradient Descent(16/49): loss=3.084282579571183e+23\n",
      "Gradient Descent(17/49): loss=1.5988701587470489e+25\n",
      "Gradient Descent(18/49): loss=8.288429268773439e+26\n",
      "Gradient Descent(19/49): loss=4.296662831466958e+28\n",
      "Gradient Descent(20/49): loss=2.227359478352361e+30\n",
      "Gradient Descent(21/49): loss=1.1546473253066237e+32\n",
      "Gradient Descent(22/49): loss=5.985609681830422e+33\n",
      "Gradient Descent(23/49): loss=3.102897523605397e+35\n",
      "Gradient Descent(24/49): loss=1.6085200261600686e+37\n",
      "Gradient Descent(25/49): loss=8.338453509582505e+38\n",
      "Gradient Descent(26/49): loss=4.3225950439334e+40\n",
      "Gradient Descent(27/49): loss=2.2408025531791395e+42\n",
      "Gradient Descent(28/49): loss=1.1616161197845902e+44\n",
      "Gradient Descent(29/49): loss=6.021735417201556e+45\n",
      "Gradient Descent(30/49): loss=3.1216248481042266e+47\n",
      "Gradient Descent(31/49): loss=1.6182281380988145e+49\n",
      "Gradient Descent(32/49): loss=8.388779671987208e+50\n",
      "Gradient Descent(33/49): loss=4.348683768892016e+52\n",
      "Gradient Descent(34/49): loss=2.2543267628037134e+54\n",
      "Gradient Descent(35/49): loss=1.1686269739470904e+56\n",
      "Gradient Descent(36/49): loss=6.0580791869685776e+57\n",
      "Gradient Descent(37/49): loss=3.14046520008221e+59\n",
      "Gradient Descent(38/49): loss=1.6279948426792412e+61\n",
      "Gradient Descent(39/49): loss=8.439409574482075e+62\n",
      "Gradient Descent(40/49): loss=4.3749299505547136e+64\n",
      "Gradient Descent(41/49): loss=2.2679325968648233e+66\n",
      "Gradient Descent(42/49): loss=1.1756801416374347e+68\n",
      "Gradient Descent(43/49): loss=6.094642307056901e+69\n",
      "Gradient Descent(44/49): loss=3.1594192617079356e+71\n",
      "Gradient Descent(45/49): loss=1.6378204935330137e+73\n",
      "Gradient Descent(46/49): loss=8.490345050269187e+74\n",
      "Gradient Descent(47/49): loss=4.401334539240671e+76\n",
      "Gradient Descent(48/49): loss=2.281620548001068e+78\n",
      "Gradient Descent(49/49): loss=1.1827758782359352e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2117664011697415\n",
      "Gradient Descent(2/49): loss=9.230671618635226\n",
      "Gradient Descent(3/49): loss=104.2609559515468\n",
      "Gradient Descent(4/49): loss=1558.7655235086368\n",
      "Gradient Descent(5/49): loss=39501.3568186537\n",
      "Gradient Descent(6/49): loss=1615609.926190086\n",
      "Gradient Descent(7/49): loss=79972921.75570862\n",
      "Gradient Descent(8/49): loss=4153498049.851932\n",
      "Gradient Descent(9/49): loss=217974985324.60196\n",
      "Gradient Descent(10/49): loss=11464231394079.729\n",
      "Gradient Descent(11/49): loss=603225234741054.4\n",
      "Gradient Descent(12/49): loss=3.1743494267966396e+16\n",
      "Gradient Descent(13/49): loss=1.6704688723195436e+18\n",
      "Gradient Descent(14/49): loss=8.790706668833545e+19\n",
      "Gradient Descent(15/49): loss=4.6260417866018e+21\n",
      "Gradient Descent(16/49): loss=2.4344193371104468e+23\n",
      "Gradient Descent(17/49): loss=1.2810947228659982e+25\n",
      "Gradient Descent(18/49): loss=6.741663919929389e+26\n",
      "Gradient Descent(19/49): loss=3.54774956665599e+28\n",
      "Gradient Descent(20/49): loss=1.866976334209708e+30\n",
      "Gradient Descent(21/49): loss=9.824821530493892e+31\n",
      "Gradient Descent(22/49): loss=5.170237904929482e+33\n",
      "Gradient Descent(23/49): loss=2.7207985316249883e+35\n",
      "Gradient Descent(24/49): loss=1.4317996165396637e+37\n",
      "Gradient Descent(25/49): loss=7.534737019646296e+38\n",
      "Gradient Descent(26/49): loss=3.965098279075915e+40\n",
      "Gradient Descent(27/49): loss=2.0866029327548767e+42\n",
      "Gradient Descent(28/49): loss=1.0980589868244056e+44\n",
      "Gradient Descent(29/49): loss=5.778452237455262e+45\n",
      "Gradient Descent(30/49): loss=3.040866716743282e+47\n",
      "Gradient Descent(31/49): loss=1.600233074362004e+49\n",
      "Gradient Descent(32/49): loss=8.421105332181693e+50\n",
      "Gradient Descent(33/49): loss=4.4315428890864444e+52\n",
      "Gradient Descent(34/49): loss=2.3320658753385853e+54\n",
      "Gradient Descent(35/49): loss=1.2272320009160246e+56\n",
      "Gradient Descent(36/49): loss=6.458215438934431e+57\n",
      "Gradient Descent(37/49): loss=3.398586952145823e+59\n",
      "Gradient Descent(38/49): loss=1.7884806384226093e+61\n",
      "Gradient Descent(39/49): loss=9.411743877828082e+62\n",
      "Gradient Descent(40/49): loss=4.952858919398679e+64\n",
      "Gradient Descent(41/49): loss=2.6064044871914546e+66\n",
      "Gradient Descent(42/49): loss=1.371600617220166e+68\n",
      "Gradient Descent(43/49): loss=7.217944345951993e+69\n",
      "Gradient Descent(44/49): loss=3.7983885343277453e+71\n",
      "Gradient Descent(45/49): loss=1.9988731924490493e+73\n",
      "Gradient Descent(46/49): loss=1.0518918755631256e+75\n",
      "Gradient Descent(47/49): loss=5.535501311716785e+76\n",
      "Gradient Descent(48/49): loss=2.9130156324873422e+78\n",
      "Gradient Descent(49/49): loss=1.532952409776212e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.236718979250899\n",
      "Gradient Descent(2/49): loss=9.76313541766575\n",
      "Gradient Descent(3/49): loss=119.69261734726322\n",
      "Gradient Descent(4/49): loss=2209.513922460814\n",
      "Gradient Descent(5/49): loss=72840.96485904511\n",
      "Gradient Descent(6/49): loss=3432134.6770426887\n",
      "Gradient Descent(7/49): loss=180436059.91146857\n",
      "Gradient Descent(8/49): loss=9724110511.87949\n",
      "Gradient Descent(9/49): loss=526769862393.769\n",
      "Gradient Descent(10/49): loss=28566131991618.13\n",
      "Gradient Descent(11/49): loss=1549443236016243.8\n",
      "Gradient Descent(12/49): loss=8.404637777574e+16\n",
      "Gradient Descent(13/49): loss=4.558964633985035e+18\n",
      "Gradient Descent(14/49): loss=2.4729437523937247e+20\n",
      "Gradient Descent(15/49): loss=1.3414127041246229e+22\n",
      "Gradient Descent(16/49): loss=7.276300471652742e+23\n",
      "Gradient Descent(17/49): loss=3.946924701753974e+25\n",
      "Gradient Descent(18/49): loss=2.140952632596018e+27\n",
      "Gradient Descent(19/49): loss=1.161329015454081e+29\n",
      "Gradient Descent(20/49): loss=6.299462499907751e+30\n",
      "Gradient Descent(21/49): loss=3.417052985047901e+32\n",
      "Gradient Descent(22/49): loss=1.8535313295156626e+34\n",
      "Gradient Descent(23/49): loss=1.0054214565981271e+36\n",
      "Gradient Descent(24/49): loss=5.453764332390879e+37\n",
      "Gradient Descent(25/49): loss=2.958316156679061e+39\n",
      "Gradient Descent(26/49): loss=1.6046961235363267e+41\n",
      "Gradient Descent(27/49): loss=8.704443719035047e+42\n",
      "Gradient Descent(28/49): loss=4.721600516543888e+44\n",
      "Gradient Descent(29/49): loss=2.56116440721825e+46\n",
      "Gradient Descent(30/49): loss=1.3892668593663903e+48\n",
      "Gradient Descent(31/49): loss=7.535878606989014e+49\n",
      "Gradient Descent(32/49): loss=4.08772914983193e+51\n",
      "Gradient Descent(33/49): loss=2.217329985502739e+53\n",
      "Gradient Descent(34/49): loss=1.202758814098978e+55\n",
      "Gradient Descent(35/49): loss=6.524192494356137e+56\n",
      "Gradient Descent(36/49): loss=3.538954543874935e+58\n",
      "Gradient Descent(37/49): loss=1.9196550798351755e+60\n",
      "Gradient Descent(38/49): loss=1.04128933555108e+62\n",
      "Gradient Descent(39/49): loss=5.6483244918431586e+63\n",
      "Gradient Descent(40/49): loss=3.0638525216693372e+65\n",
      "Gradient Descent(41/49): loss=1.661942809428799e+67\n",
      "Gradient Descent(42/49): loss=9.014970147150506e+68\n",
      "Gradient Descent(43/49): loss=4.890041118920678e+70\n",
      "Gradient Descent(44/49): loss=2.6525325934988335e+72\n",
      "Gradient Descent(45/49): loss=1.4388282201452317e+74\n",
      "Gradient Descent(46/49): loss=7.804717092488258e+75\n",
      "Gradient Descent(47/49): loss=4.233556726294315e+77\n",
      "Gradient Descent(48/49): loss=2.296432060555017e+79\n",
      "Gradient Descent(49/49): loss=1.2456665989594664e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2453272371981932\n",
      "Gradient Descent(2/49): loss=9.97244755061857\n",
      "Gradient Descent(3/49): loss=121.35573866540437\n",
      "Gradient Descent(4/49): loss=2111.240275989968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=63655.81277093616\n",
      "Gradient Descent(6/49): loss=2823360.7915557246\n",
      "Gradient Descent(7/49): loss=142884952.61369172\n",
      "Gradient Descent(8/49): loss=7465826078.161734\n",
      "Gradient Descent(9/49): loss=392827962906.6564\n",
      "Gradient Descent(10/49): loss=20700204139766.188\n",
      "Gradient Descent(11/49): loss=1091150024126996.2\n",
      "Gradient Descent(12/49): loss=5.752061315484543e+16\n",
      "Gradient Descent(13/49): loss=3.032276099789564e+18\n",
      "Gradient Descent(14/49): loss=1.5985097637950125e+20\n",
      "Gradient Descent(15/49): loss=8.426789057574284e+21\n",
      "Gradient Descent(16/49): loss=4.4423115268330154e+23\n",
      "Gradient Descent(17/49): loss=2.3418329492946796e+25\n",
      "Gradient Descent(18/49): loss=1.2345333196249134e+27\n",
      "Gradient Descent(19/49): loss=6.5080326062784626e+28\n",
      "Gradient Descent(20/49): loss=3.430809662467474e+30\n",
      "Gradient Descent(21/49): loss=1.808604174695508e+32\n",
      "Gradient Descent(22/49): loss=9.534335572594946e+33\n",
      "Gradient Descent(23/49): loss=5.02617190001743e+35\n",
      "Gradient Descent(24/49): loss=2.6496239592355863e+37\n",
      "Gradient Descent(25/49): loss=1.3967900949292458e+39\n",
      "Gradient Descent(26/49): loss=7.363394199738988e+40\n",
      "Gradient Descent(27/49): loss=3.881726705936977e+42\n",
      "Gradient Descent(28/49): loss=2.046312041818742e+44\n",
      "Gradient Descent(29/49): loss=1.078744921966768e+46\n",
      "Gradient Descent(30/49): loss=5.686770066772502e+47\n",
      "Gradient Descent(31/49): loss=2.997868461190872e+49\n",
      "Gradient Descent(32/49): loss=1.5803725498090585e+51\n",
      "Gradient Descent(33/49): loss=8.331177396615331e+52\n",
      "Gradient Descent(34/49): loss=4.391908529559192e+54\n",
      "Gradient Descent(35/49): loss=2.3152622509095114e+56\n",
      "Gradient Descent(36/49): loss=1.2205261686141333e+58\n",
      "Gradient Descent(37/49): loss=6.434191753814221e+59\n",
      "Gradient Descent(38/49): loss=3.3918833196225183e+61\n",
      "Gradient Descent(39/49): loss=1.7880835533247975e+63\n",
      "Gradient Descent(40/49): loss=9.426157955299514e+64\n",
      "Gradient Descent(41/49): loss=4.969144402286957e+66\n",
      "Gradient Descent(42/49): loss=2.619561035140334e+68\n",
      "Gradient Descent(43/49): loss=1.3809419612895754e+70\n",
      "Gradient Descent(44/49): loss=7.279848321412351e+71\n",
      "Gradient Descent(45/49): loss=3.837684208920724e+73\n",
      "Gradient Descent(46/49): loss=2.023094360919602e+75\n",
      "Gradient Descent(47/49): loss=1.0665053637479229e+77\n",
      "Gradient Descent(48/49): loss=5.622247349777885e+78\n",
      "Gradient Descent(49/49): loss=2.963854316775493e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2626067764178186\n",
      "Gradient Descent(2/49): loss=10.111399670540802\n",
      "Gradient Descent(3/49): loss=121.20882971010853\n",
      "Gradient Descent(4/49): loss=2036.3000206943586\n",
      "Gradient Descent(5/49): loss=59142.53837399168\n",
      "Gradient Descent(6/49): loss=2581659.3570021717\n",
      "Gradient Descent(7/49): loss=130150869.26555191\n",
      "Gradient Descent(8/49): loss=6796316448.261288\n",
      "Gradient Descent(9/49): loss=357633168777.18396\n",
      "Gradient Descent(10/49): loss=18850042820550.664\n",
      "Gradient Descent(11/49): loss=993887487673651.2\n",
      "Gradient Descent(12/49): loss=5.240755295692593e+16\n",
      "Gradient Descent(13/49): loss=2.763485916572668e+18\n",
      "Gradient Descent(14/49): loss=1.4572096830307559e+20\n",
      "Gradient Descent(15/49): loss=7.683996193573147e+21\n",
      "Gradient Descent(16/49): loss=4.0518400889310434e+23\n",
      "Gradient Descent(17/49): loss=2.136571674269987e+25\n",
      "Gradient Descent(18/49): loss=1.1266334427907698e+27\n",
      "Gradient Descent(19/49): loss=5.940839389792538e+28\n",
      "Gradient Descent(20/49): loss=3.132657999051679e+30\n",
      "Gradient Descent(21/49): loss=1.651878715402905e+32\n",
      "Gradient Descent(22/49): loss=8.710504917110818e+33\n",
      "Gradient Descent(23/49): loss=4.593127522241445e+35\n",
      "Gradient Descent(24/49): loss=2.4219974199362505e+37\n",
      "Gradient Descent(25/49): loss=1.2771410055072452e+39\n",
      "Gradient Descent(26/49): loss=6.73447929598154e+40\n",
      "Gradient Descent(27/49): loss=3.551151454101998e+42\n",
      "Gradient Descent(28/49): loss=1.8725540751897133e+44\n",
      "Gradient Descent(29/49): loss=9.874145920921426e+45\n",
      "Gradient Descent(30/49): loss=5.206725880948097e+47\n",
      "Gradient Descent(31/49): loss=2.74555334876038e+49\n",
      "Gradient Descent(32/49): loss=1.4477549545044784e+51\n",
      "Gradient Descent(33/49): loss=7.634141981755998e+52\n",
      "Gradient Descent(34/49): loss=4.025551673387761e+54\n",
      "Gradient Descent(35/49): loss=2.122709574152785e+56\n",
      "Gradient Descent(36/49): loss=1.1193238347895711e+58\n",
      "Gradient Descent(37/49): loss=5.9022951720942576e+59\n",
      "Gradient Descent(38/49): loss=3.112333286914741e+61\n",
      "Gradient Descent(39/49): loss=1.6411613120664476e+63\n",
      "Gradient Descent(40/49): loss=8.653991086197658e+64\n",
      "Gradient Descent(41/49): loss=4.5633272713266366e+66\n",
      "Gradient Descent(42/49): loss=2.4062834798206606e+68\n",
      "Gradient Descent(43/49): loss=1.2688549036665161e+70\n",
      "Gradient Descent(44/49): loss=6.690785936321012e+71\n",
      "Gradient Descent(45/49): loss=3.5281115528900885e+73\n",
      "Gradient Descent(46/49): loss=1.860404928225945e+75\n",
      "Gradient Descent(47/49): loss=9.810082377163729e+76\n",
      "Gradient Descent(48/49): loss=5.172944598599185e+78\n",
      "Gradient Descent(49/49): loss=2.727740174992573e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2284271678452388\n",
      "Gradient Descent(2/49): loss=9.5832075792024\n",
      "Gradient Descent(3/49): loss=110.44329567092326\n",
      "Gradient Descent(4/49): loss=1679.4685720057819\n",
      "Gradient Descent(5/49): loss=43163.45405480784\n",
      "Gradient Descent(6/49): loss=1791496.7717073713\n",
      "Gradient Descent(7/49): loss=90128694.98439237\n",
      "Gradient Descent(8/49): loss=4760352089.270713\n",
      "Gradient Descent(9/49): loss=254102097640.98557\n",
      "Gradient Descent(10/49): loss=13593790174394.035\n",
      "Gradient Descent(11/49): loss=727567477185550.2\n",
      "Gradient Descent(12/49): loss=3.89446337847989e+16\n",
      "Gradient Descent(13/49): loss=2.0846378393054228e+18\n",
      "Gradient Descent(14/49): loss=1.1158746309611941e+20\n",
      "Gradient Descent(15/49): loss=5.973110711979183e+21\n",
      "Gradient Descent(16/49): loss=3.1973177762954674e+23\n",
      "Gradient Descent(17/49): loss=1.7114769563002545e+25\n",
      "Gradient Descent(18/49): loss=9.161283298692561e+26\n",
      "Gradient Descent(19/49): loss=4.903899611612347e+28\n",
      "Gradient Descent(20/49): loss=2.62498501843882e+30\n",
      "Gradient Descent(21/49): loss=1.4051157024474529e+32\n",
      "Gradient Descent(22/49): loss=7.521376782815993e+33\n",
      "Gradient Descent(23/49): loss=4.0260818814220145e+35\n",
      "Gradient Descent(24/49): loss=2.155102155360404e+37\n",
      "Gradient Descent(25/49): loss=1.1535943472662752e+39\n",
      "Gradient Descent(26/49): loss=6.175020124845105e+40\n",
      "Gradient Descent(27/49): loss=3.305397051624157e+42\n",
      "Gradient Descent(28/49): loss=1.7693302123707115e+44\n",
      "Gradient Descent(29/49): loss=9.470963250450046e+45\n",
      "Gradient Descent(30/49): loss=5.069666717055985e+47\n",
      "Gradient Descent(31/49): loss=2.7137177013968205e+49\n",
      "Gradient Descent(32/49): loss=1.452612996846252e+51\n",
      "Gradient Descent(33/49): loss=7.775622783167584e+52\n",
      "Gradient Descent(34/49): loss=4.162176009534661e+54\n",
      "Gradient Descent(35/49): loss=2.2279513316730898e+56\n",
      "Gradient Descent(36/49): loss=1.1925894351735573e+58\n",
      "Gradient Descent(37/49): loss=6.3837550698180615e+59\n",
      "Gradient Descent(38/49): loss=3.417129784106797e+61\n",
      "Gradient Descent(39/49): loss=1.829139093483842e+63\n",
      "Gradient Descent(40/49): loss=9.79111135571159e+64\n",
      "Gradient Descent(41/49): loss=5.241037268377213e+66\n",
      "Gradient Descent(42/49): loss=2.8054498259276037e+68\n",
      "Gradient Descent(43/49): loss=1.5017158479840754e+70\n",
      "Gradient Descent(44/49): loss=8.038463091532346e+71\n",
      "Gradient Descent(45/49): loss=4.302870543763184e+73\n",
      "Gradient Descent(46/49): loss=2.3032630374191257e+75\n",
      "Gradient Descent(47/49): loss=1.2329026787084063e+77\n",
      "Gradient Descent(48/49): loss=6.599545907138919e+78\n",
      "Gradient Descent(49/49): loss=3.5326394315294146e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2537196965798956\n",
      "Gradient Descent(2/49): loss=10.132605308870216\n",
      "Gradient Descent(3/49): loss=126.65866114938541\n",
      "Gradient Descent(4/49): loss=2375.266331874927\n",
      "Gradient Descent(5/49): loss=79416.37565461257\n",
      "Gradient Descent(6/49): loss=3800131.935032389\n",
      "Gradient Descent(7/49): loss=203096211.10761335\n",
      "Gradient Descent(8/49): loss=11130454013.133493\n",
      "Gradient Descent(9/49): loss=613202619934.1561\n",
      "Gradient Descent(10/49): loss=33819179800798.016\n",
      "Gradient Descent(11/49): loss=1865596911519504.5\n",
      "Gradient Descent(12/49): loss=1.0291818142363128e+17\n",
      "Gradient Descent(13/49): loss=5.677673123574038e+18\n",
      "Gradient Descent(14/49): loss=3.132199984948603e+20\n",
      "Gradient Descent(15/49): loss=1.727940354276204e+22\n",
      "Gradient Descent(16/49): loss=9.532527029559613e+23\n",
      "Gradient Descent(17/49): loss=5.258808430424082e+25\n",
      "Gradient Descent(18/49): loss=2.901126439246739e+27\n",
      "Gradient Descent(19/49): loss=1.600464198966234e+29\n",
      "Gradient Descent(20/49): loss=8.829279613152752e+30\n",
      "Gradient Descent(21/49): loss=4.8708480039578815e+32\n",
      "Gradient Descent(22/49): loss=2.6871003430961895e+34\n",
      "Gradient Descent(23/49): loss=1.4823924392649922e+36\n",
      "Gradient Descent(24/49): loss=8.1779132276781525e+37\n",
      "Gradient Descent(25/49): loss=4.511508760298091e+39\n",
      "Gradient Descent(26/49): loss=2.4888636902333552e+41\n",
      "Gradient Descent(27/49): loss=1.3730312402524286e+43\n",
      "Gradient Descent(28/49): loss=7.574600385336451e+44\n",
      "Gradient Descent(29/49): loss=4.178679211041881e+46\n",
      "Gradient Descent(30/49): loss=2.3052516384358375e+48\n",
      "Gradient Descent(31/49): loss=1.2717379937825488e+50\n",
      "Gradient Descent(32/49): loss=7.015796010571001e+51\n",
      "Gradient Descent(33/49): loss=3.870403644664576e+53\n",
      "Gradient Descent(34/49): loss=2.1351852804815922e+55\n",
      "Gradient Descent(35/49): loss=1.1779174991916012e+57\n",
      "Gradient Descent(36/49): loss=6.498216560339076e+58\n",
      "Gradient Descent(37/49): loss=3.584870629228666e+60\n",
      "Gradient Descent(38/49): loss=1.9776653038531836e+62\n",
      "Gradient Descent(39/49): loss=1.0910184658201008e+64\n",
      "Gradient Descent(40/49): loss=6.018820729884431e+65\n",
      "Gradient Descent(41/49): loss=3.3204023683737847e+67\n",
      "Gradient Descent(42/49): loss=1.8317661187616327e+69\n",
      "Gradient Descent(43/49): loss=1.0105302736205207e+71\n",
      "Gradient Descent(44/49): loss=5.574791581983852e+72\n",
      "Gradient Descent(45/49): loss=3.0754448425588517e+74\n",
      "Gradient Descent(46/49): loss=1.6966304193664387e+76\n",
      "Gradient Descent(47/49): loss=9.359799727458652e+77\n",
      "Gradient Descent(48/49): loss=5.1635199945811246e+79\n",
      "Gradient Descent(49/49): loss=2.8485586776201693e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2624869133011765\n",
      "Gradient Descent(2/49): loss=10.349786042756394\n",
      "Gradient Descent(3/49): loss=128.43187574356577\n",
      "Gradient Descent(4/49): loss=2270.7296580545853\n",
      "Gradient Descent(5/49): loss=69439.14773003508\n",
      "Gradient Descent(6/49): loss=3127130.1548064156\n",
      "Gradient Descent(7/49): loss=160869607.02152434\n",
      "Gradient Descent(8/49): loss=8547656631.818132\n",
      "Gradient Descent(9/49): loss=457403990852.0502\n",
      "Gradient Descent(10/49): loss=24513885111697.184\n",
      "Gradient Descent(11/49): loss=1314209954400051.5\n",
      "Gradient Descent(12/49): loss=7.046074438313333e+16\n",
      "Gradient Descent(13/49): loss=3.7777747004743516e+18\n",
      "Gradient Descent(14/49): loss=2.0254719426068198e+20\n",
      "Gradient Descent(15/49): loss=1.0859671709896757e+22\n",
      "Gradient Descent(16/49): loss=5.8224695072850594e+23\n",
      "Gradient Descent(17/49): loss=3.1217474232264293e+25\n",
      "Gradient Descent(18/49): loss=1.673741189065423e+27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=8.973850834059371e+28\n",
      "Gradient Descent(20/49): loss=4.8113770122419987e+30\n",
      "Gradient Descent(21/49): loss=2.579644924282236e+32\n",
      "Gradient Descent(22/49): loss=1.3830901046685695e+34\n",
      "Gradient Descent(23/49): loss=7.415509861961551e+35\n",
      "Gradient Descent(24/49): loss=3.9758643581690916e+37\n",
      "Gradient Descent(25/49): loss=2.131680449330479e+39\n",
      "Gradient Descent(26/49): loss=1.1429116108354054e+41\n",
      "Gradient Descent(27/49): loss=6.127780318070704e+42\n",
      "Gradient Descent(28/49): loss=3.2854414348882776e+44\n",
      "Gradient Descent(29/49): loss=1.7615065915873002e+46\n",
      "Gradient Descent(30/49): loss=9.444409628659531e+47\n",
      "Gradient Descent(31/49): loss=5.063669568987488e+49\n",
      "Gradient Descent(32/49): loss=2.7149128968403403e+51\n",
      "Gradient Descent(33/49): loss=1.4556147349290128e+53\n",
      "Gradient Descent(34/49): loss=7.804354456485193e+54\n",
      "Gradient Descent(35/49): loss=4.184345419217671e+56\n",
      "Gradient Descent(36/49): loss=2.2434586595152112e+58\n",
      "Gradient Descent(37/49): loss=1.202842082261672e+60\n",
      "Gradient Descent(38/49): loss=6.449100671960611e+61\n",
      "Gradient Descent(39/49): loss=3.457719021509641e+63\n",
      "Gradient Descent(40/49): loss=1.8538741197963878e+65\n",
      "Gradient Descent(41/49): loss=9.939642957311124e+66\n",
      "Gradient Descent(42/49): loss=5.329191505714159e+68\n",
      "Gradient Descent(43/49): loss=2.8572738705555316e+70\n",
      "Gradient Descent(44/49): loss=1.5319423148156205e+72\n",
      "Gradient Descent(45/49): loss=8.213588764126054e+73\n",
      "Gradient Descent(46/49): loss=4.403758531488778e+75\n",
      "Gradient Descent(47/49): loss=2.361098146082267e+77\n",
      "Gradient Descent(48/49): loss=1.2659151076451732e+79\n",
      "Gradient Descent(49/49): loss=6.787269993089452e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2801423317828349\n",
      "Gradient Descent(2/49): loss=10.494582578141543\n",
      "Gradient Descent(3/49): loss=128.2916107823127\n",
      "Gradient Descent(4/49): loss=2190.7156647781535\n",
      "Gradient Descent(5/49): loss=64529.4371301139\n",
      "Gradient Descent(6/49): loss=2859631.2298865234\n",
      "Gradient Descent(7/49): loss=146535237.9175565\n",
      "Gradient Descent(8/49): loss=7781150245.360049\n",
      "Gradient Descent(9/49): loss=416423004378.652\n",
      "Gradient Descent(10/49): loss=22322804498897.574\n",
      "Gradient Descent(11/49): loss=1197060790605470.0\n",
      "Gradient Descent(12/49): loss=6.4197210960266984e+16\n",
      "Gradient Descent(13/49): loss=3.4428888392839235e+18\n",
      "Gradient Descent(14/49): loss=1.8464234176741137e+20\n",
      "Gradient Descent(15/49): loss=9.902387273028652e+21\n",
      "Gradient Descent(16/49): loss=5.310661007695755e+23\n",
      "Gradient Descent(17/49): loss=2.848113334807688e+25\n",
      "Gradient Descent(18/49): loss=1.5274463218400481e+27\n",
      "Gradient Descent(19/49): loss=8.191711477664642e+28\n",
      "Gradient Descent(20/49): loss=4.393223905427716e+30\n",
      "Gradient Descent(21/49): loss=2.356090828770677e+32\n",
      "Gradient Descent(22/49): loss=1.263574111630071e+34\n",
      "Gradient Descent(23/49): loss=6.776561905380467e+35\n",
      "Gradient Descent(24/49): loss=3.6342776284202597e+37\n",
      "Gradient Descent(25/49): loss=1.9490671028845762e+39\n",
      "Gradient Descent(26/49): loss=1.045286838253497e+41\n",
      "Gradient Descent(27/49): loss=5.605884849264319e+42\n",
      "Gradient Descent(28/49): loss=3.0064422312749057e+44\n",
      "Gradient Descent(29/49): loss=1.6123582865208702e+46\n",
      "Gradient Descent(30/49): loss=8.647095284482194e+47\n",
      "Gradient Descent(31/49): loss=4.6374467439404e+49\n",
      "Gradient Descent(32/49): loss=2.4870678066283706e+51\n",
      "Gradient Descent(33/49): loss=1.3338172094050408e+53\n",
      "Gradient Descent(34/49): loss=7.153276413950563e+54\n",
      "Gradient Descent(35/49): loss=3.8363100351063455e+56\n",
      "Gradient Descent(36/49): loss=2.057417305551834e+58\n",
      "Gradient Descent(37/49): loss=1.1033951715184934e+60\n",
      "Gradient Descent(38/49): loss=5.917520481844022e+61\n",
      "Gradient Descent(39/49): loss=3.173572764946308e+63\n",
      "Gradient Descent(40/49): loss=1.7019905761729217e+65\n",
      "Gradient Descent(41/49): loss=9.127794243061812e+66\n",
      "Gradient Descent(42/49): loss=4.895246125922455e+68\n",
      "Gradient Descent(43/49): loss=2.6253258996908654e+70\n",
      "Gradient Descent(44/49): loss=1.4079651772950966e+72\n",
      "Gradient Descent(45/49): loss=7.550932784036578e+73\n",
      "Gradient Descent(46/49): loss=4.049573585234309e+75\n",
      "Gradient Descent(47/49): loss=2.1717907828416556e+77\n",
      "Gradient Descent(48/49): loss=1.1647337936107763e+79\n",
      "Gradient Descent(49/49): loss=6.246480189053487e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.24525672764908\n",
      "Gradient Descent(2/49): loss=9.946177474829662\n",
      "Gradient Descent(3/49): loss=116.92972111841628\n",
      "Gradient Descent(4/49): loss=1808.307876821461\n",
      "Gradient Descent(5/49): loss=47129.59582621096\n",
      "Gradient Descent(6/49): loss=1984806.99714826\n",
      "Gradient Descent(7/49): loss=101471833.99260028\n",
      "Gradient Descent(8/49): loss=5449594116.621978\n",
      "Gradient Descent(9/49): loss=295833496812.1744\n",
      "Gradient Descent(10/49): loss=16095748075582.246\n",
      "Gradient Descent(11/49): loss=876152147373505.1\n",
      "Gradient Descent(12/49): loss=4.769693675473181e+16\n",
      "Gradient Descent(13/49): loss=2.5966314545478733e+18\n",
      "Gradient Descent(14/49): loss=1.413617734087124e+20\n",
      "Gradient Descent(15/49): loss=7.695804741163355e+21\n",
      "Gradient Descent(16/49): loss=4.18963487027709e+23\n",
      "Gradient Descent(17/49): loss=2.280858416942027e+25\n",
      "Gradient Descent(18/49): loss=1.241710869972799e+27\n",
      "Gradient Descent(19/49): loss=6.75993685410916e+28\n",
      "Gradient Descent(20/49): loss=3.680143855738266e+30\n",
      "Gradient Descent(21/49): loss=2.0034889514788778e+32\n",
      "Gradient Descent(22/49): loss=1.0907095309601353e+34\n",
      "Gradient Descent(23/49): loss=5.9378779206785514e+35\n",
      "Gradient Descent(24/49): loss=3.232610809760391e+37\n",
      "Gradient Descent(25/49): loss=1.759849694954133e+39\n",
      "Gradient Descent(26/49): loss=9.580710859096041e+40\n",
      "Gradient Descent(27/49): loss=5.215787508943814e+42\n",
      "Gradient Descent(28/49): loss=2.839501132906588e+44\n",
      "Gradient Descent(29/49): loss=1.5458387961457727e+46\n",
      "Gradient Descent(30/49): loss=8.415624688352915e+47\n",
      "Gradient Descent(31/49): loss=4.581508697530078e+49\n",
      "Gradient Descent(32/49): loss=2.4941965359498506e+51\n",
      "Gradient Descent(33/49): loss=1.3578532249208578e+53\n",
      "Gradient Descent(34/49): loss=7.392221718910702e+54\n",
      "Gradient Descent(35/49): loss=4.024362938396322e+56\n",
      "Gradient Descent(36/49): loss=2.1908835632603952e+58\n",
      "Gradient Descent(37/49): loss=1.1927281065950179e+60\n",
      "Gradient Descent(38/49): loss=6.493272212717179e+61\n",
      "Gradient Descent(39/49): loss=3.534970274894391e+63\n",
      "Gradient Descent(40/49): loss=1.9244557189383172e+65\n",
      "Gradient Descent(41/49): loss=1.047683438940652e+67\n",
      "Gradient Descent(42/49): loss=5.70364169686408e+68\n",
      "Gradient Descent(43/49): loss=3.1050914233311367e+70\n",
      "Gradient Descent(44/49): loss=1.690427495216916e+72\n",
      "Gradient Descent(45/49): loss=9.202772888148293e+73\n",
      "Gradient Descent(46/49): loss=5.010036163660923e+75\n",
      "Gradient Descent(47/49): loss=2.727489058598368e+77\n",
      "Gradient Descent(48/49): loss=1.484858855657002e+79\n",
      "Gradient Descent(49/49): loss=8.083646804273999e+80\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.270890825975541\n",
      "Gradient Descent(2/49): loss=10.512888832242329\n",
      "Gradient Descent(3/49): loss=133.96123274736655\n",
      "Gradient Descent(4/49): loss=2551.8523600655303\n",
      "Gradient Descent(5/49): loss=86522.83008358664\n",
      "Gradient Descent(6/49): loss=4204011.262949104\n",
      "Gradient Descent(7/49): loss=228376137.473544\n",
      "Gradient Descent(8/49): loss=12725797953.305725\n",
      "Gradient Descent(9/49): loss=712910266225.4865\n",
      "Gradient Descent(10/49): loss=39981687649499.65\n",
      "Gradient Descent(11/49): loss=2242771430730152.2\n",
      "Gradient Descent(12/49): loss=1.2581398087811691e+17\n",
      "Gradient Descent(13/49): loss=7.057922529806977e+18\n",
      "Gradient Descent(14/49): loss=3.9593665378986636e+20\n",
      "Gradient Descent(15/49): loss=2.2211336991432304e+22\n",
      "Gradient Descent(16/49): loss=1.246016316359166e+24\n",
      "Gradient Descent(17/49): loss=6.9899290031668105e+25\n",
      "Gradient Descent(18/49): loss=3.9212253478200344e+27\n",
      "Gradient Descent(19/49): loss=2.1997373982052894e+29\n",
      "Gradient Descent(20/49): loss=1.234013399520903e+31\n",
      "Gradient Descent(21/49): loss=6.922594812834822e+32\n",
      "Gradient Descent(22/49): loss=3.8834520728527007e+34\n",
      "Gradient Descent(23/49): loss=2.1785472658605917e+36\n",
      "Gradient Descent(24/49): loss=1.222126113713951e+38\n",
      "Gradient Descent(25/49): loss=6.855909262228678e+39\n",
      "Gradient Descent(26/49): loss=3.846042669775987e+41\n",
      "Gradient Descent(27/49): loss=2.15756125875693e+43\n",
      "Gradient Descent(28/49): loss=1.2103533384770925e+45\n",
      "Gradient Descent(29/49): loss=6.789866095420529e+46\n",
      "Gradient Descent(30/49): loss=3.808993632532923e+48\n",
      "Gradient Descent(31/49): loss=2.1367774104501575e+50\n",
      "Gradient Descent(32/49): loss=1.1986939707152744e+52\n",
      "Gradient Descent(33/49): loss=6.724459124296395e+53\n",
      "Gradient Descent(34/49): loss=3.772301489707989e+55\n",
      "Gradient Descent(35/49): loss=2.1161937735389254e+57\n",
      "Gradient Descent(36/49): loss=1.1871469179712043e+59\n",
      "Gradient Descent(37/49): loss=6.659682220365446e+60\n",
      "Gradient Descent(38/49): loss=3.7359628033272725e+62\n",
      "Gradient Descent(39/49): loss=2.095808419381244e+64\n",
      "Gradient Descent(40/49): loss=1.1757110983110977e+66\n",
      "Gradient Descent(41/49): loss=6.595529314172684e+67\n",
      "Gradient Descent(42/49): loss=3.6999741685351385e+69\n",
      "Gradient Descent(43/49): loss=2.0756194379138025e+71\n",
      "Gradient Descent(44/49): loss=1.1643854402235552e+73\n",
      "Gradient Descent(45/49): loss=6.531994394730217e+74\n",
      "Gradient Descent(46/49): loss=3.6643322132742344e+76\n",
      "Gradient Descent(47/49): loss=2.0556249374727465e+78\n",
      "Gradient Descent(48/49): loss=1.153168882519e+80\n",
      "Gradient Descent(49/49): loss=6.4690715089542155e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2798184047010752\n",
      "Gradient Descent(2/49): loss=10.738155458254074\n",
      "Gradient Descent(3/49): loss=135.85034705121726\n",
      "Gradient Descent(4/49): loss=2440.712390533341\n",
      "Gradient Descent(5/49): loss=75692.68375183549\n",
      "Gradient Descent(6/49): loss=3460626.1273216736\n",
      "Gradient Descent(7/49): loss=180938254.0826245\n",
      "Gradient Descent(8/49): loss=9775156987.765703\n",
      "Gradient Descent(9/49): loss=531916434260.0256\n",
      "Gradient Descent(10/49): loss=28989050558633.387\n",
      "Gradient Descent(11/49): loss=1580402810517764.5\n",
      "Gradient Descent(12/49): loss=8.616524611419246e+16\n",
      "Gradient Descent(13/49): loss=4.697891498526574e+18\n",
      "Gradient Descent(14/49): loss=2.5613871074800645e+20\n",
      "Gradient Descent(15/49): loss=1.3965219045346137e+22\n",
      "Gradient Descent(16/49): loss=7.614131059276906e+23\n",
      "Gradient Descent(17/49): loss=4.151384478377844e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=2.2634221905080114e+27\n",
      "Gradient Descent(19/49): loss=1.2340654175423001e+29\n",
      "Gradient Descent(20/49): loss=6.728384397824135e+30\n",
      "Gradient Descent(21/49): loss=3.6684567903972615e+32\n",
      "Gradient Descent(22/49): loss=2.0001198545593176e+34\n",
      "Gradient Descent(23/49): loss=1.0905074425534572e+36\n",
      "Gradient Descent(24/49): loss=5.945676103131448e+37\n",
      "Gradient Descent(25/49): loss=3.241707754013846e+39\n",
      "Gradient Descent(26/49): loss=1.7674472978605754e+41\n",
      "Gradient Descent(27/49): loss=9.636494674286256e+42\n",
      "Gradient Descent(28/49): loss=5.254019722112879e+44\n",
      "Gradient Descent(29/49): loss=2.864602137332219e+46\n",
      "Gradient Descent(30/49): loss=1.561841378453816e+48\n",
      "Gradient Descent(31/49): loss=8.515487926439393e+49\n",
      "Gradient Descent(32/49): loss=4.642823248614508e+51\n",
      "Gradient Descent(33/49): loss=2.5313649557235283e+53\n",
      "Gradient Descent(34/49): loss=1.3801534531768643e+55\n",
      "Gradient Descent(35/49): loss=7.524887116767578e+56\n",
      "Gradient Descent(36/49): loss=4.1027268373495753e+58\n",
      "Gradient Descent(37/49): loss=2.2368930245346665e+60\n",
      "Gradient Descent(38/49): loss=1.219601158346755e+62\n",
      "Gradient Descent(39/49): loss=6.649522212847867e+63\n",
      "Gradient Descent(40/49): loss=3.6254594673470195e+65\n",
      "Gradient Descent(41/49): loss=1.9766768090465692e+67\n",
      "Gradient Descent(42/49): loss=1.077725800719467e+69\n",
      "Gradient Descent(43/49): loss=5.875987901616658e+70\n",
      "Gradient Descent(44/49): loss=3.20371227977437e+72\n",
      "Gradient Descent(45/49): loss=1.74673136558925e+74\n",
      "Gradient Descent(46/49): loss=9.523547051323374e+75\n",
      "Gradient Descent(47/49): loss=5.19243830079009e+77\n",
      "Gradient Descent(48/49): loss=2.831026650282061e+79\n",
      "Gradient Descent(49/49): loss=1.5435353162285346e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2978533606296088\n",
      "Gradient Descent(2/49): loss=10.888976423049169\n",
      "Gradient Descent(3/49): loss=135.7177238659101\n",
      "Gradient Descent(4/49): loss=2355.3287792630763\n",
      "Gradient Descent(5/49): loss=70355.49577221966\n",
      "Gradient Descent(6/49): loss=3164828.110550714\n",
      "Gradient Descent(7/49): loss=164818503.55430374\n",
      "Gradient Descent(8/49): loss=8898596264.06359\n",
      "Gradient Descent(9/49): loss=484258842907.9333\n",
      "Gradient Descent(10/49): loss=26397908460572.695\n",
      "Gradient Descent(11/49): loss=1439520871228976.5\n",
      "Gradient Descent(12/49): loss=7.850542143205126e+16\n",
      "Gradient Descent(13/49): loss=4.2814254824120325e+18\n",
      "Gradient Descent(14/49): loss=2.3349556078367536e+20\n",
      "Gradient Descent(15/49): loss=1.2734127235119844e+22\n",
      "Gradient Descent(16/49): loss=6.94480105378254e+23\n",
      "Gradient Descent(17/49): loss=3.7874808676389628e+25\n",
      "Gradient Descent(18/49): loss=2.06557557404464e+27\n",
      "Gradient Descent(19/49): loss=1.1265013890617759e+29\n",
      "Gradient Descent(20/49): loss=6.143592110136408e+30\n",
      "Gradient Descent(21/49): loss=3.350526185298153e+32\n",
      "Gradient Descent(22/49): loss=1.827273933112626e+34\n",
      "Gradient Descent(23/49): loss=9.965390037224246e+35\n",
      "Gradient Descent(24/49): loss=5.434817232077907e+37\n",
      "Gradient Descent(25/49): loss=2.9639821658523475e+39\n",
      "Gradient Descent(26/49): loss=1.6164647134108162e+41\n",
      "Gradient Descent(27/49): loss=8.81570138918456e+42\n",
      "Gradient Descent(28/49): loss=4.807812403110604e+44\n",
      "Gradient Descent(29/49): loss=2.6220330162116077e+46\n",
      "Gradient Descent(30/49): loss=1.4299761641397726e+48\n",
      "Gradient Descent(31/49): loss=7.798650197633146e+49\n",
      "Gradient Descent(32/49): loss=4.253143963530921e+51\n",
      "Gradient Descent(33/49): loss=2.319533908574277e+53\n",
      "Gradient Descent(34/49): loss=1.2650024544570694e+55\n",
      "Gradient Descent(35/49): loss=6.898934324120366e+56\n",
      "Gradient Descent(36/49): loss=3.7624665976599515e+58\n",
      "Gradient Descent(37/49): loss=2.0519335644366673e+60\n",
      "Gradient Descent(38/49): loss=1.1190614570453648e+62\n",
      "Gradient Descent(39/49): loss=6.1030170096578405e+63\n",
      "Gradient Descent(40/49): loss=3.3283977734801635e+65\n",
      "Gradient Descent(41/49): loss=1.8152057778926437e+67\n",
      "Gradient Descent(42/49): loss=9.89957403032885e+68\n",
      "Gradient Descent(43/49): loss=5.398923206146501e+70\n",
      "Gradient Descent(44/49): loss=2.9444066680614322e+72\n",
      "Gradient Descent(45/49): loss=1.6057888389770684e+74\n",
      "Gradient Descent(46/49): loss=8.757478453481108e+75\n",
      "Gradient Descent(47/49): loss=4.776059404674906e+77\n",
      "Gradient Descent(48/49): loss=2.6047159074557984e+79\n",
      "Gradient Descent(49/49): loss=1.4205319456270542e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2622550805812651\n",
      "Gradient Descent(2/49): loss=10.31978541136031\n",
      "Gradient Descent(3/49): loss=123.7320999908772\n",
      "Gradient Descent(4/49): loss=1945.750896679572\n",
      "Gradient Descent(5/49): loss=51422.11327004569\n",
      "Gradient Descent(6/49): loss=2197098.4543231013\n",
      "Gradient Descent(7/49): loss=114129405.88565408\n",
      "Gradient Descent(8/49): loss=6231572147.459653\n",
      "Gradient Descent(9/49): loss=343980140551.6251\n",
      "Gradient Descent(10/49): loss=19031246100087.684\n",
      "Gradient Descent(11/49): loss=1053439928462322.4\n",
      "Gradient Descent(12/49): loss=5.831709580347807e+16\n",
      "Gradient Descent(13/49): loss=3.2284278902971443e+18\n",
      "Gradient Descent(14/49): loss=1.7872618684100153e+20\n",
      "Gradient Descent(15/49): loss=9.894313905841288e+21\n",
      "Gradient Descent(16/49): loss=5.477511046904241e+23\n",
      "Gradient Descent(17/49): loss=3.032360681560983e+25\n",
      "Gradient Descent(18/49): loss=1.678720736561012e+27\n",
      "Gradient Descent(19/49): loss=9.2934304727092e+28\n",
      "Gradient Descent(20/49): loss=5.144861089827714e+30\n",
      "Gradient Descent(21/49): loss=2.8482050533807964e+32\n",
      "Gradient Descent(22/49): loss=1.576771828158874e+34\n",
      "Gradient Descent(23/49): loss=8.729039347544445e+35\n",
      "Gradient Descent(24/49): loss=4.832413071458978e+37\n",
      "Gradient Descent(25/49): loss=2.6752332259539834e+39\n",
      "Gradient Descent(26/49): loss=1.48101428984165e+41\n",
      "Gradient Descent(27/49): loss=8.198923762742423e+42\n",
      "Gradient Descent(28/49): loss=4.538940058063159e+44\n",
      "Gradient Descent(29/49): loss=2.5127659979362464e+46\n",
      "Gradient Descent(30/49): loss=1.3910721180747767e+48\n",
      "Gradient Descent(31/49): loss=7.701002159669339e+49\n",
      "Gradient Descent(32/49): loss=4.2632896952394175e+51\n",
      "Gradient Descent(33/49): loss=2.3601654237576492e+53\n",
      "Gradient Descent(34/49): loss=1.306592144962944e+55\n",
      "Gradient Descent(35/49): loss=7.233319394031461e+56\n",
      "Gradient Descent(36/49): loss=4.004379611325144e+58\n",
      "Gradient Descent(37/49): loss=2.2168323003720134e+60\n",
      "Gradient Descent(38/49): loss=1.2272426505404328e+62\n",
      "Gradient Descent(39/49): loss=6.7940390576803e+63\n",
      "Gradient Descent(40/49): loss=3.7611931672158337e+65\n",
      "Gradient Descent(41/49): loss=2.0822038144039707e+67\n",
      "Gradient Descent(42/49): loss=1.1527120602337083e+69\n",
      "Gradient Descent(43/49): loss=6.381436267748851e+70\n",
      "Gradient Descent(44/49): loss=3.53277546441949e+72\n",
      "Gradient Descent(45/49): loss=1.95575133220077e+74\n",
      "Gradient Descent(46/49): loss=1.082707721429934e+76\n",
      "Gradient Descent(47/49): loss=5.9938908937102e+77\n",
      "Gradient Descent(48/49): loss=3.318229595541671e+79\n",
      "Gradient Descent(49/49): loss=1.8369783240937105e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2882323674378344\n",
      "Gradient Descent(2/49): loss=10.904195237271425\n",
      "Gradient Descent(3/49): loss=141.61324570024408\n",
      "Gradient Descent(4/49): loss=2739.876194261104\n",
      "Gradient Descent(5/49): loss=94198.2345207781\n",
      "Gradient Descent(6/49): loss=4646928.619241946\n",
      "Gradient Descent(7/49): loss=256553038.6236716\n",
      "Gradient Descent(8/49): loss=14533642193.564777\n",
      "Gradient Descent(9/49): loss=827794788288.6649\n",
      "Gradient Descent(10/49): loss=47201497603728.4\n",
      "Gradient Descent(11/49): loss=2692083202407846.5\n",
      "Gradient Descent(12/49): loss=1.535471024605418e+17\n",
      "Gradient Descent(13/49): loss=8.7578795753553e+18\n",
      "Gradient Descent(14/49): loss=4.9952493751863345e+20\n",
      "Gradient Descent(15/49): loss=2.8491515852849347e+22\n",
      "Gradient Descent(16/49): loss=1.6250771131274197e+24\n",
      "Gradient Descent(17/49): loss=9.268989696864904e+25\n",
      "Gradient Descent(18/49): loss=5.286774984942301e+27\n",
      "Gradient Descent(19/49): loss=3.015430017259132e+29\n",
      "Gradient Descent(20/49): loss=1.7199177601151385e+31\n",
      "Gradient Descent(21/49): loss=9.809934519184004e+32\n",
      "Gradient Descent(22/49): loss=5.5953149332464755e+34\n",
      "Gradient Descent(23/49): loss=3.191412658364523e+36\n",
      "Gradient Descent(24/49): loss=1.82029338428366e+38\n",
      "Gradient Descent(25/49): loss=1.0382449277383832e+40\n",
      "Gradient Descent(26/49): loss=5.921861493765174e+41\n",
      "Gradient Descent(27/49): loss=3.3776657717680927e+43\n",
      "Gradient Descent(28/49): loss=1.9265270013129714e+45\n",
      "Gradient Descent(29/49): loss=1.0988376404232166e+47\n",
      "Gradient Descent(30/49): loss=6.267465543893069e+48\n",
      "Gradient Descent(31/49): loss=3.5747887493877408e+50\n",
      "Gradient Descent(32/49): loss=2.0389604878164741e+52\n",
      "Gradient Descent(33/49): loss=1.1629665869315528e+54\n",
      "Gradient Descent(34/49): loss=6.63323929228087e+55\n",
      "Gradient Descent(35/49): loss=3.7834159642325674e+57\n",
      "Gradient Descent(36/49): loss=2.1579556725877344e+59\n",
      "Gradient Descent(37/49): loss=1.2308381443852639e+61\n",
      "Gradient Descent(38/49): loss=7.020359856869147e+62\n",
      "Gradient Descent(39/49): loss=4.004218811772002e+64\n",
      "Gradient Descent(40/49): loss=2.283895501006242e+66\n",
      "Gradient Descent(41/49): loss=1.302670734222e+68\n",
      "Gradient Descent(42/49): loss=7.430073053039551e+69\n",
      "Gradient Descent(43/49): loss=4.237907870592924e+71\n",
      "Gradient Descent(44/49): loss=2.4171852674162777e+73\n",
      "Gradient Descent(45/49): loss=1.3786955251098621e+75\n",
      "Gradient Descent(46/49): loss=7.863697402846099e+76\n",
      "Gradient Descent(47/49): loss=4.485235189154737e+78\n",
      "Gradient Descent(48/49): loss=2.5582539194285767e+80\n",
      "Gradient Descent(49/49): loss=1.4591571768848605e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.29732171139789\n",
      "Gradient Descent(2/49): loss=11.13776898141518\n",
      "Gradient Descent(3/49): loss=143.62430285328807\n",
      "Gradient Descent(4/49): loss=2621.777312157259\n",
      "Gradient Descent(5/49): loss=82450.21616644459\n",
      "Gradient Descent(6/49): loss=3826473.1969644777\n",
      "Gradient Descent(7/49): loss=203311916.58266783\n",
      "Gradient Descent(8/49): loss=11166474393.348253\n",
      "Gradient Descent(9/49): loss=617791580375.0789\n",
      "Gradient Descent(10/49): loss=34233428943259.027\n",
      "Gradient Descent(11/49): loss=1897600782907273.5\n",
      "Gradient Descent(12/49): loss=1.0519388863519686e+17\n",
      "Gradient Descent(13/49): loss=5.8315343344198e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=3.232783263102737e+20\n",
      "Gradient Descent(15/49): loss=1.792134685649345e+22\n",
      "Gradient Descent(16/49): loss=9.934928979373889e+23\n",
      "Gradient Descent(17/49): loss=5.5075557531183675e+25\n",
      "Gradient Descent(18/49): loss=3.053184440877868e+27\n",
      "Gradient Descent(19/49): loss=1.692572106665392e+29\n",
      "Gradient Descent(20/49): loss=9.382991406611366e+30\n",
      "Gradient Descent(21/49): loss=5.201582100739209e+32\n",
      "Gradient Descent(22/49): loss=2.8835640126507023e+34\n",
      "Gradient Descent(23/49): loss=1.5985408389301388e+36\n",
      "Gradient Descent(24/49): loss=8.861716967330811e+37\n",
      "Gradient Descent(25/49): loss=4.9126069035338076e+39\n",
      "Gradient Descent(26/49): loss=2.7233668912716983e+41\n",
      "Gradient Descent(27/49): loss=1.5097335020108404e+43\n",
      "Gradient Descent(28/49): loss=8.369402060364998e+44\n",
      "Gradient Descent(29/49): loss=4.639685795853861e+46\n",
      "Gradient Descent(30/49): loss=2.5720695611209187e+48\n",
      "Gradient Descent(31/49): loss=1.4258598789505336e+50\n",
      "Gradient Descent(32/49): loss=7.904437831435931e+51\n",
      "Gradient Descent(33/49): loss=4.3819268887080336e+53\n",
      "Gradient Descent(34/49): loss=2.4291775920634184e+55\n",
      "Gradient Descent(35/49): loss=1.3466458760390053e+57\n",
      "Gradient Descent(36/49): loss=7.465304806769939e+58\n",
      "Gradient Descent(37/49): loss=4.1384878422459696e+60\n",
      "Gradient Descent(38/49): loss=2.294224022157337e+62\n",
      "Gradient Descent(39/49): loss=1.2718326269111985e+64\n",
      "Gradient Descent(40/49): loss=7.050567927341273e+65\n",
      "Gradient Descent(41/49): loss=3.90857311301891e+67\n",
      "Gradient Descent(42/49): loss=2.166767831648315e+69\n",
      "Gradient Descent(43/49): loss=1.201175646587734e+71\n",
      "Gradient Descent(44/49): loss=6.658871859186915e+72\n",
      "Gradient Descent(45/49): loss=3.6914313541930455e+74\n",
      "Gradient Descent(46/49): loss=2.046392501744844e+76\n",
      "Gradient Descent(47/49): loss=1.1344440325135107e+78\n",
      "Gradient Descent(48/49): loss=6.288936564262225e+79\n",
      "Gradient Descent(49/49): loss=3.4863529601971187e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3157398629581396\n",
      "Gradient Descent(2/49): loss=11.294798036607919\n",
      "Gradient Descent(3/49): loss=143.50038147272556\n",
      "Gradient Descent(4/49): loss=2530.713669485476\n",
      "Gradient Descent(5/49): loss=76652.36990128053\n",
      "Gradient Descent(6/49): loss=3499655.90209012\n",
      "Gradient Descent(7/49): loss=185202099.70030203\n",
      "Gradient Descent(8/49): loss=10165175557.961012\n",
      "Gradient Descent(9/49): loss=562439150570.7178\n",
      "Gradient Descent(10/49): loss=31173449827537.15\n",
      "Gradient Descent(11/49): loss=1728437835689957.5\n",
      "Gradient Descent(12/49): loss=9.584216877620125e+16\n",
      "Gradient Descent(13/49): loss=5.314552851150458e+18\n",
      "Gradient Descent(14/49): loss=2.946987993406791e+20\n",
      "Gradient Descent(15/49): loss=1.6341440237171819e+22\n",
      "Gradient Descent(16/49): loss=9.061547331151136e+23\n",
      "Gradient Descent(17/49): loss=5.024749449633745e+25\n",
      "Gradient Descent(18/49): loss=2.7862909382184433e+27\n",
      "Gradient Descent(19/49): loss=1.5450356843099319e+29\n",
      "Gradient Descent(20/49): loss=8.567430033357439e+30\n",
      "Gradient Descent(21/49): loss=4.750754828966857e+32\n",
      "Gradient Descent(22/49): loss=2.6343572526898405e+34\n",
      "Gradient Descent(23/49): loss=1.4607864191406016e+36\n",
      "Gradient Descent(24/49): loss=8.10025656227258e+37\n",
      "Gradient Descent(25/49): loss=4.491700875289413e+39\n",
      "Gradient Descent(26/49): loss=2.4907083618863462e+41\n",
      "Gradient Descent(27/49): loss=1.3811311830890301e+43\n",
      "Gradient Descent(28/49): loss=7.658557597872528e+44\n",
      "Gradient Descent(29/49): loss=4.246772877051818e+46\n",
      "Gradient Descent(30/49): loss=2.3548925027700955e+48\n",
      "Gradient Descent(31/49): loss=1.3058194681352332e+50\n",
      "Gradient Descent(32/49): loss=7.240944040355092e+51\n",
      "Gradient Descent(33/49): loss=4.015200559877345e+53\n",
      "Gradient Descent(34/49): loss=2.226482547881749e+55\n",
      "Gradient Descent(35/49): loss=1.2346144263771136e+57\n",
      "Gradient Descent(36/49): loss=6.846102536346897e+58\n",
      "Gradient Descent(37/49): loss=3.796255651710527e+60\n",
      "Gradient Descent(38/49): loss=2.1050746606016255e+62\n",
      "Gradient Descent(39/49): loss=1.167292124994316e+64\n",
      "Gradient Descent(40/49): loss=6.4727913483332945e+65\n",
      "Gradient Descent(41/49): loss=3.58924959244981e+67\n",
      "Gradient Descent(42/49): loss=1.9902870251207268e+69\n",
      "Gradient Descent(43/49): loss=1.1036408420012294e+71\n",
      "Gradient Descent(44/49): loss=6.11983644951557e+72\n",
      "Gradient Descent(45/49): loss=3.393531368493603e+74\n",
      "Gradient Descent(46/49): loss=1.8817586456680712e+76\n",
      "Gradient Descent(47/49): loss=1.0434604004024204e+78\n",
      "Gradient Descent(48/49): loss=5.786127831613572e+79\n",
      "Gradient Descent(49/49): loss=3.208485465367227e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2794222266417936\n",
      "Gradient Descent(2/49): loss=10.70423747685688\n",
      "Gradient Descent(3/49): loss=130.86264452682514\n",
      "Gradient Descent(4/49): loss=2092.2873407766306\n",
      "Gradient Descent(5/49): loss=56064.76599383394\n",
      "Gradient Descent(6/49): loss=2430053.489167642\n",
      "Gradient Descent(7/49): loss=128240839.71343833\n",
      "Gradient Descent(8/49): loss=7117830609.607179\n",
      "Gradient Descent(9/49): loss=399462030660.0564\n",
      "Gradient Descent(10/49): loss=22470821478396.754\n",
      "Gradient Descent(11/49): loss=1264664259503879.2\n",
      "Gradient Descent(12/49): loss=7.118296524651015e+16\n",
      "Gradient Descent(13/49): loss=4.0066944080358707e+18\n",
      "Gradient Descent(14/49): loss=2.25526880361793e+20\n",
      "Gradient Descent(15/49): loss=1.2694360062864008e+22\n",
      "Gradient Descent(16/49): loss=7.145348206858102e+23\n",
      "Gradient Descent(17/49): loss=4.021943826951835e+25\n",
      "Gradient Descent(18/49): loss=2.2638550029546423e+27\n",
      "Gradient Descent(19/49): loss=1.2742692847312767e+29\n",
      "Gradient Descent(20/49): loss=7.17255393344378e+30\n",
      "Gradient Descent(21/49): loss=4.037257316693971e+32\n",
      "Gradient Descent(22/49): loss=2.272474601475803e+34\n",
      "Gradient Descent(23/49): loss=1.2791210490886454e+36\n",
      "Gradient Descent(24/49): loss=7.19986334351206e+37\n",
      "Gradient Descent(25/49): loss=4.0526291238965063e+39\n",
      "Gradient Descent(26/49): loss=2.281127020369726e+41\n",
      "Gradient Descent(27/49): loss=1.283991286638598e+43\n",
      "Gradient Descent(28/49): loss=7.227276734009235e+44\n",
      "Gradient Descent(29/49): loss=4.0680594590868757e+46\n",
      "Gradient Descent(30/49): loss=2.289812383244095e+48\n",
      "Gradient Descent(31/49): loss=1.2888800675580338e+50\n",
      "Gradient Descent(32/49): loss=7.254794500651965e+51\n",
      "Gradient Descent(33/49): loss=4.083548545087531e+53\n",
      "Gradient Descent(34/49): loss=2.2985308155300716e+55\n",
      "Gradient Descent(35/49): loss=1.29378746245026e+57\n",
      "Gradient Descent(36/49): loss=7.282417040850002e+58\n",
      "Gradient Descent(37/49): loss=4.099096605591095e+60\n",
      "Gradient Descent(38/49): loss=2.3072824431389075e+62\n",
      "Gradient Descent(39/49): loss=1.2987135421877633e+64\n",
      "Gradient Descent(40/49): loss=7.310144753527615e+65\n",
      "Gradient Descent(41/49): loss=4.114703865142344e+67\n",
      "Gradient Descent(42/49): loss=2.3160673924613727e+69\n",
      "Gradient Descent(43/49): loss=1.3036583779127678e+71\n",
      "Gradient Descent(44/49): loss=7.337978039127658e+72\n",
      "Gradient Descent(45/49): loss=4.1303705491411456e+74\n",
      "Gradient Descent(46/49): loss=2.324885790369661e+76\n",
      "Gradient Descent(47/49): loss=1.3086220410386031e+78\n",
      "Gradient Descent(48/49): loss=7.365917299617919e+79\n",
      "Gradient Descent(49/49): loss=4.146096883845154e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3057443209667765\n",
      "Gradient Descent(2/49): loss=11.3067357840883\n",
      "Gradient Descent(3/49): loss=149.62798254114725\n",
      "Gradient Descent(4/49): loss=2939.9699705973226\n",
      "Gradient Descent(5/49): loss=102482.85004546691\n",
      "Gradient Descent(6/49): loss=5132287.885059691\n",
      "Gradient Descent(7/49): loss=287930774.5171338\n",
      "Gradient Descent(8/49): loss=16580176350.494415\n",
      "Gradient Descent(9/49): loss=960011419061.2585\n",
      "Gradient Descent(10/49): loss=55648949835987.72\n",
      "Gradient Descent(11/49): loss=3226555431465396.5\n",
      "Gradient Descent(12/49): loss=1.8708638729456534e+17\n",
      "Gradient Descent(13/49): loss=1.0847996747924175e+19\n",
      "Gradient Descent(14/49): loss=6.290103454294042e+20\n",
      "Gradient Descent(15/49): loss=3.647255706269072e+22\n",
      "Gradient Descent(16/49): loss=2.1148261598395063e+24\n",
      "Gradient Descent(17/49): loss=1.2262616404808209e+26\n",
      "Gradient Descent(18/49): loss=7.11036039251567e+27\n",
      "Gradient Descent(19/49): loss=4.122874212191237e+29\n",
      "Gradient Descent(20/49): loss=2.39060903158531e+31\n",
      "Gradient Descent(21/49): loss=1.3861716967205688e+33\n",
      "Gradient Descent(22/49): loss=8.037583508698847e+34\n",
      "Gradient Descent(23/49): loss=4.66051563541769e+36\n",
      "Gradient Descent(24/49): loss=2.702355249494006e+38\n",
      "Gradient Descent(25/49): loss=1.566934748372256e+40\n",
      "Gradient Descent(26/49): loss=9.085720710169866e+41\n",
      "Gradient Descent(27/49): loss=5.268267929405743e+43\n",
      "Gradient Descent(28/49): loss=3.0547545826429986e+45\n",
      "Gradient Descent(29/49): loss=1.7712701186081602e+47\n",
      "Gradient Descent(30/49): loss=1.0270539737957408e+49\n",
      "Gradient Descent(31/49): loss=5.955273868214265e+50\n",
      "Gradient Descent(32/49): loss=3.4531083808930496e+52\n",
      "Gradient Descent(33/49): loss=2.0022517442627002e+54\n",
      "Gradient Descent(34/49): loss=1.1609864519706226e+56\n",
      "Gradient Descent(35/49): loss=6.73186848517732e+57\n",
      "Gradient Descent(36/49): loss=3.903409314105384e+59\n",
      "Gradient Descent(37/49): loss=2.263354417424139e+61\n",
      "Gradient Descent(38/49): loss=1.3123843303754292e+63\n",
      "Gradient Descent(39/49): loss=7.609734548666457e+64\n",
      "Gradient Descent(40/49): loss=4.412431523363509e+66\n",
      "Gradient Descent(41/49): loss=2.5585060587670497e+68\n",
      "Gradient Descent(42/49): loss=1.4835251761047186e+70\n",
      "Gradient Descent(43/49): loss=8.602078312830324e+71\n",
      "Gradient Descent(44/49): loss=4.9878325283535284e+73\n",
      "Gradient Descent(45/49): loss=2.8921468075679664e+75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=1.6769835612918198e+77\n",
      "Gradient Descent(47/49): loss=9.723828187020262e+78\n",
      "Gradient Descent(48/49): loss=5.638268423922757e+80\n",
      "Gradient Descent(49/49): loss=3.2692958173241864e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3149968333916198\n",
      "Gradient Descent(2/49): loss=11.548841842297017\n",
      "Gradient Descent(3/49): loss=151.76726942409542\n",
      "Gradient Descent(4/49): loss=2814.5406568923527\n",
      "Gradient Descent(5/49): loss=89747.65452500245\n",
      "Gradient Descent(6/49): loss=4227502.845464045\n",
      "Gradient Descent(7/49): loss=228232925.40690374\n",
      "Gradient Descent(8/49): loss=12741837218.901693\n",
      "Gradient Descent(9/49): loss=716645972071.5559\n",
      "Gradient Descent(10/49): loss=40371172328229.11\n",
      "Gradient Descent(11/49): loss=2275028847206103.5\n",
      "Gradient Descent(12/49): loss=1.2821367215387318e+17\n",
      "Gradient Descent(13/49): loss=7.22584427867488e+18\n",
      "Gradient Descent(14/49): loss=4.0723427109344405e+20\n",
      "Gradient Descent(15/49): loss=2.2950933443248887e+22\n",
      "Gradient Descent(16/49): loss=1.2934702814146526e+24\n",
      "Gradient Descent(17/49): loss=7.289748926102226e+25\n",
      "Gradient Descent(18/49): loss=4.108361865388549e+27\n",
      "Gradient Descent(19/49): loss=2.3153934948467793e+29\n",
      "Gradient Descent(20/49): loss=1.304911108946769e+31\n",
      "Gradient Descent(21/49): loss=7.354227288626665e+32\n",
      "Gradient Descent(22/49): loss=4.144700634607763e+34\n",
      "Gradient Descent(23/49): loss=2.3358733251473158e+36\n",
      "Gradient Descent(24/49): loss=1.316453146357271e+38\n",
      "Gradient Descent(25/49): loss=7.419275985117434e+39\n",
      "Gradient Descent(26/49): loss=4.1813608251580644e+41\n",
      "Gradient Descent(27/49): loss=2.356534301357418e+43\n",
      "Gradient Descent(28/49): loss=1.3280972739931166e+45\n",
      "Gradient Descent(29/49): loss=7.484900042286412e+46\n",
      "Gradient Descent(30/49): loss=4.2183452778707714e+48\n",
      "Gradient Descent(31/49): loss=2.377378025465059e+50\n",
      "Gradient Descent(32/49): loss=1.3398443948185174e+52\n",
      "Gradient Descent(33/49): loss=7.551104549203595e+53\n",
      "Gradient Descent(34/49): loss=4.2556568608645514e+55\n",
      "Gradient Descent(35/49): loss=2.3984061138887754e+57\n",
      "Gradient Descent(36/49): loss=1.3516954198159761e+59\n",
      "Gradient Descent(37/49): loss=7.617894639991023e+60\n",
      "Gradient Descent(38/49): loss=4.29329846763153e+62\n",
      "Gradient Descent(39/49): loss=2.41962019734484e+64\n",
      "Gradient Descent(40/49): loss=1.3636512680257978e+66\n",
      "Gradient Descent(41/49): loss=7.68527549418284e+67\n",
      "Gradient Descent(42/49): loss=4.331273017257222e+69\n",
      "Gradient Descent(43/49): loss=2.441021920973389e+71\n",
      "Gradient Descent(44/49): loss=1.3757128666171838e+73\n",
      "Gradient Descent(45/49): loss=7.753252337125278e+74\n",
      "Gradient Descent(46/49): loss=4.369583454645787e+76\n",
      "Gradient Descent(47/49): loss=2.4626129444657608e+78\n",
      "Gradient Descent(48/49): loss=1.3878811509602036e+80\n",
      "Gradient Descent(49/49): loss=7.8218304403841e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3338018387684278\n",
      "Gradient Descent(2/49): loss=11.71226633233069\n",
      "Gradient Descent(3/49): loss=151.65317453307563\n",
      "Gradient Descent(4/49): loss=2717.4714528188974\n",
      "Gradient Descent(5/49): loss=83453.70153315784\n",
      "Gradient Descent(6/49): loss=3866710.462678003\n",
      "Gradient Descent(7/49): loss=207906884.03844744\n",
      "Gradient Descent(8/49): loss=11599303881.56955\n",
      "Gradient Descent(9/49): loss=652435597811.8192\n",
      "Gradient Descent(10/49): loss=36762478457867.305\n",
      "Gradient Descent(11/49): loss=2072213905678166.0\n",
      "Gradient Descent(12/49): loss=1.1681512858889875e+17\n",
      "Gradient Descent(13/49): loss=6.585230821235725e+18\n",
      "Gradient Descent(14/49): loss=3.7123121946961864e+20\n",
      "Gradient Descent(15/49): loss=2.09275466451671e+22\n",
      "Gradient Descent(16/49): loss=1.179756060525292e+24\n",
      "Gradient Descent(17/49): loss=6.650681205237209e+25\n",
      "Gradient Descent(18/49): loss=3.7492124267469337e+27\n",
      "Gradient Descent(19/49): loss=2.1135570041101334e+29\n",
      "Gradient Descent(20/49): loss=1.1914830910309263e+31\n",
      "Gradient Descent(21/49): loss=6.716790479535501e+32\n",
      "Gradient Descent(22/49): loss=3.7864804533353263e+34\n",
      "Gradient Descent(23/49): loss=2.1345662436875653e+36\n",
      "Gradient Descent(24/49): loss=1.2033267053260722e+38\n",
      "Gradient Descent(25/49): loss=6.783556912479122e+39\n",
      "Gradient Descent(26/49): loss=3.824118934714011e+41\n",
      "Gradient Descent(27/49): loss=2.1557843201604144e+43\n",
      "Gradient Descent(28/49): loss=1.2152880478852024e+45\n",
      "Gradient Descent(29/49): loss=6.85098701906661e+46\n",
      "Gradient Descent(30/49): loss=3.8621315512067877e+48\n",
      "Gradient Descent(31/49): loss=2.1772133091647858e+50\n",
      "Gradient Descent(32/49): loss=1.2273682889240602e+52\n",
      "Gradient Descent(33/49): loss=6.919087396329957e+53\n",
      "Gradient Descent(34/49): loss=3.9005220217982075e+55\n",
      "Gradient Descent(35/49): loss=2.1988553072190798e+57\n",
      "Gradient Descent(36/49): loss=1.2395686103206377e+59\n",
      "Gradient Descent(37/49): loss=6.987864706912179e+60\n",
      "Gradient Descent(38/49): loss=3.93929410244405e+62\n",
      "Gradient Descent(39/49): loss=2.2207124316817893e+64\n",
      "Gradient Descent(40/49): loss=1.2518902057011615e+66\n",
      "Gradient Descent(41/49): loss=7.057325679685657e+67\n",
      "Gradient Descent(42/49): loss=3.9784515864357764e+69\n",
      "Gradient Descent(43/49): loss=2.2427868209588203e+71\n",
      "Gradient Descent(44/49): loss=1.2643342805568654e+73\n",
      "Gradient Descent(45/49): loss=7.127477110409657e+74\n",
      "Gradient Descent(46/49): loss=4.0179983047708996e+76\n",
      "Gradient Descent(47/49): loss=2.265080634712064e+78\n",
      "Gradient Descent(48/49): loss=1.2769020523616115e+80\n",
      "Gradient Descent(49/49): loss=7.198325862392822e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.2967581658306662\n",
      "Gradient Descent(2/49): loss=11.099741741600525\n",
      "Gradient Descent(3/49): loss=138.3339181206309\n",
      "Gradient Descent(4/49): loss=2248.430010924499\n",
      "Gradient Descent(5/49): loss=61082.82004162133\n",
      "Gradient Descent(6/49): loss=2685487.7260274724\n",
      "Gradient Descent(7/49): loss=143959010.8899837\n",
      "Gradient Descent(8/49): loss=8121236932.268773\n",
      "Gradient Descent(9/49): loss=463321778413.72595\n",
      "Gradient Descent(10/49): loss=26495754770722.152\n",
      "Gradient Descent(11/49): loss=1515957033506901.5\n",
      "Gradient Descent(12/49): loss=8.67447099610206e+16\n",
      "Gradient Descent(13/49): loss=4.963735673082506e+18\n",
      "Gradient Descent(14/49): loss=2.8403790039325904e+20\n",
      "Gradient Descent(15/49): loss=1.6253405105470032e+22\n",
      "Gradient Descent(16/49): loss=9.300633141019192e+23\n",
      "Gradient Descent(17/49): loss=5.3220712786616314e+25\n",
      "Gradient Descent(18/49): loss=3.0454316945392564e+27\n",
      "Gradient Descent(19/49): loss=1.7426775662648503e+29\n",
      "Gradient Descent(20/49): loss=9.972067692716372e+30\n",
      "Gradient Descent(21/49): loss=5.706284168757489e+32\n",
      "Gradient Descent(22/49): loss=3.265288605988229e+34\n",
      "Gradient Descent(23/49): loss=1.868485579251579e+36\n",
      "Gradient Descent(24/49): loss=1.0691974833315533e+38\n",
      "Gradient Descent(25/49): loss=6.118234312627613e+39\n",
      "Gradient Descent(26/49): loss=3.501017509653779e+41\n",
      "Gradient Descent(27/49): loss=2.0033759703522148e+43\n",
      "Gradient Descent(28/49): loss=1.146385377256115e+45\n",
      "Gradient Descent(29/49): loss=6.559924111276738e+46\n",
      "Gradient Descent(30/49): loss=3.7537642401467656e+48\n",
      "Gradient Descent(31/49): loss=2.1480044176703306e+50\n",
      "Gradient Descent(32/49): loss=1.2291456477167496e+52\n",
      "Gradient Descent(33/49): loss=7.033500540653473e+53\n",
      "Gradient Descent(34/49): loss=4.0247573546120447e+55\n",
      "Gradient Descent(35/49): loss=2.3030739345047e+57\n",
      "Gradient Descent(36/49): loss=1.3178805777488275e+59\n",
      "Gradient Descent(37/49): loss=7.541265572010835e+60\n",
      "Gradient Descent(38/49): loss=4.315314102643632e+62\n",
      "Gradient Descent(39/49): loss=2.4693382863467158e+64\n",
      "Gradient Descent(40/49): loss=1.4130214921509911e+66\n",
      "Gradient Descent(41/49): loss=8.085687361347959e+67\n",
      "Gradient Descent(42/49): loss=4.6268468291970276e+69\n",
      "Gradient Descent(43/49): loss=2.64760565480009e+71\n",
      "Gradient Descent(44/49): loss=1.5150308540786375e+73\n",
      "Gradient Descent(45/49): loss=8.669412246679989e+74\n",
      "Gradient Descent(46/49): loss=4.9608698397492904e+76\n",
      "Gradient Descent(47/49): loss=2.8387425660095457e+78\n",
      "Gradient Descent(48/49): loss=1.6244045129959517e+80\n",
      "Gradient Descent(49/49): loss=9.295277611421105e+81\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3234266865623665\n",
      "Gradient Descent(2/49): loss=11.720723743465276\n",
      "Gradient Descent(3/49): loss=158.01910175457385\n",
      "Gradient Descent(4/49): loss=3152.7948034562455\n",
      "Gradient Descent(5/49): loss=111419.41716475971\n",
      "Gradient Descent(6/49): loss=5663758.0479001785\n",
      "Gradient Descent(7/49): loss=322842177.11585295\n",
      "Gradient Descent(8/49): loss=18894561545.481907\n",
      "Gradient Descent(9/49): loss=1111999835952.0723\n",
      "Gradient Descent(10/49): loss=65520076776171.62\n",
      "Gradient Descent(11/49): loss=3861425251927740.0\n",
      "Gradient Descent(12/49): loss=2.275842602680696e+17\n",
      "Gradient Descent(13/49): loss=1.3413472300564009e+19\n",
      "Gradient Descent(14/49): loss=7.90571435151233e+20\n",
      "Gradient Descent(15/49): loss=4.6595202900003714e+22\n",
      "Gradient Descent(16/49): loss=2.7462579972835456e+24\n",
      "Gradient Descent(17/49): loss=1.6186072076978348e+26\n",
      "Gradient Descent(18/49): loss=9.539851336976609e+27\n",
      "Gradient Descent(19/49): loss=5.622658985432706e+29\n",
      "Gradient Descent(20/49): loss=3.313918944313692e+31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=1.953178878231248e+33\n",
      "Gradient Descent(22/49): loss=1.1511771393626734e+35\n",
      "Gradient Descent(23/49): loss=6.784881922300677e+36\n",
      "Gradient Descent(24/49): loss=3.998917379914502e+38\n",
      "Gradient Descent(25/49): loss=2.3569076653821364e+40\n",
      "Gradient Descent(26/49): loss=1.389129410634623e+42\n",
      "Gradient Descent(27/49): loss=8.187340335104844e+43\n",
      "Gradient Descent(28/49): loss=4.8255073465192953e+45\n",
      "Gradient Descent(29/49): loss=2.8440885804478766e+47\n",
      "Gradient Descent(30/49): loss=1.6762672342150051e+49\n",
      "Gradient Descent(31/49): loss=9.87969172204988e+50\n",
      "Gradient Descent(32/49): loss=5.822956300189856e+52\n",
      "Gradient Descent(33/49): loss=3.431971465085984e+54\n",
      "Gradient Descent(34/49): loss=2.0227574328147607e+56\n",
      "Gradient Descent(35/49): loss=1.1921857957244912e+58\n",
      "Gradient Descent(36/49): loss=7.026581380790699e+59\n",
      "Gradient Descent(37/49): loss=4.1413717625172316e+61\n",
      "Gradient Descent(38/49): loss=2.4408683463429988e+63\n",
      "Gradient Descent(39/49): loss=1.438614697212733e+65\n",
      "Gradient Descent(40/49): loss=8.478999902380192e+66\n",
      "Gradient Descent(41/49): loss=4.997407539618134e+68\n",
      "Gradient Descent(42/49): loss=2.9454042227340313e+70\n",
      "Gradient Descent(43/49): loss=1.7359812996084286e+72\n",
      "Gradient Descent(44/49): loss=1.0231638324307269e+74\n",
      "Gradient Descent(45/49): loss=6.030388854018513e+75\n",
      "Gradient Descent(46/49): loss=3.5542293988517175e+77\n",
      "Gradient Descent(47/49): loss=2.094814600760642e+79\n",
      "Gradient Descent(48/49): loss=1.2346553131819743e+81\n",
      "Gradient Descent(49/49): loss=7.276890956435747e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3328437706822658\n",
      "Gradient Descent(2/49): loss=11.971591316710441\n",
      "Gradient Descent(3/49): loss=160.2931561606625\n",
      "Gradient Descent(4/49): loss=3019.6470681905694\n",
      "Gradient Descent(5/49): loss=97623.13539526195\n",
      "Gradient Descent(6/49): loss=4666767.955369918\n",
      "Gradient Descent(7/49): loss=255966771.63143507\n",
      "Gradient Descent(8/49): loss=14523773376.110098\n",
      "Gradient Descent(9/49): loss=830309889065.0685\n",
      "Gradient Descent(10/49): loss=47545192315095.07\n",
      "Gradient Descent(11/49): loss=2723483056229206.0\n",
      "Gradient Descent(12/49): loss=1.5601821613361917e+17\n",
      "Gradient Descent(13/49): loss=8.937847219736032e+18\n",
      "Gradient Descent(14/49): loss=5.120260363405439e+20\n",
      "Gradient Descent(15/49): loss=2.9332662525308686e+22\n",
      "Gradient Descent(16/49): loss=1.6803935085583745e+24\n",
      "Gradient Descent(17/49): loss=9.62654664423774e+25\n",
      "Gradient Descent(18/49): loss=5.514803556465048e+27\n",
      "Gradient Descent(19/49): loss=3.1592905990249614e+29\n",
      "Gradient Descent(20/49): loss=1.809877176994703e+31\n",
      "Gradient Descent(21/49): loss=1.0368325715445384e+33\n",
      "Gradient Descent(22/49): loss=5.939749918375432e+34\n",
      "Gradient Descent(23/49): loss=3.4027315558192516e+36\n",
      "Gradient Descent(24/49): loss=1.9493383054994076e+38\n",
      "Gradient Descent(25/49): loss=1.1167263026639404e+40\n",
      "Gradient Descent(26/49): loss=6.3974407702515826e+41\n",
      "Gradient Descent(27/49): loss=3.664930996184727e+43\n",
      "Gradient Descent(28/49): loss=2.099545691654338e+45\n",
      "Gradient Descent(29/49): loss=1.2027762912680005e+47\n",
      "Gradient Descent(30/49): loss=6.890399254405e+48\n",
      "Gradient Descent(31/49): loss=3.947334365483145e+50\n",
      "Gradient Descent(32/49): loss=2.261327394484852e+52\n",
      "Gradient Descent(33/49): loss=1.2954569113178444e+54\n",
      "Gradient Descent(34/49): loss=7.421342938550828e+55\n",
      "Gradient Descent(35/49): loss=4.251498489096737e+57\n",
      "Gradient Descent(36/49): loss=2.4355752796301907e+59\n",
      "Gradient Descent(37/49): loss=1.3952790899393783e+61\n",
      "Gradient Descent(38/49): loss=7.9931987941578045e+62\n",
      "Gradient Descent(39/49): loss=4.579100154486199e+64\n",
      "Gradient Descent(40/49): loss=2.623249935950671e+66\n",
      "Gradient Descent(41/49): loss=1.5027931240427835e+68\n",
      "Gradient Descent(42/49): loss=8.609119332168579e+69\n",
      "Gradient Descent(43/49): loss=4.931945354935511e+71\n",
      "Gradient Descent(44/49): loss=2.8253859710343772e+73\n",
      "Gradient Descent(45/49): loss=1.6185917139834344e+75\n",
      "Gradient Descent(46/49): loss=9.272499982070425e+76\n",
      "Gradient Descent(47/49): loss=5.311979245581235e+78\n",
      "Gradient Descent(48/49): loss=3.0430977147530317e+80\n",
      "Gradient Descent(49/49): loss=1.7433132309841927e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3520392880604728\n",
      "Gradient Descent(2/49): loss=12.141602305899118\n",
      "Gradient Descent(3/49): loss=160.1900795656052\n",
      "Gradient Descent(4/49): loss=2916.2310546333306\n",
      "Gradient Descent(5/49): loss=90795.22541530822\n",
      "Gradient Descent(6/49): loss=4268790.842221418\n",
      "Gradient Descent(7/49): loss=233174827.20766944\n",
      "Gradient Descent(8/49): loss=13221490552.140703\n",
      "Gradient Descent(9/49): loss=755914435963.6611\n",
      "Gradient Descent(10/49): loss=43295125936213.14\n",
      "Gradient Descent(11/49): loss=2480682235209578.0\n",
      "Gradient Descent(12/49): loss=1.4214733189182694e+17\n",
      "Gradient Descent(13/49): loss=8.145427190018911e+18\n",
      "Gradient Descent(14/49): loss=4.667567846551688e+20\n",
      "Gradient Descent(15/49): loss=2.6746549278083876e+22\n",
      "Gradient Descent(16/49): loss=1.532656929117081e+24\n",
      "Gradient Descent(17/49): loss=8.78258097346536e+25\n",
      "Gradient Descent(18/49): loss=5.032680679599388e+27\n",
      "Gradient Descent(19/49): loss=2.8838760429880735e+29\n",
      "Gradient Descent(20/49): loss=1.6525469354718955e+31\n",
      "Gradient Descent(21/49): loss=9.46958653366995e+32\n",
      "Gradient Descent(22/49): loss=5.42635535459167e+34\n",
      "Gradient Descent(23/49): loss=3.109463367773547e+36\n",
      "Gradient Descent(24/49): loss=1.7818151970735073e+38\n",
      "Gradient Descent(25/49): loss=1.0210332205315212e+40\n",
      "Gradient Descent(26/49): loss=5.850824704723806e+41\n",
      "Gradient Descent(27/49): loss=3.352696958046705e+43\n",
      "Gradient Descent(28/49): loss=1.9211952946428676e+45\n",
      "Gradient Descent(29/49): loss=1.1009021711011983e+47\n",
      "Gradient Descent(30/49): loss=6.308497598942057e+48\n",
      "Gradient Descent(31/49): loss=3.614957168814566e+50\n",
      "Gradient Descent(32/49): loss=2.071478212903708e+52\n",
      "Gradient Descent(33/49): loss=1.1870187629198761e+54\n",
      "Gradient Descent(34/49): loss=6.801971339822848e+55\n",
      "Gradient Descent(35/49): loss=3.897732331876756e+57\n",
      "Gradient Descent(36/49): loss=2.233516810341753e+59\n",
      "Gradient Descent(37/49): loss=1.2798717093221358e+61\n",
      "Gradient Descent(38/49): loss=7.334046400450005e+62\n",
      "Gradient Descent(39/49): loss=4.2026272017874255e+64\n",
      "Gradient Descent(40/49): loss=2.4082306591516013e+66\n",
      "Gradient Descent(41/49): loss=1.3799879525861972e+68\n",
      "Gradient Descent(42/49): loss=7.907742317149661e+69\n",
      "Gradient Descent(43/49): loss=4.531372062868379e+71\n",
      "Gradient Descent(44/49): loss=2.5966112638259165e+73\n",
      "Gradient Descent(45/49): loss=1.4879356543412538e+75\n",
      "Gradient Descent(46/49): loss=8.526314825415405e+76\n",
      "Gradient Descent(47/49): loss=4.885832548604659e+78\n",
      "Gradient Descent(48/49): loss=2.799727687962946e+80\n",
      "Gradient Descent(49/49): loss=1.6043274198959004e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3142628981478826\n",
      "Gradient Descent(2/49): loss=11.506508258092156\n",
      "Gradient Descent(3/49): loss=146.15884199969824\n",
      "Gradient Descent(4/49): loss=2414.7156674491544\n",
      "Gradient Descent(5/49): loss=66503.12939046086\n",
      "Gradient Descent(6/49): loss=2965359.3907860615\n",
      "Gradient Descent(7/49): loss=161451410.54254514\n",
      "Gradient Descent(8/49): loss=9256120308.658113\n",
      "Gradient Descent(9/49): loss=536739723417.4973\n",
      "Gradient Descent(10/49): loss=31199596444902.152\n",
      "Gradient Descent(11/49): loss=1814493435918180.5\n",
      "Gradient Descent(12/49): loss=1.0553786780578219e+17\n",
      "Gradient Descent(13/49): loss=6.138623503619276e+18\n",
      "Gradient Descent(14/49): loss=3.570555039307463e+20\n",
      "Gradient Descent(15/49): loss=2.0768297545180947e+22\n",
      "Gradient Descent(16/49): loss=1.207997827022058e+24\n",
      "Gradient Descent(17/49): loss=7.026376672656714e+25\n",
      "Gradient Descent(18/49): loss=4.086925364937826e+27\n",
      "Gradient Descent(19/49): loss=2.377179554843641e+29\n",
      "Gradient Descent(20/49): loss=1.3826977822356256e+31\n",
      "Gradient Descent(21/49): loss=8.042527344083179e+32\n",
      "Gradient Descent(22/49): loss=4.677974240828131e+34\n",
      "Gradient Descent(23/49): loss=2.720965942880261e+36\n",
      "Gradient Descent(24/49): loss=1.5826627683630517e+38\n",
      "Gradient Descent(25/49): loss=9.205633186689546e+39\n",
      "Gradient Descent(26/49): loss=5.354500280279792e+41\n",
      "Gradient Descent(27/49): loss=3.1144705279991537e+43\n",
      "Gradient Descent(28/49): loss=1.8115465798927237e+45\n",
      "Gradient Descent(29/49): loss=1.0536946751039977e+47\n",
      "Gradient Descent(30/49): loss=6.128865140239885e+48\n",
      "Gradient Descent(31/49): loss=3.5648835279099932e+50\n",
      "Gradient Descent(32/49): loss=2.0735314412655873e+52\n",
      "Gradient Descent(33/49): loss=1.2060794144479709e+54\n",
      "Gradient Descent(34/49): loss=7.015218216644754e+55\n",
      "Gradient Descent(35/49): loss=4.0804350059874417e+57\n",
      "Gradient Descent(36/49): loss=2.3734044079460574e+59\n",
      "Gradient Descent(37/49): loss=1.3805019502558166e+61\n",
      "Gradient Descent(38/49): loss=8.029755183227889e+62\n",
      "Gradient Descent(39/49): loss=4.6705452528065677e+64\n",
      "Gradient Descent(40/49): loss=2.7166448367040667e+66\n",
      "Gradient Descent(41/49): loss=1.5801493764258133e+68\n",
      "Gradient Descent(42/49): loss=9.191013923072258e+69\n",
      "Gradient Descent(43/49): loss=5.345996916138603e+71\n",
      "Gradient Descent(44/49): loss=3.109524505845825e+73\n",
      "Gradient Descent(45/49): loss=1.808669702608032e+75\n",
      "Gradient Descent(46/49): loss=1.0520213257629174e+77\n",
      "Gradient Descent(47/49): loss=6.1191320242943716e+78\n",
      "Gradient Descent(48/49): loss=3.55922221477697e+80\n",
      "Gradient Descent(49/49): loss=2.070238511584087e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3412794642246053\n",
      "Gradient Descent(2/49): loss=12.146374396815993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3/49): loss=166.8006448191185\n",
      "Gradient Descent(4/49): loss=3379.041843096058\n",
      "Gradient Descent(5/49): loss=121053.28601469919\n",
      "Gradient Descent(6/49): loss=6245291.419853809\n",
      "Gradient Descent(7/49): loss=361651540.9574437\n",
      "Gradient Descent(8/49): loss=21509238978.160057\n",
      "Gradient Descent(9/49): loss=1286518897699.9355\n",
      "Gradient Descent(10/49): loss=77040218725475.73\n",
      "Gradient Descent(11/49): loss=4614497174882760.0\n",
      "Gradient Descent(12/49): loss=2.7640956961764826e+17\n",
      "Gradient Descent(13/49): loss=1.6557176276858784e+19\n",
      "Gradient Descent(14/49): loss=9.917915453499952e+20\n",
      "Gradient Descent(15/49): loss=5.940933942447197e+22\n",
      "Gradient Descent(16/49): loss=3.558681207357179e+24\n",
      "Gradient Descent(17/49): loss=2.1316870879078524e+26\n",
      "Gradient Descent(18/49): loss=1.2769027608108512e+28\n",
      "Gradient Descent(19/49): loss=7.648780494720263e+29\n",
      "Gradient Descent(20/49): loss=4.581699160007355e+31\n",
      "Gradient Descent(21/49): loss=2.744485504326637e+33\n",
      "Gradient Descent(22/49): loss=1.6439753943819715e+35\n",
      "Gradient Descent(23/49): loss=9.847583793307273e+36\n",
      "Gradient Descent(24/49): loss=5.898805231369939e+38\n",
      "Gradient Descent(25/49): loss=3.5334457556266204e+40\n",
      "Gradient Descent(26/49): loss=2.1165708000594796e+42\n",
      "Gradient Descent(27/49): loss=1.2678479482898066e+44\n",
      "Gradient Descent(28/49): loss=7.594541226485232e+45\n",
      "Gradient Descent(29/49): loss=4.549209273760725e+47\n",
      "Gradient Descent(30/49): loss=2.7250237241845595e+49\n",
      "Gradient Descent(31/49): loss=1.6323175854317746e+51\n",
      "Gradient Descent(32/49): loss=9.77775230381548e+52\n",
      "Gradient Descent(33/49): loss=5.8569754420358344e+54\n",
      "Gradient Descent(34/49): loss=3.508389276257657e+56\n",
      "Gradient Descent(35/49): loss=2.1015617080138687e+58\n",
      "Gradient Descent(36/49): loss=1.2588573458704558e+60\n",
      "Gradient Descent(37/49): loss=7.540686581835703e+61\n",
      "Gradient Descent(38/49): loss=4.516949780847469e+63\n",
      "Gradient Descent(39/49): loss=2.7056999520236566e+65\n",
      "Gradient Descent(40/49): loss=1.6207424446962248e+67\n",
      "Gradient Descent(41/49): loss=9.708416005533885e+68\n",
      "Gradient Descent(42/49): loss=5.815442277392449e+70\n",
      "Gradient Descent(43/49): loss=3.4835104781670856e+72\n",
      "Gradient Descent(44/49): loss=2.0866590489040152e+74\n",
      "Gradient Descent(45/49): loss=1.2499304978878704e+76\n",
      "Gradient Descent(46/49): loss=7.487213832901097e+77\n",
      "Gradient Descent(47/49): loss=4.4849190474440644e+79\n",
      "Gradient Descent(48/49): loss=2.6865132091910486e+81\n",
      "Gradient Descent(49/49): loss=1.6092493859551528e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.350862523269827\n",
      "Gradient Descent(2/49): loss=12.406236726220374\n",
      "Gradient Descent(3/49): loss=169.21626276280747\n",
      "Gradient Descent(4/49): loss=3237.7706412506295\n",
      "Gradient Descent(5/49): loss=106117.14014742653\n",
      "Gradient Descent(6/49): loss=5147558.088242943\n",
      "Gradient Descent(7/49): loss=286804102.32033956\n",
      "Gradient Descent(8/49): loss=16537349571.420376\n",
      "Gradient Descent(9/49): loss=960853497013.7306\n",
      "Gradient Descent(10/49): loss=55919804758902.484\n",
      "Gradient Descent(11/49): loss=3255581809081058.5\n",
      "Gradient Descent(12/49): loss=1.895504500208566e+17\n",
      "Gradient Descent(13/49): loss=1.103641890105681e+19\n",
      "Gradient Descent(14/49): loss=6.425886798943281e+20\n",
      "Gradient Descent(15/49): loss=3.7414357836822747e+22\n",
      "Gradient Descent(16/49): loss=2.1784299123769604e+24\n",
      "Gradient Descent(17/49): loss=1.2683785917224564e+26\n",
      "Gradient Descent(18/49): loss=7.385063228289858e+27\n",
      "Gradient Descent(19/49): loss=4.2999116612856296e+29\n",
      "Gradient Descent(20/49): loss=2.503599458771686e+31\n",
      "Gradient Descent(21/49): loss=1.457706749389432e+33\n",
      "Gradient Descent(22/49): loss=8.4874158276599e+34\n",
      "Gradient Descent(23/49): loss=4.9417502842759376e+36\n",
      "Gradient Descent(24/49): loss=2.8773063990307075e+38\n",
      "Gradient Descent(25/49): loss=1.6752955203436582e+40\n",
      "Gradient Descent(26/49): loss=9.75431424831575e+41\n",
      "Gradient Descent(27/49): loss=5.679394787337617e+43\n",
      "Gradient Descent(28/49): loss=3.306795775623818e+45\n",
      "Gradient Descent(29/49): loss=1.9253633021009843e+47\n",
      "Gradient Descent(30/49): loss=1.1210319888526728e+49\n",
      "Gradient Descent(31/49): loss=6.527145908824752e+50\n",
      "Gradient Descent(32/49): loss=3.8003941135248765e+52\n",
      "Gradient Descent(33/49): loss=2.212758167178019e+54\n",
      "Gradient Descent(34/49): loss=1.2883660378769981e+56\n",
      "Gradient Descent(35/49): loss=7.501439028340573e+57\n",
      "Gradient Descent(36/49): loss=4.3676708203699635e+59\n",
      "Gradient Descent(37/49): loss=2.5430518495237752e+61\n",
      "Gradient Descent(38/49): loss=1.480677682760585e+63\n",
      "Gradient Descent(39/49): loss=8.621162799475524e+64\n",
      "Gradient Descent(40/49): loss=5.019623708820271e+66\n",
      "Gradient Descent(41/49): loss=2.922647763905251e+68\n",
      "Gradient Descent(42/49): loss=1.7016952758532568e+70\n",
      "Gradient Descent(43/49): loss=9.908025344771559e+71\n",
      "Gradient Descent(44/49): loss=5.768892211527998e+73\n",
      "Gradient Descent(45/49): loss=3.3589051491263773e+75\n",
      "Gradient Descent(46/49): loss=1.9557036926920112e+77\n",
      "Gradient Descent(47/49): loss=1.1386975111827959e+79\n",
      "Gradient Descent(48/49): loss=6.630002422243571e+80\n",
      "Gradient Descent(49/49): loss=3.8602817418381524e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3704522108342758\n",
      "Gradient Descent(2/49): loss=12.583029035163335\n",
      "Gradient Descent(3/49): loss=169.12546591454557\n",
      "Gradient Descent(4/49): loss=3127.650231845934\n",
      "Gradient Descent(5/49): loss=98714.88006271858\n",
      "Gradient Descent(6/49): loss=4708913.320527509\n",
      "Gradient Descent(7/49): loss=261270832.67653784\n",
      "Gradient Descent(8/49): loss=15054556282.008528\n",
      "Gradient Descent(9/49): loss=874760302662.0887\n",
      "Gradient Descent(10/49): loss=50921013716137.734\n",
      "Gradient Descent(11/49): loss=2965335761957596.5\n",
      "Gradient Descent(12/49): loss=1.726978369754247e+17\n",
      "Gradient Descent(13/49): loss=1.0057908361043818e+19\n",
      "Gradient Descent(14/49): loss=5.857740275599814e+20\n",
      "Gradient Descent(15/49): loss=3.4115591453466457e+22\n",
      "Gradient Descent(16/49): loss=1.9868989230792597e+24\n",
      "Gradient Descent(17/49): loss=1.157173981398053e+26\n",
      "Gradient Descent(18/49): loss=6.739404901068449e+27\n",
      "Gradient Descent(19/49): loss=3.925043185300939e+29\n",
      "Gradient Descent(20/49): loss=2.2859531721757136e+31\n",
      "Gradient Descent(21/49): loss=1.33134379906028e+33\n",
      "Gradient Descent(22/49): loss=7.753773493265993e+34\n",
      "Gradient Descent(23/49): loss=4.5158135282207064e+36\n",
      "Gradient Descent(24/49): loss=2.6300190274307232e+38\n",
      "Gradient Descent(25/49): loss=1.5317284563284517e+40\n",
      "Gradient Descent(26/49): loss=8.920817832327457e+41\n",
      "Gradient Descent(27/49): loss=5.195502536286815e+43\n",
      "Gradient Descent(28/49): loss=3.025871294753264e+45\n",
      "Gradient Descent(29/49): loss=1.7622736257878201e+47\n",
      "Gradient Descent(30/49): loss=1.0263517610720102e+49\n",
      "Gradient Descent(31/49): loss=5.977493631187411e+50\n",
      "Gradient Descent(32/49): loss=3.481304506514089e+52\n",
      "Gradient Descent(33/49): loss=2.0275188590485984e+54\n",
      "Gradient Descent(34/49): loss=1.180831126982902e+56\n",
      "Gradient Descent(35/49): loss=6.877184615219865e+57\n",
      "Gradient Descent(36/49): loss=4.005286374238743e+59\n",
      "Gradient Descent(37/49): loss=2.3326869696298888e+61\n",
      "Gradient Descent(38/49): loss=1.3585616582323347e+63\n",
      "Gradient Descent(39/49): loss=7.912290861348733e+64\n",
      "Gradient Descent(40/49): loss=4.608134367345553e+66\n",
      "Gradient Descent(41/49): loss=2.683786872806059e+68\n",
      "Gradient Descent(42/49): loss=1.563042959356063e+70\n",
      "Gradient Descent(43/49): loss=9.103194137909013e+71\n",
      "Gradient Descent(44/49): loss=5.301718869365022e+73\n",
      "Gradient Descent(45/49): loss=3.0877319042035332e+75\n",
      "Gradient Descent(46/49): loss=1.7983013711510555e+77\n",
      "Gradient Descent(47/49): loss=1.0473343935985108e+79\n",
      "Gradient Descent(48/49): loss=6.099696911826555e+80\n",
      "Gradient Descent(49/49): loss=3.5524759468951313e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3319364235934428\n",
      "Gradient Descent(2/49): loss=11.92474906105122\n",
      "Gradient Descent(3/49): loss=154.35070196459566\n",
      "Gradient Descent(4/49): loss=2591.7059192877937\n",
      "Gradient Descent(5/49): loss=72354.2211044346\n",
      "Gradient Descent(6/49): loss=3271779.2009885944\n",
      "Gradient Descent(7/49): loss=180901405.65575314\n",
      "Gradient Descent(8/49): loss=10538423687.423018\n",
      "Gradient Descent(9/49): loss=621050768726.3267\n",
      "Gradient Descent(10/49): loss=36689895054862.08\n",
      "Gradient Descent(11/49): loss=2168658642703541.8\n",
      "Gradient Descent(12/49): loss=1.2819863400226133e+17\n",
      "Gradient Descent(13/49): loss=7.578540821246593e+18\n",
      "Gradient Descent(14/49): loss=4.480122567180364e+20\n",
      "Gradient Descent(15/49): loss=2.64846745189727e+22\n",
      "Gradient Descent(16/49): loss=1.5656672882443717e+24\n",
      "Gradient Descent(17/49): loss=9.255594840737763e+25\n",
      "Gradient Descent(18/49): loss=5.471535192375052e+27\n",
      "Gradient Descent(19/49): loss=3.2345514185284624e+29\n",
      "Gradient Descent(20/49): loss=1.912136633627382e+31\n",
      "Gradient Descent(21/49): loss=1.1303782295872073e+33\n",
      "Gradient Descent(22/49): loss=6.682341206573106e+34\n",
      "Gradient Descent(23/49): loss=3.9503312106025256e+36\n",
      "Gradient Descent(24/49): loss=2.3352768425119797e+38\n",
      "Gradient Descent(25/49): loss=1.3805216931017173e+40\n",
      "Gradient Descent(26/49): loss=8.161088700192224e+41\n",
      "Gradient Descent(27/49): loss=4.824507221089948e+43\n",
      "Gradient Descent(28/49): loss=2.8520545213289237e+45\n",
      "Gradient Descent(29/49): loss=1.6860198606552734e+47\n",
      "Gradient Descent(30/49): loss=9.96707092822199e+48\n",
      "Gradient Descent(31/49): loss=5.8921312379794684e+50\n",
      "Gradient Descent(32/49): loss=3.483190876797197e+52\n",
      "Gradient Descent(33/49): loss=2.0591222758241935e+54\n",
      "Gradient Descent(34/49): loss=1.217270226285773e+56\n",
      "Gradient Descent(35/49): loss=7.196011724018197e+57\n",
      "Gradient Descent(36/49): loss=4.25399254939552e+59\n",
      "Gradient Descent(37/49): loss=2.5147892060697724e+61\n",
      "Gradient Descent(38/49): loss=1.4866421785021322e+63\n",
      "Gradient Descent(39/49): loss=8.78843030488262e+64\n",
      "Gradient Descent(40/49): loss=5.195366332307188e+66\n",
      "Gradient Descent(41/49): loss=3.071291503771191e+68\n",
      "Gradient Descent(42/49): loss=1.815623942142354e+70\n",
      "Gradient Descent(43/49): loss=1.0733238102709763e+72\n",
      "Gradient Descent(44/49): loss=6.345058439443572e+73\n",
      "Gradient Descent(45/49): loss=3.7509432116100856e+75\n",
      "Gradient Descent(46/49): loss=2.2174066812783617e+77\n",
      "Gradient Descent(47/49): loss=1.3108415971105347e+79\n",
      "Gradient Descent(48/49): loss=7.749168013350752e+80\n",
      "Gradient Descent(49/49): loss=4.5809962875382264e+82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.359302653953492\n",
      "Gradient Descent(2/49): loss=12.583905036195924\n",
      "Gradient Descent(3/49): loss=175.98704331574524\n",
      "Gradient Descent(4/49): loss=3619.433362089135\n",
      "Gradient Descent(5/49): loss=131432.55223073342\n",
      "Gradient Descent(6/49): loss=6881142.94345951\n",
      "Gradient Descent(7/49): loss=404757304.85228014\n",
      "Gradient Descent(8/49): loss=24460267626.01493\n",
      "Gradient Descent(9/49): loss=1486685286800.3235\n",
      "Gradient Descent(10/49): loss=90468112391577.34\n",
      "Gradient Descent(11/49): loss=5506549367957731.0\n",
      "Gradient Descent(12/49): loss=3.351859585934247e+17\n",
      "Gradient Descent(13/49): loss=2.0403125904392258e+19\n",
      "Gradient Descent(14/49): loss=1.2419627122851064e+21\n",
      "Gradient Descent(15/49): loss=7.559979233013038e+22\n",
      "Gradient Descent(16/49): loss=4.6018524412286754e+24\n",
      "Gradient Descent(17/49): loss=2.8012042959086253e+26\n",
      "Gradient Descent(18/49): loss=1.7051275848955269e+28\n",
      "Gradient Descent(19/49): loss=1.0379321806173748e+30\n",
      "Gradient Descent(20/49): loss=6.318021133966984e+31\n",
      "Gradient Descent(21/49): loss=3.8458573494604915e+33\n",
      "Gradient Descent(22/49): loss=2.3410207783243663e+35\n",
      "Gradient Descent(23/49): loss=1.425008206641765e+37\n",
      "Gradient Descent(24/49): loss=8.674200621365217e+38\n",
      "Gradient Descent(25/49): loss=5.280092849220721e+40\n",
      "Gradient Descent(26/49): loss=3.214057607535973e+42\n",
      "Gradient Descent(27/49): loss=1.956436486923547e+44\n",
      "Gradient Descent(28/49): loss=1.1909070075132187e+46\n",
      "Gradient Descent(29/49): loss=7.24919776350252e+47\n",
      "Gradient Descent(30/49): loss=4.412676042951783e+49\n",
      "Gradient Descent(31/49): loss=2.6860503045006048e+51\n",
      "Gradient Descent(32/49): loss=1.635031932568836e+53\n",
      "Gradient Descent(33/49): loss=9.952640931707573e+54\n",
      "Gradient Descent(34/49): loss=6.0582952260677745e+56\n",
      "Gradient Descent(35/49): loss=3.687758987593436e+58\n",
      "Gradient Descent(36/49): loss=2.244784356506763e+60\n",
      "Gradient Descent(37/49): loss=1.366427910329854e+62\n",
      "Gradient Descent(38/49): loss=8.317615136243743e+63\n",
      "Gradient Descent(39/49): loss=5.0630348686283295e+65\n",
      "Gradient Descent(40/49): loss=3.081931738972432e+67\n",
      "Gradient Descent(41/49): loss=1.8760098419505282e+69\n",
      "Gradient Descent(42/49): loss=1.1419503172606592e+71\n",
      "Gradient Descent(43/49): loss=6.9511923548111845e+72\n",
      "Gradient Descent(44/49): loss=4.2312764770269987e+74\n",
      "Gradient Descent(45/49): loss=2.5756301525234365e+76\n",
      "Gradient Descent(46/49): loss=1.567817824858626e+78\n",
      "Gradient Descent(47/49): loss=9.54350037227282e+79\n",
      "Gradient Descent(48/49): loss=5.809246323869603e+81\n",
      "Gradient Descent(49/49): loss=3.536159850681256e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3690530911543035\n",
      "Gradient Descent(2/49): loss=12.85299943814567\n",
      "Gradient Descent(3/49): loss=178.55128648001767\n",
      "Gradient Descent(4/49): loss=3469.6159938010937\n",
      "Gradient Descent(5/49): loss=115272.61790598146\n",
      "Gradient Descent(6/49): loss=5673415.678931467\n",
      "Gradient Descent(7/49): loss=321062869.8601406\n",
      "Gradient Descent(8/49): loss=18810433186.115597\n",
      "Gradient Descent(9/49): loss=1110615941926.2395\n",
      "Gradient Descent(10/49): loss=65683721025272.03\n",
      "Gradient Descent(11/49): loss=3886054745581323.5\n",
      "Gradient Descent(12/49): loss=2.2992913325885766e+17\n",
      "Gradient Descent(13/49): loss=1.3604619565728444e+19\n",
      "Gradient Descent(14/49): loss=8.049712647737257e+20\n",
      "Gradient Descent(15/49): loss=4.762935419366344e+22\n",
      "Gradient Descent(16/49): loss=2.8181822912892417e+24\n",
      "Gradient Descent(17/49): loss=1.6674909517485342e+26\n",
      "Gradient Descent(18/49): loss=9.866381264750532e+27\n",
      "Gradient Descent(19/49): loss=5.837841538132003e+29\n",
      "Gradient Descent(20/49): loss=3.454193885502843e+31\n",
      "Gradient Descent(21/49): loss=2.0438128239087516e+33\n",
      "Gradient Descent(22/49): loss=1.2093041090646272e+35\n",
      "Gradient Descent(23/49): loss=7.155334437177358e+36\n",
      "Gradient Descent(24/49): loss=4.2337415811393414e+38\n",
      "Gradient Descent(25/49): loss=2.505063590422782e+40\n",
      "Gradient Descent(26/49): loss=1.4822216877898162e+42\n",
      "Gradient Descent(27/49): loss=8.770161125465391e+43\n",
      "Gradient Descent(28/49): loss=5.189218778826277e+45\n",
      "Gradient Descent(29/49): loss=3.0704101269398825e+47\n",
      "Gradient Descent(30/49): loss=1.81673171809249e+49\n",
      "Gradient Descent(31/49): loss=1.07494243409519e+51\n",
      "Gradient Descent(32/49): loss=6.360329514320019e+52\n",
      "Gradient Descent(33/49): loss=3.7633449241197524e+54\n",
      "Gradient Descent(34/49): loss=2.2267344775158346e+56\n",
      "Gradient Descent(35/49): loss=1.3175370669797318e+58\n",
      "Gradient Descent(36/49): loss=7.79573829027919e+59\n",
      "Gradient Descent(37/49): loss=4.6126622934290257e+61\n",
      "Gradient Descent(38/49): loss=2.7292672792458e+63\n",
      "Gradient Descent(39/49): loss=1.6148808232011533e+65\n",
      "Gradient Descent(40/49): loss=9.555092287859466e+66\n",
      "Gradient Descent(41/49): loss=5.653654890057294e+68\n",
      "Gradient Descent(42/49): loss=3.34521244305318e+70\n",
      "Gradient Descent(43/49): loss=1.9793295676462022e+72\n",
      "Gradient Descent(44/49): loss=1.1711499954193536e+74\n",
      "Gradient Descent(45/49): loss=6.929580268948515e+75\n",
      "Gradient Descent(46/49): loss=4.100165042190572e+77\n",
      "Gradient Descent(47/49): loss=2.4260276554604014e+79\n",
      "Gradient Descent(48/49): loss=1.4354568961238827e+81\n",
      "Gradient Descent(49/49): loss=8.493458415413554e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3890406070898351\n",
      "Gradient Descent(2/49): loss=13.036771680142438\n",
      "Gradient Descent(3/49): loss=178.47410305435866\n",
      "Gradient Descent(4/49): loss=3352.416624568423\n",
      "Gradient Descent(5/49): loss=107252.92367989036\n",
      "Gradient Descent(6/49): loss=5190326.288302317\n",
      "Gradient Descent(7/49): loss=292484696.6637298\n",
      "Gradient Descent(8/49): loss=17123871612.866194\n",
      "Gradient Descent(9/49): loss=1011102708581.294\n",
      "Gradient Descent(10/49): loss=59811976801956.805\n",
      "Gradient Descent(11/49): loss=3539590326468900.0\n",
      "Gradient Descent(12/49): loss=2.0948588976198925e+17\n",
      "Gradient Descent(13/49): loss=1.239836683602729e+19\n",
      "Gradient Descent(14/49): loss=7.337969301782908e+20\n",
      "Gradient Descent(15/49): loss=4.34297835401895e+22\n",
      "Gradient Descent(16/49): loss=2.5703929269902013e+24\n",
      "Gradient Descent(17/49): loss=1.52128781607396e+26\n",
      "Gradient Descent(18/49): loss=9.003746525717288e+27\n",
      "Gradient Descent(19/49): loss=5.328870105168508e+29\n",
      "Gradient Descent(20/49): loss=3.153893385109332e+31\n",
      "Gradient Descent(21/49): loss=1.866632755001556e+33\n",
      "Gradient Descent(22/49): loss=1.104767161298828e+35\n",
      "Gradient Descent(23/49): loss=6.538567789612719e+36\n",
      "Gradient Descent(24/49): loss=3.869853326300215e+38\n",
      "Gradient Descent(25/49): loss=2.2903738630454306e+40\n",
      "Gradient Descent(26/49): loss=1.3555584644178533e+42\n",
      "Gradient Descent(27/49): loss=8.022876876579322e+43\n",
      "Gradient Descent(28/49): loss=4.748342108903046e+45\n",
      "Gradient Descent(29/49): loss=2.8103077150542285e+47\n",
      "Gradient Descent(30/49): loss=1.6632814721763627e+49\n",
      "Gradient Descent(31/49): loss=9.844136429849393e+50\n",
      "Gradient Descent(32/49): loss=5.826255127022358e+52\n",
      "Gradient Descent(33/49): loss=3.448270861243449e+54\n",
      "Gradient Descent(34/49): loss=2.04086015343743e+56\n",
      "Gradient Descent(35/49): loss=1.2078836998282341e+58\n",
      "Gradient Descent(36/49): loss=7.148863335164977e+59\n",
      "Gradient Descent(37/49): loss=4.2310569297468235e+61\n",
      "Gradient Descent(38/49): loss=2.5041523251256323e+63\n",
      "Gradient Descent(39/49): loss=1.4820833119367346e+65\n",
      "Gradient Descent(40/49): loss=8.771714569764313e+66\n",
      "Gradient Descent(41/49): loss=5.191541924378614e+68\n",
      "Gradient Descent(42/49): loss=3.072615660053955e+70\n",
      "Gradient Descent(43/49): loss=1.8185285088569545e+72\n",
      "Gradient Descent(44/49): loss=1.076296648656481e+74\n",
      "Gradient Descent(45/49): loss=6.370064974330971e+75\n",
      "Gradient Descent(46/49): loss=3.7701248840503184e+77\n",
      "Gradient Descent(47/49): loss=2.2313495543000324e+79\n",
      "Gradient Descent(48/49): loss=1.3206249094130547e+81\n",
      "Gradient Descent(49/49): loss=7.816122525497246e+82\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3497787421673462\n",
      "Gradient Descent(2/49): loss=12.354678167416576\n",
      "Gradient Descent(3/49): loss=162.92315519218806\n",
      "Gradient Descent(4/49): loss=2779.988138734143\n",
      "Gradient Descent(5/49): loss=78666.38427700433\n",
      "Gradient Descent(6/49): loss=3607020.852337746\n",
      "Gradient Descent(7/49): loss=202509596.2287148\n",
      "Gradient Descent(8/49): loss=11985870129.815989\n",
      "Gradient Descent(9/49): loss=717763110546.6508\n",
      "Gradient Descent(10/49): loss=43090151654821.766\n",
      "Gradient Descent(11/49): loss=2588239456239486.0\n",
      "Gradient Descent(12/49): loss=1.5548172242839725e+17\n",
      "Gradient Descent(13/49): loss=9.34037814274779e+18\n",
      "Gradient Descent(14/49): loss=5.6111481119493076e+20\n",
      "Gradient Descent(15/49): loss=3.370850265624893e+22\n",
      "Gradient Descent(16/49): loss=2.0250105326457141e+24\n",
      "Gradient Descent(17/49): loss=1.216508455021107e+26\n",
      "Gradient Descent(18/49): loss=7.308074757422659e+27\n",
      "Gradient Descent(19/49): loss=4.3902659739550585e+29\n",
      "Gradient Descent(20/49): loss=2.6374162786093125e+31\n",
      "Gradient Descent(21/49): loss=1.5844061996659873e+33\n",
      "Gradient Descent(22/49): loss=9.518190305978804e+34\n",
      "Gradient Descent(23/49): loss=5.71797476684337e+36\n",
      "Gradient Descent(24/49): loss=3.435026447595689e+38\n",
      "Gradient Descent(25/49): loss=2.0635639674565654e+40\n",
      "Gradient Descent(26/49): loss=1.2396691299905062e+42\n",
      "Gradient Descent(27/49): loss=7.447210632125957e+43\n",
      "Gradient Descent(28/49): loss=4.473850712058449e+45\n",
      "Gradient Descent(29/49): loss=2.6876291248488787e+47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=1.6145711552839378e+49\n",
      "Gradient Descent(31/49): loss=9.699403803050603e+50\n",
      "Gradient Descent(32/49): loss=5.82683728906875e+52\n",
      "Gradient Descent(33/49): loss=3.500424715032843e+54\n",
      "Gradient Descent(34/49): loss=2.1028514402829228e+56\n",
      "Gradient Descent(35/49): loss=1.2632707570911326e+58\n",
      "Gradient Descent(36/49): loss=7.588995471343707e+59\n",
      "Gradient Descent(37/49): loss=4.5590267914292753e+61\n",
      "Gradient Descent(38/49): loss=2.738797955994824e+63\n",
      "Gradient Descent(39/49): loss=1.645310410954985e+65\n",
      "Gradient Descent(40/49): loss=9.884067360542392e+66\n",
      "Gradient Descent(41/49): loss=5.937772406790661e+68\n",
      "Gradient Descent(42/49): loss=3.567068077216154e+70\n",
      "Gradient Descent(43/49): loss=2.142886893566857e+72\n",
      "Gradient Descent(44/49): loss=1.2873217273173885e+74\n",
      "Gradient Descent(45/49): loss=7.733479702538173e+75\n",
      "Gradient Descent(46/49): loss=4.645824508392221e+77\n",
      "Gradient Descent(47/49): loss=2.790940972624991e+79\n",
      "Gradient Descent(48/49): loss=1.6766349005664026e+81\n",
      "Gradient Descent(49/49): loss=1.0072246662935904e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3774962557490276\n",
      "Gradient Descent(2/49): loss=13.033534964301735\n",
      "Gradient Descent(3/49): loss=185.5931261014775\n",
      "Gradient Descent(4/49): loss=3874.7238710852225\n",
      "Gradient Descent(5/49): loss=142608.19868394634\n",
      "Gradient Descent(6/49): loss=7575890.6404305\n",
      "Gradient Descent(7/49): loss=452594937.63658386\n",
      "Gradient Descent(8/49): loss=27787693548.59786\n",
      "Gradient Descent(9/49): loss=1716016459413.4814\n",
      "Gradient Descent(10/49): loss=106100509587736.86\n",
      "Gradient Descent(11/49): loss=6561800135061375.0\n",
      "Gradient Descent(12/49): loss=4.0583664445872115e+17\n",
      "Gradient Descent(13/49): loss=2.5100607736690184e+19\n",
      "Gradient Descent(14/49): loss=1.5524520464708275e+21\n",
      "Gradient Descent(15/49): loss=9.601793349286994e+22\n",
      "Gradient Descent(16/49): loss=5.938633961977879e+24\n",
      "Gradient Descent(17/49): loss=3.67299865370734e+26\n",
      "Gradient Descent(18/49): loss=2.2717209467593575e+28\n",
      "Gradient Descent(19/49): loss=1.405041643349331e+30\n",
      "Gradient Descent(20/49): loss=8.690072707731053e+31\n",
      "Gradient Descent(21/49): loss=5.37474202461769e+33\n",
      "Gradient Descent(22/49): loss=3.324235918730049e+35\n",
      "Gradient Descent(23/49): loss=2.0560139245341102e+37\n",
      "Gradient Descent(24/49): loss=1.2716285369705516e+39\n",
      "Gradient Descent(25/49): loss=7.864923076356857e+40\n",
      "Gradient Descent(26/49): loss=4.8643934292618057e+42\n",
      "Gradient Descent(27/49): loss=3.008589302771078e+44\n",
      "Gradient Descent(28/49): loss=1.8607889605101696e+46\n",
      "Gradient Descent(29/49): loss=1.1508834231270652e+48\n",
      "Gradient Descent(30/49): loss=7.118123987931906e+49\n",
      "Gradient Descent(31/49): loss=4.40250403206814e+51\n",
      "Gradient Descent(32/49): loss=2.722914321980912e+53\n",
      "Gradient Descent(33/49): loss=1.6841012184981182e+55\n",
      "Gradient Descent(34/49): loss=1.0416034361608468e+57\n",
      "Gradient Descent(35/49): loss=6.44223581281914e+58\n",
      "Gradient Descent(36/49): loss=3.984472480327029e+60\n",
      "Gradient Descent(37/49): loss=2.4643650756918965e+62\n",
      "Gradient Descent(38/49): loss=1.5241905311870466e+64\n",
      "Gradient Descent(39/49): loss=9.426999263524596e+65\n",
      "Gradient Descent(40/49): loss=5.830525337621729e+67\n",
      "Gradient Descent(41/49): loss=3.6061343341973274e+69\n",
      "Gradient Descent(42/49): loss=2.2303658904227093e+71\n",
      "Gradient Descent(43/49): loss=1.3794638646672462e+73\n",
      "Gradient Descent(44/49): loss=8.531876146841774e+74\n",
      "Gradient Descent(45/49): loss=5.2768986886515224e+76\n",
      "Gradient Descent(46/49): loss=3.2637205804492563e+78\n",
      "Gradient Descent(47/49): loss=2.018585660959559e+80\n",
      "Gradient Descent(48/49): loss=1.2484794485901376e+82\n",
      "Gradient Descent(49/49): loss=7.721747774681746e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.387415474335696\n",
      "Gradient Descent(2/49): loss=13.312102865558526\n",
      "Gradient Descent(3/49): loss=188.31332942508178\n",
      "Gradient Descent(4/49): loss=3715.919365922052\n",
      "Gradient Descent(5/49): loss=125135.11384787654\n",
      "Gradient Descent(6/49): loss=6248153.192695215\n",
      "Gradient Descent(7/49): loss=359090645.24743164\n",
      "Gradient Descent(8/49): loss=21373978712.175003\n",
      "Gradient Descent(9/49): loss=1282237693514.6863\n",
      "Gradient Descent(10/49): loss=77053427809650.61\n",
      "Gradient Descent(11/49): loss=4632074523116946.0\n",
      "Gradient Descent(12/49): loss=2.784798115139265e+17\n",
      "Gradient Descent(13/49): loss=1.6742463842172258e+19\n",
      "Gradient Descent(14/49): loss=1.0065761539639917e+21\n",
      "Gradient Descent(15/49): loss=6.05165629685019e+22\n",
      "Gradient Descent(16/49): loss=3.638328818265712e+24\n",
      "Gradient Descent(17/49): loss=2.1874072875976467e+26\n",
      "Gradient Descent(18/49): loss=1.3150957265143602e+28\n",
      "Gradient Descent(19/49): loss=7.906514633672876e+29\n",
      "Gradient Descent(20/49): loss=4.753492267956511e+31\n",
      "Gradient Descent(21/49): loss=2.857857069805726e+33\n",
      "Gradient Descent(22/49): loss=1.7181782511016394e+35\n",
      "Gradient Descent(23/49): loss=1.0329895549222018e+37\n",
      "Gradient Descent(24/49): loss=6.2104581983568115e+38\n",
      "Gradient Descent(25/49): loss=3.733802616856912e+40\n",
      "Gradient Descent(26/49): loss=2.2448073131442616e+42\n",
      "Gradient Descent(27/49): loss=1.3496053193588082e+44\n",
      "Gradient Descent(28/49): loss=8.113990485403195e+45\n",
      "Gradient Descent(29/49): loss=4.8782292610177596e+47\n",
      "Gradient Descent(30/49): loss=2.93285045944533e+49\n",
      "Gradient Descent(31/49): loss=1.7632651843989157e+51\n",
      "Gradient Descent(32/49): loss=1.060096364784109e+53\n",
      "Gradient Descent(33/49): loss=6.373427619235686e+54\n",
      "Gradient Descent(34/49): loss=3.8317818046577897e+56\n",
      "Gradient Descent(35/49): loss=2.3037135864213744e+58\n",
      "Gradient Descent(36/49): loss=1.3850204836327734e+60\n",
      "Gradient Descent(37/49): loss=8.326910738336434e+61\n",
      "Gradient Descent(38/49): loss=5.0062394934663705e+63\n",
      "Gradient Descent(39/49): loss=3.0098117601473475e+65\n",
      "Gradient Descent(40/49): loss=1.8095352496308187e+67\n",
      "Gradient Descent(41/49): loss=1.0879144878802068e+69\n",
      "Gradient Descent(42/49): loss=6.540673541347671e+70\n",
      "Gradient Descent(43/49): loss=3.932332076746444e+72\n",
      "Gradient Descent(44/49): loss=2.364165626683015e+74\n",
      "Gradient Descent(45/49): loss=1.4213649817219725e+76\n",
      "Gradient Descent(46/49): loss=8.545418258618591e+77\n",
      "Gradient Descent(47/49): loss=5.137608858652381e+79\n",
      "Gradient Descent(48/49): loss=3.0887926120973818e+81\n",
      "Gradient Descent(49/49): loss=1.8570194935100938e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4078044768271527\n",
      "Gradient Descent(2/49): loss=13.50305748302411\n",
      "Gradient Descent(3/49): loss=188.25116796178688\n",
      "Gradient Descent(4/49): loss=3591.2488363634898\n",
      "Gradient Descent(5/49): loss=116452.05514008443\n",
      "Gradient Descent(6/49): loss=5716526.014070021\n",
      "Gradient Descent(7/49): loss=327133217.60929036\n",
      "Gradient Descent(8/49): loss=19457617708.58297\n",
      "Gradient Descent(9/49): loss=1167345481911.7698\n",
      "Gradient Descent(10/49): loss=70165141230085.86\n",
      "Gradient Descent(11/49): loss=4219086847194377.0\n",
      "Gradient Descent(12/49): loss=2.5371912559553456e+17\n",
      "Gradient Descent(13/49): loss=1.5257945322241894e+19\n",
      "Gradient Descent(14/49): loss=9.175730444829279e+20\n",
      "Gradient Descent(15/49): loss=5.518049788123387e+22\n",
      "Gradient Descent(16/49): loss=3.3184147363984275e+24\n",
      "Gradient Descent(17/49): loss=1.9956102655935914e+26\n",
      "Gradient Descent(18/49): loss=1.2001092939754518e+28\n",
      "Gradient Descent(19/49): loss=7.21715229163097e+29\n",
      "Gradient Descent(20/49): loss=4.34021196926168e+31\n",
      "Gradient Descent(21/49): loss=2.610093175057506e+33\n",
      "Gradient Descent(22/49): loss=1.5696437019334007e+35\n",
      "Gradient Descent(23/49): loss=9.439438310376414e+36\n",
      "Gradient Descent(24/49): loss=5.6766383036943184e+38\n",
      "Gradient Descent(25/49): loss=3.4137860083848966e+40\n",
      "Gradient Descent(26/49): loss=2.052964146660645e+42\n",
      "Gradient Descent(27/49): loss=1.234600463275094e+44\n",
      "Gradient Descent(28/49): loss=7.424573421793247e+45\n",
      "Gradient Descent(29/49): loss=4.464949765964606e+47\n",
      "Gradient Descent(30/49): loss=2.6851073159394905e+49\n",
      "Gradient Descent(31/49): loss=1.6147552998402337e+51\n",
      "Gradient Descent(32/49): loss=9.710727995427745e+52\n",
      "Gradient Descent(33/49): loss=5.8397850256638904e+54\n",
      "Gradient Descent(34/49): loss=3.5118983007273333e+56\n",
      "Gradient Descent(35/49): loss=2.1119663858258987e+58\n",
      "Gradient Descent(36/49): loss=1.2700829104119347e+60\n",
      "Gradient Descent(37/49): loss=7.637955841279285e+61\n",
      "Gradient Descent(38/49): loss=4.593272530091252e+63\n",
      "Gradient Descent(39/49): loss=2.7622773650595234e+65\n",
      "Gradient Descent(40/49): loss=1.6611634061628064e+67\n",
      "Gradient Descent(41/49): loss=9.989814552583422e+68\n",
      "Gradient Descent(42/49): loss=6.007620588364173e+70\n",
      "Gradient Descent(43/49): loss=3.6128303427217514e+72\n",
      "Gradient Descent(44/49): loss=2.172664350770066e+74\n",
      "Gradient Descent(45/49): loss=1.3065851239366127e+76\n",
      "Gradient Descent(46/49): loss=7.857470876656017e+77\n",
      "Gradient Descent(47/49): loss=4.7252832935585035e+79\n",
      "Gradient Descent(48/49): loss=2.8416652832553576e+81\n",
      "Gradient Descent(49/49): loss=1.7089052825820234e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.367789853869594\n",
      "Gradient Descent(2/49): loss=12.79651157634629\n",
      "Gradient Descent(3/49): loss=171.8902371017301\n",
      "Gradient Descent(4/49): loss=2980.176401282351\n",
      "Gradient Descent(5/49): loss=85471.76289589127\n",
      "Gradient Descent(6/49): loss=3973532.1321956757\n",
      "Gradient Descent(7/49): loss=226495276.03231356\n",
      "Gradient Descent(8/49): loss=13618144756.772863\n",
      "Gradient Descent(9/49): loss=828579057668.3082\n",
      "Gradient Descent(10/49): loss=50542027855940.18\n",
      "Gradient Descent(11/49): loss=3084644360107443.0\n",
      "Gradient Descent(12/49): loss=1.8828121186555283e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=1.149262741528867e+19\n",
      "Gradient Descent(14/49): loss=7.015100238688723e+20\n",
      "Gradient Descent(15/49): loss=4.282022079599016e+22\n",
      "Gradient Descent(16/49): loss=2.6137498597176993e+24\n",
      "Gradient Descent(17/49): loss=1.595435177839732e+26\n",
      "Gradient Descent(18/49): loss=9.738550240904585e+27\n",
      "Gradient Descent(19/49): loss=5.944419573799318e+29\n",
      "Gradient Descent(20/49): loss=3.628478902027153e+31\n",
      "Gradient Descent(21/49): loss=2.2148266925426184e+33\n",
      "Gradient Descent(22/49): loss=1.3519321485812053e+35\n",
      "Gradient Descent(23/49): loss=8.252205648998328e+36\n",
      "Gradient Descent(24/49): loss=5.037153539464869e+38\n",
      "Gradient Descent(25/49): loss=3.074682922283662e+40\n",
      "Gradient Descent(26/49): loss=1.8767891426211734e+42\n",
      "Gradient Descent(27/49): loss=1.1455937327171915e+44\n",
      "Gradient Descent(28/49): loss=6.99271415545364e+45\n",
      "Gradient Descent(29/49): loss=4.268358831180293e+47\n",
      "Gradient Descent(30/49): loss=2.6054099605239067e+49\n",
      "Gradient Descent(31/49): loss=1.5903445166816212e+51\n",
      "Gradient Descent(32/49): loss=9.707476827296293e+52\n",
      "Gradient Descent(33/49): loss=5.925452338410418e+54\n",
      "Gradient Descent(34/49): loss=3.616901285413941e+56\n",
      "Gradient Descent(35/49): loss=2.2077597053017872e+58\n",
      "Gradient Descent(36/49): loss=1.347618453401153e+60\n",
      "Gradient Descent(37/49): loss=8.225874815933181e+61\n",
      "Gradient Descent(38/49): loss=5.021081175953541e+63\n",
      "Gradient Descent(39/49): loss=3.0648723375515515e+65\n",
      "Gradient Descent(40/49): loss=1.8708007531275923e+67\n",
      "Gradient Descent(41/49): loss=1.1419384145372477e+69\n",
      "Gradient Descent(42/49): loss=6.970402061340717e+70\n",
      "Gradient Descent(43/49): loss=4.254739509435998e+72\n",
      "Gradient Descent(44/49): loss=2.597096714629707e+74\n",
      "Gradient Descent(45/49): loss=1.5852701041230826e+76\n",
      "Gradient Descent(46/49): loss=9.676502568695118e+77\n",
      "Gradient Descent(47/49): loss=5.9065456238930353e+79\n",
      "Gradient Descent(48/49): loss=3.6053606103505298e+81\n",
      "Gradient Descent(49/49): loss=2.2007152671580765e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3958602696112115\n",
      "Gradient Descent(2/49): loss=13.495485494471632\n",
      "Gradient Descent(3/49): loss=195.6341265486504\n",
      "Gradient Descent(4/49): loss=4145.701264428749\n",
      "Gradient Descent(5/49): loss=154634.24328479893\n",
      "Gradient Descent(6/49): loss=8334457.260847238\n",
      "Gradient Descent(7/49): loss=505640041.6739522\n",
      "Gradient Descent(8/49): loss=31535953463.067585\n",
      "Gradient Descent(9/49): loss=1978478342265.106\n",
      "Gradient Descent(10/49): loss=124277389718066.39\n",
      "Gradient Descent(11/49): loss=7808442920480062.0\n",
      "Gradient Descent(12/49): loss=4.906366097448704e+17\n",
      "Gradient Descent(13/49): loss=3.0829060181005107e+19\n",
      "Gradient Descent(14/49): loss=1.9371427415123406e+21\n",
      "Gradient Descent(15/49): loss=1.2172034378911023e+23\n",
      "Gradient Descent(16/49): loss=7.648297286050616e+24\n",
      "Gradient Descent(17/49): loss=4.805807375374246e+26\n",
      "Gradient Descent(18/49): loss=3.019728936419371e+28\n",
      "Gradient Descent(19/49): loss=1.8974465985406565e+30\n",
      "Gradient Descent(20/49): loss=1.1922605209908878e+32\n",
      "Gradient Descent(21/49): loss=7.491568674775967e+33\n",
      "Gradient Descent(22/49): loss=4.7073269827547985e+35\n",
      "Gradient Descent(23/49): loss=2.9578487876935163e+37\n",
      "Gradient Descent(24/49): loss=1.8585642091391868e+39\n",
      "Gradient Descent(25/49): loss=1.1678287726760048e+41\n",
      "Gradient Descent(26/49): loss=7.338051790643571e+42\n",
      "Gradient Descent(27/49): loss=4.610864652596306e+44\n",
      "Gradient Descent(28/49): loss=2.8972366850380896e+46\n",
      "Gradient Descent(29/49): loss=1.820478596005616e+48\n",
      "Gradient Descent(30/49): loss=1.1438976786499735e+50\n",
      "Gradient Descent(31/49): loss=7.187680767529122e+51\n",
      "Gradient Descent(32/49): loss=4.516379024116915e+53\n",
      "Gradient Descent(33/49): loss=2.837866642830231e+55\n",
      "Gradient Descent(34/49): loss=1.7831734315219992e+57\n",
      "Gradient Descent(35/49): loss=1.1204569795128825e+59\n",
      "Gradient Descent(36/49): loss=7.040391140572485e+60\n",
      "Gradient Descent(37/49): loss=4.423829590833436e+62\n",
      "Gradient Descent(38/49): loss=2.7797132088235574e+64\n",
      "Gradient Descent(39/49): loss=1.746632722770063e+66\n",
      "Gradient Descent(40/49): loss=1.0974966261150043e+68\n",
      "Gradient Descent(41/49): loss=6.896119765943567e+69\n",
      "Gradient Descent(42/49): loss=4.3331766763219943e+71\n",
      "Gradient Descent(43/49): loss=2.7227514523382072e+73\n",
      "Gradient Descent(44/49): loss=1.710840804557811e+75\n",
      "Gradient Descent(45/49): loss=1.0750067752333259e+77\n",
      "Gradient Descent(46/49): loss=6.754804793753182e+78\n",
      "Gradient Descent(47/49): loss=4.244381417206234e+80\n",
      "Gradient Descent(48/49): loss=2.666956953572662e+82\n",
      "Gradient Descent(49/49): loss=1.675782332703509e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4059496728140037\n",
      "Gradient Descent(2/49): loss=13.783772467285283\n",
      "Gradient Descent(3/49): loss=198.51790595452692\n",
      "Gradient Descent(4/49): loss=3977.449749428879\n",
      "Gradient Descent(5/49): loss=135752.90303221313\n",
      "Gradient Descent(6/49): loss=6875871.293916817\n",
      "Gradient Descent(7/49): loss=401267106.358686\n",
      "Gradient Descent(8/49): loss=24262340819.83831\n",
      "Gradient Descent(9/49): loss=1478696469583.1003\n",
      "Gradient Descent(10/49): loss=90277002229478.84\n",
      "Gradient Descent(11/49): loss=5513637415797578.0\n",
      "Gradient Descent(12/49): loss=3.367709061123301e+17\n",
      "Gradient Descent(13/49): loss=2.0570202481100595e+19\n",
      "Gradient Descent(14/49): loss=1.256447137636188e+21\n",
      "Gradient Descent(15/49): loss=7.674502533424553e+22\n",
      "Gradient Descent(16/49): loss=4.687662367660818e+24\n",
      "Gradient Descent(17/49): loss=2.8632708407868475e+26\n",
      "Gradient Descent(18/49): loss=1.7489143486022891e+28\n",
      "Gradient Descent(19/49): loss=1.0682543057778985e+30\n",
      "Gradient Descent(20/49): loss=6.525003715429474e+31\n",
      "Gradient Descent(21/49): loss=3.985537269514894e+33\n",
      "Gradient Descent(22/49): loss=2.4344058669916116e+35\n",
      "Gradient Descent(23/49): loss=1.4869593544129588e+37\n",
      "Gradient Descent(24/49): loss=9.082495863397033e+38\n",
      "Gradient Descent(25/49): loss=5.547678950592861e+40\n",
      "Gradient Descent(26/49): loss=3.388577567415664e+42\n",
      "Gradient Descent(27/49): loss=2.0697769342196358e+44\n",
      "Gradient Descent(28/49): loss=1.2642403699481357e+46\n",
      "Gradient Descent(29/49): loss=7.72210611965869e+47\n",
      "Gradient Descent(30/49): loss=4.716739343303561e+49\n",
      "Gradient Descent(31/49): loss=2.881031377700242e+51\n",
      "Gradient Descent(32/49): loss=1.7597626655111731e+53\n",
      "Gradient Descent(33/49): loss=1.074880566347401e+55\n",
      "Gradient Descent(34/49): loss=6.5654775757825005e+56\n",
      "Gradient Descent(35/49): loss=4.010259106700792e+58\n",
      "Gradient Descent(36/49): loss=2.449506211428934e+60\n",
      "Gradient Descent(37/49): loss=1.4961827952222074e+62\n",
      "Gradient Descent(38/49): loss=9.138833558674967e+63\n",
      "Gradient Descent(39/49): loss=5.582090576088897e+65\n",
      "Gradient Descent(40/49): loss=3.409596531067509e+67\n",
      "Gradient Descent(41/49): loss=2.08261552660306e+69\n",
      "Gradient Descent(42/49): loss=1.2720823100703184e+71\n",
      "Gradient Descent(43/49): loss=7.770005471116823e+72\n",
      "Gradient Descent(44/49): loss=4.745996744333847e+74\n",
      "Gradient Descent(45/49): loss=2.898902089703505e+76\n",
      "Gradient Descent(46/49): loss=1.7706782744257424e+78\n",
      "Gradient Descent(47/49): loss=1.081547929010608e+80\n",
      "Gradient Descent(48/49): loss=6.606202491113168e+81\n",
      "Gradient Descent(49/49): loss=4.035134290674782e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.426743820046227\n",
      "Gradient Descent(2/49): loss=13.982115768164567\n",
      "Gradient Descent(3/49): loss=198.47225255531313\n",
      "Gradient Descent(4/49): loss=3844.897543626072\n",
      "Gradient Descent(5/49): loss=126357.5401975108\n",
      "Gradient Descent(6/49): loss=6291273.341920203\n",
      "Gradient Descent(7/49): loss=365562465.2604858\n",
      "Gradient Descent(8/49): loss=22087071396.858566\n",
      "Gradient Descent(9/49): loss=1346199472907.1626\n",
      "Gradient Descent(10/49): loss=82206397881095.27\n",
      "Gradient Descent(11/49): loss=5022037961079541.0\n",
      "Gradient Descent(12/49): loss=3.06826447419017e+17\n",
      "Gradient Descent(13/49): loss=1.87462272655078e+19\n",
      "Gradient Descent(14/49): loss=1.145346123771243e+21\n",
      "Gradient Descent(15/49): loss=6.997775541898529e+22\n",
      "Gradient Descent(16/49): loss=4.275464934260507e+24\n",
      "Gradient Descent(17/49): loss=2.6122016986945108e+26\n",
      "Gradient Descent(18/49): loss=1.595989647962821e+28\n",
      "Gradient Descent(19/49): loss=9.75109602967627e+29\n",
      "Gradient Descent(20/49): loss=5.957674847115464e+31\n",
      "Gradient Descent(21/49): loss=3.639989748751563e+33\n",
      "Gradient Descent(22/49): loss=2.223942345172787e+35\n",
      "Gradient Descent(23/49): loss=1.3587729351077192e+37\n",
      "Gradient Descent(24/49): loss=8.301761478620787e+38\n",
      "Gradient Descent(25/49): loss=5.072167826367478e+40\n",
      "Gradient Descent(26/49): loss=3.098967192094391e+42\n",
      "Gradient Descent(27/49): loss=1.893391146829513e+44\n",
      "Gradient Descent(28/49): loss=1.156814452259354e+46\n",
      "Gradient Descent(29/49): loss=7.067845855289549e+47\n",
      "Gradient Descent(30/49): loss=4.318276361137121e+49\n",
      "Gradient Descent(31/49): loss=2.6383584352224907e+51\n",
      "Gradient Descent(32/49): loss=1.6119707611480092e+53\n",
      "Gradient Descent(33/49): loss=9.848736623904011e+54\n",
      "Gradient Descent(34/49): loss=6.017330799346854e+56\n",
      "Gradient Descent(35/49): loss=3.676438037838005e+58\n",
      "Gradient Descent(36/49): loss=2.2462113346883393e+60\n",
      "Gradient Descent(37/49): loss=1.37237872858304e+62\n",
      "Gradient Descent(38/49): loss=8.38488946067259e+63\n",
      "Gradient Descent(39/49): loss=5.122956936259858e+65\n",
      "Gradient Descent(40/49): loss=3.129998063047521e+67\n",
      "Gradient Descent(41/49): loss=1.9123502298720123e+69\n",
      "Gradient Descent(42/49): loss=1.1683979759817546e+71\n",
      "Gradient Descent(43/49): loss=7.13861827689192e+72\n",
      "Gradient Descent(44/49): loss=4.361516533812556e+74\n",
      "Gradient Descent(45/49): loss=2.6647770950715218e+76\n",
      "Gradient Descent(46/49): loss=1.6281118990074185e+78\n",
      "Gradient Descent(47/49): loss=9.947354923576852e+79\n",
      "Gradient Descent(48/49): loss=6.077584104380941e+81\n",
      "Gradient Descent(49/49): loss=3.713251294399608e+83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.3859697587001851\n",
      "Gradient Descent(2/49): loss=13.250467269217312\n",
      "Gradient Descent(3/49): loss=181.26636828393438\n",
      "Gradient Descent(4/49): loss=3192.912451026568\n",
      "Gradient Descent(5/49): loss=92804.45276823273\n",
      "Gradient Descent(6/49): loss=4373946.692028879\n",
      "Gradient Descent(7/49): loss=253098003.94236642\n",
      "Gradient Descent(8/49): loss=15457093603.162485\n",
      "Gradient Descent(9/49): loss=955418154048.3577\n",
      "Gradient Descent(10/49): loss=59207837989012.47\n",
      "Gradient Descent(11/49): loss=3671155925572318.5\n",
      "Gradient Descent(12/49): loss=2.2765484573877466e+17\n",
      "Gradient Descent(13/49): loss=1.4117625482963894e+19\n",
      "Gradient Descent(14/49): loss=8.754849394056087e+20\n",
      "Gradient Descent(15/49): loss=5.4292042751310595e+22\n",
      "Gradient Descent(16/49): loss=3.366850146454691e+24\n",
      "Gradient Descent(17/49): loss=2.0879082636802536e+26\n",
      "Gradient Descent(18/49): loss=1.2947891274517861e+28\n",
      "Gradient Descent(19/49): loss=8.029466224479578e+29\n",
      "Gradient Descent(20/49): loss=4.979368957738317e+31\n",
      "Gradient Descent(21/49): loss=3.087890841881699e+33\n",
      "Gradient Descent(22/49): loss=1.9149153100536494e+35\n",
      "Gradient Descent(23/49): loss=1.1875098027946505e+37\n",
      "Gradient Descent(24/49): loss=7.364187462134789e+38\n",
      "Gradient Descent(25/49): loss=4.566804993933459e+40\n",
      "Gradient Descent(26/49): loss=2.832044670216795e+42\n",
      "Gradient Descent(27/49): loss=1.7562556370937314e+44\n",
      "Gradient Descent(28/49): loss=1.0891190719062406e+46\n",
      "Gradient Descent(29/49): loss=6.754030152198283e+47\n",
      "Gradient Descent(30/49): loss=4.188423880683808e+49\n",
      "Gradient Descent(31/49): loss=2.5973965482775827e+51\n",
      "Gradient Descent(32/49): loss=1.6107416587221725e+53\n",
      "Gradient Descent(33/49): loss=9.988804723959209e+54\n",
      "Gradient Descent(34/49): loss=6.19442722382595e+56\n",
      "Gradient Descent(35/49): loss=3.8413934090871303e+58\n",
      "Gradient Descent(36/49): loss=2.3821901186635183e+60\n",
      "Gradient Descent(37/49): loss=1.4772841927706819e+62\n",
      "Gradient Descent(38/49): loss=9.161185621215362e+63\n",
      "Gradient Descent(39/49): loss=5.681190010498583e+65\n",
      "Gradient Descent(40/49): loss=3.5231160321263176e+67\n",
      "Gradient Descent(41/49): loss=2.184814546404582e+69\n",
      "Gradient Descent(42/49): loss=1.3548843009011419e+71\n",
      "Gradient Descent(43/49): loss=8.40213862476007e+72\n",
      "Gradient Descent(44/49): loss=5.21047689627309e+74\n",
      "Gradient Descent(45/49): loss=3.231209421681064e+76\n",
      "Gradient Descent(46/49): loss=2.003792461728092e+78\n",
      "Gradient Descent(47/49): loss=1.2426258114800698e+80\n",
      "Gradient Descent(48/49): loss=7.705982215467648e+81\n",
      "Gradient Descent(49/49): loss=4.7787645610205885e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4143946955400437\n",
      "Gradient Descent(2/49): loss=13.969979950685477\n",
      "Gradient Descent(3/49): loss=206.12568984960527\n",
      "Gradient Descent(4/49): loss=4433.187996157245\n",
      "Gradient Descent(5/49): loss=167567.89306115848\n",
      "Gradient Descent(6/49): loss=9162133.192804743\n",
      "Gradient Descent(7/49): loss=564411688.5948058\n",
      "Gradient Descent(8/49): loss=35754315458.59379\n",
      "Gradient Descent(9/49): loss=2278538256816.0864\n",
      "Gradient Descent(10/49): loss=145387836701344.38\n",
      "Gradient Descent(11/49): loss=9279259231373806.0\n",
      "Gradient Descent(12/49): loss=5.922733582732271e+17\n",
      "Gradient Descent(13/49): loss=3.78038488557046e+19\n",
      "Gradient Descent(14/49): loss=2.4129641319664263e+21\n",
      "Gradient Descent(15/49): loss=1.540160314253929e+23\n",
      "Gradient Descent(16/49): loss=9.830622867399837e+24\n",
      "Gradient Descent(17/49): loss=6.274746020698586e+26\n",
      "Gradient Descent(18/49): loss=4.005080688284657e+28\n",
      "Gradient Descent(19/49): loss=2.55638575357162e+30\n",
      "Gradient Descent(20/49): loss=1.6317044851173918e+32\n",
      "Gradient Descent(21/49): loss=1.041493649047264e+34\n",
      "Gradient Descent(22/49): loss=6.647705089423093e+35\n",
      "Gradient Descent(23/49): loss=4.243135135432233e+37\n",
      "Gradient Descent(24/49): loss=2.708332505031246e+39\n",
      "Gradient Descent(25/49): loss=1.7286899247110794e+41\n",
      "Gradient Descent(26/49): loss=1.10339806882877e+43\n",
      "Gradient Descent(27/49): loss=7.042832152206488e+44\n",
      "Gradient Descent(28/49): loss=4.495339091612086e+46\n",
      "Gradient Descent(29/49): loss=2.869310685225563e+48\n",
      "Gradient Descent(30/49): loss=1.831439996086517e+50\n",
      "Gradient Descent(31/49): loss=1.1689819706650537e+52\n",
      "Gradient Descent(32/49): loss=7.461444822980757e+53\n",
      "Gradient Descent(33/49): loss=4.7625335756643885e+55\n",
      "Gradient Descent(34/49): loss=3.0398571050840585e+57\n",
      "Gradient Descent(35/49): loss=1.9402973380698696e+59\n",
      "Gradient Descent(36/49): loss=1.238464056032305e+61\n",
      "Gradient Descent(37/49): loss=7.904939042022171e+62\n",
      "Gradient Descent(38/49): loss=5.045609596315748e+64\n",
      "Gradient Descent(39/49): loss=3.2205404827410536e+66\n",
      "Gradient Descent(40/49): loss=2.0556249553170826e+68\n",
      "Gradient Descent(41/49): loss=1.3120760256135694e+70\n",
      "Gradient Descent(42/49): loss=8.374793721670604e+71\n",
      "Gradient Descent(43/49): loss=5.345511122172519e+73\n",
      "Gradient Descent(44/49): loss=3.4119633398647562e+75\n",
      "Gradient Descent(45/49): loss=2.1778074287964546e+77\n",
      "Gradient Descent(46/49): loss=1.390063351943578e+79\n",
      "Gradient Descent(47/49): loss=8.872575678026953e+80\n",
      "Gradient Descent(48/49): loss=5.6632382295559116e+82\n",
      "Gradient Descent(49/49): loss=3.614764010875857e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4246556865892275\n",
      "Gradient Descent(2/49): loss=14.268235747905884\n",
      "Gradient Descent(3/49): loss=209.18095011580365\n",
      "Gradient Descent(4/49): loss=4255.010047344044\n",
      "Gradient Descent(5/49): loss=147177.12995131497\n",
      "Gradient Descent(6/49): loss=7560978.077184977\n",
      "Gradient Descent(7/49): loss=448006712.87574035\n",
      "Gradient Descent(8/49): loss=27513616289.482056\n",
      "Gradient Descent(9/49): loss=1703347104401.7761\n",
      "Gradient Descent(10/49): loss=105638414070698.72\n",
      "Gradient Descent(11/49): loss=6553999440633855.0\n",
      "Gradient Descent(12/49): loss=4.066557362333375e+17\n",
      "Gradient Descent(13/49): loss=2.5232205254915723e+19\n",
      "Gradient Descent(14/49): loss=1.5656158426014697e+21\n",
      "Gradient Descent(15/49): loss=9.714390849362713e+22\n",
      "Gradient Descent(16/49): loss=6.027622121980211e+24\n",
      "Gradient Descent(17/49): loss=3.7400420112113086e+26\n",
      "Gradient Descent(18/49): loss=2.320635581192238e+28\n",
      "Gradient Descent(19/49): loss=1.4399168497638557e+30\n",
      "Gradient Descent(20/49): loss=8.93445119896732e+31\n",
      "Gradient Descent(21/49): loss=5.543682487418744e+33\n",
      "Gradient Descent(22/49): loss=3.439765335049566e+35\n",
      "Gradient Descent(23/49): loss=2.1343187650284796e+37\n",
      "Gradient Descent(24/49): loss=1.3243102790589654e+39\n",
      "Gradient Descent(25/49): loss=8.217131123795151e+40\n",
      "Gradient Descent(26/49): loss=5.0985969808846424e+42\n",
      "Gradient Descent(27/49): loss=3.1635969758603585e+44\n",
      "Gradient Descent(28/49): loss=1.9629607641465722e+46\n",
      "Gradient Descent(29/49): loss=1.2179854105882183e+48\n",
      "Gradient Descent(30/49): loss=7.55740250901417e+49\n",
      "Gradient Descent(31/49): loss=4.689246044061436e+51\n",
      "Gradient Descent(32/49): loss=2.909601339285281e+53\n",
      "Gradient Descent(33/49): loss=1.8053605790833312e+55\n",
      "Gradient Descent(34/49): loss=1.1201970443513415e+57\n",
      "Gradient Descent(35/49): loss=6.950641510133353e+58\n",
      "Gradient Descent(36/49): loss=4.312760656350746e+60\n",
      "Gradient Descent(37/49): loss=2.6759982444570034e+62\n",
      "Gradient Descent(38/49): loss=1.6604136364005992e+64\n",
      "Gradient Descent(39/49): loss=1.0302598103925675e+66\n",
      "Gradient Descent(40/49): loss=6.392595517410111e+67\n",
      "Gradient Descent(41/49): loss=3.966502142177148e+69\n",
      "Gradient Descent(42/49): loss=2.4611504358514703e+71\n",
      "Gradient Descent(43/49): loss=1.5271040455223955e+73\n",
      "Gradient Descent(44/49): loss=9.475433650377673e+74\n",
      "Gradient Descent(45/49): loss=5.879353350281748e+76\n",
      "Gradient Descent(46/49): loss=3.6480436772507554e+78\n",
      "Gradient Descent(47/49): loss=2.2635521082418457e+80\n",
      "Gradient Descent(48/49): loss=1.4044974786561547e+82\n",
      "Gradient Descent(49/49): loss=8.714679730009231e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4458586367470585\n",
      "Gradient Descent(2/49): loss=14.474177942088971\n",
      "Gradient Descent(3/49): loss=209.15337120201502\n",
      "Gradient Descent(4/49): loss=4114.146634609428\n",
      "Gradient Descent(5/49): loss=137017.34311271485\n",
      "Gradient Descent(6/49): loss=6918611.36687857\n",
      "Gradient Descent(7/49): loss=408150220.0241124\n",
      "Gradient Descent(8/49): loss=25046916492.66192\n",
      "Gradient Descent(9/49): loss=1550718848917.3381\n",
      "Gradient Descent(10/49): loss=96194319861544.44\n",
      "Gradient Descent(11/49): loss=5969625236022988.0\n",
      "Gradient Descent(12/49): loss=3.704963087172631e+17\n",
      "Gradient Descent(13/49): loss=2.299477519237816e+19\n",
      "Gradient Descent(14/49): loss=1.4271718780473107e+21\n",
      "Gradient Descent(15/49): loss=8.857757492360124e+22\n",
      "Gradient Descent(16/49): loss=5.497577721421916e+24\n",
      "Gradient Descent(17/49): loss=3.412078295487366e+26\n",
      "Gradient Descent(18/49): loss=2.1177105611380948e+28\n",
      "Gradient Descent(19/49): loss=1.314359648655986e+30\n",
      "Gradient Descent(20/49): loss=8.157589230826331e+31\n",
      "Gradient Descent(21/49): loss=5.063017731341369e+33\n",
      "Gradient Descent(22/49): loss=3.1423681461578695e+35\n",
      "Gradient Descent(23/49): loss=1.950314632497727e+37\n",
      "Gradient Descent(24/49): loss=1.2104651615657968e+39\n",
      "Gradient Descent(25/49): loss=7.512766827208555e+40\n",
      "Gradient Descent(26/49): loss=4.662807918155831e+42\n",
      "Gradient Descent(27/49): loss=2.8939774362323957e+44\n",
      "Gradient Descent(28/49): loss=1.7961506346447742e+46\n",
      "Gradient Descent(29/49): loss=1.1147830877820697e+48\n",
      "Gradient Descent(30/49): loss=6.91891486623942e+49\n",
      "Gradient Descent(31/49): loss=4.2942329723994496e+51\n",
      "Gradient Descent(32/49): loss=2.665220945443635e+53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=1.6541726389060196e+55\n",
      "Gradient Descent(34/49): loss=1.0266642711116222e+57\n",
      "Gradient Descent(35/49): loss=6.372004353029752e+58\n",
      "Gradient Descent(36/49): loss=3.9547922935963326e+60\n",
      "Gradient Descent(37/49): loss=2.4545466730656155e+62\n",
      "Gradient Descent(38/49): loss=1.523417394135441e+64\n",
      "Gradient Descent(39/49): loss=9.455108685531397e+65\n",
      "Gradient Descent(40/49): loss=5.868324767681085e+67\n",
      "Gradient Descent(41/49): loss=3.642182943034418e+69\n",
      "Gradient Descent(42/49): loss=2.2605252973708296e+71\n",
      "Gradient Descent(43/49): loss=1.4029977900550322e+73\n",
      "Gradient Descent(44/49): loss=8.707722940277223e+74\n",
      "Gradient Descent(45/49): loss=5.404458890961953e+76\n",
      "Gradient Descent(46/49): loss=3.3542840194187967e+78\n",
      "Gradient Descent(47/49): loss=2.081840478376878e+80\n",
      "Gradient Descent(48/49): loss=1.2920968386449657e+82\n",
      "Gradient Descent(49/49): loss=8.019414829218613e+83\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4043184566591203\n",
      "Gradient Descent(2/49): loss=13.716765209625981\n",
      "Gradient Descent(3/49): loss=191.0663614930279\n",
      "Gradient Descent(4/49): loss=3418.866692076595\n",
      "Gradient Descent(5/49): loss=100700.60264748259\n",
      "Gradient Descent(6/49): loss=4811096.5120851975\n",
      "Gradient Descent(7/49): loss=282579293.2324081\n",
      "Gradient Descent(8/49): loss=17526940795.856056\n",
      "Gradient Descent(9/49): loss=1100442837783.9739\n",
      "Gradient Descent(10/49): loss=69273359205823.97\n",
      "Gradient Descent(11/49): loss=4363220008444248.0\n",
      "Gradient Descent(12/49): loss=2.74852322759804e+17\n",
      "Gradient Descent(13/49): loss=1.7314208133034996e+19\n",
      "Gradient Descent(14/49): loss=1.0907071889179568e+21\n",
      "Gradient Descent(15/49): loss=6.870909208982376e+22\n",
      "Gradient Descent(16/49): loss=4.328329840127902e+24\n",
      "Gradient Descent(17/49): loss=2.726631890054984e+26\n",
      "Gradient Descent(18/49): loss=1.7176420973420235e+28\n",
      "Gradient Descent(19/49): loss=1.0820288547655242e+30\n",
      "Gradient Descent(20/49): loss=6.816242131757692e+31\n",
      "Gradient Descent(21/49): loss=4.2938925888115973e+33\n",
      "Gradient Descent(22/49): loss=2.704938176822635e+35\n",
      "Gradient Descent(23/49): loss=1.7039761449858638e+37\n",
      "Gradient Descent(24/49): loss=1.0734199870307026e+39\n",
      "Gradient Descent(25/49): loss=6.7620105595245255e+40\n",
      "Gradient Descent(26/49): loss=4.259729403176819e+42\n",
      "Gradient Descent(27/49): loss=2.683417073747505e+44\n",
      "Gradient Descent(28/49): loss=1.6904189233967624e+46\n",
      "Gradient Descent(29/49): loss=1.0648796135843427e+48\n",
      "Gradient Descent(30/49): loss=6.708210466249056e+49\n",
      "Gradient Descent(31/49): loss=4.225838027645633e+51\n",
      "Gradient Descent(32/49): loss=2.6620671974654845e+53\n",
      "Gradient Descent(33/49): loss=1.6769695661453973e+55\n",
      "Gradient Descent(34/49): loss=1.0564071892908538e+57\n",
      "Gradient Descent(35/49): loss=6.654838418747093e+58\n",
      "Gradient Descent(36/49): loss=4.192216299603352e+60\n",
      "Gradient Descent(37/49): loss=2.640887185652914e+62\n",
      "Gradient Descent(38/49): loss=1.6636272150379373e+64\n",
      "Gradient Descent(39/49): loss=1.0480021735311702e+66\n",
      "Gradient Descent(40/49): loss=6.601891011388763e+67\n",
      "Gradient Descent(41/49): loss=4.1588620736729355e+69\n",
      "Gradient Descent(42/49): loss=2.619875686829424e+71\n",
      "Gradient Descent(43/49): loss=1.6503910187091358e+73\n",
      "Gradient Descent(44/49): loss=1.039664029987636e+75\n",
      "Gradient Descent(45/49): loss=6.54936486563995e+76\n",
      "Gradient Descent(46/49): loss=4.125773221546411e+78\n",
      "Gradient Descent(47/49): loss=2.599031360267044e+80\n",
      "Gradient Descent(48/49): loss=1.6372601325672512e+82\n",
      "Gradient Descent(49/49): loss=1.0313922266096346e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4330995335355243\n",
      "Gradient Descent(2/49): loss=14.457243667564269\n",
      "Gradient Descent(3/49): loss=217.08388038689154\n",
      "Gradient Descent(4/49): loss=4738.042286916206\n",
      "Gradient Descent(5/49): loss=181469.70472254883\n",
      "Gradient Descent(6/49): loss=10064600.694980333\n",
      "Gradient Descent(7/49): loss=629476002.599494\n",
      "Gradient Descent(8/49): loss=40497359931.320755\n",
      "Gradient Descent(9/49): loss=2621223595150.1377\n",
      "Gradient Descent(10/49): loss=169876658726963.56\n",
      "Gradient Descent(11/49): loss=1.1012320069960472e+16\n",
      "Gradient Descent(12/49): loss=7.139175562760105e+17\n",
      "Gradient Descent(13/49): loss=4.628308895088253e+19\n",
      "Gradient Descent(14/49): loss=3.0005278527532245e+21\n",
      "Gradient Descent(15/49): loss=1.945240076687586e+23\n",
      "Gradient Descent(16/49): loss=1.261097893917639e+25\n",
      "Gradient Descent(17/49): loss=8.17568973719261e+26\n",
      "Gradient Descent(18/49): loss=5.3002945535973895e+28\n",
      "Gradient Descent(19/49): loss=3.436177654116387e+30\n",
      "Gradient Descent(20/49): loss=2.227671830994336e+32\n",
      "Gradient Descent(21/49): loss=1.4441982593272418e+34\n",
      "Gradient Descent(22/49): loss=9.362728312325424e+35\n",
      "Gradient Descent(23/49): loss=6.069850928321696e+37\n",
      "Gradient Descent(24/49): loss=3.935080572995093e+39\n",
      "Gradient Descent(25/49): loss=2.5511102824144684e+41\n",
      "Gradient Descent(26/49): loss=1.6538832057731684e+43\n",
      "Gradient Descent(27/49): loss=1.0722114513017852e+45\n",
      "Gradient Descent(28/49): loss=6.951140154816775e+46\n",
      "Gradient Descent(29/49): loss=4.506419829152291e+48\n",
      "Gradient Descent(30/49): loss=2.921509166018605e+50\n",
      "Gradient Descent(31/49): loss=1.894012571113755e+52\n",
      "Gradient Descent(32/49): loss=1.2278871691597692e+54\n",
      "Gradient Descent(33/49): loss=7.960384863235719e+55\n",
      "Gradient Descent(34/49): loss=5.160712544475495e+57\n",
      "Gradient Descent(35/49): loss=3.345686725488439e+59\n",
      "Gradient Descent(36/49): loss=2.169006618493401e+61\n",
      "Gradient Descent(37/49): loss=1.40616563864969e+63\n",
      "Gradient Descent(38/49): loss=9.116163069583282e+64\n",
      "Gradient Descent(39/49): loss=5.910002835159402e+66\n",
      "Gradient Descent(40/49): loss=3.8314511538446553e+68\n",
      "Gradient Descent(41/49): loss=2.4839273945799884e+70\n",
      "Gradient Descent(42/49): loss=1.6103285814707252e+72\n",
      "Gradient Descent(43/49): loss=1.0439750155176882e+74\n",
      "Gradient Descent(44/49): loss=6.768083517649488e+75\n",
      "Gradient Descent(45/49): loss=4.3877443253909987e+77\n",
      "Gradient Descent(46/49): loss=2.84457191091033e+79\n",
      "Gradient Descent(47/49): loss=1.8441341965883635e+81\n",
      "Gradient Descent(48/49): loss=1.195551050048274e+83\n",
      "Gradient Descent(49/49): loss=7.750750004613362e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4435335156613667\n",
      "Gradient Descent(2/49): loss=14.76572225775431\n",
      "Gradient Descent(3/49): loss=220.3188231612785\n",
      "Gradient Descent(4/49): loss=4549.438263988701\n",
      "Gradient Descent(5/49): loss=159461.953997895\n",
      "Gradient Descent(6/49): loss=8308209.4136372\n",
      "Gradient Descent(7/49): loss=499761580.2161692\n",
      "Gradient Descent(8/49): loss=31170017207.435894\n",
      "Gradient Descent(9/49): loss=1959965757456.1885\n",
      "Gradient Descent(10/49): loss=123462372764125.02\n",
      "Gradient Descent(11/49): loss=7780175571412499.0\n",
      "Gradient Descent(12/49): loss=4.9032139002927366e+17\n",
      "Gradient Descent(13/49): loss=3.0901549009471984e+19\n",
      "Gradient Descent(14/49): loss=1.9475176044540794e+21\n",
      "Gradient Descent(15/49): loss=1.2273909328380634e+23\n",
      "Gradient Descent(16/49): loss=7.735430900783162e+24\n",
      "Gradient Descent(17/49): loss=4.87512919225166e+26\n",
      "Gradient Descent(18/49): loss=3.0724707074999354e+28\n",
      "Gradient Descent(19/49): loss=1.9363745850463296e+30\n",
      "Gradient Descent(20/49): loss=1.2203685216707857e+32\n",
      "Gradient Descent(21/49): loss=7.691173703280374e+33\n",
      "Gradient Descent(22/49): loss=4.8472368702379705e+35\n",
      "Gradient Descent(23/49): loss=3.054892033744405e+37\n",
      "Gradient Descent(24/49): loss=1.9252959134604494e+39\n",
      "Gradient Descent(25/49): loss=1.2133863696141212e+41\n",
      "Gradient Descent(26/49): loss=7.647169828139005e+42\n",
      "Gradient Descent(27/49): loss=4.819504145163371e+44\n",
      "Gradient Descent(28/49): loss=3.037413935777569e+46\n",
      "Gradient Descent(29/49): loss=1.914280627087789e+48\n",
      "Gradient Descent(30/49): loss=1.2064441649128294e+50\n",
      "Gradient Descent(31/49): loss=7.603417714499211e+51\n",
      "Gradient Descent(32/49): loss=4.791930088645035e+53\n",
      "Gradient Descent(33/49): loss=3.0200358360784737e+55\n",
      "Gradient Descent(34/49): loss=1.903328362993126e+57\n",
      "Gradient Descent(35/49): loss=1.1995416789749593e+59\n",
      "Gradient Descent(36/49): loss=7.559915921891969e+60\n",
      "Gradient Descent(37/49): loss=4.764513792877516e+62\n",
      "Gradient Descent(38/49): loss=3.002757162521299e+64\n",
      "Gradient Descent(39/49): loss=1.892438760603845e+66\n",
      "Gradient Descent(40/49): loss=1.1926786845555775e+68\n",
      "Gradient Descent(41/49): loss=7.516663018143654e+69\n",
      "Gradient Descent(42/49): loss=4.737254355256821e+71\n",
      "Gradient Descent(43/49): loss=2.9855773462545875e+73\n",
      "Gradient Descent(44/49): loss=1.881611461410611e+75\n",
      "Gradient Descent(45/49): loss=1.1858549557100816e+77\n",
      "Gradient Descent(46/49): loss=7.47365757927481e+78\n",
      "Gradient Descent(47/49): loss=4.7101508783429835e+80\n",
      "Gradient Descent(48/49): loss=2.9684958216814493e+82\n",
      "Gradient Descent(49/49): loss=1.870846108955249e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.465148926929648\n",
      "Gradient Descent(2/49): loss=14.979477493491155\n",
      "Gradient Descent(3/49): loss=220.3109682918288\n",
      "Gradient Descent(4/49): loss=4399.81437862132\n",
      "Gradient Descent(5/49): loss=148482.26387598386\n",
      "Gradient Descent(6/49): loss=7602884.136643704\n",
      "Gradient Descent(7/49): loss=455308593.8542528\n",
      "Gradient Descent(8/49): loss=28375583588.559635\n",
      "Gradient Descent(9/49): loss=1784341340771.678\n",
      "Gradient Descent(10/49): loss=112424575879534.2\n",
      "Gradient Descent(11/49): loss=7086453841077816.0\n",
      "Gradient Descent(12/49): loss=4.4672123584831334e+17\n",
      "Gradient Descent(13/49): loss=2.816131162082954e+19\n",
      "Gradient Descent(14/49): loss=1.7752970442149863e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=1.1191533184202422e+23\n",
      "Gradient Descent(16/49): loss=7.055182157457966e+24\n",
      "Gradient Descent(17/49): loss=4.447612014504623e+26\n",
      "Gradient Descent(18/49): loss=2.803790515577917e+28\n",
      "Gradient Descent(19/49): loss=1.767519569088142e+30\n",
      "Gradient Descent(20/49): loss=1.1142506585770912e+32\n",
      "Gradient Descent(21/49): loss=7.024276007870031e+33\n",
      "Gradient Descent(22/49): loss=4.4281287209443376e+35\n",
      "Gradient Descent(23/49): loss=2.7915081849532837e+37\n",
      "Gradient Descent(24/49): loss=1.7597767449286904e+39\n",
      "Gradient Descent(25/49): loss=1.109369554667597e+41\n",
      "Gradient Descent(26/49): loss=6.993505354415273e+42\n",
      "Gradient Descent(27/49): loss=4.408730790966296e+44\n",
      "Gradient Descent(28/49): loss=2.779279660512867e+46\n",
      "Gradient Descent(29/49): loss=1.7520678393809334e+48\n",
      "Gradient Descent(30/49): loss=1.1045098330358215e+50\n",
      "Gradient Descent(31/49): loss=6.962869495418122e+51\n",
      "Gradient Descent(32/49): loss=4.389417835870991e+53\n",
      "Gradient Descent(33/49): loss=2.7671047045390246e+55\n",
      "Gradient Descent(34/49): loss=1.7443927035856086e+57\n",
      "Gradient Descent(35/49): loss=1.09967139997675e+59\n",
      "Gradient Descent(36/49): loss=6.932367840344434e+60\n",
      "Gradient Descent(37/49): loss=4.370189483409032e+62\n",
      "Gradient Descent(38/49): loss=2.7549830823677667e+64\n",
      "Gradient Descent(39/49): loss=1.7367511896101487e+66\n",
      "Gradient Descent(40/49): loss=1.0948541622331835e+68\n",
      "Gradient Descent(41/49): loss=6.901999801297738e+69\n",
      "Gradient Descent(42/49): loss=4.351045362968619e+71\n",
      "Gradient Descent(43/49): loss=2.7429145603641896e+73\n",
      "Gradient Descent(44/49): loss=1.7291431501703799e+75\n",
      "Gradient Descent(45/49): loss=1.0900580269566475e+77\n",
      "Gradient Descent(46/49): loss=6.871764792958491e+78\n",
      "Gradient Descent(47/49): loss=4.3319851055618737e+80\n",
      "Gradient Descent(48/49): loss=2.730898905917041e+82\n",
      "Gradient Descent(49/49): loss=1.7215684386273517e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4228359477463994\n",
      "Gradient Descent(2/49): loss=14.195627343387253\n",
      "Gradient Descent(3/49): loss=201.30542870176825\n",
      "Gradient Descent(4/49): loss=3658.7392064536316\n",
      "Gradient Descent(5/49): loss=109198.51970703051\n",
      "Gradient Descent(6/49): loss=5288025.0929780435\n",
      "Gradient Descent(7/49): loss=315224426.6377911\n",
      "Gradient Descent(8/49): loss=19854525578.207367\n",
      "Gradient Descent(9/49): loss=1266086891184.6594\n",
      "Gradient Descent(10/49): loss=80950997061997.11\n",
      "Gradient Descent(11/49): loss=5178776741190131.0\n",
      "Gradient Descent(12/49): loss=3.3134819419988243e+17\n",
      "Gradient Descent(13/49): loss=2.1200845678056673e+19\n",
      "Gradient Descent(14/49): loss=1.35651354791518e+21\n",
      "Gradient Descent(15/49): loss=8.679517665947344e+22\n",
      "Gradient Descent(16/49): loss=5.553504841348453e+24\n",
      "Gradient Descent(17/49): loss=3.553356169534693e+26\n",
      "Gradient Descent(18/49): loss=2.2735804811088436e+28\n",
      "Gradient Descent(19/49): loss=1.4547284228697344e+30\n",
      "Gradient Descent(20/49): loss=9.30793874666945e+31\n",
      "Gradient Descent(21/49): loss=5.955594347307116e+33\n",
      "Gradient Descent(22/49): loss=3.8106292913866085e+35\n",
      "Gradient Descent(23/49): loss=2.4381942002162714e+37\n",
      "Gradient Descent(24/49): loss=1.5600549157094944e+39\n",
      "Gradient Descent(25/49): loss=9.981860098814504e+40\n",
      "Gradient Descent(26/49): loss=6.38679638959976e+42\n",
      "Gradient Descent(27/49): loss=4.086529736782096e+44\n",
      "Gradient Descent(28/49): loss=2.614726424784518e+46\n",
      "Gradient Descent(29/49): loss=1.6730073477576025e+48\n",
      "Gradient Descent(30/49): loss=1.0704575282217526e+50\n",
      "Gradient Descent(31/49): loss=6.849218691492813e+51\n",
      "Gradient Descent(32/49): loss=4.3824061625149735e+53\n",
      "Gradient Descent(33/49): loss=2.8040400866602393e+55\n",
      "Gradient Descent(34/49): loss=1.794137858524191e+57\n",
      "Gradient Descent(35/49): loss=1.1479617109267558e+59\n",
      "Gradient Descent(36/49): loss=7.345121688909057e+60\n",
      "Gradient Descent(37/49): loss=4.6997048866140825e+62\n",
      "Gradient Descent(38/49): loss=3.007060598412531e+64\n",
      "Gradient Descent(39/49): loss=1.9240385642681907e+66\n",
      "Gradient Descent(40/49): loss=1.231077417843027e+68\n",
      "Gradient Descent(41/49): loss=7.876929479838715e+69\n",
      "Gradient Descent(42/49): loss=5.039976944672027e+71\n",
      "Gradient Descent(43/49): loss=3.2247803751247517e+73\n",
      "Gradient Descent(44/49): loss=2.0633444521572835e+75\n",
      "Gradient Descent(45/49): loss=1.3202109393522795e+77\n",
      "Gradient Descent(46/49): loss=8.44724167389078e+78\n",
      "Gradient Descent(47/49): loss=5.404885671688615e+80\n",
      "Gradient Descent(48/49): loss=3.458263685566998e+82\n",
      "Gradient Descent(49/49): loss=2.2127364842437857e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4519747835976533\n",
      "Gradient Descent(2/49): loss=14.957503990370915\n",
      "Gradient Descent(3/49): loss=228.52518916896258\n",
      "Gradient Descent(4/49): loss=5061.159362330086\n",
      "Gradient Descent(5/49): loss=196403.7519282924\n",
      "Gradient Descent(6/49): loss=11047959.517173331\n",
      "Gradient Descent(7/49): loss=701450007.5313717\n",
      "Gradient Descent(8/49): loss=45825504049.99653\n",
      "Gradient Descent(9/49): loss=3012186819878.487\n",
      "Gradient Descent(10/49): loss=198251837755266.53\n",
      "Gradient Descent(11/49): loss=1.3051787805220204e+16\n",
      "Gradient Descent(12/49): loss=8.593050695816307e+17\n",
      "Gradient Descent(13/49): loss=5.657569384529365e+19\n",
      "Gradient Descent(14/49): loss=3.7248902433817693e+21\n",
      "Gradient Descent(15/49): loss=2.452433824617414e+23\n",
      "Gradient Descent(16/49): loss=1.6146602783242027e+25\n",
      "Gradient Descent(17/49): loss=1.063077766853367e+27\n",
      "Gradient Descent(18/49): loss=6.999208185908876e+28\n",
      "Gradient Descent(19/49): loss=4.6082155798850906e+30\n",
      "Gradient Descent(20/49): loss=3.0340076007268204e+32\n",
      "Gradient Descent(21/49): loss=1.9975632567734202e+34\n",
      "Gradient Descent(22/49): loss=1.3151776428944006e+36\n",
      "Gradient Descent(23/49): loss=8.659011055131853e+37\n",
      "Gradient Descent(24/49): loss=5.701014829290513e+39\n",
      "Gradient Descent(25/49): loss=3.753496776578319e+41\n",
      "Gradient Descent(26/49): loss=2.4712684449441274e+43\n",
      "Gradient Descent(27/49): loss=1.6270608689702386e+45\n",
      "Gradient Descent(28/49): loss=1.071242210351653e+47\n",
      "Gradient Descent(29/49): loss=7.052962154792457e+48\n",
      "Gradient Descent(30/49): loss=4.643606709691364e+50\n",
      "Gradient Descent(31/49): loss=3.057308801754777e+52\n",
      "Gradient Descent(32/49): loss=2.0129045575240903e+54\n",
      "Gradient Descent(33/49): loss=1.3252782170305134e+56\n",
      "Gradient Descent(34/49): loss=8.725512324816478e+57\n",
      "Gradient Descent(35/49): loss=5.7447986658316105e+59\n",
      "Gradient Descent(36/49): loss=3.782323659904348e+61\n",
      "Gradient Descent(37/49): loss=2.4902478050901634e+63\n",
      "Gradient Descent(38/49): loss=1.6395567086168562e+65\n",
      "Gradient Descent(39/49): loss=1.0794693585416687e+67\n",
      "Gradient Descent(40/49): loss=7.107128956908235e+68\n",
      "Gradient Descent(41/49): loss=4.6792696439629594e+70\n",
      "Gradient Descent(42/49): loss=3.0807889562255234e+72\n",
      "Gradient Descent(43/49): loss=2.0283636795854209e+74\n",
      "Gradient Descent(44/49): loss=1.3354563636523722e+76\n",
      "Gradient Descent(45/49): loss=8.792524324750974e+77\n",
      "Gradient Descent(46/49): loss=5.788918762564753e+79\n",
      "Gradient Descent(47/49): loss=3.811371933909758e+81\n",
      "Gradient Descent(48/49): loss=2.509372926863967e+83\n",
      "Gradient Descent(49/49): loss=1.6521485164053358e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4625831600304215\n",
      "Gradient Descent(2/49): loss=15.276463592918185\n",
      "Gradient Descent(3/49): loss=231.9483211289757\n",
      "Gradient Descent(4/49): loss=4861.608726229512\n",
      "Gradient Descent(5/49): loss=172664.70104804385\n",
      "Gradient Descent(6/49): loss=9122650.467636256\n",
      "Gradient Descent(7/49): loss=557024565.5272498\n",
      "Gradient Descent(8/49): loss=35278278003.5321\n",
      "Gradient Descent(9/49): loss=2252798895184.7925\n",
      "Gradient Descent(10/49): loss=144119782934169.84\n",
      "Gradient Descent(11/49): loss=9223510558091438.0\n",
      "Gradient Descent(12/49): loss=5.903454933485811e+17\n",
      "Gradient Descent(13/49): loss=3.778543180338628e+19\n",
      "Gradient Descent(14/49): loss=2.4184900128200605e+21\n",
      "Gradient Descent(15/49): loss=1.547977317640207e+23\n",
      "Gradient Descent(16/49): loss=9.907976574818127e+24\n",
      "Gradient Descent(17/49): loss=6.341695244659813e+26\n",
      "Gradient Descent(18/49): loss=4.05906278050216e+28\n",
      "Gradient Descent(19/49): loss=2.5980420145277243e+30\n",
      "Gradient Descent(20/49): loss=1.662901678833174e+32\n",
      "Gradient Descent(21/49): loss=1.0643561490789175e+34\n",
      "Gradient Descent(22/49): loss=6.812513490878187e+35\n",
      "Gradient Descent(23/49): loss=4.36041451949784e+37\n",
      "Gradient Descent(24/49): loss=2.7909250832786796e+39\n",
      "Gradient Descent(25/49): loss=1.7863583348889258e+41\n",
      "Gradient Descent(26/49): loss=1.1433757644538676e+43\n",
      "Gradient Descent(27/49): loss=7.318286108715108e+44\n",
      "Gradient Descent(28/49): loss=4.684139128538775e+46\n",
      "Gradient Descent(29/49): loss=2.9981281203776316e+48\n",
      "Gradient Descent(30/49): loss=1.918980623661137e+50\n",
      "Gradient Descent(31/49): loss=1.2282619308220744e+52\n",
      "Gradient Descent(32/49): loss=7.861608148124489e+53\n",
      "Gradient Descent(33/49): loss=5.031897604551827e+55\n",
      "Gradient Descent(34/49): loss=3.220714264260901e+57\n",
      "Gradient Descent(35/49): loss=2.0614490172912464e+59\n",
      "Gradient Descent(36/49): loss=1.3194501909241505e+61\n",
      "Gradient Descent(37/49): loss=8.445267342179508e+62\n",
      "Gradient Descent(38/49): loss=5.405474262800893e+64\n",
      "Gradient Descent(39/49): loss=3.459825583006597e+66\n",
      "Gradient Descent(40/49): loss=2.214494507393082e+68\n",
      "Gradient Descent(41/49): loss=1.4174084229449744e+70\n",
      "Gradient Descent(42/49): loss=9.07225839002146e+71\n",
      "Gradient Descent(43/49): loss=5.80678588915873e+73\n",
      "Gradient Descent(44/49): loss=3.716688933774145e+75\n",
      "Gradient Descent(45/49): loss=2.3789023556439006e+77\n",
      "Gradient Descent(46/49): loss=1.522639241143406e+79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=9.745798322362343e+80\n",
      "Gradient Descent(48/49): loss=6.23789157494949e+82\n",
      "Gradient Descent(49/49): loss=3.9926222576905085e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4846146905939934\n",
      "Gradient Descent(2/49): loss=15.498249993233314\n",
      "Gradient Descent(3/49): loss=231.9619258792085\n",
      "Gradient Descent(4/49): loss=4702.754625919995\n",
      "Gradient Descent(5/49): loss=160806.08121842612\n",
      "Gradient Descent(6/49): loss=8348756.430442068\n",
      "Gradient Descent(7/49): loss=507486844.5919311\n",
      "Gradient Descent(8/49): loss=32115620659.217464\n",
      "Gradient Descent(9/49): loss=2050932834348.3967\n",
      "Gradient Descent(10/49): loss=131234897743537.61\n",
      "Gradient Descent(11/49): loss=8401072431949921.0\n",
      "Gradient Descent(12/49): loss=5.3784954509285466e+17\n",
      "Gradient Descent(13/49): loss=3.4434652739444597e+19\n",
      "Gradient Descent(14/49): loss=2.2046138120880238e+21\n",
      "Gradient Descent(15/49): loss=1.411463836783785e+23\n",
      "Gradient Descent(16/49): loss=9.036641994976189e+24\n",
      "Gradient Descent(17/49): loss=5.7855468975198556e+26\n",
      "Gradient Descent(18/49): loss=3.704091989988977e+28\n",
      "Gradient Descent(19/49): loss=2.371478054256003e+30\n",
      "Gradient Descent(20/49): loss=1.518296030335897e+32\n",
      "Gradient Descent(21/49): loss=9.720616354961517e+33\n",
      "Gradient Descent(22/49): loss=6.223449211118751e+35\n",
      "Gradient Descent(23/49): loss=3.9844510542571175e+37\n",
      "Gradient Descent(24/49): loss=2.5509728874179662e+39\n",
      "Gradient Descent(25/49): loss=1.6332143584472913e+41\n",
      "Gradient Descent(26/49): loss=1.0456360213762791e+43\n",
      "Gradient Descent(27/49): loss=6.6944959401354926e+44\n",
      "Gradient Descent(28/49): loss=4.286030222400229e+46\n",
      "Gradient Descent(29/49): loss=2.7440535077770106e+48\n",
      "Gradient Descent(30/49): loss=1.7568307414609524e+50\n",
      "Gradient Descent(31/49): loss=1.1247791799229739e+52\n",
      "Gradient Descent(32/49): loss=7.201195731218628e+53\n",
      "Gradient Descent(33/49): loss=4.6104356201607556e+55\n",
      "Gradient Descent(34/49): loss=2.9517482097449145e+57\n",
      "Gradient Descent(35/49): loss=1.889803526511102e+59\n",
      "Gradient Descent(36/49): loss=1.2099126060357311e+61\n",
      "Gradient Descent(37/49): loss=7.746247129439729e+62\n",
      "Gradient Descent(38/49): loss=4.959394942330435e+64\n",
      "Gradient Descent(39/49): loss=3.1751631187361348e+66\n",
      "Gradient Descent(40/49): loss=2.0328408904342175e+68\n",
      "Gradient Descent(41/49): loss=1.3014896971549263e+70\n",
      "Gradient Descent(42/49): loss=8.332552930095104e+71\n",
      "Gradient Descent(43/49): loss=5.3347666512162053e+73\n",
      "Gradient Descent(44/49): loss=3.4154880817065088e+75\n",
      "Gradient Descent(45/49): loss=2.1867046112728966e+77\n",
      "Gradient Descent(46/49): loss=1.3999981679259603e+79\n",
      "Gradient Descent(47/49): loss=8.963235638192432e+80\n",
      "Gradient Descent(48/49): loss=5.738549874303139e+82\n",
      "Gradient Descent(49/49): loss=3.6740030039538048e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4415222319620222\n",
      "Gradient Descent(2/49): loss=14.687277598535779\n",
      "Gradient Descent(3/49): loss=211.99918821945056\n",
      "Gradient Descent(4/49): loss=3913.260798934707\n",
      "Gradient Descent(5/49): loss=118338.7795093869\n",
      "Gradient Descent(6/49): loss=5808001.410298106\n",
      "Gradient Descent(7/49): loss=351344405.45216805\n",
      "Gradient Descent(8/49): loss=22469560816.504124\n",
      "Gradient Descent(9/49): loss=1455086959908.2114\n",
      "Gradient Descent(10/49): loss=94483348206479.66\n",
      "Gradient Descent(11/49): loss=6138638958150359.0\n",
      "Gradient Descent(12/49): loss=3.9888007248811936e+17\n",
      "Gradient Descent(13/49): loss=2.5919342581669482e+19\n",
      "Gradient Descent(14/49): loss=1.6842558043507762e+21\n",
      "Gradient Descent(15/49): loss=1.0944417236330488e+23\n",
      "Gradient Descent(16/49): loss=7.111764067201338e+24\n",
      "Gradient Descent(17/49): loss=4.6212776611706885e+26\n",
      "Gradient Descent(18/49): loss=3.0029409393516967e+28\n",
      "Gradient Descent(19/49): loss=1.9513335853455936e+30\n",
      "Gradient Descent(20/49): loss=1.2679912260011223e+32\n",
      "Gradient Descent(21/49): loss=8.239502263838326e+33\n",
      "Gradient Descent(22/49): loss=5.3540904830643425e+35\n",
      "Gradient Descent(23/49): loss=3.47912822681023e+37\n",
      "Gradient Descent(24/49): loss=2.2607636641346082e+39\n",
      "Gradient Descent(25/49): loss=1.4690612164526388e+41\n",
      "Gradient Descent(26/49): loss=9.546070170547915e+42\n",
      "Gradient Descent(27/49): loss=6.203108126499357e+44\n",
      "Gradient Descent(28/49): loss=4.0308262710826287e+46\n",
      "Gradient Descent(29/49): loss=2.6192611987918763e+48\n",
      "Gradient Descent(30/49): loss=1.702015608242495e+50\n",
      "Gradient Descent(31/49): loss=1.1059825312714984e+52\n",
      "Gradient Descent(32/49): loss=7.186757592316051e+53\n",
      "Gradient Descent(33/49): loss=4.6700090851644865e+55\n",
      "Gradient Descent(34/49): loss=3.0346069942358028e+57\n",
      "Gradient Descent(35/49): loss=1.9719104270522969e+59\n",
      "Gradient Descent(36/49): loss=1.2813622125380893e+61\n",
      "Gradient Descent(37/49): loss=8.326387939308653e+62\n",
      "Gradient Descent(38/49): loss=5.410549447883138e+64\n",
      "Gradient Descent(39/49): loss=3.5158156863899505e+66\n",
      "Gradient Descent(40/49): loss=2.284603451041722e+68\n",
      "Gradient Descent(41/49): loss=1.484552489118368e+70\n",
      "Gradient Descent(42/49): loss=9.646733624352337e+71\n",
      "Gradient Descent(43/49): loss=6.268519995172073e+73\n",
      "Gradient Descent(44/49): loss=4.073331394854444e+75\n",
      "Gradient Descent(45/49): loss=2.6468813476044355e+77\n",
      "Gradient Descent(46/49): loss=1.719963388480175e+79\n",
      "Gradient Descent(47/49): loss=1.1176451337305218e+81\n",
      "Gradient Descent(48/49): loss=7.262542059428793e+82\n",
      "Gradient Descent(49/49): loss=4.719254401342987e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4710204457264306\n",
      "Gradient Descent(2/49): loss=15.470990275009736\n",
      "Gradient Descent(3/49): loss=240.46654133134518\n",
      "Gradient Descent(4/49): loss=5403.4727233719605\n",
      "Gradient Descent(5/49): loss=212437.79948206642\n",
      "Gradient Descent(6/49): loss=12118753.97647712\n",
      "Gradient Descent(7/49): loss=781005754.8437228\n",
      "Gradient Descent(8/49): loss=51805573305.3907\n",
      "Gradient Descent(9/49): loss=3457777412119.583\n",
      "Gradient Descent(10/49): loss=231092905038593.75\n",
      "Gradient Descent(11/49): loss=1.5448831908932772e+16\n",
      "Gradient Descent(12/49): loss=1.0328321241419658e+18\n",
      "Gradient Descent(13/49): loss=6.905085814416923e+19\n",
      "Gradient Descent(14/49): loss=4.616464759977281e+21\n",
      "Gradient Descent(15/49): loss=3.0863857143940686e+23\n",
      "Gradient Descent(16/49): loss=2.0634356234754977e+25\n",
      "Gradient Descent(17/49): loss=1.3795316158716045e+27\n",
      "Gradient Descent(18/49): loss=9.223003939972851e+28\n",
      "Gradient Descent(19/49): loss=6.166136447003562e+30\n",
      "Gradient Descent(20/49): loss=4.122435481836456e+32\n",
      "Gradient Descent(21/49): loss=2.756097671327163e+34\n",
      "Gradient Descent(22/49): loss=1.8426181337307628e+36\n",
      "Gradient Descent(23/49): loss=1.2319017653410276e+38\n",
      "Gradient Descent(24/49): loss=8.236009033398028e+39\n",
      "Gradient Descent(25/49): loss=5.5062706058744036e+41\n",
      "Gradient Descent(26/49): loss=3.6812752222792555e+43\n",
      "Gradient Descent(27/49): loss=2.4611553321970178e+45\n",
      "Gradient Descent(28/49): loss=1.6454313256837599e+47\n",
      "Gradient Descent(29/49): loss=1.1000704474530456e+49\n",
      "Gradient Descent(30/49): loss=7.354636869190825e+50\n",
      "Gradient Descent(31/49): loss=4.917019960212116e+52\n",
      "Gradient Descent(32/49): loss=3.2873254953489406e+54\n",
      "Gradient Descent(33/49): loss=2.197776091985832e+56\n",
      "Gradient Descent(34/49): loss=1.469346359932577e+58\n",
      "Gradient Descent(35/49): loss=9.823469885398058e+59\n",
      "Gradient Descent(36/49): loss=6.567584282425495e+61\n",
      "Gradient Descent(37/49): loss=4.390827661708048e+63\n",
      "Gradient Descent(38/49): loss=2.9355340907328774e+65\n",
      "Gradient Descent(39/49): loss=1.9625822422970022e+67\n",
      "Gradient Descent(40/49): loss=1.3121050339490348e+69\n",
      "Gradient Descent(41/49): loss=8.772216435115657e+70\n",
      "Gradient Descent(42/49): loss=5.864757713253272e+72\n",
      "Gradient Descent(43/49): loss=3.920945554589573e+74\n",
      "Gradient Descent(44/49): loss=2.621389457797068e+76\n",
      "Gradient Descent(45/49): loss=1.7525575384248902e+78\n",
      "Gradient Descent(46/49): loss=1.1716908055588998e+80\n",
      "Gradient Descent(47/49): loss=7.833462318533392e+81\n",
      "Gradient Descent(48/49): loss=5.237143758810247e+83\n",
      "Gradient Descent(49/49): loss=3.5013476334153894e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4818046196963919\n",
      "Gradient Descent(2/49): loss=15.800693395238811\n",
      "Gradient Descent(3/49): loss=244.08668249011404\n",
      "Gradient Descent(4/49): loss=5192.433336422992\n",
      "Gradient Descent(5/49): loss=186846.02136482534\n",
      "Gradient Descent(6/49): loss=10009758.441111013\n",
      "Gradient Descent(7/49): loss=620332579.547278\n",
      "Gradient Descent(8/49): loss=39890099097.927155\n",
      "Gradient Descent(9/49): loss=2586617517523.967\n",
      "Gradient Descent(10/49): loss=168033879253352.1\n",
      "Gradient Descent(11/49): loss=1.0920330938812454e+16\n",
      "Gradient Descent(12/49): loss=7.097620751288308e+17\n",
      "Gradient Descent(13/49): loss=4.613155360932096e+19\n",
      "Gradient Descent(14/49): loss=2.998369718154345e+21\n",
      "Gradient Descent(15/49): loss=1.9488242738577683e+23\n",
      "Gradient Descent(16/49): loss=1.266660606026505e+25\n",
      "Gradient Descent(17/49): loss=8.232805706848968e+26\n",
      "Gradient Descent(18/49): loss=5.3510064278225085e+28\n",
      "Gradient Descent(19/49): loss=3.4779479644232786e+30\n",
      "Gradient Descent(20/49): loss=2.2605321469589454e+32\n",
      "Gradient Descent(21/49): loss=1.4692587813870535e+34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=9.549615870906139e+35\n",
      "Gradient Descent(23/49): loss=6.206882302666439e+37\n",
      "Gradient Descent(24/49): loss=4.034234302194885e+39\n",
      "Gradient Descent(25/49): loss=2.6220968291951505e+41\n",
      "Gradient Descent(26/49): loss=1.7042618912676407e+43\n",
      "Gradient Descent(27/49): loss=1.1077045522071178e+45\n",
      "Gradient Descent(28/49): loss=7.199652713396655e+46\n",
      "Gradient Descent(29/49): loss=4.6794968107910977e+48\n",
      "Gradient Descent(30/49): loss=3.0414925933105543e+50\n",
      "Gradient Descent(31/49): loss=1.9768529756939448e+52\n",
      "Gradient Descent(32/49): loss=1.2848782522453656e+54\n",
      "Gradient Descent(33/49): loss=8.351213486240787e+55\n",
      "Gradient Descent(34/49): loss=5.427966935458095e+57\n",
      "Gradient Descent(35/49): loss=3.527969330561198e+59\n",
      "Gradient Descent(36/49): loss=2.2930441075595793e+61\n",
      "Gradient Descent(37/49): loss=1.4903903029047488e+63\n",
      "Gradient Descent(38/49): loss=9.686962617376447e+64\n",
      "Gradient Descent(39/49): loss=6.296152394950591e+66\n",
      "Gradient Descent(40/49): loss=4.0922564219803856e+68\n",
      "Gradient Descent(41/49): loss=2.6598089710582158e+70\n",
      "Gradient Descent(42/49): loss=1.7287733301663898e+72\n",
      "Gradient Descent(43/49): loss=1.1236360428943159e+74\n",
      "Gradient Descent(44/49): loss=7.303201263347085e+75\n",
      "Gradient Descent(45/49): loss=4.746799377809874e+77\n",
      "Gradient Descent(46/49): loss=3.0852366682345804e+79\n",
      "Gradient Descent(47/49): loss=2.0052849386297374e+81\n",
      "Gradient Descent(48/49): loss=1.3033579324714349e+83\n",
      "Gradient Descent(49/49): loss=8.471324286198512e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.504255927740097\n",
      "Gradient Descent(2/49): loss=16.030733094346555\n",
      "Gradient Descent(3/49): loss=244.1235713921951\n",
      "Gradient Descent(4/49): loss=5023.858038844682\n",
      "Gradient Descent(5/49): loss=174045.70160508214\n",
      "Gradient Descent(6/49): loss=9161234.667840155\n",
      "Gradient Descent(7/49): loss=565174396.3532978\n",
      "Gradient Descent(8/49): loss=36314097000.21262\n",
      "Gradient Descent(9/49): loss=2354836736861.9033\n",
      "Gradient Descent(10/49): loss=153010666388790.66\n",
      "Gradient Descent(11/49): loss=9946566983194060.0\n",
      "Gradient Descent(12/49): loss=6.466453467547574e+17\n",
      "Gradient Descent(13/49): loss=4.204052285973277e+19\n",
      "Gradient Descent(14/49): loss=2.733203814714487e+21\n",
      "Gradient Descent(15/49): loss=1.776954686174134e+23\n",
      "Gradient Descent(16/49): loss=1.1552627769354597e+25\n",
      "Gradient Descent(17/49): loss=7.510783228773707e+26\n",
      "Gradient Descent(18/49): loss=4.883033228724606e+28\n",
      "Gradient Descent(19/49): loss=3.174637429767113e+30\n",
      "Gradient Descent(20/49): loss=2.0639472113314255e+32\n",
      "Gradient Descent(21/49): loss=1.3418471198189165e+34\n",
      "Gradient Descent(22/49): loss=8.723835973717775e+35\n",
      "Gradient Descent(23/49): loss=5.671682934116376e+37\n",
      "Gradient Descent(24/49): loss=3.687367277662447e+39\n",
      "Gradient Descent(25/49): loss=2.3972915267515866e+41\n",
      "Gradient Descent(26/49): loss=1.5585663785242572e+43\n",
      "Gradient Descent(27/49): loss=1.0132806666020316e+45\n",
      "Gradient Descent(28/49): loss=6.587706006346887e+46\n",
      "Gradient Descent(29/49): loss=4.282907180257539e+48\n",
      "Gradient Descent(30/49): loss=2.784473669138925e+50\n",
      "Gradient Descent(31/49): loss=1.8102875658542396e+52\n",
      "Gradient Descent(32/49): loss=1.1769337621712323e+54\n",
      "Gradient Descent(33/49): loss=7.651674279080098e+55\n",
      "Gradient Descent(34/49): loss=4.9746316364589575e+57\n",
      "Gradient Descent(35/49): loss=3.2341888867535273e+59\n",
      "Gradient Descent(36/49): loss=2.1026637788693725e+61\n",
      "Gradient Descent(37/49): loss=1.3670181680103574e+63\n",
      "Gradient Descent(38/49): loss=8.887482109361445e+64\n",
      "Gradient Descent(39/49): loss=5.77807523649687e+66\n",
      "Gradient Descent(40/49): loss=3.7565367814863234e+68\n",
      "Gradient Descent(41/49): loss=2.4422611359445338e+70\n",
      "Gradient Descent(42/49): loss=1.5878027563954748e+72\n",
      "Gradient Descent(43/49): loss=1.0322882987867264e+74\n",
      "Gradient Descent(44/49): loss=6.711281533677946e+75\n",
      "Gradient Descent(45/49): loss=4.363248123341587e+77\n",
      "Gradient Descent(46/49): loss=2.8367062371484727e+79\n",
      "Gradient Descent(47/49): loss=1.8442458573074125e+81\n",
      "Gradient Descent(48/49): loss=1.1990112820475052e+83\n",
      "Gradient Descent(49/49): loss=7.795208262395944e+84\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4603773093059886\n",
      "Gradient Descent(2/49): loss=15.191941885324955\n",
      "Gradient Descent(3/49): loss=223.1636718728773\n",
      "Gradient Descent(4/49): loss=4183.194069318889\n",
      "Gradient Descent(5/49): loss=128164.34062355229\n",
      "Gradient Descent(6/49): loss=6374534.669858023\n",
      "Gradient Descent(7/49): loss=391278041.3888993\n",
      "Gradient Descent(8/49): loss=25404914744.316803\n",
      "Gradient Descent(9/49): loss=1670517444298.5537\n",
      "Gradient Descent(10/49): loss=110147206292193.05\n",
      "Gradient Descent(11/49): loss=7266924398841847.0\n",
      "Gradient Descent(12/49): loss=4.7949295680056435e+17\n",
      "Gradient Descent(13/49): loss=3.163919695422103e+19\n",
      "Gradient Descent(14/49): loss=2.0877147328635045e+21\n",
      "Gradient Descent(15/49): loss=1.3775817819080983e+23\n",
      "Gradient Descent(16/49): loss=9.089996977762964e+24\n",
      "Gradient Descent(17/49): loss=5.99805040603238e+26\n",
      "Gradient Descent(18/49): loss=3.9578240993754307e+28\n",
      "Gradient Descent(19/49): loss=2.611577192765963e+30\n",
      "Gradient Descent(20/49): loss=1.7232538046748782e+32\n",
      "Gradient Descent(21/49): loss=1.1370920545243526e+34\n",
      "Gradient Descent(22/49): loss=7.503121925434771e+35\n",
      "Gradient Descent(23/49): loss=4.950948201971845e+37\n",
      "Gradient Descent(24/49): loss=3.266891880770729e+39\n",
      "Gradient Descent(25/49): loss=2.155664354637744e+41\n",
      "Gradient Descent(26/49): loss=1.4224189166491887e+43\n",
      "Gradient Descent(27/49): loss=9.385856244681794e+44\n",
      "Gradient Descent(28/49): loss=6.193273754637399e+46\n",
      "Gradient Descent(29/49): loss=4.0866425821953816e+48\n",
      "Gradient Descent(30/49): loss=2.6965782970770776e+50\n",
      "Gradient Descent(31/49): loss=1.7793419331427598e+52\n",
      "Gradient Descent(32/49): loss=1.174101904799878e+54\n",
      "Gradient Descent(33/49): loss=7.747332073604974e+55\n",
      "Gradient Descent(34/49): loss=5.112090697863182e+57\n",
      "Gradient Descent(35/49): loss=3.3732220401673316e+59\n",
      "Gradient Descent(36/49): loss=2.225826497371608e+61\n",
      "Gradient Descent(37/49): loss=1.4687155299612865e+63\n",
      "Gradient Descent(38/49): loss=9.691345262071004e+64\n",
      "Gradient Descent(39/49): loss=6.3948512201774905e+66\n",
      "Gradient Descent(40/49): loss=4.2196538274467654e+68\n",
      "Gradient Descent(41/49): loss=2.784346001249435e+70\n",
      "Gradient Descent(42/49): loss=1.8372556071417876e+72\n",
      "Gradient Descent(43/49): loss=1.2123163444698286e+74\n",
      "Gradient Descent(44/49): loss=7.999490726035017e+75\n",
      "Gradient Descent(45/49): loss=5.278478028266347e+77\n",
      "Gradient Descent(46/49): loss=3.483013012842175e+79\n",
      "Gradient Descent(47/49): loss=2.298272263835147e+81\n",
      "Gradient Descent(48/49): loss=1.51651899640874e+83\n",
      "Gradient Descent(49/49): loss=1.0006777276382661e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4902365199218566\n",
      "Gradient Descent(2/49): loss=15.9979338880259\n",
      "Gradient Descent(3/49): loss=252.92530370330843\n",
      "Gradient Descent(4/49): loss=5765.9554492807065\n",
      "Gradient Descent(5/49): loss=229643.4846815325\n",
      "Gradient Descent(6/49): loss=13284001.559518242\n",
      "Gradient Descent(7/49): loss=868874750.553518\n",
      "Gradient Descent(8/49): loss=58511423955.790405\n",
      "Gradient Descent(9/49): loss=3965121447625.852\n",
      "Gradient Descent(10/49): loss=269060349229954.9\n",
      "Gradient Descent(11/49): loss=1.8262673649473852e+16\n",
      "Gradient Descent(12/49): loss=1.2396655624136481e+18\n",
      "Gradient Descent(13/49): loss=8.414921659605862e+19\n",
      "Gradient Descent(14/49): loss=5.712112392222976e+21\n",
      "Gradient Descent(15/49): loss=3.877427156400754e+23\n",
      "Gradient Descent(16/49): loss=2.632028581036924e+25\n",
      "Gradient Descent(17/49): loss=1.7866421053409544e+27\n",
      "Gradient Descent(18/49): loss=1.212786993159989e+29\n",
      "Gradient Descent(19/49): loss=8.232495405445235e+30\n",
      "Gradient Descent(20/49): loss=5.5882839276833356e+32\n",
      "Gradient Descent(21/49): loss=3.793371963160822e+34\n",
      "Gradient Descent(22/49): loss=2.574971321652534e+36\n",
      "Gradient Descent(23/49): loss=1.74791119134591e+38\n",
      "Gradient Descent(24/49): loss=1.1864961396435735e+40\n",
      "Gradient Descent(25/49): loss=8.054030984865152e+41\n",
      "Gradient Descent(26/49): loss=5.467140847559289e+43\n",
      "Gradient Descent(27/49): loss=3.711139068526011e+45\n",
      "Gradient Descent(28/49): loss=2.5191509730518193e+47\n",
      "Gradient Descent(29/49): loss=1.710019890887112e+49\n",
      "Gradient Descent(30/49): loss=1.1607752209019625e+51\n",
      "Gradient Descent(31/49): loss=7.879435324936385e+52\n",
      "Gradient Descent(32/49): loss=5.348623912872118e+54\n",
      "Gradient Descent(33/49): loss=3.6306888224351176e+56\n",
      "Gradient Descent(34/49): loss=2.464540700577449e+58\n",
      "Gradient Descent(35/49): loss=1.6729499998099782e+60\n",
      "Gradient Descent(36/49): loss=1.1356118814383607e+62\n",
      "Gradient Descent(37/49): loss=7.708624557879534e+63\n",
      "Gradient Descent(38/49): loss=5.232676193831262e+65\n",
      "Gradient Descent(39/49): loss=3.5519825805370174e+67\n",
      "Gradient Descent(40/49): loss=2.4111142721409053e+69\n",
      "Gradient Descent(41/49): loss=1.6366837115633088e+71\n",
      "Gradient Descent(42/49): loss=1.110994034023153e+73\n",
      "Gradient Descent(43/49): loss=7.541516634610394e+74\n",
      "Gradient Descent(44/49): loss=5.119241994860172e+76\n",
      "Gradient Descent(45/49): loss=3.474982536227484e+78\n",
      "Gradient Descent(46/49): loss=2.3588460243157485e+80\n",
      "Gradient Descent(47/49): loss=1.601203605607382e+82\n",
      "Gradient Descent(48/49): loss=1.0869098534542207e+84\n",
      "Gradient Descent(49/49): loss=7.378031284708246e+85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.501197894659278\n",
      "Gradient Descent(2/49): loss=16.338647352311778\n",
      "Gradient Descent(3/49): loss=256.7515958634211\n",
      "Gradient Descent(4/49): loss=5542.862857600464\n",
      "Gradient Descent(5/49): loss=202070.05403218497\n",
      "Gradient Descent(6/49): loss=10975386.605195971\n",
      "Gradient Descent(7/49): loss=690270138.9159135\n",
      "Gradient Descent(8/49): loss=45062630126.74859\n",
      "Gradient Descent(9/49): loss=2966777143483.2354\n",
      "Gradient Descent(10/49): loss=195687118907729.56\n",
      "Gradient Descent(11/49): loss=1.2912689025149662e+16\n",
      "Gradient Descent(12/49): loss=8.521378990481441e+17\n",
      "Gradient Descent(13/49): loss=5.623562655685938e+19\n",
      "Gradient Descent(14/49): loss=3.711205870841446e+21\n",
      "Gradient Descent(15/49): loss=2.4491701598661745e+23\n",
      "Gradient Descent(16/49): loss=1.616303676602685e+25\n",
      "Gradient Descent(17/49): loss=1.0666623879858312e+27\n",
      "Gradient Descent(18/49): loss=7.03932483410356e+28\n",
      "Gradient Descent(19/49): loss=4.645527468068942e+30\n",
      "Gradient Descent(20/49): loss=3.065766387433108e+32\n",
      "Gradient Descent(21/49): loss=2.023219883613556e+34\n",
      "Gradient Descent(22/49): loss=1.3352024193101078e+36\n",
      "Gradient Descent(23/49): loss=8.81152619633274e+37\n",
      "Gradient Descent(24/49): loss=5.815072889761423e+39\n",
      "Gradient Descent(25/49): loss=3.8375954357736025e+41\n",
      "Gradient Descent(26/49): loss=2.532580245830009e+43\n",
      "Gradient Descent(27/49): loss=1.671349366787923e+45\n",
      "Gradient Descent(28/49): loss=1.102989218391731e+47\n",
      "Gradient Descent(29/49): loss=7.279059902517538e+48\n",
      "Gradient Descent(30/49): loss=4.803738076578368e+50\n",
      "Gradient Descent(31/49): loss=3.170175794320295e+52\n",
      "Gradient Descent(32/49): loss=2.092123760014218e+54\n",
      "Gradient Descent(33/49): loss=1.380674798873239e+56\n",
      "Gradient Descent(34/49): loss=9.111616323456451e+57\n",
      "Gradient Descent(35/49): loss=6.013114173853968e+59\n",
      "Gradient Descent(36/49): loss=3.968290672503626e+61\n",
      "Gradient Descent(37/49): loss=2.618831175691202e+63\n",
      "Gradient Descent(38/49): loss=1.7282697495657527e+65\n",
      "Gradient Descent(39/49): loss=1.1405532189282213e+67\n",
      "Gradient Descent(40/49): loss=7.526959524312362e+68\n",
      "Gradient Descent(41/49): loss=4.96733679239237e+70\n",
      "Gradient Descent(42/49): loss=3.2781410248527888e+72\n",
      "Gradient Descent(43/49): loss=2.163374264310263e+74\n",
      "Gradient Descent(44/49): loss=1.4276958105212003e+76\n",
      "Gradient Descent(45/49): loss=9.421926483116286e+77\n",
      "Gradient Descent(46/49): loss=6.217900059596273e+79\n",
      "Gradient Descent(47/49): loss=4.103436937276827e+81\n",
      "Gradient Descent(48/49): loss=2.708019514115699e+83\n",
      "Gradient Descent(49/49): loss=1.7871286438480734e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.524072638367958\n",
      "Gradient Descent(2/49): loss=16.57716653203083\n",
      "Gradient Descent(3/49): loss=256.81368540889235\n",
      "Gradient Descent(4/49): loss=5364.053354719179\n",
      "Gradient Descent(5/49): loss=188261.31440929178\n",
      "Gradient Descent(6/49): loss=10045689.002486316\n",
      "Gradient Descent(7/49): loss=628904079.2736615\n",
      "Gradient Descent(8/49): loss=41023043205.23964\n",
      "Gradient Descent(9/49): loss=2700928586003.5537\n",
      "Gradient Descent(10/49): loss=178191187729508.3\n",
      "Gradient Descent(11/49): loss=1.1761238385475244e+16\n",
      "Gradient Descent(12/49): loss=7.763580839858314e+17\n",
      "Gradient Descent(13/49): loss=5.124839815323541e+19\n",
      "Gradient Descent(14/49): loss=3.382988427482483e+21\n",
      "Gradient Descent(15/49): loss=2.2331668099236306e+23\n",
      "Gradient Descent(16/49): loss=1.4741507980964748e+25\n",
      "Gradient Descent(17/49): loss=9.731116678053569e+26\n",
      "Gradient Descent(18/49): loss=6.423673406066361e+28\n",
      "Gradient Descent(19/49): loss=4.2403746133187703e+30\n",
      "Gradient Descent(20/49): loss=2.7991424428780625e+32\n",
      "Gradient Descent(21/49): loss=1.8477609010650602e+34\n",
      "Gradient Descent(22/49): loss=1.2197379794912257e+36\n",
      "Gradient Descent(23/49): loss=8.051695096283088e+37\n",
      "Gradient Descent(24/49): loss=5.315059054783235e+39\n",
      "Gradient Descent(25/49): loss=3.508559678182776e+41\n",
      "Gradient Descent(26/49): loss=2.316059123424433e+43\n",
      "Gradient Descent(27/49): loss=1.5288694949535775e+45\n",
      "Gradient Descent(28/49): loss=1.009232410761405e+47\n",
      "Gradient Descent(29/49): loss=6.662112510539601e+48\n",
      "Gradient Descent(30/49): loss=4.39777227027456e+50\n",
      "Gradient Descent(31/49): loss=2.9030432780291687e+52\n",
      "Gradient Descent(32/49): loss=1.9163475860436285e+54\n",
      "Gradient Descent(33/49): loss=1.2650132012597411e+56\n",
      "Gradient Descent(34/49): loss=8.350564433173846e+57\n",
      "Gradient Descent(35/49): loss=5.512347719624201e+59\n",
      "Gradient Descent(36/49): loss=3.63879323669839e+61\n",
      "Gradient Descent(37/49): loss=2.4020284809508015e+63\n",
      "Gradient Descent(38/49): loss=1.5856193105750706e+65\n",
      "Gradient Descent(39/49): loss=1.0466939164157314e+67\n",
      "Gradient Descent(40/49): loss=6.909402196069265e+68\n",
      "Gradient Descent(41/49): loss=4.5610123416522905e+70\n",
      "Gradient Descent(42/49): loss=3.0108007886036327e+72\n",
      "Gradient Descent(43/49): loss=1.9874801271360664e+74\n",
      "Gradient Descent(44/49): loss=1.311968985365125e+76\n",
      "Gradient Descent(45/49): loss=8.660527444067001e+77\n",
      "Gradient Descent(46/49): loss=5.7169595048439635e+79\n",
      "Gradient Descent(47/49): loss=3.7738609098706135e+81\n",
      "Gradient Descent(48/49): loss=2.491188918686972e+83\n",
      "Gradient Descent(49/49): loss=1.6444756117950408e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4794011797782987\n",
      "Gradient Descent(2/49): loss=15.709848096227402\n",
      "Gradient Descent(3/49): loss=234.8153322503139\n",
      "Gradient Descent(4/49): loss=4469.334512591524\n",
      "Gradient Descent(5/49): loss=138720.66404699453\n",
      "Gradient Descent(6/49): loss=6991389.902709296\n",
      "Gradient Descent(7/49): loss=435394200.4337737\n",
      "Gradient Descent(8/49): loss=28696917828.789333\n",
      "Gradient Descent(9/49): loss=1915829093285.3943\n",
      "Gradient Descent(10/49): loss=128258062154223.56\n",
      "Gradient Descent(11/49): loss=8591548823511716.0\n",
      "Gradient Descent(12/49): loss=5.7559059577606637e+17\n",
      "Gradient Descent(13/49): loss=3.856273108249612e+19\n",
      "Gradient Descent(14/49): loss=2.5835948964806294e+21\n",
      "Gradient Descent(15/49): loss=1.7309383327237315e+23\n",
      "Gradient Descent(16/49): loss=1.1596819294232478e+25\n",
      "Gradient Descent(17/49): loss=7.769556136819974e+26\n",
      "Gradient Descent(18/49): loss=5.20539311447907e+28\n",
      "Gradient Descent(19/49): loss=3.4874730383930316e+30\n",
      "Gradient Descent(20/49): loss=2.336512907454685e+32\n",
      "Gradient Descent(21/49): loss=1.565400651778173e+34\n",
      "Gradient Descent(22/49): loss=1.0487762309518277e+36\n",
      "Gradient Descent(23/49): loss=7.026517980340314e+37\n",
      "Gradient Descent(24/49): loss=4.707577600542662e+39\n",
      "Gradient Descent(25/49): loss=3.1539500684610914e+41\n",
      "Gradient Descent(26/49): loss=2.113061510276379e+43\n",
      "Gradient Descent(27/49): loss=1.4156942403309209e+45\n",
      "Gradient Descent(28/49): loss=9.484769716163984e+46\n",
      "Gradient Descent(29/49): loss=6.354539985105405e+48\n",
      "Gradient Descent(30/49): loss=4.257370461349932e+50\n",
      "Gradient Descent(31/49): loss=2.8523234235143554e+52\n",
      "Gradient Descent(32/49): loss=1.9109797905040942e+54\n",
      "Gradient Descent(33/49): loss=1.2803049365333123e+56\n",
      "Gradient Descent(34/49): loss=8.577697883864954e+57\n",
      "Gradient Descent(35/49): loss=5.746826313587985e+59\n",
      "Gradient Descent(36/49): loss=3.8502186863762323e+61\n",
      "Gradient Descent(37/49): loss=2.5795427117520797e+63\n",
      "Gradient Descent(38/49): loss=1.7282240682323198e+65\n",
      "Gradient Descent(39/49): loss=1.1578635300009647e+67\n",
      "Gradient Descent(40/49): loss=7.757373472281036e+68\n",
      "Gradient Descent(41/49): loss=5.197231075099075e+70\n",
      "Gradient Descent(42/49): loss=3.4820046945647597e+72\n",
      "Gradient Descent(43/49): loss=2.3328492648828827e+74\n",
      "Gradient Descent(44/49): loss=1.5629461100840926e+76\n",
      "Gradient Descent(45/49): loss=1.0471317542025782e+78\n",
      "Gradient Descent(46/49): loss=7.015500429508601e+79\n",
      "Gradient Descent(47/49): loss=4.700196138537932e+81\n",
      "Gradient Descent(48/49): loss=3.1490046879342106e+83\n",
      "Gradient Descent(49/49): loss=2.1097482386589262e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5096230061839309\n",
      "Gradient Descent(2/49): loss=16.538568206607017\n",
      "Gradient Descent(3/49): loss=265.91929244004183\n",
      "Gradient Descent(4/49): loss=6149.621533580743\n",
      "Gradient Descent(5/49): loss=248096.50605691527\n",
      "Gradient Descent(6/49): loss=14551223.12398063\n",
      "Gradient Descent(7/49): loss=965852700.2835723\n",
      "Gradient Descent(8/49): loss=66024620456.27413\n",
      "Gradient Descent(9/49): loss=4542209541646.826\n",
      "Gradient Descent(10/49): loss=312906174941208.9\n",
      "Gradient Descent(11/49): loss=2.1561776697224176e+16\n",
      "Gradient Descent(12/49): loss=1.48587044582856e+18\n",
      "Gradient Descent(13/49): loss=1.0239595864296284e+20\n",
      "Gradient Descent(14/49): loss=7.056443028994058e+21\n",
      "Gradient Descent(15/49): loss=4.862830200944473e+23\n",
      "Gradient Descent(16/49): loss=3.351138844930295e+25\n",
      "Gradient Descent(17/49): loss=2.3093818561763867e+27\n",
      "Gradient Descent(18/49): loss=1.5914722821668976e+29\n",
      "Gradient Descent(19/49): loss=1.0967367830083434e+31\n",
      "Gradient Descent(20/49): loss=7.557980022392761e+32\n",
      "Gradient Descent(21/49): loss=5.20845684275972e+34\n",
      "Gradient Descent(22/49): loss=3.589321829747229e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=2.473521733301302e+38\n",
      "Gradient Descent(24/49): loss=1.704586564071769e+40\n",
      "Gradient Descent(25/49): loss=1.1746876185867561e+42\n",
      "Gradient Descent(26/49): loss=8.095165304863471e+43\n",
      "Gradient Descent(27/49): loss=5.5786491894675205e+45\n",
      "Gradient Descent(28/49): loss=3.844433758560785e+47\n",
      "Gradient Descent(29/49): loss=2.649327896772231e+49\n",
      "Gradient Descent(30/49): loss=1.8257404719188354e+51\n",
      "Gradient Descent(31/49): loss=1.2581788290016715e+53\n",
      "Gradient Descent(32/49): loss=8.670531163086426e+54\n",
      "Gradient Descent(33/49): loss=5.9751530479737605e+56\n",
      "Gradient Descent(34/49): loss=4.117677830247278e+58\n",
      "Gradient Descent(35/49): loss=2.8376295264035375e+60\n",
      "Gradient Descent(36/49): loss=1.955505423461845e+62\n",
      "Gradient Descent(37/49): loss=1.347604197661169e+64\n",
      "Gradient Descent(38/49): loss=9.286791290709156e+65\n",
      "Gradient Descent(39/49): loss=6.399838515409244e+67\n",
      "Gradient Descent(40/49): loss=4.410342791303039e+69\n",
      "Gradient Descent(41/49): loss=3.0393147405149806e+71\n",
      "Gradient Descent(42/49): loss=2.0944934507420787e+73\n",
      "Gradient Descent(43/49): loss=1.4433854963168545e+75\n",
      "Gradient Descent(44/49): loss=9.946852257951667e+76\n",
      "Gradient Descent(45/49): loss=6.854708606535654e+78\n",
      "Gradient Descent(46/49): loss=4.723808986199875e+80\n",
      "Gradient Descent(47/49): loss=3.255334780653769e+82\n",
      "Gradient Descent(48/49): loss=2.2433600861281136e+84\n",
      "Gradient Descent(49/49): loss=1.5459744742511804e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5207629849190798\n",
      "Gradient Descent(2/49): loss=16.89056319748564\n",
      "Gradient Descent(3/49): loss=269.96120779621094\n",
      "Gradient Descent(4/49): loss=5913.8882314460625\n",
      "Gradient Descent(5/49): loss=218404.59813439188\n",
      "Gradient Descent(6/49): loss=12025809.681239262\n",
      "Gradient Descent(7/49): loss=767473174.3362352\n",
      "Gradient Descent(8/49): loss=50858995931.3453\n",
      "Gradient Descent(9/49): loss=3399284115829.8774\n",
      "Gradient Descent(10/49): loss=227628918293410.78\n",
      "Gradient Descent(11/49): loss=1.5249210972349164e+16\n",
      "Gradient Descent(12/49): loss=1.0216608241866803e+18\n",
      "Gradient Descent(13/49): loss=6.845020358973458e+19\n",
      "Gradient Descent(14/49): loss=4.586111814735024e+21\n",
      "Gradient Descent(15/49): loss=3.072663123581934e+23\n",
      "Gradient Descent(16/49): loss=2.0586634221953217e+25\n",
      "Gradient Descent(17/49): loss=1.3792905728034336e+27\n",
      "Gradient Descent(18/49): loss=9.24115366685415e+28\n",
      "Gradient Descent(19/49): loss=6.191510546483831e+30\n",
      "Gradient Descent(20/49): loss=4.148270253652292e+32\n",
      "Gradient Descent(21/49): loss=2.7793130561523692e+34\n",
      "Gradient Descent(22/49): loss=1.862120978611753e+36\n",
      "Gradient Descent(23/49): loss=1.2476084805659708e+38\n",
      "Gradient Descent(24/49): loss=8.358892567454359e+39\n",
      "Gradient Descent(25/49): loss=5.600401571698655e+41\n",
      "Gradient Descent(26/49): loss=3.7522312329272244e+43\n",
      "Gradient Descent(27/49): loss=2.5139695868424412e+45\n",
      "Gradient Descent(28/49): loss=1.684342646078978e+47\n",
      "Gradient Descent(29/49): loss=1.1284981983268723e+49\n",
      "Gradient Descent(30/49): loss=7.560861720099734e+50\n",
      "Gradient Descent(31/49): loss=5.065726293159038e+52\n",
      "Gradient Descent(32/49): loss=3.39400240702506e+54\n",
      "Gradient Descent(33/49): loss=2.2739586926455225e+56\n",
      "Gradient Descent(34/49): loss=1.52353696778624e+58\n",
      "Gradient Descent(35/49): loss=1.0207594798086703e+60\n",
      "Gradient Descent(36/49): loss=6.839019581738785e+61\n",
      "Gradient Descent(37/49): loss=4.5820969351343025e+63\n",
      "Gradient Descent(38/49): loss=3.0699740031493777e+65\n",
      "Gradient Descent(39/49): loss=2.056861850247344e+67\n",
      "Gradient Descent(40/49): loss=1.378083549457693e+69\n",
      "Gradient Descent(41/49): loss=9.233066717910631e+70\n",
      "Gradient Descent(42/49): loss=6.1860923491131426e+72\n",
      "Gradient Descent(43/49): loss=4.144640098562544e+74\n",
      "Gradient Descent(44/49): loss=2.776880876839089e+76\n",
      "Gradient Descent(45/49): loss=1.8604914349086465e+78\n",
      "Gradient Descent(46/49): loss=1.246516697291162e+80\n",
      "Gradient Descent(47/49): loss=8.351577693245146e+81\n",
      "Gradient Descent(48/49): loss=5.59550065537696e+83\n",
      "Gradient Descent(49/49): loss=3.7489476520881924e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5440648224775761\n",
      "Gradient Descent(2/49): loss=17.137792123654563\n",
      "Gradient Descent(3/49): loss=270.0505095013224\n",
      "Gradient Descent(4/49): loss=5724.308681071788\n",
      "Gradient Descent(5/49): loss=203516.55347258828\n",
      "Gradient Descent(6/49): loss=11007876.657983977\n",
      "Gradient Descent(7/49): loss=699255602.6628765\n",
      "Gradient Descent(8/49): loss=46299930081.76125\n",
      "Gradient Descent(9/49): loss=3094676411813.0366\n",
      "Gradient Descent(10/49): loss=207276737205867.38\n",
      "Gradient Descent(11/49): loss=1.3889374838566348e+16\n",
      "Gradient Descent(12/49): loss=9.308030299732602e+17\n",
      "Gradient Descent(13/49): loss=6.2379551779278774e+19\n",
      "Gradient Descent(14/49): loss=4.1805050839684864e+21\n",
      "Gradient Descent(15/49): loss=2.8016617958107152e+23\n",
      "Gradient Descent(16/49): loss=1.8775986200453264e+25\n",
      "Gradient Descent(17/49): loss=1.258316316259543e+27\n",
      "Gradient Descent(18/49): loss=8.432899085802424e+28\n",
      "Gradient Descent(19/49): loss=5.6515032220312e+30\n",
      "Gradient Descent(20/49): loss=3.78748617288808e+32\n",
      "Gradient Descent(21/49): loss=2.5382718451766483e+34\n",
      "Gradient Descent(22/49): loss=1.701081843226628e+36\n",
      "Gradient Descent(23/49): loss=1.1400195148007232e+38\n",
      "Gradient Descent(24/49): loss=7.640105614570629e+39\n",
      "Gradient Descent(25/49): loss=5.120194263692948e+41\n",
      "Gradient Descent(26/49): loss=3.431417132239389e+43\n",
      "Gradient Descent(27/49): loss=2.2996439058805032e+45\n",
      "Gradient Descent(28/49): loss=1.5411597861908335e+47\n",
      "Gradient Descent(29/49): loss=1.0328440331557529e+49\n",
      "Gradient Descent(30/49): loss=6.9218442265622e+50\n",
      "Gradient Descent(31/49): loss=4.6388347087024497e+52\n",
      "Gradient Descent(32/49): loss=3.108822844074633e+54\n",
      "Gradient Descent(33/49): loss=2.0834498495298963e+56\n",
      "Gradient Descent(34/49): loss=1.3962723169574057e+58\n",
      "Gradient Descent(35/49): loss=9.357443297910223e+59\n",
      "Gradient Descent(36/49): loss=6.271108007384216e+61\n",
      "Gradient Descent(37/49): loss=4.2027287142697425e+63\n",
      "Gradient Descent(38/49): loss=2.8165562807959214e+65\n",
      "Gradient Descent(39/49): loss=1.8875806225502723e+67\n",
      "Gradient Descent(40/49): loss=1.2650060042898973e+69\n",
      "Gradient Descent(41/49): loss=8.477731609299075e+70\n",
      "Gradient Descent(42/49): loss=5.681548782818046e+72\n",
      "Gradient Descent(43/49): loss=3.807621903969405e+74\n",
      "Gradient Descent(44/49): loss=2.5517662732091322e+76\n",
      "Gradient Descent(45/49): loss=1.7101254476710581e+78\n",
      "Gradient Descent(46/49): loss=1.1460802964113786e+80\n",
      "Gradient Descent(47/49): loss=7.680723350507497e+81\n",
      "Gradient Descent(48/49): loss=5.147415183015644e+83\n",
      "Gradient Descent(49/49): loss=3.449659863688461e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.4985938433789532\n",
      "Gradient Descent(2/49): loss=16.241226105934924\n",
      "Gradient Descent(3/49): loss=246.9710500084221\n",
      "Gradient Descent(4/49): loss=4772.511647468312\n",
      "Gradient Descent(5/49): loss=150055.83759273044\n",
      "Gradient Descent(6/49): loss=7662604.440659617\n",
      "Gradient Descent(7/49): loss=484094208.43362665\n",
      "Gradient Descent(8/49): loss=32385696779.359787\n",
      "Gradient Descent(9/49): loss=2194891660640.1506\n",
      "Gradient Descent(10/49): loss=149175154717632.94\n",
      "Gradient Descent(11/49): loss=1.014478805521901e+16\n",
      "Gradient Descent(12/49): loss=6.899949371524785e+17\n",
      "Gradient Descent(13/49): loss=4.693112180293201e+19\n",
      "Gradient Descent(14/49): loss=3.1921152464945986e+21\n",
      "Gradient Descent(15/49): loss=2.1711845820251182e+23\n",
      "Gradient Descent(16/49): loss=1.476777440944702e+25\n",
      "Gradient Descent(17/49): loss=1.0044616918703495e+27\n",
      "Gradient Descent(18/49): loss=6.832060642405386e+28\n",
      "Gradient Descent(19/49): loss=4.6469719179730156e+30\n",
      "Gradient Descent(20/49): loss=3.1607371698295384e+32\n",
      "Gradient Descent(21/49): loss=2.1498428728067372e+34\n",
      "Gradient Descent(22/49): loss=1.4622615325306209e+36\n",
      "Gradient Descent(23/49): loss=9.94588402983817e+37\n",
      "Gradient Descent(24/49): loss=6.764905383506723e+39\n",
      "Gradient Descent(25/49): loss=4.601294838198017e+41\n",
      "Gradient Descent(26/49): loss=3.129668929242891e+43\n",
      "Gradient Descent(27/49): loss=2.1287111456879124e+45\n",
      "Gradient Descent(28/49): loss=1.447888337145056e+47\n",
      "Gradient Descent(29/49): loss=9.848121672530819e+48\n",
      "Gradient Descent(30/49): loss=6.698410228803065e+50\n",
      "Gradient Descent(31/49): loss=4.5560667389481945e+52\n",
      "Gradient Descent(32/49): loss=3.098906071845507e+54\n",
      "Gradient Descent(33/49): loss=2.1077871313926153e+56\n",
      "Gradient Descent(34/49): loss=1.4336564220607546e+58\n",
      "Gradient Descent(35/49): loss=9.751320263342228e+59\n",
      "Gradient Descent(36/49): loss=6.632568683477559e+61\n",
      "Gradient Descent(37/49): loss=4.511283206072322e+63\n",
      "Gradient Descent(38/49): loss=3.068445595759626e+65\n",
      "Gradient Descent(39/49): loss=2.087068788202718e+67\n",
      "Gradient Descent(40/49): loss=1.419564398570244e+69\n",
      "Gradient Descent(41/49): loss=9.655470356697702e+70\n",
      "Gradient Descent(42/49): loss=6.567374322923865e+72\n",
      "Gradient Descent(43/49): loss=4.4669398697373295e+74\n",
      "Gradient Descent(44/49): loss=3.038284528749816e+76\n",
      "Gradient Descent(45/49): loss=2.066554094488742e+78\n",
      "Gradient Descent(46/49): loss=1.4056108916189383e+80\n",
      "Gradient Descent(47/49): loss=9.56056259987056e+81\n",
      "Gradient Descent(48/49): loss=6.502820785684654e+83\n",
      "Gradient Descent(49/49): loss=4.423032403062116e+85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5291799045126535\n",
      "Gradient Descent(2/49): loss=17.093128618581723\n",
      "Gradient Descent(3/49): loss=279.4667807202859\n",
      "Gradient Descent(4/49): loss=6555.52725375776\n",
      "Gradient Descent(5/49): loss=267876.81973822066\n",
      "Gradient Descent(6/49): loss=15928474.775572892\n",
      "Gradient Descent(7/49): loss=1072804592.5577391\n",
      "Gradient Descent(8/49): loss=74435172252.86412\n",
      "Gradient Descent(9/49): loss=5197993968519.247\n",
      "Gradient Descent(10/49): loss=363485742006318.75\n",
      "Gradient Descent(11/49): loss=2.542520266831724e+16\n",
      "Gradient Descent(12/49): loss=1.7785575678270285e+18\n",
      "Gradient Descent(13/49): loss=1.2441622236213333e+20\n",
      "Gradient Descent(14/49): loss=8.703366488737018e+21\n",
      "Gradient Descent(15/49): loss=6.088324342963012e+23\n",
      "Gradient Descent(16/49): loss=4.259006882053912e+25\n",
      "Gradient Descent(17/49): loss=2.9793320888674854e+27\n",
      "Gradient Descent(18/49): loss=2.0841524771272446e+29\n",
      "Gradient Descent(19/49): loss=1.4579413852709518e+31\n",
      "Gradient Descent(20/49): loss=1.0198836729654158e+33\n",
      "Gradient Descent(21/49): loss=7.1344617619207335e+34\n",
      "Gradient Descent(22/49): loss=4.990818657273998e+36\n",
      "Gradient Descent(23/49): loss=3.491261387480509e+38\n",
      "Gradient Descent(24/49): loss=2.4422658711418093e+40\n",
      "Gradient Descent(25/49): loss=1.708454888749814e+42\n",
      "Gradient Descent(26/49): loss=1.1951270913549818e+44\n",
      "Gradient Descent(27/49): loss=8.360353989421419e+45\n",
      "Gradient Descent(28/49): loss=5.848375401581055e+47\n",
      "Gradient Descent(29/49): loss=4.091153901030545e+49\n",
      "Gradient Descent(30/49): loss=2.8619127693808425e+51\n",
      "Gradient Descent(31/49): loss=2.002013343345955e+53\n",
      "Gradient Descent(32/49): loss=1.4004820376836302e+55\n",
      "Gradient Descent(33/49): loss=9.796887440303028e+56\n",
      "Gradient Descent(34/49): loss=6.853283436374023e+58\n",
      "Gradient Descent(35/49): loss=4.794124067003363e+60\n",
      "Gradient Descent(36/49): loss=3.3536662802875454e+62\n",
      "Gradient Descent(37/49): loss=2.346013028104146e+64\n",
      "Gradient Descent(38/49): loss=1.6411224815018334e+66\n",
      "Gradient Descent(39/49): loss=1.1480255936460851e+68\n",
      "Gradient Descent(40/49): loss=8.030861672556834e+69\n",
      "Gradient Descent(41/49): loss=5.617883395692377e+71\n",
      "Gradient Descent(42/49): loss=3.929916257360694e+73\n",
      "Gradient Descent(43/49): loss=2.749121101678649e+75\n",
      "Gradient Descent(44/49): loss=1.923111419369161e+77\n",
      "Gradient Descent(45/49): loss=1.345287237091789e+79\n",
      "Gradient Descent(46/49): loss=9.410779490227088e+80\n",
      "Gradient Descent(47/49): loss=6.583186710752573e+82\n",
      "Gradient Descent(48/49): loss=4.605181463834801e+84\n",
      "Gradient Descent(49/49): loss=3.221493973459464e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5404998904757967\n",
      "Gradient Descent(2/49): loss=17.456680709863427\n",
      "Gradient Descent(3/49): loss=283.7341306122529\n",
      "Gradient Descent(4/49): loss=6306.541929620905\n",
      "Gradient Descent(5/49): loss=235921.2909012007\n",
      "Gradient Descent(6/49): loss=13167750.635683527\n",
      "Gradient Descent(7/49): loss=852633110.8444439\n",
      "Gradient Descent(8/49): loss=57348868724.60498\n",
      "Gradient Descent(9/49): loss=3890868834487.7656\n",
      "Gradient Descent(10/49): loss=264484329686307.38\n",
      "Gradient Descent(11/49): loss=1.7986062529553582e+16\n",
      "Gradient Descent(12/49): loss=1.2232419760425682e+18\n",
      "Gradient Descent(13/49): loss=8.319504416858158e+19\n",
      "Gradient Descent(14/49): loss=5.658280452191326e+21\n",
      "Gradient Descent(15/49): loss=3.848326452856633e+23\n",
      "Gradient Descent(16/49): loss=2.6173357408032256e+25\n",
      "Gradient Descent(17/49): loss=1.7801105976487443e+27\n",
      "Gradient Descent(18/49): loss=1.2106944184221232e+29\n",
      "Gradient Descent(19/49): loss=8.234212975586033e+30\n",
      "Gradient Descent(20/49): loss=5.60027884239558e+32\n",
      "Gradient Descent(21/49): loss=3.808879271035102e+34\n",
      "Gradient Descent(22/49): loss=2.5905069568754062e+36\n",
      "Gradient Descent(23/49): loss=1.7618637441955988e+38\n",
      "Gradient Descent(24/49): loss=1.1982843145338397e+40\n",
      "Gradient Descent(25/49): loss=8.149808991692041e+41\n",
      "Gradient Descent(26/49): loss=5.542873740019373e+43\n",
      "Gradient Descent(27/49): loss=3.769836732261732e+45\n",
      "Gradient Descent(28/49): loss=2.563953222549911e+47\n",
      "Gradient Descent(29/49): loss=1.7438039348404573e+49\n",
      "Gradient Descent(30/49): loss=1.1860014201588485e+51\n",
      "Gradient Descent(31/49): loss=8.066270183909822e+52\n",
      "Gradient Descent(32/49): loss=5.486057063162355e+54\n",
      "Gradient Descent(33/49): loss=3.7311943951877553e+56\n",
      "Gradient Descent(34/49): loss=2.5376716746463334e+58\n",
      "Gradient Descent(35/49): loss=1.725929245768536e+60\n",
      "Gradient Descent(36/49): loss=1.1738444303730888e+62\n",
      "Gradient Descent(37/49): loss=7.983587682381178e+63\n",
      "Gradient Descent(38/49): loss=5.429822780009348e+65\n",
      "Gradient Descent(39/49): loss=3.6929481575525746e+67\n",
      "Gradient Descent(40/49): loss=2.511659523139569e+69\n",
      "Gradient Descent(41/49): loss=1.7082377794218514e+71\n",
      "Gradient Descent(42/49): loss=1.1618120546038568e+73\n",
      "Gradient Descent(43/49): loss=7.901752709623989e+74\n",
      "Gradient Descent(44/49): loss=5.3741649207916644e+76\n",
      "Gradient Descent(45/49): loss=3.655093959178301e+78\n",
      "Gradient Descent(46/49): loss=2.4859140066087876e+80\n",
      "Gradient Descent(47/49): loss=1.6907276576942276e+82\n",
      "Gradient Descent(48/49): loss=1.149903015507645e+84\n",
      "Gradient Descent(49/49): loss=7.820756578128652e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5642324800689513\n",
      "Gradient Descent(2/49): loss=17.712853768754904\n",
      "Gradient Descent(3/49): loss=283.8527541467185\n",
      "Gradient Descent(4/49): loss=6105.63282372086\n",
      "Gradient Descent(5/49): loss=219878.66525928598\n",
      "Gradient Descent(6/49): loss=12053966.565367943\n",
      "Gradient Descent(7/49): loss=776859276.4084073\n",
      "Gradient Descent(8/49): loss=52208189613.46068\n",
      "Gradient Descent(9/49): loss=3542207406248.7505\n",
      "Gradient Descent(10/49): loss=240836460200437.6\n",
      "Gradient Descent(11/49): loss=1.6382131421560338e+16\n",
      "Gradient Descent(12/49): loss=1.1144543662575515e+18\n",
      "Gradient Descent(13/49): loss=7.581649962537945e+19\n",
      "Gradient Descent(14/49): loss=5.157832775398073e+21\n",
      "Gradient Descent(15/49): loss=3.5089020240704155e+23\n",
      "Gradient Descent(16/49): loss=2.3871259071334664e+25\n",
      "Gradient Descent(17/49): loss=1.6239753471341144e+27\n",
      "Gradient Descent(18/49): loss=1.1047996878411387e+29\n",
      "Gradient Descent(19/49): loss=7.516015285177602e+30\n",
      "Gradient Descent(20/49): loss=5.11318806642862e+32\n",
      "Gradient Descent(21/49): loss=3.478531005281984e+34\n",
      "Gradient Descent(22/49): loss=2.3664644831776143e+36\n",
      "Gradient Descent(23/49): loss=1.609919285373112e+38\n",
      "Gradient Descent(24/49): loss=1.0952372722446706e+40\n",
      "Gradient Descent(25/49): loss=7.450961631509837e+41\n",
      "Gradient Descent(26/49): loss=5.068931695545239e+43\n",
      "Gradient Descent(27/49): loss=3.448423143858921e+45\n",
      "Gradient Descent(28/49): loss=2.3459819333436164e+47\n",
      "Gradient Descent(29/49): loss=1.5959848898983438e+49\n",
      "Gradient Descent(30/49): loss=1.0857576235267426e+51\n",
      "Gradient Descent(31/49): loss=7.386471040596856e+52\n",
      "Gradient Descent(32/49): loss=5.0250583787157405e+54\n",
      "Gradient Descent(33/49): loss=3.41857587618203e+56\n",
      "Gradient Descent(34/49): loss=2.3256766669047689e+58\n",
      "Gradient Descent(35/49): loss=1.5821711013260058e+60\n",
      "Gradient Descent(36/49): loss=1.0763600243720402e+62\n",
      "Gradient Descent(37/49): loss=7.32253863754186e+63\n",
      "Gradient Descent(38/49): loss=4.981564800269783e+65\n",
      "Gradient Descent(39/49): loss=3.3889869466933115e+67\n",
      "Gradient Descent(40/49): loss=2.3055471494089606e+69\n",
      "Gradient Descent(41/49): loss=1.5684768757620162e+71\n",
      "Gradient Descent(42/49): loss=1.0670437646138807e+73\n",
      "Gradient Descent(43/49): loss=7.259159591040719e+74\n",
      "Gradient Descent(44/49): loss=4.9384476734435985e+76\n",
      "Gradient Descent(45/49): loss=3.359654119388852e+78\n",
      "Gradient Descent(46/49): loss=2.285591859689704e+80\n",
      "Gradient Descent(47/49): loss=1.5549011783481717e+82\n",
      "Gradient Descent(48/49): loss=1.057808140232298e+84\n",
      "Gradient Descent(49/49): loss=7.196329111606073e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5179553001079507\n",
      "Gradient Descent(2/49): loss=16.78630777135837\n",
      "Gradient Descent(3/49): loss=259.64814124214695\n",
      "Gradient Descent(4/49): loss=5093.590173802627\n",
      "Gradient Descent(5/49): loss=162220.705405815\n",
      "Gradient Descent(6/49): loss=8392505.31465261\n",
      "Gradient Descent(7/49): loss=537814428.708264\n",
      "Gradient Descent(8/49): loss=36515537864.51925\n",
      "Gradient Descent(9/49): loss=2512041015177.541\n",
      "Gradient Descent(10/49): loss=173307135038126.1\n",
      "Gradient Descent(11/49): loss=1.1963917943301286e+16\n",
      "Gradient Descent(12/49): loss=8.260148608268918e+17\n",
      "Gradient Descent(13/49): loss=5.70314798043917e+19\n",
      "Gradient Descent(14/49): loss=3.937713116370165e+21\n",
      "Gradient Descent(15/49): loss=2.718780046270362e+23\n",
      "Gradient Descent(16/49): loss=1.877172563125536e+25\n",
      "Gradient Descent(17/49): loss=1.2960875779463324e+27\n",
      "Gradient Descent(18/49): loss=8.948793864558307e+28\n",
      "Gradient Descent(19/49): loss=6.17866517794073e+30\n",
      "Gradient Descent(20/49): loss=4.266038974840791e+32\n",
      "Gradient Descent(21/49): loss=2.9454725273321916e+34\n",
      "Gradient Descent(22/49): loss=2.033691783097943e+36\n",
      "Gradient Descent(23/49): loss=1.404155778159878e+38\n",
      "Gradient Descent(24/49): loss=9.694947217315363e+39\n",
      "Gradient Descent(25/49): loss=6.6938443019286795e+41\n",
      "Gradient Descent(26/49): loss=4.621742701026599e+43\n",
      "Gradient Descent(27/49): loss=3.191066990957517e+45\n",
      "Gradient Descent(28/49): loss=2.203261669784544e+47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=1.5212347466529415e+49\n",
      "Gradient Descent(30/49): loss=1.0503315090351945e+51\n",
      "Gradient Descent(31/49): loss=7.251979231341059e+52\n",
      "Gradient Descent(32/49): loss=5.0071051205644924e+54\n",
      "Gradient Descent(33/49): loss=3.457139201396565e+56\n",
      "Gradient Descent(34/49): loss=2.3869703491436818e+58\n",
      "Gradient Descent(35/49): loss=1.6480757978705266e+60\n",
      "Gradient Descent(36/49): loss=1.13790849412977e+62\n",
      "Gradient Descent(37/49): loss=7.856651633897931e+63\n",
      "Gradient Descent(38/49): loss=5.424599184808543e+65\n",
      "Gradient Descent(39/49): loss=3.7453965998522797e+67\n",
      "Gradient Descent(40/49): loss=2.585996718332741e+69\n",
      "Gradient Descent(41/49): loss=1.785492897465485e+71\n",
      "Gradient Descent(42/49): loss=1.2327876769136482e+73\n",
      "Gradient Descent(43/49): loss=8.511741819345357e+74\n",
      "Gradient Descent(44/49): loss=5.876904040813864e+76\n",
      "Gradient Descent(45/49): loss=4.057688994564735e+78\n",
      "Gradient Descent(46/49): loss=2.801617971344573e+80\n",
      "Gradient Descent(47/49): loss=1.9343678798140845e+82\n",
      "Gradient Descent(48/49): loss=1.335577917020797e+84\n",
      "Gradient Descent(49/49): loss=9.221453638927356e+85\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5489072149080245\n",
      "Gradient Descent(2/49): loss=17.66185252241982\n",
      "Gradient Descent(3/49): loss=293.5865065094691\n",
      "Gradient Descent(4/49): loss=6984.772575157921\n",
      "Gradient Descent(5/49): loss=289068.8436967051\n",
      "Gradient Descent(6/49): loss=17424381.49957016\n",
      "Gradient Descent(7/49): loss=1190670141.6206636\n",
      "Gradient Descent(8/49): loss=83842334634.12143\n",
      "Gradient Descent(9/49): loss=5942495832558.7705\n",
      "Gradient Descent(10/49): loss=421771029290322.2\n",
      "Gradient Descent(11/49): loss=2.994415294091888e+16\n",
      "Gradient Descent(12/49): loss=2.1260537971296238e+18\n",
      "Gradient Descent(13/49): loss=1.509531420662226e+20\n",
      "Gradient Descent(14/49): loss=1.0717938658318813e+22\n",
      "Gradient Descent(15/49): loss=7.609929459052042e+23\n",
      "Gradient Descent(16/49): loss=5.4031876254085314e+25\n",
      "Gradient Descent(17/49): loss=3.836361090434773e+27\n",
      "Gradient Descent(18/49): loss=2.723885883249027e+29\n",
      "Gradient Descent(19/49): loss=1.9340083320391424e+31\n",
      "Gradient Descent(20/49): loss=1.373180958982232e+33\n",
      "Gradient Descent(21/49): loss=9.749833622595263e+34\n",
      "Gradient Descent(22/49): loss=6.922558534440291e+36\n",
      "Gradient Descent(23/49): loss=4.9151419929673425e+38\n",
      "Gradient Descent(24/49): loss=3.4898398750768665e+40\n",
      "Gradient Descent(25/49): loss=2.4778495455682583e+42\n",
      "Gradient Descent(26/49): loss=1.7593180748265066e+44\n",
      "Gradient Descent(27/49): loss=1.249147711146207e+46\n",
      "Gradient Descent(28/49): loss=8.8691750888519e+47\n",
      "Gradient Descent(29/49): loss=6.297275018382765e+49\n",
      "Gradient Descent(30/49): loss=4.471179366725309e+51\n",
      "Gradient Descent(31/49): loss=3.1746183660506467e+53\n",
      "Gradient Descent(32/49): loss=2.254036562493603e+55\n",
      "Gradient Descent(33/49): loss=1.6004068014571153e+57\n",
      "Gradient Descent(34/49): loss=1.136317827656093e+59\n",
      "Gradient Descent(35/49): loss=8.068062471825584e+60\n",
      "Gradient Descent(36/49): loss=5.72847054450886e+62\n",
      "Gradient Descent(37/49): loss=4.067317883804095e+64\n",
      "Gradient Descent(38/49): loss=2.887869395395715e+66\n",
      "Gradient Descent(39/49): loss=2.0504395975716404e+68\n",
      "Gradient Descent(40/49): loss=1.455849267280899e+70\n",
      "Gradient Descent(41/49): loss=1.0336793590761894e+72\n",
      "Gradient Descent(42/49): loss=7.339310747298594e+73\n",
      "Gradient Descent(43/49): loss=5.211043615454818e+75\n",
      "Gradient Descent(44/49): loss=3.6999353886421206e+77\n",
      "Gradient Descent(45/49): loss=2.6270211670319952e+79\n",
      "Gradient Descent(46/49): loss=1.8652326289856772e+81\n",
      "Gradient Descent(47/49): loss=1.3243489637213182e+83\n",
      "Gradient Descent(48/49): loss=9.403117608249789e+84\n",
      "Gradient Descent(49/49): loss=6.676383881943558e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5604086113294298\n",
      "Gradient Descent(2/49): loss=18.037241714301793\n",
      "Gradient Descent(3/49): loss=298.0894503264221\n",
      "Gradient Descent(4/49): loss=6721.899338993892\n",
      "Gradient Descent(5/49): loss=254695.7930446047\n",
      "Gradient Descent(6/49): loss=14408408.955924619\n",
      "Gradient Descent(7/49): loss=946501237.3435996\n",
      "Gradient Descent(8/49): loss=64609090091.291214\n",
      "Gradient Descent(9/49): loss=4449066581747.342\n",
      "Gradient Descent(10/49): loss=306963763630320.8\n",
      "Gradient Descent(11/49): loss=2.1188047716469044e+16\n",
      "Gradient Descent(12/49): loss=1.4626337563872617e+18\n",
      "Gradient Descent(13/49): loss=1.0096926981497353e+20\n",
      "Gradient Descent(14/49): loss=6.970193121761898e+21\n",
      "Gradient Descent(15/49): loss=4.8117254631010205e+23\n",
      "Gradient Descent(16/49): loss=3.3216736765201862e+25\n",
      "Gradient Descent(17/49): loss=2.2930478120775482e+27\n",
      "Gradient Descent(18/49): loss=1.5829575196953368e+29\n",
      "Gradient Descent(19/49): loss=1.0927615646746378e+31\n",
      "Gradient Descent(20/49): loss=7.543650559060685e+32\n",
      "Gradient Descent(21/49): loss=5.207601145869945e+34\n",
      "Gradient Descent(22/49): loss=3.594958367015582e+36\n",
      "Gradient Descent(23/49): loss=2.481704204803637e+38\n",
      "Gradient Descent(24/49): loss=1.7131925133420803e+40\n",
      "Gradient Descent(25/49): loss=1.182666565213929e+42\n",
      "Gradient Descent(26/49): loss=8.16429089890449e+43\n",
      "Gradient Descent(27/49): loss=5.636047204046826e+45\n",
      "Gradient Descent(28/49): loss=3.8907271286115626e+47\n",
      "Gradient Descent(29/49): loss=2.6858819738849313e+49\n",
      "Gradient Descent(30/49): loss=1.8541423593008854e+51\n",
      "Gradient Descent(31/49): loss=1.2799683388847105e+53\n",
      "Gradient Descent(32/49): loss=8.835993311565697e+54\n",
      "Gradient Descent(33/49): loss=6.099742894426698e+56\n",
      "Gradient Descent(34/49): loss=4.2108297353969433e+58\n",
      "Gradient Descent(35/49): loss=2.9068581032660066e+60\n",
      "Gradient Descent(36/49): loss=2.0066886014156612e+62\n",
      "Gradient Descent(37/49): loss=1.3852754417310865e+64\n",
      "Gradient Descent(38/49): loss=9.56295883730805e+65\n",
      "Gradient Descent(39/49): loss=6.601588317321673e+67\n",
      "Gradient Descent(40/49): loss=4.557268210898883e+69\n",
      "Gradient Descent(41/49): loss=3.1460146479560354e+71\n",
      "Gradient Descent(42/49): loss=2.171785312412401e+73\n",
      "Gradient Descent(43/49): loss=1.4992464978745841e+75\n",
      "Gradient Descent(44/49): loss=1.0349734149792376e+77\n",
      "Gradient Descent(45/49): loss=7.144722173654192e+78\n",
      "Gradient Descent(46/49): loss=4.932209291552628e+80\n",
      "Gradient Descent(47/49): loss=3.4048473690665555e+82\n",
      "Gradient Descent(48/49): loss=2.3504650596426313e+84\n",
      "Gradient Descent(49/49): loss=1.6225943185569314e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5845756111420835\n",
      "Gradient Descent(2/49): loss=18.302597449037705\n",
      "Gradient Descent(3/49): loss=298.2396067061811\n",
      "Gradient Descent(4/49): loss=6509.076648277044\n",
      "Gradient Descent(5/49): loss=237418.68382022032\n",
      "Gradient Descent(6/49): loss=13190565.364037924\n",
      "Gradient Descent(7/49): loss=862399996.2826182\n",
      "Gradient Descent(8/49): loss=58817781299.557434\n",
      "Gradient Descent(9/49): loss=4050381504122.0537\n",
      "Gradient Descent(10/49): loss=279517224606751.94\n",
      "Gradient Descent(11/49): loss=1.9298530725853236e+16\n",
      "Gradient Descent(12/49): loss=1.332552690089457e+18\n",
      "Gradient Descent(13/49): loss=9.201407701356898e+19\n",
      "Gradient Descent(14/49): loss=6.353693932972948e+21\n",
      "Gradient Descent(15/49): loss=4.387314561879386e+23\n",
      "Gradient Descent(16/49): loss=3.0295028056802507e+25\n",
      "Gradient Descent(17/49): loss=2.0919146765699824e+27\n",
      "Gradient Descent(18/49): loss=1.4444967851611267e+29\n",
      "Gradient Descent(19/49): loss=9.974455415737992e+30\n",
      "Gradient Descent(20/49): loss=6.887503102562293e+32\n",
      "Gradient Descent(21/49): loss=4.755918695929815e+34\n",
      "Gradient Descent(22/49): loss=3.284029394406597e+36\n",
      "Gradient Descent(23/49): loss=2.267668930635335e+38\n",
      "Gradient Descent(24/49): loss=1.5658575979051008e+40\n",
      "Gradient Descent(25/49): loss=1.0812469068096558e+42\n",
      "Gradient Descent(26/49): loss=7.466163430503512e+43\n",
      "Gradient Descent(27/49): loss=5.155491869610518e+45\n",
      "Gradient Descent(28/49): loss=3.55994034486703e+47\n",
      "Gradient Descent(29/49): loss=2.4581893599164383e+49\n",
      "Gradient Descent(30/49): loss=1.697414659748184e+51\n",
      "Gradient Descent(31/49): loss=1.172088926146016e+53\n",
      "Gradient Descent(32/49): loss=8.09344047374934e+54\n",
      "Gradient Descent(33/49): loss=5.58863557541734e+56\n",
      "Gradient Descent(34/49): loss=3.859032224444399e+58\n",
      "Gradient Descent(35/49): loss=2.664716549922472e+60\n",
      "Gradient Descent(36/49): loss=1.8400246171701441e+62\n",
      "Gradient Descent(37/49): loss=1.2705631268326616e+64\n",
      "Gradient Descent(38/49): loss=8.773418813002194e+65\n",
      "Gradient Descent(39/49): loss=6.058170274484642e+67\n",
      "Gradient Descent(40/49): loss=4.183252601626503e+69\n",
      "Gradient Descent(41/49): loss=2.8885953243536376e+71\n",
      "Gradient Descent(42/49): loss=1.9946160900333104e+73\n",
      "Gradient Descent(43/49): loss=1.3773107340710716e+75\n",
      "Gradient Descent(44/49): loss=9.510526199333274e+76\n",
      "Gradient Descent(45/49): loss=6.567153391802158e+78\n",
      "Gradient Descent(46/49): loss=4.534712671785032e+80\n",
      "Gradient Descent(47/49): loss=3.131283493593612e+82\n",
      "Gradient Descent(48/49): loss=2.162195717108615e+84\n",
      "Gradient Descent(49/49): loss=1.4930268462270234e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5374855499652922\n",
      "Gradient Descent(2/49): loss=17.34532693162766\n",
      "Gradient Descent(3/49): loss=272.8643649176136\n",
      "Gradient Descent(4/49): loss=5433.471159347193\n",
      "Gradient Descent(5/49): loss=175269.00277775122\n",
      "Gradient Descent(6/49): loss=9185727.620057303\n",
      "Gradient Descent(7/49): loss=597029022.5409029\n",
      "Gradient Descent(8/49): loss=41135281856.47651\n",
      "Gradient Descent(9/49): loss=2872131130819.981\n",
      "Gradient Descent(10/49): loss=201118412392720.47\n",
      "Gradient Descent(11/49): loss=1.4091942334785008e+16\n",
      "Gradient Descent(12/49): loss=9.875255576983351e+17\n",
      "Gradient Descent(13/49): loss=6.9205150222871994e+19\n",
      "Gradient Descent(14/49): loss=4.8498822655371e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=3.3987917792378674e+23\n",
      "Gradient Descent(16/49): loss=2.3818699638371113e+25\n",
      "Gradient Descent(17/49): loss=1.6692122512812364e+27\n",
      "Gradient Descent(18/49): loss=1.1697824060197575e+29\n",
      "Gradient Descent(19/49): loss=8.19782433495499e+30\n",
      "Gradient Descent(20/49): loss=5.7450277523936876e+32\n",
      "Gradient Descent(21/49): loss=4.026110164308605e+34\n",
      "Gradient Descent(22/49): loss=2.821494299807515e+36\n",
      "Gradient Descent(23/49): loss=1.9773006100201373e+38\n",
      "Gradient Descent(24/49): loss=1.385690448730588e+40\n",
      "Gradient Descent(25/49): loss=9.710905918772745e+41\n",
      "Gradient Descent(26/49): loss=6.805393935539477e+43\n",
      "Gradient Descent(27/49): loss=4.769213810253059e+45\n",
      "Gradient Descent(28/49): loss=3.3422606513820637e+47\n",
      "Gradient Descent(29/49): loss=2.3422531901929126e+49\n",
      "Gradient Descent(30/49): loss=1.6414488812235914e+51\n",
      "Gradient Descent(31/49): loss=1.1503258661153855e+53\n",
      "Gradient Descent(32/49): loss=8.061473088749122e+54\n",
      "Gradient Descent(33/49): loss=5.649472925449136e+56\n",
      "Gradient Descent(34/49): loss=3.9591454296269685e+58\n",
      "Gradient Descent(35/49): loss=2.7745654753607203e+60\n",
      "Gradient Descent(36/49): loss=1.9444129330174727e+62\n",
      "Gradient Descent(37/49): loss=1.36264279493855e+64\n",
      "Gradient Descent(38/49): loss=9.549388172997242e+65\n",
      "Gradient Descent(39/49): loss=6.692202447868142e+67\n",
      "Gradient Descent(40/49): loss=4.689889319809394e+69\n",
      "Gradient Descent(41/49): loss=3.2866701214438984e+71\n",
      "Gradient Descent(42/49): loss=2.3032953979457757e+73\n",
      "Gradient Descent(43/49): loss=1.6141472962511754e+75\n",
      "Gradient Descent(44/49): loss=1.1311929404794083e+77\n",
      "Gradient Descent(45/49): loss=7.927389721881622e+78\n",
      "Gradient Descent(46/49): loss=5.555507425281514e+80\n",
      "Gradient Descent(47/49): loss=3.893294493541365e+82\n",
      "Gradient Descent(48/49): loss=2.7284171999232477e+84\n",
      "Gradient Descent(49/49): loss=1.9120722640392332e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5688049373700441\n",
      "Gradient Descent(2/49): loss=18.244979327233242\n",
      "Gradient Descent(3/49): loss=308.29768038834936\n",
      "Gradient Descent(4/49): loss=7438.502589674139\n",
      "Gradient Descent(5/49): loss=311761.670111944\n",
      "Gradient Descent(6/49): loss=19048172.629184432\n",
      "Gradient Descent(7/49): loss=1320469612.2185245\n",
      "Gradient Descent(8/49): loss=94355478664.31967\n",
      "Gradient Descent(9/49): loss=6786923243012.606\n",
      "Gradient Descent(10/49): loss=488865481768868.5\n",
      "Gradient Descent(11/49): loss=3.52237206448363e+16\n",
      "Gradient Descent(12/49): loss=2.53809857206666e+18\n",
      "Gradient Descent(13/49): loss=1.8288898174211555e+20\n",
      "Gradient Descent(14/49): loss=1.317855601619711e+22\n",
      "Gradient Descent(15/49): loss=9.496167617623687e+23\n",
      "Gradient Descent(16/49): loss=6.842723188638471e+25\n",
      "Gradient Descent(17/49): loss=4.930711396129206e+27\n",
      "Gradient Descent(18/49): loss=3.552959010568485e+29\n",
      "Gradient Descent(19/49): loss=2.5601818341780954e+31\n",
      "Gradient Descent(20/49): loss=1.844809074978177e+33\n",
      "Gradient Descent(21/49): loss=1.3293276586338982e+35\n",
      "Gradient Descent(22/49): loss=9.578834189386862e+36\n",
      "Gradient Descent(23/49): loss=6.902291081664677e+38\n",
      "Gradient Descent(24/49): loss=4.973634706907956e+40\n",
      "Gradient Descent(25/49): loss=3.583888582078744e+42\n",
      "Gradient Descent(26/49): loss=2.5824689840842234e+44\n",
      "Gradient Descent(27/49): loss=1.860868690814304e+46\n",
      "Gradient Descent(28/49): loss=1.3408998542845616e+48\n",
      "Gradient Descent(29/49): loss=9.66222081168742e+49\n",
      "Gradient Descent(30/49): loss=6.962377594083335e+51\n",
      "Gradient Descent(31/49): loss=5.016931687584591e+53\n",
      "Gradient Descent(32/49): loss=3.615087406244554e+55\n",
      "Gradient Descent(33/49): loss=2.6049501505331504e+57\n",
      "Gradient Descent(34/49): loss=1.8770681104532565e+59\n",
      "Gradient Descent(35/49): loss=1.3525727893715905e+61\n",
      "Gradient Descent(36/49): loss=9.746333339532672e+62\n",
      "Gradient Descent(37/49): loss=7.022987177601138e+64\n",
      "Gradient Descent(38/49): loss=5.060605581453839e+66\n",
      "Gradient Descent(39/49): loss=3.646557825524856e+68\n",
      "Gradient Descent(40/49): loss=2.6276270222735306e+70\n",
      "Gradient Descent(41/49): loss=1.8934085508950584e+72\n",
      "Gradient Descent(42/49): loss=1.364347340856891e+74\n",
      "Gradient Descent(43/49): loss=9.831178092141279e+75\n",
      "Gradient Descent(44/49): loss=7.084124385707903e+77\n",
      "Gradient Descent(45/49): loss=5.104659669658097e+79\n",
      "Gradient Descent(46/49): loss=3.6783022042363882e+81\n",
      "Gradient Descent(47/49): loss=2.650501302978419e+83\n",
      "Gradient Descent(48/49): loss=1.9098912397679718e+85\n",
      "Gradient Descent(49/49): loss=1.3762243933415486e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5804891474799785\n",
      "Gradient Descent(2/49): loss=18.632490081411316\n",
      "Gradient Descent(3/49): loss=313.046734626109\n",
      "Gradient Descent(4/49): loss=7161.080181342401\n",
      "Gradient Descent(5/49): loss=274807.981518165\n",
      "Gradient Descent(6/49): loss=15755490.476839285\n",
      "Gradient Descent(7/49): loss=1049893383.4939792\n",
      "Gradient Descent(8/49): loss=72724346737.7941\n",
      "Gradient Descent(9/49): loss=5082306660091.936\n",
      "Gradient Descent(10/49): loss=355873873748247.0\n",
      "Gradient Descent(11/49): loss=2.49298575101231e+16\n",
      "Gradient Descent(12/49): loss=1.7465659995998991e+18\n",
      "Gradient Descent(13/49): loss=1.223656014743037e+20\n",
      "Gradient Descent(14/49): loss=8.573056981305864e+21\n",
      "Gradient Descent(15/49): loss=6.006375986830174e+23\n",
      "Gradient Descent(16/49): loss=4.208132604657649e+25\n",
      "Gradient Descent(17/49): loss=2.948263800625733e+27\n",
      "Gradient Descent(18/49): loss=2.065585937858114e+29\n",
      "Gradient Descent(19/49): loss=1.4471721546804655e+31\n",
      "Gradient Descent(20/49): loss=1.0139046785731825e+33\n",
      "Gradient Descent(21/49): loss=7.103527344880522e+34\n",
      "Gradient Descent(22/49): loss=4.976809142627618e+36\n",
      "Gradient Descent(23/49): loss=3.4868070522924946e+38\n",
      "Gradient Descent(24/49): loss=2.442895251054906e+40\n",
      "Gradient Descent(25/49): loss=1.7115191973999646e+42\n",
      "Gradient Descent(26/49): loss=1.199109115220501e+44\n",
      "Gradient Descent(27/49): loss=8.401089934540142e+45\n",
      "Gradient Descent(28/49): loss=5.885895719778044e+47\n",
      "Gradient Descent(29/49): loss=4.123723075700877e+49\n",
      "Gradient Descent(30/49): loss=2.889125600361304e+51\n",
      "Gradient Descent(31/49): loss=2.0241530727046738e+53\n",
      "Gradient Descent(32/49): loss=1.4181438360545658e+55\n",
      "Gradient Descent(33/49): loss=9.935671204215232e+56\n",
      "Gradient Descent(34/49): loss=6.961040182843155e+58\n",
      "Gradient Descent(35/49): loss=4.8769810746756626e+60\n",
      "Gradient Descent(36/49): loss=3.4168664133510034e+62\n",
      "Gradient Descent(37/49): loss=2.393894072566786e+64\n",
      "Gradient Descent(38/49): loss=1.6771884344902204e+66\n",
      "Gradient Descent(39/49): loss=1.1750566063149364e+68\n",
      "Gradient Descent(40/49): loss=8.232575419970895e+69\n",
      "Gradient Descent(41/49): loss=5.767832603235836e+71\n",
      "Gradient Descent(42/49): loss=4.0410067617780375e+73\n",
      "Gradient Descent(43/49): loss=2.8311736439047253e+75\n",
      "Gradient Descent(44/49): loss=1.9835512966115185e+77\n",
      "Gradient Descent(45/49): loss=1.3896977865557356e+79\n",
      "Gradient Descent(46/49): loss=9.736375062530736e+80\n",
      "Gradient Descent(47/49): loss=6.821411120846519e+82\n",
      "Gradient Descent(48/49): loss=4.779155422913035e+84\n",
      "Gradient Descent(49/49): loss=3.3483286891414506e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6050942156969734\n",
      "Gradient Descent(2/49): loss=18.907271228377628\n",
      "Gradient Descent(3/49): loss=313.2307394707875\n",
      "Gradient Descent(4/49): loss=6935.734475623041\n",
      "Gradient Descent(5/49): loss=256211.61278520446\n",
      "Gradient Descent(6/49): loss=14424744.830414366\n",
      "Gradient Descent(7/49): loss=956621509.6677008\n",
      "Gradient Descent(8/49): loss=66205807436.481926\n",
      "Gradient Descent(9/49): loss=4626872531601.354\n",
      "Gradient Descent(10/49): loss=324053531810965.6\n",
      "Gradient Descent(11/49): loss=2.2706600109591644e+16\n",
      "Gradient Descent(12/49): loss=1.5912290530307784e+18\n",
      "Gradient Descent(13/49): loss=1.1151241242857028e+20\n",
      "Gradient Descent(14/49): loss=7.814764587742562e+21\n",
      "Gradient Descent(15/49): loss=5.476575331691655e+23\n",
      "Gradient Descent(16/49): loss=3.8379767247735775e+25\n",
      "Gradient Descent(17/49): loss=2.6896491378360524e+27\n",
      "Gradient Descent(18/49): loss=1.884902668095264e+29\n",
      "Gradient Descent(19/49): loss=1.3209373770197139e+31\n",
      "Gradient Descent(20/49): loss=9.25711223458186e+32\n",
      "Gradient Descent(21/49): loss=6.487372408787711e+34\n",
      "Gradient Descent(22/49): loss=4.546342283094525e+36\n",
      "Gradient Descent(23/49): loss=3.186070854692431e+38\n",
      "Gradient Descent(24/49): loss=2.232794378214825e+40\n",
      "Gradient Descent(25/49): loss=1.564739443269639e+42\n",
      "Gradient Descent(26/49): loss=1.0965673996732035e+44\n",
      "Gradient Descent(27/49): loss=7.684730305726991e+45\n",
      "Gradient Descent(28/49): loss=5.385449165218616e+47\n",
      "Gradient Descent(29/49): loss=3.774115884006955e+49\n",
      "Gradient Descent(30/49): loss=2.644895582323242e+51\n",
      "Gradient Descent(31/49): loss=1.853539439802832e+53\n",
      "Gradient Descent(32/49): loss=1.2989580677081802e+55\n",
      "Gradient Descent(33/49): loss=9.103081517616317e+56\n",
      "Gradient Descent(34/49): loss=6.379427879652355e+58\n",
      "Gradient Descent(35/49): loss=4.470694895231593e+60\n",
      "Gradient Descent(36/49): loss=3.1330572620784503e+62\n",
      "Gradient Descent(37/49): loss=2.195642520345585e+64\n",
      "Gradient Descent(38/49): loss=1.538703468812891e+66\n",
      "Gradient Descent(39/49): loss=1.0783214220883919e+68\n",
      "Gradient Descent(40/49): loss=7.556862728279903e+69\n",
      "Gradient Descent(41/49): loss=5.295839730557228e+71\n",
      "Gradient Descent(42/49): loss=3.711317706856438e+73\n",
      "Gradient Descent(43/49): loss=2.600886700129952e+75\n",
      "Gradient Descent(44/49): loss=1.822698071473552e+77\n",
      "Gradient Descent(45/49): loss=1.2773444762462903e+79\n",
      "Gradient Descent(46/49): loss=8.951613745209105e+80\n",
      "Gradient Descent(47/49): loss=6.273279458560564e+82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=4.396306217553313e+84\n",
      "Gradient Descent(49/49): loss=3.080925771945828e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5571845929509782\n",
      "Gradient Descent(2/49): loss=17.918519408091907\n",
      "Gradient Descent(3/49): loss=286.6379303679707\n",
      "Gradient Descent(4/49): loss=5793.093256361788\n",
      "Gradient Descent(5/49): loss=189257.4964314791\n",
      "Gradient Descent(6/49): loss=10047233.894671008\n",
      "Gradient Descent(7/49): loss=662252903.9985942\n",
      "Gradient Descent(8/49): loss=46298753087.702194\n",
      "Gradient Descent(9/49): loss=3280591419492.7803\n",
      "Gradient Descent(10/49): loss=233136258474493.28\n",
      "Gradient Descent(11/49): loss=1.6578420354694336e+16\n",
      "Gradient Descent(12/49): loss=1.179060103971944e+18\n",
      "Gradient Descent(13/49): loss=8.385743358209286e+19\n",
      "Gradient Descent(14/49): loss=5.964169191649712e+21\n",
      "Gradient Descent(15/49): loss=4.241885479990043e+23\n",
      "Gradient Descent(16/49): loss=3.0169495825312315e+25\n",
      "Gradient Descent(17/49): loss=2.145740474976723e+27\n",
      "Gradient Descent(18/49): loss=1.5261117640131653e+29\n",
      "Gradient Descent(19/49): loss=1.0854141730248075e+31\n",
      "Gradient Descent(20/49): loss=7.719774891578796e+32\n",
      "Gradient Descent(21/49): loss=5.490523881662526e+34\n",
      "Gradient Descent(22/49): loss=3.9050170399254037e+36\n",
      "Gradient Descent(23/49): loss=2.7773593942711725e+38\n",
      "Gradient Descent(24/49): loss=1.9753371434979381e+40\n",
      "Gradient Descent(25/49): loss=1.4049160647096056e+42\n",
      "Gradient Descent(26/49): loss=9.992163390316456e+43\n",
      "Gradient Descent(27/49): loss=7.106711335059065e+45\n",
      "Gradient Descent(28/49): loss=5.054495610910763e+47\n",
      "Gradient Descent(29/49): loss=3.594901308947436e+49\n",
      "Gradient Descent(30/49): loss=2.5567962494962743e+51\n",
      "Gradient Descent(31/49): loss=1.8184663498738106e+53\n",
      "Gradient Descent(32/49): loss=1.2933450861698266e+55\n",
      "Gradient Descent(33/49): loss=9.198638798214178e+56\n",
      "Gradient Descent(34/49): loss=6.542334033261965e+58\n",
      "Gradient Descent(35/49): loss=4.653094391649347e+60\n",
      "Gradient Descent(36/49): loss=3.3094133236733106e+62\n",
      "Gradient Descent(37/49): loss=2.3537490592415538e+64\n",
      "Gradient Descent(38/49): loss=1.674053402229894e+66\n",
      "Gradient Descent(39/49): loss=1.1906344826837867e+68\n",
      "Gradient Descent(40/49): loss=8.468131718303572e+69\n",
      "Gradient Descent(41/49): loss=6.02277658185249e+71\n",
      "Gradient Descent(42/49): loss=4.2835703271485247e+73\n",
      "Gradient Descent(43/49): loss=3.046597279220966e+75\n",
      "Gradient Descent(44/49): loss=2.1668267993478003e+77\n",
      "Gradient Descent(45/49): loss=1.541108964546954e+79\n",
      "Gradient Descent(46/49): loss=1.096080610283161e+81\n",
      "Gradient Descent(47/49): loss=7.795637634174992e+82\n",
      "Gradient Descent(48/49): loss=5.544479626153173e+84\n",
      "Gradient Descent(49/49): loss=3.943391902935876e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.588873071898711\n",
      "Gradient Descent(2/49): loss=18.84275045277498\n",
      "Gradient Descent(3/49): loss=323.6199934471315\n",
      "Gradient Descent(4/49): loss=7917.908989792839\n",
      "Gradient Descent(5/49): loss=336049.28612199\n",
      "Gradient Descent(6/49): loss=20809719.236192744\n",
      "Gradient Descent(7/49): loss=1463310049.9912388\n",
      "Gradient Descent(8/49): loss=106095035574.02863\n",
      "Gradient Descent(9/49): loss=7743801527973.696\n",
      "Gradient Descent(10/49): loss=566020615897298.9\n",
      "Gradient Descent(11/49): loss=4.138487957778592e+16\n",
      "Gradient Descent(12/49): loss=3.0260703193864023e+18\n",
      "Gradient Descent(13/49): loss=2.212698528449178e+20\n",
      "Gradient Descent(14/49): loss=1.6179560923190895e+22\n",
      "Gradient Descent(15/49): loss=1.1830728340131759e+24\n",
      "Gradient Descent(16/49): loss=8.650800345456257e+25\n",
      "Gradient Descent(17/49): loss=6.32559099702331e+27\n",
      "Gradient Descent(18/49): loss=4.625364139418539e+29\n",
      "Gradient Descent(19/49): loss=3.382133536417861e+31\n",
      "Gradient Descent(20/49): loss=2.473065236892262e+33\n",
      "Gradient Descent(21/49): loss=1.8083412734622626e+35\n",
      "Gradient Descent(22/49): loss=1.3222854426012283e+37\n",
      "Gradient Descent(23/49): loss=9.668743490944685e+38\n",
      "Gradient Descent(24/49): loss=7.069925878472101e+40\n",
      "Gradient Descent(25/49): loss=5.16963263881262e+42\n",
      "Gradient Descent(26/49): loss=3.7801105810254665e+44\n",
      "Gradient Descent(27/49): loss=2.7640718409079156e+46\n",
      "Gradient Descent(28/49): loss=2.0211295352178588e+48\n",
      "Gradient Descent(29/49): loss=1.4778793147388652e+50\n",
      "Gradient Descent(30/49): loss=1.0806468516118997e+52\n",
      "Gradient Descent(31/49): loss=7.901846965799589e+53\n",
      "Gradient Descent(32/49): loss=5.777945438676882e+55\n",
      "Gradient Descent(33/49): loss=4.224917748574607e+57\n",
      "Gradient Descent(34/49): loss=3.089321311817135e+59\n",
      "Gradient Descent(35/49): loss=2.258956679302796e+61\n",
      "Gradient Descent(36/49): loss=1.6517819818376816e+63\n",
      "Gradient Descent(37/49): loss=1.2078070113171362e+65\n",
      "Gradient Descent(38/49): loss=8.831660549801149e+66\n",
      "Gradient Descent(39/49): loss=6.45783865601646e+68\n",
      "Gradient Descent(40/49): loss=4.722065558563589e+70\n",
      "Gradient Descent(41/49): loss=3.4528430218054934e+72\n",
      "Gradient Descent(42/49): loss=2.52476904129586e+74\n",
      "Gradient Descent(43/49): loss=1.84614784733334e+76\n",
      "Gradient Descent(44/49): loss=1.34993015934009e+78\n",
      "Gradient Descent(45/49): loss=9.870885680842015e+79\n",
      "Gradient Descent(46/49): loss=7.217735188010238e+81\n",
      "Gradient Descent(47/49): loss=5.27771295592553e+83\n",
      "Gradient Descent(48/49): loss=3.8591404809938734e+85\n",
      "Gradient Descent(49/49): loss=2.8218596533039964e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6007414989274422\n",
      "Gradient Descent(2/49): loss=19.242671727555972\n",
      "Gradient Descent(3/49): loss=328.62604091940335\n",
      "Gradient Descent(4/49): loss=7625.249968091267\n",
      "Gradient Descent(5/49): loss=296342.14993572683\n",
      "Gradient Descent(6/49): loss=17217238.830435332\n",
      "Gradient Descent(7/49): loss=1163694923.0387223\n",
      "Gradient Descent(8/49): loss=81787904181.67903\n",
      "Gradient Descent(9/49): loss=5800010625705.505\n",
      "Gradient Descent(10/49): loss=412129732679867.1\n",
      "Gradient Descent(11/49): loss=2.9297487667571852e+16\n",
      "Gradient Descent(12/49): loss=2.082902898145475e+18\n",
      "Gradient Descent(13/49): loss=1.4808701556429944e+20\n",
      "Gradient Descent(14/49): loss=1.0528511881075862e+22\n",
      "Gradient Descent(15/49): loss=7.485442101234083e+23\n",
      "Gradient Descent(16/49): loss=5.321916048631055e+25\n",
      "Gradient Descent(17/49): loss=3.783716644084056e+27\n",
      "Gradient Descent(18/49): loss=2.6901047803554277e+29\n",
      "Gradient Descent(19/49): loss=1.9125807844215629e+31\n",
      "Gradient Descent(20/49): loss=1.3597854201236448e+33\n",
      "Gradient Descent(21/49): loss=9.667651186626048e+34\n",
      "Gradient Descent(22/49): loss=6.873399147069365e+36\n",
      "Gradient Descent(23/49): loss=4.886772901008936e+38\n",
      "Gradient Descent(24/49): loss=3.4743434616695604e+40\n",
      "Gradient Descent(25/49): loss=2.4701500835361964e+42\n",
      "Gradient Descent(26/49): loss=1.756199841066404e+44\n",
      "Gradient Descent(27/49): loss=1.2486034360091942e+46\n",
      "Gradient Descent(28/49): loss=8.877181878500319e+47\n",
      "Gradient Descent(29/49): loss=6.311400067570576e+49\n",
      "Gradient Descent(30/49): loss=4.487209044280642e+51\n",
      "Gradient Descent(31/49): loss=3.1902659935205598e+53\n",
      "Gradient Descent(32/49): loss=2.2681798438578422e+55\n",
      "Gradient Descent(33/49): loss=1.6126052857447045e+57\n",
      "Gradient Descent(34/49): loss=1.1465121756786008e+59\n",
      "Gradient Descent(35/49): loss=8.151344787216471e+60\n",
      "Gradient Descent(36/49): loss=5.795352482912216e+62\n",
      "Gradient Descent(37/49): loss=4.120315270416387e+64\n",
      "Gradient Descent(38/49): loss=2.9294159376299377e+66\n",
      "Gradient Descent(39/49): loss=2.0827235714837114e+68\n",
      "Gradient Descent(40/49): loss=1.4807516472799952e+70\n",
      "Gradient Descent(41/49): loss=1.0527683418690395e+72\n",
      "Gradient Descent(42/49): loss=7.484855300870538e+73\n",
      "Gradient Descent(43/49): loss=5.3214991985331864e+75\n",
      "Gradient Descent(44/49): loss=3.783420331010906e+77\n",
      "Gradient Descent(45/49): loss=2.6898941195090545e+79\n",
      "Gradient Descent(46/49): loss=1.9124310124527722e+81\n",
      "Gradient Descent(47/49): loss=1.3596789371243698e+83\n",
      "Gradient Descent(48/49): loss=9.666894125966455e+84\n",
      "Gradient Descent(49/49): loss=6.872860900550825e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6257882937336203\n",
      "Gradient Descent(2/49): loss=19.527125252817868\n",
      "Gradient Descent(3/49): loss=328.8463177750398\n",
      "Gradient Descent(4/49): loss=7386.745511927709\n",
      "Gradient Descent(5/49): loss=276336.6146088881\n",
      "Gradient Descent(6/49): loss=15764070.801092017\n",
      "Gradient Descent(7/49): loss=1060330979.1094635\n",
      "Gradient Descent(8/49): loss=74457181157.67004\n",
      "Gradient Descent(9/49): loss=5280257635158.772\n",
      "Gradient Descent(10/49): loss=375278603273316.94\n",
      "Gradient Descent(11/49): loss=2.6684662992167164e+16\n",
      "Gradient Descent(12/49): loss=1.897647919980935e+18\n",
      "Gradient Descent(13/49): loss=1.3495210522714851e+20\n",
      "Gradient Descent(14/49): loss=9.597230163935734e+21\n",
      "Gradient Descent(15/49): loss=6.825156983638822e+23\n",
      "Gradient Descent(16/49): loss=4.853773282587963e+25\n",
      "Gradient Descent(17/49): loss=3.4518057846687375e+27\n",
      "Gradient Descent(18/49): loss=2.4547836545879805e+29\n",
      "Gradient Descent(19/49): loss=1.7457421368806432e+31\n",
      "Gradient Descent(20/49): loss=1.2415006937738599e+33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=8.829047201611008e+34\n",
      "Gradient Descent(22/49): loss=6.278858713601389e+36\n",
      "Gradient Descent(23/49): loss=4.4652685443130766e+38\n",
      "Gradient Descent(24/49): loss=3.175517093523186e+40\n",
      "Gradient Descent(25/49): loss=2.2582984004632328e+42\n",
      "Gradient Descent(26/49): loss=1.606009829371387e+44\n",
      "Gradient Descent(27/49): loss=1.1421287689476865e+46\n",
      "Gradient Descent(28/49): loss=8.122354552266418e+47\n",
      "Gradient Descent(29/49): loss=5.776287688953669e+49\n",
      "Gradient Descent(30/49): loss=4.107860504100871e+51\n",
      "Gradient Descent(31/49): loss=2.9213430545402963e+53\n",
      "Gradient Descent(32/49): loss=2.0775401778593158e+55\n",
      "Gradient Descent(33/49): loss=1.4774619447420013e+57\n",
      "Gradient Descent(34/49): loss=1.0507107498686674e+59\n",
      "Gradient Descent(35/49): loss=7.472226840213925e+60\n",
      "Gradient Descent(36/49): loss=5.313943343455034e+62\n",
      "Gradient Descent(37/49): loss=3.7790600394355366e+64\n",
      "Gradient Descent(38/49): loss=2.687513557939645e+66\n",
      "Gradient Descent(39/49): loss=1.9112501650511473e+68\n",
      "Gradient Descent(40/49): loss=1.3592032615487674e+70\n",
      "Gradient Descent(41/49): loss=9.666099917146667e+71\n",
      "Gradient Descent(42/49): loss=6.874136507132722e+73\n",
      "Gradient Descent(43/49): loss=4.888605862109127e+75\n",
      "Gradient Descent(44/49): loss=3.476577349060579e+77\n",
      "Gradient Descent(45/49): loss=2.4724001903451217e+79\n",
      "Gradient Descent(46/49): loss=1.7582703007802677e+81\n",
      "Gradient Descent(47/49): loss=1.2504102137988956e+83\n",
      "Gradient Descent(48/49): loss=8.892408079001227e+84\n",
      "Gradient Descent(49/49): loss=6.323918388609979e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.577052429065007\n",
      "Gradient Descent(2/49): loss=18.506123004319146\n",
      "Gradient Descent(3/49): loss=300.98750485222837\n",
      "Gradient Descent(4/49): loss=6173.433948565949\n",
      "Gradient Descent(5/49): loss=204246.13045366525\n",
      "Gradient Descent(6/49): loss=10982334.557017963\n",
      "Gradient Descent(7/49): loss=734044901.1560807\n",
      "Gradient Descent(8/49): loss=52065225277.486534\n",
      "Gradient Descent(9/49): loss=3743489909764.402\n",
      "Gradient Descent(10/49): loss=269958753558675.34\n",
      "Gradient Descent(11/49): loss=1.9480405645969748e+16\n",
      "Gradient Descent(12/49): loss=1.4059149919723113e+18\n",
      "Gradient Descent(13/49): loss=1.0146895671015896e+20\n",
      "Gradient Descent(14/49): loss=7.323356073183158e+21\n",
      "Gradient Descent(15/49): loss=5.285519947311798e+23\n",
      "Gradient Descent(16/49): loss=3.8147441291712684e+25\n",
      "Gradient Descent(17/49): loss=2.7532340941763515e+27\n",
      "Gradient Descent(18/49): loss=1.9871052495079802e+29\n",
      "Gradient Descent(19/49): loss=1.4341632965552687e+31\n",
      "Gradient Descent(20/49): loss=1.0350857676279092e+33\n",
      "Gradient Descent(21/49): loss=7.470575694358469e+34\n",
      "Gradient Descent(22/49): loss=5.391775537089776e+36\n",
      "Gradient Descent(23/49): loss=3.8914328201649334e+38\n",
      "Gradient Descent(24/49): loss=2.808583052040813e+40\n",
      "Gradient Descent(25/49): loss=2.0270525343101867e+42\n",
      "Gradient Descent(26/49): loss=1.4629946491587374e+44\n",
      "Gradient Descent(27/49): loss=1.0558943624988463e+46\n",
      "Gradient Descent(28/49): loss=7.620758595378094e+47\n",
      "Gradient Descent(29/49): loss=5.500167784927788e+49\n",
      "Gradient Descent(30/49): loss=3.969663293192997e+51\n",
      "Gradient Descent(31/49): loss=2.8650447181822444e+53\n",
      "Gradient Descent(32/49): loss=2.067802891811957e+55\n",
      "Gradient Descent(33/49): loss=1.4924056061849511e+57\n",
      "Gradient Descent(34/49): loss=1.0771212779476231e+59\n",
      "Gradient Descent(35/49): loss=7.773960661896196e+60\n",
      "Gradient Descent(36/49): loss=5.610739069964568e+62\n",
      "Gradient Descent(37/49): loss=4.049466453506374e+64\n",
      "Gradient Descent(38/49): loss=2.9226414476938317e+66\n",
      "Gradient Descent(39/49): loss=2.1093724642123593e+68\n",
      "Gradient Descent(40/49): loss=1.5224078192308442e+70\n",
      "Gradient Descent(41/49): loss=1.0987749235271546e+72\n",
      "Gradient Descent(42/49): loss=7.930242588888303e+73\n",
      "Gradient Descent(43/49): loss=5.7235331979311975e+75\n",
      "Gradient Descent(44/49): loss=4.1308739172394795e+77\n",
      "Gradient Descent(45/49): loss=2.981396059045718e+79\n",
      "Gradient Descent(46/49): loss=2.151777720399017e+81\n",
      "Gradient Descent(47/49): loss=1.5530131744681768e+83\n",
      "Gradient Descent(48/49): loss=1.120863877903028e+85\n",
      "Gradient Descent(49/49): loss=8.089666291582041e+86\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.609111618494028\n",
      "Gradient Descent(2/49): loss=19.45540932943968\n",
      "Gradient Descent(3/49): loss=339.5736252450667\n",
      "Gradient Descent(4/49): loss=8424.231578576864\n",
      "Gradient Descent(5/49): loss=362030.8032199899\n",
      "Gradient Descent(6/49): loss=22719573.53254127\n",
      "Gradient Descent(7/49): loss=1620391942.399223\n",
      "Gradient Descent(8/49): loss=119193521357.72401\n",
      "Gradient Descent(9/49): loss=8827116610688.785\n",
      "Gradient Descent(10/49): loss=654654576114577.2\n",
      "Gradient Descent(11/49): loss=4.856673996951664e+16\n",
      "Gradient Descent(12/49): loss=3.603247081177699e+18\n",
      "Gradient Descent(13/49): loss=2.6733459087002196e+20\n",
      "Gradient Descent(14/49): loss=1.9834330642580542e+22\n",
      "Gradient Descent(15/49): loss=1.4715675847598565e+24\n",
      "Gradient Descent(16/49): loss=1.0917996091712148e+26\n",
      "Gradient Descent(17/49): loss=8.100385144333005e+27\n",
      "Gradient Descent(18/49): loss=6.0099160440210126e+29\n",
      "Gradient Descent(19/49): loss=4.45893500847623e+31\n",
      "Gradient Descent(20/49): loss=3.3082161663434727e+33\n",
      "Gradient Descent(21/49): loss=2.454463719045267e+35\n",
      "Gradient Descent(22/49): loss=1.821039450043444e+37\n",
      "Gradient Descent(23/49): loss=1.351083192997069e+39\n",
      "Gradient Descent(24/49): loss=1.0024087036426757e+41\n",
      "Gradient Descent(25/49): loss=7.437167558199758e+42\n",
      "Gradient Descent(26/49): loss=5.517855250831633e+44\n",
      "Gradient Descent(27/49): loss=4.0938605095111064e+46\n",
      "Gradient Descent(28/49): loss=3.037356565090815e+48\n",
      "Gradient Descent(29/49): loss=2.2535049452874203e+50\n",
      "Gradient Descent(30/49): loss=1.6719421739288533e+52\n",
      "Gradient Descent(31/49): loss=1.2404635005606012e+54\n",
      "Gradient Descent(32/49): loss=9.203366720556038e+55\n",
      "Gradient Descent(33/49): loss=6.828250807440882e+57\n",
      "Gradient Descent(34/49): loss=5.066081848632328e+59\n",
      "Gradient Descent(35/49): loss=3.758676419599939e+61\n",
      "Gradient Descent(36/49): loss=2.7886735448363906e+63\n",
      "Gradient Descent(37/49): loss=2.0689996348496171e+65\n",
      "Gradient Descent(38/49): loss=1.5350522103723085e+67\n",
      "Gradient Descent(39/49): loss=1.1389007754659235e+69\n",
      "Gradient Descent(40/49): loss=8.449842732334624e+70\n",
      "Gradient Descent(41/49): loss=6.2691890056867106e+72\n",
      "Gradient Descent(42/49): loss=4.651297312152904e+74\n",
      "Gradient Descent(43/49): loss=3.450935466519806e+76\n",
      "Gradient Descent(44/49): loss=2.560351401955932e+78\n",
      "Gradient Descent(45/49): loss=1.8996006633843913e+80\n",
      "Gradient Descent(46/49): loss=1.4093700878612633e+82\n",
      "Gradient Descent(47/49): loss=1.045653480147354e+84\n",
      "Gradient Descent(48/49): loss=7.7580133845718e+85\n",
      "Gradient Descent(49/49): loss=5.755900287991324e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6211656656718219\n",
      "Gradient Descent(2/49): loss=19.868034614853887\n",
      "Gradient Descent(3/49): loss=344.8479244501008\n",
      "Gradient Descent(4/49): loss=8115.62149066653\n",
      "Gradient Descent(5/49): loss=319387.21689200454\n",
      "Gradient Descent(6/49): loss=18802468.59386734\n",
      "Gradient Descent(7/49): loss=1288866123.6714544\n",
      "Gradient Descent(8/49): loss=91902402863.12856\n",
      "Gradient Descent(9/49): loss=6612700467830.154\n",
      "Gradient Descent(10/49): loss=476768440982614.44\n",
      "Gradient Descent(11/49): loss=3.4389847078793212e+16\n",
      "Gradient Descent(12/49): loss=2.4808236750528543e+18\n",
      "Gradient Descent(13/49): loss=1.7896620695272197e+20\n",
      "Gradient Descent(14/49): loss=1.2910654672557941e+22\n",
      "Gradient Descent(15/49): loss=9.313779659055448e+23\n",
      "Gradient Descent(16/49): loss=6.718986308273396e+25\n",
      "Gradient Descent(17/49): loss=4.847095487952187e+27\n",
      "Gradient Descent(18/49): loss=3.49670826247963e+29\n",
      "Gradient Descent(19/49): loss=2.52253514179146e+31\n",
      "Gradient Descent(20/49): loss=1.8197639229516803e+33\n",
      "Gradient Descent(21/49): loss=1.3127827956980556e+35\n",
      "Gradient Descent(22/49): loss=9.470451892093292e+36\n",
      "Gradient Descent(23/49): loss=6.832010545454138e+38\n",
      "Gradient Descent(24/49): loss=4.928631561100956e+40\n",
      "Gradient Descent(25/49): loss=3.555528625647371e+42\n",
      "Gradient Descent(26/49): loss=2.5649683185032377e+44\n",
      "Gradient Descent(27/49): loss=1.8503753358834384e+46\n",
      "Gradient Descent(28/49): loss=1.3348659548526646e+48\n",
      "Gradient Descent(29/49): loss=9.629760421411916e+49\n",
      "Gradient Descent(30/49): loss=6.94693616513952e+51\n",
      "Gradient Descent(31/49): loss=5.011539225338936e+53\n",
      "Gradient Descent(32/49): loss=3.615338447061504e+55\n",
      "Gradient Descent(33/49): loss=2.6081152913488224e+57\n",
      "Gradient Descent(34/49): loss=1.8815016830572157e+59\n",
      "Gradient Descent(35/49): loss=1.3573205889668794e+61\n",
      "Gradient Descent(36/49): loss=9.791748781429862e+62\n",
      "Gradient Descent(37/49): loss=7.063795022192262e+64\n",
      "Gradient Descent(38/49): loss=5.0958415324317484e+66\n",
      "Gradient Descent(39/49): loss=3.6761543677406685e+68\n",
      "Gradient Descent(40/49): loss=2.6519880670249143e+70\n",
      "Gradient Descent(41/49): loss=1.9131516264277548e+72\n",
      "Gradient Descent(42/49): loss=1.3801529468453457e+74\n",
      "Gradient Descent(43/49): loss=9.956462051272804e+75\n",
      "Gradient Descent(44/49): loss=7.182619636832446e+77\n",
      "Gradient Descent(45/49): loss=5.181561942559327e+79\n",
      "Gradient Descent(46/49): loss=3.7379933119247775e+81\n",
      "Gradient Descent(47/49): loss=2.6965988547254383e+83\n",
      "Gradient Descent(48/49): loss=1.9453339737417067e+85\n",
      "Gradient Descent(49/49): loss=1.4033693824211823e+87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6466578452520244\n",
      "Gradient Descent(2/49): loss=20.16241175057015\n",
      "Gradient Descent(3/49): loss=345.10700817777644\n",
      "Gradient Descent(4/49): loss=7863.295313766165\n",
      "Gradient Descent(5/49): loss=297877.2073005016\n",
      "Gradient Descent(6/49): loss=17216633.65988519\n",
      "Gradient Descent(7/49): loss=1174403862.05102\n",
      "Gradient Descent(8/49): loss=83665351313.05338\n",
      "Gradient Descent(9/49): loss=6020115764933.841\n",
      "Gradient Descent(10/49): loss=434136771852084.7\n",
      "Gradient Descent(11/49): loss=3.1322803673190884e+16\n",
      "Gradient Descent(12/49): loss=2.260171761375154e+18\n",
      "Gradient Descent(13/49): loss=1.6309196110244246e+20\n",
      "Gradient Descent(14/49): loss=1.1768630606445918e+22\n",
      "Gradient Descent(15/49): loss=8.49219204285985e+23\n",
      "Gradient Descent(16/49): loss=6.127929946862429e+25\n",
      "Gradient Descent(17/49): loss=4.42188864007251e+27\n",
      "Gradient Descent(18/49): loss=3.190816402000143e+29\n",
      "Gradient Descent(19/49): loss=2.302479815177728e+31\n",
      "Gradient Descent(20/49): loss=1.6614598380292443e+33\n",
      "Gradient Descent(21/49): loss=1.198902494409813e+35\n",
      "Gradient Descent(22/49): loss=8.651230431476136e+36\n",
      "Gradient Descent(23/49): loss=6.2426918225588235e+38\n",
      "Gradient Descent(24/49): loss=4.504700400732074e+40\n",
      "Gradient Descent(25/49): loss=3.2505730343813266e+42\n",
      "Gradient Descent(26/49): loss=2.345599953801636e+44\n",
      "Gradient Descent(27/49): loss=1.692575150621591e+46\n",
      "Gradient Descent(28/49): loss=1.2213551743376093e+48\n",
      "Gradient Descent(29/49): loss=8.813248034117785e+49\n",
      "Gradient Descent(30/49): loss=6.359603049375787e+51\n",
      "Gradient Descent(31/49): loss=4.589063054739941e+53\n",
      "Gradient Descent(32/49): loss=3.311448773905175e+55\n",
      "Gradient Descent(33/49): loss=2.389527633722949e+57\n",
      "Gradient Descent(34/49): loss=1.7242731813700988e+59\n",
      "Gradient Descent(35/49): loss=1.2442283412139902e+61\n",
      "Gradient Descent(36/49): loss=8.9782998529852e+62\n",
      "Gradient Descent(37/49): loss=6.478703753963976e+64\n",
      "Gradient Descent(38/49): loss=4.675005626780364e+66\n",
      "Gradient Descent(39/49): loss=3.373464575696194e+68\n",
      "Gradient Descent(40/49): loss=2.4342779778245516e+70\n",
      "Gradient Descent(41/49): loss=1.7565648431623984e+72\n",
      "Gradient Descent(42/49): loss=1.2675298697774808e+74\n",
      "Gradient Descent(43/49): loss=9.146442711933221e+75\n",
      "Gradient Descent(44/49): loss=6.600034940191478e+77\n",
      "Gradient Descent(45/49): loss=4.762557705075405e+79\n",
      "Gradient Descent(46/49): loss=3.436641790492573e+81\n",
      "Gradient Descent(47/49): loss=2.479866392710294e+83\n",
      "Gradient Descent(48/49): loss=1.789461253339555e+85\n",
      "Gradient Descent(49/49): loss=1.2912677822549127e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.5970890583073802\n",
      "Gradient Descent(2/49): loss=19.108377506096787\n",
      "Gradient Descent(3/49): loss=315.9322211770574\n",
      "Gradient Descent(4/49): loss=6575.510828935709\n",
      "Gradient Descent(5/49): loss=220298.1780553531\n",
      "Gradient Descent(6/49): loss=11996709.454389887\n",
      "Gradient Descent(7/49): loss=813011136.448324\n",
      "Gradient Descent(8/49): loss=58499926971.67905\n",
      "Gradient Descent(9/49): loss=4267602817209.9575\n",
      "Gradient Descent(10/49): loss=312263667056304.7\n",
      "Gradient Descent(11/49): loss=2.2863511718772824e+16\n",
      "Gradient Descent(12/49): loss=1.6742716168038894e+18\n",
      "Gradient Descent(13/49): loss=1.2260895825315327e+20\n",
      "Gradient Descent(14/49): loss=8.978863517236967e+21\n",
      "Gradient Descent(15/49): loss=6.575384613307284e+23\n",
      "Gradient Descent(16/49): loss=4.815274912696701e+25\n",
      "Gradient Descent(17/49): loss=3.526314490564246e+27\n",
      "Gradient Descent(18/49): loss=2.5823850751262686e+29\n",
      "Gradient Descent(19/49): loss=1.8911281778929385e+31\n",
      "Gradient Descent(20/49): loss=1.3849080147278076e+33\n",
      "Gradient Descent(21/49): loss=1.0141936606734273e+35\n",
      "Gradient Descent(22/49): loss=7.427127075909865e+36\n",
      "Gradient Descent(23/49): loss=5.439022027160718e+38\n",
      "Gradient Descent(24/49): loss=3.983096062532175e+40\n",
      "Gradient Descent(25/49): loss=2.9168946483649984e+42\n",
      "Gradient Descent(26/49): loss=2.1360957044686947e+44\n",
      "Gradient Descent(27/49): loss=1.564302249039818e+46\n",
      "Gradient Descent(28/49): loss=1.1455673644359318e+48\n",
      "Gradient Descent(29/49): loss=8.389200918595891e+49\n",
      "Gradient Descent(30/49): loss=6.143566431575509e+51\n",
      "Gradient Descent(31/49): loss=4.4990469134573245e+53\n",
      "Gradient Descent(32/49): loss=3.294734964605673e+55\n",
      "Gradient Descent(33/49): loss=2.412795131014403e+57\n",
      "Gradient Descent(34/49): loss=1.7669343382051543e+59\n",
      "Gradient Descent(35/49): loss=1.2939585775008666e+61\n",
      "Gradient Descent(36/49): loss=9.47589711787972e+62\n",
      "Gradient Descent(37/49): loss=6.939374084297528e+64\n",
      "Gradient Descent(38/49): loss=5.081831522944601e+66\n",
      "Gradient Descent(39/49): loss=3.72151887387515e+68\n",
      "Gradient Descent(40/49): loss=2.7253368526833233e+70\n",
      "Gradient Descent(41/49): loss=1.995814400602466e+72\n",
      "Gradient Descent(42/49): loss=1.4615716650697306e+74\n",
      "Gradient Descent(43/49): loss=1.0703358646424533e+76\n",
      "Gradient Descent(44/49): loss=7.838266781706189e+77\n",
      "Gradient Descent(45/49): loss=5.740107210340181e+79\n",
      "Gradient Descent(46/49): loss=4.203586290670663e+81\n",
      "Gradient Descent(47/49): loss=3.0783637056958004e+83\n",
      "Gradient Descent(48/49): loss=2.2543424707556728e+85\n",
      "Gradient Descent(49/49): loss=1.6508965350811843e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6295205771559922\n",
      "Gradient Descent(2/49): loss=20.083201398263448\n",
      "Gradient Descent(3/49): loss=356.1792518355588\n",
      "Gradient Descent(4/49): loss=8958.759816166967\n",
      "Gradient Descent(5/49): loss=389810.6955671053\n",
      "Gradient Descent(6/49): loss=24789010.37504463\n",
      "Gradient Descent(7/49): loss=1793016336.4385035\n",
      "Gradient Descent(8/49): loss=133796647724.61478\n",
      "Gradient Descent(9/49): loss=10052472767064.736\n",
      "Gradient Descent(10/49): loss=756372854816480.1\n",
      "Gradient Descent(11/49): loss=5.692910453005616e+16\n",
      "Gradient Descent(12/49): loss=4.2851062081621796e+18\n",
      "Gradient Descent(13/49): loss=3.2254845189590535e+20\n",
      "Gradient Descent(14/49): loss=2.4278935324658774e+22\n",
      "Gradient Descent(15/49): loss=1.8275303261388842e+24\n",
      "Gradient Descent(16/49): loss=1.3756235612650887e+26\n",
      "Gradient Descent(17/49): loss=1.0354631110890474e+28\n",
      "Gradient Descent(18/49): loss=7.794166160168355e+29\n",
      "Gradient Descent(19/49): loss=5.866846009424456e+31\n",
      "Gradient Descent(20/49): loss=4.4161083303312697e+33\n",
      "Gradient Descent(21/49): loss=3.3241051077618415e+35\n",
      "Gradient Descent(22/49): loss=2.5021294635584706e+37\n",
      "Gradient Descent(23/49): loss=1.8834097146343114e+39\n",
      "Gradient Descent(24/49): loss=1.4176852976009379e+41\n",
      "Gradient Descent(25/49): loss=1.0671239440985464e+43\n",
      "Gradient Descent(26/49): loss=8.032484458966348e+44\n",
      "Gradient Descent(27/49): loss=6.046233611414264e+46\n",
      "Gradient Descent(28/49): loss=4.55113745573315e+48\n",
      "Gradient Descent(29/49): loss=3.425744599393994e+50\n",
      "Gradient Descent(30/49): loss=2.578635818940976e+52\n",
      "Gradient Descent(31/49): loss=1.940997787138495e+54\n",
      "Gradient Descent(32/49): loss=1.461033148614134e+56\n",
      "Gradient Descent(33/49): loss=1.099752856749145e+58\n",
      "Gradient Descent(34/49): loss=8.278089700258688e+59\n",
      "Gradient Descent(35/49): loss=6.231106258554421e+61\n",
      "Gradient Descent(36/49): loss=4.690295298948356e+63\n",
      "Gradient Descent(37/49): loss=3.5304918707069024e+65\n",
      "Gradient Descent(38/49): loss=2.6574814707129675e+67\n",
      "Gradient Descent(39/49): loss=2.0003467012002813e+69\n",
      "Gradient Descent(40/49): loss=1.5057064250872312e+71\n",
      "Gradient Descent(41/49): loss=1.1333794472671322e+73\n",
      "Gradient Descent(42/49): loss=8.531204689606758e+74\n",
      "Gradient Descent(43/49): loss=6.421631663734865e+76\n",
      "Gradient Descent(44/49): loss=4.833708101614226e+78\n",
      "Gradient Descent(45/49): loss=3.6384419467033755e+80\n",
      "Gradient Descent(46/49): loss=2.7387379463624098e+82\n",
      "Gradient Descent(47/49): loss=2.061510297186858e+84\n",
      "Gradient Descent(48/49): loss=1.5517456538885087e+86\n",
      "Gradient Descent(49/49): loss=1.1680342211473367e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6417616477131167\n",
      "Gradient Descent(2/49): loss=20.508828751176832\n",
      "Gradient Descent(3/49): loss=361.73344647942645\n",
      "Gradient Descent(4/49): loss=8633.456347036441\n",
      "Gradient Descent(5/49): loss=344036.94243279943\n",
      "Gradient Descent(6/49): loss=20520600.21391485\n",
      "Gradient Descent(7/49): loss=1426447864.6251106\n",
      "Gradient Descent(8/49): loss=103180721469.8688\n",
      "Gradient Descent(9/49): loss=7532117656401.894\n",
      "Gradient Descent(10/49): loss=550964325184447.3\n",
      "Gradient Descent(11/49): loss=4.0320580561207816e+16\n",
      "Gradient Descent(12/49): loss=2.951030381338172e+18\n",
      "Gradient Descent(13/49): loss=2.1598830584740158e+20\n",
      "Gradient Descent(14/49): loss=1.5808436977129765e+22\n",
      "Gradient Descent(15/49): loss=1.157039270541382e+24\n",
      "Gradient Descent(16/49): loss=8.468517148551533e+25\n",
      "Gradient Descent(17/49): loss=6.198215423483236e+27\n",
      "Gradient Descent(18/49): loss=4.536552765473727e+29\n",
      "Gradient Descent(19/49): loss=3.320360723470169e+31\n",
      "Gradient Descent(20/49): loss=2.4302142861095576e+33\n",
      "Gradient Descent(21/49): loss=1.7787047760869272e+35\n",
      "Gradient Descent(22/49): loss=1.301856671095814e+37\n",
      "Gradient Descent(23/49): loss=9.528454720945142e+38\n",
      "Gradient Descent(24/49): loss=6.973997321287969e+40\n",
      "Gradient Descent(25/49): loss=5.1043574285381055e+42\n",
      "Gradient Descent(26/49): loss=3.735944187810883e+44\n",
      "Gradient Descent(27/49): loss=2.7343851150398543e+46\n",
      "Gradient Descent(28/49): loss=2.001331278380984e+48\n",
      "Gradient Descent(29/49): loss=1.4647998424932128e+50\n",
      "Gradient Descent(30/49): loss=1.0721056537445982e+52\n",
      "Gradient Descent(31/49): loss=7.846877774336044e+53\n",
      "Gradient Descent(32/49): loss=5.74322974515709e+55\n",
      "Gradient Descent(33/49): loss=4.203542970114451e+57\n",
      "Gradient Descent(34/49): loss=3.076626617017737e+59\n",
      "Gradient Descent(35/49): loss=2.2518221909087197e+61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/49): loss=1.6481373304843942e+63\n",
      "Gradient Descent(37/49): loss=1.20629269535711e+65\n",
      "Gradient Descent(38/49): loss=8.829009815852158e+66\n",
      "Gradient Descent(39/49): loss=6.46206469030615e+68\n",
      "Gradient Descent(40/49): loss=4.729667418279105e+70\n",
      "Gradient Descent(41/49): loss=3.4617037989556806e+72\n",
      "Gradient Descent(42/49): loss=2.533665082959421e+74\n",
      "Gradient Descent(43/49): loss=1.8544217314446008e+76\n",
      "Gradient Descent(44/49): loss=1.3572748747191306e+78\n",
      "Gradient Descent(45/49): loss=9.934067608821538e+79\n",
      "Gradient Descent(46/49): loss=7.270870557966845e+81\n",
      "Gradient Descent(47/49): loss=5.321642730090172e+83\n",
      "Gradient Descent(48/49): loss=3.894978066373556e+85\n",
      "Gradient Descent(49/49): loss=2.8507840354915967e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6677028702521863\n",
      "Gradient Descent(2/49): loss=20.81338503201534\n",
      "Gradient Descent(3/49): loss=362.0339867104571\n",
      "Gradient Descent(4/49): loss=8366.61728891567\n",
      "Gradient Descent(5/49): loss=320921.4688730567\n",
      "Gradient Descent(6/49): loss=18791080.460787293\n",
      "Gradient Descent(7/49): loss=1299789126.0791752\n",
      "Gradient Descent(8/49): loss=93933088551.7314\n",
      "Gradient Descent(9/49): loss=6857136052276.315\n",
      "Gradient Descent(10/49): loss=501697320083335.4\n",
      "Gradient Descent(11/49): loss=3.672452745294647e+16\n",
      "Gradient Descent(12/49): loss=2.6885503557726874e+18\n",
      "Gradient Descent(13/49): loss=1.9682969260156767e+20\n",
      "Gradient Descent(14/49): loss=1.441004590613744e+22\n",
      "Gradient Descent(15/49): loss=1.0549712491246477e+24\n",
      "Gradient Descent(16/49): loss=7.72353281657417e+25\n",
      "Gradient Descent(17/49): loss=5.65446334405363e+27\n",
      "Gradient Descent(18/49): loss=4.1396802306409906e+29\n",
      "Gradient Descent(19/49): loss=3.030694765670377e+31\n",
      "Gradient Descent(20/49): loss=2.2187971670599103e+33\n",
      "Gradient Descent(21/49): loss=1.6244000963021107e+35\n",
      "Gradient Descent(22/49): loss=1.1892369938623977e+37\n",
      "Gradient Descent(23/49): loss=8.706504209152372e+38\n",
      "Gradient Descent(24/49): loss=6.374105072017967e+40\n",
      "Gradient Descent(25/49): loss=4.666536016421844e+42\n",
      "Gradient Descent(26/49): loss=3.4164103268648394e+44\n",
      "Gradient Descent(27/49): loss=2.501182778925242e+46\n",
      "Gradient Descent(28/49): loss=1.8311369815268064e+48\n",
      "Gradient Descent(29/49): loss=1.340590809023535e+50\n",
      "Gradient Descent(30/49): loss=9.814578239471271e+51\n",
      "Gradient Descent(31/49): loss=7.185335403639117e+53\n",
      "Gradient Descent(32/49): loss=5.260444575718265e+55\n",
      "Gradient Descent(33/49): loss=3.851215786000611e+57\n",
      "Gradient Descent(34/49): loss=2.819507518205415e+59\n",
      "Gradient Descent(35/49): loss=2.0641852046083888e+61\n",
      "Gradient Descent(36/49): loss=1.5112073762570286e+63\n",
      "Gradient Descent(37/49): loss=1.1063676500321528e+65\n",
      "Gradient Descent(38/49): loss=8.099810762367886e+66\n",
      "Gradient Descent(39/49): loss=5.929939689059334e+68\n",
      "Gradient Descent(40/49): loss=4.3413587980667255e+70\n",
      "Gradient Descent(41/49): loss=3.1783453461297244e+72\n",
      "Gradient Descent(42/49): loss=2.326893401154203e+74\n",
      "Gradient Descent(43/49): loss=1.7035382599088118e+76\n",
      "Gradient Descent(44/49): loss=1.247174710080697e+78\n",
      "Gradient Descent(45/49): loss=9.130671110070278e+79\n",
      "Gradient Descent(46/49): loss=6.6846412332138375e+81\n",
      "Gradient Descent(47/49): loss=4.893882155880205e+83\n",
      "Gradient Descent(48/49): loss=3.582852350645515e+85\n",
      "Gradient Descent(49/49): loss=2.6230363865836487e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.617294480678097\n",
      "Gradient Descent(2/49): loss=19.72552468143141\n",
      "Gradient Descent(3/49): loss=331.4916853815885\n",
      "Gradient Descent(4/49): loss=7000.382908853485\n",
      "Gradient Descent(5/49): loss=237480.39934649118\n",
      "Gradient Descent(6/49): loss=13096430.571991557\n",
      "Gradient Descent(7/49): loss=899808639.5600015\n",
      "Gradient Descent(8/49): loss=65674589635.24679\n",
      "Gradient Descent(9/49): loss=4860491098866.83\n",
      "Gradient Descent(10/49): loss=360818374218752.2\n",
      "Gradient Descent(11/49): loss=2.680311922257407e+16\n",
      "Gradient Descent(12/49): loss=1.991335986602113e+18\n",
      "Gradient Descent(13/49): loss=1.47950793409353e+20\n",
      "Gradient Descent(14/49): loss=1.0992411872210142e+22\n",
      "Gradient Descent(15/49): loss=8.167127270054992e+23\n",
      "Gradient Descent(16/49): loss=6.0680030753842065e+25\n",
      "Gradient Descent(17/49): loss=4.5083984419824355e+27\n",
      "Gradient Descent(18/49): loss=3.349645107743895e+29\n",
      "Gradient Descent(19/49): loss=2.4887157886655457e+31\n",
      "Gradient Descent(20/49): loss=1.849063432654648e+33\n",
      "Gradient Descent(21/49): loss=1.373815199819966e+35\n",
      "Gradient Descent(22/49): loss=1.020715768852236e+37\n",
      "Gradient Descent(23/49): loss=7.583703258833856e+38\n",
      "Gradient Descent(24/49): loss=5.634531852369481e+40\n",
      "Gradient Descent(25/49): loss=4.186338535648276e+42\n",
      "Gradient Descent(26/49): loss=3.11036139190255e+44\n",
      "Gradient Descent(27/49): loss=2.310933027956778e+46\n",
      "Gradient Descent(28/49): loss=1.7169745848841622e+48\n",
      "Gradient Descent(29/49): loss=1.275675967011735e+50\n",
      "Gradient Descent(30/49): loss=9.478003851298398e+51\n",
      "Gradient Descent(31/49): loss=7.041957309555387e+53\n",
      "Gradient Descent(32/49): loss=5.232026018095464e+55\n",
      "Gradient Descent(33/49): loss=3.8872851752285855e+57\n",
      "Gradient Descent(34/49): loss=2.8881710414453997e+59\n",
      "Gradient Descent(35/49): loss=2.1458502756112576e+61\n",
      "Gradient Descent(36/49): loss=1.5943215755796842e+63\n",
      "Gradient Descent(37/49): loss=1.1845473634617317e+65\n",
      "Gradient Descent(38/49): loss=8.80093751333643e+66\n",
      "Gradient Descent(39/49): loss=6.538911275552029e+68\n",
      "Gradient Descent(40/49): loss=4.858273406071741e+70\n",
      "Gradient Descent(41/49): loss=3.609594853563947e+72\n",
      "Gradient Descent(42/49): loss=2.681852979001087e+74\n",
      "Gradient Descent(43/49): loss=1.9925602990805644e+76\n",
      "Gradient Descent(44/49): loss=1.4804303504179338e+78\n",
      "Gradient Descent(45/49): loss=1.0999285810572259e+80\n",
      "Gradient Descent(46/49): loss=8.172237775893953e+81\n",
      "Gradient Descent(47/49): loss=6.071800607413671e+83\n",
      "Gradient Descent(48/49): loss=4.511220014294731e+85\n",
      "Gradient Descent(49/49): loss=3.3517414904113652e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.650099947884605\n",
      "Gradient Descent(2/49): loss=20.726374110923732\n",
      "Gradient Descent(3/49): loss=373.4580538567468\n",
      "Gradient Descent(4/49): loss=9522.834403385415\n",
      "Gradient Descent(5/49): loss=419499.04749760526\n",
      "Gradient Descent(6/49): loss=27030070.968797795\n",
      "Gradient Descent(7/49): loss=1982592440.7892168\n",
      "Gradient Descent(8/49): loss=150064525969.61752\n",
      "Gradient Descent(9/49): loss=11437266085857.242\n",
      "Gradient Descent(10/49): loss=872991409427948.5\n",
      "Gradient Descent(11/49): loss=6.6655362116300424e+16\n",
      "Gradient Descent(12/49): loss=5.089668624255913e+18\n",
      "Gradient Descent(13/49): loss=3.88642389383254e+20\n",
      "Gradient Descent(14/49): loss=2.96764644152893e+22\n",
      "Gradient Descent(15/49): loss=2.2660758057218862e+24\n",
      "Gradient Descent(16/49): loss=1.7303612049910743e+26\n",
      "Gradient Descent(17/49): loss=1.3212929510487632e+28\n",
      "Gradient Descent(18/49): loss=1.0089310072982675e+30\n",
      "Gradient Descent(19/49): loss=7.704133879268841e+31\n",
      "Gradient Descent(20/49): loss=5.882828302175693e+33\n",
      "Gradient Descent(21/49): loss=4.49209078883328e+35\n",
      "Gradient Descent(22/49): loss=3.4301323477215124e+37\n",
      "Gradient Descent(23/49): loss=2.6192275437011372e+39\n",
      "Gradient Descent(24/49): loss=2.0000257221106445e+41\n",
      "Gradient Descent(25/49): loss=1.5272070953607558e+43\n",
      "Gradient Descent(26/49): loss=1.1661657579378195e+45\n",
      "Gradient Descent(27/49): loss=8.904768574726036e+46\n",
      "Gradient Descent(28/49): loss=6.79962542457493e+48\n",
      "Gradient Descent(29/49): loss=5.192151320557887e+50\n",
      "Gradient Descent(30/49): loss=3.9646941783203235e+52\n",
      "Gradient Descent(31/49): loss=3.0274156042735787e+54\n",
      "Gradient Descent(32/49): loss=2.3117155646245597e+56\n",
      "Gradient Descent(33/49): loss=1.7652115038925294e+58\n",
      "Gradient Descent(34/49): loss=1.3479044313051375e+60\n",
      "Gradient Descent(35/49): loss=1.0292513684199388e+62\n",
      "Gradient Descent(36/49): loss=7.8592988849258e+63\n",
      "Gradient Descent(37/49): loss=6.001311327612819e+65\n",
      "Gradient Descent(38/49): loss=4.582563683894692e+67\n",
      "Gradient Descent(39/49): loss=3.4992168828714006e+69\n",
      "Gradient Descent(40/49): loss=2.6719800613802975e+71\n",
      "Gradient Descent(41/49): loss=2.040307213697353e+73\n",
      "Gradient Descent(42/49): loss=1.5579657896529909e+75\n",
      "Gradient Descent(43/49): loss=1.189652903951886e+77\n",
      "Gradient Descent(44/49): loss=9.084114948354578e+78\n",
      "Gradient Descent(45/49): loss=6.936573190448589e+80\n",
      "Gradient Descent(46/49): loss=5.2967237755137245e+82\n",
      "Gradient Descent(47/49): loss=4.044545048947764e+84\n",
      "Gradient Descent(48/49): loss=3.0883892281849363e+86\n",
      "Gradient Descent(49/49): loss=2.3582746413592514e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6625294450513275\n",
      "Gradient Descent(2/49): loss=21.165306190150634\n",
      "Gradient Descent(3/49): loss=379.30418253458174\n",
      "Gradient Descent(4/49): loss=9180.066505028784\n",
      "Gradient Descent(5/49): loss=370390.15292909957\n",
      "Gradient Descent(6/49): loss=22381696.789063994\n",
      "Gradient Descent(7/49): loss=1577567744.292424\n",
      "Gradient Descent(8/49): loss=115746912596.20178\n",
      "Gradient Descent(9/49): loss=8571354058267.116\n",
      "Gradient Descent(10/49): loss=636045896870939.2\n",
      "Gradient Descent(11/49): loss=4.722013279936794e+16\n",
      "Gradient Descent(12/49): loss=3.5059866466515246e+18\n",
      "Gradient Descent(13/49): loss=2.6031736267897602e+20\n",
      "Gradient Descent(14/49): loss=1.9328501803241165e+22\n",
      "Gradient Descent(15/49): loss=1.4351382236537644e+24\n",
      "Gradient Descent(16/49): loss=1.065588140181291e+26\n",
      "Gradient Descent(17/49): loss=7.911977591193752e+27\n",
      "Gradient Descent(18/49): loss=5.874632778086425e+29\n",
      "Gradient Descent(19/49): loss=4.361906991312992e+31\n",
      "Gradient Descent(20/49): loss=3.238710117026784e+33\n",
      "Gradient Descent(21/49): loss=2.40473793788085e+35\n",
      "Gradient Descent(22/49): loss=1.7855147083560248e+37\n",
      "Gradient Descent(23/49): loss=1.3257422871558057e+39\n",
      "Gradient Descent(24/49): loss=9.843618782473732e+40\n",
      "Gradient Descent(25/49): loss=7.308873804016444e+42\n",
      "Gradient Descent(26/49): loss=5.426829041587183e+44\n",
      "Gradient Descent(27/49): loss=4.029413318154453e+46\n",
      "Gradient Descent(28/49): loss=2.9918340091605395e+48\n",
      "Gradient Descent(29/49): loss=2.2214327574787777e+50\n",
      "Gradient Descent(30/49): loss=1.6494108566486174e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=1.2246853589743304e+54\n",
      "Gradient Descent(32/49): loss=9.093272439915816e+55\n",
      "Gradient Descent(33/49): loss=6.75174264643658e+57\n",
      "Gradient Descent(34/49): loss=5.01315990089613e+59\n",
      "Gradient Descent(35/49): loss=3.7222645334707837e+61\n",
      "Gradient Descent(36/49): loss=2.763776446599595e+63\n",
      "Gradient Descent(37/49): loss=2.0521003217513024e+65\n",
      "Gradient Descent(38/49): loss=1.5236817491924598e+67\n",
      "Gradient Descent(39/49): loss=1.1313316645459706e+69\n",
      "Gradient Descent(40/49): loss=8.400122505127158e+70\n",
      "Gradient Descent(41/49): loss=6.237079745263088e+72\n",
      "Gradient Descent(42/49): loss=4.631023383888443e+74\n",
      "Gradient Descent(43/49): loss=3.438528679773526e+76\n",
      "Gradient Descent(44/49): loss=2.553102954038129e+78\n",
      "Gradient Descent(45/49): loss=1.8956755347892943e+80\n",
      "Gradient Descent(46/49): loss=1.4075365537119907e+82\n",
      "Gradient Descent(47/49): loss=1.0450940119642355e+84\n",
      "Gradient Descent(48/49): loss=7.759809086045203e+85\n",
      "Gradient Descent(49/49): loss=5.761647886461134e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.688923368734105\n",
      "Gradient Descent(2/49): loss=21.480301489702757\n",
      "Gradient Descent(3/49): loss=379.64894719284115\n",
      "Gradient Descent(4/49): loss=8897.994233400954\n",
      "Gradient Descent(5/49): loss=345562.2497530994\n",
      "Gradient Descent(6/49): loss=20496648.761626087\n",
      "Gradient Descent(7/49): loss=1437514820.0426078\n",
      "Gradient Descent(8/49): loss=105373337270.61604\n",
      "Gradient Descent(9/49): loss=7803236992101.333\n",
      "Gradient Descent(10/49): loss=579169921915146.5\n",
      "Gradient Descent(11/49): loss=4.3008640373954424e+16\n",
      "Gradient Descent(12/49): loss=3.19413829027534e+18\n",
      "Gradient Descent(13/49): loss=2.3722605313260886e+20\n",
      "Gradient Descent(14/49): loss=1.7618681621942685e+22\n",
      "Gradient Descent(15/49): loss=1.3085338223670608e+24\n",
      "Gradient Descent(16/49): loss=9.718441221766785e+25\n",
      "Gradient Descent(17/49): loss=7.217857397961609e+27\n",
      "Gradient Descent(18/49): loss=5.36068129664208e+29\n",
      "Gradient Descent(19/49): loss=3.981362121984739e+31\n",
      "Gradient Descent(20/49): loss=2.9569458580367884e+33\n",
      "Gradient Descent(21/49): loss=2.1961149327001168e+35\n",
      "Gradient Descent(22/49): loss=1.6310480574638648e+37\n",
      "Gradient Descent(23/49): loss=1.2113745624887838e+39\n",
      "Gradient Descent(24/49): loss=8.996843004913245e+40\n",
      "Gradient Descent(25/49): loss=6.681928658695253e+42\n",
      "Gradient Descent(26/49): loss=4.962648628581564e+44\n",
      "Gradient Descent(27/49): loss=3.6857444412722085e+46\n",
      "Gradient Descent(28/49): loss=2.737391482469661e+48\n",
      "Gradient Descent(29/49): loss=2.033052548187767e+50\n",
      "Gradient Descent(30/49): loss=1.5099421073538795e+52\n",
      "Gradient Descent(31/49): loss=1.1214295319580417e+54\n",
      "Gradient Descent(32/49): loss=8.328823926577856e+55\n",
      "Gradient Descent(33/49): loss=6.1857928673247354e+57\n",
      "Gradient Descent(34/49): loss=4.594170045466115e+59\n",
      "Gradient Descent(35/49): loss=3.412076488714798e+61\n",
      "Gradient Descent(36/49): loss=2.5341391044786184e+63\n",
      "Gradient Descent(37/49): loss=1.8820976089157487e+65\n",
      "Gradient Descent(38/49): loss=1.3978283209576567e+67\n",
      "Gradient Descent(39/49): loss=1.0381629547879335e+69\n",
      "Gradient Descent(40/49): loss=7.710405523588468e+70\n",
      "Gradient Descent(41/49): loss=5.726495350657794e+72\n",
      "Gradient Descent(42/49): loss=4.253051139889154e+74\n",
      "Gradient Descent(43/49): loss=3.1587284876490235e+76\n",
      "Gradient Descent(44/49): loss=2.345978294290017e+78\n",
      "Gradient Descent(45/49): loss=1.7423511323621602e+80\n",
      "Gradient Descent(46/49): loss=1.2940390266323742e+82\n",
      "Gradient Descent(47/49): loss=9.610789532288111e+83\n",
      "Gradient Descent(48/49): loss=7.137904926586055e+85\n",
      "Gradient Descent(49/49): loss=5.301300852527407e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6376686961771576\n",
      "Gradient Descent(2/49): loss=20.357808280548515\n",
      "Gradient Descent(3/49): loss=347.6859844851513\n",
      "Gradient Descent(4/49): loss=7449.151959117603\n",
      "Gradient Descent(5/49): loss=255863.20531403003\n",
      "Gradient Descent(6/49): loss=14287985.956507955\n",
      "Gradient Descent(7/49): loss=995149206.9714732\n",
      "Gradient Descent(8/49): loss=73668041645.55833\n",
      "Gradient Descent(9/49): loss=5530584634093.784\n",
      "Gradient Descent(10/49): loss=416490920969229.25\n",
      "Gradient Descent(11/49): loss=3.138574279909828e+16\n",
      "Gradient Descent(12/49): loss=2.3654992261667686e+18\n",
      "Gradient Descent(13/49): loss=1.782900081942982e+20\n",
      "Gradient Descent(14/49): loss=1.3437986338154266e+22\n",
      "Gradient Descent(15/49): loss=1.0128427785711149e+24\n",
      "Gradient Descent(16/49): loss=7.633962425007487e+25\n",
      "Gradient Descent(17/49): loss=5.753843306663392e+27\n",
      "Gradient Descent(18/49): loss=4.336766604565906e+29\n",
      "Gradient Descent(19/49): loss=3.2686925317061836e+31\n",
      "Gradient Descent(20/49): loss=2.4636674851967278e+33\n",
      "Gradient Descent(21/49): loss=1.8569068273301743e+35\n",
      "Gradient Descent(22/49): loss=1.3995813096618517e+37\n",
      "Gradient Descent(23/49): loss=1.0548875223780866e+39\n",
      "Gradient Descent(24/49): loss=7.950861283933679e+40\n",
      "Gradient Descent(25/49): loss=5.992695317304134e+42\n",
      "Gradient Descent(26/49): loss=4.516793323839085e+44\n",
      "Gradient Descent(27/49): loss=3.4043816429925505e+46\n",
      "Gradient Descent(28/49): loss=2.565938607368898e+48\n",
      "Gradient Descent(29/49): loss=1.9339902593878175e+50\n",
      "Gradient Descent(30/49): loss=1.457680364084069e+52\n",
      "Gradient Descent(31/49): loss=1.0986777381747798e+54\n",
      "Gradient Descent(32/49): loss=8.28091536459246e+55\n",
      "Gradient Descent(33/49): loss=6.241462522892677e+57\n",
      "Gradient Descent(34/49): loss=4.70429327067402e+59\n",
      "Gradient Descent(35/49): loss=3.545703446161665e+61\n",
      "Gradient Descent(36/49): loss=2.672455181843111e+63\n",
      "Gradient Descent(37/49): loss=2.0142735588030227e+65\n",
      "Gradient Descent(38/49): loss=1.518191211309608e+67\n",
      "Gradient Descent(39/49): loss=1.1442857619932264e+69\n",
      "Gradient Descent(40/49): loss=8.624670564196507e+70\n",
      "Gradient Descent(41/49): loss=6.5005564878610354e+72\n",
      "Gradient Descent(42/49): loss=4.899576666416966e+74\n",
      "Gradient Descent(43/49): loss=3.6928917631784295e+76\n",
      "Gradient Descent(44/49): loss=2.783393444585941e+78\n",
      "Gradient Descent(45/49): loss=2.0978895576121508e+80\n",
      "Gradient Descent(46/49): loss=1.5812139690487497e+82\n",
      "Gradient Descent(47/49): loss=1.19178705420543e+84\n",
      "Gradient Descent(48/49): loss=8.982695640022551e+85\n",
      "Gradient Descent(49/49): loss=6.770405893951664e+87\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6708497306798666\n",
      "Gradient Descent(2/49): loss=21.385176929739675\n",
      "Gradient Descent(3/49): loss=391.4317246875912\n",
      "Gradient Descent(4/49): loss=10117.848903036353\n",
      "Gradient Descent(5/49): loss=451211.81049858424\n",
      "Gradient Descent(6/49): loss=29455608.868499234\n",
      "Gradient Descent(7/49): loss=2190645741.49387\n",
      "Gradient Descent(8/49): loss=168172970777.4471\n",
      "Gradient Descent(9/49): loss=13000875063479.365\n",
      "Gradient Descent(10/49): loss=1006562433451876.1\n",
      "Gradient Descent(11/49): loss=7.795576066938498e+16\n",
      "Gradient Descent(12/49): loss=6.037893894282639e+18\n",
      "Gradient Descent(13/49): loss=4.6765878456642686e+20\n",
      "Gradient Descent(14/49): loss=3.6222137415610023e+22\n",
      "Gradient Descent(15/49): loss=2.8055585779695446e+24\n",
      "Gradient Descent(16/49): loss=2.1730247350739894e+26\n",
      "Gradient Descent(17/49): loss=1.6831003565624467e+28\n",
      "Gradient Descent(18/49): loss=1.3036330341502844e+30\n",
      "Gradient Descent(19/49): loss=1.0097194047043363e+32\n",
      "Gradient Descent(20/49): loss=7.820707590484324e+33\n",
      "Gradient Descent(21/49): loss=6.057471703006345e+35\n",
      "Gradient Descent(22/49): loss=4.691770278927552e+37\n",
      "Gradient Descent(23/49): loss=3.6339762576797346e+39\n",
      "Gradient Descent(24/49): loss=2.814669656931053e+41\n",
      "Gradient Descent(25/49): loss=2.180081738538316e+43\n",
      "Gradient Descent(26/49): loss=1.6885663207420697e+45\n",
      "Gradient Descent(27/49): loss=1.3078666589152933e+47\n",
      "Gradient Descent(28/49): loss=1.0129985280948404e+49\n",
      "Gradient Descent(29/49): loss=7.84610580082687e+50\n",
      "Gradient Descent(30/49): loss=6.077143700647724e+52\n",
      "Gradient Descent(31/49): loss=4.707007080433356e+54\n",
      "Gradient Descent(32/49): loss=3.6457778105343453e+56\n",
      "Gradient Descent(33/49): loss=2.8238104631362755e+58\n",
      "Gradient Descent(34/49): loss=2.18716168294124e+60\n",
      "Gradient Descent(35/49): loss=1.6940500397513787e+62\n",
      "Gradient Descent(36/49): loss=1.3121140332535735e+64\n",
      "Gradient Descent(37/49): loss=1.016288300736128e+66\n",
      "Gradient Descent(38/49): loss=7.871586493531081e+67\n",
      "Gradient Descent(39/49): loss=6.096879584293031e+69\n",
      "Gradient Descent(40/49): loss=4.722293364357808e+71\n",
      "Gradient Descent(41/49): loss=3.657617689630489e+73\n",
      "Gradient Descent(42/49): loss=2.832980954650447e+75\n",
      "Gradient Descent(43/49): loss=2.1942646198823873e+77\n",
      "Gradient Descent(44/49): loss=1.6995515674625064e+79\n",
      "Gradient Descent(45/49): loss=1.3163752011911422e+81\n",
      "Gradient Descent(46/49): loss=1.0195887571085648e+83\n",
      "Gradient Descent(47/49): loss=7.89714993629117e+84\n",
      "Gradient Descent(48/49): loss=6.116679561387442e+86\n",
      "Gradient Descent(49/49): loss=4.737629291392965e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6834690576864542\n",
      "Gradient Descent(2/49): loss=21.83772103115448\n",
      "Gradient Descent(3/49): loss=397.5822307240294\n",
      "Gradient Descent(4/49): loss=9756.815903005867\n",
      "Gradient Descent(5/49): loss=398550.974614522\n",
      "Gradient Descent(6/49): loss=24396502.793330215\n",
      "Gradient Descent(7/49): loss=1743446601.3572116\n",
      "Gradient Descent(8/49): loss=129737216204.78835\n",
      "Gradient Descent(9/49): loss=9744995806000.318\n",
      "Gradient Descent(10/49): loss=733514761816226.0\n",
      "Gradient Descent(11/49): loss=5.523808322675215e+16\n",
      "Gradient Descent(12/49): loss=4.1601917177843005e+18\n",
      "Gradient Descent(13/49): loss=3.133271916184666e+20\n",
      "Gradient Descent(14/49): loss=2.359853524452921e+22\n",
      "Gradient Descent(15/49): loss=1.7773481295538053e+24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(16/49): loss=1.3386284942939503e+26\n",
      "Gradient Descent(17/49): loss=1.0082022290927982e+28\n",
      "Gradient Descent(18/49): loss=7.59338197060301e+29\n",
      "Gradient Descent(19/49): loss=5.7190361464133265e+31\n",
      "Gradient Descent(20/49): loss=4.3073527172309456e+33\n",
      "Gradient Descent(21/49): loss=3.244128373764632e+35\n",
      "Gradient Descent(22/49): loss=2.4433496852212217e+37\n",
      "Gradient Descent(23/49): loss=1.840234724553467e+39\n",
      "Gradient Descent(24/49): loss=1.3859922965348352e+41\n",
      "Gradient Descent(25/49): loss=1.0438747951137806e+43\n",
      "Gradient Descent(26/49): loss=7.862053711252614e+44\n",
      "Gradient Descent(27/49): loss=5.921389121373178e+46\n",
      "Gradient Descent(28/49): loss=4.459757006815186e+48\n",
      "Gradient Descent(29/49): loss=3.358913280677e+50\n",
      "Gradient Descent(30/49): loss=2.52980115505565e+52\n",
      "Gradient Descent(31/49): loss=1.9053465657889804e+54\n",
      "Gradient Descent(32/49): loss=1.4350319702039765e+56\n",
      "Gradient Descent(33/49): loss=1.0808095453515343e+58\n",
      "Gradient Descent(34/49): loss=8.140231699207009e+59\n",
      "Gradient Descent(35/49): loss=6.130901822783587e+61\n",
      "Gradient Descent(36/49): loss=4.61755371954252e+63\n",
      "Gradient Descent(37/49): loss=3.4777595481344356e+65\n",
      "Gradient Descent(38/49): loss=2.6193114816298506e+67\n",
      "Gradient Descent(39/49): loss=1.9727622174104395e+69\n",
      "Gradient Descent(40/49): loss=1.4858067830941669e+71\n",
      "Gradient Descent(41/49): loss=1.1190511340928625e+73\n",
      "Gradient Descent(42/49): loss=8.428252279927334e+74\n",
      "Gradient Descent(43/49): loss=6.347827577305804e+76\n",
      "Gradient Descent(44/49): loss=4.780933651828509e+78\n",
      "Gradient Descent(45/49): loss=3.600810876606661e+80\n",
      "Gradient Descent(46/49): loss=2.7119888944975688e+82\n",
      "Gradient Descent(47/49): loss=2.042563193657406e+84\n",
      "Gradient Descent(48/49): loss=1.5383781285199472e+86\n",
      "Gradient Descent(49/49): loss=1.1586458003636564e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7103193406977808\n",
      "Gradient Descent(2/49): loss=22.16341959835022\n",
      "Gradient Descent(3/49): loss=397.9741096160911\n",
      "Gradient Descent(4/49): loss=9458.759905372899\n",
      "Gradient Descent(5/49): loss=371897.393398101\n",
      "Gradient Descent(6/49): loss=22343202.24605808\n",
      "Gradient Descent(7/49): loss=1588694022.476494\n",
      "Gradient Descent(8/49): loss=118110138409.28429\n",
      "Gradient Descent(9/49): loss=8871697416937.139\n",
      "Gradient Descent(10/49): loss=667921859992878.4\n",
      "Gradient Descent(11/49): loss=5.031137572070498e+16\n",
      "Gradient Descent(12/49): loss=3.7901446074986977e+18\n",
      "Gradient Descent(13/49): loss=2.855329426612617e+20\n",
      "Gradient Descent(14/49): loss=2.1510923882157232e+22\n",
      "Gradient Descent(15/49): loss=1.6205500073954806e+24\n",
      "Gradient Descent(16/49): loss=1.2208601786119371e+26\n",
      "Gradient Descent(17/49): loss=9.197492568419476e+27\n",
      "Gradient Descent(18/49): loss=6.92903840660122e+29\n",
      "Gradient Descent(19/49): loss=5.220071995028814e+31\n",
      "Gradient Descent(20/49): loss=3.9326021955653326e+33\n",
      "Gradient Descent(21/49): loss=2.962671787958475e+35\n",
      "Gradient Descent(22/49): loss=2.2319633888578293e+37\n",
      "Gradient Descent(23/49): loss=1.6814756833626743e+39\n",
      "Gradient Descent(24/49): loss=1.266759341958225e+41\n",
      "Gradient Descent(25/49): loss=9.543279431968339e+42\n",
      "Gradient Descent(26/49): loss=7.189541004359017e+44\n",
      "Gradient Descent(27/49): loss=5.416324673487941e+46\n",
      "Gradient Descent(28/49): loss=4.080451443402094e+48\n",
      "Gradient Descent(29/49): loss=3.0740557454876293e+50\n",
      "Gradient Descent(30/49): loss=2.3158757939995e+52\n",
      "Gradient Descent(31/49): loss=1.7446920736898387e+54\n",
      "Gradient Descent(32/49): loss=1.3143841478386285e+56\n",
      "Gradient Descent(33/49): loss=9.902066468587363e+57\n",
      "Gradient Descent(34/49): loss=7.459837408231051e+59\n",
      "Gradient Descent(35/49): loss=5.619955625806117e+61\n",
      "Gradient Descent(36/49): loss=4.233859199287594e+63\n",
      "Gradient Descent(37/49): loss=3.189627269845409e+65\n",
      "Gradient Descent(38/49): loss=2.4029429514928996e+67\n",
      "Gradient Descent(39/49): loss=1.8102851335383058e+69\n",
      "Gradient Descent(40/49): loss=1.3637994454565728e+71\n",
      "Gradient Descent(41/49): loss=1.0274342383800265e+73\n",
      "Gradient Descent(42/49): loss=7.740295816311651e+74\n",
      "Gradient Descent(43/49): loss=5.831242242664289e+76\n",
      "Gradient Descent(44/49): loss=4.393034439455737e+78\n",
      "Gradient Descent(45/49): loss=3.3095437958389846e+80\n",
      "Gradient Descent(46/49): loss=2.493283466708609e+82\n",
      "Gradient Descent(47/49): loss=1.8783442156524317e+84\n",
      "Gradient Descent(48/49): loss=1.415072549746012e+86\n",
      "Gradient Descent(49/49): loss=1.0660614302523459e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6582117048045617\n",
      "Gradient Descent(2/49): loss=21.005474035892405\n",
      "Gradient Descent(3/49): loss=364.5356942980109\n",
      "Gradient Descent(4/49): loss=7922.963883326527\n",
      "Gradient Descent(5/49): loss=275520.8281978134\n",
      "Gradient Descent(6/49): loss=15578304.909444042\n",
      "Gradient Descent(7/49): loss=1099803523.0257728\n",
      "Gradient Descent(8/49): loss=82566851655.2302\n",
      "Gradient Descent(9/49): loss=6287274727886.37\n",
      "Gradient Descent(10/49): loss=480262359989276.6\n",
      "Gradient Descent(11/49): loss=3.671057721829478e+16\n",
      "Gradient Descent(12/49): loss=2.8065217834021576e+18\n",
      "Gradient Descent(13/49): loss=2.1456538567572246e+20\n",
      "Gradient Descent(14/49): loss=1.6404158602431982e+22\n",
      "Gradient Descent(15/49): loss=1.2541483807568736e+24\n",
      "Gradient Descent(16/49): loss=9.588353309032826e+25\n",
      "Gradient Descent(17/49): loss=7.330593990412676e+27\n",
      "Gradient Descent(18/49): loss=5.6044669367057875e+29\n",
      "Gradient Descent(19/49): loss=4.284789171684715e+31\n",
      "Gradient Descent(20/49): loss=3.275854504415319e+33\n",
      "Gradient Descent(21/49): loss=2.504492591316046e+35\n",
      "Gradient Descent(22/49): loss=1.9147624327404368e+37\n",
      "Gradient Descent(23/49): loss=1.4638953960472992e+39\n",
      "Gradient Descent(24/49): loss=1.119193532278005e+41\n",
      "Gradient Descent(25/49): loss=8.55658243120069e+42\n",
      "Gradient Descent(26/49): loss=6.541773231383455e+44\n",
      "Gradient Descent(27/49): loss=5.001388972167164e+46\n",
      "Gradient Descent(28/49): loss=3.823717326506148e+48\n",
      "Gradient Descent(29/49): loss=2.9233507480399693e+50\n",
      "Gradient Descent(30/49): loss=2.234992512868193e+52\n",
      "Gradient Descent(31/49): loss=1.7087212459627416e+54\n",
      "Gradient Descent(32/49): loss=1.3063705044172626e+56\n",
      "Gradient Descent(33/49): loss=9.987608563091574e+57\n",
      "Gradient Descent(34/49): loss=7.635837189545089e+59\n",
      "Gradient Descent(35/49): loss=5.837834874777405e+61\n",
      "Gradient Descent(36/49): loss=4.463206218151092e+63\n",
      "Gradient Descent(37/49): loss=3.412259882822054e+65\n",
      "Gradient Descent(38/49): loss=2.6087787430848033e+67\n",
      "Gradient Descent(39/49): loss=1.994492437294258e+69\n",
      "Gradient Descent(40/49): loss=1.524851462765365e+71\n",
      "Gradient Descent(41/49): loss=1.1657963399710782e+73\n",
      "Gradient Descent(42/49): loss=8.912875381482926e+74\n",
      "Gradient Descent(43/49): loss=6.814170266464806e+76\n",
      "Gradient Descent(44/49): loss=5.209644972355406e+78\n",
      "Gradient Descent(45/49): loss=3.9829355118344006e+80\n",
      "Gradient Descent(46/49): loss=3.0450779996739635e+82\n",
      "Gradient Descent(47/49): loss=2.3280567803689533e+84\n",
      "Gradient Descent(48/49): loss=1.7798717711671803e+86\n",
      "Gradient Descent(49/49): loss=1.3607672924952011e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.691769925541776\n",
      "Gradient Descent(2/49): loss=22.059861327671655\n",
      "Gradient Descent(3/49): loss=410.12247866943557\n",
      "Gradient Descent(4/49): loss=10745.251399491963\n",
      "Gradient Descent(5/49): loss=485071.06995284715\n",
      "Gradient Descent(6/49): loss=32079338.380595624\n",
      "Gradient Descent(7/49): loss=2418826661.7823453\n",
      "Gradient Descent(8/49): loss=188314911445.69568\n",
      "Gradient Descent(9/49): loss=14764869884195.668\n",
      "Gradient Descent(10/49): loss=1159403063743917.0\n",
      "Gradient Descent(11/49): loss=9.107110585679368e+16\n",
      "Gradient Descent(12/49): loss=7.154133143423181e+18\n",
      "Gradient Descent(13/49): loss=5.620046326314025e+20\n",
      "Gradient Descent(14/49): loss=4.414933396452379e+22\n",
      "Gradient Descent(15/49): loss=3.4682365670635977e+24\n",
      "Gradient Descent(16/49): loss=2.724540906551707e+26\n",
      "Gradient Descent(17/49): loss=2.1403163363724617e+28\n",
      "Gradient Descent(18/49): loss=1.6813673229398533e+30\n",
      "Gradient Descent(19/49): loss=1.3208309587886281e+32\n",
      "Gradient Descent(20/49): loss=1.037604572435908e+34\n",
      "Gradient Descent(21/49): loss=8.151105496479017e+35\n",
      "Gradient Descent(22/49): loss=6.4032602188404045e+37\n",
      "Gradient Descent(23/49): loss=5.030206203074786e+39\n",
      "Gradient Descent(24/49): loss=3.951576787558692e+41\n",
      "Gradient Descent(25/49): loss=3.1042383706715744e+43\n",
      "Gradient Descent(26/49): loss=2.4385951178500384e+45\n",
      "Gradient Descent(27/49): loss=1.9156860520075426e+47\n",
      "Gradient Descent(28/49): loss=1.504904616183973e+49\n",
      "Gradient Descent(29/49): loss=1.1822072314189952e+51\n",
      "Gradient Descent(30/49): loss=9.287059943794428e+52\n",
      "Gradient Descent(31/49): loss=7.295631434778703e+54\n",
      "Gradient Descent(32/49): loss=5.731225851266045e+56\n",
      "Gradient Descent(33/49): loss=4.502276472141512e+58\n",
      "Gradient Descent(34/49): loss=3.536851270155521e+60\n",
      "Gradient Descent(35/49): loss=2.7784426355432445e+62\n",
      "Gradient Descent(36/49): loss=2.1826599111318646e+64\n",
      "Gradient Descent(37/49): loss=1.7146311486581404e+66\n",
      "Gradient Descent(38/49): loss=1.3469620076653005e+68\n",
      "Gradient Descent(39/49): loss=1.0581323286431562e+70\n",
      "Gradient Descent(40/49): loss=8.312365297225213e+71\n",
      "Gradient Descent(41/49): loss=6.529941006822346e+73\n",
      "Gradient Descent(42/49): loss=5.1297227717860955e+75\n",
      "Gradient Descent(43/49): loss=4.0297539729512975e+77\n",
      "Gradient Descent(44/49): loss=3.165651986464578e+79\n",
      "Gradient Descent(45/49): loss=2.486839783935425e+81\n",
      "Gradient Descent(46/49): loss=1.9535855922908136e+83\n",
      "Gradient Descent(47/49): loss=1.5346773407198277e+85\n",
      "Gradient Descent(48/49): loss=1.2055957770229994e+87\n",
      "Gradient Descent(49/49): loss=9.470793234582874e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7045804856184958\n",
      "Gradient Descent(2/49): loss=22.52632941932152\n",
      "Gradient Descent(3/49): loss=416.59022011960514\n",
      "Gradient Descent(4/49): loss=10365.122088495591\n",
      "Gradient Descent(5/49): loss=428629.07605229405\n",
      "Gradient Descent(6/49): loss=26576484.829181\n",
      "Gradient Descent(7/49): loss=1925405474.1501415\n",
      "Gradient Descent(8/49): loss=145301156729.55066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=11069281293457.748\n",
      "Gradient Descent(10/49): loss=845066686866880.0\n",
      "Gradient Descent(11/49): loss=6.4545785072506616e+16\n",
      "Gradient Descent(12/49): loss=4.930494688624973e+18\n",
      "Gradient Descent(13/49): loss=3.766372506562594e+20\n",
      "Gradient Descent(14/49): loss=2.877122203453795e+22\n",
      "Gradient Descent(15/49): loss=2.1978287464712875e+24\n",
      "Gradient Descent(16/49): loss=1.6789180654521087e+26\n",
      "Gradient Descent(17/49): loss=1.2825230521966288e+28\n",
      "Gradient Descent(18/49): loss=9.797174944057896e+29\n",
      "Gradient Descent(19/49): loss=7.484047713017751e+31\n",
      "Gradient Descent(20/49): loss=5.717053183790946e+33\n",
      "Gradient Descent(21/49): loss=4.367248628577368e+35\n",
      "Gradient Descent(22/49): loss=3.3361348882467054e+37\n",
      "Gradient Descent(23/49): loss=2.5484685987021273e+39\n",
      "Gradient Descent(24/49): loss=1.9467714634266815e+41\n",
      "Gradient Descent(25/49): loss=1.4871358951582924e+43\n",
      "Gradient Descent(26/49): loss=1.1360209517226406e+45\n",
      "Gradient Descent(27/49): loss=8.678047560781089e+46\n",
      "Gradient Descent(28/49): loss=6.629147935430281e+48\n",
      "Gradient Descent(29/49): loss=5.063996485617993e+50\n",
      "Gradient Descent(30/49): loss=3.868379564935236e+52\n",
      "Gradient Descent(31/49): loss=2.9550495346723962e+54\n",
      "Gradient Descent(32/49): loss=2.2573580502599765e+56\n",
      "Gradient Descent(33/49): loss=1.7243925380216404e+58\n",
      "Gradient Descent(34/49): loss=1.3172609568261998e+60\n",
      "Gradient Descent(35/49): loss=1.0062537329054975e+62\n",
      "Gradient Descent(36/49): loss=7.686757659817549e+63\n",
      "Gradient Descent(37/49): loss=5.871903018949017e+65\n",
      "Gradient Descent(38/49): loss=4.485538193064434e+67\n",
      "Gradient Descent(39/49): loss=3.4264961148900053e+69\n",
      "Gradient Descent(40/49): loss=2.617495408580011e+71\n",
      "Gradient Descent(41/49): loss=1.999500943300274e+73\n",
      "Gradient Descent(42/49): loss=1.5274158682966688e+75\n",
      "Gradient Descent(43/49): loss=1.1667907647364023e+77\n",
      "Gradient Descent(44/49): loss=8.913097715766854e+78\n",
      "Gradient Descent(45/49): loss=6.808702407646906e+80\n",
      "Gradient Descent(46/49): loss=5.201157886319418e+82\n",
      "Gradient Descent(47/49): loss=3.973156959840259e+84\n",
      "Gradient Descent(48/49): loss=3.0350888345551836e+86\n",
      "Gradient Descent(49/49): loss=2.3184999552627353e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7318907861432142\n",
      "Gradient Descent(2/49): loss=22.862999914844437\n",
      "Gradient Descent(3/49): loss=417.0322285932609\n",
      "Gradient Descent(4/49): loss=10050.3006364033\n",
      "Gradient Descent(5/49): loss=400029.9653738585\n",
      "Gradient Descent(6/49): loss=24341268.214413203\n",
      "Gradient Descent(7/49): loss=1754531189.8874633\n",
      "Gradient Descent(8/49): loss=132279628407.53714\n",
      "Gradient Descent(9/49): loss=10077300331557.113\n",
      "Gradient Descent(10/49): loss=769497207615054.0\n",
      "Gradient Descent(11/49): loss=5.878879753445316e+16\n",
      "Gradient Descent(12/49): loss=4.491919066100067e+18\n",
      "Gradient Descent(13/49): loss=3.432261028300653e+20\n",
      "Gradient Descent(14/49): loss=2.6225944779810727e+22\n",
      "Gradient Descent(15/49): loss=2.0039298663171008e+24\n",
      "Gradient Descent(16/49): loss=1.5312073803127012e+26\n",
      "Gradient Descent(17/49): loss=1.1699991225738933e+28\n",
      "Gradient Descent(18/49): loss=8.93999064415226e+29\n",
      "Gradient Descent(19/49): loss=6.831067768870154e+31\n",
      "Gradient Descent(20/49): loss=5.219634868920233e+33\n",
      "Gradient Descent(21/49): loss=3.9883352194270085e+35\n",
      "Gradient Descent(22/49): loss=3.047496275603231e+37\n",
      "Gradient Descent(23/49): loss=2.3285990366720763e+39\n",
      "Gradient Descent(24/49): loss=1.7792879738714716e+41\n",
      "Gradient Descent(25/49): loss=1.3595581051554829e+43\n",
      "Gradient Descent(26/49): loss=1.0388415301162472e+45\n",
      "Gradient Descent(27/49): loss=7.937812445102344e+46\n",
      "Gradient Descent(28/49): loss=6.065301067292827e+48\n",
      "Gradient Descent(29/49): loss=4.634510741004179e+50\n",
      "Gradient Descent(30/49): loss=3.541240504005202e+52\n",
      "Gradient Descent(31/49): loss=2.705870157178553e+54\n",
      "Gradient Descent(32/49): loss=2.067561720032466e+56\n",
      "Gradient Descent(33/49): loss=1.5798287492851867e+58\n",
      "Gradient Descent(34/49): loss=1.2071508448264773e+60\n",
      "Gradient Descent(35/49): loss=9.223867857985312e+61\n",
      "Gradient Descent(36/49): loss=7.047979018214888e+63\n",
      "Gradient Descent(37/49): loss=5.385377263204601e+65\n",
      "Gradient Descent(38/49): loss=4.114979370978133e+67\n",
      "Gradient Descent(39/49): loss=3.1442653682351446e+69\n",
      "Gradient Descent(40/49): loss=2.4025405268394245e+71\n",
      "Gradient Descent(41/49): loss=1.835786839564874e+73\n",
      "Gradient Descent(42/49): loss=1.4027290206642629e+75\n",
      "Gradient Descent(43/49): loss=1.0718285276955828e+77\n",
      "Gradient Descent(44/49): loss=8.189866865647533e+78\n",
      "Gradient Descent(45/49): loss=6.257896439950232e+80\n",
      "Gradient Descent(46/49): loss=4.7816733160099966e+82\n",
      "Gradient Descent(47/49): loss=3.6536877719926117e+84\n",
      "Gradient Descent(48/49): loss=2.791791377824038e+86\n",
      "Gradient Descent(49/49): loss=2.133214325821321e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6789235065603094\n",
      "Gradient Descent(2/49): loss=21.668769662127318\n",
      "Gradient Descent(3/49): loss=382.0618872950975\n",
      "Gradient Descent(4/49): loss=8423.010124157256\n",
      "Gradient Descent(5/49): loss=296531.4984633581\n",
      "Gradient Descent(6/49): loss=16974784.507703405\n",
      "Gradient Descent(7/49): loss=1214605558.1625764\n",
      "Gradient Descent(8/49): loss=92466025027.7907\n",
      "Gradient Descent(9/49): loss=7141015690511.799\n",
      "Gradient Descent(10/49): loss=553240493358265.0\n",
      "Gradient Descent(11/49): loss=4.2891244762742104e+16\n",
      "Gradient Descent(12/49): loss=3.3257446295429253e+18\n",
      "Gradient Descent(13/49): loss=2.578834260385898e+20\n",
      "Gradient Descent(14/49): loss=1.9996826396738705e+22\n",
      "Gradient Descent(15/49): loss=1.5505986420860828e+24\n",
      "Gradient Descent(16/49): loss=1.2023692754708984e+26\n",
      "Gradient Descent(17/49): loss=9.323444136258272e+27\n",
      "Gradient Descent(18/49): loss=7.229610215316361e+29\n",
      "Gradient Descent(19/49): loss=5.60600388497818e+31\n",
      "Gradient Descent(20/49): loss=4.347022681235428e+33\n",
      "Gradient Descent(21/49): loss=3.370780074727577e+35\n",
      "Gradient Descent(22/49): loss=2.613779394719954e+37\n",
      "Gradient Descent(23/49): loss=2.0267838817306727e+39\n",
      "Gradient Descent(24/49): loss=1.5716142347546584e+41\n",
      "Gradient Descent(25/49): loss=1.2186653570460389e+43\n",
      "Gradient Descent(26/49): loss=9.449807844837332e+44\n",
      "Gradient Descent(27/49): loss=7.32759553621868e+46\n",
      "Gradient Descent(28/49): loss=5.68198393280001e+48\n",
      "Gradient Descent(29/49): loss=4.4059393361738395e+50\n",
      "Gradient Descent(30/49): loss=3.4164653866730533e+52\n",
      "Gradient Descent(31/49): loss=2.649204822795305e+54\n",
      "Gradient Descent(32/49): loss=2.054253562907057e+56\n",
      "Gradient Descent(33/49): loss=1.5929148491673162e+58\n",
      "Gradient Descent(34/49): loss=1.2351823370368214e+60\n",
      "Gradient Descent(35/49): loss=9.577884257436983e+61\n",
      "Gradient Descent(36/49): loss=7.426908894190835e+63\n",
      "Gradient Descent(37/49): loss=5.7589937652233755e+65\n",
      "Gradient Descent(38/49): loss=4.4656545085431014e+67\n",
      "Gradient Descent(39/49): loss=3.462769887006039e+69\n",
      "Gradient Descent(40/49): loss=2.6851103835767656e+71\n",
      "Gradient Descent(41/49): loss=2.0820955498795293e+73\n",
      "Gradient Descent(42/49): loss=1.6145041579457791e+75\n",
      "Gradient Descent(43/49): loss=1.2519231771927944e+77\n",
      "Gradient Descent(44/49): loss=9.707696532579265e+78\n",
      "Gradient Descent(45/49): loss=7.52756827938646e+80\n",
      "Gradient Descent(46/49): loss=5.8370473377137825e+82\n",
      "Gradient Descent(47/49): loss=4.526179020655596e+84\n",
      "Gradient Descent(48/49): loss=3.509701967749792e+86\n",
      "Gradient Descent(49/49): loss=2.7215025844564937e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7128605324703343\n",
      "Gradient Descent(2/49): loss=22.750680788321866\n",
      "Gradient Descent(3/49): loss=429.55305939306857\n",
      "Gradient Descent(4/49): loss=11406.546197171223\n",
      "Gradient Descent(5/49): loss=521205.32194081234\n",
      "Gradient Descent(6/49): loss=34915885.47301202\n",
      "Gradient Descent(7/49): loss=2668919798.2470727\n",
      "Gradient Descent(8/49): loss=210701918514.00177\n",
      "Gradient Descent(9/49): loss=16753242064185.453\n",
      "Gradient Descent(10/49): loss=1334127333928485.2\n",
      "Gradient Descent(11/49): loss=1.0627693714279862e+17\n",
      "Gradient Descent(12/49): loss=8.466647793640905e+18\n",
      "Gradient Descent(13/49): loss=6.74513333605734e+20\n",
      "Gradient Descent(14/49): loss=5.373670021706931e+22\n",
      "Gradient Descent(15/49): loss=4.2810643094815995e+24\n",
      "Gradient Descent(16/49): loss=3.4106140271100836e+26\n",
      "Gradient Descent(17/49): loss=2.717148721063609e+28\n",
      "Gradient Descent(18/49): loss=2.164682712318712e+30\n",
      "Gradient Descent(19/49): loss=1.7245472121393314e+32\n",
      "Gradient Descent(20/49): loss=1.3739025451197168e+34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(21/49): loss=1.094552929792958e+36\n",
      "Gradient Descent(22/49): loss=8.720022540108253e+37\n",
      "Gradient Descent(23/49): loss=6.947018369830164e+39\n",
      "Gradient Descent(24/49): loss=5.534511408521068e+41\n",
      "Gradient Descent(25/49): loss=4.4092033301766783e+43\n",
      "Gradient Descent(26/49): loss=3.512699237896524e+45\n",
      "Gradient Descent(27/49): loss=2.798477414609193e+47\n",
      "Gradient Descent(28/49): loss=2.2294752011753395e+49\n",
      "Gradient Descent(29/49): loss=1.7761657273728262e+51\n",
      "Gradient Descent(30/49): loss=1.4150256927867125e+53\n",
      "Gradient Descent(31/49): loss=1.1273146871312127e+55\n",
      "Gradient Descent(32/49): loss=8.981027060498075e+56\n",
      "Gradient Descent(33/49): loss=7.154953978880422e+58\n",
      "Gradient Descent(34/49): loss=5.700168376628664e+60\n",
      "Gradient Descent(35/49): loss=4.541177989100293e+62\n",
      "Gradient Descent(36/49): loss=3.617840064732545e+64\n",
      "Gradient Descent(37/49): loss=2.882240415460428e+66\n",
      "Gradient Descent(38/49): loss=2.296207036207849e+68\n",
      "Gradient Descent(39/49): loss=1.8293292692893583e+70\n",
      "Gradient Descent(40/49): loss=1.4573797234787962e+72\n",
      "Gradient Descent(41/49): loss=1.1610570573947179e+74\n",
      "Gradient Descent(42/49): loss=9.249843872592661e+75\n",
      "Gradient Descent(43/49): loss=7.369113440412942e+77\n",
      "Gradient Descent(44/49): loss=5.870783728423567e+79\n",
      "Gradient Descent(45/49): loss=4.677102865170718e+81\n",
      "Gradient Descent(46/49): loss=3.7261279282829626e+83\n",
      "Gradient Descent(47/49): loss=2.968510579769611e+85\n",
      "Gradient Descent(48/49): loss=2.364936264081766e+87\n",
      "Gradient Descent(49/49): loss=1.8840840828679117e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7258637288474539\n",
      "Gradient Descent(2/49): loss=23.23138954553924\n",
      "Gradient Descent(3/49): loss=436.35131920536685\n",
      "Gradient Descent(4/49): loss=11006.45789536899\n",
      "Gradient Descent(5/49): loss=460739.91980315815\n",
      "Gradient Descent(6/49): loss=28933874.50010888\n",
      "Gradient Descent(7/49): loss=2124873024.2205782\n",
      "Gradient Descent(8/49): loss=162602730049.27707\n",
      "Gradient Descent(9/49): loss=12562274569028.578\n",
      "Gradient Descent(10/49): loss=972615052676816.2\n",
      "Gradient Descent(11/49): loss=7.53393455863941e+16\n",
      "Gradient Descent(12/49): loss=5.836454463838233e+18\n",
      "Gradient Descent(13/49): loss=4.521543355790231e+20\n",
      "Gradient Descent(14/49): loss=3.5028908227578185e+22\n",
      "Gradient Descent(15/49): loss=2.713732381871764e+24\n",
      "Gradient Descent(16/49): loss=2.1023622352905827e+26\n",
      "Gradient Descent(17/49): loss=1.6287262728255764e+28\n",
      "Gradient Descent(18/49): loss=1.2617945955991697e+30\n",
      "Gradient Descent(19/49): loss=9.775280430395594e+31\n",
      "Gradient Descent(20/49): loss=7.573031925179266e+33\n",
      "Gradient Descent(21/49): loss=5.8669224843668665e+35\n",
      "Gradient Descent(22/49): loss=4.545178176819119e+37\n",
      "Gradient Descent(23/49): loss=3.521206341864785e+39\n",
      "Gradient Descent(24/49): loss=2.7279225631338382e+41\n",
      "Gradient Descent(25/49): loss=2.1133557048282168e+43\n",
      "Gradient Descent(26/49): loss=1.6372430784837103e+45\n",
      "Gradient Descent(27/49): loss=1.268392676121089e+47\n",
      "Gradient Descent(28/49): loss=9.826396593031315e+48\n",
      "Gradient Descent(29/49): loss=7.612632256662465e+50\n",
      "Gradient Descent(30/49): loss=5.897601356358399e+52\n",
      "Gradient Descent(31/49): loss=4.568945482435565e+54\n",
      "Gradient Descent(32/49): loss=3.5396191705906352e+56\n",
      "Gradient Descent(33/49): loss=2.7421872116833294e+58\n",
      "Gradient Descent(34/49): loss=2.1244067063476253e+60\n",
      "Gradient Descent(35/49): loss=1.645804427482681e+62\n",
      "Gradient Descent(36/49): loss=1.2750252601953415e+64\n",
      "Gradient Descent(37/49): loss=9.877780050833518e+65\n",
      "Gradient Descent(38/49): loss=7.652439663642381e+67\n",
      "Gradient Descent(39/49): loss=5.92844065208193e+69\n",
      "Gradient Descent(40/49): loss=4.592837070279943e+71\n",
      "Gradient Descent(41/49): loss=3.5581282823048355e+73\n",
      "Gradient Descent(42/49): loss=2.7565264518659738e+75\n",
      "Gradient Descent(43/49): loss=2.1355154949373597e+77\n",
      "Gradient Descent(44/49): loss=1.6544105448473025e+79\n",
      "Gradient Descent(45/49): loss=1.281692526881976e+81\n",
      "Gradient Descent(46/49): loss=9.92943219916891e+82\n",
      "Gradient Descent(47/49): loss=7.692455228536542e+84\n",
      "Gradient Descent(48/49): loss=5.95944121034328e+86\n",
      "Gradient Descent(49/49): loss=4.6168535902282044e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7536377050704048\n",
      "Gradient Descent(2/49): loss=23.57930507824112\n",
      "Gradient Descent(3/49): loss=436.84660187722244\n",
      "Gradient Descent(4/49): loss=10674.056980788613\n",
      "Gradient Descent(5/49): loss=430068.4911504895\n",
      "Gradient Descent(6/49): loss=26502077.026959192\n",
      "Gradient Descent(7/49): loss=1936328928.6269648\n",
      "Gradient Descent(8/49): loss=148031119998.79886\n",
      "Gradient Descent(9/49): loss=11436490765255.787\n",
      "Gradient Descent(10/49): loss=885638183045728.6\n",
      "Gradient Descent(11/49): loss=6.861951485215163e+16\n",
      "Gradient Descent(12/49): loss=5.317280063688909e+18\n",
      "Gradient Descent(13/49): loss=4.1204310981567984e+20\n",
      "Gradient Descent(14/49): loss=3.192995228724389e+22\n",
      "Gradient Descent(15/49): loss=2.4743118560926976e+24\n",
      "Gradient Descent(16/49): loss=1.9173911809446824e+26\n",
      "Gradient Descent(17/49): loss=1.4858228814043585e+28\n",
      "Gradient Descent(18/49): loss=1.151392416738776e+30\n",
      "Gradient Descent(19/49): loss=8.922358903035442e+31\n",
      "Gradient Descent(20/49): loss=6.914105676907657e+33\n",
      "Gradient Descent(21/49): loss=5.3578720423885044e+35\n",
      "Gradient Descent(22/49): loss=4.1519169889834967e+37\n",
      "Gradient Descent(23/49): loss=3.2173994726186444e+39\n",
      "Gradient Descent(24/49): loss=2.4932240682775255e+41\n",
      "Gradient Descent(25/49): loss=1.9320467686847145e+43\n",
      "Gradient Descent(26/49): loss=1.4971798017993984e+45\n",
      "Gradient Descent(27/49): loss=1.1601931150156152e+47\n",
      "Gradient Descent(28/49): loss=8.990557196349115e+48\n",
      "Gradient Descent(29/49): loss=6.966953833348556e+50\n",
      "Gradient Descent(30/49): loss=5.398825084581155e+52\n",
      "Gradient Descent(31/49): loss=4.1836522806256236e+54\n",
      "Gradient Descent(32/49): loss=3.241991753941419e+56\n",
      "Gradient Descent(33/49): loss=2.5122810949890964e+58\n",
      "Gradient Descent(34/49): loss=1.9468144212786602e+60\n",
      "Gradient Descent(35/49): loss=1.508623536776256e+62\n",
      "Gradient Descent(36/49): loss=1.1690610829873064e+64\n",
      "Gradient Descent(37/49): loss=9.059276767456045e+65\n",
      "Gradient Descent(38/49): loss=7.020205936515676e+67\n",
      "Gradient Descent(39/49): loss=5.44009115254441e+69\n",
      "Gradient Descent(40/49): loss=4.215630141853172e+71\n",
      "Gradient Descent(41/49): loss=3.2667720070442606e+73\n",
      "Gradient Descent(42/49): loss=2.5314837846084953e+75\n",
      "Gradient Descent(43/49): loss=1.9616949508312533e+77\n",
      "Gradient Descent(44/49): loss=1.520154742256034e+79\n",
      "Gradient Descent(45/49): loss=1.1779968335159594e+81\n",
      "Gradient Descent(46/49): loss=9.128521598493388e+82\n",
      "Gradient Descent(47/49): loss=7.073865073596671e+84\n",
      "Gradient Descent(48/49): loss=5.481672638832457e+86\n",
      "Gradient Descent(49/49): loss=4.247852426742193e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.6998041014444014\n",
      "Gradient Descent(2/49): loss=22.347944856135705\n",
      "Gradient Descent(3/49): loss=400.28614055266496\n",
      "Gradient Descent(4/49): loss=8950.529103057583\n",
      "Gradient Descent(5/49): loss=318977.6285746476\n",
      "Gradient Descent(6/49): loss=18485317.510983035\n",
      "Gradient Descent(7/49): loss=1340457260.7740633\n",
      "Gradient Descent(8/49): loss=103469757297.33675\n",
      "Gradient Descent(9/49): loss=8103436309449.942\n",
      "Gradient Descent(10/49): loss=636675170314709.1\n",
      "Gradient Descent(11/49): loss=5.005776832992107e+16\n",
      "Gradient Descent(12/49): loss=3.9363311724349153e+18\n",
      "Gradient Descent(13/49): loss=3.095467900099231e+20\n",
      "Gradient Descent(14/49): loss=2.434244238678178e+22\n",
      "Gradient Descent(15/49): loss=1.9142677764437202e+24\n",
      "Gradient Descent(16/49): loss=1.5053634864174562e+26\n",
      "Gradient Descent(17/49): loss=1.1838048083181705e+28\n",
      "Gradient Descent(18/49): loss=9.30933864174631e+29\n",
      "Gradient Descent(19/49): loss=7.320783430752059e+31\n",
      "Gradient Descent(20/49): loss=5.757000807972656e+33\n",
      "Gradient Descent(21/49): loss=4.527255671773748e+35\n",
      "Gradient Descent(22/49): loss=3.560194726739109e+37\n",
      "Gradient Descent(23/49): loss=2.7997063588500215e+39\n",
      "Gradient Descent(24/49): loss=2.2016648799934337e+41\n",
      "Gradient Descent(25/49): loss=1.731370230479926e+43\n",
      "Gradient Descent(26/49): loss=1.361534583320102e+45\n",
      "Gradient Descent(27/49): loss=1.0706990272455266e+47\n",
      "Gradient Descent(28/49): loss=8.419884599251416e+48\n",
      "Gradient Descent(29/49): loss=6.621324467539042e+50\n",
      "Gradient Descent(30/49): loss=5.206952326677941e+52\n",
      "Gradient Descent(31/49): loss=4.09470230090894e+54\n",
      "Gradient Descent(32/49): loss=3.220038494910885e+56\n",
      "Gradient Descent(33/49): loss=2.532210438450345e+58\n",
      "Gradient Descent(34/49): loss=1.9913084004215154e+60\n",
      "Gradient Descent(35/49): loss=1.5659477132619507e+62\n",
      "Gradient Descent(36/49): loss=1.2314477456888124e+64\n",
      "Gradient Descent(37/49): loss=9.683998626002841e+65\n",
      "Gradient Descent(38/49): loss=7.615412811200295e+67\n",
      "Gradient Descent(39/49): loss=5.988694807253715e+69\n",
      "Gradient Descent(40/49): loss=4.709457830267614e+71\n",
      "Gradient Descent(41/49): loss=3.703476929264212e+73\n",
      "Gradient Descent(42/49): loss=2.9123822443937243e+75\n",
      "Gradient Descent(43/49): loss=2.2902722224180141e+77\n",
      "Gradient Descent(44/49): loss=1.8010502786427815e+79\n",
      "Gradient Descent(45/49): loss=1.4163303708825217e+81\n",
      "Gradient Descent(46/49): loss=1.1137899609309447e+83\n",
      "Gradient Descent(47/49): loss=8.758747976981791e+84\n",
      "Gradient Descent(48/49): loss=6.887803698658139e+86\n",
      "Gradient Descent(49/49): loss=5.416509290588794e+88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7341215514655408\n",
      "Gradient Descent(2/49): loss=23.45789080593395\n",
      "Gradient Descent(3/49): loss=449.7467480512775\n",
      "Gradient Descent(4/49): loss=12103.295558509702\n",
      "Gradient Descent(5/49): loss=559749.7604029998\n",
      "Gradient Descent(6/49): loss=37980841.303089336\n",
      "Gradient Descent(7/49): loss=2942853767.2238483\n",
      "Gradient Descent(8/49): loss=235565854316.5277\n",
      "Gradient Descent(9/49): loss=18992656275110.273\n",
      "Gradient Descent(10/49): loss=1533681714019046.0\n",
      "Gradient Descent(11/49): loss=1.2388823885858022e+17\n",
      "Gradient Descent(12/49): loss=1.0008203110328703e+19\n",
      "Gradient Descent(13/49): loss=8.085164038165704e+20\n",
      "Gradient Descent(14/49): loss=6.5316513882852374e+22\n",
      "Gradient Descent(15/49): loss=5.276640020843712e+24\n",
      "Gradient Descent(16/49): loss=4.262771004634135e+26\n",
      "Gradient Descent(17/49): loss=3.4437098532565636e+28\n",
      "Gradient Descent(18/49): loss=2.7820255002442857e+30\n",
      "Gradient Descent(19/49): loss=2.247479092450021e+32\n",
      "Gradient Descent(20/49): loss=1.8156419745929558e+34\n",
      "Gradient Descent(21/49): loss=1.4667792867156436e+36\n",
      "Gradient Descent(22/49): loss=1.1849480823180682e+38\n",
      "Gradient Descent(23/49): loss=9.572687387328343e+39\n",
      "Gradient Descent(24/49): loss=7.733363611704542e+41\n",
      "Gradient Descent(25/49): loss=6.247452813513211e+43\n",
      "Gradient Descent(26/49): loss=5.047049203531777e+45\n",
      "Gradient Descent(27/49): loss=4.0772946068153015e+47\n",
      "Gradient Descent(28/49): loss=3.2938714564407774e+49\n",
      "Gradient Descent(29/49): loss=2.6609774906673976e+51\n",
      "Gradient Descent(30/49): loss=2.149689597629198e+53\n",
      "Gradient Descent(31/49): loss=1.7366420356288795e+55\n",
      "Gradient Descent(32/49): loss=1.4029586239982306e+57\n",
      "Gradient Descent(33/49): loss=1.1333901058880252e+59\n",
      "Gradient Descent(34/49): loss=9.156172606602298e+60\n",
      "Gradient Descent(35/49): loss=7.396879182760137e+62\n",
      "Gradient Descent(36/49): loss=5.975621473638138e+64\n",
      "Gradient Descent(37/49): loss=4.827448321642211e+66\n",
      "Gradient Descent(38/49): loss=3.8998884720081905e+68\n",
      "Gradient Descent(39/49): loss=3.150552648262946e+70\n",
      "Gradient Descent(40/49): loss=2.54519637182476e+72\n",
      "Gradient Descent(41/49): loss=2.0561549970357385e+74\n",
      "Gradient Descent(42/49): loss=1.6610794430781127e+76\n",
      "Gradient Descent(43/49): loss=1.3419148460084026e+78\n",
      "Gradient Descent(44/49): loss=1.0840754555368151e+80\n",
      "Gradient Descent(45/49): loss=8.75778069519932e+81\n",
      "Gradient Descent(46/49): loss=7.07503544273344e+83\n",
      "Gradient Descent(47/49): loss=5.715617718467565e+85\n",
      "Gradient Descent(48/49): loss=4.617402438204638e+87\n",
      "Gradient Descent(49/49): loss=3.730201410680414e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.747318787373327\n",
      "Gradient Descent(2/49): loss=23.95316164644775\n",
      "Gradient Descent(3/49): loss=456.88924439320346\n",
      "Gradient Descent(4/49): loss=11682.353160169654\n",
      "Gradient Descent(5/49): loss=495005.0235726871\n",
      "Gradient Descent(6/49): loss=31481713.496819288\n",
      "Gradient Descent(7/49): loss=2343393451.460237\n",
      "Gradient Descent(8/49): loss=181821686976.1713\n",
      "Gradient Descent(9/49): loss=14244055501591.473\n",
      "Gradient Descent(10/49): loss=1118316942615331.0\n",
      "Gradient Descent(11/49): loss=8.78429886463268e+16\n",
      "Gradient Descent(12/49): loss=6.900751714942078e+18\n",
      "Gradient Descent(13/49): loss=5.421209782972656e+20\n",
      "Gradient Descent(14/49): loss=4.258909100557988e+22\n",
      "Gradient Descent(15/49): loss=3.34580828282144e+24\n",
      "Gradient Descent(16/49): loss=2.62847499513748e+26\n",
      "Gradient Descent(17/49): loss=2.0649363713968136e+28\n",
      "Gradient Descent(18/49): loss=1.622219074918204e+30\n",
      "Gradient Descent(19/49): loss=1.2744192854322368e+32\n",
      "Gradient Descent(20/49): loss=1.0011869181402504e+34\n",
      "Gradient Descent(21/49): loss=7.865348999834671e+35\n",
      "Gradient Descent(22/49): loss=6.179037477454528e+37\n",
      "Gradient Descent(23/49): loss=4.8542670069531985e+39\n",
      "Gradient Descent(24/49): loss=3.8135240740667766e+41\n",
      "Gradient Descent(25/49): loss=2.99591387178763e+43\n",
      "Gradient Descent(26/49): loss=2.3535972902877177e+45\n",
      "Gradient Descent(27/49): loss=1.8489918074795983e+47\n",
      "Gradient Descent(28/49): loss=1.4525725017760702e+49\n",
      "Gradient Descent(29/49): loss=1.1411445223179012e+51\n",
      "Gradient Descent(30/49): loss=8.964859373449049e+52\n",
      "Gradient Descent(31/49): loss=7.042815525457918e+54\n",
      "Gradient Descent(32/49): loss=5.5328531613706655e+56\n",
      "Gradient Descent(33/49): loss=4.3466230223740597e+58\n",
      "Gradient Descent(34/49): loss=3.4147177139166495e+60\n",
      "Gradient Descent(35/49): loss=2.68261061649821e+62\n",
      "Gradient Descent(36/49): loss=2.1074654840193462e+64\n",
      "Gradient Descent(37/49): loss=1.6556300564151893e+66\n",
      "Gradient Descent(38/49): loss=1.3006670355889043e+68\n",
      "Gradient Descent(39/49): loss=1.021807215272857e+70\n",
      "Gradient Descent(40/49): loss=8.027342560511106e+71\n",
      "Gradient Descent(41/49): loss=6.306300016347568e+73\n",
      "Gradient Descent(42/49): loss=4.954244769847405e+75\n",
      "Gradient Descent(43/49): loss=3.892066849965073e+77\n",
      "Gradient Descent(44/49): loss=3.057617269294432e+79\n",
      "Gradient Descent(45/49): loss=2.4020716308023186e+81\n",
      "Gradient Descent(46/49): loss=1.8870733683542643e+83\n",
      "Gradient Descent(47/49): loss=1.4824894694594834e+85\n",
      "Gradient Descent(48/49): loss=1.1646473655526095e+87\n",
      "Gradient Descent(49/49): loss=9.149498286710853e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7755600974793524\n",
      "Gradient Descent(2/49): loss=24.31259980976385\n",
      "Gradient Descent(3/49): loss=457.441078945927\n",
      "Gradient Descent(4/49): loss=11331.525403452051\n",
      "Gradient Descent(5/49): loss=462127.2028809832\n",
      "Gradient Descent(6/49): loss=28837603.5861938\n",
      "Gradient Descent(7/49): loss=2135495215.3014543\n",
      "Gradient Descent(8/49): loss=165528270889.84528\n",
      "Gradient Descent(9/49): loss=12967548893537.633\n",
      "Gradient Descent(10/49): loss=1018308904103070.0\n",
      "Gradient Descent(11/49): loss=8.000774419532912e+16\n",
      "Gradient Descent(12/49): loss=6.28688992354205e+18\n",
      "Gradient Descent(13/49): loss=4.940274763099466e+20\n",
      "Gradient Descent(14/49): loss=3.882119406366281e+22\n",
      "Gradient Descent(15/49): loss=3.0506138606666413e+24\n",
      "Gradient Descent(16/49): loss=2.3972079787108683e+26\n",
      "Gradient Descent(17/49): loss=1.883754131047809e+28\n",
      "Gradient Descent(18/49): loss=1.4802760997044504e+30\n",
      "Gradient Descent(19/49): loss=1.1632183320511655e+32\n",
      "Gradient Descent(20/49): loss=9.140706178022903e+33\n",
      "Gradient Descent(21/49): loss=7.182874198580084e+35\n",
      "Gradient Descent(22/49): loss=5.644386850380807e+37\n",
      "Gradient Descent(23/49): loss=4.4354254350294443e+39\n",
      "Gradient Descent(24/49): loss=3.485409365308256e+41\n",
      "Gradient Descent(25/49): loss=2.738875587410816e+43\n",
      "Gradient Descent(26/49): loss=2.152240582693231e+45\n",
      "Gradient Descent(27/49): loss=1.6912559106676564e+47\n",
      "Gradient Descent(28/49): loss=1.3290087448258046e+49\n",
      "Gradient Descent(29/49): loss=1.0443506702224617e+51\n",
      "Gradient Descent(30/49): loss=8.206630141752901e+52\n",
      "Gradient Descent(31/49): loss=6.448866286377046e+54\n",
      "Gradient Descent(32/49): loss=5.067594817997644e+56\n",
      "Gradient Descent(33/49): loss=3.9821754862006894e+58\n",
      "Gradient Descent(34/49): loss=3.129240235738397e+60\n",
      "Gradient Descent(35/49): loss=2.4589937050480587e+62\n",
      "Gradient Descent(36/49): loss=1.932306114566805e+64\n",
      "Gradient Descent(37/49): loss=1.5184288242492378e+66\n",
      "Gradient Descent(38/49): loss=1.1931991918515498e+68\n",
      "Gradient Descent(39/49): loss=9.37629929502363e+69\n",
      "Gradient Descent(40/49): loss=7.368006035391068e+71\n",
      "Gradient Descent(41/49): loss=5.789865620690227e+73\n",
      "Gradient Descent(42/49): loss=4.5497443602285785e+75\n",
      "Gradient Descent(43/49): loss=3.575242518489493e+77\n",
      "Gradient Descent(44/49): loss=2.8094675335500708e+79\n",
      "Gradient Descent(45/49): loss=2.207712562505233e+81\n",
      "Gradient Descent(46/49): loss=1.734846443476943e+83\n",
      "Gradient Descent(47/49): loss=1.3632626971281824e+85\n",
      "Gradient Descent(48/49): loss=1.0712678279793192e+87\n",
      "Gradient Descent(49/49): loss=8.418148326665787e+88\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7208534894568372\n",
      "Gradient Descent(2/49): loss=23.043251297019975\n",
      "Gradient Descent(3/49): loss=419.230543747985\n",
      "Gradient Descent(4/49): loss=9506.807693879406\n",
      "Gradient Descent(5/49): loss=342946.0037751613\n",
      "Gradient Descent(6/49): loss=20118321.71781248\n",
      "Gradient Descent(7/49): loss=1478333559.9904218\n",
      "Gradient Descent(8/49): loss=115692248866.43169\n",
      "Gradient Descent(9/49): loss=9187462096390.148\n",
      "Gradient Descent(10/49): loss=731975303173133.1\n",
      "Gradient Descent(11/49): loss=5.835879749035303e+16\n",
      "Gradient Descent(12/49): loss=4.653544082506613e+18\n",
      "Gradient Descent(13/49): loss=3.7108731216621666e+20\n",
      "Gradient Descent(14/49): loss=2.9591815365476544e+22\n",
      "Gradient Descent(15/49): loss=2.359759903419788e+24\n",
      "Gradient Descent(16/49): loss=1.8817597741920465e+26\n",
      "Gradient Descent(17/49): loss=1.500584918654658e+28\n",
      "Gradient Descent(18/49): loss=1.1966219956400897e+30\n",
      "Gradient Descent(19/49): loss=9.542307056137191e+31\n",
      "Gradient Descent(20/49): loss=7.609389126991726e+33\n",
      "Gradient Descent(21/49): loss=6.0680087693259574e+35\n",
      "Gradient Descent(22/49): loss=4.838854973973352e+37\n",
      "Gradient Descent(23/49): loss=3.8586822052245864e+39\n",
      "Gradient Descent(24/49): loss=3.0770561302270837e+41\n",
      "Gradient Descent(25/49): loss=2.4537585437203807e+43\n",
      "Gradient Descent(26/49): loss=1.956717959004907e+45\n",
      "Gradient Descent(27/49): loss=1.5603593845412369e+47\n",
      "Gradient Descent(28/49): loss=1.2442883746843857e+49\n",
      "Gradient Descent(29/49): loss=9.922416429916937e+50\n",
      "Gradient Descent(30/49): loss=7.912502424018616e+52\n",
      "Gradient Descent(31/49): loss=6.309722541107401e+54\n",
      "Gradient Descent(32/49): loss=5.031606489611471e+56\n",
      "Gradient Descent(33/49): loss=4.0123894040286375e+58\n",
      "Gradient Descent(34/49): loss=3.199627944435033e+60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=2.551501848880979e+62\n",
      "Gradient Descent(36/49): loss=2.0346620913115722e+64\n",
      "Gradient Descent(37/49): loss=1.6225149229799718e+66\n",
      "Gradient Descent(38/49): loss=1.2938535035051974e+68\n",
      "Gradient Descent(39/49): loss=1.0317667127880906e+70\n",
      "Gradient Descent(40/49): loss=8.227689972114756e+71\n",
      "Gradient Descent(41/49): loss=6.561064767665304e+73\n",
      "Gradient Descent(42/49): loss=5.232036091709275e+75\n",
      "Gradient Descent(43/49): loss=4.172219393391179e+77\n",
      "Gradient Descent(44/49): loss=3.3270822986434275e+79\n",
      "Gradient Descent(45/49): loss=2.653138672304848e+81\n",
      "Gradient Descent(46/49): loss=2.1157110593115295e+83\n",
      "Gradient Descent(47/49): loss=1.6871463724150325e+85\n",
      "Gradient Descent(48/49): loss=1.3453930154713046e+87\n",
      "Gradient Descent(49/49): loss=1.0728662288429604e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7555529825273954\n",
      "Gradient Descent(2/49): loss=24.181748885392683\n",
      "Gradient Descent(3/49): loss=470.7273718568642\n",
      "Gradient Descent(4/49): loss=12837.121482033957\n",
      "Gradient Descent(5/49): loss=600846.5749723853\n",
      "Gradient Descent(6/49): loss=41290818.47848045\n",
      "Gradient Descent(7/49): loss=3242711696.96757\n",
      "Gradient Descent(8/49): loss=263160656537.19687\n",
      "Gradient Descent(9/49): loss=21512726310180.527\n",
      "Gradient Descent(10/49): loss=1761384609146172.0\n",
      "Gradient Descent(11/49): loss=1.4426475030405155e+17\n",
      "Gradient Descent(12/49): loss=1.1816746704401652e+19\n",
      "Gradient Descent(13/49): loss=9.679266135107433e+20\n",
      "Gradient Descent(14/49): loss=7.928451956223464e+22\n",
      "Gradient Descent(15/49): loss=6.494334907128679e+24\n",
      "Gradient Descent(16/49): loss=5.3196251528682404e+26\n",
      "Gradient Descent(17/49): loss=4.357399659589614e+28\n",
      "Gradient Descent(18/49): loss=3.569223654726351e+30\n",
      "Gradient Descent(19/49): loss=2.9236146583679e+32\n",
      "Gradient Descent(20/49): loss=2.394784832870818e+34\n",
      "Gradient Descent(21/49): loss=1.9616109050975793e+36\n",
      "Gradient Descent(22/49): loss=1.6067904265302995e+38\n",
      "Gradient Descent(23/49): loss=1.3161506535719051e+40\n",
      "Gradient Descent(24/49): loss=1.0780824395622233e+42\n",
      "Gradient Descent(25/49): loss=8.83076525729282e+43\n",
      "Gradient Descent(26/49): loss=7.233437088640103e+45\n",
      "Gradient Descent(27/49): loss=5.9250371390074045e+47\n",
      "Gradient Descent(28/49): loss=4.853303439073284e+49\n",
      "Gradient Descent(29/49): loss=3.975427279037519e+51\n",
      "Gradient Descent(30/49): loss=3.256343282325863e+53\n",
      "Gradient Descent(31/49): loss=2.6673287745100157e+55\n",
      "Gradient Descent(32/49): loss=2.1848565014458587e+57\n",
      "Gradient Descent(33/49): loss=1.7896548702688773e+59\n",
      "Gradient Descent(34/49): loss=1.4659381760575767e+61\n",
      "Gradient Descent(35/49): loss=1.2007760667844314e+63\n",
      "Gradient Descent(36/49): loss=9.83577060828025e+64\n",
      "Gradient Descent(37/49): loss=8.056654869693855e+66\n",
      "Gradient Descent(38/49): loss=6.599349484087969e+68\n",
      "Gradient Descent(39/49): loss=5.4056446897031913e+70\n",
      "Gradient Descent(40/49): loss=4.427859834029519e+72\n",
      "Gradient Descent(41/49): loss=3.6269388454549614e+74\n",
      "Gradient Descent(42/49): loss=2.97089020017576e+76\n",
      "Gradient Descent(43/49): loss=2.4335090713097737e+78\n",
      "Gradient Descent(44/49): loss=1.9933306184781263e+80\n",
      "Gradient Descent(45/49): loss=1.632772608660882e+82\n",
      "Gradient Descent(46/49): loss=1.337433121670835e+84\n",
      "Gradient Descent(47/49): loss=1.0955152882000184e+86\n",
      "Gradient Descent(48/49): loss=8.97356082508743e+87\n",
      "Gradient Descent(49/49): loss=7.350403481255808e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7689456611961156\n",
      "Gradient Descent(2/49): loss=24.691908004442073\n",
      "Gradient Descent(3/49): loss=478.2282686053125\n",
      "Gradient Descent(4/49): loss=12394.396478201885\n",
      "Gradient Descent(5/49): loss=531552.2311222813\n",
      "Gradient Descent(6/49): loss=34233900.994404204\n",
      "Gradient Descent(7/49): loss=2582634929.5110655\n",
      "Gradient Descent(8/49): loss=203154920342.671\n",
      "Gradient Descent(9/49): loss=16136928205952.324\n",
      "Gradient Descent(10/49): loss=1284602142382943.8\n",
      "Gradient Descent(11/49): loss=1.0231284558395664e+17\n",
      "Gradient Descent(12/49): loss=8.149659890384689e+18\n",
      "Gradient Descent(13/49): loss=6.49171568572131e+20\n",
      "Gradient Descent(14/49): loss=5.171087490332771e+22\n",
      "Gradient Descent(15/49): loss=4.119123510360271e+24\n",
      "Gradient Descent(16/49): loss=3.281163427879122e+26\n",
      "Gradient Descent(17/49): loss=2.613671106142792e+28\n",
      "Gradient Descent(18/49): loss=2.081967843119517e+30\n",
      "Gradient Descent(19/49): loss=1.6584298242027773e+32\n",
      "Gradient Descent(20/49): loss=1.321052816824786e+34\n",
      "Gradient Descent(21/49): loss=1.052308948886117e+36\n",
      "Gradient Descent(22/49): loss=8.382360718970806e+37\n",
      "Gradient Descent(23/49): loss=6.677123795046875e+39\n",
      "Gradient Descent(24/49): loss=5.3187859207197736e+41\n",
      "Gradient Descent(25/49): loss=4.236776872617446e+43\n",
      "Gradient Descent(26/49): loss=3.374882639743088e+45\n",
      "Gradient Descent(27/49): loss=2.688324916436607e+47\n",
      "Gradient Descent(28/49): loss=2.1414347187148445e+49\n",
      "Gradient Descent(29/49): loss=1.7057992605282763e+51\n",
      "Gradient Descent(30/49): loss=1.3587858139168684e+53\n",
      "Gradient Descent(31/49): loss=1.0823658626337732e+55\n",
      "Gradient Descent(32/49): loss=8.621784600605372e+56\n",
      "Gradient Descent(33/49): loss=6.867841297059418e+58\n",
      "Gradient Descent(34/49): loss=5.470705459085987e+60\n",
      "Gradient Descent(35/49): loss=4.357791178559696e+62\n",
      "Gradient Descent(36/49): loss=3.471278813666079e+64\n",
      "Gradient Descent(37/49): loss=2.7651110639471803e+66\n",
      "Gradient Descent(38/49): loss=2.202600138560562e+68\n",
      "Gradient Descent(39/49): loss=1.7545217020908684e+70\n",
      "Gradient Descent(40/49): loss=1.3975965719858962e+72\n",
      "Gradient Descent(41/49): loss=1.1132812866885531e+74\n",
      "Gradient Descent(42/49): loss=8.868047104106777e+75\n",
      "Gradient Descent(43/49): loss=7.064006229241433e+77\n",
      "Gradient Descent(44/49): loss=5.626964248267557e+79\n",
      "Gradient Descent(45/49): loss=4.482261994647328e+81\n",
      "Gradient Descent(46/49): loss=3.5704283343981194e+83\n",
      "Gradient Descent(47/49): loss=2.8440904405625024e+85\n",
      "Gradient Descent(48/49): loss=2.2655126154387412e+87\n",
      "Gradient Descent(49/49): loss=1.8046357941054437e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7976579633700576\n",
      "Gradient Descent(2/49): loss=25.06315091280575\n",
      "Gradient Descent(3/49): loss=478.8400696551303\n",
      "Gradient Descent(4/49): loss=12024.260007048313\n",
      "Gradient Descent(5/49): loss=496326.2954328766\n",
      "Gradient Descent(6/49): loss=31360610.948018115\n",
      "Gradient Descent(7/49): loss=2353551091.9485035\n",
      "Gradient Descent(8/49): loss=184950346777.32205\n",
      "Gradient Descent(9/49): loss=14690779782733.049\n",
      "Gradient Descent(10/49): loss=1169721792987904.5\n",
      "Gradient Descent(11/49): loss=9.318676205237141e+16\n",
      "Gradient Descent(12/49): loss=7.424683977973005e+18\n",
      "Gradient Descent(13/49): loss=5.91579791185916e+20\n",
      "Gradient Descent(14/49): loss=4.713584211331179e+22\n",
      "Gradient Descent(15/49): loss=3.7556904217978295e+24\n",
      "Gradient Descent(16/49): loss=2.9924605296960365e+26\n",
      "Gradient Descent(17/49): loss=2.384334064941127e+28\n",
      "Gradient Descent(18/49): loss=1.899790810934347e+30\n",
      "Gradient Descent(19/49): loss=1.5137162154145148e+32\n",
      "Gradient Descent(20/49): loss=1.206099518580054e+34\n",
      "Gradient Descent(21/49): loss=9.60996542245296e+35\n",
      "Gradient Descent(22/49): loss=7.65703277394461e+37\n",
      "Gradient Descent(23/49): loss=6.10097417881927e+39\n",
      "Gradient Descent(24/49): loss=4.861137078757958e+41\n",
      "Gradient Descent(25/49): loss=3.873259090412531e+43\n",
      "Gradient Descent(26/49): loss=3.0861372017302166e+45\n",
      "Gradient Descent(27/49): loss=2.4589738526603544e+47\n",
      "Gradient Descent(28/49): loss=1.9592623440971337e+49\n",
      "Gradient Descent(29/49): loss=1.561101973021753e+51\n",
      "Gradient Descent(30/49): loss=1.2438555650878725e+53\n",
      "Gradient Descent(31/49): loss=9.910798228031983e+54\n",
      "Gradient Descent(32/49): loss=7.896730478495734e+56\n",
      "Gradient Descent(33/49): loss=6.291960628723981e+58\n",
      "Gradient Descent(34/49): loss=5.013311352238819e+60\n",
      "Gradient Descent(35/49): loss=3.994508579686313e+62\n",
      "Gradient Descent(36/49): loss=3.182746426882513e+64\n",
      "Gradient Descent(37/49): loss=2.535950196564356e+66\n",
      "Gradient Descent(38/49): loss=2.0205955916362186e+68\n",
      "Gradient Descent(39/49): loss=1.6099711068738565e+70\n",
      "Gradient Descent(40/49): loss=1.2827935365679585e+72\n",
      "Gradient Descent(41/49): loss=1.0221048380524883e+74\n",
      "Gradient Descent(42/49): loss=8.143931741076002e+75\n",
      "Gradient Descent(43/49): loss=6.488925767114017e+77\n",
      "Gradient Descent(44/49): loss=5.1702493279434316e+79\n",
      "Gradient Descent(45/49): loss=4.119553693860391e+81\n",
      "Gradient Descent(46/49): loss=3.2823799318299475e+83\n",
      "Gradient Descent(47/49): loss=2.615336227547372e+85\n",
      "Gradient Descent(48/49): loss=2.0838488307806263e+87\n",
      "Gradient Descent(49/49): loss=1.6603700525412838e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7420716705976165\n",
      "Gradient Descent(2/49): loss=23.75494264610095\n",
      "Gradient Descent(3/49): loss=438.91770722194707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=10093.18273098452\n",
      "Gradient Descent(5/49): loss=368527.9800901864\n",
      "Gradient Descent(6/49): loss=21882770.834345546\n",
      "Gradient Descent(7/49): loss=1629287697.5878108\n",
      "Gradient Descent(8/49): loss=129258585434.68158\n",
      "Gradient Descent(9/49): loss=10407449263711.35\n",
      "Gradient Descent(10/49): loss=840727780184724.8\n",
      "Gradient Descent(11/49): loss=6.79641177983134e+16\n",
      "Gradient Descent(12/49): loss=5.495061769389907e+18\n",
      "Gradient Descent(13/49): loss=4.4430427816426884e+20\n",
      "Gradient Descent(14/49): loss=3.5924580048626675e+22\n",
      "Gradient Descent(15/49): loss=2.9047156520062703e+24\n",
      "Gradient Descent(16/49): loss=2.3486359671960758e+26\n",
      "Gradient Descent(17/49): loss=1.8990125043075588e+28\n",
      "Gradient Descent(18/49): loss=1.5354650974447792e+30\n",
      "Gradient Descent(19/49): loss=1.2415152976081624e+32\n",
      "Gradient Descent(20/49): loss=1.0038393175102241e+34\n",
      "Gradient Descent(21/49): loss=8.116640829997116e+35\n",
      "Gradient Descent(22/49): loss=6.562789205069714e+37\n",
      "Gradient Descent(23/49): loss=5.306407299850044e+39\n",
      "Gradient Descent(24/49): loss=4.290547441353696e+41\n",
      "Gradient Descent(25/49): loss=3.469164032514929e+43\n",
      "Gradient Descent(26/49): loss=2.805026456180905e+45\n",
      "Gradient Descent(27/49): loss=2.268031533283004e+47\n",
      "Gradient Descent(28/49): loss=1.8338390444165402e+49\n",
      "Gradient Descent(29/49): loss=1.4827684674906637e+51\n",
      "Gradient Descent(30/49): loss=1.1989069241811293e+53\n",
      "Gradient Descent(31/49): loss=9.693879013234946e+54\n",
      "Gradient Descent(32/49): loss=7.838080540524104e+56\n",
      "Gradient Descent(33/49): loss=6.337556562843965e+58\n",
      "Gradient Descent(34/49): loss=5.12429324750987e+60\n",
      "Gradient Descent(35/49): loss=4.143297345924137e+62\n",
      "Gradient Descent(36/49): loss=3.350103529903235e+64\n",
      "Gradient Descent(37/49): loss=2.7087589241237382e+66\n",
      "Gradient Descent(38/49): loss=2.1901934801495194e+68\n",
      "Gradient Descent(39/49): loss=1.770902326437584e+70\n",
      "Gradient Descent(40/49): loss=1.4318803695681083e+72\n",
      "Gradient Descent(41/49): loss=1.157760855664411e+74\n",
      "Gradient Descent(42/49): loss=9.361188458174902e+75\n",
      "Gradient Descent(43/49): loss=7.569080343381976e+77\n",
      "Gradient Descent(44/49): loss=6.120053826556683e+79\n",
      "Gradient Descent(45/49): loss=4.948429286088796e+81\n",
      "Gradient Descent(46/49): loss=4.0011008225393824e+83\n",
      "Gradient Descent(47/49): loss=3.2351291423179614e+85\n",
      "Gradient Descent(48/49): loss=2.615795260273511e+87\n",
      "Gradient Descent(49/49): loss=2.1150268019183068e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7771548256558987\n",
      "Gradient Descent(2/49): loss=24.922514542224643\n",
      "Gradient Descent(3/49): loss=492.5193125262008\n",
      "Gradient Descent(4/49): loss=13609.707521152757\n",
      "Gradient Descent(5/49): loss=644645.2597920401\n",
      "Gradient Descent(6/49): loss=44863510.16985169\n",
      "Gradient Descent(7/49): loss=3570742403.0121393\n",
      "Gradient Descent(8/49): loss=293764264441.34296\n",
      "Gradient Descent(9/49): loss=24346317313820.49\n",
      "Gradient Descent(10/49): loss=2020970226041727.0\n",
      "Gradient Descent(11/49): loss=1.67816946045553e+17\n",
      "Gradient Descent(12/49): loss=1.3936183424879622e+19\n",
      "Gradient Descent(13/49): loss=1.1573342716952311e+21\n",
      "Gradient Descent(14/49): loss=9.611147973333507e+22\n",
      "Gradient Descent(15/49): loss=7.981638134686828e+24\n",
      "Gradient Descent(16/49): loss=6.628402519308956e+26\n",
      "Gradient Descent(17/49): loss=5.504599522060546e+28\n",
      "Gradient Descent(18/49): loss=4.5713301256546523e+30\n",
      "Gradient Descent(19/49): loss=3.796290550676654e+32\n",
      "Gradient Descent(20/49): loss=3.1526539440230263e+34\n",
      "Gradient Descent(21/49): loss=2.618141777826462e+36\n",
      "Gradient Descent(22/49): loss=2.174252706011196e+38\n",
      "Gradient Descent(23/49): loss=1.805622166702415e+40\n",
      "Gradient Descent(24/49): loss=1.4994905605386577e+42\n",
      "Gradient Descent(25/49): loss=1.2452615960354976e+44\n",
      "Gradient Descent(26/49): loss=1.0341355146669949e+46\n",
      "Gradient Descent(27/49): loss=8.58804500275518e+47\n",
      "Gradient Descent(28/49): loss=7.131997298545282e+49\n",
      "Gradient Descent(29/49): loss=5.9228131024162524e+51\n",
      "Gradient Descent(30/49): loss=4.918638296919727e+53\n",
      "Gradient Descent(31/49): loss=4.0847148605880595e+55\n",
      "Gradient Descent(32/49): loss=3.392177770574829e+57\n",
      "Gradient Descent(33/49): loss=2.8170558827024806e+59\n",
      "Gradient Descent(34/49): loss=2.3394422058617662e+61\n",
      "Gradient Descent(35/49): loss=1.9428048510407924e+63\n",
      "Gradient Descent(36/49): loss=1.6134148045077618e+65\n",
      "Gradient Descent(37/49): loss=1.3398707183638432e+67\n",
      "Gradient Descent(38/49): loss=1.1127042697965773e+69\n",
      "Gradient Descent(39/49): loss=9.240524291294887e+70\n",
      "Gradient Descent(40/49): loss=7.673852927123055e+72\n",
      "Gradient Descent(41/49): loss=6.372800599917486e+74\n",
      "Gradient Descent(42/49): loss=5.2923333131345933e+76\n",
      "Gradient Descent(43/49): loss=4.395052294226213e+78\n",
      "Gradient Descent(44/49): loss=3.649899491599248e+80\n",
      "Gradient Descent(45/49): loss=3.031082546225268e+82\n",
      "Gradient Descent(46/49): loss=2.5171820273894214e+84\n",
      "Gradient Descent(47/49): loss=2.0904100308660455e+86\n",
      "Gradient Descent(48/49): loss=1.735994476997504e+88\n",
      "Gradient Descent(49/49): loss=1.441667797067165e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.79074435031582\n",
      "Gradient Descent(2/49): loss=25.447892947670844\n",
      "Gradient Descent(3/49): loss=500.39322992334775\n",
      "Gradient Descent(4/49): loss=13144.236999983363\n",
      "Gradient Descent(5/49): loss=570515.9932341932\n",
      "Gradient Descent(6/49): loss=37205243.46140967\n",
      "Gradient Descent(7/49): loss=2844398591.6470323\n",
      "Gradient Descent(8/49): loss=226817963233.86908\n",
      "Gradient Descent(9/49): loss=18265649334409.344\n",
      "Gradient Descent(10/49): loss=1474205351217560.2\n",
      "Gradient Descent(11/49): loss=1.190412251763514e+17\n",
      "Gradient Descent(12/49): loss=9.613583240330291e+18\n",
      "Gradient Descent(13/49): loss=7.76397364244461e+20\n",
      "Gradient Descent(14/49): loss=6.270255596850983e+22\n",
      "Gradient Descent(15/49): loss=5.06392168815103e+24\n",
      "Gradient Descent(16/49): loss=4.0896753985318815e+26\n",
      "Gradient Descent(17/49): loss=3.302864251395151e+28\n",
      "Gradient Descent(18/49): loss=2.6674274492340865e+30\n",
      "Gradient Descent(19/49): loss=2.154242099509291e+32\n",
      "Gradient Descent(20/49): loss=1.7397882847386454e+34\n",
      "Gradient Descent(21/49): loss=1.4050710813240092e+36\n",
      "Gradient Descent(22/49): loss=1.1347499928356347e+38\n",
      "Gradient Descent(23/49): loss=9.164358752848742e+39\n",
      "Gradient Descent(24/49): loss=7.401231273964827e+41\n",
      "Gradient Descent(25/49): loss=5.977311217078888e+43\n",
      "Gradient Descent(26/49): loss=4.827338595876739e+45\n",
      "Gradient Descent(27/49): loss=3.898608767878255e+47\n",
      "Gradient Descent(28/49): loss=3.1485569166330974e+49\n",
      "Gradient Descent(29/49): loss=2.542807254464102e+51\n",
      "Gradient Descent(30/49): loss=2.0535975383508695e+53\n",
      "Gradient Descent(31/49): loss=1.6585066926000031e+55\n",
      "Gradient Descent(32/49): loss=1.3394272237042156e+57\n",
      "Gradient Descent(33/49): loss=1.081735331913221e+59\n",
      "Gradient Descent(34/49): loss=8.736206847232364e+60\n",
      "Gradient Descent(35/49): loss=7.055451349882699e+62\n",
      "Gradient Descent(36/49): loss=5.69805576047369e+64\n",
      "Gradient Descent(37/49): loss=4.601808989868003e+66\n",
      "Gradient Descent(38/49): loss=3.7164687165977797e+68\n",
      "Gradient Descent(39/49): loss=3.0014587202251505e+70\n",
      "Gradient Descent(40/49): loss=2.4240092238587848e+72\n",
      "Gradient Descent(41/49): loss=1.9576550154625511e+74\n",
      "Gradient Descent(42/49): loss=1.581022515031818e+76\n",
      "Gradient Descent(43/49): loss=1.2768501974526735e+78\n",
      "Gradient Descent(44/49): loss=1.0311974758323428e+80\n",
      "Gradient Descent(45/49): loss=8.328057874638889e+81\n",
      "Gradient Descent(46/49): loss=6.72582600217798e+83\n",
      "Gradient Descent(47/49): loss=5.4318469074683484e+85\n",
      "Gradient Descent(48/49): loss=4.386815956377461e+87\n",
      "Gradient Descent(49/49): loss=3.542838110674445e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.81993130274252\n",
      "Gradient Descent(2/49): loss=25.83122727292793\n",
      "Gradient Descent(3/49): loss=501.0685529585107\n",
      "Gradient Descent(4/49): loss=12753.874298871286\n",
      "Gradient Descent(5/49): loss=532792.1919489923\n",
      "Gradient Descent(6/49): loss=34084696.15484968\n",
      "Gradient Descent(7/49): loss=2592138863.536391\n",
      "Gradient Descent(8/49): loss=206493585574.33868\n",
      "Gradient Descent(9/49): loss=16628721220222.324\n",
      "Gradient Descent(10/49): loss=1342366905310571.5\n",
      "Gradient Descent(11/49): loss=1.0842279375392982e+17\n",
      "Gradient Descent(12/49): loss=8.758360700266579e+18\n",
      "Gradient Descent(13/49): loss=7.075169584950481e+20\n",
      "Gradient Descent(14/49): loss=5.715490550778764e+22\n",
      "Gradient Descent(15/49): loss=4.617115669861525e+24\n",
      "Gradient Descent(16/49): loss=3.729822198242219e+26\n",
      "Gradient Descent(17/49): loss=3.013044411217592e+28\n",
      "Gradient Descent(18/49): loss=2.4340132793070492e+30\n",
      "Gradient Descent(19/49): loss=1.9662573314817483e+32\n",
      "Gradient Descent(20/49): loss=1.5883922776243156e+34\n",
      "Gradient Descent(21/49): loss=1.2831433544533008e+36\n",
      "Gradient Descent(22/49): loss=1.0365555734136507e+38\n",
      "Gradient Descent(23/49): loss=8.37355742875315e+39\n",
      "Gradient Descent(24/49): loss=6.764370942683205e+41\n",
      "Gradient Descent(25/49): loss=5.464429501984031e+43\n",
      "Gradient Descent(26/49): loss=4.414304010700969e+45\n",
      "Gradient Descent(27/49): loss=3.565986145821113e+47\n",
      "Gradient Descent(28/49): loss=2.88069357284e+49\n",
      "Gradient Descent(29/49): loss=2.3270969435274802e+51\n",
      "Gradient Descent(30/49): loss=1.8798876199929165e+53\n",
      "Gradient Descent(31/49): loss=1.5186206460508297e+55\n",
      "Gradient Descent(32/49): loss=1.226780070300444e+57\n",
      "Gradient Descent(33/49): loss=9.91023890528583e+58\n",
      "Gradient Descent(34/49): loss=8.005741007496857e+60\n",
      "Gradient Descent(35/49): loss=6.4672395581636595e+62\n",
      "Gradient Descent(36/49): loss=5.224399273410253e+64\n",
      "Gradient Descent(37/49): loss=4.220401536472406e+66\n",
      "Gradient Descent(38/49): loss=3.4093468352834877e+68\n",
      "Gradient Descent(39/49): loss=2.7541563860232233e+70\n",
      "Gradient Descent(40/49): loss=2.224877011681878e+72\n",
      "Gradient Descent(41/49): loss=1.7973117801992174e+74\n",
      "Gradient Descent(42/49): loss=1.451913799406368e+76\n",
      "Gradient Descent(43/49): loss=1.1728925966717328e+78\n",
      "Gradient Descent(44/49): loss=9.474922298347666e+79\n",
      "Gradient Descent(45/49): loss=7.654081270056066e+81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=6.183159950434483e+83\n",
      "Gradient Descent(47/49): loss=4.9949125994030726e+85\n",
      "Gradient Descent(48/49): loss=4.035016411619987e+87\n",
      "Gradient Descent(49/49): loss=3.259588054451476e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7634586448667398\n",
      "Gradient Descent(2/49): loss=24.483274546918768\n",
      "Gradient Descent(3/49): loss=459.3707701047003\n",
      "Gradient Descent(4/49): loss=10711.042552355304\n",
      "Gradient Descent(5/49): loss=395819.68976779544\n",
      "Gradient Descent(6/49): loss=23788226.922343273\n",
      "Gradient Descent(7/49): loss=1794456908.1365218\n",
      "Gradient Descent(8/49): loss=144305688944.92355\n",
      "Gradient Descent(9/49): loss=11779331461811.975\n",
      "Gradient Descent(10/49): loss=964718471279292.0\n",
      "Gradient Descent(11/49): loss=7.906747702869982e+16\n",
      "Gradient Descent(12/49): loss=6.481339849921535e+18\n",
      "Gradient Descent(13/49): loss=5.313087593293497e+20\n",
      "Gradient Descent(14/49): loss=4.355444712400161e+22\n",
      "Gradient Descent(15/49): loss=3.5704155769793635e+24\n",
      "Gradient Descent(16/49): loss=2.926881858959331e+26\n",
      "Gradient Descent(17/49): loss=2.399339216306812e+28\n",
      "Gradient Descent(18/49): loss=1.966881157106971e+30\n",
      "Gradient Descent(19/49): loss=1.6123695536674786e+32\n",
      "Gradient Descent(20/49): loss=1.3217552928536932e+34\n",
      "Gradient Descent(21/49): loss=1.0835214858411486e+36\n",
      "Gradient Descent(22/49): loss=8.882270543384768e+37\n",
      "Gradient Descent(23/49): loss=7.281325847022439e+39\n",
      "Gradient Descent(24/49): loss=5.968936189419221e+41\n",
      "Gradient Descent(25/49): loss=4.893092272191494e+43\n",
      "Gradient Descent(26/49): loss=4.011158977813191e+45\n",
      "Gradient Descent(27/49): loss=3.288185762760061e+47\n",
      "Gradient Descent(28/49): loss=2.695521586210654e+49\n",
      "Gradient Descent(29/49): loss=2.2096794846617314e+51\n",
      "Gradient Descent(30/49): loss=1.8114057961594636e+53\n",
      "Gradient Descent(31/49): loss=1.484917148000918e+55\n",
      "Gradient Descent(32/49): loss=1.2172749701376636e+57\n",
      "Gradient Descent(33/49): loss=9.978727465828425e+58\n",
      "Gradient Descent(34/49): loss=8.180156848704464e+60\n",
      "Gradient Descent(35/49): loss=6.705761460922959e+62\n",
      "Gradient Descent(36/49): loss=5.497111803904055e+64\n",
      "Gradient Descent(37/49): loss=4.506309739872809e+66\n",
      "Gradient Descent(38/49): loss=3.694090314344867e+68\n",
      "Gradient Descent(39/49): loss=3.028265707035337e+70\n",
      "Gradient Descent(40/49): loss=2.482449645802095e+72\n",
      "Gradient Descent(41/49): loss=2.0350117328297333e+74\n",
      "Gradient Descent(42/49): loss=1.6682202435637402e+76\n",
      "Gradient Descent(43/49): loss=1.3675394279747498e+78\n",
      "Gradient Descent(44/49): loss=1.1210534665797049e+80\n",
      "Gradient Descent(45/49): loss=9.189942529053936e+81\n",
      "Gradient Descent(46/49): loss=7.53354288667284e+83\n",
      "Gradient Descent(47/49): loss=6.175693509063009e+85\n",
      "Gradient Descent(48/49): loss=5.0625835535300055e+87\n",
      "Gradient Descent(49/49): loss=4.150101069436099e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.7989270808510507\n",
      "Gradient Descent(2/49): loss=25.680449302597648\n",
      "Gradient Descent(3/49): loss=515.1475148282274\n",
      "Gradient Descent(4/49): loss=14422.80064428202\n",
      "Gradient Descent(5/49): loss=691302.933640748\n",
      "Gradient Descent(6/49): loss=48717752.19854363\n",
      "Gradient Descent(7/49): loss=3929372285.9826417\n",
      "Gradient Descent(8/49): loss=327680698085.6346\n",
      "Gradient Descent(9/49): loss=27529876565557.145\n",
      "Gradient Descent(10/49): loss=2316637254804094.5\n",
      "Gradient Descent(11/49): loss=1.950127654413843e+17\n",
      "Gradient Descent(12/49): loss=1.6417259520000707e+19\n",
      "Gradient Descent(13/49): loss=1.3821186237152643e+21\n",
      "Gradient Descent(14/49): loss=1.1635672553550718e+23\n",
      "Gradient Descent(15/49): loss=9.795756701053148e+24\n",
      "Gradient Descent(16/49): loss=8.246782855059802e+26\n",
      "Gradient Descent(17/49): loss=6.942743875143614e+28\n",
      "Gradient Descent(18/49): loss=5.844908702789774e+30\n",
      "Gradient Descent(19/49): loss=4.920670906763216e+32\n",
      "Gradient Descent(20/49): loss=4.1425800491292537e+34\n",
      "Gradient Descent(21/49): loss=3.487526353602488e+36\n",
      "Gradient Descent(22/49): loss=2.9360543243639543e+38\n",
      "Gradient Descent(23/49): loss=2.4717849047197167e+40\n",
      "Gradient Descent(24/49): loss=2.0809290088760408e+42\n",
      "Gradient Descent(25/49): loss=1.7518779776161474e+44\n",
      "Gradient Descent(26/49): loss=1.4748587940125234e+46\n",
      "Gradient Descent(27/49): loss=1.2416438188440297e+48\n",
      "Gradient Descent(28/49): loss=1.0453064246776384e+50\n",
      "Gradient Descent(29/49): loss=8.800152707961029e+51\n",
      "Gradient Descent(30/49): loss=7.408611088113696e+53\n",
      "Gradient Descent(31/49): loss=6.237109749841799e+55\n",
      "Gradient Descent(32/49): loss=5.2508543867264245e+57\n",
      "Gradient Descent(33/49): loss=4.4205526111358995e+59\n",
      "Gradient Descent(34/49): loss=3.721543952393586e+61\n",
      "Gradient Descent(35/49): loss=3.1330674257121868e+63\n",
      "Gradient Descent(36/49): loss=2.637644918245677e+65\n",
      "Gradient Descent(37/49): loss=2.2205620784448098e+67\n",
      "Gradient Descent(38/49): loss=1.86943129081487e+69\n",
      "Gradient Descent(39/49): loss=1.5738237561569248e+71\n",
      "Gradient Descent(40/49): loss=1.3249597498521917e+73\n",
      "Gradient Descent(41/49): loss=1.1154478586694813e+75\n",
      "Gradient Descent(42/49): loss=9.390654512705758e+76\n",
      "Gradient Descent(43/49): loss=7.905738622528574e+78\n",
      "Gradient Descent(44/49): loss=6.65562800581967e+80\n",
      "Gradient Descent(45/49): loss=5.603193612500461e+82\n",
      "Gradient Descent(46/49): loss=4.71717749725709e+84\n",
      "Gradient Descent(47/49): loss=3.9712644394414726e+86\n",
      "Gradient Descent(48/49): loss=3.343300364919969e+88\n",
      "Gradient Descent(49/49): loss=2.8146343565190426e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.81271485473244\n",
      "Gradient Descent(2/49): loss=26.221382850036154\n",
      "Gradient Descent(3/49): loss=523.4095403044012\n",
      "Gradient Descent(4/49): loss=13933.586268683852\n",
      "Gradient Descent(5/49): loss=612037.6590283532\n",
      "Gradient Descent(6/49): loss=40411506.9854345\n",
      "Gradient Descent(7/49): loss=3130628098.849199\n",
      "Gradient Descent(8/49): loss=253046606405.04703\n",
      "Gradient Descent(9/49): loss=20657677969839.28\n",
      "Gradient Descent(10/49): loss=1690201934254505.5\n",
      "Gradient Descent(11/49): loss=1.3836141939385923e+17\n",
      "Gradient Descent(12/49): loss=1.1327670823009114e+19\n",
      "Gradient Descent(13/49): loss=9.274217208156721e+20\n",
      "Gradient Descent(14/49): loss=7.593052094962463e+22\n",
      "Gradient Descent(15/49): loss=6.216644751959257e+24\n",
      "Gradient Descent(16/49): loss=5.0897429061192047e+26\n",
      "Gradient Descent(17/49): loss=4.167116756193784e+28\n",
      "Gradient Descent(18/49): loss=3.411736629913936e+30\n",
      "Gradient Descent(19/49): loss=2.7932855138154426e+32\n",
      "Gradient Descent(20/49): loss=2.2869420514245576e+34\n",
      "Gradient Descent(21/49): loss=1.87238430212535e+36\n",
      "Gradient Descent(22/49): loss=1.5329741183380893e+38\n",
      "Gradient Descent(23/49): loss=1.2550893771359547e+40\n",
      "Gradient Descent(24/49): loss=1.02757726028027e+42\n",
      "Gradient Descent(25/49): loss=8.413066392571046e+43\n",
      "Gradient Descent(26/49): loss=6.888016002466638e+45\n",
      "Gradient Descent(27/49): loss=5.639414006304843e+47\n",
      "Gradient Descent(28/49): loss=4.617148148772909e+49\n",
      "Gradient Descent(29/49): loss=3.780190105547156e+51\n",
      "Gradient Descent(30/49): loss=3.094948824172769e+53\n",
      "Gradient Descent(31/49): loss=2.533922357553546e+55\n",
      "Gradient Descent(32/49): loss=2.074594081802269e+57\n",
      "Gradient Descent(33/49): loss=1.6985289984987388e+59\n",
      "Gradient Descent(34/49): loss=1.3906338517242905e+61\n",
      "Gradient Descent(35/49): loss=1.1385513648991535e+63\n",
      "Gradient Descent(36/49): loss=9.321642853051429e+64\n",
      "Gradient Descent(37/49): loss=7.631893312739893e+66\n",
      "Gradient Descent(38/49): loss=6.248447452368971e+68\n",
      "Gradient Descent(39/49): loss=5.115781099801558e+70\n",
      "Gradient Descent(40/49): loss=4.188435040958048e+72\n",
      "Gradient Descent(41/49): loss=3.4291905283057707e+74\n",
      "Gradient Descent(42/49): loss=2.807575517927127e+76\n",
      "Gradient Descent(43/49): loss=2.298641683452378e+78\n",
      "Gradient Descent(44/49): loss=1.8819631226894885e+80\n",
      "Gradient Descent(45/49): loss=1.5408165703511046e+82\n",
      "Gradient Descent(46/49): loss=1.2615102149694295e+84\n",
      "Gradient Descent(47/49): loss=1.0328341822736003e+86\n",
      "Gradient Descent(48/49): loss=8.456106303496317e+87\n",
      "Gradient Descent(49/49): loss=6.923253997909261e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8423801155967392\n",
      "Gradient Descent(2/49): loss=26.61709985786103\n",
      "Gradient Descent(3/49): loss=524.1520856951706\n",
      "Gradient Descent(4/49): loss=13522.042998175817\n",
      "Gradient Descent(5/49): loss=571657.8192204057\n",
      "Gradient Descent(6/49): loss=37024338.38721978\n",
      "Gradient Descent(7/49): loss=2853030826.743564\n",
      "Gradient Descent(8/49): loss=230372670168.3293\n",
      "Gradient Descent(9/49): loss=18806371210096.375\n",
      "Gradient Descent(10/49): loss=1539044483381485.8\n",
      "Gradient Descent(11/49): loss=1.2601939027910346e+17\n",
      "Gradient Descent(12/49): loss=1.0319941053600903e+19\n",
      "Gradient Descent(13/49): loss=8.451407484660698e+20\n",
      "Gradient Descent(14/49): loss=6.9212341697153e+22\n",
      "Gradient Descent(15/49): loss=5.668114862151512e+24\n",
      "Gradient Descent(16/49): loss=4.641879638821819e+26\n",
      "Gradient Descent(17/49): loss=3.8014487329467954e+28\n",
      "Gradient Descent(18/49): loss=3.113181257438697e+30\n",
      "Gradient Descent(19/49): loss=2.5495273659408947e+32\n",
      "Gradient Descent(20/49): loss=2.087925262663013e+34\n",
      "Gradient Descent(21/49): loss=1.7098980624601272e+36\n",
      "Gradient Descent(22/49): loss=1.4003141953402365e+38\n",
      "Gradient Descent(23/49): loss=1.146781722687518e+40\n",
      "Gradient Descent(24/49): loss=9.391523158651966e+41\n",
      "Gradient Descent(25/49): loss=7.691150416387146e+43\n",
      "Gradient Descent(26/49): loss=6.298636943997879e+45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=5.158243592241729e+47\n",
      "Gradient Descent(28/49): loss=4.2243230072591637e+49\n",
      "Gradient Descent(29/49): loss=3.459492470750786e+51\n",
      "Gradient Descent(30/49): loss=2.8331375547313957e+53\n",
      "Gradient Descent(31/49): loss=2.3201866955609574e+55\n",
      "Gradient Descent(32/49): loss=1.9001076362381223e+57\n",
      "Gradient Descent(33/49): loss=1.5560855668200706e+59\n",
      "Gradient Descent(34/49): loss=1.274350065799301e+61\n",
      "Gradient Descent(35/49): loss=1.0436239014293236e+63\n",
      "Gradient Descent(36/49): loss=8.546716297702902e+64\n",
      "Gradient Descent(37/49): loss=6.999299208592026e+66\n",
      "Gradient Descent(38/49): loss=5.732048158023503e+68\n",
      "Gradient Descent(39/49): loss=4.694237966790687e+70\n",
      "Gradient Descent(40/49): loss=3.844327451787769e+72\n",
      "Gradient Descent(41/49): loss=3.148296626869317e+74\n",
      "Gradient Descent(42/49): loss=2.5782849601293542e+76\n",
      "Gradient Descent(43/49): loss=2.111476180133509e+78\n",
      "Gradient Descent(44/49): loss=1.7291849924329556e+80\n",
      "Gradient Descent(45/49): loss=1.416109149697478e+82\n",
      "Gradient Descent(46/49): loss=1.1597169375356236e+84\n",
      "Gradient Descent(47/49): loss=9.49745558451008e+85\n",
      "Gradient Descent(48/49): loss=7.777903353849024e+87\n",
      "Gradient Descent(49/49): loss=6.369682915967735e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.785014412264207\n",
      "Gradient Descent(2/49): loss=25.228504625233175\n",
      "Gradient Descent(3/49): loss=480.6134085042363\n",
      "Gradient Descent(4/49): loss=11361.828578251036\n",
      "Gradient Descent(5/49): loss=424922.25438142556\n",
      "Gradient Descent(6/49): loss=25844874.49523831\n",
      "Gradient Descent(7/49): loss=1975068467.474332\n",
      "Gradient Descent(8/49): loss=160983344145.6661\n",
      "Gradient Descent(9/49): loss=13320780391197.521\n",
      "Gradient Descent(10/49): loss=1105955541284532.9\n",
      "Gradient Descent(11/49): loss=9.188976573950584e+16\n",
      "Gradient Descent(12/49): loss=7.636023622719978e+18\n",
      "Gradient Descent(13/49): loss=6.345749108158126e+20\n",
      "Gradient Descent(14/49): loss=5.2735362870493196e+22\n",
      "Gradient Descent(15/49): loss=4.3824980168480337e+24\n",
      "Gradient Descent(16/49): loss=3.642014596704086e+26\n",
      "Gradient Descent(17/49): loss=3.0266463038994383e+28\n",
      "Gradient Descent(18/49): loss=2.5152529658483178e+30\n",
      "Gradient Descent(19/49): loss=2.0902665433360375e+32\n",
      "Gradient Descent(20/49): loss=1.737087396519589e+34\n",
      "Gradient Descent(21/49): loss=1.4435827016176898e+36\n",
      "Gradient Descent(22/49): loss=1.1996696428018221e+38\n",
      "Gradient Descent(23/49): loss=9.969690342374344e+39\n",
      "Gradient Descent(24/49): loss=8.285174682831959e+41\n",
      "Gradient Descent(25/49): loss=6.885281003494278e+43\n",
      "Gradient Descent(26/49): loss=5.721918524580619e+45\n",
      "Gradient Descent(27/49): loss=4.7551220618772776e+47\n",
      "Gradient Descent(28/49): loss=3.951679096131294e+49\n",
      "Gradient Descent(29/49): loss=3.28398881786777e+51\n",
      "Gradient Descent(30/49): loss=2.7291139522029737e+53\n",
      "Gradient Descent(31/49): loss=2.2679927908355732e+55\n",
      "Gradient Descent(32/49): loss=1.8847843620197586e+57\n",
      "Gradient Descent(33/49): loss=1.5663242430349695e+59\n",
      "Gradient Descent(34/49): loss=1.3016723205884272e+61\n",
      "Gradient Descent(35/49): loss=1.081736963288659e+63\n",
      "Gradient Descent(36/49): loss=8.98962695324118e+64\n",
      "Gradient Descent(37/49): loss=7.470706419493604e+66\n",
      "Gradient Descent(38/49): loss=6.208428302593934e+68\n",
      "Gradient Descent(39/49): loss=5.159429353009136e+70\n",
      "Gradient Descent(40/49): loss=4.28767313581921e+72\n",
      "Gradient Descent(41/49): loss=3.5632120650909295e+74\n",
      "Gradient Descent(42/49): loss=2.9611586094897743e+76\n",
      "Gradient Descent(43/49): loss=2.4608303267888283e+78\n",
      "Gradient Descent(44/49): loss=2.0450393564993777e+80\n",
      "Gradient Descent(45/49): loss=1.6995019624489225e+82\n",
      "Gradient Descent(46/49): loss=1.4123478412228521e+84\n",
      "Gradient Descent(47/49): loss=1.1737123396624552e+86\n",
      "Gradient Descent(48/49): loss=9.753975727984786e+87\n",
      "Gradient Descent(49/49): loss=8.105908005489413e+89\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8208697481128502\n",
      "Gradient Descent(2/49): loss=26.4558167033214\n",
      "Gradient Descent(3/49): loss=538.6374951989744\n",
      "Gradient Descent(4/49): loss=15278.213136930952\n",
      "Gradient Descent(5/49): loss=740984.6716966459\n",
      "Gradient Descent(6/49): loss=52873588.226790965\n",
      "Gradient Descent(7/49): loss=4321217993.100627\n",
      "Gradient Descent(8/49): loss=365242301474.4204\n",
      "Gradient Descent(9/49): loss=31103795290734.594\n",
      "Gradient Descent(10/49): loss=2653102855755679.5\n",
      "Gradient Descent(11/49): loss=2.2638517910230778e+17\n",
      "Gradient Descent(12/49): loss=1.9318570558148383e+19\n",
      "Gradient Descent(13/49): loss=1.648576604209465e+21\n",
      "Gradient Descent(14/49): loss=1.4068403608690583e+23\n",
      "Gradient Descent(15/49): loss=1.2005516241194683e+25\n",
      "Gradient Descent(16/49): loss=1.0245117222165228e+27\n",
      "Gradient Descent(17/49): loss=8.742850248398531e+28\n",
      "Gradient Descent(18/49): loss=7.46086446830805e+30\n",
      "Gradient Descent(19/49): loss=6.3668594480990826e+32\n",
      "Gradient Descent(20/49): loss=5.433271093268685e+34\n",
      "Gradient Descent(21/49): loss=4.636577109309176e+36\n",
      "Gradient Descent(22/49): loss=3.9567043355456985e+38\n",
      "Gradient Descent(23/49): loss=3.376522988812214e+40\n",
      "Gradient Descent(24/49): loss=2.8814150684862537e+42\n",
      "Gradient Descent(25/49): loss=2.458906047555639e+44\n",
      "Gradient Descent(26/49): loss=2.0983505697713515e+46\n",
      "Gradient Descent(27/49): loss=1.7906642338111556e+48\n",
      "Gradient Descent(28/49): loss=1.5280947065961416e+50\n",
      "Gradient Descent(29/49): loss=1.3040263988281162e+52\n",
      "Gradient Descent(30/49): loss=1.112813781436705e+54\n",
      "Gradient Descent(31/49): loss=9.496391432476397e+55\n",
      "Gradient Descent(32/49): loss=8.103912059965918e+57\n",
      "Gradient Descent(33/49): loss=6.915615383235584e+59\n",
      "Gradient Descent(34/49): loss=5.901561588397342e+61\n",
      "Gradient Descent(35/49): loss=5.036201010553091e+63\n",
      "Gradient Descent(36/49): loss=4.29773039538557e+65\n",
      "Gradient Descent(37/49): loss=3.6675435537059163e+67\n",
      "Gradient Descent(38/49): loss=3.129762567882822e+69\n",
      "Gradient Descent(39/49): loss=2.670837738633698e+71\n",
      "Gradient Descent(40/49): loss=2.279206192607552e+73\n",
      "Gradient Descent(41/49): loss=1.9450005491827654e+75\n",
      "Gradient Descent(42/49): loss=1.659800306172913e+77\n",
      "Gradient Descent(43/49): loss=1.4164196804619468e+79\n",
      "Gradient Descent(44/49): loss=1.2087265580916659e+81\n",
      "Gradient Descent(45/49): loss=1.0314879921462609e+83\n",
      "Gradient Descent(46/49): loss=8.802383556639238e+84\n",
      "Gradient Descent(47/49): loss=7.51166827613513e+86\n",
      "Gradient Descent(48/49): loss=6.410213770807118e+88\n",
      "Gradient Descent(49/49): loss=5.4702682649062005e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8348571744459754\n",
      "Gradient Descent(2/49): loss=27.012646131193502\n",
      "Gradient Descent(3/49): loss=547.3031943637467\n",
      "Gradient Descent(4/49): loss=14764.220099164277\n",
      "Gradient Descent(5/49): loss=656265.7779340511\n",
      "Gradient Descent(6/49): loss=43869472.22352337\n",
      "Gradient Descent(7/49): loss=3443419823.3715153\n",
      "Gradient Descent(8/49): loss=282098643442.58124\n",
      "Gradient Descent(9/49): loss=23343448993569.34\n",
      "Gradient Descent(10/49): loss=1936047576771805.0\n",
      "Gradient Descent(11/49): loss=1.6065310771396986e+17\n",
      "Gradient Descent(12/49): loss=1.3332516584348641e+19\n",
      "Gradient Descent(13/49): loss=1.1064870591442962e+21\n",
      "Gradient Descent(14/49): loss=9.182967800966082e+22\n",
      "Gradient Descent(15/49): loss=7.621147182382216e+24\n",
      "Gradient Descent(16/49): loss=6.324959708018123e+26\n",
      "Gradient Descent(17/49): loss=5.249225211710069e+28\n",
      "Gradient Descent(18/49): loss=4.356449210913094e+30\n",
      "Gradient Descent(19/49): loss=3.6155144853924044e+32\n",
      "Gradient Descent(20/49): loss=3.0005962128614507e+34\n",
      "Gradient Descent(21/49): loss=2.490261806588318e+36\n",
      "Gradient Descent(22/49): loss=2.066723886074208e+38\n",
      "Gradient Descent(23/49): loss=1.7152203073609717e+40\n",
      "Gradient Descent(24/49): loss=1.423499637572469e+42\n",
      "Gradient Descent(25/49): loss=1.1813941389765605e+44\n",
      "Gradient Descent(26/49): loss=9.804653789644881e+45\n",
      "Gradient Descent(27/49): loss=8.137101138666207e+47\n",
      "Gradient Descent(28/49): loss=6.753161953644343e+49\n",
      "Gradient Descent(29/49): loss=5.6045999177079324e+51\n",
      "Gradient Descent(30/49): loss=4.651382634266557e+53\n",
      "Gradient Descent(31/49): loss=3.8602863233821957e+55\n",
      "Gradient Descent(32/49): loss=3.203737828126748e+57\n",
      "Gradient Descent(33/49): loss=2.6588535697988143e+59\n",
      "Gradient Descent(34/49): loss=2.2066419553954483e+61\n",
      "Gradient Descent(35/49): loss=1.8313414377610517e+63\n",
      "Gradient Descent(36/49): loss=1.519871156922534e+65\n",
      "Gradient Descent(37/49): loss=1.2613750150650039e+67\n",
      "Gradient Descent(38/49): loss=1.0468432941723091e+69\n",
      "Gradient Descent(39/49): loss=8.687986280567444e+70\n",
      "Gradient Descent(40/49): loss=7.210353835337522e+72\n",
      "Gradient Descent(41/49): loss=5.984033670386004e+74\n",
      "Gradient Descent(42/49): loss=4.966283179171791e+76\n",
      "Gradient Descent(43/49): loss=4.121629317993667e+78\n",
      "Gradient Descent(44/49): loss=3.4206322156961435e+80\n",
      "Gradient Descent(45/49): loss=2.838859065752636e+82\n",
      "Gradient Descent(46/49): loss=2.356032536390623e+84\n",
      "Gradient Descent(47/49): loss=1.9553240171363427e+86\n",
      "Gradient Descent(48/49): loss=1.6227670683390754e+88\n",
      "Gradient Descent(49/49): loss=1.3467706298327466e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8650044019327163\n",
      "Gradient Descent(2/49): loss=27.42104171750314\n",
      "Gradient Descent(3/49): loss=548.1168114445243\n",
      "Gradient Descent(4/49): loss=14330.503884521564\n",
      "Gradient Descent(5/49): loss=613062.893159258\n",
      "Gradient Descent(6/49): loss=40194949.5337059\n",
      "Gradient Descent(7/49): loss=3138138560.4092226\n",
      "Gradient Descent(8/49): loss=256822317503.87653\n",
      "Gradient Descent(9/49): loss=21251436839656.395\n",
      "Gradient Descent(10/49): loss=1762901062215409.0\n",
      "Gradient Descent(11/49): loss=1.4632235019479638e+17\n",
      "Gradient Descent(12/49): loss=1.2146406250110648e+19\n",
      "Gradient Descent(13/49): loss=1.0083170444034914e+21\n",
      "Gradient Descent(14/49): loss=8.370456373527281e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=6.948671407361664e+24\n",
      "Gradient Descent(16/49): loss=5.768389119997482e+26\n",
      "Gradient Descent(17/49): loss=4.7885866842713505e+28\n",
      "Gradient Descent(18/49): loss=3.975210811148404e+30\n",
      "Gradient Descent(19/49): loss=3.299992688962401e+32\n",
      "Gradient Descent(20/49): loss=2.7394652191068946e+34\n",
      "Gradient Descent(21/49): loss=2.274147367997487e+36\n",
      "Gradient Descent(22/49): loss=1.88786709738159e+38\n",
      "Gradient Descent(23/49): loss=1.5671993062374475e+40\n",
      "Gradient Descent(24/49): loss=1.30099924347563e+42\n",
      "Gradient Descent(25/49): loss=1.0800151740677449e+44\n",
      "Gradient Descent(26/49): loss=8.965668366574631e+45\n",
      "Gradient Descent(27/49): loss=7.442785174642051e+47\n",
      "Gradient Descent(28/49): loss=6.178574635036994e+49\n",
      "Gradient Descent(29/49): loss=5.129099339154258e+51\n",
      "Gradient Descent(30/49): loss=4.25788496293776e+53\n",
      "Gradient Descent(31/49): loss=3.534652608346775e+55\n",
      "Gradient Descent(32/49): loss=2.9342664657318993e+57\n",
      "Gradient Descent(33/49): loss=2.4358602233179313e+59\n",
      "Gradient Descent(34/49): loss=2.022111862312531e+61\n",
      "Gradient Descent(35/49): loss=1.678641633277092e+63\n",
      "Gradient Descent(36/49): loss=1.3935122905360024e+65\n",
      "Gradient Descent(37/49): loss=1.1568142153628957e+67\n",
      "Gradient Descent(38/49): loss=9.603210089743483e+68\n",
      "Gradient Descent(39/49): loss=7.972035855283825e+70\n",
      "Gradient Descent(40/49): loss=6.617928284814635e+72\n",
      "Gradient Descent(41/49): loss=5.493825614685433e+74\n",
      "Gradient Descent(42/49): loss=4.5606598599488416e+76\n",
      "Gradient Descent(43/49): loss=3.785999013610798e+78\n",
      "Gradient Descent(44/49): loss=3.1429198780947303e+80\n",
      "Gradient Descent(45/49): loss=2.609072354380193e+82\n",
      "Gradient Descent(46/49): loss=2.1659026683549767e+84\n",
      "Gradient Descent(47/49): loss=1.798008537751568e+86\n",
      "Gradient Descent(48/49): loss=1.4926038686137588e+88\n",
      "Gradient Descent(49/49): loss=1.239074376914117e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8067389727900174\n",
      "Gradient Descent(2/49): loss=25.990892489022357\n",
      "Gradient Descent(3/49): loss=502.6698437579474\n",
      "Gradient Descent(4/49): loss=12047.03692595023\n",
      "Gradient Descent(5/49): loss=455942.0058213732\n",
      "Gradient Descent(6/49): loss=28063556.3336344\n",
      "Gradient Descent(7/49): loss=2172446130.5944285\n",
      "Gradient Descent(8/49): loss=179455306197.68677\n",
      "Gradient Descent(9/49): loss=15051381491717.346\n",
      "Gradient Descent(10/49): loss=1266695305502905.2\n",
      "Gradient Descent(11/49): loss=1.0668259366686408e+17\n",
      "Gradient Descent(12/49): loss=8.986418318516964e+18\n",
      "Gradient Descent(13/49): loss=7.569992674082992e+20\n",
      "Gradient Descent(14/49): loss=6.376872797190293e+22\n",
      "Gradient Descent(15/49): loss=5.371812075853867e+24\n",
      "Gradient Descent(16/49): loss=4.525161005615706e+26\n",
      "Gradient Descent(17/49): loss=3.811950897827426e+28\n",
      "Gradient Descent(18/49): loss=3.211149813404313e+30\n",
      "Gradient Descent(19/49): loss=2.705040920740374e+32\n",
      "Gradient Descent(20/49): loss=2.2786997850256014e+34\n",
      "Gradient Descent(21/49): loss=1.919554218753171e+36\n",
      "Gradient Descent(22/49): loss=1.6170135368897119e+38\n",
      "Gradient Descent(23/49): loss=1.362156251157113e+40\n",
      "Gradient Descent(24/49): loss=1.147466987901218e+42\n",
      "Gradient Descent(25/49): loss=9.666148705078814e+43\n",
      "Gradient Descent(26/49): loss=8.142668309752539e+45\n",
      "Gradient Descent(27/49): loss=6.859303454313139e+47\n",
      "Gradient Descent(28/49): loss=5.778209560863314e+49\n",
      "Gradient Descent(29/49): loss=4.8675067303311946e+51\n",
      "Gradient Descent(30/49): loss=4.100339650242676e+53\n",
      "Gradient Descent(31/49): loss=3.4540856703053814e+55\n",
      "Gradient Descent(32/49): loss=2.9096876930922822e+57\n",
      "Gradient Descent(33/49): loss=2.4510922077343887e+59\n",
      "Gradient Descent(34/49): loss=2.0647758950484952e+61\n",
      "Gradient Descent(35/49): loss=1.739346844366155e+63\n",
      "Gradient Descent(36/49): loss=1.465208622524861e+65\n",
      "Gradient Descent(37/49): loss=1.234277288900096e+67\n",
      "Gradient Descent(38/49): loss=1.0397430116602598e+69\n",
      "Gradient Descent(39/49): loss=8.758692556513934e+70\n",
      "Gradient Descent(40/49): loss=7.37823620252399e+72\n",
      "Gradient Descent(41/49): loss=6.215353388531724e+74\n",
      "Gradient Descent(42/49): loss=5.235752378206325e+76\n",
      "Gradient Descent(43/49): loss=4.410546151160814e+78\n",
      "Gradient Descent(44/49): loss=3.7154005664003325e+80\n",
      "Gradient Descent(45/49): loss=3.129816783614151e+82\n",
      "Gradient Descent(46/49): loss=2.636526781951696e+84\n",
      "Gradient Descent(47/49): loss=2.2209841510024197e+86\n",
      "Gradient Descent(48/49): loss=1.8709351381412923e+88\n",
      "Gradient Descent(49/49): loss=1.5760573030437657e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8429828274412992\n",
      "Gradient Descent(2/49): loss=27.248882291846627\n",
      "Gradient Descent(3/49): loss=563.0153504215461\n",
      "Gradient Descent(4/49): loss=16177.824546373009\n",
      "Gradient Descent(5/49): loss=793863.8492755633\n",
      "Gradient Descent(6/49): loss=57352338.182535656\n",
      "Gradient Descent(7/49): loss=4749099886.651564\n",
      "Gradient Descent(8/49): loss=406812161331.23004\n",
      "Gradient Descent(9/49): loss=35112804165358.09\n",
      "Gradient Descent(10/49): loss=3035662487125144.5\n",
      "Gradient Descent(11/49): loss=2.625406895496006e+17\n",
      "Gradient Descent(12/49): loss=2.2707709401773785e+19\n",
      "Gradient Descent(13/49): loss=1.964071501938084e+21\n",
      "Gradient Descent(14/49): loss=1.6988022628055366e+23\n",
      "Gradient Descent(15/49): loss=1.469361671275014e+25\n",
      "Gradient Descent(16/49): loss=1.270909589281624e+27\n",
      "Gradient Descent(17/49): loss=1.0992604984677606e+29\n",
      "Gradient Descent(18/49): loss=9.507943470087839e+30\n",
      "Gradient Descent(19/49): loss=8.223800392264817e+32\n",
      "Gradient Descent(20/49): loss=7.113093712599611e+34\n",
      "Gradient Descent(21/49): loss=6.152399104416407e+36\n",
      "Gradient Descent(22/49): loss=5.3214559332504415e+38\n",
      "Gradient Descent(23/49): loss=4.602739966804815e+40\n",
      "Gradient Descent(24/49): loss=3.981093796091986e+42\n",
      "Gradient Descent(25/49): loss=3.443407172160244e+44\n",
      "Gradient Descent(26/49): loss=2.978340516599936e+46\n",
      "Gradient Descent(27/49): loss=2.576085774734469e+48\n",
      "Gradient Descent(28/49): loss=2.2281595679882744e+50\n",
      "Gradient Descent(29/49): loss=1.9272242831004202e+52\n",
      "Gradient Descent(30/49): loss=1.6669333250335173e+54\n",
      "Gradient Descent(31/49): loss=1.441797270028733e+56\n",
      "Gradient Descent(32/49): loss=1.24706809603228e+58\n",
      "Gradient Descent(33/49): loss=1.0786390489632356e+60\n",
      "Gradient Descent(34/49): loss=9.329580330456807e+61\n",
      "Gradient Descent(35/49): loss=8.0695269864474e+63\n",
      "Gradient Descent(36/49): loss=6.979656477411381e+65\n",
      "Gradient Descent(37/49): loss=6.036983905560665e+67\n",
      "Gradient Descent(38/49): loss=5.2216287139556565e+69\n",
      "Gradient Descent(39/49): loss=4.516395414155744e+71\n",
      "Gradient Descent(40/49): loss=3.906410940803013e+73\n",
      "Gradient Descent(41/49): loss=3.378810985104599e+75\n",
      "Gradient Descent(42/49): loss=2.922468692122917e+77\n",
      "Gradient Descent(43/49): loss=2.527760000216322e+79\n",
      "Gradient Descent(44/49): loss=2.1863606737398354e+81\n",
      "Gradient Descent(45/49): loss=1.8910707485153157e+83\n",
      "Gradient Descent(46/49): loss=1.6356626876997417e+85\n",
      "Gradient Descent(47/49): loss=1.4147500457259703e+87\n",
      "Gradient Descent(48/49): loss=1.2236738704949124e+89\n",
      "Gradient Descent(49/49): loss=1.058404448090057e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.857171309456427\n",
      "Gradient Descent(2/49): loss=27.821953256553133\n",
      "Gradient Descent(3/49): loss=572.1007782243846\n",
      "Gradient Descent(4/49): loss=15637.980499247435\n",
      "Gradient Descent(5/49): loss=703356.4126275611\n",
      "Gradient Descent(6/49): loss=47596992.089651324\n",
      "Gradient Descent(7/49): loss=3785033682.7611732\n",
      "Gradient Descent(8/49): loss=314255752776.7411\n",
      "Gradient Descent(9/49): loss=26356671949262.215\n",
      "Gradient Descent(10/49): loss=2215622234941560.8\n",
      "Gradient Descent(11/49): loss=1.8634842966552138e+17\n",
      "Gradient Descent(12/49): loss=1.5674956859426812e+19\n",
      "Gradient Descent(13/49): loss=1.31855530289744e+21\n",
      "Gradient Descent(14/49): loss=1.1091566923834599e+23\n",
      "Gradient Descent(15/49): loss=9.330137276160682e+24\n",
      "Gradient Descent(16/49): loss=7.84843910026277e+26\n",
      "Gradient Descent(17/49): loss=6.602046530968205e+28\n",
      "Gradient Descent(18/49): loss=5.553590783714153e+30\n",
      "Gradient Descent(19/49): loss=4.671637885733861e+32\n",
      "Gradient Descent(20/49): loss=3.9297458891012395e+34\n",
      "Gradient Descent(21/49): loss=3.305672043356242e+36\n",
      "Gradient Descent(22/49): loss=2.7807059201853853e+38\n",
      "Gradient Descent(23/49): loss=2.339108451535984e+40\n",
      "Gradient Descent(24/49): loss=1.967640054397772e+42\n",
      "Gradient Descent(25/49): loss=1.6551636932993909e+44\n",
      "Gradient Descent(26/49): loss=1.392310979588836e+46\n",
      "Gradient Descent(27/49): loss=1.1712012967245856e+48\n",
      "Gradient Descent(28/49): loss=9.852055306310936e+49\n",
      "Gradient Descent(29/49): loss=8.287473214899979e+51\n",
      "Gradient Descent(30/49): loss=6.971358782739297e+53\n",
      "Gradient Descent(31/49): loss=5.864253436173999e+55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=4.932964926267232e+57\n",
      "Gradient Descent(33/49): loss=4.149572188281698e+59\n",
      "Gradient Descent(34/49): loss=3.490588237121396e+61\n",
      "Gradient Descent(35/49): loss=2.9362560014110282e+63\n",
      "Gradient Descent(36/49): loss=2.4699559845340575e+65\n",
      "Gradient Descent(37/49): loss=2.0777079936503207e+67\n",
      "Gradient Descent(38/49): loss=1.7477519979745643e+69\n",
      "Gradient Descent(39/49): loss=1.4701955499807002e+71\n",
      "Gradient Descent(40/49): loss=1.2367171988290864e+73\n",
      "Gradient Descent(41/49): loss=1.040317004020156e+75\n",
      "Gradient Descent(42/49): loss=8.751066693971339e+76\n",
      "Gradient Descent(43/49): loss=7.361330054819412e+78\n",
      "Gradient Descent(44/49): loss=6.192294273487697e+80\n",
      "Gradient Descent(45/49): loss=5.2089103577641205e+82\n",
      "Gradient Descent(46/49): loss=4.38169536473591e+84\n",
      "Gradient Descent(47/49): loss=3.685848469388834e+86\n",
      "Gradient Descent(48/49): loss=3.1005074083041497e+88\n",
      "Gradient Descent(49/49): loss=2.608123005811726e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8878041617504504\n",
      "Gradient Descent(2/49): loss=28.243327983922416\n",
      "Gradient Descent(3/49): loss=572.989469448669\n",
      "Gradient Descent(4/49): loss=15181.05968776414\n",
      "Gradient Descent(5/49): loss=657154.2146677034\n",
      "Gradient Descent(6/49): loss=43612927.28280022\n",
      "Gradient Descent(7/49): loss=3449522809.5782523\n",
      "Gradient Descent(8/49): loss=286098992285.5194\n",
      "Gradient Descent(9/49): loss=23994606356806.074\n",
      "Gradient Descent(10/49): loss=2017469487549535.2\n",
      "Gradient Descent(11/49): loss=1.6972525017269702e+17\n",
      "Gradient Descent(12/49): loss=1.428042425808861e+19\n",
      "Gradient Descent(13/49): loss=1.2015673633501382e+21\n",
      "Gradient Descent(14/49): loss=1.0110157152781676e+23\n",
      "Gradient Descent(15/49): loss=8.506840879726829e+24\n",
      "Gradient Descent(16/49): loss=7.157788332758466e+26\n",
      "Gradient Descent(17/49): loss=6.022674952289537e+28\n",
      "Gradient Descent(18/49): loss=5.06757289782641e+30\n",
      "Gradient Descent(19/49): loss=4.263935107009935e+32\n",
      "Gradient Descent(20/49): loss=3.5877417016031314e+34\n",
      "Gradient Descent(21/49): loss=3.0187819928555586e+36\n",
      "Gradient Descent(22/49): loss=2.5400503934509294e+38\n",
      "Gradient Descent(23/49): loss=2.137238136638584e+40\n",
      "Gradient Descent(24/49): loss=1.7983056023175523e+42\n",
      "Gradient Descent(25/49): loss=1.513122465807373e+44\n",
      "Gradient Descent(26/49): loss=1.273164913449979e+46\n",
      "Gradient Descent(27/49): loss=1.0712608750906559e+48\n",
      "Gradient Descent(28/49): loss=9.013756587041719e+49\n",
      "Gradient Descent(29/49): loss=7.584315800160422e+51\n",
      "Gradient Descent(30/49): loss=6.381561960443381e+53\n",
      "Gradient Descent(31/49): loss=5.369546064276141e+55\n",
      "Gradient Descent(32/49): loss=4.518020057644268e+57\n",
      "Gradient Descent(33/49): loss=3.801532754711216e+59\n",
      "Gradient Descent(34/49): loss=3.198669129564957e+61\n",
      "Gradient Descent(35/49): loss=2.69141024439473e+63\n",
      "Gradient Descent(36/49): loss=2.2645946830449637e+65\n",
      "Gradient Descent(37/49): loss=1.9054653927829095e+67\n",
      "Gradient Descent(38/49): loss=1.6032883898726635e+69\n",
      "Gradient Descent(39/49): loss=1.3490319324804096e+71\n",
      "Gradient Descent(40/49): loss=1.1350965717380377e+73\n",
      "Gradient Descent(41/49): loss=9.550880124849142e+74\n",
      "Gradient Descent(42/49): loss=8.036259947430577e+76\n",
      "Gradient Descent(43/49): loss=6.761834836001088e+78\n",
      "Gradient Descent(44/49): loss=5.689513610616392e+80\n",
      "Gradient Descent(45/49): loss=4.787245756587096e+82\n",
      "Gradient Descent(46/49): loss=4.02806346946737e+84\n",
      "Gradient Descent(47/49): loss=3.3892756167222945e+86\n",
      "Gradient Descent(48/49): loss=2.851789524465259e+88\n",
      "Gradient Descent(49/49): loss=2.399540318209561e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8286323264441722\n",
      "Gradient Descent(2/49): loss=26.770699728483773\n",
      "Gradient Descent(3/49): loss=525.5648507471775\n",
      "Gradient Descent(4/49): loss=12768.22006112801\n",
      "Gradient Descent(5/49): loss=488990.7154079713\n",
      "Gradient Descent(6/49): loss=30455811.094159767\n",
      "Gradient Descent(7/49): loss=2388016981.088519\n",
      "Gradient Descent(8/49): loss=199900495101.0013\n",
      "Gradient Descent(9/49): loss=16992826006193.498\n",
      "Gradient Descent(10/49): loss=1449470884569650.5\n",
      "Gradient Descent(11/49): loss=1.2373230798535413e+17\n",
      "Gradient Descent(12/49): loss=1.0564024737773683e+19\n",
      "Gradient Descent(13/49): loss=9.019692158545503e+20\n",
      "Gradient Descent(14/49): loss=7.701184847330778e+22\n",
      "Gradient Descent(15/49): loss=6.575429968358857e+24\n",
      "Gradient Descent(16/49): loss=5.6142395173882115e+26\n",
      "Gradient Descent(17/49): loss=4.793555439727655e+28\n",
      "Gradient Descent(18/49): loss=4.0928382413492347e+30\n",
      "Gradient Descent(19/49): loss=3.4945512054640136e+32\n",
      "Gradient Descent(20/49): loss=2.9837211780219175e+34\n",
      "Gradient Descent(21/49): loss=2.5475637775958725e+36\n",
      "Gradient Descent(22/49): loss=2.1751634331708114e+38\n",
      "Gradient Descent(23/49): loss=1.8572002014862673e+40\n",
      "Gradient Descent(24/49): loss=1.5857165194158357e+42\n",
      "Gradient Descent(25/49): loss=1.3539180525272287e+44\n",
      "Gradient Descent(26/49): loss=1.1560036554545097e+46\n",
      "Gradient Descent(27/49): loss=9.870201885038662e+47\n",
      "Gradient Descent(28/49): loss=8.427385570257478e+49\n",
      "Gradient Descent(29/49): loss=7.195478712288331e+51\n",
      "Gradient Descent(30/49): loss=6.143650776074891e+53\n",
      "Gradient Descent(31/49): loss=5.245578003574253e+55\n",
      "Gradient Descent(32/49): loss=4.4787846175660346e+57\n",
      "Gradient Descent(33/49): loss=3.8240803276355436e+59\n",
      "Gradient Descent(34/49): loss=3.2650800609733144e+61\n",
      "Gradient Descent(35/49): loss=2.7877938984501753e+63\n",
      "Gradient Descent(36/49): loss=2.380276953429144e+65\n",
      "Gradient Descent(37/49): loss=2.0323304309460446e+67\n",
      "Gradient Descent(38/49): loss=1.7352463857615364e+69\n",
      "Gradient Descent(39/49): loss=1.4815897914281482e+71\n",
      "Gradient Descent(40/49): loss=1.2650124662848969e+73\n",
      "Gradient Descent(41/49): loss=1.080094199564918e+75\n",
      "Gradient Descent(42/49): loss=9.222071015315015e+76\n",
      "Gradient Descent(43/49): loss=7.873997827760726e+78\n",
      "Gradient Descent(44/49): loss=6.722984640718841e+80\n",
      "Gradient Descent(45/49): loss=5.74022541890831e+82\n",
      "Gradient Descent(46/49): loss=4.9011249646941355e+84\n",
      "Gradient Descent(47/49): loss=4.184683382018766e+86\n",
      "Gradient Descent(48/49): loss=3.5729705187872456e+88\n",
      "Gradient Descent(49/49): loss=3.050677234740781e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8652663188363954\n",
      "Gradient Descent(2/49): loss=28.05991362626577\n",
      "Gradient Descent(3/49): loss=588.3077663716225\n",
      "Gradient Descent(4/49): loss=17123.58366953677\n",
      "Gradient Descent(5/49): loss=850122.4978883828\n",
      "Gradient Descent(6/49): loss=62176670.05563718\n",
      "Gradient Descent(7/49): loss=5216056364.830935\n",
      "Gradient Descent(8/49): loss=452786713898.50836\n",
      "Gradient Descent(9/49): loss=39606405391226.53\n",
      "Gradient Descent(10/49): loss=3470256159177115.5\n",
      "Gradient Descent(11/49): loss=3.041688738555719e+17\n",
      "Gradient Descent(12/49): loss=2.666257253896046e+19\n",
      "Gradient Descent(13/49): loss=2.3372044481653262e+21\n",
      "Gradient Descent(14/49): loss=2.048768817532149e+23\n",
      "Gradient Descent(15/49): loss=1.7959306103140084e+25\n",
      "Gradient Descent(16/49): loss=1.574295394329073e+27\n",
      "Gradient Descent(17/49): loss=1.3800121602563757e+29\n",
      "Gradient Descent(18/49): loss=1.2097053607140904e+31\n",
      "Gradient Descent(19/49): loss=1.0604160633146146e+33\n",
      "Gradient Descent(20/49): loss=9.295505040159365e+34\n",
      "Gradient Descent(21/49): loss=8.148350156885277e+36\n",
      "Gradient Descent(22/49): loss=7.142765239064109e+38\n",
      "Gradient Descent(23/49): loss=6.261279188826605e+40\n",
      "Gradient Descent(24/49): loss=5.488577010211588e+42\n",
      "Gradient Descent(25/49): loss=4.8112337253364044e+44\n",
      "Gradient Descent(26/49): loss=4.217481127940426e+46\n",
      "Gradient Descent(27/49): loss=3.697003321801845e+48\n",
      "Gradient Descent(28/49): loss=3.240757491685201e+50\n",
      "Gradient Descent(29/49): loss=2.840816792881685e+52\n",
      "Gradient Descent(30/49): loss=2.4902326296936005e+54\n",
      "Gradient Descent(31/49): loss=2.182913930081404e+56\n",
      "Gradient Descent(32/49): loss=1.9135213189820507e+58\n",
      "Gradient Descent(33/49): loss=1.677374351659552e+60\n",
      "Gradient Descent(34/49): loss=1.4703701953537785e+62\n",
      "Gradient Descent(35/49): loss=1.2889123463976192e+64\n",
      "Gradient Descent(36/49): loss=1.1298481443283724e+66\n",
      "Gradient Descent(37/49): loss=9.904139973599481e+67\n",
      "Gradient Descent(38/49): loss=8.681873675595931e+69\n",
      "Gradient Descent(39/49): loss=7.610446815162636e+71\n",
      "Gradient Descent(40/49): loss=6.671244352382745e+73\n",
      "Gradient Descent(41/49): loss=5.847948522618538e+75\n",
      "Gradient Descent(42/49): loss=5.126255330608851e+77\n",
      "Gradient Descent(43/49): loss=4.493626031925037e+79\n",
      "Gradient Descent(44/49): loss=3.939069284010008e+81\n",
      "Gradient Descent(45/49): loss=3.452950181878856e+83\n",
      "Gradient Descent(46/49): loss=3.026822860652952e+85\n",
      "Gradient Descent(47/49): loss=2.653283756554572e+87\n",
      "Gradient Descent(48/49): loss=2.325842976908668e+89\n",
      "Gradient Descent(49/49): loss=2.038811544325769e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8796572597637933\n",
      "Gradient Descent(2/49): loss=28.649576737278608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3/49): loss=597.8294784333215\n",
      "Gradient Descent(4/49): loss=16556.777633841917\n",
      "Gradient Descent(5/49): loss=753473.4632522037\n",
      "Gradient Descent(6/49): loss=51613052.29539729\n",
      "Gradient Descent(7/49): loss=4157904661.0146246\n",
      "Gradient Descent(8/49): loss=349825526234.3548\n",
      "Gradient Descent(9/49): loss=29734657582347.477\n",
      "Gradient Descent(10/49): loss=2533278814495984.0\n",
      "Gradient Descent(11/49): loss=2.1593880281393437e+17\n",
      "Gradient Descent(12/49): loss=1.840897804724995e+19\n",
      "Gradient Descent(13/49): loss=1.5694237583277852e+21\n",
      "Gradient Descent(14/49): loss=1.3379915399164802e+23\n",
      "Gradient Descent(15/49): loss=1.1406885913099184e+25\n",
      "Gradient Descent(16/49): loss=9.724806314857742e+26\n",
      "Gradient Descent(17/49): loss=8.290769720371143e+28\n",
      "Gradient Descent(18/49): loss=7.068198727033782e+30\n",
      "Gradient Descent(19/49): loss=6.025910151073529e+32\n",
      "Gradient Descent(20/49): loss=5.137319220851004e+34\n",
      "Gradient Descent(21/49): loss=4.3797614169318507e+36\n",
      "Gradient Descent(22/49): loss=3.733914371555722e+38\n",
      "Gradient Descent(23/49): loss=3.183305026692923e+40\n",
      "Gradient Descent(24/49): loss=2.71388946950242e+42\n",
      "Gradient Descent(25/49): loss=2.313694726367754e+44\n",
      "Gradient Descent(26/49): loss=1.9725133786690713e+46\n",
      "Gradient Descent(27/49): loss=1.6816432110456772e+48\n",
      "Gradient Descent(28/49): loss=1.4336652515706267e+50\n",
      "Gradient Descent(29/49): loss=1.222254542497759e+52\n",
      "Gradient Descent(30/49): loss=1.0420188150753745e+54\n",
      "Gradient Descent(31/49): loss=8.883609536457227e+55\n",
      "Gradient Descent(32/49): loss=7.573617410212012e+57\n",
      "Gradient Descent(33/49): loss=6.456798944265787e+59\n",
      "Gradient Descent(34/49): loss=5.504668423104797e+61\n",
      "Gradient Descent(35/49): loss=4.692940683128638e+63\n",
      "Gradient Descent(36/49): loss=4.000911692141784e+65\n",
      "Gradient Descent(37/49): loss=3.410930469644458e+67\n",
      "Gradient Descent(38/49): loss=2.907948878651826e+69\n",
      "Gradient Descent(39/49): loss=2.479137805976396e+71\n",
      "Gradient Descent(40/49): loss=2.1135599412156953e+73\n",
      "Gradient Descent(41/49): loss=1.801890808305543e+75\n",
      "Gradient Descent(42/49): loss=1.536180934233826e+77\n",
      "Gradient Descent(43/49): loss=1.309653088758862e+79\n",
      "Gradient Descent(44/49): loss=1.1165294235025074e+81\n",
      "Gradient Descent(45/49): loss=9.518841014060282e+82\n",
      "Gradient Descent(46/49): loss=8.115176576961034e+84\n",
      "Gradient Descent(47/49): loss=6.918498877960163e+86\n",
      "Gradient Descent(48/49): loss=5.898285301668831e+88\n",
      "Gradient Descent(49/49): loss=5.028514149320828e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9107793950499417\n",
      "Gradient Descent(2/49): loss=29.084235871354995\n",
      "Gradient Descent(3/49): loss=598.7974036020562\n",
      "Gradient Descent(4/49): loss=16075.58002030711\n",
      "Gradient Descent(5/49): loss=704085.9762036089\n",
      "Gradient Descent(6/49): loss=47295710.843858436\n",
      "Gradient Descent(7/49): loss=3789403996.622984\n",
      "Gradient Descent(8/49): loss=318482754123.2731\n",
      "Gradient Descent(9/49): loss=27069846442572.312\n",
      "Gradient Descent(10/49): loss=2306713238677446.5\n",
      "Gradient Descent(11/49): loss=1.9667565440140618e+17\n",
      "Gradient Descent(12/49): loss=1.6771176673415485e+19\n",
      "Gradient Descent(13/49): loss=1.4301744479155784e+21\n",
      "Gradient Descent(14/49): loss=1.2195997053508628e+23\n",
      "Gradient Descent(15/49): loss=1.0400308906732865e+25\n",
      "Gradient Descent(16/49): loss=8.869013347548491e+26\n",
      "Gradient Descent(17/49): loss=7.5631795385450754e+28\n",
      "Gradient Descent(18/49): loss=6.44961095786864e+30\n",
      "Gradient Descent(19/49): loss=5.499999233014415e+32\n",
      "Gradient Descent(20/49): loss=4.690204073667823e+34\n",
      "Gradient Descent(21/49): loss=3.9996395135266696e+36\n",
      "Gradient Descent(22/49): loss=3.4107505745094435e+38\n",
      "Gradient Descent(23/49): loss=2.9085669951710207e+40\n",
      "Gradient Descent(24/49): loss=2.480322668165925e+42\n",
      "Gradient Descent(25/49): loss=2.1151311104178037e+44\n",
      "Gradient Descent(26/49): loss=1.8037087156752324e+46\n",
      "Gradient Descent(27/49): loss=1.5381387541315077e+48\n",
      "Gradient Descent(28/49): loss=1.3116701196820417e+50\n",
      "Gradient Descent(29/49): loss=1.1185457087310496e+52\n",
      "Gradient Descent(30/49): loss=9.538560677313362e+53\n",
      "Gradient Descent(31/49): loss=8.134145890024243e+55\n",
      "Gradient Descent(32/49): loss=6.936510821550339e+57\n",
      "Gradient Descent(33/49): loss=5.915210155806801e+59\n",
      "Gradient Descent(34/49): loss=5.044281208162222e+61\n",
      "Gradient Descent(35/49): loss=4.301583922938064e+63\n",
      "Gradient Descent(36/49): loss=3.6682380467088396e+65\n",
      "Gradient Descent(37/49): loss=3.1281431696749975e+67\n",
      "Gradient Descent(38/49): loss=2.6675694339857826e+69\n",
      "Gradient Descent(39/49): loss=2.2748085043290036e+71\n",
      "Gradient Descent(40/49): loss=1.9398759280411487e+73\n",
      "Gradient Descent(41/49): loss=1.654257318377448e+75\n",
      "Gradient Descent(42/49): loss=1.410691908615338e+77\n",
      "Gradient Descent(43/49): loss=1.202987974679003e+79\n",
      "Gradient Descent(44/49): loss=1.0258654341065742e+81\n",
      "Gradient Descent(45/49): loss=8.748216200378025e+82\n",
      "Gradient Descent(46/49): loss=7.460168180362496e+84\n",
      "Gradient Descent(47/49): loss=6.361766559551536e+86\n",
      "Gradient Descent(48/49): loss=5.4250886548059776e+88\n",
      "Gradient Descent(49/49): loss=4.626322993306907e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8506944732266706\n",
      "Gradient Descent(2/49): loss=27.56818991603424\n",
      "Gradient Descent(3/49): loss=549.3237662747301\n",
      "Gradient Descent(4/49): loss=13526.988486418712\n",
      "Gradient Descent(5/49): loss=524185.8313642866\n",
      "Gradient Descent(6/49): loss=33033912.788261224\n",
      "Gradient Descent(7/49): loss=2623318715.3816986\n",
      "Gradient Descent(8/49): loss=222514283086.61472\n",
      "Gradient Descent(9/49): loss=19169120817292.613\n",
      "Gradient Descent(10/49): loss=1657123939455138.0\n",
      "Gradient Descent(11/49): loss=1.4336450444948677e+17\n",
      "Gradient Descent(12/49): loss=1.2405148827383185e+19\n",
      "Gradient Descent(13/49): loss=1.0734419865171321e+21\n",
      "Gradient Descent(14/49): loss=9.288781858854956e+22\n",
      "Gradient Descent(15/49): loss=8.037847100306219e+24\n",
      "Gradient Descent(16/49): loss=6.955380473830673e+26\n",
      "Gradient Descent(17/49): loss=6.018691475420208e+28\n",
      "Gradient Descent(18/49): loss=5.208147551491597e+30\n",
      "Gradient Descent(19/49): loss=4.506760505311351e+32\n",
      "Gradient Descent(20/49): loss=3.8998300395188507e+34\n",
      "Gradient Descent(21/49): loss=3.374635577462005e+36\n",
      "Gradient Descent(22/49): loss=2.9201696396504365e+38\n",
      "Gradient Descent(23/49): loss=2.526907136691857e+40\n",
      "Gradient Descent(24/49): loss=2.1866057337145002e+42\n",
      "Gradient Descent(25/49): loss=1.8921331002986824e+44\n",
      "Gradient Descent(26/49): loss=1.6373174249225621e+46\n",
      "Gradient Descent(27/49): loss=1.4168180608076018e+48\n",
      "Gradient Descent(28/49): loss=1.2260135920348627e+50\n",
      "Gradient Descent(29/49): loss=1.060904973922694e+52\n",
      "Gradient Descent(30/49): loss=9.18031717597705e+53\n",
      "Gradient Descent(31/49): loss=7.943993620834745e+55\n",
      "Gradient Descent(32/49): loss=6.874167138037739e+57\n",
      "Gradient Descent(33/49): loss=5.948415381118133e+59\n",
      "Gradient Descent(34/49): loss=5.147335646020298e+61\n",
      "Gradient Descent(35/49): loss=4.454138212488192e+63\n",
      "Gradient Descent(36/49): loss=3.8542944506225254e+65\n",
      "Gradient Descent(37/49): loss=3.3352323173197607e+67\n",
      "Gradient Descent(38/49): loss=2.8860728605458776e+69\n",
      "Gradient Descent(39/49): loss=2.4974022088730863e+71\n",
      "Gradient Descent(40/49): loss=2.1610742674405955e+73\n",
      "Gradient Descent(41/49): loss=1.8700399850696125e+75\n",
      "Gradient Descent(42/49): loss=1.618199614167276e+77\n",
      "Gradient Descent(43/49): loss=1.4002748669535204e+79\n",
      "Gradient Descent(44/49): loss=1.2116982885517844e+81\n",
      "Gradient Descent(45/49): loss=1.0485175283290286e+83\n",
      "Gradient Descent(46/49): loss=9.073125031208887e+84\n",
      "Gradient Descent(47/49): loss=7.851237161780195e+86\n",
      "Gradient Descent(48/49): loss=6.793902294797914e+88\n",
      "Gradient Descent(49/49): loss=5.87895989385645e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8877202222981402\n",
      "Gradient Descent(2/49): loss=28.889180275313254\n",
      "Gradient Descent(3/49): loss=614.5420268284282\n",
      "Gradient Descent(4/49): loss=18117.51058475243\n",
      "Gradient Descent(5/49): loss=909951.6739689138\n",
      "Gradient Descent(6/49): loss=67370675.20699531\n",
      "Gradient Descent(7/49): loss=5725359082.600348\n",
      "Gradient Descent(8/49): loss=503598552961.8265\n",
      "Gradient Descent(9/49): loss=44639344440959.63\n",
      "Gradient Descent(10/49): loss=3963541754552371.0\n",
      "Gradient Descent(11/49): loss=3.520530876274591e+17\n",
      "Gradient Descent(12/49): loss=3.1272845320649257e+19\n",
      "Gradient Descent(13/49): loss=2.778012041864886e+21\n",
      "Gradient Descent(14/49): loss=2.467757470598113e+23\n",
      "Gradient Descent(15/49): loss=2.192154599039861e+25\n",
      "Gradient Descent(16/49): loss=1.9473318139644353e+27\n",
      "Gradient Descent(17/49): loss=1.7298512340623657e+29\n",
      "Gradient Descent(18/49): loss=1.5366591842747129e+31\n",
      "Gradient Descent(19/49): loss=1.3650430779012383e+33\n",
      "Gradient Descent(20/49): loss=1.2125932830911538e+35\n",
      "Gradient Descent(21/49): loss=1.0771692806814e+37\n",
      "Gradient Descent(22/49): loss=9.568696078433698e+38\n",
      "Gradient Descent(23/49): loss=8.500051596706983e+40\n",
      "Gradient Descent(24/49): loss=7.550754727134221e+42\n",
      "Gradient Descent(25/49): loss=6.707476572429041e+44\n",
      "Gradient Descent(26/49): loss=5.958376823976211e+46\n",
      "Gradient Descent(27/49): loss=5.29293751429989e+48\n",
      "Gradient Descent(28/49): loss=4.701815336276062e+50\n",
      "Gradient Descent(29/49): loss=4.1767104555295215e+52\n",
      "Gradient Descent(30/49): loss=3.710249974033693e+54\n",
      "Gradient Descent(31/49): loss=3.295884408648122e+56\n",
      "Gradient Descent(32/49): loss=2.9277957310676785e+58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=2.6008156779909413e+60\n",
      "Gradient Descent(34/49): loss=2.3103531845156615e+62\n",
      "Gradient Descent(35/49): loss=2.0523299218670268e+64\n",
      "Gradient Descent(36/49): loss=1.8231230343570276e+66\n",
      "Gradient Descent(37/49): loss=1.619514271555114e+68\n",
      "Gradient Descent(38/49): loss=1.4386448014440706e+70\n",
      "Gradient Descent(39/49): loss=1.2779750701021545e+72\n",
      "Gradient Descent(40/49): loss=1.1352491443080477e+74\n",
      "Gradient Descent(41/49): loss=1.0084630364104889e+76\n",
      "Gradient Descent(42/49): loss=8.958365667178267e+77\n",
      "Gradient Descent(43/49): loss=7.957883683325502e+79\n",
      "Gradient Descent(44/49): loss=7.069136834786485e+81\n",
      "Gradient Descent(45/49): loss=6.27964639564228e+83\n",
      "Gradient Descent(46/49): loss=5.578327280390613e+85\n",
      "Gradient Descent(47/49): loss=4.955332400363145e+87\n",
      "Gradient Descent(48/49): loss=4.4019144026217514e+89\n",
      "Gradient Descent(49/49): loss=3.9103028500345985e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9023150253680754\n",
      "Gradient Descent(2/49): loss=29.49579113028706\n",
      "Gradient Descent(3/49): loss=624.5170909446606\n",
      "Gradient Descent(4/49): loss=17522.591832561022\n",
      "Gradient Descent(5/49): loss=806789.0032452553\n",
      "Gradient Descent(6/49): loss=55937834.86411236\n",
      "Gradient Descent(7/49): loss=4564655055.353013\n",
      "Gradient Descent(8/49): loss=389143654433.0428\n",
      "Gradient Descent(9/49): loss=33518674404215.8\n",
      "Gradient Descent(10/49): loss=2893897048663015.5\n",
      "Gradient Descent(11/49): loss=2.4998257170328035e+17\n",
      "Gradient Descent(12/49): loss=2.1596748775527006e+19\n",
      "Gradient Descent(13/49): loss=1.8658586814176336e+21\n",
      "Gradient Descent(14/49): loss=1.612024965186307e+23\n",
      "Gradient Descent(15/49): loss=1.3927250134529093e+25\n",
      "Gradient Descent(16/49): loss=1.2032590100947945e+27\n",
      "Gradient Descent(17/49): loss=1.0395679926183784e+29\n",
      "Gradient Descent(18/49): loss=8.981454692913384e+30\n",
      "Gradient Descent(19/49): loss=7.75962027113357e+32\n",
      "Gradient Descent(20/49): loss=6.704003845606159e+34\n",
      "Gradient Descent(21/49): loss=5.791993164564646e+36\n",
      "Gradient Descent(22/49): loss=5.0040521444089824e+38\n",
      "Gradient Descent(23/49): loss=4.323302385332957e+40\n",
      "Gradient Descent(24/49): loss=3.73516162015757e+42\n",
      "Gradient Descent(25/49): loss=3.227031349006691e+44\n",
      "Gradient Descent(26/49): loss=2.7880269681699954e+46\n",
      "Gradient Descent(27/49): loss=2.4087446121761797e+48\n",
      "Gradient Descent(28/49): loss=2.0810597146039014e+50\n",
      "Gradient Descent(29/49): loss=1.797952972621144e+52\n",
      "Gradient Descent(30/49): loss=1.5533599872565345e+54\n",
      "Gradient Descent(31/49): loss=1.3420413585634071e+56\n",
      "Gradient Descent(32/49): loss=1.1594704529988886e+58\n",
      "Gradient Descent(33/49): loss=1.0017364388952494e+60\n",
      "Gradient Descent(34/49): loss=8.65460512956684e+61\n",
      "Gradient Descent(35/49): loss=7.477235232785366e+63\n",
      "Gradient Descent(36/49): loss=6.460034385093316e+65\n",
      "Gradient Descent(37/49): loss=5.581213236893509e+67\n",
      "Gradient Descent(38/49): loss=4.821946655199724e+69\n",
      "Gradient Descent(39/49): loss=4.1659704724940865e+71\n",
      "Gradient Descent(40/49): loss=3.599233093750134e+73\n",
      "Gradient Descent(41/49): loss=3.1095944987317112e+75\n",
      "Gradient Descent(42/49): loss=2.6865661919293635e+77\n",
      "Gradient Descent(43/49): loss=2.321086529629999e+79\n",
      "Gradient Descent(44/49): loss=2.0053266114246606e+81\n",
      "Gradient Descent(45/49): loss=1.7325225781776324e+83\n",
      "Gradient Descent(46/49): loss=1.4968307241296715e+85\n",
      "Gradient Descent(47/49): loss=1.2932023195075926e+87\n",
      "Gradient Descent(48/49): loss=1.1172754622285289e+89\n",
      "Gradient Descent(49/49): loss=9.652816420660679e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9339301018311896\n",
      "Gradient Descent(2/49): loss=29.94404467620536\n",
      "Gradient Descent(3/49): loss=625.5685715086163\n",
      "Gradient Descent(4/49): loss=17016.003352248827\n",
      "Gradient Descent(5/49): loss=754020.0793514512\n",
      "Gradient Descent(6/49): loss=51261839.40808541\n",
      "Gradient Descent(7/49): loss=4160173394.5748286\n",
      "Gradient Descent(8/49): loss=354279247502.17316\n",
      "Gradient Descent(9/49): loss=30514726817379.793\n",
      "Gradient Descent(10/49): loss=2635075485264601.0\n",
      "Gradient Descent(11/49): loss=2.276820807740953e+17\n",
      "Gradient Descent(12/49): loss=1.9675298997003887e+19\n",
      "Gradient Descent(13/49): loss=1.7003039749371241e+21\n",
      "Gradient Descent(14/49): loss=1.4693818185705943e+23\n",
      "Gradient Descent(15/49): loss=1.269823606730718e+25\n",
      "Gradient Descent(16/49): loss=1.0973679599826271e+27\n",
      "Gradient Descent(17/49): loss=9.48333708105829e+28\n",
      "Gradient Descent(18/49): loss=8.195399080611791e+30\n",
      "Gradient Descent(19/49): loss=7.082376780434544e+32\n",
      "Gradient Descent(20/49): loss=6.120514743598712e+34\n",
      "Gradient Descent(21/49): loss=5.289283794845906e+36\n",
      "Gradient Descent(22/49): loss=4.5709428431451546e+38\n",
      "Gradient Descent(23/49): loss=3.9501602269592853e+40\n",
      "Gradient Descent(24/49): loss=3.4136864874792096e+42\n",
      "Gradient Descent(25/49): loss=2.9500715832416477e+44\n",
      "Gradient Descent(26/49): loss=2.5494205101059097e+46\n",
      "Gradient Descent(27/49): loss=2.2031821106546523e+48\n",
      "Gradient Descent(28/49): loss=1.9039665655263718e+50\n",
      "Gradient Descent(29/49): loss=1.645387671364638e+52\n",
      "Gradient Descent(30/49): loss=1.421926539098799e+54\n",
      "Gradient Descent(31/49): loss=1.2288138034464253e+56\n",
      "Gradient Descent(32/49): loss=1.0619278296173303e+58\n",
      "Gradient Descent(33/49): loss=9.177067446288376e+59\n",
      "Gradient Descent(34/49): loss=7.930724157033686e+61\n",
      "Gradient Descent(35/49): loss=6.853647532076978e+63\n",
      "Gradient Descent(36/49): loss=5.922849359511005e+65\n",
      "Gradient Descent(37/49): loss=5.11846347091442e+67\n",
      "Gradient Descent(38/49): loss=4.423321734667439e+69\n",
      "Gradient Descent(39/49): loss=3.8225876338793664e+71\n",
      "Gradient Descent(40/49): loss=3.303439608330123e+73\n",
      "Gradient Descent(41/49): loss=2.854797401939322e+75\n",
      "Gradient Descent(42/49): loss=2.4670855751588195e+77\n",
      "Gradient Descent(43/49): loss=2.1320291348948064e+79\n",
      "Gradient Descent(44/49): loss=1.8424769200589e+81\n",
      "Gradient Descent(45/49): loss=1.5922489732379412e+83\n",
      "Gradient Descent(46/49): loss=1.3760046409136398e+85\n",
      "Gradient Descent(47/49): loss=1.189128587073614e+87\n",
      "Gradient Descent(48/49): loss=1.0276322873859223e+89\n",
      "Gradient Descent(49/49): loss=8.880689015111575e+90\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.8729254131375124\n",
      "Gradient Descent(2/49): loss=28.383628606309678\n",
      "Gradient Descent(3/49): loss=573.9724975053696\n",
      "Gradient Descent(4/49): loss=14325.012467717315\n",
      "Gradient Descent(5/49): loss=561650.7248909797\n",
      "Gradient Descent(6/49): loss=35810912.210167624\n",
      "Gradient Descent(7/49): loss=2880007386.131331\n",
      "Gradient Descent(8/49): loss=247509881505.65372\n",
      "Gradient Descent(9/49): loss=21606817565242.63\n",
      "Gradient Descent(10/49): loss=1892839793457686.8\n",
      "Gradient Descent(11/49): loss=1.6594908791488173e+17\n",
      "Gradient Descent(12/49): loss=1.4551594797069167e+19\n",
      "Gradient Descent(13/49): loss=1.276035692461285e+21\n",
      "Gradient Descent(14/49): loss=1.1189706574330715e+23\n",
      "Gradient Descent(15/49): loss=9.812403069098197e+24\n",
      "Gradient Descent(16/49): loss=8.604630764826256e+26\n",
      "Gradient Descent(17/49): loss=7.54551935735089e+28\n",
      "Gradient Descent(18/49): loss=6.61677009351503e+30\n",
      "Gradient Descent(19/49): loss=5.8023370679103674e+32\n",
      "Gradient Descent(20/49): loss=5.088149505978572e+34\n",
      "Gradient Descent(21/49): loss=4.4618685708468106e+36\n",
      "Gradient Descent(22/49): loss=3.912674169866037e+38\n",
      "Gradient Descent(23/49): loss=3.431078015099964e+40\n",
      "Gradient Descent(24/49): loss=3.008759695963202e+42\n",
      "Gradient Descent(25/49): loss=2.6384229295333292e+44\n",
      "Gradient Descent(26/49): loss=2.3136695045562317e+46\n",
      "Gradient Descent(27/49): loss=2.0288887412225447e+48\n",
      "Gradient Descent(28/49): loss=1.7791605569219993e+50\n",
      "Gradient Descent(29/49): loss=1.5601704632653347e+52\n",
      "Gradient Descent(30/49): loss=1.368135025799313e+54\n",
      "Gradient Descent(31/49): loss=1.199736498601104e+56\n",
      "Gradient Descent(32/49): loss=1.0520655044517463e+58\n",
      "Gradient Descent(33/49): loss=9.22570770288212e+59\n",
      "Gradient Descent(34/49): loss=8.090150495274969e+61\n",
      "Gradient Descent(35/49): loss=7.094364697436654e+63\n",
      "Gradient Descent(36/49): loss=6.221146379122491e+65\n",
      "Gradient Descent(37/49): loss=5.455409176307657e+67\n",
      "Gradient Descent(38/49): loss=4.783923648030155e+69\n",
      "Gradient Descent(39/49): loss=4.195088714806888e+71\n",
      "Gradient Descent(40/49): loss=3.6787312298235834e+73\n",
      "Gradient Descent(41/49): loss=3.2259302201438103e+75\n",
      "Gradient Descent(42/49): loss=2.8288627613971767e+77\n",
      "Gradient Descent(43/49): loss=2.480668823166045e+79\n",
      "Gradient Descent(44/49): loss=2.175332750037146e+81\n",
      "Gradient Descent(45/49): loss=1.9075793307002975e+83\n",
      "Gradient Descent(46/49): loss=1.672782659504793e+85\n",
      "Gradient Descent(47/49): loss=1.4668862158999483e+87\n",
      "Gradient Descent(48/49): loss=1.2863327809928007e+89\n",
      "Gradient Descent(49/49): loss=1.1280029804094159e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9103445378265338\n",
      "Gradient Descent(2/49): loss=29.736953818363546\n",
      "Gradient Descent(3/49): loss=641.7460223511824\n",
      "Gradient Descent(4/49): loss=19161.698727997904\n",
      "Gradient Descent(5/49): loss=973551.8406316987\n",
      "Gradient Descent(6/49): loss=72959947.33704975\n",
      "Gradient Descent(7/49): loss=6280529122.49289\n",
      "Gradient Descent(8/49): loss=559719453122.4484\n",
      "Gradient Descent(9/49): loss=50272124811610.65\n",
      "Gradient Descent(10/49): loss=4522976113299766.0\n",
      "Gradient Descent(11/49): loss=4.07082462423659e+17\n",
      "Gradient Descent(12/49): loss=3.664168915189082e+19\n",
      "Gradient Descent(13/49): loss=3.2981938787118445e+21\n",
      "Gradient Descent(14/49): loss=2.9687835019851977e+23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=2.672275519694019e+25\n",
      "Gradient Descent(16/49): loss=2.4053817765997778e+27\n",
      "Gradient Descent(17/49): loss=2.1651441530047926e+29\n",
      "Gradient Descent(18/49): loss=1.948900290415509e+31\n",
      "Gradient Descent(19/49): loss=1.754253795847646e+33\n",
      "Gradient Descent(20/49): loss=1.5790476283519592e+35\n",
      "Gradient Descent(21/49): loss=1.421340183913062e+37\n",
      "Gradient Descent(22/49): loss=1.2793837767595402e+39\n",
      "Gradient Descent(23/49): loss=1.1516052713966492e+41\n",
      "Gradient Descent(24/49): loss=1.0365886493173722e+43\n",
      "Gradient Descent(25/49): loss=9.330593169225879e+44\n",
      "Gradient Descent(26/49): loss=8.398699807000678e+46\n",
      "Gradient Descent(27/49): loss=7.559879331226655e+48\n",
      "Gradient Descent(28/49): loss=6.8048360836838765e+50\n",
      "Gradient Descent(29/49): loss=6.125202810385659e+52\n",
      "Gradient Descent(30/49): loss=5.513447937168476e+54\n",
      "Gradient Descent(31/49): loss=4.962792106136631e+56\n",
      "Gradient Descent(32/49): loss=4.4671330480325204e+58\n",
      "Gradient Descent(33/49): loss=4.020977957982363e+60\n",
      "Gradient Descent(34/49): loss=3.619382625216682e+62\n",
      "Gradient Descent(35/49): loss=3.257896642212226e+64\n",
      "Gradient Descent(36/49): loss=2.9325140860735964e+66\n",
      "Gradient Descent(37/49): loss=2.639629125612928e+68\n",
      "Gradient Descent(38/49): loss=2.375996062175211e+70\n",
      "Gradient Descent(39/49): loss=2.138693361386997e+72\n",
      "Gradient Descent(40/49): loss=1.925091277236074e+74\n",
      "Gradient Descent(41/49): loss=1.732822709697369e+76\n",
      "Gradient Descent(42/49): loss=1.5597569729545667e+78\n",
      "Gradient Descent(43/49): loss=1.403976183521504e+80\n",
      "Gradient Descent(44/49): loss=1.2637540066013099e+82\n",
      "Gradient Descent(45/49): loss=1.1375365251531764e+84\n",
      "Gradient Descent(46/49): loss=1.0239250196622807e+86\n",
      "Gradient Descent(47/49): loss=9.216604677808064e+87\n",
      "Gradient Descent(48/49): loss=8.29609592067699e+89\n",
      "Gradient Descent(49/49): loss=7.467523012112296e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9251446062692736\n",
      "Gradient Descent(2/49): loss=30.360873038250354\n",
      "Gradient Descent(3/49): loss=652.1920301694955\n",
      "Gradient Descent(4/49): loss=18537.475641470017\n",
      "Gradient Descent(5/49): loss=863483.6271023997\n",
      "Gradient Descent(6/49): loss=60592784.74306138\n",
      "Gradient Descent(7/49): loss=5008107488.975309\n",
      "Gradient Descent(8/49): loss=432576279965.8439\n",
      "Gradient Descent(9/49): loss=37754337811871.46\n",
      "Gradient Descent(10/49): loss=3302943090057700.0\n",
      "Gradient Descent(11/49): loss=2.891135824032383e+17\n",
      "Gradient Descent(12/49): loss=2.530979261670709e+19\n",
      "Gradient Descent(13/49): loss=2.2157490852943217e+21\n",
      "Gradient Descent(14/49): loss=1.9397924222483822e+23\n",
      "Gradient Descent(15/49): loss=1.6982066812373895e+25\n",
      "Gradient Descent(16/49): loss=1.4867089949657218e+27\n",
      "Gradient Descent(17/49): loss=1.301551700215631e+29\n",
      "Gradient Descent(18/49): loss=1.1394542316920611e+31\n",
      "Gradient Descent(19/49): loss=9.975446619806272e+32\n",
      "Gradient Descent(20/49): loss=8.73308752371281e+34\n",
      "Gradient Descent(21/49): loss=7.645453944843394e+36\n",
      "Gradient Descent(22/49): loss=6.693276102690362e+38\n",
      "Gradient Descent(23/49): loss=5.859684109075949e+40\n",
      "Gradient Descent(24/49): loss=5.129909080611317e+42\n",
      "Gradient Descent(25/49): loss=4.491021475814922e+44\n",
      "Gradient Descent(26/49): loss=3.9317020203073877e+46\n",
      "Gradient Descent(27/49): loss=3.442041161400689e+48\n",
      "Gradient Descent(28/49): loss=3.013363498958773e+50\n",
      "Gradient Descent(29/49): loss=2.638074081938653e+52\n",
      "Gradient Descent(30/49): loss=2.3095238474220357e+54\n",
      "Gradient Descent(31/49): loss=2.0218918180991374e+56\n",
      "Gradient Descent(32/49): loss=1.7700819710778965e+58\n",
      "Gradient Descent(33/49): loss=1.5496329508275139e+60\n",
      "Gradient Descent(34/49): loss=1.35663902662548e+62\n",
      "Gradient Descent(35/49): loss=1.1876808940984783e+64\n",
      "Gradient Descent(36/49): loss=1.0397650948574531e+66\n",
      "Gradient Descent(37/49): loss=9.102709809140924e+67\n",
      "Gradient Descent(38/49): loss=7.969042842392052e+69\n",
      "Gradient Descent(39/49): loss=6.976564688474295e+71\n",
      "Gradient Descent(40/49): loss=6.107691452422363e+73\n",
      "Gradient Descent(41/49): loss=5.3470291674672576e+75\n",
      "Gradient Descent(42/49): loss=4.681101057651848e+77\n",
      "Gradient Descent(43/49): loss=4.098108767625996e+79\n",
      "Gradient Descent(44/49): loss=3.5877233292881236e+81\n",
      "Gradient Descent(45/49): loss=3.1409021617976274e+83\n",
      "Gradient Descent(46/49): loss=2.7497288627165055e+85\n",
      "Gradient Descent(47/49): loss=2.407272951835195e+87\n",
      "Gradient Descent(48/49): loss=2.107467082740787e+89\n",
      "Gradient Descent(49/49): loss=1.8449995466654834e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9572562820941948\n",
      "Gradient Descent(2/49): loss=30.82303577704763\n",
      "Gradient Descent(3/49): loss=653.3315536063209\n",
      "Gradient Descent(4/49): loss=18004.33903005179\n",
      "Gradient Descent(5/49): loss=807126.4637130703\n",
      "Gradient Descent(6/49): loss=55531013.464441106\n",
      "Gradient Descent(7/49): loss=4564404999.508968\n",
      "Gradient Descent(8/49): loss=393821844547.153\n",
      "Gradient Descent(9/49): loss=34370774485015.44\n",
      "Gradient Descent(10/49): loss=3007533346768759.0\n",
      "Gradient Descent(11/49): loss=2.6332181004944298e+17\n",
      "Gradient Descent(12/49): loss=2.3057948949337625e+19\n",
      "Gradient Descent(13/49): loss=2.0191447071658517e+21\n",
      "Gradient Descent(14/49): loss=1.768141940102754e+23\n",
      "Gradient Descent(15/49): loss=1.5483440197157247e+25\n",
      "Gradient Descent(16/49): loss=1.3558696629339972e+27\n",
      "Gradient Descent(17/49): loss=1.1873218481918045e+29\n",
      "Gradient Descent(18/49): loss=1.0397261875027588e+31\n",
      "Gradient Descent(19/49): loss=9.104781073476513e+32\n",
      "Gradient Descent(20/49): loss=7.972968216476148e+34\n",
      "Gradient Descent(21/49): loss=6.981850709024505e+36\n",
      "Gradient Descent(22/49): loss=6.113938749249971e+38\n",
      "Gradient Descent(23/49): loss=5.353916688827699e+40\n",
      "Gradient Descent(24/49): loss=4.6883727637095527e+42\n",
      "Gradient Descent(25/49): loss=4.105562422622884e+44\n",
      "Gradient Descent(26/49): loss=3.5952010762723623e+46\n",
      "Gradient Descent(27/49): loss=3.148282609857996e+48\n",
      "Gradient Descent(28/49): loss=2.75692045625754e+50\n",
      "Gradient Descent(29/49): loss=2.4142084253593204e+52\n",
      "Gradient Descent(30/49): loss=2.114098833663113e+54\n",
      "Gradient Descent(31/49): loss=1.8512957835570115e+56\n",
      "Gradient Descent(32/49): loss=1.6211617090188648e+58\n",
      "Gradient Descent(33/49): loss=1.4196355385951536e+60\n",
      "Gradient Descent(34/49): loss=1.2431610315186414e+62\n",
      "Gradient Descent(35/49): loss=1.0886240223429665e+64\n",
      "Gradient Descent(36/49): loss=9.532974666801395e+65\n",
      "Gradient Descent(37/49): loss=8.347933182871146e+67\n",
      "Gradient Descent(38/49): loss=7.310203882988461e+69\n",
      "Gradient Descent(39/49): loss=6.401474429683717e+71\n",
      "Gradient Descent(40/49): loss=5.6057088871703606e+73\n",
      "Gradient Descent(41/49): loss=4.908864742470472e+75\n",
      "Gradient Descent(42/49): loss=4.2986451035693085e+77\n",
      "Gradient Descent(43/49): loss=3.7642817017486026e+79\n",
      "Gradient Descent(44/49): loss=3.2963448688409943e+81\n",
      "Gradient Descent(45/49): loss=2.886577136160324e+83\n",
      "Gradient Descent(46/49): loss=2.5277475187034355e+85\n",
      "Gradient Descent(47/49): loss=2.2135239132430245e+87\n",
      "Gradient Descent(48/49): loss=1.938361358579035e+89\n",
      "Gradient Descent(49/49): loss=1.6974041861276108e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.895325146176698\n",
      "Gradient Descent(2/49): loss=29.21728333616471\n",
      "Gradient Descent(3/49): loss=599.5375304692777\n",
      "Gradient Descent(4/49): loss=15164.02379878116\n",
      "Gradient Descent(5/49): loss=601514.9450918322\n",
      "Gradient Descent(6/49): loss=38800680.39611314\n",
      "Gradient Descent(7/49): loss=3159865630.356281\n",
      "Gradient Descent(8/49): loss=275119834159.38226\n",
      "Gradient Descent(9/49): loss=24335262670438.55\n",
      "Gradient Descent(10/49): loss=2160186276218353.0\n",
      "Gradient Descent(11/49): loss=1.9190594476689306e+17\n",
      "Gradient Descent(12/49): loss=1.7051452545134967e+19\n",
      "Gradient Descent(13/49): loss=1.5151341534861743e+21\n",
      "Gradient Descent(14/49): loss=1.3463082336953368e+23\n",
      "Gradient Descent(15/49): loss=1.196296234314681e+25\n",
      "Gradient Descent(16/49): loss=1.0629997218730138e+27\n",
      "Gradient Descent(17/49): loss=9.445557719688452e+28\n",
      "Gradient Descent(18/49): loss=8.39309367855229e+30\n",
      "Gradient Descent(19/49): loss=7.457899671571557e+32\n",
      "Gradient Descent(20/49): loss=6.626908944075337e+34\n",
      "Gradient Descent(21/49): loss=5.888510719738538e+36\n",
      "Gradient Descent(22/49): loss=5.232387949010907e+38\n",
      "Gradient Descent(23/49): loss=4.649373152617255e+40\n",
      "Gradient Descent(24/49): loss=4.131320330790111e+42\n",
      "Gradient Descent(25/49): loss=3.670991145548392e+44\n",
      "Gradient Descent(26/49): loss=3.261953785152107e+46\n",
      "Gradient Descent(27/49): loss=2.8984930975307552e+48\n",
      "Gradient Descent(28/49): loss=2.575530737030883e+50\n",
      "Gradient Descent(29/49): loss=2.288554208751348e+52\n",
      "Gradient Descent(30/49): loss=2.033553819059196e+54\n",
      "Gradient Descent(31/49): loss=1.8069666513455768e+56\n",
      "Gradient Descent(32/49): loss=1.6056267842400244e+58\n",
      "Gradient Descent(33/49): loss=1.426721056721884e+60\n",
      "Gradient Descent(34/49): loss=1.26774976207007e+62\n",
      "Gradient Descent(35/49): loss=1.1264917214592051e+64\n",
      "Gradient Descent(36/49): loss=1.0009732492033985e+66\n",
      "Gradient Descent(37/49): loss=8.894405760238879e+67\n",
      "Gradient Descent(38/49): loss=7.903353450327403e+69\n",
      "Gradient Descent(39/49): loss=7.022728380577462e+71\n",
      "Gradient Descent(40/49): loss=6.240226280823143e+73\n",
      "Gradient Descent(41/49): loss=5.544913874723318e+75\n",
      "Gradient Descent(42/49): loss=4.92707611783001e+77\n",
      "Gradient Descent(43/49): loss=4.3780804570389944e+79\n",
      "Gradient Descent(44/49): loss=3.890256214825593e+81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=3.4567874129989126e+83\n",
      "Gradient Descent(46/49): loss=3.0716175384874215e+85\n",
      "Gradient Descent(47/49): loss=2.729364920522653e+87\n",
      "Gradient Descent(48/49): loss=2.425247536855726e+89\n",
      "Gradient Descent(49/49): loss=2.1550161983830986e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9331392654215758\n",
      "Gradient Descent(2/49): loss=30.60350784543455\n",
      "Gradient Descent(3/49): loss=669.9482592211061\n",
      "Gradient Descent(4/49): loss=20258.317014290104\n",
      "Gradient Descent(5/49): loss=1041133.2628263118\n",
      "Gradient Descent(6/49): loss=78971665.26522242\n",
      "Gradient Descent(7/49): loss=6885354167.73101\n",
      "Gradient Descent(8/49): loss=621663623216.9414\n",
      "Gradient Descent(9/49): loss=56571569381505.29\n",
      "Gradient Descent(10/49): loss=5156904644765963.0\n",
      "Gradient Descent(11/49): loss=4.702653426790813e+17\n",
      "Gradient Descent(12/49): loss=4.288765647032248e+19\n",
      "Gradient Descent(13/49): loss=3.9113742629691117e+21\n",
      "Gradient Descent(14/49): loss=3.5672053485749254e+23\n",
      "Gradient Descent(15/49): loss=3.253323216317372e+25\n",
      "Gradient Descent(16/49): loss=2.967060444044855e+27\n",
      "Gradient Descent(17/49): loss=2.7059862926297988e+29\n",
      "Gradient Descent(18/49): loss=2.4678842973913456e+31\n",
      "Gradient Descent(19/49): loss=2.2507330999290547e+33\n",
      "Gradient Descent(20/49): loss=2.0526892182690635e+35\n",
      "Gradient Descent(21/49): loss=1.8720713830112431e+37\n",
      "Gradient Descent(22/49): loss=1.7073462616580972e+39\n",
      "Gradient Descent(23/49): loss=1.557115441036942e+41\n",
      "Gradient Descent(24/49): loss=1.420103555539571e+43\n",
      "Gradient Descent(25/49): loss=1.295147460045471e+45\n",
      "Gradient Descent(26/49): loss=1.1811863555435728e+47\n",
      "Gradient Descent(27/49): loss=1.0772527836122437e+49\n",
      "Gradient Descent(28/49): loss=9.824644133027486e+50\n",
      "Gradient Descent(29/49): loss=8.960165507019663e+52\n",
      "Gradient Descent(30/49): loss=8.171753075848566e+54\n",
      "Gradient Descent(31/49): loss=7.452713711630141e+56\n",
      "Gradient Descent(32/49): loss=6.796943220381289e+58\n",
      "Gradient Descent(33/49): loss=6.198874521235663e+60\n",
      "Gradient Descent(34/49): loss=5.653430385412206e+62\n",
      "Gradient Descent(35/49): loss=5.1559803337219843e+64\n",
      "Gradient Descent(36/49): loss=4.702301326699613e+66\n",
      "Gradient Descent(37/49): loss=4.2885419136422096e+68\n",
      "Gradient Descent(38/49): loss=3.911189536203893e+70\n",
      "Gradient Descent(39/49): loss=3.5670407089759084e+72\n",
      "Gradient Descent(40/49): loss=3.253173823900333e+74\n",
      "Gradient Descent(41/49): loss=2.9669243476473924e+76\n",
      "Gradient Descent(42/49): loss=2.705862201396119e+78\n",
      "Gradient Descent(43/49): loss=2.467771130986157e+80\n",
      "Gradient Descent(44/49): loss=2.2506298923080944e+82\n",
      "Gradient Descent(45/49): loss=2.052595092206406e+84\n",
      "Gradient Descent(46/49): loss=1.871985539225608e+86\n",
      "Gradient Descent(47/49): loss=1.7072679713478687e+88\n",
      "Gradient Descent(48/49): loss=1.5570440395581363e+90\n",
      "Gradient Descent(49/49): loss=1.42003843673677e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.948146002467387\n",
      "Gradient Descent(2/49): loss=31.24510110959268\n",
      "Gradient Descent(3/49): loss=680.8833380924558\n",
      "Gradient Descent(4/49): loss=19603.555919605667\n",
      "Gradient Descent(5/49): loss=923746.8104178291\n",
      "Gradient Descent(6/49): loss=65600679.642280236\n",
      "Gradient Descent(7/49): loss=5491298732.094399\n",
      "Gradient Descent(8/49): loss=480522530008.10516\n",
      "Gradient Descent(9/49): loss=42492034487933.25\n",
      "Gradient Descent(10/49): loss=3766535378192268.0\n",
      "Gradient Descent(11/49): loss=3.3405078734538214e+17\n",
      "Gradient Descent(12/49): loss=2.9630319359896752e+19\n",
      "Gradient Descent(13/49): loss=2.6282836884078026e+21\n",
      "Gradient Descent(14/49): loss=2.331368251551533e+23\n",
      "Gradient Descent(15/49): loss=2.0679980923838747e+25\n",
      "Gradient Descent(16/49): loss=1.834380940337035e+27\n",
      "Gradient Descent(17/49): loss=1.627155117512977e+29\n",
      "Gradient Descent(18/49): loss=1.4433391460751924e+31\n",
      "Gradient Descent(19/49): loss=1.2802884469475439e+33\n",
      "Gradient Descent(20/49): loss=1.1356572107281338e+35\n",
      "Gradient Descent(21/49): loss=1.0073646322429737e+37\n",
      "Gradient Descent(22/49): loss=8.935649707952262e+38\n",
      "Gradient Descent(23/49): loss=7.926209949045068e+40\n",
      "Gradient Descent(24/49): loss=7.030804273870634e+42\n",
      "Gradient Descent(25/49): loss=6.236550514720714e+44\n",
      "Gradient Descent(26/49): loss=5.532021772703322e+46\n",
      "Gradient Descent(27/49): loss=4.9070820193676334e+48\n",
      "Gradient Descent(28/49): loss=4.3527402700431004e+50\n",
      "Gradient Descent(29/49): loss=3.8610212308814674e+52\n",
      "Gradient Descent(30/49): loss=3.424850558604484e+54\n",
      "Gradient Descent(31/49): loss=3.0379530821941384e+56\n",
      "Gradient Descent(32/49): loss=2.694762522243835e+58\n",
      "Gradient Descent(33/49): loss=2.3903414091059724e+60\n",
      "Gradient Descent(34/49): loss=2.120310047702903e+62\n",
      "Gradient Descent(35/49): loss=1.8807835070184986e+64\n",
      "Gradient Descent(36/49): loss=1.6683157277422954e+66\n",
      "Gradient Descent(37/49): loss=1.4798499439441016e+68\n",
      "Gradient Descent(38/49): loss=1.3126747054977387e+70\n",
      "Gradient Descent(39/49): loss=1.164384868550331e+72\n",
      "Gradient Descent(40/49): loss=1.0328469928083571e+74\n",
      "Gradient Descent(41/49): loss=9.161686478126566e+75\n",
      "Gradient Descent(42/49): loss=8.126711866126502e+77\n",
      "Gradient Descent(43/49): loss=7.208655951359952e+79\n",
      "Gradient Descent(44/49): loss=6.394310697992876e+81\n",
      "Gradient Descent(45/49): loss=5.671960151566404e+83\n",
      "Gradient Descent(46/49): loss=5.031211882002502e+85\n",
      "Gradient Descent(47/49): loss=4.46284746810366e+87\n",
      "Gradient Descent(48/49): loss=3.9586898724751116e+89\n",
      "Gradient Descent(49/49): loss=3.511485798795605e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9807579358389584\n",
      "Gradient Descent(2/49): loss=31.72149263462332\n",
      "Gradient Descent(3/49): loss=682.1155623590606\n",
      "Gradient Descent(4/49): loss=19042.669339370656\n",
      "Gradient Descent(5/49): loss=863583.4474395285\n",
      "Gradient Descent(6/49): loss=60124159.08926366\n",
      "Gradient Descent(7/49): loss=5004868140.60819\n",
      "Gradient Descent(8/49): loss=437473951176.26154\n",
      "Gradient Descent(9/49): loss=38683860094955.24\n",
      "Gradient Descent(10/49): loss=3429657865861870.5\n",
      "Gradient Descent(11/49): loss=3.042496333236862e+17\n",
      "Gradient Descent(12/49): loss=2.6994019223432196e+19\n",
      "Gradient Descent(13/49): loss=2.395069730879417e+21\n",
      "Gradient Descent(14/49): loss=2.1250625978124e+23\n",
      "Gradient Descent(15/49): loss=1.8854974917986582e+25\n",
      "Gradient Descent(16/49): loss=1.6729399047591202e+27\n",
      "Gradient Descent(17/49): loss=1.4843446649994751e+29\n",
      "Gradient Descent(18/49): loss=1.3170103223023302e+31\n",
      "Gradient Descent(19/49): loss=1.1685400546148381e+33\n",
      "Gradient Descent(20/49): loss=1.0368072575680655e+35\n",
      "Gradient Descent(21/49): loss=9.199250683073834e+36\n",
      "Gradient Descent(22/49): loss=8.162193360061858e+38\n",
      "Gradient Descent(23/49): loss=7.242046416921249e+40\n",
      "Gradient Descent(24/49): loss=6.425630218660451e+42\n",
      "Gradient Descent(25/49): loss=5.7012509075488964e+44\n",
      "Gradient Descent(26/49): loss=5.058532907236972e+46\n",
      "Gradient Descent(27/49): loss=4.4882703091911015e+48\n",
      "Gradient Descent(28/49): loss=3.982295012758809e+50\n",
      "Gradient Descent(29/49): loss=3.5333597301766943e+52\n",
      "Gradient Descent(30/49): loss=3.135034180751362e+54\n",
      "Gradient Descent(31/49): loss=2.781612987361464e+56\n",
      "Gradient Descent(32/49): loss=2.4680339560455794e+58\n",
      "Gradient Descent(33/49): loss=2.1898055681613277e+60\n",
      "Gradient Descent(34/49): loss=1.9429426465563986e+62\n",
      "Gradient Descent(35/49): loss=1.723909274272878e+64\n",
      "Gradient Descent(36/49): loss=1.5295681481855374e+66\n",
      "Gradient Descent(37/49): loss=1.3571356421471226e+68\n",
      "Gradient Descent(38/49): loss=1.2041419359908768e+70\n",
      "Gradient Descent(39/49): loss=1.0683956393023993e+72\n",
      "Gradient Descent(40/49): loss=9.479524032530922e+73\n",
      "Gradient Descent(41/49): loss=8.41087070909495e+75\n",
      "Gradient Descent(42/49): loss=7.46268967116314e+77\n",
      "Gradient Descent(43/49): loss=6.621399740203192e+79\n",
      "Gradient Descent(44/49): loss=5.874950782018828e+81\n",
      "Gradient Descent(45/49): loss=5.212651107828227e+83\n",
      "Gradient Descent(46/49): loss=4.6250143329039104e+85\n",
      "Gradient Descent(47/49): loss=4.103623499267598e+87\n",
      "Gradient Descent(48/49): loss=3.641010516213445e+89\n",
      "Gradient Descent(49/49): loss=3.2305491918405937e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9178936723442277\n",
      "Gradient Descent(2/49): loss=30.06942362467356\n",
      "Gradient Descent(3/49): loss=626.0459386285113\n",
      "Gradient Descent(4/49): loss=16045.81760469407\n",
      "Gradient Descent(5/49): loss=643914.4830034795\n",
      "Gradient Descent(6/49): loss=42017954.19972445\n",
      "Gradient Descent(7/49): loss=3464811409.096154\n",
      "Gradient Descent(8/49): loss=305597624446.4699\n",
      "Gradient Descent(9/49): loss=27386870009520.547\n",
      "Gradient Descent(10/49): loss=2463156655354417.5\n",
      "Gradient Descent(11/49): loss=2.2171129640985843e+17\n",
      "Gradient Descent(12/49): loss=1.9959991465286332e+19\n",
      "Gradient Descent(13/49): loss=1.7970074789895267e+21\n",
      "Gradient Descent(14/49): loss=1.61786837164597e+23\n",
      "Gradient Descent(15/49): loss=1.4565899853434264e+25\n",
      "Gradient Descent(16/49): loss=1.3113893105946608e+27\n",
      "Gradient Descent(17/49): loss=1.1806631264909728e+29\n",
      "Gradient Descent(18/49): loss=1.0629684382154786e+31\n",
      "Gradient Descent(19/49): loss=9.570061777311002e+32\n",
      "Gradient Descent(20/49): loss=8.616067911592773e+34\n",
      "Gradient Descent(21/49): loss=7.757173151174287e+36\n",
      "Gradient Descent(22/49): loss=6.983897517725472e+38\n",
      "Gradient Descent(23/49): loss=6.287706048079126e+40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=5.660914589139733e+42\n",
      "Gradient Descent(25/49): loss=5.096604984474901e+44\n",
      "Gradient Descent(26/49): loss=4.588548715716392e+46\n",
      "Gradient Descent(27/49): loss=4.131138155819217e+48\n",
      "Gradient Descent(28/49): loss=3.7193246753622727e+50\n",
      "Gradient Descent(29/49): loss=3.3485629187377026e+52\n",
      "Gradient Descent(30/49): loss=3.0147606351824976e+54\n",
      "Gradient Descent(31/49): loss=2.7142335109152043e+56\n",
      "Gradient Descent(32/49): loss=2.4436645038418697e+58\n",
      "Gradient Descent(33/49): loss=2.2000672319910593e+60\n",
      "Gradient Descent(34/49): loss=1.9807530115820237e+62\n",
      "Gradient Descent(35/49): loss=1.7833011809100738e+64\n",
      "Gradient Descent(36/49): loss=1.6055323825029981e+66\n",
      "Gradient Descent(37/49): loss=1.445484508651686e+68\n",
      "Gradient Descent(38/49): loss=1.3013910448163229e+70\n",
      "Gradient Descent(39/49): loss=1.1716615718752278e+72\n",
      "Gradient Descent(40/49): loss=1.0548642120115797e+74\n",
      "Gradient Descent(41/49): loss=9.49709824486186e+75\n",
      "Gradient Descent(42/49): loss=8.550377768581211e+77\n",
      "Gradient Descent(43/49): loss=7.698031346048492e+79\n",
      "Gradient Descent(44/49): loss=6.930651277478935e+81\n",
      "Gradient Descent(45/49): loss=6.239767671857562e+83\n",
      "Gradient Descent(46/49): loss=5.6177549612511644e+85\n",
      "Gradient Descent(47/49): loss=5.0577477342625955e+87\n",
      "Gradient Descent(48/49): loss=4.5535649596473964e+89\n",
      "Gradient Descent(49/49): loss=4.0996417637169815e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.956104405083266\n",
      "Gradient Descent(2/49): loss=31.489117957183648\n",
      "Gradient Descent(3/49): loss=699.1778684488072\n",
      "Gradient Descent(4/49): loss=21409.61200487096\n",
      "Gradient Descent(5/49): loss=1112916.4162622585\n",
      "Gradient Descent(6/49): loss=85434679.67694624\n",
      "Gradient Descent(7/49): loss=7543906732.509259\n",
      "Gradient Descent(8/49): loss=689991205702.6462\n",
      "Gradient Descent(9/49): loss=63611432238277.37\n",
      "Gradient Descent(10/49): loss=5874660297479443.0\n",
      "Gradient Descent(11/49): loss=5.427443235740636e+17\n",
      "Gradient Descent(12/49): loss=5.0146862444074885e+19\n",
      "Gradient Descent(13/49): loss=4.6334029589989635e+21\n",
      "Gradient Descent(14/49): loss=4.2811267260029153e+23\n",
      "Gradient Descent(15/49): loss=3.955637333505593e+25\n",
      "Gradient Descent(16/49): loss=3.6548952254877275e+27\n",
      "Gradient Descent(17/49): loss=3.377018297027276e+29\n",
      "Gradient Descent(18/49): loss=3.1202680174774546e+31\n",
      "Gradient Descent(19/49): loss=2.883038131069904e+33\n",
      "Gradient Descent(20/49): loss=2.6638445230134887e+35\n",
      "Gradient Descent(21/49): loss=2.4613159177344823e+37\n",
      "Gradient Descent(22/49): loss=2.2741852967306257e+39\n",
      "Gradient Descent(23/49): loss=2.1012819714134133e+41\n",
      "Gradient Descent(24/49): loss=1.9415242591439892e+43\n",
      "Gradient Descent(25/49): loss=1.7939127162024532e+45\n",
      "Gradient Descent(26/49): loss=1.657523885265202e+47\n",
      "Gradient Descent(27/49): loss=1.5315045182580445e+49\n",
      "Gradient Descent(28/49): loss=1.4150662384388879e+51\n",
      "Gradient Descent(29/49): loss=1.3074806083152486e+53\n",
      "Gradient Descent(30/49): loss=1.2080745725418197e+55\n",
      "Gradient Descent(31/49): loss=1.1162262472884187e+57\n",
      "Gradient Descent(32/49): loss=1.0313610297368327e+59\n",
      "Gradient Descent(33/49): loss=9.529480033673946e+60\n",
      "Gradient Descent(34/49): loss=8.804966165472127e+62\n",
      "Gradient Descent(35/49): loss=8.135536136405298e+64\n",
      "Gradient Descent(36/49): loss=7.517001994431438e+66\n",
      "Gradient Descent(37/49): loss=6.945494192009735e+68\n",
      "Gradient Descent(38/49): loss=6.417437378222971e+70\n",
      "Gradient Descent(39/49): loss=5.929528031394949e+72\n",
      "Gradient Descent(40/49): loss=5.478713792269923e+74\n",
      "Gradient Descent(41/49): loss=5.062174368462704e+76\n",
      "Gradient Descent(42/49): loss=4.677303890719155e+78\n",
      "Gradient Descent(43/49): loss=4.3216946106065754e+80\n",
      "Gradient Descent(44/49): loss=3.993121837647895e+82\n",
      "Gradient Descent(45/49): loss=3.6895300216648897e+84\n",
      "Gradient Descent(46/49): loss=3.409019893263524e+86\n",
      "Gradient Descent(47/49): loss=3.1498365820106687e+88\n",
      "Gradient Descent(48/49): loss=2.910358637970394e+90\n",
      "Gradient Descent(49/49): loss=2.689087887918937e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9713192139624156\n",
      "Gradient Descent(2/49): loss=32.14875603849357\n",
      "Gradient Descent(3/49): loss=710.6206934552214\n",
      "Gradient Descent(4/49): loss=20723.03598092148\n",
      "Gradient Descent(5/49): loss=987777.2825451046\n",
      "Gradient Descent(6/49): loss=70985703.23340784\n",
      "Gradient Descent(7/49): loss=6017494375.596572\n",
      "Gradient Descent(8/49): loss=533417240699.6313\n",
      "Gradient Descent(9/49): loss=47787385013536.91\n",
      "Gradient Descent(10/49): loss=4291517395190404.0\n",
      "Gradient Descent(11/49): loss=3.856089961562867e+17\n",
      "Gradient Descent(12/49): loss=3.4652735006356718e+19\n",
      "Gradient Descent(13/49): loss=3.114154296058426e+21\n",
      "Gradient Descent(14/49): loss=2.7986301585618223e+23\n",
      "Gradient Descent(15/49): loss=2.515078364822635e+25\n",
      "Gradient Descent(16/49): loss=2.2602562307874426e+27\n",
      "Gradient Descent(17/49): loss=2.031252258104739e+29\n",
      "Gradient Descent(18/49): loss=1.8254504729078875e+31\n",
      "Gradient Descent(19/49): loss=1.640500055285723e+33\n",
      "Gradient Descent(20/49): loss=1.4742883872519865e+35\n",
      "Gradient Descent(21/49): loss=1.3249169009212046e+37\n",
      "Gradient Descent(22/49): loss=1.190679387834197e+39\n",
      "Gradient Descent(23/49): loss=1.0700425088106835e+41\n",
      "Gradient Descent(24/49): loss=9.616282790845784e+42\n",
      "Gradient Descent(25/49): loss=8.641983281238594e+44\n",
      "Gradient Descent(26/49): loss=7.76639754233419e+46\n",
      "Gradient Descent(27/49): loss=6.979524123417565e+48\n",
      "Gradient Descent(28/49): loss=6.272374897606231e+50\n",
      "Gradient Descent(29/49): loss=5.63687239422514e+52\n",
      "Gradient Descent(30/49): loss=5.065757533227721e+54\n",
      "Gradient Descent(31/49): loss=4.552506707752187e+56\n",
      "Gradient Descent(32/49): loss=4.091257267681187e+58\n",
      "Gradient Descent(33/49): loss=3.676740553034633e+60\n",
      "Gradient Descent(34/49): loss=3.3042217122638335e+62\n",
      "Gradient Descent(35/49): loss=2.9694456180174843e+64\n",
      "Gradient Descent(36/49): loss=2.6685882625962926e+66\n",
      "Gradient Descent(37/49): loss=2.398213077908195e+68\n",
      "Gradient Descent(38/49): loss=2.1552316809841272e+70\n",
      "Gradient Descent(39/49): loss=1.9368685966674715e+72\n",
      "Gradient Descent(40/49): loss=1.7406295545189882e+74\n",
      "Gradient Descent(41/49): loss=1.5642729978058393e+76\n",
      "Gradient Descent(42/49): loss=1.4057844791338325e+78\n",
      "Gradient Descent(43/49): loss=1.2633536502551434e+80\n",
      "Gradient Descent(44/49): loss=1.1353535832152797e+82\n",
      "Gradient Descent(45/49): loss=1.0203221866335467e+84\n",
      "Gradient Descent(46/49): loss=9.169455048430109e+85\n",
      "Gradient Descent(47/49): loss=8.240427091229643e+87\n",
      "Gradient Descent(48/49): loss=7.405526095850046e+89\n",
      "Gradient Descent(49/49): loss=6.655215336433868e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.004435063065479\n",
      "Gradient Descent(2/49): loss=32.6397007918441\n",
      "Gradient Descent(3/49): loss=711.9504515160253\n",
      "Gradient Descent(4/49): loss=20133.151612680365\n",
      "Gradient Descent(5/49): loss=923578.0797321203\n",
      "Gradient Descent(6/49): loss=65063495.33252524\n",
      "Gradient Descent(7/49): loss=5484540868.378174\n",
      "Gradient Descent(8/49): loss=485631487891.5123\n",
      "Gradient Descent(9/49): loss=43504619092735.6\n",
      "Gradient Descent(10/49): loss=3907680253603749.5\n",
      "Gradient Descent(11/49): loss=3.5120764318957843e+17\n",
      "Gradient Descent(12/49): loss=3.1569513047949345e+19\n",
      "Gradient Descent(13/49): loss=2.8378217795952816e+21\n",
      "Gradient Descent(14/49): loss=2.5509700222080984e+23\n",
      "Gradient Descent(15/49): loss=2.2931172963452257e+25\n",
      "Gradient Descent(16/49): loss=2.0613291179270953e+27\n",
      "Gradient Descent(17/49): loss=1.852970223824373e+29\n",
      "Gradient Descent(18/49): loss=1.6656722509960725e+31\n",
      "Gradient Descent(19/49): loss=1.4973063372967925e+33\n",
      "Gradient Descent(20/49): loss=1.345958827382361e+35\n",
      "Gradient Descent(21/49): loss=1.2099095023212997e+37\n",
      "Gradient Descent(22/49): loss=1.0876120235582073e+39\n",
      "Gradient Descent(23/49): loss=9.77676356397958e+40\n",
      "Gradient Descent(24/49): loss=8.78852970689592e+42\n",
      "Gradient Descent(25/49): loss=7.900186386178045e+44\n",
      "Gradient Descent(26/49): loss=7.101636680750754e+46\n",
      "Gradient Descent(27/49): loss=6.383804264874381e+48\n",
      "Gradient Descent(28/49): loss=5.73853024651219e+50\n",
      "Gradient Descent(29/49): loss=5.158480433263033e+52\n",
      "Gradient Descent(30/49): loss=4.637061971840333e+54\n",
      "Gradient Descent(31/49): loss=4.168348413621265e+56\n",
      "Gradient Descent(32/49): loss=3.747012354558358e+58\n",
      "Gradient Descent(33/49): loss=3.368264883840634e+60\n",
      "Gradient Descent(34/49): loss=3.027801153073835e+62\n",
      "Gradient Descent(35/49): loss=2.721751447321511e+64\n",
      "Gradient Descent(36/49): loss=2.446637201877003e+66\n",
      "Gradient Descent(37/49): loss=2.1993314648549005e+68\n",
      "Gradient Descent(38/49): loss=1.9770233562172552e+70\n",
      "Gradient Descent(39/49): loss=1.7771861192766263e+72\n",
      "Gradient Descent(40/49): loss=1.5975484015488203e+74\n",
      "Gradient Descent(41/49): loss=1.4360684385325255e+76\n",
      "Gradient Descent(42/49): loss=1.2909108469889366e+78\n",
      "Gradient Descent(43/49): loss=1.1604257639535302e+80\n",
      "Gradient Descent(44/49): loss=1.0431300943733515e+82\n",
      "Gradient Descent(45/49): loss=9.376906542303694e+83\n",
      "Gradient Descent(46/49): loss=8.429090175556612e+85\n",
      "Gradient Descent(47/49): loss=7.577078951051364e+87\n",
      "Gradient Descent(48/49): loss=6.811188898767947e+89\n",
      "Gradient Descent(49/49): loss=6.122714902985386e+91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9406309916401012\n",
      "Gradient Descent(2/49): loss=30.94032097312938\n",
      "Gradient Descent(3/49): loss=653.5253915064201\n",
      "Gradient Descent(4/49): loss=16972.2541847605\n",
      "Gradient Descent(5/49): loss=688992.0449881835\n",
      "Gradient Descent(6/49): loss=45478384.07143571\n",
      "Gradient Descent(7/49): loss=3796907286.6900434\n",
      "Gradient Descent(8/49): loss=339219404160.72504\n",
      "Gradient Descent(9/49): loss=30797418126614.22\n",
      "Gradient Descent(10/49): loss=2806217054416232.5\n",
      "Gradient Descent(11/49): loss=2.5590481024568266e+17\n",
      "Gradient Descent(12/49): loss=2.3340674155408814e+19\n",
      "Gradient Descent(13/49): loss=2.128950650184517e+21\n",
      "Gradient Descent(14/49): loss=1.9418765648045773e+23\n",
      "Gradient Descent(15/49): loss=1.7712444226507452e+25\n",
      "Gradient Descent(16/49): loss=1.6156063790608888e+27\n",
      "Gradient Descent(17/49): loss=1.4736442863659004e+29\n",
      "Gradient Descent(18/49): loss=1.344156322502251e+31\n",
      "Gradient Descent(19/49): loss=1.2260463699371806e+33\n",
      "Gradient Descent(20/49): loss=1.118314646630368e+35\n",
      "Gradient Descent(21/49): loss=1.0200492247479238e+37\n",
      "Gradient Descent(22/49): loss=9.304183076719667e+38\n",
      "Gradient Descent(23/49): loss=8.486631882643011e+40\n",
      "Gradient Descent(24/49): loss=7.740918264161003e+42\n",
      "Gradient Descent(25/49): loss=7.060729910414572e+44\n",
      "Gradient Descent(26/49): loss=6.4403091683119e+46\n",
      "Gradient Descent(27/49): loss=5.874404305178688e+48\n",
      "Gradient Descent(28/49): loss=5.3582250539297826e+50\n",
      "Gradient Descent(29/49): loss=4.887402064452766e+52\n",
      "Gradient Descent(30/49): loss=4.4579499179673814e+54\n",
      "Gradient Descent(31/49): loss=4.066233391283377e+56\n",
      "Gradient Descent(32/49): loss=3.708936685391779e+58\n",
      "Gradient Descent(33/49): loss=3.383035357914553e+60\n",
      "Gradient Descent(34/49): loss=3.0857707218292178e+62\n",
      "Gradient Descent(35/49): loss=2.814626493755588e+64\n",
      "Gradient Descent(36/49): loss=2.5673074941403643e+66\n",
      "Gradient Descent(37/49): loss=2.341720219038593e+68\n",
      "Gradient Descent(38/49): loss=2.1359551190381922e+70\n",
      "Gradient Descent(39/49): loss=1.9482704353206577e+72\n",
      "Gradient Descent(40/49): loss=1.777077456034612e+74\n",
      "Gradient Descent(41/49): loss=1.6209270681801654e+76\n",
      "Gradient Descent(42/49): loss=1.4784974911684226e+78\n",
      "Gradient Descent(43/49): loss=1.3485830882234363e+80\n",
      "Gradient Descent(44/49): loss=1.2300841609173046e+82\n",
      "Gradient Descent(45/49): loss=1.1219976404516093e+84\n",
      "Gradient Descent(46/49): loss=1.023408596888346e+86\n",
      "Gradient Descent(47/49): loss=9.334824944581901e+87\n",
      "Gradient Descent(48/49): loss=8.514581273885404e+89\n",
      "Gradient Descent(49/49): loss=7.766411764548255e+91\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9792399568116052\n",
      "Gradient Descent(2/49): loss=32.39406176491179\n",
      "Gradient Descent(3/49): loss=729.4646148472741\n",
      "Gradient Descent(4/49): loss=22617.91012084624\n",
      "Gradient Descent(5/49): loss=1189132.4104872362\n",
      "Gradient Descent(6/49): loss=92379604.00043276\n",
      "Gradient Descent(7/49): loss=8260563506.924138\n",
      "Gradient Descent(8/49): loss=765312038801.7898\n",
      "Gradient Descent(9/49): loss=71473065138562.92\n",
      "Gradient Descent(10/49): loss=6686672792897113.0\n",
      "Gradient Descent(11/49): loss=6.258130681389944e+17\n",
      "Gradient Descent(12/49): loss=5.857544576821061e+19\n",
      "Gradient Descent(13/49): loss=5.482700490142326e+21\n",
      "Gradient Descent(14/49): loss=5.131864423821468e+23\n",
      "Gradient Descent(15/49): loss=4.8034824149255446e+25\n",
      "Gradient Descent(16/49): loss=4.4961140409219954e+27\n",
      "Gradient Descent(17/49): loss=4.208413929765728e+29\n",
      "Gradient Descent(18/49): loss=3.93912338590305e+31\n",
      "Gradient Descent(19/49): loss=3.687064376012464e+33\n",
      "Gradient Descent(20/49): loss=3.451134271994592e+35\n",
      "Gradient Descent(21/49): loss=3.230301006388683e+37\n",
      "Gradient Descent(22/49): loss=3.0235985533122146e+39\n",
      "Gradient Descent(23/49): loss=2.830122701739487e+41\n",
      "Gradient Descent(24/49): loss=2.649027099888923e+43\n",
      "Gradient Descent(25/49): loss=2.4795195528569887e+45\n",
      "Gradient Descent(26/49): loss=2.3208585571881293e+47\n",
      "Gradient Descent(27/49): loss=2.1723500571983433e+49\n",
      "Gradient Descent(28/49): loss=2.0333444088584016e+51\n",
      "Gradient Descent(29/49): loss=1.9032335379538027e+53\n",
      "Gradient Descent(30/49): loss=1.7814482800903618e+55\n",
      "Gradient Descent(31/49): loss=1.6674558909091146e+57\n",
      "Gradient Descent(32/49): loss=1.56075771561918e+59\n",
      "Gradient Descent(33/49): loss=1.4608870076537503e+61\n",
      "Gradient Descent(34/49): loss=1.3674068869074032e+63\n",
      "Gradient Descent(35/49): loss=1.2799084286229253e+65\n",
      "Gradient Descent(36/49): loss=1.1980088745676653e+67\n",
      "Gradient Descent(37/49): loss=1.1213499586740614e+69\n",
      "Gradient Descent(38/49): loss=1.0495963398201523e+71\n",
      "Gradient Descent(39/49): loss=9.82434134894468e+72\n",
      "Gradient Descent(40/49): loss=9.195695457276541e+74\n",
      "Gradient Descent(41/49): loss=8.607275738852622e+76\n",
      "Gradient Descent(42/49): loss=8.056508176988132e+78\n",
      "Gradient Descent(43/49): loss=7.540983462733661e+80\n",
      "Gradient Descent(44/49): loss=7.058446455456886e+82\n",
      "Gradient Descent(45/49): loss=6.606786317827423e+84\n",
      "Gradient Descent(46/49): loss=6.184027282049741e+86\n",
      "Gradient Descent(47/49): loss=5.788320006951667e+88\n",
      "Gradient Descent(48/49): loss=5.417933488121919e+90\n",
      "Gradient Descent(49/49): loss=5.0712474857055587e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9946642407543609\n",
      "Gradient Descent(2/49): loss=33.0721205648851\n",
      "Gradient Descent(3/49): loss=741.4344210066425\n",
      "Gradient Descent(4/49): loss=21898.197782302777\n",
      "Gradient Descent(5/49): loss=1055783.4122309734\n",
      "Gradient Descent(6/49): loss=76773521.84626143\n",
      "Gradient Descent(7/49): loss=6590204403.781309\n",
      "Gradient Descent(8/49): loss=591733886415.3693\n",
      "Gradient Descent(9/49): loss=53701747848586.9\n",
      "Gradient Descent(10/49): loss=4885537977429452.0\n",
      "Gradient Descent(11/49): loss=4.447109002933755e+17\n",
      "Gradient Descent(12/49): loss=4.04853530622919e+19\n",
      "Gradient Descent(13/49): loss=3.68578934983808e+21\n",
      "Gradient Descent(14/49): loss=3.355566948271341e+23\n",
      "Gradient Descent(15/49): loss=3.0549347889938174e+25\n",
      "Gradient Descent(16/49): loss=2.781237812434089e+27\n",
      "Gradient Descent(17/49): loss=2.5320620205834716e+29\n",
      "Gradient Descent(18/49): loss=2.3052103482214765e+31\n",
      "Gradient Descent(19/49): loss=2.098682704857333e+33\n",
      "Gradient Descent(20/49): loss=1.910658219520855e+35\n",
      "Gradient Descent(21/49): loss=1.7394791619066452e+37\n",
      "Gradient Descent(22/49): loss=1.5836363217297186e+39\n",
      "Gradient Descent(23/49): loss=1.4417557016196503e+41\n",
      "Gradient Descent(24/49): loss=1.312586402973665e+43\n",
      "Gradient Descent(25/49): loss=1.1949895972922127e+45\n",
      "Gradient Descent(26/49): loss=1.0879284856232424e+47\n",
      "Gradient Descent(27/49): loss=9.904591575629296e+48\n",
      "Gradient Descent(28/49): loss=9.017222692154138e+50\n",
      "Gradient Descent(29/49): loss=8.209354667381267e+52\n",
      "Gradient Descent(30/49): loss=7.473864886745485e+54\n",
      "Gradient Descent(31/49): loss=6.804268862603193e+56\n",
      "Gradient Descent(32/49): loss=6.19466306337659e+58\n",
      "Gradient Descent(33/49): loss=5.639672864731751e+60\n",
      "Gradient Descent(34/49): loss=5.134405163895173e+62\n",
      "Gradient Descent(35/49): loss=4.6744052393342984e+64\n",
      "Gradient Descent(36/49): loss=4.255617475450676e+66\n",
      "Gradient Descent(37/49): loss=3.8743496060132616e+68\n",
      "Gradient Descent(38/49): loss=3.52724016108265e+70\n",
      "Gradient Descent(39/49): loss=3.211228830419583e+72\n",
      "Gradient Descent(40/49): loss=2.923529482084557e+74\n",
      "Gradient Descent(41/49): loss=2.661605598346934e+76\n",
      "Gradient Descent(42/49): loss=2.4231479123310832e+78\n",
      "Gradient Descent(43/49): loss=2.2060540482335852e+80\n",
      "Gradient Descent(44/49): loss=2.008409985606708e+82\n",
      "Gradient Descent(45/49): loss=1.8284731842879004e+84\n",
      "Gradient Descent(46/49): loss=1.6646572211947366e+86\n",
      "Gradient Descent(47/49): loss=1.5155178035356284e+88\n",
      "Gradient Descent(48/49): loss=1.379740035119683e+90\n",
      "Gradient Descent(49/49): loss=1.2561268234994295e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0282876637737566\n",
      "Gradient Descent(2/49): loss=33.577947873789356\n",
      "Gradient Descent(3/49): loss=742.8667254383741\n",
      "Gradient Descent(4/49): loss=21278.020382351984\n",
      "Gradient Descent(5/49): loss=987306.5056483774\n",
      "Gradient Descent(6/49): loss=70372604.82796274\n",
      "Gradient Descent(7/49): loss=6006624163.4399185\n",
      "Gradient Descent(8/49): loss=538725557150.8387\n",
      "Gradient Descent(9/49): loss=48888910530226.57\n",
      "Gradient Descent(10/49): loss=4448565014340535.0\n",
      "Gradient Descent(11/49): loss=4.05036184977817e+17\n",
      "Gradient Descent(12/49): loss=3.6883103124011033e+19\n",
      "Gradient Descent(13/49): loss=3.358726046777586e+21\n",
      "Gradient Descent(14/49): loss=3.0586146315690074e+23\n",
      "Gradient Descent(15/49): loss=2.785323416802552e+25\n",
      "Gradient Descent(16/49): loss=2.5364520385433544e+27\n",
      "Gradient Descent(17/49): loss=2.309817749023779e+29\n",
      "Gradient Descent(18/49): loss=2.103433477258069e+31\n",
      "Gradient Descent(19/49): loss=1.9154898317889826e+33\n",
      "Gradient Descent(20/49): loss=1.7443391192465414e+35\n",
      "Gradient Descent(21/49): loss=1.5884808747504536e+37\n",
      "Gradient Descent(22/49): loss=1.4465487024439078e+39\n",
      "Gradient Descent(23/49): loss=1.317298295387701e+41\n",
      "Gradient Descent(24/49): loss=1.1995965266180314e+43\n",
      "Gradient Descent(25/49): loss=1.0924115150785358e+45\n",
      "Gradient Descent(26/49): loss=9.948035792006755e+46\n",
      "Gradient Descent(27/49): loss=9.05916998796363e+48\n",
      "Gradient Descent(28/49): loss=8.249725130338158e+50\n",
      "Gradient Descent(29/49): loss=7.512604887264195e+52\n",
      "Gradient Descent(30/49): loss=6.841346990409688e+54\n",
      "Gradient Descent(31/49): loss=6.230066580838293e+56\n",
      "Gradient Descent(32/49): loss=5.673404616969051e+58\n",
      "Gradient Descent(33/49): loss=5.1664808923301395e+60\n",
      "Gradient Descent(34/49): loss=4.704851251217877e+62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=4.284468627174926e+64\n",
      "Gradient Descent(36/49): loss=3.901647562713966e+66\n",
      "Gradient Descent(37/49): loss=3.5530318992368935e+68\n",
      "Gradient Descent(38/49): loss=3.2355653538869515e+70\n",
      "Gradient Descent(39/49): loss=2.9464647253861972e+72\n",
      "Gradient Descent(40/49): loss=2.683195493954619e+74\n",
      "Gradient Descent(41/49): loss=2.443449601398087e+76\n",
      "Gradient Descent(42/49): loss=2.225125216565224e+78\n",
      "Gradient Descent(43/49): loss=2.026308308778426e+80\n",
      "Gradient Descent(44/49): loss=1.8452558676956343e+82\n",
      "Gradient Descent(45/49): loss=1.6803806224916738e+84\n",
      "Gradient Descent(46/49): loss=1.530237126394722e+86\n",
      "Gradient Descent(47/49): loss=1.393509084581423e+88\n",
      "Gradient Descent(48/49): loss=1.2689978143361562e+90\n",
      "Gradient Descent(49/49): loss=1.1556117363050371e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9635371040643181\n",
      "Gradient Descent(2/49): loss=31.830248865044283\n",
      "Gradient Descent(3/49): loss=682.0041633800292\n",
      "Gradient Descent(4/49): loss=17945.260895400435\n",
      "Gradient Descent(5/49): loss=736897.3357542525\n",
      "Gradient Descent(6/49): loss=49198584.13281038\n",
      "Gradient Descent(7/49): loss=4158370279.104704\n",
      "Gradient Descent(8/49): loss=376285852254.9351\n",
      "Gradient Descent(9/49): loss=34606374003809.7\n",
      "Gradient Descent(10/49): loss=3194358791747169.0\n",
      "Gradient Descent(11/49): loss=2.950975525568237e+17\n",
      "Gradient Descent(12/49): loss=2.7266305161065038e+19\n",
      "Gradient Descent(13/49): loss=2.519442642713028e+21\n",
      "Gradient Descent(14/49): loss=2.328019149936575e+23\n",
      "Gradient Descent(15/49): loss=2.1511440065727803e+25\n",
      "Gradient Descent(16/49): loss=1.98770812579869e+27\n",
      "Gradient Descent(17/49): loss=1.836689672585246e+29\n",
      "Gradient Descent(18/49): loss=1.6971450602540417e+31\n",
      "Gradient Descent(19/49): loss=1.5682025180877953e+33\n",
      "Gradient Descent(20/49): loss=1.4490565349873887e+35\n",
      "Gradient Descent(21/49): loss=1.338962804786104e+37\n",
      "Gradient Descent(22/49): loss=1.2372335719195673e+39\n",
      "Gradient Descent(23/49): loss=1.1432333340637223e+41\n",
      "Gradient Descent(24/49): loss=1.0563748719573401e+43\n",
      "Gradient Descent(25/49): loss=9.761155810046932e+44\n",
      "Gradient Descent(26/49): loss=9.019540816176452e+46\n",
      "Gradient Descent(27/49): loss=8.334270870970207e+48\n",
      "Gradient Descent(28/49): loss=7.701065094813764e+50\n",
      "Gradient Descent(29/49): loss=7.115967852824774e+52\n",
      "Gradient Descent(30/49): loss=6.575324044012656e+54\n",
      "Gradient Descent(31/49): loss=6.075756267871362e+56\n",
      "Gradient Descent(32/49): loss=5.614143725767048e+58\n",
      "Gradient Descent(33/49): loss=5.187602725316567e+60\n",
      "Gradient Descent(34/49): loss=4.793468665969172e+62\n",
      "Gradient Descent(35/49): loss=4.429279393253177e+64\n",
      "Gradient Descent(36/49): loss=4.092759817703057e+66\n",
      "Gradient Descent(37/49): loss=3.7818077023814614e+68\n",
      "Gradient Descent(38/49): loss=3.4944805302106207e+70\n",
      "Gradient Descent(39/49): loss=3.2289833690728944e+72\n",
      "Gradient Descent(40/49): loss=2.983657658874035e+74\n",
      "Gradient Descent(41/49): loss=2.7569708505231133e+76\n",
      "Gradient Descent(42/49): loss=2.5475068321016592e+78\n",
      "Gradient Descent(43/49): loss=2.3539570824164336e+80\n",
      "Gradient Descent(44/49): loss=2.1751124966708078e+82\n",
      "Gradient Descent(45/49): loss=2.009855833189958e+84\n",
      "Gradient Descent(46/49): loss=1.8571547340151587e+86\n",
      "Gradient Descent(47/49): loss=1.7160552757660806e+88\n",
      "Gradient Descent(48/49): loss=1.585676010483996e+90\n",
      "Gradient Descent(49/49): loss=1.4652024592284413e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0025459206065923\n",
      "Gradient Descent(2/49): loss=33.31861889055946\n",
      "Gradient Descent(3/49): loss=760.8389061702518\n",
      "Gradient Descent(4/49): loss=23885.61990394042\n",
      "Gradient Descent(5/49): loss=1270023.426508888\n",
      "Gradient Descent(6/49): loss=99838909.58069794\n",
      "Gradient Descent(7/49): loss=9040025876.736382\n",
      "Gradient Descent(8/49): loss=848289699220.2776\n",
      "Gradient Descent(9/49): loss=80246143071911.19\n",
      "Gradient Descent(10/49): loss=7604589109746348.0\n",
      "Gradient Descent(11/49): loss=7.209351003739694e+17\n",
      "Gradient Descent(12/49): loss=6.835235476915843e+19\n",
      "Gradient Descent(13/49): loss=6.480654225793462e+21\n",
      "Gradient Descent(14/49): loss=6.144491962092867e+23\n",
      "Gradient Descent(15/49): loss=5.825772150220607e+25\n",
      "Gradient Descent(16/49): loss=5.523585663462674e+27\n",
      "Gradient Descent(17/49): loss=5.237074002207381e+29\n",
      "Gradient Descent(18/49): loss=4.96542391641577e+31\n",
      "Gradient Descent(19/49): loss=4.7078644886491713e+33\n",
      "Gradient Descent(20/49): loss=4.4636648202322514e+35\n",
      "Gradient Descent(21/49): loss=4.232131931013215e+37\n",
      "Gradient Descent(22/49): loss=4.012608787446141e+39\n",
      "Gradient Descent(23/49): loss=3.8044724369707644e+41\n",
      "Gradient Descent(24/49): loss=3.607132239995162e+43\n",
      "Gradient Descent(25/49): loss=3.420028193758033e+45\n",
      "Gradient Descent(26/49): loss=3.2426293431694065e+47\n",
      "Gradient Descent(27/49): loss=3.0744322740887436e+49\n",
      "Gradient Descent(28/49): loss=2.914959684760027e+51\n",
      "Gradient Descent(29/49): loss=2.763759031346545e+53\n",
      "Gradient Descent(30/49): loss=2.6204012437237404e+55\n",
      "Gradient Descent(31/49): loss=2.484479507883655e+57\n",
      "Gradient Descent(32/49): loss=2.355608111497528e+59\n",
      "Gradient Descent(33/49): loss=2.2334213493592764e+61\n",
      "Gradient Descent(34/49): loss=2.1175724856043697e+63\n",
      "Gradient Descent(35/49): loss=2.007732769759353e+65\n",
      "Gradient Descent(36/49): loss=1.9035905038287832e+67\n",
      "Gradient Descent(37/49): loss=1.8048501577734887e+69\n",
      "Gradient Descent(38/49): loss=1.7112315308691804e+71\n",
      "Gradient Descent(39/49): loss=1.6224689565660586e+73\n",
      "Gradient Descent(40/49): loss=1.5383105485927134e+75\n",
      "Gradient Descent(41/49): loss=1.4585174861651025e+77\n",
      "Gradient Descent(42/49): loss=1.3828633362720462e+79\n",
      "Gradient Descent(43/49): loss=1.3111334111142637e+81\n",
      "Gradient Descent(44/49): loss=1.243124158873451e+83\n",
      "Gradient Descent(45/49): loss=1.1786425860824491e+85\n",
      "Gradient Descent(46/49): loss=1.1175057099574281e+87\n",
      "Gradient Descent(47/49): loss=1.0595400391379728e+89\n",
      "Gradient Descent(48/49): loss=1.0045810813613076e+91\n",
      "Gradient Descent(49/49): loss=9.524728766740448e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0181810828432214\n",
      "Gradient Descent(2/49): loss=34.01547947445373\n",
      "Gradient Descent(3/49): loss=773.3555008197474\n",
      "Gradient Descent(4/49): loss=23131.40415831732\n",
      "Gradient Descent(5/49): loss=1127983.6065822172\n",
      "Gradient Descent(6/49): loss=82991364.80560152\n",
      "Gradient Descent(7/49): loss=7213199714.829108\n",
      "Gradient Descent(8/49): loss=655987727839.1381\n",
      "Gradient Descent(9/49): loss=60302768070908.72\n",
      "Gradient Descent(10/49): loss=5557139910530879.0\n",
      "Gradient Descent(11/49): loss=5.124005125607159e+17\n",
      "Gradient Descent(12/49): loss=4.725233237685651e+19\n",
      "Gradient Descent(13/49): loss=4.35762187731976e+21\n",
      "Gradient Descent(14/49): loss=4.0186362355925726e+23\n",
      "Gradient Descent(15/49): loss=3.706026315698536e+25\n",
      "Gradient Descent(16/49): loss=3.4177354985244485e+27\n",
      "Gradient Descent(17/49): loss=3.151870991688518e+29\n",
      "Gradient Descent(18/49): loss=2.9066880472597967e+31\n",
      "Gradient Descent(19/49): loss=2.680577809170612e+33\n",
      "Gradient Descent(20/49): loss=2.4720566089924553e+35\n",
      "Gradient Descent(21/49): loss=2.279756199730533e+37\n",
      "Gradient Descent(22/49): loss=2.1024147713874853e+39\n",
      "Gradient Descent(23/49): loss=1.9388686700435436e+41\n",
      "Gradient Descent(24/49): loss=1.7880447620735824e+43\n",
      "Gradient Descent(25/49): loss=1.6489533925516946e+45\n",
      "Gradient Descent(26/49): loss=1.520681891461599e+47\n",
      "Gradient Descent(27/49): loss=1.4023885850653617e+49\n",
      "Gradient Descent(28/49): loss=1.2932972731274725e+51\n",
      "Gradient Descent(29/49): loss=1.1926921357542498e+53\n",
      "Gradient Descent(30/49): loss=1.0999130364282391e+55\n",
      "Gradient Descent(31/49): loss=1.0143511904182495e+57\n",
      "Gradient Descent(32/49): loss=9.354451701419146e+58\n",
      "Gradient Descent(33/49): loss=8.626772212698922e+60\n",
      "Gradient Descent(34/49): loss=7.955698653989973e+62\n",
      "Gradient Descent(35/49): loss=7.336827670020866e+64\n",
      "Gradient Descent(36/49): loss=6.766098440969584e+66\n",
      "Gradient Descent(37/49): loss=6.239766036751061e+68\n",
      "Gradient Descent(38/49): loss=5.754376844066711e+70\n",
      "Gradient Descent(39/49): loss=5.306745904975142e+72\n",
      "Gradient Descent(40/49): loss=4.893936018286262e+74\n",
      "Gradient Descent(41/49): loss=4.5132384666515203e+76\n",
      "Gradient Descent(42/49): loss=4.162155242886807e+78\n",
      "Gradient Descent(43/49): loss=3.8383826589032067e+80\n",
      "Gradient Descent(44/49): loss=3.539796229693271e+82\n",
      "Gradient Descent(45/49): loss=3.264436733186753e+84\n",
      "Gradient Descent(46/49): loss=3.010497354505175e+86\n",
      "Gradient Descent(47/49): loss=2.7763118302602917e+88\n",
      "Gradient Descent(48/49): loss=2.560343515103412e+90\n",
      "Gradient Descent(49/49): loss=2.3611752987839313e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0523157379637915\n",
      "Gradient Descent(2/49): loss=34.53652358770779\n",
      "Gradient Descent(3/49): loss=774.8955484934526\n",
      "Gradient Descent(4/49): loss=22479.589579819396\n",
      "Gradient Descent(5/49): loss=1054974.3435539498\n",
      "Gradient Descent(6/49): loss=76076507.75839964\n",
      "Gradient Descent(7/49): loss=6574557010.300216\n",
      "Gradient Descent(8/49): loss=597225309991.3956\n",
      "Gradient Descent(9/49): loss=54898316623251.49\n",
      "Gradient Descent(10/49): loss=5060090612629262.0\n",
      "Gradient Descent(11/49): loss=4.666860965602567e+17\n",
      "Gradient Descent(12/49): loss=4.304789693516547e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=3.970934343986072e+21\n",
      "Gradient Descent(14/49): loss=3.662997067050707e+23\n",
      "Gradient Descent(15/49): loss=3.37894510445863e+25\n",
      "Gradient Descent(16/49): loss=3.116921465505965e+27\n",
      "Gradient Descent(17/49): loss=2.8752169461292582e+29\n",
      "Gradient Descent(18/49): loss=2.652255673821809e+31\n",
      "Gradient Descent(19/49): loss=2.4465841433355398e+33\n",
      "Gradient Descent(20/49): loss=2.256861597177274e+35\n",
      "Gradient Descent(21/49): loss=2.0818512552652464e+37\n",
      "Gradient Descent(22/49): loss=1.9204122462282852e+39\n",
      "Gradient Descent(23/49): loss=1.771492168893968e+41\n",
      "Gradient Descent(24/49): loss=1.6341202315408877e+43\n",
      "Gradient Descent(25/49): loss=1.5074009233696035e+45\n",
      "Gradient Descent(26/49): loss=1.3905081767656609e+47\n",
      "Gradient Descent(27/49): loss=1.2826799822638493e+49\n",
      "Gradient Descent(28/49): loss=1.1832134211014205e+51\n",
      "Gradient Descent(29/49): loss=1.0914600829769148e+53\n",
      "Gradient Descent(30/49): loss=1.0068218391429597e+55\n",
      "Gradient Descent(31/49): loss=9.28746943278414e+56\n",
      "Gradient Descent(32/49): loss=8.567264347218115e+58\n",
      "Gradient Descent(33/49): loss=7.90290820619259e+60\n",
      "Gradient Descent(34/49): loss=7.290070153582616e+62\n",
      "Gradient Descent(35/49): loss=6.724755173356538e+64\n",
      "Gradient Descent(36/49): loss=6.203278046557784e+66\n",
      "Gradient Descent(37/49): loss=5.72223932781466e+68\n",
      "Gradient Descent(38/49): loss=5.2785031847731866e+70\n",
      "Gradient Descent(39/49): loss=4.869176955990977e+72\n",
      "Gradient Descent(40/49): loss=4.4915922940326493e+74\n",
      "Gradient Descent(41/49): loss=4.143287770840041e+76\n",
      "Gradient Descent(42/49): loss=3.821992831985228e+78\n",
      "Gradient Descent(43/49): loss=3.525612995204737e+80\n",
      "Gradient Descent(44/49): loss=3.252216196726848e+82\n",
      "Gradient Descent(45/49): loss=3.000020196385219e+84\n",
      "Gradient Descent(46/49): loss=2.7673809594138454e+86\n",
      "Gradient Descent(47/49): loss=2.5527819391862812e+88\n",
      "Gradient Descent(48/49): loss=2.3548241910343807e+90\n",
      "Gradient Descent(49/49): loss=2.1722172526997437e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=1.9866120096168796\n",
      "Gradient Descent(2/49): loss=32.73948276614996\n",
      "Gradient Descent(3/49): loss=711.5111420354426\n",
      "Gradient Descent(4/49): loss=18966.83407362274\n",
      "Gradient Descent(5/49): loss=787787.3512742125\n",
      "Gradient Descent(6/49): loss=53196184.63978446\n",
      "Gradient Descent(7/49): loss=4551582302.141201\n",
      "Gradient Descent(8/49): loss=417124172394.2628\n",
      "Gradient Descent(9/49): loss=38857245566917.31\n",
      "Gradient Descent(10/49): loss=3633156113642306.5\n",
      "Gradient Descent(11/49): loss=3.399808764668916e+17\n",
      "Gradient Descent(12/49): loss=3.182033167777958e+19\n",
      "Gradient Descent(13/49): loss=2.9783289691312085e+21\n",
      "Gradient Descent(14/49): loss=2.787690646990097e+23\n",
      "Gradient Descent(15/49): loss=2.6092600721260345e+25\n",
      "Gradient Descent(16/49): loss=2.4422513288478574e+27\n",
      "Gradient Descent(17/49): loss=2.285932404325698e+29\n",
      "Gradient Descent(18/49): loss=2.139618888421699e+31\n",
      "Gradient Descent(19/49): loss=2.0026703334264765e+33\n",
      "Gradient Descent(20/49): loss=1.874487316646866e+35\n",
      "Gradient Descent(21/49): loss=1.7545087888325367e+37\n",
      "Gradient Descent(22/49): loss=1.642209612687602e+39\n",
      "Gradient Descent(23/49): loss=1.5370982631729925e+41\n",
      "Gradient Descent(24/49): loss=1.4387146758866757e+43\n",
      "Gradient Descent(25/49): loss=1.3466282333440856e+45\n",
      "Gradient Descent(26/49): loss=1.2604358801873733e+47\n",
      "Gradient Descent(27/49): loss=1.1797603590402344e+49\n",
      "Gradient Descent(28/49): loss=1.104248559280832e+51\n",
      "Gradient Descent(29/49): loss=1.033569971503166e+53\n",
      "Gradient Descent(30/49): loss=9.674152409026186e+54\n",
      "Gradient Descent(31/49): loss=9.054948132535019e+56\n",
      "Gradient Descent(32/49): loss=8.47537667552139e+58\n",
      "Gradient Descent(33/49): loss=7.932901297786026e+60\n",
      "Gradient Descent(34/49): loss=7.425147625847845e+62\n",
      "Gradient Descent(35/49): loss=6.94989326049247e+64\n",
      "Gradient Descent(36/49): loss=6.505058049498649e+66\n",
      "Gradient Descent(37/49): loss=6.088694982971518e+68\n",
      "Gradient Descent(38/49): loss=5.698981671427063e+70\n",
      "Gradient Descent(39/49): loss=5.334212369332967e+72\n",
      "Gradient Descent(40/49): loss=4.992790509189227e+74\n",
      "Gradient Descent(41/49): loss=4.673221713474301e+76\n",
      "Gradient Descent(42/49): loss=4.374107253867985e+78\n",
      "Gradient Descent(43/49): loss=4.0941379291238424e+80\n",
      "Gradient Descent(44/49): loss=3.8320883347952204e+82\n",
      "Gradient Descent(45/49): loss=3.5868114997327873e+84\n",
      "Gradient Descent(46/49): loss=3.357233865879212e+86\n",
      "Gradient Descent(47/49): loss=3.1423505893873348e+88\n",
      "Gradient Descent(48/49): loss=2.941221142494667e+90\n",
      "Gradient Descent(49/49): loss=2.752965196905018e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0260222964682275\n",
      "Gradient Descent(2/49): loss=34.26307096670975\n",
      "Gradient Descent(3/49): loss=793.3318023162019\n",
      "Gradient Descent(4/49): loss=25215.234325024456\n",
      "Gradient Descent(5/49): loss=1355843.1693574316\n",
      "Gradient Descent(6/49): loss=107847025.32397719\n",
      "Gradient Descent(7/49): loss=9887341680.979181\n",
      "Gradient Descent(8/49): loss=939645844338.0762\n",
      "Gradient Descent(9/49): loss=90029453734938.06\n",
      "Gradient Descent(10/49): loss=8641406293183138.0\n",
      "Gradient Descent(11/49): loss=8.297647914271722e+17\n",
      "Gradient Descent(12/49): loss=7.968249928074827e+19\n",
      "Gradient Descent(13/49): loss=7.652072322395844e+21\n",
      "Gradient Descent(14/49): loss=7.348470795813968e+23\n",
      "Gradient Descent(15/49): loss=7.0569212351110285e+25\n",
      "Gradient Descent(16/49): loss=6.776940197702138e+27\n",
      "Gradient Descent(17/49): loss=6.50806759659723e+29\n",
      "Gradient Descent(18/49): loss=6.249862475604092e+31\n",
      "Gradient Descent(19/49): loss=6.001901557523103e+33\n",
      "Gradient Descent(20/49): loss=5.763778397213248e+35\n",
      "Gradient Descent(21/49): loss=5.535102683218338e+37\n",
      "Gradient Descent(22/49): loss=5.315499591247961e+39\n",
      "Gradient Descent(23/49): loss=5.104609168381833e+41\n",
      "Gradient Descent(24/49): loss=4.902085742769642e+43\n",
      "Gradient Descent(25/49): loss=4.7075973569762225e+45\n",
      "Gradient Descent(26/49): loss=4.520825223854579e+47\n",
      "Gradient Descent(27/49): loss=4.341463204016962e+49\n",
      "Gradient Descent(28/49): loss=4.169217304039661e+51\n",
      "Gradient Descent(29/49): loss=4.003805194576019e+53\n",
      "Gradient Descent(30/49): loss=3.8449557475886996e+55\n",
      "Gradient Descent(31/49): loss=3.692408591942323e+57\n",
      "Gradient Descent(32/49): loss=3.5459136866268608e+59\n",
      "Gradient Descent(33/49): loss=3.4052309109142696e+61\n",
      "Gradient Descent(34/49): loss=3.270129670775105e+63\n",
      "Gradient Descent(35/49): loss=3.140388520910242e+65\n",
      "Gradient Descent(36/49): loss=3.015794801778346e+67\n",
      "Gradient Descent(37/49): loss=2.896144291024569e+69\n",
      "Gradient Descent(38/49): loss=2.781240868738264e+71\n",
      "Gradient Descent(39/49): loss=2.6708961959914946e+73\n",
      "Gradient Descent(40/49): loss=2.5649294061316025e+75\n",
      "Gradient Descent(41/49): loss=2.463166808321589e+77\n",
      "Gradient Descent(42/49): loss=2.3654416028422048e+79\n",
      "Gradient Descent(43/49): loss=2.2715936076897024e+81\n",
      "Gradient Descent(44/49): loss=2.1814689960202146e+83\n",
      "Gradient Descent(45/49): loss=2.094920044011465e+85\n",
      "Gradient Descent(46/49): loss=2.0118048887275716e+87\n",
      "Gradient Descent(47/49): loss=1.9319872955905402e+89\n",
      "Gradient Descent(48/49): loss=1.8553364350774284e+91\n",
      "Gradient Descent(49/49): loss=1.781726668276903e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.041869740228997\n",
      "Gradient Descent(2/49): loss=34.97911959863997\n",
      "Gradient Descent(3/49): loss=806.4155776754616\n",
      "Gradient Descent(4/49): loss=24425.101103356003\n",
      "Gradient Descent(5/49): loss=1204606.7237319706\n",
      "Gradient Descent(6/49): loss=89668108.55535492\n",
      "Gradient Descent(7/49): loss=7890529639.945832\n",
      "Gradient Descent(8/49): loss=726739193600.191\n",
      "Gradient Descent(9/49): loss=67664974519013.25\n",
      "Gradient Descent(10/49): loss=6315857599711628.0\n",
      "Gradient Descent(11/49): loss=5.898581771307464e+17\n",
      "Gradient Descent(12/49): loss=5.509586974380224e+19\n",
      "Gradient Descent(13/49): loss=5.146396634556021e+21\n",
      "Gradient Descent(14/49): loss=4.8071798413056133e+23\n",
      "Gradient Descent(15/49): loss=4.490328827487322e+25\n",
      "Gradient Descent(16/49): loss=4.1943635595156265e+27\n",
      "Gradient Descent(17/49): loss=3.917906177561601e+29\n",
      "Gradient Descent(18/49): loss=3.659670620528843e+31\n",
      "Gradient Descent(19/49): loss=3.4184558022104924e+33\n",
      "Gradient Descent(20/49): loss=3.1931398462243063e+35\n",
      "Gradient Descent(21/49): loss=2.9826748302830227e+37\n",
      "Gradient Descent(22/49): loss=2.7860819043514654e+39\n",
      "Gradient Descent(23/49): loss=2.602446736408823e+41\n",
      "Gradient Descent(24/49): loss=2.4309152596280934e+43\n",
      "Gradient Descent(25/49): loss=2.2706897001283446e+45\n",
      "Gradient Descent(26/49): loss=2.121024866600409e+47\n",
      "Gradient Descent(27/49): loss=1.9812246845014407e+49\n",
      "Gradient Descent(28/49): loss=1.8506389586884772e+51\n",
      "Gradient Descent(29/49): loss=1.7286603494330412e+53\n",
      "Gradient Descent(30/49): loss=1.6147215477511472e+55\n",
      "Gradient Descent(31/49): loss=1.5082926369121665e+57\n",
      "Gradient Descent(32/49): loss=1.4088786278549023e+59\n",
      "Gradient Descent(33/49): loss=1.3160171570485138e+61\n",
      "Gradient Descent(34/49): loss=1.2292763360907632e+63\n",
      "Gradient Descent(35/49): loss=1.148252743043097e+65\n",
      "Gradient Descent(36/49): loss=1.0725695461599007e+67\n",
      "Gradient Descent(37/49): loss=1.001874751285895e+69\n",
      "Gradient Descent(38/49): loss=9.358395647703096e+70\n",
      "Gradient Descent(39/49): loss=8.741568642841191e+72\n",
      "Gradient Descent(40/49): loss=8.165397704280353e+74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/49): loss=7.627203124884242e+76\n",
      "Gradient Descent(42/49): loss=7.124481821350442e+78\n",
      "Gradient Descent(43/49): loss=6.654895692649482e+80\n",
      "Gradient Descent(44/49): loss=6.216260745774402e+82\n",
      "Gradient Descent(45/49): loss=5.806536938232715e+84\n",
      "Gradient Descent(46/49): loss=5.423818690034825e+86\n",
      "Gradient Descent(47/49): loss=5.066326021052634e+88\n",
      "Gradient Descent(48/49): loss=4.732396272529187e+90\n",
      "Gradient Descent(49/49): loss=4.420476374237464e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0765192856355834\n",
      "Gradient Descent(2/49): loss=35.51571972301573\n",
      "Gradient Descent(3/49): loss=808.0687545162292\n",
      "Gradient Descent(4/49): loss=23740.25478150001\n",
      "Gradient Descent(5/49): loss=1126797.0755709105\n",
      "Gradient Descent(6/49): loss=82201739.31233425\n",
      "Gradient Descent(7/49): loss=7192032382.6341715\n",
      "Gradient Descent(8/49): loss=661641025348.4194\n",
      "Gradient Descent(9/49): loss=61600686374824.59\n",
      "Gradient Descent(10/49): loss=5750938403327748.0\n",
      "Gradient Descent(11/49): loss=5.372323784468502e+17\n",
      "Gradient Descent(12/49): loss=5.019343413607933e+19\n",
      "Gradient Descent(13/49): loss=4.689704971385354e+21\n",
      "Gradient Descent(14/49): loss=4.381746804571782e+23\n",
      "Gradient Descent(15/49): loss=4.094017996511403e+25\n",
      "Gradient Descent(16/49): loss=3.8251844141236453e+27\n",
      "Gradient Descent(17/49): loss=3.5740040825659435e+29\n",
      "Gradient Descent(18/49): loss=3.339317544688167e+31\n",
      "Gradient Descent(19/49): loss=3.1200416828905426e+33\n",
      "Gradient Descent(20/49): loss=2.9151645455173915e+35\n",
      "Gradient Descent(21/49): loss=2.723740639725097e+37\n",
      "Gradient Descent(22/49): loss=2.5448865602699037e+39\n",
      "Gradient Descent(23/49): loss=2.3777769110093298e+41\n",
      "Gradient Descent(24/49): loss=2.2216404954191258e+43\n",
      "Gradient Descent(25/49): loss=2.0757567575142483e+45\n",
      "Gradient Descent(26/49): loss=1.939452456529815e+47\n",
      "Gradient Descent(27/49): loss=1.8120985599700827e+49\n",
      "Gradient Descent(28/49): loss=1.693107340677478e+51\n",
      "Gradient Descent(29/49): loss=1.5819296645229322e+53\n",
      "Gradient Descent(30/49): loss=1.478052456199429e+55\n",
      "Gradient Descent(31/49): loss=1.380996331424142e+57\n",
      "Gradient Descent(32/49): loss=1.2903133846215892e+59\n",
      "Gradient Descent(33/49): loss=1.2055851218784219e+61\n",
      "Gradient Descent(34/49): loss=1.1264205296303325e+63\n",
      "Gradient Descent(35/49): loss=1.0524542701686267e+65\n",
      "Gradient Descent(36/49): loss=9.833449956382608e+66\n",
      "Gradient Descent(37/49): loss=9.18773772747278e+68\n",
      "Gradient Descent(38/49): loss=8.584426109173789e+70\n",
      "Gradient Descent(39/49): loss=8.020730870833863e+72\n",
      "Gradient Descent(40/49): loss=7.494050607949036e+74\n",
      "Gradient Descent(41/49): loss=7.001954736908178e+76\n",
      "Gradient Descent(42/49): loss=6.542172278060996e+78\n",
      "Gradient Descent(43/49): loss=6.112581375344564e+80\n",
      "Gradient Descent(44/49): loss=5.711199504101594e+82\n",
      "Gradient Descent(45/49): loss=5.336174321902059e+84\n",
      "Gradient Descent(46/49): loss=4.985775120143734e+86\n",
      "Gradient Descent(47/49): loss=4.658384836982517e+88\n",
      "Gradient Descent(48/49): loss=4.352492594732759e+90\n",
      "Gradient Descent(49/49): loss=4.0666867272980666e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.009855708297784\n",
      "Gradient Descent(2/49): loss=33.668300124396424\n",
      "Gradient Descent(3/49): loss=742.0758375861395\n",
      "Gradient Descent(4/49): loss=20039.04100165356\n",
      "Gradient Descent(5/49): loss=841826.6818763053\n",
      "Gradient Descent(6/49): loss=57489886.93199135\n",
      "Gradient Descent(7/49): loss=4979101251.794753\n",
      "Gradient Descent(8/49): loss=462090238658.2854\n",
      "Gradient Descent(9/49): loss=43597965264904.086\n",
      "Gradient Descent(10/49): loss=4128829837210428.5\n",
      "Gradient Descent(11/49): loss=3.913363477545619e+17\n",
      "Gradient Descent(12/49): loss=3.709831512590985e+19\n",
      "Gradient Descent(13/49): loss=3.5170309016043473e+21\n",
      "Gradient Descent(14/49): loss=3.334280955590469e+23\n",
      "Gradient Descent(15/49): loss=3.1610334580423876e+25\n",
      "Gradient Descent(16/49): loss=2.996789182700338e+27\n",
      "Gradient Descent(17/49): loss=2.8410791726959828e+29\n",
      "Gradient Descent(18/49): loss=2.693459752044354e+31\n",
      "Gradient Descent(19/49): loss=2.5535104907768524e+33\n",
      "Gradient Descent(20/49): loss=2.4208328447864405e+35\n",
      "Gradient Descent(21/49): loss=2.2950489864977565e+37\n",
      "Gradient Descent(22/49): loss=2.175800721665981e+39\n",
      "Gradient Descent(23/49): loss=2.0627484678161216e+41\n",
      "Gradient Descent(24/49): loss=1.955570286892651e+43\n",
      "Gradient Descent(25/49): loss=1.8539609684105653e+45\n",
      "Gradient Descent(26/49): loss=1.7576311602953714e+47\n",
      "Gradient Descent(27/49): loss=1.6663065448944361e+49\n",
      "Gradient Descent(28/49): loss=1.5797270578040216e+51\n",
      "Gradient Descent(29/49): loss=1.4976461472857114e+53\n",
      "Gradient Descent(30/49): loss=1.419830072162979e+55\n",
      "Gradient Descent(31/49): loss=1.3460572361982643e+57\n",
      "Gradient Descent(32/49): loss=1.2761175570548821e+59\n",
      "Gradient Descent(33/49): loss=1.2098118680473781e+61\n",
      "Gradient Descent(34/49): loss=1.1469513509759985e+63\n",
      "Gradient Descent(35/49): loss=1.0873569984305852e+65\n",
      "Gradient Descent(36/49): loss=1.0308591040324929e+67\n",
      "Gradient Descent(37/49): loss=9.772967791631006e+68\n",
      "Gradient Descent(38/49): loss=9.265174948025505e+70\n",
      "Gradient Descent(39/49): loss=8.78376647173943e+72\n",
      "Gradient Descent(40/49): loss=8.327371459563928e+74\n",
      "Gradient Descent(41/49): loss=7.8946902389389e+76\n",
      "Gradient Descent(42/49): loss=7.484490666885785e+78\n",
      "Gradient Descent(43/49): loss=7.095604621243464e+80\n",
      "Gradient Descent(44/49): loss=6.726924674215558e+82\n",
      "Gradient Descent(45/49): loss=6.3774009387575505e+84\n",
      "Gradient Descent(46/49): loss=6.046038078820765e+86\n",
      "Gradient Descent(47/49): loss=5.7318924749416794e+88\n",
      "Gradient Descent(48/49): loss=5.434069537104413e+90\n",
      "Gradient Descent(49/49): loss=5.151721157223292e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0496690843965113\n",
      "Gradient Descent(2/49): loss=35.227701636587\n",
      "Gradient Descent(3/49): loss=826.9750245976734\n",
      "Gradient Descent(4/49): loss=26609.33314109633\n",
      "Gradient Descent(5/49): loss=1446857.3359966823\n",
      "Gradient Descent(6/49): loss=116440441.99158694\n",
      "Gradient Descent(7/49): loss=10807928273.372238\n",
      "Gradient Descent(8/49): loss=1040164873906.3276\n",
      "Gradient Descent(9/49): loss=100931756077634.77\n",
      "Gradient Descent(10/49): loss=9811617757606240.0\n",
      "Gradient Descent(11/49): loss=9.541707780768626e+17\n",
      "Gradient Descent(12/49): loss=9.280031347684935e+19\n",
      "Gradient Descent(13/49): loss=9.025703511591727e+21\n",
      "Gradient Descent(14/49): loss=8.778382460294836e+23\n",
      "Gradient Descent(15/49): loss=8.537846282511376e+25\n",
      "Gradient Descent(16/49): loss=8.30390269408231e+27\n",
      "Gradient Descent(17/49): loss=8.076369695560947e+29\n",
      "Gradient Descent(18/49): loss=7.855071342942677e+31\n",
      "Gradient Descent(19/49): loss=7.639836740836888e+33\n",
      "Gradient Descent(20/49): loss=7.430499724998397e+35\n",
      "Gradient Descent(21/49): loss=7.226898694530593e+37\n",
      "Gradient Descent(22/49): loss=7.028876478718469e+39\n",
      "Gradient Descent(23/49): loss=6.836280213905663e+41\n",
      "Gradient Descent(24/49): loss=6.648961225110223e+43\n",
      "Gradient Descent(25/49): loss=6.46677491117239e+45\n",
      "Gradient Descent(26/49): loss=6.289580633112796e+47\n",
      "Gradient Descent(27/49): loss=6.117241605562003e+49\n",
      "Gradient Descent(28/49): loss=5.949624791168027e+51\n",
      "Gradient Descent(29/49): loss=5.786600797898245e+53\n",
      "Gradient Descent(30/49): loss=5.628043779154563e+55\n",
      "Gradient Descent(31/49): loss=5.473831336625917e+57\n",
      "Gradient Descent(32/49): loss=5.323844425803173e+59\n",
      "Gradient Descent(33/49): loss=5.177967264082085e+61\n",
      "Gradient Descent(34/49): loss=5.036087241384859e+63\n",
      "Gradient Descent(35/49): loss=4.898094833230886e+65\n",
      "Gradient Descent(36/49): loss=4.7638835161890034e+67\n",
      "Gradient Descent(37/49): loss=4.633349685646541e+69\n",
      "Gradient Descent(38/49): loss=4.506392575831798e+71\n",
      "Gradient Descent(39/49): loss=4.382914182027301e+73\n",
      "Gradient Descent(40/49): loss=4.2628191849152315e+75\n",
      "Gradient Descent(41/49): loss=4.1460148769959104e+77\n",
      "Gradient Descent(42/49): loss=4.0324110910214844e+79\n",
      "Gradient Descent(43/49): loss=3.921920130391633e+81\n",
      "Gradient Descent(44/49): loss=3.814456701455642e+83\n",
      "Gradient Descent(45/49): loss=3.70993784767023e+85\n",
      "Gradient Descent(46/49): loss=3.608282885561036e+87\n",
      "Gradient Descent(47/49): loss=3.509413342438899e+89\n",
      "Gradient Descent(48/49): loss=3.413252895822464e+91\n",
      "Gradient Descent(49/49): loss=3.319727314521532e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.065730212911688\n",
      "Gradient Descent(2/49): loss=35.963329814637135\n",
      "Gradient Descent(3/49): loss=840.6469705131417\n",
      "Gradient Descent(4/49): loss=25781.820101838985\n",
      "Gradient Descent(5/49): loss=1285892.4995815337\n",
      "Gradient Descent(6/49): loss=96834364.72256632\n",
      "Gradient Descent(7/49): loss=8626540514.51331\n",
      "Gradient Descent(8/49): loss=804597511123.2599\n",
      "Gradient Descent(9/49): loss=75870429253548.33\n",
      "Gradient Descent(10/49): loss=7172324677360789.0\n",
      "Gradient Descent(11/49): loss=6.784173216384186e+17\n",
      "Gradient Descent(12/49): loss=6.417867878231631e+19\n",
      "Gradient Descent(13/49): loss=6.071521863918493e+21\n",
      "Gradient Descent(14/49): loss=5.7439057107359084e+23\n",
      "Gradient Descent(15/49): loss=5.433975951891627e+25\n",
      "Gradient Descent(16/49): loss=5.140771196860263e+27\n",
      "Gradient Descent(17/49): loss=4.863387480827786e+29\n",
      "Gradient Descent(18/49): loss=4.600970810472617e+31\n",
      "Gradient Descent(19/49): loss=4.352713529444048e+33\n",
      "Gradient Descent(20/49): loss=4.117851616053076e+35\n",
      "Gradient Descent(21/49): loss=3.89566228532545e+37\n",
      "Gradient Descent(22/49): loss=3.685461754592638e+39\n",
      "Gradient Descent(23/49): loss=3.4866031370020764e+41\n",
      "Gradient Descent(24/49): loss=3.2984744502734746e+43\n",
      "Gradient Descent(25/49): loss=3.12049673323828e+45\n",
      "Gradient Descent(26/49): loss=2.9521222640801215e+47\n",
      "Gradient Descent(27/49): loss=2.7928328747308625e+49\n",
      "Gradient Descent(28/49): loss=2.6421383562200852e+51\n",
      "Gradient Descent(29/49): loss=2.4995749500700444e+53\n",
      "Gradient Descent(30/49): loss=2.3647039210906751e+55\n",
      "Gradient Descent(31/49): loss=2.2371102071833613e+57\n",
      "Gradient Descent(32/49): loss=2.1164011419981903e+59\n",
      "Gradient Descent(33/49): loss=2.0022052465134656e+61\n",
      "Gradient Descent(34/49): loss=1.894171085818446e+63\n",
      "Gradient Descent(35/49): loss=1.7919661875815723e+65\n",
      "Gradient Descent(36/49): loss=1.695276018875657e+67\n",
      "Gradient Descent(37/49): loss=1.6038030182106786e+69\n",
      "Gradient Descent(38/49): loss=1.5172656797962788e+71\n",
      "Gradient Descent(39/49): loss=1.4353976872147657e+73\n",
      "Gradient Descent(40/49): loss=1.35794709383936e+75\n",
      "Gradient Descent(41/49): loss=1.2846755474748368e+77\n",
      "Gradient Descent(42/49): loss=1.215357556834953e+79\n",
      "Gradient Descent(43/49): loss=1.1497797975988795e+81\n",
      "Gradient Descent(44/49): loss=1.0877404559109648e+83\n",
      "Gradient Descent(45/49): loss=1.0290486073040263e+85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=9.735236291340355e+86\n",
      "Gradient Descent(47/49): loss=9.209946447187563e+88\n",
      "Gradient Descent(48/49): loss=8.712999974691214e+90\n",
      "Gradient Descent(49/49): loss=8.242867533953223e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1008983067891327\n",
      "Gradient Descent(2/49): loss=36.51583015129911\n",
      "Gradient Descent(3/49): loss=842.4188563383225\n",
      "Gradient Descent(4/49): loss=25062.495502121637\n",
      "Gradient Descent(5/49): loss=1203000.4513778223\n",
      "Gradient Descent(6/49): loss=88776430.77215667\n",
      "Gradient Descent(7/49): loss=7863014188.746203\n",
      "Gradient Descent(8/49): loss=732527416320.0548\n",
      "Gradient Descent(9/49): loss=69070726827990.28\n",
      "Gradient Descent(10/49): loss=6530790609559228.0\n",
      "Gradient Descent(11/49): loss=6.178894504218976e+17\n",
      "Gradient Descent(12/49): loss=5.8467944728740856e+19\n",
      "Gradient Descent(13/49): loss=5.532723241532995e+21\n",
      "Gradient Descent(14/49): loss=5.2355613979035697e+23\n",
      "Gradient Descent(15/49): loss=4.954368337425309e+25\n",
      "Gradient Descent(16/49): loss=4.688279448770261e+27\n",
      "Gradient Descent(17/49): loss=4.436482025163093e+29\n",
      "Gradient Descent(18/49): loss=4.198208182254467e+31\n",
      "Gradient Descent(19/49): loss=3.972731529027038e+33\n",
      "Gradient Descent(20/49): loss=3.759364741423752e+35\n",
      "Gradient Descent(21/49): loss=3.5574574216773716e+37\n",
      "Gradient Descent(22/49): loss=3.366394105964375e+39\n",
      "Gradient Descent(23/49): loss=3.1855923862218754e+41\n",
      "Gradient Descent(24/49): loss=3.0145011343745977e+43\n",
      "Gradient Descent(25/49): loss=2.8525988222644833e+45\n",
      "Gradient Descent(26/49): loss=2.6993919318840033e+47\n",
      "Gradient Descent(27/49): loss=2.554413451007463e+49\n",
      "Gradient Descent(28/49): loss=2.417221449622538e+51\n",
      "Gradient Descent(29/49): loss=2.287397732818362e+53\n",
      "Gradient Descent(30/49): loss=2.164546566025129e+55\n",
      "Gradient Descent(31/49): loss=2.048293468717486e+57\n",
      "Gradient Descent(32/49): loss=1.9382840729064323e+59\n",
      "Gradient Descent(33/49): loss=1.8341830429382174e+61\n",
      "Gradient Descent(34/49): loss=1.7356730533092124e+63\n",
      "Gradient Descent(35/49): loss=1.6424538213796803e+65\n",
      "Gradient Descent(36/49): loss=1.5542411920386603e+67\n",
      "Gradient Descent(37/49): loss=1.4707662715293782e+69\n",
      "Gradient Descent(38/49): loss=1.3917746077950224e+71\n",
      "Gradient Descent(39/49): loss=1.3170254148463087e+73\n",
      "Gradient Descent(40/49): loss=1.2462908387868428e+75\n",
      "Gradient Descent(41/49): loss=1.1793552632583886e+77\n",
      "Gradient Descent(42/49): loss=1.1160146521891931e+79\n",
      "Gradient Descent(43/49): loss=1.056075927841994e+81\n",
      "Gradient Descent(44/49): loss=9.993563822657085e+82\n",
      "Gradient Descent(45/49): loss=9.456831203566775e+84\n",
      "Gradient Descent(46/49): loss=8.948925328319854e+86\n",
      "Gradient Descent(47/49): loss=8.468297975080591e+88\n",
      "Gradient Descent(48/49): loss=8.013484073647675e+90\n",
      "Gradient Descent(49/49): loss=7.58309723955995e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.033268200107033\n",
      "Gradient Descent(2/49): loss=34.6169803699533\n",
      "Gradient Descent(3/49): loss=773.7283913543498\n",
      "Gradient Descent(4/49): loss=21164.021913307886\n",
      "Gradient Descent(5/49): loss=899187.8257911277\n",
      "Gradient Descent(6/49): loss=62099520.9686625\n",
      "Gradient Descent(7/49): loss=5443672750.56223\n",
      "Gradient Descent(8/49): loss=511570899315.515\n",
      "Gradient Descent(9/49): loss=48881307234742.625\n",
      "Gradient Descent(10/49): loss=4688317463764479.0\n",
      "Gradient Descent(11/49): loss=4.5004682181454214e+17\n",
      "Gradient Descent(12/49): loss=4.320959472012819e+19\n",
      "Gradient Descent(13/49): loss=4.148785063392313e+21\n",
      "Gradient Descent(14/49): loss=3.9835085093381356e+23\n",
      "Gradient Descent(15/49): loss=3.8248241287521285e+25\n",
      "Gradient Descent(16/49): loss=3.672462705267863e+27\n",
      "Gradient Descent(17/49): loss=3.526170948017624e+29\n",
      "Gradient Descent(18/49): loss=3.3857067697902366e+31\n",
      "Gradient Descent(19/49): loss=3.2508379653987373e+33\n",
      "Gradient Descent(20/49): loss=3.1213416305763984e+35\n",
      "Gradient Descent(21/49): loss=2.997003751333246e+37\n",
      "Gradient Descent(22/49): loss=2.8776188412168087e+39\n",
      "Gradient Descent(23/49): loss=2.762989599778792e+41\n",
      "Gradient Descent(24/49): loss=2.652926586093446e+43\n",
      "Gradient Descent(25/49): loss=2.547247905590968e+45\n",
      "Gradient Descent(26/49): loss=2.4457789094318203e+47\n",
      "Gradient Descent(27/49): loss=2.3483519058714723e+49\n",
      "Gradient Descent(28/49): loss=2.2548058831251957e+51\n",
      "Gradient Descent(29/49): loss=2.1649862432731275e+53\n",
      "Gradient Descent(30/49): loss=2.0787445467657264e+55\n",
      "Gradient Descent(31/49): loss=1.995938267106654e+57\n",
      "Gradient Descent(32/49): loss=1.9164305553074865e+59\n",
      "Gradient Descent(33/49): loss=1.84009001372583e+61\n",
      "Gradient Descent(34/49): loss=1.766790478912124e+63\n",
      "Gradient Descent(35/49): loss=1.696410813106927e+65\n",
      "Gradient Descent(36/49): loss=1.6288347040437258e+67\n",
      "Gradient Descent(37/49): loss=1.5639504727266743e+69\n",
      "Gradient Descent(38/49): loss=1.5016508888653122e+71\n",
      "Gradient Descent(39/49): loss=1.4418329936616318e+73\n",
      "Gradient Descent(40/49): loss=1.3843979296559974e+75\n",
      "Gradient Descent(41/49): loss=1.3292507773515672e+77\n",
      "Gradient Descent(42/49): loss=1.2763003983462826e+79\n",
      "Gradient Descent(43/49): loss=1.2254592847140606e+81\n",
      "Gradient Descent(44/49): loss=1.1766434143856266e+83\n",
      "Gradient Descent(45/49): loss=1.1297721122902451e+85\n",
      "Gradient Descent(46/49): loss=1.0847679170288014e+87\n",
      "Gradient Descent(47/49): loss=1.041556452858053e+89\n",
      "Gradient Descent(48/49): loss=1.0000663067743295e+91\n",
      "Gradient Descent(49/49): loss=9.602289104933838e+92\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.0734862843914437\n",
      "Gradient Descent(2/49): loss=36.2127965540569\n",
      "Gradient Descent(3/49): loss=861.800965076217\n",
      "Gradient Descent(4/49): loss=28070.585301382718\n",
      "Gradient Descent(5/49): loss=1543344.0989972511\n",
      "Gradient Descent(6/49): loss=125657821.32807393\n",
      "Gradient Descent(7/49): loss=11807596956.545732\n",
      "Gradient Descent(8/49): loss=1150698932483.7236\n",
      "Gradient Descent(9/49): loss=113072713463275.45\n",
      "Gradient Descent(10/49): loss=1.1131374354191644e+16\n",
      "Gradient Descent(11/49): loss=1.0962620770640694e+18\n",
      "Gradient Descent(12/49): loss=1.0797378008171658e+20\n",
      "Gradient Descent(13/49): loss=1.0634831770791646e+22\n",
      "Gradient Descent(14/49): loss=1.0474776993062163e+24\n",
      "Gradient Descent(15/49): loss=1.0317140650188382e+26\n",
      "Gradient Descent(16/49): loss=1.0161878672237472e+28\n",
      "Gradient Descent(17/49): loss=1.000895366973324e+30\n",
      "Gradient Descent(18/49): loss=9.858330115677585e+31\n",
      "Gradient Descent(19/49): loss=9.709973298489824e+33\n",
      "Gradient Descent(20/49): loss=9.563849089610873e+35\n",
      "Gradient Descent(21/49): loss=9.419923887180128e+37\n",
      "Gradient Descent(22/49): loss=9.278164597836799e+39\n",
      "Gradient Descent(23/49): loss=9.138538626849728e+41\n",
      "Gradient Descent(24/49): loss=9.001013870134298e+43\n",
      "Gradient Descent(25/49): loss=8.865558706764137e+45\n",
      "Gradient Descent(26/49): loss=8.732141991678827e+47\n",
      "Gradient Descent(27/49): loss=8.60073304851795e+49\n",
      "Gradient Descent(28/49): loss=8.471301662565696e+51\n",
      "Gradient Descent(29/49): loss=8.343818073804056e+53\n",
      "Gradient Descent(30/49): loss=8.218252970070293e+55\n",
      "Gradient Descent(31/49): loss=8.094577480315921e+57\n",
      "Gradient Descent(32/49): loss=7.972763167970293e+59\n",
      "Gradient Descent(33/49): loss=7.852782024400586e+61\n",
      "Gradient Descent(34/49): loss=7.734606462472725e+63\n",
      "Gradient Descent(35/49): loss=7.618209310208508e+65\n",
      "Gradient Descent(36/49): loss=7.503563804536979e+67\n",
      "Gradient Descent(37/49): loss=7.390643585141501e+69\n",
      "Gradient Descent(38/49): loss=7.279422688398576e+71\n",
      "Gradient Descent(39/49): loss=7.169875541408285e+73\n",
      "Gradient Descent(40/49): loss=7.0619769561140565e+75\n",
      "Gradient Descent(41/49): loss=6.955702123511254e+77\n",
      "Gradient Descent(42/49): loss=6.851026607943174e+79\n",
      "Gradient Descent(43/49): loss=6.747926341482291e+81\n",
      "Gradient Descent(44/49): loss=6.646377618396225e+83\n",
      "Gradient Descent(45/49): loss=6.54635708969732e+85\n",
      "Gradient Descent(46/49): loss=6.4478417577741075e+87\n",
      "Gradient Descent(47/49): loss=6.350808971103259e+89\n",
      "Gradient Descent(48/49): loss=6.2552364190417355e+91\n",
      "Gradient Descent(49/49): loss=6.161102126696283e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.089762500891296\n",
      "Gradient Descent(2/49): loss=36.96840104539398\n",
      "Gradient Descent(3/49): loss=876.0826819479028\n",
      "Gradient Descent(4/49): loss=27204.180507149744\n",
      "Gradient Descent(5/49): loss=1372091.9889983195\n",
      "Gradient Descent(6/49): loss=104522572.27824694\n",
      "Gradient Descent(7/49): loss=9425895357.033567\n",
      "Gradient Descent(8/49): loss=890224603278.1724\n",
      "Gradient Descent(9/49): loss=85009433540637.0\n",
      "Gradient Descent(10/49): loss=8138392485188166.0\n",
      "Gradient Descent(11/49): loss=7.795831403658767e+17\n",
      "Gradient Descent(12/49): loss=7.468679026973237e+19\n",
      "Gradient Descent(13/49): loss=7.155471797715917e+21\n",
      "Gradient Descent(14/49): loss=6.855446475836836e+23\n",
      "Gradient Descent(15/49): loss=6.568011377742399e+25\n",
      "Gradient Descent(16/49): loss=6.292630108444054e+27\n",
      "Gradient Descent(17/49): loss=6.028795420074556e+29\n",
      "Gradient Descent(18/49): loss=5.776022786487071e+31\n",
      "Gradient Descent(19/49): loss=5.533848314113658e+33\n",
      "Gradient Descent(20/49): loss=5.301827628670455e+35\n",
      "Gradient Descent(21/49): loss=5.07953500254753e+37\n",
      "Gradient Descent(22/49): loss=4.8665625611583244e+39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=4.662519531819602e+41\n",
      "Gradient Descent(24/49): loss=4.467031526146065e+43\n",
      "Gradient Descent(25/49): loss=4.2797398529732285e+45\n",
      "Gradient Descent(26/49): loss=4.100300860184842e+47\n",
      "Gradient Descent(27/49): loss=3.9283853041565313e+49\n",
      "Gradient Descent(28/49): loss=3.7636777456417557e+51\n",
      "Gradient Descent(29/49): loss=3.605875970987549e+53\n",
      "Gradient Descent(30/49): loss=3.4546904376183967e+55\n",
      "Gradient Descent(31/49): loss=3.309843742768294e+57\n",
      "Gradient Descent(32/49): loss=3.1710701144890147e+59\n",
      "Gradient Descent(33/49): loss=3.038114923997842e+61\n",
      "Gradient Descent(34/49): loss=2.910734218472473e+63\n",
      "Gradient Descent(35/49): loss=2.7886942734338153e+65\n",
      "Gradient Descent(36/49): loss=2.6717711638968376e+67\n",
      "Gradient Descent(37/49): loss=2.5597503535017543e+69\n",
      "Gradient Descent(38/49): loss=2.452426300872107e+71\n",
      "Gradient Descent(39/49): loss=2.3496020824771097e+73\n",
      "Gradient Descent(40/49): loss=2.2510890313065145e+75\n",
      "Gradient Descent(41/49): loss=2.1567063906949177e+77\n",
      "Gradient Descent(42/49): loss=2.066280982660553e+79\n",
      "Gradient Descent(43/49): loss=1.9796468901493914e+81\n",
      "Gradient Descent(44/49): loss=1.8966451526027046e+83\n",
      "Gradient Descent(45/49): loss=1.8171234742878132e+85\n",
      "Gradient Descent(46/49): loss=1.7409359448585771e+87\n",
      "Gradient Descent(47/49): loss=1.667942771631658e+89\n",
      "Gradient Descent(48/49): loss=1.5980100230880893e+91\n",
      "Gradient Descent(49/49): loss=1.531009383128816e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1254528014244394\n",
      "Gradient Descent(2/49): loss=37.53715082631289\n",
      "Gradient Descent(3/49): loss=877.9790553843183\n",
      "Gradient Descent(4/49): loss=26448.877536124895\n",
      "Gradient Descent(5/49): loss=1283820.9057259648\n",
      "Gradient Descent(6/49): loss=95830394.37921111\n",
      "Gradient Descent(7/49): loss=8591755228.172886\n",
      "Gradient Descent(8/49): loss=810487178486.3594\n",
      "Gradient Descent(9/49): loss=77390645774986.72\n",
      "Gradient Descent(10/49): loss=7410438202014968.0\n",
      "Gradient Descent(11/49): loss=7.100281668613556e+17\n",
      "Gradient Descent(12/49): loss=6.804090006664082e+19\n",
      "Gradient Descent(13/49): loss=6.520468242408013e+21\n",
      "Gradient Descent(14/49): loss=6.248715577189671e+23\n",
      "Gradient Descent(15/49): loss=5.9882988548817745e+25\n",
      "Gradient Descent(16/49): loss=5.738737269944685e+27\n",
      "Gradient Descent(17/49): loss=5.499576612543403e+29\n",
      "Gradient Descent(18/49): loss=5.270383029363379e+31\n",
      "Gradient Descent(19/49): loss=5.050741058512273e+33\n",
      "Gradient Descent(20/49): loss=4.8402526199956285e+35\n",
      "Gradient Descent(21/49): loss=4.638536238427353e+37\n",
      "Gradient Descent(22/49): loss=4.4452263395209725e+39\n",
      "Gradient Descent(23/49): loss=4.259972584909022e+41\n",
      "Gradient Descent(24/49): loss=4.082439236644653e+43\n",
      "Gradient Descent(25/49): loss=3.912304548612792e+45\n",
      "Gradient Descent(26/49): loss=3.749260183399371e+47\n",
      "Gradient Descent(27/49): loss=3.593010653480022e+49\n",
      "Gradient Descent(28/49): loss=3.443272785703505e+51\n",
      "Gradient Descent(29/49): loss=3.2997752080928237e+53\n",
      "Gradient Descent(30/49): loss=3.162257858033582e+55\n",
      "Gradient Descent(31/49): loss=3.030471510959299e+57\n",
      "Gradient Descent(32/49): loss=2.9041773286782206e+59\n",
      "Gradient Descent(33/49): loss=2.7831464265238074e+61\n",
      "Gradient Descent(34/49): loss=2.6671594585437457e+63\n",
      "Gradient Descent(35/49): loss=2.5560062199761417e+65\n",
      "Gradient Descent(36/49): loss=2.4494852662928942e+67\n",
      "Gradient Descent(37/49): loss=2.3474035481189816e+69\n",
      "Gradient Descent(38/49): loss=2.2495760613661143e+71\n",
      "Gradient Descent(39/49): loss=2.1558255119477086e+73\n",
      "Gradient Descent(40/49): loss=2.065981994466157e+75\n",
      "Gradient Descent(41/49): loss=1.9798826842912065e+77\n",
      "Gradient Descent(42/49): loss=1.8973715424703403e+79\n",
      "Gradient Descent(43/49): loss=1.8182990329373758e+81\n",
      "Gradient Descent(44/49): loss=1.7425218515064492e+83\n",
      "Gradient Descent(45/49): loss=1.6699026661595503e+85\n",
      "Gradient Descent(46/49): loss=1.6003098681580597e+87\n",
      "Gradient Descent(47/49): loss=1.5336173335262385e+89\n",
      "Gradient Descent(48/49): loss=1.469704194475348e+91\n",
      "Gradient Descent(49/49): loss=1.4084546203529775e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.056849485044625\n",
      "Gradient Descent(2/49): loss=35.58580491520976\n",
      "Gradient Descent(3/49): loss=806.4995848153333\n",
      "Gradient Descent(4/49): loss=22343.992042689293\n",
      "Gradient Descent(5/49): loss=960051.5134402856\n",
      "Gradient Descent(6/49): loss=67046105.55491751\n",
      "Gradient Descent(7/49): loss=5948242595.052302\n",
      "Gradient Descent(8/49): loss=565986449187.9204\n",
      "Gradient Descent(9/49): loss=54765340748165.984\n",
      "Gradient Descent(10/49): loss=5319350372653326.0\n",
      "Gradient Descent(11/49): loss=5.1710879657659546e+17\n",
      "Gradient Descent(12/49): loss=5.027916660487934e+19\n",
      "Gradient Descent(13/49): loss=4.888917558889165e+21\n",
      "Gradient Descent(14/49): loss=4.7538063746250246e+23\n",
      "Gradient Descent(15/49): loss=4.622438972733665e+25\n",
      "Gradient Descent(16/49): loss=4.494703929999898e+27\n",
      "Gradient Descent(17/49): loss=4.370499140861161e+29\n",
      "Gradient Descent(18/49): loss=4.249726676052896e+31\n",
      "Gradient Descent(19/49): loss=4.132291606638261e+33\n",
      "Gradient Descent(20/49): loss=4.018101690790731e+35\n",
      "Gradient Descent(21/49): loss=3.9070672495268105e+37\n",
      "Gradient Descent(22/49): loss=3.7991010850249733e+39\n",
      "Gradient Descent(23/49): loss=3.694118409703785e+41\n",
      "Gradient Descent(24/49): loss=3.592036779106304e+43\n",
      "Gradient Descent(25/49): loss=3.4927760270400835e+45\n",
      "Gradient Descent(26/49): loss=3.3962582025963257e+47\n",
      "Gradient Descent(27/49): loss=3.3024075089287944e+49\n",
      "Gradient Descent(28/49): loss=3.21115024372759e+51\n",
      "Gradient Descent(29/49): loss=3.122414741340193e+53\n",
      "Gradient Descent(30/49): loss=3.036131316490915e+55\n",
      "Gradient Descent(31/49): loss=2.952232209555967e+57\n",
      "Gradient Descent(32/49): loss=2.8706515333509753e+59\n",
      "Gradient Descent(33/49): loss=2.791325221388977e+61\n",
      "Gradient Descent(34/49): loss=2.714190977567673e+63\n",
      "Gradient Descent(35/49): loss=2.6391882272478816e+65\n",
      "Gradient Descent(36/49): loss=2.5662580696829795e+67\n",
      "Gradient Descent(37/49): loss=2.4953432317635357e+69\n",
      "Gradient Descent(38/49): loss=2.4263880230397875e+71\n",
      "Gradient Descent(39/49): loss=2.3593382919872878e+73\n",
      "Gradient Descent(40/49): loss=2.2941413834806976e+75\n",
      "Gradient Descent(41/49): loss=2.2307460974430942e+77\n",
      "Gradient Descent(42/49): loss=2.1691026486377938e+79\n",
      "Gradient Descent(43/49): loss=2.109162627571315e+81\n",
      "Gradient Descent(44/49): loss=2.050878962476595e+83\n",
      "Gradient Descent(45/49): loss=1.994205882346991e+85\n",
      "Gradient Descent(46/49): loss=1.93909888099145e+87\n",
      "Gradient Descent(47/49): loss=1.8855146820833735e+89\n",
      "Gradient Descent(48/49): loss=1.833411205175025e+91\n",
      "Gradient Descent(49/49): loss=1.7827475326509726e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.097473896453024\n",
      "Gradient Descent(2/49): loss=37.218643383627274\n",
      "Gradient Descent(3/49): loss=897.8426959628013\n",
      "Gradient Descent(4/49): loss=29601.75140324306\n",
      "Gradient Descent(5/49): loss=1645594.6063947699\n",
      "Gradient Descent(6/49): loss=135540110.21464446\n",
      "Gradient Descent(7/49): loss=12892578861.260822\n",
      "Gradient Descent(8/49): loss=1272173275107.0051\n",
      "Gradient Descent(9/49): loss=126583907388367.3\n",
      "Gradient Descent(10/49): loss=1.2618661584679526e+16\n",
      "Gradient Descent(11/49): loss=1.2584171854196526e+18\n",
      "Gradient Descent(12/49): loss=1.2550897218302684e+20\n",
      "Gradient Descent(13/49): loss=1.2517956081349328e+22\n",
      "Gradient Descent(14/49): loss=1.2485155176856424e+24\n",
      "Gradient Descent(15/49): loss=1.245245199891855e+26\n",
      "Gradient Descent(16/49): loss=1.2419837062220543e+28\n",
      "Gradient Descent(17/49): loss=1.2387308114199943e+30\n",
      "Gradient Descent(18/49): loss=1.2354864486898072e+32\n",
      "Gradient Descent(19/49): loss=1.2322505859880106e+34\n",
      "Gradient Descent(20/49): loss=1.2290231989282319e+36\n",
      "Gradient Descent(21/49): loss=1.225804264846762e+38\n",
      "Gradient Descent(22/49): loss=1.2225937615025095e+40\n",
      "Gradient Descent(23/49): loss=1.2193916667921912e+42\n",
      "Gradient Descent(24/49): loss=1.2161979586879157e+44\n",
      "Gradient Descent(25/49): loss=1.2130126152232035e+46\n",
      "Gradient Descent(26/49): loss=1.2098356144900341e+48\n",
      "Gradient Descent(27/49): loss=1.206666934637935e+50\n",
      "Gradient Descent(28/49): loss=1.2035065538736262e+52\n",
      "Gradient Descent(29/49): loss=1.2003544504609967e+54\n",
      "Gradient Descent(30/49): loss=1.1972106027208563e+56\n",
      "Gradient Descent(31/49): loss=1.19407498903072e+58\n",
      "Gradient Descent(32/49): loss=1.1909475878248635e+60\n",
      "Gradient Descent(33/49): loss=1.187828377593924e+62\n",
      "Gradient Descent(34/49): loss=1.1847173368849594e+64\n",
      "Gradient Descent(35/49): loss=1.181614444301161e+66\n",
      "Gradient Descent(36/49): loss=1.178519678501753e+68\n",
      "Gradient Descent(37/49): loss=1.1754330182018952e+70\n",
      "Gradient Descent(38/49): loss=1.1723544421724844e+72\n",
      "Gradient Descent(39/49): loss=1.1692839292400558e+74\n",
      "Gradient Descent(40/49): loss=1.1662214582864785e+76\n",
      "Gradient Descent(41/49): loss=1.1631670082490499e+78\n",
      "Gradient Descent(42/49): loss=1.1601205581201603e+80\n",
      "Gradient Descent(43/49): loss=1.1570820869473024e+82\n",
      "Gradient Descent(44/49): loss=1.154051573832755e+84\n",
      "Gradient Descent(45/49): loss=1.1510289979335867e+86\n",
      "Gradient Descent(46/49): loss=1.1480143384614458e+88\n",
      "Gradient Descent(47/49): loss=1.1450075746824214e+90\n",
      "Gradient Descent(48/49): loss=1.1420086859169257e+92\n",
      "Gradient Descent(49/49): loss=1.1390176515394867e+94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1139666041678185\n",
      "Gradient Descent(2/49): loss=37.99462625961122\n",
      "Gradient Descent(3/49): loss=912.756407854636\n",
      "Gradient Descent(4/49): loss=28694.891969976517\n",
      "Gradient Descent(5/49): loss=1463468.0218612524\n",
      "Gradient Descent(6/49): loss=112767093.95767644\n",
      "Gradient Descent(7/49): loss=10293594714.253183\n",
      "Gradient Descent(8/49): loss=984339268404.9867\n",
      "Gradient Descent(9/49): loss=95181294868038.42\n",
      "Gradient Descent(10/49): loss=9227260449864100.0\n",
      "Gradient Descent(11/49): loss=8.950534166366264e+17\n",
      "Gradient Descent(12/49): loss=8.683271315298286e+19\n",
      "Gradient Descent(13/49): loss=8.424246832895421e+21\n",
      "Gradient Descent(14/49): loss=8.173006241174273e+23\n",
      "Gradient Descent(15/49): loss=7.92927117384792e+25\n",
      "Gradient Descent(16/49): loss=7.69280756510658e+27\n",
      "Gradient Descent(17/49): loss=7.463396301487848e+29\n",
      "Gradient Descent(18/49): loss=7.240826569286197e+31\n",
      "Gradient Descent(19/49): loss=7.024894232545839e+33\n",
      "Gradient Descent(20/49): loss=6.815401329533758e+35\n",
      "Gradient Descent(21/49): loss=6.6121558212069115e+37\n",
      "Gradient Descent(22/49): loss=6.414971399661335e+39\n",
      "Gradient Descent(23/49): loss=6.223667313914829e+41\n",
      "Gradient Descent(24/49): loss=6.038068203459258e+43\n",
      "Gradient Descent(25/49): loss=5.858003937346353e+45\n",
      "Gradient Descent(26/49): loss=5.683309458199015e+47\n",
      "Gradient Descent(27/49): loss=5.51382463090096e+49\n",
      "Gradient Descent(28/49): loss=5.3493940958063376e+51\n",
      "Gradient Descent(29/49): loss=5.18986712632763e+53\n",
      "Gradient Descent(30/49): loss=5.035097490770387e+55\n",
      "Gradient Descent(31/49): loss=4.884943318289083e+57\n",
      "Gradient Descent(32/49): loss=4.739266968839824e+59\n",
      "Gradient Descent(33/49): loss=4.597934907011984e+61\n",
      "Gradient Descent(34/49): loss=4.460817579621386e+63\n",
      "Gradient Descent(35/49): loss=4.327789296954214e+65\n",
      "Gradient Descent(36/49): loss=4.1987281175531155e+67\n",
      "Gradient Descent(37/49): loss=4.073515736438137e+69\n",
      "Gradient Descent(38/49): loss=3.9520373766613765e+71\n",
      "Gradient Descent(39/49): loss=3.834181684096093e+73\n",
      "Gradient Descent(40/49): loss=3.719840625363058e+75\n",
      "Gradient Descent(41/49): loss=3.608909388800595e+77\n",
      "Gradient Descent(42/49): loss=3.5012862883881195e+79\n",
      "Gradient Descent(43/49): loss=3.3968726705352235e+81\n",
      "Gradient Descent(44/49): loss=3.2955728236496326e+83\n",
      "Gradient Descent(45/49): loss=3.197293890402664e+85\n",
      "Gradient Descent(46/49): loss=3.1019457826107003e+87\n",
      "Gradient Descent(47/49): loss=3.009441098654991e+89\n",
      "Gradient Descent(48/49): loss=2.9196950433644726e+91\n",
      "Gradient Descent(49/49): loss=2.832625350287449e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.150182769541503\n",
      "Gradient Descent(2/49): loss=38.57997978397933\n",
      "Gradient Descent(3/49): loss=914.7832513354899\n",
      "Gradient Descent(4/49): loss=27902.05534780345\n",
      "Gradient Descent(5/49): loss=1369505.990042554\n",
      "Gradient Descent(6/49): loss=103395212.12567984\n",
      "Gradient Descent(7/49): loss=9382816212.71585\n",
      "Gradient Descent(8/49): loss=896174796286.8271\n",
      "Gradient Descent(9/49): loss=86650850029263.16\n",
      "Gradient Descent(10/49): loss=8401899607305232.0\n",
      "Gradient Descent(11/49): loss=8.15194780265842e+17\n",
      "Gradient Descent(12/49): loss=7.910589242022004e+19\n",
      "Gradient Descent(13/49): loss=7.676632149878786e+21\n",
      "Gradient Descent(14/49): loss=7.449650757611518e+23\n",
      "Gradient Descent(15/49): loss=7.229393155415535e+25\n",
      "Gradient Descent(16/49): loss=7.015650471510427e+27\n",
      "Gradient Descent(17/49): loss=6.808227863790709e+29\n",
      "Gradient Descent(18/49): loss=6.606937984108239e+31\n",
      "Gradient Descent(19/49): loss=6.411599405908205e+33\n",
      "Gradient Descent(20/49): loss=6.2220361510504484e+35\n",
      "Gradient Descent(21/49): loss=6.038077462936797e+37\n",
      "Gradient Descent(22/49): loss=5.859557637614199e+39\n",
      "Gradient Descent(23/49): loss=5.686315871185572e+41\n",
      "Gradient Descent(24/49): loss=5.518196114229708e+43\n",
      "Gradient Descent(25/49): loss=5.355046931075591e+45\n",
      "Gradient Descent(26/49): loss=5.196721363359839e+47\n",
      "Gradient Descent(27/49): loss=5.043076797643814e+49\n",
      "Gradient Descent(28/49): loss=4.893974836951905e+51\n",
      "Gradient Descent(29/49): loss=4.749281176108467e+53\n",
      "Gradient Descent(30/49): loss=4.60886548076045e+55\n",
      "Gradient Descent(31/49): loss=4.472601269977931e+57\n",
      "Gradient Descent(32/49): loss=4.3403658023248635e+59\n",
      "Gradient Descent(33/49): loss=4.2120399652984356e+61\n",
      "Gradient Descent(34/49): loss=4.0875081680369315e+63\n",
      "Gradient Descent(35/49): loss=3.966658237200393e+65\n",
      "Gradient Descent(36/49): loss=3.849381315929327e+67\n",
      "Gradient Descent(37/49): loss=3.735571765790461e+69\n",
      "Gradient Descent(38/49): loss=3.625127071621837e+71\n",
      "Gradient Descent(39/49): loss=3.517947749191426e+73\n",
      "Gradient Descent(40/49): loss=3.4139372555853877e+75\n",
      "Gradient Descent(41/49): loss=3.313001902246276e+77\n",
      "Gradient Descent(42/49): loss=3.21505077058164e+79\n",
      "Gradient Descent(43/49): loss=3.119995630068765e+81\n",
      "Gradient Descent(44/49): loss=3.0277508587794047e+83\n",
      "Gradient Descent(45/49): loss=2.9382333662555175e+85\n",
      "Gradient Descent(46/49): loss=2.8513625186642935e+87\n",
      "Gradient Descent(47/49): loss=2.767060066166482e+89\n",
      "Gradient Descent(48/49): loss=2.685250072432107e+91\n",
      "Gradient Descent(49/49): loss=2.6058588462397085e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.080599563110561\n",
      "Gradient Descent(2/49): loss=36.57505715477343\n",
      "Gradient Descent(3/49): loss=840.4208486046441\n",
      "Gradient Descent(4/49): loss=23581.243715807585\n",
      "Gradient Descent(5/49): loss=1024607.0427602804\n",
      "Gradient Descent(6/49): loss=72351911.36569385\n",
      "Gradient Descent(7/49): loss=6495969941.891262\n",
      "Gradient Descent(8/49): loss=625793281749.1167\n",
      "Gradient Descent(9/49): loss=61313922834076.58\n",
      "Gradient Descent(10/49): loss=6030538758481600.0\n",
      "Gradient Descent(11/49): loss=5.936461787956868e+17\n",
      "Gradient Descent(12/49): loss=5.84498048420126e+19\n",
      "Gradient Descent(13/49): loss=5.755157352225058e+21\n",
      "Gradient Descent(14/49): loss=5.666769287713541e+23\n",
      "Gradient Descent(15/49): loss=5.579750737618189e+25\n",
      "Gradient Descent(16/49): loss=5.494071090957261e+27\n",
      "Gradient Descent(17/49): loss=5.409707678737994e+29\n",
      "Gradient Descent(18/49): loss=5.326639825238473e+31\n",
      "Gradient Descent(19/49): loss=5.244847534457004e+33\n",
      "Gradient Descent(20/49): loss=5.164311197194503e+35\n",
      "Gradient Descent(21/49): loss=5.085011522918913e+37\n",
      "Gradient Descent(22/49): loss=5.0069295211771766e+39\n",
      "Gradient Descent(23/49): loss=4.93004649397207e+41\n",
      "Gradient Descent(24/49): loss=4.854344030607513e+43\n",
      "Gradient Descent(25/49): loss=4.779804003131202e+45\n",
      "Gradient Descent(26/49): loss=4.706408561960545e+47\n",
      "Gradient Descent(27/49): loss=4.6341401316007073e+49\n",
      "Gradient Descent(28/49): loss=4.5629814064350373e+51\n",
      "Gradient Descent(29/49): loss=4.492915346580183e+53\n",
      "Gradient Descent(30/49): loss=4.423925173805727e+55\n",
      "Gradient Descent(31/49): loss=4.355994367516546e+57\n",
      "Gradient Descent(32/49): loss=4.289106660796691e+59\n",
      "Gradient Descent(33/49): loss=4.223246036513667e+61\n",
      "Gradient Descent(34/49): loss=4.158396723483546e+63\n",
      "Gradient Descent(35/49): loss=4.0945431926939786e+65\n",
      "Gradient Descent(36/49): loss=4.031670153585605e+67\n",
      "Gradient Descent(37/49): loss=3.969762550390483e+69\n",
      "Gradient Descent(38/49): loss=3.908805558526956e+71\n",
      "Gradient Descent(39/49): loss=3.84878458104957e+73\n",
      "Gradient Descent(40/49): loss=3.789685245154006e+75\n",
      "Gradient Descent(41/49): loss=3.7314933987344665e+77\n",
      "Gradient Descent(42/49): loss=3.6741951069958365e+79\n",
      "Gradient Descent(43/49): loss=3.6177766491160183e+81\n",
      "Gradient Descent(44/49): loss=3.562224514960698e+83\n",
      "Gradient Descent(45/49): loss=3.507525401848028e+85\n",
      "Gradient Descent(46/49): loss=3.453666211363065e+87\n",
      "Gradient Descent(47/49): loss=3.400634046221407e+89\n",
      "Gradient Descent(48/49): loss=3.3484162071806286e+91\n",
      "Gradient Descent(49/49): loss=3.2970001899992435e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1216319205812533\n",
      "Gradient Descent(2/49): loss=38.245531800446116\n",
      "Gradient Descent(3/49): loss=935.13397908362\n",
      "Gradient Descent(4/49): loss=31205.686198560845\n",
      "Gradient Descent(5/49): loss=1753913.49816587\n",
      "Gradient Descent(6/49): loss=146130660.0452507\n",
      "Gradient Descent(7/49): loss=14069552346.124475\n",
      "Gradient Descent(8/49): loss=1405592020020.1624\n",
      "Gradient Descent(9/49): loss=141609938140674.84\n",
      "Gradient Descent(10/49): loss=1.4293494462186608e+16\n",
      "Gradient Descent(11/49): loss=1.4433164860101773e+18\n",
      "Gradient Descent(12/49): loss=1.4575517530060705e+20\n",
      "Gradient Descent(13/49): loss=1.4719566791944186e+22\n",
      "Gradient Descent(14/49): loss=1.4865104658241496e+24\n",
      "Gradient Descent(15/49): loss=1.5012095939167834e+26\n",
      "Gradient Descent(16/49): loss=1.5160543924219167e+28\n",
      "Gradient Descent(17/49): loss=1.5310460557203733e+30\n",
      "Gradient Descent(18/49): loss=1.5461859814546841e+32\n",
      "Gradient Descent(19/49): loss=1.5614756236007438e+34\n",
      "Gradient Descent(20/49): loss=1.5769164599525308e+36\n",
      "Gradient Descent(21/49): loss=1.5925099850134286e+38\n",
      "Gradient Descent(22/49): loss=1.6082577085309722e+40\n",
      "Gradient Descent(23/49): loss=1.6241611552853318e+42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=1.6402218651576847e+44\n",
      "Gradient Descent(25/49): loss=1.6564413932614946e+46\n",
      "Gradient Descent(26/49): loss=1.6728213100893153e+48\n",
      "Gradient Descent(27/49): loss=1.6893632016639845e+50\n",
      "Gradient Descent(28/49): loss=1.706068669691911e+52\n",
      "Gradient Descent(29/49): loss=1.7229393317182706e+54\n",
      "Gradient Descent(30/49): loss=1.7399768212834287e+56\n",
      "Gradient Descent(31/49): loss=1.7571827880812487e+58\n",
      "Gradient Descent(32/49): loss=1.7745588981188147e+60\n",
      "Gradient Descent(33/49): loss=1.7921068338777082e+62\n",
      "Gradient Descent(34/49): loss=1.809828294476847e+64\n",
      "Gradient Descent(35/49): loss=1.8277249958372508e+66\n",
      "Gradient Descent(36/49): loss=1.8457986708478987e+68\n",
      "Gradient Descent(37/49): loss=1.8640510695336704e+70\n",
      "Gradient Descent(38/49): loss=1.8824839592248297e+72\n",
      "Gradient Descent(39/49): loss=1.901099124727995e+74\n",
      "Gradient Descent(40/49): loss=1.9198983684991236e+76\n",
      "Gradient Descent(41/49): loss=1.9388835108179536e+78\n",
      "Gradient Descent(42/49): loss=1.95805638996438e+80\n",
      "Gradient Descent(43/49): loss=1.9774188623961346e+82\n",
      "Gradient Descent(44/49): loss=1.9969728029288755e+84\n",
      "Gradient Descent(45/49): loss=2.0167201049176635e+86\n",
      "Gradient Descent(46/49): loss=2.0366626804401095e+88\n",
      "Gradient Descent(47/49): loss=2.056802460481604e+90\n",
      "Gradient Descent(48/49): loss=2.077141395122498e+92\n",
      "Gradient Descent(49/49): loss=2.0976814537265172e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1383425227412567\n",
      "Gradient Descent(2/49): loss=39.04230047174479\n",
      "Gradient Descent(3/49): loss=950.7025470189155\n",
      "Gradient Descent(4/49): loss=30256.75691674971\n",
      "Gradient Descent(5/49): loss=1560295.6743520442\n",
      "Gradient Descent(6/49): loss=121604317.10803033\n",
      "Gradient Descent(7/49): loss=11234998733.51765\n",
      "Gradient Descent(8/49): loss=1087721662328.7778\n",
      "Gradient Descent(9/49): loss=106495159832893.45\n",
      "Gradient Descent(10/49): loss=1.0453619459143108e+16\n",
      "Gradient Descent(11/49): loss=1.0267417134268865e+18\n",
      "Gradient Descent(12/49): loss=1.008589999565033e+20\n",
      "Gradient Descent(13/49): loss=9.907899193990641e+21\n",
      "Gradient Descent(14/49): loss=9.73310883334589e+23\n",
      "Gradient Descent(15/49): loss=9.561417530768937e+25\n",
      "Gradient Descent(16/49): loss=9.39275832777677e+27\n",
      "Gradient Descent(17/49): loss=9.227074979867775e+29\n",
      "Gradient Descent(18/49): loss=9.064314374941223e+31\n",
      "Gradient Descent(19/49): loss=8.904424818289973e+33\n",
      "Gradient Descent(20/49): loss=8.747355634959089e+35\n",
      "Gradient Descent(21/49): loss=8.593057068093926e+37\n",
      "Gradient Descent(22/49): loss=8.441480243961948e+39\n",
      "Gradient Descent(23/49): loss=8.292577152157106e+41\n",
      "Gradient Descent(24/49): loss=8.146300629425418e+43\n",
      "Gradient Descent(25/49): loss=8.002604344507658e+45\n",
      "Gradient Descent(26/49): loss=7.861442783417033e+47\n",
      "Gradient Descent(27/49): loss=7.722771235011366e+49\n",
      "Gradient Descent(28/49): loss=7.58654577682967e+51\n",
      "Gradient Descent(29/49): loss=7.452723261178876e+53\n",
      "Gradient Descent(30/49): loss=7.321261301467784e+55\n",
      "Gradient Descent(31/49): loss=7.192118258781246e+57\n",
      "Gradient Descent(32/49): loss=7.065253228692372e+59\n",
      "Gradient Descent(33/49): loss=6.940626028305328e+61\n",
      "Gradient Descent(34/49): loss=6.818197183529177e+63\n",
      "Gradient Descent(35/49): loss=6.697927916573668e+65\n",
      "Gradient Descent(36/49): loss=6.579780133667984e+67\n",
      "Gradient Descent(37/49): loss=6.463716412994707e+69\n",
      "Gradient Descent(38/49): loss=6.349699992836435e+71\n",
      "Gradient Descent(39/49): loss=6.237694759932651e+73\n",
      "Gradient Descent(40/49): loss=6.1276652380406766e+75\n",
      "Gradient Descent(41/49): loss=6.0195765766995e+77\n",
      "Gradient Descent(42/49): loss=5.913394540191177e+79\n",
      "Gradient Descent(43/49): loss=5.809085496697294e+81\n",
      "Gradient Descent(44/49): loss=5.706616407645818e+83\n",
      "Gradient Descent(45/49): loss=5.605954817247491e+85\n",
      "Gradient Descent(46/49): loss=5.507068842215264e+87\n",
      "Gradient Descent(47/49): loss=5.409927161665595e+89\n",
      "Gradient Descent(48/49): loss=5.314499007198616e+91\n",
      "Gradient Descent(49/49): loss=5.220754153151966e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.175088211140324\n",
      "Gradient Descent(2/49): loss=39.644617142390594\n",
      "Gradient Descent(3/49): loss=952.8660518609843\n",
      "Gradient Descent(4/49): loss=29424.774510860716\n",
      "Gradient Descent(5/49): loss=1460314.818499481\n",
      "Gradient Descent(6/49): loss=111504328.62817745\n",
      "Gradient Descent(7/49): loss=10241085907.653519\n",
      "Gradient Descent(8/49): loss=990300624409.9613\n",
      "Gradient Descent(9/49): loss=96950703665540.75\n",
      "Gradient Descent(10/49): loss=9518551253276396.0\n",
      "Gradient Descent(11/49): loss=9.351320615688777e+17\n",
      "Gradient Descent(12/49): loss=9.188388292796e+19\n",
      "Gradient Descent(13/49): loss=9.028599212470212e+21\n",
      "Gradient Descent(14/49): loss=8.87165702499317e+23\n",
      "Gradient Descent(15/49): loss=8.717458168210756e+25\n",
      "Gradient Descent(16/49): loss=8.565942861403226e+27\n",
      "Gradient Descent(17/49): loss=8.417061755718942e+29\n",
      "Gradient Descent(18/49): loss=8.270768461553493e+31\n",
      "Gradient Descent(19/49): loss=8.127017865773156e+33\n",
      "Gradient Descent(20/49): loss=7.985765744535619e+35\n",
      "Gradient Descent(21/49): loss=7.846968666170785e+37\n",
      "Gradient Descent(22/49): loss=7.710583959142111e+39\n",
      "Gradient Descent(23/49): loss=7.576569694745799e+41\n",
      "Gradient Descent(24/49): loss=7.444884673286401e+43\n",
      "Gradient Descent(25/49): loss=7.315488411201171e+45\n",
      "Gradient Descent(26/49): loss=7.1883411285675724e+47\n",
      "Gradient Descent(27/49): loss=7.063403736863178e+49\n",
      "Gradient Descent(28/49): loss=6.940637826946965e+51\n",
      "Gradient Descent(29/49): loss=6.820005657249761e+53\n",
      "Gradient Descent(30/49): loss=6.701470142172758e+55\n",
      "Gradient Descent(31/49): loss=6.5849948406849e+57\n",
      "Gradient Descent(32/49): loss=6.470543945121373e+59\n",
      "Gradient Descent(33/49): loss=6.358082270174261e+61\n",
      "Gradient Descent(34/49): loss=6.247575242075816e+63\n",
      "Gradient Descent(35/49): loss=6.138988887969964e+65\n",
      "Gradient Descent(36/49): loss=6.032289825467725e+67\n",
      "Gradient Descent(37/49): loss=5.9274452523849085e+69\n",
      "Gradient Descent(38/49): loss=5.824422936657575e+71\n",
      "Gradient Descent(39/49): loss=5.72319120643313e+73\n",
      "Gradient Descent(40/49): loss=5.623718940333331e+75\n",
      "Gradient Descent(41/49): loss=5.525975557887275e+77\n",
      "Gradient Descent(42/49): loss=5.429931010129249e+79\n",
      "Gradient Descent(43/49): loss=5.335555770361654e+81\n",
      "Gradient Descent(44/49): loss=5.242820825077472e+83\n",
      "Gradient Descent(45/49): loss=5.15169766504067e+85\n",
      "Gradient Descent(46/49): loss=5.062158276521513e+87\n",
      "Gradient Descent(47/49): loss=4.9741751326846664e+89\n",
      "Gradient Descent(48/49): loss=4.887721185126812e+91\n",
      "Gradient Descent(49/49): loss=4.802769855560581e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.104518434304841\n",
      "Gradient Descent(2/49): loss=37.58502246547141\n",
      "Gradient Descent(3/49): loss=875.524271588411\n",
      "Gradient Descent(4/49): loss=24878.14848571883\n",
      "Gradient Descent(5/49): loss=1093052.6258613348\n",
      "Gradient Descent(6/49): loss=78040526.8782603\n",
      "Gradient Descent(7/49): loss=7090241270.622833\n",
      "Gradient Descent(8/49): loss=691486732759.2527\n",
      "Gradient Descent(9/49): loss=68597233180578.99\n",
      "Gradient Descent(10/49): loss=6831465031934503.0\n",
      "Gradient Descent(11/49): loss=6.809256149093683e+17\n",
      "Gradient Descent(12/49): loss=6.788445354408412e+19\n",
      "Gradient Descent(13/49): loss=6.767994210567483e+21\n",
      "Gradient Descent(14/49): loss=6.747670768169436e+23\n",
      "Gradient Descent(15/49): loss=6.727423107664603e+25\n",
      "Gradient Descent(16/49): loss=6.707239497391928e+27\n",
      "Gradient Descent(17/49): loss=6.687117177126956e+29\n",
      "Gradient Descent(18/49): loss=6.667055389734437e+31\n",
      "Gradient Descent(19/49): loss=6.647053825644286e+33\n",
      "Gradient Descent(20/49): loss=6.627112275617873e+35\n",
      "Gradient Descent(21/49): loss=6.607230553232653e+37\n",
      "Gradient Descent(22/49): loss=6.58740847757858e+39\n",
      "Gradient Descent(23/49): loss=6.567645869394434e+41\n",
      "Gradient Descent(24/49): loss=6.547942550203637e+43\n",
      "Gradient Descent(25/49): loss=6.52829834211948e+45\n",
      "Gradient Descent(26/49): loss=6.508713067802014e+47\n",
      "Gradient Descent(27/49): loss=6.489186550445744e+49\n",
      "Gradient Descent(28/49): loss=6.469718613775872e+51\n",
      "Gradient Descent(29/49): loss=6.4503090820471535e+53\n",
      "Gradient Descent(30/49): loss=6.430957780041366e+55\n",
      "Gradient Descent(31/49): loss=6.411664533065862e+57\n",
      "Gradient Descent(32/49): loss=6.392429166952154e+59\n",
      "Gradient Descent(33/49): loss=6.373251508054426e+61\n",
      "Gradient Descent(34/49): loss=6.3541313832477054e+63\n",
      "Gradient Descent(35/49): loss=6.335068619926242e+65\n",
      "Gradient Descent(36/49): loss=6.316063046002317e+67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=6.297114489904423e+69\n",
      "Gradient Descent(38/49): loss=6.278222780575571e+71\n",
      "Gradient Descent(39/49): loss=6.259387747472346e+73\n",
      "Gradient Descent(40/49): loss=6.2406092205627685e+75\n",
      "Gradient Descent(41/49): loss=6.221887030324827e+77\n",
      "Gradient Descent(42/49): loss=6.203221007745385e+79\n",
      "Gradient Descent(43/49): loss=6.184610984318284e+81\n",
      "Gradient Descent(44/49): loss=6.166056792042542e+83\n",
      "Gradient Descent(45/49): loss=6.14755826342173e+85\n",
      "Gradient Descent(46/49): loss=6.129115231461493e+87\n",
      "Gradient Descent(47/49): loss=6.110727529668772e+89\n",
      "Gradient Descent(48/49): loss=6.092394992049577e+91\n",
      "Gradient Descent(49/49): loss=6.074117453108383e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.145960356776131\n",
      "Gradient Descent(2/49): loss=39.29375349030431\n",
      "Gradient Descent(3/49): loss=973.7092754115055\n",
      "Gradient Descent(4/49): loss=32885.34115130743\n",
      "Gradient Descent(5/49): loss=1868619.4397594065\n",
      "Gradient Descent(6/49): loss=157475351.52874112\n",
      "Gradient Descent(7/49): loss=15345671996.688158\n",
      "Gradient Descent(8/49): loss=1552044313678.4478\n",
      "Gradient Descent(9/49): loss=158309619232874.38\n",
      "Gradient Descent(10/49): loss=1.6178131648476082e+16\n",
      "Gradient Descent(11/49): loss=1.6539783093094743e+18\n",
      "Gradient Descent(12/49): loss=1.691106594789977e+20\n",
      "Gradient Descent(13/49): loss=1.729103157605895e+22\n",
      "Gradient Descent(14/49): loss=1.7679612846039839e+24\n",
      "Gradient Descent(15/49): loss=1.8076944349270284e+26\n",
      "Gradient Descent(16/49): loss=1.8483209450842878e+28\n",
      "Gradient Descent(17/49): loss=1.8898605935222233e+30\n",
      "Gradient Descent(18/49): loss=1.932333835016988e+32\n",
      "Gradient Descent(19/49): loss=1.975761636236186e+34\n",
      "Gradient Descent(20/49): loss=2.0201654468051751e+36\n",
      "Gradient Descent(21/49): loss=2.065567201058576e+38\n",
      "Gradient Descent(22/49): loss=2.1119893268844472e+40\n",
      "Gradient Descent(23/49): loss=2.1594547563554764e+42\n",
      "Gradient Descent(24/49): loss=2.2079869369562267e+44\n",
      "Gradient Descent(25/49): loss=2.2576098431432476e+46\n",
      "Gradient Descent(26/49): loss=2.308347988183189e+48\n",
      "Gradient Descent(27/49): loss=2.3602264362608012e+50\n",
      "Gradient Descent(28/49): loss=2.413270814860516e+52\n",
      "Gradient Descent(29/49): loss=2.467507327425811e+54\n",
      "Gradient Descent(30/49): loss=2.522962766303547e+56\n",
      "Gradient Descent(31/49): loss=2.579664525979177e+58\n",
      "Gradient Descent(32/49): loss=2.6376406166093313e+60\n",
      "Gradient Descent(33/49): loss=2.696919677858708e+62\n",
      "Gradient Descent(34/49): loss=2.7575309930476366e+64\n",
      "Gradient Descent(35/49): loss=2.8195045036178837e+66\n",
      "Gradient Descent(36/49): loss=2.882870823923294e+68\n",
      "Gradient Descent(37/49): loss=2.947661256353429e+70\n",
      "Gradient Descent(38/49): loss=3.0139078067960142e+72\n",
      "Gradient Descent(39/49): loss=3.0816432004481397e+74\n",
      "Gradient Descent(40/49): loss=3.1509008979818945e+76\n",
      "Gradient Descent(41/49): loss=3.2217151120738338e+78\n",
      "Gradient Descent(42/49): loss=3.2941208243054877e+80\n",
      "Gradient Descent(43/49): loss=3.3681538024440894e+82\n",
      "Gradient Descent(44/49): loss=3.443850618111519e+84\n",
      "Gradient Descent(45/49): loss=3.5212486648504087e+86\n",
      "Gradient Descent(46/49): loss=3.600386176596041e+88\n",
      "Gradient Descent(47/49): loss=3.6813022465639215e+90\n",
      "Gradient Descent(48/49): loss=3.764036846561049e+92\n",
      "Gradient Descent(49/49): loss=3.8486308467319484e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1628902566116106\n",
      "Gradient Descent(2/49): loss=40.1117207420035\n",
      "Gradient Descent(3/49): loss=989.9562108545912\n",
      "Gradient Descent(4/49): loss=31892.67307884212\n",
      "Gradient Descent(5/49): loss=1662862.7558958689\n",
      "Gradient Descent(6/49): loss=131072759.13644232\n",
      "Gradient Descent(7/49): loss=12255850526.169458\n",
      "Gradient Descent(8/49): loss=1201218102069.248\n",
      "Gradient Descent(9/49): loss=119070918088939.86\n",
      "Gradient Descent(10/49): loss=1.18338094404822e+16\n",
      "Gradient Descent(11/49): loss=1.1768031840321772e+18\n",
      "Gradient Descent(12/49): loss=1.1704226527573552e+20\n",
      "Gradient Descent(13/49): loss=1.1641132901397711e+22\n",
      "Gradient Descent(14/49): loss=1.157846263721389e+24\n",
      "Gradient Descent(15/49): loss=1.1516148706227375e+26\n",
      "Gradient Descent(16/49): loss=1.145417445375502e+28\n",
      "Gradient Descent(17/49): loss=1.1392534697708073e+30\n",
      "Gradient Descent(18/49): loss=1.1331226874630427e+32\n",
      "Gradient Descent(19/49): loss=1.1270249024510483e+34\n",
      "Gradient Descent(20/49): loss=1.1209599332088321e+36\n",
      "Gradient Descent(21/49): loss=1.1149276022416027e+38\n",
      "Gradient Descent(22/49): loss=1.1089277337050444e+40\n",
      "Gradient Descent(23/49): loss=1.1029601528592434e+42\n",
      "Gradient Descent(24/49): loss=1.0970246859407443e+44\n",
      "Gradient Descent(25/49): loss=1.091121160129286e+46\n",
      "Gradient Descent(26/49): loss=1.0852494035365663e+48\n",
      "Gradient Descent(27/49): loss=1.0794092451996073e+50\n",
      "Gradient Descent(28/49): loss=1.0736005150756581e+52\n",
      "Gradient Descent(29/49): loss=1.0678230440370166e+54\n",
      "Gradient Descent(30/49): loss=1.062076663866064e+56\n",
      "Gradient Descent(31/49): loss=1.0563612072505288e+58\n",
      "Gradient Descent(32/49): loss=1.0506765077784675e+60\n",
      "Gradient Descent(33/49): loss=1.0450223999334563e+62\n",
      "Gradient Descent(34/49): loss=1.0393987190898007e+64\n",
      "Gradient Descent(35/49): loss=1.0338053015077033e+66\n",
      "Gradient Descent(36/49): loss=1.028241984328528e+68\n",
      "Gradient Descent(37/49): loss=1.0227086055700552e+70\n",
      "Gradient Descent(38/49): loss=1.0172050041217465e+72\n",
      "Gradient Descent(39/49): loss=1.0117310197400793e+74\n",
      "Gradient Descent(40/49): loss=1.0062864930438307e+76\n",
      "Gradient Descent(41/49): loss=1.000871265509471e+78\n",
      "Gradient Descent(42/49): loss=9.95485179466575e+79\n",
      "Gradient Descent(43/49): loss=9.901280780932075e+81\n",
      "Gradient Descent(44/49): loss=9.847998054113077e+83\n",
      "Gradient Descent(45/49): loss=9.795002062822745e+85\n",
      "Gradient Descent(46/49): loss=9.742291264022742e+87\n",
      "Gradient Descent(47/49): loss=9.689864122979206e+89\n",
      "Gradient Descent(48/49): loss=9.637719113217426e+91\n",
      "Gradient Descent(49/49): loss=9.585854716476423e+93\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.200169126220903\n",
      "Gradient Descent(2/49): loss=40.73136510180737\n",
      "Gradient Descent(3/49): loss=992.2627824163629\n",
      "Gradient Descent(4/49): loss=31019.874198058922\n",
      "Gradient Descent(5/49): loss=1556518.52893405\n",
      "Gradient Descent(6/49): loss=120193148.2430274\n",
      "Gradient Descent(7/49): loss=11171802451.407347\n",
      "Gradient Descent(8/49): loss=1093635262141.2578\n",
      "Gradient Descent(9/49): loss=108399350950884.84\n",
      "Gradient Descent(10/49): loss=1.0775271045716802e+16\n",
      "Gradient Descent(11/49): loss=1.0718028065707955e+18\n",
      "Gradient Descent(12/49): loss=1.0662686228596274e+20\n",
      "Gradient Descent(13/49): loss=1.0607992442361255e+22\n",
      "Gradient Descent(14/49): loss=1.0553661382075643e+24\n",
      "Gradient Descent(15/49): loss=1.0499627228454295e+26\n",
      "Gradient Descent(16/49): loss=1.044587395414512e+28\n",
      "Gradient Descent(17/49): loss=1.0392396830778933e+30\n",
      "Gradient Descent(18/49): loss=1.0339193698316177e+32\n",
      "Gradient Descent(19/49): loss=1.0286262984811546e+34\n",
      "Gradient Descent(20/49): loss=1.0233603257245195e+36\n",
      "Gradient Descent(21/49): loss=1.0181213119617379e+38\n",
      "Gradient Descent(22/49): loss=1.012909118980863e+40\n",
      "Gradient Descent(23/49): loss=1.0077236094301843e+42\n",
      "Gradient Descent(24/49): loss=1.002564646695801e+44\n",
      "Gradient Descent(25/49): loss=9.974320948710498e+45\n",
      "Gradient Descent(26/49): loss=9.923258187468026e+47\n",
      "Gradient Descent(27/49): loss=9.872456838065237e+49\n",
      "Gradient Descent(28/49): loss=9.821915562224365e+51\n",
      "Gradient Descent(29/49): loss=9.771633028518674e+53\n",
      "Gradient Descent(30/49): loss=9.721607912338328e+55\n",
      "Gradient Descent(31/49): loss=9.671838895853979e+57\n",
      "Gradient Descent(32/49): loss=9.62232466798323e+59\n",
      "Gradient Descent(33/49): loss=9.573063924355534e+61\n",
      "Gradient Descent(34/49): loss=9.524055367277783e+63\n",
      "Gradient Descent(35/49): loss=9.475297705700907e+65\n",
      "Gradient Descent(36/49): loss=9.426789655184908e+67\n",
      "Gradient Descent(37/49): loss=9.378529937864902e+69\n",
      "Gradient Descent(38/49): loss=9.330517282418725e+71\n",
      "Gradient Descent(39/49): loss=9.282750424032298e+73\n",
      "Gradient Descent(40/49): loss=9.235228104366522e+75\n",
      "Gradient Descent(41/49): loss=9.1879490715247e+77\n",
      "Gradient Descent(42/49): loss=9.140912080018311e+79\n",
      "Gradient Descent(43/49): loss=9.09411589073607e+81\n",
      "Gradient Descent(44/49): loss=9.047559270909533e+83\n",
      "Gradient Descent(45/49): loss=9.001240994081597e+85\n",
      "Gradient Descent(46/49): loss=8.955159840073612e+87\n",
      "Gradient Descent(47/49): loss=8.909314594953707e+89\n",
      "Gradient Descent(48/49): loss=8.863704051004661e+91\n",
      "Gradient Descent(49/49): loss=8.818327006692022e+93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.128606098627465\n",
      "Gradient Descent(2/49): loss=38.61598820634881\n",
      "Gradient Descent(3/49): loss=911.842609996505\n",
      "Gradient Descent(4/49): loss=26237.159311778672\n",
      "Gradient Descent(5/49): loss=1165595.747325676\n",
      "Gradient Descent(6/49): loss=84136927.32765324\n",
      "Gradient Descent(7/49): loss=7734685164.042101\n",
      "Gradient Descent(8/49): loss=763604127931.1317\n",
      "Gradient Descent(9/49): loss=76692354644415.34\n",
      "Gradient Descent(10/49): loss=7732786466197149.0\n",
      "Gradient Descent(11/49): loss=7.803735527206378e+17\n",
      "Gradient Descent(12/49): loss=7.876892277546787e+19\n",
      "Gradient Descent(13/49): loss=7.951087207758967e+21\n",
      "Gradient Descent(14/49): loss=8.026060746031696e+23\n",
      "Gradient Descent(15/49): loss=8.101759281068223e+25\n",
      "Gradient Descent(16/49): loss=8.17817585742801e+27\n",
      "Gradient Descent(17/49): loss=8.255314126407854e+29\n",
      "Gradient Descent(18/49): loss=8.3331801888253e+31\n",
      "Gradient Descent(19/49): loss=8.411780749559471e+33\n",
      "Gradient Descent(20/49): loss=8.491122700404455e+35\n",
      "Gradient Descent(21/49): loss=8.571213026145952e+37\n",
      "Gradient Descent(22/49): loss=8.652058783784851e+39\n",
      "Gradient Descent(23/49): loss=8.733667098318948e+41\n",
      "Gradient Descent(24/49): loss=8.816045162274745e+43\n",
      "Gradient Descent(25/49): loss=8.899200236093421e+45\n",
      "Gradient Descent(26/49): loss=8.983139648715725e+47\n",
      "Gradient Descent(27/49): loss=9.067870798214119e+49\n",
      "Gradient Descent(28/49): loss=9.15340115244229e+51\n",
      "Gradient Descent(29/49): loss=9.239738249692952e+53\n",
      "Gradient Descent(30/49): loss=9.326889699361517e+55\n",
      "Gradient Descent(31/49): loss=9.414863182617631e+57\n",
      "Gradient Descent(32/49): loss=9.503666453080755e+59\n",
      "Gradient Descent(33/49): loss=9.593307337505322e+61\n",
      "Gradient Descent(34/49): loss=9.683793736469054e+63\n",
      "Gradient Descent(35/49): loss=9.7751336250697e+65\n",
      "Gradient Descent(36/49): loss=9.867335053628193e+67\n",
      "Gradient Descent(37/49): loss=9.960406148398562e+69\n",
      "Gradient Descent(38/49): loss=1.005435511228308e+72\n",
      "Gradient Descent(39/49): loss=1.0149190225555775e+74\n",
      "Gradient Descent(40/49): loss=1.0244919846592296e+76\n",
      "Gradient Descent(41/49): loss=1.034155241260697e+78\n",
      "Gradient Descent(42/49): loss=1.0439096440394946e+80\n",
      "Gradient Descent(43/49): loss=1.0537560527085159e+82\n",
      "Gradient Descent(44/49): loss=1.063695335089573e+84\n",
      "Gradient Descent(45/49): loss=1.073728367190022e+86\n",
      "Gradient Descent(46/49): loss=1.0838560332799185e+88\n",
      "Gradient Descent(47/49): loss=1.094079225969998e+90\n",
      "Gradient Descent(48/49): loss=1.1043988462902944e+92\n",
      "Gradient Descent(49/49): loss=1.1148158037695567e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1704592050376568\n",
      "Gradient Descent(2/49): loss=40.363602149633614\n",
      "Gradient Descent(3/49): loss=1013.6037546627952\n",
      "Gradient Descent(4/49): loss=34643.76704698094\n",
      "Gradient Descent(5/49): loss=1990045.6731350364\n",
      "Gradient Descent(6/49): loss=169622725.1276881\n",
      "Gradient Descent(7/49): loss=16728599306.445347\n",
      "Gradient Descent(8/49): loss=1712710934722.4202\n",
      "Gradient Descent(9/49): loss=176857272939928.22\n",
      "Gradient Descent(10/49): loss=1.8297310635986948e+16\n",
      "Gradient Descent(11/49): loss=1.8937990371284664e+18\n",
      "Gradient Descent(12/49): loss=1.9602917904340687e+20\n",
      "Gradient Descent(13/49): loss=2.029160562064848e+22\n",
      "Gradient Descent(14/49): loss=2.10045827266123e+24\n",
      "Gradient Descent(15/49): loss=2.1742632950693903e+26\n",
      "Gradient Descent(16/49): loss=2.2506621393586135e+28\n",
      "Gradient Descent(17/49): loss=2.329745583752945e+30\n",
      "Gradient Descent(18/49): loss=2.4116078765202426e+32\n",
      "Gradient Descent(19/49): loss=2.496346641568419e+34\n",
      "Gradient Descent(20/49): loss=2.5840629476952278e+36\n",
      "Gradient Descent(21/49): loss=2.674861418370786e+38\n",
      "Gradient Descent(22/49): loss=2.7688503540711473e+40\n",
      "Gradient Descent(23/49): loss=2.86614186089384e+42\n",
      "Gradient Descent(24/49): loss=2.966851984145561e+44\n",
      "Gradient Descent(25/49): loss=3.0711008467270174e+46\n",
      "Gradient Descent(26/49): loss=3.179012792403969e+48\n",
      "Gradient Descent(27/49): loss=3.290716534117896e+50\n",
      "Gradient Descent(28/49): loss=3.4063453075091624e+52\n",
      "Gradient Descent(29/49): loss=3.5260370298349924e+54\n",
      "Gradient Descent(30/49): loss=3.6499344644712662e+56\n",
      "Gradient Descent(31/49): loss=3.7781853911950407e+58\n",
      "Gradient Descent(32/49): loss=3.910942782449939e+60\n",
      "Gradient Descent(33/49): loss=4.048364985805965e+62\n",
      "Gradient Descent(34/49): loss=4.190615912829306e+64\n",
      "Gradient Descent(35/49): loss=4.3378652345898146e+66\n",
      "Gradient Descent(36/49): loss=4.490288584037238e+68\n",
      "Gradient Descent(37/49): loss=4.648067765489701e+70\n",
      "Gradient Descent(38/49): loss=4.81139097148183e+72\n",
      "Gradient Descent(39/49): loss=4.980453007233145e+74\n",
      "Gradient Descent(40/49): loss=5.155455523004082e+76\n",
      "Gradient Descent(41/49): loss=5.3366072546156394e+78\n",
      "Gradient Descent(42/49): loss=5.524124272421606e+80\n",
      "Gradient Descent(43/49): loss=5.7182302390276465e+82\n",
      "Gradient Descent(44/49): loss=5.919156676067252e+84\n",
      "Gradient Descent(45/49): loss=6.127143240351668e+86\n",
      "Gradient Descent(46/49): loss=6.342438009721734e+88\n",
      "Gradient Descent(47/49): loss=6.565297778945674e+90\n",
      "Gradient Descent(48/49): loss=6.795988366013159e+92\n",
      "Gradient Descent(49/49): loss=7.0347849291923e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.18760980577888\n",
      "Gradient Descent(2/49): loss=41.20318617635087\n",
      "Gradient Descent(3/49): loss=1030.553233188264\n",
      "Gradient Descent(4/49): loss=33605.6360732466\n",
      "Gradient Descent(5/49): loss=1771470.3121691688\n",
      "Gradient Descent(6/49): loss=141213177.73771262\n",
      "Gradient Descent(7/49): loss=13362300888.731192\n",
      "Gradient Descent(8/49): loss=1325746212106.1323\n",
      "Gradient Descent(9/49): loss=133040182913735.34\n",
      "Gradient Descent(10/49): loss=1.3385992446753126e+16\n",
      "Gradient Descent(11/49): loss=1.3476632795760584e+18\n",
      "Gradient Descent(12/49): loss=1.3569771154606902e+20\n",
      "Gradient Descent(13/49): loss=1.3663987982426077e+22\n",
      "Gradient Descent(14/49): loss=1.3758959279847423e+24\n",
      "Gradient Descent(15/49): loss=1.3854613815238327e+26\n",
      "Gradient Descent(16/49): loss=1.3950938695461608e+28\n",
      "Gradient Descent(17/49): loss=1.4047934510813217e+30\n",
      "Gradient Descent(18/49): loss=1.4145604987032165e+32\n",
      "Gradient Descent(19/49): loss=1.4243954598167619e+34\n",
      "Gradient Descent(20/49): loss=1.4342988016036554e+36\n",
      "Gradient Descent(21/49): loss=1.4442709983387093e+38\n",
      "Gradient Descent(22/49): loss=1.454312528481193e+40\n",
      "Gradient Descent(23/49): loss=1.464423874021554e+42\n",
      "Gradient Descent(24/49): loss=1.4746055203486373e+44\n",
      "Gradient Descent(25/49): loss=1.48485795623688e+46\n",
      "Gradient Descent(26/49): loss=1.4951816738614965e+48\n",
      "Gradient Descent(27/49): loss=1.5055771688202475e+50\n",
      "Gradient Descent(28/49): loss=1.516044940156738e+52\n",
      "Gradient Descent(29/49): loss=1.5265854903843184e+54\n",
      "Gradient Descent(30/49): loss=1.537199325510084e+56\n",
      "Gradient Descent(31/49): loss=1.547886955059284e+58\n",
      "Gradient Descent(32/49): loss=1.5586488920996881e+60\n",
      "Gradient Descent(33/49): loss=1.5694856532662859e+62\n",
      "Gradient Descent(34/49): loss=1.5803977587860183e+64\n",
      "Gradient Descent(35/49): loss=1.5913857325028398e+66\n",
      "Gradient Descent(36/49): loss=1.6024501019028198e+68\n",
      "Gradient Descent(37/49): loss=1.613591398139417e+70\n",
      "Gradient Descent(38/49): loss=1.6248101560590298e+72\n",
      "Gradient Descent(39/49): loss=1.6361069142266692e+74\n",
      "Gradient Descent(40/49): loss=1.6474822149518e+76\n",
      "Gradient Descent(41/49): loss=1.6589366043143645e+78\n",
      "Gradient Descent(42/49): loss=1.6704706321910995e+80\n",
      "Gradient Descent(43/49): loss=1.6820848522817187e+82\n",
      "Gradient Descent(44/49): loss=1.693779822135728e+84\n",
      "Gradient Descent(45/49): loss=1.7055561031790956e+86\n",
      "Gradient Descent(46/49): loss=1.7174142607410898e+88\n",
      "Gradient Descent(47/49): loss=1.7293548640816707e+90\n",
      "Gradient Descent(48/49): loss=1.7413784864185028e+92\n",
      "Gradient Descent(49/49): loss=1.7534857049547537e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2254255147832387\n",
      "Gradient Descent(2/49): loss=41.84052794465855\n",
      "Gradient Descent(3/49): loss=1033.0094961095226\n",
      "Gradient Descent(4/49): loss=32690.289721638335\n",
      "Gradient Descent(5/49): loss=1658400.7590155106\n",
      "Gradient Descent(6/49): loss=129499136.58846298\n",
      "Gradient Descent(7/49): loss=12180575914.586176\n",
      "Gradient Descent(8/49): loss=1207014239666.878\n",
      "Gradient Descent(9/49): loss=121116609028900.89\n",
      "Gradient Descent(10/49): loss=1.2188595964266424e+16\n",
      "Gradient Descent(11/49): loss=1.227415980567578e+18\n",
      "Gradient Descent(12/49): loss=1.2362197353949466e+20\n",
      "Gradient Descent(13/49): loss=1.2451297067379322e+22\n",
      "Gradient Descent(14/49): loss=1.25411379873361e+24\n",
      "Gradient Descent(15/49): loss=1.2631649911121578e+26\n",
      "Gradient Descent(16/49): loss=1.2722820311958938e+28\n",
      "Gradient Descent(17/49): loss=1.281464994913374e+30\n",
      "Gradient Descent(18/49): loss=1.2907142662732517e+32\n",
      "Gradient Descent(19/49): loss=1.300030302758934e+34\n",
      "Gradient Descent(20/49): loss=1.3094135814108222e+36\n",
      "Gradient Descent(21/49): loss=1.318864586449659e+38\n",
      "Gradient Descent(22/49): loss=1.328383806450343e+40\n",
      "Gradient Descent(23/49): loss=1.3379717337116524e+42\n",
      "Gradient Descent(24/49): loss=1.3476288641310247e+44\n",
      "Gradient Descent(25/49): loss=1.357355697195718e+46\n",
      "Gradient Descent(26/49): loss=1.3671527360004895e+48\n",
      "Gradient Descent(27/49): loss=1.37702048727188e+50\n",
      "Gradient Descent(28/49): loss=1.386959461393968e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(29/49): loss=1.3969701724346055e+54\n",
      "Gradient Descent(30/49): loss=1.4070531381721894e+56\n",
      "Gradient Descent(31/49): loss=1.4172088801222258e+58\n",
      "Gradient Descent(32/49): loss=1.4274379235644333e+60\n",
      "Gradient Descent(33/49): loss=1.4377407975697843e+62\n",
      "Gradient Descent(34/49): loss=1.448118035028031e+64\n",
      "Gradient Descent(35/49): loss=1.4585701726751629e+66\n",
      "Gradient Descent(36/49): loss=1.4690977511210948e+68\n",
      "Gradient Descent(37/49): loss=1.4797013148778806e+70\n",
      "Gradient Descent(38/49): loss=1.4903814123876254e+72\n",
      "Gradient Descent(39/49): loss=1.5011385960509177e+74\n",
      "Gradient Descent(40/49): loss=1.5119734222555213e+76\n",
      "Gradient Descent(41/49): loss=1.5228864514050416e+78\n",
      "Gradient Descent(42/49): loss=1.5338782479478354e+80\n",
      "Gradient Descent(43/49): loss=1.5449493804064136e+82\n",
      "Gradient Descent(44/49): loss=1.5561004214066543e+84\n",
      "Gradient Descent(45/49): loss=1.567331947707593e+86\n",
      "Gradient Descent(46/49): loss=1.5786445402310398e+88\n",
      "Gradient Descent(47/49): loss=1.5900387840918196e+90\n",
      "Gradient Descent(48/49): loss=1.6015152686280178e+92\n",
      "Gradient Descent(49/49): loss=1.6130745874312861e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1528625560784325\n",
      "Gradient Descent(2/49): loss=39.66824371867203\n",
      "Gradient Descent(3/49): loss=949.4092966188267\n",
      "Gradient Descent(4/49): loss=27660.812783619942\n",
      "Gradient Descent(5/49): loss=1242453.5344568759\n",
      "Gradient Descent(6/49): loss=90667546.8033464\n",
      "Gradient Descent(7/49): loss=8433187948.260101\n",
      "Gradient Descent(8/49): loss=842728047852.7574\n",
      "Gradient Descent(9/49): loss=85683902934311.4\n",
      "Gradient Descent(10/49): loss=8746347937641471.0\n",
      "Gradient Descent(11/49): loss=8.93595216621577e+17\n",
      "Gradient Descent(12/49): loss=9.131492452617925e+19\n",
      "Gradient Descent(13/49): loss=9.331730545221988e+21\n",
      "Gradient Descent(14/49): loss=9.536455614235152e+23\n",
      "Gradient Descent(15/49): loss=9.745694106706311e+25\n",
      "Gradient Descent(16/49): loss=9.959528537848663e+27\n",
      "Gradient Descent(17/49): loss=1.0178055961250962e+30\n",
      "Gradient Descent(18/49): loss=1.0401378479540873e+32\n",
      "Gradient Descent(19/49): loss=1.062960110532376e+34\n",
      "Gradient Descent(20/49): loss=1.0862831308959477e+36\n",
      "Gradient Descent(21/49): loss=1.110117895411321e+38\n",
      "Gradient Descent(22/49): loss=1.1344756323114918e+40\n",
      "Gradient Descent(23/49): loss=1.1593678163799207e+42\n",
      "Gradient Descent(24/49): loss=1.184806174217498e+44\n",
      "Gradient Descent(25/49): loss=1.2108026897349417e+46\n",
      "Gradient Descent(26/49): loss=1.2373696097911113e+48\n",
      "Gradient Descent(27/49): loss=1.2645194499607848e+50\n",
      "Gradient Descent(28/49): loss=1.29226500043029e+52\n",
      "Gradient Descent(29/49): loss=1.3206193320228712e+54\n",
      "Gradient Descent(30/49): loss=1.3495958023561515e+56\n",
      "Gradient Descent(31/49): loss=1.3792080621350583e+58\n",
      "Gradient Descent(32/49): loss=1.4094700615824503e+60\n",
      "Gradient Descent(33/49): loss=1.4403960570110734e+62\n",
      "Gradient Descent(34/49): loss=1.4720006175396826e+64\n",
      "Gradient Descent(35/49): loss=1.5042986319564556e+66\n",
      "Gradient Descent(36/49): loss=1.5373053157330171e+68\n",
      "Gradient Descent(37/49): loss=1.5710362181924943e+70\n",
      "Gradient Descent(38/49): loss=1.605507229834659e+72\n",
      "Gradient Descent(39/49): loss=1.6407345898218784e+74\n",
      "Gradient Descent(40/49): loss=1.6767348936293147e+76\n",
      "Gradient Descent(41/49): loss=1.713525100863109e+78\n",
      "Gradient Descent(42/49): loss=1.7511225432498222e+80\n",
      "Gradient Descent(43/49): loss=1.7895449328014907e+82\n",
      "Gradient Descent(44/49): loss=1.8288103701595745e+84\n",
      "Gradient Descent(45/49): loss=1.8689373531221513e+86\n",
      "Gradient Descent(46/49): loss=1.909944785358198e+88\n",
      "Gradient Descent(47/49): loss=1.9518519853129585e+90\n",
      "Gradient Descent(48/49): loss=1.994678695308792e+92\n",
      "Gradient Descent(49/49): loss=2.038445090845733e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1951284653658316\n",
      "Gradient Descent(2/49): loss=41.45537348550644\n",
      "Gradient Descent(3/49): loss=1054.8533049596124\n",
      "Gradient Descent(4/49): loss=36484.116654600046\n",
      "Gradient Descent(5/49): loss=2118540.585762963\n",
      "Gradient Descent(6/49): loss=182624117.3505184\n",
      "Gradient Descent(7/49): loss=18226535125.85989\n",
      "Gradient Descent(8/49): loss=1888871365150.1936\n",
      "Gradient Descent(9/49): loss=197444134788829.16\n",
      "Gradient Descent(10/49): loss=2.0678505892488656e+16\n",
      "Gradient Descent(11/49): loss=2.16659767193055e+18\n",
      "Gradient Descent(12/49): loss=2.2702728635890234e+20\n",
      "Gradient Descent(13/49): loss=2.3789582528016865e+22\n",
      "Gradient Descent(14/49): loss=2.492858139742115e+24\n",
      "Gradient Descent(15/49): loss=2.612213961582423e+26\n",
      "Gradient Descent(16/49): loss=2.7372850418923726e+28\n",
      "Gradient Descent(17/49): loss=2.868344583657547e+30\n",
      "Gradient Descent(18/49): loss=3.0056792090494495e+32\n",
      "Gradient Descent(19/49): loss=3.149589341799362e+34\n",
      "Gradient Descent(20/49): loss=3.3003898078478165e+36\n",
      "Gradient Descent(21/49): loss=3.458410511000189e+38\n",
      "Gradient Descent(22/49): loss=3.623997151628428e+40\n",
      "Gradient Descent(23/49): loss=3.79751198225653e+42\n",
      "Gradient Descent(24/49): loss=3.979334599896203e+44\n",
      "Gradient Descent(25/49): loss=4.169862776450318e+46\n",
      "Gradient Descent(26/49): loss=4.369513328906923e+48\n",
      "Gradient Descent(27/49): loss=4.578723031204561e+50\n",
      "Gradient Descent(28/49): loss=4.797949569758411e+52\n",
      "Gradient Descent(29/49): loss=5.027672544737656e+54\n",
      "Gradient Descent(30/49): loss=5.268394519282562e+56\n",
      "Gradient Descent(31/49): loss=5.52064211895763e+58\n",
      "Gradient Descent(32/49): loss=5.78496718384707e+60\n",
      "Gradient Descent(33/49): loss=6.0619479758101e+62\n",
      "Gradient Descent(34/49): loss=6.352190443540477e+64\n",
      "Gradient Descent(35/49): loss=6.656329548195299e+66\n",
      "Gradient Descent(36/49): loss=6.975030652494542e+68\n",
      "Gradient Descent(37/49): loss=7.308990976330079e+70\n",
      "Gradient Descent(38/49): loss=7.658941122067707e+72\n",
      "Gradient Descent(39/49): loss=8.025646672880977e+74\n",
      "Gradient Descent(40/49): loss=8.409909867610676e+76\n",
      "Gradient Descent(41/49): loss=8.812571355816472e+78\n",
      "Gradient Descent(42/49): loss=9.234512036859724e+80\n",
      "Gradient Descent(43/49): loss=9.676654987040092e+82\n",
      "Gradient Descent(44/49): loss=1.0139967479001602e+85\n",
      "Gradient Descent(45/49): loss=1.062546309782847e+87\n",
      "Gradient Descent(46/49): loss=1.113420395845586e+89\n",
      "Gradient Descent(47/49): loss=1.1667303029251557e+91\n",
      "Gradient Descent(48/49): loss=1.2225926566847262e+93\n",
      "Gradient Descent(49/49): loss=1.2811296667549501e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2125011702430655\n",
      "Gradient Descent(2/49): loss=42.3169979265029\n",
      "Gradient Descent(3/49): loss=1072.530180110376\n",
      "Gradient Descent(4/49): loss=35398.74203539906\n",
      "Gradient Descent(5/49): loss=1886433.1445929878\n",
      "Gradient Descent(6/49): loss=152068686.0860897\n",
      "Gradient Descent(7/49): loss=14560934451.561493\n",
      "Gradient Descent(8/49): loss=1462300435262.9985\n",
      "Gradient Descent(9/49): loss=148547354351984.72\n",
      "Gradient Descent(10/49): loss=1.5130342664124768e+16\n",
      "Gradient Descent(11/49): loss=1.542049657374161e+18\n",
      "Gradient Descent(12/49): loss=1.5718422235753713e+20\n",
      "Gradient Descent(13/49): loss=1.6022620116791794e+22\n",
      "Gradient Descent(14/49): loss=1.6332825846295565e+24\n",
      "Gradient Descent(15/49): loss=1.664906553798856e+26\n",
      "Gradient Descent(16/49): loss=1.6971434931228597e+28\n",
      "Gradient Descent(17/49): loss=1.7300047781277086e+30\n",
      "Gradient Descent(18/49): loss=1.7635023825149767e+32\n",
      "Gradient Descent(19/49): loss=1.7976486001965216e+34\n",
      "Gradient Descent(20/49): loss=1.8324559837903647e+36\n",
      "Gradient Descent(21/49): loss=1.8679373337989612e+38\n",
      "Gradient Descent(22/49): loss=1.9041056997071257e+40\n",
      "Gradient Descent(23/49): loss=1.9409743839375455e+42\n",
      "Gradient Descent(24/49): loss=1.97855694654578e+44\n",
      "Gradient Descent(25/49): loss=2.0168672101615898e+46\n",
      "Gradient Descent(26/49): loss=2.0559192650619166e+48\n",
      "Gradient Descent(27/49): loss=2.0957274743508573e+50\n",
      "Gradient Descent(28/49): loss=2.1363064792413993e+52\n",
      "Gradient Descent(29/49): loss=2.1776712044405333e+54\n",
      "Gradient Descent(30/49): loss=2.2198368636383653e+56\n",
      "Gradient Descent(31/49): loss=2.2628189651034483e+58\n",
      "Gradient Descent(32/49): loss=2.306633317386956e+60\n",
      "Gradient Descent(33/49): loss=2.351296035136528e+62\n",
      "Gradient Descent(34/49): loss=2.3968235450235324e+64\n",
      "Gradient Descent(35/49): loss=2.4432325917844858e+66\n",
      "Gradient Descent(36/49): loss=2.490540244379642e+68\n",
      "Gradient Descent(37/49): loss=2.538763902271044e+70\n",
      "Gradient Descent(38/49): loss=2.58792130182186e+72\n",
      "Gradient Descent(39/49): loss=2.6380305228194265e+74\n",
      "Gradient Descent(40/49): loss=2.689109995125418e+76\n",
      "Gradient Descent(41/49): loss=2.741178505453712e+78\n",
      "Gradient Descent(42/49): loss=2.794255204280305e+80\n",
      "Gradient Descent(43/49): loss=2.8483596128867444e+82\n",
      "Gradient Descent(44/49): loss=2.9035116305397865e+84\n",
      "Gradient Descent(45/49): loss=2.959731541810425e+86\n",
      "Gradient Descent(46/49): loss=3.017040024034196e+88\n",
      "Gradient Descent(47/49): loss=3.075458154916496e+90\n",
      "Gradient Descent(48/49): loss=3.1350074202844337e+92\n",
      "Gradient Descent(49/49): loss=3.1957097219895387e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2508573768273314\n",
      "Gradient Descent(2/49): loss=42.97241203554202\n",
      "Gradient Descent(3/49): loss=1075.1429836341006\n",
      "Gradient Descent(4/49): loss=34439.05512520475\n",
      "Gradient Descent(5/49): loss=1766258.138059079\n",
      "Gradient Descent(6/49): loss=139461926.64416242\n",
      "Gradient Descent(7/49): loss=13273412162.061678\n",
      "Gradient Descent(8/49): loss=1331343036428.6477\n",
      "Gradient Descent(9/49): loss=135233935779633.92\n",
      "Gradient Descent(10/49): loss=1.3776895065981036e+16\n",
      "Gradient Descent(11/49): loss=1.404455778044759e+18\n",
      "Gradient Descent(12/49): loss=1.4319615187938678e+20\n",
      "Gradient Descent(13/49): loss=1.4600570941614674e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=1.4887158326820476e+24\n",
      "Gradient Descent(15/49): loss=1.517939876647047e+26\n",
      "Gradient Descent(16/49): loss=1.547738246588803e+28\n",
      "Gradient Descent(17/49): loss=1.5781217330890016e+30\n",
      "Gradient Descent(18/49): loss=1.6091017097519719e+32\n",
      "Gradient Descent(19/49): loss=1.6406898599518038e+34\n",
      "Gradient Descent(20/49): loss=1.672898116573259e+36\n",
      "Gradient Descent(21/49): loss=1.705738651451154e+38\n",
      "Gradient Descent(22/49): loss=1.7392238764591064e+40\n",
      "Gradient Descent(23/49): loss=1.7733664473823112e+42\n",
      "Gradient Descent(24/49): loss=1.8081792685101066e+44\n",
      "Gradient Descent(25/49): loss=1.8436754974688304e+46\n",
      "Gradient Descent(26/49): loss=1.879868550184309e+48\n",
      "Gradient Descent(27/49): loss=1.9167721059502644e+50\n",
      "Gradient Descent(28/49): loss=1.9544001125976658e+52\n",
      "Gradient Descent(29/49): loss=1.992766791766368e+54\n",
      "Gradient Descent(30/49): loss=2.0318866442801664e+56\n",
      "Gradient Descent(31/49): loss=2.0717744556274258e+58\n",
      "Gradient Descent(32/49): loss=2.1124453015491703e+60\n",
      "Gradient Descent(33/49): loss=2.1539145537374216e+62\n",
      "Gradient Descent(34/49): loss=2.196197885644455e+64\n",
      "Gradient Descent(35/49): loss=2.2393112784070092e+66\n",
      "Gradient Descent(36/49): loss=2.2832710268861995e+68\n",
      "Gradient Descent(37/49): loss=2.3280937458265203e+70\n",
      "Gradient Descent(38/49): loss=2.3737963761349154e+72\n",
      "Gradient Descent(39/49): loss=2.4203961912843535e+74\n",
      "Gradient Descent(40/49): loss=2.4679108038417655e+76\n",
      "Gradient Descent(41/49): loss=2.5163581721251087e+78\n",
      "Gradient Descent(42/49): loss=2.5657566069906904e+80\n",
      "Gradient Descent(43/49): loss=2.616124778753902e+82\n",
      "Gradient Descent(44/49): loss=2.667481724245633e+84\n",
      "Gradient Descent(45/49): loss=2.7198468540073985e+86\n",
      "Gradient Descent(46/49): loss=2.773239959627387e+88\n",
      "Gradient Descent(47/49): loss=2.827681221221173e+90\n",
      "Gradient Descent(48/49): loss=2.883191215058464e+92\n",
      "Gradient Descent(49/49): loss=2.9397909213402953e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.1772878066577435\n",
      "Gradient Descent(2/49): loss=40.74208032592471\n",
      "Gradient Descent(3/49): loss=988.2584500644188\n",
      "Gradient Descent(4/49): loss=29151.731390460292\n",
      "Gradient Descent(5/49): loss=1323853.1397980466\n",
      "Gradient Descent(6/49): loss=97660353.6091624\n",
      "Gradient Descent(7/49): loss=9189910236.683544\n",
      "Gradient Descent(8/49): loss=929489824157.4117\n",
      "Gradient Descent(9/49): loss=95664709288759.39\n",
      "Gradient Descent(10/49): loss=9885305681313320.0\n",
      "Gradient Descent(11/49): loss=1.0223956970383809e+18\n",
      "Gradient Descent(12/49): loss=1.0576348913445732e+20\n",
      "Gradient Descent(13/49): loss=1.0941384299278993e+22\n",
      "Gradient Descent(14/49): loss=1.1319134288843676e+24\n",
      "Gradient Descent(15/49): loss=1.1709952941710003e+26\n",
      "Gradient Descent(16/49): loss=1.2114271742086686e+28\n",
      "Gradient Descent(17/49): loss=1.253255223068585e+30\n",
      "Gradient Descent(18/49): loss=1.2965275408404174e+32\n",
      "Gradient Descent(19/49): loss=1.3412939703434638e+34\n",
      "Gradient Descent(20/49): loss=1.3876060944155524e+36\n",
      "Gradient Descent(21/49): loss=1.4355172813725066e+38\n",
      "Gradient Descent(22/49): loss=1.4850827432579425e+40\n",
      "Gradient Descent(23/49): loss=1.5363595987133298e+42\n",
      "Gradient Descent(24/49): loss=1.5894069386250355e+44\n",
      "Gradient Descent(25/49): loss=1.644285894179315e+46\n",
      "Gradient Descent(26/49): loss=1.7010597072996298e+48\n",
      "Gradient Descent(27/49): loss=1.759793803523822e+50\n",
      "Gradient Descent(28/49): loss=1.8205558673993134e+52\n",
      "Gradient Descent(29/49): loss=1.883415920481867e+54\n",
      "Gradient Descent(30/49): loss=1.9484464020276481e+56\n",
      "Gradient Descent(31/49): loss=2.0157222524715954e+58\n",
      "Gradient Descent(32/49): loss=2.085320999787676e+60\n",
      "Gradient Descent(33/49): loss=2.1573228488317604e+62\n",
      "Gradient Descent(34/49): loss=2.2318107737683682e+64\n",
      "Gradient Descent(35/49): loss=2.308870613689553e+66\n",
      "Gradient Descent(36/49): loss=2.3885911715346087e+68\n",
      "Gradient Descent(37/49): loss=2.4710643164261245e+70\n",
      "Gradient Descent(38/49): loss=2.5563850895386238e+72\n",
      "Gradient Descent(39/49): loss=2.644651813622994e+74\n",
      "Gradient Descent(40/49): loss=2.7359662063127176e+76\n",
      "Gradient Descent(41/49): loss=2.8304334973421253e+78\n",
      "Gradient Descent(42/49): loss=2.928162549812099e+80\n",
      "Gradient Descent(43/49): loss=3.029265985642605e+82\n",
      "Gradient Descent(44/49): loss=3.1338603153572506e+84\n",
      "Gradient Descent(45/49): loss=3.2420660723484595e+86\n",
      "Gradient Descent(46/49): loss=3.354007951778938e+88\n",
      "Gradient Descent(47/49): loss=3.469814954279333e+90\n",
      "Gradient Descent(48/49): loss=3.5896205346069354e+92\n",
      "Gradient Descent(49/49): loss=3.713562755437559e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2199681377606537\n",
      "Gradient Descent(2/49): loss=42.569365215638506\n",
      "Gradient Descent(3/49): loss=1097.4945425578146\n",
      "Gradient Descent(4/49): loss=38409.647441981266\n",
      "Gradient Descent(5/49): loss=2254468.2980548656\n",
      "Gradient Descent(6/49): loss=196533803.1212182\n",
      "Gradient Descent(7/49): loss=19848253969.491505\n",
      "Gradient Descent(8/49): loss=2081911358553.9604\n",
      "Gradient Descent(9/49): loss=220279875405471.6\n",
      "Gradient Descent(10/49): loss=2.3352212047581096e+16\n",
      "Gradient Descent(11/49): loss=2.476665336727463e+18\n",
      "Gradient Descent(12/49): loss=2.626925555465669e+20\n",
      "Gradient Descent(13/49): loss=2.7863604476896914e+22\n",
      "Gradient Descent(14/49): loss=2.9554855270998356e+24\n",
      "Gradient Descent(15/49): loss=3.13487927854609e+26\n",
      "Gradient Descent(16/49): loss=3.3251627256706533e+28\n",
      "Gradient Descent(17/49): loss=3.5269963279802607e+30\n",
      "Gradient Descent(18/49): loss=3.741081042067993e+32\n",
      "Gradient Descent(19/49): loss=3.9681604674218643e+34\n",
      "Gradient Descent(20/49): loss=4.209023361596629e+36\n",
      "Gradient Descent(21/49): loss=4.464506364099877e+38\n",
      "Gradient Descent(22/49): loss=4.735496898751319e+40\n",
      "Gradient Descent(23/49): loss=5.022936255290877e+42\n",
      "Gradient Descent(24/49): loss=5.327822858758557e+44\n",
      "Gradient Descent(25/49): loss=5.651215737491897e+46\n",
      "Gradient Descent(26/49): loss=5.994238201665695e+48\n",
      "Gradient Descent(27/49): loss=6.358081745124783e+50\n",
      "Gradient Descent(28/49): loss=6.7440101840557e+52\n",
      "Gradient Descent(29/49): loss=7.153364046871707e+54\n",
      "Gradient Descent(30/49): loss=7.587565230558822e+56\n",
      "Gradient Descent(31/49): loss=8.048121939657586e+58\n",
      "Gradient Descent(32/49): loss=8.536633925034231e+60\n",
      "Gradient Descent(33/49): loss=9.05479804064001e+62\n",
      "Gradient Descent(34/49): loss=9.604414137560707e+64\n",
      "Gradient Descent(35/49): loss=1.0187391315826054e+67\n",
      "Gradient Descent(36/49): loss=1.080575455569935e+69\n",
      "Gradient Descent(37/49): loss=1.1461651751476554e+71\n",
      "Gradient Descent(38/49): loss=1.2157361172231887e+73\n",
      "Gradient Descent(39/49): loss=1.2895299375419633e+75\n",
      "Gradient Descent(40/49): loss=1.3678029600824042e+77\n",
      "Gradient Descent(41/49): loss=1.4508270674012882e+79\n",
      "Gradient Descent(42/49): loss=1.5388906450220384e+81\n",
      "Gradient Descent(43/49): loss=1.6322995831461515e+83\n",
      "Gradient Descent(44/49): loss=1.731378339167819e+85\n",
      "Gradient Descent(45/49): loss=1.8364710646814954e+87\n",
      "Gradient Descent(46/49): loss=1.94794280089775e+89\n",
      "Gradient Descent(47/49): loss=2.0661807466199222e+91\n",
      "Gradient Descent(48/49): loss=2.1915956031847106e+93\n",
      "Gradient Descent(49/49): loss=2.3246230010399124e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.237564350004166\n",
      "Gradient Descent(2/49): loss=43.453459189930584\n",
      "Gradient Descent(3/49): loss=1115.9243598932665\n",
      "Gradient Descent(4/49): loss=37275.19030487223\n",
      "Gradient Descent(5/49): loss=2008080.34674512\n",
      "Gradient Descent(6/49): loss=163684873.18190458\n",
      "Gradient Descent(7/49): loss=15858797327.831678\n",
      "Gradient Descent(8/49): loss=1611957931551.1448\n",
      "Gradient Descent(9/49): loss=165750771313651.62\n",
      "Gradient Descent(10/49): loss=1.7089254876426664e+16\n",
      "Gradient Descent(11/49): loss=1.7630276238956234e+18\n",
      "Gradient Descent(12/49): loss=1.8191009030571095e+20\n",
      "Gradient Descent(13/49): loss=1.8770188311597918e+22\n",
      "Gradient Descent(14/49): loss=1.9367953056578572e+24\n",
      "Gradient Descent(15/49): loss=1.9984788902246572e+26\n",
      "Gradient Descent(16/49): loss=2.0621278052921744e+28\n",
      "Gradient Descent(17/49): loss=2.127804047408884e+30\n",
      "Gradient Descent(18/49): loss=2.1955720430693145e+32\n",
      "Gradient Descent(19/49): loss=2.2654983785993435e+34\n",
      "Gradient Descent(20/49): loss=2.3376517865947098e+36\n",
      "Gradient Descent(21/49): loss=2.4121031947626767e+38\n",
      "Gradient Descent(22/49): loss=2.488925791209086e+40\n",
      "Gradient Descent(23/49): loss=2.5681950953337562e+42\n",
      "Gradient Descent(24/49): loss=2.6499890318193562e+44\n",
      "Gradient Descent(25/49): loss=2.73438800717561e+46\n",
      "Gradient Descent(26/49): loss=2.8214749887677145e+48\n",
      "Gradient Descent(27/49): loss=2.911335586373102e+50\n",
      "Gradient Descent(28/49): loss=3.0040581363381297e+52\n",
      "Gradient Descent(29/49): loss=3.0997337884162996e+54\n",
      "Gradient Descent(30/49): loss=3.1984565953713754e+56\n",
      "Gradient Descent(31/49): loss=3.3003236054349657e+58\n",
      "Gradient Descent(32/49): loss=3.4054349577085026e+60\n",
      "Gradient Descent(33/49): loss=3.513893980604073e+62\n",
      "Gradient Descent(34/49): loss=3.625807293419615e+64\n",
      "Gradient Descent(35/49): loss=3.7412849111499305e+66\n",
      "Gradient Descent(36/49): loss=3.860440352635739e+68\n",
      "Gradient Descent(37/49): loss=3.983390752156773e+70\n",
      "Gradient Descent(38/49): loss=4.110256974579219e+72\n",
      "Gradient Descent(39/49): loss=4.2411637341703875e+74\n",
      "Gradient Descent(40/49): loss=4.376239717197739e+76\n",
      "Gradient Descent(41/49): loss=4.515617708431914e+78\n",
      "Gradient Descent(42/49): loss=4.6594347216794057e+80\n",
      "Gradient Descent(43/49): loss=4.8078321344723745e+82\n",
      "Gradient Descent(44/49): loss=4.9609558270480625e+84\n",
      "Gradient Descent(45/49): loss=5.118956325754277e+86\n",
      "Gradient Descent(46/49): loss=5.281988951022647e+88\n",
      "Gradient Descent(47/49): loss=5.450213970054631e+90\n",
      "Gradient Descent(48/49): loss=5.623796754370016e+92\n",
      "Gradient Descent(49/49): loss=5.802907942372837e+94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2764647123531816\n",
      "Gradient Descent(2/49): loss=44.12732582122455\n",
      "Gradient Descent(3/49): loss=1118.7007832702252\n",
      "Gradient Descent(4/49): loss=36269.305827761804\n",
      "Gradient Descent(5/49): loss=1880400.7948967014\n",
      "Gradient Descent(6/49): loss=150123429.6041257\n",
      "Gradient Descent(7/49): loss=14456738084.567608\n",
      "Gradient Descent(8/49): loss=1467602452785.4082\n",
      "Gradient Descent(9/49): loss=150895478661839.5\n",
      "Gradient Descent(10/49): loss=1.5560559293475462e+16\n",
      "Gradient Descent(11/49): loss=1.6057139013706214e+18\n",
      "Gradient Descent(12/49): loss=1.6572134246679423e+20\n",
      "Gradient Descent(13/49): loss=1.7104253390893928e+22\n",
      "Gradient Descent(14/49): loss=1.7653601761403227e+24\n",
      "Gradient Descent(15/49): loss=1.822062774299539e+26\n",
      "Gradient Descent(16/49): loss=1.8805874341717704e+28\n",
      "Gradient Descent(17/49): loss=1.94099209492562e+30\n",
      "Gradient Descent(18/49): loss=2.0033370041226192e+32\n",
      "Gradient Descent(19/49): loss=2.0676844501350123e+34\n",
      "Gradient Descent(20/49): loss=2.1340987469959217e+36\n",
      "Gradient Descent(21/49): loss=2.2026462804059317e+38\n",
      "Gradient Descent(22/49): loss=2.2733955697744716e+40\n",
      "Gradient Descent(23/49): loss=2.346417335692186e+42\n",
      "Gradient Descent(24/49): loss=2.4217845703818903e+44\n",
      "Gradient Descent(25/49): loss=2.4995726106046117e+46\n",
      "Gradient Descent(26/49): loss=2.579859212952466e+48\n",
      "Gradient Descent(27/49): loss=2.662724631570547e+50\n",
      "Gradient Descent(28/49): loss=2.748251698377872e+52\n",
      "Gradient Descent(29/49): loss=2.8365259058659065e+54\n",
      "Gradient Descent(30/49): loss=2.9276354925559063e+56\n",
      "Gradient Descent(31/49): loss=3.021671531202385e+58\n",
      "Gradient Descent(32/49): loss=3.1187280198287004e+60\n",
      "Gradient Descent(33/49): loss=3.2189019756870015e+62\n",
      "Gradient Descent(34/49): loss=3.3222935322365123e+64\n",
      "Gradient Descent(35/49): loss=3.429006039236333e+66\n",
      "Gradient Descent(36/49): loss=3.5391461660535295e+68\n",
      "Gradient Descent(37/49): loss=3.65282400828925e+70\n",
      "Gradient Descent(38/49): loss=3.770153197830103e+72\n",
      "Gradient Descent(39/49): loss=3.891251016433485e+74\n",
      "Gradient Descent(40/49): loss=4.0162385129626494e+76\n",
      "Gradient Descent(41/49): loss=4.145240624386304e+78\n",
      "Gradient Descent(42/49): loss=4.278386300665007e+80\n",
      "Gradient Descent(43/49): loss=4.4158086336490886e+82\n",
      "Gradient Descent(44/49): loss=4.557644990116775e+84\n",
      "Gradient Descent(45/49): loss=4.7040371490851825e+86\n",
      "Gradient Descent(46/49): loss=4.8551314435324144e+88\n",
      "Gradient Descent(47/49): loss=5.0110789066708925e+90\n",
      "Gradient Descent(48/49): loss=5.172035422919849e+92\n",
      "Gradient Descent(49/49): loss=5.338161883726649e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2018818503653983\n",
      "Gradient Descent(2/49): loss=41.83779133381052\n",
      "Gradient Descent(3/49): loss=1028.4248840836517\n",
      "Gradient Descent(4/49): loss=30712.625836353294\n",
      "Gradient Descent(5/49): loss=1410032.1362426132\n",
      "Gradient Descent(6/49): loss=105144929.01241133\n",
      "Gradient Descent(7/49): loss=10009304424.074778\n",
      "Gradient Descent(8/49): loss=1024573281738.1415\n",
      "Gradient Descent(9/49): loss=106736560238983.97\n",
      "Gradient Descent(10/49): loss=1.1164263059200796e+16\n",
      "Gradient Descent(11/49): loss=1.1688033743821855e+18\n",
      "Gradient Descent(12/49): loss=1.2238880703239247e+20\n",
      "Gradient Descent(13/49): loss=1.2816278657469767e+22\n",
      "Gradient Descent(14/49): loss=1.3421055752562652e+24\n",
      "Gradient Descent(15/49): loss=1.4054403948848752e+26\n",
      "Gradient Descent(16/49): loss=1.4717647970062251e+28\n",
      "Gradient Descent(17/49): loss=1.5412193083389847e+30\n",
      "Gradient Descent(18/49): loss=1.6139515120195933e+32\n",
      "Gradient Descent(19/49): loss=1.6901160559161972e+34\n",
      "Gradient Descent(20/49): loss=1.769874909519928e+36\n",
      "Gradient Descent(21/49): loss=1.8533976914369573e+38\n",
      "Gradient Descent(22/49): loss=1.9408620261130765e+40\n",
      "Gradient Descent(23/49): loss=2.0324539206396528e+42\n",
      "Gradient Descent(24/49): loss=2.1283681601058576e+44\n",
      "Gradient Descent(25/49): loss=2.228808721789021e+46\n",
      "Gradient Descent(26/49): loss=2.333989208933002e+48\n",
      "Gradient Descent(27/49): loss=2.4441333050076122e+50\n",
      "Gradient Descent(28/49): loss=2.5594752494072115e+52\n",
      "Gradient Descent(29/49): loss=2.6802603355988464e+54\n",
      "Gradient Descent(30/49): loss=2.8067454327789495e+56\n",
      "Gradient Descent(31/49): loss=2.939199532147418e+58\n",
      "Gradient Descent(32/49): loss=3.0779043189614904e+60\n",
      "Gradient Descent(33/49): loss=3.2231547715851216e+62\n",
      "Gradient Descent(34/49): loss=3.3752597888088195e+64\n",
      "Gradient Descent(35/49): loss=3.534542846773504e+66\n",
      "Gradient Descent(36/49): loss=3.701342686894809e+68\n",
      "Gradient Descent(37/49): loss=3.876014036252393e+70\n",
      "Gradient Descent(38/49): loss=4.0589283619748804e+72\n",
      "Gradient Descent(39/49): loss=4.2504746612253493e+74\n",
      "Gradient Descent(40/49): loss=4.451060288466956e+76\n",
      "Gradient Descent(41/49): loss=4.661111821769016e+78\n",
      "Gradient Descent(42/49): loss=4.881075969995026e+80\n",
      "Gradient Descent(43/49): loss=5.111420522801439e+82\n",
      "Gradient Descent(44/49): loss=5.352635345469124e+84\n",
      "Gradient Descent(45/49): loss=5.605233420681787e+86\n",
      "Gradient Descent(46/49): loss=5.869751939467265e+88\n",
      "Gradient Descent(47/49): loss=6.146753443621663e+90\n",
      "Gradient Descent(48/49): loss=6.436827022047066e+92\n",
      "Gradient Descent(49/49): loss=6.740589563544153e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.244978222222125\n",
      "Gradient Descent(2/49): loss=43.70587706838556\n",
      "Gradient Descent(3/49): loss=1141.5648216402026\n",
      "Gradient Descent(4/49): loss=40423.724344979666\n",
      "Gradient Descent(5/49): loss=2398209.2696983134\n",
      "Gradient Descent(6/49): loss=211409144.45734614\n",
      "Gradient Descent(7/49): loss=21603140275.165924\n",
      "Gradient Descent(8/49): loss=2293331036979.6045\n",
      "Gradient Descent(9/49): loss=245594248712768.62\n",
      "Gradient Descent(10/49): loss=2.6352254373890984e+16\n",
      "Gradient Descent(11/49): loss=2.8288202156439916e+18\n",
      "Gradient Descent(12/49): loss=3.0369282272353065e+20\n",
      "Gradient Descent(13/49): loss=3.260415249583668e+22\n",
      "Gradient Descent(14/49): loss=3.500365059895254e+24\n",
      "Gradient Descent(15/49): loss=3.757977842746442e+26\n",
      "Gradient Descent(16/49): loss=4.034550815965901e+28\n",
      "Gradient Descent(17/49): loss=4.331478735233485e+30\n",
      "Gradient Descent(18/49): loss=4.6502594961383426e+32\n",
      "Gradient Descent(19/49): loss=4.992501350469735e+34\n",
      "Gradient Descent(20/49): loss=5.359930939090428e+36\n",
      "Gradient Descent(21/49): loss=5.7544019838096876e+38\n",
      "Gradient Descent(22/49): loss=6.177904635051501e+40\n",
      "Gradient Descent(23/49): loss=6.632575511304724e+42\n",
      "Gradient Descent(24/49): loss=7.120708478344823e+44\n",
      "Gradient Descent(25/49): loss=7.644766221984935e+46\n",
      "Gradient Descent(26/49): loss=8.20739267258829e+48\n",
      "Gradient Descent(27/49): loss=8.811426343991742e+50\n",
      "Gradient Descent(28/49): loss=9.459914654127004e+52\n",
      "Gradient Descent(29/49): loss=1.0156129299586976e+55\n",
      "Gradient Descent(30/49): loss=1.0903582761703763e+57\n",
      "Gradient Descent(31/49): loss=1.170604602741321e+59\n",
      "Gradient Descent(32/49): loss=1.2567567614308135e+61\n",
      "Gradient Descent(33/49): loss=1.3492493995866104e+63\n",
      "Gradient Descent(34/49): loss=1.448549152989814e+65\n",
      "Gradient Descent(35/49): loss=1.5551570000849745e+67\n",
      "Gradient Descent(36/49): loss=1.669610789472623e+69\n",
      "Gradient Descent(37/49): loss=1.7924879534163817e+71\n",
      "Gradient Descent(38/49): loss=1.9244084210534402e+73\n",
      "Gradient Descent(39/49): loss=2.0660377460071674e+75\n",
      "Gradient Descent(40/49): loss=2.2180904641800583e+77\n",
      "Gradient Descent(41/49): loss=2.381333698667795e+79\n",
      "Gradient Descent(42/49): loss=2.556591029981742e+81\n",
      "Gradient Descent(43/49): loss=2.744746651105516e+83\n",
      "Gradient Descent(44/49): loss=2.9467498283480122e+85\n",
      "Gradient Descent(45/49): loss=3.16361969049919e+87\n",
      "Gradient Descent(46/49): loss=3.3964503704494094e+89\n",
      "Gradient Descent(47/49): loss=3.646416525213057e+91\n",
      "Gradient Descent(48/49): loss=3.9147792622059404e+93\n",
      "Gradient Descent(49/49): loss=4.2028925016739306e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2627993450621826\n",
      "Gradient Descent(2/49): loss=44.612875209858714\n",
      "Gradient Descent(3/49): loss=1160.7738329758733\n",
      "Gradient Descent(4/49): loss=39238.28616462448\n",
      "Gradient Descent(5/49): loss=2136755.8581287013\n",
      "Gradient Descent(6/49): loss=176109929.54970145\n",
      "Gradient Descent(7/49): loss=17263426338.875366\n",
      "Gradient Descent(8/49): loss=1775884889646.1038\n",
      "Gradient Descent(9/49): loss=184823959453641.94\n",
      "Gradient Descent(10/49): loss=1.9287573048582336e+16\n",
      "Gradient Descent(11/49): loss=2.014039478470406e+18\n",
      "Gradient Descent(12/49): loss=2.1033945378863763e+20\n",
      "Gradient Descent(13/49): loss=2.1967864769724157e+22\n",
      "Gradient Descent(14/49): loss=2.2943424940675176e+24\n",
      "Gradient Descent(15/49): loss=2.396235012329882e+26\n",
      "Gradient Descent(16/49): loss=2.502653616110314e+28\n",
      "Gradient Descent(17/49): loss=2.6137985915912776e+30\n",
      "Gradient Descent(18/49): loss=2.7298796678934886e+32\n",
      "Gradient Descent(19/49): loss=2.8511160200016317e+34\n",
      "Gradient Descent(20/49): loss=2.977736588258956e+36\n",
      "Gradient Descent(21/49): loss=3.109980488040153e+38\n",
      "Gradient Descent(22/49): loss=3.2480974558663527e+40\n",
      "Gradient Descent(23/49): loss=3.3923483197149657e+42\n",
      "Gradient Descent(24/49): loss=3.543005491268242e+44\n",
      "Gradient Descent(25/49): loss=3.7003534802760973e+46\n",
      "Gradient Descent(26/49): loss=3.864689431822578e+48\n",
      "Gradient Descent(27/49): loss=4.036323687467649e+50\n",
      "Gradient Descent(28/49): loss=4.2155803713130727e+52\n",
      "Gradient Descent(29/49): loss=4.4027980020971886e+54\n",
      "Gradient Descent(30/49): loss=4.5983301324732e+56\n",
      "Gradient Descent(31/49): loss=4.8025460166782306e+58\n",
      "Gradient Descent(32/49): loss=5.015831307854899e+60\n",
      "Gradient Descent(33/49): loss=5.2385887863410315e+62\n",
      "Gradient Descent(34/49): loss=5.471239120303493e+64\n",
      "Gradient Descent(35/49): loss=5.714221660151874e+66\n",
      "Gradient Descent(36/49): loss=5.967995268234197e+68\n",
      "Gradient Descent(37/49): loss=6.233039185378557e+70\n",
      "Gradient Descent(38/49): loss=6.509853935919663e+72\n",
      "Gradient Descent(39/49): loss=6.79896227291834e+74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=7.100910165357744e+76\n",
      "Gradient Descent(41/49): loss=7.416267829183935e+78\n",
      "Gradient Descent(42/49): loss=7.74563080413496e+80\n",
      "Gradient Descent(43/49): loss=8.089621078391805e+82\n",
      "Gradient Descent(44/49): loss=8.448888263177322e+84\n",
      "Gradient Descent(45/49): loss=8.82411081951982e+86\n",
      "Gradient Descent(46/49): loss=9.215997339499155e+88\n",
      "Gradient Descent(47/49): loss=9.625287884392094e+90\n",
      "Gradient Descent(48/49): loss=1.005275538224701e+93\n",
      "Gradient Descent(49/49): loss=1.0499207087526912e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3022475213607896\n",
      "Gradient Descent(2/49): loss=45.305579830641236\n",
      "Gradient Descent(3/49): loss=1163.7211909526463\n",
      "Gradient Descent(4/49): loss=38184.28132059632\n",
      "Gradient Descent(5/49): loss=2001152.882223272\n",
      "Gradient Descent(6/49): loss=161527950.66456234\n",
      "Gradient Descent(7/49): loss=15737428269.299326\n",
      "Gradient Descent(8/49): loss=1616854357454.1238\n",
      "Gradient Descent(9/49): loss=168259210753032.9\n",
      "Gradient Descent(10/49): loss=1.7562209601475652e+16\n",
      "Gradient Descent(11/49): loss=1.834325391945888e+18\n",
      "Gradient Descent(12/49): loss=1.916203640693321e+20\n",
      "Gradient Descent(13/49): loss=2.001808539894566e+22\n",
      "Gradient Descent(14/49): loss=2.0912549724097456e+24\n",
      "Gradient Descent(15/49): loss=2.1847022395144438e+26\n",
      "Gradient Descent(16/49): loss=2.2823261622825086e+28\n",
      "Gradient Descent(17/49): loss=2.3843126682835175e+30\n",
      "Gradient Descent(18/49): loss=2.4908565317503936e+32\n",
      "Gradient Descent(19/49): loss=2.6021613593251e+34\n",
      "Gradient Descent(20/49): loss=2.7184398867224226e+36\n",
      "Gradient Descent(21/49): loss=2.839914363209305e+38\n",
      "Gradient Descent(22/49): loss=2.9668169711081963e+40\n",
      "Gradient Descent(23/49): loss=3.0993902683158603e+42\n",
      "Gradient Descent(24/49): loss=3.237887651618225e+44\n",
      "Gradient Descent(25/49): loss=3.3825738409529843e+46\n",
      "Gradient Descent(26/49): loss=3.533725385370673e+48\n",
      "Gradient Descent(27/49): loss=3.6916311916180175e+50\n",
      "Gradient Descent(28/49): loss=3.856593076345659e+52\n",
      "Gradient Descent(29/49): loss=4.0289263429910116e+54\n",
      "Gradient Descent(30/49): loss=4.208960384440718e+56\n",
      "Gradient Descent(31/49): loss=4.397039312622356e+58\n",
      "Gradient Descent(32/49): loss=4.593522616230441e+60\n",
      "Gradient Descent(33/49): loss=4.798785847842721e+62\n",
      "Gradient Descent(34/49): loss=5.013221341740952e+64\n",
      "Gradient Descent(35/49): loss=5.237238963806862e+66\n",
      "Gradient Descent(36/49): loss=5.471266894928459e+68\n",
      "Gradient Descent(37/49): loss=5.715752449413126e+70\n",
      "Gradient Descent(38/49): loss=5.971162929970569e+72\n",
      "Gradient Descent(39/49): loss=6.237986520902524e+74\n",
      "Gradient Descent(40/49): loss=6.516733221203977e+76\n",
      "Gradient Descent(41/49): loss=6.8079358193610566e+78\n",
      "Gradient Descent(42/49): loss=7.112150911707346e+80\n",
      "Gradient Descent(43/49): loss=7.429959966286369e+82\n",
      "Gradient Descent(44/49): loss=7.761970434253145e+84\n",
      "Gradient Descent(45/49): loss=8.108816910938615e+86\n",
      "Gradient Descent(46/49): loss=8.471162348797544e+88\n",
      "Gradient Descent(47/49): loss=8.849699324556437e+90\n",
      "Gradient Descent(48/49): loss=9.245151362985044e+92\n",
      "Gradient Descent(49/49): loss=9.658274319821327e+94\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2266446872013974\n",
      "Gradient Descent(2/49): loss=42.95567203025184\n",
      "Gradient Descent(3/49): loss=1069.9441169533418\n",
      "Gradient Descent(4/49): loss=32346.29740200004\n",
      "Gradient Descent(5/49): loss=1501238.925067533\n",
      "Gradient Descent(6/49): loss=113152549.51225121\n",
      "Gradient Descent(7/49): loss=10896133178.9014\n",
      "Gradient Descent(8/49): loss=1128718742650.0698\n",
      "Gradient Descent(9/49): loss=119010998835600.34\n",
      "Gradient Descent(10/49): loss=1.2599419422692494e+16\n",
      "Gradient Descent(11/49): loss=1.3350959191685164e+18\n",
      "Gradient Descent(12/49): loss=1.4150254565545973e+20\n",
      "Gradient Descent(13/49): loss=1.4998101272260206e+22\n",
      "Gradient Descent(14/49): loss=1.5896915699459683e+24\n",
      "Gradient Descent(15/49): loss=1.6849634629545657e+26\n",
      "Gradient Descent(16/49): loss=1.7859460527572537e+28\n",
      "Gradient Descent(17/49): loss=1.8929809199919437e+30\n",
      "Gradient Descent(18/49): loss=2.0064306280833952e+32\n",
      "Gradient Descent(19/49): loss=2.1266795916001625e+34\n",
      "Gradient Descent(20/49): loss=2.2541352929084353e+36\n",
      "Gradient Descent(21/49): loss=2.3892296424789737e+38\n",
      "Gradient Descent(22/49): loss=2.532420437609237e+40\n",
      "Gradient Descent(23/49): loss=2.6841929126024366e+42\n",
      "Gradient Descent(24/49): loss=2.8450613828138677e+44\n",
      "Gradient Descent(25/49): loss=3.015570987458323e+46\n",
      "Gradient Descent(26/49): loss=3.196299536921797e+48\n",
      "Gradient Descent(27/49): loss=3.3878594707988183e+50\n",
      "Gradient Descent(28/49): loss=3.5908999332818664e+52\n",
      "Gradient Descent(29/49): loss=3.80610897293302e+54\n",
      "Gradient Descent(30/49): loss=4.0342158742923915e+56\n",
      "Gradient Descent(31/49): loss=4.275993629223793e+58\n",
      "Gradient Descent(32/49): loss=4.532261556372299e+60\n",
      "Gradient Descent(33/49): loss=4.803888077611437e+62\n",
      "Gradient Descent(34/49): loss=5.0917936608866725e+64\n",
      "Gradient Descent(35/49): loss=5.396953939430019e+66\n",
      "Gradient Descent(36/49): loss=5.720403017913596e+68\n",
      "Gradient Descent(37/49): loss=6.063236976747489e+70\n",
      "Gradient Descent(38/49): loss=6.426617586396123e+72\n",
      "Gradient Descent(39/49): loss=6.811776244300993e+74\n",
      "Gradient Descent(40/49): loss=7.220018147749163e+76\n",
      "Gradient Descent(41/49): loss=7.65272671682972e+78\n",
      "Gradient Descent(42/49): loss=8.111368282465597e+80\n",
      "Gradient Descent(43/49): loss=8.597497055408343e+82\n",
      "Gradient Descent(44/49): loss=9.11276039303278e+84\n",
      "Gradient Descent(45/49): loss=9.658904381780227e+86\n",
      "Gradient Descent(46/49): loss=1.0237779754167821e+89\n",
      "Gradient Descent(47/49): loss=1.0851348160414598e+91\n",
      "Gradient Descent(48/49): loss=1.1501688815936162e+93\n",
      "Gradient Descent(49/49): loss=1.2191005547238901e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2701587187502437\n",
      "Gradient Descent(2/49): loss=44.8652107827454\n",
      "Gradient Descent(3/49): loss=1187.1022441753958\n",
      "Gradient Descent(4/49): loss=42529.82259142936\n",
      "Gradient Descent(5/49): loss=2550160.9253822546\n",
      "Gradient Descent(6/49): loss=227310745.69495144\n",
      "Gradient Descent(7/49): loss=23501226713.39104\n",
      "Gradient Descent(8/49): loss=2524753549774.9746\n",
      "Gradient Descent(9/49): loss=273638876103366.44\n",
      "Gradient Descent(10/49): loss=2.971612900289188e+16\n",
      "Gradient Descent(11/49): loss=3.228468494406752e+18\n",
      "Gradient Descent(12/49): loss=3.507865610460407e+20\n",
      "Gradient Descent(13/49): loss=3.811524027930379e+22\n",
      "Gradient Descent(14/49): loss=4.1414883278259584e+24\n",
      "Gradient Descent(15/49): loss=4.5000224334387236e+26\n",
      "Gradient Descent(16/49): loss=4.889596449860684e+28\n",
      "Gradient Descent(17/49): loss=5.312896775820724e+30\n",
      "Gradient Descent(18/49): loss=5.772842966287475e+32\n",
      "Gradient Descent(19/49): loss=6.272607468972638e+34\n",
      "Gradient Descent(20/49): loss=6.815637409795669e+36\n",
      "Gradient Descent(21/49): loss=7.405678346263885e+38\n",
      "Gradient Descent(22/49): loss=8.046800096937106e+40\n",
      "Gradient Descent(23/49): loss=8.743424811736237e+42\n",
      "Gradient Descent(24/49): loss=9.50035747348677e+44\n",
      "Gradient Descent(25/49): loss=1.0322819040304483e+47\n",
      "Gradient Descent(26/49): loss=1.1216482457240626e+49\n",
      "Gradient Descent(27/49): loss=1.2187511785528633e+51\n",
      "Gradient Descent(28/49): loss=1.3242604719317814e+53\n",
      "Gradient Descent(29/49): loss=1.43890387831521e+55\n",
      "Gradient Descent(30/49): loss=1.5634721528841533e+57\n",
      "Gradient Descent(31/49): loss=1.698824507795732e+59\n",
      "Gradient Descent(32/49): loss=1.8458945386161921e+61\n",
      "Gradient Descent(33/49): loss=2.005696663815021e+63\n",
      "Gradient Descent(34/49): loss=2.179333121736537e+65\n",
      "Gradient Descent(35/49): loss=2.3680015733106347e+67\n",
      "Gradient Descent(36/49): loss=2.5730033629431717e+69\n",
      "Gradient Descent(37/49): loss=2.7957524945649405e+71\n",
      "Gradient Descent(38/49): loss=3.037785384751846e+73\n",
      "Gradient Descent(39/49): loss=3.3007714601889275e+75\n",
      "Gradient Descent(40/49): loss=3.5865246725742163e+77\n",
      "Gradient Descent(41/49): loss=3.897016010386533e+79\n",
      "Gradient Descent(42/49): loss=4.2343870938183316e+81\n",
      "Gradient Descent(43/49): loss=4.6009649466430994e+83\n",
      "Gradient Descent(44/49): loss=4.9992780469083334e+85\n",
      "Gradient Descent(45/49): loss=5.432073767163883e+87\n",
      "Gradient Descent(46/49): loss=5.902337324517725e+89\n",
      "Gradient Descent(47/49): loss=6.413312371231746e+91\n",
      "Gradient Descent(48/49): loss=6.9685233678768675e+93\n",
      "Gradient Descent(49/49): loss=7.5717998933708e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2882061554171145\n",
      "Gradient Descent(2/49): loss=45.79555327526464\n",
      "Gradient Descent(3/49): loss=1207.1174220152247\n",
      "Gradient Descent(4/49): loss=41291.44363451795\n",
      "Gradient Descent(5/49): loss=2272819.0357444356\n",
      "Gradient Descent(6/49): loss=189394778.4906788\n",
      "Gradient Descent(7/49): loss=18782879895.312992\n",
      "Gradient Descent(8/49): loss=1955343277073.5188\n",
      "Gradient Descent(9/49): loss=205956982137347.9\n",
      "Gradient Descent(10/49): loss=2.175284082992114e+16\n",
      "Gradient Descent(11/49): loss=2.2989481592263025e+18\n",
      "Gradient Descent(12/49): loss=2.429995253479769e+20\n",
      "Gradient Descent(13/49): loss=2.568598286842023e+22\n",
      "Gradient Descent(14/49): loss=2.715127906309129e+24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=2.870021615606164e+26\n",
      "Gradient Descent(16/49): loss=3.033753000512027e+28\n",
      "Gradient Descent(17/49): loss=3.2068253708980663e+30\n",
      "Gradient Descent(18/49): loss=3.389771408511674e+32\n",
      "Gradient Descent(19/49): loss=3.5831543442896766e+34\n",
      "Gradient Descent(20/49): loss=3.787569579902161e+36\n",
      "Gradient Descent(21/49): loss=4.003646493554942e+38\n",
      "Gradient Descent(22/49): loss=4.2320503711355955e+40\n",
      "Gradient Descent(23/49): loss=4.473484452963378e+42\n",
      "Gradient Descent(24/49): loss=4.728692098624292e+44\n",
      "Gradient Descent(25/49): loss=4.998459075630982e+46\n",
      "Gradient Descent(26/49): loss=5.283615978724987e+48\n",
      "Gradient Descent(27/49): loss=5.58504078721806e+50\n",
      "Gradient Descent(28/49): loss=5.903661568230763e+52\n",
      "Gradient Descent(29/49): loss=6.240459334150426e+54\n",
      "Gradient Descent(30/49): loss=6.596471063102669e+56\n",
      "Gradient Descent(31/49): loss=6.972792891738914e+58\n",
      "Gradient Descent(32/49): loss=7.370583490169474e+60\n",
      "Gradient Descent(33/49): loss=7.791067629431739e+62\n",
      "Gradient Descent(34/49): loss=8.235539952479613e+64\n",
      "Gradient Descent(35/49): loss=8.70536896030456e+66\n",
      "Gradient Descent(36/49): loss=9.20200122545912e+68\n",
      "Gradient Descent(37/49): loss=9.726965845958647e+70\n",
      "Gradient Descent(38/49): loss=1.0281879153273745e+73\n",
      "Gradient Descent(39/49): loss=1.0868449688907748e+75\n",
      "Gradient Descent(40/49): loss=1.1488483464883462e+77\n",
      "Gradient Descent(41/49): loss=1.2143889524336139e+79\n",
      "Gradient Descent(42/49): loss=1.2836685819330368e+81\n",
      "Gradient Descent(43/49): loss=1.3569005424002224e+83\n",
      "Gradient Descent(44/49): loss=1.4343103102152964e+85\n",
      "Gradient Descent(45/49): loss=1.5161362249519549e+87\n",
      "Gradient Descent(46/49): loss=1.6026302232091646e+89\n",
      "Gradient Descent(47/49): loss=1.694058614307435e+91\n",
      "Gradient Descent(48/49): loss=1.7907029002377172e+93\n",
      "Gradient Descent(49/49): loss=1.8928606423873488e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3282058038501536\n",
      "Gradient Descent(2/49): loss=46.50748667489534\n",
      "Gradient Descent(3/49): loss=1210.2432704063258\n",
      "Gradient Descent(4/49): loss=40187.32791771395\n",
      "Gradient Descent(5/49): loss=2128853.1178429294\n",
      "Gradient Descent(6/49): loss=173722309.93403253\n",
      "Gradient Descent(7/49): loss=17122833182.001976\n",
      "Gradient Descent(8/49): loss=1780247834479.01\n",
      "Gradient Descent(9/49): loss=187498160636782.8\n",
      "Gradient Descent(10/49): loss=1.9806925041366016e+16\n",
      "Gradient Descent(11/49): loss=2.0938083794403126e+18\n",
      "Gradient Descent(12/49): loss=2.2137349375000445e+20\n",
      "Gradient Descent(13/49): loss=2.3406155637205884e+22\n",
      "Gradient Descent(14/49): loss=2.4747890045350404e+24\n",
      "Gradient Descent(15/49): loss=2.6166588055183985e+26\n",
      "Gradient Descent(16/49): loss=2.7666626503641533e+28\n",
      "Gradient Descent(17/49): loss=2.9252659821126626e+30\n",
      "Gradient Descent(18/49): loss=3.0929615731479077e+32\n",
      "Gradient Descent(19/49): loss=3.270270601737011e+34\n",
      "Gradient Descent(20/49): loss=3.457744161584541e+36\n",
      "Gradient Descent(21/49): loss=3.655964947960922e+38\n",
      "Gradient Descent(22/49): loss=3.8655490623246464e+40\n",
      "Gradient Descent(23/49): loss=4.087147925691051e+42\n",
      "Gradient Descent(24/49): loss=4.321450302972206e+44\n",
      "Gradient Descent(25/49): loss=4.569184443676716e+46\n",
      "Gradient Descent(26/49): loss=4.8311203454040165e+48\n",
      "Gradient Descent(27/49): loss=5.10807214711517e+50\n",
      "Gradient Descent(28/49): loss=5.400900659607175e+52\n",
      "Gradient Descent(29/49): loss=5.710516041050646e+54\n",
      "Gradient Descent(30/49): loss=6.037880625908219e+56\n",
      "Gradient Descent(31/49): loss=6.384011916024756e+58\n",
      "Gradient Descent(32/49): loss=6.749985743187013e+60\n",
      "Gradient Descent(33/49): loss=7.136939612982309e+62\n",
      "Gradient Descent(34/49): loss=7.546076240348764e+64\n",
      "Gradient Descent(35/49): loss=7.978667287807152e+66\n",
      "Gradient Descent(36/49): loss=8.436057317992613e+68\n",
      "Gradient Descent(37/49): loss=8.919667972771947e+70\n",
      "Gradient Descent(38/49): loss=9.43100239193553e+72\n",
      "Gradient Descent(39/49): loss=9.971649885197614e+74\n",
      "Gradient Descent(40/49): loss=1.0543290872027223e+77\n",
      "Gradient Descent(41/49): loss=1.1147702104662439e+79\n",
      "Gradient Descent(42/49): loss=1.1786762190541738e+81\n",
      "Gradient Descent(43/49): loss=1.2462457431319098e+83\n",
      "Gradient Descent(44/49): loss=1.3176887996609716e+85\n",
      "Gradient Descent(45/49): loss=1.3932274451654453e+87\n",
      "Gradient Descent(46/49): loss=1.4730964659194624e+89\n",
      "Gradient Descent(47/49): loss=1.5575441077007222e+91\n",
      "Gradient Descent(48/49): loss=1.6468328473784182e+93\n",
      "Gradient Descent(49/49): loss=1.7412402087335682e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.25157631716574\n",
      "Gradient Descent(2/49): loss=44.09601968539015\n",
      "Gradient Descent(3/49): loss=1112.8523809248445\n",
      "Gradient Descent(4/49): loss=34055.64035374029\n",
      "Gradient Descent(5/49): loss=1597733.15722652\n",
      "Gradient Descent(6/49): loss=121716272.76145345\n",
      "Gradient Descent(7/49): loss=11855488984.33212\n",
      "Gradient Descent(8/49): loss=1242727308235.4248\n",
      "Gradient Descent(9/49): loss=132610192023464.16\n",
      "Gradient Descent(10/49): loss=1.420873324048314e+16\n",
      "Gradient Descent(11/49): loss=1.5238291332763443e+18\n",
      "Gradient Descent(12/49): loss=1.634586968382179e+20\n",
      "Gradient Descent(13/49): loss=1.7534778551350812e+22\n",
      "Gradient Descent(14/49): loss=1.8810362322030225e+24\n",
      "Gradient Descent(15/49): loss=2.017878805639836e+26\n",
      "Gradient Descent(16/49): loss=2.164677643563138e+28\n",
      "Gradient Descent(17/49): loss=2.32215624623296e+30\n",
      "Gradient Descent(18/49): loss=2.491091362467528e+32\n",
      "Gradient Descent(19/49): loss=2.6723163976436104e+34\n",
      "Gradient Descent(20/49): loss=2.8667254227254034e+36\n",
      "Gradient Descent(21/49): loss=3.0752775603753454e+38\n",
      "Gradient Descent(22/49): loss=3.2990017108198546e+40\n",
      "Gradient Descent(23/49): loss=3.539001626519476e+42\n",
      "Gradient Descent(24/49): loss=3.796461357226879e+44\n",
      "Gradient Descent(25/49): loss=4.072651091461558e+46\n",
      "Gradient Descent(26/49): loss=4.36893342301825e+48\n",
      "Gradient Descent(27/49): loss=4.6867700733767065e+50\n",
      "Gradient Descent(28/49): loss=5.02772910316511e+52\n",
      "Gradient Descent(29/49): loss=5.393492648253886e+54\n",
      "Gradient Descent(30/49): loss=5.785865218644084e+56\n",
      "Gradient Descent(31/49): loss=6.206782601094942e+58\n",
      "Gradient Descent(32/49): loss=6.658321409409627e+60\n",
      "Gradient Descent(33/49): loss=7.142709329497038e+62\n",
      "Gradient Descent(34/49): loss=7.662336109756756e+64\n",
      "Gradient Descent(35/49): loss=8.219765351002439e+66\n",
      "Gradient Descent(36/49): loss=8.817747154096662e+68\n",
      "Gradient Descent(37/49): loss=9.459231687690013e+70\n",
      "Gradient Descent(38/49): loss=1.0147383743004171e+73\n",
      "Gradient Descent(39/49): loss=1.088559834746242e+75\n",
      "Gradient Descent(40/49): loss=1.1677517514204009e+77\n",
      "Gradient Descent(41/49): loss=1.2527048210108359e+79\n",
      "Gradient Descent(42/49): loss=1.3438381630984484e+81\n",
      "Gradient Descent(43/49): loss=1.4416013879012698e+83\n",
      "Gradient Descent(44/49): loss=1.5464768144455533e+85\n",
      "Gradient Descent(45/49): loss=1.6589818501073118e+87\n",
      "Gradient Descent(46/49): loss=1.779671543263446e+89\n",
      "Gradient Descent(47/49): loss=1.9091413216466528e+91\n",
      "Gradient Descent(48/49): loss=2.048029929913409e+93\n",
      "Gradient Descent(49/49): loss=2.1970225809178797e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2955096273450115\n",
      "Gradient Descent(2/49): loss=46.04767010835714\n",
      "Gradient Descent(3/49): loss=1234.145669842127\n",
      "Gradient Descent(4/49): loss=44731.53058047924\n",
      "Gradient Descent(5/49): loss=2710738.300405012\n",
      "Gradient Descent(6/49): loss=244302615.50592837\n",
      "Gradient Descent(7/49): loss=25553234649.385666\n",
      "Gradient Descent(8/49): loss=2777934329662.137\n",
      "Gradient Descent(9/49): loss=304689176876517.9\n",
      "Gradient Descent(10/49): loss=3.348537551646809e+16\n",
      "Gradient Descent(11/49): loss=3.6816719135587676e+18\n",
      "Gradient Descent(12/49): loss=4.0483452174992684e+20\n",
      "Gradient Descent(13/49): loss=4.4516337854130485e+22\n",
      "Gradient Descent(14/49): loss=4.895120790568556e+24\n",
      "Gradient Descent(15/49): loss=5.382795241680085e+26\n",
      "Gradient Descent(16/49): loss=5.919055467494261e+28\n",
      "Gradient Descent(17/49): loss=6.508740885591e+30\n",
      "Gradient Descent(18/49): loss=7.157173748764661e+32\n",
      "Gradient Descent(19/49): loss=7.870206711683187e+34\n",
      "Gradient Descent(20/49): loss=8.654275541488749e+36\n",
      "Gradient Descent(21/49): loss=9.51645717848141e+38\n",
      "Gradient Descent(22/49): loss=1.0464533605182734e+41\n",
      "Gradient Descent(23/49): loss=1.1507062084228061e+43\n",
      "Gradient Descent(24/49): loss=1.2653452395134208e+45\n",
      "Gradient Descent(25/49): loss=1.39140517661263e+47\n",
      "Gradient Descent(26/49): loss=1.5300238267376123e+49\n",
      "Gradient Descent(27/49): loss=1.6824523508557885e+51\n",
      "Gradient Descent(28/49): loss=1.8500665567645769e+53\n",
      "Gradient Descent(29/49): loss=2.0343793170236242e+55\n",
      "Gradient Descent(30/49): loss=2.2370542240227757e+57\n",
      "Gradient Descent(31/49): loss=2.4599206054354912e+59\n",
      "Gradient Descent(32/49): loss=2.704990035585439e+61\n",
      "Gradient Descent(33/49): loss=2.9744744917575317e+63\n",
      "Gradient Descent(34/49): loss=3.2708063193295626e+65\n",
      "Gradient Descent(35/49): loss=3.5966601859291016e+67\n",
      "Gradient Descent(36/49): loss=3.954977222772151e+69\n",
      "Gradient Descent(37/49): loss=4.3489915710804364e+71\n",
      "Gradient Descent(38/49): loss=4.7822595731845856e+73\n",
      "Gradient Descent(39/49): loss=5.258691871788066e+75\n",
      "Gradient Descent(40/49): loss=5.782588707119181e+77\n",
      "Gradient Descent(41/49): loss=6.358678730558951e+79\n",
      "Gradient Descent(42/49): loss=6.9921616850744265e+81\n",
      "Gradient Descent(43/49): loss=7.688755337687128e+83\n",
      "Gradient Descent(44/49): loss=8.454747087585961e+85\n",
      "Gradient Descent(45/49): loss=9.297050715694295e+87\n",
      "Gradient Descent(46/49): loss=1.0223268787910325e+90\n",
      "Gradient Descent(47/49): loss=1.124176127526458e+92\n",
      "Gradient Descent(48/49): loss=1.23617210103566e+94\n",
      "Gradient Descent(49/49): loss=1.3593256661136432e+96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3137847810689616\n",
      "Gradient Descent(2/49): loss=47.001802720880406\n",
      "Gradient Descent(3/49): loss=1254.9947220047957\n",
      "Gradient Descent(4/49): loss=43438.18831983458\n",
      "Gradient Descent(5/49): loss=2416645.2439238606\n",
      "Gradient Descent(6/49): loss=203593213.0989637\n",
      "Gradient Descent(7/49): loss=20425770616.882427\n",
      "Gradient Descent(8/49): loss=2151698056660.6152\n",
      "Gradient Descent(9/49): loss=229357902304149.62\n",
      "Gradient Descent(10/49): loss=2.4515575926959972e+16\n",
      "Gradient Descent(11/49): loss=2.6220856312410757e+18\n",
      "Gradient Descent(12/49): loss=2.804887031844997e+20\n",
      "Gradient Descent(13/49): loss=3.0005341122052386e+22\n",
      "Gradient Descent(14/49): loss=3.209853032125082e+24\n",
      "Gradient Descent(15/49): loss=3.4337803247581564e+26\n",
      "Gradient Descent(16/49): loss=3.673330860494438e+28\n",
      "Gradient Descent(17/49): loss=3.9295935156051967e+30\n",
      "Gradient Descent(18/49): loss=4.2037339199480045e+32\n",
      "Gradient Descent(19/49): loss=4.496999216102298e+34\n",
      "Gradient Descent(20/49): loss=4.810723599150613e+36\n",
      "Gradient Descent(21/49): loss=5.146334353536431e+38\n",
      "Gradient Descent(22/49): loss=5.505358338339974e+40\n",
      "Gradient Descent(23/49): loss=5.889428931708277e+42\n",
      "Gradient Descent(24/49): loss=6.300293461409133e+44\n",
      "Gradient Descent(25/49): loss=6.739821154169405e+46\n",
      "Gradient Descent(26/49): loss=7.210011639685872e+48\n",
      "Gradient Descent(27/49): loss=7.713004047926347e+50\n",
      "Gradient Descent(28/49): loss=8.251086741091616e+52\n",
      "Gradient Descent(29/49): loss=8.826707724511485e+54\n",
      "Gradient Descent(30/49): loss=9.44248578383531e+56\n",
      "Gradient Descent(31/49): loss=1.010122239918909e+59\n",
      "Gradient Descent(32/49): loss=1.0805914490499289e+61\n",
      "Gradient Descent(33/49): loss=1.1559768051969415e+63\n",
      "Gradient Descent(34/49): loss=1.2366212737738873e+65\n",
      "Gradient Descent(35/49): loss=1.3228917465083974e+67\n",
      "Gradient Descent(36/49): loss=1.4151807106142398e+69\n",
      "Gradient Descent(37/49): loss=1.5139080344106578e+71\n",
      "Gradient Descent(38/49): loss=1.6195228775118098e+73\n",
      "Gradient Descent(39/49): loss=1.7325057342767919e+75\n",
      "Gradient Descent(40/49): loss=1.8533706198170617e+77\n",
      "Gradient Descent(41/49): loss=1.982667408506403e+79\n",
      "Gradient Descent(42/49): loss=2.120984335632493e+81\n",
      "Gradient Descent(43/49): loss=2.268950673571258e+83\n",
      "Gradient Descent(44/49): loss=2.4272395946593767e+85\n",
      "Gradient Descent(45/49): loss=2.596571233789367e+87\n",
      "Gradient Descent(46/49): loss=2.777715964660933e+89\n",
      "Gradient Descent(47/49): loss=2.971497904593269e+91\n",
      "Gradient Descent(48/49): loss=3.1787986638439925e+93\n",
      "Gradient Descent(49/49): loss=3.400561356491762e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3543395598212755\n",
      "Gradient Descent(2/49): loss=47.73336104726075\n",
      "Gradient Descent(3/49): loss=1258.3068633494277\n",
      "Gradient Descent(4/49): loss=42281.90156052553\n",
      "Gradient Descent(5/49): loss=2263855.343249374\n",
      "Gradient Descent(6/49): loss=186755968.6589968\n",
      "Gradient Descent(7/49): loss=18620808936.25215\n",
      "Gradient Descent(8/49): loss=1959025754822.3677\n",
      "Gradient Descent(9/49): loss=208801743249125.25\n",
      "Gradient Descent(10/49): loss=2.23224925787928e+16\n",
      "Gradient Descent(11/49): loss=2.388108149805231e+18\n",
      "Gradient Descent(12/49): loss=2.5552585607723883e+20\n",
      "Gradient Descent(13/49): loss=2.7342088624426266e+22\n",
      "Gradient Descent(14/49): loss=2.925716150492851e+24\n",
      "Gradient Descent(15/49): loss=3.130642910761974e+26\n",
      "Gradient Descent(16/49): loss=3.349924905317543e+28\n",
      "Gradient Descent(17/49): loss=3.5845666012902414e+30\n",
      "Gradient Descent(18/49): loss=3.835643599141904e+32\n",
      "Gradient Descent(19/49): loss=4.1043070293723182e+34\n",
      "Gradient Descent(20/49): loss=4.391788699025488e+36\n",
      "Gradient Descent(21/49): loss=4.6994067072310275e+38\n",
      "Gradient Descent(22/49): loss=5.028571480752037e+40\n",
      "Gradient Descent(23/49): loss=5.3807922388379573e+42\n",
      "Gradient Descent(24/49): loss=5.757683912511158e+44\n",
      "Gradient Descent(25/49): loss=6.160974548903194e+46\n",
      "Gradient Descent(26/49): loss=6.592513234316309e+48\n",
      "Gradient Descent(27/49): loss=7.054278572271337e+50\n",
      "Gradient Descent(28/49): loss=7.54838775539703e+52\n",
      "Gradient Descent(29/49): loss=8.077106272751619e+54\n",
      "Gradient Descent(30/49): loss=8.642858297081598e+56\n",
      "Gradient Descent(31/49): loss=9.248237799647444e+58\n",
      "Gradient Descent(32/49): loss=9.896020443573408e+60\n",
      "Gradient Descent(33/49): loss=1.058917631025428e+63\n",
      "Gradient Descent(34/49): loss=1.1330883517168454e+65\n",
      "Gradient Descent(35/49): loss=1.2124542789538269e+67\n",
      "Gradient Descent(36/49): loss=1.2973793052642648e+69\n",
      "Gradient Descent(37/49): loss=1.3882528116279617e+71\n",
      "Gradient Descent(38/49): loss=1.4854914527871037e+73\n",
      "Gradient Descent(39/49): loss=1.5895410676069812e+75\n",
      "Gradient Descent(40/49): loss=1.7008787232459044e+77\n",
      "Gradient Descent(41/49): loss=1.820014902506332e+79\n",
      "Gradient Descent(42/49): loss=1.947495844397289e+81\n",
      "Gradient Descent(43/49): loss=2.0839060486382148e+83\n",
      "Gradient Descent(44/49): loss=2.2298709555885712e+85\n",
      "Gradient Descent(45/49): loss=2.3860598138898657e+87\n",
      "Gradient Descent(46/49): loss=2.553188748968363e+89\n",
      "Gradient Descent(47/49): loss=2.7320240464682623e+91\n",
      "Gradient Descent(48/49): loss=2.9233856656686754e+93\n",
      "Gradient Descent(49/49): loss=3.1281509989946305e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.2766767402584263\n",
      "Gradient Descent(2/49): loss=45.25913355158621\n",
      "Gradient Descent(3/49): loss=1157.1866317351878\n",
      "Gradient Descent(4/49): loss=35843.64440035254\n",
      "Gradient Descent(5/49): loss=1699786.168245731\n",
      "Gradient Descent(6/49): loss=130871027.27991118\n",
      "Gradient Descent(7/49): loss=12892814780.41363\n",
      "Gradient Descent(8/49): loss=1367465436937.7803\n",
      "Gradient Descent(9/49): loss=147667869174202.7\n",
      "Gradient Descent(10/49): loss=1.6012100759822042e+16\n",
      "Gradient Descent(11/49): loss=1.7378689227015693e+18\n",
      "Gradient Descent(12/49): loss=1.8865901606235475e+20\n",
      "Gradient Descent(13/49): loss=2.0481363033067463e+22\n",
      "Gradient Descent(14/49): loss=2.223539402023675e+24\n",
      "Gradient Descent(15/49): loss=2.4139699609492565e+26\n",
      "Gradient Descent(16/49): loss=2.6207110029915084e+28\n",
      "Gradient Descent(17/49): loss=2.845158442318762e+30\n",
      "Gradient Descent(18/49): loss=3.0888284818825597e+32\n",
      "Gradient Descent(19/49): loss=3.3533673587276606e+34\n",
      "Gradient Descent(20/49): loss=3.640562344140969e+36\n",
      "Gradient Descent(21/49): loss=3.9523537889040175e+38\n",
      "Gradient Descent(22/49): loss=4.2908482253962976e+40\n",
      "Gradient Descent(23/49): loss=4.658332598002438e+42\n",
      "Gradient Descent(24/49): loss=5.057289713780998e+44\n",
      "Gradient Descent(25/49): loss=5.490415016760426e+46\n",
      "Gradient Descent(26/49): loss=5.960634798937943e+48\n",
      "Gradient Descent(27/49): loss=6.471125970960783e+50\n",
      "Gradient Descent(28/49): loss=7.025337526047451e+52\n",
      "Gradient Descent(29/49): loss=7.627013842161887e+54\n",
      "Gradient Descent(30/49): loss=8.280219979872864e+56\n",
      "Gradient Descent(31/49): loss=8.989369146818353e+58\n",
      "Gradient Descent(32/49): loss=9.759252514328492e+60\n",
      "Gradient Descent(33/49): loss=1.0595071587658235e+63\n",
      "Gradient Descent(34/49): loss=1.1502473348526675e+65\n",
      "Gradient Descent(35/49): loss=1.2487588407395477e+67\n",
      "Gradient Descent(36/49): loss=1.3557072423253372e+69\n",
      "Gradient Descent(37/49): loss=1.4718151070745355e+71\n",
      "Gradient Descent(38/49): loss=1.5978668858456757e+73\n",
      "Gradient Descent(39/49): loss=1.734714212817857e+75\n",
      "Gradient Descent(40/49): loss=1.8832816593227246e+77\n",
      "Gradient Descent(41/49): loss=2.0445729804565268e+79\n",
      "Gradient Descent(42/49): loss=2.2196778966754464e+81\n",
      "Gradient Descent(43/49): loss=2.4097794561920157e+83\n",
      "Gradient Descent(44/49): loss=2.6161620279152457e+85\n",
      "Gradient Descent(45/49): loss=2.8402199789357866e+87\n",
      "Gradient Descent(46/49): loss=3.0834670951839124e+89\n",
      "Gradient Descent(47/49): loss=3.347546808907425e+91\n",
      "Gradient Descent(48/49): loss=3.6342433020704466e+93\n",
      "Gradient Descent(49/49): loss=3.945493560687319e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.321030948006428\n",
      "Gradient Descent(2/49): loss=47.25356080550137\n",
      "Gradient Descent(3/49): loss=1282.7347260190481\n",
      "Gradient Descent(4/49): loss=47032.5528180649\n",
      "Gradient Descent(5/49): loss=2880374.7066696733\n",
      "Gradient Descent(6/49): loss=262452335.96129808\n",
      "Gradient Descent(7/49): loss=27770616864.649494\n",
      "Gradient Descent(8/49): loss=3054770983256.706\n",
      "Gradient Descent(9/49): loss=339046455939406.5\n",
      "Gradient Descent(10/49): loss=3.77059847718902e+16\n",
      "Gradient Descent(11/49): loss=4.1952226055170104e+18\n",
      "Gradient Descent(12/49): loss=4.668127866176938e+20\n",
      "Gradient Descent(13/49): loss=5.1944554613643315e+22\n",
      "Gradient Descent(14/49): loss=5.780154282566373e+24\n",
      "Gradient Descent(15/49): loss=6.431900320396497e+26\n",
      "Gradient Descent(16/49): loss=7.15713623984954e+28\n",
      "Gradient Descent(17/49): loss=7.964147350104799e+30\n",
      "Gradient Descent(18/49): loss=8.862154029355349e+32\n",
      "Gradient Descent(19/49): loss=9.86141652010499e+34\n",
      "Gradient Descent(20/49): loss=1.0973352022289495e+37\n",
      "Gradient Descent(21/49): loss=1.2210665108326349e+39\n",
      "Gradient Descent(22/49): loss=1.3587492872654948e+41\n",
      "Gradient Descent(23/49): loss=1.5119566455044534e+43\n",
      "Gradient Descent(24/49): loss=1.6824390778441278e+45\n",
      "Gradient Descent(25/49): loss=1.8721444553811782e+47\n",
      "Gradient Descent(26/49): loss=2.0832402836873994e+49\n",
      "Gradient Descent(27/49): loss=2.318138467949755e+51\n",
      "Gradient Descent(28/49): loss=2.5795228705335807e+53\n",
      "Gradient Descent(29/49): loss=2.870379975830644e+55\n",
      "Gradient Descent(30/49): loss=3.194033012758417e+57\n",
      "Gradient Descent(31/49): loss=3.554179924781032e+59\n",
      "Gradient Descent(32/49): loss=3.9549356212843757e+61\n",
      "Gradient Descent(33/49): loss=4.400878993054243e+63\n",
      "Gradient Descent(34/49): loss=4.897105229039345e+65\n",
      "Gradient Descent(35/49): loss=5.449284032152334e+67\n",
      "Gradient Descent(36/49): loss=6.063724399260109e+69\n",
      "Gradient Descent(37/49): loss=6.747446705518916e+71\n",
      "Gradient Descent(38/49): loss=7.508262916660998e+73\n",
      "Gradient Descent(39/49): loss=8.354865845712845e+75\n",
      "Gradient Descent(40/49): loss=9.296928473956652e+77\n",
      "Gradient Descent(41/49): loss=1.0345214470944289e+80\n",
      "Gradient Descent(42/49): loss=1.1511701176322725e+82\n",
      "Gradient Descent(43/49): loss=1.2809716448619094e+84\n",
      "Gradient Descent(44/49): loss=1.4254090944570813e+86\n",
      "Gradient Descent(45/49): loss=1.5861327568885807e+88\n",
      "Gradient Descent(46/49): loss=1.7649790030512232e+90\n",
      "Gradient Descent(47/49): loss=1.9639912659785722e+92\n",
      "Gradient Descent(48/49): loss=2.1854433883755815e+94\n",
      "Gradient Descent(49/49): loss=2.43186560272955e+96\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=2.3395352220177243\n",
      "Gradient Descent(2/49): loss=48.2319349271927\n",
      "Gradient Descent(3/49): loss=1304.4461104595625\n",
      "Gradient Descent(4/49): loss=45682.16031547581\n",
      "Gradient Descent(5/49): loss=2568626.462885923\n",
      "Gradient Descent(6/49): loss=218762039.2571128\n",
      "Gradient Descent(7/49): loss=22201299777.41283\n",
      "Gradient Descent(8/49): loss=2366424898345.2646\n",
      "Gradient Descent(9/49): loss=255254363579166.66\n",
      "Gradient Descent(10/49): loss=2.760957045510701e+16\n",
      "Gradient Descent(11/49): loss=2.988306498797717e+18\n",
      "Gradient Descent(12/49): loss=3.234856674308799e+20\n",
      "Gradient Descent(13/49): loss=3.5018683186414004e+22\n",
      "Gradient Descent(14/49): loss=3.7909495828441065e+24\n",
      "Gradient Descent(15/49): loss=4.103902150467202e+26\n",
      "Gradient Descent(16/49): loss=4.4426916191575865e+28\n",
      "Gradient Descent(17/49): loss=4.8094496441507846e+30\n",
      "Gradient Descent(18/49): loss=5.206484802471335e+32\n",
      "Gradient Descent(19/49): loss=5.636296487953011e+34\n",
      "Gradient Descent(20/49): loss=6.101590486324246e+36\n",
      "Gradient Descent(21/49): loss=6.6052959691896614e+38\n",
      "Gradient Descent(22/49): loss=7.150583924166943e+40\n",
      "Gradient Descent(23/49): loss=7.740887114794184e+42\n",
      "Gradient Descent(24/49): loss=8.37992169026527e+44\n",
      "Gradient Descent(25/49): loss=9.071710579634521e+46\n",
      "Gradient Descent(26/49): loss=9.820608817415075e+48\n",
      "Gradient Descent(27/49): loss=1.063133095991912e+51\n",
      "Gradient Descent(28/49): loss=1.1508980764910056e+53\n",
      "Gradient Descent(29/49): loss=1.2459083321405316e+55\n",
      "Gradient Descent(30/49): loss=1.3487619831897004e+57\n",
      "Gradient Descent(31/49): loss=1.460106526595241e+59\n",
      "Gradient Descent(32/49): loss=1.5806429122240492e+61\n",
      "Gradient Descent(33/49): loss=1.7111299555588573e+63\n",
      "Gradient Descent(34/49): loss=1.8523891146869201e+65\n",
      "Gradient Descent(35/49): loss=2.00530966164393e+67\n",
      "Gradient Descent(36/49): loss=2.1708542806688822e+69\n",
      "Gradient Descent(37/49): loss=2.350065128612137e+71\n",
      "Gradient Descent(38/49): loss=2.544070395649592e+73\n",
      "Gradient Descent(39/49): loss=2.7540914076040215e+75\n",
      "Gradient Descent(40/49): loss=2.981450314585976e+77\n",
      "Gradient Descent(41/49): loss=3.227578414355612e+79\n",
      "Gradient Descent(42/49): loss=3.4940251628042557e+81\n",
      "Gradient Descent(43/49): loss=3.7824679282801746e+83\n",
      "Gradient Descent(44/49): loss=4.094722551163654e+85\n",
      "Gradient Descent(45/49): loss=4.432754775169204e+87\n",
      "Gradient Descent(46/49): loss=4.798692622336843e+89\n",
      "Gradient Descent(47/49): loss=5.19483978961838e+91\n",
      "Gradient Descent(48/49): loss=5.623690151394063e+93\n",
      "Gradient Descent(49/49): loss=6.087943459216753e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3806487892741544\n",
      "Gradient Descent(2/49): loss=48.98351972317769\n",
      "Gradient Descent(3/49): loss=1307.9525997636213\n",
      "Gradient Descent(4/49): loss=44471.570677503485\n",
      "Gradient Descent(5/49): loss=2406529.099984085\n",
      "Gradient Descent(6/49): loss=200681160.96436784\n",
      "Gradient Descent(7/49): loss=20239748728.944225\n",
      "Gradient Descent(8/49): loss=2154531799081.5386\n",
      "Gradient Descent(9/49): loss=232377199286829.97\n",
      "Gradient Descent(10/49): loss=2.5139680565058616e+16\n",
      "Gradient Descent(11/49): loss=2.7216459709312123e+18\n",
      "Gradient Descent(12/49): loss=2.9469570939263677e+20\n",
      "Gradient Descent(13/49): loss=3.191039329194025e+22\n",
      "Gradient Descent(14/49): loss=3.4553672798150044e+24\n",
      "Gradient Descent(15/49): loss=3.7415980460216084e+26\n",
      "Gradient Descent(16/49): loss=4.051541018063944e+28\n",
      "Gradient Descent(17/49): loss=4.387159209755301e+30\n",
      "Gradient Descent(18/49): loss=4.7505791760900116e+32\n",
      "Gradient Descent(19/49): loss=5.144103861567955e+34\n",
      "Gradient Descent(20/49): loss=5.57022703782765e+36\n",
      "Gradient Descent(21/49): loss=6.031649067927313e+38\n",
      "Gradient Descent(22/49): loss=6.531294009021815e+40\n",
      "Gradient Descent(23/49): loss=7.072328139872274e+42\n",
      "Gradient Descent(24/49): loss=7.65818002513988e+44\n",
      "Gradient Descent(25/49): loss=8.292562242248827e+46\n",
      "Gradient Descent(26/49): loss=8.97949490816887e+48\n",
      "Gradient Descent(27/49): loss=9.723331155120386e+50\n",
      "Gradient Descent(28/49): loss=1.0528784716624707e+53\n",
      "Gradient Descent(29/49): loss=1.1400959798705705e+55\n",
      "Gradient Descent(30/49): loss=1.2345383425540313e+57\n",
      "Gradient Descent(31/49): loss=1.336804046453248e+59\n",
      "Gradient Descent(32/49): loss=1.4475411552765067e+61\n",
      "Gradient Descent(33/49): loss=1.5674514165173466e+63\n",
      "Gradient Descent(34/49): loss=1.697294708469175e+65\n",
      "Gradient Descent(35/49): loss=1.837893855618319e+67\n",
      "Gradient Descent(36/49): loss=1.990139842930478e+69\n",
      "Gradient Descent(37/49): loss=2.1549974620742808e+71\n",
      "Gradient Descent(38/49): loss=2.333511425362078e+73\n",
      "Gradient Descent(39/49): loss=2.526812986152653e+75\n",
      "Gradient Descent(40/49): loss=2.736127107669552e+77\n",
      "Gradient Descent(41/49): loss=2.9627802256640704e+79\n",
      "Gradient Descent(42/49): loss=3.208208654115638e+81\n",
      "Gradient Descent(43/49): loss=3.4739676872372473e+83\n",
      "Gradient Descent(44/49): loss=3.761741455465024e+85\n",
      "Gradient Descent(45/49): loss=4.073353597890809e+87\n",
      "Gradient Descent(46/49): loss=4.410778818768819e+89\n",
      "Gradient Descent(47/49): loss=4.776155401331778e+91\n",
      "Gradient Descent(48/49): loss=5.171798758215095e+93\n",
      "Gradient Descent(49/49): loss=5.600216104362398e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3019459564794564\n",
      "Gradient Descent(2/49): loss=46.445314863420215\n",
      "Gradient Descent(3/49): loss=1202.9845581810632\n",
      "Gradient Descent(4/49): loss=37713.39719828847\n",
      "Gradient Descent(5/49): loss=1807681.4270722843\n",
      "Gradient Descent(6/49): loss=140653706.10269386\n",
      "Gradient Descent(7/49): loss=14013925762.290258\n",
      "Gradient Descent(8/49): loss=1503869836255.736\n",
      "Gradient Descent(9/49): loss=164330337131853.53\n",
      "Gradient Descent(10/49): loss=1.8031551573014548e+16\n",
      "Gradient Descent(11/49): loss=1.9804267198073326e+18\n",
      "Gradient Descent(12/49): loss=2.1755912158601744e+20\n",
      "Gradient Descent(13/49): loss=2.3901040478499797e+22\n",
      "Gradient Descent(14/49): loss=2.625796490481552e+24\n",
      "Gradient Descent(15/49): loss=2.8847381125611804e+26\n",
      "Gradient Descent(16/49): loss=3.169216904182348e+28\n",
      "Gradient Descent(17/49): loss=3.4817500450939153e+30\n",
      "Gradient Descent(18/49): loss=3.82510383129633e+32\n",
      "Gradient Descent(19/49): loss=4.202317577387517e+34\n",
      "Gradient Descent(20/49): loss=4.6167303753902714e+36\n",
      "Gradient Descent(21/49): loss=5.0720106164195934e+38\n",
      "Gradient Descent(22/49): loss=5.572188454435224e+40\n",
      "Gradient Descent(23/49): loss=6.121691479065195e+42\n",
      "Gradient Descent(24/49): loss=6.725383908215439e+44\n",
      "Gradient Descent(25/49): loss=7.388609646141353e+46\n",
      "Gradient Descent(26/49): loss=8.117239587821204e+48\n",
      "Gradient Descent(27/49): loss=8.91772358829451e+50\n",
      "Gradient Descent(28/49): loss=9.797147556977897e+52\n",
      "Gradient Descent(29/49): loss=1.0763296182356061e+55\n",
      "Gradient Descent(30/49): loss=1.1824721842288565e+57\n",
      "Gradient Descent(31/49): loss=1.299082030992539e+59\n",
      "Gradient Descent(32/49): loss=1.427191392538598e+61\n",
      "Gradient Descent(33/49): loss=1.5679342969434946e+63\n",
      "Gradient Descent(34/49): loss=1.7225566048004499e+65\n",
      "Gradient Descent(35/49): loss=1.8924270376162284e+67\n",
      "Gradient Descent(36/49): loss=2.0790492937768598e+69\n",
      "Gradient Descent(37/49): loss=2.2840753593325743e+71\n",
      "Gradient Descent(38/49): loss=2.5093201314303465e+73\n",
      "Gradient Descent(39/49): loss=2.7567774838399005e+75\n",
      "Gradient Descent(40/49): loss=3.0286379167868963e+77\n",
      "Gradient Descent(41/49): loss=3.3273079473294006e+79\n",
      "Gradient Descent(42/49): loss=3.655431411922243e+81\n",
      "Gradient Descent(43/49): loss=4.0159128697398007e+83\n",
      "Gradient Descent(44/49): loss=4.4119433139250534e+85\n",
      "Gradient Descent(45/49): loss=4.8470284183602e+87\n",
      "Gradient Descent(46/49): loss=5.325019570002375e+89\n",
      "Gradient Descent(47/49): loss=5.850147961480426e+91\n",
      "Gradient Descent(48/49): loss=6.4270620457454115e+93\n",
      "Gradient Descent(49/49): loss=7.060868684320912e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3467226807344925\n",
      "Gradient Descent(2/49): loss=48.4831906451009\n",
      "Gradient Descent(3/49): loss=1332.9098178400118\n",
      "Gradient Descent(4/49): loss=49436.712909227914\n",
      "Gradient Descent(5/49): loss=3059522.419580001\n",
      "Gradient Descent(6/49): loss=281831238.90187913\n",
      "Gradient Descent(7/49): loss=30165602649.56219\n",
      "Gradient Descent(8/49): loss=3357313855325.975\n",
      "Gradient Descent(9/49): loss=377040160525994.0\n",
      "Gradient Descent(10/49): loss=4.242884505058715e+16\n",
      "Gradient Descent(11/49): loss=4.776725950043321e+18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=5.378273929803388e+20\n",
      "Gradient Descent(13/49): loss=6.055711481531244e+22\n",
      "Gradient Descent(14/49): loss=6.818511556691932e+24\n",
      "Gradient Descent(15/49): loss=7.677405224493818e+26\n",
      "Gradient Descent(16/49): loss=8.644491528079091e+28\n",
      "Gradient Descent(17/49): loss=9.73339763282072e+30\n",
      "Gradient Descent(18/49): loss=1.0959468271183046e+33\n",
      "Gradient Descent(19/49): loss=1.2339981334411575e+35\n",
      "Gradient Descent(20/49): loss=1.389439119276042e+37\n",
      "Gradient Descent(21/49): loss=1.5644602808652984e+39\n",
      "Gradient Descent(22/49): loss=1.761528041439107e+41\n",
      "Gradient Descent(23/49): loss=1.9834195081516663e+43\n",
      "Gradient Descent(24/49): loss=2.2332616074101097e+45\n",
      "Gradient Descent(25/49): loss=2.5145751499546833e+47\n",
      "Gradient Descent(26/49): loss=2.831324446625368e+49\n",
      "Gradient Descent(27/49): loss=3.187973173998427e+51\n",
      "Gradient Descent(28/49): loss=3.5895472771575994e+53\n",
      "Gradient Descent(29/49): loss=4.0417057960337537e+55\n",
      "Gradient Descent(30/49): loss=4.550820613408434e+57\n",
      "Gradient Descent(31/49): loss=5.124066248400839e+59\n",
      "Gradient Descent(32/49): loss=5.76952096082207e+61\n",
      "Gradient Descent(33/49): loss=6.496280591171951e+63\n",
      "Gradient Descent(34/49): loss=7.314586740529544e+65\n",
      "Gradient Descent(35/49): loss=8.23597109666693e+67\n",
      "Gradient Descent(36/49): loss=9.273417940248955e+69\n",
      "Gradient Descent(37/49): loss=1.0441547121180768e+72\n",
      "Gradient Descent(38/49): loss=1.1756820083632793e+74\n",
      "Gradient Descent(39/49): loss=1.3237771843075129e+76\n",
      "Gradient Descent(40/49): loss=1.490527218437849e+78\n",
      "Gradient Descent(41/49): loss=1.678281976181883e+80\n",
      "Gradient Descent(42/49): loss=1.889687324548758e+82\n",
      "Gradient Descent(43/49): loss=2.1277224180671387e+84\n",
      "Gradient Descent(44/49): loss=2.3957416814586123e+86\n",
      "Gradient Descent(45/49): loss=2.6975220806725432e+88\n",
      "Gradient Descent(46/49): loss=3.037316348432742e+90\n",
      "Gradient Descent(47/49): loss=3.419912914357564e+92\n",
      "Gradient Descent(48/49): loss=3.85070338419792e+94\n",
      "Gradient Descent(49/49): loss=4.335758519119847e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.365457478263404\n",
      "Gradient Descent(2/49): loss=49.486263320440166\n",
      "Gradient Descent(3/49): loss=1355.5127576678165\n",
      "Gradient Descent(4/49): loss=48027.11716660294\n",
      "Gradient Descent(5/49): loss=2729171.916493271\n",
      "Gradient Descent(6/49): loss=234961224.8338777\n",
      "Gradient Descent(7/49): loss=24119293665.2715\n",
      "Gradient Descent(8/49): loss=2601118417077.1187\n",
      "Gradient Descent(9/49): loss=283895299558230.44\n",
      "Gradient Descent(10/49): loss=3.1072219551026532e+16\n",
      "Gradient Descent(11/49): loss=3.4030473692807204e+18\n",
      "Gradient Descent(12/49): loss=3.7275957373361875e+20\n",
      "Gradient Descent(13/49): loss=4.083237639700487e+22\n",
      "Gradient Descent(14/49): loss=4.4728463455809906e+24\n",
      "Gradient Descent(15/49): loss=4.899639246023069e+26\n",
      "Gradient Descent(16/49): loss=5.367158433353553e+28\n",
      "Gradient Descent(17/49): loss=5.879288462150182e+30\n",
      "Gradient Descent(18/49): loss=6.440285681387365e+32\n",
      "Gradient Descent(19/49): loss=7.054812864346029e+34\n",
      "Gradient Descent(20/49): loss=7.727977775151974e+36\n",
      "Gradient Descent(21/49): loss=8.465375575278411e+38\n",
      "Gradient Descent(22/49): loss=9.273135316923629e+40\n",
      "Gradient Descent(23/49): loss=1.0157970882991438e+43\n",
      "Gradient Descent(24/49): loss=1.112723679031706e+45\n",
      "Gradient Descent(25/49): loss=1.2188989318250342e+47\n",
      "Gradient Descent(26/49): loss=1.3352053470250207e+49\n",
      "Gradient Descent(27/49): loss=1.4626096324942955e+51\n",
      "Gradient Descent(28/49): loss=1.6021707386295085e+53\n",
      "Gradient Descent(29/49): loss=1.7550486600740776e+55\n",
      "Gradient Descent(30/49): loss=1.922514077284049e+57\n",
      "Gradient Descent(31/49): loss=2.1059589180845513e+59\n",
      "Gradient Descent(32/49): loss=2.306907927002194e+61\n",
      "Gradient Descent(33/49): loss=2.5270313385343687e+63\n",
      "Gradient Descent(34/49): loss=2.7681587596922945e+65\n",
      "Gradient Descent(35/49): loss=3.032294377206031e+67\n",
      "Gradient Descent(36/49): loss=3.3216336157892738e+69\n",
      "Gradient Descent(37/49): loss=3.638581385923197e+71\n",
      "Gradient Descent(38/49): loss=3.9857720728301215e+73\n",
      "Gradient Descent(39/49): loss=4.36609143278008e+75\n",
      "Gradient Descent(40/49): loss=4.782700578726345e+77\n",
      "Gradient Descent(41/49): loss=5.239062254631685e+79\n",
      "Gradient Descent(42/49): loss=5.7389696168719435e+81\n",
      "Gradient Descent(43/49): loss=6.286577761938819e+83\n",
      "Gradient Descent(44/49): loss=6.886438262491465e+85\n",
      "Gradient Descent(45/49): loss=7.543536998813989e+87\n",
      "Gradient Descent(46/49): loss=8.2633356001203e+89\n",
      "Gradient Descent(47/49): loss=9.051816840157286e+91\n",
      "Gradient Descent(48/49): loss=9.915534364423381e+93\n",
      "Gradient Descent(49/49): loss=1.0861667162318694e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4071334922087906\n",
      "Gradient Descent(2/49): loss=50.258281560256584\n",
      "Gradient Descent(3/49): loss=1359.221908231931\n",
      "Gradient Descent(4/49): loss=46760.01909951268\n",
      "Gradient Descent(5/49): loss=2557260.2242205725\n",
      "Gradient Descent(6/49): loss=215553031.31410083\n",
      "Gradient Descent(7/49): loss=21988616024.3572\n",
      "Gradient Descent(8/49): loss=2368217959310.783\n",
      "Gradient Descent(9/49): loss=258451151301329.75\n",
      "Gradient Descent(10/49): loss=2.829253793963966e+16\n",
      "Gradient Descent(11/49): loss=3.0993731563586514e+18\n",
      "Gradient Descent(12/49): loss=3.3958373166138635e+20\n",
      "Gradient Descent(13/49): loss=3.7207992448769063e+22\n",
      "Gradient Descent(14/49): loss=4.076893471899534e+24\n",
      "Gradient Descent(15/49): loss=4.467076139328312e+26\n",
      "Gradient Descent(16/49): loss=4.89460382750456e+28\n",
      "Gradient Descent(17/49): loss=5.363049209596847e+30\n",
      "Gradient Descent(18/49): loss=5.876328000629685e+32\n",
      "Gradient Descent(19/49): loss=6.43873095594842e+34\n",
      "Gradient Descent(20/49): loss=7.054959555014934e+36\n",
      "Gradient Descent(21/49): loss=7.730165257434284e+38\n",
      "Gradient Descent(22/49): loss=8.469992555639533e+40\n",
      "Gradient Descent(23/49): loss=9.280626157988195e+42\n",
      "Gradient Descent(24/49): loss=1.0168842690103502e+45\n",
      "Gradient Descent(25/49): loss=1.1142067345009115e+47\n",
      "Gradient Descent(26/49): loss=1.2208435955210472e+49\n",
      "Gradient Descent(27/49): loss=1.3376863005522152e+51\n",
      "Gradient Descent(28/49): loss=1.4657116155172215e+53\n",
      "Gradient Descent(29/49): loss=1.6059897892169018e+55\n",
      "Gradient Descent(30/49): loss=1.7596935002516223e+57\n",
      "Gradient Descent(31/49): loss=1.9281076602221977e+59\n",
      "Gradient Descent(32/49): loss=2.1126401551610476e+61\n",
      "Gradient Descent(33/49): loss=2.314833614988358e+63\n",
      "Gradient Descent(34/49): loss=2.536378309382133e+65\n",
      "Gradient Descent(35/49): loss=2.7791262778669845e+67\n",
      "Gradient Descent(36/49): loss=3.045106812245351e+69\n",
      "Gradient Descent(37/49): loss=3.336543420797681e+71\n",
      "Gradient Descent(38/49): loss=3.655872416067948e+73\n",
      "Gradient Descent(39/49): loss=4.005763281621335e+75\n",
      "Gradient Descent(40/49): loss=4.389140988033626e+77\n",
      "Gradient Descent(41/49): loss=4.809210444667008e+79\n",
      "Gradient Descent(42/49): loss=5.269483291639877e+81\n",
      "Gradient Descent(43/49): loss=5.773807255963037e+83\n",
      "Gradient Descent(44/49): loss=6.326398317250641e+85\n",
      "Gradient Descent(45/49): loss=6.931875951899344e+87\n",
      "Gradient Descent(46/49): loss=7.595301750365116e+89\n",
      "Gradient Descent(47/49): loss=8.322221730366166e+91\n",
      "Gradient Descent(48/49): loss=9.11871269973559e+93\n",
      "Gradient Descent(49/49): loss=9.991433056501795e+95\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3273839658288296\n",
      "Gradient Descent(2/49): loss=47.65486683769096\n",
      "Gradient Descent(3/49): loss=1250.284591755906\n",
      "Gradient Descent(4/49): loss=39668.08690597385\n",
      "Gradient Descent(5/49): loss=1921714.9992323325\n",
      "Gradient Descent(6/49): loss=151103264.50976178\n",
      "Gradient Descent(7/49): loss=15225032391.673853\n",
      "Gradient Descent(8/49): loss=1652952688314.2273\n",
      "Gradient Descent(9/49): loss=182757577493938.94\n",
      "Gradient Descent(10/49): loss=2.029146257289515e+16\n",
      "Gradient Descent(11/49): loss=2.2550987031595866e+18\n",
      "Gradient Descent(12/49): loss=2.5067532882180076e+20\n",
      "Gradient Descent(13/49): loss=2.7866271965611836e+22\n",
      "Gradient Descent(14/49): loss=3.097782745720267e+24\n",
      "Gradient Descent(15/49): loss=3.443690634234976e+26\n",
      "Gradient Descent(16/49): loss=3.828225824914325e+28\n",
      "Gradient Descent(17/49): loss=4.255700178475399e+30\n",
      "Gradient Descent(18/49): loss=4.730908092475837e+32\n",
      "Gradient Descent(19/49): loss=5.259179591578588e+34\n",
      "Gradient Descent(20/49): loss=5.846439938580683e+36\n",
      "Gradient Descent(21/49): loss=6.499276050900997e+38\n",
      "Gradient Descent(22/49): loss=7.225010370890415e+40\n",
      "Gradient Descent(23/49): loss=8.031782994220678e+42\n",
      "Gradient Descent(24/49): loss=8.928642971422198e+44\n",
      "Gradient Descent(25/49): loss=9.925649805100933e+46\n",
      "Gradient Descent(26/49): loss=1.1033986280876692e+49\n",
      "Gradient Descent(27/49): loss=1.2266083897500877e+51\n",
      "Gradient Descent(28/49): loss=1.363576230299423e+53\n",
      "Gradient Descent(29/49): loss=1.5158384300766182e+55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=1.6851028164319162e+57\n",
      "Gradient Descent(31/49): loss=1.873267919327787e+59\n",
      "Gradient Descent(32/49): loss=2.0824442659308695e+61\n",
      "Gradient Descent(33/49): loss=2.3149780530403295e+63\n",
      "Gradient Descent(34/49): loss=2.573477462871243e+65\n",
      "Gradient Descent(35/49): loss=2.8608419173599985e+67\n",
      "Gradient Descent(36/49): loss=3.180294599119058e+69\n",
      "Gradient Descent(37/49): loss=3.535418603807122e+71\n",
      "Gradient Descent(38/49): loss=3.930197129412983e+73\n",
      "Gradient Descent(39/49): loss=4.369058153230451e+75\n",
      "Gradient Descent(40/49): loss=4.856924097636974e+77\n",
      "Gradient Descent(41/49): loss=5.399267041745579e+79\n",
      "Gradient Descent(42/49): loss=6.002170098203329e+81\n",
      "Gradient Descent(43/49): loss=6.672395643561156e+83\n",
      "Gradient Descent(44/49): loss=7.417461167509998e+85\n",
      "Gradient Descent(45/49): loss=8.245723591737696e+87\n",
      "Gradient Descent(46/49): loss=9.166473004153667e+89\n",
      "Gradient Descent(47/49): loss=1.0190036859842331e+92\n",
      "Gradient Descent(48/49): loss=1.1327895817496198e+94\n",
      "Gradient Descent(49/49): loss=1.2592812510595163e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3725848255292052\n",
      "Gradient Descent(2/49): loss=49.736869408718455\n",
      "Gradient Descent(3/49): loss=1384.7121383148087\n",
      "Gradient Descent(4/49): loss=51947.95660802163\n",
      "Gradient Descent(5/49): loss=3248653.386357402\n",
      "Gradient Descent(6/49): loss=302514589.8852792\n",
      "Gradient Descent(7/49): loss=32751245383.274265\n",
      "Gradient Descent(8/49): loss=3687777308254.6353\n",
      "Gradient Descent(9/49): loss=419030318486905.25\n",
      "Gradient Descent(10/49): loss=4.771022987125698e+16\n",
      "Gradient Descent(11/49): loss=5.434692251718284e+18\n",
      "Gradient Descent(12/49): loss=6.191307096221331e+20\n",
      "Gradient Descent(13/49): loss=7.05341626245392e+22\n",
      "Gradient Descent(14/49): loss=8.035610168911664e+24\n",
      "Gradient Descent(15/49): loss=9.154585577042493e+26\n",
      "Gradient Descent(16/49): loss=1.0429383222918758e+29\n",
      "Gradient Descent(17/49): loss=1.1881700105455575e+31\n",
      "Gradient Descent(18/49): loss=1.3536255798930723e+33\n",
      "Gradient Descent(19/49): loss=1.542121244676959e+35\n",
      "Gradient Descent(20/49): loss=1.7568653917715438e+37\n",
      "Gradient Descent(21/49): loss=2.0015131857346247e+39\n",
      "Gradient Descent(22/49): loss=2.2802287822123983e+41\n",
      "Gradient Descent(23/49): loss=2.597756205816e+43\n",
      "Gradient Descent(24/49): loss=2.95950009819541e+45\n",
      "Gradient Descent(25/49): loss=3.3716177105504723e+47\n",
      "Gradient Descent(26/49): loss=3.841123706341453e+49\n",
      "Gradient Descent(27/49): loss=4.376009558037897e+51\n",
      "Gradient Descent(28/49): loss=4.985379570156564e+53\n",
      "Gradient Descent(29/49): loss=5.679605843840733e+55\n",
      "Gradient Descent(30/49): loss=6.47050482063413e+57\n",
      "Gradient Descent(31/49): loss=7.371538410408122e+59\n",
      "Gradient Descent(32/49): loss=8.398043126841597e+61\n",
      "Gradient Descent(33/49): loss=9.567491130577671e+63\n",
      "Gradient Descent(34/49): loss=1.0899787623275674e+66\n",
      "Gradient Descent(35/49): loss=1.2417609654511514e+68\n",
      "Gradient Descent(36/49): loss=1.414679210836526e+70\n",
      "Gradient Descent(37/49): loss=1.6116767439584798e+72\n",
      "Gradient Descent(38/49): loss=1.8361066644081488e+74\n",
      "Gradient Descent(39/49): loss=2.091788999079187e+76\n",
      "Gradient Descent(40/49): loss=2.3830757229341464e+78\n",
      "Gradient Descent(41/49): loss=2.7149248340717134e+80\n",
      "Gradient Descent(42/49): loss=3.092984743927531e+82\n",
      "Gradient Descent(43/49): loss=3.523690419016484e+84\n",
      "Gradient Descent(44/49): loss=4.0143729106474795e+86\n",
      "Gradient Descent(45/49): loss=4.573384136917994e+88\n",
      "Gradient Descent(46/49): loss=5.210239040906636e+90\n",
      "Gradient Descent(47/49): loss=5.93577754473996e+92\n",
      "Gradient Descent(48/49): loss=6.762349056159114e+94\n",
      "Gradient Descent(49/49): loss=7.704022668076631e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.3915515498059974\n",
      "Gradient Descent(2/49): loss=50.76510337261641\n",
      "Gradient Descent(3/49): loss=1408.2366370098373\n",
      "Gradient Descent(4/49): loss=50476.936886420335\n",
      "Gradient Descent(5/49): loss=2898708.7196881683\n",
      "Gradient Descent(6/49): loss=252254055.31312832\n",
      "Gradient Descent(7/49): loss=26190241953.33054\n",
      "Gradient Descent(8/49): loss=2857500969229.768\n",
      "Gradient Descent(9/49): loss=315552780797243.25\n",
      "Gradient Descent(10/49): loss=3.4944880712020332e+16\n",
      "Gradient Descent(11/49): loss=3.872392545848732e+18\n",
      "Gradient Descent(12/49): loss=4.2918146876590843e+20\n",
      "Gradient Descent(13/49): loss=4.756831404634991e+22\n",
      "Gradient Descent(14/49): loss=5.272275231791281e+24\n",
      "Gradient Descent(15/49): loss=5.843582799934185e+26\n",
      "Gradient Descent(16/49): loss=6.4768004753649805e+28\n",
      "Gradient Descent(17/49): loss=7.178635100167322e+30\n",
      "Gradient Descent(18/49): loss=7.956521632958818e+32\n",
      "Gradient Descent(19/49): loss=8.818701041542136e+34\n",
      "Gradient Descent(20/49): loss=9.774307385055353e+36\n",
      "Gradient Descent(21/49): loss=1.0833464524407034e+39\n",
      "Gradient Descent(22/49): loss=1.2007393361579576e+41\n",
      "Gradient Descent(23/49): loss=1.3308530712137006e+43\n",
      "Gradient Descent(24/49): loss=1.4750661062145826e+45\n",
      "Gradient Descent(25/49): loss=1.6349062603285744e+47\n",
      "Gradient Descent(26/49): loss=1.8120669092733522e+49\n",
      "Gradient Descent(27/49): loss=2.008424925245303e+51\n",
      "Gradient Descent(28/49): loss=2.2260605608455785e+53\n",
      "Gradient Descent(29/49): loss=2.4672794876546556e+55\n",
      "Gradient Descent(30/49): loss=2.73463722293747e+57\n",
      "Gradient Descent(31/49): loss=3.0309662032589242e+59\n",
      "Gradient Descent(32/49): loss=3.3594057918328506e+61\n",
      "Gradient Descent(33/49): loss=3.7234355375080674e+63\n",
      "Gradient Descent(34/49): loss=4.126912037742766e+65\n",
      "Gradient Descent(35/49): loss=4.5741097961010037e+67\n",
      "Gradient Descent(36/49): loss=5.0697665071220506e+69\n",
      "Gradient Descent(37/49): loss=5.619133248319843e+71\n",
      "Gradient Descent(38/49): loss=6.228030111054788e+73\n",
      "Gradient Descent(39/49): loss=6.90290785964244e+75\n",
      "Gradient Descent(40/49): loss=7.650916271926122e+77\n",
      "Gradient Descent(41/49): loss=8.479979885325464e+79\n",
      "Gradient Descent(42/49): loss=9.398881950830724e+81\n",
      "Gradient Descent(43/49): loss=1.0417357484363927e+84\n",
      "Gradient Descent(44/49): loss=1.1546196401311657e+86\n",
      "Gradient Descent(45/49): loss=1.2797357826854262e+88\n",
      "Gradient Descent(46/49): loss=1.418409679311704e+90\n",
      "Gradient Descent(47/49): loss=1.5721104665397036e+92\n",
      "Gradient Descent(48/49): loss=1.7424664785163194e+94\n",
      "Gradient Descent(49/49): loss=1.931282497874282e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.433793668625185\n",
      "Gradient Descent(2/49): loss=51.55796749827651\n",
      "Gradient Descent(3/49): loss=1412.1570263438869\n",
      "Gradient Descent(4/49): loss=49151.04903154736\n",
      "Gradient Descent(5/49): loss=2716451.4600368687\n",
      "Gradient Descent(6/49): loss=231429777.9040428\n",
      "Gradient Descent(7/49): loss=23876979572.796585\n",
      "Gradient Descent(8/49): loss=2601652549486.9575\n",
      "Gradient Descent(9/49): loss=287271285156976.56\n",
      "Gradient Descent(10/49): loss=3.181872140960304e+16\n",
      "Gradient Descent(11/49): loss=3.526830892657073e+18\n",
      "Gradient Descent(12/49): loss=3.909834194290486e+20\n",
      "Gradient Descent(13/49): loss=4.334595611280947e+22\n",
      "Gradient Descent(14/49): loss=4.805544956713616e+24\n",
      "Gradient Descent(15/49): loss=5.327673234143286e+26\n",
      "Gradient Descent(16/49): loss=5.906534132091917e+28\n",
      "Gradient Descent(17/49): loss=6.548289968966314e+30\n",
      "Gradient Descent(18/49): loss=7.259773941660306e+32\n",
      "Gradient Descent(19/49): loss=8.048562026719177e+34\n",
      "Gradient Descent(20/49): loss=8.923053431623032e+36\n",
      "Gradient Descent(21/49): loss=9.892559975725328e+38\n",
      "Gradient Descent(22/49): loss=1.0967405230737977e+41\n",
      "Gradient Descent(23/49): loss=1.2159034445311079e+43\n",
      "Gradient Descent(24/49): loss=1.3480136416265462e+45\n",
      "Gradient Descent(25/49): loss=1.4944778602165653e+47\n",
      "Gradient Descent(26/49): loss=1.656855691744257e+49\n",
      "Gradient Descent(27/49): loss=1.836876180198308e+51\n",
      "Gradient Descent(28/49): loss=2.0364562334501174e+53\n",
      "Gradient Descent(29/49): loss=2.2577210350183775e+55\n",
      "Gradient Descent(30/49): loss=2.503026673609716e+57\n",
      "Gradient Descent(31/49): loss=2.774985231402025e+59\n",
      "Gradient Descent(32/49): loss=3.0764925982168015e+61\n",
      "Gradient Descent(33/49): loss=3.410759307753404e+63\n",
      "Gradient Descent(34/49): loss=3.7813447242387033e+65\n",
      "Gradient Descent(35/49): loss=4.192194943520036e+67\n",
      "Gradient Descent(36/49): loss=4.647684812183551e+69\n",
      "Gradient Descent(37/49): loss=5.152664512128895e+71\n",
      "Gradient Descent(38/49): loss=5.712511206645041e+73\n",
      "Gradient Descent(39/49): loss=6.3331862979301664e+75\n",
      "Gradient Descent(40/49): loss=7.021298905748132e+77\n",
      "Gradient Descent(41/49): loss=7.784176243160592e+79\n",
      "Gradient Descent(42/49): loss=8.629941638715891e+81\n",
      "Gradient Descent(43/49): loss=9.567601035893864e+83\n",
      "Gradient Descent(44/49): loss=1.060713889087858e+86\n",
      "Gradient Descent(45/49): loss=1.1759624489805804e+88\n",
      "Gradient Descent(46/49): loss=1.303732981757772e+90\n",
      "Gradient Descent(47/49): loss=1.4453860233347213e+92\n",
      "Gradient Descent(48/49): loss=1.602429934413947e+94\n",
      "Gradient Descent(49/49): loss=1.7765369619263105e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.352990768306548\n",
      "Gradient Descent(2/49): loss=48.88809467341645\n",
      "Gradient Descent(3/49): loss=1299.1259163499071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=41711.004787821315\n",
      "Gradient Descent(5/49): loss=2042196.024661783\n",
      "Gradient Descent(6/49): loss=162260821.98918146\n",
      "Gradient Descent(7/49): loss=16532764681.252073\n",
      "Gradient Descent(8/49): loss=1815807229612.8281\n",
      "Gradient Descent(9/49): loss=203124432240433.9\n",
      "Gradient Descent(10/49): loss=2.281879190116072e+16\n",
      "Gradient Descent(11/49): loss=2.565909195760822e+18\n",
      "Gradient Descent(12/49): loss=2.8859230341015424e+20\n",
      "Gradient Descent(13/49): loss=3.246008811296252e+22\n",
      "Gradient Descent(14/49): loss=3.651064564391567e+24\n",
      "Gradient Descent(15/49): loss=4.106675947713133e+26\n",
      "Gradient Descent(16/49): loss=4.61914511425757e+28\n",
      "Gradient Descent(17/49): loss=5.19556562046887e+30\n",
      "Gradient Descent(18/49): loss=5.843917488245648e+32\n",
      "Gradient Descent(19/49): loss=6.573176884423906e+34\n",
      "Gradient Descent(20/49): loss=7.393440189109655e+36\n",
      "Gradient Descent(21/49): loss=8.316063725327405e+38\n",
      "Gradient Descent(22/49): loss=9.353820971049051e+40\n",
      "Gradient Descent(23/49): loss=1.0521079401244518e+43\n",
      "Gradient Descent(24/49): loss=1.1833999400922987e+45\n",
      "Gradient Descent(25/49): loss=1.3310758001180198e+47\n",
      "Gradient Descent(26/49): loss=1.497180053534596e+49\n",
      "Gradient Descent(27/49): loss=1.684012369921516e+51\n",
      "Gradient Descent(28/49): loss=1.894159393423411e+53\n",
      "Gradient Descent(29/49): loss=2.1305305541558203e+55\n",
      "Gradient Descent(30/49): loss=2.3963983484978388e+57\n",
      "Gradient Descent(31/49): loss=2.6954436459413685e+59\n",
      "Gradient Descent(32/49): loss=3.0318066497582727e+61\n",
      "Gradient Descent(33/49): loss=3.41014421702316e+63\n",
      "Gradient Descent(34/49): loss=3.8356943315707968e+65\n",
      "Gradient Descent(35/49): loss=4.314348622501236e+67\n",
      "Gradient Descent(36/49): loss=4.852733932230648e+69\n",
      "Gradient Descent(37/49): loss=5.458304063376668e+71\n",
      "Gradient Descent(38/49): loss=6.13944297469031e+73\n",
      "Gradient Descent(39/49): loss=6.9055808547530955e+75\n",
      "Gradient Descent(40/49): loss=7.767324680450546e+77\n",
      "Gradient Descent(41/49): loss=8.736605067770856e+79\n",
      "Gradient Descent(42/49): loss=9.826841448035446e+81\n",
      "Gradient Descent(43/49): loss=1.105312785638672e+84\n",
      "Gradient Descent(44/49): loss=1.2432441904723389e+86\n",
      "Gradient Descent(45/49): loss=1.398387983226071e+88\n",
      "Gradient Descent(46/49): loss=1.5728920887924382e+90\n",
      "Gradient Descent(47/49): loss=1.7691724704886817e+92\n",
      "Gradient Descent(48/49): loss=1.9899465784318823e+94\n",
      "Gradient Descent(49/49): loss=2.238270971918861e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.398617382390567\n",
      "Gradient Descent(2/49): loss=51.01490888855948\n",
      "Gradient Descent(3/49): loss=1438.1836785155006\n",
      "Gradient Descent(4/49): loss=54570.35492573436\n",
      "Gradient Descent(5/49): loss=3448259.956313469\n",
      "Gradient Descent(6/49): loss=324581779.98698354\n",
      "Gradient Descent(7/49): loss=35541472722.15556\n",
      "Gradient Descent(8/49): loss=4048551760474.2197\n",
      "Gradient Descent(9/49): loss=465410171553693.5\n",
      "Gradient Descent(10/49): loss=5.361233107662623e+16\n",
      "Gradient Descent(11/49): loss=6.178638118049303e+18\n",
      "Gradient Descent(12/49): loss=7.121397609463631e+20\n",
      "Gradient Descent(13/49): loss=8.208193822630993e+22\n",
      "Gradient Descent(14/49): loss=9.460893978048869e+24\n",
      "Gradient Descent(15/49): loss=1.090478831218744e+27\n",
      "Gradient Descent(16/49): loss=1.2569048798126569e+29\n",
      "Gradient Descent(17/49): loss=1.4487305212266467e+31\n",
      "Gradient Descent(18/49): loss=1.6698321273768046e+33\n",
      "Gradient Descent(19/49): loss=1.924677709573732e+35\n",
      "Gradient Descent(20/49): loss=2.218417185353412e+37\n",
      "Gradient Descent(21/49): loss=2.556986442182774e+39\n",
      "Gradient Descent(22/49): loss=2.947227288390382e+41\n",
      "Gradient Descent(23/49): loss=3.3970257120634297e+43\n",
      "Gradient Descent(24/49): loss=3.9154712410185895e+45\n",
      "Gradient Descent(25/49): loss=4.51304062398067e+47\n",
      "Gradient Descent(26/49): loss=5.201809544744001e+49\n",
      "Gradient Descent(27/49): loss=5.995696647623592e+51\n",
      "Gradient Descent(28/49): loss=6.910744805458716e+53\n",
      "Gradient Descent(29/49): loss=7.965445314032666e+55\n",
      "Gradient Descent(30/49): loss=9.1811115642307e+57\n",
      "Gradient Descent(31/49): loss=1.0582309743104245e+60\n",
      "Gradient Descent(32/49): loss=1.2197355267448297e+62\n",
      "Gradient Descent(33/49): loss=1.405888498182487e+64\n",
      "Gradient Descent(34/49): loss=1.6204516684011403e+66\n",
      "Gradient Descent(35/49): loss=1.8677609305565596e+68\n",
      "Gradient Descent(36/49): loss=2.1528139109236327e+70\n",
      "Gradient Descent(37/49): loss=2.481370960942751e+72\n",
      "Gradient Descent(38/49): loss=2.860071562417716e+74\n",
      "Gradient Descent(39/49): loss=3.296568498183155e+76\n",
      "Gradient Descent(40/49): loss=3.7996824995620906e+78\n",
      "Gradient Descent(41/49): loss=4.379580495729245e+80\n",
      "Gradient Descent(42/49): loss=5.047981067045002e+82\n",
      "Gradient Descent(43/49): loss=5.818391254161057e+84\n",
      "Gradient Descent(44/49): loss=6.706379508335699e+86\n",
      "Gradient Descent(45/49): loss=7.729890298742162e+88\n",
      "Gradient Descent(46/49): loss=8.909606734352055e+90\n",
      "Gradient Descent(47/49): loss=1.0269368528260644e+93\n",
      "Gradient Descent(48/49): loss=1.1836653750677809e+95\n",
      "Gradient Descent(49/49): loss=1.3643134105847633e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4178174366455076\n",
      "Gradient Descent(2/49): loss=52.06877260146925\n",
      "Gradient Descent(3/49): loss=1462.660535343323\n",
      "Gradient Descent(4/49): loss=53035.621031855415\n",
      "Gradient Descent(5/49): loss=3077682.546102191\n",
      "Gradient Descent(6/49): loss=270707296.090771\n",
      "Gradient Descent(7/49): loss=28425338176.680492\n",
      "Gradient Descent(8/49): loss=3137432041749.258\n",
      "Gradient Descent(9/49): loss=350524009685245.94\n",
      "Gradient Descent(10/49): loss=3.927326652776391e+16\n",
      "Gradient Descent(11/49): loss=4.4031466800107116e+18\n",
      "Gradient Descent(12/49): loss=4.9373706558126575e+20\n",
      "Gradient Descent(13/49): loss=5.5366069626852895e+22\n",
      "Gradient Descent(14/49): loss=6.208621992198756e+24\n",
      "Gradient Descent(15/49): loss=6.962217198296826e+26\n",
      "Gradient Descent(16/49): loss=7.807286331392221e+28\n",
      "Gradient Descent(17/49): loss=8.754930264966103e+30\n",
      "Gradient Descent(18/49): loss=9.817598906430654e+32\n",
      "Gradient Descent(19/49): loss=1.1009253745700263e+35\n",
      "Gradient Descent(20/49): loss=1.2345551019685779e+37\n",
      "Gradient Descent(21/49): loss=1.384404733914883e+39\n",
      "Gradient Descent(22/49): loss=1.5524430334129716e+41\n",
      "Gradient Descent(23/49): loss=1.7408777310482364e+43\n",
      "Gradient Descent(24/49): loss=1.9521845306026554e+45\n",
      "Gradient Descent(25/49): loss=2.189139635460049e+47\n",
      "Gradient Descent(26/49): loss=2.454856223076205e+49\n",
      "Gradient Descent(27/49): loss=2.7528253467074627e+51\n",
      "Gradient Descent(28/49): loss=3.086961801770646e+53\n",
      "Gradient Descent(29/49): loss=3.4616555594377205e+55\n",
      "Gradient Descent(30/49): loss=3.8818294432127155e+57\n",
      "Gradient Descent(31/49): loss=4.353003806259897e+59\n",
      "Gradient Descent(32/49): loss=4.881369059231699e+61\n",
      "Gradient Descent(33/49): loss=5.47386700148487e+63\n",
      "Gradient Descent(34/49): loss=6.138282024236422e+65\n",
      "Gradient Descent(35/49): loss=6.883343383908001e+67\n",
      "Gradient Descent(36/49): loss=7.718839889355436e+69\n",
      "Gradient Descent(37/49): loss=8.65574850977102e+71\n",
      "Gradient Descent(38/49): loss=9.706378592944266e+73\n",
      "Gradient Descent(39/49): loss=1.0884533588656561e+76\n",
      "Gradient Descent(40/49): loss=1.2205692401975463e+78\n",
      "Gradient Descent(41/49): loss=1.368721275911142e+80\n",
      "Gradient Descent(42/49): loss=1.534855925771705e+82\n",
      "Gradient Descent(43/49): loss=1.7211559097802736e+84\n",
      "Gradient Descent(44/49): loss=1.930068885313885e+86\n",
      "Gradient Descent(45/49): loss=2.164339605081098e+88\n",
      "Gradient Descent(46/49): loss=2.427045978393018e+90\n",
      "Gradient Descent(47/49): loss=2.721639509532063e+92\n",
      "Gradient Descent(48/49): loss=3.0519906445078027e+94\n",
      "Gradient Descent(49/49): loss=3.4224396219780028e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.460629318523336\n",
      "Gradient Descent(2/49): loss=52.882900559183994\n",
      "Gradient Descent(3/49): loss=1466.8010111680694\n",
      "Gradient Descent(4/49): loss=51648.584081578934\n",
      "Gradient Descent(5/49): loss=2884523.0918406504\n",
      "Gradient Descent(6/49): loss=248372802.20501775\n",
      "Gradient Descent(7/49): loss=25915050353.42243\n",
      "Gradient Descent(8/49): loss=2856528755780.9297\n",
      "Gradient Descent(9/49): loss=319108166119399.06\n",
      "Gradient Descent(10/49): loss=3.5759853032521892e+16\n",
      "Gradient Descent(11/49): loss=4.0102164052237107e+18\n",
      "Gradient Descent(12/49): loss=4.497927255092665e+20\n",
      "Gradient Descent(13/49): loss=5.045146442697931e+22\n",
      "Gradient Descent(14/49): loss=5.658990765092273e+24\n",
      "Gradient Descent(15/49): loss=6.347534699168758e+26\n",
      "Gradient Descent(16/49): loss=7.119858906471261e+28\n",
      "Gradient Descent(17/49): loss=7.986155062770073e+30\n",
      "Gradient Descent(18/49): loss=8.957856486744717e+32\n",
      "Gradient Descent(19/49): loss=1.0047788037131844e+35\n",
      "Gradient Descent(20/49): loss=1.127033512130142e+37\n",
      "Gradient Descent(21/49): loss=1.2641633493482298e+39\n",
      "Gradient Descent(22/49): loss=1.4179782204776547e+41\n",
      "Gradient Descent(23/49): loss=1.5905082478610178e+43\n",
      "Gradient Descent(24/49): loss=1.7840305654845656e+45\n",
      "Gradient Descent(25/49): loss=2.001099373653814e+47\n",
      "Gradient Descent(26/49): loss=2.244579650545846e+49\n",
      "Gradient Descent(27/49): loss=2.517684965562592e+51\n",
      "Gradient Descent(28/49): loss=2.8240198935593503e+53\n",
      "Gradient Descent(29/49): loss=3.1676275897516297e+55\n",
      "Gradient Descent(30/49): loss=3.553043153215533e+57\n",
      "Gradient Descent(31/49): loss=3.9853534832994457e+59\n",
      "Gradient Descent(32/49): loss=4.4702644189594803e+61\n",
      "Gradient Descent(33/49): loss=5.014176047157254e+63\n",
      "Gradient Descent(34/49): loss=5.624267174275521e+65\n",
      "Gradient Descent(35/49): loss=6.308590075445501e+67\n",
      "Gradient Descent(36/49): loss=7.076176772334204e+69\n",
      "Gradient Descent(37/49): loss=7.937158242095084e+71\n",
      "Gradient Descent(38/49): loss=8.90289813086138e+73\n",
      "Gradient Descent(39/49): loss=9.986142736594098e+75\n",
      "Gradient Descent(40/49): loss=1.1201189240832349e+78\n",
      "Gradient Descent(41/49): loss=1.2564074409748117e+80\n",
      "Gradient Descent(42/49): loss=1.409278625507457e+82\n",
      "Gradient Descent(43/49): loss=1.5807501448504837e+84\n",
      "Gradient Descent(44/49): loss=1.773085162307838e+86\n",
      "Gradient Descent(45/49): loss=1.988822207632043e+88\n",
      "Gradient Descent(46/49): loss=2.230808681756756e+90\n",
      "Gradient Descent(47/49): loss=2.5022384381590306e+92\n",
      "Gradient Descent(48/49): loss=2.8066939368686763e+94\n",
      "Gradient Descent(49/49): loss=3.148193527492604e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.378766363912609\n",
      "Gradient Descent(2/49): loss=50.14530555183335\n",
      "Gradient Descent(3/49): loss=1349.54847801293\n",
      "Gradient Descent(4/49): loss=43845.54786858294\n",
      "Gradient Descent(5/49): loss=2169447.210580087\n",
      "Gradient Descent(6/49): loss=174169768.59020978\n",
      "Gradient Descent(7/49): loss=17944197814.277763\n",
      "Gradient Descent(8/49): loss=1993613706644.5352\n",
      "Gradient Descent(9/49): loss=225621884236869.84\n",
      "Gradient Descent(10/49): loss=2.564333462255753e+16\n",
      "Gradient Descent(11/49): loss=2.917358658076103e+18\n",
      "Gradient Descent(12/49): loss=3.319716253237875e+20\n",
      "Gradient Descent(13/49): loss=3.7777554612147307e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=4.299041621804812e+24\n",
      "Gradient Descent(15/49): loss=4.892271785641721e+26\n",
      "Gradient Descent(16/49): loss=5.5673657779180286e+28\n",
      "Gradient Descent(17/49): loss=6.335618127753336e+30\n",
      "Gradient Descent(18/49): loss=7.209883429239643e+32\n",
      "Gradient Descent(19/49): loss=8.204790498618273e+34\n",
      "Gradient Descent(20/49): loss=9.336986913998645e+36\n",
      "Gradient Descent(21/49): loss=1.0625417514002926e+39\n",
      "Gradient Descent(22/49): loss=1.2091641382359555e+41\n",
      "Gradient Descent(23/49): loss=1.3760192588154212e+43\n",
      "Gradient Descent(24/49): loss=1.5658990709077687e+45\n",
      "Gradient Descent(25/49): loss=1.7819808004598944e+47\n",
      "Gradient Descent(26/49): loss=2.0278801055597197e+49\n",
      "Gradient Descent(27/49): loss=2.307711576614003e+51\n",
      "Gradient Descent(28/49): loss=2.6261575850749707e+53\n",
      "Gradient Descent(29/49): loss=2.988546632749523e+55\n",
      "Gradient Descent(30/49): loss=3.400942512695189e+57\n",
      "Gradient Descent(31/49): loss=3.870245773617418e+59\n",
      "Gradient Descent(32/49): loss=4.4043091855536977e+61\n",
      "Gradient Descent(33/49): loss=5.012069138911986e+63\n",
      "Gradient Descent(34/49): loss=5.703695175541125e+65\n",
      "Gradient Descent(35/49): loss=6.490760153909949e+67\n",
      "Gradient Descent(36/49): loss=7.38643389573995e+69\n",
      "Gradient Descent(37/49): loss=8.405703554346993e+71\n",
      "Gradient Descent(38/49): loss=9.565624392078086e+73\n",
      "Gradient Descent(39/49): loss=1.0885605163056078e+76\n",
      "Gradient Descent(40/49): loss=1.2387732876495496e+78\n",
      "Gradient Descent(41/49): loss=1.4097142374795577e+80\n",
      "Gradient Descent(42/49): loss=1.6042436910495713e+82\n",
      "Gradient Descent(43/49): loss=1.8256166759539436e+84\n",
      "Gradient Descent(44/49): loss=2.077537387939264e+86\n",
      "Gradient Descent(45/49): loss=2.3642211725690874e+88\n",
      "Gradient Descent(46/49): loss=2.690465059869792e+90\n",
      "Gradient Descent(47/49): loss=3.061728032201995e+92\n",
      "Gradient Descent(48/49): loss=3.4842223684649536e+94\n",
      "Gradient Descent(49/49): loss=3.965017593081365e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.424820351318577\n",
      "Gradient Descent(2/49): loss=52.317622887470485\n",
      "Gradient Descent(3/49): loss=1493.367237828089\n",
      "Gradient Descent(4/49): loss=57308.10729816813\n",
      "Gradient Descent(5/49): loss=3658855.633618549\n",
      "Gradient Descent(6/49): loss=348116525.7414527\n",
      "Gradient Descent(7/49): loss=38551139523.19563\n",
      "Gradient Descent(8/49): loss=4442216530003.248\n",
      "Gradient Descent(9/49): loss=516609017881984.9\n",
      "Gradient Descent(10/49): loss=6.020384109169956e+16\n",
      "Gradient Descent(11/49): loss=7.019198498376598e+18\n",
      "Gradient Descent(12/49): loss=8.184567176408552e+20\n",
      "Gradient Descent(13/49): loss=9.543637146573482e+22\n",
      "Gradient Descent(14/49): loss=1.1128441629143139e+25\n",
      "Gradient Descent(15/49): loss=1.2976431640895198e+27\n",
      "Gradient Descent(16/49): loss=1.5131302907040157e+29\n",
      "Gradient Descent(17/49): loss=1.7644013936414648e+31\n",
      "Gradient Descent(18/49): loss=2.0573987161508948e+33\n",
      "Gradient Descent(19/49): loss=2.3990513181840436e+35\n",
      "Gradient Descent(20/49): loss=2.797438914362571e+37\n",
      "Gradient Descent(21/49): loss=3.261982943581828e+39\n",
      "Gradient Descent(22/49): loss=3.803669374127454e+41\n",
      "Gradient Descent(23/49): loss=4.435308509580971e+43\n",
      "Gradient Descent(24/49): loss=5.1718379386496885e+45\n",
      "Gradient Descent(25/49): loss=6.03067579310115e+47\n",
      "Gradient Descent(26/49): loss=7.032132667907139e+49\n",
      "Gradient Descent(27/49): loss=8.199891945048023e+51\n",
      "Gradient Descent(28/49): loss=9.561569880119218e+53\n",
      "Gradient Descent(29/49): loss=1.1149368697183611e+56\n",
      "Gradient Descent(30/49): loss=1.3000838136863088e+58\n",
      "Gradient Descent(31/49): loss=1.5159763467469791e+60\n",
      "Gradient Descent(32/49): loss=1.7677200959682287e+62\n",
      "Gradient Descent(33/49): loss=2.0612685312638523e+64\n",
      "Gradient Descent(34/49): loss=2.403563758577696e+66\n",
      "Gradient Descent(35/49): loss=2.802700693250236e+68\n",
      "Gradient Descent(36/49): loss=3.268118496092475e+70\n",
      "Gradient Descent(37/49): loss=3.810823799424574e+72\n",
      "Gradient Descent(38/49): loss=4.443651002136174e+74\n",
      "Gradient Descent(39/49): loss=5.1815657894671657e+76\n",
      "Gradient Descent(40/49): loss=6.04201905542755e+78\n",
      "Gradient Descent(41/49): loss=7.045359597741115e+80\n",
      "Gradient Descent(42/49): loss=8.21531534510685e+82\n",
      "Gradient Descent(43/49): loss=9.579554497287397e+84\n",
      "Gradient Descent(44/49): loss=1.1170339848385513e+87\n",
      "Gradient Descent(45/49): loss=1.3025291767353603e+89\n",
      "Gradient Descent(46/49): loss=1.518827787940643e+91\n",
      "Gradient Descent(47/49): loss=1.7710450488352677e+93\n",
      "Gradient Descent(48/49): loss=2.065145627376743e+95\n",
      "Gradient Descent(49/49): loss=2.4080846871050124e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.444255138781933\n",
      "Gradient Descent(2/49): loss=53.397590570498814\n",
      "Gradient Descent(3/49): loss=1518.828063455464\n",
      "Gradient Descent(4/49): loss=55707.297837843165\n",
      "Gradient Descent(5/49): loss=3266558.316337155\n",
      "Gradient Descent(6/49): loss=290391361.6829166\n",
      "Gradient Descent(7/49): loss=30836522420.350723\n",
      "Gradient Descent(8/49): loss=3442918270126.318\n",
      "Gradient Descent(9/49): loss=389133474064049.8\n",
      "Gradient Descent(10/49): loss=4.410787368281895e+16\n",
      "Gradient Descent(11/49): loss=5.002915073847558e+18\n",
      "Gradient Descent(12/49): loss=5.6754103132411134e+20\n",
      "Gradient Descent(13/49): loss=6.438533462733623e+22\n",
      "Gradient Descent(14/49): loss=7.304327794769593e+24\n",
      "Gradient Descent(15/49): loss=8.28656206943262e+26\n",
      "Gradient Descent(16/49): loss=9.400884444497372e+28\n",
      "Gradient Descent(17/49): loss=1.0665054666644234e+31\n",
      "Gradient Descent(18/49): loss=1.2099222624971221e+33\n",
      "Gradient Descent(19/49): loss=1.3726248342507168e+35\n",
      "Gradient Descent(20/49): loss=1.557206604443764e+37\n",
      "Gradient Descent(21/49): loss=1.7666097458956475e+39\n",
      "Gradient Descent(22/49): loss=2.0041720768476904e+41\n",
      "Gradient Descent(23/49): loss=2.2736802641399966e+43\n",
      "Gradient Descent(24/49): loss=2.579430181309591e+45\n",
      "Gradient Descent(25/49): loss=2.9262953833893637e+47\n",
      "Gradient Descent(26/49): loss=3.319804789792799e+49\n",
      "Gradient Descent(27/49): loss=3.7662308135027667e+51\n",
      "Gradient Descent(28/49): loss=4.2726893413099816e+53\n",
      "Gradient Descent(29/49): loss=4.847253158753004e+55\n",
      "Gradient Descent(30/49): loss=5.4990806277147556e+57\n",
      "Gradient Descent(31/49): loss=6.238561667756569e+59\n",
      "Gradient Descent(32/49): loss=7.077483368083326e+61\n",
      "Gradient Descent(33/49): loss=8.029217869943794e+63\n",
      "Gradient Descent(34/49): loss=9.108935514246569e+65\n",
      "Gradient Descent(35/49): loss=1.0333846651900848e+68\n",
      "Gradient Descent(36/49): loss=1.172347597125753e+70\n",
      "Gradient Descent(37/49): loss=1.3299973715341763e+72\n",
      "Gradient Descent(38/49): loss=1.5088468749580444e+74\n",
      "Gradient Descent(39/49): loss=1.7117469107811607e+76\n",
      "Gradient Descent(40/49): loss=1.941931640114495e+78\n",
      "Gradient Descent(41/49): loss=2.203070133280837e+80\n",
      "Gradient Descent(42/49): loss=2.499324853612157e+82\n",
      "Gradient Descent(43/49): loss=2.8354180057722773e+84\n",
      "Gradient Descent(44/49): loss=3.216706806175418e+86\n",
      "Gradient Descent(45/49): loss=3.6492688752877546e+88\n",
      "Gradient Descent(46/49): loss=4.139999112936716e+90\n",
      "Gradient Descent(47/49): loss=4.6967196007901006e+92\n",
      "Gradient Descent(48/49): loss=5.3283042838136746e+94\n",
      "Gradient Descent(49/49): loss=6.044820418091566e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.487640441903244\n",
      "Gradient Descent(2/49): loss=54.233405847094986\n",
      "Gradient Descent(3/49): loss=1523.1977497921143\n",
      "Gradient Descent(4/49): loss=54256.672347269145\n",
      "Gradient Descent(5/49): loss=3061913.5964279324\n",
      "Gradient Descent(6/49): loss=266446864.88149154\n",
      "Gradient Descent(7/49): loss=28113720534.72592\n",
      "Gradient Descent(8/49): loss=3134673759507.3857\n",
      "Gradient Descent(9/49): loss=354257199464191.1\n",
      "Gradient Descent(10/49): loss=4.016191082377725e+16\n",
      "Gradient Descent(11/49): loss=4.556456090655757e+18\n",
      "Gradient Descent(12/49): loss=5.170270743347612e+20\n",
      "Gradient Descent(13/49): loss=5.867002894249209e+22\n",
      "Gradient Descent(14/49): loss=6.657684736326211e+24\n",
      "Gradient Descent(15/49): loss=7.554940556272963e+26\n",
      "Gradient Descent(16/49): loss=8.573123606756523e+28\n",
      "Gradient Descent(17/49): loss=9.728528764289156e+30\n",
      "Gradient Descent(18/49): loss=1.1039648869911122e+33\n",
      "Gradient Descent(19/49): loss=1.2527469553010292e+35\n",
      "Gradient Descent(20/49): loss=1.4215804819950766e+37\n",
      "Gradient Descent(21/49): loss=1.6131678140375382e+39\n",
      "Gradient Descent(22/49): loss=1.8305754963543214e+41\n",
      "Gradient Descent(23/49): loss=2.0772833543720728e+43\n",
      "Gradient Descent(24/49): loss=2.3572401919296924e+45\n",
      "Gradient Descent(25/49): loss=2.674926995760473e+47\n",
      "Gradient Descent(26/49): loss=3.0354286581174576e+49\n",
      "Gradient Descent(27/49): loss=3.44451536551249e+51\n",
      "Gradient Descent(28/49): loss=3.908734956271443e+53\n",
      "Gradient Descent(29/49): loss=4.4355177251778145e+55\n",
      "Gradient Descent(30/49): loss=5.0332953527076244e+57\n",
      "Gradient Descent(31/49): loss=5.71163586243411e+59\n",
      "Gradient Descent(32/49): loss=6.481396766731417e+61\n",
      "Gradient Descent(33/49): loss=7.354898852024404e+63\n",
      "Gradient Descent(34/49): loss=8.34612338518958e+65\n",
      "Gradient Descent(35/49): loss=9.470935897593351e+67\n",
      "Gradient Descent(36/49): loss=1.074734012865133e+70\n",
      "Gradient Descent(37/49): loss=1.219576619352622e+72\n",
      "Gradient Descent(38/49): loss=1.383939758737477e+74\n",
      "Gradient Descent(39/49): loss=1.570454226017388e+76\n",
      "Gradient Descent(40/49): loss=1.7821053701541404e+78\n",
      "Gradient Descent(41/49): loss=2.0222808775434847e+80\n",
      "Gradient Descent(42/49): loss=2.2948249952943676e+82\n",
      "Gradient Descent(43/49): loss=2.6041000622152398e+84\n",
      "Gradient Descent(44/49): loss=2.9550563323717083e+86\n",
      "Gradient Descent(45/49): loss=3.353311208810368e+88\n",
      "Gradient Descent(46/49): loss=3.8052391556637403e+90\n",
      "Gradient Descent(47/49): loss=4.3180737277689e+92\n",
      "Gradient Descent(48/49): loss=4.900023350883255e+94\n",
      "Gradient Descent(49/49): loss=5.560402705677583e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4047107526470146\n",
      "Gradient Descent(2/49): loss=51.426808636399144\n",
      "Gradient Descent(3/49): loss=1401.5929947805973\n",
      "Gradient Descent(4/49): loss=46075.221638705116\n",
      "Gradient Descent(5/49): loss=2303805.3397848057\n",
      "Gradient Descent(6/49): loss=186875875.82750118\n",
      "Gradient Descent(7/49): loss=19466879164.215763\n",
      "Gradient Descent(8/49): loss=2187645730264.6946\n",
      "Gradient Descent(9/49): loss=250458439577104.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=2.879800199655902e+16\n",
      "Gradient Descent(11/49): loss=3.3144767309097467e+18\n",
      "Gradient Descent(12/49): loss=3.8156136611433375e+20\n",
      "Gradient Descent(13/49): loss=4.392743053098389e+22\n",
      "Gradient Descent(14/49): loss=5.05722409343799e+24\n",
      "Gradient Descent(15/49): loss=5.82223498515523e+26\n",
      "Gradient Descent(16/49): loss=6.702973743209583e+28\n",
      "Gradient Descent(17/49): loss=7.71694429418641e+30\n",
      "Gradient Descent(18/49): loss=8.884300213683334e+32\n",
      "Gradient Descent(19/49): loss=1.0228244214994133e+35\n",
      "Gradient Descent(20/49): loss=1.177548905036222e+37\n",
      "Gradient Descent(21/49): loss=1.3556788390085887e+39\n",
      "Gradient Descent(22/49): loss=1.5607548075707332e+41\n",
      "Gradient Descent(23/49): loss=1.796852985609458e+43\n",
      "Gradient Descent(24/49): loss=2.0686661583521747e+45\n",
      "Gradient Descent(25/49): loss=2.3815969970766708e+47\n",
      "Gradient Descent(26/49): loss=2.7418654448351717e+49\n",
      "Gradient Descent(27/49): loss=3.156632346618426e+51\n",
      "Gradient Descent(28/49): loss=3.6341417812780326e+53\n",
      "Gradient Descent(29/49): loss=4.183884924254369e+55\n",
      "Gradient Descent(30/49): loss=4.8167886981137164e+57\n",
      "Gradient Descent(31/49): loss=5.545432960590093e+59\n",
      "Gradient Descent(32/49): loss=6.384300547051568e+61\n",
      "Gradient Descent(33/49): loss=7.350065137338798e+63\n",
      "Gradient Descent(34/49): loss=8.461922668736771e+65\n",
      "Gradient Descent(35/49): loss=9.741972882380039e+67\n",
      "Gradient Descent(36/49): loss=1.1215658586867682e+70\n",
      "Gradient Descent(37/49): loss=1.2912271370072265e+72\n",
      "Gradient Descent(38/49): loss=1.486553381088171e+74\n",
      "Gradient Descent(39/49): loss=1.711426976315449e+76\n",
      "Gradient Descent(40/49): loss=1.97031760347295e+78\n",
      "Gradient Descent(41/49): loss=2.268371079970483e+80\n",
      "Gradient Descent(42/49): loss=2.611511640243646e+82\n",
      "Gradient Descent(43/49): loss=3.0065596882925313e+84\n",
      "Gradient Descent(44/49): loss=3.4613673628589773e+86\n",
      "Gradient Descent(45/49): loss=3.98497460979059e+88\n",
      "Gradient Descent(46/49): loss=4.587788863750974e+90\n",
      "Gradient Descent(47/49): loss=5.281791910705121e+92\n",
      "Gradient Descent(48/49): loss=6.080778042863405e+94\n",
      "Gradient Descent(49/49): loss=7.000628239750822e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.451193732313235\n",
      "Gradient Descent(2/49): loss=53.64532721894032\n",
      "Gradient Descent(3/49): loss=1550.306434269919\n",
      "Gradient Descent(4/49): loss=60165.544812739594\n",
      "Gradient Descent(5/49): loss=3880975.853120684\n",
      "Gradient Descent(6/49): loss=373207077.51777554\n",
      "Gradient Descent(7/49): loss=41796083634.046715\n",
      "Gradient Descent(8/49): loss=4871553531756.646\n",
      "Gradient Descent(9/49): loss=573095279134075.9\n",
      "Gradient Descent(10/49): loss=6.756058856104357e+16\n",
      "Gradient Descent(11/49): loss=7.96825043229688e+18\n",
      "Gradient Descent(12/49): loss=9.39891795029316e+20\n",
      "Gradient Descent(13/49): loss=1.1086714497102078e+23\n",
      "Gradient Descent(14/49): loss=1.3077662620326759e+25\n",
      "Gradient Descent(15/49): loss=1.5426162399144829e+27\n",
      "Gradient Descent(16/49): loss=1.819641284514197e+29\n",
      "Gradient Descent(17/49): loss=2.1464149737186536e+31\n",
      "Gradient Descent(18/49): loss=2.5318711650352995e+33\n",
      "Gradient Descent(19/49): loss=2.9865481248182267e+35\n",
      "Gradient Descent(20/49): loss=3.5228766103029483e+37\n",
      "Gradient Descent(21/49): loss=4.155519715243526e+39\n",
      "Gradient Descent(22/49): loss=4.901773753253126e+41\n",
      "Gradient Descent(23/49): loss=5.782041134377062e+43\n",
      "Gradient Descent(24/49): loss=6.820388162039332e+45\n",
      "Gradient Descent(25/49): loss=8.045203000084509e+47\n",
      "Gradient Descent(26/49): loss=9.489971798499708e+49\n",
      "Gradient Descent(27/49): loss=1.1194194196886404e+52\n",
      "Gradient Descent(28/49): loss=1.3204463235329862e+54\n",
      "Gradient Descent(29/49): loss=1.5575739197169971e+56\n",
      "Gradient Descent(30/49): loss=1.8372852210239858e+58\n",
      "Gradient Descent(31/49): loss=2.1672274687331878e+60\n",
      "Gradient Descent(32/49): loss=2.5564212064003354e+62\n",
      "Gradient Descent(33/49): loss=3.015506899399743e+64\n",
      "Gradient Descent(34/49): loss=3.557035842748095e+66\n",
      "Gradient Descent(35/49): loss=4.195813310562527e+68\n",
      "Gradient Descent(36/49): loss=4.949303328777356e+70\n",
      "Gradient Descent(37/49): loss=5.838106137511337e+72\n",
      "Gradient Descent(38/49): loss=6.886521396793795e+74\n",
      "Gradient Descent(39/49): loss=8.123212533562318e+76\n",
      "Gradient Descent(40/49): loss=9.581990393022843e+78\n",
      "Gradient Descent(41/49): loss=1.130273761921587e+81\n",
      "Gradient Descent(42/49): loss=1.3332499037137417e+83\n",
      "Gradient Descent(43/49): loss=1.5726767847204234e+85\n",
      "Gradient Descent(44/49): loss=1.8551002796319015e+87\n",
      "Gradient Descent(45/49): loss=2.1882417804635225e+89\n",
      "Gradient Descent(46/49): loss=2.5812092976000042e+91\n",
      "Gradient Descent(47/49): loss=3.0447464706599243e+93\n",
      "Gradient Descent(48/49): loss=3.59152629707931e+95\n",
      "Gradient Descent(49/49): loss=4.236497608885134e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4708646562152743\n",
      "Gradient Descent(2/49): loss=54.75187888895977\n",
      "Gradient Descent(3/49): loss=1576.7836665820162\n",
      "Gradient Descent(4/49): loss=58496.22541098923\n",
      "Gradient Descent(5/49): loss=3465820.9074310595\n",
      "Gradient Descent(6/49): loss=311380492.09659857\n",
      "Gradient Descent(7/49): loss=33436526323.734653\n",
      "Gradient Descent(8/49): loss=3776124123254.9985\n",
      "Gradient Descent(9/49): loss=431735271185859.75\n",
      "Gradient Descent(10/49): loss=4.950445133758112e+16\n",
      "Gradient Descent(11/49): loss=5.680192385337473e+18\n",
      "Gradient Descent(12/49): loss=6.518529558731817e+20\n",
      "Gradient Descent(13/49): loss=7.480867521078524e+22\n",
      "Gradient Descent(14/49): loss=8.585348677994497e+24\n",
      "Gradient Descent(15/49): loss=9.852915507246223e+26\n",
      "Gradient Descent(16/49): loss=1.1307634848036256e+29\n",
      "Gradient Descent(17/49): loss=1.2977135465734994e+31\n",
      "Gradient Descent(18/49): loss=1.4893127630807286e+33\n",
      "Gradient Descent(19/49): loss=1.709200404552857e+35\n",
      "Gradient Descent(20/49): loss=1.9615530727788238e+37\n",
      "Gradient Descent(21/49): loss=2.2511640228072445e+39\n",
      "Gradient Descent(22/49): loss=2.5835342047409502e+41\n",
      "Gradient Descent(23/49): loss=2.964976749607236e+43\n",
      "Gradient Descent(24/49): loss=3.402736882527982e+45\n",
      "Gradient Descent(25/49): loss=3.905129540479687e+47\n",
      "Gradient Descent(26/49): loss=4.4816973084914215e+49\n",
      "Gradient Descent(27/49): loss=5.143391674139765e+51\n",
      "Gradient Descent(28/49): loss=5.902781043130143e+53\n",
      "Gradient Descent(29/49): loss=6.774289467069157e+55\n",
      "Gradient Descent(30/49): loss=7.774470617888472e+57\n",
      "Gradient Descent(31/49): loss=8.922322212865317e+59\n",
      "Gradient Descent(32/49): loss=1.0239646862516831e+62\n",
      "Gradient Descent(33/49): loss=1.1751466195410605e+64\n",
      "Gradient Descent(34/49): loss=1.3486496125896366e+66\n",
      "Gradient Descent(35/49): loss=1.547769229212022e+68\n",
      "Gradient Descent(36/49): loss=1.7762876024526724e+70\n",
      "Gradient Descent(37/49): loss=2.038545273466538e+72\n",
      "Gradient Descent(38/49): loss=2.3395236369575626e+74\n",
      "Gradient Descent(39/49): loss=2.6849395591669476e+76\n",
      "Gradient Descent(40/49): loss=3.081353965610863e+78\n",
      "Gradient Descent(41/49): loss=3.5362964611135815e+80\n",
      "Gradient Descent(42/49): loss=4.058408349202885e+82\n",
      "Gradient Descent(43/49): loss=4.6576067674182125e+84\n",
      "Gradient Descent(44/49): loss=5.345273056162553e+86\n",
      "Gradient Descent(45/49): loss=6.134468939028834e+88\n",
      "Gradient Descent(46/49): loss=7.040184620788313e+90\n",
      "Gradient Descent(47/49): loss=8.079623515484222e+92\n",
      "Gradient Descent(48/49): loss=9.272529012833476e+94\n",
      "Gradient Descent(49/49): loss=1.0641559489630271e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.514827038764909\n",
      "Gradient Descent(2/49): loss=55.6098105482953\n",
      "Gradient Descent(3/49): loss=1581.3919699301503\n",
      "Gradient Descent(4/49): loss=56979.48956125648\n",
      "Gradient Descent(5/49): loss=3249080.3151564267\n",
      "Gradient Descent(6/49): loss=285720248.3172765\n",
      "Gradient Descent(7/49): loss=30484604550.03122\n",
      "Gradient Descent(8/49): loss=3438058467416.2666\n",
      "Gradient Descent(9/49): loss=393040746158183.2\n",
      "Gradient Descent(10/49): loss=4.507565521762193e+16\n",
      "Gradient Descent(11/49): loss=5.17328630185844e+18\n",
      "Gradient Descent(12/49): loss=5.938339085306352e+20\n",
      "Gradient Descent(13/49): loss=6.8168004458247665e+22\n",
      "Gradient Descent(14/49): loss=7.825284274011726e+24\n",
      "Gradient Descent(15/49): loss=8.982983048108251e+26\n",
      "Gradient Descent(16/49): loss=1.0311960673904643e+29\n",
      "Gradient Descent(17/49): loss=1.1837553775424528e+31\n",
      "Gradient Descent(18/49): loss=1.3588849633808734e+33\n",
      "Gradient Descent(19/49): loss=1.5599239419101334e+35\n",
      "Gradient Descent(20/49): loss=1.7907054485821194e+37\n",
      "Gradient Descent(21/49): loss=2.0556297127435044e+39\n",
      "Gradient Descent(22/49): loss=2.3597479527262225e+41\n",
      "Gradient Descent(23/49): loss=2.708858684991105e+43\n",
      "Gradient Descent(24/49): loss=3.109618282242127e+45\n",
      "Gradient Descent(25/49): loss=3.5696678881183846e+47\n",
      "Gradient Descent(26/49): loss=4.097779108205683e+49\n",
      "Gradient Descent(27/49): loss=4.704021255181555e+51\n",
      "Gradient Descent(28/49): loss=5.399953336891708e+53\n",
      "Gradient Descent(29/49): loss=6.198844447925849e+55\n",
      "Gradient Descent(30/49): loss=7.115926766822807e+57\n",
      "Gradient Descent(31/49): loss=8.168685982712118e+59\n",
      "Gradient Descent(32/49): loss=9.37719468885873e+61\n",
      "Gradient Descent(33/49): loss=1.0764495099806162e+64\n",
      "Gradient Descent(34/49): loss=1.2357038389255239e+66\n",
      "Gradient Descent(35/49): loss=1.4185189025380007e+68\n",
      "Gradient Descent(36/49): loss=1.6283803719565333e+70\n",
      "Gradient Descent(37/49): loss=1.8692896027180083e+72\n",
      "Gradient Descent(38/49): loss=2.145839927210177e+74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=2.4633042341401004e+76\n",
      "Gradient Descent(40/49): loss=2.8277355048666085e+78\n",
      "Gradient Descent(41/49): loss=3.246082223487323e+80\n",
      "Gradient Descent(42/49): loss=3.72632086116469e+82\n",
      "Gradient Descent(43/49): loss=4.277607960723074e+84\n",
      "Gradient Descent(44/49): loss=4.91045472126158e+86\n",
      "Gradient Descent(45/49): loss=5.636927411525571e+88\n",
      "Gradient Descent(46/49): loss=6.470877433250086e+90\n",
      "Gradient Descent(47/49): loss=7.428205421012128e+92\n",
      "Gradient Descent(48/49): loss=8.527164414090784e+94\n",
      "Gradient Descent(49/49): loss=9.788707880810057e+96\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4308239345097635\n",
      "Gradient Descent(2/49): loss=52.73291507278831\n",
      "Gradient Descent(3/49): loss=1455.3009665630764\n",
      "Gradient Descent(4/49): loss=48403.64281131929\n",
      "Gradient Descent(5/49): loss=2445621.7947510416\n",
      "Gradient Descent(6/49): loss=200427412.3026515\n",
      "Gradient Descent(7/49): loss=21108856782.097153\n",
      "Gradient Descent(8/49): loss=2399277052940.8374\n",
      "Gradient Descent(9/49): loss=277861619197529.88\n",
      "Gradient Descent(10/49): loss=3.2319126367003988e+16\n",
      "Gradient Descent(11/49): loss=3.7628808251872927e+18\n",
      "Gradient Descent(12/49): loss=4.3820679215525416e+20\n",
      "Gradient Descent(13/49): loss=5.103404338811285e+22\n",
      "Gradient Descent(14/49): loss=5.943549802401346e+24\n",
      "Gradient Descent(15/49): loss=6.922022115255047e+26\n",
      "Gradient Descent(16/49): loss=8.061582819364525e+28\n",
      "Gradient Descent(17/49): loss=9.388748743268328e+30\n",
      "Gradient Descent(18/49): loss=1.0934404282352626e+33\n",
      "Gradient Descent(19/49): loss=1.2734518850546369e+35\n",
      "Gradient Descent(20/49): loss=1.4830983602548435e+37\n",
      "Gradient Descent(21/49): loss=1.727258621081491e+39\n",
      "Gradient Descent(22/49): loss=2.0116146199737858e+41\n",
      "Gradient Descent(23/49): loss=2.342783720966348e+43\n",
      "Gradient Descent(24/49): loss=2.7284726948942416e+45\n",
      "Gradient Descent(25/49): loss=3.1776570667478695e+47\n",
      "Gradient Descent(26/49): loss=3.700789988754668e+49\n",
      "Gradient Descent(27/49): loss=4.310045499933208e+51\n",
      "Gradient Descent(28/49): loss=5.0196018331061975e+53\n",
      "Gradient Descent(29/49): loss=5.845971362324154e+55\n",
      "Gradient Descent(30/49): loss=6.808384869037697e+57\n",
      "Gradient Descent(31/49): loss=7.929239069435384e+59\n",
      "Gradient Descent(32/49): loss=9.234617817536497e+61\n",
      "Gradient Descent(33/49): loss=1.0754899113167179e+64\n",
      "Gradient Descent(34/49): loss=1.2525462040752153e+66\n",
      "Gradient Descent(35/49): loss=1.4587510090377746e+68\n",
      "Gradient Descent(36/49): loss=1.6989030020971118e+70\n",
      "Gradient Descent(37/49): loss=1.978590857968589e+72\n",
      "Gradient Descent(38/49): loss=2.304323306512719e+74\n",
      "Gradient Descent(39/49): loss=2.6836806000354867e+76\n",
      "Gradient Descent(40/49): loss=3.1254909164227736e+78\n",
      "Gradient Descent(41/49): loss=3.6400358032591763e+80\n",
      "Gradient Descent(42/49): loss=4.239289443903838e+82\n",
      "Gradient Descent(43/49): loss=4.93719731358232e+84\n",
      "Gradient Descent(44/49): loss=5.750000710212728e+86\n",
      "Gradient Descent(45/49): loss=6.696614712256318e+88\n",
      "Gradient Descent(46/49): loss=7.799068359201547e+90\n",
      "Gradient Descent(47/49): loss=9.083017298303585e+92\n",
      "Gradient Descent(48/49): loss=1.0578340827587166e+95\n",
      "Gradient Descent(49/49): loss=1.2319837229143852e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.477737525374541\n",
      "Gradient Descent(2/49): loss=54.998339707097514\n",
      "Gradient Descent(3/49): loss=1609.0457148721666\n",
      "Gradient Descent(4/49): loss=63147.13349610198\n",
      "Gradient Descent(5/49): loss=4115178.7797731315\n",
      "Gradient Descent(6/49): loss=399946436.6331616\n",
      "Gradient Descent(7/49): loss=45293184686.94893\n",
      "Gradient Descent(8/49): loss=5339561879915.516\n",
      "Gradient Descent(9/49): loss=635379808376135.8\n",
      "Gradient Descent(10/49): loss=7.576623190506314e+16\n",
      "Gradient Descent(11/49): loss=9.039049652457029e+18\n",
      "Gradient Descent(12/49): loss=1.0784888254994243e+21\n",
      "Gradient Descent(13/49): loss=1.2868228481645306e+23\n",
      "Gradient Descent(14/49): loss=1.5354092937784622e+25\n",
      "Gradient Descent(15/49): loss=1.8320194411455034e+27\n",
      "Gradient Descent(16/49): loss=2.1859292695162002e+29\n",
      "Gradient Descent(17/49): loss=2.608207612347754e+31\n",
      "Gradient Descent(18/49): loss=3.112061828075022e+33\n",
      "Gradient Descent(19/49): loss=3.713250741377276e+35\n",
      "Gradient Descent(20/49): loss=4.430577488211872e+37\n",
      "Gradient Descent(21/49): loss=5.2864776038932125e+39\n",
      "Gradient Descent(22/49): loss=6.307720727543554e+41\n",
      "Gradient Descent(23/49): loss=7.526247864489663e+43\n",
      "Gradient Descent(24/49): loss=8.980170391894895e+45\n",
      "Gradient Descent(25/49): loss=1.0714962052736367e+48\n",
      "Gradient Descent(26/49): loss=1.2784881219538147e+50\n",
      "Gradient Descent(27/49): loss=1.525466791139562e+52\n",
      "Gradient Descent(28/49): loss=1.8201568641196703e+54\n",
      "Gradient Descent(29/49): loss=2.1717752423355353e+56\n",
      "Gradient Descent(30/49): loss=2.5913193506554404e+58\n",
      "Gradient Descent(31/49): loss=3.0919110993547724e+60\n",
      "Gradient Descent(32/49): loss=3.6892072927620012e+62\n",
      "Gradient Descent(33/49): loss=4.401889320753312e+64\n",
      "Gradient Descent(34/49): loss=5.252247448978362e+66\n",
      "Gradient Descent(35/49): loss=6.266877982424974e+68\n",
      "Gradient Descent(36/49): loss=7.477515107221712e+70\n",
      "Gradient Descent(37/49): loss=8.922023427858887e+72\n",
      "Gradient Descent(38/49): loss=1.0645582243007961e+75\n",
      "Gradient Descent(39/49): loss=1.270209860005318e+77\n",
      "Gradient Descent(40/49): loss=1.5155893323865963e+79\n",
      "Gradient Descent(41/49): loss=1.80837127530599e+81\n",
      "Gradient Descent(42/49): loss=2.157712910397857e+83\n",
      "Gradient Descent(43/49): loss=2.5745404537626492e+85\n",
      "Gradient Descent(44/49): loss=3.071890850779593e+87\n",
      "Gradient Descent(45/49): loss=3.665319527340187e+89\n",
      "Gradient Descent(46/49): loss=4.373386910570771e+91\n",
      "Gradient Descent(47/49): loss=5.218238935755389e+93\n",
      "Gradient Descent(48/49): loss=6.226299695738488e+95\n",
      "Gradient Descent(49/49): loss=7.429097896519069e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.497645988945531\n",
      "Gradient Descent(2/49): loss=56.13196121186143\n",
      "Gradient Descent(3/49): loss=1636.5726349930176\n",
      "Gradient Descent(4/49): loss=61406.794983337306\n",
      "Gradient Descent(5/49): loss=3675975.884025209\n",
      "Gradient Descent(6/49): loss=333752936.6214051\n",
      "Gradient Descent(7/49): loss=36238920512.80755\n",
      "Gradient Descent(8/49): loss=4139383295295.513\n",
      "Gradient Descent(9/49): loss=478715614370546.7\n",
      "Gradient Descent(10/49): loss=5.552451224088851e+16\n",
      "Gradient Descent(11/49): loss=6.444460559376819e+18\n",
      "Gradient Descent(12/49): loss=7.480951876298451e+20\n",
      "Gradient Descent(13/49): loss=8.6844647256285e+22\n",
      "Gradient Descent(14/49): loss=1.0081680808839224e+25\n",
      "Gradient Descent(15/49): loss=1.1703713685736897e+27\n",
      "Gradient Descent(16/49): loss=1.358672025589663e+29\n",
      "Gradient Descent(17/49): loss=1.5772684801543904e+31\n",
      "Gradient Descent(18/49): loss=1.8310349173039977e+33\n",
      "Gradient Descent(19/49): loss=2.1256298022990765e+35\n",
      "Gradient Descent(20/49): loss=2.467622009689077e+37\n",
      "Gradient Descent(21/49): loss=2.864637284442499e+39\n",
      "Gradient Descent(22/49): loss=3.3255282777455446e+41\n",
      "Gradient Descent(23/49): loss=3.860571942678703e+43\n",
      "Gradient Descent(24/49): loss=4.481698689616859e+45\n",
      "Gradient Descent(25/49): loss=5.202758410608602e+47\n",
      "Gradient Descent(26/49): loss=6.039829304428077e+49\n",
      "Gradient Descent(27/49): loss=7.011576388449304e+51\n",
      "Gradient Descent(28/49): loss=8.139667691439193e+53\n",
      "Gradient Descent(29/49): loss=9.449257407536192e+55\n",
      "Gradient Descent(30/49): loss=1.0969546784788891e+58\n",
      "Gradient Descent(31/49): loss=1.2734435254954893e+60\n",
      "Gradient Descent(32/49): loss=1.4783276323457865e+62\n",
      "Gradient Descent(33/49): loss=1.7161755074350692e+64\n",
      "Gradient Descent(34/49): loss=1.9922906856895415e+66\n",
      "Gradient Descent(35/49): loss=2.3128299868453337e+68\n",
      "Gradient Descent(36/49): loss=2.6849408002926982e+70\n",
      "Gradient Descent(37/49): loss=3.1169204576551256e+72\n",
      "Gradient Descent(38/49): loss=3.618401246794674e+74\n",
      "Gradient Descent(39/49): loss=4.200565192688573e+76\n",
      "Gradient Descent(40/49): loss=4.876393394363731e+78\n",
      "Gradient Descent(41/49): loss=5.660955477606822e+80\n",
      "Gradient Descent(42/49): loss=6.571745617670276e+82\n",
      "Gradient Descent(43/49): loss=7.6290726246137625e+84\n",
      "Gradient Descent(44/49): loss=8.85651278940787e+86\n",
      "Gradient Descent(45/49): loss=1.0281435588367644e+89\n",
      "Gradient Descent(46/49): loss=1.1935613968081852e+91\n",
      "Gradient Descent(47/49): loss=1.385593281897811e+93\n",
      "Gradient Descent(48/49): loss=1.6085211435075296e+95\n",
      "Gradient Descent(49/49): loss=1.8673158299143043e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5421891091083326\n",
      "Gradient Descent(2/49): loss=57.0124439312377\n",
      "Gradient Descent(3/49): loss=1641.429250597491\n",
      "Gradient Descent(4/49): loss=59821.34229577067\n",
      "Gradient Descent(5/49): loss=3446500.1467309776\n",
      "Gradient Descent(6/49): loss=306264925.98726493\n",
      "Gradient Descent(7/49): loss=33040082389.515404\n",
      "Gradient Descent(8/49): loss=3768807885865.42\n",
      "Gradient Descent(9/49): loss=435810404867412.94\n",
      "Gradient Descent(10/49): loss=5.055709443517948e+16\n",
      "Gradient Descent(11/49): loss=5.869342535027716e+18\n",
      "Gradient Descent(12/49): loss=6.815089363367496e+20\n",
      "Gradient Descent(13/49): loss=7.91354273919685e+22\n",
      "Gradient Descent(14/49): loss=9.189129077011904e+24\n",
      "Gradient Descent(15/49): loss=1.0670350313704299e+27\n",
      "Gradient Descent(16/49): loss=1.2390339828291871e+29\n",
      "Gradient Descent(17/49): loss=1.4387581857366085e+31\n",
      "Gradient Descent(18/49): loss=1.6706766726205023e+33\n",
      "Gradient Descent(19/49): loss=1.939978927081343e+35\n",
      "Gradient Descent(20/49): loss=2.2526909631997764e+37\n",
      "Gradient Descent(21/49): loss=2.615810154689035e+39\n",
      "Gradient Descent(22/49): loss=3.0374618079760338e+41\n",
      "Gradient Descent(23/49): loss=3.527080976628316e+43\n",
      "Gradient Descent(24/49): loss=4.0956235838346366e+45\n",
      "Gradient Descent(25/49): loss=4.755811576663464e+47\n",
      "Gradient Descent(26/49): loss=5.522417597652967e+49\n",
      "Gradient Descent(27/49): loss=6.412595543632506e+51\n",
      "Gradient Descent(28/49): loss=7.446264408489074e+53\n",
      "Gradient Descent(29/49): loss=8.646553998900945e+55\n",
      "Gradient Descent(30/49): loss=1.0040322496563185e+58\n",
      "Gradient Descent(31/49): loss=1.165875744808923e+60\n",
      "Gradient Descent(32/49): loss=1.3538073630593347e+62\n",
      "Gradient Descent(33/49): loss=1.5720323408684314e+64\n",
      "Gradient Descent(34/49): loss=1.8254337715756117e+66\n",
      "Gradient Descent(35/49): loss=2.1196818715371061e+68\n",
      "Gradient Descent(36/49): loss=2.4613608592575318e+70\n",
      "Gradient Descent(37/49): loss=2.8581162866160338e+72\n",
      "Gradient Descent(38/49): loss=3.3188261189315435e+74\n",
      "Gradient Descent(39/49): loss=3.853799392026707e+76\n",
      "Gradient Descent(40/49): loss=4.47500689152303e+78\n",
      "Gradient Descent(41/49): loss=5.196349016145006e+80\n",
      "Gradient Descent(42/49): loss=6.03396681885381e+82\n",
      "Gradient Descent(43/49): loss=7.006603185795649e+84\n",
      "Gradient Descent(44/49): loss=8.136022234959285e+86\n",
      "Gradient Descent(45/49): loss=9.44749631917921e+88\n",
      "Gradient Descent(46/49): loss=1.0970371530867878e+91\n",
      "Gradient Descent(47/49): loss=1.2738724362448697e+93\n",
      "Gradient Descent(48/49): loss=1.4792124216198762e+95\n",
      "Gradient Descent(49/49): loss=1.7176518825735482e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4571059095008563\n",
      "Gradient Descent(2/49): loss=54.063937988896\n",
      "Gradient Descent(3/49): loss=1510.7146850971362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=50834.54213153307\n",
      "Gradient Descent(5/49): loss=2595263.097926971\n",
      "Gradient Descent(6/49): loss=214875264.21414313\n",
      "Gradient Descent(7/49): loss=22878709422.035168\n",
      "Gradient Descent(8/49): loss=2629988794315.0396\n",
      "Gradient Descent(9/49): loss=308079567688082.2\n",
      "Gradient Descent(10/49): loss=3.624679384978957e+16\n",
      "Gradient Descent(11/49): loss=4.2688408012800435e+18\n",
      "Gradient Descent(12/49): loss=5.0286231853472704e+20\n",
      "Gradient Descent(13/49): loss=5.9239407821096525e+22\n",
      "Gradient Descent(14/49): loss=6.978746759328557e+24\n",
      "Gradient Descent(15/49): loss=8.221391580759173e+26\n",
      "Gradient Descent(16/49): loss=9.68530927961472e+28\n",
      "Gradient Descent(17/49): loss=1.1409896723531263e+31\n",
      "Gradient Descent(18/49): loss=1.3441568420206273e+33\n",
      "Gradient Descent(19/49): loss=1.5835004231085849e+35\n",
      "Gradient Descent(20/49): loss=1.865462061938834e+37\n",
      "Gradient Descent(21/49): loss=2.197630423739523e+39\n",
      "Gradient Descent(22/49): loss=2.5889454298174487e+41\n",
      "Gradient Descent(23/49): loss=3.049938864286643e+43\n",
      "Gradient Descent(24/49): loss=3.593017824474788e+45\n",
      "Gradient Descent(25/49): loss=4.2327986433348344e+47\n",
      "Gradient Descent(26/49): loss=4.986500270880376e+49\n",
      "Gradient Descent(27/49): loss=5.8744077020163955e+51\n",
      "Gradient Descent(28/49): loss=6.920417923374154e+53\n",
      "Gradient Descent(29/49): loss=8.15268307266442e+55\n",
      "Gradient Descent(30/49): loss=9.604368120430064e+57\n",
      "Gradient Descent(31/49): loss=1.1314543466312913e+60\n",
      "Gradient Descent(32/49): loss=1.332923647301312e+62\n",
      "Gradient Descent(33/49): loss=1.5702670238749015e+64\n",
      "Gradient Descent(34/49): loss=1.8498722948318548e+66\n",
      "Gradient Descent(35/49): loss=2.179264707948886e+68\n",
      "Gradient Descent(36/49): loss=2.5673094735132583e+70\n",
      "Gradient Descent(37/49): loss=3.024450361055224e+72\n",
      "Gradient Descent(38/49): loss=3.5629907811502144e+74\n",
      "Gradient Descent(39/49): loss=4.197424917277179e+76\n",
      "Gradient Descent(40/49): loss=4.944827819759792e+78\n",
      "Gradient Descent(41/49): loss=5.825314960709668e+80\n",
      "Gradient Descent(42/49): loss=6.862583618354647e+82\n",
      "Gradient Descent(43/49): loss=8.084550661475712e+84\n",
      "Gradient Descent(44/49): loss=9.524103899172306e+86\n",
      "Gradient Descent(45/49): loss=1.1219987217653482e+89\n",
      "Gradient Descent(46/49): loss=1.3217843326472295e+91\n",
      "Gradient Descent(47/49): loss=1.5571442178497919e+93\n",
      "Gradient Descent(48/49): loss=1.834412812509968e+95\n",
      "Gradient Descent(49/49): loss=2.1610524755037625e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5044517305024954\n",
      "Gradient Descent(2/49): loss=56.376980186714015\n",
      "Gradient Descent(3/49): loss=1669.6303661282886\n",
      "Gradient Descent(4/49): loss=66257.47766311489\n",
      "Gradient Descent(5/49): loss=4362046.132248732\n",
      "Gradient Descent(6/49): loss=428432581.5171372\n",
      "Gradient Descent(7/49): loss=49060426039.517624\n",
      "Gradient Descent(8/49): loss=5849473449404.161\n",
      "Gradient Descent(9/49): loss=704019456139800.2\n",
      "Gradient Descent(10/49): loss=8.491301569155416e+16\n",
      "Gradient Descent(11/49): loss=1.0246381290887862e+19\n",
      "Gradient Descent(12/49): loss=1.2365537990676196e+21\n",
      "Gradient Descent(13/49): loss=1.4923334356226804e+23\n",
      "Gradient Descent(14/49): loss=1.8010303799200045e+25\n",
      "Gradient Descent(15/49): loss=2.173585488275062e+27\n",
      "Gradient Descent(16/49): loss=2.6232067979759578e+29\n",
      "Gradient Descent(17/49): loss=3.1658355988480276e+31\n",
      "Gradient Descent(18/49): loss=3.8207110398592135e+33\n",
      "Gradient Descent(19/49): loss=4.611052102344705e+35\n",
      "Gradient Descent(20/49): loss=5.564880799136126e+37\n",
      "Gradient Descent(21/49): loss=6.716015700078882e+39\n",
      "Gradient Descent(22/49): loss=8.10527098661637e+41\n",
      "Gradient Descent(23/49): loss=9.781903542334783e+43\n",
      "Gradient Descent(24/49): loss=1.1805359385231864e+46\n",
      "Gradient Descent(25/49): loss=1.4247381362061118e+48\n",
      "Gradient Descent(26/49): loss=1.7194552834194368e+50\n",
      "Gradient Descent(27/49): loss=2.0751367542893707e+52\n",
      "Gradient Descent(28/49): loss=2.5043934497901338e+54\n",
      "Gradient Descent(29/49): loss=3.022444924840433e+56\n",
      "Gradient Descent(30/49): loss=3.64765900679835e+58\n",
      "Gradient Descent(31/49): loss=4.402203037853268e+60\n",
      "Gradient Descent(32/49): loss=5.312829831507208e+62\n",
      "Gradient Descent(33/49): loss=6.41182620970577e+64\n",
      "Gradient Descent(34/49): loss=7.738157751573772e+66\n",
      "Gradient Descent(35/49): loss=9.338850341514308e+68\n",
      "Gradient Descent(36/49): loss=1.1270657500289582e+71\n",
      "Gradient Descent(37/49): loss=1.3602072615315142e+73\n",
      "Gradient Descent(38/49): loss=1.641575741500029e+75\n",
      "Gradient Descent(39/49): loss=1.9811472790162808e+77\n",
      "Gradient Descent(40/49): loss=2.3909615876554204e+79\n",
      "Gradient Descent(41/49): loss=2.885548880789047e+81\n",
      "Gradient Descent(42/49): loss=3.4824450490599345e+83\n",
      "Gradient Descent(43/49): loss=4.202813405956129e+85\n",
      "Gradient Descent(44/49): loss=5.072195045849448e+87\n",
      "Gradient Descent(45/49): loss=6.121414418893595e+89\n",
      "Gradient Descent(46/49): loss=7.38767223048768e+91\n",
      "Gradient Descent(47/49): loss=8.915864414712202e+93\n",
      "Gradient Descent(48/49): loss=1.0760173946737632e+96\n",
      "Gradient Descent(49/49): loss=1.2985991932874094e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.524599136972702\n",
      "Gradient Descent(2/49): loss=57.53816323996518\n",
      "Gradient Descent(3/49): loss=1698.2411146452419\n",
      "Gradient Descent(4/49): loss=64443.53422698782\n",
      "Gradient Descent(5/49): loss=3897550.251762817\n",
      "Gradient Descent(6/49): loss=357591145.30836236\n",
      "Gradient Descent(7/49): loss=39258164575.9095\n",
      "Gradient Descent(8/49): loss=4535210846820.6875\n",
      "Gradient Descent(9/49): loss=530495535542112.06\n",
      "Gradient Descent(10/49): loss=6.223589019122373e+16\n",
      "Gradient Descent(11/49): loss=7.306296882141054e+18\n",
      "Gradient Descent(12/49): loss=8.578727420295681e+20\n",
      "Gradient Descent(13/49): loss=1.007313138644092e+23\n",
      "Gradient Descent(14/49): loss=1.1827960505441656e+25\n",
      "Gradient Descent(15/49): loss=1.3888524255620407e+27\n",
      "Gradient Descent(16/49): loss=1.630806897051607e+29\n",
      "Gradient Descent(17/49): loss=1.9149128979013708e+31\n",
      "Gradient Descent(18/49): loss=2.248513607180406e+33\n",
      "Gradient Descent(19/49): loss=2.6402315618395823e+35\n",
      "Gradient Descent(20/49): loss=3.1001914719754544e+37\n",
      "Gradient Descent(21/49): loss=3.640281899837282e+39\n",
      "Gradient Descent(22/49): loss=4.274462539183499e+41\n",
      "Gradient Descent(23/49): loss=5.0191250298532965e+43\n",
      "Gradient Descent(24/49): loss=5.893516631499572e+45\n",
      "Gradient Descent(25/49): loss=6.920237706612442e+47\n",
      "Gradient Descent(26/49): loss=8.125825871105868e+49\n",
      "Gradient Descent(27/49): loss=9.541441910939437e+51\n",
      "Gradient Descent(28/49): loss=1.1203675193626157e+54\n",
      "Gradient Descent(29/49): loss=1.3155489392054899e+56\n",
      "Gradient Descent(30/49): loss=1.5447332964716208e+58\n",
      "Gradient Descent(31/49): loss=1.8138443094858573e+60\n",
      "Gradient Descent(32/49): loss=2.129837679144389e+62\n",
      "Gradient Descent(33/49): loss=2.500880872619685e+64\n",
      "Gradient Descent(34/49): loss=2.9365642275365756e+66\n",
      "Gradient Descent(35/49): loss=3.448148833020711e+68\n",
      "Gradient Descent(36/49): loss=4.0488575945899495e+70\n",
      "Gradient Descent(37/49): loss=4.7542170060296136e+72\n",
      "Gradient Descent(38/49): loss=5.582458462017113e+74\n",
      "Gradient Descent(39/49): loss=6.554989484203763e+76\n",
      "Gradient Descent(40/49): loss=7.69694704051534e+78\n",
      "Gradient Descent(41/49): loss=9.037847259292002e+80\n",
      "Gradient Descent(42/49): loss=1.0612348331400521e+83\n",
      "Gradient Descent(43/49): loss=1.2461146319018391e+85\n",
      "Gradient Descent(44/49): loss=1.463202702502078e+87\n",
      "Gradient Descent(45/49): loss=1.718110111059186e+89\n",
      "Gradient Descent(46/49): loss=2.017425438509745e+91\n",
      "Gradient Descent(47/49): loss=2.3688850753792505e+93\n",
      "Gradient Descent(48/49): loss=2.781573183938611e+95\n",
      "Gradient Descent(49/49): loss=3.266156496160038e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5697266529335114\n",
      "Gradient Descent(2/49): loss=58.44163734654442\n",
      "Gradient Descent(3/49): loss=1703.3560328528913\n",
      "Gradient Descent(4/49): loss=62786.6712273098\n",
      "Gradient Descent(5/49): loss=3654670.2611042797\n",
      "Gradient Descent(6/49): loss=328156738.9212573\n",
      "Gradient Descent(7/49): loss=35793345214.48483\n",
      "Gradient Descent(8/49): loss=4129212177384.578\n",
      "Gradient Descent(9/49): loss=482949472290564.44\n",
      "Gradient Descent(10/49): loss=5.666799205248419e+16\n",
      "Gradient Descent(11/49): loss=6.654257835982022e+18\n",
      "Gradient Descent(12/49): loss=7.815142671448668e+20\n",
      "Gradient Descent(13/49): loss=9.178922086456004e+22\n",
      "Gradient Descent(14/49): loss=1.0780788011738348e+25\n",
      "Gradient Descent(15/49): loss=1.2662232026303868e+27\n",
      "Gradient Descent(16/49): loss=1.4872029755924501e+29\n",
      "Gradient Descent(17/49): loss=1.7467480790994774e+31\n",
      "Gradient Descent(18/49): loss=2.0515887769193234e+33\n",
      "Gradient Descent(19/49): loss=2.409629978190975e+35\n",
      "Gradient Descent(20/49): loss=2.8301561723667293e+37\n",
      "Gradient Descent(21/49): loss=3.3240721750439487e+39\n",
      "Gradient Descent(22/49): loss=3.904185900990287e+41\n",
      "Gradient Descent(23/49): loss=4.585540489824919e+43\n",
      "Gradient Descent(24/49): loss=5.385804394861656e+45\n",
      "Gradient Descent(25/49): loss=6.325729550115926e+47\n",
      "Gradient Descent(26/49): loss=7.429689496223676e+49\n",
      "Gradient Descent(27/49): loss=8.726311419572054e+51\n",
      "Gradient Descent(28/49): loss=1.0249218494266532e+54\n",
      "Gradient Descent(29/49): loss=1.203790177687307e+56\n",
      "Gradient Descent(30/49): loss=1.413874426335153e+58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=1.660622366337136e+60\n",
      "Gradient Descent(32/49): loss=1.9504325081592994e+62\n",
      "Gradient Descent(33/49): loss=2.2908200238658822e+64\n",
      "Gradient Descent(34/49): loss=2.690611625776049e+66\n",
      "Gradient Descent(35/49): loss=3.1601744551473596e+68\n",
      "Gradient Descent(36/49): loss=3.711684916281968e+70\n",
      "Gradient Descent(37/49): loss=4.359444427289681e+72\n",
      "Gradient Descent(38/49): loss=5.120250275355935e+74\n",
      "Gradient Descent(39/49): loss=6.01383119329774e+76\n",
      "Gradient Descent(40/49): loss=7.063358952500777e+78\n",
      "Gradient Descent(41/49): loss=8.296049238541167e+80\n",
      "Gradient Descent(42/49): loss=9.743867391014143e+82\n",
      "Gradient Descent(43/49): loss=1.144435730836691e+85\n",
      "Gradient Descent(44/49): loss=1.3441615012367126e+87\n",
      "Gradient Descent(45/49): loss=1.5787432118062476e+89\n",
      "Gradient Descent(46/49): loss=1.8542638860963316e+91\n",
      "Gradient Descent(47/49): loss=2.177868150797791e+93\n",
      "Gradient Descent(48/49): loss=2.5579475056512812e+95\n",
      "Gradient Descent(49/49): loss=3.004357926475404e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.4835566776202915\n",
      "Gradient Descent(2/49): loss=55.420192494835526\n",
      "Gradient Descent(3/49): loss=1567.877243960938\n",
      "Gradient Descent(4/49): loss=53371.76723867858\n",
      "Gradient Descent(5/49): loss=2753111.4686253187\n",
      "Gradient Descent(6/49): loss=230273060.9321218\n",
      "Gradient Descent(7/49): loss=24785578178.32789\n",
      "Gradient Descent(8/49): loss=2881377141881.749\n",
      "Gradient Descent(9/49): loss=341382787749732.6\n",
      "Gradient Descent(10/49): loss=4.062520717000412e+16\n",
      "Gradient Descent(11/49): loss=4.839350329987809e+18\n",
      "Gradient Descent(12/49): loss=5.766048512080652e+20\n",
      "Gradient Descent(13/49): loss=6.870561780394054e+22\n",
      "Gradient Descent(14/49): loss=8.186747271258211e+24\n",
      "Gradient Descent(15/49): loss=9.755099368700414e+26\n",
      "Gradient Descent(16/49): loss=1.1623911109470023e+29\n",
      "Gradient Descent(17/49): loss=1.385073830740163e+31\n",
      "Gradient Descent(18/49): loss=1.6504165939553495e+33\n",
      "Gradient Descent(19/49): loss=1.9665918834530438e+35\n",
      "Gradient Descent(20/49): loss=2.3433378315926315e+37\n",
      "Gradient Descent(21/49): loss=2.792258140216922e+39\n",
      "Gradient Descent(22/49): loss=3.327179469032131e+41\n",
      "Gradient Descent(23/49): loss=3.964577293167833e+43\n",
      "Gradient Descent(24/49): loss=4.724083344426319e+45\n",
      "Gradient Descent(25/49): loss=5.6290902648229064e+47\n",
      "Gradient Descent(26/49): loss=6.707472095494915e+49\n",
      "Gradient Descent(27/49): loss=7.992442791865736e+51\n",
      "Gradient Descent(28/49): loss=9.523579207157907e+53\n",
      "Gradient Descent(29/49): loss=1.1348040051949995e+56\n",
      "Gradient Descent(30/49): loss=1.3522018373498626e+58\n",
      "Gradient Descent(31/49): loss=1.6112472290914963e+60\n",
      "Gradient Descent(32/49): loss=1.9199187292505676e+62\n",
      "Gradient Descent(33/49): loss=2.2877233613648923e+64\n",
      "Gradient Descent(34/49): loss=2.725989438197592e+66\n",
      "Gradient Descent(35/49): loss=3.2482154716168586e+68\n",
      "Gradient Descent(36/49): loss=3.870485924196084e+70\n",
      "Gradient Descent(37/49): loss=4.611966607604214e+72\n",
      "Gradient Descent(38/49): loss=5.495494985961953e+74\n",
      "Gradient Descent(39/49): loss=6.548283565396821e+76\n",
      "Gradient Descent(40/49): loss=7.802758034059002e+78\n",
      "Gradient Descent(41/49): loss=9.29755596715407e+80\n",
      "Gradient Descent(42/49): loss=1.1078716856915462e+83\n",
      "Gradient Descent(43/49): loss=1.3201100120215103e+85\n",
      "Gradient Descent(44/49): loss=1.5730074758175967e+87\n",
      "Gradient Descent(45/49): loss=1.8743532708982126e+89\n",
      "Gradient Descent(46/49): loss=2.233428790477212e+91\n",
      "Gradient Descent(47/49): loss=2.661293492310596e+93\n",
      "Gradient Descent(48/49): loss=3.1711255278935213e+95\n",
      "Gradient Descent(49/49): loss=3.778627627021718e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.531336347697099\n",
      "Gradient Descent(2/49): loss=57.781570503202154\n",
      "Gradient Descent(3/49): loss=1732.1065245076513\n",
      "Gradient Descent(4/49): loss=69501.3233278466\n",
      "Gradient Descent(5/49): loss=4622184.031318165\n",
      "Gradient Descent(6/49): loss=458768703.2476294\n",
      "Gradient Descent(7/49): loss=53116960011.209946\n",
      "Gradient Descent(8/49): loss=6404769453405.932\n",
      "Gradient Descent(9/49): loss=779620913136207.2\n",
      "Gradient Descent(10/49): loss=9.510259510083242e+16\n",
      "Gradient Descent(11/49): loss=1.1606726051146605e+19\n",
      "Gradient Descent(12/49): loss=1.4166866964956434e+21\n",
      "Gradient Descent(13/49): loss=1.7292124803268603e+23\n",
      "Gradient Descent(14/49): loss=2.1106938793110423e+25\n",
      "Gradient Descent(15/49): loss=2.5763369671226874e+27\n",
      "Gradient Descent(16/49): loss=3.144707070106656e+29\n",
      "Gradient Descent(17/49): loss=3.838466505978191e+31\n",
      "Gradient Descent(18/49): loss=4.685277512068437e+33\n",
      "Gradient Descent(19/49): loss=5.718905036080933e+35\n",
      "Gradient Descent(20/49): loss=6.980562997548943e+37\n",
      "Gradient Descent(21/49): loss=8.520557600232094e+39\n",
      "Gradient Descent(22/49): loss=1.040029319223177e+42\n",
      "Gradient Descent(23/49): loss=1.2694720646248228e+44\n",
      "Gradient Descent(24/49): loss=1.5495325882440302e+46\n",
      "Gradient Descent(25/49): loss=1.8913777694987913e+48\n",
      "Gradient Descent(26/49): loss=2.3086380332333283e+50\n",
      "Gradient Descent(27/49): loss=2.8179508369202326e+52\n",
      "Gradient Descent(28/49): loss=3.43962405755659e+54\n",
      "Gradient Descent(29/49): loss=4.198445729540196e+56\n",
      "Gradient Descent(30/49): loss=5.124672420280696e+58\n",
      "Gradient Descent(31/49): loss=6.2552356531381966e+60\n",
      "Gradient Descent(32/49): loss=7.635214481503996e+62\n",
      "Gradient Descent(33/49): loss=9.319632930107648e+64\n",
      "Gradient Descent(34/49): loss=1.1375653973093757e+67\n",
      "Gradient Descent(35/49): loss=1.3885257529565533e+69\n",
      "Gradient Descent(36/49): loss=1.6948509256556382e+71\n",
      "Gradient Descent(37/49): loss=2.0687550476318363e+73\n",
      "Gradient Descent(38/49): loss=2.5251468328677603e+75\n",
      "Gradient Descent(39/49): loss=3.0822240336482556e+77\n",
      "Gradient Descent(40/49): loss=3.762199041237416e+79\n",
      "Gradient Descent(41/49): loss=4.5921845626304835e+81\n",
      "Gradient Descent(42/49): loss=5.605274688051996e+83\n",
      "Gradient Descent(43/49): loss=6.84186445470708e+85\n",
      "Gradient Descent(44/49): loss=8.3512605218734e+87\n",
      "Gradient Descent(45/49): loss=1.019364717993193e+90\n",
      "Gradient Descent(46/49): loss=1.244248608419933e+92\n",
      "Gradient Descent(47/49): loss=1.5187445398373066e+94\n",
      "Gradient Descent(48/49): loss=1.8537975141597099e+96\n",
      "Gradient Descent(49/49): loss=2.262767129930217e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5517241002967905\n",
      "Gradient Descent(2/49): loss=58.970812719788135\n",
      "Gradient Descent(3/49): loss=1761.8361179016113\n",
      "Gradient Descent(4/49): loss=67611.11063034963\n",
      "Gradient Descent(5/49): loss=4131093.233458969\n",
      "Gradient Descent(6/49): loss=382981968.41055924\n",
      "Gradient Descent(7/49): loss=42509659703.68322\n",
      "Gradient Descent(8/49): loss=4966316139791.404\n",
      "Gradient Descent(9/49): loss=587533797690575.4\n",
      "Gradient Descent(10/49): loss=6.97133477459606e+16\n",
      "Gradient Descent(11/49): loss=8.277493137819283e+18\n",
      "Gradient Descent(12/49): loss=9.829955096040129e+20\n",
      "Gradient Descent(13/49): loss=1.1674021454962942e+23\n",
      "Gradient Descent(14/49): loss=1.3864149142119042e+25\n",
      "Gradient Descent(15/49): loss=1.6465193389101895e+27\n",
      "Gradient Descent(16/49): loss=1.95542271208545e+29\n",
      "Gradient Descent(17/49): loss=2.3222796793851594e+31\n",
      "Gradient Descent(18/49): loss=2.757962773801952e+33\n",
      "Gradient Descent(19/49): loss=3.275384431251581e+35\n",
      "Gradient Descent(20/49): loss=3.889879620285319e+37\n",
      "Gradient Descent(21/49): loss=4.619660312483534e+39\n",
      "Gradient Descent(22/49): loss=5.48635523141659e+41\n",
      "Gradient Descent(23/49): loss=6.515650868198749e+43\n",
      "Gradient Descent(24/49): loss=7.738052759203013e+45\n",
      "Gradient Descent(25/49): loss=9.189789587485894e+47\n",
      "Gradient Descent(26/49): loss=1.0913886902857218e+50\n",
      "Gradient Descent(27/49): loss=1.296144228270093e+52\n",
      "Gradient Descent(28/49): loss=1.5393139725848174e+54\n",
      "Gradient Descent(29/49): loss=1.8281048162034977e+56\n",
      "Gradient Descent(30/49): loss=2.171075737989e+58\n",
      "Gradient Descent(31/49): loss=2.5783914676584748e+60\n",
      "Gradient Descent(32/49): loss=3.062123740856667e+62\n",
      "Gradient Descent(33/49): loss=3.636609072722967e+64\n",
      "Gradient Descent(34/49): loss=4.3188736533916196e+66\n",
      "Gradient Descent(35/49): loss=5.129137958178695e+68\n",
      "Gradient Descent(36/49): loss=6.09141602773434e+70\n",
      "Gradient Descent(37/49): loss=7.234227179203316e+72\n",
      "Gradient Descent(38/49): loss=8.59144124158398e+74\n",
      "Gradient Descent(39/49): loss=1.0203282365776941e+77\n",
      "Gradient Descent(40/49): loss=1.2117521159532887e+79\n",
      "Gradient Descent(41/49): loss=1.43908904789528e+81\n",
      "Gradient Descent(42/49): loss=1.7090766836770653e+83\n",
      "Gradient Descent(43/49): loss=2.0297167259806685e+85\n",
      "Gradient Descent(44/49): loss=2.4105120777039215e+87\n",
      "Gradient Descent(45/49): loss=2.862748482278451e+89\n",
      "Gradient Descent(46/49): loss=3.399829002555292e+91\n",
      "Gradient Descent(47/49): loss=4.0376712513061e+93\n",
      "Gradient Descent(48/49): loss=4.7951791461779607e+95\n",
      "Gradient Descent(49/49): loss=5.69480317064001e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.597439670240448\n",
      "Gradient Descent(2/49): loss=59.89772422700612\n",
      "Gradient Descent(3/49): loss=1767.2196306081207\n",
      "Gradient Descent(4/49): loss=65880.05446212835\n",
      "Gradient Descent(5/49): loss=3874108.8350037416\n",
      "Gradient Descent(6/49): loss=351475579.51279986\n",
      "Gradient Descent(7/49): loss=38758443403.969215\n",
      "Gradient Descent(8/49): loss=4521738440193.66\n",
      "Gradient Descent(9/49): loss=534875594604487.4\n",
      "Gradient Descent(10/49): loss=6.347642031891918e+16\n",
      "Gradient Descent(11/49): loss=7.538771317434336e+18\n",
      "Gradient Descent(12/49): loss=8.954986417724076e+20\n",
      "Gradient Descent(13/49): loss=1.0637681134817324e+23\n",
      "Gradient Descent(14/49): loss=1.2636683346876988e+25\n",
      "Gradient Descent(15/49): loss=1.50113651040474e+27\n",
      "Gradient Descent(16/49): loss=1.7832305392711307e+29\n",
      "Gradient Descent(17/49): loss=2.1183360128363e+31\n",
      "Gradient Descent(18/49): loss=2.516414724185604e+33\n",
      "Gradient Descent(19/49): loss=2.989300595259852e+35\n",
      "Gradient Descent(20/49): loss=3.5510514129625015e+37\n",
      "Gradient Descent(21/49): loss=4.218366718216376e+39\n",
      "Gradient Descent(22/49): loss=5.011084239952481e+41\n",
      "Gradient Descent(23/49): loss=5.952769623352379e+43\n",
      "Gradient Descent(24/49): loss=7.071416981246471e+45\n",
      "Gradient Descent(25/49): loss=8.400281093786038e+47\n",
      "Gradient Descent(26/49): loss=9.978865995567073e+49\n",
      "Gradient Descent(27/49): loss=1.1854099338550698e+52\n",
      "Gradient Descent(28/49): loss=1.408172744184086e+54\n",
      "Gradient Descent(29/49): loss=1.672797249989473e+56\n",
      "Gradient Descent(30/49): loss=1.9871501214105044e+58\n",
      "Gradient Descent(31/49): loss=2.360576337058528e+60\n",
      "Gradient Descent(32/49): loss=2.8041769884628953e+62\n",
      "Gradient Descent(33/49): loss=3.3311392896632464e+64\n",
      "Gradient Descent(34/49): loss=3.957128602364196e+66\n",
      "Gradient Descent(35/49): loss=4.700754130648127e+68\n",
      "Gradient Descent(36/49): loss=5.584122129264062e+70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=6.633493071086763e+72\n",
      "Gradient Descent(38/49): loss=7.880062309803984e+74\n",
      "Gradient Descent(39/49): loss=9.360887445114962e+76\n",
      "Gradient Descent(40/49): loss=1.1119989958847171e+79\n",
      "Gradient Descent(41/49): loss=1.3209663871067189e+81\n",
      "Gradient Descent(42/49): loss=1.5692030319483476e+83\n",
      "Gradient Descent(43/49): loss=1.8640884276163306e+85\n",
      "Gradient Descent(44/49): loss=2.214388830015749e+87\n",
      "Gradient Descent(45/49): loss=2.630517854117425e+89\n",
      "Gradient Descent(46/49): loss=3.1248460464738807e+91\n",
      "Gradient Descent(47/49): loss=3.712068632751928e+93\n",
      "Gradient Descent(48/49): loss=4.409642372560979e+95\n",
      "Gradient Descent(49/49): loss=5.238304508252058e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5101762388680724\n",
      "Gradient Descent(2/49): loss=56.80199568294005\n",
      "Gradient Descent(3/49): loss=1626.83254865195\n",
      "Gradient Descent(4/49): loss=56019.285582174656\n",
      "Gradient Descent(5/49): loss=2919565.3969155825\n",
      "Gradient Descent(6/49): loss=246677305.8195285\n",
      "Gradient Descent(7/49): loss=26839199810.623543\n",
      "Gradient Descent(8/49): loss=3155161555019.273\n",
      "Gradient Descent(9/49): loss=378066009303244.4\n",
      "Gradient Descent(10/49): loss=4.550308118348702e+16\n",
      "Gradient Descent(11/49): loss=5.482205580941528e+18\n",
      "Gradient Descent(12/49): loss=6.60648669232403e+20\n",
      "Gradient Descent(13/49): loss=7.961754584845958e+22\n",
      "Gradient Descent(14/49): loss=9.595160593442843e+24\n",
      "Gradient Descent(15/49): loss=1.1563702333321136e+27\n",
      "Gradient Descent(16/49): loss=1.3936118549087663e+29\n",
      "Gradient Descent(17/49): loss=1.6795263523193153e+31\n",
      "Gradient Descent(18/49): loss=2.0240993575033548e+33\n",
      "Gradient Descent(19/49): loss=2.4393652614351407e+35\n",
      "Gradient Descent(20/49): loss=2.9398274678207114e+37\n",
      "Gradient Descent(21/49): loss=3.5429649181812833e+39\n",
      "Gradient Descent(22/49): loss=4.269842550275629e+41\n",
      "Gradient Descent(23/49): loss=5.145847002591738e+43\n",
      "Gradient Descent(24/49): loss=6.201573257658905e+45\n",
      "Gradient Descent(25/49): loss=7.473893190135709e+47\n",
      "Gradient Descent(26/49): loss=9.007243339193706e+49\n",
      "Gradient Descent(27/49): loss=1.0855176881378733e+52\n",
      "Gradient Descent(28/49): loss=1.3082234007521365e+54\n",
      "Gradient Descent(29/49): loss=1.5766196027734315e+56\n",
      "Gradient Descent(30/49): loss=1.9000801930468025e+58\n",
      "Gradient Descent(31/49): loss=2.2899022273083093e+60\n",
      "Gradient Descent(32/49): loss=2.7597004746538747e+62\n",
      "Gradient Descent(33/49): loss=3.325882921541537e+64\n",
      "Gradient Descent(34/49): loss=4.008223830591361e+66\n",
      "Gradient Descent(35/49): loss=4.830554368604663e+68\n",
      "Gradient Descent(36/49): loss=5.821594924403882e+70\n",
      "Gradient Descent(37/49): loss=7.015958185692756e+72\n",
      "Gradient Descent(38/49): loss=8.455358007999609e+74\n",
      "Gradient Descent(39/49): loss=1.0190066296180971e+77\n",
      "Gradient Descent(40/49): loss=1.2280668780946521e+79\n",
      "Gradient Descent(41/49): loss=1.4800181011956106e+81\n",
      "Gradient Descent(42/49): loss=1.7836598469825957e+83\n",
      "Gradient Descent(43/49): loss=2.1495969861232375e+85\n",
      "Gradient Descent(44/49): loss=2.5906100933801937e+87\n",
      "Gradient Descent(45/49): loss=3.1221018168745276e+89\n",
      "Gradient Descent(46/49): loss=3.7626348248388585e+91\n",
      "Gradient Descent(47/49): loss=4.534580117974068e+93\n",
      "Gradient Descent(48/49): loss=5.464898350109332e+95\n",
      "Gradient Descent(49/49): loss=6.5860814452586155e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5583913769583506\n",
      "Gradient Descent(2/49): loss=59.21243451261562\n",
      "Gradient Descent(3/49): loss=1796.5211870346716\n",
      "Gradient Descent(4/49): loss=72883.56167742032\n",
      "Gradient Descent(5/49): loss=4896223.873588527\n",
      "Gradient Descent(6/49): loss=491063450.790432\n",
      "Gradient Descent(7/49): loss=57483176570.479996\n",
      "Gradient Descent(8/49): loss=7009198096878.8955\n",
      "Gradient Descent(9/49): loss=862844849318660.9\n",
      "Gradient Descent(10/49): loss=1.0644693417229182e+17\n",
      "Gradient Descent(11/49): loss=1.3138443331196357e+19\n",
      "Gradient Descent(12/49): loss=1.621816972665325e+21\n",
      "Gradient Descent(13/49): loss=2.0020289254065005e+23\n",
      "Gradient Descent(14/49): loss=2.4713896662041107e+25\n",
      "Gradient Descent(15/49): loss=3.050792265351063e+27\n",
      "Gradient Descent(16/49): loss=3.766033389501804e+29\n",
      "Gradient Descent(17/49): loss=4.64895906708761e+31\n",
      "Gradient Descent(18/49): loss=5.738881860675907e+33\n",
      "Gradient Descent(19/49): loss=7.084331059367416e+35\n",
      "Gradient Descent(20/49): loss=8.74521340775991e+37\n",
      "Gradient Descent(21/49): loss=1.0795480465023544e+40\n",
      "Gradient Descent(22/49): loss=1.3326421327964132e+42\n",
      "Gradient Descent(23/49): loss=1.645072731938057e+44\n",
      "Gradient Descent(24/49): loss=2.0307509621447447e+46\n",
      "Gradient Descent(25/49): loss=2.5068493265917315e+48\n",
      "Gradient Descent(26/49): loss=3.0945663271270445e+50\n",
      "Gradient Descent(27/49): loss=3.820070337457627e+52\n",
      "Gradient Descent(28/49): loss=4.7156647622000026e+54\n",
      "Gradient Descent(29/49): loss=5.821226361045298e+56\n",
      "Gradient Descent(30/49): loss=7.185980780092216e+58\n",
      "Gradient Descent(31/49): loss=8.870694346712e+60\n",
      "Gradient Descent(32/49): loss=1.095037971863043e+63\n",
      "Gradient Descent(33/49): loss=1.3517635857517561e+65\n",
      "Gradient Descent(34/49): loss=1.668677104096806e+67\n",
      "Gradient Descent(35/49): loss=2.0598892491901953e+69\n",
      "Gradient Descent(36/49): loss=2.542818924351449e+71\n",
      "Gradient Descent(37/49): loss=3.1389687987263476e+73\n",
      "Gradient Descent(38/49): loss=3.8748827236648935e+75\n",
      "Gradient Descent(39/49): loss=4.783327610089264e+77\n",
      "Gradient Descent(40/49): loss=5.9047523904935214e+79\n",
      "Gradient Descent(41/49): loss=7.2890890265382845e+81\n",
      "Gradient Descent(42/49): loss=8.997975752944383e+83\n",
      "Gradient Descent(43/49): loss=1.1107501548657201e+86\n",
      "Gradient Descent(44/49): loss=1.3711594034141805e+88\n",
      "Gradient Descent(45/49): loss=1.6926201642514228e+90\n",
      "Gradient Descent(46/49): loss=2.089445627763484e+92\n",
      "Gradient Descent(47/49): loss=2.5793046328918397e+94\n",
      "Gradient Descent(48/49): loss=3.1840083804327484e+96\n",
      "Gradient Descent(49/49): loss=3.930481586930492e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5790208789177935\n",
      "Gradient Descent(2/49): loss=60.43023944360006\n",
      "Gradient Descent(3/49): loss=1827.405534317149\n",
      "Gradient Descent(4/49): loss=70914.33493674686\n",
      "Gradient Descent(5/49): loss=4377177.068586739\n",
      "Gradient Descent(6/49): loss=410016864.0672727\n",
      "Gradient Descent(7/49): loss=46009804118.64692\n",
      "Gradient Descent(8/49): loss=5435616613266.389\n",
      "Gradient Descent(9/49): loss=650330032222398.5\n",
      "Gradient Descent(10/49): loss=7.803923838092293e+16\n",
      "Gradient Descent(11/49): loss=9.371186934982654e+18\n",
      "Gradient Descent(12/49): loss=1.1255030137231667e+21\n",
      "Gradient Descent(13/49): loss=1.3518084102808958e+23\n",
      "Gradient Descent(14/49): loss=1.6236314353298561e+25\n",
      "Gradient Descent(15/49): loss=1.9501169148805014e+27\n",
      "Gradient Descent(16/49): loss=2.3422543543611223e+29\n",
      "Gradient Descent(17/49): loss=2.8132446980193366e+31\n",
      "Gradient Descent(18/49): loss=3.3789438458920755e+33\n",
      "Gradient Descent(19/49): loss=4.0583961966416775e+35\n",
      "Gradient Descent(20/49): loss=4.874475712911611e+37\n",
      "Gradient Descent(21/49): loss=5.854655960700965e+39\n",
      "Gradient Descent(22/49): loss=7.031935009952867e+41\n",
      "Gradient Descent(23/49): loss=8.445946323233733e+43\n",
      "Gradient Descent(24/49): loss=1.014429302806087e+46\n",
      "Gradient Descent(25/49): loss=1.2184150490771989e+48\n",
      "Gradient Descent(26/49): loss=1.463419114285861e+50\n",
      "Gradient Descent(27/49): loss=1.7576896359572432e+52\n",
      "Gradient Descent(28/49): loss=2.111133322089526e+54\n",
      "Gradient Descent(29/49): loss=2.5356489635381015e+56\n",
      "Gradient Descent(30/49): loss=3.0455280104850464e+58\n",
      "Gradient Descent(31/49): loss=3.65793569852298e+60\n",
      "Gradient Descent(32/49): loss=4.393488921613334e+62\n",
      "Gradient Descent(33/49): loss=5.276950306188518e+64\n",
      "Gradient Descent(34/49): loss=6.3380618526193e+66\n",
      "Gradient Descent(35/49): loss=7.612546208842972e+68\n",
      "Gradient Descent(36/49): loss=9.143309284339063e+70\n",
      "Gradient Descent(37/49): loss=1.0981884690823692e+73\n",
      "Gradient Descent(38/49): loss=1.3190168637204357e+75\n",
      "Gradient Descent(39/49): loss=1.584250368456923e+77\n",
      "Gradient Descent(40/49): loss=1.9028181511467463e+79\n",
      "Gradient Descent(41/49): loss=2.285444894584583e+81\n",
      "Gradient Descent(42/49): loss=2.7450118462632604e+83\n",
      "Gradient Descent(43/49): loss=3.2969904695494485e+85\n",
      "Gradient Descent(44/49): loss=3.9599632952757553e+87\n",
      "Gradient Descent(45/49): loss=4.756249508381058e+89\n",
      "Gradient Descent(46/49): loss=5.712656330164158e+91\n",
      "Gradient Descent(47/49): loss=6.861381491668886e+93\n",
      "Gradient Descent(48/49): loss=8.241097180243325e+95\n",
      "Gradient Descent(49/49): loss=9.898251950671737e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6253281610291435\n",
      "Gradient Descent(2/49): loss=61.3810400875825\n",
      "Gradient Descent(3/49): loss=1833.0682415049544\n",
      "Gradient Descent(4/49): loss=69106.21092328956\n",
      "Gradient Descent(5/49): loss=4105355.80960915\n",
      "Gradient Descent(6/49): loss=376305582.9343144\n",
      "Gradient Descent(7/49): loss=41950337148.34936\n",
      "Gradient Descent(8/49): loss=4949043253407.688\n",
      "Gradient Descent(9/49): loss=592043623645321.0\n",
      "Gradient Descent(10/49): loss=7.1057363052291096e+16\n",
      "Gradient Descent(11/49): loss=8.534847759166168e+18\n",
      "Gradient Descent(12/49): loss=1.0253199853227981e+21\n",
      "Gradient Descent(13/49): loss=1.231802069084571e+23\n",
      "Gradient Descent(14/49): loss=1.4798802755664321e+25\n",
      "Gradient Descent(15/49): loss=1.7779240165813424e+27\n",
      "Gradient Descent(16/49): loss=2.1359940303139016e+29\n",
      "Gradient Descent(17/49): loss=2.5661788690016764e+31\n",
      "Gradient Descent(18/49): loss=3.083002141698386e+33\n",
      "Gradient Descent(19/49): loss=3.703912607820917e+35\n",
      "Gradient Descent(20/49): loss=4.449873206797645e+37\n",
      "Gradient Descent(21/49): loss=5.346068781866325e+39\n",
      "Gradient Descent(22/49): loss=6.422756356087246e+41\n",
      "Gradient Descent(23/49): loss=7.716286657282383e+43\n",
      "Gradient Descent(24/49): loss=9.270331377459488e+45\n",
      "Gradient Descent(25/49): loss=1.1137357600224415e+48\n",
      "Gradient Descent(26/49): loss=1.338039917503994e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=1.6075184842750644e+52\n",
      "Gradient Descent(28/49): loss=1.931269496134646e+54\n",
      "Gradient Descent(29/49): loss=2.3202233151193167e+56\n",
      "Gradient Descent(30/49): loss=2.78751165634722e+58\n",
      "Gradient Descent(31/49): loss=3.3489109361321305e+60\n",
      "Gradient Descent(32/49): loss=4.023374909521107e+62\n",
      "Gradient Descent(33/49): loss=4.8336745799695786e+64\n",
      "Gradient Descent(34/49): loss=5.807166985545966e+66\n",
      "Gradient Descent(35/49): loss=6.97671881714187e+68\n",
      "Gradient Descent(36/49): loss=8.381816051546794e+70\n",
      "Gradient Descent(37/49): loss=1.0069897062405734e+73\n",
      "Gradient Descent(38/49): loss=1.20979542170619e+75\n",
      "Gradient Descent(39/49): loss=1.4534458031804064e+77\n",
      "Gradient Descent(40/49): loss=1.7461668848139006e+79\n",
      "Gradient Descent(41/49): loss=2.097841407604373e+81\n",
      "Gradient Descent(42/49): loss=2.52034247684669e+83\n",
      "Gradient Descent(43/49): loss=3.0279344175264536e+85\n",
      "Gradient Descent(44/49): loss=3.637754361189955e+87\n",
      "Gradient Descent(45/49): loss=4.3703908234469984e+89\n",
      "Gradient Descent(46/49): loss=5.250578805827258e+91\n",
      "Gradient Descent(47/49): loss=6.308034889762622e+93\n",
      "Gradient Descent(48/49): loss=7.57846051682934e+95\n",
      "Gradient Descent(49/49): loss=9.104747327626473e+97\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5369645932441967\n",
      "Gradient Descent(2/49): loss=58.2096666277609\n",
      "Gradient Descent(3/49): loss=1687.625326727734\n",
      "Gradient Descent(4/49): loss=58781.18739167108\n",
      "Gradient Descent(5/49): loss=3095040.234931225\n",
      "Gradient Descent(6/49): loss=264147512.4866364\n",
      "Gradient Descent(7/49): loss=29049941836.75459\n",
      "Gradient Descent(8/49): loss=3453193502111.013\n",
      "Gradient Descent(9/49): loss=418450202841591.06\n",
      "Gradient Descent(10/49): loss=5.093407381448612e+16\n",
      "Gradient Descent(11/49): loss=6.206091942349782e+18\n",
      "Gradient Descent(12/49): loss=7.56362014494061e+20\n",
      "Gradient Descent(13/49): loss=9.218588648274517e+22\n",
      "Gradient Descent(14/49): loss=1.1235811992855405e+25\n",
      "Gradient Descent(15/49): loss=1.3694484880070646e+27\n",
      "Gradient Descent(16/49): loss=1.669118667279814e+29\n",
      "Gradient Descent(17/49): loss=2.034364603167226e+31\n",
      "Gradient Descent(18/49): loss=2.4795357914785743e+33\n",
      "Gradient Descent(19/49): loss=3.0221218842230635e+35\n",
      "Gradient Descent(20/49): loss=3.683439751204416e+37\n",
      "Gradient Descent(21/49): loss=4.489470949841261e+39\n",
      "Gradient Descent(22/49): loss=5.47188247199939e+41\n",
      "Gradient Descent(23/49): loss=6.669270861201822e+43\n",
      "Gradient Descent(24/49): loss=8.128678575954878e+45\n",
      "Gradient Descent(25/49): loss=9.90744217267426e+47\n",
      "Gradient Descent(26/49): loss=1.2075444918595265e+50\n",
      "Gradient Descent(27/49): loss=1.4717862334257616e+52\n",
      "Gradient Descent(28/49): loss=1.793850853119248e+54\n",
      "Gradient Descent(29/49): loss=2.186391481422226e+56\n",
      "Gradient Descent(30/49): loss=2.664830078667637e+58\n",
      "Gradient Descent(31/49): loss=3.247963326106714e+60\n",
      "Gradient Descent(32/49): loss=3.958701101500854e+62\n",
      "Gradient Descent(33/49): loss=4.824966552134523e+64\n",
      "Gradient Descent(34/49): loss=5.880793126914924e+66\n",
      "Gradient Descent(35/49): loss=7.167661667265016e+68\n",
      "Gradient Descent(36/49): loss=8.736130087835854e+70\n",
      "Gradient Descent(37/49): loss=1.0647819673205984e+73\n",
      "Gradient Descent(38/49): loss=1.2977836027302005e+75\n",
      "Gradient Descent(39/49): loss=1.5817719788714724e+77\n",
      "Gradient Descent(40/49): loss=1.9279043038295734e+79\n",
      "Gradient Descent(41/49): loss=2.3497792693080825e+81\n",
      "Gradient Descent(42/49): loss=2.8639713099360085e+83\n",
      "Gradient Descent(43/49): loss=3.4906817722295204e+85\n",
      "Gradient Descent(44/49): loss=4.254532576043043e+87\n",
      "Gradient Descent(45/49): loss=5.185533549524934e+89\n",
      "Gradient Descent(46/49): loss=6.320261441800572e+91\n",
      "Gradient Descent(47/49): loss=7.703296933903736e+93\n",
      "Gradient Descent(48/49): loss=9.388976104600097e+95\n",
      "Gradient Descent(49/49): loss=1.144352516190469e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5856168182862502\n",
      "Gradient Descent(2/49): loss=60.66989808165047\n",
      "Gradient Descent(3/49): loss=1862.922221933654\n",
      "Gradient Descent(4/49): loss=76409.23260946633\n",
      "Gradient Descent(5/49): loss=5184823.231207794\n",
      "Gradient Descent(6/49): loss=525431186.2833181\n",
      "Gradient Descent(7/49): loss=62180775634.0222\n",
      "Gradient Descent(8/49): loss=7666793369199.04\n",
      "Gradient Descent(9/49): loss=954410370266972.9\n",
      "Gradient Descent(10/49): loss=1.190692839588096e+17\n",
      "Gradient Descent(11/49): loss=1.4861972914815156e+19\n",
      "Gradient Descent(12/49): loss=1.8552430844037523e+21\n",
      "Gradient Descent(13/49): loss=2.3159856752182122e+23\n",
      "Gradient Descent(14/49): loss=2.8911679175740395e+25\n",
      "Gradient Descent(15/49): loss=3.6092029337470364e+27\n",
      "Gradient Descent(16/49): loss=4.505566544017094e+29\n",
      "Gradient Descent(17/49): loss=5.624546893953994e+31\n",
      "Gradient Descent(18/49): loss=7.0214317992536e+33\n",
      "Gradient Descent(19/49): loss=8.765240222229262e+35\n",
      "Gradient Descent(20/49): loss=1.0942132374740013e+38\n",
      "Gradient Descent(21/49): loss=1.3659666807724314e+40\n",
      "Gradient Descent(22/49): loss=1.7052114790296144e+42\n",
      "Gradient Descent(23/49): loss=2.128709454752784e+44\n",
      "Gradient Descent(24/49): loss=2.657385314659453e+46\n",
      "Gradient Descent(25/49): loss=3.3173605232051695e+48\n",
      "Gradient Descent(26/49): loss=4.141243943892002e+50\n",
      "Gradient Descent(27/49): loss=5.169743017937876e+52\n",
      "Gradient Descent(28/49): loss=6.453675087394164e+54\n",
      "Gradient Descent(29/49): loss=8.056478240627348e+56\n",
      "Gradient Descent(30/49): loss=1.0057345739094144e+59\n",
      "Gradient Descent(31/49): loss=1.25551388950065e+61\n",
      "Gradient Descent(32/49): loss=1.5673271732140613e+63\n",
      "Gradient Descent(33/49): loss=1.9565808777090946e+65\n",
      "Gradient Descent(34/49): loss=2.4425077268114667e+67\n",
      "Gradient Descent(35/49): loss=3.049116989489777e+69\n",
      "Gradient Descent(36/49): loss=3.806380759225783e+71\n",
      "Gradient Descent(37/49): loss=4.751714851921351e+73\n",
      "Gradient Descent(38/49): loss=5.9318274923610285e+75\n",
      "Gradient Descent(39/49): loss=7.405027131394852e+77\n",
      "Gradient Descent(40/49): loss=9.244103421299715e+79\n",
      "Gradient Descent(41/49): loss=1.1539923696078379e+82\n",
      "Gradient Descent(42/49): loss=1.440592265599998e+84\n",
      "Gradient Descent(43/49): loss=1.7983707088217865e+86\n",
      "Gradient Descent(44/49): loss=2.245005254836069e+88\n",
      "Gradient Descent(45/49): loss=2.802563770371649e+90\n",
      "Gradient Descent(46/49): loss=3.4985947895134015e+92\n",
      "Gradient Descent(47/49): loss=4.367488665418325e+94\n",
      "Gradient Descent(48/49): loss=5.452176771008921e+96\n",
      "Gradient Descent(49/49): loss=6.806252704832498e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.606489472835713\n",
      "Gradient Descent(2/49): loss=61.9167752494242\n",
      "Gradient Descent(3/49): loss=1894.9981414918607\n",
      "Gradient Descent(4/49): loss=74358.16464617869\n",
      "Gradient Descent(5/49): loss=4636397.836641144\n",
      "Gradient Descent(6/49): loss=438792114.5228462\n",
      "Gradient Descent(7/49): loss=49776051425.13795\n",
      "Gradient Descent(8/49): loss=5946252449242.3\n",
      "Gradient Descent(9/49): loss=719428117135582.5\n",
      "Gradient Descent(10/49): loss=8.73042276270128e+16\n",
      "Gradient Descent(11/49): loss=1.0602006365477038e+19\n",
      "Gradient Descent(12/49): loss=1.2876919936925764e+21\n",
      "Gradient Descent(13/49): loss=1.564056808126758e+23\n",
      "Gradient Descent(14/49): loss=1.8997520409967834e+25\n",
      "Gradient Descent(15/49): loss=2.3075027044787345e+27\n",
      "Gradient Descent(16/49): loss=2.8027717278248844e+29\n",
      "Gradient Descent(17/49): loss=3.4043428119606294e+31\n",
      "Gradient Descent(18/49): loss=4.135031822793577e+33\n",
      "Gradient Descent(19/49): loss=5.022551847729932e+35\n",
      "Gradient Descent(20/49): loss=6.100564198854503e+37\n",
      "Gradient Descent(21/49): loss=7.409955075644009e+39\n",
      "Gradient Descent(22/49): loss=9.000386265527268e+41\n",
      "Gradient Descent(23/49): loss=1.0932178684378892e+44\n",
      "Gradient Descent(24/49): loss=1.3278600191354792e+46\n",
      "Gradient Descent(25/49): loss=1.6128644447969955e+48\n",
      "Gradient Descent(26/49): loss=1.9590406216044226e+50\n",
      "Gradient Descent(27/49): loss=2.3795181110707192e+52\n",
      "Gradient Descent(28/49): loss=2.890244530139761e+54\n",
      "Gradient Descent(29/49): loss=3.5105904027953606e+56\n",
      "Gradient Descent(30/49): loss=4.264083833627476e+58\n",
      "Gradient Descent(31/49): loss=5.1793028676103145e+60\n",
      "Gradient Descent(32/49): loss=6.290959380978116e+62\n",
      "Gradient Descent(33/49): loss=7.641215612358458e+64\n",
      "Gradient Descent(34/49): loss=9.281283266761957e+66\n",
      "Gradient Descent(35/49): loss=1.1273365842282025e+69\n",
      "Gradient Descent(36/49): loss=1.3693017846902963e+71\n",
      "Gradient Descent(37/49): loss=1.6632010384366805e+73\n",
      "Gradient Descent(38/49): loss=2.0201811793318317e+75\n",
      "Gradient Descent(39/49): loss=2.45378153513098e+77\n",
      "Gradient Descent(40/49): loss=2.980447438947562e+79\n",
      "Gradient Descent(41/49): loss=3.6201539579419745e+81\n",
      "Gradient Descent(42/49): loss=4.3971634956361883e+83\n",
      "Gradient Descent(43/49): loss=5.340946001740753e+85\n",
      "Gradient Descent(44/49): loss=6.487296690655003e+87\n",
      "Gradient Descent(45/49): loss=7.879693660798612e+89\n",
      "Gradient Descent(46/49): loss=9.570946905738137e+91\n",
      "Gradient Descent(47/49): loss=1.1625201259813264e+94\n",
      "Gradient Descent(48/49): loss=1.4120369244775264e+96\n",
      "Gradient Descent(49/49): loss=1.715108608898171e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.653392125299595\n",
      "Gradient Descent(2/49): loss=62.89192252540133\n",
      "Gradient Descent(3/49): loss=1900.9509578595942\n",
      "Gradient Descent(4/49): loss=72470.00380003896\n",
      "Gradient Descent(5/49): loss=4348973.670910424\n",
      "Gradient Descent(6/49): loss=402735326.4265007\n",
      "Gradient Descent(7/49): loss=45384949709.31095\n",
      "Gradient Descent(8/49): loss=5413986032906.557\n",
      "Gradient Descent(9/49): loss=654948692332294.5\n",
      "Gradient Descent(10/49): loss=7.94933722319007e+16\n",
      "Gradient Descent(11/49): loss=9.655809350119197e+18\n",
      "Gradient Descent(12/49): loss=1.1730705337497953e+21\n",
      "Gradient Descent(13/49): loss=1.4252059281454505e+23\n",
      "Gradient Descent(14/49): loss=1.7315510818618937e+25\n",
      "Gradient Descent(15/49): loss=2.103749231060775e+27\n",
      "Gradient Descent(16/49): loss=2.5559529720084735e+29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=3.1053588943490138e+31\n",
      "Gradient Descent(18/49): loss=3.7728605493107817e+33\n",
      "Gradient Descent(19/49): loss=4.583842738266608e+35\n",
      "Gradient Descent(20/49): loss=5.569146806897085e+37\n",
      "Gradient Descent(21/49): loss=6.766243507612238e+39\n",
      "Gradient Descent(22/49): loss=8.220657992236418e+41\n",
      "Gradient Descent(23/49): loss=9.987701115187306e+43\n",
      "Gradient Descent(24/49): loss=1.2134572884672263e+46\n",
      "Gradient Descent(25/49): loss=1.4742918054452443e+48\n",
      "Gradient Descent(26/49): loss=1.7911931044141428e+50\n",
      "Gradient Descent(27/49): loss=2.176212826694831e+52\n",
      "Gradient Descent(28/49): loss=2.643993132510443e+54\n",
      "Gradient Descent(29/49): loss=3.21232353702265e+56\n",
      "Gradient Descent(30/49): loss=3.902817439133064e+58\n",
      "Gradient Descent(31/49): loss=4.7417340711946915e+60\n",
      "Gradient Descent(32/49): loss=5.760977128082852e+62\n",
      "Gradient Descent(33/49): loss=6.999308053125817e+64\n",
      "Gradient Descent(34/49): loss=8.503820121718596e+66\n",
      "Gradient Descent(35/49): loss=1.0331729381485258e+69\n",
      "Gradient Descent(36/49): loss=1.255255055779263e+71\n",
      "Gradient Descent(37/49): loss=1.5250740673512317e+73\n",
      "Gradient Descent(38/49): loss=1.8528910918931134e+75\n",
      "Gradient Descent(39/49): loss=2.2511728918056913e+77\n",
      "Gradient Descent(40/49): loss=2.7350659793085604e+79\n",
      "Gradient Descent(41/49): loss=3.322972632799817e+81\n",
      "Gradient Descent(42/49): loss=4.0372507288207315e+83\n",
      "Gradient Descent(43/49): loss=4.905064003981986e+85\n",
      "Gradient Descent(44/49): loss=5.959415082002603e+87\n",
      "Gradient Descent(45/49): loss=7.240400551505218e+89\n",
      "Gradient Descent(46/49): loss=8.796735824721354e+91\n",
      "Gradient Descent(47/49): loss=1.0687607766928006e+94\n",
      "Gradient Descent(48/49): loss=1.2984925551440812e+96\n",
      "Gradient Descent(49/49): loss=1.5776055339362795e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5639217407486643\n",
      "Gradient Descent(2/49): loss=59.64352638606932\n",
      "Gradient Descent(3/49): loss=1750.3011380098278\n",
      "Gradient Descent(4/49): loss=61661.6887021538\n",
      "Gradient Descent(5/49): loss=3279968.8060137797\n",
      "Gradient Descent(6/49): loss=282746346.6716209\n",
      "Gradient Descent(7/49): loss=31428839476.174583\n",
      "Gradient Descent(8/49): loss=3777465762070.821\n",
      "Gradient Descent(9/49): loss=462884747242161.56\n",
      "Gradient Descent(10/49): loss=5.697725535197171e+16\n",
      "Gradient Descent(11/49): loss=7.020679539052204e+18\n",
      "Gradient Descent(12/49): loss=8.652855734259788e+20\n",
      "Gradient Descent(13/49): loss=1.0665058557863432e+23\n",
      "Gradient Descent(14/49): loss=1.3145357095709555e+25\n",
      "Gradient Descent(15/49): loss=1.6202527137279898e+27\n",
      "Gradient Descent(16/49): loss=1.9970705470913995e+29\n",
      "Gradient Descent(17/49): loss=2.4615242571249856e+31\n",
      "Gradient Descent(18/49): loss=3.033994908772436e+33\n",
      "Gradient Descent(19/49): loss=3.739603682798567e+35\n",
      "Gradient Descent(20/49): loss=4.609314171403824e+37\n",
      "Gradient Descent(21/49): loss=5.681291104980092e+39\n",
      "Gradient Descent(22/49): loss=7.002575095169988e+41\n",
      "Gradient Descent(23/49): loss=8.631146874614201e+43\n",
      "Gradient Descent(24/49): loss=1.0638471613521748e+46\n",
      "Gradient Descent(25/49): loss=1.3112634962186274e+48\n",
      "Gradient Descent(26/49): loss=1.6162208435378562e+50\n",
      "Gradient Descent(27/49): loss=1.9921013759776144e+52\n",
      "Gradient Descent(28/49): loss=2.4553995254046164e+54\n",
      "Gradient Descent(29/49): loss=3.0264457934017013e+56\n",
      "Gradient Descent(30/49): loss=3.7302988966284264e+58\n",
      "Gradient Descent(31/49): loss=4.597845396248405e+60\n",
      "Gradient Descent(32/49): loss=5.667155065485583e+62\n",
      "Gradient Descent(33/49): loss=6.985151471701067e+64\n",
      "Gradient Descent(34/49): loss=8.609671081662394e+66\n",
      "Gradient Descent(35/49): loss=1.061200124789289e+69\n",
      "Gradient Descent(36/49): loss=1.3080008448306257e+71\n",
      "Gradient Descent(37/49): loss=1.6121994052886987e+73\n",
      "Gradient Descent(38/49): loss=1.9871446816610333e+75\n",
      "Gradient Descent(39/49): loss=2.4492900648022366e+77\n",
      "Gradient Descent(40/49): loss=3.01891547047521e+79\n",
      "Gradient Descent(41/49): loss=3.7210172648988975e+81\n",
      "Gradient Descent(42/49): loss=4.586405158106626e+83\n",
      "Gradient Descent(43/49): loss=5.653054199112632e+85\n",
      "Gradient Descent(44/49): loss=6.967771201290499e+87\n",
      "Gradient Descent(45/49): loss=8.588248724229866e+89\n",
      "Gradient Descent(46/49): loss=1.0585596745136423e+92\n",
      "Gradient Descent(47/49): loss=1.3047463114861185e+94\n",
      "Gradient Descent(48/49): loss=1.6081879730765244e+96\n",
      "Gradient Descent(49/49): loss=1.9822003204609117e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.6130126716807984\n",
      "Gradient Descent(2/49): loss=62.15428908764266\n",
      "Gradient Descent(3/49): loss=1931.358379338843\n",
      "Gradient Descent(4/49): loss=80083.52833393456\n",
      "Gradient Descent(5/49): loss=5488666.778145968\n",
      "Gradient Descent(6/49): loss=561992250.7149781\n",
      "Gradient Descent(7/49): loss=67232843145.91947\n",
      "Gradient Descent(8/49): loss=8381895042365.371\n",
      "Gradient Descent(9/49): loss=1055099813219267.6\n",
      "Gradient Descent(10/49): loss=1.331052471848559e+17\n",
      "Gradient Descent(11/49): loss=1.6800056993209799e+19\n",
      "Gradient Descent(12/49): loss=2.12067649691315e+21\n",
      "Gradient Descent(13/49): loss=2.677003237924614e+23\n",
      "Gradient Descent(14/49): loss=3.3792926004433373e+25\n",
      "Gradient Descent(15/49): loss=4.2658270481990954e+27\n",
      "Gradient Descent(16/49): loss=5.384939302664372e+29\n",
      "Gradient Descent(17/49): loss=6.797643880965934e+31\n",
      "Gradient Descent(18/49): loss=8.580962642905014e+33\n",
      "Gradient Descent(19/49): loss=1.0832123806999329e+36\n",
      "Gradient Descent(20/49): loss=1.3673862844691496e+38\n",
      "Gradient Descent(21/49): loss=1.726111411363413e+40\n",
      "Gradient Descent(22/49): loss=2.178945802212516e+42\n",
      "Gradient Descent(23/49): loss=2.750578426028746e+44\n",
      "Gradient Descent(24/49): loss=3.4721752464271286e+46\n",
      "Gradient Descent(25/49): loss=4.383078420095368e+48\n",
      "Gradient Descent(26/49): loss=5.532951269229846e+50\n",
      "Gradient Descent(27/49): loss=6.984485973903063e+52\n",
      "Gradient Descent(28/49): loss=8.816821610366063e+54\n",
      "Gradient Descent(29/49): loss=1.1129858890041959e+57\n",
      "Gradient Descent(30/49): loss=1.4049706842951666e+59\n",
      "Gradient Descent(31/49): loss=1.7735558404023838e+61\n",
      "Gradient Descent(32/49): loss=2.2388369765902783e+63\n",
      "Gradient Descent(33/49): loss=2.8261816704969373e+65\n",
      "Gradient Descent(34/49): loss=3.5676125230061644e+67\n",
      "Gradient Descent(35/49): loss=4.5035530614253094e+69\n",
      "Gradient Descent(36/49): loss=5.685031669297735e+71\n",
      "Gradient Descent(37/49): loss=7.176463703236618e+73\n",
      "Gradient Descent(38/49): loss=9.059163480479673e+75\n",
      "Gradient Descent(39/49): loss=1.1435777614125217e+78\n",
      "Gradient Descent(40/49): loss=1.44358814057743e+80\n",
      "Gradient Descent(41/49): loss=1.8223043416319885e+82\n",
      "Gradient Descent(42/49): loss=2.300374338211518e+84\n",
      "Gradient Descent(43/49): loss=2.903862968993961e+86\n",
      "Gradient Descent(44/49): loss=3.66567301791858e+88\n",
      "Gradient Descent(45/49): loss=4.627339105795193e+90\n",
      "Gradient Descent(46/49): loss=5.841292198009592e+92\n",
      "Gradient Descent(47/49): loss=7.373718191475403e+94\n",
      "Gradient Descent(48/49): loss=9.30816643375956e+96\n",
      "Gradient Descent(49/49): loss=1.1750104914333973e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.634129882050547\n",
      "Gradient Descent(2/49): loss=63.430754021038645\n",
      "Gradient Descent(3/49): loss=1964.6636159904488\n",
      "Gradient Descent(4/49): loss=77947.70758098984\n",
      "Gradient Descent(5/49): loss=4909376.304947781\n",
      "Gradient Descent(6/49): loss=469409051.17912734\n",
      "Gradient Descent(7/49): loss=53826972015.617775\n",
      "Gradient Descent(8/49): loss=6501602180611.896\n",
      "Gradient Descent(9/49): loss=795419812984276.8\n",
      "Gradient Descent(10/49): loss=9.760807805803282e+16\n",
      "Gradient Descent(11/49): loss=1.1986229262288566e+19\n",
      "Gradient Descent(12/49): loss=1.4721471168640684e+21\n",
      "Gradient Descent(13/49): loss=1.808158968118946e+23\n",
      "Gradient Descent(14/49): loss=2.2208842275184012e+25\n",
      "Gradient Descent(15/49): loss=2.727822725752627e+27\n",
      "Gradient Descent(16/49): loss=3.3504765279903216e+29\n",
      "Gradient Descent(17/49): loss=4.115257983878398e+31\n",
      "Gradient Descent(18/49): loss=5.054608974357909e+33\n",
      "Gradient Descent(19/49): loss=6.2083767635509426e+35\n",
      "Gradient Descent(20/49): loss=7.625504225922368e+37\n",
      "Gradient Descent(21/49): loss=9.366105978090604e+39\n",
      "Gradient Descent(22/49): loss=1.1504018436128626e+42\n",
      "Gradient Descent(23/49): loss=1.4129931957935812e+44\n",
      "Gradient Descent(24/49): loss=1.7355237932315804e+46\n",
      "Gradient Descent(25/49): loss=2.131675400732805e+48\n",
      "Gradient Descent(26/49): loss=2.618252790201956e+50\n",
      "Gradient Descent(27/49): loss=3.215896599943818e+52\n",
      "Gradient Descent(28/49): loss=3.949958911619407e+54\n",
      "Gradient Descent(29/49): loss=4.851578686875091e+56\n",
      "Gradient Descent(30/49): loss=5.959002683724369e+58\n",
      "Gradient Descent(31/49): loss=7.31920788602232e+60\n",
      "Gradient Descent(32/49): loss=8.989894269577582e+62\n",
      "Gradient Descent(33/49): loss=1.1041932438142933e+65\n",
      "Gradient Descent(34/49): loss=1.3562369958133448e+67\n",
      "Gradient Descent(35/49): loss=1.6658123920944707e+69\n",
      "Gradient Descent(36/49): loss=2.046051637156032e+71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=2.5130844996568382e+73\n",
      "Gradient Descent(38/49): loss=3.0867225380460583e+75\n",
      "Gradient Descent(39/49): loss=3.7912995079084874e+77\n",
      "Gradient Descent(40/49): loss=4.656703601148964e+79\n",
      "Gradient Descent(41/49): loss=5.7196453046562975e+81\n",
      "Gradient Descent(42/49): loss=7.025214661075769e+83\n",
      "Gradient Descent(43/49): loss=8.628793990776773e+85\n",
      "Gradient Descent(44/49): loss=1.0598407212778962e+88\n",
      "Gradient Descent(45/49): loss=1.3017605422953687e+90\n",
      "Gradient Descent(46/49): loss=1.598901113588019e+92\n",
      "Gradient Descent(47/49): loss=1.9638671537279894e+94\n",
      "Gradient Descent(48/49): loss=2.412140541222648e+96\n",
      "Gradient Descent(49/49): loss=2.962737056610397e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.681631563051804\n",
      "Gradient Descent(2/49): loss=64.4307112197606\n",
      "Gradient Descent(3/49): loss=1970.9177776744625\n",
      "Gradient Descent(4/49): loss=75976.44406024966\n",
      "Gradient Descent(5/49): loss=4605548.2532875165\n",
      "Gradient Descent(6/49): loss=430858036.73864937\n",
      "Gradient Descent(7/49): loss=49079223470.382576\n",
      "Gradient Descent(8/49): loss=5919643245226.6045\n",
      "Gradient Descent(9/49): loss=724129524779339.4\n",
      "Gradient Descent(10/49): loss=8.887528272758242e+16\n",
      "Gradient Descent(11/49): loss=1.0916480725758167e+19\n",
      "Gradient Descent(12/49): loss=1.3411048107349028e+21\n",
      "Gradient Descent(13/49): loss=1.6476350665473317e+23\n",
      "Gradient Descent(14/49): loss=2.0242473247972083e+25\n",
      "Gradient Descent(15/49): loss=2.486950309360016e+27\n",
      "Gradient Descent(16/49): loss=3.0554196773383e+29\n",
      "Gradient Descent(17/49): loss=3.7538307525579164e+31\n",
      "Gradient Descent(18/49): loss=4.61188550491869e+33\n",
      "Gradient Descent(19/49): loss=5.666075391724815e+35\n",
      "Gradient Descent(20/49): loss=6.961233178938087e+37\n",
      "Gradient Descent(21/49): loss=8.552439570382413e+39\n",
      "Gradient Descent(22/49): loss=1.050736568234214e+42\n",
      "Gradient Descent(23/49): loss=1.290915097102151e+44\n",
      "Gradient Descent(24/49): loss=1.5859939001973543e+46\n",
      "Gradient Descent(25/49): loss=1.9485221430227555e+48\n",
      "Gradient Descent(26/49): loss=2.3939174932377896e+50\n",
      "Gradient Descent(27/49): loss=2.941121806057491e+52\n",
      "Gradient Descent(28/49): loss=3.613406686947971e+54\n",
      "Gradient Descent(29/49): loss=4.439363190735183e+56\n",
      "Gradient Descent(30/49): loss=5.454117747233482e+58\n",
      "Gradient Descent(31/49): loss=6.700826024500362e+60\n",
      "Gradient Descent(32/49): loss=8.232508261009942e+62\n",
      "Gradient Descent(33/49): loss=1.0114304120088061e+65\n",
      "Gradient Descent(34/49): loss=1.2426242961471504e+67\n",
      "Gradient Descent(35/49): loss=1.5266647344610108e+69\n",
      "Gradient Descent(36/49): loss=1.8756314508523185e+71\n",
      "Gradient Descent(37/49): loss=2.304365365895806e+73\n",
      "Gradient Descent(38/49): loss=2.831099754233164e+75\n",
      "Gradient Descent(39/49): loss=3.4782356726245283e+77\n",
      "Gradient Descent(40/49): loss=4.2732946362020715e+79\n",
      "Gradient Descent(41/49): loss=5.25008905851808e+81\n",
      "Gradient Descent(42/49): loss=6.450160232075315e+83\n",
      "Gradient Descent(43/49): loss=7.924545004040385e+85\n",
      "Gradient Descent(44/49): loss=9.735946280648496e+87\n",
      "Gradient Descent(45/49): loss=1.1961399667911026e+90\n",
      "Gradient Descent(46/49): loss=1.4695549656008226e+92\n",
      "Gradient Descent(47/49): loss=1.8054674677543446e+94\n",
      "Gradient Descent(48/49): loss=2.218163221806767e+96\n",
      "Gradient Descent(49/49): loss=2.7251934285451692e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.5910476813814767\n",
      "Gradient Descent(2/49): loss=61.103897996854855\n",
      "Gradient Descent(3/49): loss=1814.9063848504966\n",
      "Gradient Descent(4/49): loss=64665.13443467285\n",
      "Gradient Descent(5/49): loss=3474802.0321205026\n",
      "Gradient Descent(6/49): loss=302539773.9451073\n",
      "Gradient Descent(7/49): loss=33987634530.209557\n",
      "Gradient Descent(8/49): loss=4130122323221.348\n",
      "Gradient Descent(9/49): loss=511749762912819.1\n",
      "Gradient Descent(10/49): loss=6.369761927263305e+16\n",
      "Gradient Descent(11/49): loss=7.936728384067505e+18\n",
      "Gradient Descent(12/49): loss=9.891530539248133e+20\n",
      "Gradient Descent(13/49): loss=1.2328470184007226e+23\n",
      "Gradient Descent(14/49): loss=1.5365981515951744e+25\n",
      "Gradient Descent(15/49): loss=1.9151935230005167e+27\n",
      "Gradient Descent(16/49): loss=2.3870708312988406e+29\n",
      "Gradient Descent(17/49): loss=2.9752126543470966e+31\n",
      "Gradient Descent(18/49): loss=3.70826475908213e+33\n",
      "Gradient Descent(19/49): loss=4.6219310108329124e+35\n",
      "Gradient Descent(20/49): loss=5.760712272438045e+37\n",
      "Gradient Descent(21/49): loss=7.180073830961313e+39\n",
      "Gradient Descent(22/49): loss=8.949146873648499e+41\n",
      "Gradient Descent(23/49): loss=1.1154095578028955e+44\n",
      "Gradient Descent(24/49): loss=1.390231380946051e+46\n",
      "Gradient Descent(25/49): loss=1.7327655828737382e+48\n",
      "Gradient Descent(26/49): loss=2.1596955775443776e+50\n",
      "Gradient Descent(27/49): loss=2.6918153463837056e+52\n",
      "Gradient Descent(28/49): loss=3.355042226490866e+54\n",
      "Gradient Descent(29/49): loss=4.181679236155237e+56\n",
      "Gradient Descent(30/49): loss=5.21198842030115e+58\n",
      "Gradient Descent(31/49): loss=6.496151846962336e+60\n",
      "Gradient Descent(32/49): loss=8.09671576675382e+62\n",
      "Gradient Descent(33/49): loss=1.0091636980169336e+65\n",
      "Gradient Descent(34/49): loss=1.2578079788560128e+67\n",
      "Gradient Descent(35/49): loss=1.567714846246143e+69\n",
      "Gradient Descent(36/49): loss=1.9539785726084909e+71\n",
      "Gradient Descent(37/49): loss=2.4354124548576548e+73\n",
      "Gradient Descent(38/49): loss=3.035465131717277e+75\n",
      "Gradient Descent(39/49): loss=3.7833626692238184e+77\n",
      "Gradient Descent(40/49): loss=4.715532040646026e+79\n",
      "Gradient Descent(41/49): loss=5.877375332595632e+81\n",
      "Gradient Descent(42/49): loss=7.325481091518909e+83\n",
      "Gradient Descent(43/49): loss=9.130380516042413e+85\n",
      "Gradient Descent(44/49): loss=1.1379982737822057e+88\n",
      "Gradient Descent(45/49): loss=1.4183856509111361e+90\n",
      "Gradient Descent(46/49): loss=1.7678566840213424e+92\n",
      "Gradient Descent(47/49): loss=2.2034326512195207e+94\n",
      "Gradient Descent(48/49): loss=2.7463286432338697e+96\n",
      "Gradient Descent(49/49): loss=3.4229868621001374e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.640578937141995\n",
      "Gradient Descent(2/49): loss=63.66593741857222\n",
      "Gradient Descent(3/49): loss=2001.8793020701564\n",
      "Gradient Descent(4/49): loss=83911.7970400536\n",
      "Gradient Descent(5/49): loss=5808467.243679672\n",
      "Gradient Descent(6/49): loss=600873240.3600122\n",
      "Gradient Descent(7/49): loss=72663931111.39001\n",
      "Gradient Descent(8/49): loss=9159169944667.139\n",
      "Gradient Descent(9/49): loss=1165763906508050.8\n",
      "Gradient Descent(10/49): loss=1.487039365069073e+17\n",
      "Gradient Descent(11/49): loss=1.897798443430944e+19\n",
      "Gradient Descent(12/49): loss=2.422290646723266e+21\n",
      "Gradient Descent(13/49): loss=3.09181384000342e+23\n",
      "Gradient Descent(14/49): loss=3.9464161261347045e+25\n",
      "Gradient Descent(15/49): loss=5.037243783090497e+27\n",
      "Gradient Descent(16/49): loss=6.429588612854572e+29\n",
      "Gradient Descent(17/49): loss=8.206792080990115e+31\n",
      "Gradient Descent(18/49): loss=1.0475232754398207e+34\n",
      "Gradient Descent(19/49): loss=1.3370693510061044e+36\n",
      "Gradient Descent(20/49): loss=1.7066489047290497e+38\n",
      "Gradient Descent(21/49): loss=2.1783840025203494e+40\n",
      "Gradient Descent(22/49): loss=2.7805114745411234e+42\n",
      "Gradient Descent(23/49): loss=3.5490730978441913e+44\n",
      "Gradient Descent(24/49): loss=4.530072962904746e+46\n",
      "Gradient Descent(25/49): loss=5.782231158246413e+48\n",
      "Gradient Descent(26/49): loss=7.380498601497255e+50\n",
      "Gradient Descent(27/49): loss=9.420543405467054e+52\n",
      "Gradient Descent(28/49): loss=1.2024477321394422e+55\n",
      "Gradient Descent(29/49): loss=1.5348165029293867e+57\n",
      "Gradient Descent(30/49): loss=1.9590553790418074e+59\n",
      "Gradient Descent(31/49): loss=2.5005581910459892e+61\n",
      "Gradient Descent(32/49): loss=3.1917378822978077e+63\n",
      "Gradient Descent(33/49): loss=4.0739666630327747e+65\n",
      "Gradient Descent(34/49): loss=5.2000524427632157e+67\n",
      "Gradient Descent(35/49): loss=6.637399773752065e+69\n",
      "Gradient Descent(36/49): loss=8.472044511380741e+71\n",
      "Gradient Descent(37/49): loss=1.0813803695636393e+74\n",
      "Gradient Descent(38/49): loss=1.3802848912168906e+76\n",
      "Gradient Descent(39/49): loss=1.7618096597134179e+78\n",
      "Gradient Descent(40/49): loss=2.248791750754447e+80\n",
      "Gradient Descent(41/49): loss=2.8703806398040525e+82\n",
      "Gradient Descent(42/49): loss=3.663783013521671e+84\n",
      "Gradient Descent(43/49): loss=4.676489864802948e+86\n",
      "Gradient Descent(44/49): loss=5.969119179518075e+88\n",
      "Gradient Descent(45/49): loss=7.619044370748571e+90\n",
      "Gradient Descent(46/49): loss=9.72502564911477e+92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=1.2413121550918215e+95\n",
      "Gradient Descent(48/49): loss=1.5844234472728095e+97\n",
      "Gradient Descent(49/49): loss=2.0223741868395417e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.661942106562297\n",
      "Gradient Descent(2/49): loss=64.972511687974\n",
      "Gradient Descent(3/49): loss=2036.4525443284895\n",
      "Gradient Descent(4/49): loss=81688.2255162083\n",
      "Gradient Descent(5/49): loss=5196758.801492566\n",
      "Gradient Descent(6/49): loss=501974288.7893767\n",
      "Gradient Descent(7/49): loss=58182317674.86204\n",
      "Gradient Descent(8/49): loss=7105299295942.904\n",
      "Gradient Descent(9/49): loss=878948674686792.6\n",
      "Gradient Descent(10/49): loss=1.090605033756938e+17\n",
      "Gradient Descent(11/49): loss=1.3541958434949081e+19\n",
      "Gradient Descent(12/49): loss=1.6817751540602684e+21\n",
      "Gradient Descent(13/49): loss=2.0886771886456426e+23\n",
      "Gradient Descent(14/49): loss=2.5940519674303086e+25\n",
      "Gradient Descent(15/49): loss=3.221713709894816e+27\n",
      "Gradient Descent(16/49): loss=4.001247670065152e+29\n",
      "Gradient Descent(17/49): loss=4.969400207210664e+31\n",
      "Gradient Descent(18/49): loss=6.171809676914335e+33\n",
      "Gradient Descent(19/49): loss=7.665157431752785e+35\n",
      "Gradient Descent(20/49): loss=9.51983966070675e+37\n",
      "Gradient Descent(21/49): loss=1.1823285823395812e+40\n",
      "Gradient Descent(22/49): loss=1.468408005336565e+42\n",
      "Gradient Descent(23/49): loss=1.8237079796468404e+44\n",
      "Gradient Descent(24/49): loss=2.2649772971510696e+46\n",
      "Gradient Descent(25/49): loss=2.8130173327466883e+48\n",
      "Gradient Descent(26/49): loss=3.4936626182913983e+50\n",
      "Gradient Descent(27/49): loss=4.3389986788774027e+52\n",
      "Gradient Descent(28/49): loss=5.388874540068698e+54\n",
      "Gradient Descent(29/49): loss=6.692781205481847e+56\n",
      "Gradient Descent(30/49): loss=8.312184655885272e+58\n",
      "Gradient Descent(31/49): loss=1.0323423347074881e+61\n",
      "Gradient Descent(32/49): loss=1.2821306794174033e+63\n",
      "Gradient Descent(33/49): loss=1.5923584879129602e+65\n",
      "Gradient Descent(34/49): loss=1.9776498563941765e+67\n",
      "Gradient Descent(35/49): loss=2.456167366948927e+69\n",
      "Gradient Descent(36/49): loss=3.05046826917295e+71\n",
      "Gradient Descent(37/49): loss=3.78856782581159e+73\n",
      "Gradient Descent(38/49): loss=4.7052599483902375e+75\n",
      "Gradient Descent(39/49): loss=5.843757377415345e+77\n",
      "Gradient Descent(40/49): loss=7.257728725015543e+79\n",
      "Gradient Descent(41/49): loss=9.013828405931076e+81\n",
      "Gradient Descent(42/49): loss=1.1194838717452196e+84\n",
      "Gradient Descent(43/49): loss=1.390357218552168e+86\n",
      "Gradient Descent(44/49): loss=1.7267718133057968e+88\n",
      "Gradient Descent(45/49): loss=2.144586193706668e+90\n",
      "Gradient Descent(46/49): loss=2.6634960721488057e+92\n",
      "Gradient Descent(47/49): loss=3.3079627888915716e+94\n",
      "Gradient Descent(48/49): loss=4.1083664162730203e+96\n",
      "Gradient Descent(49/49): loss=5.102437871139277e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7100464742857704\n",
      "Gradient Descent(2/49): loss=65.99774793212455\n",
      "Gradient Descent(3/49): loss=2043.0196157173145\n",
      "Gradient Descent(4/49): loss=79630.69402672701\n",
      "Gradient Descent(5/49): loss=4875689.56686349\n",
      "Gradient Descent(6/49): loss=460771806.00427467\n",
      "Gradient Descent(7/49): loss=53051178907.33506\n",
      "Gradient Descent(8/49): loss=6469323529298.656\n",
      "Gradient Descent(9/49): loss=800171997530875.9\n",
      "Gradient Descent(10/49): loss=9.930298994107013e+16\n",
      "Gradient Descent(11/49): loss=1.2333348556154462e+19\n",
      "Gradient Descent(12/49): loss=1.5320707593994392e+21\n",
      "Gradient Descent(13/49): loss=1.9032466214365618e+23\n",
      "Gradient Descent(14/49): loss=2.3643708721980143e+25\n",
      "Gradient Descent(15/49): loss=2.937224430414186e+27\n",
      "Gradient Descent(16/49): loss=3.648874241012922e+29\n",
      "Gradient Descent(17/49): loss=4.532947754197929e+31\n",
      "Gradient Descent(18/49): loss=5.631220639838789e+33\n",
      "Gradient Descent(19/49): loss=6.995590469633775e+35\n",
      "Gradient Descent(20/49): loss=8.690528968002942e+37\n",
      "Gradient Descent(21/49): loss=1.0796128518266295e+40\n",
      "Gradient Descent(22/49): loss=1.3411886827831534e+42\n",
      "Gradient Descent(23/49): loss=1.6661408576396986e+44\n",
      "Gradient Descent(24/49): loss=2.0698246213564318e+46\n",
      "Gradient Descent(25/49): loss=2.5713155904759123e+48\n",
      "Gradient Descent(26/49): loss=3.194311149653488e+50\n",
      "Gradient Descent(27/49): loss=3.968250244581099e+52\n",
      "Gradient Descent(28/49): loss=4.9297044858407876e+54\n",
      "Gradient Descent(29/49): loss=6.124106298715546e+56\n",
      "Gradient Descent(30/49): loss=7.607895780708514e+58\n",
      "Gradient Descent(31/49): loss=9.451187714076981e+60\n",
      "Gradient Descent(32/49): loss=1.174108476002323e+63\n",
      "Gradient Descent(33/49): loss=1.4585793395757217e+65\n",
      "Gradient Descent(34/49): loss=1.811973708835546e+67\n",
      "Gradient Descent(35/49): loss=2.2509908322617244e+69\n",
      "Gradient Descent(36/49): loss=2.796375964076471e+71\n",
      "Gradient Descent(37/49): loss=3.473900657608428e+73\n",
      "Gradient Descent(38/49): loss=4.31558057069703e+75\n",
      "Gradient Descent(39/49): loss=5.361188329144515e+77\n",
      "Gradient Descent(40/49): loss=6.660132936855958e+79\n",
      "Gradient Descent(41/49): loss=8.273794542052945e+81\n",
      "Gradient Descent(42/49): loss=1.0278424886278856e+84\n",
      "Gradient Descent(43/49): loss=1.276875049361119e+86\n",
      "Gradient Descent(44/49): loss=1.586244886468409e+88\n",
      "Gradient Descent(45/49): loss=1.9705709192971561e+90\n",
      "Gradient Descent(46/49): loss=2.4480140368647397e+92\n",
      "Gradient Descent(47/49): loss=3.0411352699878052e+94\n",
      "Gradient Descent(48/49): loss=3.777961887101183e+96\n",
      "Gradient Descent(49/49): loss=4.693311790911139e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.618342415142632\n",
      "Gradient Descent(2/49): loss=62.59110648132759\n",
      "Gradient Descent(3/49): loss=1881.4883224625423\n",
      "Gradient Descent(4/49): loss=67796.00153338045\n",
      "Gradient Descent(5/49): loss=3680009.579933992\n",
      "Gradient Descent(6/49): loss=323597213.44300383\n",
      "Gradient Descent(7/49): loss=36738816288.97833\n",
      "Gradient Descent(8/49): loss=4513468914213.343\n",
      "Gradient Descent(9/49): loss=565458621845586.9\n",
      "Gradient Descent(10/49): loss=7.1166638000455016e+16\n",
      "Gradient Descent(11/49): loss=8.966204072773324e+18\n",
      "Gradient Descent(12/49): loss=1.1299140811765852e+21\n",
      "Gradient Descent(13/49): loss=1.4239875201480537e+23\n",
      "Gradient Descent(14/49): loss=1.7946197021438722e+25\n",
      "Gradient Descent(15/49): loss=2.2617257131913194e+27\n",
      "Gradient Descent(16/49): loss=2.8504125874667484e+29\n",
      "Gradient Descent(17/49): loss=3.59232470278503e+31\n",
      "Gradient Descent(18/49): loss=4.5273436115343874e+33\n",
      "Gradient Descent(19/49): loss=5.705731534755164e+35\n",
      "Gradient Descent(20/49): loss=7.190833124081992e+37\n",
      "Gradient Descent(21/49): loss=9.062480546965885e+39\n",
      "Gradient Descent(22/49): loss=1.142128488518506e+42\n",
      "Gradient Descent(23/49): loss=1.4394044517434037e+44\n",
      "Gradient Descent(24/49): loss=1.8140561211257353e+46\n",
      "Gradient Descent(25/49): loss=2.2862230324573968e+48\n",
      "Gradient Descent(26/49): loss=2.881286688581873e+50\n",
      "Gradient Descent(27/49): loss=3.631234951244603e+52\n",
      "Gradient Descent(28/49): loss=4.576381560153028e+54\n",
      "Gradient Descent(29/49): loss=5.767533212614175e+56\n",
      "Gradient Descent(30/49): loss=7.268720695897267e+58\n",
      "Gradient Descent(31/49): loss=9.160640885328828e+60\n",
      "Gradient Descent(32/49): loss=1.1544994633969972e+63\n",
      "Gradient Descent(33/49): loss=1.4549953738701688e+65\n",
      "Gradient Descent(34/49): loss=1.833705086145728e+67\n",
      "Gradient Descent(35/49): loss=2.3109862775802908e+69\n",
      "Gradient Descent(36/49): loss=2.912495370992251e+71\n",
      "Gradient Descent(37/49): loss=3.670566704936417e+73\n",
      "Gradient Descent(38/49): loss=4.625950677750824e+75\n",
      "Gradient Descent(39/49): loss=5.830004299936672e+77\n",
      "Gradient Descent(40/49): loss=7.347451908805502e+79\n",
      "Gradient Descent(41/49): loss=9.259864448606959e+81\n",
      "Gradient Descent(42/49): loss=1.1670044346096713e+84\n",
      "Gradient Descent(43/49): loss=1.4707551692115465e+86\n",
      "Gradient Descent(44/49): loss=1.8535668791060774e+88\n",
      "Gradient Descent(45/49): loss=2.336017746013381e+90\n",
      "Gradient Descent(46/49): loss=2.9440420905239605e+92\n",
      "Gradient Descent(47/49): loss=3.710324480868511e+94\n",
      "Gradient Descent(48/49): loss=4.676056703687274e+96\n",
      "Gradient Descent(49/49): loss=5.893152043397801e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.668315614669841\n",
      "Gradient Descent(2/49): loss=65.20517497305812\n",
      "Gradient Descent(3/49): loss=2074.5355364743777\n",
      "Gradient Descent(4/49): loss=87899.54662923713\n",
      "Gradient Descent(5/49): loss=6144966.39372129\n",
      "Gradient Descent(6/49): loss=642207294.3420233\n",
      "Gradient Descent(7/49): loss=78500141766.99167\n",
      "Gradient Descent(8/49): loss=10003634583371.023\n",
      "Gradient Descent(9/49): loss=1287327317675355.8\n",
      "Gradient Descent(10/49): loss=1.6602923401290224e+17\n",
      "Gradient Descent(11/49): loss=2.142385938581467e+19\n",
      "Gradient Descent(12/49): loss=2.7647753869697404e+21\n",
      "Gradient Descent(13/49): loss=3.568067254025499e+23\n",
      "Gradient Descent(14/49): loss=4.604777947194359e+25\n",
      "Gradient Descent(15/49): loss=5.942715125326839e+27\n",
      "Gradient Descent(16/49): loss=7.669397677767944e+29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=9.897776241394755e+31\n",
      "Gradient Descent(18/49): loss=1.277362057353663e+34\n",
      "Gradient Descent(19/49): loss=1.6485054735192204e+36\n",
      "Gradient Descent(20/49): loss=2.1274863165070335e+38\n",
      "Gradient Descent(21/49): loss=2.745637245607931e+40\n",
      "Gradient Descent(22/49): loss=3.54339476887011e+42\n",
      "Gradient Descent(23/49): loss=4.572944407804508e+44\n",
      "Gradient Descent(24/49): loss=5.901634427139629e+46\n",
      "Gradient Descent(25/49): loss=7.616381439532304e+48\n",
      "Gradient Descent(26/49): loss=9.82935607900284e+50\n",
      "Gradient Descent(27/49): loss=1.2685320672931981e+53\n",
      "Gradient Descent(28/49): loss=1.6371098908387177e+55\n",
      "Gradient Descent(29/49): loss=2.1127796953536096e+57\n",
      "Gradient Descent(30/49): loss=2.7266575482061593e+59\n",
      "Gradient Descent(31/49): loss=3.5189004331781363e+61\n",
      "Gradient Descent(32/49): loss=4.54133313028905e+63\n",
      "Gradient Descent(33/49): loss=5.860838347629449e+65\n",
      "Gradient Descent(34/49): loss=7.563731871583389e+67\n",
      "Gradient Descent(35/49): loss=9.761408937058624e+69\n",
      "Gradient Descent(36/49): loss=1.2597631176545343e+72\n",
      "Gradient Descent(37/49): loss=1.6257930825720463e+74\n",
      "Gradient Descent(38/49): loss=2.0981747364220697e+76\n",
      "Gradient Descent(39/49): loss=2.7078090513186858e+78\n",
      "Gradient Descent(40/49): loss=3.494575418874113e+80\n",
      "Gradient Descent(41/49): loss=4.509940371257756e+82\n",
      "Gradient Descent(42/49): loss=5.820324278150331e+84\n",
      "Gradient Descent(43/49): loss=7.511446252975263e+86\n",
      "Gradient Descent(44/49): loss=9.693931491608951e+88\n",
      "Gradient Descent(45/49): loss=1.2510547849128812e+91\n",
      "Gradient Descent(46/49): loss=1.614554503720399e+93\n",
      "Gradient Descent(47/49): loss=2.0836707368217408e+95\n",
      "Gradient Descent(48/49): loss=2.6890908479601264e+97\n",
      "Gradient Descent(49/49): loss=3.4704185554828484e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.689926146370963\n",
      "Gradient Descent(2/49): loss=66.54238622551661\n",
      "Gradient Descent(3/49): loss=2110.416434026003\n",
      "Gradient Descent(4/49): loss=85585.13787537442\n",
      "Gradient Descent(5/49): loss=5499218.113365529\n",
      "Gradient Descent(6/49): loss=536599969.1104226\n",
      "Gradient Descent(7/49): loss=62863089529.27692\n",
      "Gradient Descent(8/49): loss=7761249898632.094\n",
      "Gradient Descent(9/49): loss=970714258383760.6\n",
      "Gradient Descent(10/49): loss=1.217820972357959e+17\n",
      "Gradient Descent(11/49): loss=1.528931438239714e+19\n",
      "Gradient Descent(12/49): loss=1.9198429861505403e+21\n",
      "Gradient Descent(13/49): loss=2.410796317040758e+23\n",
      "Gradient Descent(14/49): loss=3.027326956302267e+25\n",
      "Gradient Descent(15/49): loss=3.8015356918454517e+27\n",
      "Gradient Descent(16/49): loss=4.773743010336663e+29\n",
      "Gradient Descent(17/49): loss=5.994583994766575e+31\n",
      "Gradient Descent(18/49): loss=7.527644069381811e+33\n",
      "Gradient Descent(19/49): loss=9.452770308987036e+35\n",
      "Gradient Descent(20/49): loss=1.1870230024898964e+38\n",
      "Gradient Descent(21/49): loss=1.4905933000372686e+40\n",
      "Gradient Descent(22/49): loss=1.8717989302975258e+42\n",
      "Gradient Descent(23/49): loss=2.3504944208744338e+44\n",
      "Gradient Descent(24/49): loss=2.951611913630377e+46\n",
      "Gradient Descent(25/49): loss=3.706459718144311e+48\n",
      "Gradient Descent(26/49): loss=4.654352958391639e+50\n",
      "Gradient Descent(27/49): loss=5.844661242436526e+52\n",
      "Gradient Descent(28/49): loss=7.339380004958881e+54\n",
      "Gradient Descent(29/49): loss=9.216359447161782e+56\n",
      "Gradient Descent(30/49): loss=1.1573359248587153e+59\n",
      "Gradient Descent(31/49): loss=1.4533140234467366e+61\n",
      "Gradient Descent(32/49): loss=1.824985819052261e+63\n",
      "Gradient Descent(33/49): loss=2.2917092837534025e+65\n",
      "Gradient Descent(34/49): loss=2.8777930142871226e+67\n",
      "Gradient Descent(35/49): loss=3.6137623091162575e+69\n",
      "Gradient Descent(36/49): loss=4.537949033149672e+71\n",
      "Gradient Descent(37/49): loss=5.698488075852303e+73\n",
      "Gradient Descent(38/49): loss=7.155824385293523e+75\n",
      "Gradient Descent(39/49): loss=8.985861153267797e+77\n",
      "Gradient Descent(40/49): loss=1.1283913120024926e+80\n",
      "Gradient Descent(41/49): loss=1.416967090059758e+82\n",
      "Gradient Descent(42/49): loss=1.7793434892273972e+84\n",
      "Gradient Descent(43/49): loss=2.2343943447003532e+86\n",
      "Gradient Descent(44/49): loss=2.8058203027436216e+88\n",
      "Gradient Descent(45/49): loss=3.5233832335642556e+90\n",
      "Gradient Descent(46/49): loss=4.4244563340079777e+92\n",
      "Gradient Descent(47/49): loss=5.5559706548697346e+94\n",
      "Gradient Descent(48/49): loss=6.976859434797585e+96\n",
      "Gradient Descent(49/49): loss=8.761127550279471e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.738636859001494\n",
      "Gradient Descent(2/49): loss=67.59337650612818\n",
      "Gradient Descent(3/49): loss=2117.3083146680765\n",
      "Gradient Descent(4/49): loss=83438.07101811959\n",
      "Gradient Descent(5/49): loss=5160032.649189321\n",
      "Gradient Descent(6/49): loss=492579816.3443184\n",
      "Gradient Descent(7/49): loss=57319976612.831764\n",
      "Gradient Descent(8/49): loss=7066583778440.515\n",
      "Gradient Descent(9/49): loss=883712969406098.5\n",
      "Gradient Descent(10/49): loss=1.1088629549825789e+17\n",
      "Gradient Descent(11/49): loss=1.392473705071922e+19\n",
      "Gradient Descent(12/49): loss=1.7489443639187176e+21\n",
      "Gradient Descent(13/49): loss=2.1967649860779597e+23\n",
      "Gradient Descent(14/49): loss=2.75927850833404e+25\n",
      "Gradient Descent(15/49): loss=3.4658398376371415e+27\n",
      "Gradient Descent(16/49): loss=4.3533308640824685e+29\n",
      "Gradient Descent(17/49): loss=5.468080723346973e+31\n",
      "Gradient Descent(18/49): loss=6.868282841511109e+33\n",
      "Gradient Descent(19/49): loss=8.62703239056868e+35\n",
      "Gradient Descent(20/49): loss=1.083614197380607e+38\n",
      "Gradient Descent(21/49): loss=1.3610934514234053e+40\n",
      "Gradient Descent(22/49): loss=1.7096263487010595e+42\n",
      "Gradient Descent(23/49): loss=2.1474074753466936e+44\n",
      "Gradient Descent(24/49): loss=2.6972904744362026e+46\n",
      "Gradient Descent(25/49): loss=3.387981082778756e+48\n",
      "Gradient Descent(26/49): loss=4.255535666646639e+50\n",
      "Gradient Descent(27/49): loss=5.345243485022603e+52\n",
      "Gradient Descent(28/49): loss=6.713990940814098e+54\n",
      "Gradient Descent(29/49): loss=8.43323124187755e+56\n",
      "Gradient Descent(30/49): loss=1.0592714498115754e+59\n",
      "Gradient Descent(31/49): loss=1.330517297822978e+61\n",
      "Gradient Descent(32/49): loss=1.6712206112238946e+63\n",
      "Gradient Descent(33/49): loss=2.0991672456641966e+65\n",
      "Gradient Descent(34/49): loss=2.636697450758654e+67\n",
      "Gradient Descent(35/49): loss=3.311872106044294e+69\n",
      "Gradient Descent(36/49): loss=4.1599375930061553e+71\n",
      "Gradient Descent(37/49): loss=5.225165774403942e+73\n",
      "Gradient Descent(38/49): loss=6.563165133992311e+75\n",
      "Gradient Descent(39/49): loss=8.243783725879283e+77\n",
      "Gradient Descent(40/49): loss=1.0354755477214784e+80\n",
      "Gradient Descent(41/49): loss=1.3006280193440758e+82\n",
      "Gradient Descent(42/49): loss=1.6336776357734853e+84\n",
      "Gradient Descent(43/49): loss=2.052010704007692e+86\n",
      "Gradient Descent(44/49): loss=2.5774656132625696e+88\n",
      "Gradient Descent(45/49): loss=3.2374728721326707e+90\n",
      "Gradient Descent(46/49): loss=4.0664870731400786e+92\n",
      "Gradient Descent(47/49): loss=5.107785538021222e+94\n",
      "Gradient Descent(48/49): loss=6.4157275390705065e+96\n",
      "Gradient Descent(49/49): loss=8.058592035470226e+98\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.645805942032131\n",
      "Gradient Descent(2/49): loss=64.10547884291638\n",
      "Gradient Descent(3/49): loss=1950.095069312014\n",
      "Gradient Descent(4/49): loss=71058.90215955443\n",
      "Gradient Descent(5/49): loss=3896080.526116581\n",
      "Gradient Descent(6/49): loss=345991697.8372892\n",
      "Gradient Descent(7/49): loss=39695664558.32546\n",
      "Gradient Descent(8/49): loss=4929984203457.602\n",
      "Gradient Descent(9/49): loss=624460646887980.1\n",
      "Gradient Descent(10/49): loss=7.946286727092768e+16\n",
      "Gradient Descent(11/49): loss=1.0122405008760928e+19\n",
      "Gradient Descent(12/49): loss=1.289759658519819e+21\n",
      "Gradient Descent(13/49): loss=1.643455972011377e+23\n",
      "Gradient Descent(14/49): loss=2.0941746904365083e+25\n",
      "Gradient Descent(15/49): loss=2.6685110809824327e+27\n",
      "Gradient Descent(16/49): loss=3.400363971115522e+29\n",
      "Gradient Descent(17/49): loss=4.332931947001499e+31\n",
      "Gradient Descent(18/49): loss=5.521261864407873e+33\n",
      "Gradient Descent(19/49): loss=7.035497717106625e+35\n",
      "Gradient Descent(20/49): loss=8.965020938073815e+37\n",
      "Gradient Descent(21/49): loss=1.1423726321274988e+40\n",
      "Gradient Descent(22/49): loss=1.45567449287091e+42\n",
      "Gradient Descent(23/49): loss=1.854901080128144e+44\n",
      "Gradient Descent(24/49): loss=2.3636177139385446e+46\n",
      "Gradient Descent(25/49): loss=3.011852630580505e+48\n",
      "Gradient Descent(26/49): loss=3.837869472225862e+50\n",
      "Gradient Descent(27/49): loss=4.89042589145712e+52\n",
      "Gradient Descent(28/49): loss=6.231651590267384e+54\n",
      "Gradient Descent(29/49): loss=7.940715676791765e+56\n",
      "Gradient Descent(30/49): loss=1.0118499814419294e+59\n",
      "Gradient Descent(31/49): loss=1.2893553007273373e+61\n",
      "Gradient Descent(32/49): loss=1.642967951775483e+63\n",
      "Gradient Descent(33/49): loss=2.093560781142795e+65\n",
      "Gradient Descent(34/49): loss=2.6677311262235298e+67\n",
      "Gradient Descent(35/49): loss=3.3993707877624264e+69\n",
      "Gradient Descent(36/49): loss=4.3316665758033897e+71\n",
      "Gradient Descent(37/49): loss=5.5196495161631373e+73\n",
      "Gradient Descent(38/49): loss=7.033443190541302e+75\n",
      "Gradient Descent(39/49): loss=8.962402951439423e+77\n",
      "Gradient Descent(40/49): loss=1.1420390339114744e+80\n",
      "Gradient Descent(41/49): loss=1.45524940358541e+82\n",
      "Gradient Descent(42/49): loss=1.8543594078237928e+84\n",
      "Gradient Descent(43/49): loss=2.3629274850843716e+86\n",
      "Gradient Descent(44/49): loss=3.0109731027383535e+88\n",
      "Gradient Descent(45/49): loss=3.8367487291257495e+90\n",
      "Gradient Descent(46/49): loss=4.8889977785122653e+92\n",
      "Gradient Descent(47/49): loss=6.229831809638479e+94\n",
      "Gradient Descent(48/49): loss=7.938396811502235e+96\n",
      "Gradient Descent(49/49): loss=1.0115544987806972e+99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.696222704264334\n",
      "Gradient Descent(2/49): loss=66.7723356603625\n",
      "Gradient Descent(3/49): loss=2149.37854333171\n",
      "Gradient Descent(4/49): loss=92052.44851468678\n",
      "Gradient Descent(5/49): loss=6498936.040631955\n",
      "Gradient Descent(6/49): loss=686134393.7057235\n",
      "Gradient Descent(7/49): loss=84769216076.03543\n",
      "Gradient Descent(8/49): loss=10920679193718.467\n",
      "Gradient Descent(9/49): loss=1420794617136402.8\n",
      "Gradient Descent(10/49): loss=1.8526116017170557e+17\n",
      "Gradient Descent(11/49): loss=2.4168896480268403e+19\n",
      "Gradient Descent(12/49): loss=3.1533974931666806e+21\n",
      "Gradient Descent(13/49): loss=4.114449720004829e+23\n",
      "Gradient Descent(14/49): loss=5.3684302181378485e+25\n",
      "Gradient Descent(15/49): loss=7.004601470914506e+27\n",
      "Gradient Descent(16/49): loss=9.139441918578778e+29\n",
      "Gradient Descent(17/49): loss=1.1924933125234974e+32\n",
      "Gradient Descent(18/49): loss=1.555937807015476e+34\n",
      "Gradient Descent(19/49): loss=2.0301518188894123e+36\n",
      "Gradient Descent(20/49): loss=2.6488953429094025e+38\n",
      "Gradient Descent(21/49): loss=3.456217645201837e+40\n",
      "Gradient Descent(22/49): loss=4.5095931945514814e+42\n",
      "Gradient Descent(23/49): loss=5.884013354527568e+44\n",
      "Gradient Descent(24/49): loss=7.67732512949415e+46\n",
      "Gradient Descent(25/49): loss=1.0017197037568927e+49\n",
      "Gradient Descent(26/49): loss=1.3070208021279595e+51\n",
      "Gradient Descent(27/49): loss=1.7053706448903936e+53\n",
      "Gradient Descent(28/49): loss=2.2251283466329212e+55\n",
      "Gradient Descent(29/49): loss=2.9032962270249282e+57\n",
      "Gradient Descent(30/49): loss=3.7881540606915066e+59\n",
      "Gradient Descent(31/49): loss=4.942696185789631e+61\n",
      "Gradient Descent(32/49): loss=6.4491161641296654e+63\n",
      "Gradient Descent(33/49): loss=8.414658262430797e+65\n",
      "Gradient Descent(34/49): loss=1.0979252330315372e+68\n",
      "Gradient Descent(35/49): loss=1.432547561330345e+70\n",
      "Gradient Descent(36/49): loss=1.869155069700958e+72\n",
      "Gradient Descent(37/49): loss=2.438830492541766e+74\n",
      "Gradient Descent(38/49): loss=3.182129865930828e+76\n",
      "Gradient Descent(39/49): loss=4.151969771829279e+78\n",
      "Gradient Descent(40/49): loss=5.417394547831241e+80\n",
      "Gradient Descent(41/49): loss=7.068491655694608e+82\n",
      "Gradient Descent(42/49): loss=9.222805141010935e+84\n",
      "Gradient Descent(43/49): loss=1.2033703767695802e+87\n",
      "Gradient Descent(44/49): loss=1.5701299567171075e+89\n",
      "Gradient Descent(45/49): loss=2.0486694109909215e+91\n",
      "Gradient Descent(46/49): loss=2.67305667124855e+93\n",
      "Gradient Descent(47/49): loss=3.4877427902094885e+95\n",
      "Gradient Descent(48/49): loss=4.550726477855263e+97\n",
      "Gradient Descent(49/49): loss=5.937683115390952e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.718082001476545\n",
      "Gradient Descent(2/49): loss=68.14071765470466\n",
      "Gradient Descent(3/49): loss=2186.607724726954\n",
      "Gradient Descent(4/49): loss=89644.02549257706\n",
      "Gradient Descent(5/49): loss=5817454.41141116\n",
      "Gradient Descent(6/49): loss=573404014.338605\n",
      "Gradient Descent(7/49): loss=67891609494.40191\n",
      "Gradient Descent(8/49): loss=8473651480942.573\n",
      "Gradient Descent(9/49): loss=1071476643770495.0\n",
      "Gradient Descent(10/49): loss=1.3590534288491342e+17\n",
      "Gradient Descent(11/49): loss=1.7250647114412358e+19\n",
      "Gradient Descent(12/49): loss=2.190019846294368e+21\n",
      "Gradient Descent(13/49): loss=2.780404533166606e+23\n",
      "Gradient Descent(14/49): loss=3.5299777666678236e+25\n",
      "Gradient Descent(15/49): loss=4.481639290409533e+27\n",
      "Gradient Descent(16/49): loss=5.689866142813747e+29\n",
      "Gradient Descent(17/49): loss=7.223825589876775e+31\n",
      "Gradient Descent(18/49): loss=9.171333086304815e+33\n",
      "Gradient Descent(19/49): loss=1.164387900621946e+36\n",
      "Gradient Descent(20/49): loss=1.4783011067571557e+38\n",
      "Gradient Descent(21/49): loss=1.8768437578713927e+40\n",
      "Gradient Descent(22/49): loss=2.3828315325310727e+42\n",
      "Gradient Descent(23/49): loss=3.025231103400795e+44\n",
      "Gradient Descent(24/49): loss=3.8408184145960507e+46\n",
      "Gradient Descent(25/49): loss=4.87628402251081e+48\n",
      "Gradient Descent(26/49): loss=6.1909060261316255e+50\n",
      "Gradient Descent(27/49): loss=7.859943606127546e+52\n",
      "Gradient Descent(28/49): loss=9.97894544526135e+54\n",
      "Gradient Descent(29/49): loss=1.2669219677590236e+57\n",
      "Gradient Descent(30/49): loss=1.6084778508861754e+59\n",
      "Gradient Descent(31/49): loss=2.0421155072144733e+61\n",
      "Gradient Descent(32/49): loss=2.5926597264041988e+63\n",
      "Gradient Descent(33/49): loss=3.2916279383663634e+65\n",
      "Gradient Descent(34/49): loss=4.179034515902629e+67\n",
      "Gradient Descent(35/49): loss=5.305681508394662e+69\n",
      "Gradient Descent(36/49): loss=6.736066945941624e+71\n",
      "Gradient Descent(37/49): loss=8.552077207879025e+73\n",
      "Gradient Descent(38/49): loss=1.0857674835549015e+76\n",
      "Gradient Descent(39/49): loss=1.378485015615909e+78\n",
      "Gradient Descent(40/49): loss=1.7501177434933668e+80\n",
      "Gradient Descent(41/49): loss=2.2219408128435964e+82\n",
      "Gradient Descent(42/49): loss=2.8209650431435404e+84\n",
      "Gradient Descent(43/49): loss=3.581483236924654e+86\n",
      "Gradient Descent(44/49): loss=4.547033366311596e+88\n",
      "Gradient Descent(45/49): loss=5.772891024921835e+90\n",
      "Gradient Descent(46/49): loss=7.329233832444165e+92\n",
      "Gradient Descent(47/49): loss=9.305158947006489e+94\n",
      "Gradient Descent(48/49): loss=1.1813783678965758e+97\n",
      "Gradient Descent(49/49): loss=1.4998721204896534e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7674027171989746\n",
      "Gradient Descent(2/49): loss=69.21794286757407\n",
      "Gradient Descent(3/49): loss=2193.836656332553\n",
      "Gradient Descent(4/49): loss=87404.05105521985\n",
      "Gradient Descent(5/49): loss=5459238.441832902\n",
      "Gradient Descent(6/49): loss=526390573.4992402\n",
      "Gradient Descent(7/49): loss=61905982515.27752\n",
      "Gradient Descent(8/49): loss=7715246237736.295\n",
      "Gradient Descent(9/49): loss=975444398548177.4\n",
      "Gradient Descent(10/49): loss=1.2374582651880144e+17\n",
      "Gradient Descent(11/49): loss=1.5711000865079255e+19\n",
      "Gradient Descent(12/49): loss=1.9950681295252918e+21\n",
      "Gradient Descent(13/49): loss=2.533555417659181e+23\n",
      "Gradient Descent(14/49): loss=3.2174178743376582e+25\n",
      "Gradient Descent(15/49): loss=4.0858795042973514e+27\n",
      "Gradient Descent(16/49): loss=5.1887635191715364e+29\n",
      "Gradient Descent(17/49): loss=6.589345152795381e+31\n",
      "Gradient Descent(18/49): loss=8.367980286518441e+33\n",
      "Gradient Descent(19/49): loss=1.0626715241007855e+36\n",
      "Gradient Descent(20/49): loss=1.349514137586168e+38\n",
      "Gradient Descent(21/49): loss=1.7137830147244164e+40\n",
      "Gradient Descent(22/49): loss=2.1763775124808353e+42\n",
      "Gradient Descent(23/49): loss=2.7638382667083262e+44\n",
      "Gradient Descent(24/49): loss=3.509869919511569e+46\n",
      "Gradient Descent(25/49): loss=4.457274870348252e+48\n",
      "Gradient Descent(26/49): loss=5.66040899675498e+50\n",
      "Gradient Descent(27/49): loss=7.188300237818628e+52\n",
      "Gradient Descent(28/49): loss=9.128608964236918e+54\n",
      "Gradient Descent(29/49): loss=1.1592657355006967e+57\n",
      "Gradient Descent(30/49): loss=1.4721816333364182e+59\n",
      "Gradient Descent(31/49): loss=1.869561650243155e+61\n",
      "Gradient Descent(32/49): loss=2.3742048432832704e+63\n",
      "Gradient Descent(33/49): loss=3.0150643265155093e+65\n",
      "Gradient Descent(34/49): loss=3.828908410638754e+67\n",
      "Gradient Descent(35/49): loss=4.8624301273211286e+69\n",
      "Gradient Descent(36/49): loss=6.174926168875423e+71\n",
      "Gradient Descent(37/49): loss=7.841698943254173e+73\n",
      "Gradient Descent(38/49): loss=9.958376931951865e+75\n",
      "Gradient Descent(39/49): loss=1.264640122458944e+78\n",
      "Gradient Descent(40/49): loss=1.6059993011526407e+80\n",
      "Gradient Descent(41/49): loss=2.039500178349383e+82\n",
      "Gradient Descent(42/49): loss=2.590014188986126e+84\n",
      "Gradient Descent(43/49): loss=3.289126213550372e+86\n",
      "Gradient Descent(44/49): loss=4.176946711206663e+88\n",
      "Gradient Descent(45/49): loss=5.30441299466807e+90\n",
      "Gradient Descent(46/49): loss=6.73621167885935e+92\n",
      "Gradient Descent(47/49): loss=8.554489974293634e+94\n",
      "Gradient Descent(48/49): loss=1.0863568754817312e+97\n",
      "Gradient Descent(49/49): loss=1.3795927804613078e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.673438262049974\n",
      "Gradient Descent(2/49): loss=65.64734406726714\n",
      "Gradient Descent(3/49): loss=2020.7756175738452\n",
      "Gradient Descent(4/49): loss=74458.58694330245\n",
      "Gradient Descent(5/49): loss=4123524.042161983\n",
      "Gradient Descent(6/49): loss=369800039.760806\n",
      "Gradient Descent(7/49): loss=42872294903.9272\n",
      "Gradient Descent(8/49): loss=5382331705436.874\n",
      "Gradient Descent(9/49): loss=689244013324269.5\n",
      "Gradient Descent(10/49): loss=8.867260304461842e+16\n",
      "Gradient Descent(11/49): loss=1.142010223694362e+19\n",
      "Gradient Descent(12/49): loss=1.4711504640109277e+21\n",
      "Gradient Descent(13/49): loss=1.8952593404887085e+23\n",
      "Gradient Descent(14/49): loss=2.441663479972792e+25\n",
      "Gradient Descent(15/49): loss=3.1456055039460897e+27\n",
      "Gradient Descent(16/49): loss=4.052499778355795e+29\n",
      "Gradient Descent(17/49): loss=5.220857158772549e+31\n",
      "Gradient Descent(18/49): loss=6.726058467095546e+33\n",
      "Gradient Descent(19/49): loss=8.665217510865756e+35\n",
      "Gradient Descent(20/49): loss=1.1163446618735437e+38\n",
      "Gradient Descent(21/49): loss=1.4381928705988681e+40\n",
      "Gradient Descent(22/49): loss=1.8528316602283898e+42\n",
      "Gradient Descent(23/49): loss=2.387013057430272e+44\n",
      "Gradient Descent(24/49): loss=3.075201843037325e+46\n",
      "Gradient Descent(25/49): loss=3.961799180777152e+48\n",
      "Gradient Descent(26/49): loss=5.10400733023375e+50\n",
      "Gradient Descent(27/49): loss=6.575520272072712e+52\n",
      "Gradient Descent(28/49): loss=8.471278360499297e+54\n",
      "Gradient Descent(29/49): loss=1.0913593767758899e+57\n",
      "Gradient Descent(30/49): loss=1.4060041927443829e+59\n",
      "Gradient Descent(31/49): loss=1.8113628123623466e+61\n",
      "Gradient Descent(32/49): loss=2.333588516265336e+63\n",
      "Gradient Descent(33/49): loss=3.006374717466569e+65\n",
      "Gradient Descent(34/49): loss=3.873128822337248e+67\n",
      "Gradient Descent(35/49): loss=4.989772827473989e+69\n",
      "Gradient Descent(36/49): loss=6.4283513438038945e+71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=8.281679833569934e+73\n",
      "Gradient Descent(38/49): loss=1.0669332959198768e+76\n",
      "Gradient Descent(39/49): loss=1.3745359405565752e+78\n",
      "Gradient Descent(40/49): loss=1.770822092727753e+80\n",
      "Gradient Descent(41/49): loss=2.281359687708809e+82\n",
      "Gradient Descent(42/49): loss=2.9390880349170675e+84\n",
      "Gradient Descent(43/49): loss=3.786443024978671e+86\n",
      "Gradient Descent(44/49): loss=4.8780950454975415e+88\n",
      "Gradient Descent(45/49): loss=6.284476252760035e+90\n",
      "Gradient Descent(46/49): loss=8.09632477496684e+92\n",
      "Gradient Descent(47/49): loss=1.0430539033853942e+95\n",
      "Gradient Descent(48/49): loss=1.3437719898927259e+97\n",
      "Gradient Descent(49/49): loss=1.731188728558974e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7243002059254766\n",
      "Gradient Descent(2/49): loss=68.36775540038732\n",
      "Gradient Descent(3/49): loss=2226.460708828035\n",
      "Gradient Descent(4/49): loss=96376.34148852242\n",
      "Gradient Descent(5/49): loss=6871179.082185983\n",
      "Gradient Descent(6/49): loss=732801672.392037\n",
      "Gradient Descent(7/49): loss=91500626745.94887\n",
      "Gradient Descent(8/49): loss=11916093295566.441\n",
      "Gradient Descent(9/49): loss=1567256685964581.5\n",
      "Gradient Descent(10/49): loss=2.0659736105984512e+17\n",
      "Gradient Descent(11/49): loss=2.7247745107704308e+19\n",
      "Gradient Descent(12/49): loss=3.5940678649156846e+21\n",
      "Gradient Descent(13/49): loss=4.7408174942801974e+23\n",
      "Gradient Descent(14/49): loss=6.253494027325104e+25\n",
      "Gradient Descent(15/49): loss=8.248838764685609e+27\n",
      "Gradient Descent(16/49): loss=1.0880854912235687e+30\n",
      "Gradient Descent(17/49): loss=1.4352688292472121e+32\n",
      "Gradient Descent(18/49): loss=1.8932305044024146e+34\n",
      "Gradient Descent(19/49): loss=2.497317354069474e+36\n",
      "Gradient Descent(20/49): loss=3.2941545983098717e+38\n",
      "Gradient Descent(21/49): loss=4.345244508779593e+40\n",
      "Gradient Descent(22/49): loss=5.731713335951617e+42\n",
      "Gradient Descent(23/49): loss=7.560572874423246e+44\n",
      "Gradient Descent(24/49): loss=9.972979951916735e+46\n",
      "Gradient Descent(25/49): loss=1.3155131333743756e+49\n",
      "Gradient Descent(26/49): loss=1.7352634943861734e+51\n",
      "Gradient Descent(27/49): loss=2.288946661616081e+53\n",
      "Gradient Descent(28/49): loss=3.0192975514515533e+55\n",
      "Gradient Descent(29/49): loss=3.982686821441627e+57\n",
      "Gradient Descent(30/49): loss=5.25347172558705e+59\n",
      "Gradient Descent(31/49): loss=6.929735228729886e+61\n",
      "Gradient Descent(32/49): loss=9.140856341990698e+63\n",
      "Gradient Descent(33/49): loss=1.2057495980293926e+66\n",
      "Gradient Descent(34/49): loss=1.5904769080217482e+68\n",
      "Gradient Descent(35/49): loss=2.0979619641463356e+70\n",
      "Gradient Descent(36/49): loss=2.7673739749414743e+72\n",
      "Gradient Descent(37/49): loss=3.6503801537219634e+74\n",
      "Gradient Descent(38/49): loss=4.815133548030463e+76\n",
      "Gradient Descent(39/49): loss=6.351533294889279e+78\n",
      "Gradient Descent(40/49): loss=8.378163304024728e+80\n",
      "Gradient Descent(41/49): loss=1.1051444917306718e+83\n",
      "Gradient Descent(42/49): loss=1.457770997392594e+85\n",
      "Gradient Descent(43/49): loss=1.922912611645055e+87\n",
      "Gradient Descent(44/49): loss=2.5364703500325096e+89\n",
      "Gradient Descent(45/49): loss=3.3458004267235315e+91\n",
      "Gradient Descent(46/49): loss=4.41336934820467e+93\n",
      "Gradient Descent(47/49): loss=5.821575264352137e+95\n",
      "Gradient Descent(48/49): loss=7.679107703120963e+97\n",
      "Gradient Descent(49/49): loss=1.0129336552121993e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.746409671879042\n",
      "Gradient Descent(2/49): loss=69.76784804233013\n",
      "Gradient Descent(3/49): loss=2265.079799386301\n",
      "Gradient Descent(4/49): loss=93870.6344415614\n",
      "Gradient Descent(5/49): loss=6152196.201701116\n",
      "Gradient Descent(6/49): loss=612510390.6653476\n",
      "Gradient Descent(7/49): loss=73291595379.97427\n",
      "Gradient Descent(8/49): loss=9247012876578.264\n",
      "Gradient Descent(9/49): loss=1182061293620610.8\n",
      "Gradient Descent(10/49): loss=1.5157571013262733e+17\n",
      "Gradient Descent(11/49): loss=1.945076885473233e+19\n",
      "Gradient Descent(12/49): loss=2.496424242486096e+21\n",
      "Gradient Descent(13/49): loss=3.204184081300717e+23\n",
      "Gradient Descent(14/49): loss=4.112639232499836e+25\n",
      "Gradient Descent(15/49): loss=5.278672615684282e+27\n",
      "Gradient Descent(16/49): loss=6.775308362768951e+29\n",
      "Gradient Descent(17/49): loss=8.696278839730227e+31\n",
      "Gradient Descent(18/49): loss=1.1161893120574049e+34\n",
      "Gradient Descent(19/49): loss=1.4326571302454075e+36\n",
      "Gradient Descent(20/49): loss=1.8388515584842875e+38\n",
      "Gradient Descent(21/49): loss=2.360212352270948e+40\n",
      "Gradient Descent(22/49): loss=3.029392080391964e+42\n",
      "Gradient Descent(23/49): loss=3.888301138706928e+44\n",
      "Gradient Descent(24/49): loss=4.990732577403661e+46\n",
      "Gradient Descent(25/49): loss=6.405731133126357e+48\n",
      "Gradient Descent(26/49): loss=8.221917466726176e+50\n",
      "Gradient Descent(27/49): loss=1.0553038431488038e+53\n",
      "Gradient Descent(28/49): loss=1.3545090982384757e+55\n",
      "Gradient Descent(29/49): loss=1.7385465893277242e+57\n",
      "Gradient Descent(30/49): loss=2.23146839485529e+59\n",
      "Gradient Descent(31/49): loss=2.8641459641086823e+61\n",
      "Gradient Descent(32/49): loss=3.6762035808498216e+63\n",
      "Gradient Descent(33/49): loss=4.718500012641149e+65\n",
      "Gradient Descent(34/49): loss=6.056313770345603e+67\n",
      "Gradient Descent(35/49): loss=7.773431469028997e+69\n",
      "Gradient Descent(36/49): loss=9.977395342289307e+71\n",
      "Gradient Descent(37/49): loss=1.2806238559246143e+74\n",
      "Gradient Descent(38/49): loss=1.6437130173764926e+76\n",
      "Gradient Descent(39/49): loss=2.1097471134818423e+78\n",
      "Gradient Descent(40/49): loss=2.7079136295636676e+80\n",
      "Gradient Descent(41/49): loss=3.475675439164287e+82\n",
      "Gradient Descent(42/49): loss=4.461117085317194e+84\n",
      "Gradient Descent(43/49): loss=5.725956291734217e+86\n",
      "Gradient Descent(44/49): loss=7.349409313366882e+88\n",
      "Gradient Descent(45/49): loss=9.433152211339201e+90\n",
      "Gradient Descent(46/49): loss=1.2107688774450588e+93\n",
      "Gradient Descent(47/49): loss=1.554052390702841e+95\n",
      "Gradient Descent(48/49): loss=1.994665437837703e+97\n",
      "Gradient Descent(49/49): loss=2.560203396427876e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.796344048878212\n",
      "Gradient Descent(2/49): loss=70.87179502443381\n",
      "Gradient Descent(3/49): loss=2272.658372924094\n",
      "Gradient Descent(4/49): loss=91534.27263344039\n",
      "Gradient Descent(5/49): loss=5773994.692450522\n",
      "Gradient Descent(6/49): loss=562318149.7995926\n",
      "Gradient Descent(7/49): loss=66830836437.25442\n",
      "Gradient Descent(8/49): loss=8419416674754.513\n",
      "Gradient Descent(9/49): loss=1076117766445874.0\n",
      "Gradient Descent(10/49): loss=1.3801403440341282e+17\n",
      "Gradient Descent(11/49): loss=1.7714737024766468e+19\n",
      "Gradient Descent(12/49): loss=2.27419382591689e+21\n",
      "Gradient Descent(13/49): loss=2.9197067096103114e+23\n",
      "Gradient Descent(14/49): loss=3.7484818467300967e+25\n",
      "Gradient Descent(15/49): loss=4.8125209246234384e+27\n",
      "Gradient Descent(16/49): loss=6.178600158536676e+29\n",
      "Gradient Descent(17/49): loss=7.93245483483533e+31\n",
      "Gradient Descent(18/49): loss=1.0184158223493218e+34\n",
      "Gradient Descent(19/49): loss=1.3075029308669168e+36\n",
      "Gradient Descent(20/49): loss=1.6786501933035662e+38\n",
      "Gradient Descent(21/49): loss=2.15515117101615e+40\n",
      "Gradient Descent(22/49): loss=2.766911527415964e+42\n",
      "Gradient Descent(23/49): loss=3.5523259359567102e+44\n",
      "Gradient Descent(24/49): loss=4.5606877669549297e+46\n",
      "Gradient Descent(25/49): loss=5.85528278729714e+48\n",
      "Gradient Descent(26/49): loss=7.517361036560896e+50\n",
      "Gradient Descent(27/49): loss=9.65123615832983e+52\n",
      "Gradient Descent(28/49): loss=1.2390832225675352e+55\n",
      "Gradient Descent(29/49): loss=1.5908088945924006e+57\n",
      "Gradient Descent(30/49): loss=2.042375276351885e+59\n",
      "Gradient Descent(31/49): loss=2.622123112105371e+61\n",
      "Gradient Descent(32/49): loss=3.3664379385350596e+63\n",
      "Gradient Descent(33/49): loss=4.322033676332171e+65\n",
      "Gradient Descent(34/49): loss=5.548884441184128e+67\n",
      "Gradient Descent(35/49): loss=7.123988577466395e+69\n",
      "Gradient Descent(36/49): loss=9.146201149044175e+71\n",
      "Gradient Descent(37/49): loss=1.1742438178996533e+74\n",
      "Gradient Descent(38/49): loss=1.507564202236723e+76\n",
      "Gradient Descent(39/49): loss=1.9355007786465155e+78\n",
      "Gradient Descent(40/49): loss=2.4849112618774727e+80\n",
      "Gradient Descent(41/49): loss=3.190277186932133e+82\n",
      "Gradient Descent(42/49): loss=4.09586800366049e+84\n",
      "Gradient Descent(43/49): loss=5.258519470385768e+86\n",
      "Gradient Descent(44/49): loss=6.751200721242462e+88\n",
      "Gradient Descent(45/49): loss=8.667593879834154e+90\n",
      "Gradient Descent(46/49): loss=1.1127973640207702e+93\n",
      "Gradient Descent(47/49): loss=1.4286755823350764e+95\n",
      "Gradient Descent(48/49): loss=1.8342188663940356e+97\n",
      "Gradient Descent(49/49): loss=2.3548795061906307e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.701239375196161\n",
      "Gradient Descent(2/49): loss=67.21703312224707\n",
      "Gradient Descent(3/49): loss=2093.5798436507216\n",
      "Gradient Descent(4/49): loss=77999.94829363619\n",
      "Gradient Descent(5/49): loss=4362870.099305121\n",
      "Gradient Descent(6/49): loss=395103004.9084048\n",
      "Gradient Descent(7/49): loss=46283706213.62569\n",
      "Gradient Descent(8/49): loss=5873372434235.982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=760338866684732.9\n",
      "Gradient Descent(10/49): loss=9.88905952103653e+16\n",
      "Gradient Descent(11/49): loss=1.2875693052983527e+19\n",
      "Gradient Descent(12/49): loss=1.6768482801722138e+21\n",
      "Gradient Descent(13/49): loss=2.183944617582524e+23\n",
      "Gradient Descent(14/49): loss=2.8444292958709356e+25\n",
      "Gradient Descent(15/49): loss=3.7046737412534095e+27\n",
      "Gradient Descent(16/49): loss=4.825086352970624e+29\n",
      "Gradient Descent(17/49): loss=6.284348803641019e+31\n",
      "Gradient Descent(18/49): loss=8.184939797999035e+33\n",
      "Gradient Descent(19/49): loss=1.0660331268747167e+36\n",
      "Gradient Descent(20/49): loss=1.3884361495812968e+38\n",
      "Gradient Descent(21/49): loss=1.808344312866381e+40\n",
      "Gradient Descent(22/49): loss=2.3552463360971638e+42\n",
      "Gradient Descent(23/49): loss=3.0675492849228595e+44\n",
      "Gradient Descent(24/49): loss=3.995275768509942e+46\n",
      "Gradient Descent(25/49): loss=5.203576856912181e+48\n",
      "Gradient Descent(26/49): loss=6.777307418731258e+50\n",
      "Gradient Descent(27/49): loss=8.826985189423686e+52\n",
      "Gradient Descent(28/49): loss=1.1496552055313522e+55\n",
      "Gradient Descent(29/49): loss=1.4973482601839795e+57\n",
      "Gradient Descent(30/49): loss=1.9501949814942602e+59\n",
      "Gradient Descent(31/49): loss=2.539997251793706e+61\n",
      "Gradient Descent(32/49): loss=3.308174874994503e+63\n",
      "Gradient Descent(33/49): loss=4.308674348295642e+65\n",
      "Gradient Descent(34/49): loss=5.611757340878735e+67\n",
      "Gradient Descent(35/49): loss=7.308934931544013e+69\n",
      "Gradient Descent(36/49): loss=9.519394119985274e+71\n",
      "Gradient Descent(37/49): loss=1.239836792369211e+74\n",
      "Gradient Descent(38/49): loss=1.6148036863870908e+76\n",
      "Gradient Descent(39/49): loss=2.1031727414593353e+78\n",
      "Gradient Descent(40/49): loss=2.7392404523885695e+80\n",
      "Gradient Descent(41/49): loss=3.567675687350011e+82\n",
      "Gradient Descent(42/49): loss=4.646656630311498e+84\n",
      "Gradient Descent(43/49): loss=6.051956436672338e+86\n",
      "Gradient Descent(44/49): loss=7.882264523799096e+88\n",
      "Gradient Descent(45/49): loss=1.0266117192559154e+91\n",
      "Gradient Descent(46/49): loss=1.3370924293792199e+93\n",
      "Gradient Descent(47/49): loss=1.741472585174648e+95\n",
      "Gradient Descent(48/49): loss=2.268150427209362e+97\n",
      "Gradient Descent(49/49): loss=2.954112745871429e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7525481196532673\n",
      "Gradient Descent(2/49): loss=69.99177212367833\n",
      "Gradient Descent(3/49): loss=2305.835355592514\n",
      "Gradient Descent(4/49): loss=100877.23565720828\n",
      "Gradient Descent(5/49): loss=7262530.5703504905\n",
      "Gradient Descent(6/49): loss=782363740.5194345\n",
      "Gradient Descent(7/49): loss=98725675971.65651\n",
      "Gradient Descent(8/49): loss=12996092843081.787\n",
      "Gradient Descent(9/49): loss=1727897598150576.2\n",
      "Gradient Descent(10/49): loss=2.302547233486688e+17\n",
      "Gradient Descent(11/49): loss=3.0698845434224484e+19\n",
      "Gradient Descent(12/49): loss=4.093416121536586e+21\n",
      "Gradient Descent(13/49): loss=5.4583467296021383e+23\n",
      "Gradient Descent(14/49): loss=7.278450138597173e+25\n",
      "Gradient Descent(15/49): loss=9.705485881419824e+27\n",
      "Gradient Descent(16/49): loss=1.2941832777921295e+30\n",
      "Gradient Descent(17/49): loss=1.7257358270526767e+32\n",
      "Gradient Descent(18/49): loss=2.301191988504061e+34\n",
      "Gradient Descent(19/49): loss=3.0685372021048314e+36\n",
      "Gradient Descent(20/49): loss=4.091757930286012e+38\n",
      "Gradient Descent(21/49): loss=5.4561772793671075e+40\n",
      "Gradient Descent(22/49): loss=7.275569819202036e+42\n",
      "Gradient Descent(23/49): loss=9.701648880569812e+44\n",
      "Gradient Descent(24/49): loss=1.2936717444953902e+47\n",
      "Gradient Descent(25/49): loss=1.725053754376077e+49\n",
      "Gradient Descent(26/49): loss=2.3002824852203935e+51\n",
      "Gradient Descent(27/49): loss=3.067324423015269e+53\n",
      "Gradient Descent(28/49): loss=4.090140744224436e+55\n",
      "Gradient Descent(29/49): loss=5.454020833935744e+57\n",
      "Gradient Descent(30/49): loss=7.27269429518019e+59\n",
      "Gradient Descent(31/49): loss=9.697814497158613e+61\n",
      "Gradient Descent(32/49): loss=1.2931604465160477e+64\n",
      "Gradient Descent(33/49): loss=1.724371961253258e+66\n",
      "Gradient Descent(34/49): loss=2.299373344403869e+68\n",
      "Gradient Descent(35/49): loss=3.0661121241569836e+70\n",
      "Gradient Descent(36/49): loss=4.088524197596041e+72\n",
      "Gradient Descent(37/49): loss=5.45186524087874e+74\n",
      "Gradient Descent(38/49): loss=7.2698199076768e+76\n",
      "Gradient Descent(39/49): loss=9.69398162921876e+78\n",
      "Gradient Descent(40/49): loss=1.292649350617274e+81\n",
      "Gradient Descent(41/49): loss=1.7236904375956476e+83\n",
      "Gradient Descent(42/49): loss=2.298464562907097e+85\n",
      "Gradient Descent(43/49): loss=3.064900304435683e+87\n",
      "Gradient Descent(44/49): loss=4.086908289875493e+89\n",
      "Gradient Descent(45/49): loss=5.4497104997770396e+91\n",
      "Gradient Descent(46/49): loss=7.266946656217861e+93\n",
      "Gradient Descent(47/49): loss=9.690150276143353e+95\n",
      "Gradient Descent(48/49): loss=1.2921384567189432e+98\n",
      "Gradient Descent(49/49): loss=1.7230091832967062e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7749091575784552\n",
      "Gradient Descent(2/49): loss=71.4241215009407\n",
      "Gradient Descent(3/49): loss=2345.886995523468\n",
      "Gradient Descent(4/49): loss=98270.87993263635\n",
      "Gradient Descent(5/49): loss=6504201.304444021\n",
      "Gradient Descent(6/49): loss=654049382.2961038\n",
      "Gradient Descent(7/49): loss=79088239817.88106\n",
      "Gradient Descent(8/49): loss=10086175458646.035\n",
      "Gradient Descent(9/49): loss=1303364273572777.0\n",
      "Gradient Descent(10/49): loss=1.6895284666970054e+17\n",
      "Gradient Descent(11/49): loss=2.1917209552169697e+19\n",
      "Gradient Descent(12/49): loss=2.843676048844821e+21\n",
      "Gradient Descent(13/49): loss=3.689713110370844e+23\n",
      "Gradient Descent(14/49): loss=4.78750467371692e+25\n",
      "Gradient Descent(15/49): loss=6.211933411216587e+27\n",
      "Gradient Descent(16/49): loss=8.060177384781885e+29\n",
      "Gradient Descent(17/49): loss=1.0458332880844783e+32\n",
      "Gradient Descent(18/49): loss=1.3570015220950496e+34\n",
      "Gradient Descent(19/49): loss=1.7607520858058747e+36\n",
      "Gradient Descent(20/49): loss=2.284631123914955e+38\n",
      "Gradient Descent(21/49): loss=2.9643806282416647e+40\n",
      "Gradient Descent(22/49): loss=3.846376956819704e+42\n",
      "Gradient Descent(23/49): loss=4.9907948909485995e+44\n",
      "Gradient Descent(24/49): loss=6.475713099173961e+46\n",
      "Gradient Descent(25/49): loss=8.402441105907601e+48\n",
      "Gradient Descent(26/49): loss=1.0902431200553272e+51\n",
      "Gradient Descent(27/49): loss=1.414624685667032e+53\n",
      "Gradient Descent(28/49): loss=1.835519953748499e+55\n",
      "Gradient Descent(29/49): loss=2.3816447816477467e+57\n",
      "Gradient Descent(30/49): loss=3.0902588960506935e+59\n",
      "Gradient Descent(31/49): loss=4.009707962416388e+61\n",
      "Gradient Descent(32/49): loss=5.2027219998987365e+63\n",
      "Gradient Descent(33/49): loss=6.75069517828876e+65\n",
      "Gradient Descent(34/49): loss=8.759238988948002e+67\n",
      "Gradient Descent(35/49): loss=1.1365387658483495e+70\n",
      "Gradient Descent(36/49): loss=1.4746947399265204e+72\n",
      "Gradient Descent(37/49): loss=1.9134627355571225e+74\n",
      "Gradient Descent(38/49): loss=2.4827779887167133e+76\n",
      "Gradient Descent(39/49): loss=3.2214824081543666e+78\n",
      "Gradient Descent(40/49): loss=4.1799745902419067e+80\n",
      "Gradient Descent(41/49): loss=5.423648296461788e+82\n",
      "Gradient Descent(42/49): loss=7.037353985927172e+84\n",
      "Gradient Descent(43/49): loss=9.131187793934266e+86\n",
      "Gradient Descent(44/49): loss=1.1848002913428935e+89\n",
      "Gradient Descent(45/49): loss=1.5373155848340688e+91\n",
      "Gradient Descent(46/49): loss=1.994715248335281e+93\n",
      "Gradient Descent(47/49): loss=2.5882056756554878e+95\n",
      "Gradient Descent(48/49): loss=3.3582781427505017e+97\n",
      "Gradient Descent(49/49): loss=4.3574713517384594e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8254608540392088\n",
      "Gradient Descent(2/49): loss=72.55528306684843\n",
      "Gradient Descent(3/49): loss=2353.8281584123088\n",
      "Gradient Descent(4/49): loss=95834.54056224636\n",
      "Gradient Descent(5/49): loss=6105016.882933409\n",
      "Gradient Descent(6/49): loss=600482436.7934841\n",
      "Gradient Descent(7/49): loss=72117524144.73744\n",
      "Gradient Descent(8/49): loss=9183503684517.164\n",
      "Gradient Descent(9/49): loss=1186548829936452.0\n",
      "Gradient Descent(10/49): loss=1.5383627952236406e+17\n",
      "Gradient Descent(11/49): loss=1.9961017620085498e+19\n",
      "Gradient Descent(12/49): loss=2.5905299390116135e+21\n",
      "Gradient Descent(13/49): loss=3.3621239854296786e+23\n",
      "Gradient Descent(14/49): loss=4.363583732330616e+25\n",
      "Gradient Descent(15/49): loss=5.663357142539783e+27\n",
      "Gradient Descent(16/49): loss=7.350296031301516e+29\n",
      "Gradient Descent(17/49): loss=9.539723082796411e+31\n",
      "Gradient Descent(18/49): loss=1.2381313474707547e+34\n",
      "Gradient Descent(19/49): loss=1.6069326450067626e+36\n",
      "Gradient Descent(20/49): loss=2.0855885243515584e+38\n",
      "Gradient Descent(21/49): loss=2.706821289697223e+40\n",
      "Gradient Descent(22/49): loss=3.5131002158291104e+42\n",
      "Gradient Descent(23/49): loss=4.559544870471864e+44\n",
      "Gradient Descent(24/49): loss=5.917693247769708e+46\n",
      "Gradient Descent(25/49): loss=7.680392313173207e+48\n",
      "Gradient Descent(26/49): loss=9.968145291494876e+50\n",
      "Gradient Descent(27/49): loss=1.29373496171452e+53\n",
      "Gradient Descent(28/49): loss=1.679098871673316e+55\n",
      "Gradient Descent(29/49): loss=2.1792508545323034e+57\n",
      "Gradient Descent(30/49): loss=2.82838275166425e+59\n",
      "Gradient Descent(31/49): loss=3.670871103835666e+61\n",
      "Gradient Descent(32/49): loss=4.7643108603483214e+63\n",
      "Gradient Descent(33/49): loss=6.183452736957991e+65\n",
      "Gradient Descent(34/49): loss=8.025313391787841e+67\n",
      "Gradient Descent(35/49): loss=1.0415807765694138e+70\n",
      "Gradient Descent(36/49): loss=1.3518356993124772e+72\n",
      "Gradient Descent(37/49): loss=1.7545060345243978e+74\n",
      "Gradient Descent(38/49): loss=2.277119495178417e+76\n",
      "Gradient Descent(39/49): loss=2.9554034544697883e+78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=3.8357273727559014e+80\n",
      "Gradient Descent(41/49): loss=4.978272748466001e+82\n",
      "Gradient Descent(42/49): loss=6.461147300026308e+84\n",
      "Gradient Descent(43/49): loss=8.385724636220666e+86\n",
      "Gradient Descent(44/49): loss=1.0883574450350796e+89\n",
      "Gradient Descent(45/49): loss=1.4125457006386145e+91\n",
      "Gradient Descent(46/49): loss=1.8332996806286686e+93\n",
      "Gradient Descent(47/49): loss=2.3793833484279827e+95\n",
      "Gradient Descent(48/49): loss=3.088128568720966e+97\n",
      "Gradient Descent(49/49): loss=4.007987222088984e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.729209281470692\n",
      "Gradient Descent(2/49): loss=68.8148789579421\n",
      "Gradient Descent(3/49): loss=2168.5585187546744\n",
      "Gradient Descent(4/49): loss=81688.02376760323\n",
      "Gradient Descent(5/49): loss=4614670.193956168\n",
      "Gradient Descent(6/49): loss=421985492.0426922\n",
      "Gradient Descent(7/49): loss=49945830682.96569\n",
      "Gradient Descent(8/49): loss=6406178346678.757\n",
      "Gradient Descent(9/49): loss=838320671571959.9\n",
      "Gradient Descent(10/49): loss=1.102208226341682e+17\n",
      "Gradient Descent(11/49): loss=1.4507369659160922e+19\n",
      "Gradient Descent(12/49): loss=1.9099508837086674e+21\n",
      "Gradient Descent(13/49): loss=2.5146680363088463e+23\n",
      "Gradient Descent(14/49): loss=3.3108907953546117e+25\n",
      "Gradient Descent(15/49): loss=4.359235873387759e+27\n",
      "Gradient Descent(16/49): loss=5.7395280677047555e+29\n",
      "Gradient Descent(17/49): loss=7.556872047178218e+31\n",
      "Gradient Descent(18/49): loss=9.949653798517276e+33\n",
      "Gradient Descent(19/49): loss=1.3100077774272693e+36\n",
      "Gradient Descent(20/49): loss=1.7248041138014876e+38\n",
      "Gradient Descent(21/49): loss=2.2709401299578773e+40\n",
      "Gradient Descent(22/49): loss=2.99000276791815e+42\n",
      "Gradient Descent(23/49): loss=3.936746915707973e+44\n",
      "Gradient Descent(24/49): loss=5.183264860056194e+46\n",
      "Gradient Descent(25/49): loss=6.82447594035756e+48\n",
      "Gradient Descent(26/49): loss=8.985354427755331e+50\n",
      "Gradient Descent(27/49): loss=1.1830446014900382e+53\n",
      "Gradient Descent(28/49): loss=1.5576397574161946e+55\n",
      "Gradient Descent(29/49): loss=2.0508454295194933e+57\n",
      "Gradient Descent(30/49): loss=2.7002180419161e+59\n",
      "Gradient Descent(31/49): loss=3.55520575511995e+61\n",
      "Gradient Descent(32/49): loss=4.680913824377345e+63\n",
      "Gradient Descent(33/49): loss=6.163062208057171e+65\n",
      "Gradient Descent(34/49): loss=8.114512936036537e+67\n",
      "Gradient Descent(35/49): loss=1.0683864281464327e+70\n",
      "Gradient Descent(36/49): loss=1.406676616138374e+72\n",
      "Gradient Descent(37/49): loss=1.8520818406720318e+74\n",
      "Gradient Descent(38/49): loss=2.4385186369016452e+76\n",
      "Gradient Descent(39/49): loss=3.2106427545118927e+78\n",
      "Gradient Descent(40/49): loss=4.22724958550941e+80\n",
      "Gradient Descent(41/49): loss=5.565751291724387e+82\n",
      "Gradient Descent(42/49): loss=7.32807155449714e+84\n",
      "Gradient Descent(43/49): loss=9.648406817544537e+86\n",
      "Gradient Descent(44/49): loss=1.2703445022955534e+89\n",
      "Gradient Descent(45/49): loss=1.6725819972454873e+91\n",
      "Gradient Descent(46/49): loss=2.2021825831137688e+93\n",
      "Gradient Descent(47/49): loss=2.8994740690479366e+95\n",
      "Gradient Descent(48/49): loss=3.817553522376214e+97\n",
      "Gradient Descent(49/49): loss=5.026330482407843e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7809664454477057\n",
      "Gradient Descent(2/49): loss=71.64472577142037\n",
      "Gradient Descent(3/49): loss=2387.5567538006026\n",
      "Gradient Descent(4/49): loss=105561.31644608587\n",
      "Gradient Descent(5/49): loss=7673858.810567153\n",
      "Gradient Descent(6/49): loss=834983020.3877317\n",
      "Gradient Descent(7/49): loss=106477598117.46205\n",
      "Gradient Descent(8/49): loss=14167349057293.14\n",
      "Gradient Descent(9/49): loss=1904002009589740.0\n",
      "Gradient Descent(10/49): loss=2.5647112723842166e+17\n",
      "Gradient Descent(11/49): loss=3.4564819075109057e+19\n",
      "Gradient Descent(12/49): loss=4.6588733579504785e+21\n",
      "Gradient Descent(13/49): loss=6.279701577549291e+23\n",
      "Gradient Descent(14/49): loss=8.464468662917638e+25\n",
      "Gradient Descent(15/49): loss=1.140935211798401e+28\n",
      "Gradient Descent(16/49): loss=1.5378798204466397e+30\n",
      "Gradient Descent(17/49): loss=2.072926210685319e+32\n",
      "Gradient Descent(18/49): loss=2.794121546286359e+34\n",
      "Gradient Descent(19/49): loss=3.7662292090629305e+36\n",
      "Gradient Descent(20/49): loss=5.076544535200853e+38\n",
      "Gradient Descent(21/49): loss=6.842733936770715e+40\n",
      "Gradient Descent(22/49): loss=9.223401352369744e+42\n",
      "Gradient Descent(23/49): loss=1.2432330891974445e+45\n",
      "Gradient Descent(24/49): loss=1.6757684665663121e+47\n",
      "Gradient Descent(25/49): loss=2.2587879762371558e+49\n",
      "Gradient Descent(26/49): loss=3.0446468133205005e+51\n",
      "Gradient Descent(27/49): loss=4.1039151595384085e+53\n",
      "Gradient Descent(28/49): loss=5.531715390765224e+55\n",
      "Gradient Descent(29/49): loss=7.456264073419814e+57\n",
      "Gradient Descent(30/49): loss=1.0050385821617662e+60\n",
      "Gradient Descent(31/49): loss=1.3547032960307e+62\n",
      "Gradient Descent(32/49): loss=1.8260204661288072e+64\n",
      "Gradient Descent(33/49): loss=2.4613144092075017e+66\n",
      "Gradient Descent(34/49): loss=3.3176345683658883e+68\n",
      "Gradient Descent(35/49): loss=4.4718785572624775e+70\n",
      "Gradient Descent(36/49): loss=6.0276975715121265e+72\n",
      "Gradient Descent(37/49): loss=8.124804273722534e+74\n",
      "Gradient Descent(38/49): loss=1.095151900093416e+77\n",
      "Gradient Descent(39/49): loss=1.47616809448225e+79\n",
      "Gradient Descent(40/49): loss=1.9897442929893194e+81\n",
      "Gradient Descent(41/49): loss=2.681999676244334e+83\n",
      "Gradient Descent(42/49): loss=3.6150988288892984e+85\n",
      "Gradient Descent(43/49): loss=4.872834123879259e+87\n",
      "Gradient Descent(44/49): loss=6.568150283774578e+89\n",
      "Gradient Descent(45/49): loss=8.853286825184088e+91\n",
      "Gradient Descent(46/49): loss=1.1933449178622231e+94\n",
      "Gradient Descent(47/49): loss=1.6085236151354503e+96\n",
      "Gradient Descent(48/49): loss=2.168147852075751e+98\n",
      "Gradient Descent(49/49): loss=2.9224719265714524e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8035804585747828\n",
      "Gradient Descent(2/49): loss=73.10988418883667\n",
      "Gradient Descent(3/49): loss=2429.084616542538\n",
      "Gradient Descent(4/49): loss=102850.85027822325\n",
      "Gradient Descent(5/49): loss=6874257.860966255\n",
      "Gradient Descent(6/49): loss=698157876.2879398\n",
      "Gradient Descent(7/49): loss=85308293185.26753\n",
      "Gradient Descent(8/49): loss=10996335653315.936\n",
      "Gradient Descent(9/49): loss=1436357856701856.0\n",
      "Gradient Descent(10/49): loss=1.8821187126356394e+17\n",
      "Gradient Descent(11/49): loss=2.4680497291893907e+19\n",
      "Gradient Descent(12/49): loss=3.236954301986341e+21\n",
      "Gradient Descent(13/49): loss=4.245579910384346e+23\n",
      "Gradient Descent(14/49): loss=5.568543889690615e+25\n",
      "Gradient Descent(15/49): loss=7.303772794377847e+27\n",
      "Gradient Descent(16/49): loss=9.579726110606403e+29\n",
      "Gradient Descent(17/49): loss=1.256489849188427e+32\n",
      "Gradient Descent(18/49): loss=1.6480291492499323e+34\n",
      "Gradient Descent(19/49): loss=2.1615774271057553e+36\n",
      "Gradient Descent(20/49): loss=2.835154331473698e+38\n",
      "Gradient Descent(21/49): loss=3.7186269552568444e+40\n",
      "Gradient Descent(22/49): loss=4.877401656791263e+42\n",
      "Gradient Descent(23/49): loss=6.397266305117121e+44\n",
      "Gradient Descent(24/49): loss=8.390741435412186e+46\n",
      "Gradient Descent(25/49): loss=1.1005410511003393e+49\n",
      "Gradient Descent(26/49): loss=1.4434846008311927e+51\n",
      "Gradient Descent(27/49): loss=1.89329402183927e+53\n",
      "Gradient Descent(28/49): loss=2.483270172101779e+55\n",
      "Gradient Descent(29/49): loss=3.257090909556496e+57\n",
      "Gradient Descent(30/49): loss=4.272044706330465e+59\n",
      "Gradient Descent(31/49): loss=5.603271901112174e+61\n",
      "Gradient Descent(32/49): loss=7.349327583409502e+63\n",
      "Gradient Descent(33/49): loss=9.639477948150912e+65\n",
      "Gradient Descent(34/49): loss=1.2643270293549624e+68\n",
      "Gradient Descent(35/49): loss=1.6583085160376358e+70\n",
      "Gradient Descent(36/49): loss=2.175059988843209e+72\n",
      "Gradient Descent(37/49): loss=2.8528382440986693e+74\n",
      "Gradient Descent(38/49): loss=3.7418214158407816e+76\n",
      "Gradient Descent(39/49): loss=4.90782382667771e+78\n",
      "Gradient Descent(40/49): loss=6.437168436669775e+80\n",
      "Gradient Descent(41/49): loss=8.443077613506511e+82\n",
      "Gradient Descent(42/49): loss=1.107405535353295e+85\n",
      "Gradient Descent(43/49): loss=1.4524881516774128e+87\n",
      "Gradient Descent(44/49): loss=1.905103201502609e+89\n",
      "Gradient Descent(45/49): loss=2.4987592526549074e+91\n",
      "Gradient Descent(46/49): loss=3.277406598132674e+93\n",
      "Gradient Descent(47/49): loss=4.298691039591422e+95\n",
      "Gradient Descent(48/49): loss=5.638221593985932e+97\n",
      "Gradient Descent(49/49): loss=7.395168075607977e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.85475313268196\n",
      "Gradient Descent(2/49): loss=74.26875916712537\n",
      "Gradient Descent(3/49): loss=2437.401679939172\n",
      "Gradient Descent(4/49): loss=100310.82987231477\n",
      "Gradient Descent(5/49): loss=6453049.1842260845\n",
      "Gradient Descent(6/49): loss=641009407.8581471\n",
      "Gradient Descent(7/49): loss=77790453044.25333\n",
      "Gradient Descent(8/49): loss=10012239192735.182\n",
      "Gradient Descent(9/49): loss=1307622723508850.2\n",
      "Gradient Descent(10/49): loss=1.7137200866165005e+17\n",
      "Gradient Descent(11/49): loss=2.2477645177357808e+19\n",
      "Gradient Descent(12/49): loss=2.94879431959551e+21\n",
      "Gradient Descent(13/49): loss=3.868632787420438e+23\n",
      "Gradient Descent(14/49): loss=5.075455948955871e+25\n",
      "Gradient Descent(15/49): loss=6.658764819123194e+27\n",
      "Gradient Descent(16/49): loss=8.735998233354375e+29\n",
      "Gradient Descent(17/49): loss=1.1461236037954959e+32\n",
      "Gradient Descent(18/49): loss=1.503662570745792e+34\n",
      "Gradient Descent(19/49): loss=1.9727376137032364e+36\n",
      "Gradient Descent(20/49): loss=2.5881429616428732e+38\n",
      "Gradient Descent(21/49): loss=3.3955270817799955e+40\n",
      "Gradient Descent(22/49): loss=4.454778710083014e+42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=5.84446917320387e+44\n",
      "Gradient Descent(24/49): loss=7.667680515620615e+46\n",
      "Gradient Descent(25/49): loss=1.0059651740359656e+49\n",
      "Gradient Descent(26/49): loss=1.319781033275837e+51\n",
      "Gradient Descent(27/49): loss=1.7314933168178181e+53\n",
      "Gradient Descent(28/49): loss=2.2716413030601412e+55\n",
      "Gradient Descent(29/49): loss=2.9802911507927376e+57\n",
      "Gradient Descent(30/49): loss=3.9100078571067183e+59\n",
      "Gradient Descent(31/49): loss=5.1297543324147175e+61\n",
      "Gradient Descent(32/49): loss=6.730006811392772e+63\n",
      "Gradient Descent(33/49): loss=8.829466041909421e+65\n",
      "Gradient Descent(34/49): loss=1.1583862062852504e+68\n",
      "Gradient Descent(35/49): loss=1.5197505676365206e+70\n",
      "Gradient Descent(36/49): loss=1.993844345952768e+72\n",
      "Gradient Descent(37/49): loss=2.615834045760793e+74\n",
      "Gradient Descent(38/49): loss=3.431856538275336e+76\n",
      "Gradient Descent(39/49): loss=4.502441322067033e+78\n",
      "Gradient Descent(40/49): loss=5.90700037503424e+80\n",
      "Gradient Descent(41/49): loss=7.749718638117821e+82\n",
      "Gradient Descent(42/49): loss=1.0167282064823604e+85\n",
      "Gradient Descent(43/49): loss=1.3339016474382867e+87\n",
      "Gradient Descent(44/49): loss=1.750018927078302e+89\n",
      "Gradient Descent(45/49): loss=2.295946069947428e+91\n",
      "Gradient Descent(46/49): loss=3.0121779110742495e+93\n",
      "Gradient Descent(47/49): loss=3.951841851481933e+95\n",
      "Gradient Descent(48/49): loss=5.1846386502296344e+97\n",
      "Gradient Descent(49/49): loss=6.802012566210166e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.7573479808735657\n",
      "Gradient Descent(2/49): loss=70.44121650665691\n",
      "Gradient Descent(3/49): loss=2245.7633195516833\n",
      "Gradient Descent(4/49): loss=85527.9994991822\n",
      "Gradient Descent(5/49): loss=4879498.0941368565\n",
      "Gradient Descent(6/49): loss=450536720.13957745\n",
      "Gradient Descent(7/49): loss=53875586333.138885\n",
      "Gradient Descent(8/49): loss=6984046619623.344\n",
      "Gradient Descent(9/49): loss=923813807217601.6\n",
      "Gradient Descent(10/49): loss=1.227773344474889e+17\n",
      "Gradient Descent(11/49): loss=1.6335304245964356e+19\n",
      "Gradient Descent(12/49): loss=2.1739307539856577e+21\n",
      "Gradient Descent(13/49): loss=2.8932727059539275e+23\n",
      "Gradient Descent(14/49): loss=3.8506923989788947e+25\n",
      "Gradient Descent(15/49): loss=5.12494982890188e+27\n",
      "Gradient Descent(16/49): loss=6.820884810103308e+29\n",
      "Gradient Descent(17/49): loss=9.07803563006233e+31\n",
      "Gradient Descent(18/49): loss=1.2082117829892676e+34\n",
      "Gradient Descent(19/49): loss=1.6080303984123767e+36\n",
      "Gradient Descent(20/49): loss=2.140156058512721e+38\n",
      "Gradient Descent(21/49): loss=2.848371499316304e+40\n",
      "Gradient Descent(22/49): loss=3.790947938910851e+42\n",
      "Gradient Descent(23/49): loss=5.045439571112139e+44\n",
      "Gradient Descent(24/49): loss=6.715064642433212e+46\n",
      "Gradient Descent(25/49): loss=8.937198140334803e+48\n",
      "Gradient Descent(26/49): loss=1.1894674862085244e+51\n",
      "Gradient Descent(27/49): loss=1.5830832868770989e+53\n",
      "Gradient Descent(28/49): loss=2.1069535083956452e+55\n",
      "Gradient Descent(29/49): loss=2.8041816392982615e+57\n",
      "Gradient Descent(30/49): loss=3.7321348738089185e+59\n",
      "Gradient Descent(31/49): loss=4.967164223993149e+61\n",
      "Gradient Descent(32/49): loss=6.610886600391591e+63\n",
      "Gradient Descent(33/49): loss=8.798545744095206e+65\n",
      "Gradient Descent(34/49): loss=1.171013993892318e+68\n",
      "Gradient Descent(35/49): loss=1.558523208010704e+70\n",
      "Gradient Descent(36/49): loss=2.0742660656293593e+72\n",
      "Gradient Descent(37/49): loss=2.7606773443646007e+74\n",
      "Gradient Descent(38/49): loss=3.6742342392683693e+76\n",
      "Gradient Descent(39/49): loss=4.8901032467883115e+78\n",
      "Gradient Descent(40/49): loss=6.508324784706952e+80\n",
      "Gradient Descent(41/49): loss=8.662044412058367e+82\n",
      "Gradient Descent(42/49): loss=1.1528467905101735e+85\n",
      "Gradient Descent(43/49): loss=1.5343441561433384e+87\n",
      "Gradient Descent(44/49): loss=2.042085738426236e+89\n",
      "Gradient Descent(45/49): loss=2.7178479784910982e+91\n",
      "Gradient Descent(46/49): loss=3.617231879735161e+93\n",
      "Gradient Descent(47/49): loss=4.8142377996567193e+95\n",
      "Gradient Descent(48/49): loss=6.4073541211133516e+97\n",
      "Gradient Descent(49/49): loss=8.527660772443896e+99\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.809555183308793\n",
      "Gradient Descent(2/49): loss=73.3269582954417\n",
      "Gradient Descent(3/49): loss=2471.680132342878\n",
      "Gradient Descent(4/49): loss=110434.94867382676\n",
      "Gradient Descent(5/49): loss=8106066.492225148\n",
      "Gradient Descent(6/49): loss=890830095.6304086\n",
      "Gradient Descent(7/49): loss=114791667557.83412\n",
      "Gradient Descent(8/49): loss=15437019035772.193\n",
      "Gradient Descent(9/49): loss=2096963088039476.8\n",
      "Gradient Descent(10/49): loss=2.8550734827428557e+17\n",
      "Gradient Descent(11/49): loss=3.889289757929603e+19\n",
      "Gradient Descent(12/49): loss=5.298763901713598e+21\n",
      "Gradient Descent(13/49): loss=7.219222611238332e+23\n",
      "Gradient Descent(14/49): loss=9.835782617604625e+25\n",
      "Gradient Descent(15/49): loss=1.3400715987491679e+28\n",
      "Gradient Descent(16/49): loss=1.8257748412177148e+30\n",
      "Gradient Descent(17/49): loss=2.4875193287655297e+32\n",
      "Gradient Descent(18/49): loss=3.3891104034949723e+34\n",
      "Gradient Descent(19/49): loss=4.61747944441238e+36\n",
      "Gradient Descent(20/49): loss=6.291065766057862e+38\n",
      "Gradient Descent(21/49): loss=8.571236527755281e+40\n",
      "Gradient Descent(22/49): loss=1.1677845749783336e+43\n",
      "Gradient Descent(23/49): loss=1.591043263307624e+45\n",
      "Gradient Descent(24/49): loss=2.167710312296972e+47\n",
      "Gradient Descent(25/49): loss=2.953387947649033e+49\n",
      "Gradient Descent(26/49): loss=4.023831191759532e+51\n",
      "Gradient Descent(27/49): loss=5.482252161510365e+53\n",
      "Gradient Descent(28/49): loss=7.469271778581279e+55\n",
      "Gradient Descent(29/49): loss=1.0176478435997551e+58\n",
      "Gradient Descent(30/49): loss=1.3864900947277466e+60\n",
      "Gradient Descent(31/49): loss=1.8890176939580377e+62\n",
      "Gradient Descent(32/49): loss=2.5736843426834326e+64\n",
      "Gradient Descent(33/49): loss=3.506505585924469e+66\n",
      "Gradient Descent(34/49): loss=4.777424029902319e+68\n",
      "Gradient Descent(35/49): loss=6.508981606390634e+70\n",
      "Gradient Descent(36/49): loss=8.868135063405392e+72\n",
      "Gradient Descent(37/49): loss=1.2082353931617175e+75\n",
      "Gradient Descent(38/49): loss=1.6461553132097987e+77\n",
      "Gradient Descent(39/49): loss=2.242797496701179e+79\n",
      "Gradient Descent(40/49): loss=3.055690171421736e+81\n",
      "Gradient Descent(41/49): loss=4.1632124333371734e+83\n",
      "Gradient Descent(42/49): loss=5.672151557508335e+85\n",
      "Gradient Descent(43/49): loss=7.727999425086857e+87\n",
      "Gradient Descent(44/49): loss=1.0528980847679902e+90\n",
      "Gradient Descent(45/49): loss=1.4345166399849528e+92\n",
      "Gradient Descent(46/49): loss=1.954451261868486e+94\n",
      "Gradient Descent(47/49): loss=2.6628340365988587e+96\n",
      "Gradient Descent(48/49): loss=3.627967217606886e+98\n",
      "Gradient Descent(49/49): loss=4.942908927528043e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8324235748680278\n",
      "Gradient Descent(2/49): loss=74.8254843100723\n",
      "Gradient Descent(3/49): loss=2514.728943119339\n",
      "Gradient Descent(4/49): loss=107616.81092780732\n",
      "Gradient Descent(5/49): loss=7263185.36940063\n",
      "Gradient Descent(6/49): loss=744979658.5691259\n",
      "Gradient Descent(7/49): loss=91980150701.36554\n",
      "Gradient Descent(8/49): loss=11983068842971.557\n",
      "Gradient Descent(9/49): loss=1582096538905406.2\n",
      "Gradient Descent(10/49): loss=2.0954477691603808e+17\n",
      "Gradient Descent(11/49): loss=2.7774465877732803e+19\n",
      "Gradient Descent(12/49): loss=3.6820612910502133e+21\n",
      "Gradient Descent(13/49): loss=4.881510973891748e+23\n",
      "Gradient Descent(14/49): loss=6.4717502071562474e+25\n",
      "Gradient Descent(15/49): loss=8.580057811355817e+27\n",
      "Gradient Descent(16/49): loss=1.1375196689973483e+30\n",
      "Gradient Descent(17/49): loss=1.5080914241890658e+32\n",
      "Gradient Descent(18/49): loss=1.9993850419004074e+34\n",
      "Gradient Descent(19/49): loss=2.650728271157786e+36\n",
      "Gradient Descent(20/49): loss=3.514260751057231e+38\n",
      "Gradient Descent(21/49): loss=4.659107749950849e+40\n",
      "Gradient Descent(22/49): loss=6.176913600130737e+42\n",
      "Gradient Descent(23/49): loss=8.18917777222278e+44\n",
      "Gradient Descent(24/49): loss=1.0856980836510693e+47\n",
      "Gradient Descent(25/49): loss=1.4393878868318065e+49\n",
      "Gradient Descent(26/49): loss=1.9082998486940173e+51\n",
      "Gradient Descent(27/49): loss=2.52997009759567e+53\n",
      "Gradient Descent(28/49): loss=3.354162973449258e+55\n",
      "Gradient Descent(29/49): loss=4.4468546340329326e+57\n",
      "Gradient Descent(30/49): loss=5.895514407841832e+59\n",
      "Gradient Descent(31/49): loss=7.816106662688344e+61\n",
      "Gradient Descent(32/49): loss=1.0362373685536538e+64\n",
      "Gradient Descent(33/49): loss=1.373814266267533e+66\n",
      "Gradient Descent(34/49): loss=1.8213641926796653e+68\n",
      "Gradient Descent(35/49): loss=2.4147132577014794e+70\n",
      "Gradient Descent(36/49): loss=3.2013587070363263e+72\n",
      "Gradient Descent(37/49): loss=4.2442710489248375e+74\n",
      "Gradient Descent(38/49): loss=5.626934806508261e+76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=7.460031405090182e+78\n",
      "Gradient Descent(40/49): loss=9.890299155512642e+80\n",
      "Gradient Descent(41/49): loss=1.31122795701353e+83\n",
      "Gradient Descent(42/49): loss=1.7383890297146218e+85\n",
      "Gradient Descent(43/49): loss=2.304707127748417e+87\n",
      "Gradient Descent(44/49): loss=3.05551568371687e+89\n",
      "Gradient Descent(45/49): loss=4.050916483501606e+91\n",
      "Gradient Descent(46/49): loss=5.370590779080496e+93\n",
      "Gradient Descent(47/49): loss=7.120177726155364e+95\n",
      "Gradient Descent(48/49): loss=9.439730736795916e+97\n",
      "Gradient Descent(49/49): loss=1.2514928673181387e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.884220884806471\n",
      "Gradient Descent(2/49): loss=76.0125775797449\n",
      "Gradient Descent(3/49): loss=2523.435589302956\n",
      "Gradient Descent(4/49): loss=104969.2897912536\n",
      "Gradient Descent(5/49): loss=6818865.438432357\n",
      "Gradient Descent(6/49): loss=684031391.1325899\n",
      "Gradient Descent(7/49): loss=83875531691.27293\n",
      "Gradient Descent(8/49): loss=10910700224547.975\n",
      "Gradient Descent(9/49): loss=1440299435610717.0\n",
      "Gradient Descent(10/49): loss=1.9079603258869245e+17\n",
      "Gradient Descent(11/49): loss=2.529543277468375e+19\n",
      "Gradient Descent(12/49): loss=3.3542725650157284e+21\n",
      "Gradient Descent(13/49): loss=4.448095762246548e+23\n",
      "Gradient Descent(14/49): loss=5.898675186207633e+25\n",
      "Gradient Descent(15/49): loss=7.822325917459224e+27\n",
      "Gradient Descent(16/49): loss=1.0373315389565288e+30\n",
      "Gradient Descent(17/49): loss=1.3756226453768339e+32\n",
      "Gradient Descent(18/49): loss=1.8242361783920794e+34\n",
      "Gradient Descent(19/49): loss=2.4191500993314163e+36\n",
      "Gradient Descent(20/49): loss=3.208075403044777e+38\n",
      "Gradient Descent(21/49): loss=4.25428244350923e+40\n",
      "Gradient Descent(22/49): loss=5.641675097066948e+42\n",
      "Gradient Descent(23/49): loss=7.481519697906865e+44\n",
      "Gradient Descent(24/49): loss=9.921368392795286e+46\n",
      "Gradient Descent(25/49): loss=1.3156892551284425e+49\n",
      "Gradient Descent(26/49): loss=1.7447575248974618e+51\n",
      "Gradient Descent(27/49): loss=2.313752133203578e+53\n",
      "Gradient Descent(28/49): loss=3.0683053991808136e+55\n",
      "Gradient Descent(29/49): loss=4.068931104390531e+57\n",
      "Gradient Descent(30/49): loss=5.395877586597804e+59\n",
      "Gradient Descent(31/49): loss=7.155563508591347e+61\n",
      "Gradient Descent(32/49): loss=9.489112438847673e+63\n",
      "Gradient Descent(33/49): loss=1.2583670701682854e+66\n",
      "Gradient Descent(34/49): loss=1.6687416167620255e+68\n",
      "Gradient Descent(35/49): loss=2.2129461661303243e+70\n",
      "Gradient Descent(36/49): loss=2.9346249203594595e+72\n",
      "Gradient Descent(37/49): loss=3.8916551857447054e+74\n",
      "Gradient Descent(38/49): loss=5.1607890261078505e+76\n",
      "Gradient Descent(39/49): loss=6.843808636889276e+78\n",
      "Gradient Descent(40/49): loss=9.07568908967468e+80\n",
      "Gradient Descent(41/49): loss=1.2035423084225736e+83\n",
      "Gradient Descent(42/49): loss=1.5960375833181526e+85\n",
      "Gradient Descent(43/49): loss=2.1165321314733844e+87\n",
      "Gradient Descent(44/49): loss=2.8067686565663916e+89\n",
      "Gradient Descent(45/49): loss=3.722102856052317e+91\n",
      "Gradient Descent(46/49): loss=4.935942846098587e+93\n",
      "Gradient Descent(47/49): loss=6.545636357237225e+95\n",
      "Gradient Descent(48/49): loss=8.680277842975885e+97\n",
      "Gradient Descent(49/49): loss=1.1511061617095614e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.785655473404785\n",
      "Gradient Descent(2/49): loss=72.0963826829155\n",
      "Gradient Descent(3/49): loss=2325.2468388693133\n",
      "Gradient Descent(4/49): loss=89525.21368864538\n",
      "Gradient Descent(5/49): loss=5157950.60740339\n",
      "Gradient Descent(6/49): loss=480850422.91530925\n",
      "Gradient Descent(7/49): loss=58090932174.78591\n",
      "Gradient Descent(8/49): loss=7610514808213.021\n",
      "Gradient Descent(9/49): loss=1017495426455713.8\n",
      "Gradient Descent(10/49): loss=1.3668516282902677e+17\n",
      "Gradient Descent(11/49): loss=1.8381851996560386e+19\n",
      "Gradient Descent(12/49): loss=2.4726779940006564e+21\n",
      "Gradient Descent(13/49): loss=3.326375636984378e+23\n",
      "Gradient Descent(14/49): loss=4.474874645580777e+25\n",
      "Gradient Descent(15/49): loss=6.019935042095565e+27\n",
      "Gradient Descent(16/49): loss=8.098471283968961e+29\n",
      "Gradient Descent(17/49): loss=1.0894677014585332e+32\n",
      "Gradient Descent(18/49): loss=1.4656345326006884e+34\n",
      "Gradient Descent(19/49): loss=1.9716826841415168e+36\n",
      "Gradient Descent(20/49): loss=2.652457026825634e+38\n",
      "Gradient Descent(21/49): loss=3.5682862861247175e+40\n",
      "Gradient Descent(22/49): loss=4.800329239015217e+42\n",
      "Gradient Descent(23/49): loss=6.457766825804685e+44\n",
      "Gradient Descent(24/49): loss=8.68747752503205e+46\n",
      "Gradient Descent(25/49): loss=1.1687053401564872e+49\n",
      "Gradient Descent(26/49): loss=1.5722310281380292e+51\n",
      "Gradient Descent(27/49): loss=2.1150843766223658e+53\n",
      "Gradient Descent(28/49): loss=2.845371857041971e+55\n",
      "Gradient Descent(29/49): loss=3.827809941925505e+57\n",
      "Gradient Descent(30/49): loss=5.149460136551832e+59\n",
      "Gradient Descent(31/49): loss=6.927444178327529e+61\n",
      "Gradient Descent(32/49): loss=9.319323107912807e+63\n",
      "Gradient Descent(33/49): loss=1.2537059982581941e+66\n",
      "Gradient Descent(34/49): loss=1.6865803576807207e+68\n",
      "Gradient Descent(35/49): loss=2.2689157640359104e+70\n",
      "Gradient Descent(36/49): loss=3.0523175020073594e+72\n",
      "Gradient Descent(37/49): loss=4.106208912969156e+74\n",
      "Gradient Descent(38/49): loss=5.523983539018601e+76\n",
      "Gradient Descent(39/49): loss=7.431281453549689e+78\n",
      "Gradient Descent(40/49): loss=9.997123208604555e+80\n",
      "Gradient Descent(41/49): loss=1.344888806496207e+83\n",
      "Gradient Descent(42/49): loss=1.809246384281868e+85\n",
      "Gradient Descent(43/49): loss=2.433935402856817e+87\n",
      "Gradient Descent(44/49): loss=3.274314430995103e+89\n",
      "Gradient Descent(45/49): loss=4.404856012381747e+91\n",
      "Gradient Descent(46/49): loss=5.925746258864711e+93\n",
      "Gradient Descent(47/49): loss=7.971763123640001e+95\n",
      "Gradient Descent(48/49): loss=1.0724220127441437e+98\n",
      "Gradient Descent(49/49): loss=1.4427033964514343e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8383143332365286\n",
      "Gradient Descent(2/49): loss=75.03881365820995\n",
      "Gradient Descent(3/49): loss=2558.2616900588837\n",
      "Gradient Descent(4/49): loss=115504.68069759772\n",
      "Gradient Descent(5/49): loss=8560091.851035507\n",
      "Gradient Descent(6/49): loss=950084073.9555207\n",
      "Gradient Descent(7/49): loss=123705311906.48138\n",
      "Gradient Descent(8/49): loss=16812778238515.176\n",
      "Gradient Descent(9/49): loss=2308291020410168.5\n",
      "Gradient Descent(10/49): loss=3.176491197815829e+17\n",
      "Gradient Descent(11/49): loss=4.373539215037301e+19\n",
      "Gradient Descent(12/49): loss=6.022406993502256e+21\n",
      "Gradient Descent(13/49): loss=8.293137894479716e+23\n",
      "Gradient Descent(14/49): loss=1.142011092996329e+26\n",
      "Gradient Descent(15/49): loss=1.5726147996115804e+28\n",
      "Gradient Descent(16/49): loss=2.1655814876830308e+30\n",
      "Gradient Descent(17/49): loss=2.9821311061997907e+32\n",
      "Gradient Descent(18/49): loss=4.1065672789339762e+34\n",
      "Gradient Descent(19/49): loss=5.654981044455733e+36\n",
      "Gradient Descent(20/49): loss=7.787236508576827e+38\n",
      "Gradient Descent(21/49): loss=1.0723475812753095e+41\n",
      "Gradient Descent(22/49): loss=1.476684744130896e+43\n",
      "Gradient Descent(23/49): loss=2.0334804420190465e+45\n",
      "Gradient Descent(24/49): loss=2.8002203750784584e+47\n",
      "Gradient Descent(25/49): loss=3.8560656827477293e+49\n",
      "Gradient Descent(26/49): loss=5.310025840108974e+51\n",
      "Gradient Descent(27/49): loss=7.312213209639601e+53\n",
      "Gradient Descent(28/49): loss=1.006934121098921e+56\n",
      "Gradient Descent(29/49): loss=1.3866066198626136e+58\n",
      "Gradient Descent(30/49): loss=1.9094376463759815e+60\n",
      "Gradient Descent(31/49): loss=2.6294062592597783e+62\n",
      "Gradient Descent(32/49): loss=3.620844749424799e+64\n",
      "Gradient Descent(33/49): loss=4.986112987776839e+66\n",
      "Gradient Descent(34/49): loss=6.86616644660808e+68\n",
      "Gradient Descent(35/49): loss=9.455108977293105e+70\n",
      "Gradient Descent(36/49): loss=1.3020232828269295e+73\n",
      "Gradient Descent(37/49): loss=1.792961491078151e+75\n",
      "Gradient Descent(38/49): loss=2.469011845555874e+77\n",
      "Gradient Descent(39/49): loss=3.3999723495619955e+79\n",
      "Gradient Descent(40/49): loss=4.681958897278516e+81\n",
      "Gradient Descent(41/49): loss=6.447328643313467e+83\n",
      "Gradient Descent(42/49): loss=8.878345057461562e+85\n",
      "Gradient Descent(43/49): loss=1.2225995496770757e+88\n",
      "Gradient Descent(44/49): loss=1.68359040924452e+90\n",
      "Gradient Descent(45/49): loss=2.3184015296331635e+92\n",
      "Gradient Descent(46/49): loss=3.1925732191700133e+94\n",
      "Gradient Descent(47/49): loss=4.3963582793935685e+96\n",
      "Gradient Descent(48/49): loss=6.054040046673437e+98\n",
      "Gradient Descent(49/49): loss=8.336763875345772e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.861438506458186\n",
      "Gradient Descent(2/49): loss=76.57127211445464\n",
      "Gradient Descent(3/49): loss=2602.877244655334\n",
      "Gradient Descent(4/49): loss=112575.20857315754\n",
      "Gradient Descent(5/49): loss=7671835.749744532\n",
      "Gradient Descent(6/49): loss=794665721.5168124\n",
      "Gradient Descent(7/49): loss=99133943884.09137\n",
      "Gradient Descent(8/49): loss=13052354736427.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=1741723492754758.8\n",
      "Gradient Descent(10/49): loss=2.3316195266250064e+17\n",
      "Gradient Descent(11/49): loss=3.123659204965277e+19\n",
      "Gradient Descent(12/49): loss=4.1854935870113243e+21\n",
      "Gradient Descent(13/49): loss=5.6085144669490555e+23\n",
      "Gradient Descent(14/49): loss=7.5154202655527535e+25\n",
      "Gradient Descent(15/49): loss=1.0070700984932585e+28\n",
      "Gradient Descent(16/49): loss=1.3494797337800798e+30\n",
      "Gradient Descent(17/49): loss=1.8083108485995973e+32\n",
      "Gradient Descent(18/49): loss=2.4231473370793046e+34\n",
      "Gradient Descent(19/49): loss=3.247031926620601e+36\n",
      "Gradient Descent(20/49): loss=4.351042212195847e+38\n",
      "Gradient Descent(21/49): loss=5.830422603638143e+40\n",
      "Gradient Descent(22/49): loss=7.81280118240464e+42\n",
      "Gradient Descent(23/49): loss=1.0469200342186924e+45\n",
      "Gradient Descent(24/49): loss=1.4028791114286414e+47\n",
      "Gradient Descent(25/49): loss=1.8798664052253633e+49\n",
      "Gradient Descent(26/49): loss=2.519032233573709e+51\n",
      "Gradient Descent(27/49): loss=3.375518268822419e+53\n",
      "Gradient Descent(28/49): loss=4.523214681929368e+55\n",
      "Gradient Descent(29/49): loss=6.0611347441941976e+57\n",
      "Gradient Descent(30/49): loss=8.121956831730133e+59\n",
      "Gradient Descent(31/49): loss=1.088347076257862e+62\n",
      "Gradient Descent(32/49): loss=1.4583915956946773e+64\n",
      "Gradient Descent(33/49): loss=1.95425346637211e+66\n",
      "Gradient Descent(34/49): loss=2.618711340700128e+68\n",
      "Gradient Descent(35/49): loss=3.509088868928613e+70\n",
      "Gradient Descent(36/49): loss=4.702200085461405e+72\n",
      "Gradient Descent(37/49): loss=6.300976256114078e+74\n",
      "Gradient Descent(38/49): loss=8.443345893099573e+76\n",
      "Gradient Descent(39/49): loss=1.1314134028253883e+79\n",
      "Gradient Descent(40/49): loss=1.5161007310373449e+81\n",
      "Gradient Descent(41/49): loss=2.0315840531073668e+83\n",
      "Gradient Descent(42/49): loss=2.722334789731307e+85\n",
      "Gradient Descent(43/49): loss=3.6479449107930363e+87\n",
      "Gradient Descent(44/49): loss=4.8882680125813865e+89\n",
      "Gradient Descent(45/49): loss=6.550308392028807e+91\n",
      "Gradient Descent(46/49): loss=8.777452447421258e+93\n",
      "Gradient Descent(47/49): loss=1.1761838810596434e+96\n",
      "Gradient Descent(48/49): loss=1.5760934398123382e+98\n",
      "Gradient Descent(49/49): loss=2.1119746419085117e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9138641104127374\n",
      "Gradient Descent(2/49): loss=77.78709464135136\n",
      "Gradient Descent(3/49): loss=2611.9875345088635\n",
      "Gradient Descent(4/49): loss=109816.24778863223\n",
      "Gradient Descent(5/49): loss=7203270.168826313\n",
      "Gradient Descent(6/49): loss=729687353.1173717\n",
      "Gradient Descent(7/49): loss=90400253279.5648\n",
      "Gradient Descent(8/49): loss=11884332009395.68\n",
      "Gradient Descent(9/49): loss=1585619684126240.2\n",
      "Gradient Descent(10/49): loss=2.1229991163920256e+17\n",
      "Gradient Descent(11/49): loss=2.844851114619265e+19\n",
      "Gradient Descent(12/49): loss=3.8128827220179886e+21\n",
      "Gradient Descent(13/49): loss=5.11054338696872e+23\n",
      "Gradient Descent(14/49): loss=6.849917402981955e+25\n",
      "Gradient Descent(15/49): loss=9.181310460277248e+27\n",
      "Gradient Descent(16/49): loss=1.2306208392531542e+30\n",
      "Gradient Descent(17/49): loss=1.6494680840412463e+32\n",
      "Gradient Descent(18/49): loss=2.210871911793397e+34\n",
      "Gradient Descent(19/49): loss=2.9633520614306784e+36\n",
      "Gradient Descent(20/49): loss=3.971942204617779e+38\n",
      "Gradient Descent(21/49): loss=5.323810521426691e+40\n",
      "Gradient Descent(22/49): loss=7.135793274596052e+42\n",
      "Gradient Descent(23/49): loss=9.564492472823404e+44\n",
      "Gradient Descent(24/49): loss=1.2819810320017897e+47\n",
      "Gradient Descent(25/49): loss=1.7183090175293843e+49\n",
      "Gradient Descent(26/49): loss=2.3031431870042782e+51\n",
      "Gradient Descent(27/49): loss=3.087028285209982e+53\n",
      "Gradient Descent(28/49): loss=4.1377121871794256e+55\n",
      "Gradient Descent(29/49): loss=5.546001060618266e+57\n",
      "Gradient Descent(30/49): loss=7.433607359081551e+59\n",
      "Gradient Descent(31/49): loss=9.963668914775052e+61\n",
      "Gradient Descent(32/49): loss=1.335484825170002e+64\n",
      "Gradient Descent(33/49): loss=1.790023066317077e+66\n",
      "Gradient Descent(34/49): loss=2.399265433464761e+68\n",
      "Gradient Descent(35/49): loss=3.2158661687318926e+70\n",
      "Gradient Descent(36/49): loss=4.310400621351937e+72\n",
      "Gradient Descent(37/49): loss=5.777464776737613e+74\n",
      "Gradient Descent(38/49): loss=7.743850787580397e+76\n",
      "Gradient Descent(39/49): loss=1.0379505083572746e+79\n",
      "Gradient Descent(40/49): loss=1.391221612284879e+81\n",
      "Gradient Descent(41/49): loss=1.8647301185408564e+83\n",
      "Gradient Descent(42/49): loss=2.4993993654846212e+85\n",
      "Gradient Descent(43/49): loss=3.3500811329594274e+87\n",
      "Gradient Descent(44/49): loss=4.4902962497290106e+89\n",
      "Gradient Descent(45/49): loss=6.018588687886283e+91\n",
      "Gradient Descent(46/49): loss=8.06704230174973e+93\n",
      "Gradient Descent(47/49): loss=1.081269627698633e+96\n",
      "Gradient Descent(48/49): loss=1.4492845878965838e+98\n",
      "Gradient Descent(49/49): loss=1.9425550879340256e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8141317590643453\n",
      "Gradient Descent(2/49): loss=73.78071638346114\n",
      "Gradient Descent(3/49): loss=2407.0625964674023\n",
      "Gradient Descent(4/49): loss=93685.16015309261\n",
      "Gradient Descent(5/49): loss=5450648.370747188\n",
      "Gradient Descent(6/49): loss=513025050.98335373\n",
      "Gradient Descent(7/49): loss=62610926135.52368\n",
      "Gradient Descent(8/49): loss=8289376934228.991\n",
      "Gradient Descent(9/49): loss=1120099595827972.0\n",
      "Gradient Descent(10/49): loss=1.5208131291899453e+17\n",
      "Gradient Descent(11/49): loss=2.0671773638505107e+19\n",
      "Gradient Descent(12/49): loss=2.810547895707253e+21\n",
      "Gradient Descent(13/49): loss=3.821465231057805e+23\n",
      "Gradient Descent(14/49): loss=5.196067107621577e+25\n",
      "Gradient Descent(15/49): loss=7.065142957250314e+27\n",
      "Gradient Descent(16/49): loss=9.606550511227461e+29\n",
      "Gradient Descent(17/49): loss=1.3062131746630424e+32\n",
      "Gradient Descent(18/49): loss=1.7760724010681386e+34\n",
      "Gradient Descent(19/49): loss=2.4149451735502e+36\n",
      "Gradient Descent(20/49): loss=3.2836275140883443e+38\n",
      "Gradient Descent(21/49): loss=4.464784449122929e+40\n",
      "Gradient Descent(22/49): loss=6.070816526467663e+42\n",
      "Gradient Descent(23/49): loss=8.254556008004888e+44\n",
      "Gradient Descent(24/49): loss=1.1223810601524845e+47\n",
      "Gradient Descent(25/49): loss=1.526113873319255e+49\n",
      "Gradient Descent(26/49): loss=2.0750738203136193e+51\n",
      "Gradient Descent(27/49): loss=2.8215006986247473e+53\n",
      "Gradient Descent(28/49): loss=3.83642553551986e+55\n",
      "Gradient Descent(29/49): loss=5.216430000092868e+57\n",
      "Gradient Descent(30/49): loss=7.092837250177894e+59\n",
      "Gradient Descent(31/49): loss=9.644208827994482e+61\n",
      "Gradient Descent(32/49): loss=1.311333682661833e+64\n",
      "Gradient Descent(33/49): loss=1.7830348325639053e+66\n",
      "Gradient Descent(34/49): loss=2.424412074646649e+68\n",
      "Gradient Descent(35/49): loss=3.296499765649783e+70\n",
      "Gradient Descent(36/49): loss=4.482286991790794e+72\n",
      "Gradient Descent(37/49): loss=6.094614926452613e+74\n",
      "Gradient Descent(38/49): loss=8.286914954300896e+76\n",
      "Gradient Descent(39/49): loss=1.1267809416761168e+79\n",
      "Gradient Descent(40/49): loss=1.5320964406248325e+81\n",
      "Gradient Descent(41/49): loss=2.0832083828854716e+83\n",
      "Gradient Descent(42/49): loss=2.8325613528313577e+85\n",
      "Gradient Descent(43/49): loss=3.8514648287083385e+87\n",
      "Gradient Descent(44/49): loss=5.236879092468712e+89\n",
      "Gradient Descent(45/49): loss=7.120642106014833e+91\n",
      "Gradient Descent(46/49): loss=9.682015396320477e+93\n",
      "Gradient Descent(47/49): loss=1.316474283343109e+96\n",
      "Gradient Descent(48/49): loss=1.7900245638552084e+98\n",
      "Gradient Descent(49/49): loss=2.433916089168231e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8672438952309114\n",
      "Gradient Descent(2/49): loss=76.78063783283682\n",
      "Gradient Descent(3/49): loss=2647.3586070370125\n",
      "Gradient Descent(4/49): loss=120777.24862977322\n",
      "Gradient Descent(5/49): loss=9036909.864017166\n",
      "Gradient Descent(6/49): loss=1012932963.9250817\n",
      "Gradient Descent(7/49): loss=133258230871.55667\n",
      "Gradient Descent(8/49): loss=18302854953957.04\n",
      "Gradient Descent(9/49): loss=2539622135968174.5\n",
      "Gradient Descent(10/49): loss=3.5320936850969965e+17\n",
      "Gradient Descent(11/49): loss=4.91502083182281e+19\n",
      "Gradient Descent(12/49): loss=6.840229402074673e+21\n",
      "Gradient Descent(13/49): loss=9.519799275051507e+23\n",
      "Gradient Descent(14/49): loss=1.3249137109907876e+26\n",
      "Gradient Descent(15/49): loss=1.8439451761390046e+28\n",
      "Gradient Descent(16/49): loss=2.566306666873706e+30\n",
      "Gradient Descent(17/49): loss=3.571651948426327e+32\n",
      "Gradient Descent(18/49): loss=4.970839226991472e+34\n",
      "Gradient Descent(19/49): loss=6.918155259377227e+36\n",
      "Gradient Descent(20/49): loss=9.628328346034841e+38\n",
      "Gradient Descent(21/49): loss=1.3400206164923877e+41\n",
      "Gradient Descent(22/49): loss=1.8649709359373164e+43\n",
      "Gradient Descent(23/49): loss=2.5955694629751616e+45\n",
      "Gradient Descent(24/49): loss=3.61237845980578e+47\n",
      "Gradient Descent(25/49): loss=5.027520289099834e+49\n",
      "Gradient Descent(26/49): loss=6.997041018418758e+51\n",
      "Gradient Descent(27/49): loss=9.738117441232887e+53\n",
      "Gradient Descent(28/49): loss=1.3553004913021816e+56\n",
      "Gradient Descent(29/49): loss=1.8862366702895274e+58\n",
      "Gradient Descent(30/49): loss=2.6251660050137576e+60\n",
      "Gradient Descent(31/49): loss=3.6535693862965062e+62\n",
      "Gradient Descent(32/49): loss=5.0848476762950816e+64\n",
      "Gradient Descent(33/49): loss=7.076826291599017e+66\n",
      "Gradient Descent(34/49): loss=9.84915843102646e+68\n",
      "Gradient Descent(35/49): loss=1.3707545982104465e+71\n",
      "Gradient Descent(36/49): loss=1.9077448917828495e+73\n",
      "Gradient Descent(37/49): loss=2.6551000280246596e+75\n",
      "Gradient Descent(38/49): loss=3.695230001233888e+77\n",
      "Gradient Descent(39/49): loss=5.142828751419148e+79\n",
      "Gradient Descent(40/49): loss=7.157521333609103e+81\n",
      "Gradient Descent(41/49): loss=9.96146558971686e+83\n",
      "Gradient Descent(42/49): loss=1.386384923914416e+86\n",
      "Gradient Descent(43/49): loss=1.9294983654225122e+88\n",
      "Gradient Descent(44/49): loss=2.6853753802057673e+90\n",
      "Gradient Descent(45/49): loss=3.73736566034146e+92\n",
      "Gradient Descent(46/49): loss=5.201470968289555e+94\n",
      "Gradient Descent(47/49): loss=7.239136518284106e+96\n",
      "Gradient Descent(48/49): loss=1.0075053355068054e+99\n",
      "Gradient Descent(49/49): loss=1.40219347778689e+101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8906252533452603\n",
      "Gradient Descent(2/49): loss=78.3475998975459\n",
      "Gradient Descent(3/49): loss=2693.5877907982936\n",
      "Gradient Descent(4/49): loss=117732.67532454769\n",
      "Gradient Descent(5/49): loss=8101094.4389367895\n",
      "Gradient Descent(6/49): loss=847374583.4756956\n",
      "Gradient Descent(7/49): loss=106801636559.0316\n",
      "Gradient Descent(8/49): loss=14210604287577.54\n",
      "Gradient Descent(9/49): loss=1916477489142443.2\n",
      "Gradient Descent(10/49): loss=2.59293833322254e+17\n",
      "Gradient Descent(11/49): loss=3.5108365008655524e+19\n",
      "Gradient Descent(12/49): loss=4.75452071907186e+21\n",
      "Gradient Descent(13/49): loss=6.439040865531066e+23\n",
      "Gradient Descent(14/49): loss=8.720470665880981e+25\n",
      "Gradient Descent(15/49): loss=1.181026611674282e+28\n",
      "Gradient Descent(16/49): loss=1.599483190010748e+30\n",
      "Gradient Descent(17/49): loss=2.1662058930132163e+32\n",
      "Gradient Descent(18/49): loss=2.933727683469661e+34\n",
      "Gradient Descent(19/49): loss=3.973194888902304e+36\n",
      "Gradient Descent(20/49): loss=5.380962159772008e+38\n",
      "Gradient Descent(21/49): loss=7.28752416783492e+40\n",
      "Gradient Descent(22/49): loss=9.869611962480818e+42\n",
      "Gradient Descent(23/49): loss=1.3366575265989722e+45\n",
      "Gradient Descent(24/49): loss=1.8102569282509881e+47\n",
      "Gradient Descent(25/49): loss=2.451660265305839e+49\n",
      "Gradient Descent(26/49): loss=3.3203231887577786e+51\n",
      "Gradient Descent(27/49): loss=4.496767449313863e+53\n",
      "Gradient Descent(28/49): loss=6.090044957573631e+55\n",
      "Gradient Descent(29/49): loss=8.247846481571211e+57\n",
      "Gradient Descent(30/49): loss=1.117019201951357e+60\n",
      "Gradient Descent(31/49): loss=1.5127971893220208e+62\n",
      "Gradient Descent(32/49): loss=2.0488057251143747e+64\n",
      "Gradient Descent(33/49): loss=2.7747307629137534e+66\n",
      "Gradient Descent(34/49): loss=3.75786279405787e+68\n",
      "Gradient Descent(35/49): loss=5.089334420372778e+70\n",
      "Gradient Descent(36/49): loss=6.892567999914135e+72\n",
      "Gradient Descent(37/49): loss=9.334716430358056e+74\n",
      "Gradient Descent(38/49): loss=1.2642157587169689e+77\n",
      "Gradient Descent(39/49): loss=1.7121478692063846e+79\n",
      "Gradient Descent(40/49): loss=2.318789578294043e+81\n",
      "Gradient Descent(41/49): loss=3.1403742662117255e+83\n",
      "Gradient Descent(42/49): loss=4.253059710204717e+85\n",
      "Gradient Descent(43/49): loss=5.759987620961694e+87\n",
      "Gradient Descent(44/49): loss=7.800844487093962e+89\n",
      "Gradient Descent(45/49): loss=1.0564809981599304e+92\n",
      "Gradient Descent(46/49): loss=1.4308093198365969e+94\n",
      "Gradient Descent(47/49): loss=1.9377682261175697e+96\n",
      "Gradient Descent(48/49): loss=2.6243508803672742e+98\n",
      "Gradient Descent(49/49): loss=3.554200884531648e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9436828095007606\n",
      "Gradient Descent(2/49): loss=79.59266877076024\n",
      "Gradient Descent(3/49): loss=2703.1161713877277\n",
      "Gradient Descent(4/49): loss=114858.21369116205\n",
      "Gradient Descent(5/49): loss=7607099.618401348\n",
      "Gradient Descent(6/49): loss=778123193.296849\n",
      "Gradient Descent(7/49): loss=97393783287.68921\n",
      "Gradient Descent(8/49): loss=12938972496163.174\n",
      "Gradient Descent(9/49): loss=1744711217732195.2\n",
      "Gradient Descent(10/49): loss=2.36093457802093e+17\n",
      "Gradient Descent(11/49): loss=3.197466520843032e+19\n",
      "Gradient Descent(12/49): loss=4.331246955481693e+21\n",
      "Gradient Descent(13/49): loss=5.8673203355156423e+23\n",
      "Gradient Descent(14/49): loss=7.948246422179838e+25\n",
      "Gradient Descent(15/49): loss=1.0767228803558048e+28\n",
      "Gradient Descent(16/49): loss=1.458602042806482e+30\n",
      "Gradient Descent(17/49): loss=1.975921802373187e+32\n",
      "Gradient Descent(18/49): loss=2.676718516451437e+34\n",
      "Gradient Descent(19/49): loss=3.626065597523305e+36\n",
      "Gradient Descent(20/49): loss=4.91211595849886e+38\n",
      "Gradient Descent(21/49): loss=6.654287560695633e+40\n",
      "Gradient Descent(22/49): loss=9.014352128266547e+42\n",
      "Gradient Descent(23/49): loss=1.2211456681587524e+45\n",
      "Gradient Descent(24/49): loss=1.6542472732953778e+47\n",
      "Gradient Descent(25/49): loss=2.2409562696437412e+49\n",
      "Gradient Descent(26/49): loss=3.0357523228394064e+51\n",
      "Gradient Descent(27/49): loss=4.1124373065479066e+53\n",
      "Gradient Descent(28/49): loss=5.57098827629957e+55\n",
      "Gradient Descent(29/49): loss=7.546840975606335e+57\n",
      "Gradient Descent(30/49): loss=1.0223465907007915e+60\n",
      "Gradient Descent(31/49): loss=1.3849404736311418e+62\n",
      "Gradient Descent(32/49): loss=1.8761348968620328e+64\n",
      "Gradient Descent(33/49): loss=2.5415403898153905e+66\n",
      "Gradient Descent(34/49): loss=3.442944088864054e+68\n",
      "Gradient Descent(35/49): loss=4.6640470663168414e+70\n",
      "Gradient Descent(36/49): loss=6.318236507870809e+72\n",
      "Gradient Descent(37/49): loss=8.559114434690944e+74\n",
      "Gradient Descent(38/49): loss=1.1594760628994512e+77\n",
      "Gradient Descent(39/49): loss=1.570705416658381e+79\n",
      "Gradient Descent(40/49): loss=2.127784768363857e+81\n",
      "Gradient Descent(41/49): loss=2.8824424825078243e+83\n",
      "Gradient Descent(42/49): loss=3.904753332431546e+85\n",
      "Gradient Descent(43/49): loss=5.28964538916665e+87\n",
      "Gradient Descent(44/49): loss=7.165714697196635e+89\n",
      "Gradient Descent(45/49): loss=9.707166235903115e+91\n",
      "Gradient Descent(46/49): loss=1.3149989960990214e+94\n",
      "Gradient Descent(47/49): loss=1.781387397431938e+96\n",
      "Gradient Descent(48/49): loss=2.4131889599483137e+98\n",
      "Gradient Descent(49/49): loss=3.2690704811383184e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.842776837852251\n",
      "Gradient Descent(2/49): loss=75.49455848725519\n",
      "Gradient Descent(3/49): loss=2491.265049871487\n",
      "Gradient Descent(4/49): loss=98013.49193886883\n",
      "Gradient Descent(5/49): loss=5758236.662977021\n",
      "Gradient Descent(6/49): loss=547163981.8964002\n",
      "Gradient Descent(7/49): loss=67455785873.633156\n",
      "Gradient Descent(8/49): loss=9024700556145.48\n",
      "Gradient Descent(9/49): loss=1232421735622329.8\n",
      "Gradient Descent(10/49): loss=1.6911583591542995e+17\n",
      "Gradient Descent(11/49): loss=2.3232479304298287e+19\n",
      "Gradient Descent(12/49): loss=3.1924136231267866e+21\n",
      "Gradient Descent(13/49): loss=4.3870104279388264e+23\n",
      "Gradient Descent(14/49): loss=6.0287067085878545e+25\n",
      "Gradient Descent(15/49): loss=8.284780851897547e+27\n",
      "Gradient Descent(16/49): loss=1.1385135711335694e+30\n",
      "Gradient Descent(17/49): loss=1.5645717054557432e+32\n",
      "Gradient Descent(18/49): loss=2.150070740472994e+34\n",
      "Gradient Descent(19/49): loss=2.954677126323465e+36\n",
      "Gradient Descent(20/49): loss=4.06038590944479e+38\n",
      "Gradient Descent(21/49): loss=5.579876594509703e+40\n",
      "Gradient Descent(22/49): loss=7.667995975695705e+42\n",
      "Gradient Descent(23/49): loss=1.0537538113770421e+45\n",
      "Gradient Descent(24/49): loss=1.4480929548185402e+47\n",
      "Gradient Descent(25/49): loss=1.9900029619418854e+49\n",
      "Gradient Descent(26/49): loss=2.7347082763999334e+51\n",
      "Gradient Descent(27/49): loss=3.7580996109237413e+53\n",
      "Gradient Descent(28/49): loss=5.164467745063411e+55\n",
      "Gradient Descent(29/49): loss=7.097131489616038e+57\n",
      "Gradient Descent(30/49): loss=9.75304288211432e+59\n",
      "Gradient Descent(31/49): loss=1.3402857985586818e+62\n",
      "Gradient Descent(32/49): loss=1.841851864624082e+64\n",
      "Gradient Descent(33/49): loss=2.531115598529324e+66\n",
      "Gradient Descent(34/49): loss=3.478317825753081e+68\n",
      "Gradient Descent(35/49): loss=4.779985119597612e+70\n",
      "Gradient Descent(36/49): loss=6.568766538356294e+72\n",
      "Gradient Descent(37/49): loss=9.026951497928916e+74\n",
      "Gradient Descent(38/49): loss=1.2405046346243003e+77\n",
      "Gradient Descent(39/49): loss=1.7047302723155347e+79\n",
      "Gradient Descent(40/49): loss=2.342679922537454e+81\n",
      "Gradient Descent(41/49): loss=3.2193651444961773e+83\n",
      "Gradient Descent(42/49): loss=4.42412633236342e+85\n",
      "Gradient Descent(43/49): loss=6.079737130214323e+87\n",
      "Gradient Descent(44/49): loss=8.354915930431867e+89\n",
      "Gradient Descent(45/49): loss=1.1481519465319748e+92\n",
      "Gradient Descent(46/49): loss=1.5778170639917618e+94\n",
      "Gradient Descent(47/49): loss=2.168272844847014e+96\n",
      "Gradient Descent(48/49): loss=2.979690888756614e+98\n",
      "Gradient Descent(49/49): loss=4.094760405102866e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.8963438692919445\n",
      "Gradient Descent(2/49): loss=78.55277880307398\n",
      "Gradient Descent(3/49): loss=2739.029055979311\n",
      "Gradient Descent(4/49): loss=126259.58062699407\n",
      "Gradient Descent(5/49): loss=9537533.477834215\n",
      "Gradient Descent(6/49): loss=1079574066.2377174\n",
      "Gradient Descent(7/49): loss=143492520984.357\n",
      "Gradient Descent(8/49): loss=19916066854292.79\n",
      "Gradient Descent(9/49): loss=2792728686391969.5\n",
      "Gradient Descent(10/49): loss=3.9253063698779904e+17\n",
      "Gradient Descent(11/49): loss=5.5201409587665584e+19\n",
      "Gradient Descent(12/49): loss=7.763890081511162e+21\n",
      "Gradient Descent(13/49): loss=1.0919946756973574e+24\n",
      "Gradient Descent(14/49): loss=1.535905056000703e+26\n",
      "Gradient Descent(15/49): loss=2.1602739720356757e+28\n",
      "Gradient Descent(16/49): loss=3.038459384070667e+30\n",
      "Gradient Descent(17/49): loss=4.2736413156416504e+32\n",
      "Gradient Descent(18/49): loss=6.010944392328815e+34\n",
      "Gradient Descent(19/49): loss=8.454488796379424e+36\n",
      "Gradient Descent(20/49): loss=1.1891372836517527e+39\n",
      "Gradient Descent(21/49): loss=1.6725404855327555e+41\n",
      "Gradient Descent(22/49): loss=2.3524547706364616e+43\n",
      "Gradient Descent(23/49): loss=3.308765016939134e+45\n",
      "Gradient Descent(24/49): loss=4.653830574767182e+47\n",
      "Gradient Descent(25/49): loss=6.545686655828024e+49\n",
      "Gradient Descent(26/49): loss=9.206612296674126e+51\n",
      "Gradient Descent(27/49): loss=1.2949246494377186e+54\n",
      "Gradient Descent(28/49): loss=1.821332096635783e+56\n",
      "Gradient Descent(29/49): loss=2.5617325360792945e+58\n",
      "Gradient Descent(30/49): loss=3.603117519604992e+60\n",
      "Gradient Descent(31/49): loss=5.067842047223912e+62\n",
      "Gradient Descent(32/49): loss=7.128000370753031e+64\n",
      "Gradient Descent(33/49): loss=1.002564579006296e+67\n",
      "Gradient Descent(34/49): loss=1.4101230117807674e+69\n",
      "Gradient Descent(35/49): loss=1.9833604238487775e+71\n",
      "Gradient Descent(36/49): loss=2.789627953040684e+73\n",
      "Gradient Descent(37/49): loss=3.923656044968831e+75\n",
      "Gradient Descent(38/49): loss=5.518684576715556e+77\n",
      "Gradient Descent(39/49): loss=7.762117552666243e+79\n",
      "Gradient Descent(40/49): loss=1.091754168296147e+82\n",
      "Gradient Descent(41/49): loss=1.5355695863980186e+84\n",
      "Gradient Descent(42/49): loss=2.1598030244762274e+86\n",
      "Gradient Descent(43/49): loss=3.0377972746118474e+88\n",
      "Gradient Descent(44/49): loss=4.2727101393317735e+90\n",
      "Gradient Descent(45/49): loss=6.0096347071354e+92\n",
      "Gradient Descent(46/49): loss=8.452646712621281e+94\n",
      "Gradient Descent(47/49): loss=1.1888781919399777e+97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(48/49): loss=1.6721760690174752e+99\n",
      "Gradient Descent(49/49): loss=2.3519422130471178e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9199838155292515\n",
      "Gradient Descent(2/49): loss=80.1548220006614\n",
      "Gradient Descent(3/49): loss=2786.919863029551\n",
      "Gradient Descent(4/49): loss=123096.03295887909\n",
      "Gradient Descent(5/49): loss=8551881.51664186\n",
      "Gradient Descent(6/49): loss=903272620.6150771\n",
      "Gradient Descent(7/49): loss=115017125621.47284\n",
      "Gradient Descent(8/49): loss=15464688247962.438\n",
      "Gradient Descent(9/49): loss=2107700317859112.0\n",
      "Gradient Descent(10/49): loss=2.8819268718627216e+17\n",
      "Gradient Descent(11/49): loss=3.943569114621106e+19\n",
      "Gradient Descent(12/49): loss=5.397272273764728e+21\n",
      "Gradient Descent(13/49): loss=7.387162702421162e+23\n",
      "Gradient Descent(14/49): loss=1.011079610283209e+26\n",
      "Gradient Descent(15/49): loss=1.3838660825920082e+28\n",
      "Gradient Descent(16/49): loss=1.8941005039564615e+30\n",
      "Gradient Descent(17/49): loss=2.5924598009660268e+32\n",
      "Gradient Descent(18/49): loss=3.5483059177666924e+34\n",
      "Gradient Descent(19/49): loss=4.856574814739483e+36\n",
      "Gradient Descent(20/49): loss=6.647205601259466e+38\n",
      "Gradient Descent(21/49): loss=9.098046258681102e+40\n",
      "Gradient Descent(22/49): loss=1.2452517749306446e+43\n",
      "Gradient Descent(23/49): loss=1.7043790929542038e+45\n",
      "Gradient Descent(24/49): loss=2.332787755051762e+47\n",
      "Gradient Descent(25/49): loss=3.1928921990551696e+49\n",
      "Gradient Descent(26/49): loss=4.370119215823908e+51\n",
      "Gradient Descent(27/49): loss=5.981392659033678e+53\n",
      "Gradient Descent(28/49): loss=8.186746487829625e+55\n",
      "Gradient Descent(29/49): loss=1.1205219566176517e+58\n",
      "Gradient Descent(30/49): loss=1.5336610912880141e+60\n",
      "Gradient Descent(31/49): loss=2.0991256164499343e+62\n",
      "Gradient Descent(32/49): loss=2.873078269160321e+64\n",
      "Gradient Descent(33/49): loss=3.932389122420106e+66\n",
      "Gradient Descent(34/49): loss=5.382270429634811e+68\n",
      "Gradient Descent(35/49): loss=7.366726454551265e+70\n",
      "Gradient Descent(36/49): loss=1.0082856178571176e+73\n",
      "Gradient Descent(37/49): loss=1.38004294505796e+75\n",
      "Gradient Descent(38/49): loss=1.8888680910195594e+77\n",
      "Gradient Descent(39/49): loss=2.585298289483341e+79\n",
      "Gradient Descent(40/49): loss=3.538503973561123e+81\n",
      "Gradient Descent(41/49): loss=4.843158881062698e+83\n",
      "Gradient Descent(42/49): loss=6.628843184146692e+85\n",
      "Gradient Descent(43/49): loss=9.07291357544023e+87\n",
      "Gradient Descent(44/49): loss=1.2418118585800544e+90\n",
      "Gradient Descent(45/49): loss=1.699670871201102e+92\n",
      "Gradient Descent(46/49): loss=2.3263436006422727e+94\n",
      "Gradient Descent(47/49): loss=3.1840720694501384e+96\n",
      "Gradient Descent(48/49): loss=4.358047083265488e+98\n",
      "Gradient Descent(49/49): loss=5.964869502227816e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9736769820705433\n",
      "Gradient Descent(2/49): loss=81.42966046895499\n",
      "Gradient Descent(3/49): loss=2796.8811752817423\n",
      "Gradient Descent(4/49): loss=120101.88386881493\n",
      "Gradient Descent(5/49): loss=8031222.8176006125\n",
      "Gradient Descent(6/49): loss=829492050.149758\n",
      "Gradient Descent(7/49): loss=104887051465.7239\n",
      "Gradient Descent(8/49): loss=14080878356437.525\n",
      "Gradient Descent(9/49): loss=1918795571473213.2\n",
      "Gradient Descent(10/49): loss=2.6240636239428496e+17\n",
      "Gradient Descent(11/49): loss=3.591570264717517e+19\n",
      "Gradient Descent(12/49): loss=4.916770889574042e+21\n",
      "Gradient Descent(13/49): loss=6.731249257779132e+23\n",
      "Gradient Descent(14/49): loss=9.215440332970444e+25\n",
      "Gradient Descent(15/49): loss=1.2616462981624685e+28\n",
      "Gradient Descent(16/49): loss=1.7272667142433363e+30\n",
      "Gradient Descent(17/49): loss=2.364728312255602e+32\n",
      "Gradient Descent(18/49): loss=3.237450319108061e+34\n",
      "Gradient Descent(19/49): loss=4.432257437573755e+36\n",
      "Gradient Descent(20/49): loss=6.068017758505875e+38\n",
      "Gradient Descent(21/49): loss=8.307468609812127e+40\n",
      "Gradient Descent(22/49): loss=1.1373406844959176e+43\n",
      "Gradient Descent(23/49): loss=1.557085429263232e+45\n",
      "Gradient Descent(24/49): loss=2.1317403545726607e+47\n",
      "Gradient Descent(25/49): loss=2.9184763108789975e+49\n",
      "Gradient Descent(26/49): loss=3.995563511707152e+51\n",
      "Gradient Descent(27/49): loss=5.470158423618962e+53\n",
      "Gradient Descent(28/49): loss=7.48896447067223e+55\n",
      "Gradient Descent(29/49): loss=1.0252827157771252e+58\n",
      "Gradient Descent(30/49): loss=1.4036715641901963e+60\n",
      "Gradient Descent(31/49): loss=1.9217078663251592e+62\n",
      "Gradient Descent(32/49): loss=2.6309296403155733e+64\n",
      "Gradient Descent(33/49): loss=3.601895425201789e+66\n",
      "Gradient Descent(34/49): loss=4.931203957447274e+68\n",
      "Gradient Descent(35/49): loss=6.751104515640195e+70\n",
      "Gradient Descent(36/49): loss=9.242654040351828e+72\n",
      "Gradient Descent(37/49): loss=1.2653729995102665e+75\n",
      "Gradient Descent(38/49): loss=1.7323691018825964e+77\n",
      "Gradient Descent(39/49): loss=2.3717138790847594e+79\n",
      "Gradient Descent(40/49): loss=3.2470139984200293e+81\n",
      "Gradient Descent(41/49): loss=4.445350680329346e+83\n",
      "Gradient Descent(42/49): loss=6.085943171393757e+85\n",
      "Gradient Descent(43/49): loss=8.332009541863532e+87\n",
      "Gradient Descent(44/49): loss=1.1407004806094309e+90\n",
      "Gradient Descent(45/49): loss=1.5616851852183407e+92\n",
      "Gradient Descent(46/49): loss=2.1380376875333614e+94\n",
      "Gradient Descent(47/49): loss=2.9270977253165176e+96\n",
      "Gradient Descent(48/49): loss=4.0073667286182684e+98\n",
      "Gradient Descent(49/49): loss=5.486317712846367e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.871590709768501\n",
      "Gradient Descent(2/49): loss=77.23825185547912\n",
      "Gradient Descent(3/49): loss=2577.909605269512\n",
      "Gradient Descent(4/49): loss=102516.02499658379\n",
      "Gradient Descent(5/49): loss=6081386.2400910165\n",
      "Gradient Descent(6/49): loss=583375738.3364195\n",
      "Gradient Descent(7/49): loss=72646952605.11809\n",
      "Gradient Descent(8/49): loss=9820844875055.594\n",
      "Gradient Descent(9/49): loss=1355323379792795.2\n",
      "Gradient Descent(10/49): loss=1.8795299184047645e+17\n",
      "Gradient Descent(11/49): loss=2.6094295610421477e+19\n",
      "Gradient Descent(12/49): loss=3.623724532012544e+21\n",
      "Gradient Descent(13/49): loss=5.0325828293485054e+23\n",
      "Gradient Descent(14/49): loss=6.989284623872662e+25\n",
      "Gradient Descent(15/49): loss=9.706796303761012e+27\n",
      "Gradient Descent(16/49): loss=1.3480916775351333e+30\n",
      "Gradient Descent(17/49): loss=1.8722464397458153e+32\n",
      "Gradient Descent(18/49): loss=2.6001991764214445e+34\n",
      "Gradient Descent(19/49): loss=3.611189037523499e+36\n",
      "Gradient Descent(20/49): loss=5.015264372996644e+38\n",
      "Gradient Descent(21/49): loss=6.965261713502895e+40\n",
      "Gradient Descent(22/49): loss=9.673442342193078e+42\n",
      "Gradient Descent(23/49): loss=1.3434597377547605e+45\n",
      "Gradient Descent(24/49): loss=1.8658136402038573e+47\n",
      "Gradient Descent(25/49): loss=2.59126525502994e+49\n",
      "Gradient Descent(26/49): loss=3.598781505956733e+51\n",
      "Gradient Descent(27/49): loss=4.998032641574346e+53\n",
      "Gradient Descent(28/49): loss=6.941330070996391e+55\n",
      "Gradient Descent(29/49): loss=9.640205778916783e+57\n",
      "Gradient Descent(30/49): loss=1.3388438024028282e+60\n",
      "Gradient Descent(31/49): loss=1.8594029716177564e+62\n",
      "Gradient Descent(32/49): loss=2.5823620385410515e+64\n",
      "Gradient Descent(33/49): loss=3.586416607851238e+66\n",
      "Gradient Descent(34/49): loss=4.980860116863301e+68\n",
      "Gradient Descent(35/49): loss=6.917480654492114e+70\n",
      "Gradient Descent(36/49): loss=9.607083411811861e+72\n",
      "Gradient Descent(37/49): loss=1.3342437267471798e+75\n",
      "Gradient Descent(38/49): loss=1.853014329172403e+77\n",
      "Gradient Descent(39/49): loss=2.5734894122299876e+79\n",
      "Gradient Descent(40/49): loss=3.5740941937658005e+81\n",
      "Gradient Descent(41/49): loss=4.963746594489158e+83\n",
      "Gradient Descent(42/49): loss=6.893713181168976e+85\n",
      "Gradient Descent(43/49): loss=9.574074848418618e+87\n",
      "Gradient Descent(44/49): loss=1.3296594562929009e+90\n",
      "Gradient Descent(45/49): loss=1.8466476371877988e+92\n",
      "Gradient Descent(46/49): loss=2.5646472709927726e+94\n",
      "Gradient Descent(47/49): loss=3.5618141177312515e+96\n",
      "Gradient Descent(48/49): loss=4.946691871728046e+98\n",
      "Gradient Descent(49/49): loss=6.870027369481781e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.925614255419625\n",
      "Gradient Descent(2/49): loss=80.35558656331473\n",
      "Gradient Descent(3/49): loss=2833.3322136322395\n",
      "Gradient Descent(4/49): loss=131958.80125241575\n",
      "Gradient Descent(5/49): loss=10063014.871213604\n",
      "Gradient Descent(6/49): loss=1150214379.9896572\n",
      "Gradient Descent(7/49): loss=154452806458.00797\n",
      "Gradient Descent(8/49): loss=21661859754581.48\n",
      "Gradient Descent(9/49): loss=3069529326093770.5\n",
      "Gradient Descent(10/49): loss=4.359877070654472e+17\n",
      "Gradient Descent(11/49): loss=6.195983442627505e+19\n",
      "Gradient Descent(12/49): loss=8.806418083912511e+21\n",
      "Gradient Descent(13/49): loss=1.251700411187361e+24\n",
      "Gradient Descent(14/49): loss=1.7791158319304022e+26\n",
      "Gradient Descent(15/49): loss=2.528766180178731e+28\n",
      "Gradient Descent(16/49): loss=3.594291250842177e+30\n",
      "Gradient Descent(17/49): loss=5.108788091514406e+32\n",
      "Gradient Descent(18/49): loss=7.26143608822563e+34\n",
      "Gradient Descent(19/49): loss=1.032112769585059e+37\n",
      "Gradient Descent(20/49): loss=1.4670056406417264e+39\n",
      "Gradient Descent(21/49): loss=2.0851457452279653e+41\n",
      "Gradient Descent(22/49): loss=2.963746463260731e+43\n",
      "Gradient Descent(23/49): loss=4.212555941799878e+45\n",
      "Gradient Descent(24/49): loss=5.987566002292365e+47\n",
      "Gradient Descent(25/49): loss=8.510497457396112e+49\n",
      "Gradient Descent(26/49): loss=1.209649579555768e+52\n",
      "Gradient Descent(27/49): loss=1.7193496768486112e+54\n",
      "Gradient Descent(28/49): loss=2.4438179132547507e+56\n",
      "Gradient Descent(29/49): loss=3.4735493736744376e+58\n",
      "Gradient Descent(30/49): loss=4.937170312858866e+60\n",
      "Gradient Descent(31/49): loss=7.017505172925982e+62\n",
      "Gradient Descent(32/49): loss=9.974413627940526e+64\n",
      "Gradient Descent(33/49): loss=1.4177250286195896e+67\n",
      "Gradient Descent(34/49): loss=2.0151001670354933e+69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=2.8641863557563936e+71\n",
      "Gradient Descent(36/49): loss=4.0710450104172896e+73\n",
      "Gradient Descent(37/49): loss=5.786427773295938e+75\n",
      "Gradient Descent(38/49): loss=8.224607266657917e+77\n",
      "Gradient Descent(39/49): loss=1.1690142405809748e+80\n",
      "Gradient Descent(40/49): loss=1.6615921592040664e+82\n",
      "Gradient Descent(41/49): loss=2.3617235852972005e+84\n",
      "Gradient Descent(42/49): loss=3.356863633745644e+86\n",
      "Gradient Descent(43/49): loss=4.771317662115746e+88\n",
      "Gradient Descent(44/49): loss=6.7817685544217795e+90\n",
      "Gradient Descent(45/49): loss=9.63934660878357e+92\n",
      "Gradient Descent(46/49): loss=1.370099883218273e+95\n",
      "Gradient Descent(47/49): loss=1.9474076057024168e+97\n",
      "Gradient Descent(48/49): loss=2.767970736439742e+99\n",
      "Gradient Descent(49/49): loss=3.934287806698376e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.949514193010157\n",
      "Gradient Descent(2/49): loss=81.99329481087064\n",
      "Gradient Descent(3/49): loss=2882.933766318362\n",
      "Gradient Descent(4/49): loss=128672.29724045383\n",
      "Gradient Descent(5/49): loss=9025152.8624187\n",
      "Gradient Descent(6/49): loss=962534411.5298187\n",
      "Gradient Descent(7/49): loss=123816346759.44345\n",
      "Gradient Descent(8/49): loss=16821967442910.088\n",
      "Gradient Descent(9/49): loss=2316844740114162.5\n",
      "Gradient Descent(10/49): loss=3.201345523473289e+17\n",
      "Gradient Descent(11/49): loss=4.426933711742924e+19\n",
      "Gradient Descent(12/49): loss=6.122834266268303e+21\n",
      "Gradient Descent(13/49): loss=8.468775577787787e+23\n",
      "Gradient Descent(14/49): loss=1.17136741494587e+26\n",
      "Gradient Descent(15/49): loss=1.620192767905579e+28\n",
      "Gradient Descent(16/49): loss=2.2409929185706496e+30\n",
      "Gradient Descent(17/49): loss=3.099661982666788e+32\n",
      "Gradient Descent(18/49): loss=4.287342734416608e+34\n",
      "Gradient Descent(19/49): loss=5.930100753150506e+36\n",
      "Gradient Descent(20/49): loss=8.202305526305763e+38\n",
      "Gradient Descent(21/49): loss=1.1345138771666307e+41\n",
      "Gradient Descent(22/49): loss=1.5692194511410642e+43\n",
      "Gradient Descent(23/49): loss=2.170488819445288e+45\n",
      "Gradient Descent(24/49): loss=3.0021433343415325e+47\n",
      "Gradient Descent(25/49): loss=4.152458432034791e+49\n",
      "Gradient Descent(26/49): loss=5.743533572344955e+51\n",
      "Gradient Descent(27/49): loss=7.944252407721283e+53\n",
      "Gradient Descent(28/49): loss=1.0988208830443268e+56\n",
      "Gradient Descent(29/49): loss=1.5198501646810495e+58\n",
      "Gradient Descent(30/49): loss=2.1022029693149247e+60\n",
      "Gradient Descent(31/49): loss=2.907692762676953e+62\n",
      "Gradient Descent(32/49): loss=4.021817743354785e+64\n",
      "Gradient Descent(33/49): loss=5.562835994361448e+66\n",
      "Gradient Descent(34/49): loss=7.69431791166914e+68\n",
      "Gradient Descent(35/49): loss=1.0642508279201268e+71\n",
      "Gradient Descent(36/49): loss=1.4720340876622097e+73\n",
      "Gradient Descent(37/49): loss=2.036065463509466e+75\n",
      "Gradient Descent(38/49): loss=2.816213704860374e+77\n",
      "Gradient Descent(39/49): loss=3.8952871474835794e+79\n",
      "Gradient Descent(40/49): loss=5.387823351318728e+81\n",
      "Gradient Descent(41/49): loss=7.452246616470525e+83\n",
      "Gradient Descent(42/49): loss=1.0307683829148436e+86\n",
      "Gradient Descent(43/49): loss=1.4257223544758458e+88\n",
      "Gradient Descent(44/49): loss=1.972008712863303e+90\n",
      "Gradient Descent(45/49): loss=2.7276126739547016e+92\n",
      "Gradient Descent(46/49): loss=3.772737336599099e+94\n",
      "Gradient Descent(47/49): loss=5.218316789213338e+96\n",
      "Gradient Descent(48/49): loss=7.217791137596835e+98\n",
      "Gradient Descent(49/49): loss=9.983393306757443e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.003846628122081\n",
      "Gradient Descent(2/49): loss=83.29843231908995\n",
      "Gradient Descent(3/49): loss=2893.343252797809\n",
      "Gradient Descent(4/49): loss=125554.14549270169\n",
      "Gradient Descent(5/49): loss=8476542.681881763\n",
      "Gradient Descent(6/49): loss=883954618.9236591\n",
      "Gradient Descent(7/49): loss=112912848352.2451\n",
      "Gradient Descent(8/49): loss=15316752557543.646\n",
      "Gradient Descent(9/49): loss=2109195306612540.8\n",
      "Gradient Descent(10/49): loss=2.914899590721512e+17\n",
      "Gradient Descent(11/49): loss=4.031785742352573e+19\n",
      "Gradient Descent(12/49): loss=5.577731395147273e+21\n",
      "Gradient Descent(13/49): loss=7.716813933225197e+23\n",
      "Gradient Descent(14/49): loss=1.0676360411795318e+26\n",
      "Gradient Descent(15/49): loss=1.477098793089977e+28\n",
      "Gradient Descent(16/49): loss=2.0436010791890915e+30\n",
      "Gradient Descent(17/49): loss=2.8273707788832114e+32\n",
      "Gradient Descent(18/49): loss=3.9117349613521226e+34\n",
      "Gradient Descent(19/49): loss=5.411978733856781e+36\n",
      "Gradient Descent(20/49): loss=7.487601833750078e+38\n",
      "Gradient Descent(21/49): loss=1.0359275969499299e+41\n",
      "Gradient Descent(22/49): loss=1.4332305724305673e+43\n",
      "Gradient Descent(23/49): loss=1.982908728222702e+45\n",
      "Gradient Descent(24/49): loss=2.7434015852842117e+47\n",
      "Gradient Descent(25/49): loss=3.7955616166438007e+49\n",
      "Gradient Descent(26/49): loss=5.2512501498213324e+51\n",
      "Gradient Descent(27/49): loss=7.265230003138255e+53\n",
      "Gradient Descent(28/49): loss=1.005161923209803e+56\n",
      "Gradient Descent(29/49): loss=1.3906655280485474e+58\n",
      "Gradient Descent(30/49): loss=1.9240189727111574e+60\n",
      "Gradient Descent(31/49): loss=2.6619262020158065e+62\n",
      "Gradient Descent(32/49): loss=3.6828384779353555e+64\n",
      "Gradient Descent(33/49): loss=5.0952949951392145e+66\n",
      "Gradient Descent(34/49): loss=7.049462321802692e+68\n",
      "Gradient Descent(35/49): loss=9.753099491574994e+70\n",
      "Gradient Descent(36/49): loss=1.349364608962653e+73\n",
      "Gradient Descent(37/49): loss=1.866878164724667e+75\n",
      "Gradient Descent(38/49): loss=2.5828705294154116e+77\n",
      "Gradient Descent(39/49): loss=3.573463066726946e+79\n",
      "Gradient Descent(40/49): loss=4.94397150141005e+81\n",
      "Gradient Descent(41/49): loss=6.84010265401811e+83\n",
      "Gradient Descent(42/49): loss=9.463445390848682e+85\n",
      "Gradient Descent(43/49): loss=1.309290272317265e+88\n",
      "Gradient Descent(44/49): loss=1.8114343628403452e+90\n",
      "Gradient Descent(45/49): loss=2.506162705288717e+92\n",
      "Gradient Descent(46/49): loss=3.467335959958037e+94\n",
      "Gradient Descent(47/49): loss=4.797142114455469e+96\n",
      "Gradient Descent(48/49): loss=6.636960690293282e+98\n",
      "Gradient Descent(49/49): loss=9.18239363219248e+100\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9005733748130935\n",
      "Gradient Descent(2/49): loss=79.01214133153397\n",
      "Gradient Descent(3/49): loss=2667.0526284712664\n",
      "Gradient Descent(4/49): loss=107198.74191945702\n",
      "Gradient Descent(5/49): loss=6420794.194157483\n",
      "Gradient Descent(6/49): loss=621774214.7218622\n",
      "Gradient Descent(7/49): loss=78207158076.1452\n",
      "Gradient Descent(8/49): loss=10682479933303.824\n",
      "Gradient Descent(9/49): loss=1489737276917684.2\n",
      "Gradient Descent(10/49): loss=2.0877250893169267e+17\n",
      "Gradient Descent(11/49): loss=2.929075802402897e+19\n",
      "Gradient Descent(12/49): loss=4.1105706942183826e+21\n",
      "Gradient Descent(13/49): loss=5.768993260988991e+23\n",
      "Gradient Descent(14/49): loss=8.096625324766341e+25\n",
      "Gradient Descent(15/49): loss=1.1363430584899165e+28\n",
      "Gradient Descent(16/49): loss=1.5948329871947932e+30\n",
      "Gradient Descent(17/49): loss=2.2383141068058575e+32\n",
      "Gradient Descent(18/49): loss=3.1414262694994275e+34\n",
      "Gradient Descent(19/49): loss=4.4089250333792316e+36\n",
      "Gradient Descent(20/49): loss=6.187832635219621e+38\n",
      "Gradient Descent(21/49): loss=8.68449167326191e+40\n",
      "Gradient Descent(22/49): loss=1.218849960522523e+43\n",
      "Gradient Descent(23/49): loss=1.7106300313221937e+45\n",
      "Gradient Descent(24/49): loss=2.4008329153366993e+47\n",
      "Gradient Descent(25/49): loss=3.369518003210176e+49\n",
      "Gradient Descent(26/49): loss=4.729046949262239e+51\n",
      "Gradient Descent(27/49): loss=6.637116948780659e+53\n",
      "Gradient Descent(28/49): loss=9.315052665878997e+55\n",
      "Gradient Descent(29/49): loss=1.3073478565725985e+58\n",
      "Gradient Descent(30/49): loss=1.8348349487551314e+60\n",
      "Gradient Descent(31/49): loss=2.5751518788575226e+62\n",
      "Gradient Descent(32/49): loss=3.6141709659949103e+64\n",
      "Gradient Descent(33/49): loss=5.0724121861253435e+66\n",
      "Gradient Descent(34/49): loss=7.119022765673271e+68\n",
      "Gradient Descent(35/49): loss=9.991397244254107e+70\n",
      "Gradient Descent(36/49): loss=1.4022713815980633e+73\n",
      "Gradient Descent(37/49): loss=1.9680580999617066e+75\n",
      "Gradient Descent(38/49): loss=2.7621277419288836e+77\n",
      "Gradient Descent(39/49): loss=3.8765876184658435e+79\n",
      "Gradient Descent(40/49): loss=5.440708384163499e+81\n",
      "Gradient Descent(41/49): loss=7.635918657043311e+83\n",
      "Gradient Descent(42/49): loss=1.071684964897233e+86\n",
      "Gradient Descent(43/49): loss=1.504087085746236e+88\n",
      "Gradient Descent(44/49): loss=2.1109542781777674e+90\n",
      "Gradient Descent(45/49): loss=2.9626794929538323e+92\n",
      "Gradient Descent(46/49): loss=4.158057741329213e+94\n",
      "Gradient Descent(47/49): loss=5.835745723203343e+96\n",
      "Gradient Descent(48/49): loss=8.190345171830359e+98\n",
      "Gradient Descent(49/49): loss=1.1494975486509092e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9550550536139535\n",
      "Gradient Descent(2/49): loss=82.18941311859366\n",
      "Gradient Descent(3/49): loss=2930.32827228263\n",
      "Gradient Descent(4/49): loss=137882.23591196886\n",
      "Gradient Descent(5/49): loss=10614446.752208646\n",
      "Gradient Descent(6/49): loss=1225071024.4039645\n",
      "Gradient Descent(7/49): loss=166186376442.6839\n",
      "Gradient Descent(8/49): loss=23550348695800.51\n",
      "Gradient Descent(9/49): loss=3372100338841422.0\n",
      "Gradient Descent(10/49): loss=4.839904401489821e+17\n",
      "Gradient Descent(11/49): loss=6.950377131631146e+19\n",
      "Gradient Descent(12/49): loss=9.982365055560396e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=1.433740922557773e+24\n",
      "Gradient Descent(14/49): loss=2.059257595850103e+26\n",
      "Gradient Descent(15/49): loss=2.957680782738082e+28\n",
      "Gradient Descent(16/49): loss=4.2480739187846724e+30\n",
      "Gradient Descent(17/49): loss=6.101447279632557e+32\n",
      "Gradient Descent(18/49): loss=8.76342084647393e+34\n",
      "Gradient Descent(19/49): loss=1.2586775187683027e+37\n",
      "Gradient Descent(20/49): loss=1.8078203995837732e+39\n",
      "Gradient Descent(21/49): loss=2.596546414048457e+41\n",
      "Gradient Descent(22/49): loss=3.7293822340785646e+43\n",
      "Gradient Descent(23/49): loss=5.35645801390415e+45\n",
      "Gradient Descent(24/49): loss=7.693403532789947e+47\n",
      "Gradient Descent(25/49): loss=1.1049924738462687e+50\n",
      "Gradient Descent(26/49): loss=1.5870847825062162e+52\n",
      "Gradient Descent(27/49): loss=2.2795070251431737e+54\n",
      "Gradient Descent(28/49): loss=3.2740231240020804e+56\n",
      "Gradient Descent(29/49): loss=4.702432279552663e+58\n",
      "Gradient Descent(30/49): loss=6.754035786023588e+60\n",
      "Gradient Descent(31/49): loss=9.700724367098642e+62\n",
      "Gradient Descent(32/49): loss=1.3933010755014624e+65\n",
      "Gradient Descent(33/49): loss=2.0011782765188605e+67\n",
      "Gradient Descent(34/49): loss=2.874263549225845e+69\n",
      "Gradient Descent(35/49): loss=4.128263357315362e+71\n",
      "Gradient Descent(36/49): loss=5.929365228857774e+73\n",
      "Gradient Descent(37/49): loss=8.516261917953481e+75\n",
      "Gradient Descent(38/49): loss=1.223178439105112e+78\n",
      "Gradient Descent(39/49): loss=1.7568335829802313e+80\n",
      "Gradient Descent(40/49): loss=2.523314783528431e+82\n",
      "Gradient Descent(41/49): loss=3.6242006974685545e+84\n",
      "Gradient Descent(42/49): loss=5.205387287100497e+86\n",
      "Gradient Descent(43/49): loss=7.476422822729867e+88\n",
      "Gradient Descent(44/49): loss=1.073827846830009e+91\n",
      "Gradient Descent(45/49): loss=1.5423234773746666e+93\n",
      "Gradient Descent(46/49): loss=2.2152170069749257e+95\n",
      "Gradient Descent(47/49): loss=3.1816842964381265e+97\n",
      "Gradient Descent(48/49): loss=4.569807350849554e+99\n",
      "Gradient Descent(49/49): loss=6.563548510220699e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.979216385787979\n",
      "Gradient Descent(2/49): loss=83.8633767609974\n",
      "Gradient Descent(3/49): loss=2981.690840842687\n",
      "Gradient Descent(4/49): loss=134468.68231526282\n",
      "Gradient Descent(5/49): loss=9521901.344972761\n",
      "Gradient Descent(6/49): loss=1025343095.0026523\n",
      "Gradient Descent(7/49): loss=133237385353.69229\n",
      "Gradient Descent(8/49): loss=18290324865330.355\n",
      "Gradient Descent(9/49): loss=2545483008007449.0\n",
      "Gradient Descent(10/49): loss=3.55421333144459e+17\n",
      "Gradient Descent(11/49): loss=4.9665414658166145e+19\n",
      "Gradient Descent(12/49): loss=6.941355713912342e+21\n",
      "Gradient Descent(13/49): loss=9.701822815857911e+23\n",
      "Gradient Descent(14/49): loss=1.356022247511185e+26\n",
      "Gradient Descent(15/49): loss=1.895314731053023e+28\n",
      "Gradient Descent(16/49): loss=2.6490863116478147e+30\n",
      "Gradient Descent(17/49): loss=3.7026353006967326e+32\n",
      "Gradient Descent(18/49): loss=5.175183814981838e+34\n",
      "Gradient Descent(19/49): loss=7.233369085570583e+36\n",
      "Gradient Descent(20/49): loss=1.011010048937615e+39\n",
      "Gradient Descent(21/49): loss=1.4130916138617745e+41\n",
      "Gradient Descent(22/49): loss=1.9750821581458795e+43\n",
      "Gradient Descent(23/49): loss=2.7605779365259918e+45\n",
      "Gradient Descent(24/49): loss=3.858467614749595e+47\n",
      "Gradient Descent(25/49): loss=5.392991133163362e+49\n",
      "Gradient Descent(26/49): loss=7.537799008914199e+51\n",
      "Gradient Descent(27/49): loss=1.053560306253726e+54\n",
      "Gradient Descent(28/49): loss=1.472564229426632e+56\n",
      "Gradient Descent(29/49): loss=2.0582072017286576e+58\n",
      "Gradient Descent(30/49): loss=2.8767620458206765e+60\n",
      "Gradient Descent(31/49): loss=4.020858473978657e+62\n",
      "Gradient Descent(32/49): loss=5.619965297878289e+64\n",
      "Gradient Descent(33/49): loss=7.855041442954367e+66\n",
      "Gradient Descent(34/49): loss=1.0979013712740296e+69\n",
      "Gradient Descent(35/49): loss=1.5345398618189972e+71\n",
      "Gradient Descent(36/49): loss=2.1448307189733862e+73\n",
      "Gradient Descent(37/49): loss=2.9978359816595463e+75\n",
      "Gradient Descent(38/49): loss=4.190083857636234e+77\n",
      "Gradient Descent(39/49): loss=5.856492096777311e+79\n",
      "Gradient Descent(40/49): loss=8.185635620897653e+81\n",
      "Gradient Descent(41/49): loss=1.1441086133280783e+84\n",
      "Gradient Descent(42/49): loss=1.599123855146593e+86\n",
      "Gradient Descent(43/49): loss=2.2350999496981654e+88\n",
      "Gradient Descent(44/49): loss=3.1240055415737633e+90\n",
      "Gradient Descent(45/49): loss=4.36643140952232e+92\n",
      "Gradient Descent(46/49): loss=6.102973570417682e+94\n",
      "Gradient Descent(47/49): loss=8.530143475972172e+96\n",
      "Gradient Descent(48/49): loss=1.1922605739826243e+99\n",
      "Gradient Descent(49/49): loss=1.6664259871799774e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.034191747655378\n",
      "Gradient Descent(2/49): loss=85.1993489864836\n",
      "Gradient Descent(3/49): loss=2992.564153627885\n",
      "Gradient Descent(4/49): loss=131222.08086551024\n",
      "Gradient Descent(5/49): loss=8943997.13977973\n",
      "Gradient Descent(6/49): loss=941679481.5590414\n",
      "Gradient Descent(7/49): loss=121505926518.91722\n",
      "Gradient Descent(8/49): loss=16653773591065.973\n",
      "Gradient Descent(9/49): loss=2317341766631084.5\n",
      "Gradient Descent(10/49): loss=3.2361913262493677e+17\n",
      "Gradient Descent(11/49): loss=4.523223129578433e+19\n",
      "Gradient Descent(12/49): loss=6.323373670509444e+21\n",
      "Gradient Descent(13/49): loss=8.840363969757362e+23\n",
      "Gradient Descent(14/49): loss=1.2359367830577385e+26\n",
      "Gradient Descent(15/49): loss=1.7279194807186902e+28\n",
      "Gradient Descent(16/49): loss=2.4157445608614664e+30\n",
      "Gradient Descent(17/49): loss=3.3773695413856596e+32\n",
      "Gradient Descent(18/49): loss=4.721784575378287e+34\n",
      "Gradient Descent(19/49): loss=6.601365198772638e+36\n",
      "Gradient Descent(20/49): loss=9.229142472200003e+38\n",
      "Gradient Descent(21/49): loss=1.290294783653558e+41\n",
      "Gradient Descent(22/49): loss=1.8039169228462488e+43\n",
      "Gradient Descent(23/49): loss=2.5219944355630407e+45\n",
      "Gradient Descent(24/49): loss=3.525913999970534e+47\n",
      "Gradient Descent(25/49): loss=4.929459541979329e+49\n",
      "Gradient Descent(26/49): loss=6.891708469412934e+51\n",
      "Gradient Descent(27/49): loss=9.635061455096467e+53\n",
      "Gradient Descent(28/49): loss=1.3470449258773547e+56\n",
      "Gradient Descent(29/49): loss=1.883257352107632e+58\n",
      "Gradient Descent(30/49): loss=2.632917570998919e+60\n",
      "Gradient Descent(31/49): loss=3.680991834661805e+62\n",
      "Gradient Descent(32/49): loss=5.146268548660152e+64\n",
      "Gradient Descent(33/49): loss=7.1948217123288e+66\n",
      "Gradient Descent(34/49): loss=1.0058833693331979e+69\n",
      "Gradient Descent(35/49): loss=1.4062910703781816e+71\n",
      "Gradient Descent(36/49): loss=1.9660873565653556e+73\n",
      "Gradient Descent(37/49): loss=2.7487193619217477e+75\n",
      "Gradient Descent(38/49): loss=3.842890350407698e+77\n",
      "Gradient Descent(39/49): loss=5.372613315799514e+79\n",
      "Gradient Descent(40/49): loss=7.511266575181961e+81\n",
      "Gradient Descent(41/49): loss=1.0501244412570927e+84\n",
      "Gradient Descent(42/49): loss=1.4681429970401592e+86\n",
      "Gradient Descent(43/49): loss=2.0525604157711597e+88\n",
      "Gradient Descent(44/49): loss=2.869614382852551e+90\n",
      "Gradient Descent(45/49): loss=4.0119095364997096e+92\n",
      "Gradient Descent(46/49): loss=5.6089132481478765e+94\n",
      "Gradient Descent(47/49): loss=7.84162941338326e+96\n",
      "Gradient Descent(48/49): loss=1.0963113376221682e+99\n",
      "Gradient Descent(49/49): loss=1.5327153141764823e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9297248329860315\n",
      "Gradient Descent(2/49): loss=80.81657374103833\n",
      "Gradient Descent(3/49): loss=2758.751455930832\n",
      "Gradient Descent(4/49): loss=112067.79574568337\n",
      "Gradient Descent(5/49): loss=6777184.836229682\n",
      "Gradient Descent(6/49): loss=662478912.5094368\n",
      "Gradient Descent(7/49): loss=84160494817.93901\n",
      "Gradient Descent(8/49): loss=11614606965453.826\n",
      "Gradient Descent(9/49): loss=1636672854628293.5\n",
      "Gradient Descent(10/49): loss=2.317709471116436e+17\n",
      "Gradient Descent(11/49): loss=3.2858930758379057e+19\n",
      "Gradient Descent(12/49): loss=4.6597542487103955e+21\n",
      "Gradient Descent(13/49): loss=6.608444389435868e+23\n",
      "Gradient Descent(14/49): loss=9.372201745988387e+25\n",
      "Gradient Descent(15/49): loss=1.3291850344857609e+28\n",
      "Gradient Descent(16/49): loss=1.8850791359744663e+30\n",
      "Gradient Descent(17/49): loss=2.673460715067026e+32\n",
      "Gradient Descent(18/49): loss=3.7915609741970396e+34\n",
      "Gradient Descent(19/49): loss=5.377275482317896e+36\n",
      "Gradient Descent(20/49): loss=7.626170823132147e+38\n",
      "Gradient Descent(21/49): loss=1.0815603858134758e+41\n",
      "Gradient Descent(22/49): loss=1.5338928217405197e+43\n",
      "Gradient Descent(23/49): loss=2.1754006706529772e+45\n",
      "Gradient Descent(24/49): loss=3.0852012675415997e+47\n",
      "Gradient Descent(25/49): loss=4.3755005639475837e+49\n",
      "Gradient Descent(26/49): loss=6.205431518043952e+51\n",
      "Gradient Descent(27/49): loss=8.800680005031132e+53\n",
      "Gradient Descent(28/49): loss=1.2481318716634822e+56\n",
      "Gradient Descent(29/49): loss=1.7701281811991795e+58\n",
      "Gradient Descent(30/49): loss=2.5104348739204044e+60\n",
      "Gradient Descent(31/49): loss=3.5603541727278717e+62\n",
      "Gradient Descent(32/49): loss=5.0493729062428124e+64\n",
      "Gradient Descent(33/49): loss=7.16113215409805e+66\n",
      "Gradient Descent(34/49): loss=1.0156075750526268e+69\n",
      "Gradient Descent(35/49): loss=1.4403570892264748e+71\n",
      "Gradient Descent(36/49): loss=2.0427462293961925e+73\n",
      "Gradient Descent(37/49): loss=2.8970678097286417e+75\n",
      "Gradient Descent(38/49): loss=4.108685539782634e+77\n",
      "Gradient Descent(39/49): loss=5.827028559058844e+79\n",
      "Gradient Descent(40/49): loss=8.264020572838643e+81\n",
      "Gradient Descent(41/49): loss=1.1720216459575697e+84\n",
      "Gradient Descent(42/49): loss=1.6621869784640208e+86\n",
      "Gradient Descent(43/49): loss=2.3573502766819897e+88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=3.3432462165644165e+90\n",
      "Gradient Descent(45/49): loss=4.7414656087108543e+92\n",
      "Gradient Descent(46/49): loss=6.724451225638321e+94\n",
      "Gradient Descent(47/49): loss=9.53676521515115e+96\n",
      "Gradient Descent(48/49): loss=1.35252510155999e+99\n",
      "Gradient Descent(49/49): loss=1.9181809650127278e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.984666263874931\n",
      "Gradient Descent(2/49): loss=84.05461248458725\n",
      "Gradient Descent(3/49): loss=3030.07845131934\n",
      "Gradient Descent(4/49): loss=144037.41536545096\n",
      "Gradient Descent(5/49): loss=11192963.691065382\n",
      "Gradient Descent(6/49): loss=1304371676.5300794\n",
      "Gradient Descent(7/49): loss=178743328953.712\n",
      "Gradient Descent(8/49): loss=25592361477815.207\n",
      "Gradient Descent(9/49): loss=3702687659469344.5\n",
      "Gradient Descent(10/49): loss=5.369868507475377e+17\n",
      "Gradient Descent(11/49): loss=7.791969698538047e+19\n",
      "Gradient Descent(12/49): loss=1.1307973769484751e+22\n",
      "Gradient Descent(13/49): loss=1.6410983045677477e+24\n",
      "Gradient Descent(14/49): loss=2.3817007368224822e+26\n",
      "Gradient Descent(15/49): loss=3.4565307093068442e+28\n",
      "Gradient Descent(16/49): loss=5.016418867866384e+30\n",
      "Gradient Descent(17/49): loss=7.280265177877247e+32\n",
      "Gradient Descent(18/49): loss=1.056575684109422e+35\n",
      "Gradient Descent(19/49): loss=1.5333949428188136e+37\n",
      "Gradient Descent(20/49): loss=2.2253967113901686e+39\n",
      "Gradient Descent(21/49): loss=3.2296901377300592e+41\n",
      "Gradient Descent(22/49): loss=4.687208502125509e+43\n",
      "Gradient Descent(23/49): loss=6.802486494282776e+45\n",
      "Gradient Descent(24/49): loss=9.87236272592255e+47\n",
      "Gradient Descent(25/49): loss=1.4327635324840215e+50\n",
      "Gradient Descent(26/49): loss=2.0793516172437357e+52\n",
      "Gradient Descent(27/49): loss=3.017736737504853e+54\n",
      "Gradient Descent(28/49): loss=4.379603209657175e+56\n",
      "Gradient Descent(29/49): loss=6.356062818753052e+58\n",
      "Gradient Descent(30/49): loss=9.224473684476993e+60\n",
      "Gradient Descent(31/49): loss=1.3387362142575888e+63\n",
      "Gradient Descent(32/49): loss=1.942890957974866e+65\n",
      "Gradient Descent(33/49): loss=2.8196931063629195e+67\n",
      "Gradient Descent(34/49): loss=4.09218498929956e+69\n",
      "Gradient Descent(35/49): loss=5.938936385970499e+71\n",
      "Gradient Descent(36/49): loss=8.619103361366158e+73\n",
      "Gradient Descent(37/49): loss=1.2508795839168359e+76\n",
      "Gradient Descent(38/49): loss=1.8153857400915683e+78\n",
      "Gradient Descent(39/49): loss=2.634646394186323e+80\n",
      "Gradient Descent(40/49): loss=3.823629033270395e+82\n",
      "Gradient Descent(41/49): loss=5.549184519155666e+84\n",
      "Gradient Descent(42/49): loss=8.053461400071655e+86\n",
      "Gradient Descent(43/49): loss=1.1687886805449602e+89\n",
      "Gradient Descent(44/49): loss=1.696248248930467e+91\n",
      "Gradient Descent(45/49): loss=2.4617436581076903e+93\n",
      "Gradient Descent(46/49): loss=3.5726974763597127e+95\n",
      "Gradient Descent(47/49): loss=5.1850107201651756e+97\n",
      "Gradient Descent(48/49): loss=7.524940565530483e+99\n",
      "Gradient Descent(49/49): loss=1.0920851194105763e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0090903938627163\n",
      "Gradient Descent(2/49): loss=85.7654283296177\n",
      "Gradient Descent(3/49): loss=3083.2534737769597\n",
      "Gradient Descent(4/49): loss=140492.60517961133\n",
      "Gradient Descent(5/49): loss=10043158.04420445\n",
      "Gradient Descent(6/49): loss=1091890741.3575222\n",
      "Gradient Descent(7/49): loss=143320592778.92865\n",
      "Gradient Descent(8/49): loss=19878199685847.902\n",
      "Gradient Descent(9/49): loss=2795315988063577.0\n",
      "Gradient Descent(10/49): loss=3.943830690125242e+17\n",
      "Gradient Descent(11/49): loss=5.568591082747081e+19\n",
      "Gradient Descent(12/49): loss=7.864166429467339e+21\n",
      "Gradient Descent(13/49): loss=1.1106546402295351e+24\n",
      "Gradient Descent(14/49): loss=1.56859149573357e+26\n",
      "Gradient Descent(15/49): loss=2.215346866772102e+28\n",
      "Gradient Descent(16/49): loss=3.1287716265124784e+30\n",
      "Gradient Descent(17/49): loss=4.418817372942036e+32\n",
      "Gradient Descent(18/49): loss=6.240771118318606e+34\n",
      "Gradient Descent(19/49): loss=8.813947524782594e+36\n",
      "Gradient Descent(20/49): loss=1.244808848714143e+39\n",
      "Gradient Descent(21/49): loss=1.7580647787338487e+41\n",
      "Gradient Descent(22/49): loss=2.4829448872622163e+43\n",
      "Gradient Descent(23/49): loss=3.506705434237263e+45\n",
      "Gradient Descent(24/49): loss=4.95257992461383e+47\n",
      "Gradient Descent(25/49): loss=6.994613140369443e+49\n",
      "Gradient Descent(26/49): loss=9.878611497069106e+51\n",
      "Gradient Descent(27/49): loss=1.3951731589958828e+54\n",
      "Gradient Descent(28/49): loss=1.9704268602526164e+56\n",
      "Gradient Descent(29/49): loss=2.782867478900924e+58\n",
      "Gradient Descent(30/49): loss=3.930291228435386e+60\n",
      "Gradient Descent(31/49): loss=5.550817370008946e+62\n",
      "Gradient Descent(32/49): loss=7.839514093070984e+64\n",
      "Gradient Descent(33/49): loss=1.1071879530304227e+67\n",
      "Gradient Descent(34/49): loss=1.5637004395708668e+69\n",
      "Gradient Descent(35/49): loss=2.208440814426827e+71\n",
      "Gradient Descent(36/49): loss=3.119018647948136e+73\n",
      "Gradient Descent(37/49): loss=4.405043260701078e+75\n",
      "Gradient Descent(38/49): loss=6.221317766539567e+77\n",
      "Gradient Descent(39/49): loss=8.786473244782941e+79\n",
      "Gradient Descent(40/49): loss=1.2409286099563696e+82\n",
      "Gradient Descent(41/49): loss=1.75258465155237e+84\n",
      "Gradient Descent(42/49): loss=2.4752052102054248e+86\n",
      "Gradient Descent(43/49): loss=3.495774556282573e+88\n",
      "Gradient Descent(44/49): loss=4.937142059158156e+90\n",
      "Gradient Descent(45/49): loss=6.97280997955133e+92\n",
      "Gradient Descent(46/49): loss=9.847818520988416e+94\n",
      "Gradient Descent(47/49): loss=1.390824214437675e+97\n",
      "Gradient Descent(48/49): loss=1.964284771640983e+99\n",
      "Gradient Descent(49/49): loss=2.774192902343559e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.06471234067043\n",
      "Gradient Descent(2/49): loss=87.1327772186271\n",
      "Gradient Descent(3/49): loss=3094.606682437309\n",
      "Gradient Descent(4/49): loss=137112.97182534487\n",
      "Gradient Descent(5/49): loss=9434560.292142732\n",
      "Gradient Descent(6/49): loss=1002843449.1592013\n",
      "Gradient Descent(7/49): loss=130703106747.3863\n",
      "Gradient Descent(8/49): loss=18099626446708.426\n",
      "Gradient Descent(9/49): loss=2544783383156394.0\n",
      "Gradient Descent(10/49): loss=3.590943847399025e+17\n",
      "Gradient Descent(11/49): loss=5.071527670962548e+19\n",
      "Gradient Descent(12/49): loss=7.164018542493123e+21\n",
      "Gradient Descent(13/49): loss=1.0120343448312322e+24\n",
      "Gradient Descent(14/49): loss=1.429679404024961e+26\n",
      "Gradient Descent(15/49): loss=2.0196830193313594e+28\n",
      "Gradient Descent(16/49): loss=2.8531725471771364e+30\n",
      "Gradient Descent(17/49): loss=4.030629902571043e+32\n",
      "Gradient Descent(18/49): loss=5.6940047265283245e+34\n",
      "Gradient Descent(19/49): loss=8.043827111737278e+36\n",
      "Gradient Descent(20/49): loss=1.1363382686547891e+39\n",
      "Gradient Descent(21/49): loss=1.6052864431945076e+41\n",
      "Gradient Descent(22/49): loss=2.2677618417498726e+43\n",
      "Gradient Descent(23/49): loss=3.2036299769592466e+45\n",
      "Gradient Descent(24/49): loss=4.525715549306728e+47\n",
      "Gradient Descent(25/49): loss=6.393404163581206e+49\n",
      "Gradient Descent(26/49): loss=9.031857250766043e+51\n",
      "Gradient Descent(27/49): loss=1.2759156673199883e+54\n",
      "Gradient Descent(28/49): loss=1.8024651463291783e+56\n",
      "Gradient Descent(29/49): loss=2.5463129632663436e+58\n",
      "Gradient Descent(30/49): loss=3.597134579885018e+60\n",
      "Gradient Descent(31/49): loss=5.081613050897807e+62\n",
      "Gradient Descent(32/49): loss=7.178711450901773e+64\n",
      "Gradient Descent(33/49): loss=1.0141247981525098e+67\n",
      "Gradient Descent(34/49): loss=1.4326374771598594e+69\n",
      "Gradient Descent(35/49): loss=2.0238634778500842e+71\n",
      "Gradient Descent(36/49): loss=2.859078756682841e+73\n",
      "Gradient Descent(37/49): loss=4.0389736888768016e+75\n",
      "Gradient Descent(38/49): loss=5.705791916822231e+77\n",
      "Gradient Descent(39/49): loss=8.060478702233648e+79\n",
      "Gradient Descent(40/49): loss=1.1386906122112452e+82\n",
      "Gradient Descent(41/49): loss=1.608609560594319e+84\n",
      "Gradient Descent(42/49): loss=2.2724563552961044e+86\n",
      "Gradient Descent(43/49): loss=3.2102618393103936e+88\n",
      "Gradient Descent(44/49): loss=4.535084272537982e+90\n",
      "Gradient Descent(45/49): loss=6.406639205305236e+92\n",
      "Gradient Descent(46/49): loss=9.050554177240101e+94\n",
      "Gradient Descent(47/49): loss=1.278556951471932e+97\n",
      "Gradient Descent(48/49): loss=1.8061964451504093e+99\n",
      "Gradient Descent(49/49): loss=2.5515841079416072e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9590450842873115\n",
      "Gradient Descent(2/49): loss=82.65189789183191\n",
      "Gradient Descent(3/49): loss=2853.064405832218\n",
      "Gradient Descent(4/49): loss=117129.51382561082\n",
      "Gradient Descent(5/49): loss=7151310.60383684\n",
      "Gradient Descent(6/49): loss=705615184.4756794\n",
      "Gradient Descent(7/49): loss=90532489826.52164\n",
      "Gradient Descent(8/49): loss=12622579964149.953\n",
      "Gradient Descent(9/49): loss=1797222071289162.2\n",
      "Gradient Descent(10/49): loss=2.571631735202981e+17\n",
      "Gradient Descent(11/49): loss=3.6839756623344476e+19\n",
      "Gradient Descent(12/49): loss=5.27886825972344e+21\n",
      "Gradient Descent(13/49): loss=7.56470118162348e+23\n",
      "Gradient Descent(14/49): loss=1.0840491025103263e+26\n",
      "Gradient Descent(15/49): loss=1.553486815643492e+28\n",
      "Gradient Descent(16/49): loss=2.2262120175486858e+30\n",
      "Gradient Descent(17/49): loss=3.190256124603195e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=4.571772355539238e+34\n",
      "Gradient Descent(19/49): loss=6.551543780072241e+36\n",
      "Gradient Descent(20/49): loss=9.388640260019248e+38\n",
      "Gradient Descent(21/49): loss=1.3454319918485176e+41\n",
      "Gradient Descent(22/49): loss=1.928061140665814e+43\n",
      "Gradient Descent(23/49): loss=2.762993436137772e+45\n",
      "Gradient Descent(24/49): loss=3.959486847783512e+47\n",
      "Gradient Descent(25/49): loss=5.674112682550669e+49\n",
      "Gradient Descent(26/49): loss=8.131244267755518e+51\n",
      "Gradient Descent(27/49): loss=1.1652418103228626e+54\n",
      "Gradient Descent(28/49): loss=1.6698409638348094e+56\n",
      "Gradient Descent(29/49): loss=2.3929529646110467e+58\n",
      "Gradient Descent(30/49): loss=3.4292031485982556e+60\n",
      "Gradient Descent(31/49): loss=4.914193637846084e+62\n",
      "Gradient Descent(32/49): loss=7.042248027830687e+64\n",
      "Gradient Descent(33/49): loss=1.0091840277425742e+67\n",
      "Gradient Descent(34/49): loss=1.446203538736283e+69\n",
      "Gradient Descent(35/49): loss=2.072471043890604e+71\n",
      "Gradient Descent(36/49): loss=2.969938955838902e+73\n",
      "Gradient Descent(37/49): loss=4.256048559718736e+75\n",
      "Gradient Descent(38/49): loss=6.099098201015896e+77\n",
      "Gradient Descent(39/49): loss=8.740266550927957e+79\n",
      "Gradient Descent(40/49): loss=1.2525172880237855e+82\n",
      "Gradient Descent(41/49): loss=1.7949104271103645e+84\n",
      "Gradient Descent(42/49): loss=2.572182813087274e+86\n",
      "Gradient Descent(43/49): loss=3.686047127484124e+88\n",
      "Gradient Descent(44/49): loss=5.282261959338206e+90\n",
      "Gradient Descent(45/49): loss=7.569705552331248e+92\n",
      "Gradient Descent(46/49): loss=1.0847709293874877e+95\n",
      "Gradient Descent(47/49): loss=1.5545227764926643e+97\n",
      "Gradient Descent(48/49): loss=2.227697108364588e+99\n",
      "Gradient Descent(49/49): loss=3.192384493595303e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.014447886202557\n",
      "Gradient Descent(2/49): loss=85.95154068761377\n",
      "Gradient Descent(3/49): loss=3132.6450088601923\n",
      "Gradient Descent(4/49): loss=150432.0803133154\n",
      "Gradient Descent(5/49): loss=11799743.489477435\n",
      "Gradient Descent(6/49): loss=1388355025.4300728\n",
      "Gradient Descent(7/49): loss=192176721759.57352\n",
      "Gradient Descent(8/49): loss=27799484774408.742\n",
      "Gradient Descent(9/49): loss=4063719742381907.0\n",
      "Gradient Descent(10/49): loss=5.954664311204948e+17\n",
      "Gradient Descent(11/49): loss=8.730308335106888e+19\n",
      "Gradient Descent(12/49): loss=1.2801364283388554e+22\n",
      "Gradient Descent(13/49): loss=1.8771341402369648e+24\n",
      "Gradient Descent(14/49): loss=2.7525624320450953e+26\n",
      "Gradient Descent(15/49): loss=4.0362651598770275e+28\n",
      "Gradient Descent(16/49): loss=5.918645741529821e+30\n",
      "Gradient Descent(17/49): loss=8.678907035642064e+32\n",
      "Gradient Descent(18/49): loss=1.2726463440188374e+35\n",
      "Gradient Descent(19/49): loss=1.8661666920878866e+37\n",
      "Gradient Descent(20/49): loss=2.7364853890222923e+39\n",
      "Gradient Descent(21/49): loss=4.012692069588112e+41\n",
      "Gradient Descent(22/49): loss=5.8840795242983e+43\n",
      "Gradient Descent(23/49): loss=8.628220468506763e+45\n",
      "Gradient Descent(24/49): loss=1.265213839240506e+48\n",
      "Gradient Descent(25/49): loss=1.8552679139915507e+50\n",
      "Gradient Descent(26/49): loss=2.72050378041488e+52\n",
      "Gradient Descent(27/49): loss=3.9892571651975764e+54\n",
      "Gradient Descent(28/49): loss=5.84971535222534e+56\n",
      "Gradient Descent(29/49): loss=8.577829978119794e+58\n",
      "Gradient Descent(30/49): loss=1.2578247436525263e+61\n",
      "Gradient Descent(31/49): loss=1.8444327875234482e+63\n",
      "Gradient Descent(32/49): loss=2.704615507731875e+65\n",
      "Gradient Descent(33/49): loss=3.965959125290626e+67\n",
      "Gradient Descent(34/49): loss=5.8155518736436776e+69\n",
      "Gradient Descent(35/49): loss=8.527733778033465e+71\n",
      "Gradient Descent(36/49): loss=1.2504788018243166e+74\n",
      "Gradient Descent(37/49): loss=1.8336609403073828e+76\n",
      "Gradient Descent(38/49): loss=2.688820025660336e+78\n",
      "Gradient Descent(39/49): loss=3.9427971504808513e+80\n",
      "Gradient Descent(40/49): loss=5.7815879164400226e+82\n",
      "Gradient Descent(41/49): loss=8.477930149525966e+84\n",
      "Gradient Descent(42/49): loss=1.2431757617290767e+87\n",
      "Gradient Descent(43/49): loss=1.8229520027799275e+89\n",
      "Gradient Descent(44/49): loss=2.6731167922846912e+91\n",
      "Gradient Descent(45/49): loss=3.9197704461213266e+93\n",
      "Gradient Descent(46/49): loss=5.747822315370755e+95\n",
      "Gradient Descent(47/49): loss=8.428417383922367e+97\n",
      "Gradient Descent(48/49): loss=1.235915372812309e+100\n",
      "Gradient Descent(49/49): loss=1.8123056075362088e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.03913621723437\n",
      "Gradient Descent(2/49): loss=87.69981204106226\n",
      "Gradient Descent(3/49): loss=3187.685111146732\n",
      "Gradient Descent(4/49): loss=146751.6902239265\n",
      "Gradient Descent(5/49): loss=10589993.506768018\n",
      "Gradient Descent(6/49): loss=1162378737.8435566\n",
      "Gradient Descent(7/49): loss=154108708338.82224\n",
      "Gradient Descent(8/49): loss=21594623282730.496\n",
      "Gradient Descent(9/49): loss=3068182928149083.5\n",
      "Gradient Descent(10/49): loss=4.3738038889775814e+17\n",
      "Gradient Descent(11/49): loss=6.239926765940819e+19\n",
      "Gradient Descent(12/49): loss=8.90390714714231e+21\n",
      "Gradient Descent(13/49): loss=1.2705767114223727e+24\n",
      "Gradient Descent(14/49): loss=1.813116390410361e+26\n",
      "Gradient Descent(15/49): loss=2.5873283464840243e+28\n",
      "Gradient Descent(16/49): loss=3.6921357644998286e+30\n",
      "Gradient Descent(17/49): loss=5.2687044527504e+32\n",
      "Gradient Descent(18/49): loss=7.518479623801776e+34\n",
      "Gradient Descent(19/49): loss=1.0728925259575938e+37\n",
      "Gradient Descent(20/49): loss=1.5310254625214663e+39\n",
      "Gradient Descent(21/49): loss=2.1847845065418845e+41\n",
      "Gradient Descent(22/49): loss=3.1177034329979646e+43\n",
      "Gradient Descent(23/49): loss=4.4489855485775013e+45\n",
      "Gradient Descent(24/49): loss=6.348734841829099e+47\n",
      "Gradient Descent(25/49): loss=9.059690945679758e+49\n",
      "Gradient Descent(26/49): loss=1.2928245087585345e+52\n",
      "Gradient Descent(27/49): loss=1.8448700076727318e+54\n",
      "Gradient Descent(28/49): loss=2.6326429628710806e+56\n",
      "Gradient Descent(29/49): loss=3.756800718278136e+58\n",
      "Gradient Descent(30/49): loss=5.360982038165584e+60\n",
      "Gradient Descent(31/49): loss=7.650160487274277e+62\n",
      "Gradient Descent(32/49): loss=1.0916834838170214e+65\n",
      "Gradient Descent(33/49): loss=1.5578402973654346e+67\n",
      "Gradient Descent(34/49): loss=2.2230494718215044e+69\n",
      "Gradient Descent(35/49): loss=3.1723078177676557e+71\n",
      "Gradient Descent(36/49): loss=4.5269064041224574e+73\n",
      "Gradient Descent(37/49): loss=6.459928471287414e+75\n",
      "Gradient Descent(38/49): loss=9.218365066295054e+77\n",
      "Gradient Descent(39/49): loss=1.3154674215541442e+80\n",
      "Gradient Descent(40/49): loss=1.877181609456298e+82\n",
      "Gradient Descent(41/49): loss=2.6787518543923257e+84\n",
      "Gradient Descent(42/49): loss=3.822598442933124e+86\n",
      "Gradient Descent(43/49): loss=5.4548758713709114e+88\n",
      "Gradient Descent(44/49): loss=7.7841476723966275e+90\n",
      "Gradient Descent(45/49): loss=1.1108035529037738e+93\n",
      "Gradient Descent(46/49): loss=1.5851247754703877e+95\n",
      "Gradient Descent(47/49): loss=2.261984621170659e+97\n",
      "Gradient Descent(48/49): loss=3.227868559996747e+99\n",
      "Gradient Descent(49/49): loss=4.606191988707274e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.095408407167241\n",
      "Gradient Descent(2/49): loss=89.09908584517905\n",
      "Gradient Descent(3/49): loss=3199.5347108202145\n",
      "Gradient Descent(4/49): loss=143234.30422379542\n",
      "Gradient Descent(5/49): loss=9949243.603232056\n",
      "Gradient Descent(6/49): loss=1067631917.4139038\n",
      "Gradient Descent(7/49): loss=140543389351.13107\n",
      "Gradient Descent(8/49): loss=19662535425773.582\n",
      "Gradient Descent(9/49): loss=2793194567625445.5\n",
      "Gradient Descent(10/49): loss=3.98244068722869e+17\n",
      "Gradient Descent(11/49): loss=5.682932468488142e+19\n",
      "Gradient Descent(12/49): loss=8.111181001609371e+21\n",
      "Gradient Descent(13/49): loss=1.1577546166417094e+24\n",
      "Gradient Descent(14/49): loss=1.6525471406514948e+26\n",
      "Gradient Descent(15/49): loss=2.3588066771020887e+28\n",
      "Gradient Descent(16/49): loss=3.366906934810878e+30\n",
      "Gradient Descent(17/49): loss=4.8058469952834555e+32\n",
      "Gradient Descent(18/49): loss=6.859757811126696e+34\n",
      "Gradient Descent(19/49): loss=9.791463951936294e+36\n",
      "Gradient Descent(20/49): loss=1.397611535957959e+39\n",
      "Gradient Descent(21/49): loss=1.9949192642887817e+41\n",
      "Gradient Descent(22/49): loss=2.847502878350807e+43\n",
      "Gradient Descent(23/49): loss=4.064461548678983e+45\n",
      "Gradient Descent(24/49): loss=5.80152097698874e+47\n",
      "Gradient Descent(25/49): loss=8.280960526598389e+49\n",
      "Gradient Descent(26/49): loss=1.1820056760132019e+52\n",
      "Gradient Descent(27/49): loss=1.6871683105358604e+54\n",
      "Gradient Descent(28/49): loss=2.4082260904851188e+56\n",
      "Gradient Descent(29/49): loss=3.4374477440553555e+58\n",
      "Gradient Descent(30/49): loss=4.90653557811565e+60\n",
      "Gradient Descent(31/49): loss=7.003478502603269e+62\n",
      "Gradient Descent(32/49): loss=9.996607658404765e+64\n",
      "Gradient Descent(33/49): loss=1.426893287941581e+67\n",
      "Gradient Descent(34/49): loss=2.0367153786023614e+69\n",
      "Gradient Descent(35/49): loss=2.9071617117349578e+71\n",
      "Gradient Descent(36/49): loss=4.149617225347185e+73\n",
      "Gradient Descent(37/49): loss=5.923070274140853e+75\n",
      "Gradient Descent(38/49): loss=8.454457258880284e+77\n",
      "Gradient Descent(39/49): loss=1.2067702092662296e+80\n",
      "Gradient Descent(40/49): loss=1.722516648177266e+82\n",
      "Gradient Descent(41/49): loss=2.4586815124082238e+84\n",
      "Gradient Descent(42/49): loss=3.509466678220322e+86\n",
      "Gradient Descent(43/49): loss=5.009333784543493e+88\n",
      "Gradient Descent(44/49): loss=7.150210349822607e+90\n",
      "Gradient Descent(45/49): loss=1.0206049396121487e+93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=1.4567885304053144e+95\n",
      "Gradient Descent(47/49): loss=2.079387175146316e+97\n",
      "Gradient Descent(48/49): loss=2.9680704741408726e+99\n",
      "Gradient Descent(49/49): loss=4.236557022550006e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=2.9885341287169367\n",
      "Gradient Descent(2/49): loss=84.51846457397174\n",
      "Gradient Descent(3/49): loss=2950.050789237689\n",
      "Gradient Descent(4/49): loss=122390.40175437994\n",
      "Gradient Descent(5/49): loss=7543952.993586638\n",
      "Gradient Descent(6/49): loss=751314488.2694994\n",
      "Gradient Descent(7/49): loss=97350181814.77048\n",
      "Gradient Descent(8/49): loss=13712128526434.592\n",
      "Gradient Descent(9/49): loss=1972565680122630.5\n",
      "Gradient Descent(10/49): loss=2.8518395865981408e+17\n",
      "Gradient Descent(11/49): loss=4.127843945663096e+19\n",
      "Gradient Descent(12/49): loss=5.976383826267281e+21\n",
      "Gradient Descent(13/49): loss=8.653281182712514e+23\n",
      "Gradient Descent(14/49): loss=1.2529375782155912e+26\n",
      "Gradient Descent(15/49): loss=1.814176385865234e+28\n",
      "Gradient Descent(16/49): loss=2.6268176267266416e+30\n",
      "Gradient Descent(17/49): loss=3.803473653226168e+32\n",
      "Gradient Descent(18/49): loss=5.507200913692684e+34\n",
      "Gradient Descent(19/49): loss=7.97409551425886e+36\n",
      "Gradient Descent(20/49): loss=1.154601047053817e+39\n",
      "Gradient Descent(21/49): loss=1.6717928399093414e+41\n",
      "Gradient Descent(22/49): loss=2.4206554350869985e+43\n",
      "Gradient Descent(23/49): loss=3.504963411553508e+45\n",
      "Gradient Descent(24/49): loss=5.074976115287719e+47\n",
      "Gradient Descent(25/49): loss=7.348260037717424e+49\n",
      "Gradient Descent(26/49): loss=1.0639838366780505e+52\n",
      "Gradient Descent(27/49): loss=1.5405845722681887e+54\n",
      "Gradient Descent(28/49): loss=2.230673758842953e+56\n",
      "Gradient Descent(29/49): loss=3.229881376174418e+58\n",
      "Gradient Descent(30/49): loss=4.676673880616902e+60\n",
      "Gradient Descent(31/49): loss=6.771542369010888e+62\n",
      "Gradient Descent(32/49): loss=9.804785885403582e+64\n",
      "Gradient Descent(33/49): loss=1.4196739977373024e+67\n",
      "Gradient Descent(34/49): loss=2.05560252249041e+69\n",
      "Gradient Descent(35/49): loss=2.976388760520802e+71\n",
      "Gradient Descent(36/49): loss=4.3096318265953586e+73\n",
      "Gradient Descent(37/49): loss=6.240087560858171e+75\n",
      "Gradient Descent(38/49): loss=9.035271302499917e+77\n",
      "Gradient Descent(39/49): loss=1.308252916543211e+80\n",
      "Gradient Descent(40/49): loss=1.8942715014769858e+82\n",
      "Gradient Descent(41/49): loss=2.7427911498864222e+84\n",
      "Gradient Descent(42/49): loss=3.971396542697132e+86\n",
      "Gradient Descent(43/49): loss=5.750343222450405e+88\n",
      "Gradient Descent(44/49): loss=8.326150969936765e+90\n",
      "Gradient Descent(45/49): loss=1.2055765593873652e+93\n",
      "Gradient Descent(46/49): loss=1.745602314673503e+95\n",
      "Gradient Descent(47/49): loss=2.5275271132858647e+97\n",
      "Gradient Descent(48/49): loss=3.659707170811119e+99\n",
      "Gradient Descent(49/49): loss=5.299035767285833e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0443999205968306\n",
      "Gradient Descent(2/49): loss=87.88055576463167\n",
      "Gradient Descent(3/49): loss=3238.0912534445597\n",
      "Gradient Descent(4/49): loss=157074.1860599792\n",
      "Gradient Descent(5/49): loss=12436008.587017087\n",
      "Gradient Descent(6/49): loss=1477271243.380681\n",
      "Gradient Descent(7/49): loss=206542730527.33194\n",
      "Gradient Descent(8/49): loss=30184112968834.848\n",
      "Gradient Descent(9/49): loss=4457821331609605.0\n",
      "Gradient Descent(10/49): loss=6.599637460695702e+17\n",
      "Gradient Descent(11/49): loss=9.775927916654535e+19\n",
      "Gradient Descent(12/49): loss=1.448273945972083e+22\n",
      "Gradient Descent(13/49): loss=2.1456354421549547e+24\n",
      "Gradient Descent(14/49): loss=3.178805794717351e+26\n",
      "Gradient Descent(15/49): loss=4.7094772780924e+28\n",
      "Gradient Descent(16/49): loss=6.977206286338214e+30\n",
      "Gradient Descent(17/49): loss=1.0336903324560617e+33\n",
      "Gradient Descent(18/49): loss=1.5314377680935524e+35\n",
      "Gradient Descent(19/49): loss=2.2688629015217343e+37\n",
      "Gradient Descent(20/49): loss=3.3613764645650813e+39\n",
      "Gradient Descent(21/49): loss=4.979962311213635e+41\n",
      "Gradient Descent(22/49): loss=7.377937247947105e+43\n",
      "Gradient Descent(23/49): loss=1.0930596384768262e+46\n",
      "Gradient Descent(24/49): loss=1.61939487029727e+48\n",
      "Gradient Descent(25/49): loss=2.3991735250611797e+50\n",
      "Gradient Descent(26/49): loss=3.554434874984303e+52\n",
      "Gradient Descent(27/49): loss=5.265983118158331e+54\n",
      "Gradient Descent(28/49): loss=7.801684142785671e+56\n",
      "Gradient Descent(29/49): loss=1.1558387882770355e+59\n",
      "Gradient Descent(30/49): loss=1.7124037323672945e+61\n",
      "Gradient Descent(31/49): loss=2.536968453011152e+63\n",
      "Gradient Descent(32/49): loss=3.7585814664607234e+65\n",
      "Gradient Descent(33/49): loss=5.568431339086655e+67\n",
      "Gradient Descent(34/49): loss=8.249768657354845e+69\n",
      "Gradient Descent(35/49): loss=1.2222236165892726e+72\n",
      "Gradient Descent(36/49): loss=1.8107544962691639e+74\n",
      "Gradient Descent(37/49): loss=2.6826775405541837e+76\n",
      "Gradient Descent(38/49): loss=3.974453080979211e+78\n",
      "Gradient Descent(39/49): loss=5.888250471445639e+80\n",
      "Gradient Descent(40/49): loss=8.723588606545527e+82\n",
      "Gradient Descent(41/49): loss=1.292421213148044e+85\n",
      "Gradient Descent(42/49): loss=1.9147539705640763e+87\n",
      "Gradient Descent(43/49): loss=2.8367553321572263e+89\n",
      "Gradient Descent(44/49): loss=4.2027231374022986e+91\n",
      "Gradient Descent(45/49): loss=6.2264382019244795e+93\n",
      "Gradient Descent(46/49): loss=9.224622087846227e+95\n",
      "Gradient Descent(47/49): loss=1.3666505617494364e+98\n",
      "Gradient Descent(48/49): loss=2.0247265851582564e+100\n",
      "Gradient Descent(49/49): loss=2.9996824787448244e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0693538559029383\n",
      "Gradient Descent(2/49): loss=89.66689246541624\n",
      "Gradient Descent(3/49): loss=3295.050269749801\n",
      "Gradient Descent(4/49): loss=153253.77385255095\n",
      "Gradient Descent(5/49): loss=11163519.035879724\n",
      "Gradient Descent(6/49): loss=1237018188.502553\n",
      "Gradient Descent(7/49): loss=165646987076.2176\n",
      "Gradient Descent(8/49): loss=23449257400107.15\n",
      "Gradient Descent(9/49): loss=3366071909433138.5\n",
      "Gradient Descent(10/49): loss=4.8480716532896026e+17\n",
      "Gradient Descent(11/49): loss=6.9881015534171185e+19\n",
      "Gradient Descent(12/49): loss=1.0074673197912718e+22\n",
      "Gradient Descent(13/49): loss=1.452519705968326e+24\n",
      "Gradient Descent(14/49): loss=2.0941976442222894e+26\n",
      "Gradient Descent(15/49): loss=3.019356411723301e+28\n",
      "Gradient Descent(16/49): loss=4.3532273728206356e+30\n",
      "Gradient Descent(17/49): loss=6.2763677470996325e+32\n",
      "Gradient Descent(18/49): loss=9.049100821433687e+34\n",
      "Gradient Descent(19/49): loss=1.304675404861655e+37\n",
      "Gradient Descent(20/49): loss=1.8810464694568524e+39\n",
      "Gradient Descent(21/49): loss=2.7120430174554558e+41\n",
      "Gradient Descent(22/49): loss=3.9101518483019524e+43\n",
      "Gradient Descent(23/49): loss=5.637553452783316e+45\n",
      "Gradient Descent(24/49): loss=8.12807537051916e+47\n",
      "Gradient Descent(25/49): loss=1.1718843959929363e+50\n",
      "Gradient Descent(26/49): loss=1.6895919082557213e+52\n",
      "Gradient Descent(27/49): loss=2.4360088983220048e+54\n",
      "Gradient Descent(28/49): loss=3.5121731606954913e+56\n",
      "Gradient Descent(29/49): loss=5.063758313529522e+58\n",
      "Gradient Descent(30/49): loss=7.300792724229353e+60\n",
      "Gradient Descent(31/49): loss=1.0526089734525303e+63\n",
      "Gradient Descent(32/49): loss=1.5176237606577803e+65\n",
      "Gradient Descent(33/49): loss=2.1880697742473754e+67\n",
      "Gradient Descent(34/49): loss=3.1547010933064916e+69\n",
      "Gradient Descent(35/49): loss=4.548364547255955e+71\n",
      "Gradient Descent(36/49): loss=6.557711631897231e+73\n",
      "Gradient Descent(37/49): loss=9.454735081220388e+75\n",
      "Gradient Descent(38/49): loss=1.3631586820812511e+78\n",
      "Gradient Descent(39/49): loss=1.9653661118696215e+80\n",
      "Gradient Descent(40/49): loss=2.8336128467362292e+82\n",
      "Gradient Descent(41/49): loss=4.085428010942109e+84\n",
      "Gradient Descent(42/49): loss=5.890261985441864e+86\n",
      "Gradient Descent(43/49): loss=8.492423844996712e+88\n",
      "Gradient Descent(44/49): loss=1.224415194796407e+91\n",
      "Gradient Descent(45/49): loss=1.7653294237446513e+93\n",
      "Gradient Descent(46/49): loss=2.545205243762796e+95\n",
      "Gradient Descent(47/49): loss=3.669609561673847e+97\n",
      "Gradient Descent(48/49): loss=5.290745949910221e+99\n",
      "Gradient Descent(49/49): loss=7.628057491141755e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.126279947145809\n",
      "Gradient Descent(2/49): loss=91.09864577796715\n",
      "Gradient Descent(3/49): loss=3307.413189322218\n",
      "Gradient Descent(4/49): loss=149593.77247902247\n",
      "Gradient Descent(5/49): loss=10489097.12437345\n",
      "Gradient Descent(6/49): loss=1136239235.3932223\n",
      "Gradient Descent(7/49): loss=151068070862.5125\n",
      "Gradient Descent(8/49): loss=21351298893018.555\n",
      "Gradient Descent(9/49): loss=3064385226600146.0\n",
      "Gradient Descent(10/49): loss=4.414268060014124e+17\n",
      "Gradient Descent(11/49): loss=6.364316162372701e+19\n",
      "Gradient Descent(12/49): loss=9.177701079503565e+21\n",
      "Gradient Descent(13/49): loss=1.3235400411631937e+24\n",
      "Gradient Descent(14/49): loss=1.9087331439710276e+26\n",
      "Gradient Descent(15/49): loss=2.752672302554359e+28\n",
      "Gradient Descent(16/49): loss=3.9697584929796143e+30\n",
      "Gradient Descent(17/49): loss=5.7249767192802795e+32\n",
      "Gradient Descent(18/49): loss=8.256260336515882e+34\n",
      "Gradient Descent(19/49): loss=1.1906744542904363e+37\n",
      "Gradient Descent(20/49): loss=1.7171280956412617e+39\n",
      "Gradient Descent(21/49): loss=2.4763518588788995e+41\n",
      "Gradient Descent(22/49): loss=3.5712644532499064e+43\n",
      "Gradient Descent(23/49): loss=5.1502898304394596e+45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=7.427477210081825e+47\n",
      "Gradient Descent(25/49): loss=1.0711517122844028e+50\n",
      "Gradient Descent(26/49): loss=1.5447586822249796e+52\n",
      "Gradient Descent(27/49): loss=2.2277697537546384e+54\n",
      "Gradient Descent(28/49): loss=3.212772410895683e+56\n",
      "Gradient Descent(29/49): loss=4.633291455194607e+58\n",
      "Gradient Descent(30/49): loss=6.681889335197366e+60\n",
      "Gradient Descent(31/49): loss=9.636269489968558e+62\n",
      "Gradient Descent(32/49): loss=1.3896921218698496e+65\n",
      "Gradient Descent(33/49): loss=2.004140913241971e+67\n",
      "Gradient Descent(34/49): loss=2.8902666546932714e+69\n",
      "Gradient Descent(35/49): loss=4.1681906097702925e+71\n",
      "Gradient Descent(36/49): loss=6.011145349224327e+73\n",
      "Gradient Descent(37/49): loss=8.668957778658932e+75\n",
      "Gradient Descent(38/49): loss=1.2501915126352346e+78\n",
      "Gradient Descent(39/49): loss=1.802960469034583e+80\n",
      "Gradient Descent(40/49): loss=2.600134795387777e+82\n",
      "Gradient Descent(41/49): loss=3.74977769634989e+84\n",
      "Gradient Descent(42/49): loss=5.407732244106993e+86\n",
      "Gradient Descent(43/49): loss=7.798747123708062e+88\n",
      "Gradient Descent(44/49): loss=1.1246943072269123e+91\n",
      "Gradient Descent(45/49): loss=1.6219749975777726e+93\n",
      "Gradient Descent(46/49): loss=2.3391270640055566e+95\n",
      "Gradient Descent(47/49): loss=3.373366068980357e+97\n",
      "Gradient Descent(48/49): loss=4.864891185458378e+99\n",
      "Gradient Descent(49/49): loss=7.015890289518493e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.018191966274904\n",
      "Gradient Descent(2/49): loss=86.41662655973361\n",
      "Gradient Descent(3/49): loss=3049.7709212991704\n",
      "Gradient Descent(4/49): loss=127857.14737083021\n",
      "Gradient Descent(5/49): loss=7955923.519442641\n",
      "Gradient Descent(6/49): loss=799714649.5374568\n",
      "Gradient Descent(7/49): loss=104642202190.20854\n",
      "Gradient Descent(8/49): loss=14889382049298.832\n",
      "Gradient Descent(9/49): loss=2163979932472615.2\n",
      "Gradient Descent(10/49): loss=3.1608970230449254e+17\n",
      "Gradient Descent(11/49): loss=4.62248619760334e+19\n",
      "Gradient Descent(12/49): loss=6.761746256503362e+21\n",
      "Gradient Descent(13/49): loss=9.891666794869304e+23\n",
      "Gradient Descent(14/49): loss=1.4470596485483728e+26\n",
      "Gradient Descent(15/49): loss=2.1169220159220143e+28\n",
      "Gradient Descent(16/49): loss=3.0968746498253927e+30\n",
      "Gradient Descent(17/49): loss=4.530461808174453e+32\n",
      "Gradient Descent(18/49): loss=6.627677056157686e+34\n",
      "Gradient Descent(19/49): loss=9.695723184727196e+36\n",
      "Gradient Descent(20/49): loss=1.418401160645979e+39\n",
      "Gradient Descent(21/49): loss=2.0749992705554314e+41\n",
      "Gradient Descent(22/49): loss=3.0355460026347667e+43\n",
      "Gradient Descent(23/49): loss=4.440743505373187e+45\n",
      "Gradient Descent(24/49): loss=6.4964269569716485e+47\n",
      "Gradient Descent(25/49): loss=9.503715572914044e+49\n",
      "Gradient Descent(26/49): loss=1.3903120944652935e+52\n",
      "Gradient Descent(27/49): loss=2.033907375685379e+54\n",
      "Gradient Descent(28/49): loss=2.9754320841598717e+56\n",
      "Gradient Descent(29/49): loss=4.3528019974187494e+58\n",
      "Gradient Descent(30/49): loss=6.367776071784158e+60\n",
      "Gradient Descent(31/49): loss=9.315510359633638e+62\n",
      "Gradient Descent(32/49): loss=1.3627792856121095e+65\n",
      "Gradient Descent(33/49): loss=1.993629237256829e+67\n",
      "Gradient Descent(34/49): loss=2.9165086214677057e+69\n",
      "Gradient Descent(35/49): loss=4.2666020241554413e+71\n",
      "Gradient Descent(36/49): loss=6.241672902501576e+73\n",
      "Gradient Descent(37/49): loss=9.131032236252215e+75\n",
      "Gradient Descent(38/49): loss=1.3357917180514463e+78\n",
      "Gradient Descent(39/49): loss=1.9541487400850877e+80\n",
      "Gradient Descent(40/49): loss=2.8587520395369233e+82\n",
      "Gradient Descent(41/49): loss=4.182109097386646e+84\n",
      "Gradient Descent(42/49): loss=6.118066995862016e+86\n",
      "Gradient Descent(43/49): loss=8.95020739397866e+88\n",
      "Gradient Descent(44/49): loss=1.309338594190165e+91\n",
      "Gradient Descent(45/49): loss=1.9154500882171895e+93\n",
      "Gradient Descent(46/49): loss=2.8021392302428793e+95\n",
      "Gradient Descent(47/49): loss=4.099289411907567e+97\n",
      "Gradient Descent(48/49): loss=5.996908897749859e+99\n",
      "Gradient Descent(49/49): loss=8.77296348568292e+101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0745223670577544\n",
      "Gradient Descent(2/49): loss=89.84201776324231\n",
      "Gradient Descent(3/49): loss=3346.4815557913957\n",
      "Gradient Descent(4/49): loss=163971.90725449327\n",
      "Gradient Descent(5/49): loss=13103027.505545\n",
      "Gradient Descent(6/49): loss=1571382474.635501\n",
      "Gradient Descent(7/49): loss=221900814534.20404\n",
      "Gradient Descent(8/49): loss=32759499855039.54\n",
      "Gradient Descent(9/49): loss=4887828190414313.0\n",
      "Gradient Descent(10/49): loss=7.310623182540585e+17\n",
      "Gradient Descent(11/49): loss=1.0940447284083699e+20\n",
      "Gradient Descent(12/49): loss=1.6374611745499658e+22\n",
      "Gradient Descent(13/49): loss=2.4508658739982364e+24\n",
      "Gradient Descent(14/49): loss=3.6683515683862187e+26\n",
      "Gradient Descent(15/49): loss=5.490640540063795e+28\n",
      "Gradient Descent(16/49): loss=8.218171935184761e+30\n",
      "Gradient Descent(17/49): loss=1.2300633188265851e+33\n",
      "Gradient Descent(18/49): loss=1.8411099296852146e+35\n",
      "Gradient Descent(19/49): loss=2.7557002433307007e+37\n",
      "Gradient Descent(20/49): loss=4.124622715738202e+39\n",
      "Gradient Descent(21/49): loss=6.173571523957104e+41\n",
      "Gradient Descent(22/49): loss=9.240356752589528e+43\n",
      "Gradient Descent(23/49): loss=1.3830599124789816e+46\n",
      "Gradient Descent(24/49): loss=2.0701091664833783e+48\n",
      "Gradient Descent(25/49): loss=3.0984572125150512e+50\n",
      "Gradient Descent(26/49): loss=4.6376477401414846e+52\n",
      "Gradient Descent(27/49): loss=6.941447012651023e+54\n",
      "Gradient Descent(28/49): loss=1.0389682297856645e+57\n",
      "Gradient Descent(29/49): loss=1.5550863970244472e+59\n",
      "Gradient Descent(30/49): loss=2.3275915787235746e+61\n",
      "Gradient Descent(31/49): loss=3.4838466645398e+63\n",
      "Gradient Descent(32/49): loss=5.214483371125265e+65\n",
      "Gradient Descent(33/49): loss=7.804831683467217e+67\n",
      "Gradient Descent(34/49): loss=1.1681962194868334e+70\n",
      "Gradient Descent(35/49): loss=1.7485097213743752e+72\n",
      "Gradient Descent(36/49): loss=2.617099931280066e+74\n",
      "Gradient Descent(37/49): loss=3.9171712725295333e+76\n",
      "Gradient Descent(38/49): loss=5.863066440426432e+78\n",
      "Gradient Descent(39/49): loss=8.775605071425946e+80\n",
      "Gradient Descent(40/49): loss=1.3134977260130935e+83\n",
      "Gradient Descent(41/49): loss=1.9659912475542083e+85\n",
      "Gradient Descent(42/49): loss=2.9426176451722223e+87\n",
      "Gradient Descent(43/49): loss=4.4043932629157306e+89\n",
      "Gradient Descent(44/49): loss=6.592320971854168e+91\n",
      "Gradient Descent(45/49): loss=9.867124301061936e+93\n",
      "Gradient Descent(46/49): loss=1.4768719907341135e+96\n",
      "Gradient Descent(47/49): loss=2.2105233606718094e+98\n",
      "Gradient Descent(48/49): loss=3.3086236036251278e+100\n",
      "Gradient Descent(49/49): loss=4.952216450288162e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0997433098684226\n",
      "Gradient Descent(2/49): loss=91.66703621851734\n",
      "Gradient Descent(3/49): loss=3405.4145491444115\n",
      "Gradient Descent(4/49): loss=160006.90918041204\n",
      "Gradient Descent(5/49): loss=11764888.016113326\n",
      "Gradient Descent(6/49): loss=1316030328.9842134\n",
      "Gradient Descent(7/49): loss=177983333708.808\n",
      "Gradient Descent(8/49): loss=25452434548133.676\n",
      "Gradient Descent(9/49): loss=3691131027505085.5\n",
      "Gradient Descent(10/49): loss=5.3709338322142214e+17\n",
      "Gradient Descent(11/49): loss=7.821446492893397e+19\n",
      "Gradient Descent(12/49): loss=1.139217306354518e+22\n",
      "Gradient Descent(13/49): loss=1.6593788178233888e+24\n",
      "Gradient Descent(14/49): loss=2.417069405966395e+26\n",
      "Gradient Descent(15/49): loss=3.520738688265663e+28\n",
      "Gradient Descent(16/49): loss=5.1283625784771e+30\n",
      "Gradient Descent(17/49): loss=7.470053523673788e+32\n",
      "Gradient Descent(18/49): loss=1.0880997717215598e+35\n",
      "Gradient Descent(19/49): loss=1.584943292344486e+37\n",
      "Gradient Descent(20/49): loss=2.3086534065489887e+39\n",
      "Gradient Descent(21/49): loss=3.362820978944206e+41\n",
      "Gradient Descent(22/49): loss=4.898338098536107e+43\n",
      "Gradient Descent(23/49): loss=7.134996563420944e+45\n",
      "Gradient Descent(24/49): loss=1.0392948574853021e+48\n",
      "Gradient Descent(25/49): loss=1.5138532880792458e+50\n",
      "Gradient Descent(26/49): loss=2.205102585972798e+52\n",
      "Gradient Descent(27/49): loss=3.21198722026354e+54\n",
      "Gradient Descent(28/49): loss=4.678631265848676e+56\n",
      "Gradient Descent(29/49): loss=6.81496812430684e+58\n",
      "Gradient Descent(30/49): loss=9.926790100842344e+60\n",
      "Gradient Descent(31/49): loss=1.445951909220477e+63\n",
      "Gradient Descent(32/49): loss=2.1061963661354883e+65\n",
      "Gradient Descent(33/49): loss=3.067918859842201e+67\n",
      "Gradient Descent(34/49): loss=4.468779018855136e+69\n",
      "Gradient Descent(35/49): loss=6.509294030151657e+71\n",
      "Gradient Descent(36/49): loss=9.481540392172228e+73\n",
      "Gradient Descent(37/49): loss=1.3810961341118045e+76\n",
      "Gradient Descent(38/49): loss=2.0117264207757947e+78\n",
      "Gradient Descent(39/49): loss=2.930312446823332e+80\n",
      "Gradient Descent(40/49): loss=4.2683393464088103e+82\n",
      "Gradient Descent(41/49): loss=6.217330440599388e+84\n",
      "Gradient Descent(42/49): loss=9.056261620840246e+86\n",
      "Gradient Descent(43/49): loss=1.3191493572472087e+89\n",
      "Gradient Descent(44/49): loss=1.9214937681585165e+91\n",
      "Gradient Descent(45/49): loss=2.798878141271769e+93\n",
      "Gradient Descent(46/49): loss=4.076890063087016e+95\n",
      "Gradient Descent(47/49): loss=5.938462393702119e+97\n",
      "Gradient Descent(48/49): loss=8.650058023569162e+99\n",
      "Gradient Descent(49/49): loss=1.259981100334412e+102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.157326960606135\n",
      "Gradient Descent(2/49): loss=93.13183001098565\n",
      "Gradient Descent(3/49): loss=3418.308159530659\n",
      "Gradient Descent(4/49): loss=156199.2842047552\n",
      "Gradient Descent(5/49): loss=11055210.750883913\n",
      "Gradient Descent(6/49): loss=1208869088.142709\n",
      "Gradient Descent(7/49): loss=162320866314.0082\n",
      "Gradient Descent(8/49): loss=23175326070486.945\n",
      "Gradient Descent(9/49): loss=3360310940906487.0\n",
      "Gradient Descent(10/49): loss=4.8903409813920474e+17\n",
      "Gradient Descent(11/49): loss=7.123265928457759e+19\n",
      "Gradient Descent(12/49): loss=1.037788827965188e+22\n",
      "Gradient Descent(13/49): loss=1.512028650100734e+24\n",
      "Gradient Descent(14/49): loss=2.203007881002516e+26\n",
      "Gradient Descent(15/49): loss=3.20976517891207e+28\n",
      "Gradient Descent(16/49): loss=4.676605652411863e+30\n",
      "Gradient Descent(17/49): loss=6.813783101643588e+32\n",
      "Gradient Descent(18/49): loss=9.927636680085983e+34\n",
      "Gradient Descent(19/49): loss=1.4464500764328957e+37\n",
      "Gradient Descent(20/49): loss=2.1074681670877152e+39\n",
      "Gradient Descent(21/49): loss=3.070567142091154e+41\n",
      "Gradient Descent(22/49): loss=4.473795961607113e+43\n",
      "Gradient Descent(23/49): loss=6.518291045407465e+45\n",
      "Gradient Descent(24/49): loss=9.497106823270637e+47\n",
      "Gradient Descent(25/49): loss=1.3837221655865408e+50\n",
      "Gradient Descent(26/49): loss=2.016074018294299e+52\n",
      "Gradient Descent(27/49): loss=2.937406473877433e+54\n",
      "Gradient Descent(28/49): loss=4.279781751305672e+56\n",
      "Gradient Descent(29/49): loss=6.235613627769703e+58\n",
      "Gradient Descent(30/49): loss=9.085247700531597e+60\n",
      "Gradient Descent(31/49): loss=1.3237145645526046e+63\n",
      "Gradient Descent(32/49): loss=1.9286433415636594e+65\n",
      "Gradient Descent(33/49): loss=2.8100205577287894e+67\n",
      "Gradient Descent(34/49): loss=4.0941813163114676e+69\n",
      "Gradient Descent(35/49): loss=5.965195024901169e+71\n",
      "Gradient Descent(36/49): loss=8.691249589591594e+73\n",
      "Gradient Descent(37/49): loss=1.2663093010915964e+76\n",
      "Gradient Descent(38/49): loss=1.8450042534176393e+78\n",
      "Gradient Descent(39/49): loss=2.6881589610017463e+80\n",
      "Gradient Descent(40/49): loss=3.9166297780768575e+82\n",
      "Gradient Descent(41/49): loss=5.70650361123048e+84\n",
      "Gradient Descent(42/49): loss=8.314337915537552e+86\n",
      "Gradient Descent(43/49): loss=1.2113935201529868e+89\n",
      "Gradient Descent(44/49): loss=1.7649923247963242e+91\n",
      "Gradient Descent(45/49): loss=2.57158211164653e+93\n",
      "Gradient Descent(46/49): loss=3.746778081714048e+95\n",
      "Gradient Descent(47/49): loss=5.459030816101045e+97\n",
      "Gradient Descent(48/49): loss=7.953771694294772e+99\n",
      "Gradient Descent(49/49): loss=1.158859260848561e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.0480185969612172\n",
      "Gradient Descent(2/49): loss=88.34673860361605\n",
      "Gradient Descent(3/49): loss=3152.2861325328913\n",
      "Gradient Descent(4/49): loss=133536.62482338856\n",
      "Gradient Descent(5/49): loss=8388064.697234612\n",
      "Gradient Descent(6/49): loss=850960134.9293312\n",
      "Gradient Descent(7/49): loss=112438859917.2944\n",
      "Gradient Descent(8/49): loss=16160895346512.402\n",
      "Gradient Descent(9/49): loss=2372843748473720.5\n",
      "Gradient Descent(10/49): loss=3.5016029896993485e+17\n",
      "Gradient Descent(11/49): loss=5.173404212393596e+19\n",
      "Gradient Descent(12/49): loss=7.645481195848702e+21\n",
      "Gradient Descent(13/49): loss=1.1299541966577357e+24\n",
      "Gradient Descent(14/49): loss=1.6700261090824448e+26\n",
      "Gradient Descent(15/49): loss=2.4682387672430924e+28\n",
      "Gradient Descent(16/49): loss=3.647971381752823e+30\n",
      "Gradient Descent(17/49): loss=5.391576306415866e+32\n",
      "Gradient Descent(18/49): loss=7.968564791201885e+34\n",
      "Gradient Descent(19/49): loss=1.1777265469969332e+37\n",
      "Gradient Descent(20/49): loss=1.740639448938098e+39\n",
      "Gradient Descent(21/49): loss=2.572605415532252e+41\n",
      "Gradient Descent(22/49): loss=3.8022225848581136e+43\n",
      "Gradient Descent(23/49): loss=5.61955459548426e+45\n",
      "Gradient Descent(24/49): loss=8.305508987717057e+47\n",
      "Gradient Descent(25/49): loss=1.2275257473356676e+50\n",
      "Gradient Descent(26/49): loss=1.8142409605491988e+52\n",
      "Gradient Descent(27/49): loss=2.681385926184211e+54\n",
      "Gradient Descent(28/49): loss=3.962996449469727e+56\n",
      "Gradient Descent(29/49): loss=5.857172854211046e+58\n",
      "Gradient Descent(30/49): loss=8.656700625784574e+60\n",
      "Gradient Descent(31/49): loss=1.2794306671448607e+63\n",
      "Gradient Descent(32/49): loss=1.8909546521165105e+65\n",
      "Gradient Descent(33/49): loss=2.7947661316737186e+67\n",
      "Gradient Descent(34/49): loss=4.130568505177083e+69\n",
      "Gradient Descent(35/49): loss=6.104838606206754e+71\n",
      "Gradient Descent(36/49): loss=9.022742114340987e+73\n",
      "Gradient Descent(37/49): loss=1.333530343932957e+76\n",
      "Gradient Descent(38/49): loss=1.9709121192364682e+78\n",
      "Gradient Descent(39/49): loss=2.91294052619503e+80\n",
      "Gradient Descent(40/49): loss=4.305226207872085e+82\n",
      "Gradient Descent(41/49): loss=6.362976701470606e+84\n",
      "Gradient Descent(42/49): loss=9.404261367132429e+86\n",
      "Gradient Descent(43/49): loss=1.389917581199051e+89\n",
      "Gradient Descent(44/49): loss=2.054250522298347e+91\n",
      "Gradient Descent(45/49): loss=3.0361118281003314e+93\n",
      "Gradient Descent(46/49): loss=4.4872691683278926e+95\n",
      "Gradient Descent(47/49): loss=6.632029954451566e+97\n",
      "Gradient Descent(48/49): loss=9.801912848730025e+99\n",
      "Gradient Descent(49/49): loss=1.4486891065624406e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.104815225585325\n",
      "Gradient Descent(2/49): loss=91.83628874168754\n",
      "Gradient Descent(3/49): loss=3457.8813606227177\n",
      "Gradient Descent(4/49): loss=171133.64270946503\n",
      "Gradient Descent(5/49): loss=13802116.332425483\n",
      "Gradient Descent(6/49): loss=1670963342.3061597\n",
      "Gradient Descent(7/49): loss=238313890265.50098\n",
      "Gradient Descent(8/49): loss=35539813356643.35\n",
      "Gradient Descent(9/49): loss=5356802851841775.0\n",
      "Gradient Descent(10/49): loss=8.093988258270519e+17\n",
      "Gradient Descent(11/49): loss=1.2236674343050283e+20\n",
      "Gradient Descent(12/49): loss=1.8502053284164581e+22\n",
      "Gradient Descent(13/49): loss=2.797622827123637e+24\n",
      "Gradient Descent(14/49): loss=4.2302038782114016e+26\n",
      "Gradient Descent(15/49): loss=6.396377649835678e+28\n",
      "Gradient Descent(16/49): loss=9.671794184526963e+30\n",
      "Gradient Descent(17/49): loss=1.4624466423469852e+33\n",
      "Gradient Descent(18/49): loss=2.2113272673518843e+35\n",
      "Gradient Descent(19/49): loss=3.3436900615364825e+37\n",
      "Gradient Descent(20/49): loss=5.0559061986109437e+39\n",
      "Gradient Descent(21/49): loss=7.644903392423732e+41\n",
      "Gradient Descent(22/49): loss=1.155965826629059e+44\n",
      "Gradient Descent(23/49): loss=1.7479056617947873e+46\n",
      "Gradient Descent(24/49): loss=2.642962388820193e+48\n",
      "Gradient Descent(25/49): loss=3.996354232041224e+50\n",
      "Gradient Descent(26/49): loss=6.042782604668462e+52\n",
      "Gradient Descent(27/49): loss=9.13713336883928e+54\n",
      "Gradient Descent(28/49): loss=1.3816020145330678e+57\n",
      "Gradient Descent(29/49): loss=2.089084234089913e+59\n",
      "Gradient Descent(30/49): loss=3.158849575503863e+61\n",
      "Gradient Descent(31/49): loss=4.7764137404481945e+63\n",
      "Gradient Descent(32/49): loss=7.222290164388017e+65\n",
      "Gradient Descent(33/49): loss=1.0920635868893579e+68\n",
      "Gradient Descent(34/49): loss=1.651280758131496e+70\n",
      "Gradient Descent(35/49): loss=2.4968584017550305e+72\n",
      "Gradient Descent(36/49): loss=3.775434218387495e+74\n",
      "Gradient Descent(37/49): loss=5.7087352359880106e+76\n",
      "Gradient Descent(38/49): loss=8.632029088439394e+78\n",
      "Gradient Descent(39/49): loss=1.3052265187206565e+81\n",
      "Gradient Descent(40/49): loss=1.9735988464788812e+83\n",
      "Gradient Descent(41/49): loss=2.9842271444504866e+85\n",
      "Gradient Descent(42/49): loss=4.512371734288141e+87\n",
      "Gradient Descent(43/49): loss=6.823039159826517e+89\n",
      "Gradient Descent(44/49): loss=1.0316938877791684e+92\n",
      "Gradient Descent(45/49): loss=1.5599973166619524e+94\n",
      "Gradient Descent(46/49): loss=2.358831099824657e+96\n",
      "Gradient Descent(47/49): loss=3.5667267488676676e+98\n",
      "Gradient Descent(48/49): loss=5.3931541355521755e+100\n",
      "Gradient Descent(49/49): loss=8.154847168782281e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.130304579130822\n",
      "Gradient Descent(2/49): loss=93.70061196195849\n",
      "Gradient Descent(3/49): loss=3518.844643703966\n",
      "Gradient Descent(4/49): loss=167019.37080738274\n",
      "Gradient Descent(5/49): loss=12395297.273947902\n",
      "Gradient Descent(6/49): loss=1399646956.7862628\n",
      "Gradient Descent(7/49): loss=191168442950.09827\n",
      "Gradient Descent(8/49): loss=27615200764271.27\n",
      "Gradient Descent(9/49): loss=4045680349359813.0\n",
      "Gradient Descent(10/49): loss=5.947082395430189e+17\n",
      "Gradient Descent(11/49): loss=8.749146158591374e+19\n",
      "Gradient Descent(12/49): loss=1.2873903261200016e+22\n",
      "Gradient Descent(13/49): loss=1.894412062217497e+24\n",
      "Gradient Descent(14/49): loss=2.7876825323579114e+26\n",
      "Gradient Descent(15/49): loss=4.102166468814239e+28\n",
      "Gradient Descent(16/49): loss=6.036476417960194e+30\n",
      "Gradient Descent(17/49): loss=8.882880063469169e+32\n",
      "Gradient Descent(18/49): loss=1.3071460141031533e+35\n",
      "Gradient Descent(19/49): loss=1.9235098342737761e+37\n",
      "Gradient Descent(20/49): loss=2.83051017227941e+39\n",
      "Gradient Descent(21/49): loss=4.165192033971235e+41\n",
      "Gradient Descent(22/49): loss=6.129221810122125e+43\n",
      "Gradient Descent(23/49): loss=9.019358457479831e+45\n",
      "Gradient Descent(24/49): loss=1.3272292879115254e+48\n",
      "Gradient Descent(25/49): loss=1.9530630598585324e+50\n",
      "Gradient Descent(26/49): loss=2.873998751027748e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=4.229186958002312e+54\n",
      "Gradient Descent(28/49): loss=6.223392518643663e+56\n",
      "Gradient Descent(29/49): loss=9.157933859562399e+58\n",
      "Gradient Descent(30/49): loss=1.347621129872107e+61\n",
      "Gradient Descent(31/49): loss=1.983070349193925e+63\n",
      "Gradient Descent(32/49): loss=2.9181554983672483e+65\n",
      "Gradient Descent(33/49): loss=4.2941651142696396e+67\n",
      "Gradient Descent(34/49): loss=6.319010086654832e+69\n",
      "Gradient Descent(35/49): loss=9.29863836454696e+71\n",
      "Gradient Descent(36/49): loss=1.3683262765671202e+74\n",
      "Gradient Descent(37/49): loss=2.0135386771063364e+76\n",
      "Gradient Descent(38/49): loss=2.9629906796606058e+78\n",
      "Gradient Descent(39/49): loss=4.360141609185484e+80\n",
      "Gradient Descent(40/49): loss=6.416096743958879e+82\n",
      "Gradient Descent(41/49): loss=9.441504684415352e+84\n",
      "Gradient Descent(42/49): loss=1.3893495416784106e+87\n",
      "Gradient Descent(43/49): loss=2.044475127093115e+89\n",
      "Gradient Descent(44/49): loss=3.008514718515652e+91\n",
      "Gradient Descent(45/49): loss=4.427131781443805e+93\n",
      "Gradient Descent(46/49): loss=6.514675061965473e+95\n",
      "Gradient Descent(47/49): loss=9.586566033765918e+97\n",
      "Gradient Descent(48/49): loss=1.4106958128472549e+100\n",
      "Gradient Descent(49/49): loss=2.0758868914848108e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1885494475482163\n",
      "Gradient Descent(2/49): loss=95.1990136204012\n",
      "Gradient Descent(3/49): loss=3532.286766232548\n",
      "Gradient Descent(4/49): loss=163058.9649160086\n",
      "Gradient Descent(5/49): loss=11648715.512980983\n",
      "Gradient Descent(6/49): loss=1285734893.5191317\n",
      "Gradient Descent(7/49): loss=174348037350.58865\n",
      "Gradient Descent(8/49): loss=25144675981788.023\n",
      "Gradient Descent(9/49): loss=3683083851112774.5\n",
      "Gradient Descent(10/49): loss=5.414931490457824e+17\n",
      "Gradient Descent(11/49): loss=7.968146250911734e+19\n",
      "Gradient Descent(12/49): loss=1.1727680883822418e+22\n",
      "Gradient Descent(13/49): loss=1.7261890657743438e+24\n",
      "Gradient Descent(14/49): loss=2.5407950281694604e+26\n",
      "Gradient Descent(15/49): loss=3.7398319938431684e+28\n",
      "Gradient Descent(16/49): loss=5.50471496238921e+30\n",
      "Gradient Descent(17/49): loss=8.102473989491674e+32\n",
      "Gradient Descent(18/49): loss=1.1926155590571896e+35\n",
      "Gradient Descent(19/49): loss=1.755429126572017e+37\n",
      "Gradient Descent(20/49): loss=2.5838430508687138e+39\n",
      "Gradient Descent(21/49): loss=3.803198211560447e+41\n",
      "Gradient Descent(22/49): loss=5.597985772850545e+43\n",
      "Gradient Descent(23/49): loss=8.239761109114137e+45\n",
      "Gradient Descent(24/49): loss=1.2128230740594452e+48\n",
      "Gradient Descent(25/49): loss=1.7851728824348131e+50\n",
      "Gradient Descent(26/49): loss=2.6276233428797154e+52\n",
      "Gradient Descent(27/49): loss=3.86763909534074e+54\n",
      "Gradient Descent(28/49): loss=5.692837298139772e+56\n",
      "Gradient Descent(29/49): loss=8.379374523888088e+58\n",
      "Gradient Descent(30/49): loss=1.23337298669204e+61\n",
      "Gradient Descent(31/49): loss=1.8154206139908775e+63\n",
      "Gradient Descent(32/49): loss=2.6721454428334317e+65\n",
      "Gradient Descent(33/49): loss=3.9331718570489696e+67\n",
      "Gradient Descent(34/49): loss=5.789295975101867e+69\n",
      "Gradient Descent(35/49): loss=8.521353529788697e+71\n",
      "Gradient Descent(36/49): loss=1.2542710943080876e+74\n",
      "Gradient Descent(37/49): loss=1.8461808590820997e+76\n",
      "Gradient Descent(38/49): loss=2.717421919319154e+78\n",
      "Gradient Descent(39/49): loss=3.999814997143567e+80\n",
      "Gradient Descent(40/49): loss=5.8873890350392645e+82\n",
      "Gradient Descent(41/49): loss=8.665738209055535e+84\n",
      "Gradient Descent(42/49): loss=1.275523296676203e+87\n",
      "Gradient Descent(43/49): loss=1.877462301669302e+89\n",
      "Gradient Descent(44/49): loss=2.763465554392135e+91\n",
      "Gradient Descent(45/49): loss=4.0675873297278685e+93\n",
      "Gradient Descent(46/49): loss=5.987144170719449e+95\n",
      "Gradient Descent(47/49): loss=8.812569323097629e+97\n",
      "Gradient Descent(48/49): loss=1.2971355935306932e+100\n",
      "Gradient Descent(49/49): loss=1.9092737728533362e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.078014020775872\n",
      "Gradient Descent(2/49): loss=90.30915744233101\n",
      "Gradient Descent(3/49): loss=3257.6587801563446\n",
      "Gradient Descent(4/49): loss=139435.8987036493\n",
      "Gradient Descent(5/49): loss=8841251.055972856\n",
      "Gradient Descent(6/49): loss=905202335.3004993\n",
      "Gradient Descent(7/49): loss=120772230429.04785\n",
      "Gradient Descent(8/49): loss=17533675762225.188\n",
      "Gradient Descent(9/49): loss=2600646385046650.5\n",
      "Gradient Descent(10/49): loss=3.8770115341715923e+17\n",
      "Gradient Descent(11/49): loss=5.786663122341261e+19\n",
      "Gradient Descent(12/49): loss=8.639311679516998e+21\n",
      "Gradient Descent(13/49): loss=1.2899055951435314e+24\n",
      "Gradient Descent(14/49): loss=1.925941885221427e+26\n",
      "Gradient Descent(15/49): loss=2.8756096489563702e+28\n",
      "Gradient Descent(16/49): loss=4.2935550446225363e+30\n",
      "Gradient Descent(17/49): loss=6.410681772520974e+32\n",
      "Gradient Descent(18/49): loss=9.571751647022128e+34\n",
      "Gradient Descent(19/49): loss=1.4291526824405899e+37\n",
      "Gradient Descent(20/49): loss=2.1338595795463344e+39\n",
      "Gradient Descent(21/49): loss=3.1860533612960273e+41\n",
      "Gradient Descent(22/49): loss=4.757077794436966e+43\n",
      "Gradient Descent(23/49): loss=7.102765263725213e+45\n",
      "Gradient Descent(24/49): loss=1.0605097619156566e+48\n",
      "Gradient Descent(25/49): loss=1.5834409745512528e+50\n",
      "Gradient Descent(26/49): loss=2.3642265351331523e+52\n",
      "Gradient Descent(27/49): loss=3.530012927076214e+54\n",
      "Gradient Descent(28/49): loss=5.270641827317117e+56\n",
      "Gradient Descent(29/49): loss=7.869564742606578e+58\n",
      "Gradient Descent(30/49): loss=1.1750001473653381e+61\n",
      "Gradient Descent(31/49): loss=1.754385905021874e+63\n",
      "Gradient Descent(32/49): loss=2.6194634193372027e+65\n",
      "Gradient Descent(33/49): loss=3.911105638505698e+67\n",
      "Gradient Descent(34/49): loss=5.839649144411854e+69\n",
      "Gradient Descent(35/49): loss=8.719146267514198e+71\n",
      "Gradient Descent(36/49): loss=1.3018506720913986e+74\n",
      "Gradient Descent(37/49): loss=1.9437856877563945e+76\n",
      "Gradient Descent(38/49): loss=2.902255136418073e+78\n",
      "Gradient Descent(39/49): loss=4.3333403110850676e+80\n",
      "Gradient Descent(40/49): loss=6.470085285076061e+82\n",
      "Gradient Descent(41/49): loss=9.660446812605494e+84\n",
      "Gradient Descent(42/49): loss=1.442395710523998e+87\n",
      "Gradient Descent(43/49): loss=2.1536326694778707e+89\n",
      "Gradient Descent(44/49): loss=3.2155764477123476e+91\n",
      "Gradient Descent(45/49): loss=4.801158543712718e+93\n",
      "Gradient Descent(46/49): loss=7.168581974863303e+95\n",
      "Gradient Descent(47/49): loss=1.0703368168841129e+98\n",
      "Gradient Descent(48/49): loss=1.5981136933283898e+100\n",
      "Gradient Descent(49/49): loss=2.386134286437676e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.135278496179545\n",
      "Gradient Descent(2/49): loss=93.86373276885213\n",
      "Gradient Descent(3/49): loss=3572.3571985526496\n",
      "Gradient Descent(4/49): loss=178568.02029902796\n",
      "Gradient Descent(5/49): loss=14534640.24336486\n",
      "Gradient Descent(6/49): loss=1776301473.9342632\n",
      "Gradient Descent(7/49): loss=255848513230.7743\n",
      "Gradient Descent(8/49): loss=38540193422951.27\n",
      "Gradient Descent(9/49): loss=5868051455193134.0\n",
      "Gradient Descent(10/49): loss=8.956676357007789e+17\n",
      "Gradient Descent(11/49): loss=1.3678720736181605e+20\n",
      "Gradient Descent(12/49): loss=2.089297162148809e+22\n",
      "Gradient Descent(13/49): loss=3.1913009862407086e+24\n",
      "Gradient Descent(14/49): loss=4.874591722223797e+26\n",
      "Gradient Descent(15/49): loss=7.445766209645838e+28\n",
      "Gradient Descent(16/49): loss=1.1373148174256977e+31\n",
      "Gradient Descent(17/49): loss=1.7372088532036972e+33\n",
      "Gradient Descent(18/49): loss=2.653526190610966e+35\n",
      "Gradient Descent(19/49): loss=4.0531691170995063e+37\n",
      "Gradient Descent(20/49): loss=6.191075093080333e+39\n",
      "Gradient Descent(21/49): loss=9.456652241534108e+41\n",
      "Gradient Descent(22/49): loss=1.4444707950922659e+44\n",
      "Gradient Descent(23/49): loss=2.2063789855084954e+46\n",
      "Gradient Descent(24/49): loss=3.370167291887104e+48\n",
      "Gradient Descent(25/49): loss=5.147813521571827e+50\n",
      "Gradient Descent(26/49): loss=7.863106415124687e+52\n",
      "Gradient Descent(27/49): loss=1.2010622031370263e+55\n",
      "Gradient Descent(28/49): loss=1.834580812781113e+57\n",
      "Gradient Descent(29/49): loss=2.8022584923860253e+59\n",
      "Gradient Descent(30/49): loss=4.2803525488994473e+61\n",
      "Gradient Descent(31/49): loss=6.5380898987908984e+63\n",
      "Gradient Descent(32/49): loss=9.986705309043337e+65\n",
      "Gradient Descent(33/49): loss=1.5254345607593725e+68\n",
      "Gradient Descent(34/49): loss=2.3300483264004805e+70\n",
      "Gradient Descent(35/49): loss=3.559067916134708e+72\n",
      "Gradient Descent(36/49): loss=5.436352666224726e+74\n",
      "Gradient Descent(37/49): loss=8.30383994011128e+76\n",
      "Gradient Descent(38/49): loss=1.2683827187921293e+79\n",
      "Gradient Descent(39/49): loss=1.9374105629845485e+81\n",
      "Gradient Descent(40/49): loss=2.9593273654331925e+83\n",
      "Gradient Descent(41/49): loss=4.5202697988343936e+85\n",
      "Gradient Descent(42/49): loss=6.904555167813781e+87\n",
      "Gradient Descent(43/49): loss=1.0546468283304e+90\n",
      "Gradient Descent(44/49): loss=1.6109364115046152e+92\n",
      "Gradient Descent(45/49): loss=2.460649434673505e+94\n",
      "Gradient Descent(46/49): loss=3.7585565743740077e+96\n",
      "Gradient Descent(47/49): loss=5.741064665168173e+98\n",
      "Gradient Descent(48/49): loss=8.769276938483127e+100\n",
      "Gradient Descent(49/49): loss=1.3394766042329315e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1610376636901374\n",
      "Gradient Descent(2/49): loss=95.76799040308491\n",
      "Gradient Descent(3/49): loss=3635.408354738667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=174299.6596711967\n",
      "Gradient Descent(5/49): loss=13055988.474824924\n",
      "Gradient Descent(6/49): loss=1488110877.4079218\n",
      "Gradient Descent(7/49): loss=205255946484.61707\n",
      "Gradient Descent(8/49): loss=29949360860431.645\n",
      "Gradient Descent(9/49): loss=4432224695683058.5\n",
      "Gradient Descent(10/49): loss=6.581634910901181e+17\n",
      "Gradient Descent(11/49): loss=9.78132105404718e+19\n",
      "Gradient Descent(12/49): loss=1.4539341143595322e+22\n",
      "Gradient Descent(13/49): loss=2.1612835341539404e+24\n",
      "Gradient Descent(14/49): loss=3.212798294256228e+26\n",
      "Gradient Descent(15/49): loss=4.775911720483742e+28\n",
      "Gradient Descent(16/49): loss=7.099526503924905e+30\n",
      "Gradient Descent(17/49): loss=1.0553646466814606e+33\n",
      "Gradient Descent(18/49): loss=1.5688293789420984e+35\n",
      "Gradient Descent(19/49): loss=2.33210923632414e+37\n",
      "Gradient Descent(20/49): loss=3.4667463355956055e+39\n",
      "Gradient Descent(21/49): loss=5.153416475378416e+41\n",
      "Gradient Descent(22/49): loss=7.660699341884862e+43\n",
      "Gradient Descent(23/49): loss=1.1387846235337436e+46\n",
      "Gradient Descent(24/49): loss=1.6928355505539102e+48\n",
      "Gradient Descent(25/49): loss=2.516447923515965e+50\n",
      "Gradient Descent(26/49): loss=3.7407710097403686e+52\n",
      "Gradient Descent(27/49): loss=5.560761904328709e+54\n",
      "Gradient Descent(28/49): loss=8.266229843024016e+56\n",
      "Gradient Descent(29/49): loss=1.22879844512871e+59\n",
      "Gradient Descent(30/49): loss=1.8266436421738073e+61\n",
      "Gradient Descent(31/49): loss=2.7153574361371016e+63\n",
      "Gradient Descent(32/49): loss=4.036455625909926e+65\n",
      "Gradient Descent(33/49): loss=6.000305448964741e+67\n",
      "Gradient Descent(34/49): loss=8.919623753514888e+69\n",
      "Gradient Descent(35/49): loss=1.3259272978843877e+72\n",
      "Gradient Descent(36/49): loss=1.9710284288417662e+74\n",
      "Gradient Descent(37/49): loss=2.929989505081518e+76\n",
      "Gradient Descent(38/49): loss=4.3555122667270325e+78\n",
      "Gradient Descent(39/49): loss=6.47459216925826e+80\n",
      "Gradient Descent(40/49): loss=9.624664377244735e+82\n",
      "Gradient Descent(41/49): loss=1.4307335806328913e+85\n",
      "Gradient Descent(42/49): loss=2.1268259323309313e+87\n",
      "Gradient Descent(43/49): loss=3.161586900360802e+89\n",
      "Gradient Descent(44/49): loss=4.699788344962666e+91\n",
      "Gradient Descent(45/49): loss=6.98636829654304e+93\n",
      "Gradient Descent(46/49): loss=1.0385434064760874e+96\n",
      "Gradient Descent(47/49): loss=1.5438241463288535e+98\n",
      "Gradient Descent(48/49): loss=2.2949382567217263e+100\n",
      "Gradient Descent(49/49): loss=3.4114906252042275e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.219947407972057\n",
      "Gradient Descent(2/49): loss=97.30057376454549\n",
      "Gradient Descent(3/49): loss=3649.417269639195\n",
      "Gradient Descent(4/49): loss=170181.16281233323\n",
      "Gradient Descent(5/49): loss=12270784.901413454\n",
      "Gradient Descent(6/49): loss=1367060213.7194815\n",
      "Gradient Descent(7/49): loss=187198526419.2531\n",
      "Gradient Descent(8/49): loss=27270098660496.41\n",
      "Gradient Descent(9/49): loss=4034984294357463.0\n",
      "Gradient Descent(10/49): loss=5.99269913086407e+17\n",
      "Gradient Descent(11/49): loss=8.908173965915849e+19\n",
      "Gradient Descent(12/49): loss=1.324482157806783e+22\n",
      "Gradient Descent(13/49): loss=1.9693599164047674e+24\n",
      "Gradient Descent(14/49): loss=2.9282568744894807e+26\n",
      "Gradient Descent(15/49): loss=4.354060437219355e+28\n",
      "Gradient Descent(16/49): loss=6.474109180441497e+30\n",
      "Gradient Descent(17/49): loss=9.626438763452866e+32\n",
      "Gradient Descent(18/49): loss=1.4313679932680695e+35\n",
      "Gradient Descent(19/49): loss=2.128320140207752e+37\n",
      "Gradient Descent(20/49): loss=3.164627579893616e+39\n",
      "Gradient Descent(21/49): loss=4.705526924762238e+41\n",
      "Gradient Descent(22/49): loss=6.996710697609054e+43\n",
      "Gradient Descent(23/49): loss=1.0403502385618995e+46\n",
      "Gradient Descent(24/49): loss=1.5469106350967838e+48\n",
      "Gradient Descent(25/49): loss=2.3001220399459073e+50\n",
      "Gradient Descent(26/49): loss=3.4200821163245036e+52\n",
      "Gradient Descent(27/49): loss=5.0853656802831545e+54\n",
      "Gradient Descent(28/49): loss=7.561498005782982e+56\n",
      "Gradient Descent(29/49): loss=1.124329216149443e+59\n",
      "Gradient Descent(30/49): loss=1.6717800961138458e+61\n",
      "Gradient Descent(31/49): loss=2.4857921057446578e+63\n",
      "Gradient Descent(32/49): loss=3.696157411699167e+65\n",
      "Gradient Descent(33/49): loss=5.49586571639977e+67\n",
      "Gradient Descent(34/49): loss=8.171875980469156e+69\n",
      "Gradient Descent(35/49): loss=1.2150871306935046e+72\n",
      "Gradient Descent(36/49): loss=1.806729248835463e+74\n",
      "Gradient Descent(37/49): loss=2.6864497994760822e+76\n",
      "Gradient Descent(38/49): loss=3.9945180107960375e+78\n",
      "Gradient Descent(39/49): loss=5.939502067630496e+80\n",
      "Gradient Descent(40/49): loss=8.831524783726629e+82\n",
      "Gradient Descent(41/49): loss=1.313171190404071e+85\n",
      "Gradient Descent(42/49): loss=1.9525717444451899e+87\n",
      "Gradient Descent(43/49): loss=2.9033049499300843e+89\n",
      "Gradient Descent(44/49): loss=4.316962824166839e+91\n",
      "Gradient Descent(45/49): loss=6.418949558050197e+93\n",
      "Gradient Descent(46/49): loss=9.544421647120745e+95\n",
      "Gradient Descent(47/49): loss=1.4191727751432293e+98\n",
      "Gradient Descent(48/49): loss=2.110186913541621e+100\n",
      "Gradient Descent(49/49): loss=3.1376650454930454e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1081782377188727\n",
      "Gradient Descent(2/49): loss=92.30424179481568\n",
      "Gradient Descent(3/49): loss=3365.952259489119\n",
      "Gradient Descent(4/49): loss=145562.22824848955\n",
      "Gradient Descent(5/49): loss=9316390.17655635\n",
      "Gradient Descent(6/49): loss=962599859.4375268\n",
      "Gradient Descent(7/49): loss=129676248759.10526\n",
      "Gradient Descent(8/49): loss=19015211860483.46\n",
      "Gradient Descent(9/49): loss=2848995632903101.0\n",
      "Gradient Descent(10/49): loss=4.2904535739659955e+17\n",
      "Gradient Descent(11/49): loss=6.468945753263571e+19\n",
      "Gradient Descent(12/49): loss=9.756287169200503e+21\n",
      "Gradient Descent(13/49): loss=1.471511706916916e+24\n",
      "Gradient Descent(14/49): loss=2.219470599218496e+26\n",
      "Gradient Descent(15/49): loss=3.34762335316388e+28\n",
      "Gradient Descent(16/49): loss=5.0492181433174813e+30\n",
      "Gradient Descent(17/49): loss=7.615733905396368e+32\n",
      "Gradient Descent(18/49): loss=1.148680920840188e+35\n",
      "Gradient Descent(19/49): loss=1.732555112557567e+37\n",
      "Gradient Descent(20/49): loss=2.613212399292565e+39\n",
      "Gradient Descent(21/49): loss=3.9415075446007435e+41\n",
      "Gradient Descent(22/49): loss=5.944974748452151e+43\n",
      "Gradient Descent(23/49): loss=8.96680378278117e+45\n",
      "Gradient Descent(24/49): loss=1.3524627686667452e+48\n",
      "Gradient Descent(25/49): loss=2.0399192231073496e+50\n",
      "Gradient Descent(26/49): loss=3.076809604826787e+52\n",
      "Gradient Descent(27/49): loss=4.64075108323889e+54\n",
      "Gradient Descent(28/49): loss=6.999643586264478e+56\n",
      "Gradient Descent(29/49): loss=1.0557560501723457e+59\n",
      "Gradient Descent(30/49): loss=1.5923965609660213e+61\n",
      "Gradient Descent(31/49): loss=2.4018112962387746e+63\n",
      "Gradient Descent(32/49): loss=3.6226513194933295e+65\n",
      "Gradient Descent(33/49): loss=5.46404399178994e+67\n",
      "Gradient Descent(34/49): loss=8.241416054469028e+69\n",
      "Gradient Descent(35/49): loss=1.2430525574998525e+72\n",
      "Gradient Descent(36/49): loss=1.8748958316077623e+74\n",
      "Gradient Descent(37/49): loss=2.8279048686808813e+76\n",
      "Gradient Descent(38/49): loss=4.2653281379646784e+78\n",
      "Gradient Descent(39/49): loss=6.433393260855706e+80\n",
      "Gradient Descent(40/49): loss=9.703485291186604e+82\n",
      "Gradient Descent(41/49): loss=1.4635764203811684e+85\n",
      "Gradient Descent(42/49): loss=2.2075119135196967e+87\n",
      "Gradient Descent(43/49): loss=3.329589613818848e+89\n",
      "Gradient Descent(44/49): loss=5.022019101484536e+91\n",
      "Gradient Descent(45/49): loss=7.574710033633263e+93\n",
      "Gradient Descent(46/49): loss=1.1424933066595372e+96\n",
      "Gradient Descent(47/49): loss=1.7232223411406144e+98\n",
      "Gradient Descent(48/49): loss=2.599135784601146e+100\n",
      "Gradient Descent(49/49): loss=3.920275791180068e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.165912178840412\n",
      "Gradient Descent(2/49): loss=95.92471592426031\n",
      "Gradient Descent(3/49): loss=3689.9766980418162\n",
      "Gradient Descent(4/49): loss=186283.90193679064\n",
      "Gradient Descent(5/49): loss=15302015.065727167\n",
      "Gradient Descent(6/49): loss=1887698046.343674\n",
      "Gradient Descent(7/49): loss=274575068342.2398\n",
      "Gradient Descent(8/49): loss=41776813268838.0\n",
      "Gradient Descent(9/49): loss=6425141737170554.0\n",
      "Gradient Descent(10/49): loss=9.906256973565164e+17\n",
      "Gradient Descent(11/49): loss=1.5282126904699863e+20\n",
      "Gradient Descent(12/49): loss=2.3578413473629005e+22\n",
      "Gradient Descent(13/49): loss=3.6379630830490837e+24\n",
      "Gradient Descent(14/49): loss=5.6131280770015996e+26\n",
      "Gradient Descent(15/49): loss=8.660685967323003e+28\n",
      "Gradient Descent(16/49): loss=1.3362871302206507e+31\n",
      "Gradient Descent(17/49): loss=2.0618037026889173e+33\n",
      "Gradient Descent(18/49): loss=3.181228413607311e+35\n",
      "Gradient Descent(19/49): loss=4.908427630291951e+37\n",
      "Gradient Descent(20/49): loss=7.573383200698331e+39\n",
      "Gradient Descent(21/49): loss=1.1685235565739237e+42\n",
      "Gradient Descent(22/49): loss=1.8029555169101515e+44\n",
      "Gradient Descent(23/49): loss=2.7818425890576803e+46\n",
      "Gradient Descent(24/49): loss=4.292201398056565e+48\n",
      "Gradient Descent(25/49): loss=6.622586379961447e+50\n",
      "Gradient Descent(26/49): loss=1.0218218180515567e+53\n",
      "Gradient Descent(27/49): loss=1.576604317318552e+55\n",
      "Gradient Descent(28/49): loss=2.432597473918996e+57\n",
      "Gradient Descent(29/49): loss=3.7533389989579966e+59\n",
      "Gradient Descent(30/49): loss=5.791156897981748e+61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(31/49): loss=8.935376800856955e+63\n",
      "Gradient Descent(32/49): loss=1.3786702722752549e+66\n",
      "Gradient Descent(33/49): loss=2.1271981719598479e+68\n",
      "Gradient Descent(34/49): loss=3.282127825474615e+70\n",
      "Gradient Descent(35/49): loss=5.064108837979103e+72\n",
      "Gradient Descent(36/49): loss=7.813589136854986e+74\n",
      "Gradient Descent(37/49): loss=1.2055857635149663e+77\n",
      "Gradient Descent(38/49): loss=1.8601400812520567e+79\n",
      "Gradient Descent(39/49): loss=2.870074636409293e+81\n",
      "Gradient Descent(40/49): loss=4.428337683587389e+83\n",
      "Gradient Descent(41/49): loss=6.83263577577687e+85\n",
      "Gradient Descent(42/49): loss=1.0542310677311949e+88\n",
      "Gradient Descent(43/49): loss=1.6266096725217456e+90\n",
      "Gradient Descent(44/49): loss=2.509752470523736e+92\n",
      "Gradient Descent(45/49): loss=3.8723841187632685e+94\n",
      "Gradient Descent(46/49): loss=5.974835741518775e+96\n",
      "Gradient Descent(47/49): loss=9.218781258077087e+98\n",
      "Gradient Descent(48/49): loss=1.4223977287561465e+101\n",
      "Gradient Descent(49/49): loss=2.194666780923984e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1919425635463683\n",
      "Gradient Descent(2/49): loss=97.86954429499586\n",
      "Gradient Descent(3/49): loss=3755.1746026840356\n",
      "Gradient Descent(4/49): loss=181856.50797979737\n",
      "Gradient Descent(5/49): loss=13748249.557508703\n",
      "Gradient Descent(6/49): loss=1581676366.9206176\n",
      "Gradient Descent(7/49): loss=220302566876.78326\n",
      "Gradient Descent(8/49): loss=32467526286724.12\n",
      "Gradient Descent(9/49): loss=4853467300748836.0\n",
      "Gradient Descent(10/49): loss=7.280170688120799e+17\n",
      "Gradient Descent(11/49): loss=1.0929117488773993e+20\n",
      "Gradient Descent(12/49): loss=1.6410157344228106e+22\n",
      "Gradient Descent(13/49): loss=2.4641115638838963e+24\n",
      "Gradient Descent(14/49): loss=3.7000937612702154e+26\n",
      "Gradient Descent(15/49): loss=5.556050918661284e+28\n",
      "Gradient Descent(16/49): loss=8.342956365511895e+30\n",
      "Gradient Descent(17/49): loss=1.2527770551185181e+33\n",
      "Gradient Descent(18/49): loss=1.8811681800397272e+35\n",
      "Gradient Descent(19/49): loss=2.8247593905467164e+37\n",
      "Gradient Descent(20/49): loss=4.24165458183336e+39\n",
      "Gradient Descent(21/49): loss=6.3692623379315594e+41\n",
      "Gradient Descent(22/49): loss=9.564075044584125e+43\n",
      "Gradient Descent(23/49): loss=1.4361401149403903e+46\n",
      "Gradient Descent(24/49): loss=2.156505903748301e+48\n",
      "Gradient Descent(25/49): loss=3.238206122458427e+50\n",
      "Gradient Descent(26/49): loss=4.862485594546842e+52\n",
      "Gradient Descent(27/49): loss=7.301501282823524e+54\n",
      "Gradient Descent(28/49): loss=1.0963923686038532e+57\n",
      "Gradient Descent(29/49): loss=1.6463411829573065e+59\n",
      "Gradient Descent(30/49): loss=2.4721435211672902e+61\n",
      "Gradient Descent(31/49): loss=3.712167108807553e+63\n",
      "Gradient Descent(32/49): loss=5.574184720960668e+65\n",
      "Gradient Descent(33/49): loss=8.370187653909693e+67\n",
      "Gradient Descent(34/49): loss=1.2568661583498284e+70\n",
      "Gradient Descent(35/49): loss=1.8873083917863908e+72\n",
      "Gradient Descent(36/49): loss=2.833979530791004e+74\n",
      "Gradient Descent(37/49): loss=4.255499533566121e+76\n",
      "Gradient Descent(38/49): loss=6.390051898196657e+78\n",
      "Gradient Descent(39/49): loss=9.595292618309521e+80\n",
      "Gradient Descent(40/49): loss=1.4408277412734027e+83\n",
      "Gradient Descent(41/49): loss=2.163544836623046e+85\n",
      "Gradient Descent(42/49): loss=3.2487757738069835e+87\n",
      "Gradient Descent(43/49): loss=4.8783569676093515e+89\n",
      "Gradient Descent(44/49): loss=7.325333713485407e+91\n",
      "Gradient Descent(45/49): loss=1.0999710429190227e+94\n",
      "Gradient Descent(46/49): loss=1.6517149150938622e+96\n",
      "Gradient Descent(47/49): loss=2.480212709512505e+98\n",
      "Gradient Descent(48/49): loss=3.724283790267746e+100\n",
      "Gradient Descent(49/49): loss=5.592379112183903e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2515208418776527\n",
      "Gradient Descent(2/49): loss=99.43688968392078\n",
      "Gradient Descent(3/49): loss=3769.7690576789705\n",
      "Gradient Descent(4/49): loss=177574.45363954405\n",
      "Gradient Descent(5/49): loss=12922636.228562254\n",
      "Gradient Descent(6/49): loss=1453079181.9685166\n",
      "Gradient Descent(7/49): loss=200924097290.86365\n",
      "Gradient Descent(8/49): loss=29563078741760.086\n",
      "Gradient Descent(9/49): loss=4418473240148478.5\n",
      "Gradient Descent(10/49): loss=6.628723858795615e+17\n",
      "Gradient Descent(11/49): loss=9.953500111676195e+19\n",
      "Gradient Descent(12/49): loss=1.4949050973664722e+22\n",
      "Gradient Descent(13/49): loss=2.245293713266156e+24\n",
      "Gradient Descent(14/49): loss=3.3723903682438104e+26\n",
      "Gradient Descent(15/49): loss=5.065283254322455e+28\n",
      "Gradient Descent(16/49): loss=7.60798977580263e+30\n",
      "Gradient Descent(17/49): loss=1.1427103794754358e+33\n",
      "Gradient Descent(18/49): loss=1.716336507050105e+35\n",
      "Gradient Descent(19/49): loss=2.5779157030176437e+37\n",
      "Gradient Descent(20/49): loss=3.871996754915549e+39\n",
      "Gradient Descent(21/49): loss=5.81569011736021e+41\n",
      "Gradient Descent(22/49): loss=8.735092947106843e+43\n",
      "Gradient Descent(23/49): loss=1.3119999046875184e+46\n",
      "Gradient Descent(24/49): loss=1.970607250929552e+48\n",
      "Gradient Descent(25/49): loss=2.959827149032401e+50\n",
      "Gradient Descent(26/49): loss=4.445622915484542e+52\n",
      "Gradient Descent(27/49): loss=6.67726935106458e+54\n",
      "Gradient Descent(28/49): loss=1.0029174051485862e+57\n",
      "Gradient Descent(29/49): loss=1.506369248665437e+59\n",
      "Gradient Descent(30/49): loss=2.262547545466779e+61\n",
      "Gradient Descent(31/49): loss=3.398317776357218e+63\n",
      "Gradient Descent(32/49): loss=5.104230287776292e+65\n",
      "Gradient Descent(33/49): loss=7.666489288291463e+67\n",
      "Gradient Descent(34/49): loss=1.151496987670078e+70\n",
      "Gradient Descent(35/49): loss=1.7295339010494944e+72\n",
      "Gradient Descent(36/49): loss=2.5977380287654475e+74\n",
      "Gradient Descent(37/49): loss=3.901769639785269e+76\n",
      "Gradient Descent(38/49): loss=5.860408614484168e+78\n",
      "Gradient Descent(39/49): loss=8.802259564101439e+80\n",
      "Gradient Descent(40/49): loss=1.3220882455588734e+83\n",
      "Gradient Descent(41/49): loss=1.985759811234744e+85\n",
      "Gradient Descent(42/49): loss=2.9825861028271863e+87\n",
      "Gradient Descent(43/49): loss=4.479806576026059e+89\n",
      "Gradient Descent(44/49): loss=6.728612776537617e+91\n",
      "Gradient Descent(45/49): loss=1.0106291226695365e+94\n",
      "Gradient Descent(46/49): loss=1.5179521507751163e+96\n",
      "Gradient Descent(47/49): loss=2.2799449178314206e+98\n",
      "Gradient Descent(48/49): loss=3.4244484094514353e+100\n",
      "Gradient Descent(49/49): loss=5.143478167949979e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1385112477902153\n",
      "Gradient Descent(2/49): loss=94.33235236222154\n",
      "Gradient Descent(3/49): loss=3477.2310154159095\n",
      "Gradient Descent(4/49): loss=151923.07161134962\n",
      "Gradient Descent(5/49): loss=9814423.758453112\n",
      "Gradient Descent(6/49): loss=1023318838.6384667\n",
      "Gradient Descent(7/49): loss=139186807071.08118\n",
      "Gradient Descent(8/49): loss=20613503773433.11\n",
      "Gradient Descent(9/49): loss=3119626576058866.5\n",
      "Gradient Descent(10/49): loss=4.7455603960566944e+17\n",
      "Gradient Descent(11/49): loss=7.227611907058177e+19\n",
      "Gradient Descent(12/49): loss=1.1010925730064253e+22\n",
      "Gradient Descent(13/49): loss=1.6775719701677477e+24\n",
      "Gradient Descent(14/49): loss=2.555907179292842e+26\n",
      "Gradient Descent(15/49): loss=3.8941307344524986e+28\n",
      "Gradient Descent(16/49): loss=5.933027127581958e+30\n",
      "Gradient Descent(17/49): loss=9.039454507968905e+32\n",
      "Gradient Descent(18/49): loss=1.3772352574281963e+35\n",
      "Gradient Descent(19/49): loss=2.0983312347523696e+37\n",
      "Gradient Descent(20/49): loss=3.1969802962735827e+39\n",
      "Gradient Descent(21/49): loss=4.8708625460122104e+41\n",
      "Gradient Descent(22/49): loss=7.421159890440502e+43\n",
      "Gradient Descent(23/49): loss=1.1306747747632634e+46\n",
      "Gradient Descent(24/49): loss=1.722676057608947e+48\n",
      "Gradient Descent(25/49): loss=2.6246387252119905e+50\n",
      "Gradient Descent(26/49): loss=3.998853067852142e+52\n",
      "Gradient Descent(27/49): loss=6.092581696926988e+54\n",
      "Gradient Descent(28/49): loss=9.282549547055015e+56\n",
      "Gradient Descent(29/49): loss=1.414272805516778e+59\n",
      "Gradient Descent(30/49): loss=2.154760993501931e+61\n",
      "Gradient Descent(31/49): loss=3.282955679417666e+63\n",
      "Gradient Descent(32/49): loss=5.001853117595534e+65\n",
      "Gradient Descent(33/49): loss=7.620734805179715e+67\n",
      "Gradient Descent(34/49): loss=1.1610816552485166e+70\n",
      "Gradient Descent(35/49): loss=1.769003442080573e+72\n",
      "Gradient Descent(36/49): loss=2.6952223075327734e+74\n",
      "Gradient Descent(37/49): loss=4.106392963531296e+76\n",
      "Gradient Descent(38/49): loss=6.256427576979794e+78\n",
      "Gradient Descent(39/49): loss=9.532182227473205e+80\n",
      "Gradient Descent(40/49): loss=1.4523063984961749e+83\n",
      "Gradient Descent(41/49): loss=2.21270830202328e+85\n",
      "Gradient Descent(42/49): loss=3.3712431721794697e+87\n",
      "Gradient Descent(43/49): loss=5.13636637760809e+89\n",
      "Gradient Descent(44/49): loss=7.825676825314153e+91\n",
      "Gradient Descent(45/49): loss=1.1923062583938212e+94\n",
      "Gradient Descent(46/49): loss=1.8165766943078648e+96\n",
      "Gradient Descent(47/49): loss=2.7677040718950784e+98\n",
      "Gradient Descent(48/49): loss=4.2168248957433245e+100\n",
      "Gradient Descent(49/49): loss=6.424679712663524e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1967162735679295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2/49): loss=98.01960629807945\n",
      "Gradient Descent(3/49): loss=3810.8085974175174\n",
      "Gradient Descent(4/49): loss=194290.3886345702\n",
      "Gradient Descent(5/49): loss=16105708.883180587\n",
      "Gradient Descent(6/49): loss=2005468350.3752122\n",
      "Gradient Descent(7/49): loss=294567969212.2287\n",
      "Gradient Descent(8/49): loss=45266944133180.36\n",
      "Gradient Descent(9/49): loss=7031922250413318.0\n",
      "Gradient Descent(10/49): loss=1.0950978238233413e+18\n",
      "Gradient Descent(11/49): loss=1.7063998420720383e+20\n",
      "Gradient Descent(12/49): loss=2.6592899248976376e+22\n",
      "Gradient Descent(13/49): loss=4.1444186078452624e+24\n",
      "Gradient Descent(14/49): loss=6.458988704107769e+26\n",
      "Gradient Descent(15/49): loss=1.0066213040935695e+29\n",
      "Gradient Descent(16/49): loss=1.5688010314454445e+31\n",
      "Gradient Descent(17/49): loss=2.444948133568356e+33\n",
      "Gradient Descent(18/49): loss=3.810407673422047e+35\n",
      "Gradient Descent(19/49): loss=5.938451822683209e+37\n",
      "Gradient Descent(20/49): loss=9.254970362101827e+39\n",
      "Gradient Descent(21/49): loss=1.4423704861172308e+42\n",
      "Gradient Descent(22/49): loss=2.2479084620825634e+44\n",
      "Gradient Descent(23/49): loss=3.503324910345812e+46\n",
      "Gradient Descent(24/49): loss=5.4598688667738636e+48\n",
      "Gradient Descent(25/49): loss=8.50910743515453e+50\n",
      "Gradient Descent(26/49): loss=1.3261290904554857e+53\n",
      "Gradient Descent(27/49): loss=2.0667483375363051e+55\n",
      "Gradient Descent(28/49): loss=3.220990114350138e+57\n",
      "Gradient Descent(29/49): loss=5.019855164905429e+59\n",
      "Gradient Descent(30/49): loss=7.823353994276782e+61\n",
      "Gradient Descent(31/49): loss=1.2192556499968475e+64\n",
      "Gradient Descent(32/49): loss=1.9001880026606726e+66\n",
      "Gradient Descent(33/49): loss=2.961408827972092e+68\n",
      "Gradient Descent(34/49): loss=4.6153023985580147e+70\n",
      "Gradient Descent(35/49): loss=7.192865783655348e+72\n",
      "Gradient Descent(36/49): loss=1.1209951962810794e+75\n",
      "Gradient Descent(37/49): loss=1.7470508527223115e+77\n",
      "Gradient Descent(38/49): loss=2.7227473339078563e+79\n",
      "Gradient Descent(39/49): loss=4.243352752297102e+81\n",
      "Gradient Descent(40/49): loss=6.613188949333781e+83\n",
      "Gradient Descent(41/49): loss=1.0306536041792686e+86\n",
      "Gradient Descent(42/49): loss=1.606255106191631e+88\n",
      "Gradient Descent(43/49): loss=2.503319695098898e+90\n",
      "Gradient Descent(44/49): loss=3.9013787235378086e+92\n",
      "Gradient Descent(45/49): loss=6.0802285757883805e+94\n",
      "Gradient Descent(46/49): loss=9.475926884716505e+96\n",
      "Gradient Descent(47/49): loss=1.4768061628809787e+99\n",
      "Gradient Descent(48/49): loss=2.301575845040356e+101\n",
      "Gradient Descent(49/49): loss=3.586964561509765e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2230192786995153\n",
      "Gradient Descent(2/49): loss=100.00564843654598\n",
      "Gradient Descent(3/49): loss=3878.2134393560077\n",
      "Gradient Descent(4/49): loss=189698.88422393933\n",
      "Gradient Descent(5/49): loss=14473416.20653259\n",
      "Gradient Descent(6/49): loss=1680609651.4694855\n",
      "Gradient Descent(7/49): loss=236368278702.4437\n",
      "Gradient Descent(8/49): loss=35183165748613.805\n",
      "Gradient Descent(9/49): loss=5312324405253415.0\n",
      "Gradient Descent(10/49): loss=8.048769783876435e+17\n",
      "Gradient Descent(11/49): loss=1.2204805563407444e+20\n",
      "Gradient Descent(12/49): loss=1.8510449753973657e+22\n",
      "Gradient Descent(13/49): loss=2.8075222945086375e+24\n",
      "Gradient Descent(14/49): loss=4.258280249500023e+26\n",
      "Gradient Descent(15/49): loss=6.458719195313344e+28\n",
      "Gradient Descent(16/49): loss=9.796226911517391e+30\n",
      "Gradient Descent(17/49): loss=1.4858375627228324e+33\n",
      "Gradient Descent(18/49): loss=2.253636384901724e+35\n",
      "Gradient Descent(19/49): loss=3.4181912781751527e+37\n",
      "Gradient Descent(20/49): loss=5.184523872442381e+39\n",
      "Gradient Descent(21/49): loss=7.86359966693205e+41\n",
      "Gradient Descent(22/49): loss=1.192707397054625e+44\n",
      "Gradient Descent(23/49): loss=1.8090327525302449e+46\n",
      "Gradient Descent(24/49): loss=2.7438410358059616e+48\n",
      "Gradient Descent(25/49): loss=4.161706646412965e+50\n",
      "Gradient Descent(26/49): loss=6.312246950456619e+52\n",
      "Gradient Descent(27/49): loss=9.57406779209074e+54\n",
      "Gradient Descent(28/49): loss=1.4521417619904808e+57\n",
      "Gradient Descent(29/49): loss=2.2025284787088592e+59\n",
      "Gradient Descent(30/49): loss=3.340673635660682e+61\n",
      "Gradient Descent(31/49): loss=5.066949393789628e+63\n",
      "Gradient Descent(32/49): loss=7.685269187975716e+65\n",
      "Gradient Descent(33/49): loss=1.1656592142806974e+68\n",
      "Gradient Descent(34/49): loss=1.7680075617434528e+70\n",
      "Gradient Descent(35/49): loss=2.6816162906676823e+72\n",
      "Gradient Descent(36/49): loss=4.0673275872660294e+74\n",
      "Gradient Descent(37/49): loss=6.169098002464982e+76\n",
      "Gradient Descent(38/49): loss=9.356947368382238e+78\n",
      "Gradient Descent(39/49): loss=1.419210134442513e+81\n",
      "Gradient Descent(40/49): loss=2.1525796035898786e+83\n",
      "Gradient Descent(41/49): loss=3.264913938633419e+85\n",
      "Gradient Descent(42/49): loss=4.952041266629856e+87\n",
      "Gradient Descent(43/49): loss=7.51098288265135e+89\n",
      "Gradient Descent(44/49): loss=1.139224429401301e+92\n",
      "Gradient Descent(45/49): loss=1.7279127390136995e+94\n",
      "Gradient Descent(46/49): loss=2.620802676444488e+96\n",
      "Gradient Descent(47/49): loss=3.975088853606712e+98\n",
      "Gradient Descent(48/49): loss=6.029195382044455e+100\n",
      "Gradient Descent(49/49): loss=9.144750795163445e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.283269749265008\n",
      "Gradient Descent(2/49): loss=101.60834270119888\n",
      "Gradient Descent(3/49): loss=3893.412658357122\n",
      "Gradient Descent(4/49): loss=185247.64563065214\n",
      "Gradient Descent(5/49): loss=13605532.025754057\n",
      "Gradient Descent(6/49): loss=1544036944.8391502\n",
      "Gradient Descent(7/49): loss=215579482178.1969\n",
      "Gradient Descent(8/49): loss=32035881561688.484\n",
      "Gradient Descent(9/49): loss=4836205575501203.0\n",
      "Gradient Descent(10/49): loss=7.328541557213618e+17\n",
      "Gradient Descent(11/49): loss=1.1115299162697954e+20\n",
      "Gradient Descent(12/49): loss=1.6862320741247447e+22\n",
      "Gradient Descent(13/49): loss=2.5582056681930576e+24\n",
      "Gradient Descent(14/49): loss=3.881135069160477e+26\n",
      "Gradient Descent(15/49): loss=5.888209934259582e+28\n",
      "Gradient Descent(16/49): loss=8.933221550995558e+30\n",
      "Gradient Descent(17/49): loss=1.355292369276117e+33\n",
      "Gradient Descent(18/49): loss=2.0561646933713216e+35\n",
      "Gradient Descent(19/49): loss=3.1194843117623306e+37\n",
      "Gradient Descent(20/49): loss=4.732686259596022e+39\n",
      "Gradient Descent(21/49): loss=7.180135241731484e+41\n",
      "Gradient Descent(22/49): loss=1.0893251584360385e+44\n",
      "Gradient Descent(23/49): loss=1.652655919261839e+46\n",
      "Gradient Descent(24/49): loss=2.5073060750846605e+48\n",
      "Gradient Descent(25/49): loss=3.8039277752225216e+50\n",
      "Gradient Descent(26/49): loss=5.771081027124465e+52\n",
      "Gradient Descent(27/49): loss=8.755522762177285e+54\n",
      "Gradient Descent(28/49): loss=1.3283330883538411e+57\n",
      "Gradient Descent(29/49): loss=2.0152637844058514e+59\n",
      "Gradient Descent(30/49): loss=3.0574320223934103e+61\n",
      "Gradient Descent(31/49): loss=4.638544414825891e+63\n",
      "Gradient Descent(32/49): loss=7.037309130905548e+65\n",
      "Gradient Descent(33/49): loss=1.0676564752864717e+68\n",
      "Gradient Descent(34/49): loss=1.6197815500460133e+70\n",
      "Gradient Descent(35/49): loss=2.4574311406348616e+72\n",
      "Gradient Descent(36/49): loss=3.728260647733982e+74\n",
      "Gradient Descent(37/49): loss=5.6562835994056105e+76\n",
      "Gradient Descent(38/49): loss=8.581359293200519e+78\n",
      "Gradient Descent(39/49): loss=1.3019100974133681e+81\n",
      "Gradient Descent(40/49): loss=1.9751764770994954e+83\n",
      "Gradient Descent(41/49): loss=2.996614069925625e+85\n",
      "Gradient Descent(42/49): loss=4.546275225625746e+87\n",
      "Gradient Descent(43/49): loss=6.897324094741279e+89\n",
      "Gradient Descent(44/49): loss=1.0464188221545738e+92\n",
      "Gradient Descent(45/49): loss=1.5875611125686351e+94\n",
      "Gradient Descent(46/49): loss=2.4085483104658694e+96\n",
      "Gradient Descent(47/49): loss=3.654098678735001e+98\n",
      "Gradient Descent(48/49): loss=5.543769703896875e+100\n",
      "Gradient Descent(49/49): loss=8.410660256302845e+102\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.1690130509899035\n",
      "Gradient Descent(2/49): loss=96.39385182792077\n",
      "Gradient Descent(3/49): loss=3591.560553912836\n",
      "Gradient Descent(4/49): loss=158526.09020351755\n",
      "Gradient Descent(5/49): loss=10336328.714965016\n",
      "Gradient Descent(6/49): loss=1087533242.492465\n",
      "Gradient Descent(7/49): loss=149341856769.1799\n",
      "Gradient Descent(8/49): loss=22337095294998.406\n",
      "Gradient Descent(9/49): loss=3414410949313361.0\n",
      "Gradient Descent(10/49): loss=5.246289016588637e+17\n",
      "Gradient Descent(11/49): loss=8.070762989582308e+19\n",
      "Gradient Descent(12/49): loss=1.2419370609119307e+22\n",
      "Gradient Descent(13/49): loss=1.9112308086255206e+24\n",
      "Gradient Descent(14/49): loss=2.9412594638845465e+26\n",
      "Gradient Descent(15/49): loss=4.52642246942434e+28\n",
      "Gradient Descent(16/49): loss=6.965899347312587e+30\n",
      "Gradient Descent(17/49): loss=1.0720113604885774e+33\n",
      "Gradient Descent(18/49): loss=1.6497631553927793e+35\n",
      "Gradient Descent(19/49): loss=2.538889602972348e+37\n",
      "Gradient Descent(20/49): loss=3.907203534482154e+39\n",
      "Gradient Descent(21/49): loss=6.012959149806988e+41\n",
      "Gradient Descent(22/49): loss=9.253594654833529e+43\n",
      "Gradient Descent(23/49): loss=1.4240744349868403e+46\n",
      "Gradient Descent(24/49): loss=2.191567787484955e+48\n",
      "Gradient Descent(25/49): loss=3.3726954498653508e+50\n",
      "Gradient Descent(26/49): loss=5.190382274509083e+52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=7.987696652721937e+54\n",
      "Gradient Descent(28/49): loss=1.2292600899409178e+57\n",
      "Gradient Descent(29/49): loss=1.8917598331762067e+59\n",
      "Gradient Descent(30/49): loss=2.911308433181842e+61\n",
      "Gradient Descent(31/49): loss=4.4803344718897326e+63\n",
      "Gradient Descent(32/49): loss=6.894974352842882e+65\n",
      "Gradient Descent(33/49): loss=1.061096478948991e+68\n",
      "Gradient Descent(34/49): loss=1.6329658096171183e+70\n",
      "Gradient Descent(35/49): loss=2.513039472169218e+72\n",
      "Gradient Descent(36/49): loss=3.8674216884928006e+74\n",
      "Gradient Descent(37/49): loss=5.95173720200819e+76\n",
      "Gradient Descent(38/49): loss=9.159377635794491e+78\n",
      "Gradient Descent(39/49): loss=1.4095749833642785e+81\n",
      "Gradient Descent(40/49): loss=2.1692539741583526e+83\n",
      "Gradient Descent(41/49): loss=3.3383557880480436e+85\n",
      "Gradient Descent(42/49): loss=5.137535530812094e+87\n",
      "Gradient Descent(43/49): loss=7.906368585653171e+89\n",
      "Gradient Descent(44/49): loss=1.2167441731020348e+92\n",
      "Gradient Descent(45/49): loss=1.872498564592839e+94\n",
      "Gradient Descent(46/49): loss=2.881666460307155e+96\n",
      "Gradient Descent(47/49): loss=4.4347171984426715e+98\n",
      "Gradient Descent(48/49): loss=6.824772020307752e+100\n",
      "Gradient Descent(49/49): loss=1.0502927479915104e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.227690780362093\n",
      "Gradient Descent(2/49): loss=100.14877399111688\n",
      "Gradient Descent(3/49): loss=3934.922756958948\n",
      "Gradient Descent(4/49): loss=202596.82564281407\n",
      "Gradient Descent(5/49): loss=16947243.682542466\n",
      "Gradient Descent(6/49): loss=2129942376.124138\n",
      "Gradient Descent(7/49): loss=315905866739.1635\n",
      "Gradient Descent(8/49): loss=49029023738654.55\n",
      "Gradient Descent(9/49): loss=7692542886315316.0\n",
      "Gradient Descent(10/49): loss=1.209982388264234e+18\n",
      "Gradient Descent(11/49): loss=1.904315454120233e+20\n",
      "Gradient Descent(12/49): loss=2.997479125798229e+22\n",
      "Gradient Descent(13/49): loss=4.7183113270565347e+24\n",
      "Gradient Descent(14/49): loss=7.427112978283957e+26\n",
      "Gradient Descent(15/49): loss=1.169106718939538e+29\n",
      "Gradient Descent(16/49): loss=1.8402992130798682e+31\n",
      "Gradient Descent(17/49): loss=2.896828339575974e+33\n",
      "Gradient Descent(18/49): loss=4.559918587780988e+35\n",
      "Gradient Descent(19/49): loss=7.177801091556491e+37\n",
      "Gradient Descent(20/49): loss=1.129862903680799e+40\n",
      "Gradient Descent(21/49): loss=1.778525435745935e+42\n",
      "Gradient Descent(22/49): loss=2.799589857721718e+44\n",
      "Gradient Descent(23/49): loss=4.4068548101842383e+46\n",
      "Gradient Descent(24/49): loss=6.93686229235794e+48\n",
      "Gradient Descent(25/49): loss=1.091936551936378e+51\n",
      "Gradient Descent(26/49): loss=1.7188252890192313e+53\n",
      "Gradient Descent(27/49): loss=2.7056154214574057e+55\n",
      "Gradient Descent(28/49): loss=4.258928964796274e+57\n",
      "Gradient Descent(29/49): loss=6.704011140434104e+59\n",
      "Gradient Descent(30/49): loss=1.0552832823125941e+62\n",
      "Gradient Descent(31/49): loss=1.661129110021649e+64\n",
      "Gradient Descent(32/49): loss=2.614795445365477e+66\n",
      "Gradient Descent(33/49): loss=4.115968578152845e+68\n",
      "Gradient Descent(34/49): loss=6.478976153323175e+70\n",
      "Gradient Descent(35/49): loss=1.0198603608915091e+73\n",
      "Gradient Descent(36/49): loss=1.6053696311017937e+75\n",
      "Gradient Descent(37/49): loss=2.527024042988658e+77\n",
      "Gradient Descent(38/49): loss=3.9778069736250844e+79\n",
      "Gradient Descent(39/49): loss=6.2614949641345204e+81\n",
      "Gradient Descent(40/49): loss=9.856264883097455e+83\n",
      "Gradient Descent(41/49): loss=1.5514818426306768e+86\n",
      "Gradient Descent(42/49): loss=2.4421988821958983e+88\n",
      "Gradient Descent(43/49): loss=3.8442830694595266e+90\n",
      "Gradient Descent(44/49): loss=6.051314012904841e+92\n",
      "Gradient Descent(45/49): loss=9.525417515086188e+94\n",
      "Gradient Descent(46/49): loss=1.4994029171716167e+97\n",
      "Gradient Descent(47/49): loss=2.360221065861022e+99\n",
      "Gradient Descent(48/49): loss=3.715241190968332e+101\n",
      "Gradient Descent(49/49): loss=5.848188250973112e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2542678091495767\n",
      "Gradient Descent(2/49): loss=102.17667967234293\n",
      "Gradient Descent(3/49): loss=4004.5960602728533\n",
      "Gradient Descent(4/49): loss=197835.9982709423\n",
      "Gradient Descent(5/49): loss=15232873.363543749\n",
      "Gradient Descent(6/49): loss=1785189404.2357876\n",
      "Gradient Descent(7/49): loss=253516477203.08096\n",
      "Gradient Descent(8/49): loss=38110658720799.24\n",
      "Gradient Descent(9/49): loss=5811940840605861.0\n",
      "Gradient Descent(10/49): loss=8.894055081039235e+17\n",
      "Gradient Descent(11/49): loss=1.3621885948261445e+20\n",
      "Gradient Descent(12/49): loss=2.086700108561696e+22\n",
      "Gradient Descent(13/49): loss=3.1967092600922005e+24\n",
      "Gradient Descent(14/49): loss=4.897236373090074e+26\n",
      "Gradient Descent(15/49): loss=7.50239872215788e+28\n",
      "Gradient Descent(16/49): loss=1.1493425609797617e+31\n",
      "Gradient Descent(17/49): loss=1.7607548848182254e+33\n",
      "Gradient Descent(18/49): loss=2.697418489282548e+35\n",
      "Gradient Descent(19/49): loss=4.1323563151750875e+37\n",
      "Gradient Descent(20/49): loss=6.330633832938047e+39\n",
      "Gradient Descent(21/49): loss=9.698322624961677e+41\n",
      "Gradient Descent(22/49): loss=1.4857510990311913e+44\n",
      "Gradient Descent(23/49): loss=2.2761217724911897e+46\n",
      "Gradient Descent(24/49): loss=3.486943625092972e+48\n",
      "Gradient Descent(25/49): loss=5.341882842800305e+50\n",
      "Gradient Descent(26/49): loss=8.183588659381916e+52\n",
      "Gradient Descent(27/49): loss=1.2536988420896025e+55\n",
      "Gradient Descent(28/49): loss=1.920625354079704e+57\n",
      "Gradient Descent(29/49): loss=2.942334815102487e+59\n",
      "Gradient Descent(30/49): loss=4.507560074521785e+61\n",
      "Gradient Descent(31/49): loss=6.905433644442369e+63\n",
      "Gradient Descent(32/49): loss=1.0578897015112103e+66\n",
      "Gradient Descent(33/49): loss=1.620652196787347e+68\n",
      "Gradient Descent(34/49): loss=2.4827858133032638e+70\n",
      "Gradient Descent(35/49): loss=3.803546132204921e+72\n",
      "Gradient Descent(36/49): loss=5.826907461084444e+74\n",
      "Gradient Descent(37/49): loss=8.926630407492691e+76\n",
      "Gradient Descent(38/49): loss=1.3675303917928077e+79\n",
      "Gradient Descent(39/49): loss=2.0950115408690715e+81\n",
      "Gradient Descent(40/49): loss=3.209488712437665e+83\n",
      "Gradient Descent(41/49): loss=4.916831050482767e+85\n",
      "Gradient Descent(42/49): loss=7.532423306337231e+87\n",
      "Gradient Descent(43/49): loss=1.1539424536517649e+90\n",
      "Gradient Descent(44/49): loss=1.7678018509920636e+92\n",
      "Gradient Descent(45/49): loss=2.7082142393506775e+94\n",
      "Gradient Descent(46/49): loss=4.148895059763454e+96\n",
      "Gradient Descent(47/49): loss=6.355970649152333e+98\n",
      "Gradient Descent(48/49): loss=9.737137794752913e+100\n",
      "Gradient Descent(49/49): loss=1.4916974553155228e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3151941301341177\n",
      "Gradient Descent(2/49): loss=103.81531622121709\n",
      "Gradient Descent(3/49): loss=4020.41975218279\n",
      "Gradient Descent(4/49): loss=193209.784526955\n",
      "Gradient Descent(5/49): loss=14320781.477571506\n",
      "Gradient Descent(6/49): loss=1640190120.6969445\n",
      "Gradient Descent(7/49): loss=231222535724.53064\n",
      "Gradient Descent(8/49): loss=34701601895100.914\n",
      "Gradient Descent(9/49): loss=5291044292693546.0\n",
      "Gradient Descent(10/49): loss=8.098182348036241e+17\n",
      "Gradient Descent(11/49): loss=1.2405866272074664e+20\n",
      "Gradient Descent(12/49): loss=1.9009028231464627e+22\n",
      "Gradient Descent(13/49): loss=2.9128279784251884e+24\n",
      "Gradient Descent(14/49): loss=4.46349441023621e+26\n",
      "Gradient Descent(15/49): loss=6.83968960747101e+28\n",
      "Gradient Descent(16/49): loss=1.0480888129232048e+31\n",
      "Gradient Descent(17/49): loss=1.606052904281906e+33\n",
      "Gradient Descent(18/49): loss=2.4610567324923297e+35\n",
      "Gradient Descent(19/49): loss=3.7712333627860925e+37\n",
      "Gradient Descent(20/49): loss=5.778900144449e+39\n",
      "Gradient Descent(21/49): loss=8.855375333150663e+41\n",
      "Gradient Descent(22/49): loss=1.3569653453118625e+44\n",
      "Gradient Descent(23/49): loss=2.0793640914759605e+46\n",
      "Gradient Descent(24/49): loss=3.1863415229333054e+48\n",
      "Gradient Descent(25/49): loss=4.882633273511311e+50\n",
      "Gradient Descent(26/49): loss=7.48196874441176e+52\n",
      "Gradient Descent(27/49): loss=1.1465095401707883e+55\n",
      "Gradient Descent(28/49): loss=1.756869308875999e+57\n",
      "Gradient Descent(29/49): loss=2.6921623068313757e+59\n",
      "Gradient Descent(30/49): loss=4.12537110740507e+61\n",
      "Gradient Descent(31/49): loss=6.32156788267449e+63\n",
      "Gradient Descent(32/49): loss=9.686939539458642e+65\n",
      "Gradient Descent(33/49): loss=1.4843912045665777e+68\n",
      "Gradient Descent(34/49): loss=2.2746268201832605e+70\n",
      "Gradient Descent(35/49): loss=3.4855549906115616e+72\n",
      "Gradient Descent(36/49): loss=5.341137053681018e+74\n",
      "Gradient Descent(37/49): loss=8.184563176608826e+76\n",
      "Gradient Descent(38/49): loss=1.2541725426374058e+79\n",
      "Gradient Descent(39/49): loss=1.921848158251141e+81\n",
      "Gradient Descent(40/49): loss=2.944969864837189e+83\n",
      "Gradient Descent(41/49): loss=4.5127641679514016e+85\n",
      "Gradient Descent(42/49): loss=6.915194847561396e+87\n",
      "Gradient Descent(43/49): loss=1.0596591800507988e+90\n",
      "Gradient Descent(44/49): loss=1.6237829918298066e+92\n",
      "Gradient Descent(45/49): loss=2.4882256995398666e+94\n",
      "Gradient Descent(46/49): loss=3.812866105263242e+96\n",
      "Gradient Descent(47/49): loss=5.842696640965308e+98\n",
      "Gradient Descent(48/49): loss=8.953134753728971e+100\n",
      "Gradient Descent(49/49): loss=1.371945641613602e+103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.199683647317934\n",
      "Gradient Descent(2/49): loss=98.48910485750606\n",
      "Gradient Descent(3/49): loss=3709.0074536369407\n",
      "Gradient Descent(4/49): loss=165379.15310619088\n",
      "Gradient Descent(5/49): loss=10883118.297681587\n",
      "Gradient Descent(6/49): loss=1155425206.208892\n",
      "Gradient Descent(7/49): loss=160181515380.34354\n",
      "Gradient Descent(8/49): loss=24195107810823.52\n",
      "Gradient Descent(9/49): loss=3735367131177493.5\n",
      "Gradient Descent(10/49): loss=5.796949537409434e+17\n",
      "Gradient Descent(11/49): loss=9.007312435075164e+19\n",
      "Gradient Descent(12/49): loss=1.3999562589674205e+22\n",
      "Gradient Descent(13/49): loss=2.176018082883883e+24\n",
      "Gradient Descent(14/49): loss=3.3823398613141256e+26\n",
      "Gradient Descent(15/49): loss=5.257430633742551e+28\n",
      "Gradient Descent(16/49): loss=8.172035097928768e+30\n",
      "Gradient Descent(17/49): loss=1.2702434963216196e+33\n",
      "Gradient Descent(18/49): loss=1.9744391624906387e+35\n",
      "Gradient Descent(19/49): loss=3.0690257884054946e+37\n",
      "Gradient Descent(20/49): loss=4.770427720507148e+39\n",
      "Gradient Descent(21/49): loss=7.415050318467902e+41\n",
      "Gradient Descent(22/49): loss=1.1525794845698157e+44\n",
      "Gradient Descent(23/49): loss=1.7915447788455761e+46\n",
      "Gradient Descent(24/49): loss=2.784738699240431e+48\n",
      "Gradient Descent(25/49): loss=4.3285379827668e+50\n",
      "Gradient Descent(26/49): loss=6.728186408790395e+52\n",
      "Gradient Descent(27/49): loss=1.0458148347470668e+55\n",
      "Gradient Descent(28/49): loss=1.6255921018301227e+57\n",
      "Gradient Descent(29/49): loss=2.526785424851605e+59\n",
      "Gradient Descent(30/49): loss=3.9275809571507195e+61\n",
      "Gradient Descent(31/49): loss=6.1049474257905e+63\n",
      "Gradient Descent(32/49): loss=9.48939906733432e+65\n",
      "Gradient Descent(33/49): loss=1.4750117958217437e+68\n",
      "Gradient Descent(34/49): loss=2.2927266335574558e+70\n",
      "Gradient Descent(35/49): loss=3.563765002499737e+72\n",
      "Gradient Descent(36/49): loss=5.539439725239177e+74\n",
      "Gradient Descent(37/49): loss=8.610386051839441e+76\n",
      "Gradient Descent(38/49): loss=1.3383799019225024e+79\n",
      "Gradient Descent(39/49): loss=2.0803489542579e+81\n",
      "Gradient Descent(40/49): loss=3.233649702349191e+83\n",
      "Gradient Descent(41/49): loss=5.02631559772782e+85\n",
      "Gradient Descent(42/49): loss=7.8127969364175545e+87\n",
      "Gradient Descent(43/49): loss=1.2144043640492785e+90\n",
      "Gradient Descent(44/49): loss=1.8876440427468477e+92\n",
      "Gradient Descent(45/49): loss=2.9341133296298507e+94\n",
      "Gradient Descent(46/49): loss=4.560722697794244e+96\n",
      "Gradient Descent(47/49): loss=7.089089339572043e+98\n",
      "Gradient Descent(48/49): loss=1.1019128106328336e+101\n",
      "Gradient Descent(49/49): loss=1.7127895898544899e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.258835699222906\n",
      "Gradient Descent(2/49): loss=102.31259111482345\n",
      "Gradient Descent(3/49): loss=4062.3901710483665\n",
      "Gradient Descent(4/49): loss=211212.80767358546\n",
      "Gradient Descent(5/49): loss=17828197.04371328\n",
      "Gradient Descent(6/49): loss=2261465419.314098\n",
      "Gradient Descent(7/49): loss=338671867364.8084\n",
      "Gradient Descent(8/49): loss=53082728644257.88\n",
      "Gradient Descent(9/49): loss=8411476783413271.0\n",
      "Gradient Descent(10/49): loss=1.3362574665401047e+18\n",
      "Gradient Descent(11/49): loss=2.1240290009374933e+20\n",
      "Gradient Descent(12/49): loss=3.3766698808228954e+22\n",
      "Gradient Descent(13/49): loss=5.368216540048004e+24\n",
      "Gradient Descent(14/49): loss=8.534429318027797e+26\n",
      "Gradient Descent(15/49): loss=1.3568118945068786e+29\n",
      "Gradient Descent(16/49): loss=2.1570735717804154e+31\n",
      "Gradient Descent(17/49): loss=3.429338145157644e+33\n",
      "Gradient Descent(18/49): loss=5.451997787393927e+35\n",
      "Gradient Descent(19/49): loss=8.667643360582834e+37\n",
      "Gradient Descent(20/49): loss=1.377991049006189e+40\n",
      "Gradient Descent(21/49): loss=2.190744649482148e+42\n",
      "Gradient Descent(22/49): loss=3.482868864025587e+44\n",
      "Gradient Descent(23/49): loss=5.537101517976554e+46\n",
      "Gradient Descent(24/49): loss=8.802942177107269e+48\n",
      "Gradient Descent(25/49): loss=1.3995009974435096e+51\n",
      "Gradient Descent(26/49): loss=2.224941391685128e+53\n",
      "Gradient Descent(27/49): loss=3.5372352041740573e+55\n",
      "Gradient Descent(28/49): loss=5.623533696845867e+57\n",
      "Gradient Descent(29/49): loss=8.940352963309579e+59\n",
      "Gradient Descent(30/49): loss=1.4213467086254132e+62\n",
      "Gradient Descent(31/49): loss=2.2596719328768697e+64\n",
      "Gradient Descent(32/49): loss=3.592450183509137e+66\n",
      "Gradient Descent(33/49): loss=5.711315051191516e+68\n",
      "Gradient Descent(34/49): loss=9.079908682854501e+70\n",
      "Gradient Descent(35/49): loss=1.443533423563717e+73\n",
      "Gradient Descent(36/49): loss=2.294944605423663e+75\n",
      "Gradient Descent(37/49): loss=3.6485270489690677e+77\n",
      "Gradient Descent(38/49): loss=5.800466641155114e+79\n",
      "Gradient Descent(39/49): loss=9.221642817382973e+81\n",
      "Gradient Descent(40/49): loss=1.4660664652052598e+84\n",
      "Gradient Descent(41/49): loss=2.33076787180248e+86\n",
      "Gradient Descent(42/49): loss=3.705479254288744e+88\n",
      "Gradient Descent(43/49): loss=5.891009855625821e+90\n",
      "Gradient Descent(44/49): loss=9.365589371176142e+92\n",
      "Gradient Descent(45/49): loss=1.4889512395862145e+95\n",
      "Gradient Descent(46/49): loss=2.3671503265864194e+97\n",
      "Gradient Descent(47/49): loss=3.76332046321091e+99\n",
      "Gradient Descent(48/49): loss=5.982966417365396e+101\n",
      "Gradient Descent(49/49): loss=9.511782879309015e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2856881548965555\n",
      "Gradient Descent(2/49): loss=104.38301689274554\n",
      "Gradient Descent(3/49): loss=4134.394817043814\n",
      "Gradient Descent(4/49): loss=206277.3065404756\n",
      "Gradient Descent(5/49): loss=16028056.778365023\n",
      "Gradient Descent(6/49): loss=1895707260.4032278\n",
      "Gradient Descent(7/49): loss=271814154773.38458\n",
      "Gradient Descent(8/49): loss=41265352007710.555\n",
      "Gradient Descent(9/49): loss=6355706666529273.0\n",
      "Gradient Descent(10/49): loss=9.823237665164895e+17\n",
      "Gradient Descent(11/49): loss=1.5195206194127972e+20\n",
      "Gradient Descent(12/49): loss=2.3509562267330106e+22\n",
      "Gradient Descent(13/49): loss=3.6374996009558646e+24\n",
      "Gradient Descent(14/49): loss=5.6281574127848754e+26\n",
      "Gradient Descent(15/49): loss=8.708245732406554e+28\n",
      "Gradient Descent(16/49): loss=1.3473964268404864e+31\n",
      "Gradient Descent(17/49): loss=2.0847797146188247e+33\n",
      "Gradient Descent(18/49): loss=3.225707392500962e+35\n",
      "Gradient Descent(19/49): loss=4.991025285812617e+37\n",
      "Gradient Descent(20/49): loss=7.722440514044941e+39\n",
      "Gradient Descent(21/49): loss=1.1948664674441447e+42\n",
      "Gradient Descent(22/49): loss=1.848775490844302e+44\n",
      "Gradient Descent(23/49): loss=2.860546269200563e+46\n",
      "Gradient Descent(24/49): loss=4.4260241434473965e+48\n",
      "Gradient Descent(25/49): loss=6.848233824892677e+50\n",
      "Gradient Descent(26/49): loss=1.0596034951560622e+53\n",
      "Gradient Descent(27/49): loss=1.6394877798505236e+55\n",
      "Gradient Descent(28/49): loss=2.5367226444297342e+57\n",
      "Gradient Descent(29/49): loss=3.9249830671804875e+59\n",
      "Gradient Descent(30/49): loss=6.072990325324415e+61\n",
      "Gradient Descent(31/49): loss=9.396527541704386e+63\n",
      "Gradient Descent(32/49): loss=1.453892153817867e+66\n",
      "Gradient Descent(33/49): loss=2.249556961921863e+68\n",
      "Gradient Descent(34/49): loss=3.4806615550144257e+70\n",
      "Gradient Descent(35/49): loss=5.3855070423314036e+72\n",
      "Gradient Descent(36/49): loss=8.332808474646498e+74\n",
      "Gradient Descent(37/49): loss=1.2893065876501614e+77\n",
      "Gradient Descent(38/49): loss=1.9948994171843144e+79\n",
      "Gradient Descent(39/49): loss=3.0866387582300786e+81\n",
      "Gradient Descent(40/49): loss=4.775849219132746e+83\n",
      "Gradient Descent(41/49): loss=7.38950604539462e+85\n",
      "Gradient Descent(42/49): loss=1.1433526706867094e+88\n",
      "Gradient Descent(43/49): loss=1.7690699777979672e+90\n",
      "Gradient Descent(44/49): loss=2.737220690153688e+92\n",
      "Gradient Descent(45/49): loss=4.235206747407208e+94\n",
      "Gradient Descent(46/49): loss=6.552988678554885e+96\n",
      "Gradient Descent(47/49): loss=1.0139212364911693e+99\n",
      "Gradient Descent(48/49): loss=1.568805203604426e+101\n",
      "Gradient Descent(49/49): loss=2.427357942884715e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.347293984484987\n",
      "Gradient Descent(2/49): loss=106.05819573098532\n",
      "Gradient Descent(3/49): loss=4150.863184664086\n",
      "Gradient Descent(4/49): loss=201470.15868013457\n",
      "Gradient Descent(5/49): loss=15069741.893929519\n",
      "Gradient Descent(6/49): loss=1741807274.7684608\n",
      "Gradient Descent(7/49): loss=247914396146.26105\n",
      "Gradient Descent(8/49): loss=37574215468133.734\n",
      "Gradient Descent(9/49): loss=5786075635946191.0\n",
      "Gradient Descent(10/49): loss=8.944211906906678e+17\n",
      "Gradient Descent(11/49): loss=1.3838723194621603e+20\n",
      "Gradient Descent(12/49): loss=2.1416274623489596e+22\n",
      "Gradient Descent(13/49): loss=3.3144701588876126e+24\n",
      "Gradient Descent(14/49): loss=5.129671830554933e+26\n",
      "Gradient Descent(15/49): loss=7.939009166787023e+28\n",
      "Gradient Descent(16/49): loss=1.2286928233498093e+31\n",
      "Gradient Descent(17/49): loss=1.9016054360797045e+33\n",
      "Gradient Descent(18/49): loss=2.943049153355683e+35\n",
      "Gradient Descent(19/49): loss=4.554855720142842e+37\n",
      "Gradient Descent(20/49): loss=7.049393195650961e+39\n",
      "Gradient Descent(21/49): loss=1.0910102867271444e+42\n",
      "Gradient Descent(22/49): loss=1.6885190155420601e+44\n",
      "Gradient Descent(23/49): loss=2.613262680118293e+46\n",
      "Gradient Descent(24/49): loss=4.044456575547257e+48\n",
      "Gradient Descent(25/49): loss=6.259466036829319e+50\n",
      "Gradient Descent(26/49): loss=9.687559832666036e+52\n",
      "Gradient Descent(27/49): loss=1.499310244025741e+55\n",
      "Gradient Descent(28/49): loss=2.3204307861517934e+57\n",
      "Gradient Descent(29/49): loss=3.5912507466524306e+59\n",
      "Gradient Descent(30/49): loss=5.55805499664137e+61\n",
      "Gradient Descent(31/49): loss=8.60201014214498e+63\n",
      "Gradient Descent(32/49): loss=1.3313034601183447e+66\n",
      "Gradient Descent(33/49): loss=2.06041247758996e+68\n",
      "Gradient Descent(34/49): loss=3.1888293728545843e+70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=4.935241307155343e+72\n",
      "Gradient Descent(36/49): loss=7.638102862195174e+74\n",
      "Gradient Descent(37/49): loss=1.1821228528155052e+77\n",
      "Gradient Descent(38/49): loss=1.8295307936545067e+79\n",
      "Gradient Descent(39/49): loss=2.83150174870396e+81\n",
      "Gradient Descent(40/49): loss=4.382217659697695e+83\n",
      "Gradient Descent(41/49): loss=6.782207224755136e+85\n",
      "Gradient Descent(42/49): loss=1.0496588351271651e+88\n",
      "Gradient Descent(43/49): loss=1.6245207992746254e+90\n",
      "Gradient Descent(44/49): loss=2.5142148467279624e+92\n",
      "Gradient Descent(45/49): loss=3.8911636578183e+94\n",
      "Gradient Descent(46/49): loss=6.022219871794458e+96\n",
      "Gradient Descent(47/49): loss=9.320382120491278e+98\n",
      "Gradient Descent(48/49): loss=1.442483415108024e+101\n",
      "Gradient Descent(49/49): loss=2.2324818617544145e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2305230367743096\n",
      "Gradient Descent(2/49): loss=100.61847809878778\n",
      "Gradient Descent(3/49): loss=3829.6393775783235\n",
      "Gradient Descent(4/49): loss=172490.34155400118\n",
      "Gradient Descent(5/49): loss=11455843.250745209\n",
      "Gradient Descent(6/49): loss=1227185369.857115\n",
      "Gradient Descent(7/49): loss=171748178405.11777\n",
      "Gradient Descent(8/49): loss=26197276159495.492\n",
      "Gradient Descent(9/49): loss=4084670811877317.5\n",
      "Gradient Descent(10/49): loss=6.402234645418369e+17\n",
      "Gradient Descent(11/49): loss=1.0047062413898549e+20\n",
      "Gradient Descent(12/49): loss=1.577142961986667e+22\n",
      "Gradient Descent(13/49): loss=2.4758940455347493e+24\n",
      "Gradient Descent(14/49): loss=3.8868682437826146e+26\n",
      "Gradient Descent(15/49): loss=6.1019572714712e+28\n",
      "Gradient Descent(16/49): loss=9.579412467626837e+30\n",
      "Gradient Descent(17/49): loss=1.5038643715210533e+33\n",
      "Gradient Descent(18/49): loss=2.3609048674650774e+35\n",
      "Gradient Descent(19/49): loss=3.7063660518327024e+37\n",
      "Gradient Descent(20/49): loss=5.81859503698774e+39\n",
      "Gradient Descent(21/49): loss=9.13456678341852e+41\n",
      "Gradient Descent(22/49): loss=1.4340284862874992e+44\n",
      "Gradient Descent(23/49): loss=2.251270091190721e+46\n",
      "Gradient Descent(24/49): loss=3.534251287189187e+48\n",
      "Gradient Descent(25/49): loss=5.548393420184076e+50\n",
      "Gradient Descent(26/49): loss=8.710379382683453e+52\n",
      "Gradient Descent(27/49): loss=1.3674356384729262e+55\n",
      "Gradient Descent(28/49): loss=2.1467265008953075e+57\n",
      "Gradient Descent(29/49): loss=3.3701291234391743e+59\n",
      "Gradient Descent(30/49): loss=5.290739320503147e+61\n",
      "Gradient Descent(31/49): loss=8.30589022920064e+63\n",
      "Gradient Descent(32/49): loss=1.3039352030099817e+66\n",
      "Gradient Descent(33/49): loss=2.0470376645132752e+68\n",
      "Gradient Descent(34/49): loss=3.213628399833827e+70\n",
      "Gradient Descent(35/49): loss=5.0450500600212445e+72\n",
      "Gradient Descent(36/49): loss=7.920184583082841e+74\n",
      "Gradient Descent(37/49): loss=1.2433835756594843e+77\n",
      "Gradient Descent(38/49): loss=1.951978138895857e+79\n",
      "Gradient Descent(39/49): loss=3.064395194947332e+81\n",
      "Gradient Descent(40/49): loss=4.810770020266759e+83\n",
      "Gradient Descent(41/49): loss=7.552390183242906e+85\n",
      "Gradient Descent(42/49): loss=1.185643820836447e+88\n",
      "Gradient Descent(43/49): loss=1.8613329499403596e+90\n",
      "Gradient Descent(44/49): loss=2.9220920226190154e+92\n",
      "Gradient Descent(45/49): loss=4.587369384358251e+94\n",
      "Gradient Descent(46/49): loss=7.201675274307803e+96\n",
      "Gradient Descent(47/49): loss=1.1305853619161427e+99\n",
      "Gradient Descent(48/49): loss=1.7748971064262847e+101\n",
      "Gradient Descent(49/49): loss=2.7863970687372244e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2901510301503674\n",
      "Gradient Descent(2/49): loss=104.51143179128955\n",
      "Gradient Descent(3/49): loss=4193.282980387483\n",
      "Gradient Descent(4/49): loss=220148.18420698115\n",
      "Gradient Descent(5/49): loss=18750203.87359629\n",
      "Gradient Descent(6/49): loss=2400398709.461418\n",
      "Gradient Descent(7/49): loss=362953761400.00024\n",
      "Gradient Descent(8/49): loss=57449050690804.88\n",
      "Gradient Descent(9/49): loss=9193543707262284.0\n",
      "Gradient Descent(10/49): loss=1.4749874581724849e+18\n",
      "Gradient Descent(11/49): loss=2.36781512097887e+20\n",
      "Gradient Descent(12/49): loss=3.801592366708675e+22\n",
      "Gradient Descent(13/49): loss=6.103749102041189e+24\n",
      "Gradient Descent(14/49): loss=9.800108086355512e+26\n",
      "Gradient Descent(15/49): loss=1.5734964259378102e+29\n",
      "Gradient Descent(16/49): loss=2.526392463866905e+31\n",
      "Gradient Descent(17/49): loss=4.0563545741364376e+33\n",
      "Gradient Descent(18/49): loss=6.512849047638111e+35\n",
      "Gradient Descent(19/49): loss=1.0456976117820571e+38\n",
      "Gradient Descent(20/49): loss=1.6789633667395396e+40\n",
      "Gradient Descent(21/49): loss=2.6957295835387756e+42\n",
      "Gradient Descent(22/49): loss=4.328240944333652e+44\n",
      "Gradient Descent(23/49): loss=6.949387574641559e+46\n",
      "Gradient Descent(24/49): loss=1.1157878751168987e+49\n",
      "Gradient Descent(25/49): loss=1.7914997097034004e+51\n",
      "Gradient Descent(26/49): loss=2.876416997748409e+53\n",
      "Gradient Descent(27/49): loss=4.618351150224992e+55\n",
      "Gradient Descent(28/49): loss=7.415186102529797e+57\n",
      "Gradient Descent(29/49): loss=1.190576098408464e+60\n",
      "Gradient Descent(30/49): loss=1.9115790574937826e+62\n",
      "Gradient Descent(31/49): loss=3.06921539743129e+64\n",
      "Gradient Descent(32/49): loss=4.927906653350597e+66\n",
      "Gradient Descent(33/49): loss=7.912205837511672e+68\n",
      "Gradient Descent(34/49): loss=1.2703771726801975e+71\n",
      "Gradient Descent(35/49): loss=2.0397069970243162e+73\n",
      "Gradient Descent(36/49): loss=3.274936548908878e+75\n",
      "Gradient Descent(37/49): loss=5.258210819017762e+77\n",
      "Gradient Descent(38/49): loss=8.442539452083979e+79\n",
      "Gradient Descent(39/49): loss=1.3555270956844316e+82\n",
      "Gradient Descent(40/49): loss=2.176423003485151e+84\n",
      "Gradient Descent(41/49): loss=3.4944466290492067e+86\n",
      "Gradient Descent(42/49): loss=5.610654373584165e+88\n",
      "Gradient Descent(43/49): loss=9.008419884891708e+90\n",
      "Gradient Descent(44/49): loss=1.4463843861883012e+93\n",
      "Gradient Descent(45/49): loss=2.322302711619722e+95\n",
      "Gradient Descent(46/49): loss=3.7286698721968574e+97\n",
      "Gradient Descent(47/49): loss=5.986721260008243e+99\n",
      "Gradient Descent(48/49): loss=9.612229742376774e+101\n",
      "Gradient Descent(49/49): loss=1.5433315934957234e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.317280315940448\n",
      "Gradient Descent(2/49): loss=106.62504103387063\n",
      "Gradient Descent(3/49): loss=4267.683229824964\n",
      "Gradient Descent(4/49): loss=215032.5172632144\n",
      "Gradient Descent(5/49): loss=16860454.600600064\n",
      "Gradient Descent(6/49): loss=2012468350.6829505\n",
      "Gradient Descent(7/49): loss=291332085603.78204\n",
      "Gradient Descent(8/49): loss=44663619507528.34\n",
      "Gradient Descent(9/49): loss=6947274927353600.0\n",
      "Gradient Descent(10/49): loss=1.0844165738895415e+18\n",
      "Gradient Descent(11/49): loss=1.6941087372096982e+20\n",
      "Gradient Descent(12/49): loss=2.6471164106223443e+22\n",
      "Gradient Descent(13/49): loss=4.136427616683784e+24\n",
      "Gradient Descent(14/49): loss=6.463722905535853e+26\n",
      "Gradient Descent(15/49): loss=1.0100461124810422e+29\n",
      "Gradient Descent(16/49): loss=1.5783377762295387e+31\n",
      "Gradient Descent(17/49): loss=2.466373055210428e+33\n",
      "Gradient Descent(18/49): loss=3.854052256939889e+35\n",
      "Gradient Descent(19/49): loss=6.022494811347927e+37\n",
      "Gradient Descent(20/49): loss=9.410989112094395e+39\n",
      "Gradient Descent(21/49): loss=1.4705984626929695e+42\n",
      "Gradient Descent(22/49): loss=2.2980154508397787e+44\n",
      "Gradient Descent(23/49): loss=3.590970034590579e+46\n",
      "Gradient Descent(24/49): loss=5.611392118665051e+48\n",
      "Gradient Descent(25/49): loss=8.768583754850622e+50\n",
      "Gradient Descent(26/49): loss=1.3702136553620215e+53\n",
      "Gradient Descent(27/49): loss=2.1411501718303785e+55\n",
      "Gradient Descent(28/49): loss=3.345846131651728e+57\n",
      "Gradient Descent(29/49): loss=5.22835179146691e+59\n",
      "Gradient Descent(30/49): loss=8.1700297562219e+61\n",
      "Gradient Descent(31/49): loss=1.2766812349255209e+64\n",
      "Gradient Descent(32/49): loss=1.9949927041204414e+66\n",
      "Gradient Descent(33/49): loss=3.117454679065675e+68\n",
      "Gradient Descent(34/49): loss=4.8714582544366167e+70\n",
      "Gradient Descent(35/49): loss=7.612333768339063e+72\n",
      "Gradient Descent(36/49): loss=1.1895334492052934e+75\n",
      "Gradient Descent(37/49): loss=1.8588121196989598e+77\n",
      "Gradient Descent(38/49): loss=2.904653499780173e+79\n",
      "Gradient Descent(39/49): loss=4.538926696449455e+81\n",
      "Gradient Descent(40/49): loss=7.09270677459479e+83\n",
      "Gradient Descent(41/49): loss=1.1083344754110302e+86\n",
      "Gradient Descent(42/49): loss=1.731927384598255e+88\n",
      "Gradient Descent(43/49): loss=2.7063783831219964e+90\n",
      "Gradient Descent(44/49): loss=4.229094139722876e+92\n",
      "Gradient Descent(45/49): loss=6.608550139987023e+94\n",
      "Gradient Descent(46/49): loss=1.032678240536506e+97\n",
      "Gradient Descent(47/49): loss=1.6137039530423738e+99\n",
      "Gradient Descent(48/49): loss=2.5216377627088012e+101\n",
      "Gradient Descent(49/49): loss=3.9404111233233964e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3795693123176123\n",
      "Gradient Descent(2/49): loss=108.33736879967988\n",
      "Gradient Descent(3/49): loss=4284.816978869736\n",
      "Gradient Descent(4/49): loss=210038.30423614944\n",
      "Gradient Descent(5/49): loss=15853820.220712952\n",
      "Gradient Descent(6/49): loss=1849169411.3486338\n",
      "Gradient Descent(7/49): loss=265719653823.66638\n",
      "Gradient Descent(8/49): loss=40668633388651.305\n",
      "Gradient Descent(9/49): loss=6324625266554973.0\n",
      "Gradient Descent(10/49): loss=9.873775999065777e+17\n",
      "Gradient Descent(11/49): loss=1.5428733616465825e+20\n",
      "Gradient Descent(12/49): loss=2.4114148825031244e+22\n",
      "Gradient Descent(13/49): loss=3.769086060180038e+24\n",
      "Gradient Descent(14/49): loss=5.891223514394801e+26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=9.208231118770654e+28\n",
      "Gradient Descent(16/49): loss=1.4392864012964036e+31\n",
      "Gradient Descent(17/49): loss=2.24966734411782e+33\n",
      "Gradient Descent(18/49): loss=3.5163281967026746e+35\n",
      "Gradient Descent(19/49): loss=5.496174415894979e+37\n",
      "Gradient Descent(20/49): loss=8.590760471220298e+39\n",
      "Gradient Descent(21/49): loss=1.3427733533852691e+42\n",
      "Gradient Descent(22/49): loss=2.0988133528155456e+44\n",
      "Gradient Descent(23/49): loss=3.28053686720863e+46\n",
      "Gradient Descent(24/49): loss=5.127622293212733e+48\n",
      "Gradient Descent(25/49): loss=8.014697424896566e+50\n",
      "Gradient Descent(26/49): loss=1.2527321853970312e+53\n",
      "Gradient Descent(27/49): loss=1.9580750777375713e+55\n",
      "Gradient Descent(28/49): loss=3.0605568011665413e+57\n",
      "Gradient Descent(29/49): loss=4.783783849590632e+59\n",
      "Gradient Descent(30/49): loss=7.477262931660519e+61\n",
      "Gradient Descent(31/49): loss=1.168728828623155e+64\n",
      "Gradient Descent(32/49): loss=1.8267741650105375e+66\n",
      "Gradient Descent(33/49): loss=2.855327744316287e+68\n",
      "Gradient Descent(34/49): loss=4.4630018770904366e+70\n",
      "Gradient Descent(35/49): loss=6.9758667090186404e+72\n",
      "Gradient Descent(36/49): loss=1.0903584108218662e+75\n",
      "Gradient Descent(37/49): loss=1.7042777817313514e+77\n",
      "Gradient Descent(38/49): loss=2.663860551241811e+79\n",
      "Gradient Descent(39/49): loss=4.16373029826958e+81\n",
      "Gradient Descent(40/49): loss=6.508092170457737e+83\n",
      "Gradient Descent(41/49): loss=1.0172432089747948e+86\n",
      "Gradient Descent(42/49): loss=1.5899955303376614e+88\n",
      "Gradient Descent(43/49): loss=2.4852324047871177e+90\n",
      "Gradient Descent(44/49): loss=3.8845267096393555e+92\n",
      "Gradient Descent(45/49): loss=6.0716847763756786e+94\n",
      "Gradient Descent(46/49): loss=9.490308287028062e+96\n",
      "Gradient Descent(47/49): loss=1.4833766030356563e+99\n",
      "Gradient Descent(48/49): loss=2.3185823683317443e+101\n",
      "Gradient Descent(49/49): loss=3.6240454296889455e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2615312193590276\n",
      "Gradient Descent(2/49): loss=102.78234018179467\n",
      "Gradient Descent(3/49): loss=3953.5250847752477\n",
      "Gradient Descent(4/49): loss=179867.95349083314\n",
      "Gradient Descent(5/49): loss=12055592.995552503\n",
      "Gradient Descent(6/49): loss=1303013229.886116\n",
      "Gradient Descent(7/49): loss=184086636341.57758\n",
      "Gradient Descent(8/49): loss=28353986524483.434\n",
      "Gradient Descent(9/49): loss=4464666378314568.5\n",
      "Gradient Descent(10/49): loss=7.067251410584293e+17\n",
      "Gradient Descent(11/49): loss=1.1200787348494321e+20\n",
      "Gradient Descent(12/49): loss=1.7757095346932027e+22\n",
      "Gradient Descent(13/49): loss=2.815299275195781e+24\n",
      "Gradient Descent(14/49): loss=4.463587385899932e+26\n",
      "Gradient Descent(15/49): loss=7.076933408324342e+28\n",
      "Gradient Descent(16/49): loss=1.1220353730888243e+31\n",
      "Gradient Descent(17/49): loss=1.778967749995627e+33\n",
      "Gradient Descent(18/49): loss=2.8205228454793286e+35\n",
      "Gradient Descent(19/49): loss=4.471890627893458e+37\n",
      "Gradient Descent(20/49): loss=7.090105960579515e+39\n",
      "Gradient Descent(21/49): loss=1.1241241511703344e+42\n",
      "Gradient Descent(22/49): loss=1.7822795800870135e+44\n",
      "Gradient Descent(23/49): loss=2.825773735392764e+46\n",
      "Gradient Descent(24/49): loss=4.480215838699188e+48\n",
      "Gradient Descent(25/49): loss=7.1033054451461995e+50\n",
      "Gradient Descent(26/49): loss=1.1262169070344057e+53\n",
      "Gradient Descent(27/49): loss=1.7855976087259064e+55\n",
      "Gradient Descent(28/49): loss=2.8310344129741816e+57\n",
      "Gradient Descent(29/49): loss=4.488556552874778e+59\n",
      "Gradient Descent(30/49): loss=7.116529504559728e+61\n",
      "Gradient Descent(31/49): loss=1.1283135589955394e+64\n",
      "Gradient Descent(32/49): loss=1.7889218144848112e+66\n",
      "Gradient Descent(33/49): loss=2.836304884245728e+68\n",
      "Gradient Descent(34/49): loss=4.496912794767891e+70\n",
      "Gradient Descent(35/49): loss=7.12977818289975e+72\n",
      "Gradient Descent(36/49): loss=1.1304141142451587e+75\n",
      "Gradient Descent(37/49): loss=1.7922522088407397e+77\n",
      "Gradient Descent(38/49): loss=2.8415851674317113e+79\n",
      "Gradient Descent(39/49): loss=4.505284593283282e+81\n",
      "Gradient Descent(40/49): loss=7.143051525997757e+83\n",
      "Gradient Descent(41/49): loss=1.1325185800498854e+86\n",
      "Gradient Descent(42/49): loss=1.7955888033147757e+88\n",
      "Gradient Descent(43/49): loss=2.8468752807988703e+90\n",
      "Gradient Descent(44/49): loss=4.513671977382443e+92\n",
      "Gradient Descent(45/49): loss=7.156349579771747e+94\n",
      "Gradient Descent(46/49): loss=1.1346269636899348e+97\n",
      "Gradient Descent(47/49): loss=1.7989316094495382e+99\n",
      "Gradient Descent(48/49): loss=2.8521752426475065e+101\n",
      "Gradient Descent(49/49): loss=4.52207497608018e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3216367731444776\n",
      "Gradient Descent(2/49): loss=106.74567215324842\n",
      "Gradient Descent(3/49): loss=4327.674484279383\n",
      "Gradient Descent(4/49): loss=229413.06488187247\n",
      "Gradient Descent(5/49): loss=19714958.18491078\n",
      "Gradient Descent(6/49): loss=2547120060.4940367\n",
      "Gradient Descent(7/49): loss=388844261828.64124\n",
      "Gradient Descent(8/49): loss=62150377748694.49\n",
      "Gradient Descent(9/49): loss=1.0043934992539112e+16\n",
      "Gradient Descent(10/49): loss=1.627330220298177e+18\n",
      "Gradient Descent(11/49): loss=2.638172786919713e+20\n",
      "Gradient Descent(12/49): loss=4.277494968310464e+22\n",
      "Gradient Descent(13/49): loss=6.935683342282087e+24\n",
      "Gradient Descent(14/49): loss=1.1245845146940884e+27\n",
      "Gradient Descent(15/49): loss=1.8234575244558736e+29\n",
      "Gradient Descent(16/49): loss=2.956646252199051e+31\n",
      "Gradient Descent(17/49): loss=4.7940561794617306e+33\n",
      "Gradient Descent(18/49): loss=7.773325973214584e+35\n",
      "Gradient Descent(19/49): loss=1.2604065262862366e+38\n",
      "Gradient Descent(20/49): loss=2.0436871149256546e+40\n",
      "Gradient Descent(21/49): loss=3.313738018333326e+42\n",
      "Gradient Descent(22/49): loss=5.3730630166196746e+44\n",
      "Gradient Descent(23/49): loss=8.712157093049216e+46\n",
      "Gradient Descent(24/49): loss=1.4126333709359452e+49\n",
      "Gradient Descent(25/49): loss=2.290515447977016e+51\n",
      "Gradient Descent(26/49): loss=3.7139580059233583e+53\n",
      "Gradient Descent(27/49): loss=6.022000018356114e+55\n",
      "Gradient Descent(28/49): loss=9.764376485475476e+57\n",
      "Gradient Descent(29/49): loss=1.5832455639237089e+60\n",
      "Gradient Descent(30/49): loss=2.567154717367558e+62\n",
      "Gradient Descent(31/49): loss=4.1625149585575484e+64\n",
      "Gradient Descent(32/49): loss=6.749313028543466e+66\n",
      "Gradient Descent(33/49): loss=1.0943678716064479e+69\n",
      "Gradient Descent(34/49): loss=1.7744636133181116e+71\n",
      "Gradient Descent(35/49): loss=2.8772053682167033e+73\n",
      "Gradient Descent(36/49): loss=4.665246820933871e+75\n",
      "Gradient Descent(37/49): loss=7.564467987115912e+77\n",
      "Gradient Descent(38/49): loss=1.226541234031583e+80\n",
      "Gradient Descent(39/49): loss=1.9887762118129944e+82\n",
      "Gradient Descent(40/49): loss=3.224702693176175e+84\n",
      "Gradient Descent(41/49): loss=5.228696621375158e+86\n",
      "Gradient Descent(42/49): loss=8.478074092297801e+88\n",
      "Gradient Descent(43/49): loss=1.3746779650716824e+91\n",
      "Gradient Descent(44/49): loss=2.2289726264251156e+93\n",
      "Gradient Descent(45/49): loss=3.614169351360359e+95\n",
      "Gradient Descent(46/49): loss=5.8601976289238386e+97\n",
      "Gradient Descent(47/49): loss=9.502021878725172e+99\n",
      "Gradient Descent(48/49): loss=1.5407060563647874e+102\n",
      "Gradient Descent(49/49): loss=2.498179000654545e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3490442922812576\n",
      "Gradient Descent(2/49): loss=108.9031350775874\n",
      "Gradient Descent(3/49): loss=4404.535999841152\n",
      "Gradient Descent(4/49): loss=224111.59582328543\n",
      "Gradient Descent(5/49): loss=17731609.01263388\n",
      "Gradient Descent(6/49): loss=2135791853.9695554\n",
      "Gradient Descent(7/49): loss=312145018811.3541\n",
      "Gradient Descent(8/49): loss=48322925343820.2\n",
      "Gradient Descent(9/49): loss=7590580596076803.0\n",
      "Gradient Descent(10/49): loss=1.196537733027799e+18\n",
      "Gradient Descent(11/49): loss=1.887746190125396e+20\n",
      "Gradient Descent(12/49): loss=2.9788459879815623e+22\n",
      "Gradient Descent(13/49): loss=4.7008164272273277e+24\n",
      "Gradient Descent(14/49): loss=7.41828457016489e+26\n",
      "Gradient Descent(15/49): loss=1.1706710196729165e+29\n",
      "Gradient Descent(16/49): loss=1.8474237694307459e+31\n",
      "Gradient Descent(17/49): loss=2.915400700545621e+33\n",
      "Gradient Descent(18/49): loss=4.600764425648832e+35\n",
      "Gradient Descent(19/49): loss=7.260419976560731e+37\n",
      "Gradient Descent(20/49): loss=1.1457595623091977e+40\n",
      "Gradient Descent(21/49): loss=1.8081116237280016e+42\n",
      "Gradient Descent(22/49): loss=2.853362740136493e+44\n",
      "Gradient Descent(23/49): loss=4.502862998160894e+46\n",
      "Gradient Descent(24/49): loss=7.105922739908416e+48\n",
      "Gradient Descent(25/49): loss=1.121378509767068e+51\n",
      "Gradient Descent(26/49): loss=1.7696361305837449e+53\n",
      "Gradient Descent(27/49): loss=2.7926449520762e+55\n",
      "Gradient Descent(28/49): loss=4.407044868474872e+57\n",
      "Gradient Descent(29/49): loss=6.954713114644914e+59\n",
      "Gradient Descent(30/49): loss=1.0975162711186885e+62\n",
      "Gradient Descent(31/49): loss=1.7319793721380148e+64\n",
      "Gradient Descent(32/49): loss=2.7332192008906957e+66\n",
      "Gradient Descent(33/49): loss=4.313265689126482e+68\n",
      "Gradient Descent(34/49): loss=6.806721136355556e+70\n",
      "Gradient Descent(35/49): loss=1.074161806097636e+73\n",
      "Gradient Descent(36/49): loss=1.6951239261385467e+75\n",
      "Gradient Descent(37/49): loss=2.675057992804979e+77\n",
      "Gradient Descent(38/49): loss=4.221482072506027e+79\n",
      "Gradient Descent(39/49): loss=6.6618783355059945e+81\n",
      "Gradient Descent(40/49): loss=1.0513043095961438e+84\n",
      "Gradient Descent(41/49): loss=1.6590527411537627e+86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=2.618134419126609e+88\n",
      "Gradient Descent(43/49): loss=4.1316515542770635e+90\n",
      "Gradient Descent(44/49): loss=6.52011769955444e+92\n",
      "Gradient Descent(45/49): loss=1.0289332064325314e+95\n",
      "Gradient Descent(46/49): loss=1.6237491285960607e+97\n",
      "Gradient Descent(47/49): loss=2.562422143763692e+99\n",
      "Gradient Descent(48/49): loss=4.043732573718209e+101\n",
      "Gradient Descent(49/49): loss=6.381373641945095e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.412020113631997\n",
      "Gradient Descent(2/49): loss=110.653225078646\n",
      "Gradient Descent(3/49): loss=4422.35634805886\n",
      "Gradient Descent(4/49): loss=218924.01040194652\n",
      "Gradient Descent(5/49): loss=16674474.589789456\n",
      "Gradient Descent(6/49): loss=1962570483.6756878\n",
      "Gradient Descent(7/49): loss=284706527644.4815\n",
      "Gradient Descent(8/49): loss=44000759643944.56\n",
      "Gradient Descent(9/49): loss=6910275509365761.0\n",
      "Gradient Descent(10/49): loss=1.0894648469513819e+18\n",
      "Gradient Descent(11/49): loss=1.719222867323776e+20\n",
      "Gradient Descent(12/49): loss=2.713603954488315e+22\n",
      "Gradient Descent(13/49): loss=4.283348274208578e+24\n",
      "Gradient Descent(14/49): loss=6.761229664184173e+26\n",
      "Gradient Descent(15/49): loss=1.0672576221780811e+29\n",
      "Gradient Descent(16/49): loss=1.684663417221645e+31\n",
      "Gradient Descent(17/49): loss=2.6592373206610466e+33\n",
      "Gradient Descent(18/49): loss=4.197600145863103e+35\n",
      "Gradient Descent(19/49): loss=6.625902476729438e+37\n",
      "Gradient Descent(20/49): loss=1.0458972318217797e+40\n",
      "Gradient Descent(21/49): loss=1.650946454389515e+42\n",
      "Gradient Descent(22/49): loss=2.606015306928996e+44\n",
      "Gradient Descent(23/49): loss=4.113589366931447e+46\n",
      "Gradient Descent(24/49): loss=6.493291668264264e+48\n",
      "Gradient Descent(25/49): loss=1.0249646459163446e+51\n",
      "Gradient Descent(26/49): loss=1.6179044143564111e+53\n",
      "Gradient Descent(27/49): loss=2.5538585203140243e+55\n",
      "Gradient Descent(28/49): loss=4.0312599952793305e+57\n",
      "Gradient Descent(29/49): loss=6.363334938202184e+59\n",
      "Gradient Descent(30/49): loss=1.0044510049751361e+62\n",
      "Gradient Descent(31/49): loss=1.5855236777472442e+64\n",
      "Gradient Descent(32/49): loss=2.502745599581843e+66\n",
      "Gradient Descent(33/49): loss=3.9505783635638745e+68\n",
      "Gradient Descent(34/49): loss=6.235979161951619e+70\n",
      "Gradient Descent(35/49): loss=9.84347924014218e+72\n",
      "Gradient Descent(36/49): loss=1.5537910091538095e+75\n",
      "Gradient Descent(37/49): loss=2.452655652771406e+77\n",
      "Gradient Descent(38/49): loss=3.87151149390902e+79\n",
      "Gradient Descent(39/49): loss=6.111172284023229e+81\n",
      "Gradient Descent(40/49): loss=9.646471860866293e+83\n",
      "Gradient Descent(41/49): loss=1.5226934381437805e+86\n",
      "Gradient Descent(42/49): loss=2.4035682060839607e+88\n",
      "Gradient Descent(43/49): loss=3.7940270684689875e+90\n",
      "Gradient Descent(44/49): loss=5.988863290768764e+92\n",
      "Gradient Descent(45/49): loss=9.4534073869935e+94\n",
      "Gradient Descent(46/49): loss=1.4922182538749092e+97\n",
      "Gradient Descent(47/49): loss=2.355463195483476e+99\n",
      "Gradient Descent(48/49): loss=3.718093416207602e+101\n",
      "Gradient Descent(49/49): loss=5.8690021895285e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.2927081950720907\n",
      "Gradient Descent(2/49): loss=104.9810617187762\n",
      "Gradient Descent(3/49): loss=4080.73444209253\n",
      "Gradient Descent(4/49): loss=187520.5081987181\n",
      "Gradient Descent(5/49): loss=12683496.846541356\n",
      "Gradient Descent(6/49): loss=1383117503.3048685\n",
      "Gradient Descent(7/49): loss=197244197094.11127\n",
      "Gradient Descent(8/49): loss=30676316460833.58\n",
      "Gradient Descent(9/49): loss=4877879060241208.0\n",
      "Gradient Descent(10/49): loss=7.79755554896164e+17\n",
      "Gradient Descent(11/49): loss=1.2480324803365655e+20\n",
      "Gradient Descent(12/49): loss=1.9981108333671055e+22\n",
      "Gradient Descent(13/49): loss=3.199210111875095e+24\n",
      "Gradient Descent(14/49): loss=5.122392403022118e+26\n",
      "Gradient Descent(15/49): loss=8.201712381203079e+28\n",
      "Gradient Descent(16/49): loss=1.3132173197168118e+31\n",
      "Gradient Descent(17/49): loss=2.10265856369161e+33\n",
      "Gradient Descent(18/49): loss=3.366672962580646e+35\n",
      "Gradient Descent(19/49): loss=5.390550400210141e+37\n",
      "Gradient Descent(20/49): loss=8.631082975700591e+39\n",
      "Gradient Descent(21/49): loss=1.3819663642391673e+42\n",
      "Gradient Descent(22/49): loss=2.2127362667392566e+44\n",
      "Gradient Descent(23/49): loss=3.5429239907728493e+46\n",
      "Gradient Descent(24/49): loss=5.672754857038397e+48\n",
      "Gradient Descent(25/49): loss=9.082934816529816e+50\n",
      "Gradient Descent(26/49): loss=1.454314648887131e+53\n",
      "Gradient Descent(27/49): loss=2.328576765869563e+55\n",
      "Gradient Descent(28/49): loss=3.728402074954611e+57\n",
      "Gradient Descent(29/49): loss=5.969733201960837e+59\n",
      "Gradient Descent(30/49): loss=9.558441870309147e+61\n",
      "Gradient Descent(31/49): loss=1.530450489111791e+64\n",
      "Gradient Descent(32/49): loss=2.4504817117716495e+66\n",
      "Gradient Descent(33/49): loss=3.9235902516600814e+68\n",
      "Gradient Descent(34/49): loss=6.282258867295087e+70\n",
      "Gradient Descent(35/49): loss=1.0058842525416215e+73\n",
      "Gradient Descent(36/49): loss=1.6105721697948276e+75\n",
      "Gradient Descent(37/49): loss=2.5787685884965016e+77\n",
      "Gradient Descent(38/49): loss=4.1289968607016024e+79\n",
      "Gradient Descent(39/49): loss=6.611145781647514e+81\n",
      "Gradient Descent(40/49): loss=1.0585440004129347e+84\n",
      "Gradient Descent(41/49): loss=1.6948883564491771e+86\n",
      "Gradient Descent(42/49): loss=2.7137715009544593e+88\n",
      "Gradient Descent(43/49): loss=4.345156854355586e+90\n",
      "Gradient Descent(44/49): loss=6.957250484174027e+92\n",
      "Gradient Descent(45/49): loss=1.113960575462797e+95\n",
      "Gradient Descent(46/49): loss=1.783618638581658e+97\n",
      "Gradient Descent(47/49): loss=2.855842045015125e+99\n",
      "Gradient Descent(48/49): loss=4.572633190557916e+101\n",
      "Gradient Descent(49/49): loss=7.32147435530916e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3532929282052346\n",
      "Gradient Descent(2/49): loss=109.01569034407504\n",
      "Gradient Descent(3/49): loss=4465.639152975965\n",
      "Gradient Descent(4/49): loss=239017.82497185678\n",
      "Gradient Descent(5/49): loss=20724214.92083066\n",
      "Gradient Descent(6/49): loss=2702024544.51426\n",
      "Gradient Descent(7/49): loss=416441254016.64185\n",
      "Gradient Descent(8/49): loss=67210578987163.7\n",
      "Gradient Descent(9/49): loss=1.0968240143283302e+16\n",
      "Gradient Descent(10/49): loss=1.794544751529579e+18\n",
      "Gradient Descent(11/49): loss=2.9378461587990413e+20\n",
      "Gradient Descent(12/49): loss=4.810198069205054e+22\n",
      "Gradient Descent(13/49): loss=7.876086118552194e+24\n",
      "Gradient Descent(14/49): loss=1.2896179611599825e+27\n",
      "Gradient Descent(15/49): loss=2.1116036635122536e+29\n",
      "Gradient Descent(16/49): loss=3.457513663666089e+31\n",
      "Gradient Descent(17/49): loss=5.66129050707448e+33\n",
      "Gradient Descent(18/49): loss=9.269727892321432e+35\n",
      "Gradient Descent(19/49): loss=1.5178139239594342e+38\n",
      "Gradient Descent(20/49): loss=2.4852499846327196e+40\n",
      "Gradient Descent(21/49): loss=4.0693179777271083e+42\n",
      "Gradient Descent(22/49): loss=6.663051567112647e+44\n",
      "Gradient Descent(23/49): loss=1.0909999274956292e+47\n",
      "Gradient Descent(24/49): loss=1.7863899593302321e+49\n",
      "Gradient Descent(25/49): loss=2.9250131062092124e+51\n",
      "Gradient Descent(26/49): loss=4.78938074344359e+53\n",
      "Gradient Descent(27/49): loss=7.842073547286243e+55\n",
      "Gradient Descent(28/49): loss=1.284051546856774e+58\n",
      "Gradient Descent(29/49): loss=2.1024903235647854e+60\n",
      "Gradient Descent(30/49): loss=3.442591982778543e+62\n",
      "Gradient Descent(31/49): loss=5.636858075901687e+64\n",
      "Gradient Descent(32/49): loss=9.229722583102281e+66\n",
      "Gradient Descent(33/49): loss=1.5112635055549076e+69\n",
      "Gradient Descent(34/49): loss=2.4745244103040775e+71\n",
      "Gradient Descent(35/49): loss=4.0517560535827e+73\n",
      "Gradient Descent(36/49): loss=6.634295887073867e+75\n",
      "Gradient Descent(37/49): loss=1.0862915075631611e+78\n",
      "Gradient Descent(38/49): loss=1.7786804500278238e+80\n",
      "Gradient Descent(39/49): loss=2.9123896498170848e+82\n",
      "Gradient Descent(40/49): loss=4.768711250089357e+84\n",
      "Gradient Descent(41/49): loss=7.808229571258559e+86\n",
      "Gradient Descent(42/49): loss=1.2785099755480845e+89\n",
      "Gradient Descent(43/49): loss=2.093416622370724e+91\n",
      "Gradient Descent(44/49): loss=3.427734815240188e+93\n",
      "Gradient Descent(45/49): loss=5.612531131191629e+95\n",
      "Gradient Descent(46/49): loss=9.189889940884807e+97\n",
      "Gradient Descent(47/49): loss=1.5047413573569305e+100\n",
      "Gradient Descent(48/49): loss=2.4638451244850927e+102\n",
      "Gradient Descent(49/49): loss=4.0342699213849946e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.380980083918981\n",
      "Gradient Descent(2/49): loss=111.21768405151566\n",
      "Gradient Descent(3/49): loss=4545.029021975063\n",
      "Gradient Descent(4/49): loss=233524.77018536525\n",
      "Gradient Descent(5/49): loss=18643117.904883657\n",
      "Gradient Descent(6/49): loss=2266011569.7136555\n",
      "Gradient Descent(7/49): loss=334331880404.5991\n",
      "Gradient Descent(8/49): loss=52261890536481.984\n",
      "Gradient Descent(9/49): loss=8289860779170416.0\n",
      "Gradient Descent(10/49): loss=1.3196157068252147e+18\n",
      "Gradient Descent(11/49): loss=2.1024023489751004e+20\n",
      "Gradient Descent(12/49): loss=3.350210174809297e+22\n",
      "Gradient Descent(13/49): loss=5.338868588957282e+24\n",
      "Gradient Descent(14/49): loss=8.508076917245495e+26\n",
      "Gradient Descent(15/49): loss=1.3558597725974809e+29\n",
      "Gradient Descent(16/49): loss=2.160719683337299e+31\n",
      "Gradient Descent(17/49): loss=3.4433577668497013e+33\n",
      "Gradient Descent(18/49): loss=5.487390726032915e+35\n",
      "Gradient Descent(19/49): loss=8.744794844544506e+37\n",
      "Gradient Descent(20/49): loss=1.3935846899380298e+40\n",
      "Gradient Descent(21/49): loss=2.2208391660803788e+42\n",
      "Gradient Descent(22/49): loss=3.5391653179037638e+44\n",
      "Gradient Descent(23/49): loss=5.640071257352466e+46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=8.988109040092423e+48\n",
      "Gradient Descent(25/49): loss=1.4323596357299118e+51\n",
      "Gradient Descent(26/49): loss=2.282631548991531e+53\n",
      "Gradient Descent(27/49): loss=3.637638661743592e+55\n",
      "Gradient Descent(28/49): loss=5.7969999754266056e+57\n",
      "Gradient Descent(29/49): loss=9.238193190686231e+59\n",
      "Gradient Descent(30/49): loss=1.4722134516166062e+62\n",
      "Gradient Descent(31/49): loss=2.346143236435033e+64\n",
      "Gradient Descent(32/49): loss=3.738851917040709e+66\n",
      "Gradient Descent(33/49): loss=5.958295060790992e+68\n",
      "Gradient Descent(34/49): loss=9.495235654998916e+70\n",
      "Gradient Descent(35/49): loss=1.51317615566344e+73\n",
      "Gradient Descent(36/49): loss=2.4114220660368028e+75\n",
      "Gradient Descent(37/49): loss=3.842881318744863e+77\n",
      "Gradient Descent(38/49): loss=6.124078002748528e+79\n",
      "Gradient Descent(39/49): loss=9.759430040373399e+81\n",
      "Gradient Descent(40/49): loss=1.5552786014514342e+84\n",
      "Gradient Descent(41/49): loss=2.4785172065644873e+86\n",
      "Gradient Descent(42/49): loss=3.9498052230019e+88\n",
      "Gradient Descent(43/49): loss=6.294473670924615e+90\n",
      "Gradient Descent(44/49): loss=1.0030975341070083e+93\n",
      "Gradient Descent(45/49): loss=1.5985525010287126e+95\n",
      "Gradient Descent(46/49): loss=2.5474791948521916e+97\n",
      "Gradient Descent(47/49): loss=4.059704166130502e+99\n",
      "Gradient Descent(48/49): loss=6.469610409302677e+101\n",
      "Gradient Descent(49/49): loss=1.0310076088143108e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4446463884281364\n",
      "Gradient Descent(2/49): loss=113.00615630139619\n",
      "Gradient Descent(3/49): loss=4563.557708377617\n",
      "Gradient Descent(4/49): loss=228137.32479567442\n",
      "Gradient Descent(5/49): loss=17533215.909195516\n",
      "Gradient Descent(6/49): loss=2082317922.0126894\n",
      "Gradient Descent(7/49): loss=304947049415.3024\n",
      "Gradient Descent(8/49): loss=47587551822032.266\n",
      "Gradient Descent(9/49): loss=7546883747016448.0\n",
      "Gradient Descent(10/49): loss=1.2015282936235866e+18\n",
      "Gradient Descent(11/49): loss=1.9147143499658356e+20\n",
      "Gradient Descent(12/49): loss=3.0518978172901917e+22\n",
      "Gradient Descent(13/49): loss=4.864730699042817e+24\n",
      "Gradient Descent(14/49): loss=7.754486447332022e+26\n",
      "Gradient Descent(15/49): loss=1.2360856580683835e+29\n",
      "Gradient Descent(16/49): loss=1.9703546349527556e+31\n",
      "Gradient Descent(17/49): loss=3.140800164261531e+33\n",
      "Gradient Descent(18/49): loss=5.006523135252618e+35\n",
      "Gradient Descent(19/49): loss=7.980537706967162e+37\n",
      "Gradient Descent(20/49): loss=1.2721200025430601e+40\n",
      "Gradient Descent(21/49): loss=2.0277948192912345e+42\n",
      "Gradient Descent(22/49): loss=3.2323615865218148e+44\n",
      "Gradient Descent(23/49): loss=5.152474662100691e+46\n",
      "Gradient Descent(24/49): loss=8.213188541306479e+48\n",
      "Gradient Descent(25/49): loss=1.3092051963174187e+51\n",
      "Gradient Descent(26/49): loss=2.0869096544477728e+53\n",
      "Gradient Descent(27/49): loss=3.326592285211136e+55\n",
      "Gradient Descent(28/49): loss=5.302681028113082e+57\n",
      "Gradient Descent(29/49): loss=8.452621684633611e+59\n",
      "Gradient Descent(30/49): loss=1.3473715082003411e+62\n",
      "Gradient Descent(31/49): loss=2.147747821732414e+64\n",
      "Gradient Descent(32/49): loss=3.423570023324637e+66\n",
      "Gradient Descent(33/49): loss=5.457266251655679e+68\n",
      "Gradient Descent(34/49): loss=8.699034849165616e+70\n",
      "Gradient Descent(35/49): loss=1.3866504549606413e+73\n",
      "Gradient Descent(36/49): loss=2.210359560091894e+75\n",
      "Gradient Descent(37/49): loss=3.523374883274627e+77\n",
      "Gradient Descent(38/49): loss=5.6163579863786016e+79\n",
      "Gradient Descent(39/49): loss=8.952631518404149e+81\n",
      "Gradient Descent(40/49): loss=1.4270744724376949e+84\n",
      "Gradient Descent(41/49): loss=2.2747965731603917e+86\n",
      "Gradient Descent(42/49): loss=3.626089282098129e+88\n",
      "Gradient Descent(43/49): loss=5.78008760734138e+90\n",
      "Gradient Descent(44/49): loss=9.213621107864824e+92\n",
      "Gradient Descent(45/49): loss=1.4686769420496605e+95\n",
      "Gradient Descent(46/49): loss=2.3411120718509233e+97\n",
      "Gradient Descent(47/49): loss=3.731798039477158e+99\n",
      "Gradient Descent(48/49): loss=5.948590319486438e+101\n",
      "Gradient Descent(49/49): loss=9.482219138002044e+103\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.324053963913496\n",
      "Gradient Descent(2/49): loss=107.215015304201\n",
      "Gradient Descent(3/49): loss=4211.338436062849\n",
      "Gradient Descent(4/49): loss=195456.75100052558\n",
      "Gradient Descent(5/49): loss=13340725.258701574\n",
      "Gradient Descent(6/49): loss=1467716504.9105053\n",
      "Gradient Descent(7/49): loss=211270813986.02423\n",
      "Gradient Descent(8/49): loss=33176077165361.95\n",
      "Gradient Descent(9/49): loss=5327027884381304.0\n",
      "Gradient Descent(10/49): loss=8.599188328128865e+17\n",
      "Gradient Descent(11/49): loss=1.3898674358709699e+20\n",
      "Gradient Descent(12/49): loss=2.2470693890378498e+22\n",
      "Gradient Descent(13/49): loss=3.633200167924828e+24\n",
      "Gradient Descent(14/49): loss=5.874475804991345e+26\n",
      "Gradient Descent(15/49): loss=9.498401825968677e+28\n",
      "Gradient Descent(16/49): loss=1.53579177363549e+31\n",
      "Gradient Descent(17/49): loss=2.4832144369183034e+33\n",
      "Gradient Descent(18/49): loss=4.01509784090917e+35\n",
      "Gradient Descent(19/49): loss=6.491993044373278e+37\n",
      "Gradient Descent(20/49): loss=1.0496873418332411e+40\n",
      "Gradient Descent(21/49): loss=1.6972345915134274e+42\n",
      "Gradient Descent(22/49): loss=2.7442507347091525e+44\n",
      "Gradient Descent(23/49): loss=4.437166277931799e+46\n",
      "Gradient Descent(24/49): loss=7.174433563650053e+48\n",
      "Gradient Descent(25/49): loss=1.1600308335372042e+51\n",
      "Gradient Descent(26/49): loss=1.8756484715052041e+53\n",
      "Gradient Descent(27/49): loss=3.0327273094394366e+55\n",
      "Gradient Descent(28/49): loss=4.903602712953463e+57\n",
      "Gradient Descent(29/49): loss=7.928612470908257e+59\n",
      "Gradient Descent(30/49): loss=1.2819736710680292e+62\n",
      "Gradient Descent(31/49): loss=2.072817279620872e+64\n",
      "Gradient Descent(32/49): loss=3.351528640299832e+66\n",
      "Gradient Descent(33/49): loss=5.419071105391758e+68\n",
      "Gradient Descent(34/49): loss=8.762070922558133e+70\n",
      "Gradient Descent(35/49): loss=1.4167351813404473e+73\n",
      "Gradient Descent(36/49): loss=2.2907125402058658e+75\n",
      "Gradient Descent(37/49): loss=3.703842475974726e+77\n",
      "Gradient Descent(38/49): loss=5.988725711346298e+79\n",
      "Gradient Descent(39/49): loss=9.683142811385832e+81\n",
      "Gradient Descent(40/49): loss=1.565662867612206e+84\n",
      "Gradient Descent(41/49): loss=2.531513025024603e+86\n",
      "Gradient Descent(42/49): loss=4.093191662419083e+88\n",
      "Gradient Descent(43/49): loss=6.6182626040939e+90\n",
      "Gradient Descent(44/49): loss=1.070103809183956e+93\n",
      "Gradient Descent(45/49): loss=1.730245883144165e+95\n",
      "Gradient Descent(46/49): loss=2.7976265390740765e+97\n",
      "Gradient Descent(47/49): loss=4.523469368358857e+99\n",
      "Gradient Descent(48/49): loss=7.313976630080546e+101\n",
      "Gradient Descent(49/49): loss=1.1825934871926128e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3851194953326424\n",
      "Gradient Descent(2/49): loss=111.32186651778292\n",
      "Gradient Descent(3/49): loss=4607.252640090701\n",
      "Gradient Descent(4/49): loss=248973.11094730973\n",
      "Gradient Descent(5/49): loss=21779791.82639006\n",
      "Gradient Descent(6/49): loss=2865525189.405232\n",
      "Gradient Descent(7/49): loss=445848056765.1754\n",
      "Gradient Descent(8/49): loss=72655094893829.66\n",
      "Gradient Descent(9/49): loss=1.1972475192471316e+16\n",
      "Gradient Descent(10/49): loss=1.9779994650767104e+18\n",
      "Gradient Descent(11/49): loss=3.2698472585737e+20\n",
      "Gradient Descent(12/49): loss=5.406153119544147e+22\n",
      "Gradient Descent(13/49): loss=8.938464371457754e+24\n",
      "Gradient Descent(14/49): loss=1.4778849702339808e+27\n",
      "Gradient Descent(15/49): loss=2.4435378752807326e+29\n",
      "Gradient Descent(16/49): loss=4.0401518077506247e+31\n",
      "Gradient Descent(17/49): loss=6.6799979780398e+33\n",
      "Gradient Descent(18/49): loss=1.1044727033170777e+36\n",
      "Gradient Descent(19/49): loss=1.826138214493051e+38\n",
      "Gradient Descent(20/49): loss=3.019341964703758e+40\n",
      "Gradient Descent(21/49): loss=4.99218834024052e+42\n",
      "Gradient Descent(22/49): loss=8.25409798465344e+44\n",
      "Gradient Descent(23/49): loss=1.3647348396687196e+47\n",
      "Gradient Descent(24/49): loss=2.2564563518316174e+49\n",
      "Gradient Descent(25/49): loss=3.730831162015793e+51\n",
      "Gradient Descent(26/49): loss=6.168566543807453e+53\n",
      "Gradient Descent(27/49): loss=1.019912495445719e+56\n",
      "Gradient Descent(28/49): loss=1.6863261358680226e+58\n",
      "Gradient Descent(29/49): loss=2.7881762888577342e+60\n",
      "Gradient Descent(30/49): loss=4.60997837393238e+62\n",
      "Gradient Descent(31/49): loss=7.622150971246883e+64\n",
      "Gradient Descent(32/49): loss=1.2602485460018102e+67\n",
      "Gradient Descent(33/49): loss=2.0836984254063207e+69\n",
      "Gradient Descent(34/49): loss=3.445192729493919e+71\n",
      "Gradient Descent(35/49): loss=5.696291170851016e+73\n",
      "Gradient Descent(36/49): loss=9.418263548896377e+75\n",
      "Gradient Descent(37/49): loss=1.5572182954828936e+78\n",
      "Gradient Descent(38/49): loss=2.5747090291084206e+80\n",
      "Gradient Descent(39/49): loss=4.257031017296638e+82\n",
      "Gradient Descent(40/49): loss=7.0385868373255935e+84\n",
      "Gradient Descent(41/49): loss=1.1637618909817667e+87\n",
      "Gradient Descent(42/49): loss=1.9241671236041075e+89\n",
      "Gradient Descent(43/49): loss=3.181423234640885e+91\n",
      "Gradient Descent(44/49): loss=5.260173959814222e+93\n",
      "Gradient Descent(45/49): loss=8.697186148082584e+95\n",
      "Gradient Descent(46/49): loss=1.4379951589485522e+98\n",
      "Gradient Descent(47/49): loss=2.377585165996876e+100\n",
      "Gradient Descent(48/49): loss=3.9311058777844764e+102\n",
      "Gradient Descent(49/49): loss=6.499701311802398e+104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4130876908536223\n",
      "Gradient Descent(2/49): loss=113.56907502903393\n",
      "Gradient Descent(3/49): loss=4689.239397423372\n",
      "Gradient Descent(4/49): loss=243282.5364073468\n",
      "Gradient Descent(5/49): loss=19596636.59416837\n",
      "Gradient Descent(6/49): loss=2403476510.6096287\n",
      "Gradient Descent(7/49): loss=357975984438.9083\n",
      "Gradient Descent(8/49): loss=56500363391428.67\n",
      "Gradient Descent(9/49): loss=9049676259181400.0\n",
      "Gradient Descent(10/49): loss=1.4546597316734605e+18\n",
      "Gradient Descent(11/49): loss=2.340239018613763e+20\n",
      "Gradient Descent(12/49): loss=3.7657154133032465e+22\n",
      "Gradient Descent(13/49): loss=6.059766596094375e+24\n",
      "Gradient Descent(14/49): loss=9.751453148815083e+26\n",
      "Gradient Descent(15/49): loss=1.5692205371856056e+29\n",
      "Gradient Descent(16/49): loss=2.5252182261803506e+31\n",
      "Gradient Descent(17/49): loss=4.063627734191778e+33\n",
      "Gradient Descent(18/49): loss=6.539264928753659e+35\n",
      "Gradient Descent(19/49): loss=1.0523106197184993e+38\n",
      "Gradient Descent(20/49): loss=1.6933977363086252e+40\n",
      "Gradient Descent(21/49): loss=2.7250469975966684e+42\n",
      "Gradient Descent(22/49): loss=4.385196094690248e+44\n",
      "Gradient Descent(23/49): loss=7.056738766856905e+46\n",
      "Gradient Descent(24/49): loss=1.1355834710403123e+49\n",
      "Gradient Descent(25/49): loss=1.827401951959257e+51\n",
      "Gradient Descent(26/49): loss=2.9406890635407343e+53\n",
      "Gradient Descent(27/49): loss=4.732211300943961e+55\n",
      "Gradient Descent(28/49): loss=7.615162063349598e+57\n",
      "Gradient Descent(29/49): loss=1.2254459820826821e+60\n",
      "Gradient Descent(30/49): loss=1.9720103689323265e+62\n",
      "Gradient Descent(31/49): loss=3.1733956062001164e+64\n",
      "Gradient Descent(32/49): loss=5.106686978984945e+66\n",
      "Gradient Descent(33/49): loss=8.217775259530493e+68\n",
      "Gradient Descent(34/49): loss=1.3224196136175618e+71\n",
      "Gradient Descent(35/49): loss=2.1280621326949552e+73\n",
      "Gradient Descent(36/49): loss=3.4245169944371584e+75\n",
      "Gradient Descent(37/49): loss=5.510796167561783e+77\n",
      "Gradient Descent(38/49): loss=8.868075249661438e+79\n",
      "Gradient Descent(39/49): loss=1.427067092348138e+82\n",
      "Gradient Descent(40/49): loss=2.2964627934802312e+84\n",
      "Gradient Descent(41/49): loss=3.695510456457503e+86\n",
      "Gradient Descent(42/49): loss=5.946883865290168e+88\n",
      "Gradient Descent(43/49): loss=9.569835648942788e+90\n",
      "Gradient Descent(44/49): loss=1.5399956754210193e+93\n",
      "Gradient Descent(45/49): loss=2.4781895607344562e+95\n",
      "Gradient Descent(46/49): loss=3.987948535799757e+97\n",
      "Gradient Descent(47/49): loss=6.417480638355181e+99\n",
      "Gradient Descent(48/49): loss=1.0327128691344977e+102\n",
      "Gradient Descent(49/49): loss=1.6618606742681722e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.477448136706034\n",
      "Gradient Descent(2/49): loss=115.39655628361615\n",
      "Gradient Descent(3/49): loss=4708.498691623656\n",
      "Gradient Descent(4/49): loss=237688.55888147457\n",
      "Gradient Descent(5/49): loss=18431609.49434885\n",
      "Gradient Descent(6/49): loss=2208733180.492798\n",
      "Gradient Descent(7/49): loss=326517256667.7377\n",
      "Gradient Descent(8/49): loss=51447085220105.1\n",
      "Gradient Descent(9/49): loss=8238602032112709.0\n",
      "Gradient Descent(10/49): loss=1.324486845185463e+18\n",
      "Gradient Descent(11/49): loss=2.131316571839771e+20\n",
      "Gradient Descent(12/49): loss=3.4304015333950906e+22\n",
      "Gradient Descent(13/49): loss=5.521600110752328e+24\n",
      "Gradient Descent(14/49): loss=8.887720991622346e+26\n",
      "Gradient Descent(15/49): loss=1.430596555251277e+29\n",
      "Gradient Descent(16/49): loss=2.3027365095171015e+31\n",
      "Gradient Descent(17/49): loss=3.7065630493798765e+33\n",
      "Gradient Descent(18/49): loss=5.966210261371169e+35\n",
      "Gradient Descent(19/49): loss=9.603415549546164e+37\n",
      "Gradient Descent(20/49): loss=1.5457985284967985e+40\n",
      "Gradient Descent(21/49): loss=2.4881700470723512e+42\n",
      "Gradient Descent(22/49): loss=4.005043393310568e+44\n",
      "Gradient Descent(23/49): loss=6.446654480735976e+46\n",
      "Gradient Descent(24/49): loss=1.0376754984405568e+49\n",
      "Gradient Descent(25/49): loss=1.670277883332123e+51\n",
      "Gradient Descent(26/49): loss=2.688536263737822e+53\n",
      "Gradient Descent(27/49): loss=4.327559691453181e+55\n",
      "Gradient Descent(28/49): loss=6.96578771716248e+57\n",
      "Gradient Descent(29/49): loss=1.1212369552383764e+60\n",
      "Gradient Descent(30/49): loss=1.804781254953817e+62\n",
      "Gradient Descent(31/49): loss=2.905037479379348e+64\n",
      "Gradient Descent(32/49): loss=4.676047434244036e+66\n",
      "Gradient Descent(33/49): loss=7.526725476867958e+68\n",
      "Gradient Descent(34/49): loss=1.2115274107199127e+71\n",
      "Gradient Descent(35/49): loss=1.950115852420433e+73\n",
      "Gradient Descent(36/49): loss=3.138973005655454e+75\n",
      "Gradient Descent(37/49): loss=5.052598038216287e+77\n",
      "Gradient Descent(38/49): loss=8.1328341753152e+79\n",
      "Gradient Descent(39/49): loss=1.3090887346052558e+82\n",
      "Gradient Descent(40/49): loss=2.1071538877207316e+84\n",
      "Gradient Descent(41/49): loss=3.391746784739911e+86\n",
      "Gradient Descent(42/49): loss=5.4594713365890595e+88\n",
      "Gradient Descent(43/49): loss=8.787751317150207e+90\n",
      "Gradient Descent(44/49): loss=1.4145064320517308e+93\n",
      "Gradient Descent(45/49): loss=2.276837809930871e+95\n",
      "Gradient Descent(46/49): loss=3.6648758148180145e+97\n",
      "Gradient Descent(47/49): loss=5.899109141395473e+99\n",
      "Gradient Descent(48/49): loss=9.495407326325113e+101\n",
      "Gradient Descent(49/49): loss=1.5284131575077332e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.355568525883247\n",
      "Gradient Descent(2/49): loss=109.48457551475549\n",
      "Gradient Descent(3/49): loss=4345.409184790492\n",
      "Gradient Descent(4/49): loss=203685.65803728488\n",
      "Gradient Descent(5/49): loss=14028491.107481724\n",
      "Gradient Descent(6/49): loss=1557038537.9659238\n",
      "Gradient Descent(7/49): loss=226219219603.2462\n",
      "Gradient Descent(8/49): loss=35865858104160.82\n",
      "Gradient Descent(9/49): loss=5815039485871936.0\n",
      "Gradient Descent(10/49): loss=9.478716304307118e+17\n",
      "Gradient Descent(11/49): loss=1.547010512440385e+20\n",
      "Gradient Descent(12/49): loss=2.5256030625379805e+22\n",
      "Gradient Descent(13/49): loss=4.1235085454976345e+24\n",
      "Gradient Descent(14/49): loss=6.732489957932094e+26\n",
      "Gradient Descent(15/49): loss=1.0992239189184618e+29\n",
      "Gradient Descent(16/49): loss=1.7947212681389314e+31\n",
      "Gradient Descent(17/49): loss=2.9302720224829404e+33\n",
      "Gradient Descent(18/49): loss=4.784305336619044e+35\n",
      "Gradient Descent(19/49): loss=7.811417382846021e+37\n",
      "Gradient Descent(20/49): loss=1.275383517507506e+40\n",
      "Gradient Descent(21/49): loss=2.0823405510356966e+42\n",
      "Gradient Descent(22/49): loss=3.3998731452870664e+44\n",
      "Gradient Descent(23/49): loss=5.551031217581738e+46\n",
      "Gradient Descent(24/49): loss=9.063263910750909e+48\n",
      "Gradient Descent(25/49): loss=1.4797746490039887e+51\n",
      "Gradient Descent(26/49): loss=2.4160534586644274e+53\n",
      "Gradient Descent(27/49): loss=3.9447319354023495e+55\n",
      "Gradient Descent(28/49): loss=6.440631512675868e+57\n",
      "Gradient Descent(29/49): loss=1.051572957589137e+60\n",
      "Gradient Descent(30/49): loss=1.7169212102204982e+62\n",
      "Gradient Descent(31/49): loss=2.803246718005447e+64\n",
      "Gradient Descent(32/49): loss=4.576909013197576e+66\n",
      "Gradient Descent(33/49): loss=7.472797874171334e+68\n",
      "Gradient Descent(34/49): loss=1.2200965303700972e+71\n",
      "Gradient Descent(35/49): loss=1.992072538943448e+73\n",
      "Gradient Descent(36/49): loss=3.2524910133207957e+75\n",
      "Gradient Descent(37/49): loss=5.310397882068722e+77\n",
      "Gradient Descent(38/49): loss=8.670377735214008e+79\n",
      "Gradient Descent(39/49): loss=1.4156274490303813e+82\n",
      "Gradient Descent(40/49): loss=2.3113192246621226e+84\n",
      "Gradient Descent(41/49): loss=3.773730554569248e+86\n",
      "Gradient Descent(42/49): loss=6.161434624233247e+88\n",
      "Gradient Descent(43/49): loss=1.0059880025810194e+91\n",
      "Gradient Descent(44/49): loss=1.642493872054825e+93\n",
      "Gradient Descent(45/49): loss=2.6817279259951605e+95\n",
      "Gradient Descent(46/49): loss=4.378503196523483e+97\n",
      "Gradient Descent(47/49): loss=7.148857293139658e+99\n",
      "Gradient Descent(48/49): loss=1.1672061959039713e+102\n",
      "Gradient Descent(49/49): loss=1.9057175823945742e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.417116474526697\n",
      "Gradient Descent(2/49): loss=113.6645828390305\n",
      "Gradient Descent(3/49): loss=4752.591795077513\n",
      "Gradient Descent(4/49): loss=259289.84612448118\n",
      "Gradient Descent(5/49): loss=22883571.367604654\n",
      "Gradient Descent(6/49): loss=3038053701.00153\n",
      "Gradient Descent(7/49): loss=477173695164.6873\n",
      "Gradient Descent(8/49): loss=78511032284068.55\n",
      "Gradient Descent(9/49): loss=1.3063112927830726e+16\n",
      "Gradient Descent(10/49): loss=2.1791810930995395e+18\n",
      "Gradient Descent(11/49): loss=3.6374806150653483e+20\n",
      "Gradient Descent(12/49): loss=6.07250746915993e+22\n",
      "Gradient Descent(13/49): loss=1.0137928675714053e+25\n",
      "Gradient Descent(14/49): loss=1.6925191078725932e+27\n",
      "Gradient Descent(15/49): loss=2.8256519052243838e+29\n",
      "Gradient Descent(16/49): loss=4.717413080106353e+31\n",
      "Gradient Descent(17/49): loss=7.875700509009595e+33\n",
      "Gradient Descent(18/49): loss=1.3148447830122862e+36\n",
      "Gradient Descent(19/49): loss=2.195127670960001e+38\n",
      "Gradient Descent(20/49): loss=3.6647561440612973e+40\n",
      "Gradient Descent(21/49): loss=6.118294519473558e+42\n",
      "Gradient Descent(22/49): loss=1.0214466217575047e+45\n",
      "Gradient Descent(23/49): loss=1.7053007137736576e+47\n",
      "Gradient Descent(24/49): loss=2.8469921603951273e+49\n",
      "Gradient Descent(25/49): loss=4.7530410888180545e+51\n",
      "Gradient Descent(26/49): loss=7.935181524652034e+53\n",
      "Gradient Descent(27/49): loss=1.3247751208655846e+56\n",
      "Gradient Descent(28/49): loss=2.2117063301099947e+58\n",
      "Gradient Descent(29/49): loss=3.6924341449381574e+60\n",
      "Gradient Descent(30/49): loss=6.16450282259089e+62\n",
      "Gradient Descent(31/49): loss=1.0291610779795488e+65\n",
      "Gradient Descent(32/49): loss=1.7181799650516877e+67\n",
      "Gradient Descent(33/49): loss=2.868494014659724e+69\n",
      "Gradient Descent(34/49): loss=4.788938341444889e+71\n",
      "Gradient Descent(35/49): loss=7.995111832534709e+73\n",
      "Gradient Descent(36/49): loss=1.3347804598262962e+76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=2.228410200197669e+78\n",
      "Gradient Descent(38/49): loss=3.7203211837481977e+80\n",
      "Gradient Descent(39/49): loss=6.211060113177417e+82\n",
      "Gradient Descent(40/49): loss=1.036933797491055e+85\n",
      "Gradient Descent(41/49): loss=1.731156486632628e+87\n",
      "Gradient Descent(42/49): loss=2.890158261271307e+89\n",
      "Gradient Descent(43/49): loss=4.82510670739118e+91\n",
      "Gradient Descent(44/49): loss=8.055494762930262e+93\n",
      "Gradient Descent(45/49): loss=1.3448613639195976e+96\n",
      "Gradient Descent(46/49): loss=2.2452402259470608e+98\n",
      "Gradient Descent(47/49): loss=3.748418838889533e+100\n",
      "Gradient Descent(48/49): loss=6.257969026817665e+102\n",
      "Gradient Descent(49/49): loss=1.0447652203191789e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.445367113085177\n",
      "Gradient Descent(2/49): loss=115.95769712927282\n",
      "Gradient Descent(3/49): loss=4837.24544641908\n",
      "Gradient Descent(4/49): loss=253395.6642394143\n",
      "Gradient Descent(5/49): loss=20593879.58608425\n",
      "Gradient Descent(6/49): loss=2548551516.216096\n",
      "Gradient Descent(7/49): loss=383165253733.3551\n",
      "Gradient Descent(8/49): loss=61059493796679.32\n",
      "Gradient Descent(9/49): loss=9874934456490228.0\n",
      "Gradient Descent(10/49): loss=1.6027663978021714e+18\n",
      "Gradient Descent(11/49): loss=2.6036281614352427e+20\n",
      "Gradient Descent(12/49): loss=4.230354748753217e+22\n",
      "Gradient Descent(13/49): loss=6.873784289318467e+24\n",
      "Gradient Descent(14/49): loss=1.1169149237759376e+27\n",
      "Gradient Descent(15/49): loss=1.814869920278125e+29\n",
      "Gradient Descent(16/49): loss=2.948975751551723e+31\n",
      "Gradient Descent(17/49): loss=4.791781092898366e+33\n",
      "Gradient Descent(18/49): loss=7.786149785970152e+35\n",
      "Gradient Descent(19/49): loss=1.2651690022082822e+38\n",
      "Gradient Descent(20/49): loss=2.0557690920747937e+40\n",
      "Gradient Descent(21/49): loss=3.340412667988511e+42\n",
      "Gradient Descent(22/49): loss=5.427825934759606e+44\n",
      "Gradient Descent(23/49): loss=8.819657122382293e+46\n",
      "Gradient Descent(24/49): loss=1.433103284664989e+49\n",
      "Gradient Descent(25/49): loss=2.3286449756748372e+51\n",
      "Gradient Descent(26/49): loss=3.7838078251321187e+53\n",
      "Gradient Descent(27/49): loss=6.148297317577725e+55\n",
      "Gradient Descent(28/49): loss=9.990348784168184e+57\n",
      "Gradient Descent(29/49): loss=1.623328600976817e+60\n",
      "Gradient Descent(30/49): loss=2.637741487990311e+62\n",
      "Gradient Descent(31/49): loss=4.2860577662948264e+64\n",
      "Gradient Descent(32/49): loss=6.964401651813227e+66\n",
      "Gradient Descent(33/49): loss=1.1316434124897107e+69\n",
      "Gradient Descent(34/49): loss=1.838803786823454e+71\n",
      "Gradient Descent(35/49): loss=2.987866433117217e+73\n",
      "Gradient Descent(36/49): loss=4.8549746776249436e+75\n",
      "Gradient Descent(37/49): loss=7.888832934137765e+77\n",
      "Gradient Descent(38/49): loss=1.2818539579526644e+80\n",
      "Gradient Descent(39/49): loss=2.082880425073328e+82\n",
      "Gradient Descent(40/49): loss=3.384465787415197e+84\n",
      "Gradient Descent(41/49): loss=5.499407708813143e+86\n",
      "Gradient Descent(42/49): loss=8.935970119778114e+88\n",
      "Gradient Descent(43/49): loss=1.4520029466736966e+91\n",
      "Gradient Descent(44/49): loss=2.3593549764482117e+93\n",
      "Gradient Descent(45/49): loss=3.833708407853493e+95\n",
      "Gradient Descent(46/49): loss=6.229380615956192e+97\n",
      "Gradient Descent(47/49): loss=1.0122100778180268e+100\n",
      "Gradient Descent(48/49): loss=1.6447369406390622e+102\n",
      "Gradient Descent(49/49): loss=2.6725278311140048e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5104253584656884\n",
      "Gradient Descent(2/49): loss=117.82482092315563\n",
      "Gradient Descent(3/49): loss=4857.258158077765\n",
      "Gradient Descent(4/49): loss=247588.29348958097\n",
      "Gradient Descent(5/49): loss=19371276.741113055\n",
      "Gradient Descent(6/49): loss=2342152303.296001\n",
      "Gradient Descent(7/49): loss=349497394197.11615\n",
      "Gradient Descent(8/49): loss=55598620510966.086\n",
      "Gradient Descent(9/49): loss=8989897991389714.0\n",
      "Gradient Descent(10/49): loss=1.4593389416586977e+18\n",
      "Gradient Descent(11/49): loss=2.3711896845695277e+20\n",
      "Gradient Descent(12/49): loss=3.853663423283862e+22\n",
      "Gradient Descent(13/49): loss=6.263317673217851e+24\n",
      "Gradient Descent(14/49): loss=1.0179832062875098e+27\n",
      "Gradient Descent(15/49): loss=1.6545431580261174e+29\n",
      "Gradient Descent(16/49): loss=2.689155394535897e+31\n",
      "Gradient Descent(17/49): loss=4.3707279188783693e+33\n",
      "Gradient Descent(18/49): loss=7.103815330956347e+35\n",
      "Gradient Descent(19/49): loss=1.1545946964020301e+38\n",
      "Gradient Descent(20/49): loss=1.87658160288306e+40\n",
      "Gradient Descent(21/49): loss=3.0500387063845663e+42\n",
      "Gradient Descent(22/49): loss=4.957277689053081e+44\n",
      "Gradient Descent(23/49): loss=8.057144335809892e+46\n",
      "Gradient Descent(24/49): loss=1.3095408189843417e+49\n",
      "Gradient Descent(25/49): loss=2.128418066146989e+51\n",
      "Gradient Descent(26/49): loss=3.4593526208800583e+53\n",
      "Gradient Descent(27/49): loss=5.622542274908752e+55\n",
      "Gradient Descent(28/49): loss=9.138409725081611e+57\n",
      "Gradient Descent(29/49): loss=1.4852806474419887e+60\n",
      "Gradient Descent(30/49): loss=2.414050877595126e+62\n",
      "Gradient Descent(31/49): loss=3.923596291148403e+64\n",
      "Gradient Descent(32/49): loss=6.3770850891300254e+66\n",
      "Gradient Descent(33/49): loss=1.0364780475949958e+69\n",
      "Gradient Descent(34/49): loss=1.6846046871438093e+71\n",
      "Gradient Descent(35/49): loss=2.7380154924958298e+73\n",
      "Gradient Descent(36/49): loss=4.450141267182248e+75\n",
      "Gradient Descent(37/49): loss=7.232887232433351e+77\n",
      "Gradient Descent(38/49): loss=1.1755729667032153e+80\n",
      "Gradient Descent(39/49): loss=1.9106779293425735e+82\n",
      "Gradient Descent(40/49): loss=3.10545602278926e+84\n",
      "Gradient Descent(41/49): loss=5.0473483580754e+86\n",
      "Gradient Descent(42/49): loss=8.203537664295913e+88\n",
      "Gradient Descent(43/49): loss=1.3333343655951835e+91\n",
      "Gradient Descent(44/49): loss=2.1670901057899436e+93\n",
      "Gradient Descent(45/49): loss=3.522206918079653e+95\n",
      "Gradient Descent(46/49): loss=5.724700389994124e+97\n",
      "Gradient Descent(47/49): loss=9.304448976855087e+99\n",
      "Gradient Descent(48/49): loss=1.5122672780258824e+102\n",
      "Gradient Descent(49/49): loss=2.457912688732689e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.3872518809813394\n",
      "Gradient Descent(2/49): loss=111.79011890934623\n",
      "Gradient Descent(3/49): loss=4483.019949918903\n",
      "Gradient Descent(4/49): loss=212216.44112090164\n",
      "Gradient Descent(5/49): loss=14748051.001750162\n",
      "Gradient Descent(6/49): loss=1651322298.7339506\n",
      "Gradient Descent(7/49): loss=242145065703.80188\n",
      "Gradient Descent(8/49): loss=38759074116275.805\n",
      "Gradient Descent(9/49): loss=6345062829124625.0\n",
      "Gradient Descent(10/49): loss=1.0443274092908923e+18\n",
      "Gradient Descent(11/49): loss=1.7210272601352906e+20\n",
      "Gradient Descent(12/49): loss=2.837055400100284e+22\n",
      "Gradient Descent(13/49): loss=4.677115453331479e+24\n",
      "Gradient Descent(14/49): loss=7.710728942179015e+26\n",
      "Gradient Descent(15/49): loss=1.2712016213415389e+29\n",
      "Gradient Descent(16/49): loss=2.0957227473229543e+31\n",
      "Gradient Descent(17/49): loss=3.455041810458966e+33\n",
      "Gradient Descent(18/49): loss=5.6960370884571214e+35\n",
      "Gradient Descent(19/49): loss=9.390577788771772e+37\n",
      "Gradient Descent(20/49): loss=1.5481456681882735e+40\n",
      "Gradient Descent(21/49): loss=2.552297702384081e+42\n",
      "Gradient Descent(22/49): loss=4.207758802293623e+44\n",
      "Gradient Descent(23/49): loss=6.936978441954644e+46\n",
      "Gradient Descent(24/49): loss=1.1436413579195494e+49\n",
      "Gradient Descent(25/49): loss=1.8854254290834848e+51\n",
      "Gradient Descent(26/49): loss=3.1083425096684716e+53\n",
      "Gradient Descent(27/49): loss=5.124463162730042e+55\n",
      "Gradient Descent(28/49): loss=8.448271908419406e+57\n",
      "Gradient Descent(29/49): loss=1.3927956153082356e+60\n",
      "Gradient Descent(30/49): loss=2.2961851217035673e+62\n",
      "Gradient Descent(31/49): loss=3.785527506823667e+64\n",
      "Gradient Descent(32/49): loss=6.240881176987559e+66\n",
      "Gradient Descent(33/49): loss=1.028881649785137e+69\n",
      "Gradient Descent(34/49): loss=1.696230739287276e+71\n",
      "Gradient Descent(35/49): loss=2.7964331189150552e+73\n",
      "Gradient Descent(36/49): loss=4.610244353814097e+75\n",
      "Gradient Descent(37/49): loss=7.60052255786482e+77\n",
      "Gradient Descent(38/49): loss=1.2530343018547727e+80\n",
      "Gradient Descent(39/49): loss=2.0657723856105056e+82\n",
      "Gradient Descent(40/49): loss=3.4056653858829675e+84\n",
      "Gradient Descent(41/49): loss=5.614634410544546e+86\n",
      "Gradient Descent(42/49): loss=9.25637606523041e+88\n",
      "Gradient Descent(43/49): loss=1.5260209587298828e+91\n",
      "Gradient Descent(44/49): loss=2.515822553094271e+93\n",
      "Gradient Descent(45/49): loss=4.1476252881388633e+95\n",
      "Gradient Descent(46/49): loss=6.837841369078558e+97\n",
      "Gradient Descent(47/49): loss=1.1272974615713868e+100\n",
      "Gradient Descent(48/49): loss=1.8584806202319208e+102\n",
      "Gradient Descent(49/49): loss=3.063920866958305e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4492838657874\n",
      "Gradient Descent(2/49): loss=116.04422348311715\n",
      "Gradient Descent(3/49): loss=4901.734675774193\n",
      "Gradient Descent(4/49): loss=269979.23640243226\n",
      "Gradient Descent(5/49): loss=24037502.69928381\n",
      "Gradient Descent(6/49): loss=3220061210.562637\n",
      "Gradient Descent(7/49): loss=510533185721.5546\n",
      "Gradient Descent(8/49): loss=84807264550339.05\n",
      "Gradient Descent(9/49): loss=1.4247115096676176e+16\n",
      "Gradient Descent(10/49): loss=2.3997042670150385e+18\n",
      "Gradient Descent(11/49): loss=4.0443700396389047e+20\n",
      "Gradient Descent(12/49): loss=6.817175496325018e+22\n",
      "Gradient Descent(13/49): loss=1.1491374431766659e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(14/49): loss=1.9370582452978156e+27\n",
      "Gradient Descent(15/49): loss=3.2652325765654224e+29\n",
      "Gradient Descent(16/49): loss=5.504092595309972e+31\n",
      "Gradient Descent(17/49): loss=9.278064368131693e+33\n",
      "Gradient Descent(18/49): loss=1.5639722390312765e+36\n",
      "Gradient Descent(19/49): loss=2.6363356397027095e+38\n",
      "Gradient Descent(20/49): loss=4.443982725099652e+40\n",
      "Gradient Descent(21/49): loss=7.491072900033761e+42\n",
      "Gradient Descent(22/49): loss=1.2627450795417718e+45\n",
      "Gradient Descent(23/49): loss=2.128567105415721e+47\n",
      "Gradient Descent(24/49): loss=3.5880543077751143e+49\n",
      "Gradient Descent(25/49): loss=6.048263022949493e+51\n",
      "Gradient Descent(26/49): loss=1.019535448934392e+54\n",
      "Gradient Descent(27/49): loss=1.7185967734700316e+56\n",
      "Gradient Descent(28/49): loss=2.896981044522377e+58\n",
      "Gradient Descent(29/49): loss=4.883343959372579e+60\n",
      "Gradient Descent(30/49): loss=8.231689423936876e+62\n",
      "Gradient Descent(31/49): loss=1.3875883275046327e+65\n",
      "Gradient Descent(32/49): loss=2.339011188916126e+67\n",
      "Gradient Descent(33/49): loss=3.942792853924835e+69\n",
      "Gradient Descent(34/49): loss=6.646233913983355e+71\n",
      "Gradient Descent(35/49): loss=1.120333400102697e+74\n",
      "Gradient Descent(36/49): loss=1.8885085051624363e+76\n",
      "Gradient Descent(37/49): loss=3.1833955621995687e+78\n",
      "Gradient Descent(38/49): loss=5.366143323013593e+80\n",
      "Gradient Descent(39/49): loss=9.045528147694857e+82\n",
      "Gradient Descent(40/49): loss=1.5247743965360798e+85\n",
      "Gradient Descent(41/49): loss=2.570261152660809e+87\n",
      "Gradient Descent(42/49): loss=4.3326031758435e+89\n",
      "Gradient Descent(43/49): loss=7.303324123269221e+91\n",
      "Gradient Descent(44/49): loss=1.2310968968244475e+94\n",
      "Gradient Descent(45/49): loss=2.075218823359975e+96\n",
      "Gradient Descent(46/49): loss=3.498126894752312e+98\n",
      "Gradient Descent(47/49): loss=5.8966753934781285e+100\n",
      "Gradient Descent(48/49): loss=9.939828297312733e+102\n",
      "Gradient Descent(49/49): loss=1.6755235787497366e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.477818350613649\n",
      "Gradient Descent(2/49): loss=118.38394151711374\n",
      "Gradient Descent(3/49): loss=4989.126721020881\n",
      "Gradient Descent(4/49): loss=263875.2028105166\n",
      "Gradient Descent(5/49): loss=21636622.382290304\n",
      "Gradient Descent(6/49): loss=2701617888.1387224\n",
      "Gradient Descent(7/49): loss=409992450531.3855\n",
      "Gradient Descent(8/49): loss=65961811620972.94\n",
      "Gradient Descent(9/49): loss=1.077091389609234e+16\n",
      "Gradient Descent(10/49): loss=1.7651267296706427e+18\n",
      "Gradient Descent(11/49): loss=2.8951711547795358e+20\n",
      "Gradient Descent(12/49): loss=4.74965761734846e+22\n",
      "Gradient Descent(13/49): loss=7.792410293200629e+24\n",
      "Gradient Descent(14/49): loss=1.2784579390336082e+27\n",
      "Gradient Descent(15/49): loss=2.0975016090354423e+29\n",
      "Gradient Descent(16/49): loss=3.4412676647681007e+31\n",
      "Gradient Descent(17/49): loss=5.645919410996951e+33\n",
      "Gradient Descent(18/49): loss=9.262983972342839e+35\n",
      "Gradient Descent(19/49): loss=1.519732511496249e+38\n",
      "Gradient Descent(20/49): loss=2.4933508667147406e+40\n",
      "Gradient Descent(21/49): loss=4.090718926382596e+42\n",
      "Gradient Descent(22/49): loss=6.711442645381647e+44\n",
      "Gradient Descent(23/49): loss=1.1011135987148186e+47\n",
      "Gradient Descent(24/49): loss=1.8065432744444466e+49\n",
      "Gradient Descent(25/49): loss=2.9639072719247695e+51\n",
      "Gradient Descent(26/49): loss=4.862737826899744e+53\n",
      "Gradient Descent(27/49): loss=7.978056330286987e+55\n",
      "Gradient Descent(28/49): loss=1.3089207165794524e+58\n",
      "Gradient Descent(29/49): loss=2.1474822580366356e+60\n",
      "Gradient Descent(30/49): loss=3.5232692019985865e+62\n",
      "Gradient Descent(31/49): loss=5.780455611819818e+64\n",
      "Gradient Descent(32/49): loss=9.483711054853685e+66\n",
      "Gradient Descent(33/49): loss=1.5559461297141238e+69\n",
      "Gradient Descent(34/49): loss=2.5527647822350837e+71\n",
      "Gradient Descent(35/49): loss=4.188196434934975e+73\n",
      "Gradient Descent(36/49): loss=6.87136923059718e+75\n",
      "Gradient Descent(37/49): loss=1.1273519720650703e+78\n",
      "Gradient Descent(38/49): loss=1.8495912914412473e+80\n",
      "Gradient Descent(39/49): loss=3.0345340498308233e+82\n",
      "Gradient Descent(40/49): loss=4.978611730166095e+84\n",
      "Gradient Descent(41/49): loss=8.168164981088199e+86\n",
      "Gradient Descent(42/49): loss=1.3401109139324135e+89\n",
      "Gradient Descent(43/49): loss=2.1986544906949996e+91\n",
      "Gradient Descent(44/49): loss=3.607224983541219e+93\n",
      "Gradient Descent(45/49): loss=5.918197760017876e+95\n",
      "Gradient Descent(46/49): loss=9.70969786650139e+97\n",
      "Gradient Descent(47/49): loss=1.5930226815951683e+100\n",
      "Gradient Descent(48/49): loss=2.613594469125312e+102\n",
      "Gradient Descent(49/49): loss=4.2879967297153545e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5435780537071007\n",
      "Gradient Descent(2/49): loss=120.29134820003472\n",
      "Gradient Descent(3/49): loss=5009.916209402752\n",
      "Gradient Descent(4/49): loss=257847.3844227114\n",
      "Gradient Descent(5/49): loss=20353896.84158226\n",
      "Gradient Descent(6/49): loss=2482926510.741541\n",
      "Gradient Descent(7/49): loss=373972124683.84155\n",
      "Gradient Descent(8/49): loss=60062675146177.3\n",
      "Gradient Descent(9/49): loss=9805577100048416.0\n",
      "Gradient Descent(10/49): loss=1.6071690044073956e+18\n",
      "Gradient Descent(11/49): loss=2.636702766595695e+20\n",
      "Gradient Descent(12/49): loss=4.326720417852751e+22\n",
      "Gradient Descent(13/49): loss=7.100351407874338e+24\n",
      "Gradient Descent(14/49): loss=1.1652159344393376e+27\n",
      "Gradient Descent(15/49): loss=1.9122043922152674e+29\n",
      "Gradient Descent(16/49): loss=3.1380692500666447e+31\n",
      "Gradient Descent(17/49): loss=5.149805304171135e+33\n",
      "Gradient Descent(18/49): loss=8.45121431366904e+35\n",
      "Gradient Descent(19/49): loss=1.3869072684840448e+38\n",
      "Gradient Descent(20/49): loss=2.2760182199584926e+40\n",
      "Gradient Descent(21/49): loss=3.735115575627482e+42\n",
      "Gradient Descent(22/49): loss=6.129603112490885e+44\n",
      "Gradient Descent(23/49): loss=1.0059135669860135e+47\n",
      "Gradient Descent(24/49): loss=1.6507791543504353e+49\n",
      "Gradient Descent(25/49): loss=2.709051657994672e+51\n",
      "Gradient Descent(26/49): loss=4.445755730768466e+53\n",
      "Gradient Descent(27/49): loss=7.295816585606635e+55\n",
      "Gradient Descent(28/49): loss=1.197297892064208e+58\n",
      "Gradient Descent(29/49): loss=1.9648550995230574e+60\n",
      "Gradient Descent(30/49): loss=3.224473698409202e+62\n",
      "Gradient Descent(31/49): loss=5.291601723840364e+64\n",
      "Gradient Descent(32/49): loss=8.683912918118978e+66\n",
      "Gradient Descent(33/49): loss=1.4250948484978535e+69\n",
      "Gradient Descent(34/49): loss=2.3386868873105207e+71\n",
      "Gradient Descent(35/49): loss=3.837959531355574e+73\n",
      "Gradient Descent(36/49): loss=6.298377711119087e+75\n",
      "Gradient Descent(37/49): loss=1.033610736846681e+78\n",
      "Gradient Descent(38/49): loss=1.6962322749216737e+80\n",
      "Gradient Descent(39/49): loss=2.783643617387003e+82\n",
      "Gradient Descent(40/49): loss=4.568166696968074e+84\n",
      "Gradient Descent(41/49): loss=7.496702106887006e+86\n",
      "Gradient Descent(42/49): loss=1.230264703709391e+89\n",
      "Gradient Descent(43/49): loss=2.018956095111041e+91\n",
      "Gradient Descent(44/49): loss=3.313257465402233e+93\n",
      "Gradient Descent(45/49): loss=5.437302504312227e+95\n",
      "Gradient Descent(46/49): loss=8.923018760876944e+97\n",
      "Gradient Descent(47/49): loss=1.4643338998302619e+100\n",
      "Gradient Descent(48/49): loss=2.4030810958211527e+102\n",
      "Gradient Descent(49/49): loss=3.9436352281145144e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4191040292077775\n",
      "Gradient Descent(2/49): loss=114.13202402909809\n",
      "Gradient Descent(3/49): loss=4624.2451486605105\n",
      "Gradient Descent(4/49): loss=221058.5526630439\n",
      "Gradient Descent(5/49): loss=15500706.630500952\n",
      "Gradient Descent(6/49): loss=1750817295.2888377\n",
      "Gradient Descent(7/49): loss=259107069436.5092\n",
      "Gradient Descent(8/49): loss=41870015117893.516\n",
      "Gradient Descent(9/49): loss=6920484893112940.0\n",
      "Gradient Descent(10/49): loss=1.1500610387563127e+18\n",
      "Gradient Descent(11/49): loss=1.9136345651239358e+20\n",
      "Gradient Descent(12/49): loss=3.1851289380680934e+22\n",
      "Gradient Descent(13/49): loss=5.301825983347498e+24\n",
      "Gradient Descent(14/49): loss=8.825332010047894e+26\n",
      "Gradient Descent(15/49): loss=1.469055849626222e+29\n",
      "Gradient Descent(16/49): loss=2.4453777286714445e+31\n",
      "Gradient Descent(17/49): loss=4.0705555885774725e+33\n",
      "Gradient Descent(18/49): loss=6.775813578875295e+35\n",
      "Gradient Descent(19/49): loss=1.1278964059997256e+38\n",
      "Gradient Descent(20/49): loss=1.8774871700831545e+40\n",
      "Gradient Descent(21/49): loss=3.1252498521304294e+42\n",
      "Gradient Descent(22/49): loss=5.2022654510412586e+44\n",
      "Gradient Descent(23/49): loss=8.659648701557213e+46\n",
      "Gradient Descent(24/49): loss=1.4414780702892672e+49\n",
      "Gradient Descent(25/49): loss=2.399472656154266e+51\n",
      "Gradient Descent(26/49): loss=3.994142641711645e+53\n",
      "Gradient Descent(27/49): loss=6.648617312401416e+55\n",
      "Gradient Descent(28/49): loss=1.1067234230728913e+58\n",
      "Gradient Descent(29/49): loss=1.842242796699326e+60\n",
      "Gradient Descent(30/49): loss=3.0665823558402716e+62\n",
      "Gradient Descent(31/49): loss=5.10460801475215e+64\n",
      "Gradient Descent(32/49): loss=8.497088928541841e+66\n",
      "Gradient Descent(33/49): loss=1.4144185028682003e+69\n",
      "Gradient Descent(34/49): loss=2.3544295206043546e+71\n",
      "Gradient Descent(35/49): loss=3.919164205114884e+73\n",
      "Gradient Descent(36/49): loss=6.5238088174800155e+75\n",
      "Gradient Descent(37/49): loss=1.0859479026544677e+78\n",
      "Gradient Descent(38/49): loss=1.8076600346102075e+80\n",
      "Gradient Descent(39/49): loss=3.009016171714746e+82\n",
      "Gradient Descent(40/49): loss=5.008783813485916e+84\n",
      "Gradient Descent(41/49): loss=8.337580743522931e+86\n",
      "Gradient Descent(42/49): loss=1.387866900296236e+89\n",
      "Gradient Descent(43/49): loss=2.3102319392040038e+91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=3.845593271068781e+93\n",
      "Gradient Descent(45/49): loss=6.401343239841444e+95\n",
      "Gradient Descent(46/49): loss=1.0655623823388762e+98\n",
      "Gradient Descent(47/49): loss=1.7737264635160935e+100\n",
      "Gradient Descent(48/49): loss=2.952530625632442e+102\n",
      "Gradient Descent(49/49): loss=4.914758433505345e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4816216691147512\n",
      "Gradient Descent(2/49): loss=118.4611746359801\n",
      "Gradient Descent(3/49): loss=5054.7605610118335\n",
      "Gradient Descent(4/49): loss=281052.7760888377\n",
      "Gradient Descent(5/49): loss=25243603.68251736\n",
      "Gradient Descent(6/49): loss=3412019048.305747\n",
      "Gradient Descent(7/49): loss=546047834245.61664\n",
      "Gradient Descent(8/49): loss=91574537412934.1\n",
      "Gradient Descent(9/49): loss=1.5531966708786082e+16\n",
      "Gradient Descent(10/49): loss=2.641321821412238e+18\n",
      "Gradient Descent(11/49): loss=4.4944877051228796e+20\n",
      "Gradient Descent(12/49): loss=7.648916608408304e+22\n",
      "Gradient Descent(13/49): loss=1.3017682500621425e+25\n",
      "Gradient Descent(14/49): loss=2.2154943835240831e+27\n",
      "Gradient Descent(15/49): loss=3.7705818769489016e+29\n",
      "Gradient Descent(16/49): loss=6.417210265454071e+31\n",
      "Gradient Descent(17/49): loss=1.0921548100073643e+34\n",
      "Gradient Descent(18/49): loss=1.858754980213069e+36\n",
      "Gradient Descent(19/49): loss=3.1634435534909935e+38\n",
      "Gradient Descent(20/49): loss=5.383913013696922e+40\n",
      "Gradient Descent(21/49): loss=9.162963984132796e+42\n",
      "Gradient Descent(22/49): loss=1.559458868779381e+45\n",
      "Gradient Descent(23/49): loss=2.654066923823959e+47\n",
      "Gradient Descent(24/49): loss=4.5169971309804365e+49\n",
      "Gradient Descent(25/49): loss=7.687546571698016e+51\n",
      "Gradient Descent(26/49): loss=1.3083553205447303e+54\n",
      "Gradient Descent(27/49): loss=2.2267099507401216e+56\n",
      "Gradient Descent(28/49): loss=3.789671755728221e+58\n",
      "Gradient Descent(29/49): loss=6.449700380325977e+60\n",
      "Gradient Descent(30/49): loss=1.0976843821130112e+63\n",
      "Gradient Descent(31/49): loss=1.8681658552857458e+65\n",
      "Gradient Descent(32/49): loss=3.179460070423128e+67\n",
      "Gradient Descent(33/49): loss=5.41117177086445e+69\n",
      "Gradient Descent(34/49): loss=9.209356080985042e+71\n",
      "Gradient Descent(35/49): loss=1.5673544107956925e+74\n",
      "Gradient Descent(36/49): loss=2.6675044676717885e+76\n",
      "Gradient Descent(37/49): loss=4.539866692585831e+78\n",
      "Gradient Descent(38/49): loss=7.726468628724989e+80\n",
      "Gradient Descent(39/49): loss=1.3149795250192379e+83\n",
      "Gradient Descent(40/49): loss=2.2379837857507536e+85\n",
      "Gradient Descent(41/49): loss=3.8088588681333967e+87\n",
      "Gradient Descent(42/49): loss=6.482355220679735e+89\n",
      "Gradient Descent(43/49): loss=1.10324195938682e+92\n",
      "Gradient Descent(44/49): loss=1.8776243811336196e+94\n",
      "Gradient Descent(45/49): loss=3.1955576803721214e+96\n",
      "Gradient Descent(46/49): loss=5.438568539688367e+98\n",
      "Gradient Descent(47/49): loss=9.2559830612865e+100\n",
      "Gradient Descent(48/49): loss=1.5752899279583844e+103\n",
      "Gradient Descent(49/49): loss=2.6810100458224357e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5104414034390357\n",
      "Gradient Descent(2/49): loss=120.84820140319756\n",
      "Gradient Descent(3/49): loss=5144.96401796962\n",
      "Gradient Descent(4/49): loss=274732.4864030516\n",
      "Gradient Descent(5/49): loss=22726703.33359989\n",
      "Gradient Descent(6/49): loss=2863074047.421788\n",
      "Gradient Descent(7/49): loss=438555417501.8708\n",
      "Gradient Descent(8/49): loss=71231309419792.88\n",
      "Gradient Descent(9/49): loss=1.1743290270000416e+16\n",
      "Gradient Descent(10/49): loss=1.9430338016987853e+18\n",
      "Gradient Descent(11/49): loss=3.217719706562922e+20\n",
      "Gradient Descent(12/49): loss=5.329744449043898e+22\n",
      "Gradient Descent(13/49): loss=8.8284847131005e+24\n",
      "Gradient Descent(14/49): loss=1.462416644110967e+27\n",
      "Gradient Descent(15/49): loss=2.4224638748391084e+29\n",
      "Gradient Descent(16/49): loss=4.012765612262225e+31\n",
      "Gradient Descent(17/49): loss=6.647071458270163e+33\n",
      "Gradient Descent(18/49): loss=1.1010750430394828e+36\n",
      "Gradient Descent(19/49): loss=1.82391037814186e+38\n",
      "Gradient Descent(20/49): loss=3.0212737051265694e+40\n",
      "Gradient Descent(21/49): loss=5.004683846063466e+42\n",
      "Gradient Descent(22/49): loss=8.290165952108601e+44\n",
      "Gradient Descent(23/49): loss=1.3732506114178476e+47\n",
      "Gradient Descent(24/49): loss=2.2747641635492237e+49\n",
      "Gradient Descent(25/49): loss=3.7681046392793835e+51\n",
      "Gradient Descent(26/49): loss=6.241795435361071e+53\n",
      "Gradient Descent(27/49): loss=1.0339418351277199e+56\n",
      "Gradient Descent(28/49): loss=1.712705469921357e+58\n",
      "Gradient Descent(29/49): loss=2.8370648396640895e+60\n",
      "Gradient Descent(30/49): loss=4.69954527839968e+62\n",
      "Gradient Descent(31/49): loss=7.784709575528511e+64\n",
      "Gradient Descent(32/49): loss=1.2895227002890479e+67\n",
      "Gradient Descent(33/49): loss=2.136070431950408e+69\n",
      "Gradient Descent(34/49): loss=3.5383610457031576e+71\n",
      "Gradient Descent(35/49): loss=5.86122943442355e+73\n",
      "Gradient Descent(36/49): loss=9.709017830351709e+75\n",
      "Gradient Descent(37/49): loss=1.608280793044163e+78\n",
      "Gradient Descent(38/49): loss=2.6640872995297497e+80\n",
      "Gradient Descent(39/49): loss=4.41301119195844e+82\n",
      "Gradient Descent(40/49): loss=7.3100711766420765e+84\n",
      "Gradient Descent(41/49): loss=1.2108997299836665e+87\n",
      "Gradient Descent(42/49): loss=2.0058329401220075e+89\n",
      "Gradient Descent(43/49): loss=3.322625056438525e+91\n",
      "Gradient Descent(44/49): loss=5.503866770181656e+93\n",
      "Gradient Descent(45/49): loss=9.11705320623191e+95\n",
      "Gradient Descent(46/49): loss=1.5102229511729315e+98\n",
      "Gradient Descent(47/49): loss=2.501656303475856e+100\n",
      "Gradient Descent(48/49): loss=4.1439472601446614e+102\n",
      "Gradient Descent(49/49): loss=6.864371764818572e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5769062224302703\n",
      "Gradient Descent(2/49): loss=122.79653817644281\n",
      "Gradient Descent(3/49): loss=5166.55420161018\n",
      "Gradient Descent(4/49): loss=268476.9681496094\n",
      "Gradient Descent(5/49): loss=21381208.543445796\n",
      "Gradient Descent(6/49): loss=2631422805.8923783\n",
      "Gradient Descent(7/49): loss=400030748759.31726\n",
      "Gradient Descent(8/49): loss=64861098682492.234\n",
      "Gradient Descent(9/49): loss=1.0690806408741214e+16\n",
      "Gradient Descent(10/49): loss=1.769154370135657e+18\n",
      "Gradient Descent(11/49): loss=2.930452870744052e+20\n",
      "Gradient Descent(12/49): loss=4.855147796900558e+22\n",
      "Gradient Descent(13/49): loss=8.044400744674941e+24\n",
      "Gradient Descent(14/49): loss=1.3328784552908828e+27\n",
      "Gradient Descent(15/49): loss=2.208455919043732e+29\n",
      "Gradient Descent(16/49): loss=3.659209213178781e+31\n",
      "Gradient Descent(17/49): loss=6.062975625165333e+33\n",
      "Gradient Descent(18/49): loss=1.0045797553854433e+36\n",
      "Gradient Descent(19/49): loss=1.6644970346931035e+38\n",
      "Gradient Descent(20/49): loss=2.7579197870609e+40\n",
      "Gradient Descent(21/49): loss=4.569621571932711e+42\n",
      "Gradient Descent(22/49): loss=7.571446207916408e+44\n",
      "Gradient Descent(23/49): loss=1.2545195872086497e+47\n",
      "Gradient Descent(24/49): loss=2.0786245473904953e+49\n",
      "Gradient Descent(25/49): loss=3.444091310392281e+51\n",
      "Gradient Descent(26/49): loss=5.706545210012082e+53\n",
      "Gradient Descent(27/49): loss=9.455224992338003e+55\n",
      "Gradient Descent(28/49): loss=1.5666445522744926e+58\n",
      "Gradient Descent(29/49): loss=2.5957871496026674e+60\n",
      "Gradient Descent(30/49): loss=4.300982578505137e+62\n",
      "Gradient Descent(31/49): loss=7.126335895235383e+64\n",
      "Gradient Descent(32/49): loss=1.180768867689092e+67\n",
      "Gradient Descent(33/49): loss=1.9564263310069008e+69\n",
      "Gradient Descent(34/49): loss=3.241620009975507e+71\n",
      "Gradient Descent(35/49): loss=5.371068730027559e+73\n",
      "Gradient Descent(36/49): loss=8.899371059502186e+75\n",
      "Gradient Descent(37/49): loss=1.4745446248329882e+78\n",
      "Gradient Descent(38/49): loss=2.4431859690828924e+80\n",
      "Gradient Descent(39/49): loss=4.0481363391762984e+82\n",
      "Gradient Descent(40/49): loss=6.707392735523546e+84\n",
      "Gradient Descent(41/49): loss=1.1113538067670275e+87\n",
      "Gradient Descent(42/49): loss=1.841411905514658e+89\n",
      "Gradient Descent(43/49): loss=3.0510515959225397e+91\n",
      "Gradient Descent(44/49): loss=5.055314247237839e+93\n",
      "Gradient Descent(45/49): loss=8.376194677428161e+95\n",
      "Gradient Descent(46/49): loss=1.387859069542714e+98\n",
      "Gradient Descent(47/49): loss=2.29955590944238e+100\n",
      "Gradient Descent(48/49): loss=3.810154429003969e+102\n",
      "Gradient Descent(49/49): loss=6.313078413639714e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4511249705625584\n",
      "Gradient Descent(2/49): loss=116.51067139735592\n",
      "Gradient Descent(3/49): loss=4769.160365890078\n",
      "Gradient Descent(4/49): loss=230221.69068105155\n",
      "Gradient Descent(5/49): loss=16287806.14398921\n",
      "Gradient Descent(6/49): loss=1855784281.034907\n",
      "Gradient Descent(7/49): loss=277167166120.51245\n",
      "Gradient Descent(8/49): loss=45213898536886.09\n",
      "Gradient Descent(9/49): loss=7544947379119971.0\n",
      "Gradient Descent(10/49): loss=1.2659137456747005e+18\n",
      "Gradient Descent(11/49): loss=2.1267144393466275e+20\n",
      "Gradient Descent(12/49): loss=3.5739217267861163e+22\n",
      "Gradient Descent(13/49): loss=6.00636288141755e+24\n",
      "Gradient Descent(14/49): loss=1.0094511084930356e+27\n",
      "Gradient Descent(15/49): loss=1.696526694775507e+29\n",
      "Gradient Descent(16/49): loss=2.8512578982411145e+31\n",
      "Gradient Descent(17/49): loss=4.791951321635893e+33\n",
      "Gradient Descent(18/49): loss=8.053567736541385e+35\n",
      "Gradient Descent(19/49): loss=1.3535186335269944e+38\n",
      "Gradient Descent(20/49): loss=2.2747839917341625e+40\n",
      "Gradient Descent(21/49): loss=3.823103786094477e+42\n",
      "Gradient Descent(22/49): loss=6.425279329628412e+44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=1.0798612011364797e+47\n",
      "Gradient Descent(24/49): loss=1.81486306494483e+49\n",
      "Gradient Descent(25/49): loss=3.050140093042438e+51\n",
      "Gradient Descent(26/49): loss=5.126201952582051e+53\n",
      "Gradient Descent(27/49): loss=8.615324430048457e+55\n",
      "Gradient Descent(28/49): loss=1.447929982501108e+58\n",
      "Gradient Descent(29/49): loss=2.4334559322147373e+60\n",
      "Gradient Descent(30/49): loss=4.0897749515499936e+62\n",
      "Gradient Descent(31/49): loss=6.873458825737881e+64\n",
      "Gradient Descent(32/49): loss=1.1551842531386305e+67\n",
      "Gradient Descent(33/49): loss=1.9414543573063796e+69\n",
      "Gradient Descent(34/49): loss=3.2628950847130613e+71\n",
      "Gradient Descent(35/49): loss=5.483767513657012e+73\n",
      "Gradient Descent(36/49): loss=9.21626511521262e+75\n",
      "Gradient Descent(37/49): loss=1.5489267636227844e+78\n",
      "Gradient Descent(38/49): loss=2.6031956427846617e+80\n",
      "Gradient Descent(39/49): loss=4.375047106012411e+82\n",
      "Gradient Descent(40/49): loss=7.352899976181678e+84\n",
      "Gradient Descent(41/49): loss=1.235761278671328e+87\n",
      "Gradient Descent(42/49): loss=2.0768757127258463e+89\n",
      "Gradient Descent(43/49): loss=3.490490275555645e+91\n",
      "Gradient Descent(44/49): loss=5.86627417764827e+93\n",
      "Gradient Descent(45/49): loss=9.859122934203004e+95\n",
      "Gradient Descent(46/49): loss=1.656968325859843e+98\n",
      "Gradient Descent(47/49): loss=2.7847751277935507e+100\n",
      "Gradient Descent(48/49): loss=4.680217715298361e+102\n",
      "Gradient Descent(49/49): loss=7.865783360378128e+104\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.514129884508753\n",
      "Gradient Descent(2/49): loss=120.91582449420524\n",
      "Gradient Descent(3/49): loss=5211.749963290318\n",
      "Gradient Descent(4/49): loss=292522.25381555734\n",
      "Gradient Descent(5/49): loss=26503962.95284211\n",
      "Gradient Descent(6/49): loss=3614419543.773757\n",
      "Gradient Descent(7/49): loss=583845547003.7504\n",
      "Gradient Descent(8/49): loss=98845580445243.83\n",
      "Gradient Descent(9/49): loss=1.6925712562860788e+16\n",
      "Gradient Descent(10/49): loss=2.9059358723341373e+18\n",
      "Gradient Descent(11/49): loss=4.9921857135151075e+20\n",
      "Gradient Descent(12/49): loss=8.577420740254843e+22\n",
      "Gradient Descent(13/49): loss=1.473794125897226e+25\n",
      "Gradient Descent(14/49): loss=2.5323293324792127e+27\n",
      "Gradient Descent(15/49): loss=4.3511524582002766e+29\n",
      "Gradient Descent(16/49): loss=7.47633217231446e+31\n",
      "Gradient Descent(17/49): loss=1.2846147870636846e+34\n",
      "Gradient Descent(18/49): loss=2.2072791150206562e+36\n",
      "Gradient Descent(19/49): loss=3.7926397582018304e+38\n",
      "Gradient Descent(20/49): loss=6.516673063137615e+40\n",
      "Gradient Descent(21/49): loss=1.1197221600338833e+43\n",
      "Gradient Descent(22/49): loss=1.923953685567111e+45\n",
      "Gradient Descent(23/49): loss=3.305818100577914e+47\n",
      "Gradient Descent(24/49): loss=5.680195628473248e+49\n",
      "Gradient Descent(25/49): loss=9.759950909612934e+51\n",
      "Gradient Descent(26/49): loss=1.676995793605722e+54\n",
      "Gradient Descent(27/49): loss=2.881484669150784e+56\n",
      "Gradient Descent(28/49): loss=4.9510880887177577e+58\n",
      "Gradient Descent(29/49): loss=8.507167685007081e+60\n",
      "Gradient Descent(30/49): loss=1.461737313576468e+63\n",
      "Gradient Descent(31/49): loss=2.511618499853326e+65\n",
      "Gradient Descent(32/49): loss=4.315568488411155e+67\n",
      "Gradient Descent(33/49): loss=7.41519119215547e+69\n",
      "Gradient Descent(34/49): loss=1.2741093221872003e+72\n",
      "Gradient Descent(35/49): loss=2.1892281976514507e+74\n",
      "Gradient Descent(36/49): loss=3.7616239187110935e+76\n",
      "Gradient Descent(37/49): loss=6.463380346096095e+78\n",
      "Gradient Descent(38/49): loss=1.110565181450023e+81\n",
      "Gradient Descent(39/49): loss=1.9082197800630464e+83\n",
      "Gradient Descent(40/49): loss=3.278783442741676e+85\n",
      "Gradient Descent(41/49): loss=5.633743542917185e+87\n",
      "Gradient Descent(42/49): loss=9.680134983487062e+89\n",
      "Gradient Descent(43/49): loss=1.6632814856532596e+92\n",
      "Gradient Descent(44/49): loss=2.857920168712773e+94\n",
      "Gradient Descent(45/49): loss=4.9105985734743705e+96\n",
      "Gradient Descent(46/49): loss=8.437596897840972e+98\n",
      "Gradient Descent(47/49): loss=1.449783368467964e+101\n",
      "Gradient Descent(48/49): loss=2.4910787288548485e+103\n",
      "Gradient Descent(49/49): loss=4.2802761904425307e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5432362715613386\n",
      "Gradient Descent(2/49): loss=123.35087204391263\n",
      "Gradient Descent(3/49): loss=5304.83939161084\n",
      "Gradient Descent(4/49): loss=285979.1403167144\n",
      "Gradient Descent(5/49): loss=23866025.539823145\n",
      "Gradient Descent(6/49): loss=3033336214.812608\n",
      "Gradient Descent(7/49): loss=468957329490.6991\n",
      "Gradient Descent(8/49): loss=76893529662956.25\n",
      "Gradient Descent(9/49): loss=1.2798164190864452e+16\n",
      "Gradient Descent(10/49): loss=2.1378909269296545e+18\n",
      "Gradient Descent(11/49): loss=3.574398562851029e+20\n",
      "Gradient Descent(12/49): loss=5.977386524446713e+22\n",
      "Gradient Descent(13/49): loss=9.996350441200356e+24\n",
      "Gradient Descent(14/49): loss=1.6717711110843458e+27\n",
      "Gradient Descent(15/49): loss=2.7958470351361923e+29\n",
      "Gradient Descent(16/49): loss=4.675739379238577e+31\n",
      "Gradient Descent(17/49): loss=7.819648952514384e+33\n",
      "Gradient Descent(18/49): loss=1.3077485118215736e+36\n",
      "Gradient Descent(19/49): loss=2.187062611989122e+38\n",
      "Gradient Descent(20/49): loss=3.6576167637080057e+40\n",
      "Gradient Descent(21/49): loss=6.116953544926283e+42\n",
      "Gradient Descent(22/49): loss=1.0229918302783152e+45\n",
      "Gradient Descent(23/49): loss=1.7108390265879939e+47\n",
      "Gradient Descent(24/49): loss=2.861186265898013e+49\n",
      "Gradient Descent(25/49): loss=4.7850129211188407e+51\n",
      "Gradient Descent(26/49): loss=8.002397092486312e+53\n",
      "Gradient Descent(27/49): loss=1.3383111034706513e+56\n",
      "Gradient Descent(28/49): loss=2.2381751229948823e+58\n",
      "Gradient Descent(29/49): loss=3.743096704646737e+60\n",
      "Gradient Descent(30/49): loss=6.259909153841966e+62\n",
      "Gradient Descent(31/49): loss=1.0468995515319469e+65\n",
      "Gradient Descent(32/49): loss=1.750822007257329e+67\n",
      "Gradient Descent(33/49): loss=2.9280533138169e+69\n",
      "Gradient Descent(34/49): loss=4.896840554331683e+71\n",
      "Gradient Descent(35/49): loss=8.189416258711926e+73\n",
      "Gradient Descent(36/49): loss=1.3695879601211197e+76\n",
      "Gradient Descent(37/49): loss=2.2904821555667503e+78\n",
      "Gradient Descent(38/49): loss=3.8305743462476466e+80\n",
      "Gradient Descent(39/49): loss=6.40620569187528e+82\n",
      "Gradient Descent(40/49): loss=1.0713660056439728e+85\n",
      "Gradient Descent(41/49): loss=1.79173940590958e+87\n",
      "Gradient Descent(42/49): loss=2.9964830709366053e+89\n",
      "Gradient Descent(43/49): loss=5.011281643298857e+91\n",
      "Gradient Descent(44/49): loss=8.380806136379837e+93\n",
      "Gradient Descent(45/49): loss=1.4015957692081583e+96\n",
      "Gradient Descent(46/49): loss=2.3440116240546163e+98\n",
      "Gradient Descent(47/49): loss=3.920096374725145e+100\n",
      "Gradient Descent(48/49): loss=6.555921237520003e+102\n",
      "Gradient Descent(49/49): loss=1.0964042504077473e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6104098646351974\n",
      "Gradient Descent(2/49): loss=125.34079299673782\n",
      "Gradient Descent(3/49): loss=5327.254758094206\n",
      "Gradient Descent(4/49): loss=279488.4675866404\n",
      "Gradient Descent(5/49): loss=22455011.953820597\n",
      "Gradient Descent(6/49): loss=2788024602.2860312\n",
      "Gradient Descent(7/49): loss=427767434891.195\n",
      "Gradient Descent(8/49): loss=70017152226645.93\n",
      "Gradient Descent(9/49): loss=1.1651139810228966e+16\n",
      "Gradient Descent(10/49): loss=1.94657274652592e+18\n",
      "Gradient Descent(11/49): loss=3.2552857036742826e+20\n",
      "Gradient Descent(12/49): loss=5.445113713523994e+22\n",
      "Gradient Descent(13/49): loss=9.108534383987306e+24\n",
      "Gradient Descent(14/49): loss=1.5236867973148894e+27\n",
      "Gradient Descent(15/49): loss=2.548849866755628e+29\n",
      "Gradient Descent(16/49): loss=4.263763698590648e+31\n",
      "Gradient Descent(17/49): loss=7.132504870336249e+33\n",
      "Gradient Descent(18/49): loss=1.1931390073327074e+36\n",
      "Gradient Descent(19/49): loss=1.995905689517211e+38\n",
      "Gradient Descent(20/49): loss=3.3387891155266115e+40\n",
      "Gradient Descent(21/49): loss=5.585190133420623e+42\n",
      "Gradient Descent(22/49): loss=9.34301261665355e+44\n",
      "Gradient Descent(23/49): loss=1.5629169763702286e+47\n",
      "Gradient Descent(24/49): loss=2.614477337525068e+49\n",
      "Gradient Descent(25/49): loss=4.373547572769931e+51\n",
      "Gradient Descent(26/49): loss=7.316153824227533e+53\n",
      "Gradient Descent(27/49): loss=1.2238601704717258e+56\n",
      "Gradient Descent(28/49): loss=2.0472966436367344e+58\n",
      "Gradient Descent(29/49): loss=3.424756886590104e+60\n",
      "Gradient Descent(30/49): loss=5.7289986620654925e+62\n",
      "Gradient Descent(31/49): loss=9.583578267544532e+64\n",
      "Gradient Descent(32/49): loss=1.60315925745111e+67\n",
      "Gradient Descent(33/49): loss=2.6817953931206436e+69\n",
      "Gradient Descent(34/49): loss=4.48615850055823e+71\n",
      "Gradient Descent(35/49): loss=7.504531532777395e+73\n",
      "Gradient Descent(36/49): loss=1.2553723529706521e+76\n",
      "Gradient Descent(37/49): loss=2.1000108237532856e+78\n",
      "Gradient Descent(38/49): loss=3.512938172841953e+80\n",
      "Gradient Descent(39/49): loss=5.876510000150414e+82\n",
      "Gradient Descent(40/49): loss=9.830338048315207e+84\n",
      "Gradient Descent(41/49): loss=1.6444377043803633e+87\n",
      "Gradient Descent(42/49): loss=2.750846766710265e+89\n",
      "Gradient Descent(43/49): loss=4.601668955755115e+91\n",
      "Gradient Descent(44/49): loss=7.697759626096529e+93\n",
      "Gradient Descent(45/49): loss=1.2876959170879684e+96\n",
      "Gradient Descent(46/49): loss=2.154082298521786e+98\n",
      "Gradient Descent(47/49): loss=3.603389967483942e+100\n",
      "Gradient Descent(48/49): loss=6.027819487990084e+102\n",
      "Gradient Descent(49/49): loss=1.0083451446462165e+105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.4833147050456845\n",
      "Gradient Descent(2/49): loss=118.9264435196824\n",
      "Gradient Descent(3/49): loss=4917.842366300557\n",
      "Gradient Descent(4/49): loss=239715.8038815723\n",
      "Gradient Descent(5/49): loss=17110745.570001647\n",
      "Gradient Descent(6/49): loss=1966495703.373317\n",
      "Gradient Descent(7/49): loss=296390668846.2022\n",
      "Gradient Descent(8/49): loss=48806924613429.43\n",
      "Gradient Descent(9/49): loss=8222364502163037.0\n",
      "Gradient Descent(10/49): loss=1.392798436208749e+18\n",
      "Gradient Descent(11/49): loss=2.3623289909987285e+20\n",
      "Gradient Descent(12/49): loss=4.007967366978288e+22\n",
      "Gradient Descent(13/49): loss=6.800469227302122e+24\n",
      "Gradient Descent(14/49): loss=1.153880500530178e+27\n",
      "Gradient Descent(15/49): loss=1.9578728777107734e+29\n",
      "Gradient Descent(16/49): loss=3.322068209725292e+31\n",
      "Gradient Descent(17/49): loss=5.636800885116198e+33\n",
      "Gradient Descent(18/49): loss=9.564380927834016e+35\n",
      "Gradient Descent(19/49): loss=1.6228599427765954e+38\n",
      "Gradient Descent(20/49): loss=2.753627674468204e+40\n",
      "Gradient Descent(21/49): loss=4.672285743635181e+42\n",
      "Gradient Descent(22/49): loss=7.92781619531766e+44\n",
      "Gradient Descent(23/49): loss=1.3451717869485514e+47\n",
      "Gradient Descent(24/49): loss=2.2824534422056526e+49\n",
      "Gradient Descent(25/49): loss=3.872809232540098e+51\n",
      "Gradient Descent(26/49): loss=6.571284686166869e+53\n",
      "Gradient Descent(27/49): loss=1.1149989538300409e+56\n",
      "Gradient Descent(28/49): loss=1.891902004579453e+58\n",
      "Gradient Descent(29/49): loss=3.2101314379146444e+60\n",
      "Gradient Descent(30/49): loss=5.446869776417757e+62\n",
      "Gradient Descent(31/49): loss=9.242110778032975e+64\n",
      "Gradient Descent(32/49): loss=1.568177965319546e+67\n",
      "Gradient Descent(33/49): loss=2.6608446814539258e+69\n",
      "Gradient Descent(34/49): loss=4.514853910333397e+71\n",
      "Gradient Descent(35/49): loss=7.660689845494712e+73\n",
      "Gradient Descent(36/49): loss=1.2998464640139967e+76\n",
      "Gradient Descent(37/49): loss=2.2055465814261434e+78\n",
      "Gradient Descent(38/49): loss=3.7423156176606345e+80\n",
      "Gradient Descent(39/49): loss=6.349866423193332e+82\n",
      "Gradient Descent(40/49): loss=1.0774292633715106e+85\n",
      "Gradient Descent(41/49): loss=1.828154704686671e+87\n",
      "Gradient Descent(42/49): loss=3.101966632881098e+89\n",
      "Gradient Descent(43/49): loss=5.263338472854545e+91\n",
      "Gradient Descent(44/49): loss=8.93069950726744e+93\n",
      "Gradient Descent(45/49): loss=1.515338489068344e+96\n",
      "Gradient Descent(46/49): loss=2.571187995501678e+98\n",
      "Gradient Descent(47/49): loss=4.3627267147926635e+100\n",
      "Gradient Descent(48/49): loss=7.402564270393679e+102\n",
      "Gradient Descent(49/49): loss=1.2560483697387298e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5468085119694\n",
      "Gradient Descent(2/49): loss=123.40856326501236\n",
      "Gradient Descent(3/49): loss=5372.7846415175445\n",
      "Gradient Descent(4/49): loss=304399.758544716\n",
      "Gradient Descent(5/49): loss=27820742.04008034\n",
      "Gradient Descent(6/49): loss=3827776853.8304935\n",
      "Gradient Descent(7/49): loss=624061155662.356\n",
      "Gradient Descent(8/49): loss=106655224658789.98\n",
      "Gradient Descent(9/49): loss=1.8436996128964656e+16\n",
      "Gradient Descent(10/49): loss=3.195609723973168e+18\n",
      "Gradient Descent(11/49): loss=5.5422303519552045e+20\n",
      "Gradient Descent(12/49): loss=9.613402029663392e+22\n",
      "Gradient Descent(13/49): loss=1.667569224170545e+25\n",
      "Gradient Descent(14/49): loss=2.8926368994413424e+27\n",
      "Gradient Descent(15/49): loss=5.0177004387330594e+29\n",
      "Gradient Descent(16/49): loss=8.703936477276033e+31\n",
      "Gradient Descent(17/49): loss=1.5098254301637163e+34\n",
      "Gradient Descent(18/49): loss=2.619013689197138e+36\n",
      "Gradient Descent(19/49): loss=4.5430634569102525e+38\n",
      "Gradient Descent(20/49): loss=7.880610048801962e+40\n",
      "Gradient Descent(21/49): loss=1.3670074245417809e+43\n",
      "Gradient Descent(22/49): loss=2.3712749232526642e+45\n",
      "Gradient Descent(23/49): loss=4.113324229830247e+47\n",
      "Gradient Descent(24/49): loss=7.135164317663817e+49\n",
      "Gradient Descent(25/49): loss=1.2376989265979312e+52\n",
      "Gradient Descent(26/49): loss=2.1469703635409973e+54\n",
      "Gradient Descent(27/49): loss=3.724235064656299e+56\n",
      "Gradient Descent(28/49): loss=6.460232079747889e+58\n",
      "Gradient Descent(29/49): loss=1.1206220284071837e+61\n",
      "Gradient Descent(30/49): loss=1.9438833079824727e+63\n",
      "Gradient Descent(31/49): loss=3.371950773111041e+65\n",
      "Gradient Descent(32/49): loss=5.849143294555469e+67\n",
      "Gradient Descent(33/49): loss=1.0146197136999556e+70\n",
      "Gradient Descent(34/49): loss=1.76000674216142e+72\n",
      "Gradient Descent(35/49): loss=3.0529898942705513e+74\n",
      "Gradient Descent(36/49): loss=5.29585885737679e+76\n",
      "Gradient Descent(37/49): loss=9.186444111685182e+78\n",
      "Gradient Descent(38/49): loss=1.5935234999619154e+81\n",
      "Gradient Descent(39/49): loss=2.764200286921509e+83\n",
      "Gradient Descent(40/49): loss=4.7949109168454284e+85\n",
      "Gradient Descent(41/49): loss=8.317476417777182e+87\n",
      "Gradient Descent(42/49): loss=1.4427883053517674e+90\n",
      "Gradient Descent(43/49): loss=2.502727978417476e+92\n",
      "Gradient Descent(44/49): loss=4.3413488387172446e+94\n",
      "Gradient Descent(45/49): loss=7.530706453902621e+96\n",
      "Gradient Descent(46/49): loss=1.3063115128893118e+99\n",
      "Gradient Descent(47/49): loss=2.2659889602029018e+101\n",
      "Gradient Descent(48/49): loss=3.930690281068022e+103\n",
      "Gradient Descent(49/49): loss=6.81835894041568e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5762029549805567\n",
      "Gradient Descent(2/49): loss=125.89235074140667\n",
      "Gradient Descent(3/49): loss=5468.83616688505\n",
      "Gradient Descent(4/49): loss=297627.08682247956\n",
      "Gradient Descent(5/49): loss=25056558.797289804\n",
      "Gradient Descent(6/49): loss=3212839114.577658\n",
      "Gradient Descent(7/49): loss=501306956447.17145\n",
      "Gradient Descent(8/49): loss=82975656707370.62\n",
      "Gradient Descent(9/49): loss=1.394209073760164e+16\n",
      "Gradient Descent(10/49): loss=2.351220458651719e+18\n",
      "Gradient Descent(11/49): loss=3.96863015115174e+20\n",
      "Gradient Descent(12/49): loss=6.70007156228004e+22\n",
      "Gradient Descent(13/49): loss=1.1312020551202514e+25\n",
      "Gradient Descent(14/49): loss=1.909880447789948e+27\n",
      "Gradient Descent(15/49): loss=3.224582095276249e+29\n",
      "Gradient Descent(16/49): loss=5.444286796627441e+31\n",
      "Gradient Descent(17/49): loss=9.191970548450337e+33\n",
      "Gradient Descent(18/49): loss=1.5519448016580595e+36\n",
      "Gradient Descent(19/49): loss=2.6202571882356098e+38\n",
      "Gradient Descent(20/49): loss=4.423963881220771e+40\n",
      "Gradient Descent(21/49): loss=7.469288328232016e+42\n",
      "Gradient Descent(22/49): loss=1.2610923062549647e+45\n",
      "Gradient Descent(23/49): loss=2.1291905400602356e+47\n",
      "Gradient Descent(24/49): loss=3.5948616397312034e+49\n",
      "Gradient Descent(25/49): loss=6.069456897205722e+51\n",
      "Gradient Descent(26/49): loss=1.0247489533368976e+54\n",
      "Gradient Descent(27/49): loss=1.7301554902690356e+56\n",
      "Gradient Descent(28/49): loss=2.921142793813649e+58\n",
      "Gradient Descent(29/49): loss=4.9319701436328756e+60\n",
      "Gradient Descent(30/49): loss=8.326990912323521e+62\n",
      "Gradient Descent(31/49): loss=1.4059042458607801e+65\n",
      "Gradient Descent(32/49): loss=2.373686688674206e+67\n",
      "Gradient Descent(33/49): loss=4.007661626015884e+69\n",
      "Gradient Descent(34/49): loss=6.766416050305031e+71\n",
      "Gradient Descent(35/49): loss=1.1424214526649124e+74\n",
      "Gradient Descent(36/49): loss=1.9288302194337994e+76\n",
      "Gradient Descent(37/49): loss=3.2565792656663273e+78\n",
      "Gradient Descent(38/49): loss=5.498311052323245e+80\n",
      "Gradient Descent(39/49): loss=9.283183967553377e+82\n",
      "Gradient Descent(40/49): loss=1.567345022050476e+85\n",
      "Gradient Descent(41/49): loss=2.646258467711787e+87\n",
      "Gradient Descent(42/49): loss=4.467863667168244e+89\n",
      "Gradient Descent(43/49): loss=7.543407415399829e+91\n",
      "Gradient Descent(44/49): loss=1.2736063513499388e+94\n",
      "Gradient Descent(45/49): loss=2.1503188796184996e+96\n",
      "Gradient Descent(46/49): loss=3.6305340964599446e+98\n",
      "Gradient Descent(47/49): loss=6.1296852064549664e+100\n",
      "Gradient Descent(48/49): loss=1.034917720973012e+103\n",
      "Gradient Descent(49/49): loss=1.747324133474414e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6440889803218806\n",
      "Gradient Descent(2/49): loss=127.92451688744522\n",
      "Gradient Descent(3/49): loss=5492.1017827326405\n",
      "Gradient Descent(4/49): loss=290893.5979683398\n",
      "Gradient Descent(5/49): loss=23577170.38844754\n",
      "Gradient Descent(6/49): loss=2953132373.419096\n",
      "Gradient Descent(7/49): loss=457281459475.69305\n",
      "Gradient Descent(8/49): loss=75555592202206.6\n",
      "Gradient Descent(9/49): loss=1.269254493748595e+16\n",
      "Gradient Descent(10/49): loss=2.1408102259638907e+18\n",
      "Gradient Descent(11/49): loss=3.614318068144792e+20\n",
      "Gradient Descent(12/49): loss=6.1034389385175795e+22\n",
      "Gradient Descent(13/49): loss=1.0307342817466458e+25\n",
      "Gradient Descent(14/49): loss=1.740702437598241e+27\n",
      "Gradient Descent(15/49): loss=2.9397047539899797e+29\n",
      "Gradient Descent(16/49): loss=4.964588039366998e+31\n",
      "Gradient Descent(17/49): loss=8.384222519392013e+33\n",
      "Gradient Descent(18/49): loss=1.4159319905518696e+36\n",
      "Gradient Descent(19/49): loss=2.391233555180842e+38\n",
      "Gradient Descent(20/49): loss=4.0383280888134167e+40\n",
      "Gradient Descent(21/49): loss=6.819950199765562e+42\n",
      "Gradient Descent(22/49): loss=1.1517568585512553e+45\n",
      "Gradient Descent(23/49): loss=1.9450931786998146e+47\n",
      "Gradient Descent(24/49): loss=3.2848838240157396e+49\n",
      "Gradient Descent(25/49): loss=5.5475294733764796e+51\n",
      "Gradient Descent(26/49): loss=9.368697618161965e+53\n",
      "Gradient Descent(27/49): loss=1.582190693745703e+56\n",
      "Gradient Descent(28/49): loss=2.6720121551608666e+58\n",
      "Gradient Descent(29/49): loss=4.5125085020093216e+60\n",
      "Gradient Descent(30/49): loss=7.620748633712762e+62\n",
      "Gradient Descent(31/49): loss=1.2869961289242094e+65\n",
      "Gradient Descent(32/49): loss=2.173485986059785e+67\n",
      "Gradient Descent(33/49): loss=3.6705948257568295e+69\n",
      "Gradient Descent(34/49): loss=6.198920288093368e+71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1.0468769930283275e+74\n",
      "Gradient Descent(36/49): loss=1.767971497612356e+76\n",
      "Gradient Descent(37/49): loss=2.985759776158529e+78\n",
      "Gradient Descent(38/49): loss=5.042367172188914e+80\n",
      "Gradient Descent(39/49): loss=8.515576806343304e+82\n",
      "Gradient Descent(40/49): loss=1.4381151921004125e+85\n",
      "Gradient Descent(41/49): loss=2.428696672912846e+87\n",
      "Gradient Descent(42/49): loss=4.1015960066473206e+89\n",
      "Gradient Descent(43/49): loss=6.926797400997966e+91\n",
      "Gradient Descent(44/49): loss=1.1698012714249e+94\n",
      "Gradient Descent(45/49): loss=1.9755666802527499e+96\n",
      "Gradient Descent(46/49): loss=3.3363476373818967e+98\n",
      "Gradient Descent(47/49): loss=5.634441838247644e+100\n",
      "Gradient Descent(48/49): loss=9.515475687511916e+102\n",
      "Gradient Descent(49/49): loss=1.606978653058383e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.5156732326571523\n",
      "Gradient Descent(2/49): loss=121.37972488386065\n",
      "Gradient Descent(3/49): loss=5070.369106622214\n",
      "Gradient Descent(4/49): loss=249551.09682281787\n",
      "Gradient Descent(5/49): loss=17970970.265971523\n",
      "Gradient Descent(6/49): loss=2083236167.9684443\n",
      "Gradient Descent(7/49): loss=316846435166.8103\n",
      "Gradient Descent(8/49): loss=52666334708381.99\n",
      "Gradient Descent(9/49): loss=8956941930658917.0\n",
      "Gradient Descent(10/49): loss=1.531705415820942e+18\n",
      "Gradient Descent(11/49): loss=2.62273667048861e+20\n",
      "Gradient Descent(12/49): loss=4.492278877032673e+22\n",
      "Gradient Descent(13/49): loss=7.695022026741459e+24\n",
      "Gradient Descent(14/49): loss=1.3181363505962284e+27\n",
      "Gradient Descent(15/49): loss=2.2579406536802296e+29\n",
      "Gradient Descent(16/49): loss=3.8678098246146766e+31\n",
      "Gradient Descent(17/49): loss=6.6254868455957e+33\n",
      "Gradient Descent(18/49): loss=1.1349337267708637e+36\n",
      "Gradient Descent(19/49): loss=1.9441206505301907e+38\n",
      "Gradient Descent(20/49): loss=3.330243013599578e+40\n",
      "Gradient Descent(21/49): loss=5.704645199847933e+42\n",
      "Gradient Descent(22/49): loss=9.771952595745351e+44\n",
      "Gradient Descent(23/49): loss=1.6739175565121271e+47\n",
      "Gradient Descent(24/49): loss=2.867390072321382e+49\n",
      "Gradient Descent(25/49): loss=4.911786601961e+51\n",
      "Gradient Descent(26/49): loss=8.41380036015935e+53\n",
      "Gradient Descent(27/49): loss=1.4412685696150316e+56\n",
      "Gradient Descent(28/49): loss=2.4688666248801935e+58\n",
      "Gradient Descent(29/49): loss=4.2291232459716395e+60\n",
      "Gradient Descent(30/49): loss=7.244410552346516e+62\n",
      "Gradient Descent(31/49): loss=1.2409542403603377e+65\n",
      "Gradient Descent(32/49): loss=2.125731852910352e+67\n",
      "Gradient Descent(33/49): loss=3.6413396751564e+69\n",
      "Gradient Descent(34/49): loss=6.237548076308133e+71\n",
      "Gradient Descent(35/49): loss=1.0684805449407886e+74\n",
      "Gradient Descent(36/49): loss=1.8302875760641433e+76\n",
      "Gradient Descent(37/49): loss=3.135249047777845e+78\n",
      "Gradient Descent(38/49): loss=5.370624114014841e+80\n",
      "Gradient Descent(39/49): loss=9.199780602590708e+82\n",
      "Gradient Descent(40/49): loss=1.5759055435464775e+85\n",
      "Gradient Descent(41/49): loss=2.6994972917953235e+87\n",
      "Gradient Descent(42/49): loss=4.624189348310043e+89\n",
      "Gradient Descent(43/49): loss=7.92115153959027e+91\n",
      "Gradient Descent(44/49): loss=1.3568787302380711e+94\n",
      "Gradient Descent(45/49): loss=2.324308377854483e+96\n",
      "Gradient Descent(46/49): loss=3.9814976202160004e+98\n",
      "Gradient Descent(47/49): loss=6.820232397225433e+100\n",
      "Gradient Descent(48/49): loss=1.1682933003898296e+103\n",
      "Gradient Descent(49/49): loss=2.0012649954435842e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.579657551496697\n",
      "Gradient Descent(2/49): loss=125.93978316626767\n",
      "Gradient Descent(3/49): loss=5537.947613816607\n",
      "Gradient Descent(4/49): loss=316697.68566656206\n",
      "Gradient Descent(5/49): loss=29196177.54092015\n",
      "Gradient Descent(6/49): loss=4052627819.1018\n",
      "Gradient Descent(7/49): loss=666836756559.2942\n",
      "Gradient Descent(8/49): loss=115040526445978.73\n",
      "Gradient Descent(9/49): loss=2.0075100926535012e+16\n",
      "Gradient Descent(10/49): loss=3.5125806613157294e+18\n",
      "Gradient Descent(11/49): loss=6.149839251365976e+20\n",
      "Gradient Descent(12/49): loss=1.0768701407185364e+23\n",
      "Gradient Descent(13/49): loss=1.885720174612429e+25\n",
      "Gradient Descent(14/49): loss=3.302132311058548e+27\n",
      "Gradient Descent(15/49): loss=5.782457619056407e+29\n",
      "Gradient Descent(16/49): loss=1.0125829782815617e+32\n",
      "Gradient Descent(17/49): loss=1.7731636816567383e+34\n",
      "Gradient Descent(18/49): loss=3.1050388733259564e+36\n",
      "Gradient Descent(19/49): loss=5.437324570090653e+38\n",
      "Gradient Descent(20/49): loss=9.521458416707146e+40\n",
      "Gradient Descent(21/49): loss=1.667330490144485e+43\n",
      "Gradient Descent(22/49): loss=2.919711289878041e+45\n",
      "Gradient Descent(23/49): loss=5.112792014991447e+47\n",
      "Gradient Descent(24/49): loss=8.953159950872184e+49\n",
      "Gradient Descent(25/49): loss=1.5678140802701294e+52\n",
      "Gradient Descent(26/49): loss=2.745445187823571e+54\n",
      "Gradient Descent(27/49): loss=4.807629536051462e+56\n",
      "Gradient Descent(28/49): loss=8.418780989846514e+58\n",
      "Gradient Descent(29/49): loss=1.4742374141667277e+61\n",
      "Gradient Descent(30/49): loss=2.5815803451238072e+63\n",
      "Gradient Descent(31/49): loss=4.520681007201623e+65\n",
      "Gradient Descent(32/49): loss=7.916297010656714e+67\n",
      "Gradient Descent(33/49): loss=1.3862459718148412e+70\n",
      "Gradient Descent(34/49): loss=2.427495951435414e+72\n",
      "Gradient Descent(35/49): loss=4.2508593092758224e+74\n",
      "Gradient Descent(36/49): loss=7.443804327077079e+76\n",
      "Gradient Descent(37/49): loss=1.3035063931403078e+79\n",
      "Gradient Descent(38/49): loss=2.2826082501618967e+81\n",
      "Gradient Descent(39/49): loss=3.997142208988262e+83\n",
      "Gradient Descent(40/49): loss=6.999512876439493e+85\n",
      "Gradient Descent(41/49): loss=1.2257052150227006e+88\n",
      "Gradient Descent(42/49): loss=2.1463683268456867e+90\n",
      "Gradient Descent(43/49): loss=3.75856848614373e+92\n",
      "Gradient Descent(44/49): loss=6.581739437887592e+94\n",
      "Gradient Descent(45/49): loss=1.1525476837244014e+97\n",
      "Gradient Descent(46/49): loss=2.0182600295778492e+99\n",
      "Gradient Descent(47/49): loss=3.5342342920066776e+101\n",
      "Gradient Descent(48/49): loss=6.1889012554088466e+103\n",
      "Gradient Descent(49/49): loss=1.083756638201076e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.60934145369669\n",
      "Gradient Descent(2/49): loss=128.47303684357703\n",
      "Gradient Descent(3/49): loss=5637.038952383517\n",
      "Gradient Descent(4/49): loss=309688.5512074542\n",
      "Gradient Descent(5/49): loss=26300341.59499776\n",
      "Gradient Descent(6/49): loss=3402036702.563972\n",
      "Gradient Descent(7/49): loss=535718937963.7782\n",
      "Gradient Descent(8/49): loss=89506613748448.14\n",
      "Gradient Descent(9/49): loss=1.5182110899316944e+16\n",
      "Gradient Descent(10/49): loss=2.5846732475921915e+18\n",
      "Gradient Descent(11/49): loss=4.404161313956988e+20\n",
      "Gradient Descent(12/49): loss=7.506075554591428e+22\n",
      "Gradient Descent(13/49): loss=1.2793363402822657e+25\n",
      "Gradient Descent(14/49): loss=2.180528847623862e+27\n",
      "Gradient Descent(15/49): loss=3.71655193539101e+29\n",
      "Gradient Descent(16/49): loss=6.334595380293952e+31\n",
      "Gradient Descent(17/49): loss=1.079686388553023e+34\n",
      "Gradient Descent(18/49): loss=1.8402481525037166e+36\n",
      "Gradient Descent(19/49): loss=3.136571259365508e+38\n",
      "Gradient Descent(20/49): loss=5.346061222472578e+40\n",
      "Gradient Descent(21/49): loss=9.111978733123723e+42\n",
      "Gradient Descent(22/49): loss=1.5530715603277036e+45\n",
      "Gradient Descent(23/49): loss=2.647099320817482e+47\n",
      "Gradient Descent(24/49): loss=4.511791338737444e+49\n",
      "Gradient Descent(25/49): loss=7.690025426796945e+51\n",
      "Gradient Descent(26/49): loss=1.3107097962858664e+54\n",
      "Gradient Descent(27/49): loss=2.2340110399289927e+56\n",
      "Gradient Descent(28/49): loss=3.8077119288092817e+58\n",
      "Gradient Descent(29/49): loss=6.489972463724853e+60\n",
      "Gradient Descent(30/49): loss=1.1061693575405984e+63\n",
      "Gradient Descent(31/49): loss=1.885386501100038e+65\n",
      "Gradient Descent(32/49): loss=3.213506353524007e+67\n",
      "Gradient Descent(33/49): loss=5.477191588098182e+69\n",
      "Gradient Descent(34/49): loss=9.335481057890502e+71\n",
      "Gradient Descent(35/49): loss=1.5911659320373038e+74\n",
      "Gradient Descent(36/49): loss=2.7120284509989885e+76\n",
      "Gradient Descent(37/49): loss=4.62245839414786e+78\n",
      "Gradient Descent(38/49): loss=7.878649502278639e+80\n",
      "Gradient Descent(39/49): loss=1.342859419964498e+83\n",
      "Gradient Descent(40/49): loss=2.288807772532416e+85\n",
      "Gradient Descent(41/49): loss=3.901109037715376e+87\n",
      "Gradient Descent(42/49): loss=6.649161151399865e+89\n",
      "Gradient Descent(43/49): loss=1.133301930037238e+92\n",
      "Gradient Descent(44/49): loss=1.9316320290352817e+94\n",
      "Gradient Descent(45/49): loss=3.2923285460851493e+96\n",
      "Gradient Descent(46/49): loss=5.611538373993943e+98\n",
      "Gradient Descent(47/49): loss=9.564465539215458e+100\n",
      "Gradient Descent(48/49): loss=1.6301946980312544e+103\n",
      "Gradient Descent(49/49): loss=2.7785501893367566e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.677943569490323\n",
      "Gradient Descent(2/49): loss=130.54811615726084\n",
      "Gradient Descent(3/49): loss=5661.180473055998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=302704.37280786555\n",
      "Gradient Descent(5/49): loss=24749612.267162185\n",
      "Gradient Descent(6/49): loss=3127164324.6299515\n",
      "Gradient Descent(7/49): loss=488677457538.0354\n",
      "Gradient Descent(8/49): loss=81502758651211.86\n",
      "Gradient Descent(9/49): loss=1.3821431790017068e+16\n",
      "Gradient Descent(10/49): loss=2.3533698961488205e+18\n",
      "Gradient Descent(11/49): loss=4.0109622088237213e+20\n",
      "Gradient Descent(12/49): loss=6.837662295875328e+22\n",
      "Gradient Descent(13/49): loss=1.1657106984988837e+25\n",
      "Gradient Descent(14/49): loss=1.9873742706648192e+27\n",
      "Gradient Descent(15/49): loss=3.388206849299466e+29\n",
      "Gradient Descent(16/49): loss=5.776443062400922e+31\n",
      "Gradient Descent(17/49): loss=9.848070661628804e+33\n",
      "Gradient Descent(18/49): loss=1.678965738679487e+36\n",
      "Gradient Descent(19/49): loss=2.8624144540617172e+38\n",
      "Gradient Descent(20/49): loss=4.880037953239293e+40\n",
      "Gradient Descent(21/49): loss=8.319819097207711e+42\n",
      "Gradient Descent(22/49): loss=1.4184190877862198e+45\n",
      "Gradient Descent(23/49): loss=2.4182168928876215e+47\n",
      "Gradient Descent(24/49): loss=4.1227398809336267e+49\n",
      "Gradient Descent(25/49): loss=7.028726073274933e+51\n",
      "Gradient Descent(26/49): loss=1.1983048079659329e+54\n",
      "Gradient Descent(27/49): loss=2.0429511661497436e+56\n",
      "Gradient Descent(28/49): loss=3.4829614631667676e+58\n",
      "Gradient Descent(29/49): loss=5.937988511378629e+60\n",
      "Gradient Descent(30/49): loss=1.0123484837298834e+63\n",
      "Gradient Descent(31/49): loss=1.7259202346827597e+65\n",
      "Gradient Descent(32/49): loss=2.9424656670717664e+67\n",
      "Gradient Descent(33/49): loss=5.016514684693761e+69\n",
      "Gradient Descent(34/49): loss=8.552493870486233e+71\n",
      "Gradient Descent(35/49): loss=1.458087058488733e+74\n",
      "Gradient Descent(36/49): loss=2.4858455350304152e+76\n",
      "Gradient Descent(37/49): loss=4.238037768770465e+78\n",
      "Gradient Descent(38/49): loss=7.225293718543618e+80\n",
      "Gradient Descent(39/49): loss=1.231816990964926e+83\n",
      "Gradient Descent(40/49): loss=2.1000850046214125e+85\n",
      "Gradient Descent(41/49): loss=3.5803670991588974e+87\n",
      "Gradient Descent(42/49): loss=6.104052234328598e+89\n",
      "Gradient Descent(43/49): loss=1.0406601515292886e+92\n",
      "Gradient Descent(44/49): loss=1.7741878827484926e+94\n",
      "Gradient Descent(45/49): loss=3.0247556213869436e+96\n",
      "Gradient Descent(46/49): loss=5.156808170134997e+98\n",
      "Gradient Descent(47/49): loss=8.791675702838418e+100\n",
      "Gradient Descent(48/49): loss=1.49886439661482e+103\n",
      "Gradient Descent(49/49): loss=2.5553655018396645e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.548200553396966\n",
      "Gradient Descent(2/49): loss=123.87090195989404\n",
      "Gradient Descent(3/49): loss=5226.819747904793\n",
      "Gradient Descent(4/49): loss=259738.0351561973\n",
      "Gradient Descent(5/49): loss=18869976.407666128\n",
      "Gradient Descent(6/49): loss=2206302919.078133\n",
      "Gradient Descent(7/49): loss=338607041159.82166\n",
      "Gradient Descent(8/49): loss=56810472767508.625\n",
      "Gradient Descent(9/49): loss=9753196942408674.0\n",
      "Gradient Descent(10/49): loss=1.683708535080584e+18\n",
      "Gradient Descent(11/49): loss=2.9104098936477263e+20\n",
      "Gradient Descent(12/49): loss=5.032396736746344e+22\n",
      "Gradient Descent(13/49): loss=8.702157814662315e+24\n",
      "Gradient Descent(14/49): loss=1.5048264246848679e+27\n",
      "Gradient Descent(15/49): loss=2.6022414749440796e+29\n",
      "Gradient Descent(16/49): loss=4.499965529255902e+31\n",
      "Gradient Descent(17/49): loss=7.781635339091863e+33\n",
      "Gradient Descent(18/49): loss=1.345651456416066e+36\n",
      "Gradient Descent(19/49): loss=2.3269888465011313e+38\n",
      "Gradient Descent(20/49): loss=4.023981902136581e+40\n",
      "Gradient Descent(21/49): loss=6.958533722210551e+42\n",
      "Gradient Descent(22/49): loss=1.2033153415790463e+45\n",
      "Gradient Descent(23/49): loss=2.0808518994618506e+47\n",
      "Gradient Descent(24/49): loss=3.598345735249842e+49\n",
      "Gradient Descent(25/49): loss=6.222495716185217e+51\n",
      "Gradient Descent(26/49): loss=1.0760348167398623e+54\n",
      "Gradient Descent(27/49): loss=1.860750058573537e+56\n",
      "Gradient Descent(28/49): loss=3.2177311798996503e+58\n",
      "Gradient Descent(29/49): loss=5.564312035564832e+60\n",
      "Gradient Descent(30/49): loss=9.622173729906212e+62\n",
      "Gradient Descent(31/49): loss=1.663929461481061e+65\n",
      "Gradient Descent(32/49): loss=2.8773760799802036e+67\n",
      "Gradient Descent(33/49): loss=4.975747648745214e+69\n",
      "Gradient Descent(34/49): loss=8.60438954652184e+71\n",
      "Gradient Descent(35/49): loss=1.48792753762268e+74\n",
      "Gradient Descent(36/49): loss=2.573021996791017e+76\n",
      "Gradient Descent(37/49): loss=4.44943858391661e+78\n",
      "Gradient Descent(38/49): loss=7.694261353667763e+80\n",
      "Gradient Descent(39/49): loss=1.330542194526284e+83\n",
      "Gradient Descent(40/49): loss=2.300860927437737e+85\n",
      "Gradient Descent(41/49): loss=3.978799792436858e+87\n",
      "Gradient Descent(42/49): loss=6.880401852851343e+89\n",
      "Gradient Descent(43/49): loss=1.1898042657664256e+92\n",
      "Gradient Descent(44/49): loss=2.0574876600403065e+94\n",
      "Gradient Descent(45/49): loss=3.55794275833362e+96\n",
      "Gradient Descent(46/49): loss=6.152628235607828e+98\n",
      "Gradient Descent(47/49): loss=1.0639528732420606e+101\n",
      "Gradient Descent(48/49): loss=1.8398571685652912e+103\n",
      "Gradient Descent(49/49): loss=3.181601822650418e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6126770030906425\n",
      "Gradient Descent(2/49): loss=128.509878426476\n",
      "Gradient Descent(3/49): loss=5707.323170396076\n",
      "Gradient Descent(4/49): loss=329428.74318960967\n",
      "Gradient Descent(5/49): loss=30632583.345231317\n",
      "Gradient Descent(6/49): loss=4289532849.689547\n",
      "Gradient Descent(7/49): loss=712322064863.8822\n",
      "Gradient Descent(8/49): loss=124040898191378.11\n",
      "Gradient Descent(9/49): loss=2.1849994545079004e+16\n",
      "Gradient Descent(10/49): loss=3.859273689956238e+18\n",
      "Gradient Descent(11/49): loss=6.820721678110251e+20\n",
      "Gradient Descent(12/49): loss=1.2056398899940346e+23\n",
      "Gradient Descent(13/49): loss=2.1311760998159833e+25\n",
      "Gradient Descent(14/49): loss=3.767249669759101e+27\n",
      "Gradient Descent(15/49): loss=6.659325466285897e+29\n",
      "Gradient Descent(16/49): loss=1.1771620610575967e+32\n",
      "Gradient Descent(17/49): loss=2.080857372616827e+34\n",
      "Gradient Descent(18/49): loss=3.6783104410321673e+36\n",
      "Gradient Descent(19/49): loss=6.5021120366491076e+38\n",
      "Gradient Descent(20/49): loss=1.1493717472817915e+41\n",
      "Gradient Descent(21/49): loss=2.0317327759966952e+43\n",
      "Gradient Descent(22/49): loss=3.5914734141316656e+45\n",
      "Gradient Descent(23/49): loss=6.348611115097612e+47\n",
      "Gradient Descent(24/49): loss=1.1222375455233513e+50\n",
      "Gradient Descent(25/49): loss=1.9837679230158116e+52\n",
      "Gradient Descent(26/49): loss=3.506686430235165e+54\n",
      "Gradient Descent(27/49): loss=6.198734023938573e+56\n",
      "Gradient Descent(28/49): loss=1.095743924185374e+59\n",
      "Gradient Descent(29/49): loss=1.9369354173810338e+61\n",
      "Gradient Descent(30/49): loss=3.423901085186663e+63\n",
      "Gradient Descent(31/49): loss=6.052395209435346e+65\n",
      "Gradient Descent(32/49): loss=1.0698757604207839e+68\n",
      "Gradient Descent(33/49): loss=1.8912085267523214e+70\n",
      "Gradient Descent(34/49): loss=3.343070124566639e+72\n",
      "Gradient Descent(35/49): loss=5.90951114045703e+74\n",
      "Gradient Descent(36/49): loss=1.0446182885174162e+77\n",
      "Gradient Descent(37/49): loss=1.84656114993073e+79\n",
      "Gradient Descent(38/49): loss=3.2641474095507684e+81\n",
      "Gradient Descent(39/49): loss=5.770000257872042e+83\n",
      "Gradient Descent(40/49): loss=1.0199570913504054e+86\n",
      "Gradient Descent(41/49): loss=1.8029678019106924e+88\n",
      "Gradient Descent(42/49): loss=3.1870878905531255e+90\n",
      "Gradient Descent(43/49): loss=5.633782927984676e+92\n",
      "Gradient Descent(44/49): loss=9.958780921521125e+94\n",
      "Gradient Descent(45/49): loss=1.760403599333044e+97\n",
      "Gradient Descent(46/49): loss=3.1118475815124563e+99\n",
      "Gradient Descent(47/49): loss=5.5007813970804963e+101\n",
      "Gradient Descent(48/49): loss=9.72367546477349e+103\n",
      "Gradient Descent(49/49): loss=1.718844246281409e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.642651767709739\n",
      "Gradient Descent(2/49): loss=131.09333174407837\n",
      "Gradient Descent(3/49): loss=5809.533653472307\n",
      "Gradient Descent(4/49): loss=322176.06791179953\n",
      "Gradient Descent(5/49): loss=27599483.160382356\n",
      "Gradient Descent(6/49): loss=3601402919.223254\n",
      "Gradient Descent(7/49): loss=572314069883.7126\n",
      "Gradient Descent(8/49): loss=96517164994118.78\n",
      "Gradient Descent(9/49): loss=1.6525785029572222e+16\n",
      "Gradient Descent(10/49): loss=2.840038800037769e+18\n",
      "Gradient Descent(11/49): loss=4.885092298595727e+20\n",
      "Gradient Descent(12/49): loss=8.404541410774137e+22\n",
      "Gradient Descent(13/49): loss=1.4460307231702623e+25\n",
      "Gradient Descent(14/49): loss=2.4879769742378657e+27\n",
      "Gradient Descent(15/49): loss=4.280716566308601e+29\n",
      "Gradient Descent(16/49): loss=7.365239898902163e+31\n",
      "Gradient Descent(17/49): loss=1.2672356865620799e+34\n",
      "Gradient Descent(18/49): loss=2.180358511968344e+36\n",
      "Gradient Descent(19/49): loss=3.7514436639630044e+38\n",
      "Gradient Descent(20/49): loss=6.454594287855203e+40\n",
      "Gradient Descent(21/49): loss=1.110553460909271e+43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=1.910776936087539e+45\n",
      "Gradient Descent(23/49): loss=3.28761165321359e+47\n",
      "Gradient Descent(24/49): loss=5.656542204534192e+49\n",
      "Gradient Descent(25/49): loss=9.732435909958916e+51\n",
      "Gradient Descent(26/49): loss=1.674526686384069e+54\n",
      "Gradient Descent(27/49): loss=2.881128269792566e+56\n",
      "Gradient Descent(28/49): loss=4.957162029422707e+58\n",
      "Gradient Descent(29/49): loss=8.529108420333914e+60\n",
      "Gradient Descent(30/49): loss=1.4674866388073954e+63\n",
      "Gradient Descent(31/49): loss=2.5249028725488994e+65\n",
      "Gradient Descent(32/49): loss=4.344253874084189e+67\n",
      "Gradient Descent(33/49): loss=7.474561468355928e+69\n",
      "Gradient Descent(34/49): loss=1.2860452165910477e+72\n",
      "Gradient Descent(35/49): loss=2.2127215169996175e+74\n",
      "Gradient Descent(36/49): loss=3.80712625701561e+76\n",
      "Gradient Descent(37/49): loss=6.5503996890269136e+78\n",
      "Gradient Descent(38/49): loss=1.1270373817242413e+81\n",
      "Gradient Descent(39/49): loss=1.9391385565855132e+83\n",
      "Gradient Descent(40/49): loss=3.336409601502142e+85\n",
      "Gradient Descent(41/49): loss=5.740502137504152e+87\n",
      "Gradient Descent(42/49): loss=9.876894244595538e+89\n",
      "Gradient Descent(43/49): loss=1.6993816495874767e+92\n",
      "Gradient Descent(44/49): loss=2.9238927940682858e+94\n",
      "Gradient Descent(45/49): loss=5.030741077661714e+96\n",
      "Gradient Descent(46/49): loss=8.655705791202679e+98\n",
      "Gradient Descent(47/49): loss=1.4892685110855252e+101\n",
      "Gradient Descent(48/49): loss=2.562379950997292e+103\n",
      "Gradient Descent(49/49): loss=4.4087355398974167e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7119736321405203\n",
      "Gradient Descent(2/49): loss=133.21199919704688\n",
      "Gradient Descent(3/49): loss=5834.577333483122\n",
      "Gradient Descent(4/49): loss=314933.10994816164\n",
      "Gradient Descent(5/49): loss=25974333.056549013\n",
      "Gradient Descent(6/49): loss=3310557088.0354314\n",
      "Gradient Descent(7/49): loss=522065684454.9772\n",
      "Gradient Descent(8/49): loss=87886668292611.83\n",
      "Gradient Descent(9/49): loss=1.50446831903644e+16\n",
      "Gradient Descent(10/49): loss=2.5858810888764365e+18\n",
      "Gradient Descent(11/49): loss=4.448952212856677e+20\n",
      "Gradient Descent(12/49): loss=7.656112300437864e+22\n",
      "Gradient Descent(13/49): loss=1.3175984685406196e+25\n",
      "Gradient Descent(14/49): loss=2.267585439356872e+27\n",
      "Gradient Descent(15/49): loss=3.9025243560599666e+29\n",
      "Gradient Descent(16/49): loss=6.716266425069427e+31\n",
      "Gradient Descent(17/49): loss=1.1558734498621963e+34\n",
      "Gradient Descent(18/49): loss=1.989265174552391e+36\n",
      "Gradient Descent(19/49): loss=3.4235373918479385e+38\n",
      "Gradient Descent(20/49): loss=5.891928563599582e+40\n",
      "Gradient Descent(21/49): loss=1.014004470985134e+43\n",
      "Gradient Descent(22/49): loss=1.745107830526216e+45\n",
      "Gradient Descent(23/49): loss=3.003341136463232e+47\n",
      "Gradient Descent(24/49): loss=5.168768269938833e+49\n",
      "Gradient Descent(25/49): loss=8.895481470293152e+51\n",
      "Gradient Descent(26/49): loss=1.5309177439541456e+54\n",
      "Gradient Descent(27/49): loss=2.6347187013775684e+56\n",
      "Gradient Descent(28/49): loss=4.534366828526869e+58\n",
      "Gradient Descent(29/49): loss=7.803672750679258e+60\n",
      "Gradient Descent(30/49): loss=1.3430168026232809e+63\n",
      "Gradient Descent(31/49): loss=2.311340044303503e+65\n",
      "Gradient Descent(32/49): loss=3.9778302028432444e+67\n",
      "Gradient Descent(33/49): loss=6.845869850111327e+69\n",
      "Gradient Descent(34/49): loss=1.178178343840936e+72\n",
      "Gradient Descent(35/49): loss=2.027652059253186e+74\n",
      "Gradient Descent(36/49): loss=3.4896014638923675e+76\n",
      "Gradient Descent(37/49): loss=6.005625235961281e+78\n",
      "Gradient Descent(38/49): loss=1.0335717372890041e+81\n",
      "Gradient Descent(39/49): loss=1.7787832143203285e+83\n",
      "Gradient Descent(40/49): loss=3.0612966757846116e+85\n",
      "Gradient Descent(41/49): loss=5.268510103830026e+87\n",
      "Gradient Descent(42/49): loss=9.067137769992531e+89\n",
      "Gradient Descent(43/49): loss=1.560459897006889e+92\n",
      "Gradient Descent(44/49): loss=2.685560925549653e+94\n",
      "Gradient Descent(45/49): loss=4.621866603988232e+96\n",
      "Gradient Descent(46/49): loss=7.954260393735077e+98\n",
      "Gradient Descent(47/49): loss=1.3689330271182383e+101\n",
      "Gradient Descent(48/49): loss=2.355942023486017e+103\n",
      "Gradient Descent(49/49): loss=4.054590478916153e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.580896667265121\n",
      "Gradient Descent(2/49): loss=126.40036320000095\n",
      "Gradient Descent(3/49): loss=5387.274667862135\n",
      "Gradient Descent(4/49): loss=270287.3509481223\n",
      "Gradient Descent(5/49): loss=19809312.515168898\n",
      "Gradient Descent(6/49): loss=2336006336.4197903\n",
      "Gradient Descent(7/49): loss=361748963145.8485\n",
      "Gradient Descent(8/49): loss=61258850095948.375\n",
      "Gradient Descent(9/49): loss=1.0615979868638472e+16\n",
      "Gradient Descent(10/49): loss=1.8499717907325187e+18\n",
      "Gradient Descent(11/49): loss=3.228054151534038e+20\n",
      "Gradient Descent(12/49): loss=5.634441482362484e+22\n",
      "Gradient Descent(13/49): loss=9.835411473058596e+24\n",
      "Gradient Descent(14/49): loss=1.7168866549709566e+27\n",
      "Gradient Descent(15/49): loss=2.997039475107617e+29\n",
      "Gradient Descent(16/49): loss=5.231710599583655e+31\n",
      "Gradient Descent(17/49): loss=9.132613071010989e+33\n",
      "Gradient Descent(18/49): loss=1.5942132936882422e+36\n",
      "Gradient Descent(19/49): loss=2.782901276180391e+38\n",
      "Gradient Descent(20/49): loss=4.857906759412054e+40\n",
      "Gradient Descent(21/49): loss=8.480091730588423e+42\n",
      "Gradient Descent(22/49): loss=1.4803074519978735e+45\n",
      "Gradient Descent(23/49): loss=2.5840642084312065e+47\n",
      "Gradient Descent(24/49): loss=4.5108114698351297e+49\n",
      "Gradient Descent(25/49): loss=7.874192928352658e+51\n",
      "Gradient Descent(26/49): loss=1.3745401395641628e+54\n",
      "Gradient Descent(27/49): loss=2.399433964172078e+56\n",
      "Gradient Descent(28/49): loss=4.1885159863342937e+58\n",
      "Gradient Descent(29/49): loss=7.31158532793041e+60\n",
      "Gradient Descent(30/49): loss=1.2763298548227477e+63\n",
      "Gradient Descent(31/49): loss=2.2279954691753557e+65\n",
      "Gradient Descent(32/49): loss=3.889248372518355e+67\n",
      "Gradient Descent(33/49): loss=6.7891757916076265e+69\n",
      "Gradient Descent(34/49): loss=1.185136651468322e+72\n",
      "Gradient Descent(35/49): loss=2.0688061787850742e+74\n",
      "Gradient Descent(36/49): loss=3.611363297284435e+76\n",
      "Gradient Descent(37/49): loss=6.3040921854902114e+78\n",
      "Gradient Descent(38/49): loss=1.100459162140889e+81\n",
      "Gradient Descent(39/49): loss=1.9209908927523246e+83\n",
      "Gradient Descent(40/49): loss=3.353332987712379e+85\n",
      "Gradient Descent(41/49): loss=5.853667588381597e+87\n",
      "Gradient Descent(42/49): loss=1.0218318419562706e+90\n",
      "Gradient Descent(43/49): loss=1.783736943498712e+92\n",
      "Gradient Descent(44/49): loss=3.113738829581671e+94\n",
      "Gradient Descent(45/49): loss=5.435425629424601e+96\n",
      "Gradient Descent(46/49): loss=9.488224090064331e+98\n",
      "Gradient Descent(47/49): loss=1.6562897281846944e+101\n",
      "Gradient Descent(48/49): loss=2.891263568029343e+103\n",
      "Gradient Descent(49/49): loss=5.047066873363817e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.645866866751237\n",
      "Gradient Descent(2/49): loss=131.11924528478622\n",
      "Gradient Descent(3/49): loss=5880.996886487483\n",
      "Gradient Descent(4/49): loss=342605.9580243778\n",
      "Gradient Descent(5/49): loss=32132352.917230602\n",
      "Gradient Descent(6/49): loss=4539076841.02177\n",
      "Gradient Descent(7/49): loss=760674784204.1132\n",
      "Gradient Descent(8/49): loss=133698245876435.02\n",
      "Gradient Descent(9/49): loss=2.3772375462629028e+16\n",
      "Gradient Descent(10/49): loss=4.2383162882226616e+18\n",
      "Gradient Descent(11/49): loss=7.561122206073692e+20\n",
      "Gradient Descent(12/49): loss=1.3490936516559185e+23\n",
      "Gradient Descent(13/49): loss=2.4072017727037926e+25\n",
      "Gradient Descent(14/49): loss=4.29522832956492e+27\n",
      "Gradient Descent(15/49): loss=7.664093496198832e+29\n",
      "Gradient Descent(16/49): loss=1.367525750350439e+32\n",
      "Gradient Descent(17/49): loss=2.4401148783523004e+34\n",
      "Gradient Descent(18/49): loss=4.353966095203638e+36\n",
      "Gradient Descent(19/49): loss=7.76890507220183e+38\n",
      "Gradient Descent(20/49): loss=1.386227746669206e+41\n",
      "Gradient Descent(21/49): loss=2.4734854503976227e+43\n",
      "Gradient Descent(22/49): loss=4.4135101814373526e+45\n",
      "Gradient Descent(23/49): loss=7.875151284518104e+47\n",
      "Gradient Descent(24/49): loss=1.4051855598996536e+50\n",
      "Gradient Descent(25/49): loss=2.507312413964421e+52\n",
      "Gradient Descent(26/49): loss=4.473868591184691e+54\n",
      "Gradient Descent(27/49): loss=7.982850505471055e+56\n",
      "Gradient Descent(28/49): loss=1.4244026370882988e+59\n",
      "Gradient Descent(29/49): loss=2.5416019893565023e+61\n",
      "Gradient Descent(30/49): loss=4.535052452237567e+63\n",
      "Gradient Descent(31/49): loss=8.092022602545057e+65\n",
      "Gradient Descent(32/49): loss=1.4438825237355668e+68\n",
      "Gradient Descent(33/49): loss=2.576360503112387e+70\n",
      "Gradient Descent(34/49): loss=4.597073053301337e+72\n",
      "Gradient Descent(35/49): loss=8.202687718531361e+74\n",
      "Gradient Descent(36/49): loss=1.463628813978167e+77\n",
      "Gradient Descent(37/49): loss=2.611594368352627e+79\n",
      "Gradient Descent(38/49): loss=4.659941837488944e+81\n",
      "Gradient Descent(39/49): loss=8.314866271700946e+83\n",
      "Gradient Descent(40/49): loss=1.4836451511061291e+86\n",
      "Gradient Descent(41/49): loss=2.6473100859028518e+88\n",
      "Gradient Descent(42/49): loss=4.723670404407725e+90\n",
      "Gradient Descent(43/49): loss=8.428578959561888e+92\n",
      "Gradient Descent(44/49): loss=1.5039352282343923e+95\n",
      "Gradient Descent(45/49): loss=2.6835142454927194e+97\n",
      "Gradient Descent(46/49): loss=4.788270512299042e+99\n",
      "Gradient Descent(47/49): loss=8.543846762677707e+101\n",
      "Gradient Descent(48/49): loss=1.5245027889844037e+104\n",
      "Gradient Descent(49/49): loss=2.7202135269721405e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.676133897019705\n",
      "Gradient Descent(2/49): loss=133.75363888231462\n",
      "Gradient Descent(3/49): loss=5986.407485481659\n",
      "Gradient Descent(4/49): loss=335102.4867583627\n",
      "Gradient Descent(5/49): loss=28956165.555650707\n",
      "Gradient Descent(6/49): loss=3811432468.3242755\n",
      "Gradient Descent(7/49): loss=611219603444.8506\n",
      "Gradient Descent(8/49): loss=104040023315820.31\n",
      "Gradient Descent(9/49): loss=1.7981228429036398e+16\n",
      "Gradient Descent(10/49): loss=3.1192561851250135e+18\n",
      "Gradient Descent(11/49): loss=5.415908181730909e+20\n",
      "Gradient Descent(12/49): loss=9.405565018762526e+22\n",
      "Gradient Descent(13/49): loss=1.6335066168542385e+25\n",
      "Gradient Descent(14/49): loss=2.8370192693728044e+27\n",
      "Gradient Descent(15/49): loss=4.9272541550071386e+29\n",
      "Gradient Descent(16/49): loss=8.557520597746501e+31\n",
      "Gradient Descent(17/49): loss=1.4862470865345568e+34\n",
      "Gradient Descent(18/49): loss=2.58127394240188e+36\n",
      "Gradient Descent(19/49): loss=4.483087160839492e+38\n",
      "Gradient Descent(20/49): loss=7.786105228563905e+40\n",
      "Gradient Descent(21/49): loss=1.352269819652294e+43\n",
      "Gradient Descent(22/49): loss=2.3485858612385022e+45\n",
      "Gradient Descent(23/49): loss=4.078960772198627e+47\n",
      "Gradient Descent(24/49): loss=7.084229389208988e+49\n",
      "Gradient Descent(25/49): loss=1.2303699114020859e+52\n",
      "Gradient Descent(26/49): loss=2.1368733784794982e+54\n",
      "Gradient Descent(27/49): loss=3.7112642249608947e+56\n",
      "Gradient Descent(28/49): loss=6.445623913044275e+58\n",
      "Gradient Descent(29/49): loss=1.1194586294605799e+61\n",
      "Gradient Descent(30/49): loss=1.9442456463177116e+63\n",
      "Gradient Descent(31/49): loss=3.3767135593450575e+65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=5.864585312797211e+67\n",
      "Gradient Descent(33/49): loss=1.0185454077350752e+70\n",
      "Gradient Descent(34/49): loss=1.7689822762990587e+72\n",
      "Gradient Descent(35/49): loss=3.0723208509857197e+74\n",
      "Gradient Descent(36/49): loss=5.335924241790299e+76\n",
      "Gradient Descent(37/49): loss=9.267289744490879e+78\n",
      "Gradient Descent(38/49): loss=1.6095179638369285e+81\n",
      "Gradient Descent(39/49): loss=2.7953675209667795e+83\n",
      "Gradient Descent(40/49): loss=4.854919145262496e+85\n",
      "Gradient Descent(41/49): loss=8.431893026676048e+87\n",
      "Gradient Descent(42/49): loss=1.4644285082005181e+90\n",
      "Gradient Descent(43/49): loss=2.5433800557545274e+92\n",
      "Gradient Descent(44/49): loss=4.417274091419272e+94\n",
      "Gradient Descent(45/49): loss=7.671802865079827e+96\n",
      "Gradient Descent(46/49): loss=1.332418092754913e+99\n",
      "Gradient Descent(47/49): loss=2.3141079158610574e+101\n",
      "Gradient Descent(48/49): loss=4.0190804037931446e+103\n",
      "Gradient Descent(49/49): loss=6.980230775514233e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7461791682724765\n",
      "Gradient Descent(2/49): loss=135.91657647983726\n",
      "Gradient Descent(3/49): loss=6012.380188625294\n",
      "Gradient Descent(4/49): loss=327592.4377049255\n",
      "Gradient Descent(5/49): loss=27253397.260743883\n",
      "Gradient Descent(6/49): loss=3503766441.203127\n",
      "Gradient Descent(7/49): loss=557562289129.6018\n",
      "Gradient Descent(8/49): loss=94737112569562.61\n",
      "Gradient Descent(9/49): loss=1.6369687178330736e+16\n",
      "Gradient Descent(10/49): loss=2.840109310931128e+18\n",
      "Gradient Descent(11/49): loss=4.932372627619926e+20\n",
      "Gradient Descent(12/49): loss=8.56798555188065e+22\n",
      "Gradient Descent(13/49): loss=1.4884216512252085e+25\n",
      "Gradient Descent(14/49): loss=2.585705563888666e+27\n",
      "Gradient Descent(15/49): loss=4.491935972969691e+29\n",
      "Gradient Descent(16/49): loss=7.8034810236058145e+31\n",
      "Gradient Descent(17/49): loss=1.3556365818913826e+34\n",
      "Gradient Descent(18/49): loss=2.3550395233069427e+36\n",
      "Gradient Descent(19/49): loss=4.091222742710613e+38\n",
      "Gradient Descent(20/49): loss=7.107355696979488e+40\n",
      "Gradient Descent(21/49): loss=1.234704346610987e+43\n",
      "Gradient Descent(22/49): loss=2.144953606877189e+45\n",
      "Gradient Descent(23/49): loss=3.726257211649266e+47\n",
      "Gradient Descent(24/49): loss=6.473330128429725e+49\n",
      "Gradient Descent(25/49): loss=1.1245601302217205e+52\n",
      "Gradient Descent(26/49): loss=1.9536088248161787e+54\n",
      "Gradient Descent(27/49): loss=3.3938491485088735e+56\n",
      "Gradient Descent(28/49): loss=5.895864052476664e+58\n",
      "Gradient Descent(29/49): loss=1.0242415441640781e+61\n",
      "Gradient Descent(30/49): loss=1.7793333283370994e+63\n",
      "Gradient Descent(31/49): loss=3.0910941968430587e+65\n",
      "Gradient Descent(32/49): loss=5.369911967358261e+67\n",
      "Gradient Descent(33/49): loss=9.328720737992466e+69\n",
      "Gradient Descent(34/49): loss=1.6206044184046675e+72\n",
      "Gradient Descent(35/49): loss=2.815347092829681e+74\n",
      "Gradient Descent(36/49): loss=4.890878466755688e+76\n",
      "Gradient Descent(37/49): loss=8.496533957570627e+78\n",
      "Gradient Descent(38/49): loss=1.4760352313566513e+81\n",
      "Gradient Descent(39/49): loss=2.5641985485914783e+83\n",
      "Gradient Descent(40/49): loss=4.454578086564647e+85\n",
      "Gradient Descent(41/49): loss=7.7385840266549005e+87\n",
      "Gradient Descent(42/49): loss=1.3443626214168188e+90\n",
      "Gradient Descent(43/49): loss=2.3354542014890165e+92\n",
      "Gradient Descent(44/49): loss=4.057198735192661e+94\n",
      "Gradient Descent(45/49): loss=7.048248501877767e+96\n",
      "Gradient Descent(46/49): loss=1.2244361241984496e+99\n",
      "Gradient Descent(47/49): loss=2.1271154412940083e+101\n",
      "Gradient Descent(48/49): loss=3.695268386134281e+103\n",
      "Gradient Descent(49/49): loss=6.419495707885324e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.613761574261622\n",
      "Gradient Descent(2/49): loss=128.9684990386224\n",
      "Gradient Descent(3/49): loss=5551.81547328047\n",
      "Gradient Descent(4/49): loss=281210.04808286775\n",
      "Gradient Descent(5/49): loss=20790581.016921993\n",
      "Gradient Descent(6/49): loss=2472670449.0620494\n",
      "Gradient Descent(7/49): loss=386352767363.92804\n",
      "Gradient Descent(8/49): loss=66032213604401.43\n",
      "Gradient Descent(9/49): loss=1.1550496901735194e+16\n",
      "Gradient Descent(10/49): loss=2.0317564133534013e+18\n",
      "Gradient Descent(11/49): loss=3.578628724284709e+20\n",
      "Gradient Descent(12/49): loss=6.3051712593662445e+22\n",
      "Gradient Descent(13/49): loss=1.1109869581142439e+25\n",
      "Gradient Descent(14/49): loss=1.9576205888057486e+27\n",
      "Gradient Descent(15/49): loss=3.4494499615995714e+29\n",
      "Gradient Descent(16/49): loss=6.078152459990351e+31\n",
      "Gradient Descent(17/49): loss=1.0710097562513715e+34\n",
      "Gradient Descent(18/49): loss=1.887188526714406e+36\n",
      "Gradient Descent(19/49): loss=3.325348400295783e+38\n",
      "Gradient Descent(20/49): loss=5.85947925136895e+40\n",
      "Gradient Descent(21/49): loss=1.0324781943993043e+43\n",
      "Gradient Descent(22/49): loss=1.8192934497270557e+45\n",
      "Gradient Descent(23/49): loss=3.2057128899187354e+47\n",
      "Gradient Descent(24/49): loss=5.648673738821527e+49\n",
      "Gradient Descent(25/49): loss=9.953328979666741e+51\n",
      "Gradient Descent(26/49): loss=1.75384103168602e+54\n",
      "Gradient Descent(27/49): loss=3.090381490162283e+56\n",
      "Gradient Descent(28/49): loss=5.445452342711273e+58\n",
      "Gradient Descent(29/49): loss=9.595239717535156e+60\n",
      "Gradient Descent(30/49): loss=1.6907433844352154e+63\n",
      "Gradient Descent(31/49): loss=2.979199349013993e+65\n",
      "Gradient Descent(32/49): loss=5.249542208991311e+67\n",
      "Gradient Descent(33/49): loss=9.250033373262677e+69\n",
      "Gradient Descent(34/49): loss=1.6299157907507091e+72\n",
      "Gradient Descent(35/49): loss=2.8720171892757615e+74\n",
      "Gradient Descent(36/49): loss=5.060680301585657e+76\n",
      "Gradient Descent(37/49): loss=8.917246460253576e+78\n",
      "Gradient Descent(38/49): loss=1.5712765812926521e+81\n",
      "Gradient Descent(39/49): loss=2.7686911042812674e+83\n",
      "Gradient Descent(40/49): loss=4.8786130476276935e+85\n",
      "Gradient Descent(41/49): loss=8.5964321666941e+87\n",
      "Gradient Descent(42/49): loss=1.5147470249255921e+90\n",
      "Gradient Descent(43/49): loss=2.6690823646704496e+92\n",
      "Gradient Descent(44/49): loss=4.7030959969997875e+94\n",
      "Gradient Descent(45/49): loss=8.287159755643835e+96\n",
      "Gradient Descent(46/49): loss=1.460251223010836e+99\n",
      "Gradient Descent(47/49): loss=2.5730572321264475e+101\n",
      "Gradient Descent(48/49): loss=4.533893494125694e+103\n",
      "Gradient Descent(49/49): loss=7.98901398671465e+105\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6792271424784775\n",
      "Gradient Descent(2/49): loss=133.7682819909859\n",
      "Gradient Descent(3/49): loss=6059.0556353472875\n",
      "Gradient Descent(4/49): loss=356242.6823614113\n",
      "Gradient Descent(5/49): loss=33697961.632529266\n",
      "Gradient Descent(6/49): loss=4801870120.704546\n",
      "Gradient Descent(7/49): loss=812060992357.353\n",
      "Gradient Descent(8/49): loss=144057114016110.5\n",
      "Gradient Descent(9/49): loss=2.585372282527091e+16\n",
      "Gradient Descent(10/49): loss=4.652554240882657e+18\n",
      "Gradient Descent(11/49): loss=8.377868034805616e+20\n",
      "Gradient Descent(12/49): loss=1.5088252653119805e+23\n",
      "Gradient Descent(13/49): loss=2.7174342263047725e+25\n",
      "Gradient Descent(14/49): loss=4.8942091689203685e+27\n",
      "Gradient Descent(15/49): loss=8.814684984027886e+29\n",
      "Gradient Descent(16/49): loss=1.5875640203642627e+32\n",
      "Gradient Descent(17/49): loss=2.8592737767277646e+34\n",
      "Gradient Descent(18/49): loss=5.149680018144662e+36\n",
      "Gradient Descent(19/49): loss=9.274804197621333e+38\n",
      "Gradient Descent(20/49): loss=1.6704337493616635e+41\n",
      "Gradient Descent(21/49): loss=3.00852595087139e+43\n",
      "Gradient Descent(22/49): loss=5.4184898988694734e+45\n",
      "Gradient Descent(23/49): loss=9.758942839127439e+47\n",
      "Gradient Descent(24/49): loss=1.757629286299018e+50\n",
      "Gradient Descent(25/49): loss=3.165569016013107e+52\n",
      "Gradient Descent(26/49): loss=5.701331488532846e+54\n",
      "Gradient Descent(27/49): loss=1.02683532021287e+57\n",
      "Gradient Descent(28/49): loss=1.8493763727953228e+59\n",
      "Gradient Descent(29/49): loss=3.3308096253882526e+61\n",
      "Gradient Descent(30/49): loss=5.998937222178273e+63\n",
      "Gradient Descent(31/49): loss=1.08043544492404e+66\n",
      "Gradient Descent(32/49): loss=1.9459125965387787e+68\n",
      "Gradient Descent(33/49): loss=3.504675685306247e+70\n",
      "Gradient Descent(34/49): loss=6.3120777783257735e+72\n",
      "Gradient Descent(35/49): loss=1.136833460701568e+75\n",
      "Gradient Descent(36/49): loss=2.0474879473261728e+77\n",
      "Gradient Descent(37/49): loss=3.68761743858128e+79\n",
      "Gradient Descent(38/49): loss=6.641564064436793e+81\n",
      "Gradient Descent(39/49): loss=1.1961754156089809e+84\n",
      "Gradient Descent(40/49): loss=2.154365464256978e+86\n",
      "Gradient Descent(41/49): loss=3.8801086304053784e+88\n",
      "Gradient Descent(42/49): loss=6.988249316807108e+90\n",
      "Gradient Descent(43/49): loss=1.2586149813221443e+93\n",
      "Gradient Descent(44/49): loss=2.266821916897873e+95\n",
      "Gradient Descent(45/49): loss=4.082647735156245e+97\n",
      "Gradient Descent(46/49): loss=7.353031310102366e+99\n",
      "Gradient Descent(47/49): loss=1.3243138510767911e+102\n",
      "Gradient Descent(48/49): loss=2.3851485220038122e+104\n",
      "Gradient Descent(49/49): loss=4.295759247244306e+106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.709787841626584\n",
      "Gradient Descent(2/49): loss=136.4543637434493\n",
      "Gradient Descent(3/49): loss=6167.748986963396\n",
      "Gradient Descent(4/49): loss=348480.9792761795\n",
      "Gradient Descent(5/49): loss=30372645.82572233\n",
      "Gradient Descent(6/49): loss=4032641622.1068487\n",
      "Gradient Descent(7/49): loss=652569557446.0493\n",
      "Gradient Descent(8/49): loss=112109963642270.47\n",
      "Gradient Descent(9/49): loss=1.955714918094889e+16\n",
      "Gradient Descent(10/49): loss=3.4244257426273336e+18\n",
      "Gradient Descent(11/49): loss=6.001512920024756e+20\n",
      "Gradient Descent(12/49): loss=1.0520289382914716e+23\n",
      "Gradient Descent(13/49): loss=1.844238981468195e+25\n",
      "Gradient Descent(14/49): loss=3.233047833699814e+27\n",
      "Gradient Descent(15/49): loss=5.667719716144455e+29\n",
      "Gradient Descent(16/49): loss=9.935847401004035e+31\n",
      "Gradient Descent(17/49): loss=1.7418130323580852e+34\n",
      "Gradient Descent(18/49): loss=3.053501773229035e+36\n",
      "Gradient Descent(19/49): loss=5.3529701517619966e+38\n",
      "Gradient Descent(20/49): loss=9.384074954716492e+40\n",
      "Gradient Descent(21/49): loss=1.6450841366535787e+43\n",
      "Gradient Descent(22/49): loss=2.8839303075721243e+45\n",
      "Gradient Descent(23/49): loss=5.0557013066380565e+47\n",
      "Gradient Descent(24/49): loss=8.862945000797889e+49\n",
      "Gradient Descent(25/49): loss=1.553726957409094e+52\n",
      "Gradient Descent(26/49): loss=2.72377574041524e+54\n",
      "Gradient Descent(27/49): loss=4.774940827728904e+56\n",
      "Gradient Descent(28/49): loss=8.370755187369639e+58\n",
      "Gradient Descent(29/49): loss=1.4674431565721624e+61\n",
      "Gradient Descent(30/49): loss=2.572515107143033e+63\n",
      "Gradient Descent(31/49): loss=4.509771943696863e+65\n",
      "Gradient Descent(32/49): loss=7.90589836680988e+67\n",
      "Gradient Descent(33/49): loss=1.3859509919051353e+70\n",
      "Gradient Descent(34/49): loss=2.4296544969853437e+72\n",
      "Gradient Descent(35/49): loss=4.2593287996471133e+74\n",
      "Gradient Descent(36/49): loss=7.466856644026093e+76\n",
      "Gradient Descent(37/49): loss=1.3089843673739461e+79\n",
      "Gradient Descent(38/49): loss=2.2947274277727743e+81\n",
      "Gradient Descent(39/49): loss=4.0227936245997666e+83\n",
      "Gradient Descent(40/49): loss=7.052196417866967e+85\n",
      "Gradient Descent(41/49): loss=1.236291964172652e+88\n",
      "Gradient Descent(42/49): loss=2.1672933226953034e+90\n",
      "Gradient Descent(43/49): loss=3.799394061210245e+92\n",
      "Gradient Descent(44/49): loss=6.6605636999828664e+94\n",
      "Gradient Descent(45/49): loss=1.1676364200927204e+97\n",
      "Gradient Descent(46/49): loss=2.0469360716877892e+99\n",
      "Gradient Descent(47/49): loss=3.58840064379275e+101\n",
      "Gradient Descent(48/49): loss=6.290679693653152e+103\n",
      "Gradient Descent(49/49): loss=1.1027935544653608e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.780560177886189\n",
      "Gradient Descent(2/49): loss=138.66226056083056\n",
      "Gradient Descent(3/49): loss=6194.678196656535\n",
      "Gradient Descent(4/49): loss=340695.3011020926\n",
      "Gradient Descent(5/49): loss=28588940.461303376\n",
      "Gradient Descent(6/49): loss=3707268050.2438793\n",
      "Gradient Descent(7/49): loss=595289599060.0609\n",
      "Gradient Descent(8/49): loss=102085760927257.4\n",
      "Gradient Descent(9/49): loss=1.780437145611782e+16\n",
      "Gradient Descent(10/49): loss=3.117966903802538e+18\n",
      "Gradient Descent(11/49): loss=5.465689470042203e+20\n",
      "Gradient Descent(12/49): loss=9.583432485741039e+22\n",
      "Gradient Descent(13/49): loss=1.6804353252483234e+25\n",
      "Gradient Descent(14/49): loss=2.946648961323992e+27\n",
      "Gradient Descent(15/49): loss=5.166975558985365e+29\n",
      "Gradient Descent(16/49): loss=9.060345326325594e+31\n",
      "Gradient Descent(17/49): loss=1.5887412615304739e+34\n",
      "Gradient Descent(18/49): loss=2.785874949993675e+36\n",
      "Gradient Descent(19/49): loss=4.885061845813475e+38\n",
      "Gradient Descent(20/49): loss=8.566008785943558e+40\n",
      "Gradient Descent(21/49): loss=1.5020589069606807e+43\n",
      "Gradient Descent(22/49): loss=2.633876542376869e+45\n",
      "Gradient Descent(23/49): loss=4.61853101005365e+47\n",
      "Gradient Descent(24/49): loss=8.098644088972992e+49\n",
      "Gradient Descent(25/49): loss=1.4201060020429885e+52\n",
      "Gradient Descent(26/49): loss=2.4901712371647415e+54\n",
      "Gradient Descent(27/49): loss=4.366542202823179e+56\n",
      "Gradient Descent(28/49): loss=7.656778989522717e+58\n",
      "Gradient Descent(29/49): loss=1.3426244788494556e+61\n",
      "Gradient Descent(30/49): loss=2.35430654805688e+63\n",
      "Gradient Descent(31/49): loss=4.128302000700492e+65\n",
      "Gradient Descent(32/49): loss=7.23902221783893e+67\n",
      "Gradient Descent(33/49): loss=1.2693703770090781e+70\n",
      "Gradient Descent(34/49): loss=2.2258546880233703e+72\n",
      "Gradient Descent(35/49): loss=3.903060274550679e+74\n",
      "Gradient Descent(36/49): loss=6.844058414389893e+76\n",
      "Gradient Descent(37/49): loss=1.200113046805888e+79\n",
      "Gradient Descent(38/49): loss=2.104411210292261e+81\n",
      "Gradient Descent(39/49): loss=3.690107822584155e+83\n",
      "Gradient Descent(40/49): loss=6.470643986165675e+85\n",
      "Gradient Descent(41/49): loss=1.1346344228603316e+88\n",
      "Gradient Descent(42/49): loss=1.989593734861714e+90\n",
      "Gradient Descent(43/49): loss=3.488774137331313e+92\n",
      "Gradient Descent(44/49): loss=6.11760319106453e+94\n",
      "Gradient Descent(45/49): loss=1.0727283375228707e+97\n",
      "Gradient Descent(46/49): loss=1.8810407445278187e+99\n",
      "Gradient Descent(47/49): loss=3.2984252944641207e+101\n",
      "Gradient Descent(48/49): loss=5.783824435919641e+103\n",
      "Gradient Descent(49/49): loss=1.014199871729305e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.6467952743864656\n",
      "Gradient Descent(2/49): loss=131.57570189241792\n",
      "Gradient Descent(3/49): loss=5720.525012489376\n",
      "Gradient Descent(4/49): loss=292517.4077472513\n",
      "Gradient Descent(5/49): loss=21815439.8525645\n",
      "Gradient Descent(6/49): loss=2616633466.8363943\n",
      "Gradient Descent(7/49): loss=412503307910.70386\n",
      "Gradient Descent(8/49): loss=71152617695237.23\n",
      "Gradient Descent(9/49): loss=1.2562334346317158e+16\n",
      "Gradient Descent(10/49): loss=2.2304284749073498e+18\n",
      "Gradient Descent(11/49): loss=3.96536912511422e+20\n",
      "Gradient Descent(12/49): loss=7.0520447736019545e+22\n",
      "Gradient Descent(13/49): loss=1.254233973952107e+25\n",
      "Gradient Descent(14/49): loss=2.2307433599213614e+27\n",
      "Gradient Descent(15/49): loss=3.967550239464246e+29\n",
      "Gradient Descent(16/49): loss=7.05660290382022e+31\n",
      "Gradient Descent(17/49): loss=1.25507310139615e+34\n",
      "Gradient Descent(18/49): loss=2.2322477195666545e+36\n",
      "Gradient Descent(19/49): loss=3.970230848418318e+38\n",
      "Gradient Descent(20/49): loss=7.061372668563547e+40\n",
      "Gradient Descent(21/49): loss=1.2559215295747201e+43\n",
      "Gradient Descent(22/49): loss=2.233756753457298e+45\n",
      "Gradient Descent(23/49): loss=3.972914801053288e+47\n",
      "Gradient Descent(24/49): loss=7.066146299117931e+49\n",
      "Gradient Descent(25/49): loss=1.2567705581657255e+52\n",
      "Gradient Descent(26/49): loss=2.235266818732803e+54\n",
      "Gradient Descent(27/49): loss=3.975600572805209e+56\n",
      "Gradient Descent(28/49): loss=7.070923158716952e+58\n",
      "Gradient Descent(29/49): loss=1.2576201608000546e+61\n",
      "Gradient Descent(30/49): loss=2.236777904877864e+63\n",
      "Gradient Descent(31/49): loss=3.9782881602081583e+65\n",
      "Gradient Descent(32/49): loss=7.075703247576814e+67\n",
      "Gradient Descent(33/49): loss=1.2584703377833998e+70\n",
      "Gradient Descent(34/49): loss=2.2382900125483134e+72\n",
      "Gradient Descent(35/49): loss=3.9809775644753484e+74\n",
      "Gradient Descent(36/49): loss=7.080486567874558e+76\n",
      "Gradient Descent(37/49): loss=1.2593210895038013e+79\n",
      "Gradient Descent(38/49): loss=2.2398031424344045e+81\n",
      "Gradient Descent(39/49): loss=3.9836687868348786e+83\n",
      "Gradient Descent(40/49): loss=7.085273121794828e+85\n",
      "Gradient Descent(41/49): loss=1.2601724163497556e+88\n",
      "Gradient Descent(42/49): loss=2.241317295227269e+90\n",
      "Gradient Descent(43/49): loss=3.9863618285156784e+92\n",
      "Gradient Descent(44/49): loss=7.090062911523401e+94\n",
      "Gradient Descent(45/49): loss=1.2610243187101032e+97\n",
      "Gradient Descent(46/49): loss=2.242832471618481e+99\n",
      "Gradient Descent(47/49): loss=3.98905669074789e+101\n",
      "Gradient Descent(48/49): loss=7.094855939248014e+103\n",
      "Gradient Descent(49/49): loss=1.2618767969739015e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.712757830272367\n",
      "Gradient Descent(2/49): loss=136.45738880550772\n",
      "Gradient Descent(3/49): loss=6241.5876013249845\n",
      "Gradient Descent(4/49): loss=370352.60014461353\n",
      "Gradient Descent(5/49): loss=35331969.17217744\n",
      "Gradient Descent(6/49): loss=5078549427.278\n",
      "Gradient Descent(7/49): loss=866655543623.2604\n",
      "Gradient Descent(8/49): loss=155164838281111.53\n",
      "Gradient Descent(9/49): loss=2.81063493598014e+16\n",
      "Gradient Descent(10/49): loss=5.105068628086759e+18\n",
      "Gradient Descent(11/49): loss=9.27842023881502e+20\n",
      "Gradient Descent(12/49): loss=1.686592903837382e+23\n",
      "Gradient Descent(13/49): loss=3.0659231565006594e+25\n",
      "Gradient Descent(14/49): loss=5.573341839925084e+27\n",
      "Gradient Descent(15/49): loss=1.0131433271017979e+30\n",
      "Gradient Descent(16/49): loss=1.8417313414696837e+32\n",
      "Gradient Descent(17/49): loss=3.3479711850593997e+34\n",
      "Gradient Descent(18/49): loss=6.086072955957832e+36\n",
      "Gradient Descent(19/49): loss=1.106350149739916e+39\n",
      "Gradient Descent(20/49): loss=2.0111665833956338e+41\n",
      "Gradient Descent(21/49): loss=3.6559772945780673e+43\n",
      "Gradient Descent(22/49): loss=6.645978552791777e+45\n",
      "Gradient Descent(23/49): loss=1.2081319812991317e+48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=2.1961895793806456e+50\n",
      "Gradient Descent(25/49): loss=3.9923193353406745e+52\n",
      "Gradient Descent(26/49): loss=7.257394272780821e+54\n",
      "Gradient Descent(27/49): loss=1.319277522826239e+57\n",
      "Gradient Descent(28/49): loss=2.3982342929366697e+59\n",
      "Gradient Descent(29/49): loss=4.3596041199097027e+61\n",
      "Gradient Descent(30/49): loss=7.92505892285444e+63\n",
      "Gradient Descent(31/49): loss=1.4406482149121286e+66\n",
      "Gradient Descent(32/49): loss=2.6188666851980358e+68\n",
      "Gradient Descent(33/49): loss=4.760678314003577e+70\n",
      "Gradient Descent(34/49): loss=8.654147283449898e+72\n",
      "Gradient Descent(35/49): loss=1.5731847493946904e+75\n",
      "Gradient Descent(36/49): loss=2.859796782591239e+77\n",
      "Gradient Descent(37/49): loss=5.198650470560375e+79\n",
      "Gradient Descent(38/49): loss=9.450310203709557e+81\n",
      "Gradient Descent(39/49): loss=1.7179143597378203e+84\n",
      "Gradient Descent(40/49): loss=3.122891930293463e+86\n",
      "Gradient Descent(41/49): loss=5.676915122695755e+88\n",
      "Gradient Descent(42/49): loss=1.031971839872929e+91\n",
      "Gradient Descent(43/49): loss=1.8759587826724125e+93\n",
      "Gradient Descent(44/49): loss=3.4101912652181433e+95\n",
      "Gradient Descent(45/49): loss=6.1991790932654015e+97\n",
      "Gradient Descent(46/49): loss=1.1269110276112495e+100\n",
      "Gradient Descent(47/49): loss=2.0485429522942145e+102\n",
      "Gradient Descent(48/49): loss=3.723921520485765e+104\n",
      "Gradient Descent(49/49): loss=6.769490224848008e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.74361360153038\n",
      "Gradient Descent(2/49): loss=139.19591385839493\n",
      "Gradient Descent(3/49): loss=6353.648033014201\n",
      "Gradient Descent(4/49): loss=362325.0451186514\n",
      "Gradient Descent(5/49): loss=31851258.198745906\n",
      "Gradient Descent(6/49): loss=4265569053.6378183\n",
      "Gradient Descent(7/49): loss=696505043936.7449\n",
      "Gradient Descent(8/49): loss=120763942373018.53\n",
      "Gradient Descent(9/49): loss=2.1262888370384396e+16\n",
      "Gradient Descent(10/49): loss=3.7578216457992566e+18\n",
      "Gradient Descent(11/49): loss=6.647266232515074e+20\n",
      "Gradient Descent(12/49): loss=1.1761007553153746e+23\n",
      "Gradient Descent(13/49): loss=2.0809838700895154e+25\n",
      "Gradient Descent(14/49): loss=3.682123598248292e+27\n",
      "Gradient Descent(15/49): loss=6.515223583681658e+29\n",
      "Gradient Descent(16/49): loss=1.1528176081501847e+32\n",
      "Gradient Descent(17/49): loss=2.0398205107382877e+34\n",
      "Gradient Descent(18/49): loss=3.6093028604889593e+36\n",
      "Gradient Descent(19/49): loss=6.3863791944492324e+38\n",
      "Gradient Descent(20/49): loss=1.1300198650742027e+41\n",
      "Gradient Descent(21/49): loss=1.9994817991963108e+43\n",
      "Gradient Descent(22/49): loss=3.537926712124681e+45\n",
      "Gradient Descent(23/49): loss=6.2600847008548435e+47\n",
      "Gradient Descent(24/49): loss=1.1076730427425884e+50\n",
      "Gradient Descent(25/49): loss=1.9599408446557587e+52\n",
      "Gradient Descent(26/49): loss=3.46796208476873e+54\n",
      "Gradient Descent(27/49): loss=6.1362877630663915e+56\n",
      "Gradient Descent(28/49): loss=1.0857681425219303e+59\n",
      "Gradient Descent(29/49): loss=1.9211818363720774e+61\n",
      "Gradient Descent(30/49): loss=3.399381050021435e+63\n",
      "Gradient Descent(31/49): loss=6.014938984155068e+65\n",
      "Gradient Descent(32/49): loss=1.0642964248706382e+68\n",
      "Gradient Descent(33/49): loss=1.8831893107749388e+70\n",
      "Gradient Descent(34/49): loss=3.332156246459219e+72\n",
      "Gradient Descent(35/49): loss=5.895989950287235e+74\n",
      "Gradient Descent(36/49): loss=1.0432493233421197e+77\n",
      "Gradient Descent(37/49): loss=1.8459481102079732e+79\n",
      "Gradient Descent(38/49): loss=3.2662608537948956e+81\n",
      "Gradient Descent(39/49): loss=5.779393205061874e+83\n",
      "Gradient Descent(40/49): loss=1.022618440897281e+86\n",
      "Gradient Descent(41/49): loss=1.809443376767121e+88\n",
      "Gradient Descent(42/49): loss=3.2016685821288164e+90\n",
      "Gradient Descent(43/49): loss=5.66510223055737e+92\n",
      "Gradient Descent(44/49): loss=1.0023955465536212e+95\n",
      "Gradient Descent(45/49): loss=1.773660546372336e+97\n",
      "Gradient Descent(46/49): loss=3.138353661459975e+99\n",
      "Gradient Descent(47/49): loss=5.553071428771489e+101\n",
      "Gradient Descent(48/49): loss=9.825725721011531e+103\n",
      "Gradient Descent(49/49): loss=1.7385853429568974e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8151166609816585\n",
      "Gradient Descent(2/49): loss=141.4494660774\n",
      "Gradient Descent(3/49): loss=6381.561862752344\n",
      "Gradient Descent(4/49): loss=354254.96820095275\n",
      "Gradient Descent(5/49): loss=29983171.40713156\n",
      "Gradient Descent(6/49): loss=3921558238.0390687\n",
      "Gradient Descent(7/49): loss=635376417761.9874\n",
      "Gradient Descent(8/49): loss=109966269574067.81\n",
      "Gradient Descent(9/49): loss=1.93572400037187e+16\n",
      "Gradient Descent(10/49): loss=3.421524481920141e+18\n",
      "Gradient Descent(11/49): loss=6.053783814707737e+20\n",
      "Gradient Descent(12/49): loss=1.0713651132426558e+23\n",
      "Gradient Descent(13/49): loss=1.89615068683109e+25\n",
      "Gradient Descent(14/49): loss=3.355939509463897e+27\n",
      "Gradient Descent(15/49): loss=5.939594829847487e+29\n",
      "Gradient Descent(16/49): loss=1.0512351092777095e+32\n",
      "Gradient Descent(17/49): loss=1.8605569805661377e+34\n",
      "Gradient Descent(18/49): loss=3.29295740113248e+36\n",
      "Gradient Descent(19/49): loss=5.8281303263433145e+38\n",
      "Gradient Descent(20/49): loss=1.0315075189456487e+41\n",
      "Gradient Descent(21/49): loss=1.825641677037721e+43\n",
      "Gradient Descent(22/49): loss=3.231161646631933e+45\n",
      "Gradient Descent(23/49): loss=5.718759446801802e+47\n",
      "Gradient Descent(24/49): loss=1.0121502167727399e+50\n",
      "Gradient Descent(25/49): loss=1.7913816289092265e+52\n",
      "Gradient Descent(26/49): loss=3.170525567466727e+54\n",
      "Gradient Descent(27/49): loss=5.611441030620882e+56\n",
      "Gradient Descent(28/49): loss=9.93156174586406e+58\n",
      "Gradient Descent(29/49): loss=1.75776450600952e+61\n",
      "Gradient Descent(30/49): loss=3.1110273868796554e+63\n",
      "Gradient Descent(31/49): loss=5.506136555167744e+65\n",
      "Gradient Descent(32/49): loss=9.745185751824157e+67\n",
      "Gradient Descent(33/49): loss=1.7247782430754244e+70\n",
      "Gradient Descent(34/49): loss=3.0526457509847627e+72\n",
      "Gradient Descent(35/49): loss=5.402808226748768e+74\n",
      "Gradient Descent(36/49): loss=9.562307295436433e+76\n",
      "Gradient Descent(37/49): loss=1.6924110013688723e+79\n",
      "Gradient Descent(38/49): loss=2.995359706669588e+81\n",
      "Gradient Descent(39/49): loss=5.301418960927917e+83\n",
      "Gradient Descent(40/49): loss=9.382860741801996e+85\n",
      "Gradient Descent(41/49): loss=1.6606511643184386e+88\n",
      "Gradient Descent(42/49): loss=2.93914869402892e+90\n",
      "Gradient Descent(43/49): loss=5.201932369196428e+92\n",
      "Gradient Descent(44/49): loss=9.206781687727579e+94\n",
      "Gradient Descent(45/49): loss=1.6294873333497012e+97\n",
      "Gradient Descent(46/49): loss=2.8839925389851987e+99\n",
      "Gradient Descent(47/49): loss=5.104312745914021e+101\n",
      "Gradient Descent(48/49): loss=9.034006938612776e+103\n",
      "Gradient Descent(49/49): loss=1.5989083237941349e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.679997767639653\n",
      "Gradient Descent(2/49): loss=134.22236616026532\n",
      "Gradient Descent(3/49): loss=5893.487387895465\n",
      "Gradient Descent(4/49): loss=304220.9939979309\n",
      "Gradient Descent(5/49): loss=22885604.115343098\n",
      "Gradient Descent(6/49): loss=2768248329.7809043\n",
      "Gradient Descent(7/49): loss=440289933262.433\n",
      "Gradient Descent(8/49): loss=76643499964233.27\n",
      "Gradient Descent(9/49): loss=1.365748439756372e+16\n",
      "Gradient Descent(10/49): loss=2.447467051624618e+18\n",
      "Gradient Descent(11/49): loss=4.3918114098160986e+20\n",
      "Gradient Descent(12/49): loss=7.883290118087034e+22\n",
      "Gradient Descent(13/49): loss=1.4151537445404297e+25\n",
      "Gradient Descent(14/49): loss=2.5404306752794285e+27\n",
      "Gradient Descent(15/49): loss=4.560504241312748e+29\n",
      "Gradient Descent(16/49): loss=8.186887114640453e+31\n",
      "Gradient Descent(17/49): loss=1.46968695650129e+34\n",
      "Gradient Descent(18/49): loss=2.638340844903297e+36\n",
      "Gradient Descent(19/49): loss=4.7362756208942625e+38\n",
      "Gradient Descent(20/49): loss=8.502429421747151e+40\n",
      "Gradient Descent(21/49): loss=1.5263323317570348e+43\n",
      "Gradient Descent(22/49): loss=2.7400290808597296e+45\n",
      "Gradient Descent(23/49): loss=4.918823514410163e+47\n",
      "Gradient Descent(24/49): loss=8.830134298628414e+49\n",
      "Gradient Descent(25/49): loss=1.5851609943629108e+52\n",
      "Gradient Descent(26/49): loss=2.8456366495368252e+54\n",
      "Gradient Descent(27/49): loss=5.108407265877015e+56\n",
      "Gradient Descent(28/49): loss=9.170469742970543e+58\n",
      "Gradient Descent(29/49): loss=1.646257060757321e+61\n",
      "Gradient Descent(30/49): loss=2.955314597892592e+63\n",
      "Gradient Descent(31/49): loss=5.305298048956641e+65\n",
      "Gradient Descent(32/49): loss=9.523922565920514e+67\n",
      "Gradient Descent(33/49): loss=1.7097079222436397e+70\n",
      "Gradient Descent(34/49): loss=3.0692198084877723e+72\n",
      "Gradient Descent(35/49): loss=5.509777494890403e+74\n",
      "Gradient Descent(36/49): loss=9.890998344024642e+76\n",
      "Gradient Descent(37/49): loss=1.775604338509573e+79\n",
      "Gradient Descent(38/49): loss=3.1875152105739428e+81\n",
      "Gradient Descent(39/49): loss=5.722138089710112e+83\n",
      "Gradient Descent(40/49): loss=1.0272222140021044e+86\n",
      "Gradient Descent(41/49): loss=1.844040567348267e+88\n",
      "Gradient Descent(42/49): loss=3.3103700131032024e+90\n",
      "Gradient Descent(43/49): loss=5.942683592591037e+92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=1.0668139253878978e+95\n",
      "Gradient Descent(45/49): loss=1.9151144994838405e+97\n",
      "Gradient Descent(46/49): loss=3.4379599467635078e+99\n",
      "Gradient Descent(47/49): loss=6.171729470345336e+101\n",
      "Gradient Descent(48/49): loss=1.107931600279048e+104\n",
      "Gradient Descent(49/49): loss=1.988927798593543e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.746458930132907\n",
      "Gradient Descent(2/49): loss=139.18696799942114\n",
      "Gradient Descent(3/49): loss=6428.6822929961245\n",
      "Gradient Descent(4/49): loss=384949.7336408896\n",
      "Gradient Descent(5/49): loss=37037021.97481218\n",
      "Gradient Descent(6/49): loss=5369778921.791432\n",
      "Gradient Descent(7/49): loss=924642488516.7269\n",
      "Gradient Descent(8/49): loss=167071706174327.72\n",
      "Gradient Descent(9/49): loss=3.054345760067361e+16\n",
      "Gradient Descent(10/49): loss=5.599194047841014e+18\n",
      "Gradient Descent(11/49): loss=1.0270929253943533e+21\n",
      "Gradient Descent(12/49): loss=1.8843351321213717e+23\n",
      "Gradient Descent(13/49): loss=3.457175490189725e+25\n",
      "Gradient Descent(14/49): loss=6.342904185186902e+27\n",
      "Gradient Descent(15/49): loss=1.1637392306234967e+30\n",
      "Gradient Descent(16/49): loss=2.135125384251155e+32\n",
      "Gradient Descent(17/49): loss=3.9173388128724936e+34\n",
      "Gradient Descent(18/49): loss=7.187186213384137e+36\n",
      "Gradient Descent(19/49): loss=1.3186412615014235e+39\n",
      "Gradient Descent(20/49): loss=2.4193261827956754e+41\n",
      "Gradient Descent(21/49): loss=4.438765380175811e+43\n",
      "Gradient Descent(22/49): loss=8.143853541392266e+45\n",
      "Gradient Descent(23/49): loss=1.4941621109524607e+48\n",
      "Gradient Descent(24/49): loss=2.741356290933545e+50\n",
      "Gradient Descent(25/49): loss=5.029597698108266e+52\n",
      "Gradient Descent(26/49): loss=9.227860343613358e+54\n",
      "Gradient Descent(27/49): loss=1.6930460770901197e+57\n",
      "Gradient Descent(28/49): loss=3.1062509752156477e+59\n",
      "Gradient Descent(29/49): loss=5.699074143104023e+61\n",
      "Gradient Descent(30/49): loss=1.0456156423852126e+64\n",
      "Gradient Descent(31/49): loss=1.918402961862753e+66\n",
      "Gradient Descent(32/49): loss=3.519715825681895e+68\n",
      "Gradient Descent(33/49): loss=6.45766282675376e+70\n",
      "Gradient Descent(34/49): loss=1.1847947746167562e+73\n",
      "Gradient Descent(35/49): loss=2.173756505439632e+75\n",
      "Gradient Descent(36/49): loss=3.988215888671176e+77\n",
      "Gradient Descent(37/49): loss=7.31722524341913e+79\n",
      "Gradient Descent(38/49): loss=1.3424996729745392e+82\n",
      "Gradient Descent(39/49): loss=2.463099483725283e+84\n",
      "Gradient Descent(40/49): loss=4.5190767557401484e+86\n",
      "Gradient Descent(41/49): loss=8.291201739599924e+88\n",
      "Gradient Descent(42/49): loss=1.5211962531821126e+91\n",
      "Gradient Descent(43/49): loss=2.7909561404628867e+93\n",
      "Gradient Descent(44/49): loss=5.120599108558995e+95\n",
      "Gradient Descent(45/49): loss=9.394821670764855e+97\n",
      "Gradient Descent(46/49): loss=1.7236786624819481e+100\n",
      "Gradient Descent(47/49): loss=3.16245292951224e+102\n",
      "Gradient Descent(48/49): loss=5.802188510577685e+104\n",
      "Gradient Descent(49/49): loss=1.064534153160415e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7776111767310927\n",
      "Gradient Descent(2/49): loss=141.97869880381998\n",
      "Gradient Descent(3/49): loss=6544.195848665982\n",
      "Gradient Descent(4/49): loss=376648.51857737306\n",
      "Gradient Descent(5/49): loss=33394416.34024423\n",
      "Gradient Descent(6/49): loss=4510776697.158357\n",
      "Gradient Descent(7/49): loss=743174607948.4218\n",
      "Gradient Descent(8/49): loss=130041223101076.27\n",
      "Gradient Descent(9/49): loss=2.310846282531886e+16\n",
      "Gradient Descent(10/49): loss=4.1219053772865157e+18\n",
      "Gradient Descent(11/49): loss=7.359023535279922e+20\n",
      "Gradient Descent(12/49): loss=1.3141275119387004e+23\n",
      "Gradient Descent(13/49): loss=2.3468088173466174e+25\n",
      "Gradient Descent(14/49): loss=4.191055578422547e+27\n",
      "Gradient Descent(15/49): loss=7.484632017880717e+29\n",
      "Gradient Descent(16/49): loss=1.3366503133554318e+32\n",
      "Gradient Descent(17/49): loss=2.3870704278164573e+34\n",
      "Gradient Descent(18/49): loss=4.26297395222188e+36\n",
      "Gradient Descent(19/49): loss=7.61307537932717e+38\n",
      "Gradient Descent(20/49): loss=1.3595888110770472e+41\n",
      "Gradient Descent(21/49): loss=2.428035510217387e+43\n",
      "Gradient Descent(22/49): loss=4.33613191847558e+45\n",
      "Gradient Descent(23/49): loss=7.743725302181125e+47\n",
      "Gradient Descent(24/49): loss=1.3829210615256075e+50\n",
      "Gradient Descent(25/49): loss=2.46970364751473e+52\n",
      "Gradient Descent(26/49): loss=4.410545385592401e+54\n",
      "Gradient Descent(27/49): loss=7.876617349595303e+56\n",
      "Gradient Descent(28/49): loss=1.4066537230205334e+59\n",
      "Gradient Descent(29/49): loss=2.5120868625022688e+61\n",
      "Gradient Descent(30/49): loss=4.486235881283945e+63\n",
      "Gradient Descent(31/49): loss=8.01178999139856e+65\n",
      "Gradient Descent(32/49): loss=1.4307936667811937e+68\n",
      "Gradient Descent(33/49): loss=2.5551974266662026e+70\n",
      "Gradient Descent(34/49): loss=4.563225320901531e+72\n",
      "Gradient Descent(35/49): loss=8.149282365427433e+74\n",
      "Gradient Descent(36/49): loss=1.4553478822813808e+77\n",
      "Gradient Descent(37/49): loss=2.5990478222309148e+79\n",
      "Gradient Descent(38/49): loss=4.6415359959531535e+81\n",
      "Gradient Descent(39/49): loss=8.28913428119855e+83\n",
      "Gradient Descent(40/49): loss=1.480323478944209e+86\n",
      "Gradient Descent(41/49): loss=2.643650745632051e+88\n",
      "Gradient Descent(42/49): loss=4.7211905804976625e+90\n",
      "Gradient Descent(43/49): loss=8.431386231410563e+92\n",
      "Gradient Descent(44/49): loss=1.5057276881994765e+95\n",
      "Gradient Descent(45/49): loss=2.6890191111921575e+97\n",
      "Gradient Descent(46/49): loss=4.802212137709364e+99\n",
      "Gradient Descent(47/49): loss=8.576079403667564e+101\n",
      "Gradient Descent(48/49): loss=1.531567865577322e+104\n",
      "Gradient Descent(49/49): loss=2.735166054859488e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.849848617558887\n",
      "Gradient Descent(2/49): loss=144.27860974908145\n",
      "Gradient Descent(3/49): loss=6573.123052595055\n",
      "Gradient Descent(4/49): loss=368285.03652369906\n",
      "Gradient Descent(5/49): loss=31438374.155421913\n",
      "Gradient Descent(6/49): loss=4147154778.3242073\n",
      "Gradient Descent(7/49): loss=677958335017.2411\n",
      "Gradient Descent(8/49): loss=118414395989143.6\n",
      "Gradient Descent(9/49): loss=2.103741199013418e+16\n",
      "Gradient Descent(10/49): loss=3.753023202206665e+18\n",
      "Gradient Descent(11/49): loss=6.701988161589477e+20\n",
      "Gradient Descent(12/49): loss=1.1970989589226635e+23\n",
      "Gradient Descent(13/49): loss=2.1383627380042203e+25\n",
      "Gradient Descent(14/49): loss=3.819782876933732e+27\n",
      "Gradient Descent(15/49): loss=6.823346233232097e+29\n",
      "Gradient Descent(16/49): loss=1.2188674620446951e+32\n",
      "Gradient Descent(17/49): loss=2.1772868041445347e+34\n",
      "Gradient Descent(18/49): loss=3.8893302104418525e+36\n",
      "Gradient Descent(19/49): loss=6.947587071026451e+38\n",
      "Gradient Descent(20/49): loss=1.2410611499566729e+41\n",
      "Gradient Descent(21/49): loss=2.2169319551493187e+43\n",
      "Gradient Descent(22/49): loss=3.9601491793289303e+45\n",
      "Gradient Descent(23/49): loss=7.074092412568513e+47\n",
      "Gradient Descent(24/49): loss=1.2636590490883714e+50\n",
      "Gradient Descent(25/49): loss=2.2572990275178438e+52\n",
      "Gradient Descent(26/49): loss=4.0322576752894134e+54\n",
      "Gradient Descent(27/49): loss=7.202901238038892e+56\n",
      "Gradient Descent(28/49): loss=1.2866684230744207e+59\n",
      "Gradient Descent(29/49): loss=2.2984011250827462e+61\n",
      "Gradient Descent(30/49): loss=4.105679161037445e+63\n",
      "Gradient Descent(31/49): loss=7.334055482926482e+65\n",
      "Gradient Descent(32/49): loss=1.3100967639432473e+68\n",
      "Gradient Descent(33/49): loss=2.340251631433958e+70\n",
      "Gradient Descent(34/49): loss=4.180437544128178e+72\n",
      "Gradient Descent(35/49): loss=7.467597853846429e+74\n",
      "Gradient Descent(36/49): loss=1.3339517004649084e+77\n",
      "Gradient Descent(37/49): loss=2.3828641739949273e+79\n",
      "Gradient Descent(38/49): loss=4.2565571674967815e+81\n",
      "Gradient Descent(39/49): loss=7.603571835062889e+83\n",
      "Gradient Descent(40/49): loss=1.3582410003191036e+86\n",
      "Gradient Descent(41/49): loss=2.4262526283248906e+88\n",
      "Gradient Descent(42/49): loss=4.334062817327955e+90\n",
      "Gradient Descent(43/49): loss=7.742021702625991e+92\n",
      "Gradient Descent(44/49): loss=1.3829725726238113e+95\n",
      "Gradient Descent(45/49): loss=2.470431122636924e+97\n",
      "Gradient Descent(46/49): loss=4.412979731126595e+99\n",
      "Gradient Descent(47/49): loss=7.882992538787083e+101\n",
      "Gradient Descent(48/49): loss=1.4081544705103229e+104\n",
      "Gradient Descent(49/49): loss=2.5154140423977995e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.713369054021184\n",
      "Gradient Descent(2/49): loss=136.9088882232625\n",
      "Gradient Descent(3/49): loss=6070.78796857951\n",
      "Gradient Descent(4/49): loss=316332.65941224777\n",
      "Gradient Descent(5/49): loss=24002847.734881558\n",
      "Gradient Descent(6/49): loss=2927883276.1386895\n",
      "Gradient Descent(7/49): loss=469806701709.4956\n",
      "Gradient Descent(8/49): loss=82529760901284.25\n",
      "Gradient Descent(9/49): loss=1.4842372535166632e+16\n",
      "Gradient Descent(10/49): loss=2.6844729798570895e+18\n",
      "Gradient Descent(11/49): loss=4.8618184970024806e+20\n",
      "Gradient Descent(12/49): loss=8.807979992568495e+22\n",
      "Gradient Descent(13/49): loss=1.5958292243027759e+25\n",
      "Gradient Descent(14/49): loss=2.8913733621370735e+27\n",
      "Gradient Descent(15/49): loss=5.238702606490644e+29\n",
      "Gradient Descent(16/49): loss=9.491694255489057e+31\n",
      "Gradient Descent(17/49): loss=1.7197441323230974e+34\n",
      "Gradient Descent(18/49): loss=3.1159031911375146e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=5.645521702288493e+38\n",
      "Gradient Descent(20/49): loss=1.0228788711507978e+41\n",
      "Gradient Descent(21/49): loss=1.8532940638075325e+43\n",
      "Gradient Descent(22/49): loss=3.357874509284753e+45\n",
      "Gradient Descent(23/49): loss=6.0839353239967376e+47\n",
      "Gradient Descent(24/49): loss=1.1023124576149343e+50\n",
      "Gradient Descent(25/49): loss=1.9972151075030356e+52\n",
      "Gradient Descent(26/49): loss=3.618636583561032e+54\n",
      "Gradient Descent(27/49): loss=6.556394789271907e+56\n",
      "Gradient Descent(28/49): loss=1.1879146092778074e+59\n",
      "Gradient Descent(29/49): loss=2.1523126112611146e+61\n",
      "Gradient Descent(30/49): loss=3.8996486282880485e+63\n",
      "Gradient Descent(31/49): loss=7.065543984894776e+65\n",
      "Gradient Descent(32/49): loss=1.2801643573820652e+68\n",
      "Gradient Descent(33/49): loss=2.3194545040198634e+70\n",
      "Gradient Descent(34/49): loss=4.202483193032928e+72\n",
      "Gradient Descent(35/49): loss=7.614232121007946e+74\n",
      "Gradient Descent(36/49): loss=1.3795779335585516e+77\n",
      "Gradient Descent(37/49): loss=2.499576115509475e+79\n",
      "Gradient Descent(38/49): loss=4.52883494672121e+81\n",
      "Gradient Descent(39/49): loss=8.205529668562704e+83\n",
      "Gradient Descent(40/49): loss=1.4867116583793408e+86\n",
      "Gradient Descent(41/49): loss=2.693685410253601e+88\n",
      "Gradient Descent(42/49): loss=4.880530160988194e+90\n",
      "Gradient Descent(43/49): loss=8.842745541719697e+92\n",
      "Gradient Descent(44/49): loss=1.6021650545392846e+95\n",
      "Gradient Descent(45/49): loss=2.9028686281610925e+97\n",
      "Gradient Descent(46/49): loss=5.259536930035569e+99\n",
      "Gradient Descent(47/49): loss=9.529445614606162e+101\n",
      "Gradient Descent(48/49): loss=1.726584201798232e+104\n",
      "Gradient Descent(49/49): loss=3.1282963631483164e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7803304420600927\n",
      "Gradient Descent(2/49): loss=141.9574238544426\n",
      "Gradient Descent(3/49): loss=6620.430556361474\n",
      "Gradient Descent(4/49): loss=400048.45010691916\n",
      "Gradient Descent(5/49): loss=38815855.74802147\n",
      "Gradient Descent(6/49): loss=5676251233.136748\n",
      "Gradient Descent(7/49): loss=986215511440.9216\n",
      "Gradient Descent(8/49): loss=179831126146187.34\n",
      "Gradient Descent(9/49): loss=3.3179199621952796e+16\n",
      "Gradient Descent(10/49): loss=6.138538155199149e+18\n",
      "Gradient Descent(11/49): loss=1.136429492891253e+21\n",
      "Gradient Descent(12/49): loss=2.1041884494429045e+23\n",
      "Gradient Descent(13/49): loss=3.8962045258788956e+25\n",
      "Gradient Descent(14/49): loss=7.214435136305524e+27\n",
      "Gradient Descent(15/49): loss=1.3358685475967574e+30\n",
      "Gradient Descent(16/49): loss=2.4735762531307303e+32\n",
      "Gradient Descent(17/49): loss=4.580226176128462e+34\n",
      "Gradient Descent(18/49): loss=8.481029150573132e+36\n",
      "Gradient Descent(19/49): loss=1.57039964997434e+39\n",
      "Gradient Descent(20/49): loss=2.9078488234340925e+41\n",
      "Gradient Descent(21/49): loss=5.384352182272745e+43\n",
      "Gradient Descent(22/49): loss=9.969998505782449e+45\n",
      "Gradient Descent(23/49): loss=1.8461063994682113e+48\n",
      "Gradient Descent(24/49): loss=3.4183644422744907e+50\n",
      "Gradient Descent(25/49): loss=6.329654381558116e+52\n",
      "Gradient Descent(26/49): loss=1.1720378346589316e+55\n",
      "Gradient Descent(27/49): loss=2.1702175238420302e+57\n",
      "Gradient Descent(28/49): loss=4.018508585229835e+59\n",
      "Gradient Descent(29/49): loss=7.440918282227108e+61\n",
      "Gradient Descent(30/49): loss=1.377806310686679e+64\n",
      "Gradient Descent(31/49): loss=2.5512311219736015e+66\n",
      "Gradient Descent(32/49): loss=4.724016857262565e+68\n",
      "Gradient Descent(33/49): loss=8.747280901166267e+70\n",
      "Gradient Descent(34/49): loss=1.6197004683901094e+73\n",
      "Gradient Descent(35/49): loss=2.999137259846429e+75\n",
      "Gradient Descent(36/49): loss=5.553387480550413e+77\n",
      "Gradient Descent(37/49): loss=1.0282994687183122e+80\n",
      "Gradient Descent(38/49): loss=1.9040627023950904e+82\n",
      "Gradient Descent(39/49): loss=3.5256799064292355e+84\n",
      "Gradient Descent(40/49): loss=6.528366312182395e+86\n",
      "Gradient Descent(41/49): loss=1.2088325610137946e+89\n",
      "Gradient Descent(42/49): loss=2.238348907965421e+91\n",
      "Gradient Descent(43/49): loss=4.1446648571313424e+93\n",
      "Gradient Descent(44/49): loss=7.674517014219208e+95\n",
      "Gradient Descent(45/49): loss=1.4210608922986946e+98\n",
      "Gradient Descent(46/49): loss=2.6313239724131355e+100\n",
      "Gradient Descent(47/49): loss=4.872321717752779e+102\n",
      "Gradient Descent(48/49): loss=9.021891325496682e+104\n",
      "Gradient Descent(49/49): loss=1.6705490278382414e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8117805672287184\n",
      "Gradient Descent(2/49): loss=144.8031302021472\n",
      "Gradient Descent(3/49): loss=6739.485022343282\n",
      "Gradient Descent(4/49): loss=391465.57519259007\n",
      "Gradient Descent(5/49): loss=35004615.66192675\n",
      "Gradient Descent(6/49): loss=4768850637.224788\n",
      "Gradient Descent(7/49): loss=792734581803.9503\n",
      "Gradient Descent(8/49): loss=139983508946166.03\n",
      "Gradient Descent(9/49): loss=2.5104610524754972e+16\n",
      "Gradient Descent(10/49): loss=4.519340179743468e+18\n",
      "Gradient Descent(11/49): loss=8.143179164991865e+20\n",
      "Gradient Descent(12/49): loss=1.4676033109285307e+23\n",
      "Gradient Descent(13/49): loss=2.645126348806223e+25\n",
      "Gradient Descent(14/49): loss=4.767489084340672e+27\n",
      "Gradient Descent(15/49): loss=8.592792570412655e+29\n",
      "Gradient Descent(16/49): loss=1.5487426919739387e+32\n",
      "Gradient Descent(17/49): loss=2.791414238594999e+34\n",
      "Gradient Descent(18/49): loss=5.0311738842935177e+36\n",
      "Gradient Descent(19/49): loss=9.068059682825815e+38\n",
      "Gradient Descent(20/49): loss=1.6344039881451103e+41\n",
      "Gradient Descent(21/49): loss=2.9458081348093614e+43\n",
      "Gradient Descent(22/49): loss=5.309449579969031e+45\n",
      "Gradient Descent(23/49): loss=9.56961674119143e+47\n",
      "Gradient Descent(24/49): loss=1.7248033566331802e+50\n",
      "Gradient Descent(25/49): loss=3.1087416555082235e+52\n",
      "Gradient Descent(26/49): loss=5.6031168095367004e+54\n",
      "Gradient Descent(27/49): loss=1.0098915078932024e+57\n",
      "Gradient Descent(28/49): loss=1.820202741408104e+59\n",
      "Gradient Descent(29/49): loss=3.280687077705365e+61\n",
      "Gradient Descent(30/49): loss=5.913026860676306e+63\n",
      "Gradient Descent(31/49): loss=1.0657489064618206e+66\n",
      "Gradient Descent(32/49): loss=1.920878694426717e+68\n",
      "Gradient Descent(33/49): loss=3.4621428521584615e+70\n",
      "Gradient Descent(34/49): loss=6.24007812858217e+72\n",
      "Gradient Descent(35/49): loss=1.1246957942978392e+75\n",
      "Gradient Descent(36/49): loss=2.0271230642406297e+77\n",
      "Gradient Descent(37/49): loss=3.653634999268182e+79\n",
      "Gradient Descent(38/49): loss=6.5852186990324985e+81\n",
      "Gradient Descent(39/49): loss=1.1869030519680934e+84\n",
      "Gradient Descent(40/49): loss=2.1392438416329577e+86\n",
      "Gradient Descent(41/49): loss=3.855718633780634e+88\n",
      "Gradient Descent(42/49): loss=6.949449096712353e+90\n",
      "Gradient Descent(43/49): loss=1.2525510114943577e+93\n",
      "Gradient Descent(44/49): loss=2.2575660524483242e+95\n",
      "Gradient Descent(45/49): loss=4.068979574002734e+97\n",
      "Gradient Descent(46/49): loss=7.333825185622353e+99\n",
      "Gradient Descent(47/49): loss=1.3218299791158478e+102\n",
      "Gradient Descent(48/49): loss=2.3824326998069472e+104\n",
      "Gradient Descent(49/49): loss=4.29403604002531e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.884756047617871\n",
      "Gradient Descent(2/49): loss=147.15011037758185\n",
      "Gradient Descent(3/49): loss=6769.455005947235\n",
      "Gradient Descent(4/49): loss=382799.43957241287\n",
      "Gradient Descent(5/49): loss=32956910.26462776\n",
      "Gradient Descent(6/49): loss=4384597716.374171\n",
      "Gradient Descent(7/49): loss=723178050439.1425\n",
      "Gradient Descent(8/49): loss=127468119451397.33\n",
      "Gradient Descent(9/49): loss=2.2854663112687096e+16\n",
      "Gradient Descent(10/49): loss=4.1148879210804357e+18\n",
      "Gradient Descent(11/49): loss=7.41612579888627e+20\n",
      "Gradient Descent(12/49): loss=1.3369057968726406e+23\n",
      "Gradient Descent(13/49): loss=2.410180818330963e+25\n",
      "Gradient Descent(14/49): loss=4.3451469157526375e+27\n",
      "Gradient Descent(15/49): loss=7.833588393404371e+29\n",
      "Gradient Descent(16/49): loss=1.4122688422371483e+32\n",
      "Gradient Descent(17/49): loss=2.546091987389737e+34\n",
      "Gradient Descent(18/49): loss=4.590191691282358e+36\n",
      "Gradient Descent(19/49): loss=8.275372649049643e+38\n",
      "Gradient Descent(20/49): loss=1.4919157470329004e+41\n",
      "Gradient Descent(21/49): loss=2.689682618592412e+43\n",
      "Gradient Descent(22/49): loss=4.8490624247670316e+45\n",
      "Gradient Descent(23/49): loss=8.74207471082506e+47\n",
      "Gradient Descent(24/49): loss=1.5760545762408363e+50\n",
      "Gradient Descent(25/49): loss=2.841371309969508e+52\n",
      "Gradient Descent(26/49): loss=5.1225325841053196e+54\n",
      "Gradient Descent(27/49): loss=9.235097145929017e+56\n",
      "Gradient Descent(28/49): loss=1.6649385415211488e+59\n",
      "Gradient Descent(29/49): loss=3.001614713132235e+61\n",
      "Gradient Descent(30/49): loss=5.411425503946892e+63\n",
      "Gradient Descent(31/49): loss=9.755924321882331e+65\n",
      "Gradient Descent(32/49): loss=1.7588352515409364e+68\n",
      "Gradient Descent(33/49): loss=3.170895283724611e+70\n",
      "Gradient Descent(34/49): loss=5.716610973960335e+72\n",
      "Gradient Descent(35/49): loss=1.0306124328778733e+75\n",
      "Gradient Descent(36/49): loss=1.8580274075683313e+77\n",
      "Gradient Descent(37/49): loss=3.3497226863789396e+79\n",
      "Gradient Descent(38/49): loss=6.039007836986984e+81\n",
      "Gradient Descent(39/49): loss=1.0887353691541796e+84\n",
      "Gradient Descent(40/49): loss=1.9628136542354138e+86\n",
      "Gradient Descent(41/49): loss=3.538635329029652e+88\n",
      "Gradient Descent(42/49): loss=6.379586755388948e+90\n",
      "Gradient Descent(43/49): loss=1.1501362357305802e+93\n",
      "Gradient Descent(44/49): loss=2.0735094786869634e+95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=3.738201983935972e+97\n",
      "Gradient Descent(46/49): loss=6.739373133491523e+99\n",
      "Gradient Descent(47/49): loss=1.2149998964102433e+102\n",
      "Gradient Descent(48/49): loss=2.1904481604390237e+104\n",
      "Gradient Descent(49/49): loss=3.94902350012272e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.746909133531059\n",
      "Gradient Descent(2/49): loss=139.63566644472482\n",
      "Gradient Descent(3/49): loss=6252.513402956188\n",
      "Gradient Descent(4/49): loss=328864.55082330585\n",
      "Gradient Descent(5/49): loss=25169005.201076988\n",
      "Gradient Descent(6/49): loss=3095922429.4434214\n",
      "Gradient Descent(7/49): loss=501152606042.86395\n",
      "Gradient Descent(8/49): loss=88837847781032.56\n",
      "Gradient Descent(9/49): loss=1.6123886625905098e+16\n",
      "Gradient Descent(10/49): loss=2.9431782449179725e+18\n",
      "Gradient Descent(11/49): loss=5.3796086548544736e+20\n",
      "Gradient Descent(12/49): loss=9.83611387561416e+22\n",
      "Gradient Descent(13/49): loss=1.7985775033575576e+25\n",
      "Gradient Descent(14/49): loss=3.288838076065879e+27\n",
      "Gradient Descent(15/49): loss=6.013920038989115e+29\n",
      "Gradient Descent(16/49): loss=1.0996974984172048e+32\n",
      "Gradient Descent(17/49): loss=2.010892833154404e+34\n",
      "Gradient Descent(18/49): loss=3.677093214394183e+36\n",
      "Gradient Descent(19/49): loss=6.723886256018844e+38\n",
      "Gradient Descent(20/49): loss=1.229521360666303e+41\n",
      "Gradient Descent(21/49): loss=2.2482872551208797e+43\n",
      "Gradient Descent(22/49): loss=4.111189723180488e+45\n",
      "Gradient Descent(23/49): loss=7.517669684853892e+47\n",
      "Gradient Descent(24/49): loss=1.3746715986482804e+50\n",
      "Gradient Descent(25/49): loss=2.513707150424249e+52\n",
      "Gradient Descent(26/49): loss=4.596533196954522e+54\n",
      "Gradient Descent(27/49): loss=8.405162640819776e+56\n",
      "Gradient Descent(28/49): loss=1.5369574414352918e+59\n",
      "Gradient Descent(29/49): loss=2.8104609960919046e+61\n",
      "Gradient Descent(30/49): loss=5.139173537022385e+63\n",
      "Gradient Descent(31/49): loss=9.397427923873442e+65\n",
      "Gradient Descent(32/49): loss=1.7184018198296572e+68\n",
      "Gradient Descent(33/49): loss=3.1422478983768284e+70\n",
      "Gradient Descent(34/49): loss=5.745874882646702e+72\n",
      "Gradient Descent(35/49): loss=1.0506834353866456e+75\n",
      "Gradient Descent(36/49): loss=1.9212664806362143e+77\n",
      "Gradient Descent(37/49): loss=3.513203658966955e+79\n",
      "Gradient Descent(38/49): loss=6.424199908641151e+81\n",
      "Gradient Descent(39/49): loss=1.1747210942596267e+84\n",
      "Gradient Descent(40/49): loss=2.148080179513588e+86\n",
      "Gradient Descent(41/49): loss=3.927952328571566e+88\n",
      "Gradient Descent(42/49): loss=7.182604095823034e+90\n",
      "Gradient Descent(43/49): loss=1.3134019275660113e+93\n",
      "Gradient Descent(44/49): loss=2.4016702025068057e+95\n",
      "Gradient Descent(45/49): loss=4.3916638467999e+97\n",
      "Gradient Descent(46/49): loss=8.030541130567707e+99\n",
      "Gradient Descent(47/49): loss=1.4684546244752207e+102\n",
      "Gradient Descent(48/49): loss=2.685197608831846e+104\n",
      "Gradient Descent(49/49): loss=4.910118486673019e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8143723660539295\n",
      "Gradient Descent(2/49): loss=144.7691626629235\n",
      "Gradient Descent(3/49): loss=6816.924588110834\n",
      "Gradient Descent(4/49): loss=415663.4685541289\n",
      "Gradient Descent(5/49): loss=40671298.04007614\n",
      "Gradient Descent(6/49): loss=5998688538.102495\n",
      "Gradient Descent(7/49): loss=1051578387021.6267\n",
      "Gradient Descent(8/49): loss=193499805549791.25\n",
      "Gradient Descent(9/49): loss=3.6028740474990624e+16\n",
      "Gradient Descent(10/49): loss=6.727002606554984e+18\n",
      "Gradient Descent(11/49): loss=1.2568231493880623e+21\n",
      "Gradient Descent(12/49): loss=2.3485064447118164e+23\n",
      "Gradient Descent(13/49): loss=4.388584091326324e+25\n",
      "Gradient Descent(14/49): loss=8.200882542804302e+27\n",
      "Gradient Descent(15/49): loss=1.5324897230536824e+30\n",
      "Gradient Descent(16/49): loss=2.8637475995171917e+32\n",
      "Gradient Descent(17/49): loss=5.351455879471018e+34\n",
      "Gradient Descent(18/49): loss=1.0000211157927656e+37\n",
      "Gradient Descent(19/49): loss=1.8687292952953632e+39\n",
      "Gradient Descent(20/49): loss=3.49207544553898e+41\n",
      "Gradient Descent(21/49): loss=6.525605903214267e+43\n",
      "Gradient Descent(22/49): loss=1.2194333447561142e+46\n",
      "Gradient Descent(23/49): loss=2.2787427012623043e+48\n",
      "Gradient Descent(24/49): loss=4.258263332641384e+50\n",
      "Gradient Descent(25/49): loss=7.95737342354998e+52\n",
      "Gradient Descent(26/49): loss=1.48698628655632e+55\n",
      "Gradient Descent(27/49): loss=2.778716165139087e+57\n",
      "Gradient Descent(28/49): loss=5.192558664603853e+59\n",
      "Gradient Descent(29/49): loss=9.703281617467025e+61\n",
      "Gradient Descent(30/49): loss=1.8132423768207613e+64\n",
      "Gradient Descent(31/49): loss=3.388387606085914e+66\n",
      "Gradient Descent(32/49): loss=6.33184549172469e+68\n",
      "Gradient Descent(33/49): loss=1.1832255335565283e+71\n",
      "Gradient Descent(34/49): loss=2.211081532374535e+73\n",
      "Gradient Descent(35/49): loss=4.131825593817901e+75\n",
      "Gradient Descent(36/49): loss=7.7211005056853e+77\n",
      "Gradient Descent(37/49): loss=1.4428342064604852e+80\n",
      "Gradient Descent(38/49): loss=2.6962096216719386e+82\n",
      "Gradient Descent(39/49): loss=5.038379525136118e+84\n",
      "Gradient Descent(40/49): loss=9.415168626083461e+86\n",
      "Gradient Descent(41/49): loss=1.7594029948585762e+89\n",
      "Gradient Descent(42/49): loss=3.28777850004901e+91\n",
      "Gradient Descent(43/49): loss=6.143838277513915e+93\n",
      "Gradient Descent(44/49): loss=1.1480928164620007e+96\n",
      "Gradient Descent(45/49): loss=2.1454293809065628e+98\n",
      "Gradient Descent(46/49): loss=4.009142085429493e+100\n",
      "Gradient Descent(47/49): loss=7.49184308008772e+102\n",
      "Gradient Descent(48/49): loss=1.3999931042764629e+105\n",
      "Gradient Descent(49/49): loss=2.616152889308402e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.846121773023263\n",
      "Gradient Descent(2/49): loss=147.66962172155127\n",
      "Gradient Descent(3/49): loss=6939.609519387119\n",
      "Gradient Descent(4/49): loss=406790.738461204\n",
      "Gradient Descent(5/49): loss=36684435.68621196\n",
      "Gradient Descent(6/49): loss=5040402027.46316\n",
      "Gradient Descent(7/49): loss=845349454557.8523\n",
      "Gradient Descent(8/49): loss=150635081813099.28\n",
      "Gradient Descent(9/49): loss=2.7262838826795064e+16\n",
      "Gradient Descent(10/49): loss=4.953006546642617e+18\n",
      "Gradient Descent(11/49): loss=9.00671314506692e+20\n",
      "Gradient Descent(12/49): loss=1.6381742196481215e+23\n",
      "Gradient Descent(13/49): loss=2.9797309148945204e+25\n",
      "Gradient Descent(14/49): loss=5.420003850936464e+27\n",
      "Gradient Descent(15/49): loss=9.858787126662646e+29\n",
      "Gradient Descent(16/49): loss=1.7932783596749342e+32\n",
      "Gradient Descent(17/49): loss=3.2619102248672945e+34\n",
      "Gradient Descent(18/49): loss=5.933300158501631e+36\n",
      "Gradient Descent(19/49): loss=1.0792464755626424e+39\n",
      "Gradient Descent(20/49): loss=1.963111467182286e+41\n",
      "Gradient Descent(21/49): loss=3.570830873368171e+43\n",
      "Gradient Descent(22/49): loss=6.495216059410969e+45\n",
      "Gradient Descent(23/49): loss=1.1814570097551667e+48\n",
      "Gradient Descent(24/49): loss=2.1490288439119074e+50\n",
      "Gradient Descent(25/49): loss=3.909008058559653e+52\n",
      "Gradient Descent(26/49): loss=7.11034849307864e+54\n",
      "Gradient Descent(27/49): loss=1.2933474409787165e+57\n",
      "Gradient Descent(28/49): loss=2.35255361212533e+59\n",
      "Gradient Descent(29/49): loss=4.279212470344384e+61\n",
      "Gradient Descent(30/49): loss=7.78373732780016e+63\n",
      "Gradient Descent(31/49): loss=1.4158345071216391e+66\n",
      "Gradient Descent(32/49): loss=2.575353287419982e+68\n",
      "Gradient Descent(33/49): loss=4.684477261759038e+70\n",
      "Gradient Descent(34/49): loss=8.520899762813293e+72\n",
      "Gradient Descent(35/49): loss=1.5499217673787504e+75\n",
      "Gradient Descent(36/49): loss=2.819253308762437e+77\n",
      "Gradient Descent(37/49): loss=5.1281228422320934e+79\n",
      "Gradient Descent(38/49): loss=9.327875506357769e+81\n",
      "Gradient Descent(39/49): loss=1.6967078234856919e+84\n",
      "Gradient Descent(40/49): loss=3.0862519941603536e+86\n",
      "Gradient Descent(41/49): loss=5.6137840820999466e+88\n",
      "Gradient Descent(42/49): loss=1.0211276260029762e+91\n",
      "Gradient Descent(43/49): loss=1.8573953207627445e+93\n",
      "Gradient Descent(44/49): loss=3.378536913250944e+95\n",
      "Gradient Descent(45/49): loss=6.145440093771539e+97\n",
      "Gradient Descent(46/49): loss=1.1178339889675885e+100\n",
      "Gradient Descent(47/49): loss=2.0333008016099895e+102\n",
      "Gradient Descent(48/49): loss=3.6985028104631416e+104\n",
      "Gradient Descent(49/49): loss=6.727446833332579e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9198389511586136\n",
      "Gradient Descent(2/49): loss=150.06438884677831\n",
      "Gradient Descent(3/49): loss=6970.652350291994\n",
      "Gradient Descent(4/49): loss=397812.453444431\n",
      "Gradient Descent(5/49): loss=34541221.04045531\n",
      "Gradient Descent(6/49): loss=4634450217.050483\n",
      "Gradient Descent(7/49): loss=771185710859.4749\n",
      "Gradient Descent(8/49): loss=137167767876240.78\n",
      "Gradient Descent(9/49): loss=2.481946950363237e+16\n",
      "Gradient Descent(10/49): loss=4.50974129852781e+18\n",
      "Gradient Descent(11/49): loss=8.202553392007921e+20\n",
      "Gradient Descent(12/49): loss=1.492285064987896e+23\n",
      "Gradient Descent(13/49): loss=2.7150622568325457e+25\n",
      "Gradient Descent(14/49): loss=4.939851094433248e+27\n",
      "Gradient Descent(15/49): loss=8.987716786395218e+29\n",
      "Gradient Descent(16/49): loss=1.635254109085469e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=2.9752345128087817e+34\n",
      "Gradient Descent(18/49): loss=5.413238693029022e+36\n",
      "Gradient Descent(19/49): loss=9.849023109701543e+38\n",
      "Gradient Descent(20/49): loss=1.7919634062897775e+41\n",
      "Gradient Descent(21/49): loss=3.2603567031680755e+43\n",
      "Gradient Descent(22/49): loss=5.931999390286181e+45\n",
      "Gradient Descent(23/49): loss=1.079287328698098e+48\n",
      "Gradient Descent(24/49): loss=1.9636905893923002e+50\n",
      "Gradient Descent(25/49): loss=3.5728027452407365e+52\n",
      "Gradient Descent(26/49): loss=6.500473916492412e+54\n",
      "Gradient Descent(27/49): loss=1.1827174392790786e+57\n",
      "Gradient Descent(28/49): loss=2.151874708128459e+59\n",
      "Gradient Descent(29/49): loss=3.915191072438618e+61\n",
      "Gradient Descent(30/49): loss=7.123426413164798e+63\n",
      "Gradient Descent(31/49): loss=1.2960594495881852e+66\n",
      "Gradient Descent(32/49): loss=2.3580928607087557e+68\n",
      "Gradient Descent(33/49): loss=4.2903911093681115e+70\n",
      "Gradient Descent(34/49): loss=7.806077605362979e+72\n",
      "Gradient Descent(35/49): loss=1.4202632353935497e+75\n",
      "Gradient Descent(36/49): loss=2.5840732821112487e+77\n",
      "Gradient Descent(37/49): loss=4.701547263152868e+79\n",
      "Gradient Descent(38/49): loss=8.554148530029426e+81\n",
      "Gradient Descent(39/49): loss=1.5563697008277116e+84\n",
      "Gradient Descent(40/49): loss=2.831709827285654e+86\n",
      "Gradient Descent(41/49): loss=5.152105275296515e+88\n",
      "Gradient Descent(42/49): loss=9.373908481711249e+90\n",
      "Gradient Descent(43/49): loss=1.705519501800967e+93\n",
      "Gradient Descent(44/49): loss=3.103077842821443e+95\n",
      "Gradient Descent(45/49): loss=5.645841099114523e+97\n",
      "Gradient Descent(46/49): loss=1.0272227553102025e+100\n",
      "Gradient Descent(47/49): loss=1.8689626053991747e+102\n",
      "Gradient Descent(48/49): loss=3.400451559628498e+104\n",
      "Gradient Descent(49/49): loss=6.1868925445462436e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.7806180061692785\n",
      "Gradient Descent(2/49): loss=142.40310117018984\n",
      "Gradient Descent(3/49): loss=6438.751631496955\n",
      "Gradient Descent(4/49): loss=341829.1151402632\n",
      "Gradient Descent(5/49): loss=26385973.32995416\n",
      "Gradient Descent(6/49): loss=3272766405.2436094\n",
      "Gradient Descent(7/49): loss=534431807845.17194\n",
      "Gradient Descent(8/49): loss=95595842942941.23\n",
      "Gradient Descent(9/49): loss=1.7509407832787218e+16\n",
      "Gradient Descent(10/49): loss=3.225456045418006e+18\n",
      "Gradient Descent(11/49): loss=5.94978632143723e+20\n",
      "Gradient Descent(12/49): loss=1.0978707755144455e+23\n",
      "Gradient Descent(13/49): loss=2.0259748603101992e+25\n",
      "Gradient Descent(14/49): loss=3.7387348323249953e+27\n",
      "Gradient Descent(15/49): loss=6.899491980272736e+29\n",
      "Gradient Descent(16/49): loss=1.2732391913579974e+32\n",
      "Gradient Descent(17/49): loss=2.3496489675327504e+34\n",
      "Gradient Descent(18/49): loss=4.336067107844133e+36\n",
      "Gradient Descent(19/49): loss=8.001824302568563e+38\n",
      "Gradient Descent(20/49): loss=1.4766651616640322e+41\n",
      "Gradient Descent(21/49): loss=2.725053586305589e+43\n",
      "Gradient Descent(22/49): loss=5.0288428563948994e+45\n",
      "Gradient Descent(23/49): loss=9.280280065843366e+47\n",
      "Gradient Descent(24/49): loss=1.7125927486837845e+50\n",
      "Gradient Descent(25/49): loss=3.1604368640186555e+52\n",
      "Gradient Descent(26/49): loss=5.83230378566657e+54\n",
      "Gradient Descent(27/49): loss=1.076299540597437e+57\n",
      "Gradient Descent(28/49): loss=1.9862146137470485e+59\n",
      "Gradient Descent(29/49): loss=3.66538156252723e+61\n",
      "Gradient Descent(30/49): loss=6.76413410007539e+63\n",
      "Gradient Descent(31/49): loss=1.2482604973943014e+66\n",
      "Gradient Descent(32/49): loss=2.303553191439124e+68\n",
      "Gradient Descent(33/49): loss=4.251001547246116e+70\n",
      "Gradient Descent(34/49): loss=7.844843445268674e+72\n",
      "Gradient Descent(35/49): loss=1.4476957487969829e+75\n",
      "Gradient Descent(36/49): loss=2.6715931244603914e+77\n",
      "Gradient Descent(37/49): loss=4.930186352066926e+79\n",
      "Gradient Descent(38/49): loss=9.098218304112644e+81\n",
      "Gradient Descent(39/49): loss=1.6789948776396e+84\n",
      "Gradient Descent(40/49): loss=3.0984349956361766e+86\n",
      "Gradient Descent(41/49): loss=5.717884878648002e+88\n",
      "Gradient Descent(42/49): loss=1.0551845538640625e+91\n",
      "Gradient Descent(43/49): loss=1.947248792767864e+93\n",
      "Gradient Descent(44/49): loss=3.593473622268711e+95\n",
      "Gradient Descent(45/49): loss=6.631434422709935e+97\n",
      "Gradient Descent(46/49): loss=1.2237719578678181e+100\n",
      "Gradient Descent(47/49): loss=2.258361780273254e+102\n",
      "Gradient Descent(48/49): loss=4.167604836676497e+104\n",
      "Gradient Descent(49/49): loss=7.690942269040449e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.848584702114413\n",
      "Gradient Descent(2/49): loss=147.62259272786292\n",
      "Gradient Descent(3/49): loss=7018.257948953264\n",
      "Gradient Descent(4/49): loss=431809.86661279877\n",
      "Gradient Descent(5/49): loss=42606270.87318092\n",
      "Gradient Descent(6/49): loss=6337843677.132727\n",
      "Gradient Descent(7/49): loss=1120945455807.3997\n",
      "Gradient Descent(8/49): loss=208137937854180.3\n",
      "Gradient Descent(9/49): loss=3.910832554309084e+16\n",
      "Gradient Descent(10/49): loss=7.368805502899982e+18\n",
      "Gradient Descent(11/49): loss=1.3893337823133413e+21\n",
      "Gradient Descent(12/49): loss=2.6198807043889936e+23\n",
      "Gradient Descent(13/49): loss=4.940508203728345e+25\n",
      "Gradient Descent(14/49): loss=9.31676752823007e+27\n",
      "Gradient Descent(15/49): loss=1.7569512524927284e+30\n",
      "Gradient Descent(16/49): loss=3.313251130036431e+32\n",
      "Gradient Descent(17/49): loss=6.248115393058257e+34\n",
      "Gradient Descent(18/49): loss=1.1782670661881655e+37\n",
      "Gradient Descent(19/49): loss=2.2219712657738376e+39\n",
      "Gradient Descent(20/49): loss=4.190184427516165e+41\n",
      "Gradient Descent(21/49): loss=7.9018328509582e+43\n",
      "Gradient Descent(22/49): loss=1.4901244441350173e+46\n",
      "Gradient Descent(23/49): loss=2.8100706518675095e+48\n",
      "Gradient Descent(24/49): loss=5.299219873612571e+50\n",
      "Gradient Descent(25/49): loss=9.993247411858288e+52\n",
      "Gradient Descent(26/49): loss=1.8845225564598864e+55\n",
      "Gradient Descent(27/49): loss=3.5538250174732844e+57\n",
      "Gradient Descent(28/49): loss=6.701788849131295e+59\n",
      "Gradient Descent(29/49): loss=1.2638206315029424e+62\n",
      "Gradient Descent(30/49): loss=2.3833078370106264e+64\n",
      "Gradient Descent(31/49): loss=4.4944322828481385e+66\n",
      "Gradient Descent(32/49): loss=8.475582227113293e+68\n",
      "Gradient Descent(33/49): loss=1.5983218695429365e+71\n",
      "Gradient Descent(34/49): loss=3.0141089192515895e+73\n",
      "Gradient Descent(35/49): loss=5.683994413284207e+75\n",
      "Gradient Descent(36/49): loss=1.0718853683053805e+78\n",
      "Gradient Descent(37/49): loss=2.0213570937050608e+80\n",
      "Gradient Descent(38/49): loss=3.8118670345612136e+82\n",
      "Gradient Descent(39/49): loss=7.188403441640603e+84\n",
      "Gradient Descent(40/49): loss=1.3555862146104357e+87\n",
      "Gradient Descent(41/49): loss=2.5563590026082514e+89\n",
      "Gradient Descent(42/49): loss=4.820771471251836e+91\n",
      "Gradient Descent(43/49): loss=9.090991349151113e+93\n",
      "Gradient Descent(44/49): loss=1.71437547295471e+96\n",
      "Gradient Descent(45/49): loss=3.2329623353377496e+98\n",
      "Gradient Descent(46/49): loss=6.096707300471589e+100\n",
      "Gradient Descent(47/49): loss=1.1497145977031596e+103\n",
      "Gradient Descent(48/49): loss=2.1681271398243333e+105\n",
      "Gradient Descent(49/49): loss=4.0886453941123755e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8806347941147203\n",
      "Gradient Descent(2/49): loss=150.578589075962\n",
      "Gradient Descent(3/49): loss=7144.664695645425\n",
      "Gradient Descent(4/49): loss=422638.8866432477\n",
      "Gradient Descent(5/49): loss=38436542.467552274\n",
      "Gradient Descent(6/49): loss=5326068039.783644\n",
      "Gradient Descent(7/49): loss=901192257139.5413\n",
      "Gradient Descent(8/49): loss=162042948903258.34\n",
      "Gradient Descent(9/49): loss=2.9595475677576944e+16\n",
      "Gradient Descent(10/49): loss=5.426018822833437e+18\n",
      "Gradient Descent(11/49): loss=9.957241766386365e+20\n",
      "Gradient Descent(12/49): loss=1.8276529200627347e+23\n",
      "Gradient Descent(13/49): loss=3.3548395810699303e+25\n",
      "Gradient Descent(14/49): loss=6.158223149735668e+27\n",
      "Gradient Descent(15/49): loss=1.1304215872577659e+30\n",
      "Gradient Descent(16/49): loss=2.075036736989906e+32\n",
      "Gradient Descent(17/49): loss=3.8090021365779874e+34\n",
      "Gradient Descent(18/49): loss=6.991923396927113e+36\n",
      "Gradient Descent(19/49): loss=1.2834593300479608e+39\n",
      "Gradient Descent(20/49): loss=2.355958099875877e+41\n",
      "Gradient Descent(21/49): loss=4.3246703980562455e+43\n",
      "Gradient Descent(22/49): loss=7.938500288085643e+45\n",
      "Gradient Descent(23/49): loss=1.4572159500178184e+48\n",
      "Gradient Descent(24/49): loss=2.674911189701043e+50\n",
      "Gradient Descent(25/49): loss=4.910150669648623e+52\n",
      "Gradient Descent(26/49): loss=9.013226192888067e+54\n",
      "Gradient Descent(27/49): loss=1.6544959996102507e+57\n",
      "Gradient Descent(28/49): loss=3.037044621033013e+59\n",
      "Gradient Descent(29/49): loss=5.574894126258596e+61\n",
      "Gradient Descent(30/49): loss=1.0233450079644019e+64\n",
      "Gradient Descent(31/49): loss=1.8784841139726875e+66\n",
      "Gradient Descent(32/49): loss=3.4482042116636613e+68\n",
      "Gradient Descent(33/49): loss=6.329631534753503e+70\n",
      "Gradient Descent(34/49): loss=1.1618869680115327e+73\n",
      "Gradient Descent(35/49): loss=2.1327960703918384e+75\n",
      "Gradient Descent(36/49): loss=3.915027195514339e+77\n",
      "Gradient Descent(37/49): loss=7.186546409381298e+79\n",
      "Gradient Descent(38/49): loss=1.3191849434243055e+82\n",
      "Gradient Descent(39/49): loss=2.4215371554348196e+84\n",
      "Gradient Descent(40/49): loss=4.4450493650496636e+86\n",
      "Gradient Descent(41/49): loss=8.159471686562119e+88\n",
      "Gradient Descent(42/49): loss=1.4977781512908937e+91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=2.7493684354330415e+93\n",
      "Gradient Descent(44/49): loss=5.0468267194582014e+95\n",
      "Gradient Descent(45/49): loss=9.264113026097946e+97\n",
      "Gradient Descent(46/49): loss=1.7005495716629164e+100\n",
      "Gradient Descent(47/49): loss=3.121581998769093e+102\n",
      "Gradient Descent(48/49): loss=5.730073581748348e+104\n",
      "Gradient Descent(49/49): loss=1.0518302343234386e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9550973281811115\n",
      "Gradient Descent(2/49): loss=153.02186812271447\n",
      "Gradient Descent(3/49): loss=7176.811114541016\n",
      "Gradient Descent(4/49): loss=413338.7035449783\n",
      "Gradient Descent(5/49): loss=36193829.83589622\n",
      "Gradient Descent(6/49): loss=4897299440.986601\n",
      "Gradient Descent(7/49): loss=822139262059.8826\n",
      "Gradient Descent(8/49): loss=147556151258817.0\n",
      "Gradient Descent(9/49): loss=2.694305435052754e+16\n",
      "Gradient Descent(10/49): loss=4.940418912572744e+18\n",
      "Gradient Descent(11/49): loss=9.068207046380603e+20\n",
      "Gradient Descent(12/49): loss=1.6648879725692635e+23\n",
      "Gradient Descent(13/49): loss=3.0568494463444547e+25\n",
      "Gradient Descent(14/49): loss=5.612665939381095e+27\n",
      "Gradient Descent(15/49): loss=1.0305422605937054e+30\n",
      "Gradient Descent(16/49): loss=1.8921814054828004e+32\n",
      "Gradient Descent(17/49): loss=3.4742400246547814e+34\n",
      "Gradient Descent(18/49): loss=6.379062958068669e+36\n",
      "Gradient Descent(19/49): loss=1.1712617548422216e+39\n",
      "Gradient Descent(20/49): loss=2.150557395459132e+41\n",
      "Gradient Descent(21/49): loss=3.9486452067951954e+43\n",
      "Gradient Descent(22/49): loss=7.250119901238049e+45\n",
      "Gradient Descent(23/49): loss=1.3311967986851373e+48\n",
      "Gradient Descent(24/49): loss=2.444214635047405e+50\n",
      "Gradient Descent(25/49): loss=4.487830190167018e+52\n",
      "Gradient Descent(26/49): loss=8.240119147880718e+54\n",
      "Gradient Descent(27/49): loss=1.5129708722056797e+57\n",
      "Gradient Descent(28/49): loss=2.7779705839952587e+59\n",
      "Gradient Descent(29/49): loss=5.1006405392938063e+61\n",
      "Gradient Descent(30/49): loss=9.365302160137e+63\n",
      "Gradient Descent(31/49): loss=1.71956607949499e+66\n",
      "Gradient Descent(32/49): loss=3.1573006948304323e+68\n",
      "Gradient Descent(33/49): loss=5.797129750607935e+70\n",
      "Gradient Descent(34/49): loss=1.0644128194824822e+73\n",
      "Gradient Descent(35/49): loss=1.954371730527207e+75\n",
      "Gradient Descent(36/49): loss=3.588428090278908e+77\n",
      "Gradient Descent(37/49): loss=6.588724119351218e+79\n",
      "Gradient Descent(38/49): loss=1.209757710862916e+82\n",
      "Gradient Descent(39/49): loss=2.2212399433965166e+84\n",
      "Gradient Descent(40/49): loss=4.078425656506672e+86\n",
      "Gradient Descent(41/49): loss=7.488410194090541e+88\n",
      "Gradient Descent(42/49): loss=1.3749493544279117e+91\n",
      "Gradient Descent(43/49): loss=2.524548840464973e+93\n",
      "Gradient Descent(44/49): loss=4.635332077772975e+95\n",
      "Gradient Descent(45/49): loss=8.510947828315133e+97\n",
      "Gradient Descent(46/49): loss=1.5626978115256298e+100\n",
      "Gradient Descent(47/49): loss=2.8692743739099987e+102\n",
      "Gradient Descent(48/49): loss=5.268283715543883e+104\n",
      "Gradient Descent(49/49): loss=9.673112324090293e+106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8144956719358407\n",
      "Gradient Descent(2/49): loss=145.21159472740896\n",
      "Gradient Descent(3/49): loss=6629.591899515847\n",
      "Gradient Descent(4/49): loss=355239.10525457864\n",
      "Gradient Descent(5/49): loss=27655713.072264224\n",
      "Gradient Descent(6/49): loss=3458832938.0227065\n",
      "Gradient Descent(7/49): loss=569753881749.0522\n",
      "Gradient Descent(8/49): loss=102833556668501.64\n",
      "Gradient Descent(9/49): loss=1.900684343375984e+16\n",
      "Gradient Descent(10/49): loss=3.533331578239846e+18\n",
      "Gradient Descent(11/49): loss=6.577375437632387e+20\n",
      "Gradient Descent(12/49): loss=1.2247892072904281e+23\n",
      "Gradient Descent(13/49): loss=2.2808843616227418e+25\n",
      "Gradient Descent(14/49): loss=4.247692090633701e+27\n",
      "Gradient Descent(15/49): loss=7.910512862011058e+29\n",
      "Gradient Descent(16/49): loss=1.4731829774688537e+32\n",
      "Gradient Descent(17/49): loss=2.743524522053894e+34\n",
      "Gradient Descent(18/49): loss=5.1092955468900986e+36\n",
      "Gradient Descent(19/49): loss=9.515096775171017e+38\n",
      "Gradient Descent(20/49): loss=1.7720068470148537e+41\n",
      "Gradient Descent(21/49): loss=3.300027674327056e+43\n",
      "Gradient Descent(22/49): loss=6.145677524654405e+45\n",
      "Gradient Descent(23/49): loss=1.1445162273163923e+48\n",
      "Gradient Descent(24/49): loss=2.131445051173601e+50\n",
      "Gradient Descent(25/49): loss=3.969413362390946e+52\n",
      "Gradient Descent(26/49): loss=7.392281791576483e+54\n",
      "Gradient Descent(27/49): loss=1.3766726993926717e+57\n",
      "Gradient Descent(28/49): loss=2.563792580814226e+59\n",
      "Gradient Descent(29/49): loss=4.7745788816309266e+61\n",
      "Gradient Descent(30/49): loss=8.891750318458423e+63\n",
      "Gradient Descent(31/49): loss=1.6559203583373695e+66\n",
      "Gradient Descent(32/49): loss=3.083838541286855e+68\n",
      "Gradient Descent(33/49): loss=5.743066144965311e+70\n",
      "Gradient Descent(34/49): loss=1.0695374710403442e+73\n",
      "Gradient Descent(35/49): loss=1.991811295717352e+75\n",
      "Gradient Descent(36/49): loss=3.709371896889375e+77\n",
      "Gradient Descent(37/49): loss=6.908003734599318e+79\n",
      "Gradient Descent(38/49): loss=1.2864850687324204e+82\n",
      "Gradient Descent(39/49): loss=2.3958351727316843e+84\n",
      "Gradient Descent(40/49): loss=4.4617899689687074e+86\n",
      "Gradient Descent(41/49): loss=8.309240115417058e+88\n",
      "Gradient Descent(42/49): loss=1.547438847992538e+91\n",
      "Gradient Descent(43/49): loss=2.881812241571406e+93\n",
      "Gradient Descent(44/49): loss=5.366830363890946e+95\n",
      "Gradient Descent(45/49): loss=9.99470671242496e+97\n",
      "Gradient Descent(46/49): loss=1.8613251303692657e+100\n",
      "Gradient Descent(47/49): loss=3.466366088198743e+102\n",
      "Gradient Descent(48/49): loss=6.455451367075458e+104\n",
      "Gradient Descent(49/49): loss=1.202205747816203e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.8829674502415448\n",
      "Gradient Descent(2/49): loss=150.51812436289913\n",
      "Gradient Descent(3/49): loss=7224.525577012132\n",
      "Gradient Descent(4/49): loss=448503.0874962297\n",
      "Gradient Descent(5/49): loss=44623793.43942176\n",
      "Gradient Descent(6/49): loss=6694501306.798256\n",
      "Gradient Descent(7/49): loss=1194542120062.268\n",
      "Gradient Descent(8/49): loss=223809399551251.94\n",
      "Gradient Descent(9/49): loss=4.2435352035274696e+16\n",
      "Gradient Descent(10/49): loss=8.068505431689523e+18\n",
      "Gradient Descent(11/49): loss=1.535117339599274e+21\n",
      "Gradient Descent(12/49): loss=2.9211636242881758e+23\n",
      "Gradient Descent(13/49): loss=5.5588567623565185e+25\n",
      "Gradient Descent(14/49): loss=1.0578367132355973e+28\n",
      "Gradient Descent(15/49): loss=2.013040965043161e+30\n",
      "Gradient Descent(16/49): loss=3.8307762104112224e+32\n",
      "Gradient Descent(17/49): loss=7.289890336893601e+34\n",
      "Gradient Descent(18/49): loss=1.3872515508779231e+37\n",
      "Gradient Descent(19/49): loss=2.639911970033825e+39\n",
      "Gradient Descent(20/49): loss=5.023699713345724e+41\n",
      "Gradient Descent(21/49): loss=9.560000145414247e+43\n",
      "Gradient Descent(22/49): loss=1.8192489201507197e+46\n",
      "Gradient Descent(23/49): loss=3.461994333870898e+48\n",
      "Gradient Descent(24/49): loss=6.588106022789764e+50\n",
      "Gradient Descent(25/49): loss=1.2537034085502938e+53\n",
      "Gradient Descent(26/49): loss=2.3857725288175558e+55\n",
      "Gradient Descent(27/49): loss=4.5400774381260316e+57\n",
      "Gradient Descent(28/49): loss=8.639676622647892e+59\n",
      "Gradient Descent(29/49): loss=1.6441131932485255e+62\n",
      "Gradient Descent(30/49): loss=3.128714546014295e+64\n",
      "Gradient Descent(31/49): loss=5.953881247738321e+66\n",
      "Gradient Descent(32/49): loss=1.1330117014774745e+69\n",
      "Gradient Descent(33/49): loss=2.1560986225119995e+71\n",
      "Gradient Descent(34/49): loss=4.103012584897509e+73\n",
      "Gradient Descent(35/49): loss=7.807950942528754e+75\n",
      "Gradient Descent(36/49): loss=1.4858374586842419e+78\n",
      "Gradient Descent(37/49): loss=2.8275189865809877e+80\n",
      "Gradient Descent(38/49): loss=5.3807121181044975e+82\n",
      "Gradient Descent(39/49): loss=1.0239387616959662e+85\n",
      "Gradient Descent(40/49): loss=1.9485349981385597e+87\n",
      "Gradient Descent(41/49): loss=3.7080231562698e+89\n",
      "Gradient Descent(42/49): loss=7.05629395446734e+91\n",
      "Gradient Descent(43/49): loss=1.3427986361859983e+94\n",
      "Gradient Descent(44/49): loss=2.555318966270422e+96\n",
      "Gradient Descent(45/49): loss=4.86272091989002e+98\n",
      "Gradient Descent(46/49): loss=9.253660719799546e+100\n",
      "Gradient Descent(47/49): loss=1.7609531397721906e+103\n",
      "Gradient Descent(48/49): loss=3.35105862897968e+105\n",
      "Gradient Descent(49/49): loss=6.376997593651052e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.915319630503095\n",
      "Gradient Descent(2/49): loss=153.53045002506207\n",
      "Gradient Descent(3/49): loss=7354.747311131126\n",
      "Gradient Descent(4/49): loss=439025.25966793037\n",
      "Gradient Descent(5/49): loss=40263691.07162729\n",
      "Gradient Descent(6/49): loss=5626512844.912581\n",
      "Gradient Descent(7/49): loss=960444963790.0537\n",
      "Gradient Descent(8/49): loss=174256996820919.62\n",
      "Gradient Descent(9/49): loss=3.2115723970302452e+16\n",
      "Gradient Descent(10/49): loss=5.941742988695793e+18\n",
      "Gradient Descent(11/49): loss=1.1003072274038587e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=2.038034694076669e+23\n",
      "Gradient Descent(13/49): loss=3.775136836335039e+25\n",
      "Gradient Descent(14/49): loss=6.992935052192514e+27\n",
      "Gradient Descent(15/49): loss=1.295351579676761e+30\n",
      "Gradient Descent(16/49): loss=2.3994745792803223e+32\n",
      "Gradient Descent(17/49): loss=4.444723271515512e+34\n",
      "Gradient Descent(18/49): loss=8.233288238754784e+36\n",
      "Gradient Descent(19/49): loss=1.525112629142039e+39\n",
      "Gradient Descent(20/49): loss=2.8250784742274902e+41\n",
      "Gradient Descent(21/49): loss=5.233100977582174e+43\n",
      "Gradient Descent(22/49): loss=9.693658457805781e+45\n",
      "Gradient Descent(23/49): loss=1.7956277683636155e+48\n",
      "Gradient Descent(24/49): loss=3.3261735974926992e+50\n",
      "Gradient Descent(25/49): loss=6.161316390625262e+52\n",
      "Gradient Descent(26/49): loss=1.1413060248576247e+55\n",
      "Gradient Descent(27/49): loss=2.1141252287553427e+57\n",
      "Gradient Descent(28/49): loss=3.9161499067854613e+59\n",
      "Gradient Descent(29/49): loss=7.254172971316749e+61\n",
      "Gradient Descent(30/49): loss=1.3437439002680736e+64\n",
      "Gradient Descent(31/49): loss=2.4891158187807917e+66\n",
      "Gradient Descent(32/49): loss=4.610772601883933e+68\n",
      "Gradient Descent(33/49): loss=8.540873761630237e+70\n",
      "Gradient Descent(34/49): loss=1.5820889666581847e+73\n",
      "Gradient Descent(35/49): loss=2.930619943905817e+75\n",
      "Gradient Descent(36/49): loss=5.42860321803514e+77\n",
      "Gradient Descent(37/49): loss=1.0055801660717796e+80\n",
      "Gradient Descent(38/49): loss=1.8627102217335694e+82\n",
      "Gradient Descent(39/49): loss=3.4504353677786087e+84\n",
      "Gradient Descent(40/49): loss=6.3914956219746e+86\n",
      "Gradient Descent(41/49): loss=1.1839438195887791e+89\n",
      "Gradient Descent(42/49): loss=2.1931063570210206e+91\n",
      "Gradient Descent(43/49): loss=4.062452469135352e+93\n",
      "Gradient Descent(44/49): loss=7.52517998552575e+95\n",
      "Gradient Descent(45/49): loss=1.39394452599244e+98\n",
      "Gradient Descent(46/49): loss=2.582106136041609e+100\n",
      "Gradient Descent(47/49): loss=4.783025417052923e+102\n",
      "Gradient Descent(48/49): loss=8.859950340866075e+104\n",
      "Gradient Descent(49/49): loss=1.6411938720363583e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.990531178685369\n",
      "Gradient Descent(2/49): loss=156.02297325360306\n",
      "Gradient Descent(3/49): loss=7388.028742809764\n",
      "Gradient Descent(4/49): loss=429393.17139807984\n",
      "Gradient Descent(5/49): loss=37917344.40636043\n",
      "Gradient Descent(6/49): loss=5173757449.714038\n",
      "Gradient Descent(7/49): loss=876204815387.7189\n",
      "Gradient Descent(8/49): loss=158678702035190.16\n",
      "Gradient Descent(9/49): loss=2.9237437384484196e+16\n",
      "Gradient Descent(10/49): loss=5.409985451381942e+18\n",
      "Gradient Descent(11/49): loss=1.0020652109491308e+21\n",
      "Gradient Descent(12/49): loss=1.856532061441831e+23\n",
      "Gradient Descent(13/49): loss=3.4398106702143183e+25\n",
      "Gradient Descent(14/49): loss=6.373423550781739e+27\n",
      "Gradient Descent(15/49): loss=1.1808983111659315e+30\n",
      "Gradient Descent(16/49): loss=2.1880265002463128e+32\n",
      "Gradient Descent(17/49): loss=4.0540839647468233e+34\n",
      "Gradient Descent(18/49): loss=7.511608094349503e+36\n",
      "Gradient Descent(19/49): loss=1.3917880659220948e+39\n",
      "Gradient Descent(20/49): loss=2.578774080593595e+41\n",
      "Gradient Descent(21/49): loss=4.778080748016598e+43\n",
      "Gradient Descent(22/49): loss=8.853065419716132e+45\n",
      "Gradient Descent(23/49): loss=1.6403399494927687e+48\n",
      "Gradient Descent(24/49): loss=3.0393033625777757e+50\n",
      "Gradient Descent(25/49): loss=5.631372285149439e+52\n",
      "Gradient Descent(26/49): loss=1.0434086377961271e+55\n",
      "Gradient Descent(27/49): loss=1.9332793683322442e+57\n",
      "Gradient Descent(28/49): loss=3.5820760731995265e+59\n",
      "Gradient Descent(29/49): loss=6.637048532337733e+61\n",
      "Gradient Descent(30/49): loss=1.2297453298154297e+64\n",
      "Gradient Descent(31/49): loss=2.278533249884513e+66\n",
      "Gradient Descent(32/49): loss=4.2217796196944605e+68\n",
      "Gradient Descent(33/49): loss=7.822323048465913e+70\n",
      "Gradient Descent(34/49): loss=1.449358881480172e+73\n",
      "Gradient Descent(35/49): loss=2.6854441504271117e+75\n",
      "Gradient Descent(36/49): loss=4.975724354549321e+77\n",
      "Gradient Descent(37/49): loss=9.219269314730559e+79\n",
      "Gradient Descent(38/49): loss=1.708192026751297e+82\n",
      "Gradient Descent(39/49): loss=3.1650230627219413e+84\n",
      "Gradient Descent(40/49): loss=5.8643119922607e+86\n",
      "Gradient Descent(41/49): loss=1.0865688641458525e+89\n",
      "Gradient Descent(42/49): loss=2.0132487802308633e+91\n",
      "Gradient Descent(43/49): loss=3.730247373034511e+93\n",
      "Gradient Descent(44/49): loss=6.911587678913305e+95\n",
      "Gradient Descent(45/49): loss=1.2806133070055906e+98\n",
      "Gradient Descent(46/49): loss=2.3727839655180713e+100\n",
      "Gradient Descent(47/49): loss=4.3964120286898067e+102\n",
      "Gradient Descent(48/49): loss=8.145890652876241e+104\n",
      "Gradient Descent(49/49): loss=1.5093110949474102e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.848542130830748\n",
      "Gradient Descent(2/49): loss=148.06155142635964\n",
      "Gradient Descent(3/49): loss=6825.124770018399\n",
      "Gradient Descent(4/49): loss=369107.5860330971\n",
      "Gradient Descent(5/49): loss=28980251.365672864\n",
      "Gradient Descent(6/49): loss=3654557528.891686\n",
      "Gradient Descent(7/49): loss=607234070038.2108\n",
      "Gradient Descent(8/49): loss=110582624872242.12\n",
      "Gradient Descent(9/49): loss=2.0624661658388612e+16\n",
      "Gradient Descent(10/49): loss=3.8689935920838344e+18\n",
      "Gradient Descent(11/49): loss=7.267855484568078e+20\n",
      "Gradient Descent(12/49): loss=1.3657018591822557e+23\n",
      "Gradient Descent(13/49): loss=2.5664862529153557e+25\n",
      "Gradient Descent(14/49): loss=4.8231401975932576e+27\n",
      "Gradient Descent(15/49): loss=9.064058457028882e+29\n",
      "Gradient Descent(16/49): loss=1.7033972865262452e+32\n",
      "Gradient Descent(17/49): loss=3.201174205742933e+34\n",
      "Gradient Descent(18/49): loss=6.01592884928616e+36\n",
      "Gradient Descent(19/49): loss=1.1305664134396742e+39\n",
      "Gradient Descent(20/49): loss=2.1246601335857163e+41\n",
      "Gradient Descent(21/49): loss=3.9928487464111426e+43\n",
      "Gradient Descent(22/49): loss=7.503713588130508e+45\n",
      "Gradient Descent(23/49): loss=1.4101640505582416e+48\n",
      "Gradient Descent(24/49): loss=2.6501046797042676e+50\n",
      "Gradient Descent(25/49): loss=4.980310489851857e+52\n",
      "Gradient Descent(26/49): loss=9.359438804549142e+54\n",
      "Gradient Descent(27/49): loss=1.75890830329975e+57\n",
      "Gradient Descent(28/49): loss=3.305495643513569e+59\n",
      "Gradient Descent(29/49): loss=6.211979003560978e+61\n",
      "Gradient Descent(30/49): loss=1.1674098925650237e+64\n",
      "Gradient Descent(31/49): loss=2.193899651749338e+66\n",
      "Gradient Descent(32/49): loss=4.1229697577518423e+68\n",
      "Gradient Descent(33/49): loss=7.748248471520594e+70\n",
      "Gradient Descent(34/49): loss=1.4561192029979218e+73\n",
      "Gradient Descent(35/49): loss=2.7364676560549123e+75\n",
      "Gradient Descent(36/49): loss=5.142611413418269e+77\n",
      "Gradient Descent(37/49): loss=9.664449017295102e+79\n",
      "Gradient Descent(38/49): loss=1.816228513089495e+82\n",
      "Gradient Descent(39/49): loss=3.4132168381829847e+84\n",
      "Gradient Descent(40/49): loss=6.414418175077787e+86\n",
      "Gradient Descent(41/49): loss=1.2054540474689393e+89\n",
      "Gradient Descent(42/49): loss=2.265395583663577e+91\n",
      "Gradient Descent(43/49): loss=4.2573312199316515e+93\n",
      "Gradient Descent(44/49): loss=8.000752383781529e+95\n",
      "Gradient Descent(45/49): loss=1.5035719656224485e+98\n",
      "Gradient Descent(46/49): loss=2.8256450735665245e+100\n",
      "Gradient Descent(47/49): loss=5.310201483083386e+102\n",
      "Gradient Descent(48/49): loss=9.979399059963561e+104\n",
      "Gradient Descent(49/49): loss=1.8754167034011406e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9175206104353255\n",
      "Gradient Descent(2/49): loss=153.4561698923096\n",
      "Gradient Descent(3/49): loss=7435.823801285865\n",
      "Gradient Descent(4/49): loss=465758.9470659212\n",
      "Gradient Descent(5/49): loss=46726984.86058613\n",
      "Gradient Descent(6/49): loss=7069479090.009696\n",
      "Gradient Descent(7/49): loss=1272605360402.0046\n",
      "Gradient Descent(8/49): loss=240581957210772.03\n",
      "Gradient Descent(9/49): loss=4.602844485295964e+16\n",
      "Gradient Descent(10/49): loss=8.831027213115329e+18\n",
      "Gradient Descent(11/49): loss=1.695434038887639e+21\n",
      "Gradient Descent(12/49): loss=3.255493288734862e+23\n",
      "Gradient Descent(13/49): loss=6.251267851820514e+25\n",
      "Gradient Descent(14/49): loss=1.2003917176275207e+28\n",
      "Gradient Descent(15/49): loss=2.305041265499608e+30\n",
      "Gradient Descent(16/49): loss=4.42623647216947e+32\n",
      "Gradient Descent(17/49): loss=8.499444946387584e+34\n",
      "Gradient Descent(18/49): loss=1.6320991114871684e+37\n",
      "Gradient Descent(19/49): loss=3.1340252703736095e+39\n",
      "Gradient Descent(20/49): loss=6.018086977056901e+41\n",
      "Gradient Descent(21/49): loss=1.1556183422237354e+44\n",
      "Gradient Descent(22/49): loss=2.2190668863194076e+46\n",
      "Gradient Descent(23/49): loss=4.2611454544448025e+48\n",
      "Gradient Descent(24/49): loss=8.182430505337587e+50\n",
      "Gradient Descent(25/49): loss=1.571224678681235e+53\n",
      "Gradient Descent(26/49): loss=3.017131632571776e+55\n",
      "Gradient Descent(27/49): loss=5.7936229056089814e+57\n",
      "Gradient Descent(28/49): loss=1.1125158083933278e+60\n",
      "Gradient Descent(29/49): loss=2.136299590238849e+62\n",
      "Gradient Descent(30/49): loss=4.1022122156138045e+64\n",
      "Gradient Descent(31/49): loss=7.877240223619627e+66\n",
      "Gradient Descent(32/49): loss=1.512620758731898e+69\n",
      "Gradient Descent(33/49): loss=2.904597923630797e+71\n",
      "Gradient Descent(34/49): loss=5.577530950344325e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=1.07102092337663e+76\n",
      "Gradient Descent(36/49): loss=2.056619368897835e+78\n",
      "Gradient Descent(37/49): loss=3.949206907359679e+80\n",
      "Gradient Descent(38/49): loss=7.583433003208133e+82\n",
      "Gradient Descent(39/49): loss=1.4562026620325722e+85\n",
      "Gradient Descent(40/49): loss=2.7962615243171434e+87\n",
      "Gradient Descent(41/49): loss=5.369498845347909e+89\n",
      "Gradient Descent(42/49): loss=1.0310737246664786e+92\n",
      "Gradient Descent(43/49): loss=1.9799110798183802e+94\n",
      "Gradient Descent(44/49): loss=3.8019084282800074e+96\n",
      "Gradient Descent(45/49): loss=7.300584275912216e+98\n",
      "Gradient Descent(46/49): loss=1.401888861216207e+101\n",
      "Gradient Descent(47/49): loss=2.6919658823560174e+103\n",
      "Gradient Descent(48/49): loss=5.169225972365465e+105\n",
      "Gradient Descent(49/49): loss=9.926164862829701e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.950176282188384\n",
      "Gradient Descent(2/49): loss=156.52562437428935\n",
      "Gradient Descent(3/49): loss=7569.955543745903\n",
      "Gradient Descent(4/49): loss=455965.4661400014\n",
      "Gradient Descent(5/49): loss=42168728.11353063\n",
      "Gradient Descent(6/49): loss=5942428625.129246\n",
      "Gradient Descent(7/49): loss=1023298910402.663\n",
      "Gradient Descent(8/49): loss=187330153630833.1\n",
      "Gradient Descent(9/49): loss=3.4837719232459356e+16\n",
      "Gradient Descent(10/49): loss=6.503815706291332e+18\n",
      "Gradient Descent(11/49): loss=1.215326197232862e+21\n",
      "Gradient Descent(12/49): loss=2.271514858966727e+23\n",
      "Gradient Descent(13/49): loss=4.2458239151859e+25\n",
      "Gradient Descent(14/49): loss=7.936227132884962e+27\n",
      "Gradient Descent(15/49): loss=1.483431774066613e+30\n",
      "Gradient Descent(16/49): loss=2.7728182131972865e+32\n",
      "Gradient Descent(17/49): loss=5.18292947397348e+34\n",
      "Gradient Descent(18/49): loss=9.687890465495777e+36\n",
      "Gradient Descent(19/49): loss=1.810852784178412e+39\n",
      "Gradient Descent(20/49): loss=3.3848316371461716e+41\n",
      "Gradient Descent(21/49): loss=6.326900407976952e+43\n",
      "Gradient Descent(22/49): loss=1.1826191985212536e+46\n",
      "Gradient Descent(23/49): loss=2.2105424118873658e+48\n",
      "Gradient Descent(24/49): loss=4.131928317167395e+50\n",
      "Gradient Descent(25/49): loss=7.723367589074851e+52\n",
      "Gradient Descent(26/49): loss=1.4436457348053435e+55\n",
      "Gradient Descent(27/49): loss=2.6984511401089285e+57\n",
      "Gradient Descent(28/49): loss=5.043923436338869e+59\n",
      "Gradient Descent(29/49): loss=9.428061621535342e+61\n",
      "Gradient Descent(30/49): loss=1.762285789254259e+64\n",
      "Gradient Descent(31/49): loss=3.2940505988142106e+66\n",
      "Gradient Descent(32/49): loss=6.157213213493497e+68\n",
      "Gradient Descent(33/49): loss=1.1509014029737936e+71\n",
      "Gradient Descent(34/49): loss=2.151255760421375e+73\n",
      "Gradient Descent(35/49): loss=4.021110179193655e+75\n",
      "Gradient Descent(36/49): loss=7.516227205847347e+77\n",
      "Gradient Descent(37/49): loss=1.4049272189116234e+80\n",
      "Gradient Descent(38/49): loss=2.6260787977553893e+82\n",
      "Gradient Descent(39/49): loss=4.908645628890917e+84\n",
      "Gradient Descent(40/49): loss=9.175201418413003e+86\n",
      "Gradient Descent(41/49): loss=1.715021360942474e+89\n",
      "Gradient Descent(42/49): loss=3.2057043048518715e+91\n",
      "Gradient Descent(43/49): loss=5.992077022585093e+93\n",
      "Gradient Descent(44/49): loss=1.1200342773427026e+96\n",
      "Gradient Descent(45/49): loss=2.0935591743802527e+98\n",
      "Gradient Descent(46/49): loss=3.9132641788699676e+100\n",
      "Gradient Descent(47/49): loss=7.314642318701075e+102\n",
      "Gradient Descent(48/49): loss=1.3672471319322898e+105\n",
      "Gradient Descent(49/49): loss=2.55564747847983e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.026140502671382\n",
      "Gradient Descent(2/49): loss=159.06813136982814\n",
      "Gradient Descent(3/49): loss=7604.404108260508\n",
      "Gradient Descent(4/49): loss=445991.20155668503\n",
      "Gradient Descent(5/49): loss=39714459.32093016\n",
      "Gradient Descent(6/49): loss=5464462140.538464\n",
      "Gradient Descent(7/49): loss=933557029812.7695\n",
      "Gradient Descent(8/49): loss=170583622685806.56\n",
      "Gradient Descent(9/49): loss=3.171548739836964e+16\n",
      "Gradient Descent(10/49): loss=5.921752054382072e+18\n",
      "Gradient Descent(11/49): loss=1.1068136996514659e+21\n",
      "Gradient Descent(12/49): loss=2.0692170879606255e+23\n",
      "Gradient Descent(13/49): loss=3.868685041320752e+25\n",
      "Gradient Descent(14/49): loss=7.233140367068754e+27\n",
      "Gradient Descent(15/49): loss=1.3523587117561188e+30\n",
      "Gradient Descent(16/49): loss=2.528466892887603e+32\n",
      "Gradient Descent(17/49): loss=4.7274040842798785e+34\n",
      "Gradient Descent(18/49): loss=8.838696091095758e+36\n",
      "Gradient Descent(19/49): loss=1.6525464735921244e+39\n",
      "Gradient Descent(20/49): loss=3.089720278614271e+41\n",
      "Gradient Descent(21/49): loss=5.776764259865319e+43\n",
      "Gradient Descent(22/49): loss=1.0800655823234609e+46\n",
      "Gradient Descent(23/49): loss=2.0193686459894832e+48\n",
      "Gradient Descent(24/49): loss=3.7755575172301345e+50\n",
      "Gradient Descent(25/49): loss=7.059055113218334e+52\n",
      "Gradient Descent(26/49): loss=1.319811944701535e+55\n",
      "Gradient Descent(27/49): loss=2.4676157664717215e+57\n",
      "Gradient Descent(28/49): loss=4.613632718953183e+59\n",
      "Gradient Descent(29/49): loss=8.625981060183589e+61\n",
      "Gradient Descent(30/49): loss=1.6127757405779477e+64\n",
      "Gradient Descent(31/49): loss=3.015362045487025e+66\n",
      "Gradient Descent(32/49): loss=5.637738736140386e+68\n",
      "Gradient Descent(33/49): loss=1.0540723660214473e+71\n",
      "Gradient Descent(34/49): loss=1.9707698508403917e+73\n",
      "Gradient Descent(35/49): loss=3.684693698632091e+75\n",
      "Gradient Descent(36/49): loss=6.889169553182223e+77\n",
      "Gradient Descent(37/49): loss=1.2880489129968218e+80\n",
      "Gradient Descent(38/49): loss=2.408229307559959e+82\n",
      "Gradient Descent(39/49): loss=4.502599504779049e+84\n",
      "Gradient Descent(40/49): loss=8.418385340961597e+86\n",
      "Gradient Descent(41/49): loss=1.5739621450607756e+89\n",
      "Gradient Descent(42/49): loss=2.942793343077479e+91\n",
      "Gradient Descent(43/49): loss=5.502059047123387e+93\n",
      "Gradient Descent(44/49): loss=1.0287047110951322e+96\n",
      "Gradient Descent(45/49): loss=1.923340650410852e+98\n",
      "Gradient Descent(46/49): loss=3.596016638812422e+100\n",
      "Gradient Descent(47/49): loss=6.723372515343843e+102\n",
      "Gradient Descent(48/49): loss=1.2570503009410295e+105\n",
      "Gradient Descent(49/49): loss=2.3502720628519506e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.882757382853998\n",
      "Gradient Descent(2/49): loss=150.95337755923208\n",
      "Gradient Descent(3/49): loss=7025.44213661331\n",
      "Gradient Descent(4/49): loss=383447.9403987939\n",
      "Gradient Descent(5/49): loss=30361683.03136068\n",
      "Gradient Descent(6/49): loss=3860394114.6403456\n",
      "Gradient Descent(7/49): loss=646993547978.88\n",
      "Gradient Descent(8/49): loss=118876611832344.17\n",
      "Gradient Descent(9/49): loss=2.2371928656520816e+16\n",
      "Gradient Descent(10/49): loss=4.2348067604685414e+18\n",
      "Gradient Descent(11/49): loss=8.027200431104233e+20\n",
      "Gradient Descent(12/49): loss=1.5220776952742525e+23\n",
      "Gradient Descent(13/49): loss=2.8863114099068666e+25\n",
      "Gradient Descent(14/49): loss=5.473404072367016e+27\n",
      "Gradient Descent(15/49): loss=1.0379435127095075e+30\n",
      "Gradient Descent(16/49): loss=1.968295828896702e+32\n",
      "Gradient Descent(17/49): loss=3.7325628635567534e+34\n",
      "Gradient Descent(18/49): loss=7.078217677164809e+36\n",
      "Gradient Descent(19/49): loss=1.3422725349027234e+39\n",
      "Gradient Descent(20/49): loss=2.5454085278453824e+41\n",
      "Gradient Descent(21/49): loss=4.826966513934885e+43\n",
      "Gradient Descent(22/49): loss=9.153582018740965e+45\n",
      "Gradient Descent(23/49): loss=1.7358327126465878e+48\n",
      "Gradient Descent(24/49): loss=3.291733443973429e+50\n",
      "Gradient Descent(25/49): loss=6.242254214507893e+52\n",
      "Gradient Descent(26/49): loss=1.1837452315558146e+55\n",
      "Gradient Descent(27/49): loss=2.2447864586716325e+57\n",
      "Gradient Descent(28/49): loss=4.25688409186893e+59\n",
      "Gradient Descent(29/49): loss=8.072510461565478e+61\n",
      "Gradient Descent(30/49): loss=1.5308245126184058e+64\n",
      "Gradient Descent(31/49): loss=2.902967669835608e+66\n",
      "Gradient Descent(32/49): loss=5.505021132498259e+68\n",
      "Gradient Descent(33/49): loss=1.04394058480742e+71\n",
      "Gradient Descent(34/49): loss=1.979668957444113e+73\n",
      "Gradient Descent(35/49): loss=3.754130491814238e+75\n",
      "Gradient Descent(36/49): loss=7.119117414340305e+77\n",
      "Gradient Descent(37/49): loss=1.350028531764507e+80\n",
      "Gradient Descent(38/49): loss=2.5601165572953514e+82\n",
      "Gradient Descent(39/49): loss=4.854857977239518e+84\n",
      "Gradient Descent(40/49): loss=9.206473788078511e+86\n",
      "Gradient Descent(41/49): loss=1.7458628039778853e+89\n",
      "Gradient Descent(42/49): loss=3.310753933020962e+91\n",
      "Gradient Descent(43/49): loss=6.278323577339237e+93\n",
      "Gradient Descent(44/49): loss=1.190585218328404e+96\n",
      "Gradient Descent(45/49): loss=2.2577574166746302e+98\n",
      "Gradient Descent(46/49): loss=4.2814814715288934e+100\n",
      "Gradient Descent(47/49): loss=8.11915551939346e+102\n",
      "Gradient Descent(48/49): loss=1.5396700134394422e+105\n",
      "Gradient Descent(49/49): loss=2.9197417694762344e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.952244182695756\n",
      "Gradient Descent(2/49): loss=156.43714365101604\n",
      "Gradient Descent(3/49): loss=7652.2503551741975\n",
      "Gradient Descent(4/49): loss=483593.64099889615\n",
      "Gradient Descent(5/49): loss=48919067.01309182\n",
      "Gradient Descent(6/49): loss=7463628925.033301\n",
      "Gradient Descent(7/49): loss=1355384274050.5505\n",
      "Gradient Descent(8/49): loss=258527485156799.6\n",
      "Gradient Descent(9/49): loss=4.990753707522327e+16\n",
      "Gradient Descent(10/49): loss=9.661689463017929e+18\n",
      "Gradient Descent(11/49): loss=1.8716572362051753e+21\n",
      "Gradient Descent(12/49): loss=3.6263205937370734e+23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=7.026217286503914e+25\n",
      "Gradient Descent(14/49): loss=1.3613837482045981e+28\n",
      "Gradient Descent(15/49): loss=2.637791023610405e+30\n",
      "Gradient Descent(16/49): loss=5.110935559923283e+32\n",
      "Gradient Descent(17/49): loss=9.902856137118267e+34\n",
      "Gradient Descent(18/49): loss=1.9187595090091904e+37\n",
      "Gradient Descent(19/49): loss=3.717753770471441e+39\n",
      "Gradient Descent(20/49): loss=7.203452569812466e+41\n",
      "Gradient Descent(21/49): loss=1.3957279622319397e+44\n",
      "Gradient Descent(22/49): loss=2.704337296338175e+46\n",
      "Gradient Descent(23/49): loss=5.239875112046471e+48\n",
      "Gradient Descent(24/49): loss=1.015268739854575e+51\n",
      "Gradient Descent(25/49): loss=1.9671663772235857e+53\n",
      "Gradient Descent(26/49): loss=3.8115460505895533e+55\n",
      "Gradient Descent(27/49): loss=7.385182800994056e+57\n",
      "Gradient Descent(28/49): loss=1.4309396837973954e+60\n",
      "Gradient Descent(29/49): loss=2.772562892269405e+62\n",
      "Gradient Descent(30/49): loss=5.372067794772082e+64\n",
      "Gradient Descent(31/49): loss=1.0408821553549904e+67\n",
      "Gradient Descent(32/49): loss=2.016794468585929e+69\n",
      "Gradient Descent(33/49): loss=3.9077045442589134e+71\n",
      "Gradient Descent(34/49): loss=7.571497761955192e+73\n",
      "Gradient Descent(35/49): loss=1.4670397341967928e+76\n",
      "Gradient Descent(36/49): loss=2.84250969805009e+78\n",
      "Gradient Descent(37/49): loss=5.507595462594859e+80\n",
      "Gradient Descent(38/49): loss=1.0671417515445716e+83\n",
      "Gradient Descent(39/49): loss=2.0676745879826923e+85\n",
      "Gradient Descent(40/49): loss=4.006288944839119e+87\n",
      "Gradient Descent(41/49): loss=7.762513116340752e+89\n",
      "Gradient Descent(42/49): loss=1.504050524338405e+92\n",
      "Gradient Descent(43/49): loss=2.914221136709613e+94\n",
      "Gradient Descent(44/49): loss=5.646542251219204e+96\n",
      "Gradient Descent(45/49): loss=1.0940638304065942e+99\n",
      "Gradient Descent(46/49): loss=2.1198383218428917e+101\n",
      "Gradient Descent(47/49): loss=4.107360453625105e+103\n",
      "Gradient Descent(48/49): loss=7.958347446675511e+105\n",
      "Gradient Descent(49/49): loss=1.5419950305580526e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.985204749170591\n",
      "Gradient Descent(2/49): loss=159.56453397483355\n",
      "Gradient Descent(3/49): loss=7790.389003071592\n",
      "Gradient Descent(4/49): loss=473475.49044762214\n",
      "Gradient Descent(5/49): loss=44154594.356045954\n",
      "Gradient Descent(6/49): loss=6274536620.103876\n",
      "Gradient Descent(7/49): loss=1089955230396.3549\n",
      "Gradient Descent(8/49): loss=201318559238112.84\n",
      "Gradient Descent(9/49): loss=3.777659082838667e+16\n",
      "Gradient Descent(10/49): loss=7.11616471069497e+18\n",
      "Gradient Descent(11/49): loss=1.3417682082427272e+21\n",
      "Gradient Descent(12/49): loss=2.5305077769441434e+23\n",
      "Gradient Descent(13/49): loss=4.772673063927727e+25\n",
      "Gradient Descent(14/49): loss=9.001636030611035e+27\n",
      "Gradient Descent(15/49): loss=1.697784645425623e+30\n",
      "Gradient Descent(16/49): loss=3.2021678270648185e+32\n",
      "Gradient Descent(17/49): loss=6.039564990045028e+34\n",
      "Gradient Descent(18/49): loss=1.1391141527104257e+37\n",
      "Gradient Descent(19/49): loss=2.148467760762923e+39\n",
      "Gradient Descent(20/49): loss=4.0521959280140414e+41\n",
      "Gradient Descent(21/49): loss=7.642791830207244e+43\n",
      "Gradient Descent(22/49): loss=1.4414966108870289e+46\n",
      "Gradient Descent(23/49): loss=2.7187872251901917e+48\n",
      "Gradient Descent(24/49): loss=5.127867745297632e+50\n",
      "Gradient Descent(25/49): loss=9.671601870749821e+52\n",
      "Gradient Descent(26/49): loss=1.8241477236248464e+55\n",
      "Gradient Descent(27/49): loss=3.440500303956605e+57\n",
      "Gradient Descent(28/49): loss=6.489081003814528e+59\n",
      "Gradient Descent(29/49): loss=1.2238967752929217e+62\n",
      "Gradient Descent(30/49): loss=2.308375123830099e+64\n",
      "Gradient Descent(31/49): loss=4.353795042104262e+66\n",
      "Gradient Descent(32/49): loss=8.211633834105712e+68\n",
      "Gradient Descent(33/49): loss=1.5487851305200696e+71\n",
      "Gradient Descent(34/49): loss=2.9211426483208375e+73\n",
      "Gradient Descent(35/49): loss=5.509527566921782e+75\n",
      "Gradient Descent(36/49): loss=1.0391445288753903e+78\n",
      "Gradient Descent(37/49): loss=1.9599164152924787e+80\n",
      "Gradient Descent(38/49): loss=3.696571793617619e+82\n",
      "Gradient Descent(39/49): loss=6.9720539706435555e+84\n",
      "Gradient Descent(40/49): loss=1.314989652128339e+87\n",
      "Gradient Descent(41/49): loss=2.4801841645023524e+89\n",
      "Gradient Descent(42/49): loss=4.67784174566881e+91\n",
      "Gradient Descent(43/49): loss=8.82281393080029e+93\n",
      "Gradient Descent(44/49): loss=1.6640589803961928e+96\n",
      "Gradient Descent(45/49): loss=3.138559094588156e+98\n",
      "Gradient Descent(46/49): loss=5.9195937801896034e+100\n",
      "Gradient Descent(47/49): loss=1.1164865617117834e+103\n",
      "Gradient Descent(48/49): loss=2.10579017542493e+105\n",
      "Gradient Descent(49/49): loss=3.971702315984427e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.061925300139154\n",
      "Gradient Descent(2/49): loss=162.15777168393691\n",
      "Gradient Descent(3/49): loss=7826.037527011724\n",
      "Gradient Descent(4/49): loss=463148.5086129683\n",
      "Gradient Descent(5/49): loss=41587958.43083321\n",
      "Gradient Descent(6/49): loss=5770078212.007523\n",
      "Gradient Descent(7/49): loss=994379510001.9756\n",
      "Gradient Descent(8/49): loss=183322040919768.53\n",
      "Gradient Descent(9/49): loss=3.43909779654548e+16\n",
      "Gradient Descent(10/49): loss=6.479294878136761e+18\n",
      "Gradient Descent(11/49): loss=1.2219651344037682e+21\n",
      "Gradient Descent(12/49): loss=2.3051423388845745e+23\n",
      "Gradient Descent(13/49): loss=4.348731946044315e+25\n",
      "Gradient Descent(14/49): loss=8.204153470317613e+27\n",
      "Gradient Descent(15/49): loss=1.5477699684188816e+30\n",
      "Gradient Descent(16/49): loss=2.9199769095173163e+32\n",
      "Gradient Descent(17/49): loss=5.5087429230082255e+34\n",
      "Gradient Descent(18/49): loss=1.0392633570273437e+37\n",
      "Gradient Descent(19/49): loss=1.9606439307090848e+39\n",
      "Gradient Descent(20/49): loss=3.6988936516978746e+41\n",
      "Gradient Descent(21/49): loss=6.978224878806084e+43\n",
      "Gradient Descent(22/49): loss=1.31649155267546e+46\n",
      "Gradient Descent(23/49): loss=2.4836545660731273e+48\n",
      "Gradient Descent(24/49): loss=4.685590265351537e+50\n",
      "Gradient Descent(25/49): loss=8.839697933323597e+52\n",
      "Gradient Descent(26/49): loss=1.6676716299816325e+55\n",
      "Gradient Descent(27/49): loss=3.146180657329656e+57\n",
      "Gradient Descent(28/49): loss=5.935492665702117e+59\n",
      "Gradient Descent(29/49): loss=1.1197727346816184e+62\n",
      "Gradient Descent(30/49): loss=2.1125305816345673e+64\n",
      "Gradient Descent(31/49): loss=3.985438580633187e+66\n",
      "Gradient Descent(32/49): loss=7.518812185766957e+68\n",
      "Gradient Descent(33/49): loss=1.4184771773814763e+71\n",
      "Gradient Descent(34/49): loss=2.6760576711318218e+73\n",
      "Gradient Descent(35/49): loss=5.048572351684421e+75\n",
      "Gradient Descent(36/49): loss=9.524489350564752e+77\n",
      "Gradient Descent(37/49): loss=1.796862381476141e+80\n",
      "Gradient Descent(38/49): loss=3.389908161084386e+82\n",
      "Gradient Descent(39/49): loss=6.395301865658781e+84\n",
      "Gradient Descent(40/49): loss=1.2065189972526217e+87\n",
      "Gradient Descent(41/49): loss=2.276183550534355e+89\n",
      "Gradient Descent(42/49): loss=4.2941814986096575e+91\n",
      "Gradient Descent(43/49): loss=8.101277569936201e+93\n",
      "Gradient Descent(44/49): loss=1.528363397923472e+96\n",
      "Gradient Descent(45/49): loss=2.883365809833038e+98\n",
      "Gradient Descent(46/49): loss=5.439673839749039e+100\n",
      "Gradient Descent(47/49): loss=1.026233001096822e+103\n",
      "Gradient Descent(48/49): loss=1.9360612484604282e+105\n",
      "Gradient Descent(49/49): loss=3.652516683622563e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.917141428005593\n",
      "Gradient Descent(2/49): loss=153.8874814004396\n",
      "Gradient Descent(3/49): loss=7230.637236487141\n",
      "Gradient Descent(4/49): loss=398273.87550007924\n",
      "Gradient Descent(5/49): loss=31802172.715898104\n",
      "Gradient Descent(6/49): loss=4076815758.7492924\n",
      "Gradient Descent(7/49): loss=689159700281.7529\n",
      "Gradient Descent(8/49): loss=127751118196056.72\n",
      "Gradient Descent(9/49): loss=2.4258347718784156e+16\n",
      "Gradient Descent(10/49): loss=4.633324928173605e+18\n",
      "Gradient Descent(11/49): loss=8.861920811493463e+20\n",
      "Gradient Descent(12/49): loss=1.6955321748857392e+23\n",
      "Gradient Descent(13/49): loss=3.2442781409667943e+25\n",
      "Gradient Descent(14/49): loss=6.207806110366173e+27\n",
      "Gradient Descent(15/49): loss=1.1878459076127173e+30\n",
      "Gradient Descent(16/49): loss=2.2729114518237407e+32\n",
      "Gradient Descent(17/49): loss=4.3491564854880675e+34\n",
      "Gradient Descent(18/49): loss=8.321997417199036e+36\n",
      "Gradient Descent(19/49): loss=1.5923925062963216e+39\n",
      "Gradient Descent(20/49): loss=3.0470015492946514e+41\n",
      "Gradient Descent(21/49): loss=5.830358037877415e+43\n",
      "Gradient Descent(22/49): loss=1.115623812659226e+46\n",
      "Gradient Descent(23/49): loss=2.1347170848516286e+48\n",
      "Gradient Descent(24/49): loss=4.084725496797816e+50\n",
      "Gradient Descent(25/49): loss=7.816015762758595e+52\n",
      "Gradient Descent(26/49): loss=1.4955742424213516e+55\n",
      "Gradient Descent(27/49): loss=2.8617423281718447e+57\n",
      "Gradient Descent(28/49): loss=5.475869348747215e+59\n",
      "Gradient Descent(29/49): loss=1.0477933260925054e+62\n",
      "Gradient Descent(30/49): loss=2.0049252169524053e+64\n",
      "Gradient Descent(31/49): loss=3.8363721408326755e+66\n",
      "Gradient Descent(32/49): loss=7.340798089880085e+68\n",
      "Gradient Descent(33/49): loss=1.4046425794524824e+71\n",
      "Gradient Descent(34/49): loss=2.687746961370387e+73\n",
      "Gradient Descent(35/49): loss=5.142933749859534e+75\n",
      "Gradient Descent(36/49): loss=9.840869670989456e+77\n",
      "Gradient Descent(37/49): loss=1.8830247596334518e+80\n",
      "Gradient Descent(38/49): loss=3.603118793296711e+82\n",
      "Gradient Descent(39/49): loss=6.894473889517596e+84\n",
      "Gradient Descent(40/49): loss=1.3192396071335215e+87\n",
      "Gradient Descent(41/49): loss=2.5243306000126527e+89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=4.830240802128579e+91\n",
      "Gradient Descent(43/49): loss=9.242539866383164e+93\n",
      "Gradient Descent(44/49): loss=1.7685359111710448e+96\n",
      "Gradient Descent(45/49): loss=3.384047366111705e+98\n",
      "Gradient Descent(46/49): loss=6.475286424070844e+100\n",
      "Gradient Descent(47/49): loss=1.2390291783041177e+103\n",
      "Gradient Descent(48/49): loss=2.37085003526965e+105\n",
      "Gradient Descent(49/49): loss=4.5365597422263216e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9871381670228323\n",
      "Gradient Descent(2/49): loss=159.46146198458177\n",
      "Gradient Descent(3/49): loss=7873.904390070145\n",
      "Gradient Descent(4/49): loss=502023.75205792906\n",
      "Gradient Descent(5/49): loss=51203367.41920909\n",
      "Gradient Descent(6/49): loss=7877838214.3795185\n",
      "Gradient Descent(7/49): loss=1443140635515.0852\n",
      "Gradient Descent(8/49): loss=277722194258173.03\n",
      "Gradient Descent(9/49): loss=5.409395532099757e+16\n",
      "Gradient Descent(10/49): loss=1.0566234091522454e+19\n",
      "Gradient Descent(11/49): loss=2.0652830037376369e+21\n",
      "Gradient Descent(12/49): loss=4.0374388050226496e+23\n",
      "Gradient Descent(13/49): loss=7.89310608359837e+25\n",
      "Gradient Descent(14/49): loss=1.5430981791999189e+28\n",
      "Gradient Descent(15/49): loss=3.0167548748493745e+30\n",
      "Gradient Descent(16/49): loss=5.8977544141728564e+32\n",
      "Gradient Descent(17/49): loss=1.1530108424599874e+35\n",
      "Gradient Descent(18/49): loss=2.2541359678526876e+37\n",
      "Gradient Descent(19/49): loss=4.4068353943208265e+39\n",
      "Gradient Descent(20/49): loss=8.615362380715086e+41\n",
      "Gradient Descent(21/49): loss=1.6843031865827978e+44\n",
      "Gradient Descent(22/49): loss=3.2928124194606145e+46\n",
      "Gradient Descent(23/49): loss=6.437447673499853e+48\n",
      "Gradient Descent(24/49): loss=1.2585209015944519e+51\n",
      "Gradient Descent(25/49): loss=2.460408130805787e+53\n",
      "Gradient Descent(26/49): loss=4.8100974425351215e+55\n",
      "Gradient Descent(27/49): loss=9.403739614169777e+57\n",
      "Gradient Descent(28/49): loss=1.838430921359871e+60\n",
      "Gradient Descent(29/49): loss=3.594132112632485e+62\n",
      "Gradient Descent(30/49): loss=7.026527618182623e+64\n",
      "Gradient Descent(31/49): loss=1.37368601993101e+67\n",
      "Gradient Descent(32/49): loss=2.685555915941747e+69\n",
      "Gradient Descent(33/49): loss=5.250261321005657e+71\n",
      "Gradient Descent(34/49): loss=1.0264259915504831e+74\n",
      "Gradient Descent(35/49): loss=2.0066626244207694e+76\n",
      "Gradient Descent(36/49): loss=3.923025061129342e+78\n",
      "Gradient Descent(37/49): loss=7.669513271914313e+80\n",
      "Gradient Descent(38/49): loss=1.4993897033922087e+83\n",
      "Gradient Descent(39/49): loss=2.9313065939547352e+85\n",
      "Gradient Descent(40/49): loss=5.730703851255435e+87\n",
      "Gradient Descent(41/49): loss=1.1203524973648966e+90\n",
      "Gradient Descent(42/49): loss=2.190288925987338e+92\n",
      "Gradient Descent(43/49): loss=4.282014446869302e+94\n",
      "Gradient Descent(44/49): loss=8.371337454912483e+96\n",
      "Gradient Descent(45/49): loss=1.6365963182412306e+99\n",
      "Gradient Descent(46/49): loss=3.199545500712109e+101\n",
      "Gradient Descent(47/49): loss=6.255110864558415e+103\n",
      "Gradient Descent(48/49): loss=1.2228740588064016e+106\n",
      "Gradient Descent(49/49): loss=2.3907185597216953e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.02040503144971\n",
      "Gradient Descent(2/49): loss=162.64760272364126\n",
      "Gradient Descent(3/49): loss=8016.148744227736\n",
      "Gradient Descent(4/49): loss=491571.69997253147\n",
      "Gradient Descent(5/49): loss=46224327.36916044\n",
      "Gradient Descent(6/49): loss=6623588206.765131\n",
      "Gradient Descent(7/49): loss=1160625308773.2014\n",
      "Gradient Descent(8/49): loss=216281744477293.12\n",
      "Gradient Descent(9/49): loss=4.094852687405833e+16\n",
      "Gradient Descent(10/49): loss=7.783030634772765e+18\n",
      "Gradient Descent(11/49): loss=1.480708671066625e+21\n",
      "Gradient Descent(12/49): loss=2.8176675728515996e+23\n",
      "Gradient Descent(13/49): loss=5.36208722118866e+25\n",
      "Gradient Descent(14/49): loss=1.020431342757264e+28\n",
      "Gradient Descent(15/49): loss=1.9419368606491327e+30\n",
      "Gradient Descent(16/49): loss=3.695615324897888e+32\n",
      "Gradient Descent(17/49): loss=7.03296563175227e+34\n",
      "Gradient Descent(18/49): loss=1.3384133216797448e+37\n",
      "Gradient Descent(19/49): loss=2.5470766290134302e+39\n",
      "Gradient Descent(20/49): loss=4.8472316183641e+41\n",
      "Gradient Descent(21/49): loss=9.224557325641287e+43\n",
      "Gradient Descent(22/49): loss=1.755485699194752e+46\n",
      "Gradient Descent(23/49): loss=3.340789082232028e+48\n",
      "Gradient Descent(24/49): loss=6.357711542270919e+50\n",
      "Gradient Descent(25/49): loss=1.2099086491209054e+53\n",
      "Gradient Descent(26/49): loss=2.302524940751651e+55\n",
      "Gradient Descent(27/49): loss=4.381835857307218e+57\n",
      "Gradient Descent(28/49): loss=8.338882737190092e+59\n",
      "Gradient Descent(29/49): loss=1.5869367901731367e+62\n",
      "Gradient Descent(30/49): loss=3.020030926653452e+64\n",
      "Gradient Descent(31/49): loss=5.747290537607496e+66\n",
      "Gradient Descent(32/49): loss=1.0937420617833213e+69\n",
      "Gradient Descent(33/49): loss=2.081453321154036e+71\n",
      "Gradient Descent(34/49): loss=3.9611239976261753e+73\n",
      "Gradient Descent(35/49): loss=7.53824415138488e+75\n",
      "Gradient Descent(36/49): loss=1.434570715785276e+78\n",
      "Gradient Descent(37/49): loss=2.73006962531267e+80\n",
      "Gradient Descent(38/49): loss=5.195477697294957e+82\n",
      "Gradient Descent(39/49): loss=9.887289412993674e+84\n",
      "Gradient Descent(40/49): loss=1.8816073830360988e+87\n",
      "Gradient Descent(41/49): loss=3.580805816448742e+89\n",
      "Gradient Descent(42/49): loss=6.814477032091327e+91\n",
      "Gradient Descent(43/49): loss=1.2968337184772105e+94\n",
      "Gradient Descent(44/49): loss=2.467948289295736e+96\n",
      "Gradient Descent(45/49): loss=4.6966458936537934e+98\n",
      "Gradient Descent(46/49): loss=8.937984132831565e+100\n",
      "Gradient Descent(47/49): loss=1.700949191564508e+103\n",
      "Gradient Descent(48/49): loss=3.23700300793365e+105\n",
      "Gradient Descent(49/49): loss=6.160200742818032e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.097885571088681\n",
      "Gradient Descent(2/49): loss=165.29232549065154\n",
      "Gradient Descent(3/49): loss=8053.030772116504\n",
      "Gradient Descent(4/49): loss=480881.1843097338\n",
      "Gradient Descent(5/49): loss=43540717.39617954\n",
      "Gradient Descent(6/49): loss=6091298160.817324\n",
      "Gradient Descent(7/49): loss=1058865221004.3223\n",
      "Gradient Descent(8/49): loss=196948172791724.34\n",
      "Gradient Descent(9/49): loss=3.7278646537609624e+16\n",
      "Gradient Descent(10/49): loss=7.086474967331368e+18\n",
      "Gradient Descent(11/49): loss=1.3484988817889438e+21\n",
      "Gradient Descent(12/49): loss=2.566725503181792e+23\n",
      "Gradient Descent(13/49): loss=4.885785421260496e+25\n",
      "Gradient Descent(14/49): loss=9.300271854439924e+27\n",
      "Gradient Descent(15/49): loss=1.770347052606815e+30\n",
      "Gradient Descent(16/49): loss=3.369935166465951e+32\n",
      "Gradient Descent(17/49): loss=6.4148243164536995e+34\n",
      "Gradient Descent(18/49): loss=1.221090940350436e+37\n",
      "Gradient Descent(19/49): loss=2.3244020857277063e+39\n",
      "Gradient Descent(20/49): loss=4.424604992829724e+41\n",
      "Gradient Descent(21/49): loss=8.422436667078412e+43\n",
      "Gradient Descent(22/49): loss=1.6032490931394926e+46\n",
      "Gradient Descent(23/49): loss=3.0518575043738404e+48\n",
      "Gradient Descent(24/49): loss=5.809349443543399e+50\n",
      "Gradient Descent(25/49): loss=1.1058360657044998e+53\n",
      "Gradient Descent(26/49): loss=2.1050092030056346e+55\n",
      "Gradient Descent(27/49): loss=4.006980674767616e+57\n",
      "Gradient Descent(28/49): loss=7.627469801574497e+59\n",
      "Gradient Descent(29/49): loss=1.4519235378469743e+62\n",
      "Gradient Descent(30/49): loss=2.763802433303574e+64\n",
      "Gradient Descent(31/49): loss=5.261023525840686e+66\n",
      "Gradient Descent(32/49): loss=1.001459735541406e+69\n",
      "Gradient Descent(33/49): loss=1.9063241154208457e+71\n",
      "Gradient Descent(34/49): loss=3.6287745818062675e+73\n",
      "Gradient Descent(35/49): loss=6.907537316998419e+75\n",
      "Gradient Descent(36/49): loss=1.3148811178559172e+78\n",
      "Gradient Descent(37/49): loss=2.5029359593026807e+80\n",
      "Gradient Descent(38/49): loss=4.764452338159393e+82\n",
      "Gradient Descent(39/49): loss=9.069351534234678e+84\n",
      "Gradient Descent(40/49): loss=1.7263922779275602e+87\n",
      "Gradient Descent(41/49): loss=3.2862661525882433e+89\n",
      "Gradient Descent(42/49): loss=6.255556957548206e+91\n",
      "Gradient Descent(43/49): loss=1.190773693673856e+94\n",
      "Gradient Descent(44/49): loss=2.266691837622464e+96\n",
      "Gradient Descent(45/49): loss=4.314750917021492e+98\n",
      "Gradient Descent(46/49): loss=8.21332444354897e+100\n",
      "Gradient Descent(47/49): loss=1.563443631215827e+103\n",
      "Gradient Descent(48/49): loss=2.976085998781182e+105\n",
      "Gradient Descent(49/49): loss=5.665114939419758e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.9516942662855308\n",
      "Gradient Descent(2/49): loss=156.86427320661278\n",
      "Gradient Descent(3/49): loss=7440.8046634423845\n",
      "Gradient Descent(4/49): loss=413599.4289694771\n",
      "Gradient Descent(5/49): loss=33303956.879245184\n",
      "Gradient Descent(6/49): loss=4304315364.978034\n",
      "Gradient Descent(7/49): loss=733866409107.8252\n",
      "Gradient Descent(8/49): loss=137243894505096.81\n",
      "Gradient Descent(9/49): loss=2.6294300874987316e+16\n",
      "Gradient Descent(10/49): loss=5.067305288412102e+18\n",
      "Gradient Descent(11/49): loss=9.779109168896113e+20\n",
      "Gradient Descent(12/49): loss=1.887841101279524e+23\n",
      "Gradient Descent(13/49): loss=3.6447326596030464e+25\n",
      "Gradient Descent(14/49): loss=7.036780377358186e+27\n",
      "Gradient Descent(15/49): loss=1.358576906009344e+30\n",
      "Gradient Descent(16/49): loss=2.6229795961844592e+32\n",
      "Gradient Descent(17/49): loss=5.064140006896874e+34\n",
      "Gradient Descent(18/49): loss=9.777245521769268e+36\n",
      "Gradient Descent(19/49): loss=1.8876755221615025e+39\n",
      "Gradient Descent(20/49): loss=3.6445017983171945e+41\n",
      "Gradient Descent(21/49): loss=7.036375273361363e+43\n",
      "Gradient Descent(22/49): loss=1.3585005505979662e+46\n",
      "Gradient Descent(23/49): loss=2.622833027457098e+48\n",
      "Gradient Descent(24/49): loss=5.0638574176232986e+50\n",
      "Gradient Descent(25/49): loss=9.776700109248846e+52\n",
      "Gradient Descent(26/49): loss=1.887570228450585e+55\n",
      "Gradient Descent(27/49): loss=3.644298513322331e+57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(28/49): loss=7.035982796309408e+59\n",
      "Gradient Descent(29/49): loss=1.358424775824168e+62\n",
      "Gradient Descent(30/49): loss=2.6226867304746957e+64\n",
      "Gradient Descent(31/49): loss=5.0635749646385964e+66\n",
      "Gradient Descent(32/49): loss=9.77615478226564e+68\n",
      "Gradient Descent(33/49): loss=1.887464943133101e+71\n",
      "Gradient Descent(34/49): loss=3.644095240818951e+73\n",
      "Gradient Descent(35/49): loss=7.03559034167605e+75\n",
      "Gradient Descent(36/49): loss=1.3583490053009848e+78\n",
      "Gradient Descent(37/49): loss=2.622540441663472e+80\n",
      "Gradient Descent(38/49): loss=5.0632925274136905e+82\n",
      "Gradient Descent(39/49): loss=9.775609485702216e+84\n",
      "Gradient Descent(40/49): loss=1.887359663688355e+87\n",
      "Gradient Descent(41/49): loss=3.6438919796537614e+89\n",
      "Gradient Descent(42/49): loss=7.03519790893324e+91\n",
      "Gradient Descent(43/49): loss=1.3582732390042408e+94\n",
      "Gradient Descent(44/49): loss=2.6223941610120375e+96\n",
      "Gradient Descent(45/49): loss=5.063010105942732e+98\n",
      "Gradient Descent(46/49): loss=9.775064219554825e+100\n",
      "Gradient Descent(47/49): loss=1.887254390116007e+103\n",
      "Gradient Descent(48/49): loss=3.643688729826351e+105\n",
      "Gradient Descent(49/49): loss=7.034805498079981e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.022202563416559\n",
      "Gradient Descent(2/49): loss=162.52954324920998\n",
      "Gradient Descent(3/49): loss=8100.886489016977\n",
      "Gradient Descent(4/49): loss=521066.2574657682\n",
      "Gradient Descent(5/49): loss=53583322.205850445\n",
      "Gradient Descent(6/49): loss=8313031174.678341\n",
      "Gradient Descent(7/49): loss=1536149480507.794\n",
      "Gradient Descent(8/49): loss=298246872347082.0\n",
      "Gradient Descent(9/49): loss=5.861051025975461e+16\n",
      "Gradient Descent(10/49): loss=1.155085786369848e+19\n",
      "Gradient Descent(11/49): loss=2.2779404698185493e+21\n",
      "Gradient Descent(12/49): loss=4.493015757058046e+23\n",
      "Gradient Descent(13/49): loss=8.862356613885088e+25\n",
      "Gradient Descent(14/49): loss=1.7480914966617897e+28\n",
      "Gradient Descent(15/49): loss=3.448100780720238e+30\n",
      "Gradient Descent(16/49): loss=6.801362771391691e+32\n",
      "Gradient Descent(17/49): loss=1.3415657897555935e+35\n",
      "Gradient Descent(18/49): loss=2.6462326350467968e+37\n",
      "Gradient Descent(19/49): loss=5.219682294132962e+39\n",
      "Gradient Descent(20/49): loss=1.0295800500481604e+42\n",
      "Gradient Descent(21/49): loss=2.0308421471944742e+44\n",
      "Gradient Descent(22/49): loss=4.005827256393691e+46\n",
      "Gradient Descent(23/49): loss=7.901476749685779e+48\n",
      "Gradient Descent(24/49): loss=1.5585628343414374e+51\n",
      "Gradient Descent(25/49): loss=3.0742583766879477e+53\n",
      "Gradient Descent(26/49): loss=6.063961207333465e+55\n",
      "Gradient Descent(27/49): loss=1.196113696977629e+58\n",
      "Gradient Descent(28/49): loss=2.359329037869347e+60\n",
      "Gradient Descent(29/49): loss=4.653766212191099e+62\n",
      "Gradient Descent(30/49): loss=9.179533507242301e+64\n",
      "Gradient Descent(31/49): loss=1.8106589710038877e+67\n",
      "Gradient Descent(32/49): loss=3.57151690408904e+69\n",
      "Gradient Descent(33/49): loss=7.044801478614157e+71\n",
      "Gradient Descent(34/49): loss=1.3895840116635646e+74\n",
      "Gradient Descent(35/49): loss=2.7409483877335153e+76\n",
      "Gradient Descent(36/49): loss=5.4065087113552824e+78\n",
      "Gradient Descent(37/49): loss=1.0664314795847052e+81\n",
      "Gradient Descent(38/49): loss=2.10353143103351e+83\n",
      "Gradient Descent(39/49): loss=4.149206551056641e+85\n",
      "Gradient Descent(40/49): loss=8.184291781593857e+87\n",
      "Gradient Descent(41/49): loss=1.614347975740239e+90\n",
      "Gradient Descent(42/49): loss=3.184294324204791e+92\n",
      "Gradient Descent(43/49): loss=6.281006632732809e+94\n",
      "Gradient Descent(44/49): loss=1.238925812245244e+97\n",
      "Gradient Descent(45/49): loss=2.443775748059752e+99\n",
      "Gradient Descent(46/49): loss=4.820336978839993e+101\n",
      "Gradient Descent(47/49): loss=9.508093616208617e+103\n",
      "Gradient Descent(48/49): loss=1.875467308850684e+106\n",
      "Gradient Descent(49/49): loss=3.6993510671493524e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.055777129025748\n",
      "Gradient Descent(2/49): loss=165.77525656340933\n",
      "Gradient Descent(3/49): loss=8247.337281796285\n",
      "Gradient Descent(4/49): loss=510270.85240366374\n",
      "Gradient Descent(5/49): loss=48381064.25197288\n",
      "Gradient Descent(6/49): loss=6990366014.144113\n",
      "Gradient Descent(7/49): loss=1235531255031.2175\n",
      "Gradient Descent(8/49): loss=232282819313524.56\n",
      "Gradient Descent(9/49): loss=4.43708430709801e+16\n",
      "Gradient Descent(10/49): loss=8.508990361027509e+18\n",
      "Gradient Descent(11/49): loss=1.6333187310669201e+21\n",
      "Gradient Descent(12/49): loss=3.135910704733267e+23\n",
      "Gradient Descent(13/49): loss=6.021165624695274e+25\n",
      "Gradient Descent(14/49): loss=1.1561210162512934e+28\n",
      "Gradient Descent(15/49): loss=2.2198694242845352e+30\n",
      "Gradient Descent(16/49): loss=4.2623774347312016e+32\n",
      "Gradient Descent(17/49): loss=8.184204281222477e+34\n",
      "Gradient Descent(18/49): loss=1.571451703123996e+37\n",
      "Gradient Descent(19/49): loss=3.0173495159737324e+39\n",
      "Gradient Descent(20/49): loss=5.7936226152384695e+41\n",
      "Gradient Descent(21/49): loss=1.112435362621284e+44\n",
      "Gradient Descent(22/49): loss=2.1359907580958243e+46\n",
      "Gradient Descent(23/49): loss=4.101322802336018e+48\n",
      "Gradient Descent(24/49): loss=7.874963253170202e+50\n",
      "Gradient Descent(25/49): loss=1.5120742557400452e+53\n",
      "Gradient Descent(26/49): loss=2.903338696790291e+55\n",
      "Gradient Descent(27/49): loss=5.574710075435563e+57\n",
      "Gradient Descent(28/49): loss=1.0704018948778229e+60\n",
      "Gradient Descent(29/49): loss=2.0552821600655545e+62\n",
      "Gradient Descent(30/49): loss=3.946353960785785e+64\n",
      "Gradient Descent(31/49): loss=7.577407076463387e+66\n",
      "Gradient Descent(32/49): loss=1.454940397465128e+69\n",
      "Gradient Descent(33/49): loss=2.793635789677026e+71\n",
      "Gradient Descent(34/49): loss=5.364069166655655e+73\n",
      "Gradient Descent(35/49): loss=1.029956665467538e+76\n",
      "Gradient Descent(36/49): loss=1.9776231435181188e+78\n",
      "Gradient Descent(37/49): loss=3.7972406305105405e+80\n",
      "Gradient Descent(38/49): loss=7.291094085978931e+82\n",
      "Gradient Descent(39/49): loss=1.3999653470327822e+85\n",
      "Gradient Descent(40/49): loss=2.688078016523758e+87\n",
      "Gradient Descent(41/49): loss=5.161387343074798e+89\n",
      "Gradient Descent(42/49): loss=9.910396625951959e+91\n",
      "Gradient Descent(43/49): loss=1.9028984797170545e+94\n",
      "Gradient Descent(44/49): loss=3.6537615604882437e+96\n",
      "Gradient Descent(45/49): loss=7.015599456932866e+98\n",
      "Gradient Descent(46/49): loss=1.3470675336991467e+101\n",
      "Gradient Descent(47/49): loss=2.5865087530804714e+103\n",
      "Gradient Descent(48/49): loss=4.966363869961556e+105\n",
      "Gradient Descent(49/49): loss=9.535931420871471e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.134021315519968\n",
      "Gradient Descent(2/49): loss=168.47222616685778\n",
      "Gradient Descent(3/49): loss=8285.487087606914\n",
      "Gradient Descent(4/49): loss=499205.7047539782\n",
      "Gradient Descent(5/49): loss=45575706.27210603\n",
      "Gradient Descent(6/49): loss=6428843311.04131\n",
      "Gradient Descent(7/49): loss=1127216920161.7324\n",
      "Gradient Descent(8/49): loss=211519494118905.7\n",
      "Gradient Descent(9/49): loss=4.03942571115163e+16\n",
      "Gradient Descent(10/49): loss=7.747459516115315e+18\n",
      "Gradient Descent(11/49): loss=1.4874814924035064e+21\n",
      "Gradient Descent(12/49): loss=2.856623231605023e+23\n",
      "Gradient Descent(13/49): loss=5.4863139309051496e+25\n",
      "Gradient Descent(14/49): loss=1.0536944219933972e+28\n",
      "Gradient Descent(15/49): loss=2.023719114120745e+30\n",
      "Gradient Descent(16/49): loss=3.8867459432146694e+32\n",
      "Gradient Descent(17/49): loss=7.46486851987841e+34\n",
      "Gradient Descent(18/49): loss=1.4336996956059227e+37\n",
      "Gradient Descent(19/49): loss=2.75355799775286e+39\n",
      "Gradient Descent(20/49): loss=5.288472677956738e+41\n",
      "Gradient Descent(21/49): loss=1.0157019865762364e+44\n",
      "Gradient Descent(22/49): loss=1.9507532486755973e+46\n",
      "Gradient Descent(23/49): loss=3.7466090326332977e+48\n",
      "Gradient Descent(24/49): loss=7.195722602576142e+50\n",
      "Gradient Descent(25/49): loss=1.3820076587215307e+53\n",
      "Gradient Descent(26/49): loss=2.654278484946452e+55\n",
      "Gradient Descent(27/49): loss=5.09779684011871e+57\n",
      "Gradient Descent(28/49): loss=9.79080860222897e+59\n",
      "Gradient Descent(29/49): loss=1.8804188572420308e+62\n",
      "Gradient Descent(30/49): loss=3.611525076556483e+64\n",
      "Gradient Descent(31/49): loss=6.93628088676277e+66\n",
      "Gradient Descent(32/49): loss=1.3321793846145478e+69\n",
      "Gradient Descent(33/49): loss=2.5585784972734894e+71\n",
      "Gradient Descent(34/49): loss=4.913995819417744e+73\n",
      "Gradient Descent(35/49): loss=9.437801083291809e+75\n",
      "Gradient Descent(36/49): loss=1.8126203717108228e+78\n",
      "Gradient Descent(37/49): loss=3.4813115713550274e+80\n",
      "Gradient Descent(38/49): loss=6.686193339762347e+82\n",
      "Gradient Descent(39/49): loss=1.2841476684972874e+85\n",
      "Gradient Descent(40/49): loss=2.466328971823664e+87\n",
      "Gradient Descent(41/49): loss=4.73682174291902e+89\n",
      "Gradient Descent(42/49): loss=9.097521247378348e+91\n",
      "Gradient Descent(43/49): loss=1.7472663599854135e+94\n",
      "Gradient Descent(44/49): loss=3.3557929129503793e+96\n",
      "Gradient Descent(45/49): loss=6.445122697092292e+98\n",
      "Gradient Descent(46/49): loss=1.2378477354865272e+101\n",
      "Gradient Descent(47/49): loss=2.3774055022109314e+103\n",
      "Gradient Descent(48/49): loss=4.566035676206378e+105\n",
      "Gradient Descent(49/49): loss=8.769510197987118e+107\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=3.986415897693812\n",
      "Gradient Descent(2/49): loss=159.88416521660326\n",
      "Gradient Descent(3/49): loss=7656.040380997663\n",
      "Gradient Descent(4/49): loss=429438.9752725127\n",
      "Gradient Descent(5/49): loss=34869345.8297536\n",
      "Gradient Descent(6/49): loss=4543406414.159079\n",
      "Gradient Descent(7/49): loss=781254354044.5011\n",
      "Gradient Descent(8/49): loss=147394960496288.28\n",
      "Gradient Descent(9/49): loss=2.8490893002858984e+16\n",
      "Gradient Descent(10/49): loss=5.539723551481771e+18\n",
      "Gradient Descent(11/49): loss=1.0786489116969203e+21\n",
      "Gradient Descent(12/49): loss=2.1009557082850937e+23\n",
      "Gradient Descent(13/49): loss=4.092493573790254e+25\n",
      "Gradient Descent(14/49): loss=7.971999273343936e+27\n",
      "Gradient Descent(15/49): loss=1.5529176383517783e+30\n",
      "Gradient Descent(16/49): loss=3.0250325497640023e+32\n",
      "Gradient Descent(17/49): loss=5.8926655110517845e+34\n",
      "Gradient Descent(18/49): loss=1.1478722395893718e+37\n",
      "Gradient Descent(19/49): loss=2.236018138827122e+39\n",
      "Gradient Descent(20/49): loss=4.355691305462309e+41\n",
      "Gradient Descent(21/49): loss=8.484746359597077e+43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=1.6528012604988447e+46\n",
      "Gradient Descent(23/49): loss=3.21960361695131e+48\n",
      "Gradient Descent(24/49): loss=6.27168413900276e+50\n",
      "Gradient Descent(25/49): loss=1.2217038685265981e+53\n",
      "Gradient Descent(26/49): loss=2.379839783530685e+55\n",
      "Gradient Descent(27/49): loss=4.635851241190355e+57\n",
      "Gradient Descent(28/49): loss=9.030488892224123e+59\n",
      "Gradient Descent(29/49): loss=1.7591101480565224e+62\n",
      "Gradient Descent(30/49): loss=3.4266899056373695e+64\n",
      "Gradient Descent(31/49): loss=6.6750815589177345e+66\n",
      "Gradient Descent(32/49): loss=1.300284386541694e+69\n",
      "Gradient Descent(33/49): loss=2.5329121014638614e+71\n",
      "Gradient Descent(34/49): loss=4.934031185905176e+73\n",
      "Gradient Descent(35/49): loss=9.611333819840994e+75\n",
      "Gradient Descent(36/49): loss=1.8722568689940733e+78\n",
      "Gradient Descent(37/49): loss=3.647096073449542e+80\n",
      "Gradient Descent(38/49): loss=7.104425674302879e+82\n",
      "Gradient Descent(39/49): loss=1.3839192372564521e+85\n",
      "Gradient Descent(40/49): loss=2.6958300966903284e+87\n",
      "Gradient Descent(41/49): loss=5.251390192847277e+89\n",
      "Gradient Descent(42/49): loss=1.022953894289889e+92\n",
      "Gradient Descent(43/49): loss=1.9926812356624478e+94\n",
      "Gradient Descent(44/49): loss=3.881678860724806e+96\n",
      "Gradient Descent(45/49): loss=7.561385387758157e+98\n",
      "Gradient Descent(46/49): loss=1.4729335175225278e+101\n",
      "Gradient Descent(47/49): loss=2.869227047405606e+103\n",
      "Gradient Descent(48/49): loss=5.589161867543596e+105\n",
      "Gradient Descent(49/49): loss=1.0887507285228754e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.057437371876933\n",
      "Gradient Descent(2/49): loss=165.64180781174548\n",
      "Gradient Descent(3/49): loss=8333.298680430787\n",
      "Gradient Descent(4/49): loss=540738.5363843829\n",
      "Gradient Descent(5/49): loss=56062479.132171616\n",
      "Gradient Descent(6/49): loss=8770170188.666258\n",
      "Gradient Descent(7/49): loss=1634699713967.247\n",
      "Gradient Descent(8/49): loss=320187136800277.7\n",
      "Gradient Descent(9/49): loss=6.3481592556002664e+16\n",
      "Gradient Descent(10/49): loss=1.2622246156155632e+19\n",
      "Gradient Descent(11/49): loss=2.5114029779562676e+21\n",
      "Gradient Descent(12/49): loss=4.997628915556343e+23\n",
      "Gradient Descent(13/49): loss=9.94551824630883e+25\n",
      "Gradient Descent(14/49): loss=1.979222029563228e+28\n",
      "Gradient Descent(15/49): loss=3.9387867891818794e+30\n",
      "Gradient Descent(16/49): loss=7.83845788271156e+32\n",
      "Gradient Descent(17/49): loss=1.5599074503189623e+35\n",
      "Gradient Descent(18/49): loss=3.1043240275841376e+37\n",
      "Gradient Descent(19/49): loss=6.177820179307562e+39\n",
      "Gradient Descent(20/49): loss=1.2294290763656813e+42\n",
      "Gradient Descent(21/49): loss=2.4466491584762122e+44\n",
      "Gradient Descent(22/49): loss=4.869001571696628e+46\n",
      "Gradient Descent(23/49): loss=9.689650934814565e+48\n",
      "Gradient Descent(24/49): loss=1.9283077619999903e+51\n",
      "Gradient Descent(25/49): loss=3.837466230731746e+53\n",
      "Gradient Descent(26/49): loss=7.636824039299631e+55\n",
      "Gradient Descent(27/49): loss=1.5197809674564408e+58\n",
      "Gradient Descent(28/49): loss=3.024469566349552e+60\n",
      "Gradient Descent(29/49): loss=6.018904272162277e+62\n",
      "Gradient Descent(30/49): loss=1.1978037088063462e+65\n",
      "Gradient Descent(31/49): loss=2.3837124831274705e+67\n",
      "Gradient Descent(32/49): loss=4.743753221368792e+69\n",
      "Gradient Descent(33/49): loss=9.440398028088507e+71\n",
      "Gradient Descent(34/49): loss=1.8787047042683795e+74\n",
      "Gradient Descent(35/49): loss=3.7387527044289164e+76\n",
      "Gradient Descent(36/49): loss=7.440377273296964e+78\n",
      "Gradient Descent(37/49): loss=1.4806866980909555e+81\n",
      "Gradient Descent(38/49): loss=2.9466692579851e+83\n",
      "Gradient Descent(39/49): loss=5.8640762607979614e+85\n",
      "Gradient Descent(40/49): loss=1.1669918603612736e+88\n",
      "Gradient Descent(41/49): loss=2.3223947670219163e+90\n",
      "Gradient Descent(42/49): loss=4.621726712147754e+92\n",
      "Gradient Descent(43/49): loss=9.19755680864338e+94\n",
      "Gradient Descent(44/49): loss=1.8303776167871186e+97\n",
      "Gradient Descent(45/49): loss=3.6425784474489496e+99\n",
      "Gradient Descent(46/49): loss=7.248983829418701e+101\n",
      "Gradient Descent(47/49): loss=1.442598074887746e+104\n",
      "Gradient Descent(48/49): loss=2.8708702552543553e+106\n",
      "Gradient Descent(49/49): loss=5.713230986493402e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.0913210418986985\n",
      "Gradient Descent(2/49): loss=168.9479234825905\n",
      "Gradient Descent(3/49): loss=8484.058603813224\n",
      "Gradient Descent(4/49): loss=529590.1031550319\n",
      "Gradient Descent(5/49): loss=50628044.418140784\n",
      "Gradient Descent(6/49): loss=7375685074.154561\n",
      "Gradient Descent(7/49): loss=1314906395624.4812\n",
      "Gradient Descent(8/49): loss=249388670575241.62\n",
      "Gradient Descent(9/49): loss=4.806205567656132e+16\n",
      "Gradient Descent(10/49): loss=9.298981999761502e+18\n",
      "Gradient Descent(11/49): loss=1.8008733049257903e+21\n",
      "Gradient Descent(12/49): loss=3.488440543526874e+23\n",
      "Gradient Descent(13/49): loss=6.7577759021336216e+25\n",
      "Gradient Descent(14/49): loss=1.3091280363987172e+28\n",
      "Gradient Descent(15/49): loss=2.5360737782333065e+30\n",
      "Gradient Descent(16/49): loss=4.912945963591e+32\n",
      "Gradient Descent(17/49): loss=9.517484417227313e+34\n",
      "Gradient Descent(18/49): loss=1.8437514782646666e+37\n",
      "Gradient Descent(19/49): loss=3.571762666150976e+39\n",
      "Gradient Descent(20/49): loss=6.9193102911648056e+41\n",
      "Gradient Descent(21/49): loss=1.340426545989388e+44\n",
      "Gradient Descent(22/49): loss=2.596708703427328e+46\n",
      "Gradient Descent(23/49): loss=5.030410738200514e+48\n",
      "Gradient Descent(24/49): loss=9.745040774823634e+50\n",
      "Gradient Descent(25/49): loss=1.8878343070879721e+53\n",
      "Gradient Descent(26/49): loss=3.6571610661985306e+55\n",
      "Gradient Descent(27/49): loss=7.084746269258855e+57\n",
      "Gradient Descent(28/49): loss=1.3724752284961005e+60\n",
      "Gradient Descent(29/49): loss=2.6587942337594826e+62\n",
      "Gradient Descent(30/49): loss=5.150684420890325e+64\n",
      "Gradient Descent(31/49): loss=9.978038039480053e+66\n",
      "Gradient Descent(32/49): loss=1.9329711351273333e+69\n",
      "Gradient Descent(33/49): loss=3.744601287799956e+71\n",
      "Gradient Descent(34/49): loss=7.254137710478347e+73\n",
      "Gradient Descent(35/49): loss=1.4052901731896046e+76\n",
      "Gradient Descent(36/49): loss=2.7223641867325697e+78\n",
      "Gradient Descent(37/49): loss=5.273833765152282e+80\n",
      "Gradient Descent(38/49): loss=1.0216606109501672e+83\n",
      "Gradient Descent(39/49): loss=1.9791871538766998e+85\n",
      "Gradient Descent(40/49): loss=3.834132145338915e+87\n",
      "Gradient Descent(41/49): loss=7.427579185286565e+89\n",
      "Gradient Descent(42/49): loss=1.4388897007832748e+92\n",
      "Gradient Descent(43/49): loss=2.7874540538341704e+94\n",
      "Gradient Descent(44/49): loss=5.399927526068862e+96\n",
      "Gradient Descent(45/49): loss=1.0460878179027763e+99\n",
      "Gradient Descent(46/49): loss=2.0265081660479806e+101\n",
      "Gradient Descent(47/49): loss=3.9258036244916096e+103\n",
      "Gradient Descent(48/49): loss=7.605167527218411e+105\n",
      "Gradient Descent(49/49): loss=1.4732925701179872e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.17033253343301\n",
      "Gradient Descent(2/49): loss=171.69790917161316\n",
      "Gradient Descent(3/49): loss=8523.511202606482\n",
      "Gradient Descent(4/49): loss=518138.9377334809\n",
      "Gradient Descent(5/49): loss=47695992.15539287\n",
      "Gradient Descent(6/49): loss=6783464876.564741\n",
      "Gradient Descent(7/49): loss=1199647606876.7605\n",
      "Gradient Descent(8/49): loss=227096920580036.94\n",
      "Gradient Descent(9/49): loss=4.375466666073526e+16\n",
      "Gradient Descent(10/49): loss=8.466744610165079e+18\n",
      "Gradient Descent(11/49): loss=1.6400740195866795e+21\n",
      "Gradient Descent(12/49): loss=3.177753526368232e+23\n",
      "Gradient Descent(13/49): loss=6.15748605036194e+25\n",
      "Gradient Descent(14/49): loss=1.1931445014182284e+28\n",
      "Gradient Descent(15/49): loss=2.3119806216413673e+30\n",
      "Gradient Descent(16/49): loss=4.47997619010783e+32\n",
      "Gradient Descent(17/49): loss=8.680951131612582e+34\n",
      "Gradient Descent(18/49): loss=1.6821276071428126e+37\n",
      "Gradient Descent(19/49): loss=3.2594968888120174e+39\n",
      "Gradient Descent(20/49): loss=6.31600120804175e+41\n",
      "Gradient Descent(21/49): loss=1.2238659108595041e+44\n",
      "Gradient Descent(22/49): loss=2.371512795323262e+46\n",
      "Gradient Descent(23/49): loss=4.5953342508406795e+48\n",
      "Gradient Descent(24/49): loss=8.904483635424175e+50\n",
      "Gradient Descent(25/49): loss=1.7254420350210044e+53\n",
      "Gradient Descent(26/49): loss=3.343428252678589e+55\n",
      "Gradient Descent(27/49): loss=6.478636925450055e+57\n",
      "Gradient Descent(28/49): loss=1.2553802037827418e+60\n",
      "Gradient Descent(29/49): loss=2.4325787572053985e+62\n",
      "Gradient Descent(30/49): loss=4.7136631533428687e+64\n",
      "Gradient Descent(31/49): loss=9.13377223959146e+66\n",
      "Gradient Descent(32/49): loss=1.769871809052977e+69\n",
      "Gradient Descent(33/49): loss=3.429520835764325e+71\n",
      "Gradient Descent(34/49): loss=6.645460480685884e+73\n",
      "Gradient Descent(35/49): loss=1.2877059832912666e+76\n",
      "Gradient Descent(36/49): loss=2.495217154963736e+78\n",
      "Gradient Descent(37/49): loss=4.835039000527133e+80\n",
      "Gradient Descent(38/49): loss=9.368964977702762e+82\n",
      "Gradient Descent(39/49): loss=1.815445640538824e+85\n",
      "Gradient Descent(40/49): loss=3.5178302849836157e+87\n",
      "Gradient Descent(41/49): loss=6.816579707820438e+89\n",
      "Gradient Descent(42/49): loss=1.320864144908163e+92\n",
      "Gradient Descent(43/49): loss=2.559468478454636e+94\n",
      "Gradient Descent(44/49): loss=4.95954025056688e+96\n",
      "Gradient Descent(45/49): loss=9.610213879971241e+98\n",
      "Gradient Descent(46/49): loss=1.8621929887197966e+101\n",
      "Gradient Descent(47/49): loss=3.6084136841786316e+103\n",
      "Gradient Descent(48/49): loss=6.992105219512561e+105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=1.3548761222997927e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.021306322230437\n",
      "Gradient Descent(2/49): loss=162.9475716514784\n",
      "Gradient Descent(3/49): loss=7876.441735551701\n",
      "Gradient Descent(4/49): loss=445807.2321477634\n",
      "Gradient Descent(5/49): loss=36500725.80706137\n",
      "Gradient Descent(6/49): loss=4794623724.842003\n",
      "Gradient Descent(7/49): loss=831471324492.634\n",
      "Gradient Descent(8/49): loss=158246730443344.06\n",
      "Gradient Descent(9/49): loss=3.0859998586346664e+16\n",
      "Gradient Descent(10/49): loss=6.053790169316415e+18\n",
      "Gradient Descent(11/49): loss=1.18924682893934e+21\n",
      "Gradient Descent(12/49): loss=2.3370190891821998e+23\n",
      "Gradient Descent(13/49): loss=4.5929007700553955e+25\n",
      "Gradient Descent(14/49): loss=9.026513962584939e+27\n",
      "Gradient Descent(15/49): loss=1.774005643067852e+30\n",
      "Gradient Descent(16/49): loss=3.486505835696968e+32\n",
      "Gradient Descent(17/49): loss=6.852134911952724e+34\n",
      "Gradient Descent(18/49): loss=1.346670789202771e+37\n",
      "Gradient Descent(19/49): loss=2.6466528512024623e+39\n",
      "Gradient Descent(20/49): loss=5.201546951524205e+41\n",
      "Gradient Descent(21/49): loss=1.0222757660926318e+44\n",
      "Gradient Descent(22/49): loss=2.0091095050699843e+46\n",
      "Gradient Descent(23/49): loss=3.948563721677409e+48\n",
      "Gradient Descent(24/49): loss=7.760231796707938e+50\n",
      "Gradient Descent(25/49): loss=1.525141843556325e+53\n",
      "Gradient Descent(26/49): loss=2.9974074278999486e+55\n",
      "Gradient Descent(27/49): loss=5.8908955431195816e+57\n",
      "Gradient Descent(28/49): loss=1.1577555315615116e+60\n",
      "Gradient Descent(29/49): loss=2.2753719889446083e+62\n",
      "Gradient Descent(30/49): loss=4.4718574404829935e+64\n",
      "Gradient Descent(31/49): loss=8.788676781276175e+66\n",
      "Gradient Descent(32/49): loss=1.7272652492562066e+69\n",
      "Gradient Descent(33/49): loss=3.394646674962726e+71\n",
      "Gradient Descent(34/49): loss=6.671601858948934e+73\n",
      "Gradient Descent(35/49): loss=1.3111901068413806e+76\n",
      "Gradient Descent(36/49): loss=2.5769216038763472e+78\n",
      "Gradient Descent(37/49): loss=5.064502025965828e+80\n",
      "Gradient Descent(38/49): loss=9.953419123200726e+82\n",
      "Gradient Descent(39/49): loss=1.9561755871389303e+85\n",
      "Gradient Descent(40/49): loss=3.844531090626652e+87\n",
      "Gradient Descent(41/49): loss=7.555773318085818e+89\n",
      "Gradient Descent(42/49): loss=1.4849589999021604e+92\n",
      "Gradient Descent(43/49): loss=2.9184348690188555e+94\n",
      "Gradient Descent(44/49): loss=5.735688382821672e+96\n",
      "Gradient Descent(45/49): loss=1.1272521985695167e+99\n",
      "Gradient Descent(46/49): loss=2.2154228653452656e+101\n",
      "Gradient Descent(47/49): loss=4.3540376133423304e+103\n",
      "Gradient Descent(48/49): loss=8.557121908844016e+105\n",
      "Gradient Descent(49/49): loss=1.6817570693104316e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.092842592403956\n",
      "Gradient Descent(2/49): loss=168.79867804967725\n",
      "Gradient Descent(3/49): loss=8571.244451889215\n",
      "Gradient Descent(4/49): loss=561058.3775001693\n",
      "Gradient Descent(5/49): loss=58644500.68725696\n",
      "Gradient Descent(6/49): loss=9250257200.440819\n",
      "Gradient Descent(7/49): loss=1739094743057.953\n",
      "Gradient Descent(8/49): loss=343633699839706.7\n",
      "Gradient Descent(9/49): loss=6.873327454729609e+16\n",
      "Gradient Descent(10/49): loss=1.3787609051512197e+19\n",
      "Gradient Descent(11/49): loss=2.7676001256794054e+21\n",
      "Gradient Descent(12/49): loss=5.5563035436138916e+23\n",
      "Gradient Descent(13/49): loss=1.115538337499061e+26\n",
      "Gradient Descent(14/49): loss=2.23968400369061e+28\n",
      "Gradient Descent(15/49): loss=4.49665803796662e+30\n",
      "Gradient Descent(16/49): loss=9.028033808529009e+32\n",
      "Gradient Descent(17/49): loss=1.8125773094348954e+35\n",
      "Gradient Descent(18/49): loss=3.6391496258934743e+37\n",
      "Gradient Descent(19/49): loss=7.306397365082532e+39\n",
      "Gradient Descent(20/49): loss=1.466920792428702e+42\n",
      "Gradient Descent(21/49): loss=2.94516778163856e+44\n",
      "Gradient Descent(22/49): loss=5.913075407629097e+46\n",
      "Gradient Descent(23/49): loss=1.1871806080081538e+49\n",
      "Gradient Descent(24/49): loss=2.383527519736828e+51\n",
      "Gradient Descent(25/49): loss=4.785458420585356e+53\n",
      "Gradient Descent(26/49): loss=9.607865697176738e+55\n",
      "Gradient Descent(27/49): loss=1.928991439104361e+58\n",
      "Gradient Descent(28/49): loss=3.8728767547524654e+60\n",
      "Gradient Descent(29/49): loss=7.775656259245054e+62\n",
      "Gradient Descent(30/49): loss=1.5611348899172938e+65\n",
      "Gradient Descent(31/49): loss=3.134323410476602e+67\n",
      "Gradient Descent(32/49): loss=6.292847149154572e+69\n",
      "Gradient Descent(33/49): loss=1.2634281807122622e+72\n",
      "Gradient Descent(34/49): loss=2.5366113779394033e+74\n",
      "Gradient Descent(35/49): loss=5.0928081080669625e+76\n",
      "Gradient Descent(36/49): loss=1.0224938140371229e+79\n",
      "Gradient Descent(37/49): loss=2.052882373651801e+81\n",
      "Gradient Descent(38/49): loss=4.121615194336305e+83\n",
      "Gradient Descent(39/49): loss=8.275053665137715e+85\n",
      "Gradient Descent(40/49): loss=1.661399959292776e+88\n",
      "Gradient Descent(41/49): loss=3.335627702774682e+90\n",
      "Gradient Descent(42/49): loss=6.697010018138266e+92\n",
      "Gradient Descent(43/49): loss=1.3445728114602238e+95\n",
      "Gradient Descent(44/49): loss=2.699527162751078e+97\n",
      "Gradient Descent(45/49): loss=5.419897561751648e+99\n",
      "Gradient Descent(46/49): loss=1.0881642528073611e+102\n",
      "Gradient Descent(47/49): loss=2.1847302971997988e+104\n",
      "Gradient Descent(48/49): loss=4.386329048384561e+106\n",
      "Gradient Descent(49/49): loss=8.806525247240903e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.1270367700685675\n",
      "Gradient Descent(2/49): loss=172.1660335153913\n",
      "Gradient Descent(3/49): loss=8726.418185826384\n",
      "Gradient Descent(4/49): loss=549547.0128890004\n",
      "Gradient Descent(5/49): loss=52968612.44607283\n",
      "Gradient Descent(6/49): loss=7780394009.307108\n",
      "Gradient Descent(7/49): loss=1398995786687.1516\n",
      "Gradient Descent(8/49): loss=267670169655407.7\n",
      "Gradient Descent(9/49): loss=5.204195883941346e+16\n",
      "Gradient Descent(10/49): loss=1.0158331598799872e+19\n",
      "Gradient Descent(11/49): loss=1.9847597514691626e+21\n",
      "Gradient Descent(12/49): loss=3.8787741305365894e+23\n",
      "Gradient Descent(13/49): loss=7.580633253607447e+25\n",
      "Gradient Descent(14/49): loss=1.4815707676416124e+28\n",
      "Gradient Descent(15/49): loss=2.895614530890704e+30\n",
      "Gradient Descent(16/49): loss=5.659257316129563e+32\n",
      "Gradient Descent(17/49): loss=1.1060588078361265e+35\n",
      "Gradient Descent(18/49): loss=2.1617079331845973e+37\n",
      "Gradient Descent(19/49): loss=4.224894018472986e+39\n",
      "Gradient Descent(20/49): loss=8.257234588557504e+41\n",
      "Gradient Descent(21/49): loss=1.6138138092359874e+44\n",
      "Gradient Descent(22/49): loss=3.154076565667667e+46\n",
      "Gradient Descent(23/49): loss=6.164403183035321e+48\n",
      "Gradient Descent(24/49): loss=1.204785800605007e+51\n",
      "Gradient Descent(25/49): loss=2.3546623772717483e+53\n",
      "Gradient Descent(26/49): loss=4.6020088451888216e+55\n",
      "Gradient Descent(27/49): loss=8.994276893206652e+57\n",
      "Gradient Descent(28/49): loss=1.757863132232981e+60\n",
      "Gradient Descent(29/49): loss=3.435610031083148e+62\n",
      "Gradient Descent(30/49): loss=6.714638966621524e+64\n",
      "Gradient Descent(31/49): loss=1.312325206998467e+67\n",
      "Gradient Descent(32/49): loss=2.564839982439355e+69\n",
      "Gradient Descent(33/49): loss=5.01278501733986e+71\n",
      "Gradient Descent(34/49): loss=9.797107734638716e+73\n",
      "Gradient Descent(35/49): loss=1.9147703249211888e+76\n",
      "Gradient Descent(36/49): loss=3.742273226450235e+78\n",
      "Gradient Descent(37/49): loss=7.31398889941699e+80\n",
      "Gradient Descent(38/49): loss=1.4294636009657935e+83\n",
      "Gradient Descent(39/49): loss=2.7937780800418596e+85\n",
      "Gradient Descent(40/49): loss=5.4602271476161305e+87\n",
      "Gradient Descent(41/49): loss=1.06715994074652e+90\n",
      "Gradient Descent(42/49): loss=2.0856830830404136e+92\n",
      "Gradient Descent(43/49): loss=4.076309236118758e+94\n",
      "Gradient Descent(44/49): loss=7.966836919559558e+96\n",
      "Gradient Descent(45/49): loss=1.5570577899357537e+99\n",
      "Gradient Descent(46/49): loss=3.043151235149946e+101\n",
      "Gradient Descent(47/49): loss=5.9476080463119205e+103\n",
      "Gradient Descent(48/49): loss=1.162414837092767e+106\n",
      "Gradient Descent(49/49): loss=2.2718515459862576e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.20681922482781\n",
      "Gradient Descent(2/49): loss=174.96981204614244\n",
      "Gradient Descent(3/49): loss=8767.209345509855\n",
      "Gradient Descent(4/49): loss=537698.1501374597\n",
      "Gradient Descent(5/49): loss=49904741.89271802\n",
      "Gradient Descent(6/49): loss=7155945057.650306\n",
      "Gradient Descent(7/49): loss=1276380990892.5684\n",
      "Gradient Descent(8/49): loss=243744996894388.47\n",
      "Gradient Descent(9/49): loss=4.737789554161292e+16\n",
      "Gradient Descent(10/49): loss=9.249179545281917e+18\n",
      "Gradient Descent(11/49): loss=1.8075399157239828e+21\n",
      "Gradient Descent(12/49): loss=3.5333201145437666e+23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=6.907242612174858e+25\n",
      "Gradient Descent(14/49): loss=1.3503080606175951e+28\n",
      "Gradient Descent(15/49): loss=2.639748546600934e+30\n",
      "Gradient Descent(16/49): loss=5.160510101174785e+32\n",
      "Gradient Descent(17/49): loss=1.0088411665876913e+35\n",
      "Gradient Descent(18/49): loss=1.972209201040894e+37\n",
      "Gradient Descent(19/49): loss=3.855521868520986e+39\n",
      "Gradient Descent(20/49): loss=7.537257667237943e+41\n",
      "Gradient Descent(21/49): loss=1.4734776541169354e+44\n",
      "Gradient Descent(22/49): loss=2.880538908716224e+46\n",
      "Gradient Descent(23/49): loss=5.631238710532055e+48\n",
      "Gradient Descent(24/49): loss=1.1008651651728363e+51\n",
      "Gradient Descent(25/49): loss=2.152109285701785e+53\n",
      "Gradient Descent(26/49): loss=4.207213130301348e+55\n",
      "Gradient Descent(27/49): loss=8.224787858768133e+57\n",
      "Gradient Descent(28/49): loss=1.6078846786850414e+60\n",
      "Gradient Descent(29/49): loss=3.1432946166436433e+62\n",
      "Gradient Descent(30/49): loss=6.144906520970768e+64\n",
      "Gradient Descent(31/49): loss=1.2012833907306333e+67\n",
      "Gradient Descent(32/49): loss=2.3484194265941695e+69\n",
      "Gradient Descent(33/49): loss=4.5909848132092425e+71\n",
      "Gradient Descent(34/49): loss=8.975032873784866e+73\n",
      "Gradient Descent(35/49): loss=1.7545519831334947e+76\n",
      "Gradient Descent(36/49): loss=3.4300182571023472e+78\n",
      "Gradient Descent(37/49): loss=6.705429851695829e+80\n",
      "Gradient Descent(38/49): loss=1.3108615210112398e+83\n",
      "Gradient Descent(39/49): loss=2.5626364979916887e+85\n",
      "Gradient Descent(40/49): loss=5.00976320959749e+87\n",
      "Gradient Descent(41/49): loss=9.793713402546723e+89\n",
      "Gradient Descent(42/49): loss=1.9145979200667033e+92\n",
      "Gradient Descent(43/49): loss=3.742896126172766e+94\n",
      "Gradient Descent(44/49): loss=7.317082748544488e+96\n",
      "Gradient Descent(45/49): loss=1.430435100099669e+99\n",
      "Gradient Descent(46/49): loss=2.7963939262600355e+101\n",
      "Gradient Descent(47/49): loss=5.466741546176488e+103\n",
      "Gradient Descent(48/49): loss=1.0687071965093546e+106\n",
      "Gradient Descent(49/49): loss=2.0892428555904923e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.056365539895407\n",
      "Gradient Descent(2/49): loss=166.0549087145274\n",
      "Gradient Descent(3/49): loss=8102.107469610022\n",
      "Gradient Descent(4/49): loss=462719.2671388475\n",
      "Gradient Descent(5/49): loss=38200561.1137689\n",
      "Gradient Descent(6/49): loss=5058524238.446937\n",
      "Gradient Descent(7/49): loss=884672544918.59\n",
      "Gradient Descent(8/49): loss=169844144816642.8\n",
      "Gradient Descent(9/49): loss=3.3414311269706972e+16\n",
      "Gradient Descent(10/49): loss=6.612967684222202e+18\n",
      "Gradient Descent(11/49): loss=1.3106195465982986e+21\n",
      "Gradient Descent(12/49): loss=2.598384080457675e+23\n",
      "Gradient Descent(13/49): loss=5.151869103854712e+25\n",
      "Gradient Descent(14/49): loss=1.0214909994219915e+28\n",
      "Gradient Descent(15/49): loss=2.0253786441244992e+30\n",
      "Gradient Descent(16/49): loss=4.015858233568836e+32\n",
      "Gradient Descent(17/49): loss=7.962521720518813e+34\n",
      "Gradient Descent(18/49): loss=1.57878471503384e+37\n",
      "Gradient Descent(19/49): loss=3.130366609401305e+39\n",
      "Gradient Descent(20/49): loss=6.206796309326678e+41\n",
      "Gradient Descent(21/49): loss=1.2306648154494588e+44\n",
      "Gradient Descent(22/49): loss=2.4401250061947765e+46\n",
      "Gradient Descent(23/49): loss=4.838206123535257e+48\n",
      "Gradient Descent(24/49): loss=9.593048894888002e+50\n",
      "Gradient Descent(25/49): loss=1.9020807454293968e+53\n",
      "Gradient Descent(26/49): loss=3.7713882226361156e+55\n",
      "Gradient Descent(27/49): loss=7.47779459942406e+57\n",
      "Gradient Descent(28/49): loss=1.4826745158601842e+60\n",
      "Gradient Descent(29/49): loss=2.9398022247771017e+62\n",
      "Gradient Descent(30/49): loss=5.828951012751601e+64\n",
      "Gradient Descent(31/49): loss=1.1557467921718323e+67\n",
      "Gradient Descent(32/49): loss=2.291579813749254e+69\n",
      "Gradient Descent(33/49): loss=4.543675204942605e+71\n",
      "Gradient Descent(34/49): loss=9.009061890029915e+73\n",
      "Gradient Descent(35/49): loss=1.7862895668708926e+76\n",
      "Gradient Descent(36/49): loss=3.5418009729104665e+78\n",
      "Gradient Descent(37/49): loss=7.02257593861651e+80\n",
      "Gradient Descent(38/49): loss=1.3924151354306832e+83\n",
      "Gradient Descent(39/49): loss=2.7608386528297312e+85\n",
      "Gradient Descent(40/49): loss=5.4741074504343e+87\n",
      "Gradient Descent(41/49): loss=1.0853894829452073e+90\n",
      "Gradient Descent(42/49): loss=2.152077467157861e+92\n",
      "Gradient Descent(43/49): loss=4.2670741677736914e+94\n",
      "Gradient Descent(44/49): loss=8.460625712199554e+96\n",
      "Gradient Descent(45/49): loss=1.6775472988621797e+99\n",
      "Gradient Descent(46/49): loss=3.326190090009536e+101\n",
      "Gradient Descent(47/49): loss=6.595069195593657e+103\n",
      "Gradient Descent(48/49): loss=1.3076503903162192e+106\n",
      "Gradient Descent(49/49): loss=2.592769677741385e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.128418224997627\n",
      "Gradient Descent(2/49): loss=172.00057835113174\n",
      "Gradient Descent(3/49): loss=8814.828763984739\n",
      "Gradient Descent(4/49): loss=582043.9867160862\n",
      "Gradient Descent(5/49): loss=61333167.25919495\n",
      "Gradient Descent(6/49): loss=9754335155.16884\n",
      "Gradient Descent(7/49): loss=1849653136059.2166\n",
      "Gradient Descent(8/49): loss=368682647132602.8\n",
      "Gradient Descent(9/49): loss=7.439341797069789e+16\n",
      "Gradient Descent(10/49): loss=1.50547199211947e+19\n",
      "Gradient Descent(11/49): loss=3.0486307481934417e+21\n",
      "Gradient Descent(12/49): loss=6.174554230548739e+23\n",
      "Gradient Descent(13/49): loss=1.2506114795962649e+26\n",
      "Gradient Descent(14/49): loss=2.533045260374945e+28\n",
      "Gradient Descent(15/49): loss=5.13055515630401e+30\n",
      "Gradient Descent(16/49): loss=1.0391685042689476e+33\n",
      "Gradient Descent(17/49): loss=2.1047845026852912e+35\n",
      "Gradient Descent(18/49): loss=4.263137207821022e+37\n",
      "Gradient Descent(19/49): loss=8.634774219254939e+39\n",
      "Gradient Descent(20/49): loss=1.7489309465400604e+42\n",
      "Gradient Descent(21/49): loss=3.5423734067485524e+44\n",
      "Gradient Descent(22/49): loss=7.174902690476394e+46\n",
      "Gradient Descent(23/49): loss=1.4532411665196675e+49\n",
      "Gradient Descent(24/49): loss=2.9434683356463138e+51\n",
      "Gradient Descent(25/49): loss=5.961849996109161e+53\n",
      "Gradient Descent(26/49): loss=1.2075433238288906e+56\n",
      "Gradient Descent(27/49): loss=2.4458194685800772e+58\n",
      "Gradient Descent(28/49): loss=4.953886750760808e+60\n",
      "Gradient Descent(29/49): loss=1.0033853379051832e+63\n",
      "Gradient Descent(30/49): loss=2.032307533409944e+65\n",
      "Gradient Descent(31/49): loss=4.116338712879529e+67\n",
      "Gradient Descent(32/49): loss=8.337441120793892e+69\n",
      "Gradient Descent(33/49): loss=1.6887075940861642e+72\n",
      "Gradient Descent(34/49): loss=3.4203939758109443e+74\n",
      "Gradient Descent(35/49): loss=6.92783936706025e+76\n",
      "Gradient Descent(36/49): loss=1.4031997084315993e+79\n",
      "Gradient Descent(37/49): loss=2.8421118294173096e+81\n",
      "Gradient Descent(38/49): loss=5.756557389783468e+83\n",
      "Gradient Descent(39/49): loss=1.1659623185434135e+86\n",
      "Gradient Descent(40/49): loss=2.3615991923850768e+88\n",
      "Gradient Descent(41/49): loss=4.783302733523247e+90\n",
      "Gradient Descent(42/49): loss=9.688343862204599e+92\n",
      "Gradient Descent(43/49): loss=1.9623262841902863e+95\n",
      "Gradient Descent(44/49): loss=3.974595142773721e+97\n",
      "Gradient Descent(45/49): loss=8.050346507731327e+99\n",
      "Gradient Descent(46/49): loss=1.630557995633085e+102\n",
      "Gradient Descent(47/49): loss=3.302614831013376e+104\n",
      "Gradient Descent(48/49): loss=6.689283515974726e+106\n",
      "Gradient Descent(49/49): loss=1.3548813969130914e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.162924313535349\n",
      "Gradient Descent(2/49): loss=175.4300187417717\n",
      "Gradient Descent(3/49): loss=8974.523005020621\n",
      "Gradient Descent(4/49): loss=570159.5551458661\n",
      "Gradient Descent(5/49): loss=55406220.995051265\n",
      "Gradient Descent(6/49): loss=8205376258.363232\n",
      "Gradient Descent(7/49): loss=1488056747757.7695\n",
      "Gradient Descent(8/49): loss=287202390636001.3\n",
      "Gradient Descent(9/49): loss=5.6331706539492664e+16\n",
      "Gradient Descent(10/49): loss=1.10927816962909e+19\n",
      "Gradient Descent(11/49): loss=2.1864872236143506e+21\n",
      "Gradient Descent(12/49): loss=4.310771294635333e+23\n",
      "Gradient Descent(13/49): loss=8.499387386999492e+25\n",
      "Gradient Descent(14/49): loss=1.675815585539038e+28\n",
      "Gradient Descent(15/49): loss=3.304199564301021e+30\n",
      "Gradient Descent(16/49): loss=6.514883644073947e+32\n",
      "Gradient Descent(17/49): loss=1.2845385499370917e+35\n",
      "Gradient Descent(18/49): loss=2.5327226921161953e+37\n",
      "Gradient Descent(19/49): loss=4.9937655112272324e+39\n",
      "Gradient Descent(20/49): loss=9.846199952256654e+41\n",
      "Gradient Descent(21/49): loss=1.9413737658864295e+44\n",
      "Gradient Descent(22/49): loss=3.8278037392697124e+46\n",
      "Gradient Descent(23/49): loss=7.547274885654892e+48\n",
      "Gradient Descent(24/49): loss=1.4880950560711297e+51\n",
      "Gradient Descent(25/49): loss=2.934074787864711e+53\n",
      "Gradient Descent(26/49): loss=5.785110854085655e+55\n",
      "Gradient Descent(27/49): loss=1.1406494385380179e+58\n",
      "Gradient Descent(28/49): loss=2.2490167854229586e+60\n",
      "Gradient Descent(29/49): loss=4.434383019201129e+62\n",
      "Gradient Descent(30/49): loss=8.74326634128739e+64\n",
      "Gradient Descent(31/49): loss=1.7239085118195982e+67\n",
      "Gradient Descent(32/49): loss=3.3990278245216902e+69\n",
      "Gradient Descent(33/49): loss=6.701858058394346e+71\n",
      "Gradient Descent(34/49): loss=1.3214043471734576e+74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(35/49): loss=2.605410967398624e+76\n",
      "Gradient Descent(36/49): loss=5.1370848927213025e+78\n",
      "Gradient Descent(37/49): loss=1.012878257028849e+81\n",
      "Gradient Descent(38/49): loss=1.9970905386737664e+83\n",
      "Gradient Descent(39/49): loss=3.937660416721509e+85\n",
      "Gradient Descent(40/49): loss=7.763879131745327e+87\n",
      "Gradient Descent(41/49): loss=1.530802882756919e+90\n",
      "Gradient Descent(42/49): loss=3.018281745622778e+92\n",
      "Gradient Descent(43/49): loss=5.951141586272099e+94\n",
      "Gradient Descent(44/49): loss=1.1733856930757963e+97\n",
      "Gradient Descent(45/49): loss=2.3135628093457126e+99\n",
      "Gradient Descent(46/49): loss=4.5616483176618956e+101\n",
      "Gradient Descent(47/49): loss=8.994195139189529e+103\n",
      "Gradient Descent(48/49): loss=1.7733841052281398e+106\n",
      "Gradient Descent(49/49): loss=3.4965787777639346e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.2434813897043675\n",
      "Gradient Descent(2/49): loss=178.2883744138407\n",
      "Gradient Descent(3/49): loss=9016.689258230257\n",
      "Gradient Descent(4/49): loss=557901.0154823308\n",
      "Gradient Descent(5/49): loss=52205224.85167199\n",
      "Gradient Descent(6/49): loss=7547098172.564196\n",
      "Gradient Descent(7/49): loss=1357651979758.2517\n",
      "Gradient Descent(8/49): loss=261532095495156.3\n",
      "Gradient Descent(9/49): loss=5.128320209147227e+16\n",
      "Gradient Descent(10/49): loss=1.009999282406601e+19\n",
      "Gradient Descent(11/49): loss=1.9912535488306366e+21\n",
      "Gradient Descent(12/49): loss=3.926838970925607e+23\n",
      "Gradient Descent(13/49): loss=7.744375915628617e+25\n",
      "Gradient Descent(14/49): loss=1.5273417670368474e+28\n",
      "Gradient Descent(15/49): loss=3.0122262727971444e+30\n",
      "Gradient Descent(16/49): loss=5.9407234104886734e+32\n",
      "Gradient Descent(17/49): loss=1.1716318375205968e+35\n",
      "Gradient Descent(18/49): loss=2.3106970939651916e+37\n",
      "Gradient Descent(19/49): loss=4.5571662993402666e+39\n",
      "Gradient Descent(20/49): loss=8.987662119459722e+41\n",
      "Gradient Descent(21/49): loss=1.7725504211389163e+44\n",
      "Gradient Descent(22/49): loss=3.4958312342603157e+46\n",
      "Gradient Descent(23/49): loss=6.894492745139022e+48\n",
      "Gradient Descent(24/49): loss=1.3597346962118634e+51\n",
      "Gradient Descent(25/49): loss=2.6816743630492326e+53\n",
      "Gradient Descent(26/49): loss=5.288809213646326e+55\n",
      "Gradient Descent(27/49): loss=1.043061129411283e+58\n",
      "Gradient Descent(28/49): loss=2.057129451524832e+60\n",
      "Gradient Descent(29/49): loss=4.0570791691943027e+62\n",
      "Gradient Descent(30/49): loss=8.001388231989815e+64\n",
      "Gradient Descent(31/49): loss=1.5780370796101626e+67\n",
      "Gradient Descent(32/49): loss=3.1122112218835303e+69\n",
      "Gradient Descent(33/49): loss=6.137915778259605e+71\n",
      "Gradient Descent(34/49): loss=1.2105222754838415e+74\n",
      "Gradient Descent(35/49): loss=2.387397012896201e+76\n",
      "Gradient Descent(36/49): loss=4.708434212751315e+78\n",
      "Gradient Descent(37/49): loss=9.285993329158542e+80\n",
      "Gradient Descent(38/49): loss=1.8313874254768695e+83\n",
      "Gradient Descent(39/49): loss=3.611869816515044e+85\n",
      "Gradient Descent(40/49): loss=7.123344514640661e+87\n",
      "Gradient Descent(41/49): loss=1.4048689363676717e+90\n",
      "Gradient Descent(42/49): loss=2.770688297209867e+92\n",
      "Gradient Descent(43/49): loss=5.4643628608828306e+94\n",
      "Gradient Descent(44/49): loss=1.0776838919580198e+97\n",
      "Gradient Descent(45/49): loss=2.1254126062889938e+99\n",
      "Gradient Descent(46/49): loss=4.1917474879990794e+101\n",
      "Gradient Descent(47/49): loss=8.266981644484258e+103\n",
      "Gradient Descent(48/49): loss=1.6304175217115055e+106\n",
      "Gradient Descent(49/49): loss=3.2155161453365076e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.09159355068872\n",
      "Gradient Descent(2/49): loss=169.20659459125878\n",
      "Gradient Descent(3/49): loss=8333.137735074524\n",
      "Gradient Descent(4/49): loss=480190.50421929726\n",
      "Gradient Descent(5/49): loss=39971396.29682337\n",
      "Gradient Descent(6/49): loss=5335687829.603523\n",
      "Gradient Descent(7/49): loss=941021013440.8422\n",
      "Gradient Descent(8/49): loss=182234808549241.06\n",
      "Gradient Descent(9/49): loss=3.616739636100908e+16\n",
      "Gradient Descent(10/49): loss=7.220989274174504e+18\n",
      "Gradient Descent(11/49): loss=1.443762218403309e+21\n",
      "Gradient Descent(12/49): loss=2.8876327219337735e+23\n",
      "Gradient Descent(13/49): loss=5.77594734422372e+25\n",
      "Gradient Descent(14/49): loss=1.1553479677950567e+28\n",
      "Gradient Descent(15/49): loss=2.3110234749546495e+30\n",
      "Gradient Descent(16/49): loss=4.6227071075335656e+32\n",
      "Gradient Descent(17/49): loss=9.246737087014736e+34\n",
      "Gradient Descent(18/49): loss=1.8496121421227236e+37\n",
      "Gradient Descent(19/49): loss=3.699753862892563e+39\n",
      "Gradient Descent(20/49): loss=7.400567060079138e+41\n",
      "Gradient Descent(21/49): loss=1.4803253104111344e+44\n",
      "Gradient Descent(22/49): loss=2.961074478857448e+46\n",
      "Gradient Descent(23/49): loss=5.922996795419388e+48\n",
      "Gradient Descent(24/49): loss=1.1847689509136172e+51\n",
      "Gradient Descent(25/49): loss=2.3698771340517376e+53\n",
      "Gradient Descent(26/49): loss=4.740432829686868e+55\n",
      "Gradient Descent(27/49): loss=9.482222976833025e+57\n",
      "Gradient Descent(28/49): loss=1.8967160977223213e+60\n",
      "Gradient Descent(29/49): loss=3.793975277894743e+62\n",
      "Gradient Descent(30/49): loss=7.589036876189292e+64\n",
      "Gradient Descent(31/49): loss=1.5180246704221823e+67\n",
      "Gradient Descent(32/49): loss=3.0364839934306392e+69\n",
      "Gradient Descent(33/49): loss=6.073837416487059e+71\n",
      "Gradient Descent(34/49): loss=1.2149413941167244e+74\n",
      "Gradient Descent(35/49): loss=2.4302306596675335e+76\n",
      "Gradient Descent(36/49): loss=4.8611571618085904e+78\n",
      "Gradient Descent(37/49): loss=9.723706207803108e+80\n",
      "Gradient Descent(38/49): loss=1.945019658251336e+83\n",
      "Gradient Descent(39/49): loss=3.8905962296024816e+85\n",
      "Gradient Descent(40/49): loss=7.7823064448643085e+87\n",
      "Gradient Descent(41/49): loss=1.5566841180011977e+90\n",
      "Gradient Descent(42/49): loss=3.113813957861231e+92\n",
      "Gradient Descent(43/49): loss=6.22851948706262e+94\n",
      "Gradient Descent(44/49): loss=1.2458822372087531e+97\n",
      "Gradient Descent(45/49): loss=2.492121205073558e+99\n",
      "Gradient Descent(46/49): loss=4.984955973601149e+101\n",
      "Gradient Descent(47/49): loss=9.971339278423269e+103\n",
      "Gradient Descent(48/49): loss=1.994553362797271e+106\n",
      "Gradient Descent(49/49): loss=3.9896778215684777e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.164164269657946\n",
      "Gradient Descent(2/49): loss=175.24793511487815\n",
      "Gradient Descent(3/49): loss=9064.158064243808\n",
      "Gradient Descent(4/49): loss=603713.994951789\n",
      "Gradient Descent(5/49): loss=64132380.37683831\n",
      "Gradient Descent(6/49): loss=10283489484.449196\n",
      "Gradient Descent(7/49): loss=1966709308076.454\n",
      "Gradient Descent(8/49): loss=395435730293773.56\n",
      "Gradient Descent(9/49): loss=8.049178806818222e+16\n",
      "Gradient Descent(10/49): loss=1.6431956655914134e+19\n",
      "Gradient Descent(11/49): loss=3.35677691631022e+21\n",
      "Gradient Descent(12/49): loss=6.858430062832164e+23\n",
      "Gradient Descent(13/49): loss=1.401338548612481e+26\n",
      "Gradient Descent(14/49): loss=2.863289015583941e+28\n",
      "Gradient Descent(15/49): loss=5.850435344899106e+30\n",
      "Gradient Descent(16/49): loss=1.1953948660920474e+33\n",
      "Gradient Descent(17/49): loss=2.4425003597817877e+35\n",
      "Gradient Descent(18/49): loss=4.990659011823729e+37\n",
      "Gradient Descent(19/49): loss=1.0197205262338094e+40\n",
      "Gradient Descent(20/49): loss=2.0835523938191188e+42\n",
      "Gradient Descent(21/49): loss=4.25723565185601e+44\n",
      "Gradient Descent(22/49): loss=8.698631937735697e+46\n",
      "Gradient Descent(23/49): loss=1.77735516136059e+49\n",
      "Gradient Descent(24/49): loss=3.631595625887199e+51\n",
      "Gradient Descent(25/49): loss=7.4202877830448e+53\n",
      "Gradient Descent(26/49): loss=1.5161564352244299e+56\n",
      "Gradient Descent(27/49): loss=3.097899169524331e+58\n",
      "Gradient Descent(28/49): loss=6.329808086801395e+60\n",
      "Gradient Descent(29/49): loss=1.2933432698485307e+63\n",
      "Gradient Descent(30/49): loss=2.6426343274931286e+65\n",
      "Gradient Descent(31/49): loss=5.399584434890967e+67\n",
      "Gradient Descent(32/49): loss=1.1032745532058969e+70\n",
      "Gradient Descent(33/49): loss=2.254274851016903e+72\n",
      "Gradient Descent(34/49): loss=4.606065724221363e+74\n",
      "Gradient Descent(35/49): loss=9.41138186689008e+76\n",
      "Gradient Descent(36/49): loss=1.922988379836911e+79\n",
      "Gradient Descent(37/49): loss=3.929161903415307e+81\n",
      "Gradient Descent(38/49): loss=8.028292539427249e+83\n",
      "Gradient Descent(39/49): loss=1.6403875096772678e+86\n",
      "Gradient Descent(40/49): loss=3.3517353393517615e+88\n",
      "Gradient Descent(41/49): loss=6.848460939128704e+90\n",
      "Gradient Descent(42/49): loss=1.3993174426428077e+93\n",
      "Gradient Descent(43/49): loss=2.8591669320866826e+95\n",
      "Gradient Descent(44/49): loss=5.842016469185472e+97\n",
      "Gradient Descent(45/49): loss=1.1936748443479706e+100\n",
      "Gradient Descent(46/49): loss=2.4389859931836347e+102\n",
      "Gradient Descent(47/49): loss=4.983478292361358e+104\n",
      "Gradient Descent(48/49): loss=1.0182533216609474e+107\n",
      "Gradient Descent(49/49): loss=2.0805545168377317e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.1989836722990495\n",
      "Gradient Descent(2/49): loss=178.74031328744422\n",
      "Gradient Descent(3/49): loss=9228.481554409373\n",
      "Gradient Descent(4/49): loss=591446.1240807353\n",
      "Gradient Descent(5/49): loss=57944433.788496874\n",
      "Gradient Descent(6/49): loss=8651551340.963173\n",
      "Gradient Descent(7/49): loss=1582359417265.9336\n",
      "Gradient Descent(8/49): loss=308064839309089.44\n",
      "Gradient Descent(9/49): loss=6.095389938498249e+16\n",
      "Gradient Descent(10/49): loss=1.2108521834740791e+19\n",
      "Gradient Descent(11/49): loss=2.407696751570126e+21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=4.788666325450065e+23\n",
      "Gradient Descent(13/49): loss=9.524717925874145e+25\n",
      "Gradient Descent(14/49): loss=1.8945046231603802e+28\n",
      "Gradient Descent(15/49): loss=3.768258348547882e+30\n",
      "Gradient Descent(16/49): loss=7.495248272047926e+32\n",
      "Gradient Descent(17/49): loss=1.4908414580481094e+35\n",
      "Gradient Descent(18/49): loss=2.965356554284787e+37\n",
      "Gradient Descent(19/49): loss=5.898239244770464e+39\n",
      "Gradient Descent(20/49): loss=1.1731886418733519e+42\n",
      "Gradient Descent(21/49): loss=2.333529606440558e+44\n",
      "Gradient Descent(22/49): loss=4.641504554915029e+46\n",
      "Gradient Descent(23/49): loss=9.232179645244279e+48\n",
      "Gradient Descent(24/49): loss=1.8363256998773798e+51\n",
      "Gradient Descent(25/49): loss=3.6525416592982724e+53\n",
      "Gradient Descent(26/49): loss=7.265084060963773e+55\n",
      "Gradient Descent(27/49): loss=1.445060764153378e+58\n",
      "Gradient Descent(28/49): loss=2.8742965595068374e+60\n",
      "Gradient Descent(29/49): loss=5.71711648183405e+62\n",
      "Gradient Descent(30/49): loss=1.1371624392323099e+65\n",
      "Gradient Descent(31/49): loss=2.261871727311604e+67\n",
      "Gradient Descent(32/49): loss=4.498973527709517e+69\n",
      "Gradient Descent(33/49): loss=8.948678458918902e+71\n",
      "Gradient Descent(34/49): loss=1.779935926893265e+74\n",
      "Gradient Descent(35/49): loss=3.5403796419656463e+76\n",
      "Gradient Descent(36/49): loss=7.041988320963061e+78\n",
      "Gradient Descent(37/49): loss=1.4006859299712877e+81\n",
      "Gradient Descent(38/49): loss=2.7860328432797264e+83\n",
      "Gradient Descent(39/49): loss=5.541555631955457e+85\n",
      "Gradient Descent(40/49): loss=1.102242527259922e+88\n",
      "Gradient Descent(41/49): loss=2.1924143139416263e+90\n",
      "Gradient Descent(42/49): loss=4.36081933431224e+92\n",
      "Gradient Descent(43/49): loss=8.673883009056907e+94\n",
      "Gradient Descent(44/49): loss=1.725277767478812e+97\n",
      "Gradient Descent(45/49): loss=3.431661888739565e+99\n",
      "Gradient Descent(46/49): loss=6.825743390779825e+101\n",
      "Gradient Descent(47/49): loss=1.3576737553793656e+104\n",
      "Gradient Descent(48/49): loss=2.7004795236454643e+106\n",
      "Gradient Descent(49/49): loss=5.37138589350627e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.280319028062682\n",
      "Gradient Descent(2/49): loss=181.65403798026944\n",
      "Gradient Descent(3/49): loss=9272.060210513471\n",
      "Gradient Descent(4/49): loss=578765.6215433615\n",
      "Gradient Descent(5/49): loss=54600815.75569676\n",
      "Gradient Descent(6/49): loss=7957771825.220748\n",
      "Gradient Descent(7/49): loss=1443707186175.687\n",
      "Gradient Descent(8/49): loss=280530625128401.44\n",
      "Gradient Descent(9/49): loss=5.5491161648420024e+16\n",
      "Gradient Descent(10/49): loss=1.1024819938238853e+19\n",
      "Gradient Descent(11/49): loss=2.1927093850601646e+21\n",
      "Gradient Descent(12/49): loss=4.362167169111278e+23\n",
      "Gradient Descent(13/49): loss=8.67861665585311e+25\n",
      "Gradient Descent(14/49): loss=1.726653605741954e+28\n",
      "Gradient Descent(15/49): loss=3.435274987640897e+30\n",
      "Gradient Descent(16/49): loss=6.834679824308289e+32\n",
      "Gradient Descent(17/49): loss=1.3597996764632564e+35\n",
      "Gradient Descent(18/49): loss=2.7054014261924783e+37\n",
      "Gradient Descent(19/49): loss=5.382555308467941e+39\n",
      "Gradient Descent(20/49): loss=1.0708910498236036e+42\n",
      "Gradient Descent(21/49): loss=2.1306007568782593e+44\n",
      "Gradient Descent(22/49): loss=4.238955575121702e+46\n",
      "Gradient Descent(23/49): loss=8.433651546681821e+48\n",
      "Gradient Descent(24/49): loss=1.6779246007909724e+51\n",
      "Gradient Descent(25/49): loss=3.3383297262905982e+53\n",
      "Gradient Descent(26/49): loss=6.641803425606134e+55\n",
      "Gradient Descent(27/49): loss=1.3214258734536469e+58\n",
      "Gradient Descent(28/49): loss=2.62905453103425e+60\n",
      "Gradient Descent(29/49): loss=5.230658689228543e+62\n",
      "Gradient Descent(30/49): loss=1.0406703246447408e+65\n",
      "Gradient Descent(31/49): loss=2.0704748463638154e+67\n",
      "Gradient Descent(32/49): loss=4.119331538437711e+69\n",
      "Gradient Descent(33/49): loss=8.195652486852752e+71\n",
      "Gradient Descent(34/49): loss=1.630573287401133e+74\n",
      "Gradient Descent(35/49): loss=3.244121502041762e+76\n",
      "Gradient Descent(36/49): loss=6.454370620031396e+78\n",
      "Gradient Descent(37/49): loss=1.2841350138860585e+81\n",
      "Gradient Descent(38/49): loss=2.5548621716428765e+83\n",
      "Gradient Descent(39/49): loss=5.083048624566797e+85\n",
      "Gradient Descent(40/49): loss=1.011302433708067e+88\n",
      "Gradient Descent(41/49): loss=2.0120456992696954e+90\n",
      "Gradient Descent(42/49): loss=4.003083312185778e+92\n",
      "Gradient Descent(43/49): loss=7.964369800406231e+94\n",
      "Gradient Descent(44/49): loss=1.584558236011e+97\n",
      "Gradient Descent(45/49): loss=3.1525718496675915e+99\n",
      "Gradient Descent(46/49): loss=6.272227199636639e+101\n",
      "Gradient Descent(47/49): loss=1.247896508623893e+104\n",
      "Gradient Descent(48/49): loss=2.4827635330651358e+106\n",
      "Gradient Descent(49/49): loss=4.939604140663464e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.126990354610378\n",
      "Gradient Descent(2/49): loss=172.40304944939865\n",
      "Gradient Descent(3/49): loss=8569.63410659592\n",
      "Gradient Descent(4/49): loss=498236.73051109974\n",
      "Gradient Descent(5/49): loss=41815858.37951009\n",
      "Gradient Descent(6/49): loss=5626718142.360946\n",
      "Gradient Descent(7/49): loss=1000687854234.188\n",
      "Gradient Descent(8/49): loss=195469136209103.12\n",
      "Gradient Descent(9/49): loss=3.91337464463478e+16\n",
      "Gradient Descent(10/49): loss=7.881878571343418e+18\n",
      "Gradient Descent(11/49): loss=1.5897569164819384e+21\n",
      "Gradient Descent(12/49): loss=3.207597424298629e+23\n",
      "Gradient Descent(13/49): loss=6.472382860156536e+25\n",
      "Gradient Descent(14/49): loss=1.3060412932363115e+28\n",
      "Gradient Descent(15/49): loss=2.6354307268515916e+30\n",
      "Gradient Descent(16/49): loss=5.317980912612346e+32\n",
      "Gradient Descent(17/49): loss=1.0731045974798661e+35\n",
      "Gradient Descent(18/49): loss=2.165396165833237e+37\n",
      "Gradient Descent(19/49): loss=4.369509397681737e+39\n",
      "Gradient Descent(20/49): loss=8.817145215195195e+41\n",
      "Gradient Descent(21/49): loss=1.7791940177787857e+44\n",
      "Gradient Descent(22/49): loss=3.5901998620409334e+46\n",
      "Gradient Descent(23/49): loss=7.244592169936216e+48\n",
      "Gradient Descent(24/49): loss=1.461871698681589e+51\n",
      "Gradient Descent(25/49): loss=2.9498815299503747e+53\n",
      "Gradient Descent(26/49): loss=5.952506672504383e+55\n",
      "Gradient Descent(27/49): loss=1.2011443621198746e+58\n",
      "Gradient Descent(28/49): loss=2.4237650758406174e+60\n",
      "Gradient Descent(29/49): loss=4.890866850090133e+62\n",
      "Gradient Descent(30/49): loss=9.869181953211494e+64\n",
      "Gradient Descent(31/49): loss=1.991482397927111e+67\n",
      "Gradient Descent(32/49): loss=4.01857231942407e+69\n",
      "Gradient Descent(33/49): loss=8.10899634526014e+71\n",
      "Gradient Descent(34/49): loss=1.636298080529956e+74\n",
      "Gradient Descent(35/49): loss=3.3018530214420125e+76\n",
      "Gradient Descent(36/49): loss=6.662742873642541e+78\n",
      "Gradient Descent(37/49): loss=1.3444614981949227e+81\n",
      "Gradient Descent(38/49): loss=2.7129618453073276e+83\n",
      "Gradient Descent(39/49): loss=5.474431200874883e+85\n",
      "Gradient Descent(40/49): loss=1.1046744732127749e+88\n",
      "Gradient Descent(41/49): loss=2.2291004252148524e+90\n",
      "Gradient Descent(42/49): loss=4.498056962646822e+92\n",
      "Gradient Descent(43/49): loss=9.076538773377724e+94\n",
      "Gradient Descent(44/49): loss=1.831536523186981e+97\n",
      "Gradient Descent(45/49): loss=3.6958207522969704e+99\n",
      "Gradient Descent(46/49): loss=7.457722442434196e+101\n",
      "Gradient Descent(47/49): loss=1.504878828168784e+104\n",
      "Gradient Descent(48/49): loss=3.0366647524783076e+106\n",
      "Gradient Descent(49/49): loss=6.127624793662244e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.200080726384914\n",
      "Gradient Descent(2/49): loss=178.54117675033044\n",
      "Gradient Descent(3/49): loss=9319.340301112006\n",
      "Gradient Descent(4/49): loss=626087.466052755\n",
      "Gradient Descent(5/49): loss=67046166.02559335\n",
      "Gradient Descent(6/49): loss=10838849638.578508\n",
      "Gradient Descent(7/49): loss=2090614234545.5615\n",
      "Gradient Descent(8/49): loss=424000673918751.3\n",
      "Gradient Descent(9/49): loss=8.706017441821806e+16\n",
      "Gradient Descent(10/49): loss=1.7928345712665647e+19\n",
      "Gradient Descent(11/49): loss=3.6945190228939196e+21\n",
      "Gradient Descent(12/49): loss=7.614563738349298e+23\n",
      "Gradient Descent(13/49): loss=1.569453192924384e+26\n",
      "Gradient Descent(14/49): loss=3.2348600714595127e+28\n",
      "Gradient Descent(15/49): loss=6.667507551111953e+30\n",
      "Gradient Descent(16/49): loss=1.3742689678766746e+33\n",
      "Gradient Descent(17/49): loss=2.832565830209175e+35\n",
      "Gradient Descent(18/49): loss=5.838325377750672e+37\n",
      "Gradient Descent(19/49): loss=1.2033627977831918e+40\n",
      "Gradient Descent(20/49): loss=2.480303736857822e+42\n",
      "Gradient Descent(21/49): loss=5.112262603133852e+44\n",
      "Gradient Descent(22/49): loss=1.053710823277368e+47\n",
      "Gradient Descent(23/49): loss=2.1718495024714557e+49\n",
      "Gradient Descent(24/49): loss=4.47649407902475e+51\n",
      "Gradient Descent(25/49): loss=9.226697898155343e+53\n",
      "Gradient Descent(26/49): loss=1.9017550922884867e+56\n",
      "Gradient Descent(27/49): loss=3.9197906672206645e+58\n",
      "Gradient Descent(28/49): loss=8.079252127223698e+60\n",
      "Gradient Descent(29/49): loss=1.665250021668415e+63\n",
      "Gradient Descent(30/49): loss=3.4323197135074897e+65\n",
      "Gradient Descent(31/49): loss=7.074504406208408e+67\n",
      "Gradient Descent(32/49): loss=1.458157070756028e+70\n",
      "Gradient Descent(33/49): loss=3.0054713671955564e+72\n",
      "Gradient Descent(34/49): loss=6.194708594972363e+74\n",
      "Gradient Descent(35/49): loss=1.2768185049266686e+77\n",
      "Gradient Descent(36/49): loss=2.6317065113382445e+79\n",
      "Gradient Descent(37/49): loss=5.42432548956351e+81\n",
      "Gradient Descent(38/49): loss=1.1180314708332174e+84\n",
      "Gradient Descent(39/49): loss=2.3044236047015766e+86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(40/49): loss=4.7497483643714836e+88\n",
      "Gradient Descent(41/49): loss=9.789914266986709e+90\n",
      "Gradient Descent(42/49): loss=2.0178420834644453e+93\n",
      "Gradient Descent(43/49): loss=4.1590626462691234e+95\n",
      "Gradient Descent(44/49): loss=8.572426077016071e+97\n",
      "Gradient Descent(45/49): loss=1.7669002632558187e+100\n",
      "Gradient Descent(46/49): loss=3.641835476031666e+102\n",
      "Gradient Descent(47/49): loss=7.506346515588235e+104\n",
      "Gradient Descent(48/49): loss=1.5471659382450195e+107\n",
      "Gradient Descent(49/49): loss=3.1889314402080277e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.235214846359663\n",
      "Gradient Descent(2/49): loss=182.0973533238764\n",
      "Gradient Descent(3/49): loss=9488.403857093404\n",
      "Gradient Descent(4/49): loss=613425.5423088028\n",
      "Gradient Descent(5/49): loss=60586928.66563788\n",
      "Gradient Descent(6/49): loss=9119876162.295792\n",
      "Gradient Descent(7/49): loss=1682187330567.2703\n",
      "Gradient Descent(8/49): loss=330341693587488.75\n",
      "Gradient Descent(9/49): loss=6.593267653057516e+16\n",
      "Gradient Descent(10/49): loss=1.3212221161463316e+19\n",
      "Gradient Descent(11/49): loss=2.650172110860192e+21\n",
      "Gradient Descent(12/49): loss=5.317102414122113e+23\n",
      "Gradient Descent(13/49): loss=1.0668439072724867e+26\n",
      "Gradient Descent(14/49): loss=2.1405864786356215e+28\n",
      "Gradient Descent(15/49): loss=4.295029381494866e+30\n",
      "Gradient Descent(16/49): loss=8.617868355003997e+32\n",
      "Gradient Descent(17/49): loss=1.7291539385774712e+35\n",
      "Gradient Descent(18/49): loss=3.469504712673249e+37\n",
      "Gradient Descent(19/49): loss=6.961475680724155e+39\n",
      "Gradient Descent(20/49): loss=1.3968029388555732e+42\n",
      "Gradient Descent(21/49): loss=2.8026506746776744e+44\n",
      "Gradient Descent(22/49): loss=5.623449512508142e+46\n",
      "Gradient Descent(23/49): loss=1.1283312867609061e+49\n",
      "Gradient Descent(24/49): loss=2.2639689213251514e+51\n",
      "Gradient Descent(25/49): loss=4.542597849477515e+53\n",
      "Gradient Descent(26/49): loss=9.114610641391346e+55\n",
      "Gradient Descent(27/49): loss=1.8288241639909385e+58\n",
      "Gradient Descent(28/49): loss=3.669490617195112e+60\n",
      "Gradient Descent(29/49): loss=7.362742495866349e+62\n",
      "Gradient Descent(30/49): loss=1.4773161377333498e+65\n",
      "Gradient Descent(31/49): loss=2.964198424748296e+67\n",
      "Gradient Descent(32/49): loss=5.9475910922907966e+69\n",
      "Gradient Descent(33/49): loss=1.1933694959742591e+72\n",
      "Gradient Descent(34/49): loss=2.3944664853772497e+74\n",
      "Gradient Descent(35/49): loss=4.804437995890168e+76\n",
      "Gradient Descent(36/49): loss=9.63998644262347e+78\n",
      "Gradient Descent(37/49): loss=1.9342395238206736e+81\n",
      "Gradient Descent(38/49): loss=3.881003938934849e+83\n",
      "Gradient Descent(39/49): loss=7.787138763598281e+85\n",
      "Gradient Descent(40/49): loss=1.5624702030108273e+88\n",
      "Gradient Descent(41/49): loss=3.1350579582694942e+90\n",
      "Gradient Descent(42/49): loss=6.290416535796631e+92\n",
      "Gradient Descent(43/49): loss=1.2621565763863619e+95\n",
      "Gradient Descent(44/49): loss=2.5324860670989117e+97\n",
      "Gradient Descent(45/49): loss=5.08137088538772e+99\n",
      "Gradient Descent(46/49): loss=1.0195645460922183e+102\n",
      "Gradient Descent(47/49): loss=2.045731136527593e+104\n",
      "Gradient Descent(48/49): loss=4.104709112334906e+106\n",
      "Gradient Descent(49/49): loss=8.235997681241695e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.317332139902753\n",
      "Gradient Descent(2/49): loss=185.0672465331611\n",
      "Gradient Descent(3/49): loss=9533.433014320233\n",
      "Gradient Descent(4/49): loss=600310.4780934841\n",
      "Gradient Descent(5/49): loss=57094997.58413453\n",
      "Gradient Descent(6/49): loss=8388848109.828042\n",
      "Gradient Descent(7/49): loss=1534805455945.4692\n",
      "Gradient Descent(8/49): loss=300817249826338.9\n",
      "Gradient Descent(9/49): loss=6.0023750233658824e+16\n",
      "Gradient Descent(10/49): loss=1.202973305058515e+19\n",
      "Gradient Descent(11/49): loss=2.41353188592769e+21\n",
      "Gradient Descent(12/49): loss=4.843534253549213e+23\n",
      "Gradient Descent(13/49): loss=9.720729285692684e+25\n",
      "Gradient Descent(14/49): loss=1.9509308651640956e+28\n",
      "Gradient Descent(15/49): loss=3.915493391984639e+30\n",
      "Gradient Descent(16/49): loss=7.858352283655809e+32\n",
      "Gradient Descent(17/49): loss=1.577163018493379e+35\n",
      "Gradient Descent(18/49): loss=3.165349713479516e+37\n",
      "Gradient Descent(19/49): loss=6.352823909618952e+39\n",
      "Gradient Descent(20/49): loss=1.2750051463843259e+42\n",
      "Gradient Descent(21/49): loss=2.558922059882153e+44\n",
      "Gradient Descent(22/49): loss=5.135729944518042e+46\n",
      "Gradient Descent(23/49): loss=1.0307356553623317e+49\n",
      "Gradient Descent(24/49): loss=2.0686757339726775e+51\n",
      "Gradient Descent(25/49): loss=4.151810670441642e+53\n",
      "Gradient Descent(26/49): loss=8.332640809828635e+55\n",
      "Gradient Descent(27/49): loss=1.6723523391842815e+58\n",
      "Gradient Descent(28/49): loss=3.356393741437011e+60\n",
      "Gradient Descent(29/49): loss=6.736247310810628e+62\n",
      "Gradient Descent(30/49): loss=1.3519578252155608e+65\n",
      "Gradient Descent(31/49): loss=2.713365286081914e+67\n",
      "Gradient Descent(32/49): loss=5.4456958925774075e+69\n",
      "Gradient Descent(33/49): loss=1.0929454985862687e+72\n",
      "Gradient Descent(34/49): loss=2.1935302419441503e+74\n",
      "Gradient Descent(35/49): loss=4.402392368647351e+76\n",
      "Gradient Descent(36/49): loss=8.835555670454946e+78\n",
      "Gradient Descent(37/49): loss=1.7732868283545309e+81\n",
      "Gradient Descent(38/49): loss=3.558968210828833e+83\n",
      "Gradient Descent(39/49): loss=7.142812162792268e+85\n",
      "Gradient Descent(40/49): loss=1.4335549679172828e+88\n",
      "Gradient Descent(41/49): loss=2.877129902345018e+90\n",
      "Gradient Descent(42/49): loss=5.774369773203832e+92\n",
      "Gradient Descent(43/49): loss=1.1589100043940753e+95\n",
      "Gradient Descent(44/49): loss=2.325920318641987e+97\n",
      "Gradient Descent(45/49): loss=4.668097874864798e+99\n",
      "Gradient Descent(46/49): loss=9.368823856373888e+101\n",
      "Gradient Descent(47/49): loss=1.8803131983238976e+104\n",
      "Gradient Descent(48/49): loss=3.773769021589217e+106\n",
      "Gradient Descent(49/49): loss=7.573915154667411e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.1625559516603765\n",
      "Gradient Descent(2/49): loss=175.64469543889203\n",
      "Gradient Descent(3/49): loss=8811.699594989695\n",
      "Gradient Descent(4/49): loss=516874.10309791414\n",
      "Gradient Descent(5/49): loss=43736659.14502062\n",
      "Gradient Descent(6/49): loss=5932243452.980183\n",
      "Gradient Descent(7/49): loss=1063852684251.5104\n",
      "Gradient Descent(8/49): loss=209600504389740.06\n",
      "Gradient Descent(9/49): loss=4.23288402841348e+16\n",
      "Gradient Descent(10/49): loss=8.599970835071734e+18\n",
      "Gradient Descent(11/49): loss=1.7497797907841763e+21\n",
      "Gradient Descent(12/49): loss=3.561383985316971e+23\n",
      "Gradient Descent(13/49): loss=7.249192578946965e+25\n",
      "Gradient Descent(14/49): loss=1.4756008490748667e+28\n",
      "Gradient Descent(15/49): loss=3.0036557618836445e+30\n",
      "Gradient Descent(16/49): loss=6.1140909692608955e+32\n",
      "Gradient Descent(17/49): loss=1.2445540063636848e+35\n",
      "Gradient Descent(18/49): loss=2.5333525106164736e+37\n",
      "Gradient Descent(19/49): loss=5.1567670062919283e+39\n",
      "Gradient Descent(20/49): loss=1.0496859769354351e+42\n",
      "Gradient Descent(21/49): loss=2.136688839630728e+44\n",
      "Gradient Descent(22/49): loss=4.349338086457601e+46\n",
      "Gradient Descent(23/49): loss=8.853297419979154e+48\n",
      "Gradient Descent(24/49): loss=1.8021334200608242e+51\n",
      "Gradient Descent(25/49): loss=3.668333627174523e+53\n",
      "Gradient Descent(26/49): loss=7.467078436297882e+55\n",
      "Gradient Descent(27/49): loss=1.519961542232414e+58\n",
      "Gradient Descent(28/49): loss=3.093958513459873e+60\n",
      "Gradient Descent(29/49): loss=6.297908872714859e+62\n",
      "Gradient Descent(30/49): loss=1.2819711704752631e+65\n",
      "Gradient Descent(31/49): loss=2.60951708756824e+67\n",
      "Gradient Descent(32/49): loss=5.311803874486728e+69\n",
      "Gradient Descent(33/49): loss=1.0812445159079185e+72\n",
      "Gradient Descent(34/49): loss=2.2009278407213567e+74\n",
      "Gradient Descent(35/49): loss=4.48009981904494e+76\n",
      "Gradient Descent(36/49): loss=9.11946953337124e+78\n",
      "Gradient Descent(37/49): loss=1.8563140985509073e+81\n",
      "Gradient Descent(38/49): loss=3.7786211356583664e+83\n",
      "Gradient Descent(39/49): loss=7.691574231963033e+85\n",
      "Gradient Descent(40/49): loss=1.5656587956783871e+88\n",
      "Gradient Descent(41/49): loss=3.1869775816485083e+90\n",
      "Gradient Descent(42/49): loss=6.487253885690378e+92\n",
      "Gradient Descent(43/49): loss=1.3205133045095527e+95\n",
      "Gradient Descent(44/49): loss=2.687971548690479e+97\n",
      "Gradient Descent(45/49): loss=5.471501893919094e+99\n",
      "Gradient Descent(46/49): loss=1.1137518546186077e+102\n",
      "Gradient Descent(47/49): loss=2.2670981710615157e+104\n",
      "Gradient Descent(48/49): loss=4.61479286962957e+106\n",
      "Gradient Descent(49/49): loss=9.393644043042035e+108\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.2361675951785305\n",
      "Gradient Descent(2/49): loss=181.88073367753822\n",
      "Gradient Descent(3/49): loss=9580.484938003665\n",
      "Gradient Descent(4/49): loss=649183.9048093475\n",
      "Gradient Descent(5/49): loss=70078678.03858645\n",
      "Gradient Descent(6/49): loss=11421590666.970995\n",
      "Gradient Descent(7/49): loss=2221736193523.8003\n",
      "Gradient Descent(8/49): loss=454491497800371.75\n",
      "Gradient Descent(9/49): loss=9.413251885778034e+16\n",
      "Gradient Descent(10/49): loss=1.9553609156999238e+19\n",
      "Gradient Descent(11/49): loss=4.064552037125678e+21\n",
      "Gradient Descent(12/49): loss=8.450224948629597e+23\n",
      "Gradient Descent(13/49): loss=1.7568722233717457e+26\n",
      "Gradient Descent(14/49): loss=3.6527159323139136e+28\n",
      "Gradient Descent(15/49): loss=7.594383307335992e+30\n",
      "Gradient Descent(16/49): loss=1.5789534849624722e+33\n",
      "Gradient Descent(17/49): loss=3.282813477510174e+35\n",
      "Gradient Descent(18/49): loss=6.825321146870378e+37\n",
      "Gradient Descent(19/49): loss=1.419057444558749e+40\n",
      "Gradient Descent(20/49): loss=2.950372576633259e+42\n",
      "Gradient Descent(21/49): loss=6.134140923782742e+44\n",
      "Gradient Descent(22/49): loss=1.2753536680004657e+47\n",
      "Gradient Descent(23/49): loss=2.6515970185911656e+49\n",
      "Gradient Descent(24/49): loss=5.512954504655191e+51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=1.1462023511618149e+54\n",
      "Gradient Descent(26/49): loss=2.3830775833535404e+56\n",
      "Gradient Descent(27/49): loss=4.9546738082731413e+58\n",
      "Gradient Descent(28/49): loss=1.0301298085244003e+61\n",
      "Gradient Descent(29/49): loss=2.141750321965944e+63\n",
      "Gradient Descent(30/49): loss=4.452928556850452e+65\n",
      "Gradient Descent(31/49): loss=9.258115910641569e+67\n",
      "Gradient Descent(32/49): loss=1.9248615629148923e+70\n",
      "Gradient Descent(33/49): loss=4.0019935720705496e+72\n",
      "Gradient Descent(34/49): loss=8.320573728242483e+74\n",
      "Gradient Descent(35/49): loss=1.7299364909099756e+77\n",
      "Gradient Descent(36/49): loss=3.596723447595793e+79\n",
      "Gradient Descent(37/49): loss=7.477973686583471e+81\n",
      "Gradient Descent(38/49): loss=1.5547509079302592e+84\n",
      "Gradient Descent(39/49): loss=3.232493837263391e+86\n",
      "Gradient Descent(40/49): loss=6.72070127417144e+88\n",
      "Gradient Descent(41/49): loss=1.397305853949808e+91\n",
      "Gradient Descent(42/49): loss=2.9051486888519017e+93\n",
      "Gradient Descent(43/49): loss=6.040115612827875e+95\n",
      "Gradient Descent(44/49): loss=1.2558047977483881e+98\n",
      "Gradient Descent(45/49): loss=2.6109528213310716e+100\n",
      "Gradient Descent(46/49): loss=5.428450860706543e+102\n",
      "Gradient Descent(47/49): loss=1.1286331375410352e+105\n",
      "Gradient Descent(48/49): loss=2.346549304473126e+107\n",
      "Gradient Descent(49/49): loss=4.878727599935466e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.271617835717193\n",
      "Gradient Descent(2/49): loss=185.50157706829071\n",
      "Gradient Descent(3/49): loss=9754.401480585828\n",
      "Gradient Descent(4/49): loss=636117.0688598818\n",
      "Gradient Descent(5/49): loss=63337500.70279027\n",
      "Gradient Descent(6/49): loss=9611346358.87781\n",
      "Gradient Descent(7/49): loss=1787838021335.3757\n",
      "Gradient Descent(8/49): loss=354122055817246.06\n",
      "Gradient Descent(9/49): loss=7.129381299470312e+16\n",
      "Gradient Descent(10/49): loss=1.4411063247977038e+19\n",
      "Gradient Descent(11/49): loss=2.915851532403525e+21\n",
      "Gradient Descent(12/49): loss=5.9011690897053185e+23\n",
      "Gradient Descent(13/49): loss=1.1943614378762657e+26\n",
      "Gradient Descent(14/49): loss=2.4173501847624387e+28\n",
      "Gradient Descent(15/49): loss=4.8926577699712335e+30\n",
      "Gradient Descent(16/49): loss=9.902628066824221e+32\n",
      "Gradient Descent(17/49): loss=2.0042698029046997e+35\n",
      "Gradient Descent(18/49): loss=4.056597511962248e+37\n",
      "Gradient Descent(19/49): loss=8.210463253982929e+39\n",
      "Gradient Descent(20/49): loss=1.6617795292433903e+42\n",
      "Gradient Descent(21/49): loss=3.3634048655986294e+44\n",
      "Gradient Descent(22/49): loss=6.80745676111788e+46\n",
      "Gradient Descent(23/49): loss=1.377814132053086e+49\n",
      "Gradient Descent(24/49): loss=2.7886652080509503e+51\n",
      "Gradient Descent(25/49): loss=5.644196457053602e+53\n",
      "Gradient Descent(26/49): loss=1.1423728296198314e+56\n",
      "Gradient Descent(27/49): loss=2.3121372400546107e+58\n",
      "Gradient Descent(28/49): loss=4.6797144314317966e+60\n",
      "Gradient Descent(29/49): loss=9.471638093262093e+62\n",
      "Gradient Descent(30/49): loss=1.917038517717545e+65\n",
      "Gradient Descent(31/49): loss=3.8800433908333314e+67\n",
      "Gradient Descent(32/49): loss=7.853121664281238e+69\n",
      "Gradient Descent(33/49): loss=1.5894543865077324e+72\n",
      "Gradient Descent(34/49): loss=3.217020383473155e+74\n",
      "Gradient Descent(35/49): loss=6.511177820220725e+76\n",
      "Gradient Descent(36/49): loss=1.3178479323392812e+79\n",
      "Gradient Descent(37/49): loss=2.667294951425626e+81\n",
      "Gradient Descent(38/49): loss=5.398545752750001e+83\n",
      "Gradient Descent(39/49): loss=1.0926536725515764e+86\n",
      "Gradient Descent(40/49): loss=2.211506770193222e+88\n",
      "Gradient Descent(41/49): loss=4.476040594994318e+90\n",
      "Gradient Descent(42/49): loss=9.059406771016373e+92\n",
      "Gradient Descent(43/49): loss=1.8336038134802016e+95\n",
      "Gradient Descent(44/49): loss=3.711173402176214e+97\n",
      "Gradient Descent(45/49): loss=7.511332557102005e+99\n",
      "Gradient Descent(46/49): loss=1.5202770301785146e+102\n",
      "Gradient Descent(47/49): loss=3.077006950388824e+104\n",
      "Gradient Descent(48/49): loss=6.227793740742797e+106\n",
      "Gradient Descent(49/49): loss=1.260491623924843e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.3545207252245826\n",
      "Gradient Descent(2/49): loss=188.52844594241438\n",
      "Gradient Descent(3/49): loss=9800.92003827528\n",
      "Gradient Descent(4/49): loss=622554.5247500321\n",
      "Gradient Descent(5/49): loss=59691364.53856259\n",
      "Gradient Descent(6/49): loss=8841244853.530514\n",
      "Gradient Descent(7/49): loss=1631218417252.4204\n",
      "Gradient Descent(8/49): loss=322473118722131.25\n",
      "Gradient Descent(9/49): loss=6.490443314909581e+16\n",
      "Gradient Descent(10/49): loss=1.312127269719074e+19\n",
      "Gradient Descent(11/49): loss=2.65548617236474e+21\n",
      "Gradient Descent(12/49): loss=5.3755763402816274e+23\n",
      "Gradient Descent(13/49): loss=1.0882616585864818e+26\n",
      "Gradient Descent(14/49): loss=2.203171095737216e+28\n",
      "Gradient Descent(15/49): loss=4.4603066547793324e+30\n",
      "Gradient Descent(16/49): loss=9.029872065902852e+32\n",
      "Gradient Descent(17/49): loss=1.828094195229689e+35\n",
      "Gradient Descent(18/49): loss=3.7009699980719726e+37\n",
      "Gradient Descent(19/49): loss=7.492600290728441e+39\n",
      "Gradient Descent(20/49): loss=1.5168742063928286e+42\n",
      "Gradient Descent(21/49): loss=3.0709063214760026e+44\n",
      "Gradient Descent(22/49): loss=6.217038695239783e+46\n",
      "Gradient Descent(23/49): loss=1.2586372260678427e+49\n",
      "Gradient Descent(24/49): loss=2.548106493326396e+51\n",
      "Gradient Descent(25/49): loss=5.158632342087175e+53\n",
      "Gradient Descent(26/49): loss=1.0443632442576285e+56\n",
      "Gradient Descent(27/49): loss=2.1143095953126572e+58\n",
      "Gradient Descent(28/49): loss=4.280412097429659e+60\n",
      "Gradient Descent(29/49): loss=8.665678746595015e+62\n",
      "Gradient Descent(30/49): loss=1.7543635152391453e+65\n",
      "Gradient Descent(31/49): loss=3.551702565493211e+67\n",
      "Gradient Descent(32/49): loss=7.190408945555251e+69\n",
      "Gradient Descent(33/49): loss=1.4556956797744e+72\n",
      "Gradient Descent(34/49): loss=2.9470506172304895e+74\n",
      "Gradient Descent(35/49): loss=5.966293272138159e+76\n",
      "Gradient Descent(36/49): loss=1.2078739062382773e+79\n",
      "Gradient Descent(37/49): loss=2.4453363366908543e+81\n",
      "Gradient Descent(38/49): loss=4.950574533200407e+83\n",
      "Gradient Descent(39/49): loss=1.0022420164065673e+86\n",
      "Gradient Descent(40/49): loss=2.029035322494789e+88\n",
      "Gradient Descent(41/49): loss=4.107774641790135e+90\n",
      "Gradient Descent(42/49): loss=8.31617484459893e+92\n",
      "Gradient Descent(43/49): loss=1.683606577205066e+95\n",
      "Gradient Descent(44/49): loss=3.4084554014026496e+97\n",
      "Gradient Descent(45/49): loss=6.900405582067213e+99\n",
      "Gradient Descent(46/49): loss=1.3969846041532069e+102\n",
      "Gradient Descent(47/49): loss=2.8281902578492582e+104\n",
      "Gradient Descent(48/49): loss=5.725660906221649e+106\n",
      "Gradient Descent(49/49): loss=1.159157971146013e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.198290341838723\n",
      "Gradient Descent(2/49): loss=178.93195669190544\n",
      "Gradient Descent(3/49): loss=9059.438660714226\n",
      "Gradient Descent(4/49): loss=536119.155933709\n",
      "Gradient Descent(5/49): loss=45736597.4725067\n",
      "Gradient Descent(6/49): loss=6252917560.023298\n",
      "Gradient Descent(7/49): loss=1130703994777.202\n",
      "Gradient Descent(8/49): loss=224685411644171.7\n",
      "Gradient Descent(9/49): loss=4.576920515726142e+16\n",
      "Gradient Descent(10/49): loss=9.379935565312264e+18\n",
      "Gradient Descent(11/49): loss=1.925108782948612e+21\n",
      "Gradient Descent(12/49): loss=3.9523966069842196e+23\n",
      "Gradient Descent(13/49): loss=8.115240793025913e+25\n",
      "Gradient Descent(14/49): loss=1.6662907532040093e+28\n",
      "Gradient Descent(15/49): loss=3.421386798655647e+30\n",
      "Gradient Descent(16/49): loss=7.025124839701312e+32\n",
      "Gradient Descent(17/49): loss=1.4424674814419621e+35\n",
      "Gradient Descent(18/49): loss=2.9618157863136377e+37\n",
      "Gradient Descent(19/49): loss=6.081490914268571e+39\n",
      "Gradient Descent(20/49): loss=1.2487114168973635e+42\n",
      "Gradient Descent(21/49): loss=2.5639768701182925e+44\n",
      "Gradient Descent(22/49): loss=5.264609022436544e+46\n",
      "Gradient Descent(23/49): loss=1.0809812086628345e+49\n",
      "Gradient Descent(24/49): loss=2.2195767406761344e+51\n",
      "Gradient Descent(25/49): loss=4.557452866232375e+53\n",
      "Gradient Descent(26/49): loss=9.357809643299737e+55\n",
      "Gradient Descent(27/49): loss=1.9214373442909293e+58\n",
      "Gradient Descent(28/49): loss=3.945283788369654e+60\n",
      "Gradient Descent(29/49): loss=8.100843994221698e+62\n",
      "Gradient Descent(30/49): loss=1.6633448172263218e+65\n",
      "Gradient Descent(31/49): loss=3.4153428741093746e+67\n",
      "Gradient Descent(32/49): loss=7.012717283227463e+69\n",
      "Gradient Descent(33/49): loss=1.4399199584698157e+72\n",
      "Gradient Descent(34/49): loss=2.9565850198447444e+74\n",
      "Gradient Descent(35/49): loss=6.070750619263417e+76\n",
      "Gradient Descent(36/49): loss=1.2465061154650143e+79\n",
      "Gradient Descent(37/49): loss=2.559448729389882e+81\n",
      "Gradient Descent(38/49): loss=5.25531139968117e+83\n",
      "Gradient Descent(39/49): loss=1.0790721294972789e+86\n",
      "Gradient Descent(40/49): loss=2.215656831921406e+88\n",
      "Gradient Descent(41/49): loss=4.549404124752071e+90\n",
      "Gradient Descent(42/49): loss=9.341283177126064e+92\n",
      "Gradient Descent(43/49): loss=1.9180439680111066e+95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/49): loss=3.938316175054348e+97\n",
      "Gradient Descent(45/49): loss=8.086537406531927e+99\n",
      "Gradient Descent(46/49): loss=1.660407248189947e+102\n",
      "Gradient Descent(47/49): loss=3.4093111689742264e+104\n",
      "Gradient Descent(48/49): loss=7.000332394094045e+106\n",
      "Gradient Descent(49/49): loss=1.437376971446868e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.2724248760387935\n",
      "Gradient Descent(2/49): loss=185.2670383271992\n",
      "Gradient Descent(3/49): loss=9847.702967417976\n",
      "Gradient Descent(4/49): loss=673023.2650869306\n",
      "Gradient Descent(5/49): loss=73234201.5645479\n",
      "Gradient Descent(6/49): loss=12032934848.034142\n",
      "Gradient Descent(7/49): loss=2360461537797.4897\n",
      "Gradient Descent(8/49): loss=487028855008897.2\n",
      "Gradient Descent(9/49): loss=1.017450508772112e+17\n",
      "Gradient Descent(10/49): loss=2.131821488982271e+19\n",
      "Gradient Descent(11/49): loss=4.469803011287116e+21\n",
      "Gradient Descent(12/49): loss=9.373378378826424e+23\n",
      "Gradient Descent(13/49): loss=1.96571403950114e+26\n",
      "Gradient Descent(14/49): loss=4.122383320708653e+28\n",
      "Gradient Descent(15/49): loss=8.645244966993874e+30\n",
      "Gradient Descent(16/49): loss=1.813036073955154e+33\n",
      "Gradient Descent(17/49): loss=3.8022059410704226e+35\n",
      "Gradient Descent(18/49): loss=7.973790819026646e+37\n",
      "Gradient Descent(19/49): loss=1.672222425738033e+40\n",
      "Gradient Descent(20/49): loss=3.5068989250620896e+42\n",
      "Gradient Descent(21/49): loss=7.354488186187166e+44\n",
      "Gradient Descent(22/49): loss=1.54234546363767e+47\n",
      "Gradient Descent(23/49): loss=3.2345276367641184e+49\n",
      "Gradient Descent(24/49): loss=6.783285119768879e+51\n",
      "Gradient Descent(25/49): loss=1.4225556923098802e+54\n",
      "Gradient Descent(26/49): loss=2.9833106850043117e+56\n",
      "Gradient Descent(27/49): loss=6.256445839958354e+58\n",
      "Gradient Descent(28/49): loss=1.3120696662632966e+61\n",
      "Gradient Descent(29/49): loss=2.7516050696599666e+63\n",
      "Gradient Descent(30/49): loss=5.770524732075575e+65\n",
      "Gradient Descent(31/49): loss=1.210164788932136e+68\n",
      "Gradient Descent(32/49): loss=2.537895398369421e+70\n",
      "Gradient Descent(33/49): loss=5.322343793152537e+72\n",
      "Gradient Descent(34/49): loss=1.1161745858678284e+75\n",
      "Gradient Descent(35/49): loss=2.340783975172876e+77\n",
      "Gradient Descent(36/49): loss=4.9089718470574084e+79\n",
      "Gradient Descent(37/49): loss=1.0294843458770035e+82\n",
      "Gradient Descent(38/49): loss=2.158981659349094e+84\n",
      "Gradient Descent(39/49): loss=4.5277053741259657e+86\n",
      "Gradient Descent(40/49): loss=9.495271007105073e+88\n",
      "Gradient Descent(41/49): loss=1.9912994342254964e+91\n",
      "Gradient Descent(42/49): loss=4.1760508297022735e+93\n",
      "Gradient Descent(43/49): loss=8.757799169987533e+95\n",
      "Gradient Descent(44/49): loss=1.8366406308158767e+98\n",
      "Gradient Descent(45/49): loss=3.851708335952288e+100\n",
      "Gradient Descent(46/49): loss=8.077604761827695e+102\n",
      "Gradient Descent(47/49): loss=1.6939937554272353e+105\n",
      "Gradient Descent(48/49): loss=3.552556640289362e+107\n",
      "Gradient Descent(49/49): loss=7.4502392007231e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.308192640371638\n",
      "Gradient Descent(2/49): loss=188.9534247836616\n",
      "Gradient Descent(3/49): loss=10026.587551204133\n",
      "Gradient Descent(4/49): loss=659540.4072433385\n",
      "Gradient Descent(5/49): loss=66200065.4055454\n",
      "Gradient Descent(6/49): loss=10126997686.56078\n",
      "Gradient Descent(7/49): loss=1899623647147.4243\n",
      "Gradient Descent(8/49): loss=379500217525538.56\n",
      "Gradient Descent(9/49): loss=7.70648226672403e+16\n",
      "Gradient Descent(10/49): loss=1.5712783268692374e+19\n",
      "Gradient Descent(11/49): loss=3.206840315779841e+21\n",
      "Gradient Descent(12/49): loss=6.546442896930307e+23\n",
      "Gradient Descent(13/49): loss=1.3364682545469708e+26\n",
      "Gradient Descent(14/49): loss=2.7284627706238113e+28\n",
      "Gradient Descent(15/49): loss=5.570304076422588e+30\n",
      "Gradient Descent(16/49): loss=1.1372086002299288e+33\n",
      "Gradient Descent(17/49): loss=2.321675160870046e+35\n",
      "Gradient Descent(18/49): loss=4.73983033142756e+37\n",
      "Gradient Descent(19/49): loss=9.676630140351325e+39\n",
      "Gradient Descent(20/49): loss=1.9755384601537402e+42\n",
      "Gradient Descent(21/49): loss=4.033172864597322e+44\n",
      "Gradient Descent(22/49): loss=8.233949217685599e+46\n",
      "Gradient Descent(23/49): loss=1.6810070383381911e+49\n",
      "Gradient Descent(24/49): loss=3.431870404181393e+51\n",
      "Gradient Descent(25/49): loss=7.00635642951762e+53\n",
      "Gradient Descent(26/49): loss=1.4303870669959264e+56\n",
      "Gradient Descent(27/49): loss=2.920215638487541e+58\n",
      "Gradient Descent(28/49): loss=5.96178445123734e+60\n",
      "Gradient Descent(29/49): loss=1.2171318232315232e+63\n",
      "Gradient Descent(30/49): loss=2.484843065427222e+65\n",
      "Gradient Descent(31/49): loss=5.0729468591236545e+67\n",
      "Gradient Descent(32/49): loss=1.0356706302121423e+70\n",
      "Gradient Descent(33/49): loss=2.114379834976804e+72\n",
      "Gradient Descent(34/49): loss=4.316625340279124e+74\n",
      "Gradient Descent(35/49): loss=8.812633387862506e+76\n",
      "Gradient Descent(36/49): loss=1.799148666069954e+79\n",
      "Gradient Descent(37/49): loss=3.6730631811819378e+81\n",
      "Gradient Descent(38/49): loss=7.498765047818502e+83\n",
      "Gradient Descent(39/49): loss=1.5309150555991598e+86\n",
      "Gradient Descent(40/49): loss=3.1254491806514707e+88\n",
      "Gradient Descent(41/49): loss=6.380780269361043e+90\n",
      "Gradient Descent(42/49): loss=1.3026721758240217e+93\n",
      "Gradient Descent(43/49): loss=2.6594785058098688e+95\n",
      "Gradient Descent(44/49): loss=5.429474931703844e+97\n",
      "Gradient Descent(45/49): loss=1.1084578412497132e+100\n",
      "Gradient Descent(46/49): loss=2.262979019671741e+102\n",
      "Gradient Descent(47/49): loss=4.61999893266207e+104\n",
      "Gradient Descent(48/49): loss=9.43198763764709e+106\n",
      "Gradient Descent(49/49): loss=1.925593319249243e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.391884784028169\n",
      "Gradient Descent(2/49): loss=192.03808416009798\n",
      "Gradient Descent(3/49): loss=10074.63522218442\n",
      "Gradient Descent(4/49): loss=645517.1389305278\n",
      "Gradient Descent(5/49): loss=62393625.0766394\n",
      "Gradient Descent(6/49): loss=9315916898.076641\n",
      "Gradient Descent(7/49): loss=1733231052053.5688\n",
      "Gradient Descent(8/49): loss=345584107191618.1\n",
      "Gradient Descent(9/49): loss=7.015825875552754e+16\n",
      "Gradient Descent(10/49): loss=1.4306481637737193e+19\n",
      "Gradient Descent(11/49): loss=2.9204895112741766e+21\n",
      "Gradient Descent(12/49): loss=5.9633731702170984e+23\n",
      "Gradient Descent(13/49): loss=1.2177434286401174e+26\n",
      "Gradient Descent(14/49): loss=2.486716342130261e+28\n",
      "Gradient Descent(15/49): loss=5.078065637196464e+30\n",
      "Gradient Descent(16/49): loss=1.0369809082869068e+33\n",
      "Gradient Descent(17/49): loss=2.1175969630348364e+35\n",
      "Gradient Descent(18/49): loss=4.324300570109918e+37\n",
      "Gradient Descent(19/49): loss=8.830564070591204e+39\n",
      "Gradient Descent(20/49): loss=1.8032710904284258e+42\n",
      "Gradient Descent(21/49): loss=3.682422325452072e+44\n",
      "Gradient Descent(22/49): loss=7.519797914688916e+46\n",
      "Gradient Descent(23/49): loss=1.535602266186893e+49\n",
      "Gradient Descent(24/49): loss=3.135821396658724e+51\n",
      "Gradient Descent(25/49): loss=6.403595545730238e+53\n",
      "Gradient Descent(26/49): loss=1.3076649058206848e+56\n",
      "Gradient Descent(27/49): loss=2.6703552616707825e+58\n",
      "Gradient Descent(28/49): loss=5.4530768485050274e+60\n",
      "Gradient Descent(29/49): loss=1.1135614628705473e+63\n",
      "Gradient Descent(30/49): loss=2.2739806645679574e+65\n",
      "Gradient Descent(31/49): loss=4.643648541409872e+67\n",
      "Gradient Descent(32/49): loss=9.482697945558243e+69\n",
      "Gradient Descent(33/49): loss=1.9364419922140207e+72\n",
      "Gradient Descent(34/49): loss=3.954367850518946e+74\n",
      "Gradient Descent(35/49): loss=8.075132206433536e+76\n",
      "Gradient Descent(36/49): loss=1.649005925000736e+79\n",
      "Gradient Descent(37/49): loss=3.3674006458013526e+81\n",
      "Gradient Descent(38/49): loss=6.876498705933021e+83\n",
      "Gradient Descent(39/49): loss=1.4042354749695635e+86\n",
      "Gradient Descent(40/49): loss=2.867560009081009e+88\n",
      "Gradient Descent(41/49): loss=5.855784554836537e+90\n",
      "Gradient Descent(42/49): loss=1.195797564621899e+93\n",
      "Gradient Descent(43/49): loss=2.441913294734637e+95\n",
      "Gradient Descent(44/49): loss=4.98658026694275e+97\n",
      "Gradient Descent(45/49): loss=1.018299167799252e+100\n",
      "Gradient Descent(46/49): loss=2.0794475163966231e+102\n",
      "Gradient Descent(47/49): loss=4.246396452226645e+104\n",
      "Gradient Descent(48/49): loss=8.67147773016639e+106\n",
      "Gradient Descent(49/49): loss=1.7707844020391436e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.23419352514541\n",
      "Gradient Descent(2/49): loss=182.2652593228217\n",
      "Gradient Descent(3/49): loss=9312.957227412568\n",
      "Gradient Descent(4/49): loss=555988.8068478447\n",
      "Gradient Descent(5/49): loss=47818561.72663064\n",
      "Gradient Descent(6/49): loss=6589420702.485637\n",
      "Gradient Descent(7/49): loss=1201439548344.382\n",
      "Gradient Descent(8/49): loss=240783646300187.6\n",
      "Gradient Descent(9/49): loss=4.947248286968357e+16\n",
      "Gradient Descent(10/49): loss=1.0226800647580418e+19\n",
      "Gradient Descent(11/49): loss=2.1171319348398575e+21\n",
      "Gradient Descent(12/49): loss=4.3843650776804435e+23\n",
      "Gradient Descent(13/49): loss=9.0803244419446e+25\n",
      "Gradient Descent(14/49): loss=1.8806352004006286e+28\n",
      "Gradient Descent(15/49): loss=3.8950208544373877e+30\n",
      "Gradient Descent(16/49): loss=8.067063909985495e+32\n",
      "Gradient Descent(17/49): loss=1.6707879061612976e+35\n",
      "Gradient Descent(18/49): loss=3.460406951828923e+37\n",
      "Gradient Descent(19/49): loss=7.166927895468295e+39\n",
      "Gradient Descent(20/49): loss=1.484358815522417e+42\n",
      "Gradient Descent(21/49): loss=3.0742894685553684e+44\n",
      "Gradient Descent(22/49): loss=6.367231183962527e+46\n",
      "Gradient Descent(23/49): loss=1.3187318034507907e+49\n",
      "Gradient Descent(24/49): loss=2.73125557908561e+51\n",
      "Gradient Descent(25/49): loss=5.6567658554880864e+53\n",
      "Gradient Descent(26/49): loss=1.1715857054486735e+56\n",
      "Gradient Descent(27/49): loss=2.4264979323479957e+58\n",
      "Gradient Descent(28/49): loss=5.0255753277858295e+60\n",
      "Gradient Descent(29/49): loss=1.040858392605797e+63\n",
      "Gradient Descent(30/49): loss=2.1557456068124094e+65\n",
      "Gradient Descent(31/49): loss=4.464813998046955e+67\n",
      "Gradient Descent(32/49): loss=9.247178319259961e+69\n",
      "Gradient Descent(33/49): loss=1.9152042370767713e+72\n",
      "Gradient Descent(34/49): loss=3.966623269367705e+74\n",
      "Gradient Descent(35/49): loss=8.215364114432303e+76\n",
      "Gradient Descent(36/49): loss=1.7015028387976978e+79\n",
      "Gradient Descent(37/49): loss=3.524021419027064e+81\n",
      "Gradient Descent(38/49): loss=7.298681306072385e+83\n",
      "Gradient Descent(39/49): loss=1.511646567185656e+86\n",
      "Gradient Descent(40/49): loss=3.130805755531491e+88\n",
      "Gradient Descent(41/49): loss=6.484283358059157e+90\n",
      "Gradient Descent(42/49): loss=1.3429747467825386e+93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/49): loss=2.7814656931271326e+95\n",
      "Gradient Descent(44/49): loss=5.760757170287845e+97\n",
      "Gradient Descent(45/49): loss=1.1931235843399169e+100\n",
      "Gradient Descent(46/49): loss=2.4711055255900935e+102\n",
      "Gradient Descent(47/49): loss=5.117963133701888e+104\n",
      "Gradient Descent(48/49): loss=1.0599930422508784e+107\n",
      "Gradient Descent(49/49): loss=2.1953758170343314e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.308852568965708\n",
      "Gradient Descent(2/49): loss=188.70052514064463\n",
      "Gradient Descent(3/49): loss=10121.106925119471\n",
      "Gradient Descent(4/49): loss=697625.9580679747\n",
      "Gradient Descent(5/49): loss=76517156.61384389\n",
      "Gradient Descent(6/49): loss=12674153369.815874\n",
      "Gradient Descent(7/49): loss=2507195497865.3555\n",
      "Gradient Descent(8/49): loss=521740386542267.44\n",
      "Gradient Descent(9/49): loss=1.0993643088904582e+17\n",
      "Gradient Descent(10/49): loss=2.3233430259039203e+19\n",
      "Gradient Descent(11/49): loss=4.9134499304956e+21\n",
      "Gradient Descent(12/49): loss=1.0392746702175559e+24\n",
      "Gradient Descent(13/49): loss=2.198318817269191e+26\n",
      "Gradient Descent(14/49): loss=4.650020636826337e+28\n",
      "Gradient Descent(15/49): loss=9.836033255759637e+30\n",
      "Gradient Descent(16/49): loss=2.080584258861367e+33\n",
      "Gradient Descent(17/49): loss=4.4009930018157654e+35\n",
      "Gradient Descent(18/49): loss=9.309279274336703e+37\n",
      "Gradient Descent(19/49): loss=1.9691619848914802e+40\n",
      "Gradient Descent(20/49): loss=4.1653051908170375e+42\n",
      "Gradient Descent(21/49): loss=8.810736481664139e+44\n",
      "Gradient Descent(22/49): loss=1.8637068305749208e+47\n",
      "Gradient Descent(23/49): loss=3.942239286835135e+49\n",
      "Gradient Descent(24/49): loss=8.338892329938112e+51\n",
      "Gradient Descent(25/49): loss=1.7638991504800136e+54\n",
      "Gradient Descent(26/49): loss=3.731119302133852e+56\n",
      "Gradient Descent(27/49): loss=7.892316997243308e+58\n",
      "Gradient Descent(28/49): loss=1.6694365025892783e+61\n",
      "Gradient Descent(29/49): loss=3.531305492608802e+63\n",
      "Gradient Descent(30/49): loss=7.469657254281943e+65\n",
      "Gradient Descent(31/49): loss=1.5800326426935042e+68\n",
      "Gradient Descent(32/49): loss=3.3421923750864677e+70\n",
      "Gradient Descent(33/49): loss=7.069632341926742e+72\n",
      "Gradient Descent(34/49): loss=1.4954166559225873e+75\n",
      "Gradient Descent(35/49): loss=3.163206892030844e+77\n",
      "Gradient Descent(36/49): loss=6.691030089950687e+79\n",
      "Gradient Descent(37/49): loss=1.4153321357959428e+82\n",
      "Gradient Descent(38/49): loss=2.9938066750369135e+84\n",
      "Gradient Descent(39/49): loss=6.332703243860966e+86\n",
      "Gradient Descent(40/49): loss=1.3395364072501979e+89\n",
      "Gradient Descent(41/49): loss=2.833478401326115e+91\n",
      "Gradient Descent(42/49): loss=5.9935659884473225e+93\n",
      "Gradient Descent(43/49): loss=1.2677997912763463e+96\n",
      "Gradient Descent(44/49): loss=2.6817362382569153e+98\n",
      "Gradient Descent(45/49): loss=5.672590657504502e+100\n",
      "Gradient Descent(46/49): loss=1.1999049089377519e+103\n",
      "Gradient Descent(47/49): loss=2.538120371136205e+105\n",
      "Gradient Descent(48/49): loss=5.368804619759058e+107\n",
      "Gradient Descent(49/49): loss=1.1356460226605802e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.344939260323001\n",
      "Gradient Descent(2/49): loss=192.4533387787172\n",
      "Gradient Descent(3/49): loss=10305.076768529332\n",
      "Gradient Descent(4/49): loss=683715.7136243862\n",
      "Gradient Descent(5/49): loss=69178661.97314732\n",
      "Gradient Descent(6/49): loss=10667907451.890837\n",
      "Gradient Descent(7/49): loss=2017871640123.6033\n",
      "Gradient Descent(8/49): loss=406575937158364.7\n",
      "Gradient Descent(9/49): loss=8.327506731335251e+16\n",
      "Gradient Descent(10/49): loss=1.712570768741276e+19\n",
      "Gradient Descent(11/49): loss=3.5254244109485463e+21\n",
      "Gradient Descent(12/49): loss=7.259031580017419e+23\n",
      "Gradient Descent(13/49): loss=1.494759526325884e+26\n",
      "Gradient Descent(14/49): loss=3.0780107768836214e+28\n",
      "Gradient Descent(15/49): loss=6.338265673900751e+30\n",
      "Gradient Descent(16/49): loss=1.305182089977402e+33\n",
      "Gradient Descent(17/49): loss=2.6876447313635758e+35\n",
      "Gradient Descent(18/49): loss=5.534426663028047e+37\n",
      "Gradient Descent(19/49): loss=1.139655048100943e+40\n",
      "Gradient Descent(20/49): loss=2.34678985110516e+42\n",
      "Gradient Descent(21/49): loss=4.832534737881748e+44\n",
      "Gradient Descent(22/49): loss=9.951207171725525e+46\n",
      "Gradient Descent(23/49): loss=2.0491632145441602e+49\n",
      "Gradient Descent(24/49): loss=4.219658788548116e+51\n",
      "Gradient Descent(25/49): loss=8.689166468263094e+53\n",
      "Gradient Descent(26/49): loss=1.7892824443089837e+56\n",
      "Gradient Descent(27/49): loss=3.684509529431145e+58\n",
      "Gradient Descent(28/49): loss=7.587181395339754e+60\n",
      "Gradient Descent(29/49): loss=1.5623605005216915e+63\n",
      "Gradient Descent(30/49): loss=3.217229437917059e+65\n",
      "Gradient Descent(31/49): loss=6.624953237581414e+67\n",
      "Gradient Descent(32/49): loss=1.3642174500478271e+70\n",
      "Gradient Descent(33/49): loss=2.8092111510426e+72\n",
      "Gradient Descent(34/49): loss=5.784757621203019e+74\n",
      "Gradient Descent(35/49): loss=1.1912034708977598e+77\n",
      "Gradient Descent(36/49): loss=2.452938916365089e+79\n",
      "Gradient Descent(37/49): loss=5.0511180284620884e+81\n",
      "Gradient Descent(38/49): loss=1.0401316219998891e+84\n",
      "Gradient Descent(39/49): loss=2.1418501507745396e+86\n",
      "Gradient Descent(40/49): loss=4.410520718091697e+88\n",
      "Gradient Descent(41/49): loss=9.082191393119473e+90\n",
      "Gradient Descent(42/49): loss=1.8702145568186614e+93\n",
      "Gradient Descent(43/49): loss=3.851165800344399e+95\n",
      "Gradient Descent(44/49): loss=7.930361769278336e+97\n",
      "Gradient Descent(45/49): loss=1.6330285698425911e+100\n",
      "Gradient Descent(46/49): loss=3.3627498814153803e+102\n",
      "Gradient Descent(47/49): loss=6.924610489851464e+104\n",
      "Gradient Descent(48/49): loss=1.425923191646275e+107\n",
      "Gradient Descent(49/49): loss=2.9362762735241228e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.429424316313511\n",
      "Gradient Descent(2/49): loss=195.59661122045134\n",
      "Gradient Descent(3/49): loss=10354.69409161845\n",
      "Gradient Descent(4/49): loss=669218.1439184953\n",
      "Gradient Descent(5/49): loss=65205605.01466414\n",
      "Gradient Descent(6/49): loss=9813857421.553614\n",
      "Gradient Descent(7/49): loss=1841142290356.4302\n",
      "Gradient Descent(8/49): loss=370241069827691.8\n",
      "Gradient Descent(9/49): loss=7.581195770984214e+16\n",
      "Gradient Descent(10/49): loss=1.5592940989106358e+19\n",
      "Gradient Descent(11/49): loss=3.210623684024047e+21\n",
      "Gradient Descent(12/49): loss=6.61248835602843e+23\n",
      "Gradient Descent(13/49): loss=1.3619716655349185e+26\n",
      "Gradient Descent(14/49): loss=2.805290978681484e+28\n",
      "Gradient Descent(15/49): loss=5.778157519195472e+30\n",
      "Gradient Descent(16/49): loss=1.1901487120505571e+33\n",
      "Gradient Descent(17/49): loss=2.451394344236052e+35\n",
      "Gradient Descent(18/49): loss=5.049229972617458e+37\n",
      "Gradient Descent(19/49): loss=1.040009074944987e+40\n",
      "Gradient Descent(20/49): loss=2.142146198896145e+42\n",
      "Gradient Descent(21/49): loss=4.412259903722185e+44\n",
      "Gradient Descent(22/49): loss=9.088099342407591e+46\n",
      "Gradient Descent(23/49): loss=1.8719103467006086e+49\n",
      "Gradient Descent(24/49): loss=3.855644853892857e+51\n",
      "Gradient Descent(25/49): loss=7.941618179306021e+53\n",
      "Gradient Descent(26/49): loss=1.635765266145982e+56\n",
      "Gradient Descent(27/49): loss=3.369247860470491e+58\n",
      "Gradient Descent(28/49): loss=6.939767814016069e+60\n",
      "Gradient Descent(29/49): loss=1.4294103404350906e+63\n",
      "Gradient Descent(30/49): loss=2.944210780678004e+65\n",
      "Gradient Descent(31/49): loss=6.06430279385139e+67\n",
      "Gradient Descent(32/49): loss=1.2490874843901026e+70\n",
      "Gradient Descent(33/49): loss=2.5727929437196414e+72\n",
      "Gradient Descent(34/49): loss=5.299279365116354e+74\n",
      "Gradient Descent(35/49): loss=1.0915127024931586e+77\n",
      "Gradient Descent(36/49): loss=2.2482301792703906e+79\n",
      "Gradient Descent(37/49): loss=4.630765109225756e+81\n",
      "Gradient Descent(38/49): loss=9.538162815598479e+83\n",
      "Gradient Descent(39/49): loss=1.9646116300655212e+86\n",
      "Gradient Descent(40/49): loss=4.046585208921673e+88\n",
      "Gradient Descent(41/49): loss=8.334905282280735e+90\n",
      "Gradient Descent(42/49): loss=1.7167721048212886e+93\n",
      "Gradient Descent(43/49): loss=3.5361007234938014e+95\n",
      "Gradient Descent(44/49): loss=7.283440994630374e+97\n",
      "Gradient Descent(45/49): loss=1.5001980110410532e+100\n",
      "Gradient Descent(46/49): loss=3.090014835008334e+102\n",
      "Gradient Descent(47/49): loss=6.364620943568286e+104\n",
      "Gradient Descent(48/49): loss=1.3109451545788123e+107\n",
      "Gradient Descent(49/49): loss=2.700203536944939e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.270265501580443\n",
      "Gradient Descent(2/49): loss=185.64503142824515\n",
      "Gradient Descent(3/49): loss=9572.362695517155\n",
      "Gradient Descent(4/49): loss=576500.3646473952\n",
      "Gradient Descent(5/49): loss=49985532.20154173\n",
      "Gradient Descent(6/49): loss=6942460506.716494\n",
      "Gradient Descent(7/49): loss=1276266791561.864\n",
      "Gradient Descent(8/49): loss=257958462508282.56\n",
      "Gradient Descent(9/49): loss=5.345749958318281e+16\n",
      "Gradient Descent(10/49): loss=1.114597812579334e+19\n",
      "Gradient Descent(11/49): loss=2.327356334711203e+21\n",
      "Gradient Descent(12/49): loss=4.8613742960236385e+23\n",
      "Gradient Descent(13/49): loss=1.0155266550383102e+26\n",
      "Gradient Descent(14/49): loss=2.12144701236512e+28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(15/49): loss=4.431748409988037e+30\n",
      "Gradient Descent(16/49): loss=9.258028082210607e+32\n",
      "Gradient Descent(17/49): loss=1.9340246546942247e+35\n",
      "Gradient Descent(18/49): loss=4.0402249484172977e+37\n",
      "Gradient Descent(19/49): loss=8.44012915847506e+39\n",
      "Gradient Descent(20/49): loss=1.7631637193085542e+42\n",
      "Gradient Descent(21/49): loss=3.683292335209679e+44\n",
      "Gradient Descent(22/49): loss=7.694488198017353e+46\n",
      "Gradient Descent(23/49): loss=1.607397492356443e+49\n",
      "Gradient Descent(24/49): loss=3.3578928604108585e+51\n",
      "Gradient Descent(25/49): loss=7.014720699544771e+53\n",
      "Gradient Descent(26/49): loss=1.4653923915438473e+56\n",
      "Gradient Descent(27/49): loss=3.061240715306461e+58\n",
      "Gradient Descent(28/49): loss=6.395007078737158e+60\n",
      "Gradient Descent(29/49): loss=1.3359326933231171e+63\n",
      "Gradient Descent(30/49): loss=2.790796224485188e+65\n",
      "Gradient Descent(31/49): loss=5.830041891726436e+67\n",
      "Gradient Descent(32/49): loss=1.217910077456671e+70\n",
      "Gradient Descent(33/49): loss=2.5442440797475622e+72\n",
      "Gradient Descent(34/49): loss=5.314988402795969e+74\n",
      "Gradient Descent(35/49): loss=1.1103141379681708e+77\n",
      "Gradient Descent(36/49): loss=2.3194735181801748e+79\n",
      "Gradient Descent(37/49): loss=4.845437176351139e+81\n",
      "Gradient Descent(38/49): loss=1.012223732926518e+84\n",
      "Gradient Descent(39/49): loss=2.1145602516536485e+86\n",
      "Gradient Descent(40/49): loss=4.417368327203765e+88\n",
      "Gradient Descent(41/49): loss=9.227990984377551e+90\n",
      "Gradient Descent(42/49): loss=1.9277499927577178e+93\n",
      "Gradient Descent(43/49): loss=4.027117105845346e+95\n",
      "Gradient Descent(44/49): loss=8.412746593240771e+97\n",
      "Gradient Descent(45/49): loss=1.7574434361334264e+100\n",
      "Gradient Descent(46/49): loss=3.671342524081265e+102\n",
      "Gradient Descent(47/49): loss=7.669524749417885e+104\n",
      "Gradient Descent(48/49): loss=1.6021825666253284e+107\n",
      "Gradient Descent(49/49): loss=3.3469987524233197e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.345450673959269\n",
      "Gradient Descent(2/49): loss=192.18163056985765\n",
      "Gradient Descent(3/49): loss=10400.810904385728\n",
      "Gradient Descent(4/49): loss=723012.8606072442\n",
      "Gradient Descent(5/49): loss=79932101.68400946\n",
      "Gradient Descent(6/49): loss=13346568062.778063\n",
      "Gradient Descent(7/49): loss=2662363016890.774\n",
      "Gradient Descent(8/49): loss=558761093281722.94\n",
      "Gradient Descent(9/49): loss=1.1874790179169664e+17\n",
      "Gradient Descent(10/49): loss=2.5311379267987108e+19\n",
      "Gradient Descent(11/49): loss=5.398942001918367e+21\n",
      "Gradient Descent(12/49): loss=1.1517878974556478e+24\n",
      "Gradient Descent(13/49): loss=2.457270617853292e+26\n",
      "Gradient Descent(14/49): loss=5.242486956275072e+28\n",
      "Gradient Descent(15/49): loss=1.1184656257284664e+31\n",
      "Gradient Descent(16/49): loss=2.3862071215932248e+33\n",
      "Gradient Descent(17/49): loss=5.090889654295819e+35\n",
      "Gradient Descent(18/49): loss=1.0861235781432452e+38\n",
      "Gradient Descent(19/49): loss=2.3172068431121145e+40\n",
      "Gradient Descent(20/49): loss=4.943680138942216e+42\n",
      "Gradient Descent(21/49): loss=1.0547169492995828e+45\n",
      "Gradient Descent(22/49): loss=2.2502018981106683e+47\n",
      "Gradient Descent(23/49): loss=4.80072742333343e+49\n",
      "Gradient Descent(24/49): loss=1.0242184851323271e+52\n",
      "Gradient Descent(25/49): loss=2.1851344864721727e+54\n",
      "Gradient Descent(26/49): loss=4.661908365532315e+56\n",
      "Gradient Descent(27/49): loss=9.946019223608016e+58\n",
      "Gradient Descent(28/49): loss=2.1219485807093376e+61\n",
      "Gradient Descent(29/49): loss=4.5271034349972086e+63\n",
      "Gradient Descent(30/49): loss=9.658417596675407e+65\n",
      "Gradient Descent(31/49): loss=2.060589774702736e+68\n",
      "Gradient Descent(32/49): loss=4.396196558192844e+70\n",
      "Gradient Descent(33/49): loss=9.379132331691252e+72\n",
      "Gradient Descent(34/49): loss=2.0010052355697328e+75\n",
      "Gradient Descent(35/49): loss=4.269075018003866e+77\n",
      "Gradient Descent(36/49): loss=9.107922950614176e+79\n",
      "Gradient Descent(37/49): loss=1.9431436581573918e+82\n",
      "Gradient Descent(38/49): loss=4.1456293566719616e+84\n",
      "Gradient Descent(39/49): loss=8.844555929126451e+86\n",
      "Gradient Descent(40/49): loss=1.88695522086532e+89\n",
      "Gradient Descent(41/49): loss=4.025753281547274e+91\n",
      "Gradient Descent(42/49): loss=8.588804495560131e+93\n",
      "Gradient Descent(43/49): loss=1.8323915427474943e+96\n",
      "Gradient Descent(44/49): loss=3.909343573565241e+98\n",
      "Gradient Descent(45/49): loss=8.340448435633008e+100\n",
      "Gradient Descent(46/49): loss=1.7794056418534573e+103\n",
      "Gradient Descent(47/49): loss=3.796299998369994e+105\n",
      "Gradient Descent(48/49): loss=8.099273902836626e+107\n",
      "Gradient Descent(49/49): loss=1.7279518947748036e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.381857695571276\n",
      "Gradient Descent(2/49): loss=196.00176340794087\n",
      "Gradient Descent(3/49): loss=10589.98541993126\n",
      "Gradient Descent(4/49): loss=708663.6051127169\n",
      "Gradient Descent(5/49): loss=72277456.63634437\n",
      "Gradient Descent(6/49): loss=11235195987.978754\n",
      "Gradient Descent(7/49): loss=2142925383510.1016\n",
      "Gradient Descent(8/49): loss=435454731385457.8\n",
      "Gradient Descent(9/49): loss=8.995587189442021e+16\n",
      "Gradient Descent(10/49): loss=1.865879660886301e+19\n",
      "Gradient Descent(11/49): loss=3.874085038101197e+21\n",
      "Gradient Descent(12/49): loss=8.045622057549133e+23\n",
      "Gradient Descent(13/49): loss=1.6709968179221286e+26\n",
      "Gradient Descent(14/49): loss=3.470546121033695e+28\n",
      "Gradient Descent(15/49): loss=7.208111983082418e+30\n",
      "Gradient Descent(16/49): loss=1.4970820263714447e+33\n",
      "Gradient Descent(17/49): loss=3.1093510457718975e+35\n",
      "Gradient Descent(18/49): loss=6.457939002260861e+37\n",
      "Gradient Descent(19/49): loss=1.3412759141114704e+40\n",
      "Gradient Descent(20/49): loss=2.785751123450298e+42\n",
      "Gradient Descent(21/49): loss=5.7858411127130425e+44\n",
      "Gradient Descent(22/49): loss=1.2016851435716619e+47\n",
      "Gradient Descent(23/49): loss=2.4958293119542236e+49\n",
      "Gradient Descent(24/49): loss=5.183690576351294e+51\n",
      "Gradient Descent(25/49): loss=1.0766220214947468e+54\n",
      "Gradient Descent(26/49): loss=2.2360805686516072e+56\n",
      "Gradient Descent(27/49): loss=4.6442077253446496e+58\n",
      "Gradient Descent(28/49): loss=9.645746087386225e+60\n",
      "Gradient Descent(29/49): loss=2.0033646874703857e+63\n",
      "Gradient Descent(30/49): loss=4.1608705377925367e+65\n",
      "Gradient Descent(31/49): loss=8.64188319807645e+67\n",
      "Gradient Descent(32/49): loss=1.7948682740996435e+70\n",
      "Gradient Descent(33/49): loss=3.727835759324403e+72\n",
      "Gradient Descent(34/49): loss=7.742495451633558e+74\n",
      "Gradient Descent(35/49): loss=1.6080707329614204e+77\n",
      "Gradient Descent(36/49): loss=3.3398682612871076e+79\n",
      "Gradient Descent(37/49): loss=6.936709793984167e+81\n",
      "Gradient Descent(38/49): loss=1.4407137947235536e+84\n",
      "Gradient Descent(39/49): loss=2.9922777511996165e+86\n",
      "Gradient Descent(40/49): loss=6.21478476371663e+88\n",
      "Gradient Descent(41/49): loss=1.2907742151890461e+91\n",
      "Gradient Descent(42/49): loss=2.680862069952903e+93\n",
      "Gradient Descent(43/49): loss=5.567992723699972e+95\n",
      "Gradient Descent(44/49): loss=1.1564393154967728e+98\n",
      "Gradient Descent(45/49): loss=2.4018563902467772e+100\n",
      "Gradient Descent(46/49): loss=4.988514349229973e+102\n",
      "Gradient Descent(47/49): loss=1.0360850679301602e+105\n",
      "Gradient Descent(48/49): loss=2.1518877021042615e+107\n",
      "Gradient Descent(49/49): loss=4.469344097119704e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.467139322080613\n",
      "Gradient Descent(2/49): loss=199.2044792398789\n",
      "Gradient Descent(3/49): loss=10641.213772564597\n",
      "Gradient Descent(4/49): loss=693677.8170402884\n",
      "Gradient Descent(5/49): loss=68131250.70012146\n",
      "Gradient Descent(6/49): loss=10336099301.263971\n",
      "Gradient Descent(7/49): loss=1955265628199.4583\n",
      "Gradient Descent(8/49): loss=396540105772755.06\n",
      "Gradient Descent(9/49): loss=8.189404795326928e+16\n",
      "Gradient Descent(10/49): loss=1.6988808785401446e+19\n",
      "Gradient Descent(11/49): loss=3.528148300330727e+21\n",
      "Gradient Descent(12/49): loss=7.329013082283821e+23\n",
      "Gradient Descent(13/49): loss=1.522551404965354e+26\n",
      "Gradient Descent(14/49): loss=3.163043509174398e+28\n",
      "Gradient Descent(15/49): loss=6.571129080765635e+30\n",
      "Gradient Descent(16/49): loss=1.365133819559908e+33\n",
      "Gradient Descent(17/49): loss=2.836028227327157e+35\n",
      "Gradient Descent(18/49): loss=5.891771502609569e+37\n",
      "Gradient Descent(19/49): loss=1.2239995199289482e+40\n",
      "Gradient Descent(20/49): loss=2.5428257508147896e+42\n",
      "Gradient Descent(21/49): loss=5.282651421532614e+44\n",
      "Gradient Descent(22/49): loss=1.0974564827181983e+47\n",
      "Gradient Descent(23/49): loss=2.2799360309946518e+49\n",
      "Gradient Descent(24/49): loss=4.736505171128909e+51\n",
      "Gradient Descent(25/49): loss=9.839960828375418e+53\n",
      "Gradient Descent(26/49): loss=2.0442251323658538e+56\n",
      "Gradient Descent(27/49): loss=4.2468221822044264e+58\n",
      "Gradient Descent(28/49): loss=8.822657720870202e+60\n",
      "Gradient Descent(29/49): loss=1.8328831752316857e+63\n",
      "Gradient Descent(30/49): loss=3.8077650072500034e+65\n",
      "Gradient Descent(31/49): loss=7.91052836665646e+67\n",
      "Gradient Descent(32/49): loss=1.6433907796444916e+70\n",
      "Gradient Descent(33/49): loss=3.414099702877438e+72\n",
      "Gradient Descent(34/49): loss=7.092699390530298e+74\n",
      "Gradient Descent(35/49): loss=1.4734890314430933e+77\n",
      "Gradient Descent(36/49): loss=3.061133436279445e+79\n",
      "Gradient Descent(37/49): loss=6.359421559813475e+81\n",
      "Gradient Descent(38/49): loss=1.3211525540217781e+84\n",
      "Gradient Descent(39/49): loss=2.744658542575754e+86\n",
      "Gradient Descent(40/49): loss=5.7019535650156765e+88\n",
      "Gradient Descent(41/49): loss=1.1845653640792805e+91\n",
      "Gradient Descent(42/49): loss=2.460902365788419e+93\n",
      "Gradient Descent(43/49): loss=5.11245781582521e+95\n",
      "Gradient Descent(44/49): loss=1.0620992235187223e+98\n",
      "Gradient Descent(45/49): loss=2.206482285501254e+100\n",
      "Gradient Descent(46/49): loss=4.5839070102145e+102\n",
      "Gradient Descent(47/49): loss=9.522942294331576e+104\n",
      "Gradient Descent(48/49): loss=1.9783653930825675e+107\n",
      "Gradient Descent(49/49): loss=4.110000362888372e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.306506271143817\n",
      "Gradient Descent(2/49): loss=189.071703086996\n",
      "Gradient Descent(3/49): loss=9837.76395591665\n",
      "Gradient Descent(4/49): loss=597671.5363175749\n",
      "Gradient Descent(5/49): loss=52240583.62028972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=7312772962.904962\n",
      "Gradient Descent(7/49): loss=1355403284416.8237\n",
      "Gradient Descent(8/49): loss=276276764888005.6\n",
      "Gradient Descent(9/49): loss=5.774433969966925e+16\n",
      "Gradient Descent(10/49): loss=1.214329170498813e+19\n",
      "Gradient Descent(11/49): loss=2.5574177468427786e+21\n",
      "Gradient Descent(12/49): loss=5.387896326688023e+23\n",
      "Gradient Descent(13/49): loss=1.135201856171887e+26\n",
      "Gradient Descent(14/49): loss=2.391859177740318e+28\n",
      "Gradient Descent(15/49): loss=5.0396477545429377e+30\n",
      "Gradient Descent(16/49): loss=1.0618550814883784e+33\n",
      "Gradient Descent(17/49): loss=2.2373319980616584e+35\n",
      "Gradient Descent(18/49): loss=4.714065862980782e+37\n",
      "Gradient Descent(19/49): loss=9.932552397188571e+39\n",
      "Gradient Descent(20/49): loss=2.092792089678881e+42\n",
      "Gradient Descent(21/49): loss=4.4095198890353835e+44\n",
      "Gradient Descent(22/49): loss=9.29087306460967e+46\n",
      "Gradient Descent(23/49): loss=1.9575900433421152e+49\n",
      "Gradient Descent(24/49): loss=4.1246487291215564e+51\n",
      "Gradient Descent(25/49): loss=8.690648584239382e+53\n",
      "Gradient Descent(26/49): loss=1.8311225458192243e+56\n",
      "Gradient Descent(27/49): loss=3.85818129142716e+58\n",
      "Gradient Descent(28/49): loss=8.129200807179915e+60\n",
      "Gradient Descent(29/49): loss=1.7128253125454387e+63\n",
      "Gradient Descent(30/49): loss=3.608928627652082e+65\n",
      "Gradient Descent(31/49): loss=7.604024615990243e+67\n",
      "Gradient Descent(32/49): loss=1.6021705144721094e+70\n",
      "Gradient Descent(33/49): loss=3.375778600250587e+72\n",
      "Gradient Descent(34/49): loss=7.112776733170892e+74\n",
      "Gradient Descent(35/49): loss=1.498664422251593e+77\n",
      "Gradient Descent(36/49): loss=3.1576909198461147e+79\n",
      "Gradient Descent(37/49): loss=6.653265265547769e+81\n",
      "Gradient Descent(38/49): loss=1.4018452032633094e+84\n",
      "Gradient Descent(39/49): loss=2.9536925035719e+86\n",
      "Gradient Descent(40/49): loss=6.223439924285398e+88\n",
      "Gradient Descent(41/49): loss=1.311280860968118e+91\n",
      "Gradient Descent(42/49): loss=2.7628731332837152e+93\n",
      "Gradient Descent(43/49): loss=5.821382876727901e+95\n",
      "Gradient Descent(44/49): loss=1.226567307387851e+98\n",
      "Gradient Descent(45/49): loss=2.5843813942682242e+100\n",
      "Gradient Descent(46/49): loss=5.445300189244225e+102\n",
      "Gradient Descent(47/49): loss=1.147326560110098e+105\n",
      "Gradient Descent(48/49): loss=2.4174208763259065e+107\n",
      "Gradient Descent(49/49): loss=5.093513822895914e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.382219191019479\n",
      "Gradient Descent(2/49): loss=195.71079307745373\n",
      "Gradient Descent(3/49): loss=10686.930570318682\n",
      "Gradient Descent(4/49): loss=749205.3237010388\n",
      "Gradient Descent(5/49): loss=83483737.46624143\n",
      "Gradient Descent(6/49): loss=14051553186.073181\n",
      "Gradient Descent(7/49): loss=2826409618749.299\n",
      "Gradient Descent(8/49): loss=598233726017030.8\n",
      "Gradient Descent(9/49): loss=1.2822344926930728e+17\n",
      "Gradient Descent(10/49): loss=2.756510360490226e+19\n",
      "Gradient Descent(11/49): loss=5.930021486469074e+21\n",
      "Gradient Descent(12/49): loss=1.275922486576433e+24\n",
      "Gradient Descent(13/49): loss=2.7454215909073877e+26\n",
      "Gradient Descent(14/49): loss=5.907418218099553e+28\n",
      "Gradient Descent(15/49): loss=1.2711222174536324e+31\n",
      "Gradient Descent(16/49): loss=2.735124605486865e+33\n",
      "Gradient Descent(17/49): loss=5.885277885442642e+35\n",
      "Gradient Descent(18/49): loss=1.2663590046907298e+38\n",
      "Gradient Descent(19/49): loss=2.724875817587777e+40\n",
      "Gradient Descent(20/49): loss=5.863225360944562e+42\n",
      "Gradient Descent(21/49): loss=1.2616138843231972e+45\n",
      "Gradient Descent(22/49): loss=2.71466555566804e+47\n",
      "Gradient Descent(23/49): loss=5.84125553058994e+49\n",
      "Gradient Descent(24/49): loss=1.256886547315621e+52\n",
      "Gradient Descent(25/49): loss=2.704493553743012e+54\n",
      "Gradient Descent(26/49): loss=5.8193680232004735e+56\n",
      "Gradient Descent(27/49): loss=1.2521769239413193e+59\n",
      "Gradient Descent(28/49): loss=2.6943596668918684e+61\n",
      "Gradient Descent(29/49): loss=5.7975625295214846e+63\n",
      "Gradient Descent(30/49): loss=1.247484947786663e+66\n",
      "Gradient Descent(31/49): loss=2.684263752275093e+68\n",
      "Gradient Descent(32/49): loss=5.775838742232543e+70\n",
      "Gradient Descent(33/49): loss=1.2428105527260018e+73\n",
      "Gradient Descent(34/49): loss=2.674205667608591e+75\n",
      "Gradient Descent(35/49): loss=5.754196355175617e+77\n",
      "Gradient Descent(36/49): loss=1.238153672882063e+80\n",
      "Gradient Descent(37/49): loss=2.6641852711412628e+82\n",
      "Gradient Descent(38/49): loss=5.7326350633393566e+84\n",
      "Gradient Descent(39/49): loss=1.2335142426243335e+87\n",
      "Gradient Descent(40/49): loss=2.6542024216534474e+89\n",
      "Gradient Descent(41/49): loss=5.711154562855486e+91\n",
      "Gradient Descent(42/49): loss=1.2288921965682641e+94\n",
      "Gradient Descent(43/49): loss=2.644256978454647e+96\n",
      "Gradient Descent(44/49): loss=5.689754550994671e+98\n",
      "Gradient Descent(45/49): loss=1.224287469574281e+101\n",
      "Gradient Descent(46/49): loss=2.6343488013811857e+103\n",
      "Gradient Descent(47/49): loss=5.668434726160809e+105\n",
      "Gradient Descent(48/49): loss=1.2196999967468366e+108\n",
      "Gradient Descent(49/49): loss=2.6244777507949236e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.41894794611647\n",
      "Gradient Descent(2/49): loss=199.5991450715707\n",
      "Gradient Descent(3/49): loss=10881.43139516119\n",
      "Gradient Descent(4/49): loss=734405.1681646284\n",
      "Gradient Descent(5/49): loss=75500746.07005614\n",
      "Gradient Descent(6/49): loss=11830028176.064455\n",
      "Gradient Descent(7/49): loss=2275144915119.8613\n",
      "Gradient Descent(8/49): loss=466248180571718.7\n",
      "Gradient Descent(9/49): loss=9.714064654231144e+16\n",
      "Gradient Descent(10/49): loss=2.0321688961494147e+19\n",
      "Gradient Descent(11/49): loss=4.255514419987961e+21\n",
      "Gradient Descent(12/49): loss=8.913532495177915e+23\n",
      "Gradient Descent(13/49): loss=1.8671246179619867e+26\n",
      "Gradient Descent(14/49): loss=3.9111367466009827e+28\n",
      "Gradient Descent(15/49): loss=8.192835108916202e+30\n",
      "Gradient Descent(16/49): loss=1.7161916990198154e+33\n",
      "Gradient Descent(17/49): loss=3.594988203140572e+35\n",
      "Gradient Descent(18/49): loss=7.5305927874268075e+37\n",
      "Gradient Descent(19/49): loss=1.577469110298309e+40\n",
      "Gradient Descent(20/49): loss=3.3043996290690504e+42\n",
      "Gradient Descent(21/49): loss=6.921883189473123e+44\n",
      "Gradient Descent(22/49): loss=1.449959819497847e+47\n",
      "Gradient Descent(23/49): loss=3.037299851354679e+49\n",
      "Gradient Descent(24/49): loss=6.362376572841944e+51\n",
      "Gradient Descent(25/49): loss=1.3327573053643248e+54\n",
      "Gradient Descent(26/49): loss=2.7917901662486197e+56\n",
      "Gradient Descent(27/49): loss=5.848095749310523e+58\n",
      "Gradient Descent(28/49): loss=1.225028453304626e+61\n",
      "Gradient Descent(29/49): loss=2.5661254119904116e+63\n",
      "Gradient Descent(30/49): loss=5.37538504701617e+65\n",
      "Gradient Descent(31/49): loss=1.1260074924114003e+68\n",
      "Gradient Descent(32/49): loss=2.3587014918502044e+70\n",
      "Gradient Descent(33/49): loss=4.9408842882048535e+72\n",
      "Gradient Descent(34/49): loss=1.0349905502573825e+75\n",
      "Gradient Descent(35/49): loss=2.1680439707509893e+77\n",
      "Gradient Descent(36/49): loss=4.5415049035382167e+79\n",
      "Gradient Descent(37/49): loss=9.513306495217007e+81\n",
      "Gradient Descent(38/49): loss=1.9927975945028974e+84\n",
      "Gradient Descent(39/49): loss=4.174407977555522e+86\n",
      "Gradient Descent(40/49): loss=8.744331090697589e+88\n",
      "Gradient Descent(41/49): loss=1.831716656226683e+91\n",
      "Gradient Descent(42/49): loss=3.8369840687615604e+93\n",
      "Gradient Descent(43/49): loss=8.037513167706869e+95\n",
      "Gradient Descent(44/49): loss=1.683656141473442e+98\n",
      "Gradient Descent(45/49): loss=3.526834660888075e+100\n",
      "Gradient Descent(46/49): loss=7.387828440049299e+102\n",
      "Gradient Descent(47/49): loss=1.5475635890982686e+105\n",
      "Gradient Descent(48/49): loss=3.2417551134779614e+107\n",
      "Gradient Descent(49/49): loss=6.79065874241964e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.505029801329472\n",
      "Gradient Descent(2/49): loss=202.8621424169539\n",
      "Gradient Descent(3/49): loss=10934.313006145645\n",
      "Gradient Descent(4/49): loss=718916.8979539757\n",
      "Gradient Descent(5/49): loss=71174632.25544088\n",
      "Gradient Descent(6/49): loss=10883716518.831955\n",
      "Gradient Descent(7/49): loss=2075929770169.748\n",
      "Gradient Descent(8/49): loss=424582836955315.0\n",
      "Gradient Descent(9/49): loss=8.843494575690544e+16\n",
      "Gradient Descent(10/49): loss=1.8502861115766686e+19\n",
      "Gradient Descent(11/49): loss=3.8755151252342456e+21\n",
      "Gradient Descent(12/49): loss=8.119613538248464e+23\n",
      "Gradient Descent(13/49): loss=1.701254350847953e+26\n",
      "Gradient Descent(14/49): loss=3.56459272648672e+28\n",
      "Gradient Descent(15/49): loss=7.46882402124443e+30\n",
      "Gradient Descent(16/49): loss=1.5649300682458054e+33\n",
      "Gradient Descent(17/49): loss=3.2789722391024718e+35\n",
      "Gradient Descent(18/49): loss=6.870376984676161e+37\n",
      "Gradient Descent(19/49): loss=1.4395388885685214e+40\n",
      "Gradient Descent(20/49): loss=3.0162423734146043e+42\n",
      "Gradient Descent(21/49): loss=6.319883494913637e+44\n",
      "Gradient Descent(22/49): loss=1.324194890594313e+47\n",
      "Gradient Descent(23/49): loss=2.7745639769565556e+49\n",
      "Gradient Descent(24/49): loss=5.813498690403266e+51\n",
      "Gradient Descent(25/49): loss=1.2180929077181465e+54\n",
      "Gradient Descent(26/49): loss=2.552250221167983e+56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=5.347688300439983e+58\n",
      "Gradient Descent(28/49): loss=1.1204924157309219e+61\n",
      "Gradient Descent(29/49): loss=2.3477495006715846e+63\n",
      "Gradient Descent(30/49): loss=4.919201273047624e+65\n",
      "Gradient Descent(31/49): loss=1.0307122270851821e+68\n",
      "Gradient Descent(32/49): loss=2.159634534337146e+70\n",
      "Gradient Descent(33/49): loss=4.525047049350923e+72\n",
      "Gradient Descent(34/49): loss=9.48125734853727e+74\n",
      "Gradient Descent(35/49): loss=1.9865924028809467e+77\n",
      "Gradient Descent(36/49): loss=4.162474690968226e+79\n",
      "Gradient Descent(37/49): loss=8.721565394000728e+81\n",
      "Gradient Descent(38/49): loss=1.8274153855368306e+84\n",
      "Gradient Descent(39/49): loss=3.8289536802576455e+86\n",
      "Gradient Descent(40/49): loss=8.022744254860285e+88\n",
      "Gradient Descent(41/49): loss=1.68099253095593e+91\n",
      "Gradient Descent(42/49): loss=3.522156258960658e+93\n",
      "Gradient Descent(43/49): loss=7.37991661716713e+95\n",
      "Gradient Descent(44/49): loss=1.5463019034939412e+98\n",
      "Gradient Descent(45/49): loss=3.239941181973427e+100\n",
      "Gradient Descent(46/49): loss=6.788595964946142e+102\n",
      "Gradient Descent(47/49): loss=1.4224034507692071e+105\n",
      "Gradient Descent(48/49): loss=2.98033877285875e+107\n",
      "Gradient Descent(49/49): loss=6.24465526725324e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.342915833835537\n",
      "Gradient Descent(2/49): loss=192.54570636011752\n",
      "Gradient Descent(3/49): loss=10109.271403687224\n",
      "Gradient Descent(4/49): loss=619520.4343214063\n",
      "Gradient Descent(5/49): loss=54586887.69069606\n",
      "Gradient Descent(6/49): loss=7701123431.9197\n",
      "Gradient Descent(7/49): loss=1439077146634.7988\n",
      "Gradient Descent(8/49): loss=295809302152864.4\n",
      "Gradient Descent(9/49): loss=6.2354424004431576e+16\n",
      "Gradient Descent(10/49): loss=1.3225006091804443e+19\n",
      "Gradient Descent(11/49): loss=2.8090909735829766e+21\n",
      "Gradient Descent(12/49): loss=5.968825192980854e+23\n",
      "Gradient Descent(13/49): loss=1.2683772370106067e+26\n",
      "Gradient Descent(14/49): loss=2.6953596802847013e+28\n",
      "Gradient Descent(15/49): loss=5.727790068689352e+30\n",
      "Gradient Descent(16/49): loss=1.217188811961379e+33\n",
      "Gradient Descent(17/49): loss=2.586597952923486e+35\n",
      "Gradient Descent(18/49): loss=5.496673431341454e+37\n",
      "Gradient Descent(19/49): loss=1.1680755891531504e+40\n",
      "Gradient Descent(20/49): loss=2.482229668624819e+42\n",
      "Gradient Descent(21/49): loss=5.274884768099578e+44\n",
      "Gradient Descent(22/49): loss=1.1209441928059502e+47\n",
      "Gradient Descent(23/49): loss=2.3820726682922445e+49\n",
      "Gradient Descent(24/49): loss=5.062045223579573e+51\n",
      "Gradient Descent(25/49): loss=1.0757145315808953e+54\n",
      "Gradient Descent(26/49): loss=2.285956964716417e+56\n",
      "Gradient Descent(27/49): loss=4.857793672134021e+58\n",
      "Gradient Descent(28/49): loss=1.0323098695759315e+61\n",
      "Gradient Descent(29/49): loss=2.1937194923220404e+63\n",
      "Gradient Descent(30/49): loss=4.661783591171767e+65\n",
      "Gradient Descent(31/49): loss=9.906565687628005e+67\n",
      "Gradient Descent(32/49): loss=2.10520376598221e+70\n",
      "Gradient Descent(33/49): loss=4.473682440566047e+72\n",
      "Gradient Descent(34/49): loss=9.506839623997946e+74\n",
      "Gradient Descent(35/49): loss=2.0202596146941355e+77\n",
      "Gradient Descent(36/49): loss=4.2931710980600643e+79\n",
      "Gradient Descent(37/49): loss=9.123242351210742e+81\n",
      "Gradient Descent(38/49): loss=1.9387429267968314e+84\n",
      "Gradient Descent(39/49): loss=4.119943317855588e+86\n",
      "Gradient Descent(40/49): loss=8.755123078843193e+88\n",
      "Gradient Descent(41/49): loss=1.8605154054786437e+91\n",
      "Gradient Descent(42/49): loss=3.953705211053183e+93\n",
      "Gradient Descent(43/49): loss=8.401857275611907e+95\n",
      "Gradient Descent(44/49): loss=1.7854443341503098e+98\n",
      "Gradient Descent(45/49): loss=3.79417474705583e+100\n",
      "Gradient Descent(46/49): loss=8.062845609828144e+102\n",
      "Gradient Descent(47/49): loss=1.7134023512851936e+105\n",
      "Gradient Descent(48/49): loss=3.641081275091122e+107\n",
      "Gradient Descent(49/49): loss=7.737512932601815e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.419158120146336\n",
      "Gradient Descent(2/49): loss=199.2884531366933\n",
      "Gradient Descent(3/49): loss=10979.58317422209\n",
      "Gradient Descent(4/49): loss=776225.1810714809\n",
      "Gradient Descent(5/49): loss=87176910.6342638\n",
      "Gradient Descent(6/49): loss=14790537268.731493\n",
      "Gradient Descent(7/49): loss=2999802310332.574\n",
      "Gradient Descent(8/49): loss=640309194335746.9\n",
      "Gradient Descent(9/49): loss=1.3840997129051062e+17\n",
      "Gradient Descent(10/49): loss=3.0008627730534105e+19\n",
      "Gradient Descent(11/49): loss=6.510747182859545e+21\n",
      "Gradient Descent(12/49): loss=1.412821519726137e+24\n",
      "Gradient Descent(13/49): loss=3.065918460380485e+26\n",
      "Gradient Descent(14/49): loss=6.653311316491933e+28\n",
      "Gradient Descent(15/49): loss=1.4438298451766423e+31\n",
      "Gradient Descent(16/49): loss=3.133245332466011e+33\n",
      "Gradient Descent(17/49): loss=6.79943519348294e+35\n",
      "Gradient Descent(18/49): loss=1.475541022599916e+38\n",
      "Gradient Descent(19/49): loss=3.2020620263737254e+40\n",
      "Gradient Descent(20/49): loss=6.948774096530939e+42\n",
      "Gradient Descent(21/49): loss=1.5079489736299215e+45\n",
      "Gradient Descent(22/49): loss=3.2723903175888454e+47\n",
      "Gradient Descent(23/49): loss=7.101393069740366e+49\n",
      "Gradient Descent(24/49): loss=1.5410687184877898e+52\n",
      "Gradient Descent(25/49): loss=3.344263261843384e+54\n",
      "Gradient Descent(26/49): loss=7.257364081397298e+56\n",
      "Gradient Descent(27/49): loss=1.5749158868828509e+59\n",
      "Gradient Descent(28/49): loss=3.4177147831317254e+61\n",
      "Gradient Descent(29/49): loss=7.416760752827341e+63\n",
      "Gradient Descent(30/49): loss=1.6095064554882165e+66\n",
      "Gradient Descent(31/49): loss=3.4927795524086767e+68\n",
      "Gradient Descent(32/49): loss=7.579658323285998e+70\n",
      "Gradient Descent(33/49): loss=1.644856751928124e+73\n",
      "Gradient Descent(34/49): loss=3.569493002147158e+75\n",
      "Gradient Descent(35/49): loss=7.746133684554486e+77\n",
      "Gradient Descent(36/49): loss=1.6809834624384027e+80\n",
      "Gradient Descent(37/49): loss=3.647891343039664e+82\n",
      "Gradient Descent(38/49): loss=7.91626541722229e+84\n",
      "Gradient Descent(39/49): loss=1.7179036397419807e+87\n",
      "Gradient Descent(40/49): loss=3.7280115810900347e+89\n",
      "Gradient Descent(41/49): loss=8.09013382777884e+91\n",
      "Gradient Descent(42/49): loss=1.755634711098043e+94\n",
      "Gradient Descent(43/49): loss=3.8098915350806743e+96\n",
      "Gradient Descent(44/49): loss=8.267820986519859e+98\n",
      "Gradient Descent(45/49): loss=1.7941944865286443e+101\n",
      "Gradient Descent(46/49): loss=3.893569854425098e+103\n",
      "Gradient Descent(47/49): loss=8.449410766287263e+105\n",
      "Gradient Descent(48/49): loss=1.833601167224716e+108\n",
      "Gradient Descent(49/49): loss=3.979086037410378e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.456210011958577\n",
      "Gradient Descent(2/49): loss=203.2459322155936\n",
      "Gradient Descent(3/49): loss=11179.534201010467\n",
      "Gradient Descent(4/49): loss=760961.9670994995\n",
      "Gradient Descent(5/49): loss=78852960.88216032\n",
      "Gradient Descent(6/49): loss=12453615013.97814\n",
      "Gradient Descent(7/49): loss=2414907658575.436\n",
      "Gradient Descent(8/49): loss=499074249039140.94\n",
      "Gradient Descent(9/49): loss=1.0486501553991971e+17\n",
      "Gradient Descent(10/49): loss=2.2124750687488295e+19\n",
      "Gradient Descent(11/49): loss=4.6726327060495325e+21\n",
      "Gradient Descent(12/49): loss=9.870768806312432e+23\n",
      "Gradient Descent(13/49): loss=2.0852884274076736e+26\n",
      "Gradient Descent(14/49): loss=4.405422531481223e+28\n",
      "Gradient Descent(15/49): loss=9.307017552796635e+30\n",
      "Gradient Descent(16/49): loss=1.9662279679429395e+33\n",
      "Gradient Descent(17/49): loss=4.15391204080973e+35\n",
      "Gradient Descent(18/49): loss=8.7756793205833e+37\n",
      "Gradient Descent(19/49): loss=1.8539763897604604e+40\n",
      "Gradient Descent(20/49): loss=3.916766256542506e+42\n",
      "Gradient Descent(21/49): loss=8.274678148182864e+44\n",
      "Gradient Descent(22/49): loss=1.748133383089064e+47\n",
      "Gradient Descent(23/49): loss=3.6931591422365503e+49\n",
      "Gradient Descent(24/49): loss=7.802279037728401e+51\n",
      "Gradient Descent(25/49): loss=1.6483329268574554e+54\n",
      "Gradient Descent(26/49): loss=3.4823176979776553e+56\n",
      "Gradient Descent(27/49): loss=7.356849063720162e+58\n",
      "Gradient Descent(28/49): loss=1.5542300513762167e+61\n",
      "Gradient Descent(29/49): loss=3.2835131340582668e+63\n",
      "Gradient Descent(30/49): loss=6.936848564977101e+65\n",
      "Gradient Descent(31/49): loss=1.4654994832912738e+68\n",
      "Gradient Descent(32/49): loss=3.096058268260715e+70\n",
      "Gradient Descent(33/49): loss=6.5408257797115105e+72\n",
      "Gradient Descent(34/49): loss=1.3818345190438675e+75\n",
      "Gradient Descent(35/49): loss=2.91930514942623e+77\n",
      "Gradient Descent(36/49): loss=6.167411826825405e+79\n",
      "Gradient Descent(37/49): loss=1.3029459646978406e+82\n",
      "Gradient Descent(38/49): loss=2.7526428177511025e+84\n",
      "Gradient Descent(39/49): loss=5.815315974268917e+86\n",
      "Gradient Descent(40/49): loss=1.2285611363197738e+89\n",
      "Gradient Descent(41/49): loss=2.59549519295911e+91\n",
      "Gradient Descent(42/49): loss=5.483321177531144e+93\n",
      "Gradient Descent(43/49): loss=1.1584229174272697e+96\n",
      "Gradient Descent(44/49): loss=2.4473190830395656e+98\n",
      "Gradient Descent(45/49): loss=5.170279872838975e+100\n",
      "Gradient Descent(46/49): loss=1.0922888702474532e+103\n",
      "Gradient Descent(47/49): loss=2.307602306664619e+105\n",
      "Gradient Descent(48/49): loss=4.875110010520814e+107\n",
      "Gradient Descent(49/49): loss=1.0299303977136644e+110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.543095754060085\n",
      "Gradient Descent(2/49): loss=206.57005703242373\n",
      "Gradient Descent(3/49): loss=11234.112163406664\n",
      "Gradient Descent(4/49): loss=744956.597051292\n",
      "Gradient Descent(5/49): loss=74339946.8942561\n",
      "Gradient Descent(6/49): loss=11457825608.65909\n",
      "Gradient Descent(7/49): loss=2203479297322.668\n",
      "Gradient Descent(8/49): loss=454476699799402.6\n",
      "Gradient Descent(9/49): loss=9.546708314580038e+16\n",
      "Gradient Descent(10/49): loss=2.0144536000172364e+19\n",
      "Gradient Descent(11/49): loss=4.255383491406588e+21\n",
      "Gradient Descent(12/49): loss=8.991582384077145e+23\n",
      "Gradient Descent(13/49): loss=1.9000353561535277e+26\n",
      "Gradient Descent(14/49): loss=4.015078665154958e+28\n",
      "Gradient Descent(15/49): loss=8.48453584443799e+30\n",
      "Gradient Descent(16/49): loss=1.792926639242565e+33\n",
      "Gradient Descent(17/49): loss=3.788759588555316e+35\n",
      "Gradient Descent(18/49): loss=8.006295229848643e+37\n",
      "Gradient Descent(19/49): loss=1.6918667614854893e+40\n",
      "Gradient Descent(20/49): loss=3.575203094041431e+42\n",
      "Gradient Descent(21/49): loss=7.555014061694584e+44\n",
      "Gradient Descent(22/49): loss=1.5965033588788642e+47\n",
      "Gradient Descent(23/49): loss=3.373683959041939e+49\n",
      "Gradient Descent(24/49): loss=7.129169752336308e+51\n",
      "Gradient Descent(25/49): loss=1.5065151915473696e+54\n",
      "Gradient Descent(26/49): loss=3.1835236096330736e+56\n",
      "Gradient Descent(27/49): loss=6.72732849290614e+58\n",
      "Gradient Descent(28/49): loss=1.4215992780617816e+61\n",
      "Gradient Descent(29/49): loss=3.004081797873828e+63\n",
      "Gradient Descent(30/49): loss=6.348137332076555e+65\n",
      "Gradient Descent(31/49): loss=1.341469716817493e+68\n",
      "Gradient Descent(32/49): loss=2.834754364946522e+70\n",
      "Gradient Descent(33/49): loss=5.9903195792206695e+72\n",
      "Gradient Descent(34/49): loss=1.26585672130611e+75\n",
      "Gradient Descent(35/49): loss=2.674971205934139e+77\n",
      "Gradient Descent(36/49): loss=5.6526705053900606e+79\n",
      "Gradient Descent(37/49): loss=1.1945057117483333e+82\n",
      "Gradient Descent(38/49): loss=2.5241943503319513e+84\n",
      "Gradient Descent(39/49): loss=5.33405328713078e+86\n",
      "Gradient Descent(40/49): loss=1.1271764579541906e+89\n",
      "Gradient Descent(41/49): loss=2.3819161507656896e+91\n",
      "Gradient Descent(42/49): loss=5.033395178937429e+93\n",
      "Gradient Descent(43/49): loss=1.0636422704974764e+96\n",
      "Gradient Descent(44/49): loss=2.2476575738046456e+98\n",
      "Gradient Descent(45/49): loss=4.7496839014477805e+100\n",
      "Gradient Descent(46/49): loss=1.0036892374796168e+103\n",
      "Gradient Descent(47/49): loss=2.120966587114023e+105\n",
      "Gradient Descent(48/49): loss=4.4819642332226304e+107\n",
      "Gradient Descent(49/49): loss=9.471155043144971e+109\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.379494189655601\n",
      "Gradient Descent(2/49): loss=196.06747529087062\n",
      "Gradient Descent(3/49): loss=10386.996951885556\n",
      "Gradient Descent(4/49): loss=642065.5839991738\n",
      "Gradient Descent(5/49): loss=57027715.71865341\n",
      "Gradient Descent(6/49): loss=8108307683.303304\n",
      "Gradient Descent(7/49): loss=1527527521696.572\n",
      "Gradient Descent(8/49): loss=316630870108808.9\n",
      "Gradient Descent(9/49): loss=6.7310592296093656e+16\n",
      "Gradient Descent(10/49): loss=1.4397858286843515e+19\n",
      "Gradient Descent(11/49): loss=3.084301001988725e+21\n",
      "Gradient Descent(12/49): loss=6.609514626554767e+23\n",
      "Gradient Descent(13/49): loss=1.4165082922569008e+26\n",
      "Gradient Descent(14/49): loss=3.035829941635927e+28\n",
      "Gradient Descent(15/49): loss=6.506356413133395e+30\n",
      "Gradient Descent(16/49): loss=1.3944365532955383e+33\n",
      "Gradient Descent(17/49): loss=2.9885449098498723e+35\n",
      "Gradient Descent(18/49): loss=6.405025198861028e+37\n",
      "Gradient Descent(19/49): loss=1.37271982437469e+40\n",
      "Gradient Descent(20/49): loss=2.9420020448586844e+42\n",
      "Gradient Descent(21/49): loss=6.305275035696258e+44\n",
      "Gradient Descent(22/49): loss=1.3513414565319308e+47\n",
      "Gradient Descent(23/49): loss=2.896184102827864e+49\n",
      "Gradient Descent(24/49): loss=6.207078393865385e+51\n",
      "Gradient Descent(25/49): loss=1.3302960315983348e+54\n",
      "Gradient Descent(26/49): loss=2.851079718014397e+56\n",
      "Gradient Descent(27/49): loss=6.110411040396642e+58\n",
      "Gradient Descent(28/49): loss=1.3095783624249654e+61\n",
      "Gradient Descent(29/49): loss=2.8066777766562936e+63\n",
      "Gradient Descent(30/49): loss=6.015249158049324e+65\n",
      "Gradient Descent(31/49): loss=1.2891833446060614e+68\n",
      "Gradient Descent(32/49): loss=2.762967339076341e+70\n",
      "Gradient Descent(33/49): loss=5.921569301017844e+72\n",
      "Gradient Descent(34/49): loss=1.2691059532567025e+75\n",
      "Gradient Descent(35/49): loss=2.7199376359823257e+77\n",
      "Gradient Descent(36/49): loss=5.8293483886421994e+79\n",
      "Gradient Descent(37/49): loss=1.2493412417485029e+82\n",
      "Gradient Descent(38/49): loss=2.6775780658003215e+84\n",
      "Gradient Descent(39/49): loss=5.7385636997151e+86\n",
      "Gradient Descent(40/49): loss=1.2298843404905259e+89\n",
      "Gradient Descent(41/49): loss=2.6358781920621217e+91\n",
      "Gradient Descent(42/49): loss=5.649192866881722e+93\n",
      "Gradient Descent(43/49): loss=1.2107304557294909e+96\n",
      "Gradient Descent(44/49): loss=2.5948277408344986e+98\n",
      "Gradient Descent(45/49): loss=5.561213871131556e+100\n",
      "Gradient Descent(46/49): loss=1.1918748683687378e+103\n",
      "Gradient Descent(47/49): loss=2.5544165981875953e+105\n",
      "Gradient Descent(48/49): loss=5.474605036371826e+107\n",
      "Gradient Descent(49/49): loss=1.1733129328055533e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.456267461339844\n",
      "Gradient Descent(2/49): loss=202.91505323147712\n",
      "Gradient Descent(3/49): loss=11278.887568045331\n",
      "Gradient Descent(4/49): loss=804094.7578671589\n",
      "Gradient Descent(5/49): loss=91016617.71707274\n",
      "Gradient Descent(6/49): loss=15565005007.214174\n",
      "Gradient Descent(7/49): loss=3183030519305.8926\n",
      "Gradient Descent(8/49): loss=685146995202921.5\n",
      "Gradient Descent(9/49): loss=1.4935745729145325e+17\n",
      "Gradient Descent(10/49): loss=3.265702827472528e+19\n",
      "Gradient Descent(11/49): loss=7.145519681211868e+21\n",
      "Gradient Descent(12/49): loss=1.5637349291792024e+24\n",
      "Gradient Descent(13/49): loss=3.422231497016105e+26\n",
      "Gradient Descent(14/49): loss=7.489616877024073e+28\n",
      "Gradient Descent(15/49): loss=1.6391200110363586e+31\n",
      "Gradient Descent(16/49): loss=3.587253936483225e+33\n",
      "Gradient Descent(17/49): loss=7.85079325313159e+35\n",
      "Gradient Descent(18/49): loss=1.718165412911014e+38\n",
      "Gradient Descent(19/49): loss=3.7602472898129774e+40\n",
      "Gradient Descent(20/49): loss=8.229393745117635e+42\n",
      "Gradient Descent(21/49): loss=1.8010230768560256e+45\n",
      "Gradient Descent(22/49): loss=3.941583334208826e+47\n",
      "Gradient Descent(23/49): loss=8.626252145496717e+49\n",
      "Gradient Descent(24/49): loss=1.8878765148058585e+52\n",
      "Gradient Descent(25/49): loss=4.131664221083544e+54\n",
      "Gradient Descent(26/49): loss=9.0422488451484e+56\n",
      "Gradient Descent(27/49): loss=1.978918416466795e+59\n",
      "Gradient Descent(28/49): loss=4.330911663786771e+61\n",
      "Gradient Descent(29/49): loss=9.47830677780729e+63\n",
      "Gradient Descent(30/49): loss=2.0743507683478004e+66\n",
      "Gradient Descent(31/49): loss=4.539767714862467e+68\n",
      "Gradient Descent(32/49): loss=9.935393386395563e+70\n",
      "Gradient Descent(33/49): loss=2.1743852977161788e+73\n",
      "Gradient Descent(34/49): loss=4.758695744647756e+75\n",
      "Gradient Descent(35/49): loss=1.0414522768302968e+78\n",
      "Gradient Descent(36/49): loss=2.279243942281678e+80\n",
      "Gradient Descent(37/49): loss=4.9881814692833586e+82\n",
      "Gradient Descent(38/49): loss=1.091675792525861e+85\n",
      "Gradient Descent(39/49): loss=2.389159342589399e+87\n",
      "Gradient Descent(40/49): loss=5.228734028328352e+89\n",
      "Gradient Descent(41/49): loss=1.1443213121719862e+92\n",
      "Gradient Descent(42/49): loss=2.5043753581585106e+94\n",
      "Gradient Descent(43/49): loss=5.480887114343103e+96\n",
      "Gradient Descent(44/49): loss=1.199505635699121e+99\n",
      "Gradient Descent(45/49): loss=2.6251476085115988e+101\n",
      "Gradient Descent(46/49): loss=5.7452001569443116e+103\n",
      "Gradient Descent(47/49): loss=1.2573511956559016e+106\n",
      "Gradient Descent(48/49): loss=2.751744040294912e+108\n",
      "Gradient Descent(49/49): loss=6.022259563962764e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.493643893097602\n",
      "Gradient Descent(2/49): loss=206.94257533175576\n",
      "Gradient Descent(3/49): loss=11484.414976037619\n",
      "Gradient Descent(4/49): loss=788356.0527318432\n",
      "Gradient Descent(5/49): loss=82338669.17980626\n",
      "Gradient Descent(6/49): loss=13107215232.741411\n",
      "Gradient Descent(7/49): loss=2562609183326.9204\n",
      "Gradient Descent(8/49): loss=534057620767205.1\n",
      "Gradient Descent(9/49): loss=1.131669536777094e+17\n",
      "Gradient Descent(10/49): loss=2.4079126125907804e+19\n",
      "Gradient Descent(11/49): loss=5.128606172946477e+21\n",
      "Gradient Descent(12/49): loss=1.0926085935906648e+24\n",
      "Gradient Descent(13/49): loss=2.3278545477807337e+26\n",
      "Gradient Descent(14/49): loss=4.959676975411226e+28\n",
      "Gradient Descent(15/49): loss=1.0567018850155457e+31\n",
      "Gradient Descent(16/49): loss=2.2513962963611218e+33\n",
      "Gradient Descent(17/49): loss=4.796798813415225e+35\n",
      "Gradient Descent(18/49): loss=1.0220004390378304e+38\n",
      "Gradient Descent(19/49): loss=2.1774624098875084e+40\n",
      "Gradient Descent(20/49): loss=4.63927644229956e+42\n",
      "Gradient Descent(21/49): loss=9.884389198284794e+44\n",
      "Gradient Descent(22/49): loss=2.105956630433047e+47\n",
      "Gradient Descent(23/49): loss=4.48692705284059e+49\n",
      "Gradient Descent(24/49): loss=9.55979533803944e+51\n",
      "Gradient Descent(25/49): loss=2.0367990348223074e+54\n",
      "Gradient Descent(26/49): loss=4.339580672556974e+56\n",
      "Gradient Descent(27/49): loss=9.245860829503067e+58\n",
      "Gradient Descent(28/49): loss=1.9699125083477474e+61\n",
      "Gradient Descent(29/49): loss=4.197073006076704e+63\n",
      "Gradient Descent(30/49): loss=8.942235629090516e+65\n",
      "Gradient Descent(31/49): loss=1.9052224712412823e+68\n",
      "Gradient Descent(32/49): loss=4.059245154661429e+70\n",
      "Gradient Descent(33/49): loss=8.648581189002733e+72\n",
      "Gradient Descent(34/49): loss=1.8426567928984845e+75\n",
      "Gradient Descent(35/49): loss=3.925943437673268e+77\n",
      "Gradient Descent(36/49): loss=8.364570079035074e+79\n",
      "Gradient Descent(37/49): loss=1.782145711415422e+82\n",
      "Gradient Descent(38/49): loss=3.7970192211992876e+84\n",
      "Gradient Descent(39/49): loss=8.089885621477183e+86\n",
      "Gradient Descent(40/49): loss=1.7236217558021957e+89\n",
      "Gradient Descent(41/49): loss=3.672328752321921e+91\n",
      "Gradient Descent(42/49): loss=7.824221538010024e+93\n",
      "Gradient Descent(43/49): loss=1.6670196707512441e+96\n",
      "Gradient Descent(44/49): loss=3.5517329988312654e+98\n",
      "Gradient Descent(45/49): loss=7.567281608201932e+100\n",
      "Gradient Descent(46/49): loss=1.6122763438770753e+103\n",
      "Gradient Descent(47/49): loss=3.4350974942020956e+105\n",
      "Gradient Descent(48/49): loss=7.318779339215575e+107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=1.559330735344106e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.581337180272459\n",
      "Gradient Descent(2/49): loss=210.3286814491978\n",
      "Gradient Descent(3/49): loss=11540.73326016786\n",
      "Gradient Descent(4/49): loss=771818.6039736978\n",
      "Gradient Descent(5/49): loss=77631522.31147206\n",
      "Gradient Descent(6/49): loss=12059587150.875807\n",
      "Gradient Descent(7/49): loss=2338275361391.3027\n",
      "Gradient Descent(8/49): loss=486335250996822.94\n",
      "Gradient Descent(9/49): loss=1.0302503203825634e+17\n",
      "Gradient Descent(10/49): loss=2.1923980172476633e+19\n",
      "Gradient Descent(11/49): loss=4.670636873830541e+21\n",
      "Gradient Descent(12/49): loss=9.952894573879898e+23\n",
      "Gradient Descent(13/49): loss=2.1210504524638527e+26\n",
      "Gradient Descent(14/49): loss=4.520218820763785e+28\n",
      "Gradient Descent(15/49): loss=9.633177995850995e+30\n",
      "Gradient Descent(16/49): loss=2.0529582376450246e+33\n",
      "Gradient Descent(17/49): loss=4.375127791349847e+35\n",
      "Gradient Descent(18/49): loss=9.323981310430986e+37\n",
      "Gradient Descent(19/49): loss=1.9870648990911358e+40\n",
      "Gradient Descent(20/49): loss=4.234700601029388e+42\n",
      "Gradient Descent(21/49): loss=9.024712379844211e+44\n",
      "Gradient Descent(22/49): loss=1.923286702596356e+47\n",
      "Gradient Descent(23/49): loss=4.0987807531855557e+49\n",
      "Gradient Descent(24/49): loss=8.735049038811076e+51\n",
      "Gradient Descent(25/49): loss=1.8615555772613193e+54\n",
      "Gradient Descent(26/49): loss=3.967223483048319e+56\n",
      "Gradient Descent(27/49): loss=8.454682931146863e+58\n",
      "Gradient Descent(28/49): loss=1.8018058163767455e+61\n",
      "Gradient Descent(29/49): loss=3.839888765040619e+63\n",
      "Gradient Descent(30/49): loss=8.183315645820024e+65\n",
      "Gradient Descent(31/49): loss=1.743973824679645e+68\n",
      "Gradient Descent(32/49): loss=3.716641069223947e+70\n",
      "Gradient Descent(33/49): loss=7.920658350465384e+72\n",
      "Gradient Descent(34/49): loss=1.6879980481380778e+75\n",
      "Gradient Descent(35/49): loss=3.59734921573855e+77\n",
      "Gradient Descent(36/49): loss=7.666431483287924e+79\n",
      "Gradient Descent(37/49): loss=1.6338189084009416e+82\n",
      "Gradient Descent(38/49): loss=3.481886235163537e+84\n",
      "Gradient Descent(39/49): loss=7.42036445549955e+86\n",
      "Gradient Descent(40/49): loss=1.5813787393847528e+89\n",
      "Gradient Descent(41/49): loss=3.370129233375659e+91\n",
      "Gradient Descent(42/49): loss=7.182195363314555e+93\n",
      "Gradient Descent(43/49): loss=1.5306217258959244e+96\n",
      "Gradient Descent(44/49): loss=3.2619592607452746e+98\n",
      "Gradient Descent(45/49): loss=6.9516707091910025e+100\n",
      "Gradient Descent(46/49): loss=1.4814938442235889e+103\n",
      "Gradient Descent(47/49): loss=3.157261185531399e+105\n",
      "Gradient Descent(48/49): loss=6.7285451320166e+107\n",
      "Gradient Descent(49/49): loss=1.4339428046389013e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.416241338604008\n",
      "Gradient Descent(2/49): loss=199.63744590473215\n",
      "Gradient Descent(3/49): loss=10671.054045404702\n",
      "Gradient Descent(4/49): loss=665325.9310688578\n",
      "Gradient Descent(5/49): loss=59566441.27992473\n",
      "Gradient Descent(6/49): loss=8535152965.249647\n",
      "Gradient Descent(7/49): loss=1621005059130.5513\n",
      "Gradient Descent(8/49): loss=338820524437085.4\n",
      "Gradient Descent(9/49): loss=7.26371907401228e+16\n",
      "Gradient Descent(10/49): loss=1.5669090949558407e+19\n",
      "Gradient Descent(11/49): loss=3.3851349907307275e+21\n",
      "Gradient Descent(12/49): loss=7.315819011300406e+23\n",
      "Gradient Descent(13/49): loss=1.581200233665275e+26\n",
      "Gradient Descent(14/49): loss=3.417587235887593e+28\n",
      "Gradient Descent(15/49): loss=7.386767912544073e+30\n",
      "Gradient Descent(16/49): loss=1.596576753748237e+33\n",
      "Gradient Descent(17/49): loss=3.450843542152053e+35\n",
      "Gradient Descent(18/49): loss=7.458659228092828e+37\n",
      "Gradient Descent(19/49): loss=1.612115926930812e+40\n",
      "Gradient Descent(20/49): loss=3.48443023357339e+42\n",
      "Gradient Descent(21/49): loss=7.53125371488863e+44\n",
      "Gradient Descent(22/49): loss=1.6278065200920307e+47\n",
      "Gradient Descent(23/49): loss=3.518343913645162e+49\n",
      "Gradient Descent(24/49): loss=7.604554805527446e+51\n",
      "Gradient Descent(25/49): loss=1.6436498309946637e+54\n",
      "Gradient Descent(26/49): loss=3.5525876741226047e+56\n",
      "Gradient Descent(27/49): loss=7.678569330483475e+58\n",
      "Gradient Descent(28/49): loss=1.659647343611461e+61\n",
      "Gradient Descent(29/49): loss=3.587164726405091e+63\n",
      "Gradient Descent(30/49): loss=7.753304232912638e+65\n",
      "Gradient Descent(31/49): loss=1.6758005587422302e+68\n",
      "Gradient Descent(32/49): loss=3.622078314377962e+70\n",
      "Gradient Descent(33/49): loss=7.82876652418208e+72\n",
      "Gradient Descent(34/49): loss=1.6921109918265269e+75\n",
      "Gradient Descent(35/49): loss=3.657331713515751e+77\n",
      "Gradient Descent(36/49): loss=7.904963283909283e+79\n",
      "Gradient Descent(37/49): loss=1.7085801730541845e+82\n",
      "Gradient Descent(38/49): loss=3.6929282311735614e+84\n",
      "Gradient Descent(39/49): loss=7.981901660617299e+86\n",
      "Gradient Descent(40/49): loss=1.7252096475082982e+89\n",
      "Gradient Descent(41/49): loss=3.7288712068967375e+91\n",
      "Gradient Descent(42/49): loss=8.059588872405055e+93\n",
      "Gradient Descent(43/49): loss=1.7420009753100107e+96\n",
      "Gradient Descent(44/49): loss=3.765164012733903e+98\n",
      "Gradient Descent(45/49): loss=8.138032207624781e+100\n",
      "Gradient Descent(46/49): loss=1.7589557317649906e+103\n",
      "Gradient Descent(47/49): loss=3.8018100535535134e+105\n",
      "Gradient Descent(48/49): loss=8.217239025564851e+107\n",
      "Gradient Descent(49/49): loss=1.7760755075112825e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.493547214599997\n",
      "Gradient Descent(2/49): loss=206.5910378563496\n",
      "Gradient Descent(3/49): loss=11584.964218891679\n",
      "Gradient Descent(4/49): loss=832836.8794807398\n",
      "Gradient Descent(5/49): loss=95008009.05696617\n",
      "Gradient Descent(6/49): loss=16376499220.785961\n",
      "Gradient Descent(7/49): loss=3376607068551.467\n",
      "Gradient Descent(8/49): loss=732915662088660.8\n",
      "Gradient Descent(9/49): loss=1.6111917755130816e+17\n",
      "Gradient Descent(10/49): loss=3.552650800696235e+19\n",
      "Gradient Descent(11/49): loss=7.839108511155034e+21\n",
      "Gradient Descent(12/49): loss=1.7300289678240813e+24\n",
      "Gradient Descent(13/49): loss=3.8181861988902064e+26\n",
      "Gradient Descent(14/49): loss=8.426841571470467e+28\n",
      "Gradient Descent(15/49): loss=1.859831044638185e+31\n",
      "Gradient Descent(16/49): loss=4.104709028885386e+33\n",
      "Gradient Descent(17/49): loss=9.059230549659715e+35\n",
      "Gradient Descent(18/49): loss=1.9994026340615636e+38\n",
      "Gradient Descent(19/49): loss=4.412748850340793e+40\n",
      "Gradient Descent(20/49): loss=9.739085121933561e+42\n",
      "Gradient Descent(21/49): loss=2.1494488416034715e+45\n",
      "Gradient Descent(22/49): loss=4.743905885632943e+47\n",
      "Gradient Descent(23/49): loss=1.0469959841147965e+50\n",
      "Gradient Descent(24/49): loss=2.3107553505172125e+52\n",
      "Gradient Descent(25/49): loss=5.0999147761433046e+54\n",
      "Gradient Descent(26/49): loss=1.1255683436201672e+57\n",
      "Gradient Descent(27/49): loss=2.4841671905702427e+59\n",
      "Gradient Descent(28/49): loss=5.482640539496323e+61\n",
      "Gradient Descent(29/49): loss=1.2100372068124853e+64\n",
      "Gradient Descent(30/49): loss=2.6705928125740492e+66\n",
      "Gradient Descent(31/49): loss=5.89408815730509e+68\n",
      "Gradient Descent(32/49): loss=1.3008450798832142e+71\n",
      "Gradient Descent(33/49): loss=2.871008842579155e+73\n",
      "Gradient Descent(34/49): loss=6.336413076111876e+75\n",
      "Gradient Descent(35/49): loss=1.3984676771337476e+78\n",
      "Gradient Descent(36/49): loss=3.0864651980485073e+80\n",
      "Gradient Descent(37/49): loss=6.811932499068772e+82\n",
      "Gradient Descent(38/49): loss=1.5034164130931127e+85\n",
      "Gradient Descent(39/49): loss=3.3180905880479328e+87\n",
      "Gradient Descent(40/49): loss=7.323137525045931e+89\n",
      "Gradient Descent(41/49): loss=1.616241081660346e+92\n",
      "Gradient Descent(42/49): loss=3.5670984262039655e+94\n",
      "Gradient Descent(43/49): loss=7.872706198728219e+96\n",
      "Gradient Descent(44/49): loss=1.7375327363044254e+99\n",
      "Gradient Descent(45/49): loss=3.834793187401301e+101\n",
      "Gradient Descent(46/49): loss=8.463517540059387e+103\n",
      "Gradient Descent(47/49): loss=1.8679267864099065e+106\n",
      "Gradient Descent(48/49): loss=4.1225772415227195e+108\n",
      "Gradient Descent(49/49): loss=9.098666626536261e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.531249589533539\n",
      "Gradient Descent(2/49): loss=210.68952695755442\n",
      "Gradient Descent(3/49): loss=11796.196505359592\n",
      "Gradient Descent(4/49): loss=816609.9711197609\n",
      "Gradient Descent(5/49): loss=85962580.21456301\n",
      "Gradient Descent(6/49): loss=13792136962.557653\n",
      "Gradient Descent(7/49): loss=2718663994446.018\n",
      "Gradient Descent(8/49): loss=571330051205663.6\n",
      "Gradient Descent(9/49): loss=1.2208693037408893e+17\n",
      "Gradient Descent(10/49): loss=2.619679278564501e+19\n",
      "Gradient Descent(11/49): loss=5.626866791697269e+21\n",
      "Gradient Descent(12/49): loss=1.2089054309285119e+24\n",
      "Gradient Descent(13/49): loss=2.597431720648793e+26\n",
      "Gradient Descent(14/49): loss=5.580875235456378e+28\n",
      "Gradient Descent(15/49): loss=1.1991183174571144e+31\n",
      "Gradient Descent(16/49): loss=2.5764524892578596e+33\n",
      "Gradient Descent(17/49): loss=5.5358247293032486e+35\n",
      "Gradient Descent(18/49): loss=1.1894400204238142e+38\n",
      "Gradient Descent(19/49): loss=2.5556581885174696e+40\n",
      "Gradient Descent(20/49): loss=5.4911459883471103e+42\n",
      "Gradient Descent(21/49): loss=1.1798402628058791e+45\n",
      "Gradient Descent(22/49): loss=2.535031938587615e+47\n",
      "Gradient Descent(23/49): loss=5.446827958439437e+49\n",
      "Gradient Descent(24/49): loss=1.1703179891960727e+52\n",
      "Gradient Descent(25/49): loss=2.5145721625325283e+54\n",
      "Gradient Descent(26/49): loss=5.402867612870667e+56\n",
      "Gradient Descent(27/49): loss=1.1608725681911614e+59\n",
      "Gradient Descent(28/49): loss=2.4942775136087166e+61\n",
      "Gradient Descent(29/49): loss=5.35926206318081e+63\n",
      "Gradient Descent(30/49): loss=1.1515033794412919e+66\n",
      "Gradient Descent(31/49): loss=2.474146659060256e+68\n",
      "Gradient Descent(32/49): loss=5.316008445853883e+70\n",
      "Gradient Descent(33/49): loss=1.1422098076887184e+73\n",
      "Gradient Descent(34/49): loss=2.454178276932274e+75\n",
      "Gradient Descent(35/49): loss=5.273103920508222e+77\n",
      "Gradient Descent(36/49): loss=1.1329912426425812e+80\n",
      "Gradient Descent(37/49): loss=2.4343710559397457e+82\n",
      "Gradient Descent(38/49): loss=5.2305456696868755e+84\n",
      "Gradient Descent(39/49): loss=1.1238470789375672e+87\n",
      "Gradient Descent(40/49): loss=2.4147236953809102e+89\n",
      "Gradient Descent(41/49): loss=5.188330898671919e+91\n",
      "Gradient Descent(42/49): loss=1.1147767160941328e+94\n",
      "Gradient Descent(43/49): loss=2.395234905051471e+96\n",
      "Gradient Descent(44/49): loss=5.1464568353008395e+98\n",
      "Gradient Descent(45/49): loss=1.1057795584790819e+101\n",
      "Gradient Descent(46/49): loss=2.3759034051603373e+103\n",
      "Gradient Descent(47/49): loss=5.104920729784999e+105\n",
      "Gradient Descent(48/49): loss=1.0968550152664906e+108\n",
      "Gradient Descent(49/49): loss=2.3567279262452356e+110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.619754079966589\n",
      "Gradient Descent(2/49): loss=214.13847611235622\n",
      "Gradient Descent(3/49): loss=11854.299971946239\n",
      "Gradient Descent(4/49): loss=799525.0962435297\n",
      "Gradient Descent(5/49): loss=81053820.14841242\n",
      "Gradient Descent(6/49): loss=12690207309.946457\n",
      "Gradient Descent(7/49): loss=2480696406201.343\n",
      "Gradient Descent(8/49): loss=520278487956280.2\n",
      "Gradient Descent(9/49): loss=1.1114563545334866e+17\n",
      "Gradient Descent(10/49): loss=2.3852098949783663e+19\n",
      "Gradient Descent(11/49): loss=5.124400708997236e+21\n",
      "Gradient Descent(12/49): loss=1.1012267883544454e+24\n",
      "Gradient Descent(13/49): loss=2.366676566245067e+26\n",
      "Gradient Descent(14/49): loss=5.086370154598525e+28\n",
      "Gradient Descent(15/49): loss=1.0931473112645717e+31\n",
      "Gradient Descent(16/49): loss=2.3493613872041342e+33\n",
      "Gradient Descent(17/49): loss=5.0491824172997576e+35\n",
      "Gradient Descent(18/49): loss=1.0851563590906837e+38\n",
      "Gradient Descent(19/49): loss=2.3321881487745254e+40\n",
      "Gradient Descent(20/49): loss=5.012274529307019e+42\n",
      "Gradient Descent(21/49): loss=1.0772242363974175e+45\n",
      "Gradient Descent(22/49): loss=2.3151406590351887e+47\n",
      "Gradient Descent(23/49): loss=4.975636539048404e+49\n",
      "Gradient Descent(24/49): loss=1.0693501007109457e+52\n",
      "Gradient Descent(25/49): loss=2.2982177836332507e+54\n",
      "Gradient Descent(26/49): loss=4.939266361409747e+56\n",
      "Gradient Descent(27/49): loss=1.0615335223100314e+59\n",
      "Gradient Descent(28/49): loss=2.2814186086258625e+61\n",
      "Gradient Descent(29/49): loss=4.903162037179812e+63\n",
      "Gradient Descent(30/49): loss=1.0537740803877177e+66\n",
      "Gradient Descent(31/49): loss=2.2647422297625774e+68\n",
      "Gradient Descent(32/49): loss=4.8673216230397336e+70\n",
      "Gradient Descent(33/49): loss=1.046071357295008e+73\n",
      "Gradient Descent(34/49): loss=2.248187749445725e+75\n",
      "Gradient Descent(35/49): loss=4.83174318989834e+77\n",
      "Gradient Descent(36/49): loss=1.0384249384369265e+80\n",
      "Gradient Descent(37/49): loss=2.2317542766390918e+82\n",
      "Gradient Descent(38/49): loss=4.796424822764877e+84\n",
      "Gradient Descent(39/49): loss=1.0308344122490036e+87\n",
      "Gradient Descent(40/49): loss=2.2154409268198144e+89\n",
      "Gradient Descent(41/49): loss=4.761364620647275e+91\n",
      "Gradient Descent(42/49): loss=1.0232993701752273e+94\n",
      "Gradient Descent(43/49): loss=2.199246821930391e+96\n",
      "Gradient Descent(44/49): loss=4.726560696448665e+98\n",
      "Gradient Descent(45/49): loss=1.0158194066459611e+101\n",
      "Gradient Descent(46/49): loss=2.1831710903318254e+103\n",
      "Gradient Descent(47/49): loss=4.692011176866518e+105\n",
      "Gradient Descent(48/49): loss=1.0083941190561285e+108\n",
      "Gradient Descent(49/49): loss=2.167212866756408e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.453157280680759\n",
      "Gradient Descent(2/49): loss=203.2560562094045\n",
      "Gradient Descent(3/49): loss=10961.557674895139\n",
      "Gradient Descent(4/49): loss=689320.8492284054\n",
      "Gradient Descent(5/49): loss=62206542.95146943\n",
      "Gradient Descent(6/49): loss=8982519107.403942\n",
      "Gradient Descent(7/49): loss=1719772415718.2288\n",
      "Gradient Descent(8/49): loss=362461803688673.9\n",
      "Gradient Descent(9/49): loss=7.836016419402728e+16\n",
      "Gradient Descent(10/49): loss=1.7046487963251479e+19\n",
      "Gradient Descent(11/49): loss=3.7138551566250966e+21\n",
      "Gradient Descent(12/49): loss=8.094137776367408e+23\n",
      "Gradient Descent(13/49): loss=1.7642226559246907e+26\n",
      "Gradient Descent(14/49): loss=3.8454314667722096e+28\n",
      "Gradient Descent(15/49): loss=8.381830556896725e+30\n",
      "Gradient Descent(16/49): loss=1.826977441085695e+33\n",
      "Gradient Descent(17/49): loss=3.9822416800050476e+35\n",
      "Gradient Descent(18/49): loss=8.680046888461162e+37\n",
      "Gradient Descent(19/49): loss=1.891979976028914e+40\n",
      "Gradient Descent(20/49): loss=4.1239273119987115e+42\n",
      "Gradient Descent(21/49): loss=8.98887763376605e+44\n",
      "Gradient Descent(22/49): loss=1.959295472963677e+47\n",
      "Gradient Descent(23/49): loss=4.2706541427976394e+49\n",
      "Gradient Descent(24/49): loss=9.308696446907843e+51\n",
      "Gradient Descent(25/49): loss=2.0290060174286913e+54\n",
      "Gradient Descent(26/49): loss=4.42260142678971e+56\n",
      "Gradient Descent(27/49): loss=9.639894220241123e+58\n",
      "Gradient Descent(28/49): loss=2.101196820824458e+61\n",
      "Gradient Descent(29/49): loss=4.579954903003596e+63\n",
      "Gradient Descent(30/49): loss=9.982875809471561e+65\n",
      "Gradient Descent(31/49): loss=2.175956129218092e+68\n",
      "Gradient Descent(32/49): loss=4.742906920458292e+70\n",
      "Gradient Descent(33/49): loss=1.0338060475610064e+73\n",
      "Gradient Descent(34/49): loss=2.2533753284587256e+75\n",
      "Gradient Descent(35/49): loss=4.911656671854498e+77\n",
      "Gradient Descent(36/49): loss=1.0705882396730296e+80\n",
      "Gradient Descent(37/49): loss=2.333549055849273e+82\n",
      "Gradient Descent(38/49): loss=5.086410437049371e+84\n",
      "Gradient Descent(39/49): loss=1.1086791198699852e+87\n",
      "Gradient Descent(40/49): loss=2.4165753158306346e+89\n",
      "Gradient Descent(41/49): loss=5.2673818352119756e+91\n",
      "Gradient Descent(42/49): loss=1.1481252504801078e+94\n",
      "Gradient Descent(43/49): loss=2.5025555997820907e+96\n",
      "Gradient Descent(44/49): loss=5.45479208595219e+98\n",
      "Gradient Descent(45/49): loss=1.188974850491141e+101\n",
      "Gradient Descent(46/49): loss=2.5915950100848495e+103\n",
      "Gradient Descent(47/49): loss=5.648870279739289e+105\n",
      "Gradient Descent(48/49): loss=1.2312778544930177e+108\n",
      "Gradient Descent(49/49): loss=2.6838023885989083e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.530997379926803\n",
      "Gradient Descent(2/49): loss=210.31685351649745\n",
      "Gradient Descent(3/49): loss=11897.935223592343\n",
      "Gradient Descent(4/49): loss=862474.8804849642\n",
      "Gradient Descent(5/49): loss=99156392.85443087\n",
      "Gradient Descent(6/49): loss=17226622866.226463\n",
      "Gradient Descent(7/49): loss=3581069188568.3203\n",
      "Gradient Descent(8/49): loss=783793235536175.9\n",
      "Gradient Descent(9/49): loss=1.7375188329343133e+17\n",
      "Gradient Descent(10/49): loss=3.86344746611283e+19\n",
      "Gradient Descent(11/49): loss=8.596681317602396e+21\n",
      "Gradient Descent(12/49): loss=1.9131964735952929e+24\n",
      "Gradient Descent(13/49): loss=4.2579979200052914e+26\n",
      "Gradient Descent(14/49): loss=9.476660905166923e+28\n",
      "Gradient Descent(15/49): loss=2.109143756124287e+31\n",
      "Gradient Descent(16/49): loss=4.6941530365701485e+33\n",
      "Gradient Descent(17/49): loss=1.0447403269781961e+36\n",
      "Gradient Descent(18/49): loss=2.3251955636319386e+38\n",
      "Gradient Descent(19/49): loss=5.175003113492431e+40\n",
      "Gradient Descent(20/49): loss=1.1517593481227042e+43\n",
      "Gradient Descent(21/49): loss=2.5633793205136492e+45\n",
      "Gradient Descent(22/49): loss=5.7051097975547466e+47\n",
      "Gradient Descent(23/49): loss=1.269740983799792e+50\n",
      "Gradient Descent(24/49): loss=2.8259616784929736e+52\n",
      "Gradient Descent(25/49): loss=6.289518500395956e+54\n",
      "Gradient Descent(26/49): loss=1.399808188054703e+57\n",
      "Gradient Descent(27/49): loss=3.1154419264714977e+59\n",
      "Gradient Descent(28/49): loss=6.933791700922045e+61\n",
      "Gradient Descent(29/49): loss=1.543198958172428e+64\n",
      "Gradient Descent(30/49): loss=3.4345753769727074e+66\n",
      "Gradient Descent(31/49): loss=7.644061679563e+68\n",
      "Gradient Descent(32/49): loss=1.7012781071197994e+71\n",
      "Gradient Descent(33/49): loss=3.786399585842397e+73\n",
      "Gradient Descent(34/49): loss=8.42708888315673e+75\n",
      "Gradient Descent(35/49): loss=1.875550253865392e+78\n",
      "Gradient Descent(36/49): loss=4.174263264038025e+80\n",
      "Gradient Descent(37/49): loss=9.29032627176319e+82\n",
      "Gradient Descent(38/49): loss=2.067674144546375e+85\n",
      "Gradient Descent(39/49): loss=4.6018581511176625e+87\n",
      "Gradient Descent(40/49): loss=1.024199025695827e+90\n",
      "Gradient Descent(41/49): loss=2.2794784406413847e+92\n",
      "Gradient Descent(42/49): loss=5.073254153721642e+94\n",
      "Gradient Descent(43/49): loss=1.1291138906763093e+97\n",
      "Gradient Descent(44/49): loss=2.5129791244205074e+99\n",
      "Gradient Descent(45/49): loss=5.592938083500709e+101\n",
      "Gradient Descent(46/49): loss=1.2447758161575089e+104\n",
      "Gradient Descent(47/49): loss=2.77039868734023e+106\n",
      "Gradient Descent(48/49): loss=6.165856363203485e+108\n",
      "Gradient Descent(49/49): loss=1.3722856881713472e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.569027101266396\n",
      "Gradient Descent(2/49): loss=214.4872416762419\n",
      "Gradient Descent(3/49): loss=12115.003235512278\n",
      "Gradient Descent(4/49): loss=845746.7724310916\n",
      "Gradient Descent(5/49): loss=89729548.10785127\n",
      "Gradient Descent(6/49): loss=14509739449.496168\n",
      "Gradient Descent(7/49): loss=2883506353231.6416\n",
      "Gradient Descent(8/49): loss=611030735899602.8\n",
      "Gradient Descent(9/49): loss=1.3166806196577717e+17\n",
      "Gradient Descent(10/49): loss=2.8490619715958903e+19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(11/49): loss=6.171133257577834e+21\n",
      "Gradient Descent(12/49): loss=1.3370131856591545e+24\n",
      "Gradient Descent(13/49): loss=2.8968947825937214e+26\n",
      "Gradient Descent(14/49): loss=6.276769131606176e+28\n",
      "Gradient Descent(15/49): loss=1.3600070159523272e+31\n",
      "Gradient Descent(16/49): loss=2.946771921069068e+33\n",
      "Gradient Descent(17/49): loss=6.384868979622485e+35\n",
      "Gradient Descent(18/49): loss=1.3834309237255669e+38\n",
      "Gradient Descent(19/49): loss=2.9975261060101925e+40\n",
      "Gradient Descent(20/49): loss=6.494840204802987e+42\n",
      "Gradient Descent(21/49): loss=1.4072587802694836e+45\n",
      "Gradient Descent(22/49): loss=3.049154732928734e+47\n",
      "Gradient Descent(23/49): loss=6.606705686339552e+49\n",
      "Gradient Descent(24/49): loss=1.4314970491662928e+52\n",
      "Gradient Descent(25/49): loss=3.101672602147648e+54\n",
      "Gradient Descent(26/49): loss=6.720497912672792e+56\n",
      "Gradient Descent(27/49): loss=1.456152791979822e+59\n",
      "Gradient Descent(28/49): loss=3.15509502590929e+61\n",
      "Gradient Descent(29/49): loss=6.836250067537873e+63\n",
      "Gradient Descent(30/49): loss=1.4812331990680125e+66\n",
      "Gradient Descent(31/49): loss=3.2094375839756765e+68\n",
      "Gradient Descent(32/49): loss=6.953995908218101e+70\n",
      "Gradient Descent(33/49): loss=1.5067455847391944e+73\n",
      "Gradient Descent(34/49): loss=3.2647161245063025e+75\n",
      "Gradient Descent(35/49): loss=7.073769773452653e+77\n",
      "Gradient Descent(36/49): loss=1.5326973892831372e+80\n",
      "Gradient Descent(37/49): loss=3.3209467686261675e+82\n",
      "Gradient Descent(38/49): loss=7.195606593423391e+84\n",
      "Gradient Descent(39/49): loss=1.5590961811392888e+87\n",
      "Gradient Descent(40/49): loss=3.378145915126491e+89\n",
      "Gradient Descent(41/49): loss=7.319541899940221e+91\n",
      "Gradient Descent(42/49): loss=1.585949659103903e+94\n",
      "Gradient Descent(43/49): loss=3.4363302452470576e+96\n",
      "Gradient Descent(44/49): loss=7.445611836804247e+98\n",
      "Gradient Descent(45/49): loss=1.613265654575484e+101\n",
      "Gradient Descent(46/49): loss=3.4955167275412768e+103\n",
      "Gradient Descent(47/49): loss=7.573853170348583e+105\n",
      "Gradient Descent(48/49): loss=1.641052133838558e+108\n",
      "Gradient Descent(49/49): loss=3.555722622824561e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.658346453142476\n",
      "Gradient Descent(2/49): loss=217.99990354915067\n",
      "Gradient Descent(3/49): loss=12174.937648944813\n",
      "Gradient Descent(4/49): loss=828098.7480113344\n",
      "Gradient Descent(5/49): loss=84611439.5344249\n",
      "Gradient Descent(6/49): loss=13350939420.13049\n",
      "Gradient Descent(7/49): loss=2631138917235.57\n",
      "Gradient Descent(8/49): loss=556433184566624.4\n",
      "Gradient Descent(9/49): loss=1.1986814615648059e+17\n",
      "Gradient Descent(10/49): loss=2.594060937730586e+19\n",
      "Gradient Descent(11/49): loss=5.620061546186764e+21\n",
      "Gradient Descent(12/49): loss=1.21792285173229e+24\n",
      "Gradient Descent(13/49): loss=2.6395330714636182e+26\n",
      "Gradient Descent(14/49): loss=5.720597450509703e+28\n",
      "Gradient Descent(15/49): loss=1.239816343841692e+31\n",
      "Gradient Descent(16/49): loss=2.6870375537284965e+33\n",
      "Gradient Descent(17/49): loss=5.8235822643253714e+35\n",
      "Gradient Descent(18/49): loss=1.262137636720402e+38\n",
      "Gradient Descent(19/49): loss=2.7354150130846425e+40\n",
      "Gradient Descent(20/49): loss=5.928430545686956e+42\n",
      "Gradient Descent(21/49): loss=1.2848612950854528e+45\n",
      "Gradient Descent(22/49): loss=2.7846637237399858e+47\n",
      "Gradient Descent(23/49): loss=6.035166662999882e+49\n",
      "Gradient Descent(24/49): loss=1.3079940798626278e+52\n",
      "Gradient Descent(25/49): loss=2.834799117395133e+54\n",
      "Gradient Descent(26/49): loss=6.143824471157712e+56\n",
      "Gradient Descent(27/49): loss=1.3315433499600913e+59\n",
      "Gradient Descent(28/49): loss=2.885837154278233e+61\n",
      "Gradient Descent(29/49): loss=6.2544385665416375e+63\n",
      "Gradient Descent(30/49): loss=1.3555166037228605e+66\n",
      "Gradient Descent(31/49): loss=2.937794085623849e+68\n",
      "Gradient Descent(32/49): loss=6.367044170335272e+70\n",
      "Gradient Descent(33/49): loss=1.3799214745982619e+73\n",
      "Gradient Descent(34/49): loss=2.9906864553087172e+75\n",
      "Gradient Descent(35/49): loss=6.481677137875655e+77\n",
      "Gradient Descent(36/49): loss=1.404765733468435e+80\n",
      "Gradient Descent(37/49): loss=3.0445311050681395e+82\n",
      "Gradient Descent(38/49): loss=6.598373970043696e+84\n",
      "Gradient Descent(39/49): loss=1.4300572911235331e+87\n",
      "Gradient Descent(40/49): loss=3.099345179857997e+89\n",
      "Gradient Descent(41/49): loss=6.71717182488691e+91\n",
      "Gradient Descent(42/49): loss=1.4558042007802503e+94\n",
      "Gradient Descent(43/49): loss=3.155146133313588e+96\n",
      "Gradient Descent(44/49): loss=6.838108529449412e+98\n",
      "Gradient Descent(45/49): loss=1.4820146606464875e+101\n",
      "Gradient Descent(46/49): loss=3.2119517333077487e+103\n",
      "Gradient Descent(47/49): loss=6.961222591818428e+105\n",
      "Gradient Descent(48/49): loss=1.5086970165314465e+108\n",
      "Gradient Descent(49/49): loss=3.269780067607788e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.490242015885853\n",
      "Gradient Descent(2/49): loss=206.9237461948024\n",
      "Gradient Descent(3/49): loss=11258.624390745012\n",
      "Gradient Descent(4/49): loss=714070.1478605522\n",
      "Gradient Descent(5/49): loss=64951607.10333829\n",
      "Gradient Descent(6/49): loss=9451299657.34275\n",
      "Gradient Descent(7/49): loss=1824104776268.2266\n",
      "Gradient Descent(8/49): loss=387642962933307.6\n",
      "Gradient Descent(9/49): loss=8.450715376428374e+16\n",
      "Gradient Descent(10/49): loss=1.8538412334967423e+19\n",
      "Gradient Descent(11/49): loss=4.0729126240507097e+21\n",
      "Gradient Descent(12/49): loss=8.951463512346334e+23\n",
      "Gradient Descent(13/49): loss=1.967525567848575e+26\n",
      "Gradient Descent(14/49): loss=4.324696734756436e+28\n",
      "Gradient Descent(15/49): loss=9.505896188966545e+30\n",
      "Gradient Descent(16/49): loss=2.0894452040511535e+33\n",
      "Gradient Descent(17/49): loss=4.592710039730042e+35\n",
      "Gradient Descent(18/49): loss=1.0095017990028674e+38\n",
      "Gradient Descent(19/49): loss=2.2189380337780797e+40\n",
      "Gradient Descent(20/49): loss=4.87734248861367e+42\n",
      "Gradient Descent(21/49): loss=1.0720655291355215e+45\n",
      "Gradient Descent(22/49): loss=2.3564564139082174e+47\n",
      "Gradient Descent(23/49): loss=5.179615126154672e+49\n",
      "Gradient Descent(24/49): loss=1.1385066448536809e+52\n",
      "Gradient Descent(25/49): loss=2.5024974806228835e+54\n",
      "Gradient Descent(26/49): loss=5.500621071327299e+56\n",
      "Gradient Descent(27/49): loss=1.2090654398103244e+59\n",
      "Gradient Descent(28/49): loss=2.6575894226995376e+61\n",
      "Gradient Descent(29/49): loss=5.8415213164579805e+63\n",
      "Gradient Descent(30/49): loss=1.28399710651961e+66\n",
      "Gradient Descent(31/49): loss=2.8222931668601594e+68\n",
      "Gradient Descent(32/49): loss=6.203548808062507e+70\n",
      "Gradient Descent(33/49): loss=1.3635726531141601e+73\n",
      "Gradient Descent(34/49): loss=2.9972044032349432e+75\n",
      "Gradient Descent(35/49): loss=6.588012904376338e+77\n",
      "Gradient Descent(36/49): loss=1.4480798834201962e+80\n",
      "Gradient Descent(37/49): loss=3.182955739769805e+82\n",
      "Gradient Descent(38/49): loss=6.996304110934006e+84\n",
      "Gradient Descent(39/49): loss=1.5378244378670648e+87\n",
      "Gradient Descent(40/49): loss=3.380218990202666e+89\n",
      "Gradient Descent(41/49): loss=7.429899109662884e+91\n",
      "Gradient Descent(42/49): loss=1.6331308989084489e+94\n",
      "Gradient Descent(43/49): loss=3.589707603836455e+96\n",
      "Gradient Descent(44/49): loss=7.89036609965203e+98\n",
      "Gradient Descent(45/49): loss=1.734343964951372e+101\n",
      "Gradient Descent(46/49): loss=3.812179245898201e+103\n",
      "Gradient Descent(47/49): loss=8.379370468916289e+105\n",
      "Gradient Descent(48/49): loss=1.8418296970399932e+108\n",
      "Gradient Descent(49/49): loss=4.048438537814432e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.568617957320253\n",
      "Gradient Descent(2/49): loss=214.0929487277436\n",
      "Gradient Descent(3/49): loss=12217.924323345966\n",
      "Gradient Descent(4/49): loss=893032.6136878567\n",
      "Gradient Descent(5/49): loss=103467239.30135942\n",
      "Gradient Descent(6/49): loss=18117041113.41184\n",
      "Gradient Descent(7/49): loss=3796979569138.0737\n",
      "Gradient Descent(8/49): loss=837967756096293.6\n",
      "Gradient Descent(9/49): loss=1.8731601807032746e+17\n",
      "Gradient Descent(10/49): loss=4.199962491042513e+19\n",
      "Gradient Descent(11/49): loss=9.423835206120461e+21\n",
      "Gradient Descent(12/49): loss=2.1148679906315877e+24\n",
      "Gradient Descent(13/49): loss=4.746309707063418e+26\n",
      "Gradient Descent(14/49): loss=1.0652043497718823e+29\n",
      "Gradient Descent(15/49): loss=2.3906210552355534e+31\n",
      "Gradient Descent(16/49): loss=5.365235292431323e+33\n",
      "Gradient Descent(17/49): loss=1.2041119256919955e+36\n",
      "Gradient Descent(18/49): loss=2.7023708966448484e+38\n",
      "Gradient Descent(19/49): loss=6.064891772909264e+40\n",
      "Gradient Descent(20/49): loss=1.3611348583007704e+43\n",
      "Gradient Descent(21/49): loss=3.054775208751419e+45\n",
      "Gradient Descent(22/49): loss=6.8557876685782215e+47\n",
      "Gradient Descent(23/49): loss=1.5386344770290934e+50\n",
      "Gradient Descent(24/49): loss=3.453135027451651e+52\n",
      "Gradient Descent(25/49): loss=7.749820828694321e+54\n",
      "Gradient Descent(26/49): loss=1.7392810417032008e+57\n",
      "Gradient Descent(27/49): loss=3.903443200683735e+59\n",
      "Gradient Descent(28/49): loss=8.760440926811749e+61\n",
      "Gradient Descent(29/49): loss=1.9660930436675784e+64\n",
      "Gradient Descent(30/49): loss=4.412474085097067e+66\n",
      "Gradient Descent(31/49): loss=9.902851553420534e+68\n",
      "Gradient Descent(32/49): loss=2.2224826026808974e+71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=4.9878854515527047e+73\n",
      "Gradient Descent(34/49): loss=1.1194238932534352e+76\n",
      "Gradient Descent(35/49): loss=2.5123067980572764e+78\n",
      "Gradient Descent(36/49): loss=5.638333687179764e+80\n",
      "Gradient Descent(37/49): loss=1.2654030468161577e+83\n",
      "Gradient Descent(38/49): loss=2.8399256938845058e+85\n",
      "Gradient Descent(39/49): loss=6.373604020535515e+87\n",
      "Gradient Descent(40/49): loss=1.4304187006745835e+90\n",
      "Gradient Descent(41/49): loss=3.210267931059211e+92\n",
      "Gradient Descent(42/49): loss=7.204757728857457e+94\n",
      "Gradient Descent(43/49): loss=1.6169533212264488e+97\n",
      "Gradient Descent(44/49): loss=3.628904872891475e+99\n",
      "Gradient Descent(45/49): loss=8.144298541968257e+101\n",
      "Gradient Descent(46/49): loss=1.8278131024100404e+104\n",
      "Gradient Descent(47/49): loss=4.102134419712024e+106\n",
      "Gradient Descent(48/49): loss=9.206360746182554e+108\n",
      "Gradient Descent(49/49): loss=2.0661701815905233e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.606976428296165\n",
      "Gradient Descent(2/49): loss=218.33617611682348\n",
      "Gradient Descent(3/49): loss=12440.961289376479\n",
      "Gradient Descent(4/49): loss=875790.01992806\n",
      "Gradient Descent(5/49): loss=93644575.6580197\n",
      "Gradient Descent(6/49): loss=15261434824.169163\n",
      "Gradient Descent(7/49): loss=3057591129686.7495\n",
      "Gradient Descent(8/49): loss=653306696653818.8\n",
      "Gradient Descent(9/49): loss=1.4195627259388957e+17\n",
      "Gradient Descent(10/49): loss=3.0974429694197182e+19\n",
      "Gradient Descent(11/49): loss=6.765433585255283e+21\n",
      "Gradient Descent(12/49): loss=1.4780742054155133e+24\n",
      "Gradient Descent(13/49): loss=3.229410513664702e+26\n",
      "Gradient Descent(14/49): loss=7.0559698025119705e+28\n",
      "Gradient Descent(15/49): loss=1.5416711419527622e+31\n",
      "Gradient Descent(16/49): loss=3.3684271234444755e+33\n",
      "Gradient Descent(17/49): loss=7.359743214989294e+35\n",
      "Gradient Descent(18/49): loss=1.6080449731292665e+38\n",
      "Gradient Descent(19/49): loss=3.5134495771806363e+40\n",
      "Gradient Descent(20/49): loss=7.676606174688987e+42\n",
      "Gradient Descent(21/49): loss=1.6772770211034326e+45\n",
      "Gradient Descent(22/49): loss=3.664716076381993e+47\n",
      "Gradient Descent(23/49): loss=8.007111378813597e+49\n",
      "Gradient Descent(24/49): loss=1.749489763930916e+52\n",
      "Gradient Descent(25/49): loss=3.822495141263823e+54\n",
      "Gradient Descent(26/49): loss=8.351846010327572e+56\n",
      "Gradient Descent(27/49): loss=1.8248115223809857e+59\n",
      "Gradient Descent(28/49): loss=3.987067156287128e+61\n",
      "Gradient Descent(29/49): loss=8.711422694219962e+63\n",
      "Gradient Descent(30/49): loss=1.903376150504596e+66\n",
      "Gradient Descent(31/49): loss=4.1587245820518775e+68\n",
      "Gradient Descent(32/49): loss=9.086480433611325e+70\n",
      "Gradient Descent(33/49): loss=1.9853232653763157e+73\n",
      "Gradient Descent(34/49): loss=4.337772470696891e+75\n",
      "Gradient Descent(35/49): loss=9.47768574301636e+77\n",
      "Gradient Descent(36/49): loss=2.0707984950843848e+80\n",
      "Gradient Descent(37/49): loss=4.524529007942267e+82\n",
      "Gradient Descent(38/49): loss=9.88573383277275e+84\n",
      "Gradient Descent(39/49): loss=2.1599537375244644e+87\n",
      "Gradient Descent(40/49): loss=4.7193260785348604e+89\n",
      "Gradient Descent(41/49): loss=1.0311349844495157e+92\n",
      "Gradient Descent(42/49): loss=2.2529474303368867e+94\n",
      "Gradient Descent(43/49): loss=4.922509856041181e+96\n",
      "Gradient Descent(44/49): loss=1.0755290139725619e+99\n",
      "Gradient Descent(45/49): loss=2.3499448324662037e+101\n",
      "Gradient Descent(46/49): loss=5.134441418030905e+103\n",
      "Gradient Descent(47/49): loss=1.1218343644060654e+106\n",
      "Gradient Descent(48/49): loss=2.45111831784226e+108\n",
      "Gradient Descent(49/49): loss=5.35549738774734e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.69711429980012\n",
      "Gradient Descent(2/49): loss=221.9134283690005\n",
      "Gradient Descent(3/49): loss=12502.773331107306\n",
      "Gradient Descent(4/49): loss=857562.7389203351\n",
      "Gradient Descent(5/49): loss=88309120.70621854\n",
      "Gradient Descent(6/49): loss=14043085619.00678\n",
      "Gradient Descent(7/49): loss=2790018200320.468\n",
      "Gradient Descent(8/49): loss=594933242937266.5\n",
      "Gradient Descent(9/49): loss=1.2923437313060906e+17\n",
      "Gradient Descent(10/49): loss=2.8202096848653697e+19\n",
      "Gradient Descent(11/49): loss=6.161287624145957e+21\n",
      "Gradient Descent(12/49): loss=1.3464182195133719e+24\n",
      "Gradient Descent(13/49): loss=2.9425053407177752e+26\n",
      "Gradient Descent(14/49): loss=6.430748643715767e+28\n",
      "Gradient Descent(15/49): loss=1.4054244665035284e+31\n",
      "Gradient Descent(16/49): loss=3.071523891322412e+33\n",
      "Gradient Descent(17/49): loss=6.712748653870762e+35\n",
      "Gradient Descent(18/49): loss=1.467056699393622e+38\n",
      "Gradient Descent(19/49): loss=3.2062207295256857e+40\n",
      "Gradient Descent(20/49): loss=7.007126176369234e+42\n",
      "Gradient Descent(21/49): loss=1.5313922974479667e+45\n",
      "Gradient Descent(22/49): loss=3.346824803941956e+47\n",
      "Gradient Descent(23/49): loss=7.314413352790809e+49\n",
      "Gradient Descent(24/49): loss=1.5985492468290858e+52\n",
      "Gradient Descent(25/49): loss=3.493594867139987e+54\n",
      "Gradient Descent(26/49): loss=7.635176157334848e+56\n",
      "Gradient Descent(27/49): loss=1.6686512652587511e+59\n",
      "Gradient Descent(28/49): loss=3.646801314957021e+61\n",
      "Gradient Descent(29/49): loss=7.970005541397364e+63\n",
      "Gradient Descent(30/49): loss=1.7418275042673884e+66\n",
      "Gradient Descent(31/49): loss=3.8067264054755284e+68\n",
      "Gradient Descent(32/49): loss=8.319518374030644e+70\n",
      "Gradient Descent(33/49): loss=1.8182127792603318e+73\n",
      "Gradient Descent(34/49): loss=3.9736647748564095e+75\n",
      "Gradient Descent(35/49): loss=8.68435857620489e+77\n",
      "Gradient Descent(36/49): loss=1.8979478177755517e+80\n",
      "Gradient Descent(37/49): loss=4.147923980095625e+82\n",
      "Gradient Descent(38/49): loss=9.065198307094294e+84\n",
      "Gradient Descent(39/49): loss=1.9811795187493017e+87\n",
      "Gradient Descent(40/49): loss=4.3298250656469644e+89\n",
      "Gradient Descent(41/49): loss=9.462739202422317e+91\n",
      "Gradient Descent(42/49): loss=2.0680612231542956e+94\n",
      "Gradient Descent(43/49): loss=4.519703154895703e+96\n",
      "Gradient Descent(44/49): loss=9.877713667111509e+98\n",
      "Gradient Descent(45/49): loss=2.158752996505005e+101\n",
      "Gradient Descent(46/49): loss=4.717908067567971e+103\n",
      "Gradient Descent(47/49): loss=1.0310886224621245e+106\n",
      "Gradient Descent(48/49): loss=2.253421923752904e+108\n",
      "Gradient Descent(49/49): loss=4.924804964218088e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.527495544219291\n",
      "Gradient Descent(2/49): loss=210.64095783306567\n",
      "Gradient Descent(3/49): loss=11562.372317127063\n",
      "Gradient Descent(4/49): loss=739594.079841509\n",
      "Gradient Descent(5/49): loss=67805330.75225401\n",
      "Gradient Descent(6/49): loss=9942423051.620932\n",
      "Gradient Descent(7/49): loss=1934290394637.1543\n",
      "Gradient Descent(8/49): loss=414457218524739.7\n",
      "Gradient Descent(9/49): loss=9.110759986757562e+16\n",
      "Gradient Descent(10/49): loss=2.0153846572768395e+19\n",
      "Gradient Descent(11/49): loss=4.4649623046836385e+21\n",
      "Gradient Descent(12/49): loss=9.895434105167198e+23\n",
      "Gradient Descent(13/49): loss=2.1932569093984877e+26\n",
      "Gradient Descent(14/49): loss=4.86130816115819e+28\n",
      "Gradient Descent(15/49): loss=1.0775041407302165e+31\n",
      "Gradient Descent(16/49): loss=2.3882800860648554e+33\n",
      "Gradient Descent(17/49): loss=5.2936069272518506e+35\n",
      "Gradient Descent(18/49): loss=1.1733245342383653e+38\n",
      "Gradient Descent(19/49): loss=2.600666244839613e+40\n",
      "Gradient Descent(20/49): loss=5.764359941213642e+42\n",
      "Gradient Descent(21/49): loss=1.2776666605539589e+45\n",
      "Gradient Descent(22/49): loss=2.8319399074657665e+47\n",
      "Gradient Descent(23/49): loss=6.276976528795086e+49\n",
      "Gradient Descent(24/49): loss=1.3912877967386414e+52\n",
      "Gradient Descent(25/49): loss=3.0837804227559446e+54\n",
      "Gradient Descent(26/49): loss=6.8351794057791e+56\n",
      "Gradient Descent(27/49): loss=1.5150131041899002e+59\n",
      "Gradient Descent(28/49): loss=3.358016768260095e+61\n",
      "Gradient Descent(29/49): loss=7.443022495799179e+63\n",
      "Gradient Descent(30/49): loss=1.6497411328197893e+66\n",
      "Gradient Descent(31/49): loss=3.656640574247322e+68\n",
      "Gradient Descent(32/49): loss=8.104920234593213e+70\n",
      "Gradient Descent(33/49): loss=1.7964503394660294e+73\n",
      "Gradient Descent(34/49): loss=3.981820583987014e+75\n",
      "Gradient Descent(35/49): loss=8.82567962762359e+77\n",
      "Gradient Descent(36/49): loss=1.956206193787268e+80\n",
      "Gradient Descent(37/49): loss=4.3359184041014684e+82\n",
      "Gradient Descent(38/49): loss=9.610535160727782e+84\n",
      "Gradient Descent(39/49): loss=2.130168916191267e+87\n",
      "Gradient Descent(40/49): loss=4.721505655636973e+89\n",
      "Gradient Descent(41/49): loss=1.0465186815358648e+92\n",
      "Gradient Descent(42/49): loss=2.3196019038885143e+94\n",
      "Gradient Descent(43/49): loss=5.141382650357049e+96\n",
      "Gradient Descent(44/49): loss=1.1395841464468416e+99\n",
      "Gradient Descent(45/49): loss=2.5258809062632688e+101\n",
      "Gradient Descent(46/49): loss=5.598598727893835e+103\n",
      "Gradient Descent(47/49): loss=1.2409257949673015e+106\n",
      "Gradient Descent(48/49): loss=2.75050401619783e+108\n",
      "Gradient Descent(49/49): loss=6.096474401452639e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.606408946780352\n",
      "Gradient Descent(2/49): loss=217.9197740165555\n",
      "Gradient Descent(3/49): loss=12545.056918423756\n",
      "Gradient Descent(4/49): loss=924534.459308376\n",
      "Gradient Descent(5/49): loss=107946184.80414629\n",
      "Gradient Descent(6/49): loss=19049483483.33663\n",
      "Gradient Descent(7/49): loss=4024927451602.769\n",
      "Gradient Descent(8/49): loss=895637780590844.8\n",
      "Gradient Descent(9/49): loss=2.0187594101724726e+17\n",
      "Gradient Descent(10/49): loss=4.564203380521076e+19\n",
      "Gradient Descent(11/49): loss=1.0326630409077558e+22\n",
      "Gradient Descent(12/49): loss=2.3368238146101862e+24\n",
      "Gradient Descent(13/49): loss=5.288233626256713e+26\n",
      "Gradient Descent(14/49): loss=1.1967387972450867e+29\n",
      "Gradient Descent(15/49): loss=2.7082519580338284e+31\n",
      "Gradient Descent(16/49): loss=6.128849911028187e+33\n",
      "Gradient Descent(17/49): loss=1.386976041839189e+36\n",
      "Gradient Descent(18/49): loss=3.1387660387998305e+38\n",
      "Gradient Descent(19/49): loss=7.103116431365242e+40\n",
      "Gradient Descent(20/49): loss=1.6074553660182578e+43\n",
      "Gradient Descent(21/49): loss=3.637717021353118e+45\n",
      "Gradient Descent(22/49): loss=8.232256651318217e+47\n",
      "Gradient Descent(23/49): loss=1.862982996664219e+50\n",
      "Gradient Descent(24/49): loss=4.2159832873056144e+52\n",
      "Gradient Descent(25/49): loss=9.54088958980632e+54\n",
      "Gradient Descent(26/49): loss=2.159130337138426e+57\n",
      "Gradient Descent(27/49): loss=4.886173106680382e+59\n",
      "Gradient Descent(28/49): loss=1.10575481330545e+62\n",
      "Gradient Descent(29/49): loss=2.5023544611559232e+64\n",
      "Gradient Descent(30/49): loss=5.66289902057817e+66\n",
      "Gradient Descent(31/49): loss=1.2815300875660994e+69\n",
      "Gradient Descent(32/49): loss=2.900138885346867e+71\n",
      "Gradient Descent(33/49): loss=6.563096439097188e+73\n",
      "Gradient Descent(34/49): loss=1.4852473130347675e+76\n",
      "Gradient Descent(35/49): loss=3.3611567365303527e+78\n",
      "Gradient Descent(36/49): loss=7.606392893880894e+80\n",
      "Gradient Descent(37/49): loss=1.7213482557140536e+83\n",
      "Gradient Descent(38/49): loss=3.895459856975756e+85\n",
      "Gradient Descent(39/49): loss=8.815535988686121e+87\n",
      "Gradient Descent(40/49): loss=1.9949807627629717e+90\n",
      "Gradient Descent(41/49): loss=4.514697970608086e+92\n",
      "Gradient Descent(42/49): loss=1.0216889378714485e+95\n",
      "Gradient Descent(43/49): loss=2.3121110040242534e+97\n",
      "Gradient Descent(44/49): loss=5.232372688763291e+99\n",
      "Gradient Descent(45/49): loss=1.184100759282936e+102\n",
      "Gradient Descent(46/49): loss=2.6796535559201437e+104\n",
      "Gradient Descent(47/49): loss=6.0641318937281776e+106\n",
      "Gradient Descent(48/49): loss=1.3723302231845394e+109\n",
      "Gradient Descent(49/49): loss=3.1056221640125064e+111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.64509757062285\n",
      "Gradient Descent(2/49): loss=222.23678895405595\n",
      "Gradient Descent(3/49): loss=12774.198481170793\n",
      "Gradient Descent(4/49): loss=906763.7990716172\n",
      "Gradient Descent(5/49): loss=97712818.23052111\n",
      "Gradient Descent(6/49): loss=16048689923.75719\n",
      "Gradient Descent(7/49): loss=3241394687965.7603\n",
      "Gradient Descent(8/49): loss=698313185992381.6\n",
      "Gradient Descent(9/49): loss=1.530004641316664e+17\n",
      "Gradient Descent(10/49): loss=3.3663065462677385e+19\n",
      "Gradient Descent(11/49): loss=7.414129378324914e+21\n",
      "Gradient Descent(12/49): loss=1.6333358456896253e+24\n",
      "Gradient Descent(13/49): loss=3.598465872180451e+26\n",
      "Gradient Descent(14/49): loss=7.9280387544464e+28\n",
      "Gradient Descent(15/49): loss=1.7466895505489986e+31\n",
      "Gradient Descent(16/49): loss=3.8482747009937466e+33\n",
      "Gradient Descent(17/49): loss=8.47845078143546e+35\n",
      "Gradient Descent(18/49): loss=1.8679573845651197e+38\n",
      "Gradient Descent(19/49): loss=4.115450953875737e+40\n",
      "Gradient Descent(20/49): loss=9.067089403131723e+42\n",
      "Gradient Descent(21/49): loss=1.9976452454166325e+45\n",
      "Gradient Descent(22/49): loss=4.4011769928126246e+47\n",
      "Gradient Descent(23/49): loss=9.696596013427445e+49\n",
      "Gradient Descent(24/49): loss=2.1363370389832758e+52\n",
      "Gradient Descent(25/49): loss=4.7067403218864974e+54\n",
      "Gradient Descent(26/49): loss=1.0369807784744817e+57\n",
      "Gradient Descent(27/49): loss=2.284657876546428e+59\n",
      "Gradient Descent(28/49): loss=5.033518191672352e+61\n",
      "Gradient Descent(29/49): loss=1.1089759060204174e+64\n",
      "Gradient Descent(30/49): loss=2.443276279737008e+66\n",
      "Gradient Descent(31/49): loss=5.382983477564961e+68\n",
      "Gradient Descent(32/49): loss=1.1859694853197985e+71\n",
      "Gradient Descent(33/49): loss=2.6129071842256103e+73\n",
      "Gradient Descent(34/49): loss=5.756711313307301e+75\n",
      "Gradient Descent(35/49): loss=1.2683085470784192e+78\n",
      "Gradient Descent(36/49): loss=2.794315161981046e+80\n",
      "Gradient Descent(37/49): loss=6.156386190460895e+82\n",
      "Gradient Descent(38/49): loss=1.3563642155248594e+85\n",
      "Gradient Descent(39/49): loss=2.98831786739921e+87\n",
      "Gradient Descent(40/49): loss=6.583809550859927e+89\n",
      "Gradient Descent(41/49): loss=1.4505333811667065e+92\n",
      "Gradient Descent(42/49): loss=3.1957897226904846e+94\n",
      "Gradient Descent(43/49): loss=7.040907906193229e+96\n",
      "Gradient Descent(44/49): loss=1.5512404896827733e+99\n",
      "Gradient Descent(45/49): loss=3.417665859135265e+101\n",
      "Gradient Descent(46/49): loss=7.529741521308329e+103\n",
      "Gradient Descent(47/49): loss=1.6589394550132552e+106\n",
      "Gradient Descent(48/49): loss=3.6549463319711903e+108\n",
      "Gradient Descent(49/49): loss=8.052513700376776e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.73605761993952\n",
      "Gradient Descent(2/49): loss=225.87951726348834\n",
      "Gradient Descent(3/49): loss=12837.935763242258\n",
      "Gradient Descent(4/49): loss=887940.7630892646\n",
      "Gradient Descent(5/49): loss=92151748.70638718\n",
      "Gradient Descent(6/49): loss=14767998530.321741\n",
      "Gradient Descent(7/49): loss=2957769190437.247\n",
      "Gradient Descent(8/49): loss=635920061803694.5\n",
      "Gradient Descent(9/49): loss=1.3928883627906978e+17\n",
      "Gradient Descent(10/49): loss=3.065007541277704e+19\n",
      "Gradient Descent(11/49): loss=6.752050972587796e+21\n",
      "Gradient Descent(12/49): loss=1.4878491152471532e+24\n",
      "Gradient Descent(13/49): loss=3.2787704706160516e+26\n",
      "Gradient Descent(14/49): loss=7.225537798770188e+28\n",
      "Gradient Descent(15/49): loss=1.5923225694625145e+31\n",
      "Gradient Descent(16/49): loss=3.509072494019536e+33\n",
      "Gradient Descent(17/49): loss=7.733101868209653e+35\n",
      "Gradient Descent(18/49): loss=1.7041787535902854e+38\n",
      "Gradient Descent(19/49): loss=3.755576111214044e+40\n",
      "Gradient Descent(20/49): loss=8.27633365609862e+42\n",
      "Gradient Descent(21/49): loss=1.8238932407282163e+45\n",
      "Gradient Descent(22/49): loss=4.019396380644061e+47\n",
      "Gradient Descent(23/49): loss=8.857726376057779e+49\n",
      "Gradient Descent(24/49): loss=1.9520173957427742e+52\n",
      "Gradient Descent(25/49): loss=4.301749401056935e+54\n",
      "Gradient Descent(26/49): loss=9.479960552536691e+56\n",
      "Gradient Descent(27/49): loss=2.0891419675814927e+59\n",
      "Gradient Descent(28/49): loss=4.603937048601496e+61\n",
      "Gradient Descent(29/49): loss=1.0145905197636722e+64\n",
      "Gradient Descent(30/49): loss=2.2358992139282293e+66\n",
      "Gradient Descent(31/49): loss=4.927352658498429e+68\n",
      "Gradient Descent(32/49): loss=1.0858630867603682e+71\n",
      "Gradient Descent(33/49): loss=2.3929658072171532e+73\n",
      "Gradient Descent(34/49): loss=5.273487444530875e+75\n",
      "Gradient Descent(35/49): loss=1.1621423817988567e+78\n",
      "Gradient Descent(36/49): loss=2.561065954511319e+80\n",
      "Gradient Descent(37/49): loss=5.643937374699684e+82\n",
      "Gradient Descent(38/49): loss=1.243780115596798e+85\n",
      "Gradient Descent(39/49): loss=2.7409747366948856e+87\n",
      "Gradient Descent(40/49): loss=6.040410529955097e+89\n",
      "Gradient Descent(41/49): loss=1.331152705711896e+92\n",
      "Gradient Descent(42/49): loss=2.9335216822378186e+94\n",
      "Gradient Descent(43/49): loss=6.464734979866243e+96\n",
      "Gradient Descent(44/49): loss=1.4246630121385083e+99\n",
      "Gradient Descent(45/49): loss=3.1395945920084367e+101\n",
      "Gradient Descent(46/49): loss=6.918867211532867e+103\n",
      "Gradient Descent(47/49): loss=1.524742194825892e+106\n",
      "Gradient Descent(48/49): loss=3.3601436327715943e+108\n",
      "Gradient Descent(49/49): loss=7.404901150613915e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.564917865681073\n",
      "Gradient Descent(2/49): loss=214.40813507854824\n",
      "Gradient Descent(3/49): loss=11872.921166106416\n",
      "Gradient Descent(4/49): loss=765913.3494540298\n",
      "Gradient Descent(5/49): loss=70771524.47790605\n",
      "Gradient Descent(6/49): loss=10456853822.27133\n",
      "Gradient Descent(7/49): loss=2050631155691.6658\n",
      "Gradient Descent(8/49): loss=443003004460054.9\n",
      "Gradient Descent(9/49): loss=9.819285108162912e+16\n",
      "Gradient Descent(10/49): loss=2.1902435690847048e+19\n",
      "Gradient Descent(11/49): loss=4.892878879368265e+21\n",
      "Gradient Descent(12/49): loss=1.0934389204141174e+24\n",
      "Gradient Descent(13/49): loss=2.44378168512228e+26\n",
      "Gradient Descent(14/49): loss=5.461844479668871e+28\n",
      "Gradient Descent(15/49): loss=1.220726629025126e+31\n",
      "Gradient Descent(16/49): loss=2.728337075327767e+33\n",
      "Gradient Descent(17/49): loss=6.097864290479691e+35\n",
      "Gradient Descent(18/49): loss=1.3628796741534874e+38\n",
      "Gradient Descent(19/49): loss=3.046051778221499e+40\n",
      "Gradient Descent(20/49): loss=6.807960855089467e+42\n",
      "Gradient Descent(21/49): loss=1.521587104300083e+45\n",
      "Gradient Descent(22/49): loss=3.40076473083543e+47\n",
      "Gradient Descent(23/49): loss=7.600748404365707e+49\n",
      "Gradient Descent(24/49): loss=1.6987760365597396e+52\n",
      "Gradient Descent(25/49): loss=3.7967840387150165e+54\n",
      "Gradient Descent(26/49): loss=8.485856126064367e+56\n",
      "Gradient Descent(27/49): loss=1.8965986334225533e+59\n",
      "Gradient Descent(28/49): loss=4.238919824780058e+61\n",
      "Gradient Descent(29/49): loss=9.47403470838113e+63\n",
      "Gradient Descent(30/49): loss=2.1174576865290348e+66\n",
      "Gradient Descent(31/49): loss=4.732542356293611e+68\n",
      "Gradient Descent(32/49): loss=1.0577286760722218e+71\n",
      "Gradient Descent(33/49): loss=2.3640357929341442e+73\n",
      "Gradient Descent(34/49): loss=5.283647268623472e+75\n",
      "Gradient Descent(35/49): loss=1.1809012597302073e+78\n",
      "Gradient Descent(36/49): loss=2.6393279383233637e+80\n",
      "Gradient Descent(37/49): loss=5.898928389326937e+82\n",
      "Gradient Descent(38/49): loss=1.3184172999931223e+85\n",
      "Gradient Descent(39/49): loss=2.946677874690112e+87\n",
      "Gradient Descent(40/49): loss=6.585859042682221e+89\n",
      "Gradient Descent(41/49): loss=1.4719470934582677e+92\n",
      "Gradient Descent(42/49): loss=3.2898187341978284e+94\n",
      "Gradient Descent(43/49): loss=7.352782822140345e+96\n",
      "Gradient Descent(44/49): loss=1.643355442887122e+99\n",
      "Gradient Descent(45/49): loss=3.6729183725307797e+101\n",
      "Gradient Descent(46/49): loss=8.209014933235248e+103\n",
      "Gradient Descent(47/49): loss=1.8347243074625932e+106\n",
      "Gradient Descent(48/49): loss=4.1006299924798567e+108\n",
      "Gradient Descent(49/49): loss=9.16495533788444e+110\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.644370348307102\n",
      "Gradient Descent(2/49): loss=221.7977819200465\n",
      "Gradient Descent(3/49): loss=12879.460082940619\n",
      "Gradient Descent(4/49): loss=957005.3342734847\n",
      "Gradient Descent(5/49): loss=112599036.29825498\n",
      "Gradient Descent(6/49): loss=20025746050.184727\n",
      "Gradient Descent(7/49): loss=4265529763146.8784\n",
      "Gradient Descent(8/49): loss=957012922704706.9\n",
      "Gradient Descent(9/49): loss=2.175001625871163e+17\n",
      "Gradient Descent(10/49): loss=4.958325000402763e+19\n",
      "Gradient Descent(11/49): loss=1.1311626433613116e+22\n",
      "Gradient Descent(12/49): loss=2.5810070347028833e+24\n",
      "Gradient Descent(13/49): loss=5.889395885339543e+26\n",
      "Gradient Descent(14/49): loss=1.3438673672884558e+29\n",
      "Gradient Descent(15/49): loss=3.0665004457327564e+31\n",
      "Gradient Descent(16/49): loss=6.997290152183554e+33\n",
      "Gradient Descent(17/49): loss=1.5966759619195152e+36\n",
      "Gradient Descent(18/49): loss=3.643373571198037e+38\n",
      "Gradient Descent(19/49): loss=8.313628679912883e+40\n",
      "Gradient Descent(20/49): loss=1.8970446095448457e+43\n",
      "Gradient Descent(21/49): loss=4.328769530748945e+45\n",
      "Gradient Descent(22/49): loss=9.877598849135481e+47\n",
      "Gradient Descent(23/49): loss=2.2539190024651896e+50\n",
      "Gradient Descent(24/49): loss=5.143103042844424e+52\n",
      "Gradient Descent(25/49): loss=1.1735785039486725e+55\n",
      "Gradient Descent(26/49): loss=2.6779290507255214e+57\n",
      "Gradient Descent(27/49): loss=6.110629989038812e+59\n",
      "Gradient Descent(28/49): loss=1.394353552900325e+62\n",
      "Gradient Descent(29/49): loss=3.1817043970478775e+64\n",
      "Gradient Descent(30/49): loss=7.260169308664197e+66\n",
      "Gradient Descent(31/49): loss=1.6566610788662834e+69\n",
      "Gradient Descent(32/49): loss=3.7802505885848986e+71\n",
      "Gradient Descent(33/49): loss=8.625961395964118e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(34/49): loss=1.968314223119175e+76\n",
      "Gradient Descent(35/49): loss=4.491396034702773e+78\n",
      "Gradient Descent(36/49): loss=1.0248687990770106e+81\n",
      "Gradient Descent(37/49): loss=2.33859594479308e+83\n",
      "Gradient Descent(38/49): loss=5.336323047328597e+85\n",
      "Gradient Descent(39/49): loss=1.217668393244833e+88\n",
      "Gradient Descent(40/49): loss=2.7785355248492936e+90\n",
      "Gradient Descent(41/49): loss=6.340198781276136e+92\n",
      "Gradient Descent(42/49): loss=1.4467376870510205e+95\n",
      "Gradient Descent(43/49): loss=3.301237086311812e+97\n",
      "Gradient Descent(44/49): loss=7.532924867848481e+99\n",
      "Gradient Descent(45/49): loss=1.7188997815375573e+102\n",
      "Gradient Descent(46/49): loss=3.922269916139198e+104\n",
      "Gradient Descent(47/49): loss=8.950028070449413e+106\n",
      "Gradient Descent(48/49): loss=2.042261348007412e+109\n",
      "Gradient Descent(49/49): loss=4.660132214932397e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.683390528246453\n",
      "Gradient Descent(2/49): loss=226.18954090845648\n",
      "Gradient Descent(3/49): loss=13114.844331511918\n",
      "Gradient Descent(4/49): loss=938692.726746628\n",
      "Gradient Descent(5/49): loss=101939587.73266461\n",
      "Gradient Descent(6/49): loss=16873028168.76691\n",
      "Gradient Descent(7/49): loss=3435415805922.7046\n",
      "Gradient Descent(8/49): loss=746214110698135.9\n",
      "Gradient Descent(9/49): loss=1.6485269562097325e+17\n",
      "Gradient Descent(10/49): loss=3.6572460259781734e+19\n",
      "Gradient Descent(11/49): loss=8.121941889538339e+21\n",
      "Gradient Descent(12/49): loss=1.8041596231058035e+24\n",
      "Gradient Descent(13/49): loss=4.00789882479615e+26\n",
      "Gradient Descent(14/49): loss=8.903588115178999e+28\n",
      "Gradient Descent(15/49): loss=1.9779484306816158e+31\n",
      "Gradient Descent(16/49): loss=4.3940526502079625e+33\n",
      "Gradient Descent(17/49): loss=9.76147942547241e+35\n",
      "Gradient Descent(18/49): loss=2.1685330887236918e+38\n",
      "Gradient Descent(19/49): loss=4.817441714668476e+40\n",
      "Gradient Descent(20/49): loss=1.0702047789204462e+43\n",
      "Gradient Descent(21/49): loss=2.3774823600366263e+45\n",
      "Gradient Descent(22/49): loss=5.281626922893959e+47\n",
      "Gradient Descent(23/49): loss=1.173324497494842e+50\n",
      "Gradient Descent(24/49): loss=2.606564978055526e+52\n",
      "Gradient Descent(25/49): loss=5.7905387634455876e+54\n",
      "Gradient Descent(26/49): loss=1.2863803301770135e+57\n",
      "Gradient Descent(27/49): loss=2.8577208813673614e+59\n",
      "Gradient Descent(28/49): loss=6.348486869882105e+61\n",
      "Gradient Descent(29/49): loss=1.4103296721470932e+64\n",
      "Gradient Descent(30/49): loss=3.1330769440110424e+66\n",
      "Gradient Descent(31/49): loss=6.960196137793585e+68\n",
      "Gradient Descent(32/49): loss=1.5462221688860408e+71\n",
      "Gradient Descent(33/49): loss=3.434964975444738e+73\n",
      "Gradient Descent(34/49): loss=7.630846730798434e+75\n",
      "Gradient Descent(35/49): loss=1.6952086046058523e+78\n",
      "Gradient Descent(36/49): loss=3.765941466929574e+80\n",
      "Gradient Descent(37/49): loss=8.366117947848192e+82\n",
      "Gradient Descent(38/49): loss=1.858550647479219e+85\n",
      "Gradient Descent(39/49): loss=4.128809240770646e+87\n",
      "Gradient Descent(40/49): loss=9.172236317473773e+89\n",
      "Gradient Descent(41/49): loss=2.037631533877414e+92\n",
      "Gradient Descent(42/49): loss=4.526641185576294e+94\n",
      "Gradient Descent(43/49): loss=1.0056028326163702e+97\n",
      "Gradient Descent(44/49): loss=2.233967782090335e+99\n",
      "Gradient Descent(45/49): loss=4.962806278531528e+101\n",
      "Gradient Descent(46/49): loss=1.102497822738802e+104\n",
      "Gradient Descent(47/49): loss=2.4492220347223435e+106\n",
      "Gradient Descent(48/49): loss=5.440998115050911e+108\n",
      "Gradient Descent(49/49): loss=1.2087291420821637e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.77517641356068\n",
      "Gradient Descent(2/49): loss=229.89863900637405\n",
      "Gradient Descent(3/49): loss=13180.555410214065\n",
      "Gradient Descent(4/49): loss=919257.0382143552\n",
      "Gradient Descent(5/49): loss=96144357.16241339\n",
      "Gradient Descent(6/49): loss=15527082997.41325\n",
      "Gradient Descent(7/49): loss=3134847291686.9053\n",
      "Gradient Descent(8/49): loss=679542922312682.1\n",
      "Gradient Descent(9/49): loss=1.50078929785204e+17\n",
      "Gradient Descent(10/49): loss=3.329905199062568e+19\n",
      "Gradient Descent(11/49): loss=7.396651144394043e+21\n",
      "Gradient Descent(12/49): loss=1.6434557516782156e+24\n",
      "Gradient Descent(13/49): loss=3.651825371733637e+26\n",
      "Gradient Descent(14/49): loss=8.114636476389379e+28\n",
      "Gradient Descent(15/49): loss=1.803141706775377e+31\n",
      "Gradient Descent(16/49): loss=4.0067391331656925e+33\n",
      "Gradient Descent(17/49): loss=8.903328113130636e+35\n",
      "Gradient Descent(18/49): loss=1.978398228144883e+38\n",
      "Gradient Descent(19/49): loss=4.396175849939706e+40\n",
      "Gradient Descent(20/49): loss=9.768691608176317e+42\n",
      "Gradient Descent(21/49): loss=2.1706896873886572e+45\n",
      "Gradient Descent(22/49): loss=4.8234645015654644e+47\n",
      "Gradient Descent(23/49): loss=1.0718164799954764e+50\n",
      "Gradient Descent(24/49): loss=2.3816710300877635e+52\n",
      "Gradient Descent(25/49): loss=5.2922837084940317e+54\n",
      "Gradient Descent(26/49): loss=1.1759922549076627e+57\n",
      "Gradient Descent(27/49): loss=2.6131588172104334e+59\n",
      "Gradient Descent(28/49): loss=5.806670048606013e+61\n",
      "Gradient Descent(29/49): loss=1.2902934498780662e+64\n",
      "Gradient Descent(30/49): loss=2.867146183375702e+66\n",
      "Gradient Descent(31/49): loss=6.37105244363031e+68\n",
      "Gradient Descent(32/49): loss=1.4157042105086505e+71\n",
      "Gradient Descent(33/49): loss=3.1458199871761084e+73\n",
      "Gradient Descent(34/49): loss=6.990290286811424e+75\n",
      "Gradient Descent(35/49): loss=1.5533043369641238e+78\n",
      "Gradient Descent(36/49): loss=3.451579640095503e+80\n",
      "Gradient Descent(37/49): loss=7.669715282715072e+82\n",
      "Gradient Descent(38/49): loss=1.7042785811625408e+85\n",
      "Gradient Descent(39/49): loss=3.787057765697315e+87\n",
      "Gradient Descent(40/49): loss=8.415177353778207e+89\n",
      "Gradient Descent(41/49): loss=1.869926847617139e+92\n",
      "Gradient Descent(42/49): loss=4.1551428667982175e+94\n",
      "Gradient Descent(43/49): loss=9.233095008772881e+96\n",
      "Gradient Descent(44/49): loss=2.0516753857543424e+99\n",
      "Gradient Descent(45/49): loss=4.559004195787633e+101\n",
      "Gradient Descent(46/49): loss=1.0130510606855699e+104\n",
      "Gradient Descent(47/49): loss=2.2510890700746124e+106\n",
      "Gradient Descent(48/49): loss=5.002119042232691e+108\n",
      "Gradient Descent(49/49): loss=1.1115150993042379e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.6025089802712\n",
      "Gradient Descent(2/49): loss=218.22572386782684\n",
      "Gradient Descent(3/49): loss=12190.392251811709\n",
      "Gradient Descent(4/49): loss=793049.1204060338\n",
      "Gradient Descent(5/49): loss=73854115.40309992\n",
      "Gradient Descent(6/49): loss=10995593839.683636\n",
      "Gradient Descent(7/49): loss=2173443158931.7124\n",
      "Gradient Descent(8/49): loss=473384240831064.0\n",
      "Gradient Descent(9/49): loss=1.0579627908474918e+17\n",
      "Gradient Descent(10/49): loss=2.3794533001447838e+19\n",
      "Gradient Descent(11/49): loss=5.359773958636597e+21\n",
      "Gradient Descent(12/49): loss=1.207743136404325e+24\n",
      "Gradient Descent(13/49): loss=2.7217028554482532e+26\n",
      "Gradient Descent(14/49): loss=6.133606952762606e+28\n",
      "Gradient Descent(15/49): loss=1.3822715040023863e+31\n",
      "Gradient Descent(16/49): loss=3.115094951299714e+33\n",
      "Gradient Descent(17/49): loss=7.020197780710213e+35\n",
      "Gradient Descent(18/49): loss=1.5820763424961424e+38\n",
      "Gradient Descent(19/49): loss=3.5653776045729543e+40\n",
      "Gradient Descent(20/49): loss=8.034958346723425e+42\n",
      "Gradient Descent(21/49): loss=1.8107634829949298e+45\n",
      "Gradient Descent(22/49): loss=4.080748462654388e+47\n",
      "Gradient Descent(23/49): loss=9.196401502869635e+49\n",
      "Gradient Descent(24/49): loss=2.0725070750394634e+52\n",
      "Gradient Descent(25/49): loss=4.670615538873281e+54\n",
      "Gradient Descent(26/49): loss=1.052572981521175e+57\n",
      "Gradient Descent(27/49): loss=2.3720853754874735e+59\n",
      "Gradient Descent(28/49): loss=5.345747161845164e+61\n",
      "Gradient Descent(29/49): loss=1.2047210869255957e+64\n",
      "Gradient Descent(30/49): loss=2.7149673438394253e+66\n",
      "Gradient Descent(31/49): loss=6.118468214850674e+68\n",
      "Gradient Descent(32/49): loss=1.3788620102957679e+71\n",
      "Gradient Descent(33/49): loss=3.1074124710203994e+73\n",
      "Gradient Descent(34/49): loss=7.002885127701646e+75\n",
      "Gradient Descent(35/49): loss=1.5781747859073908e+78\n",
      "Gradient Descent(36/49): loss=3.556585049526831e+80\n",
      "Gradient Descent(37/49): loss=8.015143396962321e+82\n",
      "Gradient Descent(38/49): loss=1.8062979734567716e+85\n",
      "Gradient Descent(39/49): loss=4.070684961357689e+87\n",
      "Gradient Descent(40/49): loss=9.173722330492424e+89\n",
      "Gradient Descent(41/49): loss=2.0673960818846513e+92\n",
      "Gradient Descent(42/49): loss=4.659097371178649e+94\n",
      "Gradient Descent(43/49): loss=1.0499772396944465e+97\n",
      "Gradient Descent(44/49): loss=2.366235594680213e+99\n",
      "Gradient Descent(45/49): loss=5.3325640574464144e+101\n",
      "Gradient Descent(46/49): loss=1.2017501338708602e+104\n",
      "Gradient Descent(47/49): loss=2.708271984547353e+106\n",
      "Gradient Descent(48/49): loss=6.103379509231892e+108\n",
      "Gradient Descent(49/49): loss=1.375461609700131e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.682502161900499\n",
      "Gradient Descent(2/49): loss=225.72742698596264\n",
      "Gradient Descent(3/49): loss=13221.262579689836\n",
      "Gradient Descent(4/49): loss=990470.7016375503\n",
      "Gradient Descent(5/49): loss=117431775.65576036\n",
      "Gradient Descent(6/49): loss=21047693709.080112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7/49): loss=4519432294508.3955\n",
      "Gradient Descent(8/49): loss=1022314418943964.4\n",
      "Gradient Descent(9/49): loss=2.342615934078991e+17\n",
      "Gradient Descent(10/49): loss=5.384639714652408e+19\n",
      "Gradient Descent(11/49): loss=1.2385920862861524e+22\n",
      "Gradient Descent(12/49): loss=2.849537649935186e+24\n",
      "Gradient Descent(13/49): loss=6.55598608148932e+26\n",
      "Gradient Descent(14/49): loss=1.5083626536368847e+29\n",
      "Gradient Descent(15/49): loss=3.47035968699209e+31\n",
      "Gradient Descent(16/49): loss=7.984421162922347e+33\n",
      "Gradient Descent(17/49): loss=1.8370138818604487e+36\n",
      "Gradient Descent(18/49): loss=4.2265056309416495e+38\n",
      "Gradient Descent(19/49): loss=9.724123561227143e+40\n",
      "Gradient Descent(20/49): loss=2.237275599360413e+43\n",
      "Gradient Descent(21/49): loss=5.147406938917349e+45\n",
      "Gradient Descent(22/49): loss=1.1842885250596051e+48\n",
      "Gradient Descent(23/49): loss=2.724749232498777e+50\n",
      "Gradient Descent(24/49): loss=6.268960834240831e+52\n",
      "Gradient Descent(25/49): loss=1.442329792134651e+55\n",
      "Gradient Descent(26/49): loss=3.3184371130812524e+57\n",
      "Gradient Descent(27/49): loss=7.634886926365106e+59\n",
      "Gradient Descent(28/49): loss=1.7565949388824414e+62\n",
      "Gradient Descent(29/49): loss=4.0414819617721e+64\n",
      "Gradient Descent(30/49): loss=9.298430779790862e+66\n",
      "Gradient Descent(31/49): loss=2.1393344269351224e+69\n",
      "Gradient Descent(32/49): loss=4.922069001381241e+71\n",
      "Gradient Descent(33/49): loss=1.1324439484230748e+74\n",
      "Gradient Descent(34/49): loss=2.605467936268565e+76\n",
      "Gradient Descent(35/49): loss=5.9945246529654165e+78\n",
      "Gradient Descent(36/49): loss=1.3791889477816634e+81\n",
      "Gradient Descent(37/49): loss=3.1731659536041775e+83\n",
      "Gradient Descent(38/49): loss=7.300654624087509e+85\n",
      "Gradient Descent(39/49): loss=1.6796965150742075e+88\n",
      "Gradient Descent(40/49): loss=3.8645580814679478e+90\n",
      "Gradient Descent(41/49): loss=8.891373549334165e+92\n",
      "Gradient Descent(42/49): loss=2.045680823711921e+95\n",
      "Gradient Descent(43/49): loss=4.706595678702494e+97\n",
      "Gradient Descent(44/49): loss=1.0828689708586064e+100\n",
      "Gradient Descent(45/49): loss=2.4914084151193374e+102\n",
      "Gradient Descent(46/49): loss=5.7321024592715844e+104\n",
      "Gradient Descent(47/49): loss=1.3188122189919682e+107\n",
      "Gradient Descent(48/49): loss=3.0342543269604336e+109\n",
      "Gradient Descent(49/49): loss=6.98105400305978e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.721855301166969\n",
      "Gradient Descent(2/49): loss=230.1948947462872\n",
      "Gradient Descent(3/49): loss=13463.030082540468\n",
      "Gradient Descent(4/49): loss=971601.9606085854\n",
      "Gradient Descent(5/49): loss=106330356.67429686\n",
      "Gradient Descent(6/49): loss=17736031495.89154\n",
      "Gradient Descent(7/49): loss=3640176629915.4624\n",
      "Gradient Descent(8/49): loss=797182475245900.4\n",
      "Gradient Descent(9/49): loss=1.7756837270640963e+17\n",
      "Gradient Descent(10/49): loss=3.9719712903923835e+19\n",
      "Gradient Descent(11/49): loss=8.893979995527659e+21\n",
      "Gradient Descent(12/49): loss=1.9920311234067834e+24\n",
      "Gradient Descent(13/49): loss=4.4619319980370636e+26\n",
      "Gradient Descent(14/49): loss=9.994390979039662e+28\n",
      "Gradient Descent(15/49): loss=2.2386764217331083e+31\n",
      "Gradient Descent(16/49): loss=5.014489275393496e+33\n",
      "Gradient Descent(17/49): loss=1.123213162209455e+36\n",
      "Gradient Descent(18/49): loss=2.5159249652501063e+38\n",
      "Gradient Descent(19/49): loss=5.635509560311598e+40\n",
      "Gradient Descent(20/49): loss=1.2623177775527964e+43\n",
      "Gradient Descent(21/49): loss=2.8275103710164545e+45\n",
      "Gradient Descent(22/49): loss=6.333440787972331e+47\n",
      "Gradient Descent(23/49): loss=1.4186498704939304e+50\n",
      "Gradient Descent(24/49): loss=3.1776841727128867e+52\n",
      "Gradient Descent(25/49): loss=7.117807509489529e+54\n",
      "Gradient Descent(26/49): loss=1.5943429550749945e+57\n",
      "Gradient Descent(27/49): loss=3.5712253457385476e+59\n",
      "Gradient Descent(28/49): loss=7.999314344162336e+61\n",
      "Gradient Descent(29/49): loss=1.791794798193812e+64\n",
      "Gradient Descent(30/49): loss=4.0135047339118475e+66\n",
      "Gradient Descent(31/49): loss=8.98999163596775e+68\n",
      "Gradient Descent(32/49): loss=2.0137001192969195e+71\n",
      "Gradient Descent(33/49): loss=4.5105583349298855e+73\n",
      "Gradient Descent(34/49): loss=1.0103359630285405e+76\n",
      "Gradient Descent(35/49): loss=2.2630873661113522e+78\n",
      "Gradient Descent(36/49): loss=5.069169676293253e+80\n",
      "Gradient Descent(37/49): loss=1.1354612991015408e+83\n",
      "Gradient Descent(38/49): loss=2.543360045308528e+85\n",
      "Gradient Descent(39/49): loss=5.696962393337652e+87\n",
      "Gradient Descent(40/49): loss=1.2760828169401133e+90\n",
      "Gradient Descent(41/49): loss=2.858343171782568e+92\n",
      "Gradient Descent(42/49): loss=6.402504272619829e+94\n",
      "Gradient Descent(43/49): loss=1.434119645450081e+97\n",
      "Gradient Descent(44/49): loss=3.2123354704522372e+99\n",
      "Gradient Descent(45/49): loss=7.195424180586625e+101\n",
      "Gradient Descent(46/49): loss=1.6117285885861105e+104\n",
      "Gradient Descent(47/49): loss=3.6101680430103025e+106\n",
      "Gradient Descent(48/49): loss=8.086543473306856e+108\n",
      "Gradient Descent(49/49): loss=1.8113335602836765e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.814470680663596\n",
      "Gradient Descent(2/49): loss=233.97126445357992\n",
      "Gradient Descent(3/49): loss=13530.764472200186\n",
      "Gradient Descent(4/49): loss=951536.3147916697\n",
      "Gradient Descent(5/49): loss=100292132.14757897\n",
      "Gradient Descent(6/49): loss=16321797868.521568\n",
      "Gradient Descent(7/49): loss=3321729249472.8613\n",
      "Gradient Descent(8/49): loss=725959391929740.1\n",
      "Gradient Descent(9/49): loss=1.6165509457406064e+17\n",
      "Gradient Descent(10/49): loss=3.616459473709415e+19\n",
      "Gradient Descent(11/49): loss=8.099740691283308e+21\n",
      "Gradient Descent(12/49): loss=1.8145913558419201e+24\n",
      "Gradient Descent(13/49): loss=4.0655174292536566e+26\n",
      "Gradient Descent(14/49): loss=9.10877429686468e+28\n",
      "Gradient Descent(15/49): loss=2.040825108162774e+31\n",
      "Gradient Descent(16/49): loss=4.5724825691081713e+33\n",
      "Gradient Descent(17/49): loss=1.024468079447055e+36\n",
      "Gradient Descent(18/49): loss=2.29532839189971e+38\n",
      "Gradient Descent(19/49): loss=5.14270049618148e+40\n",
      "Gradient Descent(20/49): loss=1.1522259114195196e+43\n",
      "Gradient Descent(21/49): loss=2.5815708168005875e+45\n",
      "Gradient Descent(22/49): loss=5.784028824080755e+47\n",
      "Gradient Descent(23/49): loss=1.295916006708785e+50\n",
      "Gradient Descent(24/49): loss=2.9035095563014534e+52\n",
      "Gradient Descent(25/49): loss=6.505334990783146e+54\n",
      "Gradient Descent(26/49): loss=1.4575251956892012e+57\n",
      "Gradient Descent(27/49): loss=3.2655961592740156e+59\n",
      "Gradient Descent(28/49): loss=7.316592747079624e+61\n",
      "Gradient Descent(29/49): loss=1.6392881059279458e+64\n",
      "Gradient Descent(30/49): loss=3.672837326239714e+66\n",
      "Gradient Descent(31/49): loss=8.229019643489314e+68\n",
      "Gradient Descent(32/49): loss=1.8437180380722938e+71\n",
      "Gradient Descent(33/49): loss=4.130864126205691e+73\n",
      "Gradient Descent(34/49): loss=9.25523213246536e+75\n",
      "Gradient Descent(35/49): loss=2.073641717780282e+78\n",
      "Gradient Descent(36/49): loss=4.6460098592614493e+80\n",
      "Gradient Descent(37/49): loss=1.0409420020475119e+83\n",
      "Gradient Descent(38/49): loss=2.33223838185944e+85\n",
      "Gradient Descent(39/49): loss=5.225397629377362e+87\n",
      "Gradient Descent(40/49): loss=1.1707542675518827e+90\n",
      "Gradient Descent(41/49): loss=2.6230837387091467e+92\n",
      "Gradient Descent(42/49): loss=5.877038838105756e+94\n",
      "Gradient Descent(43/49): loss=1.316754970300761e+97\n",
      "Gradient Descent(44/49): loss=2.950199410917271e+99\n",
      "Gradient Descent(45/49): loss=6.609943961091262e+101\n",
      "Gradient Descent(46/49): loss=1.4809629141368065e+104\n",
      "Gradient Descent(47/49): loss=3.318108543671341e+106\n",
      "Gradient Descent(48/49): loss=7.434247139133966e+108\n",
      "Gradient Descent(49/49): loss=1.6656486609256332e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.640268887989668\n",
      "Gradient Descent(2/49): loss=222.09417211969313\n",
      "Gradient Descent(3/49): loss=12514.908504669496\n",
      "Gradient Descent(4/49): loss=821023.0239556676\n",
      "Gradient Descent(5/49): loss=77057150.23886354\n",
      "Gradient Descent(6/49): loss=11559683592.794872\n",
      "Gradient Descent(7/49): loss=2303057324512.4966\n",
      "Gradient Descent(8/49): loss=505710614883629.7\n",
      "Gradient Descent(9/49): loss=1.1395339999699894e+17\n",
      "Gradient Descent(10/49): loss=2.5841248861402755e+19\n",
      "Gradient Descent(11/49): loss=5.869014503327319e+21\n",
      "Gradient Descent(12/49): loss=1.3334492226165055e+24\n",
      "Gradient Descent(13/49): loss=3.0298841389803358e+26\n",
      "Gradient Descent(14/49): loss=6.884695221692245e+28\n",
      "Gradient Descent(15/49): loss=1.5643920857861792e+31\n",
      "Gradient Descent(16/49): loss=3.554733331342717e+33\n",
      "Gradient Descent(17/49): loss=8.077343801365143e+35\n",
      "Gradient Descent(18/49): loss=1.835397520143184e+38\n",
      "Gradient Descent(19/49): loss=4.170534516977548e+40\n",
      "Gradient Descent(20/49): loss=9.476616392693423e+42\n",
      "Gradient Descent(21/49): loss=2.1533512784569796e+45\n",
      "Gradient Descent(22/49): loss=4.893014064666963e+47\n",
      "Gradient Descent(23/49): loss=1.1118291231838922e+50\n",
      "Gradient Descent(24/49): loss=2.526385542405261e+52\n",
      "Gradient Descent(25/49): loss=5.740651846406777e+54\n",
      "Gradient Descent(26/49): loss=1.3044360438471848e+57\n",
      "Gradient Descent(27/49): loss=2.9640421297330633e+59\n",
      "Gradient Descent(28/49): loss=6.735129551405068e+61\n",
      "Gradient Descent(29/49): loss=1.5304090862667814e+64\n",
      "Gradient Descent(30/49): loss=3.477515842051296e+66\n",
      "Gradient Descent(31/49): loss=7.901884888319263e+68\n",
      "Gradient Descent(32/49): loss=1.7955284066057688e+71\n",
      "Gradient Descent(33/49): loss=4.0799408046224224e+73\n",
      "Gradient Descent(34/49): loss=9.27076224914219e+75\n",
      "Gradient Descent(35/49): loss=2.1065754822409307e+78\n",
      "Gradient Descent(36/49): loss=4.786726423481776e+80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(37/49): loss=1.0876776097709181e+83\n",
      "Gradient Descent(38/49): loss=2.4715065749182094e+85\n",
      "Gradient Descent(39/49): loss=5.6159515420662e+87\n",
      "Gradient Descent(40/49): loss=1.2761006603383163e+90\n",
      "Gradient Descent(41/49): loss=2.899656243680405e+92\n",
      "Gradient Descent(42/49): loss=6.5888268792883384e+94\n",
      "Gradient Descent(43/49): loss=1.4971650429200605e+97\n",
      "Gradient Descent(44/49): loss=3.4019761132106126e+99\n",
      "Gradient Descent(45/49): loss=7.730237577737198e+101\n",
      "Gradient Descent(46/49): loss=1.7565253552549493e+104\n",
      "Gradient Descent(47/49): loss=3.9913150050385605e+106\n",
      "Gradient Descent(48/49): loss=9.069379739829345e+108\n",
      "Gradient Descent(49/49): loss=2.0608157652651405e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.720804387560544\n",
      "Gradient Descent(2/49): loss=229.70916577269807\n",
      "Gradient Descent(3/49): loss=13570.594875046274\n",
      "Gradient Descent(4/49): loss=1024956.5801255595\n",
      "Gradient Descent(5/49): loss=122450564.18754472\n",
      "Gradient Descent(6/49): loss=22117262511.196518\n",
      "Gradient Descent(7/49): loss=4787310922595.796\n",
      "Gradient Descent(8/49): loss=1091775721038094.0\n",
      "Gradient Descent(9/49): loss=2.5223780693408106e+17\n",
      "Gradient Descent(10/49): loss=5.8456281736453005e+19\n",
      "Gradient Descent(11/49): loss=1.355719099296204e+22\n",
      "Gradient Descent(12/49): loss=3.144727843443375e+24\n",
      "Gradient Descent(13/49): loss=7.294810932699304e+26\n",
      "Gradient Descent(14/49): loss=1.69219015766399e+29\n",
      "Gradient Descent(15/49): loss=3.9254121864534156e+31\n",
      "Gradient Descent(16/49): loss=9.105873195671888e+33\n",
      "Gradient Descent(17/49): loss=2.1123116010157934e+36\n",
      "Gradient Descent(18/49): loss=4.899980853031691e+38\n",
      "Gradient Descent(19/49): loss=1.1366605435800495e+41\n",
      "Gradient Descent(20/49): loss=2.6367392693738474e+43\n",
      "Gradient Descent(21/49): loss=6.116508588772001e+45\n",
      "Gradient Descent(22/49): loss=1.4188614609887647e+48\n",
      "Gradient Descent(23/49): loss=3.291367642673727e+50\n",
      "Gradient Descent(24/49): loss=7.635066042138079e+52\n",
      "Gradient Descent(25/49): loss=1.7711249485494998e+55\n",
      "Gradient Descent(26/49): loss=4.1085218727156294e+57\n",
      "Gradient Descent(27/49): loss=9.530638700792125e+59\n",
      "Gradient Descent(28/49): loss=2.210845575588889e+62\n",
      "Gradient Descent(29/49): loss=5.128552568774754e+64\n",
      "Gradient Descent(30/49): loss=1.1896828860911904e+67\n",
      "Gradient Descent(31/49): loss=2.759736495781718e+69\n",
      "Gradient Descent(32/49): loss=6.40182826465067e+71\n",
      "Gradient Descent(33/49): loss=1.485047764260191e+74\n",
      "Gradient Descent(34/49): loss=3.444901629604246e+76\n",
      "Gradient Descent(35/49): loss=7.99122258775448e+78\n",
      "Gradient Descent(36/49): loss=1.8537434537535272e+81\n",
      "Gradient Descent(37/49): loss=4.300174040452649e+83\n",
      "Gradient Descent(38/49): loss=9.975218922953156e+85\n",
      "Gradient Descent(39/49): loss=2.3139759373638817e+88\n",
      "Gradient Descent(40/49): loss=5.367786592009784e+90\n",
      "Gradient Descent(41/49): loss=1.2451785877335162e+93\n",
      "Gradient Descent(42/49): loss=2.888471232552225e+95\n",
      "Gradient Descent(43/49): loss=6.700457382959236e+97\n",
      "Gradient Descent(44/49): loss=1.5543214914133806e+100\n",
      "Gradient Descent(45/49): loss=3.6055975892238853e+102\n",
      "Gradient Descent(46/49): loss=8.363992936619391e+104\n",
      "Gradient Descent(47/49): loss=1.9402159035412254e+107\n",
      "Gradient Descent(48/49): loss=4.5007662977245445e+109\n",
      "Gradient Descent(49/49): loss=1.044053768952218e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.760491889384403\n",
      "Gradient Descent(2/49): loss=234.25331527957468\n",
      "Gradient Descent(3/49): loss=13818.888713115357\n",
      "Gradient Descent(4/49): loss=1005517.2085534952\n",
      "Gradient Descent(5/49): loss=110890762.3160581\n",
      "Gradient Descent(6/49): loss=18639342348.451157\n",
      "Gradient Descent(7/49): loss=3856223666072.6836\n",
      "Gradient Descent(8/49): loss=851400845976733.5\n",
      "Gradient Descent(9/49): loss=1.912064475793653e+17\n",
      "Gradient Descent(10/49): loss=4.3123167703601684e+19\n",
      "Gradient Descent(11/49): loss=9.735770217848429e+21\n",
      "Gradient Descent(12/49): loss=2.198570722857361e+24\n",
      "Gradient Descent(13/49): loss=4.965209396165226e+26\n",
      "Gradient Descent(14/49): loss=1.1213502810573173e+29\n",
      "Gradient Descent(15/49): loss=2.53248357159559e+31\n",
      "Gradient Descent(16/49): loss=5.719425026616416e+33\n",
      "Gradient Descent(17/49): loss=1.2916897174508967e+36\n",
      "Gradient Descent(18/49): loss=2.9171855716967534e+38\n",
      "Gradient Descent(19/49): loss=6.588247670838891e+40\n",
      "Gradient Descent(20/49): loss=1.48790697218414e+43\n",
      "Gradient Descent(21/49): loss=3.3603277688309955e+45\n",
      "Gradient Descent(22/49): loss=7.589051551780993e+47\n",
      "Gradient Descent(23/49): loss=1.7139311228067588e+50\n",
      "Gradient Descent(24/49): loss=3.8707865847480535e+52\n",
      "Gradient Descent(25/49): loss=8.741885006540479e+54\n",
      "Gradient Descent(26/49): loss=1.974290025927764e+57\n",
      "Gradient Descent(27/49): loss=4.458787897075159e+59\n",
      "Gradient Descent(28/49): loss=1.0069842449699108e+62\n",
      "Gradient Descent(29/49): loss=2.2741993856284622e+64\n",
      "Gradient Descent(30/49): loss=5.136110988258365e+66\n",
      "Gradient Descent(31/49): loss=1.159952651927147e+69\n",
      "Gradient Descent(32/49): loss=2.619667210830831e+71\n",
      "Gradient Descent(33/49): loss=5.91632450178063e+73\n",
      "Gradient Descent(34/49): loss=1.3361580992293057e+76\n",
      "Gradient Descent(35/49): loss=3.0176141717695736e+78\n",
      "Gradient Descent(36/49): loss=6.815058259136182e+80\n",
      "Gradient Descent(37/49): loss=1.5391304663771872e+83\n",
      "Gradient Descent(38/49): loss=3.476012240034344e+85\n",
      "Gradient Descent(39/49): loss=7.850316368117157e+87\n",
      "Gradient Descent(40/49): loss=1.7729358478587283e+90\n",
      "Gradient Descent(41/49): loss=4.004044389075333e+92\n",
      "Gradient Descent(42/49): loss=9.042837894585534e+94\n",
      "Gradient Descent(43/49): loss=2.042258008199453e+97\n",
      "Gradient Descent(44/49): loss=4.612288554406247e+99\n",
      "Gradient Descent(45/49): loss=1.0416512322976133e+102\n",
      "Gradient Descent(46/49): loss=2.3524922106414806e+104\n",
      "Gradient Descent(47/49): loss=5.312929538730377e+106\n",
      "Gradient Descent(48/49): loss=1.199885812834077e+109\n",
      "Gradient Descent(49/49): loss=2.709853299098183e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.85394042124827\n",
      "Gradient Descent(2/49): loss=238.09786654319763\n",
      "Gradient Descent(3/49): loss=13888.696900017798\n",
      "Gradient Descent(4/49): loss=984803.885460922\n",
      "Gradient Descent(5/49): loss=104600416.12523027\n",
      "Gradient Descent(6/49): loss=17153657835.314495\n",
      "Gradient Descent(7/49): loss=3518914055996.757\n",
      "Gradient Descent(8/49): loss=775335747239456.0\n",
      "Gradient Descent(9/49): loss=1.7407100034235187e+17\n",
      "Gradient Descent(10/49): loss=3.9263405796946395e+19\n",
      "Gradient Descent(11/49): loss=8.866352502977436e+21\n",
      "Gradient Descent(12/49): loss=2.002731935087863e+24\n",
      "Gradient Descent(13/49): loss=4.5240779573860285e+26\n",
      "Gradient Descent(14/49): loss=1.0219849581682197e+29\n",
      "Gradient Descent(15/49): loss=2.3086636926380886e+31\n",
      "Gradient Descent(16/49): loss=5.215275645329997e+33\n",
      "Gradient Descent(17/49): loss=1.1781320348363192e+36\n",
      "Gradient Descent(18/49): loss=2.661403282328503e+38\n",
      "Gradient Descent(19/49): loss=6.012116912087356e+40\n",
      "Gradient Descent(20/49): loss=1.358138773219283e+43\n",
      "Gradient Descent(21/49): loss=3.0680390215956413e+45\n",
      "Gradient Descent(22/49): loss=6.930708132014776e+47\n",
      "Gradient Descent(23/49): loss=1.5656487702878218e+50\n",
      "Gradient Descent(24/49): loss=3.5368046457049297e+52\n",
      "Gradient Descent(25/49): loss=7.989650896999915e+54\n",
      "Gradient Descent(26/49): loss=1.804864216447367e+57\n",
      "Gradient Descent(27/49): loss=4.077192961004039e+59\n",
      "Gradient Descent(28/49): loss=9.210389507296327e+61\n",
      "Gradient Descent(29/49): loss=2.080629386135955e+64\n",
      "Gradient Descent(30/49): loss=4.700147196840088e+66\n",
      "Gradient Descent(31/49): loss=1.0617644746905656e+69\n",
      "Gradient Descent(32/49): loss=2.3985287109153574e+71\n",
      "Gradient Descent(33/49): loss=5.418282598654309e+73\n",
      "Gradient Descent(34/49): loss=1.2239914488110062e+76\n",
      "Gradient Descent(35/49): loss=2.764999867549384e+78\n",
      "Gradient Descent(36/49): loss=6.246141895006585e+80\n",
      "Gradient Descent(37/49): loss=1.4110050792564686e+83\n",
      "Gradient Descent(38/49): loss=3.18746414531375e+85\n",
      "Gradient Descent(39/49): loss=7.20048979767982e+87\n",
      "Gradient Descent(40/49): loss=1.6265925187807471e+90\n",
      "Gradient Descent(41/49): loss=3.6744767321328405e+92\n",
      "Gradient Descent(42/49): loss=8.300652498455196e+94\n",
      "Gradient Descent(43/49): loss=1.8751195591355776e+97\n",
      "Gradient Descent(44/49): loss=4.235899962933351e+99\n",
      "Gradient Descent(45/49): loss=9.568909037592164e+101\n",
      "Gradient Descent(46/49): loss=2.1616190413124156e+104\n",
      "Gradient Descent(47/49): loss=4.883103038609648e+106\n",
      "Gradient Descent(48/49): loss=1.103094246949304e+109\n",
      "Gradient Descent(49/49): loss=2.491892773983179e+111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.6781975888364835\n",
      "Gradient Descent(2/49): loss=226.01392973516414\n",
      "Gradient Descent(3/49): loss=12846.594485701722\n",
      "Gradient Descent(4/49): loss=849857.1671437402\n",
      "Gradient Descent(5/49): loss=80384798.3956362\n",
      "Gradient Descent(6/49): loss=12150203507.55098\n",
      "Gradient Descent(7/49): loss=2439820022427.683\n",
      "Gradient Descent(8/49): loss=540097875221124.75\n",
      "Gradient Descent(9/49): loss=1.2270200245080966e+17\n",
      "Gradient Descent(10/49): loss=2.8054502550485365e+19\n",
      "Gradient Descent(11/49): loss=6.42424259202829e+21\n",
      "Gradient Descent(12/49): loss=1.471640413009897e+24\n",
      "Gradient Descent(13/49): loss=3.371474891562717e+26\n",
      "Gradient Descent(14/49): loss=7.7240907544443355e+28\n",
      "Gradient Descent(15/49): loss=1.7696077593890155e+31\n",
      "Gradient Descent(16/49): loss=4.054218853981412e+33\n",
      "Gradient Descent(17/49): loss=9.288326877090066e+35\n",
      "Gradient Descent(18/49): loss=2.1279813773391607e+38\n",
      "Gradient Descent(19/49): loss=4.875264273218578e+40\n",
      "Gradient Descent(20/49): loss=1.1169365522774203e+43\n",
      "Gradient Descent(21/49): loss=2.55893258928922e+45\n",
      "Gradient Descent(22/49): loss=5.862585466188174e+47\n",
      "Gradient Descent(23/49): loss=1.343134574709734e+50\n",
      "Gradient Descent(24/49): loss=3.0771585270850486e+52\n",
      "Gradient Descent(25/49): loss=7.049855449435146e+54\n",
      "Gradient Descent(26/49): loss=1.6151414176587395e+57\n",
      "Gradient Descent(27/49): loss=3.700333741234611e+59\n",
      "Gradient Descent(28/49): loss=8.477567132398931e+61\n",
      "Gradient Descent(29/49): loss=1.942234120221648e+64\n",
      "Gradient Descent(30/49): loss=4.449712186101763e+66\n",
      "Gradient Descent(31/49): loss=1.019441391385029e+69\n",
      "Gradient Descent(32/49): loss=2.3355684750017963e+71\n",
      "Gradient Descent(33/49): loss=5.3508520916646776e+73\n",
      "Gradient Descent(34/49): loss=1.2258950406860002e+76\n",
      "Gradient Descent(35/49): loss=2.8085595061009996e+78\n",
      "Gradient Descent(36/49): loss=6.434487649853091e+80\n",
      "Gradient Descent(37/49): loss=1.4741589496741548e+83\n",
      "Gradient Descent(38/49): loss=3.377339000649142e+85\n",
      "Gradient Descent(39/49): loss=7.737577231971461e+87\n",
      "Gradient Descent(40/49): loss=1.772700383622015e+90\n",
      "Gradient Descent(41/49): loss=4.0613056980019546e+92\n",
      "Gradient Descent(42/49): loss=9.304563887396673e+94\n",
      "Gradient Descent(43/49): loss=2.131701368287499e+97\n",
      "Gradient Descent(44/49): loss=4.88378690130124e+99\n",
      "Gradient Descent(45/49): loss=1.1188891113994361e+102\n",
      "Gradient Descent(46/49): loss=2.5634059571162223e+104\n",
      "Gradient Descent(47/49): loss=5.8728340762566005e+106\n",
      "Gradient Descent(48/49): loss=1.3454825596973237e+109\n",
      "Gradient Descent(49/49): loss=3.08253782576398e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.759277025287236\n",
      "Gradient Descent(2/49): loss=233.74345684928522\n",
      "Gradient Descent(3/49): loss=13927.589153931953\n",
      "Gradient Descent(4/49): loss=1060489.5538006788\n",
      "Gradient Descent(5/49): loss=127661747.24171287\n",
      "Gradient Descent(6/49): loss=23236462067.931297\n",
      "Gradient Descent(7/49): loss=5069872879522.678\n",
      "Gradient Descent(8/49): loss=1165643115903992.5\n",
      "Gradient Descent(9/49): loss=2.7151131659479882e+17\n",
      "Gradient Descent(10/49): loss=6.343950792322592e+19\n",
      "Gradient Descent(11/49): loss=1.4833738500069847e+22\n",
      "Gradient Descent(12/49): loss=3.469098504221204e+24\n",
      "Gradient Descent(13/49): loss=8.113352879780766e+26\n",
      "Gradient Descent(14/49): loss=1.897528355920548e+29\n",
      "Gradient Descent(15/49): loss=4.4378964786415085e+31\n",
      "Gradient Descent(16/49): loss=1.0379257629922071e+34\n",
      "Gradient Descent(17/49): loss=2.4274789421719505e+36\n",
      "Gradient Descent(18/49): loss=5.677336857191257e+38\n",
      "Gradient Descent(19/49): loss=1.3278036507359893e+41\n",
      "Gradient Descent(20/49): loss=3.1054393673577707e+43\n",
      "Gradient Descent(21/49): loss=7.262936551444761e+45\n",
      "Gradient Descent(22/49): loss=1.698640389170122e+48\n",
      "Gradient Descent(23/49): loss=3.9727445659815255e+50\n",
      "Gradient Descent(24/49): loss=9.291371786070971e+52\n",
      "Gradient Descent(25/49): loss=2.1730465735535684e+55\n",
      "Gradient Descent(26/49): loss=5.082275814119676e+57\n",
      "Gradient Descent(27/49): loss=1.1886320231300298e+60\n",
      "Gradient Descent(28/49): loss=2.7799476810861206e+62\n",
      "Gradient Descent(29/49): loss=6.501683413530879e+64\n",
      "Gradient Descent(30/49): loss=1.5206000996849145e+67\n",
      "Gradient Descent(31/49): loss=3.5563476658208774e+69\n",
      "Gradient Descent(32/49): loss=8.317511436971635e+71\n",
      "Gradient Descent(33/49): loss=1.945282154752101e+74\n",
      "Gradient Descent(34/49): loss=4.5495851617063736e+76\n",
      "Gradient Descent(35/49): loss=1.064047449006532e+79\n",
      "Gradient Descent(36/49): loss=2.488571888415135e+81\n",
      "Gradient Descent(37/49): loss=5.820219812182467e+83\n",
      "Gradient Descent(38/49): loss=1.3612208198531908e+86\n",
      "Gradient Descent(39/49): loss=3.183594744176879e+88\n",
      "Gradient Descent(40/49): loss=7.445724710737032e+90\n",
      "Gradient Descent(41/49): loss=1.741390501083206e+93\n",
      "Gradient Descent(42/49): loss=4.072727632395442e+95\n",
      "Gradient Descent(43/49): loss=9.52521008777726e+97\n",
      "Gradient Descent(44/49): loss=2.227736185808464e+100\n",
      "Gradient Descent(45/49): loss=5.210182733847205e+102\n",
      "Gradient Descent(46/49): loss=1.2185466256287397e+105\n",
      "Gradient Descent(47/49): loss=2.8499113268812173e+107\n",
      "Gradient Descent(48/49): loss=6.66531292300379e+109\n",
      "Gradient Descent(49/49): loss=1.5588694266561386e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.79930029289875\n",
      "Gradient Descent(2/49): loss=238.36526936608806\n",
      "Gradient Descent(3/49): loss=14182.554954072953\n",
      "Gradient Descent(4/49): loss=1040464.7383114131\n",
      "Gradient Descent(5/49): loss=115626610.90656132\n",
      "Gradient Descent(6/49): loss=19584665725.837914\n",
      "Gradient Descent(7/49): loss=4084128809247.31\n",
      "Gradient Descent(8/49): loss=909061836890473.6\n",
      "Gradient Descent(9/49): loss=2.0582962996771987e+17\n",
      "Gradient Descent(10/49): loss=4.68024994817699e+19\n",
      "Gradient Descent(11/49): loss=1.0653288930613232e+22\n",
      "Gradient Descent(12/49): loss=2.425545186057176e+24\n",
      "Gradient Descent(13/49): loss=5.522836450334759e+26\n",
      "Gradient Descent(14/49): loss=1.2575394962316994e+29\n",
      "Gradient Descent(15/49): loss=2.863404537893582e+31\n",
      "Gradient Descent(16/49): loss=6.51994872958911e+33\n",
      "Gradient Descent(17/49): loss=1.4845873285876103e+36\n",
      "Gradient Descent(18/49): loss=3.3803941534461227e+38\n",
      "Gradient Descent(19/49): loss=7.697132102150049e+40\n",
      "Gradient Descent(20/49): loss=1.7526312051831115e+43\n",
      "Gradient Descent(21/49): loss=3.990728108949883e+45\n",
      "Gradient Descent(22/49): loss=9.086857974213422e+47\n",
      "Gradient Descent(23/49): loss=2.069070745819262e+50\n",
      "Gradient Descent(24/49): loss=4.7112585707883913e+52\n",
      "Gradient Descent(25/49): loss=1.0727500432616704e+55\n",
      "Gradient Descent(26/49): loss=2.4426438031952985e+57\n",
      "Gradient Descent(27/49): loss=5.561881620762717e+59\n",
      "Gradient Descent(28/49): loss=1.2664362737994628e+62\n",
      "Gradient Descent(29/49): loss=2.883665897540518e+64\n",
      "Gradient Descent(30/49): loss=6.566085622050568e+66\n",
      "Gradient Descent(31/49): loss=1.4950927717691055e+69\n",
      "Gradient Descent(32/49): loss=3.404315028560659e+71\n",
      "Gradient Descent(33/49): loss=7.751599788667498e+73\n",
      "Gradient Descent(34/49): loss=1.7650334584069176e+76\n",
      "Gradient Descent(35/49): loss=4.018967947558839e+78\n",
      "Gradient Descent(36/49): loss=9.151159875509809e+80\n",
      "Gradient Descent(37/49): loss=2.0837122405519002e+83\n",
      "Gradient Descent(38/49): loss=4.744597144505474e+85\n",
      "Gradient Descent(39/49): loss=1.0803412114950836e+88\n",
      "Gradient Descent(40/49): loss=2.4599288363318667e+90\n",
      "Gradient Descent(41/49): loss=5.601239511582384e+92\n",
      "Gradient Descent(42/49): loss=1.2753980360218434e+95\n",
      "Gradient Descent(43/49): loss=2.9040717629102947e+97\n",
      "Gradient Descent(44/49): loss=6.612549624459708e+99\n",
      "Gradient Descent(45/49): loss=1.505672590271028e+102\n",
      "Gradient Descent(46/49): loss=3.4284051959439256e+104\n",
      "Gradient Descent(47/49): loss=7.806452919130107e+106\n",
      "Gradient Descent(48/49): loss=1.7775234750750081e+109\n",
      "Gradient Descent(49/49): loss=4.0474076218406657e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.893585635314699\n",
      "Gradient Descent(2/49): loss=242.2789202954917\n",
      "Gradient Descent(3/49): loss=14254.488410516145\n",
      "Gradient Descent(4/49): loss=1019085.5944716497\n",
      "Gradient Descent(5/49): loss=109074711.97779013\n",
      "Gradient Descent(6/49): loss=18024235325.9706\n",
      "Gradient Descent(7/49): loss=3726923890190.625\n",
      "Gradient Descent(8/49): loss=827847416438479.4\n",
      "Gradient Descent(9/49): loss=1.8738373764457322e+17\n",
      "Gradient Descent(10/49): loss=4.261339871717699e+19\n",
      "Gradient Descent(11/49): loss=9.701929137607084e+21\n",
      "Gradient Descent(12/49): loss=2.2094868414088563e+24\n",
      "Gradient Descent(13/49): loss=5.032158688966155e+26\n",
      "Gradient Descent(14/49): loss=1.1461051035283545e+29\n",
      "Gradient Descent(15/49): loss=2.6103354501013633e+31\n",
      "Gradient Descent(16/49): loss=5.945229505309932e+33\n",
      "Gradient Descent(17/49): loss=1.354069736803426e+36\n",
      "Gradient Descent(18/49): loss=3.0839935746339507e+38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/49): loss=7.024022652211573e+40\n",
      "Gradient Descent(20/49): loss=1.599772930656085e+43\n",
      "Gradient Descent(21/49): loss=3.6436007663220035e+45\n",
      "Gradient Descent(22/49): loss=8.298569311105831e+47\n",
      "Gradient Descent(23/49): loss=1.89006032854152e+50\n",
      "Gradient Descent(24/49): loss=4.3047517127916493e+52\n",
      "Gradient Descent(25/49): loss=9.804389325042049e+54\n",
      "Gradient Descent(26/49): loss=2.2330219360021825e+57\n",
      "Gradient Descent(27/49): loss=5.08587205317498e+59\n",
      "Gradient Descent(28/49): loss=1.1583448475914218e+62\n",
      "Gradient Descent(29/49): loss=2.638215770890184e+64\n",
      "Gradient Descent(30/49): loss=6.008730878586243e+66\n",
      "Gradient Descent(31/49): loss=1.368532747383766e+69\n",
      "Gradient Descent(32/49): loss=3.116934205418105e+71\n",
      "Gradient Descent(33/49): loss=7.09904739910573e+73\n",
      "Gradient Descent(34/49): loss=1.6168603715518287e+76\n",
      "Gradient Descent(35/49): loss=3.682518673454734e+78\n",
      "Gradient Descent(36/49): loss=8.387207713754158e+80\n",
      "Gradient Descent(37/49): loss=1.9102483781205848e+83\n",
      "Gradient Descent(38/49): loss=4.350731483766917e+85\n",
      "Gradient Descent(39/49): loss=9.909111642573123e+87\n",
      "Gradient Descent(40/49): loss=2.256873215718682e+90\n",
      "Gradient Descent(41/49): loss=5.14019510078477e+92\n",
      "Gradient Descent(42/49): loss=1.1707173220945835e+95\n",
      "Gradient Descent(43/49): loss=2.666394993534545e+97\n",
      "Gradient Descent(44/49): loss=6.072911135222423e+99\n",
      "Gradient Descent(45/49): loss=1.383150273899264e+102\n",
      "Gradient Descent(46/49): loss=3.150226699501182e+104\n",
      "Gradient Descent(47/49): loss=7.174873508338093e+106\n",
      "Gradient Descent(48/49): loss=1.6341303268365407e+109\n",
      "Gradient Descent(49/49): loss=3.7218522695676344e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.716295082811639\n",
      "Gradient Descent(2/49): loss=229.9854485974694\n",
      "Gradient Descent(3/49): loss=13185.576400885611\n",
      "Gradient Descent(4/49): loss=879574.1411344087\n",
      "Gradient Descent(5/49): loss=83841355.16168655\n",
      "Gradient Descent(6/49): loss=12768275304.611605\n",
      "Gradient Descent(7/49): loss=2584093725635.8813\n",
      "Gradient Descent(8/49): loss=576668139708586.0\n",
      "Gradient Descent(9/49): loss=1.3208228273404917e+17\n",
      "Gradient Descent(10/49): loss=3.0447077468558074e+19\n",
      "Gradient Descent(11/49): loss=7.029396627624682e+21\n",
      "Gradient Descent(12/49): loss=1.6234977576629183e+24\n",
      "Gradient Descent(13/49): loss=3.7499372414832314e+26\n",
      "Gradient Descent(14/49): loss=8.661748616534459e+28\n",
      "Gradient Descent(15/49): loss=2.0007340969986664e+31\n",
      "Gradient Descent(16/49): loss=4.6214015376125254e+33\n",
      "Gradient Descent(17/49): loss=1.0674761071459915e+36\n",
      "Gradient Descent(18/49): loss=2.4657137242789217e+38\n",
      "Gradient Descent(19/49): loss=5.695438269918328e+40\n",
      "Gradient Descent(20/49): loss=1.3155629908895134e+43\n",
      "Gradient Descent(21/49): loss=3.0387582178190984e+45\n",
      "Gradient Descent(22/49): loss=7.01908732037993e+47\n",
      "Gradient Descent(23/49): loss=1.621306575987703e+50\n",
      "Gradient Descent(24/49): loss=3.74498121108127e+52\n",
      "Gradient Descent(25/49): loss=8.650359209733473e+54\n",
      "Gradient Descent(26/49): loss=1.998106538854275e+57\n",
      "Gradient Descent(27/49): loss=4.615334050082686e+59\n",
      "Gradient Descent(28/49): loss=1.0660747052090639e+62\n",
      "Gradient Descent(29/49): loss=2.462476745461658e+64\n",
      "Gradient Descent(30/49): loss=5.687961352342892e+66\n",
      "Gradient Descent(31/49): loss=1.313835933897533e+69\n",
      "Gradient Descent(32/49): loss=3.0347689695349896e+71\n",
      "Gradient Descent(33/49): loss=7.009872740450201e+73\n",
      "Gradient Descent(34/49): loss=1.6191781427380427e+76\n",
      "Gradient Descent(35/49): loss=3.740064841394382e+78\n",
      "Gradient Descent(36/49): loss=8.639003114370407e+80\n",
      "Gradient Descent(37/49): loss=1.9954834468130683e+83\n",
      "Gradient Descent(38/49): loss=4.60927509087401e+85\n",
      "Gradient Descent(39/49): loss=1.0646751741930977e+88\n",
      "Gradient Descent(40/49): loss=2.459244033378273e+90\n",
      "Gradient Descent(41/49): loss=5.680494259941911e+92\n",
      "Gradient Descent(42/49): loss=1.3121111446962363e+95\n",
      "Gradient Descent(43/49): loss=3.030784958585057e+97\n",
      "Gradient Descent(44/49): loss=7.00067025748188e+99\n",
      "Gradient Descent(45/49): loss=1.617052503681159e+102\n",
      "Gradient Descent(46/49): loss=3.735154925868481e+104\n",
      "Gradient Descent(47/49): loss=8.62766192716661e+106\n",
      "Gradient Descent(48/49): loss=1.992863798338291e+109\n",
      "Gradient Descent(49/49): loss=4.6032240857999437e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.7979200750805795\n",
      "Gradient Descent(2/49): loss=237.83076079540425\n",
      "Gradient Descent(3/49): loss=14292.379334849835\n",
      "Gradient Descent(4/49): loss=1097096.7818578733\n",
      "Gradient Descent(5/49): loss=133071858.8999103\n",
      "Gradient Descent(6/49): loss=24407378025.891766\n",
      "Gradient Descent(7/49): loss=5367858069624.248\n",
      "Gradient Descent(8/49): loss=1244176374334275.0\n",
      "Gradient Descent(9/49): loss=2.921698681746043e+17\n",
      "Gradient Descent(10/49): loss=6.882459959214848e+19\n",
      "Gradient Descent(11/49): loss=1.622453734404726e+22\n",
      "Gradient Descent(12/49): loss=3.825397092477574e+24\n",
      "Gradient Descent(13/49): loss=9.019833977685973e+26\n",
      "Gradient Descent(14/49): loss=2.1267907597097263e+29\n",
      "Gradient Descent(15/49): loss=5.014781048084836e+31\n",
      "Gradient Descent(16/49): loss=1.1824408377787505e+34\n",
      "Gradient Descent(17/49): loss=2.7880908430342e+36\n",
      "Gradient Descent(18/49): loss=6.5740716430192404e+38\n",
      "Gradient Descent(19/49): loss=1.550107966525684e+41\n",
      "Gradient Descent(20/49): loss=3.6550175330474906e+43\n",
      "Gradient Descent(21/49): loss=8.618208189751895e+45\n",
      "Gradient Descent(22/49): loss=2.032097294669606e+48\n",
      "Gradient Descent(23/49): loss=4.791505756386397e+50\n",
      "Gradient Descent(24/49): loss=1.1297946940741646e+53\n",
      "Gradient Descent(25/49): loss=2.663955999757173e+55\n",
      "Gradient Descent(26/49): loss=6.281372718306106e+57\n",
      "Gradient Descent(27/49): loss=1.4810921512922484e+60\n",
      "Gradient Descent(28/49): loss=3.49228434451365e+62\n",
      "Gradient Descent(29/49): loss=8.234497720006011e+64\n",
      "Gradient Descent(30/49): loss=1.9416217584719058e+67\n",
      "Gradient Descent(31/49): loss=4.578172441304304e+69\n",
      "Gradient Descent(32/49): loss=1.0794925845296466e+72\n",
      "Gradient Descent(33/49): loss=2.5453480728273057e+74\n",
      "Gradient Descent(34/49): loss=6.001705713123145e+76\n",
      "Gradient Descent(35/49): loss=1.4151491440981772e+79\n",
      "Gradient Descent(36/49): loss=3.336796563788316e+81\n",
      "Gradient Descent(37/49): loss=7.867871280242389e+83\n",
      "Gradient Descent(38/49): loss=1.8551744854406925e+86\n",
      "Gradient Descent(39/49): loss=4.3743374145848595e+88\n",
      "Gradient Descent(40/49): loss=1.0314300874018176e+91\n",
      "Gradient Descent(41/49): loss=2.432020953963561e+93\n",
      "Gradient Descent(42/49): loss=5.734490386466299e+95\n",
      "Gradient Descent(43/49): loss=1.3521421326111906e+98\n",
      "Gradient Descent(44/49): loss=3.1882316013594282e+100\n",
      "Gradient Descent(45/49): loss=7.517568233952429e+102\n",
      "Gradient Descent(46/49): loss=1.7725761242701097e+105\n",
      "Gradient Descent(47/49): loss=4.179577781737705e+107\n",
      "Gradient Descent(48/49): loss=9.855074879104698e+109\n",
      "Gradient Descent(49/49): loss=2.3237395245311614e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.838280511710014\n",
      "Gradient Descent(2/49): loss=242.5312259093595\n",
      "Gradient Descent(3/49): loss=14554.165303554993\n",
      "Gradient Descent(4/49): loss=1076471.3871652263\n",
      "Gradient Descent(5/49): loss=120543882.01013298\n",
      "Gradient Descent(6/49): loss=20573771293.484913\n",
      "Gradient Descent(7/49): loss=4324490410931.9453\n",
      "Gradient Descent(8/49): loss=970368617968736.4\n",
      "Gradient Descent(9/49): loss=2.215046097322686e+17\n",
      "Gradient Descent(10/49): loss=5.077880401879345e+19\n",
      "Gradient Descent(11/49): loss=1.165299690398566e+22\n",
      "Gradient Descent(12/49): loss=2.6748802077484355e+24\n",
      "Gradient Descent(13/49): loss=6.140423685660424e+26\n",
      "Gradient Descent(14/49): loss=1.4096101458000618e+29\n",
      "Gradient Descent(15/49): loss=3.235946474121475e+31\n",
      "Gradient Descent(16/49): loss=7.428549837519605e+33\n",
      "Gradient Descent(17/49): loss=1.7053237917571987e+36\n",
      "Gradient Descent(18/49): loss=3.914800948373933e+38\n",
      "Gradient Descent(19/49): loss=8.986954117425633e+40\n",
      "Gradient Descent(20/49): loss=2.0630766580474685e+43\n",
      "Gradient Descent(21/49): loss=4.7360710256207986e+45\n",
      "Gradient Descent(22/49): loss=1.0872290506828592e+48\n",
      "Gradient Descent(23/49): loss=2.4958810843653855e+50\n",
      "Gradient Descent(24/49): loss=5.729632025058595e+52\n",
      "Gradient Descent(25/49): loss=1.315314393314459e+55\n",
      "Gradient Descent(26/49): loss=3.0194817846853496e+57\n",
      "Gradient Descent(27/49): loss=6.9316281296623e+59\n",
      "Gradient Descent(28/49): loss=1.5912488285778618e+62\n",
      "Gradient Descent(29/49): loss=3.6529265377278725e+64\n",
      "Gradient Descent(30/49): loss=8.385786088504124e+66\n",
      "Gradient Descent(31/49): loss=1.925070422190565e+69\n",
      "Gradient Descent(32/49): loss=4.419259078732344e+71\n",
      "Gradient Descent(33/49): loss=1.0145005907230748e+74\n",
      "Gradient Descent(34/49): loss=2.3289230847101957e+76\n",
      "Gradient Descent(35/49): loss=5.346357394065277e+78\n",
      "Gradient Descent(36/49): loss=1.2273285267655767e+81\n",
      "Gradient Descent(37/49): loss=2.8174983480989223e+83\n",
      "Gradient Descent(38/49): loss=6.467947879008687e+85\n",
      "Gradient Descent(39/49): loss=1.4848047663914635e+88\n",
      "Gradient Descent(40/49): loss=3.4085698208140494e+90\n",
      "Gradient Descent(41/49): loss=7.824832251583233e+92\n",
      "Gradient Descent(42/49): loss=1.7962958948804755e+95\n",
      "Gradient Descent(43/49): loss=4.123639764049349e+97\n",
      "Gradient Descent(44/49): loss=9.466371855612597e+99\n",
      "Gradient Descent(45/49): loss=2.1731334751883506e+102\n",
      "Gradient Descent(46/49): loss=4.988721310566622e+104\n",
      "Gradient Descent(47/49): loss=1.145228334966584e+107\n",
      "Gradient Descent(48/49): loss=2.6290262725888575e+109\n",
      "Gradient Descent(49/49): loss=6.0352847820205e+111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.933406322862889\n",
      "Gradient Descent(2/49): loss=246.51490281289082\n",
      "Gradient Descent(3/49): loss=14628.276502036962\n",
      "Gradient Descent(4/49): loss=1054407.8472730028\n",
      "Gradient Descent(5/49): loss=113720687.1219895\n",
      "Gradient Descent(6/49): loss=18935162454.21624\n",
      "Gradient Descent(7/49): loss=3946305093248.43\n",
      "Gradient Descent(8/49): loss=883679442351965.1\n",
      "Gradient Descent(9/49): loss=2.0165402054598086e+17\n",
      "Gradient Descent(10/49): loss=4.623378079281183e+19\n",
      "Gradient Descent(11/49): loss=1.0612354279291197e+22\n",
      "Gradient Descent(12/49): loss=2.4366101956387914e+24\n",
      "Gradient Descent(13/49): loss=5.594871561381424e+26\n",
      "Gradient Descent(14/49): loss=1.2846991516209986e+29\n",
      "Gradient Descent(15/49): loss=2.9499490935986363e+31\n",
      "Gradient Descent(16/49): loss=6.773732418379514e+33\n",
      "Gradient Descent(17/49): loss=1.555398433022328e+36\n",
      "Gradient Descent(18/49): loss=3.571538228426751e+38\n",
      "Gradient Descent(19/49): loss=8.201040472785321e+40\n",
      "Gradient Descent(20/49): loss=1.883140002162013e+43\n",
      "Gradient Descent(21/49): loss=4.3241053163204376e+45\n",
      "Gradient Descent(22/49): loss=9.92910074081532e+47\n",
      "Gradient Descent(23/49): loss=2.279940804266978e+50\n",
      "Gradient Descent(24/49): loss=5.235247588593741e+52\n",
      "Gradient Descent(25/49): loss=1.202128461522561e+55\n",
      "Gradient Descent(26/49): loss=2.760352425646857e+57\n",
      "Gradient Descent(27/49): loss=6.338378765382066e+59\n",
      "Gradient Descent(28/49): loss=1.455431741257934e+62\n",
      "Gradient Descent(29/49): loss=3.3419926954040656e+64\n",
      "Gradient Descent(30/49): loss=7.673953274154044e+66\n",
      "Gradient Descent(31/49): loss=1.762109143293019e+69\n",
      "Gradient Descent(32/49): loss=4.0461917370993325e+71\n",
      "Gradient Descent(33/49): loss=9.290949789169084e+73\n",
      "Gradient Descent(34/49): loss=2.133407252883754e+76\n",
      "Gradient Descent(35/49): loss=4.898774194176285e+78\n",
      "Gradient Descent(36/49): loss=1.12486673948867e+81\n",
      "Gradient Descent(37/49): loss=2.5829424493827247e+83\n",
      "Gradient Descent(38/49): loss=5.931006280669259e+85\n",
      "Gradient Descent(39/49): loss=1.361890022355862e+88\n",
      "Gradient Descent(40/49): loss=3.1272002510561553e+90\n",
      "Gradient Descent(41/49): loss=7.180742387178116e+92\n",
      "Gradient Descent(42/49): loss=1.6488570315765037e+95\n",
      "Gradient Descent(43/49): loss=3.7861398780073745e+97\n",
      "Gradient Descent(44/49): loss=8.693813290853855e+99\n",
      "Gradient Descent(45/49): loss=1.9962915257110084e+102\n",
      "Gradient Descent(46/49): loss=4.583926203957076e+104\n",
      "Gradient Descent(47/49): loss=1.0525706878328331e+107\n",
      "Gradient Descent(48/49): loss=2.4169347489243776e+109\n",
      "Gradient Descent(49/49): loss=5.549815939284212e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.754561369915141\n",
      "Gradient Descent(2/49): loss=234.00918257206308\n",
      "Gradient Descent(3/49): loss=13531.982115576984\n",
      "Gradient Descent(4/49): loss=910197.0296651636\n",
      "Gradient Descent(5/49): loss=87431244.94991755\n",
      "Gradient Descent(6/49): loss=13415063397.301712\n",
      "Gradient Descent(7/49): loss=2736257687938.4727\n",
      "Gradient Descent(8/49): loss=615550217654982.1\n",
      "Gradient Descent(9/49): loss=1.4213698736450037e+17\n",
      "Gradient Descent(10/49): loss=3.3032679848834236e+19\n",
      "Gradient Descent(11/49): loss=7.688734081137872e+21\n",
      "Gradient Descent(12/49): loss=1.7903084992697134e+24\n",
      "Gradient Descent(13/49): loss=4.169075674833853e+26\n",
      "Gradient Descent(14/49): loss=9.70869835524201e+28\n",
      "Gradient Descent(15/49): loss=2.26091624530747e+31\n",
      "Gradient Descent(16/49): loss=5.265122466578462e+33\n",
      "Gradient Descent(17/49): loss=1.226118962166922e+36\n",
      "Gradient Descent(18/49): loss=2.855333044541046e+38\n",
      "Gradient Descent(19/49): loss=6.649376761805565e+40\n",
      "Gradient Descent(20/49): loss=1.5484782619587377e+43\n",
      "Gradient Descent(21/49): loss=3.606029581424344e+45\n",
      "Gradient Descent(22/49): loss=8.397566607566402e+47\n",
      "Gradient Descent(23/49): loss=1.955589196931645e+50\n",
      "Gradient Descent(24/49): loss=4.554092019659133e+52\n",
      "Gradient Descent(25/49): loss=1.0605373641934248e+55\n",
      "Gradient Descent(26/49): loss=2.4697338042269124e+57\n",
      "Gradient Descent(27/49): loss=5.751409869826932e+59\n",
      "Gradient Descent(28/49): loss=1.3393635959523453e+62\n",
      "Gradient Descent(29/49): loss=3.1190523415372314e+64\n",
      "Gradient Descent(30/49): loss=7.263514954900408e+66\n",
      "Gradient Descent(31/49): loss=1.6914961251999538e+69\n",
      "Gradient Descent(32/49): loss=3.9390834318255325e+71\n",
      "Gradient Descent(33/49): loss=9.173168091678834e+73\n",
      "Gradient Descent(34/49): loss=2.1362079350321248e+76\n",
      "Gradient Descent(35/49): loss=4.9747091692714685e+78\n",
      "Gradient Descent(36/49): loss=1.1584888770887184e+81\n",
      "Gradient Descent(37/49): loss=2.6978390749520465e+83\n",
      "Gradient Descent(38/49): loss=6.282611614388963e+85\n",
      "Gradient Descent(39/49): loss=1.4630675737379525e+88\n",
      "Gradient Descent(40/49): loss=3.407128844986933e+90\n",
      "Gradient Descent(41/49): loss=7.934375127106031e+92\n",
      "Gradient Descent(42/49): loss=1.847723156999681e+95\n",
      "Gradient Descent(43/49): loss=4.302898224775703e+97\n",
      "Gradient Descent(44/49): loss=1.0020404335269633e+100\n",
      "Gradient Descent(45/49): loss=2.333508667812483e+102\n",
      "Gradient Descent(46/49): loss=5.434174630648196e+104\n",
      "Gradient Descent(47/49): loss=1.2654872177553946e+107\n",
      "Gradient Descent(48/49): loss=2.9470122091222366e+109\n",
      "Gradient Descent(49/49): loss=6.862875293296258e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.836733536940569\n",
      "Gradient Descent(2/49): loss=241.9715402013649\n",
      "Gradient Descent(3/49): loss=14665.101084980528\n",
      "Gradient Descent(4/49): loss=1134806.008543929\n",
      "Gradient Descent(5/49): loss=138687626.77314675\n",
      "Gradient Descent(6/49): loss=25632174614.465405\n",
      "Gradient Descent(7/49): loss=5682040436056.812\n",
      "Gradient Descent(8/49): loss=1327649429612496.5\n",
      "Gradient Descent(9/49): loss=3.1430674819566016e+17\n",
      "Gradient Descent(10/49): loss=7.464213019565759e+19\n",
      "Gradient Descent(11/49): loss=1.7739285128609293e+22\n",
      "Gradient Descent(12/49): loss=4.216616951672506e+24\n",
      "Gradient Descent(13/49): loss=1.002328552886493e+27\n",
      "Gradient Descent(14/49): loss=2.382650155031878e+29\n",
      "Gradient Descent(15/49): loss=5.663846224070661e+31\n",
      "Gradient Descent(16/49): loss=1.3463651533439072e+34\n",
      "Gradient Descent(17/49): loss=3.200474175744176e+36\n",
      "Gradient Descent(18/49): loss=7.60791768309212e+38\n",
      "Gradient Descent(19/49): loss=1.808494888739221e+41\n",
      "Gradient Descent(20/49): loss=4.299013086514953e+43\n",
      "Gradient Descent(21/49): loss=1.0219278827053772e+46\n",
      "Gradient Descent(22/49): loss=2.4292473098625178e+48\n",
      "Gradient Descent(23/49): loss=5.774617360455384e+50\n",
      "Gradient Descent(24/49): loss=1.372697029424021e+53\n",
      "Gradient Descent(25/49): loss=3.2630683852674566e+55\n",
      "Gradient Descent(26/49): loss=7.756711829851124e+57\n",
      "Gradient Descent(27/49): loss=1.8438650775143033e+60\n",
      "Gradient Descent(28/49): loss=4.383092344610302e+62\n",
      "Gradient Descent(29/49): loss=1.0419145487196022e+65\n",
      "Gradient Descent(30/49): loss=2.476758054546776e+67\n",
      "Gradient Descent(31/49): loss=5.887556199595117e+69\n",
      "Gradient Descent(32/49): loss=1.3995439699794507e+72\n",
      "Gradient Descent(33/49): loss=3.3268868398072005e+74\n",
      "Gradient Descent(34/49): loss=7.908416085737645e+76\n",
      "Gradient Descent(35/49): loss=1.87992703078472e+79\n",
      "Gradient Descent(36/49): loss=4.468816009123002e+81\n",
      "Gradient Descent(37/49): loss=1.0622921100856879e+84\n",
      "Gradient Descent(38/49): loss=2.52519800512386e+86\n",
      "Gradient Descent(39/49): loss=6.0027038745181334e+88\n",
      "Gradient Descent(40/49): loss=1.4269159777586382e+91\n",
      "Gradient Descent(41/49): loss=3.3919534432246028e+93\n",
      "Gradient Descent(42/49): loss=8.063087343849983e+95\n",
      "Gradient Descent(43/49): loss=1.916694276698189e+98\n",
      "Gradient Descent(44/49): loss=4.556216240333375e+100\n",
      "Gradient Descent(45/49): loss=1.0830682118193133e+103\n",
      "Gradient Descent(46/49): loss=2.5745853347988385e+105\n",
      "Gradient Descent(47/49): loss=6.120103585190649e+107\n",
      "Gradient Descent(48/49): loss=1.4548233219230206e+110\n",
      "Gradient Descent(49/49): loss=3.4582926065705753e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.877432545818193\n",
      "Gradient Descent(2/49): loss=246.7516558586665\n",
      "Gradient Descent(3/49): loss=14933.858042400945\n",
      "Gradient Descent(4/49): loss=1113564.5717952147\n",
      "Gradient Descent(5/49): loss=125648732.92656215\n",
      "Gradient Descent(6/49): loss=21608495554.874207\n",
      "Gradient Descent(7/49): loss=4577934387436.47\n",
      "Gradient Descent(8/49): loss=1035535446973473.1\n",
      "Gradient Descent(9/49): loss=2.383022916566445e+17\n",
      "Gradient Descent(10/49): loss=5.507469423487236e+19\n",
      "Gradient Descent(11/49): loss=1.274187634228506e+22\n",
      "Gradient Descent(12/49): loss=2.9486739710961226e+24\n",
      "Gradient Descent(13/49): loss=6.824134316133376e+26\n",
      "Gradient Descent(14/49): loss=1.5793380296437988e+29\n",
      "Gradient Descent(15/49): loss=3.6551420863358104e+31\n",
      "Gradient Descent(16/49): loss=8.459288511684242e+33\n",
      "Gradient Descent(17/49): loss=1.9577782932245566e+36\n",
      "Gradient Descent(18/49): loss=4.530991055440952e+38\n",
      "Gradient Descent(19/49): loss=1.0486315173923364e+41\n",
      "Gradient Descent(20/49): loss=2.4269040690445683e+43\n",
      "Gradient Descent(21/49): loss=5.616714038679402e+45\n",
      "Gradient Descent(22/49): loss=1.2999062056458985e+48\n",
      "Gradient Descent(23/49): loss=3.008442537616504e+50\n",
      "Gradient Descent(24/49): loss=6.962599657528092e+52\n",
      "Gradient Descent(25/49): loss=1.6113917212977444e+55\n",
      "Gradient Descent(26/49): loss=3.7293301456176586e+57\n",
      "Gradient Descent(27/49): loss=8.630988450042587e+59\n",
      "Gradient Descent(28/49): loss=1.9975158732543597e+62\n",
      "Gradient Descent(29/49): loss=4.6229579462401736e+64\n",
      "Gradient Descent(30/49): loss=1.0699159120015517e+67\n",
      "Gradient Descent(31/49): loss=2.4761636858174563e+69\n",
      "Gradient Descent(32/49): loss=5.730718208957892e+71\n",
      "Gradient Descent(33/49): loss=1.3262908013142756e+74\n",
      "Gradient Descent(34/49): loss=3.069505820232554e+76\n",
      "Gradient Descent(35/49): loss=7.1039216822623766e+78\n",
      "Gradient Descent(36/49): loss=1.6440986342189924e+81\n",
      "Gradient Descent(37/49): loss=3.80502550554582e+83\n",
      "Gradient Descent(38/49): loss=8.806174274776368e+85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=2.0380600667381423e+88\n",
      "Gradient Descent(40/49): loss=4.716791544235358e+90\n",
      "Gradient Descent(41/49): loss=1.091632324035323e+93\n",
      "Gradient Descent(42/49): loss=2.5264231410336483e+95\n",
      "Gradient Descent(43/49): loss=5.847036357402319e+97\n",
      "Gradient Descent(44/49): loss=1.3532109332563195e+100\n",
      "Gradient Descent(45/49): loss=3.1318085230753833e+102\n",
      "Gradient Descent(46/49): loss=7.248112163567537e+104\n",
      "Gradient Descent(47/49): loss=1.6774694094026946e+107\n",
      "Gradient Descent(48/49): loss=3.882257277454696e+109\n",
      "Gradient Descent(49/49): loss=8.984915900026377e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.973402483892833\n",
      "Gradient Descent(2/49): loss=250.80629327999344\n",
      "Gradient Descent(3/49): loss=15010.200469942532\n",
      "Gradient Descent(4/49): loss=1090797.6202280058\n",
      "Gradient Descent(5/49): loss=118544177.71176808\n",
      "Gradient Descent(6/49): loss=19888133025.71538\n",
      "Gradient Descent(7/49): loss=4177629180946.128\n",
      "Gradient Descent(8/49): loss=943026966834645.4\n",
      "Gradient Descent(9/49): loss=2.1694640037673123e+17\n",
      "Gradient Descent(10/49): loss=5.014514063833607e+19\n",
      "Gradient Descent(11/49): loss=1.1603986467463807e+22\n",
      "Gradient Descent(12/49): loss=2.686013237538483e+24\n",
      "Gradient Descent(13/49): loss=6.217832081211804e+26\n",
      "Gradient Descent(14/49): loss=1.4393855041602303e+29\n",
      "Gradient Descent(15/49): loss=3.332092424939905e+31\n",
      "Gradient Descent(16/49): loss=7.713604861979541e+33\n",
      "Gradient Descent(17/49): loss=1.7856561847858653e+36\n",
      "Gradient Descent(18/49): loss=4.1336939820899194e+38\n",
      "Gradient Descent(19/49): loss=9.56927001453005e+40\n",
      "Gradient Descent(20/49): loss=2.215232412742408e+43\n",
      "Gradient Descent(21/49): loss=5.128138969060709e+45\n",
      "Gradient Descent(22/49): loss=1.1871354509017912e+48\n",
      "Gradient Descent(23/49): loss=2.7481520827783323e+50\n",
      "Gradient Descent(24/49): loss=6.361818160207748e+52\n",
      "Gradient Descent(25/49): loss=1.4727252744598976e+55\n",
      "Gradient Descent(26/49): loss=3.409276529783421e+57\n",
      "Gradient Descent(27/49): loss=7.892284228500658e+59\n",
      "Gradient Descent(28/49): loss=1.8270195978323836e+62\n",
      "Gradient Descent(29/49): loss=4.2294480459909e+64\n",
      "Gradient Descent(30/49): loss=9.790935354475303e+66\n",
      "Gradient Descent(31/49): loss=2.266546700021098e+69\n",
      "Gradient Descent(32/49): loss=5.246928671659966e+71\n",
      "Gradient Descent(33/49): loss=1.2146346018474243e+74\n",
      "Gradient Descent(34/49): loss=2.8118110771616478e+76\n",
      "Gradient Descent(35/49): loss=6.509185166982627e+78\n",
      "Gradient Descent(36/49): loss=1.5068399111947083e+81\n",
      "Gradient Descent(37/49): loss=3.488250003220909e+83\n",
      "Gradient Descent(38/49): loss=8.07510339656682e+85\n",
      "Gradient Descent(39/49): loss=1.869341211353433e+88\n",
      "Gradient Descent(40/49): loss=4.32742021105248e+90\n",
      "Gradient Descent(41/49): loss=1.001773542962108e+93\n",
      "Gradient Descent(42/49): loss=2.3190496472140876e+95\n",
      "Gradient Descent(43/49): loss=5.3684700539624374e+97\n",
      "Gradient Descent(44/49): loss=1.2427707511528999e+100\n",
      "Gradient Descent(45/49): loss=2.876944687027049e+102\n",
      "Gradient Descent(46/49): loss=6.659965825986083e+104\n",
      "Gradient Descent(47/49): loss=1.541744789300648e+107\n",
      "Gradient Descent(48/49): loss=3.569052841174014e+109\n",
      "Gradient Descent(49/49): loss=8.262157441031842e+111\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.792996450146986\n",
      "Gradient Descent(2/49): loss=238.08558750661226\n",
      "Gradient Descent(3/49): loss=13885.941168996505\n",
      "Gradient Descent(4/49): loss=941749.4176070032\n",
      "Gradient Descent(5/49): loss=91159024.6142313\n",
      "Gradient Descent(6/49): loss=14091776330.826487\n",
      "Gradient Descent(7/49): loss=2896708647438.947\n",
      "Gradient Descent(8/49): loss=656879946873414.5\n",
      "Gradient Descent(9/49): loss=1.529115634715337e+17\n",
      "Gradient Descent(10/49): loss=3.5826001195466727e+19\n",
      "Gradient Descent(11/49): loss=8.406855877311594e+21\n",
      "Gradient Descent(12/49): loss=1.9734751282161617e+24\n",
      "Gradient Descent(13/49): loss=4.6330692808457013e+26\n",
      "Gradient Descent(14/49): loss=1.0877154859298681e+29\n",
      "Gradient Descent(15/49): loss=2.5536659161895147e+31\n",
      "Gradient Descent(16/49): loss=5.995334081381845e+33\n",
      "Gradient Descent(17/49): loss=1.4075467448601163e+36\n",
      "Gradient Descent(18/49): loss=3.3045497615555245e+38\n",
      "Gradient Descent(19/49): loss=7.758214321641282e+40\n",
      "Gradient Descent(20/49): loss=1.8214248249222787e+43\n",
      "Gradient Descent(21/49): loss=4.276226794707219e+45\n",
      "Gradient Descent(22/49): loss=1.0039456668159753e+48\n",
      "Gradient Descent(23/49): loss=2.357000576357521e+50\n",
      "Gradient Descent(24/49): loss=5.533617904487827e+52\n",
      "Gradient Descent(25/49): loss=1.2991480536796978e+55\n",
      "Gradient Descent(26/49): loss=3.050058197932892e+57\n",
      "Gradient Descent(27/49): loss=7.160735055892875e+59\n",
      "Gradient Descent(28/49): loss=1.681152398188592e+62\n",
      "Gradient Descent(29/49): loss=3.946904003394652e+64\n",
      "Gradient Descent(30/49): loss=9.266293304995964e+66\n",
      "Gradient Descent(31/49): loss=2.1754821384144174e+69\n",
      "Gradient Descent(32/49): loss=5.107460317501944e+71\n",
      "Gradient Descent(33/49): loss=1.1990974522029397e+74\n",
      "Gradient Descent(34/49): loss=2.815165680196213e+76\n",
      "Gradient Descent(35/49): loss=6.6092691568937245e+78\n",
      "Gradient Descent(36/49): loss=1.5516826983064555e+81\n",
      "Gradient Descent(37/49): loss=3.64294317430293e+83\n",
      "Gradient Descent(38/49): loss=8.552673162937464e+85\n",
      "Gradient Descent(39/49): loss=2.007942883875135e+88\n",
      "Gradient Descent(40/49): loss=4.714122179222864e+90\n",
      "Gradient Descent(41/49): loss=1.1067519947456411e+93\n",
      "Gradient Descent(42/49): loss=2.5983628156098467e+95\n",
      "Gradient Descent(43/49): loss=6.1002730093073975e+97\n",
      "Gradient Descent(44/49): loss=1.4321837798987695e+100\n",
      "Gradient Descent(45/49): loss=3.362391119668865e+102\n",
      "Gradient Descent(46/49): loss=7.894010671191528e+104\n",
      "Gradient Descent(47/49): loss=1.8533062412746003e+107\n",
      "Gradient Descent(48/49): loss=4.351075982810803e+109\n",
      "Gradient Descent(49/49): loss=1.0215182891291936e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.875717410867209\n",
      "Gradient Descent(2/49): loss=246.16625966812836\n",
      "Gradient Descent(3/49): loss=15045.89183534687\n",
      "Gradient Descent(4/49): loss=1173645.573205786\n",
      "Gradient Descent(5/49): loss=144515976.89888585\n",
      "Gradient Descent(6/49): loss=26913097267.807785\n",
      "Gradient Descent(7/49): loss=6013229378638.537\n",
      "Gradient Descent(8/49): loss=1416351087307657.5\n",
      "Gradient Descent(9/49): loss=3.380211091073047e+17\n",
      "Gradient Descent(10/49): loss=8.092486078195489e+19\n",
      "Gradient Descent(11/49): loss=1.938845815170229e+22\n",
      "Gradient Descent(12/49): loss=4.646018177759331e+24\n",
      "Gradient Descent(13/49): loss=1.1133623948090926e+27\n",
      "Gradient Descent(14/49): loss=2.668065228171842e+29\n",
      "Gradient Descent(15/49): loss=6.393774872441911e+31\n",
      "Gradient Descent(16/49): loss=1.5322106434940244e+34\n",
      "Gradient Descent(17/49): loss=3.67180547459571e+36\n",
      "Gradient Descent(18/49): loss=8.799152981822737e+38\n",
      "Gradient Descent(19/49): loss=2.108638224104061e+41\n",
      "Gradient Descent(20/49): loss=5.053162704243462e+43\n",
      "Gradient Descent(21/49): loss=1.2109451983680446e+46\n",
      "Gradient Descent(22/49): loss=2.9019217456766748e+48\n",
      "Gradient Descent(23/49): loss=6.95419563954439e+50\n",
      "Gradient Descent(24/49): loss=1.6665107205375245e+53\n",
      "Gradient Descent(25/49): loss=3.9936437305234105e+55\n",
      "Gradient Descent(26/49): loss=9.570409628813552e+57\n",
      "Gradient Descent(27/49): loss=2.2934629787642467e+60\n",
      "Gradient Descent(28/49): loss=5.496078683117315e+62\n",
      "Gradient Descent(29/49): loss=1.3170860472006814e+65\n",
      "Gradient Descent(30/49): loss=3.1562787866544856e+67\n",
      "Gradient Descent(31/49): loss=7.563739514406295e+69\n",
      "Gradient Descent(32/49): loss=1.8125824525922593e+72\n",
      "Gradient Descent(33/49): loss=4.343691557843523e+74\n",
      "Gradient Descent(34/49): loss=1.0409267905411872e+77\n",
      "Gradient Descent(35/49): loss=2.4944878540231658e+79\n",
      "Gradient Descent(36/49): loss=5.977816797888439e+81\n",
      "Gradient Descent(37/49): loss=1.432530273157421e+84\n",
      "Gradient Descent(38/49): loss=3.4329305378468204e+86\n",
      "Gradient Descent(39/49): loss=8.22671066609016e+88\n",
      "Gradient Descent(40/49): loss=1.9714575531729e+91\n",
      "Gradient Descent(41/49): loss=4.724421511483204e+93\n",
      "Gradient Descent(42/49): loss=1.132165315060585e+96\n",
      "Gradient Descent(43/49): loss=2.713132808981298e+98\n",
      "Gradient Descent(44/49): loss=6.501779856042303e+100\n",
      "Gradient Descent(45/49): loss=1.5580933287342617e+103\n",
      "Gradient Descent(46/49): loss=3.7338311582329803e+105\n",
      "Gradient Descent(47/49): loss=8.947792061671169e+107\n",
      "Gradient Descent(48/49): loss=2.144258253412742e+110\n",
      "Gradient Descent(49/49): loss=5.138522917876071e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.916756395223289\n",
      "Gradient Descent(2/49): loss=251.02703220904826\n",
      "Gradient Descent(3/49): loss=15321.773249610545\n",
      "Gradient Descent(4/49): loss=1151772.2982512433\n",
      "Gradient Descent(5/49): loss=130947503.20453504\n",
      "Gradient Descent(6/49): loss=22690744087.14606\n",
      "Gradient Descent(7/49): loss=4845115369677.143\n",
      "Gradient Descent(8/49): loss=1104788225703925.5\n",
      "Gradient Descent(9/49): loss=2.5629804304492934e+17\n",
      "Gradient Descent(10/49): loss=5.97144024505445e+19\n",
      "Gradient Descent(11/49): loss=1.3927470585579942e+22\n",
      "Gradient Descent(12/49): loss=3.2492118001685976e+24\n",
      "Gradient Descent(13/49): loss=7.580736102469496e+26\n",
      "Gradient Descent(14/49): loss=1.768689064435607e+29\n",
      "Gradient Descent(15/49): loss=4.126608394098235e+31\n",
      "Gradient Descent(16/49): loss=9.627985532589748e+33\n",
      "Gradient Descent(17/49): loss=2.2463514280915475e+36\n",
      "Gradient Descent(18/49): loss=5.2410704229970356e+38\n",
      "Gradient Descent(19/49): loss=1.2228193332366188e+41\n",
      "Gradient Descent(20/49): loss=2.8530185745888457e+43\n",
      "Gradient Descent(21/49): loss=6.656514803451697e+45\n",
      "Gradient Descent(22/49): loss=1.5530634722163553e+48\n",
      "Gradient Descent(23/49): loss=3.62352705593147e+50\n",
      "Gradient Descent(24/49): loss=8.454225187904317e+52\n",
      "Gradient Descent(25/49): loss=1.972495925234221e+55\n",
      "Gradient Descent(26/49): loss=4.602125077807234e+57\n",
      "Gradient Descent(27/49): loss=1.073743928230001e+60\n",
      "Gradient Descent(28/49): loss=2.505203582950313e+62\n",
      "Gradient Descent(29/49): loss=5.845010925810653e+64\n",
      "Gradient Descent(30/49): loss=1.3637276010363584e+67\n",
      "Gradient Descent(31/49): loss=3.1817784319547363e+69\n",
      "Gradient Descent(32/49): loss=7.423560234726295e+71\n",
      "Gradient Descent(33/49): loss=1.7320265297277344e+74\n",
      "Gradient Descent(34/49): loss=4.0410743697443766e+76\n",
      "Gradient Descent(35/49): loss=9.428424900842734e+78\n",
      "Gradient Descent(36/49): loss=2.199791144067805e+81\n",
      "Gradient Descent(37/49): loss=5.13243848088204e+83\n",
      "Gradient Descent(38/49): loss=1.1974738979686875e+86\n",
      "Gradient Descent(39/49): loss=2.7938839240990086e+88\n",
      "Gradient Descent(40/49): loss=6.518544909062407e+90\n",
      "Gradient Descent(41/49): loss=1.5208730529191627e+93\n",
      "Gradient Descent(42/49): loss=3.5484220410600255e+95\n",
      "Gradient Descent(43/49): loss=8.278994066804678e+97\n",
      "Gradient Descent(44/49): loss=1.9316119098874442e+100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=4.506736615960711e+102\n",
      "Gradient Descent(46/49): loss=1.051488387583216e+105\n",
      "Gradient Descent(47/49): loss=2.4532781110543587e+107\n",
      "Gradient Descent(48/49): loss=5.723861110831455e+109\n",
      "Gradient Descent(49/49): loss=1.3354615552334515e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.013574118404535\n",
      "Gradient Descent(2/49): loss=255.1535729635679\n",
      "Gradient Descent(3/49): loss=15400.401422210647\n",
      "Gradient Descent(4/49): loss=1128282.4704535769\n",
      "Gradient Descent(5/49): loss=123551192.93037702\n",
      "Gradient Descent(6/49): loss=20884904603.267426\n",
      "Gradient Descent(7/49): loss=4421493893984.405\n",
      "Gradient Descent(8/49): loss=1006095737451474.4\n",
      "Gradient Descent(9/49): loss=2.333294911462231e+17\n",
      "Gradient Descent(10/49): loss=5.436954129298334e+19\n",
      "Gradient Descent(11/49): loss=1.2683695251715063e+22\n",
      "Gradient Descent(12/49): loss=2.959777672564438e+24\n",
      "Gradient Descent(13/49): loss=6.907206572866749e+26\n",
      "Gradient Descent(14/49): loss=1.6119558271961683e+29\n",
      "Gradient Descent(15/49): loss=3.761885900171343e+31\n",
      "Gradient Descent(16/49): loss=8.77927268561476e+33\n",
      "Gradient Descent(17/49): loss=2.0488566069668165e+36\n",
      "Gradient Descent(18/49): loss=4.781505026383635e+38\n",
      "Gradient Descent(19/49): loss=1.1158804659084046e+41\n",
      "Gradient Descent(20/49): loss=2.604178416143367e+43\n",
      "Gradient Descent(21/49): loss=6.077483598111924e+45\n",
      "Gradient Descent(22/49): loss=1.4183285854840926e+48\n",
      "Gradient Descent(23/49): loss=3.3100146533189507e+50\n",
      "Gradient Descent(24/49): loss=7.724724099518496e+52\n",
      "Gradient Descent(25/49): loss=1.8027522130161263e+55\n",
      "Gradient Descent(26/49): loss=4.207160669647228e+57\n",
      "Gradient Descent(27/49): loss=9.81843249029554e+59\n",
      "Gradient Descent(28/49): loss=2.2913699793304594e+62\n",
      "Gradient Descent(29/49): loss=5.347469045966661e+64\n",
      "Gradient Descent(30/49): loss=1.2479619378589924e+67\n",
      "Gradient Descent(31/49): loss=2.912422652580761e+69\n",
      "Gradient Descent(32/49): loss=6.796846482207489e+71\n",
      "Gradient Descent(33/49): loss=1.5862094075445809e+74\n",
      "Gradient Descent(34/49): loss=3.70180537572746e+76\n",
      "Gradient Descent(35/49): loss=8.639063023196293e+78\n",
      "Gradient Descent(36/49): loss=2.0161354351075254e+81\n",
      "Gradient Descent(37/49): loss=4.7051423074262423e+83\n",
      "Gradient Descent(38/49): loss=1.0980593737717692e+86\n",
      "Gradient Descent(39/49): loss=2.5625885670343974e+88\n",
      "Gradient Descent(40/49): loss=5.980423573398267e+90\n",
      "Gradient Descent(41/49): loss=1.3956772685771983e+93\n",
      "Gradient Descent(42/49): loss=3.257152297184618e+95\n",
      "Gradient Descent(43/49): loss=7.601356936815367e+97\n",
      "Gradient Descent(44/49): loss=1.7739614856454854e+100\n",
      "Gradient Descent(45/49): loss=4.139970506202745e+102\n",
      "Gradient Descent(46/49): loss=9.661627905068084e+104\n",
      "Gradient Descent(47/49): loss=2.2547758163042806e+107\n",
      "Gradient Descent(48/49): loss=5.262067667834495e+109\n",
      "Gradient Descent(49/49): loss=1.2280314495413597e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.831600323507174\n",
      "Gradient Descent(2/49): loss=242.21512123101365\n",
      "Gradient Descent(3/49): loss=14247.584788778757\n",
      "Gradient Descent(4/49): loss=974255.3996357874\n",
      "Gradient Descent(5/49): loss=95029386.83665136\n",
      "Gradient Descent(6/49): loss=14799668263.787785\n",
      "Gradient Descent(7/49): loss=3065861556438.078\n",
      "Gradient Descent(8/49): loss=700800546240846.0\n",
      "Gradient Descent(9/49): loss=1.6445431737780982e+17\n",
      "Gradient Descent(10/49): loss=3.88427846649766e+19\n",
      "Gradient Descent(11/49): loss=9.188732533000374e+21\n",
      "Gradient Descent(12/49): loss=2.1745251680896943e+24\n",
      "Gradient Descent(13/49): loss=5.146506883991954e+26\n",
      "Gradient Descent(14/49): loss=1.218064013349936e+29\n",
      "Gradient Descent(15/49): loss=2.8829023524707183e+31\n",
      "Gradient Descent(16/49): loss=6.823234487587107e+33\n",
      "Gradient Descent(17/49): loss=1.61491917458133e+36\n",
      "Gradient Descent(18/49): loss=3.822181596579849e+38\n",
      "Gradient Descent(19/49): loss=9.046318008039714e+40\n",
      "Gradient Descent(20/49): loss=2.1410774913604688e+43\n",
      "Gradient Descent(21/49): loss=5.0674902479871674e+45\n",
      "Gradient Descent(22/49): loss=1.1993707618350313e+48\n",
      "Gradient Descent(23/49): loss=2.838664021107988e+50\n",
      "Gradient Descent(24/49): loss=6.718534152454666e+52\n",
      "Gradient Descent(25/49): loss=1.5901389111992151e+55\n",
      "Gradient Descent(26/49): loss=3.7635318948077795e+57\n",
      "Gradient Descent(27/49): loss=8.907506270981285e+59\n",
      "Gradient Descent(28/49): loss=2.1082236097703213e+62\n",
      "Gradient Descent(29/49): loss=4.989731866114574e+64\n",
      "Gradient Descent(30/49): loss=1.1809669515290043e+67\n",
      "Gradient Descent(31/49): loss=2.795105985704415e+69\n",
      "Gradient Descent(32/49): loss=6.615441237543143e+71\n",
      "Gradient Descent(33/49): loss=1.5657389376724107e+74\n",
      "Gradient Descent(34/49): loss=3.7057821737284076e+76\n",
      "Gradient Descent(35/49): loss=8.770824553637429e+78\n",
      "Gradient Descent(36/49): loss=2.0758738572400534e+81\n",
      "Gradient Descent(37/49): loss=4.913166652484912e+83\n",
      "Gradient Descent(38/49): loss=1.1628455395253937e+86\n",
      "Gradient Descent(39/49): loss=2.752216328974308e+88\n",
      "Gradient Descent(40/49): loss=6.513930237514033e+90\n",
      "Gradient Descent(41/49): loss=1.541713370874921e+93\n",
      "Gradient Descent(42/49): loss=3.648918596404244e+95\n",
      "Gradient Descent(43/49): loss=8.636240156384524e+97\n",
      "Gradient Descent(44/49): loss=2.0440204972575776e+100\n",
      "Gradient Descent(45/49): loss=4.837776297965142e+102\n",
      "Gradient Descent(46/49): loss=1.1450021925198138e+105\n",
      "Gradient Descent(47/49): loss=2.709984794928679e+107\n",
      "Gradient Descent(48/49): loss=6.413976878579073e+109\n",
      "Gradient Descent(49/49): loss=1.5180564657017064e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.914871696860495\n",
      "Gradient Descent(2/49): loss=250.41538580729414\n",
      "Gradient Descent(3/49): loss=15434.89079604234\n",
      "Gradient Descent(4/49): loss=1213644.4204675015\n",
      "Gradient Descent(5/49): loss=150564038.74101645\n",
      "Gradient Descent(6/49): loss=28252475323.088715\n",
      "Gradient Descent(7/49): loss=6362271224627.7295\n",
      "Gradient Descent(8/49): loss=1510585767543159.0\n",
      "Gradient Descent(9/49): loss=3.6341831212421446e+17\n",
      "Gradient Descent(10/49): loss=8.770788670193299e+19\n",
      "Gradient Descent(11/49): loss=2.1183370394595817e+22\n",
      "Gradient Descent(12/49): loss=5.117150164079262e+24\n",
      "Gradient Descent(13/49): loss=1.2361733387590311e+27\n",
      "Gradient Descent(14/49): loss=2.9863098005568724e+29\n",
      "Gradient Descent(15/49): loss=7.214252787759479e+31\n",
      "Gradient Descent(16/49): loss=1.742802164732084e+34\n",
      "Gradient Descent(17/49): loss=4.2102208895793033e+36\n",
      "Gradient Descent(18/49): loss=1.0170953904041982e+39\n",
      "Gradient Descent(19/49): loss=2.4570754546732143e+41\n",
      "Gradient Descent(20/49): loss=5.935745906639384e+43\n",
      "Gradient Descent(21/49): loss=1.4339437323897136e+46\n",
      "Gradient Descent(22/49): loss=3.464088018921613e+48\n",
      "Gradient Descent(23/49): loss=8.36846351223761e+50\n",
      "Gradient Descent(24/49): loss=2.021634010842068e+53\n",
      "Gradient Descent(25/49): loss=4.8838165665917676e+55\n",
      "Gradient Descent(26/49): loss=1.1798210817688456e+58\n",
      "Gradient Descent(27/49): loss=2.8501844121424502e+60\n",
      "Gradient Descent(28/49): loss=6.885409413977308e+62\n",
      "Gradient Descent(29/49): loss=1.6633612406311117e+65\n",
      "Gradient Descent(30/49): loss=4.018309515796239e+67\n",
      "Gradient Descent(31/49): loss=9.707338953390764e+69\n",
      "Gradient Descent(32/49): loss=2.345076435391138e+72\n",
      "Gradient Descent(33/49): loss=5.665181275972574e+74\n",
      "Gradient Descent(34/49): loss=1.3685813564655573e+77\n",
      "Gradient Descent(35/49): loss=3.3061871068610595e+79\n",
      "Gradient Descent(36/49): loss=7.987010150279888e+81\n",
      "Gradient Descent(37/49): loss=1.9294833921616371e+84\n",
      "Gradient Descent(38/49): loss=4.6612012387351954e+86\n",
      "Gradient Descent(39/49): loss=1.1260421870564009e+89\n",
      "Gradient Descent(40/49): loss=2.7202666052985936e+91\n",
      "Gradient Descent(41/49): loss=6.571556988683255e+93\n",
      "Gradient Descent(42/49): loss=1.5875414994763564e+96\n",
      "Gradient Descent(43/49): loss=3.8351459431909403e+98\n",
      "Gradient Descent(44/49): loss=9.264856641810541e+100\n",
      "Gradient Descent(45/49): loss=2.2381825845689426e+103\n",
      "Gradient Descent(46/49): loss=5.406949589765858e+105\n",
      "Gradient Descent(47/49): loss=1.3061983444884981e+108\n",
      "Gradient Descent(48/49): loss=3.155483673037819e+110\n",
      "Gradient Descent(49/49): loss=7.622944289297409e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.9562520599253\n",
      "Gradient Descent(2/49): loss=255.3578300012945\n",
      "Gradient Descent(3/49): loss=15718.052817868918\n",
      "Gradient Descent(4/49): loss=1191123.1720527227\n",
      "Gradient Descent(5/49): loss=136446719.2501974\n",
      "Gradient Descent(6/49): loss=23822493841.898647\n",
      "Gradient Descent(7/49): loss=5126717895956.549\n",
      "Gradient Descent(8/49): loss=1178365081729359.8\n",
      "Gradient Descent(9/49): loss=2.7557195476900355e+17\n",
      "Gradient Descent(10/49): loss=6.472388908216194e+19\n",
      "Gradient Descent(11/49): loss=1.5217926654245602e+22\n",
      "Gradient Descent(12/49): loss=3.5789819899135887e+24\n",
      "Gradient Descent(13/49): loss=8.41765783497137e+26\n",
      "Gradient Descent(14/49): loss=1.9798387408845616e+29\n",
      "Gradient Descent(15/49): loss=4.656611781909291e+31\n",
      "Gradient Descent(16/49): loss=1.0952434258759156e+34\n",
      "Gradient Descent(17/49): loss=2.5760327339564098e+36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=6.058876833920683e+38\n",
      "Gradient Descent(19/49): loss=1.4250591039398185e+41\n",
      "Gradient Descent(20/49): loss=3.3517655371792574e+43\n",
      "Gradient Descent(21/49): loss=7.883414936600893e+45\n",
      "Gradient Descent(22/49): loss=1.8541938684514936e+48\n",
      "Gradient Descent(23/49): loss=4.361098495541957e+50\n",
      "Gradient Descent(24/49): loss=1.0257384846134722e+53\n",
      "Gradient Descent(25/49): loss=2.4125560105918487e+55\n",
      "Gradient Descent(26/49): loss=5.6743766481964764e+57\n",
      "Gradient Descent(27/49): loss=1.33462395087376e+60\n",
      "Gradient Descent(28/49): loss=3.139060377340451e+62\n",
      "Gradient Descent(29/49): loss=7.383128443137954e+64\n",
      "Gradient Descent(30/49): loss=1.7365255539957293e+67\n",
      "Gradient Descent(31/49): loss=4.084340429540929e+69\n",
      "Gradient Descent(32/49): loss=9.606444722911008e+71\n",
      "Gradient Descent(33/49): loss=2.2594536818449606e+74\n",
      "Gradient Descent(34/49): loss=5.3142771208864944e+76\n",
      "Gradient Descent(35/49): loss=1.2499278717020395e+79\n",
      "Gradient Descent(36/49): loss=2.9398536224564913e+81\n",
      "Gradient Descent(37/49): loss=6.914590447288567e+83\n",
      "Gradient Descent(38/49): loss=1.6263245451582645e+86\n",
      "Gradient Descent(39/49): loss=3.825145605292336e+88\n",
      "Gradient Descent(40/49): loss=8.99681367119962e+90\n",
      "Gradient Descent(41/49): loss=2.116067323614962e+93\n",
      "Gradient Descent(42/49): loss=4.9770297371001426e+95\n",
      "Gradient Descent(43/49): loss=1.1706066592277272e+98\n",
      "Gradient Descent(44/49): loss=2.7532886541014854e+100\n",
      "Gradient Descent(45/49): loss=6.475786168690496e+102\n",
      "Gradient Descent(46/49): loss=1.523116961969565e+105\n",
      "Gradient Descent(47/49): loss=3.582399448356757e+107\n",
      "Gradient Descent(48/49): loss=8.425870191210961e+109\n",
      "Gradient Descent(49/49): loss=1.981780354273482e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.0539212263979945\n",
      "Gradient Descent(2/49): loss=259.5572252125521\n",
      "Gradient Descent(3/49): loss=15799.022295097286\n",
      "Gradient Descent(4/49): loss=1166890.5457872562\n",
      "Gradient Descent(5/49): loss=128747919.37314494\n",
      "Gradient Descent(6/49): loss=21927300632.271915\n",
      "Gradient Descent(7/49): loss=4678524287612.963\n",
      "Gradient Descent(8/49): loss=1073102637364985.2\n",
      "Gradient Descent(9/49): loss=2.5087620720238832e+17\n",
      "Gradient Descent(10/49): loss=5.89306191849031e+19\n",
      "Gradient Descent(11/49): loss=1.3858899935618855e+22\n",
      "Gradient Descent(12/49): loss=3.2601700911999416e+24\n",
      "Gradient Descent(13/49): loss=7.669763641135899e+26\n",
      "Gradient Descent(14/49): loss=1.8043927834732463e+29\n",
      "Gradient Descent(15/49): loss=4.245041929310787e+31\n",
      "Gradient Descent(16/49): loss=9.986960378018967e+33\n",
      "Gradient Descent(17/49): loss=2.349550461921548e+36\n",
      "Gradient Descent(18/49): loss=5.52759548200688e+38\n",
      "Gradient Descent(19/49): loss=1.300432263888043e+41\n",
      "Gradient Descent(20/49): loss=3.059420835973068e+43\n",
      "Gradient Descent(21/49): loss=7.197649673676698e+45\n",
      "Gradient Descent(22/49): loss=1.693332287828844e+48\n",
      "Gradient Descent(23/49): loss=3.983764655343551e+50\n",
      "Gradient Descent(24/49): loss=9.372277929995759e+52\n",
      "Gradient Descent(25/49): loss=2.2049393274158144e+55\n",
      "Gradient Descent(26/49): loss=5.187380777546725e+57\n",
      "Gradient Descent(27/49): loss=1.2203927335634772e+60\n",
      "Gradient Descent(28/49): loss=2.8711183697585057e+62\n",
      "Gradient Descent(29/49): loss=6.75464583363641e+64\n",
      "Gradient Descent(30/49): loss=1.5891103905165475e+67\n",
      "Gradient Descent(31/49): loss=3.738570304711533e+69\n",
      "Gradient Descent(32/49): loss=8.795429195279204e+71\n",
      "Gradient Descent(33/49): loss=2.0692288341261774e+74\n",
      "Gradient Descent(34/49): loss=4.8681057773478654e+76\n",
      "Gradient Descent(35/49): loss=1.1452795103473768e+79\n",
      "Gradient Descent(36/49): loss=2.6944056206110614e+81\n",
      "Gradient Descent(37/49): loss=6.338908172886791e+83\n",
      "Gradient Descent(38/49): loss=1.4913031845285757e+86\n",
      "Gradient Descent(39/49): loss=3.5084672746919974e+88\n",
      "Gradient Descent(40/49): loss=8.254084578700637e+90\n",
      "Gradient Descent(41/49): loss=1.9418711049065295e+93\n",
      "Gradient Descent(42/49): loss=4.5684816433811265e+95\n",
      "Gradient Descent(43/49): loss=1.0747893860295339e+98\n",
      "Gradient Descent(44/49): loss=2.528569258881404e+100\n",
      "Gradient Descent(45/49): loss=5.94875850102999e+102\n",
      "Gradient Descent(46/49): loss=1.3995158558256118e+105\n",
      "Gradient Descent(47/49): loss=3.292526718588751e+107\n",
      "Gradient Descent(48/49): loss=7.74605885849379e+109\n",
      "Gradient Descent(49/49): loss=1.8223520404708834e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.870372989995706\n",
      "Gradient Descent(2/49): loss=246.39824355736863\n",
      "Gradient Descent(3/49): loss=14617.045905584218\n",
      "Gradient Descent(4/49): loss=1007739.5890156768\n",
      "Gradient Descent(5/49): loss=99047163.58637968\n",
      "Gradient Descent(6/49): loss=15540040493.068144\n",
      "Gradient Descent(7/49): loss=3244150338644.831\n",
      "Gradient Descent(8/49): loss=747462984402967.9\n",
      "Gradient Descent(9/49): loss=1.768165817921412e+17\n",
      "Gradient Descent(10/49): loss=4.209989562311291e+19\n",
      "Gradient Descent(11/49): loss=1.0039732166453476e+22\n",
      "Gradient Descent(12/49): loss=2.395121747212529e+24\n",
      "Gradient Descent(13/49): loss=5.714425307964231e+26\n",
      "Gradient Descent(14/49): loss=1.363411701184319e+29\n",
      "Gradient Descent(15/49): loss=3.252997677145508e+31\n",
      "Gradient Descent(16/49): loss=7.761417349971221e+33\n",
      "Gradient Descent(17/49): loss=1.8518186457402608e+36\n",
      "Gradient Descent(18/49): loss=4.418307110247936e+38\n",
      "Gradient Descent(19/49): loss=1.0541765580932541e+41\n",
      "Gradient Descent(20/49): loss=2.515190077492872e+43\n",
      "Gradient Descent(21/49): loss=6.00106413269583e+45\n",
      "Gradient Descent(22/49): loss=1.4318111007083547e+48\n",
      "Gradient Descent(23/49): loss=3.416199165513949e+50\n",
      "Gradient Descent(24/49): loss=8.150807556141834e+52\n",
      "Gradient Descent(25/49): loss=1.9447245490877723e+55\n",
      "Gradient Descent(26/49): loss=4.6399740710082644e+57\n",
      "Gradient Descent(27/49): loss=1.1070647197687787e+60\n",
      "Gradient Descent(28/49): loss=2.6413774624616622e+62\n",
      "Gradient Descent(29/49): loss=6.30213823511403e+64\n",
      "Gradient Descent(30/49): loss=1.5036452343116054e+67\n",
      "Gradient Descent(31/49): loss=3.587590284945812e+69\n",
      "Gradient Descent(32/49): loss=8.559734543055226e+71\n",
      "Gradient Descent(33/49): loss=2.042291611587389e+74\n",
      "Gradient Descent(34/49): loss=4.8727621233818246e+76\n",
      "Gradient Descent(35/49): loss=1.162606288756632e+79\n",
      "Gradient Descent(36/49): loss=2.773895684689032e+81\n",
      "Gradient Descent(37/49): loss=6.618317261783812e+83\n",
      "Gradient Descent(38/49): loss=1.5790832949991045e+86\n",
      "Gradient Descent(39/49): loss=3.767580117295748e+88\n",
      "Gradient Descent(40/49): loss=8.989177445671308e+90\n",
      "Gradient Descent(41/49): loss=2.1447536252464627e+93\n",
      "Gradient Descent(42/49): loss=5.117229180099143e+95\n",
      "Gradient Descent(43/49): loss=1.2209343848830049e+98\n",
      "Gradient Descent(44/49): loss=2.9130623619259422e+100\n",
      "Gradient Descent(45/49): loss=6.950359027920024e+102\n",
      "Gradient Descent(46/49): loss=1.658306092185786e+105\n",
      "Gradient Descent(47/49): loss=3.956600060994923e+107\n",
      "Gradient Descent(48/49): loss=9.440165549914436e+109\n",
      "Gradient Descent(49/49): loss=2.2523561703475013e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.954196394920432\n",
      "Gradient Descent(2/49): loss=254.7193872411054\n",
      "Gradient Descent(3/49): loss=15832.23897152592\n",
      "Gradient Descent(4/49): loss=1254832.1105375485\n",
      "Gradient Descent(5/49): loss=156839150.29451105\n",
      "Gradient Descent(6/49): loss=29652724796.89594\n",
      "Gradient Descent(7/49): loss=6730050754189.978\n",
      "Gradient Descent(8/49): loss=1610674281085797.2\n",
      "Gradient Descent(9/49): loss=3.9061028859414e+17\n",
      "Gradient Descent(10/49): loss=9.502879350162085e+19\n",
      "Gradient Descent(11/49): loss=2.3136236713827356e+22\n",
      "Gradient Descent(12/49): loss=5.6338759488443e+24\n",
      "Gradient Descent(13/49): loss=1.371955569381755e+27\n",
      "Gradient Descent(14/49): loss=3.3410049164493456e+29\n",
      "Gradient Descent(15/49): loss=8.136079777682123e+31\n",
      "Gradient Descent(16/49): loss=1.9813149751025997e+34\n",
      "Gradient Descent(17/49): loss=4.824939836218957e+36\n",
      "Gradient Descent(18/49): loss=1.1749795178214047e+39\n",
      "Gradient Descent(19/49): loss=2.8613349061734522e+41\n",
      "Gradient Descent(20/49): loss=6.96798313093524e+43\n",
      "Gradient Descent(21/49): loss=1.6968579535369107e+46\n",
      "Gradient Descent(22/49): loss=4.13222429023216e+48\n",
      "Gradient Descent(23/49): loss=1.006287977704959e+51\n",
      "Gradient Descent(24/49): loss=2.4505337149113824e+53\n",
      "Gradient Descent(25/49): loss=5.96759140621031e+55\n",
      "Gradient Descent(26/49): loss=1.4532404502246938e+58\n",
      "Gradient Descent(27/49): loss=3.538961806218155e+60\n",
      "Gradient Descent(28/49): loss=8.618154458840592e+62\n",
      "Gradient Descent(29/49): loss=2.098711157208078e+65\n",
      "Gradient Descent(30/49): loss=5.110825690611035e+67\n",
      "Gradient Descent(31/49): loss=1.2445990554773489e+70\n",
      "Gradient Descent(32/49): loss=3.030873879617531e+72\n",
      "Gradient Descent(33/49): loss=7.380848019866622e+74\n",
      "Gradient Descent(34/49): loss=1.7973996826038543e+77\n",
      "Gradient Descent(35/49): loss=4.3770656303024984e+79\n",
      "Gradient Descent(36/49): loss=1.0659122574351728e+82\n",
      "Gradient Descent(37/49): loss=2.5957320189234324e+84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/49): loss=6.321181379673029e+86\n",
      "Gradient Descent(39/49): loss=1.5393474265997697e+89\n",
      "Gradient Descent(40/49): loss=3.7486513318527444e+91\n",
      "Gradient Descent(41/49): loss=9.128794815892155e+93\n",
      "Gradient Descent(42/49): loss=2.223063374354235e+96\n",
      "Gradient Descent(43/49): loss=5.4136508335051483e+98\n",
      "Gradient Descent(44/49): loss=1.3183436732038681e+101\n",
      "Gradient Descent(45/49): loss=3.210458328638363e+103\n",
      "Gradient Descent(46/49): loss=7.818175859163592e+105\n",
      "Gradient Descent(47/49): loss=1.9038986807448212e+108\n",
      "Gradient Descent(48/49): loss=4.636414237591256e+110\n",
      "Gradient Descent(49/49): loss=1.1290693774801582e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.995919539924228\n",
      "Gradient Descent(2/49): loss=259.74452632194584\n",
      "Gradient Descent(3/49): loss=16122.840469141584\n",
      "Gradient Descent(4/49): loss=1231646.4084182687\n",
      "Gradient Descent(5/49): loss=142153099.03255716\n",
      "Gradient Descent(6/49): loss=25005795512.796898\n",
      "Gradient Descent(7/49): loss=5423457649158.857\n",
      "Gradient Descent(8/49): loss=1256516976654247.8\n",
      "Gradient Descent(9/49): loss=2.9620911643734426e+17\n",
      "Gradient Descent(10/49): loss=7.013095814881358e+19\n",
      "Gradient Descent(11/49): loss=1.6622040827333538e+22\n",
      "Gradient Descent(12/49): loss=3.9406929029111724e+24\n",
      "Gradient Descent(13/49): loss=9.343050832625204e+26\n",
      "Gradient Descent(14/49): loss=2.2151934813162058e+29\n",
      "Gradient Descent(15/49): loss=5.25213998489163e+31\n",
      "Gradient Descent(16/49): loss=1.2452637087529478e+34\n",
      "Gradient Descent(17/49): loss=2.9524766441716797e+36\n",
      "Gradient Descent(18/49): loss=7.000219124958834e+38\n",
      "Gradient Descent(19/49): loss=1.659727557087888e+41\n",
      "Gradient Descent(20/49): loss=3.935156205810242e+43\n",
      "Gradient Descent(21/49): loss=9.330118254020162e+45\n",
      "Gradient Descent(22/49): loss=2.21213853018919e+48\n",
      "Gradient Descent(23/49): loss=5.244903380388767e+50\n",
      "Gradient Descent(24/49): loss=1.2435483173650635e+53\n",
      "Gradient Descent(25/49): loss=2.948409733167547e+55\n",
      "Gradient Descent(26/49): loss=6.990576749814875e+57\n",
      "Gradient Descent(27/49): loss=1.6574413910430978e+60\n",
      "Gradient Descent(28/49): loss=3.929735790134488e+62\n",
      "Gradient Descent(29/49): loss=9.317266639844908e+64\n",
      "Gradient Descent(30/49): loss=2.2090914573927586e+67\n",
      "Gradient Descent(31/49): loss=5.237678877039125e+69\n",
      "Gradient Descent(32/49): loss=1.241835412797216e+72\n",
      "Gradient Descent(33/49): loss=2.9443484961205807e+74\n",
      "Gradient Descent(34/49): loss=6.98094769827837e+76\n",
      "Gradient Descent(35/49): loss=1.6551583764730427e+79\n",
      "Gradient Descent(36/49): loss=3.924322842132925e+81\n",
      "Gradient Descent(37/49): loss=9.304432728729287e+83\n",
      "Gradient Descent(38/49): loss=2.2060485817827256e+86\n",
      "Gradient Descent(39/49): loss=5.230464324986569e+88\n",
      "Gradient Descent(40/49): loss=1.240124867642289e+91\n",
      "Gradient Descent(41/49): loss=2.9402928531564925e+93\n",
      "Gradient Descent(42/49): loss=6.971331910116045e+95\n",
      "Gradient Descent(43/49): loss=1.6528785066028256e+98\n",
      "Gradient Descent(44/49): loss=3.918917350105169e+100\n",
      "Gradient Descent(45/49): loss=9.291616495468083e+102\n",
      "Gradient Descent(46/49): loss=2.2030098975304383e+105\n",
      "Gradient Descent(47/49): loss=5.223259710497191e+107\n",
      "Gradient Descent(48/49): loss=1.2384166786489345e+110\n",
      "Gradient Descent(49/49): loss=2.9362427965694323e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.094443807873212\n",
      "Gradient Descent(2/49): loss=264.01773545804946\n",
      "Gradient Descent(3/49): loss=16206.207868866592\n",
      "Gradient Descent(4/49): loss=1206650.594881793\n",
      "Gradient Descent(5/49): loss=134140725.52245978\n",
      "Gradient Descent(6/49): loss=23017212627.97492\n",
      "Gradient Descent(7/49): loss=4949373861840.8545\n",
      "Gradient Descent(8/49): loss=1144276239392012.0\n",
      "Gradient Descent(9/49): loss=2.696640137472164e+17\n",
      "Gradient Descent(10/49): loss=6.385368929689341e+19\n",
      "Gradient Descent(11/49): loss=1.5137611083311313e+22\n",
      "Gradient Descent(12/49): loss=3.589657542165542e+24\n",
      "Gradient Descent(13/49): loss=8.512930204074545e+26\n",
      "Gradient Descent(14/49): loss=2.0188894966419827e+29\n",
      "Gradient Descent(15/49): loss=4.787930497163379e+31\n",
      "Gradient Descent(16/49): loss=1.1354906675423525e+34\n",
      "Gradient Descent(17/49): loss=2.6928949324000875e+36\n",
      "Gradient Descent(18/49): loss=6.3863876344345236e+38\n",
      "Gradient Descent(19/49): loss=1.5145762700499932e+41\n",
      "Gradient Descent(20/49): loss=3.59192303605138e+43\n",
      "Gradient Descent(21/49): loss=8.518495478563566e+45\n",
      "Gradient Descent(22/49): loss=2.020220492079346e+48\n",
      "Gradient Descent(23/49): loss=4.7910935060114124e+50\n",
      "Gradient Descent(24/49): loss=1.1362411713790216e+53\n",
      "Gradient Descent(25/49): loss=2.6946750213098144e+55\n",
      "Gradient Descent(26/49): loss=6.390609364791685e+57\n",
      "Gradient Descent(27/49): loss=1.5155774900648132e+60\n",
      "Gradient Descent(28/49): loss=3.5942975032181075e+62\n",
      "Gradient Descent(29/49): loss=8.524126695156715e+64\n",
      "Gradient Descent(30/49): loss=2.0215559744296928e+67\n",
      "Gradient Descent(31/49): loss=4.794260695438093e+69\n",
      "Gradient Descent(32/49): loss=1.1369922924002315e+72\n",
      "Gradient Descent(33/49): loss=2.6964563570930974e+74\n",
      "Gradient Descent(34/49): loss=6.394833926586002e+76\n",
      "Gradient Descent(35/49): loss=1.516579374297743e+79\n",
      "Gradient Descent(36/49): loss=3.5966735414085056e+81\n",
      "Gradient Descent(37/49): loss=8.529761635099537e+83\n",
      "Gradient Descent(38/49): loss=2.0228923396568007e+86\n",
      "Gradient Descent(39/49): loss=4.797429978586218e+88\n",
      "Gradient Descent(40/49): loss=1.1377439099572938e+91\n",
      "Gradient Descent(41/49): loss=2.698238870442844e+93\n",
      "Gradient Descent(42/49): loss=6.399061281059348e+95\n",
      "Gradient Descent(43/49): loss=1.517581920833896e+98\n",
      "Gradient Descent(44/49): loss=3.599051150297163e+100\n",
      "Gradient Descent(45/49): loss=8.535400300062499e+102\n",
      "Gradient Descent(46/49): loss=2.024229588298365e+105\n",
      "Gradient Descent(47/49): loss=4.800601356813504e+107\n",
      "Gradient Descent(48/49): loss=1.1384960243769786e+110\n",
      "Gradient Descent(49/49): loss=2.70002256213702e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.909314449612583\n",
      "Gradient Descent(2/49): loss=250.6354162800102\n",
      "Gradient Descent(3/49): loss=14994.45916777506\n",
      "Gradient Descent(4/49): loss=1042227.126495671\n",
      "Gradient Descent(5/49): loss=103217329.65204552\n",
      "Gradient Descent(6/49): loss=16314243023.163849\n",
      "Gradient Descent(7/49): loss=3432028674609.0283\n",
      "Gradient Descent(8/49): loss=797026365293603.0\n",
      "Gradient Descent(9/49): loss=1.9005289204334902e+17\n",
      "Gradient Descent(10/49): loss=4.561539662122283e+19\n",
      "Gradient Descent(11/49): loss=1.096565050299905e+22\n",
      "Gradient Descent(12/49): loss=2.6370750157608786e+24\n",
      "Gradient Descent(13/49): loss=6.342351036299878e+26\n",
      "Gradient Descent(14/49): loss=1.5254135924028682e+29\n",
      "Gradient Descent(15/49): loss=3.6688270744159925e+31\n",
      "Gradient Descent(16/49): loss=8.824039106086272e+33\n",
      "Gradient Descent(17/49): loss=2.1223047128820665e+36\n",
      "Gradient Descent(18/49): loss=5.104439781548576e+38\n",
      "Gradient Descent(19/49): loss=1.227689209031222e+41\n",
      "Gradient Descent(20/49): loss=2.952764387204263e+43\n",
      "Gradient Descent(21/49): loss=7.101811656471387e+45\n",
      "Gradient Descent(22/49): loss=1.7080851094960808e+48\n",
      "Gradient Descent(23/49): loss=4.108183774199718e+50\n",
      "Gradient Descent(24/49): loss=9.880757012052826e+52\n",
      "Gradient Descent(25/49): loss=2.3764603653979402e+55\n",
      "Gradient Descent(26/49): loss=5.715719819263746e+57\n",
      "Gradient Descent(27/49): loss=1.3747106212251186e+60\n",
      "Gradient Descent(28/49): loss=3.3063714665298154e+62\n",
      "Gradient Descent(29/49): loss=7.952286180010682e+64\n",
      "Gradient Descent(30/49): loss=1.9126361368935977e+67\n",
      "Gradient Descent(31/49): loss=4.600157626805198e+69\n",
      "Gradient Descent(32/49): loss=1.106402299071029e+72\n",
      "Gradient Descent(33/49): loss=2.661052395806244e+74\n",
      "Gradient Descent(34/49): loss=6.400203487620883e+76\n",
      "Gradient Descent(35/49): loss=1.539338524393987e+79\n",
      "Gradient Descent(36/49): loss=3.7023246171259817e+81\n",
      "Gradient Descent(37/49): loss=8.904608929977613e+83\n",
      "Gradient Descent(38/49): loss=2.1416830882157827e+86\n",
      "Gradient Descent(39/49): loss=5.15104760514285e+88\n",
      "Gradient Descent(40/49): loss=1.238899049838088e+91\n",
      "Gradient Descent(41/49): loss=2.97972562737972e+93\n",
      "Gradient Descent(42/49): loss=7.166657215229692e+95\n",
      "Gradient Descent(43/49): loss=1.7236813741730307e+98\n",
      "Gradient Descent(44/49): loss=4.145694973881541e+100\n",
      "Gradient Descent(45/49): loss=9.970976697890313e+102\n",
      "Gradient Descent(46/49): loss=2.3981594626771863e+105\n",
      "Gradient Descent(47/49): loss=5.767909185511381e+107\n",
      "Gradient Descent(48/49): loss=1.3872628943183839e+110\n",
      "Gradient Descent(49/49): loss=3.3365614402994222e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.993691505047014\n",
      "Gradient Descent(2/49): loss=259.0787346024404\n",
      "Gradient Descent(3/49): loss=16238.079175981271\n",
      "Gradient Descent(4/49): loss=1297238.829647153\n",
      "Gradient Descent(5/49): loss=163348863.2964703\n",
      "Gradient Descent(6/49): loss=31116351241.74003\n",
      "Gradient Descent(7/49): loss=7117492782357.971\n",
      "Gradient Descent(8/49): loss=1716954640651200.5\n",
      "Gradient Descent(9/49): loss=4.1971592081590496e+17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(10/49): loss=1.0292782253471303e+20\n",
      "Gradient Descent(11/49): loss=2.5260240516663195e+22\n",
      "Gradient Descent(12/49): loss=6.200398501183204e+24\n",
      "Gradient Descent(13/49): loss=1.522018831287535e+27\n",
      "Gradient Descent(14/49): loss=3.736154048644709e+29\n",
      "Gradient Descent(15/49): loss=9.171292528197897e+31\n",
      "Gradient Descent(16/49): loss=2.251316523350831e+34\n",
      "Gradient Descent(17/49): loss=5.526403981976548e+36\n",
      "Gradient Descent(18/49): loss=1.356590315238291e+39\n",
      "Gradient Descent(19/49): loss=3.3300810127475248e+41\n",
      "Gradient Descent(20/49): loss=8.174494131460414e+43\n",
      "Gradient Descent(21/49): loss=2.0066284897244533e+46\n",
      "Gradient Descent(22/49): loss=4.925757889613762e+48\n",
      "Gradient Descent(23/49): loss=1.2091471297200755e+51\n",
      "Gradient Descent(24/49): loss=2.9681458449287194e+53\n",
      "Gradient Descent(25/49): loss=7.286036198769663e+55\n",
      "Gradient Descent(26/49): loss=1.7885348720481166e+58\n",
      "Gradient Descent(27/49): loss=4.39039403766986e+60\n",
      "Gradient Descent(28/49): loss=1.0777290455586244e+63\n",
      "Gradient Descent(29/49): loss=2.6455481801290823e+65\n",
      "Gradient Descent(30/49): loss=6.494141734629009e+67\n",
      "Gradient Descent(31/49): loss=1.594145107098074e+70\n",
      "Gradient Descent(32/49): loss=3.91321705982138e+72\n",
      "Gradient Descent(33/49): loss=9.605943454640951e+74\n",
      "Gradient Descent(34/49): loss=2.3580125570129277e+77\n",
      "Gradient Descent(35/49): loss=5.788315583248876e+79\n",
      "Gradient Descent(36/49): loss=1.4208829037672589e+82\n",
      "Gradient Descent(37/49): loss=3.4879028228190423e+84\n",
      "Gradient Descent(38/49): loss=8.561906170574568e+86\n",
      "Gradient Descent(39/49): loss=2.1017282016611346e+89\n",
      "Gradient Descent(40/49): loss=5.159203272793474e+91\n",
      "Gradient Descent(41/49): loss=1.2664519793265998e+94\n",
      "Gradient Descent(42/49): loss=3.1088145419628274e+96\n",
      "Gradient Descent(43/49): loss=7.631341743773385e+98\n",
      "Gradient Descent(44/49): loss=1.8732985201969676e+101\n",
      "Gradient Descent(45/49): loss=4.598467037117552e+103\n",
      "Gradient Descent(46/49): loss=1.1288056261974335e+106\n",
      "Gradient Descent(47/49): loss=2.7709280754868477e+108\n",
      "Gradient Descent(48/49): loss=6.801917195776147e+110\n",
      "Gradient Descent(49/49): loss=1.66969608296553e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.035758835220069\n",
      "Gradient Descent(2/49): loss=264.18760030330395\n",
      "Gradient Descent(3/49): loss=16536.28177033625\n",
      "Gradient Descent(4/49): loss=1273371.8426257465\n",
      "Gradient Descent(5/49): loss=148073556.88729408\n",
      "Gradient Descent(6/49): loss=26242775971.65763\n",
      "Gradient Descent(7/49): loss=5736082739826.228\n",
      "Gradient Descent(8/49): loss=1339508342011964.8\n",
      "Gradient Descent(9/49): loss=3.18299906387547e+17\n",
      "Gradient Descent(10/49): loss=7.596537998734179e+19\n",
      "Gradient Descent(11/49): loss=1.8149307457488082e+22\n",
      "Gradient Descent(12/49): loss=4.337291428533842e+24\n",
      "Gradient Descent(13/49): loss=1.0365855880928758e+27\n",
      "Gradient Descent(14/49): loss=2.4774140743720786e+29\n",
      "Gradient Descent(15/49): loss=5.920981715337133e+31\n",
      "Gradient Descent(16/49): loss=1.4151069133851743e+34\n",
      "Gradient Descent(17/49): loss=3.3820878699122874e+36\n",
      "Gradient Descent(18/49): loss=8.08314828119451e+38\n",
      "Gradient Descent(19/49): loss=1.931862493413752e+41\n",
      "Gradient Descent(20/49): loss=4.617127604837924e+43\n",
      "Gradient Descent(21/49): loss=1.1034878212107734e+46\n",
      "Gradient Descent(22/49): loss=2.637322326844099e+48\n",
      "Gradient Descent(23/49): loss=6.303167939263677e+50\n",
      "Gradient Descent(24/49): loss=1.5064493887100655e+53\n",
      "Gradient Descent(25/49): loss=3.6003955195535036e+55\n",
      "Gradient Descent(26/49): loss=8.604901030449075e+57\n",
      "Gradient Descent(27/49): loss=2.0565607678853214e+60\n",
      "Gradient Descent(28/49): loss=4.915154953019239e+62\n",
      "Gradient Descent(29/49): loss=1.1747159913505158e+65\n",
      "Gradient Descent(30/49): loss=2.8075567780156997e+67\n",
      "Gradient Descent(31/49): loss=6.710026184899066e+69\n",
      "Gradient Descent(32/49): loss=1.6036880092538436e+72\n",
      "Gradient Descent(33/49): loss=3.8327946272585595e+74\n",
      "Gradient Descent(34/49): loss=9.160332040879386e+76\n",
      "Gradient Descent(35/49): loss=2.1893080965619806e+79\n",
      "Gradient Descent(36/49): loss=5.232419436633982e+81\n",
      "Gradient Descent(37/49): loss=1.250541812906953e+84\n",
      "Gradient Descent(38/49): loss=2.9887795593746436e+86\n",
      "Gradient Descent(39/49): loss=7.143146404493857e+88\n",
      "Gradient Descent(40/49): loss=1.7072032092828405e+91\n",
      "Gradient Descent(41/49): loss=4.0801946827689626e+93\n",
      "Gradient Descent(42/49): loss=9.751615132149403e+95\n",
      "Gradient Descent(43/49): loss=2.3306240284846256e+98\n",
      "Gradient Descent(44/49): loss=5.570162776668778e+100\n",
      "Gradient Descent(45/49): loss=1.3312620559721869e+103\n",
      "Gradient Descent(46/49): loss=3.1816999479702814e+105\n",
      "Gradient Descent(47/49): loss=7.604223761580332e+107\n",
      "Gradient Descent(48/49): loss=1.8174001308033813e+110\n",
      "Gradient Descent(49/49): loss=4.3435639705027974e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.135141862830184\n",
      "Gradient Descent(2/49): loss=268.5355912133369\n",
      "Gradient Descent(3/49): loss=16622.104783588256\n",
      "Gradient Descent(4/49): loss=1247591.977428606\n",
      "Gradient Descent(5/49): loss=139736166.3165285\n",
      "Gradient Descent(6/49): loss=24156602426.020634\n",
      "Gradient Descent(7/49): loss=5234725733570.318\n",
      "Gradient Descent(8/49): loss=1219857385228107.8\n",
      "Gradient Descent(9/49): loss=2.8977519084814624e+17\n",
      "Gradient Descent(10/49): loss=6.916585689490575e+19\n",
      "Gradient Descent(11/49): loss=1.6528474973464487e+22\n",
      "Gradient Descent(12/49): loss=3.950924346620579e+24\n",
      "Gradient Descent(13/49): loss=9.444852481175496e+26\n",
      "Gradient Descent(14/49): loss=2.2578709084381257e+29\n",
      "Gradient Descent(15/49): loss=5.397651749354468e+31\n",
      "Gradient Descent(16/49): loss=1.290360498737936e+34\n",
      "Gradient Descent(17/49): loss=3.084731493900426e+36\n",
      "Gradient Descent(18/49): loss=7.374349247537273e+38\n",
      "Gradient Descent(19/49): loss=1.7629095999804542e+41\n",
      "Gradient Descent(20/49): loss=4.214406132342712e+43\n",
      "Gradient Descent(21/49): loss=1.0074946023564262e+46\n",
      "Gradient Descent(22/49): loss=2.4085134230647847e+48\n",
      "Gradient Descent(23/49): loss=5.757784603337296e+50\n",
      "Gradient Descent(24/49): loss=1.3764541738229422e+53\n",
      "Gradient Descent(25/49): loss=3.290547012721709e+55\n",
      "Gradient Descent(26/49): loss=7.866371324856813e+57\n",
      "Gradient Descent(27/49): loss=1.88053225136444e+60\n",
      "Gradient Descent(28/49): loss=4.495594477274936e+62\n",
      "Gradient Descent(29/49): loss=1.074715399825825e+65\n",
      "Gradient Descent(30/49): loss=2.5692112499500278e+67\n",
      "Gradient Descent(31/49): loss=6.141948322262323e+69\n",
      "Gradient Descent(32/49): loss=1.4682922314805999e+72\n",
      "Gradient Descent(33/49): loss=3.5100947841126095e+74\n",
      "Gradient Descent(34/49): loss=8.391221535668587e+76\n",
      "Gradient Descent(35/49): loss=2.0060027774568323e+79\n",
      "Gradient Descent(36/49): loss=4.795543921775427e+81\n",
      "Gradient Descent(37/49): loss=1.146421219557466e+84\n",
      "Gradient Descent(38/49): loss=2.740630956759227e+86\n",
      "Gradient Descent(39/49): loss=6.551743733464826e+88\n",
      "Gradient Descent(40/49): loss=1.5662577934153708e+91\n",
      "Gradient Descent(41/49): loss=3.7442909479261987e+93\n",
      "Gradient Descent(42/49): loss=8.951090147267908e+95\n",
      "Gradient Descent(43/49): loss=2.1398447914122267e+98\n",
      "Gradient Descent(44/49): loss=5.1155062187945715e+100\n",
      "Gradient Descent(45/49): loss=1.222911305509025e+103\n",
      "Gradient Descent(46/49): loss=2.923487915325322e+105\n",
      "Gradient Descent(47/49): loss=6.988881002695154e+107\n",
      "Gradient Descent(48/49): loss=1.6707596913188835e+110\n",
      "Gradient Descent(49/49): loss=3.99411285591993e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.948424702357802\n",
      "Gradient Descent(2/49): loss=254.9271031754826\n",
      "Gradient Descent(3/49): loss=15379.960956151715\n",
      "Gradient Descent(4/49): loss=1077743.6893201238\n",
      "Gradient Descent(5/49): loss=107545006.24833727\n",
      "Gradient Descent(6/49): loss=17123676181.073624\n",
      "Gradient Descent(7/49): loss=3629970816304.4204\n",
      "Gradient Descent(8/49): loss=849658331162405.5\n",
      "Gradient Descent(9/49): loss=2.042211718044237e+17\n",
      "Gradient Descent(10/49): loss=4.940862704942535e+19\n",
      "Gradient Descent(11/49): loss=1.1972743010499254e+22\n",
      "Gradient Descent(12/49): loss=2.9023544722805776e+24\n",
      "Gradient Descent(13/49): loss=7.036345555625469e+26\n",
      "Gradient Descent(14/49): loss=1.705899592910253e+29\n",
      "Gradient Descent(15/49): loss=4.135824294707078e+31\n",
      "Gradient Descent(16/49): loss=1.0027005418164134e+34\n",
      "Gradient Descent(17/49): loss=2.430975336385973e+36\n",
      "Gradient Descent(18/49): loss=5.893725271398829e+38\n",
      "Gradient Descent(19/49): loss=1.428891427743442e+41\n",
      "Gradient Descent(20/49): loss=3.464244814590279e+43\n",
      "Gradient Descent(21/49): loss=8.398813174143301e+45\n",
      "Gradient Descent(22/49): loss=2.036232036907663e+48\n",
      "Gradient Descent(23/49): loss=4.936698581580738e+50\n",
      "Gradient Descent(24/49): loss=1.1968671764314708e+53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=2.901718657421688e+55\n",
      "Gradient Descent(26/49): loss=7.035008840279524e+57\n",
      "Gradient Descent(27/49): loss=1.7055874543949725e+60\n",
      "Gradient Descent(28/49): loss=4.135074497609403e+62\n",
      "Gradient Descent(29/49): loss=1.0025191646854184e+65\n",
      "Gradient Descent(30/49): loss=2.4305358371235334e+67\n",
      "Gradient Descent(31/49): loss=5.892659874881857e+69\n",
      "Gradient Descent(32/49): loss=1.4286331380383115e+72\n",
      "Gradient Descent(33/49): loss=3.463618614407273e+74\n",
      "Gradient Descent(34/49): loss=8.397294999429743e+76\n",
      "Gradient Descent(35/49): loss=2.0358639664925276e+79\n",
      "Gradient Descent(36/49): loss=4.935806221341779e+81\n",
      "Gradient Descent(37/49): loss=1.1966508300949202e+84\n",
      "Gradient Descent(38/49): loss=2.901194141243189e+86\n",
      "Gradient Descent(39/49): loss=7.033737188413075e+88\n",
      "Gradient Descent(40/49): loss=1.7052791515174114e+91\n",
      "Gradient Descent(41/49): loss=4.1343270393870967e+93\n",
      "Gradient Descent(42/49): loss=1.0023379487984457e+96\n",
      "Gradient Descent(43/49): loss=2.430096492198237e+98\n",
      "Gradient Descent(44/49): loss=5.891594714610313e+100\n",
      "Gradient Descent(45/49): loss=1.42837489756732e+103\n",
      "Gradient Descent(46/49): loss=3.462992528900413e+105\n",
      "Gradient Descent(47/49): loss=8.3957771000066e+107\n",
      "Gradient Descent(48/49): loss=2.0354959626603248e+110\n",
      "Gradient Descent(49/49): loss=4.934914022435306e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.033357027240248\n",
      "Gradient Descent(2/49): loss=263.4939005348247\n",
      "Gradient Descent(3/49): loss=16652.556048743543\n",
      "Gradient Descent(4/49): loss=1340895.4006210915\n",
      "Gradient Descent(5/49): loss=170100948.54531822\n",
      "Gradient Descent(6/49): loss=32645952684.613228\n",
      "Gradient Descent(7/49): loss=7525563799329.233\n",
      "Gradient Descent(8/49): loss=1829782908870596.8\n",
      "Gradient Descent(9/49): loss=4.508614432696723e+17\n",
      "Gradient Descent(10/49): loss=1.114480468584804e+20\n",
      "Gradient Descent(11/49): loss=2.7569606218229146e+22\n",
      "Gradient Descent(12/49): loss=6.821289091365754e+24\n",
      "Gradient Descent(13/49): loss=1.687799081069651e+27\n",
      "Gradient Descent(14/49): loss=4.176181710774332e+29\n",
      "Gradient Descent(15/49): loss=1.0333300444108284e+32\n",
      "Gradient Descent(16/49): loss=2.556813023749331e+34\n",
      "Gradient Descent(17/49): loss=6.326433398138417e+36\n",
      "Gradient Descent(18/49): loss=1.5653769134778793e+39\n",
      "Gradient Descent(19/49): loss=3.873280164645974e+41\n",
      "Gradient Descent(20/49): loss=9.583825550857096e+43\n",
      "Gradient Descent(21/49): loss=2.3713676347419516e+46\n",
      "Gradient Descent(22/49): loss=5.867578066620069e+48\n",
      "Gradient Descent(23/49): loss=1.4518403584615037e+51\n",
      "Gradient Descent(24/49): loss=3.5923517378669213e+53\n",
      "Gradient Descent(25/49): loss=8.888712132405052e+55\n",
      "Gradient Descent(26/49): loss=2.1993726989468654e+58\n",
      "Gradient Descent(27/49): loss=5.442003517290622e+60\n",
      "Gradient Descent(28/49): loss=1.3465385969547203e+63\n",
      "Gradient Descent(29/49): loss=3.3317990099195624e+65\n",
      "Gradient Descent(30/49): loss=8.244015186498514e+67\n",
      "Gradient Descent(31/49): loss=2.0398525299056467e+70\n",
      "Gradient Descent(32/49): loss=5.047295825676344e+72\n",
      "Gradient Descent(33/49): loss=1.2488743562784594e+75\n",
      "Gradient Descent(34/49): loss=3.0901441319044523e+77\n",
      "Gradient Descent(35/49): loss=7.646078012522305e+79\n",
      "Gradient Descent(36/49): loss=1.8919023345860787e+82\n",
      "Gradient Descent(37/49): loss=4.6812162232067025e+84\n",
      "Gradient Descent(38/49): loss=1.1582936881997061e+87\n",
      "Gradient Descent(39/49): loss=2.866016445624089e+89\n",
      "Gradient Descent(40/49): loss=7.091509131293377e+91\n",
      "Gradient Descent(41/49): loss=1.754682944544882e+94\n",
      "Gradient Descent(42/49): loss=4.341688318908221e+96\n",
      "Gradient Descent(43/49): loss=1.0742828222698189e+99\n",
      "Gradient Descent(44/49): loss=2.6581447065141e+101\n",
      "Gradient Descent(45/49): loss=6.57716304710168e+103\n",
      "Gradient Descent(46/49): loss=1.6274160560991238e+106\n",
      "Gradient Descent(47/49): loss=4.0267863221306345e+108\n",
      "Gradient Descent(48/49): loss=9.963652517331748e+110\n",
      "Gradient Descent(49/49): loss=2.4653498731862867e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.0757699458128265\n",
      "Gradient Descent(2/49): loss=268.6875331234158\n",
      "Gradient Descent(3/49): loss=16958.52414902828\n",
      "Gradient Descent(4/49): loss=1316329.9405037563\n",
      "Gradient Descent(5/49): loss=154215208.4206435\n",
      "Gradient Descent(6/49): loss=27535640774.692207\n",
      "Gradient Descent(7/49): loss=6065375036619.1875\n",
      "Gradient Descent(8/49): loss=1427617743922106.8\n",
      "Gradient Descent(9/49): loss=3.4194029723658157e+17\n",
      "Gradient Descent(10/49): loss=8.225902159362569e+19\n",
      "Gradient Descent(11/49): loss=1.980997123784895e+22\n",
      "Gradient Descent(12/49): loss=4.771982906945057e+24\n",
      "Gradient Descent(13/49): loss=1.1495876064621749e+27\n",
      "Gradient Descent(14/49): loss=2.7694413792504238e+29\n",
      "Gradient Descent(15/49): loss=6.6718147054518385e+31\n",
      "Gradient Descent(16/49): loss=1.6072972131330768e+34\n",
      "Gradient Descent(17/49): loss=3.872117336419293e+36\n",
      "Gradient Descent(18/49): loss=9.328264498516848e+38\n",
      "Gradient Descent(19/49): loss=2.2472593732491978e+41\n",
      "Gradient Descent(20/49): loss=5.413841673334259e+43\n",
      "Gradient Descent(21/49): loss=1.3042411587110373e+46\n",
      "Gradient Descent(22/49): loss=3.142029455368227e+48\n",
      "Gradient Descent(23/49): loss=7.569419990291429e+50\n",
      "Gradient Descent(24/49): loss=1.8235385697053789e+53\n",
      "Gradient Descent(25/49): loss=4.393061713418597e+55\n",
      "Gradient Descent(26/49): loss=1.0583264614489102e+58\n",
      "Gradient Descent(27/49): loss=2.5495997371991168e+60\n",
      "Gradient Descent(28/49): loss=6.1422056961769e+62\n",
      "Gradient Descent(29/49): loss=1.4797103350658695e+65\n",
      "Gradient Descent(30/49): loss=3.564749837446198e+67\n",
      "Gradient Descent(31/49): loss=8.587789854834797e+69\n",
      "Gradient Descent(32/49): loss=2.068872654571429e+72\n",
      "Gradient Descent(33/49): loss=4.984092686459646e+74\n",
      "Gradient Descent(34/49): loss=1.2007109211062462e+77\n",
      "Gradient Descent(35/49): loss=2.892616182641521e+79\n",
      "Gradient Descent(36/49): loss=6.968561901952793e+81\n",
      "Gradient Descent(37/49): loss=1.6787866732116196e+84\n",
      "Gradient Descent(38/49): loss=4.044341908426107e+86\n",
      "Gradient Descent(39/49): loss=9.743168523586047e+88\n",
      "Gradient Descent(40/49): loss=2.3472133422058014e+91\n",
      "Gradient Descent(41/49): loss=5.6546394127248265e+93\n",
      "Gradient Descent(42/49): loss=1.3622514116203876e+96\n",
      "Gradient Descent(43/49): loss=3.281781158822835e+98\n",
      "Gradient Descent(44/49): loss=7.906093899065135e+100\n",
      "Gradient Descent(45/49): loss=1.904646218496013e+103\n",
      "Gradient Descent(46/49): loss=4.588457035730665e+105\n",
      "Gradient Descent(47/49): loss=1.1053988800802372e+108\n",
      "Gradient Descent(48/49): loss=2.6630012541636247e+110\n",
      "Gradient Descent(49/49): loss=6.415399732594532e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.176015391268917\n",
      "Gradient Descent(2/49): loss=273.1112820738485\n",
      "Gradient Descent(3/49): loss=17046.861555002\n",
      "Gradient Descent(4/49): loss=1289744.6745112594\n",
      "Gradient Descent(5/49): loss=145540987.8134172\n",
      "Gradient Descent(6/49): loss=25347504497.87213\n",
      "Gradient Descent(7/49): loss=5535293852027.46\n",
      "Gradient Descent(8/49): loss=1300099790873391.2\n",
      "Gradient Descent(9/49): loss=3.1129711161326054e+17\n",
      "Gradient Descent(10/49): loss=7.489613619996667e+19\n",
      "Gradient Descent(11/49): loss=1.8040821196828436e+22\n",
      "Gradient Descent(12/49): loss=4.34689024665386e+24\n",
      "Gradient Descent(13/49): loss=1.0474462352438166e+27\n",
      "Gradient Descent(14/49): loss=2.5240172041863922e+29\n",
      "Gradient Descent(15/49): loss=6.082116250342124e+31\n",
      "Gradient Descent(16/49): loss=1.4656071379393072e+34\n",
      "Gradient Descent(17/49): loss=3.531673412835427e+36\n",
      "Gradient Descent(18/49): loss=8.510273689757287e+38\n",
      "Gradient Descent(19/49): loss=2.05072074630355e+41\n",
      "Gradient Descent(20/49): loss=4.9416220302248245e+43\n",
      "Gradient Descent(21/49): loss=1.1907827214250438e+46\n",
      "Gradient Descent(22/49): loss=2.8694292709656225e+48\n",
      "Gradient Descent(23/49): loss=6.9144640692011055e+50\n",
      "Gradient Descent(24/49): loss=1.6661784923109881e+53\n",
      "Gradient Descent(25/49): loss=4.0149905190981234e+55\n",
      "Gradient Descent(26/49): loss=9.67492315070142e+57\n",
      "Gradient Descent(27/49): loss=2.331366351346291e+60\n",
      "Gradient Descent(28/49): loss=5.617893785332992e+62\n",
      "Gradient Descent(29/49): loss=1.3537439349700493e+65\n",
      "Gradient Descent(30/49): loss=3.2621169276156756e+67\n",
      "Gradient Descent(31/49): loss=7.860723564144596e+69\n",
      "Gradient Descent(32/49): loss=1.8941986545240756e+72\n",
      "Gradient Descent(33/49): loss=4.564450732203436e+74\n",
      "Gradient Descent(34/49): loss=1.0998957494216471e+77\n",
      "Gradient Descent(35/49): loss=2.6504189234874114e+79\n",
      "Gradient Descent(36/49): loss=6.386714807902592e+81\n",
      "Gradient Descent(37/49): loss=1.5390067462923652e+84\n",
      "Gradient Descent(38/49): loss=3.708544746984454e+86\n",
      "Gradient Descent(39/49): loss=8.936480735720516e+88\n",
      "Gradient Descent(40/49): loss=2.1534238734706562e+91\n",
      "Gradient Descent(41/49): loss=5.189105774376711e+93\n",
      "Gradient Descent(42/49): loss=1.2504188826639196e+96\n",
      "Gradient Descent(43/49): loss=3.0131345362877075e+98\n",
      "Gradient Descent(44/49): loss=7.260750664951319e+100\n",
      "Gradient Descent(45/49): loss=1.7496231775811574e+103\n",
      "Gradient Descent(46/49): loss=4.216067187523663e+105\n",
      "Gradient Descent(47/49): loss=1.0159457623491194e+108\n",
      "Gradient Descent(48/49): loss=2.4481246292504443e+110\n",
      "Gradient Descent(49/49): loss=5.89924622204681e+112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=4.987703748231366\n",
      "Gradient Descent(2/49): loss=259.2737700025552\n",
      "Gradient Descent(3/49): loss=15773.689398756056\n",
      "Gradient Descent(4/49): loss=1114315.500354501\n",
      "Gradient Descent(5/49): loss=112035464.6983176\n",
      "Gradient Descent(6/49): loss=17969792277.876602\n",
      "Gradient Descent(7/49): loss=3838472431824.5737\n",
      "Gradient Descent(8/49): loss=905535483832032.6\n",
      "Gradient Descent(9/49): loss=2.1938292877711184e+17\n",
      "Gradient Descent(10/49): loss=5.3500287737825305e+19\n",
      "Gradient Descent(11/49): loss=1.3067759306277487e+22\n",
      "Gradient Descent(12/49): loss=3.1931022679397146e+24\n",
      "Gradient Descent(13/49): loss=7.803054690257574e+26\n",
      "Gradient Descent(14/49): loss=1.9068921337350686e+29\n",
      "Gradient Descent(15/49): loss=4.66004302364439e+31\n",
      "Gradient Descent(16/49): loss=1.138817898587081e+34\n",
      "Gradient Descent(17/49): loss=2.7830356953200984e+36\n",
      "Gradient Descent(18/49): loss=6.801164846418441e+38\n",
      "Gradient Descent(19/49): loss=1.6620643484700245e+41\n",
      "Gradient Descent(20/49): loss=4.061742323185738e+43\n",
      "Gradient Descent(21/49): loss=9.926060162628064e+45\n",
      "Gradient Descent(22/49): loss=2.42572429605592e+48\n",
      "Gradient Descent(23/49): loss=5.9279696721799775e+50\n",
      "Gradient Descent(24/49): loss=1.4486734742249877e+53\n",
      "Gradient Descent(25/49): loss=3.540259061680732e+55\n",
      "Gradient Descent(26/49): loss=8.651662674039361e+57\n",
      "Gradient Descent(27/49): loss=2.114287845077765e+60\n",
      "Gradient Descent(28/49): loss=5.166883245757458e+62\n",
      "Gradient Descent(29/49): loss=1.2626796553478295e+65\n",
      "Gradient Descent(30/49): loss=3.085728545034316e+67\n",
      "Gradient Descent(31/49): loss=7.540883876058374e+69\n",
      "Gradient Descent(32/49): loss=1.8428364258971e+72\n",
      "Gradient Descent(33/49): loss=4.503511986698796e+74\n",
      "Gradient Descent(34/49): loss=1.1005654071802324e+77\n",
      "Gradient Descent(35/49): loss=2.689554772051691e+79\n",
      "Gradient Descent(36/49): loss=6.572716918660391e+81\n",
      "Gradient Descent(37/49): loss=1.6062363980001846e+84\n",
      "Gradient Descent(38/49): loss=3.92531033694119e+86\n",
      "Gradient Descent(39/49): loss=9.59264854194632e+88\n",
      "Gradient Descent(40/49): loss=2.3442453755391938e+91\n",
      "Gradient Descent(41/49): loss=5.7288520023498875e+93\n",
      "Gradient Descent(42/49): loss=1.4000132241822318e+96\n",
      "Gradient Descent(43/49): loss=3.421343450801505e+98\n",
      "Gradient Descent(44/49): loss=8.361057457281908e+100\n",
      "Gradient Descent(45/49): loss=2.043269926250539e+103\n",
      "Gradient Descent(46/49): loss=4.993330105492548e+105\n",
      "Gradient Descent(47/49): loss=1.2202668488432196e+108\n",
      "Gradient Descent(48/49): loss=2.9820803971038683e+110\n",
      "Gradient Descent(49/49): loss=7.2875891885623e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.073192961500127\n",
      "Gradient Descent(2/49): loss=267.965359692426\n",
      "Gradient Descent(3/49): loss=17075.81606978889\n",
      "Gradient Descent(4/49): loss=1385833.293581734\n",
      "Gradient Descent(5/49): loss=177103401.33000037\n",
      "Gradient Descent(6/49): loss=34244222649.639416\n",
      "Gradient Descent(7/49): loss=7955273671014.892\n",
      "Gradient Descent(8/49): loss=1949534084420280.5\n",
      "Gradient Descent(9/49): loss=4.8418086526572666e+17\n",
      "Gradient Descent(10/49): loss=1.2063555800658318e+20\n",
      "Gradient Descent(11/49): loss=3.0079676796985472e+22\n",
      "Gradient Descent(12/49): loss=7.501517901120232e+24\n",
      "Gradient Descent(13/49): loss=1.8708700726938264e+27\n",
      "Gradient Descent(14/49): loss=4.6659757902085776e+29\n",
      "Gradient Descent(15/49): loss=1.1637035774751394e+32\n",
      "Gradient Descent(16/49): loss=2.9023013421441553e+34\n",
      "Gradient Descent(17/49): loss=7.238401915524393e+36\n",
      "Gradient Descent(18/49): loss=1.8052730494734873e+39\n",
      "Gradient Descent(19/49): loss=4.502389955024133e+41\n",
      "Gradient Descent(20/49): loss=1.1229057758781341e+44\n",
      "Gradient Descent(21/49): loss=2.80055125083157e+46\n",
      "Gradient Descent(22/49): loss=6.984635289791891e+48\n",
      "Gradient Descent(23/49): loss=1.741983122717789e+51\n",
      "Gradient Descent(24/49): loss=4.344543521528439e+53\n",
      "Gradient Descent(25/49): loss=1.0835385351513029e+56\n",
      "Gradient Descent(26/49): loss=2.7023685027907163e+58\n",
      "Gradient Descent(27/49): loss=6.739765396397407e+60\n",
      "Gradient Descent(28/49): loss=1.680912042586645e+63\n",
      "Gradient Descent(29/49): loss=4.192230929021812e+65\n",
      "Gradient Descent(30/49): loss=1.0455514457021954e+68\n",
      "Gradient Descent(31/49): loss=2.60762788147506e+70\n",
      "Gradient Descent(32/49): loss=6.503480241165387e+72\n",
      "Gradient Descent(33/49): loss=1.6219820146770138e+75\n",
      "Gradient Descent(34/49): loss=4.0452581669784193e+77\n",
      "Gradient Descent(35/49): loss=1.0088961215000841e+80\n",
      "Gradient Descent(36/49): loss=2.516208711441062e+82\n",
      "Gradient Descent(37/49): loss=6.275478857147653e+84\n",
      "Gradient Descent(38/49): loss=1.5651179771948477e+87\n",
      "Gradient Descent(39/49): loss=3.903438029670722e+89\n",
      "Gradient Descent(40/49): loss=9.735258730328062e+91\n",
      "Gradient Descent(41/49): loss=2.427994548037585e+94\n",
      "Gradient Descent(42/49): loss=6.055470828869706e+96\n",
      "Gradient Descent(43/49): loss=1.5102474998936577e+99\n",
      "Gradient Descent(44/49): loss=3.766589874499973e+101\n",
      "Gradient Descent(45/49): loss=9.393956476461637e+103\n",
      "Gradient Descent(46/49): loss=2.342873028972125e+106\n",
      "Gradient Descent(47/49): loss=5.843175922348535e+108\n",
      "Gradient Descent(48/49): loss=1.457300691813064e+111\n",
      "Gradient Descent(49/49): loss=3.634539391901916e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.1159528717025\n",
      "Gradient Descent(2/49): loss=273.24480800608853\n",
      "Gradient Descent(3/49): loss=17389.716909256855\n",
      "Gradient Descent(4/49): loss=1360551.8090560145\n",
      "Gradient Descent(5/49): loss=160585375.51499152\n",
      "Gradient Descent(6/49): loss=28886676740.641445\n",
      "Gradient Descent(7/49): loss=6412151545715.464\n",
      "Gradient Descent(8/49): loss=1521138577691174.8\n",
      "Gradient Descent(9/49): loss=3.67232177756325e+17\n",
      "Gradient Descent(10/49): loss=8.904598503064927e+19\n",
      "Gradient Descent(11/49): loss=2.1615083149743314e+22\n",
      "Gradient Descent(12/49): loss=5.248252627609408e+24\n",
      "Gradient Descent(13/49): loss=1.2743855987705385e+27\n",
      "Gradient Descent(14/49): loss=3.0945245088564713e+29\n",
      "Gradient Descent(15/49): loss=7.5143030167291435e+31\n",
      "Gradient Descent(16/49): loss=1.8246681877309424e+34\n",
      "Gradient Descent(17/49): loss=4.430769927777049e+36\n",
      "Gradient Descent(18/49): loss=1.075906481287674e+39\n",
      "Gradient Descent(19/49): loss=2.6125815419459076e+41\n",
      "Gradient Descent(20/49): loss=6.344029389109874e+43\n",
      "Gradient Descent(21/49): loss=1.5404957999710482e+46\n",
      "Gradient Descent(22/49): loss=3.740725593584208e+48\n",
      "Gradient Descent(23/49): loss=9.08345739565417e+50\n",
      "Gradient Descent(24/49): loss=2.205700370035902e+53\n",
      "Gradient Descent(25/49): loss=5.35601578834881e+55\n",
      "Gradient Descent(26/49): loss=1.300580328804903e+58\n",
      "Gradient Descent(27/49): loss=3.1581482551903354e+60\n",
      "Gradient Descent(28/49): loss=7.6688076705937305e+62\n",
      "Gradient Descent(29/49): loss=1.862186519961592e+65\n",
      "Gradient Descent(30/49): loss=4.521874565225963e+67\n",
      "Gradient Descent(31/49): loss=1.0980290837922371e+70\n",
      "Gradient Descent(32/49): loss=2.6663010029633174e+72\n",
      "Gradient Descent(33/49): loss=6.474474258778677e+74\n",
      "Gradient Descent(34/49): loss=1.5721712170156364e+77\n",
      "Gradient Descent(35/49): loss=3.817641767995279e+79\n",
      "Gradient Descent(36/49): loss=9.270229928524016e+81\n",
      "Gradient Descent(37/49): loss=2.251053612419738e+84\n",
      "Gradient Descent(38/49): loss=5.466145289877054e+86\n",
      "Gradient Descent(39/49): loss=1.327322644169624e+89\n",
      "Gradient Descent(40/49): loss=3.2230855718162342e+91\n",
      "Gradient Descent(41/49): loss=7.826492412286808e+93\n",
      "Gradient Descent(42/49): loss=1.9004764879719531e+96\n",
      "Gradient Descent(43/49): loss=4.6148525943294174e+98\n",
      "Gradient Descent(44/49): loss=1.120606574307905e+101\n",
      "Gradient Descent(45/49): loss=2.7211250385875863e+103\n",
      "Gradient Descent(46/49): loss=6.607601316458006e+105\n",
      "Gradient Descent(47/49): loss=1.6044979388348906e+108\n",
      "Gradient Descent(48/49): loss=3.8961394800154563e+110\n",
      "Gradient Descent(49/49): loss=9.460842847051677e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.217064393189405\n",
      "Gradient Descent(2/49): loss=277.74529971720517\n",
      "Gradient Descent(3/49): loss=17480.628590450877\n",
      "Gradient Descent(4/49): loss=1333139.2990901368\n",
      "Gradient Descent(5/49): loss=151562131.95205775\n",
      "Gradient Descent(6/49): loss=26592028332.71595\n",
      "Gradient Descent(7/49): loss=5851824258913.711\n",
      "Gradient Descent(8/49): loss=1385270679335769.2\n",
      "Gradient Descent(9/49): loss=3.3432253522946394e+17\n",
      "Gradient Descent(10/49): loss=8.107557640469804e+19\n",
      "Gradient Descent(11/49): loss=1.9684713605714147e+22\n",
      "Gradient Descent(12/49): loss=4.7807299879650994e+24\n",
      "Gradient Descent(13/49): loss=1.1611549536954583e+27\n",
      "Gradient Descent(14/49): loss=2.8202894974360482e+29\n",
      "Gradient Descent(15/49): loss=6.850133688635932e+31\n",
      "Gradient Descent(16/49): loss=1.663814314118933e+34\n",
      "Gradient Descent(17/49): loss=4.041204014523022e+36\n",
      "Gradient Descent(18/49): loss=9.815597068141996e+38\n",
      "Gradient Descent(19/49): loss=2.3840901603849533e+41\n",
      "Gradient Descent(20/49): loss=5.790667520845583e+43\n",
      "Gradient Descent(21/49): loss=1.4064833169783727e+46\n",
      "Gradient Descent(22/49): loss=3.41617838402619e+48\n",
      "Gradient Descent(23/49): loss=8.297485374513083e+50\n",
      "Gradient Descent(24/49): loss=2.015359147025363e+53\n",
      "Gradient Descent(25/49): loss=4.8950643576764915e+55\n",
      "Gradient Descent(26/49): loss=1.188952108172922e+58\n",
      "Gradient Descent(27/49): loss=2.887821307828679e+60\n",
      "Gradient Descent(28/49): loss=7.014169745462153e+62\n",
      "Gradient Descent(29/49): loss=1.7036572548579668e+65\n",
      "Gradient Descent(30/49): loss=4.1379780463794413e+67\n",
      "Gradient Descent(31/49): loss=1.0050649720472091e+70\n",
      "Gradient Descent(32/49): loss=2.4411816271478756e+72\n",
      "Gradient Descent(33/49): loss=5.929335816554931e+74\n",
      "Gradient Descent(34/49): loss=1.4401641743697776e+77\n",
      "Gradient Descent(35/49): loss=3.497985125664993e+79\n",
      "Gradient Descent(36/49): loss=8.496184085906682e+81\n",
      "Gradient Descent(37/49): loss=2.0636206681379395e+84\n",
      "Gradient Descent(38/49): loss=5.012285773127284e+86\n",
      "Gradient Descent(39/49): loss=1.2174237765394864e+89\n",
      "Gradient Descent(40/49): loss=2.956975557199661e+91\n",
      "Gradient Descent(41/49): loss=7.18213707861865e+93\n",
      "Gradient Descent(42/49): loss=1.7444544947445598e+96\n",
      "Gradient Descent(43/49): loss=4.237069622764525e+98\n",
      "Gradient Descent(44/49): loss=1.02913312111261e+101\n",
      "Gradient Descent(45/49): loss=2.499640259108925e+103\n",
      "Gradient Descent(46/49): loss=6.071324784691816e+105\n",
      "Gradient Descent(47/49): loss=1.4746515826382616e+108\n",
      "Gradient Descent(48/49): loss=3.5817508818842226e+110\n",
      "Gradient Descent(49/49): loss=8.699641007353107e+112\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.0271515872332735\n",
      "Gradient Descent(2/49): loss=263.6758845022109\n",
      "Gradient Descent(3/49): loss=16175.784385733596\n",
      "Gradient Descent(4/49): loss=1151969.337326769\n",
      "Gradient Descent(5/49): loss=116694130.19263399\n",
      "Gradient Descent(6/49): loss=18854097318.147305\n",
      "Gradient Descent(7/49): loss=4058051481172.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8/49): loss=964843824929899.6\n",
      "Gradient Descent(9/49): loss=2.3560346082767942e+17\n",
      "Gradient Descent(10/49): loss=5.79125307913672e+19\n",
      "Gradient Descent(11/49): loss=1.4257979985994254e+22\n",
      "Gradient Descent(12/49): loss=3.511647561685529e+24\n",
      "Gradient Descent(13/49): loss=8.64976226138356e+26\n",
      "Gradient Descent(14/49): loss=2.1306255357780727e+29\n",
      "Gradient Descent(15/49): loss=5.248224707252262e+31\n",
      "Gradient Descent(16/49): loss=1.2927611066625667e+34\n",
      "Gradient Descent(17/49): loss=3.1843754663785124e+36\n",
      "Gradient Descent(18/49): loss=7.843868306120036e+38\n",
      "Gradient Descent(19/49): loss=1.932129918878197e+41\n",
      "Gradient Descent(20/49): loss=4.759292091564466e+43\n",
      "Gradient Descent(21/49): loss=1.1723259919131552e+46\n",
      "Gradient Descent(22/49): loss=2.8877156627924123e+48\n",
      "Gradient Descent(23/49): loss=7.113125365440372e+50\n",
      "Gradient Descent(24/49): loss=1.7521306933759144e+53\n",
      "Gradient Descent(25/49): loss=4.315911514221837e+55\n",
      "Gradient Descent(26/49): loss=1.0631108894461958e+58\n",
      "Gradient Descent(27/49): loss=2.6186930838017937e+60\n",
      "Gradient Descent(28/49): loss=6.450459246752638e+62\n",
      "Gradient Descent(29/49): loss=1.5889003851344743e+65\n",
      "Gradient Descent(30/49): loss=3.913836732092412e+67\n",
      "Gradient Descent(31/49): loss=9.64070379036338e+69\n",
      "Gradient Descent(32/49): loss=2.3747329266807993e+72\n",
      "Gradient Descent(33/49): loss=5.849527789349638e+74\n",
      "Gradient Descent(34/49): loss=1.4408767812976664e+77\n",
      "Gradient Descent(35/49): loss=3.54921965267476e+79\n",
      "Gradient Descent(36/49): loss=8.742565850487199e+81\n",
      "Gradient Descent(37/49): loss=2.1535003502109192e+84\n",
      "Gradient Descent(38/49): loss=5.304579728272867e+86\n",
      "Gradient Descent(39/49): loss=1.306643209547048e+89\n",
      "Gradient Descent(40/49): loss=3.218570677627157e+91\n",
      "Gradient Descent(41/49): loss=7.928099370349404e+93\n",
      "Gradient Descent(42/49): loss=1.9528780294635155e+96\n",
      "Gradient Descent(43/49): loss=4.8103995923971063e+98\n",
      "Gradient Descent(44/49): loss=1.1849149762257006e+101\n",
      "Gradient Descent(45/49): loss=2.9187253032014714e+103\n",
      "Gradient Descent(46/49): loss=7.189509430190384e+105\n",
      "Gradient Descent(47/49): loss=1.7709458916911142e+108\n",
      "Gradient Descent(48/49): loss=4.3622577892837305e+110\n",
      "Gradient Descent(49/49): loss=1.0745270710668179e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.113199307826656\n",
      "Gradient Descent(2/49): loss=272.4935887400499\n",
      "Gradient Descent(3/49): loss=17508.007575291722\n",
      "Gradient Descent(4/49): loss=1432084.6367876986\n",
      "Gradient Descent(5/49): loss=184364446.9708734\n",
      "Gradient Descent(6/49): loss=35913953266.84049\n",
      "Gradient Descent(7/49): loss=8407677401788.324\n",
      "Gradient Descent(8/49): loss=2076603027867754.0\n",
      "Gradient Descent(9/49): loss=5.1981641606251354e+17\n",
      "Gradient Descent(10/49): loss=1.3053966426398343e+20\n",
      "Gradient Descent(11/49): loss=3.2806996784809192e+22\n",
      "Gradient Descent(12/49): loss=8.246487040919822e+24\n",
      "Gradient Descent(13/49): loss=2.0729559538283976e+27\n",
      "Gradient Descent(14/49): loss=5.21093394306324e+29\n",
      "Gradient Descent(15/49): loss=1.3099119461144347e+32\n",
      "Gradient Descent(16/49): loss=3.2928267740311703e+34\n",
      "Gradient Descent(17/49): loss=8.277433954783109e+36\n",
      "Gradient Descent(18/49): loss=2.0807628143403878e+39\n",
      "Gradient Descent(19/49): loss=5.2305750136846854e+41\n",
      "Gradient Descent(20/49): loss=1.3148502478981502e+44\n",
      "Gradient Descent(21/49): loss=3.305241145770288e+46\n",
      "Gradient Descent(22/49): loss=8.30864126941228e+48\n",
      "Gradient Descent(23/49): loss=2.08860765979975e+51\n",
      "Gradient Descent(24/49): loss=5.250295222948287e+53\n",
      "Gradient Descent(25/49): loss=1.3198074707239432e+56\n",
      "Gradient Descent(26/49): loss=3.3177025020721634e+58\n",
      "Gradient Descent(27/49): loss=8.33996634843949e+60\n",
      "Gradient Descent(28/49): loss=2.096482088121549e+63\n",
      "Gradient Descent(29/49): loss=5.270089784758973e+65\n",
      "Gradient Descent(30/49): loss=1.324783383401431e+68\n",
      "Gradient Descent(31/49): loss=3.3302108400735525e+70\n",
      "Gradient Descent(32/49): loss=8.371409528755432e+72\n",
      "Gradient Descent(33/49): loss=2.1043862044659344e+75\n",
      "Gradient Descent(34/49): loss=5.28995897564796e+77\n",
      "Gradient Descent(35/49): loss=1.329778056169126e+80\n",
      "Gradient Descent(36/49): loss=3.342766336769856e+82\n",
      "Gradient Descent(37/49): loss=8.402971255543608e+84\n",
      "Gradient Descent(38/49): loss=2.1123201207573415e+87\n",
      "Gradient Descent(39/49): loss=5.309903076977323e+89\n",
      "Gradient Descent(40/49): loss=1.334791559755818e+92\n",
      "Gradient Descent(41/49): loss=3.35536916995778e+94\n",
      "Gradient Descent(42/49): loss=8.434651975745657e+96\n",
      "Gradient Descent(43/49): loss=2.1202839493469222e+99\n",
      "Gradient Descent(44/49): loss=5.329922371172672e+101\n",
      "Gradient Descent(45/49): loss=1.3398239651569595e+104\n",
      "Gradient Descent(46/49): loss=3.368019518104113e+106\n",
      "Gradient Descent(47/49): loss=8.466452137987844e+108\n",
      "Gradient Descent(48/49): loss=2.1282778030095533e+111\n",
      "Gradient Descent(49/49): loss=5.3500171417251155e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.156307612889088\n",
      "Gradient Descent(2/49): loss=277.8599102208809\n",
      "Gradient Descent(3/49): loss=17830.01124738522\n",
      "Gradient Descent(4/49): loss=1406069.2072192987\n",
      "Gradient Descent(5/49): loss=167191591.43788373\n",
      "Gradient Descent(6/49): loss=30298254602.553833\n",
      "Gradient Descent(7/49): loss=6777265840736.764\n",
      "Gradient Descent(8/49): loss=1620379793576482.8\n",
      "Gradient Descent(9/49): loss=3.942836918759622e+17\n",
      "Gradient Descent(10/49): loss=9.636275436738881e+19\n",
      "Gradient Descent(11/49): loss=2.357656033411899e+22\n",
      "Gradient Descent(12/49): loss=5.769889019700518e+24\n",
      "Gradient Descent(13/49): loss=1.4121567912004414e+27\n",
      "Gradient Descent(14/49): loss=3.456251719581758e+29\n",
      "Gradient Descent(15/49): loss=8.459204548364247e+31\n",
      "Gradient Descent(16/49): loss=2.0703992893323324e+34\n",
      "Gradient Descent(17/49): loss=5.0673254377102003e+36\n",
      "Gradient Descent(18/49): loss=1.2402336461066996e+39\n",
      "Gradient Descent(19/49): loss=3.035485954005865e+41\n",
      "Gradient Descent(20/49): loss=7.429386421209111e+43\n",
      "Gradient Descent(21/49): loss=1.8183507840214845e+46\n",
      "Gradient Descent(22/49): loss=4.4504342444984975e+48\n",
      "Gradient Descent(23/49): loss=1.0892488478948586e+51\n",
      "Gradient Descent(24/49): loss=2.66594895570513e+53\n",
      "Gradient Descent(25/49): loss=6.524940419430993e+55\n",
      "Gradient Descent(26/49): loss=1.5969865959379112e+58\n",
      "Gradient Descent(27/49): loss=3.90864287436393e+60\n",
      "Gradient Descent(28/49): loss=9.566447932735905e+62\n",
      "Gradient Descent(29/49): loss=2.3413990224071756e+65\n",
      "Gradient Descent(30/49): loss=5.730600762870062e+67\n",
      "Gradient Descent(31/49): loss=1.4025710606834295e+70\n",
      "Gradient Descent(32/49): loss=3.432808638515864e+72\n",
      "Gradient Descent(33/49): loss=8.401838223388949e+74\n",
      "Gradient Descent(34/49): loss=2.0563594702009901e+77\n",
      "Gradient Descent(35/49): loss=5.0329632138284065e+79\n",
      "Gradient Descent(36/49): loss=1.2318234763338247e+82\n",
      "Gradient Descent(37/49): loss=3.0149019819537674e+84\n",
      "Gradient Descent(38/49): loss=7.379006923817804e+86\n",
      "Gradient Descent(39/49): loss=1.8060203451942875e+89\n",
      "Gradient Descent(40/49): loss=4.420255355402497e+91\n",
      "Gradient Descent(41/49): loss=1.0818625304501957e+94\n",
      "Gradient Descent(42/49): loss=2.6478708596813723e+96\n",
      "Gradient Descent(43/49): loss=6.480694073610467e+98\n",
      "Gradient Descent(44/49): loss=1.5861572524267616e+101\n",
      "Gradient Descent(45/49): loss=3.882137932834636e+103\n",
      "Gradient Descent(46/49): loss=9.501576786598694e+105\n",
      "Gradient Descent(47/49): loss=2.325521735537923e+108\n",
      "Gradient Descent(48/49): loss=5.69174092250367e+110\n",
      "Gradient Descent(49/49): loss=1.3930600705139605e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.258288868591652\n",
      "Gradient Descent(2/49): loss=282.43813790318416\n",
      "Gradient Descent(3/49): loss=17923.558204879428\n",
      "Gradient Descent(4/49): loss=1377807.1066192116\n",
      "Gradient Descent(5/49): loss=157806741.41173795\n",
      "Gradient Descent(6/49): loss=27892360887.456226\n",
      "Gradient Descent(7/49): loss=6185096394728.1455\n",
      "Gradient Descent(8/49): loss=1475651441721116.8\n",
      "Gradient Descent(9/49): loss=3.589499155930776e+17\n",
      "Gradient Descent(10/49): loss=8.773739545682402e+19\n",
      "Gradient Descent(11/49): loss=2.1471004836657696e+22\n",
      "Gradient Descent(12/49): loss=5.255894443660692e+24\n",
      "Gradient Descent(13/49): loss=1.2866840075036191e+27\n",
      "Gradient Descent(14/49): loss=3.149957981254128e+29\n",
      "Gradient Descent(15/49): loss=7.711510879171381e+31\n",
      "Gradient Descent(16/49): loss=1.8878811288099948e+34\n",
      "Gradient Descent(17/49): loss=4.621786995549436e+36\n",
      "Gradient Descent(18/49): loss=1.1314757081833629e+39\n",
      "Gradient Descent(19/49): loss=2.7700049766321665e+41\n",
      "Gradient Descent(20/49): loss=6.781345409273975e+43\n",
      "Gradient Descent(21/49): loss=1.6601647285876526e+46\n",
      "Gradient Descent(22/49): loss=4.064306948437653e+48\n",
      "Gradient Descent(23/49): loss=9.949971040573134e+50\n",
      "Gradient Descent(24/49): loss=2.435886978163371e+53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=5.963379537699195e+55\n",
      "Gradient Descent(26/49): loss=1.4599156623223922e+58\n",
      "Gradient Descent(27/49): loss=3.574070252649042e+60\n",
      "Gradient Descent(28/49): loss=8.749805554213246e+62\n",
      "Gradient Descent(29/49): loss=2.142070295898586e+65\n",
      "Gradient Descent(30/49): loss=5.244076710209242e+67\n",
      "Gradient Descent(31/49): loss=1.2838206381561907e+70\n",
      "Gradient Descent(32/49): loss=3.142965906175696e+72\n",
      "Gradient Descent(33/49): loss=7.694404026383198e+74\n",
      "Gradient Descent(34/49): loss=1.8836937812430518e+77\n",
      "Gradient Descent(35/49): loss=4.6115361882832464e+79\n",
      "Gradient Descent(36/49): loss=1.1289661954403349e+82\n",
      "Gradient Descent(37/49): loss=2.7638613650812247e+84\n",
      "Gradient Descent(38/49): loss=6.766305028654232e+86\n",
      "Gradient Descent(39/49): loss=1.6564826412502612e+89\n",
      "Gradient Descent(40/49): loss=4.055292702802006e+91\n",
      "Gradient Descent(41/49): loss=9.927902952841568e+93\n",
      "Gradient Descent(42/49): loss=2.4304844129485045e+96\n",
      "Gradient Descent(43/49): loss=5.950153330109718e+98\n",
      "Gradient Descent(44/49): loss=1.4566777084929013e+101\n",
      "Gradient Descent(45/49): loss=3.56614330538776e+103\n",
      "Gradient Descent(46/49): loss=8.730399319228694e+105\n",
      "Gradient Descent(47/49): loss=2.1373193880917052e+108\n",
      "Gradient Descent(48/49): loss=5.232445847753431e+110\n",
      "Gradient Descent(49/49): loss=1.2809732463109823e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.066768219363525\n",
      "Gradient Descent(2/49): loss=268.1339163976551\n",
      "Gradient Descent(3/49): loss=16586.387584260836\n",
      "Gradient Descent(4/49): loss=1190732.5421860495\n",
      "Gradient Descent(5/49): loss=121526585.62692814\n",
      "Gradient Descent(6/49): loss=19778152758.381313\n",
      "Gradient Descent(7/49): loss=4289249124160.8965\n",
      "Gradient Descent(8/49): loss=1027779215868835.6\n",
      "Gradient Descent(9/49): loss=2.5295207308706778e+17\n",
      "Gradient Descent(10/49): loss=6.2669054959328035e+19\n",
      "Gradient Descent(11/49): loss=1.5551256034272772e+22\n",
      "Gradient Descent(12/49): loss=3.8605220046235835e+24\n",
      "Gradient Descent(13/49): loss=9.584448430363648e+26\n",
      "Gradient Descent(14/49): loss=2.3795672334541863e+29\n",
      "Gradient Descent(15/49): loss=5.907873482709291e+31\n",
      "Gradient Descent(16/49): loss=1.4667799297071023e+34\n",
      "Gradient Descent(17/49): loss=3.641655570376137e+36\n",
      "Gradient Descent(18/49): loss=9.041340168019666e+38\n",
      "Gradient Descent(19/49): loss=2.2447436871176733e+41\n",
      "Gradient Descent(20/49): loss=5.573149720412344e+43\n",
      "Gradient Descent(21/49): loss=1.3836768098375846e+46\n",
      "Gradient Descent(22/49): loss=3.435331205183846e+48\n",
      "Gradient Descent(23/49): loss=8.529087432937577e+50\n",
      "Gradient Descent(24/49): loss=2.1175638706973582e+53\n",
      "Gradient Descent(25/49): loss=5.257393339857293e+55\n",
      "Gradient Descent(26/49): loss=1.305282221352653e+58\n",
      "Gradient Descent(27/49): loss=3.2406966099781184e+60\n",
      "Gradient Descent(28/49): loss=8.045857321982749e+62\n",
      "Gradient Descent(29/49): loss=1.9975896492865665e+65\n",
      "Gradient Descent(30/49): loss=4.9595266821777866e+67\n",
      "Gradient Descent(31/49): loss=1.2313292131853895e+70\n",
      "Gradient Descent(32/49): loss=3.0570893724439016e+72\n",
      "Gradient Descent(33/49): loss=7.590005443737016e+74\n",
      "Gradient Descent(34/49): loss=1.8844127736410552e+77\n",
      "Gradient Descent(35/49): loss=4.6785361720547124e+79\n",
      "Gradient Descent(36/49): loss=1.1615661398288661e+82\n",
      "Gradient Descent(37/49): loss=2.88388471859219e+84\n",
      "Gradient Descent(38/49): loss=7.15998063730999e+86\n",
      "Gradient Descent(39/49): loss=1.7776481284480418e+89\n",
      "Gradient Descent(40/49): loss=4.413465662334738e+91\n",
      "Gradient Descent(41/49): loss=1.0957556133233752e+94\n",
      "Gradient Descent(42/49): loss=2.7204932721613457e+96\n",
      "Gradient Descent(43/49): loss=6.754319625548698e+98\n",
      "Gradient Descent(44/49): loss=1.676932417768027e+101\n",
      "Gradient Descent(45/49): loss=4.163413178026624e+103\n",
      "Gradient Descent(46/49): loss=1.0336736953321643e+106\n",
      "Gradient Descent(47/49): loss=2.56635904901496e+108\n",
      "Gradient Descent(48/49): loss=6.371642035782237e+110\n",
      "Gradient Descent(49/49): loss=1.5819229288174027e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.153376066219834\n",
      "Gradient Descent(2/49): loss=277.0790663531434\n",
      "Gradient Descent(3/49): loss=17949.280773246704\n",
      "Gradient Descent(4/49): loss=1479682.227608298\n",
      "Gradient Descent(5/49): loss=191892546.47430024\n",
      "Gradient Descent(6/49): loss=37658038469.15734\n",
      "Gradient Descent(7/49): loss=8883876961463.812\n",
      "Gradient Descent(8/49): loss=2211405428848750.0\n",
      "Gradient Descent(9/49): loss=5.5791901355290214e+17\n",
      "Gradient Descent(10/49): loss=1.4121310110250928e+20\n",
      "Gradient Descent(11/49): loss=3.5769401048652086e+22\n",
      "Gradient Descent(12/49): loss=9.062066152813299e+24\n",
      "Gradient Descent(13/49): loss=2.2959449567859858e+27\n",
      "Gradient Descent(14/49): loss=5.81701442261767e+29\n",
      "Gradient Descent(15/49): loss=1.473804427856775e+32\n",
      "Gradient Descent(16/49): loss=3.734047356302841e+34\n",
      "Gradient Descent(17/49): loss=9.460625359895605e+36\n",
      "Gradient Descent(18/49): loss=2.3969550069341743e+39\n",
      "Gradient Descent(19/49): loss=6.0729530346039315e+41\n",
      "Gradient Descent(20/49): loss=1.5386504344088662e+44\n",
      "Gradient Descent(21/49): loss=3.898342613262807e+46\n",
      "Gradient Descent(22/49): loss=9.876886128297674e+48\n",
      "Gradient Descent(23/49): loss=2.5024193425636378e+51\n",
      "Gradient Descent(24/49): loss=6.3401587147298545e+53\n",
      "Gradient Descent(25/49): loss=1.6063499767723463e+56\n",
      "Gradient Descent(26/49): loss=4.069866960715779e+58\n",
      "Gradient Descent(27/49): loss=1.031146220776194e+61\n",
      "Gradient Descent(28/49): loss=2.6125240428843423e+63\n",
      "Gradient Descent(29/49): loss=6.6191212624635815e+65\n",
      "Gradient Descent(30/49): loss=1.6770282519132518e+68\n",
      "Gradient Descent(31/49): loss=4.248938259621098e+70\n",
      "Gradient Descent(32/49): loss=1.0765159330783598e+73\n",
      "Gradient Descent(33/49): loss=2.727473272993458e+75\n",
      "Gradient Descent(34/49): loss=6.910357967133052e+77\n",
      "Gradient Descent(35/49): loss=1.7508163217125883e+80\n",
      "Gradient Descent(36/49): loss=4.435888570396231e+82\n",
      "Gradient Descent(37/49): loss=1.1238818809802111e+85\n",
      "Gradient Descent(38/49): loss=2.8474801888064533e+87\n",
      "Gradient Descent(39/49): loss=7.214408883052488e+89\n",
      "Gradient Descent(40/49): loss=1.8278510149593556e+92\n",
      "Gradient Descent(41/49): loss=4.631064564051064e+94\n",
      "Gradient Descent(42/49): loss=1.1733318974515197e+97\n",
      "Gradient Descent(43/49): loss=2.9727673249558435e+99\n",
      "Gradient Descent(44/49): loss=7.531837826551852e+101\n",
      "Gradient Descent(45/49): loss=1.908275180813885e+104\n",
      "Gradient Descent(46/49): loss=4.834828164877417e+106\n",
      "Gradient Descent(47/49): loss=1.2249576800512843e+109\n",
      "Gradient Descent(48/49): loss=3.1035670074423007e+111\n",
      "Gradient Descent(49/49): loss=7.863233421485195e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.196834169372592\n",
      "Gradient Descent(2/49): loss=282.5333270831032\n",
      "Gradient Descent(3/49): loss=18279.56026802917\n",
      "Gradient Descent(4/49): loss=1452914.5567565607\n",
      "Gradient Descent(5/49): loss=174041606.05615005\n",
      "Gradient Descent(6/49): loss=31772831735.01226\n",
      "Gradient Descent(7/49): loss=7161609544843.084\n",
      "Gradient Descent(8/49): loss=1725666654979690.8\n",
      "Gradient Descent(9/49): loss=4.2320959564922797e+17\n",
      "Gradient Descent(10/49): loss=1.0424835163729112e+20\n",
      "Gradient Descent(11/49): loss=2.5707250144673664e+22\n",
      "Gradient Descent(12/49): loss=6.34100866004319e+24\n",
      "Gradient Descent(13/49): loss=1.5641905387382727e+27\n",
      "Gradient Descent(14/49): loss=3.858584255331209e+29\n",
      "Gradient Descent(15/49): loss=9.518489766692388e+31\n",
      "Gradient Descent(16/49): loss=2.3480564357100294e+34\n",
      "Gradient Descent(17/49): loss=5.792274282732709e+36\n",
      "Gradient Descent(18/49): loss=1.4288601770810424e+39\n",
      "Gradient Descent(19/49): loss=3.524766500310671e+41\n",
      "Gradient Descent(20/49): loss=8.695027774754909e+43\n",
      "Gradient Descent(21/49): loss=2.1449224526862596e+46\n",
      "Gradient Descent(22/49): loss=5.291176116060966e+48\n",
      "Gradient Descent(23/49): loss=1.3052474068510872e+51\n",
      "Gradient Descent(24/49): loss=3.2198338436368876e+53\n",
      "Gradient Descent(25/49): loss=7.942808333688624e+55\n",
      "Gradient Descent(26/49): loss=1.9593621065397396e+58\n",
      "Gradient Descent(27/49): loss=4.833428811647714e+60\n",
      "Gradient Descent(28/49): loss=1.192328564449241e+63\n",
      "Gradient Descent(29/49): loss=2.941281357399203e+65\n",
      "Gradient Descent(30/49): loss=7.25566448823634e+67\n",
      "Gradient Descent(31/49): loss=1.7898548547020306e+70\n",
      "Gradient Descent(32/49): loss=4.41528188919746e+72\n",
      "Gradient Descent(33/49): loss=1.0891784945500774e+75\n",
      "Gradient Descent(34/49): loss=2.686826849929646e+77\n",
      "Gradient Descent(35/49): loss=6.627966451435428e+79\n",
      "Gradient Descent(36/49): loss=1.6350119205673788e+82\n",
      "Gradient Descent(37/49): loss=4.033309462238462e+84\n",
      "Gradient Descent(38/49): loss=9.94952086498404e+86\n",
      "Gradient Descent(39/49): loss=2.4543855702014113e+89\n",
      "Gradient Descent(40/49): loss=6.054571480335124e+91\n",
      "Gradient Descent(41/49): loss=1.4935646727860601e+94\n",
      "Gradient Descent(42/49): loss=3.684382022806721e+96\n",
      "Gradient Descent(43/49): loss=9.088773413915525e+98\n",
      "Gradient Descent(44/49): loss=2.2420531220204114e+101\n",
      "Gradient Descent(45/49): loss=5.530781738122392e+103\n",
      "Gradient Descent(46/49): loss=1.3643542311424799e+106\n",
      "Gradient Descent(47/49): loss=3.365640801201403e+108\n",
      "Gradient Descent(48/49): loss=8.302490470694075e+110\n",
      "Gradient Descent(49/49): loss=2.0480898612638166e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.299688817475655\n",
      "Gradient Descent(2/49): loss=287.19029247373055\n",
      "Gradient Descent(3/49): loss=18375.80463690141\n",
      "Gradient Descent(4/49): loss=1423780.0057961987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5/49): loss=164282164.57172847\n",
      "Gradient Descent(6/49): loss=29250769106.47489\n",
      "Gradient Descent(7/49): loss=6535924452758.531\n",
      "Gradient Descent(8/49): loss=1571538327865642.8\n",
      "Gradient Descent(9/49): loss=3.8528372629599123e+17\n",
      "Gradient Descent(10/49): loss=9.491712205474036e+19\n",
      "Gradient Descent(11/49): loss=2.3411394641159443e+22\n",
      "Gradient Descent(12/49): loss=5.776133393596531e+24\n",
      "Gradient Descent(13/49): loss=1.4252081636039426e+27\n",
      "Gradient Descent(14/49): loss=3.5166327718408264e+29\n",
      "Gradient Descent(15/49): loss=8.677159994453735e+31\n",
      "Gradient Descent(16/49): loss=2.141059069400205e+34\n",
      "Gradient Descent(17/49): loss=5.28299019992927e+36\n",
      "Gradient Descent(18/49): loss=1.3035598884111377e+39\n",
      "Gradient Descent(19/49): loss=3.2164897996346703e+41\n",
      "Gradient Descent(20/49): loss=7.9365794869307e+43\n",
      "Gradient Descent(21/49): loss=1.9583240717244248e+46\n",
      "Gradient Descent(22/49): loss=4.8320982320931326e+48\n",
      "Gradient Descent(23/49): loss=1.1923038511872213e+51\n",
      "Gradient Descent(24/49): loss=2.9419693170423387e+53\n",
      "Gradient Descent(25/49): loss=7.259209515954446e+55\n",
      "Gradient Descent(26/49): loss=1.7911853292047649e+58\n",
      "Gradient Descent(27/49): loss=4.4196890536189316e+60\n",
      "Gradient Descent(28/49): loss=1.0905432850631845e+63\n",
      "Gradient Descent(29/49): loss=2.6908785712484023e+65\n",
      "Gradient Descent(30/49): loss=6.639651616198268e+67\n",
      "Gradient Descent(31/49): loss=1.6383115186067746e+70\n",
      "Gradient Descent(32/49): loss=4.042478110525472e+72\n",
      "Gradient Descent(33/49): loss=9.974677641266925e+74\n",
      "Gradient Descent(34/49): loss=2.4612178798973917e+77\n",
      "Gradient Descent(35/49): loss=6.072971648993905e+79\n",
      "Gradient Descent(36/49): loss=1.4984851585354323e+82\n",
      "Gradient Descent(37/49): loss=3.6974613091154443e+84\n",
      "Gradient Descent(38/49): loss=9.123360384675079e+86\n",
      "Gradient Descent(39/49): loss=2.2511582339876465e+89\n",
      "Gradient Descent(40/49): loss=5.554656596666696e+91\n",
      "Gradient Descent(41/49): loss=1.370592677185483e+94\n",
      "Gradient Descent(42/49): loss=3.3818909487252976e+96\n",
      "Gradient Descent(43/49): loss=8.344701222654011e+98\n",
      "Gradient Descent(44/49): loss=2.0590267265007346e+101\n",
      "Gradient Descent(45/49): loss=5.080578617883592e+103\n",
      "Gradient Descent(46/49): loss=1.2536155437070576e+106\n",
      "Gradient Descent(47/49): loss=3.09325383902561e+108\n",
      "Gradient Descent(48/49): loss=7.632498943298377e+110\n",
      "Gradient Descent(49/49): loss=1.8832932294299174e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.10655364462212\n",
      "Gradient Descent(2/49): loss=272.6483373943108\n",
      "Gradient Descent(3/49): loss=17005.642453536464\n",
      "Gradient Descent(4/49): loss=1230633.0305792352\n",
      "Gradient Descent(5/49): loss=126538575.51879826\n",
      "Gradient Descent(6/49): loss=20743577315.652687\n",
      "Gradient Descent(7/49): loss=4532630661463.473\n",
      "Gradient Descent(8/49): loss=1094547858380304.4\n",
      "Gradient Descent(9/49): loss=2.7150230655200557e+17\n",
      "Gradient Descent(10/49): loss=6.779520685647538e+19\n",
      "Gradient Descent(11/49): loss=1.6956050986743143e+22\n",
      "Gradient Descent(12/49): loss=4.2424764374348827e+24\n",
      "Gradient Descent(13/49): loss=1.0615853113998984e+27\n",
      "Gradient Descent(14/49): loss=2.656441027214204e+29\n",
      "Gradient Descent(15/49): loss=6.647338926349843e+31\n",
      "Gradient Descent(16/49): loss=1.6633974681002605e+34\n",
      "Gradient Descent(17/49): loss=4.162405501539017e+36\n",
      "Gradient Descent(18/49): loss=1.041580332511923e+39\n",
      "Gradient Descent(19/49): loss=2.6064005743556384e+41\n",
      "Gradient Descent(20/49): loss=6.522131584096128e+43\n",
      "Gradient Descent(21/49): loss=1.63206687660115e+46\n",
      "Gradient Descent(22/49): loss=4.084005751858428e+48\n",
      "Gradient Descent(23/49): loss=1.0219619809317883e+51\n",
      "Gradient Descent(24/49): loss=2.5573085689468874e+53\n",
      "Gradient Descent(25/49): loss=6.399286117150789e+55\n",
      "Gradient Descent(26/49): loss=1.6013266176188338e+58\n",
      "Gradient Descent(27/49): loss=4.007082804787741e+60\n",
      "Gradient Descent(28/49): loss=1.0027131521926622e+63\n",
      "Gradient Descent(29/49): loss=2.5091412245807327e+65\n",
      "Gradient Descent(30/49): loss=6.278754468437437e+67\n",
      "Gradient Descent(31/49): loss=1.571165356844754e+70\n",
      "Gradient Descent(32/49): loss=3.931608714687377e+72\n",
      "Gradient Descent(33/49): loss=9.838268784418952e+74\n",
      "Gradient Descent(34/49): loss=2.4618811203893e+77\n",
      "Gradient Descent(35/49): loss=6.160493053948792e+79\n",
      "Gradient Descent(36/49): loss=1.541572188577051e+82\n",
      "Gradient Descent(37/49): loss=3.8575561919853437e+84\n",
      "Gradient Descent(38/49): loss=9.652963308880103e+86\n",
      "Gradient Descent(39/49): loss=2.415511168344931e+89\n",
      "Gradient Descent(40/49): loss=6.04445911343245e+91\n",
      "Gradient Descent(41/49): loss=1.512536412696056e+94\n",
      "Gradient Descent(42/49): loss=3.7848984612161587e+96\n",
      "Gradient Descent(43/49): loss=9.471148093672652e+98\n",
      "Gradient Descent(44/49): loss=2.370014602279599e+101\n",
      "Gradient Descent(45/49): loss=5.930610692035296e+103\n",
      "Gradient Descent(46/49): loss=1.484047530620814e+106\n",
      "Gradient Descent(47/49): loss=3.7136092512354076e+108\n",
      "Gradient Descent(48/49): loss=9.292757399145933e+110\n",
      "Gradient Descent(49/49): loss=2.3253749718189707e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.193723236679661\n",
      "Gradient Descent(2/49): loss=281.7222732177995\n",
      "Gradient Descent(3/49): loss=18399.787759155617\n",
      "Gradient Descent(4/49): loss=1528659.5436345222\n",
      "Gradient Descent(5/49): loss=199696402.30261788\n",
      "Gradient Descent(6/49): loss=39479477279.81807\n",
      "Gradient Descent(7/49): loss=9385023178559.676\n",
      "Gradient Descent(8/49): loss=2354378816241904.5\n",
      "Gradient Descent(9/49): loss=5.986487576647224e+17\n",
      "Gradient Descent(10/49): loss=1.527122544701236e+20\n",
      "Gradient Descent(11/49): loss=3.898610974260317e+22\n",
      "Gradient Descent(12/49): loss=9.95463078983048e+24\n",
      "Gradient Descent(13/49): loss=2.541904274296501e+27\n",
      "Gradient Descent(14/49): loss=6.490791744705109e+29\n",
      "Gradient Descent(15/49): loss=1.6574376998957337e+32\n",
      "Gradient Descent(16/49): loss=4.23230542120063e+34\n",
      "Gradient Descent(17/49): loss=1.0807291049404986e+37\n",
      "Gradient Descent(18/49): loss=2.7596671783678946e+39\n",
      "Gradient Descent(19/49): loss=7.046875076091914e+41\n",
      "Gradient Descent(20/49): loss=1.799436135534513e+44\n",
      "Gradient Descent(21/49): loss=4.594902523876467e+46\n",
      "Gradient Descent(22/49): loss=1.1733191742202484e+49\n",
      "Gradient Descent(23/49): loss=2.9960981271580906e+51\n",
      "Gradient Descent(24/49): loss=7.650607085303329e+53\n",
      "Gradient Descent(25/49): loss=1.9536005260712954e+56\n",
      "Gradient Descent(26/49): loss=4.988564924211987e+58\n",
      "Gradient Descent(27/49): loss=1.2738417947258293e+61\n",
      "Gradient Descent(28/49): loss=3.2527850045906887e+63\n",
      "Gradient Descent(29/49): loss=8.306063068347919e+65\n",
      "Gradient Descent(30/49): loss=2.1209727540554877e+68\n",
      "Gradient Descent(31/49): loss=5.415953847723986e+70\n",
      "Gradient Descent(32/49): loss=1.3829765622679931e+73\n",
      "Gradient Descent(33/49): loss=3.5314632021583903e+75\n",
      "Gradient Descent(34/49): loss=9.017674404942029e+77\n",
      "Gradient Descent(35/49): loss=2.302684383737828e+80\n",
      "Gradient Descent(36/49): loss=5.879958770970991e+82\n",
      "Gradient Descent(37/49): loss=1.501461311523607e+85\n",
      "Gradient Descent(38/49): loss=3.834016798097214e+87\n",
      "Gradient Descent(39/49): loss=9.790252133220061e+89\n",
      "Gradient Descent(40/49): loss=2.4999639250300108e+92\n",
      "Gradient Descent(41/49): loss=6.383716722927593e+94\n",
      "Gradient Descent(42/49): loss=1.630097090224841e+97\n",
      "Gradient Descent(43/49): loss=4.1624912866447406e+99\n",
      "Gradient Descent(44/49): loss=1.0629019470860818e+102\n",
      "Gradient Descent(45/49): loss=2.714145138859991e+104\n",
      "Gradient Descent(46/49): loss=6.930633493515221e+106\n",
      "Gradient Descent(47/49): loss=1.7697535748442163e+109\n",
      "Gradient Descent(48/49): loss=4.5191074071431517e+111\n",
      "Gradient Descent(49/49): loss=1.1539647128043662e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.237532541153012\n",
      "Gradient Descent(2/49): loss=287.265547953822\n",
      "Gradient Descent(3/49): loss=18738.51900005137\n",
      "Gradient Descent(4/49): loss=1501120.9532857977\n",
      "Gradient Descent(5/49): loss=181143391.15685168\n",
      "Gradient Descent(6/49): loss=33312954958.640545\n",
      "Gradient Descent(7/49): loss=7566113866682.61\n",
      "Gradient Descent(8/49): loss=1837341530382901.0\n",
      "Gradient Descent(9/49): loss=4.541316330618525e+17\n",
      "Gradient Descent(10/49): loss=1.1274450233104785e+20\n",
      "Gradient Descent(11/49): loss=2.8020998656448533e+22\n",
      "Gradient Descent(12/49): loss=6.966083232982418e+24\n",
      "Gradient Descent(13/49): loss=1.731898499160199e+27\n",
      "Gradient Descent(14/49): loss=4.305893414940152e+29\n",
      "Gradient Descent(15/49): loss=1.0705472775349937e+32\n",
      "Gradient Descent(16/49): loss=2.6616371785542788e+34\n",
      "Gradient Descent(17/49): loss=6.617469710087996e+36\n",
      "Gradient Descent(18/49): loss=1.6452621085454295e+39\n",
      "Gradient Descent(19/49): loss=4.090517318466698e+41\n",
      "Gradient Descent(20/49): loss=1.0170009936931025e+44\n",
      "Gradient Descent(21/49): loss=2.5285090895877154e+46\n",
      "Gradient Descent(22/49): loss=6.286481780414687e+48\n",
      "Gradient Descent(23/49): loss=1.5629705797909083e+51\n",
      "Gradient Descent(24/49): loss=3.8859208038025635e+53\n",
      "Gradient Descent(25/49): loss=9.661333801621866e+55\n",
      "Gradient Descent(26/49): loss=2.402039967852591e+58\n",
      "Gradient Descent(27/49): loss=5.972049124515168e+60\n",
      "Gradient Descent(28/49): loss=1.4847950584897127e+63\n",
      "Gradient Descent(29/49): loss=3.691557654248979e+65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=9.178100261530924e+67\n",
      "Gradient Descent(31/49): loss=2.2818964865348105e+70\n",
      "Gradient Descent(32/49): loss=5.6733435317599146e+72\n",
      "Gradient Descent(33/49): loss=1.4105296633432817e+75\n",
      "Gradient Descent(34/49): loss=3.506916018805031e+77\n",
      "Gradient Descent(35/49): loss=8.71903674382967e+79\n",
      "Gradient Descent(36/49): loss=2.167762254145909e+82\n",
      "Gradient Descent(37/49): loss=5.389578377250492e+84\n",
      "Gradient Descent(38/49): loss=1.3399788205081838e+87\n",
      "Gradient Descent(39/49): loss=3.3315096538711396e+89\n",
      "Gradient Descent(40/49): loss=8.282934329982326e+91\n",
      "Gradient Descent(41/49): loss=2.059336704460067e+94\n",
      "Gradient Descent(42/49): loss=5.1200063810546305e+96\n",
      "Gradient Descent(43/49): loss=1.2729567382188924e+99\n",
      "Gradient Descent(44/49): loss=3.16487663642931e+101\n",
      "Gradient Descent(45/49): loss=7.868644568260086e+103\n",
      "Gradient Descent(46/49): loss=1.956334304753911e+106\n",
      "Gradient Descent(47/49): loss=4.863917640142445e+108\n",
      "Gradient Descent(48/49): loss=1.2092869175068648e+111\n",
      "Gradient Descent(49/49): loss=3.006578147590639e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.341264239841415\n",
      "Gradient Descent(2/49): loss=292.0022613529635\n",
      "Gradient Descent(3/49): loss=18837.524064934547\n",
      "Gradient Descent(4/49): loss=1471090.5694471325\n",
      "Gradient Descent(5/49): loss=170995960.5727316\n",
      "Gradient Descent(6/49): loss=30669602512.858997\n",
      "Gradient Descent(7/49): loss=6905158782288.853\n",
      "Gradient Descent(8/49): loss=1673243167705400.5\n",
      "Gradient Descent(9/49): loss=4.134348027641868e+17\n",
      "Gradient Descent(10/49): loss=1.026527463237317e+20\n",
      "Gradient Descent(11/49): loss=2.551849227383626e+22\n",
      "Gradient Descent(12/49): loss=6.345520081694058e+24\n",
      "Gradient Descent(13/49): loss=1.5780136214953925e+27\n",
      "Gradient Descent(14/49): loss=3.924297689746878e+29\n",
      "Gradient Descent(15/49): loss=9.759218045141596e+31\n",
      "Gradient Descent(16/49): loss=2.4269931940141636e+34\n",
      "Gradient Descent(17/49): loss=6.03562443909611e+36\n",
      "Gradient Descent(18/49): loss=1.500983388752529e+39\n",
      "Gradient Descent(19/49): loss=3.732755725897445e+41\n",
      "Gradient Descent(20/49): loss=9.282891114489362e+43\n",
      "Gradient Descent(21/49): loss=2.3085375484461494e+46\n",
      "Gradient Descent(22/49): loss=5.741040746981582e+48\n",
      "Gradient Descent(23/49): loss=1.427724183327194e+51\n",
      "Gradient Descent(24/49): loss=3.5505693715151567e+53\n",
      "Gradient Descent(25/49): loss=8.829816717543883e+55\n",
      "Gradient Descent(26/49): loss=2.1958636800894825e+58\n",
      "Gradient Descent(27/49): loss=5.460835095202729e+60\n",
      "Gradient Descent(28/49): loss=1.3580405836387196e+63\n",
      "Gradient Descent(29/49): loss=3.3772750772680332e+65\n",
      "Gradient Descent(30/49): loss=8.398855737414363e+67\n",
      "Gradient Descent(31/49): loss=2.088689137959115e+70\n",
      "Gradient Descent(32/49): loss=5.194305571405772e+72\n",
      "Gradient Descent(33/49): loss=1.2917580638878636e+75\n",
      "Gradient Descent(34/49): loss=3.212438838417698e+77\n",
      "Gradient Descent(35/49): loss=7.988928870716313e+79\n",
      "Gradient Descent(36/49): loss=1.9867455136609746e+82\n",
      "Gradient Descent(37/49): loss=4.940784678307008e+84\n",
      "Gradient Descent(38/49): loss=1.2287106259729377e+87\n",
      "Gradient Descent(39/49): loss=3.0556478387075577e+89\n",
      "Gradient Descent(40/49): loss=7.599009495669253e+91\n",
      "Gradient Descent(41/49): loss=1.889777499350083e+94\n",
      "Gradient Descent(42/49): loss=4.699637497604302e+96\n",
      "Gradient Descent(43/49): loss=1.1687403737468824e+99\n",
      "Gradient Descent(44/49): loss=2.90650941040084e+101\n",
      "Gradient Descent(45/49): loss=7.228121097302288e+103\n",
      "Gradient Descent(46/49): loss=1.7975422481106108e+106\n",
      "Gradient Descent(47/49): loss=4.470260099749157e+108\n",
      "Gradient Descent(48/49): loss=1.111697117573375e+111\n",
      "Gradient Descent(49/49): loss=2.764650050878025e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.1465078630090595\n",
      "Gradient Descent(2/49): loss=277.2196211798231\n",
      "Gradient Descent(3/49): loss=17433.69425983294\n",
      "Gradient Descent(4/49): loss=1271699.3014462716\n",
      "Gradient Descent(5/49): loss=131736010.00549345\n",
      "Gradient Descent(6/49): loss=21752048827.694656\n",
      "Gradient Descent(7/49): loss=4788786509891.931\n",
      "Gradient Descent(8/49): loss=1165366796429056.5\n",
      "Gradient Descent(9/49): loss=2.913321787465515e+17\n",
      "Gradient Descent(10/49): loss=7.331808836950686e+19\n",
      "Gradient Descent(11/49): loss=1.8481486023517826e+22\n",
      "Gradient Descent(12/49): loss=4.660498890481582e+24\n",
      "Gradient Descent(13/49): loss=1.175354488995299e+27\n",
      "Gradient Descent(14/49): loss=2.964252549697535e+29\n",
      "Gradient Descent(15/49): loss=7.475907398594757e+31\n",
      "Gradient Descent(16/49): loss=1.885442090791881e+34\n",
      "Gradient Descent(17/49): loss=4.755132480975689e+36\n",
      "Gradient Descent(18/49): loss=1.1992564904302709e+39\n",
      "Gradient Descent(19/49): loss=3.024555387686676e+41\n",
      "Gradient Descent(20/49): loss=7.628005691013642e+43\n",
      "Gradient Descent(21/49): loss=1.9238024577623987e+46\n",
      "Gradient Descent(22/49): loss=4.851878783745653e+48\n",
      "Gradient Descent(23/49): loss=1.2236561835466314e+51\n",
      "Gradient Descent(24/49): loss=3.0860920527748743e+53\n",
      "Gradient Descent(25/49): loss=7.783202738068876e+55\n",
      "Gradient Descent(26/49): loss=1.9629435488635328e+58\n",
      "Gradient Descent(27/49): loss=4.950593612549106e+60\n",
      "Gradient Descent(28/49): loss=1.2485523147521386e+63\n",
      "Gradient Descent(29/49): loss=3.148880729618684e+65\n",
      "Gradient Descent(30/49): loss=7.941557379862356e+67\n",
      "Gradient Descent(31/49): loss=2.002880992741905e+70\n",
      "Gradient Descent(32/49): loss=5.0513168629355366e+72\n",
      "Gradient Descent(33/49): loss=1.2739549749706338e+75\n",
      "Gradient Descent(34/49): loss=3.212946885516185e+77\n",
      "Gradient Descent(35/49): loss=8.103133856348337e+79\n",
      "Gradient Descent(36/49): loss=2.0436309915328784e+82\n",
      "Gradient Descent(37/49): loss=5.154089397501023e+84\n",
      "Gradient Descent(38/49): loss=1.2998744698771486e+87\n",
      "Gradient Descent(39/49): loss=3.278316511657013e+89\n",
      "Gradient Descent(40/49): loss=8.267997717978458e+91\n",
      "Gradient Descent(41/49): loss=2.0852100772279263e+94\n",
      "Gradient Descent(42/49): loss=5.258952910349854e+96\n",
      "Gradient Descent(43/49): loss=1.3263213148309327e+99\n",
      "Gradient Descent(44/49): loss=3.3450161280448984e+101\n",
      "Gradient Descent(45/49): loss=8.436215848877325e+103\n",
      "Gradient Descent(46/49): loss=2.127635118173421e+106\n",
      "Gradient Descent(47/49): loss=5.365949943880573e+108\n",
      "Gradient Descent(48/49): loss=1.35330623913329e+111\n",
      "Gradient Descent(49/49): loss=3.4130727942509564e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.234240819206135\n",
      "Gradient Descent(2/49): loss=286.423692030747\n",
      "Gradient Descent(3/49): loss=18859.6825317806\n",
      "Gradient Descent(4/49): loss=1579050.7539280914\n",
      "Gradient Descent(5/49): loss=207784964.261501\n",
      "Gradient Descent(6/49): loss=41381377192.261024\n",
      "Gradient Descent(7/49): loss=9912317701986.64\n",
      "Gradient Descent(8/49): loss=2505983613075439.0\n",
      "Gradient Descent(9/49): loss=6.421754496748687e+17\n",
      "Gradient Descent(10/49): loss=1.6509739766398383e+20\n",
      "Gradient Descent(11/49): loss=4.247782983237029e+22\n",
      "Gradient Descent(12/49): loss=1.0931103776272697e+25\n",
      "Gradient Descent(13/49): loss=2.8130962173689844e+27\n",
      "Gradient Descent(14/49): loss=7.239517628570967e+29\n",
      "Gradient Descent(15/49): loss=1.863098146161159e+32\n",
      "Gradient Descent(16/49): loss=4.794707174600614e+34\n",
      "Gradient Descent(17/49): loss=1.2339242616841078e+37\n",
      "Gradient Descent(18/49): loss=3.175520584778196e+39\n",
      "Gradient Descent(19/49): loss=8.172244744041343e+41\n",
      "Gradient Descent(20/49): loss=2.103138131073659e+44\n",
      "Gradient Descent(21/49): loss=5.412454152841086e+46\n",
      "Gradient Descent(22/49): loss=1.3929023266131994e+49\n",
      "Gradient Descent(23/49): loss=3.584652796575219e+51\n",
      "Gradient Descent(24/49): loss=9.225151991319273e+53\n",
      "Gradient Descent(25/49): loss=2.3741052228088973e+56\n",
      "Gradient Descent(26/49): loss=6.109791593974795e+58\n",
      "Gradient Descent(27/49): loss=1.5723630512738223e+61\n",
      "Gradient Descent(28/49): loss=4.046497375539265e+63\n",
      "Gradient Descent(29/49): loss=1.0413715202084717e+66\n",
      "Gradient Descent(30/49): loss=2.6799835572777375e+68\n",
      "Gradient Descent(31/49): loss=6.8969735852209e+70\n",
      "Gradient Descent(32/49): loss=1.774945391215514e+73\n",
      "Gradient Descent(33/49): loss=4.5678457411350515e+75\n",
      "Gradient Descent(34/49): loss=1.1755412204832392e+78\n",
      "Gradient Descent(35/49): loss=3.0252710782475543e+80\n",
      "Gradient Descent(36/49): loss=7.785575645844823e+82\n",
      "Gradient Descent(37/49): loss=2.003628321872055e+85\n",
      "Gradient Descent(38/49): loss=5.156364326574116e+87\n",
      "Gradient Descent(39/49): loss=1.3269972668146242e+90\n",
      "Gradient Descent(40/49): loss=3.4150452423587496e+92\n",
      "Gradient Descent(41/49): loss=8.788664678527908e+94\n",
      "Gradient Descent(42/49): loss=2.2617746281526553e+97\n",
      "Gradient Descent(43/49): loss=5.820707303867583e+99\n",
      "Gradient Descent(44/49): loss=1.4979668219627224e+102\n",
      "Gradient Descent(45/49): loss=3.8550376828090907e+104\n",
      "Gradient Descent(46/49): loss=9.920991118084613e+106\n",
      "Gradient Descent(47/49): loss=2.553180354216208e+109\n",
      "Gradient Descent(48/49): loss=6.570643843509785e+111\n",
      "Gradient Descent(49/49): loss=1.6909639950408758e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.278402728230348\n",
      "Gradient Descent(2/49): loss=292.05706423985765\n",
      "Gradient Descent(3/49): loss=19207.044412622807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/49): loss=1550722.1774462375\n",
      "Gradient Descent(5/49): loss=188505145.87681052\n",
      "Gradient Descent(6/49): loss=34921263423.75642\n",
      "Gradient Descent(7/49): loss=7991751191926.479\n",
      "Gradient Descent(8/49): loss=1955764720385399.8\n",
      "Gradient Descent(9/49): loss=4.871789315930447e+17\n",
      "Gradient Descent(10/49): loss=1.2189581096537876e+20\n",
      "Gradient Descent(11/49): loss=3.0532723920386222e+22\n",
      "Gradient Descent(12/49): loss=7.649968585933041e+24\n",
      "Gradient Descent(13/49): loss=1.9168256846058615e+27\n",
      "Gradient Descent(14/49): loss=4.8030011354407395e+29\n",
      "Gradient Descent(15/49): loss=1.2034955952502821e+32\n",
      "Gradient Descent(16/49): loss=3.015620941375311e+34\n",
      "Gradient Descent(17/49): loss=7.55629842818198e+36\n",
      "Gradient Descent(18/49): loss=1.8933961019239566e+39\n",
      "Gradient Descent(19/49): loss=4.744318776745251e+41\n",
      "Gradient Descent(20/49): loss=1.188793021914462e+44\n",
      "Gradient Descent(21/49): loss=2.9787813945768786e+46\n",
      "Gradient Descent(22/49): loss=7.46398947087658e+48\n",
      "Gradient Descent(23/49): loss=1.8702661069987827e+51\n",
      "Gradient Descent(24/49): loss=4.686361529156953e+53\n",
      "Gradient Descent(25/49): loss=1.174270565020064e+56\n",
      "Gradient Descent(26/49): loss=2.94239219764534e+58\n",
      "Gradient Descent(27/49): loss=7.3728083651811e+60\n",
      "Gradient Descent(28/49): loss=1.847418683110562e+63\n",
      "Gradient Descent(29/49): loss=4.6291123024762535e+65\n",
      "Gradient Descent(30/49): loss=1.1599255168761755e+68\n",
      "Gradient Descent(31/49): loss=2.906447536346703e+70\n",
      "Gradient Descent(32/49): loss=7.282741140384519e+72\n",
      "Gradient Descent(33/49): loss=1.824850366455141e+75\n",
      "Gradient Descent(34/49): loss=4.572562440102996e+77\n",
      "Gradient Descent(35/49): loss=1.1457557097821728e+80\n",
      "Gradient Descent(36/49): loss=2.8709419798952724e+82\n",
      "Gradient Descent(37/49): loss=7.193774188995356e+84\n",
      "Gradient Descent(38/49): loss=1.8025577474102528e+87\n",
      "Gradient Descent(39/49): loss=4.5167033985014727e+89\n",
      "Gradient Descent(40/49): loss=1.1317590029692417e+92\n",
      "Gradient Descent(41/49): loss=2.8358701641264893e+94\n",
      "Gradient Descent(42/49): loss=7.105894069924698e+96\n",
      "Gradient Descent(43/49): loss=1.7805374580166214e+99\n",
      "Gradient Descent(44/49): loss=4.461526738512024e+101\n",
      "Gradient Descent(45/49): loss=1.1179332818210472e+104\n",
      "Gradient Descent(46/49): loss=2.8012267904057346e+106\n",
      "Gradient Descent(47/49): loss=7.019087506281882e+108\n",
      "Gradient Descent(48/49): loss=1.7587861714583896e+111\n",
      "Gradient Descent(49/49): loss=4.4070241240686285e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.383015135688933\n",
      "Gradient Descent(2/49): loss=296.87454454717147\n",
      "Gradient Descent(3/49): loss=19308.874623401978\n",
      "Gradient Descent(4/49): loss=1519772.0455465664\n",
      "Gradient Descent(5/49): loss=177955904.48175582\n",
      "Gradient Descent(6/49): loss=32151295872.81315\n",
      "Gradient Descent(7/49): loss=7293687342596.496\n",
      "Gradient Descent(8/49): loss=1781094124621198.2\n",
      "Gradient Descent(9/49): loss=4.4352070238075706e+17\n",
      "Gradient Descent(10/49): loss=1.109848796660493e+20\n",
      "Gradient Descent(11/49): loss=2.780588320248749e+22\n",
      "Gradient Descent(12/49): loss=6.968477682155449e+24\n",
      "Gradient Descent(13/49): loss=1.7465080824735342e+27\n",
      "Gradient Descent(14/49): loss=4.377347245212674e+29\n",
      "Gradient Descent(15/49): loss=1.0971178727953105e+32\n",
      "Gradient Descent(16/49): loss=2.7497679388680237e+34\n",
      "Gradient Descent(17/49): loss=6.891899113829055e+36\n",
      "Gradient Descent(18/49): loss=1.7273558167903882e+39\n",
      "Gradient Descent(19/49): loss=4.3293700551981146e+41\n",
      "Gradient Descent(20/49): loss=1.0850946264137779e+44\n",
      "Gradient Descent(21/49): loss=2.7196343448856996e+46\n",
      "Gradient Descent(22/49): loss=6.816374159033967e+48\n",
      "Gradient Descent(23/49): loss=1.708426603964416e+51\n",
      "Gradient Descent(24/49): loss=4.281926715108004e+53\n",
      "Gradient Descent(25/49): loss=1.0732036337453435e+56\n",
      "Gradient Descent(26/49): loss=2.689831274835691e+58\n",
      "Gradient Descent(27/49): loss=6.741677030887047e+60\n",
      "Gradient Descent(28/49): loss=1.6897048381433846e+63\n",
      "Gradient Descent(29/49): loss=4.235003289188402e+65\n",
      "Gradient Descent(30/49): loss=1.0614429487663135e+68\n",
      "Gradient Descent(31/49): loss=2.6603548015228348e+70\n",
      "Gradient Descent(32/49): loss=6.66779847019698e+72\n",
      "Gradient Descent(33/49): loss=1.671188234505878e+75\n",
      "Gradient Descent(34/49): loss=4.1885940728924893e+77\n",
      "Gradient Descent(35/49): loss=1.0498111430671621e+80\n",
      "Gradient Descent(36/49): loss=2.631201345674775e+82\n",
      "Gradient Descent(37/49): loss=6.5947295065412334e+84\n",
      "Gradient Descent(38/49): loss=1.652874544775369e+87\n",
      "Gradient Descent(39/49): loss=4.1426934312566947e+89\n",
      "Gradient Descent(40/49): loss=1.0383068043261438e+92\n",
      "Gradient Descent(41/49): loss=2.6023673675098977e+94\n",
      "Gradient Descent(42/49): loss=6.52246126796361e+96\n",
      "Gradient Descent(43/49): loss=1.6347615453229093e+99\n",
      "Gradient Descent(44/49): loss=4.097295791073303e+101\n",
      "Gradient Descent(45/49): loss=1.0269285356985021e+104\n",
      "Gradient Descent(46/49): loss=2.573849366036657e+106\n",
      "Gradient Descent(47/49): loss=6.450984979730238e+108\n",
      "Gradient Descent(48/49): loss=1.6168470368872384e+111\n",
      "Gradient Descent(49/49): loss=4.0523956402088425e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.186630874524341\n",
      "Gradient Descent(2/49): loss=281.84824342405204\n",
      "Gradient Descent(3/49): loss=17870.69009161301\n",
      "Gradient Descent(4/49): loss=1313960.4467356324\n",
      "Gradient Descent(5/49): loss=137124968.92379764\n",
      "Gradient Descent(6/49): loss=22805306165.67727\n",
      "Gradient Descent(7/49): loss=5058333212996.185\n",
      "Gradient Descent(8/49): loss=1240464440372553.2\n",
      "Gradient Descent(9/49): loss=3.125244370295379e+17\n",
      "Gradient Descent(10/49): loss=7.926667060021094e+19\n",
      "Gradient Descent(11/49): loss=2.0137388185180063e+22\n",
      "Gradient Descent(12/49): loss=5.117833982502136e+24\n",
      "Gradient Descent(13/49): loss=1.3007995843174916e+27\n",
      "Gradient Descent(14/49): loss=3.306317146273188e+29\n",
      "Gradient Descent(15/49): loss=8.403902839746382e+31\n",
      "Gradient Descent(16/49): loss=2.136082949509738e+34\n",
      "Gradient Descent(17/49): loss=5.429443815553349e+36\n",
      "Gradient Descent(18/49): loss=1.3800429628589413e+39\n",
      "Gradient Descent(19/49): loss=3.507760057109467e+41\n",
      "Gradient Descent(20/49): loss=8.915940303845058e+43\n",
      "Gradient Descent(21/49): loss=2.266232304767793e+46\n",
      "Gradient Descent(22/49): loss=5.7602548755311776e+48\n",
      "Gradient Descent(23/49): loss=1.4641277579252294e+51\n",
      "Gradient Descent(24/49): loss=3.7214847916987266e+53\n",
      "Gradient Descent(25/49): loss=9.459180716934955e+55\n",
      "Gradient Descent(26/49): loss=2.404311849810682e+58\n",
      "Gradient Descent(27/49): loss=6.111222149283984e+60\n",
      "Gradient Descent(28/49): loss=1.5533357771722262e+63\n",
      "Gradient Descent(29/49): loss=3.948231593783578e+65\n",
      "Gradient Descent(30/49): loss=1.0035520295894384e+68\n",
      "Gradient Descent(31/49): loss=2.550804460606527e+70\n",
      "Gradient Descent(32/49): loss=6.483573551151183e+72\n",
      "Gradient Descent(33/49): loss=1.6479791627458345e+75\n",
      "Gradient Descent(34/49): loss=4.188793879514661e+77\n",
      "Gradient Descent(35/49): loss=1.0646975739562613e+80\n",
      "Gradient Descent(36/49): loss=2.7062227376050744e+82\n",
      "Gradient Descent(37/49): loss=6.878611997129794e+84\n",
      "Gradient Descent(38/49): loss=1.7483890867360798e+87\n",
      "Gradient Descent(39/49): loss=4.444013414179273e+89\n",
      "Gradient Descent(40/49): loss=1.1295686626752776e+92\n",
      "Gradient Descent(41/49): loss=2.871110513813876e+94\n",
      "Gradient Descent(42/49): loss=7.297719788905126e+96\n",
      "Gradient Descent(43/49): loss=1.8549168992674392e+99\n",
      "Gradient Descent(44/49): loss=4.7147832510901576e+101\n",
      "Gradient Descent(45/49): loss=1.1983922898939534e+104\n",
      "Gradient Descent(46/49): loss=3.0460447575086533e+106\n",
      "Gradient Descent(47/49): loss=7.742363450591692e+108\n",
      "Gradient Descent(48/49): loss=1.9679353579191814e+111\n",
      "Gradient Descent(49/49): loss=5.002050856515356e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.274928813799258\n",
      "Gradient Descent(2/49): loss=291.18380749936006\n",
      "Gradient Descent(3/49): loss=19329.1210089626\n",
      "Gradient Descent(4/49): loss=1630890.730409646\n",
      "Gradient Descent(5/49): loss=216167435.5065553\n",
      "Gradient Descent(6/49): loss=43366957644.84187\n",
      "Gradient Descent(7/49): loss=10467015033350.14\n",
      "Gradient Descent(8/49): loss=2666704237957949.0\n",
      "Gradient Descent(9/49): loss=6.886791386875455e+17\n",
      "Gradient Descent(10/49): loss=1.7843294255531383e+20\n",
      "Gradient Descent(11/49): loss=4.626686361862808e+22\n",
      "Gradient Descent(12/49): loss=1.1998999767344644e+25\n",
      "Gradient Descent(13/49): loss=3.111995760054806e+27\n",
      "Gradient Descent(14/49): loss=8.071187689483072e+29\n",
      "Gradient Descent(15/49): loss=2.0933264619662246e+32\n",
      "Gradient Descent(16/49): loss=5.429211161638155e+34\n",
      "Gradient Descent(17/49): loss=1.4081099362939205e+37\n",
      "Gradient Descent(18/49): loss=3.6520474128260966e+39\n",
      "Gradient Descent(19/49): loss=9.471881467060803e+41\n",
      "Gradient Descent(20/49): loss=2.456609363183219e+44\n",
      "Gradient Descent(21/49): loss=6.371415844786552e+46\n",
      "Gradient Descent(22/49): loss=1.6524784314426268e+49\n",
      "Gradient Descent(23/49): loss=4.285836983446297e+51\n",
      "Gradient Descent(24/49): loss=1.1115666201311598e+54\n",
      "Gradient Descent(25/49): loss=2.8829382819828234e+56\n",
      "Gradient Descent(26/49): loss=7.477134511959537e+58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/49): loss=1.939255545612597e+61\n",
      "Gradient Descent(28/49): loss=5.029616713694401e+63\n",
      "Gradient Descent(29/49): loss=1.3044719322271085e+66\n",
      "Gradient Descent(30/49): loss=3.3832538716821314e+68\n",
      "Gradient Descent(31/49): loss=8.77474361652986e+70\n",
      "Gradient Descent(32/49): loss=2.2758010027059892e+73\n",
      "Gradient Descent(33/49): loss=5.902474682178514e+75\n",
      "Gradient Descent(34/49): loss=1.5308547334469712e+78\n",
      "Gradient Descent(35/49): loss=3.9703960475983054e+80\n",
      "Gradient Descent(36/49): loss=1.0297544522260713e+83\n",
      "Gradient Descent(37/49): loss=2.6707517818552233e+85\n",
      "Gradient Descent(38/49): loss=6.926811595583245e+87\n",
      "Gradient Descent(39/49): loss=1.7965248289519316e+90\n",
      "Gradient Descent(40/49): loss=4.659433011140023e+92\n",
      "Gradient Descent(41/49): loss=1.2084617832956329e+95\n",
      "Gradient Descent(42/49): loss=3.1342437549686946e+97\n",
      "Gradient Descent(43/49): loss=8.128915660676159e+99\n",
      "Gradient Descent(44/49): loss=2.108300278612003e+102\n",
      "Gradient Descent(45/49): loss=5.4680479541668503e+104\n",
      "Gradient Descent(46/49): loss=1.4181826342475618e+107\n",
      "Gradient Descent(47/49): loss=3.678171810012561e+109\n",
      "Gradient Descent(48/49): loss=9.539637235192182e+111\n",
      "Gradient Descent(49/49): loss=2.4741823731924825e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.319444730604599\n",
      "Gradient Descent(2/49): loss=296.908369393785\n",
      "Gradient Descent(3/49): loss=19685.29543135155\n",
      "Gradient Descent(4/49): loss=1601752.7062029713\n",
      "Gradient Descent(5/49): loss=196135302.24251527\n",
      "Gradient Descent(6/49): loss=36600491575.08458\n",
      "Gradient Descent(7/49): loss=8439536732171.926\n",
      "Gradient Descent(8/49): loss=2081315321251916.5\n",
      "Gradient Descent(9/49): loss=5.224884184862289e+17\n",
      "Gradient Descent(10/49): loss=1.3174994729822321e+20\n",
      "Gradient Descent(11/49): loss=3.325849427205945e+22\n",
      "Gradient Descent(12/49): loss=8.397936034314376e+24\n",
      "Gradient Descent(13/49): loss=2.1206624625428146e+27\n",
      "Gradient Descent(14/49): loss=5.35522440886052e+29\n",
      "Gradient Descent(15/49): loss=1.3523389497796115e+32\n",
      "Gradient Descent(16/49): loss=3.4150248726937463e+34\n",
      "Gradient Descent(17/49): loss=8.623871803537477e+36\n",
      "Gradient Descent(18/49): loss=2.1777636222048117e+39\n",
      "Gradient Descent(19/49): loss=5.4994491715449005e+41\n",
      "Gradient Descent(20/49): loss=1.3887614336299098e+44\n",
      "Gradient Descent(21/49): loss=3.507002721744194e+46\n",
      "Gradient Descent(22/49): loss=8.856141735512188e+48\n",
      "Gradient Descent(23/49): loss=2.236418180044621e+51\n",
      "Gradient Descent(24/49): loss=5.647568010395923e+53\n",
      "Gradient Descent(25/49): loss=1.4261654960932076e+56\n",
      "Gradient Descent(26/49): loss=3.601458217952241e+58\n",
      "Gradient Descent(27/49): loss=9.094667716467109e+60\n",
      "Gradient Descent(28/49): loss=2.296652518711851e+63\n",
      "Gradient Descent(29/49): loss=5.799676201644095e+65\n",
      "Gradient Descent(30/49): loss=1.4645769775735146e+68\n",
      "Gradient Descent(31/49): loss=3.6984577218816606e+70\n",
      "Gradient Descent(32/49): loss=9.339618012573737e+72\n",
      "Gradient Descent(33/49): loss=2.3585091727481427e+75\n",
      "Gradient Descent(34/49): loss=5.955881183200788e+77\n",
      "Gradient Descent(35/49): loss=1.5040230107340651e+80\n",
      "Gradient Descent(36/49): loss=3.79806975195886e+82\n",
      "Gradient Descent(37/49): loss=9.591165652248793e+84\n",
      "Gradient Descent(38/49): loss=2.422031836604202e+87\n",
      "Gradient Descent(39/49): loss=6.116293295538038e+89\n",
      "Gradient Descent(40/49): loss=1.5445314595654438e+92\n",
      "Gradient Descent(41/49): loss=3.900364672386974e+94\n",
      "Gradient Descent(42/49): loss=9.849488324365238e+96\n",
      "Gradient Descent(43/49): loss=2.487265381583846e+99\n",
      "Gradient Descent(44/49): loss=6.281025850978996e+101\n",
      "Gradient Descent(45/49): loss=1.5861309385307487e+104\n",
      "Gradient Descent(46/49): loss=4.005414742517443e+106\n",
      "Gradient Descent(47/49): loss=1.0114768503562051e+109\n",
      "Gradient Descent(48/49): loss=2.5542558875275503e+111\n",
      "Gradient Descent(49/49): loss=6.450195213732905e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.424941505018208\n",
      "Gradient Descent(2/49): loss=301.80764414480365\n",
      "Gradient Descent(3/49): loss=19790.016419000698\n",
      "Gradient Descent(4/49): loss=1569858.3683741812\n",
      "Gradient Descent(5/49): loss=185169992.56212345\n",
      "Gradient Descent(6/49): loss=33698371935.027122\n",
      "Gradient Descent(7/49): loss=7702437209376.28\n",
      "Gradient Descent(8/49): loss=1895436482042647.5\n",
      "Gradient Descent(9/49): loss=4.7566608346274835e+17\n",
      "Gradient Descent(10/49): loss=1.1995692430418138e+20\n",
      "Gradient Descent(11/49): loss=3.0288200420782303e+22\n",
      "Gradient Descent(12/49): loss=7.649807814597548e+24\n",
      "Gradient Descent(13/49): loss=1.9322316838610894e+27\n",
      "Gradient Descent(14/49): loss=4.880627117155201e+29\n",
      "Gradient Descent(15/49): loss=1.2328037864829343e+32\n",
      "Gradient Descent(16/49): loss=3.1139580456556455e+34\n",
      "Gradient Descent(17/49): loss=7.865596593548999e+36\n",
      "Gradient Descent(18/49): loss=1.9867837928570627e+39\n",
      "Gradient Descent(19/49): loss=5.018449465489634e+41\n",
      "Gradient Descent(20/49): loss=1.2676183098541321e+44\n",
      "Gradient Descent(21/49): loss=3.2018977017850436e+46\n",
      "Gradient Descent(22/49): loss=8.087725473348596e+48\n",
      "Gradient Descent(23/49): loss=2.0428917294743628e+51\n",
      "Gradient Descent(24/49): loss=5.160173440809762e+53\n",
      "Gradient Descent(25/49): loss=1.3034166008485437e+56\n",
      "Gradient Descent(26/49): loss=3.292321188146642e+58\n",
      "Gradient Descent(27/49): loss=8.316127628622322e+60\n",
      "Gradient Descent(28/49): loss=2.1005842013388288e+63\n",
      "Gradient Descent(29/49): loss=5.305899793706525e+65\n",
      "Gradient Descent(30/49): loss=1.3402258573072773e+68\n",
      "Gradient Descent(31/49): loss=3.3852982876261716e+70\n",
      "Gradient Descent(32/49): loss=8.550979996185089e+72\n",
      "Gradient Descent(33/49): loss=2.1599059427767535e+75\n",
      "Gradient Descent(34/49): loss=5.455741545090249e+77\n",
      "Gradient Descent(35/49): loss=1.3780746289608464e+80\n",
      "Gradient Descent(36/49): loss=3.4809011154397826e+82\n",
      "Gradient Descent(37/49): loss=8.792464733645695e+84\n",
      "Gradient Descent(38/49): loss=2.2209029653126e+87\n",
      "Gradient Descent(39/49): loss=5.609814916242728e+89\n",
      "Gradient Descent(40/49): loss=1.4169922723332132e+92\n",
      "Gradient Descent(41/49): loss=3.5792038237098974e+94\n",
      "Gradient Descent(42/49): loss=9.040769143056529e+96\n",
      "Gradient Descent(43/49): loss=2.283622579876499e+99\n",
      "Gradient Descent(44/49): loss=5.768239410611447e+101\n",
      "Gradient Descent(45/49): loss=1.457008972994592e+104\n",
      "Gradient Descent(46/49): loss=3.680282658659077e+106\n",
      "Gradient Descent(47/49): loss=9.296085815991038e+108\n",
      "Gradient Descent(48/49): loss=2.348113433487076e+111\n",
      "Gradient Descent(49/49): loss=5.931137906491716e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.226922679167968\n",
      "Gradient Descent(2/49): loss=286.5346817790781\n",
      "Gradient Descent(3/49): loss=18316.778874708143\n",
      "Gradient Descent(4/49): loss=1357446.161240677\n",
      "Gradient Descent(5/49): loss=142711705.97341922\n",
      "Gradient Descent(6/49): loss=23905151200.96325\n",
      "Gradient Descent(7/49): loss=5341914488127.551\n",
      "Gradient Descent(8/49): loss=1320081114258240.8\n",
      "Gradient Descent(9/49): loss=3.351668251588504e+17\n",
      "Gradient Descent(10/49): loss=8.56719147151893e+19\n",
      "Gradient Descent(11/49): loss=2.1934341913974562e+22\n",
      "Gradient Descent(12/49): loss=5.618003820429029e+24\n",
      "Gradient Descent(13/49): loss=1.4390662839188362e+27\n",
      "Gradient Descent(14/49): loss=3.6862903880287116e+29\n",
      "Gradient Descent(15/49): loss=9.442797951528634e+31\n",
      "Gradient Descent(16/49): loss=2.418869460750321e+34\n",
      "Gradient Descent(17/49): loss=6.196183998364487e+36\n",
      "Gradient Descent(18/49): loss=1.587216662774654e+39\n",
      "Gradient Descent(19/49): loss=4.065819837821864e+41\n",
      "Gradient Descent(20/49): loss=1.0415018577042127e+44\n",
      "Gradient Descent(21/49): loss=2.6679148709385953e+46\n",
      "Gradient Descent(22/49): loss=6.834140244496906e+48\n",
      "Gradient Descent(23/49): loss=1.7506358015087837e+51\n",
      "Gradient Descent(24/49): loss=4.484434910505744e+53\n",
      "Gradient Descent(25/49): loss=1.1487344454686157e+56\n",
      "Gradient Descent(26/49): loss=2.9426022509900857e+58\n",
      "Gradient Descent(27/49): loss=7.537780417128145e+60\n",
      "Gradient Descent(28/49): loss=1.930880519027866e+63\n",
      "Gradient Descent(29/49): loss=4.9461504215346135e+65\n",
      "Gradient Descent(30/49): loss=1.2670076553863754e+68\n",
      "Gradient Descent(31/49): loss=3.245571327184924e+70\n",
      "Gradient Descent(32/49): loss=8.313867082857507e+72\n",
      "Gradient Descent(33/49): loss=2.1296831560122697e+75\n",
      "Gradient Descent(34/49): loss=5.455403965206819e+77\n",
      "Gradient Descent(35/49): loss=1.3974582246929892e+80\n",
      "Gradient Descent(36/49): loss=3.579733970604435e+82\n",
      "Gradient Descent(37/49): loss=9.169859301600567e+84\n",
      "Gradient Descent(38/49): loss=2.3489544279446183e+87\n",
      "Gradient Descent(39/49): loss=6.017090037136798e+89\n",
      "Gradient Descent(40/49): loss=1.5413399291313828e+92\n",
      "Gradient Descent(41/49): loss=3.9483018576621026e+94\n",
      "Gradient Descent(42/49): loss=1.0113984115109191e+97\n",
      "Gradient Descent(43/49): loss=2.590801776773242e+99\n",
      "Gradient Descent(44/49): loss=6.636607068132578e+101\n",
      "Gradient Descent(45/49): loss=1.7000356326620287e+104\n",
      "Gradient Descent(46/49): loss=4.3548173376096255e+106\n",
      "Gradient Descent(47/49): loss=1.1155315617855546e+109\n",
      "Gradient Descent(48/49): loss=2.857549625773202e+111\n",
      "Gradient Descent(49/49): loss=7.319909309142621e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.315787220459028\n",
      "Gradient Descent(2/49): loss=296.00310634165436\n",
      "Gradient Descent(3/49): loss=19808.26104350578\n",
      "Gradient Descent(4/49): loss=1684215.0593869977\n",
      "Gradient Descent(5/49): loss=224853278.67107105\n",
      "Gradient Descent(6/49): loss=45439553592.57763\n",
      "Gradient Descent(7/49): loss=11050424632112.467\n",
      "Gradient Descent(8/49): loss=2837050254891013.5\n",
      "Gradient Descent(9/49): loss=7.383506965835519e+17\n",
      "Gradient Descent(10/49): loss=1.9278770597456495e+20\n",
      "Gradient Descent(11/49): loss=5.037722471151537e+22\n",
      "Gradient Descent(12/49): loss=1.3166473241632481e+25\n",
      "Gradient Descent(13/49): loss=3.4413095840270626e+27\n",
      "Gradient Descent(14/49): loss=8.994614400245072e+29\n",
      "Gradient Descent(15/49): loss=2.350944782486653e+32\n",
      "Gradient Descent(16/49): loss=6.144726571556701e+34\n",
      "Gradient Descent(17/49): loss=1.6060636631171834e+37\n",
      "Gradient Descent(18/49): loss=4.197811806729313e+39\n",
      "Gradient Descent(19/49): loss=1.0971933745460632e+42\n",
      "Gradient Descent(20/49): loss=2.8677638703449992e+44\n",
      "Gradient Descent(21/49): loss=7.49555165978299e+46\n",
      "Gradient Descent(22/49): loss=1.9591325238222716e+49\n",
      "Gradient Descent(23/49): loss=5.120637439652574e+51\n",
      "Gradient Descent(24/49): loss=1.3383947981920472e+54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(25/49): loss=3.498198528876114e+56\n",
      "Gradient Descent(26/49): loss=9.143335706299578e+58\n",
      "Gradient Descent(27/49): loss=2.38981827783675e+61\n",
      "Gradient Descent(28/49): loss=6.2463323939291866e+63\n",
      "Gradient Descent(29/49): loss=1.6326207200476745e+66\n",
      "Gradient Descent(30/49): loss=4.26722474474709e+68\n",
      "Gradient Descent(31/49): loss=1.1153360237673462e+71\n",
      "Gradient Descent(32/49): loss=2.9151838028791236e+73\n",
      "Gradient Descent(33/49): loss=7.619494415560374e+75\n",
      "Gradient Descent(34/49): loss=1.9915277757587364e+78\n",
      "Gradient Descent(35/49): loss=5.205309782127833e+80\n",
      "Gradient Descent(36/49): loss=1.3605258363817143e+83\n",
      "Gradient Descent(37/49): loss=3.556043019413756e+85\n",
      "Gradient Descent(38/49): loss=9.294525409051751e+87\n",
      "Gradient Descent(39/49): loss=2.4293351376202103e+90\n",
      "Gradient Descent(40/49): loss=6.349618674589506e+92\n",
      "Gradient Descent(41/49): loss=1.6596169333882617e+95\n",
      "Gradient Descent(42/49): loss=4.337785474601119e+97\n",
      "Gradient Descent(43/49): loss=1.1337786717592258e+100\n",
      "Gradient Descent(44/49): loss=2.963387848621768e+102\n",
      "Gradient Descent(45/49): loss=7.745486628120489e+104\n",
      "Gradient Descent(46/49): loss=2.0244586996702363e+107\n",
      "Gradient Descent(47/49): loss=5.291382224831148e+109\n",
      "Gradient Descent(48/49): loss=1.3830228225362058e+112\n",
      "Gradient Descent(49/49): loss=3.6148439980010724e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.360658548275764\n",
      "Gradient Descent(2/49): loss=301.81995891393234\n",
      "Gradient Descent(3/49): loss=20173.432954477343\n",
      "Gradient Descent(4/49): loss=1654247.7242905982\n",
      "Gradient Descent(5/49): loss=204042530.82210386\n",
      "Gradient Descent(6/49): loss=38353472199.46589\n",
      "Gradient Descent(7/49): loss=8910530233042.953\n",
      "Gradient Descent(8/49): loss=2214392126429383.5\n",
      "Gradient Descent(9/49): loss=5.602052587250542e+17\n",
      "Gradient Descent(10/49): loss=1.4235784379033033e+20\n",
      "Gradient Descent(11/49): loss=3.6215612021425595e+22\n",
      "Gradient Descent(12/49): loss=9.215706080178093e+24\n",
      "Gradient Descent(13/49): loss=2.3452575834803962e+27\n",
      "Gradient Descent(14/49): loss=5.9684238775207066e+29\n",
      "Gradient Descent(15/49): loss=1.5189047358489937e+32\n",
      "Gradient Descent(16/49): loss=3.8654659164510866e+34\n",
      "Gradient Descent(17/49): loss=9.837240007939842e+36\n",
      "Gradient Descent(18/49): loss=2.5034833818316926e+39\n",
      "Gradient Descent(19/49): loss=6.371125571525911e+41\n",
      "Gradient Descent(20/49): loss=1.6213904790459588e+44\n",
      "Gradient Descent(21/49): loss=4.1262835874270253e+46\n",
      "Gradient Descent(22/49): loss=1.0500996809655639e+49\n",
      "Gradient Descent(23/49): loss=2.672403184755675e+51\n",
      "Gradient Descent(24/49): loss=6.801010334011265e+53\n",
      "Gradient Descent(25/49): loss=1.7307920386912684e+56\n",
      "Gradient Descent(26/49): loss=4.404700087306885e+58\n",
      "Gradient Descent(27/49): loss=1.1209540155844294e+61\n",
      "Gradient Descent(28/49): loss=2.852720685060726e+63\n",
      "Gradient Descent(29/49): loss=7.259901114436539e+65\n",
      "Gradient Descent(30/49): loss=1.8475753503457798e+68\n",
      "Gradient Descent(31/49): loss=4.7019024383367887e+70\n",
      "Gradient Descent(32/49): loss=1.1965891694485016e+73\n",
      "Gradient Descent(33/49): loss=3.045204912733008e+75\n",
      "Gradient Descent(34/49): loss=7.749755051524616e+77\n",
      "Gradient Descent(35/49): loss=1.9722384890260167e+80\n",
      "Gradient Descent(36/49): loss=5.0191581949812565e+82\n",
      "Gradient Descent(37/49): loss=1.2773277231136625e+85\n",
      "Gradient Descent(38/49): loss=3.2506768044612105e+87\n",
      "Gradient Descent(39/49): loss=8.272661350607697e+89\n",
      "Gradient Descent(40/49): loss=2.1053131374953907e+92\n",
      "Gradient Descent(41/49): loss=5.3578204389874736e+94\n",
      "Gradient Descent(42/49): loss=1.363514022934513e+97\n",
      "Gradient Descent(43/49): loss=3.4700126887611434e+99\n",
      "Gradient Descent(44/49): loss=8.830850183886649e+101\n",
      "Gradient Descent(45/49): loss=2.2473668532345906e+104\n",
      "Gradient Descent(46/49): loss=5.719333549824236e+106\n",
      "Gradient Descent(47/49): loss=1.4555156496619967e+109\n",
      "Gradient Descent(48/49): loss=3.7041480234634615e+111\n",
      "Gradient Descent(49/49): loss=9.426702202009783e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.467043347829239\n",
      "Gradient Descent(2/49): loss=306.80206431648986\n",
      "Gradient Descent(3/49): loss=20281.111547039978\n",
      "Gradient Descent(4/49): loss=1621384.1698096318\n",
      "Gradient Descent(5/49): loss=192646447.65037623\n",
      "Gradient Descent(6/49): loss=35313444246.815956\n",
      "Gradient Descent(7/49): loss=8132376135261.501\n",
      "Gradient Descent(8/49): loss=2016633464641258.0\n",
      "Gradient Descent(9/49): loss=5.100031039993125e+17\n",
      "Gradient Descent(10/49): loss=1.2961525306364063e+20\n",
      "Gradient Descent(11/49): loss=3.2981200661201663e+22\n",
      "Gradient Descent(12/49): loss=8.394721257756805e+24\n",
      "Gradient Descent(13/49): loss=2.13686886871393e+27\n",
      "Gradient Descent(14/49): loss=5.439478440169588e+29\n",
      "Gradient Descent(15/49): loss=1.3846453772838743e+32\n",
      "Gradient Descent(16/49): loss=3.5246851577964264e+34\n",
      "Gradient Descent(17/49): loss=8.972267526931668e+36\n",
      "Gradient Descent(18/49): loss=2.2839370385287586e+39\n",
      "Gradient Descent(19/49): loss=5.81387978405624e+41\n",
      "Gradient Descent(20/49): loss=1.4799531556943496e+44\n",
      "Gradient Descent(21/49): loss=3.7672972722272574e+46\n",
      "Gradient Descent(22/49): loss=9.58984998009387e+48\n",
      "Gradient Descent(23/49): loss=2.4411458932108958e+51\n",
      "Gradient Descent(24/49): loss=6.214063081690438e+53\n",
      "Gradient Descent(25/49): loss=1.581821884990694e+56\n",
      "Gradient Descent(26/49): loss=4.026609390577744e+58\n",
      "Gradient Descent(27/49): loss=1.0249942384876885e+61\n",
      "Gradient Descent(28/49): loss=2.609175827661457e+63\n",
      "Gradient Descent(29/49): loss=6.641791967238192e+65\n",
      "Gradient Descent(30/49): loss=1.6907024842249721e+68\n",
      "Gradient Descent(31/49): loss=4.3037705852041677e+70\n",
      "Gradient Descent(32/49): loss=1.0955470535408382e+73\n",
      "Gradient Descent(33/49): loss=2.7887716660555086e+75\n",
      "Gradient Descent(34/49): loss=7.098962459218543e+77\n",
      "Gradient Descent(35/49): loss=1.807077596591972e+80\n",
      "Gradient Descent(36/49): loss=4.60000945048544e+82\n",
      "Gradient Descent(37/49): loss=1.1709561882932976e+85\n",
      "Gradient Descent(38/49): loss=2.9807295173223264e+87\n",
      "Gradient Descent(39/49): loss=7.587601094099325e+89\n",
      "Gradient Descent(40/49): loss=1.931463087428867e+92\n",
      "Gradient Descent(41/49): loss=4.916639148309077e+94\n",
      "Gradient Descent(42/49): loss=1.2515559148927319e+97\n",
      "Gradient Descent(43/49): loss=3.1859002884962146e+99\n",
      "Gradient Descent(44/49): loss=8.109873899729138e+101\n",
      "Gradient Descent(45/49): loss=2.0644103303230835e+104\n",
      "Gradient Descent(46/49): loss=5.255063228649073e+106\n",
      "Gradient Descent(47/49): loss=1.3377035142416235e+109\n",
      "Gradient Descent(48/49): loss=3.405193456586477e+111\n",
      "Gradient Descent(49/49): loss=8.668095996856991e+113\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.267383276939937\n",
      "Gradient Descent(2/49): loss=291.27941587920435\n",
      "Gradient Descent(3/49): loss=18772.11138756147\n",
      "Gradient Descent(4/49): loss=1402186.7525578449\n",
      "Gradient Descent(5/49): loss=148502652.96518132\n",
      "Gradient Descent(6/49): loss=25053450827.1127\n",
      "Gradient Descent(7/49): loss=5640202311122.79\n",
      "Gradient Descent(8/49): loss=1404469627181580.0\n",
      "Gradient Descent(9/49): loss=3.5935236375005075e+17\n",
      "Gradient Descent(10/49): loss=9.25669000911971e+19\n",
      "Gradient Descent(11/49): loss=2.3883744135129403e+22\n",
      "Gradient Descent(12/49): loss=6.164830509928214e+24\n",
      "Gradient Descent(13/49): loss=1.591407574769132e+27\n",
      "Gradient Descent(14/49): loss=4.108201453987559e+29\n",
      "Gradient Descent(15/49): loss=1.0605336787505454e+32\n",
      "Gradient Descent(16/49): loss=2.7377751806849267e+34\n",
      "Gradient Descent(17/49): loss=7.067588258553832e+36\n",
      "Gradient Descent(18/49): loss=1.8245036357801297e+39\n",
      "Gradient Descent(19/49): loss=4.7099710088355984e+41\n",
      "Gradient Descent(20/49): loss=1.2158828609847186e+44\n",
      "Gradient Descent(21/49): loss=3.138811535134212e+46\n",
      "Gradient Descent(22/49): loss=8.102867613198964e+48\n",
      "Gradient Descent(23/49): loss=2.091761892242493e+51\n",
      "Gradient Descent(24/49): loss=5.399900409058487e+53\n",
      "Gradient Descent(25/49): loss=1.3939887009127769e+56\n",
      "Gradient Descent(26/49): loss=3.598593216671979e+58\n",
      "Gradient Descent(27/49): loss=9.289797779996912e+60\n",
      "Gradient Descent(28/49): loss=2.398168884257859e+63\n",
      "Gradient Descent(29/49): loss=6.190892561522037e+65\n",
      "Gradient Descent(30/49): loss=1.598183971107982e+68\n",
      "Gradient Descent(31/49): loss=4.125724974426875e+70\n",
      "Gradient Descent(32/49): loss=1.0650592717938948e+73\n",
      "Gradient Descent(33/49): loss=2.7494592089036026e+75\n",
      "Gradient Descent(34/49): loss=7.097751403724415e+77\n",
      "Gradient Descent(35/49): loss=1.8322903218906186e+80\n",
      "Gradient Descent(36/49): loss=4.730072431013069e+82\n",
      "Gradient Descent(37/49): loss=1.2210720613065158e+85\n",
      "Gradient Descent(38/49): loss=3.152207499249715e+87\n",
      "Gradient Descent(39/49): loss=8.137449404660348e+89\n",
      "Gradient Descent(40/49): loss=2.100689209995472e+92\n",
      "Gradient Descent(41/49): loss=5.42294635277733e+94\n",
      "Gradient Descent(42/49): loss=1.3999380301079856e+97\n",
      "Gradient Descent(43/49): loss=3.6139514585809943e+99\n",
      "Gradient Descent(44/49): loss=9.329445206922731e+101\n",
      "Gradient Descent(45/49): loss=2.408403899900453e+104\n",
      "Gradient Descent(46/49): loss=6.2173143379967895e+106\n",
      "Gradient Descent(47/49): loss=1.6050047742846828e+109\n",
      "Gradient Descent(48/49): loss=4.143332933535805e+111\n",
      "Gradient Descent(49/49): loss=1.0696047808190121e+114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.356816039185449\n",
      "Gradient Descent(2/49): loss=300.8820772862828\n",
      "Gradient Descent(3/49): loss=20297.262439124588\n",
      "Gradient Descent(4/49): loss=1739060.0532245962\n",
      "Gradient Descent(5/49): loss=233852222.11689776\n",
      "Gradient Descent(6/49): loss=47602619178.24632\n",
      "Gradient Descent(7/49): loss=11663913095928.975\n",
      "Gradient Descent(8/49): loss=3017557573387857.5\n",
      "Gradient Descent(9/49): loss=7.913924228033846e+17\n",
      "Gradient Descent(10/49): loss=2.0823519210705974e+20\n",
      "Gradient Descent(11/49): loss=5.4834761935802455e+22\n",
      "Gradient Descent(12/49): loss=1.4442370175959669e+25\n",
      "Gradient Descent(13/49): loss=3.8039967445909517e+27\n",
      "Gradient Descent(14/49): loss=1.0019506882981667e+30\n",
      "Gradient Descent(15/49): loss=2.6390865825935915e+32\n",
      "Gradient Descent(16/49): loss=6.951222431299928e+34\n",
      "Gradient Descent(17/49): loss=1.8309175748657222e+37\n",
      "Gradient Descent(18/49): loss=4.822546408967455e+39\n",
      "Gradient Descent(19/49): loss=1.2702349013112123e+42\n",
      "Gradient Descent(20/49): loss=3.34573598821466e+44\n",
      "Gradient Descent(21/49): loss=8.812503337971536e+46\n",
      "Gradient Descent(22/49): loss=2.3211698521196928e+49\n",
      "Gradient Descent(23/49): loss=6.113846742399666e+51\n",
      "Gradient Descent(24/49): loss=1.610357034218706e+54\n",
      "Gradient Descent(25/49): loss=4.241600888810456e+56\n",
      "Gradient Descent(26/49): loss=1.1172167238485818e+59\n",
      "Gradient Descent(27/49): loss=2.942693668656574e+61\n",
      "Gradient Descent(28/49): loss=7.750909776682817e+63\n",
      "Gradient Descent(29/49): loss=2.0415513516126707e+66\n",
      "Gradient Descent(30/49): loss=5.377345423126856e+68\n",
      "Gradient Descent(31/49): loss=1.4163662244783742e+71\n",
      "Gradient Descent(32/49): loss=3.730638677618405e+73\n",
      "Gradient Descent(33/49): loss=9.826318011831835e+75\n",
      "Gradient Descent(34/49): loss=2.5882036298217e+78\n",
      "Gradient Descent(35/49): loss=6.817200523488297e+80\n",
      "Gradient Descent(36/49): loss=1.7956169461307168e+83\n",
      "Gradient Descent(37/49): loss=4.729566346365903e+85\n",
      "Gradient Descent(38/49): loss=1.2457444151927091e+88\n",
      "Gradient Descent(39/49): loss=3.2812292593721955e+90\n",
      "Gradient Descent(40/49): loss=8.642595801559155e+92\n",
      "Gradient Descent(41/49): loss=2.2764170463182098e+95\n",
      "Gradient Descent(42/49): loss=5.995970062412517e+97\n",
      "Gradient Descent(43/49): loss=1.5793088989336666e+100\n",
      "Gradient Descent(44/49): loss=4.1598216340118133e+102\n",
      "Gradient Descent(45/49): loss=1.0956764720617032e+105\n",
      "Gradient Descent(46/49): loss=2.8859577093736347e+107\n",
      "Gradient Descent(47/49): loss=7.601470062254201e+109\n",
      "Gradient Descent(48/49): loss=2.0021896689500497e+112\n",
      "Gradient Descent(49/49): loss=5.273668695159537e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.4020441812438476\n",
      "Gradient Descent(2/49): loss=306.7923303443785\n",
      "Gradient Descent(3/49): loss=20671.61986913315\n",
      "Gradient Descent(4/49): loss=1708243.1357979672\n",
      "Gradient Descent(5/49): loss=212235746.49137563\n",
      "Gradient Descent(6/49): loss=40183139558.57129\n",
      "Gradient Descent(7/49): loss=9405837743373.775\n",
      "Gradient Descent(8/49): loss=2355414567546591.0\n",
      "Gradient Descent(9/49): loss=6.004833157562819e+17\n",
      "Gradient Descent(10/49): loss=1.5377390494488658e+20\n",
      "Gradient Descent(11/49): loss=3.9422702870280725e+22\n",
      "Gradient Descent(12/49): loss=1.010948472014176e+25\n",
      "Gradient Descent(13/49): loss=2.592632318760714e+27\n",
      "Gradient Descent(14/49): loss=6.649056982325416e+29\n",
      "Gradient Descent(15/49): loss=1.7052221142504734e+32\n",
      "Gradient Descent(16/49): loss=4.373229763367796e+34\n",
      "Gradient Descent(17/49): loss=1.1215631764763508e+37\n",
      "Gradient Descent(18/49): loss=2.8763730784810296e+39\n",
      "Gradient Descent(19/49): loss=7.376777683801893e+41\n",
      "Gradient Descent(20/49): loss=1.8918564356338938e+44\n",
      "Gradient Descent(21/49): loss=4.851875615254666e+46\n",
      "Gradient Descent(22/49): loss=1.2443173037764245e+49\n",
      "Gradient Descent(23/49): loss=3.191189707533391e+51\n",
      "Gradient Descent(24/49): loss=8.184159875217533e+53\n",
      "Gradient Descent(25/49): loss=2.0989185539557763e+56\n",
      "Gradient Descent(26/49): loss=5.382909380208514e+58\n",
      "Gradient Descent(27/49): loss=1.3805068015113533e+61\n",
      "Gradient Descent(28/49): loss=3.540462776554043e+63\n",
      "Gradient Descent(29/49): loss=9.079909391566986e+65\n",
      "Gradient Descent(30/49): loss=2.3286434503714603e+68\n",
      "Gradient Descent(31/49): loss=5.972064351208343e+70\n",
      "Gradient Descent(32/49): loss=1.531602126950096e+73\n",
      "Gradient Descent(33/49): loss=3.927963493567238e+75\n",
      "Gradient Descent(34/49): loss=1.0073697950211564e+78\n",
      "Gradient Descent(35/49): loss=2.583511546334139e+80\n",
      "Gradient Descent(36/49): loss=6.625701845568495e+82\n",
      "Gradient Descent(37/49): loss=1.699234710549743e+85\n",
      "Gradient Descent(38/49): loss=4.357875842946844e+87\n",
      "Gradient Descent(39/49): loss=1.1176255843076024e+90\n",
      "Gradient Descent(40/49): loss=2.8662747441980046e+92\n",
      "Gradient Descent(41/49): loss=7.350879422035677e+94\n",
      "Gradient Descent(42/49): loss=1.885214541512058e+97\n",
      "Gradient Descent(43/49): loss=4.834841742709965e+99\n",
      "Gradient Descent(44/49): loss=1.239948778365646e+102\n",
      "Gradient Descent(45/49): loss=3.179986139750464e+104\n",
      "Gradient Descent(46/49): loss=8.155427083313816e+106\n",
      "Gradient Descent(47/49): loss=2.0915497108571457e+109\n",
      "Gradient Descent(48/49): loss=5.364011164954371e+111\n",
      "Gradient Descent(49/49): loss=1.375660144647649e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.50932066412203\n",
      "Gradient Descent(2/49): loss=311.8583113150183\n",
      "Gradient Descent(3/49): loss=20782.3241078435\n",
      "Gradient Descent(4/49): loss=1674384.7907658871\n",
      "Gradient Descent(5/49): loss=200393724.64168468\n",
      "Gradient Descent(6/49): loss=36999220048.83947\n",
      "Gradient Descent(7/49): loss=8584514166158.514\n",
      "Gradient Descent(8/49): loss=2145067095488658.0\n",
      "Gradient Descent(9/49): loss=5.4667184109841286e+17\n",
      "Gradient Descent(10/49): loss=1.400093999701934e+20\n",
      "Gradient Descent(11/49): loss=3.5901845823915735e+22\n",
      "Gradient Descent(12/49): loss=9.208871021729575e+24\n",
      "Gradient Descent(13/49): loss=2.3622612669249486e+27\n",
      "Gradient Descent(14/49): loss=6.059786240840534e+29\n",
      "Gradient Descent(15/49): loss=1.5544924030383608e+32\n",
      "Gradient Descent(16/49): loss=3.987680690735342e+34\n",
      "Gradient Descent(17/49): loss=1.0229449499800916e+37\n",
      "Gradient Descent(18/49): loss=2.6241229435998325e+39\n",
      "Gradient Descent(19/49): loss=6.7315658911754275e+41\n",
      "Gradient Descent(20/49): loss=1.726823799798611e+44\n",
      "Gradient Descent(21/49): loss=4.4297574838393657e+46\n",
      "Gradient Descent(22/49): loss=1.1363493700158962e+49\n",
      "Gradient Descent(23/49): loss=2.915035180868834e+51\n",
      "Gradient Descent(24/49): loss=7.477832372719884e+53\n",
      "Gradient Descent(25/49): loss=1.9182607936151753e+56\n",
      "Gradient Descent(26/49): loss=4.9208437537973264e+58\n",
      "Gradient Descent(27/49): loss=1.2623259219960578e+61\n",
      "Gradient Descent(28/49): loss=3.238198189311804e+63\n",
      "Gradient Descent(29/49): loss=8.306830534448039e+65\n",
      "Gradient Descent(30/49): loss=2.130920638390736e+68\n",
      "Gradient Descent(31/49): loss=5.4663722201733103e+70\n",
      "Gradient Descent(32/49): loss=1.4022683299951412e+73\n",
      "Gradient Descent(33/49): loss=3.597187293705758e+75\n",
      "Gradient Descent(34/49): loss=9.227732060413127e+77\n",
      "Gradient Descent(35/49): loss=2.367156114661287e+80\n",
      "Gradient Descent(36/49): loss=6.07237838560248e+82\n",
      "Gradient Descent(37/49): loss=1.557724859359676e+85\n",
      "Gradient Descent(38/49): loss=3.9959742021682865e+87\n",
      "Gradient Descent(39/49): loss=1.0250725427183772e+90\n",
      "Gradient Descent(40/49): loss=2.6295808347934873e+92\n",
      "Gradient Descent(41/49): loss=6.745566853616328e+94\n",
      "Gradient Descent(42/49): loss=1.730415417337088e+97\n",
      "Gradient Descent(43/49): loss=4.438970929407607e+99\n",
      "Gradient Descent(44/49): loss=1.138712861357263e+102\n",
      "Gradient Descent(45/49): loss=2.9210981582018293e+104\n",
      "Gradient Descent(46/49): loss=7.49338550517432e+106\n",
      "Gradient Descent(47/49): loss=1.922250581395136e+109\n",
      "Gradient Descent(48/49): loss=4.931078609424809e+111\n",
      "Gradient Descent(49/49): loss=1.264951431809666e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.308012667840252\n",
      "Gradient Descent(2/49): loss=296.0829273409465\n",
      "Gradient Descent(3/49): loss=19236.840276531544\n",
      "Gradient Descent(4/49): loss=1448213.1511679199\n",
      "Gradient Descent(5/49): loss=154504424.15548888\n",
      "Gradient Descent(6/49): loss=26252139038.4911\n",
      "Gradient Descent(7/49): loss=5953898039809.735\n",
      "Gradient Descent(8/49): loss=1493895869664335.0\n",
      "Gradient Descent(9/49): loss=3.8517964529559405e+17\n",
      "Gradient Descent(10/49): loss=9.998696016560731e+19\n",
      "Gradient Descent(11/49): loss=2.5997863106281625e+22\n",
      "Gradient Descent(12/49): loss=6.762460393796477e+24\n",
      "Gradient Descent(13/49): loss=1.759193318027372e+27\n",
      "Gradient Descent(14/49): loss=4.57648963958866e+29\n",
      "Gradient Descent(15/49): loss=1.1905669871728971e+32\n",
      "Gradient Descent(16/49): loss=3.097246541745206e+34\n",
      "Gradient Descent(17/49): loss=8.057454458309492e+36\n",
      "Gradient Descent(18/49): loss=2.0961385089148848e+39\n",
      "Gradient Descent(19/49): loss=5.453082924811868e+41\n",
      "Gradient Descent(20/49): loss=1.4186139606454528e+44\n",
      "Gradient Descent(21/49): loss=3.690509752605393e+46\n",
      "Gradient Descent(22/49): loss=9.6008234907968e+48\n",
      "Gradient Descent(23/49): loss=2.4976444417291727e+51\n",
      "Gradient Descent(24/49): loss=6.497596548231839e+53\n",
      "Gradient Descent(25/49): loss=1.6903431168339504e+56\n",
      "Gradient Descent(26/49): loss=4.3974103830870826e+58\n",
      "Gradient Descent(27/49): loss=1.1439818273999571e+61\n",
      "Gradient Descent(28/49): loss=2.9760570595247918e+63\n",
      "Gradient Descent(29/49): loss=7.7421820953899865e+65\n",
      "Gradient Descent(30/49): loss=2.0141207779043093e+68\n",
      "Gradient Descent(31/49): loss=5.2397146670077604e+70\n",
      "Gradient Descent(32/49): loss=1.363106428017805e+73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(33/49): loss=3.546107473757778e+75\n",
      "Gradient Descent(34/49): loss=9.225162435575236e+77\n",
      "Gradient Descent(35/49): loss=2.399916601302649e+80\n",
      "Gradient Descent(36/49): loss=6.243358567863438e+82\n",
      "Gradient Descent(37/49): loss=1.6242033654734823e+85\n",
      "Gradient Descent(38/49): loss=4.2253484943091465e+87\n",
      "Gradient Descent(39/49): loss=1.0992201024750318e+90\n",
      "Gradient Descent(40/49): loss=2.859609888539493e+92\n",
      "Gradient Descent(41/49): loss=7.439245967409597e+94\n",
      "Gradient Descent(42/49): loss=1.9353122530949417e+97\n",
      "Gradient Descent(43/49): loss=5.034695093276642e+99\n",
      "Gradient Descent(44/49): loss=1.3097707949571265e+102\n",
      "Gradient Descent(45/49): loss=3.4073553681800825e+104\n",
      "Gradient Descent(46/49): loss=8.864200247681783e+106\n",
      "Gradient Descent(47/49): loss=2.306012656172381e+109\n",
      "Gradient Descent(48/49): loss=5.999068412085945e+111\n",
      "Gradient Descent(49/49): loss=1.5606515305351217e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.398015269978515\n",
      "Gradient Descent(2/49): loss=305.82121107254295\n",
      "Gradient Descent(3/49): loss=20796.286966462187\n",
      "Gradient Descent(4/49): loss=1795462.7621560139\n",
      "Gradient Descent(5/49): loss=243174266.31047368\n",
      "Gradient Descent(6/49): loss=49859731505.22663\n",
      "Gradient Descent(7/49): loss=12308906418545.824\n",
      "Gradient Descent(8/49): loss=3208789700894543.5\n",
      "Gradient Descent(9/49): loss=8.480186803896012e+17\n",
      "Gradient Descent(10/49): loss=2.2485389179426315e+20\n",
      "Gradient Descent(11/49): loss=5.966729167529191e+22\n",
      "Gradient Descent(12/49): loss=1.5836283669270484e+25\n",
      "Gradient Descent(13/49): loss=4.203291089083341e+27\n",
      "Gradient Descent(14/49): loss=1.1156558140376625e+30\n",
      "Gradient Descent(15/49): loss=2.9612296187414322e+32\n",
      "Gradient Descent(16/49): loss=7.859848844598535e+34\n",
      "Gradient Descent(17/49): loss=2.0862020351064286e+37\n",
      "Gradient Descent(18/49): loss=5.537306331292826e+39\n",
      "Gradient Descent(19/49): loss=1.4697407602160487e+42\n",
      "Gradient Descent(20/49): loss=3.901062677556232e+44\n",
      "Gradient Descent(21/49): loss=1.0354404282025245e+47\n",
      "Gradient Descent(22/49): loss=2.748320058118323e+49\n",
      "Gradient Descent(23/49): loss=7.294734623370673e+51\n",
      "Gradient Descent(24/49): loss=1.9362065589316515e+54\n",
      "Gradient Descent(25/49): loss=5.1391805629840575e+56\n",
      "Gradient Descent(26/49): loss=1.3640681433046639e+59\n",
      "Gradient Descent(27/49): loss=3.6205809015171197e+61\n",
      "Gradient Descent(28/49): loss=9.60993490594451e+63\n",
      "Gradient Descent(29/49): loss=2.550719108577139e+66\n",
      "Gradient Descent(30/49): loss=6.77025186386652e+68\n",
      "Gradient Descent(31/49): loss=1.7969956059080523e+71\n",
      "Gradient Descent(32/49): loss=4.769679581475091e+73\n",
      "Gradient Descent(33/49): loss=1.2659932631523756e+76\n",
      "Gradient Descent(34/49): loss=3.3602654328648546e+78\n",
      "Gradient Descent(35/49): loss=8.918991994625956e+80\n",
      "Gradient Descent(36/49): loss=2.3673254327525433e+83\n",
      "Gradient Descent(37/49): loss=6.28347879214798e+85\n",
      "Gradient Descent(38/49): loss=1.6677937551436606e+88\n",
      "Gradient Descent(39/49): loss=4.426745281884298e+90\n",
      "Gradient Descent(40/49): loss=1.1749698504535627e+93\n",
      "Gradient Descent(41/49): loss=3.1186663373755306e+95\n",
      "Gradient Descent(42/49): loss=8.277727058379459e+97\n",
      "Gradient Descent(43/49): loss=2.1971175445042846e+100\n",
      "Gradient Descent(44/49): loss=5.831704126414454e+102\n",
      "Gradient Descent(45/49): loss=1.547881364067494e+105\n",
      "Gradient Descent(46/49): loss=4.1084675513204376e+107\n",
      "Gradient Descent(47/49): loss=1.0904909130695357e+110\n",
      "Gradient Descent(48/49): loss=2.894437930037993e+112\n",
      "Gradient Descent(49/49): loss=7.682568309772214e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.443601629508845\n",
      "Gradient Descent(2/49): loss=311.8259832749623\n",
      "Gradient Descent(3/49): loss=21180.021067674777\n",
      "Gradient Descent(4/49): loss=1763775.5758942266\n",
      "Gradient Descent(5/49): loss=220724114.3155017\n",
      "Gradient Descent(6/49): loss=42092532608.61004\n",
      "Gradient Descent(7/49): loss=9926613447405.994\n",
      "Gradient Descent(8/49): loss=2504823696460837.5\n",
      "Gradient Descent(9/49): loss=6.434856360447844e+17\n",
      "Gradient Descent(10/49): loss=1.6605622918918855e+20\n",
      "Gradient Descent(11/49): loss=4.289981142481837e+22\n",
      "Gradient Descent(12/49): loss=1.1086002530227703e+25\n",
      "Gradient Descent(13/49): loss=2.864995798146776e+27\n",
      "Gradient Descent(14/49): loss=7.404236070427439e+29\n",
      "Gradient Descent(15/49): loss=1.913543377668809e+32\n",
      "Gradient Descent(16/49): loss=4.945347414826418e+34\n",
      "Gradient Descent(17/49): loss=1.2780722637093655e+37\n",
      "Gradient Descent(18/49): loss=3.303041575886535e+39\n",
      "Gradient Descent(19/49): loss=8.536359114705312e+41\n",
      "Gradient Descent(20/49): loss=2.2061310926157293e+44\n",
      "Gradient Descent(21/49): loss=5.701510839563383e+46\n",
      "Gradient Descent(22/49): loss=1.4734947515062604e+49\n",
      "Gradient Descent(23/49): loss=3.808090248240501e+51\n",
      "Gradient Descent(24/49): loss=9.841603659680797e+53\n",
      "Gradient Descent(25/49): loss=2.5434576462394594e+56\n",
      "Gradient Descent(26/49): loss=6.573295391605981e+58\n",
      "Gradient Descent(27/49): loss=1.6987981840075972e+61\n",
      "Gradient Descent(28/49): loss=4.390362973300875e+63\n",
      "Gradient Descent(29/49): loss=1.1346425501739227e+66\n",
      "Gradient Descent(30/49): loss=2.9323628239722476e+68\n",
      "Gradient Descent(31/49): loss=7.578379402479196e+70\n",
      "Gradient Descent(32/49): loss=1.958551441806972e+73\n",
      "Gradient Descent(33/49): loss=5.0616676026397785e+75\n",
      "Gradient Descent(34/49): loss=1.3081340817872767e+78\n",
      "Gradient Descent(35/49): loss=3.380733209428925e+80\n",
      "Gradient Descent(36/49): loss=8.73714490927387e+82\n",
      "Gradient Descent(37/49): loss=2.25802204541739e+85\n",
      "Gradient Descent(38/49): loss=5.835617482066674e+87\n",
      "Gradient Descent(39/49): loss=1.5081531850459817e+90\n",
      "Gradient Descent(40/49): loss=3.897661278440718e+92\n",
      "Gradient Descent(41/49): loss=1.0073090447369384e+95\n",
      "Gradient Descent(42/49): loss=2.6032829410326498e+97\n",
      "Gradient Descent(43/49): loss=6.727907494210558e+99\n",
      "Gradient Descent(44/49): loss=1.7387560352044557e+102\n",
      "Gradient Descent(45/49): loss=4.493629783943368e+104\n",
      "Gradient Descent(46/49): loss=1.161330757524493e+107\n",
      "Gradient Descent(47/49): loss=3.00133565339884e+109\n",
      "Gradient Descent(48/49): loss=7.756632334068733e+111\n",
      "Gradient Descent(49/49): loss=2.0046190134644388e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.551773453896576\n",
      "Gradient Descent(2/49): loss=316.97689347534805\n",
      "Gradient Descent(3/49): loss=21293.820223222127\n",
      "Gradient Descent(4/49): loss=1728896.2927629382\n",
      "Gradient Descent(5/49): loss=208420516.0856508\n",
      "Gradient Descent(6/49): loss=38758503250.29868\n",
      "Gradient Descent(7/49): loss=9059905315171.807\n",
      "Gradient Descent(8/49): loss=2281139090608325.0\n",
      "Gradient Descent(9/49): loss=5.858207321311814e+17\n",
      "Gradient Descent(10/49): loss=1.5119226226626685e+20\n",
      "Gradient Descent(11/49): loss=3.906838995620201e+22\n",
      "Gradient Descent(12/49): loss=1.0098387949625642e+25\n",
      "Gradient Descent(13/49): loss=2.61042166946706e+27\n",
      "Gradient Descent(14/49): loss=6.748032393705992e+29\n",
      "Gradient Descent(15/49): loss=1.744398024248976e+32\n",
      "Gradient Descent(16/49): loss=4.509355643331423e+34\n",
      "Gradient Descent(17/49): loss=1.165691172402384e+37\n",
      "Gradient Descent(18/49): loss=3.013370659864368e+39\n",
      "Gradient Descent(19/49): loss=7.789715746157918e+41\n",
      "Gradient Descent(20/49): loss=2.0136809737851475e+44\n",
      "Gradient Descent(21/49): loss=5.20546731136509e+46\n",
      "Gradient Descent(22/49): loss=1.345639666309983e+49\n",
      "Gradient Descent(23/49): loss=3.478546695894471e+51\n",
      "Gradient Descent(24/49): loss=8.992219402148327e+53\n",
      "Gradient Descent(25/49): loss=2.3245342623140792e+56\n",
      "Gradient Descent(26/49): loss=6.00903880902386e+58\n",
      "Gradient Descent(27/49): loss=1.55336696876288e+61\n",
      "Gradient Descent(28/49): loss=4.015532294483073e+63\n",
      "Gradient Descent(29/49): loss=1.0380354373621403e+66\n",
      "Gradient Descent(30/49): loss=2.683374183542207e+68\n",
      "Gradient Descent(31/49): loss=6.936658181149121e+70\n",
      "Gradient Descent(32/49): loss=1.7931612749805144e+73\n",
      "Gradient Descent(33/49): loss=4.635412721976013e+75\n",
      "Gradient Descent(34/49): loss=1.1982776676509556e+78\n",
      "Gradient Descent(35/49): loss=3.097608465334156e+80\n",
      "Gradient Descent(36/49): loss=8.007474781132892e+82\n",
      "Gradient Descent(37/49): loss=2.0699727899136246e+85\n",
      "Gradient Descent(38/49): loss=5.350984508972318e+87\n",
      "Gradient Descent(39/49): loss=1.383256599061683e+90\n",
      "Gradient Descent(40/49): loss=3.575788372475026e+92\n",
      "Gradient Descent(41/49): loss=9.24359406157974e+94\n",
      "Gradient Descent(42/49): loss=2.3895158850278717e+97\n",
      "Gradient Descent(43/49): loss=6.177019595151701e+99\n",
      "Gradient Descent(44/49): loss=1.596790852823489e+102\n",
      "Gradient Descent(45/49): loss=4.1277852342608583e+104\n",
      "Gradient Descent(46/49): loss=1.0670533908716917e+107\n",
      "Gradient Descent(47/49): loss=2.758387063164739e+109\n",
      "Gradient Descent(48/49): loss=7.130570274481786e+111\n",
      "Gradient Descent(49/49): loss=1.8432885332991786e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.34881085186891\n",
      "Gradient Descent(2/49): loss=300.9456997630459\n",
      "Gradient Descent(3/49): loss=19711.12007126067\n",
      "Gradient Descent(4/49): loss=1495556.920641059\n",
      "Gradient Descent(5/49): loss=160723820.6683738\n",
      "Gradient Descent(6/49): loss=27503219066.822056\n",
      "Gradient Descent(7/49): loss=6283733577566.723\n",
      "Gradient Descent(8/49): loss=1588639436045132.0\n",
      "Gradient Descent(9/49): loss=4.127531444395615e+17\n",
      "Gradient Descent(10/49): loss=1.0796982642271917e+20\n",
      "Gradient Descent(11/49): loss=2.8289901276644808e+22\n",
      "Gradient Descent(12/49): loss=7.41539014336086e+24\n",
      "Gradient Descent(13/49): loss=1.9439206349319374e+27\n",
      "Gradient Descent(14/49): loss=5.096044270349752e+29\n",
      "Gradient Descent(15/49): loss=1.335950306978862e+32\n",
      "Gradient Descent(16/49): loss=3.502256967549234e+34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(17/49): loss=9.181335445074214e+36\n",
      "Gradient Descent(18/49): loss=2.4069315314173995e+39\n",
      "Gradient Descent(19/49): loss=6.30988764432747e+41\n",
      "Gradient Descent(20/49): loss=1.6541676300024361e+44\n",
      "Gradient Descent(21/49): loss=4.3364806244683764e+46\n",
      "Gradient Descent(22/49): loss=1.1368294162221019e+49\n",
      "Gradient Descent(23/49): loss=2.9802534211277016e+51\n",
      "Gradient Descent(24/49): loss=7.812878807974786e+53\n",
      "Gradient Descent(25/49): loss=2.0481840515843512e+56\n",
      "Gradient Descent(26/49): loss=5.369413774710628e+58\n",
      "Gradient Descent(27/49): loss=1.4076178486868554e+61\n",
      "Gradient Descent(28/49): loss=3.690138423069614e+63\n",
      "Gradient Descent(29/49): loss=9.67387675150481e+65\n",
      "Gradient Descent(30/49): loss=2.536053683467476e+68\n",
      "Gradient Descent(31/49): loss=6.648387663640972e+70\n",
      "Gradient Descent(32/49): loss=1.7429070533562222e+73\n",
      "Gradient Descent(33/49): loss=4.569115325888211e+75\n",
      "Gradient Descent(34/49): loss=1.1978157309688616e+78\n",
      "Gradient Descent(35/49): loss=3.1401320015435e+80\n",
      "Gradient Descent(36/49): loss=8.23200825651388e+82\n",
      "Gradient Descent(37/49): loss=2.15806086820557e+85\n",
      "Gradient Descent(38/49): loss=5.657461175643388e+87\n",
      "Gradient Descent(39/49): loss=1.4831308711197256e+90\n",
      "Gradient Descent(40/49): loss=3.888099471788628e+92\n",
      "Gradient Descent(41/49): loss=1.019284123666712e+95\n",
      "Gradient Descent(42/49): loss=2.672102738876311e+97\n",
      "Gradient Descent(43/49): loss=7.005046857224745e+99\n",
      "Gradient Descent(44/49): loss=1.8364069898206532e+102\n",
      "Gradient Descent(45/49): loss=4.8142299416370327e+104\n",
      "Gradient Descent(46/49): loss=1.2620737156537312e+107\n",
      "Gradient Descent(47/49): loss=3.3085874232304558e+109\n",
      "Gradient Descent(48/49): loss=8.673622310158584e+111\n",
      "Gradient Descent(49/49): loss=2.2738321330444083e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.439384912838234\n",
      "Gradient Descent(2/49): loss=310.8210004503778\n",
      "Gradient Descent(3/49): loss=21305.498379167897\n",
      "Gradient Descent(4/49): loss=1853460.9862394894\n",
      "Gradient Descent(5/49): loss=252829690.32584554\n",
      "Gradient Descent(6/49): loss=52214594514.44214\n",
      "Gradient Descent(7/49): loss=12986892327680.506\n",
      "Gradient Descent(8/49): loss=3411339049570541.5\n",
      "Gradient Descent(9/49): loss=9.084565647709896e+17\n",
      "Gradient Descent(10/49): loss=2.4272759968071005e+20\n",
      "Gradient Descent(11/49): loss=6.49047391950318e+22\n",
      "Gradient Descent(12/49): loss=1.7358613800231032e+25\n",
      "Gradient Descent(13/49): loss=4.6427255685957436e+27\n",
      "Gradient Descent(14/49): loss=1.24175403871262e+30\n",
      "Gradient Descent(15/49): loss=3.3212322088270844e+32\n",
      "Gradient Descent(16/49): loss=8.883071550926933e+34\n",
      "Gradient Descent(17/49): loss=2.3758944397096266e+37\n",
      "Gradient Descent(18/49): loss=6.354642700622228e+39\n",
      "Gradient Descent(19/49): loss=1.6996329256570397e+42\n",
      "Gradient Descent(20/49): loss=4.5458922296210506e+44\n",
      "Gradient Descent(21/49): loss=1.2158587810770085e+47\n",
      "Gradient Descent(22/49): loss=3.251974532190424e+49\n",
      "Gradient Descent(23/49): loss=8.697834421952767e+51\n",
      "Gradient Descent(24/49): loss=2.3263504336612588e+54\n",
      "Gradient Descent(25/49): loss=6.222130794470523e+56\n",
      "Gradient Descent(26/49): loss=1.664190874398104e+59\n",
      "Gradient Descent(27/49): loss=4.451097795776563e+61\n",
      "Gradient Descent(28/49): loss=1.1905047607434486e+64\n",
      "Gradient Descent(29/49): loss=3.1841618638386778e+66\n",
      "Gradient Descent(30/49): loss=8.516460504360445e+68\n",
      "Gradient Descent(31/49): loss=2.2778395893132263e+71\n",
      "Gradient Descent(32/49): loss=6.092382148647613e+73\n",
      "Gradient Descent(33/49): loss=1.62948788928332e+76\n",
      "Gradient Descent(34/49): loss=4.358280089029442e+78\n",
      "Gradient Descent(35/49): loss=1.1656794419494094e+81\n",
      "Gradient Descent(36/49): loss=3.117763277316322e+83\n",
      "Gradient Descent(37/49): loss=8.338868734895126e+85\n",
      "Gradient Descent(38/49): loss=2.2303403303173828e+88\n",
      "Gradient Descent(39/49): loss=5.96533912114977e+90\n",
      "Gradient Descent(40/49): loss=1.5955085574431977e+93\n",
      "Gradient Descent(41/49): loss=4.267397888326225e+95\n",
      "Gradient Descent(42/49): loss=1.1413717997522885e+98\n",
      "Gradient Descent(43/49): loss=3.052749285070188e+100\n",
      "Gradient Descent(44/49): loss=8.164980245279398e+102\n",
      "Gradient Descent(45/49): loss=2.1838315623181006e+105\n",
      "Gradient Descent(46/49): loss=5.840945292346569e+107\n",
      "Gradient Descent(47/49): loss=1.562237788704323e+110\n",
      "Gradient Descent(48/49): loss=4.178410833009785e+112\n",
      "Gradient Descent(49/49): loss=1.1175710391625815e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.48533089307076\n",
      "Gradient Descent(2/49): loss=316.9214193412682\n",
      "Gradient Descent(3/49): loss=21698.803464074936\n",
      "Gradient Descent(4/49): loss=1820882.422697933\n",
      "Gradient Descent(5/49): loss=229517055.54840434\n",
      "Gradient Descent(6/49): loss=44084798309.11883\n",
      "Gradient Descent(7/49): loss=10474061561986.033\n",
      "Gradient Descent(8/49): loss=2663083209972853.5\n",
      "Gradient Descent(9/49): loss=6.893849585946797e+17\n",
      "Gradient Descent(10/49): loss=1.7926684399703818e+20\n",
      "Gradient Descent(11/49): loss=4.666850319281835e+22\n",
      "Gradient Descent(12/49): loss=1.2152556728032458e+25\n",
      "Gradient Descent(13/49): loss=3.164761643795551e+27\n",
      "Gradient Descent(14/49): loss=8.241791903151215e+29\n",
      "Gradient Descent(15/49): loss=2.1463674832694426e+32\n",
      "Gradient Descent(16/49): loss=5.589680165346186e+34\n",
      "Gradient Descent(17/49): loss=1.455693512512328e+37\n",
      "Gradient Descent(18/49): loss=3.790992813669733e+39\n",
      "Gradient Descent(19/49): loss=9.872700956238855e+41\n",
      "Gradient Descent(20/49): loss=2.5711002191077612e+44\n",
      "Gradient Descent(21/49): loss=6.695793149351037e+46\n",
      "Gradient Descent(22/49): loss=1.7437533385891503e+49\n",
      "Gradient Descent(23/49): loss=4.5411732982030974e+51\n",
      "Gradient Descent(24/49): loss=1.1826360109673308e+54\n",
      "Gradient Descent(25/49): loss=3.0798823180673562e+56\n",
      "Gradient Descent(26/49): loss=8.020790002321217e+58\n",
      "Gradient Descent(27/49): loss=2.0888159227369225e+61\n",
      "Gradient Descent(28/49): loss=5.439803258552918e+63\n",
      "Gradient Descent(29/49): loss=1.4166619073350564e+66\n",
      "Gradient Descent(30/49): loss=3.689344750729195e+68\n",
      "Gradient Descent(31/49): loss=9.607983823986323e+70\n",
      "Gradient Descent(32/49): loss=2.5021612074539647e+73\n",
      "Gradient Descent(33/49): loss=6.516258585341645e+75\n",
      "Gradient Descent(34/49): loss=1.6969980121402547e+78\n",
      "Gradient Descent(35/49): loss=4.419410641078834e+80\n",
      "Gradient Descent(36/49): loss=1.1509259453903648e+83\n",
      "Gradient Descent(37/49): loss=2.9973013131210386e+85\n",
      "Gradient Descent(38/49): loss=7.805728246564251e+87\n",
      "Gradient Descent(39/49): loss=2.0328084197770303e+90\n",
      "Gradient Descent(40/49): loss=5.293945601213084e+92\n",
      "Gradient Descent(41/49): loss=1.378676896255526e+95\n",
      "Gradient Descent(42/49): loss=3.590422205761361e+97\n",
      "Gradient Descent(43/49): loss=9.350364578267855e+99\n",
      "Gradient Descent(44/49): loss=2.435070661222921e+102\n",
      "Gradient Descent(45/49): loss=6.341537889260651e+104\n",
      "Gradient Descent(46/49): loss=1.6514963381281043e+107\n",
      "Gradient Descent(47/49): loss=4.300912810864725e+109\n",
      "Gradient Descent(48/49): loss=1.120066123042502e+112\n",
      "Gradient Descent(49/49): loss=2.9169345558884843e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.5944017171528815\n",
      "Gradient Descent(2/49): loss=322.1583212146127\n",
      "Gradient Descent(3/49): loss=21815.768053011987\n",
      "Gradient Descent(4/49): loss=1784955.4696424943\n",
      "Gradient Descent(5/49): loss=216735757.8941507\n",
      "Gradient Descent(6/49): loss=40594197486.5233\n",
      "Gradient Descent(7/49): loss=9559649295928.316\n",
      "Gradient Descent(8/49): loss=2425271792396171.0\n",
      "Gradient Descent(9/49): loss=6.276070386059153e+17\n",
      "Gradient Descent(10/49): loss=1.63220314482619e+20\n",
      "Gradient Descent(11/49): loss=4.250047213714308e+22\n",
      "Gradient Descent(12/49): loss=1.1069919031205992e+25\n",
      "Gradient Descent(13/49): loss=2.8835491837802037e+27\n",
      "Gradient Descent(14/49): loss=7.511353498672186e+29\n",
      "Gradient Descent(15/49): loss=1.9566402557602314e+32\n",
      "Gradient Descent(16/49): loss=5.096878085315907e+34\n",
      "Gradient Descent(17/49): loss=1.3276928737453062e+37\n",
      "Gradient Descent(18/49): loss=3.4585258948356414e+39\n",
      "Gradient Descent(19/49): loss=9.009163035356654e+41\n",
      "Gradient Descent(20/49): loss=2.346809634828936e+44\n",
      "Gradient Descent(21/49): loss=6.1132376514182e+46\n",
      "Gradient Descent(22/49): loss=1.5924459332844437e+49\n",
      "Gradient Descent(23/49): loss=4.1481849638163776e+51\n",
      "Gradient Descent(24/49): loss=1.0805665758958207e+54\n",
      "Gradient Descent(25/49): loss=2.814783176575557e+56\n",
      "Gradient Descent(26/49): loss=7.332268559733314e+58\n",
      "Gradient Descent(27/49): loss=1.9099930211133882e+61\n",
      "Gradient Descent(28/49): loss=4.975367870097105e+63\n",
      "Gradient Descent(29/49): loss=1.29604062261783e+66\n",
      "Gradient Descent(30/49): loss=3.3760745724371865e+68\n",
      "Gradient Descent(31/49): loss=8.794384465847049e+70\n",
      "Gradient Descent(32/49): loss=2.290861664151603e+73\n",
      "Gradient Descent(33/49): loss=5.967497992224568e+75\n",
      "Gradient Descent(34/49): loss=1.554482003189507e+78\n",
      "Gradient Descent(35/49): loss=4.049292184745765e+80\n",
      "Gradient Descent(36/49): loss=1.0548058558284792e+83\n",
      "Gradient Descent(37/49): loss=2.747678712051088e+85\n",
      "Gradient Descent(38/49): loss=7.157467189759654e+87\n",
      "Gradient Descent(39/49): loss=1.864458764694648e+90\n",
      "Gradient Descent(40/49): loss=4.8567550406940744e+92\n",
      "Gradient Descent(41/49): loss=1.2651429987066902e+95\n",
      "Gradient Descent(42/49): loss=3.2955889143377484e+97\n",
      "Gradient Descent(43/49): loss=8.584726235222814e+99\n",
      "Gradient Descent(44/49): loss=2.2362474947373984e+102\n",
      "Gradient Descent(45/49): loss=5.825232768869446e+104\n",
      "Gradient Descent(46/49): loss=1.5174231336811713e+107\n",
      "Gradient Descent(47/49): loss=3.9527570107343095e+109\n",
      "Gradient Descent(48/49): loss=1.0296592716367421e+112\n",
      "Gradient Descent(49/49): loss=2.6821740187630775e+114\n",
      "Gradient Descent(0/49): loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/49): loss=5.389777829025912\n",
      "Gradient Descent(2/49): loss=305.86821872645896\n",
      "Gradient Descent(3/49): loss=20195.107200105533\n",
      "Gradient Descent(4/49): loss=1544250.2679670008\n",
      "Gradient Descent(5/49): loss=167167835.00664613\n",
      "Gradient Descent(6/49): loss=28808765577.072815\n",
      "Gradient Descent(7/49): loss=6630472578196.771\n",
      "Gradient Descent(8/49): loss=1688994273909367.2\n",
      "Gradient Descent(9/49): loss=4.4218354423321306e+17\n",
      "Gradient Descent(10/49): loss=1.165557809688264e+20\n",
      "Gradient Descent(11/49): loss=3.077406241203675e+22\n",
      "Gradient Descent(12/49): loss=8.128494836553736e+24\n",
      "Gradient Descent(13/49): loss=2.1472251701590332e+27\n",
      "Gradient Descent(14/49): loss=5.672248323298297e+29\n",
      "Gradient Descent(15/49): loss=1.4984261550603496e+32\n",
      "Gradient Descent(16/49): loss=3.9583669354159086e+34\n",
      "Gradient Descent(17/49): loss=1.0456754199668539e+37\n",
      "Gradient Descent(18/49): loss=2.762344141899124e+39\n",
      "Gradient Descent(19/49): loss=7.297240769377508e+41\n",
      "Gradient Descent(20/49): loss=1.9277005456334197e+44\n",
      "Gradient Descent(21/49): loss=5.092376029905372e+46\n",
      "Gradient Descent(22/49): loss=1.3452449186567665e+49\n",
      "Gradient Descent(23/49): loss=3.5537122173699346e+51\n",
      "Gradient Descent(24/49): loss=9.387785338521409e+53\n",
      "Gradient Descent(25/49): loss=2.479956399725615e+56\n",
      "Gradient Descent(26/49): loss=6.551261583825684e+58\n",
      "Gradient Descent(27/49): loss=1.7306364073364206e+61\n",
      "Gradient Descent(28/49): loss=4.5717948155100876e+63\n",
      "Gradient Descent(29/49): loss=1.2077238030196022e+66\n",
      "Gradient Descent(30/49): loss=3.1904248620951156e+68\n",
      "Gradient Descent(31/49): loss=8.428094879992861e+70\n",
      "Gradient Descent(32/49): loss=2.2264364897004263e+73\n",
      "Gradient Descent(33/49): loss=5.881542048650849e+75\n",
      "Gradient Descent(34/49): loss=1.5537176573450123e+78\n",
      "Gradient Descent(35/49): loss=4.104431352827584e+80\n",
      "Gradient Descent(36/49): loss=1.0842611365350063e+83\n",
      "Gradient Descent(37/49): loss=2.864275489442078e+85\n",
      "Gradient Descent(38/49): loss=7.566511242519091e+87\n",
      "Gradient Descent(39/49): loss=1.9988333033677869e+90\n",
      "Gradient Descent(40/49): loss=5.280286312403484e+92\n",
      "Gradient Descent(41/49): loss=1.3948848807941976e+95\n",
      "Gradient Descent(42/49): loss=3.684845320030652e+97\n",
      "Gradient Descent(43/49): loss=9.73419758110871e+99\n",
      "Gradient Descent(44/49): loss=2.5714675737670603e+102\n",
      "Gradient Descent(45/49): loss=6.793005204423257e+104\n",
      "Gradient Descent(46/49): loss=1.7944974371083046e+107\n",
      "Gradient Descent(47/49): loss=4.7404954874633405e+109\n",
      "Gradient Descent(48/49): loss=1.252289192614961e+112\n",
      "Gradient Descent(49/49): loss=3.3081525466854215e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.4809249677645955\n",
      "Gradient Descent(2/49): loss=315.88194018036467\n",
      "Gradient Descent(3/49): loss=21825.062430044418\n",
      "Gradient Descent(4/49): loss=1913093.2874586238\n",
      "Gradient Descent(5/49): loss=262829058.4768571\n",
      "Gradient Descent(6/49): loss=54671042967.9092\n",
      "Gradient Descent(7/49): loss=13699422705420.64\n",
      "Gradient Descent(8/49): loss=3625828299574302.0\n",
      "Gradient Descent(9/49): loss=9.729466068412659e+17\n",
      "Gradient Descent(10/49): loss=2.6194575019659133e+20\n",
      "Gradient Descent(11/49): loss=7.057928951253135e+22\n",
      "Gradient Descent(12/49): loss=1.9020632022703566e+25\n",
      "Gradient Descent(13/49): loss=5.126158594723557e+27\n",
      "Gradient Descent(14/49): loss=1.3815409198280336e+30\n",
      "Gradient Descent(15/49): loss=3.723373173444308e+32\n",
      "Gradient Descent(16/49): loss=1.0034821207905724e+35\n",
      "Gradient Descent(17/49): loss=2.704473776118392e+37\n",
      "Gradient Descent(18/49): loss=7.288798178466035e+39\n",
      "Gradient Descent(19/49): loss=1.9643961714283582e+42\n",
      "Gradient Descent(20/49): loss=5.294223040952364e+44\n",
      "Gradient Descent(21/49): loss=1.426840370990853e+47\n",
      "Gradient Descent(22/49): loss=3.8454621740753947e+49\n",
      "Gradient Descent(23/49): loss=1.0363863844388679e+52\n",
      "Gradient Descent(24/49): loss=2.793153824520738e+54\n",
      "Gradient Descent(25/49): loss=7.5277989025975935e+56\n",
      "Gradient Descent(26/49): loss=2.0288090050927755e+59\n",
      "Gradient Descent(27/49): loss=5.467821380995645e+61\n",
      "Gradient Descent(28/49): loss=1.4736266735521065e+64\n",
      "Gradient Descent(29/49): loss=3.971555436232585e+66\n",
      "Gradient Descent(30/49): loss=1.0703696442361744e+69\n",
      "Gradient Descent(31/49): loss=2.8847417433736355e+71\n",
      "Gradient Descent(32/49): loss=7.774636519985521e+73\n",
      "Gradient Descent(33/49): loss=2.0953339465043898e+76\n",
      "Gradient Descent(34/49): loss=5.647112036797759e+78\n",
      "Gradient Descent(35/49): loss=1.5219471058227647e+81\n",
      "Gradient Descent(36/49): loss=4.1017833147787546e+83\n",
      "Gradient Descent(37/49): loss=1.10546722005176e+86\n",
      "Gradient Descent(38/49): loss=2.9793328433657182e+88\n",
      "Gradient Descent(39/49): loss=8.029567978634455e+90\n",
      "Gradient Descent(40/49): loss=2.1640402503896023e+93\n",
      "Gradient Descent(41/49): loss=5.832281659196806e+95\n",
      "Gradient Descent(42/49): loss=1.571851972073013e+98\n",
      "Gradient Descent(43/49): loss=4.236281384342679e+100\n",
      "Gradient Descent(44/49): loss=1.1417156504667653e+103\n",
      "Gradient Descent(45/49): loss=3.0770255992402913e+105\n",
      "Gradient Descent(46/49): loss=8.292858676772426e+107\n",
      "Gradient Descent(47/49): loss=2.234999443940242e+110\n",
      "Gradient Descent(48/49): loss=6.023523020360199e+112\n",
      "Gradient Descent(49/49): loss=1.6233932261228402e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.527231971929588\n",
      "Gradient Descent(2/49): loss=322.0791422246419\n",
      "Gradient Descent(3/49): loss=22228.136010386996\n",
      "Gradient Descent(4/49): loss=1879601.809290319\n",
      "Gradient Descent(5/49): loss=238624253.7516608\n",
      "Gradient Descent(6/49): loss=46163195022.94169\n",
      "Gradient Descent(7/49): loss=11049438300810.7\n",
      "Gradient Descent(8/49): loss=2830680518890307.0\n",
      "Gradient Descent(9/49): loss=7.383642506192499e+17\n",
      "Gradient Descent(10/49): loss=1.9347195498639576e+20\n",
      "Gradient Descent(11/49): loss=5.075197347819301e+22\n",
      "Gradient Descent(12/49): loss=1.3317056426228187e+25\n",
      "Gradient Descent(13/49): loss=3.4945660045563204e+27\n",
      "Gradient Descent(14/49): loss=9.17034304231632e+29\n",
      "Gradient Descent(15/49): loss=2.406465962152915e+32\n",
      "Gradient Descent(16/49): loss=6.315013891462543e+34\n",
      "Gradient Descent(17/49): loss=1.657177419956152e+37\n",
      "Gradient Descent(18/49): loss=4.348742884597733e+39\n",
      "Gradient Descent(19/49): loss=1.1411913256726435e+42\n",
      "Gradient Descent(20/49): loss=2.9946991148847296e+44\n",
      "Gradient Descent(21/49): loss=7.858649636866198e+46\n",
      "Gradient Descent(22/49): loss=2.062256399059407e+49\n",
      "Gradient Descent(23/49): loss=5.4117458497610747e+51\n",
      "Gradient Descent(24/49): loss=1.4201431575713226e+54\n",
      "Gradient Descent(25/49): loss=3.726720810609828e+56\n",
      "Gradient Descent(26/49): loss=9.779611249894026e+58\n",
      "Gradient Descent(27/49): loss=2.5663525941301148e+61\n",
      "Gradient Descent(28/49): loss=6.73458838915519e+63\n",
      "Gradient Descent(29/49): loss=1.7672817396597048e+66\n",
      "Gradient Descent(30/49): loss=4.63767726675627e+68\n",
      "Gradient Descent(31/49): loss=1.217013108205927e+71\n",
      "Gradient Descent(32/49): loss=3.193669633206189e+73\n",
      "Gradient Descent(33/49): loss=8.380785430568803e+75\n",
      "Gradient Descent(34/49): loss=2.1992745806560587e+78\n",
      "Gradient Descent(35/49): loss=5.771307142022398e+80\n",
      "Gradient Descent(36/49): loss=1.5144987542948097e+83\n",
      "Gradient Descent(37/49): loss=3.9743275142288326e+85\n",
      "Gradient Descent(38/49): loss=1.0429377472622112e+88\n",
      "Gradient Descent(39/49): loss=2.736863383226776e+90\n",
      "Gradient Descent(40/49): loss=7.182040537041171e+92\n",
      "Gradient Descent(41/49): loss=1.884701537965964e+95\n",
      "Gradient Descent(42/49): loss=4.9458087418073694e+97\n",
      "Gradient Descent(43/49): loss=1.2978725606038109e+100\n",
      "Gradient Descent(44/49): loss=3.405859934148376e+102\n",
      "Gradient Descent(45/49): loss=8.937612399818895e+104\n",
      "Gradient Descent(46/49): loss=2.3453963743041868e+107\n",
      "Gradient Descent(47/49): loss=6.1547580120063705e+109\n",
      "Gradient Descent(48/49): loss=1.615123422265671e+112\n",
      "Gradient Descent(49/49): loss=4.238385431340123e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.6372054538909415\n",
      "Gradient Descent(2/49): loss=327.40310703210787\n",
      "Gradient Descent(3/49): loss=22348.337811681144\n",
      "Gradient Descent(4/49): loss=1842599.8594252407\n",
      "Gradient Descent(5/49): loss=225348635.16308528\n",
      "Gradient Descent(6/49): loss=42509309260.86235\n",
      "Gradient Descent(7/49): loss=10084893317168.355\n",
      "Gradient Descent(8/49): loss=2577909143439931.5\n",
      "Gradient Descent(9/49): loss=6.72197333848007e+17\n",
      "Gradient Descent(10/49): loss=1.761538352338691e+20\n",
      "Gradient Descent(11/49): loss=4.6219215643352504e+22\n",
      "Gradient Descent(12/49): loss=1.213066862340271e+25\n",
      "Gradient Descent(13/49): loss=3.184045664994921e+27\n",
      "Gradient Descent(14/49): loss=8.357604115605686e+29\n",
      "Gradient Descent(15/49): loss=2.1937455851047985e+32\n",
      "Gradient Descent(16/49): loss=5.758259130299786e+34\n",
      "Gradient Descent(17/49): loss=1.5114586422541112e+37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(18/49): loss=3.967357717597889e+39\n",
      "Gradient Descent(19/49): loss=1.0413733514191366e+42\n",
      "Gradient Descent(20/49): loss=2.733452687207385e+44\n",
      "Gradient Descent(21/49): loss=7.174913387633864e+46\n",
      "Gradient Descent(22/49): loss=1.8833097925396311e+49\n",
      "Gradient Descent(23/49): loss=4.943412670321045e+51\n",
      "Gradient Descent(24/49): loss=1.2975735020469971e+54\n",
      "Gradient Descent(25/49): loss=3.405940603197188e+56\n",
      "Gradient Descent(26/49): loss=8.940095782024995e+58\n",
      "Gradient Descent(27/49): loss=2.346644345963402e+61\n",
      "Gradient Descent(28/49): loss=6.159598085642675e+63\n",
      "Gradient Descent(29/49): loss=1.6168043803449671e+66\n",
      "Gradient Descent(30/49): loss=4.243874954107319e+68\n",
      "Gradient Descent(31/49): loss=1.1139550860355967e+71\n",
      "Gradient Descent(32/49): loss=2.9239691252062836e+73\n",
      "Gradient Descent(33/49): loss=7.674991166462747e+75\n",
      "Gradient Descent(34/49): loss=2.0145728933142073e+78\n",
      "Gradient Descent(35/49): loss=5.287959105687033e+80\n",
      "Gradient Descent(36/49): loss=1.3880119004984872e+83\n",
      "Gradient Descent(37/49): loss=3.643328167673321e+85\n",
      "Gradient Descent(38/49): loss=9.563203408122432e+87\n",
      "Gradient Descent(39/49): loss=2.510200981525414e+90\n",
      "Gradient Descent(40/49): loss=6.588910325068772e+92\n",
      "Gradient Descent(41/49): loss=1.7294925622017822e+95\n",
      "Gradient Descent(42/49): loss=4.5396649447949936e+97\n",
      "Gradient Descent(43/49): loss=1.191595631077145e+100\n",
      "Gradient Descent(44/49): loss=3.12776419684929e+102\n",
      "Gradient Descent(45/49): loss=8.209923413573605e+104\n",
      "Gradient Descent(46/49): loss=2.1549847819295493e+107\n",
      "Gradient Descent(47/49): loss=5.656519770537766e+109\n",
      "Gradient Descent(48/49): loss=1.4847536828467015e+112\n",
      "Gradient Descent(49/49): loss=3.8972611926666133e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.430913599311256\n",
      "Gradient Descent(2/49): loss=310.8509717943615\n",
      "Gradient Descent(3/49): loss=20688.96000563135\n",
      "Gradient Descent(4/49): loss=1594326.0540109968\n",
      "Gradient Descent(5/49): loss=173843655.653422\n",
      "Gradient Descent(6/49): loss=30170926924.056374\n",
      "Gradient Descent(7/49): loss=6994911693415.274\n",
      "Gradient Descent(8/49): loss=1795269361621616.2\n",
      "Gradient Descent(9/49): loss=4.735880791280739e+17\n",
      "Gradient Descent(10/49): loss=1.2578781817229153e+20\n",
      "Gradient Descent(11/49): loss=3.346562325714052e+22\n",
      "Gradient Descent(12/49): loss=8.907058165394635e+24\n",
      "Gradient Descent(13/49): loss=2.3708933029681284e+27\n",
      "Gradient Descent(14/49): loss=6.3110260843048214e+29\n",
      "Gradient Descent(15/49): loss=1.679927030159777e+32\n",
      "Gradient Descent(16/49): loss=4.471790613406973e+34\n",
      "Gradient Descent(17/49): loss=1.1903444378239712e+37\n",
      "Gradient Descent(18/49): loss=3.1685741071227294e+39\n",
      "Gradient Descent(19/49): loss=8.434417593154405e+41\n",
      "Gradient Descent(20/49): loss=2.245155015046941e+44\n",
      "Gradient Descent(21/49): loss=5.976371209598097e+46\n",
      "Gradient Descent(22/49): loss=1.590848409370471e+49\n",
      "Gradient Descent(23/49): loss=4.234674475543142e+51\n",
      "Gradient Descent(24/49): loss=1.1272266929067106e+54\n",
      "Gradient Descent(25/49): loss=3.000561258119825e+56\n",
      "Gradient Descent(26/49): loss=7.987184760965828e+58\n",
      "Gradient Descent(27/49): loss=2.1261062487287042e+61\n",
      "Gradient Descent(28/49): loss=5.6594756677908044e+63\n",
      "Gradient Descent(29/49): loss=1.5064940829494338e+66\n",
      "Gradient Descent(30/49): loss=4.0101319542337486e+68\n",
      "Gradient Descent(31/49): loss=1.0674557884013088e+71\n",
      "Gradient Descent(32/49): loss=2.841457271720141e+73\n",
      "Gradient Descent(33/49): loss=7.563666350156799e+75\n",
      "Gradient Descent(34/49): loss=2.0133700135445734e+78\n",
      "Gradient Descent(35/49): loss=5.359383430968508e+80\n",
      "Gradient Descent(36/49): loss=1.4266126229610808e+83\n",
      "Gradient Descent(37/49): loss=3.79749574219969e+85\n",
      "Gradient Descent(38/49): loss=1.0108542206848348e+88\n",
      "Gradient Descent(39/49): loss=2.6907897331425977e+90\n",
      "Gradient Descent(40/49): loss=7.162604893790036e+92\n",
      "Gradient Descent(41/49): loss=1.906611588138811e+95\n",
      "Gradient Descent(42/49): loss=5.0752035075630345e+97\n",
      "Gradient Descent(43/49): loss=1.3509668567746657e+100\n",
      "Gradient Descent(44/49): loss=3.596134510436538e+102\n",
      "Gradient Descent(45/49): loss=9.572539364901578e+104\n",
      "Gradient Descent(46/49): loss=2.5481113019175248e+107\n",
      "Gradient Descent(47/49): loss=6.782809617650979e+109\n",
      "Gradient Descent(48/49): loss=1.805514000690585e+112\n",
      "Gradient Descent(49/49): loss=4.806092151261996e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.52263543475761\n",
      "Gradient Descent(2/49): loss=321.004527033725\n",
      "Gradient Descent(3/49): loss=22355.146887258852\n",
      "Gradient Descent(4/49): loss=1974399.0019690173\n",
      "Gradient Descent(5/49): loss=273183227.0804967\n",
      "Gradient Descent(6/49): loss=57233046541.346794\n",
      "Gradient Descent(7/49): loss=14448116093693.588\n",
      "Gradient Descent(8/49): loss=3852911821057867.5\n",
      "Gradient Descent(9/49): loss=1.0417435119469775e+18\n",
      "Gradient Descent(10/49): loss=2.8260377341542112e+20\n",
      "Gradient Descent(11/49): loss=7.672554842271342e+22\n",
      "Gradient Descent(12/49): loss=2.083455042366509e+25\n",
      "Gradient Descent(13/49): loss=5.657802604506144e+27\n",
      "Gradient Descent(14/49): loss=1.5364417251314838e+30\n",
      "Gradient Descent(15/49): loss=4.172395791840474e+32\n",
      "Gradient Descent(16/49): loss=1.1330658942299718e+35\n",
      "Gradient Descent(17/49): loss=3.0769815937136336e+37\n",
      "Gradient Descent(18/49): loss=8.3559271354946e+39\n",
      "Gradient Descent(19/49): loss=2.2691562082892503e+42\n",
      "Gradient Descent(20/49): loss=6.162176649083715e+44\n",
      "Gradient Descent(21/49): loss=1.6734159126317148e+47\n",
      "Gradient Descent(22/49): loss=4.544369589553846e+49\n",
      "Gradient Descent(23/49): loss=1.2340802314064884e+52\n",
      "Gradient Descent(24/49): loss=3.351298761127563e+54\n",
      "Gradient Descent(25/49): loss=9.100869700790307e+56\n",
      "Gradient Descent(26/49): loss=2.4714546572661977e+59\n",
      "Gradient Descent(27/49): loss=6.711543318099517e+61\n",
      "Gradient Descent(28/49): loss=1.822603282576664e+64\n",
      "Gradient Descent(29/49): loss=4.949506496815503e+66\n",
      "Gradient Descent(30/49): loss=1.3441002107373393e+69\n",
      "Gradient Descent(31/49): loss=3.6500717347608357e+71\n",
      "Gradient Descent(32/49): loss=9.912224968398186e+73\n",
      "Gradient Descent(33/49): loss=2.691788297979129e+76\n",
      "Gradient Descent(34/49): loss=7.309886795586308e+78\n",
      "Gradient Descent(35/49): loss=1.9850909153741074e+81\n",
      "Gradient Descent(36/49): loss=5.390761926272515e+83\n",
      "Gradient Descent(37/49): loss=1.4639286251668916e+86\n",
      "Gradient Descent(38/49): loss=3.975480737033563e+88\n",
      "Gradient Descent(39/49): loss=1.0795913693348941e+91\n",
      "Gradient Descent(40/49): loss=2.9317649910488625e+93\n",
      "Gradient Descent(41/49): loss=7.961573431283508e+95\n",
      "Gradient Descent(42/49): loss=2.162064547985534e+98\n",
      "Gradient Descent(43/49): loss=5.8713558946630306e+100\n",
      "Gradient Descent(44/49): loss=1.5944399104048128e+103\n",
      "Gradient Descent(45/49): loss=4.329900407165901e+105\n",
      "Gradient Descent(46/49): loss=1.175838450457208e+108\n",
      "Gradient Descent(47/49): loss=3.1931359420772155e+110\n",
      "Gradient Descent(48/49): loss=8.671358842382196e+112\n",
      "Gradient Descent(49/49): loss=2.354815627562858e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.569304866085333\n",
      "Gradient Descent(2/49): loss=327.29965765218105\n",
      "Gradient Descent(3/49): loss=22768.18971327212\n",
      "Gradient Descent(4/49): loss=1939972.6358733957\n",
      "Gradient Descent(5/49): loss=248055661.03477097\n",
      "Gradient Descent(6/49): loss=48331096009.530106\n",
      "Gradient Descent(7/49): loss=11654053907814.729\n",
      "Gradient Descent(8/49): loss=3008127863176655.0\n",
      "Gradient Descent(9/49): loss=7.906172705936435e+17\n",
      "Gradient Descent(10/49): loss=2.08742209764615e+20\n",
      "Gradient Descent(11/49): loss=5.517516361014907e+22\n",
      "Gradient Descent(12/49): loss=1.458807130588809e+25\n",
      "Gradient Descent(13/49): loss=3.857287102424589e+27\n",
      "Gradient Descent(14/49): loss=1.0199371633349943e+30\n",
      "Gradient Descent(15/49): loss=2.696911435224668e+32\n",
      "Gradient Descent(16/49): loss=7.131163624483022e+34\n",
      "Gradient Descent(17/49): loss=1.8856202294912785e+37\n",
      "Gradient Descent(18/49): loss=4.985951887522044e+39\n",
      "Gradient Descent(19/49): loss=1.318384063846931e+42\n",
      "Gradient Descent(20/49): loss=3.4860676269422266e+44\n",
      "Gradient Descent(21/49): loss=9.217850734354151e+46\n",
      "Gradient Descent(22/49): loss=2.4373816366598485e+49\n",
      "Gradient Descent(23/49): loss=6.444918033796463e+51\n",
      "Gradient Descent(24/49): loss=1.7041635104737257e+54\n",
      "Gradient Descent(25/49): loss=4.506144616907572e+56\n",
      "Gradient Descent(26/49): loss=1.1915135598026187e+59\n",
      "Gradient Descent(27/49): loss=3.1505969823225967e+61\n",
      "Gradient Descent(28/49): loss=8.330800151921874e+63\n",
      "Gradient Descent(29/49): loss=2.2028279580240398e+66\n",
      "Gradient Descent(30/49): loss=5.824711821388422e+68\n",
      "Gradient Descent(31/49): loss=1.5401687489319259e+71\n",
      "Gradient Descent(32/49): loss=4.072510105094137e+73\n",
      "Gradient Descent(33/49): loss=1.0768520376481558e+76\n",
      "Gradient Descent(34/49): loss=2.8474092907381194e+78\n",
      "Gradient Descent(35/49): loss=7.529112065097697e+80\n",
      "Gradient Descent(36/49): loss=1.9908458075622898e+83\n",
      "Gradient Descent(37/49): loss=5.264189183558042e+85\n",
      "Gradient Descent(38/49): loss=1.3919555022807911e+88\n",
      "Gradient Descent(39/49): loss=3.6806050329296484e+90\n",
      "Gradient Descent(40/49): loss=9.732246028145176e+92\n",
      "Gradient Descent(41/49): loss=2.5733979034679648e+95\n",
      "Gradient Descent(42/49): loss=6.804571884456782e+97\n",
      "Gradient Descent(43/49): loss=1.7992630859123284e+100\n",
      "Gradient Descent(44/49): loss=4.757606661076652e+102\n",
      "Gradient Descent(45/49): loss=1.2580050865682132e+105\n",
      "Gradient Descent(46/49): loss=3.326413700356966e+107\n",
      "Gradient Descent(47/49): loss=8.795694249621434e+109\n",
      "Gradient Descent(48/49): loss=2.3257551315557394e+112\n",
      "Gradient Descent(49/49): loss=6.14975552633665e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.680184664110761\n",
      "Gradient Descent(2/49): loss=332.7117655092998\n",
      "Gradient Descent(3/49): loss=22891.70178500276\n",
      "Gradient Descent(4/49): loss=1901867.7563114022\n",
      "Gradient Descent(5/49): loss=234268588.10983983\n",
      "Gradient Descent(6/49): loss=44506951172.91967\n",
      "Gradient Descent(7/49): loss=10636833940529.756\n",
      "Gradient Descent(8/49): loss=2739517702321811.0\n",
      "Gradient Descent(9/49): loss=7.197680156101659e+17\n",
      "Gradient Descent(10/49): loss=1.900571474412769e+20\n",
      "Gradient Descent(11/49): loss=5.024733379405279e+22\n",
      "Gradient Descent(12/49): loss=1.3288442785885951e+25\n",
      "Gradient Descent(13/49): loss=3.514533524890132e+27\n",
      "Gradient Descent(14/49): loss=9.295425828549728e+29\n",
      "Gradient Descent(15/49): loss=2.458514966772086e+32\n",
      "Gradient Descent(16/49): loss=6.5024482856002105e+34\n",
      "Gradient Descent(17/49): loss=1.7198124013533123e+37\n",
      "Gradient Descent(18/49): loss=4.5486788439436087e+39\n",
      "Gradient Descent(19/49): loss=1.2030660757953692e+42\n",
      "Gradient Descent(20/49): loss=3.1819524707730632e+44\n",
      "Gradient Descent(21/49): loss=8.415848256467776e+46\n",
      "Gradient Descent(22/49): loss=2.2258818302249977e+49\n",
      "Gradient Descent(23/49): loss=5.887166416787719e+51\n",
      "Gradient Descent(24/49): loss=1.5570785451788757e+54\n",
      "Gradient Descent(25/49): loss=4.118269170974416e+56\n",
      "Gradient Descent(26/49): loss=1.089228351204691e+59\n",
      "Gradient Descent(27/49): loss=2.8808665772267323e+61\n",
      "Gradient Descent(28/49): loss=7.619515436412584e+63\n",
      "Gradient Descent(29/49): loss=2.0152622111926607e+66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=5.330105062131489e+68\n",
      "Gradient Descent(31/49): loss=1.4097431002066224e+71\n",
      "Gradient Descent(32/49): loss=3.728586182474667e+73\n",
      "Gradient Descent(33/49): loss=9.861622956766819e+75\n",
      "Gradient Descent(34/49): loss=2.608270335778655e+78\n",
      "Gradient Descent(35/49): loss=6.898534018515579e+80\n",
      "Gradient Descent(36/49): loss=1.8245720526667983e+83\n",
      "Gradient Descent(37/49): loss=4.825754524711523e+85\n",
      "Gradient Descent(38/49): loss=1.2763489772155455e+88\n",
      "Gradient Descent(39/49): loss=3.3757761678450927e+90\n",
      "Gradient Descent(40/49): loss=8.928486596394484e+92\n",
      "Gradient Descent(41/49): loss=2.3614679687985656e+95\n",
      "Gradient Descent(42/49): loss=6.245774026152954e+97\n",
      "Gradient Descent(43/49): loss=1.651925569230291e+100\n",
      "Gradient Descent(44/49): loss=4.369127148773412e+102\n",
      "Gradient Descent(45/49): loss=1.1555770064775254e+105\n",
      "Gradient Descent(46/49): loss=3.056350095635186e+107\n",
      "Gradient Descent(47/49): loss=8.083646398922139e+109\n",
      "Gradient Descent(48/49): loss=2.1380187824727837e+112\n",
      "Gradient Descent(49/49): loss=5.654780143297766e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.472218162724946\n",
      "Gradient Descent(2/49): loss=315.8944485121476\n",
      "Gradient Descent(3/49): loss=21192.838760167895\n",
      "Gradient Descent(4/49): loss=1645817.804096978\n",
      "Gradient Descent(5/49): loss=180758671.76566955\n",
      "Gradient Descent(6/49): loss=31591927471.227272\n",
      "Gradient Descent(7/49): loss=7377881864292.478\n",
      "Gradient Descent(8/49): loss=1907789415063742.5\n",
      "Gradient Descent(9/49): loss=5.0709089549646483e+17\n",
      "Gradient Descent(10/49): loss=1.3571181586947981e+20\n",
      "Gradient Descent(11/49): loss=3.63810100225508e+22\n",
      "Gradient Descent(12/49): loss=9.756804925286856e+24\n",
      "Gradient Descent(13/49): loss=2.616875381797177e+27\n",
      "Gradient Descent(14/49): loss=7.018895197086462e+29\n",
      "Gradient Descent(15/49): loss=1.8825952795268505e+32\n",
      "Gradient Descent(16/49): loss=5.049469762139919e+34\n",
      "Gradient Descent(17/49): loss=1.3543619130325545e+37\n",
      "Gradient Descent(18/49): loss=3.632651400937535e+39\n",
      "Gradient Descent(19/49): loss=9.743449172071211e+41\n",
      "Gradient Descent(20/49): loss=2.6133749634200696e+44\n",
      "Gradient Descent(21/49): loss=7.009559537571631e+46\n",
      "Gradient Descent(22/49): loss=1.880094728547785e+49\n",
      "Gradient Descent(23/49): loss=5.042765054786756e+51\n",
      "Gradient Descent(24/49): loss=1.3525637305432969e+54\n",
      "Gradient Descent(25/49): loss=3.627828434029171e+56\n",
      "Gradient Descent(26/49): loss=9.730513135581798e+58\n",
      "Gradient Descent(27/49): loss=2.609905280900888e+61\n",
      "Gradient Descent(28/49): loss=7.00025320387942e+63\n",
      "Gradient Descent(29/49): loss=1.877598596279722e+66\n",
      "Gradient Descent(30/49): loss=5.036069962152032e+68\n",
      "Gradient Descent(31/49): loss=1.3507679817157252e+71\n",
      "Gradient Descent(32/49): loss=3.6230119004316756e+73\n",
      "Gradient Descent(33/49): loss=9.717594293282675e+75\n",
      "Gradient Descent(34/49): loss=2.6064402062159453e+78\n",
      "Gradient Descent(35/49): loss=6.990959226683453e+80\n",
      "Gradient Descent(36/49): loss=1.8751057780874725e+83\n",
      "Gradient Descent(37/49): loss=5.029383758378792e+85\n",
      "Gradient Descent(38/49): loss=1.3489746170396924e+88\n",
      "Gradient Descent(39/49): loss=3.6182017615692653e+90\n",
      "Gradient Descent(40/49): loss=9.704692602853762e+92\n",
      "Gradient Descent(41/49): loss=2.602979731982627e+95\n",
      "Gradient Descent(42/49): loss=6.981677588757345e+97\n",
      "Gradient Descent(43/49): loss=1.8726162695177834e+100\n",
      "Gradient Descent(44/49): loss=5.022706431631167e+102\n",
      "Gradient Descent(45/49): loss=1.347183633347665e+105\n",
      "Gradient Descent(46/49): loss=3.613398008950297e+107\n",
      "Gradient Descent(47/49): loss=9.691808041522188e+109\n",
      "Gradient Descent(48/49): loss=2.5995238520929604e+112\n",
      "Gradient Descent(49/49): loss=6.972408273718498e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.564516313817271\n",
      "Gradient Descent(2/49): loss=326.1892597923221\n",
      "Gradient Descent(3/49): loss=22895.921550619223\n",
      "Gradient Descent(4/49): loss=2037418.2524919426\n",
      "Gradient Descent(5/49): loss=283903351.35346127\n",
      "Gradient Descent(6/49): loss=59904714028.45689\n",
      "Gradient Descent(7/49): loss=15234660287480.523\n",
      "Gradient Descent(8/49): loss=4093277157166797.5\n",
      "Gradient Descent(9/49): loss=1.1151169364727162e+18\n",
      "Gradient Descent(10/49): loss=3.0480347188024056e+20\n",
      "Gradient Descent(11/49): loss=8.338071431729927e+22\n",
      "Gradient Descent(12/49): loss=2.281359619007489e+25\n",
      "Gradient Descent(13/49): loss=6.242255009068573e+27\n",
      "Gradient Descent(14/49): loss=1.7080238503943327e+30\n",
      "Gradient Descent(15/49): loss=4.673556158442295e+32\n",
      "Gradient Descent(16/49): loss=1.2787959870102096e+35\n",
      "Gradient Descent(17/49): loss=3.499090109747896e+37\n",
      "Gradient Descent(18/49): loss=9.574343477950531e+39\n",
      "Gradient Descent(19/49): loss=2.6197683086139935e+42\n",
      "Gradient Descent(20/49): loss=7.168309785424434e+44\n",
      "Gradient Descent(21/49): loss=1.9614202154707396e+47\n",
      "Gradient Descent(22/49): loss=5.366912671526219e+49\n",
      "Gradient Descent(23/49): loss=1.4685150788880126e+52\n",
      "Gradient Descent(24/49): loss=4.018206870360113e+54\n",
      "Gradient Descent(25/49): loss=1.099477062588889e+57\n",
      "Gradient Descent(26/49): loss=3.0084309995996075e+59\n",
      "Gradient Descent(27/49): loss=8.231783442613453e+61\n",
      "Gradient Descent(28/49): loss=2.2524119268517654e+64\n",
      "Gradient Descent(29/49): loss=6.1631352714659046e+66\n",
      "Gradient Descent(30/49): loss=1.6863805381939656e+69\n",
      "Gradient Descent(31/49): loss=4.614338635022266e+71\n",
      "Gradient Descent(32/49): loss=1.2625929057187824e+74\n",
      "Gradient Descent(33/49): loss=3.4547547799635946e+76\n",
      "Gradient Descent(34/49): loss=9.45303156355588e+78\n",
      "Gradient Descent(35/49): loss=2.5865744874235254e+81\n",
      "Gradient Descent(36/49): loss=7.077483592441864e+83\n",
      "Gradient Descent(37/49): loss=1.9365680070238483e+86\n",
      "Gradient Descent(38/49): loss=5.2989111127481675e+88\n",
      "Gradient Descent(39/49): loss=1.4499082334814393e+91\n",
      "Gradient Descent(40/49): loss=3.967294111538634e+93\n",
      "Gradient Descent(41/49): loss=1.0855461196779842e+96\n",
      "Gradient Descent(42/49): loss=2.970312623207316e+98\n",
      "Gradient Descent(43/49): loss=8.127482489829058e+100\n",
      "Gradient Descent(44/49): loss=2.2238727030406482e+103\n",
      "Gradient Descent(45/49): loss=6.08504516068569e+105\n",
      "Gradient Descent(46/49): loss=1.6650132247658847e+108\n",
      "Gradient Descent(47/49): loss=4.55587257849182e+110\n",
      "Gradient Descent(48/49): loss=1.2465952007301737e+113\n",
      "Gradient Descent(49/49): loss=3.4109812504850295e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.611549575537993\n",
      "Gradient Descent(2/49): loss=332.58347339673287\n",
      "Gradient Descent(3/49): loss=23319.137650596105\n",
      "Gradient Descent(4/49): loss=2002034.582074821\n",
      "Gradient Descent(5/49): loss=257821504.41880172\n",
      "Gradient Descent(6/49): loss=50591993013.757065\n",
      "Gradient Descent(7/49): loss=12289274761851.72\n",
      "Gradient Descent(8/49): loss=3195963474984491.5\n",
      "Gradient Descent(9/49): loss=8.463491599769189e+17\n",
      "Gradient Descent(10/49): loss=2.251529773327707e+20\n",
      "Gradient Descent(11/49): loss=5.99648849700509e+22\n",
      "Gradient Descent(12/49): loss=1.5974883953495574e+25\n",
      "Gradient Descent(13/49): loss=4.2560664114268645e+27\n",
      "Gradient Descent(14/49): loss=1.1339306147043332e+30\n",
      "Gradient Descent(15/49): loss=3.0211089861916688e+32\n",
      "Gradient Descent(16/49): loss=8.049089482914975e+34\n",
      "Gradient Descent(17/49): loss=2.144505822532731e+37\n",
      "Gradient Descent(18/49): loss=5.713572353372097e+39\n",
      "Gradient Descent(19/49): loss=1.522257911554971e+42\n",
      "Gradient Descent(20/49): loss=4.05572733643905e+44\n",
      "Gradient Descent(21/49): loss=1.0805609298097583e+47\n",
      "Gradient Descent(22/49): loss=2.878921156299744e+49\n",
      "Gradient Descent(23/49): loss=7.670263467818113e+51\n",
      "Gradient Descent(24/49): loss=2.0435759950510566e+54\n",
      "Gradient Descent(25/49): loss=5.444666751129959e+56\n",
      "Gradient Descent(26/49): loss=1.45061383098433e+59\n",
      "Gradient Descent(27/49): loss=3.8648471666457744e+61\n",
      "Gradient Descent(28/49): loss=1.0297050326202179e+64\n",
      "Gradient Descent(29/49): loss=2.7434266052067617e+66\n",
      "Gradient Descent(30/49): loss=7.309267508389948e+68\n",
      "Gradient Descent(31/49): loss=1.9473964205132352e+71\n",
      "Gradient Descent(32/49): loss=5.188417053110585e+73\n",
      "Gradient Descent(33/49): loss=1.3823416348846781e+76\n",
      "Gradient Descent(34/49): loss=3.682950649447265e+78\n",
      "Gradient Descent(35/49): loss=9.81242635247348e+80\n",
      "Gradient Descent(36/49): loss=2.614309017069477e+83\n",
      "Gradient Descent(37/49): loss=6.965261588952205e+85\n",
      "Gradient Descent(38/49): loss=1.8557434750738022e+88\n",
      "Gradient Descent(39/49): loss=4.944227580398946e+90\n",
      "Gradient Descent(40/49): loss=1.3172826252725941e+93\n",
      "Gradient Descent(41/49): loss=3.509614973477969e+95\n",
      "Gradient Descent(42/49): loss=9.350610890743256e+97\n",
      "Gradient Descent(43/49): loss=2.4912682642062475e+100\n",
      "Gradient Descent(44/49): loss=6.637446084282168e+102\n",
      "Gradient Descent(45/49): loss=1.768404115876712e+105\n",
      "Gradient Descent(46/49): loss=4.711530726336478e+107\n",
      "Gradient Descent(47/49): loss=1.2552855756166878e+110\n",
      "Gradient Descent(48/49): loss=3.3444372283157287e+112\n",
      "Gradient Descent(49/49): loss=8.910530473234687e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.7233393478123356\n",
      "Gradient Descent(2/49): loss=338.08481330982664\n",
      "Gradient Descent(3/49): loss=23446.034346796805\n",
      "Gradient Descent(4/49): loss=1962798.222826027\n",
      "Gradient Descent(5/49): loss=243505318.12821302\n",
      "Gradient Descent(6/49): loss=46590345235.10502\n",
      "Gradient Descent(7/49): loss=11216719003478.713\n",
      "Gradient Descent(8/49): loss=2910587703040855.5\n",
      "Gradient Descent(9/49): loss=7.705058447834163e+17\n",
      "Gradient Descent(10/49): loss=2.0499887272150555e+20\n",
      "Gradient Descent(11/49): loss=5.460924289710994e+22\n",
      "Gradient Descent(12/49): loss=1.4551696953768007e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=3.8778750281570925e+27\n",
      "Gradient Descent(14/49): loss=1.0334322651568985e+30\n",
      "Gradient Descent(15/49): loss=2.7540524209297584e+32\n",
      "Gradient Descent(16/49): loss=7.339439159770198e+34\n",
      "Gradient Descent(17/49): loss=1.9559316162259747e+37\n",
      "Gradient Descent(18/49): loss=5.2124810404417345e+39\n",
      "Gradient Descent(19/49): loss=1.3891057757247176e+42\n",
      "Gradient Descent(20/49): loss=3.701912484860263e+44\n",
      "Gradient Descent(21/49): loss=9.865451788399927e+46\n",
      "Gradient Descent(22/49): loss=2.6291042646592895e+49\n",
      "Gradient Descent(23/49): loss=7.006459899788432e+51\n",
      "Gradient Descent(24/49): loss=1.8671941234115495e+54\n",
      "Gradient Descent(25/49): loss=4.9759992126991055e+56\n",
      "Gradient Descent(26/49): loss=1.3260843023415791e+59\n",
      "Gradient Descent(27/49): loss=3.53396273140341e+61\n",
      "Gradient Descent(28/49): loss=9.417872276216787e+63\n",
      "Gradient Descent(29/49): loss=2.5098260777614925e+66\n",
      "Gradient Descent(30/49): loss=6.688588203217878e+68\n",
      "Gradient Descent(31/49): loss=1.7824825611871262e+71\n",
      "Gradient Descent(32/49): loss=4.750246216993241e+73\n",
      "Gradient Descent(33/49): loss=1.2659220131180794e+76\n",
      "Gradient Descent(34/49): loss=3.373632586799454e+78\n",
      "Gradient Descent(35/49): loss=8.990598719965335e+80\n",
      "Gradient Descent(36/49): loss=2.395959348380811e+83\n",
      "Gradient Descent(37/49): loss=6.385137828857958e+85\n",
      "Gradient Descent(38/49): loss=1.701614224843428e+88\n",
      "Gradient Descent(39/49): loss=4.53473526773868e+90\n",
      "Gradient Descent(40/49): loss=1.208489189161877e+93\n",
      "Gradient Descent(41/49): loss=3.2205763602368035e+95\n",
      "Gradient Descent(42/49): loss=8.582709870420578e+97\n",
      "Gradient Descent(43/49): loss=2.287258567419852e+100\n",
      "Gradient Descent(44/49): loss=6.095454504719281e+102\n",
      "Gradient Descent(45/49): loss=1.6244147534668391e+105\n",
      "Gradient Descent(46/49): loss=4.3290016999352026e+107\n",
      "Gradient Descent(47/49): loss=1.153661999070506e+110\n",
      "Gradient Descent(48/49): loss=3.07446404587753e+112\n",
      "Gradient Descent(49/49): loss=8.193326274948443e+114\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.513691519266978\n",
      "Gradient Descent(2/49): loss=320.9991404074359\n",
      "Gradient Descent(3/49): loss=21706.905681430486\n",
      "Gradient Descent(4/49): loss=1698759.7187185637\n",
      "Gradient Descent(5/49): loss=187920477.96105057\n",
      "Gradient Descent(6/49): loss=33074069973.08975\n",
      "Gradient Descent(7/49): loss=7780249658013.035\n",
      "Gradient Descent(8/49): loss=2026895624716649.0\n",
      "Gradient Descent(9/49): loss=5.4282343050240864e+17\n",
      "Gradient Descent(10/49): loss=1.4637671666266821e+20\n",
      "Gradient Descent(11/49): loss=3.953788000109959e+22\n",
      "Gradient Descent(12/49): loss=1.0683935948758133e+25\n",
      "Gradient Descent(13/49): loss=2.887300063672664e+27\n",
      "Gradient Descent(14/49): loss=7.803023489437133e+29\n",
      "Gradient Descent(15/49): loss=2.1088049552332332e+32\n",
      "Gradient Descent(16/49): loss=5.6991556610032005e+34\n",
      "Gradient Descent(17/49): loss=1.540227138949864e+37\n",
      "Gradient Descent(18/49): loss=4.162546129273503e+39\n",
      "Gradient Descent(19/49): loss=1.124950352021883e+42\n",
      "Gradient Descent(20/49): loss=3.040238501341019e+44\n",
      "Gradient Descent(21/49): loss=8.216407185572792e+46\n",
      "Gradient Descent(22/49): loss=2.2205279956925816e+49\n",
      "Gradient Descent(23/49): loss=6.001095696240464e+51\n",
      "Gradient Descent(24/49): loss=1.621828214996031e+54\n",
      "Gradient Descent(25/49): loss=4.383077511354374e+56\n",
      "Gradient Descent(26/49): loss=1.184550144887327e+59\n",
      "Gradient Descent(27/49): loss=3.201310134529962e+61\n",
      "Gradient Descent(28/49): loss=8.651711893901863e+63\n",
      "Gradient Descent(29/49): loss=2.338171421997376e+66\n",
      "Gradient Descent(30/49): loss=6.319033349340455e+68\n",
      "Gradient Descent(31/49): loss=1.7077525665747543e+71\n",
      "Gradient Descent(32/49): loss=4.6152926680581764e+73\n",
      "Gradient Descent(33/49): loss=1.247307533230926e+76\n",
      "Gradient Descent(34/49): loss=3.37091533375978e+78\n",
      "Gradient Descent(35/49): loss=9.110079017917049e+80\n",
      "Gradient Descent(36/49): loss=2.4620475893152873e+83\n",
      "Gradient Descent(37/49): loss=6.653815318321066e+85\n",
      "Gradient Descent(38/49): loss=1.798229184620923e+88\n",
      "Gradient Descent(39/49): loss=4.859810568410026e+90\n",
      "Gradient Descent(40/49): loss=1.3133898038590873e+93\n",
      "Gradient Descent(41/49): loss=3.549506205229287e+95\n",
      "Gradient Descent(42/49): loss=9.592730401851544e+97\n",
      "Gradient Descent(43/49): loss=2.5924867077859327e+100\n",
      "Gradient Descent(44/49): loss=7.006334013879544e+102\n",
      "Gradient Descent(45/49): loss=1.8934992479081566e+105\n",
      "Gradient Descent(46/49): loss=5.117283010952977e+107\n",
      "Gradient Descent(47/49): loss=1.382973108815158e+110\n",
      "Gradient Descent(48/49): loss=3.737558809258958e+112\n",
      "Gradient Descent(49/49): loss=1.0100952624189e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.60656760494358\n",
      "Gradient Descent(2/49): loss=331.4366392486661\n",
      "Gradient Descent(3/49): loss=23447.558267917964\n",
      "Gradient Descent(4/49): loss=2102191.960856551\n",
      "Gradient Descent(5/49): loss=295000892.4440111\n",
      "Gradient Descent(6/49): loss=62690297659.41076\n",
      "Gradient Descent(7/49): loss=16060815018475.562\n",
      "Gradient Descent(8/49): loss=4347646570412141.0\n",
      "Gradient Descent(9/49): loss=1.1933523037810752e+18\n",
      "Gradient Descent(10/49): loss=3.2865341954675927e+20\n",
      "Gradient Descent(11/49): loss=9.058476147698274e+22\n",
      "Gradient Descent(12/49): loss=2.4972091654370794e+25\n",
      "Gradient Descent(13/49): loss=6.884531714660951e+27\n",
      "Gradient Descent(14/49): loss=1.8980103719888437e+30\n",
      "Gradient Descent(15/49): loss=5.232676361214012e+32\n",
      "Gradient Descent(16/49): loss=1.4426116456414827e+35\n",
      "Gradient Descent(17/49): loss=3.9771782539489655e+37\n",
      "Gradient Descent(18/49): loss=1.0964799473302e+40\n",
      "Gradient Descent(19/49): loss=3.0229177775205705e+42\n",
      "Gradient Descent(20/49): loss=8.333970839920981e+44\n",
      "Gradient Descent(21/49): loss=2.297616908724283e+47\n",
      "Gradient Descent(22/49): loss=6.33436756768574e+49\n",
      "Gradient Descent(23/49): loss=1.7463404073697905e+52\n",
      "Gradient Descent(24/49): loss=4.814537182827114e+54\n",
      "Gradient Descent(25/49): loss=1.3273339027739614e+57\n",
      "Gradient Descent(26/49): loss=3.6593658383986233e+59\n",
      "Gradient Descent(27/49): loss=1.0088613205203801e+62\n",
      "Gradient Descent(28/49): loss=2.781359418514837e+64\n",
      "Gradient Descent(29/49): loss=7.668011507241587e+66\n",
      "Gradient Descent(30/49): loss=2.1140166238057084e+69\n",
      "Gradient Descent(31/49): loss=5.828194547577889e+71\n",
      "Gradient Descent(32/49): loss=1.6067920801524684e+74\n",
      "Gradient Descent(33/49): loss=4.429812299099717e+76\n",
      "Gradient Descent(34/49): loss=1.221267969119745e+79\n",
      "Gradient Descent(35/49): loss=3.366949549309391e+81\n",
      "Gradient Descent(36/49): loss=9.282442145572305e+83\n",
      "Gradient Descent(37/49): loss=2.559103750264744e+86\n",
      "Gradient Descent(38/49): loss=7.055268324772414e+88\n",
      "Gradient Descent(39/49): loss=1.945087655371056e+91\n",
      "Gradient Descent(40/49): loss=5.362469310759993e+93\n",
      "Gradient Descent(41/49): loss=1.4783949211459713e+96\n",
      "Gradient Descent(42/49): loss=4.075830398664592e+98\n",
      "Gradient Descent(43/49): loss=1.1236776588627178e+101\n",
      "Gradient Descent(44/49): loss=3.097899955407656e+103\n",
      "Gradient Descent(45/49): loss=8.54069141450039e+105\n",
      "Gradient Descent(46/49): loss=2.354608311685166e+108\n",
      "Gradient Descent(47/49): loss=6.491488841341435e+110\n",
      "Gradient Descent(48/49): loss=1.789657632997165e+113\n",
      "Gradient Descent(49/49): loss=4.933959714984555e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.653966100287568\n",
      "Gradient Descent(2/49): loss=337.93109927690125\n",
      "Gradient Descent(3/49): loss=23881.154988091763\n",
      "Gradient Descent(4/49): loss=2065828.1193999974\n",
      "Gradient Descent(5/49): loss=267932292.32526538\n",
      "Gradient Descent(6/49): loss=52949499952.49327\n",
      "Gradient Descent(7/49): loss=12956525554898.07\n",
      "Gradient Descent(8/49): loss=3394752791437708.0\n",
      "Gradient Descent(9/49): loss=9.057770649469919e+17\n",
      "Gradient Descent(10/49): loss=2.427846439024669e+20\n",
      "Gradient Descent(11/49): loss=6.51499513065596e+22\n",
      "Gradient Descent(12/49): loss=1.7487546121892206e+25\n",
      "Gradient Descent(13/49): loss=4.694331598272776e+27\n",
      "Gradient Descent(14/49): loss=1.2601611688734527e+30\n",
      "Gradient Descent(15/49): loss=3.3828306654195956e+32\n",
      "Gradient Descent(16/49): loss=9.081025148159943e+34\n",
      "Gradient Descent(17/49): loss=2.4377524970511027e+37\n",
      "Gradient Descent(18/49): loss=6.544016252105597e+39\n",
      "Gradient Descent(19/49): loss=1.756706205143293e+42\n",
      "Gradient Descent(20/49): loss=4.715783983864533e+44\n",
      "Gradient Descent(21/49): loss=1.2659270251740802e+47\n",
      "Gradient Descent(22/49): loss=3.398313491307855e+49\n",
      "Gradient Descent(23/49): loss=9.12259107849684e+51\n",
      "Gradient Descent(24/49): loss=2.448910855326761e+54\n",
      "Gradient Descent(25/49): loss=6.573970405726894e+56\n",
      "Gradient Descent(26/49): loss=1.7647472467774166e+59\n",
      "Gradient Descent(27/49): loss=4.7373697367059743e+61\n",
      "Gradient Descent(28/49): loss=1.2717216056430076e+64\n",
      "Gradient Descent(29/49): loss=3.4138687333781574e+66\n",
      "Gradient Descent(30/49): loss=9.164348295273652e+68\n",
      "Gradient Descent(31/49): loss=2.460120357175447e+71\n",
      "Gradient Descent(32/49): loss=6.604061714798217e+73\n",
      "Gradient Descent(33/49): loss=1.7728250979938307e+76\n",
      "Gradient Descent(34/49): loss=4.759054296894801e+78\n",
      "Gradient Descent(35/49): loss=1.2775427100181712e+81\n",
      "Gradient Descent(36/49): loss=3.429495177194089e+83\n",
      "Gradient Descent(37/49): loss=9.206296649158891e+85\n",
      "Gradient Descent(38/49): loss=2.4713811687486243e+88\n",
      "Gradient Descent(39/49): loss=6.634290762076503e+90\n",
      "Gradient Descent(40/49): loss=1.780939924295897e+93\n",
      "Gradient Descent(41/49): loss=4.780838114725982e+95\n",
      "Gradient Descent(42/49): loss=1.283390459577331e+98\n",
      "Gradient Descent(43/49): loss=3.445193148583583e+100\n",
      "Gradient Descent(44/49): loss=9.248437014995532e+102\n",
      "Gradient Descent(45/49): loss=2.4826935249045016e+105\n",
      "Gradient Descent(46/49): loss=6.664658178034601e+107\n",
      "Gradient Descent(47/49): loss=1.7890918949310248e+110\n",
      "Gradient Descent(48/49): loss=4.802721644535747e+112\n",
      "Gradient Descent(49/49): loss=1.2892649762845907e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.766669504995667\n",
      "Gradient Descent(2/49): loss=343.52276917948933\n",
      "Gradient Descent(3/49): loss=24011.511975737503\n",
      "Gradient Descent(4/49): loss=2025431.1021101882\n",
      "Gradient Descent(5/49): loss=253068793.9628457\n",
      "Gradient Descent(6/49): loss=48762826279.616585\n",
      "Gradient Descent(7/49): loss=11825849609433.807\n",
      "Gradient Descent(8/49): loss=3091634159756003.5\n",
      "Gradient Descent(9/49): loss=8.24608511432574e+17\n",
      "Gradient Descent(10/49): loss=2.2105220071839007e+20\n",
      "Gradient Descent(11/49): loss=5.933118274282558e+22\n",
      "Gradient Descent(12/49): loss=1.592958718453057e+25\n",
      "Gradient Descent(13/49): loss=4.277193193818758e+27\n",
      "Gradient Descent(14/49): loss=1.1484743331055824e+30\n",
      "Gradient Descent(15/49): loss=3.0837964861291272e+32\n",
      "Gradient Descent(16/49): loss=8.280386607033657e+34\n",
      "Gradient Descent(17/49): loss=2.223389957480923e+37\n",
      "Gradient Descent(18/49): loss=5.970087479165575e+39\n",
      "Gradient Descent(19/49): loss=1.603045160290536e+42\n",
      "Gradient Descent(20/49): loss=4.3043821477089655e+44\n",
      "Gradient Descent(21/49): loss=1.1557818925709836e+47\n",
      "Gradient Descent(22/49): loss=3.1034228321028615e+49\n",
      "Gradient Descent(23/49): loss=8.33308891350347e+51\n",
      "Gradient Descent(24/49): loss=2.2375414050284002e+54\n",
      "Gradient Descent(25/49): loss=6.008086066527038e+56\n",
      "Gradient Descent(26/49): loss=1.6132482778513114e+59\n",
      "Gradient Descent(27/49): loss=4.331778834678308e+61\n",
      "Gradient Descent(28/49): loss=1.1631382552944836e+64\n",
      "Gradient Descent(29/49): loss=3.1231756111342928e+66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=8.38612766245461e+68\n",
      "Gradient Descent(31/49): loss=2.2517829903725283e+71\n",
      "Gradient Descent(32/49): loss=6.046326552399438e+73\n",
      "Gradient Descent(33/49): loss=1.6235163394765088e+76\n",
      "Gradient Descent(34/49): loss=4.359349898991559e+78\n",
      "Gradient Descent(35/49): loss=1.17054144019055e+81\n",
      "Gradient Descent(36/49): loss=3.143054113459253e+83\n",
      "Gradient Descent(37/49): loss=8.439503994429317e+85\n",
      "Gradient Descent(38/49): loss=2.266115221083396e+88\n",
      "Gradient Descent(39/49): loss=6.084810432716778e+90\n",
      "Gradient Descent(40/49): loss=1.6338497556359116e+93\n",
      "Gradient Descent(41/49): loss=4.38709644862288e+95\n",
      "Gradient Descent(42/49): loss=1.1779917451484742e+98\n",
      "Gradient Descent(43/49): loss=3.163059139202494e+100\n",
      "Gradient Descent(44/49): loss=8.493220058033276e+102\n",
      "Gradient Descent(45/49): loss=2.2805386741003696e+105\n",
      "Gradient Descent(46/49): loss=6.12353925664285e+107\n",
      "Gradient Descent(47/49): loss=1.6442489423003666e+110\n",
      "Gradient Descent(48/49): loss=4.415019600508147e+112\n",
      "Gradient Descent(49/49): loss=1.1854894700798994e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.5553336689373545\n",
      "Gradient Descent(2/49): loss=326.1655409900588\n",
      "Gradient Descent(3/49): loss=22231.324948201116\n",
      "Gradient Descent(4/49): loss=1753186.6843791858\n",
      "Gradient Descent(5/49): loss=195336879.1996641\n",
      "Gradient Descent(6/49): loss=34619738022.74058\n",
      "Gradient Descent(7/49): loss=8202918651368.684\n",
      "Gradient Descent(8/49): loss=2152946424267061.2\n",
      "Gradient Descent(9/49): loss=5.809248101825661e+17\n",
      "Gradient Descent(10/49): loss=1.5783471986321346e+20\n",
      "Gradient Descent(11/49): loss=4.2955208635978145e+22\n",
      "Gradient Descent(12/49): loss=1.1695165657223531e+25\n",
      "Gradient Descent(13/49): loss=3.18448984593366e+27\n",
      "Gradient Descent(14/49): loss=8.671290994457085e+29\n",
      "Gradient Descent(15/49): loss=2.3611858506902008e+32\n",
      "Gradient Descent(16/49): loss=6.429499894495026e+34\n",
      "Gradient Descent(17/49): loss=1.7507509180162946e+37\n",
      "Gradient Descent(18/49): loss=4.767289959987437e+39\n",
      "Gradient Descent(19/49): loss=1.2981317777537761e+42\n",
      "Gradient Descent(20/49): loss=3.534809366959348e+44\n",
      "Gradient Descent(21/49): loss=9.625276485401919e+46\n",
      "Gradient Descent(22/49): loss=2.6209602224286634e+49\n",
      "Gradient Descent(23/49): loss=7.136867702910549e+51\n",
      "Gradient Descent(24/49): loss=1.9433671741304443e+54\n",
      "Gradient Descent(25/49): loss=5.291783637747265e+56\n",
      "Gradient Descent(26/49): loss=1.4409512747526332e+59\n",
      "Gradient Descent(27/49): loss=3.923706482254489e+61\n",
      "Gradient Descent(28/49): loss=1.0684242297873587e+64\n",
      "Gradient Descent(29/49): loss=2.9093163312787943e+66\n",
      "Gradient Descent(30/49): loss=7.922060619244698e+68\n",
      "Gradient Descent(31/49): loss=2.157174996072086e+71\n",
      "Gradient Descent(32/49): loss=5.87398176728701e+73\n",
      "Gradient Descent(33/49): loss=1.5994836703209748e+76\n",
      "Gradient Descent(34/49): loss=4.355389773034898e+78\n",
      "Gradient Descent(35/49): loss=1.185971474860366e+81\n",
      "Gradient Descent(36/49): loss=3.2293971664501854e+83\n",
      "Gradient Descent(37/49): loss=8.79363988067604e+85\n",
      "Gradient Descent(38/49): loss=2.3945057967589243e+88\n",
      "Gradient Descent(39/49): loss=6.5202329052746785e+90\n",
      "Gradient Descent(40/49): loss=1.7754576830246264e+93\n",
      "Gradient Descent(41/49): loss=4.83456654080729e+95\n",
      "Gradient Descent(42/49): loss=1.3164511810654114e+98\n",
      "Gradient Descent(43/49): loss=3.584693058830306e+100\n",
      "Gradient Descent(44/49): loss=9.76110964906907e+102\n",
      "Gradient Descent(45/49): loss=2.6579475569448016e+105\n",
      "Gradient Descent(46/49): loss=7.237584116415061e+107\n",
      "Gradient Descent(47/49): loss=1.97079222670575e+110\n",
      "Gradient Descent(48/49): loss=5.366461982852467e+112\n",
      "Gradient Descent(49/49): loss=1.4612861682298735e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.648789308136539\n",
      "Gradient Descent(2/49): loss=336.7471682058951\n",
      "Gradient Descent(3/49): loss=24010.230951337235\n",
      "Gradient Descent(4/49): loss=2168761.860691293\n",
      "Gradient Descent(5/49): loss=306487624.6012908\n",
      "Gradient Descent(6/49): loss=65594197536.23932\n",
      "Gradient Descent(7/49): loss=16928414731993.213\n",
      "Gradient Descent(8/49): loss=4616778654865756.0\n",
      "Gradient Descent(9/49): loss=1.2767516613420498e+18\n",
      "Gradient Descent(10/49): loss=3.542693840503712e+20\n",
      "Gradient Descent(11/49): loss=9.838063555460488e+22\n",
      "Gradient Descent(12/49): loss=2.732554031297292e+25\n",
      "Gradient Descent(13/49): loss=7.590103418931204e+27\n",
      "Gradient Descent(14/49): loss=2.1082948330667718e+30\n",
      "Gradient Descent(15/49): loss=5.856202941602066e+32\n",
      "Gradient Descent(16/49): loss=1.626676377235138e+35\n",
      "Gradient Descent(17/49): loss=4.51841654170637e+37\n",
      "Gradient Descent(18/49): loss=1.2550799312675721e+40\n",
      "Gradient Descent(19/49): loss=3.486233799896985e+42\n",
      "Gradient Descent(20/49): loss=9.683706853108935e+44\n",
      "Gradient Descent(21/49): loss=2.689841928098661e+47\n",
      "Gradient Descent(22/49): loss=7.471570245486237e+49\n",
      "Gradient Descent(23/49): loss=2.0753770454529896e+52\n",
      "Gradient Descent(24/49): loss=5.764771981395274e+54\n",
      "Gradient Descent(25/49): loss=1.60127992503054e+57\n",
      "Gradient Descent(26/49): loss=4.447873058261266e+59\n",
      "Gradient Descent(27/49): loss=1.2354850912172796e+62\n",
      "Gradient Descent(28/49): loss=3.431805248544852e+64\n",
      "Gradient Descent(29/49): loss=9.532520746435333e+66\n",
      "Gradient Descent(30/49): loss=2.6478469843167327e+69\n",
      "Gradient Descent(31/49): loss=7.354920947826947e+71\n",
      "Gradient Descent(32/49): loss=2.0429753860094517e+74\n",
      "Gradient Descent(33/49): loss=5.674769936274619e+76\n",
      "Gradient Descent(34/49): loss=1.5762800692644847e+79\n",
      "Gradient Descent(35/49): loss=4.378430993083628e+81\n",
      "Gradient Descent(36/49): loss=1.216196178268041e+84\n",
      "Gradient Descent(37/49): loss=3.378226461420233e+86\n",
      "Gradient Descent(38/49): loss=9.383695022699652e+88\n",
      "Gradient Descent(39/49): loss=2.6065076833842364e+91\n",
      "Gradient Descent(40/49): loss=7.2400928281517095e+93\n",
      "Gradient Descent(41/49): loss=2.0110795949081424e+96\n",
      "Gradient Descent(42/49): loss=5.586173040944949e+98\n",
      "Gradient Descent(43/49): loss=1.5516705217629512e+101\n",
      "Gradient Descent(44/49): loss=4.310073086638326e+103\n",
      "Gradient Descent(45/49): loss=1.1972084119416057e+106\n",
      "Gradient Descent(46/49): loss=3.325484168858114e+108\n",
      "Gradient Descent(47/49): loss=9.237192828766498e+110\n",
      "Gradient Descent(48/49): loss=2.565813789007299e+113\n",
      "Gradient Descent(49/49): loss=7.127057453383381e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.6965544403340616\n",
      "Gradient Descent(2/49): loss=343.343047157048\n",
      "Gradient Descent(3/49): loss=24454.41899608812\n",
      "Gradient Descent(4/49): loss=2131394.523833138\n",
      "Gradient Descent(5/49): loss=278398821.19226766\n",
      "Gradient Descent(6/49): loss=55407356701.18321\n",
      "Gradient Descent(7/49): loss=13657291546026.832\n",
      "Gradient Descent(8/49): loss=3605089719084517.0\n",
      "Gradient Descent(9/49): loss=9.691307895460623e+17\n",
      "Gradient Descent(10/49): loss=2.6172292602098323e+20\n",
      "Gradient Descent(11/49): loss=7.076131985814047e+22\n",
      "Gradient Descent(12/49): loss=1.9136939192759456e+25\n",
      "Gradient Descent(13/49): loss=5.175821363796905e+27\n",
      "Gradient Descent(14/49): loss=1.3998888534320425e+30\n",
      "Gradient Descent(15/49): loss=3.786253423411391e+32\n",
      "Gradient Descent(16/49): loss=1.0240620185145098e+35\n",
      "Gradient Descent(17/49): loss=2.769765181436298e+37\n",
      "Gradient Descent(18/49): loss=7.491342822704619e+39\n",
      "Gradient Descent(19/49): loss=2.0261724189793877e+42\n",
      "Gradient Descent(20/49): loss=5.480158610639468e+44\n",
      "Gradient Descent(21/49): loss=1.4822104054674968e+47\n",
      "Gradient Descent(22/49): loss=4.0089125930334454e+49\n",
      "Gradient Descent(23/49): loss=1.0842846683742136e+52\n",
      "Gradient Descent(24/49): loss=2.9326487290632837e+54\n",
      "Gradient Descent(25/49): loss=7.931891706080263e+56\n",
      "Gradient Descent(26/49): loss=2.1453270353704105e+59\n",
      "Gradient Descent(27/49): loss=5.802434348875664e+61\n",
      "Gradient Descent(28/49): loss=1.5693758488994597e+64\n",
      "Gradient Descent(29/49): loss=4.24466768087802e+66\n",
      "Gradient Descent(30/49): loss=1.1480489988249674e+69\n",
      "Gradient Descent(31/49): loss=3.105111171931306e+71\n",
      "Gradient Descent(32/49): loss=8.398348328268799e+73\n",
      "Gradient Descent(33/49): loss=2.271488869078593e+76\n",
      "Gradient Descent(34/49): loss=6.143662397260321e+78\n",
      "Gradient Descent(35/49): loss=1.6616672951966172e+81\n",
      "Gradient Descent(36/49): loss=4.494286992653341e+83\n",
      "Gradient Descent(37/49): loss=1.2155631654255485e+86\n",
      "Gradient Descent(38/49): loss=3.287715741239381e+88\n",
      "Gradient Descent(39/49): loss=8.892236210044586e+90\n",
      "Gradient Descent(40/49): loss=2.4050699950543337e+93\n",
      "Gradient Descent(41/49): loss=6.504957295867425e+95\n",
      "Gradient Descent(42/49): loss=1.759386192837301e+98\n",
      "Gradient Descent(43/49): loss=4.7585858519211326e+100\n",
      "Gradient Descent(44/49): loss=1.2870476875566667e+103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/49): loss=3.4810588725139406e+105\n",
      "Gradient Descent(46/49): loss=9.415168521776265e+107\n",
      "Gradient Descent(47/49): loss=2.5465067251053753e+110\n",
      "Gradient Descent(48/49): loss=6.887499130800072e+112\n",
      "Gradient Descent(49/49): loss=1.8628517179670165e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.810175135660757\n",
      "Gradient Descent(2/49): loss=349.02615394626184\n",
      "Gradient Descent(3/49): loss=24588.313272230418\n",
      "Gradient Descent(4/49): loss=2089807.030358961\n",
      "Gradient Descent(5/49): loss=262969258.0047724\n",
      "Gradient Descent(6/49): loss=51027845457.910645\n",
      "Gradient Descent(7/49): loss=12465582187137.918\n",
      "Gradient Descent(8/49): loss=3283198018599300.0\n",
      "Gradient Descent(9/49): loss=8.822852294285765e+17\n",
      "Gradient Descent(10/49): loss=2.3829517419327203e+20\n",
      "Gradient Descent(11/49): loss=6.444134511814701e+22\n",
      "Gradient Descent(12/49): loss=1.7432025231992655e+25\n",
      "Gradient Descent(13/49): loss=4.7158944284540653e+27\n",
      "Gradient Descent(14/49): loss=1.2758171145392908e+30\n",
      "Gradient Descent(15/49): loss=3.451554798325213e+32\n",
      "Gradient Descent(16/49): loss=9.337736495461508e+34\n",
      "Gradient Descent(17/49): loss=2.526204921931288e+37\n",
      "Gradient Descent(18/49): loss=6.834324088418822e+39\n",
      "Gradient Descent(19/49): loss=1.848938941196761e+42\n",
      "Gradient Descent(20/49): loss=5.002067766016878e+44\n",
      "Gradient Descent(21/49): loss=1.353245442778232e+47\n",
      "Gradient Descent(22/49): loss=3.6610324260378637e+49\n",
      "Gradient Descent(23/49): loss=9.904454876875635e+51\n",
      "Gradient Descent(24/49): loss=2.6795235603999485e+54\n",
      "Gradient Descent(25/49): loss=7.249108204358173e+56\n",
      "Gradient Descent(26/49): loss=1.9611534877006028e+59\n",
      "Gradient Descent(27/49): loss=5.305649872916488e+61\n",
      "Gradient Descent(28/49): loss=1.4353756985632496e+64\n",
      "Gradient Descent(29/49): loss=3.883225326539365e+66\n",
      "Gradient Descent(30/49): loss=1.0505569344507367e+69\n",
      "Gradient Descent(31/49): loss=2.842147389644497e+71\n",
      "Gradient Descent(32/49): loss=7.689066170113327e+73\n",
      "Gradient Descent(33/49): loss=2.0801784870057537e+76\n",
      "Gradient Descent(34/49): loss=5.62765678180891e+78\n",
      "Gradient Descent(35/49): loss=1.5224905483676435e+81\n",
      "Gradient Descent(36/49): loss=4.118903408185e+83\n",
      "Gradient Descent(37/49): loss=1.1143166244380295e+86\n",
      "Gradient Descent(38/49): loss=3.014641074203007e+88\n",
      "Gradient Descent(39/49): loss=8.155725766772373e+90\n",
      "Gradient Descent(40/49): loss=2.2064272709605843e+93\n",
      "Gradient Descent(41/49): loss=5.969206715940418e+95\n",
      "Gradient Descent(42/49): loss=1.6148925136387992e+98\n",
      "Gradient Descent(43/49): loss=4.368885104351477e+100\n",
      "Gradient Descent(44/49): loss=1.1819459743494282e+103\n",
      "Gradient Descent(45/49): loss=3.197603628645241e+105\n",
      "Gradient Descent(46/49): loss=8.650707551631646e+107\n",
      "Gradient Descent(47/49): loss=2.3403382606105607e+110\n",
      "Gradient Descent(48/49): loss=6.331485767363208e+112\n",
      "Gradient Descent(49/49): loss=1.7129024764079972e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.597144611736076\n",
      "Gradient Descent(2/49): loss=331.3941457520692\n",
      "Gradient Descent(3/49): loss=22766.262716075544\n",
      "Gradient Descent(4/49): loss=1809134.2845622578\n",
      "Gradient Descent(5/49): loss=203015895.76213208\n",
      "Gradient Descent(6/49): loss=36231398566.04772\n",
      "Gradient Descent(7/49): loss=8646830862424.713\n",
      "Gradient Descent(8/49): loss=2286318291959156.0\n",
      "Gradient Descent(9/49): loss=6.215422676319301e+17\n",
      "Gradient Descent(10/49): loss=1.7014148466081782e+20\n",
      "Gradient Descent(11/49): loss=4.665338238200336e+22\n",
      "Gradient Descent(12/49): loss=1.2797762415889926e+25\n",
      "Gradient Descent(13/49): loss=3.510977884311125e+27\n",
      "Gradient Descent(14/49): loss=9.632357619245214e+29\n",
      "Gradient Descent(15/49): loss=2.642649925318894e+32\n",
      "Gradient Descent(16/49): loss=7.250154917738557e+34\n",
      "Gradient Descent(17/49): loss=1.9890929867185088e+37\n",
      "Gradient Descent(18/49): loss=5.457112686427371e+39\n",
      "Gradient Descent(19/49): loss=1.497168793502684e+42\n",
      "Gradient Descent(20/49): loss=4.107509842573883e+44\n",
      "Gradient Descent(21/49): loss=1.126902804811704e+47\n",
      "Gradient Descent(22/49): loss=3.0916783660333916e+49\n",
      "Gradient Descent(23/49): loss=8.482075897626479e+51\n",
      "Gradient Descent(24/49): loss=2.3270729686739113e+54\n",
      "Gradient Descent(25/49): loss=6.384367066404388e+56\n",
      "Gradient Descent(26/49): loss=1.751562730835068e+59\n",
      "Gradient Descent(27/49): loss=4.80544424864805e+61\n",
      "Gradient Descent(28/49): loss=1.3183823805075092e+64\n",
      "Gradient Descent(29/49): loss=3.617006069151011e+66\n",
      "Gradient Descent(30/49): loss=9.923322017728377e+68\n",
      "Gradient Descent(31/49): loss=2.722481466298622e+71\n",
      "Gradient Descent(32/49): loss=7.469177480180457e+73\n",
      "Gradient Descent(33/49): loss=2.049182443334803e+76\n",
      "Gradient Descent(34/49): loss=5.621969349655919e+78\n",
      "Gradient Descent(35/49): loss=1.542397528891326e+81\n",
      "Gradient Descent(36/49): loss=4.23159570814043e+83\n",
      "Gradient Descent(37/49): loss=1.1609459884199684e+86\n",
      "Gradient Descent(38/49): loss=3.185076460484229e+88\n",
      "Gradient Descent(39/49): loss=8.738315270753932e+90\n",
      "Gradient Descent(40/49): loss=2.397372707325292e+93\n",
      "Gradient Descent(41/49): loss=6.577235679586825e+95\n",
      "Gradient Descent(42/49): loss=1.804476586083036e+98\n",
      "Gradient Descent(43/49): loss=4.950614374102021e+100\n",
      "Gradient Descent(44/49): loss=1.3582100687859752e+103\n",
      "Gradient Descent(45/49): loss=3.726274057219947e+105\n",
      "Gradient Descent(46/49): loss=1.0223100732805749e+108\n",
      "Gradient Descent(47/49): loss=2.8047263026881428e+110\n",
      "Gradient Descent(48/49): loss=7.694817686523739e+112\n",
      "Gradient Descent(49/49): loss=2.111087244844157e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.691181423396144\n",
      "Gradient Descent(2/49): loss=342.12135147780197\n",
      "Gradient Descent(3/49): loss=24584.11559392544\n",
      "Gradient Descent(4/49): loss=2237170.510266338\n",
      "Gradient Descent(5/49): loss=318375642.48418945\n",
      "Gradient Descent(6/49): loss=68620966187.81085\n",
      "Gradient Descent(7/49): loss=17839371459990.82\n",
      "Gradient Descent(8/49): loss=4901470016720643.0\n",
      "Gradient Descent(9/49): loss=1.36563458096392e+18\n",
      "Gradient Descent(10/49): loss=3.817747735650948e+20\n",
      "Gradient Descent(11/49): loss=1.0681446200958557e+23\n",
      "Gradient Descent(12/49): loss=2.989071923843094e+25\n",
      "Gradient Descent(13/49): loss=8.364934900406843e+27\n",
      "Gradient Descent(14/49): loss=2.340957370325692e+30\n",
      "Gradient Descent(15/49): loss=6.551271137676196e+32\n",
      "Gradient Descent(16/49): loss=1.833402891052039e+35\n",
      "Gradient Descent(17/49): loss=5.13086176157371e+37\n",
      "Gradient Descent(18/49): loss=1.4358951581516664e+40\n",
      "Gradient Descent(19/49): loss=4.0184183832754215e+42\n",
      "Gradient Descent(20/49): loss=1.124572796530603e+45\n",
      "Gradient Descent(21/49): loss=3.1471684993317464e+47\n",
      "Gradient Descent(22/49): loss=8.807495250520836e+49\n",
      "Gradient Descent(23/49): loss=2.464817902394264e+52\n",
      "Gradient Descent(24/49): loss=6.897905839509895e+54\n",
      "Gradient Descent(25/49): loss=1.9304105558709968e+57\n",
      "Gradient Descent(26/49): loss=5.402342393360754e+59\n",
      "Gradient Descent(27/49): loss=1.5118702726911573e+62\n",
      "Gradient Descent(28/49): loss=4.2310382330753205e+64\n",
      "Gradient Descent(29/49): loss=1.1840754364380476e+67\n",
      "Gradient Descent(30/49): loss=3.313689364033197e+69\n",
      "Gradient Descent(31/49): loss=9.273511520802017e+71\n",
      "Gradient Descent(32/49): loss=2.595234690972222e+74\n",
      "Gradient Descent(33/49): loss=7.262883198147168e+76\n",
      "Gradient Descent(34/49): loss=2.032551142038225e+79\n",
      "Gradient Descent(35/49): loss=5.688187503903334e+81\n",
      "Gradient Descent(36/49): loss=1.5918653366387397e+84\n",
      "Gradient Descent(37/49): loss=4.454908084962215e+86\n",
      "Gradient Descent(38/49): loss=1.2467264402758831e+89\n",
      "Gradient Descent(39/49): loss=3.489021068986073e+91\n",
      "Gradient Descent(40/49): loss=9.76418533093345e+93\n",
      "Gradient Descent(41/49): loss=2.732552005039066e+96\n",
      "Gradient Descent(42/49): loss=7.647171993538238e+98\n",
      "Gradient Descent(43/49): loss=2.140096122266512e+101\n",
      "Gradient Descent(44/49): loss=5.989157058857941e+103\n",
      "Gradient Descent(45/49): loss=1.6760930456562347e+106\n",
      "Gradient Descent(46/49): loss=4.690623188019989e+108\n",
      "Gradient Descent(47/49): loss=1.3126923919295864e+111\n",
      "Gradient Descent(48/49): loss=3.673629807294778e+113\n",
      "Gradient Descent(49/49): loss=1.0280821343991258e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.739314595677466\n",
      "Gradient Descent(2/49): loss=348.819830947283\n",
      "Gradient Descent(3/49): loss=25039.109066305886\n",
      "Gradient Descent(4/49): loss=2198775.8885879256\n",
      "Gradient Descent(5/49): loss=289232182.21983254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/49): loss=57969432982.77495\n",
      "Gradient Descent(7/49): loss=14393120893517.842\n",
      "Gradient Descent(8/49): loss=3827597952021661.0\n",
      "Gradient Descent(9/49): loss=1.0366534816983507e+18\n",
      "Gradient Descent(10/49): loss=2.8205920194682123e+20\n",
      "Gradient Descent(11/49): loss=7.683224183276748e+22\n",
      "Gradient Descent(12/49): loss=2.0934839136761915e+25\n",
      "Gradient Descent(13/49): loss=5.704612334603576e+27\n",
      "Gradient Descent(14/49): loss=1.5544979607160617e+30\n",
      "Gradient Descent(15/49): loss=4.236000799890778e+32\n",
      "Gradient Descent(16/49): loss=1.1543097638975745e+35\n",
      "Gradient Descent(17/49): loss=3.14549369098808e+37\n",
      "Gradient Descent(18/49): loss=8.571469728838785e+39\n",
      "Gradient Descent(19/49): loss=2.3357253805854997e+42\n",
      "Gradient Descent(20/49): loss=6.364851358783256e+44\n",
      "Gradient Descent(21/49): loss=1.7344219143074693e+47\n",
      "Gradient Descent(22/49): loss=4.726299497008666e+49\n",
      "Gradient Descent(23/49): loss=1.2879165532035265e+52\n",
      "Gradient Descent(24/49): loss=3.509572444763444e+54\n",
      "Gradient Descent(25/49): loss=9.563584468613483e+56\n",
      "Gradient Descent(26/49): loss=2.606076646882022e+59\n",
      "Gradient Descent(27/49): loss=7.101558533534049e+61\n",
      "Gradient Descent(28/49): loss=1.93517461067581e+64\n",
      "Gradient Descent(29/49): loss=5.273350569625941e+66\n",
      "Gradient Descent(30/49): loss=1.4369879636061411e+69\n",
      "Gradient Descent(31/49): loss=3.9157920192956384e+71\n",
      "Gradient Descent(32/49): loss=1.0670532757908265e+74\n",
      "Gradient Descent(33/49): loss=2.9077200417319144e+76\n",
      "Gradient Descent(34/49): loss=7.92353674639443e+78\n",
      "Gradient Descent(35/49): loss=2.159163663296428e+81\n",
      "Gradient Descent(36/49): loss=5.883720709721127e+83\n",
      "Gradient Descent(37/49): loss=1.6033138190714272e+86\n",
      "Gradient Descent(38/49): loss=4.369029954427096e+88\n",
      "Gradient Descent(39/49): loss=1.1905606073885405e+91\n",
      "Gradient Descent(40/49): loss=3.244277504733246e+93\n",
      "Gradient Descent(41/49): loss=8.840655790556904e+95\n",
      "Gradient Descent(42/49): loss=2.409078591244995e+98\n",
      "Gradient Descent(43/49): loss=6.564738856809877e+100\n",
      "Gradient Descent(44/49): loss=1.788891255550005e+103\n",
      "Gradient Descent(45/49): loss=4.874728445387586e+105\n",
      "Gradient Descent(46/49): loss=1.328363439786935e+108\n",
      "Gradient Descent(47/49): loss=3.619790205610586e+110\n",
      "Gradient Descent(48/49): loss=9.863927853010138e+112\n",
      "Gradient Descent(49/49): loss=2.6879202153367883e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.8538562398076035\n",
      "Gradient Descent(2/49): loss=354.5954905202867\n",
      "Gradient Descent(3/49): loss=25176.61897535453\n",
      "Gradient Descent(4/49): loss=2155967.4494077386\n",
      "Gradient Descent(5/49): loss=273217232.7102448\n",
      "Gradient Descent(6/49): loss=53388973834.84624\n",
      "Gradient Descent(7/49): loss=13137330621428.95\n",
      "Gradient Descent(8/49): loss=3485847358381715.0\n",
      "Gradient Descent(9/49): loss=9.437573610083071e+17\n",
      "Gradient Descent(10/49): loss=2.5681099073177534e+20\n",
      "Gradient Descent(11/49): loss=6.997001084203143e+22\n",
      "Gradient Descent(12/49): loss=1.9069737717095323e+25\n",
      "Gradient Descent(13/49): loss=5.197693027371311e+27\n",
      "Gradient Descent(14/49): loss=1.4167221852749822e+30\n",
      "Gradient Descent(15/49): loss=3.861542093370868e+32\n",
      "Gradient Descent(16/49): loss=1.0525369403747046e+35\n",
      "Gradient Descent(17/49): loss=2.868890964410192e+37\n",
      "Gradient Descent(18/49): loss=7.819712182490243e+39\n",
      "Gradient Descent(19/49): loss=2.1314124662729402e+42\n",
      "Gradient Descent(20/49): loss=5.809573312212069e+44\n",
      "Gradient Descent(21/49): loss=1.5835105892897535e+47\n",
      "Gradient Descent(22/49): loss=4.3161617080562877e+49\n",
      "Gradient Descent(23/49): loss=1.1764526247731893e+52\n",
      "Gradient Descent(24/49): loss=3.206647183245645e+54\n",
      "Gradient Descent(25/49): loss=8.740331689802325e+56\n",
      "Gradient Descent(26/49): loss=2.3823449753680713e+59\n",
      "Gradient Descent(27/49): loss=6.493537983558169e+61\n",
      "Gradient Descent(28/49): loss=1.7699382742585853e+64\n",
      "Gradient Descent(29/49): loss=4.824306106497607e+66\n",
      "Gradient Descent(30/49): loss=1.3149571229505398e+69\n",
      "Gradient Descent(31/49): loss=3.584167747708773e+71\n",
      "Gradient Descent(32/49): loss=9.769336368087136e+73\n",
      "Gradient Descent(33/49): loss=2.662819928945536e+76\n",
      "Gradient Descent(34/49): loss=7.2580262433711096e+78\n",
      "Gradient Descent(35/49): loss=1.9783142065608366e+81\n",
      "Gradient Descent(36/49): loss=5.392274660697173e+83\n",
      "Gradient Descent(37/49): loss=1.469767841729471e+86\n",
      "Gradient Descent(38/49): loss=4.006134042702466e+88\n",
      "Gradient Descent(39/49): loss=1.0919486406244048e+91\n",
      "Gradient Descent(40/49): loss=2.9763153730052434e+93\n",
      "Gradient Descent(41/49): loss=8.112518180820183e+95\n",
      "Gradient Descent(42/49): loss=2.2112223667912666e+98\n",
      "Gradient Descent(43/49): loss=6.027110505536653e+100\n",
      "Gradient Descent(44/49): loss=1.6428045225801181e+103\n",
      "Gradient Descent(45/49): loss=4.477778691680755e+105\n",
      "Gradient Descent(46/49): loss=1.2205044322729018e+108\n",
      "Gradient Descent(47/49): loss=3.326718830399914e+110\n",
      "Gradient Descent(48/49): loss=9.06760998477303e+112\n",
      "Gradient Descent(49/49): loss=2.4715509493801437e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.639124347663139\n",
      "Gradient Descent(2/49): loss=336.6854521677406\n",
      "Gradient Descent(3/49): loss=23311.887133271277\n",
      "Gradient Descent(4/49): loss=1866638.8108325314\n",
      "Gradient Descent(5/49): loss=210965768.32559407\n",
      "Gradient Descent(6/49): loss=37911604484.04231\n",
      "Gradient Descent(7/49): loss=9112968231854.328\n",
      "Gradient Descent(8/49): loss=2427406585956642.5\n",
      "Gradient Descent(9/49): loss=6.648315822290391e+17\n",
      "Gradient Descent(10/49): loss=1.8335634512975833e+20\n",
      "Gradient Descent(11/49): loss=5.065429772152227e+22\n",
      "Gradient Descent(12/49): loss=1.399959188931135e+25\n",
      "Gradient Descent(13/49): loss=3.8695261984445453e+27\n",
      "Gradient Descent(14/49): loss=1.0695736951056684e+30\n",
      "Gradient Descent(15/49): loss=2.9564203442765396e+32\n",
      "Gradient Descent(16/49): loss=8.17188541157541e+34\n",
      "Gradient Descent(17/49): loss=2.2588037698117727e+37\n",
      "Gradient Descent(18/49): loss=6.243595742778034e+39\n",
      "Gradient Descent(19/49): loss=1.7258023520835585e+42\n",
      "Gradient Descent(20/49): loss=4.770318091969157e+44\n",
      "Gradient Descent(21/49): loss=1.3185713125353251e+47\n",
      "Gradient Descent(22/49): loss=3.6446842185836317e+49\n",
      "Gradient Descent(23/49): loss=1.0074330396722209e+52\n",
      "Gradient Descent(24/49): loss=2.7846619036719594e+54\n",
      "Gradient Descent(25/49): loss=7.697128853662297e+56\n",
      "Gradient Descent(26/49): loss=2.1275757933777885e+59\n",
      "Gradient Descent(27/49): loss=5.880866544691547e+61\n",
      "Gradient Descent(28/49): loss=1.6255398009378155e+64\n",
      "Gradient Descent(29/49): loss=4.4931807657125824e+66\n",
      "Gradient Descent(30/49): loss=1.2419673379711736e+69\n",
      "Gradient Descent(31/49): loss=3.432941938053878e+71\n",
      "Gradient Descent(32/49): loss=9.489050146279079e+73\n",
      "Gradient Descent(33/49): loss=2.6228836462536377e+76\n",
      "Gradient Descent(34/49): loss=7.249954964652092e+78\n",
      "Gradient Descent(35/49): loss=2.0039717379213547e+81\n",
      "Gradient Descent(36/49): loss=5.539210582641414e+83\n",
      "Gradient Descent(37/49): loss=1.5311021257551585e+86\n",
      "Gradient Descent(38/49): loss=4.232144065507154e+88\n",
      "Gradient Descent(39/49): loss=1.1698137629044861e+91\n",
      "Gradient Descent(40/49): loss=3.2335010781745893e+93\n",
      "Gradient Descent(41/49): loss=8.937772450716146e+95\n",
      "Gradient Descent(42/49): loss=2.4705040898231776e+98\n",
      "Gradient Descent(43/49): loss=6.828760176529376e+100\n",
      "Gradient Descent(44/49): loss=1.8875486076159593e+103\n",
      "Gradient Descent(45/49): loss=5.217403531549513e+105\n",
      "Gradient Descent(46/49): loss=1.442150920044741e+108\n",
      "Gradient Descent(47/49): loss=3.986272604005908e+110\n",
      "Gradient Descent(48/49): loss=1.1018520359128119e+113\n",
      "Gradient Descent(49/49): loss=3.0456469731275807e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.733743950722401\n",
      "Gradient Descent(2/49): loss=347.55969588881476\n",
      "Gradient Descent(3/49): loss=25169.390286133468\n",
      "Gradient Descent(4/49): loss=2307461.3054877636\n",
      "Gradient Descent(5/49): loss=330677368.612041\n",
      "Gradient Descent(6/49): loss=71775313247.18073\n",
      "Gradient Descent(7/49): loss=18795677793161.195\n",
      "Gradient Descent(8/49): loss=5202557025838853.0\n",
      "Gradient Descent(9/49): loss=1.4603391041183972e+18\n",
      "Gradient Descent(10/49): loss=4.113011095857613e+20\n",
      "Gradient Descent(11/49): loss=1.1593576829798954e+23\n",
      "Gradient Descent(12/49): loss=3.2685778333664827e+25\n",
      "Gradient Descent(13/49): loss=9.215527535354368e+27\n",
      "Gradient Descent(14/49): loss=2.598282297169462e+30\n",
      "Gradient Descent(15/49): loss=7.32577545754146e+32\n",
      "Gradient Descent(16/49): loss=2.0654807044328123e+35\n",
      "Gradient Descent(17/49): loss=5.823562568530795e+37\n",
      "Gradient Descent(18/49): loss=1.6419365292233158e+40\n",
      "Gradient Descent(19/49): loss=4.629392329900954e+42\n",
      "Gradient Descent(20/49): loss=1.3052437171480795e+45\n",
      "Gradient Descent(21/49): loss=3.6800967396247035e+47\n",
      "Gradient Descent(22/49): loss=1.0375925841754166e+50\n",
      "Gradient Descent(23/49): loss=2.925462146614456e+52\n",
      "Gradient Descent(24/49): loss=8.248255530980773e+54\n",
      "Gradient Descent(25/49): loss=2.3255716838857357e+57\n",
      "Gradient Descent(26/49): loss=6.55688179952647e+59\n",
      "Gradient Descent(27/49): loss=1.848693774131642e+62\n",
      "Gradient Descent(28/49): loss=5.212338387371881e+64\n",
      "Gradient Descent(29/49): loss=1.4696036652816007e+67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(30/49): loss=4.143504838906034e+69\n",
      "Gradient Descent(31/49): loss=1.1682491514981152e+72\n",
      "Gradient Descent(32/49): loss=3.2938445423329946e+74\n",
      "Gradient Descent(33/49): loss=9.286898993372745e+76\n",
      "Gradient Descent(34/49): loss=2.61841419061089e+79\n",
      "Gradient Descent(35/49): loss=7.382542739492587e+81\n",
      "Gradient Descent(36/49): loss=2.0814864774208713e+84\n",
      "Gradient Descent(37/49): loss=5.868690651134141e+86\n",
      "Gradient Descent(38/49): loss=1.6546602791955465e+89\n",
      "Gradient Descent(39/49): loss=4.665266585517442e+91\n",
      "Gradient Descent(40/49): loss=1.3153583601177404e+94\n",
      "Gradient Descent(41/49): loss=3.708614682176219e+96\n",
      "Gradient Descent(42/49): loss=1.0456331352637307e+99\n",
      "Gradient Descent(43/49): loss=2.9481322468364014e+101\n",
      "Gradient Descent(44/49): loss=8.312173219954876e+103\n",
      "Gradient Descent(45/49): loss=2.3435930905975045e+106\n",
      "Gradient Descent(46/49): loss=6.607692632187909e+108\n",
      "Gradient Descent(47/49): loss=1.8630197407834654e+111\n",
      "Gradient Descent(48/49): loss=5.252730034144625e+113\n",
      "Gradient Descent(49/49): loss=1.480991972742182e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.7822465663177915\n",
      "Gradient Descent(2/49): loss=354.3619666034715\n",
      "Gradient Descent(3/49): loss=25635.406728721413\n",
      "Gradient Descent(4/49): loss=2268015.137009583\n",
      "Gradient Descent(5/49): loss=300443768.24657667\n",
      "Gradient Descent(6/49): loss=60639732361.361145\n",
      "Gradient Descent(7/49): loss=15165627067479.17\n",
      "Gradient Descent(8/49): loss=4062932345748011.0\n",
      "Gradient Descent(9/49): loss=1.1086023536194044e+18\n",
      "Gradient Descent(10/49): loss=3.038908622642602e+20\n",
      "Gradient Descent(11/49): loss=8.339842282631647e+22\n",
      "Gradient Descent(12/49): loss=2.2893986287070288e+25\n",
      "Gradient Descent(13/49): loss=6.28514816543981e+27\n",
      "Gradient Descent(14/49): loss=1.7255087668848046e+30\n",
      "Gradient Descent(15/49): loss=4.737188724047201e+32\n",
      "Gradient Descent(16/49): loss=1.3005428480348802e+35\n",
      "Gradient Descent(17/49): loss=3.5704976995698785e+37\n",
      "Gradient Descent(18/49): loss=9.802410332396325e+39\n",
      "Gradient Descent(19/49): loss=2.6911444273103544e+42\n",
      "Gradient Descent(20/49): loss=7.388242393299287e+44\n",
      "Gradient Descent(21/49): loss=2.0283610631168727e+47\n",
      "Gradient Descent(22/49): loss=5.568643236583885e+49\n",
      "Gradient Descent(23/49): loss=1.528810035949136e+52\n",
      "Gradient Descent(24/49): loss=4.197180581974876e+54\n",
      "Gradient Descent(25/49): loss=1.152289978711127e+57\n",
      "Gradient Descent(26/49): loss=3.163485985667427e+59\n",
      "Gradient Descent(27/49): loss=8.685004440210547e+61\n",
      "Gradient Descent(28/49): loss=2.3843728870057578e+64\n",
      "Gradient Descent(29/49): loss=6.546034723904419e+66\n",
      "Gradient Descent(30/49): loss=1.797142168495873e+69\n",
      "Gradient Descent(31/49): loss=4.933857075325273e+71\n",
      "Gradient Descent(32/49): loss=1.3545364449442153e+74\n",
      "Gradient Descent(33/49): loss=3.718731517088272e+76\n",
      "Gradient Descent(34/49): loss=1.0209370259325088e+79\n",
      "Gradient Descent(35/49): loss=2.8028708341280186e+81\n",
      "Gradient Descent(36/49): loss=7.694975021235839e+83\n",
      "Gradient Descent(37/49): loss=2.1125711487117301e+86\n",
      "Gradient Descent(38/49): loss=5.799832807842452e+88\n",
      "Gradient Descent(39/49): loss=1.5922806017415547e+91\n",
      "Gradient Descent(40/49): loss=4.371432071721347e+93\n",
      "Gradient Descent(41/49): loss=1.2001288175446729e+96\n",
      "Gradient Descent(42/49): loss=3.2948222803655025e+98\n",
      "Gradient Descent(43/49): loss=9.045573858815335e+100\n",
      "Gradient Descent(44/49): loss=2.4833632734268234e+103\n",
      "Gradient Descent(45/49): loss=6.817801992512755e+105\n",
      "Gradient Descent(46/49): loss=1.8717528968272884e+108\n",
      "Gradient Descent(47/49): loss=5.138692661694778e+110\n",
      "Gradient Descent(48/49): loss=1.4107718126743919e+113\n",
      "Gradient Descent(49/49): loss=3.873119562632415e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.897712817436212\n",
      "Gradient Descent(2/49): loss=360.2313038938742\n",
      "Gradient Descent(3/49): loss=25776.611979873942\n",
      "Gradient Descent(4/49): loss=2223954.619467642\n",
      "Gradient Descent(5/49): loss=283823527.1445822\n",
      "Gradient Descent(6/49): loss=55849906079.6656\n",
      "Gradient Descent(7/49): loss=13842568457598.129\n",
      "Gradient Descent(8/49): loss=3700178642066672.0\n",
      "Gradient Descent(9/49): loss=1.0092590726452113e+18\n",
      "Gradient Descent(10/49): loss=2.7668832196692446e+20\n",
      "Gradient Descent(11/49): loss=7.5949695851502e+22\n",
      "Gradient Descent(12/49): loss=2.085432968303803e+25\n",
      "Gradient Descent(13/49): loss=5.726637689969475e+27\n",
      "Gradient Descent(14/49): loss=1.5725750491413536e+30\n",
      "Gradient Descent(15/49): loss=4.3184219570879595e+32\n",
      "Gradient Descent(16/49): loss=1.1858759680573189e+35\n",
      "Gradient Descent(17/49): loss=3.2565187539844437e+37\n",
      "Gradient Descent(18/49): loss=8.942684917615497e+39\n",
      "Gradient Descent(19/49): loss=2.455739424519261e+42\n",
      "Gradient Descent(20/49): loss=6.743675084391289e+44\n",
      "Gradient Descent(21/49): loss=1.8518721179121487e+47\n",
      "Gradient Descent(22/49): loss=5.085402690765786e+49\n",
      "Gradient Descent(23/49): loss=1.3964960257645525e+52\n",
      "Gradient Descent(24/49): loss=3.8349001418272936e+54\n",
      "Gradient Descent(25/49): loss=1.0530970963409532e+57\n",
      "Gradient Descent(26/49): loss=2.8918966682515695e+59\n",
      "Gradient Descent(27/49): loss=7.941401005571644e+61\n",
      "Gradient Descent(28/49): loss=2.1807781247394483e+64\n",
      "Gradient Descent(29/49): loss=5.988607332642479e+66\n",
      "Gradient Descent(30/49): loss=1.64452391454839e+69\n",
      "Gradient Descent(31/49): loss=4.516006402323618e+71\n",
      "Gradient Descent(32/49): loss=1.2401348284088938e+74\n",
      "Gradient Descent(33/49): loss=3.4055186277889046e+76\n",
      "Gradient Descent(34/49): loss=9.351851797515547e+78\n",
      "Gradient Descent(35/49): loss=2.5681002396829693e+81\n",
      "Gradient Descent(36/49): loss=7.05222771260323e+83\n",
      "Gradient Descent(37/49): loss=1.9366033670301088e+86\n",
      "Gradient Descent(38/49): loss=5.318082106863699e+88\n",
      "Gradient Descent(39/49): loss=1.4603918270944688e+91\n",
      "Gradient Descent(40/49): loss=4.010363596853355e+93\n",
      "Gradient Descent(41/49): loss=1.1012808946599414e+96\n",
      "Gradient Descent(42/49): loss=3.024213589746854e+98\n",
      "Gradient Descent(43/49): loss=8.304754836624538e+100\n",
      "Gradient Descent(44/49): loss=2.280558262494012e+103\n",
      "Gradient Descent(45/49): loss=6.262612311796633e+105\n",
      "Gradient Descent(46/49): loss=1.719768076653998e+108\n",
      "Gradient Descent(47/49): loss=4.722633447877829e+110\n",
      "Gradient Descent(48/49): loss=1.2968764210583118e+113\n",
      "Gradient Descent(49/49): loss=3.561336000474066e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.681272876718548\n",
      "Gradient Descent(2/49): loss=342.0399596935617\n",
      "Gradient Descent(3/49): loss=23868.368356499126\n",
      "Gradient Descent(4/49): loss=1925737.2740695165\n",
      "Gradient Descent(5/49): loss=219194963.1390781\n",
      "Gradient Descent(6/49): loss=39662997245.065475\n",
      "Gradient Descent(7/49): loss=9602354155457.51\n",
      "Gradient Descent(8/49): loss=2576626415019440.0\n",
      "Gradient Descent(9/49): loss=7.109575408728593e+17\n",
      "Gradient Descent(10/49): loss=1.9754253771278487e+20\n",
      "Gradient Descent(11/49): loss=5.498146671723397e+22\n",
      "Gradient Descent(12/49): loss=1.530916360809856e+25\n",
      "Gradient Descent(13/49): loss=4.263145373405934e+27\n",
      "Gradient Descent(14/49): loss=1.1871876731261144e+30\n",
      "Gradient Descent(15/49): loss=3.306063380697843e+32\n",
      "Gradient Descent(16/49): loss=9.206691537573742e+34\n",
      "Gradient Descent(17/49): loss=2.563870945427181e+37\n",
      "Gradient Descent(18/49): loss=7.139844699510537e+39\n",
      "Gradient Descent(19/49): loss=1.9882975569953633e+42\n",
      "Gradient Descent(20/49): loss=5.5369932555676915e+44\n",
      "Gradient Descent(21/49): loss=1.5419369319795432e+47\n",
      "Gradient Descent(22/49): loss=4.2939721851800363e+49\n",
      "Gradient Descent(23/49): loss=1.1957815359366931e+52\n",
      "Gradient Descent(24/49): loss=3.3300017327699383e+54\n",
      "Gradient Descent(25/49): loss=9.27335906019842e+56\n",
      "Gradient Descent(26/49): loss=2.5824367420951086e+59\n",
      "Gradient Descent(27/49): loss=7.191546756285815e+61\n",
      "Gradient Descent(28/49): loss=2.0026955125293517e+64\n",
      "Gradient Descent(29/49): loss=5.5770885622059694e+66\n",
      "Gradient Descent(30/49): loss=1.5531026377248178e+69\n",
      "Gradient Descent(31/49): loss=4.325066343134528e+71\n",
      "Gradient Descent(32/49): loss=1.204440609277344e+74\n",
      "Gradient Descent(33/49): loss=3.3541154428280866e+76\n",
      "Gradient Descent(34/49): loss=9.340510704440572e+78\n",
      "Gradient Descent(35/49): loss=2.6011370719609595e+81\n",
      "Gradient Descent(36/49): loss=7.243623267743899e+83\n",
      "Gradient Descent(37/49): loss=2.0171977328916702e+86\n",
      "Gradient Descent(38/49): loss=5.617474215843136e+88\n",
      "Gradient Descent(39/49): loss=1.5643491984509913e+91\n",
      "Gradient Descent(40/49): loss=4.356385664917397e+93\n",
      "Gradient Descent(41/49): loss=1.2131623860126552e+96\n",
      "Gradient Descent(42/49): loss=3.3784037687209897e+98\n",
      "Gradient Descent(43/49): loss=9.408148617285897e+100\n",
      "Gradient Descent(44/49): loss=2.6199728174719662e+103\n",
      "Gradient Descent(45/49): loss=7.2960768834797e+105\n",
      "Gradient Descent(46/49): loss=2.0318049689160862e+108\n",
      "Gradient Descent(47/49): loss=5.658152316157072e+110\n",
      "Gradient Descent(48/49): loss=1.5756771994663073e+113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(49/49): loss=4.3879317808895624e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.776476890115302\n",
      "Gradient Descent(2/49): loss=353.0627102740033\n",
      "Gradient Descent(3/49): loss=25766.235232420207\n",
      "Gradient Descent(4/49): loss=2379678.493044591\n",
      "Gradient Descent(5/49): loss=343405560.9590664\n",
      "Gradient Descent(6/49): loss=75062110254.03319\n",
      "Gradient Descent(7/49): loss=19799409955095.723\n",
      "Gradient Descent(8/49): loss=5520917641000363.0\n",
      "Gradient Descent(9/49): loss=1.5612227344361126e+18\n",
      "Gradient Descent(10/49): loss=4.429885270313851e+20\n",
      "Gradient Descent(11/49): loss=1.2579772066933787e+23\n",
      "Gradient Descent(12/49): loss=3.573034690637144e+25\n",
      "Gradient Descent(13/49): loss=1.0148965293521375e+28\n",
      "Gradient Descent(14/49): loss=2.8827772685824766e+30\n",
      "Gradient Descent(15/49): loss=8.18844717950456e+32\n",
      "Gradient Descent(16/49): loss=2.3259066861242226e+35\n",
      "Gradient Descent(17/49): loss=6.606677191032421e+37\n",
      "Gradient Descent(18/49): loss=1.8766094668564137e+40\n",
      "Gradient Descent(19/49): loss=5.3304604102695045e+42\n",
      "Gradient Descent(20/49): loss=1.5141034266775966e+45\n",
      "Gradient Descent(21/49): loss=4.3007714368686606e+47\n",
      "Gradient Descent(22/49): loss=1.2216229504812483e+50\n",
      "Gradient Descent(23/49): loss=3.469988245341617e+52\n",
      "Gradient Descent(24/49): loss=9.856411438688363e+54\n",
      "Gradient Descent(25/49): loss=2.799688055983484e+57\n",
      "Gradient Descent(26/49): loss=7.952441169463408e+59\n",
      "Gradient Descent(27/49): loss=2.25887024872729e+62\n",
      "Gradient Descent(28/49): loss=6.416262241811318e+64\n",
      "Gradient Descent(29/49): loss=1.8225226162897038e+67\n",
      "Gradient Descent(30/49): loss=5.176828130936392e+69\n",
      "Gradient Descent(31/49): loss=1.4704645779273504e+72\n",
      "Gradient Descent(32/49): loss=4.176816421656893e+74\n",
      "Gradient Descent(33/49): loss=1.186413850567772e+77\n",
      "Gradient Descent(34/49): loss=3.3699777120217834e+79\n",
      "Gradient Descent(35/49): loss=9.572334117718335e+81\n",
      "Gradient Descent(36/49): loss=2.7189966311754067e+84\n",
      "Gradient Descent(37/49): loss=7.723239274169225e+86\n",
      "Gradient Descent(38/49): loss=2.193766045980139e+89\n",
      "Gradient Descent(39/49): loss=6.23133544572588e+91\n",
      "Gradient Descent(40/49): loss=1.76999464041805e+94\n",
      "Gradient Descent(41/49): loss=5.027623780481234e+96\n",
      "Gradient Descent(42/49): loss=1.4280834699075888e+99\n",
      "Gradient Descent(43/49): loss=4.0564339856552233e+101\n",
      "Gradient Descent(44/49): loss=1.1522195324509765e+104\n",
      "Gradient Descent(45/49): loss=3.2728496399951113e+106\n",
      "Gradient Descent(46/49): loss=9.296444353126439e+108\n",
      "Gradient Descent(47/49): loss=2.6406308604786956e+111\n",
      "Gradient Descent(48/49): loss=7.500643338942348e+113\n",
      "Gradient Descent(49/49): loss=2.1305382490235838e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.825350352255026\n",
      "Gradient Descent(2/49): loss=359.969972127231\n",
      "Gradient Descent(3/49): loss=26243.49566849479\n",
      "Gradient Descent(4/49): loss=2339156.035628714\n",
      "Gradient Descent(5/49): loss=312045280.75946754\n",
      "Gradient Descent(6/49): loss=63422396342.89184\n",
      "Gradient Descent(7/49): loss=15976491345446.936\n",
      "Gradient Descent(8/49): loss=4311780348882813.5\n",
      "Gradient Descent(9/49): loss=1.1852494382032015e+18\n",
      "Gradient Descent(10/49): loss=3.273216807755599e+20\n",
      "Gradient Descent(11/49): loss=9.049819379522584e+22\n",
      "Gradient Descent(12/49): loss=2.5028160262851626e+25\n",
      "Gradient Descent(13/49): loss=6.922271024707313e+27\n",
      "Gradient Descent(14/49): loss=1.914590306035603e+30\n",
      "Gradient Descent(15/49): loss=5.295475813629458e+32\n",
      "Gradient Descent(16/49): loss=1.4646524627542564e+35\n",
      "Gradient Descent(17/49): loss=4.0510191708562223e+37\n",
      "Gradient Descent(18/49): loss=1.1204540198915425e+40\n",
      "Gradient Descent(19/49): loss=3.099015773719618e+42\n",
      "Gradient Descent(20/49): loss=8.57143500199389e+44\n",
      "Gradient Descent(21/49): loss=2.37073649928579e+47\n",
      "Gradient Descent(22/49): loss=6.557118556129876e+49\n",
      "Gradient Descent(23/49): loss=1.8136053405641034e+52\n",
      "Gradient Descent(24/49): loss=5.0161733438495034e+54\n",
      "Gradient Descent(25/49): loss=1.3874019034269967e+57\n",
      "Gradient Descent(26/49): loss=3.837355509244405e+59\n",
      "Gradient Descent(27/49): loss=1.0613577268388734e+62\n",
      "Gradient Descent(28/49): loss=2.9355638840521375e+64\n",
      "Gradient Descent(29/49): loss=8.119350431468263e+66\n",
      "Gradient Descent(30/49): loss=2.2456963647469374e+69\n",
      "Gradient Descent(31/49): loss=6.21127540337686e+71\n",
      "Gradient Descent(32/49): loss=1.7179500640525236e+74\n",
      "Gradient Descent(33/49): loss=4.75160451100504e+76\n",
      "Gradient Descent(34/49): loss=1.3142259429674141e+79\n",
      "Gradient Descent(35/49): loss=3.634961253968589e+81\n",
      "Gradient Descent(36/49): loss=1.0053783665249553e+84\n",
      "Gradient Descent(37/49): loss=2.7807329686742057e+86\n",
      "Gradient Descent(38/49): loss=7.691110233253477e+88\n",
      "Gradient Descent(39/49): loss=2.1272512422600134e+91\n",
      "Gradient Descent(40/49): loss=5.883673111499106e+93\n",
      "Gradient Descent(41/49): loss=1.62733994909781e+96\n",
      "Gradient Descent(42/49): loss=4.5009898744271455e+98\n",
      "Gradient Descent(43/49): loss=1.2449095138927353e+101\n",
      "Gradient Descent(44/49): loss=3.4432419112648754e+103\n",
      "Gradient Descent(45/49): loss=9.523515345640396e+105\n",
      "Gradient Descent(46/49): loss=2.634068325025956e+108\n",
      "Gradient Descent(47/49): loss=7.285456776295641e+110\n",
      "Gradient Descent(48/49): loss=2.0150532898097568e+113\n",
      "Gradient Descent(49/49): loss=5.573349599690636e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.9417448685465715\n",
      "Gradient Descent(2/49): loss=365.9341211415016\n",
      "Gradient Descent(3/49): loss=26388.47735331465\n",
      "Gradient Descent(4/49): loss=2293811.632011722\n",
      "Gradient Descent(5/49): loss=294799243.65307283\n",
      "Gradient Descent(6/49): loss=58414464256.05677\n",
      "Gradient Descent(7/49): loss=14582831181581.467\n",
      "Gradient Descent(8/49): loss=3926818020958607.0\n",
      "Gradient Descent(9/49): loss=1.0790380236751654e+18\n",
      "Gradient Descent(10/49): loss=2.980216512644254e+20\n",
      "Gradient Descent(11/49): loss=8.241530689892951e+22\n",
      "Gradient Descent(12/49): loss=2.279835284138221e+25\n",
      "Gradient Descent(13/49): loss=6.307140206382843e+27\n",
      "Gradient Descent(14/49): loss=1.7448967795014562e+30\n",
      "Gradient Descent(15/49): loss=4.827352676320223e+32\n",
      "Gradient Descent(16/49): loss=1.3355151441982567e+35\n",
      "Gradient Descent(17/49): loss=3.6947812335034787e+37\n",
      "Gradient Descent(18/49): loss=1.0221830414930714e+40\n",
      "Gradient Descent(19/49): loss=2.8279297808009293e+42\n",
      "Gradient Descent(20/49): loss=7.82363486244171e+44\n",
      "Gradient Descent(21/49): loss=2.1644548227709993e+47\n",
      "Gradient Descent(22/49): loss=5.98809218911854e+49\n",
      "Gradient Descent(23/49): loss=1.6566410945813484e+52\n",
      "Gradient Descent(24/49): loss=4.583195498035315e+54\n",
      "Gradient Descent(25/49): loss=1.267968121882262e+57\n",
      "Gradient Descent(26/49): loss=3.507908747947463e+59\n",
      "Gradient Descent(27/49): loss=9.704836873708153e+61\n",
      "Gradient Descent(28/49): loss=2.684900478109574e+64\n",
      "Gradient Descent(29/49): loss=7.427935854215699e+66\n",
      "Gradient Descent(30/49): loss=2.0549823542506388e+69\n",
      "Gradient Descent(31/49): loss=5.685230135482207e+71\n",
      "Gradient Descent(32/49): loss=1.57285251751865e+74\n",
      "Gradient Descent(33/49): loss=4.3513894475881296e+76\n",
      "Gradient Descent(34/49): loss=1.203837608020149e+79\n",
      "Gradient Descent(35/49): loss=3.3304878911423944e+81\n",
      "Gradient Descent(36/49): loss=9.213991587526614e+83\n",
      "Gradient Descent(37/49): loss=2.54910522872049e+86\n",
      "Gradient Descent(38/49): loss=7.052250271084051e+88\n",
      "Gradient Descent(39/49): loss=1.951046717320814e+91\n",
      "Gradient Descent(40/49): loss=5.397686053168351e+93\n",
      "Gradient Descent(41/49): loss=1.493301747719155e+96\n",
      "Gradient Descent(42/49): loss=4.131307541371496e+98\n",
      "Gradient Descent(43/49): loss=1.142950647949243e+101\n",
      "Gradient Descent(44/49): loss=3.1620405175981734e+103\n",
      "Gradient Descent(45/49): loss=8.747971973131597e+105\n",
      "Gradient Descent(46/49): loss=2.4201781481543115e+108\n",
      "Gradient Descent(47/49): loss=6.695565882919633e+110\n",
      "Gradient Descent(48/49): loss=1.8523678732785036e+113\n",
      "Gradient Descent(49/49): loss=5.124685199062009e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.723590198902299\n",
      "Gradient Descent(2/49): loss=347.4581697682444\n",
      "Gradient Descent(3/49): loss=24435.878566897725\n",
      "Gradient Descent(4/49): loss=1986467.4158341219\n",
      "Gradient Descent(5/49): loss=227712177.29984105\n",
      "Gradient Descent(6/49): loss=41488309628.31174\n",
      "Gradient Descent(7/49): loss=10116055069434.113\n",
      "Gradient Descent(8/49): loss=2734413545849354.0\n",
      "Gradient Descent(9/49): loss=7.600944222466065e+17\n",
      "Gradient Descent(10/49): loss=2.127674418563245e+20\n",
      "Gradient Descent(11/49): loss=5.966012950670046e+22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(12/49): loss=1.6735680971281972e+25\n",
      "Gradient Descent(13/49): loss=4.6951158738719544e+27\n",
      "Gradient Descent(14/49): loss=1.3172246571109757e+30\n",
      "Gradient Descent(15/49): loss=3.695523450287586e+32\n",
      "Gradient Descent(16/49): loss=1.0367945312784642e+35\n",
      "Gradient Descent(17/49): loss=2.908771354168903e+37\n",
      "Gradient Descent(18/49): loss=8.160683000975972e+39\n",
      "Gradient Descent(19/49): loss=2.2895147229208194e+42\n",
      "Gradient Descent(20/49): loss=6.423332081340578e+44\n",
      "Gradient Descent(21/49): loss=1.8020934595434532e+47\n",
      "Gradient Descent(22/49): loss=5.055850759009016e+49\n",
      "Gradient Descent(23/49): loss=1.4184406898444649e+52\n",
      "Gradient Descent(24/49): loss=3.979496402373096e+54\n",
      "Gradient Descent(25/49): loss=1.1164648426931101e+57\n",
      "Gradient Descent(26/49): loss=3.132290166734842e+59\n",
      "Gradient Descent(27/49): loss=8.78777487068895e+61\n",
      "Gradient Descent(28/49): loss=2.465448060912558e+64\n",
      "Gradient Descent(29/49): loss=6.916920643167138e+66\n",
      "Gradient Descent(30/49): loss=1.940571855574344e+69\n",
      "Gradient Descent(31/49): loss=5.444357859399851e+71\n",
      "Gradient Descent(32/49): loss=1.5274380289534219e+74\n",
      "Gradient Descent(33/49): loss=4.285293128307127e+76\n",
      "Gradient Descent(34/49): loss=1.2022574302473415e+79\n",
      "Gradient Descent(35/49): loss=3.3729849634719292e+81\n",
      "Gradient Descent(36/49): loss=9.463054481989962e+83\n",
      "Gradient Descent(37/49): loss=2.6549006621402585e+86\n",
      "Gradient Descent(38/49): loss=7.448438069597552e+88\n",
      "Gradient Descent(39/49): loss=2.0896913571110564e+91\n",
      "Gradient Descent(40/49): loss=5.862719038785827e+93\n",
      "Gradient Descent(41/49): loss=1.6448110583784457e+96\n",
      "Gradient Descent(42/49): loss=4.614588213874859e+98\n",
      "Gradient Descent(43/49): loss=1.2946425837278995e+101\n",
      "Gradient Descent(44/49): loss=3.632175487646011e+103\n",
      "Gradient Descent(45/49): loss=1.0190224652636168e+106\n",
      "Gradient Descent(46/49): loss=2.858911383119586e+108\n",
      "Gradient Descent(47/49): loss=8.020798927545201e+110\n",
      "Gradient Descent(48/49): loss=2.2502696591424925e+113\n",
      "Gradient Descent(49/49): loss=6.313228376125068e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.819380241574855\n",
      "Gradient Descent(2/49): loss=358.6309054790845\n",
      "Gradient Descent(3/49): loss=26374.832767922217\n",
      "Gradient Descent(4/49): loss=2453867.183710612\n",
      "Gradient Descent(5/49): loss=356573320.69525176\n",
      "Gradient Descent(6/49): loss=78486395585.23561\n",
      "Gradient Descent(7/49): loss=20852730981665.625\n",
      "Gradient Descent(8/49): loss=5857473311670745.0\n",
      "Gradient Descent(9/49): loss=1.6686634795404014e+18\n",
      "Gradient Descent(10/49): loss=4.769863031388549e+20\n",
      "Gradient Descent(11/49): loss=1.3645737647067917e+23\n",
      "Gradient Descent(12/49): loss=3.904564807313614e+25\n",
      "Gradient Descent(13/49): loss=1.117296448283099e+28\n",
      "Gradient Descent(14/49): loss=3.197194163293315e+30\n",
      "Gradient Descent(15/49): loss=9.148939429140685e+32\n",
      "Gradient Descent(16/49): loss=2.6180188368085334e+35\n",
      "Gradient Descent(17/49): loss=7.491604588410328e+37\n",
      "Gradient Descent(18/49): loss=2.1437638466741593e+40\n",
      "Gradient Descent(19/49): loss=6.134498177800125e+42\n",
      "Gradient Descent(20/49): loss=1.7554204036350133e+45\n",
      "Gradient Descent(21/49): loss=5.023232065094691e+47\n",
      "Gradient Descent(22/49): loss=1.4374254925188715e+50\n",
      "Gradient Descent(23/49): loss=4.1132721320161717e+52\n",
      "Gradient Descent(24/49): loss=1.1770354512416942e+55\n",
      "Gradient Descent(25/49): loss=3.3681517026271954e+57\n",
      "Gradient Descent(26/49): loss=9.63815140822378e+59\n",
      "Gradient Descent(27/49): loss=2.7580100532704524e+62\n",
      "Gradient Descent(28/49): loss=7.892197509421191e+64\n",
      "Gradient Descent(29/49): loss=2.2583957391255607e+67\n",
      "Gradient Descent(30/49): loss=6.462523661391071e+69\n",
      "Gradient Descent(31/49): loss=1.849286701639382e+72\n",
      "Gradient Descent(32/49): loss=5.291835641997672e+74\n",
      "Gradient Descent(33/49): loss=1.5142878839225752e+77\n",
      "Gradient Descent(34/49): loss=4.333218093918558e+79\n",
      "Gradient Descent(35/49): loss=1.2399741983554696e+82\n",
      "Gradient Descent(36/49): loss=3.5482543902998387e+84\n",
      "Gradient Descent(37/49): loss=1.0153525158007175e+87\n",
      "Gradient Descent(38/49): loss=2.9054870872877554e+89\n",
      "Gradient Descent(39/49): loss=8.314211156249111e+91\n",
      "Gradient Descent(40/49): loss=2.3791572660275053e+94\n",
      "Gradient Descent(41/49): loss=6.808089414756943e+96\n",
      "Gradient Descent(42/49): loss=1.9481722432211009e+99\n",
      "Gradient Descent(43/49): loss=5.574802059782718e+101\n",
      "Gradient Descent(44/49): loss=1.595260281214804e+104\n",
      "Gradient Descent(45/49): loss=4.5649250637621015e+106\n",
      "Gradient Descent(46/49): loss=1.3062784225966283e+109\n",
      "Gradient Descent(47/49): loss=3.737987575934673e+111\n",
      "Gradient Descent(48/49): loss=1.0696457107564604e+114\n",
      "Gradient Descent(49/49): loss=3.0608500517918506e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.8686259534891825\n",
      "Gradient Descent(2/49): loss=365.6443675659378\n",
      "Gradient Descent(3/49): loss=26863.561742968042\n",
      "Gradient Descent(4/49): loss=2412243.207369318\n",
      "Gradient Descent(5/49): loss=324048737.0391689\n",
      "Gradient Descent(6/49): loss=66321708585.499825\n",
      "Gradient Descent(7/49): loss=16827465393499.77\n",
      "Gradient Descent(8/49): loss=4574863494956923.0\n",
      "Gradient Descent(9/49): loss=1.266882383039623e+18\n",
      "Gradient Descent(10/49): loss=3.524622067613434e+20\n",
      "Gradient Descent(11/49): loss=9.817269323440311e+22\n",
      "Gradient Descent(12/49): loss=2.7352260401465475e+25\n",
      "Gradient Descent(13/49): loss=7.621255648263224e+27\n",
      "Gradient Descent(14/49): loss=2.1235742898596856e+30\n",
      "Gradient Descent(15/49): loss=5.917118595483584e+32\n",
      "Gradient Descent(16/49): loss=1.648745244253862e+35\n",
      "Gradient Descent(17/49): loss=4.594063072878744e+37\n",
      "Gradient Descent(18/49): loss=1.2800896306467096e+40\n",
      "Gradient Descent(19/49): loss=3.566841515240351e+42\n",
      "Gradient Descent(20/49): loss=9.9386465928471e+44\n",
      "Gradient Descent(21/49): loss=2.769304320757848e+47\n",
      "Gradient Descent(22/49): loss=7.716389099077671e+49\n",
      "Gradient Descent(23/49): loss=2.1500945304455678e+52\n",
      "Gradient Descent(24/49): loss=5.991023042726794e+54\n",
      "Gradient Descent(25/49): loss=1.6693385611831306e+57\n",
      "Gradient Descent(26/49): loss=4.651444689799048e+59\n",
      "Gradient Descent(27/49): loss=1.2960784711598626e+62\n",
      "Gradient Descent(28/49): loss=3.611392836914086e+64\n",
      "Gradient Descent(29/49): loss=1.006278440135109e+67\n",
      "Gradient Descent(30/49): loss=2.8038940785682867e+69\n",
      "Gradient Descent(31/49): loss=7.812769995126645e+71\n",
      "Gradient Descent(32/49): loss=2.176950101764157e+74\n",
      "Gradient Descent(33/49): loss=6.06585340222117e+76\n",
      "Gradient Descent(34/49): loss=1.6901892913126942e+79\n",
      "Gradient Descent(35/49): loss=4.70954316077276e+81\n",
      "Gradient Descent(36/49): loss=1.3122670281477935e+84\n",
      "Gradient Descent(37/49): loss=3.656500629418322e+86\n",
      "Gradient Descent(38/49): loss=1.0188472746897715e+89\n",
      "Gradient Descent(39/49): loss=2.838915877085294e+91\n",
      "Gradient Descent(40/49): loss=7.910354728701621e+93\n",
      "Gradient Descent(41/49): loss=2.2041411103077757e+96\n",
      "Gradient Descent(42/49): loss=6.14161842391374e+98\n",
      "Gradient Descent(43/49): loss=1.7113004556995345e+101\n",
      "Gradient Descent(44/49): loss=4.768367305716062e+103\n",
      "Gradient Descent(45/49): loss=1.3286577869183876e+106\n",
      "Gradient Descent(46/49): loss=3.7021718369360323e+108\n",
      "Gradient Descent(47/49): loss=1.0315730991944587e+111\n",
      "Gradient Descent(48/49): loss=2.8743751123728545e+113\n",
      "Gradient Descent(49/49): loss=8.009158336021041e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.985952393138692\n",
      "Gradient Descent(2/49): loss=371.70447141981873\n",
      "Gradient Descent(3/49): loss=27012.402353111396\n",
      "Gradient Descent(4/49): loss=2365582.4228124805\n",
      "Gradient Descent(5/49): loss=306155784.660852\n",
      "Gradient Descent(6/49): loss=61086601713.54275\n",
      "Gradient Descent(7/49): loss=15359718578296.777\n",
      "Gradient Descent(8/49): loss=4166422693613624.0\n",
      "Gradient Descent(9/49): loss=1.1533560891811692e+18\n",
      "Gradient Descent(10/49): loss=3.209116308627153e+20\n",
      "Gradient Descent(11/49): loss=8.940430745334503e+22\n",
      "Gradient Descent(12/49): loss=2.4915378835860095e+25\n",
      "Gradient Descent(13/49): loss=6.944006484086403e+27\n",
      "Gradient Descent(14/49): loss=1.9353567047393922e+30\n",
      "Gradient Descent(15/49): loss=5.394037576180381e+32\n",
      "Gradient Descent(16/49): loss=1.5033753236903822e+35\n",
      "Gradient Descent(17/49): loss=4.19006723371629e+37\n",
      "Gradient Descent(18/49): loss=1.1678164717332736e+40\n",
      "Gradient Descent(19/49): loss=3.2548292409619257e+42\n",
      "Gradient Descent(20/49): loss=9.071556781159514e+44\n",
      "Gradient Descent(21/49): loss=2.5283397815554603e+47\n",
      "Gradient Descent(22/49): loss=7.046753062210792e+49\n",
      "Gradient Descent(23/49): loss=1.9640053558160473e+52\n",
      "Gradient Descent(24/49): loss=5.473892732840083e+54\n",
      "Gradient Descent(25/49): loss=1.5256323798723243e+57\n",
      "Gradient Descent(26/49): loss=4.252100419420372e+59\n",
      "Gradient Descent(27/49): loss=1.1851058102445616e+62\n",
      "Gradient Descent(28/49): loss=3.3030164928865045e+64\n",
      "Gradient Descent(29/49): loss=9.205859812660062e+66\n",
      "Gradient Descent(30/49): loss=2.5657714720124973e+69\n",
      "Gradient Descent(31/49): loss=7.151079182783084e+71\n",
      "Gradient Descent(32/49): loss=1.9930821601318322e+74\n",
      "Gradient Descent(33/49): loss=5.554932892645957e+76\n",
      "Gradient Descent(34/49): loss=1.5482191381291934e+79\n",
      "Gradient Descent(35/49): loss=4.315052127529523e+81\n",
      "Gradient Descent(36/49): loss=1.2026511237805614e+84\n",
      "Gradient Descent(37/49): loss=3.3519171560014e+86\n",
      "Gradient Descent(38/49): loss=9.342151184607949e+88\n",
      "Gradient Descent(39/49): loss=2.603757333316251e+91\n",
      "Gradient Descent(40/49): loss=7.256949836101946e+93\n",
      "Gradient Descent(41/49): loss=2.0225894421822593e+96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=5.637172839856076e+98\n",
      "Gradient Descent(43/49): loss=1.571140289950523e+101\n",
      "Gradient Descent(44/49): loss=4.378935826223166e+103\n",
      "Gradient Descent(45/49): loss=1.2204561930484723e+106\n",
      "Gradient Descent(46/49): loss=3.4015417860897406e+108\n",
      "Gradient Descent(47/49): loss=9.480460329849139e+110\n",
      "Gradient Descent(48/49): loss=2.642305569591832e+113\n",
      "Gradient Descent(49/49): loss=7.364387888543773e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.766076314214396\n",
      "Gradient Descent(2/49): loss=352.9405858127206\n",
      "Gradient Descent(3/49): loss=25014.591986032454\n",
      "Gradient Descent(4/49): loss=2048867.7198697107\n",
      "Gradient Descent(5/49): loss=236526344.1322728\n",
      "Gradient Descent(6/49): loss=43390368520.40472\n",
      "Gradient Descent(7/49): loss=10655182090025.969\n",
      "Gradient Descent(8/49): loss=2901225348503047.5\n",
      "Gradient Descent(9/49): loss=8.124265051653829e+17\n",
      "Gradient Descent(10/49): loss=2.2910283450372525e+20\n",
      "Gradient Descent(11/49): loss=6.471737416645193e+22\n",
      "Gradient Descent(12/49): loss=1.8289094923514201e+25\n",
      "Gradient Descent(13/49): loss=5.169011096164361e+27\n",
      "Gradient Descent(14/49): loss=1.460943353021951e+30\n",
      "Gradient Descent(15/49): loss=4.12916157236382e+32\n",
      "Gradient Descent(16/49): loss=1.1670541443790867e+35\n",
      "Gradient Descent(17/49): loss=3.2985288420376656e+37\n",
      "Gradient Descent(18/49): loss=9.322869466741533e+39\n",
      "Gradient Descent(19/49): loss=2.6349897500773323e+42\n",
      "Gradient Descent(20/49): loss=7.447461366508795e+44\n",
      "Gradient Descent(21/49): loss=2.104929663258424e+47\n",
      "Gradient Descent(22/49): loss=5.949314364355117e+49\n",
      "Gradient Descent(23/49): loss=1.6814975828537393e+52\n",
      "Gradient Descent(24/49): loss=4.7525377682721585e+54\n",
      "Gradient Descent(25/49): loss=1.3432439909081735e+57\n",
      "Gradient Descent(26/49): loss=3.796507270617144e+59\n",
      "Gradient Descent(27/49): loss=1.0730342032728591e+62\n",
      "Gradient Descent(28/49): loss=3.032793879534243e+64\n",
      "Gradient Descent(29/49): loss=8.571803850880365e+66\n",
      "Gradient Descent(30/49): loss=2.4227106811904665e+69\n",
      "Gradient Descent(31/49): loss=6.847481751640279e+71\n",
      "Gradient Descent(32/49): loss=1.9353531027488654e+74\n",
      "Gradient Descent(33/49): loss=5.470027914163162e+76\n",
      "Gradient Descent(34/49): loss=1.546033400273392e+79\n",
      "Gradient Descent(35/49): loss=4.36966558904043e+81\n",
      "Gradient Descent(36/49): loss=1.2350300683457116e+84\n",
      "Gradient Descent(37/49): loss=3.490654464596979e+86\n",
      "Gradient Descent(38/49): loss=9.865888210747706e+88\n",
      "Gradient Descent(39/49): loss=2.788467067541988e+91\n",
      "Gradient Descent(40/49): loss=7.881245378693366e+93\n",
      "Gradient Descent(41/49): loss=2.227533164805465e+96\n",
      "Gradient Descent(42/49): loss=6.295837474776104e+98\n",
      "Gradient Descent(43/49): loss=1.7794379062480297e+101\n",
      "Gradient Descent(44/49): loss=5.029353560790531e+103\n",
      "Gradient Descent(45/49): loss=1.4214824327739443e+106\n",
      "Gradient Descent(46/49): loss=4.017638215849177e+108\n",
      "Gradient Descent(47/49): loss=1.1355340355457392e+111\n",
      "Gradient Descent(48/49): loss=3.209441658524891e+113\n",
      "Gradient Descent(49/49): loss=9.071076195901724e+115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.862454005101055\n",
      "Gradient Descent(2/49): loss=364.2647943604039\n",
      "Gradient Descent(3/49): loss=26995.367375187925\n",
      "Gradient Descent(4/49): loss=2530073.3658010247\n",
      "Gradient Descent(5/49): loss=370194100.07529056\n",
      "Gradient Descent(6/49): loss=82053379516.20012\n",
      "Gradient Descent(7/49): loss=21957894008753.51\n",
      "Gradient Descent(8/49): loss=6213190959177632.0\n",
      "Gradient Descent(9/49): loss=1.783060944469122e+18\n",
      "Gradient Descent(10/49): loss=5.134534166861209e+20\n",
      "Gradient Descent(11/49): loss=1.479759529096849e+23\n",
      "Gradient Descent(12/49): loss=4.265462153604111e+25\n",
      "Gradient Descent(13/49): loss=1.2295927532907465e+28\n",
      "Gradient Descent(14/49): loss=3.54455182984678e+30\n",
      "Gradient Descent(15/49): loss=1.0217920541634293e+33\n",
      "Gradient Descent(16/49): loss=2.9455336359517365e+35\n",
      "Gradient Descent(17/49): loss=8.491130536829325e+37\n",
      "Gradient Descent(18/49): loss=2.4477500279499497e+40\n",
      "Gradient Descent(19/49): loss=7.056163163412385e+42\n",
      "Gradient Descent(20/49): loss=2.0340900060296534e+45\n",
      "Gradient Descent(21/49): loss=5.863699660624202e+47\n",
      "Gradient Descent(22/49): loss=1.6903368883437352e+50\n",
      "Gradient Descent(23/49): loss=4.872757749450194e+52\n",
      "Gradient Descent(24/49): loss=1.404676680058178e+55\n",
      "Gradient Descent(25/49): loss=4.049281078512565e+57\n",
      "Gradient Descent(26/49): loss=1.1672919103438642e+60\n",
      "Gradient Descent(27/49): loss=3.3649686883550905e+62\n",
      "Gradient Descent(28/49): loss=9.700242221566459e+64\n",
      "Gradient Descent(29/49): loss=2.796302369251924e+67\n",
      "Gradient Descent(30/49): loss=8.060939883438621e+69\n",
      "Gradient Descent(31/49): loss=2.3237383953508058e+72\n",
      "Gradient Descent(32/49): loss=6.69867311766134e+74\n",
      "Gradient Descent(33/49): loss=1.93103585270431e+77\n",
      "Gradient Descent(34/49): loss=5.566624014833776e+79\n",
      "Gradient Descent(35/49): loss=1.604698477199558e+82\n",
      "Gradient Descent(36/49): loss=4.625886705954302e+84\n",
      "Gradient Descent(37/49): loss=1.333510819656873e+87\n",
      "Gradient Descent(38/49): loss=3.8441302590766623e+89\n",
      "Gradient Descent(39/49): loss=1.1081527971817202e+92\n",
      "Gradient Descent(40/49): loss=3.194487541108072e+94\n",
      "Gradient Descent(41/49): loss=9.208793838040827e+96\n",
      "Gradient Descent(42/49): loss=2.6546318575442387e+99\n",
      "Gradient Descent(43/49): loss=7.652544321252905e+101\n",
      "Gradient Descent(44/49): loss=2.2060096364139714e+104\n",
      "Gradient Descent(45/49): loss=6.359294780477056e+106\n",
      "Gradient Descent(46/49): loss=1.833202785584433e+109\n",
      "Gradient Descent(47/49): loss=5.284599266245224e+111\n",
      "Gradient Descent(48/49): loss=1.523398809144617e+114\n",
      "Gradient Descent(49/49): loss=4.3915230176992094e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.91207337002025\n",
      "Gradient Descent(2/49): loss=371.3856750127206\n",
      "Gradient Descent(3/49): loss=27495.792998727047\n",
      "Gradient Descent(4/49): loss=2487322.144910888\n",
      "Gradient Descent(5/49): loss=336466477.4426539\n",
      "Gradient Descent(6/49): loss=69342099221.88287\n",
      "Gradient Descent(7/49): loss=17720373935481.805\n",
      "Gradient Descent(8/49): loss=4852938956561120.0\n",
      "Gradient Descent(9/49): loss=1.353805283783699e+18\n",
      "Gradient Descent(10/49): loss=3.794301797537093e+20\n",
      "Gradient Descent(11/49): loss=1.0646606124893505e+23\n",
      "Gradient Descent(12/49): loss=2.988239208160633e+25\n",
      "Gradient Descent(13/49): loss=8.387846160266779e+27\n",
      "Gradient Descent(14/49): loss=2.3544702707229424e+30\n",
      "Gradient Descent(15/49): loss=6.609032108023286e+32\n",
      "Gradient Descent(16/49): loss=1.8551668784468123e+35\n",
      "Gradient Descent(17/49): loss=5.207487286942601e+37\n",
      "Gradient Descent(18/49): loss=1.4617512826531037e+40\n",
      "Gradient Descent(19/49): loss=4.1031628685152816e+42\n",
      "Gradient Descent(20/49): loss=1.1517654059054051e+45\n",
      "Gradient Descent(21/49): loss=3.233026799287793e+47\n",
      "Gradient Descent(22/49): loss=9.075166031141653e+49\n",
      "Gradient Descent(23/49): loss=2.5474158926246433e+52\n",
      "Gradient Descent(24/49): loss=7.150643534043614e+54\n",
      "Gradient Descent(25/49): loss=2.0071988676522686e+57\n",
      "Gradient Descent(26/49): loss=5.6342443517526185e+59\n",
      "Gradient Descent(27/49): loss=1.5815428120679546e+62\n",
      "Gradient Descent(28/49): loss=4.439419929712299e+64\n",
      "Gradient Descent(29/49): loss=1.2461533865502482e+67\n",
      "Gradient Descent(30/49): loss=3.497975608068484e+69\n",
      "Gradient Descent(31/49): loss=9.818882239300366e+71\n",
      "Gradient Descent(32/49): loss=2.7561784080731505e+74\n",
      "Gradient Descent(33/49): loss=7.736643776745912e+76\n",
      "Gradient Descent(34/49): loss=2.171690219796299e+79\n",
      "Gradient Descent(35/49): loss=6.095974620072917e+81\n",
      "Gradient Descent(36/49): loss=1.7111513525193387e+84\n",
      "Gradient Descent(37/49): loss=4.80323350032854e+86\n",
      "Gradient Descent(38/49): loss=1.3482765288243538e+89\n",
      "Gradient Descent(39/49): loss=3.784637157561254e+91\n",
      "Gradient Descent(40/49): loss=1.0623546511547571e+94\n",
      "Gradient Descent(41/49): loss=2.9820491578045646e+96\n",
      "Gradient Descent(42/49): loss=8.370667149521887e+98\n",
      "Gradient Descent(43/49): loss=2.349661753385406e+101\n",
      "Gradient Descent(44/49): loss=6.595544007071805e+103\n",
      "Gradient Descent(45/49): loss=1.8513814035804928e+106\n",
      "Gradient Descent(46/49): loss=5.196861847708903e+108\n",
      "Gradient Descent(47/49): loss=1.4587687340890892e+111\n",
      "Gradient Descent(48/49): loss=4.09479082168415e+113\n",
      "Gradient Descent(49/49): loss=1.1494153584131404e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.030335391212567\n",
      "Gradient Descent(2/49): loss=377.5428859676371\n",
      "Gradient Descent(3/49): loss=27648.57644381937\n",
      "Gradient Descent(4/49): loss=2439311.785132509\n",
      "Gradient Descent(5/49): loss=317904859.6037806\n",
      "Gradient Descent(6/49): loss=63870407082.565025\n",
      "Gradient Descent(7/49): loss=16174897170508.46\n",
      "Gradient Descent(8/49): loss=4419682321557956.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/49): loss=1.2324901187056125e+18\n",
      "Gradient Descent(10/49): loss=3.454654595102664e+20\n",
      "Gradient Descent(11/49): loss=9.695689443291068e+22\n",
      "Gradient Descent(12/49): loss=2.7220077872085732e+25\n",
      "Gradient Descent(13/49): loss=7.642470095556563e+27\n",
      "Gradient Descent(14/49): loss=2.145786226918455e+30\n",
      "Gradient Descent(15/49): loss=6.024780263257924e+32\n",
      "Gradient Descent(16/49): loss=1.6915953280269486e+35\n",
      "Gradient Descent(17/49): loss=4.74954347286819e+37\n",
      "Gradient Descent(18/49): loss=1.3335438109317924e+40\n",
      "Gradient Descent(19/49): loss=3.744231707743772e+42\n",
      "Gradient Descent(20/49): loss=1.0512793825728456e+45\n",
      "Gradient Descent(21/49): loss=2.951709262854493e+47\n",
      "Gradient Descent(22/49): loss=8.287604341095805e+49\n",
      "Gradient Descent(23/49): loss=2.326936009022501e+52\n",
      "Gradient Descent(24/49): loss=6.533409375400225e+54\n",
      "Gradient Descent(25/49): loss=1.83440532533886e+57\n",
      "Gradient Descent(26/49): loss=5.150515916398979e+59\n",
      "Gradient Descent(27/49): loss=1.446126100848789e+62\n",
      "Gradient Descent(28/49): loss=4.060332466690685e+64\n",
      "Gradient Descent(29/49): loss=1.140031960586716e+67\n",
      "Gradient Descent(30/49): loss=3.2009025906652093e+69\n",
      "Gradient Descent(31/49): loss=8.987272067051586e+71\n",
      "Gradient Descent(32/49): loss=2.52338385562742e+74\n",
      "Gradient Descent(33/49): loss=7.084982000472834e+76\n",
      "Gradient Descent(34/49): loss=1.9892720576410816e+79\n",
      "Gradient Descent(35/49): loss=5.58533997552502e+81\n",
      "Gradient Descent(36/49): loss=1.5682129813450693e+84\n",
      "Gradient Descent(37/49): loss=4.403119533700481e+86\n",
      "Gradient Descent(38/49): loss=1.2362773334158891e+89\n",
      "Gradient Descent(39/49): loss=3.471133666528988e+91\n",
      "Gradient Descent(40/49): loss=9.746008120701857e+93\n",
      "Gradient Descent(41/49): loss=2.736416497143083e+96\n",
      "Gradient Descent(42/49): loss=7.683120261239306e+98\n",
      "Gradient Descent(43/49): loss=2.1572131658428284e+101\n",
      "Gradient Descent(44/49): loss=6.056873359594056e+103\n",
      "Gradient Descent(45/49): loss=1.7006068512392948e+106\n",
      "Gradient Descent(46/49): loss=4.774845849964866e+108\n",
      "Gradient Descent(47/49): loss=1.340648067736045e+111\n",
      "Gradient Descent(48/49): loss=3.76417856827277e+113\n",
      "Gradient Descent(49/49): loss=1.0568799250776888e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.808731222654833\n",
      "Gradient Descent(2/49): loss=358.48771323013466\n",
      "Gradient Descent(3/49): loss=25604.684891953464\n",
      "Gradient Descent(4/49): loss=2112977.423738017\n",
      "Gradient Descent(5/49): loss=245646638.67088568\n",
      "Gradient Descent(6/49): loss=45372097786.66441\n",
      "Gradient Descent(7/49): loss=11220892709168.113\n",
      "Gradient Descent(8/49): loss=3077541781318655.0\n",
      "Gradient Descent(9/49): loss=8.68148602109329e+17\n",
      "Gradient Descent(10/49): loss=2.4662515918871642e+20\n",
      "Gradient Descent(11/49): loss=7.018226439843642e+22\n",
      "Gradient Descent(12/49): loss=1.9980161562014624e+25\n",
      "Gradient Descent(13/49): loss=5.688722292601744e+27\n",
      "Gradient Descent(14/49): loss=1.6197246229366967e+30\n",
      "Gradient Descent(15/49): loss=4.611797577825147e+32\n",
      "Gradient Descent(16/49): loss=1.3131064090640734e+35\n",
      "Gradient Descent(17/49): loss=3.7387786895587816e+37\n",
      "Gradient Descent(18/49): loss=1.0645342366515243e+40\n",
      "Gradient Descent(19/49): loss=3.031024991565079e+42\n",
      "Gradient Descent(20/49): loss=8.630171045354172e+44\n",
      "Gradient Descent(21/49): loss=2.457249695095306e+47\n",
      "Gradient Descent(22/49): loss=6.996473226444781e+49\n",
      "Gradient Descent(23/49): loss=1.992090494897884e+52\n",
      "Gradient Descent(24/49): loss=5.672035626367547e+54\n",
      "Gradient Descent(25/49): loss=1.6149862784506205e+57\n",
      "Gradient Descent(26/49): loss=4.598315051939904e+59\n",
      "Gradient Descent(27/49): loss=1.3092681714415658e+62\n",
      "Gradient Descent(28/49): loss=3.727850583067024e+64\n",
      "Gradient Descent(29/49): loss=1.061422730102125e+67\n",
      "Gradient Descent(30/49): loss=3.022165687366578e+69\n",
      "Gradient Descent(31/49): loss=8.604946156577327e+71\n",
      "Gradient Descent(32/49): loss=2.450067468740054e+74\n",
      "Gradient Descent(33/49): loss=6.976023431349391e+76\n",
      "Gradient Descent(34/49): loss=1.986267869584884e+79\n",
      "Gradient Descent(35/49): loss=5.655456992899169e+81\n",
      "Gradient Descent(36/49): loss=1.6102658804633667e+84\n",
      "Gradient Descent(37/49): loss=4.5848747661596895e+86\n",
      "Gradient Descent(38/49): loss=1.305441348314422e+89\n",
      "Gradient Descent(39/49): loss=3.7169545534095585e+91\n",
      "Gradient Descent(40/49): loss=1.0583203274471657e+94\n",
      "Gradient Descent(41/49): loss=3.013332284249883e+96\n",
      "Gradient Descent(42/49): loss=8.57979500139195e+98\n",
      "Gradient Descent(43/49): loss=2.4429062354215472e+101\n",
      "Gradient Descent(44/49): loss=6.9556334085991925e+103\n",
      "Gradient Descent(45/49): loss=1.980462263074669e+106\n",
      "Gradient Descent(46/49): loss=5.638926816663001e+108\n",
      "Gradient Descent(47/49): loss=1.605559279595495e+111\n",
      "Gradient Descent(48/49): loss=4.5714737646138234e+113\n",
      "Gradient Descent(49/49): loss=1.3016257105011219e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.9056981806939035\n",
      "Gradient Descent(2/49): loss=369.9648917849647\n",
      "Gradient Descent(3/49): loss=27628.025700980004\n",
      "Gradient Descent(4/49): loss=2608343.9187863637\n",
      "Gradient Descent(5/49): loss=384281710.4784832\n",
      "Gradient Descent(6/49): loss=85768449416.25435\n",
      "Gradient Descent(7/49): loss=23117245671663.297\n",
      "Gradient Descent(8/49): loss=6589085040315929.0\n",
      "Gradient Descent(9/49): loss=1.9048374790394888e+18\n",
      "Gradient Descent(10/49): loss=5.525591391640091e+20\n",
      "Gradient Descent(11/49): loss=1.6041911328392834e+23\n",
      "Gradient Descent(12/49): loss=4.658205531012728e+25\n",
      "Gradient Descent(13/49): loss=1.352700112856297e+28\n",
      "Gradient Descent(14/49): loss=3.9281608551154393e+30\n",
      "Gradient Descent(15/49): loss=1.1407176481031602e+33\n",
      "Gradient Descent(16/49): loss=3.3125873161378512e+35\n",
      "Gradient Descent(17/49): loss=9.619590278068368e+37\n",
      "Gradient Descent(18/49): loss=2.7934817002701583e+40\n",
      "Gradient Descent(19/49): loss=8.112133523260597e+42\n",
      "Gradient Descent(20/49): loss=2.355723698824291e+45\n",
      "Gradient Descent(21/49): loss=6.8409058258668585e+47\n",
      "Gradient Descent(22/49): loss=1.9865654256169224e+50\n",
      "Gradient Descent(23/49): loss=5.768888347279376e+52\n",
      "Gradient Descent(24/49): loss=1.6752568193551407e+55\n",
      "Gradient Descent(25/49): loss=4.8648634569673585e+57\n",
      "Gradient Descent(26/49): loss=1.4127324349024832e+60\n",
      "Gradient Descent(27/49): loss=4.1025055487782515e+62\n",
      "Gradient Descent(28/49): loss=1.1913474456979204e+65\n",
      "Gradient Descent(29/49): loss=3.459614422200165e+67\n",
      "Gradient Descent(30/49): loss=1.0046550226398169e+70\n",
      "Gradient Descent(31/49): loss=2.9174688024150696e+72\n",
      "Gradient Descent(32/49): loss=8.472185995447698e+74\n",
      "Gradient Descent(33/49): loss=2.4602811684580874e+77\n",
      "Gradient Descent(34/49): loss=7.144535579273315e+79\n",
      "Gradient Descent(35/49): loss=2.074738013602423e+82\n",
      "Gradient Descent(36/49): loss=6.024937208759339e+84\n",
      "Gradient Descent(37/49): loss=1.7496121501367487e+87\n",
      "Gradient Descent(38/49): loss=5.080787682659332e+89\n",
      "Gradient Descent(39/49): loss=1.4754357686785492e+92\n",
      "Gradient Descent(40/49): loss=4.2845929479119955e+94\n",
      "Gradient Descent(41/49): loss=1.2442247313645607e+97\n",
      "Gradient Descent(42/49): loss=3.6131674606186554e+99\n",
      "Gradient Descent(43/49): loss=1.0492460702140325e+102\n",
      "Gradient Descent(44/49): loss=3.0469590127192694e+104\n",
      "Gradient Descent(45/49): loss=8.848219201142507e+106\n",
      "Gradient Descent(46/49): loss=2.569479363018855e+109\n",
      "Gradient Descent(47/49): loss=7.461641768693198e+111\n",
      "Gradient Descent(48/49): loss=2.1668240922897898e+114\n",
      "Gradient Descent(49/49): loss=6.292350654821007e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.955692601848237\n",
      "Gradient Descent(2/49): loss=377.1944186064542\n",
      "Gradient Descent(3/49): loss=28140.379688730794\n",
      "Gradient Descent(4/49): loss=2564439.224206283\n",
      "Gradient Descent(5/49): loss=349311172.8253095\n",
      "Gradient Descent(6/49): loss=72488149296.24147\n",
      "Gradient Descent(7/49): loss=18657117512971.91\n",
      "Gradient Descent(8/49): loss=5146801164204820.0\n",
      "Gradient Descent(9/49): loss=1.4463395586687485e+18\n",
      "Gradient Descent(10/49): loss=4.083509680230948e+20\n",
      "Gradient Descent(11/49): loss=1.1542564624757868e+23\n",
      "Gradient Descent(12/49): loss=3.263595934454838e+25\n",
      "Gradient Descent(13/49): loss=9.228295874363333e+27\n",
      "Gradient Descent(14/49): loss=2.6094821540042814e+30\n",
      "Gradient Descent(15/49): loss=7.378856387235783e+32\n",
      "Gradient Descent(16/49): loss=2.086528190655468e+35\n",
      "Gradient Descent(17/49): loss=5.900102718221554e+37\n",
      "Gradient Descent(18/49): loss=1.6683797785991357e+40\n",
      "Gradient Descent(19/49): loss=4.717699444512664e+42\n",
      "Gradient Descent(20/49): loss=1.3340300826794981e+45\n",
      "Gradient Descent(21/49): loss=3.772254430850208e+47\n",
      "Gradient Descent(22/49): loss=1.0666853528566119e+50\n",
      "Gradient Descent(23/49): loss=3.016280218970158e+52\n",
      "Gradient Descent(24/49): loss=8.529175295344963e+54\n",
      "Gradient Descent(25/49): loss=2.411806129996754e+57\n",
      "Gradient Descent(26/49): loss=6.819895953914804e+59\n",
      "Gradient Descent(27/49): loss=1.9284709597407038e+62\n",
      "Gradient Descent(28/49): loss=5.45316272813328e+64\n",
      "Gradient Descent(29/49): loss=1.541998005689475e+67\n",
      "Gradient Descent(30/49): loss=4.360328066652556e+69\n",
      "Gradient Descent(31/49): loss=1.2329757093516723e+72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=3.486501649905331e+74\n",
      "Gradient Descent(33/49): loss=9.858826627804637e+76\n",
      "Gradient Descent(34/49): loss=2.787793388244889e+79\n",
      "Gradient Descent(35/49): loss=7.88308007529335e+81\n",
      "Gradient Descent(36/49): loss=2.229108933808409e+84\n",
      "Gradient Descent(37/49): loss=6.303280686387659e+86\n",
      "Gradient Descent(38/49): loss=1.7823869802319433e+89\n",
      "Gradient Descent(39/49): loss=5.040079135554065e+91\n",
      "Gradient Descent(40/49): loss=1.4251898142423455e+94\n",
      "Gradient Descent(41/49): loss=4.030028005496439e+96\n",
      "Gradient Descent(42/49): loss=1.1395763260994073e+99\n",
      "Gradient Descent(43/49): loss=3.222394983943179e+101\n",
      "Gradient Descent(44/49): loss=9.11200873054662e+103\n",
      "Gradient Descent(45/49): loss=2.5766147079820904e+106\n",
      "Gradient Descent(46/49): loss=7.285927340185354e+108\n",
      "Gradient Descent(47/49): loss=2.0602512685350013e+111\n",
      "Gradient Descent(48/49): loss=5.825799642673928e+113\n",
      "Gradient Descent(49/49): loss=1.6473690367250143e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.074893862768204\n",
      "Gradient Descent(2/49): loss=383.4498981059511\n",
      "Gradient Descent(3/49): loss=28297.19131439576\n",
      "Gradient Descent(4/49): loss=2515045.383069153\n",
      "Gradient Descent(5/49): loss=330058491.99231184\n",
      "Gradient Descent(6/49): loss=66770108375.55201\n",
      "Gradient Descent(7/49): loss=17030102740633.63\n",
      "Gradient Descent(8/49): loss=4687320503957090.0\n",
      "Gradient Descent(9/49): loss=1.3167327324201871e+18\n",
      "Gradient Descent(10/49): loss=3.71797281693261e+20\n",
      "Gradient Descent(11/49): loss=1.051161864313473e+23\n",
      "Gradient Descent(12/49): loss=2.9728303084007634e+25\n",
      "Gradient Descent(13/49): loss=8.408228541313748e+27\n",
      "Gradient Descent(14/49): loss=2.3781938700564317e+30\n",
      "Gradient Descent(15/49): loss=6.726545232023271e+32\n",
      "Gradient Descent(16/49): loss=1.902555733822464e+35\n",
      "Gradient Descent(17/49): loss=5.381245860464307e+37\n",
      "Gradient Descent(18/49): loss=1.5220478730801227e+40\n",
      "Gradient Descent(19/49): loss=4.305006366366076e+42\n",
      "Gradient Descent(20/49): loss=1.217641062608648e+45\n",
      "Gradient Descent(21/49): loss=3.4440129299890933e+47\n",
      "Gradient Descent(22/49): loss=9.74115068004481e+49\n",
      "Gradient Descent(23/49): loss=2.755216618128529e+52\n",
      "Gradient Descent(24/49): loss=7.792938290632743e+54\n",
      "Gradient Descent(25/49): loss=2.2041783140486053e+57\n",
      "Gradient Descent(26/49): loss=6.234364829972897e+59\n",
      "Gradient Descent(27/49): loss=1.7633466668956564e+62\n",
      "Gradient Descent(28/49): loss=4.987503221985245e+64\n",
      "Gradient Descent(29/49): loss=1.410680546049757e+67\n",
      "Gradient Descent(30/49): loss=3.990011664014874e+69\n",
      "Gradient Descent(31/49): loss=1.1285470068723275e+72\n",
      "Gradient Descent(32/49): loss=3.192016600369795e+74\n",
      "Gradient Descent(33/49): loss=9.028396615285207e+76\n",
      "Gradient Descent(34/49): loss=2.5536190956354117e+79\n",
      "Gradient Descent(35/49): loss=7.222733740511135e+81\n",
      "Gradient Descent(36/49): loss=2.0428999287905483e+84\n",
      "Gradient Descent(37/49): loss=5.778200151065162e+86\n",
      "Gradient Descent(38/49): loss=1.6343236648667269e+89\n",
      "Gradient Descent(39/49): loss=4.622570647801083e+91\n",
      "Gradient Descent(40/49): loss=1.3074619093675698e+94\n",
      "Gradient Descent(41/49): loss=3.698064939819261e+96\n",
      "Gradient Descent(42/49): loss=1.0459719094788455e+99\n",
      "Gradient Descent(43/49): loss=2.9584586891334434e+101\n",
      "Gradient Descent(44/49): loss=8.36779433175233e+103\n",
      "Gradient Descent(45/49): loss=2.3667723411414174e+106\n",
      "Gradient Descent(46/49): loss=6.694250710173836e+108\n",
      "Gradient Descent(47/49): loss=1.8934221847907195e+111\n",
      "Gradient Descent(48/49): loss=5.3554127639844494e+113\n",
      "Gradient Descent(49/49): loss=1.5147411973425272e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.851554924223618\n",
      "Gradient Descent(2/49): loss=364.10005940585484\n",
      "Gradient Descent(3/49): loss=26206.33563532232\n",
      "Gradient Descent(4/49): loss=2178836.530592128\n",
      "Gradient Descent(5/49): loss=255082483.2490811\n",
      "Gradient Descent(6/49): loss=47436521218.78751\n",
      "Gradient Descent(7/49): loss=11814392547854.807\n",
      "Gradient Descent(8/49): loss=3263866416851027.5\n",
      "Gradient Descent(9/49): loss=9.274666190888335e+17\n",
      "Gradient Descent(10/49): loss=2.654158105076841e+20\n",
      "Gradient Descent(11/49): loss=7.60859755173621e+22\n",
      "Gradient Descent(12/49): loss=2.1820503944758664e+25\n",
      "Gradient Descent(13/49): loss=6.258485512429957e+27\n",
      "Gradient Descent(14/49): loss=1.7950828223684722e+30\n",
      "Gradient Descent(15/49): loss=5.148756413119343e+32\n",
      "Gradient Descent(16/49): loss=1.4767971174480365e+35\n",
      "Gradient Descent(17/49): loss=4.235839348380761e+37\n",
      "Gradient Descent(18/49): loss=1.2149493198400702e+40\n",
      "Gradient Descent(19/49): loss=3.4847919369062464e+42\n",
      "Gradient Descent(20/49): loss=9.995293389662096e+44\n",
      "Gradient Descent(21/49): loss=2.8669111923424976e+47\n",
      "Gradient Descent(22/49): loss=8.223050056489365e+49\n",
      "Gradient Descent(23/49): loss=2.3585855193936539e+52\n",
      "Gradient Descent(24/49): loss=6.765039266667143e+54\n",
      "Gradient Descent(25/49): loss=1.9403899457294293e+57\n",
      "Gradient Descent(26/49): loss=5.5655451403562266e+59\n",
      "Gradient Descent(27/49): loss=1.5963437028481262e+62\n",
      "Gradient Descent(28/49): loss=4.578730660443384e+64\n",
      "Gradient Descent(29/49): loss=1.3132995371535846e+67\n",
      "Gradient Descent(30/49): loss=3.766886069950171e+69\n",
      "Gradient Descent(31/49): loss=1.08044130547237e+72\n",
      "Gradient Descent(32/49): loss=3.0989878453803777e+74\n",
      "Gradient Descent(33/49): loss=8.888706510176143e+76\n",
      "Gradient Descent(34/49): loss=2.5495131754655273e+79\n",
      "Gradient Descent(35/49): loss=7.312669649324959e+81\n",
      "Gradient Descent(36/49): loss=2.097464642064272e+84\n",
      "Gradient Descent(37/49): loss=6.016076392998077e+86\n",
      "Gradient Descent(38/49): loss=1.7255678327320164e+89\n",
      "Gradient Descent(39/49): loss=4.949379214707133e+91\n",
      "Gradient Descent(40/49): loss=1.4196112228280974e+94\n",
      "Gradient Descent(41/49): loss=4.071815750126819e+96\n",
      "Gradient Descent(42/49): loss=1.1679031016640582e+99\n",
      "Gradient Descent(43/49): loss=3.3498511194522005e+101\n",
      "Gradient Descent(44/49): loss=9.608247898739703e+103\n",
      "Gradient Descent(45/49): loss=2.755896438129842e+106\n",
      "Gradient Descent(46/49): loss=7.904630748227263e+108\n",
      "Gradient Descent(47/49): loss=2.2672545456105399e+111\n",
      "Gradient Descent(48/49): loss=6.503078180779707e+113\n",
      "Gradient Descent(49/49): loss=1.865252664602997e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.949112768353401\n",
      "Gradient Descent(2/49): loss=375.7317146304003\n",
      "Gradient Descent(3/49): loss=28272.996573141023\n",
      "Gradient Descent(4/49): loss=2688726.6270640083\n",
      "Gradient Descent(5/49): loss=398850330.601438\n",
      "Gradient Descent(6/49): loss=89637175080.86908\n",
      "Gradient Descent(7/49): loss=24333229619525.38\n",
      "Gradient Descent(8/49): loss=6986219696474007.0\n",
      "Gradient Descent(9/49): loss=2.0344393815993587e+18\n",
      "Gradient Descent(10/49): loss=5.944836595923679e+20\n",
      "Gradient Descent(11/49): loss=1.7385727174026204e+23\n",
      "Gradient Descent(12/49): loss=5.085472701750119e+25\n",
      "Gradient Descent(13/49): loss=1.4876139027022215e+28\n",
      "Gradient Descent(14/49): loss=4.351650526596033e+30\n",
      "Gradient Descent(15/49): loss=1.2729723156609457e+33\n",
      "Gradient Descent(16/49): loss=3.72378146108891e+35\n",
      "Gradient Descent(17/49): loss=1.0893049537232495e+38\n",
      "Gradient Descent(18/49): loss=3.1865063458241623e+40\n",
      "Gradient Descent(19/49): loss=9.321377586703296e+42\n",
      "Gradient Descent(20/49): loss=2.7267505809035725e+45\n",
      "Gradient Descent(21/49): loss=7.976469856527955e+47\n",
      "Gradient Descent(22/49): loss=2.3333293417176225e+50\n",
      "Gradient Descent(23/49): loss=6.825608213913877e+52\n",
      "Gradient Descent(24/49): loss=1.9966717366983006e+55\n",
      "Gradient Descent(25/49): loss=5.84079528035237e+57\n",
      "Gradient Descent(26/49): loss=1.7085877903702095e+60\n",
      "Gradient Descent(27/49): loss=4.998073202843536e+62\n",
      "Gradient Descent(28/49): loss=1.4620691943239891e+65\n",
      "Gradient Descent(29/49): loss=4.276940817463484e+67\n",
      "Gradient Descent(30/49): loss=1.2511188134664537e+70\n",
      "Gradient Descent(31/49): loss=3.659854910823963e+72\n",
      "Gradient Descent(32/49): loss=1.0706047918159365e+75\n",
      "Gradient Descent(33/49): loss=3.131803440812315e+77\n",
      "Gradient Descent(34/49): loss=9.161357082334253e+79\n",
      "Gradient Descent(35/49): loss=2.6799403339395693e+82\n",
      "Gradient Descent(36/49): loss=7.83953744945176e+84\n",
      "Gradient Descent(37/49): loss=2.2932729748879216e+87\n",
      "Gradient Descent(38/49): loss=6.708432699328605e+89\n",
      "Gradient Descent(39/49): loss=1.9623947857154222e+92\n",
      "Gradient Descent(40/49): loss=5.7405260924634234e+94\n",
      "Gradient Descent(41/49): loss=1.6792563890878789e+97\n",
      "Gradient Descent(42/49): loss=4.912271061696943e+99\n",
      "Gradient Descent(43/49): loss=1.4369697885557545e+102\n",
      "Gradient Descent(44/49): loss=4.2035183874985144e+104\n",
      "Gradient Descent(45/49): loss=1.2296408021073766e+107\n",
      "Gradient Descent(46/49): loss=3.5970260215921233e+109\n",
      "Gradient Descent(47/49): loss=1.0522256725570766e+112\n",
      "Gradient Descent(48/49): loss=3.0780396342479694e+114\n",
      "Gradient Descent(49/49): loss=9.004083664844723e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.999483648973136\n",
      "Gradient Descent(2/49): loss=383.07112453177757\n",
      "Gradient Descent(3/49): loss=28797.514289510033\n",
      "Gradient Descent(4/49): loss=2643641.7181573175\n",
      "Gradient Descent(5/49): loss=362595832.1048275\n",
      "Gradient Descent(6/49): loss=75764595318.51535\n",
      "Gradient Descent(7/49): loss=19639675338777.89\n",
      "Gradient Descent(8/49): loss=5457283492336450.0\n",
      "Gradient Descent(9/49): loss=1.5448248660339436e+18\n",
      "Gradient Descent(10/49): loss=4.393580320402347e+20\n",
      "Gradient Descent(11/49): loss=1.2510222502785774e+23\n",
      "Gradient Descent(12/49): loss=3.5631764247237923e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=1.0149410304113774e+28\n",
      "Gradient Descent(14/49): loss=2.891026174103128e+30\n",
      "Gradient Descent(15/49): loss=8.235029382504079e+32\n",
      "Gradient Descent(16/49): loss=2.3457339704779608e+35\n",
      "Gradient Descent(17/49): loss=6.681784721386503e+37\n",
      "Gradient Descent(18/49): loss=1.9032955408398483e+40\n",
      "Gradient Descent(19/49): loss=5.421506569120621e+42\n",
      "Gradient Descent(20/49): loss=1.544307385243047e+45\n",
      "Gradient Descent(21/49): loss=4.3989346358748405e+47\n",
      "Gradient Descent(22/49): loss=1.2530294240969744e+50\n",
      "Gradient Descent(23/49): loss=3.569234070783284e+52\n",
      "Gradient Descent(24/49): loss=1.0166905586769713e+55\n",
      "Gradient Descent(25/49): loss=2.896026630946109e+57\n",
      "Gradient Descent(26/49): loss=8.249285070642475e+59\n",
      "Gradient Descent(27/49): loss=2.349795524998697e+62\n",
      "Gradient Descent(28/49): loss=6.693354590150091e+64\n",
      "Gradient Descent(29/49): loss=1.9065912413595337e+67\n",
      "Gradient Descent(30/49): loss=5.430894348520516e+69\n",
      "Gradient Descent(31/49): loss=1.5469814811359268e+72\n",
      "Gradient Descent(32/49): loss=4.406551756304217e+74\n",
      "Gradient Descent(33/49): loss=1.2551991486497261e+77\n",
      "Gradient Descent(34/49): loss=3.575414496191957e+79\n",
      "Gradient Descent(35/49): loss=1.01845104287486e+82\n",
      "Gradient Descent(36/49): loss=2.901041341745428e+84\n",
      "Gradient Descent(37/49): loss=8.263569393340363e+86\n",
      "Gradient Descent(38/49): loss=2.3538643912419407e+89\n",
      "Gradient Descent(39/49): loss=6.704944689908415e+91\n",
      "Gradient Descent(40/49): loss=1.909892662551024e+94\n",
      "Gradient Descent(41/49): loss=5.440298393447449e+96\n",
      "Gradient Descent(42/49): loss=1.5496602081404056e+99\n",
      "Gradient Descent(43/49): loss=4.41418206689947e+101\n",
      "Gradient Descent(44/49): loss=1.2573726302954417e+104\n",
      "Gradient Descent(45/49): loss=3.581605623545437e+106\n",
      "Gradient Descent(46/49): loss=1.0202145754992428e+109\n",
      "Gradient Descent(47/49): loss=2.906064735934837e+111\n",
      "Gradient Descent(48/49): loss=8.277878450531952e+113\n",
      "Gradient Descent(49/49): loss=2.3579403030655347e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.119627807805592\n",
      "Gradient Descent(2/49): loss=389.4260432379012\n",
      "Gradient Descent(3/49): loss=28958.440895544867\n",
      "Gradient Descent(4/49): loss=2592829.7650541635\n",
      "Gradient Descent(5/49): loss=342629026.6103853\n",
      "Gradient Descent(6/49): loss=69790077196.43027\n",
      "Gradient Descent(7/49): loss=17927142938013.086\n",
      "Gradient Descent(8/49): loss=4970096313466767.0\n",
      "Gradient Descent(9/49): loss=1.406393156457961e+18\n",
      "Gradient Descent(10/49): loss=4.0002860960229274e+20\n",
      "Gradient Descent(11/49): loss=1.1392842413946777e+23\n",
      "Gradient Descent(12/49): loss=3.2457181032199486e+25\n",
      "Gradient Descent(13/49): loss=9.247482436897823e+27\n",
      "Gradient Descent(14/49): loss=2.634781662283557e+30\n",
      "Gradient Descent(15/49): loss=7.507024332389487e+32\n",
      "Gradient Descent(16/49): loss=2.138905155236996e+35\n",
      "Gradient Descent(17/49): loss=6.09418112058799e+37\n",
      "Gradient Descent(18/49): loss=1.7363577861993826e+40\n",
      "Gradient Descent(19/49): loss=4.947241238162225e+42\n",
      "Gradient Descent(20/49): loss=1.4095710095286796e+45\n",
      "Gradient Descent(21/49): loss=4.01615837475245e+47\n",
      "Gradient Descent(22/49): loss=1.144286310258609e+50\n",
      "Gradient Descent(23/49): loss=3.260307581900236e+52\n",
      "Gradient Descent(24/49): loss=9.289288383051192e+54\n",
      "Gradient Descent(25/49): loss=2.6467097504200443e+57\n",
      "Gradient Descent(26/49): loss=7.541021673690976e+59\n",
      "Gradient Descent(27/49): loss=2.1485925260246354e+62\n",
      "Gradient Descent(28/49): loss=6.121783019130074e+64\n",
      "Gradient Descent(29/49): loss=1.7442221770476166e+67\n",
      "Gradient Descent(30/49): loss=4.9696485376854116e+69\n",
      "Gradient Descent(31/49): loss=1.4159553130968488e+72\n",
      "Gradient Descent(32/49): loss=4.034348573110436e+74\n",
      "Gradient Descent(33/49): loss=1.1494690728453329e+77\n",
      "Gradient Descent(34/49): loss=3.275074340959099e+79\n",
      "Gradient Descent(35/49): loss=9.331361923690554e+81\n",
      "Gradient Descent(36/49): loss=2.658697369458858e+84\n",
      "Gradient Descent(37/49): loss=7.57517687147191e+86\n",
      "Gradient Descent(38/49): loss=2.1583240459504334e+89\n",
      "Gradient Descent(39/49): loss=6.1495101254615515e+91\n",
      "Gradient Descent(40/49): loss=1.7521222012101125e+94\n",
      "Gradient Descent(41/49): loss=4.992157335041276e+96\n",
      "Gradient Descent(42/49): loss=1.422368533461541e+99\n",
      "Gradient Descent(43/49): loss=4.0526211599556386e+101\n",
      "Gradient Descent(44/49): loss=1.154675309509999e+104\n",
      "Gradient Descent(45/49): loss=3.289907982434371e+106\n",
      "Gradient Descent(46/49): loss=9.373626026071418e+108\n",
      "Gradient Descent(47/49): loss=2.670739283462558e+111\n",
      "Gradient Descent(48/49): loss=7.609486766797588e+113\n",
      "Gradient Descent(49/49): loss=2.1680996423955047e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.894547418920744\n",
      "Gradient Descent(2/49): loss=369.7781337074702\n",
      "Gradient Descent(3/49): loss=26819.724655595728\n",
      "Gradient Descent(4/49): loss=2246485.821086162\n",
      "Gradient Descent(5/49): loss=264843553.19523856\n",
      "Gradient Descent(6/49): loss=49586765560.6755\n",
      "Gradient Descent(7/49): loss=12436937168953.38\n",
      "Gradient Descent(8/49): loss=3460727510363873.0\n",
      "Gradient Descent(9/49): loss=9.905981430385838e+17\n",
      "Gradient Descent(10/49): loss=2.8556143478827463e+20\n",
      "Gradient Descent(11/49): loss=8.246193924497822e+22\n",
      "Gradient Descent(12/49): loss=2.382267838909322e+25\n",
      "Gradient Descent(13/49): loss=6.882910714136838e+27\n",
      "Gradient Descent(14/49): loss=1.9886781421133717e+30\n",
      "Gradient Descent(15/49): loss=5.7459189203477614e+32\n",
      "Gradient Descent(16/49): loss=1.6601797999328435e+35\n",
      "Gradient Descent(17/49): loss=4.796792282492935e+37\n",
      "Gradient Descent(18/49): loss=1.3859473655204358e+40\n",
      "Gradient Descent(19/49): loss=4.004447195997586e+42\n",
      "Gradient Descent(20/49): loss=1.1570134499135614e+45\n",
      "Gradient Descent(21/49): loss=3.3429835865677186e+47\n",
      "Gradient Descent(22/49): loss=9.658953631249216e+49\n",
      "Gradient Descent(23/49): loss=2.7907820317263965e+52\n",
      "Gradient Descent(24/49): loss=8.063465926110605e+54\n",
      "Gradient Descent(25/49): loss=2.3297943731423916e+57\n",
      "Gradient Descent(26/49): loss=6.731524472070003e+59\n",
      "Gradient Descent(27/49): loss=1.9449536937876667e+62\n",
      "Gradient Descent(28/49): loss=5.619596105865701e+64\n",
      "Gradient Descent(29/49): loss=1.62368186419706e+67\n",
      "Gradient Descent(30/49): loss=4.691338570347927e+69\n",
      "Gradient Descent(31/49): loss=1.355478438660639e+72\n",
      "Gradient Descent(32/49): loss=3.916412704226515e+74\n",
      "Gradient Descent(33/49): loss=1.131577458729844e+77\n",
      "Gradient Descent(34/49): loss=3.269490837172562e+79\n",
      "Gradient Descent(35/49): loss=9.446609467065702e+81\n",
      "Gradient Descent(36/49): loss=2.7294289804594806e+84\n",
      "Gradient Descent(37/49): loss=7.88619724922962e+86\n",
      "Gradient Descent(38/49): loss=2.2785757570174477e+89\n",
      "Gradient Descent(39/49): loss=6.583537434312595e+91\n",
      "Gradient Descent(40/49): loss=1.9021954839776543e+94\n",
      "Gradient Descent(41/49): loss=5.496053900151729e+96\n",
      "Gradient Descent(42/49): loss=1.587986551740148e+99\n",
      "Gradient Descent(43/49): loss=4.588203344290348e+101\n",
      "Gradient Descent(44/49): loss=1.3256793582721535e+104\n",
      "Gradient Descent(45/49): loss=3.830313586985705e+106\n",
      "Gradient Descent(46/49): loss=1.1067006575231979e+109\n",
      "Gradient Descent(47/49): loss=3.1976137659427017e+111\n",
      "Gradient Descent(48/49): loss=9.238933515255195e+113\n",
      "Gradient Descent(49/49): loss=2.6694247256638993e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.992697768079546\n",
      "Gradient Descent(2/49): loss=381.56578178498637\n",
      "Gradient Descent(3/49): loss=28930.471017523803\n",
      "Gradient Descent(4/49): loss=2771270.1938888077\n",
      "Gradient Descent(5/49): loss=413914514.80609345\n",
      "Gradient Descent(6/49): loss=93665314203.92786\n",
      "Gradient Descent(7/49): loss=25608390148167.277\n",
      "Gradient Descent(8/49): loss=7405710991501370.0\n",
      "Gradient Descent(9/49): loss=2.1723381617147487e+18\n",
      "Gradient Descent(10/49): loss=6.394187447616079e+20\n",
      "Gradient Descent(11/49): loss=1.8836591768925702e+23\n",
      "Gradient Descent(12/49): loss=5.550155540384034e+25\n",
      "Gradient Descent(13/49): loss=1.6354169916926017e+28\n",
      "Gradient Descent(14/49): loss=4.818998173643832e+30\n",
      "Gradient Descent(15/49): loss=1.4199929551007378e+33\n",
      "Gradient Descent(16/49): loss=4.1842333618544724e+35\n",
      "Gradient Descent(17/49): loss=1.2329505904694837e+38\n",
      "Gradient Descent(18/49): loss=3.6330842069656583e+40\n",
      "Gradient Descent(19/49): loss=1.0705458169279855e+43\n",
      "Gradient Descent(20/49): loss=3.1545328521557156e+45\n",
      "Gradient Descent(21/49): loss=9.295330814412026e+47\n",
      "Gradient Descent(22/49): loss=2.7390164886279885e+50\n",
      "Gradient Descent(23/49): loss=8.070946021156777e+52\n",
      "Gradient Descent(24/49): loss=2.3782321116840846e+55\n",
      "Gradient Descent(25/49): loss=7.007837696136668e+57\n",
      "Gradient Descent(26/49): loss=2.064970401086538e+60\n",
      "Gradient Descent(27/49): loss=6.084762436371409e+62\n",
      "Gradient Descent(28/49): loss=1.7929716516806431e+65\n",
      "Gradient Descent(29/49): loss=5.283275028971254e+67\n",
      "Gradient Descent(30/49): loss=1.5568006892684332e+70\n",
      "Gradient Descent(31/49): loss=4.587359872080275e+72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/49): loss=1.3517382630310252e+75\n",
      "Gradient Descent(33/49): loss=3.983110945498014e+77\n",
      "Gradient Descent(34/49): loss=1.1736867438058128e+80\n",
      "Gradient Descent(35/49): loss=3.458453935716949e+82\n",
      "Gradient Descent(36/49): loss=1.0190882438265863e+85\n",
      "Gradient Descent(37/49): loss=3.0029049627641437e+87\n",
      "Gradient Descent(38/49): loss=8.848535217650936e+89\n",
      "Gradient Descent(39/49): loss=2.607361087642876e+92\n",
      "Gradient Descent(40/49): loss=7.683002524296837e+94\n",
      "Gradient Descent(41/49): loss=2.2639184142199044e+97\n",
      "Gradient Descent(42/49): loss=6.6709942760471935e+99\n",
      "Gradient Descent(43/49): loss=1.9657141508074008e+102\n",
      "Gradient Descent(44/49): loss=5.79228816993373e+104\n",
      "Gradient Descent(45/49): loss=1.7067894754572577e+107\n",
      "Gradient Descent(46/49): loss=5.029325593040915e+109\n",
      "Gradient Descent(47/49): loss=1.4819704646960046e+112\n",
      "Gradient Descent(48/49): loss=4.3668607601587866e+114\n",
      "Gradient Descent(49/49): loss=1.2867647063753778e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.043446511394954\n",
      "Gradient Descent(2/49): loss=389.0163210190802\n",
      "Gradient Descent(3/49): loss=29467.391518429624\n",
      "Gradient Descent(4/49): loss=2724977.810448061\n",
      "Gradient Descent(5/49): loss=376333809.96887004\n",
      "Gradient Descent(6/49): loss=79176333938.40166\n",
      "Gradient Descent(7/49): loss=20670108246722.03\n",
      "Gradient Descent(8/49): loss=5785260015041478.0\n",
      "Gradient Descent(9/49): loss=1.6496200668102362e+18\n",
      "Gradient Descent(10/49): loss=4.725934142352898e+20\n",
      "Gradient Descent(11/49): loss=1.355502370661413e+23\n",
      "Gradient Descent(12/49): loss=3.889011340893117e+25\n",
      "Gradient Descent(13/49): loss=1.1158593628141198e+28\n",
      "Gradient Descent(14/49): loss=3.2017504577339913e+30\n",
      "Gradient Descent(15/49): loss=9.186866896882018e+32\n",
      "Gradient Descent(16/49): loss=2.6360148068890863e+35\n",
      "Gradient Descent(17/49): loss=7.563597071288401e+37\n",
      "Gradient Descent(18/49): loss=2.1702459474793033e+40\n",
      "Gradient Descent(19/49): loss=6.227152910235368e+42\n",
      "Gradient Descent(20/49): loss=1.786775990688883e+45\n",
      "Gradient Descent(21/49): loss=5.126850890301848e+47\n",
      "Gradient Descent(22/49): loss=1.4710629757209933e+50\n",
      "Gradient Descent(23/49): loss=4.2209659008565093e+52\n",
      "Gradient Descent(24/49): loss=1.2111346305715189e+55\n",
      "Gradient Descent(25/49): loss=3.4751455657997824e+57\n",
      "Gradient Descent(26/49): loss=9.971341251971868e+59\n",
      "Gradient Descent(27/49): loss=2.86110738329359e+62\n",
      "Gradient Descent(28/49): loss=8.209462751180988e+64\n",
      "Gradient Descent(29/49): loss=2.3555662068665765e+67\n",
      "Gradient Descent(30/49): loss=6.758898021838927e+69\n",
      "Gradient Descent(31/49): loss=1.939351241177244e+72\n",
      "Gradient Descent(32/49): loss=5.564639715680165e+74\n",
      "Gradient Descent(33/49): loss=1.596679059876167e+77\n",
      "Gradient Descent(34/49): loss=4.581399965685568e+79\n",
      "Gradient Descent(35/49): loss=1.3145550770367112e+82\n",
      "Gradient Descent(36/49): loss=3.7718930097917743e+84\n",
      "Gradient Descent(37/49): loss=1.0822807751340045e+87\n",
      "Gradient Descent(38/49): loss=3.105421265088569e+89\n",
      "Gradient Descent(39/49): loss=8.910480029981544e+91\n",
      "Gradient Descent(40/49): loss=2.5567112345524527e+94\n",
      "Gradient Descent(41/49): loss=7.336049589799806e+96\n",
      "Gradient Descent(42/49): loss=2.104955102347438e+99\n",
      "Gradient Descent(43/49): loss=6.03981192965121e+101\n",
      "Gradient Descent(44/49): loss=1.733021673710568e+104\n",
      "Gradient Descent(45/49): loss=4.972611989466402e+106\n",
      "Gradient Descent(46/49): loss=1.426806737208414e+109\n",
      "Gradient Descent(47/49): loss=4.093980124843399e+111\n",
      "Gradient Descent(48/49): loss=1.1746982142378638e+114\n",
      "Gradient Descent(49/49): loss=3.370597444183713e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.164537226324742\n",
      "Gradient Descent(2/49): loss=395.4718588488227\n",
      "Gradient Descent(3/49): loss=29632.521377137404\n",
      "Gradient Descent(4/49): loss=2672712.3775103255\n",
      "Gradient Descent(5/49): loss=355629136.8514163\n",
      "Gradient Descent(6/49): loss=72934833061.00484\n",
      "Gradient Descent(7/49): loss=18867899974191.176\n",
      "Gradient Descent(8/49): loss=5268805895558651.0\n",
      "Gradient Descent(9/49): loss=1.501798099175188e+18\n",
      "Gradient Descent(10/49): loss=4.302887690414357e+20\n",
      "Gradient Descent(11/49): loss=1.234431837023849e+23\n",
      "Gradient Descent(12/49): loss=3.5425208754516333e+25\n",
      "Gradient Descent(13/49): loss=1.0166977847427172e+28\n",
      "Gradient Descent(14/49): loss=2.917962964538685e+30\n",
      "Gradient Descent(15/49): loss=8.374709640474958e+32\n",
      "Gradient Descent(16/49): loss=2.4035892711027514e+35\n",
      "Gradient Descent(17/49): loss=6.898439856579043e+37\n",
      "Gradient Descent(18/49): loss=1.9798921751016694e+40\n",
      "Gradient Descent(19/49): loss=5.682405263569052e+42\n",
      "Gradient Descent(20/49): loss=1.630883243450826e+45\n",
      "Gradient Descent(21/49): loss=4.680729432877532e+47\n",
      "Gradient Descent(22/49): loss=1.3433964765839009e+50\n",
      "Gradient Descent(23/49): loss=3.8556257508540085e+52\n",
      "Gradient Descent(24/49): loss=1.1065869376623505e+55\n",
      "Gradient Descent(25/49): loss=3.17596864876904e+57\n",
      "Gradient Descent(26/49): loss=9.115214100830047e+59\n",
      "Gradient Descent(27/49): loss=2.6161192786393843e+62\n",
      "Gradient Descent(28/49): loss=7.508413959740085e+64\n",
      "Gradient Descent(29/49): loss=2.1549583251472002e+67\n",
      "Gradient Descent(30/49): loss=6.184855294368868e+69\n",
      "Gradient Descent(31/49): loss=1.775089316851164e+72\n",
      "Gradient Descent(32/49): loss=5.094609223384695e+74\n",
      "Gradient Descent(33/49): loss=1.462182375422029e+77\n",
      "Gradient Descent(34/49): loss=4.1965481654240635e+79\n",
      "Gradient Descent(35/49): loss=1.204433646633219e+82\n",
      "Gradient Descent(36/49): loss=3.4567943747064125e+84\n",
      "Gradient Descent(37/49): loss=9.921200210907536e+86\n",
      "Gradient Descent(38/49): loss=2.84744196372032e+89\n",
      "Gradient Descent(39/49): loss=8.17232347336489e+91\n",
      "Gradient Descent(40/49): loss=2.3455042035712256e+94\n",
      "Gradient Descent(41/49): loss=6.731733009468514e+96\n",
      "Gradient Descent(42/49): loss=1.9320463907832803e+99\n",
      "Gradient Descent(43/49): loss=5.545085122788182e+101\n",
      "Gradient Descent(44/49): loss=1.5914715695051917e+104\n",
      "Gradient Descent(45/49): loss=4.567615646033186e+106\n",
      "Gradient Descent(46/49): loss=1.3109321642719577e+109\n",
      "Gradient Descent(47/49): loss=3.7624512929743984e+111\n",
      "Gradient Descent(48/49): loss=1.0798453282185092e+114\n",
      "Gradient Descent(49/49): loss=3.0992186797281463e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.937708706746215\n",
      "Gradient Descent(2/49): loss=375.52244748478415\n",
      "Gradient Descent(3/49): loss=27445.034497276592\n",
      "Gradient Descent(4/49): loss=2315966.8654238665\n",
      "Gradient Descent(5/49): loss=274939782.637813\n",
      "Gradient Descent(6/49): loss=51826063614.15291\n",
      "Gradient Descent(7/49): loss=13089833951247.643\n",
      "Gradient Descent(8/49): loss=3668679112474558.5\n",
      "Gradient Descent(9/49): loss=1.0577730579831174e+18\n",
      "Gradient Descent(10/49): loss=3.071542478113962e+20\n",
      "Gradient Descent(11/49): loss=8.93459978460371e+22\n",
      "Gradient Descent(12/49): loss=2.6000245568665964e+25\n",
      "Gradient Descent(13/49): loss=7.567013215186725e+27\n",
      "Gradient Descent(14/49): loss=2.2023300394348156e+30\n",
      "Gradient Descent(15/49): loss=6.4097775073089834e+32\n",
      "Gradient Descent(16/49): loss=1.8655382129553175e+35\n",
      "Gradient Descent(17/49): loss=5.429570794726455e+37\n",
      "Gradient Descent(18/49): loss=1.5802539657142346e+40\n",
      "Gradient Descent(19/49): loss=4.59926344759204e+42\n",
      "Gradient Descent(20/49): loss=1.3385965059164255e+45\n",
      "Gradient Descent(21/49): loss=3.8959294859282025e+47\n",
      "Gradient Descent(22/49): loss=1.133894082113181e+50\n",
      "Gradient Descent(23/49): loss=3.300151592041506e+52\n",
      "Gradient Descent(24/49): loss=9.604954027411357e+54\n",
      "Gradient Descent(25/49): loss=2.7954819436649145e+57\n",
      "Gradient Descent(26/49): loss=8.136133994043829e+59\n",
      "Gradient Descent(27/49): loss=2.3679879785690223e+62\n",
      "Gradient Descent(28/49): loss=6.891930578764655e+64\n",
      "Gradient Descent(29/49): loss=2.005867746474621e+67\n",
      "Gradient Descent(30/49): loss=5.837994695919285e+69\n",
      "Gradient Descent(31/49): loss=1.6991240887881027e+72\n",
      "Gradient Descent(32/49): loss=4.945230030986546e+74\n",
      "Gradient Descent(33/49): loss=1.4392886441162493e+77\n",
      "Gradient Descent(34/49): loss=4.188989770145669e+79\n",
      "Gradient Descent(35/49): loss=1.2191880597487353e+82\n",
      "Gradient Descent(36/49): loss=3.5483961685163132e+84\n",
      "Gradient Descent(37/49): loss=1.0327459548231312e+87\n",
      "Gradient Descent(38/49): loss=3.0057641721822593e+89\n",
      "Gradient Descent(39/49): loss=8.74815167910487e+91\n",
      "Gradient Descent(40/49): loss=2.546113181762351e+94\n",
      "Gradient Descent(41/49): loss=7.410356578325309e+96\n",
      "Gradient Descent(42/49): loss=2.1567534786462e+99\n",
      "Gradient Descent(43/49): loss=6.277141347365127e+101\n",
      "Gradient Descent(44/49): loss=1.826935896240346e+104\n",
      "Gradient Descent(45/49): loss=5.3172209836769515e+106\n",
      "Gradient Descent(46/49): loss=1.547555064599513e+109\n",
      "Gradient Descent(47/49): loss=4.5040946865283344e+111\n",
      "Gradient Descent(48/49): loss=1.310898035829337e+114\n",
      "Gradient Descent(49/49): loss=3.81531424168561e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.036453179872338\n",
      "Gradient Descent(2/49): loss=387.4676141476479\n",
      "Gradient Descent(3/49): loss=29600.642274991373\n",
      "Gradient Descent(4/49): loss=2856024.2554641264\n",
      "Gradient Descent(5/49): loss=429489201.6253626\n",
      "Gradient Descent(6/49): loss=97858817993.13098\n",
      "Gradient Descent(7/49): loss=26945375954976.414\n",
      "Gradient Descent(8/49): loss=7848729241629987.0\n",
      "Gradient Descent(9/49): loss=2.3190318644477317e+18\n",
      "Gradient Descent(10/49): loss=6.875684367673666e+20\n",
      "Gradient Descent(11/49): loss=2.040259610633724e+23\n",
      "Gradient Descent(12/49): loss=6.055376277532934e+25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(13/49): loss=1.7972870703049968e+28\n",
      "Gradient Descent(14/49): loss=5.334561087658455e+30\n",
      "Gradient Descent(15/49): loss=1.5833652655455807e+33\n",
      "Gradient Descent(16/49): loss=4.6996316074778024e+35\n",
      "Gradient Descent(17/49): loss=1.394911278577202e+38\n",
      "Gradient Descent(18/49): loss=4.140276748761719e+40\n",
      "Gradient Descent(19/49): loss=1.2288875981925057e+43\n",
      "Gradient Descent(20/49): loss=3.647497072851071e+45\n",
      "Gradient Descent(21/49): loss=1.0826242304636447e+48\n",
      "Gradient Descent(22/49): loss=3.2133685130355594e+50\n",
      "Gradient Descent(23/49): loss=9.537692682570979e+52\n",
      "Gradient Descent(24/49): loss=2.8309103465342153e+55\n",
      "Gradient Descent(25/49): loss=8.402507458405018e+57\n",
      "Gradient Descent(26/49): loss=2.4939727135836207e+60\n",
      "Gradient Descent(27/49): loss=7.402433055716749e+62\n",
      "Gradient Descent(28/49): loss=2.1971377171016373e+65\n",
      "Gradient Descent(29/49): loss=6.5213884564379e+67\n",
      "Gradient Descent(30/49): loss=1.9356323032797102e+70\n",
      "Gradient Descent(31/49): loss=5.7452066205337536e+72\n",
      "Gradient Descent(32/49): loss=1.7052515116997218e+75\n",
      "Gradient Descent(33/49): loss=5.061406682504911e+77\n",
      "Gradient Descent(34/49): loss=1.502290860318356e+80\n",
      "Gradient Descent(35/49): loss=4.458993261294535e+82\n",
      "Gradient Descent(36/49): loss=1.323486778056865e+85\n",
      "Gradient Descent(37/49): loss=3.928279656522262e+87\n",
      "Gradient Descent(38/49): loss=1.165964127159859e+90\n",
      "Gradient Descent(39/49): loss=3.46073208806934e+92\n",
      "Gradient Descent(40/49): loss=1.0271899714930914e+95\n",
      "Gradient Descent(41/49): loss=3.0488324744161106e+97\n",
      "Gradient Descent(42/49): loss=9.049328473819525e+99\n",
      "Gradient Descent(43/49): loss=2.6859575432318153e+102\n",
      "Gradient Descent(44/49): loss=7.972268820737013e+104\n",
      "Gradient Descent(45/49): loss=2.3662723303370327e+107\n",
      "Gradient Descent(46/49): loss=7.023401828541108e+109\n",
      "Gradient Descent(47/49): loss=2.0846363545201058e+112\n",
      "Gradient Descent(48/49): loss=6.187469885215776e+114\n",
      "Gradient Descent(49/49): loss=1.8365209595160056e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.087581189113685\n",
      "Gradient Descent(2/49): loss=395.0305383445006\n",
      "Gradient Descent(3/49): loss=30150.208351018093\n",
      "Gradient Descent(4/49): loss=2808496.609537687\n",
      "Gradient Descent(5/49): loss=390538814.72869605\n",
      "Gradient Descent(6/49): loss=82728426741.97682\n",
      "Gradient Descent(7/49): loss=21750561740618.59\n",
      "Gradient Descent(8/49): loss=6131647334028605.0\n",
      "Gradient Descent(9/49): loss=1.7611042339907794e+18\n",
      "Gradient Descent(10/49): loss=5.0820825644186436e+20\n",
      "Gradient Descent(11/49): loss=1.4682803387248247e+23\n",
      "Gradient Descent(12/49): loss=4.243293224301587e+25\n",
      "Gradient Descent(13/49): loss=1.2263898873305709e+28\n",
      "Gradient Descent(14/49): loss=3.544556308071856e+30\n",
      "Gradient Descent(15/49): loss=1.0244650199024789e+33\n",
      "Gradient Descent(16/49): loss=2.9609622351498998e+35\n",
      "Gradient Descent(17/49): loss=8.557929837201664e+37\n",
      "Gradient Descent(18/49): loss=2.473458364570019e+40\n",
      "Gradient Descent(19/49): loss=7.148920943256879e+42\n",
      "Gradient Descent(20/49): loss=2.0662191690267648e+45\n",
      "Gradient Descent(21/49): loss=5.971896587985268e+47\n",
      "Gradient Descent(22/49): loss=1.7260293293843236e+50\n",
      "Gradient Descent(23/49): loss=4.988661813197949e+52\n",
      "Gradient Descent(24/49): loss=1.4418495829228495e+55\n",
      "Gradient Descent(25/49): loss=4.1673103882947965e+57\n",
      "Gradient Descent(26/49): loss=1.2044582235271865e+60\n",
      "Gradient Descent(27/49): loss=3.4811892492994457e+62\n",
      "Gradient Descent(28/49): loss=1.0061518409455487e+65\n",
      "Gradient Descent(29/49): loss=2.9080335900780285e+67\n",
      "Gradient Descent(30/49): loss=8.404953424400682e+69\n",
      "Gradient Descent(31/49): loss=2.4292443631797155e+72\n",
      "Gradient Descent(32/49): loss=7.021131323474599e+74\n",
      "Gradient Descent(33/49): loss=2.0292847359724246e+77\n",
      "Gradient Descent(34/49): loss=5.865146726258253e+79\n",
      "Gradient Descent(35/49): loss=1.6951759164567652e+82\n",
      "Gradient Descent(36/49): loss=4.899487637487233e+84\n",
      "Gradient Descent(37/49): loss=1.4160759881526096e+87\n",
      "Gradient Descent(38/49): loss=4.092818173230094e+89\n",
      "Gradient Descent(39/49): loss=1.1829280871413887e+92\n",
      "Gradient Descent(40/49): loss=3.418961703455346e+94\n",
      "Gradient Descent(41/49): loss=9.881665045203476e+96\n",
      "Gradient Descent(42/49): loss=2.8560514137057526e+99\n",
      "Gradient Descent(43/49): loss=8.254711772172522e+101\n",
      "Gradient Descent(44/49): loss=2.385820721386479e+104\n",
      "Gradient Descent(45/49): loss=6.895626003303919e+106\n",
      "Gradient Descent(46/49): loss=1.993010520497474e+109\n",
      "Gradient Descent(47/49): loss=5.760305058468209e+111\n",
      "Gradient Descent(48/49): loss=1.6648740197483683e+114\n",
      "Gradient Descent(49/49): loss=4.811907483195208e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.209622118325646\n",
      "Gradient Descent(2/49): loss=401.58788450620136\n",
      "Gradient Descent(3/49): loss=30319.631225688536\n",
      "Gradient Descent(4/49): loss=2754741.5786650334\n",
      "Gradient Descent(5/49): loss=369071832.19347847\n",
      "Gradient Descent(6/49): loss=76209047830.7097\n",
      "Gradient Descent(7/49): loss=19854333408852.117\n",
      "Gradient Descent(8/49): loss=5584284133701463.0\n",
      "Gradient Descent(9/49): loss=1.6032926701904957e+18\n",
      "Gradient Descent(10/49): loss=4.6271537054370785e+20\n",
      "Gradient Descent(11/49): loss=1.3371360379548008e+23\n",
      "Gradient Descent(12/49): loss=3.865235781695134e+25\n",
      "Gradient Descent(13/49): loss=1.1174052009645624e+28\n",
      "Gradient Descent(14/49): loss=3.230381867526847e+30\n",
      "Gradient Descent(15/49): loss=9.338973322536066e+32\n",
      "Gradient Descent(16/49): loss=2.699882871724516e+35\n",
      "Gradient Descent(17/49): loss=7.805322296051162e+37\n",
      "Gradient Descent(18/49): loss=2.2565075404471256e+40\n",
      "Gradient Descent(19/49): loss=6.523531224786323e+42\n",
      "Gradient Descent(20/49): loss=1.8859436127445132e+45\n",
      "Gradient Descent(21/49): loss=5.45223620736293e+47\n",
      "Gradient Descent(22/49): loss=1.5762337467609448e+50\n",
      "Gradient Descent(23/49): loss=4.556869383684847e+52\n",
      "Gradient Descent(24/49): loss=1.317384469338993e+55\n",
      "Gradient Descent(25/49): loss=3.808539797687388e+57\n",
      "Gradient Descent(26/49): loss=1.1010434484522043e+60\n",
      "Gradient Descent(27/49): loss=3.1831009777446044e+62\n",
      "Gradient Descent(28/49): loss=9.202299735548578e+64\n",
      "Gradient Descent(29/49): loss=2.6603717888611607e+67\n",
      "Gradient Descent(30/49): loss=7.691097071775946e+69\n",
      "Gradient Descent(31/49): loss=2.2234852442485845e+72\n",
      "Gradient Descent(32/49): loss=6.428064273865257e+74\n",
      "Gradient Descent(33/49): loss=1.8583442555251592e+77\n",
      "Gradient Descent(34/49): loss=5.372446859444308e+79\n",
      "Gradient Descent(35/49): loss=1.553166759696867e+82\n",
      "Gradient Descent(36/49): loss=4.490183051669591e+84\n",
      "Gradient Descent(37/49): loss=1.2981055454364586e+87\n",
      "Gradient Descent(38/49): loss=3.752804702396979e+89\n",
      "Gradient Descent(39/49): loss=1.0849305115322974e+92\n",
      "Gradient Descent(40/49): loss=3.1365187058680133e+94\n",
      "Gradient Descent(41/49): loss=9.067631048891308e+96\n",
      "Gradient Descent(42/49): loss=2.6214392627403504e+99\n",
      "Gradient Descent(43/49): loss=7.578543691493317e+101\n",
      "Gradient Descent(44/49): loss=2.190946221803171e+104\n",
      "Gradient Descent(45/49): loss=6.333994421938568e+106\n",
      "Gradient Descent(46/49): loss=1.8311487948859823e+109\n",
      "Gradient Descent(47/49): loss=5.293825168835706e+111\n",
      "Gradient Descent(48/49): loss=1.5304373405627003e+114\n",
      "Gradient Descent(49/49): loss=4.424472623646791e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=5.981038787700029\n",
      "Gradient Descent(2/49): loss=381.3335140698272\n",
      "Gradient Descent(3/49): loss=28082.449826226355\n",
      "Gradient Descent(4/49): loss=2387322.035546754\n",
      "Gradient Descent(5/49): loss=285381370.421121\n",
      "Gradient Descent(6/49): loss=54157757426.43412\n",
      "Gradient Descent(7/49): loss=13774444026560.271\n",
      "Gradient Descent(8/49): loss=3888302227609457.5\n",
      "Gradient Descent(9/49): loss=1.1292341912723272e+18\n",
      "Gradient Descent(10/49): loss=3.302923704864225e+20\n",
      "Gradient Descent(11/49): loss=9.67765681711003e+22\n",
      "Gradient Descent(12/49): loss=2.8367846736429155e+25\n",
      "Gradient Descent(13/49): loss=8.316247657239476e+27\n",
      "Gradient Descent(14/49): loss=2.438031850206922e+30\n",
      "Gradient Descent(15/49): loss=7.147497157817302e+32\n",
      "Gradient Descent(16/49): loss=2.0954111651573048e+35\n",
      "Gradient Descent(17/49): loss=6.143058811146123e+37\n",
      "Gradient Descent(18/49): loss=1.8009436765773994e+40\n",
      "Gradient Descent(19/49): loss=5.2797772173687564e+42\n",
      "Gradient Descent(20/49): loss=1.5478578243543913e+45\n",
      "Gradient Descent(21/49): loss=4.5378123902009125e+47\n",
      "Gradient Descent(22/49): loss=1.3303380304705232e+50\n",
      "Gradient Descent(23/49): loss=3.900115569498833e+52\n",
      "Gradient Descent(24/49): loss=1.1433862001482462e+55\n",
      "Gradient Descent(25/49): loss=3.352034008733776e+57\n",
      "Gradient Descent(26/49): loss=9.827066300313345e+59\n",
      "Gradient Descent(27/49): loss=2.880974113602728e+62\n",
      "Gradient Descent(28/49): loss=8.446072906809304e+64\n",
      "Gradient Descent(29/49): loss=2.476112062594391e+67\n",
      "Gradient Descent(30/49): loss=7.259149919937958e+69\n",
      "Gradient Descent(31/49): loss=2.1281451011922943e+72\n",
      "Gradient Descent(32/49): loss=6.239024709063283e+74\n",
      "Gradient Descent(33/49): loss=1.8290777869654682e+77\n",
      "Gradient Descent(34/49): loss=5.3622572545844224e+79\n",
      "Gradient Descent(35/49): loss=1.57203827356338e+82\n",
      "Gradient Descent(36/49): loss=4.60870155275621e+84\n",
      "Gradient Descent(37/49): loss=1.351120412242324e+87\n",
      "Gradient Descent(38/49): loss=3.9610427090601547e+89\n",
      "Gradient Descent(39/49): loss=1.1612480427972699e+92\n",
      "Gradient Descent(40/49): loss=3.404399083645446e+94\n",
      "Gradient Descent(41/49): loss=9.980583556298258e+96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/49): loss=2.9259803470980923e+99\n",
      "Gradient Descent(43/49): loss=8.578016448949561e+101\n",
      "Gradient Descent(44/49): loss=2.514793589486243e+104\n",
      "Gradient Descent(45/49): loss=7.37255149294517e+106\n",
      "Gradient Descent(46/49): loss=2.1613907297748004e+109\n",
      "Gradient Descent(47/49): loss=6.336490007871337e+111\n",
      "Gradient Descent(48/49): loss=1.8576514216861201e+114\n",
      "Gradient Descent(49/49): loss=5.4460257969409344e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.080379003731782\n",
      "Gradient Descent(2/49): loss=393.4377346279428\n",
      "Gradient Descent(3/49): loss=30283.705818476483\n",
      "Gradient Descent(4/49): loss=2943039.395194458\n",
      "Gradient Descent(5/49): loss=445589722.42882633\n",
      "Gradient Descent(6/49): loss=102223836931.74539\n",
      "Gradient Descent(7/49): loss=28346944019377.85\n",
      "Gradient Descent(8/49): loss=8316501440878568.0\n",
      "Gradient Descent(9/49): loss=2.4750464589852273e+18\n",
      "Gradient Descent(10/49): loss=7.39149789796105e+20\n",
      "Gradient Descent(11/49): loss=2.209240996742457e+23\n",
      "Gradient Descent(12/49): loss=6.604504909880599e+25\n",
      "Gradient Descent(13/49): loss=1.974504562832519e+28\n",
      "Gradient Descent(14/49): loss=5.903111237221346e+30\n",
      "Gradient Descent(15/49): loss=1.7648385294902003e+33\n",
      "Gradient Descent(16/49): loss=5.276297432113151e+35\n",
      "Gradient Descent(17/49): loss=1.5774428350656161e+38\n",
      "Gradient Descent(18/49): loss=4.716045717152349e+40\n",
      "Gradient Descent(19/49): loss=1.409945698840741e+43\n",
      "Gradient Descent(20/49): loss=4.2152833054410025e+45\n",
      "Gradient Descent(21/49): loss=1.2602338777164167e+48\n",
      "Gradient Descent(22/49): loss=3.7676932050757375e+50\n",
      "Gradient Descent(23/49): loss=1.1264188607364675e+53\n",
      "Gradient Descent(24/49): loss=3.3676294240794534e+55\n",
      "Gradient Descent(25/49): loss=1.006812681610261e+58\n",
      "Gradient Descent(26/49): loss=3.0100454895767736e+60\n",
      "Gradient Descent(27/49): loss=8.999066077347516e+62\n",
      "Gradient Descent(28/49): loss=2.690430777371923e+65\n",
      "Gradient Descent(29/49): loss=8.043521078315929e+67\n",
      "Gradient Descent(30/49): loss=2.404753613490502e+70\n",
      "Gradient Descent(31/49): loss=7.189438412967395e+72\n",
      "Gradient Descent(32/49): loss=2.1494104179274194e+75\n",
      "Gradient Descent(33/49): loss=6.426044538279869e+77\n",
      "Gradient Descent(34/49): loss=1.9211802484783338e+80\n",
      "Gradient Descent(35/49): loss=5.743709874957202e+82\n",
      "Gradient Descent(36/49): loss=1.717184171230722e+85\n",
      "Gradient Descent(37/49): loss=5.133827338288556e+87\n",
      "Gradient Descent(38/49): loss=1.5348489452047788e+90\n",
      "Gradient Descent(39/49): loss=4.588703766928022e+92\n",
      "Gradient Descent(40/49): loss=1.3718745630574343e+95\n",
      "Gradient Descent(41/49): loss=4.101462880058531e+97\n",
      "Gradient Descent(42/49): loss=1.226205238400784e+100\n",
      "Gradient Descent(43/49): loss=3.6659585388227667e+102\n",
      "Gradient Descent(44/49): loss=1.0960034737655125e+105\n",
      "Gradient Descent(45/49): loss=3.276697217890074e+107\n",
      "Gradient Descent(46/49): loss=9.79626882097432e+109\n",
      "Gradient Descent(47/49): loss=2.928768709199982e+112\n",
      "Gradient Descent(48/49): loss=8.756074694095543e+114\n",
      "Gradient Descent(49/49): loss=2.617784183767944e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.131887682129333\n",
      "Gradient Descent(2/49): loss=401.11430882993284\n",
      "Gradient Descent(3/49): loss=30846.164038366587\n",
      "Gradient Descent(4/49): loss=2894248.1628143853\n",
      "Gradient Descent(5/49): loss=405224916.3210263\n",
      "Gradient Descent(6/49): loss=86426105173.55272\n",
      "Gradient Descent(7/49): loss=22883269145383.59\n",
      "Gradient Descent(8/49): loss=6497406481598729.0\n",
      "Gradient Descent(9/49): loss=1.8796777111950817e+18\n",
      "Gradient Descent(10/49): loss=5.463633464809609e+20\n",
      "Gradient Descent(11/49): loss=1.5899814431833264e+23\n",
      "Gradient Descent(12/49): loss=4.628388739712001e+25\n",
      "Gradient Descent(13/49): loss=1.3474082098158124e+28\n",
      "Gradient Descent(14/49): loss=3.922621353987741e+30\n",
      "Gradient Descent(15/49): loss=1.1419722010862483e+33\n",
      "Gradient Descent(16/49): loss=3.324567526029498e+35\n",
      "Gradient Descent(17/49): loss=9.678652660430155e+37\n",
      "Gradient Descent(18/49): loss=2.8176995352155016e+40\n",
      "Gradient Descent(19/49): loss=8.203033091535419e+42\n",
      "Gradient Descent(20/49): loss=2.388109567510427e+45\n",
      "Gradient Descent(21/49): loss=6.952388523555197e+47\n",
      "Gradient Descent(22/49): loss=2.0240154326495335e+50\n",
      "Gradient Descent(23/49): loss=5.89241878239011e+52\n",
      "Gradient Descent(24/49): loss=1.7154315400731365e+55\n",
      "Gradient Descent(25/49): loss=4.994053337627478e+57\n",
      "Gradient Descent(26/49): loss=1.4538947289047408e+60\n",
      "Gradient Descent(27/49): loss=4.23265379809035e+62\n",
      "Gradient Descent(28/49): loss=1.23223214296854e+65\n",
      "Gradient Descent(29/49): loss=3.587338172684794e+67\n",
      "Gradient Descent(30/49): loss=1.0443645086386976e+70\n",
      "Gradient Descent(31/49): loss=3.0404081644967256e+72\n",
      "Gradient Descent(32/49): loss=8.85139405856275e+74\n",
      "Gradient Descent(33/49): loss=2.576863780818301e+77\n",
      "Gradient Descent(34/49): loss=7.501899588878461e+79\n",
      "Gradient Descent(35/49): loss=2.1839919463551007e+82\n",
      "Gradient Descent(36/49): loss=6.358150712674433e+84\n",
      "Gradient Descent(37/49): loss=1.8510178369728374e+87\n",
      "Gradient Descent(38/49): loss=5.388779202672354e+89\n",
      "Gradient Descent(39/49): loss=1.5688093715317046e+92\n",
      "Gradient Descent(40/49): loss=4.567199270263654e+94\n",
      "Gradient Descent(41/49): loss=1.3296267572605906e+97\n",
      "Gradient Descent(42/49): loss=3.8708784290054806e+99\n",
      "Gradient Descent(43/49): loss=1.1269102197529726e+102\n",
      "Gradient Descent(44/49): loss=3.280719523165151e+104\n",
      "Gradient Descent(45/49): loss=9.551000959096936e+106\n",
      "Gradient Descent(46/49): loss=2.7805369729583503e+109\n",
      "Gradient Descent(47/49): loss=8.094843557339206e+111\n",
      "Gradient Descent(48/49): loss=2.3566128720840641e+114\n",
      "Gradient Descent(49/49): loss=6.860693711414914e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.25488248380831\n",
      "Gradient Descent(2/49): loss=407.7746618596906\n",
      "Gradient Descent(3/49): loss=31019.971201909728\n",
      "Gradient Descent(4/49): loss=2838966.652523359\n",
      "Gradient Descent(5/49): loss=382970465.81569695\n",
      "Gradient Descent(6/49): loss=79617550262.25874\n",
      "Gradient Descent(7/49): loss=20888483029100.93\n",
      "Gradient Descent(8/49): loss=5917406382849660.0\n",
      "Gradient Descent(9/49): loss=1.7112413441193984e+18\n",
      "Gradient Descent(10/49): loss=4.974548070171721e+20\n",
      "Gradient Descent(11/49): loss=1.4479662724604599e+23\n",
      "Gradient Descent(12/49): loss=4.216018584117805e+25\n",
      "Gradient Descent(13/49): loss=1.2276682698607982e+28\n",
      "Gradient Descent(14/49): loss=3.57493428837998e+30\n",
      "Gradient Descent(15/49): loss=1.0410155133862164e+33\n",
      "Gradient Descent(16/49): loss=3.0314252266970452e+35\n",
      "Gradient Descent(17/49): loss=8.82747808407771e+37\n",
      "Gradient Descent(18/49): loss=2.5705524368920467e+40\n",
      "Gradient Descent(19/49): loss=7.48542209637992e+42\n",
      "Gradient Descent(20/49): loss=2.1797471783866487e+45\n",
      "Gradient Descent(21/49): loss=6.347401263113683e+47\n",
      "Gradient Descent(22/49): loss=1.8483566904230409e+50\n",
      "Gradient Descent(23/49): loss=5.382395587808088e+52\n",
      "Gradient Descent(24/49): loss=1.5673480348379575e+55\n",
      "Gradient Descent(25/49): loss=4.564101285838486e+57\n",
      "Gradient Descent(26/49): loss=1.329061579457525e+60\n",
      "Gradient Descent(27/49): loss=3.8702135894120483e+62\n",
      "Gradient Descent(28/49): loss=1.1270021990843235e+65\n",
      "Gradient Descent(29/49): loss=3.2818187611548907e+67\n",
      "Gradient Descent(30/49): loss=9.556622329414211e+69\n",
      "Gradient Descent(31/49): loss=2.7828785497868146e+72\n",
      "Gradient Descent(32/49): loss=8.103713588248726e+74\n",
      "Gradient Descent(33/49): loss=2.359793025297479e+77\n",
      "Gradient Descent(34/49): loss=6.871692911651704e+79\n",
      "Gradient Descent(35/49): loss=2.001029877020317e+82\n",
      "Gradient Descent(36/49): loss=5.826978330097585e+84\n",
      "Gradient Descent(37/49): loss=1.6968100701218282e+87\n",
      "Gradient Descent(38/49): loss=4.941093395174199e+89\n",
      "Gradient Descent(39/49): loss=1.4388412922420667e+92\n",
      "Gradient Descent(40/49): loss=4.189890978953731e+94\n",
      "Gradient Descent(41/49): loss=1.2200919246738272e+97\n",
      "Gradient Descent(42/49): loss=3.5528950804012884e+99\n",
      "Gradient Descent(43/49): loss=1.0345993770686028e+102\n",
      "Gradient Descent(44/49): loss=3.012742698019184e+104\n",
      "Gradient Descent(45/49): loss=8.773075613273212e+106\n",
      "Gradient Descent(46/49): loss=2.554710555495247e+109\n",
      "Gradient Descent(47/49): loss=7.439290745977835e+111\n",
      "Gradient Descent(48/49): loss=2.166313780015005e+114\n",
      "Gradient Descent(49/49): loss=6.308283348140855e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.024537661782189\n",
      "Gradient Descent(2/49): loss=387.21184877684044\n",
      "Gradient Descent(3/49): loss=28732.157446040652\n",
      "Gradient Descent(4/49): loss=2460594.5174627\n",
      "Gradient Descent(5/49): loss=296178786.1334708\n",
      "Gradient Descent(6/49): loss=56585301561.12736\n",
      "Gradient Descent(7/49): loss=14492184281811.717\n",
      "Gradient Descent(8/49): loss=4120206019971876.0\n",
      "Gradient Descent(9/49): loss=1.2052379912345457e+18\n",
      "Gradient Descent(10/49): loss=3.550801834225426e+20\n",
      "Gradient Descent(11/49): loss=1.0479481620318875e+23\n",
      "Gradient Descent(12/49): loss=3.094128542254969e+25\n",
      "Gradient Descent(13/49): loss=9.136544677753319e+27\n",
      "Gradient Descent(14/49): loss=2.697966680965348e+30\n",
      "Gradient Descent(15/49): loss=7.966982272221659e+32\n",
      "Gradient Descent(16/49): loss=2.3526199152495003e+35\n",
      "Gradient Descent(17/49): loss=6.947200697059344e+37\n",
      "Gradient Descent(18/49): loss=2.0514831777650785e+40\n",
      "Gradient Descent(19/49): loss=6.057955574959304e+42\n",
      "Gradient Descent(20/49): loss=1.7888923652063644e+45\n",
      "Gradient Descent(21/49): loss=5.282534443802384e+47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=1.5599133129174589e+50\n",
      "Gradient Descent(23/49): loss=4.606367587163837e+52\n",
      "Gradient Descent(24/49): loss=1.3602436861558256e+55\n",
      "Gradient Descent(25/49): loss=4.016750401967445e+57\n",
      "Gradient Descent(26/49): loss=1.1861318641603174e+60\n",
      "Gradient Descent(27/49): loss=3.5026044896583042e+62\n",
      "Gradient Descent(28/49): loss=1.0343064360437123e+65\n",
      "Gradient Descent(29/49): loss=3.054269492316656e+67\n",
      "Gradient Descent(30/49): loss=9.019147330629084e+69\n",
      "Gradient Descent(31/49): loss=2.6633215823367273e+72\n",
      "Gradient Descent(32/49): loss=7.864692293973068e+74\n",
      "Gradient Descent(33/49): loss=2.322415185950226e+77\n",
      "Gradient Descent(34/49): loss=6.85800803683768e+79\n",
      "Gradient Descent(35/49): loss=2.0251449662342364e+82\n",
      "Gradient Descent(36/49): loss=5.980179830986284e+84\n",
      "Gradient Descent(37/49): loss=1.7659254723594362e+87\n",
      "Gradient Descent(38/49): loss=5.214714042158806e+89\n",
      "Gradient Descent(39/49): loss=1.5398861937902176e+92\n",
      "Gradient Descent(40/49): loss=4.547228228921488e+94\n",
      "Gradient Descent(41/49): loss=1.3427800475959946e+97\n",
      "Gradient Descent(42/49): loss=3.965180909007378e+99\n",
      "Gradient Descent(43/49): loss=1.1709035794287591e+102\n",
      "Gradient Descent(44/49): loss=3.4576359156896085e+104\n",
      "Gradient Descent(45/49): loss=1.0210273788127991e+107\n",
      "Gradient Descent(46/49): loss=3.01505691664885e+109\n",
      "Gradient Descent(47/49): loss=8.903354013094264e+111\n",
      "Gradient Descent(48/49): loss=2.6291282345206294e+114\n",
      "Gradient Descent(49/49): loss=7.763720574726642e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.124475239657871\n",
      "Gradient Descent(2/49): loss=399.4766681460757\n",
      "Gradient Descent(3/49): loss=30979.859370112154\n",
      "Gradient Descent(4/49): loss=3032367.158100581\n",
      "Gradient Descent(5/49): loss=462231810.25068754\n",
      "Gradient Descent(6/49): loss=106766726689.95349\n",
      "Gradient Descent(7/49): loss=29815963612661.79\n",
      "Gradient Descent(8/49): loss=8810313785491724.0\n",
      "Gradient Descent(9/49): loss=2.640937294497664e+18\n",
      "Gradient Descent(10/49): loss=7.943936482165728e+20\n",
      "Gradient Descent(11/49): loss=2.391532099958464e+23\n",
      "Gradient Descent(12/49): loss=7.201177855560325e+25\n",
      "Gradient Descent(13/49): loss=2.168461167433664e+28\n",
      "Gradient Descent(14/49): loss=6.529873011395492e+30\n",
      "Gradient Descent(15/49): loss=1.9663418020663644e+33\n",
      "Gradient Descent(16/49): loss=5.921252390432443e+35\n",
      "Gradient Descent(17/49): loss=1.7830692169014307e+38\n",
      "Gradient Descent(18/49): loss=5.369364016198386e+40\n",
      "Gradient Descent(19/49): loss=1.6168789144074122e+43\n",
      "Gradient Descent(20/49): loss=4.868914496365111e+45\n",
      "Gradient Descent(21/49): loss=1.4661783374044793e+48\n",
      "Gradient Descent(22/49): loss=4.415109197221397e+50\n",
      "Gradient Descent(23/49): loss=1.3295237507376322e+53\n",
      "Gradient Descent(24/49): loss=4.0036006468431644e+55\n",
      "Gradient Descent(25/49): loss=1.2056060021896556e+58\n",
      "Gradient Descent(26/49): loss=3.6304465922754005e+60\n",
      "Gradient Descent(27/49): loss=1.0932379596175719e+63\n",
      "Gradient Descent(28/49): loss=3.2920722174836937e+65\n",
      "Gradient Descent(29/49): loss=9.913431371262744e+67\n",
      "Gradient Descent(30/49): loss=2.985235895822989e+70\n",
      "Gradient Descent(31/49): loss=8.989453822763418e+72\n",
      "Gradient Descent(32/49): loss=2.7069981352115333e+75\n",
      "Gradient Descent(33/49): loss=8.151595245400697e+77\n",
      "Gradient Descent(34/49): loss=2.4546934177938943e+80\n",
      "Gradient Descent(35/49): loss=7.391828953676592e+82\n",
      "Gradient Descent(36/49): loss=2.225904664278501e+85\n",
      "Gradient Descent(37/49): loss=6.70287638621902e+87\n",
      "Gradient Descent(38/49): loss=2.0184400783173057e+90\n",
      "Gradient Descent(39/49): loss=6.078137377162174e+92\n",
      "Gradient Descent(40/49): loss=1.8303121490955723e+95\n",
      "Gradient Descent(41/49): loss=5.511626926555208e+97\n",
      "Gradient Descent(42/49): loss=1.6597186109778868e+100\n",
      "Gradient Descent(43/49): loss=4.997917864059916e+102\n",
      "Gradient Descent(44/49): loss=1.5050251778024057e+105\n",
      "Gradient Descent(45/49): loss=4.5320888566566205e+107\n",
      "Gradient Descent(46/49): loss=1.3647498864187211e+110\n",
      "Gradient Descent(47/49): loss=4.109677262272299e+112\n",
      "Gradient Descent(48/49): loss=1.2375488994805855e+115\n",
      "Gradient Descent(49/49): loss=3.726636377667388e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.176365990441896\n",
      "Gradient Descent(2/49): loss=407.26816684303344\n",
      "Gradient Descent(3/49): loss=31555.460124590918\n",
      "Gradient Descent(4/49): loss=2982283.4709108346\n",
      "Gradient Descent(5/49): loss=420406554.4603544\n",
      "Gradient Descent(6/49): loss=90274775585.69147\n",
      "Gradient Descent(7/49): loss=24070554863322.25\n",
      "Gradient Descent(8/49): loss=6883544901385453.0\n",
      "Gradient Descent(9/49): loss=2.0057632225238e+18\n",
      "Gradient Descent(10/49): loss=5.872296954109667e+20\n",
      "Gradient Descent(11/49): loss=1.721275568967289e+23\n",
      "Gradient Descent(12/49): loss=5.046851795830069e+25\n",
      "Gradient Descent(13/49): loss=1.4798660879245696e+28\n",
      "Gradient Descent(14/49): loss=4.339424720116957e+30\n",
      "Gradient Descent(15/49): loss=1.2724591636814733e+33\n",
      "Gradient Descent(16/49): loss=3.731264479441363e+35\n",
      "Gradient Descent(17/49): loss=1.0941285090496044e+38\n",
      "Gradient Descent(18/49): loss=3.208342060182873e+40\n",
      "Gradient Descent(19/49): loss=9.407906718312301e+42\n",
      "Gradient Descent(20/49): loss=2.7587055100580853e+45\n",
      "Gradient Descent(21/49): loss=8.089425551405222e+47\n",
      "Gradient Descent(22/49): loss=2.3720837737205225e+50\n",
      "Gradient Descent(23/49): loss=6.955724351694735e+52\n",
      "Gradient Descent(24/49): loss=2.0396455552517327e+55\n",
      "Gradient Descent(25/49): loss=5.980906920299686e+57\n",
      "Gradient Descent(26/49): loss=1.7537972466450793e+60\n",
      "Gradient Descent(27/49): loss=5.142706320844366e+62\n",
      "Gradient Descent(28/49): loss=1.5080094550864451e+65\n",
      "Gradient Descent(29/49): loss=4.421976241211451e+67\n",
      "Gradient Descent(30/49): loss=1.2966678565498698e+70\n",
      "Gradient Descent(31/49): loss=3.8022536497143426e+72\n",
      "Gradient Descent(32/49): loss=1.114944952459376e+75\n",
      "Gradient Descent(33/49): loss=3.2693827438577645e+77\n",
      "Gradient Descent(34/49): loss=9.586897991920648e+79\n",
      "Gradient Descent(35/49): loss=2.8111916012331313e+82\n",
      "Gradient Descent(36/49): loss=8.243331915603502e+84\n",
      "Gradient Descent(37/49): loss=2.4172141465207086e+87\n",
      "Gradient Descent(38/49): loss=7.088061344563725e+89\n",
      "Gradient Descent(39/49): loss=2.0784510837243665e+92\n",
      "Gradient Descent(40/49): loss=6.09469740375243e+94\n",
      "Gradient Descent(41/49): loss=1.7871643328139035e+97\n",
      "Gradient Descent(42/49): loss=5.240549515248601e+99\n",
      "Gradient Descent(43/49): loss=1.5367002752641093e+102\n",
      "Gradient Descent(44/49): loss=4.5061070964514295e+104\n",
      "Gradient Descent(45/49): loss=1.3213377710367394e+107\n",
      "Gradient Descent(46/49): loss=3.874593896233078e+109\n",
      "Gradient Descent(47/49): loss=1.1361574753855428e+112\n",
      "Gradient Descent(48/49): loss=3.331584789129602e+114\n",
      "Gradient Descent(49/49): loss=9.769294703969886e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.300318322772728\n",
      "Gradient Descent(2/49): loss=414.0327346411275\n",
      "Gradient Descent(3/49): loss=31733.7443783273\n",
      "Gradient Descent(4/49): loss=2925437.823000699\n",
      "Gradient Descent(5/49): loss=397338742.35812366\n",
      "Gradient Descent(6/49): loss=83165330675.792\n",
      "Gradient Descent(7/49): loss=21972471824862.926\n",
      "Gradient Descent(8/49): loss=6269090273779448.0\n",
      "Gradient Descent(9/49): loss=1.826028971004388e+18\n",
      "Gradient Descent(10/49): loss=5.346627793111781e+20\n",
      "Gradient Descent(11/49): loss=1.5675325807435802e+23\n",
      "Gradient Descent(12/49): loss=4.597195601585738e+25\n",
      "Gradient Descent(13/49): loss=1.3483541514668493e+28\n",
      "Gradient Descent(14/49): loss=3.954790908956638e+30\n",
      "Gradient Descent(15/49): loss=1.1599658250711785e+33\n",
      "Gradient Descent(16/49): loss=3.402259103901947e+35\n",
      "Gradient Descent(17/49): loss=9.979061634138664e+37\n",
      "Gradient Descent(18/49): loss=2.9269281204721135e+40\n",
      "Gradient Descent(19/49): loss=8.584883722349714e+42\n",
      "Gradient Descent(20/49): loss=2.5180061082953593e+45\n",
      "Gradient Descent(21/49): loss=7.38548706487992e+47\n",
      "Gradient Descent(22/49): loss=2.1662147292207754e+50\n",
      "Gradient Descent(23/49): loss=6.353658482089414e+52\n",
      "Gradient Descent(24/49): loss=1.8635722286972827e+55\n",
      "Gradient Descent(25/49): loss=5.4659869764422895e+57\n",
      "Gradient Descent(26/49): loss=1.6032120014770172e+60\n",
      "Gradient Descent(27/49): loss=4.702332319411352e+62\n",
      "Gradient Descent(28/49): loss=1.3792267786050872e+65\n",
      "Gradient Descent(29/49): loss=4.0453680803646856e+67\n",
      "Gradient Descent(30/49): loss=1.186534597463713e+70\n",
      "Gradient Descent(31/49): loss=3.4801885094507154e+72\n",
      "Gradient Descent(32/49): loss=1.0207634979378125e+75\n",
      "Gradient Descent(33/49): loss=2.993970343539447e+77\n",
      "Gradient Descent(34/49): loss=8.781523277530054e+79\n",
      "Gradient Descent(35/49): loss=2.5756818613853068e+82\n",
      "Gradient Descent(36/49): loss=7.5546540633156506e+84\n",
      "Gradient Descent(37/49): loss=2.2158325867805727e+87\n",
      "Gradient Descent(38/49): loss=6.499191109862327e+89\n",
      "Gradient Descent(39/49): loss=1.9062579607552416e+92\n",
      "Gradient Descent(40/49): loss=5.591187197786186e+94\n",
      "Gradient Descent(41/49): loss=1.6399340972877697e+97\n",
      "Gradient Descent(42/49): loss=4.810040780805798e+99\n",
      "Gradient Descent(43/49): loss=1.4108184195498762e+102\n",
      "Gradient Descent(44/49): loss=4.138028560763636e+104\n",
      "Gradient Descent(45/49): loss=1.2137125609090114e+107\n",
      "Gradient Descent(46/49): loss=3.55990336672902e+109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(47/49): loss=1.0441444200723404e+112\n",
      "Gradient Descent(48/49): loss=3.062548214531926e+114\n",
      "Gradient Descent(49/49): loss=8.982666943412778e+116\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.068205328992689\n",
      "Gradient Descent(2/49): loss=393.15796890228245\n",
      "Gradient Descent(3/49): loss=29394.346314485345\n",
      "Gradient Descent(4/49): loss=2535828.32371623\n",
      "Gradient Descent(5/49): loss=307342776.24931085\n",
      "Gradient Descent(6/49): loss=59112266454.650185\n",
      "Gradient Descent(7/49): loss=15244529427958.225\n",
      "Gradient Descent(8/49): loss=4365029068792084.0\n",
      "Gradient Descent(9/49): loss=1.286055237652438e+18\n",
      "Gradient Descent(10/49): loss=3.8162870138544017e+20\n",
      "Gradient Descent(11/49): loss=1.134448427389324e+23\n",
      "Gradient Descent(12/49): loss=3.3737614978350857e+25\n",
      "Gradient Descent(13/49): loss=1.0034350492563595e+28\n",
      "Gradient Descent(14/49): loss=2.9845246876929836e+30\n",
      "Gradient Descent(15/49): loss=8.876949870948522e+32\n",
      "Gradient Descent(16/49): loss=2.6402983971231486e+35\n",
      "Gradient Descent(17/49): loss=7.853123289582661e+37\n",
      "Gradient Descent(18/49): loss=2.3357795831970564e+40\n",
      "Gradient Descent(19/49): loss=6.947384089624607e+42\n",
      "Gradient Descent(20/49): loss=2.066382731091273e+45\n",
      "Gradient Descent(21/49): loss=6.146108437009094e+47\n",
      "Gradient Descent(22/49): loss=1.8280567468418123e+50\n",
      "Gradient Descent(23/49): loss=5.437247819783862e+52\n",
      "Gradient Descent(24/49): loss=1.617218059853324e+55\n",
      "Gradient Descent(25/49): loss=4.810143550218844e+57\n",
      "Gradient Descent(26/49): loss=1.4306964254304587e+60\n",
      "Gradient Descent(27/49): loss=4.25536626998784e+62\n",
      "Gradient Descent(28/49): loss=1.2656872394367578e+65\n",
      "Gradient Descent(29/49): loss=3.7645741551588365e+67\n",
      "Gradient Descent(30/49): loss=1.1197093664305572e+70\n",
      "Gradient Descent(31/49): loss=3.330387485007382e+72\n",
      "Gradient Descent(32/49): loss=9.905678324056054e+74\n",
      "Gradient Descent(33/49): loss=2.946277677939879e+77\n",
      "Gradient Descent(34/49): loss=8.763208203971018e+79\n",
      "Gradient Descent(35/49): loss=2.6064691254709164e+82\n",
      "Gradient Descent(36/49): loss=7.752504726470704e+84\n",
      "Gradient Descent(37/49): loss=2.305852348168262e+87\n",
      "Gradient Descent(38/49): loss=6.858370602985285e+89\n",
      "Gradient Descent(39/49): loss=2.0399071677446263e+92\n",
      "Gradient Descent(40/49): loss=6.067361322242591e+94\n",
      "Gradient Descent(41/49): loss=1.804634740087033e+97\n",
      "Gradient Descent(42/49): loss=5.367582993928696e+99\n",
      "Gradient Descent(43/49): loss=1.5964974272480127e+102\n",
      "Gradient Descent(44/49): loss=4.7485135080210586e+104\n",
      "Gradient Descent(45/49): loss=1.4123656042920725e+107\n",
      "Gradient Descent(46/49): loss=4.200844320686402e+109\n",
      "Gradient Descent(47/49): loss=1.2494706011683399e+112\n",
      "Gradient Descent(48/49): loss=3.716340487782947e+114\n",
      "Gradient Descent(49/49): loss=1.10536307202591e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.168741887650611\n",
      "Gradient Descent(2/49): loss=405.5849416328877\n",
      "Gradient Descent(3/49): loss=31689.302918424713\n",
      "Gradient Descent(4/49): loss=3124060.0653993916\n",
      "Gradient Descent(5/49): loss=479431608.78284365\n",
      "Gradient Descent(6/49): loss=111494054189.1749\n",
      "Gradient Descent(7/49): loss=31355420440981.992\n",
      "Gradient Descent(8/49): loss=9331514301068988.0\n",
      "Gradient Descent(9/49): loss=2.8172906262195343e+18\n",
      "Gradient Descent(10/49): loss=8.535454681300908e+20\n",
      "Gradient Descent(11/49): loss=2.5881276277413495e+23\n",
      "Gradient Descent(12/49): loss=7.849317938989726e+25\n",
      "Gradient Descent(13/49): loss=2.380669071302196e+28\n",
      "Gradient Descent(14/49): loss=7.220564242905665e+30\n",
      "Gradient Descent(15/49): loss=2.1900016350968994e+33\n",
      "Gradient Descent(16/49): loss=6.64229298758307e+35\n",
      "Gradient Descent(17/49): loss=2.0146132490590833e+38\n",
      "Gradient Descent(18/49): loss=6.110339763958828e+40\n",
      "Gradient Descent(19/49): loss=1.8532714597215284e+43\n",
      "Gradient Descent(20/49): loss=5.620988755993363e+45\n",
      "Gradient Descent(21/49): loss=1.7048508703693874e+48\n",
      "Gradient Descent(22/49): loss=5.170827796907806e+50\n",
      "Gradient Descent(23/49): loss=1.568316652838655e+53\n",
      "Gradient Descent(24/49): loss=4.756718305450788e+55\n",
      "Gradient Descent(25/49): loss=1.442716877136751e+58\n",
      "Gradient Descent(26/49): loss=4.375773072773549e+60\n",
      "Gradient Descent(27/49): loss=1.3271758504975664e+63\n",
      "Gradient Descent(28/49): loss=4.0253361151280434e+65\n",
      "Gradient Descent(29/49): loss=1.220888010709323e+68\n",
      "Gradient Descent(30/49): loss=3.7029641551966215e+70\n",
      "Gradient Descent(31/49): loss=1.1231123095970617e+73\n",
      "Gradient Descent(32/49): loss=3.4064095872984145e+75\n",
      "Gradient Descent(33/49): loss=1.0331670463661676e+78\n",
      "Gradient Descent(34/49): loss=3.1336048068828e+80\n",
      "Gradient Descent(35/49): loss=9.504251147242416e+82\n",
      "Gradient Descent(36/49): loss=2.8826477950076205e+85\n",
      "Gradient Descent(37/49): loss=8.743096306407518e+87\n",
      "Gradient Descent(38/49): loss=2.6517888572965028e+90\n",
      "Gradient Descent(39/49): loss=8.04289910260784e+92\n",
      "Gradient Descent(40/49): loss=2.4394184249148613e+95\n",
      "Gradient Descent(41/49): loss=7.398777699305665e+97\n",
      "Gradient Descent(42/49): loss=2.2440558325148578e+100\n",
      "Gradient Descent(43/49): loss=6.806241225380302e+102\n",
      "Gradient Descent(44/49): loss=2.0643389949059742e+105\n",
      "Gradient Descent(45/49): loss=6.261158464378813e+107\n",
      "Gradient Descent(46/49): loss=1.8990149104773697e+110\n",
      "Gradient Descent(47/49): loss=5.759729051312499e+112\n",
      "Gradient Descent(48/49): loss=1.7469309251602101e+115\n",
      "Gradient Descent(49/49): loss=5.298456976176126e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.221016114051376\n",
      "Gradient Descent(2/49): loss=413.492648797202\n",
      "Gradient Descent(3/49): loss=32278.300464362637\n",
      "Gradient Descent(4/49): loss=3072654.502183004\n",
      "Gradient Descent(5/49): loss=436098546.94391537\n",
      "Gradient Descent(6/49): loss=94280024420.10439\n",
      "Gradient Descent(7/49): loss=25314837738688.445\n",
      "Gradient Descent(8/49): loss=7291118509743219.0\n",
      "Gradient Descent(9/49): loss=2.1398070359880899e+18\n",
      "Gradient Descent(10/49): loss=6.309891470433718e+20\n",
      "Gradient Descent(11/49): loss=1.862880199280166e+23\n",
      "Gradient Descent(12/49): loss=5.501437601542731e+25\n",
      "Gradient Descent(13/49): loss=1.6247977424468446e+28\n",
      "Gradient Descent(14/49): loss=4.798774385867417e+30\n",
      "Gradient Descent(15/49): loss=1.4173050067194844e+33\n",
      "Gradient Descent(16/49): loss=4.18597661907949e+35\n",
      "Gradient Descent(17/49): loss=1.236318580600365e+38\n",
      "Gradient Descent(18/49): loss=3.651438786382418e+40\n",
      "Gradient Descent(19/49): loss=1.0784441525972579e+43\n",
      "Gradient Descent(20/49): loss=3.1851603252246294e+45\n",
      "Gradient Descent(21/49): loss=9.40729873096194e+47\n",
      "Gradient Descent(22/49): loss=2.778424330355118e+50\n",
      "Gradient Descent(23/49): loss=8.206013203988557e+52\n",
      "Gradient Descent(24/49): loss=2.4236273764749482e+55\n",
      "Gradient Descent(25/49): loss=7.158128452885711e+57\n",
      "Gradient Descent(26/49): loss=2.1141369933932403e+60\n",
      "Gradient Descent(27/49): loss=6.244055630257762e+62\n",
      "Gradient Descent(28/49): loss=1.8441676596926323e+65\n",
      "Gradient Descent(29/49): loss=5.446707330049587e+67\n",
      "Gradient Descent(30/49): loss=1.6086726487851033e+70\n",
      "Gradient Descent(31/49): loss=4.751178159825427e+72\n",
      "Gradient Descent(32/49): loss=1.40324968684275e+75\n",
      "Gradient Descent(33/49): loss=4.14446610374355e+77\n",
      "Gradient Descent(34/49): loss=1.2240586579945138e+80\n",
      "Gradient Descent(35/49): loss=3.61522946672898e+82\n",
      "Gradient Descent(36/49): loss=1.0677498183395062e+85\n",
      "Gradient Descent(37/49): loss=3.1535748561918913e+87\n",
      "Gradient Descent(38/49): loss=9.314011768291739e+89\n",
      "Gradient Descent(39/49): loss=2.7508722378842107e+92\n",
      "Gradient Descent(40/49): loss=8.124638724339696e+94\n",
      "Gradient Descent(41/49): loss=2.3995936086007447e+97\n",
      "Gradient Descent(42/49): loss=7.087145264918217e+99\n",
      "Gradient Descent(43/49): loss=2.0931722699220514e+102\n",
      "Gradient Descent(44/49): loss=6.182136795274376e+104\n",
      "Gradient Descent(45/49): loss=1.825880072303344e+107\n",
      "Gradient Descent(46/49): loss=5.392695355726705e+109\n",
      "Gradient Descent(47/49): loss=1.5927203347474969e+112\n",
      "Gradient Descent(48/49): loss=4.7040633623485353e+114\n",
      "Gradient Descent(49/49): loss=1.3893344383336247e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.345929635218907\n",
      "Gradient Descent(2/49): loss=420.36264866450915\n",
      "Gradient Descent(3/49): loss=32461.156156964826\n",
      "Gradient Descent(4/49): loss=3014206.268217369\n",
      "Gradient Descent(5/49): loss=412190725.82718056\n",
      "Gradient Descent(6/49): loss=86857545744.17896\n",
      "Gradient Descent(7/49): loss=23108509063258.36\n",
      "Gradient Descent(8/49): loss=6640297590898802.0\n",
      "Gradient Descent(9/49): loss=1.9480618355225805e+18\n",
      "Gradient Descent(10/49): loss=5.745048511599545e+20\n",
      "Gradient Descent(11/49): loss=1.6964883487696334e+23\n",
      "Gradient Descent(12/49): loss=5.011276513120056e+25\n",
      "Gradient Descent(13/49): loss=1.4804051386171771e+28\n",
      "Gradient Descent(14/49): loss=4.3734221089828167e+30\n",
      "Gradient Descent(15/49): loss=1.2920054194299652e+33\n",
      "Gradient Descent(16/49): loss=3.816873801284908e+35\n",
      "Gradient Descent(17/49): loss=1.1275904702254885e+38\n",
      "Gradient Descent(18/49): loss=3.331156409939387e+40\n",
      "Gradient Descent(19/49): loss=9.840987064266113e+42\n",
      "Gradient Descent(20/49): loss=2.9072494629862004e+45\n",
      "Gradient Descent(21/49): loss=8.588670419281376e+47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(22/49): loss=2.537286893701897e+50\n",
      "Gradient Descent(23/49): loss=7.495717575728646e+52\n",
      "Gradient Descent(24/49): loss=2.2144039807051914e+55\n",
      "Gradient Descent(25/49): loss=6.541848649236252e+57\n",
      "Gradient Descent(26/49): loss=1.9326095925789404e+60\n",
      "Gradient Descent(27/49): loss=5.709364489447735e+62\n",
      "Gradient Descent(28/49): loss=1.6866750014352497e+65\n",
      "Gradient Descent(29/49): loss=4.9828182553847896e+67\n",
      "Gradient Descent(30/49): loss=1.4720368621737345e+70\n",
      "Gradient Descent(31/49): loss=4.348728796713778e+72\n",
      "Gradient Descent(32/49): loss=1.2847125390216952e+75\n",
      "Gradient Descent(33/49): loss=3.7953305093820086e+77\n",
      "Gradient Descent(34/49): loss=1.1212262072583882e+80\n",
      "Gradient Descent(35/49): loss=3.3123550234568996e+82\n",
      "Gradient Descent(36/49): loss=9.785443588807868e+84\n",
      "Gradient Descent(37/49): loss=2.8908406723204325e+87\n",
      "Gradient Descent(38/49): loss=8.540195155077365e+89\n",
      "Gradient Descent(39/49): loss=2.5229662078976896e+92\n",
      "Gradient Descent(40/49): loss=7.453411041092397e+94\n",
      "Gradient Descent(41/49): loss=2.2019056764842278e+97\n",
      "Gradient Descent(42/49): loss=6.504925840535589e+99\n",
      "Gradient Descent(43/49): loss=1.9217017623765424e+102\n",
      "Gradient Descent(44/49): loss=5.677140299599474e+104\n",
      "Gradient Descent(45/49): loss=1.6771552491828115e+107\n",
      "Gradient Descent(46/49): loss=4.954694760775863e+109\n",
      "Gradient Descent(47/49): loss=1.4637285477549646e+112\n",
      "Gradient Descent(48/49): loss=4.324184162612954e+114\n",
      "Gradient Descent(49/49): loss=1.2774615006909634e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.112041789331536\n",
      "Gradient Descent(2/49): loss=399.17239372484215\n",
      "Gradient Descent(3/49): loss=30069.20756000223\n",
      "Gradient Descent(4/49): loss=2613068.3060019803\n",
      "Gradient Descent(5/49): loss=318884370.38720345\n",
      "Gradient Descent(6/49): loss=61742341860.007805\n",
      "Gradient Descent(7/49): loss=16033014137787.2\n",
      "Gradient Descent(8/49): loss=4623440674682183.0\n",
      "Gradient Descent(9/49): loss=1.3719717865247342e+18\n",
      "Gradient Descent(10/49): loss=4.100559686766521e+20\n",
      "Gradient Descent(11/49): loss=1.2277388087032262e+23\n",
      "Gradient Descent(12/49): loss=3.677523236118112e+25\n",
      "Gradient Descent(13/49): loss=1.1016669608716694e+28\n",
      "Gradient Descent(14/49): loss=3.3003218566617226e+30\n",
      "Gradient Descent(15/49): loss=9.887009740424274e+32\n",
      "Gradient Descent(16/49): loss=2.9619265521050496e+35\n",
      "Gradient Descent(17/49): loss=8.873271454002123e+37\n",
      "Gradient Descent(18/49): loss=2.6582344959996136e+40\n",
      "Gradient Descent(19/49): loss=7.963478663532419e+42\n",
      "Gradient Descent(20/49): loss=2.385680904264372e+45\n",
      "Gradient Descent(21/49): loss=7.146968837126627e+47\n",
      "Gradient Descent(22/49): loss=2.14107274294968e+50\n",
      "Gradient Descent(23/49): loss=6.414177248318427e+52\n",
      "Gradient Descent(24/49): loss=1.9215446980533807e+55\n",
      "Gradient Descent(25/49): loss=5.756520101782063e+57\n",
      "Gradient Descent(26/49): loss=1.7245252590703437e+60\n",
      "Gradient Descent(27/49): loss=5.16629372709355e+62\n",
      "Gradient Descent(28/49): loss=1.5477065780408557e+65\n",
      "Gradient Descent(29/49): loss=4.636584325720446e+67\n",
      "Gradient Descent(30/49): loss=1.389017434863501e+70\n",
      "Gradient Descent(31/49): loss=4.161186983383551e+72\n",
      "Gradient Descent(32/49): loss=1.2465989753671937e+75\n",
      "Gradient Descent(33/49): loss=3.734532986842618e+77\n",
      "Gradient Descent(34/49): loss=1.1187829370473793e+80\n",
      "Gradient Descent(35/49): loss=3.351624593056782e+82\n",
      "Gradient Descent(36/49): loss=1.0040721073588501e+85\n",
      "Gradient Descent(37/49): loss=3.0079764865807234e+87\n",
      "Gradient Descent(38/49): loss=9.011227856555597e+89\n",
      "Gradient Descent(39/49): loss=2.699563239440999e+92\n",
      "Gradient Descent(40/49): loss=8.087290433389197e+94\n",
      "Gradient Descent(41/49): loss=2.422772158044797e+97\n",
      "Gradient Descent(42/49): loss=7.258085978417312e+99\n",
      "Gradient Descent(43/49): loss=2.1743609647805573e+102\n",
      "Gradient Descent(44/49): loss=6.513901349777526e+104\n",
      "Gradient Descent(45/49): loss=1.951419818600171e+107\n",
      "Gradient Descent(46/49): loss=5.846019311538369e+109\n",
      "Gradient Descent(47/49): loss=1.7513372296995138e+112\n",
      "Gradient Descent(48/49): loss=5.246616421669887e+114\n",
      "Gradient Descent(49/49): loss=1.571769469027872e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.213178947709996\n",
      "Gradient Descent(2/49): loss=411.76308402986405\n",
      "Gradient Descent(3/49): loss=32412.23873559324\n",
      "Gradient Descent(4/49): loss=3218171.6292482265\n",
      "Gradient Descent(5/49): loss=497205681.534893\n",
      "Gradient Descent(6/49): loss=116412603822.61938\n",
      "Gradient Descent(7/49): loss=32968420925415.254\n",
      "Gradient Descent(8/49): loss=9881515576168814.0\n",
      "Gradient Descent(9/49): loss=3.0047252148653194e+18\n",
      "Gradient Descent(10/49): loss=9.168661846359773e+20\n",
      "Gradient Descent(11/49): loss=2.8000926494185156e+23\n",
      "Gradient Descent(12/49): loss=8.553155794564723e+25\n",
      "Gradient Descent(13/49): loss=2.612770891592001e+28\n",
      "Gradient Descent(14/49): loss=7.98144078279392e+30\n",
      "Gradient Descent(15/49): loss=2.4381614750917362e+33\n",
      "Gradient Descent(16/49): loss=7.448072949295774e+35\n",
      "Gradient Descent(17/49): loss=2.275230743734398e+38\n",
      "Gradient Descent(18/49): loss=6.950355037109642e+40\n",
      "Gradient Descent(19/49): loss=2.1231884157053933e+43\n",
      "Gradient Descent(20/49): loss=6.48589753234329e+45\n",
      "Gradient Descent(21/49): loss=1.9813063461932002e+48\n",
      "Gradient Descent(22/49): loss=6.0524774225623175e+50\n",
      "Gradient Descent(23/49): loss=1.8489055477012519e+53\n",
      "Gradient Descent(24/49): loss=5.648020613181829e+55\n",
      "Gradient Descent(25/49): loss=1.7253524327752876e+58\n",
      "Gradient Descent(26/49): loss=5.270591630521376e+60\n",
      "Gradient Descent(27/49): loss=1.6100557548721656e+63\n",
      "Gradient Descent(28/49): loss=4.918384339976936e+65\n",
      "Gradient Descent(29/49): loss=1.5024637775759236e+68\n",
      "Gradient Descent(30/49): loss=4.589713301946378e+70\n",
      "Gradient Descent(31/49): loss=1.402061634261188e+73\n",
      "Gradient Descent(32/49): loss=4.283005706333619e+75\n",
      "Gradient Descent(33/49): loss=1.3083688642655902e+78\n",
      "Gradient Descent(34/49): loss=3.996793846079073e+80\n",
      "Gradient Descent(35/49): loss=1.220937113710871e+83\n",
      "Gradient Descent(36/49): loss=3.729708093648637e+85\n",
      "Gradient Descent(37/49): loss=1.139347989967181e+88\n",
      "Gradient Descent(38/49): loss=3.4804703468693413e+90\n",
      "Gradient Descent(39/49): loss=1.063211059492522e+93\n",
      "Gradient Descent(40/49): loss=3.247887912747195e+95\n",
      "Gradient Descent(41/49): loss=9.921619794666677e+97\n",
      "Gradient Descent(42/49): loss=3.030847799998593e+100\n",
      "Gradient Descent(43/49): loss=9.258607542786873e+102\n",
      "Gradient Descent(44/49): loss=2.8283113929834923e+105\n",
      "Gradient Descent(45/49): loss=8.639901085247115e+107\n",
      "Gradient Descent(46/49): loss=2.6393094815529375e+110\n",
      "Gradient Descent(47/49): loss=8.062539687300172e+112\n",
      "Gradient Descent(48/49): loss=2.4629376230271523e+115\n",
      "Gradient Descent(49/49): loss=7.523760465300505e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.265838052957769\n",
      "Gradient Descent(2/49): loss=419.788293151595\n",
      "Gradient Descent(3/49): loss=33014.89124050744\n",
      "Gradient Descent(4/49): loss=3165414.207353587\n",
      "Gradient Descent(5/49): loss=452316098.11169404\n",
      "Gradient Descent(6/49): loss=98447623522.42099\n",
      "Gradient Descent(7/49): loss=26618634533716.316\n",
      "Gradient Descent(8/49): loss=7721233840758447.0\n",
      "Gradient Descent(9/49): loss=2.2822801828930332e+18\n",
      "Gradient Descent(10/49): loss=6.778350214015202e+20\n",
      "Gradient Descent(11/49): loss=2.0155636078111533e+23\n",
      "Gradient Descent(12/49): loss=5.9951177208707e+25\n",
      "Gradient Descent(13/49): loss=1.783326666082714e+28\n",
      "Gradient Descent(14/49): loss=5.3048369147767e+30\n",
      "Gradient Descent(15/49): loss=1.5780295960860188e+33\n",
      "Gradient Descent(16/49): loss=4.694169222267433e+35\n",
      "Gradient Descent(17/49): loss=1.396376273407031e+38\n",
      "Gradient Descent(18/49): loss=4.1538060100771036e+40\n",
      "Gradient Descent(19/49): loss=1.2356343341370532e+43\n",
      "Gradient Descent(20/49): loss=3.6756464085185864e+45\n",
      "Gradient Descent(21/49): loss=1.0933960122045058e+48\n",
      "Gradient Descent(22/49): loss=3.2525295062420297e+50\n",
      "Gradient Descent(23/49): loss=9.675312578049823e+52\n",
      "Gradient Descent(24/49): loss=2.87811911636321e+55\n",
      "Gradient Descent(25/49): loss=8.56155248854866e+57\n",
      "Gradient Descent(26/49): loss=2.546808455477312e+60\n",
      "Gradient Descent(27/49): loss=7.5760013357017165e+62\n",
      "Gradient Descent(28/49): loss=2.2536361584287594e+65\n",
      "Gradient Descent(29/49): loss=6.70390052684306e+67\n",
      "Gradient Descent(30/49): loss=1.9942119807459192e+70\n",
      "Gradient Descent(31/49): loss=5.93219038413035e+72\n",
      "Gradient Descent(32/49): loss=1.7646510548194253e+75\n",
      "Gradient Descent(33/49): loss=5.249314576291784e+77\n",
      "Gradient Descent(34/49): loss=1.561515714147189e+80\n",
      "Gradient Descent(35/49): loss=4.645047062984584e+82\n",
      "Gradient Descent(36/49): loss=1.38176401440347e+85\n",
      "Gradient Descent(37/49): loss=4.110338960212145e+87\n",
      "Gradient Descent(38/49): loss=1.222704180433556e+90\n",
      "Gradient Descent(39/49): loss=3.637183033616639e+92\n",
      "Gradient Descent(40/49): loss=1.0819542970187166e+95\n",
      "Gradient Descent(41/49): loss=3.2184937904356052e+97\n",
      "Gradient Descent(42/49): loss=9.574066397828028e+99\n",
      "Gradient Descent(43/49): loss=2.8480013745067e+102\n",
      "Gradient Descent(44/49): loss=8.471961120963448e+104\n",
      "Gradient Descent(45/49): loss=2.5201576753995692e+107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(46/49): loss=7.496723153225586e+109\n",
      "Gradient Descent(47/49): loss=2.230053245664362e+112\n",
      "Gradient Descent(48/49): loss=6.633748341578435e+114\n",
      "Gradient Descent(49/49): loss=1.9733437820353177e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.39171642114684\n",
      "Gradient Descent(2/49): loss=426.7649518259969\n",
      "Gradient Descent(3/49): loss=33202.41428709606\n",
      "Gradient Descent(4/49): loss=3105324.134954608\n",
      "Gradient Descent(5/49): loss=427540847.6487179\n",
      "Gradient Descent(6/49): loss=90699523406.10425\n",
      "Gradient Descent(7/49): loss=24298893464834.684\n",
      "Gradient Descent(8/49): loss=7032036226231060.0\n",
      "Gradient Descent(9/49): loss=2.0777687671327012e+18\n",
      "Gradient Descent(10/49): loss=6.171570350290127e+20\n",
      "Gradient Descent(11/49): loss=1.8355332152617547e+23\n",
      "Gradient Descent(12/49): loss=5.460968071038416e+25\n",
      "Gradient Descent(13/49): loss=1.6248448604262304e+28\n",
      "Gradient Descent(14/49): loss=4.8346250593428307e+30\n",
      "Gradient Descent(15/49): loss=1.4385197671796888e+33\n",
      "Gradient Descent(16/49): loss=4.280252587308346e+35\n",
      "Gradient Descent(17/49): loss=1.2735708020968169e+38\n",
      "Gradient Descent(18/49): loss=3.789455589321941e+40\n",
      "Gradient Descent(19/49): loss=1.1275363650965172e+43\n",
      "Gradient Descent(20/49): loss=3.3549364294316435e+45\n",
      "Gradient Descent(21/49): loss=9.982470460999595e+47\n",
      "Gradient Descent(22/49): loss=2.9702415717474824e+50\n",
      "Gradient Descent(23/49): loss=8.837827298980906e+52\n",
      "Gradient Descent(24/49): loss=2.629657873989109e+55\n",
      "Gradient Descent(25/49): loss=7.824435011374053e+57\n",
      "Gradient Descent(26/49): loss=2.3281273146916284e+60\n",
      "Gradient Descent(27/49): loss=6.927243673869788e+62\n",
      "Gradient Descent(28/49): loss=2.0611718532037916e+65\n",
      "Gradient Descent(29/49): loss=6.132929067393901e+67\n",
      "Gradient Descent(30/49): loss=1.8248269248981077e+70\n",
      "Gradient Descent(31/49): loss=5.4296947987499226e+72\n",
      "Gradient Descent(32/49): loss=1.6155825632186516e+75\n",
      "Gradient Descent(33/49): loss=4.807097111935366e+77\n",
      "Gradient Descent(34/49): loss=1.43033127304491e+80\n",
      "Gradient Descent(35/49): loss=4.255889787561673e+82\n",
      "Gradient Descent(36/49): loss=1.2663218811760288e+85\n",
      "Gradient Descent(37/49): loss=3.7678868269376793e+87\n",
      "Gradient Descent(38/49): loss=1.1211186785642214e+90\n",
      "Gradient Descent(39/49): loss=3.335840881524409e+92\n",
      "Gradient Descent(40/49): loss=9.925652475169281e+94\n",
      "Gradient Descent(41/49): loss=2.9533356223158778e+97\n",
      "Gradient Descent(42/49): loss=8.787524366644974e+99\n",
      "Gradient Descent(43/49): loss=2.614690450719029e+102\n",
      "Gradient Descent(44/49): loss=7.779900080881968e+104\n",
      "Gradient Descent(45/49): loss=2.3148761357912367e+107\n",
      "Gradient Descent(46/49): loss=6.887815355397673e+109\n",
      "Gradient Descent(47/49): loss=2.0494401249609633e+112\n",
      "Gradient Descent(48/49): loss=6.098021809641796e+114\n",
      "Gradient Descent(49/49): loss=1.8144404190181002e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.156047042798724\n",
      "Gradient Descent(2/49): loss=405.2556445054179\n",
      "Gradient Descent(3/49): loss=30756.93449826973\n",
      "Gradient Descent(4/49): loss=2692360.1679215576\n",
      "Gradient Descent(5/49): loss=330814887.68528783\n",
      "Gradient Descent(6/49): loss=64479340379.81844\n",
      "Gradient Descent(7/49): loss=16859235254592.996\n",
      "Gradient Descent(8/49): loss=4896142218981138.0\n",
      "Gradient Descent(9/49): loss=1.4632893506343693e+18\n",
      "Gradient Descent(10/49): loss=4.404874765222681e+20\n",
      "Gradient Descent(11/49): loss=1.3283250597018848e+23\n",
      "Gradient Descent(12/49): loss=4.007397858007493e+25\n",
      "Gradient Descent(13/49): loss=1.2091110902405454e+28\n",
      "Gradient Descent(14/49): loss=3.648220411751908e+30\n",
      "Gradient Descent(15/49): loss=1.1007752151039603e+33\n",
      "Gradient Descent(16/49): loss=3.3213670747770865e+35\n",
      "Gradient Descent(17/49): loss=1.0021558605507124e+38\n",
      "Gradient Descent(18/49): loss=3.0238044660757697e+40\n",
      "Gradient Descent(19/49): loss=9.123724172939178e+42\n",
      "Gradient Descent(20/49): loss=2.7529009948672617e+45\n",
      "Gradient Descent(21/49): loss=8.306327277956173e+47\n",
      "Gradient Descent(22/49): loss=2.5062678606704864e+50\n",
      "Gradient Descent(23/49): loss=7.562161205212607e+52\n",
      "Gradient Descent(24/49): loss=2.2817306558525446e+55\n",
      "Gradient Descent(25/49): loss=6.884665169915516e+57\n",
      "Gradient Descent(26/49): loss=2.0773097990476905e+60\n",
      "Gradient Descent(27/49): loss=6.267866184803362e+62\n",
      "Gradient Descent(28/49): loss=1.8912030612196235e+65\n",
      "Gradient Descent(29/49): loss=5.706326384947687e+67\n",
      "Gradient Descent(30/49): loss=1.721769675571052e+70\n",
      "Gradient Descent(31/49): loss=5.195095085229989e+72\n",
      "Gradient Descent(32/49): loss=1.5675158720419433e+75\n",
      "Gradient Descent(33/49): loss=4.729665133731866e+77\n",
      "Gradient Descent(34/49): loss=1.4270817078297505e+80\n",
      "Gradient Descent(35/49): loss=4.305933175474778e+82\n",
      "Gradient Descent(36/49): loss=1.2992290777695567e+85\n",
      "Gradient Descent(37/49): loss=3.9201634761454713e+87\n",
      "Gradient Descent(38/49): loss=1.1828307988678642e+90\n",
      "Gradient Descent(39/49): loss=3.56895498686207e+92\n",
      "Gradient Descent(40/49): loss=1.0768606727554627e+95\n",
      "Gradient Descent(41/49): loss=3.249211359616862e+97\n",
      "Gradient Descent(42/49): loss=9.803844384482077e+99\n",
      "Gradient Descent(43/49): loss=2.958113649044845e+102\n",
      "Gradient Descent(44/49): loss=8.925515356522757e+104\n",
      "Gradient Descent(45/49): loss=2.6930954598464328e+107\n",
      "Gradient Descent(46/49): loss=8.125876059968942e+109\n",
      "Gradient Descent(47/49): loss=2.451820320759858e+112\n",
      "Gradient Descent(48/49): loss=7.397876660838599e+114\n",
      "Gradient Descent(49/49): loss=2.232161085605952e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.257786419836034\n",
      "Gradient Descent(2/49): loss=418.01162628913846\n",
      "Gradient Descent(3/49): loss=33148.87139477297\n",
      "Gradient Descent(4/49): loss=3314756.367656014\n",
      "Gradient Descent(5/49): loss=515571021.164167\n",
      "Gradient Descent(6/49): loss=121529383835.62544\n",
      "Gradient Descent(7/49): loss=34658196623133.83\n",
      "Gradient Descent(8/49): loss=1.046179760629944e+16\n",
      "Gradient Descent(9/49): loss=3.203894002624795e+18\n",
      "Gradient Descent(10/49): loss=9.846331271790039e+20\n",
      "Gradient Descent(11/49): loss=3.028567294002139e+23\n",
      "Gradient Descent(12/49): loss=9.317252784312763e+25\n",
      "Gradient Descent(13/49): loss=2.866550396337985e+28\n",
      "Gradient Descent(14/49): loss=8.819344919569898e+30\n",
      "Gradient Descent(15/49): loss=2.7134028863992896e+33\n",
      "Gradient Descent(16/49): loss=8.34819388253655e+35\n",
      "Gradient Descent(17/49): loss=2.5684483707092626e+38\n",
      "Gradient Descent(18/49): loss=7.902220981102775e+40\n",
      "Gradient Descent(19/49): loss=2.431238163803172e+43\n",
      "Gradient Descent(20/49): loss=7.4800730531842e+45\n",
      "Gradient Descent(21/49): loss=2.30135795593403e+48\n",
      "Gradient Descent(22/49): loss=7.080476894210618e+50\n",
      "Gradient Descent(23/49): loss=2.178416135642602e+53\n",
      "Gradient Descent(24/49): loss=6.702227732630022e+55\n",
      "Gradient Descent(25/49): loss=2.0620420426174479e+58\n",
      "Gradient Descent(26/49): loss=6.3441851801342325e+60\n",
      "Gradient Descent(27/49): loss=1.9518848194166558e+63\n",
      "Gradient Descent(28/49): loss=6.005269770811892e+65\n",
      "Gradient Descent(29/49): loss=1.8476123519935113e+68\n",
      "Gradient Descent(30/49): loss=5.684459705425333e+70\n",
      "Gradient Descent(31/49): loss=1.748910268311441e+73\n",
      "Gradient Descent(32/49): loss=5.380787770710986e+75\n",
      "Gradient Descent(33/49): loss=1.6554809905362788e+78\n",
      "Gradient Descent(34/49): loss=5.093338423315776e+80\n",
      "Gradient Descent(35/49): loss=1.567042837865533e+83\n",
      "Gradient Descent(36/49): loss=4.8212450295166324e+85\n",
      "Gradient Descent(37/49): loss=1.483329177286546e+88\n",
      "Gradient Descent(38/49): loss=4.563687252398643e+90\n",
      "Gradient Descent(39/49): loss=1.4040876197018556e+93\n",
      "Gradient Descent(40/49): loss=4.3198885786134396e+95\n",
      "Gradient Descent(41/49): loss=1.3290792589993592e+98\n",
      "Gradient Descent(42/49): loss=4.0891139772619835e+100\n",
      "Gradient Descent(43/49): loss=1.2580779517715187e+103\n",
      "Gradient Descent(44/49): loss=3.8706676838423577e+105\n",
      "Gradient Descent(45/49): loss=1.190869635513854e+108\n",
      "Gradient Descent(46/49): loss=3.6638911025838245e+110\n",
      "Gradient Descent(47/49): loss=1.127251683245807e+113\n",
      "Gradient Descent(48/49): loss=3.468160820839925e+115\n",
      "Gradient Descent(49/49): loss=1.0670322926087955e+118\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.310831807161079\n",
      "Gradient Descent(2/49): loss=426.1556404111254\n",
      "Gradient Descent(3/49): loss=33765.440981667605\n",
      "Gradient Descent(4/49): loss=3260616.5343205063\n",
      "Gradient Descent(5/49): loss=469074807.46361524\n",
      "Gradient Descent(6/49): loss=102783535593.73782\n",
      "Gradient Descent(7/49): loss=27984563519388.9\n",
      "Gradient Descent(8/49): loss=8175050277956362.0\n",
      "Gradient Descent(9/49): loss=2.433679735649846e+18\n",
      "Gradient Descent(10/49): loss=7.279727938797707e+20\n",
      "Gradient Descent(11/49): loss=2.18014825239594e+23\n",
      "Gradient Descent(12/49): loss=6.531096193606988e+25\n",
      "Gradient Descent(13/49): loss=1.956672966857139e+28\n",
      "Gradient Descent(14/49): loss=5.862169749918334e+30\n",
      "Gradient Descent(15/49): loss=1.7563073490553482e+33\n",
      "Gradient Descent(16/49): loss=5.2619066602594434e+35\n",
      "Gradient Descent(17/49): loss=1.5764706288491815e+38\n",
      "Gradient Descent(18/49): loss=4.723116501825227e+40\n",
      "Gradient Descent(19/49): loss=1.4150488737398645e+43\n",
      "Gradient Descent(20/49): loss=4.23949594079669e+45\n",
      "Gradient Descent(21/49): loss=1.2701558359510013e+48\n",
      "Gradient Descent(22/49): loss=3.805395430402145e+50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/49): loss=1.1400990313795194e+53\n",
      "Gradient Descent(24/49): loss=3.4157443691380173e+55\n",
      "Gradient Descent(25/49): loss=1.0233593112721632e+58\n",
      "Gradient Descent(26/49): loss=3.065991382232073e+60\n",
      "Gradient Descent(27/49): loss=9.185730810655288e+62\n",
      "Gradient Descent(28/49): loss=2.752051131481031e+65\n",
      "Gradient Descent(29/49): loss=8.24516370706237e+67\n",
      "Gradient Descent(30/49): loss=2.4702565943851218e+70\n",
      "Gradient Descent(31/49): loss=7.400905377871748e+72\n",
      "Gradient Descent(32/49): loss=2.217316230901276e+75\n",
      "Gradient Descent(33/49): loss=6.643094346967606e+77\n",
      "Gradient Descent(34/49): loss=1.9902755361501327e+80\n",
      "Gradient Descent(35/49): loss=5.9628788978526266e+82\n",
      "Gradient Descent(36/49): loss=1.78648252991306e+85\n",
      "Gradient Descent(37/49): loss=5.352313679947308e+87\n",
      "Gradient Descent(38/49): loss=1.6035567798105132e+90\n",
      "Gradient Descent(39/49): loss=4.804266901826301e+92\n",
      "Gradient Descent(40/49): loss=1.439361596332794e+95\n",
      "Gradient Descent(41/49): loss=4.3123370273415374e+97\n",
      "Gradient Descent(42/49): loss=1.291979074942704e+100\n",
      "Gradient Descent(43/49): loss=3.8707780015951563e+102\n",
      "Gradient Descent(44/49): loss=1.159687693726587e+105\n",
      "Gradient Descent(45/49): loss=3.474432133350598e+107\n",
      "Gradient Descent(46/49): loss=1.0409422049196157e+110\n",
      "Gradient Descent(47/49): loss=3.11866984990721e+112\n",
      "Gradient Descent(48/49): loss=9.343555854257301e+114\n",
      "Gradient Descent(49/49): loss=2.7993356207366273e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.437678680556534\n",
      "Gradient Descent(2/49): loss=433.24019410392805\n",
      "Gradient Descent(3/49): loss=33957.728883065036\n",
      "Gradient Descent(4/49): loss=3198844.5532756997\n",
      "Gradient Descent(5/49): loss=443403914.87119895\n",
      "Gradient Descent(6/49): loss=94696767905.74988\n",
      "Gradient Descent(7/49): loss=25546016484689.625\n",
      "Gradient Descent(8/49): loss=7445362212385567.0\n",
      "Gradient Descent(9/49): loss=2.2156023034200182e+18\n",
      "Gradient Descent(10/49): loss=6.62806410466653e+20\n",
      "Gradient Descent(11/49): loss=1.9854161621467005e+23\n",
      "Gradient Descent(12/49): loss=5.949188784803574e+25\n",
      "Gradient Descent(13/49): loss=1.7827849728797247e+28\n",
      "Gradient Descent(14/49): loss=5.342553153885546e+30\n",
      "Gradient Descent(15/49): loss=1.6010352230868986e+33\n",
      "Gradient Descent(16/49): loss=4.797924983414006e+35\n",
      "Gradient Descent(17/49): loss=1.437825401804141e+38\n",
      "Gradient Descent(18/49): loss=4.3088252703543385e+40\n",
      "Gradient Descent(19/49): loss=1.2912538290507864e+43\n",
      "Gradient Descent(20/49): loss=3.869584757108595e+45\n",
      "Gradient Descent(21/49): loss=1.1596237604608542e+48\n",
      "Gradient Descent(22/49): loss=3.4751203310254268e+50\n",
      "Gradient Descent(23/49): loss=1.0414120275658783e+53\n",
      "Gradient Descent(24/49): loss=3.1208675034791755e+55\n",
      "Gradient Descent(25/49): loss=9.352507668918712e+57\n",
      "Gradient Descent(26/49): loss=2.8027271135273937e+60\n",
      "Gradient Descent(27/49): loss=8.399115564491178e+62\n",
      "Gradient Descent(28/49): loss=2.517017868960364e+65\n",
      "Gradient Descent(29/49): loss=7.542911993555259e+67\n",
      "Gradient Descent(30/49): loss=2.2604337475768702e+70\n",
      "Gradient Descent(31/49): loss=6.773989583267179e+72\n",
      "Gradient Descent(32/49): loss=2.0300057421900733e+75\n",
      "Gradient Descent(33/49): loss=6.083450915696803e+77\n",
      "Gradient Descent(34/49): loss=1.8230675054034532e+80\n",
      "Gradient Descent(35/49): loss=5.463305573292929e+82\n",
      "Gradient Descent(36/49): loss=1.63722449655355e+85\n",
      "Gradient Descent(37/49): loss=4.90637767951069e+87\n",
      "Gradient Descent(38/49): loss=1.4703262738051192e+90\n",
      "Gradient Descent(39/49): loss=4.406222864721006e+92\n",
      "Gradient Descent(40/49): loss=1.3204416107824379e+95\n",
      "Gradient Descent(41/49): loss=3.957053696593124e+97\n",
      "Gradient Descent(42/49): loss=1.1858361498046773e+100\n",
      "Gradient Descent(43/49): loss=3.553672712084407e+102\n",
      "Gradient Descent(44/49): loss=1.0649523331442792e+105\n",
      "Gradient Descent(45/49): loss=3.191412276130016e+107\n",
      "Gradient Descent(46/49): loss=9.563913800875857e+109\n",
      "Gradient Descent(47/49): loss=2.8660805711226334e+112\n",
      "Gradient Descent(48/49): loss=8.588971012488987e+114\n",
      "Gradient Descent(49/49): loss=2.573913092208668e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.200221089394259\n",
      "Gradient Descent(2/49): loss=411.40824448713084\n",
      "Gradient Descent(3/49): loss=31457.722648832074\n",
      "Gradient Descent(4/49): loss=2773750.4778856323\n",
      "Gradient Descent(5/49): loss=343145943.2959953\n",
      "Gradient Descent(6/49): loss=67327201090.5969\n",
      "Gradient Descent(7/49): loss=17724854073823.438\n",
      "Gradient Descent(8/49): loss=5183868578035073.0\n",
      "Gradient Descent(9/49): loss=1.5603263175065016e+18\n",
      "Gradient Descent(10/49): loss=4.730566036104716e+20\n",
      "Gradient Descent(11/49): loss=1.4367485892382785e+23\n",
      "Gradient Descent(12/49): loss=4.365524624874469e+25\n",
      "Gradient Descent(13/49): loss=1.3265937313583678e+28\n",
      "Gradient Descent(14/49): loss=4.0313509824872366e+30\n",
      "Gradient Descent(15/49): loss=1.2250843831385473e+33\n",
      "Gradient Descent(16/49): loss=3.722905907741912e+35\n",
      "Gradient Descent(17/49): loss=1.1313533785516045e+38\n",
      "Gradient Descent(18/49): loss=3.438068581452426e+40\n",
      "Gradient Descent(19/49): loss=1.0447943194649477e+43\n",
      "Gradient Descent(20/49): loss=3.175024435982859e+45\n",
      "Gradient Descent(21/49): loss=9.6485786671314e+47\n",
      "Gradient Descent(22/49): loss=2.9321056326548706e+50\n",
      "Gradient Descent(23/49): loss=8.910372955764285e+52\n",
      "Gradient Descent(24/49): loss=2.7077723710941656e+55\n",
      "Gradient Descent(25/49): loss=8.228646825553945e+57\n",
      "Gradient Descent(26/49): loss=2.5006026836891463e+60\n",
      "Gradient Descent(27/49): loss=7.599079063955636e+62\n",
      "Gradient Descent(28/49): loss=2.3092833978350905e+65\n",
      "Gradient Descent(29/49): loss=7.017679072206952e+67\n",
      "Gradient Descent(30/49): loss=2.1326018108761095e+70\n",
      "Gradient Descent(31/49): loss=6.480761569396949e+72\n",
      "Gradient Descent(32/49): loss=1.9694380031553388e+75\n",
      "Gradient Descent(33/49): loss=5.984923232769653e+77\n",
      "Gradient Descent(34/49): loss=1.8187577392514196e+80\n",
      "Gradient Descent(35/49): loss=5.527021125308751e+82\n",
      "Gradient Descent(36/49): loss=1.679605912340092e+85\n",
      "Gradient Descent(37/49): loss=5.104152773814812e+87\n",
      "Gradient Descent(38/49): loss=1.5511004901229274e+90\n",
      "Gradient Descent(39/49): loss=4.713637771193544e+92\n",
      "Gradient Descent(40/49): loss=1.4324269239487933e+95\n",
      "Gradient Descent(41/49): loss=4.353000786341449e+97\n",
      "Gradient Descent(42/49): loss=1.3228329856893269e+100\n",
      "Gradient Descent(43/49): loss=4.019955873930444e+102\n",
      "Gradient Descent(44/49): loss=1.22162399964097e+105\n",
      "Gradient Descent(45/49): loss=3.712391984640521e+107\n",
      "Gradient Descent(46/49): loss=1.1281584392311933e+110\n",
      "Gradient Descent(47/49): loss=3.4283595839941163e+112\n",
      "Gradient Descent(48/49): loss=1.041843860617158e+115\n",
      "Gradient Descent(49/49): loss=3.1660582949735186e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.302564304028716\n",
      "Gradient Descent(2/49): loss=424.33110137347165\n",
      "Gradient Descent(3/49): loss=33899.40778748662\n",
      "Gradient Descent(4/49): loss=3413869.8195624216\n",
      "Gradient Descent(5/49): loss=534545058.97792035\n",
      "Gradient Descent(6/49): loss=126851632869.2558\n",
      "Gradient Descent(7/49): loss=36428108793784.76\n",
      "Gradient Descent(8/49): loss=1.1073910752331062e+16\n",
      "Gradient Descent(9/49): loss=3.415485869107662e+18\n",
      "Gradient Descent(10/49): loss=1.0571409854566211e+21\n",
      "Gradient Descent(11/49): loss=3.2747717431485736e+23\n",
      "Gradient Descent(12/49): loss=1.0146525530567943e+26\n",
      "Gradient Descent(13/49): loss=3.1439440634392174e+28\n",
      "Gradient Descent(14/49): loss=9.741757958829591e+30\n",
      "Gradient Descent(15/49): loss=3.018568763753328e+33\n",
      "Gradient Descent(16/49): loss=9.353305147645727e+35\n",
      "Gradient Descent(17/49): loss=2.8982056755599237e+38\n",
      "Gradient Descent(18/49): loss=8.98035114696233e+40\n",
      "Gradient Descent(19/49): loss=2.7826426587493514e+43\n",
      "Gradient Descent(20/49): loss=8.622268836361061e+45\n",
      "Gradient Descent(21/49): loss=2.6716876380838618e+48\n",
      "Gradient Descent(22/49): loss=8.278464719511749e+50\n",
      "Gradient Descent(23/49): loss=2.5651568371007926e+53\n",
      "Gradient Descent(24/49): loss=7.948369440367915e+55\n",
      "Gradient Descent(25/49): loss=2.4628738425245272e+58\n",
      "Gradient Descent(26/49): loss=7.631436371575129e+60\n",
      "Gradient Descent(27/49): loss=2.3646692773231677e+63\n",
      "Gradient Descent(28/49): loss=7.32714068342823e+65\n",
      "Gradient Descent(29/49): loss=2.2703805183075204e+68\n",
      "Gradient Descent(30/49): loss=7.03497847334728e+70\n",
      "Gradient Descent(31/49): loss=2.179851426726986e+73\n",
      "Gradient Descent(32/49): loss=6.754465931354611e+75\n",
      "Gradient Descent(33/49): loss=2.0929320897036142e+78\n",
      "Gradient Descent(34/49): loss=6.485138538899264e+80\n",
      "Gradient Descent(35/49): loss=2.0094785719814542e+83\n",
      "Gradient Descent(36/49): loss=6.226550299630141e+85\n",
      "Gradient Descent(37/49): loss=1.92935267757515e+88\n",
      "Gradient Descent(38/49): loss=5.978273000842063e+90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(39/49): loss=1.8524217209223083e+93\n",
      "Gradient Descent(40/49): loss=5.739895504372986e+95\n",
      "Gradient Descent(41/49): loss=1.7785583071611382e+98\n",
      "Gradient Descent(42/49): loss=5.511023065771633e+100\n",
      "Gradient Descent(43/49): loss=1.7076401211689744e+103\n",
      "Gradient Descent(44/49): loss=5.291276680616961e+105\n",
      "Gradient Descent(45/49): loss=1.639549725013229e+108\n",
      "Gradient Descent(46/49): loss=5.0802924568999536e+110\n",
      "Gradient Descent(47/49): loss=1.5741743634781262e+113\n",
      "Gradient Descent(48/49): loss=4.877721012431415e+115\n",
      "Gradient Descent(49/49): loss=1.5114057773464432e+118\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.3559973766613025\n",
      "Gradient Descent(2/49): loss=432.59523312646024\n",
      "Gradient Descent(3/49): loss=34530.160580034266\n",
      "Gradient Descent(4/49): loss=3358316.4431330534\n",
      "Gradient Descent(5/49): loss=486390678.4364326\n",
      "Gradient Descent(6/49): loss=107293919781.95787\n",
      "Gradient Descent(7/49): loss=29415348184293.844\n",
      "Gradient Descent(8/49): loss=8653782375876158.0\n",
      "Gradient Descent(9/49): loss=2.5945301465912607e+18\n",
      "Gradient Descent(10/49): loss=7.816208119452792e+20\n",
      "Gradient Descent(11/49): loss=2.3575143820481886e+23\n",
      "Gradient Descent(12/49): loss=7.1128267928436696e+25\n",
      "Gradient Descent(13/49): loss=2.1461612860088663e+28\n",
      "Gradient Descent(14/49): loss=6.475756286431679e+30\n",
      "Gradient Descent(15/49): loss=1.953982311929699e+33\n",
      "Gradient Descent(16/49): loss=5.895915568997095e+35\n",
      "Gradient Descent(17/49): loss=1.7790248173454292e+38\n",
      "Gradient Descent(18/49): loss=5.368003470963481e+40\n",
      "Gradient Descent(19/49): loss=1.6197335466284564e+43\n",
      "Gradient Descent(20/49): loss=4.887360426829073e+45\n",
      "Gradient Descent(21/49): loss=1.4747050227695877e+48\n",
      "Gradient Descent(22/49): loss=4.449753475985427e+50\n",
      "Gradient Descent(23/49): loss=1.3426621387091907e+53\n",
      "Gradient Descent(24/49): loss=4.051329199431513e+55\n",
      "Gradient Descent(25/49): loss=1.2224421773011004e+58\n",
      "Gradient Descent(26/49): loss=3.6885792372929836e+60\n",
      "Gradient Descent(27/49): loss=1.1129865315863519e+63\n",
      "Gradient Descent(28/49): loss=3.358309364669723e+65\n",
      "Gradient Descent(29/49): loss=1.013331380816732e+68\n",
      "Gradient Descent(30/49): loss=3.057611362879746e+70\n",
      "Gradient Descent(31/49): loss=9.225992033204707e+72\n",
      "Gradient Descent(32/49): loss=2.7838374108011774e+75\n",
      "Gradient Descent(33/49): loss=8.399910494052732e+77\n",
      "Gradient Descent(34/49): loss=2.534576769258655e+80\n",
      "Gradient Descent(35/49): loss=7.647795061404269e+82\n",
      "Gradient Descent(36/49): loss=2.3076345530598356e+85\n",
      "Gradient Descent(37/49): loss=6.963022920619173e+87\n",
      "Gradient Descent(38/49): loss=2.1010124037525883e+90\n",
      "Gradient Descent(39/49): loss=6.339564253983183e+92\n",
      "Gradient Descent(40/49): loss=1.912890892914246e+95\n",
      "Gradient Descent(41/49): loss=5.771929144649271e+97\n",
      "Gradient Descent(42/49): loss=1.7416135010239141e+100\n",
      "Gradient Descent(43/49): loss=5.255119234720078e+102\n",
      "Gradient Descent(44/49): loss=1.5856720308431914e+105\n",
      "Gradient Descent(45/49): loss=4.784583711795289e+107\n",
      "Gradient Descent(46/49): loss=1.4436933268604895e+110\n",
      "Gradient Descent(47/49): loss=4.356179236415721e+112\n",
      "Gradient Descent(48/49): loss=1.3144271838567195e+115\n",
      "Gradient Descent(49/49): loss=3.966133457545803e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.483816413447981\n",
      "Gradient Descent(2/49): loss=439.78892755880094\n",
      "Gradient Descent(3/49): loss=34727.31244217189\n",
      "Gradient Descent(4/49): loss=3294821.6513108625\n",
      "Gradient Descent(5/49): loss=459795118.5209186\n",
      "Gradient Descent(6/49): loss=98854964961.7891\n",
      "Gradient Descent(7/49): loss=26852365701512.246\n",
      "Gradient Descent(8/49): loss=7881381837392286.0\n",
      "Gradient Descent(9/49): loss=2.362039908977943e+18\n",
      "Gradient Descent(10/49): loss=7.11651776635751e+20\n",
      "Gradient Descent(11/49): loss=2.146938799303869e+23\n",
      "Gradient Descent(12/49): loss=6.479084640419098e+25\n",
      "Gradient Descent(13/49): loss=1.9554323728241037e+28\n",
      "Gradient Descent(14/49): loss=5.901747972083687e+30\n",
      "Gradient Descent(15/49): loss=1.7812327701557873e+33\n",
      "Gradient Descent(16/49): loss=5.376024362605014e+35\n",
      "Gradient Descent(17/49): loss=1.6225642851349922e+38\n",
      "Gradient Descent(18/49): loss=4.897141234475647e+40\n",
      "Gradient Descent(19/49): loss=1.4780303584365879e+43\n",
      "Gradient Descent(20/49): loss=4.460916371402501e+45\n",
      "Gradient Descent(21/49): loss=1.3463711879753247e+48\n",
      "Gradient Descent(22/49): loss=4.063549336963499e+50\n",
      "Gradient Descent(23/49): loss=1.2264398824467768e+53\n",
      "Gradient Descent(24/49): loss=3.701578744473387e+55\n",
      "Gradient Descent(25/49): loss=1.1171917513201946e+58\n",
      "Gradient Descent(26/49): loss=3.371851567612089e+60\n",
      "Gradient Descent(27/49): loss=1.0176751645879835e+63\n",
      "Gradient Descent(28/49): loss=3.0714956452032693e+65\n",
      "Gradient Descent(29/49): loss=9.270232611328759e+67\n",
      "Gradient Descent(30/49): loss=2.797894660939862e+70\n",
      "Gradient Descent(31/49): loss=8.444463976178347e+72\n",
      "Gradient Descent(32/49): loss=2.548665353291805e+75\n",
      "Gradient Descent(33/49): loss=7.692252701170957e+77\n",
      "Gradient Descent(34/49): loss=2.3216367555767255e+80\n",
      "Gradient Descent(35/49): loss=7.007046484606921e+82\n",
      "Gradient Descent(36/49): loss=2.1148312852777456e+85\n",
      "Gradient Descent(37/49): loss=6.3828766870816334e+87\n",
      "Gradient Descent(38/49): loss=1.9264475178756292e+90\n",
      "Gradient Descent(39/49): loss=5.814306340337393e+92\n",
      "Gradient Descent(40/49): loss=1.7548444951444658e+95\n",
      "Gradient Descent(41/49): loss=5.29638278735788e+97\n",
      "Gradient Descent(42/49): loss=1.5985274312247029e+100\n",
      "Gradient Descent(43/49): loss=4.824594541159556e+102\n",
      "Gradient Descent(44/49): loss=1.456134691961703e+105\n",
      "Gradient Descent(45/49): loss=4.394831986492087e+107\n",
      "Gradient Descent(46/49): loss=1.3264259340922712e+110\n",
      "Gradient Descent(47/49): loss=4.003351582131575e+112\n",
      "Gradient Descent(48/49): loss=1.2082713009621088e+115\n",
      "Gradient Descent(49/49): loss=3.6467432519412015e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.244563929118136\n",
      "Gradient Descent(2/49): loss=417.6307188953219\n",
      "Gradient Descent(3/49): loss=32171.76975178872\n",
      "Gradient Descent(4/49): loss=2857286.6821620045\n",
      "Gradient Descent(5/49): loss=355889455.00184053\n",
      "Gradient Descent(6/49): loss=70289993260.29874\n",
      "Gradient Descent(7/49): loss=18631598699821.41\n",
      "Gradient Descent(8/49): loss=5487389594425604.0\n",
      "Gradient Descent(9/49): loss=1.6634186064039644e+18\n",
      "Gradient Descent(10/49): loss=5.079050809717456e+20\n",
      "Gradient Descent(11/49): loss=1.5535888339030805e+23\n",
      "Gradient Descent(12/49): loss=4.754209472057188e+25\n",
      "Gradient Descent(13/49): loss=1.4550119426607703e+28\n",
      "Gradient Descent(14/49): loss=4.4531366775517213e+30\n",
      "Gradient Descent(15/49): loss=1.362913294200554e+33\n",
      "Gradient Descent(16/49): loss=4.1712968522754224e+35\n",
      "Gradient Descent(17/49): loss=1.2766567043939674e+38\n",
      "Gradient Descent(18/49): loss=3.907304004955659e+40\n",
      "Gradient Descent(19/49): loss=1.1958598481816546e+43\n",
      "Gradient Descent(20/49): loss=3.660019242014543e+45\n",
      "Gradient Descent(21/49): loss=1.1201764897457176e+48\n",
      "Gradient Descent(22/49): loss=3.428384622079687e+50\n",
      "Gradient Descent(23/49): loss=1.049282967945958e+53\n",
      "Gradient Descent(24/49): loss=3.2114096526739873e+55\n",
      "Gradient Descent(25/49): loss=9.828761423169366e+57\n",
      "Gradient Descent(26/49): loss=3.0081665549350036e+60\n",
      "Gradient Descent(27/49): loss=9.206720595436034e+62\n",
      "Gradient Descent(28/49): loss=2.817786268628436e+65\n",
      "Gradient Descent(29/49): loss=8.624047372097996e+67\n",
      "Gradient Descent(30/49): loss=2.639454734527865e+70\n",
      "Gradient Descent(31/49): loss=8.078250263515042e+72\n",
      "Gradient Descent(32/49): loss=2.4724094134409904e+75\n",
      "Gradient Descent(33/49): loss=7.566995460984652e+77\n",
      "Gradient Descent(34/49): loss=2.3159360256144486e+80\n",
      "Gradient Descent(35/49): loss=7.08809685745577e+82\n",
      "Gradient Descent(36/49): loss=2.169365496499189e+85\n",
      "Gradient Descent(37/49): loss=6.639506699814487e+87\n",
      "Gradient Descent(38/49): loss=2.0320710958121227e+90\n",
      "Gradient Descent(39/49): loss=6.219306832766031e+92\n",
      "Gradient Descent(40/49): loss=1.9034657576599994e+95\n",
      "Gradient Descent(41/49): loss=5.825700496871451e+97\n",
      "Gradient Descent(42/49): loss=1.7829995702665497e+100\n",
      "Gradient Descent(43/49): loss=5.457004645669579e+102\n",
      "Gradient Descent(44/49): loss=1.6701574245700542e+105\n",
      "Gradient Descent(45/49): loss=5.1116427490308585e+107\n",
      "Gradient Descent(46/49): loss=1.5644568116353706e+110\n",
      "Gradient Descent(47/49): loss=4.78813805197231e+112\n",
      "Gradient Descent(48/49): loss=1.4654457594632672e+115\n",
      "Gradient Descent(49/49): loss=4.485107260105637e+117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.34751260028805\n",
      "Gradient Descent(2/49): loss=430.722044256282\n",
      "Gradient Descent(3/49): loss=34664.057141079895\n",
      "Gradient Descent(4/49): loss=3515568.560085646\n",
      "Gradient Descent(5/49): loss=554145674.6103758\n",
      "Gradient Descent(6/49): loss=132386826670.6886\n",
      "Gradient Descent(7/49): loss=38281653115318.19\n",
      "Gradient Descent(8/49): loss=1.1719478817502522e+16\n",
      "Gradient Descent(9/49): loss=3.640227470747128e+18\n",
      "Gradient Descent(10/49): loss=1.1347028284838958e+21\n",
      "Gradient Descent(11/49): loss=3.540011536652425e+23\n",
      "Gradient Descent(12/49): loss=1.104627217110285e+26\n",
      "Gradient Descent(13/49): loss=3.4470535398714902e+28\n",
      "Gradient Descent(14/49): loss=1.0756857304047333e+31\n",
      "Gradient Descent(15/49): loss=3.356788712581085e+33\n",
      "Gradient Descent(16/49): loss=1.0475213839910352e+36\n",
      "Gradient Descent(17/49): loss=3.268901682725052e+38\n",
      "Gradient Descent(18/49): loss=1.0200955119690246e+41\n",
      "Gradient Descent(19/49): loss=3.1833164903572604e+43\n",
      "Gradient Descent(20/49): loss=9.93387754522028e+45\n",
      "Gradient Descent(21/49): loss=3.0999721025226e+48\n",
      "Gradient Descent(22/49): loss=9.673792528549157e+50\n",
      "Gradient Descent(23/49): loss=3.0188098083892576e+53\n",
      "Gradient Descent(24/49): loss=9.420516961056618e+55\n",
      "Gradient Descent(25/49): loss=2.939772474804041e+58\n",
      "Gradient Descent(26/49): loss=9.173872558528787e+60\n",
      "Gradient Descent(27/49): loss=2.8628044667213946e+63\n",
      "Gradient Descent(28/49): loss=8.933685706218726e+65\n",
      "Gradient Descent(29/49): loss=2.7878516058381604e+68\n",
      "Gradient Descent(30/49): loss=8.699787334989814e+70\n",
      "Gradient Descent(31/49): loss=2.714861132334052e+73\n",
      "Gradient Descent(32/49): loss=8.472012802215073e+75\n",
      "Gradient Descent(33/49): loss=2.6437816677270144e+78\n",
      "Gradient Descent(34/49): loss=8.250201775878043e+80\n",
      "Gradient Descent(35/49): loss=2.5745631787068546e+83\n",
      "Gradient Descent(36/49): loss=8.034198121715264e+85\n",
      "Gradient Descent(37/49): loss=2.507156941916367e+88\n",
      "Gradient Descent(38/49): loss=7.823849793310125e+90\n",
      "Gradient Descent(39/49): loss=2.441515509655002e+93\n",
      "Gradient Descent(40/49): loss=7.619008725068961e+95\n",
      "Gradient Descent(41/49): loss=2.3775926764798443e+98\n",
      "Gradient Descent(42/49): loss=7.419530727994327e+100\n",
      "Gradient Descent(43/49): loss=2.3153434466813607e+103\n",
      "Gradient Descent(44/49): loss=7.22527538819083e+105\n",
      "Gradient Descent(45/49): loss=2.2547240026105567e+108\n",
      "Gradient Descent(46/49): loss=7.036105968026205e+110\n",
      "Gradient Descent(47/49): loss=2.1956916738356863e+113\n",
      "Gradient Descent(48/49): loss=6.851889309881567e+115\n",
      "Gradient Descent(49/49): loss=2.1382049071058477e+118\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.401334761458445\n",
      "Gradient Descent(2/49): loss=439.10761589401574\n",
      "Gradient Descent(3/49): loss=35309.26330914429\n",
      "Gradient Descent(4/49): loss=3458569.921135589\n",
      "Gradient Descent(5/49): loss=504280127.3425674\n",
      "Gradient Descent(6/49): loss=111985137416.03117\n",
      "Gradient Descent(7/49): loss=30913821065027.918\n",
      "Gradient Descent(8/49): loss=9158702274797042.0\n",
      "Gradient Descent(9/49): loss=2.7653846504700974e+18\n",
      "Gradient Descent(10/49): loss=8.390110513122336e+20\n",
      "Gradient Descent(11/49): loss=2.5486038699409622e+23\n",
      "Gradient Descent(12/49): loss=7.744031495071457e+25\n",
      "Gradient Descent(13/49): loss=2.353229332997528e+28\n",
      "Gradient Descent(14/49): loss=7.151043948757318e+30\n",
      "Gradient Descent(15/49): loss=2.1730846468037426e+33\n",
      "Gradient Descent(16/49): loss=6.603654419088741e+35\n",
      "Gradient Descent(17/49): loss=2.0067448448338755e+38\n",
      "Gradient Descent(18/49): loss=6.098176709945819e+40\n",
      "Gradient Descent(19/49): loss=1.853138426515513e+43\n",
      "Gradient Descent(20/49): loss=5.6313914900068954e+45\n",
      "Gradient Descent(21/49): loss=1.7112898688283507e+48\n",
      "Gradient Descent(22/49): loss=5.200336402017223e+50\n",
      "Gradient Descent(23/49): loss=1.5802991175590062e+53\n",
      "Gradient Descent(24/49): loss=4.8022764450986373e+55\n",
      "Gradient Descent(25/49): loss=1.459335058724057e+58\n",
      "Gradient Descent(26/49): loss=4.434686003545232e+60\n",
      "Gradient Descent(27/49): loss=1.347630198594701e+63\n",
      "Gradient Descent(28/49): loss=4.0952327869721924e+65\n",
      "Gradient Descent(29/49): loss=1.244475791428594e+68\n",
      "Gradient Descent(30/49): loss=3.7817630303669845e+70\n",
      "Gradient Descent(31/49): loss=1.1492173424629774e+73\n",
      "Gradient Descent(32/49): loss=3.4922878287523747e+75\n",
      "Gradient Descent(33/49): loss=1.061250455263212e+78\n",
      "Gradient Descent(34/49): loss=3.224970517961963e+80\n",
      "Gradient Descent(35/49): loss=9.80016996943907e+82\n",
      "Gradient Descent(36/49): loss=2.9781150213611936e+85\n",
      "Gradient Descent(37/49): loss=9.050015569234775e+87\n",
      "Gradient Descent(38/49): loss=2.7501550885685946e+90\n",
      "Gradient Descent(39/49): loss=8.357281767438019e+92\n",
      "Gradient Descent(40/49): loss=2.5396443579008112e+95\n",
      "Gradient Descent(41/49): loss=7.717573302060177e+97\n",
      "Gradient Descent(42/49): loss=2.345247179487064e+100\n",
      "Gradient Descent(43/49): loss=7.126831346614791e+102\n",
      "Gradient Descent(44/49): loss=2.165730140828931e+105\n",
      "Gradient Descent(45/49): loss=6.581307757651522e+107\n",
      "Gradient Descent(46/49): loss=1.9999542410369487e+110\n",
      "Gradient Descent(47/49): loss=6.07754129350874e+112\n",
      "Gradient Descent(48/49): loss=1.8468676640898119e+115\n",
      "Gradient Descent(49/49): loss=5.612335653405875e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.530129619821188\n",
      "Gradient Descent(2/49): loss=446.4117063332943\n",
      "Gradient Descent(3/49): loss=35511.379862628084\n",
      "Gradient Descent(4/49): loss=3393310.57020914\n",
      "Gradient Descent(5/49): loss=476730042.1116542\n",
      "Gradient Descent(6/49): loss=103179987068.55344\n",
      "Gradient Descent(7/49): loss=28220528317708.418\n",
      "Gradient Descent(8/49): loss=8341253844397832.0\n",
      "Gradient Descent(9/49): loss=2.5175852522689797e+18\n",
      "Gradient Descent(10/49): loss=7.639043407839843e+20\n",
      "Gradient Descent(11/49): loss=2.3209588550752693e+23\n",
      "Gradient Descent(12/49): loss=7.054045924316631e+25\n",
      "Gradient Descent(13/49): loss=2.144096974208499e+28\n",
      "Gradient Descent(14/49): loss=6.517173979951346e+30\n",
      "Gradient Descent(15/49): loss=1.9809630483101503e+33\n",
      "Gradient Descent(16/49): loss=6.021351382744398e+35\n",
      "Gradient Descent(17/49): loss=1.8302554218341635e+38\n",
      "Gradient Descent(18/49): loss=5.5632613858633235e+40\n",
      "Gradient Descent(19/49): loss=1.6910141317665572e+43\n",
      "Gradient Descent(20/49): loss=5.140022387686714e+45\n",
      "Gradient Descent(21/49): loss=1.562366019313062e+48\n",
      "Gradient Descent(22/49): loss=4.7489823874152264e+50\n",
      "Gradient Descent(23/49): loss=1.443505135083545e+53\n",
      "Gradient Descent(24/49): loss=4.387691730698048e+55\n",
      "Gradient Descent(25/49): loss=1.333686888658388e+58\n",
      "Gradient Descent(26/49): loss=4.053887160158926e+60\n",
      "Gradient Descent(27/49): loss=1.2322233387056686e+63\n",
      "Gradient Descent(28/49): loss=3.7454776032579153e+65\n",
      "Gradient Descent(29/49): loss=1.1384788808855464e+68\n",
      "Gradient Descent(30/49): loss=3.4605310711110024e+70\n",
      "Gradient Descent(31/49): loss=1.0518662660487667e+73\n",
      "Gradient Descent(32/49): loss=3.197262555704108e+75\n",
      "Gradient Descent(33/49): loss=9.718429214872867e+77\n",
      "Gradient Descent(34/49): loss=2.954022847951405e+80\n",
      "Gradient Descent(35/49): loss=8.979075520624826e+82\n",
      "Gradient Descent(36/49): loss=2.729288206453661e+85\n",
      "Gradient Descent(37/49): loss=8.295969999112427e+87\n",
      "Gradient Descent(38/49): loss=2.5216508122313545e+90\n",
      "Gradient Descent(39/49): loss=7.664833430578242e+92\n",
      "Gradient Descent(40/49): loss=2.3298099496386443e+95\n",
      "Gradient Descent(41/49): loss=7.081712147560425e+97\n",
      "Gradient Descent(42/49): loss=2.152563858210075e+100\n",
      "Gradient Descent(43/49): loss=6.542953267690413e+102\n",
      "Gradient Descent(44/49): loss=1.9888022043991022e+105\n",
      "Gradient Descent(45/49): loss=6.045181810719108e+107\n",
      "Gradient Descent(46/49): loss=1.8374991260475946e+110\n",
      "Gradient Descent(47/49): loss=5.5852795564208125e+112\n",
      "Gradient Descent(48/49): loss=1.6977068060148391e+115\n",
      "Gradient Descent(49/49): loss=5.160365510936065e+117\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=6.289075561970358\n",
      "Gradient Descent(2/49): loss=423.92359493754844\n",
      "Gradient Descent(3/49): loss=32899.27578454811\n",
      "Gradient Descent(4/49): loss=2943017.118071013\n",
      "Gradient Descent(5/49): loss=369057649.9541274\n",
      "Gradient Descent(6/49): loss=73371920161.20503\n",
      "Gradient Descent(7/49): loss=19581266479870.68\n",
      "Gradient Descent(8/49): loss=5807511607225475.0\n",
      "Gradient Descent(9/49): loss=1.7729205660743278e+18\n",
      "Gradient Descent(10/49): loss=5.451834824530375e+20\n",
      "Gradient Descent(11/49): loss=1.6794657792042174e+23\n",
      "Gradient Descent(12/49): loss=5.1759373310148775e+25\n",
      "Gradient Descent(13/49): loss=1.5953393225270241e+28\n",
      "Gradient Descent(14/49): loss=4.9173192198851037e+30\n",
      "Gradient Descent(15/49): loss=1.5156763855817288e+33\n",
      "Gradient Descent(16/49): loss=4.671810696223671e+35\n",
      "Gradient Descent(17/49): loss=1.4400055056746306e+38\n",
      "Gradient Descent(18/49): loss=4.438570356688426e+40\n",
      "Gradient Descent(19/49): loss=1.3681133283848432e+43\n",
      "Gradient Descent(20/49): loss=4.216975149916272e+45\n",
      "Gradient Descent(21/49): loss=1.2998104082285912e+48\n",
      "Gradient Descent(22/49): loss=4.006443096829943e+50\n",
      "Gradient Descent(23/49): loss=1.2349175070296808e+53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/49): loss=3.806421836876982e+55\n",
      "Gradient Descent(25/49): loss=1.173264377405418e+58\n",
      "Gradient Descent(26/49): loss=3.616386617883411e+60\n",
      "Gradient Descent(27/49): loss=1.1146892739496038e+63\n",
      "Gradient Descent(28/49): loss=3.435838887672985e+65\n",
      "Gradient Descent(29/49): loss=1.059038526514052e+68\n",
      "Gradient Descent(30/49): loss=3.264304984337212e+70\n",
      "Gradient Descent(31/49): loss=1.0061661369245098e+73\n",
      "Gradient Descent(32/49): loss=3.101334893495408e+75\n",
      "Gradient Descent(33/49): loss=9.559333959510472e+77\n",
      "Gradient Descent(34/49): loss=2.946501067689024e+80\n",
      "Gradient Descent(35/49): loss=9.082085194078675e+82\n",
      "Gradient Descent(36/49): loss=2.799397304722433e+85\n",
      "Gradient Descent(37/49): loss=8.628663024210053e+87\n",
      "Gradient Descent(38/49): loss=2.659637682002815e+90\n",
      "Gradient Descent(39/49): loss=8.197877909569817e+92\n",
      "Gradient Descent(40/49): loss=2.526855544083154e+95\n",
      "Gradient Descent(41/49): loss=7.788599697502446e+97\n",
      "Gradient Descent(42/49): loss=2.4007025407518932e+100\n",
      "Gradient Descent(43/49): loss=7.399754658107319e+102\n",
      "Gradient Descent(44/49): loss=2.280847713146127e+105\n",
      "Gradient Descent(45/49): loss=7.030322667339067e+107\n",
      "Gradient Descent(46/49): loss=2.166976625490046e+110\n",
      "Gradient Descent(47/49): loss=6.67933453074022e+112\n",
      "Gradient Descent(48/49): loss=2.0587905401816886e+115\n",
      "Gradient Descent(49/49): loss=6.345869440786669e+117\n",
      "Cross validation finished: optimal gamma 0.32\n",
      "Gradient Descent(0/499): loss=0.5\n",
      "Gradient Descent(1/499): loss=0.39742959115546833\n",
      "Gradient Descent(2/499): loss=0.3868562047853754\n",
      "Gradient Descent(3/499): loss=0.38106021105129984\n",
      "Gradient Descent(4/499): loss=0.3769580919062119\n",
      "Gradient Descent(5/499): loss=0.37372559553177925\n",
      "Gradient Descent(6/499): loss=0.37105507246251873\n",
      "Gradient Descent(7/499): loss=0.36879601579617444\n",
      "Gradient Descent(8/499): loss=0.3668590803474846\n",
      "Gradient Descent(9/499): loss=0.36518413287770773\n",
      "Gradient Descent(10/499): loss=0.36372727926728665\n",
      "Gradient Descent(11/499): loss=0.3624547098768707\n",
      "Gradient Descent(12/499): loss=0.3613394202506741\n",
      "Gradient Descent(13/499): loss=0.3603592994138174\n",
      "Gradient Descent(14/499): loss=0.35949592950914994\n",
      "Gradient Descent(15/499): loss=0.3587337827436062\n",
      "Gradient Descent(16/499): loss=0.3580596543822474\n",
      "Gradient Descent(17/499): loss=0.3574622441074781\n",
      "Gradient Descent(18/499): loss=0.3569318355874206\n",
      "Gradient Descent(19/499): loss=0.3564600441615056\n",
      "Gradient Descent(20/499): loss=0.3560396137452749\n",
      "Gradient Descent(21/499): loss=0.3556642505600095\n",
      "Gradient Descent(22/499): loss=0.3553284852233035\n",
      "Gradient Descent(23/499): loss=0.3550275572047675\n",
      "Gradient Descent(24/499): loss=0.35475731725872633\n",
      "Gradient Descent(25/499): loss=0.35451414452993907\n",
      "Gradient Descent(26/499): loss=0.3542948757834315\n",
      "Gradient Descent(27/499): loss=0.35409674475117164\n",
      "Gradient Descent(28/499): loss=0.3539173299873701\n",
      "Gradient Descent(29/499): loss=0.3537545099253039\n",
      "Gradient Descent(30/499): loss=0.3536064240606093\n",
      "Gradient Descent(31/499): loss=0.3534714393681467\n",
      "Gradient Descent(32/499): loss=0.3533481212048202\n",
      "Gradient Descent(33/499): loss=0.35323520806821357\n",
      "Gradient Descent(34/499): loss=0.3531315896770058\n",
      "Gradient Descent(35/499): loss=0.35303628791853814\n",
      "Gradient Descent(36/499): loss=0.35294844027505373\n",
      "Gradient Descent(37/499): loss=0.35286728539564066\n",
      "Gradient Descent(38/499): loss=0.35279215052774826\n",
      "Gradient Descent(39/499): loss=0.35272244056187935\n",
      "Gradient Descent(40/499): loss=0.3526576284768924\n",
      "Gradient Descent(41/499): loss=0.352597247002261\n",
      "Gradient Descent(42/499): loss=0.35254088133842043\n",
      "Gradient Descent(43/499): loss=0.3524881627976123\n",
      "Gradient Descent(44/499): loss=0.352438763245971\n",
      "Gradient Descent(45/499): loss=0.35239239024339075\n",
      "Gradient Descent(46/499): loss=0.3523487827913616\n",
      "Gradient Descent(47/499): loss=0.35230770761075925\n",
      "Gradient Descent(48/499): loss=0.3522689558817824\n",
      "Gradient Descent(49/499): loss=0.3522323403870804\n",
      "Gradient Descent(50/499): loss=0.35219769300677894\n",
      "Gradient Descent(51/499): loss=0.3521648625207646\n",
      "Gradient Descent(52/499): loss=0.3521337126793631\n",
      "Gradient Descent(53/499): loss=0.35210412050856166\n",
      "Gradient Descent(54/499): loss=0.35207597482028063\n",
      "Gradient Descent(55/499): loss=0.3520491749019894\n",
      "Gradient Descent(56/499): loss=0.3520236293632514\n",
      "Gradient Descent(57/499): loss=0.35199925511965163\n",
      "Gradient Descent(58/499): loss=0.3519759764970493\n",
      "Gradient Descent(59/499): loss=0.35195372444126954\n",
      "Gradient Descent(60/499): loss=0.3519324358202366\n",
      "Gradient Descent(61/499): loss=0.3519120528071984\n",
      "Gradient Descent(62/499): loss=0.3518925223351209\n",
      "Gradient Descent(63/499): loss=0.3518737956135867\n",
      "Gradient Descent(64/499): loss=0.3518558277006138\n",
      "Gradient Descent(65/499): loss=0.3518385771227679\n",
      "Gradient Descent(66/499): loss=0.3518220055377614\n",
      "Gradient Descent(67/499): loss=0.35180607743446257\n",
      "Gradient Descent(68/499): loss=0.3517907598658637\n",
      "Gradient Descent(69/499): loss=0.351776022211109\n",
      "Gradient Descent(70/499): loss=0.351761835963166\n",
      "Gradient Descent(71/499): loss=0.3517481745391405\n",
      "Gradient Descent(72/499): loss=0.3517350131106047\n",
      "Gradient Descent(73/499): loss=0.35172232845162843\n",
      "Gradient Descent(74/499): loss=0.35171009880248394\n",
      "Gradient Descent(75/499): loss=0.35169830374723887\n",
      "Gradient Descent(76/499): loss=0.3516869241036699\n",
      "Gradient Descent(77/499): loss=0.3516759418241135\n",
      "Gradient Descent(78/499): loss=0.35166533990604\n",
      "Gradient Descent(79/499): loss=0.35165510231127545\n",
      "Gradient Descent(80/499): loss=0.3516452138929291\n",
      "Gradient Descent(81/499): loss=0.3516356603291881\n",
      "Gradient Descent(82/499): loss=0.3516264280632461\n",
      "Gradient Descent(83/499): loss=0.3516175042487108\n",
      "Gradient Descent(84/499): loss=0.3516088766999152\n",
      "Gradient Descent(85/499): loss=0.35160053384662265\n",
      "Gradient Descent(86/499): loss=0.3515924646926716\n",
      "Gradient Descent(87/499): loss=0.35158465877816053\n",
      "Gradient Descent(88/499): loss=0.351577106144814\n",
      "Gradient Descent(89/499): loss=0.3515697973042179\n",
      "Gradient Descent(90/499): loss=0.35156272320863496\n",
      "Gradient Descent(91/499): loss=0.35155587522415754\n",
      "Gradient Descent(92/499): loss=0.3515492451059682\n",
      "Gradient Descent(93/499): loss=0.35154282497551026\n",
      "Gradient Descent(94/499): loss=0.351536607299392\n",
      "Gradient Descent(95/499): loss=0.3515305848698613\n",
      "Gradient Descent(96/499): loss=0.35152475078671164\n",
      "Gradient Descent(97/499): loss=0.3515190984404879\n",
      "Gradient Descent(98/499): loss=0.35151362149688126\n",
      "Gradient Descent(99/499): loss=0.3515083138822055\n",
      "Gradient Descent(100/499): loss=0.3515031697698649\n",
      "Gradient Descent(101/499): loss=0.35149818356772955\n",
      "Gradient Descent(102/499): loss=0.3514933499063417\n",
      "Gradient Descent(103/499): loss=0.3514886636278857\n",
      "Gradient Descent(104/499): loss=0.35148411977586\n",
      "Gradient Descent(105/499): loss=0.35147971358539565\n",
      "Gradient Descent(106/499): loss=0.3514754404741693\n",
      "Gradient Descent(107/499): loss=0.35147129603386756\n",
      "Gradient Descent(108/499): loss=0.351467276022158\n",
      "Gradient Descent(109/499): loss=0.3514633763551314\n",
      "Gradient Descent(110/499): loss=0.3514595931001791\n",
      "Gradient Descent(111/499): loss=0.35145592246927393\n",
      "Gradient Descent(112/499): loss=0.35145236081262804\n",
      "Gradient Descent(113/499): loss=0.3514489046126983\n",
      "Gradient Descent(114/499): loss=0.3514455504785167\n",
      "Gradient Descent(115/499): loss=0.35144229514032455\n",
      "Gradient Descent(116/499): loss=0.35143913544448785\n",
      "Gradient Descent(117/499): loss=0.3514360683486779\n",
      "Gradient Descent(118/499): loss=0.3514330909172973\n",
      "Gradient Descent(119/499): loss=0.35143020031713756\n",
      "Gradient Descent(120/499): loss=0.3514273938132536\n",
      "Gradient Descent(121/499): loss=0.35142466876503964\n",
      "Gradient Descent(122/499): loss=0.351422022622497\n",
      "Gradient Descent(123/499): loss=0.3514194529226798\n",
      "Gradient Descent(124/499): loss=0.3514169572863097\n",
      "Gradient Descent(125/499): loss=0.35141453341454826\n",
      "Gradient Descent(126/499): loss=0.3514121790859198\n",
      "Gradient Descent(127/499): loss=0.351409892153373\n",
      "Gradient Descent(128/499): loss=0.3514076705414767\n",
      "Gradient Descent(129/499): loss=0.35140551224374095\n",
      "Gradient Descent(130/499): loss=0.3514034153200561\n",
      "Gradient Descent(131/499): loss=0.3514013778942435\n",
      "Gradient Descent(132/499): loss=0.35139939815171384\n",
      "Gradient Descent(133/499): loss=0.35139747433722374\n",
      "Gradient Descent(134/499): loss=0.3513956047527296\n",
      "Gradient Descent(135/499): loss=0.35139378775533\n",
      "Gradient Descent(136/499): loss=0.3513920217552952\n",
      "Gradient Descent(137/499): loss=0.3513903052141773\n",
      "Gradient Descent(138/499): loss=0.351388636642997\n",
      "Gradient Descent(139/499): loss=0.3513870146005059\n",
      "Gradient Descent(140/499): loss=0.3513854376915161\n",
      "Gradient Descent(141/499): loss=0.3513839045652986\n",
      "Gradient Descent(142/499): loss=0.3513824139140441\n",
      "Gradient Descent(143/499): loss=0.3513809644713839\n",
      "Gradient Descent(144/499): loss=0.3513795550109689\n",
      "Gradient Descent(145/499): loss=0.3513781843451038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(146/499): loss=0.35137685132343305\n",
      "Gradient Descent(147/499): loss=0.35137555483167676\n",
      "Gradient Descent(148/499): loss=0.35137429379041546\n",
      "Gradient Descent(149/499): loss=0.35137306715391975\n",
      "Gradient Descent(150/499): loss=0.35137187390902325\n",
      "Gradient Descent(151/499): loss=0.3513707130740386\n",
      "Gradient Descent(152/499): loss=0.3513695836977122\n",
      "Gradient Descent(153/499): loss=0.35136848485821826\n",
      "Gradient Descent(154/499): loss=0.35136741566218826\n",
      "Gradient Descent(155/499): loss=0.35136637524377684\n",
      "Gradient Descent(156/499): loss=0.3513653627637609\n",
      "Gradient Descent(157/499): loss=0.35136437740867027\n",
      "Gradient Descent(158/499): loss=0.3513634183899505\n",
      "Gradient Descent(159/499): loss=0.3513624849431544\n",
      "Gradient Descent(160/499): loss=0.35136157632716253\n",
      "Gradient Descent(161/499): loss=0.35136069182343105\n",
      "Gradient Descent(162/499): loss=0.3513598307352655\n",
      "Gradient Descent(163/499): loss=0.3513589923871206\n",
      "Gradient Descent(164/499): loss=0.35135817612392317\n",
      "Gradient Descent(165/499): loss=0.35135738131041916\n",
      "Gradient Descent(166/499): loss=0.3513566073305431\n",
      "Gradient Descent(167/499): loss=0.35135585358680865\n",
      "Gradient Descent(168/499): loss=0.3513551194997202\n",
      "Gradient Descent(169/499): loss=0.3513544045072045\n",
      "Gradient Descent(170/499): loss=0.35135370806406097\n",
      "Gradient Descent(171/499): loss=0.3513530296414314\n",
      "Gradient Descent(172/499): loss=0.3513523687262861\n",
      "Gradient Descent(173/499): loss=0.3513517248209289\n",
      "Gradient Descent(174/499): loss=0.35135109744251725\n",
      "Gradient Descent(175/499): loss=0.35135048612259867\n",
      "Gradient Descent(176/499): loss=0.35134989040666276\n",
      "Gradient Descent(177/499): loss=0.3513493098537079\n",
      "Gradient Descent(178/499): loss=0.3513487440358219\n",
      "Gradient Descent(179/499): loss=0.35134819253777666\n",
      "Gradient Descent(180/499): loss=0.35134765495663617\n",
      "Gradient Descent(181/499): loss=0.351347130901377\n",
      "Gradient Descent(182/499): loss=0.3513466199925211\n",
      "Gradient Descent(183/499): loss=0.3513461218617813\n",
      "Gradient Descent(184/499): loss=0.35134563615171666\n",
      "Gradient Descent(185/499): loss=0.3513451625154006\n",
      "Gradient Descent(186/499): loss=0.351344700616099\n",
      "Gradient Descent(187/499): loss=0.3513442501269583\n",
      "Gradient Descent(188/499): loss=0.351343810730704\n",
      "Gradient Descent(189/499): loss=0.3513433821193492\n",
      "Gradient Descent(190/499): loss=0.351342963993911\n",
      "Gradient Descent(191/499): loss=0.35134255606413795\n",
      "Gradient Descent(192/499): loss=0.3513421580482442\n",
      "Gradient Descent(193/499): loss=0.3513417696726535\n",
      "Gradient Descent(194/499): loss=0.35134139067175046\n",
      "Gradient Descent(195/499): loss=0.35134102078763985\n",
      "Gradient Descent(196/499): loss=0.35134065976991374\n",
      "Gradient Descent(197/499): loss=0.3513403073754259\n",
      "Gradient Descent(198/499): loss=0.35133996336807266\n",
      "Gradient Descent(199/499): loss=0.3513396275185816\n",
      "Gradient Descent(200/499): loss=0.35133929960430627\n",
      "Gradient Descent(201/499): loss=0.35133897940902714\n",
      "Gradient Descent(202/499): loss=0.3513386667227593\n",
      "Gradient Descent(203/499): loss=0.3513383613415659\n",
      "Gradient Descent(204/499): loss=0.35133806306737714\n",
      "Gradient Descent(205/499): loss=0.3513377717078152\n",
      "Gradient Descent(206/499): loss=0.3513374870760245\n",
      "Gradient Descent(207/499): loss=0.35133720899050713\n",
      "Gradient Descent(208/499): loss=0.3513369372749634\n",
      "Gradient Descent(209/499): loss=0.35133667175813743\n",
      "Gradient Descent(210/499): loss=0.3513364122736671\n",
      "Gradient Descent(211/499): loss=0.35133615865993945\n",
      "Gradient Descent(212/499): loss=0.35133591075994913\n",
      "Gradient Descent(213/499): loss=0.35133566842116287\n",
      "Gradient Descent(214/499): loss=0.3513354314953868\n",
      "Gradient Descent(215/499): loss=0.3513351998386381\n",
      "Gradient Descent(216/499): loss=0.35133497331102137\n",
      "Gradient Descent(217/499): loss=0.35133475177660745\n",
      "Gradient Descent(218/499): loss=0.35133453510331725\n",
      "Gradient Descent(219/499): loss=0.3513343231628079\n",
      "Gradient Descent(220/499): loss=0.3513341158303635\n",
      "Gradient Descent(221/499): loss=0.3513339129847882\n",
      "Gradient Descent(222/499): loss=0.35133371450830336\n",
      "Gradient Descent(223/499): loss=0.351333520286447\n",
      "Gradient Descent(224/499): loss=0.3513333302079772\n",
      "Gradient Descent(225/499): loss=0.35133314416477757\n",
      "Gradient Descent(226/499): loss=0.3513329620517659\n",
      "Gradient Descent(227/499): loss=0.351332783766806\n",
      "Gradient Descent(228/499): loss=0.3513326092106215\n",
      "Gradient Descent(229/499): loss=0.3513324382867124\n",
      "Gradient Descent(230/499): loss=0.351332270901275\n",
      "Gradient Descent(231/499): loss=0.3513321069631226\n",
      "Gradient Descent(232/499): loss=0.35133194638360976\n",
      "Gradient Descent(233/499): loss=0.3513317890765592\n",
      "Gradient Descent(234/499): loss=0.35133163495818925\n",
      "Gradient Descent(235/499): loss=0.3513314839470453\n",
      "Gradient Descent(236/499): loss=0.35133133596393207\n",
      "Gradient Descent(237/499): loss=0.3513311909318481\n",
      "Gradient Descent(238/499): loss=0.35133104877592297\n",
      "Gradient Descent(239/499): loss=0.35133090942335515\n",
      "Gradient Descent(240/499): loss=0.35133077280335306\n",
      "Gradient Descent(241/499): loss=0.3513306388470766\n",
      "Gradient Descent(242/499): loss=0.3513305074875813\n",
      "Gradient Descent(243/499): loss=0.35133037865976385\n",
      "Gradient Descent(244/499): loss=0.35133025230030895\n",
      "Gradient Descent(245/499): loss=0.3513301283476382\n",
      "Gradient Descent(246/499): loss=0.35133000674186077\n",
      "Gradient Descent(247/499): loss=0.3513298874247238\n",
      "Gradient Descent(248/499): loss=0.3513297703395672\n",
      "Gradient Descent(249/499): loss=0.35132965543127676\n",
      "Gradient Descent(250/499): loss=0.35132954264624067\n",
      "Gradient Descent(251/499): loss=0.3513294319323067\n",
      "Gradient Descent(252/499): loss=0.35132932323874017\n",
      "Gradient Descent(253/499): loss=0.3513292165161839\n",
      "Gradient Descent(254/499): loss=0.35132911171661907\n",
      "Gradient Descent(255/499): loss=0.3513290087933271\n",
      "Gradient Descent(256/499): loss=0.35132890770085246\n",
      "Gradient Descent(257/499): loss=0.3513288083949675\n",
      "Gradient Descent(258/499): loss=0.35132871083263684\n",
      "Gradient Descent(259/499): loss=0.3513286149719843\n",
      "Gradient Descent(260/499): loss=0.35132852077225984\n",
      "Gradient Descent(261/499): loss=0.35132842819380744\n",
      "Gradient Descent(262/499): loss=0.35132833719803497\n",
      "Gradient Descent(263/499): loss=0.35132824774738347\n",
      "Gradient Descent(264/499): loss=0.35132815980529836\n",
      "Gradient Descent(265/499): loss=0.3513280733362009\n",
      "Gradient Descent(266/499): loss=0.35132798830546147\n",
      "Gradient Descent(267/499): loss=0.35132790467937197\n",
      "Gradient Descent(268/499): loss=0.35132782242512045\n",
      "Gradient Descent(269/499): loss=0.3513277415107664\n",
      "Gradient Descent(270/499): loss=0.351327661905215\n",
      "Gradient Descent(271/499): loss=0.35132758357819527\n",
      "Gradient Descent(272/499): loss=0.3513275065002354\n",
      "Gradient Descent(273/499): loss=0.3513274306426415\n",
      "Gradient Descent(274/499): loss=0.35132735597747544\n",
      "Gradient Descent(275/499): loss=0.3513272824775339\n",
      "Gradient Descent(276/499): loss=0.351327210116328\n",
      "Gradient Descent(277/499): loss=0.3513271388680632\n",
      "Gradient Descent(278/499): loss=0.3513270687076201\n",
      "Gradient Descent(279/499): loss=0.35132699961053576\n",
      "Gradient Descent(280/499): loss=0.35132693155298583\n",
      "Gradient Descent(281/499): loss=0.351326864511766\n",
      "Gradient Descent(282/499): loss=0.3513267984642757\n",
      "Gradient Descent(283/499): loss=0.351326733388501\n",
      "Gradient Descent(284/499): loss=0.3513266692629985\n",
      "Gradient Descent(285/499): loss=0.3513266060668795\n",
      "Gradient Descent(286/499): loss=0.35132654377979483\n",
      "Gradient Descent(287/499): loss=0.3513264823819202\n",
      "Gradient Descent(288/499): loss=0.35132642185394103\n",
      "Gradient Descent(289/499): loss=0.3513263621770396\n",
      "Gradient Descent(290/499): loss=0.3513263033328805\n",
      "Gradient Descent(291/499): loss=0.3513262453035974\n",
      "Gradient Descent(292/499): loss=0.351326188071781\n",
      "Gradient Descent(293/499): loss=0.3513261316204659\n",
      "Gradient Descent(294/499): loss=0.35132607593311843\n",
      "Gradient Descent(295/499): loss=0.35132602099362514\n",
      "Gradient Descent(296/499): loss=0.3513259667862814\n",
      "Gradient Descent(297/499): loss=0.35132591329578006\n",
      "Gradient Descent(298/499): loss=0.35132586050720066\n",
      "Gradient Descent(299/499): loss=0.35132580840599914\n",
      "Gradient Descent(300/499): loss=0.3513257569779973\n",
      "Gradient Descent(301/499): loss=0.35132570620937326\n",
      "Gradient Descent(302/499): loss=0.35132565608665123\n",
      "Gradient Descent(303/499): loss=0.3513256065966932\n",
      "Gradient Descent(304/499): loss=0.3513255577266885\n",
      "Gradient Descent(305/499): loss=0.3513255094641459\n",
      "Gradient Descent(306/499): loss=0.35132546179688495\n",
      "Gradient Descent(307/499): loss=0.3513254147130271\n",
      "Gradient Descent(308/499): loss=0.35132536820098786\n",
      "Gradient Descent(309/499): loss=0.3513253222494692\n",
      "Gradient Descent(310/499): loss=0.35132527684745135\n",
      "Gradient Descent(311/499): loss=0.3513252319841858\n",
      "Gradient Descent(312/499): loss=0.3513251876491877\n",
      "Gradient Descent(313/499): loss=0.3513251438322289\n",
      "Gradient Descent(314/499): loss=0.3513251005233315\n",
      "Gradient Descent(315/499): loss=0.35132505771276035\n",
      "Gradient Descent(316/499): loss=0.35132501539101746\n",
      "Gradient Descent(317/499): loss=0.35132497354883513\n",
      "Gradient Descent(318/499): loss=0.35132493217717026\n",
      "Gradient Descent(319/499): loss=0.35132489126719774\n",
      "Gradient Descent(320/499): loss=0.3513248508103054\n",
      "Gradient Descent(321/499): loss=0.3513248107980881\n",
      "Gradient Descent(322/499): loss=0.351324771222342\n",
      "Gradient Descent(323/499): loss=0.3513247320750596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(324/499): loss=0.35132469334842453\n",
      "Gradient Descent(325/499): loss=0.35132465503480637\n",
      "Gradient Descent(326/499): loss=0.3513246171267557\n",
      "Gradient Descent(327/499): loss=0.3513245796169998\n",
      "Gradient Descent(328/499): loss=0.35132454249843725\n",
      "Gradient Descent(329/499): loss=0.3513245057641344\n",
      "Gradient Descent(330/499): loss=0.35132446940732015\n",
      "Gradient Descent(331/499): loss=0.3513244334213821\n",
      "Gradient Descent(332/499): loss=0.3513243977998623\n",
      "Gradient Descent(333/499): loss=0.3513243625364536\n",
      "Gradient Descent(334/499): loss=0.35132432762499505\n",
      "Gradient Descent(335/499): loss=0.35132429305946855\n",
      "Gradient Descent(336/499): loss=0.3513242588339952\n",
      "Gradient Descent(337/499): loss=0.35132422494283144\n",
      "Gradient Descent(338/499): loss=0.35132419138036575\n",
      "Gradient Descent(339/499): loss=0.35132415814111523\n",
      "Gradient Descent(340/499): loss=0.351324125219722\n",
      "Gradient Descent(341/499): loss=0.35132409261095027\n",
      "Gradient Descent(342/499): loss=0.3513240603096834\n",
      "Gradient Descent(343/499): loss=0.3513240283109203\n",
      "Gradient Descent(344/499): loss=0.35132399660977276\n",
      "Gradient Descent(345/499): loss=0.3513239652014628\n",
      "Gradient Descent(346/499): loss=0.35132393408131934\n",
      "Gradient Descent(347/499): loss=0.35132390324477575\n",
      "Gradient Descent(348/499): loss=0.35132387268736753\n",
      "Gradient Descent(349/499): loss=0.3513238424047289\n",
      "Gradient Descent(350/499): loss=0.3513238123925912\n",
      "Gradient Descent(351/499): loss=0.35132378264677944\n",
      "Gradient Descent(352/499): loss=0.35132375316321096\n",
      "Gradient Descent(353/499): loss=0.35132372393789246\n",
      "Gradient Descent(354/499): loss=0.35132369496691795\n",
      "Gradient Descent(355/499): loss=0.3513236662464663\n",
      "Gradient Descent(356/499): loss=0.35132363777279957\n",
      "Gradient Descent(357/499): loss=0.3513236095422605\n",
      "Gradient Descent(358/499): loss=0.35132358155127064\n",
      "Gradient Descent(359/499): loss=0.3513235537963285\n",
      "Gradient Descent(360/499): loss=0.3513235262740073\n",
      "Gradient Descent(361/499): loss=0.35132349898095366\n",
      "Gradient Descent(362/499): loss=0.3513234719138851\n",
      "Gradient Descent(363/499): loss=0.3513234450695887\n",
      "Gradient Descent(364/499): loss=0.35132341844491943\n",
      "Gradient Descent(365/499): loss=0.35132339203679824\n",
      "Gradient Descent(366/499): loss=0.35132336584221036\n",
      "Gradient Descent(367/499): loss=0.3513233398582042\n",
      "Gradient Descent(368/499): loss=0.35132331408188905\n",
      "Gradient Descent(369/499): loss=0.35132328851043426\n",
      "Gradient Descent(370/499): loss=0.35132326314106743\n",
      "Gradient Descent(371/499): loss=0.351323237971073\n",
      "Gradient Descent(372/499): loss=0.3513232129977912\n",
      "Gradient Descent(373/499): loss=0.3513231882186158\n",
      "Gradient Descent(374/499): loss=0.35132316363099403\n",
      "Gradient Descent(375/499): loss=0.3513231392324243\n",
      "Gradient Descent(376/499): loss=0.35132311502045543\n",
      "Gradient Descent(377/499): loss=0.3513230909926852\n",
      "Gradient Descent(378/499): loss=0.3513230671467594\n",
      "Gradient Descent(379/499): loss=0.3513230434803706\n",
      "Gradient Descent(380/499): loss=0.35132301999125665\n",
      "Gradient Descent(381/499): loss=0.3513229966772003\n",
      "Gradient Descent(382/499): loss=0.35132297353602754\n",
      "Gradient Descent(383/499): loss=0.35132295056560664\n",
      "Gradient Descent(384/499): loss=0.35132292776384755\n",
      "Gradient Descent(385/499): loss=0.35132290512870046\n",
      "Gradient Descent(386/499): loss=0.35132288265815487\n",
      "Gradient Descent(387/499): loss=0.35132286035023913\n",
      "Gradient Descent(388/499): loss=0.3513228382030188\n",
      "Gradient Descent(389/499): loss=0.35132281621459627\n",
      "Gradient Descent(390/499): loss=0.35132279438310987\n",
      "Gradient Descent(391/499): loss=0.35132277270673284\n",
      "Gradient Descent(392/499): loss=0.35132275118367257\n",
      "Gradient Descent(393/499): loss=0.35132272981216983\n",
      "Gradient Descent(394/499): loss=0.35132270859049797\n",
      "Gradient Descent(395/499): loss=0.3513226875169621\n",
      "Gradient Descent(396/499): loss=0.3513226665898987\n",
      "Gradient Descent(397/499): loss=0.3513226458076742\n",
      "Gradient Descent(398/499): loss=0.35132262516868495\n",
      "Gradient Descent(399/499): loss=0.35132260467135623\n",
      "Gradient Descent(400/499): loss=0.3513225843141415\n",
      "Gradient Descent(401/499): loss=0.3513225640955221\n",
      "Gradient Descent(402/499): loss=0.3513225440140062\n",
      "Gradient Descent(403/499): loss=0.35132252406812836\n",
      "Gradient Descent(404/499): loss=0.3513225042564491\n",
      "Gradient Descent(405/499): loss=0.351322484577554\n",
      "Gradient Descent(406/499): loss=0.3513224650300533\n",
      "Gradient Descent(407/499): loss=0.35132244561258136\n",
      "Gradient Descent(408/499): loss=0.35132242632379596\n",
      "Gradient Descent(409/499): loss=0.35132240716237795\n",
      "Gradient Descent(410/499): loss=0.3513223881270309\n",
      "Gradient Descent(411/499): loss=0.35132236921647986\n",
      "Gradient Descent(412/499): loss=0.3513223504294717\n",
      "Gradient Descent(413/499): loss=0.3513223317647743\n",
      "Gradient Descent(414/499): loss=0.35132231322117585\n",
      "Gradient Descent(415/499): loss=0.3513222947974847\n",
      "Gradient Descent(416/499): loss=0.3513222764925292\n",
      "Gradient Descent(417/499): loss=0.35132225830515623\n",
      "Gradient Descent(418/499): loss=0.35132224023423203\n",
      "Gradient Descent(419/499): loss=0.35132222227864085\n",
      "Gradient Descent(420/499): loss=0.35132220443728523\n",
      "Gradient Descent(421/499): loss=0.3513221867090849\n",
      "Gradient Descent(422/499): loss=0.351322169092977\n",
      "Gradient Descent(423/499): loss=0.3513221515879153\n",
      "Gradient Descent(424/499): loss=0.3513221341928703\n",
      "Gradient Descent(425/499): loss=0.35132211690682846\n",
      "Gradient Descent(426/499): loss=0.3513220997287918\n",
      "Gradient Descent(427/499): loss=0.3513220826577782\n",
      "Gradient Descent(428/499): loss=0.3513220656928201\n",
      "Gradient Descent(429/499): loss=0.3513220488329649\n",
      "Gradient Descent(430/499): loss=0.3513220320772746\n",
      "Gradient Descent(431/499): loss=0.3513220154248251\n",
      "Gradient Descent(432/499): loss=0.3513219988747063\n",
      "Gradient Descent(433/499): loss=0.3513219824260215\n",
      "Gradient Descent(434/499): loss=0.35132196607788746\n",
      "Gradient Descent(435/499): loss=0.35132194982943366\n",
      "Gradient Descent(436/499): loss=0.3513219336798025\n",
      "Gradient Descent(437/499): loss=0.3513219176281487\n",
      "Gradient Descent(438/499): loss=0.35132190167363936\n",
      "Gradient Descent(439/499): loss=0.35132188581545315\n",
      "Gradient Descent(440/499): loss=0.3513218700527808\n",
      "Gradient Descent(441/499): loss=0.3513218543848244\n",
      "Gradient Descent(442/499): loss=0.35132183881079704\n",
      "Gradient Descent(443/499): loss=0.35132182332992296\n",
      "Gradient Descent(444/499): loss=0.35132180794143725\n",
      "Gradient Descent(445/499): loss=0.3513217926445855\n",
      "Gradient Descent(446/499): loss=0.3513217774386234\n",
      "Gradient Descent(447/499): loss=0.35132176232281703\n",
      "Gradient Descent(448/499): loss=0.3513217472964423\n",
      "Gradient Descent(449/499): loss=0.3513217323587848\n",
      "Gradient Descent(450/499): loss=0.3513217175091399\n",
      "Gradient Descent(451/499): loss=0.35132170274681174\n",
      "Gradient Descent(452/499): loss=0.35132168807111425\n",
      "Gradient Descent(453/499): loss=0.35132167348136994\n",
      "Gradient Descent(454/499): loss=0.3513216589769105\n",
      "Gradient Descent(455/499): loss=0.3513216445570756\n",
      "Gradient Descent(456/499): loss=0.3513216302212141\n",
      "Gradient Descent(457/499): loss=0.3513216159686826\n",
      "Gradient Descent(458/499): loss=0.35132160179884614\n",
      "Gradient Descent(459/499): loss=0.35132158771107763\n",
      "Gradient Descent(460/499): loss=0.3513215737047578\n",
      "Gradient Descent(461/499): loss=0.3513215597792752\n",
      "Gradient Descent(462/499): loss=0.35132154593402576\n",
      "Gradient Descent(463/499): loss=0.35132153216841283\n",
      "Gradient Descent(464/499): loss=0.351321518481847\n",
      "Gradient Descent(465/499): loss=0.3513215048737459\n",
      "Gradient Descent(466/499): loss=0.3513214913435344\n",
      "Gradient Descent(467/499): loss=0.3513214778906441\n",
      "Gradient Descent(468/499): loss=0.351321464514513\n",
      "Gradient Descent(469/499): loss=0.3513214512145863\n",
      "Gradient Descent(470/499): loss=0.3513214379903152\n",
      "Gradient Descent(471/499): loss=0.3513214248411574\n",
      "Gradient Descent(472/499): loss=0.35132141176657694\n",
      "Gradient Descent(473/499): loss=0.35132139876604385\n",
      "Gradient Descent(474/499): loss=0.35132138583903433\n",
      "Gradient Descent(475/499): loss=0.35132137298503036\n",
      "Gradient Descent(476/499): loss=0.3513213602035198\n",
      "Gradient Descent(477/499): loss=0.35132134749399624\n",
      "Gradient Descent(478/499): loss=0.35132133485595873\n",
      "Gradient Descent(479/499): loss=0.3513213222889121\n",
      "Gradient Descent(480/499): loss=0.3513213097923662\n",
      "Gradient Descent(481/499): loss=0.3513212973658366\n",
      "Gradient Descent(482/499): loss=0.35132128500884374\n",
      "Gradient Descent(483/499): loss=0.3513212727209135\n",
      "Gradient Descent(484/499): loss=0.3513212605015766\n",
      "Gradient Descent(485/499): loss=0.3513212483503687\n",
      "Gradient Descent(486/499): loss=0.3513212362668304\n",
      "Gradient Descent(487/499): loss=0.35132122425050716\n",
      "Gradient Descent(488/499): loss=0.35132121230094904\n",
      "Gradient Descent(489/499): loss=0.35132120041771087\n",
      "Gradient Descent(490/499): loss=0.35132118860035183\n",
      "Gradient Descent(491/499): loss=0.35132117684843567\n",
      "Gradient Descent(492/499): loss=0.35132116516153067\n",
      "Gradient Descent(493/499): loss=0.3513211535392091\n",
      "Gradient Descent(494/499): loss=0.35132114198104797\n",
      "Gradient Descent(495/499): loss=0.35132113048662794\n",
      "Gradient Descent(496/499): loss=0.3513211190555343\n",
      "Gradient Descent(497/499): loss=0.35132110768735614\n",
      "Gradient Descent(498/499): loss=0.35132109638168646\n",
      "Gradient Descent(499/499): loss=0.3513210851381223\n",
      "Gradient descent regression loss 0.3513210851381223\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "gamma_opt = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=2)\n",
    "w_gd, loss_gd = least_squares_GD(set3_y, set3_x, gamma_opt, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv for gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gammas = np.arange(0, 3, 0.01)\n",
    "gamma_opt = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=2)\n",
    "w_sgd, loss_sgd = least_squares_SGD(y, x2, gamma_opt, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test1 = filtering_with_mean_bis(tX_test)\n",
    "\n",
    "tX_test2 = keep(tX_test, to_keep)\n",
    "tX_test2 = standardize(filtering_with_mean(tX_test))\n",
    "\n",
    "tX_test3 = std(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_ls.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_ls = build_poly(tX_test1, degree_ls)\n",
    "y_pred = predict_labels(w_ls, tX_test_ls)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_ls2.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_ls2 = cut(tX_test1, [15,18,20]) \n",
    "tX_test_ls2 = build_poly(tX_test_ls2, degree_ls2) \n",
    "y_pred = predict_labels(w_ls2, tX_test_ls2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_rr.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test_ls = build_poly(tX_test1, degree_rr)\n",
    "y_pred = predict_labels(w_rr, tX_test1)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-2d8ff60821ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/pred_sg.csv'\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_gd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ma1/Machine_Learning/Projet1/code_local/scripts/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/pred_sg.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "y_pred = predict_labels(w_gd, tX_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_sgd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_sgd, tX_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
