{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers2 import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = Path(\"../data/\")\n",
    "DATA_TRAIN_PATH = \"../data/train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(250000,)\n",
      "(250000, 30)\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(np.shape(y))\n",
    "print(np.shape(tX))\n",
    "print(tX.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 DER_mass_MMC\n",
      "1 DER_mass_transverse_met_lep\n",
      "2 DER_mass_vis\n",
      "3 DER_pt_h\n",
      "4 DER_deltaeta_jet_jet\n",
      "5 DER_mass_jet_jet\n",
      "6 DER_prodeta_jet_jet\n",
      "7 DER_deltar_tau_lep\n",
      "8 DER_pt_tot\n",
      "9 DER_sum_pt\n",
      "10 DER_pt_ratio_lep_tau\n",
      "11 DER_met_phi_centrality\n",
      "12 DER_lep_eta_centrality\n",
      "13 PRI_tau_pt\n",
      "14 PRI_tau_eta\n",
      "15 PRI_tau_phi\n",
      "16 PRI_lep_pt\n",
      "17 PRI_lep_eta\n",
      "18 PRI_lep_phi\n",
      "19 PRI_met\n",
      "20 PRI_met_phi\n",
      "21 PRI_met_sumet\n",
      "22 PRI_jet_num\n",
      "23 PRI_jet_leading_pt\n",
      "24 PRI_jet_leading_eta\n",
      "25 PRI_jet_leading_phi\n",
      "26 PRI_jet_subleading_pt\n",
      "27 PRI_jet_subleading_eta\n",
      "28 PRI_jet_subleading_phi\n",
      "29 PRI_jet_all_pt\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', \n",
    "                 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', \n",
    "                 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
    "                 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(i, feature_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set is composed of : \n",
    "* a y vector of length 250'000 and type float\n",
    "* a tX float matrix of 250'000 rows and 30 columns\n",
    "\n",
    "It means that our data set is composed of 250'000 different obsevations of 30 different features. In the rest of the notebook, we name the features by their index nummer. So, it means from the feature 0 from the feature  29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)\n",
    "plt.title('Histogram of y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more y = -1 than y = 1 in the data, so there is more y = 'b' than y = 's'. So, we have to pay attention to normalize the data in order to compare them in the next plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ind = np.arange(30)\n",
    "plt.boxplot(tX[:,], labels = ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many outliers depending on the feature. There are also feature that has a long interquantile range. Maybe we have to treat these feature in order to be more efficient in our futur predictions. Let's do more plots to be have a better idea :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaUAAARuCAYAAADDHFv8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdf5jddX3n/eeriSlbaxeB6FIm2VASvDZSl+JA2Pte7tYfVIiVFLu4gC2IuhFNWturruLNytpWd1mkyy0blmywiGxLEbpSc9cU9PK+ura9NiQpIvJDTIyWTEAMaKmW1Uh433+c78TDMD/OzJwzM2fm+biuuc453+/n/T2fz5A358zn+/mRqkKSJEmSJEmSpJnwY7NdAUmSJEmSJEnSwmGntCRJkiRJkiRpxtgpLUmSJEmSJEmaMXZKS5IkSZIkSZJmjJ3SkiRJkiRJkqQZY6e0JEmSJEmSJGnG2CktSZIkSZIkSZoxdkpr0pIcleSOJP+Q5G+TXDjbdZI0viQbk+xK8oMkN812fSR1JsmPJ/mD5vP2u0m+mOTs2a6XpIkl+cMkjyX5+yRfTfL22a6TpM4lWZXk+0n+cLbrImliSf6iydnvNT8Pz3adND47pTUV1wEHgZcCbwauT/Ly2a2SpAk8CnwIuHG2KyJpUhYD+4CfB/4x8AHgtiQrZrFOkjrzH4EVVfVTwDnAh5K8cpbrJKlz1wE7Z7sSkiZlY1X9ZPPzstmujMZnp7QmJckLgV8BPlBV36uqvwK2Ar82uzWTNJ6q+lRV/Snw5GzXRVLnquofquqDVfWNqnq2qv4M+Dpgx5Y0x1XVA1X1g+GXzc8Js1glSR1Kcj7wd8DnZ7sukjRf2SmtyToROFRVX2079iXAkdKSJPVYkpfS+ix+YLbrImliSf5rkqeBrwCPAdtmuUqSJpDkp4DfBX57tusiadL+Y5Inkvx1kl+Y7cpofHZKa7J+EnhqxLGngBfNQl0kSVowkrwA+CPgE1X1ldmuj6SJVdW7aH1PPgP4FPCD8SMkzQG/B/xBVe2b7YpImpT3AT8DHAdsAf7fJM5QmsPslNZkfQ/4qRHHfgr47izURZKkBSHJjwH/ndaeDhtnuTqSJqGqDjVL3g0A75zt+kgaW5KTgdcC18x2XSRNTlXdXVXfraofVNUngL8G1s52vTS2xbNdAfWdrwKLk6yqqt3NsX+O04glSeqJJAH+gNYGw2ur6oezXCVJU7MY15SW5rpfAFYAj7Q+fvlJYFGS1VV1yizWS9LkFZDZroTG5khpTUpV/QOtqYe/m+SFSf5PYB2t0VuS5qgki5McASyi9cX6iCTemJT6w/XAPwPeUFX/e7YrI2liSV6S5PwkP5lkUZLXARcA/99s103SuLbQunl0cvOzGfgM8LrZrJSk8SU5Msnrhv/OTfJm4P8C7prtumlsdkprKt4F/CPgW8AfA++sKkdKS3PbvwP+N3AZ8KvN8383qzWSNKEk/xR4B60/jL+Z5HvNz5tnuWqSxle0luoYAr4DXA38ZlV9elZrJWlcVfV0VX1z+IfW8pXfr6oDs103SeN6AfAh4ADwBPDrwC9X1cOzWiuNK1U123WQJEmSJEmSJC0QjpSWJEmSJEmSJM0YO6UlSZIkSZIkSTPGTmlJkiRJkiRJ0oyxU1qSJEmSJEmSNGM66pROclaSh5PsSXLZKOeT5Nrm/H1JTpkoNskHk+xPcm/zs7Y7TZIkSZIkSZIkzVWLJyqQZBFwHXAmMATsTLK1qh5sK3Y2sKr5WQNcD6zpIPaaqrq608oec8wxtWLFik6LSwvG3/zN3zxRVUtnux5jMXel0Zm7Uv+ay/lr7kpjm8u5C+avNBZzV+pP4+XuhJ3SwGnAnqraC5DkVmAd0N4pvQ64uaoK2J7kyCTHAis6iO3YihUr2LVr11RCpXktyd/Odh3GY+5KozN3pf41l/PX3JXGNpdzF8xfaSzmrtSfxsvdTpbvOA7Y1/Z6qDnWSZmJYjc2y33cmOTFo715kvVJdiXZdeDAgQ6qK0mSJEmSJEmaqzrplM4ox6rDMuPFXg+cAJwMPAb8/mhvXlVbqmqwqgaXLp2zMzUkSZIkSZIkSR3oZPmOIWBZ2+sB4NEOyywZK7aqHh8+mOQG4M86rrUkSZIkSZIkqS910im9E1iV5HhgP3A+cOGIMltpLcVxK62NDp+qqseSHBgrNsmxVfVYE38ucP+0W6N57Yc//CFDQ0N8//vfn+2qzJojjjiCgYEBXvCCF8x2VaSOmbvmrvqTudti/qrfmLst5q76jbnbYu6q35i7LVPJ3Qk7pavqmSQbgbuARcCNVfVAkkub85uBbcBaYA/wNHDJeLHNpa9KcjKt5Ty+Abyj41prQRoaGuJFL3oRK1asIBltZZj5rap48sknGRoa4vjjj5/t6kgdM3fNXfWnhZ67YP6qP5m75q76k7lr7qo/mbtTz91O1pSmqrZV1YlVdUJVfbg5trnpkKZaNjTnf7aqdo0X2xz/tabsK6rqnLZR09Kovv/973P00Ucv2CRPwtFHH73g776p/5i7k8/dJGcleTjJniSXjXI+Sa5tzt+X5JSJYpOcnGR7knubDYRPm3bjNK8t9NwFP3vVn8xdc1f9ydw1d9WfzN2p525HndLSXLGQkxxsv/rXQv+3O5n2J1kEXAecDawGLkiyekSxs4FVzc96WpsHTxR7FfA7VXUycEXzWhrXQs9d8Heg/uS/W38H6k/+u/V3oP7kv9up/Q46WVNaC9Guj08vfvCS7tRjgbn88su5+eab+c53vsP3vve92a7OzJrOvzn/vWmWdTl3TwP2VNVegGa/hnXAg21l1gE3V1UB25McmeRYYMU4sQX8VBP/j3n+psVTM5ncNVc1xyzkz91b7n5kzHMXrlk+gzWRJm8h5y6Mn7+TYa5rppm7U89d81WzqVe5a6e0+la3vowNmwv/k3/DG97Axo0bWbVq1WxXReoZc3dCxwH72l4P0dpEeKIyx00Q+5vAXUmupjVT6v8Y7c2TrKc1+prly2f/d6u5w9yV+pO5K/Unc1fqT+Zu51y+Q+rQBz7wAT760Y8efn355Zdz7bXXdvU9Tj/9dI499tiuXlNa6Powd0eb91Qdlhkv9p3Ab1XVMuC3gD8Y7c2raktVDVbV4NKlSzusstR9fZi7kjB3pX5l7kr9qZ9z15HSUofe9ra38cY3vpF3v/vdPPvss9x6663s2LHjeeXOOOMMvvvd7z7v+NVXX81rX/vamaiqpDZ9mLtDwLK21wM8f6mNscosGSf2YuDdzfPbgY91qb5ST/Rh7pLkLOCjwCLgY1V15Yjzac6vBZ4G3lJV94wXm+RkYDNwBPAM8K6qev4vQpoj+jF3JZm7Ur/q59y1U1rq0IoVKzj66KP54he/yOOPP87P/dzPcfTRRz+v3F/+5V/OQu0kjaUPc3cnsCrJ8cB+4HzgwhFltgIbmzWj1wBPVdVjSQ6ME/so8PPAXwCvBnb3uiHSdPRb7rZtNHomrRtHO5Nsrar29eDbNyldQ2uT0jUTxA5vUvrnSdY2r39hhpolTVq/5a6kFnNX6k/9nLt2SkuT8Pa3v52bbrqJb37zm7z1rW8dtUynd58OHTrEK1/5SgDOOeccfvd3f7fr9Z3miK0bgV8CvlVVJ41y7fcAHwGWVtUTXa+81EX9lLtV9UySjcBdtHL3xqp6IMmlzfnNwDZaebuHVu5eMl5sc+l/A3w0yWLg+zTrRktzWT/lLv22SanUQ32Wu5Ia5q7Un/o1d+2Ulibh3HPP5YorruCHP/wht9xyy6hlOr37tGjRIu69995uVu85pjNiqzl3E7AJuHmUay9rrtvdFfylHumn3AWoqm20Op7bj21ue17Ahk5jm+N/BbyyuzWVeqvPcndWNymV5pI+y11JDXNX6k/9mrtudChNwpIlS3jVq17Fm970JhYtWtT167/3ve9lYGCAp59+moGBAT74wQ9O53KHR2xV1UFgeNRVu8MjtqpqOzA8Youq+gLw7TGufQ3wXp6/+Zo0J/VZ7kpq9FnuzuompUnWJ9mVZNeBAwc6rLLUG32Wu5IaczV377zzToCTkuxJctnI82m5tjl/X5JT2s6dleThkbFJzkvyQJJnkwyOcs3lSb7XzBCW5rS5mrsTcaS0+taFa5bP+Hs+++yzbN++ndtvv70n17/qqqu46qqrunW56YzYemysiyY5B9hfVV9qrf4xZrn1NMsDLF8+8/+tNHeZu1J/MncnNKublFbVFmALwODgoDeNdZi5O7EebVL6SeBlzSWOBP6uqk7uWqU175m7LYcOHWLDhg0AXwUG6d6eDfcDbwT+2xhvfQ3w55OqrIS5OxmOlJY69OCDD7Jy5Upe85rXsGrVqtmuTiemM2Jr9AsmPwFcDlwx0ZtX1ZaqGqyqwaVLl05UXOqZPsxdSfRl7h7epDTJElobjW4dUWYrcFEzout0mk1KJ4gd3qQU3KRUfaDfcret0+psYDVwQZLVI4q1d3itp9XhNW5sVf3rqjq56Yj+H8CnZqA50pTN1dzdsWMHK1euBDg4hRnAY84erqqHqurh0d4zyS8De4EHRjsvzSVzNXc74UhpqUOrV69m7969s12NyZjOiK2xnAAcDwyPkh4A7klyWlV9c9o1lnqgD3NXEv2Xu25SKrX0W+7Su01KaY4FeBOtm0rSnDVXc3f//v0sW9b+J2vX9mwYVZIXAu+jNbrapTs0583V3O2EndLS/HV41BWwn9aoqwtHlNkKbGy+QK/hRyO2RlVVXwZeMvw6yTeAwap6ost1lySp77hJqdSXerVJ6bAzgMerylkO0hS0Pjqff3jE66ns2TCW3wGuqarvjbdcJbhkpTRddkpL89R0RmwBJPlj4BeAY5IMAf++qkbdXEmSJEnqU73apHTYBcAfj1sBO7akMQ0MDLBv377nHKI7ezaMZQ3wr5JcRWs9+GeTfL+qNo0s6H4O0vTYKS3NY9McsXVBB9dfMc0qSpIkSbOpV5uU0iy780YmmO1gx5Y0tlNPPZXdu3cDLGnbd6GjGcBJDjDx7OHnqKozhp8n+SDwvdE6pCVNnxsdSpIkSZIWql5tUgrwWuArVTXU+2ZI89PixYvZtGkTwInAQ8BtwzOAh2cB0xqItZfWDOAbgHdBa/YwMDx7+HAsQJJzmxnB/wL4TJK7ZrBZkrBTWppVX/jCFzjllFNYvHgxf/InfzLb1ZHUIXNX6k/mrtSfepm7Y3VaTbfDq3E+EyzdIc1n3crdtWvXAtxfVSdU1YehNQN4eBZwtWxozv9sVe0ajq2qbVV1Yntsc/yOqhqoqh+vqpdW1etGvm9VfbCqrp5yxaU+NVPfmV2+Q/1r18e7e73BSyYu02XLly/npptu4uqr/ZzTAmLuSv3J3JX6k7k7oV5sUtqce0v3aqkFx9yV+pO52zE7paUOfeADH+CYY47h3e9+NwCXX345L33pS/mN3/iNKV9zxYoVAPzYjzlpQeoVc1fqT+au1J/MXak/mbtSf+rn3LVTWurQ2972Nt74xjfy7ne/m2effZZbb72VHTt2PK/cGWecwXe/+93nHb/66qt57WtfOxNVldTG3JX6k7kr9SdzV+pP5q7Un/o5d+2Uljq0YsUKjj76aL74xS/y+OOP83M/93McffTRzyv3l3/5l7NQO0ljMXel/mTuSv3J3JX6k7kr9ad+zl07paVJePvb385NN93EN7/5Td761reOWmYu3n2SFrp+y90kZwEfBRYBH6uqK0ecT3N+LfA08Jaqume82CSfBF7WXOJI4O+q6uQZaI40Zf2Wu5JazF2pP5m7Un/q19y1U1qahHPPPZcrrriCH/7wh9xyyy2jlpmLd5+kha6fcjfJIuA64ExgCNiZZGtVPdhW7GxgVfOzBrgeWDNebFX967b3+H3gqRlpkDQN/ZS7kn7E3JX6k7kr9ad+zV1Xm5cmYcmSJbzqVa/iTW96E4sWLZr29Xbu3MnAwAC3334773jHO3j5y1/ehVpKGqnPcvc0YE9V7a2qg8CtwLoRZdYBN1fLduDIJMd2EtuMsn4T8MfdrLTUC32Wu5Ia5q7Un8xdqT/1a+46Ulr9a/CSGX/LZ599lu3bt3P77bd35XqnnnoqQ0NDXbmW1DfM3YkcB+xrez1EazT0RGWO6zD2DODxqtrdldpq4TB3pf5k7kr9ydyV+pO52zFHSksdevDBB1m5ciWvec1rWLVq1WxXR1KH+jB3M8qx6rBMJ7EXMM4o6STrk+xKsuvAgQPjVlTqpT7MXUmYu1K/Mnel/tTPuetIaalDq1evZu/evbNdDUmT1Ie5OwQsa3s9ADzaYZkl48UmWQy8EXjlWG9eVVuALQCDg4MjO7SlGdOHuesmpRL9mbuSzF2pX/Vz7jpSWpKkuWUnsCrJ8UmWAOcDW0eU2QpclJbTgaeq6rEOYl8LfKWqnEcpdVnbRqNnA6uBC5KsHlGsfZPS9bQ2KR03tqr+dVWd3HRE/w/gUzPQHEmSJKmnOuqUTnJWkoeT7Ely2Sjnk+Ta5vx9SU6ZROx7klSSY6bXFC0EVQt70N5Cb7/610L/tzuZ9lfVM8BG4C7gIeC2qnogyaVJLm2KbQP2AnuAG4B3jRfbdvnzcYNDTcJCz12Y1O/ATUo1Z5i7/g7Un/x36+9A/cl/t1P7HUy4fEfbyI0zaU0X3plka1U92FasfdTHGlqjPtZMFJtkWXPukUnXXAvOEUccwZNPPsnRRx9N6++yhaWqePLJJzniiCNmuyrSpJi7k8/dqtpGq+O5/djmtucFbOg0tu3cWzquRIfu/vq3Oy77tUNjf9xfuGZ5N6qjLlrouQuTzt9Z3aQ0yXpao69Zvtx8WsjMXb83qz+Zu+au+pO5O/Xc7WRN6cMjNwCSDI/caO+UPjzqA9ieZHjUx4oJYq8B3gt8elK11oI0MDDA0NAQC3njrSOOOIKBgYGOy09zbcsbgV8CvlVVJ7XFfAR4A3AQ+BpwSVX93XTapfnN3J187kpzgbnbMon8ndVNSl0PXsPM3RY/e9VvzN0Wc1f9xtxtmUrudtIp3ZNRH0nOAfZX1ZfGu5PgqA8Ne8ELXsDxxx8/29XoG9OZ5dCcuwnYBNw84tKfA95fVc8k+U/A+4H39aod6n/mrtSfzN1Jm9VNSqVh5q7Un8xdqT+Zu1PXyZrSXR/1keQngMuBKyZ686raUlWDVTW4dOnSCSsr6bDprG1JVX0BeN68/Kr6bLNuLcB2Wn84S5K00LlJqSRJPXDnnXcCnNTNfc6SnJfkgSTPJhlsO35mkr9J8uXm8dW9bp+0UHUyUroXoz5OAI4HhkdJDwD3JDmtqr45mQZIGtN0Zjk81uF7vBX45GgnnOUgSVpImhlEwxuNLgJuHN6ktDm/mdZ672tpbVL6NHDJeLFtl3eTUqmHprnk3ZixSX6d1gbEzwCfqar3dqO+JzxyezcuA2t+uzvXkXro0KFDbNiwAeCrwCDd2+fsflqzkP7biLd8AnhDVT2a5CRan83H9ayB0gLWSaf04ZEbwH5aX4ovHFFmK7CxWTN6Dc2ojyQHRottvmS/ZDg4yTeAwap6YroNknTYdGY5THzx5HJaX7D/aLTzrm0pSVpo+mmTUkkt01nybrzYJK+iNSvxFVX1gyQvQdKk7dixg5UrV7J3796DVXWwW/ucVdVDzbHnvF9VfbHt5QPAEUl+vKp+0KMmSgvWhJ3SPR71Ial3pjPLYVxJLqa1CeJrmg9+SZIkqR8dXvIOnttp1VZm0h1ewDuBK4c7sqrqWzPUHmle2b9/P8uWtf/J2p19zjr0K8AX7ZCWeqOTkdI9G/XRVmZFJ/WQNClTnuUw3kWbKYrvA36+qp7ufrUlSZKkGTOdJe/Giz0ROCPJh4HvA++pqp2jVcBl76SxjTEGalr7nHXyvkleDvwn4BfHKWPuStPQyUaHkvpQsxnh8EyFh4Dbhmc5DM90oHXDaC+tWQ43AO8ajk/yx8D/Al6WZCjJ25pTm4AXAZ9Lcm+SwzeoJEmSpD4znSXvxotdDLwYOB34t8BtGblOwHBA1ZaqGqyqwaVLl3ZWa2mBGBgYYN++fc85ROczgKc6M3gAuAO4qKq+NlY5c1eano5GSkvqT9Oc5XDBGMdXdrOOkiRJ0iyazpJ3S8aJHQI+1Xzf3pHkWeAY4ED3qi7Nf6eeeiq7d+8GWJJkCV3Y52y890tyJPAZ4P1V9dfdbY2kdo6UliRJkiQtVIeXvGvr8No6osxW4KK0nM6PlrwbL/ZPgVcDJDmRVgf2E71vjjS/LF68mE2bNkFrSZxJzQAea/YwQJJzkwwB/wL4TJK7mmttBFYCH2hmBt/rRqVSbzhSWpIkSZK0IFXVM0mGO60WATcOd3g15zfT6vBaS6vD62ngkvFim0vfCNyY5H7gIHCxG4RLU7N27VqA+6tqcPjYdPc5q6o7aC3RMfL4h4APTb/WkiZip7QkSZIkacGa5pJ3Y3V4HQR+tbs1lSRp/nD5DkmSJEmSJEnSjLFTWpKkOSbJWUkeTrInyWWjnE+Sa5vz9yU5pZPYJL/enHsgyVUz0RZJkiRJkkZy+Q5JkuaQJIuA64AzgSFgZ5KtVfVgW7GzgVXNzxrgemDNeLFJXgWsA15RVT9wwxZJkiRJ0mxxpLQkSXPLacCeqtrbrEd5K63O5HbrgJurZTtwZJJjJ4h9J3BlVf0AoKq+NRONkSRJkiRpJDulJUmaW44D9rW9HmqOdVJmvNgTgTOS3J3kfyY5tau1liRJkiSpQ3ZKS5I0t2SUY9VhmfFiFwMvBk4H/i1wW5LnlU+yPsmuJLsOHDjQea0luR68JEmS1CE7pSVJmluGgGVtrweARzssM17sEPCpZsmPHcCzwDEj37yqtlTVYFUNLl26dFoNkRaStjXdzwZWAxckWT2iWPt68OtprQc/buyI9eBfDlzd+9ZIkiRJvWWntCRJc8tOYFWS45MsAc4Hto4osxW4qBl1eTrwVFU9NkHsnwKvBkhyIrAEeKL3zZEWDNeDlyRJkjpkp7QkSXNIVT0DbATuAh4CbquqB5JcmuTSptg2YC+wB7gBeNd4sU3MjcDPJLmfVofXxVU1clkQSVPnevCSJElShxbPdgUkSdJzVdU2Wh3P7cc2tz0vYEOnsc3xg8CvdremktrMxHrwp9JaD/5nRt5USrKe1pIgLF++fBLVliRJkmaeI6UlSZKk6XM9eEmSJKlDdkpLkiRJ0+d68JIkSVKH7JSW5rEkZyV5OMmeJJeNcj5Jrm3O35fklLZzNyb5VrP+bHvMUUk+l2R38/jimWiLJElzmevBS5IkSZ1zTWlpnkqyCLgOOJPW1N+dSbZW1YNtxc4GVjU/a4Drm0eAm4BNwM0jLn0Z8PmqurLp6L4MeF+v2iFJUr9wPXhJkjSWEx65ferBi4760fPBS6ZfGWkOcKS0NH+dBuypqr3NH7S3AutGlFkH3NysU7kdODLJsQBV9QXg26Ncdx3wieb5J4Bf7kntJUmSJEkL3p133glw0hRnAI86ezjJeUkeSPJsksER13t/U/7hJK/rZdukhcxOaWn+Og7Y1/Z6qDk22TIjvbRZ/5Lm8SXTrKckSZI0a6a55N1YHV4fTLI/yb3Nz9qZao80nxw6dIgNGzYAfBVYDVyQZPWIYu0zgNfTmgHcPnv47FFi7wfeCHyh/ULN+fOBlwNnAf+1uY6kLrNTWpq/MsqxkWtQdlJmam+erE+yK8muAwcOdOOSkiRJUldN0Gk1bCodXgDXVNXJzc/zlueRNLEdO3awcuVKgINTmAE85uzhqnqoqh4e5S3XAbdW1Q+q6uu09oE4rSeNkxY4O6Wl+WsIWNb2egB4dAplRnp8eImP5vFboxWqqi1VNVhVg0uXLp1UxSVJkqQZMp0l7zqJlTQN+/fvZ9my9j9ZJzUDeCozg6cSI2kK7JSW5q+dwKokxydZQmsK0tYRZbYCFzVTEk8HnhpemmMcW4GLm+cXA5/uZqUlSZKkGTSdJe8mit3YLPdxY5IXj1UBZxhKY2vtEfz8wyNejzUDeCozgzuOMXel6bFTWpqnquoZYCNwF/AQcFtVPZDk0iSXNsW2AXtpTUm6AXjXcHySPwb+F/CyJENJ3tacuhI4M8lu4MzmtSRJktSPprPk3Xix1wMnACcDjwG/P1YFnGEojW1gYIB9+/Y95xCdzwCeyszgjmPMXWl6Fs92BST1TrN23bYRxza3PS9gwxixF4xx/EngNV2spiRJkjRbprPk3ZKxYqvq8eGDSW4A/qx7VZYWjlNPPZXdu3cDLGmbAXzhiGJbac1MuBVYQzMDOMkBmtnDwP4xYkfaCtyS5D8DP01rLfkdXWuQpMMcKS1JkiRJWqims+TdmLHDe7A0zgXu73VDpPlo8eLFbNq0CeBEJjkDeKzZwwBJzk0yBPwL4DNJ7mpiHgBuAx4E7gQ2VNWhGWmstMA4UlqSJEmStCBV1TNJhjutFgE3Dnd4Nec30+rwWkurw+tp4JLxYptLX5XkZFrLeXwDeMfMtUqaX9auXQtwf1UNDh+bxAzg580ebo7fAdwxRsyHgQ9Pr9aSJtLRSOkkZyV5OMmeJJeNcj5Jrm3O35fklIlik/xeU/beJJ9N8tPdaZIkSf2tR5+7H0yyv/ncvTfJ2plqjyRJc1lVbauqE6vqhKYziqraPNzpVS0bmvM/W1W7xottjv9aU/YVVXVOB5uJS5K0oEzYKZ1kEXAdcDawGrggyeoRxc6mtc7OKmA9rU0dJor9SPMBfTKt9bWumH5zJEnqbz383AW4pqpObn6eN2JEkiRJkqSZ0MlI6dOAPVW1t6oOArcC60aUWQfc3NxB3g4c2ayhNWZsVf19W/wLef4Ox5IkLUQ9+dyVJEmSJGmu6KRT+jhgX9vroeZYJ2XGjU3y4ST7gDczxkjpJOuT7Eqy68CBAx1UV5Kkvtazz11au5Lfl+TGJC/uXpUlSZIkSepcJ53SGeXYyFHNY5UZN7aqLq+qZcAf0doR9fmFq7ZU1WBVDS5durSD6kqS1Nd69bl7PXACcDLwGPD7o765N4OlKXM9eEmSJKkznXRKDwHL2l4PAI92WKaTWIBbgF/poC6SJM13PfncrarHq+pQVT0L3CCC2ycAACAASURBVEBrqY/n8WawNDWuBy9JkiR1rpNO6Z3AqiTHJ1kCnA9sHVFmK3BRM/rjdOCpZnfhMWOTrGqLPwf4yjTbIknSfNCrz91j2+LPBe7vdUOkBcb14CVJkqQOLZ6oQFU9k2QjcBewCLixqh5IcmlzfjOwDVgL7AGeBi4ZL7a59JVJXgY8C/wtcGlXWyZJUh/q4efuVUlOprWcxzeAd8xcq6QFYbQ13dd0UGas9eDbYzcmuQjYBfx2VX1n5JsnWU9r9DXLly+fYhMkSZKkmTFhpzRAM01w24hjm9ueF7Ch09jmuMt1SJI0ih597v5al6sp6bl6uR787zWvf4/WevBvfV7hqi3AFoDBwcGR7ytJkiTNKR11SktSr9399W9POfZrhx45/PzCNY4OkyTNiumsB79krNiqenz4YJIbgD/rXpUlSZKk2dHJmtKSJEmSxud68JIkSVKHHCktSZIkTZPrwUuSJEmds1NakiTNiBMeuX3sk4uOeu7rwUt6WxmpB1wPXpIkSeqMy3dIkiRJkiRJkmaMndLSPJbkrCQPJ9mT5LJRzifJtc35+5KcMlFskpOTbE9yb5JdSU6bqfZIkiRJkiSp/9kpLc1TSRYB1wFnA6uBC5KsHlHsbGBV87MeuL6D2KuA36mqk4ErmteSJEmSJHXdnXfeCXBSlwdbHZXkc0l2N48vbo6/IMknknw5yUNJ3j8TbZQWIjulpfnrNGBPVe2tqoPArcC6EWXWATdXy3bgyCTHThBbwE81z/8x8GivGyJJkiT1Si9mF7adf0+SSnJMr9shzUeHDh1iw4YNAF+lu4OtLgM+X1WrgM83rwHOA368qn4WeCXwjiQretI4aYGzU1qav44D9rW9HmqOdVJmvNjfBD6SZB9wNTDqneMk65vlPXYdOHBgyo2QJEmSeqWHswtJsgw4E3ikx82Q5q0dO3awcuVKgINdHmy1DvhE8/wTwC83zwt4YZLFwD8CDgJ/35vWSQubndLS/JVRjlWHZcaLfSfwW1W1DPgt4A9Ge/Oq2lJVg1U1uHTp0g6rLEmSJM2oXs0uBLgGeC/P/w4uqUP79+9n2bJl7Ye6NdjqpVX1GEDz+JLm+J8A/wA8RuuG0tVV9e3R6uZALGl67JSW5q8hoP3Te4DnL7UxVpnxYi8GPtU8v53Wl3FJkiSpH/VkdmGSc4D9VfWliSpgx5Y0tqpR7+l0Y7DVWE4DDgE/DRwP/HaSnxmjbg7EkqbBTmlp/toJrEpyfJIlwPnA1hFltgIXNevknQ481dwlHi/2UeDnm+evBnb3uiGSJElSj3R9dmGSnwAup7Up+ITs2JLGNjAwwL59+55ziO4Mtnq8mfFA8/it5viFwJ1V9cOq+hbw18BgF5oiaQQ7paV5qqqeATYCdwEPAbdV1QNJLk1yaVNsG7AX2APcALxrvNgm5t8Av5/kS8B/oLWuniRJktSPejG78ARaIyy/lOQbzfF7kvyTrtZcWgBOPfVUdu/eDbCky4OtttKaBUzz+Onm+SPAq5trvRA4HfhKj5onLWh2SkvzWFVtq6oTq+qEqvpwc2xzVW1unldVbWjO/2xV7Rovtjn+V1X1yqr651W1pqr+ZuZbJs1vSc5K8nCSPUkuG+V8klzbnL8vySmTiH1PkkpyTK/bIUlSH+j67MKq+nJVvaSqVlTVClqd16dU1TdnrFXSPLF48WI2bdoEcCLdHWx1JXBmkt20NiS9sjl+HfCTwP20cvzjVXVfb1spLUyLZ7sCkiTpR5IsovVl+Exaf8TuTLK1qh5sK3Y2sKr5WQNcD6yZKDbJsubcIzPVHkmS5rKqeibJcKfVIuDG4Q6v5vxmWh1ea2l1eD0NXDJe7Cw0Q5rX1q5dC3B/VR1eRmN4oFXzvIANo8VW1TZaOTzy+JPAa0Y5/j3gvOnXWtJE7JSWJGluOQ3YU1V7AZLcCqwD2jul1wE3N1/Atyc5slkLb8UEsdcA7+VH0xMlSVrwRuu0mm6H14gyK6ZfS0mS5hc7pSVJmluOA9p3cxmiNRp6ojLHjReb5Bxgf1V9KRltXyZJ05XkLOCjtEZMfqyqrhxxPs35tbRGW76lqu7pMPY9wEeApVX1RK/bIknShHZ9vDvXGbykO9eR1FdcU1qSpLlltB7j6rDMqMeT/ARwOXDFhG+erE+yK8muAwcOTFhZSS1ty+ecDawGLkiyekSx9qV31tNaemfCWJfekSRJ0nxjp7QkSXPLELCs7fUA8GiHZcY6fgJwPPClJN9ojt+T5J+MfPOq2lJVg1U1uHTp0mk2RVpQDi+9U1UHgeHlc9odXnqnqrYDw0vvTBQ7vPTOyBtUkiRJUl+yU1qSpLllJ7AqyfFJlgDnA1tHlNkKXJSW04GnquqxsWKr6stV9ZKqWtGsazkEnFJV35yxVknz31jL6nRSZszY9qV3xntzZzlIkiSpn7imtCRJc0hVPZNkI3AXrbVlb6yqB5Jc2pzfTGtDpbXAHlrr0l4yXuwsNENaiHq59M4vTvTmVbUF2AIwODjoiGpJkiTNaXZKS5I0x1TVNlodz+3HNrc9L2BDp7GjlFkx/VpKGmE6S+8sGeN4+9I7w8fvSXKaMx0kSZLUz1y+Q5IkSZo+l96RJEmSOuRIaUmSJGmaXHpHkiRJ6pyd0pIkSVIXuPSOJEmS1BmX75AkSZIkSZIkzRg7pSVJkiRJkiRJM8ZOaUmSJEmSJEnSjOmoUzrJWUkeTrInyWWjnE+Sa5vz9yU5ZaLYJB9J8pWm/B1JjuxOkyRJkiRJkiRJc9WEndJJFgHXAWcDq4ELkqweUexsYFXzsx64voPYzwEnVdUrgK8C7592ayQ9Ry9uKDXnfr0590CSq2aiLZIkSZIkSZofOhkpfRqwp6r2VtVB4FZg3Ygy64Cbq2U7cGSSY8eLrarPVtUzTfx2YKAL7ZHU6NUNpSSvopXHr6iqlwNX9741kiRJUm/0aGbw7zVl703y2SQ/PVPtkeabO++8E+CkLufoUUk+l2R38/jitnOvSPK/mkFYX05yRK/bKC1Eizsocxywr+31ELCmgzLHdRgL8Fbgk6O9eZL1tDrLWL58eQfVldQ4fFMIIMnwTaEH28ocvqEEbE8yfENpxTix7wSurKofAFTVt2aoPZIkqUtOeOT2sU8uOmpyFxu8ZHqVkWZR22CMM2n9vbozydaqav/O3D6QYw2tgRxrJoj9SFV9oHmP3wCuAC6doWZJ88ahQ4fYsGEDtGbYD9K9HL0M+HxVXdl0Vl8GvC/JYuAPgV+rqi8lORr44Yw0VlpgOumUzijHqsMyE8YmuRx4Bvij0d68qrYAWwAGBwdHvq9m0d1f//aY57526JGOr3PhGm829EivbiidCJyR5MPA94H3VNXOkW/uDSVJkiT1gZ4M5Kiqv2+LfyHP/xtaUgd27NjBypUr2bt378GqOtjFwVbrgF9o4j8B/AXwPuAXgfuq6ksAVfVkj5soLVidLN8xBCxrez0APNphmXFjk1wM/BLw5uZ/HpK6p1c3lBYDLwZOB/4tcFuS55Wvqi1VNVhVg0uXLu281pIkSdLMGWuQRidlxo1N8uEk+4A30xopLWmS9u/fz7Jl7d1KXcvRl1bVYwDN40ua4ycCleSuJPckee9YdUuyPsmuJLsOHDgwyZZJ6qRTeiewKsnxSZYA5wNbR5TZClzUrONzOvBUk9RjxiY5i9ZdqHOq6ukutUfSj/TqhtIQ8KlmDfkdwLPAMV2styRJkjRTejYzuKour6pltGYFbxyzAnZsSWMaY/xi12bvj2Ix8C9p3Uz6l8C5SV4zRt0ciCVNw4Sd0s1mhBuBu4CHgNuq6oEklyYZXhNrG7AX2APcALxrvNgmZhPwIuBzzeYPm7vXLEn06IYS8KfAqwGSnAgsAZ7ofXOkhcMNlyRJmjE9mxnc5hbgV8aqgB1b0tgGBgbYt2/fcw7RnRx9vFnig+ZxeK+kIeB/VtUTzQDKbcApSOq6TtaUpqq20UrE9mOb254XsKHT2Ob4yknVVNKkVNUzSYZvCi0Cbhy+odSc30wrN9fSuqH0NHDJeLHNpW8EbkxyP3AQuNjld6TuccMlSZJm1OHBGMB+WoMxLhxRZiuwsVmPdg3NQI4kB8aKTbKqqnY38ecAX+l9U6T559RTT2X37t0AS9oGTE07R5uYi4Erm8dPN8fvAt6b5Cdo/b3788A1vWqftJB11CktqT/16IbSQeBXu1vT7rnl7s432RyPG3BqFrnhktSnmuXpPkrrhu7HqurKEefTnF9L62bwW6rqnvFik/werTx+ltYorrdU1WgjMSVNQQ8HclyZ5GW0cvdv8UawNCWLFy9m06ZNvP71rz+R1gz8ruUorf2R3gY8ApzXxHwnyX+mdcOqgG1V9ZkZaq60oNgpLUnS3DLahixrOigz1mYuh2OTfBi4CHgKeFX3qizJWQ5S/+rRQI4xl+uQNDlr164FuL+qBoePdSFHnwTGWiv6D4E/nF6tJU2kk40OJUnSzJnVDZfcbEmassOzHJpZRcMzFdodnuVQVduB4VkOY8Y6y0GSJEnzkZ3SkiTNLbO64ZKbLUlTNtYMhk7KjBub5MNJ9gFvpjVSWpIkSeprdkpLkjS3HN5wqW0zl60jymwFLkrL6TSbuYwXm2RVW7wbLknd5ywHSZIkqUOuKS1J0hzihktS35rOLIclHcRCa5bDZ4B/P/JEVW0BtgAMDg66xIckSZLmNDulJUmaY9xwSepLh2cqAPtpzVS4cESZrcDGJLfS2ujwqap6LMmBsWKTrKqq3U28sxwkSZI0L9gpLUmSJE2TsxwkSZKkztkpLUmSJHWBsxwkSZKkzrjRoSRJkiRJkiRpxjhSWpIkSZIkSZNy99e/3ZXrrBnsymUk9RlHSkuSJEmSJEmSZowjpReYW+5+pKNyJzzSnTuekiRJkiRJktTOkdKSJEmSJEmSpBljp7QkSZIkSZIkacbYKS1JkiRJkiRJmjF2SkvzWJKzkjycZE+Sy0Y5nyTXNufvS3LKJGLfk6SSHNPrdkiSJEmSJGn+cKNDaZ5Ksgi4DjgTGAJ2JtlaVQ+2FTsbWNX8rAGuB9ZMFJtkWXOus50zJUmSpDkqyVnAR4FFwMeq6soR59OcXws8Dbylqu4ZLzbJR4A3AAeBrwGXVNXfzUyLpPnlzjvvBDgpyR66l6NHAZ8EVgDfAN5UVd9pu+Zy4EHgg1V1dS/b12u33D29P9svXLO8SzWRnsuR0tL8dRqwp6r2VtVB4FZg3Ygy64Cbq2U7cGSSYzuIvQZ4L1A9b4UkSZLUI22DMc4GVgMXJFk9olj7QI71tAZyTBT7OeCkqnoF8FXg/T1uijQvHTp0iA0bNkArj7qZo5cBn6+qVcDnm9ftrgH+vOsNknSYndLS/HUcsK/t9VBzrJMyY8YmOQfYX1VfGu/Nk6xPsivJrgMHDkytBZIkSVJv9WQgR1V9tqqeaeK3AwMz0RhpvtmxYwcrV64EONjlwVbrgE80zz8B/PLwxZL8MrAXeKBHzZKEndLSfJZRjo0c2TxWmVGPJ/kJ4HLgionevKq2VNVgVQ0uXbp0wspKkiRJs6AnAzlGeCuOuJSmZP/+/Sxbtqz9ULdy9KVV9RhA8/gSgCQvBN4H/M5EdXMgljQ9dkpL89cQ0P7pPQA82mGZsY6fABwPfCnJN5rj9yT5J12tubTA9WKT0iQfSfKVpvwdSY6cqfZIkjSHdX0gx3MCk8uBZ4A/GrMCdmxJY6oadcXIruXoKH4HuKaqvtdB3RyIJU2DndLS/LUTWJXk+CRLgPOBrSPKbAUuajq4Tgeeau4SjxpbVV+uqpdU1YqqWkGr8/qUqvrmjLVKmudc21LqX95QkvpSLwZyAJDkYuCXgDfXGD1rYMeWNJ6BgQH27dv3nEN0J0cfb5b4oHn8VnN8DXBVMwjrN4H/O8nG6bdE0kh2SkvzVLOG3UbgLuAh4LaqeiDJpUkubYpto7VW1h7gBuBd48XOcBOkhcq1LaU+5A0lqW91fSAHtG400VoC4JyqenqmGiPNN6eeeiq7d+8GWNLNHG0eL26eXwx8GqCqzmgbhPX/AP+hqjb1roXSwrV4tisgqXeqahutjuf2Y5vbnhewodPYUcqsmH4tJY0w2tp3azooM9a6eSNjobW25SdHe/Mk62l1lrF8+fLJ1Fta6A7fFAJIMnxT6MG2ModvKAHbkwzfUFoxVmxVfbYtfjvwr3reEmkBqapnmlGQdwGLgBuHB3I05zfT+k68ltZAjqeBS8aLbS69Cfhx4HNJALZX1aVImpTFixezadMmXv/6159Ia8BUt3L0SuC2JG8DHgHOm8l2SbJTWpKkuWZW17asqi3AFoDBwcGJ1tyT9CPeUJL6VC8GclTVyi5XU1qw1q5dC3B/VQ0OH+tCjj4JvGa8962qD06txpI64fIdkiTNLbO+tqWkKZn1G0quSStJkqR+Yae0JElzi2tbSv3JG0qSJElShzrqlO7RTuLnJXkgybNJBkdeU5KkhaiHm5RuAl5Ea23Le5McnvIoqSu8oSRJkiR1aMI1pdt2Az+T1iiOnUm2VlX7pi3tO4mvobWT+JoJYu8H3gj8ty62R5KkvufallL/cbM0SZIkqXOdbHTYq53EH2qOdastkiRJ0qzxhpIkSZLUmU6W7xhrl/BOynQSO64k65PsSrLrwIEDkwmVJEmSJEmSJM0xnYyU7ulO4hOpqi3AFoDBwcEJY2+5+5HJXH5MF65Z3pXrSJIkSZIkSZJ+pJNO6ensJL6kg1hJkiRJkiRJ0gLRSaf04d3Agf20dgO/cESZrcDGZs3oNTQ7iSc50EGsJEmSJEmSpBlywiO3d1Zw0VHjnx+8ZPqV0YI0Yad0r3YST3Iu8F+ApcBnktxbVa/rdgMlSZIkSZIkSXNHJyOle7WT+B3AHZOprCRJkiRJkiSpv3XUKS1Jc1nH045G8bXl53WxJpIkSZIkSZrIj812BSRJkiRJkiRJC4ed0pIkSZIkSZKkGWOntDSPJTkrycNJ9iS5bJTzSXJtc/6+JKdMFJvkI0m+0pS/I8mRM9UeSZIkSZIk9T87paV5Kski4DrgbGA1cEGS1SOKnQ2san7WA9d3EPs54KSqegXwVeD9PW6KJEmS1DM9GshxXpIHkjybZHCm2iLNR3feeSfASV3O0aOSfC7J7ubxxc3xM5P8TZIvN4+vnok2SguRndLS/HUasKeq9lbVQeBWYN2IMuuAm6tlO3BkkmPHi62qz1bVM038dmBgJhojSZIkdVsPB3LcD7wR+EKv2yDNZ4cOHWLDhg3QGhDVzRy9DPh8Va0CPt+8BngCeENV/SxwMfDfe9Q0acFbPNsVkNQzxwH72l4PAWs6KHNch7EAbwU+Oe2aSnqOJGcBHwUWAR+rqitHnE9zfi3wNPCWqrpnvNgk5wEfBP4ZcFpV7ZqZ1kgLh7kr9aXDgzEAkgwPxniwrczhgRzA9iTDAzlWjBVbVQ81x2asIdJ8tGPHDlauXMnevXsPVtXBbuVo8/gLTfwngL8A3ldVX2y77gPAEUl+vKp+0LNGTtauj0+q+AmPfLtHFZGmx5HS0vw12jfg6rDMhLFJLgeeAf5o1DdP1ifZlWTXgQMHOqiuJHDEltSvzF2pb401SKOTMp3ETsjvzdLY9u/fz7Jly9oPdStHX1pVjwE0jy8Z5e1/BfjiWB3S5q40PXZKS/PXEND+6T0APNphmXFjk1wM/BLw5uZu9PNU1ZaqGqyqwaVLl065EdIC1Kuldx6qqodnrhnSgmPuSv2ppwM5OuH3ZmlsY/25OeJ113M0ycuB/wS8Y5y6mbvSNNgpLc1fO4FVSY5PsgQ4H9g6osxW4KJmY4jTgaeau8RjxjbTi98HnFNVT89UY6QFZFZHbDniQ5qyWR9tKWlKejaQQ9L0DQwMsG/fvuccojs5+nhzY5jm8VvDhZIMAHcAF1XV17rQDEmjsFNamqeazQg3AncBDwG3VdUDSS5NcmlTbBuwF9gD3AC8a7zYJmYT8CLgc0nuTbJ5ptokLRCzOmLLER/SlM1q7npDSZqyngzkkNQdp556Krt37wZY0uUc3UprI0Oax08DJDkS+Azw/qr66x42TVrw3OhQmseqahutjuf2Y5vbnhewodPY5vjKLldT0nNNZ8TWkg5iJfXGrOZuVW0BtgAMDg5OevmAdnd/fewNkdYcf9R0Li3NOVX1TJLhwRiLgBuHB3I05zfT+k68ltZAjqeBS8aLBUhyLvBfgKXAZ5LcW1Wvm9nWSf1v8eLFbNq0ide//vUn0how1ZUcBa4EbkvyNuAR4Lzm+EZgJfCBJB9ojv1iVR0eSS2pO+yUliRpbjk8ogPYT2tEx4UjymwFNjY7iK+hGQ2S5EAHsZJ6w9yV+lSPBnLcQWv6v6RpWrt2LcD9VTU4fKwLOfok8JpRjn8I+ND0ay1pInZKS5I0hzhiS+pP5q4kSZLUOTulJUmaYxyxJfUnc1eSJEnqjBsdSpIkSZIkSZJmjJ3SkiRJkiRJkqQZY6e0JEmSJEmSJGnGuKa0JEmadXd//dvPef21Q49M+VoXrlk+3epIkiRJknrITmlJkiRJkiRJk7fr4925zuAl3bmO+obLd0iSJEmSJEmSZoyd0pIkSZIkSZKkGWOntCRJkiRJkiRpxtgpLUmSJEmSJEmaMXZKS5IkSZIkSZJmzOLZrkC3nfDI7dOK/9ry87pUE0mSJEmSJEnSSPOuU1qSJEmSJEnS9N399W9POXbN8Ud1sSaab+yUliRJkiRJkjR7dn28O9cZvKQ711HPddQpneQs4KPAIuBjVXXliPNpzq8FngbeUlX3jBeb5Cjgk8AK4BvAm6rqO9NvkvrNLXc/0pXrXLhmeVeuM5+YuxMbc8mfRR3c0fXDTj1i7kr9aSHk7mijpb52qPVdzu9i6lcLIXelfnbnnXcCnJRkDzOQo0neD7wNOAT8RlXd1eMmSgvShJ3SSRYB1wFnAkPAziRbq+rBtmJnA6uanzXA9cCaCWIvAz5fVVcmuax5/b7uNU2zybW9Z5+5Oz2dTFEa/iN8Iv6Rrskwd6X+ZO5K/cnclea2Q4cOsWHDBoCvwv/P3v1H21XV995/f5oQuVYtCrGlCWkiBlvo9QeeEO5zL1WLtsC1ptjWgm1RxEGp0Mfee3srXqr1qY9jqNgfeP2RQS0iT6sR1GqGpdXW3hZ72xgiAgqIDcHCCQgRe9EWBRO+zx9rHdwczo99cvbZP85+v8bYI3vNNefa37n3mplnz73WnEywxG00ybHAGcBxwA8Df53kmKo60I/6SuOkmyulTwB2V9UegCTbgC1A538AW4ArqqqAHUkOS3IkzS9Os5XdAjy/Lf8B4G+xk5Z6yba7xLr+8WWmq669ylqzs+3Sffua6UdM78DRgIxt251qr7M1va7nk7Rv1GCMbduVRsHOnTt5+tOfzp49ex6qqof60Ea3ANuq6kHg9vbq7BOAf1zami5PA5mP2mlARkY3g9JrgDs7tidpfnmaL8+aecr+YFXdDVBVdyd56kwvnuRc4Nx281+T3DpPvEcAX58nzxx+E4BfOvgDdGORMfbNAOP8zW4zPhLjEn9mi9GP9/FHZkgbs7Y7lOao06v6GkiPjNln1Bej3nYH/f7R9hdLEsdB9CtD8H4AwxHHMMQASxvH9PY7Sm0Xhucz6tDXvnEI699X41z/oWq7MMi+t+vvXHMZtnPJeOY2pPHMeS4+GXgS32u7S91G1wA7ZjjWYxxE3zsow/a590MP6jxS39mH+TOe6Tsv0N2gdGZIqy7zdFN2TlV1KXBpt/mT7KqqiYW8Rr+NQowwGnEa49wvPUOabbePlludllt9YGjrNDJtd1jeP+MYvjiGIYYBxDEybReG5zMaFOs/3vWfZqBtF0az751iPHMznrl1E0+SXwB+uqpe3ZG8lG206zIL7XsHZdg+934YtzqPan2/r4s8k8BRHdtrgbu6zDNX2Xva2ylo/723+7AldcG2K40m2640mmy70miy7UrDrd9ttJvXk9QD3QxKXwtsTLIhySqaCd+3T8uzHTgrjROB+9vbIOYqux14Rfv8FcAnFlkXSY9m25VGk21XGk22XWk02Xal4dbvNrodOCPJ45JsoFk8cedSVU4aZ/NO31FV+5NcAHwKWAFcVlU3JTmv3b8VuBo4DdgNPACcPVfZ9tBvBa5Mcg5wB/DYlYoOztDfOsFoxAijEacxzsK2OxSWW52WW31gCOs0Ym13WN4/43i0YYhjGGKAPsYxYm0XhuczGhTrL8C22wPGMzfjmdu88fS7jbbHvpJmMcT9wPlVdaBXFR6QYfvc+2Hc6jyS9U2zOKkkSZIkSZIkSUuvm+k7JEmSJEmSJEnqCQelJUmSJEmSJEl9M1KD0kl+IclNSR5OMjFt3+uT7E5ya5Kf7kh/bpIvtvvemSRt+uOSfLhN/1yS9UsU85uS7E1yffs47WBj7pckp7Qx7U5yYT9fe4ZYvtq+F9cn2dWmPSXJXyX5p/bfJ3fkn/E9XYK4Lktyb5IvdaQtOK5Bf9bDYpjOuW4lOSrJ/0pyS/v/0mvb9IGfn4uRZEWSLyT5ZLs96vU5LMlHkny5/az+w6jXaVj0q93O0dYW3L/2IJaB90lJntFR5+uTfDPJb/Tj/RiGvm+WGC5u2/iNSf4syWFt+vok3+54T7b2IoblYBT73W4Mwzk6KHP8XzkW9V9OMsTfeZM8O8mOqX4wyQkHG1uvJPn19jVvSvL2QcfTvsZvJqkkRwwyntn6x0HFM0N8y7IvGibL6T0e134uPfh+PtT1raqReQA/BjwD+FtgoiP9WOAG4HHABuA2YEW7byfwH4AAfwGc2qa/BtjaPj8D+PASxfwm4DdnSF9wzH16j1e0sTwNWNXGeOwAP/OvAkdMS3s7cGH7/ELgbfO9p0sQ108AxwNfWkxcg/ysh+UxbOfcAuI+Eji+ff5E4CvtZz3wyTc1jQAAIABJREFU83OR9fqvwAeBT7bbo16fDwCvbp+vAg4b9ToNw6Of7XaOtvYmFti/9iCWoeqT2s/ha8CP9OP9YAj6vlli+ClgZfv8bR0xrO/MN+04Y9v/9rP9DqBuAz9HB1j3nv1dMor1X04Phvg7L/DpjmOfBvztIM8n4AXAXwOPa7efOsh42uMfRbOo3j/T/s0wwPdntv5x4O2fZdwXDctjub3HjGk/Rw++nw9zfUfqSumquqWqbp1h1xZgW1U9WFW306y4ekKSI4EnVdU/VvNJXAH8bEeZD7TPPwKc3OdfCw4m5n44AdhdVXuq6iFgWxvrMOn87D7Aoz/Tx7ynSxFAVV0DfGMxcQ3BZz0sRuGce4yquruqrmuffwu4BVjDEJyfByvJWuA/A+/rSB7l+jyJZoDijwGq6qGq+j+McJ2GSN/a7RxtbTb9/hwHeT6dDNxWVf88T3w9iWMY+r6ZYqiqT1fV/nZzB7B2rmPY/45mv9uNYThHB6VXf5eMav2XkyH/zlvAk9rnPwDctYjYeuHXgLdW1YMAVXXvgOMB+APgt2jeqykDiWeO/nEY2v+y7YuGyLJ6j8exn+vF9/Nhr+9IDUrPYQ1wZ8f2ZJu2pn0+Pf1RZdr/qO8HDl+i+C5ob5m5rOPS+oOJuR9mi2tQCvh0ks8nObdN+8Gquhua/5iAp7bpg459oXEN+rMeFoP+3BYtza2QzwE+x/Cen934Q5o/oh/uSBvl+jwN2Ae8v73l6X1Jvp/RrtOwGMh7Na2twcL6114Ytj7pDOBDHdv9fj9g+Pq+V9FcATJlQ9v+/y7JSR2xjXP/O27/1w3bObrkFvl3ycjXfxkbhu+8vwFcnORO4B3A6xcRWy8cA5yUZmqSv0uyaZDxJHkJsLeqbpi2axjaW2f/OAzxjFtfNAjL9j0eo36uF9/Ph7q+KwcdwHRJ/hr4oRl2XVRVn5it2AxpNUf6XGUWbK6YgfcCb26P/Wbg92g6hIOJuR8G/frT/cequivJU4G/SvLlOfIOW+xThvWzHhYj/T4keQLwUeA3quqbc1x8MtT1TPJi4N6q+nyS53dTZIa0oalPayXNbdy/XlWfS3IJzS1OsxmFOg2Lvr9XM7S1hfavvTA0fVKSVcBL+N6AwCDejzlDnOV1lyyeJBcB+4E/bZPuBtZV1X1Jngt8PMlxSxnDiBj3+k9Zln+f9eDvkpGu/6gY5u+883y3PRn4L1X10SQvo7kb7YUHGVtX5olnJfBk4ERgE3BlkqcNMJ7/QTNlxmOKDSKeqXNphv5xGNq//9csvWX5Ho9LP9fD7+dDXd+hG5SuqhceRLFJmrmbpqyluZVokkffwjmV3llmMslKmtuPpt/u15VuY07yR8AnFxFzP8wW10BU1V3tv/cm+TOaW1DuSXJkVd3d3oowdZvWoGNfaFyD/qyHxaA/t4OW5BCaDvFPq+pjbfKwnp/z+Y/AS9IsjnYo8KQkf8Lo1geaGCerauqq2o/QDEqPcp2GRV/fq5naWlXd07G/m/510YasTzoVuG7qfRjE+9Eair4vySuAFwMnt7cm0t7OPXVL9+eT3EZzVd2497/j9n/dUJyj/dCjv0tGtv6jZJi/884VW5IrgNe2m1fxvVvKl+x8mieeXwM+1v6/vzPJw8ARg4gnyb+nmcf1hnaQbC1wXZrFIAfy/rRxPaZ/XMp4FmDc+qJBWHbv8Zj1c736fj7U9V0u03dsB85Is7rwBmAjsLO9lP1bSU5s5846C/hER5lXtM9/Hvibjv+ke6Y9SaacDkytBn4wMffDtcDGJBvaq7DOaGPtuyTfn+SJU89pfnX+Eo/+7F7Boz/Tx7ynfQx5QXENwWc9LIbmnFuI9jP7Y+CWqvr9jl3Den7OqapeX1Vrq2o9zWfwN1X1y4xofQCq6mvAnUme0SadDNzMCNdpiPSt3c7W1hbav/YgjmHrk86kY+qOfr8fHQbe9yU5BXgd8JKqeqAjfXWSFe3zp7Ux7LH/Hc1+dxEGfo72Q6/+LhnV+o+JYfjOexfwvPb5TwL/tIjYeuHjbRwkOYZmMbevDyKeqvpiVT21qta3f09P0izK9rVBxAOz94+DimeaceuLBmFZvcfj1s/16vv50Ne3hmC1xW4fNF+yJmmuerkH+FTHvotoVpe8lY6VJIEJmi9mtwHvAtKmH0rz6+5umi9nT1uimP8/4IvAjTQnyZEHG3Mf3+fTaFYyvY3mtp9Bfd5Po1k99AbgpqlYaOZB+wzNH0GfAZ4y33u6BLF9iOa24O+25+Q5BxPXoD/rYXkMyzm3wJj/E81tLzcC17eP04bh/OxB3Z7P91b3Hen6AM8GdrWf08dpbvEc6ToNy6Nf7XaOtrbg/nWRcQxNnwQ8HrgP+IGOtCV/PxiCvm+WGHbTzKE3dX5sbfP+XPtZ3QBcB/xML2JYDo9+td8B1Gvg5+gA696zv0tGsf7L6cEQf+dtz7PPt/+vfg547iDPJ5pB6D9pj38d8JODjGdabF8Fjhjw+zNj/zgM70/7WsuyLxqmx3J6j8e5n2OR38+Hub5TnZUkSZIkSZIkSUtuuUzfIUmSJEmSJEkaAQ5KS5IkSZIkSZL6xkFpSZIkSZIkSVLfOCgtSZIkSZIkSeobB6UlSZIkSZIkSX3joLQkSZIkSZIkqW8clNZBSXJGkluS/FuS25KcNOiYJM0uyb9OexxI8j8HHZek+SVZn+TqJP+S5GtJ3pVk5aDjkjS3JD+W5G+S3J9kd5LTBx2TpMdKckGSXUkeTHL5tH0nJ/lykgeS/K8kPzKgMCVNM1vbTbIqyUeSfDVJJXn+4KLUXByU1oIleRHwNuBs4InATwB7BhqUpDlV1ROmHsAPAt8GrhpwWJK68x7gXuBI4NnA84DXDDQiSXNqfzj6BPBJ4CnAucCfJDlmoIFJmsldwP8LXNaZmOQI4GPAG2ja8S7gw32PTtJsZmy7rb8Hfhn4Wl8j0oI4KK2D8f8Av1tVO6rq4araW1V7Bx2UpK79PM0A12cHHYikrmwArqyq71TV14C/BI4bcEyS5vajwA8Df1BVB6rqb4D/DfzKYMOSNF1VfayqPg7cN23XS4GbquqqqvoO8CbgWUl+tN8xSnqs2dpuVT1UVX9YVX8PHBhMdOqGg9JakCQrgAlgdXsb4mR7G/G/G3Rskrr2CuCKqqpBByKpK5cAZyR5fJI1wKk0A9OShldmSfvxfgci6aAdB9wwtVFV/wbchj8MS1JPOCithfpB4BCaKy1PormN+DnAbw8yKEndSbKO5tb/Dww6Fkld+zuaL8DfBCZpbh/++EAjkjSfL9PclfTfkxyS5Kdo+t/HDzYsSQvwBOD+aWn300xhKUlaJAeltVDfbv/9n1V1d1V9Hfh94LQBxiSpe2cBf19Vtw86EEnzS/J9wKdo5rT8fuAI4Mk0aztIGlJV9V3gZ4H/TDOf5X8DrqT5YUnSaPhX4EnT0p4EfGsAsUjSsuOgtBakqv6F5o9pb/uXRtNZeJW0NEqeAhwFvKuqHqyq+4D344/B0tCrqhur6nlVdXhV/TTwNGDnoOOS1LWbgGdNbST5fuDoNl2StEgOSutgvB/49SRPTfJk4DdoVhaXNMSS/F/AGuCqQcciqTvtHUm3A7+WZGWSw2jmhb9h7pKSBi3JM5Mc2s4H/5vAkcDlAw5L0jRt/3oosAJY0bbblcCfAT+e5Ofa/W8EbqyqLw8yXkmNOdouSR7X7gNY1e6bab0HDZCD0joYbwauBb4C3AJ8AXjLQCOS1I1XAB+rKm85lEbLS4FTgH3AbmA/8F8GGpGkbvwKcDfN3NInAy+qqgcHG5KkGfw2zTSVFwK/3D7/7araB/wczXfdfwE2A2cMKkhJjzFj22333dpur6GZCu/bwI8MIEbNIVXOwiBJkiRJkiRJ6g+vlJYkSZIkSZIk9Y2D0pIkSZIkSZKkvnFQWpIkSZIkSZLUNw5KS5IkSZIkSZL6xkFpSZIkSZIkSVLfrBx0AAtxxBFH1Pr16wcdhjR0Pv/5z3+9qlYPOo7Z2Halmdl2pdE1zO3XtivNbpjbLth+pdnYdqXRNFfbHalB6fXr17Nr165BhyENnST/POgY5mLblWaW5F+S3AqsAN5XVW+dtj/AJcBpwAPAK6vquiRHAVcAPwQ8DFxaVZe0ZZ4CfBhYD3wVeFlV/Uu77/XAOcAB4P+uqk/NFZ9tV5rdMPe9tl1pdsPcdsH2K83GtiuNprnartN3SJI0AAcOHABYB5wKHAucmeTYadlOBTa2j3OB97bp+4H/VlU/BpwInN9R9kLgM1W1EfhMu027/wzgOOAU4D1JVixN7SRJkiRJmp2D0pIkDcDOnTsBHqyqPVX1ELAN2DIt2xbgimrsAA5LcmRV3V1V1wFU1beAW4A1HWU+0D7/APCzHenbqurBqrod2A2csETVkyRJkiRpVg5KS3qUJKckuTXJ7iQXzrD/vye5vn18KcmBdroASQuwd+9egIc6kib53sDylDXAnXPlSbIeeA7wuTbpB6vqboD236d2eyxJkiRJkvphpOaU1nj77ne/y+TkJN/5zncGHcrAHHrooaxdu5ZDDjlkSY7f3sr/buBFNANW1ybZXlU3T+WpqouBi9v8PwP8l6r6xpIEpGXBtjtz262qmbJOT8xceZI8Afgo8BtV9c15wpjzWB3HPJdmqhDWrVs3zyG1nNl2G0vd90q9Zttt2HY1amy7DdvuaPM8bnged8dBaY2MyclJnvjEJ7J+/Xqatb/GS1Vx3333MTk5yYYNG5bqZU4AdlfVHoAkU9MJ3DxL/jOBDy1VMFoebLszt921a9cCrOrIuha4a1rxSeComfIkOYRmQPpPq+pjHXnumZriI8mRwL3zHWtavJcClwJMTEzMOHKu8TDubRf61vdKPWXbte1qNNl2bbvLgeex5/FCOH2HRsZ3vvMdDj/88LH9jy0Jhx9++FL/4tj17f1JHk+zWNpHlzIgjT7b7sxtd9OmTQCHJtmQZBXNIoTbpxXfDpyVxonA/e1gc4A/Bm6pqt+focwr2uevAD7RkX5Gkscl2UCzeOLO3tRSy9G4t13oW98r9ZRt17ar0WTbte0uB57HnscLseyulP7g5+7oyXFevtlblofROP/HBn2pf1e397d+Bvjfs03d4RQAA7br/YsrP3F2b+Jo2XYfW/+VK1cC3AF8ClgBXFZVNyU5D6CqtgJXA6fRLEr4ADD1wfxH4FeALya5vk37H1V1NfBW4Mok57TH/4X2eDcluZLmzof9wPlVdWCxdevsd+07l59xb7uwfN+DXv3NDLb9YbRcz9uFWNbvwWL/zpvS47/3tHjL+rzt0nJ+D8ZlvGo5f4bd8j3ojldKS0Pkoosu4qijjuIJT3jCoELo6vb+1hnMMXVHVV1aVRNVNbF69eoehigNn0W03fur6piqOrqq3gLNYHQ7IE01zm/3//uq2tWm/31VpaqeWVXPbh9Xt/vuq6qTq2pj++8jPxxV1VvaYz2jqv6iN7WXRtcQ9LuSDoJtVxpNtl0tB57HvbPsrpTW+OjlFT4wHL82/szP/AwXXHABGzduHFQI1wIb21v799IMPL98eqYkPwA8D/jl/oan5cC2K40m2640mmy70miy7Wo58DzWXLxSWurSG97wBi655JJHti+66CLe+c539vQ1TjzxRI488sieHnMhqmo/cAHNdAK3AFdOTScwNaVA63Tg01X1b4OIU1qIcWi70nJk25VGk21XGk22XS0HnsejxSulpS6dc845vPSlL+W1r30tDz/8MNu2bWPnzseuEXbSSSfxrW996zHp73jHO3jhC1/Yj1AXpZ0C4OppaVunbV8OXN6/qKSDNy5tV1pubLvSaLLtSqNpXNpuksuAFwP3VtWPz5Ln+cAfAocAX6+q5/UvQi3GuJzHy4WD0lKX1q9fz+GHH84XvvAF7rnnHp7znOdw+OGHPybfZz/72QFEJ2k2tl1pNNl2pdFk25VG0xi13cuBdwFXzLQzyWHAe4BTquqOJE/tY2xapDE6j5cFB6WlBXj1q1/N5Zdfzte+9jVe9apXzZin21/cDhw4wHOf+1wAXvKSl/C7v/u7SxO0JNuuNKJsu9Josu1Ko2kc2m5VXZNk/RxZXg58rKruaPPf24+41DvjcB4vFw5KSwtw+umn88Y3vpHvfve7fPCDH5wxT7e/uK1YsYLrr7++l+FJmoVtVxpNtl1pNNl2pdFk2wXgGOCQJH8LPBG4pKpmvKpaw8nzeHR0tdBhklOS3Jpkd5ILZ9ifJO9s99+Y5Pj5yib5cJLr28dXk/gpa+itWrWKF7zgBbzsZS9jxYoVPT/+b/3Wb7F27VoeeOAB1q5dy5ve9Kaev4Y0jmy70miy7UqjybYrjSbbLtBcvPlc4D8DPw28IckxM2VMcm6SXUl27du3r58xag6ex6Nj3iulk6wA3g28CJgErk2yvapu7sh2KrCxfWwG3gtsnqtsVf1ix2v8HnB/j+qkMfHyzev6/poPP/wwO3bs4KqrrlqS47/97W/n7W9/+5IcWxoWtl1pNNl2pdFk25VGk213YCZpFjf8N+DfklwDPAv4yvSMVXUpcCnAxMRE9TXKEeF5rLl0c6X0CcDuqtpTVQ8B24At0/JsAa6oxg7gsCRHdlM2SYCXAR9aZF2kJXXzzTfz9Kc/nZNPPpmNGzcOOhxJXbLtSqPJtiuNJtuuNJpsu4/4BHBSkpVJHk9z4eUtA45JXfI8Hi3dzCm9BrizY3uSplHOl2dNl2VPAu6pqn+a6cWTnAucC7BuXf9/YZGmHHvssezZs2fQYUhaINuuNJpsu0vr6DsO/uqh29b9Qg8j0XJj25VG07i03SQfAp4PHJFkEvgd4BCAqtpaVbck+UvgRuBh4H1V9aVBxauFGZfzeLnoZlA6M6RNvy1htjzdlD2TOa6S9nYISZIkSZIkLVZVndlFnouBi/sQjjTWuhmUngSO6theC9zVZZ5Vc5VNshJ4Kc0k8pIkSZIkSZKkZa6bQelrgY1JNgB7gTOAl0/Lsx24IMk2muk57q+qu5Psm6fsC4EvV9XkIushSZIkSZIkSZrLrvf35jgTZy+q+LyD0lW1P8kFwKeAFcBlVXVTkvPa/VuBq4HTgN3AA8DZc5XtOPwZuMChJEmSJEmSJI2N7+smU1VdXVXHVNXRVfWWNm1rOyBNNc5v9//7qto1V9mOfa+cOoY0jq655hqOP/54Vq5cyUc+8pFBhyOpSz1su09KcmuS3UkunL4zjXe2+29McnzHvsuS3JvkS9PKfDjJ9e3jq0mub9PXJ/l2xz77X40d+11pNNl2pdFk29Vy4Hm8dLqZvkMaTr263WDKIm87OBjr1q3j8ssv5x3veEffX1saGNsuAAcOHABYBxxLszbDtUm2V9XNHdlOBTa2j83Ae9t/AS4H3gVc0XncqvrFqedJfg+4v2P3bVX17IMOWuPNtiuNJtvukkhyCnAJzR3B76uqt86SbxOwA/jFqurJaMbnbv9GLw7D5omeHEZLxbar5cDzWHPo6kppSfCGN7yBSy655JHtiy66iHe+852LOub69et55jOfyfd9n01RWirD2nZ37twJ8GBV7amqh4BtwJZp2bYAV7R3JO0ADktyJEBVXQPM+q00SYCX4TRZGlHD2nYlzW0c2m6SFcC7aX48PhY4M8mxs+R7G810ltJQG4e2q+XP83i0eKW01KVzzjmHl770pbz2ta/l4YcfZtu2bVODSo9y0kkn8a1vfesx6e94xzt44Qtf2I9QJXUY1ra7d+9egIc6kib53lXQU9YAd07Lswa4u4uXOAm4p6r+qSNtQ5IvAN8EfruqPju9UJJzgXOhuSpAGpRhbbuS5jYmbfcEYHdV7QFIMvXD8s3T8v068FFgU3/DkxZuTNquljnP49HioLTUpfXr13P44YfzhS98gXvuuYfnPOc5HH744Y/J99nPPmaMR9IADWvbraoZk6dtp4s8szmTR18lfTewrqruS/Jc4ONJjquqb06L61LgUoCJiYluX0vquWFtu5LmNiZtd6YfjR/1w3KSNcDpwE8yz6C0PwhrGIxJ29Uy53k8WhyUlhbg1a9+NZdffjlf+9rXeNWrXjVjHn9xk4bPMLbdtWvXAqzqTALumpZtEjhqnjyPkWQl8FLguVNpVfUg8GD7/PNJbgOOAXbNeBBpCAxj25U0vzFou938aPyHwOuq6kAzo9bs/EFYw2IM2q7GgOfx6HBQWlqA008/nTe+8Y1897vf5YMf/OCMeUb9F7duFm1J8nyaP7QPAb5eVc/ra5DSAg1j2920aRPAoUk2AHuBM4CXT8u2HbigvS14M3B/VXUzdccLgS9X1eRUQpLVwDfaL8dPo1k8cc/iayItnWFsu3OZrw9t53q/BDgNeAB4ZVVd12XZ3wQuBlZX1deXui7SYoxa2z0I3fxoPAFsawekjwBOS7K/qj7enxClhRuDtqsx4Hk8OpylW1qAVatW8YIXvICXvexlrFixYtHHu/baa1m7di1XXXUVv/qrv8pxxx3XgygPXjeLtiQ5DHgP8JKqOg74hb4HKi3QMLbdlStXAtxBs/jRLcCVVXVTkvOSnNdmu5pm4Hg38EfAa6bKJ/kQ8I/AM5JMJjmn4/Bn8NgFDn8CuDHJDcBHgPOqataFEqVhMIxtdzZdLnx2Ks0PQhtpbtV/bzdlkxwFvIjm/wxp6I1S2z3YkICNSTYkWUXT727vzFBVG6pqfVWtp+l3X+OAtIbdGLRdjQHP49HhldIaXRNn9/0lH374YXbs2MFVV13Vk+Nt2rSJycnJ+TP2TzeLtrwc+FhV3QFQVff2PUqNNttup/uraqIzoaq2djwv4PyZClbVmbMdtKpeOUPaR2kWW5IOjm13Pt30oVuAK9q2vSPJYUmOBNbPU/YPgN8CPrFUwWsZs+32XFXtT3IBzQ/LK4DLpn5YbvdvnfMAUjdsu1oOPI81B6+Ulrp088038/SnP52TTz6ZjRs3DjqcpTLToi1rpuU5Bnhykr9N8vkkZ810oCTnJtmVZNe+ffuWKFxpfmPSdqVlZwTbbjd96Gx5Zi2b5CXA3qq6Ya4Xt9/VsBjBtntQqurqqjqmqo6uqre0aVtnGpCuqldW1Uf6H6XUvXFpu1rePI9Hi1dKS1069thj2bNn2U+/2s2iLStpFk87Gfh3wD8m2VFVX3lUIRds0ZAYk7YrLTsj2Ha76UNnyzNjepLHAxcBPzXfi9vvaliMYNuVxPi03SSXAS8G7q2qH58j3yZgB/CL/qg0OsblPF4uvFJaUqduFm2ZBP6yqv6tXWjpGuBZfYpPkqRh1W0fOlOe2dKPBjYANyT5apt+XZIf6mnkkiSNj8uBU+bK0K718DaaKXokLREHpTVSmikYx1cf6j/voi0081melGRlewXXZppF2qRZ2XbHu/4aXZ67C3oPuulDtwNnpXEizbzyd89Wtqq+WFVP7VgsbRI4vqq+1oOqaRmz7foeaDR53i79e1BV1wDzLfb96zRrsbh+0kHwPPY96JaD0hoZhx56KPfdd9/YNu6q4r777uPQQw9dytfYD0wt2nILcOXUoi0dC7fcAvwlcCOwE3hfVX1pyYLSyLPtLn3blZbCuLddWFj77aYPBa4G9gC7gT8CXjNX2V7XR+PBtmvfq9Fk2x2OtptkDXA6MO+Cpa7n8Fiex8NxHo8K55TWyFi7di2Tk5OM83/2hx56KGvXrl3S16iqq2m+NHembZ22fTFw8ZIGomXDttuftiv1mm23sZD2O18fWs03tPO7LTtDnvVdBaKxZttt2Pdq1Nh2G0PQdv8QeF1VHUhmWvLhe1zP4bE8jxtDcB6PBAelNTIOOeQQNmzYMOgwJC2QbVcaTbZdaTTZdqXRZNsdGhPAtnZA+gjgtCT7q+rjgw1rNHgeayG6GpROcgpwCbCC5lb9t07bn3b/acADwCur6rr5yib5dZpbFfcDf15Vv7XYCh19x1WLPURj83/rzXEkSZIkSZI09KrqkRHVJJcDn3RAWloa8w5Kt6uOvht4Ec3iKtcm2V5VN3dkOxXY2D42A+8FNs9VNskLgC3AM6vqwSRP7WXFJEmSJEmSpClJPgQ8HzgiySTwO8Ah8NhpKyUtrW6ulD4B2F1VewCSbKMZTO4clN4CXNHOk7cjyWFJjgTWz1H214C3VtWDAFXlqqaSJEmSJElaElV15gLyvnIJQ5HG3vd1kWcNcGfH9mSb1k2eucoeA5yU5HNJ/i7Jpple3NVMJUmSJEmSJGn56GZQeqblRqevKjpbnrnKrgSeDJwI/HfgysywtGlVXVpVE1U1sXr16i7ClSRJkiRJkiQNq26m75gEjurYXgvc1WWeVXOUnQQ+1k75sTPJwzQrm3o5tCRJkiRJkiQtU91cKX0tsDHJhiSrgDOA7dPybAfOSuNE4P6qunuesh8HfhIgyTE0A9hfX3SNJEmSJEmSJElDa95B6araD1wAfAq4Bbiyqm5Kcl6S89psVwN7gN3AHwGvmatsW+Yy4GlJvgRsA17RXjUtSdK4eFKSW5PsTnLh9J3tj73vbPffmOT4jn2XJbm37Uc7y7wpyd4k17eP0zr2vb491q1JfnppqyZJkiRJ0sy6mb6DqrqaZuC5M21rx/MCzu+2bJv+EPDLCwlWkqTl4sCBAwDrgGNpprS6Nsn2qrq5I9upwMb2sRl4b/svwOXAu4ArZjj8H1TVOzoTkhxLc8fSccAPA3+d5JiqOtCrOkmSJEmS1I1upu+QJEk9tnPnToAHq2pP+0PtNmDLtGxbgCuqsQM4LMmRAFV1DfCNBbzkFmBbVT1YVbfT3N10wmLrIUmSJEnSQjkoLUnSAOzduxfgoY6kSWDNtGxrgDvnyTOTC9rpPi5L8uSFHCvJuUl2Jdm1b59rD0uSJEmSes9BaUmSBmCWZRSmJ6aLPNO9FzgaeDZwN/B7CzlWVV1aVRNVNbF69ep5XkqSJEkVfK13AAAgAElEQVSSpIVzUFqSpAFYu3YtwKrOJOCuadkmgaPmyfMoVXVPVR2oqodpFh+emqJjwceSJEmSJGkpOCgtSdIAbNq0CeDQJBuSrKJZhHD7tGzbgbPSOBG4v6runuu4U3NOt04HvtRxrDOSPC7JBprFE3f2oCqSJEmSJC3IykEHIEnSOFq5ciXAHcCngBXAZVV1U5LzAKpqK3A1cBrNooQPAGdPlU/yIeD5wBFJJoHfqao/Bt6e5Nk0U3N8FfjV9ng3JbkSuBnYD5xfVQeWvqaSJEmSJD2ag9KSJA3O/VU10ZnQDkZPPS/g/JkKVtWZs6T/ymwvVlVvAd5ycKFKkiRJoy3JZcCLgXur6sdn2P9LwOvazX8Ffq2qbuhjiNLYcPoOSY+S5JQktybZneTCGfY/P8n9Sa5vH28cRJySJEmSJC3Q5cApc+y/HXheVT0TeDNwaT+CksaRV0pLekSSFcC7gRfRLIp2bZLtVXXztKyfraoX9z1ASZIkSZIOUlVdk2T9HPv/oWNzB83i4JKWgIPSkjqdAOyuqj0ASbYBW2jmoNWAffBzd3Sd9+g7vjHrvs0bntKLcCRJkiRpOTsH+IvZdiY5FzgXYN26df2KSVo2nL5DUqc1wJ0d25Nt2nT/IckNSf4iyXEzHSjJuUl2Jdm1b9++pYhVkiRJkqSeS/ICmkHp182Wp6ouraqJqppYvXp1/4KTlgkHpSV1ygxpNW37OuBHqupZwP8EPj7TgeygJUmSJEmjJskzgfcBW6rqvkHHIy1XDkpL6jQJHNWxvRa4qzNDVX2zqv61fX41cEiSI/oXoiRJkiRJvZdkHfAx4Feq6iuDjkdazpxTWlKna4GNSTYAe4EzgJd3ZkjyQ8A9VVVJTqD5cctfjyVJkiRJQy3Jh4DnA0ckmQR+BzgEoKq2Am8EDgfekwRgf1VNDCZaaXlzUFrSI6pqf5ILgE8BK4DLquqmJOe1+7cCPw/8WpL9wLeBM6pq+hQfkiRJkiQNlao6c579rwZe3adwpLHW1fQdSU5JcmuS3UkunGF/kryz3X9jkuPnK5vkTUn2Jrm+fZzWmypJWoyqurqqjqmqo6vqLW3a1nZAmqp6V1UdV1XPqqoTq+ofBhuxJEmS1F9dfEfe0n43vr5d/Ps/DSJOSZKG1bxXSidZAbwbeBHNfLPXJtleVTd3ZDsV2Ng+NgPvBTZ3UfYPquodPauNJEmSJElLqMvvyJ8BtrdT3j0TuBL40f5HK0nScOrmSukTgN1VtaeqHgK2AVum5dkCXFGNHcBhSY7ssqwkSZIkSaNi3u+5VfWvHVPcfT/gdHeSJHXoZlB6DXBnx/Zkm9ZNnvnKXtDe0nRZkifP9OJJzm1vd9q1b9++LsKVJEmSJGnJdPMdmSSnJ/ky8OfAq/oUmyRJI6GbQenMkDb9V97Z8sxV9r3A0cCzgbuB35vpxavq0qqaqKqJ1atXdxGuJEmS1H9LtA7Lmzvmpf10kh/uV30kzaqb78hU1Z9V1Y8CPwu8edaDeSGWJGkMdTMoPQkc1bG9Friryzyzlq2qe6rqQFU9DPwRzS1QkiRJ0sjpmGP2VOBY4Mwkx07L1rkOy7k0F2nMV/biqnpmVT0b+CTwxqWui6R5dfMd+RFVdQ1wdJIjZtnvhViSpLHTzaD0tcDGJBuSrALOALZPy7MdOKu9+uNE4P6qunuusu2c01NOB760yLpIkjRqnrSIqyovS3Jvki9NK3Nxki+3+f8syWFt+vok326vtrw+ydalr540VpZkHZaq+mZHeeellYbDvN+Rkzw9SdrnxwOrgPv6HqkkSUNq5XwZqmp/kguATwErgMuq6qYk57X7twJXA6cBu4EHgLPnKtse+u1Jnk3zh/VXgV/tZcUkSRpmBw4cAFhHc1XkJHBtku1VdXNHts6rKjfTXFW5ud13OfAu4Ipph/4r4PVtH/w24PXA69p9t7VXW0rqvZnmmN3cRZ7Z1mF5pGyStwBnAfcDL+hdyJIORpffkX+O5sKt7wLfBn6xY+FDSZLG3ryD0gBVdTXNwHNn2taO5wWc323ZNv1XFhSpJEnLyM6dOwEerKo9AEmmrozsHJR+5KpKYEeSw5IcWVV3V9U1SdZPP25Vfbpjcwfw80tUBUmPtlTrsFBVFwEXJXk9cAHwO4958eRcmilBWLduXZchSzpYXXxHfhvwtn7HJUnSqOhqUFqS1Ee73j9j8tF3fKPPgWgp7d27F+ChjqSFXFV5d5cv8yrgwx3bG5J8Afgm8NtV9dnpBRzYkg7aYtZhWdVFWYAPAn/ODIPSVXUpcCnAxMSEV2NKkiRpqHUzp7QkSeqxWe7g7faqynkluQjYD/xpm3Q3sK6qngP8V+CDSZ40Q1wutiQdnKVah2VjR/mXAF9e6opIkiRJS80rpSVJGoC1a9dCc3XkI0l0f1XlnJK8AngxcPLU/JVV9SDwYPv880luA44Bdh1kFSR1WMJ1WN6a5BnAw8A/A+f1sVqSJEnSknBQWpKkAdi0aRPAoUk2AHtprox8+bRs24EL2vmmN/O9qypnleQUmoUNn1dVD3Skrwa+UVUHkjyNZvHEPb2qj6QlW4fl53ocpiRJYyvJZTQXb9xbVT8+w/4Al9D8iPwA8Mqquq6/UUrjwek7JEkagJUrVwLcQXNl5C3AlVNXVU5dWUkzQLWH5qrKPwJeM1U+yYeAfwSekWQyyTntrncBTwT+Ksn1SaYGxH4CuDHJDcBHgPOqyonKJUmSNE4uB06ZY/+pNBdvbKRZZ+W9fYhJGkteKS1J0uDcX1UTnQkLuKryzFnSnz5L+keBjx58qJIkSdJoq6prkqyfI8sW4Ir27/AdSQ5LcuR8dytKWjivlJYkSZIkSZJgDXBnx/ZkmyapxxyUliRJkiRJkiAzpNWMGZNzk+xKsmvfvn1LHJa0/DgoLelRkpyS5NYku5NcOEe+TUkOJPn5fsYnSZIkSdISmQSO6theC9w1U8aqurSqJqpqYvXq1X0JTlpOnFNa0iOSrADeDbyIpjO+Nsn2qrp5hnxvo1mgTZIkSZKk5WA7cEGSbcBmmjVgejKf9NF3XNWLw8Dm/9ab40gD5qC0pE4nALurag9A2xFvAW6elu/XaRZM29Tf8CRJkiRJOjhJPgQ8HzgiySTwO8Ah8MiC41cDpwG7gQeAswcTqbT8OSgtqdNMizps7syQZA1wOvCTOCgtSZIkSRoRVXXmPPsLOL9P4UhjzTmlJXXqZlGHPwReV1UH5jyQiz5IkiRJkiRpBl4pLalTN4s6TADbkgAcAZyWZH9VfbwzU1VdClwKMDExMeNqxZIkSZIkSRo/DkpL6nQtsDHJBmAvcAbw8s4MVbVh6nmSy4FPTh+QliRJkiRJkmbT1fQdSU5JcmuS3UkunGF/kryz3X9jkuMXUPY3k1SSIxZXFUmLVVX7gQuATwG3AFdW1U1Jzkty3mCjkyRJkiRJ0nIw75XSSVYA7wZeRHNr/7VJtlfVzR3ZTgU2to/NwHuBzfOVTXJUu++O3lVJ0mJU1dU0Kw53pm2dJe8r+xGTJEmSJEmSlo9urpQ+AdhdVXuq6iFgG7BlWp4twBXV2AEcluTILsr+AfBbPHYhNUmSJEmSJEnSMtTNoPQa4M6O7ck2rZs8s5ZN8hJgb1XdMNeLJzk3ya4ku/bt29dFuJIkSZIkSZKkYdXNoHRmSJt+ZfNseWZMT/J44CLgjfO9eFVdWlUTVTWxevXqeYOVJGmEPGkRazZcluTeJF+aVuYpSf4qyT+1/z65Y9/r22PdmuSnl7ZqkiRJkiTNrJtB6UngqI7ttcBdXeaZLf1oYANwQ5KvtunXJfmhhQQvSdKoOnDgAMA6mnUZjgXOTHLstGydazacS7Nmw5TLgVNmOPSFwGeqaiPwmXab9thnAMe15d7Trv0gSZIkSVJfdTMofS2wMcmGJKtovtBun5ZnO3BWe0XXicD9VXX3bGWr6otV9dSqWl9V62kGr4+vqq/1qmKSJA2znTt3Ajx4kGs2UFXXAN+Y4dBbgA+0zz8A/GxH+raqerCqbgd206z9IEmSJElSX62cL0NV7U9yAfApYAVwWVXdlOS8dv9W4GrgNJovuA8AZ89VdklqIknSCNm7dy/AQx1Jk8DmadlmW5vh7jkO/YPtD8NU1d1JntpxrB0zHEuSJEmSpL6ad1AaoKquphl47kzb2vG8gPO7LTtDnvXdxCFJo+iDn7tjQfmPvmOmi19753O3z3/82w7MH/PLN6/rRThjq+k6H5s8bbubdR261dWxkpxLM1UI69b5GUuSJEmSeq+b6TskSVKPrV27FmBVZxLdr9kwl3umpvho/713IcdygWFJkiRJ0lLr6kppSQdh1/t7c5yJs3tzHPXPtM9+qa981mjatGkTwKFJNgB7adZdePm0bNuBC5Jso5naY2rNhrlsB14BvLX99xMd6R9M8vvAD9MsnrizB1WRJEmSRkKSU4BLaKaYfV9VvXXa/h8A/oRmQfKVwDuqqkdf7iV18kppSZIGYOXKlQB30Ky7cAtw5dSaDVPrNtBMf7WHZs2GPwJeM1U+yYeAfwSekWQyyTntrrcCL0ryT8CL2m3aNR2uBG4G/hI4v6oOLG0tJUmSpOGQZAXwbuBU4FjgzCTHTst2PnBzVT0LeD7we0lWIannvFJakqTBub+qJjoTFrBmw5mzpN8HnDzLvrcAbznoaCVJkqTRdQKwu6r2ALR3I26huWhjSgFPTBLgCcA3gP39DlQaB14pLUmSJEmSpOVuDXBnx/Zkm9bpXcCP0ay98kXgtVX18EwHS3Jukl1Jdu3bt28p4pWWNQelJUmSJEmStNxlhrSatv3TwPU0a7A8G3hXkifNdDAXCJcWx0FpSZIkSZIkLXeTwFEd22tprojudDbwsWrsBm4HfrRP8UljxUFpSZIkSZIWIMkpSW5NsjvJhTPs/6UkN7aPf0jyrEHEKelRrgU2JtnQLl54BrB9Wp47aNdnSfKDwDNoFh6X1GMudChJkiRJUpeSrADeDbyI5srLa5Nsr6rOxdJuB55XVf+S5FTgUmBz/6OVNKWq9ie5APgUsAK4rKpuSnJeu38r8Gbg8iRfpJnu43VV9fWBBS0tYw5KS5IkSZLUvROA3VW1ByDJNmAL8MigdFX9Q0f+HTTTBEgasKq6Grh6WtrWjud3AT/V77ikceSgtKRHSXIKcAnNL8fvq6q3Ttu/hebX44eB/cBvVNXf9z1QSZKGTBd9aNr9pwEPAK+squvmKpvkYuBngIeA24Czq+r/9KdGkmaxBrizY3uSua+CPgf4iyWNSJKkLn3u9m/05DibJxZX3jmlJT2i41bEU4FjgTOTHDst22eAZ1XVs4FXAe/rb5SSJA2fLvvQU4GN7eNc4L1dlP0r4Mer6pnAV4DXL3FVJM0vM6TVjBmTF9AMSr9u1oMl5ybZlWTXvn37ehSiJEnDzUFpSZ0euRWxqh4Cpm5FfERV/WtVTf3R/f3M8ge4JEljZt4+tN2+oho7gMOSHDlX2ar6dFXtb8s7BYA0HCaBozq21wJ3Tc+U5Jk0F3Bsqar7ZjtYVV1aVRNVNbF69eqeBytJ0jByUFpSp5luRVwzPVOS05N8GfhzmqulH8MrPiRJY6abPnS2PF31vzR97oxTANjvSn11LbAxyYYkq4AzgO2dGZKsAz4G/EpVfWUAMUqSNNQclJbUqatbEavqz6rqR4GfpZlf+rGFvOJDkjReuulDZ8szb9kkF9Gs5fCnM724/a7UP+3dCxcAnwJuAa6sqpuSnJfkvDbbG4HDgfckuT7JrgGFK0nSUOpqUDrJKUluTbI7yYUz7E+Sd7b7b0xy/Hxlk7y5zXt9kk8n+eHeVEnSInR1K+KUqroGODrJEUsdmCRJQ66bPnS2PHOWTfIK4MXAL3VMoSVpgKrq6qo6pqqOrqq3tGlbq2pr+/zVVfXkqnp2+1jkclCSJC0v8w5KL+GiLRdX1TPbxdI+SfNLsqTB6uZWxKcnSfv8eGAVMOsceZIkjYl5+9B2+6z2go4Tgfur6u65yiY5hWaBtJdU1QP9qowkSZK0lFZ2keeRhVcAkkwtvHJzR55HFm0BdiSZWrRl/Wxlq+qbHeVdLE0aAlW1P8nUrYgrgMumbkVs928Ffo7mC/V3gW8Dv+hVW5KkcddlH3o1cBqwG3gAOHuusu2h3wU8Dvir9jfhHVV1HpIkSdII62ZQeqaFVzZ3kWe2RVseKZvkLcBZwP3AC2Z68STn0lx9zbp167oIV9JiVNXVNF+aO9O2djx/G/C2fsclLVNPSnIrzSDU+6rqrZ0727sSLqEZxHoAeGVVXdfuO6Xd96iyST4MPKM9xGHA/6mqZydZTzPv5a3tPge2pB7rog8t4Pxuy7bpT+9xmJIkSdLAdTOn9JIt2lJVF1XVUTQLtlww04u7aIskaTk6cOAAwDp6PD1WVf3i1PyVwEeBj3Uc77aOuS0dkJYkSZIkDUQ3V0ovZtGWVV2UBfgg8OfA73QRjyRJI2/nzp0AD/Z6eqypgu1V1i8DfnLpayNpnBx9x1WPTljxlO4LT5zd22AkSZI0kroZlH5k4RVgL83CKy+flmc7cEH7pXgz7aItSfbNVjbJxqr6p7b8S4AvL7o2ktQLu94/6Ag0Bvbu3QvwUEdSz6bHap0E3NPR1wJsSPIF4JvAb1fVZw+6ApIkSZIkHaR5B6WXcNGWtyZ5BvAw8M+AtxFLGhqfu/0bgw5By9ws64P2ZHqs1pnAhzq27wbWVdV9SZ4LfDzJcdMWHnYtB0mSJC1bs63LMi3P84E/BA4Bvl5Vz+trkNKY6OZK6aVatOXnFhSpJEnLyNq1a6GZ5uqRJHo0PVaSlcBLgedOpVXVg8CD7fPPJ7kNOAbY1fmCVXUpcCnAxMTEjCPnkiRJ0qjpWJflRTR/Z1+bZHtVdU6BdxjwHuCUqrojyVMHE620/HWz0KEkSeqxTZs2ARyaZEOSVTRTXG2flm07cFYaJ9JOj0XH1FqzlH0h8OWqmpxKSLK6/UOcJE+jWTxxzxJVT5IkSRo2J9Cuy1JVDwFT67J0ejnwsaq6A6Cq7u1zjNLY6OpKaUmS1FsrV64EuIPeT48FzSB159QdAD8B/G6S/cAB4Lyqcp4aSZIkjYtu1mU5Bjgkyd8CTwQuqaor+hOeNF4clJYkaXDur6qJzoTFTo/V7nvlDGkfBT66mGAlSZKkEdbNuiwraabAOxn4d8A/JtlRVV95zMFci0VaFKfvkCRJkiRJ0nI323ot0/P8ZVX9W1V9HbgGeNZMB6uqS6tqoqomVq9evSQBS8uZg9KSJEmSJEla7uZblwXgE8BJSVYmeTzN9B639DlOaSw4fYckSZIkSZKWtdnWZelc06Wqbknyl8CNwMPA+6rqS4OLWlq+HJSWJEmSJEnSsjfTuiyda7q02xcDF/czLmkcOX2HJEmSJEmSJKlvvFJaWiKfu/0bPTnO5omeHEaSJEmSJEkaCl4pLUmSJEmSJEnqGwelJUmSJEmSJEl946C0pEdJckqSW5PsTnLhDPt/KcmN7eMfkjxrEHFKkiRJkiRpNDkoLekRSVYA7wZOBY4Fzkxy7LRstwPPq6pnAm8GLu1vlJIkSZIkSRplDkpL6nQCsLuq9lTVQ8A2YEtnhqr6h6r6l3ZzB7C2zzFKkiRJkiRphDkoLanTGuDOju3JNm025wB/saQRSZIkSZIk/f/s3Xu4XGV99//3x8TAr1UaSQJSdtIECGrkQYQdwvMoKoo1CWiMVQtRQA5PigbBVitQFGnVp4jYVotCIwZMCwQQ0LSNINoqeklOHOR8CMGGHQKJgAhNTcjO9/fHWjtOduawZmbtmVkzn9d1zZU9a933mu+a7O+ste+5D9ZVMjVKZ5hjVpK+lu6/R9KhtepK+rKkh9LyN0kam88pmVkTVGZblC0oHUXSKH12hf3zJa2WtHrTpk05hmhmZmZmZmZmZkVWs1E64xyzs4Cp6WM+cGmGurcCB6Xz0j4CnNv02ZhZswaAiSXP+4AnhxeSdDBwOTAnIp4pd6CIWBgR/RHRP2HChBEJ1szMzMzMzMzMiidLT+mac8ymzxdHYjkwVtI+1epGxA8iYlta3/PSmnWGVcBUSVMkjQGOA5aWFpA0CbgROCEiHmlDjGbdZI8RGIl0gaT1ku5OH7NL9p2bln9Y0rtG/vTMzMzMzMzMdjU6Q5lyc8zOyFBm34x1AU4Bri334pLmk/S+ZtKkSRnCNbNGRcQ2SWcAtwCjgEURcb+k09P9lwHnA+OAb0gC2BYR/e2K2ayoBgcHASaRjCQaAFZJWhoRD5QUKx2JNINkJNKMkpFI76xQ9+8j4uLS10tHKh0HvB74Q+CHkg6MiMGROkczMzMzMzOzcrI0SmeZY7ZSmZp1JZ0HbAOuKvfiEbEQWAjQ399fdm5bM8tPRCwDlg3bdlnJz6cBp7U6LrNus3LlSoAtEbEWQNLQaKLSRukdI5GA5ZKGRiJNJh2JVKXucHOAJRGxBXhc0hqSEU2353piZmZmZmZmZjVkaZTOMsdspTJjqtWVdBJwLPCO9A9uM7OmXL1iXdPH2H/dszlEYlbd+vXrAbaWbMpzJNIZkk4EVgOfjIjn0jrLyxxrJx6hZGZmZmbdStJM4KskI4Mvj4gLK5SbTnLv/KcR8Z0WhmjWM7LMKV1zjtn0+Ynp3JdHAM9HxIZqddMPgrOB90TE5pzOx8zMrBAqfBebx0ikS4H9gUOADcBXahxreFxepNTMzMzMuk7JFHizSKbQOz6d4q5cuS+RTGtpZiOkZk/pjHPMLgNmA2uAzcDJ1eqmh74E2A24NZ2XdnlEnJ7nyZmZmXWqvr4+SEYU7dhEDiORIuLpoY2Svgn8W41jmZmZmZn1gsPJNgXex4EbgOmtDc+st2SZviPLHLMBLMhaN91+QF2RmpmZdZHp06cD7C5pCrCeZDTRvGHFlpJMxbGEZHqO5yNig6RNpCORhteVtE86WglgLnBfybGulvR3JAsdTgVWjtT5mZmZdbNaUwBIei1wBXAocN7wBYjNrC1qTYGHpH1J7qHfTo1GaU97Z9acTI3SZmZmlq/Ro0cDrCP/kUgXSTqEZGqOXwJ/lta5X9J1JD1BtgELImKwBadq1jMyNFIp3T+bJKc/EhF3Vqsr6QPABcDrgMMjYnVrzsbMKimZAuCdJI1aqyQtjYjS3pbPAmcC721DiGZWXpbp7P4BODsiBtNR/RVFxEJgIUB/f7/XSbORt/qKdkeQKzdKm5mZtc/zEdFfuiGHkUgnVHqxiPgi8MWGozWzijI2Us0iGaUwlaRn1qXAjBp17wPeB/xTy07GzGqpOQVARGwENko6pj0hmlkZWaaz6weWpA3S44HZkrZFxHdbE6JZ78iy0KGZmZmZmVW3o5EqIrYCQ41UpeYAiyOxHBgraZ9qdSPiwYh4uHWnYWYZlJsCYN9GDyZpvqTVklZv2rSp6eDMrKJVpFPgSRpDMgXe0tICETElIiZHxGTgO8DH3CBtNjLcKG1mZmZm1rwsjVSVyjTdwOVGLbOWyjIFQGYRsTAi+iOif8KECU2EZWbVRMQ2YGgKvAeB64amzxuaQs/MWsfTd5iZmZmZNS9LI1WlMk03cHleS7OWyjIFgJl1oHJT4JVOnzds+0daEZNZr3JPaTMzMzOz5mVppKpUxg1cZsVScwoAMzMzq849pc3MzMzMmrejkQpYT9JINW9YmaXAGemiaDNIFjvdIGlThrpm1iEiYpukoSkARgGLhqYASPdfJunVwGpgD2C7pE8A0yLiN20LfJirV6zL5TjzZkzK5ThmZtZb3ChtZmZmZtakLI1UJMOFZwNrgM3AydXqAkiaC/wjMAH4d0l3R8S7Wnt2ZjZcrSkAIuIpklEPZmZmVoYbpc2sq+y/7vp2h2BmZj0qQyNVAAuy1k233wTclG+kZmZmZmbt5TmlzczMzMzMzMzMzKxl3FPazMzMGrbT6IRRezZ+oP6Tmw/GzMzMzMzMCsE9pc3MzMzMzMzMzMysZdwobWZmZmZmZmZmZmYt40ZpM9uJpJmSHpa0RtI5Zfa/VtLtkrZI+lQ7YjQzMzMzMzMzs+LynNJmtoOkUcDXgXcCA8AqSUsj4oGSYs8CZwLvbUOIZmZmZmZmZmZWcJkapSXNBL4KjAIuj4gLh+1Xun82sBn4SETcWa2upA8AFwCvAw6PiNV5nJCZNeVwYE1ErAWQtASYA+xolI6IjcBGSce0J0QzMzPrJCsefzZz2ccG11XcN2/GpDzCMTMzM7MCqNkonbHn5CxgavqYAVwKzKhR9z7gfcA/5Xg+ZtacfYEnSp4PkOR03STNB+YDTJrkPzKLZv9119cuNGrP6vv7T84nmO62h6SHyfdL3y8D7wa2Ao8BJ0fEryVNBh4EHk4PvzwiTh/Z0zMzMzMzMzPbVZY5pXf0nIyIrcBQz8lSc4DFkVgOjJW0T7W6EfFgRDyMmXUSldkWjRwoIhZGRH9E9E+YMKHJsMy6z+DgIMAkki92pwHHS5o2rFjpl77zSb70Lf3CuFzdW4GDIuJg4BHg3JLjPRYRh6QPN0ibmZmZWU/JsIbShyTdkz5+LukN7YjTrBdkaZQu13Ny34xlstStStJ8Saslrd60aVM9Vc2sfgPAxJLnfcCTbYrFrKutXLkSYMsIfOn7g4jYltZfTpLHZmZmZmY9rUbHjiGPA29NO3h8HljY2ijNekeWRuksPScrlWm616V7W5q11CpgqqQpksYAxwFL2xyTWVdav349JFNsDBmJL31PAb5f8nyKpLsk/UTSkeXi8pfBZmZmZtalas4EEBE/j4jn0qfu4GE2grIsdJil52SlMmMy1DWzDhER2ySdAdxCMk/tooi4X9Lp6f7LJL0aWA3sAWyX9AlgWkT8pm2BmxVQRNnvaHP70lfSecA24Kp00wZgUkQ8I+kw4LuSXj88dyNiIWmPkP7+/oam7zEzM2+jJ68AACAASURBVLPOl2kNkSxmfDKf45iNvHrXUDqVnTt4mFmOsjRK7+g5Cawn6Tk5b1iZpcAZkpaQJPTzEbFB0qYMdc2sg0TEMmDZsG2Xlfz8FP622KxpfX19kHx5u2MTOX3pK+kk4FjgHZG2fkfEFmBL+vMdkh4DDiT5ksnMzMzMrNtlHs0v6SiSRuk3VzyYNJ9k3RcmTZqUR3yZXL1iXS7HmTejdTFbPlY8/my7Q8hVzUbpLD0nSRqwZgNrgM3AydXqAkiaC/wjMAH4d0l3R8S78j5BMzMbGbUuiI8NZrtZ6tWboenTpwPsnveXvpJmAmeTzIW3eehAkiYAz0bEoKT9SBZPXDuS52hmZmZm1kEyraEk6WDgcmBWRDxT6WAeYWjWnCw9pbP0nAxgQda66fabgJvqCdbMzKxbjB49GmAdOX/pC1wC7AbcKglgeUScDrwF+BtJ24BB4PSI6K6v2s3MzMzMKqs5E4CkScCNwAkR8UjrQ7SutPqKdkfQkTI1SpuZmdmIeD4i+ks35PCl7wEVyt8A3NBUtGZmTao6h+2oPatX7j8532DMzKynZJwJ4HxgHPCNtIPHtuH362aWDzdKm5mZmZmZmZlZ18swE8BpwGmtjsusF72s3QGYmZmZmZmZmZmZWe9wT2kzMzPLRenilzOm1BiGb2ZmZmZmlrp6xbpcjjNvxqRcjmMjz43SZmZmZmZmZmZmPcSNwNZubpQ2s46Q1wVx/1yOYmZmZmZmZma15PW3fF46sbG9dESp/Y7nlDYzMzMzMzMzMzOzlnFPaTMzMzMzMzMzM7Mhq69odwRdz43SZmZmZmZmZmZmVnj7r7s+l+OsyOUoVo0bpc3MzMzMzMzMzAogr0bXxyZ9IJfjmDXKc0qbmZmZmZmZmZmZWcu4p7SZmZmZmZmZmVkPcY9razc3SpuZWVtdvWJdLseZN2NSLscxM7P2WPH4s1X3PzaY7Xrh64GZmVnr5NW4bb3HjdJm1lF8QTMzM7NyMt8jjNpz5+f9J+cfjJmZmZk1xXNKm5mZmZmZmZmZmVnLZOopLWkm8FVgFHB5RFw4bL/S/bOBzcBHIuLOanUl7QlcC0wGfgl8MCKea/6UzKwZzeS7mdVtD0kP06Lrq6RzgVOBQeDMiLhlpE/QrJf4ntmsd/ie+Xc8FZsViXPXrHPUbJSWNAr4OvBOYABYJWlpRDxQUmwWMDV9zAAuBWbUqHsO8KOIuFDSOenzs/M7NTOrVzP53upYzYYr2h9Eg4ODAJOAabTg+ippGnAc8HrgD4EfSjowIgZH4vxK54adMWXPKiXNuoPvmc16h++Zd5bb9HszPpnPccwqcO6adZYsPaUPB9ZExFoASUuAOUBp0s4BFkdEAMsljZW0D0mPjkp15wBvS+t/G/gxvsE2a7eG8z0iNrQ+XOtkzf6B0u2rOK9cuRJgSwuvr3OAJRGxBXhc0hqSnL995M7SrKf4nrlD7LJg4uNfaeg4TX+h5rmsu5nvmc2Kyblr1kGyNErvCzxR8nyAXb8lKldm3xp19x5K6ojYIGmvci8uaT4wP336YjrMuZrxwK9qlMngU80fIpFTPLnopFjA8VRTEkum38U/yul1m8n3nS7SFXK3k97jdvL78DtV3ovcPofr8qHWvdSrSHosDxnp6+u+wPIyx9pJ+667pzR/iPbo9Xzu5fMffu3t9Hvmbvm/KtB5VP1cK9B5VFXE88jjvjm3e2ao+9pbxPccMsXdnnu/Grr4/e5I1eIueu62UlH///Pi82/Z+TfXXpWlUVpltkXGMlnqVhURC4GFWctLWh0R/fW8xkjqpHg6KRZwPNW0MZZm8n3nDWVyt5Pe43by+/A7vfxeSPoA8K5hm0fy+tpw7lY9aA//H4LPv9fPf5iOvmfulv8rn0dn6ZbzaEBu98xQ37W3qO+5424tx135Jcpsa0nutlJR///z4vMvzvm/LEOZAWBiyfM+4MmMZarVfTodrkj678bsYZvZCGkm382sPq2+vjp3zUaW75nNeofvmc2Kyblr1kGyNEqvAqZKmiJpDMkiSUuHlVkKnKjEEcDz6TDDanWXAielP58EfK/JczGz5jWT72ZWn1ZfX5cCx0naTdIUksVbVo7UyZn1IN8zm/UO3zObFZNz16yD1Jy+IyK2SToDuAUYBSyKiPslnZ7uvwxYBswG1gCbgZOr1U0PfSFwnaRTgXVAXitaddrQiU6Kp5NiAcdTTVtiaSbfM+qk97id/D78Ts++F62+vqbHvo5kIZdtwIKIGMzhVHr2/zDl8zegEPfM3fJ/5fPoLN1yHnVpwT1zNUV9zx13aznuMtqcu61U1P//vPj8C0LJgqJmZmZmZmZmZmZmZiMvy/QdZmZmZmZmZmZmZma5cKO0mZmZmZmZmZmZmbVMoRqlJX1A0v2StkvqH7bvXElrJD0s6V0l2w+TdG+672uSlG7fTdK16fYVkiY3GdshkpZLulvSakmHNxpbXiR9PH3N+yVd1AHxfEpSSBrfzlgkfVnSQ5LukXSTpLHtjKdMfDPT118j6ZyRep1W69bzqoekiZL+U9KDaV6e1e6Y2knSKEl3Sfq3dsdijenWvJa0SNJGSfeVbNtT0q2SHk3/fVXJvrZfO/JS6XOqV86/GxUtTyX9Mv29uVvS6nRb3b9/bYi7az43KpzLBZLWp/8vd0uaXYRz6TZFy+ch5fK6E9Wbx52i3pztBI3cb1h9VKX9rJsV9XMyD+U+CzpeRBTmAbwOeA3wY6C/ZPs04BfAbsAU4DFgVLpvJfC/AQHfB2al2z8GXJb+fBxwbZOx/aDk2LOBHzcaW07v1VHAD4Hd0ud7tTmeiSSLCfwXML7NsfwxMDr9+UvAl9oZz7DYRqWvux8wJo1n2kjlVKse3XpeDbwP+wCHpj+/EnikF9+HkvfjL4CrgX9rdyx+NPT/17V5DbwFOBS4r2TbRcA56c/ndNK1I+dzL/s51Svn322PIuYp8Muhe8WSbXX//rUh7q753KhwLhcAnypTtqPPpZseRcznkth3yetOfNSTx530qCdnO+VR7/2GHw29x2Xbz7r5UeTPyZzOf5fPgk5/FKqndEQ8GBEPl9k1B1gSEVsi4nGSVVIPl7QPsEdE3B7J/9Bi4L0ldb6d/vwd4B1NfnsfwB7pz38APNlEbHn4KHBhRGwBiIiNbY7n74FPk7xPQ9oSS0T8ICK2pU+XA33tjGeYw4E1EbE2IrYCS9K4iq5bz6suEbEhIu5Mf34BeBDYt71RtYekPuAY4PJ2x2IN69q8jojbgGeHbS69b/g2O99PtPvakZsqn1M9cf5dqFvytK7fvzbE11WfGxXOpZKOPpcu0y353LHqzOOOUWfOdoQG7jesTlXaz7pZT39OFvGzoFCN0lXsCzxR8nwg3bZv+vPw7TvVSRsonwfGNRHDJ4AvS3oCuBg4t4nY8nAgcKSSqUl+Iml6u+KR9B5gfUT8Ytiudr03pU4h6bnRKfFUiqHouvW8GqZkyqA3AivaG0nb/APJF1Xb2x2INazX8nrviNgAyR9SwF7p9k64doyIYZ9TPXf+XaKIeRrADyTdIWl+uq3e379O0W15c4aS6e8WlQypL+q5FFGn/75XUy6vi6JSHhdBuZztOBnvN8yyKPLnZE8a3e4AhpP0Q+DVZXadFxHfq1StzLaosr1anYZiA94B/HlE3CDpg8C3gKMbjC2TGvGMBl4FHAFMB66TtN9IxVMjlr8imTJjl2ojEUuteIZ+jySdB2wDrhrpeOrQytdqpW49r4ZIegVwA/CJiPhNu+NpNUnHAhsj4g5Jb2t3PNYw53WiE64duRv+OVVlMFlXnn8XKeL/w5si4klJewG3SnqoStkinh8UM28uBT5PEs/nga+QdO4o4rkUVZHf013yOu3RZyOnUs52lDruN6yMBtvPulmRPyd7Usc1SkfE0Q1UGyCZs3hIH8n0GQP8bmqG0u2ldQYkjSaZcqNqN/dqsUlaDAwtWnY9vxuS3khsmdSI56PAjelwuZWStgPjRyqeSrFI+l8k88v9Ir3A9AF3KlkIsi3vTRrXScCxwDvS94iRjKcOlWIoum49r7pJejnJjddVEXFju+NpkzcB70kXXNkd2EPSv0TEh9scl9Wn1/L6aUn7RMSGdFj60LRYnXDtyFWFz6meOf8uU7g8jYgn0383SrqJZChuvb9/naJr8iYinh76WdI3gaFFigt3LgXW6b/vFVXI66I0SlfK445WJWc7Rp33G1ZGg+1n3aywn5O9qlum71gKHCdpN0lTgKnAynS4xwuSjkjniz4R+F5JnZPSn98P/EdJ42QjngTemv78duDRJmLLw3fTOJB0IMkk779qdTwRcW9E7BURkyNiMsmHxKER8VSrYxkiaSZwNvCeiNhcsqtd/1elVgFTJU2RNIZkEc6lI/RardSt51WX9PfnW8CDEfF37Y6nXSLi3IjoSz8TjiP5/HWDdPH0Wl6X3jecxM73E+2+duSmyudUT5x/FypUnkr6fUmvHPqZZKTdfdT5+9faqKvqmrxJG4eGzCX5f4ECnkuBFSqfh1TJ66KolMcdrUrOdoQG7jfMsijk52RPiw5YbTHrg+TDdADYAjwN3FKy7zySVTYfpmRlZ6Cf5AP4MeASQOn23Ul6NK8huXndr8nY3gzcQbK65wrgsEZjy+m9GgP8S3r8O4G3tzOektf4JSUrL7fpvVlDMs/Q3enjsk54b0peazbJ6sOPkQy7aVvO+bxyfw/eTDJ86J6S37/Z7Y6rze/J24B/a3ccfjT8/9eVeQ1cA2wAXkrvO04lWXfiRyRfOv8I2LOkfNuvHTmee9nPqV45/258FClPgf1I7qV/Adw/FG8jv39tiL1rPjcqnMs/A/emnw1LgX2KcC7d9ihSPpfEXDavO/FRbx53yqPenO2ERyP3G37U/R5XbD/r5kcRPydzPPddPgvaHVOtx1ADrZmZmZmZmZmZmZnZiOuW6TvMzMzMzMzMzMzMrADcKG1mZmZmZmZmZmZmLeNGaTMzMzMzMzMzMzNrGTdKm5mZmZmZmZmZmVnLuFHazMzMzMzMzMzMzFrGjdJmZmZmZmZmZmZm1jJulLaqJJ0habWkLZKuLNl+hKRbJT0raZOk6yXt08ZQzaxEldydlm5/Ln38UNK0NoZqZsNUyt9hZT4nKSQd3eLwzKyCKtfeyWm+vljy+GwbQzWzEtWuu5J+T9I3JP1K0vOSbmtTmGY2TJXr7oeGXXM3p9fhw9oYrpXhRmmr5UngC8CiYdtfBSwEJgN/BLwAXNHSyMysmkq5+yTwfmBPYDywFFjS2tDMrIZK+QuApP1J8nhDK4Mys5qq5i4wNiJekT4+38K4zKy6arm7kOS++XXpv3/ewrjMrLqyuRsRV5Vcb18BfAxYC9zZhhititHtDsA6W0TcCCCpH+gr2f790nKSLgF+0trozKySKrn7a+DX6T4Bg8AB7YjRzMqrlL8lLgHOBr7RyrjMrLoMuWtmHahS7kp6DfAeoC8ifpNuvqP1EZpZOXVcd08CFkdEtCQwy8w9pS0vbwHub3cQZpaNpF8DvwX+Efh/bQ7HzDKS9AFga0Qsa3csZla3/5I0IOkKSePbHYyZ1TQD+C/gr9PpO+6V9CftDsrMspP0RyTtVYvbHYvtyo3S1jRJBwPnA3/Z7ljMLJuIGAv8AXAGcFebwzGzDCS9guRLpE+0OxYzq8uvgOkkU94dBrwSuKqtEZlZFn3AQcDzwB+S3Dd/W9Lr2hqVmdXjROCnEfF4uwOxXblR2poi6QDg+8BZEfHTdsdjZtlFxH8DlwGLJe3V7njMrKa/Bv7ZN9VmxRIRL0bE6ojYFhFPkzRs/bGkPdodm5lV9T/AS8AXImJrRPwE+E/gj9sblpnV4UTg2+0Owspzo7Q1LB0G8UPg8xHxz+2Ox8wa8jLg94B92x2ImdX0DuBMSU9JegqYCFwn6ew2x2Vm9Rma01JtjcLMarmn3QGYWeMkvYlklMN32h2LleeFDq0qSaNJfk9GAaMk7Q5sA/YG/gP4ekRc1sYQzayMKrl7FMkw4nuA3ydZrfg54ME2hWpmw1TJ33cALy8pugr4C5IRS2bWZlVy9zCSRYYfBV4FfA34cUQ8365Yzex3quTubcA64FxJf0syx/Tb8LSVZh2hUu5GxLa0yEnADRHxQrtitOrcU9pq+QzJsKVzgA+nP38GOA3YD/icpBeHHu0L08yGqZS7Y4FrSObGeww4AJgZEb9tU5xmtquy+RsRz0TEU0MPYBB4LiJ8/TXrDJWuvfsBNwMvAPcBW4Dj2xSjme2q0nX3JWAOMJvk3vmbwIkR8VC7AjWznVS67pI2UH8QT93R0RQRtUuZmZmZmZmZmZmZmeXAPaXNzMzMzMzMzMzMrGXcKG1mZmZmZmZmZmZmLeNGaTMzMzMzMzMzMzNrGTdKm3UxSTMlPSxpjaRzyuyXpK+l+++RdGjJvkWSNkq6r0y9j6fHvV/SRSN9HmZmZmZmZmZm1j1GtzuAeowfPz4mT57c7jDMOs4dd9zxq4iYULpN0ijg68A7gQFglaSlEfFASbFZwNT0MQO4NP0X4ErgEmDxsOMeRbIK9cERsUXSXrXic+6alVcudzuJc9essk7OX+euWWWdnLvg/DWrxLlrVkzVcrdQjdKTJ09m9erV7Q7DrONI+q8ymw8H1kTE2rTMEpLG5NJG6TnA4ogIYLmksZL2iYgNEXGbpMlljvtR4MKI2AIQERtrxefcNSuvQu52DOeuWWWdnL/OXbPKOjl3wflrVolz16yYquWup+8w6177Ak+UPB9It9VbZrgDgSMlrZD0E0nTyxWSNF/SakmrN23aVGfoZmZmZmZmZmbWrdwobda9VGZbNFBmuNHAq4AjgL8ErpO0y3EiYmFE9EdE/4QJHTvKyszMzMzMzMzMWsyN0mbdawCYWPK8D3iygTLljntjJFYC24HxTcZqZmZmZmZmZmY9olBzSltve+mllxgYGOC3v/1tu0Npm913352+vj5e/vKXZym+CpgqaQqwHjgOmDeszFLgjHS+6RnA8xGxocZxvwu8HfixpAOBMcCv6jgN6zHO3bpz16wjOHcTzl8rGuduwrlrRePcTTh3rWicu4lGcteN0lYYAwMDvPKVr2Ty5MmUmS2i60UEzzzzDAMDA0yZMiVL+W2SzgBuAUYBiyLifkmnp/svA5YBs4E1wGbg5KH6kq4B3gaMlzQAfC4ivgUsAhZJug/YCpyULpRoVpZzt77cNesUvZ674Py1YnLuOnetmJy7zl0rJudu47nrRmkrjN/+9rc9neSSGDduHPUsGhgRy0ganku3XVbycwALKtQ9vsL2rcCHMwdhPc+5W3/umnWCXs9dcP5aMTl3nbtWTM5d564Vk3O38dz1nNJWKL2c5ODzt+Lq9d/dXj9/Ky7/7vo9sGLy763fAysm/976PbBi8u9tY+9B1/WUvnrFulyOM2/GpFyOY2YZrb6i8br9J9cuY2ZWdM18TpbyZ6Z1O+eKdbtGf8f9O21mvcL3AoXQdY3S1jvy+gJiSCd8EXHeeeexePFinnvuOV588cV2h2M2Ipy71nPyuiluM+euWTE5d82KyblrVkzO3ew8fYdZB3n3u9/NypUr2x2GmdXJuWtWTM5ds2Jy7poVk3PXrJhGKnfdU9oso89+9rOMHz+es846C0i+Kdp7770588wzc3uNI444IrdjmVnCuWtWTM5ds2Jy7poVU7flrqSZwFeBUcDlEXHhsP1zgM8D24FtwCci4mdZ6toI6pIRhq1U5Nx1o7RZRqeeeirve9/7OOuss9i+fTtLliwp+03RkUceyQsvvLDL9osvvpijjz66FaGaWQnnrlkxOXfNism5a1ZM3ZS7kkYBXwfeCQwAqyQtjYgHSor9CFgaESHpYOA64LUZ65p1jCLnrhulzTKaPHky48aN46677uLpp5/mjW98I+PGjdul3E9/+tM2RGdmlTh3zYrJuWtWTM5ds2Lqstw9HFgTEWsBJC0B5gA7GpYjonRi3N8HImtds05S5Nx1o7RZHU477TSuvPJKnnrqKU455ZSyZbJ++zQ4OMhhhx0GwHve8x7+5m/+ZmSCNjPnrllBdVPuZhhG/FrgCuBQ4LyIuLhk31jgcuAgkj+aT4mI21sVe8vkNGR3xePP5nKcGf25HKYndWru3nzzzQAHSVpD+TwUSZ7OBjYDH4mIOyVNBBYDryYZ6r8wIr6a1tkTuBaYDPwS+GBEPJfuOxc4FRgEzoyIWxoO3qwFOjV3G7Av8ETJ8wFgxvBCkuYCfwvsBRxTT920/nxgPsCkSe1fjM56V1Fz143SZnWYO3cu559/Pi+99BJXX3112TJZv30aNWoUd999d57hmVkFzl2zYuqW3M04FPhZ4EzgvWUO8VXg5oh4v6QxwO81G1OeK8N3wqrw1lnqyd1nXtyyy77h2370sxUV95U+H/eK3SrGNDg4yIIFCwAeAfopn4ezgKnpYwZwafrvNuCTaQP1K4E7JN2a1j0H+FFEXCjpnPT52ZKmAccBrwf+EPihpAMjYrBikGZt1i3XXUBltsUuGyJuAm6S9BaS+aWPzlo3rb8QWAjQ399ftoxZKxQ1d90obYXVjj+AxowZw1FHHcXYsWMZNWpU7sf/9Kc/zdVXX83mzZvp6+vjtNNO44ILLsj9dczaybnbOC/YYu3k3G1KlmHEG4GNko4prShpD+AtwEfScluBrc0GtP+665s9xO/M+GR+x7LcdXru7rb1ubqP/5kLPs/137mJzZs3c/CBUzjphHn81dmfIunIXN7KlSs54IADWLt27daI2FphSP4cYHFEBLBc0lhJ+0TEBmADQES8IOlBkt6UD6R13pbW/zbwY+DsdPuSiNgCPJ72zj4c6L5RDjYiOj13G9HC6+4AMLHkeR/wZKXCEXGbpP0lja+3rtlwzt3s3ChtVoft27ezfPlyrr8+xz/kSlx00UVcdNFFI3Jss17WDbnrBVusF3VD7qYyDwUuYz9gE3CFpDcAdwBnRcR/lxbyEOIRkNN0IvSfnMthsvZuP3C3bWV7Hw+p1ps4LyOdu1+44LN84YLP1lVn/fr1TJxY2s5UNg/L5eq+pA3SAJImA28Ehrpv7502WhMRGyTtVXKs5WWOtQvnr3WKLrrurgKmSpoCrCcZtTCvtICkA4DH0vvmQ4ExwDPAr2vVNes0Rc3dTI3SGXpmlZ17q1pdSdcCr0kPMRb4dUQc0vQZmY2QBx54gGOPPZa5c+cyderUdodjZhl1Ue56wRbrKV2Uu1DHUOAyRpPMM/3xiFgh6ask0wPs1CLnIcT589zU1VVq+H74oQeZ94G5zD52DnvuM6lqAznAyDeRJ5LOz7tuHva8aq5KegVwA8lIpN/UeElPAWCF0k3X3YjYJukM4BaStqhFEXG/pNPT/ZcBfwKcKOkl4H+AP01HSZSt25YTMcugyLlbs1E6Y++qsnNvVasbEX9a8hpfAZ7P6ZzMRsS0adNYu3Ztu8Mwszp1Ue62ZMEWs07RRbkLzQ0FHgAGImKoV+Z3SBqlO0eHLVDYrbJOuTJ6yptrTIlReYqLPLzmta/jjnsfGtHXaERfXx9PPPHETpvYNQ8r5qqkl5M0SF8VETeWlHl6aIoPSfsAG2sdy6wTddl1l4hYBiwbtu2ykp+/BHwpa12zTlXk3M3SUzpL76qyc2+RrEBctW7ay/qDwNubPx0zM7Ou1ZIFWzyE2GxE1BxGXElEPCXpCUmviYiHgXfQYaMc3JhsRTB9+nQeffRRgDHpgqHl8nApcEb6d+sM4Pm0sVnAt4AHI+LvytQ5Cbgw/fd7JduvlvR3JAsdTgVW5n9mZmZmxZSlUTpL76pKc29lqXsk8HREPJolYDPLrsmpdxYBxwIbI+KgMsf+FPBlYEJE/GpET8TMoEULtngIsVn+sgwjlvRqYDWwB7Bd0ieAaekUAR8Hrkob0tYC+UxSbD2p1nQaWTWyQGE7jR49mksuuYRjjjnmQOBByg/nX0ZyX7yG5N54KNfeBJwA3Cvp7nTbX6W9KS8ErpN0KrAO+EB6vPslXUfyJdI2YEFEDLbiXM3MWi6vdRisp2RplM7Su6pSmSx1jweuqfji7rFl1pBmpt5J910JXAIsLnPsielxs624Y2Z58IItZgWWYRjxUyRfGJWrezfQpTMTd7+sCxTWsn8uR+lts2fPBrgvInbk07A8DGDB8HoR8TPK/21LRDxDMoKh3L4vAl9sLmozM7PulKVROkvvqkplxlSrK2k08D7gsEov7h5bZg1reOqdiNiQ9rKcXOHYfw98mt8NTzSzEeYFW8zMiinrXNBmZmZmvSRLo3SWOfAqzb21qUbdo4GHImKgyfOwXpT38JD+1o+Eve222/jEJz7BPffcw5IlS3j/+9+f5+GbmXpnQ6WDSnoPsD4ifpHM/mFWJ+duw7xgi7WVc9eskEbfd+3OG0b9XlPH2/KGE5uq34if/fx2zjnvfO67/0GuvPwy3vueY1seg1nL+bprVkzO3cxeVqtARGwDhnpXPQhcN9Qza6h3FskfuWtJ5t76JvCxanVLDn8cVabuMOt2kyZN4sorr2TevBEZRd/M1DvlDyj9HnAecH7NF5fmS1otafWmTZtqFTcrlBHOXTMbIc5dMxg9uLmpx25bn2v5fNIT+/q47JKv8sE/mdvS1zWz5vi6a1ZMrcrdLD2ls/TMKjv3VqW6Jfs+kjVQs3b77Gc/y/jx4znrrLMAOO+889h7770588wzGz7m5MmTAXjZy2p+P9SIZqbeqWR/YAow1Eu6D7hT0uHpXJg7eOod6xQFzF0zw7lrVlSf/39fYty4PfnYn/1fAP76C3/LXhMm8NE/O63hY/7RpOR2Vc5dsxHj665ZMRU5dzM1SpsZnHrqqbzvfe/jrLPOYvv27SxZsoSVK1fuUu7II4/khRde2GX7xRdfzNFHH92KUIc0PPVOpQNGtcW2xQAAIABJREFUxL3AXkPPJf0S6I+IX+Ucu1luCpi71im8inhbOXfNiunED8/jQyedwsf+7P+yfft2brjpe/znrbv2UfrjY+bw4ov/vcv2L/71+Rz1tre0IlQzK+HrrlkxFTl33ShtltHkyZMZN24cd911F08//TRvfOMbGTdu3C7lfvrTn7Yhul1lXBRtGTCbZOqdzcCOyYokXQO8DRgvaQD4XER8q7VnYda8ouWumSWcu2bF9EeTJrLnq/bkF/fcy8ZNmzj4fx3EuD333KXcD/7d62WbdRJfd82Kqci560ZpszqcdtppXHnllTz11FOccsopZct00rdPTU69c3yG409uMkSzliha7ppZwrlrVkwnnTCPq665lqc3buKED5W/pXRPabPO4+uu2TBZRk6OPgj+u2Tw+NYXdy0z5hX5xVRGUXPXjdJmdZg7dy7nn38+L730EldffXXZMp347ZNZr3PumhWTc9esmN59zCy+8LdfZtu2l1i08Btly7intFnn8XXXrJiKmrtulLbi6j+5dpmcjRkzhqOOOoqxY8cyatSopo+3atUq5s6dy3PPPce//uu/8rnPfY77778/h0jNOphz16yYnLtmhbTtoD9t+WuOGTOGtxz5f/iDPf4gl9y94867mXfiKfz6+V/z/Vtu5YsXfplVP/9JDpGadTBfd82K6Q1lRgj9/vgRfcmi5q4bpc3qsH37dpYvX87111+fy/GmT5/OwMBALscys8qcu2bF5Nw1K6bt27ezavWdLF60MJfjHXboITx83525HMvMKvN116yYipq7bpQ2y+iBBx7g2GOPZe7cuUydOrXd4ZhZRs5ds2GyzI0Hu86PN9wI9/hw7poV00MPPcwH5p3IscfM4oD992t3OGaWka+7ZsVU5Nx1o7RZRtOmTWPt2rXtDqNrrXj82YbrPja4bsfP82ZMyiMc6yLOXbNicu6aFdNrX/sa7r1zRbvDMLM6+bprVkxFzt2XtTsAs3pERLtDaKteP38rrl7/3e3187fi8u9ufu+BpJmSHpa0RtI5Zfa/VtLtkrZI+lSZ/aMk3SXp33IJyLpYOHcpn7s333wzwEFV8lCSvpbuv0fSoSX7FknaKOm+YXWulXR3+vilpLvT7ZMl/U/JvstyP0nrOs5dvwdWTP69bew9cKO0Fcbuu+/OM88807PJHhE888wz7L777u0Oxawuzl3nrhXT7vE/PPP8Cz2bu5Bf/koaBXwdmAVMA46XNG1YsWeBM4GLKxzmLODBpgKxnrB9y4s8/8J/O3eH5e7g4CALFiwAeITKeTgLmJo+5gOXluy7EphZ5rX+NCIOiYhDgBuAG0t2Pza0LyJOb/7MrJv1+j0z+L7Zisn3zI3nrqfvsMLo6+tjYGCATZs2tTuUttl9993p6+trdxhmdXHuOnetmPoGf8nARti06f8rX2C33sjpnPL3cGBNRKwFkLQEmAM8MFQgIjYCGyUdM7yypD7gGOCLwF80G4x1t5eeeoCNwK92ewWgdofTUrs9/dyOn4fn7sqVKznggANYu3bt1ojYWi4P0+eLI2lZWC5prKR9ImJDRNwmaXKl15Yk4IPA23M9KesZvmdO+L7ZiqbmPTP0xH1zI7nrRmkrjJe//OVMmTKl3WGYWZ2cu2b5aGbu/VIzpuyZqdzLGWTK4GOVCxxyci7x9Ih9gSdKng8AM+qo/w/Ap4FXViogaT5Jz04mTfL6Cj1t+0u89OQv2h1FWxzygU9W3Ld+/XomTpxYuqlcHpbL1X2BDRle/kjg6Yh4tGTbFEl3Ab8BPhMRPy1X0flr4Htms6Kqec8Mvm+uwI3SZmZmZiMor8ZkK7Ry3VUzjfGUdCywMSLukPS2SuUiYiGwEKC/v793x4+aVVBhWPXwjQ3nKnA8cE3J8w3ApIh4RtJhwHclvT4iflMmtrryt9HrSuni4OAFwq27SZoJfBUYBVweERcO2/8h4Oz06YvARyPiF+m+XwIvAIPAtojob1XcZr2k6xql9193fT4HmlH5W3YzMzMzszoMAKVdNPuAJzPWfRPwHkmzgd2BPST9S0R8OOcYzbpaX18fTzzxxE6b2DUPG8pVSaOB9wGHDW2LiC3AlvTnOyQ9BhwIrG4kfjPLrmQth3eS5PUqSUsjonS6nseBt0bEc5JmkXwxVDp64qiI+FXLgjbrQV7o0MzMzMxsZK0CpkqaImkMcBywNEvFiDg3IvoiYnJa7z/cIG1Wv+nTp/Poo48CjKmSh0uBE5U4Ang+IrJM3XE08FBEDAxtkDQhbRhD0n4kiyeuzeFUzKy2HWs5RMRWYGgO+R0i4ucRMTQR/XKSL6HMrIUyNUpLminpYUlrJJ1TZr8kfS3df4+kQ7PUlfTxdN/9ki5q/nTMzMzMzDpLRGwDzgBuAR4ErouI+yWdLul0AEmvljRAspDhZyQNSNqjfVGbdZfRo0dzySWXQNJbuWweAstIGo7XAN8EPjZUX9I1wO3Aa9L8PLXk8Mex89QdAG8B7pH0C+A7wOkR4fmczFqj0vzwlZwKfL/keQA/kHRHOue7mY2AmtN3ZBz2MIvkm9+pJMMdLgVmVKsr6SiSb6oOjogtkvbK88TMzMzMzDpFRCwjafAq3XZZyc9PUaOXVkT8GPjxCIRn1hNmz54NcF/p/LDD8jCABeXqRsTxlY4bER8ps+0G4IYmwjWzxmWeHz5tmzoVeHPJ5jdFxJNpO9Wtkh6KiNvK1PUipWZNyNJTuuawh/T54kgsB8ZK2qdG3Y8CF6ZzbRERG3M4HzMr0eQoh0WSNkq6b1idL0t6KC1/k6SxrTgXM8uU0x9Kc/MeST+X9IaSfb+UdK+kuyV5PkszMzMz61aZ5oeXdDBwOTAnIp4Z2h4RT6b/bgRuImnb2kVELIyI/ojonzBhQo7hm/WGLI3SWYY9VCpTre6BwJGSVkj6iaTp5V5c0nxJqyWt3rRpU4ZwzQx2GuUwC5gGHC9p2rBipaMc5pOMchhyJTCzzKFvBQ6KiIOBR4Bz843czMrJmNNDC7YcDHyeZMGWUkdFxCFeQdzMzMzMuljNtRwkTQJuBE6IiEdKtv++pFcO/Qz8MbBTRy0zy0eWRukswx4qlalWdzTwKuAI4C+B6yTtUt7fPJk1rJlRDqTDk3aZ9y4ifpDOjQleEMKslbxgi5mZmZlZDVnWcgDOB8YB3xg2knBv4GfpfPArgX+PiJtbfApmPaHmnNJkG/ZQqcyYKnUHgBvTebtWStoOjAfcHdosH+VGKszIUGZfIMsq4wCnANc2GqCZ1SVLTpeqtGBLAP8UEcN7UZuZmZmZdYUMazmcBpxWpt5a4A3Dt5tZ/rL0lK457CF9fmI6P+0RwPMRsaFG3e8CbweQdCBJA/avmj4jMxvSzCiH2geXzgO2AVdV2O+pd8zy1ciCLWeXbH5TRBxKMv3HAklvqVDXuWtmZmZmZmYjqmajdMZhD8uAtcAa4JvAx6rVTessAvZLF1FbApyU9po2s3w0M8qhKkknAccCH6qUt556xyx3XrDFzMzMzMzMukKW6TuyDHsIYEHWuun2rcCH6wnWzOqyY6QCsJ5kpMK8YWWWAmdIWkIyDcDQKIeKJM0k6X351ojYnH/YZlZBzZyutmAL8LKIeKFkwZa/aVnkBbXi8V2m1TczMzMzM7McZGqUNrPiiYhtkoZGKowCFg2Nckj3X0byhdFsklEOm4GTh+pLugZ4GzBe0gDwuYj4FnAJsBtwa7o26fKIOB0zG1EZc7p0wRaAbRHRT7Jgy03pttHA1V6wxczMzMzMzNrFjdJmXazJUQ7HV9h+QJ4xmll2XrDFzMzMzMw6ydUr1rH/uuZHGM6YsmcO0ViRZFno0MzMzMzMzMzMzMwsF+4pbWZmZmZmZmZmZoWX17ow7rk98twobWZmZtZDcrtR78/lMGZmZmZmHbfIeJ7x+L65PE/fYWZmZmY2wiTNlPSwpDWSzimz/7WSbpe0RdKnSrZPlPSfkh6UdL+ks1obuVn3uPnmmwEOqpKHkvS1dP89kg4t2bdI0kZJ9w2rc4Gk9ZLuTh+zS/admx7rYUnvGslzMzMzKxo3SpuZmZmZjSBJo4CvA7OAacDxkqYNK/YscCZw8bDt24BPRsTrgCOABWXqmlkNg4ODLFiwAOARKufhLGBq+pgPXFqy70pgZoXD/31EHJI+lgGkxz4OeH1a7xvpZ4GZmZnhRmkzMzMzs5F2OLAmItZGxFZgCTCntEBEbIyIVcBLw7ZviIg7059fAB4E9m1N2GbdY+XKlRxwwAEAWyvlYfp8cSSWA2Ml7QMQEbeRfHmU1RxgSURsiYjHgTUknwVmZmaGG6XNzMzMzEbavsATJc8HaKBhWdJk4I3AijL75ktaLWn1pk2bGgzTrHutX7+eiRMnlm4ql4eN5uoZ6XQfiyS9qt5jOX/NzKwXuVHazMzMzGxkqcy2qOsA0iuAG4BPRMRvdjlYxMKI6I+I/gkTJjQYpln3iiibcsM3NpKrlwL7A4cAG4Cv1Hss56+ZmfUiN0qbmZmZmY2sAaC0i2Yf8GTWypJeTtIgfVVE3JhzbGY9oa+vjyeeeGKnTeyah3XnakQ8HRGDEbEd+Ca/m6Kjqbw3MzPrdm6UNjMzMzMbWauAqZKmSBpDsvjZ0iwVJQn4FvBgRPzdCMZo1tWmT5/Oo48+CjCmSh4uBU5U4gjg+YjYUO24Q3NOp+YC95Uc6zhJu0maQrJ44socTsXMzKwruFHazMzMzGwERcQ24AzgFpKFCq+LiPslnS7pdABJr5Y0APwF8BlJA5L2AN4EnAC8XdLd6WN2m07FrLBGjx7NJZdcAnAgFfIQWAasJVmU8JvAx4bqS7oGuB14TZqfp6a7LpJ0r6R7gKOAPweIiPuB64AHgJuBBRExONLnaWZmVhSj2x2AmZmZmVm3i4hlJA1epdsuK/n5KZLh/cP9jPJz05pZnWbPng1wX0T0D20blocBLChXNyKOr7D9hEqvFxFfBL7YaLxmZmbdLFNPaUkzJT0saY2kc8rsl6SvpfvvkXRorbqSLpC03j0+zMzMzMzMzMzMzHpHzUZpSaOArwOzgGnA8ZKmDSs2i2SOrKnAfJIViLPU/fuIOCR9LMPMzMzMzMzMzMzMulqWntKHA2siYm1EbAWWAHOGlZkDLI7EcmBsuuBDlrpmZmZmZmZmZmZm1iOyNErvCzxR8nwg3ZalTK26Z6TTfSyS9KrMUZtZJk1OvbNI0kZJ9w2rs6ekWyU9mv7r3DUzMzMzM7OOkeFv4Q+lfwPfI+nnkt6Qta6Z5SNLo3S5hVUiY5lqdS8F9gcOATYAXyn74tJ8Saslrd60aVOGcM0Mmpt6J3UlMLPMoc8BfhQRU4Efpc/NzMzMzMzM2i7j38KPA2+NiIOBzwML66hrZjnI0ig9AEwsed4HPJmxTMW6EfF0RAxGxHbgmyRTfewiIhZGRH9E9E+YMCFDuGaWambqHSLiNuDZMsedA3w7/fnbwHtHJHozMzMzMzOz+tX8Wzgifh4Rz6VPl5O0V2Wqa2b5GJ2hzCpgqqQpwHrgOGDesDJLSabiWALMAJ6PiA2SNlWqK2mfiNiQ1p8L3IeZ5anc9DkzMpTZl2T0QiV7D+Vumud7lSskaT5J72smTZpUX+RmZmZmZmZmjcnyt3CpU4HvN1i35a5esS6X4+y/7vp8jpPLUawX1ewpHRHbgDOAW4AHgesi4n5Jp0s6PS22DFgLrCHp9fyxanXTOhdJulfSPcBRwJ/nd1pmRnNT7zTNoxzM8ue58czMzMzMasr8d66ko0gapc9uoK6nmzVrQpae0kTEMpKG59Jtl5X8HMCCrHXT7SfUFamZ1auZqXeqeXpopEM61cfGpiM1s5pK5rd7J0nurpK0NCIeKCk2NDfec5JmkcyNNyNjXTMzMzOzbpDp71xJBwOXA7Mi4pl66kLSEYt0Lur+/v5cOneZ9ZJMjdJmVkgNT71T47hLgZOAC9N/v5dr1GZWyY757QDSvJ0D7GhYjoifl5QvOzdepbpdZfUV7Y7AzMzMzNqn5t/CkiYBNwInRMQj9dQ1s3xkWejQzAqomal3ACRdA9wOvEbSgKRT010XAu+U9ChJr8sLW3JCZlZpDvhKas2NV62umZmZmVkhZfxb+HxgHPANSXdLWl2tbstPwqwHuKe0WRdrcuqd4ytsfwZ4R45hmlk2jcyN9+YG6nqRUjMzMzMrtAx/C58GnJa1rllT8hrJ2X9yPsfpEG6UNjMzKwbPjZfRisefbXcIZruQNBP4KjAKuDwiLhy2/7XAFcChwHkRcXHWumaWzc033wxwkKQ1lM9DkeTabGAz8JGIuDPdtwg4FtgYEQeV1Pky8G5gK/AYcHJE/FrSZJJelg+nRZdHxFAPTTMz6yF5/X3y2OC6XI4zb0ZndD7y9B1mZmbFsGN+O0ljSOa3W1paIMvceJXqmtnIKVlsdBYwDThe0rRhxZ4FzgQubqCumdUwODjIggULAB6hci7NAqamj/nApSX7rgRmljn0rcBBEXFweuxzS/Y9FhGHpA83SJuZmZVwo7SZmVkBeG48s0LbsdhoRGwFhhYb3SEiNkbEKuCleuuaWW0rV67kgAMOANhaJZfmAIsjsRwYK2kfgIi4jeTLo51ExA/S6yzsvMiwmZmZVeHpO8zMzArCc+OZFVa5xUZntKCumaXWr1/PxImlM1mVzaVKCwNvyPgypwDXljyfIuku4DfAZyLip3UFbWZm1sXcKG1mZmZmNrIyLzbaaF0vUmpWXbK+966bhz1vOFclnQdsA65KN20AJkXEM5IOA74r6fUR8ZsydZ2/ZmbWc9wobWZmZmZ1u3pFdy20MsIyLzbaaN1uWKTUbCT19fXxxBNP7LSJXXOpoVyVdBLJIojviLT1OyK2AFvSn++Q9BhwILB6eH3nr5mZ9SI3SpuZmZlZ3fZfd30+B5rxyXyO09l2LDYKrCdZbHReC+qaWWr69Ok8+uijAGNKFv0dnktLgTMkLSGZ2uP5iKg6dYekmcDZwFsjYnPJ9gnAsxExKGk/ksUT1+Z2QmZmZgXnhQ7NzMzMzEZQloVKJb1a0gDwF8BnJA1I2sMLlZrlY/To0VxyySWQ9FautGDwMpKG4zXAN4GPDdWXdA1wO/CaND9PTXddArwSuDVdZHhorYe3APdI+gXwHeD0iNhloUQzM7Ne5Z7SZmZmZmYjLMNCpU+RTBWQqa6Z1W/27NkA90VE/9C2YXkYwIJydSPi+ArbD6iw/QbghmbiNTMz62buKW1mZmZmZmZmZmZmLeOe0mZmZtZ2eS2a9/+zd+9hclVlvse/PxMDI4oxFxDTiR1JwBMzDmCHMM4BQUGTgES8zGBU7hPQjoRxPAonA6LIDAJzwROGPBkEZCQGENGMgwF0BtFnyKW5mhAwocGkQyCtIIIoIZ33/LF3x0p1VVd113V3/z7PU09X7b3Wrnd111t799p7rQ1wYNW2ZPXgGyaamZmZmQ0/vlLazMzMzMzMzMzMzOqmrE5pSbMkPS5pk6TzC6yXpK+n6x+RdNgA6n5eUkgaV1lTzMzMzMzMzMzMzKzZleyUljQCuBqYDUwDPi5pWl6x2cDU9DEfuKacupImAscB1Ruza2ZmZmZmZmZmZmZNq5wrpQ8HNkVEZ0TsAJYDc/PKzAVujMQqYLSkA8qo+8/AF4CotCFm1lctRjlIOkTSKkkPSeqQdHi92mNmZmZmZmZmZtlXTqf0BGBLzuuudFk5ZYrWlXQisDUiHu7vzSXNTzu+Orq7u8sI18ygpqMcLge+HBGHABelr83MzMzMzMzMzMpSTqe0CizLv7K5WJmCyyW9DlhE0qHVr4hYGhFtEdE2fvz4ksGa2W61GuUQwL7p8zcCT9e6IWZmZmZmZmZmNnSMLKNMFzAx53ULfTuhipUZVWT5gcBk4GFJvcsfkHR4RDwzkAaYWVGFRirMLKNMsVEOvXXPA+6UdCXJia13F3pzSfNJrr5m0qRJg2uBmZmZmZmZmZkNOeVcKb0WmCppsqRRwMnAirwyK4BT0vlpjwBeiIhtxepGxM8jYr+IaI2IVpIOr8PcIW1WVVUf5ZD+/DTwNxExEfgb4BuF3tyjHMzMzMzMzMzMrJCSV0pHxE5JC4A7gRHAdRGxXtI56folwB3AHGAT8DJwen91a9ISM8tXi1EOAKcCC9PntwLXVileMzMzMzMzMzMbBsqZvoOIuIOk4zl32ZKc5wG0l1u3QJnWcuIwy5SO66uznbbTB1tz90gFYCvJSIV5eWVWAAskLSeZnuOFiNgmqbufuk8D7wHuAd4LbBxsgGZmZmZmZmbVJmkWcBXJBZLXRsRleevfDlwPHAYsiogrc9Y9BbwI9AA7I6KtXnGbDSdldUqbWfbUcJTDXwNXSRoJ/IF03mgzMzMzMzOzRpM0ArgaOI5kdPBaSSsi4tGcYs8B5wIfKrKZYyLiV7WN1Gx4K2dOaTPLqIi4IyIOiogDI+LSdNmS3pEOkWhP1/9pRHT0Vzdd/rOIeFdE/FlEzIyI++vfMjMzs2yRNEvS45I2STq/wHpJ+nq6/hFJh+Ws+xtJ6yWtk/RtSXvXN3qzoWHlypUA0weZh9dJ2i5pXV6dMZLulrQx/fmmnHUXpNt6XNIHatk2M9vD4cCmiOiMiB3AcmBuboGI2B4Ra4FXGxGgmblT2szMLDPK6NR6u6T7JL0i6fN5656S9HNJD0nqyK9rZrWTc8XWbGAa8HFJ0/KKzQampo/5wDVp3QkkV3K1RcR0khFMJ9cpdLMho6enh/b2doBfMMA8TN0AzCqw6fOBH0fEVODH6WvSbZ8MvCOt96/pd4GZ1d4EYEvO6650WbkCuEvS/ZKKjgyWNF9Sh6SO7u7uQYZqNnx5+g4zM7MM8DBEs0zbfcUWQHovh7lAbv7OBW5M79WyStJoSQek60YCfyLpVeB19L1xsZmVsGbNGqZMmUJnZ+eOiNgxkDyMiG0Rca+k1gKbngscnT7/Jsl9V76YLl8eEa8AT0raRPJdcF8Nmmdme1KBZTGA+n8REU9L2g+4W9JjEXFvnw1GLAWWArS1tQ1k+xU5cPOt9Xors5pyp7SZmVk2lOzUiojtwHZJxzcmRDMrotAVWzPLKDMhIjokXQlsBn4P3BURd+W/QXol13yASZMmVTF0s6Fh69atTJw4MXdR2XkIbOtn0/tHxDaA9Ibh++Vsa1WBbfXh/DWrui4gN+FbGMAJ3Yh4Ov25XdLtJMfhfTqlB2rZ6s2VbgKAA6uyFbPG8/QdZmZm2eBhiGbZVc4VWwXLpPPTzgUmA28B9pH0yT4FI5ZGRFtEtI0fP77igM2GmuTi576L815XenXloLbl/DWrurXAVEmTJY0imUpnRTkVJe0j6Q29z4H3A+v6r2Vmg+Erpc3MzLJhSA9DNBviyrliq1iZY4EnI6IbQNJ3gXcD36pZtGZDUEtLC1u2bNljEeXnYX+e7Z3iI51yZ3sF2zKzKoiInZIWAHeS3IvhuohYL+mcdP0SSW8GOoB9gV2SziOZb34ccLskSPrMlkXEyka0w6xWqnXV/ryZlY3ucae0mZlZNjTlMEQzK8vuK7aArSRXbM3LK7MCWJBOzTMTeCHt5NoMHCHpdSTTd7yP5J9oMxuAGTNmsHHjRoBROVdOlpWHJTa9AjgVuCz9+f2c5csk/RPJKIepwJpqtMXMSouIO4A78pYtyXn+DMnxdL7fAn9W2+jMDDx9h5mZWVZ4GKJZRkXETqD3iq0NwC29V2z1XrVF8o9zJ7AJ+DfgM2nd1cB3gAeAn5Mcvy+tbwvMsm/kyJEsXrwY4CAGmIcAkr5NcpPCgyV1STozXXUZcJykjSQ3I74MICLWA7eQ3PthJdAeET01bqaZmVlm+EppMzOzDPAwRLNsK+OKrQDai9T9EvClmgZoNgzMmTMHYF1EtPUuG0AefrzI8l+TjGAotO5S4NIKQjYzM9vtwM23VmU7T0z6WFW2Uyl3SpuZmWWEhyGamZmZmZnZUODpO8zMzMzMzMzMzMysbtwpbWZmZmZmZmZmZmZ1405pMzMzMzMzMzMzM6sbzyltZmZmDVetm3ZY9lTtbz/zb6uzHTMzMzMzq7myrpSWNEvS45I2STq/wHpJ+nq6/hFJh5WqK+mStOxDku6S9JbqNMnMzMzMzMzMzMzMmlXJTmlJI4CrgdnANODjkqblFZsNTE0f84Fryqh7RUS8MyIOAX4AXFR5c8wsVy1OKKXrPpuuWy/p8nq0xczMzMzMzMzMhoZypu84HNgUEZ0AkpYDc4FHc8rMBW6MiABWSRot6QCgtVjdiPhtTv19gKi0MWb2RzknhY4DuoC1klZERG7u5p5QmklyQmlmf3UlHUOSx++MiFck7Ve/VpmZmZmZmZmZWdaVM33HBGBLzuuudFk5ZfqtK+lSSVuAT1DkSmlJ8yV1SOro7u4uI1wzS+0+oRQRO4Dek0K5dp9QiohVQO8Jpf7qfhq4LCJeAYiI7fVojJmZmZmZmZmZDQ3ldEqrwLL8q5qLlem3bkQsioiJwE3AgkJvHhFLI6ItItrGjx9fRrhmlqrVCaWDgCMlrZb0E0kzCr25TyiZmZmZmZmZmVkh5XRKdwETc163AE+XWaacugDLgI+UEYuZla9WJ5RGAm8CjgD+D3CLpD7lfULJzMzMzMzMzMwKKWdO6bXAVEmTga3AycC8vDIrgAXpnNEzgRciYpuk7mJ1JU2NiI1p/ROBxypujZnlquSE0qh+6nYB303nkF8jaRcwDvDl0GbD0LLVm6uynQOrshUzMzMzMzPLgpKd0hGxU9IC4E5gBHBdRKyXdE66fglwBzAH2AS8DJzeX91005dJOhjYBfwSOKeqLTOzmpxQAr4HvBe4R9JBJB3Yv6p5a8zMzPpRrRMk82ZOqsp28kmaBVxFckx8bURclrde6fo5JMfTp0XEA+m60cC1wHSSkUtnRMR9NQnUbAhbuXIlwHRJmxi9BinsAAAgAElEQVR4HhbMYUk3AwenmxgN/CYiDpHUCmwAHk/XrYoI/89rZmaWKudKaSLiDpKO59xlS3KeB9Bebt10uafrMKuhGp5Qug64TtI6YAdwavodYGZmZgVIGgFcDRxHMuJoraQVEfFoTrHZwNT0MRO4Jv0JSUfYyoj4qKRRwOvqFrzZENHT00N7ezvAL4A2BpCH/eVwRPxVb2VJ/wi8kLO9JyLikFq2y8zMLKvK6pQ2s2yq0QmlHcAnqxupmZnZkHY4sCkiOgHSEUpzgdzOsLnAjem+eZWk0ZIOAH4HHAWcBrv3wzvqGLvZkLBmzRqmTJlCZ2fnjojYMcA8bKVEDqdXWf8lyYhCMzMzK6GcGx2amZmZmdngTQC25LzuSpeVU+ZtJPdtuF7Sg5KulbRP/htImi+pQ1JHd7dv82CWb+vWrUycmHvLlAHlYTk5fCTwbM59kwAmp3n7E0lHFovN+WtmZsORO6XNzMzMzGpLBZblT31VrMxI4DDgmog4lOTK6fP7FIxYGhFtEdE2fvz4SuM1G3KKzDZXbh6Wk8MfB76d83obMCnN288ByyTtWyQ256+ZmQ077pQ2MzMzM6utLiD3Es0W4Okyy3QBXRGxOl3+HZJOajMbgJaWFrZs2bLHIgaWh0VzWNJI4MPAzb3LIuKViPh1+vx+4AngoIobYmZmNkS4U9rMzMzMrLbWAlMlTU5vVHgysCKvzArgFCWOAF6IiG0R8QywRdLBabn3seccuGZWhhkzZrBx40aAUQPNQ0rn8LHAYxHR1btA0vj0BolIehvJzRM7a9Q8M8sjaZakxyVtktRnhJGkt0u6T9Irkj4/kLpmVh2+0aGZmZmZWQ1FxE5JC4A7gRHAdRGxXtI56folJDcXngNsAl4GTs/ZxGeBm9LOsM68dWZWhpEjR7J48WKOP/74g4ANDCAPi+VwzuZPZs+pOyC5QelXJO0EeoBzIuK52rXQzHqlJ4SuBo4jGemwVtKKiMg9qfsccC7woUHUNbMqcKe0mZlZRkiaBVxF8g/xtRFxWd76twPXkwztXxQRV5Zb18xqKyLuIOnwyl22JOd5AO1F6j4EtNU0QLNhYM6cOQDrImJ3Pg0gD/vkcM660wosuw24rbKIzWyQDgc2RUQngKTlwFxyRhpFxHZgu6TjB1rXzKrDndJmNbL6yepcCDHT/4KaGb7iw8zMzMysTBOA3Enku4CZ1a4raT4wH2DSpEkDj9JsmPOc0mZmZtmw+6qNiNgB9F61sVtEbI+ItcCrA61rZmZmZjZEqMCyqHbdiFgaEW0R0TZ+/PiygzOzhDulzczMsqHQVRsTql1X0nxJHZI6uru7BxWomZmZmVkDdQETc163AE/Xoa6ZDYA7pc3MzLLBV3yYmZmZmZW2FpgqaXJ6k+CTgRV1qGtmA+A5pc3MzLLBV3yYmZmZmZUQETslLQDuJLnJ93URsV7SOen6JZLeDHQA+wK7JJ0HTIuI3xaq25iWmA1t7pQ2MzPLht1XbQBbSa7amFeHumZmZmZmmRIRdwB35C1bkvP8GZILNcqqa2bV505pMzOzDPAVH2ZmZmZmZjZUlNUpLWkWcBXJP7LXRsRleeuVrp8DvAycFhEP9FdX0hXAB4EdwBPA6RHxm2o0yszMbCjyFR9mZmZmZmY2FJS80aGkEcDVwGxgGvBxSdPyis0GpqaP+cA1ZdS9G5geEe8EfgFcUHFrzGwPkmZJelzSJknnF1gvSV9P1z8i6bAB1P28pJA0rtbtMDMzMzMzMzOzoaNkpzRwOLApIjojYgewHJibV2YucGMkVgGjJR3QX92IuCsidqb1V1Hkyi4zG5wanlBC0kTgOGBzjZthZmZmZmZmZmZDTDmd0hOALTmvu9Jl5ZQppy7AGcAPy4jFzMpXkxNKqX8GvgBEzVthZmZmZmZmZmZDSjmd0iqwLL8jqliZknUlLQJ2AjcVfHNpvqQOSR3d3d1lhGtmqZqcUJJ0IrA1Ih6udsBmZmZmZmZmZjb0ldMp3QVMzHndAjxdZpl+60o6FTgB+EREFLziMiKWRkRbRLSNHz++jHDNLFX1E0qSXgcsAi4q+eY+oWRmZrZbJfd5SNePkPSgpB/UL2qzoWXlypUA06t5vxVJF0vaKumh9DEnZ90FafnHJX2g1u0zMzPLknI6pdcCUyVNljQKOBlYkVdmBXBKuhM/AnghIrb1V1fSLOCLwIkR8XKV2mNmf1SLE0oHApOBhyU9lS5/QNKb89/cJ5TMzMwSldznIcdCYEONQzUbsnp6emhvbwf4BVW+3wrwzxFxSPq4I60zjeT/33cAs4B/TbdjZmZmwMhSBSJip6QFwJ3ACOC6iFgv6Zx0/RLgDmAOsAl4GTi9v7rpphcDewF3SwJYFRHnVLNxZsPc7pNCwFaSg+J5eWVWAAskLQdmkp5QktRdqG6av/v1Vk47ptsi4lc1b42ZmVl27b5XA0C6350LPJpTZvd9HoBVkkZLOiDdL7cAxwOXAp+rc+xmQ8KaNWuYMmUKnZ2dOyJix0DyEGildA7nmwssj4hXgCclbSL5Lriv6o0zs0w4cPOtjQ7BrKmU7JQGSM/23pG3bEnO8wDay62bLp8yoEjNbEBqeELJzMzMBqbQvRpmllFmArAN+BeSGwy/odgbSJpPcmUnkyZNqjxisyFm69atTJyYOxBwQHlYKocXSDoF6AD+NiKeT+usKrCtPpy/ZmY2HJXVKW1m2VSLE0p5ZVorj9LMzGzIG/R9HiSdAGyPiPslHV3sDSJiKbAUoK2treC9WsyGs2K3MMp7PaD7raQ/rwEuSV9fAvwjcEaJOvmxOX/NzGzYcae0mZmZmVltVXKfh48CJ6Y3T9sb2FfStyLikzWM12zIaWlpYcuWLXssovw8HFVkORHxbO9CSf8G9N6MtJy8NzMzq7uqTSUz828rql7OjQ7NzMzMzGzwBn3j8Ii4ICJa0tFJJwP/5Q5ps4GbMWMGGzduBBg10DyknxxO55zudRKwLmdbJ0vaK71Py1RgTY2aZ2Zmljm+UtrMzMzMMq9ZrvgopJL7PJhZdYwcOZLFixdz/PHHHwRsoHr3W7lc0iEkU3M8BZyd1lkv6RaSmyHuBNojoqc+rTUzM2t+7pQ2MzOzQfNdxM3KU8l9HnLK3APcU4PwzIaFOXPmAKyLiLbeZZXebyUiPlXs/SLiUuDSCkI2MzMbsjx9h5mZmZmZmZmZmZnVjTulzczMzMzMzMzMzKxu3CltZmZmZmZmZmZmZnXjTmkzMzMzMzMzMzMzqxt3SpuZmZmZmZmZmZlZ3bhT2szMzMzMzMzMhgxJsyQ9LmmTpPMLrJekr6frH5F0WM66pyT9XNJDkjrqG7nZ8OFOaTMzs4zwwbWZmZmZWf8kjQCuBmYD04CPS5qWV2w2MDV9zAeuyVt/TEQcEhFttY7XbLhyp7SZmVkG+ODazMzMzKwshwObIqIzInYAy4G5eWXmAjdGYhUwWtIB9Q7UbDhzp7SZmVk2+ODazMzMzKy0CcCWnNdd6bJyywRwl6T7Jc0v9iaS5kvqkNTR3d1dhbDNhhd3SpuZmWWDD67NzMzMzEpTgWUxgDJ/ERGHkYxCbJd0VKE3iYilEdEWEW3jx48ffLRmw1RZndIVzmFZsK6kj0laL2mXJA8jNquBGuXuFZIeS8vfLml0vdpjNsz54NrMzMzMrLQuYGLO6xbg6XLLRETvz+3A7SQjFs2sykp2Slcyh2WJuuuADwP3Vt4MM8tXw9y9G5geEe8EfgFcUOOmmFnCB9dmZmZmZqWtBaZKmixpFHAysCKvzArglPRCrSOAFyJim6R9JL0BQNI+wPtJ+q/MrMrKuVK6kjksi9aNiA0R8XjVWmJm+WqVu3dFxM60/iqSTi8zqz0fXJtl2GBHL0maKOm/JW1IRxkurH/0ZkPDypUrAabXYxShpFZJv5f0UPpYUo82mhmk/68uAO4ENgC3RMR6SedIOictdgfQCWwC/g34TLp8f+Bnkh4G1gD/GREr69oAs2FiZBllCs1PObOMMhPKrNuvdN7L+QCTJk0aSFWz4a4euXsGcHPFkZpZSRGxU1LvwfUI4Lreg+t0/RKSg+s5JAfXLwOnp9X3B26XBMm+f5kPrs3qJ2cE0nEk+9S1klZExKM5xXJHL80kGb00E9gJ/G1EPJCeXLpf0t15dc2shJ6eHtrb2yEZ6dfGAPKwRA7fDVyQ7qe/RjKK8Ivp9p6IiEPq0DwzyxMRd5AcG+cuW5LzPID2AvU6gT+reYBmVlandCVzWJZTt18RsRRYCtDW1jagumbDXE1zV9Iikn+Ubyr45j6hZFZ1Prg2y6zdI5AAJPWOQMrtDNs9eglYJWm0pAMiYhuwDSAiXpS0geTksTulzQZgzZo1TJkyhc7Ozh0RsWMgeQi0UiSHI+KunPqrgI/Woz1mZmZZV870HZXMYVlOXTOrjZrlrqRTgROAT6QH7X34ZmlmZma7FRuZNKAyklqBQ4HVVY/QbIjbunUrEyfmHt4OKA/LyWFIRhH+MOf1ZEkPSvqJpCMHG7uZmdlQVE6n9KDnsCyzrpnVRk1yV9IskiGJJ0bEy/VqjJmZWYZVMnopWSm9HrgNOC8iftvnDaT5kjokdXR3d1cUrNlQVOw6irzX1RxFuA2YFBGHAp8Dlknat1AQzl8zMxuOSnZKVzJBfLG6AJJOktQF/Dnwn5LurGrLzIa5WuUusBh4A3C3b9piZmZWlkpGLyHptSQd0jdFxHcLvYFHKJn1r6WlhS1btuyxiBqOIoyIVyLi1+nz+4EngIMKxeb8NTOz4aicOaUHPYdlsbrp8tuB2wcSrJkNTI1yd0qVwzQzMxvqdo9AAraSjECal1dmBbAgnat2JunoJSV3KP0GsCEi/qmeQZsNJTNmzGDjxo0Ao3JGApabh90UyeGcUYTvyR1FKGk88FxE9Eh6G8nNEztr2kgzM7MMKWf6DjMzMzMzG6RKRi8BfwF8CnhvOkLpIUlz6tsCs+wbOXIkixcvhuRq5XqMIjwKeETSw8B3gHMi4rlat9PMzCwryrpS2szMzMzMBm+wo5ci4mcUns/WzAZozpw5AOsioq13Wa1GEUbEbSTT7jSNAzffuueCEWPKr9x2enWDMTOzYc9XSpuZmZmZmZmZmZlZ3bhT2szMzMzMzMzMzMzqxp3SZmZmZmZmZmZmZlY37pQ2MzMzMzMzMzMzs7rxjQ7NLPP2uGmLb9hiZmZmZmZmZtbUfKW0mZmZmZmZmZmZmdWNr5Q2MzMzMzMzG2ZWP/lc2WWf6NlcdN28mZOqEY6ZmQ0zvlLazMzMzMzMzMzMzOrGndJmZmZmZmZmZmZmVjfulDYzMzMzMzMzMzOzunGntJmZmZmZmZmZmZnVjTulzczMzMzMzMzMzKxu3CltZmZmZmZmZmZmZnUzspxCkmYBVwEjgGsj4rK89UrXzwFeBk6LiAf6qytpDHAz0Ao8BfxlRDxfeZPMrNdwzN3VTz5XdtknejYXXTdv5qRqhGNWVbXIaTOrD+evWeOtXLkSYLqkTdTh2FjSBcCZQA9wbkTcWeMm1syBm28tvnLEmOLr2k6vfjBmZfB+16z5leyUljQCuBo4DugC1kpaERGP5hSbDUxNHzOBa4CZJeqeD/w4Ii6TdH76+ovVa5rZ8ObcLW3QB9fgA2yruxrmtJnVmPPXrPF6enpob28H+AXQRo2PjSVNA04G3gG8BfiRpIMioqce7TUbzrzfNcuGcq6UPhzYFBGdAJKWA3OB3IScC9wYEQGskjRa0gEkZ4uL1Z0LHJ3W/yZwDxnt2DJrUs5ds6GlVjltZrXn/DVrsDVr1jBlyhQ6Ozt3RMSOOhwbzwWWR8QrwJPp1dmHA/fVtqX11+9IxSf/seztzJycc1GILwCxyni/a5YB5XRKTwC25LzuIjmLVKrMhBJ194+IbQARsU3SfoXeXNJ8YH768iVJj5eIdxzwqxJlyvD5yjeRqFI8VdNs8UDzxdRk8Xy+nHjeWmBZM+duk/2OB+OMIdCGzP8dsh4/wMEDKFurnN7DIPa71dKMf89mi8nx9K/ex4CF9r3F1Dx/G5i7lWi2z1CtuJ1102/+vgnYlz/mbq2PjScAqwpsq4+8/H1F0rr+GlInDf57ntEkcTRNDOA4mmq/Cz5uzuF4Smu2mOp53Fw0d8vplFaBZVFmmXLq9isilgJLyy0vqSMi2gbyHrXkeEprtpiGUDxNm7vN9jseDLeh8bIePyRtGEjxAsuqntMD3e9WSzP+PZstJsfTv2aLJ0/N87dRuVuJJv+bVY3b2RwkfQz4QESclbO4lsfGg9r3Nsvv0XE0VwyOY8B83FxHjqe0ZoupWeIpp1O6C5iY87oFeLrMMqP6qfuspAPSs8kHANsHEriZleTcNRtaapXTZlZ7zl+zxqv3sXE572dmteH9rlkGvKaMMmuBqZImSxpFcrOGFXllVgCnKHEE8EI6hKm/uiuAU9PnpwLfr7AtZrYn567Z0FKrnDaz2nP+mjVevY+NVwAnS9pL0mSSm6mtqVXjzGwP3u+aZUDJK6UjYqekBcCdwAjguohYL+mcdP0S4A5gDrAJeBk4vb+66aYvA26RdCawGfhYldrUbMMWHU9pzRbTkIinyXO32X7Hg+E2NF7W44cBtKGGOd0smvHv2WwxOZ7+NVs8uw2D/B2spv2bVZnb2QTqfWycbvsWkpuj7QTaI6KnjFCb5ffoOP6oGWIAx1G2YbDfbba/geMprdliaop4lNxo1MzMzMzMzMzMzMys9sqZvsPMzMzMzMzMzMzMrCrcKW1mZmZmZmZmZmZmdZOpTmlJH5O0XtIuSW156y6QtEnS45I+kLP8XZJ+nq77uiSly/eSdHO6fLWk1gpjO0TSKkkPSeqQdPhgY6smSZ9N33e9pMubJKbPSwpJ4xoZj6QrJD0m6RFJt0sa3ch4CsQ3K33/TZLOr9X71FsW2iVpoqT/lrQhzZ2F6fIxku6WtDH9+aacOgU/M40maYSkByX9IH2dmTZIGi3pO2mebpD051mKH0DS36SfoXWSvi1p76y1oZ76+15uUDxFjzvqHEdTfW9Kuk7SdknrGh0LFP/OtmxotryvpmbL3Vpw/lVPPT8vhb7HG3F80izH3Onx2RpJD6dxfLkRcaTbbYpjd0lPKfm/9yFJHY2MxQprtv2nj5sLxuJj5lIiIjMP4H8BBwP3AG05y6cBDwN7AZOBJ4AR6bo1wJ8DAn4IzE6XfwZYkj4/Gbi5wtjuytn2HOCewcZWxd/XMcCPgL3S1/s1QUwTSW4Y8EtgXCPjAd4PjEyffw34WqN/PzmxjUjf923AqDSeabV4r3o+stIu4ADgsPT5G4BfpJ+Ly4Hz0+Xnl/OZafQD+BywDPhB+jozbQC+CZyVPh8FjM5Y/BOAJ4E/SV/fApyWpTY04HdW8Hu5gfEUPO6ocwxN970JHAUcBqxr9Gcmjafgd3aj4/Kj7L9fU+V9FdvVdLlbo3Y6/6rze6zr56XQ93gjjk+KfX7qHQvJ/3avT5+/FlgNHNGg30lTHLsDT5H+v97Iz4gf/f6Nmmr/iY+bC8XjY+YSj0xdKR0RGyLi8QKr5gLLI+KViHiS5O6ph0s6ANg3Iu6L5Ld+I/ChnDrfTJ9/B3ifVNFVrwHsmz5/I/B0BbFVy6eByyLiFYCI2N4EMf0z8AWS31evhsQTEXdFxM705SqgpZHx5Dkc2BQRnRGxA1iexpV1mWhXRGyLiAfS5y8CG0g6GHO/N77Jnt8nfT4z9Y26L0ktwPHAtTmLM9EGSfuS7MS/ARAROyLiN2Qk/hwjgT+RNBJ4Hcm+IWttqJt+vpcbFU+x4456arrvzYi4F3iukTHk6uc72zKg2fK+ipoud2vB+Vc1df28FPker/vxSbMcc0fipfTla9NH1DuODBy7N1Msw16z7T993NyXj5lLy1SndD8mAFtyXnelyyakz/OX71EnTeQXgLEVxHAecIWkLcCVwAUVxFYtBwFHKpme5CeSZjQyJkknAlsj4uG8VY38HfU6g+TK52aJp1gMWZe5dimZ2udQkism9o+IbZB8oQP7pcWatV3/QnISaFfOsqy04W1AN3B9OoTxWkn7kJ34iYitJPuDzcA24IWIuIsMtaHBcr+XhzN/LgYg7zvbsmco5f2wy13nX0Wa4fPS0OOTRh9zp9NmPARsB+6OiEbE0UzH7gHcJel+SfMbHIuVNpT2n5XwZ7FMzbLPHtnINy9E0o+ANxdYtSgivl+sWoFl0c/y/uoMKjbgfcDfRMRtkv6S5Oq+YwcZW9lKxDQSeBPJ0KMZwC2S3lbLmErE839Jhpj0qdaIeHo/T5IWATuBm2odzwDU873qKVPtkvR64DbgvIj4bT+DKZquXZJOALZHxP2Sji6nSoFljWzDSJKhTp+NiNWSriIZJlhMs8VPOs/eXJJhjL8BbpX0yf6qFFjWtPkxWIP8Xm5oPA02LD4X1ZD/nd3oeOyPmi3v62RY5a7zr2LN/HmpeWzNcMwdET3AIem8vLdLmt5P8arH0YTH7n8REU9L2g+4W9JjDYxl2Gq2/aePm4eGZtpnN12ndEQcO4hqXSRzFfdqIRki3cWeQxh6l+fW6UqHVb+REpfV9xebpBuB3knCb+WPQ24GE1vZSsT0aeC76VQTayTtAsbVMqZi8Uj6U5KOmYfTg4wW4AElN4Ssezw5cZ0KnAC8L/09Uct4BqBYDFmXmXZJei3JF/VNEfHddPGzkg6IiG3pdC69U+I0Y7v+AjhR0hxgb2BfSd8iO23oArrSq1QgmWbpfLITPyQnJp+MiG4ASd8F3k222lB1g/xeblg8TWBYfC4qVeQ725pEs+V9nQyb3HX+VUUzfF4acnzSbMfcEfEbSfcAs+ocR1Mdu0fE0+nP7ZJuJ5kWYVgfwzZCs+0/fdycfc22zx4q03esAE6WtJekycBUYE06pORFSUek80WfAnw/p86p6fOPAv9VYRI/Dbwnff5eYGMFsVXL99JYkHQQyUTvv2pETBHx84jYLyJaI6KV5MvisIh4phHxQHJXVuCLwIkR8XLOqkb+zXqtBaZKmixpFMnNOFfU6L3qKRPtSv++3wA2RMQ/5azK/d44lT2/T/p8ZuoVbyERcUFEtKT5djLJd9wnyUgb0u+GLZIOThe9D3iUjMSf2gwcIel16WfqfSTzdmWpDXXVz/fycJaJ781G6uc72zJgCOf9sMhd51/VNMPnpe7HJ81yzC1pfHqFNJL+hOTCgsfqGUczHbtL2kfSG3qfk4x2XteIWKy4Ibz/rEQzfJc2rabcZ0cT3AGy3AdwEkln5ivAs8CdOesWkdxl83Fgds7yNpIv0CeAxYDS5XuTXNG8ieQL820Vxva/gftJ7u65GnjXYGOr4u9rFPCt9D0eAN7b6Jhy3ucpcu7m24h40r/9FuCh9LGkWX4/6XvNIbkb6hMkw2HqnnPDtV1pPgfwSM7nYw7JvPM/Jjnp9GNgTKnPTDM8gKP54x28M9MG4BCgI/07fI9kOqLMxJ/G9GWSf2rWAf9OclfyTLWhzr+vot/LDYqn6HFHneNoqu9N4Nsk86S/mv5+zmxwPAW/sxv9e/Kj7L9fU+V9ldvWVLlbozY6/6r3u6zb56XQ93gjjk+KfX7qHQvwTuDBNI51wEXp8oYcs9HgY3eSe7s8nD7W934eG/X78KPo36mp9p/4uLlQLD5mLvHo7aA1MzMzMzMzMzMzM6u5oTJ9h5mZmZmZmZmZmZllgDulzczMzMzMzMzMzKxu3CltZmZmZmZmZmZmZnXjTmkzMzMzMzMzMzMzqxt3SpuZmZmZmZmZmZlZ3bhT2szMzMzMzMzMzMzqxp3S1i9JCyR1SHpF0g156/5S0gZJL0p6VNKHGhSmmeUpkbtnSdok6SVJKyW9pUFhmlkeSXtJ+oakX6b71wclzc5Z/z5Jj0l6WdJ/S3prI+M1s0R/uStplKTvSHpKUkg6usHhmlmqRO4eIeluSc9J6pZ0q6QDGh2zmSVK5O+09P/h59PHjyRNa3TMtid3SlspTwNfBa7LXShpAvAt4HPAvsD/AZZJ2q/uEZpZIcVy9z3A3wNzgTHAk8C36x6dmRUzEtgCvAd4I3AhcIukVknjgO+my8YAHcDNjQrUzPZQNHfT9T8DPgk804jgzKyo/nL3TcBSoBV4K/AicH0jgjSzgvrL36eBj5IcM48DVgDLGxKlFaWIaHQMlgGSvgq0RMRp6euZwH9ExH45ZbqBEyPivsZEaWb5CuTulcCfRER7+votwFZgSkQ80bBAzawoSY8AXwbGAqdFxLvT5fsAvwIOjYjHGhiimRXQm7sRcVvOsi7gkxFxT8MCM7N+FcrddPlhwE8i4g2NiczMSimy7x0JnA1cERGva1hw1oevlLbB6gA2SDpR0oh06o5XgEcaHJeZ9U/pI/c1wPQGxGJmJUjaHzgIWA+8A3i4d11E/A54Il1uZk0kL3fNLCNK5O5RRZabWRMolL+SfgP8Afh/JCOGrYmMbHQAlk0R0SPpRmAZsDewA/hY+g+ymTWvO4CbJS0BNgIXAQH4jLFZk5H0WuAm4JsR8Zik1wPdecVeAHzFllkTyc/dRsdjZuXpL3clvZPkuHluI2Izs/4Vy9+IGJ2OLjwV+GWj4rPCfKW0DYqkY4HLgaOBUSRz+Fwr6ZBGxmVm/YuIHwNfAm4j2Sk/RTI/XlcDwzKzPJJeA/w7yUnfBenil0ju45BrX5IcNrMmUCR3zazJ9Ze7kqYAPwQWRsRPGxCemfWj1L43vXhyCXCj74PWXNwpbYN1CHBvRHRExK6IWAusBo5tcFxmVkJEXB0RU9M54W8jGTWzrsFhmVlKkoBvAPsDH4mIV9NV64E/yym3D3AgHkps1hT6yV0za2L95a6ktwI/Ai6JiH9vUIhmVsQA9r2vIRkdPKFesVlp7pS2fkkaKWlvYAQwQtLe6STxa4Eje6+MlnQocB+IP/cAACAASURBVCSeU9qsKRTL3fTndCUmkdxR/KqIeL6xEZtZjmuA/wV8MCJ+n7P8dmC6pI+k+X0R8IinBzBrGsVyF0l7pXkLMCrdH6vPFsysEQrmrqQJwH8BV0fEkkYFZ2b9Kpa/x0k6NL0H2r7APwHPAxsaFKcVoIhodAzWxCRdTDLUP9eXI+JiSQuA80jOSHWT7Kz/sc4hmlkBxXIX+BfgXpKrK18Ergf+LiJ66hqgmRWUXpH1FMnNg3fmrDo7Im5Kp89aDLyVZITSaRHxVL3jNLM9lZG7T5Hkba7Jzl+zxuovd4EpwMXAHvdNiojX1yk8M+tHifzdAVwCtAC/J7mw8vyI8IWUTcSd0mZmZmZmZmZmZmZWN56+w8zMzMzMzMzMzMzqxp3SZmZmZmZmZmZmZlY37pQ2MzMzMzMzMzMzs7pxp7SZmZmZmZmZmZmZ1c3IRgcwEOPGjYvW1tZGh2HWdO6///5fRcT4RsdRjHPXrDDnrll2NXP+OnfNimvm3AXnr1kxzl2zbOovdzPVKd3a2kpHR0ejwzBrOpJ+2egY+uPcNSvMuWuWXc2cv85ds+KaOXfB+WtWjHPXLJv6y11P32FmZmZmZmZmZmZmdeNOaTMzMzOzAVi5ciXAdEmbJJ2fv16Jr6frH5F0WM66WZIez68r6ZK07EOS7pL0lnR5q6Tfp8sfkrSkHm00MzMzM6sld0qbmZmZmZWpp6eH9vZ2gF8A04CPS5qWV2w2MDV9zAeuAZA0Arg6XZ9f94qIeGdEHAL8ALgoZ3tPRMQh6eOcGjXNzMzMzKxuMjWntA1vr776Kl1dXfzhD39odCgNs/fee9PS0sJrX/vaRodiVjbnrnPXssm5m8jP3zVr1jBlyhQ6Ozt3RMQOScuBucCjOdXmAjdGRACrJI2WdADQCmyKiE6A3LoR8duc+vsAUfPG2ZDk3E1432tZ49xNOHcta5y7icHkrjulLTO6urp4wxveQGtrK5IaHU7dRQS//vWv6erqYvLkyY0Ox6xszl3nrmXTcM9dKJy/W7duZeLEibnFuoCZeVUnAFvyykwosnx3XUmXAqcALwDH5JSbLOlB4LfA30XETytplw1tzl3vey2bnLvOXcsm5+7gc9fTd1hm/OEPf2Ds2LHDNsklMXbs2AGdfSs2b2XO+v7mvLxO0nZJ6/LqjJF0t6SN6c83VdQwG/KcuwPPXbNmMNxzFwrnb3Lxcx/5Cwv90qKf5b3bXhQRE4GbgAXp4m3ApIg4FPgcsEzSvgVinS+pQ1JHd3d3P62yoc65632vZZNz17lr2eTcHXzuulPaMmU4JzkMrP0l5q3sVXDOy9QNwKwCmz4f+HFETAV+nL4265dzd3i337LLn92+v4OWlha2bNmyxyLg6bxqXcDEAmWKLc+3DPgIQES8EhG/Tp/fDzwBHJRfISKWRkRbRLSNHz++dMNsSHPu+ndg2eTPrX8Hlk3+3A7ud+DpO8zy/e5X1dnOPuOqs53BO5wi81bmlCk452VEbIuIeyW1FtjuXODo9Pk3gXuAL1Ya7LLVmwsunzdzUqWbNjMblGLfSwPl77GhZcaMGWzcuBFglKRRwMnAvLxiK4AF6b53JvBCRGyT1A1MlTQZ2JpbV9LUiNiY1j8ReCxdPh54LiJ6JL2N5ERyZ00bWUzH9QOv03Z69eMwG+a8fzIzK2Iwxyq9fMxSd+6Utsyq1sFYr2Y4KFu0aBE33ngjzz//PC+99FKlm+t33sp+ykwgGSpczP4RsQ0g/Qd7v0KFJM0nufqaSZMa/7u15uHcHTxJs4CrgBHAtRFxWd76twPXA4cBiyLiypx1o4Frgekk0wWcERH31SxY263pOg/KPVgfOX2PE7XLHti+5/pRr68ojKzm7siRI1m8eDHHH3/8QcAG4LqIWC/pHICIWALcAcwBNgEvA6en63ZKWgDcSZLH10XE+nTTl0k6GNgF/BI4J11+FPAVSTuBHuCciHiu4sbbsOH9rlk2OXfNssm5Wz53StvgVXIGajDy/jlmR5V3YtW6QroCH/zgB1mwYAFTp06txub6nbdyAGUGJSKWAksB2traqrJNs2ZV5dwtKGdKnuNITiCtlbQiInJHPzwHnAt8qMAmrgJWRsRH06s7X1ezYIeIAzffWpXtPDHpY1XZzpDVwBFKg83dOXPmAKyLiLbeZWlndO/zANoL1Y2IO0g6rfOXf6RI+duA2wYUYJkG+k/TgZsL94XPnDymGuGYla0e+10zqz7nrlk21Sp33SltQ8Yrr/ZUVP+lV3b2u/6Sv/8aY8eO4TNn/zUAX/7qP7Df+PF8+uyzCpZ//T4Dj+GII44YeKXiypm3sty5LXM92zvFh6QDgO0lyps11IUXXsi4ceNYuHAhkJzl3X///Tn33HOr9h5Vzt1iSk7JExHbge2Sjs+tmN4U7SjgtLTcDmBHNYJququAqV5MB1ZlKzZYF37lHxg3dgwL288GYNHFl7L/fuM59zPzq/Yedcpds2FlCO13zYYV565ZNmU5dyvqlPYwYquG1U+WNwJ1r8m79ug4rrQTeqBO+eQ8PnHqGXzm7L9m165d3Hb79/nvu/tc6MT7j5/LSy/9jteM2DO9rrzySo499th6hQuwliLzVuYoOOdlie2uAE4FLkt/fr+qUZtV2ZlnnsmHP/xhFi5cyK5du1i+fDlr1qzpU+7II4/kxRdf7LO8AblbTDlT8hTzNqAbuF7SnwH3Awsj4nf5BRs19U61h7lZ9p156if48LzTWNh+dpK7t32PNffc2afckcedwIsvvQSvafh+18wYUvtds2HFuWuWTVnO3UF3SnsYcTatvvUfGx1CZr110kTGvGkMDz/yc7Z3d/POP53O2DF9h6ve9Z/F+2hfeu6ZMt8tdpd9/Zg3DybcovNWljPnJYCkb5Pc0HCcpC7gSxHxDZLO6FsknQlsBjwu3Zpaa2srY8eO5cEHH+TZZ5/l0EMPZezYsX3K/fSnP21AdANSyXQ7I0lOEH82IlZLugo4H7iwzwYHOPWOp7ioo3pPm1VKpdNolZi2o/Wtkxg7ZgwPPvwIz27v5tB3Tmfs2L773Z/e/YPkSeNvMGxmDKn9rtmw4tw1y6Ys524lV0p7GLENO6d+ah43fftmnt3ezac+8fGCZXqvlM536Zcv4pijj9r9uqenhyPf+wEA5sx6P393wReqHm+heSsHMOdlwQZGxK+B91UxTLOaO+uss7jhhht45plnOOOMMwqWKffMcU9PD+9617sAOPHEE/nKV75Sm6D7Gsx0O7l1uyJidfr6OySd0k2jWp3b0Hwd3FVr2zCct/esUz/BDd9azjPPbueMU/IH+yR2Xymd58q//zLHHvOe3a97enp41/9Odl8nzpnFVy7MTYH4Yye5O7fNKjZE9rtmw06z5u7KlSsBpkvaROFR+iK58HEOycVWp0XEA+m6giP8JV1C0oe1i2RKytMi4ul03QXAmSQ3GD43IvoO1TJrIs2au6VU0ik9pIcRmxXyweNn89V/uIKdO1/luqX/WrBMf1dK5xoxYgT/85MfVTM8MyvipJNO4qKLLuLVV19l2bJlBcuUe+Z4xIgRPPTQQ9UMr1zlTMlTUEQ8I2mLpIMj4nGSE0uPlqpnQ1O1ps3a67UjqhJPf/d0OO4DH+DvLvkaO3e+ytIlVxcs+8MffK/sbf/snh8VXdf7ejD3hDCzPQ2R/a7ZsNOMudvT00N7ezvAL4A2Co/Snw1MTR8zgWuAmSVG+F8RERcCSDoXuAg4R9I0kuPsdwBvAX4k6aCIqO/8oWYD0Iy5W45KOqWbchixDR8fO2R83d9z1KhRHHXku3njvm9kxIjq/DOe6+8uvoRbv3M7L7/8ew6efhinfmoef/+1K0tXNMuQRoxEGTVqFMcccwyjR4+uSe5+4QtfYNmyZbz88su0tLRw1llncfHFF1f1PcqZkkfSm4EOYF9gl6TzgGkR8Vvgs8BN6ZRZneRM12PZUG5ncq1Ue79b6gbD4P2uWTV4v2uWTc7dxJo1a5gyZQqdnZ07ImJHoVH66esb05HAqySNlnQA0EqREf7p8XGvffhjf9ZcYHlEvAI8mV6dfTjge6BZWZy75aukU7ophxFXbYjszL+tznZsSNm1axdrOx7gxuuW1mT7X734Qr56cZ9zM2ZWoV27drFq1SpuvbV6U0Tkuvzyy7n88strsu1cZUzJ8wzJ/rhQ3YdIri4xywzvd82yaajsd82Gm2bM3a1btzJxYm7XU8FR+oVG8k8osnx3XUmXAqcALwDH5GxrVYFtmTWtZszdclTSKe1hxDasPPbY43xs3imccPxsphz4tkaHY2ZlevTRRznhhBM46aSTmDp1aqPDsTqo5vzU1jje75pl03DZ70q6DjgB2B4R0/spN4Okg+uvIuI79YrPbKCaNXeTi5/7Ls57XWwkf78j/CNiEbAonUN6AfClUnX2eFNPN2tNoFlztxyD7pT2MGIbbt7+9oP5+QOrSxc0s6Yybdo0Ojs7Gx2GmQ2Q97tm2TSM9rs3AIuBG4sVSOez/RrJ/8xmTa1Zc7elpYUtW7bssYi+o/SLjeQfVWR5vmXAf5J0Spc9K4Cnm7Vm0Ky5W45KrpT2MGIzMzMzMzMbdiLiXkmtJYp9FrgNmFHzgMyGqBkzZrBx40aAUelFjYVG6a8AFqRzRs8EXoiIbZK6KTLCX9LUiNiY1j8ReCxnW8sk/RPJjQ6nAmtq1kCzYew1jQ7AzMzMzMzMbCiRNAE4CVhSRtn5kjokdXR3d9c+OLMMGTlyJIsXLwY4CNgA3NI7Sr93pD7JxZKdwCbg34DPQDLCn2Rajjtz66Z1LpO0TtIjwPuBhWmd9cAtJFPMrgTaI6Kn9i01G34qulLazMzMzMzMzPr4F+CLEdEjFZqi9o88BYBZ/+bMmQOwLiJ2j7bPG6UfQHuhuoVG+KfLP1Ls/SLiUuDSCkI2szK4U7rGlq3eXJXtzJvpSfPNzMzMzMwyog1YnnZIjwPmSNoZEd9rbFhmZmbNwZ3SRVSrM9lqZ+S6m6u6vZ3T/6qq2yvHz/7nPs5fdBHr1m/ghmuX8KETT6h7DGZ113F9dbfXVv/75N57772cd955PPLIIyxfvpyPfvSjdY/BrN683zXLKO93GyIiJvc+l3QD8AN3SNuAOHfNssm5WzZ3SmdFtT/U1hQmtrSwZPFVfH3xNY0OxcwGYNKkSdxwww1ceeWVjQ7FzAbA+12zbGrG/a6kbwNHA+MkdQFfAl4Le04rYDacNWPumllp9cpdd0oXceDmW6uynScmfawq21n95HNV2Y4N3iV//zXGjh3DZ87+awC+/NV/YL/x4/n02WcNeptvnTQRAL3G9xw1q5ULL7yQcePGsXDhQgAWLVrE/vvvz7nnnjvobba2tgLwGueuWc14v2uWTcNlvxsRHx9A2dNqGIpZVQyX3DUbarKcu+6UrrFqdW5b453yyXl84tQz+MzZf82uXbu47fbv899397lfAu8/fi4vvfS7Pssv/fJFHHP0UfUI1cxynHnmmXz4wx9m4cKF7Nq1i+XLl7NmzZo+5Y488khefPHFPsuvvPJKjj322HqEamY5vN81yybvd82yyblrlk1Zzl13SpuV6a2TJjLmTWN4+JGfs727m3f+6XTGjhnTp9xd//n9BkRnZsW0trYyduxYHnzwQZ599lkOPfRQxo4d26fcT3/60wZEZ2bFNPN+d+XKlQDTJW0Cro2Iy3LXK7mz2VXAHOBl4LSIeCBdNytdNyK3rqRLgLnALmB7WufpdN0FwJlAD3BuRNxZ80aaDZL3u2bZ5Nw1y6Ys5647pc0G4NRPzeOmb9/Ms9u7+dQnCo/Y8xVbZs3nrLPO4oYbbuCZZ57hjDPOKFimGc8cmw13zbjf7enpob29HeAXQBuwVtKKiHg0p9hsYGr6mAlcA8yUNAK4GjgO6Mqre0VEXAgg6VzgIuAcSdOAk4F3AG8BfiTpoIjoqXrjzKrE+12zbHLummVTVnPXndJmA/DB42fz1X+4gp07X+W6pf9asIyvlDZrPieddBIXXXQRr776KsuWLStYphnPHOcrdoVlzvq3A9cDhwGLIuLKvPUjgA5ga0ScUJ+ozQavGfe7a9asYcqUKXR2du6IiB2SlpNc4ZzbKT0XuDEiAlglabSkA4BWYFNEdALk1o2I3+bU3weInG0tj4hXgCfTq7MPB+6rYTPNKjJU9rtmw41z1yybspq77pS2zNo5/a/q/p6jRo3iqCPfzRv3fSMjRoyoeHv3P/AQ8045g9+88Bt+eOfdXHrZFaz9n59UIVKzJtZ2et3fctSoURxzzDGMHj26Krm7du1aTjrpJJ5//nn+4z/+gy996UusX7++CpEWV+IKy17PAecCHyqymYXABmDfWsZqQ5P3u4mtW7cyceLE3EVdJFdD55oAbMkrM6HI8t11JV0KnAK8APx/9u4/Ws6rvu/9+xML4SRAzA+RGEuuhSVDHJcY58hymvIjcQi2oChpoMuQxI5JrqsiN6GEG+xymzTN5S4SkjShAquCmOIG4hoMjdoIGYcGuF3BlgQY4x8YyTK1JAsscGpIzLWQ/L1/zHPEaDRzzhzNOXNmznm/1pqlmf3s/ZzvPprn7Jn97B8/2Xau27qc6zhJrgKugtaO6dIxtrvSePLalcaT127f7JSWZuCJJ55g567PccP1W2blfD92wfncd9fnZuVcknp74oknuO222/jQh2Zn89k1a9awf//+WTnXDFxIjxGWkxmq6mHg4SSv6CycZDnwCuBtwJuGErE0oFFsd1uDn09M7nidHnl6pU+e+63AW5s1pK8Gfnu6Mm1ltwBbACYmJroGKQ3LAml3pUXHa1caT+N67X7PIIWTXJLkviR7klzT5fjzk3wmyeNJ3tzl+ClJPp/kvw8ShzQMX/rSffzoxI/zkhf/Y1ad/dz5DkdSn+655x5WrVrFxRdfzOrVq+c7nEH0GnnZrz8GfpPWJmrSyBvVdnf58uXs27fvuCTgoY5s+4EVXfL0Su/0QeDnpzmXNJIWULsrLSpeu9J4Gudr96RHSjuNWIvN85//PL74udvnOwxJM3Tuueeyd+/e+Q5jNvQ1WrJrweSVwMNV9dkkL50mr0sAaCSMaru7Zs0adu/eDbA0yVJamxC+riPbVuDqZkbDWuDRqjqY5BCwOslK4EB72SSrq2p3U/5VwJfazvXBJH9Ea6PD1cCOOaugNKAF1O5Ki4rXrjSexvnaHWSk9LFpxFV1GJicRnxMVT1cVTuB73QWbptG/N4BYtCiUr2mzC4ai73+Gl+L/b07S/UfZLTkTwCvSvIVWu31TyX5s24Zq2pLVU1U1cSyZcsGiVdjz3YXTrx+lyxZwqZNmwDOoTW44qaqujvJhiQbmmzbgL3AHuA9wBuacx2htSzHLe1lmzJvT3JXkjuBn6E1eIPm+E20lurZDmysqqNzVF0tEF67/g40nnzf+jvQePJ9e3K/g0HWlJ5yo5Y+TE4jfuoAMWgReeLxv+PRb/09P/DU7yfpNmBwYasqvvGNb3DqqafOdyjSjJx66ql84xvf4JnPfKbX7mB20mOEZR8xXAtcC9CMlH5zVf3ioAFpYVvs7S70vn7XrVsHcFdVTbTl3dz2vICNPc65jVandWf6z3fJPnnsbbTWg5emtdjbXfBzs8aT167XrsaT1+7JX7uDdEo7jVhD9Z2v3sPDwNef/BS6v/0Wpid/7W+PPT/11FNZvnz5PEYjzdzy5cvZv38/hw4dmu9Q5s1sXLtVdSTJ5AjLU4DrJ0dnNsc3J/khYBetZbGeSPJG4Nyq+uZgNdBiZLvbYturcWO72+K1q3Hjtdvitatx47XbcjLX7iCd0rMxjXgdcCrwtCR/1m3UljuJ65gnvsN3HvrCfEcxdOe/5jfmOwRpIE960pNYuXLlfIexIHQbYdkxOvOrtNrjqc7xSeCTcxCeFhrbXWks2e5K48lrVxpPXrsnb5A1pY9NI27b5GVrPwWr6tqqWl5VZzXl/ofTiCVJkiRJkiRp4TvpkdJOI5YkSZIkSZIkzdQgI6Wpqm1VdU5Vnd1swEJVbZ6cSlxVX21GRD+tqk5rnn+z4xyfrKpXDhKHpO6SXJLkviR7klzT5XiSvLM5fmeSC6Yrm+T8JLcluSPJriQXDqs+kiRJkiRJGn8DdUpLGl1JTgHeBVwKnAu8Nsm5HdkuBVY3j6uA6/oo+/vA71TV+cBvNa8lSZKkRSPJ9UkeTnJXj+O/0Az6uDPJ3yT50WHHKEnSKLNTWlq4LgT2VNXeqjoM3Ais78izHrihWm4DTkty+jRli9aSPAA/QP8bnEqSJEkLxX8CLpni+APAS6rqBcDvAluGEZS0EG3fvh3gvFmeAfyOJF9q8n80yWlN+llJvt3MDL4jyebOnydpdtgpLS1cZwD72l7vb9L6yTNV2TcC70iyD/gD4NpZjFmSJEkaeVX1aeCRKY7/TVX9bfPyNmD5UAKTFpijR4+yceNGgC8zuzOAbwXOa24cfZnjv9feX1XnN48Nc1MzSXZKSwtXuqRVn3mmKvsvgH9VVSuAfwX8adcfnlzVrDm969ChQ32GLEmSJC04vwJ8rNdBPzdLve3YsYNVq1YBHJ7NGcBV9fGqOtKU98aRNA/slJYWrv3AirbXyzlxqY1eeaYqewXwkeb5h2g19Ceoqi1VNVFVE8uWLTupCkiSJEnjLMlP0uqUfkuvPH5ulno7cOAAK1a0fzWdtRnA7V7P8TeOVib5fJJPJXlRr9i8oSQNxk5paeHaCaxOsjLJUuAyYGtHnq3A5c0aXBcBj1bVwWnKPgS8pHn+U8Duua6IJEmSNG6SvAB4L7C+qr4x3/FI46iqc7JvK7nj9cnMAG4VTN4KHAE+0CQdBM6sqhcCbwI+mORpdOENJWkwS+Y7AElzo6qOJLkauAU4Bbi+qu5OsqE5vhnYBqwD9gCPAVdOVbY59f8B/EmSJcD/R2vNLkmSJEmNJGfSml34S1X15fmORxpXy5cvZ9++fccl0f8M4KU90gFIcgXwSuDianq/q+px4PHm+WeT3A+cA+yajfpI+i47paUFrKq20ep4bk/b3Pa8gI39lm3S/yfwY7MbqSRJkjQ+kvw58FLgWUn2A78NPAmOfd7+LeCZwLuTABypqon5iVYaX2vWrGH37t0AS9tm8b6uI9tW4OokNwJraWYAJzlEMwMYONBeNskltJbVeUlVPTZ5oiTLgEeq6miS59LaPHHvnFZSWqTslJYkSZIkaQaq6rXTHP9V4FeHFI60YC1ZsoRNmzbxile84hzgXmZvBvAm4MnArc2No9uqagPwYuDfJTkCHAU2VNUjQ6qutKjYKS1JkiRJkqSRtG7dOoC72mcbzMIM4FU98t8M3DxgyJL64EaHkiSNiSSXJLkvyZ4k13Q5/vwkn0nyeJI3t6WvSPLXSe5NcneSXx9u5JIkSZIkfZcjpSVJGgNJTgHeBbyM1mYuO5Nsrap72rI9Avwa8LMdxY8Av1FVn0vyVOCzSW7tKCtJkiRJ0lA4UlqSpPFwIbCnqvZW1WHgRmB9e4aqeriqdgLf6Ug/WFWfa55/i9Z6fGcMJ2xp4dm+fTvAeVPMWkiSdzbH70xyQduxrjMekrwjyZea/B9NclqTflaSbye5o3ls7vx5kiRJ0rgZqFPaacSSJA3NGcC+ttf7OYmO5SRnAS8Ebp+VqKRF5ujRo2zcuBHgy8C5wGuTnNuR7VJgdfO4CrgOjpvxcGmXsrcC51XVC5pzX9t2vvur6vzmsWFuaiZJkiQNz0l3Sk/zoXrS5DTiP+hIn5xG/MPARcDGLmUlSdJ3pUtazegEyVNobdzyxqr6Zo88VyXZlWTXoUOHTiJMaWHbsWMHq1atAjjca9ZC8/qGarkNOC3J6Uwx46GqPl5VR5rytwHLh1AdSZIkaV4MMlLaacSSJA3PfmBF2+vlwEP9Fk7yJFod0h+oqo/0yldVW6pqoqomli1bdtLBSgvVgQMHWLGi/VLsOmuh18yGfmc8vB74WNvrlUk+n+RTSV7ULS5vKEmSJGmcDNIpPZRpxH7AliQJgJ3A6iQrkywFLgO29lMwSYA/Be6tqj+awxilBa+q6wSFzsReMxumnfGQ5K20ZhV+oEk6CJxZVS8E3gR8MMnTusTlDSVJkiSNjSUDlB3KNOKq2gJsAZiYmJjR+SVJWiiq6kiSq4FbgFOA66vq7iQbmuObk/wQsAt4GvBEkjfSWmLrBcAvAV9Mckdzyn9dVduGXhFpzC1fvpx9+/Ydl8SJsxZ6zWxY2iMdgCRXAK8ELq6m97uqHgceb55/Nsn9wDm0rnVJWjh2vW92zjNx5eycR5I0pwbplB7KNGJJktTSdCJv60jb3Pb8q3Rfh/Z/0v1msqQZWrNmDbt37wZY2jZr4XUd2bYCVye5EVgLPFpVB5McopnxABxoL5vkEuAtwEuq6rHJEyVZBjxSVUeTPJfW5ol757SSkiRJ0hwbZPkOpxFLkiRpUVmyZAmbNm2C1mjle4GbJmctTM5coHXzaC+wB3gP8AZozXgAJmc8HCvblNkEPBW4NckdSSZvOL0YuDPJF4APAxuq6pG5rqckSZI0l056pLTTiCVJkrQYrVu3DuCuqpqYTOuYtVDAxm5lu814aNJX9ch/M63ZhZIkSdKCMcjyHU4jliRJkiRJkiTNyCDLd0iSJEmSJEmSNCN2SkuSJEmSNANJrk/ycJK7ehxPkncm2ZPkziQXDDtGSZJG2UDLd0iSJEmStAj9J1oblN7Q4/ilwOrmsRa4rvlX0iL1wdsfnJXzvG7tmbNyHmm+2SktSZIkSdIMVNWnk5w1RZb1wA3Nxqe3JTktyelVdXAoAY6h2x94ZFbOs3Zi+jySpPlnp7QkSZIkSbPrDGBf2+v9TZqd0ovNrvfNznkmrpyd80jSiHBNaUmSJEmSZle6pFXXjMlVSXYl2XXo0KE5DkuSpNHgSGlJN6LwmAAAIABJREFUkiRJkmbXfmBF2+vlwEPdMlbVFmALwMTERNeO63ZnP/ih2YgP1v7G7JxHkqST4EhpSZIkSZJm11bg8rRcBDzqetLSydm+fTvAeUn2JLmm83hznb2zOX5nkgvajl2S5L7OsknekeRLTf6PJjmt7di1Tf77krx8rusnLVZ2SkuSJEmSNANJ/hz4DPC8JPuT/EqSDUk2NFm2AXuBPcB7gDfMU6jSWDt69CgbN24E+DJwLvDaJOd2ZLsUWN08rgKuA0hyCvCu5nhn2VuB86rqBc25r23KnAtcBvwIcAnw7uY8kmaZy3dIkiRJkjQDVfXaaY4XsHFI4UgL1o4dO1i1ahV79+49XFWHk9wIrAfuacu2Hrihue5uS3JaktOBs4A9VbUXoL1sVX28rfxtwKvbznVjVT0OPJBkD3AhrZtQkmaRI6UlSRoTvaYfth1/fpLPJHk8yZtnUlaSJEkaNQcOHGDFivbl2dkPnNGR7QxgX5c8vdI7vR742DTnOoGblEqDsVNakqQxMM30w0mPAL8G/MFJlJUkSZJGSmvw84nJHa/TI0+v9O8WTN4KHAE+MM25usW2paomqmpi2bJl3bJImsJAndKO2JIkaWgupJl+WFWHgcnph8dU1cNVtRP4zkzLSpIkSaNm+fLl7Nu377gk4KGObPuBFV3y9EoHIMkVwCuBX6jv9n5PWUbS7DnpTmlHbEmjr48bRzPepbg59i+bY3cn+f1h1EVS/1MJZ7msJEmSNC/WrFnD7t27AZYmWUprE8KtHdm2Apc3328vAh6tqoPATmB1kpWdZZNcArwFeFVVPdZxrsuSPDnJSlqbJ+6YwypKi9YgI6UdsSWNsD5v/sx4l+IkP0nren1BVf0IHTedJM2ZvqcSDlLWtfEkSZI0KpYsWcKmTZsAzgHuBW6qqruTbEiyocm2DdgL7AHeA7wBoKqOAFcDt7SXbcpsAp4K3JrkjiSbmzJ3AzfR2khxO7Cxqo7OfU2lxWfJAGW7jbpaO4Sykvpz7OYPHL/TcFueGe9SDPwL4O3NbsRU1cNDqo+02A0ylbDvslW1BdgCMDEx0W+ntyRJkjQn1q1bB3BXVU1MplXV5rbnBWzsVraqttHqtO5MX9Xr51XV24C3DRCypD4MMlLaEVvSaOtnuv7J7FJ8DvCiJLcn+VSSNd1+uNeuNOt6Tj+c47KSOmzfvh3gvNlcHivJO5J8qcn/0SSntR27tsl/X5KXz3X9JEmSpLk2SKf00EZsuZupdFL6uflzMrsULwGeDlwE/J/ATUlOyO+1K82uXtMP26cuJvmhJPuBNwH/V5L9SZ42zdRFSTNw9OhRNm7cCPBlZnF5LOBW4LyqekFz7mubMufSupH0I8AlwLub80iSJElja5DlO46NugIO0Pqw/LohlJXUn35u/vTKs3SKsvuBjzRTpHYkeQJ4FuBwaGmOdZt+2DF18au0rte+ykqauR07drBq1Sr27t17uKoOz9byWFX18bbytwGvbjvXjc2yWQ8k2UNria7PzGE1JUmSpDl10iOlHbEljbx+puvPeJdi4L8CPwWQ5BxaHdhfn/vqSJI0/w4cOMCKFe33bWdteax2rwc+Ns25juOyWZIkSRong4yUdsSWNMKq6kiSyZs/pwDXT944ao5vpnUNrqO1S/FjwJVTlW1OfT1wfZK7gMPAFc1IMEmSFrweTd5sLI/VKpi8FTgCfGCac3XG5SalkiRJGhsDdUpLGm193Dg6mV2KDwO/OLuRSpI0HpYvX86+ffuOS2J2lsciyRXAK4GL2274DrKPS2+73sfZDz4y8GkkSZKkkzHIRoeSJEnSorJmzRp2794NsHQ2l8dKcgnwFuBVVfVYx7kuS/LkZj+W1cCOOayiJEmSNOfslJYkSZL6tGTJEjZt2gRwDj32VaE102gvreWx3gO8AXrvydKU2QQ8Fbg1yR1JNjdl7gZuorWR4nZgY1UdnfuaSpIkSXPH5TskSZKkGVi3bh3AXVU1MZk2C8tjrer186rqbcDbBghZkiRJGimOlJYkSZIkaQaSXJLkviR7klzT5fgPJPlvSb6Q5O4kV85HnJIkjSpHSkuSJEmS1KckpwDvAl5GazPSnUm2VtU9bdk2AvdU1T9Jsgy4L8kHmk3DJS1CZz/4oZMue/+Zr5nFSKTR4EhpSZIkSZL6dyGwp6r2Np3MNwLrO/IU8NQkAZ4CPAIcGW6YkiSNLjulJUmSJEnq3xnAvrbX+5u0dpuAHwYeAr4I/HpVPdHtZEmuSrIrya5Dhw7NRbySJI0cO6UlSZIkSepfuqRVx+uXA3cAzwHOBzYleVq3k1XVlqqaqKqJZcuWzW6kkiSNKDulJUmSJEnq335gRdvr5bRGRLe7EvhItewBHgCeP6T4JEkaeXZKS5IkSZLUv53A6iQrkywFLgO2duR5ELgYIMkPAs8D9g41SkmSRpid0pIkjYkklyS5L8meJNd0OZ4k72yO35nkgrZj/yrJ3UnuSvLnSU4dbvSSJC0MVXUEuBq4BbgXuKmq7k6yIcmGJtvvAv8oyReBTwBvqaqvz0/EkiSNniXzHYAkSZpeklOAdwEvozVteGeSrVV1T1u2S4HVzWMtcB2wNskZwK8B51bVt5PcRGtU138aYhUkSVowqmobsK0jbXPb84eAnxl2XJIkjYuBRko7YkuSpKG5ENhTVXur6jBwI7C+I8964IZm/crbgNOSnN4cWwJ8b5IlwPdx4tqXkiRJkiQNxUl3SreN2LoUOBd4bZJzO7K1j9i6itaILdpGbE1U1XnAKbRGbEmSpO7OAPa1vd7fpE2bp6oOAH9Aa33Lg8CjVfXxbj8kyVVJdiXZdejQoVkLXpIkSToZ27dvBzjvJAdEdh1MmeQ1zUDJJ5JMtKWfleTbSe5oHps7f56k2THISGlHbEmSNDzpklb95EnydFpt8krgOcD3J/nFbj+kqrZU1URVTSxbtmyggCVJkqRBHD16lI0bNwJ8mZkPiJxqMOVdwD8FPt3lx95fVec3jw1djkuaBYN0SjtiS5Kk4dkPrGh7vZwTb+j2yvPTwANVdaiqvgN8BPhHcxirJEmSNLAdO3awatUqgMMnMSCy52DKqrq3qu4bWkUknWCQTmlHbEmSNDw7gdVJViZZSmvZq60debYClzdTGC+iddP3IK2bwBcl+b4kAS4G7h1m8JIkSdJMHThwgBUr2sdc9D8gcor06axM8vkkn0ryol6ZHEQpDWaQTmlHbEmSNCRVdQS4GriFVofyTVV1d5INSSanFW4D9gJ7gPcAb2jK3g58GPgc8EVa7f+W4dZAkiRJmpmqzrGPreSO170GTfYzmLLTQeDMqnoh8Cbgg0me1iM2B1FKA1gyQNljI7aAA7RGbL2uI89W4OokNwJraUZsJTk2Ygv4Nq0RW7sGiEWSpAWvqrbR6nhuT9vc9ryAjT3K/jbw23MaoCRJkjSLli9fzr59+45Lov8BkUt7pPdUVY8DjzfPP5vkfuAc7LOSZt1Jj5R2xJYkSZIkSZLmypo1a9i9ezfA0pNYwq6f5e+Ok2RZs0EiSZ5La/PEvbNaKUnAYMt3UFXbquqcqjq7qt7WpG2eHLXVLDK/sTn+D6tqV1vZ366q51fVeVX1S83dKEmSJGmkbd++HeC8JHuSXNN5vPlS/M7m+J1JLmg7dkmS+zrLJnlNkruTPJFkoi39rCTfTnJH89jc+fMkSVqolixZwqZNm6A1WnmmAyK7DqYESPJzSfYDPw78ZZJbmnO9GLgzyRdoDabcUFWPDKGq0qIzyPIdkiRJ0qJy9OhRNm7cCPBlYALYmWRrVd3Tlu1SWiOrVtNawu46YG0z8updwMtoTTVuL3sX8E+B/9jlx95fVefPVZ0kSRpl69atA7irqo7dtJ3BEnYnLH/XpH8U+GiX9JuBmwePWtJ0BhopLUmSJC0mO3bsYNWqVQCHq+owcCOwviPbeuCGZtbgbcBpSU4HLgT2VNXezrJVdW9V3Te0ikiSJEnzyE5pSZIkqU8HDhxgxYr2PZPYD5zRke0MYF+XPL3Sp7MyyeeTfCrJi7plSHJVkl1Jdh06dKiPU0qSJEnzx05pSZIkqU+tGcInJne8To88vdKnchA4s6peCLwJ+GCSp3WJa0tVTVTVxLJly6Y5pSRJkjS/7JSWJEmS+rR8+XL27dt3XBLwUEe2/cCKLnl6pfdUVY9X1Tea558F7qe12ZMkSZI0tuyUlhawJJckuS/JniTXdDmeJO9sjt+Z5IIZlH1zkkryrLmuhyRJo2LNmjXs3r0bYGmSpcBlwNaObFuBy5t29iLg0ao6COwEVidZOUXZ4yRZ1myQSJLn0to8ce+sVkqSJEkasiXzHYCkudF8gX0X8DJaI7N2JtlaVfe0ZbuU1pfb1cBa4Dpg7XRlk6xojj04rPpIkjQKlixZwqZNm3jFK15xDnAvcH1V3Z1kA0BVbQa2AeuAPcBjwJXNsSNJrgZuAU6ZLAuQ5OeA/wAsA/4yyR1V9XLgxcC/S3IEOApsqKpHhlhlSV0kuQT4E1rX8nur6u1d8rwU+GPgScDXq+olQw1SksbIB28fvHvh7AcfYe3KZ8xCNBoGO6WlhetCYE9V7QVIciOwHmjvlF4P3FCtBTJvS3JaktOBs6Yp+++B3wT+YhgVkSRplKxbtw7grqqamExrOqMnnxewsVvZqtpGq9O6M/2jwEe7pN8M3Dx41JJmSz+DP5KcBrwbuKSqHkzy7PmJVpKk0eTyHdLCdQbQvujl/iatnzw9yyZ5FXCgqr4w2wFLkiRJY+DY4I+qOgxMDuBo9zrgI1X1IEBVPTzkGCVJGml2SksLV7qkVZ95uqYn+T7grcBvTfvDk6uS7Eqy69ChQ9MGK0mSJI2JfgZ/nAM8Pcknk3w2yeW9TubnZknSYmSntLRw7QdWtL1eDjzUZ55e6WcDK4EvJPlKk/65JD/U+cOraktVTVTVxLJlywasiiRJkjQy+hn8sQT4MeAVwMuBf5PknG4n83OzJGkxck1paeHaCaxOshI4AFxGaxphu63A1c2a0WuBR6vqYJJD3co2mzEdWw+v6ZieqKqvz3ltJE27qVKSNMfX0dpc7Zer6nPNsdOA9wLn0fri/Pqq+swQw5ckaaHod/DH16vq74G/T/Jp4EeBLw8nRElanG5/4OT2g77/6PEbLb5u7ZmzEY6mMFCntF+OpdFVVUeSXA3cQusavb6q7k6yoTm+mdZGS+uAPbSu0SunKjsP1ZDU6GdTJeBSYHXzWAtc1/wLrfZ4e1W9OslS4PuGFrykBW3KL38P/GHf51m78hkwceUsRCTNuX4Gf/wFsCnJEmAprfb43w81SkmSRthJd0r75VgafVW1jVbHc3va5rbnBWzst2yXPGcNHqWkPh3bVAmgmeGwHmhvd9cDNzTX9m1JTktyOvD3wIuBXwZoNmU6PMTYJUlaMPoZ/FFV9ybZDtwJPEFrENdd8xe1JEmjZZCR0n45liRpeLptqrS2jzxnAEeAQ8D7kvwo8Fng15spxcdJchVwFcCZZzplTZKkbqYb/NG8fgfwjmHGJUnSuBhko8N+dhzulee5fPfL8eeTvDfJ93f7Ie5ELEkS0N+mSr3yLAEuAK6rqhfSujl8Tbcf4mZLkiRJkqS5NkintF+OJUkann43VeqWZz+wv6pub9I/TKsdliRJkiRp6AbplPbLsSRJw3NsU6VmL4bLgK0debYCl6flIuDRqjpYVV8F9iV5XpPvYo5fbkuSJEmSpKEZZE3pfnYc3gpc3aw3vZbmyzFAkn1JnldV9+GXY2nRO/vBD3U/cMozZnaiiSsHD0YaQf1sqkRrbct1wB7gMaD9gviXwAeaDu29HcckSZIkSRqak+6U9suxJEnDNd2mSs3Gwht7lL0DmJjTACVJkiRJ6sMgy3dQVduq6pyqOruq3takbZ78glwtG5vj/7CqdrWVvaNZK/oFVfWzVfW3g1VFkiRJkiRJC8n27dsBzkuyJ8kJ+5E1S9e9szl+Z5IL2o5dkuS+zrJJXpPk7iRPJJnoON+1Tf77krx8LusmLWaDLN8hSZIkSZKkHm5/4JFZOc/aRTrf7ejRo2zcuBHgy7Rm/e1MsrWq2peAvRRY3TzWAtcBa5OcArwLeBmtvc3ay94F/FPgP7b/vCTn0lqe9keA5wB/leScqjo6h9WUFqWBRkpLkiRJkiRJc2HHjh2sWrUK4HBVHQZuBNZ3ZFsP3NDM1r8NOC3J6cCFwJ6q2ttZtqrubfY467QeuLGqHq+qB2gtR3vhnFROWuTslJYkSZIkSdLIOXDgACtWrGhP2g+c0ZHtDGBflzy90qdyMmUknQQ7pSVJkiRJkjRyWvt4n5jc8To98vRKn0rfZZJclWRXkl2HDh2a5rSSOtkpLUmSJM2AGy5JkjQcy5cvZ9++fcclAQ91ZNsPrOiSp1f6VPouU1VbqmqiqiaWLVs2zWkldbJTWpIkSepTx4ZL5wKvbTZFate+4dJVtDZcom3DpUu7lJ3ccOnT7Sfq2HDpEuDdzXkkSVrw1qxZw+7duwGWJllKq03c2pFtK3B5c1P4IuDRqjoI7ARWJ1k5RdlOW4HLkjw5yUpabfmOWaySpIad0pIkSVKf3HBJkqThWbJkCZs2bQI4B7gXuKmq7k6yIcmGJts2YC+tNvI9wBsAquoIcDVwS3tZgCQ/l2Q/8OPAXya5pSlzN3ATcA+wHdhYVUeHUllpkVky3wFIkiRJ46LHhktrO7LNZMOlzrKdzgBu63Ku4yS5itaobM4888xpTilJ0vhYt24dwF1VdWx5q6ra3Pa8gI3dylbVNlqd1p3pHwU+2qPM24C3DRa1pOk4UlqSJEnq06huuOS6ltJw9Vofvku+NUmOJnn1MOOTJGnU2SktSZIk9WmUN1ySNBzTrA/fme/3aC0dIEmS2tgpLUmSJPXJDZckMcX68B3+JXAz8PAwg5MkaRzYKS1JkiT1yQ2XJNF73fhjkpwB/BywGUmSdAI3OpQkaUwkuQT4E+AU4L1V9faO42mOrwMeA365qj7XdvwUYBdwoKpeObTApQXGDZekRa+ftd7/GHhLVR1tNc9TnMyNSiVJi9BAI6Wn29yhmbL4zub4nUku6Dh+SpLPJ/nvg8QhSdJC1+f6lZfSmtq/mtaX2+s6jv86rdGZkiTp5PWz1vsEcGOSrwCvBt6d5Ge7ncyNSiVJi9FJd0r75ViSpKHqZ/3K9cAN1XIbcFqS0wGSLAdeAbx3mEFLkrQATbs+fFWtrKqzquos4MPAG6rqvw4/VEmSRtMgI6X9cixJ0vBMu37lNHn+GPhN4ImpfkiSq5LsSrLr0KFDg0UsSdIC1Gt9+I615SVJ0hQGWVO62xfftX3kOQM4yHe/HD91qh/i+lqSJAH9rV/ZNU+SVwIPV9Vnk7x0qh9SVVuALQATExOd55ckSXRfH759bfmO9F8eRkySJI2TQUZKz8qX4+l+iOtrSZIE9Ld+Za88PwG8qlnX8kbgp5L82dyFKkmSJElSb4N0SvvlWJKk4Zl2/crm9eXNRsMXAY9W1cGquraqljfrWl4G/I+q+sWhRi9JkiRJUmOQTmm/HEuSNCR9rl+5DdgL7AHeA7xhXoKVJEmSJGkKJ72mdFUdSTL55fgU4PrJL8fN8c20vhyvo/Xl+DHgysFDltSvJJcAf0LrGn1vVb2943ia4+toXaO/XFWfm6pskncA/wQ4DNwPXFlV/3s4NZIWt+nWr6yqAjZOc45PAp+cg/AkSZIkSerLIBsd+uVYGmFJTgHeBbyM1lI6O5Nsrap72rJdCqxuHmuB64C105S9Fbi2uTH1e8C1wFuGVS9JkiRJkrQwnf3gh+Y7BA3JQJ3SkkbahcCeqtoLkORGYD3Q3im9HrihuYF0W5LTkpwOnNWrbFV9vK38bcCr57wmkiRJkqSR8sHbH5yV87xu7Zmzch5J42WQNaUljbYzgH1tr/c3af3k6acswOuBj3X74UmuSrIrya5Dhw7NMHRJkiRJkiQtVHZKSwtXuqRVn3mmLZvkrcAR4APdfnhVbamqiaqaWLZsWR/hSpIkSZIkaTFw+Q5p4doPrGh7vRx4qM88S6cqm+QK4JXAxc3SH5IkSZIkSVJfHCktLVw7gdVJViZZClwGbO3IsxW4PC0XAY9W1cGpyia5hNbGhq+qqseGVRlJkiRJkiQtDHZKSwtUVR0BrgZuAe4Fbqqqu5NsSLKhybYN2AvsAd4DvGGqsk2ZTcBTgVuT3JFk87DqJEmSJElaXLZv3w5wXpI9Sa7pPN4Msnpnc/zOJBe0HbskyX2dZZM8I8mtSXY3/z69ST8rybeb77p+35XmkMt3SAtYVW2j1fHcnra57XkBG/st26SvmuUwJUmSJElj5uwHPzQ7J1r7Gz0PHT16lI0bNwJ8GZgAdibZWlX3tGW7FFjdPNYC1wFrk5wCvAt4Ga2lK9vLXgN8oqre3nRWX0NrRjDA/VV1/uxUTlIvjpSWJEmSJEnSyNmxYwerVq0COFxVh4EbgfUd2dYDN1TLbcBpSU4HLgT2VNXeLmXXA+9vnr8f+Nk5roqkDnZKS5IkSTPgNGJJkobjwIEDrFixoj1pP3BGR7YzgH1d8vRKB/jBZj8lmn+f3ZZvZZLPJ/lUkhcNXgtJ3dgpLUmSJPWpYxrxucBrk5zbka19GvFVtKYR0zaN+NIuZSenEa8GPtG8nnR/VZ3fPDYgSdIi0Vpx8sTkjtfpkadX+lQOAmdW1QuBNwEfTPK0bhmTXJVkV5Jdhw4dmua0kjrZKS1JkiT1yWnEkqD3rIe247/QzJS4M8nfJPnR+YhTGnfLly9n3759xyUBD3Vk2w+s6JKnVzrA15q2mebfhwGq6vGq+kbz/LPA/cA53WKrqi1VNVFVE8uWLTuJ2kmLm53SkiRJUp9GdRqxo7Wk4Zlm1sOkB4CXVNULgN8Ftgw3SmlhWLNmDbt37wZYmmQpcBmwtSPbVuDyZvmsi4BHm7Z0J7A6ycouZbcCVzTPrwD+AiDJsuYaJ8lzac162jtnFZQWsYE6pfu4O9x1Pb0kK5L8dZJ7k9yd5NcHiUOSpMXAdleaf6M6jdjRWtJQTTXrAYCq+puq+tvm5W20RmhKmqElS5awadMmaI1Wvhe4qaruTrIhyeSSVttodRzvAd4DvAGgqo4AVwO3tJdtyrwdeFmS3cDLmtcALwbuTPIF4MPAhqp6ZI6rKS1KS062YNvd4ZfRGuWxM8nWqrqnLVv7enpraa2ntxY4AvxGVX0uyVOBzya5taOsJElq2O5Ko2HAacRLe6RDM424qg52TiMGHm+efzbJ5DTiXbNTI0knodush7VT5P8V4GO9Dia5itb685x55pmzEZ+0oKxbtw7grqqamEyrqs1tzwvY2K1sVW2j1Wndmf4N4OIu6TcDNw8etaTpDDJSetq7w/RYT6+qDlbV5wCq6lu07lh1TnuUJEnfZbsrjQCnEUtiBrMekvwkrU7pt/Q6mTMdJEmL0SCd0lOtidd3niRnAS8Ebu/2Q1wfT5IkwHZXGglOI5bE1JunHZPkBcB7gfWTG6dJkqSWk16+g/7uDk+ZJ8lTaE2LeGNVfbPbD6mqLTSbQkxMTEy35p4kSQuV7a40IpxGLC16x2Y9AAdozXp4XXuGJGcCHwF+qaq+PPwQJUkabYN0Svdzd7hnniRPovUB+wNV9ZEB4pAkaTGw3ZUkaQRU1ZEkk7MeTgGun5wx0RzfDPwW8Ezg3UkAjrTfyJIkabEbpFN62rvDtNbGuzrJjbQ2fni02bwlwJ8C91bVHw0QgyRJi4XtrqSFb9f7Zl5m4srZj0OaRrdZDx0zJn4V+NVhxyVJ0rg46U7pPu8ObwPW0VpP7zFg8hPjTwC/BHwxyR1N2r9uGnZJktTBdleSJEmStFAMMlK6n7vDXdfTq6r/Sfd1LyVJUg+2u5IkSZKkheB75jsASZIkSZIkSdLiYae0JEmSJEmSJGlo7JSWJEmSJEmSJA2NndKSJEmSJEmSpKEZaKNDSZIkSZoNtz/wyEmVu//og8e9ft3aM2cjHEmSJM0hR0pLkiRJkiRJkobGkdKSJEmSJEmSFq2zH/zQ8QmnPKP/whNXzm4wi4QjpSVJkiRJkiRJQ2OntCRJkiRJkiRpaFy+Q9JI67Xp0dqVM5hKI0mSJEmSpJFhp7QkSZKksTXjNSBd91GSJGneuXyHJEmSJEmSJGloBuqUTnJJkvuS7ElyTZfjSfLO5vidSS7ot6ykwc3FNZrkGUluTbK7+ffpw6qPtNjZ7kqjYfv27QDnDat9TXJtk/++JC+f6/pJmt4gbbKkmbHdlRamk16+I8kpwLuAlwH7gZ1JtlbVPW3ZLgVWN4+1wHXA2j7LShrAHF6j1wCfqKq3N436NcBbhlUvabGy3ZVGw9GjR9m4cSPAl4EJ5rh9TXIucBnwI8BzgL9Kck5VHR1GfcdRr/0oJt1/9MG+zvO6tWfORjhagAZpk4cdqzTubHelhWuQNaUvBPZU1V6AJDcC64H2PwzrgRuqqoDbkpyW5HTgrD7KShrMXF2j64GXNuXfD3wSO6WlYbDdlUbAjh07WLVqFXv37j1cVYeH0L6uB26sqseBB5LsofX34DNzW9OF64Q1qHtpX5vadah1vJNuk6vq4PDDlcaX7a7my3Q3udtNd8PbG93dDdIpfQawr+31fk6889stzxl9lpU0mLm6Rn9w8sN0VR1M8uzZDLpfvRqItRNDDkQaHttdaQQcOHCAFStWtCfNdft6BnBbl3Npjh33WeOBP5xR2fvPfM2x534RXZAGaZPtlJZmwHZX42DaG969NmFe5De9B+mUTpe06jNPP2VbJ0iuAq5qXv5dkvv6jnD+PAv4+nwHMQSLoZ4jUMc395PpH3RJG8o12stJXLuz9Lvu6/c1ikbgvTavFmv9u127vdju9rZY3j/WcyimbUeeDjyN46/fuWxf+yozw2t3XN+B290GAAAgAElEQVRLYxT3sffRs35hbGI+zhj9ro/pN+aZtL29DNImn3gyPzdPZ4b1HZt69dKjvmNfrym8ear/45Fsd2HY1+53//9/4eROMBPj1AaMeayvn5dA+jCbv9ee7e4gndL7gfbbVcuBh/rMs7SPsgBU1RZgywBxDl2SXVW14MdrLoZ6jnkd5+oa/drk1MNmStTD3X74TK/dMf9dD8z6L+7698l2t4fF8v6xnqMhyY8D/7aqJjc+muv2tZ9rf0bX7qj/jnsZx7jHMWYYz7iHHPMgbfIJ/Nw8Neu78E1V51Ftd2HhXrvjEicY61wZVqzfM0DZncDqJCuTLKW1EPzWjjxbgcubnVAvAh5tpkf0U1bSYObqGt0KXNE8vwL4i7muiCTAdlcaFcNuX7cClyV5cpKVtDZx2jFXlZPUl0H+DkiaGdtdaYE66ZHSVXUkydXALcApwPVVdXeSDc3xzcA2YB2wB3gMuHKqsgPVRNJx5vAafTtwU5JfAR4EXoOkOWe7K42GYbevzblvorUp0xFgY1UdHU5tJXUzyN8BSTNjuystXGltTqrZlOSqZhrHgrYY6rkY6jgqFvvv2vov7vprMIvl/WM9NVvG9Xc8jnGPY8wwnnGPY8wnazHVFazvYrBY6jwu9RyXOMFY58qwYrVTWpIkSZIkSZI0NIOsKS1JkiRJkiRJ0ozYKT1HkrwjyZeS3Jnko0lOm++YZkuSS5Lcl2RPkmvmO565kGRFkr9Ocm+Su5P8+nzHtJAtxPdUkuuTPJzkrra0ZyS5Ncnu5t+ntx27tqn/fUle3pb+Y0m+2Bx7Z5IMuy4no9c1tJh+Bxou293xZrs7PKP8fkrylebv/R1JdjVpM243hhDn2LXxPWL+t0kONL/vO5KsG7GY/SzRYZSv37nQ7X27kC22tjDJqUl2JPlCU9/fme+YBpHkNU09nkgy0XFsRn+b0tpk8b806bcnOWsO4x7ptmCa2Efqb+Iof44Z2c8uVeVjDh7AzwBLmue/B/zefMc0S/U6BbgfeC6wFPgCcO58xzUH9TwduKB5/lTgywuxnqPwWKjvKeDFwAXAXW1pvw9c0zy/ZvLvAnBuU+8nAyub38cpzbEdwI8DAT4GXDrfdeuz/l2vocX0O/Ax3Ift7ng/bHeH9nse6fcT8BXgWR1pM243hhDn2LXxPWL+t8Cbu+QdlZj9LHH872Okr985qvMJ79uF/FhsbWFzPT6lef4k4HbgovmOa4D6/DDwPOCTwERb+oz/NgFvADY3zy8D/sscxj3SbcEUcY/c30RG+HNMt7+no9CeOlJ6jlTVx6vqSPPyNmD5fMYziy4E9lTV3qo6DNwIrJ/nmGZdVR2sqs81z78F3AucMb9RLVgL8j1VVZ8GHulIXg+8v3n+fuBn29JvrKrHq+oBWrtGX5jkdOBpVfWZarUAN7SVGWlTXEOL5neg4bLdHW+2u0Mzju+nGbUbwwhoHNv4HjH3Miox+1nieON4/Q5khu/bsbfY2sJq+bvm5ZOax9huelZV91bVfV0Onczfpva/cx8GLp6H0cij/jd1XP4mjsTnmFH97GKn9HC8ntYdhIXgDGBf2+v9LOCGEqCZKvNCWnduNfsW03vqB6vqILQ+dALPbtJ7/Q7OaJ53po+VjmtoUf4ONHS2u2PMdndOjfr7qYCPJ/lskquatJm2G/NlXNu3q9Na9uj6tmm7IxeznyWA0XvPaw4tlrYwySlJ7gAeBm6tqoVY35P523SsTDPo4lHgmXMY41i0BR1G8W/iuH2Omff2dMkghRe7JH8F/FCXQ2+tqr9o8rwVOAJ8YJixzaFud+fG9m7mdJI8BbgZeGNVfXO+41mgFtV7qodev4Ox/910XkNT3OBfsL8DzR7b3WMW7DVguzvnRv399BNV9VCSZwO3JvnSFHlHvS6TRrl9uw743ebn/i7wh7Ru6o1UzH6WOGah1EPTWExtYVUdBc5Pay+QjyY5r6pGdg3xfj6LdivWJW26v02zer1PFTdj0hZ0MSpxtFson2OG9n9vp/QAquqnpzqe5ArglcDFzdD2hWA/sKLt9XLgoXmKZU4leRKtDwMfqKqPzHc8C9iieU8BX0tyelUdbKa+PNyk9/od7Of4JQjG6nfT4xpaVL8DzS7bXWABXwO2u0Mx0u+nqnqo+ffhJB+lNY11pu3GfBm79q2qvjb5PMl7gP/evByZmP0scZxRe89rDizWtrCq/neSTwKXACPbKT3dZ9EeTuZv02SZ/UmWAD/AAEvZ9Bv3qLYFPYzc38Qx/Bwz7+2py3fMkSSXAG8BXlVVj813PLNoJ7A6ycokS2ktur91nmOadc16TX8K3FtVfzTf8Sxwi+I91dgKXNE8vwL4i7b0y9LaZXklsBrY0Uyh+VaSi5r35OVtZUbaFNfQovkdaLhsd8eb7e7QjOz7Kcn3J3nq5HNam5fexQzbjeFGfZyxa9+aL6CTfo7vdgSNRMx+ljjByF6/mh2LrS1MsqwZIU2S7wV+GphqZOm4Opm/Te1/514N/I+5GnAx6m3BFEbqb+KYfo6Z//a05nFnyoX8oLUQ+D7gjuaxeb5jmsW6raO1E/D9tKapzHtMc1DHf0xrGsKdbf+H6+Y7roX6WIjvKeDPgYPAd2jdUfwVWuuAfQLY3fz7jLb8b23qfx9tO9gCE7Qas/uBTUDmu2591r/rNbSYfgc+hvuw3R3vh+3uUH/XI/l+Ap5La6f3LwB3T8Z2Mu3GEGIduza+R8z/Gfhic91tBU4fsZj9LHHi72Qkr985rO8J79v5jmmO67uo2kLgBcDnm/reBfzWfMc0YH1+rnmfPg58Dbil7diM/jYBpwIfovX5dgfw3DmMe6TbgmliH5m/iYz455huf09HoT2dfMNLkiRJkiRJkjTnXL5DkiRJkiRJkjQ0dkpLkiRJkiRJkobGTmlJkiRJkiRJ0tDYKS1JkiRJkiRJGho7pSVJkiRJkiRJQ2OntCRJkiRJkiRpaOyUliRJkiRJkiQNjZ3SmlKSJyf50yT/K8m3knw+yaVd8v12kkry0/MRp6TjTXXtJjmruV7/ru3xb+Y7ZknTt7tJvi/Ju5N8PcmjST49n/FKapmm3f2Fjjb3saYd/rH5jlta7Ppod/9ZknubY/ck+dn5jFfSd/Vx/f5qkj1N27s9yXPmM16daMl8B6CRtwTYB7wEeBBYB9yU5B9W1VcAkpwNvBo4OF9BSjpBz2u3Lc9pVXVkPoKT1NN07e6WJs8PA48A589TnJKON9W1+wHgA5MZk/wy8G+Az81DnJKON9Vn5u8AfwasB7Y3xz6U5Kyqenie4pX0XVNdv/8A+H+AnwR2A38C/HmTVyMiVTXfMWjMJLkT+J2qurl5/THgPwDvBn61qv5qPuOT1N3ktQt8FngAeJKd0tLoa7t27wJ2Asur6pvzG5Wk6XR+Zm5L/2vgk1X1O/MTmaSptLW7+4H/VlXPbjt2CHhVVX1mvuKT1Fvb9fvjwPdW1cYm/TnAAWBVVd0/jyGqjct3aEaS/CBwDnB38/o1wOGq2javgUmaUue12/hfSfYneV+SZ81TaJKm0HHtrgX+F/A7zfIdX0zy8/MaoKSuerS7JPkHwIuBG+YjLklT67h2dwH3JnlVklOapTseB+6czxgldddx/aZ5HDvc/HvesONSb3ZKq29JnkRr6uH7q+pLSZ5CazrEG+c3MklT6bx2ga8Da2hNafox4Km0TSuWNBq6XLvLaX2QfhR4DnA18P4kPzx/UUrq1OXabXc58P9W1QPDj0zSVDqv3ao6SusG0gdpdUZ/EPjnVfX38ximpC66tL3bgH+W5AVJvhf4LaCA75vHMNXBTmn1Jcn3AP8ZOEzrSzC0pkT8Zz9US6Or27VbVX9XVbuq6khVfa1J/5kkT5vHUCW16dHufpvW+pb/d1UdrqpPAX8N/Mz8RCmpU49rt93lwPuHGpSkaXW7dpP8NPD7wEuBpbTWon1vEvdzkEZIj++8nwB+G7iZ1kzDrwDforUsj0aEndKaVpIAfwr8IPDzVfWd5tDFwK8l+WqSrwIraC0q/5Z5ClVSmymu3U6Tmwukx3FJQzTFtet0YWmETdfuJvkJWrMcPjwP4UnqYYpr93zg081gjieqaidwO/DT8xSqpA5Ttb1V9a6qWt2sC38zrY0R75qfSNWNndLqx3XADwP/pKq+3ZZ+Ma1pxOc3j4eAfw68a+gRSuqm67WbZG2S5yX5niTPBN5Ja8OlR+crUEnH6dXufprWzuLXJlnSdHC9FLhl+CFK6qLXtTvpCuDmqvrWcMOSNI1e1+5O4EWTI6OTvBB4Ed4klkZJr++8pyY5Ly1nAluAP6mqv52vQHWiVNX0ubRoNZuxfIXWGlpH2g7986r6QEferwC/WlV/NbQAJXU11bULPEFrPfhnA98EbgV+s6q+OuQwJXWYrt1N8iPAe4EX0JqK+Naq+ujQA5V0nD6u3VOBr9IaxfWJeQhRUhd9XLtX09pD6QeBQ8C7quoPhx6opBNM8533L2kN6Dib1rId7wP+r2ateI0IO6UlSZIkSZIkSUPj8h2SJEmSJEmSpKGxU1qSJEmSJEmSNDR2SkuSJEmSJEmShsZOaUmSJEmSJEnS0CyZ7wBm4lnPelad9f+3d/9BdpX3neffXzeWqaQ8KxsaR0HSSsYitZrEhXGDmNpldwgwkVQJWkicAZJgC3sV2SghU8MQPCoTV1yuImCXy4wpNMKjOOyayLBjZzqxUtiTKseu2hVIxoARP+K2TKBB4Mb2EqdUNsj67h/3tLi0bt97uu+vc/q+X1W31Pec57n3+7T6c2/3c885z5o1wy5DqpxvfvObL2Xm+LDrmI/ZlVozu1J9VTm/ZleaX5WzC+ZXms982Y2IjcCngTHgs5l5y5z9UezfDBwF3peZD3XqGxF/AOwAjgFfzswb29VndqXW2r3v1mpSes2aNRw8eHDYZUiVExH/OOwa2jG7UmtmV6qvKufX7Erzq3J2wfxK82mV3YgYA+4ALgWmgQMRMZmZjzc12wSsK24bgDuBDe36RsRFwBbgnZn504g4o1N9Zldqrd37rpfvkCRJkiRJUt2cD0xl5uHMfAXYS2MyudkW4O5s2A8sj4gVHfp+ELglM38KkJnfH8RgpFHjpLQkSZIkSZLq5kzg2ab708W2Mm3a9T0buDAiHoiIv4+I81o9eURsi4iDEXFwZmami2FIo8lJaUmSJEmSJNVNtNiWJdu063sK8BbgAuA/APcW16Z+fePM3Zk5kZkT4+OVvVS9VFm1uqZ0K6+++irT09P85Cc/GXYpQ3PqqaeycuVK3vjGNw67FKk0s9tgfiVVla/TDb5Oq27MboPZVd2Y3YYFZncaWNV0fyXwfMk2y9r0nQa+mJkJPBgRx4HTAQ+H1knMbsNi3ndLTUr3YzXTiPgC8EvFQywH/r/MPKd05YXp6Wne/OY3s2bNGlp8cLXkZSY/+MEPmJ6eZu3atcMuRxVjdqvN/EqqMl+nfZ1WPZlds6t6MruLyu4BYF1ErAWeA64Erp7TZhLYERF7aSx0+HJmHomImTZ9/wr4VeBrEXE2jQnsl7ocnpYos7v4992Ol+9oWpF0E7AeuCoi1s9p1rya6TYaq5m27ZuZ/zYzzykms/4r8MXSVTf5yU9+wmmnnTay//ERwWmnnTbyn8joZGa3+syvpCrzddrXadWT2TW7qiezu/DsZuYxYAdwP/AEcG9mHoqI7RGxvWi2DzgMTAF3AR9q17foswd4e0Q8RmMBxPcWR01LJzG7i3/fLXOk9IkVSYsnml2R9PGmNidWMwX2R8TsaqZrOvUtjtT8bRqfQi3KKP/Hg+PXvMxuDfg9kFRlvkb5PVA9+XPr90D15M/twr8HmbmPxsRz87ZdTV8ncF3ZvsX2V4DfXVAhGmlmd3HfgzKT0q1WJN1Qos18q5nO7Xsh8GJmfqfVk0fENhpHcLJ69eoS5UoVcfDPe/M4E1sX23Oo2ZVqa/jZrZV7HnimJ49z9Qbf46VB6lV2wfxKg+Z7r1RTi/k7Y0T+ptBoKjMp3a/VTGddBfzlfE+embuB3QATExMdT5fo5S/YUI036p07d3L33Xfzox/9iH/+538edjmqj6Fmd6EfKJldqZ7Oeua+Rff97ur39LAS9Zuv01I9mV2pnsyuVE9mt7yO15Smu9VM2/aNiFOAK4AvlC959PzGb/wGDz744LDLUP0MNbuZuTszJzJzYnx8fFEDqDuzK0nV5uu0VE9mV6onsyvVU7+yW2ZS+sRqphGxjMaKpJNz2kwC10TDBRSrmZboewnwZGZOdz2SIfnIRz7Cpz/96RP3d+7cye23397T57jgggtYsWJFTx9TI8HstmF2JanafJ2W6snsSvVkdqV6qnN2O16+IzOPRcTsiqRjwJ7Z1UyL/btoXBh+M43VTI8CW9v1bXr4K2lz+n8dvP/97+eKK67g+uuv5/jx4+zdu7flpwcXXnghP/7xj0/a/olPfIJLLrlkEKVqxJjd9syuJFWbr9NSPZldqZ7MrlRPdc5umWtK92U102Lf+8oWWlVr1qzhtNNO41vf+hYvvvgi73rXuzjttNNOaveNb3xjCNVp1Jnd+ZldSao2X6elejK7Uj2ZXame6pzdUpPSau8DH/gAn/vc53jhhRe49tprW7Yp+4nEz372M9797ncDcNlll/Gnf/qn/SlaktmVpIrzdVqqJ7Mr1ZPZleqprtl1UroHLr/8cm6++WZeffVV7rnnnpZtyn4iMTY2xsMPP9zL8iTNw+xKUrX5Oi3Vk9mV6snsSvVU1+wuuUnpqzesHvhzLlu2jIsuuojly5czNjbW88e/8cYbueeeezh69CgrV67kAx/4AB/96Ed7/jzSMJldSao2X6elejK7Uj2ZXamezG55S25SehiOHz/O/v37ue+++/ry+Lfeeiu33nprXx5bGmVmV5KqzddpqZ7MrlRPZleqp7pm9w09f8QR8/jjj/OOd7yDiy++mHXr1g27HEklmV1JqrY6vk5HxMaIeCoipiLiphb7IyJuL/Y/GhHnduobEV+IiIeL29MR4bnQqjSze2K72VWt1DG7kuqdXY+U7tL69es5fPjwsMuQtEBmV5KqrW6v0xExBtwBXApMAwciYjIzH29qtglYV9w2AHcCG9r1zcx/2/QcnwReHsiApEUyu2ZX9VS37EpqqHN2PVJakiRJ6t75wFRmHs7MV4C9wJY5bbYAd2fDfmB5RKwo0zciAvht4C/7PRBpxJhdSZKGwCOlJUmqmIjYCHwaGAM+m5m3zNkfxf7NwFHgfZn5UMm+NwC3AeOZ+VK/xyKNkDOBZ5vuT9M4orJTmzNL9r0QeDEzv9PqySNiG7ANYPXqwS+wI9XYULML5lcaFQ9874cL7vPdnz1z0rZhLKQn9YNHSkuSVCFNpwJvAtYDV0XE+jnNmk8j3kbjNOKOfSNiFY1TjE/+7VZSt6LFtizZpkzfq2hzpGVm7s7MicycGB8fb1uopNcZanbB/EqSRpOT0pIkVUs/TyP+FHAjJ//BLKl708CqpvsrgedLtmnbNyJOAa4AvtDDeiU1mF1JkoZg6V2+4+Cf9/bxJrb29vFK+PrXv84f/dEf8eijj7J3715+67d+a+A1SANndqVZfTmNOCIuA57LzEcaV/9ozVOINS9fpzs5AKyLiLXAc8CVwNVz2kwCOyJiL41svpyZRyJipkPfS4AnM3O6lwVrRJjdTsyuqsnsSvVkdkvzSOkKWr16NZ/73Oe4+uq5vwtJqjKzqx7p+WnEEfFzwE7g5k5P7inEWsr6+TqdmceAHcD9wBPAvZl5KCK2R8T2otk+4DAwBdwFfKhd36aHvxIXSdMIM7tSPfn3kVRPg8ru0jtSesA+8pGPcPrpp3P99dcDsHPnTt72trfxh3/4h4t+zDVr1gDwhjf4mYHUL2ZXFdbNacTL5tl+FrAWmD1KeiXwUEScn5kv9LR6qUfq+DqdmftoTF41b9vV9HUC15Xt27Tvfb2rUuovs/u6fe/rXZVSf9Uxu5LqnV0npbv0/ve/nyuuuILrr7+e48ePs3fvXh588MGT2l144YX8+Mc/Pmn7Jz7xCS655JJBlCqpidlVhfX8NOLiqK0zZjtHxNPARGa+1PfRSIvk67RUT2ZXqiezK9VTnbPrpHSX1qxZw2mnnca3vvUtXnzxRd71rndx2mmnndTuG9/4xhCqkzQfs6uqysxjETF7KvAYsGf2NOJi/y4aR2RtpnEa8VFga7u+QxiG1DVfp6V6MrtSPZldqZ7qnF0npXvgAx/4AJ/73Od44YUXuPbaa1u2qeInEtKoM7uqqn6dRtzUZk33VUr95+u0VE9mV6onsyvVU12zW2pSOiI2Ap+mcdTVZzPzljn7o9i/mcYRW+/LzIc69Y2IP6CxMMQx4MuZeWPXIxqCyy+/nJtvvplXX32Ve+65p2WbKn4iIY06sytJ1ebrtFRPZleqpzpmtx/zVRHxUeD/AGaKh/mPxYEfUiXVMbtQYlI6IsaAO4BLaSysdCAiJjPz8aZmm4B1xW0DcCewoV3fiLgI2AK8MzN/GhFn0AsTW3vyMAuxbNkyLrroIpYvX87Y2FjXj3fgwAEuv/xyfvSjH/HXf/3X/Mmf/AmHDnn2tRauVh8omV1JqjZfp6V6MrtSPZndjvo1X1X0+1RmfqJnxWp0mN3SyhwpfT4wlZmHAYpFlbYAzSHfAtxdnE68PyKWR8QKYE2bvh8EbsnMnwJk5vd7M6TBO378OPv37+e+++7ryeOdd955TE9P9+SxNLpq94HSEJhdSao2X6elejK7Uj3VMLv9mq+SaqWG2QXgDSXanAk823R/uthWpk27vmcDF0bEAxHx9xFx3kIKr4rHH3+cd7zjHVx88cWsW7du2OVIzU68QWfmK8Dsm2yzE2/QmbkfmH2Dbtd3SXygZHYlqdp8nZbqyexK9VTT7PZrvgpgR0Q8GhF7IuItrZ48IrZFxMGIODgzM9OqidR3Nc0uUO5I6WixLUu2adf3FOAtwAXAecC9EfH24tOr1x44YhuwDWD16tUlyh2s9evXc/jw4WGXIbXS6k12Q4k2871Bz/ad/UDp48BPgBsy88DcJze7kqRu+Dot1ZPZleqpptnt13zVncDHivsfAz4JnLR6XGbuBnYDTExMzH1eaSBqml2g3JHS08CqpvsrgedLtmnXdxr4YnGE5oPAceD0uU+embszcyIzJ8bHx1sWOGcee+SM+vg1r0F8oPQfaHygdFJ7s1uO3wNJVeZrlN8D1ZM/t34PVE/+3C74e9CX+arMfDEzf5aZx4G7aJxJLM3L7C7ue1BmUvoAsC4i1kbEMuBKYHJOm0ngmmi4AHg5M4906PtXwK8CRMTZwDLgpYUO4NRTT+UHP/jByP4AZCY/+MEPOPXUU4ddiqpnqB8odTLq2QXzK6nafJ32dVr1ZHbNrurJ7C4qu32ZryouaTnrcuCxxY9KS53ZXfz7bsfLd2TmsYjYAdwPjAF7MvNQRGwv9u8C9gGbgSngKLC1Xd/iofcAeyLiMeAV4L1zL91RxsqVK5menmaUr99z6qmnsnLlymGXoeo58SYLPEfjTfbqOW0maVwray+Ny3O8nJlHImKmTd/ZD5S+1s0HSma3wfxKqipfpxt8nVbdmN0Gs6u6MbsNC8luH+erbo2Ic2icLfw08Ps9Gp6WILPbsJj33TLXlCYz99EIcvO2XU1fJ3Bd2b7F9leA311Isa288Y1vZO3atd0+jLTkVP0DJbMrSdXm67RUT2ZXqiezuzh9mq/6vR6XqSXM7C5eqUlpSfVU5Q+UJEmSJEmSNJqclJYkSZJGzFnP3Ne7B9vw73v3WJI66ll+za4kaYjKLHQoSZIkqYOI2BgRT0XEVETc1GJ/RMTtxf5HI+LcMn0j4g+KfYci4tZBjEUaJWZXkqTB80hpSZIkqUsRMQbcAVwKTAMHImIyMx9varYJWFfcNgB3Ahva9Y2Ii4AtwDsz86cRccbgRiUtfWZXkqTh8EhpSZIkqXvnA1OZebhYf2EvjQmpZluAu7NhP7A8IlZ06PtB4JbM/ClAZn5/EIORRojZlSRpCJyUliRJkrp3JvBs0/3pYluZNu36ng1cGBEPRMTfR8R5Pa1aktmVJGkIvHyHJEmS1L1osS1LtmnX9xTgLcAFwHnAvRHx9sx83WNHxDZgG8Dq1asXULY08oaaXTC/kqTR5JHSkiRJUvemgVVN91cCz5ds067vNPDF4rIBDwLHgdPnPnlm7s7MicycGB8f72og0ogZanbB/EqSRpOT0pIkSVL3DgDrImJtRCwDrgQm57SZBK6JhguAlzPzSIe+fwX8KkBEnA0sA17q/3CkkWF2JUkaAi/fIfXJA9/7YU8eZ8NETx5GUklmV9JiZOaxiNgB3A+MAXsy81BEbC/27wL2AZuBKeAosLVd3+Kh9wB7IuIx4BXgva1O/5e0OGZXkqThcFJakiRJ6oHM3Edj8qp5266mrxO4rmzfYvsrwO/2tlJJzcyuJEmD5+U7JEmSJEmSJEkD46S0JEmSJEmSJGlgnJSWJEmSJEmSJA2Mk9KSJEmSJEmSpIFxUlqSJEmSJEmSNDBOSkuSJEmSJEmSBqbUpHREbIyIpyJiKiJuarE/IuL2Yv+jEXFup74R8dGIeC4iHi5um3szJEmzzK4kSZIkSZKqpuOkdESMAXcAm4D1wFURsX5Os03AuuK2DbizZN9PZeY5xW1ft4OR9BqzK0mSJEmSpCoqc6T0+cBUZh7OzFeAvcCWOW22AHdnw35geUSsKNlXUn+YXUmSJEmSJFVOmUnpM4Fnm+5PF9vKtOnUd0dxyYA9EfGWVk8eEdsi4mBEHJyZmSlRrqSC2ZUkSZIkSVLllJmUjhbbsmSbdn3vBM4CzgGOAJ9s9eSZuTszJzJzYnx8vES5kgpmV5IkSZK0ZPVjHaWm/TdEREbE6f0ehzSKykxKT0owMTkAACAASURBVAOrmu6vBJ4v2Wbevpn5Ymb+LDOPA3fRuFyApN4xu5IkSZKkJamf6yhFxCrgUuCZPg9DGlllJqUPAOsiYm1ELAOuBCbntJkErik+gboAeDkzj7TrW1y3dtblwGNdjkXS65ldqab6ccRHRHysaPtwRHwlIn5xUOORJEmS+qCf6yh9CriRk882ltQjp3RqkJnHImIHcD8wBuzJzEMRsb3YvwvYB2wGpoCjwNZ2fYuHvjUizqER8KeB3+/lwKRRZ3alemo6auNSGmctHIiIycx8vKlZ8xEfG2gc8bGhQ9/bMvMjxXP8IXAzsH1Aw5IkSZJ6rdVaSBtKtJlvHaUNABFxGfBcZj4S0erKlpJ6oeOkNEBm7qMxedW8bVfT1wlcV7Zvsf33FlSppAUzu1ItnThqAyAiZo/aaJ6UPnHEB7A/ImaP+FgzX9/M/Kem/j+PR31IkiSp3nq+jlJE/BywE/g3HZ88YhuNS4KwevXqTs0lzVHm8h2SJGlw5juao0ybtn0j4uMR8SzwOzSOlD5JRGyLiIMRcXBmZmbRg5BGUZ8uvfPRiHiuuPTOwxGxeVDjkUaF2ZVqqx/rKJ0FrAUeiYini+0PRcQvzH3yzNydmROZOTE+Pt7lUKTR46S0JEnV0vMjPk58kbkzM1cBnwd2tHpyf7mWFqefiy0Bn8rMc4rbSWcxSVo8syvVWs/XUcrMb2fmGZm5JjPX0Ji8PjczXxjYqKQR4aS0JEnV0o8jPua6B/jNriuV1Kyfiy1J6h+zK9VUZh6jcaDF/cATwL2z6yjNrqVE45KUh2mso3QX8KF2fQc8BGmkOSktSVK19PyID4CIWNfU/zLgyX4PRBoxfbv0DrCjuGTAnoh4S6sn99I70qINNbtgfqVuZOa+zDw7M8/KzI8X23bNrqVUfJh0XbH/VzLzYLu+LR5/TWa+NJjRSKPFSWlJkiqkj0d83BIRj0XEozQWbrl+UGOSRkS/Lr1zJ43rW54DHAE+2erJvfSOtGhDzS6YX0nSaDpl2AVIkqTXK647uW/Otl1NXydwXdm+xXYv1yH1VzeX3lk2X9/MfHF2Y0TcBfxN70qWhNmVJGkoPFJakiRJ6l6/Lr2zoqn/5cBj/R6INGLMriRJQ7DkjpS+54FnevI4V29Y3ZPHkVSO2ZUk1VlmHouI2cvnjAF7Zi+9U+zfReMshs00Lr1zFNjarm/x0LdGxDk0LgnwNPD7gxuVtPSZXUmShmPJTUpLkiRJw9CnS+/8Xo/LlDSH2ZUkafC8fIckSZIkSZIkaWCclJYkSZIkSZIkDYyT0pIkSZIkSZKkgXFSWpIkSZIkSZI0ME5KS5IkSZIkSZIGxklpSZIkSZIkSdLAOCktSZIkSZIkSRqYU8o0ioiNwKeBMeCzmXnLnP1R7N8MHAXel5kPlex7A3AbMJ6ZL3U3HEmSJL3OwT9ffN+Jrb2rQ5IkSZIKHY+Ujogx4A5gE7AeuCoi1s9ptglYV9y2AXeW6RsRq4BLgWe6Homkk0TExoh4KiKmIuKmFvsjIm4v9j8aEecuoO8NEZERcXq/xyFJkiRJkqSlo8zlO84HpjLzcGa+AuwFtsxpswW4Oxv2A8sjYkWJvp8CbgSy24FIej0/UJIkSZIkSVIVlZmUPhN4tun+dLGtTJt5+0bEZcBzmfnIAmuWVI4fKEmSJEmSJKlyykxKR4ttcyei5mvTcntE/BywE7i545NHbIuIgxFxcGZmpmOxkk4Y6gdKZleSJEmSJEmtlJmUngZWNd1fCTxfss18288C1gKPRMTTxfaHIuIX5j55Zu7OzInMnBgfHy9RrqTCUD9QMruSJEmSJElqpcyk9AFgXUSsjYhlwJXA5Jw2k8A1xaJpFwAvZ+aR+fpm5rcz84zMXJOZa2hMXp+bmS/0amCShvuBkiRJkiRJktTKKZ0aZOaxiNgB3A+MAXsy81BEbC/27wL2AZuBKeAosLVd376MRNJcJz4UAp6j8aHQ1XPaTAI7ImIvsIHiA6WImGnVt8jvGbOdi4npicx8qe+jkSRJkiRJ0pJQ5khpMnNfZp6dmWdl5seLbbuKCWmKRdKuK/b/SmYebNe3xeOvcVJL6q3MPAbMfij0BHDv7AdKsx8q0fhA6TCND5TuAj7Uru+AhyBJUq1ExMaIeCoipiLiphb7IyJuL/Y/GhHnLqDvDRGREXF6v8chjRqzK0nS4HU8UlpSfWXmPhoTz83bdjV9ncB1Zfu2aLOm+yolSaq/iBgD7gAupXEZrAMRMZmZjzc12wSsK24bgDuBDZ36RsSqYt8zgxqPNCrMriRJw1HqSGlJkiRJbZ0PTGXm4cx8BdgLbJnTZgtwd3GW4X5geUSsKNH3U8CNnLxgsaTumV2pxvpxpkNEfKxo+3BEfCUifnFQ45FGiZPSkiRJUvfOBJ5tuj9dbCvTZt6+EXEZ8FxmPtLuySNiW0QcjIiDMzMzixuBNJqGmt2irfmVFqHpbIVNwHrgqohYP6dZ85kO22ic6dCp722Z+c7MPAf4G+Dmfo9FGkVOSkuSJEndixbb5h4dOV+bltsj4ueAnZT4Yzgzd2fmRGZOjI+PdyxW0glDzS6YX6kLfTnTITP/qan/z+PZDlJfeE1pSZIkqXvTwKqm+yuB50u2WTbP9rOAtcAjETG7/aGIOD8zX+hp9dLoMrtSfbU6W2FDiTbznelwom9EfBy4BngZuKh3JUua5ZHSkiRJUvcOAOsiYm1ELAOuBCbntJkErimub3kB8HJmHpmvb2Z+OzPPyMw1xeLC08C5TmpJPWV2pfrq+ZkOJ77I3JmZq4DPAztaPrmX3pG64pHSkiRJUpcy81hE7ADuB8aAPZl5KCK2F/t3AfuAzcAUcBTY2q7vEIYhjRyzK9VaP850mOse4MvAn8zdkZm7gd0AExMTXuJDWiAnpSVJkqQeyMx9NCavmrftavo6gevK9m3RZk33VUoVc/DPe/dYE1sX1c3sSovUq/wuMrs0na0APEfjbIWr57SZBHZExF4al+d4OTOPRMTMfH0jYl1mfqfofxnw5GILlDQ/J6UlSZIkSZJUK3080+GWiPgl4Djwj8D2AQ5LGhlOSkuSJEmSJKl2+nGmQ2b+Zo/LlNSCCx1KkiRJkiRJkgbGSWlJkiRJkiRJ0sA4KS1JUsVExMaIeCoipiLiphb7IyJuL/Y/GhHnduobEbdFxJNF+y9FxPJBjUeSJEmSpGZOSkuSVCERMQbcAWwC1gNXRcT6Oc02AeuK2zbgzhJ9vwr8cma+E/gH4MN9HookSZIkSS05KS1JUrWcD0xl5uHMfAXYC2yZ02YLcHc27AeWR8SKdn0z8yuZeazovx9YOYjBSJIkSZI0l5PSkiRVy5nAs033p4ttZdqU6QtwLfC3XVcqSZIkSdIiOCktSVK1RIttWbJNx74RsRM4Bny+5ZNHbIuIgxFxcGZmpkS5kiRJkiQtTKlJ6T4tuPSxou3DEfGViPjF3gxJ0iyzK9XSNLCq6f5K4PmSbdr2jYj3Ar8O/E5mzp3oBiAzd2fmRGZOjI+PL3oQkiRJkiTNp+OkdB8XXLotM9+ZmecAfwPc3P1wJM0yu1JtHQDWRcTaiFgGXAlMzmkzCVxTfLB0AfByZh5p1zciNgJ/DFyWmUcHNRhJkiRJkuY6pUSbE4smAUTE7KJJjze1ObHgErA/ImYXXFozX9/M/Kem/j/PyacmS+qO2ZVqKDOPRcQO4H5gDNiTmYciYnuxfxewD9gMTAFHga3t+hYP/RngTcBXIwJgf2ZuH9zIJEmSJElqKDMp3WrRpA0l2sy34NKJvhHxceAa4GXgolZPHhHbaBzByerVq0uUK6lgdqWaysx9NCaem7ftavo6gevK9i22v6PHZUqSJEmStChlJqX7tuBSZu4EdkbEh4EdwJ+c1DhzN7AbYGJiouMRmWc9c1+nJuVs+Pe9eRxpeMyuJEmSJEmSKqfMQod9W3CpyT3Ab5aoRVJ5ZleSpAFygWGpnsyuJEmDV2ZSul8LLq1r6n8Z8GSXY5H0emZXkqQBcYFhqZ7MriRJw9Hx8h19XHDploj4JeA48I+Aiy1JPWR2JUkaKBcYlurJ7EqSNARlrindrwWXPOVf6jOzK0nSwAx1gWFJi2Z2JUkagjKX75AkSZLUXl8XGM7MVcDnaSwwfPKTR2yLiIMRcXBmZqZkyZIYcnbB/EqSRpOT0pIkSVL3hrrAcGbuzsyJzJwYHx9fYOnSSBv64uDmV5I0ipyUliRJkrrnAsNSPZldSZKGoNQ1pSVJkiTNzwWGpXoyu5IkDYeT0pIkSVIPuMCwVE9mV5KkwfPyHZIkSZIkSZKkgXFSWpIkSZIkSbUTERsj4qmImIqIm1rsj4i4vdj/aESc26lvRNwWEU8W7b8UEcsHNR5plDgpLUmSJEmSpFqJiDHgDmATsB64KiLWz2m2CVhX3LYBd5bo+1XglzPzncA/AB/u81CkkeSktCRJkiRJkurmfGAqMw9n5ivAXmDLnDZbgLuzYT+wPCJWtOubmV/JzGNF//3AykEMRho1TkpLkiRJkiSpbs4Enm26P11sK9OmTF+Aa4G/bfXkEbEtIg5GxMGZmZkFli7JSWlJkiRJkiTVTbTYliXbdOwbETuBY8DnWz15Zu7OzInMnBgfHy9RrqRmpwy7AEmSJEmSJGmBpoFVTfdXAs+XbLOsXd+IeC/w68DFmTl3oltSDzgpLUmSJEmSpLo5AKyLiLXAc8CVwNVz2kwCOyJiL7ABeDkzj0TEzHx9I2Ij8MfA/5aZRwczlNbOeua+kzeOvbV9p4mt/SlG6jEnpSVJkiRJQ/HA937Ys8faMNGzh5JUQq/yu9jsZuaxiNgB3A+MAXsy81BEbC/27wL2AZuBKeAosLVd3+KhPwO8CfhqRADsz8zti6tS0nyclJYkSZIkSVLtZOY+GhPPzdt2NX2dwHVl+xbb39HjMiW14EKHkiRJkiRJkqSBKTUpHREbI+KpiJiKiJta7I+IuL3Y/2hEnNupb0TcFhFPFu2/FBHLezMkSbPMriRJkiRJkqqm46R0RIwBdwCbgPXAVRGxfk6zTcC64rYNuLNE368Cv5yZ7wT+Afhw16ORdILZlSRJkiRJUhWVOVL6fGAqMw9n5ivAXmDLnDZbgLuzYT+wPCJWtOubmV/JzGNF//3Ayh6MR9JrzK4kSZIkSZIqp8yk9JnAs033p4ttZdqU6QtwLfC3JWqRVJ7ZlSRJkiRJUuWUmZSOFtuyZJuOfSNiJ3AM+HzLJ4/YFhEHI+LgzMxMiXIlFcyuJEkD5FoOUj2ZXUmSBu+UEm2mgVVN91cCz5dss6xd34h4L/DrwMWZOXeyDIDM3A3sBpiYmGjZRlJLZleSdJIHvvfD0m2/+7Nn5t139YbVvShnyWhaj+FSGu+vByJiMjMfb2rWvJbDBhprOWzo0PerwIcz81hE/BmNtRz+eFDjkpY6sytJ0nCUOVL6ALAuItZGxDLgSmByTptJ4JriE+QLgJcz80i7vhGxkcab8mWZebRH45H0GrMrSdLguJaDVE9mV5KkIeh4pHTxye4O4H5gDNiTmYciYnuxfxewD9gMTAFHga3t+hYP/RngTcBXIwJgf2Zu7+XgpFFmdiVJGqhW6zFsKNFmvrUc5vaFxloOX2j15BGxDdgGsHq1R7FLCzDU7IL5lSSNpjKX7yAz99GYvGretqvp6wSuK9u32P6OBVUqacHMriRJAzPUtRy8bJa0aEPNLphfSdJoKjUpLUmSJKmtoa7lIGnRzK4kSUNQ5prSkiRJktpzLQepnsyuJElD4KS0JEkVExEbI+KpiJiKiJta7I+IuL3Y/2hEnNupb0S8JyIORcTxiJgY1FikUVEsaDa7HsMTwL2zaznMrudA47JYh2ms5XAX8KF2fYs+nwHeTGMth4cj4sRluCR1z+xKkjQcXr5DkqQKiYgx4A7gUhqnCx+IiMnMfLyp2SZgXXHbANwJbOjQ9zHgCuA/D2ww0ohxLQepnsyuJEmD55HSkiRVy/nAVGYezsxXgL3AljlttgB3Z8N+YHlErGjXNzOfyMynBjcMSZIkSZJac1JakqRqORN4tun+dLGtTJsyfduKiG0RcTAiDs7MzCykqyRJkiRJpTgpLUlStUSLbVmyTZm+bWXm7sycyMyJ8fHxhXSVJEmSJKkUryktSVK1TAOrmu6vBJ4v2WZZib6SJEmSJA2VR0pLklQtB4B1EbE2IpYBVwKTc9pMAtdEwwXAy5l5pGRfSZIkSZKGyiOlJUmqkMw8FhE7gPuBMWBPZh6KiO3F/l3APmAzMAUcBba26wsQEZcD/wkYB74cEQ9n5q8NdnSSJEmSJDkpLUlS5WTmPhoTz83bdjV9ncB1ZfsW278EfKm3lUqSJEmStHBevkOSJEmSJEmSNDBOSkuSJEmSJKl2ImJjRDwVEVMRcVOL/RERtxf7H42Iczv1jYj3RMShiDgeERODGos0apyUliRJkiRJUq1ExBhwB7AJWA9cFRHr5zTbBKwrbtuAO0v0fQy4Avh6v8cgjTInpSVJkiRJklQ35wNTmXk4M18B9gJb5rTZAtydDfuB5RGxol3fzHwiM58a3DCk0eSktCRJkiRJkurmTODZpvvTxbYybcr0bSsitkXEwYg4ODMzs5CuknBSWpIkSZIkSfUTLbZlyTZl+raVmbszcyIzJ8bHxxfSVRIlJ6W9cLxUT2ZXkiRJkrRETQOrmu6vBJ4v2aZMX0l91HFS2gvHS/VkdiVJkiRJS9gBYF1ErI2IZcCVwOScNpPANcUBWRcAL2fmkZJ9JfVRmSOlvXC8VE9mV5KkAfIMJamezK5UT5l5DNgB3A88AdybmYciYntEbC+a7QMOA1PAXcCH2vUFiIjLI2Ia+FfAlyPi/gEOSxoZp5Ro0+ri7xtKtJnvwvFz+7YVEdtoHMHJ6tWrF9JVGnVmV5KkAWk6y+hSGu+bByJiMjMfb2rWfIbSBhpnKG3o0Hf2DKX/PLDBSCPE7Er1lpn7aEw8N2/b1fR1AteV7Vts/xLwpd5WKmmuMkdKe+F4qZ7MriRJg+MZSlI9mV1JkoagzKS0F46X6snsSpI0OPOdfVSmTZm+bUXEtog4GBEHZ2ZmFtJVGnVDzS6YX0nSaCozKe2F46V6MruSJA2OZyhJ9TTU7IL5lSSNpo7XlM7MYxExe/H3MWDP7IXji/27aFyDZzONC8cfBba26wuNC8cD/wkYp3Hh+Icz89d6PUBpVJldSZIGqpszlJaV6CupP8yuJElDUGahQy8cL9WU2ZUkaWBOnGUEPEfjLKOr57SZBHZExF4ai6W9nJlHImKmRF9J/WF2JUkaglKT0pIkSZLm5xlKUj2ZXUmShsNJaUmSJKkHPENJqiezK0nS4DkpLUmSJEmSJNXAA9/7Ydv93/3ZM6Ue5+oNq3tRjrRobxh2AZIkSZIkSZKk0eGktCRJkiRJkiRpYLx8hyRJkiRJkrQEnPXMfeUajr31ta8ntvanGKkNj5SWJEmSJEmSJA2Mk9KSJEmSJEmSpIFxUlqSJEmSJEmSNDBOSkuSJEmSJEmSBsZJaUmSJEmSJEnSwDgpLUmSJEmSJEkamFOGXYAkSZI6u+eBZxbV76xnftjjSiRJkiSpOx4pLUmSJEmSJEkaGCelJUmSJEmSJEkD46S0JEmSJEmSJGlgSl1TOiI2Ap8GxoDPZuYtc/ZHsX8zcBR4X2Y+1K5vRLwV+AKwBnga+O3M/FH3Q5I0y+xK9WR2pXoyu1I9mV2pvszv4j3wvaZ1R773yUU9xndXv4erN6zuUUUaNR2PlI6IMeAOYBOwHrgqItbPabYJWFfctgF3luh7E/B3mbkO+LvivqQeMbtSPZldqZ7MrlRPZleqL/Mr1VuZI6XPB6Yy8zBAROwFtgCPN7XZAtydmQnsj4jlEbGCxqdK8/XdAvzrov9fAF8D/rjL8Uh6jdmV6snsqjLOeua++XeOvbV954mtvS2m+syuVE9mV6ov8yvVWJlJ6TOBZ5vuTwMbSrQ5s0Pft2XmEYDMPBIRZyygbkmdmV2pnsyuauF1p3y28N2fPVPqcZbQKZ9mV6onsyvVl/kdsrOeuY8Hyv3K19GGtW0OeBi9gx1GQplJ6WixLUu2KdO3/ZNHbKNxigXAP0fEUx26nA68tJDnaO2G7h+ioUf19Iz1tFfBem4oU8//2GKb2e1OBX8WrKeDitV0Q5l6Rjy7r+X1dxb3AP1SsZ+lrg1pPOVejxf5f1+F/6O5+R2h7M7Vk/feKvyfNqtaPVC9mipYz6J+bx5qdmHB+a1adqGSPwvW00YF6/Fv3iWgz2O7tn8PXY7/d4vXKrtAuUnpaWBV0/2VwPMl2yxr0/fFiFhRfOq0Avh+qyfPzN3A7hJ1AhARBzNzomz7frOe9qynvS7rMbtdsJ72qlYPVK+mLuoxu0O21Ma01MYDlR2T2e2C9XRWtZqWUD1DzS4sLL9V+75D9WqynvaWWD2+91bEUh4bLO3xDXNsHRc6BA4A6yJibUQsA64EJue0mQSuiYYLgJeLUx3a9Z0E3lt8/V7gv3U5FkmvZ3alejK7Uj2ZXamezK5UX+ZXqrGOR0pn5rGI2AHcD4wBezLzUERsL/bvAvYBm4Ep4CiwtV3f4qFvAe6NiPcDzwDv6enIpBFndqV6MrtSPZldqZ7MrlRf5leqt2gsQLp0RMS24hSKSrCe9qynvarV009VG6v1tFe1eqB6NVWtnn5ZiuNcamNaauOBpTmmQava99B6OqtaTdYzHFUcZ9Vqsp72rGd4lvJYl/LYYGmPb5hjW3KT0pIkSZIkSZKk6ipzTWlJkiRJkiRJknqiVpPSEfGeiDgUEccjYmLOvg9HxFREPBURv9a0/d0R8e1i3+0REcX2N0XEF4rtD0TEmi5rOyci9kfEwxFxMCLOX2xtvRIRf1A856GIuHXY9TQ9zw0RkRFx+jBriojbIuLJiHg0Ir4UEcuHWU+L+jYWzz8VETf163kGwewuqq7K5dfslq7P7A4gu70QER+NiOeK/D8cEZub9g39Z6kX6vrzGBFPF9/nhyPiYLHtrRHx1Yj4TvHvW5rat/z/GlVVz25U8L03fN9tV4fvuwNU5fya3dI1md3yNS6p/M5nKYwzltjvZhGxJyK+HxGPNW1b8HgGnZky5hlbNf/uycza3ID/Cfgl4GvARNP29cAjwJuAtcB3gbFi34PAvwIC+FtgU7H9Q8Cu4usrgS90WdtXmh57M/C1xdbWo+/VRcB/B95U3D9jmPU01bWKxkIC/wicPuTv0b8BTim+/jPgz6rwPSqeZ6x43rcDy4p61vfjuQZxM7sLrqly+TW7pWszuwPKbo/G91Hghhbbh/6zNOo/j8DTs681TdtuBW4qvr6pTPZH9Vb17FKx91583+1Ui++7A7xVOb9mt1RNZrd8fUsuv0t5nCyx382A/xU4F3ism/EMMjNdju2jVPDvnlodKZ2ZT2TmUy12bQH2ZuZPM/N7NFZVPT8iVgD/IjP/32x8R+8G/vemPn9RfP1/Axd3OeufwL8ovv4fgOe7qK0XPgjckpk/BcjM7w+5nlmfAm6k8f2aNZSaMvMrmXmsuLsfWDnMeuY4H5jKzMOZ+Qqwt6irlszuglUxv2a3HLM7uOz2UxV+lnphSf088vqfob/g9T9bJ/1/DaG+yqhBdqv23uv7bhu+7w5WxfNrdjszu+UtufzOYymPs7a/m2Xm14Efztm8oPFU9W+DecY2n6GOrVaT0m2cCTzbdH+62HZm8fXc7a/rU7xQvwyc1kUNfwTcFhHPAp8APtxFbb1wNnBhNE7T+vuIOG/I9RARlwHPZeYjc3YNraYm19L45Kcq9cxXw1JjdlurVH7N7oKY3cFlt1d2FKe17mk6Ra8KP0u9UOefxwS+EhHfjIhtxba3ZeYRgOLfM4rtdR7noFUlu1V77/V9tzzfd4enCvk1u22Y3QUblfwulXGOwu9mCx1P3f42qNzfPaf0+gG7FRH/HfiFFrt2ZuZ/m69bi23ZZnu7PouqDbgY+HeZ+V8j4reB/wJcssjaSulQzynAW4ALgPOAeyPi7f2sp0RN/5HGKUQndetXTWV+niJiJ3AM+Hy/61mAQT5XT5jdhalafs1uz5jd1tvb9emrDj/bdwIfK+r4GPBJGn+0VeFnqRfqVm+z/zkzn4+IM4CvRsSTbdrWeZyLVvXsVu291/fdxdfj+27vVTm/ZrereszuwtQyv4uwVMY5yr+bVSUz3ajk3z2Vm5TOzEsW0W2axrWbZq2kcSrRNK+dotK8vbnPdEScQuP0o7aHt7erLSLuBq4v7t4HfLaL2krpUM8HgS8Wh9k/GBHHgdP7WU+7miLiV2hcn+aR4oyxlcBD0VgcYyjfo6Ku9wK/DlxcfK/oZz0LMF8NlWV2F6Zq+TW7PWN2e5jdXig7voi4C/ib4m4VfpZ6oXY/j7My8/ni3+9HxJdonPL5YkSsyMwjxSmFs6dp13ac3ah6dqv23uv77uLqaarL990eqnJ+ze7i6jG7i1LL/C7CkhjniPxuttDx1OZvg8x8cfbrSv3dk0O+APdibpy86MO/5PUX5j7MaxfmPkDjk9PZC3NvLrZfx+sXfbi3y5qeAP518fXFwDcXW1uPvkfbgT8tvj6bxuH4Max6WtT3NK8t/DCs79FG4HFgfM72oX+PaHxgdLh4/tnFEP5lv/4/BnUzu6Vrqmx+zW7H2szugLLbo3GtaPr639G4nlolfpZG+ecR+HngzU1f/z9F7m/j9YvP3Nrp/2vUb1XNLhV778X33U41+L47hFsV82t2F1Sb2e1c35LN71IbJ0v0dzNgDa9fDHDB4xlkZrocWyX/7hn6N2qB39TLaczW/xR4Ebi/ad9OGqtEPkXTipDABPBYse8zQBTbT6Xx6e4UjRUl395lbf8L8M3iP/MB4N2Lra1H36tlwP9VPP5DwK8Os54W9T1N08qtmMHS9AAAAQZJREFUQ/oeTdH4xeXh4rarYt+jzcA/FM+1s1/PM4ib2V1wTZXNr9ktVZ/ZHUB2ezS+/xP4NvAoMMnrf1kb+s/SqP480lid/pHidmi2bhrXQf074DvFv2/t9P81qreqZ5eKvffi+26nGnzfHeCtyvk1uwuqzeyWq3FJ5XepjpMl+LsZ8JfAEeDV4jX3/YsZz6Az08XYKvl3z+yblSRJkiRJkiRJffeGYRcgSZIkSZIkSRodTkpLkiRJkiRJkgbGSWlJkiRJkiRJ0sA4KS1JkiRJkiRJGhgnpSVJkiRJkiRJA+OktCRJkiRJkiRpYJyUliRJkiRJkiQNjJPSkiRJkiRJkqSB+f8Bt49Y2oHBF+kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "fig, axs = plt.subplots(5, 6, figsize=(25,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].hist(tX_2[:,n], alpha=0.4, density=True, label=['y = -1'])\n",
    "        axs[i,j].hist(tX_1[:,n], alpha=0.4, density=True, label=['y = 1'])\n",
    "        axs[i,j].legend()\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the histograms of the features with a color for each y, we can see that there are useless features as they have almost the same distribution for y=1 than for y = -1. We can cut feature 15, 18, 20. \n",
    "\n",
    "\n",
    "There are also features that are very inequally distributed with value that are about -1000 and values around 0 ; it can be problematic for the prediction with such a large gap between values of a single distribution. Moreover, there is not a big difference in the distribution of y=1 and y=-1. Maybe it can be useful to put off these big negative values of these features. The features in question are : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20,20))\n",
    "\n",
    "n = 0\n",
    "for i in range(5) :\n",
    "    for j in range(6) :\n",
    "        axs[i,j].scatter(tX[:,n], y)\n",
    "        axs[i,j].set_title(n)\n",
    "        n = n + 1\n",
    "plt.show()\n",
    "\n",
    "#meme constat comment faire pour se debarrasser de ces valeurs ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots confirm the obsevations that we made in the previous plot ; no difference of the distribution of y for features 15, 18, 20 and very large gap in the distributions of features : 0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of -999 in the entire matrix :\n",
      "[(38114,), (0,), (0,), (0,), (177457,), (177457,), (177457,), (0,), (0,), (0,), (0,), (0,), (177457,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (99913,), (99913,), (99913,), (177457,), (177457,), (177457,), (0,)]\n",
      "(1580052,)\n",
      "number of -999 in the rows where y = 1 :\n",
      "[(2835,), (0,), (0,), (0,), (53202,), (53202,), (53202,), (0,), (0,), (0,), (0,), (0,), (53202,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (25492,), (25492,), (25492,), (53202,), (53202,), (53202,), (0,)]\n",
      "(451725,)\n",
      "number of -999 in the rows where y = -1 :\n",
      "[(35279,), (0,), (0,), (0,), (124255,), (124255,), (124255,), (0,), (0,), (0,), (0,), (0,), (124255,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (74421,), (74421,), (74421,), (124255,), (124255,), (124255,), (0,)]\n",
      "(1128327,)\n"
     ]
    }
   ],
   "source": [
    "def nb_outliers(tX, outlier) : \n",
    "    sum = 0\n",
    "    nb_outliers = []\n",
    "    for col in range(tX.shape[1]) :\n",
    "        sum = np.where(tX[:,col] == outlier)[0].shape\n",
    "        nb_outliers.append(sum)   \n",
    "    print(nb_outliers)\n",
    "    print(np.where(tX==outlier)[0].shape)\n",
    "\n",
    "out = -999\n",
    "\n",
    "print('number of -999 in the entire matrix :')\n",
    "nb_outliers(tX, out)\n",
    "\n",
    "ind_1 = np.where(y == 1)\n",
    "ind_2 = np.where(y == -1)\n",
    "tX_1 = tX[ind_1[0],:]\n",
    "tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "print('number of -999 in the rows where y = 1 :')\n",
    "nb_outliers(tX_1, out)\n",
    "print('number of -999 in the rows where y = -1 :')\n",
    "nb_outliers(tX_2, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem with features  0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28. They are inequally distributed; they have a lot of -999 values and the rest is values around 0. So, here we can see how much of these -999 there are. We can see that the -999 appear only in the features that we identified with the histograms. It seems that there is a correlation between features as many features have the same number of -999. We can also see that there is more -999 in the obsevations where y=-1, so we have to take this into account when we filter the data. As there are many -999, we can't delete the rows where there is -999 because we will loose to much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms(y, tX):\n",
    "    ind_1 = np.where(y == 1)\n",
    "    ind_2 = np.where(y == -1)\n",
    "    tX_1 = tX[ind_1[0],:]\n",
    "    tX_2 = tX[ind_2[0],:]\n",
    "\n",
    "    fig, axs = plt.subplots(5, 6, figsize=(25,20))\n",
    "\n",
    "    n = 0\n",
    "    for i in range(5) :\n",
    "        for j in range(6) :\n",
    "            axs[i,j].hist(tX_2[:,n], alpha=0.4, density=True, label=['y = -1'])\n",
    "            axs[i,j].hist(tX_1[:,n], alpha=0.4, density=True, label=['y = 1'])\n",
    "            axs[i,j].legend()\n",
    "            axs[i,j].set_title(n)\n",
    "            n = n + 1\n",
    "            if n>=tX.shape[1]: break\n",
    "        if n>=tX.shape[1]: break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX, y, ids)\n",
    "\n",
    "print('Set 1')\n",
    "set1_x = outliers(set1_x, -999)\n",
    "_ = outliers(set1_x, -999)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x = outliers(set2_x, -999)\n",
    "_ = outliers(set2_x, -999)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x = outliers(set3_x, -999)\n",
    "_ = outliers(set3_x, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set 2\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set 3\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Set 1')\n",
    "print(np.all(set1_x==set1_x[0], axis = 0))\n",
    "histograms(set1_y, set1_x)\n",
    "\n",
    "print('\\nSet 2')\n",
    "print(np.all(set2_x==set2_x[0], axis = 0))\n",
    "histograms(set2_y, set2_x)\n",
    "\n",
    "print('\\nSet 3')\n",
    "print(np.all(set3_x==set3_x[0], axis = 0))\n",
    "histograms(set3_y, set3_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things to delete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST FOR y=[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to filter the data, we want to get rif of these -999, but we can't just delete the rows. So, we have the idea to replace the -999 by the mean of the rest of values of the feature. As there is a significant difference of amount of -999 in between y=1 and y=-1 in certain features, we calculate the mean for the rows where y = 1 and y = -1 separatly.\n",
    "\n",
    "Then, we can also standardize the data. It can be a good idea because the features are not all in the same range of values and it can create disproportionality between the importance of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above in the histograms, some features seem to be useless as they have a similar distribution between the y = 1 and y = -1. So, it is useful to have function that cut or keep some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log = [1, 2, 5, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "#to_log2 = [7, 8, 10, 11, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results for least squares regression is with the tX matrix which is standardize, filter with filtering_with_mean_bis function and whose features 15, 18, 20 are cut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8094823719674634, 0.8367332181680027, 3.915894970097548, 7.460633536153627, 99.75861666852452, 21782.808502372467, 640181.0545578048, 18065042.09027731, 734470157.2616756, 22052422020.83767]\n",
      "Cross validation finished: optimal degree 1\n",
      "Least square loss rmse 0.5723030100580345\n"
     ]
    }
   ],
   "source": [
    "tX_cut = cut(tX, [15,18,20])\n",
    "x_essai = std(filtering_with_mean_bis(tX_cut, y))\n",
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(y, x_essai, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model\n",
    "tX_poly = build_poly(x_essai, degree_opt)\n",
    "w_ls, loss_ls = least_squares(y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_ls = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares best model on the entire is degree 1 ; its accuracy is : 0.606. Let's try the same model but applied to the 3 different sets according to the PRI_jet_num parameter thanks to the separate_sets function. We sould normally improve our results : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tX_cut = cut(tX, [15,18,20])\n",
    "\n",
    "# Separation into set according to the PRI_jet_num parameter, as it is the 22th column of the entire matrix\n",
    "# if we cut 3 paramaters before the 22th, PRI_jet_num is the 22-3th now\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_cut, y, ids, 22-3)\n",
    "\n",
    "def best_filtering_ls(set_x, set_y) :\n",
    "        set_x = outliers(set_x, -999)\n",
    "        set_x = std(filtering_with_mean_bis(set_x, set_y))\n",
    "        return set_x\n",
    "\n",
    "print('Set 1')\n",
    "set1_x_ls = best_filtering_ls(set1_x, set1_y)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x_ls = best_filtering_ls(set2_x, set2_y)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x_ls = best_filtering_ls(set3_x, set3_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do least squares regression for each set : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.6992124921989132, 1.9845019446411356, 49.09508655110148, 680.395540586843, 303328.3207567297, 17473312.61822008, 1257541187.5109198, 149638769519.0206, 14766571347092.793, 968707857329139.4]\n",
      "Cross validation finished: optimal degree 1\n",
      "Least square loss rmse 0.49359178738802756\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(set1_y, set1_x_ls, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model \n",
    "tX_poly = build_poly(set1_x_ls, degree_opt)\n",
    "w_set1_ls, loss_ls = least_squares(set1_y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_set1_ls = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8627394947257983, 0.8194579205690277, 0.8118322002522186, 0.8032677941638553, 0.8300628265844276, 0.892754526525992, 1.9111011828648834, 3.795161870621623, 8.013056147923317, 6.684984206978702]\n",
      "Cross validation finished: optimal degree 4\n",
      "Least square loss rmse 0.563695013641898\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(set2_y, set2_x_ls, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model\n",
    "tX_poly = build_poly(set2_x_ls, degree_opt)\n",
    "w_set2_ls, loss_ls = least_squares(set2_y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_set2_ls = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8494265790401101, 0.8048402897267272, 0.7838667573027494, 0.7804477199035814, 0.7723920603771754, 1.1329656722635941, 1.1764287361183663, 3.804494848573087, 17.168085104218402, 52.06761713224065]\n",
      "Cross validation finished: optimal degree 5\n",
      "Least square loss rmse 0.5401761404390758\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,11)\n",
    "# Cross-validation on the degrees\n",
    "degree_opt, _ = best_degree_selection(set3_y, set3_x_ls, degrees, k_fold=10, lambdas=0, fonction=0)\n",
    "print(\"Cross validation finished: optimal degree {d}\".format(d=degree_opt))\n",
    "# Best degree model\n",
    "tX_poly = build_poly(set3_x_ls, degree_opt)\n",
    "w_set3_ls, loss_ls = least_squares(set3_y, tX_poly)\n",
    "print(\"Least square loss rmse {loss}\".format(loss=np.sqrt(loss_ls)))\n",
    "degree_set3_ls = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least square best model (standardize + filter_with_mean_bis + cut 15,18,20) with separate_sets is :\n",
    "* set 1 : degree 1\n",
    "* set 2 : degree 4\n",
    "* set 3 : degree 5\n",
    "\n",
    "We have a better accuracy : 0.683"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results for Ridge regression is with tX that is only filtered by filtering_with_mean_bis function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8095450909357379, 0.844544477269849, 2.2534289074606, 0.9046219752885352, 344.6792667724601, 25071.139924639538, 1438117.9697410704]\n",
      "Cross validation finished: optimal lambda 0.0001 and degree 1\n",
      "Ridge regression loss 0.3275363710713481\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = filtering_with_mean_bis(tX, y)\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "degree_rr = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look if we can improve the results with the separation of tX into sets according to the feature 22 (PRI_jet_num) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n",
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 2\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Set 3\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX, y, ids)\n",
    "\n",
    "def best_filtering_rr(set_x, set_y) :\n",
    "        set_x = outliers(set_x, -999)\n",
    "        set_x = filtering_with_mean_bis(set_x, set_y)\n",
    "        return set_x\n",
    "    \n",
    "\n",
    "print('Set 1')\n",
    "set1_x_rr = best_filtering_rr(set1_x, set1_y)\n",
    "\n",
    "print('\\nSet 2')\n",
    "set2_x_rr = best_filtering_rr(set2_x, set2_y)\n",
    "\n",
    "print('\\nSet 3')\n",
    "set3_x_rr = best_filtering_rr(set3_x, set3_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression according to the different sets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.6992441526016957, 2.6438327297625164, 55.80989893325343, 63.684666434180755, 419376.3821326866, 13368684.219238853, 1250756574.718208]\n",
      "Cross validation finished: optimal lambda 0.0001 and degree 1\n",
      "Ridge regression loss 0.24363595402634777\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = set1_x_rr\n",
    "y = set1_y\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr_set1, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "lambda_rr_set1 = lambda_opt\n",
    "degree_rr_set1 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.862899048310838, 0.8169222621606087, 0.8101900728020305, 0.8002962438481538, 0.8261304186737602, 1.2120544270432492, 1.7233468351025003]\n",
      "Cross validation finished: optimal lambda 0.0002592943797404667 and degree 4\n",
      "Ridge regression loss 0.31512983201516304\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = set2_x_rr\n",
    "y = set2_y\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr_set2, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "lambda_rr_set2 = lambda_opt\n",
    "degree_rr_set2 = degree_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rmses [0.8495901136277599, 0.8051128492381056, 0.7850629220809948, 0.7798421457960129, 0.8176384928243913, 0.9426713170684411, 2.8857314167790853]\n",
      "Cross validation finished: optimal lambda 0.0002592943797404667 and degree 4\n",
      "Ridge regression loss 0.29715852129995324\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,8)\n",
    "x_essai = set3_x_rr\n",
    "y = set3_y\n",
    "# Cross-validation on the degrees and the lambdas\n",
    "degree_opt, lambda_opt = best_degree_selection(y, x_essai, degrees, k_fold=4, lambdas=np.logspace(-4, 0, 30), fonction=1)\n",
    "print(\"Cross validation finished: optimal lambda {l} and degree {d}\".format(l=lambda_opt, d=degree_opt))\n",
    "# Best degree and lambda model\n",
    "x_essai = build_poly(x_essai, degree_opt)\n",
    "w_rr_set3, loss_rr = ridge_regression(y, x_essai, lambda_opt)\n",
    "print(\"Ridge regression loss {loss}\".format(loss=loss_rr))\n",
    "lambda_rr_set3 = lambda_opt\n",
    "degree_rr_set3 = degree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least square best model (filter_with_mean_bis) with separate_sets is :\n",
    "* set 1 : degree 1 and lambda = 0.0001\n",
    "* set 2 : degree 4 and lambda = 0.0002592943797404667\n",
    "* set 3 : degree 4 and lambda = 0.0002592943797404667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have better results with the matrix that is separated in sets, so let's do a submission with this method. We have 0.801 of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the data to get better results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Preprocessing for gradient descent done!\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "max_iters = 500\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "tX_log = log_distribution(tX, to_log)\n",
    "\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "print('')\n",
    "print(\"Preprocessing for gradient descent done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we perform a 4-fold cross validation for the gamma parameter for gradient descent method for each set. Then we we perform a stochastic gradient descent with the optimal gamma found. For the cross validation we use 50 iterations and for the final descent 500 iterations as we want a more precise final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.5\n",
      "Gradient Descent(2/49): loss=0.5\n",
      "Gradient Descent(3/49): loss=0.5\n",
      "Gradient Descent(4/49): loss=0.5\n",
      "Gradient Descent(5/49): loss=0.5\n",
      "Gradient Descent(6/49): loss=0.5\n",
      "Gradient Descent(7/49): loss=0.5\n",
      "Gradient Descent(8/49): loss=0.5\n",
      "Gradient Descent(9/49): loss=0.5\n",
      "Gradient Descent(10/49): loss=0.5\n",
      "Gradient Descent(11/49): loss=0.5\n",
      "Gradient Descent(12/49): loss=0.5\n",
      "Gradient Descent(13/49): loss=0.5\n",
      "Gradient Descent(14/49): loss=0.5\n",
      "Gradient Descent(15/49): loss=0.5\n",
      "Gradient Descent(16/49): loss=0.5\n",
      "Gradient Descent(17/49): loss=0.5\n",
      "Gradient Descent(18/49): loss=0.5\n",
      "Gradient Descent(19/49): loss=0.5\n",
      "Gradient Descent(20/49): loss=0.5\n",
      "Gradient Descent(21/49): loss=0.5\n",
      "Gradient Descent(22/49): loss=0.5\n",
      "Gradient Descent(23/49): loss=0.5\n",
      "Gradient Descent(24/49): loss=0.5\n",
      "Gradient Descent(25/49): loss=0.5\n",
      "Gradient Descent(26/49): loss=0.5\n",
      "Gradient Descent(27/49): loss=0.5\n",
      "Gradient Descent(28/49): loss=0.5\n",
      "Gradient Descent(29/49): loss=0.5\n",
      "Gradient Descent(30/49): loss=0.5\n",
      "Gradient Descent(31/49): loss=0.5\n",
      "Gradient Descent(32/49): loss=0.5\n",
      "Gradient Descent(33/49): loss=0.5\n",
      "Gradient Descent(34/49): loss=0.5\n",
      "Gradient Descent(35/49): loss=0.5\n",
      "Gradient Descent(36/49): loss=0.5\n",
      "Gradient Descent(37/49): loss=0.5\n",
      "Gradient Descent(38/49): loss=0.5\n",
      "Gradient Descent(39/49): loss=0.5\n",
      "Gradient Descent(40/49): loss=0.5\n",
      "Gradient Descent(41/49): loss=0.5\n",
      "Gradient Descent(42/49): loss=0.5\n",
      "Gradient Descent(43/49): loss=0.5\n",
      "Gradient Descent(44/49): loss=0.5\n",
      "Gradient Descent(45/49): loss=0.5\n",
      "Gradient Descent(46/49): loss=0.5\n",
      "Gradient Descent(47/49): loss=0.5\n",
      "Gradient Descent(48/49): loss=0.5\n",
      "Gradient Descent(49/49): loss=0.5\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4947022503378182\n",
      "Gradient Descent(2/49): loss=0.4896703267175971\n",
      "Gradient Descent(3/49): loss=0.4848904379636344\n",
      "Gradient Descent(4/49): loss=0.4803495167342438\n",
      "Gradient Descent(5/49): loss=0.4760351810856928\n",
      "Gradient Descent(6/49): loss=0.4719356981068582\n",
      "Gradient Descent(7/49): loss=0.4680399495109904\n",
      "Gradient Descent(8/49): loss=0.46433739907735627\n",
      "Gradient Descent(9/49): loss=0.46081806184153673\n",
      "Gradient Descent(10/49): loss=0.45747247493881665\n",
      "Gradient Descent(11/49): loss=0.4542916700104386\n",
      "Gradient Descent(12/49): loss=0.451267147087529\n",
      "Gradient Descent(13/49): loss=0.44839084987223826\n",
      "Gradient Descent(14/49): loss=0.4456551423401184\n",
      "Gradient Descent(15/49): loss=0.4430527865919723\n",
      "Gradient Descent(16/49): loss=0.4405769218873861\n",
      "Gradient Descent(17/49): loss=0.4382210447959111\n",
      "Gradient Descent(18/49): loss=0.43597899040539545\n",
      "Gradient Descent(19/49): loss=0.4338449145303076\n",
      "Gradient Descent(20/49): loss=0.43181327686604193\n",
      "Gradient Descent(21/49): loss=0.42987882503816754\n",
      "Gradient Descent(22/49): loss=0.42803657949838836\n",
      "Gradient Descent(23/49): loss=0.4262818192216274\n",
      "Gradient Descent(24/49): loss=0.42461006816114855\n",
      "Gradient Descent(25/49): loss=0.4230170824209845\n",
      "Gradient Descent(26/49): loss=0.421498838107172\n",
      "Gradient Descent(27/49): loss=0.42005151982138816\n",
      "Gradient Descent(28/49): loss=0.41867150976257744\n",
      "Gradient Descent(29/49): loss=0.4173553774040248\n",
      "Gradient Descent(30/49): loss=0.4160998697151048\n",
      "Gradient Descent(31/49): loss=0.4149019018986086\n",
      "Gradient Descent(32/49): loss=0.4137585486161255\n",
      "Gradient Descent(33/49): loss=0.4126670356754565\n",
      "Gradient Descent(34/49): loss=0.4116247321554359\n",
      "Gradient Descent(35/49): loss=0.4106291429448767\n",
      "Gradient Descent(36/49): loss=0.4096779016736105\n",
      "Gradient Descent(37/49): loss=0.40876876401478207\n",
      "Gradient Descent(38/49): loss=0.40789960133868125\n",
      "Gradient Descent(39/49): loss=0.40706839469945943\n",
      "Gradient Descent(40/49): loss=0.40627322913707953\n",
      "Gradient Descent(41/49): loss=0.4055122882777959\n",
      "Gradient Descent(42/49): loss=0.40478384921736205\n",
      "Gradient Descent(43/49): loss=0.4040862776720063\n",
      "Gradient Descent(44/49): loss=0.4034180233830215\n",
      "Gradient Descent(45/49): loss=0.4027776157615711\n",
      "Gradient Descent(46/49): loss=0.4021636597610297\n",
      "Gradient Descent(47/49): loss=0.40157483196485694\n",
      "Gradient Descent(48/49): loss=0.40100987687863976\n",
      "Gradient Descent(49/49): loss=0.40046760341555093\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4946970165493175\n",
      "Gradient Descent(2/49): loss=0.4896601295946725\n",
      "Gradient Descent(3/49): loss=0.484875532907455\n",
      "Gradient Descent(4/49): loss=0.48033014470565466\n",
      "Gradient Descent(5/49): loss=0.47601156921428056\n",
      "Gradient Descent(6/49): loss=0.47190806029352467\n",
      "Gradient Descent(7/49): loss=0.4680084870215269\n",
      "Gradient Descent(8/49): loss=0.4643023011249186\n",
      "Gradient Descent(9/49): loss=0.46077950615628777\n",
      "Gradient Descent(10/49): loss=0.4574306283233193\n",
      "Gradient Descent(11/49): loss=0.45424668887967284\n",
      "Gradient Descent(12/49): loss=0.4512191779926461\n",
      "Gradient Descent(13/49): loss=0.4483400300073921\n",
      "Gradient Descent(14/49): loss=0.44560160003189736\n",
      "Gradient Descent(15/49): loss=0.4429966417711257\n",
      "Gradient Descent(16/49): loss=0.4405182865426807\n",
      "Gradient Descent(17/49): loss=0.4381600234100771\n",
      "Gradient Descent(18/49): loss=0.4359156803732314\n",
      "Gradient Descent(19/49): loss=0.433779406559099\n",
      "Gradient Descent(20/49): loss=0.4317456553585308\n",
      "Gradient Descent(21/49): loss=0.42980916845837075\n",
      "Gradient Descent(22/49): loss=0.4279649607206194\n",
      "Gradient Descent(23/49): loss=0.4262083058631192\n",
      "Gradient Descent(24/49): loss=0.4245347228987089\n",
      "Gradient Descent(25/49): loss=0.42293996329214756\n",
      "Gradient Descent(26/49): loss=0.4214199987963258\n",
      "Gradient Descent(27/49): loss=0.4199710099313795\n",
      "Gradient Descent(28/49): loss=0.4185893750723008\n",
      "Gradient Descent(29/49): loss=0.417271660112512\n",
      "Gradient Descent(30/49): loss=0.4160146086726327\n",
      "Gradient Descent(31/49): loss=0.4148151328253419\n",
      "Gradient Descent(32/49): loss=0.4136703043088099\n",
      "Gradient Descent(33/49): loss=0.4125773462026678\n",
      "Gradient Descent(34/49): loss=0.4115336250418859\n",
      "Gradient Descent(35/49): loss=0.41053664334526746\n",
      "Gradient Descent(36/49): loss=0.40958403253651465\n",
      "Gradient Descent(37/49): loss=0.40867354623701757\n",
      "Gradient Descent(38/49): loss=0.4078030539106357\n",
      "Gradient Descent(39/49): loss=0.40697053484180346\n",
      "Gradient Descent(40/49): loss=0.4061740724292971\n",
      "Gradient Descent(41/49): loss=0.40541184877894376\n",
      "Gradient Descent(42/49): loss=0.40468213957945537\n",
      "Gradient Descent(43/49): loss=0.4039833092464139\n",
      "Gradient Descent(44/49): loss=0.4033138063202377\n",
      "Gradient Descent(45/49): loss=0.40267215910471815\n",
      "Gradient Descent(46/49): loss=0.40205697153342795\n",
      "Gradient Descent(47/49): loss=0.4014669192519881\n",
      "Gradient Descent(48/49): loss=0.4009007459048114\n",
      "Gradient Descent(49/49): loss=0.4003572596155589\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.49470728638201206\n",
      "Gradient Descent(2/49): loss=0.4896806337001474\n",
      "Gradient Descent(3/49): loss=0.48490621635614095\n",
      "Gradient Descent(4/49): loss=0.48037093502977324\n",
      "Gradient Descent(5/49): loss=0.4760623781247805\n",
      "Gradient Descent(6/49): loss=0.47196878528752795\n",
      "Gradient Descent(7/49): loss=0.46807901288524484\n",
      "Gradient Descent(8/49): loss=0.4643825013369257\n",
      "Gradient Descent(9/49): loss=0.4608692441959389\n",
      "Gradient Descent(10/49): loss=0.4575297588889944\n",
      "Gradient Descent(11/49): loss=0.4543550590214104\n",
      "Gradient Descent(12/49): loss=0.4513366281635987\n",
      "Gradient Descent(13/49): loss=0.44846639503840335\n",
      "Gradient Descent(14/49): loss=0.4457367100333582\n",
      "Gradient Descent(15/49): loss=0.4431403229661224\n",
      "Gradient Descent(16/49): loss=0.4406703620353015\n",
      "Gradient Descent(17/49): loss=0.4383203138925963\n",
      "Gradient Descent(18/49): loss=0.43608400477573733\n",
      "Gradient Descent(19/49): loss=0.4339555826449898\n",
      "Gradient Descent(20/49): loss=0.43192950026914906\n",
      "Gradient Descent(21/49): loss=0.4300004992099074\n",
      "Gradient Descent(22/49): loss=0.4281635946562713\n",
      "Gradient Descent(23/49): loss=0.42641406106334495\n",
      "Gradient Descent(24/49): loss=0.4247474185522947\n",
      "Gradient Descent(25/49): loss=0.42315942003065504\n",
      "Gradient Descent(26/49): loss=0.42164603899436987\n",
      "Gradient Descent(27/49): loss=0.4202034579750552\n",
      "Gradient Descent(28/49): loss=0.41882805759795827\n",
      "Gradient Descent(29/49): loss=0.4175164062179607\n",
      "Gradient Descent(30/49): loss=0.4162652501027433\n",
      "Gradient Descent(31/49): loss=0.4150715041339042\n",
      "Gradient Descent(32/49): loss=0.41393224299840303\n",
      "Gradient Descent(33/49): loss=0.41284469284419545\n",
      "Gradient Descent(34/49): loss=0.4118062233753366\n",
      "Gradient Descent(35/49): loss=0.41081434036316533\n",
      "Gradient Descent(36/49): loss=0.4098666785514403\n",
      "Gradient Descent(37/49): loss=0.40896099493449367\n",
      "Gradient Descent(38/49): loss=0.4080951623885925\n",
      "Gradient Descent(39/49): loss=0.40726716363776433\n",
      "Gradient Descent(40/49): loss=0.40647508553635087\n",
      "Gradient Descent(41/49): loss=0.40571711365150215\n",
      "Gradient Descent(42/49): loss=0.40499152712972913\n",
      "Gradient Descent(43/49): loss=0.4042966938324794\n",
      "Gradient Descent(44/49): loss=0.4036310657265078\n",
      "Gradient Descent(45/49): loss=0.40299317451557376\n",
      "Gradient Descent(46/49): loss=0.4023816275007186\n",
      "Gradient Descent(47/49): loss=0.40179510365705345\n",
      "Gradient Descent(48/49): loss=0.40123234991563894\n",
      "Gradient Descent(49/49): loss=0.4006921776396377\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4947187082270816\n",
      "Gradient Descent(2/49): loss=0.4896955647877981\n",
      "Gradient Descent(3/49): loss=0.4849175804333833\n",
      "Gradient Descent(4/49): loss=0.48037242409817804\n",
      "Gradient Descent(5/49): loss=0.4760483893815246\n",
      "Gradient Descent(6/49): loss=0.47193436274696526\n",
      "Gradient Descent(7/49): loss=0.4680197933500186\n",
      "Gradient Descent(8/49): loss=0.46429466441044664\n",
      "Gradient Descent(9/49): loss=0.4607494660493216\n",
      "Gradient Descent(10/49): loss=0.45737516951534934\n",
      "Gradient Descent(11/49): loss=0.45416320272885913\n",
      "Gradient Descent(12/49): loss=0.45110542707558465\n",
      "Gradient Descent(13/49): loss=0.448194115385905\n",
      "Gradient Descent(14/49): loss=0.4454219310385543\n",
      "Gradient Descent(15/49): loss=0.44278190813097856\n",
      "Gradient Descent(16/49): loss=0.4402674326615231\n",
      "Gradient Descent(17/49): loss=0.43787222467147524\n",
      "Gradient Descent(18/49): loss=0.4355903212976852\n",
      "Gradient Descent(19/49): loss=0.43341606068903626\n",
      "Gradient Descent(20/49): loss=0.43134406674246034\n",
      "Gradient Descent(21/49): loss=0.4293692346164824\n",
      "Gradient Descent(22/49): loss=0.4274867169824526\n",
      "Gradient Descent(23/49): loss=0.4256919109756826\n",
      "Gradient Descent(24/49): loss=0.42398044581065203\n",
      "Gradient Descent(25/49): loss=0.4223481710263045\n",
      "Gradient Descent(26/49): loss=0.4207911453291981\n",
      "Gradient Descent(27/49): loss=0.4193056260039455\n",
      "Gradient Descent(28/49): loss=0.4178880588619443\n",
      "Gradient Descent(29/49): loss=0.4165350687008978\n",
      "Gradient Descent(30/49): loss=0.4152434502490391\n",
      "Gradient Descent(31/49): loss=0.41401015956930987\n",
      "Gradient Descent(32/49): loss=0.4128323059000255\n",
      "Gradient Descent(33/49): loss=0.41170714390975227\n",
      "Gradient Descent(34/49): loss=0.4106320663452765\n",
      "Gradient Descent(35/49): loss=0.4096045970526238\n",
      "Gradient Descent(36/49): loss=0.4086223843521157\n",
      "Gradient Descent(37/49): loss=0.4076831947494258\n",
      "Gradient Descent(38/49): loss=0.4067849069655236\n",
      "Gradient Descent(39/49): loss=0.40592550626926655\n",
      "Gradient Descent(40/49): loss=0.40510307909723825\n",
      "Gradient Descent(41/49): loss=0.4043158079462141\n",
      "Gradient Descent(42/49): loss=0.4035619665243844\n",
      "Gradient Descent(43/49): loss=0.40283991514817796\n",
      "Gradient Descent(44/49): loss=0.40214809637219656\n",
      "Gradient Descent(45/49): loss=0.4014850308404117\n",
      "Gradient Descent(46/49): loss=0.40084931334738144\n",
      "Gradient Descent(47/49): loss=0.4002396090988146\n",
      "Gradient Descent(48/49): loss=0.39965465016136087\n",
      "Gradient Descent(49/49): loss=0.3990932320920152\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48954097897108784\n",
      "Gradient Descent(2/49): loss=0.4801171219999913\n",
      "Gradient Descent(3/49): loss=0.4716225013963935\n",
      "Gradient Descent(4/49): loss=0.46396215169792826\n",
      "Gradient Descent(5/49): loss=0.4570509225282022\n",
      "Gradient Descent(6/49): loss=0.4508124531547485\n",
      "Gradient Descent(7/49): loss=0.4451782556120623\n",
      "Gradient Descent(8/49): loss=0.44008689470291196\n",
      "Gradient Descent(9/49): loss=0.43548325447538977\n",
      "Gradient Descent(10/49): loss=0.43131788191275033\n",
      "Gradient Descent(11/49): loss=0.4275463995847501\n",
      "Gradient Descent(12/49): loss=0.42412897990773735\n",
      "Gradient Descent(13/49): loss=0.4210298744591476\n",
      "Gradient Descent(14/49): loss=0.41821699250180566\n",
      "Gradient Descent(15/49): loss=0.41566152350463403\n",
      "Gradient Descent(16/49): loss=0.41333759900793815\n",
      "Gradient Descent(17/49): loss=0.41122198968126517\n",
      "Gradient Descent(18/49): loss=0.4092938338668686\n",
      "Gradient Descent(19/49): loss=0.40753439429821225\n",
      "Gradient Descent(20/49): loss=0.40592684003616175\n",
      "Gradient Descent(21/49): loss=0.40445605098033294\n",
      "Gradient Descent(22/49): loss=0.4031084425937962\n",
      "Gradient Descent(23/49): loss=0.40187180872970946\n",
      "Gradient Descent(24/49): loss=0.4007351806718497\n",
      "Gradient Descent(25/49): loss=0.3996887007003903\n",
      "Gradient Descent(26/49): loss=0.3987235086722598\n",
      "Gradient Descent(27/49): loss=0.39783164026436507\n",
      "Gradient Descent(28/49): loss=0.3970059356699382\n",
      "Gradient Descent(29/49): loss=0.396239957665117\n",
      "Gradient Descent(30/49): loss=0.3955279180762366\n",
      "Gradient Descent(31/49): loss=0.39486461177965215\n",
      "Gradient Descent(32/49): loss=0.39424535745652056\n",
      "Gradient Descent(33/49): loss=0.39366594440600916\n",
      "Gradient Descent(34/49): loss=0.39312258479288636\n",
      "Gradient Descent(35/49): loss=0.392611870770307\n",
      "Gradient Descent(36/49): loss=0.39213073597664583\n",
      "Gradient Descent(37/49): loss=0.39167642095718136\n",
      "Gradient Descent(38/49): loss=0.391246442107943\n",
      "Gradient Descent(39/49): loss=0.3908385637806744\n",
      "Gradient Descent(40/49): loss=0.3904507732251621\n",
      "Gradient Descent(41/49): loss=0.39008125807858435\n",
      "Gradient Descent(42/49): loss=0.38972838614145944\n",
      "Gradient Descent(43/49): loss=0.3893906872065863\n",
      "Gradient Descent(44/49): loss=0.3890668367314021\n",
      "Gradient Descent(45/49): loss=0.3887556411657141\n",
      "Gradient Descent(46/49): loss=0.38845602476607205\n",
      "Gradient Descent(47/49): loss=0.3881670177453511\n",
      "Gradient Descent(48/49): loss=0.3878877456216371\n",
      "Gradient Descent(49/49): loss=0.3876174196444205\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4895306504801764\n",
      "Gradient Descent(2/49): loss=0.4800975162686228\n",
      "Gradient Descent(3/49): loss=0.4715945529309665\n",
      "Gradient Descent(4/49): loss=0.46392668801175174\n",
      "Gradient Descent(5/49): loss=0.4570086736611158\n",
      "Gradient Descent(6/49): loss=0.4507640608149442\n",
      "Gradient Descent(7/49): loss=0.4451242818414965\n",
      "Gradient Descent(8/49): loss=0.44002783000471174\n",
      "Gradient Descent(9/49): loss=0.4354195253699859\n",
      "Gradient Descent(10/49): loss=0.4312498579108439\n",
      "Gradient Descent(11/49): loss=0.4274743995810741\n",
      "Gradient Descent(12/49): loss=0.42405327801105697\n",
      "Gradient Descent(13/49): loss=0.42095070528199047\n",
      "Gradient Descent(14/49): loss=0.4181345559387808\n",
      "Gradient Descent(15/49): loss=0.41557598903149656\n",
      "Gradient Descent(16/49): loss=0.4132491095352756\n",
      "Gradient Descent(17/49): loss=0.4111306649972044\n",
      "Gradient Descent(18/49): loss=0.4091997737028498\n",
      "Gradient Descent(19/49): loss=0.40743768105088773\n",
      "Gradient Descent(20/49): loss=0.4058275411770382\n",
      "Gradient Descent(21/49): loss=0.40435422118305897\n",
      "Gradient Descent(22/49): loss=0.4030041256070863\n",
      "Gradient Descent(23/49): loss=0.4017650390219088\n",
      "Gradient Descent(24/49): loss=0.40062598487112877\n",
      "Gradient Descent(25/49): loss=0.3995770988525727\n",
      "Gradient Descent(26/49): loss=0.39860951533635924\n",
      "Gradient Descent(27/49): loss=0.39771526546407254\n",
      "Gradient Descent(28/49): loss=0.3968871857175601\n",
      "Gradient Descent(29/49): loss=0.396118835872843\n",
      "Gradient Descent(30/49): loss=0.39540442536810766\n",
      "Gradient Descent(31/49): loss=0.39473874721620633\n",
      "Gradient Descent(32/49): loss=0.39411711868282073\n",
      "Gradient Descent(33/49): loss=0.3935353280325917\n",
      "Gradient Descent(34/49): loss=0.39298958671811307\n",
      "Gradient Descent(35/49): loss=0.3924764864516437\n",
      "Gradient Descent(36/49): loss=0.39199296065753014\n",
      "Gradient Descent(37/49): loss=0.3915362498553638\n",
      "Gradient Descent(38/49): loss=0.39110387057049073\n",
      "Gradient Descent(39/49): loss=0.3906935874102038\n",
      "Gradient Descent(40/49): loss=0.3903033879813129\n",
      "Gradient Descent(41/49): loss=0.3899314603582508\n",
      "Gradient Descent(42/49): loss=0.3895761728408579\n",
      "Gradient Descent(43/49): loss=0.389236055767848\n",
      "Gradient Descent(44/49): loss=0.3889097851760364\n",
      "Gradient Descent(45/49): loss=0.3885961681169823\n",
      "Gradient Descent(46/49): loss=0.38829412946204395\n",
      "Gradient Descent(47/49): loss=0.3880027000441821\n",
      "Gradient Descent(48/49): loss=0.3877210060003931\n",
      "Gradient Descent(49/49): loss=0.3874482591925994\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4895511775071247\n",
      "Gradient Descent(2/49): loss=0.4801383879259219\n",
      "Gradient Descent(3/49): loss=0.47165544302773993\n",
      "Gradient Descent(4/49): loss=0.4640071540237735\n",
      "Gradient Descent(5/49): loss=0.45710818072826337\n",
      "Gradient Descent(6/49): loss=0.4508820025906069\n",
      "Gradient Descent(7/49): loss=0.44525999849769327\n",
      "Gradient Descent(8/49): loss=0.44018062368151323\n",
      "Gradient Descent(9/49): loss=0.435588673340432\n",
      "Gradient Descent(10/49): loss=0.43143462371383756\n",
      "Gradient Descent(11/49): loss=0.42767404235546547\n",
      "Gradient Descent(12/49): loss=0.4242670602448354\n",
      "Gradient Descent(13/49): loss=0.42117789917160997\n",
      "Gradient Descent(14/49): loss=0.4183744485354239\n",
      "Gradient Descent(15/49): loss=0.41582788633376083\n",
      "Gradient Descent(16/49): loss=0.4135123396714562\n",
      "Gradient Descent(17/49): loss=0.41140458062514035\n",
      "Gradient Descent(18/49): loss=0.4094837537412198\n",
      "Gradient Descent(19/49): loss=0.4077311318428981\n",
      "Gradient Descent(20/49): loss=0.4061298971756141\n",
      "Gradient Descent(21/49): loss=0.4046649452358892\n",
      "Gradient Descent(22/49): loss=0.40332270891013755\n",
      "Gradient Descent(23/49): loss=0.40209100080125504\n",
      "Gradient Descent(24/49): loss=0.4009588718450703\n",
      "Gradient Descent(25/49): loss=0.3999164845189871\n",
      "Gradient Descent(26/49): loss=0.39895499912396515\n",
      "Gradient Descent(27/49): loss=0.39806647178072907\n",
      "Gradient Descent(28/49): loss=0.3972437629238137\n",
      "Gradient Descent(29/49): loss=0.39648045520460473\n",
      "Gradient Descent(30/49): loss=0.3957707798285437\n",
      "Gradient Descent(31/49): loss=0.395109550453599\n",
      "Gradient Descent(32/49): loss=0.3944921038682591\n",
      "Gradient Descent(33/49): loss=0.39391424674883074\n",
      "Gradient Descent(34/49): loss=0.39337220786876337\n",
      "Gradient Descent(35/49): loss=0.39286259519797445\n",
      "Gradient Descent(36/49): loss=0.39238235738855737\n",
      "Gradient Descent(37/49): loss=0.39192874919552034\n",
      "Gradient Descent(38/49): loss=0.3914993004280044\n",
      "Gradient Descent(39/49): loss=0.39109178806832484\n",
      "Gradient Descent(40/49): loss=0.3907042112337024\n",
      "Gradient Descent(41/49): loss=0.39033476868915185\n",
      "Gradient Descent(42/49): loss=0.38998183865010216\n",
      "Gradient Descent(43/49): loss=0.3896439606402854\n",
      "Gradient Descent(44/49): loss=0.3893198191945976\n",
      "Gradient Descent(45/49): loss=0.38900822921829104\n",
      "Gradient Descent(46/49): loss=0.38870812283325745\n",
      "Gradient Descent(47/49): loss=0.3884185375595663\n",
      "Gradient Descent(48/49): loss=0.38813860569601116\n",
      "Gradient Descent(49/49): loss=0.3878675447774017\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4895698445871708\n",
      "Gradient Descent(2/49): loss=0.4801457974161256\n",
      "Gradient Descent(3/49): loss=0.4716279447481536\n",
      "Gradient Descent(4/49): loss=0.4639263640514945\n",
      "Gradient Descent(5/49): loss=0.45696012010202736\n",
      "Gradient Descent(6/49): loss=0.45065636253197355\n",
      "Gradient Descent(7/49): loss=0.44494951449460746\n",
      "Gradient Descent(8/49): loss=0.4397805431767737\n",
      "Gradient Descent(9/49): loss=0.4350963038436209\n",
      "Gradient Descent(10/49): loss=0.4308489499532253\n",
      "Gradient Descent(11/49): loss=0.42699540264327335\n",
      "Gradient Descent(12/49): loss=0.42349687357710397\n",
      "Gradient Descent(13/49): loss=0.4203184357505733\n",
      "Gradient Descent(14/49): loss=0.41742863741187164\n",
      "Gradient Descent(15/49): loss=0.4147991547402784\n",
      "Gradient Descent(16/49): loss=0.41240447937282493\n",
      "Gradient Descent(17/49): loss=0.4102216372652871\n",
      "Gradient Descent(18/49): loss=0.4082299357305819\n",
      "Gradient Descent(19/49): loss=0.4064107358177493\n",
      "Gradient Descent(20/49): loss=0.40474724748205737\n",
      "Gradient Descent(21/49): loss=0.4032243452547651\n",
      "Gradient Descent(22/49): loss=0.40182840235274847\n",
      "Gradient Descent(23/49): loss=0.40054714137626846\n",
      "Gradient Descent(24/49): loss=0.39936949993004894\n",
      "Gradient Descent(25/49): loss=0.39828550967073173\n",
      "Gradient Descent(26/49): loss=0.39728618743462574\n",
      "Gradient Descent(27/49): loss=0.39636343723521833\n",
      "Gradient Descent(28/49): loss=0.39550996204173544\n",
      "Gradient Descent(29/49): loss=0.3947191843595213\n",
      "Gradient Descent(30/49): loss=0.39398517473142974\n",
      "Gradient Descent(31/49): loss=0.39330258736788365\n",
      "Gradient Descent(32/49): loss=0.3926666021928071\n",
      "Gradient Descent(33/49): loss=0.3920728726641455\n",
      "Gradient Descent(34/49): loss=0.3915174787920042\n",
      "Gradient Descent(35/49): loss=0.3909968848352623\n",
      "Gradient Descent(36/49): loss=0.39050790120953016\n",
      "Gradient Descent(37/49): loss=0.3900476501861035\n",
      "Gradient Descent(38/49): loss=0.389613535003632\n",
      "Gradient Descent(39/49): loss=0.38920321205208414\n",
      "Gradient Descent(40/49): loss=0.3888145658226258\n",
      "Gradient Descent(41/49): loss=0.38844568634767046\n",
      "Gradient Descent(42/49): loss=0.3880948488829158\n",
      "Gradient Descent(43/49): loss=0.38776049560797593\n",
      "Gradient Descent(44/49): loss=0.3874412191445337\n",
      "Gradient Descent(45/49): loss=0.38713574771101184\n",
      "Gradient Descent(46/49): loss=0.3868429317508297\n",
      "Gradient Descent(47/49): loss=0.38656173188756915\n",
      "Gradient Descent(48/49): loss=0.3862912080750082\n",
      "Gradient Descent(49/49): loss=0.386030509823147\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4845161858998093\n",
      "Gradient Descent(2/49): loss=0.47129903537506\n",
      "Gradient Descent(3/49): loss=0.4600054927733153\n",
      "Gradient Descent(4/49): loss=0.4503449937694284\n",
      "Gradient Descent(5/49): loss=0.4420713481219598\n",
      "Gradient Descent(6/49): loss=0.4349758940617102\n",
      "Gradient Descent(7/49): loss=0.4288817218501094\n",
      "Gradient Descent(8/49): loss=0.42363879693017303\n",
      "Gradient Descent(9/49): loss=0.4191198405063824\n",
      "Gradient Descent(10/49): loss=0.41521684826830196\n",
      "Gradient Descent(11/49): loss=0.4118381470869178\n",
      "Gradient Descent(12/49): loss=0.40890590549833844\n",
      "Gradient Descent(13/49): loss=0.406354027171837\n",
      "Gradient Descent(14/49): loss=0.4041263677726004\n",
      "Gradient Descent(15/49): loss=0.4021752250338731\n",
      "Gradient Descent(16/49): loss=0.40046005974690596\n",
      "Gradient Descent(17/49): loss=0.3989464120081788\n",
      "Gradient Descent(18/49): loss=0.39760498263793514\n",
      "Gradient Descent(19/49): loss=0.396410854373822\n",
      "Gradient Descent(20/49): loss=0.3953428313914795\n",
      "Gradient Descent(21/49): loss=0.3943828790296974\n",
      "Gradient Descent(22/49): loss=0.3935156484010326\n",
      "Gradient Descent(23/49): loss=0.39272807293299433\n",
      "Gradient Descent(24/49): loss=0.39200902587989633\n",
      "Gradient Descent(25/49): loss=0.39134902952978123\n",
      "Gradient Descent(26/49): loss=0.39074000825348965\n",
      "Gradient Descent(27/49): loss=0.3901750787451904\n",
      "Gradient Descent(28/49): loss=0.38964837182007395\n",
      "Gradient Descent(29/49): loss=0.38915488099454326\n",
      "Gradient Descent(30/49): loss=0.3886903338015524\n",
      "Gradient Descent(31/49): loss=0.3882510824093433\n",
      "Gradient Descent(32/49): loss=0.3878340106330511\n",
      "Gradient Descent(33/49): loss=0.38743645487010486\n",
      "Gradient Descent(34/49): loss=0.38705613686436047\n",
      "Gradient Descent(35/49): loss=0.38669110652086686\n",
      "Gradient Descent(36/49): loss=0.38633969326186335\n",
      "Gradient Descent(37/49): loss=0.38600046464243587\n",
      "Gradient Descent(38/49): loss=0.38567219113749873\n",
      "Gradient Descent(39/49): loss=0.3853538161757007\n",
      "Gradient Descent(40/49): loss=0.385044430634949\n",
      "Gradient Descent(41/49): loss=0.38474325113231583\n",
      "Gradient Descent(42/49): loss=0.38444960154129554\n",
      "Gradient Descent(43/49): loss=0.3841628972544881\n",
      "Gradient Descent(44/49): loss=0.38388263178203286\n",
      "Gradient Descent(45/49): loss=0.38360836533750536\n",
      "Gradient Descent(46/49): loss=0.3833397151151217\n",
      "Gradient Descent(47/49): loss=0.3830763470064026\n",
      "Gradient Descent(48/49): loss=0.3828179685420928\n",
      "Gradient Descent(49/49): loss=0.3825643228771312\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48450090179257616\n",
      "Gradient Descent(2/49): loss=0.4712707643907081\n",
      "Gradient Descent(3/49): loss=0.4599661505375554\n",
      "Gradient Descent(4/49): loss=0.45029616321754923\n",
      "Gradient Descent(5/49): loss=0.4420143258988508\n",
      "Gradient Descent(6/49): loss=0.434911733166814\n",
      "Gradient Descent(7/49): loss=0.4288112699376789\n",
      "Gradient Descent(8/49): loss=0.42356273010455775\n",
      "Gradient Descent(9/49): loss=0.4190386927244756\n",
      "Gradient Descent(10/49): loss=0.4151310366242257\n",
      "Gradient Descent(11/49): loss=0.4117479933418517\n",
      "Gradient Descent(12/49): loss=0.40881165425579796\n",
      "Gradient Descent(13/49): loss=0.40625586110321826\n",
      "Gradient Descent(14/49): loss=0.40402442028180907\n",
      "Gradient Descent(15/49): loss=0.40206959072190857\n",
      "Gradient Descent(16/49): loss=0.40035080300324183\n",
      "Gradient Descent(17/49): loss=0.3988335740195542\n",
      "Gradient Descent(18/49): loss=0.3974885870692587\n",
      "Gradient Descent(19/49): loss=0.39629091194183225\n",
      "Gradient Descent(20/49): loss=0.39521934352046184\n",
      "Gradient Descent(21/49): loss=0.39425584075034054\n",
      "Gradient Descent(22/49): loss=0.393385050628528\n",
      "Gradient Descent(23/49): loss=0.3925939042386553\n",
      "Gradient Descent(24/49): loss=0.3918712738517149\n",
      "Gradient Descent(25/49): loss=0.3912076818011766\n",
      "Gradient Descent(26/49): loss=0.39059505326576993\n",
      "Gradient Descent(27/49): loss=0.390026506297642\n",
      "Gradient Descent(28/49): loss=0.38949617345185655\n",
      "Gradient Descent(29/49): loss=0.3889990502344253\n",
      "Gradient Descent(30/49): loss=0.38853086631475753\n",
      "Gradient Descent(31/49): loss=0.3880879760651737\n",
      "Gradient Descent(32/49): loss=0.38766726551233704\n",
      "Gradient Descent(33/49): loss=0.3872660732277337\n",
      "Gradient Descent(34/49): loss=0.38688212305903225\n",
      "Gradient Descent(35/49): loss=0.3865134669216946\n",
      "Gradient Descent(36/49): loss=0.38615843613937645\n",
      "Gradient Descent(37/49): loss=0.38581560004988075\n",
      "Gradient Descent(38/49): loss=0.38548373078699116\n",
      "Gradient Descent(39/49): loss=0.3851617733127002\n",
      "Gradient Descent(40/49): loss=0.3848488199136719\n",
      "Gradient Descent(41/49): loss=0.3845440884940099\n",
      "Gradient Descent(42/49): loss=0.38424690409676193\n",
      "Gradient Descent(43/49): loss=0.3839566831718063\n",
      "Gradient Descent(44/49): loss=0.3836729201801136\n",
      "Gradient Descent(45/49): loss=0.38339517618583424\n",
      "Gradient Descent(46/49): loss=0.38312306913986266\n",
      "Gradient Descent(47/49): loss=0.3828562656028779\n",
      "Gradient Descent(48/49): loss=0.3825944736935448\n",
      "Gradient Descent(49/49): loss=0.38233743707959106\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48453167337533853\n",
      "Gradient Descent(2/49): loss=0.471331809004944\n",
      "Gradient Descent(3/49): loss=0.4600565208035324\n",
      "Gradient Descent(4/49): loss=0.450414592928314\n",
      "Gradient Descent(5/49): loss=0.4421593352297591\n",
      "Gradient Descent(6/49): loss=0.435081712592729\n",
      "Gradient Descent(7/49): loss=0.4290045461032982\n",
      "Gradient Descent(8/49): loss=0.4237776165821778\n",
      "Gradient Descent(9/49): loss=0.4192735282603911\n",
      "Gradient Descent(10/49): loss=0.4153842131431112\n",
      "Gradient Descent(11/49): loss=0.4120179756624338\n",
      "Gradient Descent(12/49): loss=0.40909699317940473\n",
      "Gradient Descent(13/49): loss=0.4065552012736337\n",
      "Gradient Descent(14/49): loss=0.40433650398173293\n",
      "Gradient Descent(15/49): loss=0.4023932585677767\n",
      "Gradient Descent(16/49): loss=0.4006849923246431\n",
      "Gradient Descent(17/49): loss=0.39917731555973185\n",
      "Gradient Descent(18/49): loss=0.3978410005166604\n",
      "Gradient Descent(19/49): loss=0.39665120069670723\n",
      "Gradient Descent(20/49): loss=0.39558678901251243\n",
      "Gradient Descent(21/49): loss=0.3946297965509551\n",
      "Gradient Descent(22/49): loss=0.393764936541926\n",
      "Gradient Descent(23/49): loss=0.39297920050831964\n",
      "Gradient Descent(24/49): loss=0.39226151557999656\n",
      "Gradient Descent(25/49): loss=0.3916024536493593\n",
      "Gradient Descent(26/49): loss=0.3909939844778228\n",
      "Gradient Descent(27/49): loss=0.3904292660722109\n",
      "Gradient Descent(28/49): loss=0.3899024666727518\n",
      "Gradient Descent(29/49): loss=0.3894086135591386\n",
      "Gradient Descent(30/49): loss=0.3889434646126702\n",
      "Gradient Descent(31/49): loss=0.388503399191535\n",
      "Gradient Descent(32/49): loss=0.3880853254003058\n",
      "Gradient Descent(33/49): loss=0.3876866012784001\n",
      "Gradient Descent(34/49): loss=0.38730496780804763\n",
      "Gradient Descent(35/49): loss=0.38693849196066865\n",
      "Gradient Descent(36/49): loss=0.38658551827035076\n",
      "Gradient Descent(37/49): loss=0.3862446276517837\n",
      "Gradient Descent(38/49): loss=0.3859146023738817\n",
      "Gradient Descent(39/49): loss=0.38559439626473146\n",
      "Gradient Descent(40/49): loss=0.38528310936294313\n",
      "Gradient Descent(41/49): loss=0.38497996634878745\n",
      "Gradient Descent(42/49): loss=0.38468429818887573\n",
      "Gradient Descent(43/49): loss=0.38439552651333836\n",
      "Gradient Descent(44/49): loss=0.3841131503167645\n",
      "Gradient Descent(45/49): loss=0.3838367346355643\n",
      "Gradient Descent(46/49): loss=0.38356590090654236\n",
      "Gradient Descent(47/49): loss=0.38330031875574677\n",
      "Gradient Descent(48/49): loss=0.3830396990042633\n",
      "Gradient Descent(49/49): loss=0.38278378770956867\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.48455340908026734\n",
      "Gradient Descent(2/49): loss=0.47131175017857097\n",
      "Gradient Descent(3/49): loss=0.4599509623985761\n",
      "Gradient Descent(4/49): loss=0.45019494006038846\n",
      "Gradient Descent(5/49): loss=0.44180840357970674\n",
      "Gradient Descent(6/49): loss=0.4345908358817696\n",
      "Gradient Descent(7/49): loss=0.4283713239327565\n",
      "Gradient Descent(8/49): loss=0.4230041693671269\n",
      "Gradient Descent(9/49): loss=0.4183651528230387\n",
      "Gradient Descent(10/49): loss=0.4143483540607045\n",
      "Gradient Descent(11/49): loss=0.41086344472634845\n",
      "Gradient Descent(12/49): loss=0.4078333831538901\n",
      "Gradient Descent(13/49): loss=0.4051924512178655\n",
      "Gradient Descent(14/49): loss=0.4028845822591901\n",
      "Gradient Descent(15/49): loss=0.40086193674854226\n",
      "Gradient Descent(16/49): loss=0.3990836888397963\n",
      "Gradient Descent(17/49): loss=0.3975149924748107\n",
      "Gradient Descent(18/49): loss=0.39612610038024143\n",
      "Gradient Descent(19/49): loss=0.3948916132730596\n",
      "Gradient Descent(20/49): loss=0.3937898399708243\n",
      "Gradient Descent(21/49): loss=0.39280225197578733\n",
      "Gradient Descent(22/49): loss=0.3919130185450757\n",
      "Gradient Descent(23/49): loss=0.39110861033728767\n",
      "Gradient Descent(24/49): loss=0.3903774614937912\n",
      "Gradient Descent(25/49): loss=0.38970968151739616\n",
      "Gradient Descent(26/49): loss=0.3890968095914192\n",
      "Gradient Descent(27/49): loss=0.3885316050720145\n",
      "Gradient Descent(28/49): loss=0.38800786881448673\n",
      "Gradient Descent(29/49): loss=0.3875202907843437\n",
      "Gradient Descent(30/49): loss=0.3870643200766221\n",
      "Gradient Descent(31/49): loss=0.3866360540400275\n",
      "Gradient Descent(32/49): loss=0.38623214369050074\n",
      "Gradient Descent(33/49): loss=0.3858497130146098\n",
      "Gradient Descent(34/49): loss=0.3854862901173906\n",
      "Gradient Descent(35/49): loss=0.38513974847108595\n",
      "Gradient Descent(36/49): loss=0.3848082567784115\n",
      "Gradient Descent(37/49): loss=0.38449023618315026\n",
      "Gradient Descent(38/49): loss=0.3841843237476645\n",
      "Gradient Descent(39/49): loss=0.38388934127611063\n",
      "Gradient Descent(40/49): loss=0.3836042686978498\n",
      "Gradient Descent(41/49): loss=0.3833282213412071\n",
      "Gradient Descent(42/49): loss=0.38306043052635236\n",
      "Gradient Descent(43/49): loss=0.3828002269901345\n",
      "Gradient Descent(44/49): loss=0.38254702672737523\n",
      "Gradient Descent(45/49): loss=0.38230031889424004\n",
      "Gradient Descent(46/49): loss=0.38205965547140835\n",
      "Gradient Descent(47/49): loss=0.3818246424291958\n",
      "Gradient Descent(48/49): loss=0.38159493217467205\n",
      "Gradient Descent(49/49): loss=0.3813702170931288\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4796278711239821\n",
      "Gradient Descent(2/49): loss=0.46317586262790655\n",
      "Gradient Descent(3/49): loss=0.44986411048796554\n",
      "Gradient Descent(4/49): loss=0.43906954916518415\n",
      "Gradient Descent(5/49): loss=0.43029406710700435\n",
      "Gradient Descent(6/49): loss=0.42313920921973097\n",
      "Gradient Descent(7/49): loss=0.4172860596822474\n",
      "Gradient Descent(8/49): loss=0.4124792291336434\n",
      "Gradient Descent(9/49): loss=0.4085140977010925\n",
      "Gradient Descent(10/49): loss=0.40522664366029704\n",
      "Gradient Descent(11/49): loss=0.4024853276118663\n",
      "Gradient Descent(12/49): loss=0.4001846123090254\n",
      "Gradient Descent(13/49): loss=0.39823978518598807\n",
      "Gradient Descent(14/49): loss=0.3965828192590132\n",
      "Gradient Descent(15/49): loss=0.39515906233131876\n",
      "Gradient Descent(16/49): loss=0.3939245873925084\n",
      "Gradient Descent(17/49): loss=0.3928440711583562\n",
      "Gradient Descent(18/49): loss=0.3918890947242422\n",
      "Gradient Descent(19/49): loss=0.3910367817782518\n",
      "Gradient Descent(20/49): loss=0.3902687068962782\n",
      "Gradient Descent(21/49): loss=0.38957002003384184\n",
      "Gradient Descent(22/49): loss=0.38892874415762263\n",
      "Gradient Descent(23/49): loss=0.38833521159266715\n",
      "Gradient Descent(24/49): loss=0.3877816115489064\n",
      "Gradient Descent(25/49): loss=0.38726162678946585\n",
      "Gradient Descent(26/49): loss=0.38677014179603436\n",
      "Gradient Descent(27/49): loss=0.3863030082977461\n",
      "Gradient Descent(28/49): loss=0.38585685683804577\n",
      "Gradient Descent(29/49): loss=0.38542894530078203\n",
      "Gradient Descent(30/49): loss=0.3850170371153026\n",
      "Gradient Descent(31/49): loss=0.3846193033006556\n",
      "Gradient Descent(32/49): loss=0.3842342436629387\n",
      "Gradient Descent(33/49): loss=0.38386062338466725\n",
      "Gradient Descent(34/49): loss=0.3834974219865055\n",
      "Gradient Descent(35/49): loss=0.3831437922363742\n",
      "Gradient Descent(36/49): loss=0.3827990270580182\n",
      "Gradient Descent(37/49): loss=0.3824625328739612\n",
      "Gradient Descent(38/49): loss=0.38213380812509523\n",
      "Gradient Descent(39/49): loss=0.38181242595589476\n",
      "Gradient Descent(40/49): loss=0.38149802025241647\n",
      "Gradient Descent(41/49): loss=0.38119027437944086\n",
      "Gradient Descent(42/49): loss=0.3808889120910151\n",
      "Gradient Descent(43/49): loss=0.380593690191458\n",
      "Gradient Descent(44/49): loss=0.3803043926065162\n",
      "Gradient Descent(45/49): loss=0.380020825590798\n",
      "Gradient Descent(46/49): loss=0.3797428138510329\n",
      "Gradient Descent(47/49): loss=0.3794701974076763\n",
      "Gradient Descent(48/49): loss=0.379202829051943\n",
      "Gradient Descent(49/49): loss=0.37894057228316824\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47960777048651726\n",
      "Gradient Descent(2/49): loss=0.46313962550668836\n",
      "Gradient Descent(3/49): loss=0.44981482585406984\n",
      "Gradient Descent(4/49): loss=0.4390095834097197\n",
      "Gradient Descent(5/49): loss=0.4302252034141913\n",
      "Gradient Descent(6/49): loss=0.42306276877674376\n",
      "Gradient Descent(7/49): loss=0.417203003622145\n",
      "Gradient Descent(8/49): loss=0.4123902421799329\n",
      "Gradient Descent(9/49): loss=0.4084196554800738\n",
      "Gradient Descent(10/49): loss=0.40512706600898774\n",
      "Gradient Descent(11/49): loss=0.4023808202125421\n",
      "Gradient Descent(12/49): loss=0.40007529879838655\n",
      "Gradient Descent(13/49): loss=0.39812573162163545\n",
      "Gradient Descent(14/49): loss=0.39646405253876194\n",
      "Gradient Descent(15/49): loss=0.3950355838849656\n",
      "Gradient Descent(16/49): loss=0.39379638321734933\n",
      "Gradient Descent(17/49): loss=0.39271111905531864\n",
      "Gradient Descent(18/49): loss=0.39175136941159117\n",
      "Gradient Descent(19/49): loss=0.3908942584120613\n",
      "Gradient Descent(20/49): loss=0.3901213634074787\n",
      "Gradient Descent(21/49): loss=0.3894178385964766\n",
      "Gradient Descent(22/49): loss=0.38877171202789135\n",
      "Gradient Descent(23/49): loss=0.38817332149972117\n",
      "Gradient Descent(24/49): loss=0.38761486177294524\n",
      "Gradient Descent(25/49): loss=0.3870900210277811\n",
      "Gradient Descent(26/49): loss=0.38659368889099993\n",
      "Gradient Descent(27/49): loss=0.3861217218805513\n",
      "Gradient Descent(28/49): loss=0.38567075492675773\n",
      "Gradient Descent(29/49): loss=0.38523804987994775\n",
      "Gradient Descent(30/49): loss=0.38482137371585773\n",
      "Gradient Descent(31/49): loss=0.3844189005926955\n",
      "Gradient Descent(32/49): loss=0.3840291330693734\n",
      "Gradient Descent(33/49): loss=0.38365083872051464\n",
      "Gradient Descent(34/49): loss=0.3832829991262544\n",
      "Gradient Descent(35/49): loss=0.38292476881022297\n",
      "Gradient Descent(36/49): loss=0.3825754421766864\n",
      "Gradient Descent(37/49): loss=0.3822344268810422\n",
      "Gradient Descent(38/49): loss=0.3819012223754568\n",
      "Gradient Descent(39/49): loss=0.3815754026183708\n",
      "Gradient Descent(40/49): loss=0.381256602134911\n",
      "Gradient Descent(41/49): loss=0.38094450477453223\n",
      "Gradient Descent(42/49): loss=0.38063883464018305\n",
      "Gradient Descent(43/49): loss=0.38033934876613706\n",
      "Gradient Descent(44/49): loss=0.3800458312042751\n",
      "Gradient Descent(45/49): loss=0.3797580882450673\n",
      "Gradient Descent(46/49): loss=0.37947594455292233\n",
      "Gradient Descent(47/49): loss=0.37919924003854916\n",
      "Gradient Descent(48/49): loss=0.37892782732553565\n",
      "Gradient Descent(49/49): loss=0.3786615696961545\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47964877398665307\n",
      "Gradient Descent(2/49): loss=0.4632205933673069\n",
      "Gradient Descent(3/49): loss=0.44993373870171327\n",
      "Gradient Descent(4/49): loss=0.439163824963954\n",
      "Gradient Descent(5/49): loss=0.4304118456118534\n",
      "Gradient Descent(6/49): loss=0.4232787777044713\n",
      "Gradient Descent(7/49): loss=0.4174453830711382\n",
      "Gradient Descent(8/49): loss=0.41265612912177724\n",
      "Gradient Descent(9/49): loss=0.4087063792722653\n",
      "Gradient Descent(10/49): loss=0.405432180854777\n",
      "Gradient Descent(11/49): loss=0.40270211840309295\n",
      "Gradient Descent(12/49): loss=0.4004108105777341\n",
      "Gradient Descent(13/49): loss=0.39847371612481597\n",
      "Gradient Descent(14/49): loss=0.3968229831319286\n",
      "Gradient Descent(15/49): loss=0.395404130347485\n",
      "Gradient Descent(16/49): loss=0.39417339251283434\n",
      "Gradient Descent(17/49): loss=0.39309559590649823\n",
      "Gradient Descent(18/49): loss=0.3921424574917724\n",
      "Gradient Descent(19/49): loss=0.39129122266698063\n",
      "Gradient Descent(20/49): loss=0.39052357380314523\n",
      "Gradient Descent(21/49): loss=0.38982475543278783\n",
      "Gradient Descent(22/49): loss=0.3891828728494955\n",
      "Gradient Descent(23/49): loss=0.3885883295629911\n",
      "Gradient Descent(24/49): loss=0.38803337598174137\n",
      "Gradient Descent(25/49): loss=0.38751174722371207\n",
      "Gradient Descent(26/49): loss=0.38701837237061815\n",
      "Gradient Descent(27/49): loss=0.3865491410081753\n",
      "Gradient Descent(28/49): loss=0.38610071571425675\n",
      "Gradient Descent(29/49): loss=0.3856703814115587\n",
      "Gradient Descent(30/49): loss=0.3852559243052553\n",
      "Gradient Descent(31/49): loss=0.38485553456990673\n",
      "Gradient Descent(32/49): loss=0.38446772810589996\n",
      "Gradient Descent(33/49): loss=0.3840912836116361\n",
      "Gradient Descent(34/49): loss=0.38372519195957505\n",
      "Gradient Descent(35/49): loss=0.3833686154588879\n",
      "Gradient Descent(36/49): loss=0.3830208550642093\n",
      "Gradient Descent(37/49): loss=0.38268132397232824\n",
      "Gradient Descent(38/49): loss=0.3823495263553643\n",
      "Gradient Descent(39/49): loss=0.3820250402250945\n",
      "Gradient Descent(40/49): loss=0.38170750362062517\n",
      "Gradient Descent(41/49): loss=0.3813966034701825\n",
      "Gradient Descent(42/49): loss=0.3810920666051417\n",
      "Gradient Descent(43/49): loss=0.38079365250668196\n",
      "Gradient Descent(44/49): loss=0.38050114744762426\n",
      "Gradient Descent(45/49): loss=0.3802143597580248\n",
      "Gradient Descent(46/49): loss=0.37993311599615603\n",
      "Gradient Descent(47/49): loss=0.3796572578491563\n",
      "Gradient Descent(48/49): loss=0.3793866396219222\n",
      "Gradient Descent(49/49): loss=0.3791211262003882\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47966940170637157\n",
      "Gradient Descent(2/49): loss=0.4631555152790177\n",
      "Gradient Descent(3/49): loss=0.4497205366373557\n",
      "Gradient Descent(4/49): loss=0.4387702560975397\n",
      "Gradient Descent(5/49): loss=0.42982598863150245\n",
      "Gradient Descent(6/49): loss=0.4225020188698072\n",
      "Gradient Descent(7/49): loss=0.416487470070845\n",
      "Gradient Descent(8/49): loss=0.4115317240411562\n",
      "Gradient Descent(9/49): loss=0.407432692719963\n",
      "Gradient Descent(10/49): loss=0.40402738089504087\n",
      "Gradient Descent(11/49): loss=0.40118429044970905\n",
      "Gradient Descent(12/49): loss=0.3987973053110271\n",
      "Gradient Descent(13/49): loss=0.39678076736282014\n",
      "Gradient Descent(14/49): loss=0.3950655105655827\n",
      "Gradient Descent(15/49): loss=0.39359566622101716\n",
      "Gradient Descent(16/49): loss=0.3923260889876528\n",
      "Gradient Descent(17/49): loss=0.39122028269439835\n",
      "Gradient Descent(18/49): loss=0.39024872864699395\n",
      "Gradient Descent(19/49): loss=0.3893875381257749\n",
      "Gradient Descent(20/49): loss=0.3886173660499404\n",
      "Gradient Descent(21/49): loss=0.3879225350686255\n",
      "Gradient Descent(22/49): loss=0.38729032922135015\n",
      "Gradient Descent(23/49): loss=0.38671042426201524\n",
      "Gradient Descent(24/49): loss=0.3861744281402884\n",
      "Gradient Descent(25/49): loss=0.385675510286011\n",
      "Gradient Descent(26/49): loss=0.38520810249030685\n",
      "Gradient Descent(27/49): loss=0.38476765751757486\n",
      "Gradient Descent(28/49): loss=0.3843504542731366\n",
      "Gradient Descent(29/49): loss=0.3839534405187736\n",
      "Gradient Descent(30/49): loss=0.3835741058746791\n",
      "Gradient Descent(31/49): loss=0.383210379253476\n",
      "Gradient Descent(32/49): loss=0.38286054600594027\n",
      "Gradient Descent(33/49): loss=0.38252318097202104\n",
      "Gradient Descent(34/49): loss=0.38219709436745103\n",
      "Gradient Descent(35/49): loss=0.38188128803012\n",
      "Gradient Descent(36/49): loss=0.38157492002917553\n",
      "Gradient Descent(37/49): loss=0.3812772760258608\n",
      "Gradient Descent(38/49): loss=0.380987746086384\n",
      "Gradient Descent(39/49): loss=0.3807058058981496\n",
      "Gradient Descent(40/49): loss=0.3804310015431316\n",
      "Gradient Descent(41/49): loss=0.38016293714546684\n",
      "Gradient Descent(42/49): loss=0.3799012648420594\n",
      "Gradient Descent(43/49): loss=0.3796456766312462\n",
      "Gradient Descent(44/49): loss=0.3793958977402977\n",
      "Gradient Descent(45/49): loss=0.3791516812216926\n",
      "Gradient Descent(46/49): loss=0.37891280354392154\n",
      "Gradient Descent(47/49): loss=0.3786790609876075\n",
      "Gradient Descent(48/49): loss=0.37845026669409015\n",
      "Gradient Descent(49/49): loss=0.37822624824295537\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4748760346436066\n",
      "Gradient Descent(2/49): loss=0.45570854580085923\n",
      "Gradient Descent(3/49): loss=0.44103758578257657\n",
      "Gradient Descent(4/49): loss=0.4297647403395375\n",
      "Gradient Descent(5/49): loss=0.4210627421661724\n",
      "Gradient Descent(6/49): loss=0.4143079434764678\n",
      "Gradient Descent(7/49): loss=0.40902978356740266\n",
      "Gradient Descent(8/49): loss=0.40487291814238874\n",
      "Gradient Descent(9/49): loss=0.4015687998857241\n",
      "Gradient Descent(10/49): loss=0.3989143258833269\n",
      "Gradient Descent(11/49): loss=0.3967557771678945\n",
      "Gradient Descent(12/49): loss=0.39497672685917884\n",
      "Gradient Descent(13/49): loss=0.39348892810370034\n",
      "Gradient Descent(14/49): loss=0.39222544191242936\n",
      "Gradient Descent(15/49): loss=0.3911354504398061\n",
      "Gradient Descent(16/49): loss=0.3901803396722211\n",
      "Gradient Descent(17/49): loss=0.389330738992602\n",
      "Gradient Descent(18/49): loss=0.3885642825884621\n",
      "Gradient Descent(19/49): loss=0.38786391578271157\n",
      "Gradient Descent(20/49): loss=0.387216612993627\n",
      "Gradient Descent(21/49): loss=0.3866124068193846\n",
      "Gradient Descent(22/49): loss=0.3860436524106639\n",
      "Gradient Descent(23/49): loss=0.38550446987025094\n",
      "Gradient Descent(24/49): loss=0.3849903214177136\n",
      "Gradient Descent(25/49): loss=0.38449769061547745\n",
      "Gradient Descent(26/49): loss=0.38402383892124337\n",
      "Gradient Descent(27/49): loss=0.38356662084963006\n",
      "Gradient Descent(28/49): loss=0.3831243435733601\n",
      "Gradient Descent(29/49): loss=0.3826956602323961\n",
      "Gradient Descent(30/49): loss=0.382279488820097\n",
      "Gradient Descent(31/49): loss=0.3818749504835854\n",
      "Gradient Descent(32/49): loss=0.3814813225655776\n",
      "Gradient Descent(33/49): loss=0.3810980028435142\n",
      "Gradient Descent(34/49): loss=0.3807244822769432\n",
      "Gradient Descent(35/49): loss=0.3803603242222591\n",
      "Gradient Descent(36/49): loss=0.380005148565337\n",
      "Gradient Descent(37/49): loss=0.3796586195953397\n",
      "Gradient Descent(38/49): loss=0.37932043672577104\n",
      "Gradient Descent(39/49): loss=0.37899032738347277\n",
      "Gradient Descent(40/49): loss=0.37866804154919054\n",
      "Gradient Descent(41/49): loss=0.3783533475570559\n",
      "Gradient Descent(42/49): loss=0.3780460288543139\n",
      "Gradient Descent(43/49): loss=0.3777458814940222\n",
      "Gradient Descent(44/49): loss=0.3774527121877207\n",
      "Gradient Descent(45/49): loss=0.37716633678632194\n",
      "Gradient Descent(46/49): loss=0.3768865790888465\n",
      "Gradient Descent(47/49): loss=0.3766132699024912\n",
      "Gradient Descent(48/49): loss=0.3763462462956769\n",
      "Gradient Descent(49/49): loss=0.37608535099954443\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4748512565619997\n",
      "Gradient Descent(2/49): loss=0.45566499833922497\n",
      "Gradient Descent(3/49): loss=0.4409796255103976\n",
      "Gradient Descent(4/49): loss=0.4296954381383608\n",
      "Gradient Descent(5/49): loss=0.42098420279347426\n",
      "Gradient Descent(6/49): loss=0.4142215659833289\n",
      "Gradient Descent(7/49): loss=0.4089364643624528\n",
      "Gradient Descent(8/49): loss=0.4047732043534576\n",
      "Gradient Descent(9/49): loss=0.40146300229446874\n",
      "Gradient Descent(10/49): loss=0.39880260030903925\n",
      "Gradient Descent(11/49): loss=0.3966381819495596\n",
      "Gradient Descent(12/49): loss=0.3948532626440833\n",
      "Gradient Descent(13/49): loss=0.3933595647876229\n",
      "Gradient Descent(14/49): loss=0.3920901364076841\n",
      "Gradient Descent(15/49): loss=0.39099415800099024\n",
      "Gradient Descent(16/49): loss=0.39003302077140517\n",
      "Gradient Descent(17/49): loss=0.3891773631752955\n",
      "Gradient Descent(18/49): loss=0.38840483032485934\n",
      "Gradient Descent(19/49): loss=0.38769837902372806\n",
      "Gradient Descent(20/49): loss=0.3870449949211721\n",
      "Gradient Descent(21/49): loss=0.38643472112345234\n",
      "Gradient Descent(22/49): loss=0.3858599223152879\n",
      "Gradient Descent(23/49): loss=0.3853147270533363\n",
      "Gradient Descent(24/49): loss=0.38479460491669343\n",
      "Gradient Descent(25/49): loss=0.3842960457746406\n",
      "Gradient Descent(26/49): loss=0.3838163164123778\n",
      "Gradient Descent(27/49): loss=0.38335327578169\n",
      "Gradient Descent(28/49): loss=0.3829052346966134\n",
      "Gradient Descent(29/49): loss=0.38247084923612895\n",
      "Gradient Descent(30/49): loss=0.38204903971917376\n",
      "Gradient Descent(31/49): loss=0.3816389290871081\n",
      "Gradient Descent(32/49): loss=0.381239796019959\n",
      "Gradient Descent(33/49): loss=0.38085103924205116\n",
      "Gradient Descent(34/49): loss=0.38047215032819626\n",
      "Gradient Descent(35/49): loss=0.3801026929699987\n",
      "Gradient Descent(36/49): loss=0.379742287153411\n",
      "Gradient Descent(37/49): loss=0.37939059707145667\n",
      "Gradient Descent(38/49): loss=0.37904732187884044\n",
      "Gradient Descent(39/49): loss=0.37871218860976036\n",
      "Gradient Descent(40/49): loss=0.37838494674312456\n",
      "Gradient Descent(41/49): loss=0.3780653640230471\n",
      "Gradient Descent(42/49): loss=0.3777532232364267\n",
      "Gradient Descent(43/49): loss=0.3774483197207583\n",
      "Gradient Descent(44/49): loss=0.37715045942954994\n",
      "Gradient Descent(45/49): loss=0.3768594574239237\n",
      "Gradient Descent(46/49): loss=0.3765751366903147\n",
      "Gradient Descent(47/49): loss=0.3762973272080046\n",
      "Gradient Descent(48/49): loss=0.37602586520835535\n",
      "Gradient Descent(49/49): loss=0.3757605925814007\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47490247934106855\n",
      "Gradient Descent(2/49): loss=0.4557655875455755\n",
      "Gradient Descent(3/49): loss=0.441125968236976\n",
      "Gradient Descent(4/49): loss=0.4298830358880223\n",
      "Gradient Descent(5/49): loss=0.42120826155294977\n",
      "Gradient Descent(6/49): loss=0.4144773750549215\n",
      "Gradient Descent(7/49): loss=0.4092196221764727\n",
      "Gradient Descent(8/49): loss=0.40507973734790975\n",
      "Gradient Descent(9/49): loss=0.40178941190380363\n",
      "Gradient Descent(10/49): loss=0.3991458638367953\n",
      "Gradient Descent(11/49): loss=0.39699572571760605\n",
      "Gradient Descent(12/49): loss=0.39522292014816457\n",
      "Gradient Descent(13/49): loss=0.39373952837448034\n",
      "Gradient Descent(14/49): loss=0.3924789079353446\n",
      "Gradient Descent(15/49): loss=0.3913905017872916\n",
      "Gradient Descent(16/49): loss=0.39043592065327193\n",
      "Gradient Descent(17/49): loss=0.38958598451287774\n",
      "Gradient Descent(18/49): loss=0.3888184871504527\n",
      "Gradient Descent(19/49): loss=0.3881165061494863\n",
      "Gradient Descent(20/49): loss=0.3874671246035717\n",
      "Gradient Descent(21/49): loss=0.38686046377941186\n",
      "Gradient Descent(22/49): loss=0.38628895075433745\n",
      "Gradient Descent(23/49): loss=0.3857467637040888\n",
      "Gradient Descent(24/49): loss=0.38522941156486806\n",
      "Gradient Descent(25/49): loss=0.3847334153812707\n",
      "Gradient Descent(26/49): loss=0.38425606663639433\n",
      "Gradient Descent(27/49): loss=0.38379524388577074\n",
      "Gradient Descent(28/49): loss=0.3833492735661402\n",
      "Gradient Descent(29/49): loss=0.38291682428684576\n",
      "Gradient Descent(30/49): loss=0.3824968265091488\n",
      "Gradient Descent(31/49): loss=0.38208841148289757\n",
      "Gradient Descent(32/49): loss=0.38169086479580927\n",
      "Gradient Descent(33/49): loss=0.38130359101508604\n",
      "Gradient Descent(34/49): loss=0.38092608675239364\n",
      "Gradient Descent(35/49): loss=0.38055792012798595\n",
      "Gradient Descent(36/49): loss=0.38019871509823616\n",
      "Gradient Descent(37/49): loss=0.3798481394810494\n",
      "Gradient Descent(38/49): loss=0.379505895794302\n",
      "Gradient Descent(39/49): loss=0.379171714235307\n",
      "Gradient Descent(40/49): loss=0.37884534729077507\n",
      "Gradient Descent(41/49): loss=0.3785265655892761\n",
      "Gradient Descent(42/49): loss=0.37821515470121647\n",
      "Gradient Descent(43/49): loss=0.37791091266197013\n",
      "Gradient Descent(44/49): loss=0.3776136480474466\n",
      "Gradient Descent(45/49): loss=0.37732317847213315\n",
      "Gradient Descent(46/49): loss=0.3770393294106266\n",
      "Gradient Descent(47/49): loss=0.37676193326721946\n",
      "Gradient Descent(48/49): loss=0.3764908286360182\n",
      "Gradient Descent(49/49): loss=0.3762258597077012\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4749178224654833\n",
      "Gradient Descent(2/49): loss=0.4556402248316454\n",
      "Gradient Descent(3/49): loss=0.4407838517101148\n",
      "Gradient Descent(4/49): loss=0.4292972497810827\n",
      "Gradient Descent(5/49): loss=0.4203809015914056\n",
      "Gradient Descent(6/49): loss=0.4134266167887285\n",
      "Gradient Descent(7/49): loss=0.40797156567007214\n",
      "Gradient Descent(8/49): loss=0.4036633982788362\n",
      "Gradient Descent(9/49): loss=0.40023376272230343\n",
      "Gradient Descent(10/49): loss=0.3974781916048643\n",
      "Gradient Descent(11/49): loss=0.3952408194398054\n",
      "Gradient Descent(12/49): loss=0.39340276679157365\n",
      "Gradient Descent(13/49): loss=0.3918733087108203\n",
      "Gradient Descent(14/49): loss=0.3905831582133168\n",
      "Gradient Descent(15/49): loss=0.3894793569689242\n",
      "Gradient Descent(16/49): loss=0.38852138767240035\n",
      "Gradient Descent(17/49): loss=0.38767821529947283\n",
      "Gradient Descent(18/49): loss=0.3869260347996295\n",
      "Gradient Descent(19/49): loss=0.3862465561703197\n",
      "Gradient Descent(20/49): loss=0.3856256983991683\n",
      "Gradient Descent(21/49): loss=0.38505259455594004\n",
      "Gradient Descent(22/49): loss=0.38451883371519446\n",
      "Gradient Descent(23/49): loss=0.3840178831751528\n",
      "Gradient Descent(24/49): loss=0.38354464795892546\n",
      "Gradient Descent(25/49): loss=0.3830951348655537\n",
      "Gradient Descent(26/49): loss=0.38266619615799596\n",
      "Gradient Descent(27/49): loss=0.38225533392372635\n",
      "Gradient Descent(28/49): loss=0.38186055066952923\n",
      "Gradient Descent(29/49): loss=0.38148023515614576\n",
      "Gradient Descent(30/49): loss=0.38111307509964365\n",
      "Gradient Descent(31/49): loss=0.3807579903616432\n",
      "Gradient Descent(32/49): loss=0.380414081769509\n",
      "Gradient Descent(33/49): loss=0.38008059186418275\n",
      "Gradient Descent(34/49): loss=0.37975687475406367\n",
      "Gradient Descent(35/49): loss=0.37944237292412764\n",
      "Gradient Descent(36/49): loss=0.3791365993604152\n",
      "Gradient Descent(37/49): loss=0.3788391237392927\n",
      "Gradient Descent(38/49): loss=0.37854956172747706\n",
      "Gradient Descent(39/49): loss=0.3782675666648549\n",
      "Gradient Descent(40/49): loss=0.3779928230744066\n",
      "Gradient Descent(41/49): loss=0.37772504157488873\n",
      "Gradient Descent(42/49): loss=0.37746395487208084\n",
      "Gradient Descent(43/49): loss=0.3772093145807803\n",
      "Gradient Descent(44/49): loss=0.37696088868800626\n",
      "Gradient Descent(45/49): loss=0.3767184595123319\n",
      "Gradient Descent(46/49): loss=0.37648182204821057\n",
      "Gradient Descent(47/49): loss=0.3762507826100758\n",
      "Gradient Descent(48/49): loss=0.37602515771079476\n",
      "Gradient Descent(49/49): loss=0.37580477312418276\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4702606764586826\n",
      "Gradient Descent(2/49): loss=0.4488591731934721\n",
      "Gradient Descent(3/49): loss=0.4333788450194108\n",
      "Gradient Descent(4/49): loss=0.42211050109350085\n",
      "Gradient Descent(5/49): loss=0.4138434246459872\n",
      "Gradient Descent(6/49): loss=0.4077187955152349\n",
      "Gradient Descent(7/49): loss=0.40312663209882993\n",
      "Gradient Descent(8/49): loss=0.3996331772999103\n",
      "Gradient Descent(9/49): loss=0.39692963935164877\n",
      "Gradient Descent(10/49): loss=0.39479594766022696\n",
      "Gradient Descent(11/49): loss=0.39307508807909936\n",
      "Gradient Descent(12/49): loss=0.3916549060600824\n",
      "Gradient Descent(13/49): loss=0.390455189826229\n",
      "Gradient Descent(14/49): loss=0.38941849203636747\n",
      "Gradient Descent(15/49): loss=0.38850360183539906\n",
      "Gradient Descent(16/49): loss=0.3876808980118706\n",
      "Gradient Descent(17/49): loss=0.38692903862295686\n",
      "Gradient Descent(18/49): loss=0.386232601004521\n",
      "Gradient Descent(19/49): loss=0.38558039817549483\n",
      "Gradient Descent(20/49): loss=0.38496427699904084\n",
      "Gradient Descent(21/49): loss=0.38437825970930517\n",
      "Gradient Descent(22/49): loss=0.3838179303246579\n",
      "Gradient Descent(23/49): loss=0.38327999581775984\n",
      "Gradient Descent(24/49): loss=0.3827619720672926\n",
      "Gradient Descent(25/49): loss=0.38226195895620313\n",
      "Gradient Descent(26/49): loss=0.38177847919177627\n",
      "Gradient Descent(27/49): loss=0.3813103626978362\n",
      "Gradient Descent(28/49): loss=0.38085666361603077\n",
      "Gradient Descent(29/49): loss=0.3804166006530452\n",
      "Gradient Descent(30/49): loss=0.3799895141513079\n",
      "Gradient Descent(31/49): loss=0.37957483514645746\n",
      "Gradient Descent(32/49): loss=0.379172063022019\n",
      "Gradient Descent(33/49): loss=0.3787807493346392\n",
      "Gradient Descent(34/49): loss=0.3784004860717518\n",
      "Gradient Descent(35/49): loss=0.3780308970960957\n",
      "Gradient Descent(36/49): loss=0.3776716318840019\n",
      "Gradient Descent(37/49): loss=0.37732236091674415\n",
      "Gradient Descent(38/49): loss=0.3769827722650215\n",
      "Gradient Descent(39/49): loss=0.376652569036175\n",
      "Gradient Descent(40/49): loss=0.376331467446616\n",
      "Gradient Descent(41/49): loss=0.3760191953485549\n",
      "Gradient Descent(42/49): loss=0.37571549108792435\n",
      "Gradient Descent(43/49): loss=0.3754201026047212\n",
      "Gradient Descent(44/49): loss=0.37513278671165634\n",
      "Gradient Descent(45/49): loss=0.3748533085047382\n",
      "Gradient Descent(46/49): loss=0.37458144087218115\n",
      "Gradient Descent(47/49): loss=0.3743169640772212\n",
      "Gradient Descent(48/49): loss=0.37405966539705454\n",
      "Gradient Descent(49/49): loss=0.3738093388048972\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4702313600190232\n",
      "Gradient Descent(2/49): loss=0.4488089287878819\n",
      "Gradient Descent(3/49): loss=0.43331330478154845\n",
      "Gradient Descent(4/49): loss=0.4220332856040302\n",
      "Gradient Descent(5/49): loss=0.41375675833196734\n",
      "Gradient Descent(6/49): loss=0.4076239723489979\n",
      "Gradient Descent(7/49): loss=0.40302434911466656\n",
      "Gradient Descent(8/49): loss=0.39952376407904383\n",
      "Gradient Descent(9/49): loss=0.39681321081183557\n",
      "Gradient Descent(10/49): loss=0.3946725027131634\n",
      "Gradient Descent(11/49): loss=0.392944571306599\n",
      "Gradient Descent(12/49): loss=0.3915172448005337\n",
      "Gradient Descent(13/49): loss=0.39031031531290267\n",
      "Gradient Descent(14/49): loss=0.3892663504528629\n",
      "Gradient Descent(15/49): loss=0.38834415918009213\n",
      "Gradient Descent(16/49): loss=0.38751414130109\n",
      "Gradient Descent(17/49): loss=0.386754975043715\n",
      "Gradient Descent(18/49): loss=0.386051256019546\n",
      "Gradient Descent(19/49): loss=0.3853918131878358\n",
      "Gradient Descent(20/49): loss=0.3847685069306446\n",
      "Gradient Descent(21/49): loss=0.3841753706891651\n",
      "Gradient Descent(22/49): loss=0.3836079975844755\n",
      "Gradient Descent(23/49): loss=0.38306310183457776\n",
      "Gradient Descent(24/49): loss=0.3825382049588132\n",
      "Gradient Descent(25/49): loss=0.38203141111611844\n",
      "Gradient Descent(26/49): loss=0.3815412461434038\n",
      "Gradient Descent(27/49): loss=0.38106654214087143\n",
      "Gradient Descent(28/49): loss=0.38060635464090786\n",
      "Gradient Descent(29/49): loss=0.3801599030987896\n",
      "Gradient Descent(30/49): loss=0.3797265280849769\n",
      "Gradient Descent(31/49): loss=0.3793056604447789\n",
      "Gradient Descent(32/49): loss=0.3788967990383645\n",
      "Gradient Descent(33/49): loss=0.37849949463684346\n",
      "Gradient Descent(34/49): loss=0.3781133382384486\n",
      "Gradient Descent(35/49): loss=0.3777379525611455\n",
      "Gradient Descent(36/49): loss=0.37737298582025\n",
      "Gradient Descent(37/49): loss=0.37701810715177947\n",
      "Gradient Descent(38/49): loss=0.3766730032228247\n",
      "Gradient Descent(39/49): loss=0.3763373756995831\n",
      "Gradient Descent(40/49): loss=0.3760109393364051\n",
      "Gradient Descent(41/49): loss=0.37569342051567545\n",
      "Gradient Descent(42/49): loss=0.3753845561160438\n",
      "Gradient Descent(43/49): loss=0.3750840926207392\n",
      "Gradient Descent(44/49): loss=0.3747917854022908\n",
      "Gradient Descent(45/49): loss=0.3745073981376356\n",
      "Gradient Descent(46/49): loss=0.37423070232030037\n",
      "Gradient Descent(47/49): loss=0.3739614768454856\n",
      "Gradient Descent(48/49): loss=0.37369950765047016\n",
      "Gradient Descent(49/49): loss=0.3734445873974988\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4702927894385848\n",
      "Gradient Descent(2/49): loss=0.4489287881747865\n",
      "Gradient Descent(3/49): loss=0.43348582304344735\n",
      "Gradient Descent(4/49): loss=0.42225159632929243\n",
      "Gradient Descent(5/49): loss=0.41401392297809286\n",
      "Gradient Descent(6/49): loss=0.4079135487693796\n",
      "Gradient Descent(7/49): loss=0.4033406609253345\n",
      "Gradient Descent(8/49): loss=0.3998619842595743\n",
      "Gradient Descent(9/49): loss=0.39716933792624076\n",
      "Gradient Descent(10/49): loss=0.3950432814575226\n",
      "Gradient Descent(11/49): loss=0.393327389896534\n",
      "Gradient Descent(12/49): loss=0.3919100285062979\n",
      "Gradient Descent(13/49): loss=0.3907114267189823\n",
      "Gradient Descent(14/49): loss=0.3896745014756235\n",
      "Gradient Descent(15/49): loss=0.38875833651939495\n",
      "Gradient Descent(16/49): loss=0.38793354509234473\n",
      "Gradient Descent(17/49): loss=0.38717896949397385\n",
      "Gradient Descent(18/49): loss=0.3864793303981867\n",
      "Gradient Descent(19/49): loss=0.3858235514622992\n",
      "Gradient Descent(20/49): loss=0.385203564438085\n",
      "Gradient Descent(21/49): loss=0.38461345642083006\n",
      "Gradient Descent(22/49): loss=0.3840488608745352\n",
      "Gradient Descent(23/49): loss=0.38350652245714123\n",
      "Gradient Descent(24/49): loss=0.38298398582974214\n",
      "Gradient Descent(25/49): loss=0.3824793729631745\n",
      "Gradient Descent(26/49): loss=0.3819912236478787\n",
      "Gradient Descent(27/49): loss=0.3815183811677375\n",
      "Gradient Descent(28/49): loss=0.3810599102656436\n",
      "Gradient Descent(29/49): loss=0.38061503821079173\n",
      "Gradient Descent(30/49): loss=0.3801831124032519\n",
      "Gradient Descent(31/49): loss=0.37976356982446763\n",
      "Gradient Descent(32/49): loss=0.37935591497925447\n",
      "Gradient Descent(33/49): loss=0.37895970392958495\n",
      "Gradient Descent(34/49): loss=0.3785745327025218\n",
      "Gradient Descent(35/49): loss=0.3782000288421905\n",
      "Gradient Descent(36/49): loss=0.377835845224309\n",
      "Gradient Descent(37/49): loss=0.377481655501208\n",
      "Gradient Descent(38/49): loss=0.3771371507238052\n",
      "Gradient Descent(39/49): loss=0.3768020368148366\n",
      "Gradient Descent(40/49): loss=0.37647603265925283\n",
      "Gradient Descent(41/49): loss=0.3761588686433545\n",
      "Gradient Descent(42/49): loss=0.37585028552134747\n",
      "Gradient Descent(43/49): loss=0.37555003352181077\n",
      "Gradient Descent(44/49): loss=0.37525787163086094\n",
      "Gradient Descent(45/49): loss=0.37497356700625595\n",
      "Gradient Descent(46/49): loss=0.3746968944892521\n",
      "Gradient Descent(47/49): loss=0.374427636190076\n",
      "Gradient Descent(48/49): loss=0.3741655811294063\n",
      "Gradient Descent(49/49): loss=0.3739105249229747\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.47029867135760256\n",
      "Gradient Descent(2/49): loss=0.44873005086092843\n",
      "Gradient Descent(3/49): loss=0.43300064095985114\n",
      "Gradient Descent(4/49): loss=0.4214680395382122\n",
      "Gradient Descent(5/49): loss=0.4129553605907347\n",
      "Gradient Descent(6/49): loss=0.4066187828105744\n",
      "Gradient Descent(7/49): loss=0.40185292903856795\n",
      "Gradient Descent(8/49): loss=0.3982232184578272\n",
      "Gradient Descent(9/49): loss=0.3954174704862349\n",
      "Gradient Descent(10/49): loss=0.3932112618427826\n",
      "Gradient Descent(11/49): loss=0.39144311613994504\n",
      "Gradient Descent(12/49): loss=0.3899967275196293\n",
      "Gradient Descent(13/49): loss=0.3887882190839625\n",
      "Gradient Descent(14/49): loss=0.38775700679954844\n",
      "Gradient Descent(15/49): loss=0.3868592463782481\n",
      "Gradient Descent(16/49): loss=0.38606313128342745\n",
      "Gradient Descent(17/49): loss=0.385345517803926\n",
      "Gradient Descent(18/49): loss=0.38468950178641154\n",
      "Gradient Descent(19/49): loss=0.3840826780091604\n",
      "Gradient Descent(20/49): loss=0.3835158893618751\n",
      "Gradient Descent(21/49): loss=0.38298232756596107\n",
      "Gradient Descent(22/49): loss=0.38247688627154125\n",
      "Gradient Descent(23/49): loss=0.3819956953939513\n",
      "Gradient Descent(24/49): loss=0.3815357856458172\n",
      "Gradient Descent(25/49): loss=0.3810948466300509\n",
      "Gradient Descent(26/49): loss=0.3806710521944663\n",
      "Gradient Descent(27/49): loss=0.3802629341635995\n",
      "Gradient Descent(28/49): loss=0.3798692908840556\n",
      "Gradient Descent(29/49): loss=0.37948912083853725\n",
      "Gradient Descent(30/49): loss=0.3791215743251523\n",
      "Gradient Descent(31/49): loss=0.37876591816704314\n",
      "Gradient Descent(32/49): loss=0.37842150983110595\n",
      "Gradient Descent(33/49): loss=0.3780877783501473\n",
      "Gradient Descent(34/49): loss=0.37776421017258177\n",
      "Gradient Descent(35/49): loss=0.3774503385882988\n",
      "Gradient Descent(36/49): loss=0.37714573575645877\n",
      "Gradient Descent(37/49): loss=0.376850006632239\n",
      "Gradient Descent(38/49): loss=0.3765627842847326\n",
      "Gradient Descent(39/49): loss=0.37628372623871975\n",
      "Gradient Descent(40/49): loss=0.37601251157423626\n",
      "Gradient Descent(41/49): loss=0.3757488385908172\n",
      "Gradient Descent(42/49): loss=0.3754924228959078\n",
      "Gradient Descent(43/49): loss=0.3752429958149163\n",
      "Gradient Descent(44/49): loss=0.3750003030478422\n",
      "Gradient Descent(45/49): loss=0.374764103517273\n",
      "Gradient Descent(46/49): loss=0.3745341683669426\n",
      "Gradient Descent(47/49): loss=0.3743102800804984\n",
      "Gradient Descent(48/49): loss=0.3740922316977319\n",
      "Gradient Descent(49/49): loss=0.37387982611107595\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46578179656921015\n",
      "Gradient Descent(2/49): loss=0.4425909793625242\n",
      "Gradient Descent(3/49): loss=0.42675369729334656\n",
      "Gradient Descent(4/49): loss=0.4158319322765183\n",
      "Gradient Descent(5/49): loss=0.40820422297731085\n",
      "Gradient Descent(6/49): loss=0.40279007991479654\n",
      "Gradient Descent(7/49): loss=0.3988682336244702\n",
      "Gradient Descent(8/49): loss=0.3959564206241145\n",
      "Gradient Descent(9/49): loss=0.39373167506318046\n",
      "Gradient Descent(10/49): loss=0.39197736552531093\n",
      "Gradient Descent(11/49): loss=0.39054793833661955\n",
      "Gradient Descent(12/49): loss=0.38934541024338826\n",
      "Gradient Descent(13/49): loss=0.38830367292001156\n",
      "Gradient Descent(14/49): loss=0.3873780002332395\n",
      "Gradient Descent(15/49): loss=0.38653802576767204\n",
      "Gradient Descent(16/49): loss=0.3857630380711293\n",
      "Gradient Descent(17/49): loss=0.38503882566491693\n",
      "Gradient Descent(18/49): loss=0.38435555940244\n",
      "Gradient Descent(19/49): loss=0.3837063698446359\n",
      "Gradient Descent(20/49): loss=0.38308639069856165\n",
      "Gradient Descent(21/49): loss=0.38249211504238445\n",
      "Gradient Descent(22/49): loss=0.38192096163148936\n",
      "Gradient Descent(23/49): loss=0.3813709824102055\n",
      "Gradient Descent(24/49): loss=0.3808406650052682\n",
      "Gradient Descent(25/49): loss=0.3803287991569718\n",
      "Gradient Descent(26/49): loss=0.3798343862245376\n",
      "Gradient Descent(27/49): loss=0.3793565777348597\n",
      "Gradient Descent(28/49): loss=0.37889463353257125\n",
      "Gradient Descent(29/49): loss=0.3784478931731432\n",
      "Gradient Descent(30/49): loss=0.37801575627437334\n",
      "Gradient Descent(31/49): loss=0.37759766893685254\n",
      "Gradient Descent(32/49): loss=0.37719311428333113\n",
      "Gradient Descent(33/49): loss=0.3768016057996945\n",
      "Gradient Descent(34/49): loss=0.3764226825868069\n",
      "Gradient Descent(35/49): loss=0.37605590592020627\n",
      "Gradient Descent(36/49): loss=0.3757008567088553\n",
      "Gradient Descent(37/49): loss=0.37535713357536127\n",
      "Gradient Descent(38/49): loss=0.37502435136880247\n",
      "Gradient Descent(39/49): loss=0.37470213998134944\n",
      "Gradient Descent(40/49): loss=0.37439014338057236\n",
      "Gradient Descent(41/49): loss=0.3740880187969414\n",
      "Gradient Descent(42/49): loss=0.3737954360248067\n",
      "Gradient Descent(43/49): loss=0.37351207680792947\n",
      "Gradient Descent(44/49): loss=0.3732376342893709\n",
      "Gradient Descent(45/49): loss=0.3729718125115211\n",
      "Gradient Descent(46/49): loss=0.3727143259561593\n",
      "Gradient Descent(47/49): loss=0.3724648991172723\n",
      "Gradient Descent(48/49): loss=0.3722232661013203\n",
      "Gradient Descent(49/49): loss=0.3719891702510101\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4657480808575879\n",
      "Gradient Descent(2/49): loss=0.4425346099291246\n",
      "Gradient Descent(3/49): loss=0.42668151502860246\n",
      "Gradient Descent(4/49): loss=0.4157479048831384\n",
      "Gradient Descent(5/49): loss=0.40811049242006125\n",
      "Gradient Descent(6/49): loss=0.4026876920439774\n",
      "Gradient Descent(7/49): loss=0.39875761345894695\n",
      "Gradient Descent(8/49): loss=0.3958376668478149\n",
      "Gradient Descent(9/49): loss=0.39360473452084466\n",
      "Gradient Descent(10/49): loss=0.3918421317588591\n",
      "Gradient Descent(11/49): loss=0.3904043038482578\n",
      "Gradient Descent(12/49): loss=0.389193291567123\n",
      "Gradient Descent(13/49): loss=0.38814302033637277\n",
      "Gradient Descent(14/49): loss=0.3872087992471695\n",
      "Gradient Descent(15/49): loss=0.3863602945081279\n",
      "Gradient Descent(16/49): loss=0.38557682296230095\n",
      "Gradient Descent(17/49): loss=0.38484419664147557\n",
      "Gradient Descent(18/49): loss=0.3841526053149816\n",
      "Gradient Descent(19/49): loss=0.38349519434748164\n",
      "Gradient Descent(20/49): loss=0.38286710871895846\n",
      "Gradient Descent(21/49): loss=0.38226484983014297\n",
      "Gradient Descent(22/49): loss=0.38168584234071345\n",
      "Gradient Descent(23/49): loss=0.3811281421462641\n",
      "Gradient Descent(24/49): loss=0.380590239266831\n",
      "Gradient Descent(25/49): loss=0.3800709246070449\n",
      "Gradient Descent(26/49): loss=0.3795691997317241\n",
      "Gradient Descent(27/49): loss=0.37908421563427863\n",
      "Gradient Descent(28/49): loss=0.37861523106384304\n",
      "Gradient Descent(29/49): loss=0.37816158406008504\n",
      "Gradient Descent(30/49): loss=0.37772267241735213\n",
      "Gradient Descent(31/49): loss=0.37729794019410756\n",
      "Gradient Descent(32/49): loss=0.37688686832206997\n",
      "Gradient Descent(33/49): loss=0.3764889680014858\n",
      "Gradient Descent(34/49): loss=0.37610377599485234\n",
      "Gradient Descent(35/49): loss=0.3757308512185703\n",
      "Gradient Descent(36/49): loss=0.3753697722257636\n",
      "Gradient Descent(37/49): loss=0.3750201353043243\n",
      "Gradient Descent(38/49): loss=0.37468155300264666\n",
      "Gradient Descent(39/49): loss=0.3743536529553106\n",
      "Gradient Descent(40/49): loss=0.3740360769214587\n",
      "Gradient Descent(41/49): loss=0.3737284799760627\n",
      "Gradient Descent(42/49): loss=0.37343052981290814\n",
      "Gradient Descent(43/49): loss=0.37314190613080506\n",
      "Gradient Descent(44/49): loss=0.37286230008317167\n",
      "Gradient Descent(45/49): loss=0.3725914137770439\n",
      "Gradient Descent(46/49): loss=0.3723289598116123\n",
      "Gradient Descent(47/49): loss=0.3720746608491769\n",
      "Gradient Descent(48/49): loss=0.3718282492133354\n",
      "Gradient Descent(49/49): loss=0.37158946651056396\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46581970427920194\n",
      "Gradient Descent(2/49): loss=0.44267334199244923\n",
      "Gradient Descent(3/49): loss=0.4268788438535168\n",
      "Gradient Descent(4/49): loss=0.4159941957021524\n",
      "Gradient Descent(5/49): loss=0.4083965467925483\n",
      "Gradient Descent(6/49): loss=0.40300546679429605\n",
      "Gradient Descent(7/49): loss=0.3991004450625717\n",
      "Gradient Descent(8/49): loss=0.3962002309951249\n",
      "Gradient Descent(9/49): loss=0.3939828830656185\n",
      "Gradient Descent(10/49): loss=0.39223268919849885\n",
      "Gradient Descent(11/49): loss=0.3908048669295275\n",
      "Gradient Descent(12/49): loss=0.3896020522533043\n",
      "Gradient Descent(13/49): loss=0.3885586190558583\n",
      "Gradient Descent(14/49): loss=0.38763020832298123\n",
      "Gradient Descent(15/49): loss=0.3867867284030348\n",
      "Gradient Descent(16/49): loss=0.3860076708296582\n",
      "Gradient Descent(17/49): loss=0.385278972639738\n",
      "Gradient Descent(18/49): loss=0.38459091262969364\n",
      "Gradient Descent(19/49): loss=0.3839366995353763\n",
      "Gradient Descent(20/49): loss=0.3833115236705262\n",
      "Gradient Descent(21/49): loss=0.38271191925853276\n",
      "Gradient Descent(22/49): loss=0.3821353352167028\n",
      "Gradient Descent(23/49): loss=0.38157984590887306\n",
      "Gradient Descent(24/49): loss=0.3810439559571258\n",
      "Gradient Descent(25/49): loss=0.3805264683135468\n",
      "Gradient Descent(26/49): loss=0.38002639491485357\n",
      "Gradient Descent(27/49): loss=0.37954289602817204\n",
      "Gradient Descent(28/49): loss=0.3790752389482655\n",
      "Gradient Descent(29/49): loss=0.3786227697623176\n",
      "Gradient Descent(30/49): loss=0.3781848939511077\n",
      "Gradient Descent(31/49): loss=0.3777610629752404\n",
      "Gradient Descent(32/49): loss=0.3773507649232069\n",
      "Gradient Descent(33/49): loss=0.3769535179227552\n",
      "Gradient Descent(34/49): loss=0.3765688654378225\n",
      "Gradient Descent(35/49): loss=0.376196372856916\n",
      "Gradient Descent(36/49): loss=0.3758356249701778\n",
      "Gradient Descent(37/49): loss=0.3754862240615794\n",
      "Gradient Descent(38/49): loss=0.37514778843002894\n",
      "Gradient Descent(39/49): loss=0.37481995121228806\n",
      "Gradient Descent(40/49): loss=0.37450235942065035\n",
      "Gradient Descent(41/49): loss=0.37419467313552757\n",
      "Gradient Descent(42/49): loss=0.37389656481158706\n",
      "Gradient Descent(43/49): loss=0.37360771866869\n",
      "Gradient Descent(44/49): loss=0.3733278301474976\n",
      "Gradient Descent(45/49): loss=0.3730566054155279\n",
      "Gradient Descent(46/49): loss=0.37279376091351085\n",
      "Gradient Descent(47/49): loss=0.37253902293470886\n",
      "Gradient Descent(48/49): loss=0.3722921272318247\n",
      "Gradient Descent(49/49): loss=0.37205281864749606\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46581194838272927\n",
      "Gradient Descent(2/49): loss=0.44239020530163703\n",
      "Gradient Descent(3/49): loss=0.42624247584137687\n",
      "Gradient Descent(4/49): loss=0.4150167412237039\n",
      "Gradient Descent(5/49): loss=0.40712751972080613\n",
      "Gradient Descent(6/49): loss=0.4015051270564532\n",
      "Gradient Descent(7/49): loss=0.3974272426623361\n",
      "Gradient Descent(8/49): loss=0.39440564728871225\n",
      "Gradient Descent(9/49): loss=0.39210999503470934\n",
      "Gradient Descent(10/49): loss=0.39031647339995595\n",
      "Gradient Descent(11/49): loss=0.3888732041680271\n",
      "Gradient Descent(12/49): loss=0.3876769135342858\n",
      "Gradient Descent(13/49): loss=0.3866571930677737\n",
      "Gradient Descent(14/49): loss=0.38576587655483596\n",
      "Gradient Descent(15/49): loss=0.38496986635990915\n",
      "Gradient Descent(16/49): loss=0.3842462867135604\n",
      "Gradient Descent(17/49): loss=0.38357920729796374\n",
      "Gradient Descent(18/49): loss=0.3829574269421723\n",
      "Gradient Descent(19/49): loss=0.38237297328217873\n",
      "Gradient Descent(20/49): loss=0.38182008616252794\n",
      "Gradient Descent(21/49): loss=0.3812945280265065\n",
      "Gradient Descent(22/49): loss=0.3807931154499044\n",
      "Gradient Descent(23/49): loss=0.3803134003238433\n",
      "Gradient Descent(24/49): loss=0.37985345237733453\n",
      "Gradient Descent(25/49): loss=0.37941171038388133\n",
      "Gradient Descent(26/49): loss=0.378986879968225\n",
      "Gradient Descent(27/49): loss=0.3785778630712678\n",
      "Gradient Descent(28/49): loss=0.3781837089575952\n",
      "Gradient Descent(29/49): loss=0.37780357991276947\n",
      "Gradient Descent(30/49): loss=0.37743672698411435\n",
      "Gradient Descent(31/49): loss=0.377082472611616\n",
      "Gradient Descent(32/49): loss=0.3767401980061483\n",
      "Gradient Descent(33/49): loss=0.3764093338167073\n",
      "Gradient Descent(34/49): loss=0.37608935309226094\n",
      "Gradient Descent(35/49): loss=0.37577976585851647\n",
      "Gradient Descent(36/49): loss=0.3754801148435774\n",
      "Gradient Descent(37/49): loss=0.37518997203171695\n",
      "Gradient Descent(38/49): loss=0.3749089358233876\n",
      "Gradient Descent(39/49): loss=0.37463662864703035\n",
      "Gradient Descent(40/49): loss=0.37437269491435426\n",
      "Gradient Descent(41/49): loss=0.37411679924236335\n",
      "Gradient Descent(42/49): loss=0.37386862488715333\n",
      "Gradient Descent(43/49): loss=0.37362787234952705\n",
      "Gradient Descent(44/49): loss=0.3733942581229227\n",
      "Gradient Descent(45/49): loss=0.3731675135614571\n",
      "Gradient Descent(46/49): loss=0.3729473838510451\n",
      "Gradient Descent(47/49): loss=0.37273362707023966\n",
      "Gradient Descent(48/49): loss=0.372526013330096\n",
      "Gradient Descent(49/49): loss=0.3723243239843115\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4614393949751893\n",
      "Gradient Descent(2/49): loss=0.43686834512202066\n",
      "Gradient Descent(3/49): loss=0.42104004738568784\n",
      "Gradient Descent(4/49): loss=0.4106939835952835\n",
      "Gradient Descent(5/49): loss=0.40379790992422443\n",
      "Gradient Descent(6/49): loss=0.3990818600905075\n",
      "Gradient Descent(7/49): loss=0.39575036891096355\n",
      "Gradient Descent(8/49): loss=0.3933040890140693\n",
      "Gradient Descent(9/49): loss=0.39142887158937784\n",
      "Gradient Descent(10/49): loss=0.3899266158886555\n",
      "Gradient Descent(11/49): loss=0.38867206302621216\n",
      "Gradient Descent(12/49): loss=0.3875857487499906\n",
      "Gradient Descent(13/49): loss=0.3866170436920318\n",
      "Gradient Descent(14/49): loss=0.3857335032228614\n",
      "Gradient Descent(15/49): loss=0.38491417056762384\n",
      "Gradient Descent(16/49): loss=0.38414536049131387\n",
      "Gradient Descent(17/49): loss=0.38341800152423794\n",
      "Gradient Descent(18/49): loss=0.3827259585834994\n",
      "Gradient Descent(19/49): loss=0.38206497298932424\n",
      "Gradient Descent(20/49): loss=0.38143199168560754\n",
      "Gradient Descent(21/49): loss=0.38082474206378175\n",
      "Gradient Descent(22/49): loss=0.38024146193134634\n",
      "Gradient Descent(23/49): loss=0.3796807275886848\n",
      "Gradient Descent(24/49): loss=0.37914134401866445\n",
      "Gradient Descent(25/49): loss=0.37862227445187174\n",
      "Gradient Descent(26/49): loss=0.3781225949319775\n",
      "Gradient Descent(27/49): loss=0.37764146478351746\n",
      "Gradient Descent(28/49): loss=0.377178107218419\n",
      "Gradient Descent(29/49): loss=0.37673179642547255\n",
      "Gradient Descent(30/49): loss=0.3763018488207664\n",
      "Gradient Descent(31/49): loss=0.3758876169818682\n",
      "Gradient Descent(32/49): loss=0.3754884853241246\n",
      "Gradient Descent(33/49): loss=0.3751038669173924\n",
      "Gradient Descent(34/49): loss=0.3747332010575673\n",
      "Gradient Descent(35/49): loss=0.37437595134480367\n",
      "Gradient Descent(36/49): loss=0.37403160410803143\n",
      "Gradient Descent(37/49): loss=0.3736996670714261\n",
      "Gradient Descent(38/49): loss=0.3733796681944348\n",
      "Gradient Descent(39/49): loss=0.37307115464006224\n",
      "Gradient Descent(40/49): loss=0.37277369184105413\n",
      "Gradient Descent(41/49): loss=0.37248686264330966\n",
      "Gradient Descent(42/49): loss=0.372210266512185\n",
      "Gradient Descent(43/49): loss=0.3719435187915342\n",
      "Gradient Descent(44/49): loss=0.37168625000810146\n",
      "Gradient Descent(45/49): loss=0.3714381052157569\n",
      "Gradient Descent(46/49): loss=0.3711987433753432\n",
      "Gradient Descent(47/49): loss=0.37096783676678796\n",
      "Gradient Descent(48/49): loss=0.37074507043076554\n",
      "Gradient Descent(49/49): loss=0.37053014163764736\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46140141907769394\n",
      "Gradient Descent(2/49): loss=0.436806382016321\n",
      "Gradient Descent(3/49): loss=0.4209620164385305\n",
      "Gradient Descent(4/49): loss=0.41060397383891034\n",
      "Gradient Descent(5/49): loss=0.4036978044075243\n",
      "Gradient Descent(6/49): loss=0.39897236804832\n",
      "Gradient Descent(7/49): loss=0.3956316307835286\n",
      "Gradient Descent(8/49): loss=0.39317600906744665\n",
      "Gradient Descent(9/49): loss=0.39129128910898126\n",
      "Gradient Descent(10/49): loss=0.3897793857994495\n",
      "Gradient Descent(11/49): loss=0.3885150881934054\n",
      "Gradient Descent(12/49): loss=0.38741898795974017\n",
      "Gradient Descent(13/49): loss=0.3864405083892202\n",
      "Gradient Descent(14/49): loss=0.3855472498342749\n",
      "Gradient Descent(15/49): loss=0.3847182917907775\n",
      "Gradient Descent(16/49): loss=0.38393997708994804\n",
      "Gradient Descent(17/49): loss=0.38320325522675774\n",
      "Gradient Descent(18/49): loss=0.38250200621905267\n",
      "Gradient Descent(19/49): loss=0.3818319817881525\n",
      "Gradient Descent(20/49): loss=0.38119013559074727\n",
      "Gradient Descent(21/49): loss=0.3805741988837507\n",
      "Gradient Descent(22/49): loss=0.37998241117372344\n",
      "Gradient Descent(23/49): loss=0.3794133488352683\n",
      "Gradient Descent(24/49): loss=0.378865815725904\n",
      "Gradient Descent(25/49): loss=0.3783387730818779\n",
      "Gradient Descent(26/49): loss=0.37783129433822693\n",
      "Gradient Descent(27/49): loss=0.3773425357911348\n",
      "Gradient Descent(28/49): loss=0.376871717351789\n",
      "Gradient Descent(29/49): loss=0.37641810974631057\n",
      "Gradient Descent(30/49): loss=0.3759810258480373\n",
      "Gradient Descent(31/49): loss=0.3755598146714853\n",
      "Gradient Descent(32/49): loss=0.3751538570915062\n",
      "Gradient Descent(33/49): loss=0.3747625626899811\n",
      "Gradient Descent(34/49): loss=0.37438536734755995\n",
      "Gradient Descent(35/49): loss=0.3740217313347785\n",
      "Gradient Descent(36/49): loss=0.37367113774404465\n",
      "Gradient Descent(37/49): loss=0.3733330911596042\n",
      "Gradient Descent(38/49): loss=0.37300711649819857\n",
      "Gradient Descent(39/49): loss=0.3726927579759667\n",
      "Gradient Descent(40/49): loss=0.37238957817187196\n",
      "Gradient Descent(41/49): loss=0.3720971571674634\n",
      "Gradient Descent(42/49): loss=0.3718150917489959\n",
      "Gradient Descent(43/49): loss=0.3715429946620157\n",
      "Gradient Descent(44/49): loss=0.3712804939112172\n",
      "Gradient Descent(45/49): loss=0.3710272321002007\n",
      "Gradient Descent(46/49): loss=0.3707828658069872\n",
      "Gradient Descent(47/49): loss=0.37054706499201123\n",
      "Gradient Descent(48/49): loss=0.3703195124359104\n",
      "Gradient Descent(49/49): loss=0.37009990320487285\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4614832238629201\n",
      "Gradient Descent(2/49): loss=0.4369635458385442\n",
      "Gradient Descent(3/49): loss=0.4211827088325581\n",
      "Gradient Descent(4/49): loss=0.4108755033232366\n",
      "Gradient Descent(5/49): loss=0.4040087618355295\n",
      "Gradient Descent(6/49): loss=0.399313387105225\n",
      "Gradient Descent(7/49): loss=0.3959954351219342\n",
      "Gradient Descent(8/49): loss=0.3935571464566616\n",
      "Gradient Descent(9/49): loss=0.39168577009463634\n",
      "Gradient Descent(10/49): loss=0.3901843326107617\n",
      "Gradient Descent(11/49): loss=0.38892843674920674\n",
      "Gradient Descent(12/49): loss=0.3878392533212014\n",
      "Gradient Descent(13/49): loss=0.3868666090470993\n",
      "Gradient Descent(14/49): loss=0.3859783807049589\n",
      "Gradient Descent(15/49): loss=0.38515383495933486\n",
      "Gradient Descent(16/49): loss=0.38437944053491635\n",
      "Gradient Descent(17/49): loss=0.3836462316168715\n",
      "Gradient Descent(18/49): loss=0.3829481457340933\n",
      "Gradient Descent(19/49): loss=0.3822809745193831\n",
      "Gradient Descent(20/49): loss=0.38164170035040856\n",
      "Gradient Descent(21/49): loss=0.38102807621568213\n",
      "Gradient Descent(22/49): loss=0.38043835905924167\n",
      "Gradient Descent(23/49): loss=0.37987114008692574\n",
      "Gradient Descent(24/49): loss=0.3793252364080586\n",
      "Gradient Descent(25/49): loss=0.37879962153307345\n",
      "Gradient Descent(26/49): loss=0.3782933805285037\n",
      "Gradient Descent(27/49): loss=0.37780568085148153\n",
      "Gradient Descent(28/49): loss=0.3773357531802132\n",
      "Gradient Descent(29/49): loss=0.3768828786375774\n",
      "Gradient Descent(30/49): loss=0.37644638012039167\n",
      "Gradient Descent(31/49): loss=0.3760256162793359\n",
      "Gradient Descent(32/49): loss=0.37561997722195667\n",
      "Gradient Descent(33/49): loss=0.37522888134577465\n",
      "Gradient Descent(34/49): loss=0.37485177292112254\n",
      "Gradient Descent(35/49): loss=0.37448812017866995\n",
      "Gradient Descent(36/49): loss=0.3741374137429163\n",
      "Gradient Descent(37/49): loss=0.3737991653081504\n",
      "Gradient Descent(38/49): loss=0.37347290648880727\n",
      "Gradient Descent(39/49): loss=0.3731581877989753\n",
      "Gradient Descent(40/49): loss=0.3728545777305863\n",
      "Gradient Descent(41/49): loss=0.3725616619094441\n",
      "Gradient Descent(42/49): loss=0.372279042314562\n",
      "Gradient Descent(43/49): loss=0.37200633655046095\n",
      "Gradient Descent(44/49): loss=0.37174317716487365\n",
      "Gradient Descent(45/49): loss=0.3714892110062019\n",
      "Gradient Descent(46/49): loss=0.3712440986163711\n",
      "Gradient Descent(47/49): loss=0.371007513655645\n",
      "Gradient Descent(48/49): loss=0.3707791423566108\n",
      "Gradient Descent(49/49): loss=0.3705586830050186\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.46145765354086354\n",
      "Gradient Descent(2/49): loss=0.4365869399988366\n",
      "Gradient Descent(3/49): loss=0.4203920758457389\n",
      "Gradient Descent(4/49): loss=0.40971472717224344\n",
      "Gradient Descent(5/49): loss=0.4025557279824045\n",
      "Gradient Descent(6/49): loss=0.39764813412091593\n",
      "Gradient Descent(7/49): loss=0.39418803094505\n",
      "Gradient Descent(8/49): loss=0.39166458180128166\n",
      "Gradient Descent(9/49): loss=0.38975261729547045\n",
      "Gradient Descent(10/49): loss=0.3882446913195845\n",
      "Gradient Descent(11/49): loss=0.38700806981425423\n",
      "Gradient Descent(12/49): loss=0.3859574843161031\n",
      "Gradient Descent(13/49): loss=0.3850378606976007\n",
      "Gradient Descent(14/49): loss=0.3842133636735867\n",
      "Gradient Descent(15/49): loss=0.3834604423750453\n",
      "Gradient Descent(16/49): loss=0.38276341197699665\n",
      "Gradient Descent(17/49): loss=0.3821116436562607\n",
      "Gradient Descent(18/49): loss=0.3814977751119905\n",
      "Gradient Descent(19/49): loss=0.3809165690941813\n",
      "Gradient Descent(20/49): loss=0.3803641836896522\n",
      "Gradient Descent(21/49): loss=0.37983770447942933\n",
      "Gradient Descent(22/49): loss=0.379334843425299\n",
      "Gradient Descent(23/49): loss=0.3788537440577275\n",
      "Gradient Descent(24/49): loss=0.3783928545597752\n",
      "Gradient Descent(25/49): loss=0.3779508443186199\n",
      "Gradient Descent(26/49): loss=0.37752654839143035\n",
      "Gradient Descent(27/49): loss=0.3771189299709944\n",
      "Gradient Descent(28/49): loss=0.3767270545212191\n",
      "Gradient Descent(29/49): loss=0.3763500715332824\n",
      "Gradient Descent(30/49): loss=0.3759872013055263\n",
      "Gradient Descent(31/49): loss=0.37563772507602067\n",
      "Gradient Descent(32/49): loss=0.37530097742776036\n",
      "Gradient Descent(33/49): loss=0.3749763402644035\n",
      "Gradient Descent(34/49): loss=0.37466323789667483\n",
      "Gradient Descent(35/49): loss=0.37436113293522516\n",
      "Gradient Descent(36/49): loss=0.37406952278615496\n",
      "Gradient Descent(37/49): loss=0.37378793661048415\n",
      "Gradient Descent(38/49): loss=0.373515932651293\n",
      "Gradient Descent(39/49): loss=0.37325309586015887\n",
      "Gradient Descent(40/49): loss=0.3729990357730464\n",
      "Gradient Descent(41/49): loss=0.37275338459827956\n",
      "Gradient Descent(42/49): loss=0.3725157954877593\n",
      "Gradient Descent(43/49): loss=0.37228594096853496\n",
      "Gradient Descent(44/49): loss=0.3720635115160968\n",
      "Gradient Descent(45/49): loss=0.37184821425386116\n",
      "Gradient Descent(46/49): loss=0.3716397717656669\n",
      "Gradient Descent(47/49): loss=0.371437921009916\n",
      "Gradient Descent(48/49): loss=0.37124241232543326\n",
      "Gradient Descent(49/49): loss=0.37105300852029854\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45723347167662015\n",
      "Gradient Descent(2/49): loss=0.43165679754319175\n",
      "Gradient Descent(3/49): loss=0.4161271340591979\n",
      "Gradient Descent(4/49): loss=0.406496628173678\n",
      "Gradient Descent(5/49): loss=0.400347111361927\n",
      "Gradient Descent(6/49): loss=0.3962640576948828\n",
      "Gradient Descent(7/49): loss=0.3934175253070039\n",
      "Gradient Descent(8/49): loss=0.3913191436687106\n",
      "Gradient Descent(9/49): loss=0.38968065338157426\n",
      "Gradient Descent(10/49): loss=0.3883313038985096\n",
      "Gradient Descent(11/49): loss=0.387169494620935\n",
      "Gradient Descent(12/49): loss=0.38613440099268137\n",
      "Gradient Descent(13/49): loss=0.38518929441829597\n",
      "Gradient Descent(14/49): loss=0.38431171956194954\n",
      "Gradient Descent(15/49): loss=0.38348770045417\n",
      "Gradient Descent(16/49): loss=0.38270831749986634\n",
      "Gradient Descent(17/49): loss=0.38196768181922003\n",
      "Gradient Descent(18/49): loss=0.3812617342813023\n",
      "Gradient Descent(19/49): loss=0.3805875319217027\n",
      "Gradient Descent(20/49): loss=0.3799428227925047\n",
      "Gradient Descent(21/49): loss=0.3793257917533944\n",
      "Gradient Descent(22/49): loss=0.3787349077361323\n",
      "Gradient Descent(23/49): loss=0.37816883135951296\n",
      "Gradient Descent(24/49): loss=0.37762635852068016\n",
      "Gradient Descent(25/49): loss=0.3771063854960879\n",
      "Gradient Descent(26/49): loss=0.37660788695234987\n",
      "Gradient Descent(27/49): loss=0.37612990174544325\n",
      "Gradient Descent(28/49): loss=0.3756715234513358\n",
      "Gradient Descent(29/49): loss=0.37523189379829935\n",
      "Gradient Descent(30/49): loss=0.3748101979017882\n",
      "Gradient Descent(31/49): loss=0.3744056606385715\n",
      "Gradient Descent(32/49): loss=0.37401754375737556\n",
      "Gradient Descent(33/49): loss=0.37364514347954336\n",
      "Gradient Descent(34/49): loss=0.3732877884372626\n",
      "Gradient Descent(35/49): loss=0.37294483785378113\n",
      "Gradient Descent(36/49): loss=0.37261567990462235\n",
      "Gradient Descent(37/49): loss=0.3722997302200261\n",
      "Gradient Descent(38/49): loss=0.37199643050196246\n",
      "Gradient Descent(39/49): loss=0.3717052472373018\n",
      "Gradient Descent(40/49): loss=0.3714256704939504\n",
      "Gradient Descent(41/49): loss=0.3711572127901635\n",
      "Gradient Descent(42/49): loss=0.3708994080294915\n",
      "Gradient Descent(43/49): loss=0.37065181049534635\n",
      "Gradient Descent(44/49): loss=0.3704139939002464\n",
      "Gradient Descent(45/49): loss=0.3701855504855674\n",
      "Gradient Descent(46/49): loss=0.36996609016820114\n",
      "Gradient Descent(47/49): loss=0.3697552397309645\n",
      "Gradient Descent(48/49): loss=0.36955264205394633\n",
      "Gradient Descent(49/49): loss=0.3693579553842613\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.457191374679341\n",
      "Gradient Descent(2/49): loss=0.4315897324797413\n",
      "Gradient Descent(3/49): loss=0.4160439160628807\n",
      "Gradient Descent(4/49): loss=0.4064012394850032\n",
      "Gradient Descent(5/49): loss=0.4002410414653499\n",
      "Gradient Descent(6/49): loss=0.39614764677075065\n",
      "Gradient Descent(7/49): loss=0.3932906637781443\n",
      "Gradient Descent(8/49): loss=0.3911816062999798\n",
      "Gradient Descent(9/49): loss=0.38953224218006033\n",
      "Gradient Descent(10/49): loss=0.3881718976109477\n",
      "Gradient Descent(11/49): loss=0.3869990560402835\n",
      "Gradient Descent(12/49): loss=0.3859529674226283\n",
      "Gradient Descent(13/49): loss=0.3849969631239839\n",
      "Gradient Descent(14/49): loss=0.3841086332958759\n",
      "Gradient Descent(15/49): loss=0.38327403494000983\n",
      "Gradient Descent(16/49): loss=0.3824842713171563\n",
      "Gradient Descent(17/49): loss=0.3817334685419157\n",
      "Gradient Descent(18/49): loss=0.38101757652478035\n",
      "Gradient Descent(19/49): loss=0.3803336569239153\n",
      "Gradient Descent(20/49): loss=0.37967945919193424\n",
      "Gradient Descent(21/49): loss=0.3790531672827681\n",
      "Gradient Descent(22/49): loss=0.37845324760714955\n",
      "Gradient Descent(23/49): loss=0.3778783571630853\n",
      "Gradient Descent(24/49): loss=0.3773272875076975\n",
      "Gradient Descent(25/49): loss=0.37679893013598287\n",
      "Gradient Descent(26/49): loss=0.37629225469193905\n",
      "Gradient Descent(27/49): loss=0.3758062949099694\n",
      "Gradient Descent(28/49): loss=0.37534013924451676\n",
      "Gradient Descent(29/49): loss=0.37489292436950306\n",
      "Gradient Descent(30/49): loss=0.3744638304570042\n",
      "Gradient Descent(31/49): loss=0.37405207757827974\n",
      "Gradient Descent(32/49): loss=0.37365692282922514\n",
      "Gradient Descent(33/49): loss=0.37327765793733325\n",
      "Gradient Descent(34/49): loss=0.3729136072003631\n",
      "Gradient Descent(35/49): loss=0.3725641256630768\n",
      "Gradient Descent(36/49): loss=0.3722285974724739\n",
      "Gradient Descent(37/49): loss=0.37190643437276383\n",
      "Gradient Descent(38/49): loss=0.3715970743141533\n",
      "Gradient Descent(39/49): loss=0.37129998015753196\n",
      "Gradient Descent(40/49): loss=0.37101463846221705\n",
      "Gradient Descent(41/49): loss=0.3707405583471918\n",
      "Gradient Descent(42/49): loss=0.3704772704184372\n",
      "Gradient Descent(43/49): loss=0.37022432575642594\n",
      "Gradient Descent(44/49): loss=0.3699812949588738\n",
      "Gradient Descent(45/49): loss=0.36974776723458114\n",
      "Gradient Descent(46/49): loss=0.3695233495447593\n",
      "Gradient Descent(47/49): loss=0.36930766578865015\n",
      "Gradient Descent(48/49): loss=0.36910035603059976\n",
      "Gradient Descent(49/49): loss=0.3689010757660097\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4572833481897392\n",
      "Gradient Descent(2/49): loss=0.4317648466555245\n",
      "Gradient Descent(3/49): loss=0.41628646926954743\n",
      "Gradient Descent(4/49): loss=0.40669532346912596\n",
      "Gradient Descent(5/49): loss=0.40057323484205043\n",
      "Gradient Descent(6/49): loss=0.39650763591889937\n",
      "Gradient Descent(7/49): loss=0.3936709533603708\n",
      "Gradient Descent(8/49): loss=0.3915769355563944\n",
      "Gradient Descent(9/49): loss=0.3899389920192869\n",
      "Gradient Descent(10/49): loss=0.3885875960166811\n",
      "Gradient Descent(11/49): loss=0.387422004228419\n",
      "Gradient Descent(12/49): loss=0.3863819740685607\n",
      "Gradient Descent(13/49): loss=0.3854311634431806\n",
      "Gradient Descent(14/49): loss=0.38454736998484496\n",
      "Gradient Descent(15/49): loss=0.3837167820373014\n",
      "Gradient Descent(16/49): loss=0.382930586761238\n",
      "Gradient Descent(17/49): loss=0.38218296534041296\n",
      "Gradient Descent(18/49): loss=0.38146990566699523\n",
      "Gradient Descent(19/49): loss=0.3807884975047165\n",
      "Gradient Descent(20/49): loss=0.38013651283608746\n",
      "Gradient Descent(21/49): loss=0.3795121550464675\n",
      "Gradient Descent(22/49): loss=0.3789139082439374\n",
      "Gradient Descent(23/49): loss=0.378340446094495\n",
      "Gradient Descent(24/49): loss=0.37779057612152256\n",
      "Gradient Descent(25/49): loss=0.37726320520713424\n",
      "Gradient Descent(26/49): loss=0.3767573178228002\n",
      "Gradient Descent(27/49): loss=0.376271961945523\n",
      "Gradient Descent(28/49): loss=0.37580623964935295\n",
      "Gradient Descent(29/49): loss=0.3753593005699223\n",
      "Gradient Descent(30/49): loss=0.37493033715846924\n",
      "Gradient Descent(31/49): loss=0.37451858107053576\n",
      "Gradient Descent(32/49): loss=0.37412330029090257\n",
      "Gradient Descent(33/49): loss=0.3737437967501816\n",
      "Gradient Descent(34/49): loss=0.3733794042812105\n",
      "Gradient Descent(35/49): loss=0.3730294868195838\n",
      "Gradient Descent(36/49): loss=0.3726934367869331\n",
      "Gradient Descent(37/49): loss=0.3723706736166749\n",
      "Gradient Descent(38/49): loss=0.372060642395071\n",
      "Gradient Descent(39/49): loss=0.371762812598728\n",
      "Gradient Descent(40/49): loss=0.3714766769149701\n",
      "Gradient Descent(41/49): loss=0.371201750134991\n",
      "Gradient Descent(42/49): loss=0.3709375681120089\n",
      "Gradient Descent(43/49): loss=0.3706836867782455\n",
      "Gradient Descent(44/49): loss=0.37043968121566295\n",
      "Gradient Descent(45/49): loss=0.3702051447762141\n",
      "Gradient Descent(46/49): loss=0.36997968824796407\n",
      "Gradient Descent(47/49): loss=0.3697629390639088\n",
      "Gradient Descent(48/49): loss=0.3695545405506791\n",
      "Gradient Descent(49/49): loss=0.36935415121461956\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45723578683200544\n",
      "Gradient Descent(2/49): loss=0.431287546707888\n",
      "Gradient Descent(3/49): loss=0.41534263933461935\n",
      "Gradient Descent(4/49): loss=0.40536630147782404\n",
      "Gradient Descent(5/49): loss=0.3989650153771168\n",
      "Gradient Descent(6/49): loss=0.39471659436897794\n",
      "Gradient Descent(7/49): loss=0.3917745010530184\n",
      "Gradient Descent(8/49): loss=0.3896337601530617\n",
      "Gradient Descent(9/49): loss=0.38799229519379375\n",
      "Gradient Descent(10/49): loss=0.3866687041859094\n",
      "Gradient Descent(11/49): loss=0.38555345755124526\n",
      "Gradient Descent(12/49): loss=0.3845799075656347\n",
      "Gradient Descent(13/49): loss=0.38370705104020897\n",
      "Gradient Descent(14/49): loss=0.382909269780147\n",
      "Gradient Descent(15/49): loss=0.3821702166104993\n",
      "Gradient Descent(16/49): loss=0.38147916612761784\n",
      "Gradient Descent(17/49): loss=0.38082883202823703\n",
      "Gradient Descent(18/49): loss=0.3802140579215828\n",
      "Gradient Descent(19/49): loss=0.3796310289905546\n",
      "Gradient Descent(20/49): loss=0.37907679469418853\n",
      "Gradient Descent(21/49): loss=0.37854897758310063\n",
      "Gradient Descent(22/49): loss=0.3780455937704446\n",
      "Gradient Descent(23/49): loss=0.3775649406300925\n",
      "Gradient Descent(24/49): loss=0.37710552517290125\n",
      "Gradient Descent(25/49): loss=0.37666601720565884\n",
      "Gradient Descent(26/49): loss=0.3762452177317326\n",
      "Gradient Descent(27/49): loss=0.3758420368470708\n",
      "Gradient Descent(28/49): loss=0.37545547765459586\n",
      "Gradient Descent(29/49): loss=0.37508462407984267\n",
      "Gradient Descent(30/49): loss=0.3747286312875236\n",
      "Gradient Descent(31/49): loss=0.3743867178909887\n",
      "Gradient Descent(32/49): loss=0.37405815944454224\n",
      "Gradient Descent(33/49): loss=0.3737422828900489\n",
      "Gradient Descent(34/49): loss=0.3734384617406501\n",
      "Gradient Descent(35/49): loss=0.373146111853535\n",
      "Gradient Descent(36/49): loss=0.3728646876872212\n",
      "Gradient Descent(37/49): loss=0.37259367896670337\n",
      "Gradient Descent(38/49): loss=0.37233260769815296\n",
      "Gradient Descent(39/49): loss=0.3720810254872348\n",
      "Gradient Descent(40/49): loss=0.371838511123756\n",
      "Gradient Descent(41/49): loss=0.3716046684016194\n",
      "Gradient Descent(42/49): loss=0.37137912414775925\n",
      "Gradient Descent(43/49): loss=0.37116152643738765\n",
      "Gradient Descent(44/49): loss=0.370951542975809\n",
      "Gradient Descent(45/49): loss=0.3707488596294736\n",
      "Gradient Descent(46/49): loss=0.37055317909096336\n",
      "Gradient Descent(47/49): loss=0.3703642196643337\n",
      "Gradient Descent(48/49): loss=0.37018171415873685\n",
      "Gradient Descent(49/49): loss=0.3700054088795505\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4531640266735023\n",
      "Gradient Descent(2/49): loss=0.42692300995449284\n",
      "Gradient Descent(3/49): loss=0.41191479369435563\n",
      "Gradient Descent(4/49): loss=0.40307049763789526\n",
      "Gradient Descent(5/49): loss=0.3976317538674551\n",
      "Gradient Descent(6/49): loss=0.3940911051522525\n",
      "Gradient Descent(7/49): loss=0.3916215330252874\n",
      "Gradient Descent(8/49): loss=0.38976755717407785\n",
      "Gradient Descent(9/49): loss=0.38827721529953585\n",
      "Gradient Descent(10/49): loss=0.3870103411043571\n",
      "Gradient Descent(11/49): loss=0.38588834420369894\n",
      "Gradient Descent(12/49): loss=0.38486664372631024\n",
      "Gradient Descent(13/49): loss=0.3839195028093475\n",
      "Gradient Descent(14/49): loss=0.3830316678605299\n",
      "Gradient Descent(15/49): loss=0.3821937496281797\n",
      "Gradient Descent(16/49): loss=0.381399665693322\n",
      "Gradient Descent(17/49): loss=0.38064522064457607\n",
      "Gradient Descent(18/49): loss=0.37992731523454476\n",
      "Gradient Descent(19/49): loss=0.3792435039195743\n",
      "Gradient Descent(20/49): loss=0.37859174576468296\n",
      "Gradient Descent(21/49): loss=0.3779702629394855\n",
      "Gradient Descent(22/49): loss=0.37737745926693217\n",
      "Gradient Descent(23/49): loss=0.3768118724299343\n",
      "Gradient Descent(24/49): loss=0.37627214514909557\n",
      "Gradient Descent(25/49): loss=0.3757570071380201\n",
      "Gradient Descent(26/49): loss=0.37526526324987225\n",
      "Gradient Descent(27/49): loss=0.37479578523673246\n",
      "Gradient Descent(28/49): loss=0.3743475056636417\n",
      "Gradient Descent(29/49): loss=0.373919413146228\n",
      "Gradient Descent(30/49): loss=0.3735105484330842\n",
      "Gradient Descent(31/49): loss=0.37312000105298776\n",
      "Gradient Descent(32/49): loss=0.37274690636012503\n",
      "Gradient Descent(33/49): loss=0.3723904428753268\n",
      "Gradient Descent(34/49): loss=0.3720498298589292\n",
      "Gradient Descent(35/49): loss=0.37172432507302045\n",
      "Gradient Descent(36/49): loss=0.3714132227041216\n",
      "Gradient Descent(37/49): loss=0.37111585142551656\n",
      "Gradient Descent(38/49): loss=0.3708315725836108\n",
      "Gradient Descent(39/49): loss=0.37055977849607646\n",
      "Gradient Descent(40/49): loss=0.37029989085183374\n",
      "Gradient Descent(41/49): loss=0.3700513592045307\n",
      "Gradient Descent(42/49): loss=0.3698136595523725\n",
      "Gradient Descent(43/49): loss=0.3695862929980441\n",
      "Gradient Descent(44/49): loss=0.36936878448318405\n",
      "Gradient Descent(45/49): loss=0.36916068159242904\n",
      "Gradient Descent(46/49): loss=0.3689615534225281\n",
      "Gradient Descent(47/49): loss=0.36877098951241954\n",
      "Gradient Descent(48/49): loss=0.3685885988305073\n",
      "Gradient Descent(49/49): loss=0.36841400881567343\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45311794766252944\n",
      "Gradient Descent(2/49): loss=0.4268512959265571\n",
      "Gradient Descent(3/49): loss=0.41182693115533586\n",
      "Gradient Descent(4/49): loss=0.4029701486887495\n",
      "Gradient Descent(5/49): loss=0.39751992932773866\n",
      "Gradient Descent(6/49): loss=0.393967792661879\n",
      "Gradient Descent(7/49): loss=0.39148643776903774\n",
      "Gradient Descent(8/49): loss=0.38962039887578914\n",
      "Gradient Descent(9/49): loss=0.3881178234846179\n",
      "Gradient Descent(10/49): loss=0.3868386671809588\n",
      "Gradient Descent(11/49): loss=0.3857044441296978\n",
      "Gradient Descent(12/49): loss=0.38467065383685606\n",
      "Gradient Descent(13/49): loss=0.38371161728681324\n",
      "Gradient Descent(14/49): loss=0.38281212034625023\n",
      "Gradient Descent(15/49): loss=0.38196279913927816\n",
      "Gradient Descent(16/49): loss=0.38115758622910373\n",
      "Gradient Descent(17/49): loss=0.3803922937114406\n",
      "Gradient Descent(18/49): loss=0.37966382458628023\n",
      "Gradient Descent(19/49): loss=0.37896973193867395\n",
      "Gradient Descent(20/49): loss=0.3783079710390095\n",
      "Gradient Descent(21/49): loss=0.3776767586979039\n",
      "Gradient Descent(22/49): loss=0.3770744924242722\n",
      "Gradient Descent(23/49): loss=0.37649970305816566\n",
      "Gradient Descent(24/49): loss=0.3759510262416564\n",
      "Gradient Descent(25/49): loss=0.3754271845714544\n",
      "Gradient Descent(26/49): loss=0.37492697587436086\n",
      "Gradient Descent(27/49): loss=0.3744492650471857\n",
      "Gradient Descent(28/49): loss=0.37399297801762105\n",
      "Gradient Descent(29/49): loss=0.37355709700552336\n",
      "Gradient Descent(30/49): loss=0.3731406566133452\n",
      "Gradient Descent(31/49): loss=0.3727427404712034\n",
      "Gradient Descent(32/49): loss=0.37236247827355295\n",
      "Gradient Descent(33/49): loss=0.3719990431081282\n",
      "Gradient Descent(34/49): loss=0.3716516490145909\n",
      "Gradient Descent(35/49): loss=0.37131954873187817\n",
      "Gradient Descent(36/49): loss=0.3710020316061041\n",
      "Gradient Descent(37/49): loss=0.3706984216387425\n",
      "Gradient Descent(38/49): loss=0.37040807565976847\n",
      "Gradient Descent(39/49): loss=0.3701303816136759\n",
      "Gradient Descent(40/49): loss=0.36986475694847826\n",
      "Gradient Descent(41/49): loss=0.36961064709935126\n",
      "Gradient Descent(42/49): loss=0.36936752405972273\n",
      "Gradient Descent(43/49): loss=0.36913488503348396\n",
      "Gradient Descent(44/49): loss=0.36891225116269455\n",
      "Gradient Descent(45/49): loss=0.36869916632571437\n",
      "Gradient Descent(46/49): loss=0.3684951960011693\n",
      "Gradient Descent(47/49): loss=0.3682999261935586\n",
      "Gradient Descent(48/49): loss=0.3681129624166581\n",
      "Gradient Descent(49/49): loss=0.36793392873118014\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4532200772596591\n",
      "Gradient Descent(2/49): loss=0.4270438414883151\n",
      "Gradient Descent(3/49): loss=0.4120898106235744\n",
      "Gradient Descent(4/49): loss=0.4032842122293585\n",
      "Gradient Descent(5/49): loss=0.3978700645161344\n",
      "Gradient Descent(6/49): loss=0.3943431606448086\n",
      "Gradient Descent(7/49): loss=0.39187968387619637\n",
      "Gradient Descent(8/49): loss=0.39002668725465084\n",
      "Gradient Descent(9/49): loss=0.388534013723694\n",
      "Gradient Descent(10/49): loss=0.3872627063571679\n",
      "Gradient Descent(11/49): loss=0.3861349530542075\n",
      "Gradient Descent(12/49): loss=0.3851066605322129\n",
      "Gradient Descent(13/49): loss=0.3841523923742607\n",
      "Gradient Descent(14/49): loss=0.3832570789425834\n",
      "Gradient Descent(15/49): loss=0.3824114443004626\n",
      "Gradient Descent(16/49): loss=0.38160947740929224\n",
      "Gradient Descent(17/49): loss=0.38084702976814155\n",
      "Gradient Descent(18/49): loss=0.3801210349355864\n",
      "Gradient Descent(19/49): loss=0.37942907207789994\n",
      "Gradient Descent(20/49): loss=0.378769120272654\n",
      "Gradient Descent(21/49): loss=0.3781394188753583\n",
      "Gradient Descent(22/49): loss=0.37753838706527376\n",
      "Gradient Descent(23/49): loss=0.37696457656303606\n",
      "Gradient Descent(24/49): loss=0.37641664305816597\n",
      "Gradient Descent(25/49): loss=0.3758933282804782\n",
      "Gradient Descent(26/49): loss=0.3753934481994223\n",
      "Gradient Descent(27/49): loss=0.37491588481031646\n",
      "Gradient Descent(28/49): loss=0.37445958006817387\n",
      "Gradient Descent(29/49): loss=0.3740235311466089\n",
      "Gradient Descent(30/49): loss=0.373606786546181\n",
      "Gradient Descent(31/49): loss=0.37320844277274373\n",
      "Gradient Descent(32/49): loss=0.372827641418222\n",
      "Gradient Descent(33/49): loss=0.37246356654064283\n",
      "Gradient Descent(34/49): loss=0.3721154422778073\n",
      "Gradient Descent(35/49): loss=0.371782530651266\n",
      "Gradient Descent(36/49): loss=0.371464129530747\n",
      "Gradient Descent(37/49): loss=0.37115957073754874\n",
      "Gradient Descent(38/49): loss=0.37086821827076094\n",
      "Gradient Descent(39/49): loss=0.37058946664370895\n",
      "Gradient Descent(40/49): loss=0.3703227393204294\n",
      "Gradient Descent(41/49): loss=0.3700674872437008\n",
      "Gradient Descent(42/49): loss=0.3698231874474114\n",
      "Gradient Descent(43/49): loss=0.36958934174699515\n",
      "Gradient Descent(44/49): loss=0.3693654755024233\n",
      "Gradient Descent(45/49): loss=0.3691511364488244\n",
      "Gradient Descent(46/49): loss=0.36894589359030794\n",
      "Gradient Descent(47/49): loss=0.3687493361529708\n",
      "Gradient Descent(48/49): loss=0.36856107259341725\n",
      "Gradient Descent(49/49): loss=0.3683807296594259\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.45314634825615474\n",
      "Gradient Descent(2/49): loss=0.42646035709444763\n",
      "Gradient Descent(3/49): loss=0.41099719528471784\n",
      "Gradient Descent(4/49): loss=0.40180476610785093\n",
      "Gradient Descent(5/49): loss=0.3961355442677361\n",
      "Gradient Descent(6/49): loss=0.39246185490795843\n",
      "Gradient Descent(7/49): loss=0.38993215650654856\n",
      "Gradient Descent(8/49): loss=0.38807024098368853\n",
      "Gradient Descent(9/49): loss=0.38660857324315007\n",
      "Gradient Descent(10/49): loss=0.3853958368230014\n",
      "Gradient Descent(11/49): loss=0.3843455865632414\n",
      "Gradient Descent(12/49): loss=0.38340769780334727\n",
      "Gradient Descent(13/49): loss=0.38255246984350477\n",
      "Gradient Descent(14/49): loss=0.38176176017922686\n",
      "Gradient Descent(15/49): loss=0.3810240288473126\n",
      "Gradient Descent(16/49): loss=0.38033155976505395\n",
      "Gradient Descent(17/49): loss=0.37967889578719605\n",
      "Gradient Descent(18/49): loss=0.37906195163352463\n",
      "Gradient Descent(19/49): loss=0.37847750631743077\n",
      "Gradient Descent(20/49): loss=0.37792290874139806\n",
      "Gradient Descent(21/49): loss=0.3773959035936618\n",
      "Gradient Descent(22/49): loss=0.3768945255970024\n",
      "Gradient Descent(23/49): loss=0.3764170329729484\n",
      "Gradient Descent(24/49): loss=0.3759618637201402\n",
      "Gradient Descent(25/49): loss=0.3755276054274726\n",
      "Gradient Descent(26/49): loss=0.3751129733340646\n",
      "Gradient Descent(27/49): loss=0.37471679359167437\n",
      "Gradient Descent(28/49): loss=0.3743379899513388\n",
      "Gradient Descent(29/49): loss=0.37397557281454946\n",
      "Gradient Descent(30/49): loss=0.37362863000021035\n",
      "Gradient Descent(31/49): loss=0.3732963188162\n",
      "Gradient Descent(32/49): loss=0.3729778591638075\n",
      "Gradient Descent(33/49): loss=0.3726725274868641\n",
      "Gradient Descent(34/49): loss=0.37237965142884855\n",
      "Gradient Descent(35/49): loss=0.37209860509404385\n",
      "Gradient Descent(36/49): loss=0.37182880483061614\n",
      "Gradient Descent(37/49): loss=0.3715697054686355\n",
      "Gradient Descent(38/49): loss=0.37132079695709924\n",
      "Gradient Descent(39/49): loss=0.3710816013524089\n",
      "Gradient Descent(40/49): loss=0.3708516701173808\n",
      "Gradient Descent(41/49): loss=0.3706305816952692\n",
      "Gradient Descent(42/49): loss=0.37041793932777234\n",
      "Gradient Descent(43/49): loss=0.37021336908980434\n",
      "Gradient Descent(44/49): loss=0.37001651811708025\n",
      "Gradient Descent(45/49): loss=0.36982705300539703\n",
      "Gradient Descent(46/49): loss=0.3696446583629482\n",
      "Gradient Descent(47/49): loss=0.3694690354991705\n",
      "Gradient Descent(48/49): loss=0.36929990123551104\n",
      "Gradient Descent(49/49): loss=0.36913698682516183\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4492310599658361\n",
      "Gradient Descent(2/49): loss=0.42263480194160535\n",
      "Gradient Descent(3/49): loss=0.40831274926683575\n",
      "Gradient Descent(4/49): loss=0.400272946630821\n",
      "Gradient Descent(5/49): loss=0.395478480591483\n",
      "Gradient Descent(6/49): loss=0.39238193477279004\n",
      "Gradient Descent(7/49): loss=0.39019096468558434\n",
      "Gradient Descent(8/49): loss=0.3884977260826962\n",
      "Gradient Descent(9/49): loss=0.3870909139759841\n",
      "Gradient Descent(10/49): loss=0.38586003209347075\n",
      "Gradient Descent(11/49): loss=0.38474647165091264\n",
      "Gradient Descent(12/49): loss=0.3837184383505357\n",
      "Gradient Descent(13/49): loss=0.38275806758984887\n",
      "Gradient Descent(14/49): loss=0.38185478533767714\n",
      "Gradient Descent(15/49): loss=0.38100187710036365\n",
      "Gradient Descent(16/49): loss=0.3801947084142401\n",
      "Gradient Descent(17/49): loss=0.37942979747102173\n",
      "Gradient Descent(18/49): loss=0.3787043285062771\n",
      "Gradient Descent(19/49): loss=0.37801589384204576\n",
      "Gradient Descent(20/49): loss=0.3773623549934855\n",
      "Gradient Descent(21/49): loss=0.3767417660893518\n",
      "Gradient Descent(22/49): loss=0.3761523301397039\n",
      "Gradient Descent(23/49): loss=0.3755923727990242\n",
      "Gradient Descent(24/49): loss=0.37506032559096675\n",
      "Gradient Descent(25/49): loss=0.3745547143650531\n",
      "Gradient Descent(26/49): loss=0.37407415073965494\n",
      "Gradient Descent(27/49): loss=0.3736173253248345\n",
      "Gradient Descent(28/49): loss=0.37318300206611293\n",
      "Gradient Descent(29/49): loss=0.3727700133409341\n",
      "Gradient Descent(30/49): loss=0.3723772555955872\n",
      "Gradient Descent(31/49): loss=0.3720036853952711\n",
      "Gradient Descent(32/49): loss=0.37164831580710755\n",
      "Gradient Descent(33/49): loss=0.37131021306272866\n",
      "Gradient Descent(34/49): loss=0.370988493462809\n",
      "Gradient Descent(35/49): loss=0.37068232049552907\n",
      "Gradient Descent(36/49): loss=0.3703909021470819\n",
      "Gradient Descent(37/49): loss=0.3701134883864333\n",
      "Gradient Descent(38/49): loss=0.3698493688094177\n",
      "Gradient Descent(39/49): loss=0.3695978704293538\n",
      "Gradient Descent(40/49): loss=0.3693583556029721\n",
      "Gradient Descent(41/49): loss=0.3691302200817139\n",
      "Gradient Descent(42/49): loss=0.36891289117949466\n",
      "Gradient Descent(43/49): loss=0.3687058260488785\n",
      "Gradient Descent(44/49): loss=0.3685085100583471\n",
      "Gradient Descent(45/49): loss=0.3683204552639652\n",
      "Gradient Descent(46/49): loss=0.36814119896930375\n",
      "Gradient Descent(47/49): loss=0.36797030236796213\n",
      "Gradient Descent(48/49): loss=0.3678073492634623\n",
      "Gradient Descent(49/49): loss=0.36765194486167985\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44918113802725884\n",
      "Gradient Descent(2/49): loss=0.42255885414084277\n",
      "Gradient Descent(3/49): loss=0.40822067780813803\n",
      "Gradient Descent(4/49): loss=0.40016790822086856\n",
      "Gradient Descent(5/49): loss=0.3953609727930183\n",
      "Gradient Descent(6/49): loss=0.39225164638346066\n",
      "Gradient Descent(7/49): loss=0.39004749444502335\n",
      "Gradient Descent(8/49): loss=0.3883408092786079\n",
      "Gradient Descent(9/49): loss=0.3869204593907177\n",
      "Gradient Descent(10/49): loss=0.3856760970502576\n",
      "Gradient Descent(11/49): loss=0.38454922412105585\n",
      "Gradient Descent(12/49): loss=0.38350812252916305\n",
      "Gradient Descent(13/49): loss=0.3825349769354454\n",
      "Gradient Descent(14/49): loss=0.38161924282994636\n",
      "Gradient Descent(15/49): loss=0.38075422133019304\n",
      "Gradient Descent(16/49): loss=0.3799352840523942\n",
      "Gradient Descent(17/49): loss=0.3791589489042028\n",
      "Gradient Descent(18/49): loss=0.3784223957131656\n",
      "Gradient Descent(19/49): loss=0.37772320982713437\n",
      "Gradient Descent(20/49): loss=0.3770592442877735\n",
      "Gradient Descent(21/49): loss=0.37642854396958936\n",
      "Gradient Descent(22/49): loss=0.3758293023218045\n",
      "Gradient Descent(23/49): loss=0.37525983543596364\n",
      "Gradient Descent(24/49): loss=0.37471856545865423\n",
      "Gradient Descent(25/49): loss=0.3742040091572103\n",
      "Gradient Descent(26/49): loss=0.3737147694190446\n",
      "Gradient Descent(27/49): loss=0.37324952849647486\n",
      "Gradient Descent(28/49): loss=0.3728070423507153\n",
      "Gradient Descent(29/49): loss=0.37238613573539714\n",
      "Gradient Descent(30/49): loss=0.37198569781316126\n",
      "Gradient Descent(31/49): loss=0.37160467818183446\n",
      "Gradient Descent(32/49): loss=0.3712420832324702\n",
      "Gradient Descent(33/49): loss=0.370896972787422\n",
      "Gradient Descent(34/49): loss=0.370568456981738\n",
      "Gradient Descent(35/49): loss=0.37025569336035946\n",
      "Gradient Descent(36/49): loss=0.3699578841694501\n",
      "Gradient Descent(37/49): loss=0.3696742738241073\n",
      "Gradient Descent(38/49): loss=0.3694041465374674\n",
      "Gradient Descent(39/49): loss=0.3691468240982525\n",
      "Gradient Descent(40/49): loss=0.36890166378538564\n",
      "Gradient Descent(41/49): loss=0.3686680564095492\n",
      "Gradient Descent(42/49): loss=0.3684454244725982\n",
      "Gradient Descent(43/49): loss=0.368233220436602\n",
      "Gradient Descent(44/49): loss=0.36803092509503016\n",
      "Gradient Descent(45/49): loss=0.3678380460392399\n",
      "Gradient Descent(46/49): loss=0.3676541162139915\n",
      "Gradient Descent(47/49): loss=0.3674786925562089\n",
      "Gradient Descent(48/49): loss=0.36731135471166076\n",
      "Gradient Descent(49/49): loss=0.3671517038246309\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44929341107267984\n",
      "Gradient Descent(2/49): loss=0.42276827748431284\n",
      "Gradient Descent(3/49): loss=0.40850233892624915\n",
      "Gradient Descent(4/49): loss=0.40049952512462117\n",
      "Gradient Descent(5/49): loss=0.3957261527855759\n",
      "Gradient Descent(6/49): loss=0.39263944658120886\n",
      "Gradient Descent(7/49): loss=0.39045099025265395\n",
      "Gradient Descent(8/49): loss=0.3887557256756835\n",
      "Gradient Descent(9/49): loss=0.38734415245642245\n",
      "Gradient Descent(10/49): loss=0.3861068807717604\n",
      "Gradient Descent(11/49): loss=0.38498595686163234\n",
      "Gradient Descent(12/49): loss=0.38394996626689915\n",
      "Gradient Descent(13/49): loss=0.38298126325975335\n",
      "Gradient Descent(14/49): loss=0.38206940146225765\n",
      "Gradient Descent(15/49): loss=0.3812077436389269\n",
      "Gradient Descent(16/49): loss=0.38039170529760313\n",
      "Gradient Descent(17/49): loss=0.37961784000493537\n",
      "Gradient Descent(18/49): loss=0.3788833595342339\n",
      "Gradient Descent(19/49): loss=0.3781858793875053\n",
      "Gradient Descent(20/49): loss=0.3775232816199903\n",
      "Gradient Descent(21/49): loss=0.3768936390641541\n",
      "Gradient Descent(22/49): loss=0.37629517194708795\n",
      "Gradient Descent(23/49): loss=0.3757262217928003\n",
      "Gradient Descent(24/49): loss=0.37518523469949183\n",
      "Gradient Descent(25/49): loss=0.3746707498219746\n",
      "Gradient Descent(26/49): loss=0.37418139084001084\n",
      "Gradient Descent(27/49): loss=0.37371585921583933\n",
      "Gradient Descent(28/49): loss=0.37327292858374933\n",
      "Gradient Descent(29/49): loss=0.37285143990190456\n",
      "Gradient Descent(30/49): loss=0.37245029715151695\n",
      "Gradient Descent(31/49): loss=0.37206846345333244\n",
      "Gradient Descent(32/49): loss=0.3717049575188864\n",
      "Gradient Descent(33/49): loss=0.3713588503812905\n",
      "Gradient Descent(34/49): loss=0.37102926236654554\n",
      "Gradient Descent(35/49): loss=0.37071536027639207\n",
      "Gradient Descent(36/49): loss=0.37041635476017637\n",
      "Gradient Descent(37/49): loss=0.37013149785755634\n",
      "Gradient Descent(38/49): loss=0.36986008069693493\n",
      "Gradient Descent(39/49): loss=0.36960143133675005\n",
      "Gradient Descent(40/49): loss=0.3693549127384546\n",
      "Gradient Descent(41/49): loss=0.36911992086135076\n",
      "Gradient Descent(42/49): loss=0.36889588287052294\n",
      "Gradient Descent(43/49): loss=0.36868225544999095\n",
      "Gradient Descent(44/49): loss=0.3684785232139553\n",
      "Gradient Descent(45/49): loss=0.36828419720963285\n",
      "Gradient Descent(46/49): loss=0.3680988135057363\n",
      "Gradient Descent(47/49): loss=0.3679219318611269\n",
      "Gradient Descent(48/49): loss=0.36775313446859625\n",
      "Gradient Descent(49/49): loss=0.3675920247691121\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44918933781331166\n",
      "Gradient Descent(2/49): loss=0.42207474273446705\n",
      "Gradient Descent(3/49): loss=0.40726797594211395\n",
      "Gradient Descent(4/49): loss=0.3988888532739576\n",
      "Gradient Descent(5/49): loss=0.39389278828330804\n",
      "Gradient Descent(6/49): loss=0.39069887252176366\n",
      "Gradient Descent(7/49): loss=0.3884833083897216\n",
      "Gradient Descent(8/49): loss=0.38681467954649323\n",
      "Gradient Descent(9/49): loss=0.38546534951251293\n",
      "Gradient Descent(10/49): loss=0.38431379067740085\n",
      "Gradient Descent(11/49): loss=0.3832939290621101\n",
      "Gradient Descent(12/49): loss=0.38236882901535807\n",
      "Gradient Descent(13/49): loss=0.3815169933289751\n",
      "Gradient Descent(14/49): loss=0.38072521000502735\n",
      "Gradient Descent(15/49): loss=0.3799848012762488\n",
      "Gradient Descent(16/49): loss=0.37928964403682525\n",
      "Gradient Descent(17/49): loss=0.3786351148980942\n",
      "Gradient Descent(18/49): loss=0.3780175196005222\n",
      "Gradient Descent(19/49): loss=0.3774337774827229\n",
      "Gradient Descent(20/49): loss=0.37688124131348383\n",
      "Gradient Descent(21/49): loss=0.37635758981050405\n",
      "Gradient Descent(22/49): loss=0.3758607598793841\n",
      "Gradient Descent(23/49): loss=0.37538890112012596\n",
      "Gradient Descent(24/49): loss=0.37494034327247044\n",
      "Gradient Descent(25/49): loss=0.3745135715426133\n",
      "Gradient Descent(26/49): loss=0.3741072070120398\n",
      "Gradient Descent(27/49): loss=0.37371999053283095\n",
      "Gradient Descent(28/49): loss=0.3733507691629046\n",
      "Gradient Descent(29/49): loss=0.3729984845507464\n",
      "Gradient Descent(30/49): loss=0.37266216287932835\n",
      "Gradient Descent(31/49): loss=0.3723409060952484\n",
      "Gradient Descent(32/49): loss=0.3720338842197337\n",
      "Gradient Descent(33/49): loss=0.37174032858334033\n",
      "Gradient Descent(34/49): loss=0.3714595258568092\n",
      "Gradient Descent(35/49): loss=0.37119081277251037\n",
      "Gradient Descent(36/49): loss=0.37093357144751704\n",
      "Gradient Descent(37/49): loss=0.37068722523240627\n",
      "Gradient Descent(38/49): loss=0.3704512350205028\n",
      "Gradient Descent(39/49): loss=0.3702250959610948\n",
      "Gradient Descent(40/49): loss=0.37000833452759224\n",
      "Gradient Descent(41/49): loss=0.3698005058979466\n",
      "Gradient Descent(42/49): loss=0.3696011916100985\n",
      "Gradient Descent(43/49): loss=0.36940999745993136\n",
      "Gradient Descent(44/49): loss=0.369226551613285\n",
      "Gradient Descent(45/49): loss=0.3690505029071298\n",
      "Gradient Descent(46/49): loss=0.3688815193180785\n",
      "Gradient Descent(47/49): loss=0.3687192865791052\n",
      "Gradient Descent(48/49): loss=0.36856350692767914\n",
      "Gradient Descent(49/49): loss=0.36841389797056867\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4454345715536217\n",
      "Gradient Descent(2/49): loss=0.41876113934743564\n",
      "Gradient Descent(3/49): loss=0.40523992466621206\n",
      "Gradient Descent(4/49): loss=0.3979845167897357\n",
      "Gradient Descent(5/49): loss=0.3937517747561384\n",
      "Gradient Descent(6/49): loss=0.39100429399286213\n",
      "Gradient Descent(7/49): loss=0.389008698578815\n",
      "Gradient Descent(8/49): loss=0.38741261388725445\n",
      "Gradient Descent(9/49): loss=0.3860450218870189\n",
      "Gradient Descent(10/49): loss=0.38482160327880766\n",
      "Gradient Descent(11/49): loss=0.38369968564460794\n",
      "Gradient Descent(12/49): loss=0.3826567282706421\n",
      "Gradient Descent(13/49): loss=0.3816800128157791\n",
      "Gradient Descent(14/49): loss=0.3807616850678713\n",
      "Gradient Descent(15/49): loss=0.37989635915522163\n",
      "Gradient Descent(16/49): loss=0.37907995235529546\n",
      "Gradient Descent(17/49): loss=0.37830911280480684\n",
      "Gradient Descent(18/49): loss=0.3775809340238127\n",
      "Gradient Descent(19/49): loss=0.3768928089441128\n",
      "Gradient Descent(20/49): loss=0.37624235232637976\n",
      "Gradient Descent(21/49): loss=0.37562735709948697\n",
      "Gradient Descent(22/49): loss=0.3750457678297778\n",
      "Gradient Descent(23/49): loss=0.3744956630786709\n",
      "Gradient Descent(24/49): loss=0.37397524256069753\n",
      "Gradient Descent(25/49): loss=0.3734828170429678\n",
      "Gradient Descent(26/49): loss=0.37301679992568637\n",
      "Gradient Descent(27/49): loss=0.3725756999401305\n",
      "Gradient Descent(28/49): loss=0.3721581146513827\n",
      "Gradient Descent(29/49): loss=0.3717627245824676\n",
      "Gradient Descent(30/49): loss=0.3713882878451534\n",
      "Gradient Descent(31/49): loss=0.371033635200432\n",
      "Gradient Descent(32/49): loss=0.3706976654934481\n",
      "Gradient Descent(33/49): loss=0.37037934142085827\n",
      "Gradient Descent(34/49): loss=0.3700776855971101\n",
      "Gradient Descent(35/49): loss=0.3697917768919196\n",
      "Gradient Descent(36/49): loss=0.3695207470153881\n",
      "Gradient Descent(37/49): loss=0.3692637773303249\n",
      "Gradient Descent(38/49): loss=0.3690200958737872\n",
      "Gradient Descent(39/49): loss=0.36878897457182197\n",
      "Gradient Descent(40/49): loss=0.36856972663302\n",
      "Gradient Descent(41/49): loss=0.36836170410787494\n",
      "Gradient Descent(42/49): loss=0.3681642956021173\n",
      "Gradient Descent(43/49): loss=0.3679769241332241\n",
      "Gradient Descent(44/49): loss=0.3677990451202112\n",
      "Gradient Descent(45/49): loss=0.3676301444976141\n",
      "Gradient Descent(46/49): loss=0.36746973694528867\n",
      "Gradient Descent(47/49): loss=0.3673173642263016\n",
      "Gradient Descent(48/49): loss=0.3671725936257741\n",
      "Gradient Descent(49/49): loss=0.36703501648406445\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44538094577352955\n",
      "Gradient Descent(2/49): loss=0.4186813360835746\n",
      "Gradient Descent(3/49): loss=0.4051439848873835\n",
      "Gradient Descent(4/49): loss=0.3978749442783713\n",
      "Gradient Descent(5/49): loss=0.3936285655804092\n",
      "Gradient Descent(6/49): loss=0.3908669156753634\n",
      "Gradient Descent(7/49): loss=0.38885672325906595\n",
      "Gradient Descent(8/49): loss=0.38724585158815217\n",
      "Gradient Descent(9/49): loss=0.3858634974247945\n",
      "Gradient Descent(10/49): loss=0.384625499491763\n",
      "Gradient Descent(11/49): loss=0.38348929073621413\n",
      "Gradient Descent(12/49): loss=0.38243239561689707\n",
      "Gradient Descent(13/49): loss=0.3814421326157696\n",
      "Gradient Descent(14/49): loss=0.3805106651882882\n",
      "Gradient Descent(15/49): loss=0.37963261258727576\n",
      "Gradient Descent(16/49): loss=0.37880388925361436\n",
      "Gradient Descent(17/49): loss=0.37802113562965284\n",
      "Gradient Descent(18/49): loss=0.37728143474705333\n",
      "Gradient Descent(19/49): loss=0.3765821676023\n",
      "Gradient Descent(20/49): loss=0.3759209364336865\n",
      "Gradient Descent(21/49): loss=0.37529552159326135\n",
      "Gradient Descent(22/49): loss=0.3747038553317538\n",
      "Gradient Descent(23/49): loss=0.37414400433017114\n",
      "Gradient Descent(24/49): loss=0.3736141569410951\n",
      "Gradient Descent(25/49): loss=0.37311261311481525\n",
      "Gradient Descent(26/49): loss=0.3726377759725439\n",
      "Gradient Descent(27/49): loss=0.37218814447793\n",
      "Gradient Descent(28/49): loss=0.3717623069036871\n",
      "Gradient Descent(29/49): loss=0.37135893491596994\n",
      "Gradient Descent(30/49): loss=0.37097677816539887\n",
      "Gradient Descent(31/49): loss=0.37061465930984905\n",
      "Gradient Descent(32/49): loss=0.3702714694148739\n",
      "Gradient Descent(33/49): loss=0.36994616369022093\n",
      "Gradient Descent(34/49): loss=0.3696377575290109\n",
      "Gradient Descent(35/49): loss=0.3693453228217116\n",
      "Gradient Descent(36/49): loss=0.36906798452107376\n",
      "Gradient Descent(37/49): loss=0.36880491743726196\n",
      "Gradient Descent(38/49): loss=0.36855534324484085\n",
      "Gradient Descent(39/49): loss=0.3683185276852524\n",
      "Gradient Descent(40/49): loss=0.3680937779500722\n",
      "Gradient Descent(41/49): loss=0.3678804402317399\n",
      "Gradient Descent(42/49): loss=0.3676778974296679\n",
      "Gradient Descent(43/49): loss=0.3674855670006955\n",
      "Gradient Descent(44/49): loss=0.3673028989437909\n",
      "Gradient Descent(45/49): loss=0.36712937390973316\n",
      "Gradient Descent(46/49): loss=0.3669645014272521\n",
      "Gradient Descent(47/49): loss=0.36680781823777286\n",
      "Gradient Descent(48/49): loss=0.36665888673151736\n",
      "Gradient Descent(49/49): loss=0.3665172934782605\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4455033496288015\n",
      "Gradient Descent(2/49): loss=0.41890705189338656\n",
      "Gradient Descent(3/49): loss=0.40544289254000104\n",
      "Gradient Descent(4/49): loss=0.3982218655756347\n",
      "Gradient Descent(5/49): loss=0.39400629201414\n",
      "Gradient Descent(6/49): loss=0.3912647823309427\n",
      "Gradient Descent(7/49): loss=0.38926843639234815\n",
      "Gradient Descent(8/49): loss=0.3876677420549533\n",
      "Gradient Descent(9/49): loss=0.38629336932572905\n",
      "Gradient Descent(10/49): loss=0.38506194679210615\n",
      "Gradient Descent(11/49): loss=0.3839313193450274\n",
      "Gradient Descent(12/49): loss=0.38287922599276436\n",
      "Gradient Descent(13/49): loss=0.38189310207676225\n",
      "Gradient Descent(14/49): loss=0.3809651822760633\n",
      "Gradient Descent(15/49): loss=0.38009013711014666\n",
      "Gradient Descent(16/49): loss=0.37926392414773746\n",
      "Gradient Descent(17/49): loss=0.37848322372100984\n",
      "Gradient Descent(18/49): loss=0.37774515717209384\n",
      "Gradient Descent(19/49): loss=0.37704714250377624\n",
      "Gradient Descent(20/49): loss=0.3763868174396229\n",
      "Gradient Descent(21/49): loss=0.37576199598360294\n",
      "Gradient Descent(22/49): loss=0.3751706419517353\n",
      "Gradient Descent(23/49): loss=0.37461085135230066\n",
      "Gradient Descent(24/49): loss=0.3740808395737126\n",
      "Gradient Descent(25/49): loss=0.3735789313352003\n",
      "Gradient Descent(26/49): loss=0.37310355234009807\n",
      "Gradient Descent(27/49): loss=0.3726532220633122\n",
      "Gradient Descent(28/49): loss=0.37222654735440414\n",
      "Gradient Descent(29/49): loss=0.37182221666767973\n",
      "Gradient Descent(30/49): loss=0.37143899480037773\n",
      "Gradient Descent(31/49): loss=0.371075718058922\n",
      "Gradient Descent(32/49): loss=0.37073128979590797\n",
      "Gradient Descent(33/49): loss=0.3704046762744595\n",
      "Gradient Descent(34/49): loss=0.37009490282566676\n",
      "Gradient Descent(35/49): loss=0.3698010502710065\n",
      "Gradient Descent(36/49): loss=0.36952225158609847\n",
      "Gradient Descent(37/49): loss=0.3692576887854658\n",
      "Gradient Descent(38/49): loss=0.36900659001054037\n",
      "Gradient Descent(39/49): loss=0.3687682268051994\n",
      "Gradient Descent(40/49): loss=0.368541911564791\n",
      "Gradient Descent(41/49): loss=0.3683269951460053\n",
      "Gradient Descent(42/49): loss=0.3681228646261261\n",
      "Gradient Descent(43/49): loss=0.36792894120122566\n",
      "Gradient Descent(44/49): loss=0.3677446782137485\n",
      "Gradient Descent(45/49): loss=0.3675695593007191\n",
      "Gradient Descent(46/49): loss=0.36740309665450477\n",
      "Gradient Descent(47/49): loss=0.36724482938868974\n",
      "Gradient Descent(48/49): loss=0.36709432200218206\n",
      "Gradient Descent(49/49): loss=0.3669511629351813\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4453647555034761\n",
      "Gradient Descent(2/49): loss=0.4181011151141937\n",
      "Gradient Descent(3/49): loss=0.4040758103866109\n",
      "Gradient Descent(4/49): loss=0.39649950029591013\n",
      "Gradient Descent(5/49): loss=0.39209922362378746\n",
      "Gradient Descent(6/49): loss=0.38929120536500295\n",
      "Gradient Descent(7/49): loss=0.38730519680413694\n",
      "Gradient Descent(8/49): loss=0.3857637148008319\n",
      "Gradient Descent(9/49): loss=0.38447945432223585\n",
      "Gradient Descent(10/49): loss=0.3833575686217624\n",
      "Gradient Descent(11/49): loss=0.38234843960409054\n",
      "Gradient Descent(12/49): loss=0.38142479327540263\n",
      "Gradient Descent(13/49): loss=0.3805705716149638\n",
      "Gradient Descent(14/49): loss=0.37977549193770843\n",
      "Gradient Descent(15/49): loss=0.3790323642550249\n",
      "Gradient Descent(16/49): loss=0.3783357505361274\n",
      "Gradient Descent(17/49): loss=0.37768128012629626\n",
      "Gradient Descent(18/49): loss=0.37706528847361787\n",
      "Gradient Descent(19/49): loss=0.3764846170774918\n",
      "Gradient Descent(20/49): loss=0.3759364953525601\n",
      "Gradient Descent(21/49): loss=0.3754184653275838\n",
      "Gradient Descent(22/49): loss=0.3749283297101102\n",
      "Gradient Descent(23/49): loss=0.3744641134533915\n",
      "Gradient Descent(24/49): loss=0.37402403369896087\n",
      "Gradient Descent(25/49): loss=0.3736064753281251\n",
      "Gradient Descent(26/49): loss=0.37320997054948984\n",
      "Gradient Descent(27/49): loss=0.3728331815677272\n",
      "Gradient Descent(28/49): loss=0.3724748857098097\n",
      "Gradient Descent(29/49): loss=0.3721339625707479\n",
      "Gradient Descent(30/49): loss=0.3718093828516659\n",
      "Gradient Descent(31/49): loss=0.37150019863387834\n",
      "Gradient Descent(32/49): loss=0.37120553488124525\n",
      "Gradient Descent(33/49): loss=0.37092458199864625\n",
      "Gradient Descent(34/49): loss=0.3706565893017747\n",
      "Gradient Descent(35/49): loss=0.37040085927533156\n",
      "Gradient Descent(36/49): loss=0.37015674251461644\n",
      "Gradient Descent(37/49): loss=0.36992363326046895\n",
      "Gradient Descent(38/49): loss=0.36970096545011527\n",
      "Gradient Descent(39/49): loss=0.3694882092171803\n",
      "Gradient Descent(40/49): loss=0.369284867783268\n",
      "Gradient Descent(41/49): loss=0.3690904746913342\n",
      "Gradient Descent(42/49): loss=0.3689045913377945\n",
      "Gradient Descent(43/49): loss=0.3687268047660847\n",
      "Gradient Descent(44/49): loss=0.36855672568936565\n",
      "Gradient Descent(45/49): loss=0.36839398671434387\n",
      "Gradient Descent(46/49): loss=0.368238240741882\n",
      "Gradient Descent(47/49): loss=0.3680891595232601\n",
      "Gradient Descent(48/49): loss=0.3679464323537055\n",
      "Gradient Descent(49/49): loss=0.36780976488719347\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4417745614368587\n",
      "Gradient Descent(2/49): loss=0.4152721342721159\n",
      "Gradient Descent(3/49): loss=0.40262378435588525\n",
      "Gradient Descent(4/49): loss=0.3961057713514115\n",
      "Gradient Descent(5/49): loss=0.3923465577834114\n",
      "Gradient Descent(6/49): loss=0.38986254509350676\n",
      "Gradient Descent(7/49): loss=0.3879956236522878\n",
      "Gradient Descent(8/49): loss=0.3864508481863518\n",
      "Gradient Descent(9/49): loss=0.38509390705319363\n",
      "Gradient Descent(10/49): loss=0.38386193943867913\n",
      "Gradient Descent(11/49): loss=0.3827240927265849\n",
      "Gradient Descent(12/49): loss=0.3816640062211773\n",
      "Gradient Descent(13/49): loss=0.38067199722304484\n",
      "Gradient Descent(14/49): loss=0.37974155535852283\n",
      "Gradient Descent(15/49): loss=0.37886775676486595\n",
      "Gradient Descent(16/49): loss=0.37804653731093696\n",
      "Gradient Descent(17/49): loss=0.3772743522291565\n",
      "Gradient Descent(18/49): loss=0.37654801087580136\n",
      "Gradient Descent(19/49): loss=0.3758645917752822\n",
      "Gradient Descent(20/49): loss=0.37522139513743064\n",
      "Gradient Descent(21/49): loss=0.37461591337166905\n",
      "Gradient Descent(22/49): loss=0.37404581063453257\n",
      "Gradient Descent(23/49): loss=0.37350890721275165\n",
      "Gradient Descent(24/49): loss=0.3730031667239197\n",
      "Gradient Descent(25/49): loss=0.3725266851267555\n",
      "Gradient Descent(26/49): loss=0.3720776810097839\n",
      "Gradient Descent(27/49): loss=0.37165448685847047\n",
      "Gradient Descent(28/49): loss=0.37125554111719405\n",
      "Gradient Descent(29/49): loss=0.370879380923911\n",
      "Gradient Descent(30/49): loss=0.3705246354298412\n",
      "Gradient Descent(31/49): loss=0.37019001963718984\n",
      "Gradient Descent(32/49): loss=0.3698743287011983\n",
      "Gradient Descent(33/49): loss=0.3695764326519359\n",
      "Gradient Descent(34/49): loss=0.3692952714978366\n",
      "Gradient Descent(35/49): loss=0.3690298506780066\n",
      "Gradient Descent(36/49): loss=0.3687792368342756\n",
      "Gradient Descent(37/49): loss=0.36854255387718227\n",
      "Gradient Descent(38/49): loss=0.3683189793227488\n",
      "Gradient Descent(39/49): loss=0.36810774087917625\n",
      "Gradient Descent(40/49): loss=0.3679081132645324\n",
      "Gradient Descent(41/49): loss=0.3677194152382166\n",
      "Gradient Descent(42/49): loss=0.36754100683048013\n",
      "Gradient Descent(43/49): loss=0.36737228675561373\n",
      "Gradient Descent(44/49): loss=0.36721268999560613\n",
      "Gradient Descent(45/49): loss=0.36706168554214663\n",
      "Gradient Descent(46/49): loss=0.3669187742858102\n",
      "Gradient Descent(47/49): loss=0.3667834870421414\n",
      "Gradient Descent(48/49): loss=0.36665538270514597\n",
      "Gradient Descent(49/49): loss=0.36653404651942384\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44171737090134144\n",
      "Gradient Descent(2/49): loss=0.41518881789263057\n",
      "Gradient Descent(3/49): loss=0.40252423326718484\n",
      "Gradient Descent(4/49): loss=0.3959917327272577\n",
      "Gradient Descent(5/49): loss=0.3922175766680902\n",
      "Gradient Descent(6/49): loss=0.38971795655209845\n",
      "Gradient Descent(7/49): loss=0.3878350453259671\n",
      "Gradient Descent(8/49): loss=0.38627420973658805\n",
      "Gradient Descent(9/49): loss=0.38490137255375745\n",
      "Gradient Descent(10/49): loss=0.3836538265143839\n",
      "Gradient Descent(11/49): loss=0.38250081105561207\n",
      "Gradient Descent(12/49): loss=0.3814260153978358\n",
      "Gradient Descent(13/49): loss=0.3804197793221979\n",
      "Gradient Descent(14/49): loss=0.37947559770304634\n",
      "Gradient Descent(15/49): loss=0.37858854149246346\n",
      "Gradient Descent(16/49): loss=0.3777545353621098\n",
      "Gradient Descent(17/49): loss=0.3769700201509701\n",
      "Gradient Descent(18/49): loss=0.3762317893785783\n",
      "Gradient Descent(19/49): loss=0.3755369053488168\n",
      "Gradient Descent(20/49): loss=0.3748826522808178\n",
      "Gradient Descent(21/49): loss=0.37426650715469584\n",
      "Gradient Descent(22/49): loss=0.3736861194158972\n",
      "Gradient Descent(23/49): loss=0.3731392954099078\n",
      "Gradient Descent(24/49): loss=0.37262398557383325\n",
      "Gradient Descent(25/49): loss=0.372138273404899\n",
      "Gradient Descent(26/49): loss=0.3716803656920232\n",
      "Gradient Descent(27/49): loss=0.3712485837209134\n",
      "Gradient Descent(28/49): loss=0.37084135527508\n",
      "Gradient Descent(29/49): loss=0.37045720731385173\n",
      "Gradient Descent(30/49): loss=0.3700947592412427\n",
      "Gradient Descent(31/49): loss=0.3697527166991646\n",
      "Gradient Descent(32/49): loss=0.3694298658311729\n",
      "Gradient Descent(33/49): loss=0.36912506797171035\n",
      "Gradient Descent(34/49): loss=0.3688372547222718\n",
      "Gradient Descent(35/49): loss=0.36856542338086246\n",
      "Gradient Descent(36/49): loss=0.36830863269509306\n",
      "Gradient Descent(37/49): loss=0.36806599891250297\n",
      "Gradient Descent(38/49): loss=0.3678366921044314\n",
      "Gradient Descent(39/49): loss=0.36761993274208676\n",
      "Gradient Descent(40/49): loss=0.3674149885054778\n",
      "Gradient Descent(41/49): loss=0.3672211713076325\n",
      "Gradient Descent(42/49): loss=0.3670378345180826\n",
      "Gradient Descent(43/49): loss=0.36686437037097547\n",
      "Gradient Descent(44/49): loss=0.3667002075444029\n",
      "Gradient Descent(45/49): loss=0.366544808898648\n",
      "Gradient Descent(46/49): loss=0.36639766936204454\n",
      "Gradient Descent(47/49): loss=0.36625831395405123\n",
      "Gradient Descent(48/49): loss=0.3661262959359534\n",
      "Gradient Descent(49/49): loss=0.3660011950803558\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44184989292802407\n",
      "Gradient Descent(2/49): loss=0.4154302120678773\n",
      "Gradient Descent(3/49): loss=0.40283887927227463\n",
      "Gradient Descent(4/49): loss=0.39635190542387266\n",
      "Gradient Descent(5/49): loss=0.39260573447853336\n",
      "Gradient Descent(6/49): loss=0.39012402785987416\n",
      "Gradient Descent(7/49): loss=0.38825347621003814\n",
      "Gradient Descent(8/49): loss=0.38670190882590244\n",
      "Gradient Descent(9/49): loss=0.38533650404881686\n",
      "Gradient Descent(10/49): loss=0.384095168019758\n",
      "Gradient Descent(11/49): loss=0.3829474361499711\n",
      "Gradient Descent(12/49): loss=0.3818771469479465\n",
      "Gradient Descent(13/49): loss=0.38087472649606946\n",
      "Gradient Descent(14/49): loss=0.3799337312389347\n",
      "Gradient Descent(15/49): loss=0.37904928495861473\n",
      "Gradient Descent(16/49): loss=0.37821736217833724\n",
      "Gradient Descent(17/49): loss=0.3774344520495493\n",
      "Gradient Descent(18/49): loss=0.37669739471987107\n",
      "Gradient Descent(19/49): loss=0.3760032968875472\n",
      "Gradient Descent(20/49): loss=0.37534948443116506\n",
      "Gradient Descent(21/49): loss=0.3747334729344117\n",
      "Gradient Descent(22/49): loss=0.3741529472537053\n",
      "Gradient Descent(23/49): loss=0.3736057459624271\n",
      "Gradient Descent(24/49): loss=0.3730898486537384\n",
      "Gradient Descent(25/49): loss=0.3726033650837505\n",
      "Gradient Descent(26/49): loss=0.3721445256121909\n",
      "Gradient Descent(27/49): loss=0.37171167263063154\n",
      "Gradient Descent(28/49): loss=0.3713032527871566\n",
      "Gradient Descent(29/49): loss=0.3709178098800988\n",
      "Gradient Descent(30/49): loss=0.37055397832979586\n",
      "Gradient Descent(31/49): loss=0.37021047715937416\n",
      "Gradient Descent(32/49): loss=0.36988610442982495\n",
      "Gradient Descent(33/49): loss=0.36957973208442557\n",
      "Gradient Descent(34/49): loss=0.36929030116459427\n",
      "Gradient Descent(35/49): loss=0.3690168173645662\n",
      "Gradient Descent(36/49): loss=0.3687583468963945\n",
      "Gradient Descent(37/49): loss=0.3685140126400829\n",
      "Gradient Descent(38/49): loss=0.36828299055636327\n",
      "Gradient Descent(39/49): loss=0.36806450634190063\n",
      "Gradient Descent(40/49): loss=0.3678578323086434\n",
      "Gradient Descent(41/49): loss=0.3676622844707053\n",
      "Gradient Descent(42/49): loss=0.3674772198236301\n",
      "Gradient Descent(43/49): loss=0.36730203380217585\n",
      "Gradient Descent(44/49): loss=0.36713615790391\n",
      "Gradient Descent(45/49): loss=0.3669790574669315\n",
      "Gradient Descent(46/49): loss=0.36683022959096445\n",
      "Gradient Descent(47/49): loss=0.36668920119190657\n",
      "Gradient Descent(48/49): loss=0.3665555271806761\n",
      "Gradient Descent(49/49): loss=0.3664287887578947\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.44167260132664793\n",
      "Gradient Descent(2/49): loss=0.41451092563017\n",
      "Gradient Descent(3/49): loss=0.4013495390060603\n",
      "Gradient Descent(4/49): loss=0.39453694400934924\n",
      "Gradient Descent(5/49): loss=0.3906473386844524\n",
      "Gradient Descent(6/49): loss=0.3881392150109951\n",
      "Gradient Descent(7/49): loss=0.3863139154183665\n",
      "Gradient Descent(8/49): loss=0.3848509994611016\n",
      "Gradient Descent(9/49): loss=0.3836004604307238\n",
      "Gradient Descent(10/49): loss=0.3824895894184634\n",
      "Gradient Descent(11/49): loss=0.38148111033018295\n",
      "Gradient Descent(12/49): loss=0.38055428555493603\n",
      "Gradient Descent(13/49): loss=0.37969633497280386\n",
      "Gradient Descent(14/49): loss=0.37889849856739477\n",
      "Gradient Descent(15/49): loss=0.37815419848689424\n",
      "Gradient Descent(16/49): loss=0.377458156397092\n",
      "Gradient Descent(17/49): loss=0.3768059500172735\n",
      "Gradient Descent(18/49): loss=0.3761937750047801\n",
      "Gradient Descent(19/49): loss=0.3756183055319354\n",
      "Gradient Descent(20/49): loss=0.37507660439014423\n",
      "Gradient Descent(21/49): loss=0.37456605956959194\n",
      "Gradient Descent(22/49): loss=0.37408433620933856\n",
      "Gradient Descent(23/49): loss=0.37362933833827866\n",
      "Gradient Descent(24/49): loss=0.37319917742785785\n",
      "Gradient Descent(25/49): loss=0.37279214603477\n",
      "Gradient Descent(26/49): loss=0.37240669544535066\n",
      "Gradient Descent(27/49): loss=0.372041416571914\n",
      "Gradient Descent(28/49): loss=0.3716950235464908\n",
      "Gradient Descent(29/49): loss=0.37136633958012494\n",
      "Gradient Descent(30/49): loss=0.37105428473974317\n",
      "Gradient Descent(31/49): loss=0.3707578653560818\n",
      "Gradient Descent(32/49): loss=0.3704761648236183\n",
      "Gradient Descent(33/49): loss=0.37020833559140853\n",
      "Gradient Descent(34/49): loss=0.36995359217478485\n",
      "Gradient Descent(35/49): loss=0.3697112050436237\n",
      "Gradient Descent(36/49): loss=0.36948049526446414\n",
      "Gradient Descent(37/49): loss=0.3692608297919092\n",
      "Gradient Descent(38/49): loss=0.3690516173200874\n",
      "Gradient Descent(39/49): loss=0.3688523046179528\n",
      "Gradient Descent(40/49): loss=0.368662373283237\n",
      "Gradient Descent(41/49): loss=0.3684813368592443\n",
      "Gradient Descent(42/49): loss=0.3683087382666731\n",
      "Gradient Descent(43/49): loss=0.3681441475094364\n",
      "Gradient Descent(44/49): loss=0.3679871596192657\n",
      "Gradient Descent(45/49): loss=0.36783739280881794\n",
      "Gradient Descent(46/49): loss=0.3676944868072405\n",
      "Gradient Descent(47/49): loss=0.3675581013557493\n",
      "Gradient Descent(48/49): loss=0.3674279148438707\n",
      "Gradient Descent(49/49): loss=0.3673036230696298\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4382510296155473\n",
      "Gradient Descent(2/49): loss=0.41213904507300353\n",
      "Gradient Descent(3/49): loss=0.4003996983742317\n",
      "Gradient Descent(4/49): loss=0.39455447267867755\n",
      "Gradient Descent(5/49): loss=0.3911820545921173\n",
      "Gradient Descent(6/49): loss=0.3888882523566602\n",
      "Gradient Descent(7/49): loss=0.3870989250030709\n",
      "Gradient Descent(8/49): loss=0.3855740497231836\n",
      "Gradient Descent(9/49): loss=0.3842106381941318\n",
      "Gradient Descent(10/49): loss=0.382962339478245\n",
      "Gradient Descent(11/49): loss=0.3818064753975679\n",
      "Gradient Descent(12/49): loss=0.38073045256329263\n",
      "Gradient Descent(13/49): loss=0.3797261243983159\n",
      "Gradient Descent(14/49): loss=0.3787874290400136\n",
      "Gradient Descent(15/49): loss=0.3779093838106658\n",
      "Gradient Descent(16/49): loss=0.3770876451527509\n",
      "Gradient Descent(17/49): loss=0.37631830657583176\n",
      "Gradient Descent(18/49): loss=0.3755977983530907\n",
      "Gradient Descent(19/49): loss=0.3749228318427397\n",
      "Gradient Descent(20/49): loss=0.37429036421873424\n",
      "Gradient Descent(21/49): loss=0.37369757316761437\n",
      "Gradient Descent(22/49): loss=0.37314183692516606\n",
      "Gradient Descent(23/49): loss=0.37262071751735887\n",
      "Gradient Descent(24/49): loss=0.37213194615881867\n",
      "Gradient Descent(25/49): loss=0.3716734102528598\n",
      "Gradient Descent(26/49): loss=0.37124314166822203\n",
      "Gradient Descent(27/49): loss=0.37083930608309645\n",
      "Gradient Descent(28/49): loss=0.37046019324902973\n",
      "Gradient Descent(29/49): loss=0.3701042080634044\n",
      "Gradient Descent(30/49): loss=0.3697698623619735\n",
      "Gradient Descent(31/49): loss=0.36945576735838187\n",
      "Gradient Descent(32/49): loss=0.36916062666874644\n",
      "Gradient Descent(33/49): loss=0.36888322986779776\n",
      "Gradient Descent(34/49): loss=0.3686224465297066\n",
      "Gradient Descent(35/49): loss=0.3683772207121015\n",
      "Gradient Descent(36/49): loss=0.36814656584623673\n",
      "Gradient Descent(37/49): loss=0.3679295600000528\n",
      "Gradient Descent(38/49): loss=0.3677253414841176\n",
      "Gradient Descent(39/49): loss=0.3675331047732621\n",
      "Gradient Descent(40/49): loss=0.3673520967192124\n",
      "Gradient Descent(41/49): loss=0.36718161303171504\n",
      "Gradient Descent(42/49): loss=0.36702099500762203\n",
      "Gradient Descent(43/49): loss=0.3668696264891485\n",
      "Gradient Descent(44/49): loss=0.36672693103410686\n",
      "Gradient Descent(45/49): loss=0.3665923692823401\n",
      "Gradient Descent(46/49): loss=0.3664654365038713\n",
      "Gradient Descent(47/49): loss=0.36634566031545235\n",
      "Gradient Descent(48/49): loss=0.36623259855326457\n",
      "Gradient Descent(49/49): loss=0.36612583729048737\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4381904134106945\n",
      "Gradient Descent(2/49): loss=0.41205252288279104\n",
      "Gradient Descent(3/49): loss=0.4002967203627034\n",
      "Gradient Descent(4/49): loss=0.3944359724314889\n",
      "Gradient Descent(5/49): loss=0.39104720566559725\n",
      "Gradient Descent(6/49): loss=0.38873634691129355\n",
      "Gradient Descent(7/49): loss=0.3869296857473321\n",
      "Gradient Descent(8/49): loss=0.3853875569449384\n",
      "Gradient Descent(9/49): loss=0.38400720770636554\n",
      "Gradient Descent(10/49): loss=0.382742425909868\n",
      "Gradient Descent(11/49): loss=0.3815706070025165\n",
      "Gradient Descent(12/49): loss=0.38047919007767556\n",
      "Gradient Descent(13/49): loss=0.37946003613469276\n",
      "Gradient Descent(14/49): loss=0.37850707642747117\n",
      "Gradient Descent(15/49): loss=0.3776153134724175\n",
      "Gradient Descent(16/49): loss=0.376780384979828\n",
      "Gradient Descent(17/49): loss=0.37599836417773463\n",
      "Gradient Descent(18/49): loss=0.37526566087063057\n",
      "Gradient Descent(19/49): loss=0.3745789664978765\n",
      "Gradient Descent(20/49): loss=0.3739352192241436\n",
      "Gradient Descent(21/49): loss=0.37333157877454376\n",
      "Gradient Descent(22/49): loss=0.37276540648477424\n",
      "Gradient Descent(23/49): loss=0.3722342484896938\n",
      "Gradient Descent(24/49): loss=0.37173582103882813\n",
      "Gradient Descent(25/49): loss=0.3712679974032876\n",
      "Gradient Descent(26/49): loss=0.3708287960605985\n",
      "Gradient Descent(27/49): loss=0.3704163699538747\n",
      "Gradient Descent(28/49): loss=0.3700289966804889\n",
      "Gradient Descent(29/49): loss=0.3696650694996226\n",
      "Gradient Descent(30/49): loss=0.36932308906982375\n",
      "Gradient Descent(31/49): loss=0.36900165584262873\n",
      "Gradient Descent(32/49): loss=0.36869946304923285\n",
      "Gradient Descent(33/49): loss=0.36841529022557196\n",
      "Gradient Descent(34/49): loss=0.36814799722785957\n",
      "Gradient Descent(35/49): loss=0.36789651869609474\n",
      "Gradient Descent(36/49): loss=0.36765985892763164\n",
      "Gradient Descent(37/49): loss=0.36743708712680156\n",
      "Gradient Descent(38/49): loss=0.36722733299993643\n",
      "Gradient Descent(39/49): loss=0.3670297826680773\n",
      "Gradient Descent(40/49): loss=0.36684367487222086\n",
      "Gradient Descent(41/49): loss=0.36666829744824236\n",
      "Gradient Descent(42/49): loss=0.3665029840506619\n",
      "Gradient Descent(43/49): loss=0.36634711110623114\n",
      "Gradient Descent(44/49): loss=0.366200094979952\n",
      "Gradient Descent(45/49): loss=0.36606138933760257\n",
      "Gradient Descent(46/49): loss=0.36593048269016754\n",
      "Gradient Descent(47/49): loss=0.3658068961067707\n",
      "Gradient Descent(48/49): loss=0.36569018108379286\n",
      "Gradient Descent(49/49): loss=0.3655799175588418\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43833304097034764\n",
      "Gradient Descent(2/49): loss=0.41230895546259794\n",
      "Gradient Descent(3/49): loss=0.40062563884561825\n",
      "Gradient Descent(4/49): loss=0.39480754982089156\n",
      "Gradient Descent(5/49): loss=0.3914440358617002\n",
      "Gradient Descent(6/49): loss=0.3891491840631395\n",
      "Gradient Descent(7/49): loss=0.38735374118599714\n",
      "Gradient Descent(8/49): loss=0.3858202383060121\n",
      "Gradient Descent(9/49): loss=0.3844469356727037\n",
      "Gradient Descent(10/49): loss=0.3831880732258646\n",
      "Gradient Descent(11/49): loss=0.3820212538295315\n",
      "Gradient Descent(12/49): loss=0.3809340264570481\n",
      "Gradient Descent(13/49): loss=0.3799183273407109\n",
      "Gradient Descent(14/49): loss=0.37896815246092425\n",
      "Gradient Descent(15/49): loss=0.3780785662009603\n",
      "Gradient Descent(16/49): loss=0.377245266603313\n",
      "Gradient Descent(17/49): loss=0.37646438503231994\n",
      "Gradient Descent(18/49): loss=0.3757323862662747\n",
      "Gradient Descent(19/49): loss=0.3750460128295014\n",
      "Gradient Descent(20/49): loss=0.37440224969602043\n",
      "Gradient Descent(21/49): loss=0.373798299025813\n",
      "Gradient Descent(22/49): loss=0.3732315603192329\n",
      "Gradient Descent(23/49): loss=0.3726996138364256\n",
      "Gradient Descent(24/49): loss=0.37220020621240024\n",
      "Gradient Descent(25/49): loss=0.37173123769239935\n",
      "Gradient Descent(26/49): loss=0.3712907506484195\n",
      "Gradient Descent(27/49): loss=0.3708769191578361\n",
      "Gradient Descent(28/49): loss=0.3704880394907213\n",
      "Gradient Descent(29/49): loss=0.3701225213911709\n",
      "Gradient Descent(30/49): loss=0.36977888006252013\n",
      "Gradient Descent(31/49): loss=0.36945572878296196\n",
      "Gradient Descent(32/49): loss=0.3691517720899564\n",
      "Gradient Descent(33/49): loss=0.36886579948068915\n",
      "Gradient Descent(34/49): loss=0.3685966795827072\n",
      "Gradient Descent(35/49): loss=0.3683433547543403\n",
      "Gradient Descent(36/49): loss=0.3681048360790101\n",
      "Gradient Descent(37/49): loss=0.36788019872127625\n",
      "Gradient Descent(38/49): loss=0.36766857761566873\n",
      "Gradient Descent(39/49): loss=0.36746916346210934\n",
      "Gradient Descent(40/49): loss=0.36728119900412975\n",
      "Gradient Descent(41/49): loss=0.36710397556822394\n",
      "Gradient Descent(42/49): loss=0.3669368298445477\n",
      "Gradient Descent(43/49): loss=0.36677914089087027\n",
      "Gradient Descent(44/49): loss=0.3666303273431949\n",
      "Gradient Descent(45/49): loss=0.36648984481782165\n",
      "Gradient Descent(46/49): loss=0.3663571834908741\n",
      "Gradient Descent(47/49): loss=0.36623186584241596\n",
      "Gradient Descent(48/49): loss=0.36611344455331146\n",
      "Gradient Descent(49/49): loss=0.3660015005439051\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43811287528282744\n",
      "Gradient Descent(2/49): loss=0.41127666558923404\n",
      "Gradient Descent(3/49): loss=0.3990254488806676\n",
      "Gradient Descent(4/49): loss=0.39291811258249154\n",
      "Gradient Descent(5/49): loss=0.3894537875259732\n",
      "Gradient Descent(6/49): loss=0.38717086753955193\n",
      "Gradient Descent(7/49): loss=0.3854527219438344\n",
      "Gradient Descent(8/49): loss=0.38403433042204566\n",
      "Gradient Descent(9/49): loss=0.3827978985817451\n",
      "Gradient Descent(10/49): loss=0.3816879281890721\n",
      "Gradient Descent(11/49): loss=0.3806757758903043\n",
      "Gradient Descent(12/49): loss=0.3797447641498212\n",
      "Gradient Descent(13/49): loss=0.37888385464812047\n",
      "Gradient Descent(14/49): loss=0.37808490591121424\n",
      "Gradient Descent(15/49): loss=0.37734144274232206\n",
      "Gradient Descent(16/49): loss=0.37664807152673896\n",
      "Gradient Descent(17/49): loss=0.37600017758682197\n",
      "Gradient Descent(18/49): loss=0.37539375037283484\n",
      "Gradient Descent(19/49): loss=0.3748252701639535\n",
      "Gradient Descent(20/49): loss=0.37429162704833563\n",
      "Gradient Descent(21/49): loss=0.3737900587695347\n",
      "Gradient Descent(22/49): loss=0.3733181008854123\n",
      "Gradient Descent(23/49): loss=0.37287354574192166\n",
      "Gradient Descent(24/49): loss=0.3724544081867997\n",
      "Gradient Descent(25/49): loss=0.37205889665556846\n",
      "Gradient Descent(26/49): loss=0.37168538864673833\n",
      "Gradient Descent(27/49): loss=0.3713324098348127\n",
      "Gradient Descent(28/49): loss=0.37099861622397207\n",
      "Gradient Descent(29/49): loss=0.3706827788567117\n",
      "Gradient Descent(30/49): loss=0.37038377067684985\n",
      "Gradient Descent(31/49): loss=0.37010055521383134\n",
      "Gradient Descent(32/49): loss=0.36983217680997293\n",
      "Gradient Descent(33/49): loss=0.36957775215727023\n",
      "Gradient Descent(34/49): loss=0.3693364629476217\n",
      "Gradient Descent(35/49): loss=0.3691075494713415\n",
      "Gradient Descent(36/49): loss=0.3688903050247352\n",
      "Gradient Descent(37/49): loss=0.36868407100920303\n",
      "Gradient Descent(38/49): loss=0.3684882326225316\n",
      "Gradient Descent(39/49): loss=0.36830221505831406\n",
      "Gradient Descent(40/49): loss=0.368125480142291\n",
      "Gradient Descent(41/49): loss=0.3679575233452207\n",
      "Gradient Descent(42/49): loss=0.3677978711209943\n",
      "Gradient Descent(43/49): loss=0.3676460785264006\n",
      "Gradient Descent(44/49): loss=0.36750172708542006\n",
      "Gradient Descent(45/49): loss=0.367364422866407\n",
      "Gradient Descent(46/49): loss=0.3672337947451426\n",
      "Gradient Descent(47/49): loss=0.36710949283065325\n",
      "Gradient Descent(48/49): loss=0.3669911870340036\n",
      "Gradient Descent(49/49): loss=0.36687856576307865\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4348639760896874\n",
      "Gradient Descent(2/49): loss=0.40933427636468117\n",
      "Gradient Descent(3/49): loss=0.3985103326769797\n",
      "Gradient Descent(4/49): loss=0.39326307613252437\n",
      "Gradient Descent(5/49): loss=0.39019674209925337\n",
      "Gradient Descent(6/49): loss=0.38803298286913795\n",
      "Gradient Descent(7/49): loss=0.38628375386495173\n",
      "Gradient Descent(8/49): loss=0.38475844959986805\n",
      "Gradient Descent(9/49): loss=0.38337935110946963\n",
      "Gradient Descent(10/49): loss=0.38211206594950775\n",
      "Gradient Descent(11/49): loss=0.3809391467205958\n",
      "Gradient Descent(12/49): loss=0.3798500121983317\n",
      "Gradient Descent(13/49): loss=0.37883705574074056\n",
      "Gradient Descent(14/49): loss=0.37789411557767977\n",
      "Gradient Descent(15/49): loss=0.3770158536731368\n",
      "Gradient Descent(16/49): loss=0.3761974882318418\n",
      "Gradient Descent(17/49): loss=0.3754346665735823\n",
      "Gradient Descent(18/49): loss=0.3747233956283563\n",
      "Gradient Descent(19/49): loss=0.3740599974703819\n",
      "Gradient Descent(20/49): loss=0.37344107675125066\n",
      "Gradient Descent(21/49): loss=0.37286349452481543\n",
      "Gradient Descent(22/49): loss=0.3723243460130383\n",
      "Gradient Descent(23/49): loss=0.3718209411247498\n",
      "Gradient Descent(24/49): loss=0.3713507870857493\n",
      "Gradient Descent(25/49): loss=0.3709115727911998\n",
      "Gradient Descent(26/49): loss=0.3705011546181017\n",
      "Gradient Descent(27/49): loss=0.37011754350567055\n",
      "Gradient Descent(28/49): loss=0.36975889315391397\n",
      "Gradient Descent(29/49): loss=0.36942348921869855\n",
      "Gradient Descent(30/49): loss=0.36910973940135455\n",
      "Gradient Descent(31/49): loss=0.36881616434557374\n",
      "Gradient Descent(32/49): loss=0.3685413892657794\n",
      "Gradient Descent(33/49): loss=0.36828413624031303\n",
      "Gradient Descent(34/49): loss=0.36804321711033405\n",
      "Gradient Descent(35/49): loss=0.36781752693167913\n",
      "Gradient Descent(36/49): loss=0.36760603793235413\n",
      "Gradient Descent(37/49): loss=0.367407793933033\n",
      "Gradient Descent(38/49): loss=0.3672219051920459\n",
      "Gradient Descent(39/49): loss=0.36704754363996334\n",
      "Gradient Descent(40/49): loss=0.3668839384720965\n",
      "Gradient Descent(41/49): loss=0.3667303720700985\n",
      "Gradient Descent(42/49): loss=0.3665861762264141\n",
      "Gradient Descent(43/49): loss=0.3664507286476302\n",
      "Gradient Descent(44/49): loss=0.36632344971484904\n",
      "Gradient Descent(45/49): loss=0.36620379948107795\n",
      "Gradient Descent(46/49): loss=0.3660912748873209\n",
      "Gradient Descent(47/49): loss=0.3659854071805873\n",
      "Gradient Descent(48/49): loss=0.36588575951842345\n",
      "Gradient Descent(49/49): loss=0.3657919247458366\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4348000733015889\n",
      "Gradient Descent(2/49): loss=0.40924482154573866\n",
      "Gradient Descent(3/49): loss=0.3984040499620499\n",
      "Gradient Descent(4/49): loss=0.39314007515459026\n",
      "Gradient Descent(5/49): loss=0.39005592318881177\n",
      "Gradient Descent(6/49): loss=0.3878736778447799\n",
      "Gradient Descent(7/49): loss=0.38610583597071135\n",
      "Gradient Descent(8/49): loss=0.3845621688888018\n",
      "Gradient Descent(9/49): loss=0.383165179863266\n",
      "Gradient Descent(10/49): loss=0.3818805935919745\n",
      "Gradient Descent(11/49): loss=0.3806910149971696\n",
      "Gradient Descent(12/49): loss=0.3795858771495674\n",
      "Gradient Descent(13/49): loss=0.37855756639257937\n",
      "Gradient Descent(14/49): loss=0.37759990273144706\n",
      "Gradient Descent(15/49): loss=0.37670752463754337\n",
      "Gradient Descent(16/49): loss=0.3758756249431343\n",
      "Gradient Descent(17/49): loss=0.3750998255638664\n",
      "Gradient Descent(18/49): loss=0.3743761089319296\n",
      "Gradient Descent(19/49): loss=0.373700773943545\n",
      "Gradient Descent(20/49): loss=0.37307040351375603\n",
      "Gradient Descent(21/49): loss=0.37248183836923743\n",
      "Gradient Descent(22/49): loss=0.37193215470868124\n",
      "Gradient Descent(23/49): loss=0.37141864458815566\n",
      "Gradient Descent(24/49): loss=0.3709387984145304\n",
      "Gradient Descent(25/49): loss=0.3704902891703896\n",
      "Gradient Descent(26/49): loss=0.3700709581136918\n",
      "Gradient Descent(27/49): loss=0.36967880176158446\n",
      "Gradient Descent(28/49): loss=0.36931196000818833\n",
      "Gradient Descent(29/49): loss=0.36896870525317016\n",
      "Gradient Descent(30/49): loss=0.3686474324372899\n",
      "Gradient Descent(31/49): loss=0.3683466498957555\n",
      "Gradient Descent(32/49): loss=0.36806497095175567\n",
      "Gradient Descent(33/49): loss=0.36780110618189915\n",
      "Gradient Descent(34/49): loss=0.36755385629305165\n",
      "Gradient Descent(35/49): loss=0.36732210555663297\n",
      "Gradient Descent(36/49): loss=0.3671048157520553\n",
      "Gradient Descent(37/49): loss=0.3669010205758651\n",
      "Gradient Descent(38/49): loss=0.36670982047741274\n",
      "Gradient Descent(39/49): loss=0.3665303778856205\n",
      "Gradient Descent(40/49): loss=0.3663619127947517\n",
      "Gradient Descent(41/49): loss=0.36620369868003055\n",
      "Gradient Descent(42/49): loss=0.36605505871660377\n",
      "Gradient Descent(43/49): loss=0.3659153622776999\n",
      "Gradient Descent(44/49): loss=0.3657840216899575\n",
      "Gradient Descent(45/49): loss=0.3656604892258116\n",
      "Gradient Descent(46/49): loss=0.36554425431454357\n",
      "Gradient Descent(47/49): loss=0.36543484095515943\n",
      "Gradient Descent(48/49): loss=0.3653318053156706\n",
      "Gradient Descent(49/49): loss=0.36523473350462987\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4349527937557719\n",
      "Gradient Descent(2/49): loss=0.40951562963483334\n",
      "Gradient Descent(3/49): loss=0.39874583072366704\n",
      "Gradient Descent(4/49): loss=0.39352141991868567\n",
      "Gradient Descent(5/49): loss=0.3904599879580259\n",
      "Gradient Descent(6/49): loss=0.3882921864128638\n",
      "Gradient Descent(7/49): loss=0.38653472237509584\n",
      "Gradient Descent(8/49): loss=0.3849992327399342\n",
      "Gradient Descent(9/49): loss=0.38360899689544536\n",
      "Gradient Descent(10/49): loss=0.3823300603864192\n",
      "Gradient Descent(11/49): loss=0.38114517780091617\n",
      "Gradient Descent(12/49): loss=0.3800438758107683\n",
      "Gradient Descent(13/49): loss=0.37901861959835703\n",
      "Gradient Descent(14/49): loss=0.3780633051331852\n",
      "Gradient Descent(15/49): loss=0.37717264549042817\n",
      "Gradient Descent(16/49): loss=0.37634190540303086\n",
      "Gradient Descent(17/49): loss=0.3755667744258646\n",
      "Gradient Descent(18/49): loss=0.37484329732301563\n",
      "Gradient Descent(19/49): loss=0.37416782953609923\n",
      "Gradient Descent(20/49): loss=0.3735370046914838\n",
      "Gradient Descent(21/49): loss=0.37294770862413895\n",
      "Gradient Descent(22/49): loss=0.37239705742613866\n",
      "Gradient Descent(23/49): loss=0.37188237829308285\n",
      "Gradient Descent(24/49): loss=0.37140119249792747\n",
      "Gradient Descent(25/49): loss=0.3709512000838155\n",
      "Gradient Descent(26/49): loss=0.37053026600183453\n",
      "Gradient Descent(27/49): loss=0.37013640749494076\n",
      "Gradient Descent(28/49): loss=0.369767782575276\n",
      "Gradient Descent(29/49): loss=0.36942267947234575\n",
      "Gradient Descent(30/49): loss=0.3690995069506279\n",
      "Gradient Descent(31/49): loss=0.3687967854106741\n",
      "Gradient Descent(32/49): loss=0.36851313869958474\n",
      "Gradient Descent(33/49): loss=0.36824728656606603\n",
      "Gradient Descent(34/49): loss=0.3679980377028525\n",
      "Gradient Descent(35/49): loss=0.3677642833255575\n",
      "Gradient Descent(36/49): loss=0.36754499124233425\n",
      "Gradient Descent(37/49): loss=0.3673392003732922\n",
      "Gradient Descent(38/49): loss=0.3671460156825884\n",
      "Gradient Descent(39/49): loss=0.36696460348959214\n",
      "Gradient Descent(40/49): loss=0.3667941871286061\n",
      "Gradient Descent(41/49): loss=0.36663404292937185\n",
      "Gradient Descent(42/49): loss=0.3664834964930342\n",
      "Gradient Descent(43/49): loss=0.3663419192404439\n",
      "Gradient Descent(44/49): loss=0.3662087252116605\n",
      "Gradient Descent(45/49): loss=0.36608336809730074\n",
      "Gradient Descent(46/49): loss=0.3659653384840053\n",
      "Gradient Descent(47/49): loss=0.3658541612977532\n",
      "Gradient Descent(48/49): loss=0.36574939343009694\n",
      "Gradient Descent(49/49): loss=0.3656506215335926\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4346855773720145\n",
      "Gradient Descent(2/49): loss=0.40837186620851923\n",
      "Gradient Descent(3/49): loss=0.39704673007727853\n",
      "Gradient Descent(4/49): loss=0.39157429342127814\n",
      "Gradient Descent(5/49): loss=0.3884545309171262\n",
      "Gradient Descent(6/49): loss=0.3863346235974059\n",
      "Gradient Descent(7/49): loss=0.3846836320023424\n",
      "Gradient Descent(8/49): loss=0.3832870342713613\n",
      "Gradient Descent(9/49): loss=0.3820532872244587\n",
      "Gradient Descent(10/49): loss=0.3809394821046713\n",
      "Gradient Descent(11/49): loss=0.3799225822542726\n",
      "Gradient Descent(12/49): loss=0.3789881664880848\n",
      "Gradient Descent(13/49): loss=0.3781259296613989\n",
      "Gradient Descent(14/49): loss=0.377327813318033\n",
      "Gradient Descent(15/49): loss=0.3765871750493527\n",
      "Gradient Descent(16/49): loss=0.3758983789510231\n",
      "Gradient Descent(17/49): loss=0.37525656489603426\n",
      "Gradient Descent(18/49): loss=0.37465749980379504\n",
      "Gradient Descent(19/49): loss=0.3740974709383162\n",
      "Gradient Descent(20/49): loss=0.37357320379945175\n",
      "Gradient Descent(21/49): loss=0.37308179631211535\n",
      "Gradient Descent(22/49): loss=0.3726206648775471\n",
      "Gradient Descent(23/49): loss=0.3721874995890022\n",
      "Gradient Descent(24/49): loss=0.37178022677549444\n",
      "Gradient Descent(25/49): loss=0.37139697751798706\n",
      "Gradient Descent(26/49): loss=0.3710360610854289\n",
      "Gradient Descent(27/49): loss=0.37069594244913323\n",
      "Gradient Descent(28/49): loss=0.3703752231916571\n",
      "Gradient Descent(29/49): loss=0.37007262524924617\n",
      "Gradient Descent(30/49): loss=0.3697869770251657\n",
      "Gradient Descent(31/49): loss=0.36951720149091793\n",
      "Gradient Descent(32/49): loss=0.3692623059575212\n",
      "Gradient Descent(33/49): loss=0.3690213732526036\n",
      "Gradient Descent(34/49): loss=0.3687935540832684\n",
      "Gradient Descent(35/49): loss=0.3685780604012351\n",
      "Gradient Descent(36/49): loss=0.3683741596170412\n",
      "Gradient Descent(37/49): loss=0.36818116953519764\n",
      "Gradient Descent(38/49): loss=0.36799845390305486\n",
      "Gradient Descent(39/49): loss=0.3678254184834706\n",
      "Gradient Descent(40/49): loss=0.3676615075758032\n",
      "Gradient Descent(41/49): loss=0.3675062009217731\n",
      "Gradient Descent(42/49): loss=0.36735901094275103\n",
      "Gradient Descent(43/49): loss=0.367219480263397\n",
      "Gradient Descent(44/49): loss=0.36708717948355235\n",
      "Gradient Descent(45/49): loss=0.36696170516612714\n",
      "Gradient Descent(46/49): loss=0.36684267801360976\n",
      "Gradient Descent(47/49): loss=0.36672974120991725\n",
      "Gradient Descent(48/49): loss=0.3666225589077385\n",
      "Gradient Descent(49/49): loss=0.36652081484440335\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4316134008592792\n",
      "Gradient Descent(2/49): loss=0.4068313790189575\n",
      "Gradient Descent(3/49): loss=0.3969050648208042\n",
      "Gradient Descent(4/49): loss=0.39217651484382476\n",
      "Gradient Descent(5/49): loss=0.3893442185147288\n",
      "Gradient Descent(6/49): loss=0.38726285166918184\n",
      "Gradient Descent(7/49): loss=0.3855273734874112\n",
      "Gradient Descent(8/49): loss=0.3839894212474995\n",
      "Gradient Descent(9/49): loss=0.3825906238935812\n",
      "Gradient Descent(10/49): loss=0.38130471242837183\n",
      "Gradient Descent(11/49): loss=0.38011725374246746\n",
      "Gradient Descent(12/49): loss=0.3790184776990498\n",
      "Gradient Descent(13/49): loss=0.3780006910563077\n",
      "Gradient Descent(14/49): loss=0.377057312074572\n",
      "Gradient Descent(15/49): loss=0.37618248539221455\n",
      "Gradient Descent(16/49): loss=0.37537090945233376\n",
      "Gradient Descent(17/49): loss=0.37461774503952705\n",
      "Gradient Descent(18/49): loss=0.373918557070469\n",
      "Gradient Descent(19/49): loss=0.3732692716650397\n",
      "Gradient Descent(20/49): loss=0.37266614138988746\n",
      "Gradient Descent(21/49): loss=0.3721057156263263\n",
      "Gradient Descent(22/49): loss=0.37158481459926623\n",
      "Gradient Descent(23/49): loss=0.37110050626452096\n",
      "Gradient Descent(24/49): loss=0.37065008555315027\n",
      "Gradient Descent(25/49): loss=0.3702310556245117\n",
      "Gradient Descent(26/49): loss=0.36984111086651444\n",
      "Gradient Descent(27/49): loss=0.36947812143594366\n",
      "Gradient Descent(28/49): loss=0.36914011916864853\n",
      "Gradient Descent(29/49): loss=0.36882528471607723\n",
      "Gradient Descent(30/49): loss=0.3685319357848986\n",
      "Gradient Descent(31/49): loss=0.36825851637240714\n",
      "Gradient Descent(32/49): loss=0.3680035869033711\n",
      "Gradient Descent(33/49): loss=0.3677658151847591\n",
      "Gradient Descent(34/49): loss=0.36754396810389484\n",
      "Gradient Descent(35/49): loss=0.3673369040034264\n",
      "Gradient Descent(36/49): loss=0.36714356567329964\n",
      "Gradient Descent(37/49): loss=0.36696297390587224\n",
      "Gradient Descent(38/49): loss=0.3667942215655628\n",
      "Gradient Descent(39/49): loss=0.3666364681290739\n",
      "Gradient Descent(40/49): loss=0.3664889346563702\n",
      "Gradient Descent(41/49): loss=0.3663508991562833\n",
      "Gradient Descent(42/49): loss=0.36622169231392115\n",
      "Gradient Descent(43/49): loss=0.366100693550032\n",
      "Gradient Descent(44/49): loss=0.3659873273851327\n",
      "Gradient Descent(45/49): loss=0.36588106008361937\n",
      "Gradient Descent(46/49): loss=0.36578139655524666\n",
      "Gradient Descent(47/49): loss=0.3656878774933163\n",
      "Gradient Descent(48/49): loss=0.3656000767306943\n",
      "Gradient Descent(49/49): loss=0.3655175987963774\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4315463505740243\n",
      "Gradient Descent(2/49): loss=0.40673923155005753\n",
      "Gradient Descent(3/49): loss=0.3967955473570536\n",
      "Gradient Descent(4/49): loss=0.39204894664012074\n",
      "Gradient Descent(5/49): loss=0.3891973336972344\n",
      "Gradient Descent(6/49): loss=0.3870960924656866\n",
      "Gradient Descent(7/49): loss=0.38534079571812996\n",
      "Gradient Descent(8/49): loss=0.3837834546830075\n",
      "Gradient Descent(9/49): loss=0.3823658968191324\n",
      "Gradient Descent(10/49): loss=0.3810619443112963\n",
      "Gradient Descent(11/49): loss=0.3798571938147262\n",
      "Gradient Descent(12/49): loss=0.37874187151262306\n",
      "Gradient Descent(13/49): loss=0.3777082634104377\n",
      "Gradient Descent(14/49): loss=0.37674975924596543\n",
      "Gradient Descent(15/49): loss=0.3758604724726897\n",
      "Gradient Descent(16/49): loss=0.3750350703549743\n",
      "Gradient Descent(17/49): loss=0.37426868378763267\n",
      "Gradient Descent(18/49): loss=0.37355684960476865\n",
      "Gradient Descent(19/49): loss=0.3728954677648121\n",
      "Gradient Descent(20/49): loss=0.3722807665084451\n",
      "Gradient Descent(21/49): loss=0.37170927255493985\n",
      "Gradient Descent(22/49): loss=0.3711777849344362\n",
      "Gradient Descent(23/49): loss=0.37068335168427624\n",
      "Gradient Descent(24/49): loss=0.37022324892203445\n",
      "Gradient Descent(25/49): loss=0.3697949619518036\n",
      "Gradient Descent(26/49): loss=0.3693961681423945\n",
      "Gradient Descent(27/49): loss=0.3690247213682307\n",
      "Gradient Descent(28/49): loss=0.3686786378397482\n",
      "Gradient Descent(29/49): loss=0.3683560831766157\n",
      "Gradient Descent(30/49): loss=0.3680553605975245\n",
      "Gradient Descent(31/49): loss=0.36777490011658537\n",
      "Gradient Descent(32/49): loss=0.36751324864971496\n",
      "Gradient Descent(33/49): loss=0.3672690609455398\n",
      "Gradient Descent(34/49): loss=0.36704109126481144\n",
      "Gradient Descent(35/49): loss=0.3668281857404532\n",
      "Gradient Descent(36/49): loss=0.36662927535741935\n",
      "Gradient Descent(37/49): loss=0.36644336949771283\n",
      "Gradient Descent(38/49): loss=0.3662695500013309\n",
      "Gradient Descent(39/49): loss=0.36610696569870715\n",
      "Gradient Descent(40/49): loss=0.3659548273744651\n",
      "Gradient Descent(41/49): loss=0.3658124031260908\n",
      "Gradient Descent(42/49): loss=0.3656790140845084\n",
      "Gradient Descent(43/49): loss=0.3655540304665704\n",
      "Gradient Descent(44/49): loss=0.3654368679321843\n",
      "Gradient Descent(45/49): loss=0.3653269842212387\n",
      "Gradient Descent(46/49): loss=0.36522387604768486\n",
      "Gradient Descent(47/49): loss=0.3651270762301081\n",
      "Gradient Descent(48/49): loss=0.3650361510399143\n",
      "Gradient Descent(49/49): loss=0.3649506977498703\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4317091512842971\n",
      "Gradient Descent(2/49): loss=0.40702373224434063\n",
      "Gradient Descent(3/49): loss=0.39714884729302086\n",
      "Gradient Descent(4/49): loss=0.3924386279091102\n",
      "Gradient Descent(5/49): loss=0.38960747731688544\n",
      "Gradient Descent(6/49): loss=0.38751945027104556\n",
      "Gradient Descent(7/49): loss=0.3857739329806056\n",
      "Gradient Descent(8/49): loss=0.3842244468887278\n",
      "Gradient Descent(9/49): loss=0.3828133868849014\n",
      "Gradient Descent(10/49): loss=0.3815148016883056\n",
      "Gradient Descent(11/49): loss=0.3803144086697018\n",
      "Gradient Descent(12/49): loss=0.37920252937824195\n",
      "Gradient Descent(13/49): loss=0.3781715418345242\n",
      "Gradient Descent(14/49): loss=0.3772149270871563\n",
      "Gradient Descent(15/49): loss=0.37632688694295396\n",
      "Gradient Descent(16/49): loss=0.3755021715887731\n",
      "Gradient Descent(17/49): loss=0.3747359878495336\n",
      "Gradient Descent(18/49): loss=0.37402394084850127\n",
      "Gradient Descent(19/49): loss=0.37336199119192853\n",
      "Gradient Descent(20/49): loss=0.3727464205163848\n",
      "Gradient Descent(21/49): loss=0.3721738022752493\n",
      "Gradient Descent(22/49): loss=0.37164097623968245\n",
      "Gradient Descent(23/49): loss=0.37114502586942544\n",
      "Gradient Descent(24/49): loss=0.37068325802615243\n",
      "Gradient Descent(25/49): loss=0.37025318466651475\n",
      "Gradient Descent(26/49): loss=0.3698525062463134\n",
      "Gradient Descent(27/49): loss=0.3694790966262775\n",
      "Gradient Descent(28/49): loss=0.36913098930961324\n",
      "Gradient Descent(29/49): loss=0.3688063648697239\n",
      "Gradient Descent(30/49): loss=0.36850353944752995\n",
      "Gradient Descent(31/49): loss=0.36822095421409085\n",
      "Gradient Descent(32/49): loss=0.36795716570722325\n",
      "Gradient Descent(33/49): loss=0.36771083696146467\n",
      "Gradient Descent(34/49): loss=0.36748072935965265\n",
      "Gradient Descent(35/49): loss=0.3672656951419859\n",
      "Gradient Descent(36/49): loss=0.3670646705149938\n",
      "Gradient Descent(37/49): loss=0.3668766693085572\n",
      "Gradient Descent(38/49): loss=0.3667007771341551\n",
      "Gradient Descent(39/49): loss=0.3665361460019568\n",
      "Gradient Descent(40/49): loss=0.366381989358336\n",
      "Gradient Descent(41/49): loss=0.36623757750891556\n",
      "Gradient Descent(42/49): loss=0.36610223339540876\n",
      "Gradient Descent(43/49): loss=0.3659753286973665\n",
      "Gradient Descent(44/49): loss=0.36585628023249017\n",
      "Gradient Descent(45/49): loss=0.36574454663147626\n",
      "Gradient Descent(46/49): loss=0.3656396252654378\n",
      "Gradient Descent(47/49): loss=0.365541049405829\n",
      "Gradient Descent(48/49): loss=0.3654483855985051\n",
      "Gradient Descent(49/49): loss=0.36536123123509734\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.43139070759420906\n",
      "Gradient Descent(2/49): loss=0.40577109861545446\n",
      "Gradient Descent(3/49): loss=0.3953629528536459\n",
      "Gradient Descent(4/49): loss=0.3904490566568352\n",
      "Gradient Descent(5/49): loss=0.3876008255283507\n",
      "Gradient Descent(6/49): loss=0.3855939944841832\n",
      "Gradient Descent(7/49): loss=0.38398144593421324\n",
      "Gradient Descent(8/49): loss=0.38259228321903965\n",
      "Gradient Descent(9/49): loss=0.38135524479994426\n",
      "Gradient Descent(10/49): loss=0.3802360695887423\n",
      "Gradient Descent(11/49): loss=0.37921504112169446\n",
      "Gradient Descent(12/49): loss=0.3782787711916766\n",
      "Gradient Descent(13/49): loss=0.3774170727291455\n",
      "Gradient Descent(14/49): loss=0.37662167948588776\n",
      "Gradient Descent(15/49): loss=0.37588565534004714\n",
      "Gradient Descent(16/49): loss=0.37520307522162955\n",
      "Gradient Descent(17/49): loss=0.3745688233518337\n",
      "Gradient Descent(18/49): loss=0.3739784495215575\n",
      "Gradient Descent(19/49): loss=0.3734280589811177\n",
      "Gradient Descent(20/49): loss=0.37291422468806307\n",
      "Gradient Descent(21/49): loss=0.37243391591659836\n",
      "Gradient Descent(22/49): loss=0.37198443952386273\n",
      "Gradient Descent(23/49): loss=0.37156339130215155\n",
      "Gradient Descent(24/49): loss=0.37116861549509966\n",
      "Gradient Descent(25/49): loss=0.3707981709791587\n",
      "Gradient Descent(26/49): loss=0.370450302915006\n",
      "Gradient Descent(27/49): loss=0.3701234189037178\n",
      "Gradient Descent(28/49): loss=0.36981606886297896\n",
      "Gradient Descent(29/49): loss=0.3695269279826056\n",
      "Gradient Descent(30/49): loss=0.36925478223472186\n",
      "Gradient Descent(31/49): loss=0.3689985160080559\n",
      "Gradient Descent(32/49): loss=0.36875710151242813\n",
      "Gradient Descent(33/49): loss=0.36852958966201593\n",
      "Gradient Descent(34/49): loss=0.3683151021971025\n",
      "Gradient Descent(35/49): loss=0.36811282484586744\n",
      "Gradient Descent(36/49): loss=0.36792200136210396\n",
      "Gradient Descent(37/49): loss=0.3677419283029168\n",
      "Gradient Descent(38/49): loss=0.36757195043360963\n",
      "Gradient Descent(39/49): loss=0.3674114566660149\n",
      "Gradient Descent(40/49): loss=0.3672598764522024\n",
      "Gradient Descent(41/49): loss=0.36711667656843006\n",
      "Gradient Descent(42/49): loss=0.36698135823486716\n",
      "Gradient Descent(43/49): loss=0.3668534545254378\n",
      "Gradient Descent(44/49): loss=0.36673252802941375\n",
      "Gradient Descent(45/49): loss=0.3666181687324301\n",
      "Gradient Descent(46/49): loss=0.3665099920896017\n",
      "Gradient Descent(47/49): loss=0.3664076372675733\n",
      "Gradient Descent(48/49): loss=0.36631076553580166\n",
      "Gradient Descent(49/49): loss=0.3662190587902466\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4284993039243225\n",
      "Gradient Descent(2/49): loss=0.404605050164866\n",
      "Gradient Descent(3/49): loss=0.3955394249881494\n",
      "Gradient Descent(4/49): loss=0.39125025106874944\n",
      "Gradient Descent(5/49): loss=0.38858985071761176\n",
      "Gradient Descent(6/49): loss=0.3865544300059746\n",
      "Gradient Descent(7/49): loss=0.38481509682097276\n",
      "Gradient Descent(8/49): loss=0.3832579675322551\n",
      "Gradient Descent(9/49): loss=0.38183872389530726\n",
      "Gradient Descent(10/49): loss=0.38053619748766376\n",
      "Gradient Descent(11/49): loss=0.3793373934107265\n",
      "Gradient Descent(12/49): loss=0.37823257163674573\n",
      "Gradient Descent(13/49): loss=0.3772135755261453\n",
      "Gradient Descent(14/49): loss=0.37627322341675234\n",
      "Gradient Descent(15/49): loss=0.37540505690691633\n",
      "Gradient Descent(16/49): loss=0.3746032144595925\n",
      "Gradient Descent(17/49): loss=0.3738623529034323\n",
      "Gradient Descent(18/49): loss=0.373177589930538\n",
      "Gradient Descent(19/49): loss=0.3725444576195978\n",
      "Gradient Descent(20/49): loss=0.3719588628885409\n",
      "Gradient Descent(21/49): loss=0.3714170529409545\n",
      "Gradient Descent(22/49): loss=0.3709155846369048\n",
      "Gradient Descent(23/49): loss=0.3704512971087758\n",
      "Gradient Descent(24/49): loss=0.3700212871424065\n",
      "Gradient Descent(25/49): loss=0.3696228869592347\n",
      "Gradient Descent(26/49): loss=0.3692536441088885\n",
      "Gradient Descent(27/49): loss=0.3689113032325558\n",
      "Gradient Descent(28/49): loss=0.36859378949470917\n",
      "Gradient Descent(29/49): loss=0.3682991935092996\n",
      "Gradient Descent(30/49): loss=0.36802575760920875\n",
      "Gradient Descent(31/49): loss=0.36777186332626083\n",
      "Gradient Descent(32/49): loss=0.3675360199645619\n",
      "Gradient Descent(33/49): loss=0.3673168541630575\n",
      "Gradient Descent(34/49): loss=0.36711310035449585\n",
      "Gradient Descent(35/49): loss=0.36692359203777347\n",
      "Gradient Descent(36/49): loss=0.366747253789226\n",
      "Gradient Descent(37/49): loss=0.36658309394595745\n",
      "Gradient Descent(38/49): loss=0.36643019790096926\n",
      "Gradient Descent(39/49): loss=0.366287721955763\n",
      "Gradient Descent(40/49): loss=0.3661548876813388\n",
      "Gradient Descent(41/49): loss=0.36603097674320845\n",
      "Gradient Descent(42/49): loss=0.3659153261502227\n",
      "Gradient Descent(43/49): loss=0.36580732389077053\n",
      "Gradient Descent(44/49): loss=0.36570640492326745\n",
      "Gradient Descent(45/49): loss=0.3656120474908788\n",
      "Gradient Descent(46/49): loss=0.3655237697331401\n",
      "Gradient Descent(47/49): loss=0.36544112656959327\n",
      "Gradient Descent(48/49): loss=0.3653637068327652\n",
      "Gradient Descent(49/49): loss=0.36529113062981855\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.428429245228001\n",
      "Gradient Descent(2/49): loss=0.4045104177412345\n",
      "Gradient Descent(3/49): loss=0.3954266997729007\n",
      "Gradient Descent(4/49): loss=0.39111803459712424\n",
      "Gradient Descent(5/49): loss=0.3884368179095163\n",
      "Gradient Descent(6/49): loss=0.38638019022966213\n",
      "Gradient Descent(7/49): loss=0.3846199090180902\n",
      "Gradient Descent(8/49): loss=0.38304244451558594\n",
      "Gradient Descent(9/49): loss=0.3816036462332957\n",
      "Gradient Descent(10/49): loss=0.38028240866621105\n",
      "Gradient Descent(11/49): loss=0.3790657440116741\n",
      "Gradient Descent(12/49): loss=0.37794389143047463\n",
      "Gradient Descent(13/49): loss=0.3769086609415001\n",
      "Gradient Descent(14/49): loss=0.3759528331795782\n",
      "Gradient Descent(15/49): loss=0.3750699117973946\n",
      "Gradient Descent(16/49): loss=0.3742539989703096\n",
      "Gradient Descent(17/49): loss=0.37349971762242906\n",
      "Gradient Descent(18/49): loss=0.3728021540551918\n",
      "Gradient Descent(19/49): loss=0.37215681132453643\n",
      "Gradient Descent(20/49): loss=0.371559569439006\n",
      "Gradient Descent(21/49): loss=0.371006650528458\n",
      "Gradient Descent(22/49): loss=0.37049458795435214\n",
      "Gradient Descent(23/49): loss=0.37002019869857317\n",
      "Gradient Descent(24/49): loss=0.36958055855514327\n",
      "Gradient Descent(25/49): loss=0.36917297975870444\n",
      "Gradient Descent(26/49): loss=0.36879499075489586\n",
      "Gradient Descent(27/49): loss=0.3684443178679087\n",
      "Gradient Descent(28/49): loss=0.3681188686578844\n",
      "Gradient Descent(29/49): loss=0.3678167167898717\n",
      "Gradient Descent(30/49): loss=0.36753608825937445\n",
      "Gradient Descent(31/49): loss=0.36727534883867885\n",
      "Gradient Descent(32/49): loss=0.36703299262420225\n",
      "Gradient Descent(33/49): loss=0.36680763157874424\n",
      "Gradient Descent(34/49): loss=0.3665979859742353\n",
      "Gradient Descent(35/49): loss=0.3664028756507406\n",
      "Gradient Descent(36/49): loss=0.3662212120163298\n",
      "Gradient Descent(37/49): loss=0.36605199072020655\n",
      "Gradient Descent(38/49): loss=0.3658942849383276\n",
      "Gradient Descent(39/49): loss=0.36574723921680624\n",
      "Gradient Descent(40/49): loss=0.36561006382375844\n",
      "Gradient Descent(41/49): loss=0.3654820295650275\n",
      "Gradient Descent(42/49): loss=0.3653624630234793\n",
      "Gradient Descent(43/49): loss=0.3652507421853616\n",
      "Gradient Descent(44/49): loss=0.36514629242062263\n",
      "Gradient Descent(45/49): loss=0.36504858278714025\n",
      "Gradient Descent(46/49): loss=0.36495712263154384\n",
      "Gradient Descent(47/49): loss=0.3648714584617881\n",
      "Gradient Descent(48/49): loss=0.3647911710688481\n",
      "Gradient Descent(49/49): loss=0.36471587287691554\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42860211355592315\n",
      "Gradient Descent(2/49): loss=0.4048079110533483\n",
      "Gradient Descent(3/49): loss=0.39579025240101673\n",
      "Gradient Descent(4/49): loss=0.3915148200760544\n",
      "Gradient Descent(5/49): loss=0.3888521262556812\n",
      "Gradient Descent(6/49): loss=0.38680778407426425\n",
      "Gradient Descent(7/49): loss=0.38505686385895876\n",
      "Gradient Descent(8/49): loss=0.3834870009990749\n",
      "Gradient Descent(9/49): loss=0.3820544457476045\n",
      "Gradient Descent(10/49): loss=0.3807382619219626\n",
      "Gradient Descent(11/49): loss=0.37952557721108354\n",
      "Gradient Descent(12/49): loss=0.37840674032825655\n",
      "Gradient Descent(13/49): loss=0.377373671673246\n",
      "Gradient Descent(14/49): loss=0.37641925977179125\n",
      "Gradient Descent(15/49): loss=0.375537109725802\n",
      "Gradient Descent(16/49): loss=0.37472141626004213\n",
      "Gradient Descent(17/49): loss=0.37396688495745783\n",
      "Gradient Descent(18/49): loss=0.37326867489388094\n",
      "Gradient Descent(19/49): loss=0.3726223525891045\n",
      "Gradient Descent(20/49): loss=0.3720238530502335\n",
      "Gradient Descent(21/49): loss=0.3714694458757589\n",
      "Gradient Descent(22/49): loss=0.3709557052897427\n",
      "Gradient Descent(23/49): loss=0.37047948339139936\n",
      "Gradient Descent(24/49): loss=0.3700378861221967\n",
      "Gradient Descent(25/49): loss=0.3696282515786359\n",
      "Gradient Descent(26/49): loss=0.369248130378846\n",
      "Gradient Descent(27/49): loss=0.36889526784545806\n",
      "Gradient Descent(28/49): loss=0.36856758780622245\n",
      "Gradient Descent(29/49): loss=0.3682631778431064\n",
      "Gradient Descent(30/49): loss=0.3679802758434359\n",
      "Gradient Descent(31/49): loss=0.3677172577249992\n",
      "Gradient Descent(32/49): loss=0.3674726262221713\n",
      "Gradient Descent(33/49): loss=0.36724500063285265\n",
      "Gradient Descent(34/49): loss=0.36703310743689965\n",
      "Gradient Descent(35/49): loss=0.3668357717061302\n",
      "Gradient Descent(36/49): loss=0.36665190923419677\n",
      "Gradient Descent(37/49): loss=0.36648051932183046\n",
      "Gradient Descent(38/49): loss=0.3663206781593207\n",
      "Gradient Descent(39/49): loss=0.3661715327537523\n",
      "Gradient Descent(40/49): loss=0.3660322953535389\n",
      "Gradient Descent(41/49): loss=0.3659022383272817\n",
      "Gradient Descent(42/49): loss=0.36578068945799375\n",
      "Gradient Descent(43/49): loss=0.36566702761732783\n",
      "Gradient Descent(44/49): loss=0.36556067878767623\n",
      "Gradient Descent(45/49): loss=0.3654611124029154\n",
      "Gradient Descent(46/49): loss=0.3653678379811917\n",
      "Gradient Descent(47/49): loss=0.3652804020254995\n",
      "Gradient Descent(48/49): loss=0.36519838516994524\n",
      "Gradient Descent(49/49): loss=0.3651213995515145\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42822826594941105\n",
      "Gradient Descent(2/49): loss=0.4034499738477638\n",
      "Gradient Descent(3/49): loss=0.39392956577267857\n",
      "Gradient Descent(4/49): loss=0.38949641452348094\n",
      "Gradient Descent(5/49): loss=0.3868559374104509\n",
      "Gradient Descent(6/49): loss=0.3849234158917554\n",
      "Gradient Descent(7/49): loss=0.38332955855944734\n",
      "Gradient Descent(8/49): loss=0.38193940166438295\n",
      "Gradient Descent(9/49): loss=0.3806965439545556\n",
      "Gradient Descent(10/49): loss=0.37957224448351257\n",
      "Gradient Descent(11/49): loss=0.3785484884658059\n",
      "Gradient Descent(12/49): loss=0.3776121445461091\n",
      "Gradient Descent(13/49): loss=0.3767527969969679\n",
      "Gradient Descent(14/49): loss=0.3759618308598147\n",
      "Gradient Descent(15/49): loss=0.3752319690489882\n",
      "Gradient Descent(16/49): loss=0.37455699023745537\n",
      "Gradient Descent(17/49): loss=0.37393153148886377\n",
      "Gradient Descent(18/49): loss=0.3733509386771916\n",
      "Gradient Descent(19/49): loss=0.3728111484125276\n",
      "Gradient Descent(20/49): loss=0.37230859293461793\n",
      "Gradient Descent(21/49): loss=0.3718401226828828\n",
      "Gradient Descent(22/49): loss=0.37140294284583236\n",
      "Gradient Descent(23/49): loss=0.37099456111935564\n",
      "Gradient Descent(24/49): loss=0.37061274452017157\n",
      "Gradient Descent(25/49): loss=0.3702554835486025\n",
      "Gradient Descent(26/49): loss=0.3699209623361616\n",
      "Gradient Descent(27/49): loss=0.36960753368027444\n",
      "Gradient Descent(28/49): loss=0.36931369807993436\n",
      "Gradient Descent(29/49): loss=0.3690380860549706\n",
      "Gradient Descent(30/49): loss=0.36877944316710803\n",
      "Gradient Descent(31/49): loss=0.3685366172700517\n",
      "Gradient Descent(32/49): loss=0.36830854760379655\n",
      "Gradient Descent(33/49): loss=0.3680942554194345\n",
      "Gradient Descent(34/49): loss=0.3678928358782589\n",
      "Gradient Descent(35/49): loss=0.3677034510155645\n",
      "Gradient Descent(36/49): loss=0.3675253235973518\n",
      "Gradient Descent(37/49): loss=0.36735773172885017\n",
      "Gradient Descent(38/49): loss=0.3672000040987453\n",
      "Gradient Descent(39/49): loss=0.36705151576331885\n",
      "Gradient Descent(40/49): loss=0.36691168439127714\n",
      "Gradient Descent(41/49): loss=0.3667799669035677\n",
      "Gradient Descent(42/49): loss=0.36665585645353255\n",
      "Gradient Descent(43/49): loss=0.3665388797017918\n",
      "Gradient Descent(44/49): loss=0.36642859434766734\n",
      "Gradient Descent(45/49): loss=0.36632458688504627\n",
      "Gradient Descent(46/49): loss=0.36622647055559876\n",
      "Gradient Descent(47/49): loss=0.3661338834763913\n",
      "Gradient Descent(48/49): loss=0.36604648692236147\n",
      "Gradient Descent(49/49): loss=0.3659639637469448\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42552168528481743\n",
      "Gradient Descent(2/49): loss=0.40263113318866595\n",
      "Gradient Descent(3/49): loss=0.39437456235327223\n",
      "Gradient Descent(4/49): loss=0.3904485709419528\n",
      "Gradient Descent(5/49): loss=0.38790807493798496\n",
      "Gradient Descent(6/49): loss=0.3858917092821415\n",
      "Gradient Descent(7/49): loss=0.3841375059190075\n",
      "Gradient Descent(8/49): loss=0.3825585006226158\n",
      "Gradient Descent(9/49): loss=0.38111999940668895\n",
      "Gradient Descent(10/49): loss=0.379803679417486\n",
      "Gradient Descent(11/49): loss=0.37859691672697177\n",
      "Gradient Descent(12/49): loss=0.3774895148977863\n",
      "Gradient Descent(13/49): loss=0.3764726350625088\n",
      "Gradient Descent(14/49): loss=0.37553839715355997\n",
      "Gradient Descent(15/49): loss=0.37467969574925314\n",
      "Gradient Descent(16/49): loss=0.37389009115727345\n",
      "Gradient Descent(17/49): loss=0.3731637313421317\n",
      "Gradient Descent(18/49): loss=0.37249528944034715\n",
      "Gradient Descent(19/49): loss=0.3718799109316828\n",
      "Gradient Descent(20/49): loss=0.3713131677406715\n",
      "Gradient Descent(21/49): loss=0.3707910177720434\n",
      "Gradient Descent(22/49): loss=0.37030976892627465\n",
      "Gradient Descent(23/49): loss=0.3698660469191144\n",
      "Gradient Descent(24/49): loss=0.369456766390571\n",
      "Gradient Descent(25/49): loss=0.36907910489283957\n",
      "Gradient Descent(26/49): loss=0.3687304794187957\n",
      "Gradient Descent(27/49): loss=0.3684085251857181\n",
      "Gradient Descent(28/49): loss=0.36811107642966595\n",
      "Gradient Descent(29/49): loss=0.36783614899842126\n",
      "Gradient Descent(30/49): loss=0.36758192455749433\n",
      "Gradient Descent(31/49): loss=0.3673467362459149\n",
      "Gradient Descent(32/49): loss=0.3671290556374063\n",
      "Gradient Descent(33/49): loss=0.3669274808787464\n",
      "Gradient Descent(34/49): loss=0.36674072589117196\n",
      "Gradient Descent(35/49): loss=0.3665676105329271\n",
      "Gradient Descent(36/49): loss=0.36640705163181014\n",
      "Gradient Descent(37/49): loss=0.36625805480602025\n",
      "Gradient Descent(38/49): loss=0.3661197069999657\n",
      "Gradient Descent(39/49): loss=0.3659911696690888\n",
      "Gradient Descent(40/49): loss=0.3658716725543379\n",
      "Gradient Descent(41/49): loss=0.36576050799275694\n",
      "Gradient Descent(42/49): loss=0.36565702571587755\n",
      "Gradient Descent(43/49): loss=0.36556062809225304\n",
      "Gradient Descent(44/49): loss=0.36547076577463855\n",
      "Gradient Descent(45/49): loss=0.3653869337160513\n",
      "Gradient Descent(46/49): loss=0.365308667522297\n",
      "Gradient Descent(47/49): loss=0.36523554011155107\n",
      "Gradient Descent(48/49): loss=0.3651671586542996\n",
      "Gradient Descent(49/49): loss=0.36510316176937424\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42544875726351894\n",
      "Gradient Descent(2/49): loss=0.40253419214165803\n",
      "Gradient Descent(3/49): loss=0.3942586220966417\n",
      "Gradient Descent(4/49): loss=0.3903116204365538\n",
      "Gradient Descent(5/49): loss=0.38774882982291703\n",
      "Gradient Descent(6/49): loss=0.3857099886453089\n",
      "Gradient Descent(7/49): loss=0.38393378328383915\n",
      "Gradient Descent(8/49): loss=0.38233357065832646\n",
      "Gradient Descent(9/49): loss=0.3808747893565629\n",
      "Gradient Descent(10/49): loss=0.37953915031578433\n",
      "Gradient Descent(11/49): loss=0.37831401471589304\n",
      "Gradient Descent(12/49): loss=0.37718914940458453\n",
      "Gradient Descent(13/49): loss=0.3761556708485173\n",
      "Gradient Descent(14/49): loss=0.3752056531633159\n",
      "Gradient Descent(15/49): loss=0.37433194701402994\n",
      "Gradient Descent(16/49): loss=0.37352807183735526\n",
      "Gradient Descent(17/49): loss=0.37278813797682775\n",
      "Gradient Descent(18/49): loss=0.3721067839914135\n",
      "Gradient Descent(19/49): loss=0.3714791234683196\n",
      "Gradient Descent(20/49): loss=0.37090069874127624\n",
      "Gradient Descent(21/49): loss=0.3703674400742168\n",
      "Gradient Descent(22/49): loss=0.3698756293770599\n",
      "Gradient Descent(23/49): loss=0.3694218677805793\n",
      "Gradient Descent(24/49): loss=0.3690030465512529\n",
      "Gradient Descent(25/49): loss=0.36861632092813984\n",
      "Gradient Descent(26/49): loss=0.3682590865355205\n",
      "Gradient Descent(27/49): loss=0.36792895807866616\n",
      "Gradient Descent(28/49): loss=0.3676237500718874\n",
      "Gradient Descent(29/49): loss=0.36734145938156376\n",
      "Gradient Descent(30/49): loss=0.3670802493944607\n",
      "Gradient Descent(31/49): loss=0.3668384356447466\n",
      "Gradient Descent(32/49): loss=0.366614472752728\n",
      "Gradient Descent(33/49): loss=0.36640694254513495\n",
      "Gradient Descent(34/49): loss=0.3662145432413245\n",
      "Gradient Descent(35/49): loss=0.36603607960240164\n",
      "Gradient Descent(36/49): loss=0.36587045395130074\n",
      "Gradient Descent(37/49): loss=0.3657166579815702\n",
      "Gradient Descent(38/49): loss=0.3655737652811223\n",
      "Gradient Descent(39/49): loss=0.3654409245047603\n",
      "Gradient Descent(40/49): loss=0.36531735313595376\n",
      "Gradient Descent(41/49): loss=0.3652023317842657\n",
      "Gradient Descent(42/49): loss=0.36509519897009457\n",
      "Gradient Descent(43/49): loss=0.3649953463530911\n",
      "Gradient Descent(44/49): loss=0.36490221436479964\n",
      "Gradient Descent(45/49): loss=0.3648152882098236\n",
      "Gradient Descent(46/49): loss=0.3647340942031727\n",
      "Gradient Descent(47/49): loss=0.36465819641445896\n",
      "Gradient Descent(48/49): loss=0.3645871935923253\n",
      "Gradient Descent(49/49): loss=0.36452071634491673\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4256316805706501\n",
      "Gradient Descent(2/49): loss=0.4028439639265576\n",
      "Gradient Descent(3/49): loss=0.3946312452493941\n",
      "Gradient Descent(4/49): loss=0.39071446463967896\n",
      "Gradient Descent(5/49): loss=0.38816859063910036\n",
      "Gradient Descent(6/49): loss=0.38614136093106055\n",
      "Gradient Descent(7/49): loss=0.38437421968604935\n",
      "Gradient Descent(8/49): loss=0.38278138103474185\n",
      "Gradient Descent(9/49): loss=0.3813285652127984\n",
      "Gradient Descent(10/49): loss=0.3799976281725679\n",
      "Gradient Descent(11/49): loss=0.3787760588818239\n",
      "Gradient Descent(12/49): loss=0.37765375529898276\n",
      "Gradient Descent(13/49): loss=0.3766219645468026\n",
      "Gradient Descent(14/49): loss=0.3756728846081433\n",
      "Gradient Descent(15/49): loss=0.3747994791517943\n",
      "Gradient Descent(16/49): loss=0.37399536806081546\n",
      "Gradient Descent(17/49): loss=0.3732547494612679\n",
      "Gradient Descent(18/49): loss=0.372572337795661\n",
      "Gradient Descent(19/49): loss=0.37194331179586604\n",
      "Gradient Descent(20/49): loss=0.3713632694786888\n",
      "Gradient Descent(21/49): loss=0.3708281885761205\n",
      "Gradient Descent(22/49): loss=0.3703343913966391\n",
      "Gradient Descent(23/49): loss=0.3698785134177513\n",
      "Gradient Descent(24/49): loss=0.3694574750869587\n",
      "Gradient Descent(25/49): loss=0.36906845642086217\n",
      "Gradient Descent(26/49): loss=0.3687088740686696\n",
      "Gradient Descent(27/49): loss=0.36837636056140916\n",
      "Gradient Descent(28/49): loss=0.36806874550956414\n",
      "Gradient Descent(29/49): loss=0.36778403854424135\n",
      "Gradient Descent(30/49): loss=0.36752041382311657\n",
      "Gradient Descent(31/49): loss=0.3672761959440129\n",
      "Gradient Descent(32/49): loss=0.36704984712717703\n",
      "Gradient Descent(33/49): loss=0.36683995554288323\n",
      "Gradient Descent(34/49): loss=0.36664522467444083\n",
      "Gradient Descent(35/49): loss=0.36646446361839546\n",
      "Gradient Descent(36/49): loss=0.36629657823397205\n",
      "Gradient Descent(37/49): loss=0.3661405630628475\n",
      "Gradient Descent(38/49): loss=0.3659954939483241\n",
      "Gradient Descent(39/49): loss=0.36586052129005475\n",
      "Gradient Descent(40/49): loss=0.3657348638767661\n",
      "Gradient Descent(41/49): loss=0.3656178032450296\n",
      "Gradient Descent(42/49): loss=0.36550867851713587\n",
      "Gradient Descent(43/49): loss=0.36540688167560426\n",
      "Gradient Descent(44/49): loss=0.36531185323586973\n",
      "Gradient Descent(45/49): loss=0.3652230782822816\n",
      "Gradient Descent(46/49): loss=0.3651400828357866\n",
      "Gradient Descent(47/49): loss=0.3650624305245698\n",
      "Gradient Descent(48/49): loss=0.36498971953155274\n",
      "Gradient Descent(49/49): loss=0.36492157979500256\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4251982524376208\n",
      "Gradient Descent(2/49): loss=0.40138514285346727\n",
      "Gradient Descent(3/49): loss=0.39270741472667037\n",
      "Gradient Descent(4/49): loss=0.3886791977498809\n",
      "Gradient Descent(5/49): loss=0.3861924702085147\n",
      "Gradient Descent(6/49): loss=0.38430515440665514\n",
      "Gradient Descent(7/49): loss=0.3827170600385405\n",
      "Gradient Descent(8/49): loss=0.381321505143509\n",
      "Gradient Descent(9/49): loss=0.38007236791970733\n",
      "Gradient Descent(10/49): loss=0.3789440931293431\n",
      "Gradient Descent(11/49): loss=0.37791928935308244\n",
      "Gradient Descent(12/49): loss=0.37698462291197343\n",
      "Gradient Descent(13/49): loss=0.3761292726467391\n",
      "Gradient Descent(14/49): loss=0.3753442193648316\n",
      "Gradient Descent(15/49): loss=0.3746218381900663\n",
      "Gradient Descent(16/49): loss=0.37395562215732175\n",
      "Gradient Descent(17/49): loss=0.37333997633826216\n",
      "Gradient Descent(18/49): loss=0.3727700574066684\n",
      "Gradient Descent(19/49): loss=0.37224164592847403\n",
      "Gradient Descent(20/49): loss=0.3717510435884885\n",
      "Gradient Descent(21/49): loss=0.3712949899339756\n",
      "Gradient Descent(22/49): loss=0.37087059459352373\n",
      "Gradient Descent(23/49): loss=0.3704752818534929\n",
      "Gradient Descent(24/49): loss=0.37010674514677605\n",
      "Gradient Descent(25/49): loss=0.3697629095193735\n",
      "Gradient Descent(26/49): loss=0.3694419005366285\n",
      "Gradient Descent(27/49): loss=0.369142018402053\n",
      "Gradient Descent(28/49): loss=0.3688617163073609\n",
      "Gradient Descent(29/49): loss=0.36859958222718864\n",
      "Gradient Descent(30/49): loss=0.36835432352692987\n",
      "Gradient Descent(31/49): loss=0.3681247538756061\n",
      "Gradient Descent(32/49): loss=0.36790978205426633\n",
      "Gradient Descent(33/49): loss=0.36770840232922\n",
      "Gradient Descent(34/49): loss=0.3675196861224982\n",
      "Gradient Descent(35/49): loss=0.36734277476251065\n",
      "Gradient Descent(36/49): loss=0.3671768731384519\n",
      "Gradient Descent(37/49): loss=0.3670212441146414\n",
      "Gradient Descent(38/49): loss=0.36687520358722786\n",
      "Gradient Descent(39/49): loss=0.36673811608685236\n",
      "Gradient Descent(40/49): loss=0.36660939084794747\n",
      "Gradient Descent(41/49): loss=0.3664884782791614\n",
      "Gradient Descent(42/49): loss=0.3663748667805882\n",
      "Gradient Descent(43/49): loss=0.36626807986256915\n",
      "Gradient Descent(44/49): loss=0.36616767352822666\n",
      "Gradient Descent(45/49): loss=0.36607323388791735\n",
      "Gradient Descent(46/49): loss=0.3659843749787276\n",
      "Gradient Descent(47/49): loss=0.3659007367661783\n",
      "Gradient Descent(48/49): loss=0.36582198330863963\n",
      "Gradient Descent(49/49): loss=0.36574780106770405\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.42268054494076385\n",
      "Gradient Descent(2/49): loss=0.4008866177338421\n",
      "Gradient Descent(3/49): loss=0.3933767367895098\n",
      "Gradient Descent(4/49): loss=0.38974310057161216\n",
      "Gradient Descent(5/49): loss=0.3872802422319952\n",
      "Gradient Descent(6/49): loss=0.3852638746953224\n",
      "Gradient Descent(7/49): loss=0.38348857666662667\n",
      "Gradient Descent(8/49): loss=0.38188746343495866\n",
      "Gradient Descent(9/49): loss=0.380431958666572\n",
      "Gradient Descent(10/49): loss=0.37910498127878006\n",
      "Gradient Descent(11/49): loss=0.3778935854976697\n",
      "Gradient Descent(12/49): loss=0.3767868256505637\n",
      "Gradient Descent(13/49): loss=0.37577505662160243\n",
      "Gradient Descent(14/49): loss=0.3748496472041265\n",
      "Gradient Descent(15/49): loss=0.3740028237773374\n",
      "Gradient Descent(16/49): loss=0.37322756269713536\n",
      "Gradient Descent(17/49): loss=0.3725175057765015\n",
      "Gradient Descent(18/49): loss=0.3718668895894611\n",
      "Gradient Descent(19/49): loss=0.37127048453261474\n",
      "Gradient Descent(20/49): loss=0.370723541462166\n",
      "Gradient Descent(21/49): loss=0.3702217445301594\n",
      "Gradient Descent(22/49): loss=0.36976116925062263\n",
      "Gradient Descent(23/49): loss=0.36933824506192087\n",
      "Gradient Descent(24/49): loss=0.36894972180263447\n",
      "Gradient Descent(25/49): loss=0.3685926396226798\n",
      "Gradient Descent(26/49): loss=0.36826430192793286\n",
      "Gradient Descent(27/49): loss=0.367962251015385\n",
      "Gradient Descent(28/49): loss=0.3676842461026367\n",
      "Gradient Descent(29/49): loss=0.3674282434937879\n",
      "Gradient Descent(30/49): loss=0.3671923786557253\n",
      "Gradient Descent(31/49): loss=0.3669749500058604\n",
      "Gradient Descent(32/49): loss=0.366774404235555\n",
      "Gradient Descent(33/49): loss=0.366589323013488\n",
      "Gradient Descent(34/49): loss=0.3664184109306105\n",
      "Gradient Descent(35/49): loss=0.36626048456352456\n",
      "Gradient Descent(36/49): loss=0.36611446254642804\n",
      "Gradient Descent(37/49): loss=0.365979356553476\n",
      "Gradient Descent(38/49): loss=0.3658542631037197\n",
      "Gradient Descent(39/49): loss=0.3657383561099084\n",
      "Gradient Descent(40/49): loss=0.3656308801005158\n",
      "Gradient Descent(41/49): loss=0.3655311440515159\n",
      "Gradient Descent(42/49): loss=0.36543851577081254\n",
      "Gradient Descent(43/49): loss=0.365352416783897\n",
      "Gradient Descent(44/49): loss=0.3652723176743773\n",
      "Gradient Descent(45/49): loss=0.3651977338375422\n",
      "Gradient Descent(46/49): loss=0.3651282216091748\n",
      "Gradient Descent(47/49): loss=0.36506337473545564\n",
      "Gradient Descent(48/49): loss=0.36500282115304994\n",
      "Gradient Descent(49/49): loss=0.3649462200513956\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.422604886680578\n",
      "Gradient Descent(2/49): loss=0.400787513950619\n",
      "Gradient Descent(3/49): loss=0.3932575479045667\n",
      "Gradient Descent(4/49): loss=0.3896013327245386\n",
      "Gradient Descent(5/49): loss=0.3871147396140677\n",
      "Gradient Descent(6/49): loss=0.38507469584280624\n",
      "Gradient Descent(7/49): loss=0.38327641440980065\n",
      "Gradient Descent(8/49): loss=0.3816532901953397\n",
      "Gradient Descent(9/49): loss=0.38017684179467165\n",
      "Gradient Descent(10/49): loss=0.37882999297661357\n",
      "Gradient Descent(11/49): loss=0.37759976230026693\n",
      "Gradient Descent(12/49): loss=0.37647515289618416\n",
      "Gradient Descent(13/49): loss=0.3754464649040605\n",
      "Gradient Descent(14/49): loss=0.3745050141215593\n",
      "Gradient Descent(15/49): loss=0.37364297766415694\n",
      "Gradient Descent(16/49): loss=0.3728532867800776\n",
      "Gradient Descent(17/49): loss=0.3721295420832214\n",
      "Gradient Descent(18/49): loss=0.37146594237189884\n",
      "Gradient Descent(19/49): loss=0.3708572231685722\n",
      "Gradient Descent(20/49): loss=0.37029860288330624\n",
      "Gradient Descent(21/49): loss=0.3697857352529923\n",
      "Gradient Descent(22/49): loss=0.3693146670894399\n",
      "Gradient Descent(23/49): loss=0.3688818005942833\n",
      "Gradient Descent(24/49): loss=0.3684838596462219\n",
      "Gradient Descent(25/49): loss=0.3681178595704347\n",
      "Gradient Descent(26/49): loss=0.3677810799778515\n",
      "Gradient Descent(27/49): loss=0.36747104032245037\n",
      "Gradient Descent(28/49): loss=0.36718547787323835\n",
      "Gradient Descent(29/49): loss=0.36692232783735673\n",
      "Gradient Descent(30/49): loss=0.36667970540396866\n",
      "Gradient Descent(31/49): loss=0.36645588950669683\n",
      "Gradient Descent(32/49): loss=0.36624930812638895\n",
      "Gradient Descent(33/49): loss=0.36605852497666475\n",
      "Gradient Descent(34/49): loss=0.3658822274325984\n",
      "Gradient Descent(35/49): loss=0.36571921557846804\n",
      "Gradient Descent(36/49): loss=0.3655683922641057\n",
      "Gradient Descent(37/49): loss=0.36542875407130926\n",
      "Gradient Descent(38/49): loss=0.3652993831022575\n",
      "Gradient Descent(39/49): loss=0.3651794395111063\n",
      "Gradient Descent(40/49): loss=0.36506815470810405\n",
      "Gradient Descent(41/49): loss=0.36496482517279677\n",
      "Gradient Descent(42/49): loss=0.3648688068192952\n",
      "Gradient Descent(43/49): loss=0.3647795098622855\n",
      "Gradient Descent(44/49): loss=0.3646963941375337\n",
      "Gradient Descent(45/49): loss=0.36461896483517003\n",
      "Gradient Descent(46/49): loss=0.36454676860807944\n",
      "Gradient Descent(47/49): loss=0.3644793900213533\n",
      "Gradient Descent(48/49): loss=0.364416448311999\n",
      "Gradient Descent(49/49): loss=0.36435759443101995\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.422797852328478\n",
      "Gradient Descent(2/49): loss=0.40110883883114135\n",
      "Gradient Descent(3/49): loss=0.3936381496438567\n",
      "Gradient Descent(4/49): loss=0.3900093622876663\n",
      "Gradient Descent(5/49): loss=0.3875384051348084\n",
      "Gradient Descent(6/49): loss=0.3855095011149851\n",
      "Gradient Descent(7/49): loss=0.383720058234548\n",
      "Gradient Descent(8/49): loss=0.3821040752762061\n",
      "Gradient Descent(9/49): loss=0.380633280082311\n",
      "Gradient Descent(10/49): loss=0.3792907435367257\n",
      "Gradient Descent(11/49): loss=0.3780636357936578\n",
      "Gradient Descent(12/49): loss=0.3769411160450001\n",
      "Gradient Descent(13/49): loss=0.375913635076966\n",
      "Gradient Descent(14/49): loss=0.37497264690020554\n",
      "Gradient Descent(15/49): loss=0.3741104512772847\n",
      "Gradient Descent(16/49): loss=0.37332008603091693\n",
      "Gradient Descent(17/49): loss=0.3725952431848901\n",
      "Gradient Descent(18/49): loss=0.37193019931584204\n",
      "Gradient Descent(19/49): loss=0.3713197558059084\n",
      "Gradient Descent(20/49): loss=0.3707591866720789\n",
      "Gradient Descent(21/49): loss=0.37024419252221485\n",
      "Gradient Descent(22/49): loss=0.3697708596355977\n",
      "Gradient Descent(23/49): loss=0.3693356234244528\n",
      "Gradient Descent(24/49): loss=0.36893523569600406\n",
      "Gradient Descent(25/49): loss=0.3685667352448729\n",
      "Gradient Descent(26/49): loss=0.3682274213845459\n",
      "Gradient Descent(27/49): loss=0.36791483008590825\n",
      "Gradient Descent(28/49): loss=0.36762671243716183\n",
      "Gradient Descent(29/49): loss=0.3673610151768268\n",
      "Gradient Descent(30/49): loss=0.36711586308242466\n",
      "Gradient Descent(31/49): loss=0.36688954302345345\n",
      "Gradient Descent(32/49): loss=0.3666804895094718\n",
      "Gradient Descent(33/49): loss=0.36648727158323585\n",
      "Gradient Descent(34/49): loss=0.36630858092544816\n",
      "Gradient Descent(35/49): loss=0.36614322105218444\n",
      "Gradient Descent(36/49): loss=0.36599009749877826\n",
      "Gradient Descent(37/49): loss=0.36584820889514696\n",
      "Gradient Descent(38/49): loss=0.365716638847418\n",
      "Gradient Descent(39/49): loss=0.3655945485494606\n",
      "Gradient Descent(40/49): loss=0.3654811700556877\n",
      "Gradient Descent(41/49): loss=0.36537580015337817\n",
      "Gradient Descent(42/49): loss=0.3652777947789135\n",
      "Gradient Descent(43/49): loss=0.36518656392779003\n",
      "Gradient Descent(44/49): loss=0.36510156701316\n",
      "Gradient Descent(45/49): loss=0.36502230863202545\n",
      "Gradient Descent(46/49): loss=0.364948334702129\n",
      "Gradient Descent(47/49): loss=0.3648792289360959\n",
      "Gradient Descent(48/49): loss=0.36481460962254436\n",
      "Gradient Descent(49/49): loss=0.36475412668671325\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4223006670588379\n",
      "Gradient Descent(2/49): loss=0.3995542964908798\n",
      "Gradient Descent(3/49): loss=0.39166228287150984\n",
      "Gradient Descent(4/49): loss=0.387967630900333\n",
      "Gradient Descent(5/49): loss=0.3855902116913366\n",
      "Gradient Descent(6/49): loss=0.3837270156021964\n",
      "Gradient Descent(7/49): loss=0.3821367596401269\n",
      "Gradient Descent(8/49): loss=0.38073401693390807\n",
      "Gradient Descent(9/49): loss=0.3794792972947318\n",
      "Gradient Descent(10/49): loss=0.37834858312188024\n",
      "Gradient Descent(11/49): loss=0.3773244307548455\n",
      "Gradient Descent(12/49): loss=0.3763930545551376\n",
      "Gradient Descent(13/49): loss=0.37554315183625403\n",
      "Gradient Descent(14/49): loss=0.3747652888532349\n",
      "Gradient Descent(15/49): loss=0.37405150475297877\n",
      "Gradient Descent(16/49): loss=0.37339502401453717\n",
      "Gradient Descent(17/49): loss=0.3727900359916262\n",
      "Gradient Descent(18/49): loss=0.37223152178031615\n",
      "Gradient Descent(19/49): loss=0.37171511664211015\n",
      "Gradient Descent(20/49): loss=0.3712369999060073\n",
      "Gradient Descent(21/49): loss=0.37079380639043147\n",
      "Gradient Descent(22/49): loss=0.3703825547978174\n",
      "Gradient Descent(23/49): loss=0.37000058955671705\n",
      "Gradient Descent(24/49): loss=0.36964553335625955\n",
      "Gradient Descent(25/49): loss=0.3693152482092297\n",
      "Gradient Descent(26/49): loss=0.36900780333890604\n",
      "Gradient Descent(27/49): loss=0.36872144854292543\n",
      "Gradient Descent(28/49): loss=0.36845459196796176\n",
      "Gradient Descent(29/49): loss=0.3682057814493502\n",
      "Gradient Descent(30/49): loss=0.3679736887432026\n",
      "Gradient Descent(31/49): loss=0.3677570961152898\n",
      "Gradient Descent(32/49): loss=0.367554884858937\n",
      "Gradient Descent(33/49): loss=0.36736602539955976\n",
      "Gradient Descent(34/49): loss=0.3671895687110906\n",
      "Gradient Descent(35/49): loss=0.36702463882317127\n",
      "Gradient Descent(36/49): loss=0.366870426240591\n",
      "Gradient Descent(37/49): loss=0.36672618213033475\n",
      "Gradient Descent(38/49): loss=0.3665912131586263\n",
      "Gradient Descent(39/49): loss=0.36646487688191476\n",
      "Gradient Descent(40/49): loss=0.3663465776130188\n",
      "Gradient Descent(41/49): loss=0.36623576269748315\n",
      "Gradient Descent(42/49): loss=0.36613191914634036\n",
      "Gradient Descent(43/49): loss=0.36603457058044253\n",
      "Gradient Descent(44/49): loss=0.3659432744487982\n",
      "Gradient Descent(45/49): loss=0.3658576194892354\n",
      "Gradient Descent(46/49): loss=0.3657772234045263\n",
      "Gradient Descent(47/49): loss=0.36570173073102963\n",
      "Gradient Descent(48/49): loss=0.365630810880145\n",
      "Gradient Descent(49/49): loss=0.365564156335534\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41997588289216176\n",
      "Gradient Descent(2/49): loss=0.3993496397011045\n",
      "Gradient Descent(3/49): loss=0.3925168359177709\n",
      "Gradient Descent(4/49): loss=0.3891115225504002\n",
      "Gradient Descent(5/49): loss=0.3866929149194913\n",
      "Gradient Descent(6/49): loss=0.3846636934113485\n",
      "Gradient Descent(7/49): loss=0.3828644342457436\n",
      "Gradient Descent(8/49): loss=0.38124248974193437\n",
      "Gradient Descent(9/49): loss=0.3797727533903591\n",
      "Gradient Descent(10/49): loss=0.3784382924988991\n",
      "Gradient Descent(11/49): loss=0.3772254053764297\n",
      "Gradient Descent(12/49): loss=0.37612222514419935\n",
      "Gradient Descent(13/49): loss=0.37511823121100746\n",
      "Gradient Descent(14/49): loss=0.3742040140060924\n",
      "Gradient Descent(15/49): loss=0.3733711235724401\n",
      "Gradient Descent(16/49): loss=0.3726119542482714\n",
      "Gradient Descent(17/49): loss=0.3719196497277193\n",
      "Gradient Descent(18/49): loss=0.3712880220704587\n",
      "Gradient Descent(19/49): loss=0.37071148135164356\n",
      "Gradient Descent(20/49): loss=0.3701849739161727\n",
      "Gradient Descent(21/49): loss=0.36970392782658745\n",
      "Gradient Descent(22/49): loss=0.36926420444944924\n",
      "Gradient Descent(23/49): loss=0.36886205534973454\n",
      "Gradient Descent(24/49): loss=0.3684940838164065\n",
      "Gradient Descent(25/49): loss=0.3681572104540382\n",
      "Gradient Descent(26/49): loss=0.3678486423606485\n",
      "Gradient Descent(27/49): loss=0.36756584547951215\n",
      "Gradient Descent(28/49): loss=0.3673065197678046\n",
      "Gradient Descent(29/49): loss=0.36706857687078454\n",
      "Gradient Descent(30/49): loss=0.36685012002890477\n",
      "Gradient Descent(31/49): loss=0.36664942597826045\n",
      "Gradient Descent(32/49): loss=0.36646492863316243\n",
      "Gradient Descent(33/49): loss=0.3662952043641751\n",
      "Gradient Descent(34/49): loss=0.366138958706285\n",
      "Gradient Descent(35/49): loss=0.365995014350469\n",
      "Gradient Descent(36/49): loss=0.3658623002882095\n",
      "Gradient Descent(37/49): loss=0.36573984199277215\n",
      "Gradient Descent(38/49): loss=0.36562675253363314\n",
      "Gradient Descent(39/49): loss=0.3655222245314967\n",
      "Gradient Descent(40/49): loss=0.36542552287113284\n",
      "Gradient Descent(41/49): loss=0.36533597809790913\n",
      "Gradient Descent(42/49): loss=0.3652529804315637\n",
      "Gradient Descent(43/49): loss=0.36517597433758076\n",
      "Gradient Descent(44/49): loss=0.36510445360258775\n",
      "Gradient Descent(45/49): loss=0.3650379568655911\n",
      "Gradient Descent(46/49): loss=0.36497606356168594\n",
      "Gradient Descent(47/49): loss=0.36491839023917455\n",
      "Gradient Descent(48/49): loss=0.3648645872148811\n",
      "Gradient Descent(49/49): loss=0.3648143355358891\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41989763347917824\n",
      "Gradient Descent(2/49): loss=0.39924848954430997\n",
      "Gradient Descent(3/49): loss=0.3923943457884514\n",
      "Gradient Descent(4/49): loss=0.388964861438245\n",
      "Gradient Descent(5/49): loss=0.3865211283807544\n",
      "Gradient Descent(6/49): loss=0.384467098411362\n",
      "Gradient Descent(7/49): loss=0.38264394290140996\n",
      "Gradient Descent(8/49): loss=0.3809992463317749\n",
      "Gradient Descent(9/49): loss=0.3795079584716537\n",
      "Gradient Descent(10/49): loss=0.37815312349083063\n",
      "Gradient Descent(11/49): loss=0.37692098479697966\n",
      "Gradient Descent(12/49): loss=0.3757996112994902\n",
      "Gradient Descent(13/49): loss=0.3747784187185269\n",
      "Gradient Descent(14/49): loss=0.37384793806704963\n",
      "Gradient Descent(15/49): loss=0.37299966520477107\n",
      "Gradient Descent(16/49): loss=0.37222594530468106\n",
      "Gradient Descent(17/49): loss=0.371519877284359\n",
      "Gradient Descent(18/49): loss=0.3708752321125595\n",
      "Gradient Descent(19/49): loss=0.37028638182076234\n",
      "Gradient Descent(20/49): loss=0.36974823722791267\n",
      "Gradient Descent(21/49): loss=0.3692561929705185\n",
      "Gradient Descent(22/49): loss=0.3688060787696255\n",
      "Gradient Descent(23/49): loss=0.36839411608628986\n",
      "Gradient Descent(24/49): loss=0.3680168794712022\n",
      "Gradient Descent(25/49): loss=0.3676712620281319\n",
      "Gradient Descent(26/49): loss=0.36735444449891547\n",
      "Gradient Descent(27/49): loss=0.36706386754793513\n",
      "Gradient Descent(28/49): loss=0.3667972068813946\n",
      "Gradient Descent(29/49): loss=0.3665523508843938\n",
      "Gradient Descent(30/49): loss=0.36632738049897107\n",
      "Gradient Descent(31/49): loss=0.3661205511004449\n",
      "Gradient Descent(32/49): loss=0.36593027615865414\n",
      "Gradient Descent(33/49): loss=0.3657551124959159\n",
      "Gradient Descent(34/49): loss=0.3655937469753518\n",
      "Gradient Descent(35/49): loss=0.36544498447220686\n",
      "Gradient Descent(36/49): loss=0.36530773699733443\n",
      "Gradient Descent(37/49): loss=0.36518101385649027\n",
      "Gradient Descent(38/49): loss=0.36506391274177585\n",
      "Gradient Descent(39/49): loss=0.3649556116627309\n",
      "Gradient Descent(40/49): loss=0.36485536163441046\n",
      "Gradient Descent(41/49): loss=0.36476248004847284\n",
      "Gradient Descent(42/49): loss=0.3646763446609916\n",
      "Gradient Descent(43/49): loss=0.3645963881375236\n",
      "Gradient Descent(44/49): loss=0.3645220931020224\n",
      "Gradient Descent(45/49): loss=0.3644529876415755\n",
      "Gradient Descent(46/49): loss=0.36438864122374814\n",
      "Gradient Descent(47/49): loss=0.36432866098760286\n",
      "Gradient Descent(48/49): loss=0.36427268837329935\n",
      "Gradient Descent(49/49): loss=0.36422039605860473\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4201006288294067\n",
      "Gradient Descent(2/49): loss=0.39958063383674436\n",
      "Gradient Descent(3/49): loss=0.3927819285995271\n",
      "Gradient Descent(4/49): loss=0.3893773584040669\n",
      "Gradient Descent(5/49): loss=0.38694828140421245\n",
      "Gradient Descent(6/49): loss=0.38490506919277107\n",
      "Gradient Descent(7/49): loss=0.38309055772200623\n",
      "Gradient Descent(8/49): loss=0.3814527451586883\n",
      "Gradient Descent(9/49): loss=0.3799667595043907\n",
      "Gradient Descent(10/49): loss=0.37861581341563827\n",
      "Gradient Descent(11/49): loss=0.37738633213411515\n",
      "Gradient Descent(12/49): loss=0.37626656614827025\n",
      "Gradient Descent(13/49): loss=0.37524610022260646\n",
      "Gradient Descent(14/49): loss=0.3743156158293186\n",
      "Gradient Descent(15/49): loss=0.3734667391457061\n",
      "Gradient Descent(16/49): loss=0.3726919263899194\n",
      "Gradient Descent(17/49): loss=0.3719843701897651\n",
      "Gradient Descent(18/49): loss=0.3713379201479549\n",
      "Gradient Descent(19/49): loss=0.3707470140650304\n",
      "Gradient Descent(20/49): loss=0.3702066176686468\n",
      "Gradient Descent(21/49): loss=0.3697121713894141\n",
      "Gradient Descent(22/49): loss=0.3692595431147242\n",
      "Gradient Descent(23/49): loss=0.3688449860946615\n",
      "Gradient Descent(24/49): loss=0.368465101335914\n",
      "Gradient Descent(25/49): loss=0.3681168039342918\n",
      "Gradient Descent(26/49): loss=0.3677972928820436\n",
      "Gradient Descent(27/49): loss=0.3675040239527958\n",
      "Gradient Descent(28/49): loss=0.3672346853205414\n",
      "Gradient Descent(29/49): loss=0.3669871756133071\n",
      "Gradient Descent(30/49): loss=0.3667595841392462\n",
      "Gradient Descent(31/49): loss=0.36655017305446125\n",
      "Gradient Descent(32/49): loss=0.3663573612689759\n",
      "Gradient Descent(33/49): loss=0.3661797099107059\n",
      "Gradient Descent(34/49): loss=0.36601590918765753\n",
      "Gradient Descent(35/49): loss=0.3658647665063659\n",
      "Gradient Descent(36/49): loss=0.3657251957201662\n",
      "Gradient Descent(37/49): loss=0.3655962073945808\n",
      "Gradient Descent(38/49): loss=0.3654768999891536\n",
      "Gradient Descent(39/49): loss=0.36536645186571226\n",
      "Gradient Descent(40/49): loss=0.3652641140424452\n",
      "Gradient Descent(41/49): loss=0.36516920362152366\n",
      "Gradient Descent(42/49): loss=0.3650810978254078\n",
      "Gradient Descent(43/49): loss=0.364999228583555\n",
      "Gradient Descent(44/49): loss=0.36492307761712034\n",
      "Gradient Descent(45/49): loss=0.36485217197446707\n",
      "Gradient Descent(46/49): loss=0.3647860799749779\n",
      "Gradient Descent(47/49): loss=0.3647244075228399\n",
      "Gradient Descent(48/49): loss=0.3646667947562131\n",
      "Gradient Descent(49/49): loss=0.36461291300055476\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4195355098130625\n",
      "Gradient Descent(2/49): loss=0.39793616552861194\n",
      "Gradient Descent(3/49): loss=0.3907644514708716\n",
      "Gradient Descent(4/49): loss=0.387338089417524\n",
      "Gradient Descent(5/49): loss=0.38503441417606493\n",
      "Gradient Descent(6/49): loss=0.3831806677095833\n",
      "Gradient Descent(7/49): loss=0.38158386001325406\n",
      "Gradient Descent(8/49): loss=0.3801737528891514\n",
      "Gradient Descent(9/49): loss=0.37891473180885155\n",
      "Gradient Descent(10/49): loss=0.3777832150195756\n",
      "Gradient Descent(11/49): loss=0.3767613117603981\n",
      "Gradient Descent(12/49): loss=0.37583466494377965\n",
      "Gradient Descent(13/49): loss=0.37499147006179034\n",
      "Gradient Descent(14/49): loss=0.37422189038996084\n",
      "Gradient Descent(15/49): loss=0.37351764810951754\n",
      "Gradient Descent(16/49): loss=0.3728717173321661\n",
      "Gradient Descent(17/49): loss=0.37227808672026136\n",
      "Gradient Descent(18/49): loss=0.37173157342203084\n",
      "Gradient Descent(19/49): loss=0.3712276760876895\n",
      "Gradient Descent(20/49): loss=0.3707624580804284\n",
      "Gradient Descent(21/49): loss=0.3703324541921189\n",
      "Gradient Descent(22/49): loss=0.3699345957444649\n",
      "Gradient Descent(23/49): loss=0.36956615012608524\n",
      "Gradient Descent(24/49): loss=0.3692246717036274\n",
      "Gradient Descent(25/49): loss=0.36890796172509827\n",
      "Gradient Descent(26/49): loss=0.3686140353576276\n",
      "Gradient Descent(27/49): loss=0.36834109440715224\n",
      "Gradient Descent(28/49): loss=0.36808750458178024\n",
      "Gradient Descent(29/49): loss=0.36785177640481287\n",
      "Gradient Descent(30/49): loss=0.367632549073546\n",
      "Gradient Descent(31/49): loss=0.3674285767082305\n",
      "Gradient Descent(32/49): loss=0.36723871655139245\n",
      "Gradient Descent(33/49): loss=0.3670619187683035\n",
      "Gradient Descent(34/49): loss=0.36689721757039456\n",
      "Gradient Descent(35/49): loss=0.36674372343912987\n",
      "Gradient Descent(36/49): loss=0.36660061627169666\n",
      "Gradient Descent(37/49): loss=0.3664671393044057\n",
      "Gradient Descent(38/49): loss=0.3663425936969912\n",
      "Gradient Descent(39/49): loss=0.3662263336826057\n",
      "Gradient Descent(40/49): loss=0.36611776220547604\n",
      "Gradient Descent(41/49): loss=0.3660163269818543\n",
      "Gradient Descent(42/49): loss=0.3659215169308366\n",
      "Gradient Descent(43/49): loss=0.36583285893039225\n",
      "Gradient Descent(44/49): loss=0.3657499148610245\n",
      "Gradient Descent(45/49): loss=0.3656722789052045\n",
      "Gradient Descent(46/49): loss=0.36559957507538693\n",
      "Gradient Descent(47/49): loss=0.3655314549472306\n",
      "Gradient Descent(48/49): loss=0.36546759557778447\n",
      "Gradient Descent(49/49): loss=0.3654076975910058\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4174076991390114\n",
      "Gradient Descent(2/49): loss=0.3979994812483884\n",
      "Gradient Descent(3/49): loss=0.39176991749625034\n",
      "Gradient Descent(4/49): loss=0.3885364730864752\n",
      "Gradient Descent(5/49): loss=0.38613653334451814\n",
      "Gradient Descent(6/49): loss=0.3840863638261574\n",
      "Gradient Descent(7/49): loss=0.38226254148523797\n",
      "Gradient Descent(8/49): loss=0.38062190276195806\n",
      "Gradient Descent(9/49): loss=0.3791408953083016\n",
      "Gradient Descent(10/49): loss=0.3778020167638215\n",
      "Gradient Descent(11/49): loss=0.3765905448803524\n",
      "Gradient Descent(12/49): loss=0.37549359190672277\n",
      "Gradient Descent(13/49): loss=0.3744997237112832\n",
      "Gradient Descent(14/49): loss=0.37359873985846503\n",
      "Gradient Descent(15/49): loss=0.37278151414627547\n",
      "Gradient Descent(16/49): loss=0.37203986632744046\n",
      "Gradient Descent(17/49): loss=0.3713664541559576\n",
      "Gradient Descent(18/49): loss=0.3707546805265622\n",
      "Gradient Descent(19/49): loss=0.37019861261481835\n",
      "Gradient Descent(20/49): loss=0.36969291093144874\n",
      "Gradient Descent(21/49): loss=0.3692327667592089\n",
      "Gradient Descent(22/49): loss=0.36881384678288837\n",
      "Gradient Descent(23/49): loss=0.3684322439527197\n",
      "Gradient Descent(24/49): loss=0.3680844337862185\n",
      "Gradient Descent(25/49): loss=0.36776723543802564\n",
      "Gradient Descent(26/49): loss=0.36747777696533424\n",
      "Gradient Descent(27/49): loss=0.36721346429590923\n",
      "Gradient Descent(28/49): loss=0.36697195347144934\n",
      "Gradient Descent(29/49): loss=0.3667511257942435\n",
      "Gradient Descent(30/49): loss=0.3665490655519432\n",
      "Gradient Descent(31/49): loss=0.3663640400353477\n",
      "Gradient Descent(32/49): loss=0.36619448159858486\n",
      "Gradient Descent(33/49): loss=0.3660389715408699\n",
      "Gradient Descent(34/49): loss=0.36589622561488006\n",
      "Gradient Descent(35/49): loss=0.3657650809892756\n",
      "Gradient Descent(36/49): loss=0.36564448451254483\n",
      "Gradient Descent(37/49): loss=0.3655334821425232\n",
      "Gradient Descent(38/49): loss=0.36543120942101315\n",
      "Gradient Descent(39/49): loss=0.36533688288617533\n",
      "Gradient Descent(40/49): loss=0.3652497923270272\n",
      "Gradient Descent(41/49): loss=0.36516929379468094\n",
      "Gradient Descent(42/49): loss=0.365094803294054\n",
      "Gradient Descent(43/49): loss=0.3650257910878435\n",
      "Gradient Descent(44/49): loss=0.3649617765516999\n",
      "Gradient Descent(45/49): loss=0.36490232352589275\n",
      "Gradient Descent(46/49): loss=0.3648470361143948\n",
      "Gradient Descent(47/49): loss=0.3647955548873524\n",
      "Gradient Descent(48/49): loss=0.36474755344738824\n",
      "Gradient Descent(49/49): loss=0.3647027353241838\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41732699765931974\n",
      "Gradient Descent(2/49): loss=0.397896372475826\n",
      "Gradient Descent(3/49): loss=0.39164406098066906\n",
      "Gradient Descent(4/49): loss=0.3883848532299536\n",
      "Gradient Descent(5/49): loss=0.3859584538859751\n",
      "Gradient Descent(6/49): loss=0.38388241075438145\n",
      "Gradient Descent(7/49): loss=0.3820338429644103\n",
      "Gradient Descent(8/49): loss=0.38036976801864714\n",
      "Gradient Descent(9/49): loss=0.378866651300432\n",
      "Gradient Descent(10/49): loss=0.3775069408128638\n",
      "Gradient Descent(11/49): loss=0.376275841872871\n",
      "Gradient Descent(12/49): loss=0.37516039090238423\n",
      "Gradient Descent(13/49): loss=0.3741490821019955\n",
      "Gradient Descent(14/49): loss=0.37323164976378004\n",
      "Gradient Descent(15/49): loss=0.37239890883561505\n",
      "Gradient Descent(16/49): loss=0.371642625879911\n",
      "Gradient Descent(17/49): loss=0.3709554101854667\n",
      "Gradient Descent(18/49): loss=0.37033062004437856\n",
      "Gradient Descent(19/49): loss=0.36976228117930937\n",
      "Gradient Descent(20/49): loss=0.36924501524177894\n",
      "Gradient Descent(21/49): loss=0.3687739768303717\n",
      "Gradient Descent(22/49): loss=0.3683447978129701\n",
      "Gradient Descent(23/49): loss=0.36795353796776015\n",
      "Gradient Descent(24/49): loss=0.3675966411262388\n",
      "Gradient Descent(25/49): loss=0.3672708961302901\n",
      "Gradient Descent(26/49): loss=0.36697340201736717\n",
      "Gradient Descent(27/49): loss=0.36670153693056845\n",
      "Gradient Descent(28/49): loss=0.3664529303188002\n",
      "Gradient Descent(29/49): loss=0.3662254380495036\n",
      "Gradient Descent(30/49): loss=0.36601712010487614\n",
      "Gradient Descent(31/49): loss=0.3658262205737967\n",
      "Gradient Descent(32/49): loss=0.36565114968703216\n",
      "Gradient Descent(33/49): loss=0.36549046767376125\n",
      "Gradient Descent(34/49): loss=0.36534287024377154\n",
      "Gradient Descent(35/49): loss=0.3652071755225249\n",
      "Gradient Descent(36/49): loss=0.3650823122861521\n",
      "Gradient Descent(37/49): loss=0.364967309360779\n",
      "Gradient Descent(38/49): loss=0.36486128606575685\n",
      "Gradient Descent(39/49): loss=0.36476344359367485\n",
      "Gradient Descent(40/49): loss=0.3646730572317289\n",
      "Gradient Descent(41/49): loss=0.36458946933932707\n",
      "Gradient Descent(42/49): loss=0.3645120830059099\n",
      "Gradient Descent(43/49): loss=0.36444035632100313\n",
      "Gradient Descent(44/49): loss=0.3643737971956573\n",
      "Gradient Descent(45/49): loss=0.36431195868074795\n",
      "Gradient Descent(46/49): loss=0.3642544347332345\n",
      "Gradient Descent(47/49): loss=0.3642008563864848\n",
      "Gradient Descent(48/49): loss=0.36415088828522807\n",
      "Gradient Descent(49/49): loss=0.36410422554968663\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41754001007343644\n",
      "Gradient Descent(2/49): loss=0.39823859711548365\n",
      "Gradient Descent(3/49): loss=0.3920377243022958\n",
      "Gradient Descent(4/49): loss=0.3888012371219598\n",
      "Gradient Descent(5/49): loss=0.38638877795729265\n",
      "Gradient Descent(6/49): loss=0.38432333162518084\n",
      "Gradient Descent(7/49): loss=0.3824832143891157\n",
      "Gradient Descent(8/49): loss=0.38082573110860435\n",
      "Gradient Descent(9/49): loss=0.3793275280702385\n",
      "Gradient Descent(10/49): loss=0.37797125575148505\n",
      "Gradient Descent(11/49): loss=0.3767423340940974\n",
      "Gradient Descent(12/49): loss=0.37562800536806784\n",
      "Gradient Descent(13/49): loss=0.3746169489560673\n",
      "Gradient Descent(14/49): loss=0.3736990596335879\n",
      "Gradient Descent(15/49): loss=0.3728652884653501\n",
      "Gradient Descent(16/49): loss=0.3721075160464808\n",
      "Gradient Descent(17/49): loss=0.37141844653131006\n",
      "Gradient Descent(18/49): loss=0.3707915168273041\n",
      "Gradient Descent(19/49): loss=0.37022081767083603\n",
      "Gradient Descent(20/49): loss=0.369701024418887\n",
      "Gradient Descent(21/49): loss=0.3692273360034547\n",
      "Gradient Descent(22/49): loss=0.36879542086585904\n",
      "Gradient Descent(23/49): loss=0.36840136893007647\n",
      "Gradient Descent(24/49): loss=0.3680416488430457\n",
      "Gradient Descent(25/49): loss=0.36771306983453117\n",
      "Gradient Descent(26/49): loss=0.36741274764543336\n",
      "Gradient Descent(27/49): loss=0.3671380740504887\n",
      "Gradient Descent(28/49): loss=0.36688668956455284\n",
      "Gradient Descent(29/49): loss=0.3666564589745254\n",
      "Gradient Descent(30/49): loss=0.36644544938373963\n",
      "Gradient Descent(31/49): loss=0.36625191049389777\n",
      "Gradient Descent(32/49): loss=0.36607425688255235\n",
      "Gradient Descent(33/49): loss=0.36591105206260693\n",
      "Gradient Descent(34/49): loss=0.3657609941350434\n",
      "Gradient Descent(35/49): loss=0.36562290286763394\n",
      "Gradient Descent(36/49): loss=0.36549570805123976\n",
      "Gradient Descent(37/49): loss=0.3653784390018056\n",
      "Gradient Descent(38/49): loss=0.3652702150906643\n",
      "Gradient Descent(39/49): loss=0.3651702371985306\n",
      "Gradient Descent(40/49): loss=0.3650777799998239\n",
      "Gradient Descent(41/49): loss=0.3649921849939129\n",
      "Gradient Descent(42/49): loss=0.3649128542086763\n",
      "Gradient Descent(43/49): loss=0.3648392445095912\n",
      "Gradient Descent(44/49): loss=0.36477086245448936\n",
      "Gradient Descent(45/49): loss=0.36470725964029216\n",
      "Gradient Descent(46/49): loss=0.3646480284935272\n",
      "Gradient Descent(47/49): loss=0.3645927984613228\n",
      "Gradient Descent(48/49): loss=0.3645412325639523\n",
      "Gradient Descent(49/49): loss=0.3644930242739047\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41690278070029474\n",
      "Gradient Descent(2/49): loss=0.3965105206455697\n",
      "Gradient Descent(3/49): loss=0.38998828165038785\n",
      "Gradient Descent(4/49): loss=0.38677202193248267\n",
      "Gradient Descent(5/49): loss=0.38451443535121976\n",
      "Gradient Descent(6/49): loss=0.3826604325646509\n",
      "Gradient Descent(7/49): loss=0.3810550830009855\n",
      "Gradient Descent(8/49): loss=0.37963836737191964\n",
      "Gradient Descent(9/49): loss=0.3783765656767358\n",
      "Gradient Descent(10/49): loss=0.3772458367504235\n",
      "Gradient Descent(11/49): loss=0.37622763228938605\n",
      "Gradient Descent(12/49): loss=0.37530698035519405\n",
      "Gradient Descent(13/49): loss=0.3744715829863081\n",
      "Gradient Descent(14/49): loss=0.3737112212316987\n",
      "Gradient Descent(15/49): loss=0.3730173208638667\n",
      "Gradient Descent(16/49): loss=0.3723826225641245\n",
      "Gradient Descent(17/49): loss=0.3718009272370391\n",
      "Gradient Descent(18/49): loss=0.3712668975726241\n",
      "Gradient Descent(19/49): loss=0.37077590243401487\n",
      "Gradient Descent(20/49): loss=0.3703238941248721\n",
      "Gradient Descent(21/49): loss=0.369907311034054\n",
      "Gradient Descent(22/49): loss=0.3695229999482633\n",
      "Gradient Descent(23/49): loss=0.36916815366549405\n",
      "Gradient Descent(24/49): loss=0.36884026055667146\n",
      "Gradient Descent(25/49): loss=0.3685370634942805\n",
      "Gradient Descent(26/49): loss=0.36825652615553034\n",
      "Gradient Descent(27/49): loss=0.3679968051582135\n",
      "Gradient Descent(28/49): loss=0.36775622683306664\n",
      "Gradient Descent(29/49): loss=0.36753326770210054\n",
      "Gradient Descent(30/49): loss=0.36732653793690456\n",
      "Gradient Descent(31/49): loss=0.36713476722870036\n",
      "Gradient Descent(32/49): loss=0.3669567926238111\n",
      "Gradient Descent(33/49): loss=0.36679154797258534\n",
      "Gradient Descent(34/49): loss=0.36663805471301303\n",
      "Gradient Descent(35/49): loss=0.3664954137671735\n",
      "Gradient Descent(36/49): loss=0.3663627983730097\n",
      "Gradient Descent(37/49): loss=0.3662394477085734\n",
      "Gradient Descent(38/49): loss=0.3661246611930543\n",
      "Gradient Descent(39/49): loss=0.3660177933702672\n",
      "Gradient Descent(40/49): loss=0.3659182492971405\n",
      "Gradient Descent(41/49): loss=0.36582548037312196\n",
      "Gradient Descent(42/49): loss=0.36573898055706694\n",
      "Gradient Descent(43/49): loss=0.36565828292671165\n",
      "Gradient Descent(44/49): loss=0.3655829565426899\n",
      "Gradient Descent(45/49): loss=0.3655126035846306\n",
      "Gradient Descent(46/49): loss=0.3654468567313974\n",
      "Gradient Descent(47/49): loss=0.36538537676127275\n",
      "Gradient Descent(48/49): loss=0.36532785035096066\n",
      "Gradient Descent(49/49): loss=0.3652739880548615\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4149759936813126\n",
      "Gradient Descent(2/49): loss=0.39681657079085514\n",
      "Gradient Descent(3/49): loss=0.3911147771513668\n",
      "Gradient Descent(4/49): loss=0.38800460108857265\n",
      "Gradient Descent(5/49): loss=0.38560438410964726\n",
      "Gradient Descent(6/49): loss=0.38352870647029536\n",
      "Gradient Descent(7/49): loss=0.3816811794679608\n",
      "Gradient Descent(8/49): loss=0.3800244222119222\n",
      "Gradient Descent(9/49): loss=0.3785351038328378\n",
      "Gradient Descent(10/49): loss=0.37719469537912015\n",
      "Gradient Descent(11/49): loss=0.37598729533779884\n",
      "Gradient Descent(12/49): loss=0.3748989375323587\n",
      "Gradient Descent(13/49): loss=0.3739172542698939\n",
      "Gradient Descent(14/49): loss=0.3730312509974243\n",
      "Gradient Descent(15/49): loss=0.3722311312365681\n",
      "Gradient Descent(16/49): loss=0.3715081519315929\n",
      "Gradient Descent(17/49): loss=0.3708545005079841\n",
      "Gradient Descent(18/49): loss=0.3702631887952311\n",
      "Gradient Descent(19/49): loss=0.3697279606673884\n",
      "Gradient Descent(20/49): loss=0.3692432111517137\n",
      "Gradient Descent(21/49): loss=0.368803915290502\n",
      "Gradient Descent(22/49): loss=0.36840556539069447\n",
      "Gradient Descent(23/49): loss=0.36804411554155114\n",
      "Gradient Descent(24/49): loss=0.36771593246379825\n",
      "Gradient Descent(25/49): loss=0.36741775189628173\n",
      "Gradient Descent(26/49): loss=0.36714663984082546\n",
      "Gradient Descent(27/49): loss=0.3668999580802421\n",
      "Gradient Descent(28/49): loss=0.36667533346311004\n",
      "Gradient Descent(29/49): loss=0.36647063051530415\n",
      "Gradient Descent(30/49): loss=0.3662839269947003\n",
      "Gradient Descent(31/49): loss=0.3661134920537398\n",
      "Gradient Descent(32/49): loss=0.36595776671601854\n",
      "Gradient Descent(33/49): loss=0.3658153464088398\n",
      "Gradient Descent(34/49): loss=0.36568496532463646\n",
      "Gradient Descent(35/49): loss=0.36556548241104625\n",
      "Gradient Descent(36/49): loss=0.3654558688128194\n",
      "Gradient Descent(37/49): loss=0.36535519660915106\n",
      "Gradient Descent(38/49): loss=0.36526262870788545\n",
      "Gradient Descent(39/49): loss=0.36517740977368\n",
      "Gradient Descent(40/49): loss=0.3650988580809657\n",
      "Gradient Descent(41/49): loss=0.36502635819462276\n",
      "Gradient Descent(42/49): loss=0.36495935439195026\n",
      "Gradient Descent(43/49): loss=0.36489734474891816\n",
      "Gradient Descent(44/49): loss=0.364839875822004\n",
      "Gradient Descent(45/49): loss=0.36478653786428655\n",
      "Gradient Descent(46/49): loss=0.36473696052100335\n",
      "Gradient Descent(47/49): loss=0.36469080895557526\n",
      "Gradient Descent(48/49): loss=0.3646477803622564\n",
      "Gradient Descent(49/49): loss=0.364607600826164\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41489297922100243\n",
      "Gradient Descent(2/49): loss=0.3967115634751639\n",
      "Gradient Descent(3/49): loss=0.39098548227817437\n",
      "Gradient Descent(4/49): loss=0.38784796902486224\n",
      "Gradient Descent(5/49): loss=0.3854200182748758\n",
      "Gradient Descent(6/49): loss=0.38331746630455\n",
      "Gradient Descent(7/49): loss=0.38144440381358374\n",
      "Gradient Descent(8/49): loss=0.3797635778625872\n",
      "Gradient Descent(9/49): loss=0.3782516377282602\n",
      "Gradient Descent(10/49): loss=0.37688998017066905\n",
      "Gradient Descent(11/49): loss=0.37566261543183527\n",
      "Gradient Descent(12/49): loss=0.3745554911489592\n",
      "Gradient Descent(13/49): loss=0.3735561607492158\n",
      "Gradient Descent(14/49): loss=0.3726535589243295\n",
      "Gradient Descent(15/49): loss=0.371837825771942\n",
      "Gradient Descent(16/49): loss=0.37110016093161075\n",
      "Gradient Descent(17/49): loss=0.3704326994777072\n",
      "Gradient Descent(18/49): loss=0.3698284048732495\n",
      "Gradient Descent(19/49): loss=0.3692809758596731\n",
      "Gradient Descent(20/49): loss=0.3687847650071426\n",
      "Gradient Descent(21/49): loss=0.3683347071723245\n",
      "Gradient Descent(22/49): loss=0.36792625646115323\n",
      "Gradient Descent(23/49): loss=0.36755533054557377\n",
      "Gradient Descent(24/49): loss=0.3672182613728955\n",
      "Gradient Descent(25/49): loss=0.3669117514549293\n",
      "Gradient Descent(26/49): loss=0.3666328350436247\n",
      "Gradient Descent(27/49): loss=0.3663788435980189\n",
      "Gradient Descent(28/49): loss=0.3661473750289159\n",
      "Gradient Descent(29/49): loss=0.36593626627629305\n",
      "Gradient Descent(30/49): loss=0.36574356883249504\n",
      "Gradient Descent(31/49): loss=0.3655675268737352\n",
      "Gradient Descent(32/49): loss=0.36540655770474884\n",
      "Gradient Descent(33/49): loss=0.3652592342578282\n",
      "Gradient Descent(34/49): loss=0.3651242694188465\n",
      "Gradient Descent(35/49): loss=0.3650005019800387\n",
      "Gradient Descent(36/49): loss=0.36488688404288183\n",
      "Gradient Descent(37/49): loss=0.36478246971493916\n",
      "Gradient Descent(38/49): loss=0.36468640496244176\n",
      "Gradient Descent(39/49): loss=0.364597918496049\n",
      "Gradient Descent(40/49): loss=0.36451631358096925\n",
      "Gradient Descent(41/49): loss=0.36444096067469156\n",
      "Gradient Descent(42/49): loss=0.3643712908062102\n",
      "Gradient Descent(43/49): loss=0.36430678961999685\n",
      "Gradient Descent(44/49): loss=0.36424699201625993\n",
      "Gradient Descent(45/49): loss=0.36419147732636425\n",
      "Gradient Descent(46/49): loss=0.36413986496877787\n",
      "Gradient Descent(47/49): loss=0.36409181053668793\n",
      "Gradient Descent(48/49): loss=0.36404700227354736\n",
      "Gradient Descent(49/49): loss=0.3640051578973824\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41511599606056687\n",
      "Gradient Descent(2/49): loss=0.3970631269419482\n",
      "Gradient Descent(3/49): loss=0.3913844234260655\n",
      "Gradient Descent(4/49): loss=0.38826777844177907\n",
      "Gradient Descent(5/49): loss=0.38585327227984195\n",
      "Gradient Descent(6/49): loss=0.38376115497741026\n",
      "Gradient Descent(7/49): loss=0.3818963299547012\n",
      "Gradient Descent(8/49): loss=0.380221764261887\n",
      "Gradient Descent(9/49): loss=0.37871431599359906\n",
      "Gradient Descent(10/49): loss=0.3773556254188077\n",
      "Gradient Descent(11/49): loss=0.37612995003638383\n",
      "Gradient Descent(12/49): loss=0.37502346531377134\n",
      "Gradient Descent(13/49): loss=0.3740239234900871\n",
      "Gradient Descent(14/49): loss=0.3731204276142441\n",
      "Gradient Descent(15/49): loss=0.3723032580040378\n",
      "Gradient Descent(16/49): loss=0.37156373002611154\n",
      "Gradient Descent(17/49): loss=0.3708940738028687\n",
      "Gradient Descent(18/49): loss=0.3702873306656673\n",
      "Gradient Descent(19/49): loss=0.36973726306921234\n",
      "Gradient Descent(20/49): loss=0.3692382756777223\n",
      "Gradient Descent(21/49): loss=0.36878534591479184\n",
      "Gradient Descent(22/49): loss=0.3683739626379109\n",
      "Gradient Descent(23/49): loss=0.36800007185056305\n",
      "Gradient Descent(24/49): loss=0.3676600285479311\n",
      "Gradient Descent(25/49): loss=0.3673505539321608\n",
      "Gradient Descent(26/49): loss=0.3670686973441851\n",
      "Gradient Descent(27/49): loss=0.3668118023496591\n",
      "Gradient Descent(28/49): loss=0.36657747649180095\n",
      "Gradient Descent(29/49): loss=0.3663635642872971\n",
      "Gradient Descent(30/49): loss=0.36616812309526714\n",
      "Gradient Descent(31/49): loss=0.36598940153537857\n",
      "Gradient Descent(32/49): loss=0.3658258201708279\n",
      "Gradient Descent(33/49): loss=0.3656759542061523\n",
      "Gradient Descent(34/49): loss=0.36553851797951864\n",
      "Gradient Descent(35/49): loss=0.36541235105493913\n",
      "Gradient Descent(36/49): loss=0.36529640574236455\n",
      "Gradient Descent(37/49): loss=0.3651897358932696\n",
      "Gradient Descent(38/49): loss=0.36509148683656195\n",
      "Gradient Descent(39/49): loss=0.3650008863347694\n",
      "Gradient Descent(40/49): loss=0.36491723645374646\n",
      "Gradient Descent(41/49): loss=0.3648399062508613\n",
      "Gradient Descent(42/49): loss=0.3647683251969571\n",
      "Gradient Descent(43/49): loss=0.36470197725651926\n",
      "Gradient Descent(44/49): loss=0.36464039555857614\n",
      "Gradient Descent(45/49): loss=0.36458315759802573\n",
      "Gradient Descent(46/49): loss=0.36452988091345684\n",
      "Gradient Descent(47/49): loss=0.36448021919318613\n",
      "Gradient Descent(48/49): loss=0.3644338587662752\n",
      "Gradient Descent(49/49): loss=0.3643905154397767\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41440247972053446\n",
      "Gradient Descent(2/49): loss=0.39525817243095446\n",
      "Gradient Descent(3/49): loss=0.38931181706180157\n",
      "Gradient Descent(4/49): loss=0.38625502222181307\n",
      "Gradient Descent(5/49): loss=0.38402267590342937\n",
      "Gradient Descent(6/49): loss=0.3821624267725511\n",
      "Gradient Descent(7/49): loss=0.380548103700258\n",
      "Gradient Descent(8/49): loss=0.3791260240878722\n",
      "Gradient Descent(9/49): loss=0.3778630075310952\n",
      "Gradient Descent(10/49): loss=0.37673454352084074\n",
      "Gradient Descent(11/49): loss=0.3757213302828825\n",
      "Gradient Descent(12/49): loss=0.3748077793201936\n",
      "Gradient Descent(13/49): loss=0.3739811207870977\n",
      "Gradient Descent(14/49): loss=0.3732307768908248\n",
      "Gradient Descent(15/49): loss=0.3725478964894404\n",
      "Gradient Descent(16/49): loss=0.3719250016161251\n",
      "Gradient Descent(17/49): loss=0.3713557161003894\n",
      "Gradient Descent(18/49): loss=0.37083455569817303\n",
      "Gradient Descent(19/49): loss=0.3703567647615501\n",
      "Gradient Descent(20/49): loss=0.36991818833151996\n",
      "Gradient Descent(21/49): loss=0.3695151713159664\n",
      "Gradient Descent(22/49): loss=0.3691444784632032\n",
      "Gradient Descent(23/49): loss=0.3688032303679489\n",
      "Gradient Descent(24/49): loss=0.3684888518911552\n",
      "Gradient Descent(25/49): loss=0.3681990302368779\n",
      "Gradient Descent(26/49): loss=0.3679316805801372\n",
      "Gradient Descent(27/49): loss=0.3676849176323338\n",
      "Gradient Descent(28/49): loss=0.3674570319044492\n",
      "Gradient Descent(29/49): loss=0.3672464697122699\n",
      "Gradient Descent(30/49): loss=0.3670518161841329\n",
      "Gradient Descent(31/49): loss=0.3668717806967172\n",
      "Gradient Descent(32/49): loss=0.36670518429056714\n",
      "Gradient Descent(33/49): loss=0.36655094871374405\n",
      "Gradient Descent(34/49): loss=0.3664080868162936\n",
      "Gradient Descent(35/49): loss=0.3662756940754688\n",
      "Gradient Descent(36/49): loss=0.3661529410758977\n",
      "Gradient Descent(37/49): loss=0.366039066803218\n",
      "Gradient Descent(38/49): loss=0.36593337263643644\n",
      "Gradient Descent(39/49): loss=0.365835216945193\n",
      "Gradient Descent(40/49): loss=0.36574401021456115\n",
      "Gradient Descent(41/49): loss=0.3656592106330252\n",
      "Gradient Descent(42/49): loss=0.36558032008962865\n",
      "Gradient Descent(43/49): loss=0.36550688053457603\n",
      "Gradient Descent(44/49): loss=0.36543847066426477\n",
      "Gradient Descent(45/49): loss=0.3653747028971635\n",
      "Gradient Descent(46/49): loss=0.3653152206114094\n",
      "Gradient Descent(47/49): loss=0.36525969561869004\n",
      "Gradient Descent(48/49): loss=0.3652078258520433\n",
      "Gradient Descent(49/49): loss=0.36515933324779865\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41268076651906527\n",
      "Gradient Descent(2/49): loss=0.3957824830008909\n",
      "Gradient Descent(3/49): loss=0.3905335414499239\n",
      "Gradient Descent(4/49): loss=0.3875057716692862\n",
      "Gradient Descent(5/49): loss=0.38509181141643517\n",
      "Gradient Descent(6/49): loss=0.38298860458108075\n",
      "Gradient Descent(7/49): loss=0.3811191219271838\n",
      "Gradient Descent(8/49): loss=0.37944899696747264\n",
      "Gradient Descent(9/49): loss=0.37795422570462767\n",
      "Gradient Descent(10/49): loss=0.37661496857689525\n",
      "Gradient Descent(11/49): loss=0.37541404993145855\n",
      "Gradient Descent(12/49): loss=0.37433639209386965\n",
      "Gradient Descent(13/49): loss=0.37336868492845027\n",
      "Gradient Descent(14/49): loss=0.3724991430423728\n",
      "Gradient Descent(15/49): loss=0.3717173103145557\n",
      "Gradient Descent(16/49): loss=0.3710138963021351\n",
      "Gradient Descent(17/49): loss=0.3703806366181212\n",
      "Gradient Descent(18/49): loss=0.36981017241342296\n",
      "Gradient Descent(19/49): loss=0.3692959456066834\n",
      "Gradient Descent(20/49): loss=0.3688321073661976\n",
      "Gradient Descent(21/49): loss=0.3684134378909602\n",
      "Gradient Descent(22/49): loss=0.36803527590933455\n",
      "Gradient Descent(23/49): loss=0.3676934565853459\n",
      "Gradient Descent(24/49): loss=0.36738425673113745\n",
      "Gradient Descent(25/49): loss=0.3671043463900599\n",
      "Gradient Descent(26/49): loss=0.36685074599010553\n",
      "Gradient Descent(27/49): loss=0.3666207883794516\n",
      "Gradient Descent(28/49): loss=0.36641208514977136\n",
      "Gradient Descent(29/49): loss=0.3662224967322845\n",
      "Gradient Descent(30/49): loss=0.366050105818932\n",
      "Gradient Descent(31/49): loss=0.365893193718625\n",
      "Gradient Descent(32/49): loss=0.36575021930788887\n",
      "Gradient Descent(33/49): loss=0.3656198002776936\n",
      "Gradient Descent(34/49): loss=0.3655006964149323\n",
      "Gradient Descent(35/49): loss=0.365391794688743\n",
      "Gradient Descent(36/49): loss=0.36529209593942025\n",
      "Gradient Descent(37/49): loss=0.3652007029916217\n",
      "Gradient Descent(38/49): loss=0.3651168100344802\n",
      "Gradient Descent(39/49): loss=0.36503969312949003\n",
      "Gradient Descent(40/49): loss=0.3649687017230264\n",
      "Gradient Descent(41/49): loss=0.36490325105439003\n",
      "Gradient Descent(42/49): loss=0.36484281536259044\n",
      "Gradient Descent(43/49): loss=0.3647869218059377\n",
      "Gradient Descent(44/49): loss=0.3647351450180726\n",
      "Gradient Descent(45/49): loss=0.36468710223251793\n",
      "Gradient Descent(46/49): loss=0.3646424489152881\n",
      "Gradient Descent(47/49): loss=0.36460087485171006\n",
      "Gradient Descent(48/49): loss=0.3645621006394573\n",
      "Gradient Descent(49/49): loss=0.3645258745449897\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4125955781642264\n",
      "Gradient Descent(2/49): loss=0.39567561044922267\n",
      "Gradient Descent(3/49): loss=0.39040073426535393\n",
      "Gradient Descent(4/49): loss=0.3873440863979931\n",
      "Gradient Descent(5/49): loss=0.3849011792409274\n",
      "Gradient Descent(6/49): loss=0.3827701584893432\n",
      "Gradient Descent(7/49): loss=0.3808744046881243\n",
      "Gradient Descent(8/49): loss=0.3791796254804054\n",
      "Gradient Descent(9/49): loss=0.37766176105459387\n",
      "Gradient Descent(10/49): loss=0.3763008749473765\n",
      "Gradient Descent(11/49): loss=0.3750796890915269\n",
      "Gradient Descent(12/49): loss=0.3739830303307897\n",
      "Gradient Descent(13/49): loss=0.37299750299926054\n",
      "Gradient Descent(14/49): loss=0.37211124566223275\n",
      "Gradient Descent(15/49): loss=0.3713137341576044\n",
      "Gradient Descent(16/49): loss=0.3705956164375547\n",
      "Gradient Descent(17/49): loss=0.36994857161831407\n",
      "Gradient Descent(18/49): loss=0.3693651884334221\n",
      "Gradient Descent(19/49): loss=0.36883885970527136\n",
      "Gradient Descent(20/49): loss=0.36836369028555904\n",
      "Gradient Descent(21/49): loss=0.36793441645817765\n",
      "Gradient Descent(22/49): loss=0.3675463351778692\n",
      "Gradient Descent(23/49): loss=0.3671952417992596\n",
      "Gradient Descent(24/49): loss=0.36687737516824986\n",
      "Gradient Descent(25/49): loss=0.36658936912085793\n",
      "Gradient Descent(26/49): loss=0.3663282095754145\n",
      "Gradient Descent(27/49): loss=0.36609119652026245\n",
      "Gradient Descent(28/49): loss=0.36587591029609895\n",
      "Gradient Descent(29/49): loss=0.3656801816536545\n",
      "Gradient Descent(30/49): loss=0.3655020651364249\n",
      "Gradient Descent(31/49): loss=0.36533981539687255\n",
      "Gradient Descent(32/49): loss=0.36519186610465254\n",
      "Gradient Descent(33/49): loss=0.3650568111484271\n",
      "Gradient Descent(34/49): loss=0.3649333878698351\n",
      "Gradient Descent(35/49): loss=0.36482046210012387\n",
      "Gradient Descent(36/49): loss=0.364717014797622\n",
      "Gradient Descent(37/49): loss=0.36462213010824096\n",
      "Gradient Descent(38/49): loss=0.3645349846921038\n",
      "Gradient Descent(39/49): loss=0.3644548381776411\n",
      "Gradient Descent(40/49): loss=0.36438102462044764\n",
      "Gradient Descent(41/49): loss=0.36431294485817856\n",
      "Gradient Descent(42/49): loss=0.36425005966503426\n",
      "Gradient Descent(43/49): loss=0.36419188362018984\n",
      "Gradient Descent(44/49): loss=0.36413797961403566\n",
      "Gradient Descent(45/49): loss=0.36408795392449655\n",
      "Gradient Descent(46/49): loss=0.3640414518031189\n",
      "Gradient Descent(47/49): loss=0.3639981535171822\n",
      "Gradient Descent(48/49): loss=0.3639577707999122\n",
      "Gradient Descent(49/49): loss=0.363920043666038\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41282858679079826\n",
      "Gradient Descent(2/49): loss=0.3960357716931987\n",
      "Gradient Descent(3/49): loss=0.39080424780588885\n",
      "Gradient Descent(4/49): loss=0.3877669607727916\n",
      "Gradient Descent(5/49): loss=0.385337176416805\n",
      "Gradient Descent(6/49): loss=0.3832164525796596\n",
      "Gradient Descent(7/49): loss=0.3813286908774404\n",
      "Gradient Descent(8/49): loss=0.3796398019744928\n",
      "Gradient Descent(9/49): loss=0.3781259800238152\n",
      "Gradient Descent(10/49): loss=0.3767675758803322\n",
      "Gradient Descent(11/49): loss=0.37554758947937233\n",
      "Gradient Descent(12/49): loss=0.3744510946965951\n",
      "Gradient Descent(13/49): loss=0.3734649058034806\n",
      "Gradient Descent(14/49): loss=0.37257733561328893\n",
      "Gradient Descent(15/49): loss=0.37177800278975565\n",
      "Gradient Descent(16/49): loss=0.37105767159566183\n",
      "Gradient Descent(17/49): loss=0.3704081155586198\n",
      "Gradient Descent(18/49): loss=0.36982199992988274\n",
      "Gradient Descent(19/49): loss=0.3692927794994581\n",
      "Gradient Descent(20/49): loss=0.3688146092729899\n",
      "Gradient Descent(21/49): loss=0.36838226609221747\n",
      "Gradient Descent(22/49): loss=0.3679910796629753\n",
      "Gradient Descent(23/49): loss=0.36763687172651965\n",
      "Gradient Descent(24/49): loss=0.3673159023145462\n",
      "Gradient Descent(25/49): loss=0.3670248221888372\n",
      "Gradient Descent(26/49): loss=0.36676063069630266\n",
      "Gradient Descent(27/49): loss=0.3665206383772743\n",
      "Gradient Descent(28/49): loss=0.3663024337545\n",
      "Gradient Descent(29/49): loss=0.36610385380595245\n",
      "Gradient Descent(30/49): loss=0.36592295768891536\n",
      "Gradient Descent(31/49): loss=0.365758003337848\n",
      "Gradient Descent(32/49): loss=0.36560742660579126\n",
      "Gradient Descent(33/49): loss=0.3654698226598115\n",
      "Gradient Descent(34/49): loss=0.3653439293762069\n",
      "Gradient Descent(35/49): loss=0.3652286125117406\n",
      "Gradient Descent(36/49): loss=0.3651228524537158\n",
      "Gradient Descent(37/49): loss=0.3650257323748475\n",
      "Gradient Descent(38/49): loss=0.3649364276390906\n",
      "Gradient Descent(39/49): loss=0.36485419632226745\n",
      "Gradient Descent(40/49): loss=0.36477837072684594\n",
      "Gradient Descent(41/49): loss=0.3647083497838369\n",
      "Gradient Descent(42/49): loss=0.36464359224676823\n",
      "Gradient Descent(43/49): loss=0.364583610593253\n",
      "Gradient Descent(44/49): loss=0.364527965558994\n",
      "Gradient Descent(45/49): loss=0.3644762612373065\n",
      "Gradient Descent(46/49): loss=0.36442814068453216\n",
      "Gradient Descent(47/49): loss=0.36438328197818015\n",
      "Gradient Descent(48/49): loss=0.36434139468036136\n",
      "Gradient Descent(49/49): loss=0.3643022166641694\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4120346068737817\n",
      "Gradient Descent(2/49): loss=0.39416097138426315\n",
      "Gradient Descent(3/49): loss=0.3887164074570998\n",
      "Gradient Descent(4/49): loss=0.38577603600667737\n",
      "Gradient Descent(5/49): loss=0.38355375928896795\n",
      "Gradient Descent(6/49): loss=0.3816839617110166\n",
      "Gradient Descent(7/49): loss=0.3800611911471663\n",
      "Gradient Descent(8/49): loss=0.37863520399673795\n",
      "Gradient Descent(9/49): loss=0.3773724809043419\n",
      "Gradient Descent(10/49): loss=0.37624762180799\n",
      "Gradient Descent(11/49): loss=0.3752405427194587\n",
      "Gradient Descent(12/49): loss=0.37433505794482075\n",
      "Gradient Descent(13/49): loss=0.3735179521790626\n",
      "Gradient Descent(14/49): loss=0.3727783116201626\n",
      "Gradient Descent(15/49): loss=0.37210702555998443\n",
      "Gradient Descent(16/49): loss=0.37149640979238985\n",
      "Gradient Descent(17/49): loss=0.37093991965322426\n",
      "Gradient Descent(18/49): loss=0.370431929829955\n",
      "Gradient Descent(19/49): loss=0.36996756425577665\n",
      "Gradient Descent(20/49): loss=0.36954256376451705\n",
      "Gradient Descent(21/49): loss=0.36915318234416644\n",
      "Gradient Descent(22/49): loss=0.3687961051472083\n",
      "Gradient Descent(23/49): loss=0.3684683831308513\n",
      "Gradient Descent(24/49): loss=0.36816738047346187\n",
      "Gradient Descent(25/49): loss=0.36789073186180954\n",
      "Gradient Descent(26/49): loss=0.3676363074519384\n",
      "Gradient Descent(27/49): loss=0.3674021838365397\n",
      "Gradient Descent(28/49): loss=0.36718661974928796\n",
      "Gradient Descent(29/49): loss=0.36698803553544546\n",
      "Gradient Descent(30/49): loss=0.3668049956431607\n",
      "Gradient Descent(31/49): loss=0.36663619355987265\n",
      "Gradient Descent(32/49): loss=0.3664804387469134\n",
      "Gradient Descent(33/49): loss=0.36633664522311016\n",
      "Gradient Descent(34/49): loss=0.3662038215226047\n",
      "Gradient Descent(35/49): loss=0.36608106180900474\n",
      "Gradient Descent(36/49): loss=0.36596753797166176\n",
      "Gradient Descent(37/49): loss=0.36586249256356107\n",
      "Gradient Descent(38/49): loss=0.36576523246643133\n",
      "Gradient Descent(39/49): loss=0.3656751231890574\n",
      "Gradient Descent(40/49): loss=0.3655915837207835\n",
      "Gradient Descent(41/49): loss=0.3655140818748382\n",
      "Gradient Descent(42/49): loss=0.3654421300662051\n",
      "Gradient Descent(43/49): loss=0.36537528147686416\n",
      "Gradient Descent(44/49): loss=0.3653131265678026\n",
      "Gradient Descent(45/49): loss=0.36525528990257766\n",
      "Gradient Descent(46/49): loss=0.3652014272516551\n",
      "Gradient Descent(47/49): loss=0.3651512229504596\n",
      "Gradient Descent(48/49): loss=0.3651043874871907\n",
      "Gradient Descent(49/49): loss=0.36506065529911585\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.41052201765226953\n",
      "Gradient Descent(2/49): loss=0.39487993880810757\n",
      "Gradient Descent(3/49): loss=0.39001128631249604\n",
      "Gradient Descent(4/49): loss=0.38703239766062497\n",
      "Gradient Descent(5/49): loss=0.38459562240696316\n",
      "Gradient Descent(6/49): loss=0.3824646243665222\n",
      "Gradient Descent(7/49): loss=0.38057543561879537\n",
      "Gradient Descent(8/49): loss=0.3788947116230248\n",
      "Gradient Descent(9/49): loss=0.3773971931206482\n",
      "Gradient Descent(10/49): loss=0.37606155544096387\n",
      "Gradient Descent(11/49): loss=0.3748692915542992\n",
      "Gradient Descent(12/49): loss=0.3738041939710399\n",
      "Gradient Descent(13/49): loss=0.37285200886817405\n",
      "Gradient Descent(14/49): loss=0.37200016843391565\n",
      "Gradient Descent(15/49): loss=0.37123757148325703\n",
      "Gradient Descent(16/49): loss=0.3705543987351127\n",
      "Gradient Descent(17/49): loss=0.36994195496220594\n",
      "Gradient Descent(18/49): loss=0.36939253289624063\n",
      "Gradient Descent(19/49): loss=0.36889929520776155\n",
      "Gradient Descent(20/49): loss=0.36845617174517653\n",
      "Gradient Descent(21/49): loss=0.36805776978872173\n",
      "Gradient Descent(22/49): loss=0.36769929548208646\n",
      "Gradient Descent(23/49): loss=0.36737648491134356\n",
      "Gradient Descent(24/49): loss=0.36708554354185313\n",
      "Gradient Descent(25/49): loss=0.3668230929182614\n",
      "Gradient Descent(26/49): loss=0.3665861236924526\n",
      "Gradient Descent(27/49): loss=0.3663719541771416\n",
      "Gradient Descent(28/49): loss=0.3661781937341964\n",
      "Gradient Descent(29/49): loss=0.36600271040082655\n",
      "Gradient Descent(30/49): loss=0.36584360223656115\n",
      "Gradient Descent(31/49): loss=0.36569917194193535\n",
      "Gradient Descent(32/49): loss=0.36556790435794956\n",
      "Gradient Descent(33/49): loss=0.36544844650526953\n",
      "Gradient Descent(34/49): loss=0.3653395898650808\n",
      "Gradient Descent(35/49): loss=0.3652402546405871\n",
      "Gradient Descent(36/49): loss=0.36514947577021933\n",
      "Gradient Descent(37/49): loss=0.36506639049145434\n",
      "Gradient Descent(38/49): loss=0.3649902272783418\n",
      "Gradient Descent(39/49): loss=0.36492029599691417\n",
      "Gradient Descent(40/49): loss=0.36485597914106876\n",
      "Gradient Descent(41/49): loss=0.3647967240276032\n",
      "Gradient Descent(42/49): loss=0.3647420358431899\n",
      "Gradient Descent(43/49): loss=0.3646914714484544\n",
      "Gradient Descent(44/49): loss=0.3646446338551949\n",
      "Gradient Descent(45/49): loss=0.3646011673023523\n",
      "Gradient Descent(46/49): loss=0.3645607528647719\n",
      "Gradient Descent(47/49): loss=0.364523104536238\n",
      "Gradient Descent(48/49): loss=0.36448796573482856\n",
      "Gradient Descent(49/49): loss=0.3644551061844446\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4104347944889913\n",
      "Gradient Descent(2/49): loss=0.3947712084818035\n",
      "Gradient Descent(3/49): loss=0.38987489483574633\n",
      "Gradient Descent(4/49): loss=0.3868656302954692\n",
      "Gradient Descent(5/49): loss=0.384398755408459\n",
      "Gradient Descent(6/49): loss=0.38223906139392333\n",
      "Gradient Descent(7/49): loss=0.3803229157500376\n",
      "Gradient Descent(8/49): loss=0.37861699461463527\n",
      "Gradient Descent(9/49): loss=0.377095949066511\n",
      "Gradient Descent(10/49): loss=0.375738337010398\n",
      "Gradient Descent(11/49): loss=0.37452553631507096\n",
      "Gradient Descent(12/49): loss=0.3734412355632549\n",
      "Gradient Descent(13/49): loss=0.37247108911608773\n",
      "Gradient Descent(14/49): loss=0.3716024478740788\n",
      "Gradient Descent(15/49): loss=0.3708241378394665\n",
      "Gradient Descent(16/49): loss=0.3701262735671913\n",
      "Gradient Descent(17/49): loss=0.36950009888601104\n",
      "Gradient Descent(18/49): loss=0.3689378497546989\n",
      "Gradient Descent(19/49): loss=0.3684326355006974\n",
      "Gradient Descent(20/49): loss=0.3679783355489007\n",
      "Gradient Descent(21/49): loss=0.3675695093308414\n",
      "Gradient Descent(22/49): loss=0.3672013174859435\n",
      "Gradient Descent(23/49): loss=0.3668694527865829\n",
      "Gradient Descent(24/49): loss=0.3665700794703487\n",
      "Gradient Descent(25/49): loss=0.3662997798654936\n",
      "Gradient Descent(26/49): loss=0.366055507361342\n",
      "Gradient Descent(27/49): loss=0.3658345449126362\n",
      "Gradient Descent(28/49): loss=0.36563446838133706\n",
      "Gradient Descent(29/49): loss=0.36545311411564363\n",
      "Gradient Descent(30/49): loss=0.36528855024730467\n",
      "Gradient Descent(31/49): loss=0.3651390512573113\n",
      "Gradient Descent(32/49): loss=0.3650030754188815\n",
      "Gradient Descent(33/49): loss=0.3648792447769672\n",
      "Gradient Descent(34/49): loss=0.36476632736670733\n",
      "Gradient Descent(35/49): loss=0.36466322141044877\n",
      "Gradient Descent(36/49): loss=0.36456894126507633\n",
      "Gradient Descent(37/49): loss=0.36448260491921924\n",
      "Gradient Descent(38/49): loss=0.36440342286404814\n",
      "Gradient Descent(39/49): loss=0.36433068818240466\n",
      "Gradient Descent(40/49): loss=0.36426376771933483\n",
      "Gradient Descent(41/49): loss=0.36420209421312527\n",
      "Gradient Descent(42/49): loss=0.3641451592799698\n",
      "Gradient Descent(43/49): loss=0.36409250715770464\n",
      "Gradient Descent(44/49): loss=0.36404372912486266\n",
      "Gradient Descent(45/49): loss=0.3639984585208113\n",
      "Gradient Descent(46/49): loss=0.36395636630112427\n",
      "Gradient Descent(47/49): loss=0.3639171570697319\n",
      "Gradient Descent(48/49): loss=0.3638805655359291\n",
      "Gradient Descent(49/49): loss=0.363846353350092\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4106777822641306\n",
      "Gradient Descent(2/49): loss=0.39513922984876826\n",
      "Gradient Descent(3/49): loss=0.3902823704670008\n",
      "Gradient Descent(4/49): loss=0.38729129237084803\n",
      "Gradient Descent(5/49): loss=0.3848373465537672\n",
      "Gradient Descent(6/49): loss=0.3826878096446436\n",
      "Gradient Descent(7/49): loss=0.3807793720352267\n",
      "Gradient Descent(8/49): loss=0.3790789360176329\n",
      "Gradient Descent(9/49): loss=0.37756146212650826\n",
      "Gradient Descent(10/49): loss=0.3762058391420903\n",
      "Gradient Descent(11/49): loss=0.3749937508139927\n",
      "Gradient Descent(12/49): loss=0.3739091490348646\n",
      "Gradient Descent(13/49): loss=0.37293790686357936\n",
      "Gradient Descent(14/49): loss=0.3720675535610832\n",
      "Gradient Descent(15/49): loss=0.37128705925869193\n",
      "Gradient Descent(16/49): loss=0.3705866544595454\n",
      "Gradient Descent(17/49): loss=0.3699576760809762\n",
      "Gradient Descent(18/49): loss=0.3693924347504597\n",
      "Gradient Descent(19/49): loss=0.3688840996548724\n",
      "Gradient Descent(20/49): loss=0.36842659816921514\n",
      "Gradient Descent(21/49): loss=0.36801452808237706\n",
      "Gradient Descent(22/49): loss=0.3676430806463575\n",
      "Gradient Descent(23/49): loss=0.3673079729768884\n",
      "Gradient Descent(24/49): loss=0.3670053885666912\n",
      "Gradient Descent(25/49): loss=0.36673192485920186\n",
      "Gradient Descent(26/49): loss=0.3664845469831994\n",
      "Gradient Descent(27/49): loss=0.36626054687546244\n",
      "Gradient Descent(28/49): loss=0.3660575071248274\n",
      "Gradient Descent(29/49): loss=0.3658732689608029\n",
      "Gradient Descent(30/49): loss=0.3657059038861761\n",
      "Gradient Descent(31/49): loss=0.3655536885181567\n",
      "Gradient Descent(32/49): loss=0.36541508225840014\n",
      "Gradient Descent(33/49): loss=0.3652887074602008\n",
      "Gradient Descent(34/49): loss=0.36517333180250755\n",
      "Gradient Descent(35/49): loss=0.36506785261616387\n",
      "Gradient Descent(36/49): loss=0.36497128293876585\n",
      "Gradient Descent(37/49): loss=0.3648827391014633\n",
      "Gradient Descent(38/49): loss=0.3648014296744709\n",
      "Gradient Descent(39/49): loss=0.36472664561851775\n",
      "Gradient Descent(40/49): loss=0.364657751507349\n",
      "Gradient Descent(41/49): loss=0.3645941777020555\n",
      "Gradient Descent(42/49): loss=0.3645354133717491\n",
      "Gradient Descent(43/49): loss=0.36448100026717156\n",
      "Gradient Descent(44/49): loss=0.3644305271644489\n",
      "Gradient Descent(45/49): loss=0.3643836249055607\n",
      "Gradient Descent(46/49): loss=0.3643399619703454\n",
      "Gradient Descent(47/49): loss=0.36429924052215834\n",
      "Gradient Descent(48/49): loss=0.3642611928757355\n",
      "Gradient Descent(49/49): loss=0.36422557834152697\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40979916216003653\n",
      "Gradient Descent(2/49): loss=0.3932018079152877\n",
      "Gradient Descent(3/49): loss=0.38818635317262873\n",
      "Gradient Descent(4/49): loss=0.38532668860235225\n",
      "Gradient Descent(5/49): loss=0.38310390701524666\n",
      "Gradient Descent(6/49): loss=0.38122313190491\n",
      "Gradient Descent(7/49): loss=0.37959298477708026\n",
      "Gradient Descent(8/49): loss=0.37816459486522214\n",
      "Gradient Descent(9/49): loss=0.37690356857627394\n",
      "Gradient Descent(10/49): loss=0.3757835157964667\n",
      "Gradient Descent(11/49): loss=0.37478357859530437\n",
      "Gradient Descent(12/49): loss=0.3738870027834562\n",
      "Gradient Descent(13/49): loss=0.3730801545936255\n",
      "Gradient Descent(14/49): loss=0.37235180505837134\n",
      "Gradient Descent(15/49): loss=0.3716925987637921\n",
      "Gradient Descent(16/49): loss=0.371094655399272\n",
      "Gradient Descent(17/49): loss=0.37055126857045123\n",
      "Gradient Descent(18/49): loss=0.37005667645550977\n",
      "Gradient Descent(19/49): loss=0.36960588583788007\n",
      "Gradient Descent(20/49): loss=0.3691945359932536\n",
      "Gradient Descent(21/49): loss=0.36881879247911553\n",
      "Gradient Descent(22/49): loss=0.3684752634741912\n",
      "Gradient Descent(23/49): loss=0.3681609332170679\n",
      "Gradient Descent(24/49): loss=0.3678731084899976\n",
      "Gradient Descent(25/49): loss=0.36760937512259123\n",
      "Gradient Descent(26/49): loss=0.3673675622496447\n",
      "Gradient Descent(27/49): loss=0.36714571261940454\n",
      "Gradient Descent(28/49): loss=0.36694205766549565\n",
      "Gradient Descent(29/49): loss=0.3667549963657164\n",
      "Gradient Descent(30/49): loss=0.36658307714198707\n",
      "Gradient Descent(31/49): loss=0.36642498222850683\n",
      "Gradient Descent(32/49): loss=0.3662795140647501\n",
      "Gradient Descent(33/49): loss=0.36614558336748787\n",
      "Gradient Descent(34/49): loss=0.36602219860976826\n",
      "Gradient Descent(35/49): loss=0.3659084566908015\n",
      "Gradient Descent(36/49): loss=0.3658035346234799\n",
      "Gradient Descent(37/49): loss=0.3657066820991281\n",
      "Gradient Descent(38/49): loss=0.3656172148145142\n",
      "Gradient Descent(39/49): loss=0.36553450846596897\n",
      "Gradient Descent(40/49): loss=0.36545799333104806\n",
      "Gradient Descent(41/49): loss=0.36538714937052824\n",
      "Gradient Descent(42/49): loss=0.36532150179342376\n",
      "Gradient Descent(43/49): loss=0.36526061703571966\n",
      "Gradient Descent(44/49): loss=0.365204099110056\n",
      "Gradient Descent(45/49): loss=0.3651515862890032\n",
      "Gradient Descent(46/49): loss=0.36510274808907656\n",
      "Gradient Descent(47/49): loss=0.36505728252643915\n",
      "Gradient Descent(48/49): loss=0.36501491361847055\n",
      "Gradient Descent(49/49): loss=0.3649753891081613\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40849974708092546\n",
      "Gradient Descent(2/49): loss=0.3940928053993424\n",
      "Gradient Descent(3/49): loss=0.38953568076803574\n",
      "Gradient Descent(4/49): loss=0.3865788838659369\n",
      "Gradient Descent(5/49): loss=0.38411364553620303\n",
      "Gradient Descent(6/49): loss=0.3819557624282531\n",
      "Gradient Descent(7/49): loss=0.38004936081576\n",
      "Gradient Descent(8/49): loss=0.3783607354176341\n",
      "Gradient Descent(9/49): loss=0.37686300188465177\n",
      "Gradient Descent(10/49): loss=0.37553324278724\n",
      "Gradient Descent(11/49): loss=0.3743515847707084\n",
      "Gradient Descent(12/49): loss=0.3733006818627157\n",
      "Gradient Descent(13/49): loss=0.3723653411742474\n",
      "Gradient Descent(14/49): loss=0.3715322252500056\n",
      "Gradient Descent(15/49): loss=0.37078960584784226\n",
      "Gradient Descent(16/49): loss=0.37012715608708263\n",
      "Gradient Descent(17/49): loss=0.3695357729247284\n",
      "Gradient Descent(18/49): loss=0.36900742442812445\n",
      "Gradient Descent(19/49): loss=0.3685350177380394\n",
      "Gradient Descent(20/49): loss=0.3681122845149719\n",
      "Gradient Descent(21/49): loss=0.36773368128033845\n",
      "Gradient Descent(22/49): loss=0.3673943025200383\n",
      "Gradient Descent(23/49): loss=0.3670898047699193\n",
      "Gradient Descent(24/49): loss=0.3668163401832303\n",
      "Gradient Descent(25/49): loss=0.3665704983083324\n",
      "Gradient Descent(26/49): loss=0.3663492549930694\n",
      "Gradient Descent(27/49): loss=0.36614992748877073\n",
      "Gradient Descent(28/49): loss=0.36597013495805636\n",
      "Gradient Descent(29/49): loss=0.36580776370117657\n",
      "Gradient Descent(30/49): loss=0.36566093650919856\n",
      "Gradient Descent(31/49): loss=0.3655279856318803\n",
      "Gradient Descent(32/49): loss=0.36540742891590927\n",
      "Gradient Descent(33/49): loss=0.36529794872721494\n",
      "Gradient Descent(34/49): loss=0.36519837332088106\n",
      "Gradient Descent(35/49): loss=0.36510766036505216\n",
      "Gradient Descent(36/49): loss=0.36502488236221525\n",
      "Gradient Descent(37/49): loss=0.3649492137432381\n",
      "Gradient Descent(38/49): loss=0.36487991943727394\n",
      "Gradient Descent(39/49): loss=0.3648163447447425\n",
      "Gradient Descent(40/49): loss=0.3647579063615622\n",
      "Gradient Descent(41/49): loss=0.36470408442109853\n",
      "Gradient Descent(42/49): loss=0.36465441543625743\n",
      "Gradient Descent(43/49): loss=0.36460848603812696\n",
      "Gradient Descent(44/49): loss=0.3645659274198029\n",
      "Gradient Descent(45/49): loss=0.3645264104047666\n",
      "Gradient Descent(46/49): loss=0.36448964106860776\n",
      "Gradient Descent(47/49): loss=0.3644553568511695\n",
      "Gradient Descent(48/49): loss=0.3644233231034802\n",
      "Gradient Descent(49/49): loss=0.3643933300202617\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4084106281952976\n",
      "Gradient Descent(2/49): loss=0.39398219983360966\n",
      "Gradient Descent(3/49): loss=0.38939563801263133\n",
      "Gradient Descent(4/49): loss=0.38640701678529615\n",
      "Gradient Descent(5/49): loss=0.3839105848630241\n",
      "Gradient Descent(6/49): loss=0.3817231775510601\n",
      "Gradient Descent(7/49): loss=0.3797891790182452\n",
      "Gradient Descent(8/49): loss=0.37807485249980416\n",
      "Gradient Descent(9/49): loss=0.37655319256651965\n",
      "Gradient Descent(10/49): loss=0.37520114586801584\n",
      "Gradient Descent(11/49): loss=0.3739987125372889\n",
      "Gradient Descent(12/49): loss=0.37292843486037325\n",
      "Gradient Descent(13/49): loss=0.3719750220313581\n",
      "Gradient Descent(14/49): loss=0.37112504997036794\n",
      "Gradient Descent(15/49): loss=0.3703667126083731\n",
      "Gradient Descent(16/49): loss=0.3696896120311102\n",
      "Gradient Descent(17/49): loss=0.36908457947880213\n",
      "Gradient Descent(18/49): loss=0.3685435215823414\n",
      "Gradient Descent(19/49): loss=0.36805928762127604\n",
      "Gradient Descent(20/49): loss=0.36762555450107054\n",
      "Gradient Descent(21/49): loss=0.36723672678685665\n",
      "Gradient Descent(22/49): loss=0.36688784960629617\n",
      "Gradient Descent(23/49): loss=0.3665745326020563\n",
      "Gradient Descent(24/49): loss=0.3662928834069755\n",
      "Gradient Descent(25/49): loss=0.36603944935199173\n",
      "Gradient Descent(26/49): loss=0.3658111663113395\n",
      "Gradient Descent(27/49): loss=0.3656053137505278\n",
      "Gradient Descent(28/49): loss=0.36541947517687495\n",
      "Gradient Descent(29/49): loss=0.36525150330501227\n",
      "Gradient Descent(30/49): loss=0.3650994893447235\n",
      "Gradient Descent(31/49): loss=0.36496173589889525\n",
      "Gradient Descent(32/49): loss=0.3648367330277129\n",
      "Gradient Descent(33/49): loss=0.3647231370935714\n",
      "Gradient Descent(34/49): loss=0.36461975205111774\n",
      "Gradient Descent(35/49): loss=0.3645255128897375\n",
      "Gradient Descent(36/49): loss=0.36443947097275425\n",
      "Gradient Descent(37/49): loss=0.36436078104953146\n",
      "Gradient Descent(38/49): loss=0.3642886897443012\n",
      "Gradient Descent(39/49): loss=0.36422252534954025\n",
      "Gradient Descent(40/49): loss=0.3641616887725748\n",
      "Gradient Descent(41/49): loss=0.36410564550228736\n",
      "Gradient Descent(42/49): loss=0.3640539184786696\n",
      "Gradient Descent(43/49): loss=0.3640060817618633\n",
      "Gradient Descent(44/49): loss=0.36396175490948335\n",
      "Gradient Descent(45/49): loss=0.3639205979816975\n",
      "Gradient Descent(46/49): loss=0.3638823071029034\n",
      "Gradient Descent(47/49): loss=0.3638466105170832\n",
      "Gradient Descent(48/49): loss=0.3638132650811748\n",
      "Gradient Descent(49/49): loss=0.363782053147185\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40866358248056384\n",
      "Gradient Descent(2/49): loss=0.39435734999066174\n",
      "Gradient Descent(3/49): loss=0.38980655700974576\n",
      "Gradient Descent(4/49): loss=0.3868352562611605\n",
      "Gradient Descent(5/49): loss=0.38435164536708116\n",
      "Gradient Descent(6/49): loss=0.3821742344016316\n",
      "Gradient Descent(7/49): loss=0.38024761936293594\n",
      "Gradient Descent(8/49): loss=0.37853834236544404\n",
      "Gradient Descent(9/49): loss=0.377019767805411\n",
      "Gradient Descent(10/49): loss=0.3756692145396459\n",
      "Gradient Descent(11/49): loss=0.37446701313334513\n",
      "Gradient Descent(12/49): loss=0.373395982601236\n",
      "Gradient Descent(13/49): loss=0.37244105743294287\n",
      "Gradient Descent(14/49): loss=0.37158899445306925\n",
      "Gradient Descent(15/49): loss=0.3708281319024791\n",
      "Gradient Descent(16/49): loss=0.3701481866514193\n",
      "Gradient Descent(17/49): loss=0.3695400811330079\n",
      "Gradient Descent(18/49): loss=0.3689957943874083\n",
      "Gradient Descent(19/49): loss=0.36850823315114667\n",
      "Gradient Descent(20/49): loss=0.3680711198653323\n",
      "Gradient Descent(21/49): loss=0.36767889510210405\n",
      "Gradient Descent(22/49): loss=0.36732663235785473\n",
      "Gradient Descent(23/49): loss=0.36700996350296067\n",
      "Gradient Descent(24/49): loss=0.3667250134470093\n",
      "Gradient Descent(25/49): loss=0.36646834279647345\n",
      "Gradient Descent(26/49): loss=0.36623689746115123\n",
      "Gradient Descent(27/49): loss=0.3660279643149839\n",
      "Gradient Descent(28/49): loss=0.365839132142091\n",
      "Gradient Descent(29/49): loss=0.365668257204564\n",
      "Gradient Descent(30/49): loss=0.36551343285818855\n",
      "Gradient Descent(31/49): loss=0.3653729627185794\n",
      "Gradient Descent(32/49): loss=0.36524533694543104\n",
      "Gradient Descent(33/49): loss=0.3651292112684833\n",
      "Gradient Descent(34/49): loss=0.36502338842687065\n",
      "Gradient Descent(35/49): loss=0.3649268017349535\n",
      "Gradient Descent(36/49): loss=0.3648385005235389\n",
      "Gradient Descent(37/49): loss=0.36475763723641996\n",
      "Gradient Descent(38/49): loss=0.36468345598909274\n",
      "Gradient Descent(39/49): loss=0.3646152824199363\n",
      "Gradient Descent(40/49): loss=0.3645525146845592\n",
      "Gradient Descent(41/49): loss=0.36449461546184503\n",
      "Gradient Descent(42/49): loss=0.36444110485581416\n",
      "Gradient Descent(43/49): loss=0.3643915540910761\n",
      "Gradient Descent(44/49): loss=0.36434557991161876\n",
      "Gradient Descent(45/49): loss=0.36430283960319426\n",
      "Gradient Descent(46/49): loss=0.36426302656880555\n",
      "Gradient Descent(47/49): loss=0.36422586639493404\n",
      "Gradient Descent(48/49): loss=0.3641911133533109\n",
      "Gradient Descent(49/49): loss=0.3641585472893578\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40769614557929884\n",
      "Gradient Descent(2/49): loss=0.39236461234411607\n",
      "Gradient Descent(3/49): loss=0.3877085705231888\n",
      "Gradient Descent(4/49): loss=0.3849007202415712\n",
      "Gradient Descent(5/49): loss=0.3826704699645612\n",
      "Gradient Descent(6/49): loss=0.3807785381646869\n",
      "Gradient Descent(7/49): loss=0.3791423581823467\n",
      "Gradient Descent(8/49): loss=0.3777130282766843\n",
      "Gradient Descent(9/49): loss=0.3764549803220292\n",
      "Gradient Descent(10/49): loss=0.37534080525303526\n",
      "Gradient Descent(11/49): loss=0.37434889826940737\n",
      "Gradient Descent(12/49): loss=0.3734619683696703\n",
      "Gradient Descent(13/49): loss=0.37266598881201146\n",
      "Gradient Descent(14/49): loss=0.3719494337613851\n",
      "Gradient Descent(15/49): loss=0.3713027154865997\n",
      "Gradient Descent(16/49): loss=0.37071776568007614\n",
      "Gradient Descent(17/49): loss=0.3701877214985576\n",
      "Gradient Descent(18/49): loss=0.3697066882180656\n",
      "Gradient Descent(19/49): loss=0.36926955825131336\n",
      "Gradient Descent(20/49): loss=0.3688718718470909\n",
      "Gradient Descent(21/49): loss=0.368509708782646\n",
      "Gradient Descent(22/49): loss=0.3681796032371541\n",
      "Gradient Descent(23/49): loss=0.36787847611682\n",
      "Gradient Descent(24/49): loss=0.3676035806142642\n",
      "Gradient Descent(25/49): loss=0.3673524578858119\n",
      "Gradient Descent(26/49): loss=0.36712290053391505\n",
      "Gradient Descent(27/49): loss=0.3669129221699841\n",
      "Gradient Descent(28/49): loss=0.3667207317643311\n",
      "Gradient Descent(29/49): loss=0.3665447118073539\n",
      "Gradient Descent(30/49): loss=0.36638339954038746\n",
      "Gradient Descent(31/49): loss=0.3662354706882009\n",
      "Gradient Descent(32/49): loss=0.36609972525421336\n",
      "Gradient Descent(33/49): loss=0.3659750750359719\n",
      "Gradient Descent(34/49): loss=0.36586053259091605\n",
      "Gradient Descent(35/49): loss=0.3657552014372451\n",
      "Gradient Descent(36/49): loss=0.36565826731641415\n",
      "Gradient Descent(37/49): loss=0.36556899037577684\n",
      "Gradient Descent(38/49): loss=0.36548669815465795\n",
      "Gradient Descent(39/49): loss=0.36541077927647564\n",
      "Gradient Descent(40/49): loss=0.3653406777648008\n",
      "Gradient Descent(41/49): loss=0.3652758879134157\n",
      "Gradient Descent(42/49): loss=0.3652159496502582\n",
      "Gradient Descent(43/49): loss=0.3651604443431545\n",
      "Gradient Descent(44/49): loss=0.36510899100185557\n",
      "Gradient Descent(45/49): loss=0.36506124283641683\n",
      "Gradient Descent(46/49): loss=0.3650168841366103\n",
      "Gradient Descent(47/49): loss=0.36497562744102763\n",
      "Gradient Descent(48/49): loss=0.3649372109679358\n",
      "Gradient Descent(49/49): loss=0.3649013962829013\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4066139548050328\n",
      "Gradient Descent(2/49): loss=0.3934060962186582\n",
      "Gradient Descent(3/49): loss=0.38909665604970545\n",
      "Gradient Descent(4/49): loss=0.3861411699022893\n",
      "Gradient Descent(5/49): loss=0.3836444081026234\n",
      "Gradient Descent(6/49): loss=0.3814612814899497\n",
      "Gradient Descent(7/49): loss=0.379540241545872\n",
      "Gradient Descent(8/49): loss=0.37784629477942144\n",
      "Gradient Descent(9/49): loss=0.37635069970378027\n",
      "Gradient Descent(10/49): loss=0.3750288783143802\n",
      "Gradient Descent(11/49): loss=0.3738595697771665\n",
      "Gradient Descent(12/49): loss=0.3728242880214534\n",
      "Gradient Descent(13/49): loss=0.37190691062638204\n",
      "Gradient Descent(14/49): loss=0.371093347086814\n",
      "Gradient Descent(15/49): loss=0.37037126310616825\n",
      "Gradient Descent(16/49): loss=0.3697298477365208\n",
      "Gradient Descent(17/49): loss=0.3691596148256343\n",
      "Gradient Descent(18/49): loss=0.36865223268261665\n",
      "Gradient Descent(19/49): loss=0.36820037733175043\n",
      "Gradient Descent(20/49): loss=0.36779760568399955\n",
      "Gradient Descent(21/49): loss=0.36743824564111727\n",
      "Gradient Descent(22/49): loss=0.36711730066552456\n",
      "Gradient Descent(23/49): loss=0.3668303667559675\n",
      "Gradient Descent(24/49): loss=0.36657356009607533\n",
      "Gradient Descent(25/49): loss=0.36634345391005435\n",
      "Gradient Descent(26/49): loss=0.36613702328016223\n",
      "Gradient Descent(27/49): loss=0.36595159686388257\n",
      "Gradient Descent(28/49): loss=0.3657848146020119\n",
      "Gradient Descent(29/49): loss=0.3656345906377321\n",
      "Gradient Descent(30/49): loss=0.3654990807755255\n",
      "Gradient Descent(31/49): loss=0.3653766539009719\n",
      "Gradient Descent(32/49): loss=0.3652658668608738\n",
      "Gradient Descent(33/49): loss=0.3651654423700356\n",
      "Gradient Descent(34/49): loss=0.36507424956826684\n",
      "Gradient Descent(35/49): loss=0.3649912869002904\n",
      "Gradient Descent(36/49): loss=0.3649156670334962\n",
      "Gradient Descent(37/49): loss=0.3648466035649168\n",
      "Gradient Descent(38/49): loss=0.3647833993003012\n",
      "Gradient Descent(39/49): loss=0.3647254359154317\n",
      "Gradient Descent(40/49): loss=0.364672164833503\n",
      "Gradient Descent(41/49): loss=0.36462309917294833\n",
      "Gradient Descent(42/49): loss=0.3645778066380065\n",
      "Gradient Descent(43/49): loss=0.3645359032399419\n",
      "Gradient Descent(44/49): loss=0.3644970477504522\n",
      "Gradient Descent(45/49): loss=0.36446093680072056\n",
      "Gradient Descent(46/49): loss=0.36442730054998995\n",
      "Gradient Descent(47/49): loss=0.3643958988566718\n",
      "Gradient Descent(48/49): loss=0.364366517893007\n",
      "Gradient Descent(49/49): loss=0.3643389671513227\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4065230792831451\n",
      "Gradient Descent(2/49): loss=0.3932935739422461\n",
      "Gradient Descent(3/49): loss=0.38895290206848754\n",
      "Gradient Descent(4/49): loss=0.38596419564266854\n",
      "Gradient Descent(5/49): loss=0.38343520288761485\n",
      "Gradient Descent(6/49): loss=0.38122177399140444\n",
      "Gradient Descent(7/49): loss=0.3792725389708132\n",
      "Gradient Descent(8/49): loss=0.37755242275261375\n",
      "Gradient Descent(9/49): loss=0.3760325339447096\n",
      "Gradient Descent(10/49): loss=0.3746881420021555\n",
      "Gradient Descent(11/49): loss=0.37349784921456797\n",
      "Gradient Descent(12/49): loss=0.37244305037909337\n",
      "Gradient Descent(13/49): loss=0.37150751908318064\n",
      "Gradient Descent(14/49): loss=0.37067707266126876\n",
      "Gradient Descent(15/49): loss=0.3699392936673994\n",
      "Gradient Descent(16/49): loss=0.3692832949063123\n",
      "Gradient Descent(17/49): loss=0.3686995194012238\n",
      "Gradient Descent(18/49): loss=0.3681795690616127\n",
      "Gradient Descent(19/49): loss=0.367716057281947\n",
      "Gradient Descent(20/49): loss=0.3673024816912603\n",
      "Gradient Descent(21/49): loss=0.3669331139879357\n",
      "Gradient Descent(22/49): loss=0.36660290433605525\n",
      "Gradient Descent(23/49): loss=0.3663073982244573\n",
      "Gradient Descent(24/49): loss=0.366042664029773\n",
      "Gradient Descent(25/49): loss=0.36580522980102137\n",
      "Gradient Descent(26/49): loss=0.36559202801013563\n",
      "Gradient Descent(27/49): loss=0.36540034720040665\n",
      "Gradient Descent(28/49): loss=0.3652277896210217\n",
      "Gradient Descent(29/49): loss=0.36507223406661093\n",
      "Gradient Descent(30/49): loss=0.36493180325067287\n",
      "Gradient Descent(31/49): loss=0.36480483513462564\n",
      "Gradient Descent(32/49): loss=0.3646898577129957\n",
      "Gradient Descent(33/49): loss=0.3645855668222922\n",
      "Gradient Descent(34/49): loss=0.36449080659836663\n",
      "Gradient Descent(35/49): loss=0.36440455225610524\n",
      "Gradient Descent(36/49): loss=0.3643258949074374\n",
      "Gradient Descent(37/49): loss=0.3642540281699444\n",
      "Gradient Descent(38/49): loss=0.3641882363497024\n",
      "Gradient Descent(39/49): loss=0.36412788400912643\n",
      "Gradient Descent(40/49): loss=0.36407240675411434\n",
      "Gradient Descent(41/49): loss=0.36402130309524355\n",
      "Gradient Descent(42/49): loss=0.36397412725557166\n",
      "Gradient Descent(43/49): loss=0.36393048281312035\n",
      "Gradient Descent(44/49): loss=0.3638900170796671\n",
      "Gradient Descent(45/49): loss=0.36385241612932834\n",
      "Gradient Descent(46/49): loss=0.3638174004007797\n",
      "Gradient Descent(47/49): loss=0.36378472080606195\n",
      "Gradient Descent(48/49): loss=0.36375415528688015\n",
      "Gradient Descent(49/49): loss=0.3637255057663157\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40678598744009786\n",
      "Gradient Descent(2/49): loss=0.3936751308033559\n",
      "Gradient Descent(3/49): loss=0.38936683235039804\n",
      "Gradient Descent(4/49): loss=0.38639485435049753\n",
      "Gradient Descent(5/49): loss=0.3838786230985976\n",
      "Gradient Descent(6/49): loss=0.3816749965477691\n",
      "Gradient Descent(7/49): loss=0.3797327814042901\n",
      "Gradient Descent(8/49): loss=0.37801725413521114\n",
      "Gradient Descent(9/49): loss=0.3764999543870387\n",
      "Gradient Descent(10/49): loss=0.3751565617670086\n",
      "Gradient Descent(11/49): loss=0.3739660300999256\n",
      "Gradient Descent(12/49): loss=0.37291004164666447\n",
      "Gradient Descent(13/49): loss=0.3719725999487255\n",
      "Gradient Descent(14/49): loss=0.3711397044196569\n",
      "Gradient Descent(15/49): loss=0.3703990811422282\n",
      "Gradient Descent(16/49): loss=0.3697399558654113\n",
      "Gradient Descent(17/49): loss=0.36915286043247464\n",
      "Gradient Descent(18/49): loss=0.36862946656562817\n",
      "Gradient Descent(19/49): loss=0.3681624424759053\n",
      "Gradient Descent(20/49): loss=0.3677453287464063\n",
      "Gradient Descent(21/49): loss=0.367372430616161\n",
      "Gradient Descent(22/49): loss=0.3670387242953839\n",
      "Gradient Descent(23/49): loss=0.3667397753336948\n",
      "Gradient Descent(24/49): loss=0.3664716673753298\n",
      "Gradient Descent(25/49): loss=0.366230939889979\n",
      "Gradient Descent(26/49): loss=0.36601453367798636\n",
      "Gradient Descent(27/49): loss=0.36581974312353777\n",
      "Gradient Descent(28/49): loss=0.3656441743159983\n",
      "Gradient Descent(29/49): loss=0.36548570828297333\n",
      "Gradient Descent(30/49): loss=0.36534246868306425\n",
      "Gradient Descent(31/49): loss=0.3652127933949315\n",
      "Gradient Descent(32/49): loss=0.36509520951481267\n",
      "Gradient Descent(33/49): loss=0.36498841133919935\n",
      "Gradient Descent(34/49): loss=0.3648912409647172\n",
      "Gradient Descent(35/49): loss=0.3648026711848221\n",
      "Gradient Descent(36/49): loss=0.36472179040391167\n",
      "Gradient Descent(37/49): loss=0.36464778932484976\n",
      "Gradient Descent(38/49): loss=0.3645799491965429\n",
      "Gradient Descent(39/49): loss=0.364517631434779\n",
      "Gradient Descent(40/49): loss=0.3644602684526243\n",
      "Gradient Descent(41/49): loss=0.36440735555677206\n",
      "Gradient Descent(42/49): loss=0.36435844378374627\n",
      "Gradient Descent(43/49): loss=0.36431313356515743\n",
      "Gradient Descent(44/49): loss=0.36427106912456925\n",
      "Gradient Descent(45/49): loss=0.3642319335202316\n",
      "Gradient Descent(46/49): loss=0.36419544425817857\n",
      "Gradient Descent(47/49): loss=0.3641613494091787\n",
      "Gradient Descent(48/49): loss=0.36412942417090827\n",
      "Gradient Descent(49/49): loss=0.36409946782364855\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40572555713156866\n",
      "Gradient Descent(2/49): loss=0.3916343549011311\n",
      "Gradient Descent(3/49): loss=0.38727227810611153\n",
      "Gradient Descent(4/49): loss=0.38449351670922216\n",
      "Gradient Descent(5/49): loss=0.38225158265743076\n",
      "Gradient Descent(6/49): loss=0.3803491053222667\n",
      "Gradient Descent(7/49): loss=0.3787083376822722\n",
      "Gradient Descent(8/49): loss=0.3772794435467231\n",
      "Gradient Descent(9/49): loss=0.3760255329579372\n",
      "Gradient Descent(10/49): loss=0.37491818938039817\n",
      "Gradient Descent(11/49): loss=0.37393509654880147\n",
      "Gradient Descent(12/49): loss=0.3730584580185143\n",
      "Gradient Descent(13/49): loss=0.37227387711757726\n",
      "Gradient Descent(14/49): loss=0.3715695467749572\n",
      "Gradient Descent(15/49): loss=0.3709356570484017\n",
      "Gradient Descent(16/49): loss=0.37036395802536737\n",
      "Gradient Descent(17/49): loss=0.36984743455960445\n",
      "Gradient Descent(18/49): loss=0.36938006201356005\n",
      "Gradient Descent(19/49): loss=0.3689566210106756\n",
      "Gradient Descent(20/49): loss=0.36857255542315\n",
      "Gradient Descent(21/49): loss=0.36822386223510606\n",
      "Gradient Descent(22/49): loss=0.3679070050686379\n",
      "Gradient Descent(23/49): loss=0.3676188454126257\n",
      "Gradient Descent(24/49): loss=0.3673565872109097\n",
      "Gradient Descent(25/49): loss=0.36711773162998085\n",
      "Gradient Descent(26/49): loss=0.3669000396660589\n",
      "Gradient Descent(27/49): loss=0.3667015008591335\n",
      "Gradient Descent(28/49): loss=0.36652030682269787\n",
      "Gradient Descent(29/49): loss=0.36635482861926894\n",
      "Gradient Descent(30/49): loss=0.36620359724680784\n",
      "Gradient Descent(31/49): loss=0.366065286673803\n",
      "Gradient Descent(32/49): loss=0.36593869898827514\n",
      "Gradient Descent(33/49): loss=0.3658227513206773\n",
      "Gradient Descent(34/49): loss=0.3657164642715047\n",
      "Gradient Descent(35/49): loss=0.36561895162783575\n",
      "Gradient Descent(36/49): loss=0.36552941119363846\n",
      "Gradient Descent(37/49): loss=0.3654471165898585\n",
      "Gradient Descent(38/49): loss=0.3653714099045148\n",
      "Gradient Descent(39/49): loss=0.36530169509202254\n",
      "Gradient Descent(40/49): loss=0.3652374320360635\n",
      "Gradient Descent(41/49): loss=0.3651781312024648\n",
      "Gradient Descent(42/49): loss=0.36512334881843117\n",
      "Gradient Descent(43/49): loss=0.36507268252263664\n",
      "Gradient Descent(44/49): loss=0.36502576743747206\n",
      "Gradient Descent(45/49): loss=0.36498227262048616\n",
      "Gradient Descent(46/49): loss=0.36494189785694175\n",
      "Gradient Descent(47/49): loss=0.36490437075961024\n",
      "Gradient Descent(48/49): loss=0.36486944414556555\n",
      "Gradient Descent(49/49): loss=0.3648368936629248\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40486464082459184\n",
      "Gradient Descent(2/49): loss=0.392805970967343\n",
      "Gradient Descent(3/49): loss=0.38868610003193316\n",
      "Gradient Descent(4/49): loss=0.38571635861739956\n",
      "Gradient Descent(5/49): loss=0.3831869052157709\n",
      "Gradient Descent(6/49): loss=0.38098060617256685\n",
      "Gradient Descent(7/49): loss=0.37904748589824\n",
      "Gradient Descent(8/49): loss=0.37735065865278494\n",
      "Gradient Descent(9/49): loss=0.3758593795132532\n",
      "Gradient Descent(10/49): loss=0.3745473658196912\n",
      "Gradient Descent(11/49): loss=0.3733919574308815\n",
      "Gradient Descent(12/49): loss=0.3723735322800102\n",
      "Gradient Descent(13/49): loss=0.3714750522673756\n",
      "Gradient Descent(14/49): loss=0.3706816938172351\n",
      "Gradient Descent(15/49): loss=0.36998054018681037\n",
      "Gradient Descent(16/49): loss=0.3693603218211426\n",
      "Gradient Descent(17/49): loss=0.3688111955064835\n",
      "Gradient Descent(18/49): loss=0.36832455553798255\n",
      "Gradient Descent(19/49): loss=0.3678928716478332\n",
      "Gradient Descent(20/49): loss=0.36750954948862324\n",
      "Gradient Descent(21/49): loss=0.3671688102378773\n",
      "Gradient Descent(22/49): loss=0.366865586483985\n",
      "Gradient Descent(23/49): loss=0.36659543202500144\n",
      "Gradient Descent(24/49): loss=0.36635444359247543\n",
      "Gradient Descent(25/49): loss=0.36613919282366525\n",
      "Gradient Descent(26/49): loss=0.3659466670620856\n",
      "Gradient Descent(27/49): loss=0.3657742177792798\n",
      "Gradient Descent(28/49): loss=0.365619515588392\n",
      "Gradient Descent(29/49): loss=0.36548051096907197\n",
      "Gradient Descent(30/49): loss=0.36535539994863275\n",
      "Gradient Descent(31/49): loss=0.3652425940903369\n",
      "Gradient Descent(32/49): loss=0.3651406942295269\n",
      "Gradient Descent(33/49): loss=0.3650484674747487\n",
      "Gradient Descent(34/49): loss=0.36496482705621835\n",
      "Gradient Descent(35/49): loss=0.3648888146597815\n",
      "Gradient Descent(36/49): loss=0.36481958493236477\n",
      "Gradient Descent(37/49): loss=0.3647563918860658\n",
      "Gradient Descent(38/49): loss=0.3646985769634784\n",
      "Gradient Descent(39/49): loss=0.36464555855746084\n",
      "Gradient Descent(40/49): loss=0.3645968228050235\n",
      "Gradient Descent(41/49): loss=0.36455191549795113\n",
      "Gradient Descent(42/49): loss=0.36451043497267444\n",
      "Gradient Descent(43/49): loss=0.3644720258591999\n",
      "Gradient Descent(44/49): loss=0.36443637358394565\n",
      "Gradient Descent(45/49): loss=0.3644031995344391\n",
      "Gradient Descent(46/49): loss=0.36437225680525404\n",
      "Gradient Descent(47/49): loss=0.36434332645453243\n",
      "Gradient Descent(48/49): loss=0.3643162142091513\n",
      "Gradient Descent(49/49): loss=0.36429074756419866\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4047721477525337\n",
      "Gradient Descent(2/49): loss=0.39269146742222033\n",
      "Gradient Descent(3/49): loss=0.38853858294331867\n",
      "Gradient Descent(4/49): loss=0.385534278694696\n",
      "Gradient Descent(5/49): loss=0.3829716111400891\n",
      "Gradient Descent(6/49): loss=0.38073427829541734\n",
      "Gradient Descent(7/49): loss=0.37877240315221267\n",
      "Gradient Descent(8/49): loss=0.3770489709673705\n",
      "Gradient Descent(9/49): loss=0.3755330607000908\n",
      "Gradient Descent(10/49): loss=0.3741982221983096\n",
      "Gradient Descent(11/49): loss=0.3730216488920318\n",
      "Gradient Descent(12/49): loss=0.37198359242392515\n",
      "Gradient Descent(13/49): loss=0.3710669045191512\n",
      "Gradient Descent(14/49): loss=0.37025666362132004\n",
      "Gradient Descent(15/49): loss=0.36953986416066986\n",
      "Gradient Descent(16/49): loss=0.3689051547590517\n",
      "Gradient Descent(17/49): loss=0.3683426159383265\n",
      "Gradient Descent(18/49): loss=0.36784357034877696\n",
      "Gradient Descent(19/49): loss=0.3674004201012749\n",
      "Gradient Descent(20/49): loss=0.3670065068780317\n",
      "Gradient Descent(21/49): loss=0.366655991303799\n",
      "Gradient Descent(22/49): loss=0.36634374868067676\n",
      "Gradient Descent(23/49): loss=0.36606527868048216\n",
      "Gradient Descent(24/49): loss=0.36581662698293615\n",
      "Gradient Descent(25/49): loss=0.3655943171684632\n",
      "Gradient Descent(26/49): loss=0.36539529143725746\n",
      "Gradient Descent(27/49): loss=0.3652168589433307\n",
      "Gradient Descent(28/49): loss=0.3650566507125734\n",
      "Gradient Descent(29/49): loss=0.36491258026442225\n",
      "Gradient Descent(30/49): loss=0.36478280918304623\n",
      "Gradient Descent(31/49): loss=0.3646657169903848\n",
      "Gradient Descent(32/49): loss=0.3645598747633913\n",
      "Gradient Descent(33/49): loss=0.3644640220142611\n",
      "Gradient Descent(34/49): loss=0.36437704641751145\n",
      "Gradient Descent(35/49): loss=0.36429796602340686\n",
      "Gradient Descent(36/49): loss=0.36422591364487683\n",
      "Gradient Descent(37/49): loss=0.3641601231460195\n",
      "Gradient Descent(38/49): loss=0.3640999173955408\n",
      "Gradient Descent(39/49): loss=0.3640446976789133\n",
      "Gradient Descent(40/49): loss=0.36399393438935307\n",
      "Gradient Descent(41/49): loss=0.3639471588405114\n",
      "Gradient Descent(42/49): loss=0.3639039560635652\n",
      "Gradient Descent(43/49): loss=0.36386395846858505\n",
      "Gradient Descent(44/49): loss=0.3638268402650214\n",
      "Gradient Descent(45/49): loss=0.3637923125491921\n",
      "Gradient Descent(46/49): loss=0.36376011897802746\n",
      "Gradient Descent(47/49): loss=0.36373003195825926\n",
      "Gradient Descent(48/49): loss=0.3637018492889225\n",
      "Gradient Descent(49/49): loss=0.36367539120262976\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40504499714273284\n",
      "Gradient Descent(2/49): loss=0.3930787210737998\n",
      "Gradient Descent(3/49): loss=0.38895517281787795\n",
      "Gradient Descent(4/49): loss=0.3859672375488189\n",
      "Gradient Descent(5/49): loss=0.38341728954058335\n",
      "Gradient Descent(6/49): loss=0.3811895249374334\n",
      "Gradient Descent(7/49): loss=0.37923427036879337\n",
      "Gradient Descent(8/49): loss=0.37751494704757715\n",
      "Gradient Descent(9/49): loss=0.3760011242605678\n",
      "Gradient Descent(10/49): loss=0.3746667959854514\n",
      "Gradient Descent(11/49): loss=0.3734895249234144\n",
      "Gradient Descent(12/49): loss=0.37244985845826223\n",
      "Gradient Descent(13/49): loss=0.37153088120934896\n",
      "Gradient Descent(14/49): loss=0.3707178536857636\n",
      "Gradient Descent(15/49): loss=0.36999791224361656\n",
      "Gradient Descent(16/49): loss=0.369359816038664\n",
      "Gradient Descent(17/49): loss=0.3687937316433707\n",
      "Gradient Descent(18/49): loss=0.36829104864816525\n",
      "Gradient Descent(19/49): loss=0.36784422114866033\n",
      "Gradient Descent(20/49): loss=0.3674466310681832\n",
      "Gradient Descent(21/49): loss=0.36709247001746803\n",
      "Gradient Descent(22/49): loss=0.36677663696512075\n",
      "Gradient Descent(23/49): loss=0.3664946494428093\n",
      "Gradient Descent(24/49): loss=0.3662425663719241\n",
      "Gradient Descent(25/49): loss=0.36601692089498516\n",
      "Gradient Descent(26/49): loss=0.36581466183983813\n",
      "Gradient Descent(27/49): loss=0.3656331026481752\n",
      "Gradient Descent(28/49): loss=0.36546987677006465\n",
      "Gradient Descent(29/49): loss=0.36532289866910866\n",
      "Gradient Descent(30/49): loss=0.3651903297034178\n",
      "Gradient Descent(31/49): loss=0.3650705482496838\n",
      "Gradient Descent(32/49): loss=0.36496212352435436\n",
      "Gradient Descent(33/49): loss=0.36486379262983193\n",
      "Gradient Descent(34/49): loss=0.36477444041678253\n",
      "Gradient Descent(35/49): loss=0.36469308180777926\n",
      "Gradient Descent(36/49): loss=0.36461884627400865\n",
      "Gradient Descent(37/49): loss=0.36455096419681193\n",
      "Gradient Descent(38/49): loss=0.3644887548803888\n",
      "Gradient Descent(39/49): loss=0.36443161601186147\n",
      "Gradient Descent(40/49): loss=0.36437901439076914\n",
      "Gradient Descent(41/49): loss=0.3643304777725085\n",
      "Gradient Descent(42/49): loss=0.3642855876897364\n",
      "Gradient Descent(43/49): loss=0.36424397313271084\n",
      "Gradient Descent(44/49): loss=0.36420530498432835\n",
      "Gradient Descent(45/49): loss=0.36416929111849844\n",
      "Gradient Descent(46/49): loss=0.36413567208174297\n",
      "Gradient Descent(47/49): loss=0.36410421728774145\n",
      "Gradient Descent(48/49): loss=0.3640747216631301\n",
      "Gradient Descent(49/49): loss=0.3640470026903882\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4038873968168461\n",
      "Gradient Descent(2/49): loss=0.39099704572701155\n",
      "Gradient Descent(3/49): loss=0.3868687040153165\n",
      "Gradient Descent(4/49): loss=0.38410172374034957\n",
      "Gradient Descent(5/49): loss=0.38184591297053583\n",
      "Gradient Descent(6/49): loss=0.37993396497793486\n",
      "Gradient Descent(7/49): loss=0.378290054433996\n",
      "Gradient Descent(8/49): loss=0.3768628665277069\n",
      "Gradient Descent(9/49): loss=0.37561413683353656\n",
      "Gradient Descent(10/49): loss=0.37451447399543497\n",
      "Gradient Descent(11/49): loss=0.3735408882615838\n",
      "Gradient Descent(12/49): loss=0.3726751071681824\n",
      "Gradient Descent(13/49): loss=0.37190238436673057\n",
      "Gradient Descent(14/49): loss=0.3712106446158246\n",
      "Gradient Descent(15/49): loss=0.37058986386059256\n",
      "Gradient Descent(16/49): loss=0.3700316155946832\n",
      "Gradient Descent(17/49): loss=0.3695287357139513\n",
      "Gradient Descent(18/49): loss=0.369075072336993\n",
      "Gradient Descent(19/49): loss=0.3686652969277329\n",
      "Gradient Descent(20/49): loss=0.36829475993556354\n",
      "Gradient Descent(21/49): loss=0.367959378998315\n",
      "Gradient Descent(22/49): loss=0.36765555115802123\n",
      "Gradient Descent(23/49): loss=0.36738008294763624\n",
      "Gradient Descent(24/49): loss=0.3671301339152677\n",
      "Gradient Descent(25/49): loss=0.36690317036800835\n",
      "Gradient Descent(26/49): loss=0.36669692698488005\n",
      "Gradient Descent(27/49): loss=0.3665093745694787\n",
      "Gradient Descent(28/49): loss=0.3663386926592537\n",
      "Gradient Descent(29/49): loss=0.36618324603047475\n",
      "Gradient Descent(30/49): loss=0.36604156437156543\n",
      "Gradient Descent(31/49): loss=0.36591232456787554\n",
      "Gradient Descent(32/49): loss=0.36579433516606624\n",
      "Gradient Descent(33/49): loss=0.36568652267882285\n",
      "Gradient Descent(34/49): loss=0.3655879194596623\n",
      "Gradient Descent(35/49): loss=0.3654976529296259\n",
      "Gradient Descent(36/49): loss=0.3654149359772781\n",
      "Gradient Descent(37/49): loss=0.3653390583839654\n",
      "Gradient Descent(38/49): loss=0.3652693791501342\n",
      "Gradient Descent(39/49): loss=0.3652053196173477\n",
      "Gradient Descent(40/49): loss=0.365146357295766\n",
      "Gradient Descent(41/49): loss=0.36509202031912014\n",
      "Gradient Descent(42/49): loss=0.3650418824593249\n",
      "Gradient Descent(43/49): loss=0.3649955586412938\n",
      "Gradient Descent(44/49): loss=0.36495270090562254\n",
      "Gradient Descent(45/49): loss=0.3649129947728498\n",
      "Gradient Descent(46/49): loss=0.36487615596820727\n",
      "Gradient Descent(47/49): loss=0.36484192747027394\n",
      "Gradient Descent(48/49): loss=0.36481007685087846\n",
      "Gradient Descent(49/49): loss=0.36478039387704764\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4032518051396025\n",
      "Gradient Descent(2/49): loss=0.3922797356039105\n",
      "Gradient Descent(3/49): loss=0.3882975770086906\n",
      "Gradient Descent(4/49): loss=0.38530241819521055\n",
      "Gradient Descent(5/49): loss=0.38274043777527994\n",
      "Gradient Descent(6/49): loss=0.3805132586480469\n",
      "Gradient Descent(7/49): loss=0.37857054397646234\n",
      "Gradient Descent(8/49): loss=0.37687313053788507\n",
      "Gradient Descent(9/49): loss=0.3753881752643263\n",
      "Gradient Descent(10/49): loss=0.3740876614672961\n",
      "Gradient Descent(11/49): loss=0.3729475249280775\n",
      "Gradient Descent(12/49): loss=0.3719470166581633\n",
      "Gradient Descent(13/49): loss=0.3710682006048996\n",
      "Gradient Descent(14/49): loss=0.3702955431001123\n",
      "Gradient Descent(15/49): loss=0.3696155708059179\n",
      "Gradient Descent(16/49): loss=0.3690165826093055\n",
      "Gradient Descent(17/49): loss=0.36848840531151605\n",
      "Gradient Descent(18/49): loss=0.36802218549250404\n",
      "Gradient Descent(19/49): loss=0.367610211574268\n",
      "Gradient Descent(20/49): loss=0.3672457612720285\n",
      "Gradient Descent(21/49): loss=0.36692297049864114\n",
      "Gradient Descent(22/49): loss=0.36663672047130336\n",
      "Gradient Descent(23/49): loss=0.3663825403149468\n",
      "Gradient Descent(24/49): loss=0.36615652289791445\n",
      "Gradient Descent(25/49): loss=0.36595525199597917\n",
      "Gradient Descent(26/49): loss=0.3657757391774205\n",
      "Gradient Descent(27/49): loss=0.3656153690474773\n",
      "Gradient Descent(28/49): loss=0.36547185169487356\n",
      "Gradient Descent(29/49): loss=0.36534318135394817\n",
      "Gradient Descent(30/49): loss=0.36522760043931923\n",
      "Gradient Descent(31/49): loss=0.3651235682308231\n",
      "Gradient Descent(32/49): loss=0.3650297335886141\n",
      "Gradient Descent(33/49): loss=0.364944911164945\n",
      "Gradient Descent(34/49): loss=0.3648680606528493\n",
      "Gradient Descent(35/49): loss=0.3647982686748168\n",
      "Gradient Descent(36/49): loss=0.36473473296830977\n",
      "Gradient Descent(37/49): loss=0.3646767485710569\n",
      "Gradient Descent(38/49): loss=0.36462369574863834\n",
      "Gradient Descent(39/49): loss=0.3645750294409346\n",
      "Gradient Descent(40/49): loss=0.36453027003338906\n",
      "Gradient Descent(41/49): loss=0.36448899528437695\n",
      "Gradient Descent(42/49): loss=0.3644508332619093\n",
      "Gradient Descent(43/49): loss=0.3644154561618796\n",
      "Gradient Descent(44/49): loss=0.364382574896522\n",
      "Gradient Descent(45/49): loss=0.36435193435602453\n",
      "Gradient Descent(46/49): loss=0.364323309258658\n",
      "Gradient Descent(47/49): loss=0.36429650051554957\n",
      "Gradient Descent(48/49): loss=0.3642713320456296\n",
      "Gradient Descent(49/49): loss=0.36424764798443876\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4031578336034635\n",
      "Gradient Descent(2/49): loss=0.39216316406494184\n",
      "Gradient Descent(3/49): loss=0.3881462529618494\n",
      "Gradient Descent(4/49): loss=0.3851152419693221\n",
      "Gradient Descent(5/49): loss=0.382519115823892\n",
      "Gradient Descent(6/49): loss=0.3802602144839799\n",
      "Gradient Descent(7/49): loss=0.3782882203733745\n",
      "Gradient Descent(8/49): loss=0.3765637969604708\n",
      "Gradient Descent(9/49): loss=0.375053901364895\n",
      "Gradient Descent(10/49): loss=0.37373033587938687\n",
      "Gradient Descent(11/49): loss=0.37256888088767703\n",
      "Gradient Descent(12/49): loss=0.37154865401475873\n",
      "Gradient Descent(13/49): loss=0.37065160262900704\n",
      "Gradient Descent(14/49): loss=0.36986208891202355\n",
      "Gradient Descent(15/49): loss=0.36916654463614446\n",
      "Gradient Descent(16/49): loss=0.3685531809223154\n",
      "Gradient Descent(17/49): loss=0.36801174254830255\n",
      "Gradient Descent(18/49): loss=0.3675332989467937\n",
      "Gradient Descent(19/49): loss=0.36711006573663413\n",
      "Gradient Descent(20/49): loss=0.3667352518495563\n",
      "Gradient Descent(21/49): loss=0.3664029282324691\n",
      "Gradient Descent(22/49): loss=0.3661079148186829\n",
      "Gradient Descent(23/49): loss=0.3658456830273238\n",
      "Gradient Descent(24/49): loss=0.36561227150530595\n",
      "Gradient Descent(25/49): loss=0.3654042131959169\n",
      "Gradient Descent(26/49): loss=0.3652184721207139\n",
      "Gradient Descent(27/49): loss=0.36505238851079835\n",
      "Gradient Descent(28/49): loss=0.3649036311301864\n",
      "Gradient Descent(29/49): loss=0.36477015580611205\n",
      "Gradient Descent(30/49): loss=0.3646501693251328\n",
      "Gradient Descent(31/49): loss=0.36454209797494197\n",
      "Gradient Descent(32/49): loss=0.36444456011391774\n",
      "Gradient Descent(33/49): loss=0.3643563422368934\n",
      "Gradient Descent(34/49): loss=0.3642763780791035\n",
      "Gradient Descent(35/49): loss=0.3642037303628455\n",
      "Gradient Descent(36/49): loss=0.36413757484489345\n",
      "Gradient Descent(37/49): loss=0.36407718636851777\n",
      "Gradient Descent(38/49): loss=0.3640219266633193\n",
      "Gradient Descent(39/49): loss=0.3639712336699398\n",
      "Gradient Descent(40/49): loss=0.3639246121959034\n",
      "Gradient Descent(41/49): loss=0.36388162573405497\n",
      "Gradient Descent(42/49): loss=0.3638418892968633\n",
      "Gradient Descent(43/49): loss=0.3638050631387469\n",
      "Gradient Descent(44/49): loss=0.3637708472549617\n",
      "Gradient Descent(45/49): loss=0.3637389765598093\n",
      "Gradient Descent(46/49): loss=0.36370921665928874\n",
      "Gradient Descent(47/49): loss=0.36368136014406865\n",
      "Gradient Descent(48/49): loss=0.3636552233380167\n",
      "Gradient Descent(49/49): loss=0.3636306434456835\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4034406115884686\n",
      "Gradient Descent(2/49): loss=0.39255541969141433\n",
      "Gradient Descent(3/49): loss=0.3885652236063613\n",
      "Gradient Descent(4/49): loss=0.3855504098360079\n",
      "Gradient Descent(5/49): loss=0.3829669544771185\n",
      "Gradient Descent(6/49): loss=0.38071734451977585\n",
      "Gradient Descent(7/49): loss=0.37875154047080123\n",
      "Gradient Descent(8/49): loss=0.3770307314637088\n",
      "Gradient Descent(9/49): loss=0.3755224205651192\n",
      "Gradient Descent(10/49): loss=0.37419888401350226\n",
      "Gradient Descent(11/49): loss=0.3730362860250619\n",
      "Gradient Descent(12/49): loss=0.37201404602690036\n",
      "Gradient Descent(13/49): loss=0.37111434571043983\n",
      "Gradient Descent(14/49): loss=0.37032172828201015\n",
      "Gradient Descent(15/49): loss=0.36962276513663744\n",
      "Gradient Descent(16/49): loss=0.3690057750394087\n",
      "Gradient Descent(17/49): loss=0.36846058572037776\n",
      "Gradient Descent(18/49): loss=0.3679783304546616\n",
      "Gradient Descent(19/49): loss=0.36755127386117287\n",
      "Gradient Descent(20/49): loss=0.36717266229792356\n",
      "Gradient Descent(21/49): loss=0.3668365950776544\n",
      "Gradient Descent(22/49): loss=0.3665379133814423\n",
      "Gradient Descent(23/49): loss=0.36627210426764756\n",
      "Gradient Descent(24/49): loss=0.36603521759379987\n",
      "Gradient Descent(25/49): loss=0.36582379401271486\n",
      "Gradient Descent(26/49): loss=0.365634802487493\n",
      "Gradient Descent(27/49): loss=0.36546558600516427\n",
      "Gradient Descent(28/49): loss=0.3653138143648082\n",
      "Gradient Descent(29/49): loss=0.3651774430802286\n",
      "Gradient Descent(30/49): loss=0.365054677575424\n",
      "Gradient Descent(31/49): loss=0.36494394196771873\n",
      "Gradient Descent(32/49): loss=0.3648438518322106\n",
      "Gradient Descent(33/49): loss=0.3647531904251319\n",
      "Gradient Descent(34/49): loss=0.36467088791524455\n",
      "Gradient Descent(35/49): loss=0.36459600323350216\n",
      "Gradient Descent(36/49): loss=0.3645277082035512\n",
      "Gradient Descent(37/49): loss=0.3644652736605734\n",
      "Gradient Descent(38/49): loss=0.36440805730461234\n",
      "Gradient Descent(39/49): loss=0.3643554930678357\n",
      "Gradient Descent(40/49): loss=0.36430708180393373\n",
      "Gradient Descent(41/49): loss=0.3642623831327079\n",
      "Gradient Descent(42/49): loss=0.36422100829443166\n",
      "Gradient Descent(43/49): loss=0.36418261388721795\n",
      "Gradient Descent(44/49): loss=0.36414689637682707\n",
      "Gradient Descent(45/49): loss=0.3641135872824144\n",
      "Gradient Descent(46/49): loss=0.3640824489539585\n",
      "Gradient Descent(47/49): loss=0.3640532708677573\n",
      "Gradient Descent(48/49): loss=0.3640258663756585\n",
      "Gradient Descent(49/49): loss=0.3640000698517783\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40218166463513105\n",
      "Gradient Descent(2/49): loss=0.3904397348727311\n",
      "Gradient Descent(3/49): loss=0.3864908139653497\n",
      "Gradient Descent(4/49): loss=0.38372293444777955\n",
      "Gradient Descent(5/49): loss=0.38145248474935783\n",
      "Gradient Descent(6/49): loss=0.3795323818808504\n",
      "Gradient Descent(7/49): loss=0.37788671650918504\n",
      "Gradient Descent(8/49): loss=0.3764623964824772\n",
      "Gradient Descent(9/49): loss=0.3752197857707916\n",
      "Gradient Descent(10/49): loss=0.37412856075032674\n",
      "Gradient Descent(11/49): loss=0.3731650956846681\n",
      "Gradient Descent(12/49): loss=0.37231066881659236\n",
      "Gradient Descent(13/49): loss=0.3715502015388517\n",
      "Gradient Descent(14/49): loss=0.37087136115375535\n",
      "Gradient Descent(15/49): loss=0.3702639158975934\n",
      "Gradient Descent(16/49): loss=0.3697192666338797\n",
      "Gradient Descent(17/49): loss=0.3692301031535605\n",
      "Gradient Descent(18/49): loss=0.3687901489411277\n",
      "Gradient Descent(19/49): loss=0.36839396917664985\n",
      "Gradient Descent(20/49): loss=0.36803682427833\n",
      "Gradient Descent(21/49): loss=0.36771455651960183\n",
      "Gradient Descent(22/49): loss=0.3674235008981948\n",
      "Gradient Descent(23/49): loss=0.36716041398140126\n",
      "Gradient Descent(24/49): loss=0.3669224162375655\n",
      "Gradient Descent(25/49): loss=0.3667069446200395\n",
      "Gradient Descent(26/49): loss=0.36651171305662295\n",
      "Gradient Descent(27/49): loss=0.3663346791259623\n",
      "Gradient Descent(28/49): loss=0.3661740156497723\n",
      "Gradient Descent(29/49): loss=0.36602808624991834\n",
      "Gradient Descent(30/49): loss=0.36589542414992476\n",
      "Gradient Descent(31/49): loss=0.36577471366764375\n",
      "Gradient Descent(32/49): loss=0.3656647739680486\n",
      "Gradient Descent(33/49): loss=0.3655645447353248\n",
      "Gradient Descent(34/49): loss=0.36547307349073943\n",
      "Gradient Descent(35/49): loss=0.36538950433357736\n",
      "Gradient Descent(36/49): loss=0.3653130679212953\n",
      "Gradient Descent(37/49): loss=0.3652430725351871\n",
      "Gradient Descent(38/49): loss=0.36517889610156484\n",
      "Gradient Descent(39/49): loss=0.365119979057394\n",
      "Gradient Descent(40/49): loss=0.3650658179646638\n",
      "Gradient Descent(41/49): loss=0.3650159597903621\n",
      "Gradient Descent(42/49): loss=0.36496999677941744\n",
      "Gradient Descent(43/49): loss=0.36492756185678843\n",
      "Gradient Descent(44/49): loss=0.3648883245023986\n",
      "Gradient Descent(45/49): loss=0.3648519870490623\n",
      "Gradient Descent(46/49): loss=0.3648182813591356\n",
      "Gradient Descent(47/49): loss=0.3647869658404923\n",
      "Gradient Descent(48/49): loss=0.3647578227666973\n",
      "Gradient Descent(49/49): loss=0.36473065587000303\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4017754477500647\n",
      "Gradient Descent(2/49): loss=0.39181584234409955\n",
      "Gradient Descent(3/49): loss=0.38792607281299535\n",
      "Gradient Descent(4/49): loss=0.38489794719420545\n",
      "Gradient Descent(5/49): loss=0.38230450156666673\n",
      "Gradient Descent(6/49): loss=0.3800588200811288\n",
      "Gradient Descent(7/49): loss=0.378108895876785\n",
      "Gradient Descent(8/49): loss=0.3764130439567301\n",
      "Gradient Descent(9/49): loss=0.37493625893247395\n",
      "Gradient Descent(10/49): loss=0.3736487706477237\n",
      "Gradient Descent(11/49): loss=0.3725251119329403\n",
      "Gradient Descent(12/49): loss=0.371543420431308\n",
      "Gradient Descent(13/49): loss=0.37068488335038724\n",
      "Gradient Descent(14/49): loss=0.36993328254653\n",
      "Gradient Descent(15/49): loss=0.3692746158418173\n",
      "Gradient Descent(16/49): loss=0.36869677888927443\n",
      "Gradient Descent(17/49): loss=0.368189296324828\n",
      "Gradient Descent(18/49): loss=0.36774309361064306\n",
      "Gradient Descent(19/49): loss=0.3673503027726634\n",
      "Gradient Descent(20/49): loss=0.36700409654522703\n",
      "Gradient Descent(21/49): loss=0.36669854643648425\n",
      "Gradient Descent(22/49): loss=0.3664285010148928\n",
      "Gradient Descent(23/49): loss=0.36618948134600543\n",
      "Gradient Descent(24/49): loss=0.36597759101746064\n",
      "Gradient Descent(25/49): loss=0.36578943860499225\n",
      "Gradient Descent(26/49): loss=0.3656220707729084\n",
      "Gradient Descent(27/49): loss=0.36547291448374397\n",
      "Gradient Descent(28/49): loss=0.36533972702515044\n",
      "Gradient Descent(29/49): loss=0.3652205527565941\n",
      "Gradient Descent(30/49): loss=0.36511368564121\n",
      "Gradient Descent(31/49): loss=0.36501763676490284\n",
      "Gradient Descent(32/49): loss=0.3649311061600566\n",
      "Gradient Descent(33/49): loss=0.3648529583486972\n",
      "Gradient Descent(34/49): loss=0.3647822011026287\n",
      "Gradient Descent(35/49): loss=0.3647179669883796\n",
      "Gradient Descent(36/49): loss=0.3646594973247356\n",
      "Gradient Descent(37/49): loss=0.36460612823185734\n",
      "Gradient Descent(38/49): loss=0.3645572784948265\n",
      "Gradient Descent(39/49): loss=0.36451243900208413\n",
      "Gradient Descent(40/49): loss=0.3644711635515353\n",
      "Gradient Descent(41/49): loss=0.364433060844908\n",
      "Gradient Descent(42/49): loss=0.3643977875149105\n",
      "Gradient Descent(43/49): loss=0.36436504205040576\n",
      "Gradient Descent(44/49): loss=0.3643345595026734\n",
      "Gradient Descent(45/49): loss=0.3643061068712627\n",
      "Gradient Descent(46/49): loss=0.36427947908129543\n",
      "Gradient Descent(47/49): loss=0.3642544954756434\n",
      "Gradient Descent(48/49): loss=0.3642309967554222\n",
      "Gradient Descent(49/49): loss=0.3642088423109378\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40168013683593456\n",
      "Gradient Descent(2/49): loss=0.39169709483872184\n",
      "Gradient Descent(3/49): loss=0.38777090484958915\n",
      "Gradient Descent(4/49): loss=0.38470569081308903\n",
      "Gradient Descent(5/49): loss=0.3820772169488675\n",
      "Gradient Descent(6/49): loss=0.3797991646636174\n",
      "Gradient Descent(7/49): loss=0.3778194688908353\n",
      "Gradient Descent(8/49): loss=0.37609623039037393\n",
      "Gradient Descent(9/49): loss=0.37459422260372205\n",
      "Gradient Descent(10/49): loss=0.3732834819974662\n",
      "Gradient Descent(11/49): loss=0.3721383774070347\n",
      "Gradient Descent(12/49): loss=0.3711369059086176\n",
      "Gradient Descent(13/49): loss=0.37026013141660497\n",
      "Gradient Descent(14/49): loss=0.3694917250713168\n",
      "Gradient Descent(15/49): loss=0.3688175833412407\n",
      "Gradient Descent(16/49): loss=0.36822550780948693\n",
      "Gradient Descent(17/49): loss=0.36770493503105445\n",
      "Gradient Descent(18/49): loss=0.36724670759179306\n",
      "Gradient Descent(19/49): loss=0.36684287937846616\n",
      "Gradient Descent(20/49): loss=0.3664865494427856\n",
      "Gradient Descent(21/49): loss=0.36617171988879865\n",
      "Gradient Descent(22/49): loss=0.3658931740310169\n",
      "Gradient Descent(23/49): loss=0.3656463717207185\n",
      "Gradient Descent(24/49): loss=0.3654273592604054\n",
      "Gradient Descent(25/49): loss=0.36523269175017814\n",
      "Gradient Descent(26/49): loss=0.36505936605595873\n",
      "Gradient Descent(27/49): loss=0.3649047628740426\n",
      "Gradient Descent(28/49): loss=0.36476659660166116\n",
      "Gradient Descent(29/49): loss=0.36464287191864386\n",
      "Gradient Descent(30/49): loss=0.3645318461483541\n",
      "Gradient Descent(31/49): loss=0.3644319966027868\n",
      "Gradient Descent(32/49): loss=0.36434199223174235\n",
      "Gradient Descent(33/49): loss=0.36426066899314063\n",
      "Gradient Descent(34/49): loss=0.36418700844385166\n",
      "Gradient Descent(35/49): loss=0.36412011912035774\n",
      "Gradient Descent(36/49): loss=0.3640592203381696\n",
      "Gradient Descent(37/49): loss=0.36400362808982933\n",
      "Gradient Descent(38/49): loss=0.36395274276491546\n",
      "Gradient Descent(39/49): loss=0.363906038452859\n",
      "Gradient Descent(40/49): loss=0.36386305362150856\n",
      "Gradient Descent(41/49): loss=0.3638233829920395\n",
      "Gradient Descent(42/49): loss=0.363786670454646\n",
      "Gradient Descent(43/49): loss=0.36375260289003297\n",
      "Gradient Descent(44/49): loss=0.3637209047795089\n",
      "Gradient Descent(45/49): loss=0.36369133350186406\n",
      "Gradient Descent(46/49): loss=0.3636636752285409\n",
      "Gradient Descent(47/49): loss=0.3636377413401442\n",
      "Gradient Descent(48/49): loss=0.36361336529735394\n",
      "Gradient Descent(49/49): loss=0.36359039990798486\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40197283077730533\n",
      "Gradient Descent(2/49): loss=0.3920936756480924\n",
      "Gradient Descent(3/49): loss=0.388192041583783\n",
      "Gradient Descent(4/49): loss=0.38514299532499535\n",
      "Gradient Descent(5/49): loss=0.38252711870714523\n",
      "Gradient Descent(6/49): loss=0.3802580385953682\n",
      "Gradient Descent(7/49): loss=0.3782840760707938\n",
      "Gradient Descent(8/49): loss=0.37656394778925256\n",
      "Gradient Descent(9/49): loss=0.37506302410757564\n",
      "Gradient Descent(10/49): loss=0.37375184113845955\n",
      "Gradient Descent(11/49): loss=0.3726051631811991\n",
      "Gradient Descent(12/49): loss=0.37160129319538704\n",
      "Gradient Descent(13/49): loss=0.37072152952440984\n",
      "Gradient Descent(14/49): loss=0.36994972240687773\n",
      "Gradient Descent(15/49): loss=0.36927190503484064\n",
      "Gradient Descent(16/49): loss=0.3686759833432813\n",
      "Gradient Descent(17/49): loss=0.36815147346928767\n",
      "Gradient Descent(18/49): loss=0.3676892785608265\n",
      "Gradient Descent(19/49): loss=0.367281498399263\n",
      "Gradient Descent(20/49): loss=0.36692126657031565\n",
      "Gradient Descent(21/49): loss=0.3666026108771515\n",
      "Gradient Descent(22/49): loss=0.36632033343912107\n",
      "Gradient Descent(23/49): loss=0.3660699075184581\n",
      "Gradient Descent(24/49): loss=0.36584738860202165\n",
      "Gradient Descent(25/49): loss=0.36564933766123486\n",
      "Gradient Descent(26/49): loss=0.3654727548392897\n",
      "Gradient Descent(27/49): loss=0.36531502208439426\n",
      "Gradient Descent(28/49): loss=0.3651738534721434\n",
      "Gradient Descent(29/49): loss=0.3650472521474543\n",
      "Gradient Descent(30/49): loss=0.36493347297362777\n",
      "Gradient Descent(31/49): loss=0.3648309901083485\n",
      "Gradient Descent(32/49): loss=0.3647384688381232\n",
      "Gradient Descent(33/49): loss=0.3646547410972672\n",
      "Gradient Descent(34/49): loss=0.36457878417793166\n",
      "Gradient Descent(35/49): loss=0.364509702206129\n",
      "Gradient Descent(36/49): loss=0.3644467100171676\n",
      "Gradient Descent(37/49): loss=0.3643891191139327\n",
      "Gradient Descent(38/49): loss=0.36433632543433286\n",
      "Gradient Descent(39/49): loss=0.36428779869107486\n",
      "Gradient Descent(40/49): loss=0.36424307307861736\n",
      "Gradient Descent(41/49): loss=0.3642017391694667\n",
      "Gradient Descent(42/49): loss=0.3641634368455353\n",
      "Gradient Descent(43/49): loss=0.36412784913063867\n",
      "Gradient Descent(44/49): loss=0.3640946968077981\n",
      "Gradient Descent(45/49): loss=0.3640637337202606\n",
      "Gradient Descent(46/49): loss=0.3640347426683319\n",
      "Gradient Descent(47/49): loss=0.3640075318255709\n",
      "Gradient Descent(48/49): loss=0.36398193160781045\n",
      "Gradient Descent(49/49): loss=0.36395779193709654\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40060836058642346\n",
      "Gradient Descent(2/49): loss=0.3899505122995593\n",
      "Gradient Descent(3/49): loss=0.38613306032540295\n",
      "Gradient Descent(4/49): loss=0.38335543986005516\n",
      "Gradient Descent(5/49): loss=0.3810705550429129\n",
      "Gradient Descent(6/49): loss=0.37914370883051834\n",
      "Gradient Descent(7/49): loss=0.3774975925173927\n",
      "Gradient Descent(8/49): loss=0.3760771972780327\n",
      "Gradient Descent(9/49): loss=0.3748415489499606\n",
      "Gradient Descent(10/49): loss=0.3737594377667869\n",
      "Gradient Descent(11/49): loss=0.3728066374701269\n",
      "Gradient Descent(12/49): loss=0.37196400074222774\n",
      "Gradient Descent(13/49): loss=0.3712161314153906\n",
      "Gradient Descent(14/49): loss=0.3705504479726495\n",
      "Gradient Descent(15/49): loss=0.3699565159759677\n",
      "Gradient Descent(16/49): loss=0.3694255668919771\n",
      "Gradient Descent(17/49): loss=0.3689501470422151\n",
      "Gradient Descent(18/49): loss=0.3685238580419585\n",
      "Gradient Descent(19/49): loss=0.36814116205950176\n",
      "Gradient Descent(20/49): loss=0.36779723340002757\n",
      "Gradient Descent(21/49): loss=0.3674878435239877\n",
      "Gradient Descent(22/49): loss=0.3672092704693087\n",
      "Gradient Descent(23/49): loss=0.3669582263127704\n",
      "Gradient Descent(24/49): loss=0.366731798153848\n",
      "Gradient Descent(25/49): loss=0.36652739938989803\n",
      "Gradient Descent(26/49): loss=0.3663427289496426\n",
      "Gradient Descent(27/49): loss=0.36617573678223947\n",
      "Gradient Descent(28/49): loss=0.36602459434410867\n",
      "Gradient Descent(29/49): loss=0.36588766914171217\n",
      "Gradient Descent(30/49): loss=0.36576350261469365\n",
      "Gradient Descent(31/49): loss=0.3656507908071469\n",
      "Gradient Descent(32/49): loss=0.365548367393962\n",
      "Gradient Descent(33/49): loss=0.3654551887171678\n",
      "Gradient Descent(34/49): loss=0.36537032055296975\n",
      "Gradient Descent(35/49): loss=0.36529292638007144\n",
      "Gradient Descent(36/49): loss=0.36522225695828087\n",
      "Gradient Descent(37/49): loss=0.3651576410564508\n",
      "Gradient Descent(38/49): loss=0.36509847719268085\n",
      "Gradient Descent(39/49): loss=0.36504422626897626\n",
      "Gradient Descent(40/49): loss=0.364994404998346\n",
      "Gradient Descent(41/49): loss=0.36494858003542235\n",
      "Gradient Descent(42/49): loss=0.36490636273270266\n",
      "Gradient Descent(43/49): loss=0.3648674044538737\n",
      "Gradient Descent(44/49): loss=0.36483139238370443\n",
      "Gradient Descent(45/49): loss=0.3647980457809326\n",
      "Gradient Descent(46/49): loss=0.36476711262661377\n",
      "Gradient Descent(47/49): loss=0.36473836662567855\n",
      "Gradient Descent(48/49): loss=0.3647116045240963\n",
      "Gradient Descent(49/49): loss=0.3646866437081328\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.4004355686559783\n",
      "Gradient Descent(2/49): loss=0.39140388966087486\n",
      "Gradient Descent(3/49): loss=0.3875677652776366\n",
      "Gradient Descent(4/49): loss=0.38450199289256043\n",
      "Gradient Descent(5/49): loss=0.3818787134360872\n",
      "Gradient Descent(6/49): loss=0.3796169082515096\n",
      "Gradient Descent(7/49): loss=0.3776620451639855\n",
      "Gradient Descent(8/49): loss=0.3759697596331071\n",
      "Gradient Descent(9/49): loss=0.3745028381605473\n",
      "Gradient Descent(10/49): loss=0.3732297452151047\n",
      "Gradient Descent(11/49): loss=0.37212361705180214\n",
      "Gradient Descent(12/49): loss=0.3711614955848816\n",
      "Gradient Descent(13/49): loss=0.37032371562523575\n",
      "Gradient Descent(14/49): loss=0.3695934024703131\n",
      "Gradient Descent(15/49): loss=0.3689560544450256\n",
      "Gradient Descent(16/49): loss=0.368399193278434\n",
      "Gradient Descent(17/49): loss=0.36791206974404483\n",
      "Gradient Descent(18/49): loss=0.36748541485265673\n",
      "Gradient Descent(19/49): loss=0.3671112288818238\n",
      "Gradient Descent(20/49): loss=0.36678260200911367\n",
      "Gradient Descent(21/49): loss=0.36649356146079776\n",
      "Gradient Descent(22/49): loss=0.3662389409903781\n",
      "Gradient Descent(23/49): loss=0.3660142692234294\n",
      "Gradient Descent(24/49): loss=0.3658156739884136\n",
      "Gradient Descent(25/49): loss=0.3656398002276552\n",
      "Gradient Descent(26/49): loss=0.3654837394712165\n",
      "Gradient Descent(27/49): loss=0.3653449691762876\n",
      "Gradient Descent(28/49): loss=0.36522130049935386\n",
      "Gradient Descent(29/49): loss=0.36511083328832655\n",
      "Gradient Descent(30/49): loss=0.36501191726533644\n",
      "Gradient Descent(31/49): loss=0.36492311852460035\n",
      "Gradient Descent(32/49): loss=0.3648431905989575\n",
      "Gradient Descent(33/49): loss=0.3647710494575973\n",
      "Gradient Descent(34/49): loss=0.3647057518896054\n",
      "Gradient Descent(35/49): loss=0.36464647680603884\n",
      "Gradient Descent(36/49): loss=0.3645925090596029\n",
      "Gradient Descent(37/49): loss=0.364543225437518\n",
      "Gradient Descent(38/49): loss=0.36449808253139254\n",
      "Gradient Descent(39/49): loss=0.36445660622914605\n",
      "Gradient Descent(40/49): loss=0.3644183826093217\n",
      "Gradient Descent(41/49): loss=0.36438305004839533\n",
      "Gradient Descent(42/49): loss=0.3643502923776709\n",
      "Gradient Descent(43/49): loss=0.3643198329486775\n",
      "Gradient Descent(44/49): loss=0.36429142948520377\n",
      "Gradient Descent(45/49): loss=0.3642648696166446\n",
      "Gradient Descent(46/49): loss=0.36423996700159517\n",
      "Gradient Descent(47/49): loss=0.36421655796292435\n",
      "Gradient Descent(48/49): loss=0.3641944985661739\n",
      "Gradient Descent(49/49): loss=0.3641736620822903\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40033905744994686\n",
      "Gradient Descent(2/49): loss=0.3912828378887742\n",
      "Gradient Descent(3/49): loss=0.38740872104776525\n",
      "Gradient Descent(4/49): loss=0.38430467826227677\n",
      "Gradient Descent(5/49): loss=0.38164553464024326\n",
      "Gradient Descent(6/49): loss=0.37935074682354314\n",
      "Gradient Descent(7/49): loss=0.3773656500443696\n",
      "Gradient Descent(8/49): loss=0.37564562805059104\n",
      "Gradient Descent(9/49): loss=0.3741532269141083\n",
      "Gradient Descent(10/49): loss=0.37285670628055795\n",
      "Gradient Descent(11/49): loss=0.3717290299915427\n",
      "Gradient Descent(12/49): loss=0.37074709200520684\n",
      "Gradient Descent(13/49): loss=0.3698910967445452\n",
      "Gradient Descent(14/49): loss=0.3691440517978303\n",
      "Gradient Descent(15/49): loss=0.36849134726779714\n",
      "Gradient Descent(16/49): loss=0.3679204041636544\n",
      "Gradient Descent(17/49): loss=0.3674203788494026\n",
      "Gradient Descent(18/49): loss=0.3669819135385054\n",
      "Gradient Descent(19/49): loss=0.36659692491751406\n",
      "Gradient Descent(20/49): loss=0.3662584245355079\n",
      "Gradient Descent(21/49): loss=0.3659603657896257\n",
      "Gradient Descent(22/49): loss=0.3656975132723228\n",
      "Gradient Descent(23/49): loss=0.3654653309892693\n",
      "Gradient Descent(24/49): loss=0.36525988655344066\n",
      "Gradient Descent(25/49): loss=0.36507776894379995\n",
      "Gradient Descent(26/49): loss=0.3649160178104326\n",
      "Gradient Descent(27/49): loss=0.36477206263059825\n",
      "Gradient Descent(28/49): loss=0.36464367028614675\n",
      "Gradient Descent(29/49): loss=0.3645288998531411\n",
      "Gradient Descent(30/49): loss=0.3644260635780146\n",
      "Gradient Descent(31/49): loss=0.36433369316798325\n",
      "Gradient Descent(32/49): loss=0.36425051065217556\n",
      "Gradient Descent(33/49): loss=0.36417540317836744\n",
      "Gradient Descent(34/49): loss=0.36410740120182644\n",
      "Gradient Descent(35/49): loss=0.36404565960040347\n",
      "Gradient Descent(36/49): loss=0.3639894413159695\n",
      "Gradient Descent(37/49): loss=0.3639381031784638\n",
      "Gradient Descent(38/49): loss=0.3638910836167655\n",
      "Gradient Descent(39/49): loss=0.36384789200158646\n",
      "Gradient Descent(40/49): loss=0.3638080994006979\n",
      "Gradient Descent(41/49): loss=0.3637713305569168\n",
      "Gradient Descent(42/49): loss=0.36373725692515196\n",
      "Gradient Descent(43/49): loss=0.3637055906270605\n",
      "Gradient Descent(44/49): loss=0.3636760792010155\n",
      "Gradient Descent(45/49): loss=0.36364850104160046\n",
      "Gradient Descent(46/49): loss=0.3636226614370772\n",
      "Gradient Descent(47/49): loss=0.3635983891255703\n",
      "Gradient Descent(48/49): loss=0.3635755333013171\n",
      "Gradient Descent(49/49): loss=0.3635539610115151\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.40064165470924284\n",
      "Gradient Descent(2/49): loss=0.3916830880381988\n",
      "Gradient Descent(3/49): loss=0.3878318634562367\n",
      "Gradient Descent(4/49): loss=0.3847440584881983\n",
      "Gradient Descent(5/49): loss=0.38209740165838646\n",
      "Gradient Descent(6/49): loss=0.37981122691966873\n",
      "Gradient Descent(7/49): loss=0.3778313851875934\n",
      "Gradient Descent(8/49): loss=0.37611396357407784\n",
      "Gradient Descent(9/49): loss=0.37462215093523726\n",
      "Gradient Descent(10/49): loss=0.373324728331539\n",
      "Gradient Descent(11/49): loss=0.3721950640323135\n",
      "Gradient Descent(12/49): loss=0.3712103602031531\n",
      "Gradient Descent(13/49): loss=0.3703510546474448\n",
      "Gradient Descent(14/49): loss=0.3696003313628055\n",
      "Gradient Descent(15/49): loss=0.3689437137699848\n",
      "Gradient Descent(16/49): loss=0.3683687236006018\n",
      "Gradient Descent(17/49): loss=0.3678645932066755\n",
      "Gradient Descent(18/49): loss=0.36742202193651197\n",
      "Gradient Descent(19/49): loss=0.3670329691717617\n",
      "Gradient Descent(20/49): loss=0.3666904780461938\n",
      "Gradient Descent(21/49): loss=0.3663885249586332\n",
      "Gradient Descent(22/49): loss=0.3661218908518421\n",
      "Gradient Descent(23/49): loss=0.36588605091675425\n",
      "Gradient Descent(24/49): loss=0.365677079937792\n",
      "Gradient Descent(25/49): loss=0.36549157094870177\n",
      "Gradient Descent(26/49): loss=0.3653265652407422\n",
      "Gradient Descent(27/49): loss=0.3651794920723757\n",
      "Gradient Descent(28/49): loss=0.36504811668444975\n",
      "Gradient Descent(29/49): loss=0.364930495437086\n",
      "Gradient Descent(30/49): loss=0.3648249370619482\n",
      "Gradient Descent(31/49): loss=0.3647299691724737\n",
      "Gradient Descent(32/49): loss=0.3646443093000397\n",
      "Gradient Descent(33/49): loss=0.364566839829932\n",
      "Gradient Descent(34/49): loss=0.3644965863006754\n",
      "Gradient Descent(35/49): loss=0.3644326986064449\n",
      "Gradient Descent(36/49): loss=0.3643744347070995\n",
      "Gradient Descent(37/49): loss=0.36432114650566183\n",
      "Gradient Descent(38/49): loss=0.3642722676003217\n",
      "Gradient Descent(39/49): loss=0.3642273026584781\n",
      "Gradient Descent(40/49): loss=0.3641858181950136\n",
      "Gradient Descent(41/49): loss=0.36414743456676235\n",
      "Gradient Descent(42/49): loss=0.3641118190207306\n",
      "Gradient Descent(43/49): loss=0.36407867965564206\n",
      "Gradient Descent(44/49): loss=0.3640477601753633\n",
      "Gradient Descent(45/49): loss=0.36401883532911455\n",
      "Gradient Descent(46/49): loss=0.3639917069474892\n",
      "Gradient Descent(47/49): loss=0.36396620049549455\n",
      "Gradient Descent(48/49): loss=0.36394216207435404\n",
      "Gradient Descent(49/49): loss=0.3639194558129194\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3991674846707233\n",
      "Gradient Descent(2/49): loss=0.3895185078790607\n",
      "Gradient Descent(3/49): loss=0.3857911520633143\n",
      "Gradient Descent(4/49): loss=0.38299803346474437\n",
      "Gradient Descent(5/49): loss=0.38069953139224005\n",
      "Gradient Descent(6/49): loss=0.37876735967770747\n",
      "Gradient Descent(7/49): loss=0.377122001717926\n",
      "Gradient Descent(8/49): loss=0.3757064909043288\n",
      "Gradient Descent(9/49): loss=0.3744785640012828\n",
      "Gradient Descent(10/49): loss=0.37340617135538495\n",
      "Gradient Descent(11/49): loss=0.3724645188401663\n",
      "Gradient Descent(12/49): loss=0.37163405426833196\n",
      "Gradient Descent(13/49): loss=0.3708990760972552\n",
      "Gradient Descent(14/49): loss=0.3702467608545959\n",
      "Gradient Descent(15/49): loss=0.3696664754141643\n",
      "Gradient Descent(16/49): loss=0.36914928463970303\n",
      "Gradient Descent(17/49): loss=0.3686875940375304\n",
      "Gradient Descent(18/49): loss=0.3682748864449646\n",
      "Gradient Descent(19/49): loss=0.36790552479363414\n",
      "Gradient Descent(20/49): loss=0.36757460176904894\n",
      "Gradient Descent(21/49): loss=0.36727782314016494\n",
      "Gradient Descent(22/49): loss=0.3670114155818421\n",
      "Gradient Descent(23/49): loss=0.36677205257753304\n",
      "Gradient Descent(24/49): loss=0.36655679388408674\n",
      "Gradient Descent(25/49): loss=0.3663630353445182\n",
      "Gradient Descent(26/49): loss=0.3661884667364978\n",
      "Gradient Descent(27/49): loss=0.36603103597174036\n",
      "Gradient Descent(28/49): loss=0.3658889184009226\n",
      "Gradient Descent(29/49): loss=0.3657604902890402\n",
      "Gradient Descent(30/49): loss=0.36564430574725004\n",
      "Gradient Descent(31/49): loss=0.3655390765665869\n",
      "Gradient Descent(32/49): loss=0.365443654515202\n",
      "Gradient Descent(33/49): loss=0.36535701574679097\n",
      "Gradient Descent(34/49): loss=0.3652782470325107\n",
      "Gradient Descent(35/49): loss=0.3652065335780524\n",
      "Gradient Descent(36/49): loss=0.3651411482258856\n",
      "Gradient Descent(37/49): loss=0.36508144187299774\n",
      "Gradient Descent(38/49): loss=0.3650268349587973\n",
      "Gradient Descent(39/49): loss=0.3649768098977241\n",
      "Gradient Descent(40/49): loss=0.3649309043475595\n",
      "Gradient Descent(41/49): loss=0.36488870521822236\n",
      "Gradient Descent(42/49): loss=0.3648498433375341\n",
      "Gradient Descent(43/49): loss=0.36481398870044796\n",
      "Gradient Descent(44/49): loss=0.3647808462368712\n",
      "Gradient Descent(45/49): loss=0.36475015204071537\n",
      "Gradient Descent(46/49): loss=0.36472167000935096\n",
      "Gradient Descent(47/49): loss=0.3646951888483905\n",
      "Gradient Descent(48/49): loss=0.36467051940177164\n",
      "Gradient Descent(49/49): loss=0.3646474922715704\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3992321678573436\n",
      "Gradient Descent(2/49): loss=0.3910346222844262\n",
      "Gradient Descent(3/49): loss=0.3872198200371235\n",
      "Gradient Descent(4/49): loss=0.38411391444423393\n",
      "Gradient Descent(5/49): loss=0.38146276377976035\n",
      "Gradient Descent(6/49): loss=0.3791871649899688\n",
      "Gradient Descent(7/49): loss=0.3772295152537355\n",
      "Gradient Descent(8/49): loss=0.37554266352835575\n",
      "Gradient Descent(9/49): loss=0.37408715426554995\n",
      "Gradient Descent(10/49): loss=0.37282968099590535\n",
      "Gradient Descent(11/49): loss=0.37174199458872187\n",
      "Gradient Descent(12/49): loss=0.3708000626008247\n",
      "Gradient Descent(13/49): loss=0.3699833945807801\n",
      "Gradient Descent(14/49): loss=0.3692744891639674\n",
      "Gradient Descent(15/49): loss=0.368658375814679\n",
      "Gradient Descent(16/49): loss=0.3681222323681663\n",
      "Gradient Descent(17/49): loss=0.36765506428582706\n",
      "Gradient Descent(18/49): loss=0.36724743465756354\n",
      "Gradient Descent(19/49): loss=0.3668912362197867\n",
      "Gradient Descent(20/49): loss=0.3665794983417618\n",
      "Gradient Descent(21/49): loss=0.36630622324006396\n",
      "Gradient Descent(22/49): loss=0.3660662467131983\n",
      "Gradient Descent(23/49): loss=0.3658551195132006\n",
      "Gradient Descent(24/49): loss=0.3656690061356838\n",
      "Gradient Descent(25/49): loss=0.3655045983491873\n",
      "Gradient Descent(26/49): loss=0.36535904122506124\n",
      "Gradient Descent(27/49): loss=0.3652298697906137\n",
      "Gradient Descent(28/49): loss=0.36511495472643674\n",
      "Gradient Descent(29/49): loss=0.36501245577589736\n",
      "Gradient Descent(30/49): loss=0.36492078174034487\n",
      "Gradient Descent(31/49): loss=0.36483855610525007\n",
      "Gradient Descent(32/49): loss=0.36476458748633755\n",
      "Gradient Descent(33/49): loss=0.3646978442056954\n",
      "Gradient Descent(34/49): loss=0.3646374324097698\n",
      "Gradient Descent(35/49): loss=0.3645825772272979\n",
      "Gradient Descent(36/49): loss=0.36453260653819824\n",
      "Gradient Descent(37/49): loss=0.36448693698637696\n",
      "Gradient Descent(38/49): loss=0.3644450619220829\n",
      "Gradient Descent(39/49): loss=0.3644065410043125\n",
      "Gradient Descent(40/49): loss=0.3643709912320431\n",
      "Gradient Descent(41/49): loss=0.3643380792057753\n",
      "Gradient Descent(42/49): loss=0.36430751444882753\n",
      "Gradient Descent(43/49): loss=0.3642790436417722\n",
      "Gradient Descent(44/49): loss=0.36425244564392056\n",
      "Gradient Descent(45/49): loss=0.36422752719335905\n",
      "Gradient Descent(46/49): loss=0.36420411919215084\n",
      "Gradient Descent(47/49): loss=0.3641820734962798\n",
      "Gradient Descent(48/49): loss=0.36416126014107153\n",
      "Gradient Descent(49/49): loss=0.3641415649424055\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3991345954455003\n",
      "Gradient Descent(2/49): loss=0.3909111185372143\n",
      "Gradient Descent(3/49): loss=0.3870568683271249\n",
      "Gradient Descent(4/49): loss=0.3839115680718254\n",
      "Gradient Descent(5/49): loss=0.3812237617203345\n",
      "Gradient Descent(6/49): loss=0.37891460241999086\n",
      "Gradient Descent(7/49): loss=0.3769262847675106\n",
      "Gradient Descent(8/49): loss=0.3752113719863741\n",
      "Gradient Descent(9/49): loss=0.3737301506663714\n",
      "Gradient Descent(10/49): loss=0.3724490987362007\n",
      "Gradient Descent(11/49): loss=0.3713397862425524\n",
      "Gradient Descent(12/49): loss=0.3703780250859506\n",
      "Gradient Descent(13/49): loss=0.369543186899316\n",
      "Gradient Descent(14/49): loss=0.3688176451708655\n",
      "Gradient Descent(15/49): loss=0.3681863138757637\n",
      "Gradient Descent(16/49): loss=0.3676362631556791\n",
      "Gradient Descent(17/49): loss=0.36715639749498513\n",
      "Gradient Descent(18/49): loss=0.3667371851108535\n",
      "Gradient Descent(19/49): loss=0.3663704296206658\n",
      "Gradient Descent(20/49): loss=0.3660490768116613\n",
      "Gradient Descent(21/49): loss=0.36576705069605964\n",
      "Gradient Descent(22/49): loss=0.3655191141002646\n",
      "Gradient Descent(23/49): loss=0.3653007498823548\n",
      "Gradient Descent(24/49): loss=0.3651080595494911\n",
      "Gradient Descent(25/49): loss=0.36493767659377585\n",
      "Gradient Descent(26/49): loss=0.36478669230964045\n",
      "Gradient Descent(27/49): loss=0.3646525922193872\n",
      "Gradient Descent(28/49): loss=0.364533201532469\n",
      "Gradient Descent(29/49): loss=0.3644266383111831\n",
      "Gradient Descent(30/49): loss=0.3643312732206251\n",
      "Gradient Descent(31/49): loss=0.36424569491181935\n",
      "Gradient Descent(32/49): loss=0.3641686802301382\n",
      "Gradient Descent(33/49): loss=0.3640991685613879\n",
      "Gradient Descent(34/49): loss=0.3640362397292735\n",
      "Gradient Descent(35/49): loss=0.36397909494356756\n",
      "Gradient Descent(36/49): loss=0.36392704037083495\n",
      "Gradient Descent(37/49): loss=0.3638794729611259\n",
      "Gradient Descent(38/49): loss=0.3638358682164293\n",
      "Gradient Descent(39/49): loss=0.36379576963130844\n",
      "Gradient Descent(40/49): loss=0.36375877957423525\n",
      "Gradient Descent(41/49): loss=0.3637245514107088\n",
      "Gradient Descent(42/49): loss=0.3636927826971051\n",
      "Gradient Descent(43/49): loss=0.36366320929809004\n",
      "Gradient Descent(44/49): loss=0.36363560030090025\n",
      "Gradient Descent(45/49): loss=0.3636097536173777\n",
      "Gradient Descent(46/49): loss=0.3635854921797424\n",
      "Gradient Descent(47/49): loss=0.3635626606490682\n",
      "Gradient Descent(48/49): loss=0.3635411225665921\n",
      "Gradient Descent(49/49): loss=0.36352075788759364\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39944708338428137\n",
      "Gradient Descent(2/49): loss=0.39131440605857043\n",
      "Gradient Descent(3/49): loss=0.38748189928826654\n",
      "Gradient Descent(4/49): loss=0.3843529688298381\n",
      "Gradient Descent(5/49): loss=0.38167749487759906\n",
      "Gradient Descent(6/49): loss=0.3793765533924944\n",
      "Gradient Descent(7/49): loss=0.37739299585178737\n",
      "Gradient Descent(8/49): loss=0.375680171470774\n",
      "Gradient Descent(9/49): loss=0.37419905029327033\n",
      "Gradient Descent(10/49): loss=0.3729166497550714\n",
      "Gradient Descent(11/49): loss=0.37180495088708615\n",
      "Gradient Descent(12/49): loss=0.3708400745677969\n",
      "Gradient Descent(13/49): loss=0.37000162375620377\n",
      "Gradient Descent(14/49): loss=0.36927214500523936\n",
      "Gradient Descent(15/49): loss=0.3686366817728358\n",
      "Gradient Descent(16/49): loss=0.36808240101191736\n",
      "Gradient Descent(17/49): loss=0.36759827941746637\n",
      "Gradient Descent(18/49): loss=0.36717483879720153\n",
      "Gradient Descent(19/49): loss=0.36680392219250707\n",
      "Gradient Descent(20/49): loss=0.36647850398608744\n",
      "Gradient Descent(21/49): loss=0.3661925284770889\n",
      "Gradient Descent(22/49): loss=0.36594077238683714\n",
      "Gradient Descent(23/49): loss=0.3657187275444384\n",
      "Gradient Descent(24/49): loss=0.36552250063644065\n",
      "Gradient Descent(25/49): loss=0.3653487274213274\n",
      "Gradient Descent(26/49): loss=0.3651944992324448\n",
      "Gradient Descent(27/49): loss=0.3650572999408749\n",
      "Gradient Descent(28/49): loss=0.3649349518374182\n",
      "Gradient Descent(29/49): loss=0.3648255691316813\n",
      "Gradient Descent(30/49): loss=0.36472751796538055\n",
      "Gradient Descent(31/49): loss=0.36463938200355245\n",
      "Gradient Descent(32/49): loss=0.3645599328072061\n",
      "Gradient Descent(33/49): loss=0.36448810430869677\n",
      "Gradient Descent(34/49): loss=0.3644229708105212\n",
      "Gradient Descent(35/49): loss=0.3643637280123786\n",
      "Gradient Descent(36/49): loss=0.3643096766427402\n",
      "Gradient Descent(37/49): loss=0.3642602083318513\n",
      "Gradient Descent(38/49): loss=0.3642147934147772\n",
      "Gradient Descent(39/49): loss=0.3641729703971944\n",
      "Gradient Descent(40/49): loss=0.3641343368542906\n",
      "Gradient Descent(41/49): loss=0.36409854156535537\n",
      "Gradient Descent(42/49): loss=0.36406527771423747\n",
      "Gradient Descent(43/49): loss=0.36403427700949487\n",
      "Gradient Descent(44/49): loss=0.36400530459836183\n",
      "Gradient Descent(45/49): loss=0.36397815466608513\n",
      "Gradient Descent(46/49): loss=0.3639526466271629\n",
      "Gradient Descent(47/49): loss=0.36392862182789665\n",
      "Gradient Descent(48/49): loss=0.36390594069075005\n",
      "Gradient Descent(49/49): loss=0.36388448024055303\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39785903688803087\n",
      "Gradient Descent(2/49): loss=0.38913389139309573\n",
      "Gradient Descent(3/49): loss=0.38546184559954777\n",
      "Gradient Descent(4/49): loss=0.3826498614665504\n",
      "Gradient Descent(5/49): loss=0.3803389177803099\n",
      "Gradient Descent(6/49): loss=0.37840279344091443\n",
      "Gradient Descent(7/49): loss=0.3767593076863121\n",
      "Gradient Descent(8/49): loss=0.3753495522894082\n",
      "Gradient Descent(9/49): loss=0.3741300309335449\n",
      "Gradient Descent(10/49): loss=0.37306789863010514\n",
      "Gradient Descent(11/49): loss=0.3721378228821786\n",
      "Gradient Descent(12/49): loss=0.3713198643719462\n",
      "Gradient Descent(13/49): loss=0.3705980261134388\n",
      "Gradient Descent(14/49): loss=0.36995924808402253\n",
      "Gradient Descent(15/49): loss=0.3693927017124029\n",
      "Gradient Descent(16/49): loss=0.368889287873471\n",
      "Gradient Descent(17/49): loss=0.36844127412731364\n",
      "Gradient Descent(18/49): loss=0.36804202808009234\n",
      "Gradient Descent(19/49): loss=0.36768581777200493\n",
      "Gradient Descent(20/49): loss=0.3673676593519968\n",
      "Gradient Descent(21/49): loss=0.36708319856260047\n",
      "Gradient Descent(22/49): loss=0.3668286167685464\n",
      "Gradient Descent(23/49): loss=0.3666005551040699\n",
      "Gradient Descent(24/49): loss=0.36639605223970473\n",
      "Gradient Descent(25/49): loss=0.3662124925813662\n",
      "Gradient Descent(26/49): loss=0.3660475626136474\n",
      "Gradient Descent(27/49): loss=0.36589921371977446\n",
      "Gradient Descent(28/49): loss=0.3657656302425022\n",
      "Gradient Descent(29/49): loss=0.3656452018537293\n",
      "Gradient Descent(30/49): loss=0.36553649951639805\n",
      "Gradient Descent(31/49): loss=0.36543825447770867\n",
      "Gradient Descent(32/49): loss=0.3653493398463863\n",
      "Gradient Descent(33/49): loss=0.3652687543912875\n",
      "Gradient Descent(34/49): loss=0.36519560826261555\n",
      "Gradient Descent(35/49): loss=0.3651291103863357\n",
      "Gradient Descent(36/49): loss=0.3650685573211\n",
      "Gradient Descent(37/49): loss=0.3650133233979248\n",
      "Gradient Descent(38/49): loss=0.36496285198801065\n",
      "Gradient Descent(39/49): loss=0.3649166477648221\n",
      "Gradient Descent(40/49): loss=0.36487426984388194\n",
      "Gradient Descent(41/49): loss=0.3648353256983901\n",
      "Gradient Descent(42/49): loss=0.36479946576128836\n",
      "Gradient Descent(43/49): loss=0.3647663786351642\n",
      "Gradient Descent(44/49): loss=0.36473578684070873\n",
      "Gradient Descent(45/49): loss=0.36470744304257374\n",
      "Gradient Descent(46/49): loss=0.3646811266985658\n",
      "Gradient Descent(47/49): loss=0.3646566410843549\n",
      "Gradient Descent(48/49): loss=0.36463381065134737\n",
      "Gradient Descent(49/49): loss=0.3646124786802065\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3981652453541604\n",
      "Gradient Descent(2/49): loss=0.39069993120216906\n",
      "Gradient Descent(3/49): loss=0.386880211670858\n",
      "Gradient Descent(4/49): loss=0.3837332834800924\n",
      "Gradient Descent(5/49): loss=0.3810563873647419\n",
      "Gradient Descent(6/49): loss=0.37876924936564993\n",
      "Gradient Descent(7/49): loss=0.37681084728248726\n",
      "Gradient Descent(8/49): loss=0.37513116531979346\n",
      "Gradient Descent(9/49): loss=0.3736884804813194\n",
      "Gradient Descent(10/49): loss=0.3724477155101129\n",
      "Gradient Descent(11/49): loss=0.3713792515390112\n",
      "Gradient Descent(12/49): loss=0.37045800653555505\n",
      "Gradient Descent(13/49): loss=0.36966269438945093\n",
      "Gradient Descent(14/49): loss=0.36897521865130245\n",
      "Gradient Descent(15/49): loss=0.36838017158225944\n",
      "Gradient Descent(16/49): loss=0.3678644176306435\n",
      "Gradient Descent(17/49): loss=0.3674167455305808\n",
      "Gradient Descent(18/49): loss=0.3670275766637682\n",
      "Gradient Descent(19/49): loss=0.36668871984146006\n",
      "Gradient Descent(20/49): loss=0.36639316457688337\n",
      "Gradient Descent(21/49): loss=0.36613490640690916\n",
      "Gradient Descent(22/49): loss=0.36590879899690093\n",
      "Gradient Descent(23/49): loss=0.3657104286996836\n",
      "Gradient Descent(24/49): loss=0.36553600799273706\n",
      "Gradient Descent(25/49): loss=0.3653822848271836\n",
      "Gradient Descent(26/49): loss=0.36524646541828076\n",
      "Gradient Descent(27/49): loss=0.3651261484131825\n",
      "Gradient Descent(28/49): loss=0.3650192687057036\n",
      "Gradient Descent(29/49): loss=0.3649240494437139\n",
      "Gradient Descent(30/49): loss=0.3648389610036669\n",
      "Gradient Descent(31/49): loss=0.3647626858973142\n",
      "Gradient Descent(32/49): loss=0.3646940887348602\n",
      "Gradient Descent(33/49): loss=0.36463219050221124\n",
      "Gradient Descent(34/49): loss=0.36457614652207215\n",
      "Gradient Descent(35/49): loss=0.36452522756307487\n",
      "Gradient Descent(36/49): loss=0.3644788036408372\n",
      "Gradient Descent(37/49): loss=0.36443633012229854\n",
      "Gradient Descent(38/49): loss=0.36439733580181993\n",
      "Gradient Descent(39/49): loss=0.36436141266604144\n",
      "Gradient Descent(40/49): loss=0.3643282071057224\n",
      "Gradient Descent(41/49): loss=0.3642974123678668\n",
      "Gradient Descent(42/49): loss=0.3642687620713276\n",
      "Gradient Descent(43/49): loss=0.3642420246345739\n",
      "Gradient Descent(44/49): loss=0.36421699848605366\n",
      "Gradient Descent(45/49): loss=0.3641935079461694\n",
      "Gradient Descent(46/49): loss=0.364171399685763\n",
      "Gradient Descent(47/49): loss=0.3641505396795868\n",
      "Gradient Descent(48/49): loss=0.36413081058486185\n",
      "Gradient Descent(49/49): loss=0.3641121094849698\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39806675082259496\n",
      "Gradient Descent(2/49): loss=0.3905738092830598\n",
      "Gradient Descent(3/49): loss=0.38671331770060563\n",
      "Gradient Descent(4/49): loss=0.3835259349263283\n",
      "Gradient Descent(5/49): loss=0.3808116345477529\n",
      "Gradient Descent(6/49): loss=0.3784903896804368\n",
      "Gradient Descent(7/49): loss=0.37650091155631094\n",
      "Gradient Descent(8/49): loss=0.3747928680290517\n",
      "Gradient Descent(9/49): loss=0.37332426236206706\n",
      "Gradient Descent(10/49): loss=0.3720597913584284\n",
      "Gradient Descent(11/49): loss=0.37096964678127176\n",
      "Gradient Descent(12/49): loss=0.370028582847935\n",
      "Gradient Descent(13/49): loss=0.3692151675347568\n",
      "Gradient Descent(14/49): loss=0.3685111713564964\n",
      "Gradient Descent(15/49): loss=0.3679010634347432\n",
      "Gradient Descent(16/49): loss=0.36737159325487445\n",
      "Gradient Descent(17/49): loss=0.36691144180342183\n",
      "Gradient Descent(18/49): loss=0.366510929399016\n",
      "Gradient Descent(19/49): loss=0.36616177016951634\n",
      "Gradient Descent(20/49): loss=0.3658568651229454\n",
      "Gradient Descent(21/49): loss=0.36559012730106794\n",
      "Gradient Descent(22/49): loss=0.36535633371246073\n",
      "Gradient Descent(23/49): loss=0.36515099969899717\n",
      "Gradient Descent(24/49): loss=0.3649702721545976\n",
      "Gradient Descent(25/49): loss=0.3648108386310973\n",
      "Gradient Descent(26/49): loss=0.3646698498654969\n",
      "Gradient Descent(27/49): loss=0.3645448536702202\n",
      "Gradient Descent(28/49): loss=0.3644337384621404\n",
      "Gradient Descent(29/49): loss=0.3643346849815685\n",
      "Gradient Descent(30/49): loss=0.3642461249805168\n",
      "Gradient Descent(31/49): loss=0.3641667058492348\n",
      "Gradient Descent(32/49): loss=0.36409526030836226\n",
      "Gradient Descent(33/49): loss=0.3640307804266595\n",
      "Gradient Descent(34/49): loss=0.36397239533568176\n",
      "Gradient Descent(35/49): loss=0.36391935210661436\n",
      "Gradient Descent(36/49): loss=0.36387099933372263\n",
      "Gradient Descent(37/49): loss=0.3638267730359273\n",
      "Gradient Descent(38/49): loss=0.36378618454486444\n",
      "Gradient Descent(39/49): loss=0.36374881009606413\n",
      "Gradient Descent(40/49): loss=0.36371428188094573\n",
      "Gradient Descent(41/49): loss=0.3636822803522894\n",
      "Gradient Descent(42/49): loss=0.36365252760566036\n",
      "Gradient Descent(43/49): loss=0.3636247816847019\n",
      "Gradient Descent(44/49): loss=0.36359883167995005\n",
      "Gradient Descent(45/49): loss=0.36357449350940046\n",
      "Gradient Descent(46/49): loss=0.3635516062849585\n",
      "Gradient Descent(47/49): loss=0.3635300291825024\n",
      "Gradient Descent(48/49): loss=0.36350963874495135\n",
      "Gradient Descent(49/49): loss=0.3634903265577096\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39838911680242073\n",
      "Gradient Descent(2/49): loss=0.39097952900851624\n",
      "Gradient Descent(3/49): loss=0.3871401513790545\n",
      "Gradient Descent(4/49): loss=0.3839693024023305\n",
      "Gradient Descent(5/49): loss=0.38126713343810037\n",
      "Gradient Descent(6/49): loss=0.37895367934281565\n",
      "Gradient Descent(7/49): loss=0.3769684539176481\n",
      "Gradient Descent(8/49): loss=0.37526198764757995\n",
      "Gradient Descent(9/49): loss=0.37379300283698413\n",
      "Gradient Descent(10/49): loss=0.372526750497388\n",
      "Gradient Descent(11/49): loss=0.3714338377727074\n",
      "Gradient Descent(12/49): loss=0.37048932725889716\n",
      "Gradient Descent(13/49): loss=0.36967201532910055\n",
      "Gradient Descent(14/49): loss=0.3689638416549425\n",
      "Gradient Descent(15/49): loss=0.3683494006416054\n",
      "Gradient Descent(16/49): loss=0.36781553444010845\n",
      "Gradient Descent(17/49): loss=0.36735099232114843\n",
      "Gradient Descent(18/49): loss=0.36694614455784336\n",
      "Gradient Descent(19/49): loss=0.3665927413782976\n",
      "Gradient Descent(20/49): loss=0.36628370937139826\n",
      "Gradient Descent(21/49): loss=0.36601297914521413\n",
      "Gradient Descent(22/49): loss=0.36577533915632504\n",
      "Gradient Descent(23/49): loss=0.3655663115226432\n",
      "Gradient Descent(24/49): loss=0.36538204635293564\n",
      "Gradient Descent(25/49): loss=0.36521923171095644\n",
      "Gradient Descent(26/49): loss=0.36507501680928506\n",
      "Gradient Descent(27/49): loss=0.364946946419454\n",
      "Gradient Descent(28/49): loss=0.36483290480763825\n",
      "Gradient Descent(29/49): loss=0.3647310677723324\n",
      "Gradient Descent(30/49): loss=0.36463986158247197\n",
      "Gradient Descent(31/49): loss=0.364557927799668\n",
      "Gradient Descent(32/49): loss=0.364484093123228\n",
      "Gradient Descent(33/49): loss=0.36441734352673744\n",
      "Gradient Descent(34/49): loss=0.3643568020644848\n",
      "Gradient Descent(35/49): loss=0.3643017098184022\n",
      "Gradient Descent(36/49): loss=0.3642514095343075\n",
      "Gradient Descent(37/49): loss=0.3642053315624189\n",
      "Gradient Descent(38/49): loss=0.36416298177327405\n",
      "Gradient Descent(39/49): loss=0.3641239311679238\n",
      "Gradient Descent(40/49): loss=0.36408780694190146\n",
      "Gradient Descent(41/49): loss=0.36405428479709057\n",
      "Gradient Descent(42/49): loss=0.364023082325154\n",
      "Gradient Descent(43/49): loss=0.3639939533114039\n",
      "Gradient Descent(44/49): loss=0.3639666828295493\n",
      "Gradient Descent(45/49): loss=0.3639410830161908\n",
      "Gradient Descent(46/49): loss=0.3639169894297057\n",
      "Gradient Descent(47/49): loss=0.36389425791167784\n",
      "Gradient Descent(48/49): loss=0.3638727618805974\n",
      "Gradient Descent(49/49): loss=0.363852389997477\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39668301723834587\n",
      "Gradient Descent(2/49): loss=0.3887878725338197\n",
      "Gradient Descent(3/49): loss=0.38514275657115665\n",
      "Gradient Descent(4/49): loss=0.3823103112840302\n",
      "Gradient Descent(5/49): loss=0.3799882805535568\n",
      "Gradient Descent(6/49): loss=0.37804950501451984\n",
      "Gradient Descent(7/49): loss=0.3764089138981682\n",
      "Gradient Descent(8/49): loss=0.3750057048933411\n",
      "Gradient Descent(9/49): loss=0.37379520670707017\n",
      "Gradient Descent(10/49): loss=0.3727438208926233\n",
      "Gradient Descent(11/49): loss=0.37182570280901667\n",
      "Gradient Descent(12/49): loss=0.3710205409690514\n",
      "Gradient Descent(13/49): loss=0.37031205090892655\n",
      "Gradient Descent(14/49): loss=0.3696869403133908\n",
      "Gradient Descent(15/49): loss=0.3691341879477952\n",
      "Gradient Descent(16/49): loss=0.3686445333564608\n",
      "Gradient Descent(17/49): loss=0.36821010939346444\n",
      "Gradient Descent(18/49): loss=0.3678241725264767\n",
      "Gradient Descent(19/49): loss=0.3674809008522998\n",
      "Gradient Descent(20/49): loss=0.367175239642822\n",
      "Gradient Descent(21/49): loss=0.36690278077570093\n",
      "Gradient Descent(22/49): loss=0.3666596667452721\n",
      "Gradient Descent(23/49): loss=0.36644251284574536\n",
      "Gradient Descent(24/49): loss=0.36624834306137516\n",
      "Gradient Descent(25/49): loss=0.3660745365090067\n",
      "Gradient Descent(26/49): loss=0.3659187821691373\n",
      "Gradient Descent(27/49): loss=0.3657790402521485\n",
      "Gradient Descent(28/49): loss=0.3656535089691252\n",
      "Gradient Descent(29/49): loss=0.3655405957729604\n",
      "Gradient Descent(30/49): loss=0.3654388923460146\n",
      "Gradient Descent(31/49): loss=0.3653471527626506\n",
      "Gradient Descent(32/49): loss=0.36526427436671044\n",
      "Gradient Descent(33/49): loss=0.36518928098770287\n",
      "Gradient Descent(34/49): loss=0.36512130818339084\n",
      "Gradient Descent(35/49): loss=0.3650595902462776\n",
      "Gradient Descent(36/49): loss=0.3650034487510304\n",
      "Gradient Descent(37/49): loss=0.36495228245183625\n",
      "Gradient Descent(38/49): loss=0.36490555836492694\n",
      "Gradient Descent(39/49): loss=0.36486280389336656\n",
      "Gradient Descent(40/49): loss=0.3648235998696071\n",
      "Gradient Descent(41/49): loss=0.3647875744069957\n",
      "Gradient Descent(42/49): loss=0.36475439746486604\n",
      "Gradient Descent(43/49): loss=0.3647237760434603\n",
      "Gradient Descent(44/49): loss=0.3646954499350175\n",
      "Gradient Descent(45/49): loss=0.3646691879661521\n",
      "Gradient Descent(46/49): loss=0.36464478467433503\n",
      "Gradient Descent(47/49): loss=0.3646220573680352\n",
      "Gradient Descent(48/49): loss=0.36460084352599265\n",
      "Gradient Descent(49/49): loss=0.3645809984963127\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3972348011464289\n",
      "Gradient Descent(2/49): loss=0.3903928536587442\n",
      "Gradient Descent(3/49): loss=0.3865475701875307\n",
      "Gradient Descent(4/49): loss=0.3833598159181763\n",
      "Gradient Descent(5/49): loss=0.3806593468769288\n",
      "Gradient Descent(6/49): loss=0.3783628341927907\n",
      "Gradient Descent(7/49): loss=0.3764055987373626\n",
      "Gradient Descent(8/49): loss=0.3747346971306366\n",
      "Gradient Descent(9/49): loss=0.37330612037314453\n",
      "Gradient Descent(10/49): loss=0.37208302586731734\n",
      "Gradient Descent(11/49): loss=0.37103444478874215\n",
      "Gradient Descent(12/49): loss=0.3701342733577379\n",
      "Gradient Descent(13/49): loss=0.3693604615722627\n",
      "Gradient Descent(14/49): loss=0.36869435087531294\n",
      "Gradient Descent(15/49): loss=0.3681201287510367\n",
      "Gradient Descent(16/49): loss=0.3676243770222023\n",
      "Gradient Descent(17/49): loss=0.3671956961237314\n",
      "Gradient Descent(18/49): loss=0.36682439146391294\n",
      "Gradient Descent(19/49): loss=0.3665022108242188\n",
      "Gradient Descent(20/49): loss=0.3662221239182526\n",
      "Gradient Descent(21/49): loss=0.3659781369194207\n",
      "Gradient Descent(22/49): loss=0.3657651360981366\n",
      "Gradient Descent(23/49): loss=0.36557875576826876\n",
      "Gradient Descent(24/49): loss=0.3654152665912698\n",
      "Gradient Descent(25/49): loss=0.3652714809712\n",
      "Gradient Descent(26/49): loss=0.36514467282965646\n",
      "Gradient Descent(27/49): loss=0.3650325095031605\n",
      "Gradient Descent(28/49): loss=0.36493299387746947\n",
      "Gradient Descent(29/49): loss=0.36484441517963095\n",
      "Gradient Descent(30/49): loss=0.36476530710196486\n",
      "Gradient Descent(31/49): loss=0.36469441214247095\n",
      "Gradient Descent(32/49): loss=0.3646306512213248\n",
      "Gradient Descent(33/49): loss=0.3645730977794443\n",
      "Gradient Descent(34/49): loss=0.36452095568765536\n",
      "Gradient Descent(35/49): loss=0.3644735403978813\n",
      "Gradient Descent(36/49): loss=0.364430262854342\n",
      "Gradient Descent(37/49): loss=0.3643906157557182\n",
      "Gradient Descent(38/49): loss=0.36435416182085123\n",
      "Gradient Descent(39/49): loss=0.36432052376264284\n",
      "Gradient Descent(40/49): loss=0.3642893757189288\n",
      "Gradient Descent(41/49): loss=0.36426043592650037\n",
      "Gradient Descent(42/49): loss=0.36423346045616\n",
      "Gradient Descent(43/49): loss=0.36420823785365997\n",
      "Gradient Descent(44/49): loss=0.3641845845542584\n",
      "Gradient Descent(45/49): loss=0.3641623409581193\n",
      "Gradient Descent(46/49): loss=0.3641413680703521\n",
      "Gradient Descent(47/49): loss=0.36412154462360374\n",
      "Gradient Descent(48/49): loss=0.3641027646131448\n",
      "Gradient Descent(49/49): loss=0.36408493518463314\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3971355235812308\n",
      "Gradient Descent(2/49): loss=0.3902639298022308\n",
      "Gradient Descent(3/49): loss=0.386376689634875\n",
      "Gradient Descent(4/49): loss=0.383147495477257\n",
      "Gradient Descent(5/49): loss=0.3804089164415096\n",
      "Gradient Descent(6/49): loss=0.3780777801771747\n",
      "Gradient Descent(7/49): loss=0.37608908516959355\n",
      "Gradient Descent(8/49): loss=0.37438954456215\n",
      "Gradient Descent(9/49): loss=0.3729348610523248\n",
      "Gradient Descent(10/49): loss=0.37168795600446985\n",
      "Gradient Descent(11/49): loss=0.37061766241504784\n",
      "Gradient Descent(12/49): loss=0.36969770420199005\n",
      "Gradient Descent(13/49): loss=0.3689058769587902\n",
      "Gradient Descent(14/49): loss=0.3682233807572959\n",
      "Gradient Descent(15/49): loss=0.367634271930103\n",
      "Gradient Descent(16/49): loss=0.36712500980485224\n",
      "Gradient Descent(17/49): loss=0.3666840801337605\n",
      "Gradient Descent(18/49): loss=0.3663016809971236\n",
      "Gradient Descent(19/49): loss=0.3659694599316733\n",
      "Gradient Descent(20/49): loss=0.3656802932893994\n",
      "Gradient Descent(21/49): loss=0.365428100574516\n",
      "Gradient Descent(22/49): loss=0.3652076878696085\n",
      "Gradient Descent(23/49): loss=0.36501461553973086\n",
      "Gradient Descent(24/49): loss=0.3648450862624452\n",
      "Gradient Descent(25/49): loss=0.36469585012192374\n",
      "Gradient Descent(26/49): loss=0.36456412406331185\n",
      "Gradient Descent(27/49): loss=0.36444752345758125\n",
      "Gradient Descent(28/49): loss=0.36434400389854954\n",
      "Gradient Descent(29/49): loss=0.36425181165914633\n",
      "Gradient Descent(30/49): loss=0.3641694414862509\n",
      "Gradient Descent(31/49): loss=0.36409560062262575\n",
      "Gradient Descent(32/49): loss=0.3640291781185949\n",
      "Gradient Descent(33/49): loss=0.3639692186415281\n",
      "Gradient Descent(34/49): loss=0.36391490011298355\n",
      "Gradient Descent(35/49): loss=0.3638655146056251\n",
      "Gradient Descent(36/49): loss=0.3638204520181025\n",
      "Gradient Descent(37/49): loss=0.3637791861186676\n",
      "Gradient Descent(38/49): loss=0.3637412626096256\n",
      "Gradient Descent(39/49): loss=0.3637062889166082\n",
      "Gradient Descent(40/49): loss=0.3636739254506231\n",
      "Gradient Descent(41/49): loss=0.3636438781281378\n",
      "Gradient Descent(42/49): loss=0.36361589196612776\n",
      "Gradient Descent(43/49): loss=0.3635897455959518\n",
      "Gradient Descent(44/49): loss=0.36356524656281997\n",
      "Gradient Descent(45/49): loss=0.36354222729712454\n",
      "Gradient Descent(46/49): loss=0.3635205416605185\n",
      "Gradient Descent(47/49): loss=0.363500061983782\n",
      "Gradient Descent(48/49): loss=0.3634806765255979\n",
      "Gradient Descent(49/49): loss=0.3634622872916561\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39746775496366105\n",
      "Gradient Descent(2/49): loss=0.39067150628981717\n",
      "Gradient Descent(3/49): loss=0.38680525849450237\n",
      "Gradient Descent(4/49): loss=0.3835927736805778\n",
      "Gradient Descent(5/49): loss=0.38086607962722496\n",
      "Gradient Descent(6/49): loss=0.37854228000752105\n",
      "Gradient Descent(7/49): loss=0.376557321622083\n",
      "Gradient Descent(8/49): loss=0.374858850466396\n",
      "Gradient Descent(9/49): loss=0.37340331903147334\n",
      "Gradient Descent(10/49): loss=0.3721542144933654\n",
      "Gradient Descent(11/49): loss=0.37108078769564534\n",
      "Gradient Descent(12/49): loss=0.37015706912966206\n",
      "Gradient Descent(13/49): loss=0.3693610790959116\n",
      "Gradient Descent(14/49): loss=0.36867418243202915\n",
      "Gradient Descent(15/49): loss=0.3680805562475457\n",
      "Gradient Descent(16/49): loss=0.3675667481959463\n",
      "Gradient Descent(17/49): loss=0.3671213082672475\n",
      "Gradient Descent(18/49): loss=0.3667344807898106\n",
      "Gradient Descent(19/49): loss=0.3663979460402861\n",
      "Gradient Descent(20/49): loss=0.36610460292399605\n",
      "Gradient Descent(21/49): loss=0.3658483857949134\n",
      "Gradient Descent(22/49): loss=0.3656241097533657\n",
      "Gradient Descent(23/49): loss=0.36542733977164443\n",
      "Gradient Descent(24/49): loss=0.3652542798111342\n",
      "Gradient Descent(25/49): loss=0.36510167875263805\n",
      "Gradient Descent(26/49): loss=0.3649667504970536\n",
      "Gradient Descent(27/49): loss=0.36484710603152753\n",
      "Gradient Descent(28/49): loss=0.3647406956161636\n",
      "Gradient Descent(29/49): loss=0.36464575954344797\n",
      "Gradient Descent(30/49): loss=0.36456078616873866\n",
      "Gradient Descent(31/49): loss=0.36448447611489065\n",
      "Gradient Descent(32/49): loss=0.36441571172488846\n",
      "Gradient Descent(33/49): loss=0.36435353097927875\n",
      "Gradient Descent(34/49): loss=0.3642971052150865\n",
      "Gradient Descent(35/49): loss=0.36424572008372136\n",
      "Gradient Descent(36/49): loss=0.36419875927033024\n",
      "Gradient Descent(37/49): loss=0.364155690568775\n",
      "Gradient Descent(38/49): loss=0.36411605396705216\n",
      "Gradient Descent(39/49): loss=0.3640794514493298\n",
      "Gradient Descent(40/49): loss=0.3640455382643138\n",
      "Gradient Descent(41/49): loss=0.36401401544661816\n",
      "Gradient Descent(42/49): loss=0.3639846234092176\n",
      "Gradient Descent(43/49): loss=0.36395713645176664\n",
      "Gradient Descent(44/49): loss=0.3639313580522985\n",
      "Gradient Descent(45/49): loss=0.36390711682917637\n",
      "Gradient Descent(46/49): loss=0.36388426307666255\n",
      "Gradient Descent(47/49): loss=0.36386266579153637\n",
      "Gradient Descent(48/49): loss=0.36384221012018736\n",
      "Gradient Descent(49/49): loss=0.36382279516585003\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39563942572166844\n",
      "Gradient Descent(2/49): loss=0.3884727009036838\n",
      "Gradient Descent(3/49): loss=0.3848321925057257\n",
      "Gradient Descent(4/49): loss=0.3819789316230929\n",
      "Gradient Descent(5/49): loss=0.37964722790906386\n",
      "Gradient Descent(6/49): loss=0.377707019665041\n",
      "Gradient Descent(7/49): loss=0.37607026035618385\n",
      "Gradient Descent(8/49): loss=0.3746743168233049\n",
      "Gradient Descent(9/49): loss=0.373473400340881\n",
      "Gradient Descent(10/49): loss=0.37243319769271294\n",
      "Gradient Descent(11/49): loss=0.3715273750710783\n",
      "Gradient Descent(12/49): loss=0.37073526123019473\n",
      "Gradient Descent(13/49): loss=0.3700402905291078\n",
      "Gradient Descent(14/49): loss=0.36942894176927477\n",
      "Gradient Descent(15/49): loss=0.36889000362658064\n",
      "Gradient Descent(16/49): loss=0.36841405720520054\n",
      "Gradient Descent(17/49): loss=0.3679931043832711\n",
      "Gradient Descent(18/49): loss=0.3676202951839865\n",
      "Gradient Descent(19/49): loss=0.3672897233151491\n",
      "Gradient Descent(20/49): loss=0.3669962693720353\n",
      "Gradient Descent(21/49): loss=0.3667354779634783\n",
      "Gradient Descent(22/49): loss=0.3665034594624332\n",
      "Gradient Descent(23/49): loss=0.36629681001324277\n",
      "Gradient Descent(24/49): loss=0.3661125453737725\n",
      "Gradient Descent(25/49): loss=0.36594804547191034\n",
      "Gradient Descent(26/49): loss=0.36580100743376576\n",
      "Gradient Descent(27/49): loss=0.3656694054392655\n",
      "Gradient Descent(28/49): loss=0.36555145617378754\n",
      "Gradient Descent(29/49): loss=0.3654455889336471\n",
      "Gradient Descent(30/49): loss=0.36535041964913967\n",
      "Gradient Descent(31/49): loss=0.3652647282382095\n",
      "Gradient Descent(32/49): loss=0.3651874388143673\n",
      "Gradient Descent(33/49): loss=0.3651176023560616\n",
      "Gradient Descent(34/49): loss=0.36505438150922687\n",
      "Gradient Descent(35/49): loss=0.3649970372455875\n",
      "Gradient Descent(36/49): loss=0.3649449171401208\n",
      "Gradient Descent(37/49): loss=0.36489744506443833\n",
      "Gradient Descent(38/49): loss=0.3648541121204977\n",
      "Gradient Descent(39/49): loss=0.3648144686622585\n",
      "Gradient Descent(40/49): loss=0.3647781172725852\n",
      "Gradient Descent(41/49): loss=0.36474470657952573\n",
      "Gradient Descent(42/49): loss=0.3647139258105858\n",
      "Gradient Descent(43/49): loss=0.36468549999615996\n",
      "Gradient Descent(44/49): loss=0.3646591857441687\n",
      "Gradient Descent(45/49): loss=0.3646347675174536\n",
      "Gradient Descent(46/49): loss=0.3646120543537729\n",
      "Gradient Descent(47/49): loss=0.3645908769755104\n",
      "Gradient Descent(48/49): loss=0.36457108524256976\n",
      "Gradient Descent(49/49): loss=0.3645525459075192\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3964408352341489\n",
      "Gradient Descent(2/49): loss=0.3901075731560177\n",
      "Gradient Descent(3/49): loss=0.3862210528507395\n",
      "Gradient Descent(4/49): loss=0.3829933298772071\n",
      "Gradient Descent(5/49): loss=0.3802714256603585\n",
      "Gradient Descent(6/49): loss=0.3779676046230926\n",
      "Gradient Descent(7/49): loss=0.37601334253785307\n",
      "Gradient Descent(8/49): loss=0.374352712432795\n",
      "Gradient Descent(9/49): loss=0.37293940638978096\n",
      "Gradient Descent(10/49): loss=0.37173482681140546\n",
      "Gradient Descent(11/49): loss=0.37070667849570627\n",
      "Gradient Descent(12/49): loss=0.3698278665203621\n",
      "Gradient Descent(13/49): loss=0.36907561063344246\n",
      "Gradient Descent(14/49): loss=0.36843072428543255\n",
      "Gradient Descent(15/49): loss=0.3678770231455933\n",
      "Gradient Descent(16/49): loss=0.3674008372248864\n",
      "Gradient Descent(17/49): loss=0.3669906067593968\n",
      "Gradient Descent(18/49): loss=0.3666365463011225\n",
      "Gradient Descent(19/49): loss=0.36633036466710217\n",
      "Gradient Descent(20/49): loss=0.3660650308514188\n",
      "Gradient Descent(21/49): loss=0.3658345779130227\n",
      "Gradient Descent(22/49): loss=0.3656339383530295\n",
      "Gradient Descent(23/49): loss=0.3654588056855985\n",
      "Gradient Descent(24/49): loss=0.36530551785785303\n",
      "Gradient Descent(25/49): loss=0.3651709589396085\n",
      "Gradient Descent(26/49): loss=0.3650524761229931\n",
      "Gradient Descent(27/49): loss=0.3649478095759217\n",
      "Gradient Descent(28/49): loss=0.36485503310537315\n",
      "Gradient Descent(29/49): loss=0.36477250392474914\n",
      "Gradient Descent(30/49): loss=0.36469882009858084\n",
      "Gradient Descent(31/49): loss=0.3646327844687131\n",
      "Gradient Descent(32/49): loss=0.36457337405776363\n",
      "Gradient Descent(33/49): loss=0.3645197141052475\n",
      "Gradient Descent(34/49): loss=0.36447105602497903\n",
      "Gradient Descent(35/49): loss=0.3644267586838212\n",
      "Gradient Descent(36/49): loss=0.36438627249530786\n",
      "Gradient Descent(37/49): loss=0.3643491259001405\n",
      "Gradient Descent(38/49): loss=0.3643149138715847\n",
      "Gradient Descent(39/49): loss=0.36428328813939986\n",
      "Gradient Descent(40/49): loss=0.3642539488728374\n",
      "Gradient Descent(41/49): loss=0.36422663760283414\n",
      "Gradient Descent(42/49): loss=0.3642011311969945\n",
      "Gradient Descent(43/49): loss=0.36417723672924607\n",
      "Gradient Descent(44/49): loss=0.36415478711000715\n",
      "Gradient Descent(45/49): loss=0.3641336373629852\n",
      "Gradient Descent(46/49): loss=0.36411366145191043\n",
      "Gradient Descent(47/49): loss=0.3640947495750798\n",
      "Gradient Descent(48/49): loss=0.36407680585793695\n",
      "Gradient Descent(49/49): loss=0.36405974638440364\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3963409137214078\n",
      "Gradient Descent(2/49): loss=0.3899756469475488\n",
      "Gradient Descent(3/49): loss=0.3860461245607387\n",
      "Gradient Descent(4/49): loss=0.3827760649704651\n",
      "Gradient Descent(5/49): loss=0.38001539003097407\n",
      "Gradient Descent(6/49): loss=0.37767645738788685\n",
      "Gradient Descent(7/49): loss=0.37569037574322706\n",
      "Gradient Descent(8/49): loss=0.37400085144253126\n",
      "Gradient Descent(9/49): loss=0.37256127488493307\n",
      "Gradient Descent(10/49): loss=0.3713328024174919\n",
      "Gradient Descent(11/49): loss=0.3702829314868523\n",
      "Gradient Descent(12/49): loss=0.3693843858097388\n",
      "Gradient Descent(13/49): loss=0.36861422173370456\n",
      "Gradient Descent(14/49): loss=0.367953102568666\n",
      "Gradient Descent(15/49): loss=0.3673847044864747\n",
      "Gradient Descent(16/49): loss=0.3668952272443222\n",
      "Gradient Descent(17/49): loss=0.36647298933613437\n",
      "Gradient Descent(18/49): loss=0.36610809168685904\n",
      "Gradient Descent(19/49): loss=0.36579213734868116\n",
      "Gradient Descent(20/49): loss=0.3655179971987245\n",
      "Gradient Descent(21/49): loss=0.36527961359872096\n",
      "Gradient Descent(22/49): loss=0.3650718355085993\n",
      "Gradient Descent(23/49): loss=0.36489027975360255\n",
      "Gradient Descent(24/49): loss=0.36473121410484544\n",
      "Gradient Descent(25/49): loss=0.3645914586025553\n",
      "Gradient Descent(26/49): loss=0.36446830217173903\n",
      "Gradient Descent(27/49): loss=0.36435943208353416\n",
      "Gradient Descent(28/49): loss=0.36426287422636217\n",
      "Gradient Descent(29/49): loss=0.36417694248790916\n",
      "Gradient Descent(30/49): loss=0.3641001958264802\n",
      "Gradient Descent(31/49): loss=0.3640314018397752\n",
      "Gradient Descent(32/49): loss=0.3639695058296156\n",
      "Gradient Descent(33/49): loss=0.3639136045197395\n",
      "Gradient Descent(34/49): loss=0.36386292371618667\n",
      "Gradient Descent(35/49): loss=0.36381679931061894\n",
      "Gradient Descent(36/49): loss=0.36377466111986934\n",
      "Gradient Descent(37/49): loss=0.36373601913313564\n",
      "Gradient Descent(38/49): loss=0.36370045180398625\n",
      "Gradient Descent(39/49): loss=0.3636675960797828\n",
      "Gradient Descent(40/49): loss=0.3636371389079087\n",
      "Gradient Descent(41/49): loss=0.3636088099977383\n",
      "Gradient Descent(42/49): loss=0.3635823756507166\n",
      "Gradient Descent(43/49): loss=0.36355763349923315\n",
      "Gradient Descent(44/49): loss=0.3635344080189609\n",
      "Gradient Descent(45/49): loss=0.36351254669965366\n",
      "Gradient Descent(46/49): loss=0.36349191677665144\n",
      "Gradient Descent(47/49): loss=0.36347240243996887\n",
      "Gradient Descent(48/49): loss=0.36345390245027126\n",
      "Gradient Descent(49/49): loss=0.3634363281015922\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3966829978680022\n",
      "Gradient Descent(2/49): loss=0.3903845374067263\n",
      "Gradient Descent(3/49): loss=0.38647636545520725\n",
      "Gradient Descent(4/49): loss=0.3832231924236276\n",
      "Gradient Descent(5/49): loss=0.3804741152513643\n",
      "Gradient Descent(6/49): loss=0.3781420429398282\n",
      "Gradient Descent(7/49): loss=0.37615917657566966\n",
      "Gradient Descent(8/49): loss=0.37447021934317215\n",
      "Gradient Descent(9/49): loss=0.3730293377000469\n",
      "Gradient Descent(10/49): loss=0.3717982626007135\n",
      "Gradient Descent(11/49): loss=0.37074491008553834\n",
      "Gradient Descent(12/49): loss=0.36984230757918335\n",
      "Gradient Descent(13/49): loss=0.3690677317858656\n",
      "Gradient Descent(14/49): loss=0.3684020059762256\n",
      "Gradient Descent(15/49): loss=0.36782892233341735\n",
      "Gradient Descent(16/49): loss=0.3673347644409099\n",
      "Gradient Descent(17/49): loss=0.36690791088893127\n",
      "Gradient Descent(18/49): loss=0.36653850509194724\n",
      "Gradient Descent(19/49): loss=0.36621817945895874\n",
      "Gradient Descent(20/49): loss=0.36593982439074274\n",
      "Gradient Descent(21/49): loss=0.3656973943950542\n",
      "Gradient Descent(22/49): loss=0.3654857450432634\n",
      "Gradient Descent(23/49): loss=0.36530049563149025\n",
      "Gradient Descent(24/49): loss=0.3651379133225961\n",
      "Gradient Descent(25/49): loss=0.3649948152820721\n",
      "Gradient Descent(26/49): loss=0.3648684859184822\n",
      "Gradient Descent(27/49): loss=0.3647566068264834\n",
      "Gradient Descent(28/49): loss=0.36465719742976704\n",
      "Gradient Descent(29/49): loss=0.3645685646498682\n",
      "Gradient Descent(30/49): loss=0.36448926019826006\n",
      "Gradient Descent(31/49): loss=0.3644180443142026\n",
      "Gradient Descent(32/49): loss=0.36435385495799194\n",
      "Gradient Descent(33/49): loss=0.3642957816253572\n",
      "Gradient Descent(34/49): loss=0.36424304307927935\n",
      "Gradient Descent(35/49): loss=0.3641949684048906\n",
      "Gradient Descent(36/49): loss=0.3641509808849499\n",
      "Gradient Descent(37/49): loss=0.36411058427065013\n",
      "Gradient Descent(38/49): loss=0.3640733510875875\n",
      "Gradient Descent(39/49): loss=0.36403891267162775\n",
      "Gradient Descent(40/49): loss=0.3640069506757661\n",
      "Gradient Descent(41/49): loss=0.36397718982828153\n",
      "Gradient Descent(42/49): loss=0.3639493917556556\n",
      "Gradient Descent(43/49): loss=0.36392334971182055\n",
      "Gradient Descent(44/49): loss=0.3638988840791064\n",
      "Gradient Descent(45/49): loss=0.3638758385264505\n",
      "Gradient Descent(46/49): loss=0.3638540767275572\n",
      "Gradient Descent(47/49): loss=0.36383347955623996\n",
      "Gradient Descent(48/49): loss=0.36381394268852585\n",
      "Gradient Descent(49/49): loss=0.3637953745515891\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3947282623379986\n",
      "Gradient Descent(2/49): loss=0.3881816660154346\n",
      "Gradient Descent(3/49): loss=0.38452900640529497\n",
      "Gradient Descent(4/49): loss=0.38165537827982465\n",
      "Gradient Descent(5/49): loss=0.3793153984613386\n",
      "Gradient Descent(6/49): loss=0.37737488969815475\n",
      "Gradient Descent(7/49): loss=0.37574282082301\n",
      "Gradient Descent(8/49): loss=0.37435479734067933\n",
      "Gradient Descent(9/49): loss=0.3731639684826112\n",
      "Gradient Descent(10/49): loss=0.3721353414890096\n",
      "Gradient Descent(11/49): loss=0.37124211322236267\n",
      "Gradient Descent(12/49): loss=0.37046326279995645\n",
      "Gradient Descent(13/49): loss=0.36978194834265143\n",
      "Gradient Descent(14/49): loss=0.369184422609433\n",
      "Gradient Descent(15/49): loss=0.3686592867748615\n",
      "Gradient Descent(16/49): loss=0.3681969667774947\n",
      "Gradient Descent(17/49): loss=0.3677893378230979\n",
      "Gradient Descent(18/49): loss=0.36742944881086526\n",
      "Gradient Descent(19/49): loss=0.36711131520005824\n",
      "Gradient Descent(20/49): loss=0.366829759598548\n",
      "Gradient Descent(21/49): loss=0.36658028630507833\n",
      "Gradient Descent(22/49): loss=0.3663589805482208\n",
      "Gradient Descent(23/49): loss=0.3661624261106653\n",
      "Gradient Descent(24/49): loss=0.36598763696481157\n",
      "Gradient Descent(25/49): loss=0.36583199983085773\n",
      "Gradient Descent(26/49): loss=0.36569322543021066\n",
      "Gradient Descent(27/49): loss=0.3655693067919983\n",
      "Gradient Descent(28/49): loss=0.3654584833735734\n",
      "Gradient Descent(29/49): loss=0.3653592100385407\n",
      "Gradient Descent(30/49): loss=0.3652701301379326\n",
      "Gradient Descent(31/49): loss=0.3651900520877782\n",
      "Gradient Descent(32/49): loss=0.36511792894658357\n",
      "Gradient Descent(33/49): loss=0.36505284058050075\n",
      "Gradient Descent(34/49): loss=0.364993978069778\n",
      "Gradient Descent(35/49): loss=0.36494063006254307\n",
      "Gradient Descent(36/49): loss=0.364892170824551\n",
      "Gradient Descent(37/49): loss=0.364848049768643\n",
      "Gradient Descent(38/49): loss=0.3648077822770004\n",
      "Gradient Descent(39/49): loss=0.3647709416540545\n",
      "Gradient Descent(40/49): loss=0.3647371520690221\n",
      "Gradient Descent(41/49): loss=0.3647060823651415\n",
      "Gradient Descent(42/49): loss=0.3646774406282968\n",
      "Gradient Descent(43/49): loss=0.3646509694212351\n",
      "Gradient Descent(44/49): loss=0.36462644160132235\n",
      "Gradient Descent(45/49): loss=0.36460365665000755\n",
      "Gradient Descent(46/49): loss=0.3645824374510764\n",
      "Gradient Descent(47/49): loss=0.364562627462571\n",
      "Gradient Descent(48/49): loss=0.36454408823405243\n",
      "Gradient Descent(49/49): loss=0.3645266972268493\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3957833476173205\n",
      "Gradient Descent(2/49): loss=0.38983941945308137\n",
      "Gradient Descent(3/49): loss=0.38590024134583284\n",
      "Gradient Descent(4/49): loss=0.38263372571744403\n",
      "Gradient Descent(5/49): loss=0.3798924279597891\n",
      "Gradient Descent(6/49): loss=0.37758325858039227\n",
      "Gradient Descent(7/49): loss=0.3756336665808452\n",
      "Gradient Descent(8/49): loss=0.3739846851247847\n",
      "Gradient Descent(9/49): loss=0.37258769853923196\n",
      "Gradient Descent(10/49): loss=0.37140236889586065\n",
      "Gradient Descent(11/49): loss=0.37039510163277883\n",
      "Gradient Descent(12/49): loss=0.36953784374620813\n",
      "Gradient Descent(13/49): loss=0.36880711997721966\n",
      "Gradient Descent(14/49): loss=0.36818325079256997\n",
      "Gradient Descent(15/49): loss=0.367649713330631\n",
      "Gradient Descent(16/49): loss=0.36719261647359\n",
      "Gradient Descent(17/49): loss=0.36680026787962117\n",
      "Gradient Descent(18/49): loss=0.3664628156230784\n",
      "Gradient Descent(19/49): loss=0.3661719507009167\n",
      "Gradient Descent(20/49): loss=0.3659206594285513\n",
      "Gradient Descent(21/49): loss=0.36570301689504875\n",
      "Gradient Descent(22/49): loss=0.365514014331216\n",
      "Gradient Descent(23/49): loss=0.36534941457583336\n",
      "Gradient Descent(24/49): loss=0.36520563088633196\n",
      "Gradient Descent(25/49): loss=0.3650796251912332\n",
      "Gradient Descent(26/49): loss=0.36496882256829727\n",
      "Gradient Descent(27/49): loss=0.3648710392893392\n",
      "Gradient Descent(28/49): loss=0.36478442222673224\n",
      "Gradient Descent(29/49): loss=0.36470739778839734\n",
      "Gradient Descent(30/49): loss=0.36463862885369747\n",
      "Gradient Descent(31/49): loss=0.36457697843476233\n",
      "Gradient Descent(32/49): loss=0.36452147899641224\n",
      "Gradient Descent(33/49): loss=0.3644713065409824\n",
      "Gradient Descent(34/49): loss=0.3644257587083876\n",
      "Gradient Descent(35/49): loss=0.364384236261849\n",
      "Gradient Descent(36/49): loss=0.3643462274300165\n",
      "Gradient Descent(37/49): loss=0.3643112946601499\n",
      "Gradient Descent(38/49): loss=0.36427906340734956\n",
      "Gradient Descent(39/49): loss=0.36424921264383464\n",
      "Gradient Descent(40/49): loss=0.36422146682183515\n",
      "Gradient Descent(41/49): loss=0.3641955890653289\n",
      "Gradient Descent(42/49): loss=0.3641713754009272\n",
      "Gradient Descent(43/49): loss=0.36414864986774204\n",
      "Gradient Descent(44/49): loss=0.36412726037094995\n",
      "Gradient Descent(45/49): loss=0.3641070751647505\n",
      "Gradient Descent(46/49): loss=0.364087979868116\n",
      "Gradient Descent(47/49): loss=0.36406987493166504\n",
      "Gradient Descent(48/49): loss=0.3640526734866004\n",
      "Gradient Descent(49/49): loss=0.3640362995173056\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39568292124312604\n",
      "Gradient Descent(2/49): loss=0.389704274748738\n",
      "Gradient Descent(3/49): loss=0.38572117868241823\n",
      "Gradient Descent(4/49): loss=0.38241153534788763\n",
      "Gradient Descent(5/49): loss=0.37963085664578217\n",
      "Gradient Descent(6/49): loss=0.37728611689444896\n",
      "Gradient Descent(7/49): loss=0.3753043682950192\n",
      "Gradient Descent(8/49): loss=0.3736262590716142\n",
      "Gradient Descent(9/49): loss=0.37220285976680034\n",
      "Gradient Descent(10/49): loss=0.37099357637860014\n",
      "Gradient Descent(11/49): loss=0.369964597389584\n",
      "Gradient Descent(12/49): loss=0.3690876788386295\n",
      "Gradient Descent(13/49): loss=0.36833917256434506\n",
      "Gradient Descent(14/49): loss=0.3676992397090903\n",
      "Gradient Descent(15/49): loss=0.3671512092664698\n",
      "Gradient Descent(16/49): loss=0.3666810519187647\n",
      "Gradient Descent(17/49): loss=0.3662769464392441\n",
      "Gradient Descent(18/49): loss=0.3659289209819209\n",
      "Gradient Descent(19/49): loss=0.36562855533655036\n",
      "Gradient Descent(20/49): loss=0.36536873307911066\n",
      "Gradient Descent(21/49): loss=0.36514343474585353\n",
      "Gradient Descent(22/49): loss=0.3649475648714497\n",
      "Gradient Descent(23/49): loss=0.36477680707864346\n",
      "Gradient Descent(24/49): loss=0.36462750247505255\n",
      "Gradient Descent(25/49): loss=0.3644965474663159\n",
      "Gradient Descent(26/49): loss=0.36438130778144334\n",
      "Gradient Descent(27/49): loss=0.3642795460619854\n",
      "Gradient Descent(28/49): loss=0.3641893608189237\n",
      "Gradient Descent(29/49): loss=0.3641091349310673\n",
      "Gradient Descent(30/49): loss=0.3640374921625807\n",
      "Gradient Descent(31/49): loss=0.3639732604278015\n",
      "Gradient Descent(32/49): loss=0.36391544073882937\n",
      "Gradient Descent(33/49): loss=0.36386318094343095\n",
      "Gradient Descent(34/49): loss=0.36381575350399753\n",
      "Gradient Descent(35/49): loss=0.3637725366877367\n",
      "Gradient Descent(36/49): loss=0.3637329986381122\n",
      "Gradient Descent(37/49): loss=0.36369668388114207\n",
      "Gradient Descent(38/49): loss=0.3636632018902681\n",
      "Gradient Descent(39/49): loss=0.3636322173923834\n",
      "Gradient Descent(40/49): loss=0.36360344214709767\n",
      "Gradient Descent(41/49): loss=0.3635766279729741\n",
      "Gradient Descent(42/49): loss=0.3635515608295619\n",
      "Gradient Descent(43/49): loss=0.36352805579362363\n",
      "Gradient Descent(44/49): loss=0.3635059527929078\n",
      "Gradient Descent(45/49): loss=0.363485112981876\n",
      "Gradient Descent(46/49): loss=0.36346541566157886\n",
      "Gradient Descent(47/49): loss=0.3634467556608963\n",
      "Gradient Descent(48/49): loss=0.3634290411090582\n",
      "Gradient Descent(49/49): loss=0.3634121915400975\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3960348455154442\n",
      "Gradient Descent(2/49): loss=0.39011397196596814\n",
      "Gradient Descent(3/49): loss=0.38615301808033137\n",
      "Gradient Descent(4/49): loss=0.3828604412687959\n",
      "Gradient Descent(5/49): loss=0.3800910406115365\n",
      "Gradient Descent(6/49): loss=0.37775266797543167\n",
      "Gradient Descent(7/49): loss=0.37577361115060154\n",
      "Gradient Descent(8/49): loss=0.3740955737774735\n",
      "Gradient Descent(9/49): loss=0.372670424701669\n",
      "Gradient Descent(10/49): loss=0.37145815081063555\n",
      "Gradient Descent(11/49): loss=0.37042535840110646\n",
      "Gradient Descent(12/49): loss=0.3695441034233212\n",
      "Gradient Descent(13/49): loss=0.368790953149087\n",
      "Gradient Descent(14/49): loss=0.36814622352244764\n",
      "Gradient Descent(15/49): loss=0.36759335456546066\n",
      "Gradient Descent(16/49): loss=0.36711839615682107\n",
      "Gradient Descent(17/49): loss=0.3667095829506675\n",
      "Gradient Descent(18/49): loss=0.36635698179550097\n",
      "Gradient Descent(19/49): loss=0.36605219844394926\n",
      "Gradient Descent(20/49): loss=0.36578813297327883\n",
      "Gradient Descent(21/49): loss=0.36555877538261533\n",
      "Gradient Descent(22/49): loss=0.3653590344423129\n",
      "Gradient Descent(23/49): loss=0.3651845941477553\n",
      "Gradient Descent(24/49): loss=0.36503179315012874\n",
      "Gradient Descent(25/49): loss=0.364897523357185\n",
      "Gradient Descent(26/49): loss=0.36477914456061805\n",
      "Gradient Descent(27/49): loss=0.36467441248622384\n",
      "Gradient Descent(28/49): loss=0.36458141810376854\n",
      "Gradient Descent(29/49): loss=0.3644985363950912\n",
      "Gradient Descent(30/49): loss=0.36442438307677383\n",
      "Gradient Descent(31/49): loss=0.3643577780198296\n",
      "Gradient Descent(32/49): loss=0.36429771431288593\n",
      "Gradient Descent(33/49): loss=0.3642433320849398\n",
      "Gradient Descent(34/49): loss=0.36419389634508476\n",
      "Gradient Descent(35/49): loss=0.36414877821462105\n",
      "Gradient Descent(36/49): loss=0.3641074390256906\n",
      "Gradient Descent(37/49): loss=0.3640694168433173\n",
      "Gradient Descent(38/49): loss=0.3640343150371641\n",
      "Gradient Descent(39/49): loss=0.36400179258765836\n",
      "Gradient Descent(40/49): loss=0.36397155586021424\n",
      "Gradient Descent(41/49): loss=0.3639433516226015\n",
      "Gradient Descent(42/49): loss=0.36391696111532407\n",
      "Gradient Descent(43/49): loss=0.3638921950142433\n",
      "Gradient Descent(44/49): loss=0.36386888914945104\n",
      "Gradient Descent(45/49): loss=0.36384690086532057\n",
      "Gradient Descent(46/49): loss=0.36382610592433356\n",
      "Gradient Descent(47/49): loss=0.3638063958722155\n",
      "Gradient Descent(48/49): loss=0.363787675794534\n",
      "Gradient Descent(49/49): loss=0.36376986240559483\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3939495270873362\n",
      "Gradient Descent(2/49): loss=0.3879090972921138\n",
      "Gradient Descent(3/49): loss=0.38423247124026433\n",
      "Gradient Descent(4/49): loss=0.3813393806395577\n",
      "Gradient Descent(5/49): loss=0.37899245601336584\n",
      "Gradient Descent(6/49): loss=0.3770526925113657\n",
      "Gradient Descent(7/49): loss=0.37542610048332925\n",
      "Gradient Descent(8/49): loss=0.37404659369913934\n",
      "Gradient Descent(9/49): loss=0.3728663113904765\n",
      "Gradient Descent(10/49): loss=0.3718496128461199\n",
      "Gradient Descent(11/49): loss=0.37096924245504564\n",
      "Gradient Descent(12/49): loss=0.37020383780967925\n",
      "Gradient Descent(13/49): loss=0.36953628466598154\n",
      "Gradient Descent(14/49): loss=0.36895261226859305\n",
      "Gradient Descent(15/49): loss=0.36844123708264376\n",
      "Gradient Descent(16/49): loss=0.36799243365726086\n",
      "Gradient Descent(17/49): loss=0.3675979554550791\n",
      "Gradient Descent(18/49): loss=0.3672507561975706\n",
      "Gradient Descent(19/49): loss=0.36694477978280815\n",
      "Gradient Descent(20/49): loss=0.36667479794513524\n",
      "Gradient Descent(21/49): loss=0.3664362819172098\n",
      "Gradient Descent(22/49): loss=0.3662252989065941\n",
      "Gradient Descent(23/49): loss=0.36603842714150364\n",
      "Gradient Descent(24/49): loss=0.3658726851587982\n",
      "Gradient Descent(25/49): loss=0.36572547227122515\n",
      "Gradient Descent(26/49): loss=0.3655945179942117\n",
      "Gradient Descent(27/49): loss=0.36547783878370893\n",
      "Gradient Descent(28/49): loss=0.3653737008304597\n",
      "Gradient Descent(29/49): loss=0.3652805879332085\n",
      "Gradient Descent(30/49): loss=0.3651971736728213\n",
      "Gradient Descent(31/49): loss=0.3651222972562759\n",
      "Gradient Descent(32/49): loss=0.36505494251047205\n",
      "Gradient Descent(33/49): loss=0.36499421959160977\n",
      "Gradient Descent(34/49): loss=0.3649393490436851\n",
      "Gradient Descent(35/49): loss=0.3648896478942819\n",
      "Gradient Descent(36/49): loss=0.36484451752061486\n",
      "Gradient Descent(37/49): loss=0.36480343305598545\n",
      "Gradient Descent(38/49): loss=0.364765934138097\n",
      "Gradient Descent(39/49): loss=0.3647316168272085\n",
      "Gradient Descent(40/49): loss=0.3647001265447785\n",
      "Gradient Descent(41/49): loss=0.3646711519027241\n",
      "Gradient Descent(42/49): loss=0.36464441931022257\n",
      "Gradient Descent(43/49): loss=0.36461968825951546\n",
      "Gradient Descent(44/49): loss=0.3645967472047905\n",
      "Gradient Descent(45/49): loss=0.3645754099591719\n",
      "Gradient Descent(46/49): loss=0.3645555125443865\n",
      "Gradient Descent(47/49): loss=0.3645369104359779\n",
      "Gradient Descent(48/49): loss=0.3645194761541848\n",
      "Gradient Descent(49/49): loss=0.36450309715690926\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39526233829594365\n",
      "Gradient Descent(2/49): loss=0.3895848685662523\n",
      "Gradient Descent(3/49): loss=0.3855850642879756\n",
      "Gradient Descent(4/49): loss=0.3822809853629943\n",
      "Gradient Descent(5/49): loss=0.37952218669698135\n",
      "Gradient Descent(6/49): loss=0.37720950980176293\n",
      "Gradient Descent(7/49): loss=0.37526617415680436\n",
      "Gradient Descent(8/49): loss=0.37363010891710347\n",
      "Gradient Descent(9/49): loss=0.37225038321471626\n",
      "Gradient Descent(10/49): loss=0.37108493678504106\n",
      "Gradient Descent(11/49): loss=0.3700989056806011\n",
      "Gradient Descent(12/49): loss=0.36926331400965123\n",
      "Gradient Descent(13/49): loss=0.36855402808512233\n",
      "Gradient Descent(14/49): loss=0.3679509110637852\n",
      "Gradient Descent(15/49): loss=0.36743713496229263\n",
      "Gradient Descent(16/49): loss=0.36699861792125976\n",
      "Gradient Descent(17/49): loss=0.3666235620287353\n",
      "Gradient Descent(18/49): loss=0.366302072418522\n",
      "Gradient Descent(19/49): loss=0.36602584241686364\n",
      "Gradient Descent(20/49): loss=0.3657878926150896\n",
      "Gradient Descent(21/49): loss=0.36558235414986534\n",
      "Gradient Descent(22/49): loss=0.365404288352832\n",
      "Gradient Descent(23/49): loss=0.3652495364140515\n",
      "Gradient Descent(24/49): loss=0.36511459388145945\n",
      "Gradient Descent(25/49): loss=0.36499650576037457\n",
      "Gradient Descent(26/49): loss=0.3648927787347836\n",
      "Gradient Descent(27/49): loss=0.3648013076449327\n",
      "Gradient Descent(28/49): loss=0.3647203138538187\n",
      "Gradient Descent(29/49): loss=0.364648293541743\n",
      "Gradient Descent(30/49): loss=0.36458397430124856\n",
      "Gradient Descent(31/49): loss=0.3645262786787297\n",
      "Gradient Descent(32/49): loss=0.3644742935349624\n",
      "Gradient Descent(33/49): loss=0.36442724428368234\n",
      "Gradient Descent(34/49): loss=0.36438447322225404\n",
      "Gradient Descent(35/49): loss=0.36434542129716435\n",
      "Gradient Descent(36/49): loss=0.36430961275415835\n",
      "Gradient Descent(37/49): loss=0.36427664221210443\n",
      "Gradient Descent(38/49): loss=0.36424616377416885\n",
      "Gradient Descent(39/49): loss=0.3642178818521329\n",
      "Gradient Descent(40/49): loss=0.364191543431767\n",
      "Gradient Descent(41/49): loss=0.3641669315507646\n",
      "Gradient Descent(42/49): loss=0.36414385979727726\n",
      "Gradient Descent(43/49): loss=0.3641221676677151\n",
      "Gradient Descent(44/49): loss=0.3641017166481782\n",
      "Gradient Descent(45/49): loss=0.36408238690544864\n",
      "Gradient Descent(46/49): loss=0.3640640744915851\n",
      "Gradient Descent(47/49): loss=0.36404668898138254\n",
      "Gradient Descent(48/49): loss=0.36403015147473894\n",
      "Gradient Descent(49/49): loss=0.36401439290672666\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3951615461463855\n",
      "Gradient Descent(2/49): loss=0.38944627441242424\n",
      "Gradient Descent(3/49): loss=0.3854017450856961\n",
      "Gradient Descent(4/49): loss=0.38205387182723805\n",
      "Gradient Descent(5/49): loss=0.37925514248108044\n",
      "Gradient Descent(6/49): loss=0.37690646877854006\n",
      "Gradient Descent(7/49): loss=0.3749306629346131\n",
      "Gradient Descent(8/49): loss=0.37326525771690766\n",
      "Gradient Descent(9/49): loss=0.37185899815952494\n",
      "Gradient Descent(10/49): loss=0.37066955798167744\n",
      "Gradient Descent(11/49): loss=0.3696618462317032\n",
      "Gradient Descent(12/49): loss=0.3688066859174469\n",
      "Gradient Descent(13/49): loss=0.36807976045180607\n",
      "Gradient Descent(14/49): loss=0.367460764095149\n",
      "Gradient Descent(15/49): loss=0.3669327118066692\n",
      "Gradient Descent(16/49): loss=0.366481375434185\n",
      "Gradient Descent(17/49): loss=0.36609482099600477\n",
      "Gradient Descent(18/49): loss=0.3657630274568269\n",
      "Gradient Descent(19/49): loss=0.3654775716055486\n",
      "Gradient Descent(20/49): loss=0.36523136683349805\n",
      "Gradient Descent(21/49): loss=0.36501844606457606\n",
      "Gradient Descent(22/49): loss=0.3648337809951529\n",
      "Gradient Descent(23/49): loss=0.36467313129703743\n",
      "Gradient Descent(24/49): loss=0.3645329186198148\n",
      "Gradient Descent(25/49): loss=0.3644101211716507\n",
      "Gradient Descent(26/49): loss=0.36430218541414333\n",
      "Gradient Descent(27/49): loss=0.3642069520174793\n",
      "Gradient Descent(28/49): loss=0.36412259371779293\n",
      "Gradient Descent(29/49): loss=0.36404756312286585\n",
      "Gradient Descent(30/49): loss=0.3639805488433964\n",
      "Gradient Descent(31/49): loss=0.3639204385992805\n",
      "Gradient Descent(32/49): loss=0.36386628817489586\n",
      "Gradient Descent(33/49): loss=0.3638172952831424\n",
      "Gradient Descent(34/49): loss=0.3637727775520688\n",
      "Gradient Descent(35/49): loss=0.36373215397597836\n",
      "Gradient Descent(36/49): loss=0.3636949292795655\n",
      "Gradient Descent(37/49): loss=0.3636606807326052\n",
      "Gradient Descent(38/49): loss=0.36362904702704374\n",
      "Gradient Descent(39/49): loss=0.36359971889050496\n",
      "Gradient Descent(40/49): loss=0.3635724311622853\n",
      "Gradient Descent(41/49): loss=0.36354695610153404\n",
      "Gradient Descent(42/49): loss=0.36352309773390984\n",
      "Gradient Descent(43/49): loss=0.3635006870737206\n",
      "Gradient Descent(44/49): loss=0.36347957808434883\n",
      "Gradient Descent(45/49): loss=0.3634596442614409\n",
      "Gradient Descent(46/49): loss=0.36344077574156486\n",
      "Gradient Descent(47/49): loss=0.36342287685436325\n",
      "Gradient Descent(48/49): loss=0.3634058640491285\n",
      "Gradient Descent(49/49): loss=0.36338966413757356\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.39552329790598706\n",
      "Gradient Descent(2/49): loss=0.38985630967673984\n",
      "Gradient Descent(3/49): loss=0.38583508348736734\n",
      "Gradient Descent(4/49): loss=0.38250447091898687\n",
      "Gradient Descent(5/49): loss=0.37971667974504625\n",
      "Gradient Descent(6/49): loss=0.37737386923047744\n",
      "Gradient Descent(7/49): loss=0.3754002325432448\n",
      "Gradient Descent(8/49): loss=0.37373441263666096\n",
      "Gradient Descent(9/49): loss=0.3723259717482558\n",
      "Gradient Descent(10/49): loss=0.3711331685827685\n",
      "Gradient Descent(11/49): loss=0.3701213278828338\n",
      "Gradient Descent(12/49): loss=0.3692615679564158\n",
      "Gradient Descent(13/49): loss=0.3685297822298679\n",
      "Gradient Descent(14/49): loss=0.3679058143043738\n",
      "Gradient Descent(15/49): loss=0.36737278500353354\n",
      "Gradient Descent(16/49): loss=0.3669165406367347\n",
      "Gradient Descent(17/49): loss=0.36652519883199797\n",
      "Gradient Descent(18/49): loss=0.3661887734305208\n",
      "Gradient Descent(19/49): loss=0.3658988637897369\n",
      "Gradient Descent(20/49): loss=0.3656483967956149\n",
      "Gradient Descent(21/49): loss=0.3654314121793732\n",
      "Gradient Descent(22/49): loss=0.36524288353384454\n",
      "Gradient Descent(23/49): loss=0.365078568848559\n",
      "Gradient Descent(24/49): loss=0.3649348855168971\n",
      "Gradient Descent(25/49): loss=0.36480880567808166\n",
      "Gradient Descent(26/49): loss=0.3646977684901138\n",
      "Gradient Descent(27/49): loss=0.36459960652420853\n",
      "Gradient Descent(28/49): loss=0.36451248395542696\n",
      "Gradient Descent(29/49): loss=0.3644348446201811\n",
      "Gradient Descent(30/49): loss=0.36436536833638794\n",
      "Gradient Descent(31/49): loss=0.3643029341498592\n",
      "Gradient Descent(32/49): loss=0.3642465893917952\n",
      "Gradient Descent(33/49): loss=0.36419552361556085\n",
      "Gradient Descent(34/49): loss=0.3641490466331342\n",
      "Gradient Descent(35/49): loss=0.36410656999826574\n",
      "Gradient Descent(36/49): loss=0.3640675913889435\n",
      "Gradient Descent(37/49): loss=0.36403168142987624\n",
      "Gradient Descent(38/49): loss=0.3639984725693672\n",
      "Gradient Descent(39/49): loss=0.36396764968659157\n",
      "Gradient Descent(40/49): loss=0.36393894215693023\n",
      "Gradient Descent(41/49): loss=0.3639121171463069\n",
      "Gradient Descent(42/49): loss=0.3638869739418081\n",
      "Gradient Descent(43/49): loss=0.3638633391563649\n",
      "Gradient Descent(44/49): loss=0.36384106267090655\n",
      "Gradient Descent(45/49): loss=0.3638200141989343\n",
      "Gradient Descent(46/49): loss=0.36380008037657796\n",
      "Gradient Descent(47/49): loss=0.36378116229643953\n",
      "Gradient Descent(48/49): loss=0.3637631734163499\n",
      "Gradient Descent(49/49): loss=0.36374603778496173\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3933032199696814\n",
      "Gradient Descent(2/49): loss=0.38765036406705883\n",
      "Gradient Descent(3/49): loss=0.3839421753532795\n",
      "Gradient Descent(4/49): loss=0.3810307246534703\n",
      "Gradient Descent(5/49): loss=0.37867808901299976\n",
      "Gradient Descent(6/49): loss=0.3767400298854423\n",
      "Gradient Descent(7/49): loss=0.3751196340582829\n",
      "Gradient Descent(8/49): loss=0.37374918830873016\n",
      "Gradient Descent(9/49): loss=0.3725798692946889\n",
      "Gradient Descent(10/49): loss=0.3715754161142298\n",
      "Gradient Descent(11/49): loss=0.3707081347277924\n",
      "Gradient Descent(12/49): loss=0.3699563275867627\n",
      "Gradient Descent(13/49): loss=0.3693026111707346\n",
      "Gradient Descent(14/49): loss=0.36873279365385053\n",
      "Gradient Descent(15/49): loss=0.3682351099442973\n",
      "Gradient Descent(16/49): loss=0.36779968756508974\n",
      "Gradient Descent(17/49): loss=0.36741816381517134\n",
      "Gradient Descent(18/49): loss=0.36708340378845805\n",
      "Gradient Descent(19/49): loss=0.3667892870026024\n",
      "Gradient Descent(20/49): loss=0.3665305417851354\n",
      "Gradient Descent(21/49): loss=0.36630261375238216\n",
      "Gradient Descent(22/49): loss=0.36610155928142113\n",
      "Gradient Descent(23/49): loss=0.3659239577987188\n",
      "Gradient Descent(24/49): loss=0.36576683860037434\n",
      "Gradient Descent(25/49): loss=0.36562761915785413\n",
      "Gradient Descent(26/49): loss=0.365504052687081\n",
      "Gradient Descent(27/49): loss=0.3653941833166916\n",
      "Gradient Descent(28/49): loss=0.3652963075770613\n",
      "Gradient Descent(29/49): loss=0.3652089412047469\n",
      "Gradient Descent(30/49): loss=0.36513079045518043\n",
      "Gradient Descent(31/49): loss=0.36506072726405814\n",
      "Gradient Descent(32/49): loss=0.3649977677106277\n",
      "Gradient Descent(33/49): loss=0.364941053324285\n",
      "Gradient Descent(34/49): loss=0.3648898348463656\n",
      "Gradient Descent(35/49): loss=0.3648434581163651\n",
      "Gradient Descent(36/49): loss=0.3648013517991921\n",
      "Gradient Descent(37/49): loss=0.3647630167096747\n",
      "Gradient Descent(38/49): loss=0.36472801652398956\n",
      "Gradient Descent(39/49): loss=0.3646959696961376\n",
      "Gradient Descent(40/49): loss=0.3646665424219333\n",
      "Gradient Descent(41/49): loss=0.3646394425138922\n",
      "Gradient Descent(42/49): loss=0.3646144140684319\n",
      "Gradient Descent(43/49): loss=0.36459123282238465\n",
      "Gradient Descent(44/49): loss=0.3645697021093056\n",
      "Gradient Descent(45/49): loss=0.36454964933775785\n",
      "Gradient Descent(46/49): loss=0.36453092292389516\n",
      "Gradient Descent(47/49): loss=0.3645133896194797\n",
      "Gradient Descent(48/49): loss=0.3644969321841227\n",
      "Gradient Descent(49/49): loss=0.3644814473571917\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3948778072700184\n",
      "Gradient Descent(2/49): loss=0.389341542769073\n",
      "Gradient Descent(3/49): loss=0.385275745071439\n",
      "Gradient Descent(4/49): loss=0.3819351891896873\n",
      "Gradient Descent(5/49): loss=0.3791605804907981\n",
      "Gradient Descent(6/49): loss=0.37684609550916764\n",
      "Gradient Descent(7/49): loss=0.37491048634328733\n",
      "Gradient Descent(8/49): loss=0.37328849747102166\n",
      "Gradient Descent(9/49): loss=0.3719268723178072\n",
      "Gradient Descent(10/49): loss=0.3707818477189967\n",
      "Gradient Descent(11/49): loss=0.36981732247111415\n",
      "Gradient Descent(12/49): loss=0.36900343470361296\n",
      "Gradient Descent(13/49): loss=0.3683154299355348\n",
      "Gradient Descent(14/49): loss=0.3677327501315043\n",
      "Gradient Descent(15/49): loss=0.36723829553872755\n",
      "Gradient Descent(16/49): loss=0.36681782349902364\n",
      "Gradient Descent(17/49): loss=0.3664594568080819\n",
      "Gradient Descent(18/49): loss=0.3661532802680443\n",
      "Gradient Descent(19/49): loss=0.36589100863044494\n",
      "Gradient Descent(20/49): loss=0.3656657125982131\n",
      "Gradient Descent(21/49): loss=0.36547159223627\n",
      "Gradient Descent(22/49): loss=0.3653037892304492\n",
      "Gradient Descent(23/49): loss=0.36515823107776835\n",
      "Gradient Descent(24/49): loss=0.3650315015925999\n",
      "Gradient Descent(25/49): loss=0.3649207331509865\n",
      "Gradient Descent(26/49): loss=0.36482351692766185\n",
      "Gradient Descent(27/49): loss=0.36473782805152394\n",
      "Gradient Descent(28/49): loss=0.36466196314915494\n",
      "Gradient Descent(29/49): loss=0.36459448818855517\n",
      "Gradient Descent(30/49): loss=0.3645341948967654\n",
      "Gradient Descent(31/49): loss=0.3644800643213516\n",
      "Gradient Descent(32/49): loss=0.3644312363492753\n",
      "Gradient Descent(33/49): loss=0.36438698419739274\n",
      "Gradient Descent(34/49): loss=0.3643466930546108\n",
      "Gradient Descent(35/49): loss=0.36430984219292784\n",
      "Gradient Descent(36/49): loss=0.3642759899783256\n",
      "Gradient Descent(37/49): loss=0.36424476130690064\n",
      "Gradient Descent(38/49): loss=0.36421583707011373\n",
      "Gradient Descent(39/49): loss=0.36418894531835383\n",
      "Gradient Descent(40/49): loss=0.3641638538464124\n",
      "Gradient Descent(41/49): loss=0.3641403639698257\n",
      "Gradient Descent(42/49): loss=0.36411830529886957\n",
      "Gradient Descent(43/49): loss=0.3640975313485797\n",
      "Gradient Descent(44/49): loss=0.3640779158495415\n",
      "Gradient Descent(45/49): loss=0.3640593496462399\n",
      "Gradient Descent(46/49): loss=0.3640417380881756\n",
      "Gradient Descent(47/49): loss=0.36402499883436334\n",
      "Gradient Descent(48/49): loss=0.3640090600047169\n",
      "Gradient Descent(49/49): loss=0.3639938586225982\n",
      "Gradient Descent(0/49): loss=0.5\n",
      "Gradient Descent(1/49): loss=0.3947767884311861\n",
      "Gradient Descent(2/49): loss=0.38919925432213587\n",
      "Gradient Descent(3/49): loss=0.3850880001449313\n",
      "Gradient Descent(4/49): loss=0.3817031260833772\n",
      "Gradient Descent(5/49): loss=0.3788881128315327\n",
      "Gradient Descent(6/49): loss=0.37653724386805254\n",
      "Gradient Descent(7/49): loss=0.3745688766693618\n",
      "Gradient Descent(8/49): loss=0.37291735743328136\n",
      "Gradient Descent(9/49): loss=0.37152909811874796\n",
      "Gradient Descent(10/49): loss=0.3703600600564991\n",
      "Gradient Descent(11/49): loss=0.3693739046518317\n",
      "Gradient Descent(12/49): loss=0.3685405582795776\n",
      "Gradient Descent(13/49): loss=0.3678350730933636\n",
      "Gradient Descent(14/49): loss=0.36723671223521936\n",
      "Gradient Descent(15/49): loss=0.36672820975659104\n",
      "Gradient Descent(16/49): loss=0.36629516850879196\n",
      "Gradient Descent(17/49): loss=0.3659255680316259\n",
      "Gradient Descent(18/49): loss=0.36560936079105477\n",
      "Gradient Descent(19/49): loss=0.36533813981532864\n",
      "Gradient Descent(20/49): loss=0.3651048643348248\n",
      "Gradient Descent(21/49): loss=0.36490363275741045\n",
      "Gradient Descent(22/49): loss=0.3647294944244751\n",
      "Gradient Descent(23/49): loss=0.3645782932462429\n",
      "Gradient Descent(24/49): loss=0.36444653761950596\n",
      "Gradient Descent(25/49): loss=0.3643312920679174\n",
      "Gradient Descent(26/49): loss=0.36423008687487596\n",
      "Gradient Descent(27/49): loss=0.3641408426471939\n",
      "Gradient Descent(28/49): loss=0.3640618072885724\n",
      "Gradient Descent(29/49): loss=0.36399150330173674\n",
      "Gradient Descent(30/49): loss=0.3639286836972899\n",
      "Gradient Descent(31/49): loss=0.3638722950817416\n",
      "Gradient Descent(32/49): loss=0.36382144673925354\n",
      "Gradient Descent(33/49): loss=0.3637753847212327\n",
      "Gradient Descent(34/49): loss=0.3637334701228758\n",
      "Gradient Descent(35/49): loss=0.36369516086239073\n",
      "Gradient Descent(36/49): loss=0.3636599963919815\n",
      "Gradient Descent(37/49): loss=0.363627584863878\n",
      "Gradient Descent(38/49): loss=0.3635975923530745\n",
      "Gradient Descent(39/49): loss=0.36356973380372654\n",
      "Gradient Descent(40/49): loss=0.3635437654206034\n",
      "Gradient Descent(41/49): loss=0.36351947827242415\n",
      "Gradient Descent(42/49): loss=0.3634966929118511\n",
      "Gradient Descent(43/49): loss=0.363475254848627\n",
      "Gradient Descent(44/49): loss=0.3634550307388493\n",
      "Gradient Descent(45/49): loss=0.3634359051755613\n",
      "Gradient Descent(46/49): loss=0.3634177779843984\n",
      "Gradient Descent(47/49): loss=0.36340056194356635\n",
      "Gradient Descent(48/49): loss=0.36338418086044594\n",
      "Gradient Descent(49/49): loss=0.363368567948017\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gamma_opt1 = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=2)\n",
    "w_gd1, loss_gd1 = least_squares_GD(set1_y, set1_x, gamma_opt1, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt1))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set2_x.shape[1])\n",
    "gamma_opt2 = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=2)\n",
    "w_gd2, loss_gd2 = least_squares_GD(set2_y, set2_x, gamma_opt2, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt2))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(set3_x.shape[1])\n",
    "gamma_opt3 = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=2)\n",
    "w_gd3, loss_gd3 = least_squares_GD(set3_y, set3_x, gamma_opt3, max_iters=max_iters)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt3))\n",
    "print(\"Gradient descent regression loss {loss}\".format(loss=loss_gd3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/julia/miniconda3/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "max_iters = 500\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "tX_log = log_distribution(tX, to_log)\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "print('')\n",
    "print(\"Preprocessing for stochastic gradient descent done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we perform a 4-fold cross validation for the gamma parameter for stochastic gradient descent method for each set. Then we we perform a stochastic gradient descent with the optimal gamma found. For the cross validation we use 50 iterations and for the final descent 500 iterations as we want a more precise final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_opt_sgd1 = cross_validation(set1_y, set1_x, k_fold, gammas, fonction=3)\n",
    "w_sgd1, loss_sgd1 = least_squares_SGD(set1_y, set1_x, gamma_opt_sgd1, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt_sgd1))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_sgd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_opt2 = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=3)\n",
    "w_sgd2, loss_sgd2 = least_squares_SGD(set2_y, set2_x, gamma_opt2, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt2))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_sgd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_opt3 = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=3)\n",
    "w_sgd3, loss_sgd3 = least_squares_SGD(set3_y, set3_x, gamma_opt3, max_iters=500)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt3))\n",
    "print(\"Stochastic gradient descent regression loss {loss}\".format(loss=loss_sgd3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.2614574679971575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.09751882802022077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.06105344416415092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Preprocessing for logistic regression done!\n"
     ]
    }
   ],
   "source": [
    "y[y==-1]=0 # transform y=-1 to 0\n",
    "k_fold = 4\n",
    "max_iters = 50\n",
    "gammas = np.arange(0, 0.5, 0.01) \n",
    "tX_log = log_distribution(tX, to_log)\n",
    "set1_x, set1_y, set1_ids, set2_x, set2_y, set2_ids, set3_x, set3_y, set3_ids = separate_sets(tX_log, y, ids)\n",
    "\n",
    "set1_x = outliers(set1_x, -999)\n",
    "set1_x = filtering_with_mean_bis(set1_x, set1_y, lr=1)\n",
    "#set1_x = filtering_with_mean(set1_x)\n",
    "set1_x = std(set1_x)\n",
    "#set1_x = scaling(set1_x)\n",
    "\n",
    "set2_x = outliers(set2_x, -999)\n",
    "set2_x = filtering_with_mean_bis(set2_x, set2_y, lr=1)\n",
    "#set2_x = filtering_with_mean(set2_x)\n",
    "set2_x = std(set2_x)\n",
    "\n",
    "set3_x = outliers(set3_x, -999)\n",
    "set3_x = filtering_with_mean_bis(set3_x, set3_y, lr=1)\n",
    "#set3_x = filtering_with_mean(set3_x)\n",
    "set3_x = std(set3_x)\n",
    "\n",
    "print('')\n",
    "print(\"Preprocessing for logistic regression done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=51940.29082807904\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00186535]\n",
      " [-0.00893223]\n",
      " [-0.00523796]\n",
      " ...\n",
      " [ 0.00447915]\n",
      " [ 0.00186535]\n",
      " [-0.00759268]]\n",
      "t [[ 0.00186535]\n",
      " [-0.00893223]\n",
      " [-0.00523796]\n",
      " ...\n",
      " [ 0.00447915]\n",
      " [ 0.00186535]\n",
      " [-0.00759268]]\n",
      "t [[ 0.00370957]\n",
      " [-0.01783104]\n",
      " [-0.01045069]\n",
      " ...\n",
      " [ 0.00890217]\n",
      " [ 0.00370957]\n",
      " [-0.01511593]]\n",
      "t [[ 0.00370957]\n",
      " [-0.01783104]\n",
      " [-0.01045069]\n",
      " ...\n",
      " [ 0.00890217]\n",
      " [ 0.00370957]\n",
      " [-0.01511593]]\n",
      "Current iteration=2, loss=51672.01505723351\n",
      "t [[ 0.00553286]\n",
      " [-0.02669651]\n",
      " [-0.01563839]\n",
      " ...\n",
      " [ 0.01326972]\n",
      " [ 0.00553286]\n",
      " [-0.02257059]]\n",
      "t [[ 0.00553286]\n",
      " [-0.02669651]\n",
      " [-0.01563839]\n",
      " ...\n",
      " [ 0.01326972]\n",
      " [ 0.00553286]\n",
      " [-0.02257059]]\n",
      "t [[ 0.00733541]\n",
      " [-0.03552873]\n",
      " [-0.02080124]\n",
      " ...\n",
      " [ 0.01758248]\n",
      " [ 0.00733541]\n",
      " [-0.0299575 ]]\n",
      "t [[ 0.00733541]\n",
      " [-0.03552873]\n",
      " [-0.02080124]\n",
      " ...\n",
      " [ 0.01758248]\n",
      " [ 0.00733541]\n",
      " [-0.0299575 ]]\n",
      "Current iteration=4, loss=51408.91553735361\n",
      "t [[ 0.00911744]\n",
      " [-0.0443278 ]\n",
      " [-0.02593944]\n",
      " ...\n",
      " [ 0.02184111]\n",
      " [ 0.00911744]\n",
      " [-0.03727747]]\n",
      "t [[ 0.00911744]\n",
      " [-0.0443278 ]\n",
      " [-0.02593944]\n",
      " ...\n",
      " [ 0.02184111]\n",
      " [ 0.00911744]\n",
      " [-0.03727747]]\n",
      "t [[ 0.01087914]\n",
      " [-0.0530938 ]\n",
      " [-0.03105318]\n",
      " ...\n",
      " [ 0.02604626]\n",
      " [ 0.01087914]\n",
      " [-0.04453133]]\n",
      "t [[ 0.01087914]\n",
      " [-0.0530938 ]\n",
      " [-0.03105318]\n",
      " ...\n",
      " [ 0.02604626]\n",
      " [ 0.01087914]\n",
      " [-0.04453133]]\n",
      "Current iteration=6, loss=51150.854606933666\n",
      "t [[ 0.01262071]\n",
      " [-0.06182683]\n",
      " [-0.03614265]\n",
      " ...\n",
      " [ 0.03019858]\n",
      " [ 0.01262071]\n",
      " [-0.05171989]]\n",
      "t [[ 0.01262071]\n",
      " [-0.06182683]\n",
      " [-0.03614265]\n",
      " ...\n",
      " [ 0.03019858]\n",
      " [ 0.01262071]\n",
      " [-0.05171989]]\n",
      "t [[ 0.01434233]\n",
      " [-0.07052698]\n",
      " [-0.04120804]\n",
      " ...\n",
      " [ 0.03429872]\n",
      " [ 0.01434233]\n",
      " [-0.05884395]]\n",
      "t [[ 0.01434233]\n",
      " [-0.07052698]\n",
      " [-0.04120804]\n",
      " ...\n",
      " [ 0.03429872]\n",
      " [ 0.01434233]\n",
      " [-0.05884395]]\n",
      "Current iteration=8, loss=50897.698854672744\n",
      "t [[ 0.01604422]\n",
      " [-0.07919435]\n",
      " [-0.04624952]\n",
      " ...\n",
      " [ 0.0383473 ]\n",
      " [ 0.01604422]\n",
      " [-0.06590429]]\n",
      "t [[ 0.01604422]\n",
      " [-0.07919435]\n",
      " [-0.04624952]\n",
      " ...\n",
      " [ 0.0383473 ]\n",
      " [ 0.01604422]\n",
      " [-0.06590429]]\n",
      "t [[ 0.01772655]\n",
      " [-0.08782903]\n",
      " [-0.0512673 ]\n",
      " ...\n",
      " [ 0.04234497]\n",
      " [ 0.01772655]\n",
      " [-0.07290171]]\n",
      "loss=50649.31900558634\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.00563294]\n",
      " [-0.00328378]\n",
      " [-0.00525539]\n",
      " ...\n",
      " [ 0.00441359]\n",
      " [ 0.00185803]\n",
      " [-0.00747794]]\n",
      "t [[ 0.00563294]\n",
      " [-0.00328378]\n",
      " [-0.00525539]\n",
      " ...\n",
      " [ 0.00441359]\n",
      " [ 0.00185803]\n",
      " [-0.00747794]]\n",
      "t [[ 0.01122763]\n",
      " [-0.00659579]\n",
      " [-0.01048528]\n",
      " ...\n",
      " [ 0.00877145]\n",
      " [ 0.00369485]\n",
      " [-0.01488648]]\n",
      "t [[ 0.01122763]\n",
      " [-0.00659579]\n",
      " [-0.01048528]\n",
      " ...\n",
      " [ 0.00877145]\n",
      " [ 0.00369485]\n",
      " [-0.01488648]]\n",
      "Current iteration=2, loss=51671.32544172018\n",
      "t [[ 0.01678433]\n",
      " [-0.00993539]\n",
      " [-0.01568987]\n",
      " ...\n",
      " [ 0.01307425]\n",
      " [ 0.00551068]\n",
      " [-0.02222648]]\n",
      "t [[ 0.01678433]\n",
      " [-0.00993539]\n",
      " [-0.01568987]\n",
      " ...\n",
      " [ 0.01307425]\n",
      " [ 0.00551068]\n",
      " [-0.02222648]]\n",
      "t [[ 0.02230332]\n",
      " [-0.01330193]\n",
      " [-0.02086935]\n",
      " ...\n",
      " [ 0.01732265]\n",
      " [ 0.00730571]\n",
      " [-0.02949877]]\n",
      "t [[ 0.02230332]\n",
      " [-0.01330193]\n",
      " [-0.02086935]\n",
      " ...\n",
      " [ 0.01732265]\n",
      " [ 0.00730571]\n",
      " [-0.02949877]]\n",
      "Current iteration=4, loss=51407.5712858294\n",
      "t [[ 0.02778487]\n",
      " [-0.01669479]\n",
      " [-0.02602392]\n",
      " ...\n",
      " [ 0.02151731]\n",
      " [ 0.00908014]\n",
      " [-0.03670418]]\n",
      "t [[ 0.02778487]\n",
      " [-0.01669479]\n",
      " [-0.02602392]\n",
      " ...\n",
      " [ 0.02151731]\n",
      " [ 0.00908014]\n",
      " [-0.03670418]]\n",
      "t [[ 0.03322926]\n",
      " [-0.02011334]\n",
      " [-0.03115377]\n",
      " ...\n",
      " [ 0.02565889]\n",
      " [ 0.01083418]\n",
      " [-0.04384355]]\n",
      "t [[ 0.03322926]\n",
      " [-0.02011334]\n",
      " [-0.03115377]\n",
      " ...\n",
      " [ 0.02565889]\n",
      " [ 0.01083418]\n",
      " [-0.04384355]]\n",
      "Current iteration=6, loss=51148.88898778222\n",
      "t [[ 0.03863675]\n",
      " [-0.02355697]\n",
      " [-0.0362591 ]\n",
      " ...\n",
      " [ 0.02974804]\n",
      " [ 0.01256802]\n",
      " [-0.05091768]]\n",
      "t [[ 0.03863675]\n",
      " [-0.02355697]\n",
      " [-0.0362591 ]\n",
      " ...\n",
      " [ 0.02974804]\n",
      " [ 0.01256802]\n",
      " [-0.05091768]]\n",
      "t [[ 0.04400762]\n",
      " [-0.02702507]\n",
      " [-0.04134009]\n",
      " ...\n",
      " [ 0.0337854 ]\n",
      " [ 0.01428186]\n",
      " [-0.05792738]]\n",
      "t [[ 0.04400762]\n",
      " [-0.02702507]\n",
      " [-0.04134009]\n",
      " ...\n",
      " [ 0.0337854 ]\n",
      " [ 0.01428186]\n",
      " [-0.05792738]]\n",
      "Current iteration=8, loss=50895.143501190825\n",
      "t [[ 0.04934214]\n",
      " [-0.03051705]\n",
      " [-0.04639694]\n",
      " ...\n",
      " [ 0.0377716 ]\n",
      " [ 0.0159759 ]\n",
      " [-0.06487345]]\n",
      "t [[ 0.04934214]\n",
      " [-0.03051705]\n",
      " [-0.04639694]\n",
      " ...\n",
      " [ 0.0377716 ]\n",
      " [ 0.0159759 ]\n",
      " [-0.06487345]]\n",
      "t [[ 0.05464057]\n",
      " [-0.03403232]\n",
      " [-0.05142983]\n",
      " ...\n",
      " [ 0.04170726]\n",
      " [ 0.01765033]\n",
      " [-0.07175668]]\n",
      "loss=50646.20398980245\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00556075]\n",
      " [-0.00314352]\n",
      " [-0.00523571]\n",
      " ...\n",
      " [ 0.00448169]\n",
      " [ 0.00189227]\n",
      " [-0.00770822]]\n",
      "t [[ 0.00556075]\n",
      " [-0.00314352]\n",
      " [-0.00523571]\n",
      " ...\n",
      " [ 0.00448169]\n",
      " [ 0.00189227]\n",
      " [-0.00770822]]\n",
      "t [[ 0.01108374]\n",
      " [-0.0063154 ]\n",
      " [-0.01044592]\n",
      " ...\n",
      " [ 0.00890612]\n",
      " [ 0.00376309]\n",
      " [-0.01534495]]\n",
      "t [[ 0.01108374]\n",
      " [-0.0063154 ]\n",
      " [-0.01044592]\n",
      " ...\n",
      " [ 0.00890612]\n",
      " [ 0.00376309]\n",
      " [-0.01534495]]\n",
      "Current iteration=2, loss=51673.1494836519\n",
      "t [[ 0.01656926]\n",
      " [-0.00951499]\n",
      " [-0.01563082]\n",
      " ...\n",
      " [ 0.01327397]\n",
      " [ 0.00561265]\n",
      " [-0.02291105]]\n",
      "t [[ 0.01656926]\n",
      " [-0.00951499]\n",
      " [-0.01563082]\n",
      " ...\n",
      " [ 0.01327397]\n",
      " [ 0.00561265]\n",
      " [-0.02291105]]\n",
      "t [[ 0.02201757]\n",
      " [-0.01274165]\n",
      " [-0.02079063]\n",
      " ...\n",
      " [ 0.01758595]\n",
      " [ 0.00744116]\n",
      " [-0.03040739]]\n",
      "t [[ 0.02201757]\n",
      " [-0.01274165]\n",
      " [-0.02079063]\n",
      " ...\n",
      " [ 0.01758595]\n",
      " [ 0.00744116]\n",
      " [-0.03040739]]\n",
      "Current iteration=4, loss=51411.23321187365\n",
      "t [[ 0.02742894]\n",
      " [-0.01599474]\n",
      " [-0.02592552]\n",
      " ...\n",
      " [ 0.02184271]\n",
      " [ 0.00924884]\n",
      " [-0.03783483]]\n",
      "t [[ 0.02742894]\n",
      " [-0.01599474]\n",
      " [-0.02592552]\n",
      " ...\n",
      " [ 0.02184271]\n",
      " [ 0.00924884]\n",
      " [-0.03783483]]\n",
      "t [[ 0.03280365]\n",
      " [-0.01927364]\n",
      " [-0.0310357 ]\n",
      " ...\n",
      " [ 0.02604495]\n",
      " [ 0.01103587]\n",
      " [-0.0451942 ]]\n",
      "t [[ 0.03280365]\n",
      " [-0.01927364]\n",
      " [-0.0310357 ]\n",
      " ...\n",
      " [ 0.02604495]\n",
      " [ 0.01103587]\n",
      " [-0.0451942 ]]\n",
      "Current iteration=6, loss=51154.40059192433\n",
      "t [[ 0.03814195]\n",
      " [-0.02257773]\n",
      " [-0.03612137]\n",
      " ...\n",
      " [ 0.03019331]\n",
      " [ 0.01280246]\n",
      " [-0.05248635]]\n",
      "t [[ 0.03814195]\n",
      " [-0.02257773]\n",
      " [-0.03612137]\n",
      " ...\n",
      " [ 0.03019331]\n",
      " [ 0.01280246]\n",
      " [-0.05248635]]\n",
      "t [[ 0.04344413]\n",
      " [-0.02590641]\n",
      " [-0.0411827 ]\n",
      " ...\n",
      " [ 0.03428847]\n",
      " [ 0.01454882]\n",
      " [-0.0597121 ]]\n",
      "t [[ 0.04344413]\n",
      " [-0.02590641]\n",
      " [-0.0411827 ]\n",
      " ...\n",
      " [ 0.03428847]\n",
      " [ 0.01454882]\n",
      " [-0.0597121 ]]\n",
      "Current iteration=8, loss=50902.51464357138\n",
      "t [[ 0.04871044]\n",
      " [-0.02925907]\n",
      " [-0.0462199 ]\n",
      " ...\n",
      " [ 0.03833106]\n",
      " [ 0.01627513]\n",
      " [-0.06687227]]\n",
      "t [[ 0.04871044]\n",
      " [-0.02925907]\n",
      " [-0.0462199 ]\n",
      " ...\n",
      " [ 0.03833106]\n",
      " [ 0.01627513]\n",
      " [-0.06687227]]\n",
      "t [[ 0.05394115]\n",
      " [-0.03263512]\n",
      " [-0.05123315]\n",
      " ...\n",
      " [ 0.04232174]\n",
      " [ 0.0179816 ]\n",
      " [-0.07396768]]\n",
      "loss=50655.442707778784\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.00555185]\n",
      " [-0.00330258]\n",
      " [-0.00521926]\n",
      " ...\n",
      " [ 0.0079964 ]\n",
      " [-0.00521926]\n",
      " [ 0.00748977]]\n",
      "t [[ 0.00555185]\n",
      " [-0.00330258]\n",
      " [-0.00521926]\n",
      " ...\n",
      " [ 0.0079964 ]\n",
      " [-0.00521926]\n",
      " [ 0.00748977]]\n",
      "t [[ 0.01106601]\n",
      " [-0.00663198]\n",
      " [-0.01041325]\n",
      " ...\n",
      " [ 0.0159318 ]\n",
      " [-0.01041325]\n",
      " [ 0.01491251]]\n",
      "t [[ 0.01106601]\n",
      " [-0.00663198]\n",
      " [-0.01041325]\n",
      " ...\n",
      " [ 0.0159318 ]\n",
      " [-0.01041325]\n",
      " [ 0.01491251]]\n",
      "Current iteration=2, loss=51673.74669171749\n",
      "t [[ 0.01654277]\n",
      " [-0.00998754]\n",
      " [-0.01558215]\n",
      " ...\n",
      " [ 0.02380672]\n",
      " [-0.01558215]\n",
      " [ 0.02226889]]\n",
      "t [[ 0.01654277]\n",
      " [-0.00998754]\n",
      " [-0.01558215]\n",
      " ...\n",
      " [ 0.02380672]\n",
      " [-0.01558215]\n",
      " [ 0.02226889]]\n",
      "t [[ 0.0219824 ]\n",
      " [-0.01336867]\n",
      " [-0.02072616]\n",
      " ...\n",
      " [ 0.03162169]\n",
      " [-0.02072616]\n",
      " [ 0.02955955]]\n",
      "t [[ 0.0219824 ]\n",
      " [-0.01336867]\n",
      " [-0.02072616]\n",
      " ...\n",
      " [ 0.03162169]\n",
      " [-0.02072616]\n",
      " [ 0.02955955]]\n",
      "Current iteration=4, loss=51412.371541760534\n",
      "t [[ 0.02738517]\n",
      " [-0.01677473]\n",
      " [-0.02584548]\n",
      " ...\n",
      " [ 0.03937721]\n",
      " [-0.02584548]\n",
      " [ 0.03678516]]\n",
      "t [[ 0.02738517]\n",
      " [-0.01677473]\n",
      " [-0.02584548]\n",
      " ...\n",
      " [ 0.03937721]\n",
      " [-0.02584548]\n",
      " [ 0.03678516]]\n",
      "t [[ 0.03275135]\n",
      " [-0.02020512]\n",
      " [-0.03094029]\n",
      " ...\n",
      " [ 0.0470738 ]\n",
      " [-0.03094029]\n",
      " [ 0.04394636]]\n",
      "t [[ 0.03275135]\n",
      " [-0.02020512]\n",
      " [-0.03094029]\n",
      " ...\n",
      " [ 0.0470738 ]\n",
      " [-0.03094029]\n",
      " [ 0.04394636]]\n",
      "Current iteration=6, loss=51156.02701117622\n",
      "t [[ 0.03808122]\n",
      " [-0.02365924]\n",
      " [-0.03601079]\n",
      " ...\n",
      " [ 0.05471198]\n",
      " [-0.03601079]\n",
      " [ 0.05104378]]\n",
      "t [[ 0.03808122]\n",
      " [-0.02365924]\n",
      " [-0.03601079]\n",
      " ...\n",
      " [ 0.05471198]\n",
      " [-0.03601079]\n",
      " [ 0.05104378]]\n",
      "t [[ 0.04337504]\n",
      " [-0.02713652]\n",
      " [-0.04105716]\n",
      " ...\n",
      " [ 0.06229225]\n",
      " [-0.04105716]\n",
      " [ 0.05807806]]\n",
      "t [[ 0.04337504]\n",
      " [-0.02713652]\n",
      " [-0.04105716]\n",
      " ...\n",
      " [ 0.06229225]\n",
      " [-0.04105716]\n",
      " [ 0.05807806]]\n",
      "Current iteration=8, loss=50904.579030813686\n",
      "t [[ 0.04863308]\n",
      " [-0.03063635]\n",
      " [-0.0460796 ]\n",
      " ...\n",
      " [ 0.06981513]\n",
      " [-0.0460796 ]\n",
      " [ 0.06504983]]\n",
      "t [[ 0.04863308]\n",
      " [-0.03063635]\n",
      " [-0.0460796 ]\n",
      " ...\n",
      " [ 0.06981513]\n",
      " [-0.0460796 ]\n",
      " [ 0.06504983]]\n",
      "t [[ 0.05385562]\n",
      " [-0.03415818]\n",
      " [-0.05107828]\n",
      " ...\n",
      " [ 0.0772811 ]\n",
      " [-0.05107828]\n",
      " [ 0.07195971]]\n",
      "loss=50657.897715290695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0037307 ]\n",
      " [-0.01786446]\n",
      " [-0.01047592]\n",
      " ...\n",
      " [ 0.00895831]\n",
      " [ 0.0037307 ]\n",
      " [-0.01518537]]\n",
      "t [[ 0.0037307 ]\n",
      " [-0.01786446]\n",
      " [-0.01047592]\n",
      " ...\n",
      " [ 0.00895831]\n",
      " [ 0.0037307 ]\n",
      " [-0.01518537]]\n",
      "t [[ 0.00737687]\n",
      " [-0.03559523]\n",
      " [-0.02085093]\n",
      " ...\n",
      " [ 0.01769206]\n",
      " [ 0.00737687]\n",
      " [-0.03009301]]\n",
      "t [[ 0.00737687]\n",
      " [-0.03559523]\n",
      " [-0.02085093]\n",
      " ...\n",
      " [ 0.01769206]\n",
      " [ 0.00737687]\n",
      " [-0.03009301]]\n",
      "Current iteration=2, loss=51407.64740611778\n",
      "t [[ 0.01094014]\n",
      " [-0.05319301]\n",
      " [-0.03112658]\n",
      " ...\n",
      " [ 0.0262067 ]\n",
      " [ 0.01094014]\n",
      " [-0.04472969]]\n",
      "t [[ 0.01094014]\n",
      " [-0.05319301]\n",
      " [-0.03112658]\n",
      " ...\n",
      " [ 0.0262067 ]\n",
      " [ 0.01094014]\n",
      " [-0.04472969]]\n",
      "t [[ 0.01442211]\n",
      " [-0.07065854]\n",
      " [-0.0413044 ]\n",
      " ...\n",
      " [ 0.03450751]\n",
      " [ 0.01442211]\n",
      " [-0.05910203]]\n",
      "t [[ 0.01442211]\n",
      " [-0.07065854]\n",
      " [-0.0413044 ]\n",
      " ...\n",
      " [ 0.03450751]\n",
      " [ 0.01442211]\n",
      " [-0.05910203]]\n",
      "Current iteration=4, loss=50895.2939849141\n",
      "t [[ 0.01782437]\n",
      " [-0.08799256]\n",
      " [-0.05138593]\n",
      " ...\n",
      " [ 0.04259971]\n",
      " [ 0.01782437]\n",
      " [-0.07321653]]\n",
      "t [[ 0.01782437]\n",
      " [-0.08799256]\n",
      " [-0.05138593]\n",
      " ...\n",
      " [ 0.04259971]\n",
      " [ 0.01782437]\n",
      " [-0.07321653]]\n",
      "t [[ 0.02114848]\n",
      " [-0.10519585]\n",
      " [-0.06137266]\n",
      " ...\n",
      " [ 0.05048838]\n",
      " [ 0.02114848]\n",
      " [-0.08707953]]\n",
      "t [[ 0.02114848]\n",
      " [-0.10519585]\n",
      " [-0.06137266]\n",
      " ...\n",
      " [ 0.05048838]\n",
      " [ 0.02114848]\n",
      " [-0.08707953]]\n",
      "Current iteration=6, loss=50402.16634832396\n",
      "t [[ 0.02439597]\n",
      " [-0.12226918]\n",
      " [-0.07126607]\n",
      " ...\n",
      " [ 0.0581785 ]\n",
      " [ 0.02439597]\n",
      " [-0.10069724]]\n",
      "t [[ 0.02439597]\n",
      " [-0.12226918]\n",
      " [-0.07126607]\n",
      " ...\n",
      " [ 0.0581785 ]\n",
      " [ 0.02439597]\n",
      " [-0.10069724]]\n",
      "t [[ 0.02756836]\n",
      " [-0.13921337]\n",
      " [-0.08106762]\n",
      " ...\n",
      " [ 0.06567493]\n",
      " [ 0.02756836]\n",
      " [-0.11407574]]\n",
      "t [[ 0.02756836]\n",
      " [-0.13921337]\n",
      " [-0.08106762]\n",
      " ...\n",
      " [ 0.06567493]\n",
      " [ 0.02756836]\n",
      " [-0.11407574]]\n",
      "Current iteration=8, loss=49927.2659312359\n",
      "t [[ 0.03066714]\n",
      " [-0.15602924]\n",
      " [-0.09077873]\n",
      " ...\n",
      " [ 0.07298243]\n",
      " [ 0.03066714]\n",
      " [-0.12722096]]\n",
      "t [[ 0.03066714]\n",
      " [-0.15602924]\n",
      " [-0.09077873]\n",
      " ...\n",
      " [ 0.07298243]\n",
      " [ 0.03066714]\n",
      " [-0.12722096]]\n",
      "t [[ 0.03369378]\n",
      " [-0.17271762]\n",
      " [-0.10040082]\n",
      " ...\n",
      " [ 0.08010563]\n",
      " [ 0.03369378]\n",
      " [-0.14013869]]\n",
      "loss=49469.65603080118\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01126589]\n",
      " [-0.00656756]\n",
      " [-0.01051078]\n",
      " ...\n",
      " [ 0.00882719]\n",
      " [ 0.00371605]\n",
      " [-0.01495588]]\n",
      "t [[ 0.01126589]\n",
      " [-0.00656756]\n",
      " [-0.01051078]\n",
      " ...\n",
      " [ 0.00882719]\n",
      " [ 0.00371605]\n",
      " [-0.01495588]]\n",
      "t [[ 0.02237874]\n",
      " [-0.01324805]\n",
      " [-0.02091956]\n",
      " ...\n",
      " [ 0.01743143]\n",
      " [ 0.0073473 ]\n",
      " [-0.02963418]]\n",
      "t [[ 0.02237874]\n",
      " [-0.01324805]\n",
      " [-0.02091956]\n",
      " ...\n",
      " [ 0.01743143]\n",
      " [ 0.0073473 ]\n",
      " [-0.02963418]]\n",
      "Current iteration=2, loss=51406.29473412025\n",
      "t [[ 0.03334078]\n",
      " [-0.02003627]\n",
      " [-0.03122794]\n",
      " ...\n",
      " [ 0.02581815]\n",
      " [ 0.01089537]\n",
      " [-0.04404171]]\n",
      "t [[ 0.03334078]\n",
      " [-0.02003627]\n",
      " [-0.03122794]\n",
      " ...\n",
      " [ 0.02581815]\n",
      " [ 0.01089537]\n",
      " [-0.04404171]]\n",
      "t [[ 0.04415418]\n",
      " [-0.02692717]\n",
      " [-0.04143746]\n",
      " ...\n",
      " [ 0.03399262]\n",
      " [ 0.01436189]\n",
      " [-0.05818517]]\n",
      "t [[ 0.04415418]\n",
      " [-0.02692717]\n",
      " [-0.04143746]\n",
      " ...\n",
      " [ 0.03399262]\n",
      " [ 0.01436189]\n",
      " [-0.05818517]]\n",
      "Current iteration=4, loss=50892.72340100409\n",
      "t [[ 0.05482114]\n",
      " [-0.03391583]\n",
      " [-0.05154969]\n",
      " ...\n",
      " [ 0.04196005]\n",
      " [ 0.01774844]\n",
      " [-0.07207107]]\n",
      "t [[ 0.05482114]\n",
      " [-0.03391583]\n",
      " [-0.05154969]\n",
      " ...\n",
      " [ 0.04196005]\n",
      " [ 0.01774844]\n",
      " [-0.07207107]]\n",
      "t [[ 0.06534384]\n",
      " [-0.04099749]\n",
      " [-0.06156613]\n",
      " ...\n",
      " [ 0.04972551]\n",
      " [ 0.02105661]\n",
      " [-0.08570582]]\n",
      "t [[ 0.06534384]\n",
      " [-0.04099749]\n",
      " [-0.06156613]\n",
      " ...\n",
      " [ 0.04972551]\n",
      " [ 0.02105661]\n",
      " [-0.08570582]]\n",
      "Current iteration=6, loss=50398.49959959005\n",
      "t [[ 0.07572441]\n",
      " [-0.04816748]\n",
      " [-0.0714883 ]\n",
      " ...\n",
      " [ 0.05729395]\n",
      " [ 0.02428793]\n",
      " [-0.09909567]]\n",
      "t [[ 0.07572441]\n",
      " [-0.04816748]\n",
      " [-0.0714883 ]\n",
      " ...\n",
      " [ 0.05729395]\n",
      " [ 0.02428793]\n",
      " [-0.09909567]]\n",
      "t [[ 0.08596502]\n",
      " [-0.05542132]\n",
      " [-0.08131766]\n",
      " ...\n",
      " [ 0.06467023]\n",
      " [ 0.02744393]\n",
      " [-0.11224674]]\n",
      "t [[ 0.08596502]\n",
      " [-0.05542132]\n",
      " [-0.08131766]\n",
      " ...\n",
      " [ 0.06467023]\n",
      " [ 0.02744393]\n",
      " [-0.11224674]]\n",
      "Current iteration=8, loss=49922.612919930776\n",
      "t [[ 0.09606777]\n",
      " [-0.06275463]\n",
      " [-0.09105568]\n",
      " ...\n",
      " [ 0.07185909]\n",
      " [ 0.03052611]\n",
      " [-0.12516498]]\n",
      "t [[ 0.09606777]\n",
      " [-0.06275463]\n",
      " [-0.09105568]\n",
      " ...\n",
      " [ 0.07185909]\n",
      " [ 0.03052611]\n",
      " [-0.12516498]]\n",
      "t [[ 0.10603476]\n",
      " [-0.07016316]\n",
      " [-0.1007038 ]\n",
      " ...\n",
      " [ 0.07886514]\n",
      " [ 0.03353595]\n",
      " [-0.13785624]]\n",
      "loss=49464.11590500273\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01112149]\n",
      " [-0.00628704]\n",
      " [-0.01047141]\n",
      " ...\n",
      " [ 0.00896338]\n",
      " [ 0.00378455]\n",
      " [-0.01541644]]\n",
      "t [[ 0.01112149]\n",
      " [-0.00628704]\n",
      " [-0.01047141]\n",
      " ...\n",
      " [ 0.00896338]\n",
      " [ 0.00378455]\n",
      " [-0.01541644]]\n",
      "t [[ 0.02209199]\n",
      " [-0.01268752]\n",
      " [-0.02084083]\n",
      " ...\n",
      " [ 0.01769771]\n",
      " [ 0.00748326]\n",
      " [-0.03054691]]\n",
      "t [[ 0.02209199]\n",
      " [-0.01268752]\n",
      " [-0.02084083]\n",
      " ...\n",
      " [ 0.01769771]\n",
      " [ 0.00748326]\n",
      " [-0.03054691]]\n",
      "Current iteration=2, loss=51409.953588081305\n",
      "t [[ 0.03291367]\n",
      " [-0.01919622]\n",
      " [-0.03110985]\n",
      " ...\n",
      " [ 0.02620856]\n",
      " [ 0.0110978 ]\n",
      " [-0.0453984 ]]\n",
      "t [[ 0.03291367]\n",
      " [-0.01919622]\n",
      " [-0.03110985]\n",
      " ...\n",
      " [ 0.02620856]\n",
      " [ 0.0110978 ]\n",
      " [-0.0453984 ]]\n",
      "t [[ 0.04358872]\n",
      " [-0.02580806]\n",
      " [-0.04128005]\n",
      " ...\n",
      " [ 0.03450137]\n",
      " [ 0.01462981]\n",
      " [-0.05997777]]\n",
      "t [[ 0.04358872]\n",
      " [-0.02580806]\n",
      " [-0.04128005]\n",
      " ...\n",
      " [ 0.03450137]\n",
      " [ 0.01462981]\n",
      " [-0.05997777]]\n",
      "Current iteration=4, loss=50900.090290211665\n",
      "t [[ 0.05411929]\n",
      " [-0.03251811]\n",
      " [-0.05135296]\n",
      " ...\n",
      " [ 0.04258146]\n",
      " [ 0.01808089]\n",
      " [-0.07429172]]\n",
      "t [[ 0.05411929]\n",
      " [-0.03251811]\n",
      " [-0.05135296]\n",
      " ...\n",
      " [ 0.04258146]\n",
      " [ 0.01808089]\n",
      " [-0.07429172]]\n",
      "t [[ 0.06450753]\n",
      " [-0.03932156]\n",
      " [-0.06133014]\n",
      " ...\n",
      " [ 0.05045407]\n",
      " [ 0.02145265]\n",
      " [-0.08834682]]\n",
      "t [[ 0.06450753]\n",
      " [-0.03932156]\n",
      " [-0.06133014]\n",
      " ...\n",
      " [ 0.05045407]\n",
      " [ 0.02145265]\n",
      " [-0.08834682]]\n",
      "Current iteration=6, loss=50409.60833437936\n",
      "t [[ 0.07475558]\n",
      " [-0.04621376]\n",
      " [-0.07121309]\n",
      " ...\n",
      " [ 0.0581243 ]\n",
      " [ 0.02474665]\n",
      " [-0.10214951]]\n",
      "t [[ 0.07475558]\n",
      " [-0.04621376]\n",
      " [-0.07121309]\n",
      " ...\n",
      " [ 0.0581243 ]\n",
      " [ 0.02474665]\n",
      " [-0.10214951]]\n",
      "t [[ 0.08486555]\n",
      " [-0.05319019]\n",
      " [-0.08100329]\n",
      " ...\n",
      " [ 0.06559714]\n",
      " [ 0.02796444]\n",
      " [-0.11570606]]\n",
      "t [[ 0.08486555]\n",
      " [-0.05319019]\n",
      " [-0.08100329]\n",
      " ...\n",
      " [ 0.06559714]\n",
      " [ 0.02796444]\n",
      " [-0.11570606]]\n",
      "Current iteration=8, loss=49937.48370033768\n",
      "t [[ 0.09483953]\n",
      " [-0.06024646]\n",
      " [-0.09070221]\n",
      " ...\n",
      " [ 0.07287749]\n",
      " [ 0.03110753]\n",
      " [-0.12902261]]\n",
      "t [[ 0.09483953]\n",
      " [-0.06024646]\n",
      " [-0.09070221]\n",
      " ...\n",
      " [ 0.07287749]\n",
      " [ 0.03110753]\n",
      " [-0.12902261]]\n",
      "t [[ 0.10467961]\n",
      " [-0.06737832]\n",
      " [-0.10031129]\n",
      " ...\n",
      " [ 0.07997009]\n",
      " [ 0.03417742]\n",
      " [-0.14210515]]\n",
      "loss=49482.756956635756\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01110369]\n",
      " [-0.00660517]\n",
      " [-0.01043852]\n",
      " ...\n",
      " [ 0.01599281]\n",
      " [-0.01043852]\n",
      " [ 0.01497953]]\n",
      "t [[ 0.01110369]\n",
      " [-0.00660517]\n",
      " [-0.01043852]\n",
      " ...\n",
      " [ 0.01599281]\n",
      " [-0.01043852]\n",
      " [ 0.01497953]]\n",
      "t [[ 0.02205666]\n",
      " [-0.01331757]\n",
      " [-0.02077594]\n",
      " ...\n",
      " [ 0.03174161]\n",
      " [-0.02077594]\n",
      " [ 0.02969097]]\n",
      "t [[ 0.02205666]\n",
      " [-0.01331757]\n",
      " [-0.02077594]\n",
      " ...\n",
      " [ 0.03174161]\n",
      " [-0.02077594]\n",
      " [ 0.02969097]]\n",
      "Current iteration=2, loss=51411.10536302588\n",
      "t [[ 0.03286113]\n",
      " [-0.02013213]\n",
      " [-0.03101382]\n",
      " ...\n",
      " [ 0.04725061]\n",
      " [-0.03101382]\n",
      " [ 0.04413963]]\n",
      "t [[ 0.03286113]\n",
      " [-0.02013213]\n",
      " [-0.03101382]\n",
      " ...\n",
      " [ 0.04725061]\n",
      " [-0.03101382]\n",
      " [ 0.04413963]]\n",
      "t [[ 0.04351928]\n",
      " [-0.02704394]\n",
      " [-0.0411537 ]\n",
      " ...\n",
      " [ 0.06252397]\n",
      " [-0.0411537 ]\n",
      " [ 0.05833071]]\n",
      "t [[ 0.04351928]\n",
      " [-0.02704394]\n",
      " [-0.0411537 ]\n",
      " ...\n",
      " [ 0.06252397]\n",
      " [-0.0411537 ]\n",
      " [ 0.05833071]]\n",
      "Current iteration=4, loss=50902.17872414069\n",
      "t [[ 0.0540333 ]\n",
      " [-0.03404819]\n",
      " [-0.05119712]\n",
      " ...\n",
      " [ 0.0775658 ]\n",
      " [-0.05119712]\n",
      " [ 0.07226935]]\n",
      "t [[ 0.0540333 ]\n",
      " [-0.03404819]\n",
      " [-0.05119712]\n",
      " ...\n",
      " [ 0.0775658 ]\n",
      " [-0.05119712]\n",
      " [ 0.07226935]]\n",
      "t [[ 0.06440534]\n",
      " [-0.04114026]\n",
      " [-0.06114557]\n",
      " ...\n",
      " [ 0.09238015]\n",
      " [-0.06114557]\n",
      " [ 0.08596061]]\n",
      "t [[ 0.06440534]\n",
      " [-0.04114026]\n",
      " [-0.06114557]\n",
      " ...\n",
      " [ 0.09238015]\n",
      " [-0.06114557]\n",
      " [ 0.08596061]]\n",
      "Current iteration=6, loss=50412.44149512727\n",
      "t [[ 0.07463755]\n",
      " [-0.04831562]\n",
      " [-0.07100054]\n",
      " ...\n",
      " [ 0.10697103]\n",
      " [-0.07100054]\n",
      " [ 0.09940947]]\n",
      "t [[ 0.07463755]\n",
      " [-0.04831562]\n",
      " [-0.07100054]\n",
      " ...\n",
      " [ 0.10697103]\n",
      " [-0.07100054]\n",
      " [ 0.09940947]]\n",
      "t [[ 0.08473205]\n",
      " [-0.0555699 ]\n",
      " [-0.0807635 ]\n",
      " ...\n",
      " [ 0.12134239]\n",
      " [-0.0807635 ]\n",
      " [ 0.11262081]]\n",
      "t [[ 0.08473205]\n",
      " [-0.0555699 ]\n",
      " [-0.0807635 ]\n",
      " ...\n",
      " [ 0.12134239]\n",
      " [-0.0807635 ]\n",
      " [ 0.11262081]]\n",
      "Current iteration=8, loss=49940.890668424654\n",
      "t [[ 0.09469094]\n",
      " [-0.06289887]\n",
      " [-0.09043588]\n",
      " ...\n",
      " [ 0.13549809]\n",
      " [-0.09043588]\n",
      " [ 0.12559944]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.09469094]\n",
      " [-0.06289887]\n",
      " [-0.09043588]\n",
      " ...\n",
      " [ 0.13549809]\n",
      " [-0.09043588]\n",
      " [ 0.12559944]]\n",
      "t [[ 0.10451629]\n",
      " [-0.07029841]\n",
      " [-0.10001911]\n",
      " ...\n",
      " [ 0.14944199]\n",
      " [-0.10001911]\n",
      " [ 0.13835007]]\n",
      "loss=49486.58582960366\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00559605]\n",
      " [-0.02679669]\n",
      " [-0.01571389]\n",
      " ...\n",
      " [ 0.01343746]\n",
      " [ 0.00559605]\n",
      " [-0.02277805]]\n",
      "t [[ 0.00559605]\n",
      " [-0.02679669]\n",
      " [-0.01571389]\n",
      " ...\n",
      " [ 0.01343746]\n",
      " [ 0.00559605]\n",
      " [-0.02277805]]\n",
      "t [[ 0.01100192]\n",
      " [-0.05329258]\n",
      " [-0.03120071]\n",
      " ...\n",
      " [ 0.0263697 ]\n",
      " [ 0.01100192]\n",
      " [-0.04493124]]\n",
      "t [[ 0.01100192]\n",
      " [-0.05329258]\n",
      " [-0.03120071]\n",
      " ...\n",
      " [ 0.0263697 ]\n",
      " [ 0.01100192]\n",
      " [-0.04493124]]\n",
      "Current iteration=2, loss=51147.13448441545\n",
      "t [[ 0.01622311]\n",
      " [-0.07949009]\n",
      " [-0.04646577]\n",
      " ...\n",
      " [ 0.03881508]\n",
      " [ 0.01622311]\n",
      " [-0.06648247]]\n",
      "t [[ 0.01622311]\n",
      " [-0.07949009]\n",
      " [-0.04646577]\n",
      " ...\n",
      " [ 0.03881508]\n",
      " [ 0.01622311]\n",
      " [-0.06648247]]\n",
      "t [[ 0.02126504]\n",
      " [-0.10539172]\n",
      " [-0.06151424]\n",
      " ...\n",
      " [ 0.05079136]\n",
      " [ 0.02126504]\n",
      " [-0.08745393]]\n",
      "t [[ 0.02126504]\n",
      " [-0.10539172]\n",
      " [-0.06151424]\n",
      " ...\n",
      " [ 0.05079136]\n",
      " [ 0.02126504]\n",
      " [-0.08745393]]\n",
      "Current iteration=4, loss=50398.71526095763\n",
      "t [[ 0.02613298]\n",
      " [-0.1310001 ]\n",
      " [-0.07635119]\n",
      " ...\n",
      " [ 0.06231575]\n",
      " [ 0.02613298]\n",
      " [-0.1078671 ]]\n",
      "t [[ 0.02613298]\n",
      " [-0.1310001 ]\n",
      " [-0.07635119]\n",
      " ...\n",
      " [ 0.06231575]\n",
      " [ 0.02613298]\n",
      " [-0.1078671 ]]\n",
      "t [[ 0.0308321 ]\n",
      " [-0.15631794]\n",
      " [-0.09098161]\n",
      " ...\n",
      " [ 0.07340487]\n",
      " [ 0.0308321 ]\n",
      " [-0.12774271]]\n",
      "t [[ 0.0308321 ]\n",
      " [-0.15631794]\n",
      " [-0.09098161]\n",
      " ...\n",
      " [ 0.07340487]\n",
      " [ 0.0308321 ]\n",
      " [-0.12774271]]\n",
      "Current iteration=6, loss=49691.56442517912\n",
      "t [[ 0.03536743]\n",
      " [-0.18134803]\n",
      " [-0.10541032]\n",
      " ...\n",
      " [ 0.08407475]\n",
      " [ 0.03536743]\n",
      " [-0.1471008 ]]\n",
      "t [[ 0.03536743]\n",
      " [-0.18134803]\n",
      " [-0.10541032]\n",
      " ...\n",
      " [ 0.08407475]\n",
      " [ 0.03536743]\n",
      " [-0.1471008 ]]\n",
      "t [[ 0.03974389]\n",
      " [-0.20609324]\n",
      " [-0.11964208]\n",
      " ...\n",
      " [ 0.09434086]\n",
      " [ 0.03974389]\n",
      " [-0.16596068]]\n",
      "t [[ 0.03974389]\n",
      " [-0.20609324]\n",
      " [-0.11964208]\n",
      " ...\n",
      " [ 0.09434086]\n",
      " [ 0.03974389]\n",
      " [-0.16596068]]\n",
      "Current iteration=8, loss=49022.53277985993\n",
      "t [[ 0.04396626]\n",
      " [-0.23055651]\n",
      " [-0.1336815 ]\n",
      " ...\n",
      " [ 0.10421809]\n",
      " [ 0.04396626]\n",
      " [-0.18434095]]\n",
      "t [[ 0.04396626]\n",
      " [-0.23055651]\n",
      " [-0.1336815 ]\n",
      " ...\n",
      " [ 0.10421809]\n",
      " [ 0.04396626]\n",
      " [-0.18434095]]\n",
      "t [[ 0.0480392 ]\n",
      " [-0.25474083]\n",
      " [-0.14753308]\n",
      " ...\n",
      " [ 0.11372076]\n",
      " [ 0.0480392 ]\n",
      " [-0.20225952]]\n",
      "loss=48388.76160986588\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01689883]\n",
      " [-0.00985134]\n",
      " [-0.01576617]\n",
      " ...\n",
      " [ 0.01324078]\n",
      " [ 0.00557408]\n",
      " [-0.02243381]]\n",
      "t [[ 0.01689883]\n",
      " [-0.00985134]\n",
      " [-0.01576617]\n",
      " ...\n",
      " [ 0.01324078]\n",
      " [ 0.00557408]\n",
      " [-0.02243381]]\n",
      "t [[ 0.03345336]\n",
      " [-0.01995676]\n",
      " [-0.03130286]\n",
      " ...\n",
      " [ 0.02597996]\n",
      " [ 0.01095735]\n",
      " [-0.04424311]]\n",
      "t [[ 0.03345336]\n",
      " [-0.01995676]\n",
      " [-0.03130286]\n",
      " ...\n",
      " [ 0.02597996]\n",
      " [ 0.01095735]\n",
      " [-0.04424311]]\n",
      "Current iteration=2, loss=51145.14464651521\n",
      "t [[ 0.04967106]\n",
      " [-0.03029868]\n",
      " [-0.04661543]\n",
      " ...\n",
      " [ 0.03823585]\n",
      " [ 0.01615535]\n",
      " [-0.06545095]]\n",
      "t [[ 0.04967106]\n",
      " [-0.03029868]\n",
      " [-0.04661543]\n",
      " ...\n",
      " [ 0.03823585]\n",
      " [ 0.01615535]\n",
      " [-0.06545095]]\n",
      "t [[ 0.0655594 ]\n",
      " [-0.04086028]\n",
      " [-0.06170917]\n",
      " ...\n",
      " [ 0.05002616]\n",
      " [ 0.02117353]\n",
      " [-0.08607969]]\n",
      "t [[ 0.0655594 ]\n",
      " [-0.04086028]\n",
      " [-0.06170917]\n",
      " ...\n",
      " [ 0.05002616]\n",
      " [ 0.02117353]\n",
      " [-0.08607969]]\n",
      "Current iteration=4, loss=50395.02754296204\n",
      "t [[ 0.08112571]\n",
      " [-0.0516254 ]\n",
      " [-0.07658921]\n",
      " ...\n",
      " [ 0.06136807]\n",
      " [ 0.02601719]\n",
      " [-0.10615096]]\n",
      "t [[ 0.08112571]\n",
      " [-0.0516254 ]\n",
      " [-0.07658921]\n",
      " ...\n",
      " [ 0.06136807]\n",
      " [ 0.02601719]\n",
      " [-0.10615096]]\n",
      "t [[ 0.09637729]\n",
      " [-0.06257863]\n",
      " [-0.09126061]\n",
      " ...\n",
      " [ 0.07227812]\n",
      " [ 0.03069153]\n",
      " [-0.12568565]]\n",
      "t [[ 0.09637729]\n",
      " [-0.06257863]\n",
      " [-0.09126061]\n",
      " ...\n",
      " [ 0.07227812]\n",
      " [ 0.03069153]\n",
      " [-0.12568565]]\n",
      "Current iteration=6, loss=49686.42909706644\n",
      "t [[ 0.11132132]\n",
      " [-0.07370522]\n",
      " [-0.10572829]\n",
      " ...\n",
      " [ 0.08277232]\n",
      " [ 0.03520164]\n",
      " [-0.14470392]]\n",
      "t [[ 0.11132132]\n",
      " [-0.07370522]\n",
      " [-0.10572829]\n",
      " ...\n",
      " [ 0.08277232]\n",
      " [ 0.03520164]\n",
      " [-0.14470392]]\n",
      "t [[ 0.12596487]\n",
      " [-0.08499111]\n",
      " [-0.11999705]\n",
      " ...\n",
      " [ 0.09286606]\n",
      " [ 0.03955243]\n",
      " [-0.1632252 ]]\n",
      "t [[ 0.12596487]\n",
      " [-0.08499111]\n",
      " [-0.11999705]\n",
      " ...\n",
      " [ 0.09286606]\n",
      " [ 0.03955243]\n",
      " [-0.1632252 ]]\n",
      "Current iteration=8, loss=49016.16406248758\n",
      "t [[ 0.14031491]\n",
      " [-0.09642289]\n",
      " [-0.1340716 ]\n",
      " ...\n",
      " [ 0.10257418]\n",
      " [ 0.04374874]\n",
      " [-0.18126822]]\n",
      "t [[ 0.14031491]\n",
      " [-0.09642289]\n",
      " [-0.1340716 ]\n",
      " ...\n",
      " [ 0.10257418]\n",
      " [ 0.04374874]\n",
      " [-0.18126822]]\n",
      "t [[ 0.1543783 ]\n",
      " [-0.10798781]\n",
      " [-0.1479565 ]\n",
      " ...\n",
      " [ 0.11191095]\n",
      " [ 0.04779524]\n",
      " [-0.198851  ]]\n",
      "loss=48381.342702145754\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01668224]\n",
      " [-0.00943057]\n",
      " [-0.01570712]\n",
      " ...\n",
      " [ 0.01344507]\n",
      " [ 0.00567682]\n",
      " [-0.02312466]]\n",
      "t [[ 0.01668224]\n",
      " [-0.00943057]\n",
      " [-0.01570712]\n",
      " ...\n",
      " [ 0.01344507]\n",
      " [ 0.00567682]\n",
      " [-0.02312466]]\n",
      "t [[ 0.03302475]\n",
      " [-0.01911635]\n",
      " [-0.03118476]\n",
      " ...\n",
      " [ 0.02637481]\n",
      " [ 0.01116052]\n",
      " [-0.04560591]]\n",
      "t [[ 0.03302475]\n",
      " [-0.01911635]\n",
      " [-0.03118476]\n",
      " ...\n",
      " [ 0.02637481]\n",
      " [ 0.01116052]\n",
      " [-0.04560591]]\n",
      "Current iteration=2, loss=51150.648271306796\n",
      "t [[ 0.04903494]\n",
      " [-0.02903972]\n",
      " [-0.04643833]\n",
      " ...\n",
      " [ 0.03880803]\n",
      " [ 0.01645673]\n",
      " [-0.06746743]]\n",
      "t [[ 0.04903494]\n",
      " [-0.02903972]\n",
      " [-0.04643833]\n",
      " ...\n",
      " [ 0.03880803]\n",
      " [ 0.01645673]\n",
      " [-0.06746743]]\n",
      "t [[ 0.06472018]\n",
      " [-0.03918373]\n",
      " [-0.06147312]\n",
      " ...\n",
      " [ 0.05076295]\n",
      " [ 0.02157097]\n",
      " [-0.08873217]]\n",
      "t [[ 0.06472018]\n",
      " [-0.03918373]\n",
      " [-0.06147312]\n",
      " ...\n",
      " [ 0.05076295]\n",
      " [ 0.02157097]\n",
      " [-0.08873217]]\n",
      "Current iteration=4, loss=50406.1320536689\n",
      "t [[ 0.08008775]\n",
      " [-0.04953221]\n",
      " [-0.07629431]\n",
      " ...\n",
      " [ 0.06225725]\n",
      " [ 0.02650861]\n",
      " [-0.10942234]]\n",
      "t [[ 0.08008775]\n",
      " [-0.04953221]\n",
      " [-0.07629431]\n",
      " ...\n",
      " [ 0.06225725]\n",
      " [ 0.02650861]\n",
      " [-0.10942234]]\n",
      "t [[ 0.09514484]\n",
      " [-0.06006965]\n",
      " [-0.09090699]\n",
      " ...\n",
      " [ 0.07330797]\n",
      " [ 0.03127492]\n",
      " [-0.12955942]]\n",
      "t [[ 0.09514484]\n",
      " [-0.06006965]\n",
      " [-0.09090699]\n",
      " ...\n",
      " [ 0.07330797]\n",
      " [ 0.03127492]\n",
      " [-0.12955942]]\n",
      "Current iteration=6, loss=49703.183243552434\n",
      "t [[ 0.10989855]\n",
      " [-0.07078127]\n",
      " [-0.1053161 ]\n",
      " ...\n",
      " [ 0.0839316 ]\n",
      " [ 0.03587504]\n",
      " [-0.14916413]]\n",
      "t [[ 0.10989855]\n",
      " [-0.07078127]\n",
      " [-0.1053161 ]\n",
      " ...\n",
      " [ 0.0839316 ]\n",
      " [ 0.03587504]\n",
      " [-0.14916413]]\n",
      "t [[ 0.12435588]\n",
      " [-0.08165297]\n",
      " [-0.11952648]\n",
      " ...\n",
      " [ 0.09414401]\n",
      " [ 0.04031399]\n",
      " [-0.16825646]]\n",
      "t [[ 0.12435588]\n",
      " [-0.08165297]\n",
      " [-0.11952648]\n",
      " ...\n",
      " [ 0.09414401]\n",
      " [ 0.04031399]\n",
      " [-0.16825646]]\n",
      "Current iteration=8, loss=49038.57656371454\n",
      "t [[ 0.13852371]\n",
      " [-0.09267132]\n",
      " [-0.13354286]\n",
      " ...\n",
      " [ 0.10396052]\n",
      " [ 0.04459663]\n",
      " [-0.18685567]]\n",
      "t [[ 0.13852371]\n",
      " [-0.09267132]\n",
      " [-0.13354286]\n",
      " ...\n",
      " [ 0.10396052]\n",
      " [ 0.04459663]\n",
      " [-0.18685567]]\n",
      "t [[ 0.1524088 ]\n",
      " [-0.10382353]\n",
      " [-0.14736981]\n",
      " ...\n",
      " [ 0.11339586]\n",
      " [ 0.04872771]\n",
      " [-0.2049803 ]]\n",
      "loss=48409.38969730936\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.01665554]\n",
      " [-0.00990775]\n",
      " [-0.01565778]\n",
      " ...\n",
      " [ 0.02398921]\n",
      " [-0.01565778]\n",
      " [ 0.0224693 ]]\n",
      "t [[ 0.01665554]\n",
      " [-0.00990775]\n",
      " [-0.01565778]\n",
      " ...\n",
      " [ 0.02398921]\n",
      " [-0.01565778]\n",
      " [ 0.0224693 ]]\n",
      "t [[ 0.03297197]\n",
      " [-0.02005676]\n",
      " [-0.03108809]\n",
      " ...\n",
      " [ 0.04742942]\n",
      " [-0.03108809]\n",
      " [ 0.04433542]]\n",
      "t [[ 0.03297197]\n",
      " [-0.02005676]\n",
      " [-0.03108809]\n",
      " ...\n",
      " [ 0.04742942]\n",
      " [-0.03108809]\n",
      " [ 0.04433542]]\n",
      "Current iteration=2, loss=51152.3131770457\n",
      "t [[ 0.04895679]\n",
      " [-0.03042991]\n",
      " [-0.04629622]\n",
      " ...\n",
      " [ 0.0703349 ]\n",
      " [-0.04629622]\n",
      " [ 0.06561631]]\n",
      "t [[ 0.04895679]\n",
      " [-0.03042991]\n",
      " [-0.04629622]\n",
      " ...\n",
      " [ 0.0703349 ]\n",
      " [-0.04629622]\n",
      " [ 0.06561631]]\n",
      "t [[ 0.06461743]\n",
      " [-0.04101079]\n",
      " [-0.06128739]\n",
      " ...\n",
      " [ 0.09271967]\n",
      " [-0.06128739]\n",
      " [ 0.08632951]]\n",
      "t [[ 0.06461743]\n",
      " [-0.04101079]\n",
      " [-0.06128739]\n",
      " ...\n",
      " [ 0.09271967]\n",
      " [-0.06128739]\n",
      " [ 0.08632951]]\n",
      "Current iteration=4, loss=50408.99799575459\n",
      "t [[ 0.0799612 ]\n",
      " [-0.0517837 ]\n",
      " [-0.07606668]\n",
      " ...\n",
      " [ 0.11459744]\n",
      " [-0.07606668]\n",
      " [ 0.10649213]]\n",
      "t [[ 0.0799612 ]\n",
      " [-0.0517837 ]\n",
      " [-0.07606668]\n",
      " ...\n",
      " [ 0.11459744]\n",
      " [-0.07606668]\n",
      " [ 0.10649213]]\n",
      "t [[ 0.09499532]\n",
      " [-0.06273367]\n",
      " [-0.09063909]\n",
      " ...\n",
      " [ 0.13598166]\n",
      " [-0.09063909]\n",
      " [ 0.12612084]]\n",
      "t [[ 0.09499532]\n",
      " [-0.06273367]\n",
      " [-0.09063909]\n",
      " ...\n",
      " [ 0.13598166]\n",
      " [-0.09063909]\n",
      " [ 0.12612084]]\n",
      "Current iteration=6, loss=49706.86050201656\n",
      "t [[ 0.10972689]\n",
      " [-0.0738464 ]\n",
      " [-0.10500948]\n",
      " ...\n",
      " [ 0.15688548]\n",
      " [-0.10500948]\n",
      " [ 0.14523188]]\n",
      "t [[ 0.10972689]\n",
      " [-0.0738464 ]\n",
      " [-0.10500948]\n",
      " ...\n",
      " [ 0.15688548]\n",
      " [-0.10500948]\n",
      " [ 0.14523188]]\n",
      "t [[ 0.12416289]\n",
      " [-0.08510828]\n",
      " [-0.11918262]\n",
      " ...\n",
      " [ 0.17732173]\n",
      " [-0.11918262]\n",
      " [ 0.16384104]]\n",
      "t [[ 0.12416289]\n",
      " [-0.08510828]\n",
      " [-0.11918262]\n",
      " ...\n",
      " [ 0.17732173]\n",
      " [-0.11918262]\n",
      " [ 0.16384104]]\n",
      "Current iteration=8, loss=49042.739186660525\n",
      "t [[ 0.13831018]\n",
      " [-0.09650636]\n",
      " [-0.13316315]\n",
      " ...\n",
      " [ 0.19730294]\n",
      " [-0.13316315]\n",
      " [ 0.18196366]]\n",
      "t [[ 0.13831018]\n",
      " [-0.09650636]\n",
      " [-0.13316315]\n",
      " ...\n",
      " [ 0.19730294]\n",
      " [-0.13316315]\n",
      " [ 0.18196366]]\n",
      "t [[ 0.15217547]\n",
      " [-0.10802832]\n",
      " [-0.14695558]\n",
      " ...\n",
      " [ 0.21684131]\n",
      " [-0.14695558]\n",
      " [ 0.19961463]]\n",
      "loss=48413.766435818296\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0074614 ]\n",
      " [-0.03572892]\n",
      " [-0.02095185]\n",
      " ...\n",
      " [ 0.01791661]\n",
      " [ 0.0074614 ]\n",
      " [-0.03037074]]\n",
      "t [[ 0.0074614 ]\n",
      " [-0.03572892]\n",
      " [-0.02095185]\n",
      " ...\n",
      " [ 0.01791661]\n",
      " [ 0.0074614 ]\n",
      " [-0.03037074]]\n",
      "t [[ 0.01458472]\n",
      " [-0.07092311]\n",
      " [-0.04150005]\n",
      " ...\n",
      " [ 0.03493511]\n",
      " [ 0.01458472]\n",
      " [-0.05963066]]\n",
      "t [[ 0.01458472]\n",
      " [-0.07092311]\n",
      " [-0.04150005]\n",
      " ...\n",
      " [ 0.03493511]\n",
      " [ 0.01458472]\n",
      " [-0.05963066]]\n",
      "Current iteration=2, loss=50890.42293954559\n",
      "t [[ 0.02138306]\n",
      " [-0.10558833]\n",
      " [-0.06165721]\n",
      " ...\n",
      " [ 0.05109909]\n",
      " [ 0.02138306]\n",
      " [-0.08783424]]\n",
      "t [[ 0.02138306]\n",
      " [-0.10558833]\n",
      " [-0.06165721]\n",
      " ...\n",
      " [ 0.05109909]\n",
      " [ 0.02138306]\n",
      " [-0.08783424]]\n",
      "t [[ 0.02786919]\n",
      " [-0.13973069]\n",
      " [-0.08143555]\n",
      " ...\n",
      " [ 0.06645032]\n",
      " [ 0.02786919]\n",
      " [-0.1150336 ]]\n",
      "t [[ 0.02786919]\n",
      " [-0.13973069]\n",
      " [-0.08143555]\n",
      " ...\n",
      " [ 0.06645032]\n",
      " [ 0.02786919]\n",
      " [-0.1150336 ]]\n",
      "Current iteration=4, loss=49918.49329051089\n",
      "t [[ 0.03405547]\n",
      " [-0.17335658]\n",
      " [-0.10084698]\n",
      " ...\n",
      " [ 0.08102868]\n",
      " [ 0.03405547]\n",
      " [-0.14127857]]\n",
      "t [[ 0.03405547]\n",
      " [-0.17335658]\n",
      " [-0.10084698]\n",
      " ...\n",
      " [ 0.08102868]\n",
      " [ 0.03405547]\n",
      " [-0.14127857]]\n",
      "t [[ 0.03995386]\n",
      " [-0.2064727 ]\n",
      " [-0.11990301]\n",
      " ...\n",
      " [ 0.09487218]\n",
      " [ 0.03995386]\n",
      " [-0.16661662]]\n",
      "t [[ 0.03995386]\n",
      " [-0.2064727 ]\n",
      " [-0.11990301]\n",
      " ...\n",
      " [ 0.09487218]\n",
      " [ 0.03995386]\n",
      " [-0.16661662]]\n",
      "Current iteration=6, loss=49016.56523323331\n",
      "t [[ 0.0455759 ]\n",
      " [-0.23908593]\n",
      " [-0.13861479]\n",
      " ...\n",
      " [ 0.108017  ]\n",
      " [ 0.0455759 ]\n",
      " [-0.19109296]]\n",
      "t [[ 0.0455759 ]\n",
      " [-0.23908593]\n",
      " [-0.13861479]\n",
      " ...\n",
      " [ 0.108017  ]\n",
      " [ 0.0455759 ]\n",
      " [-0.19109296]]\n",
      "t [[ 0.05093275]\n",
      " [-0.27120339]\n",
      " [-0.15699308]\n",
      " ...\n",
      " [ 0.12049749]\n",
      " [ 0.05093275]\n",
      " [-0.21475052]]\n",
      "t [[ 0.05093275]\n",
      " [-0.27120339]\n",
      " [-0.15699308]\n",
      " ...\n",
      " [ 0.12049749]\n",
      " [ 0.05093275]\n",
      " [-0.21475052]]\n",
      "Current iteration=8, loss=48177.66971296611\n",
      "t [[ 0.05603511]\n",
      " [-0.30283233]\n",
      " [-0.17504829]\n",
      " ...\n",
      " [ 0.13234626]\n",
      " [ 0.05603511]\n",
      " [-0.23763007]]\n",
      "t [[ 0.05603511]\n",
      " [-0.30283233]\n",
      " [-0.17504829]\n",
      " ...\n",
      " [ 0.13234626]\n",
      " [ 0.05603511]\n",
      " [-0.23763007]]\n",
      "t [[ 0.06089332]\n",
      " [-0.33398016]\n",
      " [-0.19279041]\n",
      " ...\n",
      " [ 0.14359425]\n",
      " [ 0.06089332]\n",
      " [-0.25977025]]\n",
      "loss=47395.684209349325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02253178]\n",
      " [-0.01313512]\n",
      " [-0.02102156]\n",
      " ...\n",
      " [ 0.01765437]\n",
      " [ 0.00743211]\n",
      " [-0.02991175]]\n",
      "t [[ 0.02253178]\n",
      " [-0.01313512]\n",
      " [-0.02102156]\n",
      " ...\n",
      " [ 0.01765437]\n",
      " [ 0.00743211]\n",
      " [-0.02991175]]\n",
      "t [[ 0.04445148]\n",
      " [-0.0267219 ]\n",
      " [-0.04163517]\n",
      " ...\n",
      " [ 0.03441706]\n",
      " [ 0.01452501]\n",
      " [-0.0587133 ]]\n",
      "t [[ 0.04445148]\n",
      " [-0.0267219 ]\n",
      " [-0.04163517]\n",
      " ...\n",
      " [ 0.03441706]\n",
      " [ 0.01452501]\n",
      " [-0.0587133 ]]\n",
      "Current iteration=2, loss=50887.82116219625\n",
      "t [[ 0.06577698]\n",
      " [-0.04071863]\n",
      " [-0.06185362]\n",
      " ...\n",
      " [ 0.05033155]\n",
      " [ 0.02129191]\n",
      " [-0.0864595 ]]\n",
      "t [[ 0.06577698]\n",
      " [-0.04071863]\n",
      " [-0.06185362]\n",
      " ...\n",
      " [ 0.05033155]\n",
      " [ 0.02129191]\n",
      " [-0.0864595 ]]\n",
      "t [[ 0.08652594]\n",
      " [-0.05508592]\n",
      " [-0.08168935]\n",
      " ...\n",
      " [ 0.0654395 ]\n",
      " [ 0.02774564]\n",
      " [-0.11320288]]\n",
      "t [[ 0.08652594]\n",
      " [-0.05508592]\n",
      " [-0.08168935]\n",
      " ...\n",
      " [ 0.0654395 ]\n",
      " [ 0.02774564]\n",
      " [-0.11320288]]\n",
      "Current iteration=4, loss=49913.78939660083\n",
      "t [[ 0.10671574]\n",
      " [-0.06978663]\n",
      " [-0.10115442]\n",
      " ...\n",
      " [ 0.07978067]\n",
      " [ 0.03389865]\n",
      " [-0.13899359]]\n",
      "t [[ 0.10671574]\n",
      " [-0.06978663]\n",
      " [-0.10115442]\n",
      " ...\n",
      " [ 0.07978067]\n",
      " [ 0.03389865]\n",
      " [-0.13899359]]\n",
      "t [[ 0.12636347]\n",
      " [-0.08478585]\n",
      " [-0.12026056]\n",
      " ...\n",
      " [ 0.09339294]\n",
      " [ 0.03976296]\n",
      " [-0.16387943]]\n",
      "t [[ 0.12636347]\n",
      " [-0.08478585]\n",
      " [-0.12026056]\n",
      " ...\n",
      " [ 0.09339294]\n",
      " [ 0.03976296]\n",
      " [-0.16387943]]\n",
      "Current iteration=6, loss=49010.165210782296\n",
      "t [[ 0.14548586]\n",
      " [-0.10005083]\n",
      " [-0.13901906]\n",
      " ...\n",
      " [ 0.10631236]\n",
      " [ 0.04535021]\n",
      " [-0.1879059 ]]\n",
      "t [[ 0.14548586]\n",
      " [-0.10005083]\n",
      " [-0.13901906]\n",
      " ...\n",
      " [ 0.10631236]\n",
      " [ 0.04535021]\n",
      " [-0.1879059 ]]\n",
      "t [[ 0.16409927]\n",
      " [-0.11555092]\n",
      " [-0.15744088]\n",
      " ...\n",
      " [ 0.11857315]\n",
      " [ 0.05067159]\n",
      " [-0.2111162 ]]\n",
      "t [[ 0.16409927]\n",
      " [-0.11555092]\n",
      " [-0.15744088]\n",
      " ...\n",
      " [ 0.11857315]\n",
      " [ 0.05067159]\n",
      " [-0.2111162 ]]\n",
      "Current iteration=8, loss=48169.902769163076\n",
      "t [[ 0.18221969]\n",
      " [-0.13125745]\n",
      " [-0.17553654]\n",
      " ...\n",
      " [ 0.1302078 ]\n",
      " [ 0.05573789]\n",
      " [-0.23355134]]\n",
      "t [[ 0.18221969]\n",
      " [-0.13125745]\n",
      " [-0.17553654]\n",
      " ...\n",
      " [ 0.1302078 ]\n",
      " [ 0.05573789]\n",
      " [-0.23355134]]\n",
      "t [[ 0.19986267]\n",
      " [-0.14714368]\n",
      " [-0.19331624]\n",
      " ...\n",
      " [ 0.1412471 ]\n",
      " [ 0.06055949]\n",
      " [-0.25525018]]\n",
      "loss=47386.817072618294\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02224299]\n",
      " [-0.01257409]\n",
      " [-0.02094283]\n",
      " ...\n",
      " [ 0.01792676]\n",
      " [ 0.00756909]\n",
      " [-0.03083288]]\n",
      "t [[ 0.02224299]\n",
      " [-0.01257409]\n",
      " [-0.02094283]\n",
      " ...\n",
      " [ 0.01792676]\n",
      " [ 0.00756909]\n",
      " [-0.03083288]]\n",
      "t [[ 0.04388204]\n",
      " [-0.02560187]\n",
      " [-0.04147771]\n",
      " ...\n",
      " [ 0.03493742]\n",
      " [ 0.01479489]\n",
      " [-0.06052199]]\n",
      "t [[ 0.04388204]\n",
      " [-0.02560187]\n",
      " [-0.04147771]\n",
      " ...\n",
      " [ 0.03493742]\n",
      " [ 0.01479489]\n",
      " [-0.06052199]]\n",
      "Current iteration=2, loss=50895.178710066175\n",
      "t [[ 0.06493484]\n",
      " [-0.03904145]\n",
      " [-0.06161752]\n",
      " ...\n",
      " [ 0.0510767 ]\n",
      " [ 0.02169077]\n",
      " [-0.08912362]]\n",
      "t [[ 0.06493484]\n",
      " [-0.03904145]\n",
      " [-0.06161752]\n",
      " ...\n",
      " [ 0.0510767 ]\n",
      " [ 0.02169077]\n",
      " [-0.08912362]]\n",
      "t [[ 0.08541886]\n",
      " [-0.05285326]\n",
      " [-0.08137476]\n",
      " ...\n",
      " [ 0.06638745]\n",
      " [ 0.02826975]\n",
      " [-0.11669172]]\n",
      "t [[ 0.08541886]\n",
      " [-0.05285326]\n",
      " [-0.08137476]\n",
      " ...\n",
      " [ 0.06638745]\n",
      " [ 0.02826975]\n",
      " [-0.11669172]]\n",
      "Current iteration=4, loss=49928.654530242544\n",
      "t [[ 0.10535129]\n",
      " [-0.06700002]\n",
      " [-0.10076158]\n",
      " ...\n",
      " [ 0.08091061]\n",
      " [ 0.03454443]\n",
      " [-0.14327784]]\n",
      "t [[ 0.10535129]\n",
      " [-0.06700002]\n",
      " [-0.10076158]\n",
      " ...\n",
      " [ 0.08091061]\n",
      " [ 0.03454443]\n",
      " [-0.14327784]]\n",
      "t [[ 0.12474902]\n",
      " [-0.08144671]\n",
      " [-0.11978975]\n",
      " ...\n",
      " [ 0.09468525]\n",
      " [ 0.04052701]\n",
      " [-0.16893113]]\n",
      "t [[ 0.12474902]\n",
      " [-0.08144671]\n",
      " [-0.11978975]\n",
      " ...\n",
      " [ 0.09468525]\n",
      " [ 0.04052701]\n",
      " [-0.16893113]]\n",
      "Current iteration=6, loss=49032.58074879649\n",
      "t [[ 0.14362857]\n",
      " [-0.09616052]\n",
      " [-0.13847065]\n",
      " ...\n",
      " [ 0.10774852]\n",
      " [ 0.04622927]\n",
      " [-0.19369837]]\n",
      "t [[ 0.14362857]\n",
      " [-0.09616052]\n",
      " [-0.13847065]\n",
      " ...\n",
      " [ 0.10774852]\n",
      " [ 0.04622927]\n",
      " [-0.19369837]]\n",
      "t [[ 0.1620061 ]\n",
      " [-0.1111107 ]\n",
      " [-0.15681527]\n",
      " ...\n",
      " [ 0.12013576]\n",
      " [ 0.05166256]\n",
      " [-0.21762403]]\n",
      "t [[ 0.1620061 ]\n",
      " [-0.1111107 ]\n",
      " [-0.15681527]\n",
      " ...\n",
      " [ 0.12013576]\n",
      " [ 0.05166256]\n",
      " [-0.21762403]]\n",
      "Current iteration=8, loss=48199.8295677561\n",
      "t [[ 0.17989739]\n",
      " [-0.12626857]\n",
      " [-0.17483422]\n",
      " ...\n",
      " [ 0.13188048]\n",
      " [ 0.05683781]\n",
      " [-0.24075029]]\n",
      "t [[ 0.17989739]\n",
      " [-0.12626857]\n",
      " [-0.17483422]\n",
      " ...\n",
      " [ 0.13188048]\n",
      " [ 0.05683781]\n",
      " [-0.24075029]]\n",
      "t [[ 0.19731779]\n",
      " [-0.14160736]\n",
      " [-0.19253771]\n",
      " ...\n",
      " [ 0.14301447]\n",
      " [ 0.06176553]\n",
      " [-0.26311716]]\n",
      "loss=47424.15483128788\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02220738]\n",
      " [-0.01321033]\n",
      " [-0.02087705]\n",
      " ...\n",
      " [ 0.03198562]\n",
      " [-0.02087705]\n",
      " [ 0.02995906]]\n",
      "t [[ 0.02220738]\n",
      " [-0.01321033]\n",
      " [-0.02087705]\n",
      " ...\n",
      " [ 0.03198562]\n",
      " [-0.02087705]\n",
      " [ 0.02995906]]\n",
      "t [[ 0.04381194]\n",
      " [-0.02684955]\n",
      " [-0.0413497 ]\n",
      " ...\n",
      " [ 0.06299528]\n",
      " [-0.0413497 ]\n",
      " [ 0.05884587]]\n",
      "t [[ 0.04381194]\n",
      " [-0.02684955]\n",
      " [-0.0413497 ]\n",
      " ...\n",
      " [ 0.06299528]\n",
      " [-0.0413497 ]\n",
      " [ 0.05884587]]\n",
      "Current iteration=2, loss=50897.31651399419\n",
      "t [[ 0.06483154]\n",
      " [-0.040877  ]\n",
      " [-0.06143061]\n",
      " ...\n",
      " [ 0.09306296]\n",
      " [-0.06143061]\n",
      " [ 0.08670313]]\n",
      "t [[ 0.06483154]\n",
      " [-0.040877  ]\n",
      " [-0.06143061]\n",
      " ...\n",
      " [ 0.09306296]\n",
      " [-0.06143061]\n",
      " [ 0.08670313]]\n",
      "t [[ 0.08528378]\n",
      " [-0.05525432]\n",
      " [-0.08113205]\n",
      " ...\n",
      " [ 0.12222177]\n",
      " [-0.08113205]\n",
      " [ 0.11357218]]\n",
      "t [[ 0.08528378]\n",
      " [-0.05525432]\n",
      " [-0.08113205]\n",
      " ...\n",
      " [ 0.12222177]\n",
      " [-0.08113205]\n",
      " [ 0.11357218]]\n",
      "Current iteration=4, loss=49932.140172717045\n",
      "t [[ 0.10518588]\n",
      " [-0.06994546]\n",
      " [-0.10046597]\n",
      " ...\n",
      " [ 0.15050389]\n",
      " [-0.10046597]\n",
      " [ 0.139493  ]]\n",
      "t [[ 0.10518588]\n",
      " [-0.06994546]\n",
      " [-0.10046597]\n",
      " ...\n",
      " [ 0.15050389]\n",
      " [-0.10046597]\n",
      " [ 0.139493  ]]\n",
      "t [[ 0.12455474]\n",
      " [-0.08491658]\n",
      " [-0.11944395]\n",
      " ...\n",
      " [ 0.17794051]\n",
      " [-0.11944395]\n",
      " [ 0.16450411]]\n",
      "t [[ 0.12455474]\n",
      " [-0.08491658]\n",
      " [-0.11944395]\n",
      " ...\n",
      " [ 0.17794051]\n",
      " [-0.11944395]\n",
      " [ 0.16450411]]\n",
      "Current iteration=6, loss=49036.79062727255\n",
      "t [[ 0.14340682]\n",
      " [-0.10013601]\n",
      " [-0.13807717]\n",
      " ...\n",
      " [ 0.20456186]\n",
      " [-0.13807717]\n",
      " [ 0.18864261]]\n",
      "t [[ 0.14340682]\n",
      " [-0.10013601]\n",
      " [-0.13807717]\n",
      " ...\n",
      " [ 0.20456186]\n",
      " [-0.13807717]\n",
      " [ 0.18864261]]\n",
      "t [[ 0.1617582 ]\n",
      " [-0.11557416]\n",
      " [-0.15637646]\n",
      " ...\n",
      " [ 0.23039714]\n",
      " [-0.15637646]\n",
      " [ 0.21194414]]\n",
      "t [[ 0.1617582 ]\n",
      " [-0.11557416]\n",
      " [-0.15637646]\n",
      " ...\n",
      " [ 0.23039714]\n",
      " [-0.15637646]\n",
      " [ 0.21194414]]\n",
      "Current iteration=8, loss=48204.27560371185\n",
      "t [[ 0.17962449]\n",
      " [-0.13120343]\n",
      " [-0.17435227]\n",
      " ...\n",
      " [ 0.25547455]\n",
      " [-0.17435227]\n",
      " [ 0.23444295]]\n",
      "t [[ 0.17962449]\n",
      " [-0.13120343]\n",
      " [-0.17435227]\n",
      " ...\n",
      " [ 0.25547455]\n",
      " [-0.17435227]\n",
      " [ 0.23444295]]\n",
      "t [[ 0.19702089]\n",
      " [-0.14699811]\n",
      " [-0.19201466]\n",
      " ...\n",
      " [ 0.27982129]\n",
      " [-0.19201466]\n",
      " [ 0.25617183]]\n",
      "loss=47428.45893262229\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.00932676]\n",
      " [-0.04466115]\n",
      " [-0.02618981]\n",
      " ...\n",
      " [ 0.02239576]\n",
      " [ 0.00932676]\n",
      " [-0.03796342]]\n",
      "t [[ 0.00932676]\n",
      " [-0.04466115]\n",
      " [-0.02618981]\n",
      " ...\n",
      " [ 0.02239576]\n",
      " [ 0.00932676]\n",
      " [-0.03796342]]\n",
      "t [[ 0.01812528]\n",
      " [-0.08848682]\n",
      " [-0.05174897]\n",
      " ...\n",
      " [ 0.04338831]\n",
      " [ 0.01812528]\n",
      " [-0.07419132]]\n",
      "t [[ 0.01812528]\n",
      " [-0.08848682]\n",
      " [-0.05174897]\n",
      " ...\n",
      " [ 0.04338831]\n",
      " [ 0.01812528]\n",
      " [-0.07419132]]\n",
      "Current iteration=2, loss=50637.459491719994\n",
      "t [[ 0.0264213 ]\n",
      " [-0.13148837]\n",
      " [-0.07670216]\n",
      " ...\n",
      " [ 0.06306301]\n",
      " [ 0.0264213 ]\n",
      " [-0.10879036]]\n",
      "t [[ 0.0264213 ]\n",
      " [-0.13148837]\n",
      " [-0.07670216]\n",
      " ...\n",
      " [ 0.06306301]\n",
      " [ 0.0264213 ]\n",
      " [-0.10879036]]\n",
      "t [[ 0.0342396 ]\n",
      " [-0.17367798]\n",
      " [-0.10107321]\n",
      " ...\n",
      " [ 0.08150063]\n",
      " [ 0.0342396 ]\n",
      " [-0.14186142]]\n",
      "t [[ 0.0342396 ]\n",
      " [-0.17367798]\n",
      " [-0.10107321]\n",
      " ...\n",
      " [ 0.08150063]\n",
      " [ 0.0342396 ]\n",
      " [-0.14186142]]\n",
      "Current iteration=4, loss=49453.967050045896\n",
      "t [[ 0.04160399]\n",
      " [-0.21506854]\n",
      " [-0.12488502]\n",
      " ...\n",
      " [ 0.09877726]\n",
      " [ 0.04160399]\n",
      " [-0.17349958]]\n",
      "t [[ 0.04160399]\n",
      " [-0.21506854]\n",
      " [-0.12488502]\n",
      " ...\n",
      " [ 0.09877726]\n",
      " [ 0.04160399]\n",
      " [-0.17349958]]\n",
      "t [[ 0.04853724]\n",
      " [-0.25567352]\n",
      " [-0.14815958]\n",
      " ...\n",
      " [ 0.11496438]\n",
      " [ 0.04853724]\n",
      " [-0.20379418]]\n",
      "t [[ 0.04853724]\n",
      " [-0.25567352]\n",
      " [-0.14815958]\n",
      " ...\n",
      " [ 0.11496438]\n",
      " [ 0.04853724]\n",
      " [-0.20379418]]\n",
      "Current iteration=6, loss=48374.85668658281\n",
      "t [[ 0.0550611 ]\n",
      " [-0.29550685]\n",
      " [-0.17091793]\n",
      " ...\n",
      " [ 0.13012899]\n",
      " [ 0.0550611 ]\n",
      " [-0.23282898]]\n",
      "t [[ 0.0550611 ]\n",
      " [-0.29550685]\n",
      " [-0.17091793]\n",
      " ...\n",
      " [ 0.13012899]\n",
      " [ 0.0550611 ]\n",
      " [-0.23282898]]\n",
      "t [[ 0.06119632]\n",
      " [-0.33458289]\n",
      " [-0.19318016]\n",
      " ...\n",
      " [ 0.14433379]\n",
      " [ 0.06119632]\n",
      " [-0.26068237]]\n",
      "t [[ 0.06119632]\n",
      " [-0.33458289]\n",
      " [-0.19318016]\n",
      " ...\n",
      " [ 0.14433379]\n",
      " [ 0.06119632]\n",
      " [-0.26068237]]\n",
      "Current iteration=8, loss=47387.428513181985\n",
      "t [[ 0.0669626 ]\n",
      " [-0.37291625]\n",
      " [-0.21496545]\n",
      " ...\n",
      " [ 0.15763744]\n",
      " [ 0.0669626 ]\n",
      " [-0.2874276 ]]\n",
      "t [[ 0.0669626 ]\n",
      " [-0.37291625]\n",
      " [-0.21496545]\n",
      " ...\n",
      " [ 0.15763744]\n",
      " [ 0.0669626 ]\n",
      " [-0.2874276 ]]\n",
      "t [[ 0.07237869]\n",
      " [-0.41052177]\n",
      " [-0.2362921 ]\n",
      " ...\n",
      " [ 0.17009477]\n",
      " [ 0.07237869]\n",
      " [-0.31313305]]\n",
      "loss=46480.87990746132\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02816472]\n",
      " [-0.0164189 ]\n",
      " [-0.02627695]\n",
      " ...\n",
      " [ 0.02206797]\n",
      " [ 0.00929014]\n",
      " [-0.03738969]]\n",
      "t [[ 0.02816472]\n",
      " [-0.0164189 ]\n",
      " [-0.02627695]\n",
      " ...\n",
      " [ 0.02206797]\n",
      " [ 0.00929014]\n",
      " [-0.03738969]]\n",
      "t [[ 0.05537314]\n",
      " [-0.03354346]\n",
      " [-0.05191652]\n",
      " ...\n",
      " [ 0.04274276]\n",
      " [ 0.0180503 ]\n",
      " [-0.0730448 ]]\n",
      "t [[ 0.05537314]\n",
      " [-0.03354346]\n",
      " [-0.05191652]\n",
      " ...\n",
      " [ 0.04274276]\n",
      " [ 0.0180503 ]\n",
      " [-0.0730448 ]]\n",
      "Current iteration=2, loss=50634.270342145945\n",
      "t [[ 0.08166036]\n",
      " [-0.05129209]\n",
      " [-0.07694378]\n",
      " ...\n",
      " [ 0.06210953]\n",
      " [ 0.02630637]\n",
      " [-0.1070728 ]]\n",
      "t [[ 0.08166036]\n",
      " [-0.05129209]\n",
      " [-0.07694378]\n",
      " ...\n",
      " [ 0.06210953]\n",
      " [ 0.02630637]\n",
      " [-0.1070728 ]]\n",
      "t [[ 0.10706091]\n",
      " [-0.06958889]\n",
      " [-0.10138294]\n",
      " ...\n",
      " [ 0.08024884]\n",
      " [ 0.03408331]\n",
      " [-0.13957526]]\n",
      "t [[ 0.10706091]\n",
      " [-0.06958889]\n",
      " [-0.10138294]\n",
      " ...\n",
      " [ 0.08024884]\n",
      " [ 0.03408331]\n",
      " [-0.13957526]]\n",
      "Current iteration=4, loss=49448.34017753279\n",
      "t [[ 0.13160855]\n",
      " [-0.0883636 ]\n",
      " [-0.12525725]\n",
      " ...\n",
      " [ 0.09723651]\n",
      " [ 0.04140505]\n",
      " [-0.17064792]]\n",
      "t [[ 0.13160855]\n",
      " [-0.0883636 ]\n",
      " [-0.12525725]\n",
      " ...\n",
      " [ 0.09723651]\n",
      " [ 0.04140505]\n",
      " [-0.17064792]]\n",
      "t [[ 0.15533625]\n",
      " [-0.1075513 ]\n",
      " [-0.14858905]\n",
      " ...\n",
      " [ 0.11314377]\n",
      " [ 0.04829452]\n",
      " [-0.2003807 ]]\n",
      "t [[ 0.15533625]\n",
      " [-0.1075513 ]\n",
      " [-0.14858905]\n",
      " ...\n",
      " [ 0.11314377]\n",
      " [ 0.04829452]\n",
      " [-0.2003807 ]]\n",
      "Current iteration=6, loss=48367.37028399317\n",
      "t [[ 0.178276  ]\n",
      " [-0.12709228]\n",
      " [-0.17139969]\n",
      " ...\n",
      " [ 0.12803737]\n",
      " [ 0.0547736 ]\n",
      " [-0.22885789]]\n",
      "t [[ 0.178276  ]\n",
      " [-0.12709228]\n",
      " [-0.17139969]\n",
      " ...\n",
      " [ 0.12803737]\n",
      " [ 0.0547736 ]\n",
      " [-0.22885789]]\n",
      "t [[ 0.20045884]\n",
      " [-0.14693174]\n",
      " [-0.19370959]\n",
      " ...\n",
      " [ 0.14197977]\n",
      " [ 0.06086315]\n",
      " [-0.25615832]]\n",
      "t [[ 0.20045884]\n",
      " [-0.14693174]\n",
      " [-0.19370959]\n",
      " ...\n",
      " [ 0.14197977]\n",
      " [ 0.06086315]\n",
      " [-0.25615832]]\n",
      "Current iteration=8, loss=47378.526434457344\n",
      "t [[ 0.22191471]\n",
      " [-0.16701953]\n",
      " [-0.21553821]\n",
      " ...\n",
      " [ 0.15502937]\n",
      " [ 0.06658299]\n",
      " [-0.28235568]]\n",
      "t [[ 0.22191471]\n",
      " [-0.16701953]\n",
      " [-0.21553821]\n",
      " ...\n",
      " [ 0.15502937]\n",
      " [ 0.06658299]\n",
      " [-0.28235568]]\n",
      "t [[ 0.24267251]\n",
      " [-0.18730987]\n",
      " [-0.23690412]\n",
      " ...\n",
      " [ 0.16724078]\n",
      " [ 0.07195197]\n",
      " [-0.3075187 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=46470.902803270415\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02780374]\n",
      " [-0.01571761]\n",
      " [-0.02617854]\n",
      " ...\n",
      " [ 0.02240845]\n",
      " [ 0.00946136]\n",
      " [-0.0385411 ]]\n",
      "t [[ 0.02780374]\n",
      " [-0.01571761]\n",
      " [-0.02617854]\n",
      " ...\n",
      " [ 0.02240845]\n",
      " [ 0.00946136]\n",
      " [-0.0385411 ]]\n",
      "t [[ 0.05466388]\n",
      " [-0.03214407]\n",
      " [-0.0517197 ]\n",
      " ...\n",
      " [ 0.0433856 ]\n",
      " [ 0.01838636]\n",
      " [-0.07529518]]\n",
      "t [[ 0.05466388]\n",
      " [-0.03214407]\n",
      " [-0.0517197 ]\n",
      " ...\n",
      " [ 0.0433856 ]\n",
      " [ 0.01838636]\n",
      " [-0.07529518]]\n",
      "Current iteration=2, loss=50643.49016486426\n",
      "t [[ 0.08061517]\n",
      " [-0.04919739]\n",
      " [-0.07664872]\n",
      " ...\n",
      " [ 0.063019  ]\n",
      " [ 0.02680124]\n",
      " [-0.11037253]]\n",
      "t [[ 0.08061517]\n",
      " [-0.04919739]\n",
      " [-0.07664872]\n",
      " ...\n",
      " [ 0.063019  ]\n",
      " [ 0.02680124]\n",
      " [-0.11037253]]\n",
      "t [[ 0.10569176]\n",
      " [-0.06680138]\n",
      " [-0.10098993]\n",
      " ...\n",
      " [ 0.08139156]\n",
      " [ 0.03473128]\n",
      " [-0.14387753]]\n",
      "t [[ 0.10569176]\n",
      " [-0.06680138]\n",
      " [-0.10098993]\n",
      " ...\n",
      " [ 0.08139156]\n",
      " [ 0.03473128]\n",
      " [-0.14387753]]\n",
      "Current iteration=4, loss=49466.980007699996\n",
      "t [[ 0.12992703]\n",
      " [-0.08488551]\n",
      " [-0.12476675]\n",
      " ...\n",
      " [ 0.09858144]\n",
      " [ 0.04220075]\n",
      " [-0.1759086 ]]\n",
      "t [[ 0.12992703]\n",
      " [-0.08488551]\n",
      " [-0.12476675]\n",
      " ...\n",
      " [ 0.09858144]\n",
      " [ 0.04220075]\n",
      " [-0.1759086 ]]\n",
      "t [[ 0.15335353]\n",
      " [-0.10338472]\n",
      " [-0.14800162]\n",
      " ...\n",
      " [ 0.1146621 ]\n",
      " [ 0.04923287]\n",
      " [-0.20655821]]\n",
      "t [[ 0.15335353]\n",
      " [-0.10338472]\n",
      " [-0.14800162]\n",
      " ...\n",
      " [ 0.1146621 ]\n",
      " [ 0.04923287]\n",
      " [-0.20655821]]\n",
      "Current iteration=6, loss=48395.43555921479\n",
      "t [[ 0.17600286]\n",
      " [-0.12223918]\n",
      " [-0.17071602]\n",
      " ...\n",
      " [ 0.1297024 ]\n",
      " [ 0.05584983]\n",
      " [-0.23591309]]\n",
      "t [[ 0.17600286]\n",
      " [-0.12223918]\n",
      " [-0.17071602]\n",
      " ...\n",
      " [ 0.1297024 ]\n",
      " [ 0.05584983]\n",
      " [-0.23591309]]\n",
      "t [[ 0.19790565]\n",
      " [-0.14139401]\n",
      " [-0.19293044]\n",
      " ...\n",
      " [ 0.14376681]\n",
      " [ 0.06207274]\n",
      " [-0.26405437]]\n",
      "t [[ 0.19790565]\n",
      " [-0.14139401]\n",
      " [-0.19293044]\n",
      " ...\n",
      " [ 0.14376681]\n",
      " [ 0.06207274]\n",
      " [-0.26405437]]\n",
      "Current iteration=8, loss=47415.88549024683\n",
      "t [[ 0.21909149]\n",
      " [-0.16079903]\n",
      " [-0.21466446]\n",
      " ...\n",
      " [ 0.15691559]\n",
      " [ 0.06792169]\n",
      " [-0.29105786]]\n",
      "t [[ 0.21909149]\n",
      " [-0.16079903]\n",
      " [-0.21466446]\n",
      " ...\n",
      " [ 0.15691559]\n",
      " [ 0.06792169]\n",
      " [-0.29105786]]\n",
      "t [[ 0.23958888]\n",
      " [-0.18040845]\n",
      " [-0.23593669]\n",
      " ...\n",
      " [ 0.16920503]\n",
      " [ 0.07341577]\n",
      " [-0.31699432]]\n",
      "loss=46517.33112160947\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.02775923]\n",
      " [-0.01651292]\n",
      " [-0.02609631]\n",
      " ...\n",
      " [ 0.03998202]\n",
      " [-0.02609631]\n",
      " [ 0.03744883]]\n",
      "t [[ 0.02775923]\n",
      " [-0.01651292]\n",
      " [-0.02609631]\n",
      " ...\n",
      " [ 0.03998202]\n",
      " [-0.02609631]\n",
      " [ 0.03744883]]\n",
      "t [[ 0.0545766 ]\n",
      " [-0.03369592]\n",
      " [-0.0515608 ]\n",
      " ...\n",
      " [ 0.07843921]\n",
      " [-0.0515608 ]\n",
      " [ 0.07322237]]\n",
      "t [[ 0.0545766 ]\n",
      " [-0.03369592]\n",
      " [-0.0515608 ]\n",
      " ...\n",
      " [ 0.07843921]\n",
      " [-0.0515608 ]\n",
      " [ 0.07322237]]\n",
      "Current iteration=2, loss=50646.06183470582\n",
      "t [[ 0.0804872 ]\n",
      " [-0.05146947]\n",
      " [-0.07641824]\n",
      " ...\n",
      " [ 0.1154382 ]\n",
      " [-0.07641824]\n",
      " [ 0.10740433]]\n",
      "t [[ 0.0804872 ]\n",
      " [-0.05146947]\n",
      " [-0.07641824]\n",
      " ...\n",
      " [ 0.1154382 ]\n",
      " [-0.07641824]\n",
      " [ 0.10740433]]\n",
      "t [[ 0.10552533]\n",
      " [-0.06975979]\n",
      " [-0.10069257]\n",
      " ...\n",
      " [ 0.15104341]\n",
      " [-0.10069257]\n",
      " [ 0.14007502]]\n",
      "t [[ 0.10552533]\n",
      " [-0.06975979]\n",
      " [-0.10069257]\n",
      " ...\n",
      " [ 0.15104341]\n",
      " [-0.10069257]\n",
      " [ 0.14007502]]\n",
      "Current iteration=4, loss=49470.94148664939\n",
      "t [[ 0.12972442]\n",
      " [-0.08849869]\n",
      " [-0.12440678]\n",
      " ...\n",
      " [ 0.18531685]\n",
      " [-0.12440678]\n",
      " [ 0.1713112 ]]\n",
      "t [[ 0.12972442]\n",
      " [-0.08849869]\n",
      " [-0.12440678]\n",
      " ...\n",
      " [ 0.18531685]\n",
      " [-0.12440678]\n",
      " [ 0.1713112 ]]\n",
      "t [[ 0.1531169 ]\n",
      " [-0.10762341]\n",
      " [-0.14758296]\n",
      " ...\n",
      " [ 0.2183181 ]\n",
      " [-0.14758296]\n",
      " [ 0.20118608]]\n",
      "t [[ 0.1531169 ]\n",
      " [-0.10762341]\n",
      " [-0.14758296]\n",
      " ...\n",
      " [ 0.2183181 ]\n",
      " [-0.14758296]\n",
      " [ 0.20118608]]\n",
      "Current iteration=6, loss=48399.91222309224\n",
      "t [[ 0.17573419]\n",
      " [-0.12707631]\n",
      " [-0.17024225]\n",
      " ...\n",
      " [ 0.25010423]\n",
      " [-0.17024225]\n",
      " [ 0.22976927]]\n",
      "t [[ 0.17573419]\n",
      " [-0.12707631]\n",
      " [-0.17024225]\n",
      " ...\n",
      " [ 0.25010423]\n",
      " [-0.17024225]\n",
      " [ 0.22976927]]\n",
      "t [[ 0.19760661]\n",
      " [-0.14680467]\n",
      " [-0.19240486]\n",
      " ...\n",
      " [ 0.28072978]\n",
      " [-0.19240486]\n",
      " [ 0.25712684]]\n",
      "t [[ 0.19760661]\n",
      " [-0.14680467]\n",
      " [-0.19240486]\n",
      " ...\n",
      " [ 0.28072978]\n",
      " [-0.19240486]\n",
      " [ 0.25712684]]\n",
      "Current iteration=8, loss=47420.23942541274\n",
      "t [[ 0.21876337]\n",
      " [-0.16676035]\n",
      " [-0.21409007]\n",
      " ...\n",
      " [ 0.31024683]\n",
      " [-0.21409007]\n",
      " [ 0.28332144]]\n",
      "t [[ 0.21876337]\n",
      " [-0.16676035]\n",
      " [-0.21409007]\n",
      " ...\n",
      " [ 0.31024683]\n",
      " [-0.21409007]\n",
      " [ 0.28332144]]\n",
      "t [[ 0.2392326 ]\n",
      " [-0.18689951]\n",
      " [-0.23531625]\n",
      " ...\n",
      " [ 0.33870501]\n",
      " [-0.23531625]\n",
      " [ 0.30841234]]\n",
      "loss=46521.105742624975\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01119211]\n",
      " [-0.05359338]\n",
      " [-0.03142777]\n",
      " ...\n",
      " [ 0.02687492]\n",
      " [ 0.01119211]\n",
      " [-0.0455561 ]]\n",
      "t [[ 0.01119211]\n",
      " [-0.05359338]\n",
      " [-0.03142777]\n",
      " ...\n",
      " [ 0.02687492]\n",
      " [ 0.01119211]\n",
      " [-0.0455561 ]]\n",
      "t [[ 0.02162362]\n",
      " [-0.10598373]\n",
      " [-0.06194747]\n",
      " ...\n",
      " [ 0.05172935]\n",
      " [ 0.02162362]\n",
      " [-0.08861326]]\n",
      "t [[ 0.02162362]\n",
      " [-0.10598373]\n",
      " [-0.06194747]\n",
      " ...\n",
      " [ 0.05172935]\n",
      " [ 0.02162362]\n",
      " [-0.08861326]]\n",
      "Current iteration=2, loss=50388.19096799019\n",
      "t [[ 0.03133916]\n",
      " [-0.15719087]\n",
      " [-0.09160192]\n",
      " ...\n",
      " [ 0.07471119]\n",
      " [ 0.03133916]\n",
      " [-0.12935629]]\n",
      "t [[ 0.03133916]\n",
      " [-0.15719087]\n",
      " [-0.09160192]\n",
      " ...\n",
      " [ 0.07471119]\n",
      " [ 0.03133916]\n",
      " [-0.12935629]]\n",
      "t [[ 0.04038133]\n",
      " [-0.20723629]\n",
      " [-0.12043209]\n",
      " ...\n",
      " [ 0.09595849]\n",
      " [ 0.04038133]\n",
      " [-0.16795773]]\n",
      "t [[ 0.04038133]\n",
      " [-0.20723629]\n",
      " [-0.12043209]\n",
      " ...\n",
      " [ 0.09595849]\n",
      " [ 0.04038133]\n",
      " [-0.16795773]]\n",
      "Current iteration=4, loss=49004.500592529585\n",
      "t [[ 0.0487906 ]\n",
      " [-0.25614282]\n",
      " [-0.14847699]\n",
      " ...\n",
      " [ 0.11559951]\n",
      " [ 0.0487906 ]\n",
      " [-0.2045779 ]]\n",
      "t [[ 0.0487906 ]\n",
      " [-0.25614282]\n",
      " [-0.14847699]\n",
      " ...\n",
      " [ 0.11559951]\n",
      " [ 0.0487906 ]\n",
      " [-0.2045779 ]]\n",
      "t [[ 0.05660527]\n",
      " [-0.30393442]\n",
      " [-0.17577365]\n",
      " ...\n",
      " [ 0.13375291]\n",
      " [ 0.05660527]\n",
      " [-0.23936518]]\n",
      "t [[ 0.05660527]\n",
      " [-0.30393442]\n",
      " [-0.17577365]\n",
      " ...\n",
      " [ 0.13375291]\n",
      " [ 0.05660527]\n",
      " [-0.23936518]]\n",
      "Current iteration=6, loss=47764.29028244347\n",
      "t [[ 0.06386149]\n",
      " [-0.35063586]\n",
      " [-0.2023571 ]\n",
      " ...\n",
      " [ 0.15052825]\n",
      " [ 0.06386149]\n",
      " [-0.27245656]]\n",
      "t [[ 0.06386149]\n",
      " [-0.35063586]\n",
      " [-0.2023571 ]\n",
      " ...\n",
      " [ 0.15052825]\n",
      " [ 0.06386149]\n",
      " [-0.27245656]]\n",
      "t [[ 0.07059328]\n",
      " [-0.39627253]\n",
      " [-0.22826044]\n",
      " ...\n",
      " [ 0.16602656]\n",
      " [ 0.07059328]\n",
      " [-0.3039782 ]]\n",
      "t [[ 0.07059328]\n",
      " [-0.39627253]\n",
      " [-0.22826044]\n",
      " ...\n",
      " [ 0.16602656]\n",
      " [ 0.07059328]\n",
      " [-0.3039782 ]]\n",
      "Current iteration=8, loss=46647.09141677129\n",
      "t [[ 0.07683262]\n",
      " [-0.4408702 ]\n",
      " [-0.25351491]\n",
      " ...\n",
      " [ 0.18034095]\n",
      " [ 0.07683262]\n",
      " [-0.33404617]]\n",
      "t [[ 0.07683262]\n",
      " [-0.4408702 ]\n",
      " [-0.25351491]\n",
      " ...\n",
      " [ 0.18034095]\n",
      " [ 0.07683262]\n",
      " [-0.33404617]]\n",
      "t [[ 0.08260951]\n",
      " [-0.48445487]\n",
      " [-0.27814993]\n",
      " ...\n",
      " [ 0.19355728]\n",
      " [ 0.08260951]\n",
      " [-0.3627671 ]]\n",
      "loss=45636.03506009193\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03379767]\n",
      " [-0.01970268]\n",
      " [-0.03153234]\n",
      " ...\n",
      " [ 0.02648156]\n",
      " [ 0.01114816]\n",
      " [-0.04486763]]\n",
      "t [[ 0.03379767]\n",
      " [-0.01970268]\n",
      " [-0.03153234]\n",
      " ...\n",
      " [ 0.02648156]\n",
      " [ 0.01114816]\n",
      " [-0.04486763]]\n",
      "t [[ 0.06621837]\n",
      " [-0.04042141]\n",
      " [-0.06214691]\n",
      " ...\n",
      " [ 0.05095709]\n",
      " [ 0.02153322]\n",
      " [-0.08723767]]\n",
      "t [[ 0.06621837]\n",
      " [-0.04042141]\n",
      " [-0.06214691]\n",
      " ...\n",
      " [ 0.05095709]\n",
      " [ 0.02153322]\n",
      " [-0.08723767]]\n",
      "Current iteration=2, loss=50384.43835963416\n",
      "t [[ 0.09732311]\n",
      " [-0.06201497]\n",
      " [-0.09188723]\n",
      " ...\n",
      " [ 0.07357412]\n",
      " [ 0.03120008]\n",
      " [-0.12729632]]\n",
      "t [[ 0.09732311]\n",
      " [-0.06201497]\n",
      " [-0.09188723]\n",
      " ...\n",
      " [ 0.07357412]\n",
      " [ 0.03120008]\n",
      " [-0.12729632]]\n",
      "t [[ 0.12717152]\n",
      " [-0.08435417]\n",
      " [-0.12079491]\n",
      " ...\n",
      " [ 0.0944703 ]\n",
      " [ 0.04019161]\n",
      " [-0.16521731]]\n",
      "t [[ 0.12717152]\n",
      " [-0.08435417]\n",
      " [-0.12079491]\n",
      " ...\n",
      " [ 0.0944703 ]\n",
      " [ 0.04019161]\n",
      " [-0.16521731]]\n",
      "Current iteration=4, loss=48998.03665103969\n",
      "t [[ 0.15582158]\n",
      " [-0.10732147]\n",
      " [-0.14890955]\n",
      " ...\n",
      " [ 0.11377345]\n",
      " [ 0.04854853]\n",
      " [-0.20116205]]\n",
      "t [[ 0.15582158]\n",
      " [-0.10732147]\n",
      " [-0.14890955]\n",
      " ...\n",
      " [ 0.11377345]\n",
      " [ 0.04854853]\n",
      " [-0.20116205]]\n",
      "t [[ 0.18332938]\n",
      " [-0.13081042]\n",
      " [-0.17626875]\n",
      " ...\n",
      " [ 0.13160178]\n",
      " [ 0.05630937]\n",
      " [-0.23527984]]\n",
      "t [[ 0.18332938]\n",
      " [-0.13081042]\n",
      " [-0.17626875]\n",
      " ...\n",
      " [ 0.13160178]\n",
      " [ 0.05630937]\n",
      " [-0.23527984]]\n",
      "Current iteration=6, loss=47755.87312274046\n",
      "t [[ 0.20974889]\n",
      " [-0.15472502]\n",
      " [-0.2029081 ]\n",
      " ...\n",
      " [ 0.14806443]\n",
      " [ 0.0635105 ]\n",
      " [-0.26770849]]\n",
      "t [[ 0.20974889]\n",
      " [-0.15472502]\n",
      " [-0.2029081 ]\n",
      " ...\n",
      " [ 0.14806443]\n",
      " [ 0.0635105 ]\n",
      " [-0.26770849]]\n",
      "t [[ 0.23513187]\n",
      " [-0.17897908]\n",
      " [-0.2288612 ]\n",
      " ...\n",
      " [ 0.163262  ]\n",
      " [ 0.07018613]\n",
      " [-0.29857487]]\n",
      "t [[ 0.23513187]\n",
      " [-0.17897908]\n",
      " [-0.2288612 ]\n",
      " ...\n",
      " [ 0.163262  ]\n",
      " [ 0.07018613]\n",
      " [-0.29857487]]\n",
      "Current iteration=8, loss=46637.27173495265\n",
      "t [[ 0.25952779]\n",
      " [-0.20349545]\n",
      " [-0.25415975]\n",
      " ...\n",
      " [ 0.17728721]\n",
      " [ 0.0763684 ]\n",
      " [-0.32799564]]\n",
      "t [[ 0.25952779]\n",
      " [-0.20349545]\n",
      " [-0.25415975]\n",
      " ...\n",
      " [ 0.17728721]\n",
      " [ 0.0763684 ]\n",
      " [-0.32799564]]\n",
      "t [[ 0.28298377]\n",
      " [-0.22820536]\n",
      " [-0.27883361]\n",
      " ...\n",
      " [ 0.19022554]\n",
      " [ 0.08208748]\n",
      " [-0.35607794]]\n",
      "loss=45625.21325225715\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03336448]\n",
      " [-0.01886113]\n",
      " [-0.03141424]\n",
      " ...\n",
      " [ 0.02689014]\n",
      " [ 0.01135364]\n",
      " [-0.04624932]]\n",
      "t [[ 0.03336448]\n",
      " [-0.01886113]\n",
      " [-0.03141424]\n",
      " ...\n",
      " [ 0.02689014]\n",
      " [ 0.01135364]\n",
      " [-0.04624932]]\n",
      "t [[ 0.0653703 ]\n",
      " [-0.0387429 ]\n",
      " [-0.06191073]\n",
      " ...\n",
      " [ 0.05171937]\n",
      " [ 0.02193497]\n",
      " [-0.08992555]]\n",
      "t [[ 0.0653703 ]\n",
      " [-0.0387429 ]\n",
      " [-0.06191073]\n",
      " ...\n",
      " [ 0.05171937]\n",
      " [ 0.02193497]\n",
      " [-0.08992555]]\n",
      "Current iteration=2, loss=50395.5280158329\n",
      "t [[ 0.09607783]\n",
      " [-0.05950342]\n",
      " [-0.09153325]\n",
      " ...\n",
      " [ 0.07463939]\n",
      " [ 0.03178952]\n",
      " [-0.13121981]]\n",
      "t [[ 0.09607783]\n",
      " [-0.05950342]\n",
      " [-0.09153325]\n",
      " ...\n",
      " [ 0.07463939]\n",
      " [ 0.03178952]\n",
      " [-0.13121981]]\n",
      "t [[ 0.12554602]\n",
      " [-0.081013  ]\n",
      " [-0.12032364]\n",
      " ...\n",
      " [ 0.09579196]\n",
      " [ 0.04096073]\n",
      " [-0.17031069]]\n",
      "t [[ 0.12554602]\n",
      " [-0.081013  ]\n",
      " [-0.12032364]\n",
      " ...\n",
      " [ 0.09579196]\n",
      " [ 0.04096073]\n",
      " [-0.17031069]]\n",
      "Current iteration=4, loss=49020.45704791873\n",
      "t [[ 0.15383218]\n",
      " [-0.10315373]\n",
      " [-0.14832177]\n",
      " ...\n",
      " [ 0.11530886]\n",
      " [ 0.04948988]\n",
      " [-0.20736411]]\n",
      "t [[ 0.15383218]\n",
      " [-0.10315373]\n",
      " [-0.14832177]\n",
      " ...\n",
      " [ 0.11530886]\n",
      " [ 0.04948988]\n",
      " [-0.20736411]]\n",
      "t [[ 0.18099169]\n",
      " [-0.12581893]\n",
      " [-0.17556542]\n",
      " ...\n",
      " [ 0.13331207]\n",
      " [ 0.057416  ]\n",
      " [-0.24253371]]\n",
      "t [[ 0.18099169]\n",
      " [-0.12581893]\n",
      " [-0.17556542]\n",
      " ...\n",
      " [ 0.13331207]\n",
      " [ 0.057416  ]\n",
      " [-0.24253371]]\n",
      "Current iteration=6, loss=47789.55314516799\n",
      "t [[ 0.20707783]\n",
      " [-0.1489125 ]\n",
      " [-0.20209035]\n",
      " ...\n",
      " [ 0.14991419]\n",
      " [ 0.06477593]\n",
      " [-0.27596126]]\n",
      "t [[ 0.20707783]\n",
      " [-0.1489125 ]\n",
      " [-0.20209035]\n",
      " ...\n",
      " [ 0.14991419]\n",
      " [ 0.06477593]\n",
      " [-0.27596126]]\n",
      "t [[ 0.23214173]\n",
      " [-0.17234816]\n",
      " [-0.2279303 ]\n",
      " ...\n",
      " [ 0.16521902]\n",
      " [ 0.07160433]\n",
      " [-0.30777736]]\n",
      "t [[ 0.23214173]\n",
      " [-0.17234816]\n",
      " [-0.2279303 ]\n",
      " ...\n",
      " [ 0.16521902]\n",
      " [ 0.07160433]\n",
      " [-0.30777736]]\n",
      "Current iteration=8, loss=46681.939082999925\n",
      "t [[ 0.25623221]\n",
      " [-0.19604876]\n",
      " [-0.25311709]\n",
      " ...\n",
      " [ 0.17932213]\n",
      " [ 0.07793374]\n",
      " [-0.33810204]]\n",
      "t [[ 0.25623221]\n",
      " [-0.19604876]\n",
      " [-0.25311709]\n",
      " ...\n",
      " [ 0.17932213]\n",
      " [ 0.07793374]\n",
      " [-0.33810204]]\n",
      "t [[ 0.27939584]\n",
      " [-0.21994553]\n",
      " [-0.27768067]\n",
      " ...\n",
      " [ 0.19231154]\n",
      " [ 0.08379471]\n",
      " [-0.36704558]]\n",
      "loss=45680.473517600316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03331107]\n",
      " [-0.0198155 ]\n",
      " [-0.03131557]\n",
      " ...\n",
      " [ 0.04797842]\n",
      " [-0.03131557]\n",
      " [ 0.04493859]]\n",
      "t [[ 0.03331107]\n",
      " [-0.0198155 ]\n",
      " [-0.03131557]\n",
      " ...\n",
      " [ 0.04797842]\n",
      " [-0.03131557]\n",
      " [ 0.04493859]]\n",
      "t [[ 0.06526598]\n",
      " [-0.04059583]\n",
      " [-0.06172138]\n",
      " ...\n",
      " [ 0.09376127]\n",
      " [-0.06172138]\n",
      " [ 0.08746497]]\n",
      "t [[ 0.06526598]\n",
      " [-0.04059583]\n",
      " [-0.06172138]\n",
      " ...\n",
      " [ 0.09376127]\n",
      " [-0.06172138]\n",
      " [ 0.08746497]]\n",
      "Current iteration=2, loss=50398.49571501711\n",
      "t [[ 0.09592562]\n",
      " [-0.06220339]\n",
      " [-0.09126043]\n",
      " ...\n",
      " [ 0.13746412]\n",
      " [-0.09126043]\n",
      " [ 0.12772428]]\n",
      "t [[ 0.09592562]\n",
      " [-0.06220339]\n",
      " [-0.09126043]\n",
      " ...\n",
      " [ 0.13746412]\n",
      " [-0.09126043]\n",
      " [ 0.12772428]]\n",
      "t [[ 0.1253492 ]\n",
      " [-0.08451264]\n",
      " [-0.11997385]\n",
      " ...\n",
      " [ 0.17919777]\n",
      " [-0.11997385]\n",
      " [ 0.16585438]]\n",
      "t [[ 0.1253492 ]\n",
      " [-0.08451264]\n",
      " [-0.11997385]\n",
      " ...\n",
      " [ 0.17919777]\n",
      " [-0.11997385]\n",
      " [ 0.16585438]]\n",
      "Current iteration=4, loss=49024.763605911525\n",
      "t [[ 0.15359392]\n",
      " [-0.10740973]\n",
      " [-0.14790083]\n",
      " ...\n",
      " [ 0.21906787]\n",
      " [-0.14790083]\n",
      " [ 0.20198561]]\n",
      "t [[ 0.15359392]\n",
      " [-0.10740973]\n",
      " [-0.14790083]\n",
      " ...\n",
      " [ 0.21906787]\n",
      " [-0.14790083]\n",
      " [ 0.20198561]]\n",
      "t [[ 0.18071486]\n",
      " [-0.13079187]\n",
      " [-0.17507856]\n",
      " ...\n",
      " [ 0.25717481]\n",
      " [-0.17507856]\n",
      " [ 0.2362407 ]]\n",
      "t [[ 0.18071486]\n",
      " [-0.13079187]\n",
      " [-0.17507856]\n",
      " ...\n",
      " [ 0.25717481]\n",
      " [-0.17507856]\n",
      " [ 0.2362407 ]]\n",
      "Current iteration=6, loss=47794.071269121036\n",
      "t [[ 0.20676479]\n",
      " [-0.15456669]\n",
      " [-0.20154228]\n",
      " ...\n",
      " [ 0.29361368]\n",
      " [-0.20154228]\n",
      " [ 0.26873494]]\n",
      "t [[ 0.20676479]\n",
      " [-0.15456669]\n",
      " [-0.20154228]\n",
      " ...\n",
      " [ 0.29361368]\n",
      " [-0.20154228]\n",
      " [ 0.26873494]]\n",
      "t [[ 0.2317942 ]\n",
      " [-0.17865144]\n",
      " [-0.22732525]\n",
      " ...\n",
      " [ 0.32847432]\n",
      " [-0.22732525]\n",
      " [ 0.29957633]]\n",
      "t [[ 0.2317942 ]\n",
      " [-0.17865144]\n",
      " [-0.22732525]\n",
      " ...\n",
      " [ 0.32847432]\n",
      " [-0.22732525]\n",
      " [ 0.29957633]]\n",
      "Current iteration=8, loss=46685.90635391552\n",
      "t [[ 0.25585121]\n",
      " [-0.20297227]\n",
      " [-0.25245887]\n",
      " ...\n",
      " [ 0.3618415 ]\n",
      " [-0.25245887]\n",
      " [ 0.32886592]]\n",
      "t [[ 0.25585121]\n",
      " [-0.20297227]\n",
      " [-0.25245887]\n",
      " ...\n",
      " [ 0.3618415 ]\n",
      " [-0.25245887]\n",
      " [ 0.32886592]]\n",
      "t [[ 0.27898166]\n",
      " [-0.22746346]\n",
      " [-0.2769727 ]\n",
      " ...\n",
      " [ 0.39379506]\n",
      " [-0.2769727 ]\n",
      " [ 0.35669809]]\n",
      "loss=45683.390578324965\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01305746]\n",
      " [-0.06252561]\n",
      " [-0.03666573]\n",
      " ...\n",
      " [ 0.03135407]\n",
      " [ 0.01305746]\n",
      " [-0.05314879]]\n",
      "t [[ 0.01305746]\n",
      " [-0.06252561]\n",
      " [-0.03666573]\n",
      " ...\n",
      " [ 0.03135407]\n",
      " [ 0.01305746]\n",
      " [-0.05314879]]\n",
      "t [[ 0.02507976]\n",
      " [-0.12341386]\n",
      " [-0.07209558]\n",
      " ...\n",
      " [ 0.05995828]\n",
      " [ 0.02507976]\n",
      " [-0.10289656]]\n",
      "t [[ 0.02507976]\n",
      " [-0.12341386]\n",
      " [-0.07209558]\n",
      " ...\n",
      " [ 0.05995828]\n",
      " [ 0.02507976]\n",
      " [-0.10289656]]\n",
      "Current iteration=2, loss=50142.56433524297\n",
      "t [[ 0.03613802]\n",
      " [-0.18269652]\n",
      " [-0.10635781]\n",
      " ...\n",
      " [ 0.08604799]\n",
      " [ 0.03613802]\n",
      " [-0.14953756]]\n",
      "t [[ 0.03613802]\n",
      " [-0.18269652]\n",
      " [-0.10635781]\n",
      " ...\n",
      " [ 0.08604799]\n",
      " [ 0.03613802]\n",
      " [-0.14953756]]\n",
      "t [[ 0.04629945]\n",
      " [-0.24040837]\n",
      " [-0.13951712]\n",
      " ...\n",
      " [ 0.10983997]\n",
      " [ 0.04629945]\n",
      " [-0.19334277]]\n",
      "t [[ 0.04629945]\n",
      " [-0.24040837]\n",
      " [-0.13951712]\n",
      " ...\n",
      " [ 0.10983997]\n",
      " [ 0.04629945]\n",
      " [-0.19334277]]\n",
      "Current iteration=4, loss=48569.48290636854\n",
      "t [[ 0.05562722]\n",
      " [-0.29658655]\n",
      " [-0.17163447]\n",
      " ...\n",
      " [ 0.13153255]\n",
      " [ 0.05562722]\n",
      " [-0.23456026]]\n",
      "t [[ 0.05562722]\n",
      " [-0.29658655]\n",
      " [-0.17163447]\n",
      " ...\n",
      " [ 0.13153255]\n",
      " [ 0.05562722]\n",
      " [-0.23456026]]\n",
      "t [[ 0.06418041]\n",
      " [-0.35127001]\n",
      " [-0.20276707]\n",
      " ...\n",
      " [ 0.15130648]\n",
      " [ 0.06418041]\n",
      " [-0.27341608]]\n",
      "t [[ 0.06418041]\n",
      " [-0.35127001]\n",
      " [-0.20276707]\n",
      " ...\n",
      " [ 0.15130648]\n",
      " [ 0.06418041]\n",
      " [-0.27341608]]\n",
      "Current iteration=6, loss=47182.87226097152\n",
      "t [[ 0.07201408]\n",
      " [-0.40449896]\n",
      " [-0.23296842]\n",
      " ...\n",
      " [ 0.16932609]\n",
      " [ 0.07201408]\n",
      " [-0.31011556]]\n",
      "t [[ 0.07201408]\n",
      " [-0.40449896]\n",
      " [-0.23296842]\n",
      " ...\n",
      " [ 0.16932609]\n",
      " [ 0.07201408]\n",
      " [-0.31011556]]\n",
      "t [[ 0.07917941]\n",
      " [-0.4563144 ]\n",
      " [-0.26228851]\n",
      " ...\n",
      " [ 0.18574069]\n",
      " [ 0.07917941]\n",
      " [-0.34484484]]\n",
      "t [[ 0.07917941]\n",
      " [-0.4563144 ]\n",
      " [-0.26228851]\n",
      " ...\n",
      " [ 0.18574069]\n",
      " [ 0.07917941]\n",
      " [-0.34484484]]\n",
      "Current iteration=8, loss=45952.42021947871\n",
      "t [[ 0.08572389]\n",
      " [-0.50675773]\n",
      " [-0.29077393]\n",
      " ...\n",
      " [ 0.20068598]\n",
      " [ 0.08572389]\n",
      " [-0.37777237]]\n",
      "t [[ 0.08572389]\n",
      " [-0.50675773]\n",
      " [-0.29077393]\n",
      " ...\n",
      " [ 0.20068598]\n",
      " [ 0.08572389]\n",
      " [-0.37777237]]\n",
      "t [[ 0.0916915 ]\n",
      " [-0.55587039]\n",
      " [-0.31846809]\n",
      " ...\n",
      " [ 0.21428532]\n",
      " [ 0.0916915 ]\n",
      " [-0.40905052]]\n",
      "loss=44853.90186563758\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03943061]\n",
      " [-0.02298647]\n",
      " [-0.03678772]\n",
      " ...\n",
      " [ 0.03089515]\n",
      " [ 0.01300619]\n",
      " [-0.05234556]]\n",
      "t [[ 0.03943061]\n",
      " [-0.02298647]\n",
      " [-0.03678772]\n",
      " ...\n",
      " [ 0.03089515]\n",
      " [ 0.01300619]\n",
      " [-0.05234556]]\n",
      "t [[ 0.07698721]\n",
      " [-0.04735572]\n",
      " [-0.07232639]\n",
      " ...\n",
      " [ 0.05906012]\n",
      " [ 0.02497381]\n",
      " [-0.10129197]]\n",
      "t [[ 0.07698721]\n",
      " [-0.04735572]\n",
      " [-0.07232639]\n",
      " ...\n",
      " [ 0.05906012]\n",
      " [ 0.02497381]\n",
      " [-0.10129197]]\n",
      "Current iteration=2, loss=50138.271533734434\n",
      "t [[ 0.11276715]\n",
      " [-0.07288319]\n",
      " [-0.10668532]\n",
      " ...\n",
      " [ 0.08472967]\n",
      " [ 0.03597442]\n",
      " [-0.14713563]]\n",
      "t [[ 0.11276715]\n",
      " [-0.07288319]\n",
      " [-0.10668532]\n",
      " ...\n",
      " [ 0.08472967]\n",
      " [ 0.03597442]\n",
      " [-0.14713563]]\n",
      "t [[ 0.14686507]\n",
      " [-0.09936691]\n",
      " [-0.13993025]\n",
      " ...\n",
      " [ 0.10811991]\n",
      " [ 0.04607566]\n",
      " [-0.19014941]]\n",
      "t [[ 0.14686507]\n",
      " [-0.09936691]\n",
      " [-0.13993025]\n",
      " ...\n",
      " [ 0.10811991]\n",
      " [ 0.04607566]\n",
      " [-0.19014941]]\n",
      "Current iteration=4, loss=48562.26098105233\n",
      "t [[ 0.17937228]\n",
      " [-0.12662653]\n",
      " [-0.17212307]\n",
      " ...\n",
      " [ 0.12942848]\n",
      " [ 0.05534108]\n",
      " [-0.230583  ]]\n",
      "t [[ 0.17937228]\n",
      " [-0.12662653]\n",
      " [-0.17212307]\n",
      " ...\n",
      " [ 0.12942848]\n",
      " [ 0.05534108]\n",
      " [-0.230583  ]]\n",
      "t [[ 0.21037627]\n",
      " [-0.15450199]\n",
      " [-0.20332186]\n",
      " ...\n",
      " [ 0.14883542]\n",
      " [ 0.06383012]\n",
      " [-0.26866382]]\n",
      "t [[ 0.21037627]\n",
      " [-0.15450199]\n",
      " [-0.20332186]\n",
      " ...\n",
      " [ 0.14883542]\n",
      " [ 0.06383012]\n",
      " [-0.26866382]]\n",
      "Current iteration=6, loss=47173.659921431354\n",
      "t [[ 0.23996043]\n",
      " [-0.18285204]\n",
      " [-0.23358098]\n",
      " ...\n",
      " [ 0.16650439]\n",
      " [ 0.07159815]\n",
      " [-0.30459838]]\n",
      "t [[ 0.23996043]\n",
      " [-0.18285204]\n",
      " [-0.23358098]\n",
      " ...\n",
      " [ 0.16650439]\n",
      " [ 0.07159815]\n",
      " [-0.30459838]]\n",
      "t [[ 0.26820382]\n",
      " [-0.21155268]\n",
      " [-0.26295112]\n",
      " ...\n",
      " [ 0.18258407]\n",
      " [ 0.07869662]\n",
      " [-0.33857378]]\n",
      "t [[ 0.26820382]\n",
      " [-0.21155268]\n",
      " [-0.26295112]\n",
      " ...\n",
      " [ 0.18258407]\n",
      " [ 0.07869662]\n",
      " [-0.33857378]]\n",
      "Current iteration=8, loss=45941.862502032454\n",
      "t [[ 0.2951811 ]\n",
      " [-0.24049554]\n",
      " [-0.29147959]\n",
      " ...\n",
      " [ 0.19720954]\n",
      " [ 0.08517327]\n",
      " [-0.37075928]]\n",
      "t [[ 0.2951811 ]\n",
      " [-0.24049554]\n",
      " [-0.29147959]\n",
      " ...\n",
      " [ 0.19720954]\n",
      " [ 0.08517327]\n",
      " [-0.37075928]]\n",
      "t [[ 0.32096259]\n",
      " [-0.26958634]\n",
      " [-0.31921043]\n",
      " ...\n",
      " [ 0.21050363]\n",
      " [ 0.0910723 ]\n",
      " [-0.40130788]]\n",
      "loss=44842.44314483965\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03892523]\n",
      " [-0.02200465]\n",
      " [-0.03664995]\n",
      " ...\n",
      " [ 0.03137183]\n",
      " [ 0.01324591]\n",
      " [-0.05395755]]\n",
      "t [[ 0.03892523]\n",
      " [-0.02200465]\n",
      " [-0.03664995]\n",
      " ...\n",
      " [ 0.03137183]\n",
      " [ 0.01324591]\n",
      " [-0.05395755]]\n",
      "t [[ 0.07600135]\n",
      " [-0.04539835]\n",
      " [-0.07205085]\n",
      " ...\n",
      " [ 0.0599388 ]\n",
      " [ 0.02544074]\n",
      " [-0.10441316]]\n",
      "t [[ 0.07600135]\n",
      " [-0.04539835]\n",
      " [-0.07205085]\n",
      " ...\n",
      " [ 0.0599388 ]\n",
      " [ 0.02544074]\n",
      " [-0.10441316]]\n",
      "Current iteration=2, loss=50151.237797442234\n",
      "t [[ 0.1113247 ]\n",
      " [-0.06995545]\n",
      " [-0.10627247]\n",
      " ...\n",
      " [ 0.08594237]\n",
      " [ 0.03665701]\n",
      " [-0.15167121]]\n",
      "t [[ 0.1113247 ]\n",
      " [-0.06995545]\n",
      " [-0.10627247]\n",
      " ...\n",
      " [ 0.08594237]\n",
      " [ 0.03665701]\n",
      " [-0.15167121]]\n",
      "t [[ 0.14498884]\n",
      " [-0.09547324]\n",
      " [-0.13938092]\n",
      " ...\n",
      " [ 0.10960519]\n",
      " [ 0.04696329]\n",
      " [-0.19601216]]\n",
      "t [[ 0.14498884]\n",
      " [-0.09547324]\n",
      " [-0.13938092]\n",
      " ...\n",
      " [ 0.10960519]\n",
      " [ 0.04696329]\n",
      " [-0.19601216]]\n",
      "Current iteration=4, loss=48588.46032141039\n",
      "t [[ 0.17708397]\n",
      " [-0.12177086]\n",
      " [-0.17143846]\n",
      " ...\n",
      " [ 0.1311311 ]\n",
      " [ 0.05642398]\n",
      " [-0.23769281]]\n",
      "t [[ 0.17708397]\n",
      " [-0.12177086]\n",
      " [-0.17143846]\n",
      " ...\n",
      " [ 0.1311311 ]\n",
      " [ 0.05642398]\n",
      " [-0.23769281]]\n",
      "t [[ 0.20769648]\n",
      " [-0.14868802]\n",
      " [-0.20250346]\n",
      " ...\n",
      " [ 0.15070588]\n",
      " [ 0.06509929]\n",
      " [-0.27694717]]\n",
      "t [[ 0.20769648]\n",
      " [-0.14868802]\n",
      " [-0.20250346]\n",
      " ...\n",
      " [ 0.15070588]\n",
      " [ 0.06509929]\n",
      " [-0.27694717]]\n",
      "Current iteration=6, loss=47212.90024761482\n",
      "t [[ 0.23690869]\n",
      " [-0.17608332]\n",
      " [-0.2326305 ]\n",
      " ...\n",
      " [ 0.16849837]\n",
      " [ 0.07304531]\n",
      " [-0.31398775]]\n",
      "t [[ 0.23690869]\n",
      " [-0.17608332]\n",
      " [-0.2326305 ]\n",
      " ...\n",
      " [ 0.16849837]\n",
      " [ 0.07304531]\n",
      " [-0.31398775]]\n",
      "t [[ 0.26479869]\n",
      " [-0.20383273]\n",
      " [-0.26187049]\n",
      " ...\n",
      " [ 0.18466183]\n",
      " [ 0.08031415]\n",
      " [-0.34900708]]\n",
      "t [[ 0.26479869]\n",
      " [-0.20383273]\n",
      " [-0.26187049]\n",
      " ...\n",
      " [ 0.18466183]\n",
      " [ 0.08031415]\n",
      " [-0.34900708]]\n",
      "Current iteration=8, loss=45993.68282144274\n",
      "t [[ 0.29144023]\n",
      " [-0.23182793]\n",
      " [-0.29027085]\n",
      " ...\n",
      " [ 0.19933534]\n",
      " [ 0.08695412]\n",
      " [-0.38217935]]\n",
      "t [[ 0.29144023]\n",
      " [-0.23182793]\n",
      " [-0.29027085]\n",
      " ...\n",
      " [ 0.19933534]\n",
      " [ 0.08695412]\n",
      " [-0.38217935]]\n",
      "t [[ 0.31690279]\n",
      " [-0.25997472]\n",
      " [-0.31787572]\n",
      " ...\n",
      " [ 0.21264516]\n",
      " [ 0.09300995]\n",
      " [-0.41366197]]\n",
      "loss=44906.23879999541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.03886292]\n",
      " [-0.02311808]\n",
      " [-0.03653483]\n",
      " ...\n",
      " [ 0.05597483]\n",
      " [-0.03653483]\n",
      " [ 0.05242836]]\n",
      "t [[ 0.03886292]\n",
      " [-0.02311808]\n",
      " [-0.03653483]\n",
      " ...\n",
      " [ 0.05597483]\n",
      " [-0.03653483]\n",
      " [ 0.05242836]]\n",
      "t [[ 0.07588012]\n",
      " [-0.04754927]\n",
      " [-0.07183149]\n",
      " ...\n",
      " [ 0.1089615 ]\n",
      " [-0.07183149]\n",
      " [ 0.10157374]]\n",
      "t [[ 0.07588012]\n",
      " [-0.04754927]\n",
      " [-0.07183149]\n",
      " ...\n",
      " [ 0.1089615 ]\n",
      " [-0.07183149]\n",
      " [ 0.10157374]]\n",
      "Current iteration=2, loss=50154.564879058424\n",
      "t [[ 0.11114873]\n",
      " [-0.07307479]\n",
      " [-0.10595851]\n",
      " ...\n",
      " [ 0.15914427]\n",
      " [-0.10595851]\n",
      " [ 0.1476674 ]]\n",
      "t [[ 0.11114873]\n",
      " [-0.07307479]\n",
      " [-0.10595851]\n",
      " ...\n",
      " [ 0.15914427]\n",
      " [-0.10595851]\n",
      " [ 0.1476674 ]]\n",
      "t [[ 0.14476255]\n",
      " [-0.09949854]\n",
      " [-0.13898085]\n",
      " ...\n",
      " [ 0.20669812]\n",
      " [-0.13898085]\n",
      " [ 0.19092666]]\n",
      "t [[ 0.14476255]\n",
      " [-0.09949854]\n",
      " [-0.13898085]\n",
      " ...\n",
      " [ 0.20669812]\n",
      " [-0.13898085]\n",
      " [ 0.19092666]]\n",
      "Current iteration=4, loss=48592.9934842592\n",
      "t [[ 0.17681147]\n",
      " [-0.12664606]\n",
      " [-0.17095976]\n",
      " ...\n",
      " [ 0.25178821]\n",
      " [-0.17095976]\n",
      " [ 0.23155457]]\n",
      "t [[ 0.17681147]\n",
      " [-0.12664606]\n",
      " [-0.17095976]\n",
      " ...\n",
      " [ 0.25178821]\n",
      " [-0.17095976]\n",
      " [ 0.23155457]]\n",
      "t [[ 0.20738119]\n",
      " [-0.15436312]\n",
      " [-0.20195273]\n",
      " ...\n",
      " [ 0.29456972]\n",
      " [-0.20195273]\n",
      " [ 0.26973994]]\n",
      "t [[ 0.20738119]\n",
      " [-0.15436312]\n",
      " [-0.20195273]\n",
      " ...\n",
      " [ 0.29456972]\n",
      " [-0.20195273]\n",
      " [ 0.26973994]]\n",
      "Current iteration=6, loss=47217.27028246354\n",
      "t [[ 0.23655305]\n",
      " [-0.18251405]\n",
      " [-0.23201356]\n",
      " ...\n",
      " [ 0.3351879 ]\n",
      " [-0.23201356]\n",
      " [ 0.30565777]]\n",
      "t [[ 0.23655305]\n",
      " [-0.18251405]\n",
      " [-0.23201356]\n",
      " ...\n",
      " [ 0.3351879 ]\n",
      " [-0.23201356]\n",
      " [ 0.30565777]]\n",
      "t [[ 0.26440401]\n",
      " [-0.21098011]\n",
      " [-0.26119248]\n",
      " ...\n",
      " [ 0.37377837]\n",
      " [-0.26119248]\n",
      " [ 0.33946984]]\n",
      "t [[ 0.26440401]\n",
      " [-0.21098011]\n",
      " [-0.26119248]\n",
      " ...\n",
      " [ 0.37377837]\n",
      " [-0.26119248]\n",
      " [ 0.33946984]]\n",
      "Current iteration=8, loss=45997.036044600434\n",
      "t [[ 0.2910067 ]\n",
      " [-0.23965777]\n",
      " [-0.28953633]\n",
      " ...\n",
      " [ 0.41046757]\n",
      " [-0.28953633]\n",
      " [ 0.37132551]]\n",
      "t [[ 0.2910067 ]\n",
      " [-0.23965777]\n",
      " [-0.28953633]\n",
      " ...\n",
      " [ 0.41046757]\n",
      " [-0.28953633]\n",
      " [ 0.37132551]]\n",
      "t [[ 0.31642956]\n",
      " [-0.26845712]\n",
      " [-0.31708873]\n",
      " ...\n",
      " [ 0.44537316]\n",
      " [-0.31708873]\n",
      " [ 0.40136244]]\n",
      "loss=44908.070116017254\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01492281]\n",
      " [-0.07145783]\n",
      " [-0.04190369]\n",
      " ...\n",
      " [ 0.03583322]\n",
      " [ 0.01492281]\n",
      " [-0.06074147]]\n",
      "t [[ 0.01492281]\n",
      " [-0.07145783]\n",
      " [-0.04190369]\n",
      " ...\n",
      " [ 0.03583322]\n",
      " [ 0.01492281]\n",
      " [-0.06074147]]\n",
      "t [[ 0.02849373]\n",
      " [-0.14077724]\n",
      " [-0.08219333]\n",
      " ...\n",
      " [ 0.06807516]\n",
      " [ 0.02849373]\n",
      " [-0.1170413 ]]\n",
      "t [[ 0.02849373]\n",
      " [-0.14077724]\n",
      " [-0.08219333]\n",
      " ...\n",
      " [ 0.06807516]\n",
      " [ 0.02849373]\n",
      " [-0.1170413 ]]\n",
      "Current iteration=2, loss=49900.52673212012\n",
      "t [[ 0.04081925]\n",
      " [-0.20800605]\n",
      " [-0.12097118]\n",
      " ...\n",
      " [ 0.09707786]\n",
      " [ 0.04081925]\n",
      " [-0.16933974]]\n",
      "t [[ 0.04081925]\n",
      " [-0.20800605]\n",
      " [-0.12097118]\n",
      " ...\n",
      " [ 0.09707786]\n",
      " [ 0.04081925]\n",
      " [-0.16933974]]\n",
      "t [[ 0.05199903]\n",
      " [-0.27319713]\n",
      " [-0.15833323]\n",
      " ...\n",
      " [ 0.12316099]\n",
      " [ 0.05199903]\n",
      " [-0.21803662]]\n",
      "t [[ 0.05199903]\n",
      " [-0.27319713]\n",
      " [-0.15833323]\n",
      " ...\n",
      " [ 0.12316099]\n",
      " [ 0.05199903]\n",
      " [-0.21803662]]\n",
      "Current iteration=4, loss=48148.327632222354\n",
      "t [[ 0.06212562]\n",
      " [-0.33640712]\n",
      " [-0.1943689 ]\n",
      " ...\n",
      " [ 0.14661257]\n",
      " [ 0.06212562]\n",
      " [-0.26349228]]\n",
      "t [[ 0.06212562]\n",
      " [-0.33640712]\n",
      " [-0.1943689 ]\n",
      " ...\n",
      " [ 0.14661257]\n",
      " [ 0.06212562]\n",
      " [-0.26349228]]\n",
      "t [[ 0.07128453]\n",
      " [-0.39769535]\n",
      " [-0.22916116]\n",
      " ...\n",
      " [ 0.16769099]\n",
      " [ 0.07128453]\n",
      " [-0.30602982]]\n",
      "t [[ 0.07128453]\n",
      " [-0.39769535]\n",
      " [-0.22916116]\n",
      " ...\n",
      " [ 0.16769099]\n",
      " [ 0.07128453]\n",
      " [-0.30602982]]\n",
      "Current iteration=6, loss=46628.75427374328\n",
      "t [[ 0.07955443]\n",
      " [-0.45712284]\n",
      " [-0.26278669]\n",
      " ...\n",
      " [ 0.18662762]\n",
      " [ 0.07955443]\n",
      " [-0.34593797]]\n",
      "t [[ 0.07955443]\n",
      " [-0.45712284]\n",
      " [-0.26278669]\n",
      " ...\n",
      " [ 0.18662762]\n",
      " [ 0.07955443]\n",
      " [-0.34593797]]\n",
      "t [[ 0.08700752]\n",
      " [-0.51475146]\n",
      " [-0.29531627]\n",
      " ...\n",
      " [ 0.20362967]\n",
      " [ 0.08700752]\n",
      " [-0.38347421]]\n",
      "t [[ 0.08700752]\n",
      " [-0.51475146]\n",
      " [-0.29531627]\n",
      " ...\n",
      " [ 0.20362967]\n",
      " [ 0.08700752]\n",
      " [-0.38347421]]\n",
      "Current iteration=8, loss=45299.607411363206\n",
      "t [[ 0.09370992]\n",
      " [-0.57064323]\n",
      " [-0.32681511]\n",
      " ...\n",
      " [ 0.21888279]\n",
      " [ 0.09370992]\n",
      " [-0.41886787]]\n",
      "t [[ 0.09370992]\n",
      " [-0.57064323]\n",
      " [-0.32681511]\n",
      " ...\n",
      " [ 0.21888279]\n",
      " [ 0.09370992]\n",
      " [-0.41886787]]\n",
      "t [[ 0.09972214]\n",
      " [-0.6248598 ]\n",
      " [-0.35734331]\n",
      " ...\n",
      " [ 0.23255351]\n",
      " [ 0.09972214]\n",
      " [-0.45232306]]\n",
      "loss=44128.15089609153\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.04506356]\n",
      " [-0.02627025]\n",
      " [-0.04204311]\n",
      " ...\n",
      " [ 0.03530875]\n",
      " [ 0.01486422]\n",
      " [-0.0598235 ]]\n",
      "t [[ 0.04506356]\n",
      " [-0.02627025]\n",
      " [-0.04204311]\n",
      " ...\n",
      " [ 0.03530875]\n",
      " [ 0.01486422]\n",
      " [-0.0598235 ]]\n",
      "t [[ 0.08767971]\n",
      " [-0.05434634]\n",
      " [-0.08245496]\n",
      " ...\n",
      " [ 0.06705189]\n",
      " [ 0.02837209]\n",
      " [-0.11520779]]\n",
      "t [[ 0.08767971]\n",
      " [-0.05434634]\n",
      " [-0.08245496]\n",
      " ...\n",
      " [ 0.06705189]\n",
      " [ 0.02837209]\n",
      " [-0.11520779]]\n",
      "Current iteration=2, loss=49895.71636173284\n",
      "t [[ 0.12799448]\n",
      " [-0.08389262]\n",
      " [-0.12133942]\n",
      " ...\n",
      " [ 0.0955806 ]\n",
      " [ 0.04063077]\n",
      " [-0.16659636]]\n",
      "t [[ 0.12799448]\n",
      " [-0.08389262]\n",
      " [-0.12133942]\n",
      " ...\n",
      " [ 0.0955806 ]\n",
      " [ 0.04063077]\n",
      " [-0.16659636]]\n",
      "t [[ 0.16614894]\n",
      " [-0.1146125 ]\n",
      " [-0.15879396]\n",
      " ...\n",
      " [ 0.12121353]\n",
      " [ 0.05174055]\n",
      " [-0.21439178]]\n",
      "t [[ 0.16614894]\n",
      " [-0.1146125 ]\n",
      " [-0.15879396]\n",
      " ...\n",
      " [ 0.12121353]\n",
      " [ 0.05174055]\n",
      " [-0.21439178]]\n",
      "Current iteration=4, loss=48140.4204338345\n",
      "t [[ 0.20227804]\n",
      " [-0.14624625]\n",
      " [-0.19490943]\n",
      " ...\n",
      " [ 0.14423766]\n",
      " [ 0.06179455]\n",
      " [-0.25895668]]\n",
      "t [[ 0.20227804]\n",
      " [-0.14624625]\n",
      " [-0.19490943]\n",
      " ...\n",
      " [ 0.14423766]\n",
      " [ 0.06179455]\n",
      " [-0.25895668]]\n",
      "t [[ 0.23650985]\n",
      " [-0.17856823]\n",
      " [-0.22977003]\n",
      " ...\n",
      " [ 0.16491032]\n",
      " [ 0.07087875]\n",
      " [-0.30061609]]\n",
      "t [[ 0.23650985]\n",
      " [-0.17856823]\n",
      " [-0.22977003]\n",
      " ...\n",
      " [ 0.16491032]\n",
      " [ 0.07087875]\n",
      " [-0.30061609]]\n",
      "Current iteration=6, loss=46618.86468929843\n",
      "t [[ 0.26896506]\n",
      " [-0.21138377]\n",
      " [-0.26345363]\n",
      " ...\n",
      " [ 0.18346195]\n",
      " [ 0.07907228]\n",
      " [-0.33966029]]\n",
      "t [[ 0.26896506]\n",
      " [-0.21138377]\n",
      " [-0.26345363]\n",
      " ...\n",
      " [ 0.18346195]\n",
      " [ 0.07907228]\n",
      " [-0.33966029]]\n",
      "t [[ 0.29975678]\n",
      " [-0.24452597]\n",
      " [-0.29603203]\n",
      " ...\n",
      " [ 0.20009882]\n",
      " [ 0.08644771]\n",
      " [-0.376348  ]]\n",
      "t [[ 0.29975678]\n",
      " [-0.24452597]\n",
      " [-0.29603203]\n",
      " ...\n",
      " [ 0.20009882]\n",
      " [ 0.08644771]\n",
      " [-0.376348  ]]\n",
      "Current iteration=8, loss=45288.45970494573\n",
      "t [[ 0.32899057]\n",
      " [-0.2778526 ]\n",
      " [-0.32757141]\n",
      " ...\n",
      " [ 0.21500575]\n",
      " [ 0.09307148]\n",
      " [-0.41090952]]\n",
      "t [[ 0.32899057]\n",
      " [-0.2778526 ]\n",
      " [-0.32757141]\n",
      " ...\n",
      " [ 0.21500575]\n",
      " [ 0.09307148]\n",
      " [-0.41090952]]\n",
      "t [[ 0.3567647 ]\n",
      " [-0.31124311]\n",
      " [-0.35813272]\n",
      " ...\n",
      " [ 0.22834843]\n",
      " [ 0.09900437]\n",
      " [-0.4435497 ]]\n",
      "loss=44116.21789129369\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04448598]\n",
      " [-0.02514818]\n",
      " [-0.04188566]\n",
      " ...\n",
      " [ 0.03585351]\n",
      " [ 0.01513818]\n",
      " [-0.06166577]]\n",
      "t [[ 0.04448598]\n",
      " [-0.02514818]\n",
      " [-0.04188566]\n",
      " ...\n",
      " [ 0.03585351]\n",
      " [ 0.01513818]\n",
      " [-0.06166577]]\n",
      "t [[ 0.08655707]\n",
      " [-0.05211037]\n",
      " [-0.08214006]\n",
      " ...\n",
      " [ 0.06804395]\n",
      " [ 0.02890368]\n",
      " [-0.11875812]]\n",
      "t [[ 0.08655707]\n",
      " [-0.05211037]\n",
      " [-0.08214006]\n",
      " ...\n",
      " [ 0.06804395]\n",
      " [ 0.02890368]\n",
      " [-0.11875812]]\n",
      "Current iteration=2, loss=49910.565232028064\n",
      "t [[ 0.12635777]\n",
      " [-0.08054933]\n",
      " [-0.12086775]\n",
      " ...\n",
      " [ 0.09693253]\n",
      " [ 0.04140511]\n",
      " [-0.17173251]]\n",
      "t [[ 0.12635777]\n",
      " [-0.08054933]\n",
      " [-0.12086775]\n",
      " ...\n",
      " [ 0.09693253]\n",
      " [ 0.04140511]\n",
      " [-0.17173251]]\n",
      "t [[ 0.16402751]\n",
      " [-0.11016742]\n",
      " [-0.1581668 ]\n",
      " ...\n",
      " [ 0.12284768]\n",
      " [ 0.05274411]\n",
      " [-0.22100278]]\n",
      "t [[ 0.16402751]\n",
      " [-0.11016742]\n",
      " [-0.1581668 ]\n",
      " ...\n",
      " [ 0.12284768]\n",
      " [ 0.05274411]\n",
      " [-0.22100278]]\n",
      "Current iteration=4, loss=48170.39028866656\n",
      "t [[ 0.19969957]\n",
      " [-0.14070432]\n",
      " [-0.1941285 ]\n",
      " ...\n",
      " [ 0.14608543]\n",
      " [ 0.06301504]\n",
      " [-0.26694198]]\n",
      "t [[ 0.19969957]\n",
      " [-0.14070432]\n",
      " [-0.1941285 ]\n",
      " ...\n",
      " [ 0.14608543]\n",
      " [ 0.06301504]\n",
      " [-0.26694198]]\n",
      "t [[ 0.2335004 ]\n",
      " [-0.17193408]\n",
      " [-0.2288375 ]\n",
      " ...\n",
      " [ 0.16691134]\n",
      " [ 0.07230503]\n",
      " [-0.30988454]]\n",
      "t [[ 0.2335004 ]\n",
      " [-0.17193408]\n",
      " [-0.2288375 ]\n",
      " ...\n",
      " [ 0.16691134]\n",
      " [ 0.07230503]\n",
      " [-0.30988454]]\n",
      "Current iteration=6, loss=46663.59487517049\n",
      "t [[ 0.26554919]\n",
      " [-0.20366195]\n",
      " [-0.26237194]\n",
      " ...\n",
      " [ 0.18556297]\n",
      " [ 0.08069418]\n",
      " [-0.35012919]]\n",
      "t [[ 0.26554919]\n",
      " [-0.20366195]\n",
      " [-0.26237194]\n",
      " ...\n",
      " [ 0.18556297]\n",
      " [ 0.08069418]\n",
      " [-0.35012919]]\n",
      "t [[ 0.29595766]\n",
      " [-0.23572107]\n",
      " [-0.29480386]\n",
      " ...\n",
      " [ 0.20225273]\n",
      " [ 0.08825596]\n",
      " [-0.38794215]]\n",
      "t [[ 0.29595766]\n",
      " [-0.23572107]\n",
      " [-0.29480386]\n",
      " ...\n",
      " [ 0.20225273]\n",
      " [ 0.08825596]\n",
      " [-0.38794215]]\n",
      "Current iteration=8, loss=45347.25497652479\n",
      "t [[ 0.32483012]\n",
      " [-0.26796932]\n",
      " [-0.3261996 ]\n",
      " ...\n",
      " [ 0.21717064]\n",
      " [ 0.09505761]\n",
      " [-0.42356035]]\n",
      "t [[ 0.32483012]\n",
      " [-0.26796932]\n",
      " [-0.3261996 ]\n",
      " ...\n",
      " [ 0.21717064]\n",
      " [ 0.09505761]\n",
      " [-0.42356035]]\n",
      "t [[ 0.35226365]\n",
      " [-0.30028631]\n",
      " [-0.35662019]\n",
      " ...\n",
      " [ 0.23048684]\n",
      " [ 0.10116062]\n",
      " [-0.45719455]]\n",
      "loss=44188.22990267445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.04441477]\n",
      " [-0.02642067]\n",
      " [-0.04175409]\n",
      " ...\n",
      " [ 0.06397123]\n",
      " [-0.04175409]\n",
      " [ 0.05991812]]\n",
      "t [[ 0.04441477]\n",
      " [-0.02642067]\n",
      " [-0.04175409]\n",
      " ...\n",
      " [ 0.06397123]\n",
      " [-0.04175409]\n",
      " [ 0.05991812]]\n",
      "t [[ 0.08641905]\n",
      " [-0.0545562 ]\n",
      " [-0.08189114]\n",
      " ...\n",
      " [ 0.12403999]\n",
      " [-0.08189114]\n",
      " [ 0.11554876]]\n",
      "t [[ 0.08641905]\n",
      " [-0.0545562 ]\n",
      " [-0.08189114]\n",
      " ...\n",
      " [ 0.12403999]\n",
      " [-0.08189114]\n",
      " [ 0.11554876]]\n",
      "Current iteration=2, loss=49914.21623142274\n",
      "t [[ 0.12615847]\n",
      " [-0.08407968]\n",
      " [-0.12051381]\n",
      " ...\n",
      " [ 0.18048227]\n",
      " [-0.12051381]\n",
      " [ 0.16723817]]\n",
      "t [[ 0.12615847]\n",
      " [-0.08407968]\n",
      " [-0.12051381]\n",
      " ...\n",
      " [ 0.18048227]\n",
      " [-0.12051381]\n",
      " [ 0.16723817]]\n",
      "t [[ 0.1637726 ]\n",
      " [-0.11470337]\n",
      " [-0.15771849]\n",
      " ...\n",
      " [ 0.23355774]\n",
      " [-0.15771849]\n",
      " [ 0.21530821]]\n",
      "t [[ 0.1637726 ]\n",
      " [-0.11470337]\n",
      " [-0.15771849]\n",
      " ...\n",
      " [ 0.23355774]\n",
      " [-0.15771849]\n",
      " [ 0.21530821]]\n",
      "Current iteration=4, loss=48175.043061741424\n",
      "t [[ 0.19939405]\n",
      " [-0.1461764 ]\n",
      " [-0.19359506]\n",
      " ...\n",
      " [ 0.28350878]\n",
      " [-0.19359506]\n",
      " [ 0.26005585]]\n",
      "t [[ 0.19939405]\n",
      " [-0.1461764 ]\n",
      " [-0.19359506]\n",
      " ...\n",
      " [ 0.28350878]\n",
      " [-0.19359506]\n",
      " [ 0.26005585]]\n",
      "t [[ 0.23314795]\n",
      " [-0.17828168]\n",
      " [-0.22822689]\n",
      " ...\n",
      " [ 0.33056046]\n",
      " [-0.22822689]\n",
      " [ 0.30175361]]\n",
      "t [[ 0.23314795]\n",
      " [-0.17828168]\n",
      " [-0.22822689]\n",
      " ...\n",
      " [ 0.33056046]\n",
      " [-0.22822689]\n",
      " [ 0.30175361]]\n",
      "Current iteration=6, loss=46667.6587085491\n",
      "t [[ 0.26515186]\n",
      " [-0.21083262]\n",
      " [-0.26169107]\n",
      " ...\n",
      " [ 0.37492094]\n",
      " [-0.26169107]\n",
      " [ 0.34065071]]\n",
      "t [[ 0.26515186]\n",
      " [-0.21083262]\n",
      " [-0.26169107]\n",
      " ...\n",
      " [ 0.37492094]\n",
      " [-0.26169107]\n",
      " [ 0.34065071]]\n",
      "t [[ 0.29551583]\n",
      " [-0.24366968]\n",
      " [-0.29405873]\n",
      " ...\n",
      " [ 0.41678224]\n",
      " [-0.29405873]\n",
      " [ 0.37697444]]\n",
      "t [[ 0.29551583]\n",
      " [-0.24366968]\n",
      " [-0.29405873]\n",
      " ...\n",
      " [ 0.41678224]\n",
      " [-0.29405873]\n",
      " [ 0.37697444]]\n",
      "Current iteration=8, loss=45349.822003538226\n",
      "t [[ 0.32434259]\n",
      " [-0.27665722]\n",
      " [-0.32539542]\n",
      " ...\n",
      " [ 0.45632118]\n",
      " [-0.32539542]\n",
      " [ 0.4109318 ]]\n",
      "t [[ 0.32434259]\n",
      " [-0.27665722]\n",
      " [-0.32539542]\n",
      " ...\n",
      " [ 0.45632118]\n",
      " [-0.32539542]\n",
      " [ 0.4109318 ]]\n",
      "t [[ 0.35172793]\n",
      " [-0.30968044]\n",
      " [-0.35576149]\n",
      " ...\n",
      " [ 0.49370042]\n",
      " [-0.35576149]\n",
      " [ 0.44271113]]\n",
      "loss=44188.82359307853\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01678816]\n",
      " [-0.08039006]\n",
      " [-0.04714166]\n",
      " ...\n",
      " [ 0.04031237]\n",
      " [ 0.01678816]\n",
      " [-0.06833416]]\n",
      "t [[ 0.01678816]\n",
      " [-0.08039006]\n",
      " [-0.04714166]\n",
      " ...\n",
      " [ 0.04031237]\n",
      " [ 0.01678816]\n",
      " [-0.06833416]]\n",
      "t [[ 0.03186555]\n",
      " [-0.15807389]\n",
      " [-0.09224074]\n",
      " ...\n",
      " [ 0.07608006]\n",
      " [ 0.03186555]\n",
      " [-0.13104756]]\n",
      "t [[ 0.03186555]\n",
      " [-0.15807389]\n",
      " [-0.09224074]\n",
      " ...\n",
      " [ 0.07608006]\n",
      " [ 0.03186555]\n",
      " [-0.13104756]]\n",
      "Current iteration=2, loss=49662.025499829295\n",
      "t [[ 0.04538427]\n",
      " [-0.2331202 ]\n",
      " [-0.13544339]\n",
      " ...\n",
      " [ 0.10780525]\n",
      " [ 0.04538427]\n",
      " [-0.1887685 ]]\n",
      "t [[ 0.04538427]\n",
      " [-0.2331202 ]\n",
      " [-0.13544339]\n",
      " ...\n",
      " [ 0.10780525]\n",
      " [ 0.04538427]\n",
      " [-0.1887685 ]]\n",
      "t [[ 0.05748513]\n",
      " [-0.30560554]\n",
      " [-0.17688532]\n",
      " ...\n",
      " [ 0.13593732]\n",
      " [ 0.05748513]\n",
      " [-0.24205921]]\n",
      "t [[ 0.05748513]\n",
      " [-0.30560554]\n",
      " [-0.17688532]\n",
      " ...\n",
      " [ 0.13593732]\n",
      " [ 0.05748513]\n",
      " [-0.24205921]]\n",
      "Current iteration=4, loss=47740.4726598888\n",
      "t [[ 0.06829736]\n",
      " [-0.37561213]\n",
      " [-0.21669156]\n",
      " ...\n",
      " [ 0.16087478]\n",
      " [ 0.06829736]\n",
      " [-0.29141837]]\n",
      "t [[ 0.06829736]\n",
      " [-0.37561213]\n",
      " [-0.21669156]\n",
      " ...\n",
      " [ 0.16087478]\n",
      " [ 0.06829736]\n",
      " [-0.29141837]]\n",
      "t [[ 0.07793881]\n",
      " [-0.44322588]\n",
      " [-0.25497667]\n",
      " ...\n",
      " [ 0.18296953]\n",
      " [ 0.07793881]\n",
      " [-0.33728577]]\n",
      "t [[ 0.07793881]\n",
      " [-0.44322588]\n",
      " [-0.25497667]\n",
      " ...\n",
      " [ 0.18296953]\n",
      " [ 0.07793881]\n",
      " [-0.33728577]]\n",
      "Current iteration=6, loss=46100.223877781056\n",
      "t [[ 0.08651651]\n",
      " [-0.50853471]\n",
      " [-0.29184532]\n",
      " ...\n",
      " [ 0.20253199]\n",
      " [ 0.08651651]\n",
      " [-0.38004794]]\n",
      "t [[ 0.08651651]\n",
      " [-0.50853471]\n",
      " [-0.29184532]\n",
      " ...\n",
      " [ 0.20253199]\n",
      " [ 0.08651651]\n",
      " [-0.38004794]]\n",
      "t [[ 0.0941274 ]\n",
      " [-0.57162718]\n",
      " [-0.32739292]\n",
      " ...\n",
      " [ 0.21983613]\n",
      " [ 0.0941274 ]\n",
      " [-0.42004394]]\n",
      "t [[ 0.0941274 ]\n",
      " [-0.57162718]\n",
      " [-0.32739292]\n",
      " ...\n",
      " [ 0.21983613]\n",
      " [ 0.0941274 ]\n",
      " [-0.42004394]]\n",
      "Current iteration=8, loss=44685.23046101026\n",
      "t [[ 0.10085915]\n",
      " [-0.6325914 ]\n",
      " [-0.36170645]\n",
      " ...\n",
      " [ 0.23512405]\n",
      " [ 0.10085915]\n",
      " [-0.45757088]]\n",
      "t [[ 0.10085915]\n",
      " [-0.6325914 ]\n",
      " [-0.36170645]\n",
      " ...\n",
      " [ 0.23512405]\n",
      " [ 0.10085915]\n",
      " [-0.45757088]]\n",
      "t [[ 0.10679096]\n",
      " [-0.69151426]\n",
      " [-0.39486513]\n",
      " ...\n",
      " [ 0.2486099 ]\n",
      " [ 0.10679096]\n",
      " [-0.49288906]]\n",
      "loss=43453.24171901587\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.0506965 ]\n",
      " [-0.02955403]\n",
      " [-0.0472985 ]\n",
      " ...\n",
      " [ 0.03972234]\n",
      " [ 0.01672225]\n",
      " [-0.06730144]]\n",
      "t [[ 0.0506965 ]\n",
      " [-0.02955403]\n",
      " [-0.0472985 ]\n",
      " ...\n",
      " [ 0.03972234]\n",
      " [ 0.01672225]\n",
      " [-0.06730144]]\n",
      "t [[ 0.09829591]\n",
      " [-0.06139323]\n",
      " [-0.09253266]\n",
      " ...\n",
      " [ 0.07493249]\n",
      " [ 0.03172808]\n",
      " [-0.12898521]]\n",
      "t [[ 0.09829591]\n",
      " [-0.06139323]\n",
      " [-0.09253266]\n",
      " ...\n",
      " [ 0.07493249]\n",
      " [ 0.03172808]\n",
      " [-0.12898521]]\n",
      "Current iteration=2, loss=49656.71955040064\n",
      "t [[ 0.14300711]\n",
      " [-0.09503912]\n",
      " [-0.13585089]\n",
      " ...\n",
      " [ 0.10613137]\n",
      " [ 0.04517056]\n",
      " [-0.1856842 ]]\n",
      "t [[ 0.14300711]\n",
      " [-0.09503912]\n",
      " [-0.13585089]\n",
      " ...\n",
      " [ 0.10613137]\n",
      " [ 0.04517056]\n",
      " [-0.1856842 ]]\n",
      "t [[ 0.18503053]\n",
      " [-0.13007656]\n",
      " [-0.17739105]\n",
      " ...\n",
      " [ 0.13376688]\n",
      " [ 0.05719138]\n",
      " [-0.23796446]]\n",
      "t [[ 0.18503053]\n",
      " [-0.13007656]\n",
      " [-0.17739105]\n",
      " ...\n",
      " [ 0.13376688]\n",
      " [ 0.05719138]\n",
      " [-0.23796446]]\n",
      "Current iteration=4, loss=47731.94695920196\n",
      "t [[ 0.22455616]\n",
      " [-0.16614935]\n",
      " [-0.21728009]\n",
      " ...\n",
      " [ 0.15823605]\n",
      " [ 0.06792055]\n",
      " [-0.28632778]]\n",
      "t [[ 0.22455616]\n",
      " [-0.16614935]\n",
      " [-0.21728009]\n",
      " ...\n",
      " [ 0.15823605]\n",
      " [ 0.06792055]\n",
      " [-0.28632778]]\n",
      "t [[ 0.26176242]\n",
      " [-0.20295481]\n",
      " [-0.25563432]\n",
      " ...\n",
      " [ 0.17988934]\n",
      " [ 0.07747657]\n",
      " [-0.33121648]]\n",
      "t [[ 0.26176242]\n",
      " [-0.20295481]\n",
      " [-0.25563432]\n",
      " ...\n",
      " [ 0.17988934]\n",
      " [ 0.07747657]\n",
      " [-0.33121648]]\n",
      "Current iteration=6, loss=46089.759503792164\n",
      "t [[ 0.29681548]\n",
      " [-0.24023791]\n",
      " [-0.29255996]\n",
      " ...\n",
      " [ 0.19903584]\n",
      " [ 0.08596706]\n",
      " [-0.37301902]]\n",
      "t [[ 0.29681548]\n",
      " [-0.24023791]\n",
      " [-0.29255996]\n",
      " ...\n",
      " [ 0.19903584]\n",
      " [ 0.08596706]\n",
      " [-0.37301902]]\n",
      "t [[ 0.32986925]\n",
      " [-0.27778541]\n",
      " [-0.32815385]\n",
      " ...\n",
      " [ 0.21594829]\n",
      " [ 0.09348944]\n",
      " [-0.41207598]]\n",
      "t [[ 0.32986925]\n",
      " [-0.27778541]\n",
      " [-0.32815385]\n",
      " ...\n",
      " [ 0.21594829]\n",
      " [ 0.09348944]\n",
      " [-0.41207598]]\n",
      "Current iteration=8, loss=44673.61469946476\n",
      "t [[ 0.36106569]\n",
      " [-0.3154203 ]\n",
      " [-0.36250418]\n",
      " ...\n",
      " [ 0.2308676 ]\n",
      " [ 0.10013179]\n",
      " [-0.44868556]]\n",
      "t [[ 0.36106569]\n",
      " [-0.3154203 ]\n",
      " [-0.36250418]\n",
      " ...\n",
      " [ 0.2308676 ]\n",
      " [ 0.10013179]\n",
      " [-0.44868556]]\n",
      "t [[ 0.39053543]\n",
      " [-0.35299663]\n",
      " [-0.39569128]\n",
      " ...\n",
      " [ 0.24400677]\n",
      " [ 0.10597366]\n",
      " [-0.48310883]]\n",
      "loss=43440.96157365698\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05004672]\n",
      " [-0.0282917 ]\n",
      " [-0.04712136]\n",
      " ...\n",
      " [ 0.0403352 ]\n",
      " [ 0.01703045]\n",
      " [-0.06937399]]\n",
      "t [[ 0.05004672]\n",
      " [-0.0282917 ]\n",
      " [-0.04712136]\n",
      " ...\n",
      " [ 0.0403352 ]\n",
      " [ 0.01703045]\n",
      " [-0.06937399]]\n",
      "t [[ 0.0970375 ]\n",
      " [-0.05887891]\n",
      " [-0.0921784 ]\n",
      " ...\n",
      " [ 0.0760349 ]\n",
      " [ 0.03232383]\n",
      " [-0.13296051]]\n",
      "t [[ 0.0970375 ]\n",
      " [-0.05887891]\n",
      " [-0.0921784 ]\n",
      " ...\n",
      " [ 0.0760349 ]\n",
      " [ 0.03232383]\n",
      " [-0.13296051]]\n",
      "Current iteration=2, loss=49673.456262111424\n",
      "t [[ 0.14117902]\n",
      " [-0.09128091]\n",
      " [-0.1353205 ]\n",
      " ...\n",
      " [ 0.10761447]\n",
      " [ 0.04603526]\n",
      " [-0.19140958]]\n",
      "t [[ 0.14117902]\n",
      " [-0.09128091]\n",
      " [-0.1353205 ]\n",
      " ...\n",
      " [ 0.10761447]\n",
      " [ 0.04603526]\n",
      " [-0.19140958]]\n",
      "t [[ 0.18266933]\n",
      " [-0.12508115]\n",
      " [-0.1766863 ]\n",
      " ...\n",
      " [ 0.13553567]\n",
      " [ 0.05830837]\n",
      " [-0.2453032 ]]\n",
      "t [[ 0.18266933]\n",
      " [-0.12508115]\n",
      " [-0.1766863 ]\n",
      " ...\n",
      " [ 0.13553567]\n",
      " [ 0.05830837]\n",
      " [-0.2453032 ]]\n",
      "Current iteration=4, loss=47765.67275303522\n",
      "t [[ 0.22169606]\n",
      " [-0.15992274]\n",
      " [-0.21640342]\n",
      " ...\n",
      " [ 0.16020812]\n",
      " [ 0.06927487]\n",
      " [-0.29515763]]\n",
      "t [[ 0.22169606]\n",
      " [-0.15992274]\n",
      " [-0.21640342]\n",
      " ...\n",
      " [ 0.16020812]\n",
      " [ 0.06927487]\n",
      " [-0.29515763]]\n",
      "t [[ 0.25843537]\n",
      " [-0.19550271]\n",
      " [-0.2545887 ]\n",
      " ...\n",
      " [ 0.18199332]\n",
      " [ 0.0790548 ]\n",
      " [-0.341428  ]]\n",
      "t [[ 0.25843537]\n",
      " [-0.19550271]\n",
      " [-0.2545887 ]\n",
      " ...\n",
      " [ 0.18199332]\n",
      " [ 0.0790548 ]\n",
      " [-0.341428  ]]\n",
      "Current iteration=6, loss=46139.89613927779\n",
      "t [[ 0.2930514 ]\n",
      " [-0.23156601]\n",
      " [-0.29134871]\n",
      " ...\n",
      " [ 0.20120972]\n",
      " [ 0.08775712]\n",
      " [-0.38451405]]\n",
      "t [[ 0.2930514 ]\n",
      " [-0.23156601]\n",
      " [-0.29134871]\n",
      " ...\n",
      " [ 0.20120972]\n",
      " [ 0.08775712]\n",
      " [-0.38451405]]\n",
      "t [[ 0.3256962 ]\n",
      " [-0.26789954]\n",
      " [-0.32678053]\n",
      " ...\n",
      " [ 0.21813783]\n",
      " [ 0.09548042]\n",
      " [-0.42476617]]\n",
      "t [[ 0.3256962 ]\n",
      " [-0.26789954]\n",
      " [-0.32678053]\n",
      " ...\n",
      " [ 0.21813783]\n",
      " [ 0.09548042]\n",
      " [-0.42476617]]\n",
      "Current iteration=8, loss=44739.19121299153\n",
      "t [[ 0.35651006]\n",
      " [-0.30432653]\n",
      " [-0.3609725 ]\n",
      " ...\n",
      " [ 0.23302506]\n",
      " [ 0.10231381]\n",
      " [-0.46249113]]\n",
      "t [[ 0.35651006]\n",
      " [-0.30432653]\n",
      " [-0.3609725 ]\n",
      " ...\n",
      " [ 0.23302506]\n",
      " [ 0.10231381]\n",
      " [-0.46249113]]\n",
      "t [[ 0.38562203]\n",
      " [-0.34070133]\n",
      " [-0.39400502]\n",
      " ...\n",
      " [ 0.24608986]\n",
      " [ 0.10833772]\n",
      " [-0.49795748]]\n",
      "loss=43520.859947786004\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.04996661]\n",
      " [-0.02972325]\n",
      " [-0.04697335]\n",
      " ...\n",
      " [ 0.07196764]\n",
      " [-0.04697335]\n",
      " [ 0.06740789]]\n",
      "t [[ 0.04996661]\n",
      " [-0.02972325]\n",
      " [-0.04697335]\n",
      " ...\n",
      " [ 0.07196764]\n",
      " [-0.04697335]\n",
      " [ 0.06740789]]\n",
      "t [[ 0.09688282]\n",
      " [-0.06161657]\n",
      " [-0.09190036]\n",
      " ...\n",
      " [ 0.1389968 ]\n",
      " [-0.09190036]\n",
      " [ 0.1293901 ]]\n",
      "t [[ 0.09688282]\n",
      " [-0.06161657]\n",
      " [-0.09190036]\n",
      " ...\n",
      " [ 0.1389968 ]\n",
      " [-0.09190036]\n",
      " [ 0.1293901 ]]\n",
      "Current iteration=2, loss=49677.396888184914\n",
      "t [[ 0.14095682]\n",
      " [-0.09521406]\n",
      " [-0.13492772]\n",
      " ...\n",
      " [ 0.20148181]\n",
      " [-0.13492772]\n",
      " [ 0.18644114]]\n",
      "t [[ 0.14095682]\n",
      " [-0.09521406]\n",
      " [-0.13492772]\n",
      " ...\n",
      " [ 0.20148181]\n",
      " [-0.13492772]\n",
      " [ 0.18644114]]\n",
      "t [[ 0.1823866 ]\n",
      " [-0.1301133 ]\n",
      " [-0.17619175]\n",
      " ...\n",
      " [ 0.25978991]\n",
      " [-0.17619175]\n",
      " [ 0.23901533]]\n",
      "t [[ 0.1823866 ]\n",
      " [-0.1301133 ]\n",
      " [-0.17619175]\n",
      " ...\n",
      " [ 0.25978991]\n",
      " [-0.17619175]\n",
      " [ 0.23901533]]\n",
      "Current iteration=4, loss=47770.34884106092\n",
      "t [[ 0.22135848]\n",
      " [-0.16597068]\n",
      " [-0.21581808]\n",
      " ...\n",
      " [ 0.31426003]\n",
      " [-0.21581808]\n",
      " [ 0.28752648]]\n",
      "t [[ 0.22135848]\n",
      " [-0.16597068]\n",
      " [-0.21581808]\n",
      " ...\n",
      " [ 0.31426003]\n",
      " [-0.21581808]\n",
      " [ 0.28752648]]\n",
      "t [[ 0.25804651]\n",
      " [-0.20249549]\n",
      " [-0.25392189]\n",
      " ...\n",
      " [ 0.36520307]\n",
      " [-0.25392189]\n",
      " [ 0.33234923]]\n",
      "t [[ 0.25804651]\n",
      " [-0.20249549]\n",
      " [-0.25392189]\n",
      " ...\n",
      " [ 0.36520307]\n",
      " [-0.25392189]\n",
      " [ 0.33234923]]\n",
      "Current iteration=6, loss=46143.52313539799\n",
      "t [[ 0.29261241]\n",
      " [-0.23944367]\n",
      " [-0.29060838]\n",
      " ...\n",
      " [ 0.41290306]\n",
      " [-0.29060838]\n",
      " [ 0.37382144]]\n",
      "t [[ 0.29261241]\n",
      " [-0.23944367]\n",
      " [-0.29060838]\n",
      " ...\n",
      " [ 0.41290306]\n",
      " [-0.29060838]\n",
      " [ 0.37382144]]\n",
      "t [[ 0.32520596]\n",
      " [-0.27661172]\n",
      " [-0.32597344]\n",
      " ...\n",
      " [ 0.4576188 ]\n",
      " [-0.32597344]\n",
      " [ 0.41224711]]\n",
      "t [[ 0.32520596]\n",
      " [-0.27661172]\n",
      " [-0.32597344]\n",
      " ...\n",
      " [ 0.4576188 ]\n",
      " [-0.32597344]\n",
      " [ 0.41224711]]\n",
      "Current iteration=8, loss=44740.84485166036\n",
      "t [[ 0.3559655 ]\n",
      " [-0.31383102]\n",
      " [-0.36010443]\n",
      " ...\n",
      " [ 0.49958577]\n",
      " [-0.36010443]\n",
      " [ 0.44789941]]\n",
      "t [[ 0.3559655 ]\n",
      " [-0.31383102]\n",
      " [-0.36010443]\n",
      " ...\n",
      " [ 0.49958577]\n",
      " [-0.36010443]\n",
      " [ 0.44789941]]\n",
      "t [[ 0.38501862]\n",
      " [-0.35096276]\n",
      " [-0.39308091]\n",
      " ...\n",
      " [ 0.53901807]\n",
      " [-0.39308091]\n",
      " [ 0.48102368]]\n",
      "loss=43520.12146939334\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.01865351]\n",
      " [-0.08932229]\n",
      " [-0.05237962]\n",
      " ...\n",
      " [ 0.04479153]\n",
      " [ 0.01865351]\n",
      " [-0.07592684]]\n",
      "t [[ 0.01865351]\n",
      " [-0.08932229]\n",
      " [-0.05237962]\n",
      " ...\n",
      " [ 0.04479153]\n",
      " [ 0.01865351]\n",
      " [-0.07592684]]\n",
      "t [[ 0.03519526]\n",
      " [-0.17530385]\n",
      " [-0.10223785]\n",
      " ...\n",
      " [ 0.08397305]\n",
      " [ 0.03519526]\n",
      " [-0.14491546]]\n",
      "t [[ 0.03519526]\n",
      " [-0.17530385]\n",
      " [-0.10223785]\n",
      " ...\n",
      " [ 0.08397305]\n",
      " [ 0.03519526]\n",
      " [-0.14491546]]\n",
      "Current iteration=2, loss=49427.00821183044\n",
      "t [[ 0.0498345 ]\n",
      " [-0.25803974]\n",
      " [-0.14977582]\n",
      " ...\n",
      " [ 0.11823466]\n",
      " [ 0.0498345 ]\n",
      " [-0.20782951]]\n",
      "t [[ 0.0498345 ]\n",
      " [-0.25803974]\n",
      " [-0.14977582]\n",
      " ...\n",
      " [ 0.11823466]\n",
      " [ 0.0498345 ]\n",
      " [-0.20782951]]\n",
      "t [[ 0.0627628 ]\n",
      " [-0.33763668]\n",
      " [-0.19517833]\n",
      " ...\n",
      " [ 0.14818454]\n",
      " [ 0.0627628 ]\n",
      " [-0.26543021]]\n",
      "t [[ 0.0627628 ]\n",
      " [-0.33763668]\n",
      " [-0.19517833]\n",
      " ...\n",
      " [ 0.14818454]\n",
      " [ 0.0627628 ]\n",
      " [-0.26543021]]\n",
      "Current iteration=4, loss=47345.37962553825\n",
      "t [[ 0.0741538 ]\n",
      " [-0.4142094 ]\n",
      " [-0.23861355]\n",
      " ...\n",
      " [ 0.17435338]\n",
      " [ 0.0741538 ]\n",
      " [-0.31838166]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.0741538 ]\n",
      " [-0.4142094 ]\n",
      " [-0.23861355]\n",
      " ...\n",
      " [ 0.17435338]\n",
      " [ 0.0741538 ]\n",
      " [-0.31838166]]\n",
      "t [[ 0.08416375]\n",
      " [-0.48787737]\n",
      " [-0.28023374]\n",
      " ...\n",
      " [ 0.19720244]\n",
      " [ 0.08416375]\n",
      " [-0.36725971]]\n",
      "t [[ 0.08416375]\n",
      " [-0.48787737]\n",
      " [-0.28023374]\n",
      " ...\n",
      " [ 0.19720244]\n",
      " [ 0.08416375]\n",
      " [-0.36725971]]\n",
      "Current iteration=6, loss=45595.69509237472\n",
      "t [[ 0.09293272]\n",
      " [-0.55876214]\n",
      " [-0.32017629]\n",
      " ...\n",
      " [ 0.21713254]\n",
      " [ 0.09293272]\n",
      " [-0.412562  ]]\n",
      "t [[ 0.09293272]\n",
      " [-0.55876214]\n",
      " [-0.32017629]\n",
      " ...\n",
      " [ 0.21713254]\n",
      " [ 0.09293272]\n",
      " [-0.412562  ]]\n",
      "t [[ 0.10058591]\n",
      " [-0.62698525]\n",
      " [-0.35856503]\n",
      " ...\n",
      " [ 0.23449244]\n",
      " [ 0.10058591]\n",
      " [-0.45471791]]\n",
      "t [[ 0.10058591]\n",
      " [-0.62698525]\n",
      " [-0.35856503]\n",
      " ...\n",
      " [ 0.23449244]\n",
      " [ 0.10058591]\n",
      " [-0.45471791]]\n",
      "Current iteration=8, loss=44106.209831713466\n",
      "t [[ 0.1072351 ]\n",
      " [-0.69266671]\n",
      " [-0.39551151]\n",
      " ...\n",
      " [ 0.24958598]\n",
      " [ 0.1072351 ]\n",
      " [-0.49409763]]\n",
      "t [[ 0.1072351 ]\n",
      " [-0.69266671]\n",
      " [-0.39551151]\n",
      " ...\n",
      " [ 0.24958598]\n",
      " [ 0.1072351 ]\n",
      " [-0.49409763]]\n",
      "t [[ 0.11298005]\n",
      " [-0.75592385]\n",
      " [-0.43111629]\n",
      " ...\n",
      " [ 0.26267818]\n",
      " [ 0.11298005]\n",
      " [-0.53102043]]\n",
      "loss=42824.31107030198\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.05632945]\n",
      " [-0.03283781]\n",
      " [-0.05255389]\n",
      " ...\n",
      " [ 0.04413593]\n",
      " [ 0.01858027]\n",
      " [-0.07477938]]\n",
      "t [[ 0.05632945]\n",
      " [-0.03283781]\n",
      " [-0.05255389]\n",
      " ...\n",
      " [ 0.04413593]\n",
      " [ 0.01858027]\n",
      " [-0.07477938]]\n",
      "t [[ 0.10883587]\n",
      " [-0.06849634]\n",
      " [-0.10255952]\n",
      " ...\n",
      " [ 0.08270197]\n",
      " [ 0.03504182]\n",
      " [-0.14262435]]\n",
      "t [[ 0.10883587]\n",
      " [-0.06849634]\n",
      " [-0.10255952]\n",
      " ...\n",
      " [ 0.08270197]\n",
      " [ 0.03504182]\n",
      " [-0.14262435]]\n",
      "Current iteration=2, loss=49421.2280461105\n",
      "t [[ 0.15780713]\n",
      " [-0.10631854]\n",
      " [-0.15022117]\n",
      " ...\n",
      " [ 0.11638644]\n",
      " [ 0.04959521]\n",
      " [-0.20440487]]\n",
      "t [[ 0.15780713]\n",
      " [-0.10631854]\n",
      " [-0.15022117]\n",
      " ...\n",
      " [ 0.11638644]\n",
      " [ 0.04959521]\n",
      " [-0.20440487]]\n",
      "t [[ 0.2035173 ]\n",
      " [-0.145745  ]\n",
      " [-0.19572651]\n",
      " ...\n",
      " [ 0.14579547]\n",
      " [ 0.06243323]\n",
      " [-0.26088727]]\n",
      "t [[ 0.2035173 ]\n",
      " [-0.145745  ]\n",
      " [-0.19572651]\n",
      " ...\n",
      " [ 0.14579547]\n",
      " [ 0.06243323]\n",
      " [-0.26088727]]\n",
      "Current iteration=4, loss=47336.296670123236\n",
      "t [[ 0.24622379]\n",
      " [-0.18630581]\n",
      " [-0.23924632]\n",
      " ...\n",
      " [ 0.17145773]\n",
      " [ 0.07373051]\n",
      " [-0.3127397 ]]\n",
      "t [[ 0.24622379]\n",
      " [-0.18630581]\n",
      " [-0.23924632]\n",
      " ...\n",
      " [ 0.17145773]\n",
      " [ 0.07373051]\n",
      " [-0.3127397 ]]\n",
      "t [[ 0.28616554]\n",
      " [-0.22761077]\n",
      " [-0.28093516]\n",
      " ...\n",
      " [ 0.19383256]\n",
      " [ 0.08364419]\n",
      " [-0.36054116]]\n",
      "t [[ 0.28616554]\n",
      " [-0.22761077]\n",
      " [-0.28093516]\n",
      " ...\n",
      " [ 0.19383256]\n",
      " [ 0.08364419]\n",
      " [-0.36054116]]\n",
      "Current iteration=6, loss=45584.74483515097\n",
      "t [[ 0.32356252]\n",
      " [-0.26933912]\n",
      " [-0.32093246]\n",
      " ...\n",
      " [ 0.21331904]\n",
      " [ 0.09231505]\n",
      " [-0.40479167]]\n",
      "t [[ 0.32356252]\n",
      " [-0.26933912]\n",
      " [-0.32093246]\n",
      " ...\n",
      " [ 0.21331904]\n",
      " [ 0.09231505]\n",
      " [-0.40479167]]\n",
      "t [[ 0.35861599]\n",
      " [-0.3112296 ]\n",
      " [-0.35936382]\n",
      " ...\n",
      " [ 0.23026426]\n",
      " [ 0.0998689 ]\n",
      " [-0.4459223 ]]\n",
      "t [[ 0.35861599]\n",
      " [-0.3112296 ]\n",
      " [-0.35936382]\n",
      " ...\n",
      " [ 0.23026426]\n",
      " [ 0.0998689 ]\n",
      " [-0.4459223 ]]\n",
      "Current iteration=8, loss=44094.22634002665\n",
      "t [[ 0.39150945]\n",
      " [-0.35307124]\n",
      " [-0.39634235]\n",
      " ...\n",
      " [ 0.24497047]\n",
      " [ 0.10641801]\n",
      " [-0.48430442]]\n",
      "t [[ 0.39150945]\n",
      " [-0.35307124]\n",
      " [-0.39634235]\n",
      " ...\n",
      " [ 0.24497047]\n",
      " [ 0.10641801]\n",
      " [-0.48430442]]\n",
      "t [[ 0.42240994]\n",
      " [-0.39469513]\n",
      " [-0.43196997]\n",
      " ...\n",
      " [ 0.25770115]\n",
      " [ 0.11206259]\n",
      " [-0.52025796]]\n",
      "loss=42811.78301306418\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05560747]\n",
      " [-0.03143522]\n",
      " [-0.05235707]\n",
      " ...\n",
      " [ 0.04481689]\n",
      " [ 0.01892273]\n",
      " [-0.07708221]]\n",
      "t [[ 0.05560747]\n",
      " [-0.03143522]\n",
      " [-0.05235707]\n",
      " ...\n",
      " [ 0.04481689]\n",
      " [ 0.01892273]\n",
      " [-0.07708221]]\n",
      "t [[ 0.10744272]\n",
      " [-0.06570393]\n",
      " [-0.10216591]\n",
      " ...\n",
      " [ 0.08391173]\n",
      " [ 0.03570121]\n",
      " [-0.14702045]]\n",
      "t [[ 0.10744272]\n",
      " [-0.06570393]\n",
      " [-0.10216591]\n",
      " ...\n",
      " [ 0.08391173]\n",
      " [ 0.03570121]\n",
      " [-0.14702045]]\n",
      "Current iteration=2, loss=49439.85708148578\n",
      "t [[ 0.1557905 ]\n",
      " [-0.10214601]\n",
      " [-0.14963212]\n",
      " ...\n",
      " [ 0.11799283]\n",
      " [ 0.05054892]\n",
      " [-0.21070832]]\n",
      "t [[ 0.1557905 ]\n",
      " [-0.10214601]\n",
      " [-0.14963212]\n",
      " ...\n",
      " [ 0.11799283]\n",
      " [ 0.05054892]\n",
      " [-0.21070832]]\n",
      "t [[ 0.20092166]\n",
      " [-0.14020028]\n",
      " [-0.19494445]\n",
      " ...\n",
      " [ 0.14768522]\n",
      " [ 0.06366121]\n",
      " [-0.26893385]]\n",
      "t [[ 0.20092166]\n",
      " [-0.14020028]\n",
      " [-0.19494445]\n",
      " ...\n",
      " [ 0.14768522]\n",
      " [ 0.06366121]\n",
      " [-0.26893385]]\n",
      "Current iteration=4, loss=47373.758307696175\n",
      "t [[ 0.24309035]\n",
      " [-0.17939604]\n",
      " [-0.23827455]\n",
      " ...\n",
      " [ 0.17353434]\n",
      " [ 0.07521502]\n",
      " [-0.32238447]]\n",
      "t [[ 0.24309035]\n",
      " [-0.17939604]\n",
      " [-0.23827455]\n",
      " ...\n",
      " [ 0.17353434]\n",
      " [ 0.07521502]\n",
      " [-0.32238447]]\n",
      "t [[ 0.28253254]\n",
      " [-0.21934287]\n",
      " [-0.27977758]\n",
      " ...\n",
      " [ 0.19601378]\n",
      " [ 0.08536949]\n",
      " [-0.37165599]]\n",
      "t [[ 0.28253254]\n",
      " [-0.21934287]\n",
      " [-0.27977758]\n",
      " ...\n",
      " [ 0.19601378]\n",
      " [ 0.08536949]\n",
      " [-0.37165599]]\n",
      "Current iteration=6, loss=45640.19418667483\n",
      "t [[ 0.31946554]\n",
      " [-0.2597201 ]\n",
      " [-0.31959338]\n",
      " ...\n",
      " [ 0.21553431]\n",
      " [ 0.09426712]\n",
      " [-0.41726284]]\n",
      "t [[ 0.31946554]\n",
      " [-0.2597201 ]\n",
      " [-0.31959338]\n",
      " ...\n",
      " [ 0.21553431]\n",
      " [ 0.09426712]\n",
      " [-0.41726284]]\n",
      "t [[ 0.3540882 ]\n",
      " [-0.30026675]\n",
      " [-0.35784781]\n",
      " ...\n",
      " [ 0.23245253]\n",
      " [ 0.10203519]\n",
      " [-0.45964848]]\n",
      "t [[ 0.3540882 ]\n",
      " [-0.30026675]\n",
      " [-0.35784781]\n",
      " ...\n",
      " [ 0.23245253]\n",
      " [ 0.10203519]\n",
      " [-0.45964848]]\n",
      "Current iteration=8, loss=44166.38030851222\n",
      "t [[ 0.38658185]\n",
      " [-0.34077225]\n",
      " [-0.39465409]\n",
      " ...\n",
      " [ 0.24707851]\n",
      " [ 0.10878726]\n",
      " [-0.49919491]]\n",
      "t [[ 0.38658185]\n",
      " [-0.34077225]\n",
      " [-0.39465409]\n",
      " ...\n",
      " [ 0.24707851]\n",
      " [ 0.10878726]\n",
      " [-0.49919491]]\n",
      "t [[ 0.41711148]\n",
      " [-0.38106822]\n",
      " [-0.43011415]\n",
      " ...\n",
      " [ 0.25968227]\n",
      " [ 0.11462459]\n",
      " [-0.53623129]]\n",
      "loss=42899.23522536875\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.05551846]\n",
      " [-0.03302584]\n",
      " [-0.05219261]\n",
      " ...\n",
      " [ 0.07996404]\n",
      " [-0.05219261]\n",
      " [ 0.07489765]]\n",
      "t [[ 0.05551846]\n",
      " [-0.03302584]\n",
      " [-0.05219261]\n",
      " ...\n",
      " [ 0.07996404]\n",
      " [-0.05219261]\n",
      " [ 0.07489765]]\n",
      "t [[ 0.10727149]\n",
      " [-0.06873035]\n",
      " [-0.10185919]\n",
      " ...\n",
      " [ 0.15383202]\n",
      " [-0.10185919]\n",
      " [ 0.14309788]]\n",
      "t [[ 0.10727149]\n",
      " [-0.06873035]\n",
      " [-0.10185919]\n",
      " ...\n",
      " [ 0.15383202]\n",
      " [-0.10185919]\n",
      " [ 0.14309788]]\n",
      "Current iteration=2, loss=49444.05420674929\n",
      "t [[ 0.1555458 ]\n",
      " [-0.10647394]\n",
      " [-0.14920163]\n",
      " ...\n",
      " [ 0.2221466 ]\n",
      " [-0.14920163]\n",
      " [ 0.20528091]]\n",
      "t [[ 0.1555458 ]\n",
      " [-0.10647394]\n",
      " [-0.14920163]\n",
      " ...\n",
      " [ 0.2221466 ]\n",
      " [-0.14920163]\n",
      " [ 0.20528091]]\n",
      "t [[ 0.20061179]\n",
      " [-0.14571478]\n",
      " [-0.19440554]\n",
      " ...\n",
      " [ 0.28540787]\n",
      " [-0.19440554]\n",
      " [ 0.26206424]]\n",
      "t [[ 0.20061179]\n",
      " [-0.14571478]\n",
      " [-0.19440554]\n",
      " ...\n",
      " [ 0.28540787]\n",
      " [-0.19440554]\n",
      " [ 0.26206424]]\n",
      "Current iteration=4, loss=47378.37136080803\n",
      "t [[ 0.24272142]\n",
      " [-0.18600013]\n",
      " [-0.23763997]\n",
      " ...\n",
      " [ 0.3440719 ]\n",
      " [-0.23763997]\n",
      " [ 0.31400276]]\n",
      "t [[ 0.24272142]\n",
      " [-0.18600013]\n",
      " [-0.23763997]\n",
      " ...\n",
      " [ 0.3440719 ]\n",
      " [-0.23763997]\n",
      " [ 0.31400276]]\n",
      "t [[ 0.28210752]\n",
      " [-0.22695581]\n",
      " [-0.27905798]\n",
      " ...\n",
      " [ 0.39855189]\n",
      " [-0.27905798]\n",
      " [ 0.3615919 ]]\n",
      "t [[ 0.28210752]\n",
      " [-0.22695581]\n",
      " [-0.27905798]\n",
      " ...\n",
      " [ 0.39855189]\n",
      " [-0.27905798]\n",
      " [ 0.3615919 ]]\n",
      "Current iteration=6, loss=45643.277588695615\n",
      "t [[ 0.31898411]\n",
      " [-0.26827527]\n",
      " [-0.31879764]\n",
      " ...\n",
      " [ 0.4492206 ]\n",
      " [-0.31879764]\n",
      " [ 0.40527228]]\n",
      "t [[ 0.31898411]\n",
      " [-0.26827527]\n",
      " [-0.31879764]\n",
      " ...\n",
      " [ 0.4492206 ]\n",
      " [-0.31879764]\n",
      " [ 0.40527228]]\n",
      "t [[ 0.35354721]\n",
      " [-0.3097095 ]\n",
      " [-0.35698337]\n",
      " ...\n",
      " [ 0.49641356]\n",
      " [-0.35698337]\n",
      " [ 0.44543495]]\n",
      "t [[ 0.35354721]\n",
      " [-0.3097095 ]\n",
      " [-0.35698337]\n",
      " ...\n",
      " [ 0.49641356]\n",
      " [-0.35698337]\n",
      " [ 0.44543495]]\n",
      "Current iteration=8, loss=44167.02956812945\n",
      "t [[ 0.38597596]\n",
      " [-0.35105773]\n",
      " [-0.39372719]\n",
      " ...\n",
      " [ 0.54043244]\n",
      " [-0.39372719]\n",
      " [ 0.48242669]]\n",
      "t [[ 0.38597596]\n",
      " [-0.35105773]\n",
      " [-0.39372719]\n",
      " ...\n",
      " [ 0.54043244]\n",
      " [-0.39372719]\n",
      " [ 0.48242669]]\n",
      "t [[ 0.41643392]\n",
      " [-0.3921595 ]\n",
      " [-0.42913001]\n",
      " ...\n",
      " [ 0.58154844]\n",
      " [-0.42913001]\n",
      " [ 0.51655506]]\n",
      "loss=42897.11240708742\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02051886]\n",
      " [-0.09825452]\n",
      " [-0.05761758]\n",
      " ...\n",
      " [ 0.04927068]\n",
      " [ 0.02051886]\n",
      " [-0.08351952]]\n",
      "t [[ 0.02051886]\n",
      " [-0.09825452]\n",
      " [-0.05761758]\n",
      " ...\n",
      " [ 0.04927068]\n",
      " [ 0.02051886]\n",
      " [-0.08351952]]\n",
      "t [[ 0.03848289]\n",
      " [-0.19246716]\n",
      " [-0.11218469]\n",
      " ...\n",
      " [ 0.09175422]\n",
      " [ 0.03848289]\n",
      " [-0.15864511]]\n",
      "t [[ 0.03848289]\n",
      " [-0.19246716]\n",
      " [-0.11218469]\n",
      " ...\n",
      " [ 0.09175422]\n",
      " [ 0.03848289]\n",
      " [-0.15864511]]\n",
      "Current iteration=2, loss=49195.42270237483\n",
      "t [[ 0.05417137]\n",
      " [-0.28276547]\n",
      " [-0.16396989]\n",
      " ...\n",
      " [ 0.12837062]\n",
      " [ 0.05417137]\n",
      " [-0.22652852]]\n",
      "t [[ 0.05417137]\n",
      " [-0.28276547]\n",
      " [-0.16396989]\n",
      " ...\n",
      " [ 0.12837062]\n",
      " [ 0.05417137]\n",
      " [-0.22652852]]\n",
      "t [[ 0.06783705]\n",
      " [-0.36929371]\n",
      " [-0.21321713]\n",
      " ...\n",
      " [ 0.15991802]\n",
      " [ 0.06783705]\n",
      " [-0.28816909]]\n",
      "t [[ 0.06783705]\n",
      " [-0.36929371]\n",
      " [-0.21321713]\n",
      " ...\n",
      " [ 0.15991802]\n",
      " [ 0.06783705]\n",
      " [-0.28816909]]\n",
      "Current iteration=4, loss=46962.53332762555\n",
      "t [[ 0.07970603]\n",
      " [-0.45220691]\n",
      " [-0.26014576]\n",
      " ...\n",
      " [ 0.18708154]\n",
      " [ 0.07970603]\n",
      " [-0.34442399]]\n",
      "t [[ 0.07970603]\n",
      " [-0.45220691]\n",
      " [-0.26014576]\n",
      " ...\n",
      " [ 0.18708154]\n",
      " [ 0.07970603]\n",
      " [-0.34442399]]\n",
      "t [[ 0.08997912]\n",
      " [-0.5316658 ]\n",
      " [-0.30495185]\n",
      " ...\n",
      " [ 0.21044731]\n",
      " [ 0.08997912]\n",
      " [-0.3960238 ]]\n",
      "t [[ 0.08997912]\n",
      " [-0.5316658 ]\n",
      " [-0.30495185]\n",
      " ...\n",
      " [ 0.21044731]\n",
      " [ 0.08997912]\n",
      " [-0.3960238 ]]\n",
      "Current iteration=6, loss=45113.69919208919\n",
      "t [[ 0.09883388]\n",
      " [-0.60783286]\n",
      " [-0.3478102 ]\n",
      " ...\n",
      " [ 0.23051712]\n",
      " [ 0.09883388]\n",
      " [-0.44358925]]\n",
      "t [[ 0.09883388]\n",
      " [-0.60783286]\n",
      " [-0.3478102 ]\n",
      " ...\n",
      " [ 0.23051712]\n",
      " [ 0.09883388]\n",
      " [-0.44358925]]\n",
      "t [[ 0.106427  ]\n",
      " [-0.68086931]\n",
      " [-0.38887654]\n",
      " ...\n",
      " [ 0.24772131]\n",
      " [ 0.106427  ]\n",
      " [-0.48764704]]\n",
      "t [[ 0.106427  ]\n",
      " [-0.68086931]\n",
      " [-0.38887654]\n",
      " ...\n",
      " [ 0.24772131]\n",
      " [ 0.106427  ]\n",
      " [-0.48764704]]\n",
      "Current iteration=8, loss=43559.77094919375\n",
      "t [[ 0.1128966 ]\n",
      " [-0.75093303]\n",
      " [-0.42828958]\n",
      " ...\n",
      " [ 0.26242956]\n",
      " [ 0.1128966 ]\n",
      " [-0.52864407]]\n",
      "t [[ 0.1128966 ]\n",
      " [-0.75093303]\n",
      " [-0.42828958]\n",
      " ...\n",
      " [ 0.26242956]\n",
      " [ 0.1128966 ]\n",
      " [-0.52864407]]\n",
      "t [[ 0.1183645 ]\n",
      " [-0.81817709]\n",
      " [-0.46617313]\n",
      " ...\n",
      " [ 0.27495974]\n",
      " [ 0.1183645 ]\n",
      " [-0.5669597 ]]\n",
      "loss=42237.0771533831\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.06196239]\n",
      " [-0.03612159]\n",
      " [-0.05780928]\n",
      " ...\n",
      " [ 0.04854953]\n",
      " [ 0.0204383 ]\n",
      " [-0.08225731]]\n",
      "t [[ 0.06196239]\n",
      " [-0.03612159]\n",
      " [-0.05780928]\n",
      " ...\n",
      " [ 0.04854953]\n",
      " [ 0.0204383 ]\n",
      " [-0.08225731]]\n",
      "t [[ 0.11929967]\n",
      " [-0.07565561]\n",
      " [-0.11253558]\n",
      " ...\n",
      " [ 0.09036044]\n",
      " [ 0.03831334]\n",
      " [-0.15612531]]\n",
      "t [[ 0.11929967]\n",
      " [-0.07565561]\n",
      " [-0.11253558]\n",
      " ...\n",
      " [ 0.09036044]\n",
      " [ 0.03831334]\n",
      " [-0.15612531]]\n",
      "Current iteration=2, loss=49189.189063778744\n",
      "t [[ 0.17239661]\n",
      " [-0.11772671]\n",
      " [-0.16445167]\n",
      " ...\n",
      " [ 0.12635032]\n",
      " [ 0.05390617]\n",
      " [-0.22276415]]\n",
      "t [[ 0.17239661]\n",
      " [-0.11772671]\n",
      " [-0.16445167]\n",
      " ...\n",
      " [ 0.12635032]\n",
      " [ 0.05390617]\n",
      " [-0.22276415]]\n",
      "t [[ 0.22161671]\n",
      " [-0.16160401]\n",
      " [-0.2138053 ]\n",
      " ...\n",
      " [ 0.15731461]\n",
      " [ 0.06747112]\n",
      " [-0.28317978]]\n",
      "t [[ 0.22161671]\n",
      " [-0.16160401]\n",
      " [-0.2138053 ]\n",
      " ...\n",
      " [ 0.15731461]\n",
      " [ 0.06747112]\n",
      " [-0.28317978]]\n",
      "Current iteration=4, loss=46952.949239538124\n",
      "t [[ 0.26729782]\n",
      " [-0.20668691]\n",
      " [-0.26081918]\n",
      " ...\n",
      " [ 0.18393573]\n",
      " [ 0.0792356 ]\n",
      " [-0.3382345 ]]\n",
      "t [[ 0.26729782]\n",
      " [-0.20668691]\n",
      " [-0.26081918]\n",
      " ...\n",
      " [ 0.18393573]\n",
      " [ 0.0792356 ]\n",
      " [-0.3382345 ]]\n",
      "t [[ 0.30974996]\n",
      " [-0.25248853]\n",
      " [-0.30569235]\n",
      " ...\n",
      " [ 0.20679735]\n",
      " [ 0.08940148]\n",
      " [-0.38866269]]\n",
      "t [[ 0.30974996]\n",
      " [-0.25248853]\n",
      " [-0.30569235]\n",
      " ...\n",
      " [ 0.20679735]\n",
      " [ 0.08940148]\n",
      " [-0.38866269]]\n",
      "Current iteration=6, loss=45102.3401205487\n",
      "t [[ 0.34925503]\n",
      " [-0.29861877]\n",
      " [-0.34860215]\n",
      " ...\n",
      " [ 0.22639899]\n",
      " [ 0.09814722]\n",
      " [-0.43508783]]\n",
      "t [[ 0.34925503]\n",
      " [-0.29861877]\n",
      " [-0.34860215]\n",
      " ...\n",
      " [ 0.22639899]\n",
      " [ 0.09814722]\n",
      " [-0.43508783]]\n",
      "t [[ 0.38606794]\n",
      " [-0.34476849]\n",
      " [-0.3897065 ]\n",
      " ...\n",
      " [ 0.24316884]\n",
      " [ 0.10563024]\n",
      " [-0.47803847]]\n",
      "t [[ 0.38606794]\n",
      " [-0.34476849]\n",
      " [-0.3897065 ]\n",
      " ...\n",
      " [ 0.24316884]\n",
      " [ 0.10563024]\n",
      " [-0.47803847]]\n",
      "Current iteration=8, loss=43547.50217545025\n",
      "t [[ 0.42041848]\n",
      " [-0.39069522]\n",
      " [-0.42914603]\n",
      " ...\n",
      " [ 0.25747448]\n",
      " [ 0.11198925]\n",
      " [-0.51796261]]\n",
      "t [[ 0.42041848]\n",
      " [-0.39069522]\n",
      " [-0.42914603]\n",
      " ...\n",
      " [ 0.25747448]\n",
      " [ 0.11198925]\n",
      " [-0.51796261]]\n",
      "t [[ 0.45251369]\n",
      " [-0.43621088]\n",
      " [-0.46704616]\n",
      " ...\n",
      " [ 0.26963175]\n",
      " [ 0.11734656]\n",
      " [-0.55524012]]\n",
      "loss=42224.378413405764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06116822]\n",
      " [-0.03457874]\n",
      " [-0.05759278]\n",
      " ...\n",
      " [ 0.04929858]\n",
      " [ 0.020815  ]\n",
      " [-0.08479043]]\n",
      "t [[ 0.06116822]\n",
      " [-0.03457874]\n",
      " [-0.05759278]\n",
      " ...\n",
      " [ 0.04929858]\n",
      " [ 0.020815  ]\n",
      " [-0.08479043]]\n",
      "t [[ 0.11777277]\n",
      " [-0.07258537]\n",
      " [-0.11210263]\n",
      " ...\n",
      " [ 0.09167453]\n",
      " [ 0.03903588]\n",
      " [-0.16093806]]\n",
      "t [[ 0.11777277]\n",
      " [-0.07258537]\n",
      " [-0.11210263]\n",
      " ...\n",
      " [ 0.09167453]\n",
      " [ 0.03903588]\n",
      " [-0.16093806]]\n",
      "Current iteration=2, loss=49209.714165054975\n",
      "t [[ 0.17019428]\n",
      " [-0.11314046]\n",
      " [-0.16380406]\n",
      " ...\n",
      " [ 0.12807228]\n",
      " [ 0.05494755]\n",
      " [-0.2296347 ]]\n",
      "t [[ 0.17019428]\n",
      " [-0.11314046]\n",
      " [-0.16380406]\n",
      " ...\n",
      " [ 0.12807228]\n",
      " [ 0.05494755]\n",
      " [-0.2296347 ]]\n",
      "t [[ 0.21879187]\n",
      " [-0.15551096]\n",
      " [-0.21294626]\n",
      " ...\n",
      " [ 0.15931218]\n",
      " [ 0.06880774]\n",
      " [-0.29191488]]\n",
      "t [[ 0.21879187]\n",
      " [-0.15551096]\n",
      " [-0.21294626]\n",
      " ...\n",
      " [ 0.15931218]\n",
      " [ 0.06880774]\n",
      " [-0.29191488]]\n",
      "Current iteration=4, loss=46994.12169708167\n",
      "t [[ 0.26389914]\n",
      " [-0.19909546]\n",
      " [-0.25975299]\n",
      " ...\n",
      " [ 0.18609823]\n",
      " [ 0.08084683]\n",
      " [-0.34866583]]\n",
      "t [[ 0.26389914]\n",
      " [-0.19909546]\n",
      " [-0.25975299]\n",
      " ...\n",
      " [ 0.18609823]\n",
      " [ 0.08084683]\n",
      " [-0.34866583]]\n",
      "t [[ 0.30582231]\n",
      " [-0.24340693]\n",
      " [-0.30442399]\n",
      " ...\n",
      " [ 0.20903182]\n",
      " [ 0.09126923]\n",
      " [-0.40064318]]\n",
      "t [[ 0.30582231]\n",
      " [-0.24340693]\n",
      " [-0.30442399]\n",
      " ...\n",
      " [ 0.20903182]\n",
      " [ 0.09126923]\n",
      " [-0.40064318]]\n",
      "Current iteration=6, loss=45163.00041507535\n",
      "t [[ 0.3448399 ]\n",
      " [-0.28805554]\n",
      " [-0.34713707]\n",
      " ...\n",
      " [ 0.22862661]\n",
      " [ 0.10025553]\n",
      " [-0.4484883 ]]\n",
      "t [[ 0.3448399 ]\n",
      " [-0.28805554]\n",
      " [-0.34713707]\n",
      " ...\n",
      " [ 0.22862661]\n",
      " [ 0.10025553]\n",
      " [-0.4484883 ]]\n",
      "t [[ 0.38120378]\n",
      " [-0.33273262]\n",
      " [-0.38805035]\n",
      " ...\n",
      " [ 0.24532212]\n",
      " [ 0.10796494]\n",
      " [-0.4927449 ]]\n",
      "t [[ 0.38120378]\n",
      " [-0.33273262]\n",
      " [-0.38805035]\n",
      " ...\n",
      " [ 0.24532212]\n",
      " [ 0.10796494]\n",
      " [-0.4927449 ]]\n",
      "Current iteration=8, loss=43626.024187674506\n",
      "t [[ 0.41514098]\n",
      " [-0.37719637]\n",
      " [-0.42730451]\n",
      " ...\n",
      " [ 0.2594951 ]\n",
      " [ 0.11453772]\n",
      " [-0.53387381]]\n",
      "t [[ 0.41514098]\n",
      " [-0.37719637]\n",
      " [-0.42730451]\n",
      " ...\n",
      " [ 0.2594951 ]\n",
      " [ 0.11453772]\n",
      " [-0.53387381]]\n",
      "t [[ 0.44685591]\n",
      " [-0.42125948]\n",
      " [-0.46502493]\n",
      " ...\n",
      " [ 0.2714691 ]\n",
      " [ 0.12009745]\n",
      " [-0.57226593]]\n",
      "loss=42319.055444430116\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.0610703 ]\n",
      " [-0.03632842]\n",
      " [-0.05741187]\n",
      " ...\n",
      " [ 0.08796044]\n",
      " [-0.05741187]\n",
      " [ 0.08238742]]\n",
      "t [[ 0.0610703 ]\n",
      " [-0.03632842]\n",
      " [-0.05741187]\n",
      " ...\n",
      " [ 0.08796044]\n",
      " [-0.05741187]\n",
      " [ 0.08238742]]\n",
      "t [[ 0.11758512]\n",
      " [-0.07589749]\n",
      " [-0.11176766]\n",
      " ...\n",
      " [ 0.16854576]\n",
      " [-0.11176766]\n",
      " [ 0.1566722 ]]\n",
      "t [[ 0.11758512]\n",
      " [-0.07589749]\n",
      " [-0.11176766]\n",
      " ...\n",
      " [ 0.16854576]\n",
      " [-0.11176766]\n",
      " [ 0.1566722 ]]\n",
      "Current iteration=2, loss=49214.13581450993\n",
      "t [[ 0.16992746]\n",
      " [-0.11785529]\n",
      " [-0.16333694]\n",
      " ...\n",
      " [ 0.24248043]\n",
      " [-0.16333694]\n",
      " [ 0.22376215]]\n",
      "t [[ 0.16992746]\n",
      " [-0.11785529]\n",
      " [-0.16333694]\n",
      " ...\n",
      " [ 0.24248043]\n",
      " [-0.16333694]\n",
      " [ 0.22376215]]\n",
      "t [[ 0.21845544]\n",
      " [-0.16149452]\n",
      " [-0.2123648 ]\n",
      " ...\n",
      " [ 0.31042481]\n",
      " [-0.2123648 ]\n",
      " [ 0.28447104]]\n",
      "t [[ 0.21845544]\n",
      " [-0.16149452]\n",
      " [-0.2123648 ]\n",
      " ...\n",
      " [ 0.31042481]\n",
      " [-0.2123648 ]\n",
      " [ 0.28447104]]\n",
      "Current iteration=4, loss=46998.594585061684\n",
      "t [[ 0.26349929]\n",
      " [-0.20623724]\n",
      " [-0.25907169]\n",
      " ...\n",
      " [ 0.37297379]\n",
      " [-0.25907169]\n",
      " [ 0.33952013]]\n",
      "t [[ 0.26349929]\n",
      " [-0.20623724]\n",
      " [-0.25907169]\n",
      " ...\n",
      " [ 0.37297379]\n",
      " [-0.25907169]\n",
      " [ 0.33952013]]\n",
      "t [[ 0.30536084]\n",
      " [-0.25161713]\n",
      " [-0.30365474]\n",
      " ...\n",
      " [ 0.43065948]\n",
      " [-0.30365474]\n",
      " [ 0.38954428]]\n",
      "t [[ 0.30536084]\n",
      " [-0.25161713]\n",
      " [-0.30365474]\n",
      " ...\n",
      " [ 0.43065948]\n",
      " [-0.30365474]\n",
      " [ 0.38954428]]\n",
      "Current iteration=6, loss=45165.454094129964\n",
      "t [[ 0.34431449]\n",
      " [-0.29726188]\n",
      " [-0.34628959]\n",
      " ...\n",
      " [ 0.48395598]\n",
      " [-0.34628959]\n",
      " [ 0.43510025]]\n",
      "t [[ 0.34431449]\n",
      " [-0.29726188]\n",
      " [-0.34628959]\n",
      " ...\n",
      " [ 0.48395598]\n",
      " [-0.34628959]\n",
      " [ 0.43510025]]\n",
      "t [[ 0.3806088 ]\n",
      " [-0.3428771 ]\n",
      " [-0.38713265]\n",
      " ...\n",
      " [ 0.53328479]\n",
      " [-0.38713265]\n",
      " [ 0.47667549]]\n",
      "t [[ 0.3806088 ]\n",
      " [-0.3428771 ]\n",
      " [-0.38713265]\n",
      " ...\n",
      " [ 0.53328479]\n",
      " [-0.38713265]\n",
      " [ 0.47667549]]\n",
      "Current iteration=8, loss=43625.60691114378\n",
      "t [[ 0.4144686 ]\n",
      " [-0.3882323 ]\n",
      " [-0.42632318]\n",
      " ...\n",
      " [ 0.57902051]\n",
      " [-0.42632318]\n",
      " [ 0.51469666]]\n",
      "t [[ 0.4144686 ]\n",
      " [-0.3882323 ]\n",
      " [-0.42632318]\n",
      " ...\n",
      " [ 0.57902051]\n",
      " [-0.42632318]\n",
      " [ 0.51469666]]\n",
      "t [[ 0.44609701]\n",
      " [-0.43314899]\n",
      " [-0.46398537]\n",
      " ...\n",
      " [ 0.62149614]\n",
      " [-0.46398537]\n",
      " [ 0.54953752]]\n",
      "loss=42315.52689475991\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02238421]\n",
      " [-0.10718675]\n",
      " [-0.06285554]\n",
      " ...\n",
      " [ 0.05374983]\n",
      " [ 0.02238421]\n",
      " [-0.09111221]]\n",
      "t [[ 0.02238421]\n",
      " [-0.10718675]\n",
      " [-0.06285554]\n",
      " ...\n",
      " [ 0.05374983]\n",
      " [ 0.02238421]\n",
      " [-0.09111221]]\n",
      "t [[ 0.04172848]\n",
      " [-0.20956384]\n",
      " [-0.12208131]\n",
      " ...\n",
      " [ 0.09942368]\n",
      " [ 0.04172848]\n",
      " [-0.17223664]]\n",
      "t [[ 0.04172848]\n",
      " [-0.20956384]\n",
      " [-0.12208131]\n",
      " ...\n",
      " [ 0.09942368]\n",
      " [ 0.04172848]\n",
      " [-0.17223664]]\n",
      "Current iteration=2, loss=48967.21709389023\n",
      "t [[ 0.05839633]\n",
      " [-0.30729822]\n",
      " [-0.17802701]\n",
      " ...\n",
      " [ 0.13821765]\n",
      " [ 0.05839633]\n",
      " [-0.24487129]]\n",
      "t [[ 0.05839633]\n",
      " [-0.30729822]\n",
      " [-0.17802701]\n",
      " ...\n",
      " [ 0.13821765]\n",
      " [ 0.05839633]\n",
      " [-0.24487129]]\n",
      "t [[ 0.07271284]\n",
      " [-0.40057987]\n",
      " [-0.23100661]\n",
      " ...\n",
      " [ 0.1711529 ]\n",
      " [ 0.07271284]\n",
      " [-0.31029502]]\n",
      "t [[ 0.07271284]\n",
      " [-0.40057987]\n",
      " [-0.23100661]\n",
      " ...\n",
      " [ 0.1711529 ]\n",
      " [ 0.07271284]\n",
      " [-0.31029502]]\n",
      "Current iteration=4, loss=46591.44107788619\n",
      "t [[ 0.08496493]\n",
      " [-0.48961277]\n",
      " [-0.28129887]\n",
      " ...\n",
      " [ 0.19909142]\n",
      " [ 0.08496493]\n",
      " [-0.36958585]]\n",
      "t [[ 0.08496493]\n",
      " [-0.48961277]\n",
      " [-0.28129887]\n",
      " ...\n",
      " [ 0.19909142]\n",
      " [ 0.08496493]\n",
      " [-0.36958585]]\n",
      "t [[ 0.09540397]\n",
      " [-0.57460732]\n",
      " [-0.32914983]\n",
      " ...\n",
      " [ 0.22275911]\n",
      " [ 0.09540397]\n",
      " [-0.42364669]]\n",
      "t [[ 0.09540397]\n",
      " [-0.57460732]\n",
      " [-0.32914983]\n",
      " ...\n",
      " [ 0.22275911]\n",
      " [ 0.09540397]\n",
      " [-0.42364669]]\n",
      "Current iteration=6, loss=44652.87585871165\n",
      "t [[ 0.10424927]\n",
      " [-0.65577468]\n",
      " [-0.37477622]\n",
      " ...\n",
      " [ 0.24276823]\n",
      " [ 0.10424927]\n",
      " [-0.47323164]]\n",
      "t [[ 0.10424927]\n",
      " [-0.65577468]\n",
      " [-0.37477622]\n",
      " ...\n",
      " [ 0.24276823]\n",
      " [ 0.10424927]\n",
      " [-0.47323164]]\n",
      "t [[ 0.11169185]\n",
      " [-0.73332272]\n",
      " [-0.41836883]\n",
      " ...\n",
      " [ 0.25963632]\n",
      " [ 0.11169185]\n",
      " [-0.51897006]]\n",
      "t [[ 0.11169185]\n",
      " [-0.73332272]\n",
      " [-0.41836883]\n",
      " ...\n",
      " [ 0.25963632]\n",
      " [ 0.11169185]\n",
      " [-0.51897006]]\n",
      "Current iteration=8, loss=43043.41009914562\n",
      "t [[ 0.11789808]\n",
      " [-0.8074533 ]\n",
      " [-0.4600959 ]\n",
      " ...\n",
      " [ 0.27380159]\n",
      " [ 0.11789808]\n",
      " [-0.56138737]]\n",
      "t [[ 0.11789808]\n",
      " [-0.8074533 ]\n",
      " [-0.4600959 ]\n",
      " ...\n",
      " [ 0.27380159]\n",
      " [ 0.11789808]\n",
      " [-0.56138737]]\n",
      "t [[ 0.12301295]\n",
      " [-0.87836045]\n",
      " [-0.50010602]\n",
      " ...\n",
      " [ 0.28563557]\n",
      " [ 0.12301295]\n",
      " [-0.60092279]]\n",
      "loss=41687.75824607316\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.06759533]\n",
      " [-0.03940537]\n",
      " [-0.06306467]\n",
      " ...\n",
      " [ 0.05296312]\n",
      " [ 0.02229633]\n",
      " [-0.08973525]]\n",
      "t [[ 0.06759533]\n",
      " [-0.03940537]\n",
      " [-0.06306467]\n",
      " ...\n",
      " [ 0.05296312]\n",
      " [ 0.02229633]\n",
      " [-0.08973525]]\n",
      "t [[ 0.12968738]\n",
      " [-0.08287098]\n",
      " [-0.12246089]\n",
      " ...\n",
      " [ 0.09790797]\n",
      " [ 0.04154268]\n",
      " [-0.16948823]]\n",
      "t [[ 0.12968738]\n",
      " [-0.08287098]\n",
      " [-0.12246089]\n",
      " ...\n",
      " [ 0.09790797]\n",
      " [ 0.04154268]\n",
      " [-0.16948823]]\n",
      "Current iteration=2, loss=48960.55011462279\n",
      "t [[ 0.18677771]\n",
      " [-0.12925946]\n",
      " [-0.17854384]\n",
      " ...\n",
      " [ 0.13602754]\n",
      " [ 0.05810491]\n",
      " [-0.24076784]]\n",
      "t [[ 0.18677771]\n",
      " [-0.12925946]\n",
      " [-0.17854384]\n",
      " ...\n",
      " [ 0.13602754]\n",
      " [ 0.05810491]\n",
      " [-0.24076784]]\n",
      "t [[ 0.23933621]\n",
      " [-0.17764008]\n",
      " [-0.23163237]\n",
      " ...\n",
      " [ 0.16833939]\n",
      " [ 0.07231007]\n",
      " [-0.3048613 ]]\n",
      "t [[ 0.23933621]\n",
      " [-0.17764008]\n",
      " [-0.23163237]\n",
      " ...\n",
      " [ 0.16833939]\n",
      " [ 0.07231007]\n",
      " [-0.3048613 ]]\n",
      "Current iteration=4, loss=46581.40723126406\n",
      "t [[ 0.28779492]\n",
      " [-0.22726523]\n",
      " [-0.28200951]\n",
      " ...\n",
      " [ 0.19570209]\n",
      " [ 0.08444673]\n",
      " [-0.3628529 ]]\n",
      "t [[ 0.28779492]\n",
      " [-0.22726523]\n",
      " [-0.28200951]\n",
      " ...\n",
      " [ 0.19570209]\n",
      " [ 0.08444673]\n",
      " [-0.3628529 ]]\n",
      "t [[ 0.33254552]\n",
      " [-0.27754375]\n",
      " [-0.32992498]\n",
      " ...\n",
      " [ 0.21883845]\n",
      " [ 0.09476758]\n",
      " [-0.41565004]]\n",
      "t [[ 0.33254552]\n",
      " [-0.27754375]\n",
      " [-0.32992498]\n",
      " ...\n",
      " [ 0.21883845]\n",
      " [ 0.09476758]\n",
      " [-0.41565004]]\n",
      "Current iteration=6, loss=44641.174713791115\n",
      "t [[ 0.37393985]\n",
      " [-0.32801451]\n",
      " [-0.3755986 ]\n",
      " ...\n",
      " [ 0.23835784]\n",
      " [ 0.10349298]\n",
      " [-0.46400989]]\n",
      "t [[ 0.37393985]\n",
      " [-0.32801451]\n",
      " [-0.3755986 ]\n",
      " ...\n",
      " [ 0.23835784]\n",
      " [ 0.10349298]\n",
      " [-0.46400989]]\n",
      "t [[ 0.41229231]\n",
      " [-0.37832238]\n",
      " [-0.41922386]\n",
      " ...\n",
      " [ 0.25477504]\n",
      " [ 0.11081483]\n",
      " [-0.50856366]]\n",
      "t [[ 0.41229231]\n",
      " [-0.37832238]\n",
      " [-0.41922386]\n",
      " ...\n",
      " [ 0.25477504]\n",
      " [ 0.11081483]\n",
      " [-0.50856366]]\n",
      "Current iteration=8, loss=43030.92369748092\n",
      "t [[ 0.44788332]\n",
      " [-0.42819737]\n",
      " [-0.4609712 ]\n",
      " ...\n",
      " [ 0.26852558]\n",
      " [ 0.11690015]\n",
      " [-0.54983773]]\n",
      "t [[ 0.44788332]\n",
      " [-0.42819737]\n",
      " [-0.4609712 ]\n",
      " ...\n",
      " [ 0.26852558]\n",
      " [ 0.11690015]\n",
      " [-0.54983773]]\n",
      "t [[ 0.48096303]\n",
      " [-0.47743714]\n",
      " [-0.50099113]\n",
      " ...\n",
      " [ 0.2799784 ]\n",
      " [ 0.12189449]\n",
      " [-0.58827155]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=41674.948677381224\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06672897]\n",
      " [-0.03772226]\n",
      " [-0.06282848]\n",
      " ...\n",
      " [ 0.05378027]\n",
      " [ 0.02270727]\n",
      " [-0.09249865]]\n",
      "t [[ 0.06672897]\n",
      " [-0.03772226]\n",
      " [-0.06282848]\n",
      " ...\n",
      " [ 0.05378027]\n",
      " [ 0.02270727]\n",
      " [-0.09249865]]\n",
      "t [[ 0.12802774]\n",
      " [-0.07952316]\n",
      " [-0.12198859]\n",
      " ...\n",
      " [ 0.09932342]\n",
      " [ 0.04232787]\n",
      " [-0.17471349]]\n",
      "t [[ 0.12802774]\n",
      " [-0.07952316]\n",
      " [-0.12198859]\n",
      " ...\n",
      " [ 0.09932342]\n",
      " [ 0.04232787]\n",
      " [-0.17471349]]\n",
      "Current iteration=2, loss=48982.97429741201\n",
      "t [[ 0.18439247]\n",
      " [-0.12426006]\n",
      " [-0.17783778]\n",
      " ...\n",
      " [ 0.13785752]\n",
      " [ 0.05923264]\n",
      " [-0.24819469]]\n",
      "t [[ 0.18439247]\n",
      " [-0.12426006]\n",
      " [-0.17783778]\n",
      " ...\n",
      " [ 0.13785752]\n",
      " [ 0.05923264]\n",
      " [-0.24819469]]\n",
      "t [[ 0.23628731]\n",
      " [-0.17099965]\n",
      " [-0.2306967 ]\n",
      " ...\n",
      " [ 0.17043214]\n",
      " [ 0.07375304]\n",
      " [-0.31426619]]\n",
      "t [[ 0.23628731]\n",
      " [-0.17099965]\n",
      " [-0.2306967 ]\n",
      " ...\n",
      " [ 0.17043214]\n",
      " [ 0.07375304]\n",
      " [-0.31426619]]\n",
      "Current iteration=4, loss=46626.26111087784\n",
      "t [[ 0.28413891]\n",
      " [-0.21899352]\n",
      " [-0.28084964]\n",
      " ...\n",
      " [ 0.1979328 ]\n",
      " [ 0.08618135]\n",
      " [-0.37404361]]\n",
      "t [[ 0.28413891]\n",
      " [-0.21899352]\n",
      " [-0.28084964]\n",
      " ...\n",
      " [ 0.1979328 ]\n",
      " [ 0.08618135]\n",
      " [-0.37404361]]\n",
      "t [[ 0.32833418]\n",
      " [-0.26765049]\n",
      " [-0.32854712]\n",
      " ...\n",
      " [ 0.22110372]\n",
      " [ 0.09677338]\n",
      " [-0.42846052]]\n",
      "t [[ 0.32833418]\n",
      " [-0.26765049]\n",
      " [-0.32854712]\n",
      " ...\n",
      " [ 0.22110372]\n",
      " [ 0.09677338]\n",
      " [-0.42846052]]\n",
      "Current iteration=6, loss=44706.93810042018\n",
      "t [[ 0.36922077]\n",
      " [-0.31650994]\n",
      " [-0.37400942]\n",
      " ...\n",
      " [ 0.24057096]\n",
      " [ 0.10575209]\n",
      " [-0.47829575]]\n",
      "t [[ 0.36922077]\n",
      " [-0.31650994]\n",
      " [-0.37400942]\n",
      " ...\n",
      " [ 0.24057096]\n",
      " [ 0.10575209]\n",
      " [-0.47829575]]\n",
      "t [[ 0.40710935]\n",
      " [-0.36521752]\n",
      " [-0.41743016]\n",
      " ...\n",
      " [ 0.25686244]\n",
      " [ 0.1133115 ]\n",
      " [-0.52419857]]\n",
      "t [[ 0.40710935]\n",
      " [-0.36521752]\n",
      " [-0.41743016]\n",
      " ...\n",
      " [ 0.25686244]\n",
      " [ 0.1133115 ]\n",
      " [-0.52419857]]\n",
      "Current iteration=8, loss=43115.6022086139\n",
      "t [[ 0.44227686]\n",
      " [-0.41350419]\n",
      " [-0.45897977]\n",
      " ...\n",
      " [ 0.27042435]\n",
      " [ 0.11962044]\n",
      " [-0.56671051]]\n",
      "t [[ 0.44227686]\n",
      " [-0.41350419]\n",
      " [-0.45897977]\n",
      " ...\n",
      " [ 0.27042435]\n",
      " [ 0.11962044]\n",
      " [-0.56671051]]\n",
      "t [[ 0.47497014]\n",
      " [-0.4611687 ]\n",
      " [-0.49880861]\n",
      " ...\n",
      " [ 0.28163447]\n",
      " [ 0.12482592]\n",
      " [-0.60628387]]\n",
      "loss=41776.52921266821\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.06662215]\n",
      " [-0.039631  ]\n",
      " [-0.06263114]\n",
      " ...\n",
      " [ 0.09595685]\n",
      " [-0.06263114]\n",
      " [ 0.08987718]]\n",
      "t [[ 0.06662215]\n",
      " [-0.039631  ]\n",
      " [-0.06263114]\n",
      " ...\n",
      " [ 0.09595685]\n",
      " [-0.06263114]\n",
      " [ 0.08987718]]\n",
      "t [[ 0.12782377]\n",
      " [-0.08311792]\n",
      " [-0.12162582]\n",
      " ...\n",
      " [ 0.18313811]\n",
      " [-0.12162582]\n",
      " [ 0.17011319]]\n",
      "t [[ 0.12782377]\n",
      " [-0.08311792]\n",
      " [-0.12162582]\n",
      " ...\n",
      " [ 0.18313811]\n",
      " [-0.12162582]\n",
      " [ 0.17011319]]\n",
      "Current iteration=2, loss=48987.58963631379\n",
      "t [[ 0.18410388]\n",
      " [-0.12935412]\n",
      " [-0.17733509]\n",
      " ...\n",
      " [ 0.2624871 ]\n",
      " [-0.17733509]\n",
      " [ 0.24188954]]\n",
      "t [[ 0.18410388]\n",
      " [-0.12935412]\n",
      " [-0.17733509]\n",
      " ...\n",
      " [ 0.2624871 ]\n",
      " [-0.17733509]\n",
      " [ 0.24188954]]\n",
      "t [[ 0.23592478]\n",
      " [-0.17743958]\n",
      " [-0.23007441]\n",
      " ...\n",
      " [ 0.33485384]\n",
      " [-0.23007441]\n",
      " [ 0.30625169]]\n",
      "t [[ 0.23592478]\n",
      " [-0.17743958]\n",
      " [-0.23007441]\n",
      " ...\n",
      " [ 0.33485384]\n",
      " [-0.23007441]\n",
      " [ 0.30625169]]\n",
      "Current iteration=4, loss=46630.525226911566\n",
      "t [[ 0.28370825]\n",
      " [-0.22665575]\n",
      " [-0.28012397]\n",
      " ...\n",
      " [ 0.40099445]\n",
      " [-0.28012397]\n",
      " [ 0.36411315]]\n",
      "t [[ 0.28370825]\n",
      " [-0.22665575]\n",
      " [-0.28012397]\n",
      " ...\n",
      " [ 0.40099445]\n",
      " [-0.28012397]\n",
      " [ 0.36411315]]\n",
      "t [[ 0.32783549]\n",
      " [-0.27643711]\n",
      " [-0.32773109]\n",
      " ...\n",
      " [ 0.46157662]\n",
      " [-0.32773109]\n",
      " [ 0.41626654]]\n",
      "t [[ 0.32783549]\n",
      " [-0.27643711]\n",
      " [-0.32773109]\n",
      " ...\n",
      " [ 0.46157662]\n",
      " [-0.32773109]\n",
      " [ 0.41626654]]\n",
      "Current iteration=6, loss=44708.69363592554\n",
      "t [[ 0.3686492 ]\n",
      " [-0.326344  ]\n",
      " [-0.37311351]\n",
      " ...\n",
      " [ 0.51718773]\n",
      " [-0.37311351]\n",
      " [ 0.46339719]]\n",
      "t [[ 0.3686492 ]\n",
      " [-0.326344  ]\n",
      " [-0.37311351]\n",
      " ...\n",
      " [ 0.51718773]\n",
      " [-0.37311351]\n",
      " [ 0.46339719]]\n",
      "t [[ 0.4064565 ]\n",
      " [-0.37603854]\n",
      " [-0.41646282]\n",
      " ...\n",
      " [ 0.56834377]\n",
      " [-0.41646282]\n",
      " [ 0.50609711]]\n",
      "t [[ 0.4064565 ]\n",
      " [-0.37603854]\n",
      " [-0.41646282]\n",
      " ...\n",
      " [ 0.56834377]\n",
      " [-0.41646282]\n",
      " [ 0.50609711]]\n",
      "Current iteration=8, loss=43114.07894936678\n",
      "t [[ 0.44153224]\n",
      " [-0.42526433]\n",
      " [-0.45794783]\n",
      " ...\n",
      " [ 0.61549806]\n",
      " [-0.45794783]\n",
      " [ 0.54487791]]\n",
      "t [[ 0.44153224]\n",
      " [-0.42526433]\n",
      " [-0.45794783]\n",
      " ...\n",
      " [ 0.61549806]\n",
      " [-0.45794783]\n",
      " [ 0.54487791]]\n",
      "t [[ 0.47412237]\n",
      " [-0.47382963]\n",
      " [-0.49771756]\n",
      " ...\n",
      " [ 0.65904924]\n",
      " [-0.49771756]\n",
      " [ 0.58018241]]\n",
      "loss=41771.59549283171\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02424956]\n",
      " [-0.11611898]\n",
      " [-0.0680935 ]\n",
      " ...\n",
      " [ 0.05822899]\n",
      " [ 0.02424956]\n",
      " [-0.09870489]]\n",
      "t [[ 0.02424956]\n",
      " [-0.11611898]\n",
      " [-0.0680935 ]\n",
      " ...\n",
      " [ 0.05822899]\n",
      " [ 0.02424956]\n",
      " [-0.09870489]]\n",
      "t [[ 0.04493207]\n",
      " [-0.22659395]\n",
      " [-0.13192775]\n",
      " ...\n",
      " [ 0.10698151]\n",
      " [ 0.04493207]\n",
      " [-0.18569019]]\n",
      "t [[ 0.04493207]\n",
      " [-0.22659395]\n",
      " [-0.13192775]\n",
      " ...\n",
      " [ 0.10698151]\n",
      " [ 0.04493207]\n",
      " [-0.18569019]]\n",
      "Current iteration=2, loss=48742.339823196264\n",
      "t [[ 0.06251086]\n",
      " [-0.33163883]\n",
      " [-0.19194862]\n",
      " ...\n",
      " [ 0.14778034]\n",
      " [ 0.06251086]\n",
      " [-0.26286362]]\n",
      "t [[ 0.06251086]\n",
      " [-0.33163883]\n",
      " [-0.19194862]\n",
      " ...\n",
      " [ 0.14778034]\n",
      " [ 0.06251086]\n",
      " [-0.26286362]]\n",
      "t [[ 0.07739511]\n",
      " [-0.43149847]\n",
      " [-0.2485516 ]\n",
      " ...\n",
      " [ 0.1819041 ]\n",
      " [ 0.07739511]\n",
      " [-0.33182688]]\n",
      "t [[ 0.07739511]\n",
      " [-0.43149847]\n",
      " [-0.2485516 ]\n",
      " ...\n",
      " [ 0.1819041 ]\n",
      " [ 0.07739511]\n",
      " [-0.33182688]]\n",
      "Current iteration=4, loss=46231.63200200881\n",
      "t [[ 0.08994108]\n",
      " [-0.52643522]\n",
      " [-0.30208332]\n",
      " ...\n",
      " [ 0.21041413]\n",
      " [ 0.08994108]\n",
      " [-0.39390636]]\n",
      "t [[ 0.08994108]\n",
      " [-0.52643522]\n",
      " [-0.30208332]\n",
      " ...\n",
      " [ 0.21041413]\n",
      " [ 0.08994108]\n",
      " [-0.39390636]]\n",
      "t [[ 0.10045658]\n",
      " [-0.61671818]\n",
      " [-0.35284588]\n",
      " ...\n",
      " [ 0.23419019]\n",
      " [ 0.10045658]\n",
      " [-0.4501935 ]]\n",
      "t [[ 0.10045658]\n",
      " [-0.61671818]\n",
      " [-0.35284588]\n",
      " ...\n",
      " [ 0.23419019]\n",
      " [ 0.10045658]\n",
      " [-0.4501935 ]]\n",
      "Current iteration=6, loss=44211.96477489799\n",
      "t [[ 0.1092066 ]\n",
      " [-0.70261536]\n",
      " [-0.40110212]\n",
      " ...\n",
      " [ 0.25396331]\n",
      " [ 0.1092066 ]\n",
      " [-0.50158432]]\n",
      "t [[ 0.1092066 ]\n",
      " [-0.70261536]\n",
      " [-0.40110212]\n",
      " ...\n",
      " [ 0.25396331]\n",
      " [ 0.1092066 ]\n",
      " [-0.50158432]]\n",
      "t [[ 0.11641901]\n",
      " [-0.78438843]\n",
      " [-0.44708091]\n",
      " ...\n",
      " [ 0.27034243]\n",
      " [ 0.11641901]\n",
      " [-0.54881425]]\n",
      "t [[ 0.11641901]\n",
      " [-0.78438843]\n",
      " [-0.44708091]\n",
      " ...\n",
      " [ 0.27034243]\n",
      " [ 0.11641901]\n",
      " [-0.54881425]]\n",
      "Current iteration=8, loss=42554.864078556646\n",
      "t [[ 0.12228985]\n",
      " [-0.86228931]\n",
      " [-0.4909819 ]\n",
      " ...\n",
      " [ 0.28383575]\n",
      " [ 0.12228985]\n",
      " [-0.59248758]]\n",
      "t [[ 0.12228985]\n",
      " [-0.86228931]\n",
      " [-0.4909819 ]\n",
      " ...\n",
      " [ 0.28383575]\n",
      " [ 0.12228985]\n",
      " [-0.59248758]]\n",
      "t [[ 0.12698809]\n",
      " [-0.93655807]\n",
      " [-0.53297982]\n",
      " ...\n",
      " [ 0.29486812]\n",
      " [ 0.12698809]\n",
      " [-0.63310182]]\n",
      "loss=41173.003706388634\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.07322828]\n",
      " [-0.04268915]\n",
      " [-0.06832006]\n",
      " ...\n",
      " [ 0.05737671]\n",
      " [ 0.02415436]\n",
      " [-0.09721319]]\n",
      "t [[ 0.07322828]\n",
      " [-0.04268915]\n",
      " [-0.06832006]\n",
      " ...\n",
      " [ 0.05737671]\n",
      " [ 0.02415436]\n",
      " [-0.09721319]]\n",
      "t [[ 0.13999907]\n",
      " [-0.09014238]\n",
      " [-0.13233548]\n",
      " ...\n",
      " [ 0.10534469]\n",
      " [ 0.04472989]\n",
      " [-0.18271326]]\n",
      "t [[ 0.13999907]\n",
      " [-0.09014238]\n",
      " [-0.13233548]\n",
      " ...\n",
      " [ 0.10534469]\n",
      " [ 0.04472989]\n",
      " [-0.18271326]]\n",
      "Current iteration=2, loss=48735.25903272216\n",
      "t [[ 0.20095258]\n",
      " [-0.14091262]\n",
      " [-0.19249912]\n",
      " ...\n",
      " [ 0.14542264]\n",
      " [ 0.0621929 ]\n",
      " [-0.25842178]]\n",
      "t [[ 0.20095258]\n",
      " [-0.14091262]\n",
      " [-0.19249912]\n",
      " ...\n",
      " [ 0.14542264]\n",
      " [ 0.0621929 ]\n",
      " [-0.25842178]]\n",
      "t [[ 0.25668324]\n",
      " [-0.19384002]\n",
      " [-0.24921263]\n",
      " ...\n",
      " [ 0.17888466]\n",
      " [ 0.07695504]\n",
      " [-0.32595082]]\n",
      "t [[ 0.25668324]\n",
      " [-0.19384002]\n",
      " [-0.24921263]\n",
      " ...\n",
      " [ 0.17888466]\n",
      " [ 0.07695504]\n",
      " [-0.32595082]]\n",
      "Current iteration=4, loss=46221.19537998829\n",
      "t [[ 0.30773146]\n",
      " [-0.24801463]\n",
      " [-0.30282791]\n",
      " ...\n",
      " [ 0.20678781]\n",
      " [ 0.08937456]\n",
      " [-0.38663424]]\n",
      "t [[ 0.30773146]\n",
      " [-0.24801463]\n",
      " [-0.30282791]\n",
      " ...\n",
      " [ 0.20678781]\n",
      " [ 0.08937456]\n",
      " [-0.38663424]]\n",
      "t [[ 0.35458117]\n",
      " [-0.30273523]\n",
      " [-0.3536515 ]\n",
      " ...\n",
      " [ 0.230008  ]\n",
      " [ 0.09976087]\n",
      " [-0.44156864]]\n",
      "t [[ 0.35458117]\n",
      " [-0.30273523]\n",
      " [-0.3536515 ]\n",
      " ...\n",
      " [ 0.230008  ]\n",
      " [ 0.09976087]\n",
      " [-0.44156864]]\n",
      "Current iteration=6, loss=44199.979294995326\n",
      "t [[ 0.39766177]\n",
      " [-0.35746989]\n",
      " [-0.40195001]\n",
      " ...\n",
      " [ 0.24927265]\n",
      " [ 0.1083802 ]\n",
      " [-0.49165332]]\n",
      "t [[ 0.39766177]\n",
      " [-0.35746989]\n",
      " [-0.40195001]\n",
      " ...\n",
      " [ 0.24927265]\n",
      " [ 0.1083802 ]\n",
      " [-0.49165332]]\n",
      "t [[ 0.43735265]\n",
      " [-0.4118211 ]\n",
      " [-0.4479554 ]\n",
      " ...\n",
      " [ 0.26518725]\n",
      " [ 0.11546137]\n",
      " [-0.53762549]]\n",
      "t [[ 0.43735265]\n",
      " [-0.4118211 ]\n",
      " [-0.4479554 ]\n",
      " ...\n",
      " [ 0.26518725]\n",
      " [ 0.11546137]\n",
      " [-0.53762549]]\n",
      "Current iteration=8, loss=42542.21545221131\n",
      "t [[ 0.47398869]\n",
      " [-0.46549656]\n",
      " [-0.49186996]\n",
      " ...\n",
      " [ 0.27825663]\n",
      " [ 0.12120122]\n",
      " [-0.58009008]]\n",
      "t [[ 0.47398869]\n",
      " [-0.46549656]\n",
      " [-0.49186996]\n",
      " ...\n",
      " [ 0.27825663]\n",
      " [ 0.12120122]\n",
      " [-0.58009008]]\n",
      "t [[ 0.50786579]\n",
      " [-0.51828545]\n",
      " [-0.53387057]\n",
      " ...\n",
      " [ 0.2889024 ]\n",
      " [ 0.1257693 ]\n",
      " [-0.61954442]]\n",
      "loss=41160.12940965386\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07228971]\n",
      " [-0.04086579]\n",
      " [-0.06806419]\n",
      " ...\n",
      " [ 0.05826196]\n",
      " [ 0.02459954]\n",
      " [-0.10020687]]\n",
      "t [[ 0.07228971]\n",
      " [-0.04086579]\n",
      " [-0.06806419]\n",
      " ...\n",
      " [ 0.05826196]\n",
      " [ 0.02459954]\n",
      " [-0.10020687]]\n",
      "t [[ 0.1382077 ]\n",
      " [-0.08651724]\n",
      " [-0.13182384]\n",
      " ...\n",
      " [ 0.1068585 ]\n",
      " [ 0.04557721]\n",
      " [-0.18834689]]\n",
      "t [[ 0.1382077 ]\n",
      " [-0.08651724]\n",
      " [-0.13182384]\n",
      " ...\n",
      " [ 0.1068585 ]\n",
      " [ 0.04557721]\n",
      " [-0.18834689]]\n",
      "Current iteration=2, loss=48759.58460015023\n",
      "t [[ 0.19838719]\n",
      " [-0.13550063]\n",
      " [-0.19173472]\n",
      " ...\n",
      " [ 0.14735325]\n",
      " [ 0.06340569]\n",
      " [-0.26639433]]\n",
      "t [[ 0.19838719]\n",
      " [-0.13550063]\n",
      " [-0.19173472]\n",
      " ...\n",
      " [ 0.14735325]\n",
      " [ 0.06340569]\n",
      " [-0.26639433]]\n",
      "t [[ 0.25341533]\n",
      " [-0.18665314]\n",
      " [-0.24820069]\n",
      " ...\n",
      " [ 0.18106047]\n",
      " [ 0.07850213]\n",
      " [-0.33600734]]\n",
      "t [[ 0.25341533]\n",
      " [-0.18665314]\n",
      " [-0.24820069]\n",
      " ...\n",
      " [ 0.18106047]\n",
      " [ 0.07850213]\n",
      " [-0.33600734]]\n",
      "Current iteration=4, loss=46269.69742661914\n",
      "t [[ 0.30382581]\n",
      " [-0.23906403]\n",
      " [-0.30157513]\n",
      " ...\n",
      " [ 0.20907001]\n",
      " [ 0.09122937]\n",
      " [-0.39855832]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.30382581]\n",
      " [-0.23906403]\n",
      " [-0.30157513]\n",
      " ...\n",
      " [ 0.20907001]\n",
      " [ 0.09122937]\n",
      " [-0.39855832]]\n",
      "t [[ 0.35009674]\n",
      " [-0.29203233]\n",
      " [-0.35216545]\n",
      " ...\n",
      " [ 0.23228308]\n",
      " [ 0.10190056]\n",
      " [-0.45517536]]\n",
      "t [[ 0.35009674]\n",
      " [-0.29203233]\n",
      " [-0.35216545]\n",
      " ...\n",
      " [ 0.23228308]\n",
      " [ 0.10190056]\n",
      " [-0.45517536]]\n",
      "Current iteration=6, loss=44270.733519474175\n",
      "t [[ 0.39265245]\n",
      " [-0.34502688]\n",
      " [-0.40023865]\n",
      " ...\n",
      " [ 0.25144637]\n",
      " [ 0.11078499]\n",
      " [-0.50678339]]\n",
      "t [[ 0.39265245]\n",
      " [-0.34502688]\n",
      " [-0.40023865]\n",
      " ...\n",
      " [ 0.25144637]\n",
      " [ 0.11078499]\n",
      " [-0.50678339]]\n",
      "t [[ 0.43186771]\n",
      " [-0.39765134]\n",
      " [-0.44602677]\n",
      " ...\n",
      " [ 0.26718042]\n",
      " [ 0.118114  ]\n",
      " [-0.55414074]]\n",
      "t [[ 0.43186771]\n",
      " [-0.39765134]\n",
      " [-0.44602677]\n",
      " ...\n",
      " [ 0.26718042]\n",
      " [ 0.118114  ]\n",
      " [-0.55414074]]\n",
      "Current iteration=8, loss=42632.83948275994\n",
      "t [[ 0.46807313]\n",
      " [-0.44961476]\n",
      " [-0.48973195]\n",
      " ...\n",
      " [ 0.28000225]\n",
      " [ 0.12408645]\n",
      " [-0.59786988]]\n",
      "t [[ 0.46807313]\n",
      " [-0.44961476]\n",
      " [-0.48973195]\n",
      " ...\n",
      " [ 0.28000225]\n",
      " [ 0.12408645]\n",
      " [-0.59786988]]\n",
      "t [[ 0.50156055]\n",
      " [-0.50070777]\n",
      " [-0.5315308 ]\n",
      " ...\n",
      " [ 0.29034354]\n",
      " [ 0.12887357]\n",
      " [-0.63848288]]\n",
      "loss=41268.30264682483\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.072174  ]\n",
      " [-0.04293359]\n",
      " [-0.0678504 ]\n",
      " ...\n",
      " [ 0.10395325]\n",
      " [-0.0678504 ]\n",
      " [ 0.09736695]]\n",
      "t [[ 0.072174  ]\n",
      " [-0.04293359]\n",
      " [-0.0678504 ]\n",
      " ...\n",
      " [ 0.10395325]\n",
      " [-0.0678504 ]\n",
      " [ 0.09736695]]\n",
      "t [[ 0.13798752]\n",
      " [-0.09039159]\n",
      " [-0.1314337 ]\n",
      " ...\n",
      " [ 0.19760921]\n",
      " [-0.1314337 ]\n",
      " [ 0.18342099]]\n",
      "t [[ 0.13798752]\n",
      " [-0.09039159]\n",
      " [-0.1314337 ]\n",
      " ...\n",
      " [ 0.19760921]\n",
      " [-0.1314337 ]\n",
      " [ 0.18342099]]\n",
      "Current iteration=2, loss=48764.36392071444\n",
      "t [[ 0.19807716]\n",
      " [-0.1409664 ]\n",
      " [-0.19119752]\n",
      " ...\n",
      " [ 0.28217048]\n",
      " [-0.19119752]\n",
      " [ 0.25966781]]\n",
      "t [[ 0.19807716]\n",
      " [-0.1409664 ]\n",
      " [-0.19119752]\n",
      " ...\n",
      " [ 0.28217048]\n",
      " [-0.19119752]\n",
      " [ 0.25966781]]\n",
      "t [[ 0.25302703]\n",
      " [-0.19353731]\n",
      " [-0.24753924]\n",
      " ...\n",
      " [ 0.35870797]\n",
      " [-0.24753924]\n",
      " [ 0.32742198]]\n",
      "t [[ 0.25302703]\n",
      " [-0.19353731]\n",
      " [-0.24753924]\n",
      " ...\n",
      " [ 0.35870797]\n",
      " [-0.24753924]\n",
      " [ 0.32742198]]\n",
      "Current iteration=4, loss=46273.692021511044\n",
      "t [[ 0.30336418]\n",
      " [-0.24723068]\n",
      " [-0.30080729]\n",
      " ...\n",
      " [ 0.42816202]\n",
      " [-0.30080729]\n",
      " [ 0.3878155 ]]\n",
      "t [[ 0.30336418]\n",
      " [-0.24723068]\n",
      " [-0.30080729]\n",
      " ...\n",
      " [ 0.42816202]\n",
      " [-0.30080729]\n",
      " [ 0.3878155 ]]\n",
      "t [[ 0.34955963]\n",
      " [-0.30137642]\n",
      " [-0.3513053 ]\n",
      " ...\n",
      " [ 0.49135222]\n",
      " [-0.3513053 ]\n",
      " [ 0.44181636]]\n",
      "t [[ 0.34955963]\n",
      " [-0.30137642]\n",
      " [-0.3513053 ]\n",
      " ...\n",
      " [ 0.49135222]\n",
      " [-0.3513053 ]\n",
      " [ 0.44181636]]\n",
      "Current iteration=6, loss=44271.73759499824\n",
      "t [[ 0.39203203]\n",
      " [-0.35546778]\n",
      " [-0.39929729]\n",
      " ...\n",
      " [ 0.54899055]\n",
      " [-0.39929729]\n",
      " [ 0.49024988]]\n",
      "t [[ 0.39203203]\n",
      " [-0.35546778]\n",
      " [-0.39929729]\n",
      " ...\n",
      " [ 0.54899055]\n",
      " [-0.39929729]\n",
      " [ 0.49024988]]\n",
      "t [[ 0.4311526 ]\n",
      " [-0.40912691]\n",
      " [-0.445013  ]\n",
      " ...\n",
      " [ 0.60169505]\n",
      " [-0.445013  ]\n",
      " [ 0.53381949]]\n",
      "t [[ 0.4311526 ]\n",
      " [-0.40912691]\n",
      " [-0.445013  ]\n",
      " ...\n",
      " [ 0.60169505]\n",
      " [-0.445013  ]\n",
      " [ 0.53381949]]\n",
      "Current iteration=8, loss=42630.188479025455\n",
      "t [[ 0.46725021]\n",
      " [-0.46207653]\n",
      " [-0.48865272]\n",
      " ...\n",
      " [ 0.65000262]\n",
      " [-0.48865272]\n",
      " [ 0.57312562]]\n",
      "t [[ 0.46725021]\n",
      " [-0.46207653]\n",
      " [-0.48865272]\n",
      " ...\n",
      " [ 0.65000262]\n",
      " [-0.48865272]\n",
      " [ 0.57312562]]\n",
      "t [[ 0.50061631]\n",
      " [-0.51411731]\n",
      " [-0.53039161]\n",
      " ...\n",
      " [ 0.69438054]\n",
      " [-0.53039161]\n",
      " [ 0.60868191]]\n",
      "loss=41261.97965252866\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02611491]\n",
      " [-0.12505121]\n",
      " [-0.07333147]\n",
      " ...\n",
      " [ 0.06270814]\n",
      " [ 0.02611491]\n",
      " [-0.10629758]]\n",
      "t [[ 0.02611491]\n",
      " [-0.12505121]\n",
      " [-0.07333147]\n",
      " ...\n",
      " [ 0.06270814]\n",
      " [ 0.02611491]\n",
      " [-0.10629758]]\n",
      "t [[ 0.04809371]\n",
      " [-0.24355753]\n",
      " [-0.14172406]\n",
      " ...\n",
      " [ 0.11442785]\n",
      " [ 0.04809371]\n",
      " [-0.19900591]]\n",
      "t [[ 0.04809371]\n",
      " [-0.24355753]\n",
      " [-0.14172406]\n",
      " ...\n",
      " [ 0.11442785]\n",
      " [ 0.04809371]\n",
      " [-0.19900591]]\n",
      "Current iteration=2, loss=48520.73966655198\n",
      "t [[ 0.06651643]\n",
      " [-0.35578817]\n",
      " [-0.20573615]\n",
      " ...\n",
      " [ 0.15706324]\n",
      " [ 0.06651643]\n",
      " [-0.28051134]]\n",
      "t [[ 0.06651643]\n",
      " [-0.35578817]\n",
      " [-0.20573615]\n",
      " ...\n",
      " [ 0.15706324]\n",
      " [ 0.06651643]\n",
      " [-0.28051134]]\n",
      "t [[ 0.08188874]\n",
      " [-0.46205287]\n",
      " [-0.26585688]\n",
      " ...\n",
      " [ 0.19218628]\n",
      " [ 0.08188874]\n",
      " [-0.35278325]]\n",
      "t [[ 0.08188874]\n",
      " [-0.46205287]\n",
      " [-0.26585688]\n",
      " ...\n",
      " [ 0.19218628]\n",
      " [ 0.08188874]\n",
      " [-0.35278325]]\n",
      "Current iteration=4, loss=45882.65630286461\n",
      "t [[ 0.09464479]\n",
      " [-0.5626826 ]\n",
      " [-0.32250932]\n",
      " ...\n",
      " [ 0.22107978]\n",
      " [ 0.09464479]\n",
      " [-0.41742328]]\n",
      "t [[ 0.09464479]\n",
      " [-0.5626826 ]\n",
      " [-0.32250932]\n",
      " ...\n",
      " [ 0.22107978]\n",
      " [ 0.09464479]\n",
      " [-0.41742328]]\n",
      "t [[ 0.10515452]\n",
      " [-0.65801471]\n",
      " [-0.37605748]\n",
      " ...\n",
      " [ 0.24479042]\n",
      " [ 0.10515452]\n",
      " [-0.47572597]]\n",
      "t [[ 0.10515452]\n",
      " [-0.65801471]\n",
      " [-0.37605748]\n",
      " ...\n",
      " [ 0.24479042]\n",
      " [ 0.10515452]\n",
      " [-0.47572597]]\n",
      "Current iteration=6, loss=43789.79771080013\n",
      "t [[ 0.11373214]\n",
      " [-0.74838253]\n",
      " [-0.42681434]\n",
      " ...\n",
      " [ 0.26417493]\n",
      " [ 0.11373214]\n",
      " [-0.52873589]]\n",
      "t [[ 0.11373214]\n",
      " [-0.74838253]\n",
      " [-0.42681434]\n",
      " ...\n",
      " [ 0.26417493]\n",
      " [ 0.11373214]\n",
      " [-0.52873589]]\n",
      "t [[ 0.12064446]\n",
      " [-0.83410877]\n",
      " [-0.4750494 ]\n",
      " ...\n",
      " [ 0.27993653]\n",
      " [ 0.12064446]\n",
      " [-0.57729626]]\n",
      "t [[ 0.12064446]\n",
      " [-0.83410877]\n",
      " [-0.4750494 ]\n",
      " ...\n",
      " [ 0.27993653]\n",
      " [ 0.12064446]\n",
      " [-0.57729626]]\n",
      "Current iteration=8, loss=42092.08333360234\n",
      "t [[ 0.12611835]\n",
      " [-0.91550145]\n",
      " [-0.52099544]\n",
      " ...\n",
      " [ 0.29265364]\n",
      " [ 0.12611835]\n",
      " [-0.62208906]]\n",
      "t [[ 0.12611835]\n",
      " [-0.91550145]\n",
      " [-0.52099544]\n",
      " ...\n",
      " [ 0.29265364]\n",
      " [ 0.12611835]\n",
      " [-0.62208906]]\n",
      "t [[ 0.13034716]\n",
      " [-0.99285152]\n",
      " [-0.56485434]\n",
      " ...\n",
      " [ 0.30280305]\n",
      " [ 0.13034716]\n",
      " [-0.66366752]]\n",
      "loss=40689.83555593919\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.07886122]\n",
      " [-0.04597293]\n",
      " [-0.07357545]\n",
      " ...\n",
      " [ 0.06179031]\n",
      " [ 0.02601238]\n",
      " [-0.10469113]]\n",
      "t [[ 0.07886122]\n",
      " [-0.04597293]\n",
      " [-0.07357545]\n",
      " ...\n",
      " [ 0.06179031]\n",
      " [ 0.02601238]\n",
      " [-0.10469113]]\n",
      "t [[ 0.15023484]\n",
      " [-0.09746974]\n",
      " [-0.14215941]\n",
      " ...\n",
      " [ 0.11267069]\n",
      " [ 0.04787501]\n",
      " [-0.19580054]]\n",
      "t [[ 0.15023484]\n",
      " [-0.09746974]\n",
      " [-0.14215941]\n",
      " ...\n",
      " [ 0.11267069]\n",
      " [ 0.04787501]\n",
      " [-0.19580054]]\n",
      "Current iteration=2, loss=48513.26400038219\n",
      "t [[ 0.21492341]\n",
      " [-0.15268202]\n",
      " [-0.206319  ]\n",
      " ...\n",
      " [ 0.15454017]\n",
      " [ 0.06617162]\n",
      " [-0.27573184]]\n",
      "t [[ 0.21492341]\n",
      " [-0.15268202]\n",
      " [-0.206319  ]\n",
      " ...\n",
      " [ 0.15454017]\n",
      " [ 0.06617162]\n",
      " [-0.27573184]]\n",
      "t [[ 0.27366518]\n",
      " [-0.21019098]\n",
      " [-0.26655095]\n",
      " ...\n",
      " [ 0.18896504]\n",
      " [ 0.08141092]\n",
      " [-0.34646702]]\n",
      "t [[ 0.27366518]\n",
      " [-0.21019098]\n",
      " [-0.26655095]\n",
      " ...\n",
      " [ 0.18896504]\n",
      " [ 0.08141092]\n",
      " [-0.34646702]]\n",
      "Current iteration=4, loss=45871.85983359079\n",
      "t [[ 0.32712347]\n",
      " [-0.26891027]\n",
      " [-0.32328472]\n",
      " ...\n",
      " [ 0.21722287]\n",
      " [ 0.09402945]\n",
      " [-0.40961647]]\n",
      "t [[ 0.32712347]\n",
      " [-0.26891027]\n",
      " [-0.32328472]\n",
      " ...\n",
      " [ 0.21722287]\n",
      " [ 0.09402945]\n",
      " [-0.40961647]]\n",
      "t [[ 0.37588487]\n",
      " [-0.32802483]\n",
      " [-0.37688968]\n",
      " ...\n",
      " [ 0.24035564]\n",
      " [ 0.10439901]\n",
      " [-0.4664805 ]]\n",
      "t [[ 0.37588487]\n",
      " [-0.32802483]\n",
      " [-0.37688968]\n",
      " ...\n",
      " [ 0.24035564]\n",
      " [ 0.10439901]\n",
      " [-0.4664805 ]]\n",
      "Current iteration=6, loss=43777.577791959586\n",
      "t [[ 0.4204636 ]\n",
      " [-0.38693402]\n",
      " [-0.42768316]\n",
      " ...\n",
      " [ 0.25921563]\n",
      " [ 0.11283523]\n",
      " [-0.51810705]]\n",
      "t [[ 0.4204636 ]\n",
      " [-0.38693402]\n",
      " [-0.42768316]\n",
      " ...\n",
      " [ 0.25921563]\n",
      " [ 0.11283523]\n",
      " [-0.51810705]]\n",
      "t [[ 0.46130902]\n",
      " [-0.44520311]\n",
      " [-0.47593828]\n",
      " ...\n",
      " [ 0.27450179]\n",
      " [ 0.11960602]\n",
      " [-0.56534087]]\n",
      "t [[ 0.46130902]\n",
      " [-0.44520311]\n",
      " [-0.47593828]\n",
      " ...\n",
      " [ 0.27450179]\n",
      " [ 0.11960602]\n",
      " [-0.56534087]]\n",
      "Current iteration=8, loss=42079.317731775576\n",
      "t [[ 0.49881384]\n",
      " [-0.50252377]\n",
      " [-0.52189079]\n",
      " ...\n",
      " [ 0.28678839]\n",
      " [ 0.12493908]\n",
      " [-0.60886417]]\n",
      "t [[ 0.49881384]\n",
      " [-0.50252377]\n",
      " [-0.52189079]\n",
      " ...\n",
      " [ 0.28678839]\n",
      " [ 0.12493908]\n",
      " [-0.60886417]]\n",
      "t [[ 0.53332189]\n",
      " [-0.55868314]\n",
      " [-0.565745  ]\n",
      " ...\n",
      " [ 0.29654832]\n",
      " [ 0.12902843]\n",
      " [-0.64922944]]\n",
      "loss=40676.93172204031\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07785046]\n",
      " [-0.04400931]\n",
      " [-0.0732999 ]\n",
      " ...\n",
      " [ 0.06274365]\n",
      " [ 0.02649182]\n",
      " [-0.10791509]]\n",
      "t [[ 0.07785046]\n",
      " [-0.04400931]\n",
      " [-0.0732999 ]\n",
      " ...\n",
      " [ 0.06274365]\n",
      " [ 0.02649182]\n",
      " [-0.10791509]]\n",
      "t [[ 0.14831274]\n",
      " [-0.09356753]\n",
      " [-0.14160844]\n",
      " ...\n",
      " [ 0.1142799 ]\n",
      " [ 0.04878397]\n",
      " [-0.20183841]]\n",
      "t [[ 0.14831274]\n",
      " [-0.09356753]\n",
      " [-0.14160844]\n",
      " ...\n",
      " [ 0.1142799 ]\n",
      " [ 0.04878397]\n",
      " [-0.20183841]]\n",
      "Current iteration=2, loss=48539.49255790251\n",
      "t [[ 0.2121806 ]\n",
      " [-0.14685799]\n",
      " [-0.20549639]\n",
      " ...\n",
      " [ 0.1565642 ]\n",
      " [ 0.0674682 ]\n",
      " [-0.28423967]]\n",
      "t [[ 0.2121806 ]\n",
      " [-0.14685799]\n",
      " [-0.20549639]\n",
      " ...\n",
      " [ 0.1565642 ]\n",
      " [ 0.0674682 ]\n",
      " [-0.28423967]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.27018321]\n",
      " [-0.20245853]\n",
      " [-0.26546314]\n",
      " ...\n",
      " [ 0.19121225]\n",
      " [ 0.08305998]\n",
      " [-0.35715759]]\n",
      "t [[ 0.27018321]\n",
      " [-0.20245853]\n",
      " [-0.26546314]\n",
      " ...\n",
      " [ 0.19121225]\n",
      " [ 0.08305998]\n",
      " [-0.35715759]]\n",
      "Current iteration=4, loss=45923.97341489662\n",
      "t [[ 0.3229757 ]\n",
      " [-0.25928212]\n",
      " [-0.32193986]\n",
      " ...\n",
      " [ 0.21954072]\n",
      " [ 0.09600139]\n",
      " [-0.42224902]]\n",
      "t [[ 0.3229757 ]\n",
      " [-0.25928212]\n",
      " [-0.32193986]\n",
      " ...\n",
      " [ 0.21954072]\n",
      " [ 0.09600139]\n",
      " [-0.42224902]]\n",
      "t [[ 0.37113766]\n",
      " [-0.31651424]\n",
      " [-0.37529678]\n",
      " ...\n",
      " [ 0.24262083]\n",
      " [ 0.10666863]\n",
      " [-0.48085144]]\n",
      "t [[ 0.37113766]\n",
      " [-0.31651424]\n",
      " [-0.37529678]\n",
      " ...\n",
      " [ 0.24262083]\n",
      " [ 0.10666863]\n",
      " [-0.48085144]]\n",
      "Current iteration=6, loss=43853.20761731478\n",
      "t [[ 0.41517722]\n",
      " [-0.37355547]\n",
      " [-0.42585158]\n",
      " ...\n",
      " [ 0.26132684]\n",
      " [ 0.11538091]\n",
      " [-0.53404263]]\n",
      "t [[ 0.41517722]\n",
      " [-0.37355547]\n",
      " [-0.42585158]\n",
      " ...\n",
      " [ 0.26132684]\n",
      " [ 0.11538091]\n",
      " [-0.53404263]]\n",
      "t [[ 0.45553815]\n",
      " [-0.42997267]\n",
      " [-0.47387732]\n",
      " ...\n",
      " [ 0.2763747 ]\n",
      " [ 0.12240897]\n",
      " [-0.58269163]]\n",
      "t [[ 0.45553815]\n",
      " [-0.42997267]\n",
      " [-0.47387732]\n",
      " ...\n",
      " [ 0.2763747 ]\n",
      " [ 0.12240897]\n",
      " [-0.58269163]]\n",
      "Current iteration=8, loss=42175.678919313315\n",
      "t [[ 0.49260797]\n",
      " [-0.48545934]\n",
      " [-0.51960946]\n",
      " ...\n",
      " [ 0.28835245]\n",
      " [ 0.12798283]\n",
      " [-0.6275006 ]]\n",
      "t [[ 0.49260797]\n",
      " [-0.48545934]\n",
      " [-0.51960946]\n",
      " ...\n",
      " [ 0.28835245]\n",
      " [ 0.12798283]\n",
      " [-0.6275006 ]]\n",
      "t [[ 0.52672563]\n",
      " [-0.53980444]\n",
      " [-0.56325192]\n",
      " ...\n",
      " [ 0.29774428]\n",
      " [ 0.13229839]\n",
      " [-0.66903865]]\n",
      "loss=40791.39913596168\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.07772584]\n",
      " [-0.04623617]\n",
      " [-0.07306966]\n",
      " ...\n",
      " [ 0.11194965]\n",
      " [-0.07306966]\n",
      " [ 0.10485671]]\n",
      "t [[ 0.07772584]\n",
      " [-0.04623617]\n",
      " [-0.07306966]\n",
      " ...\n",
      " [ 0.11194965]\n",
      " [-0.07306966]\n",
      " [ 0.10485671]]\n",
      "t [[ 0.14807645]\n",
      " [-0.09771843]\n",
      " [-0.14119137]\n",
      " ...\n",
      " [ 0.21195918]\n",
      " [-0.14119137]\n",
      " [ 0.19659573]]\n",
      "t [[ 0.14807645]\n",
      " [-0.09771843]\n",
      " [-0.14119137]\n",
      " ...\n",
      " [ 0.21195918]\n",
      " [-0.14119137]\n",
      " [ 0.19659573]]\n",
      "Current iteration=2, loss=48544.407265018075\n",
      "t [[ 0.21184941]\n",
      " [-0.15268813]\n",
      " [-0.20492567]\n",
      " ...\n",
      " [ 0.30153444]\n",
      " [-0.20492567]\n",
      " [ 0.27710175]]\n",
      "t [[ 0.21184941]\n",
      " [-0.15268813]\n",
      " [-0.20492567]\n",
      " ...\n",
      " [ 0.30153444]\n",
      " [-0.20492567]\n",
      " [ 0.27710175]]\n",
      "t [[ 0.26976934]\n",
      " [-0.2097754 ]\n",
      " [-0.2647641 ]\n",
      " ...\n",
      " [ 0.38200008]\n",
      " [-0.2647641 ]\n",
      " [ 0.34799752]]\n",
      "t [[ 0.26976934]\n",
      " [-0.2097754 ]\n",
      " [-0.2647641 ]\n",
      " ...\n",
      " [ 0.38200008]\n",
      " [-0.2647641 ]\n",
      " [ 0.34799752]]\n",
      "Current iteration=4, loss=45927.64496242599\n",
      "t [[ 0.32248268]\n",
      " [-0.26793827]\n",
      " [-0.32113192]\n",
      " ...\n",
      " [ 0.45450393]\n",
      " [-0.32113192]\n",
      " [ 0.41065989]]\n",
      "t [[ 0.32248268]\n",
      " [-0.26793827]\n",
      " [-0.32113192]\n",
      " ...\n",
      " [ 0.45450393]\n",
      " [-0.32113192]\n",
      " [ 0.41065989]]\n",
      "t [[ 0.37056051]\n",
      " [-0.3263986 ]\n",
      " [-0.37439494]\n",
      " ...\n",
      " [ 0.52003335]\n",
      " [-0.37439494]\n",
      " [ 0.46624899]]\n",
      "t [[ 0.37056051]\n",
      " [-0.3263986 ]\n",
      " [-0.37439494]\n",
      " ...\n",
      " [ 0.52003335]\n",
      " [-0.37439494]\n",
      " [ 0.46624899]]\n",
      "Current iteration=6, loss=43853.41971498275\n",
      "t [[ 0.41450485]\n",
      " [-0.38458465]\n",
      " [-0.42486747]\n",
      " ...\n",
      " [ 0.57943539]\n",
      " [-0.42486747]\n",
      " [ 0.51574021]]\n",
      "t [[ 0.41450485]\n",
      " [-0.38458465]\n",
      " [-0.42486747]\n",
      " ...\n",
      " [ 0.57943539]\n",
      " [-0.42486747]\n",
      " [ 0.51574021]]\n",
      "t [[ 0.45475611]\n",
      " [-0.44208353]\n",
      " [-0.47281995]\n",
      " ...\n",
      " [ 0.63343672]\n",
      " [-0.47281995]\n",
      " [ 0.5599541 ]]\n",
      "t [[ 0.45475611]\n",
      " [-0.44208353]\n",
      " [-0.47281995]\n",
      " ...\n",
      " [ 0.63343672]\n",
      " [-0.47281995]\n",
      " [ 0.5599541 ]]\n",
      "Current iteration=8, loss=42171.892017298305\n",
      "t [[ 0.49170061]\n",
      " [-0.49860323]\n",
      " [-0.51848582]\n",
      " ...\n",
      " [ 0.68266183]\n",
      " [-0.51848582]\n",
      " [ 0.59958253]]\n",
      "t [[ 0.49170061]\n",
      " [-0.49860323]\n",
      " [-0.51848582]\n",
      " ...\n",
      " [ 0.68266183]\n",
      " [-0.51848582]\n",
      " [ 0.59958253]]\n",
      "t [[ 0.52567752]\n",
      " [-0.55394303]\n",
      " [-0.56206743]\n",
      " ...\n",
      " [ 0.72764875]\n",
      " [-0.56206743]\n",
      " [ 0.6352108 ]]\n",
      "loss=40783.71320351164\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02798027]\n",
      " [-0.13398344]\n",
      " [-0.07856943]\n",
      " ...\n",
      " [ 0.06718729]\n",
      " [ 0.02798027]\n",
      " [-0.11389026]]\n",
      "t [[ 0.02798027]\n",
      " [-0.13398344]\n",
      " [-0.07856943]\n",
      " ...\n",
      " [ 0.06718729]\n",
      " [ 0.02798027]\n",
      " [-0.11389026]]\n",
      "t [[ 0.05121345]\n",
      " [-0.26045464]\n",
      " [-0.15147029]\n",
      " ...\n",
      " [ 0.12176281]\n",
      " [ 0.05121345]\n",
      " [-0.21218398]]\n",
      "t [[ 0.05121345]\n",
      " [-0.26045464]\n",
      " [-0.15147029]\n",
      " ...\n",
      " [ 0.12176281]\n",
      " [ 0.05121345]\n",
      " [-0.21218398]]\n",
      "Current iteration=2, loss=48302.365763529204\n",
      "t [[ 0.07041453]\n",
      " [-0.37974714]\n",
      " [-0.21939108]\n",
      " ...\n",
      " [ 0.16607094]\n",
      " [ 0.07041453]\n",
      " [-0.29782029]]\n",
      "t [[ 0.07041453]\n",
      " [-0.37974714]\n",
      " [-0.21939108]\n",
      " ...\n",
      " [ 0.16607094]\n",
      " [ 0.07041453]\n",
      " [-0.29782029]]\n",
      "t [[ 0.08619854]\n",
      " [-0.49224649]\n",
      " [-0.28292721]\n",
      " ...\n",
      " [ 0.20201387]\n",
      " [ 0.08619854]\n",
      " [-0.37318238]]\n",
      "t [[ 0.08619854]\n",
      " [-0.49224649]\n",
      " [-0.28292721]\n",
      " ...\n",
      " [ 0.20201387]\n",
      " [ 0.08619854]\n",
      " [-0.37318238]]\n",
      "Current iteration=4, loss=45544.084497598255\n",
      "t [[ 0.0990861 ]\n",
      " [-0.59836331]\n",
      " [-0.3425868 ]\n",
      " ...\n",
      " [ 0.23111747]\n",
      " [ 0.0990861 ]\n",
      " [-0.44017299]]\n",
      "t [[ 0.0990861 ]\n",
      " [-0.59836331]\n",
      " [-0.3425868 ]\n",
      " ...\n",
      " [ 0.23111747]\n",
      " [ 0.0990861 ]\n",
      " [-0.44017299]]\n",
      "t [[ 0.10951462]\n",
      " [-0.69851321]\n",
      " [-0.39880149]\n",
      " ...\n",
      " [ 0.2546072 ]\n",
      " [ 0.10951462]\n",
      " [-0.50030255]]\n",
      "t [[ 0.10951462]\n",
      " [-0.69851321]\n",
      " [-0.39880149]\n",
      " ...\n",
      " [ 0.2546072 ]\n",
      " [ 0.10951462]\n",
      " [-0.50030255]]\n",
      "Current iteration=6, loss=43385.29113038871\n",
      "t [[ 0.11785071]\n",
      " [-0.79310358]\n",
      " [-0.45193801]\n",
      " ...\n",
      " [ 0.27347104]\n",
      " [ 0.11785071]\n",
      " [-0.55476882]]\n",
      "t [[ 0.11785071]\n",
      " [-0.79310358]\n",
      " [-0.45193801]\n",
      " ...\n",
      " [ 0.27347104]\n",
      " [ 0.11785071]\n",
      " [-0.55476882]]\n",
      "t [[ 0.12440184]\n",
      " [-0.8825254 ]\n",
      " [-0.50230877]\n",
      " ...\n",
      " [ 0.28850796]\n",
      " [ 0.12440184]\n",
      " [-0.60452285]]\n",
      "t [[ 0.12440184]\n",
      " [-0.8825254 ]\n",
      " [-0.50230877]\n",
      " ...\n",
      " [ 0.28850796]\n",
      " [ 0.12440184]\n",
      " [-0.60452285]]\n",
      "Current iteration=8, loss=41653.20827124179\n",
      "t [[ 0.12942643]\n",
      " [-0.96714861]\n",
      " [-0.55018105]\n",
      " ...\n",
      " [ 0.30036569]\n",
      " [ 0.12942643]\n",
      " [-0.65032195]]\n",
      "t [[ 0.12942643]\n",
      " [-0.96714861]\n",
      " [-0.55018105]\n",
      " ...\n",
      " [ 0.30036569]\n",
      " [ 0.12942643]\n",
      " [-0.65032195]]\n",
      "t [[ 0.13314243]\n",
      " [-1.0473196 ]\n",
      " [-0.5957847 ]\n",
      " ...\n",
      " [ 0.30957086]\n",
      " [ 0.13314243]\n",
      " [-0.69277162]]\n",
      "loss=40235.598996310764\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.08449417]\n",
      " [-0.04925671]\n",
      " [-0.07883084]\n",
      " ...\n",
      " [ 0.0662039 ]\n",
      " [ 0.02787041]\n",
      " [-0.11216906]]\n",
      "t [[ 0.08449417]\n",
      " [-0.04925671]\n",
      " [-0.07883084]\n",
      " ...\n",
      " [ 0.0662039 ]\n",
      " [ 0.02787041]\n",
      " [-0.11216906]]\n",
      "t [[ 0.16039477]\n",
      " [-0.10485297]\n",
      " [-0.15193273]\n",
      " ...\n",
      " [ 0.11988611]\n",
      " [ 0.05097809]\n",
      " [-0.20875024]]\n",
      "t [[ 0.16039477]\n",
      " [-0.10485297]\n",
      " [-0.15193273]\n",
      " ...\n",
      " [ 0.11988611]\n",
      " [ 0.05097809]\n",
      " [-0.20875024]]\n",
      "Current iteration=2, loss=48294.51357229767\n",
      "t [[ 0.22869242]\n",
      " [-0.1645635 ]\n",
      " [-0.22000495]\n",
      " ...\n",
      " [ 0.16338471]\n",
      " [ 0.07004257]\n",
      " [-0.2927039 ]]\n",
      "t [[ 0.22869242]\n",
      " [-0.1645635 ]\n",
      " [-0.22000495]\n",
      " ...\n",
      " [ 0.16338471]\n",
      " [ 0.07004257]\n",
      " [-0.2927039 ]]\n",
      "t [[ 0.29028939]\n",
      " [-0.22668044]\n",
      " [-0.28365214]\n",
      " ...\n",
      " [ 0.19859489]\n",
      " [ 0.08568256]\n",
      " [-0.36642826]]\n",
      "t [[ 0.29028939]\n",
      " [-0.22668044]\n",
      " [-0.28365214]\n",
      " ...\n",
      " [ 0.19859489]\n",
      " [ 0.08568256]\n",
      " [-0.36642826]]\n",
      "Current iteration=4, loss=45532.967369409445\n",
      "t [[ 0.34598665]\n",
      " [-0.28992858]\n",
      " [-0.34339005]\n",
      " ...\n",
      " [ 0.22703625]\n",
      " [ 0.09842148]\n",
      " [-0.43183616]]\n",
      "t [[ 0.34598665]\n",
      " [-0.28992858]\n",
      " [-0.34339005]\n",
      " ...\n",
      " [ 0.22703625]\n",
      " [ 0.09842148]\n",
      " [-0.43183616]]\n",
      "t [[ 0.39648362]\n",
      " [-0.35337725]\n",
      " [-0.39965658]\n",
      " ...\n",
      " [ 0.24992854]\n",
      " [ 0.1086989 ]\n",
      " [-0.49044429]]\n",
      "t [[ 0.39648362]\n",
      " [-0.35337725]\n",
      " [-0.39965658]\n",
      " ...\n",
      " [ 0.24992854]\n",
      " [ 0.1086989 ]\n",
      " [-0.49044429]]\n",
      "Current iteration=6, loss=43372.8798394657\n",
      "t [[ 0.44238616]\n",
      " [-0.41636113]\n",
      " [-0.45282352]\n",
      " ...\n",
      " [ 0.26825435]\n",
      " [ 0.11688301]\n",
      " [-0.54345375]]\n",
      "t [[ 0.44238616]\n",
      " [-0.41636113]\n",
      " [-0.45282352]\n",
      " ...\n",
      " [ 0.26825435]\n",
      " [ 0.11688301]\n",
      " [-0.54345375]]\n",
      "t [[ 0.48421809]\n",
      " [-0.47841468]\n",
      " [-0.50320739]\n",
      " ...\n",
      " [ 0.28280742]\n",
      " [ 0.12328251]\n",
      " [-0.59181673]]\n",
      "t [[ 0.48421809]\n",
      " [-0.47841468]\n",
      " [-0.50320739]\n",
      " ...\n",
      " [ 0.28280742]\n",
      " [ 0.12328251]\n",
      " [-0.59181673]]\n",
      "Current iteration=8, loss=41640.36251858216\n",
      "t [[ 0.52243284]\n",
      " [-0.53922073]\n",
      " [-0.55107878]\n",
      " ...\n",
      " [ 0.29423051]\n",
      " [ 0.12815673]\n",
      " [-0.63629016]]\n",
      "t [[ 0.52243284]\n",
      " [-0.53922073]\n",
      " [-0.55107878]\n",
      " ...\n",
      " [ 0.29423051]\n",
      " [ 0.12815673]\n",
      " [-0.63629016]]\n",
      "t [[ 0.55742394]\n",
      " [-0.59857123]\n",
      " [-0.59667021]\n",
      " ...\n",
      " [ 0.30304557]\n",
      " [ 0.13172431]\n",
      " [-0.67747812]]\n",
      "loss=40222.69214546318\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08341121]\n",
      " [-0.04715283]\n",
      " [-0.07853561]\n",
      " ...\n",
      " [ 0.06722534]\n",
      " [ 0.02838409]\n",
      " [-0.11562331]]\n",
      "t [[ 0.08341121]\n",
      " [-0.04715283]\n",
      " [-0.07853561]\n",
      " ...\n",
      " [ 0.06722534]\n",
      " [ 0.02838409]\n",
      " [-0.11562331]]\n",
      "t [[ 0.15834294]\n",
      " [-0.10067395]\n",
      " [-0.15134243]\n",
      " ...\n",
      " [ 0.12158776]\n",
      " [ 0.05194819]\n",
      " [-0.21518824]]\n",
      "t [[ 0.15834294]\n",
      " [-0.10067395]\n",
      " [-0.15134243]\n",
      " ...\n",
      " [ 0.12158776]\n",
      " [ 0.05194819]\n",
      " [-0.21518824]]\n",
      "Current iteration=2, loss=48322.646043114335\n",
      "t [[ 0.2257749 ]\n",
      " [-0.15832796]\n",
      " [-0.21912426]\n",
      " ...\n",
      " [ 0.16549512]\n",
      " [ 0.0714217 ]\n",
      " [-0.30173676]]\n",
      "t [[ 0.2257749 ]\n",
      " [-0.15832796]\n",
      " [-0.21912426]\n",
      " ...\n",
      " [ 0.16549512]\n",
      " [ 0.0714217 ]\n",
      " [-0.30173676]]\n",
      "t [[ 0.28659823]\n",
      " [-0.21840327]\n",
      " [-0.28248889]\n",
      " ...\n",
      " [ 0.20090233]\n",
      " [ 0.08743151]\n",
      " [-0.37773584]]\n",
      "t [[ 0.28659823]\n",
      " [-0.21840327]\n",
      " [-0.28248889]\n",
      " ...\n",
      " [ 0.20090233]\n",
      " [ 0.08743151]\n",
      " [-0.37773584]]\n",
      "Current iteration=4, loss=45588.652919358916\n",
      "t [[ 0.34160409]\n",
      " [-0.27962417]\n",
      " [-0.34195395]\n",
      " ...\n",
      " [ 0.22937472]\n",
      " [ 0.10050761]\n",
      " [-0.44515332]]\n",
      "t [[ 0.34160409]\n",
      " [-0.27962417]\n",
      " [-0.34195395]\n",
      " ...\n",
      " [ 0.22937472]\n",
      " [ 0.10050761]\n",
      " [-0.44515332]]\n",
      "t [[ 0.39148362]\n",
      " [-0.34106095]\n",
      " [-0.39795822]\n",
      " ...\n",
      " [ 0.25216539]\n",
      " [ 0.1110947 ]\n",
      " [-0.50554912]]\n",
      "t [[ 0.39148362]\n",
      " [-0.34106095]\n",
      " [-0.39795822]\n",
      " ...\n",
      " [ 0.25216539]\n",
      " [ 0.1110947 ]\n",
      " [-0.50554912]]\n",
      "Current iteration=6, loss=43453.26824321544\n",
      "t [[ 0.43683542]\n",
      " [-0.40204999]\n",
      " [-0.45087369]\n",
      " ...\n",
      " [ 0.27028158]\n",
      " [ 0.11956504]\n",
      " [-0.56015852]]\n",
      "t [[ 0.43683542]\n",
      " [-0.40204999]\n",
      " [-0.45087369]\n",
      " ...\n",
      " [ 0.27028158]\n",
      " [ 0.11956504]\n",
      " [-0.56015852]]\n",
      "t [[ 0.47817662]\n",
      " [-0.46212792]\n",
      " [-0.50101668]\n",
      " ...\n",
      " [ 0.28453615]\n",
      " [ 0.12623051]\n",
      " [-0.60996122]]\n",
      "t [[ 0.47817662]\n",
      " [-0.46212792]\n",
      " [-0.50101668]\n",
      " ...\n",
      " [ 0.28453615]\n",
      " [ 0.12623051]\n",
      " [-0.60996122]]\n",
      "Current iteration=8, loss=41742.25664447836\n",
      "t [[ 0.51595444]\n",
      " [-0.5209799 ]\n",
      " [-0.54865735]\n",
      " ...\n",
      " [ 0.29558718]\n",
      " [ 0.13135304]\n",
      " [-0.65573659]]\n",
      "t [[ 0.51595444]\n",
      " [-0.5209799 ]\n",
      " [-0.54865735]\n",
      " ...\n",
      " [ 0.29558718]\n",
      " [ 0.13135304]\n",
      " [-0.65573659]]\n",
      "t [[ 0.55055661]\n",
      " [-0.57840012]\n",
      " [-0.59402764]\n",
      " ...\n",
      " [ 0.30396922]\n",
      " [ 0.13515331]\n",
      " [-0.6981072 ]]\n",
      "loss=40343.16848589705\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.08327769]\n",
      " [-0.04953875]\n",
      " [-0.07828892]\n",
      " ...\n",
      " [ 0.11994606]\n",
      " [-0.07828892]\n",
      " [ 0.11234648]]\n",
      "t [[ 0.08327769]\n",
      " [-0.04953875]\n",
      " [-0.07828892]\n",
      " ...\n",
      " [ 0.11994606]\n",
      " [-0.07828892]\n",
      " [ 0.11234648]]\n",
      "t [[ 0.15809063]\n",
      " [-0.10509838]\n",
      " [-0.15089886]\n",
      " ...\n",
      " [ 0.22618815]\n",
      " [-0.15089886]\n",
      " [ 0.20963759]]\n",
      "t [[ 0.15809063]\n",
      " [-0.10509838]\n",
      " [-0.15089886]\n",
      " ...\n",
      " [ 0.22618815]\n",
      " [-0.15089886]\n",
      " [ 0.20963759]]\n",
      "Current iteration=2, loss=48327.66863911961\n",
      "t [[ 0.22542279]\n",
      " [-0.16451533]\n",
      " [-0.21852103]\n",
      " ...\n",
      " [ 0.32058289]\n",
      " [-0.21852103]\n",
      " [ 0.29419612]]\n",
      "t [[ 0.22542279]\n",
      " [-0.16451533]\n",
      " [-0.21852103]\n",
      " ...\n",
      " [ 0.32058289]\n",
      " [-0.21852103]\n",
      " [ 0.29419612]]\n",
      "t [[ 0.28615885]\n",
      " [-0.22614187]\n",
      " [-0.28175377]\n",
      " ...\n",
      " [ 0.40474294]\n",
      " [-0.28175377]\n",
      " [ 0.36799372]]\n",
      "t [[ 0.28615885]\n",
      " [-0.22614187]\n",
      " [-0.28175377]\n",
      " ...\n",
      " [ 0.40474294]\n",
      " [-0.28175377]\n",
      " [ 0.36799372]]\n",
      "Current iteration=4, loss=45591.95451326032\n",
      "t [[ 0.34107899]\n",
      " [-0.28875602]\n",
      " [-0.34110784]\n",
      " ...\n",
      " [ 0.48004695]\n",
      " [-0.34110784]\n",
      " [ 0.43267807]]\n",
      "t [[ 0.34107899]\n",
      " [-0.28875602]\n",
      " [-0.34110784]\n",
      " ...\n",
      " [ 0.48004695]\n",
      " [-0.34110784]\n",
      " [ 0.43267807]]\n",
      "t [[ 0.39086451]\n",
      " [-0.35146999]\n",
      " [-0.39701693]\n",
      " ...\n",
      " [ 0.54766528]\n",
      " [-0.39701693]\n",
      " [ 0.48961721]]\n",
      "t [[ 0.39086451]\n",
      " [-0.35146999]\n",
      " [-0.39701693]\n",
      " ...\n",
      " [ 0.54766528]\n",
      " [-0.39701693]\n",
      " [ 0.48961721]]\n",
      "Current iteration=6, loss=43452.65861785799\n",
      "t [[ 0.43610771]\n",
      " [-0.41365094]\n",
      " [-0.44984924]\n",
      " ...\n",
      " [ 0.60858957]\n",
      " [-0.44984924]\n",
      " [ 0.53994539]]\n",
      "t [[ 0.43610771]\n",
      " [-0.41365094]\n",
      " [-0.44984924]\n",
      " ...\n",
      " [ 0.60858957]\n",
      " [-0.44984924]\n",
      " [ 0.53994539]]\n",
      "t [[ 0.47732283]\n",
      " [-0.47485721]\n",
      " [-0.4999182 ]\n",
      " ...\n",
      " [ 0.66366081]\n",
      " [-0.4999182 ]\n",
      " [ 0.58460464]]\n",
      "t [[ 0.47732283]\n",
      " [-0.47485721]\n",
      " [-0.4999182 ]\n",
      " ...\n",
      " [ 0.66366081]\n",
      " [-0.4999182 ]\n",
      " [ 0.58460464]]\n",
      "Current iteration=8, loss=41737.33602711762\n",
      "t [[ 0.51495656]\n",
      " [-0.53478901]\n",
      " [-0.54749177]\n",
      " ...\n",
      " [ 0.71359411]\n",
      " [-0.54749177]\n",
      " [ 0.62437988]]\n",
      "t [[ 0.51495656]\n",
      " [-0.53478901]\n",
      " [-0.54749177]\n",
      " ...\n",
      " [ 0.71359411]\n",
      " [-0.54749177]\n",
      " [ 0.62437988]]\n",
      "t [[ 0.54939756]\n",
      " [-0.59325091]\n",
      " [-0.59280026]\n",
      " ...\n",
      " [ 0.75899962]\n",
      " [-0.59280026]\n",
      " [ 0.65992799]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=40334.15281957277\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.02984562]\n",
      " [-0.14291567]\n",
      " [-0.08380739]\n",
      " ...\n",
      " [ 0.07166644]\n",
      " [ 0.02984562]\n",
      " [-0.12148294]]\n",
      "t [[ 0.02984562]\n",
      " [-0.14291567]\n",
      " [-0.08380739]\n",
      " ...\n",
      " [ 0.07166644]\n",
      " [ 0.02984562]\n",
      " [-0.12148294]]\n",
      "t [[ 0.05429134]\n",
      " [-0.27728532]\n",
      " [-0.16116649]\n",
      " ...\n",
      " [ 0.12898652]\n",
      " [ 0.05429134]\n",
      " [-0.22522456]]\n",
      "t [[ 0.05429134]\n",
      " [-0.27728532]\n",
      " [-0.16116649]\n",
      " ...\n",
      " [ 0.12898652]\n",
      " [ 0.05429134]\n",
      " [-0.22522456]]\n",
      "Current iteration=2, loss=48087.16763971762\n",
      "t [[ 0.07420664]\n",
      " [-0.40351664]\n",
      " [-0.23291487]\n",
      " ...\n",
      " [ 0.17480804]\n",
      " [ 0.07420664]\n",
      " [-0.31479631]]\n",
      "t [[ 0.07420664]\n",
      " [-0.40351664]\n",
      " [-0.23291487]\n",
      " ...\n",
      " [ 0.17480804]\n",
      " [ 0.07420664]\n",
      " [-0.31479631]]\n",
      "t [[ 0.09032927]\n",
      " [-0.52208282]\n",
      " [-0.29976728]\n",
      " ...\n",
      " [ 0.21140102]\n",
      " [ 0.09032927]\n",
      " [-0.39304215]]\n",
      "t [[ 0.09032927]\n",
      " [-0.52208282]\n",
      " [-0.29976728]\n",
      " ...\n",
      " [ 0.21140102]\n",
      " [ 0.09032927]\n",
      " [-0.39304215]]\n",
      "Current iteration=4, loss=45215.50663843034\n",
      "t [[ 0.10327475]\n",
      " [-0.63348581]\n",
      " [-0.36232547]\n",
      " ...\n",
      " [ 0.24055528]\n",
      " [ 0.10327475]\n",
      " [-0.46219052]]\n",
      "t [[ 0.10327475]\n",
      " [-0.63348581]\n",
      " [-0.36232547]\n",
      " ...\n",
      " [ 0.24055528]\n",
      " [ 0.10327475]\n",
      " [-0.46219052]]\n",
      "t [[ 0.11355299]\n",
      " [-0.73822998]\n",
      " [-0.42109411]\n",
      " ...\n",
      " [ 0.26368556]\n",
      " [ 0.11355299]\n",
      " [-0.52397847]]\n",
      "t [[ 0.11355299]\n",
      " [-0.73822998]\n",
      " [-0.42109411]\n",
      " ...\n",
      " [ 0.26368556]\n",
      " [ 0.11355299]\n",
      " [-0.52397847]]\n",
      "Current iteration=6, loss=42997.439325322644\n",
      "t [[ 0.12158576]\n",
      " [-0.83680561]\n",
      " [-0.47649698]\n",
      " ...\n",
      " [ 0.28191515]\n",
      " [ 0.12158576]\n",
      " [-0.57975973]]\n",
      "t [[ 0.12158576]\n",
      " [-0.83680561]\n",
      " [-0.47649698]\n",
      " ...\n",
      " [ 0.28191515]\n",
      " [ 0.12158576]\n",
      " [-0.57975973]]\n",
      "t [[ 0.1277225 ]\n",
      " [-0.92967915]\n",
      " [-0.52889137]\n",
      " ...\n",
      " [ 0.29613896]\n",
      " [ 0.1277225 ]\n",
      " [-0.63059167]]\n",
      "t [[ 0.1277225 ]\n",
      " [-0.92967915]\n",
      " [-0.52889137]\n",
      " ...\n",
      " [ 0.29613896]\n",
      " [ 0.1277225 ]\n",
      " [-0.63059167]]\n",
      "Current iteration=8, loss=41236.54841750037\n",
      "t [[ 0.13225365]\n",
      " [-1.01728802]\n",
      " [-0.57858017]\n",
      " ...\n",
      " [ 0.30707215]\n",
      " [ 0.13225365]\n",
      " [-0.67730347]]\n",
      "t [[ 0.13225365]\n",
      " [-1.01728802]\n",
      " [-0.57858017]\n",
      " ...\n",
      " [ 0.30707215]\n",
      " [ 0.13225365]\n",
      " [-0.67730347]]\n",
      "t [[ 0.1354216 ]\n",
      " [-1.10003827]\n",
      " [-0.62582176]\n",
      " ...\n",
      " [ 0.31528841]\n",
      " [ 0.1354216 ]\n",
      " [-0.72054889]]\n",
      "loss=39807.920424239885\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09012711]\n",
      " [-0.05254049]\n",
      " [-0.08408623]\n",
      " ...\n",
      " [ 0.0706175 ]\n",
      " [ 0.02972844]\n",
      " [-0.119647  ]]\n",
      "t [[ 0.09012711]\n",
      " [-0.05254049]\n",
      " [-0.08408623]\n",
      " ...\n",
      " [ 0.0706175 ]\n",
      " [ 0.02972844]\n",
      " [-0.119647  ]]\n",
      "t [[ 0.17047897]\n",
      " [-0.11229199]\n",
      " [-0.16165549]\n",
      " ...\n",
      " [ 0.12699107]\n",
      " [ 0.05403919]\n",
      " [-0.22156254]]\n",
      "t [[ 0.17047897]\n",
      " [-0.11229199]\n",
      " [-0.16165549]\n",
      " ...\n",
      " [ 0.12699107]\n",
      " [ 0.05403919]\n",
      " [-0.22156254]]\n",
      "Current iteration=2, loss=48078.956698518676\n",
      "t [[ 0.24226182]\n",
      " [-0.17655291]\n",
      " [-0.23355847]\n",
      " ...\n",
      " [ 0.17196081]\n",
      " [ 0.07380725]\n",
      " [-0.30934383]]\n",
      "t [[ 0.24226182]\n",
      " [-0.17655291]\n",
      " [-0.23355847]\n",
      " ...\n",
      " [ 0.17196081]\n",
      " [ 0.07380725]\n",
      " [-0.30934383]]\n",
      "t [[ 0.30656314]\n",
      " [-0.24329623]\n",
      " [-0.30052097]\n",
      " ...\n",
      " [ 0.20778832]\n",
      " [ 0.08977474]\n",
      " [-0.38585253]]\n",
      "t [[ 0.30656314]\n",
      " [-0.24329623]\n",
      " [-0.30052097]\n",
      " ...\n",
      " [ 0.20778832]\n",
      " [ 0.08977474]\n",
      " [-0.38585253]]\n",
      "Current iteration=4, loss=45204.10459456141\n",
      "t [[ 0.36433634]\n",
      " [-0.31104722]\n",
      " [-0.36315373]\n",
      " ...\n",
      " [ 0.23625592]\n",
      " [ 0.10256045]\n",
      " [-0.45332849]]\n",
      "t [[ 0.36433634]\n",
      " [-0.31104722]\n",
      " [-0.36315373]\n",
      " ...\n",
      " [ 0.23625592]\n",
      " [ 0.10256045]\n",
      " [-0.45332849]]\n",
      "t [[ 0.41640346]\n",
      " [-0.37875998]\n",
      " [-0.42196865]\n",
      " ...\n",
      " [ 0.25877154]\n",
      " [ 0.11267671]\n",
      " [-0.51351545]]\n",
      "t [[ 0.41640346]\n",
      " [-0.37875998]\n",
      " [-0.42196865]\n",
      " ...\n",
      " [ 0.25877154]\n",
      " [ 0.11267671]\n",
      " [-0.51351545]]\n",
      "Current iteration=6, loss=42984.8737833061\n",
      "t [[ 0.46346837]\n",
      " [-0.44571017]\n",
      " [-0.47739527]\n",
      " ...\n",
      " [ 0.27645196]\n",
      " [ 0.12054709]\n",
      " [-0.56777023]]\n",
      "t [[ 0.46346837]\n",
      " [-0.44571017]\n",
      " [-0.47739527]\n",
      " ...\n",
      " [ 0.27645196]\n",
      " [ 0.12054709]\n",
      " [-0.56777023]]\n",
      "t [[ 0.50613336]\n",
      " [-0.51140914]\n",
      " [-0.52979549]\n",
      " ...\n",
      " [ 0.29018584]\n",
      " [ 0.12652234]\n",
      " [-0.61715079]]\n",
      "t [[ 0.50613336]\n",
      " [-0.51140914]\n",
      " [-0.52979549]\n",
      " ...\n",
      " [ 0.29018584]\n",
      " [ 0.12652234]\n",
      " [-0.61715079]]\n",
      "Current iteration=8, loss=41223.65234173763\n",
      "t [[ 0.54491498]\n",
      " [-0.57553864]\n",
      " [-0.57947585]\n",
      " ...\n",
      " [ 0.30068244]\n",
      " [ 0.13089388]\n",
      " [-0.66248519]]\n",
      "t [[ 0.54491498]\n",
      " [-0.57553864]\n",
      " [-0.57947585]\n",
      " ...\n",
      " [ 0.30068244]\n",
      " [ 0.13089388]\n",
      " [-0.66248519]]\n",
      "t [[ 0.58025777]\n",
      " [-0.63790248]\n",
      " [-0.62669764]\n",
      " ...\n",
      " [ 0.30851001]\n",
      " [ 0.13390482]\n",
      " [-0.70442495]]\n",
      "loss=39795.030174887695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08897195]\n",
      " [-0.05029635]\n",
      " [-0.08377131]\n",
      " ...\n",
      " [ 0.07170703]\n",
      " [ 0.03027636]\n",
      " [-0.12333153]]\n",
      "t [[ 0.08897195]\n",
      " [-0.05029635]\n",
      " [-0.08377131]\n",
      " ...\n",
      " [ 0.07170703]\n",
      " [ 0.03027636]\n",
      " [-0.12333153]]\n",
      "t [[ 0.16829842]\n",
      " [-0.10783641]\n",
      " [-0.16102588]\n",
      " ...\n",
      " [ 0.12878222]\n",
      " [ 0.05506992]\n",
      " [-0.22839657]]\n",
      "t [[ 0.16829842]\n",
      " [-0.10783641]\n",
      " [-0.16102588]\n",
      " ...\n",
      " [ 0.12878222]\n",
      " [ 0.05506992]\n",
      " [-0.22839657]]\n",
      "Current iteration=2, loss=48108.99333954983\n",
      "t [[ 0.23917228]\n",
      " [-0.16990638]\n",
      " [-0.23261985]\n",
      " ...\n",
      " [ 0.17415073]\n",
      " [ 0.07526769]\n",
      " [-0.31889169]]\n",
      "t [[ 0.23917228]\n",
      " [-0.16990638]\n",
      " [-0.23261985]\n",
      " ...\n",
      " [ 0.17415073]\n",
      " [ 0.07526769]\n",
      " [-0.31889169]]\n",
      "t [[ 0.30266758]\n",
      " [-0.23447517]\n",
      " [-0.29928272]\n",
      " ...\n",
      " [ 0.21014524]\n",
      " [ 0.09162156]\n",
      " [-0.39776063]]\n",
      "t [[ 0.30266758]\n",
      " [-0.23447517]\n",
      " [-0.29928272]\n",
      " ...\n",
      " [ 0.21014524]\n",
      " [ 0.09162156]\n",
      " [-0.39776063]]\n",
      "Current iteration=4, loss=45263.32002200395\n",
      "t [[ 0.35972616]\n",
      " [-0.30006782]\n",
      " [-0.36162727]\n",
      " ...\n",
      " [ 0.23860076]\n",
      " [ 0.10475796]\n",
      " [-0.46730744]]\n",
      "t [[ 0.35972616]\n",
      " [-0.30006782]\n",
      " [-0.36162727]\n",
      " ...\n",
      " [ 0.23860076]\n",
      " [ 0.10475796]\n",
      " [-0.46730744]]\n",
      "t [[ 0.41116037]\n",
      " [-0.36563991]\n",
      " [-0.42016621]\n",
      " ...\n",
      " [ 0.26096267]\n",
      " [ 0.11519512]\n",
      " [-0.52932538]]\n",
      "t [[ 0.41116037]\n",
      " [-0.36563991]\n",
      " [-0.42016621]\n",
      " ...\n",
      " [ 0.26096267]\n",
      " [ 0.11519512]\n",
      " [-0.52932538]]\n",
      "Current iteration=6, loss=43069.902958909704\n",
      "t [[ 0.45766548]\n",
      " [-0.43046945]\n",
      " [-0.47532916]\n",
      " ...\n",
      " [ 0.27837525]\n",
      " [ 0.12336117]\n",
      " [-0.58521   ]]\n",
      "t [[ 0.45766548]\n",
      " [-0.43046945]\n",
      " [-0.47532916]\n",
      " ...\n",
      " [ 0.27837525]\n",
      " [ 0.12336117]\n",
      " [-0.58521   ]]\n",
      "t [[ 0.49983588]\n",
      " [-0.49407059]\n",
      " [-0.5274776 ]\n",
      " ...\n",
      " [ 0.29174837]\n",
      " [ 0.12961044]\n",
      " [-0.63605005]]\n",
      "t [[ 0.49983588]\n",
      " [-0.49407059]\n",
      " [-0.5274776 ]\n",
      " ...\n",
      " [ 0.29174837]\n",
      " [ 0.12961044]\n",
      " [-0.63605005]]\n",
      "Current iteration=8, loss=41330.88043575989\n",
      "t [[ 0.53818079]\n",
      " [-0.55612794]\n",
      " [-0.57691746]\n",
      " ...\n",
      " [ 0.30180826]\n",
      " [ 0.13423714]\n",
      " [-0.68269846]]\n",
      "t [[ 0.53818079]\n",
      " [-0.55612794]\n",
      " [-0.57691746]\n",
      " ...\n",
      " [ 0.30180826]\n",
      " [ 0.13423714]\n",
      " [-0.68269846]]\n",
      "t [[ 0.57313804]\n",
      " [-0.61644797]\n",
      " [-0.62390926]\n",
      " ...\n",
      " [ 0.30913699]\n",
      " [ 0.13748665]\n",
      " [-0.72582706]]\n",
      "loss=39921.24390840758\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.08882953]\n",
      " [-0.05284134]\n",
      " [-0.08350818]\n",
      " ...\n",
      " [ 0.12794246]\n",
      " [-0.08350818]\n",
      " [ 0.11983624]]\n",
      "t [[ 0.08882953]\n",
      " [-0.05284134]\n",
      " [-0.08350818]\n",
      " ...\n",
      " [ 0.12794246]\n",
      " [-0.08350818]\n",
      " [ 0.11983624]]\n",
      "t [[ 0.16803018]\n",
      " [-0.11253135]\n",
      " [-0.16055625]\n",
      " ...\n",
      " [ 0.24029628]\n",
      " [-0.16055625]\n",
      " [ 0.22254672]]\n",
      "t [[ 0.16803018]\n",
      " [-0.11253135]\n",
      " [-0.16055625]\n",
      " ...\n",
      " [ 0.24029628]\n",
      " [-0.16055625]\n",
      " [ 0.22254672]]\n",
      "Current iteration=2, loss=48114.09740813433\n",
      "t [[ 0.23879946]\n",
      " [-0.17644401]\n",
      " [-0.23198506]\n",
      " ...\n",
      " [ 0.33931978]\n",
      " [-0.23198506]\n",
      " [ 0.31095576]]\n",
      "t [[ 0.23879946]\n",
      " [-0.17644401]\n",
      " [-0.23198506]\n",
      " ...\n",
      " [ 0.33931978]\n",
      " [-0.23198506]\n",
      " [ 0.31095576]]\n",
      "t [[ 0.30220261]\n",
      " [-0.24262508]\n",
      " [-0.29851296]\n",
      " ...\n",
      " [ 0.42694916]\n",
      " [-0.29851296]\n",
      " [ 0.38742576]]\n",
      "t [[ 0.30220261]\n",
      " [-0.24262508]\n",
      " [-0.29851296]\n",
      " ...\n",
      " [ 0.42694916]\n",
      " [-0.29851296]\n",
      " [ 0.38742576]]\n",
      "Current iteration=4, loss=45266.21080490663\n",
      "t [[ 0.35916804]\n",
      " [-0.30966256]\n",
      " [-0.36074478]\n",
      " ...\n",
      " [ 0.5048171 ]\n",
      " [-0.36074478]\n",
      " [ 0.45390086]]\n",
      "t [[ 0.35916804]\n",
      " [-0.30966256]\n",
      " [-0.36074478]\n",
      " ...\n",
      " [ 0.5048171 ]\n",
      " [-0.36074478]\n",
      " [ 0.45390086]]\n",
      "t [[ 0.41049711]\n",
      " [-0.37655952]\n",
      " [-0.41918752]\n",
      " ...\n",
      " [ 0.57429143]\n",
      " [-0.41918752]\n",
      " [ 0.51197145]]\n",
      "t [[ 0.41049711]\n",
      " [-0.37655952]\n",
      " [-0.41918752]\n",
      " ...\n",
      " [ 0.57429143]\n",
      " [-0.41918752]\n",
      " [ 0.51197145]]\n",
      "Current iteration=6, loss=43068.45087139119\n",
      "t [[ 0.45687888]\n",
      " [-0.44262749]\n",
      " [-0.47426654]\n",
      " ...\n",
      " [ 0.63651692]\n",
      " [-0.47426654]\n",
      " [ 0.56293815]]\n",
      "t [[ 0.45687888]\n",
      " [-0.44262749]\n",
      " [-0.47426654]\n",
      " ...\n",
      " [ 0.63651692]\n",
      " [-0.47426654]\n",
      " [ 0.56293815]]\n",
      "t [[ 0.49890549]\n",
      " [-0.5074035 ]\n",
      " [-0.52634021]\n",
      " ...\n",
      " [ 0.69245356]\n",
      " [-0.52634021]\n",
      " [ 0.60786757]]\n",
      "t [[ 0.49890549]\n",
      " [-0.5074035 ]\n",
      " [-0.52634021]\n",
      " ...\n",
      " [ 0.69245356]\n",
      " [-0.52634021]\n",
      " [ 0.60786757]]\n",
      "Current iteration=8, loss=41324.83602409204\n",
      "t [[ 0.53708651]\n",
      " [-0.57058753]\n",
      " [-0.57571209]\n",
      " ...\n",
      " [ 0.74290929]\n",
      " [-0.57571209]\n",
      " [ 0.64763833]]\n",
      "t [[ 0.53708651]\n",
      " [-0.57058753]\n",
      " [-0.57571209]\n",
      " ...\n",
      " [ 0.74290929]\n",
      " [-0.57571209]\n",
      " [ 0.64763833]]\n",
      "t [[ 0.5718614 ]\n",
      " [-0.63199632]\n",
      " [-0.62264104]\n",
      " ...\n",
      " [ 0.78856708]\n",
      " [-0.62264104]\n",
      " [ 0.68297805]]\n",
      "loss=39910.93600472868\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03171097]\n",
      " [-0.1518479 ]\n",
      " [-0.08904535]\n",
      " ...\n",
      " [ 0.0761456 ]\n",
      " [ 0.03171097]\n",
      " [-0.12907563]]\n",
      "t [[ 0.03171097]\n",
      " [-0.1518479 ]\n",
      " [-0.08904535]\n",
      " ...\n",
      " [ 0.0761456 ]\n",
      " [ 0.03171097]\n",
      " [-0.12907563]]\n",
      "t [[ 0.05732744]\n",
      " [-0.29404963]\n",
      " [-0.17081273]\n",
      " ...\n",
      " [ 0.13609912]\n",
      " [ 0.05732744]\n",
      " [-0.23812785]]\n",
      "t [[ 0.05732744]\n",
      " [-0.29404963]\n",
      " [-0.17081273]\n",
      " ...\n",
      " [ 0.13609912]\n",
      " [ 0.05732744]\n",
      " [-0.23812785]]\n",
      "Current iteration=2, loss=47875.09522826213\n",
      "t [[ 0.07789426]\n",
      " [-0.42709761]\n",
      " [-0.24630899]\n",
      " ...\n",
      " [ 0.18327912]\n",
      " [ 0.07789426]\n",
      " [-0.33144526]]\n",
      "t [[ 0.07789426]\n",
      " [-0.42709761]\n",
      " [-0.24630899]\n",
      " ...\n",
      " [ 0.18327912]\n",
      " [ 0.07789426]\n",
      " [-0.33144526]]\n",
      "t [[ 0.09428562]\n",
      " [-0.55156538]\n",
      " [-0.31638173]\n",
      " ...\n",
      " [ 0.22036163]\n",
      " [ 0.09428562]\n",
      " [-0.41238011]]\n",
      "t [[ 0.09428562]\n",
      " [-0.55156538]\n",
      " [-0.31638173]\n",
      " ...\n",
      " [ 0.22036163]\n",
      " [ 0.09428562]\n",
      " [-0.41238011]]\n",
      "Current iteration=4, loss=44896.53152568758\n",
      "t [[ 0.10722019]\n",
      " [-0.66805859]\n",
      " [-0.38173476]\n",
      " ...\n",
      " [ 0.24942032]\n",
      " [ 0.10722019]\n",
      " [-0.48350954]]\n",
      "t [[ 0.10722019]\n",
      " [-0.66805859]\n",
      " [-0.38173476]\n",
      " ...\n",
      " [ 0.24942032]\n",
      " [ 0.10722019]\n",
      " [-0.48350954]]\n",
      "t [[ 0.11728504]\n",
      " [-0.77718124]\n",
      " [-0.44295087]\n",
      " ...\n",
      " [ 0.27206825]\n",
      " [ 0.11728504]\n",
      " [-0.54680589]]\n",
      "t [[ 0.11728504]\n",
      " [-0.77718124]\n",
      " [-0.44295087]\n",
      " ...\n",
      " [ 0.27206825]\n",
      " [ 0.11728504]\n",
      " [-0.54680589]]\n",
      "Current iteration=6, loss=42625.30807011229\n",
      "t [[ 0.12495943]\n",
      " [-0.87951538]\n",
      " [-0.50051388]\n",
      " ...\n",
      " [ 0.2895666 ]\n",
      " [ 0.12495943]\n",
      " [-0.60377975]]\n",
      "t [[ 0.12495943]\n",
      " [-0.87951538]\n",
      " [-0.50051388]\n",
      " ...\n",
      " [ 0.2895666 ]\n",
      " [ 0.12495943]\n",
      " [-0.60377975]]\n",
      "t [[ 0.13063573]\n",
      " [-0.97560996]\n",
      " [-0.55482759]\n",
      " ...\n",
      " [ 0.3029052 ]\n",
      " [ 0.13063573]\n",
      " [-0.65559193]]\n",
      "t [[ 0.13063573]\n",
      " [-0.97560996]\n",
      " [-0.55482759]\n",
      " ...\n",
      " [ 0.3029052 ]\n",
      " [ 0.13063573]\n",
      " [-0.65559193]]\n",
      "Current iteration=8, loss=40840.56410193572\n",
      "t [[ 0.1346365 ]\n",
      " [-1.06597518]\n",
      " [-0.60623136]\n",
      " ...\n",
      " [ 0.31286396]\n",
      " [ 0.1346365 ]\n",
      " [-0.7031392 ]]\n",
      "t [[ 0.1346365 ]\n",
      " [-1.06597518]\n",
      " [-0.60623136]\n",
      " ...\n",
      " [ 0.31286396]\n",
      " [ 0.1346365 ]\n",
      " [-0.7031392 ]]\n",
      "t [[ 0.13722828]\n",
      " [-1.15108058]\n",
      " [-0.65501249]\n",
      " ...\n",
      " [ 0.32006033]\n",
      " [ 0.13722828]\n",
      " [-0.74711908]]\n",
      "loss=39404.67172249254\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09576006]\n",
      " [-0.05582427]\n",
      " [-0.08934162]\n",
      " ...\n",
      " [ 0.07503109]\n",
      " [ 0.03158647]\n",
      " [-0.12712494]]\n",
      "t [[ 0.09576006]\n",
      " [-0.05582427]\n",
      " [-0.08934162]\n",
      " ...\n",
      " [ 0.07503109]\n",
      " [ 0.03158647]\n",
      " [-0.12712494]]\n",
      "t [[ 0.18048754]\n",
      " [-0.1197867 ]\n",
      " [-0.17132776]\n",
      " ...\n",
      " [ 0.13398571]\n",
      " [ 0.05705836]\n",
      " [-0.23423763]]\n",
      "t [[ 0.18048754]\n",
      " [-0.1197867 ]\n",
      " [-0.17132776]\n",
      " ...\n",
      " [ 0.13398571]\n",
      " [ 0.05705836]\n",
      " [-0.23423763]]\n",
      "Current iteration=2, loss=47866.54274622556\n",
      "t [[ 0.25563388]\n",
      " [-0.18864609]\n",
      " [-0.24698105]\n",
      " ...\n",
      " [ 0.18027306]\n",
      " [ 0.07746717]\n",
      " [-0.32565755]]\n",
      "t [[ 0.25563388]\n",
      " [-0.18864609]\n",
      " [-0.24698105]\n",
      " ...\n",
      " [ 0.18027306]\n",
      " [ 0.07746717]\n",
      " [-0.32565755]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.32249366]\n",
      " [-0.26002654]\n",
      " [-0.31716215]\n",
      " ...\n",
      " [ 0.21655918]\n",
      " [ 0.09369218]\n",
      " [-0.40475747]]\n",
      "t [[ 0.32249366]\n",
      " [-0.26002654]\n",
      " [-0.31716215]\n",
      " ...\n",
      " [ 0.21655918]\n",
      " [ 0.09369218]\n",
      " [-0.40475747]]\n",
      "Current iteration=4, loss=44884.87713903797\n",
      "t [[ 0.3821875 ]\n",
      " [-0.33224508]\n",
      " [-0.38258533]\n",
      " ...\n",
      " [ 0.24490888]\n",
      " [ 0.10645585]\n",
      " [-0.4741273 ]]\n",
      "t [[ 0.3821875 ]\n",
      " [-0.33224508]\n",
      " [-0.38258533]\n",
      " ...\n",
      " [ 0.24490888]\n",
      " [ 0.10645585]\n",
      " [-0.4741273 ]]\n",
      "t [[ 0.43566948]\n",
      " [-0.40414308]\n",
      " [-0.44384163]\n",
      " ...\n",
      " [ 0.26692715]\n",
      " [ 0.11634794]\n",
      " [-0.5357463 ]]\n",
      "t [[ 0.43566948]\n",
      " [-0.40414308]\n",
      " [-0.44384163]\n",
      " ...\n",
      " [ 0.26692715]\n",
      " [ 0.11634794]\n",
      " [-0.5357463 ]]\n",
      "Current iteration=6, loss=42612.62022138975\n",
      "t [[ 0.48374732]\n",
      " [-0.4749445 ]\n",
      " [-0.50142134]\n",
      " ...\n",
      " [ 0.28386743]\n",
      " [ 0.1238497 ]\n",
      " [-0.59112775]]\n",
      "t [[ 0.48374732]\n",
      " [-0.4749445 ]\n",
      " [-0.50142134]\n",
      " ...\n",
      " [ 0.28386743]\n",
      " [ 0.1238497 ]\n",
      " [-0.59112775]]\n",
      "t [[ 0.52710529]\n",
      " [-0.54414617]\n",
      " [-0.55573337]\n",
      " ...\n",
      " [ 0.29671217]\n",
      " [ 0.1293549 ]\n",
      " [-0.6414323 ]]\n",
      "t [[ 0.52710529]\n",
      " [-0.54414617]\n",
      " [-0.55573337]\n",
      " ...\n",
      " [ 0.29671217]\n",
      " [ 0.1293549 ]\n",
      " [-0.6414323 ]]\n",
      "Current iteration=8, loss=40827.64171367722\n",
      "t [[ 0.56632501]\n",
      " [-0.61143699]\n",
      " [-0.60712103]\n",
      " ...\n",
      " [ 0.30623438]\n",
      " [ 0.13318713]\n",
      " [-0.68755469]]\n",
      "t [[ 0.56632501]\n",
      " [-0.61143699]\n",
      " [-0.60712103]\n",
      " ...\n",
      " [ 0.30623438]\n",
      " [ 0.13318713]\n",
      " [-0.68755469]]\n",
      "t [[ 0.601903  ]\n",
      " [-0.67663973]\n",
      " [-0.65587478]\n",
      " ...\n",
      " [ 0.31304528]\n",
      " [ 0.13561367]\n",
      " [-0.73018928]]\n",
      "loss=39391.81219355702\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0945327 ]\n",
      " [-0.05343987]\n",
      " [-0.08900702]\n",
      " ...\n",
      " [ 0.07618872]\n",
      " [ 0.03216864]\n",
      " [-0.13103975]]\n",
      "t [[ 0.0945327 ]\n",
      " [-0.05343987]\n",
      " [-0.08900702]\n",
      " ...\n",
      " [ 0.07618872]\n",
      " [ 0.03216864]\n",
      " [-0.13103975]]\n",
      "t [[ 0.17817926]\n",
      " [-0.11505482]\n",
      " [-0.17065884]\n",
      " ...\n",
      " [ 0.13586343]\n",
      " [ 0.05814924]\n",
      " [-0.2414636 ]]\n",
      "t [[ 0.17817926]\n",
      " [-0.11505482]\n",
      " [-0.17065884]\n",
      " ...\n",
      " [ 0.13586343]\n",
      " [ 0.05814924]\n",
      " [-0.2414636 ]]\n",
      "Current iteration=2, loss=47898.4831645451\n",
      "t [[ 0.25237496]\n",
      " [-0.18158908]\n",
      " [-0.24598466]\n",
      " ...\n",
      " [ 0.18253578]\n",
      " [ 0.07900773]\n",
      " [-0.33571054]]\n",
      "t [[ 0.25237496]\n",
      " [-0.18158908]\n",
      " [-0.24598466]\n",
      " ...\n",
      " [ 0.18253578]\n",
      " [ 0.07900773]\n",
      " [-0.33571054]]\n",
      "t [[ 0.31839838]\n",
      " [-0.25066238]\n",
      " [-0.31584937]\n",
      " ...\n",
      " [ 0.21895525]\n",
      " [ 0.09563491]\n",
      " [-0.4172501 ]]\n",
      "t [[ 0.31839838]\n",
      " [-0.25066238]\n",
      " [-0.31584937]\n",
      " ...\n",
      " [ 0.21895525]\n",
      " [ 0.09563491]\n",
      " [-0.4172501 ]]\n",
      "Current iteration=4, loss=44947.57820274364\n",
      "t [[ 0.3773567 ]\n",
      " [-0.32059192]\n",
      " [-0.3809694 ]\n",
      " ...\n",
      " [ 0.24724652]\n",
      " [ 0.10876204]\n",
      " [-0.48874614]]\n",
      "t [[ 0.3773567 ]\n",
      " [-0.32059192]\n",
      " [-0.3809694 ]\n",
      " ...\n",
      " [ 0.24724652]\n",
      " [ 0.10876204]\n",
      " [-0.48874614]]\n",
      "t [[ 0.43019269]\n",
      " [-0.39022121]\n",
      " [-0.44193653]\n",
      " ...\n",
      " [ 0.26905624]\n",
      " [ 0.11898555]\n",
      " [-0.55223402]]\n",
      "t [[ 0.43019269]\n",
      " [-0.39022121]\n",
      " [-0.44193653]\n",
      " ...\n",
      " [ 0.26905624]\n",
      " [ 0.11898555]\n",
      " [-0.55223402]]\n",
      "Current iteration=6, loss=42702.17240891591\n",
      "t [[ 0.47770402]\n",
      " [-0.45877729]\n",
      " [-0.49924091]\n",
      " ...\n",
      " [ 0.28566822]\n",
      " [ 0.12679178]\n",
      " [-0.60927036]]\n",
      "t [[ 0.47770402]\n",
      " [-0.45877729]\n",
      " [-0.49924091]\n",
      " ...\n",
      " [ 0.28566822]\n",
      " [ 0.12679178]\n",
      " [-0.60927036]]\n",
      "t [[ 0.52056569]\n",
      " [-0.52576053]\n",
      " [-0.55329081]\n",
      " ...\n",
      " [ 0.29808827]\n",
      " [ 0.1325784 ]\n",
      " [-0.66104992]]\n",
      "t [[ 0.52056569]\n",
      " [-0.52576053]\n",
      " [-0.55329081]\n",
      " ...\n",
      " [ 0.29808827]\n",
      " [ 0.1325784 ]\n",
      " [-0.66104992]]\n",
      "Current iteration=8, loss=40940.01082242466\n",
      "t [[ 0.55935085]\n",
      " [-0.59086324]\n",
      " [-0.60442871]\n",
      " ...\n",
      " [ 0.30710803]\n",
      " [ 0.13667208]\n",
      " [-0.70849475]]\n",
      "t [[ 0.55935085]\n",
      " [-0.59086324]\n",
      " [-0.60442871]\n",
      " ...\n",
      " [ 0.30710803]\n",
      " [ 0.13667208]\n",
      " [-0.70849475]]\n",
      "t [[ 0.59454829]\n",
      " [-0.65391122]\n",
      " [-0.65294413]\n",
      " ...\n",
      " [ 0.31335375]\n",
      " [ 0.13934254]\n",
      " [-0.75232125]]\n",
      "loss=39523.50555101233\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09438138]\n",
      " [-0.05614392]\n",
      " [-0.08872744]\n",
      " ...\n",
      " [ 0.13593887]\n",
      " [-0.08872744]\n",
      " [ 0.12732601]]\n",
      "t [[ 0.09438138]\n",
      " [-0.05614392]\n",
      " [-0.08872744]\n",
      " ...\n",
      " [ 0.13593887]\n",
      " [-0.08872744]\n",
      " [ 0.12732601]]\n",
      "t [[ 0.17789517]\n",
      " [-0.12001727]\n",
      " [-0.17016358]\n",
      " ...\n",
      " [ 0.25428373]\n",
      " [-0.17016358]\n",
      " [ 0.2353233 ]]\n",
      "t [[ 0.17789517]\n",
      " [-0.12001727]\n",
      " [-0.17016358]\n",
      " ...\n",
      " [ 0.25428373]\n",
      " [-0.17016358]\n",
      " [ 0.2353233 ]]\n",
      "Current iteration=2, loss=47903.6433538307\n",
      "t [[ 0.25198158]\n",
      " [-0.18847022]\n",
      " [-0.24531925]\n",
      " ...\n",
      " [ 0.35774905]\n",
      " [-0.24531925]\n",
      " [ 0.32738549]]\n",
      "t [[ 0.25198158]\n",
      " [-0.18847022]\n",
      " [-0.24531925]\n",
      " ...\n",
      " [ 0.35774905]\n",
      " [-0.24531925]\n",
      " [ 0.32738549]]\n",
      "t [[ 0.31790762]\n",
      " [-0.25921372]\n",
      " [-0.31504633]\n",
      " ...\n",
      " [ 0.44863117]\n",
      " [-0.31504633]\n",
      " [ 0.40630861]]\n",
      "t [[ 0.31790762]\n",
      " [-0.25921372]\n",
      " [-0.31504633]\n",
      " ...\n",
      " [ 0.44863117]\n",
      " [-0.31504633]\n",
      " [ 0.40630861]]\n",
      "Current iteration=4, loss=44950.02282724235\n",
      "t [[ 0.37676441]\n",
      " [-0.33063774]\n",
      " [-0.38005223]\n",
      " ...\n",
      " [ 0.52883971]\n",
      " [-0.38005223]\n",
      " [ 0.47435807]]\n",
      "t [[ 0.37676441]\n",
      " [-0.33063774]\n",
      " [-0.38005223]\n",
      " ...\n",
      " [ 0.52883971]\n",
      " [-0.38005223]\n",
      " [ 0.47435807]]\n",
      "t [[ 0.42948288]\n",
      " [-0.40163861]\n",
      " [-0.4409223 ]\n",
      " ...\n",
      " [ 0.59995348]\n",
      " [-0.4409223 ]\n",
      " [ 0.53335982]]\n",
      "t [[ 0.42948288]\n",
      " [-0.40163861]\n",
      " [-0.4409223 ]\n",
      " ...\n",
      " [ 0.59995348]\n",
      " [-0.4409223 ]\n",
      " [ 0.53335982]]\n",
      "Current iteration=6, loss=42699.86459674822\n",
      "t [[ 0.47685483]\n",
      " [-0.47147936]\n",
      " [-0.49814206]\n",
      " ...\n",
      " [ 0.66327792]\n",
      " [-0.49814206]\n",
      " [ 0.58478692]]\n",
      "t [[ 0.47685483]\n",
      " [-0.47147936]\n",
      " [-0.49814206]\n",
      " ...\n",
      " [ 0.66327792]\n",
      " [-0.49814206]\n",
      " [ 0.58478692]]\n",
      "t [[ 0.51955392]\n",
      " [-0.53968404]\n",
      " [-0.55211641]\n",
      " ...\n",
      " [ 0.71989576]\n",
      " [-0.55211641]\n",
      " [ 0.62983258]]\n",
      "t [[ 0.51955392]\n",
      " [-0.53968404]\n",
      " [-0.55211641]\n",
      " ...\n",
      " [ 0.71989576]\n",
      " [-0.55211641]\n",
      " [ 0.62983258]]\n",
      "Current iteration=8, loss=40932.85823051646\n",
      "t [[ 0.55815453]\n",
      " [-0.6059604 ]\n",
      " [-0.60318541]\n",
      " ...\n",
      " [ 0.77070924]\n",
      " [-0.60318541]\n",
      " [ 0.6694688 ]]\n",
      "t [[ 0.55815453]\n",
      " [-0.6059604 ]\n",
      " [-0.60318541]\n",
      " ...\n",
      " [ 0.77070924]\n",
      " [-0.60318541]\n",
      " [ 0.6694688 ]]\n",
      "t [[ 0.59314787]\n",
      " [-0.6701443 ]\n",
      " [-0.65163681]\n",
      " ...\n",
      " [ 0.81647422]\n",
      " [-0.65163681]\n",
      " [ 0.70449253]]\n",
      "loss=39511.9453655296\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03357632]\n",
      " [-0.16078013]\n",
      " [-0.09428331]\n",
      " ...\n",
      " [ 0.08062475]\n",
      " [ 0.03357632]\n",
      " [-0.13666831]]\n",
      "t [[ 0.03357632]\n",
      " [-0.16078013]\n",
      " [-0.09428331]\n",
      " ...\n",
      " [ 0.08062475]\n",
      " [ 0.03357632]\n",
      " [-0.13666831]]\n",
      "t [[ 0.06032181]\n",
      " [-0.31074764]\n",
      " [-0.18040907]\n",
      " ...\n",
      " [ 0.14310077]\n",
      " [ 0.06032181]\n",
      " [-0.25089404]]\n",
      "t [[ 0.06032181]\n",
      " [-0.31074764]\n",
      " [-0.18040907]\n",
      " ...\n",
      " [ 0.14310077]\n",
      " [ 0.06032181]\n",
      " [-0.25089404]]\n",
      "Current iteration=2, loss=47666.09889024648\n",
      "t [[ 0.08147891]\n",
      " [-0.45049099]\n",
      " [-0.25957493]\n",
      " ...\n",
      " [ 0.19148877]\n",
      " [ 0.08147891]\n",
      " [-0.34777302]]\n",
      "t [[ 0.08147891]\n",
      " [-0.45049099]\n",
      " [-0.25957493]\n",
      " ...\n",
      " [ 0.19148877]\n",
      " [ 0.08147891]\n",
      " [-0.34777302]]\n",
      "t [[ 0.0980722 ]\n",
      " [-0.58069771]\n",
      " [-0.33277513]\n",
      " ...\n",
      " [ 0.22890934]\n",
      " [ 0.0980722 ]\n",
      " [-0.43121345]]\n",
      "t [[ 0.0980722 ]\n",
      " [-0.58069771]\n",
      " [-0.33277513]\n",
      " ...\n",
      " [ 0.22890934]\n",
      " [ 0.0980722 ]\n",
      " [-0.43121345]]\n",
      "Current iteration=4, loss=44586.78592035404\n",
      "t [[ 0.11093159]\n",
      " [-0.70209017]\n",
      " [-0.40082384]\n",
      " ...\n",
      " [ 0.2577387 ]\n",
      " [ 0.11093159]\n",
      " [-0.50416239]]\n",
      "t [[ 0.11093159]\n",
      " [-0.70209017]\n",
      " [-0.40082384]\n",
      " ...\n",
      " [ 0.2577387 ]\n",
      " [ 0.11093159]\n",
      " [-0.50416239]]\n",
      "t [[ 0.12072548]\n",
      " [-0.81538308]\n",
      " [-0.46438671]\n",
      " ...\n",
      " [ 0.27979577]\n",
      " [ 0.12072548]\n",
      " [-0.56883401]]\n",
      "t [[ 0.12072548]\n",
      " [-0.81538308]\n",
      " [-0.46438671]\n",
      " ...\n",
      " [ 0.27979577]\n",
      " [ 0.12072548]\n",
      " [-0.56883401]]\n",
      "Current iteration=6, loss=42268.02878214176\n",
      "t [[ 0.1279926 ]\n",
      " [-0.92125924]\n",
      " [-0.5240102 ]\n",
      " ...\n",
      " [ 0.29648073]\n",
      " [ 0.1279926 ]\n",
      " [-0.62689484]]\n",
      "t [[ 0.1279926 ]\n",
      " [-0.92125924]\n",
      " [-0.5240102 ]\n",
      " ...\n",
      " [ 0.29648073]\n",
      " [ 0.1279926 ]\n",
      " [-0.62689484]]\n",
      "t [[ 0.1331688 ]\n",
      " [-1.02035684]\n",
      " [-0.58014599]\n",
      " ...\n",
      " [ 0.30887625]\n",
      " [ 0.1331688 ]\n",
      " [-0.67960509]]\n",
      "t [[ 0.1331688 ]\n",
      " [-1.02035684]\n",
      " [-0.58014599]\n",
      " ...\n",
      " [ 0.30887625]\n",
      " [ 0.1331688 ]\n",
      " [-0.67960509]]\n",
      "Current iteration=8, loss=40463.850366758605\n",
      "t [[ 0.13660865]\n",
      " [-1.11326383]\n",
      " [-0.63317052]\n",
      " ...\n",
      " [ 0.31782355]\n",
      " [ 0.13660865]\n",
      " [-0.72792415]]\n",
      "t [[ 0.13660865]\n",
      " [-1.11326383]\n",
      " [-0.63317052]\n",
      " ...\n",
      " [ 0.31782355]\n",
      " [ 0.13660865]\n",
      " [-0.72792415]]\n",
      "t [[ 0.1386023 ]\n",
      " [-1.20051661]\n",
      " [-0.6834003 ]\n",
      " ...\n",
      " [ 0.3239803 ]\n",
      " [ 0.1386023 ]\n",
      " [-0.77258861]]\n",
      "loss=39023.93979821859\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.101393  ]\n",
      " [-0.05910805]\n",
      " [-0.09459701]\n",
      " ...\n",
      " [ 0.07944468]\n",
      " [ 0.03344449]\n",
      " [-0.13460288]]\n",
      "t [[ 0.101393  ]\n",
      " [-0.05910805]\n",
      " [-0.09459701]\n",
      " ...\n",
      " [ 0.07944468]\n",
      " [ 0.03344449]\n",
      " [-0.13460288]]\n",
      "t [[ 0.1904206 ]\n",
      " [-0.12733702]\n",
      " [-0.18094961]\n",
      " ...\n",
      " [ 0.14087019]\n",
      " [ 0.06003567]\n",
      " [-0.24677572]]\n",
      "t [[ 0.1904206 ]\n",
      " [-0.12733702]\n",
      " [-0.18094961]\n",
      " ...\n",
      " [ 0.14087019]\n",
      " [ 0.06003567]\n",
      " [-0.24677572]]\n",
      "Current iteration=2, loss=47657.22152032216\n",
      "t [[ 0.26881086]\n",
      " [-0.20083895]\n",
      " [-0.26027421]\n",
      " ...\n",
      " [ 0.18832604]\n",
      " [ 0.08102383]\n",
      " [-0.34165094]]\n",
      "t [[ 0.26881086]\n",
      " [-0.20083895]\n",
      " [-0.26027421]\n",
      " ...\n",
      " [ 0.18832604]\n",
      " [ 0.08102383]\n",
      " [-0.34165094]]\n",
      "t [[ 0.33808806]\n",
      " [-0.27685991]\n",
      " [-0.33358032]\n",
      " ...\n",
      " [ 0.22492105]\n",
      " [ 0.09743952]\n",
      " [-0.42316037]]\n",
      "t [[ 0.33808806]\n",
      " [-0.27685991]\n",
      " [-0.33358032]\n",
      " ...\n",
      " [ 0.22492105]\n",
      " [ 0.09743952]\n",
      " [-0.42316037]]\n",
      "Current iteration=4, loss=44574.90884903374\n",
      "t [[ 0.39955469]\n",
      " [-0.35350221]\n",
      " [-0.40169414]\n",
      " ...\n",
      " [ 0.25302113]\n",
      " [ 0.11011689]\n",
      " [-0.49426507]]\n",
      "t [[ 0.39955469]\n",
      " [-0.35350221]\n",
      " [-0.40169414]\n",
      " ...\n",
      " [ 0.25302113]\n",
      " [ 0.11011689]\n",
      " [-0.49426507]]\n",
      "t [[ 0.45430582]\n",
      " [-0.42949908]\n",
      " [-0.46529065]\n",
      " ...\n",
      " [ 0.27443568]\n",
      " [ 0.11972736]\n",
      " [-0.55718619]]\n",
      "t [[ 0.45430582]\n",
      " [-0.42949908]\n",
      " [-0.46529065]\n",
      " ...\n",
      " [ 0.27443568]\n",
      " [ 0.11972736]\n",
      " [-0.55718619]]\n",
      "Current iteration=6, loss=42255.246063960425\n",
      "t [[ 0.50325832]\n",
      " [-0.5040315 ]\n",
      " [-0.52492349]\n",
      " ...\n",
      " [ 0.29055576]\n",
      " [ 0.12681179]\n",
      " [-0.61359236]]\n",
      "t [[ 0.50325832]\n",
      " [-0.5040315 ]\n",
      " [-0.52492349]\n",
      " ...\n",
      " [ 0.29055576]\n",
      " [ 0.12681179]\n",
      " [-0.61359236]]\n",
      "t [[ 0.54718147]\n",
      " [-0.57659116]\n",
      " [-0.58104991]\n",
      " ...\n",
      " [ 0.30245543]\n",
      " [ 0.13180755]\n",
      " [-0.66474268]]\n",
      "t [[ 0.54718147]\n",
      " [-0.57659116]\n",
      " [-0.58104991]\n",
      " ...\n",
      " [ 0.30245543]\n",
      " [ 0.13180755]\n",
      " [-0.66474268]]\n",
      "Current iteration=8, loss=40450.92083581905\n",
      "t [[ 0.58672352]\n",
      " [-0.64688257]\n",
      " [-0.63405064]\n",
      " ...\n",
      " [ 0.31096804]\n",
      " [ 0.13507026]\n",
      " [-0.71159347]]\n",
      "t [[ 0.58672352]\n",
      " [-0.64688257]\n",
      " [-0.63405064]\n",
      " ...\n",
      " [ 0.31096804]\n",
      " [ 0.13507026]\n",
      " [-0.71159347]]\n",
      "t [[ 0.62243349]\n",
      " [-0.71475444]\n",
      " [-0.6842455 ]\n",
      " ...\n",
      " [ 0.31674414]\n",
      " [ 0.13689082]\n",
      " [-0.75487707]]\n",
      "loss=39011.12072471351\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.10009345]\n",
      " [-0.0565834 ]\n",
      " [-0.09424273]\n",
      " ...\n",
      " [ 0.08067041]\n",
      " [ 0.03406091]\n",
      " [-0.13874797]]\n",
      "t [[ 0.10009345]\n",
      " [-0.0565834 ]\n",
      " [-0.09424273]\n",
      " ...\n",
      " [ 0.08067041]\n",
      " [ 0.03406091]\n",
      " [-0.13874797]]\n",
      "t [[ 0.18798559]\n",
      " [-0.12232909]\n",
      " [-0.18024139]\n",
      " ...\n",
      " [ 0.14283155]\n",
      " [ 0.06118619]\n",
      " [-0.25438955]]\n",
      "t [[ 0.18798559]\n",
      " [-0.12232909]\n",
      " [-0.18024139]\n",
      " ...\n",
      " [ 0.14283155]\n",
      " [ 0.06118619]\n",
      " [-0.25438955]]\n",
      "Current iteration=2, loss=47691.06469001646\n",
      "t [[ 0.26538517]\n",
      " [-0.19337194]\n",
      " [-0.2592202 ]\n",
      " ...\n",
      " [ 0.19065501]\n",
      " [ 0.08264333]\n",
      " [-0.3521994 ]]\n",
      "t [[ 0.26538517]\n",
      " [-0.19337194]\n",
      " [-0.2592202 ]\n",
      " ...\n",
      " [ 0.19065501]\n",
      " [ 0.08264333]\n",
      " [-0.3521994 ]]\n",
      "t [[ 0.3337977 ]\n",
      " [-0.26695341]\n",
      " [-0.33219349]\n",
      " ...\n",
      " [ 0.22734635]\n",
      " [ 0.09947625]\n",
      " [-0.43622205]]\n",
      "t [[ 0.3337977 ]\n",
      " [-0.26695341]\n",
      " [-0.33219349]\n",
      " ...\n",
      " [ 0.22734635]\n",
      " [ 0.09947625]\n",
      " [-0.43622205]]\n",
      "Current iteration=4, loss=44641.04950084252\n",
      "t [[ 0.3945101 ]\n",
      " [-0.34117651]\n",
      " [-0.39998967]\n",
      " ...\n",
      " [ 0.25533868]\n",
      " [ 0.11252916]\n",
      " [-0.50950283]]\n",
      "t [[ 0.3945101 ]\n",
      " [-0.34117651]\n",
      " [-0.39998967]\n",
      " ...\n",
      " [ 0.25533868]\n",
      " [ 0.11252916]\n",
      " [-0.50950283]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.44860442]\n",
      " [-0.41477738]\n",
      " [-0.46328431]\n",
      " ...\n",
      " [ 0.27648735]\n",
      " [ 0.12248094]\n",
      " [-0.57432578]]\n",
      "t [[ 0.44860442]\n",
      " [-0.41477738]\n",
      " [-0.46328431]\n",
      " ...\n",
      " [ 0.27648735]\n",
      " [ 0.12248094]\n",
      " [-0.57432578]]\n",
      "Current iteration=6, loss=42349.20423246736\n",
      "t [[ 0.49698586]\n",
      " [-0.48694098]\n",
      " [-0.52263068]\n",
      " ...\n",
      " [ 0.29221675]\n",
      " [ 0.12987802]\n",
      " [-0.63240754]]\n",
      "t [[ 0.49698586]\n",
      " [-0.48694098]\n",
      " [-0.52263068]\n",
      " ...\n",
      " [ 0.29221675]\n",
      " [ 0.12987802]\n",
      " [-0.63240754]]\n",
      "t [[ 0.54041296]\n",
      " [-0.55716331]\n",
      " [-0.57848514]\n",
      " ...\n",
      " [ 0.3036265 ]\n",
      " [ 0.13516205]\n",
      " [-0.68504465]]\n",
      "t [[ 0.54041296]\n",
      " [-0.55716331]\n",
      " [-0.57848514]\n",
      " ...\n",
      " [ 0.3036265 ]\n",
      " [ 0.13516205]\n",
      " [-0.68504465]]\n",
      "Current iteration=8, loss=40568.24452616807\n",
      "t [[ 0.57952424]\n",
      " [-0.62515286]\n",
      " [-0.63122735]\n",
      " ...\n",
      " [ 0.31157018]\n",
      " [ 0.13869196]\n",
      " [-0.73322311]]\n",
      "t [[ 0.57952424]\n",
      " [-0.62515286]\n",
      " [-0.63122735]\n",
      " ...\n",
      " [ 0.31157018]\n",
      " [ 0.13869196]\n",
      " [-0.73322311]]\n",
      "t [[ 0.61486003]\n",
      " [-0.69076172]\n",
      " [-0.68117598]\n",
      " ...\n",
      " [ 0.31671452]\n",
      " [ 0.14076127]\n",
      " [-0.77769903]]\n",
      "loss=39148.04947553517\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.09993322]\n",
      " [-0.0594465 ]\n",
      " [-0.0939467 ]\n",
      " ...\n",
      " [ 0.14393527]\n",
      " [-0.0939467 ]\n",
      " [ 0.13481577]]\n",
      "t [[ 0.09993322]\n",
      " [-0.0594465 ]\n",
      " [-0.0939467 ]\n",
      " ...\n",
      " [ 0.14393527]\n",
      " [-0.0939467 ]\n",
      " [ 0.13481577]]\n",
      "t [[ 0.18768571]\n",
      " [-0.12755605]\n",
      " [-0.17972092]\n",
      " ...\n",
      " [ 0.26815067]\n",
      " [-0.17972092]\n",
      " [ 0.24796754]]\n",
      "t [[ 0.18768571]\n",
      " [-0.12755605]\n",
      " [-0.17972092]\n",
      " ...\n",
      " [ 0.26815067]\n",
      " [-0.17972092]\n",
      " [ 0.24796754]]\n",
      "Current iteration=2, loss=47696.25669487748\n",
      "t [[ 0.26497134]\n",
      " [-0.20058999]\n",
      " [-0.25852509]\n",
      " ...\n",
      " [ 0.37587468]\n",
      " [-0.25852509]\n",
      " [ 0.34349015]]\n",
      "t [[ 0.26497134]\n",
      " [-0.20058999]\n",
      " [-0.25852509]\n",
      " ...\n",
      " [ 0.37587468]\n",
      " [-0.25852509]\n",
      " [ 0.34349015]]\n",
      "t [[ 0.3332808 ]\n",
      " [-0.27589685]\n",
      " [-0.33135849]\n",
      " ...\n",
      " [ 0.46980127]\n",
      " [-0.33135849]\n",
      " [ 0.42465696]]\n",
      "t [[ 0.3332808 ]\n",
      " [-0.27589685]\n",
      " [-0.33135849]\n",
      " ...\n",
      " [ 0.46980127]\n",
      " [-0.33135849]\n",
      " [ 0.42465696]]\n",
      "Current iteration=4, loss=44643.01762271552\n",
      "t [[ 0.39388234]\n",
      " [-0.35166248]\n",
      " [-0.39903939]\n",
      " ...\n",
      " [ 0.55213936]\n",
      " [-0.39903939]\n",
      " [ 0.49407855]]\n",
      "t [[ 0.39388234]\n",
      " [-0.35166248]\n",
      " [-0.39903939]\n",
      " ...\n",
      " [ 0.55213936]\n",
      " [-0.39903939]\n",
      " [ 0.49407855]]\n",
      "t [[ 0.44784553]\n",
      " [-0.42668101]\n",
      " [-0.46223626]\n",
      " ...\n",
      " [ 0.62469136]\n",
      " [-0.46223626]\n",
      " [ 0.55382818]]\n",
      "t [[ 0.44784553]\n",
      " [-0.42668101]\n",
      " [-0.46223626]\n",
      " ...\n",
      " [ 0.62469136]\n",
      " [-0.46223626]\n",
      " [ 0.55382818]]\n",
      "Current iteration=6, loss=42346.03359512174\n",
      "t [[ 0.49607039]\n",
      " [-0.50017542]\n",
      " [-0.52149734]\n",
      " ...\n",
      " [ 0.68892983]\n",
      " [-0.52149734]\n",
      " [ 0.60555611]]\n",
      "t [[ 0.49607039]\n",
      " [-0.50017542]\n",
      " [-0.52149734]\n",
      " ...\n",
      " [ 0.68892983]\n",
      " [-0.52149734]\n",
      " [ 0.60555611]]\n",
      "t [[ 0.53931517]\n",
      " [-0.57166591]\n",
      " [-0.57727543]\n",
      " ...\n",
      " [ 0.74606312]\n",
      " [-0.57727543]\n",
      " [ 0.65058302]]\n",
      "t [[ 0.53931517]\n",
      " [-0.57166591]\n",
      " [-0.57727543]\n",
      " ...\n",
      " [ 0.74606312]\n",
      " [-0.57727543]\n",
      " [ 0.65058302]]\n",
      "Current iteration=8, loss=40560.00346594225\n",
      "t [[ 0.5782206 ]\n",
      " [-0.64087625]\n",
      " [-0.62994768]\n",
      " ...\n",
      " [ 0.79708848]\n",
      " [-0.62994768]\n",
      " [ 0.68997329]]\n",
      "t [[ 0.5782206 ]\n",
      " [-0.64087625]\n",
      " [-0.62994768]\n",
      " ...\n",
      " [ 0.79708848]\n",
      " [-0.62994768]\n",
      " [ 0.68997329]]\n",
      "t [[ 0.61333018]\n",
      " [-0.70766819]\n",
      " [-0.67983103]\n",
      " ...\n",
      " [ 0.84283423]\n",
      " [-0.67983103]\n",
      " [ 0.72459122]]\n",
      "loss=39135.27813654507\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03544167]\n",
      " [-0.16971236]\n",
      " [-0.09952128]\n",
      " ...\n",
      " [ 0.0851039 ]\n",
      " [ 0.03544167]\n",
      " [-0.144261  ]]\n",
      "t [[ 0.03544167]\n",
      " [-0.16971236]\n",
      " [-0.09952128]\n",
      " ...\n",
      " [ 0.0851039 ]\n",
      " [ 0.03544167]\n",
      " [-0.144261  ]]\n",
      "t [[ 0.06327451]\n",
      " [-0.32737941]\n",
      " [-0.18995558]\n",
      " ...\n",
      " [ 0.14999163]\n",
      " [ 0.06327451]\n",
      " [-0.26352336]]\n",
      "t [[ 0.06327451]\n",
      " [-0.32737941]\n",
      " [-0.18995558]\n",
      " ...\n",
      " [ 0.14999163]\n",
      " [ 0.06327451]\n",
      " [-0.26352336]]\n",
      "Current iteration=2, loss=47460.12943392856\n",
      "t [[ 0.08496208]\n",
      " [-0.47369774]\n",
      " [-0.27271418]\n",
      " ...\n",
      " [ 0.19944159]\n",
      " [ 0.08496208]\n",
      " [-0.36378543]]\n",
      "t [[ 0.08496208]\n",
      " [-0.47369774]\n",
      " [-0.27271418]\n",
      " ...\n",
      " [ 0.19944159]\n",
      " [ 0.08496208]\n",
      " [-0.36378543]]\n",
      "t [[ 0.10169356]\n",
      " [-0.60948341]\n",
      " [-0.348952  ]\n",
      " ...\n",
      " [ 0.2370575 ]\n",
      " [ 0.10169356]\n",
      " [-0.44955896]]\n",
      "t [[ 0.10169356]\n",
      " [-0.60948341]\n",
      " [-0.348952  ]\n",
      " ...\n",
      " [ 0.2370575 ]\n",
      " [ 0.10169356]\n",
      " [-0.44955896]]\n",
      "Current iteration=4, loss=44285.913762329146\n",
      "t [[ 0.11441781]\n",
      " [-0.73558907]\n",
      " [-0.41960161]\n",
      " ...\n",
      " [ 0.26553559]\n",
      " [ 0.11441781]\n",
      " [-0.5241801 ]]\n",
      "t [[ 0.11441781]\n",
      " [-0.73558907]\n",
      " [-0.41960161]\n",
      " ...\n",
      " [ 0.26553559]\n",
      " [ 0.11441781]\n",
      " [-0.5241801 ]]\n",
      "t [[ 0.12388839]\n",
      " [-0.85285151]\n",
      " [-0.48541592]\n",
      " ...\n",
      " [ 0.2869065 ]\n",
      " [ 0.12388839]\n",
      " [-0.59010918]]\n",
      "t [[ 0.12388839]\n",
      " [-0.85285151]\n",
      " [-0.48541592]\n",
      " ...\n",
      " [ 0.2869065 ]\n",
      " [ 0.12388839]\n",
      " [-0.59010918]]\n",
      "Current iteration=6, loss=41924.79316314314\n",
      "t [[ 0.13070496]\n",
      " [-0.96206309]\n",
      " [-0.54700629]\n",
      " ...\n",
      " [ 0.30270916]\n",
      " [ 0.13070496]\n",
      " [-0.64916616]]\n",
      "t [[ 0.13070496]\n",
      " [-0.96206309]\n",
      " [-0.54700629]\n",
      " ...\n",
      " [ 0.30270916]\n",
      " [ 0.13070496]\n",
      " [-0.64916616]]\n",
      "t [[ 0.13534717]\n",
      " [-1.0639578 ]\n",
      " [-0.60487337]\n",
      " ...\n",
      " [ 0.31411598]\n",
      " [ 0.13534717]\n",
      " [-0.70270552]]\n",
      "t [[ 0.13534717]\n",
      " [-1.0639578 ]\n",
      " [-0.60487337]\n",
      " ...\n",
      " [ 0.31411598]\n",
      " [ 0.13534717]\n",
      " [-0.70270552]]\n",
      "Current iteration=8, loss=40105.122824655395\n",
      "t [[ 0.13820118]\n",
      " [-1.15920586]\n",
      " [-0.65943111]\n",
      " ...\n",
      " [ 0.32202566]\n",
      " [ 0.13820118]\n",
      " [-0.75174388]]\n",
      "t [[ 0.13820118]\n",
      " [-1.15920586]\n",
      " [-0.65943111]\n",
      " ...\n",
      " [ 0.32202566]\n",
      " [ 0.13820118]\n",
      " [-0.75174388]]\n",
      "t [[ 0.13958008]\n",
      " [-1.24841355]\n",
      " [-0.71102533]\n",
      " ...\n",
      " [ 0.3271322 ]\n",
      " [ 0.13958008]\n",
      " [-0.79705209]]\n",
      "loss=38664.00051198092\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.10702595]\n",
      " [-0.06239184]\n",
      " [-0.0998524 ]\n",
      " ...\n",
      " [ 0.08385828]\n",
      " [ 0.03530252]\n",
      " [-0.14208081]]\n",
      "t [[ 0.10702595]\n",
      " [-0.06239184]\n",
      " [-0.0998524 ]\n",
      " ...\n",
      " [ 0.08385828]\n",
      " [ 0.03530252]\n",
      " [-0.14208081]]\n",
      "t [[ 0.20027827]\n",
      " [-0.13494283]\n",
      " [-0.19052109]\n",
      " ...\n",
      " [ 0.14764465]\n",
      " [ 0.06297117]\n",
      " [-0.25917701]]\n",
      "t [[ 0.20027827]\n",
      " [-0.13494283]\n",
      " [-0.19052109]\n",
      " ...\n",
      " [ 0.14764465]\n",
      " [ 0.06297117]\n",
      " [-0.25917701]]\n",
      "Current iteration=2, loss=47450.94328285798\n",
      "t [[ 0.28179502]\n",
      " [-0.21312736]\n",
      " [-0.27343945]\n",
      " ...\n",
      " [ 0.1961243 ]\n",
      " [ 0.08447877]\n",
      " [-0.3573299 ]]\n",
      "t [[ 0.28179502]\n",
      " [-0.21312736]\n",
      " [-0.27343945]\n",
      " ...\n",
      " [ 0.1961243 ]\n",
      " [ 0.08447877]\n",
      " [-0.3573299 ]]\n",
      "t [[ 0.3533534 ]\n",
      " [-0.29378524]\n",
      " [-0.34978007]\n",
      " ...\n",
      " [ 0.23288724]\n",
      " [ 0.10102133]\n",
      " [-0.44107811]]\n",
      "t [[ 0.3533534 ]\n",
      " [-0.29378524]\n",
      " [-0.34978007]\n",
      " ...\n",
      " [ 0.23288724]\n",
      " [ 0.10102133]\n",
      " [-0.44107811]]\n",
      "Current iteration=4, loss=44273.84098681237\n",
      "t [[ 0.41645211]\n",
      " [-0.37479983]\n",
      " [-0.42048921]\n",
      " ...\n",
      " [ 0.26061772]\n",
      " [ 0.11355248]\n",
      " [-0.51377294]]\n",
      "t [[ 0.41645211]\n",
      " [-0.37479983]\n",
      " [-0.42048921]\n",
      " ...\n",
      " [ 0.26061772]\n",
      " [ 0.11355248]\n",
      " [-0.51377294]]\n",
      "t [[ 0.4723357 ]\n",
      " [-0.45480282]\n",
      " [-0.48633021]\n",
      " ...\n",
      " [ 0.28133527]\n",
      " [ 0.1228291 ]\n",
      " [-0.57788158]]\n",
      "t [[ 0.4723357 ]\n",
      " [-0.45480282]\n",
      " [-0.48633021]\n",
      " ...\n",
      " [ 0.28133527]\n",
      " [ 0.1228291 ]\n",
      " [-0.57788158]]\n",
      "Current iteration=6, loss=41911.93908798279\n",
      "t [[ 0.52203499]\n",
      " [-0.53294223]\n",
      " [-0.54792234]\n",
      " ...\n",
      " [ 0.29656819]\n",
      " [ 0.12945314]\n",
      " [-0.63522525]]\n",
      "t [[ 0.52203499]\n",
      " [-0.53294223]\n",
      " [-0.54792234]\n",
      " ...\n",
      " [ 0.29656819]\n",
      " [ 0.12945314]\n",
      " [-0.63522525]]\n",
      "t [[ 0.5664068 ]\n",
      " [-0.60871459]\n",
      " [-0.60577225]\n",
      " ...\n",
      " [ 0.30747901]\n",
      " [ 0.13390583]\n",
      " [-0.68715622]]\n",
      "t [[ 0.5664068 ]\n",
      " [-0.60871459]\n",
      " [-0.60577225]\n",
      " ...\n",
      " [ 0.30747901]\n",
      " [ 0.13390583]\n",
      " [-0.68715622]]\n",
      "Current iteration=8, loss=40092.20128928097\n",
      "t [[ 0.60616716]\n",
      " [-0.68184853]\n",
      " [-0.66029849]\n",
      " ...\n",
      " [ 0.31495745]\n",
      " [ 0.13657444]\n",
      " [-0.73468681]]\n",
      "t [[ 0.60616716]\n",
      " [-0.68184853]\n",
      " [-0.66029849]\n",
      " ...\n",
      " [ 0.31495745]\n",
      " [ 0.13657444]\n",
      " [-0.73468681]]\n",
      "t [[ 0.64191783]\n",
      " [-0.75222545]\n",
      " [-0.71185038]\n",
      " ...\n",
      " [ 0.3196896 ]\n",
      " [ 0.13777278]\n",
      " [-0.77858242]]\n",
      "loss=38651.228135870784\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.1056542 ]\n",
      " [-0.05972692]\n",
      " [-0.09947843]\n",
      " ...\n",
      " [ 0.0851521 ]\n",
      " [ 0.03595318]\n",
      " [-0.1464562 ]]\n",
      "t [[ 0.1056542 ]\n",
      " [-0.05972692]\n",
      " [-0.09947843]\n",
      " ...\n",
      " [ 0.0851521 ]\n",
      " [ 0.03595318]\n",
      " [-0.1464562 ]]\n",
      "t [[ 0.19771752]\n",
      " [-0.1296591 ]\n",
      " [-0.18977359]\n",
      " ...\n",
      " [ 0.14968677]\n",
      " [ 0.06418085]\n",
      " [-0.26717464]]\n",
      "t [[ 0.19771752]\n",
      " [-0.1296591 ]\n",
      " [-0.18977359]\n",
      " ...\n",
      " [ 0.14968677]\n",
      " [ 0.06418085]\n",
      " [-0.26717464]]\n",
      "Current iteration=2, loss=47486.687562239946\n",
      "t [[ 0.27820516]\n",
      " [-0.20525083]\n",
      " [-0.27232801]\n",
      " ...\n",
      " [ 0.19851316]\n",
      " [ 0.08617604]\n",
      " [-0.36836435]]\n",
      "t [[ 0.27820516]\n",
      " [-0.20525083]\n",
      " [-0.27232801]\n",
      " ...\n",
      " [ 0.19851316]\n",
      " [ 0.08617604]\n",
      " [-0.36836435]]\n",
      "t [[ 0.34887248]\n",
      " [-0.28333714]\n",
      " [-0.34831968]\n",
      " ...\n",
      " [ 0.23533223]\n",
      " [ 0.10315022]\n",
      " [-0.45469386]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.34887248]\n",
      " [-0.28333714]\n",
      " [-0.34831968]\n",
      " ...\n",
      " [ 0.23533223]\n",
      " [ 0.10315022]\n",
      " [-0.45469386]]\n",
      "Current iteration=4, loss=44343.37368461593\n",
      "t [[ 0.4112004 ]\n",
      " [-0.36180277]\n",
      " [-0.41869713]\n",
      " ...\n",
      " [ 0.26290293]\n",
      " [ 0.11606835]\n",
      " [-0.52960953]]\n",
      "t [[ 0.4112004 ]\n",
      " [-0.36180277]\n",
      " [-0.41869713]\n",
      " ...\n",
      " [ 0.26290293]\n",
      " [ 0.11606835]\n",
      " [-0.52960953]]\n",
      "t [[ 0.4664185 ]\n",
      " [-0.43928328]\n",
      " [-0.48422406]\n",
      " ...\n",
      " [ 0.28329506]\n",
      " [ 0.12569555]\n",
      " [-0.59564841]]\n",
      "t [[ 0.4664185 ]\n",
      " [-0.43928328]\n",
      " [-0.48422406]\n",
      " ...\n",
      " [ 0.28329506]\n",
      " [ 0.12569555]\n",
      " [-0.59564841]]\n",
      "Current iteration=6, loss=42010.1874897944\n",
      "t [[ 0.51554416]\n",
      " [-0.51493169]\n",
      " [-0.54551906]\n",
      " ...\n",
      " [ 0.2980733 ]\n",
      " [ 0.13263988]\n",
      " [-0.65468446]]\n",
      "t [[ 0.51554416]\n",
      " [-0.51493169]\n",
      " [-0.54551906]\n",
      " ...\n",
      " [ 0.2980733 ]\n",
      " [ 0.13263988]\n",
      " [-0.65468446]]\n",
      "t [[ 0.55942192]\n",
      " [-0.58824963]\n",
      " [-0.60308766]\n",
      " ...\n",
      " [ 0.30842795]\n",
      " [ 0.13738717]\n",
      " [-0.7081107 ]]\n",
      "t [[ 0.55942192]\n",
      " [-0.58824963]\n",
      " [-0.60308766]\n",
      " ...\n",
      " [ 0.30842795]\n",
      " [ 0.13738717]\n",
      " [-0.7081107 ]]\n",
      "Current iteration=8, loss=40214.29994524465\n",
      "t [[ 0.59875675]\n",
      " [-0.65897025]\n",
      " [-0.65734707]\n",
      " ...\n",
      " [ 0.31527053]\n",
      " [ 0.14032823]\n",
      " [-0.75697143]]\n",
      "t [[ 0.59875675]\n",
      " [-0.65897025]\n",
      " [-0.65734707]\n",
      " ...\n",
      " [ 0.31527053]\n",
      " [ 0.14032823]\n",
      " [-0.75697143]]\n",
      "t [[ 0.63414075]\n",
      " [-0.72697865]\n",
      " [-0.70864523]\n",
      " ...\n",
      " [ 0.31930437]\n",
      " [ 0.14177968]\n",
      " [-0.80205748]]\n",
      "loss=38793.16117933752\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.10548507]\n",
      " [-0.06274909]\n",
      " [-0.09916597]\n",
      " ...\n",
      " [ 0.15193167]\n",
      " [-0.09916597]\n",
      " [ 0.14230554]]\n",
      "t [[ 0.10548507]\n",
      " [-0.06274909]\n",
      " [-0.09916597]\n",
      " ...\n",
      " [ 0.15193167]\n",
      " [-0.09916597]\n",
      " [ 0.14230554]]\n",
      "t [[ 0.19740193]\n",
      " [-0.1351476 ]\n",
      " [-0.18922834]\n",
      " ...\n",
      " [ 0.28189726]\n",
      " [-0.18922834]\n",
      " [ 0.26047963]]\n",
      "t [[ 0.19740193]\n",
      " [-0.1351476 ]\n",
      " [-0.18922834]\n",
      " ...\n",
      " [ 0.28189726]\n",
      " [-0.18922834]\n",
      " [ 0.26047963]]\n",
      "Current iteration=2, loss=47491.88810591874\n",
      "t [[ 0.27777096]\n",
      " [-0.21279941]\n",
      " [-0.27160407]\n",
      " ...\n",
      " [ 0.39370065]\n",
      " [-0.27160407]\n",
      " [ 0.35927461]]\n",
      "t [[ 0.27777096]\n",
      " [-0.21279941]\n",
      " [-0.27160407]\n",
      " ...\n",
      " [ 0.39370065]\n",
      " [-0.27160407]\n",
      " [ 0.35927461]]\n",
      "t [[ 0.34832898]\n",
      " [-0.29266383]\n",
      " [-0.34745396]\n",
      " ...\n",
      " [ 0.49047153]\n",
      " [-0.34745396]\n",
      " [ 0.44248528]]\n",
      "t [[ 0.34832898]\n",
      " [-0.29266383]\n",
      " [-0.34745396]\n",
      " ...\n",
      " [ 0.49047153]\n",
      " [-0.34745396]\n",
      " [ 0.44248528]]\n",
      "Current iteration=4, loss=44344.83948802266\n",
      "t [[ 0.41053568]\n",
      " [-0.37271882]\n",
      " [-0.41771519]\n",
      " ...\n",
      " [ 0.57473992]\n",
      " [-0.41771519]\n",
      " [ 0.51309021]]\n",
      "t [[ 0.41053568]\n",
      " [-0.37271882]\n",
      " [-0.41771519]\n",
      " ...\n",
      " [ 0.57473992]\n",
      " [-0.41771519]\n",
      " [ 0.51309021]]\n",
      "t [[ 0.46560788]\n",
      " [-0.4516627 ]\n",
      " [-0.48314373]\n",
      " ...\n",
      " [ 0.64854329]\n",
      " [-0.48314373]\n",
      " [ 0.57342024]]\n",
      "t [[ 0.46560788]\n",
      " [-0.4516627 ]\n",
      " [-0.48314373]\n",
      " ...\n",
      " [ 0.64854329]\n",
      " [-0.48314373]\n",
      " [ 0.57342024]]\n",
      "Current iteration=6, loss=42006.15196610802\n",
      "t [[ 0.51455872]\n",
      " [-0.52868812]\n",
      " [-0.54435278]\n",
      " ...\n",
      " [ 0.71352684]\n",
      " [-0.54435278]\n",
      " [ 0.62530625]]\n",
      "t [[ 0.51455872]\n",
      " [-0.52868812]\n",
      " [-0.54435278]\n",
      " ...\n",
      " [ 0.71352684]\n",
      " [-0.54435278]\n",
      " [ 0.62530625]]\n",
      "t [[ 0.55823364]\n",
      " [-0.60332112]\n",
      " [-0.60184412]\n",
      " ...\n",
      " [ 0.77102652]\n",
      " [-0.60184412]\n",
      " [ 0.67019638]]\n",
      "t [[ 0.55823364]\n",
      " [-0.60332112]\n",
      " [-0.60184412]\n",
      " ...\n",
      " [ 0.77102652]\n",
      " [-0.60184412]\n",
      " [ 0.67019638]]\n",
      "Current iteration=8, loss=40204.99299305852\n",
      "t [[ 0.59734083]\n",
      " [-0.6753098 ]\n",
      " [-0.65603243]\n",
      " ...\n",
      " [ 0.8221347 ]\n",
      " [-0.65603243]\n",
      " [ 0.70924562]]\n",
      "t [[ 0.59734083]\n",
      " [-0.6753098 ]\n",
      " [-0.65603243]\n",
      " ...\n",
      " [ 0.8221347 ]\n",
      " [-0.65603243]\n",
      " [ 0.70924562]]\n",
      "t [[ 0.63247629]\n",
      " [-0.74454837]\n",
      " [-0.70726391]\n",
      " ...\n",
      " [ 0.86775129]\n",
      " [-0.70726391]\n",
      " [ 0.74338329]]\n",
      "loss=38779.220100387494\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03730702]\n",
      " [-0.17864459]\n",
      " [-0.10475924]\n",
      " ...\n",
      " [ 0.08958306]\n",
      " [ 0.03730702]\n",
      " [-0.15185368]]\n",
      "t [[ 0.03730702]\n",
      " [-0.17864459]\n",
      " [-0.10475924]\n",
      " ...\n",
      " [ 0.08958306]\n",
      " [ 0.03730702]\n",
      " [-0.15185368]]\n",
      "t [[ 0.06618562]\n",
      " [-0.34394502]\n",
      " [-0.19945232]\n",
      " ...\n",
      " [ 0.15677187]\n",
      " [ 0.06618562]\n",
      " [-0.27601602]]\n",
      "t [[ 0.06618562]\n",
      " [-0.34394502]\n",
      " [-0.19945232]\n",
      " ...\n",
      " [ 0.15677187]\n",
      " [ 0.06618562]\n",
      " [-0.27601602]]\n",
      "Current iteration=2, loss=47257.13813284489\n",
      "t [[ 0.08834528]\n",
      " [-0.49671885]\n",
      " [-0.28572823]\n",
      " ...\n",
      " [ 0.20714216]\n",
      " [ 0.08834528]\n",
      " [-0.37948834]]\n",
      "t [[ 0.08834528]\n",
      " [-0.49671885]\n",
      " [-0.28572823]\n",
      " ...\n",
      " [ 0.20714216]\n",
      " [ 0.08834528]\n",
      " [-0.37948834]]\n",
      "t [[ 0.10515416]\n",
      " [-0.6379261 ]\n",
      " [-0.36491678]\n",
      " ...\n",
      " [ 0.24481921]\n",
      " [ 0.10515416]\n",
      " [-0.46743307]]\n",
      "t [[ 0.10515416]\n",
      " [-0.6379261 ]\n",
      " [-0.36491678]\n",
      " ...\n",
      " [ 0.24481921]\n",
      " [ 0.10515416]\n",
      " [-0.46743307]]\n",
      "Current iteration=4, loss=43993.57539956295\n",
      "t [[ 0.11768744]\n",
      " [-0.76856381]\n",
      " [-0.43807672]\n",
      " ...\n",
      " [ 0.27283521]\n",
      " [ 0.11768744]\n",
      " [-0.54359241]]\n",
      "t [[ 0.11768744]\n",
      " [-0.76856381]\n",
      " [-0.43807672]\n",
      " ...\n",
      " [ 0.27283521]\n",
      " [ 0.11768744]\n",
      " [-0.54359241]]\n",
      "t [[ 0.12678718]\n",
      " [-0.88960234]\n",
      " [-0.50605222]\n",
      " ...\n",
      " [ 0.29343675]\n",
      " [ 0.12678718]\n",
      " [-0.61067506]]\n",
      "t [[ 0.12678718]\n",
      " [-0.88960234]\n",
      " [-0.50605222]\n",
      " ...\n",
      " [ 0.29343675]\n",
      " [ 0.12678718]\n",
      " [-0.61067506]]\n",
      "Current iteration=6, loss=41594.84829428998\n",
      "t [[ 0.13311508]\n",
      " [-1.00195235]\n",
      " [-0.56952148]\n",
      " ...\n",
      " [ 0.30829993]\n",
      " [ 0.13311508]\n",
      " [-0.67065031]]\n",
      "t [[ 0.13311508]\n",
      " [-1.00195235]\n",
      " [-0.56952148]\n",
      " ...\n",
      " [ 0.30829993]\n",
      " [ 0.13311508]\n",
      " [-0.67065031]]\n",
      "t [[ 0.13719455]\n",
      " [-1.10644981]\n",
      " [-0.62903492]\n",
      " ...\n",
      " [ 0.31868308]\n",
      " [ 0.13719455]\n",
      " [-0.72496105]]\n",
      "t [[ 0.13719455]\n",
      " [-1.10644981]\n",
      " [-0.62903492]\n",
      " ...\n",
      " [ 0.31868308]\n",
      " [ 0.13719455]\n",
      " [-0.72496105]]\n",
      "Current iteration=8, loss=39763.20521759242\n",
      "t [[ 0.13944278]\n",
      " [-1.20385135]\n",
      " [-0.68504432]\n",
      " ...\n",
      " [ 0.32553801]\n",
      " [ 0.13944278]\n",
      " [-0.77467541]]\n",
      "t [[ 0.13944278]\n",
      " [-1.20385135]\n",
      " [-0.68504432]\n",
      " ...\n",
      " [ 0.32553801]\n",
      " [ 0.13944278]\n",
      " [-0.77467541]]\n",
      "t [[ 0.14019493]\n",
      " [-1.29483567]\n",
      " [-0.7379248 ]\n",
      " ...\n",
      " [ 0.3295912 ]\n",
      " [ 0.14019493]\n",
      " [-0.82059369]]\n",
      "loss=38323.29628712096\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11265889]\n",
      " [-0.06567562]\n",
      " [-0.10510778]\n",
      " ...\n",
      " [ 0.08827187]\n",
      " [ 0.03716055]\n",
      " [-0.14955875]]\n",
      "t [[ 0.11265889]\n",
      " [-0.06567562]\n",
      " [-0.10510778]\n",
      " ...\n",
      " [ 0.08827187]\n",
      " [ 0.03716055]\n",
      " [-0.14955875]]\n",
      "t [[ 0.21006067]\n",
      " [-0.14260402]\n",
      " [-0.20004229]\n",
      " ...\n",
      " [ 0.15430926]\n",
      " [ 0.06586495]\n",
      " [-0.27144173]]\n",
      "t [[ 0.21006067]\n",
      " [-0.14260402]\n",
      " [-0.20004229]\n",
      " ...\n",
      " [ 0.15430926]\n",
      " [ 0.06586495]\n",
      " [-0.27144173]]\n",
      "Current iteration=2, loss=47247.65877129226\n",
      "t [[ 0.29458866]\n",
      " [-0.22550726]\n",
      " [-0.28647828]\n",
      " ...\n",
      " [ 0.20367243]\n",
      " [ 0.08783348]\n",
      " [-0.37270032]]\n",
      "t [[ 0.29458866]\n",
      " [-0.22550726]\n",
      " [-0.28647828]\n",
      " ...\n",
      " [ 0.20367243]\n",
      " [ 0.08783348]\n",
      " [-0.37270032]]\n",
      "t [[ 0.36829661]\n",
      " [-0.31079178]\n",
      " [-0.36576589]\n",
      " ...\n",
      " [ 0.24047079]\n",
      " [ 0.10444208]\n",
      " [-0.4585272 ]]\n",
      "t [[ 0.36829661]\n",
      " [-0.31079178]\n",
      " [-0.36576589]\n",
      " ...\n",
      " [ 0.24047079]\n",
      " [ 0.10444208]\n",
      " [-0.4585272 ]]\n",
      "Current iteration=4, loss=43981.331442357885\n",
      "t [[ 0.43289356]\n",
      " [-0.39612024]\n",
      " [-0.43897929]\n",
      " ...\n",
      " [ 0.26772274]\n",
      " [ 0.11677125]\n",
      " [-0.53268077]]\n",
      "t [[ 0.43289356]\n",
      " [-0.39612024]\n",
      " [-0.43897929]\n",
      " ...\n",
      " [ 0.26772274]\n",
      " [ 0.11677125]\n",
      " [-0.53268077]]\n",
      "t [[ 0.48978143]\n",
      " [-0.48003131]\n",
      " [-0.5069742 ]\n",
      " ...\n",
      " [ 0.28766201]\n",
      " [ 0.12566662]\n",
      " [-0.59787622]]\n",
      "t [[ 0.48978143]\n",
      " [-0.48003131]\n",
      " [-0.5069742 ]\n",
      " ...\n",
      " [ 0.28766201]\n",
      " [ 0.12566662]\n",
      " [-0.59787622]]\n",
      "Current iteration=6, loss=41581.942956384984\n",
      "t [[ 0.54010931]\n",
      " [-0.56165116]\n",
      " [-0.57043744]\n",
      " ...\n",
      " [ 0.30195244]\n",
      " [ 0.13179237]\n",
      " [-0.65608307]]\n",
      "t [[ 0.54010931]\n",
      " [-0.56165116]\n",
      " [-0.57043744]\n",
      " ...\n",
      " [ 0.30195244]\n",
      " [ 0.13179237]\n",
      " [-0.65608307]]\n",
      "t [[ 0.58482365]\n",
      " [-0.64049151]\n",
      " [-0.62992587]\n",
      " ...\n",
      " [ 0.31184108]\n",
      " [ 0.13567355]\n",
      " [-0.70874064]]\n",
      "t [[ 0.58482365]\n",
      " [-0.64049151]\n",
      " [-0.62992587]\n",
      " ...\n",
      " [ 0.31184108]\n",
      " [ 0.13567355]\n",
      " [-0.70874064]]\n",
      "Current iteration=8, loss=39750.303456348134\n",
      "t [[ 0.62470895]\n",
      " [-0.71631359]\n",
      " [-0.68589613]\n",
      " ...\n",
      " [ 0.3182697 ]\n",
      " [ 0.13772845]\n",
      " [-0.7569114 ]]\n",
      "t [[ 0.62470895]\n",
      " [-0.71631359]\n",
      " [-0.68589613]\n",
      " ...\n",
      " [ 0.3182697 ]\n",
      " [ 0.13772845]\n",
      " [-0.7569114 ]]\n",
      "t [[ 0.66041971]\n",
      " [-0.78903787]\n",
      " [-0.73872698]\n",
      " ...\n",
      " [ 0.32195599]\n",
      " [ 0.13829296]\n",
      " [-0.80138896]]\n",
      "loss=38310.57407133321\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.11121494]\n",
      " [-0.06287044]\n",
      " [-0.10471414]\n",
      " ...\n",
      " [ 0.08963379]\n",
      " [ 0.03784545]\n",
      " [-0.15416442]]\n",
      "t [[ 0.11121494]\n",
      " [-0.06287044]\n",
      " [-0.10471414]\n",
      " ...\n",
      " [ 0.08963379]\n",
      " [ 0.03784545]\n",
      " [-0.15416442]]\n",
      "t [[ 0.20737517]\n",
      " [-0.13704476]\n",
      " [-0.19925551]\n",
      " ...\n",
      " [ 0.15642926]\n",
      " [ 0.06713329]\n",
      " [-0.27981912]]\n",
      "t [[ 0.20737517]\n",
      " [-0.13704476]\n",
      " [-0.19925551]\n",
      " ...\n",
      " [ 0.15642926]\n",
      " [ 0.06713329]\n",
      " [-0.27981912]]\n",
      "Current iteration=2, loss=47285.30192041946\n",
      "t [[ 0.29083719]\n",
      " [-0.21722166]\n",
      " [-0.2853096 ]\n",
      " ...\n",
      " [ 0.20611495]\n",
      " [ 0.0896074 ]\n",
      " [-0.38421146]]\n",
      "t [[ 0.29083719]\n",
      " [-0.21722166]\n",
      " [-0.2853096 ]\n",
      " ...\n",
      " [ 0.20611495]\n",
      " [ 0.0896074 ]\n",
      " [-0.38421146]]\n",
      "t [[ 0.3636296 ]\n",
      " [-0.29980279]\n",
      " [-0.36423247]\n",
      " ...\n",
      " [ 0.24292628]\n",
      " [ 0.10666136]\n",
      " [-0.4726825 ]]\n",
      "t [[ 0.3636296 ]\n",
      " [-0.29980279]\n",
      " [-0.36423247]\n",
      " ...\n",
      " [ 0.24292628]\n",
      " [ 0.10666136]\n",
      " [-0.4726825 ]]\n",
      "Current iteration=4, loss=44054.207434661526\n",
      "t [[ 0.42744125]\n",
      " [-0.382453  ]\n",
      " [-0.43710055]\n",
      " ...\n",
      " [ 0.26996393]\n",
      " [ 0.11938832]\n",
      " [-0.54909693]]\n",
      "t [[ 0.42744125]\n",
      " [-0.382453  ]\n",
      " [-0.43710055]\n",
      " ...\n",
      " [ 0.26996393]\n",
      " [ 0.11938832]\n",
      " [-0.54909693]]\n",
      "t [[ 0.48365697]\n",
      " [-0.46371598]\n",
      " [-0.50476964]\n",
      " ...\n",
      " [ 0.28951631]\n",
      " [ 0.128643  ]\n",
      " [-0.61624689]]\n",
      "t [[ 0.48365697]\n",
      " [-0.46371598]\n",
      " [-0.50476964]\n",
      " ...\n",
      " [ 0.28951631]\n",
      " [ 0.128643  ]\n",
      " [-0.61624689]]\n",
      "Current iteration=6, loss=41684.36757130396\n",
      "t [[ 0.53341047]\n",
      " [-0.54272401]\n",
      " [-0.56792559]\n",
      " ...\n",
      " [ 0.3032867 ]\n",
      " [ 0.13509615]\n",
      " [-0.6761594 ]]\n",
      "t [[ 0.53341047]\n",
      " [-0.54272401]\n",
      " [-0.56792559]\n",
      " ...\n",
      " [ 0.3032867 ]\n",
      " [ 0.13509615]\n",
      " [-0.6761594 ]]\n",
      "t [[ 0.57763428]\n",
      " [-0.61899473]\n",
      " [-0.6271238 ]\n",
      " ...\n",
      " [ 0.31255217]\n",
      " [ 0.13927776]\n",
      " [-0.7303178 ]]\n",
      "t [[ 0.57763428]\n",
      " [-0.61899473]\n",
      " [-0.6271238 ]\n",
      " ...\n",
      " [ 0.31255217]\n",
      " [ 0.13927776]\n",
      " [-0.7303178 ]]\n",
      "Current iteration=8, loss=39877.004416806376\n",
      "t [[ 0.61710056]\n",
      " [-0.6922944 ]\n",
      " [-0.68281931]\n",
      " ...\n",
      " [ 0.31827779]\n",
      " [ 0.14160992]\n",
      " [-0.77981878]]\n",
      "t [[ 0.61710056]\n",
      " [-0.6922944 ]\n",
      " [-0.68281931]\n",
      " ...\n",
      " [ 0.31827779]\n",
      " [ 0.14160992]\n",
      " [-0.77981878]]\n",
      "t [[ 0.65245309]\n",
      " [-0.76254742]\n",
      " [-0.7353893 ]\n",
      " ...\n",
      " [ 0.32119948]\n",
      " [ 0.14243149]\n",
      " [-0.82548291]]\n",
      "loss=38457.292910974786\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11103692]\n",
      " [-0.06605167]\n",
      " [-0.10438523]\n",
      " ...\n",
      " [ 0.15992808]\n",
      " [-0.10438523]\n",
      " [ 0.1497953 ]]\n",
      "t [[ 0.11103692]\n",
      " [-0.06605167]\n",
      " [-0.10438523]\n",
      " ...\n",
      " [ 0.15992808]\n",
      " [-0.10438523]\n",
      " [ 0.1497953 ]]\n",
      "t [[ 0.20704392]\n",
      " [-0.14279183]\n",
      " [-0.19868592]\n",
      " ...\n",
      " [ 0.29552371]\n",
      " [-0.19868592]\n",
      " [ 0.27285979]]\n",
      "t [[ 0.20704392]\n",
      " [-0.14279183]\n",
      " [-0.19868592]\n",
      " ...\n",
      " [ 0.29552371]\n",
      " [-0.19868592]\n",
      " [ 0.27285979]]\n",
      "Current iteration=2, loss=47290.48873548857\n",
      "t [[ 0.29038262]\n",
      " [-0.22509458]\n",
      " [-0.2845577 ]\n",
      " ...\n",
      " [ 0.41123096]\n",
      " [-0.2845577 ]\n",
      " [ 0.37474371]]\n",
      "t [[ 0.29038262]\n",
      " [-0.22509458]\n",
      " [-0.2845577 ]\n",
      " ...\n",
      " [ 0.41123096]\n",
      " [-0.2845577 ]\n",
      " [ 0.37474371]]\n",
      "t [[ 0.36305893]\n",
      " [-0.30950441]\n",
      " [-0.36333721]\n",
      " ...\n",
      " [ 0.51065387]\n",
      " [-0.36333721]\n",
      " [ 0.45980776]]\n",
      "t [[ 0.36305893]\n",
      " [-0.30950441]\n",
      " [-0.36333721]\n",
      " ...\n",
      " [ 0.51065387]\n",
      " [-0.36333721]\n",
      " [ 0.45980776]]\n",
      "Current iteration=4, loss=44055.14918896515\n",
      "t [[ 0.42673797]\n",
      " [-0.39378986]\n",
      " [-0.43608832]\n",
      " ...\n",
      " [ 0.59666452]\n",
      " [-0.43608832]\n",
      " [ 0.53141998]]\n",
      "t [[ 0.42673797]\n",
      " [-0.39378986]\n",
      " [-0.43608832]\n",
      " ...\n",
      " [ 0.59666452]\n",
      " [-0.43608832]\n",
      " [ 0.53141998]]\n",
      "t [[ 0.48279191]\n",
      " [-0.47656172]\n",
      " [-0.50365846]\n",
      " ...\n",
      " [ 0.67154589]\n",
      " [-0.50365846]\n",
      " [ 0.59217758]]\n",
      "t [[ 0.48279191]\n",
      " [-0.47656172]\n",
      " [-0.50365846]\n",
      " ...\n",
      " [ 0.67154589]\n",
      " [-0.50365846]\n",
      " [ 0.59217758]]\n",
      "Current iteration=6, loss=41679.469186998205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.53235144]\n",
      " [-0.55699312]\n",
      " [-0.56672773]\n",
      " ...\n",
      " [ 0.73712024]\n",
      " [-0.56672773]\n",
      " [ 0.64409427]]\n",
      "t [[ 0.53235144]\n",
      " [-0.55699312]\n",
      " [-0.56672773]\n",
      " ...\n",
      " [ 0.73712024]\n",
      " [-0.56672773]\n",
      " [ 0.64409427]]\n",
      "t [[ 0.57635127]\n",
      " [-0.63462603]\n",
      " [-0.62584771]\n",
      " ...\n",
      " [ 0.79485237]\n",
      " [-0.62584771]\n",
      " [ 0.68874467]]\n",
      "t [[ 0.57635127]\n",
      " [-0.63462603]\n",
      " [-0.62584771]\n",
      " ...\n",
      " [ 0.79485237]\n",
      " [-0.62584771]\n",
      " [ 0.68874467]]\n",
      "Current iteration=8, loss=39866.65606818814\n",
      "t [[ 0.61556778]\n",
      " [-0.70924111]\n",
      " [-0.68147087]\n",
      " ...\n",
      " [ 0.84592934]\n",
      " [-0.68147087]\n",
      " [ 0.72737214]]\n",
      "t [[ 0.61556778]\n",
      " [-0.70924111]\n",
      " [-0.68147087]\n",
      " ...\n",
      " [ 0.84592934]\n",
      " [-0.68147087]\n",
      " [ 0.72737214]]\n",
      "t [[ 0.65064937]\n",
      " [-0.78077127]\n",
      " [-0.73397269]\n",
      " ...\n",
      " [ 0.89132134]\n",
      " [-0.73397269]\n",
      " [ 0.76096829]]\n",
      "loss=38442.223191319456\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.03917237]\n",
      " [-0.18757682]\n",
      " [-0.1099972 ]\n",
      " ...\n",
      " [ 0.09406221]\n",
      " [ 0.03917237]\n",
      " [-0.15944636]]\n",
      "t [[ 0.03917237]\n",
      " [-0.18757682]\n",
      " [-0.1099972 ]\n",
      " ...\n",
      " [ 0.09406221]\n",
      " [ 0.03917237]\n",
      " [-0.15944636]]\n",
      "t [[ 0.0690552 ]\n",
      " [-0.36044452]\n",
      " [-0.20889938]\n",
      " ...\n",
      " [ 0.16344165]\n",
      " [ 0.0690552 ]\n",
      " [-0.28837226]]\n",
      "t [[ 0.0690552 ]\n",
      " [-0.36044452]\n",
      " [-0.20889938]\n",
      " ...\n",
      " [ 0.16344165]\n",
      " [ 0.0690552 ]\n",
      " [-0.28837226]]\n",
      "Current iteration=2, loss=47057.07674279752\n",
      "t [[ 0.09163003]\n",
      " [-0.51955532]\n",
      " [-0.29861858]\n",
      " ...\n",
      " [ 0.21459505]\n",
      " [ 0.09163003]\n",
      " [-0.39488761]]\n",
      "t [[ 0.09163003]\n",
      " [-0.51955532]\n",
      " [-0.29861858]\n",
      " ...\n",
      " [ 0.21459505]\n",
      " [ 0.09163003]\n",
      " [-0.39488761]]\n",
      "t [[ 0.10845839]\n",
      " [-0.66602942]\n",
      " [-0.38067386]\n",
      " ...\n",
      " [ 0.25220727]\n",
      " [ 0.10845839]\n",
      " [-0.48485182]]\n",
      "t [[ 0.10845839]\n",
      " [-0.66602942]\n",
      " [-0.38067386]\n",
      " ...\n",
      " [ 0.25220727]\n",
      " [ 0.10845839]\n",
      " [-0.48485182]]\n",
      "Current iteration=4, loss=43709.44683233382\n",
      "t [[ 0.12074879]\n",
      " [-0.80102288]\n",
      " [-0.45625756]\n",
      " ...\n",
      " [ 0.27966082]\n",
      " [ 0.12074879]\n",
      " [-0.56242782]]\n",
      "t [[ 0.12074879]\n",
      " [-0.80102288]\n",
      " [-0.45625756]\n",
      " ...\n",
      " [ 0.27966082]\n",
      " [ 0.12074879]\n",
      " [-0.56242782]]\n",
      "t [[ 0.12943464]\n",
      " [-0.92565124]\n",
      " [-0.52630875]\n",
      " ...\n",
      " [ 0.29942083]\n",
      " [ 0.12943464]\n",
      " [-0.63057271]]\n",
      "t [[ 0.12943464]\n",
      " [-0.92565124]\n",
      " [-0.52630875]\n",
      " ...\n",
      " [ 0.29942083]\n",
      " [ 0.12943464]\n",
      " [-0.63057271]]\n",
      "Current iteration=6, loss=41277.49215462619\n",
      "t [[ 0.13524045]\n",
      " [-1.04095195]\n",
      " [-0.59157406]\n",
      " ...\n",
      " [ 0.31329779]\n",
      " [ 0.13524045]\n",
      " [-0.69139972]]\n",
      "t [[ 0.13524045]\n",
      " [-1.04095195]\n",
      " [-0.59157406]\n",
      " ...\n",
      " [ 0.31329779]\n",
      " [ 0.13524045]\n",
      " [-0.69139972]]\n",
      "t [[ 0.13873308]\n",
      " [-1.1478688 ]\n",
      " [-0.65265431]\n",
      " ...\n",
      " [ 0.32263137]\n",
      " [ 0.13873308]\n",
      " [-0.74643359]]\n",
      "t [[ 0.13873308]\n",
      " [-1.1478688 ]\n",
      " [-0.65265431]\n",
      " ...\n",
      " [ 0.32263137]\n",
      " [ 0.13873308]\n",
      " [-0.74643359]]\n",
      "Current iteration=8, loss=39437.01845723309\n",
      "t [[ 0.14035994]\n",
      " [-1.24724852]\n",
      " [-0.71003924]\n",
      " ...\n",
      " [ 0.32842199]\n",
      " [ 0.14035994]\n",
      " [-0.79678806]]\n",
      "t [[ 0.14035994]\n",
      " [-1.24724852]\n",
      " [-0.71003924]\n",
      " ...\n",
      " [ 0.32842199]\n",
      " [ 0.14035994]\n",
      " [-0.79678806]]\n",
      "t [[ 0.14047737]\n",
      " [-1.33984443]\n",
      " [-0.76413319]\n",
      " ...\n",
      " [ 0.33142465]\n",
      " [ 0.14047737]\n",
      " [-0.84328833]]\n",
      "loss=38000.416811932155\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11829184]\n",
      " [-0.0689594 ]\n",
      " [-0.11036317]\n",
      " ...\n",
      " [ 0.09268546]\n",
      " [ 0.03901858]\n",
      " [-0.15703669]]\n",
      "t [[ 0.11829184]\n",
      " [-0.0689594 ]\n",
      " [-0.11036317]\n",
      " ...\n",
      " [ 0.09268546]\n",
      " [ 0.03901858]\n",
      " [-0.15703669]]\n",
      "t [[ 0.21976794]\n",
      " [-0.1503205 ]\n",
      " [-0.20951328]\n",
      " ...\n",
      " [ 0.16086421]\n",
      " [ 0.06871707]\n",
      " [-0.28357012]]\n",
      "t [[ 0.21976794]\n",
      " [-0.1503205 ]\n",
      " [-0.20951328]\n",
      " ...\n",
      " [ 0.16086421]\n",
      " [ 0.06871707]\n",
      " [-0.28357012]]\n",
      "Current iteration=2, loss=47047.31921562202\n",
      "t [[ 0.30719406]\n",
      " [-0.23797459]\n",
      " [-0.29939224]\n",
      " ...\n",
      " [ 0.21097497]\n",
      " [ 0.0910895 ]\n",
      " [-0.38776808]]\n",
      "t [[ 0.30719406]\n",
      " [-0.23797459]\n",
      " [-0.29939224]\n",
      " ...\n",
      " [ 0.21097497]\n",
      " [ 0.0910895 ]\n",
      " [-0.38776808]]\n",
      "t [[ 0.38292454]\n",
      " [-0.32786916]\n",
      " [-0.38154225]\n",
      " ...\n",
      " [ 0.24768445]\n",
      " [ 0.1077062 ]\n",
      " [-0.47552377]]\n",
      "t [[ 0.38292454]\n",
      " [-0.32786916]\n",
      " [-0.38154225]\n",
      " ...\n",
      " [ 0.24768445]\n",
      " [ 0.1077062 ]\n",
      " [-0.47552377]]\n",
      "Current iteration=4, loss=43697.05396112487\n",
      "t [[ 0.44889245]\n",
      " [-0.41744681]\n",
      " [-0.45717289]\n",
      " ...\n",
      " [ 0.27435937]\n",
      " [ 0.11978153]\n",
      " [-0.55101715]]\n",
      "t [[ 0.44889245]\n",
      " [-0.41744681]\n",
      " [-0.45717289]\n",
      " ...\n",
      " [ 0.27435937]\n",
      " [ 0.11978153]\n",
      " [-0.55101715]]\n",
      "t [[ 0.50666447]\n",
      " [-0.50516358]\n",
      " [-0.52723593]\n",
      " ...\n",
      " [ 0.29345002]\n",
      " [ 0.12825278]\n",
      " [-0.61721125]]\n",
      "t [[ 0.50666447]\n",
      " [-0.50516358]\n",
      " [-0.52723593]\n",
      " ...\n",
      " [ 0.29345002]\n",
      " [ 0.12825278]\n",
      " [-0.61721125]]\n",
      "Current iteration=6, loss=41264.55267051778\n",
      "t [[ 0.55751174]\n",
      " [-0.59013583]\n",
      " [-0.59248731]\n",
      " ...\n",
      " [ 0.30675289]\n",
      " [ 0.13384706]\n",
      " [-0.67621819]]\n",
      "t [[ 0.55751174]\n",
      " [-0.59013583]\n",
      " [-0.59248731]\n",
      " ...\n",
      " [ 0.30675289]\n",
      " [ 0.13384706]\n",
      " [-0.67621819]]\n",
      "t [[ 0.60247197]\n",
      " [-0.67190106]\n",
      " [-0.6535347 ]\n",
      " ...\n",
      " [ 0.31559498]\n",
      " [ 0.1371329 ]\n",
      " [-0.72955765]]\n",
      "t [[ 0.60247197]\n",
      " [-0.67190106]\n",
      " [-0.6535347 ]\n",
      " ...\n",
      " [ 0.31559498]\n",
      " [ 0.1371329 ]\n",
      " [-0.72955765]]\n",
      "Current iteration=8, loss=39424.14544790331\n",
      "t [[ 0.64239856]\n",
      " [-0.75026131]\n",
      " [-0.71087294]\n",
      " ...\n",
      " [ 0.32096552]\n",
      " [ 0.13855884]\n",
      " [-0.77833623]]\n",
      "t [[ 0.64239856]\n",
      " [-0.75026131]\n",
      " [-0.71087294]\n",
      " ...\n",
      " [ 0.32096552]\n",
      " [ 0.13855884]\n",
      " [-0.77833623]]\n",
      "t [[ 0.67799837]\n",
      " [-0.82518212]\n",
      " [-0.76491013]\n",
      " ...\n",
      " [ 0.32360988]\n",
      " [ 0.13848192]\n",
      " [-0.82337102]]\n",
      "loss=37987.746014695265\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.11677569]\n",
      " [-0.06601396]\n",
      " [-0.10994985]\n",
      " ...\n",
      " [ 0.09411548]\n",
      " [ 0.03973773]\n",
      " [-0.16187264]]\n",
      "t [[ 0.11677569]\n",
      " [-0.06601396]\n",
      " [-0.10994985]\n",
      " ...\n",
      " [ 0.09411548]\n",
      " [ 0.03973773]\n",
      " [-0.16187264]]\n",
      "t [[ 0.21695868]\n",
      " [-0.14448595]\n",
      " [-0.20868724]\n",
      " ...\n",
      " [ 0.16305921]\n",
      " [ 0.07004358]\n",
      " [-0.29232324]]\n",
      "t [[ 0.21695868]\n",
      " [-0.14448595]\n",
      " [-0.20868724]\n",
      " ...\n",
      " [ 0.16305921]\n",
      " [ 0.07004358]\n",
      " [-0.29232324]]\n",
      "Current iteration=2, loss=47086.85841406272\n",
      "t [[ 0.30328352]\n",
      " [-0.22928036]\n",
      " [-0.2981665 ]\n",
      " ...\n",
      " [ 0.21346509]\n",
      " [ 0.09293895]\n",
      " [-0.39974679]]\n",
      "t [[ 0.30328352]\n",
      " [-0.22928036]\n",
      " [-0.2981665 ]\n",
      " ...\n",
      " [ 0.21346509]\n",
      " [ 0.09293895]\n",
      " [-0.39974679]]\n",
      "t [[ 0.37807583]\n",
      " [-0.31633995]\n",
      " [-0.37993632]\n",
      " ...\n",
      " [ 0.25014161]\n",
      " [ 0.11001412]\n",
      " [-0.49020457]]\n",
      "t [[ 0.37807583]\n",
      " [-0.31633995]\n",
      " [-0.37993632]\n",
      " ...\n",
      " [ 0.25014161]\n",
      " [ 0.11001412]\n",
      " [-0.49020457]]\n",
      "Current iteration=4, loss=43773.223544934706\n",
      "t [[ 0.44324588]\n",
      " [-0.40311056]\n",
      " [-0.45520844]\n",
      " ...\n",
      " [ 0.27654542]\n",
      " [ 0.12249751]\n",
      " [-0.56799442]]\n",
      "t [[ 0.44324588]\n",
      " [-0.40311056]\n",
      " [-0.45520844]\n",
      " ...\n",
      " [ 0.27654542]\n",
      " [ 0.12249751]\n",
      " [-0.56799442]]\n",
      "t [[ 0.50034099]\n",
      " [-0.48805456]\n",
      " [-0.52493438]\n",
      " ...\n",
      " [ 0.29518598]\n",
      " [ 0.13133626]\n",
      " [-0.63616351]]\n",
      "t [[ 0.50034099]\n",
      " [-0.48805456]\n",
      " [-0.52493438]\n",
      " ...\n",
      " [ 0.29518598]\n",
      " [ 0.13133626]\n",
      " [-0.63616351]]\n",
      "Current iteration=6, loss=41371.04155600885\n",
      "t [[ 0.55061479]\n",
      " [-0.57029559]\n",
      " [-0.58986874]\n",
      " ...\n",
      " [ 0.30790239]\n",
      " [ 0.13726457]\n",
      " [-0.69688626]]\n",
      "t [[ 0.55061479]\n",
      " [-0.57029559]\n",
      " [-0.58986874]\n",
      " ...\n",
      " [ 0.30790239]\n",
      " [ 0.13726457]\n",
      " [-0.69688626]]\n",
      "t [[ 0.59508939]\n",
      " [-0.64937793]\n",
      " [-0.65061742]\n",
      " ...\n",
      " [ 0.31605381]\n",
      " [ 0.14085624]\n",
      " [-0.75172954]]\n",
      "t [[ 0.59508939]\n",
      " [-0.64937793]\n",
      " [-0.65061742]\n",
      " ...\n",
      " [ 0.31605381]\n",
      " [ 0.14085624]\n",
      " [-0.75172954]]\n",
      "Current iteration=8, loss=39555.2830234413\n",
      "t [[ 0.63460451]\n",
      " [-0.72510912]\n",
      " [-0.70767336]\n",
      " ...\n",
      " [ 0.3206542 ]\n",
      " [ 0.14256383]\n",
      " [-0.80183631]]\n",
      "t [[ 0.63460451]\n",
      " [-0.72510912]\n",
      " [-0.70767336]\n",
      " ...\n",
      " [ 0.3206542 ]\n",
      " [ 0.14256383]\n",
      " [-0.80183631]]\n",
      "t [[ 0.66985532]\n",
      " [-0.79745875]\n",
      " [-0.76144287]\n",
      " ...\n",
      " [ 0.3224681 ]\n",
      " [ 0.14274753]\n",
      " [-0.84805212]]\n",
      "loss=38139.044163837534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.11658876]\n",
      " [-0.06935425]\n",
      " [-0.10960449]\n",
      " ...\n",
      " [ 0.16792448]\n",
      " [-0.10960449]\n",
      " [ 0.15728507]]\n",
      "t [[ 0.11658876]\n",
      " [-0.06935425]\n",
      " [-0.10960449]\n",
      " ...\n",
      " [ 0.16792448]\n",
      " [-0.10960449]\n",
      " [ 0.15728507]]\n",
      "t [[ 0.21661181]\n",
      " [-0.15048863]\n",
      " [-0.20809372]\n",
      " ...\n",
      " [ 0.30903021]\n",
      " [-0.20809372]\n",
      " [ 0.28510825]]\n",
      "t [[ 0.21661181]\n",
      " [-0.15048863]\n",
      " [-0.20809372]\n",
      " ...\n",
      " [ 0.30903021]\n",
      " [-0.20809372]\n",
      " [ 0.28510825]]\n",
      "Current iteration=2, loss=47092.0102227916\n",
      "t [[ 0.30280854]\n",
      " [-0.23747161]\n",
      " [-0.29738748]\n",
      " ...\n",
      " [ 0.42846959]\n",
      " [-0.29738748]\n",
      " [ 0.38990232]]\n",
      "t [[ 0.30280854]\n",
      " [-0.23747161]\n",
      " [-0.29738748]\n",
      " ...\n",
      " [ 0.42846959]\n",
      " [-0.29738748]\n",
      " [ 0.38990232]]\n",
      "t [[ 0.3774773 ]\n",
      " [-0.32640865]\n",
      " [-0.37901263]\n",
      " ...\n",
      " [ 0.53035999]\n",
      " [-0.37901263]\n",
      " [ 0.47663831]]\n",
      "t [[ 0.3774773 ]\n",
      " [-0.32640865]\n",
      " [-0.37901263]\n",
      " ...\n",
      " [ 0.53035999]\n",
      " [-0.37901263]\n",
      " [ 0.47663831]]\n",
      "Current iteration=4, loss=43773.6231925994\n",
      "t [[ 0.44250233]\n",
      " [-0.41485968]\n",
      " [-0.45416718]\n",
      " ...\n",
      " [ 0.61793558]\n",
      " [-0.45416718]\n",
      " [ 0.54909386]]\n",
      "t [[ 0.44250233]\n",
      " [-0.41485968]\n",
      " [-0.45416718]\n",
      " ...\n",
      " [ 0.61793558]\n",
      " [-0.45416718]\n",
      " [ 0.54909386]]\n",
      "t [[ 0.49941876]\n",
      " [-0.50135806]\n",
      " [-0.52379362]\n",
      " ...\n",
      " [ 0.69373415]\n",
      " [-0.52379362]\n",
      " [ 0.61013979]]\n",
      "t [[ 0.49941876]\n",
      " [-0.50135806]\n",
      " [-0.52379362]\n",
      " ...\n",
      " [ 0.69373415]\n",
      " [-0.52379362]\n",
      " [ 0.61013979]]\n",
      "Current iteration=6, loss=41365.2856204428\n",
      "t [[ 0.54947867]\n",
      " [-0.58506907]\n",
      " [-0.58864054]\n",
      " ...\n",
      " [ 0.75975854]\n",
      " [-0.58864054]\n",
      " [ 0.66197364]]\n",
      "t [[ 0.54947867]\n",
      " [-0.58506907]\n",
      " [-0.58864054]\n",
      " ...\n",
      " [ 0.75975854]\n",
      " [-0.58864054]\n",
      " [ 0.66197364]]\n",
      "t [[ 0.59370765]\n",
      " [-0.66556093]\n",
      " [-0.64930991]\n",
      " ...\n",
      " [ 0.81760289]\n",
      " [-0.64930991]\n",
      " [ 0.70629483]]\n",
      "t [[ 0.59370765]\n",
      " [-0.66556093]\n",
      " [-0.64930991]\n",
      " ...\n",
      " [ 0.81760289]\n",
      " [-0.64930991]\n",
      " [ 0.70629483]]\n",
      "Current iteration=8, loss=39543.918975447676\n",
      "t [[ 0.63295066]\n",
      " [-0.7426549 ]\n",
      " [-0.70629214]\n",
      " ...\n",
      " [ 0.86854801]\n",
      " [-0.70629214]\n",
      " [ 0.7444323 ]]\n",
      "t [[ 0.63295066]\n",
      " [-0.7426549 ]\n",
      " [-0.70629214]\n",
      " ...\n",
      " [ 0.86854801]\n",
      " [-0.70629214]\n",
      " [ 0.7444323 ]]\n",
      "t [[ 0.66790815]\n",
      " [-0.81632837]\n",
      " [-0.7599919 ]\n",
      " ...\n",
      " [ 0.91363279]\n",
      " [-0.7599919 ]\n",
      " [ 0.77743708]]\n",
      "loss=38122.886194740444\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04103772]\n",
      " [-0.19650905]\n",
      " [-0.11523516]\n",
      " ...\n",
      " [ 0.09854136]\n",
      " [ 0.04103772]\n",
      " [-0.16703905]]\n",
      "t [[ 0.04103772]\n",
      " [-0.19650905]\n",
      " [-0.11523516]\n",
      " ...\n",
      " [ 0.09854136]\n",
      " [ 0.04103772]\n",
      " [-0.16703905]]\n",
      "t [[ 0.07188333]\n",
      " [-0.376878  ]\n",
      " [-0.21829682]\n",
      " ...\n",
      " [ 0.17000118]\n",
      " [ 0.07188333]\n",
      " [-0.30059233]]\n",
      "t [[ 0.07188333]\n",
      " [-0.376878  ]\n",
      " [-0.21829682]\n",
      " ...\n",
      " [ 0.17000118]\n",
      " [ 0.07188333]\n",
      " [-0.30059233]]\n",
      "Current iteration=2, loss=46859.89751774186\n",
      "t [[ 0.09481783]\n",
      " [-0.54220816]\n",
      " [-0.31138673]\n",
      " ...\n",
      " [ 0.22180483]\n",
      " [ 0.09481783]\n",
      " [-0.40998904]]\n",
      "t [[ 0.09481783]\n",
      " [-0.54220816]\n",
      " [-0.31138673]\n",
      " ...\n",
      " [ 0.22180483]\n",
      " [ 0.09481783]\n",
      " [-0.40998904]]\n",
      "t [[ 0.11161054]\n",
      " [-0.69379703]\n",
      " [-0.39622752]\n",
      " ...\n",
      " [ 0.25923422]\n",
      " [ 0.11161054]\n",
      " [-0.50183088]]\n",
      "t [[ 0.11161054]\n",
      " [-0.69379703]\n",
      " [-0.39622752]\n",
      " ...\n",
      " [ 0.25923422]\n",
      " [ 0.11161054]\n",
      " [-0.50183088]]\n",
      "Current iteration=4, loss=43433.218976115895\n",
      "t [[ 0.12360987]\n",
      " [-0.83297475]\n",
      " [-0.47415224]\n",
      " ...\n",
      " [ 0.2860348 ]\n",
      " [ 0.12360987]\n",
      " [-0.58071357]]\n",
      "t [[ 0.12360987]\n",
      " [-0.83297475]\n",
      " [-0.47415224]\n",
      " ...\n",
      " [ 0.2860348 ]\n",
      " [ 0.12360987]\n",
      " [-0.58071357]]\n",
      "t [[ 0.13184298]\n",
      " [-0.96101365]\n",
      " [-0.54619807]\n",
      " ...\n",
      " [ 0.30489117]\n",
      " [ 0.13184298]\n",
      " [-0.64984073]]\n",
      "t [[ 0.13184298]\n",
      " [-0.96101365]\n",
      " [-0.54619807]\n",
      " ...\n",
      " [ 0.30489117]\n",
      " [ 0.13184298]\n",
      " [-0.64984073]]\n",
      "Current iteration=6, loss=40972.069531597655\n",
      "t [[ 0.13709757]\n",
      " [-1.07908628]\n",
      " [-0.61318139]\n",
      " ...\n",
      " [ 0.31774433]\n",
      " [ 0.13709757]\n",
      " [-0.71146287]]\n",
      "t [[ 0.13709757]\n",
      " [-1.07908628]\n",
      " [-0.61318139]\n",
      " ...\n",
      " [ 0.31774433]\n",
      " [ 0.13709757]\n",
      " [-0.71146287]]\n",
      "t [[ 0.13998338]\n",
      " [-1.18824962]\n",
      " [-0.6757538 ]\n",
      " ...\n",
      " [ 0.32601027]\n",
      " [ 0.13998338]\n",
      " [-0.76717955]]\n",
      "t [[ 0.13998338]\n",
      " [-1.18824962]\n",
      " [-0.6757538 ]\n",
      " ...\n",
      " [ 0.32601027]\n",
      " [ 0.13998338]\n",
      " [-0.76717955]]\n",
      "Current iteration=8, loss=39125.57095456665\n",
      "t [[ 0.14097714]\n",
      " [-1.28944382]\n",
      " [-0.73444304]\n",
      " ...\n",
      " [ 0.33073325]\n",
      " [ 0.14097714]\n",
      " [-0.8181443 ]]\n",
      "t [[ 0.14097714]\n",
      " [-1.28944382]\n",
      " [-0.73444304]\n",
      " ...\n",
      " [ 0.33073325]\n",
      " [ 0.14097714]\n",
      " [-0.8181443 ]]\n",
      "t [[ 0.14045533]\n",
      " [-1.3834985 ]\n",
      " [-0.78968254]\n",
      " ...\n",
      " [ 0.33269298]\n",
      " [ 0.14045533]\n",
      " [-0.86520279]]\n",
      "loss=37694.082348914926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12392478]\n",
      " [-0.07224318]\n",
      " [-0.11561856]\n",
      " ...\n",
      " [ 0.09709906]\n",
      " [ 0.0408766 ]\n",
      " [-0.16451463]]\n",
      "t [[ 0.12392478]\n",
      " [-0.07224318]\n",
      " [-0.11561856]\n",
      " ...\n",
      " [ 0.09709906]\n",
      " [ 0.0408766 ]\n",
      " [-0.16451463]]\n",
      "t [[ 0.22940021]\n",
      " [-0.15809212]\n",
      " [-0.21893413]\n",
      " ...\n",
      " [ 0.16730968]\n",
      " [ 0.0715276 ]\n",
      " [-0.29556242]]\n",
      "t [[ 0.22940021]\n",
      " [-0.15809212]\n",
      " [-0.21893413]\n",
      " ...\n",
      " [ 0.16730968]\n",
      " [ 0.0715276 ]\n",
      " [-0.29556242]]\n",
      "Current iteration=2, loss=46849.876354383785\n",
      "t [[ 0.31961351]\n",
      " [-0.25052532]\n",
      " [-0.31218284]\n",
      " ...\n",
      " [ 0.21803649]\n",
      " [ 0.09424834]\n",
      " [-0.40253904]]\n",
      "t [[ 0.31961351]\n",
      " [-0.25052532]\n",
      " [-0.31218284]\n",
      " ...\n",
      " [ 0.21803649]\n",
      " [ 0.09424834]\n",
      " [-0.40253904]]\n",
      "t [[ 0.39724392]\n",
      " [-0.34500734]\n",
      " [-0.39711348]\n",
      " ...\n",
      " [ 0.25454072]\n",
      " [ 0.11081799]\n",
      " [-0.49208354]]\n",
      "t [[ 0.39724392]\n",
      " [-0.34500734]\n",
      " [-0.39711348]\n",
      " ...\n",
      " [ 0.25454072]\n",
      " [ 0.11081799]\n",
      " [-0.49208354]]\n",
      "Current iteration=4, loss=43420.69739135708\n",
      "t [[ 0.4644618 ]\n",
      " [-0.43876395]\n",
      " [-0.47507825]\n",
      " ...\n",
      " [ 0.28054985]\n",
      " [ 0.12259138]\n",
      " [-0.56880943]]\n",
      "t [[ 0.4644618 ]\n",
      " [-0.43876395]\n",
      " [-0.47507825]\n",
      " ...\n",
      " [ 0.28054985]\n",
      " [ 0.12259138]\n",
      " [-0.56880943]]\n",
      "t [[ 0.52300544]\n",
      " [-0.53018059]\n",
      " [-0.54712814]\n",
      " ...\n",
      " [ 0.2987315 ]\n",
      " [ 0.13059983]\n",
      " [-0.6359253 ]]\n",
      "t [[ 0.52300544]\n",
      " [-0.53018059]\n",
      " [-0.54712814]\n",
      " ...\n",
      " [ 0.2987315 ]\n",
      " [ 0.13059983]\n",
      " [-0.6359253 ]]\n",
      "Current iteration=6, loss=40959.11042335733\n",
      "t [[ 0.57427125]\n",
      " [-0.61837663]\n",
      " [-0.61408951]\n",
      " ...\n",
      " [ 0.31101083]\n",
      " [ 0.13563373]\n",
      " [-0.69567907]]\n",
      "t [[ 0.57427125]\n",
      " [-0.61837663]\n",
      " [-0.61408951]\n",
      " ...\n",
      " [ 0.31101083]\n",
      " [ 0.13563373]\n",
      " [-0.69567907]]\n",
      "t [[ 0.61938948]\n",
      " [-0.70292598]\n",
      " [-0.67662126]\n",
      " ...\n",
      " [ 0.31878966]\n",
      " [ 0.13830456]\n",
      " [-0.74966352]]\n",
      "t [[ 0.61938948]\n",
      " [-0.70292598]\n",
      " [-0.67662126]\n",
      " ...\n",
      " [ 0.31878966]\n",
      " [ 0.13830456]\n",
      " [-0.74966352]]\n",
      "Current iteration=8, loss=39112.73334000039\n",
      "t [[ 0.65928247]\n",
      " [-0.78367946]\n",
      " [-0.73525637]\n",
      " ...\n",
      " [ 0.32309997]\n",
      " [ 0.13909016]\n",
      " [-0.79902338]]\n",
      "t [[ 0.65928247]\n",
      " [-0.78367946]\n",
      " [-0.73525637]\n",
      " ...\n",
      " [ 0.32309997]\n",
      " [ 0.13909016]\n",
      " [-0.79902338]]\n",
      "t [[ 0.6947089 ]\n",
      " [-0.86065311]\n",
      " [-0.79043217]\n",
      " ...\n",
      " [ 0.32471095]\n",
      " [ 0.13836767]\n",
      " [-0.84459476]]\n",
      "loss=37681.462487324185\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.12233644]\n",
      " [-0.06915748]\n",
      " [-0.11518555]\n",
      " ...\n",
      " [ 0.09859717]\n",
      " [ 0.04163   ]\n",
      " [-0.16958086]]\n",
      "t [[ 0.12233644]\n",
      " [-0.06915748]\n",
      " [-0.11518555]\n",
      " ...\n",
      " [ 0.09859717]\n",
      " [ 0.04163   ]\n",
      " [-0.16958086]]\n",
      "t [[ 0.22646818]\n",
      " [-0.15198255]\n",
      " [-0.21806884]\n",
      " ...\n",
      " [ 0.16957683]\n",
      " [ 0.07291179]\n",
      " [-0.30468726]]\n",
      "t [[ 0.22646818]\n",
      " [-0.15198255]\n",
      " [-0.21806884]\n",
      " ...\n",
      " [ 0.16957683]\n",
      " [ 0.07291179]\n",
      " [-0.30468726]]\n",
      "Current iteration=2, loss=46891.308219187005\n",
      "t [[ 0.31554641]\n",
      " [-0.24142289]\n",
      " [-0.31090025]\n",
      " ...\n",
      " [ 0.22056829]\n",
      " [ 0.09617222]\n",
      " [-0.41497638]]\n",
      "t [[ 0.31554641]\n",
      " [-0.24142289]\n",
      " [-0.31090025]\n",
      " ...\n",
      " [ 0.22056829]\n",
      " [ 0.09617222]\n",
      " [-0.41497638]]\n",
      "t [[ 0.39221782]\n",
      " [-0.33293858]\n",
      " [-0.3954356 ]\n",
      " ...\n",
      " [ 0.25699104]\n",
      " [ 0.11321289]\n",
      " [-0.50727625]]\n",
      "t [[ 0.39221782]\n",
      " [-0.33293858]\n",
      " [-0.3954356 ]\n",
      " ...\n",
      " [ 0.25699104]\n",
      " [ 0.11321289]\n",
      " [-0.50727625]]\n",
      "Current iteration=4, loss=43500.11014510344\n",
      "t [[ 0.45862718]\n",
      " [-0.42375986]\n",
      " [-0.47302904]\n",
      " ...\n",
      " [ 0.28267015]\n",
      " [ 0.12540405]\n",
      " [-0.58633009]]\n",
      "t [[ 0.45862718]\n",
      " [-0.42375986]\n",
      " [-0.47302904]\n",
      " ...\n",
      " [ 0.28267015]\n",
      " [ 0.12540405]\n",
      " [-0.58633009]]\n",
      "t [[ 0.51649088]\n",
      " [-0.51228002]\n",
      " [-0.54473098]\n",
      " ...\n",
      " [ 0.30033705]\n",
      " [ 0.13378771]\n",
      " [-0.655438  ]]\n",
      "t [[ 0.51649088]\n",
      " [-0.51228002]\n",
      " [-0.54473098]\n",
      " ...\n",
      " [ 0.30033705]\n",
      " [ 0.13378771]\n",
      " [-0.655438  ]]\n",
      "Current iteration=6, loss=41069.55398487997\n",
      "t [[ 0.56718566]\n",
      " [-0.59762697]\n",
      " [-0.61136603]\n",
      " ...\n",
      " [ 0.31196262]\n",
      " [ 0.13916182]\n",
      " [-0.71691489]]\n",
      "t [[ 0.56718566]\n",
      " [-0.59762697]\n",
      " [-0.61136603]\n",
      " ...\n",
      " [ 0.31196262]\n",
      " [ 0.13916182]\n",
      " [-0.71691489]]\n",
      "t [[ 0.61182436]\n",
      " [-0.67938219]\n",
      " [-0.67359095]\n",
      " ...\n",
      " [ 0.31898297]\n",
      " [ 0.14214346]\n",
      " [-0.77240387]]\n",
      "t [[ 0.61182436]\n",
      " [-0.67938219]\n",
      " [-0.67359095]\n",
      " ...\n",
      " [ 0.31898297]\n",
      " [ 0.14214346]\n",
      " [-0.77240387]]\n",
      "Current iteration=8, loss=39248.14873938344\n",
      "t [[ 0.65131433]\n",
      " [-0.75740241]\n",
      " [-0.73193656]\n",
      " ...\n",
      " [ 0.32245618]\n",
      " [ 0.14321471]\n",
      " [-0.82308809]]\n",
      "t [[ 0.65131433]\n",
      " [-0.75740241]\n",
      " [-0.73193656]\n",
      " ...\n",
      " [ 0.32245618]\n",
      " [ 0.14321471]\n",
      " [-0.82308809]]\n",
      "t [[ 0.6864016 ]\n",
      " [-0.83170781]\n",
      " [-0.78683814]\n",
      " ...\n",
      " [ 0.32317142]\n",
      " [ 0.14275605]\n",
      " [-0.86983349]]\n",
      "loss=37837.14484012677\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12214061]\n",
      " [-0.07265684]\n",
      " [-0.11482375]\n",
      " ...\n",
      " [ 0.17592089]\n",
      " [-0.11482375]\n",
      " [ 0.16477483]]\n",
      "t [[ 0.12214061]\n",
      " [-0.07265684]\n",
      " [-0.11482375]\n",
      " ...\n",
      " [ 0.17592089]\n",
      " [-0.11482375]\n",
      " [ 0.16477483]]\n",
      "t [[ 0.22610574]\n",
      " [-0.1582379 ]\n",
      " [-0.21745183]\n",
      " ...\n",
      " [ 0.32241697]\n",
      " [-0.21745183]\n",
      " [ 0.29722523]]\n",
      "t [[ 0.22610574]\n",
      " [-0.1582379 ]\n",
      " [-0.21745183]\n",
      " ...\n",
      " [ 0.32241697]\n",
      " [-0.21745183]\n",
      " [ 0.29722523]]\n",
      "Current iteration=2, loss=46896.404713362776\n",
      "t [[ 0.31505094]\n",
      " [-0.24992666]\n",
      " [-0.31009492]\n",
      " ...\n",
      " [ 0.44542055]\n",
      " [-0.31009492]\n",
      " [ 0.40475529]]\n",
      "t [[ 0.31505094]\n",
      " [-0.24992666]\n",
      " [-0.31009492]\n",
      " ...\n",
      " [ 0.44542055]\n",
      " [-0.31009492]\n",
      " [ 0.40475529]]\n",
      "t [[ 0.39159066]\n",
      " [-0.34336696]\n",
      " [-0.39448455]\n",
      " ...\n",
      " [ 0.54960139]\n",
      " [-0.39448455]\n",
      " [ 0.49299059]]\n",
      "t [[ 0.39159066]\n",
      " [-0.34336696]\n",
      " [-0.39448455]\n",
      " ...\n",
      " [ 0.54960139]\n",
      " [-0.39448455]\n",
      " [ 0.49299059]]\n",
      "Current iteration=4, loss=43499.95291992988\n",
      "t [[ 0.45784157]\n",
      " [-0.43591338]\n",
      " [-0.47195993]\n",
      " ...\n",
      " [ 0.63857481]\n",
      " [-0.47195993]\n",
      " [ 0.56613693]]\n",
      "t [[ 0.45784157]\n",
      " [-0.43591338]\n",
      " [-0.47195993]\n",
      " ...\n",
      " [ 0.63857481]\n",
      " [-0.47195993]\n",
      " [ 0.56613693]]\n",
      "t [[ 0.51550879]\n",
      " [-0.52603354]\n",
      " [-0.54356181]\n",
      " ...\n",
      " [ 0.71514155]\n",
      " [-0.54356181]\n",
      " [ 0.62734451]]\n",
      "t [[ 0.51550879]\n",
      " [-0.52603354]\n",
      " [-0.54356181]\n",
      " ...\n",
      " [ 0.71514155]\n",
      " [-0.54356181]\n",
      " [ 0.62734451]]\n",
      "Current iteration=6, loss=41062.94841761875\n",
      "t [[ 0.56596906]\n",
      " [-0.61289734]\n",
      " [-0.61010859]\n",
      " ...\n",
      " [ 0.78148762]\n",
      " [-0.61010859]\n",
      " [ 0.67899464]]\n",
      "t [[ 0.56596906]\n",
      " [-0.61289734]\n",
      " [-0.61010859]\n",
      " ...\n",
      " [ 0.78148762]\n",
      " [-0.61010859]\n",
      " [ 0.67899464]]\n",
      "t [[ 0.61034015]\n",
      " [-0.6961096 ]\n",
      " [-0.67225301]\n",
      " ...\n",
      " [ 0.83933637]\n",
      " [-0.67225301]\n",
      " [ 0.7229091 ]]\n",
      "t [[ 0.61034015]\n",
      " [-0.6961096 ]\n",
      " [-0.67225301]\n",
      " ...\n",
      " [ 0.83933637]\n",
      " [-0.67225301]\n",
      " [ 0.7229091 ]]\n",
      "Current iteration=8, loss=39235.79535139694\n",
      "t [[ 0.64953558]\n",
      " [-0.77553991]\n",
      " [-0.73052343]\n",
      " ...\n",
      " [ 0.89006098]\n",
      " [-0.73052343]\n",
      " [ 0.76049934]]\n",
      "t [[ 0.64953558]\n",
      " [-0.77553991]\n",
      " [-0.73052343]\n",
      " ...\n",
      " [ 0.89006098]\n",
      " [-0.73052343]\n",
      " [ 0.76049934]]\n",
      "t [[ 0.68430727]\n",
      " [-0.85121548]\n",
      " [-0.7853536 ]\n",
      " ...\n",
      " [ 0.93476725]\n",
      " [-0.7853536 ]\n",
      " [ 0.79287269]]\n",
      "loss=37819.93805671831\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04290307]\n",
      " [-0.20544127]\n",
      " [-0.12047312]\n",
      " ...\n",
      " [ 0.10302051]\n",
      " [ 0.04290307]\n",
      " [-0.17463173]]\n",
      "t [[ 0.04290307]\n",
      " [-0.20544127]\n",
      " [-0.12047312]\n",
      " ...\n",
      " [ 0.10302051]\n",
      " [ 0.04290307]\n",
      " [-0.17463173]]\n",
      "t [[ 0.07467009]\n",
      " [-0.39324553]\n",
      " [-0.22764474]\n",
      " ...\n",
      " [ 0.17645064]\n",
      " [ 0.07467009]\n",
      " [-0.31267648]]\n",
      "t [[ 0.07467009]\n",
      " [-0.39324553]\n",
      " [-0.22764474]\n",
      " ...\n",
      " [ 0.17645064]\n",
      " [ 0.07467009]\n",
      " [-0.31267648]]\n",
      "Current iteration=2, loss=46665.553224595875\n",
      "t [[ 0.09791019]\n",
      " [-0.56467838]\n",
      " [-0.32403417]\n",
      " ...\n",
      " [ 0.22877606]\n",
      " [ 0.09791019]\n",
      " [-0.42479846]]\n",
      "t [[ 0.09791019]\n",
      " [-0.56467838]\n",
      " [-0.32403417]\n",
      " ...\n",
      " [ 0.22877606]\n",
      " [ 0.09791019]\n",
      " [-0.42479846]]\n",
      "t [[ 0.11461484]\n",
      " [-0.7212326 ]\n",
      " [-0.411582  ]\n",
      " ...\n",
      " [ 0.2659123 ]\n",
      " [ 0.11461484]\n",
      " [-0.5183855 ]]\n",
      "t [[ 0.11461484]\n",
      " [-0.7212326 ]\n",
      " [-0.411582  ]\n",
      " ...\n",
      " [ 0.2659123 ]\n",
      " [ 0.11461484]\n",
      " [-0.5183855 ]]\n",
      "Current iteration=4, loss=43164.59694575589\n",
      "t [[ 0.12627842]\n",
      " [-0.86442783]\n",
      " [-0.49176865]\n",
      " ...\n",
      " [ 0.2919786 ]\n",
      " [ 0.12627842]\n",
      " [-0.59847575]]\n",
      "t [[ 0.12627842]\n",
      " [-0.86442783]\n",
      " [-0.49176865]\n",
      " ...\n",
      " [ 0.2919786 ]\n",
      " [ 0.12627842]\n",
      " [-0.59847575]]\n",
      "t [[ 0.13402383]\n",
      " [-0.99570484]\n",
      " [-0.56573223]\n",
      " ...\n",
      " [ 0.30987835]\n",
      " [ 0.13402383]\n",
      " [-0.66851537]]\n",
      "t [[ 0.13402383]\n",
      " [-0.99570484]\n",
      " [-0.56573223]\n",
      " ...\n",
      " [ 0.30987835]\n",
      " [ 0.13402383]\n",
      " [-0.66851537]]\n",
      "Current iteration=6, loss=40677.96829257425\n",
      "t [[ 0.13870197]\n",
      " [-1.11637917]\n",
      " [-0.6343599 ]\n",
      " ...\n",
      " [ 0.32167825]\n",
      " [ 0.13870197]\n",
      " [-0.73088458]]\n",
      "t [[ 0.13870197]\n",
      " [-1.11637917]\n",
      " [-0.6343599 ]\n",
      " ...\n",
      " [ 0.32167825]\n",
      " [ 0.13870197]\n",
      " [-0.73088458]]\n",
      "t [[ 0.14096468]\n",
      " [-1.22762606]\n",
      " [-0.69835433]\n",
      " ...\n",
      " [ 0.32886509]\n",
      " [ 0.14096468]\n",
      " [-0.78725041]]\n",
      "t [[ 0.14096468]\n",
      " [-1.22762606]\n",
      " [-0.69835433]\n",
      " ...\n",
      " [ 0.32886509]\n",
      " [ 0.14096468]\n",
      " [-0.78725041]]\n",
      "Current iteration=8, loss=38827.950071144754\n",
      "t [[ 0.141317  ]\n",
      " [-1.33048185]\n",
      " [-0.75828112]\n",
      " ...\n",
      " [ 0.33252225]\n",
      " [ 0.141317  ]\n",
      " [-0.8388004 ]]\n",
      "t [[ 0.141317  ]\n",
      " [-1.33048185]\n",
      " [-0.75828112]\n",
      " ...\n",
      " [ 0.33252225]\n",
      " [ 0.141317  ]\n",
      " [-0.8388004 ]]\n",
      "t [[ 0.14015446]\n",
      " [-1.42585389]\n",
      " [-0.81460264]\n",
      " ...\n",
      " [ 0.33345042]\n",
      " [ 0.14015446]\n",
      " [-0.88639662]]\n",
      "loss=37403.12924916107\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12955772]\n",
      " [-0.07552696]\n",
      " [-0.12087395]\n",
      " ...\n",
      " [ 0.10151265]\n",
      " [ 0.04273463]\n",
      " [-0.17199257]]\n",
      "t [[ 0.12955772]\n",
      " [-0.07552696]\n",
      " [-0.12087395]\n",
      " ...\n",
      " [ 0.10151265]\n",
      " [ 0.04273463]\n",
      " [-0.17199257]]\n",
      "t [[ 0.23895763]\n",
      " [-0.16591878]\n",
      " [-0.22830493]\n",
      " ...\n",
      " [ 0.17364585]\n",
      " [ 0.07429662]\n",
      " [-0.30741891]]\n",
      "t [[ 0.23895763]\n",
      " [-0.16591878]\n",
      " [-0.22830493]\n",
      " ...\n",
      " [ 0.17364585]\n",
      " [ 0.07429662]\n",
      " [-0.30741891]]\n",
      "Current iteration=2, loss=46655.282449559854\n",
      "t [[ 0.33184932]\n",
      " [-0.26315546]\n",
      " [-0.3248516 ]\n",
      " ...\n",
      " [ 0.22486151]\n",
      " [ 0.09731151]\n",
      " [-0.41701904]]\n",
      "t [[ 0.33184932]\n",
      " [-0.26315546]\n",
      " [-0.3248516 ]\n",
      " ...\n",
      " [ 0.22486151]\n",
      " [ 0.09731151]\n",
      " [-0.41701904]]\n",
      "t [[ 0.41126138]\n",
      " [-0.36219664]\n",
      " [-0.4124839 ]\n",
      " ...\n",
      " [ 0.26105181]\n",
      " [ 0.1137817 ]\n",
      " [-0.50822186]]\n",
      "t [[ 0.41126138]\n",
      " [-0.36219664]\n",
      " [-0.4124839 ]\n",
      " ...\n",
      " [ 0.26105181]\n",
      " [ 0.1137817 ]\n",
      " [-0.50822186]]\n",
      "Current iteration=4, loss=43151.964953690745\n",
      "t [[ 0.47961426]\n",
      " [-0.46005702]\n",
      " [-0.49270334]\n",
      " ...\n",
      " [ 0.28631553]\n",
      " [ 0.12520857]\n",
      " [-0.58608375]]\n",
      "t [[ 0.47961426]\n",
      " [-0.46005702]\n",
      " [-0.49270334]\n",
      " ...\n",
      " [ 0.28631553]\n",
      " [ 0.12520857]\n",
      " [-0.58608375]]\n",
      "t [[ 0.53882411]\n",
      " [-0.55506507]\n",
      " [-0.566663  ]\n",
      " ...\n",
      " [ 0.30353684]\n",
      " [ 0.13271942]\n",
      " [-0.65405468]]\n",
      "t [[ 0.53882411]\n",
      " [-0.55506507]\n",
      " [-0.566663  ]\n",
      " ...\n",
      " [ 0.30353684]\n",
      " [ 0.13271942]\n",
      " [-0.65405468]]\n",
      "Current iteration=6, loss=40665.00182124511\n",
      "t [[ 0.59041543]\n",
      " [-0.64635652]\n",
      " [-0.6352607 ]\n",
      " ...\n",
      " [ 0.31476462]\n",
      " [ 0.13716798]\n",
      " [-0.71451046]]\n",
      "t [[ 0.59041543]\n",
      " [-0.64635652]\n",
      " [-0.6352607 ]\n",
      " ...\n",
      " [ 0.31476462]\n",
      " [ 0.13716798]\n",
      " [-0.71451046]]\n",
      "t [[ 0.63561179]\n",
      " [-0.73355229]\n",
      " [-0.69920669]\n",
      " ...\n",
      " [ 0.32147001]\n",
      " [ 0.13920783]\n",
      " [-0.76910949]]\n",
      "t [[ 0.63561179]\n",
      " [-0.73355229]\n",
      " [-0.69920669]\n",
      " ...\n",
      " [ 0.32147001]\n",
      " [ 0.13920783]\n",
      " [-0.76910949]]\n",
      "Current iteration=8, loss=38815.152548344726\n",
      "t [[ 0.67540427]\n",
      " [-0.81655945]\n",
      " [-0.75907206]\n",
      " ...\n",
      " [ 0.32472294]\n",
      " [ 0.13934509]\n",
      " [-0.81902872]]\n",
      "t [[ 0.67540427]\n",
      " [-0.81655945]\n",
      " [-0.75907206]\n",
      " ...\n",
      " [ 0.32472294]\n",
      " [ 0.13934509]\n",
      " [-0.81902872]]\n",
      "t [[ 0.71060259]\n",
      " [-0.89544952]\n",
      " [-0.81532315]\n",
      " ...\n",
      " [ 0.32531274]\n",
      " [ 0.1379759 ]\n",
      " [-0.86511911]]\n",
      "loss=37390.558474502504\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.12789718]\n",
      " [-0.07230101]\n",
      " [-0.12042126]\n",
      " ...\n",
      " [ 0.10307886]\n",
      " [ 0.04352227]\n",
      " [-0.17728908]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.12789718]\n",
      " [-0.07230101]\n",
      " [-0.12042126]\n",
      " ...\n",
      " [ 0.10307886]\n",
      " [ 0.04352227]\n",
      " [-0.17728908]]\n",
      "t [[ 0.23590381]\n",
      " [-0.15953442]\n",
      " [-0.22740042]\n",
      " ...\n",
      " [ 0.17598232]\n",
      " [ 0.07573802]\n",
      " [-0.31691147]]\n",
      "t [[ 0.23590381]\n",
      " [-0.15953442]\n",
      " [-0.22740042]\n",
      " ...\n",
      " [ 0.17598232]\n",
      " [ 0.07573802]\n",
      " [-0.31691147]]\n",
      "Current iteration=2, loss=46698.60305337897\n",
      "t [[ 0.32762814]\n",
      " [-0.25364523]\n",
      " [-0.32351238]\n",
      " ...\n",
      " [ 0.22742924]\n",
      " [ 0.09930876]\n",
      " [-0.42990626]]\n",
      "t [[ 0.32762814]\n",
      " [-0.25364523]\n",
      " [-0.32351238]\n",
      " ...\n",
      " [ 0.22742924]\n",
      " [ 0.09930876]\n",
      " [-0.42990626]]\n",
      "t [[ 0.40606214]\n",
      " [-0.34958897]\n",
      " [-0.4107346 ]\n",
      " ...\n",
      " [ 0.26348707]\n",
      " [ 0.11626196]\n",
      " [-0.52391328]]\n",
      "t [[ 0.40606214]\n",
      " [-0.34958897]\n",
      " [-0.4107346 ]\n",
      " ...\n",
      " [ 0.26348707]\n",
      " [ 0.11626196]\n",
      " [-0.52391328]]\n",
      "Current iteration=4, loss=43234.56994686763\n",
      "t [[ 0.47359764]\n",
      " [-0.44438627]\n",
      " [-0.49057034]\n",
      " ...\n",
      " [ 0.28836   ]\n",
      " [ 0.12811582]\n",
      " [-0.60413081]]\n",
      "t [[ 0.47359764]\n",
      " [-0.44438627]\n",
      " [-0.49057034]\n",
      " ...\n",
      " [ 0.28836   ]\n",
      " [ 0.12811582]\n",
      " [-0.60413081]]\n",
      "t [[ 0.53212618]\n",
      " [-0.53637515]\n",
      " [-0.56417162]\n",
      " ...\n",
      " [ 0.3050006 ]\n",
      " [ 0.13600913]\n",
      " [-0.67410767]]\n",
      "t [[ 0.53212618]\n",
      " [-0.53637515]\n",
      " [-0.56417162]\n",
      " ...\n",
      " [ 0.3050006 ]\n",
      " [ 0.13600913]\n",
      " [-0.67410767]]\n",
      "Current iteration=6, loss=40779.29301518467\n",
      "t [[ 0.58315024]\n",
      " [-0.62470121]\n",
      " [-0.63243408]\n",
      " ...\n",
      " [ 0.31550666]\n",
      " [ 0.14080365]\n",
      " [-0.73629138]]\n",
      "t [[ 0.58315024]\n",
      " [-0.62470121]\n",
      " [-0.63243408]\n",
      " ...\n",
      " [ 0.31550666]\n",
      " [ 0.14080365]\n",
      " [-0.73629138]]\n",
      "t [[ 0.62787422]\n",
      " [-0.70899369]\n",
      " [-0.69606548]\n",
      " ...\n",
      " [ 0.32138565]\n",
      " [ 0.1431589 ]\n",
      " [-0.79239363]]\n",
      "t [[ 0.62787422]\n",
      " [-0.70899369]\n",
      " [-0.69606548]\n",
      " ...\n",
      " [ 0.32138565]\n",
      " [ 0.1431589 ]\n",
      " [-0.79239363]]\n",
      "Current iteration=8, loss=38954.69373882777\n",
      "t [[ 0.66727289]\n",
      " [-0.78916592]\n",
      " [-0.75563443]\n",
      " ...\n",
      " [ 0.32373487]\n",
      " [ 0.14358545]\n",
      " [-0.8436318 ]]\n",
      "t [[ 0.66727289]\n",
      " [-0.78916592]\n",
      " [-0.75563443]\n",
      " ...\n",
      " [ 0.32373487]\n",
      " [ 0.14358545]\n",
      " [-0.8436318 ]]\n",
      "t [[ 0.70214238]\n",
      " [-0.86529349]\n",
      " [-0.81160501]\n",
      " ...\n",
      " [ 0.32336434]\n",
      " [ 0.14248297]\n",
      " [-0.89088797]]\n",
      "loss=37550.440666681316\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.12769245]\n",
      " [-0.07595942]\n",
      " [-0.12004301]\n",
      " ...\n",
      " [ 0.18391729]\n",
      " [-0.12004301]\n",
      " [ 0.1722646 ]]\n",
      "t [[ 0.12769245]\n",
      " [-0.07595942]\n",
      " [-0.12004301]\n",
      " ...\n",
      " [ 0.18391729]\n",
      " [-0.12004301]\n",
      " [ 0.1722646 ]]\n",
      "t [[ 0.23552582]\n",
      " [-0.16603953]\n",
      " [-0.22676032]\n",
      " ...\n",
      " [ 0.33568421]\n",
      " [-0.22676032]\n",
      " [ 0.30921099]]\n",
      "t [[ 0.23552582]\n",
      " [-0.16603953]\n",
      " [-0.22676032]\n",
      " ...\n",
      " [ 0.33568421]\n",
      " [-0.22676032]\n",
      " [ 0.30921099]]\n",
      "Current iteration=2, loss=46703.624873633795\n",
      "t [[ 0.32711204]\n",
      " [-0.2624559 ]\n",
      " [-0.32268153]\n",
      " ...\n",
      " [ 0.46208783]\n",
      " [-0.32268153]\n",
      " [ 0.41930746]]\n",
      "t [[ 0.32711204]\n",
      " [-0.2624559 ]\n",
      " [-0.32268153]\n",
      " ...\n",
      " [ 0.46208783]\n",
      " [-0.32268153]\n",
      " [ 0.41930746]]\n",
      "t [[ 0.40540547]\n",
      " [-0.36037007]\n",
      " [-0.40975721]\n",
      " ...\n",
      " [ 0.56838938]\n",
      " [-0.40975721]\n",
      " [ 0.50887797]]\n",
      "t [[ 0.40540547]\n",
      " [-0.36037007]\n",
      " [-0.40975721]\n",
      " ...\n",
      " [ 0.56838938]\n",
      " [-0.40975721]\n",
      " [ 0.50887797]]\n",
      "Current iteration=4, loss=43233.844021650904\n",
      "t [[ 0.47276813]\n",
      " [-0.45693695]\n",
      " [-0.48947445]\n",
      " ...\n",
      " [ 0.65860321]\n",
      " [-0.48947445]\n",
      " [ 0.58257338]]\n",
      "t [[ 0.47276813]\n",
      " [-0.45693695]\n",
      " [-0.48947445]\n",
      " ...\n",
      " [ 0.65860321]\n",
      " [-0.48947445]\n",
      " [ 0.58257338]]\n",
      "t [[ 0.53108153]\n",
      " [-0.55057167]\n",
      " [-0.5629751 ]\n",
      " ...\n",
      " [ 0.73580008]\n",
      " [-0.5629751 ]\n",
      " [ 0.64382753]]\n",
      "t [[ 0.53108153]\n",
      " [-0.55057167]\n",
      " [-0.5629751 ]\n",
      " ...\n",
      " [ 0.73580008]\n",
      " [-0.5629751 ]\n",
      " [ 0.64382753]]\n",
      "Current iteration=6, loss=40771.84778463395\n",
      "t [[ 0.58184994]\n",
      " [-0.64046175]\n",
      " [-0.63114835]\n",
      " ...\n",
      " [ 0.80235088]\n",
      " [-0.63114835]\n",
      " [ 0.69520449]]\n",
      "t [[ 0.58184994]\n",
      " [-0.64046175]\n",
      " [-0.63114835]\n",
      " ...\n",
      " [ 0.80235088]\n",
      " [-0.63114835]\n",
      " [ 0.69520449]]\n",
      "t [[ 0.62628405]\n",
      " [-0.72625892]\n",
      " [-0.69469798]\n",
      " ...\n",
      " [ 0.86010744]\n",
      " [-0.69469798]\n",
      " [ 0.73864536]]\n",
      "t [[ 0.62628405]\n",
      " [-0.72625892]\n",
      " [-0.69469798]\n",
      " ...\n",
      " [ 0.86010744]\n",
      " [-0.69469798]\n",
      " [ 0.73864536]]\n",
      "Current iteration=8, loss=38941.377632222226\n",
      "t [[ 0.66536574]\n",
      " [-0.8078884 ]\n",
      " [-0.75419019]\n",
      " ...\n",
      " [ 0.91053361]\n",
      " [-0.75419019]\n",
      " [ 0.77564071]]\n",
      "t [[ 0.66536574]\n",
      " [-0.8078884 ]\n",
      " [-0.75419019]\n",
      " ...\n",
      " [ 0.91053361]\n",
      " [-0.75419019]\n",
      " [ 0.77564071]]\n",
      "t [[ 0.69989759]\n",
      " [-0.88543203]\n",
      " [-0.81008761]\n",
      " ...\n",
      " [ 0.95480004]\n",
      " [-0.81008761]\n",
      " [ 0.80735103]]\n",
      "loss=37532.223401462026\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04476842]\n",
      " [-0.2143735 ]\n",
      " [-0.12571108]\n",
      " ...\n",
      " [ 0.10749967]\n",
      " [ 0.04476842]\n",
      " [-0.18222442]]\n",
      "t [[ 0.04476842]\n",
      " [-0.2143735 ]\n",
      " [-0.12571108]\n",
      " ...\n",
      " [ 0.10749967]\n",
      " [ 0.04476842]\n",
      " [-0.18222442]]\n",
      "t [[ 0.07741555]\n",
      " [-0.40954721]\n",
      " [-0.23694321]\n",
      " ...\n",
      " [ 0.18279024]\n",
      " [ 0.07741555]\n",
      " [-0.32462499]]\n",
      "t [[ 0.07741555]\n",
      " [-0.40954721]\n",
      " [-0.23694321]\n",
      " ...\n",
      " [ 0.18279024]\n",
      " [ 0.07741555]\n",
      " [-0.32462499]]\n",
      "Current iteration=2, loss=46473.99715699239\n",
      "t [[ 0.10090862]\n",
      " [-0.58696703]\n",
      " [-0.33656241]\n",
      " ...\n",
      " [ 0.23551327]\n",
      " [ 0.10090862]\n",
      " [-0.43932165]]\n",
      "t [[ 0.10090862]\n",
      " [-0.58696703]\n",
      " [-0.33656241]\n",
      " ...\n",
      " [ 0.23551327]\n",
      " [ 0.10090862]\n",
      " [-0.43932165]]\n",
      "t [[ 0.11747542]\n",
      " [-0.74833982]\n",
      " [-0.42674146]\n",
      " ...\n",
      " [ 0.27225351]\n",
      " [ 0.11747542]\n",
      " [-0.53453055]]\n",
      "t [[ 0.11747542]\n",
      " [-0.74833982]\n",
      " [-0.42674146]\n",
      " ...\n",
      " [ 0.27225351]\n",
      " [ 0.11747542]\n",
      " [-0.53453055]]\n",
      "Current iteration=4, loss=42903.2993630445\n",
      "t [[ 0.12876193]\n",
      " [-0.89539051]\n",
      " [-0.50911441]\n",
      " ...\n",
      " [ 0.29751282]\n",
      " [ 0.12876193]\n",
      " [-0.61573924]]\n",
      "t [[ 0.12876193]\n",
      " [-0.89539051]\n",
      " [-0.50911441]\n",
      " ...\n",
      " [ 0.29751282]\n",
      " [ 0.12876193]\n",
      " [-0.61573924]]\n",
      "t [[ 0.13598827]\n",
      " [-1.02973981]\n",
      " [-0.58492275]\n",
      " ...\n",
      " [ 0.31441125]\n",
      " [ 0.13598827]\n",
      " [-0.68663069]]\n",
      "t [[ 0.13598827]\n",
      " [-1.02973981]\n",
      " [-0.58492275]\n",
      " ...\n",
      " [ 0.31441125]\n",
      " [ 0.13598827]\n",
      " [-0.68663069]]\n",
      "Current iteration=6, loss=40394.615987149184\n",
      "t [[ 0.14006827]\n",
      " [-1.15285391]\n",
      " [-0.65512521]\n",
      " ...\n",
      " [ 0.3251355 ]\n",
      " [ 0.14006827]\n",
      " [-0.7497063 ]]\n",
      "t [[ 0.14006827]\n",
      " [-1.15285391]\n",
      " [-0.65512521]\n",
      " ...\n",
      " [ 0.3251355 ]\n",
      " [ 0.14006827]\n",
      " [-0.7497063 ]]\n",
      "t [[ 0.14169495]\n",
      " [-1.26603081]\n",
      " [-0.72047559]\n",
      " ...\n",
      " [ 0.33123742]\n",
      " [ 0.14169495]\n",
      " [-0.80669311]]\n",
      "t [[ 0.14169495]\n",
      " [-1.26603081]\n",
      " [-0.72047559]\n",
      " ...\n",
      " [ 0.33123742]\n",
      " [ 0.14169495]\n",
      " [-0.80669311]]\n",
      "Current iteration=8, loss=38543.31454658446\n",
      "t [[ 0.14140045]\n",
      " [-1.3704055 ]\n",
      " [-0.78157723]\n",
      " ...\n",
      " [ 0.33383477]\n",
      " [ 0.14140045]\n",
      " [-0.85880708]]\n",
      "t [[ 0.14140045]\n",
      " [-1.3704055 ]\n",
      " [-0.78157723]\n",
      " ...\n",
      " [ 0.33383477]\n",
      " [ 0.14140045]\n",
      " [-0.85880708]]\n",
      "t [[ 0.13959825]\n",
      " [-1.46696401]\n",
      " [-0.83892123]\n",
      " ...\n",
      " [ 0.33374567]\n",
      " [ 0.13959825]\n",
      " [-0.90692297]]\n",
      "loss=37126.49733860947\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.13519067]\n",
      " [-0.07881074]\n",
      " [-0.12612934]\n",
      " ...\n",
      " [ 0.10592624]\n",
      " [ 0.04459266]\n",
      " [-0.1794705 ]]\n",
      "t [[ 0.13519067]\n",
      " [-0.07881074]\n",
      " [-0.12612934]\n",
      " ...\n",
      " [ 0.10592624]\n",
      " [ 0.04459266]\n",
      " [-0.1794705 ]]\n",
      "t [[ 0.24844035]\n",
      " [-0.17380034]\n",
      " [-0.23762578]\n",
      " ...\n",
      " [ 0.17987293]\n",
      " [ 0.07702423]\n",
      " [-0.31913984]]\n",
      "t [[ 0.24844035]\n",
      " [-0.17380034]\n",
      " [-0.23762578]\n",
      " ...\n",
      " [ 0.17987293]\n",
      " [ 0.07702423]\n",
      " [-0.31913984]]\n",
      "Current iteration=2, loss=46463.490300403384\n",
      "t [[ 0.34390377]\n",
      " [-0.27586104]\n",
      " [-0.33740005]\n",
      " ...\n",
      " [ 0.23145457]\n",
      " [ 0.10028053]\n",
      " [-0.43121391]]\n",
      "t [[ 0.34390377]\n",
      " [-0.27586104]\n",
      " [-0.33740005]\n",
      " ...\n",
      " [ 0.23145457]\n",
      " [ 0.10028053]\n",
      " [-0.43121391]]\n",
      "t [[ 0.42498344]\n",
      " [-0.37942775]\n",
      " [-0.42765771]\n",
      " ...\n",
      " [ 0.26722962]\n",
      " [ 0.11660149]\n",
      " [-0.52395367]]\n",
      "t [[ 0.42498344]\n",
      " [-0.37942775]\n",
      " [-0.42765771]\n",
      " ...\n",
      " [ 0.26722962]\n",
      " [ 0.11660149]\n",
      " [-0.52395367]]\n",
      "Current iteration=4, loss=42890.57353511754\n",
      "t [[ 0.4943621 ]\n",
      " [-0.48131234]\n",
      " [-0.51005589]\n",
      " ...\n",
      " [ 0.29167689]\n",
      " [ 0.12764061]\n",
      " [-0.60286507]]\n",
      "t [[ 0.4943621 ]\n",
      " [-0.48131234]\n",
      " [-0.51005589]\n",
      " ...\n",
      " [ 0.29167689]\n",
      " [ 0.12764061]\n",
      " [-0.60286507]]\n",
      "t [[ 0.55413951]\n",
      " [-0.57980141]\n",
      " [-0.5858522 ]\n",
      " ...\n",
      " [ 0.30789469]\n",
      " [ 0.1346227 ]\n",
      " [-0.67163343]]\n",
      "t [[ 0.55413951]\n",
      " [-0.57980141]\n",
      " [-0.5858522 ]\n",
      " ...\n",
      " [ 0.30789469]\n",
      " [ 0.1346227 ]\n",
      " [-0.67163343]]\n",
      "Current iteration=6, loss=40381.65244297309\n",
      "t [[ 0.60597052]\n",
      " [-0.6740608 ]\n",
      " [-0.65601663]\n",
      " ...\n",
      " [ 0.3180499 ]\n",
      " [ 0.13846448]\n",
      " [-0.73275372]]\n",
      "t [[ 0.60597052]\n",
      " [-0.6740608 ]\n",
      " [-0.65601663]\n",
      " ...\n",
      " [ 0.3180499 ]\n",
      " [ 0.13846448]\n",
      " [-0.73275372]]\n",
      "t [[ 0.65117251]\n",
      " [-0.76376882]\n",
      " [-0.72131091]\n",
      " ...\n",
      " [ 0.32367716]\n",
      " [ 0.13986071]\n",
      " [-0.78794225]]\n",
      "t [[ 0.65117251]\n",
      " [-0.76376882]\n",
      " [-0.72131091]\n",
      " ...\n",
      " [ 0.32367716]\n",
      " [ 0.13986071]\n",
      " [-0.78794225]]\n",
      "Current iteration=8, loss=38530.56019238812\n",
      "t [[ 0.69080481]\n",
      " [-0.84889585]\n",
      " [-0.78234401]\n",
      " ...\n",
      " [ 0.32587965]\n",
      " [ 0.13934459]\n",
      " [-0.83840255]]\n",
      "t [[ 0.69080481]\n",
      " [-0.84889585]\n",
      " [-0.78234401]\n",
      " ...\n",
      " [ 0.32587965]\n",
      " [ 0.13934459]\n",
      " [-0.83840255]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.72572724]\n",
      " [-0.92957321]\n",
      " [-0.83961104]\n",
      " ...\n",
      " [ 0.32546329]\n",
      " [ 0.13733015]\n",
      " [-0.88499662]]\n",
      "loss=37113.972741060905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.13345793]\n",
      " [-0.07544453]\n",
      " [-0.12565697]\n",
      " ...\n",
      " [ 0.10756054]\n",
      " [ 0.04541454]\n",
      " [-0.1849973 ]]\n",
      "t [[ 0.13345793]\n",
      " [-0.07544453]\n",
      " [-0.12565697]\n",
      " ...\n",
      " [ 0.10756054]\n",
      " [ 0.04541454]\n",
      " [-0.1849973 ]]\n",
      "t [[ 0.24526572]\n",
      " [-0.16714146]\n",
      " [-0.23668205]\n",
      " ...\n",
      " [ 0.18227592]\n",
      " [ 0.07852234]\n",
      " [-0.32899614]]\n",
      "t [[ 0.24526572]\n",
      " [-0.16714146]\n",
      " [-0.23668205]\n",
      " ...\n",
      " [ 0.18227592]\n",
      " [ 0.07852234]\n",
      " [-0.32899614]]\n",
      "Current iteration=2, loss=46508.695189733044\n",
      "t [[ 0.33953097]\n",
      " [-0.26594341]\n",
      " [-0.33600441]\n",
      " ...\n",
      " [ 0.2340526 ]\n",
      " [ 0.1023501 ]\n",
      " [-0.44454243]]\n",
      "t [[ 0.33953097]\n",
      " [-0.26594341]\n",
      " [-0.33600441]\n",
      " ...\n",
      " [ 0.2340526 ]\n",
      " [ 0.1023501 ]\n",
      " [-0.44454243]]\n",
      "t [[ 0.41961523]\n",
      " [-0.36628177]\n",
      " [-0.42583756]\n",
      " ...\n",
      " [ 0.26964192]\n",
      " [ 0.11916552]\n",
      " [-0.54013105]]\n",
      "t [[ 0.41961523]\n",
      " [-0.36628177]\n",
      " [-0.42583756]\n",
      " ...\n",
      " [ 0.26964192]\n",
      " [ 0.11916552]\n",
      " [-0.54013105]]\n",
      "Current iteration=4, loss=42976.31951625628\n",
      "t [[ 0.48816937]\n",
      " [-0.46497612]\n",
      " [-0.50784005]\n",
      " ...\n",
      " [ 0.29363589]\n",
      " [ 0.13064038]\n",
      " [-0.62142224]]\n",
      "t [[ 0.48816937]\n",
      " [-0.46497612]\n",
      " [-0.50784005]\n",
      " ...\n",
      " [ 0.29363589]\n",
      " [ 0.13064038]\n",
      " [-0.62142224]]\n",
      "t [[ 0.54726561]\n",
      " [-0.56032443]\n",
      " [-0.58326795]\n",
      " ...\n",
      " [ 0.30920595]\n",
      " [ 0.13801174]\n",
      " [-0.69220757]]\n",
      "t [[ 0.54726561]\n",
      " [-0.56032443]\n",
      " [-0.58326795]\n",
      " ...\n",
      " [ 0.30920595]\n",
      " [ 0.13801174]\n",
      " [-0.69220757]]\n",
      "Current iteration=6, loss=40499.68692304009\n",
      "t [[ 0.59853437]\n",
      " [-0.65150376]\n",
      " [-0.6530886 ]\n",
      " ...\n",
      " [ 0.318571  ]\n",
      " [ 0.14220486]\n",
      " [-0.75505829]]\n",
      "t [[ 0.59853437]\n",
      " [-0.65150376]\n",
      " [-0.6530886 ]\n",
      " ...\n",
      " [ 0.318571  ]\n",
      " [ 0.14220486]\n",
      " [-0.75505829]]\n",
      "t [[ 0.64327204]\n",
      " [-0.73820147]\n",
      " [-0.71806083]\n",
      " ...\n",
      " [ 0.323304  ]\n",
      " [ 0.14392072]\n",
      " [-0.81174698]]\n",
      "t [[ 0.64327204]\n",
      " [-0.73820147]\n",
      " [-0.71806083]\n",
      " ...\n",
      " [ 0.323304  ]\n",
      " [ 0.14392072]\n",
      " [-0.81174698]]\n",
      "Current iteration=8, loss=38674.08171287767\n",
      "t [[ 0.68252036]\n",
      " [-0.82039439]\n",
      " [-0.77879088]\n",
      " ...\n",
      " [ 0.32453662]\n",
      " [ 0.14369721]\n",
      " [-0.86351942]]\n",
      "t [[ 0.68252036]\n",
      " [-0.82039439]\n",
      " [-0.77879088]\n",
      " ...\n",
      " [ 0.32453662]\n",
      " [ 0.14369721]\n",
      " [-0.86351942]]\n",
      "t [[ 0.71712465]\n",
      " [-0.89821786]\n",
      " [-0.83577132]\n",
      " ...\n",
      " [ 0.32309616]\n",
      " [ 0.14195205]\n",
      " [-0.91126992]]\n",
      "loss=37277.88051697582\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.1332443 ]\n",
      " [-0.07926201]\n",
      " [-0.12526227]\n",
      " ...\n",
      " [ 0.19191369]\n",
      " [-0.12526227]\n",
      " [ 0.17975436]]\n",
      "t [[ 0.1332443 ]\n",
      " [-0.07926201]\n",
      " [-0.12526227]\n",
      " ...\n",
      " [ 0.19191369]\n",
      " [-0.12526227]\n",
      " [ 0.17975436]]\n",
      "t [[ 0.24487221]\n",
      " [-0.17389341]\n",
      " [-0.23601929]\n",
      " ...\n",
      " [ 0.34883215]\n",
      " [-0.23601929]\n",
      " [ 0.32106578]]\n",
      "t [[ 0.24487221]\n",
      " [-0.17389341]\n",
      " [-0.23601929]\n",
      " ...\n",
      " [ 0.34883215]\n",
      " [-0.23601929]\n",
      " [ 0.32106578]]\n",
      "Current iteration=2, loss=46513.62390442814\n",
      "t [[ 0.33899406]\n",
      " [-0.27505555]\n",
      " [-0.3351488 ]\n",
      " ...\n",
      " [ 0.47847543]\n",
      " [-0.3351488 ]\n",
      " [ 0.43356367]]\n",
      "t [[ 0.33899406]\n",
      " [-0.27505555]\n",
      " [-0.3351488 ]\n",
      " ...\n",
      " [ 0.47847543]\n",
      " [-0.3351488 ]\n",
      " [ 0.43356367]]\n",
      "t [[ 0.4189281 ]\n",
      " [-0.37740905]\n",
      " [-0.42483477]\n",
      " ...\n",
      " [ 0.58673503]\n",
      " [-0.42483477]\n",
      " [ 0.52431352]]\n",
      "t [[ 0.4189281 ]\n",
      " [-0.37740905]\n",
      " [-0.42483477]\n",
      " ...\n",
      " [ 0.58673503]\n",
      " [-0.42483477]\n",
      " [ 0.52431352]]\n",
      "Current iteration=4, loss=42975.01567879683\n",
      "t [[ 0.48729408]\n",
      " [-0.47791732]\n",
      " [-0.5067184 ]\n",
      " ...\n",
      " [ 0.67804112]\n",
      " [-0.5067184 ]\n",
      " [ 0.59842647]]\n",
      "t [[ 0.48729408]\n",
      " [-0.47791732]\n",
      " [-0.5067184 ]\n",
      " ...\n",
      " [ 0.67804112]\n",
      " [-0.5067184 ]\n",
      " [ 0.59842647]]\n",
      "t [[ 0.54615578]\n",
      " [-0.57495758]\n",
      " [-0.58204504]\n",
      " ...\n",
      " [ 0.75574034]\n",
      " [-0.58204504]\n",
      " [ 0.65962289]]\n",
      "t [[ 0.54615578]\n",
      " [-0.57495758]\n",
      " [-0.58204504]\n",
      " ...\n",
      " [ 0.75574034]\n",
      " [-0.58204504]\n",
      " [ 0.65962289]]\n",
      "Current iteration=6, loss=40491.413581157874\n",
      "t [[ 0.5971473 ]\n",
      " [-0.66774839]\n",
      " [-0.65177545]\n",
      " ...\n",
      " [ 0.82238938]\n",
      " [-0.65177545]\n",
      " [ 0.71064755]]\n",
      "t [[ 0.5971473 ]\n",
      " [-0.66774839]\n",
      " [-0.65177545]\n",
      " ...\n",
      " [ 0.82238938]\n",
      " [-0.65177545]\n",
      " [ 0.71064755]]\n",
      "t [[ 0.6415727 ]\n",
      " [-0.75599852]\n",
      " [-0.71666453]\n",
      " ...\n",
      " [ 0.87996733]\n",
      " [-0.71666453]\n",
      " [ 0.75355744]]\n",
      "t [[ 0.6415727 ]\n",
      " [-0.75599852]\n",
      " [-0.71666453]\n",
      " ...\n",
      " [ 0.87996733]\n",
      " [-0.71666453]\n",
      " [ 0.75355744]]\n",
      "Current iteration=8, loss=38659.82947798873\n",
      "t [[ 0.68048166]\n",
      " [-0.83969564]\n",
      " [-0.77731617]\n",
      " ...\n",
      " [ 0.93002669]\n",
      " [-0.77731617]\n",
      " [ 0.78991867]]\n",
      "t [[ 0.68048166]\n",
      " [-0.83969564]\n",
      " [-0.77731617]\n",
      " ...\n",
      " [ 0.93002669]\n",
      " [-0.77731617]\n",
      " [ 0.78991867]]\n",
      "t [[ 0.71472652]\n",
      " [-0.91898044]\n",
      " [-0.83422168]\n",
      " ...\n",
      " [ 0.9738008 ]\n",
      " [-0.83422168]\n",
      " [ 0.82094163]]\n",
      "loss=37258.68992323652\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04663378]\n",
      " [-0.22330573]\n",
      " [-0.13094905]\n",
      " ...\n",
      " [ 0.11197882]\n",
      " [ 0.04663378]\n",
      " [-0.1898171 ]]\n",
      "t [[ 0.04663378]\n",
      " [-0.22330573]\n",
      " [-0.13094905]\n",
      " ...\n",
      " [ 0.11197882]\n",
      " [ 0.04663378]\n",
      " [-0.1898171 ]]\n",
      "t [[ 0.08011981]\n",
      " [-0.42578311]\n",
      " [-0.24619233]\n",
      " ...\n",
      " [ 0.1890202 ]\n",
      " [ 0.08011981]\n",
      " [-0.33643814]]\n",
      "t [[ 0.08011981]\n",
      " [-0.42578311]\n",
      " [-0.24619233]\n",
      " ...\n",
      " [ 0.1890202 ]\n",
      " [ 0.08011981]\n",
      " [-0.33643814]]\n",
      "Current iteration=2, loss=46285.18314799472\n",
      "t [[ 0.10381461]\n",
      " [-0.60907516]\n",
      " [-0.34897294]\n",
      " ...\n",
      " [ 0.24202099]\n",
      " [ 0.10381461]\n",
      " [-0.45356437]]\n",
      "t [[ 0.10381461]\n",
      " [-0.60907516]\n",
      " [-0.34897294]\n",
      " ...\n",
      " [ 0.24202099]\n",
      " [ 0.10381461]\n",
      " [-0.45356437]]\n",
      "t [[ 0.12019633]\n",
      " [-0.77512239]\n",
      " [-0.44170996]\n",
      " ...\n",
      " [ 0.27826951]\n",
      " [ 0.12019633]\n",
      " [-0.55028054]]\n",
      "t [[ 0.12019633]\n",
      " [-0.77512239]\n",
      " [-0.44170996]\n",
      " ...\n",
      " [ 0.27826951]\n",
      " [ 0.12019633]\n",
      " [-0.55028054]]\n",
      "Current iteration=4, loss=42649.0576892088\n",
      "t [[ 0.1310676 ]\n",
      " [-0.9258711 ]\n",
      " [-0.52619688]\n",
      " ...\n",
      " [ 0.30265718]\n",
      " [ 0.1310676 ]\n",
      " [-0.63252784]]\n",
      "t [[ 0.1310676 ]\n",
      " [-0.9258711 ]\n",
      " [-0.52619688]\n",
      " ...\n",
      " [ 0.30265718]\n",
      " [ 0.1310676 ]\n",
      " [-0.63252784]]\n",
      "t [[ 0.13774684]\n",
      " [-1.06313336]\n",
      " [-0.60378066]\n",
      " ...\n",
      " [ 0.31851707]\n",
      " [ 0.13774684]\n",
      " [-0.70421863]]\n",
      "t [[ 0.13774684]\n",
      " [-1.06313336]\n",
      " [-0.60378066]\n",
      " ...\n",
      " [ 0.31851707]\n",
      " [ 0.13774684]\n",
      " [-0.70421863]]\n",
      "Current iteration=6, loss=40121.4767513959\n",
      "t [[ 0.14121028]\n",
      " [-1.1885332 ]\n",
      " [-0.67549211]\n",
      " ...\n",
      " [ 0.32814949]\n",
      " [ 0.14121028]\n",
      " [-0.76796633]]\n",
      "t [[ 0.14121028]\n",
      " [-1.1885332 ]\n",
      " [-0.67549211]\n",
      " ...\n",
      " [ 0.32814949]\n",
      " [ 0.14121028]\n",
      " [-0.76796633]]\n",
      "t [[ 0.14219093]\n",
      " [-1.30349553]\n",
      " [-0.74213613]\n",
      " ...\n",
      " [ 0.33316539]\n",
      " [ 0.14219093]\n",
      " [-0.82555045]]\n",
      "t [[ 0.14219093]\n",
      " [-1.30349553]\n",
      " [-0.74213613]\n",
      " ...\n",
      " [ 0.33316539]\n",
      " [ 0.14219093]\n",
      " [-0.82555045]]\n",
      "Current iteration=8, loss=38270.88777669312\n",
      "t [[ 0.14124686]\n",
      " [-1.40925594]\n",
      " [-0.80435363]\n",
      " ...\n",
      " [ 0.33471233]\n",
      " [ 0.14124686]\n",
      " [-0.87821011]]\n",
      "t [[ 0.14124686]\n",
      " [-1.40925594]\n",
      " [-0.80435363]\n",
      " ...\n",
      " [ 0.33471233]\n",
      " [ 0.14124686]\n",
      " [-0.87821011]]\n",
      "t [[ 0.13880832]\n",
      " [-1.50687978]\n",
      " [-0.86266418]\n",
      " ...\n",
      " [ 0.33362252]\n",
      " [ 0.13880832]\n",
      " [-0.92682939]]\n",
      "loss=36863.21889912531\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14082361]\n",
      " [-0.08209452]\n",
      " [-0.13138473]\n",
      " ...\n",
      " [ 0.11033984]\n",
      " [ 0.04645069]\n",
      " [-0.18694844]]\n",
      "t [[ 0.14082361]\n",
      " [-0.08209452]\n",
      " [-0.13138473]\n",
      " ...\n",
      " [ 0.11033984]\n",
      " [ 0.04645069]\n",
      " [-0.18694844]]\n",
      "t [[ 0.25784853]\n",
      " [-0.18173666]\n",
      " [-0.24689675]\n",
      " ...\n",
      " [ 0.18599114]\n",
      " [ 0.07971049]\n",
      " [-0.3307255 ]]\n",
      "t [[ 0.25784853]\n",
      " [-0.18173666]\n",
      " [-0.24689675]\n",
      " ...\n",
      " [ 0.18599114]\n",
      " [ 0.07971049]\n",
      " [-0.3307255 ]]\n",
      "Current iteration=2, loss=46274.45325621127\n",
      "t [[ 0.35577916]\n",
      " [-0.28863814]\n",
      " [-0.3498297 ]\n",
      " ...\n",
      " [ 0.23782017]\n",
      " [ 0.1031569 ]\n",
      " [-0.44512943]]\n",
      "t [[ 0.35577916]\n",
      " [-0.28863814]\n",
      " [-0.3498297 ]\n",
      " ...\n",
      " [ 0.23782017]\n",
      " [ 0.1031569 ]\n",
      " [-0.44512943]]\n",
      "t [[ 0.43841649]\n",
      " [-0.39669167]\n",
      " [-0.44263903]\n",
      " ...\n",
      " [ 0.27308581]\n",
      " [ 0.11928141]\n",
      " [-0.53929352]]\n",
      "t [[ 0.43841649]\n",
      " [-0.39669167]\n",
      " [-0.44263903]\n",
      " ...\n",
      " [ 0.27308581]\n",
      " [ 0.11928141]\n",
      " [-0.53929352]]\n",
      "Current iteration=4, loss=42636.25300879593\n",
      "t [[ 0.50871723]\n",
      " [-0.50251713]\n",
      " [-0.52714337]\n",
      " ...\n",
      " [ 0.29665355]\n",
      " [ 0.12989475]\n",
      " [-0.61917725]]\n",
      "t [[ 0.50871723]\n",
      " [-0.50251713]\n",
      " [-0.52714337]\n",
      " ...\n",
      " [ 0.29665355]\n",
      " [ 0.12989475]\n",
      " [-0.61917725]]\n",
      "t [[ 0.56896989]\n",
      " [-0.60437555]\n",
      " [-0.60470689]\n",
      " ...\n",
      " [ 0.31183207]\n",
      " [ 0.13632024]\n",
      " [-0.68869351]]\n",
      "t [[ 0.56896989]\n",
      " [-0.60437555]\n",
      " [-0.60470689]\n",
      " ...\n",
      " [ 0.31183207]\n",
      " [ 0.13632024]\n",
      " [-0.68869351]]\n",
      "Current iteration=6, loss=40108.524706540185\n",
      "t [[ 0.6209615 ]\n",
      " [-0.70147693]\n",
      " [-0.67637228]\n",
      " ...\n",
      " [ 0.32089976]\n",
      " [ 0.13953706]\n",
      " [-0.75044704]]\n",
      "t [[ 0.6209615 ]\n",
      " [-0.70147693]\n",
      " [-0.67637228]\n",
      " ...\n",
      " [ 0.32089976]\n",
      " [ 0.13953706]\n",
      " [-0.75044704]]\n",
      "t [[ 0.66610339]\n",
      " [-0.793567  ]\n",
      " [-0.74295265]\n",
      " ...\n",
      " [ 0.32544885]\n",
      " [ 0.14028   ]\n",
      " [-0.80620436]]\n",
      "t [[ 0.66610339]\n",
      " [-0.793567  ]\n",
      " [-0.74295265]\n",
      " ...\n",
      " [ 0.32544885]\n",
      " [ 0.14028   ]\n",
      " [-0.80620436]]\n",
      "Current iteration=8, loss=38258.17832096909\n",
      "t [[ 0.70552242]\n",
      " [-0.88068592]\n",
      " [-0.80509466]\n",
      " ...\n",
      " [ 0.32661112]\n",
      " [ 0.13910808]\n",
      " [-0.85719018]]\n",
      "t [[ 0.70552242]\n",
      " [-0.88068592]\n",
      " [-0.80509466]\n",
      " ...\n",
      " [ 0.32661112]\n",
      " [ 0.13910808]\n",
      " [-0.85719018]]\n",
      "t [[ 0.74012741]\n",
      " [-0.96302866]\n",
      " [-0.86332193]\n",
      " ...\n",
      " [ 0.32520578]\n",
      " [ 0.13645206]\n",
      " [-0.90427416]]\n",
      "loss=36850.73675566114\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.13901868]\n",
      " [-0.07858805]\n",
      " [-0.13089268]\n",
      " ...\n",
      " [ 0.11204223]\n",
      " [ 0.04730682]\n",
      " [-0.19270552]]\n",
      "t [[ 0.13901868]\n",
      " [-0.07858805]\n",
      " [-0.13089268]\n",
      " ...\n",
      " [ 0.11204223]\n",
      " [ 0.04730682]\n",
      " [-0.19270552]]\n",
      "t [[ 0.25455407]\n",
      " [-0.1748035 ]\n",
      " [-0.24591383]\n",
      " ...\n",
      " [ 0.18845784]\n",
      " [ 0.08126484]\n",
      " [-0.34094159]]\n",
      "t [[ 0.25455407]\n",
      " [-0.1748035 ]\n",
      " [-0.24591383]\n",
      " ...\n",
      " [ 0.18845784]\n",
      " [ 0.08126484]\n",
      " [-0.34094159]]\n",
      "Current iteration=2, loss=46321.537469695\n",
      "t [[ 0.35125717]\n",
      " [-0.27831348]\n",
      " [-0.34837787]\n",
      " ...\n",
      " [ 0.24044302]\n",
      " [ 0.10529776]\n",
      " [-0.45889085]]\n",
      "t [[ 0.35125717]\n",
      " [-0.27831348]\n",
      " [-0.34837787]\n",
      " ...\n",
      " [ 0.24044302]\n",
      " [ 0.10529776]\n",
      " [-0.45889085]]\n",
      "t [[ 0.4328834 ]\n",
      " [-0.38300797]\n",
      " [-0.44074861]\n",
      " ...\n",
      " [ 0.27546753]\n",
      " [ 0.12192769]\n",
      " [-0.55594449]]\n",
      "t [[ 0.4328834 ]\n",
      " [-0.38300797]\n",
      " [-0.44074861]\n",
      " ...\n",
      " [ 0.27546753]\n",
      " [ 0.12192769]\n",
      " [-0.55594449]]\n",
      "Current iteration=4, loss=42725.08857334849\n",
      "t [[ 0.50235411]\n",
      " [-0.48551663]\n",
      " [-0.52484566]\n",
      " ...\n",
      " [ 0.2985179 ]\n",
      " [ 0.13298506]\n",
      " [-0.63822888]]\n",
      "t [[ 0.50235411]\n",
      " [-0.48551663]\n",
      " [-0.52484566]\n",
      " ...\n",
      " [ 0.2985179 ]\n",
      " [ 0.13298506]\n",
      " [-0.63822888]]\n",
      "t [[ 0.56192716]\n",
      " [-0.58411385]\n",
      " [-0.60203111]\n",
      " ...\n",
      " [ 0.31298074]\n",
      " [ 0.13980625]\n",
      " [-0.70977053]]\n",
      "t [[ 0.56192716]\n",
      " [-0.58411385]\n",
      " [-0.60203111]\n",
      " ...\n",
      " [ 0.31298074]\n",
      " [ 0.13980625]\n",
      " [-0.70977053]]\n",
      "Current iteration=6, loss=40230.20092307094\n",
      "t [[ 0.61336264]\n",
      " [-0.67802219]\n",
      " [-0.67334451]\n",
      " ...\n",
      " [ 0.32118954]\n",
      " [ 0.14337941]\n",
      " [-0.77325498]]\n",
      "t [[ 0.61336264]\n",
      " [-0.67802219]\n",
      " [-0.67334451]\n",
      " ...\n",
      " [ 0.32118954]\n",
      " [ 0.14337941]\n",
      " [-0.77325498]]\n",
      "t [[ 0.65804907]\n",
      " [-0.76699709]\n",
      " [-0.73959569]\n",
      " ...\n",
      " [ 0.32477668]\n",
      " [ 0.14444586]\n",
      " [-0.83050782]]\n",
      "t [[ 0.65804907]\n",
      " [-0.76699709]\n",
      " [-0.73959569]\n",
      " ...\n",
      " [ 0.32477668]\n",
      " [ 0.14444586]\n",
      " [-0.83050782]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=8, loss=38405.5410628695\n",
      "t [[ 0.69709444]\n",
      " [-0.85108529]\n",
      " [-0.80142823]\n",
      " ...\n",
      " [ 0.32490352]\n",
      " [ 0.14356956]\n",
      " [-0.88279778]]\n",
      "t [[ 0.69709444]\n",
      " [-0.85108529]\n",
      " [-0.80142823]\n",
      " ...\n",
      " [ 0.32490352]\n",
      " [ 0.14356956]\n",
      " [-0.88279778]]\n",
      "t [[ 0.73139224]\n",
      " [-0.93048555]\n",
      " [-0.85936303]\n",
      " ...\n",
      " [ 0.32241117]\n",
      " [ 0.1411851 ]\n",
      " [-0.93102793]]\n",
      "loss=37018.50535295327\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.13879615]\n",
      " [-0.08256459]\n",
      " [-0.13048153]\n",
      " ...\n",
      " [ 0.1999101 ]\n",
      " [-0.13048153]\n",
      " [ 0.18724413]]\n",
      "t [[ 0.13879615]\n",
      " [-0.08256459]\n",
      " [-0.13048153]\n",
      " ...\n",
      " [ 0.1999101 ]\n",
      " [-0.13048153]\n",
      " [ 0.18724413]]\n",
      "t [[ 0.25414504]\n",
      " [-0.18179941]\n",
      " [-0.24522882]\n",
      " ...\n",
      " [ 0.36186102]\n",
      " [-0.24522882]\n",
      " [ 0.33278987]]\n",
      "t [[ 0.25414504]\n",
      " [-0.18179941]\n",
      " [-0.24522882]\n",
      " ...\n",
      " [ 0.36186102]\n",
      " [-0.24522882]\n",
      " [ 0.33278987]]\n",
      "Current iteration=2, loss=46326.35555341283\n",
      "t [[ 0.35069922]\n",
      " [-0.28772185]\n",
      " [-0.34749824]\n",
      " ...\n",
      " [ 0.49458734]\n",
      " [-0.34749824]\n",
      " [ 0.44752875]]\n",
      "t [[ 0.35069922]\n",
      " [-0.28772185]\n",
      " [-0.34749824]\n",
      " ...\n",
      " [ 0.49458734]\n",
      " [-0.34749824]\n",
      " [ 0.44752875]]\n",
      "t [[ 0.4321648 ]\n",
      " [-0.39447531]\n",
      " [-0.43972133]\n",
      " ...\n",
      " [ 0.60464922]\n",
      " [-0.43972133]\n",
      " [ 0.53931006]]\n",
      "t [[ 0.4321648 ]\n",
      " [-0.39447531]\n",
      " [-0.43972133]\n",
      " ...\n",
      " [ 0.60464922]\n",
      " [-0.43972133]\n",
      " [ 0.53931006]]\n",
      "Current iteration=4, loss=42723.19992960936\n",
      "t [[ 0.50143116]\n",
      " [-0.49884225]\n",
      " [-0.52369916]\n",
      " ...\n",
      " [ 0.69690817]\n",
      " [-0.52369916]\n",
      " [ 0.61371866]]\n",
      "t [[ 0.50143116]\n",
      " [-0.49884225]\n",
      " [-0.52369916]\n",
      " ...\n",
      " [ 0.69690817]\n",
      " [-0.52369916]\n",
      " [ 0.61371866]]\n",
      "t [[ 0.5607496 ]\n",
      " [-0.59917787]\n",
      " [-0.60078267]\n",
      " ...\n",
      " [ 0.77499152]\n",
      " [-0.60078267]\n",
      " [ 0.6747629 ]]\n",
      "t [[ 0.5607496 ]\n",
      " [-0.59917787]\n",
      " [-0.60078267]\n",
      " ...\n",
      " [ 0.77499152]\n",
      " [-0.60078267]\n",
      " [ 0.6747629 ]]\n",
      "Current iteration=6, loss=40221.11222193109\n",
      "t [[ 0.6118859 ]\n",
      " [-0.69474542]\n",
      " [-0.6720047 ]\n",
      " ...\n",
      " [ 0.84164193]\n",
      " [-0.6720047 ]\n",
      " [ 0.72536551]]\n",
      "t [[ 0.6118859 ]\n",
      " [-0.69474542]\n",
      " [-0.6720047 ]\n",
      " ...\n",
      " [ 0.84164193]\n",
      " [-0.6720047 ]\n",
      " [ 0.72536551]]\n",
      "t [[ 0.65623756]\n",
      " [-0.78532047]\n",
      " [-0.73817124]\n",
      " ...\n",
      " [ 0.89896408]\n",
      " [-0.73817124]\n",
      " [ 0.76769546]]\n",
      "t [[ 0.65623756]\n",
      " [-0.78532047]\n",
      " [-0.73817124]\n",
      " ...\n",
      " [ 0.89896408]\n",
      " [-0.73817124]\n",
      " [ 0.76769546]]\n",
      "Current iteration=8, loss=38390.37904828199\n",
      "t [[ 0.69492137]\n",
      " [-0.87095948]\n",
      " [-0.79992366]\n",
      " ...\n",
      " [ 0.94859685]\n",
      " [-0.79992366]\n",
      " [ 0.80339063]]\n",
      "t [[ 0.69492137]\n",
      " [-0.87095948]\n",
      " [-0.79992366]\n",
      " ...\n",
      " [ 0.94859685]\n",
      " [-0.79992366]\n",
      " [ 0.80339063]]\n",
      "t [[ 0.72883823]\n",
      " [-0.95186569]\n",
      " [-0.85778169]\n",
      " ...\n",
      " [ 0.99183397]\n",
      " [-0.85778169]\n",
      " [ 0.83370816]]\n",
      "loss=36998.377375382435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.04849913]\n",
      " [-0.23223796]\n",
      " [-0.13618701]\n",
      " ...\n",
      " [ 0.11645797]\n",
      " [ 0.04849913]\n",
      " [-0.19740978]]\n",
      "t [[ 0.04849913]\n",
      " [-0.23223796]\n",
      " [-0.13618701]\n",
      " ...\n",
      " [ 0.11645797]\n",
      " [ 0.04849913]\n",
      " [-0.19740978]]\n",
      "t [[ 0.08278294]\n",
      " [-0.44195333]\n",
      " [-0.25539218]\n",
      " ...\n",
      " [ 0.19514073]\n",
      " [ 0.08278294]\n",
      " [-0.34811622]]\n",
      "t [[ 0.08278294]\n",
      " [-0.44195333]\n",
      " [-0.25539218]\n",
      " ...\n",
      " [ 0.19514073]\n",
      " [ 0.08278294]\n",
      " [-0.34811622]]\n",
      "Current iteration=2, loss=46099.06558180521\n",
      "t [[ 0.10662966]\n",
      " [-0.63100381]\n",
      " [-0.36126725]\n",
      " ...\n",
      " [ 0.24830374]\n",
      " [ 0.10662966]\n",
      " [-0.46753234]]\n",
      "t [[ 0.10662966]\n",
      " [-0.63100381]\n",
      " [-0.36126725]\n",
      " ...\n",
      " [ 0.24830374]\n",
      " [ 0.10662966]\n",
      " [-0.46753234]]\n",
      "t [[ 0.12278153]\n",
      " [-0.80158399]\n",
      " [-0.45649151]\n",
      " ...\n",
      " [ 0.28397172]\n",
      " [ 0.12278153]\n",
      " [-0.56564955]]\n",
      "t [[ 0.12278153]\n",
      " [-0.80158399]\n",
      " [-0.45649151]\n",
      " ...\n",
      " [ 0.28397172]\n",
      " [ 0.12278153]\n",
      " [-0.56564955]]\n",
      "Current iteration=4, loss=42401.61558336635\n",
      "t [[ 0.13320238]\n",
      " [-0.95587786]\n",
      " [-0.54302321]\n",
      " ...\n",
      " [ 0.3074306 ]\n",
      " [ 0.13320238]\n",
      " [-0.64886424]]\n",
      "t [[ 0.13320238]\n",
      " [-0.95587786]\n",
      " [-0.54302321]\n",
      " ...\n",
      " [ 0.3074306 ]\n",
      " [ 0.13320238]\n",
      " [-0.64886424]]\n",
      "t [[ 0.1393096 ]\n",
      " [-1.09590002]\n",
      " [-0.6223165 ]\n",
      " ...\n",
      " [ 0.32222144]\n",
      " [ 0.1393096 ]\n",
      " [-0.72130914]]\n",
      "t [[ 0.1393096 ]\n",
      " [-1.09590002]\n",
      " [-0.6223165 ]\n",
      " ...\n",
      " [ 0.32222144]\n",
      " [ 0.1393096 ]\n",
      " [-0.72130914]]\n",
      "Current iteration=6, loss=39858.048486978\n",
      "t [[ 0.14214096]\n",
      " [-1.22343919]\n",
      " [-0.69547462]\n",
      " ...\n",
      " [ 0.33075122]\n",
      " [ 0.14214096]\n",
      " [-0.78570004]]\n",
      "t [[ 0.14214096]\n",
      " [-1.22343919]\n",
      " [-0.69547462]\n",
      " ...\n",
      " [ 0.33075122]\n",
      " [ 0.14214096]\n",
      " [-0.78570004]]\n",
      "t [[ 0.14246826]\n",
      " [-1.34005079]\n",
      " [-0.76335344]\n",
      " ...\n",
      " [ 0.33468401]\n",
      " [ 0.14246826]\n",
      " [-0.84386147]]\n",
      "t [[ 0.14246826]\n",
      " [-1.34005079]\n",
      " [-0.76335344]\n",
      " ...\n",
      " [ 0.33468401]\n",
      " [ 0.14246826]\n",
      " [-0.84386147]]\n",
      "Current iteration=8, loss=38009.951833771534\n",
      "t [[ 0.14087414]\n",
      " [-1.44707264]\n",
      " [-0.82663117]\n",
      " ...\n",
      " [ 0.33519266]\n",
      " [ 0.14087414]\n",
      " [-0.89705078]]\n",
      "t [[ 0.14087414]\n",
      " [-1.44707264]\n",
      " [-0.82663117]\n",
      " ...\n",
      " [ 0.33519266]\n",
      " [ 0.14087414]\n",
      " [-0.89705078]]\n",
      "t [[ 0.13780449]\n",
      " [-1.54564973]\n",
      " [-0.88585563]\n",
      " ...\n",
      " [ 0.3331204 ]\n",
      " [ 0.13780449]\n",
      " [-0.94615841]]\n",
      "loss=36612.40901336534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14645656]\n",
      " [-0.0853783 ]\n",
      " [-0.13664012]\n",
      " ...\n",
      " [ 0.11475343]\n",
      " [ 0.04830871]\n",
      " [-0.19442638]]\n",
      "t [[ 0.14645656]\n",
      " [-0.0853783 ]\n",
      " [-0.13664012]\n",
      " ...\n",
      " [ 0.11475343]\n",
      " [ 0.04830871]\n",
      " [-0.19442638]]\n",
      "t [[ 0.26718233]\n",
      " [-0.18972761]\n",
      " [-0.25611794]\n",
      " ...\n",
      " [ 0.19200068]\n",
      " [ 0.08235551]\n",
      " [-0.34217619]]\n",
      "t [[ 0.26718233]\n",
      " [-0.18972761]\n",
      " [-0.25611794]\n",
      " ...\n",
      " [ 0.19200068]\n",
      " [ 0.08235551]\n",
      " [-0.34217619]]\n",
      "Current iteration=2, loss=46088.125228068544\n",
      "t [[ 0.36747778]\n",
      " [-0.30148286]\n",
      " [-0.36214207]\n",
      " ...\n",
      " [ 0.24396281]\n",
      " [ 0.10594213]\n",
      " [-0.45877137]]\n",
      "t [[ 0.36747778]\n",
      " [-0.30148286]\n",
      " [-0.36214207]\n",
      " ...\n",
      " [ 0.24396281]\n",
      " [ 0.10594213]\n",
      " [-0.45877137]]\n",
      "t [[ 0.45156681]\n",
      " [-0.41397975]\n",
      " [-0.45743194]\n",
      " ...\n",
      " [ 0.27863172]\n",
      " [ 0.12182546]\n",
      " [-0.55425556]]\n",
      "t [[ 0.45156681]\n",
      " [-0.41397975]\n",
      " [-0.45743194]\n",
      " ...\n",
      " [ 0.27863172]\n",
      " [ 0.12182546]\n",
      " [-0.55425556]]\n",
      "Current iteration=4, loss=42388.74558072515\n",
      "t [[ 0.52269119]\n",
      " [-0.52365947]\n",
      " [-0.543973  ]\n",
      " ...\n",
      " [ 0.30126429]\n",
      " [ 0.13197796]\n",
      " [-0.635043  ]]\n",
      "t [[ 0.52269119]\n",
      " [-0.52365947]\n",
      " [-0.543973  ]\n",
      " ...\n",
      " [ 0.30126429]\n",
      " [ 0.13197796]\n",
      " [-0.635043  ]]\n",
      "t [[ 0.58333277]\n",
      " [-0.62877489]\n",
      " [-0.62323775]\n",
      " ...\n",
      " [ 0.31537439]\n",
      " [ 0.13782213]\n",
      " [-0.70526486]]\n",
      "t [[ 0.58333277]\n",
      " [-0.62877489]\n",
      " [-0.62323775]\n",
      " ...\n",
      " [ 0.31537439]\n",
      " [ 0.13782213]\n",
      " [-0.70526486]]\n",
      "Current iteration=6, loss=39845.115015725314\n",
      "t [[ 0.63541216]\n",
      " [-0.72859429]\n",
      " [-0.69634182]\n",
      " ...\n",
      " [ 0.32334494]\n",
      " [ 0.14039876]\n",
      " [-0.76762565]]\n",
      "t [[ 0.63541216]\n",
      " [-0.72859429]\n",
      " [-0.69634182]\n",
      " ...\n",
      " [ 0.32334494]\n",
      " [ 0.14039876]\n",
      " [-0.76762565]]\n",
      "t [[ 0.68043445]\n",
      " [-0.82294046]\n",
      " [-0.76414958]\n",
      " ...\n",
      " [ 0.32681967]\n",
      " [ 0.14048137]\n",
      " [-0.82393458]]\n",
      "t [[ 0.68043445]\n",
      " [-0.82294046]\n",
      " [-0.76414958]\n",
      " ...\n",
      " [ 0.32681967]\n",
      " [ 0.14048137]\n",
      " [-0.82393458]]\n",
      "Current iteration=8, loss=37997.287889085754\n",
      "t [[ 0.71959309]\n",
      " [-0.91192927]\n",
      " [-0.82734506]\n",
      " ...\n",
      " [ 0.32695457]\n",
      " [ 0.13865352]\n",
      " [-0.87543248]]\n",
      "t [[ 0.71959309]\n",
      " [-0.91192927]\n",
      " [-0.82734506]\n",
      " ...\n",
      " [ 0.32695457]\n",
      " [ 0.13865352]\n",
      " [-0.87543248]]\n",
      "t [[ 0.75384469]\n",
      " [-0.99582251]\n",
      " [-0.88648016]\n",
      " ...\n",
      " [ 0.32457904]\n",
      " [ 0.1353615 ]\n",
      " [-0.92299366]]\n",
      "loss=36599.96498976258\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.14457943]\n",
      " [-0.08173157]\n",
      " [-0.13612838]\n",
      " ...\n",
      " [ 0.11652392]\n",
      " [ 0.04919909]\n",
      " [-0.20041374]]\n",
      "t [[ 0.14457943]\n",
      " [-0.08173157]\n",
      " [-0.13612838]\n",
      " ...\n",
      " [ 0.11652392]\n",
      " [ 0.04919909]\n",
      " [-0.20041374]]\n",
      "t [[ 0.26376901]\n",
      " [-0.18252043]\n",
      " [-0.25509586]\n",
      " ...\n",
      " [ 0.19452834]\n",
      " [ 0.08396562]\n",
      " [-0.35274813]]\n",
      "t [[ 0.26376901]\n",
      " [-0.18252043]\n",
      " [-0.25509586]\n",
      " ...\n",
      " [ 0.19452834]\n",
      " [ 0.08396562]\n",
      " [-0.35274813]]\n",
      "Current iteration=2, loss=46137.08331484095\n",
      "t [[ 0.362809  ]\n",
      " [-0.29075152]\n",
      " [-0.3606343 ]\n",
      " ...\n",
      " [ 0.24660513]\n",
      " [ 0.10815328]\n",
      " [-0.47295746]]\n",
      "t [[ 0.362809  ]\n",
      " [-0.29075152]\n",
      " [-0.3606343 ]\n",
      " ...\n",
      " [ 0.24660513]\n",
      " [ 0.10815328]\n",
      " [-0.47295746]]\n",
      "t [[ 0.44587287]\n",
      " [-0.39975893]\n",
      " [-0.45547181]\n",
      " ...\n",
      " [ 0.28097551]\n",
      " [ 0.1245525 ]\n",
      " [-0.57136815]]\n",
      "t [[ 0.44587287]\n",
      " [-0.39975893]\n",
      " [-0.45547181]\n",
      " ...\n",
      " [ 0.28097551]\n",
      " [ 0.1245525 ]\n",
      " [-0.57136815]]\n",
      "Current iteration=4, loss=42480.61932036454\n",
      "t [[ 0.51616328]\n",
      " [-0.50599591]\n",
      " [-0.54159437]\n",
      " ...\n",
      " [ 0.30302526]\n",
      " [ 0.13515691]\n",
      " [-0.65457408]]\n",
      "t [[ 0.51616328]\n",
      " [-0.50599591]\n",
      " [-0.54159437]\n",
      " ...\n",
      " [ 0.30302526]\n",
      " [ 0.13515691]\n",
      " [-0.65457408]]\n",
      "t [[ 0.57612809]\n",
      " [-0.60773089]\n",
      " [-0.62047175]\n",
      " ...\n",
      " [ 0.31635099]\n",
      " [ 0.14140281]\n",
      " [-0.72682736]]\n",
      "t [[ 0.57612809]\n",
      " [-0.60773089]\n",
      " [-0.62047175]\n",
      " ...\n",
      " [ 0.31635099]\n",
      " [ 0.14140281]\n",
      " [-0.72682736]]\n",
      "Current iteration=6, loss=39970.33427601417\n",
      "t [[ 0.62765844]\n",
      " [-0.70424601]\n",
      " [-0.69321595]\n",
      " ...\n",
      " [ 0.32339374]\n",
      " [ 0.14434044]\n",
      " [-0.79091776]]\n",
      "t [[ 0.62765844]\n",
      " [-0.70424601]\n",
      " [-0.69321595]\n",
      " ...\n",
      " [ 0.32339374]\n",
      " [ 0.14434044]\n",
      " [-0.79091776]]\n",
      "t [[ 0.6722348 ]\n",
      " [-0.79537437]\n",
      " [-0.76068763]\n",
      " ...\n",
      " [ 0.32583916]\n",
      " [ 0.14475013]\n",
      " [-0.84871618]]\n",
      "t [[ 0.6722348 ]\n",
      " [-0.79537437]\n",
      " [-0.76068763]\n",
      " ...\n",
      " [ 0.32583916]\n",
      " [ 0.14475013]\n",
      " [-0.84871618]]\n",
      "Current iteration=8, loss=38148.35885630487\n",
      "t [[ 0.71103051]\n",
      " [-0.88123838]\n",
      " [-0.82356745]\n",
      " ...\n",
      " [ 0.32487373]\n",
      " [ 0.14322061]\n",
      " [-0.90150916]]\n",
      "t [[ 0.71103051]\n",
      " [-0.88123838]\n",
      " [-0.82356745]\n",
      " ...\n",
      " [ 0.32487373]\n",
      " [ 0.14322061]\n",
      " [-0.90150916]]\n",
      "t [[ 0.74498604]\n",
      " [-0.96210336]\n",
      " [-0.88240436]\n",
      " ...\n",
      " [ 0.32134922]\n",
      " [ 0.1402022 ]\n",
      " [-0.95020544]]\n",
      "loss=36771.438548724196\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14434799]\n",
      " [-0.08586717]\n",
      " [-0.1357008 ]\n",
      " ...\n",
      " [ 0.2079065 ]\n",
      " [-0.1357008 ]\n",
      " [ 0.19473389]]\n",
      "t [[ 0.14434799]\n",
      " [-0.08586717]\n",
      " [-0.1357008 ]\n",
      " ...\n",
      " [ 0.2079065 ]\n",
      " [-0.1357008 ]\n",
      " [ 0.19473389]]\n",
      "t [[ 0.26334447]\n",
      " [-0.18975741]\n",
      " [-0.25438901]\n",
      " ...\n",
      " [ 0.37477109]\n",
      " [-0.25438901]\n",
      " [ 0.34438355]]\n",
      "t [[ 0.26334447]\n",
      " [-0.18975741]\n",
      " [-0.25438901]\n",
      " ...\n",
      " [ 0.37477109]\n",
      " [-0.25438901]\n",
      " [ 0.34438355]]\n",
      "Current iteration=2, loss=46141.774126530276\n",
      "t [[ 0.36222973]\n",
      " [-0.30045107]\n",
      " [-0.35973136]\n",
      " ...\n",
      " [ 0.51042753]\n",
      " [-0.35973136]\n",
      " [ 0.46120751]]\n",
      "t [[ 0.36222973]\n",
      " [-0.30045107]\n",
      " [-0.35973136]\n",
      " ...\n",
      " [ 0.51042753]\n",
      " [-0.35973136]\n",
      " [ 0.46120751]]\n",
      "t [[ 0.44512173]\n",
      " [-0.41156055]\n",
      " [-0.45442089]\n",
      " ...\n",
      " [ 0.62214261]\n",
      " [-0.45442089]\n",
      " [ 0.55388008]]\n",
      "t [[ 0.44512173]\n",
      " [-0.41156055]\n",
      " [-0.45442089]\n",
      " ...\n",
      " [ 0.62214261]\n",
      " [-0.45442089]\n",
      " [ 0.55388008]]\n",
      "Current iteration=4, loss=42478.14102346526\n",
      "t [[ 0.51519076]\n",
      " [-0.51970033]\n",
      " [-0.54042388]\n",
      " ...\n",
      " [ 0.71522334]\n",
      " [-0.54042388]\n",
      " [ 0.62847151]]\n",
      "t [[ 0.51519076]\n",
      " [-0.51970033]\n",
      " [-0.54042388]\n",
      " ...\n",
      " [ 0.71522334]\n",
      " [-0.54042388]\n",
      " [ 0.62847151]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.57488034]\n",
      " [-0.62322053]\n",
      " [-0.61919855]\n",
      " ...\n",
      " [ 0.79358154]\n",
      " [-0.61919855]\n",
      " [ 0.68927829]]\n",
      "t [[ 0.57488034]\n",
      " [-0.62322053]\n",
      " [-0.61919855]\n",
      " ...\n",
      " [ 0.79358154]\n",
      " [-0.61919855]\n",
      " [ 0.68927829]]\n",
      "Current iteration=6, loss=39960.44385370536\n",
      "t [[ 0.6260893 ]\n",
      " [-0.72144283]\n",
      " [-0.69185016]\n",
      " ...\n",
      " [ 0.86014528]\n",
      " [-0.69185016]\n",
      " [ 0.73939754]]\n",
      "t [[ 0.6260893 ]\n",
      " [-0.72144283]\n",
      " [-0.69185016]\n",
      " ...\n",
      " [ 0.86014528]\n",
      " [-0.69185016]\n",
      " [ 0.73939754]]\n",
      "t [[ 0.67030841]\n",
      " [-0.81421901]\n",
      " [-0.75923561]\n",
      " ...\n",
      " [ 0.91714276]\n",
      " [-0.75923561]\n",
      " [ 0.78110604]]\n",
      "t [[ 0.67030841]\n",
      " [-0.81421901]\n",
      " [-0.75923561]\n",
      " ...\n",
      " [ 0.91714276]\n",
      " [-0.75923561]\n",
      " [ 0.78110604]]\n",
      "Current iteration=8, loss=38132.313020797286\n",
      "t [[ 0.70872054]\n",
      " [-0.90168002]\n",
      " [-0.82203354]\n",
      " ...\n",
      " [ 0.96629682]\n",
      " [-0.82203354]\n",
      " [ 0.81610967]]\n",
      "t [[ 0.70872054]\n",
      " [-0.90168002]\n",
      " [-0.82203354]\n",
      " ...\n",
      " [ 0.96629682]\n",
      " [-0.82203354]\n",
      " [ 0.81610967]]\n",
      "t [[ 0.74227397]\n",
      " [-0.98409479]\n",
      " [-0.88079181]\n",
      " ...\n",
      " [ 1.00895922]\n",
      " [-0.88079181]\n",
      " [ 0.84570908]]\n",
      "loss=36750.40792506648\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05036448]\n",
      " [-0.24117019]\n",
      " [-0.14142497]\n",
      " ...\n",
      " [ 0.12093712]\n",
      " [ 0.05036448]\n",
      " [-0.20500247]]\n",
      "t [[ 0.05036448]\n",
      " [-0.24117019]\n",
      " [-0.14142497]\n",
      " ...\n",
      " [ 0.12093712]\n",
      " [ 0.05036448]\n",
      " [-0.20500247]]\n",
      "t [[ 0.08540505]\n",
      " [-0.45805795]\n",
      " [-0.26454287]\n",
      " ...\n",
      " [ 0.20115207]\n",
      " [ 0.08540505]\n",
      " [-0.35965953]]\n",
      "t [[ 0.08540505]\n",
      " [-0.45805795]\n",
      " [-0.26454287]\n",
      " ...\n",
      " [ 0.20115207]\n",
      " [ 0.08540505]\n",
      " [-0.35965953]]\n",
      "Current iteration=2, loss=45915.59940448812\n",
      "t [[ 0.10935526]\n",
      " [-0.65275407]\n",
      " [-0.37344684]\n",
      " ...\n",
      " [ 0.25436599]\n",
      " [ 0.10935526]\n",
      " [-0.48123128]]\n",
      "t [[ 0.10935526]\n",
      " [-0.65275407]\n",
      " [-0.37344684]\n",
      " ...\n",
      " [ 0.25436599]\n",
      " [ 0.10935526]\n",
      " [-0.48123128]]\n",
      "t [[ 0.12523489]\n",
      " [-0.82772833]\n",
      " [-0.47109002]\n",
      " ...\n",
      " [ 0.28937125]\n",
      " [ 0.12523489]\n",
      " [-0.58065129]]\n",
      "t [[ 0.12523489]\n",
      " [-0.82772833]\n",
      " [-0.47109002]\n",
      " ...\n",
      " [ 0.28937125]\n",
      " [ 0.12523489]\n",
      " [-0.58065129]]\n",
      "Current iteration=4, loss=42160.72828757731\n",
      "t [[ 0.13517299]\n",
      " [-0.98541897]\n",
      " [-0.55960029]\n",
      " ...\n",
      " [ 0.31185116]\n",
      " [ 0.13517299]\n",
      " [-0.66477007]]\n",
      "t [[ 0.13517299]\n",
      " [-0.98541897]\n",
      " [-0.55960029]\n",
      " ...\n",
      " [ 0.31185116]\n",
      " [ 0.13517299]\n",
      " [-0.66477007]]\n",
      "t [[ 0.14068613]\n",
      " [-1.12805406]\n",
      " [-0.64054036]\n",
      " ...\n",
      " [ 0.32554849]\n",
      " [ 0.14068613]\n",
      " [-0.73793031]]\n",
      "t [[ 0.14068613]\n",
      " [-1.12805406]\n",
      " [-0.64054036]\n",
      " ...\n",
      " [ 0.32554849]\n",
      " [ 0.14068613]\n",
      " [-0.73793031]]\n",
      "Current iteration=6, loss=39603.860289885415\n",
      "t [[ 0.14287258]\n",
      " [-1.25759341]\n",
      " [-0.71508605]\n",
      " ...\n",
      " [ 0.3329695 ]\n",
      " [ 0.14287258]\n",
      " [-0.80294011]]\n",
      "t [[ 0.14287258]\n",
      " [-1.25759341]\n",
      " [-0.71508605]\n",
      " ...\n",
      " [ 0.3329695 ]\n",
      " [ 0.14287258]\n",
      " [-0.80294011]]\n",
      "t [[ 0.14254153]\n",
      " [-1.37572616]\n",
      " [-0.784144  ]\n",
      " ...\n",
      " [ 0.33582538]\n",
      " [ 0.14254153]\n",
      " [-0.86166179]]\n",
      "t [[ 0.14254153]\n",
      " [-1.37572616]\n",
      " [-0.784144  ]\n",
      " ...\n",
      " [ 0.33582538]\n",
      " [ 0.14254153]\n",
      " [-0.86166179]]\n",
      "Current iteration=8, loss=37759.84213558206\n",
      "t [[ 0.1402989 ]\n",
      " [-1.48389348]\n",
      " [-0.84842942]\n",
      " ...\n",
      " [ 0.33531002]\n",
      " [ 0.1402989 ]\n",
      " [-0.9153664 ]]\n",
      "t [[ 0.1402989 ]\n",
      " [-1.48389348]\n",
      " [-0.84842942]\n",
      " ...\n",
      " [ 0.33531002]\n",
      " [ 0.1402989 ]\n",
      " [-0.9153664 ]]\n",
      "t [[ 0.13660505]\n",
      " [-1.58332008]\n",
      " [-0.90851819]\n",
      " ...\n",
      " [ 0.3322748 ]\n",
      " [ 0.13660505]\n",
      " [-0.96494817]]\n",
      "loss=36373.25708009602\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.1520895 ]\n",
      " [-0.08866208]\n",
      " [-0.14189551]\n",
      " ...\n",
      " [ 0.11916702]\n",
      " [ 0.05016674]\n",
      " [-0.20190432]]\n",
      "t [[ 0.1520895 ]\n",
      " [-0.08866208]\n",
      " [-0.14189551]\n",
      " ...\n",
      " [ 0.11916702]\n",
      " [ 0.05016674]\n",
      " [-0.20190432]]\n",
      "t [[ 0.27644193]\n",
      " [-0.19777304]\n",
      " [-0.26528945]\n",
      " ...\n",
      " [ 0.1979018 ]\n",
      " [ 0.08495936]\n",
      " [-0.35349222]]\n",
      "t [[ 0.27644193]\n",
      " [-0.19777304]\n",
      " [-0.26528945]\n",
      " ...\n",
      " [ 0.1979018 ]\n",
      " [ 0.08495936]\n",
      " [-0.35349222]]\n",
      "Current iteration=2, loss=45904.46069959242\n",
      "t [[ 0.3790019 ]\n",
      " [-0.31439134]\n",
      " [-0.37433868]\n",
      " ...\n",
      " [ 0.24988696]\n",
      " [ 0.10863772]\n",
      " [-0.47214547]]\n",
      "t [[ 0.3790019 ]\n",
      " [-0.31439134]\n",
      " [-0.37433868]\n",
      " ...\n",
      " [ 0.24988696]\n",
      " [ 0.10863772]\n",
      " [-0.47214547]]\n",
      "t [[ 0.46444056]\n",
      " [-0.43128368]\n",
      " [-0.47204039]\n",
      " ...\n",
      " [ 0.28387844]\n",
      " [ 0.12423752]\n",
      " [-0.56885356]]\n",
      "t [[ 0.46444056]\n",
      " [-0.43128368]\n",
      " [-0.47204039]\n",
      " ...\n",
      " [ 0.28387844]\n",
      " [ 0.12423752]\n",
      " [-0.56885356]]\n",
      "Current iteration=4, loss=42147.805163867175\n",
      "t [[ 0.53629517]\n",
      " [-0.54472825]\n",
      " [-0.56055179]\n",
      " ...\n",
      " [ 0.3055271 ]\n",
      " [ 0.13389697]\n",
      " [-0.65048402]]\n",
      "t [[ 0.53629517]\n",
      " [-0.54472825]\n",
      " [-0.56055179]\n",
      " ...\n",
      " [ 0.3055271 ]\n",
      " [ 0.13389697]\n",
      " [-0.65048402]]\n",
      "t [[ 0.59724497]\n",
      " [-0.65298816]\n",
      " [-0.64145498]\n",
      " ...\n",
      " [ 0.31854562]\n",
      " [ 0.13913797]\n",
      " [-0.72137552]]\n",
      "t [[ 0.59724497]\n",
      " [-0.65298816]\n",
      " [-0.64145498]\n",
      " ...\n",
      " [ 0.31854562]\n",
      " [ 0.13913797]\n",
      " [-0.72137552]]\n",
      "Current iteration=6, loss=39590.95116059354\n",
      "t [[ 0.64934513]\n",
      " [-0.75540402]\n",
      " [-0.71593872]\n",
      " ...\n",
      " [ 0.32541395]\n",
      " [ 0.14106185]\n",
      " [-0.78432211]]\n",
      "t [[ 0.64934513]\n",
      " [-0.75540402]\n",
      " [-0.71593872]\n",
      " ...\n",
      " [ 0.32541395]\n",
      " [ 0.14106185]\n",
      " [-0.78432211]]\n",
      "t [[ 0.69419404]\n",
      " [-0.85188485]\n",
      " [-0.78491833]\n",
      " ...\n",
      " [ 0.32782137]\n",
      " [ 0.14047945]\n",
      " [-0.84116824]]\n",
      "t [[ 0.69419404]\n",
      " [-0.85188485]\n",
      " [-0.78491833]\n",
      " ...\n",
      " [ 0.32782137]\n",
      " [ 0.14047945]\n",
      " [-0.84116824]]\n",
      "Current iteration=8, loss=37747.22339067304\n",
      "t [[ 0.73305062]\n",
      " [-0.94262749]\n",
      " [-0.84911495]\n",
      " ...\n",
      " [ 0.3269438 ]\n",
      " [ 0.13799754]\n",
      " [-0.89316629]]\n",
      "t [[ 0.73305062]\n",
      " [-0.94262749]\n",
      " [-0.84911495]\n",
      " ...\n",
      " [ 0.3269438 ]\n",
      " [ 0.13799754]\n",
      " [-0.89316629]]\n",
      "t [[ 0.76691792]\n",
      " [-1.02796322]\n",
      " [-0.90910848]\n",
      " ...\n",
      " [ 0.32361801]\n",
      " [ 0.13407676]\n",
      " [-0.94119262]]\n",
      "loss=36360.84639566728\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.15014017]\n",
      " [-0.08487509]\n",
      " [-0.14136409]\n",
      " ...\n",
      " [ 0.12100561]\n",
      " [ 0.05109136]\n",
      " [-0.20812196]]\n",
      "t [[ 0.15014017]\n",
      " [-0.08487509]\n",
      " [-0.14136409]\n",
      " ...\n",
      " [ 0.12100561]\n",
      " [ 0.05109136]\n",
      " [-0.20812196]]\n",
      "t [[ 0.27291072]\n",
      " [-0.19029208]\n",
      " [-0.26422822]\n",
      " ...\n",
      " [ 0.20048765]\n",
      " [ 0.08662476]\n",
      " [-0.36441608]]\n",
      "t [[ 0.27291072]\n",
      " [-0.19029208]\n",
      " [-0.26422822]\n",
      " ...\n",
      " [ 0.20048765]\n",
      " [ 0.08662476]\n",
      " [-0.36441608]]\n",
      "Current iteration=2, loss=45955.2867376193\n",
      "t [[ 0.37418873]\n",
      " [-0.30325367]\n",
      " [-0.3727752 ]\n",
      " ...\n",
      " [ 0.25254353]\n",
      " [ 0.11091817]\n",
      " [-0.48674817]]\n",
      "t [[ 0.37418873]\n",
      " [-0.30325367]\n",
      " [-0.3727752 ]\n",
      " ...\n",
      " [ 0.25254353]\n",
      " [ 0.11091817]\n",
      " [-0.48674817]]\n",
      "t [[ 0.45858974]\n",
      " [-0.41652629]\n",
      " [-0.47001114]\n",
      " ...\n",
      " [ 0.28617719]\n",
      " [ 0.12704389]\n",
      " [-0.58641618]]\n",
      "t [[ 0.45858974]\n",
      " [-0.41652629]\n",
      " [-0.47001114]\n",
      " ...\n",
      " [ 0.28617719]\n",
      " [ 0.12704389]\n",
      " [-0.58641618]]\n",
      "Current iteration=4, loss=42242.66579865062\n",
      "t [[ 0.52960789]\n",
      " [-0.52640288]\n",
      " [-0.55809317]\n",
      " ...\n",
      " [ 0.30717634]\n",
      " [ 0.13716273]\n",
      " [-0.67048013]]\n",
      "t [[ 0.52960789]\n",
      " [-0.52640288]\n",
      " [-0.55809317]\n",
      " ...\n",
      " [ 0.30717634]\n",
      " [ 0.13716273]\n",
      " [-0.67048013]]\n",
      "t [[ 0.58988496]\n",
      " [-0.63116434]\n",
      " [-0.63860006]\n",
      " ...\n",
      " [ 0.31934121]\n",
      " [ 0.14281114]\n",
      " [-0.74340693]]\n",
      "t [[ 0.58988496]\n",
      " [-0.63116434]\n",
      " [-0.63860006]\n",
      " ...\n",
      " [ 0.31934121]\n",
      " [ 0.14281114]\n",
      " [-0.74340693]]\n",
      "Current iteration=6, loss=39719.617657234965\n",
      "t [[ 0.64144406]\n",
      " [-0.73016649]\n",
      " [-0.71271633]\n",
      " ...\n",
      " [ 0.32521282]\n",
      " [ 0.14510032]\n",
      " [-0.80808016]]\n",
      "t [[ 0.64144406]\n",
      " [-0.73016649]\n",
      " [-0.71271633]\n",
      " ...\n",
      " [ 0.32521282]\n",
      " [ 0.14510032]\n",
      " [-0.80808016]]\n",
      "t [[ 0.68585712]\n",
      " [-0.82332907]\n",
      " [-0.78135321]\n",
      " ...\n",
      " [ 0.32652398]\n",
      " [ 0.14484829]\n",
      " [-0.86640854]]\n",
      "t [[ 0.68585712]\n",
      " [-0.82332907]\n",
      " [-0.78135321]\n",
      " ...\n",
      " [ 0.32652398]\n",
      " [ 0.14484829]\n",
      " [-0.86640854]]\n",
      "Current iteration=8, loss=37901.87544758065\n",
      "t [[ 0.72436181]\n",
      " [-0.91085539]\n",
      " [-0.84522817]\n",
      " ...\n",
      " [ 0.32448192]\n",
      " [ 0.14266716]\n",
      " [-0.91969169]]\n",
      "t [[ 0.72436181]\n",
      " [-0.91085539]\n",
      " [-0.84522817]\n",
      " ...\n",
      " [ 0.32448192]\n",
      " [ 0.14266716]\n",
      " [-0.91969169]]\n",
      "t [[ 0.75794424]\n",
      " [-0.99307984]\n",
      " [-0.90491793]\n",
      " ...\n",
      " [ 0.31994618]\n",
      " [ 0.13902179]\n",
      " [-0.96884135]]\n",
      "loss=36535.87739764879\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.14989984]\n",
      " [-0.08916976]\n",
      " [-0.14092006]\n",
      " ...\n",
      " [ 0.21590291]\n",
      " [-0.14092006]\n",
      " [ 0.20222366]]\n",
      "t [[ 0.14989984]\n",
      " [-0.08916976]\n",
      " [-0.14092006]\n",
      " ...\n",
      " [ 0.21590291]\n",
      " [-0.14092006]\n",
      " [ 0.20222366]]\n",
      "t [[ 0.27247066]\n",
      " [-0.19776729]\n",
      " [-0.26349995]\n",
      " ...\n",
      " [ 0.38756259]\n",
      " [-0.26349995]\n",
      " [ 0.35584709]]\n",
      "t [[ 0.27247066]\n",
      " [-0.19776729]\n",
      " [-0.26349995]\n",
      " ...\n",
      " [ 0.38756259]\n",
      " [-0.26349995]\n",
      " [ 0.35584709]]\n",
      "Current iteration=2, loss=45959.834498443\n",
      "t [[ 0.37358781]\n",
      " [-0.31323952]\n",
      " [-0.37184966]\n",
      " ...\n",
      " [ 0.52599996]\n",
      " [-0.37184966]\n",
      " [ 0.47460473]]\n",
      "t [[ 0.37358781]\n",
      " [-0.31323952]\n",
      " [-0.37184966]\n",
      " ...\n",
      " [ 0.52599996]\n",
      " [-0.37184966]\n",
      " [ 0.47460473]]\n",
      "t [[ 0.45780494]\n",
      " [-0.42865682]\n",
      " [-0.46893739]\n",
      " ...\n",
      " [ 0.63922562]\n",
      " [-0.46893739]\n",
      " [ 0.56803583]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.45780494]\n",
      " [-0.42865682]\n",
      " [-0.46893739]\n",
      " ...\n",
      " [ 0.63922562]\n",
      " [-0.46893739]\n",
      " [ 0.56803583]]\n",
      "Current iteration=4, loss=42239.59480231095\n",
      "t [[ 0.52858392]\n",
      " [-0.54048095]\n",
      " [-0.55689949]\n",
      " ...\n",
      " [ 0.73300498]\n",
      " [-0.55689949]\n",
      " [ 0.6427058 ]]\n",
      "t [[ 0.52858392]\n",
      " [-0.54048095]\n",
      " [-0.55689949]\n",
      " ...\n",
      " [ 0.73300498]\n",
      " [-0.55689949]\n",
      " [ 0.6427058 ]]\n",
      "t [[ 0.58856465]\n",
      " [-0.64707484]\n",
      " [-0.6373028 ]\n",
      " ...\n",
      " [ 0.81153704]\n",
      " [-0.6373028 ]\n",
      " [ 0.70319822]]\n",
      "t [[ 0.58856465]\n",
      " [-0.64707484]\n",
      " [-0.6373028 ]\n",
      " ...\n",
      " [ 0.81153704]\n",
      " [-0.6373028 ]\n",
      " [ 0.70319822]]\n",
      "Current iteration=6, loss=39708.93978217013\n",
      "t [[ 0.63977997]\n",
      " [-0.74783233]\n",
      " [-0.71132515]\n",
      " ...\n",
      " [ 0.87793417]\n",
      " [-0.71132515]\n",
      " [ 0.75278045]]\n",
      "t [[ 0.63977997]\n",
      " [-0.74783233]\n",
      " [-0.71132515]\n",
      " ...\n",
      " [ 0.87793417]\n",
      " [-0.71132515]\n",
      " [ 0.75278045]]\n",
      "t [[ 0.68381337]\n",
      " [-0.84269026]\n",
      " [-0.77987415]\n",
      " ...\n",
      " [ 0.93454569]\n",
      " [-0.77987415]\n",
      " [ 0.79383265]]\n",
      "t [[ 0.68381337]\n",
      " [-0.84269026]\n",
      " [-0.77987415]\n",
      " ...\n",
      " [ 0.93454569]\n",
      " [-0.77987415]\n",
      " [ 0.79383265]]\n",
      "Current iteration=8, loss=37884.97125936548\n",
      "t [[ 0.7219127 ]\n",
      " [-0.93185925]\n",
      " [-0.84366538]\n",
      " ...\n",
      " [ 0.9831758 ]\n",
      " [-0.84366538]\n",
      " [ 0.82812481]]\n",
      "t [[ 0.7219127 ]\n",
      " [-0.93185925]\n",
      " [-0.84366538]\n",
      " ...\n",
      " [ 0.9831758 ]\n",
      " [-0.84366538]\n",
      " [ 0.82812481]]\n",
      "t [[ 0.75507224]\n",
      " [-1.01567646]\n",
      " [-0.90327464]\n",
      " ...\n",
      " [ 1.02523188]\n",
      " [-0.90327464]\n",
      " [ 0.85699805]]\n",
      "loss=36513.977680084296\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05222983]\n",
      " [-0.25010242]\n",
      " [-0.14666293]\n",
      " ...\n",
      " [ 0.12541628]\n",
      " [ 0.05222983]\n",
      " [-0.21259515]]\n",
      "t [[ 0.05222983]\n",
      " [-0.25010242]\n",
      " [-0.14666293]\n",
      " ...\n",
      " [ 0.12541628]\n",
      " [ 0.05222983]\n",
      " [-0.21259515]]\n",
      "t [[ 0.08798623]\n",
      " [-0.47409708]\n",
      " [-0.27364449]\n",
      " ...\n",
      " [ 0.20705445]\n",
      " [ 0.08798623]\n",
      " [-0.3710684 ]]\n",
      "t [[ 0.08798623]\n",
      " [-0.47409708]\n",
      " [-0.27364449]\n",
      " ...\n",
      " [ 0.20705445]\n",
      " [ 0.08798623]\n",
      " [-0.3710684 ]]\n",
      "Current iteration=2, loss=45734.74013373561\n",
      "t [[ 0.1119929 ]\n",
      " [-0.674327  ]\n",
      " [-0.3855132 ]\n",
      " ...\n",
      " [ 0.26021221]\n",
      " [ 0.1119929 ]\n",
      " [-0.49466685]]\n",
      "t [[ 0.1119929 ]\n",
      " [-0.674327  ]\n",
      " [-0.3855132 ]\n",
      " ...\n",
      " [ 0.26021221]\n",
      " [ 0.1119929 ]\n",
      " [-0.49466685]]\n",
      "t [[ 0.12756022]\n",
      " [-0.85355911]\n",
      " [-0.48550934]\n",
      " ...\n",
      " [ 0.29447896]\n",
      " [ 0.12756022]\n",
      " [-0.59529909]]\n",
      "t [[ 0.12756022]\n",
      " [-0.85355911]\n",
      " [-0.48550934]\n",
      " ...\n",
      " [ 0.29447896]\n",
      " [ 0.12756022]\n",
      " [-0.59529909]]\n",
      "Current iteration=4, loss=41926.16203877263\n",
      "t [[ 0.13698587]\n",
      " [-1.01450253]\n",
      " [-0.57593479]\n",
      " ...\n",
      " [ 0.31593616]\n",
      " [ 0.13698587]\n",
      " [-0.68026597]]\n",
      "t [[ 0.13698587]\n",
      " [-1.01450253]\n",
      " [-0.57593479]\n",
      " ...\n",
      " [ 0.31593616]\n",
      " [ 0.13698587]\n",
      " [-0.68026597]]\n",
      "t [[ 0.14188551]\n",
      " [-1.15960952]\n",
      " [-0.65846191]\n",
      " ...\n",
      " [ 0.32852094]\n",
      " [ 0.14188551]\n",
      " [-0.75410845]]\n",
      "t [[ 0.14188551]\n",
      " [-1.15960952]\n",
      " [-0.65846191]\n",
      " ...\n",
      " [ 0.32852094]\n",
      " [ 0.14188551]\n",
      " [-0.75410845]]\n",
      "Current iteration=6, loss=39358.47010551549\n",
      "t [[ 0.14341665]\n",
      " [-1.29101685]\n",
      " [-0.73433906]\n",
      " ...\n",
      " [ 0.33483108]\n",
      " [ 0.14341665]\n",
      " [-0.81971671]]\n",
      "t [[ 0.14341665]\n",
      " [-1.29101685]\n",
      " [-0.73433906]\n",
      " ...\n",
      " [ 0.33483108]\n",
      " [ 0.14341665]\n",
      " [-0.81971671]]\n",
      "t [[ 0.14242438]\n",
      " [-1.41055014]\n",
      " [-0.80452336]\n",
      " ...\n",
      " [ 0.33661897]\n",
      " [ 0.14242438]\n",
      " [-0.8789839 ]]\n",
      "t [[ 0.14242438]\n",
      " [-1.41055014]\n",
      " [-0.80452336]\n",
      " ...\n",
      " [ 0.33661897]\n",
      " [ 0.14242438]\n",
      " [-0.8789839 ]]\n",
      "Current iteration=8, loss=37519.94268233995\n",
      "t [[ 0.13953655]\n",
      " [-1.51975476]\n",
      " [-0.86976679]\n",
      " ...\n",
      " [ 0.33509553]\n",
      " [ 0.13953655]\n",
      " [-0.93319067]]\n",
      "t [[ 0.13953655]\n",
      " [-1.51975476]\n",
      " [-0.86976679]\n",
      " ...\n",
      " [ 0.33509553]\n",
      " [ 0.13953655]\n",
      " [-0.93319067]]\n",
      "t [[ 0.13522678]\n",
      " [-1.61993488]\n",
      " [-0.930673  ]\n",
      " ...\n",
      " [ 0.33111773]\n",
      " [ 0.13522678]\n",
      " [-0.9832329 ]]\n",
      "loss=36145.019337579964\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.15772245]\n",
      " [-0.09194586]\n",
      " [-0.1471509 ]\n",
      " ...\n",
      " [ 0.12358062]\n",
      " [ 0.05202477]\n",
      " [-0.20938225]]\n",
      "t [[ 0.15772245]\n",
      " [-0.09194586]\n",
      " [-0.1471509 ]\n",
      " ...\n",
      " [ 0.12358062]\n",
      " [ 0.05202477]\n",
      " [-0.20938225]]\n",
      "t [[ 0.28562748]\n",
      " [-0.2058728 ]\n",
      " [-0.27441138]\n",
      " ...\n",
      " [ 0.20369473]\n",
      " [ 0.08752216]\n",
      " [-0.36467388]]\n",
      "t [[ 0.28562748]\n",
      " [-0.2058728 ]\n",
      " [-0.27441138]\n",
      " ...\n",
      " [ 0.20369473]\n",
      " [ 0.08752216]\n",
      " [-0.36467388]]\n",
      "Current iteration=2, loss=45723.41473670116\n",
      "t [[ 0.39035381]\n",
      " [-0.32735978]\n",
      " [-0.38642103]\n",
      " ...\n",
      " [ 0.25559706]\n",
      " [ 0.11124516]\n",
      " [-0.48525744]]\n",
      "t [[ 0.39035381]\n",
      " [-0.32735978]\n",
      " [-0.38642103]\n",
      " ...\n",
      " [ 0.25559706]\n",
      " [ 0.11124516]\n",
      " [-0.48525744]]\n",
      "t [[ 0.47704378]\n",
      " [-0.44859547]\n",
      " [-0.48646829]\n",
      " ...\n",
      " [ 0.28883674]\n",
      " [ 0.1265214 ]\n",
      " [-0.58310089]]\n",
      "t [[ 0.47704378]\n",
      " [-0.44859547]\n",
      " [-0.48646829]\n",
      " ...\n",
      " [ 0.28883674]\n",
      " [ 0.1265214 ]\n",
      " [-0.58310089]]\n",
      "Current iteration=4, loss=41913.19677995751\n",
      "t [[ 0.54954003]\n",
      " [-0.56571315]\n",
      " [-0.57688647]\n",
      " ...\n",
      " [ 0.30945916]\n",
      " [ 0.13565826]\n",
      " [-0.66552096]]\n",
      "t [[ 0.54954003]\n",
      " [-0.56571315]\n",
      " [-0.57688647]\n",
      " ...\n",
      " [ 0.30945916]\n",
      " [ 0.13565826]\n",
      " [-0.66552096]]\n",
      "t [[ 0.61072264]\n",
      " [-0.67700532]\n",
      " [-0.65936836]\n",
      " ...\n",
      " [ 0.32136826]\n",
      " [ 0.14027691]\n",
      " [-0.73705176]]\n",
      "t [[ 0.61072264]\n",
      " [-0.67700532]\n",
      " [-0.65936836]\n",
      " ...\n",
      " [ 0.32136826]\n",
      " [ 0.14027691]\n",
      " [-0.73705176]]\n",
      "Current iteration=6, loss=39345.58994808545\n",
      "t [[ 0.66278199]\n",
      " [-0.78189886]\n",
      " [-0.73517575]\n",
      " ...\n",
      " [ 0.32713326]\n",
      " [ 0.14153788]\n",
      " [-0.80056644]]\n",
      "t [[ 0.66278199]\n",
      " [-0.78189886]\n",
      " [-0.73517575]\n",
      " ...\n",
      " [ 0.32713326]\n",
      " [ 0.14153788]\n",
      " [-0.80056644]]\n",
      "t [[ 0.70740898]\n",
      " [-0.88039755]\n",
      " [-0.80527461]\n",
      " ...\n",
      " [ 0.32848305]\n",
      " [ 0.14028791]\n",
      " [-0.85793756]]\n",
      "t [[ 0.70740898]\n",
      " [-0.88039755]\n",
      " [-0.80527461]\n",
      " ...\n",
      " [ 0.32848305]\n",
      " [ 0.14028791]\n",
      " [-0.85793756]]\n",
      "Current iteration=8, loss=37507.36806543239\n",
      "t [[ 0.74592677]\n",
      " [-0.97278384]\n",
      " [-0.87042288]\n",
      " ...\n",
      " [ 0.32660949]\n",
      " [ 0.13715556]\n",
      " [-0.91042488]]\n",
      "t [[ 0.74592677]\n",
      " [-0.97278384]\n",
      " [-0.87042288]\n",
      " ...\n",
      " [ 0.32660949]\n",
      " [ 0.13715556]\n",
      " [-0.91042488]]\n",
      "t [[ 0.77938339]\n",
      " [-1.05946064]\n",
      " [-0.9312282 ]\n",
      " ...\n",
      " [ 0.32235418]\n",
      " [ 0.13261466]\n",
      " [-0.95890465]]\n",
      "loss=36132.63689949842\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.15570092]\n",
      " [-0.08801862]\n",
      " [-0.1465998 ]\n",
      " ...\n",
      " [ 0.1254873 ]\n",
      " [ 0.05298363]\n",
      " [-0.21583018]]\n",
      "t [[ 0.15570092]\n",
      " [-0.08801862]\n",
      " [-0.1465998 ]\n",
      " ...\n",
      " [ 0.1254873 ]\n",
      " [ 0.05298363]\n",
      " [-0.21583018]]\n",
      "t [[ 0.28197936]\n",
      " [-0.19811832]\n",
      " [-0.27331103]\n",
      " ...\n",
      " [ 0.20633603]\n",
      " [ 0.08924237]\n",
      " [-0.37594577]]\n",
      "t [[ 0.28197936]\n",
      " [-0.19811832]\n",
      " [-0.27331103]\n",
      " ...\n",
      " [ 0.20633603]\n",
      " [ 0.08924237]\n",
      " [-0.37594577]]\n",
      "Current iteration=2, loss=45776.10235108503\n",
      "t [[ 0.38539859]\n",
      " [-0.31581608]\n",
      " [-0.38480209]\n",
      " ...\n",
      " [ 0.2582628 ]\n",
      " [ 0.11359395]\n",
      " [-0.50026885]]\n",
      "t [[ 0.38539859]\n",
      " [-0.31581608]\n",
      " [-0.38480209]\n",
      " ...\n",
      " [ 0.2582628 ]\n",
      " [ 0.11359395]\n",
      " [-0.50026885]]\n",
      "t [[ 0.47103997]\n",
      " [-0.43330206]\n",
      " [-0.48437049]\n",
      " ...\n",
      " [ 0.29108363]\n",
      " [ 0.1294057 ]\n",
      " [-0.60110231]]\n",
      "t [[ 0.47103997]\n",
      " [-0.43330206]\n",
      " [-0.48437049]\n",
      " ...\n",
      " [ 0.29108363]\n",
      " [ 0.1294057 ]\n",
      " [-0.60110231]]\n",
      "Current iteration=4, loss=42010.9932747375\n",
      "t [[ 0.54269866]\n",
      " [-0.54672723]\n",
      " [-0.57434881]\n",
      " ...\n",
      " [ 0.31098873]\n",
      " [ 0.13900905]\n",
      " [-0.68596825]]\n",
      "t [[ 0.54269866]\n",
      " [-0.54672723]\n",
      " [-0.57434881]\n",
      " ...\n",
      " [ 0.31098873]\n",
      " [ 0.13900905]\n",
      " [-0.68596825]]\n",
      "t [[ 0.60321369]\n",
      " [-0.65440427]\n",
      " [-0.65642577]\n",
      " ...\n",
      " [ 0.32197445]\n",
      " [ 0.14404043]\n",
      " [-0.75953626]]\n",
      "t [[ 0.60321369]\n",
      " [-0.65440427]\n",
      " [-0.65642577]\n",
      " ...\n",
      " [ 0.32197445]\n",
      " [ 0.14404043]\n",
      " [-0.75953626]]\n",
      "Current iteration=6, loss=39477.61076127565\n",
      "t [[ 0.65474071]\n",
      " [-0.75577648]\n",
      " [-0.73185837]\n",
      " ...\n",
      " [ 0.3266739 ]\n",
      " [ 0.14567074]\n",
      " [-0.82477315]]\n",
      "t [[ 0.65474071]\n",
      " [-0.75577648]\n",
      " [-0.73185837]\n",
      " ...\n",
      " [ 0.3266739 ]\n",
      " [ 0.14567074]\n",
      " [-0.82477315]]\n",
      "t [[ 0.69894241]\n",
      " [-0.85085873]\n",
      " [-0.80160808]\n",
      " ...\n",
      " [ 0.32686098]\n",
      " [ 0.14475412]\n",
      " [-0.8836182 ]]\n",
      "t [[ 0.69894241]\n",
      " [-0.85085873]\n",
      " [-0.80160808]\n",
      " ...\n",
      " [ 0.32686098]\n",
      " [ 0.14475412]\n",
      " [-0.8836182 ]]\n",
      "Current iteration=8, loss=37665.47967943789\n",
      "t [[ 0.73711956]\n",
      " [-0.93993972]\n",
      " [-0.86642884]\n",
      " ...\n",
      " [ 0.3237596 ]\n",
      " [ 0.14192477]\n",
      " [-0.93737983]]\n",
      "t [[ 0.73711956]\n",
      " [-0.93993972]\n",
      " [-0.86642884]\n",
      " ...\n",
      " [ 0.3237596 ]\n",
      " [ 0.14192477]\n",
      " [-0.93737983]]\n",
      "t [[ 0.77030256]\n",
      " [-1.02342495]\n",
      " [-0.92692494]\n",
      " ...\n",
      " [ 0.3182344 ]\n",
      " [ 0.13766085]\n",
      " [-0.98697056]]\n",
      "loss=36311.085636607386\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.15545168]\n",
      " [-0.09247234]\n",
      " [-0.14613932]\n",
      " ...\n",
      " [ 0.22389931]\n",
      " [-0.14613932]\n",
      " [ 0.20971342]]\n",
      "t [[ 0.15545168]\n",
      " [-0.09247234]\n",
      " [-0.14613932]\n",
      " ...\n",
      " [ 0.22389931]\n",
      " [-0.14613932]\n",
      " [ 0.20971342]]\n",
      "t [[ 0.28152376]\n",
      " [-0.20582891]\n",
      " [-0.27256175]\n",
      " ...\n",
      " [ 0.40023579]\n",
      " [-0.27256175]\n",
      " [ 0.36718081]]\n",
      "t [[ 0.28152376]\n",
      " [-0.20582891]\n",
      " [-0.27256175]\n",
      " ...\n",
      " [ 0.40023579]\n",
      " [-0.27256175]\n",
      " [ 0.36718081]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2, loss=45780.49212201632\n",
      "t [[ 0.38477565]\n",
      " [-0.32608355]\n",
      " [-0.38385462]\n",
      " ...\n",
      " [ 0.5413086 ]\n",
      " [-0.38385462]\n",
      " [ 0.48772518]]\n",
      "t [[ 0.38477565]\n",
      " [-0.32608355]\n",
      " [-0.38385462]\n",
      " ...\n",
      " [ 0.5413086 ]\n",
      " [-0.38385462]\n",
      " [ 0.48772518]]\n",
      "t [[ 0.47022036]\n",
      " [-0.44575644]\n",
      " [-0.48327468]\n",
      " ...\n",
      " [ 0.65590848]\n",
      " [-0.48327468]\n",
      " [ 0.58178925]]\n",
      "t [[ 0.47022036]\n",
      " [-0.44575644]\n",
      " [-0.48327468]\n",
      " ...\n",
      " [ 0.65590848]\n",
      " [-0.48327468]\n",
      " [ 0.58178925]]\n",
      "Current iteration=4, loss=42007.328109727256\n",
      "t [[ 0.54162137]\n",
      " [-0.56117423]\n",
      " [-0.57313265]\n",
      " ...\n",
      " [ 0.75027079]\n",
      " [-0.57313265]\n",
      " [ 0.6564415 ]]\n",
      "t [[ 0.54162137]\n",
      " [-0.56117423]\n",
      " [-0.57313265]\n",
      " ...\n",
      " [ 0.75027079]\n",
      " [-0.57313265]\n",
      " [ 0.6564415 ]]\n",
      "t [[ 0.60181855]\n",
      " [-0.67073129]\n",
      " [-0.65510508]\n",
      " ...\n",
      " [ 0.82888349]\n",
      " [-0.65510508]\n",
      " [ 0.7165504 ]]\n",
      "t [[ 0.60181855]\n",
      " [-0.67073129]\n",
      " [-0.65510508]\n",
      " ...\n",
      " [ 0.82888349]\n",
      " [-0.65510508]\n",
      " [ 0.7165504 ]]\n",
      "Current iteration=6, loss=39466.16012545666\n",
      "t [[ 0.65297929]\n",
      " [-0.77390714]\n",
      " [-0.73044233]\n",
      " ...\n",
      " [ 0.8950415 ]\n",
      " [-0.73044233]\n",
      " [ 0.76554883]]\n",
      "t [[ 0.65297929]\n",
      " [-0.77390714]\n",
      " [-0.73044233]\n",
      " ...\n",
      " [ 0.8950415 ]\n",
      " [-0.73044233]\n",
      " [ 0.76554883]]\n",
      "t [[ 0.69677904]\n",
      " [-0.87073205]\n",
      " [-0.80010243]\n",
      " ...\n",
      " [ 0.95121262]\n",
      " [-0.80010243]\n",
      " [ 0.80591574]]\n",
      "t [[ 0.69677904]\n",
      " [-0.87073205]\n",
      " [-0.80010243]\n",
      " ...\n",
      " [ 0.95121262]\n",
      " [-0.80010243]\n",
      " [ 0.80591574]]\n",
      "Current iteration=8, loss=37647.74205075744\n",
      "t [[ 0.73452934]\n",
      " [-0.96150077]\n",
      " [-0.8648376 ]\n",
      " ...\n",
      " [ 0.99927969]\n",
      " [-0.8648376 ]\n",
      " [ 0.83948141]]\n",
      "t [[ 0.73452934]\n",
      " [-0.96150077]\n",
      " [-0.8648376 ]\n",
      " ...\n",
      " [ 0.99927969]\n",
      " [-0.8648376 ]\n",
      " [ 0.83948141]]\n",
      "t [[ 0.76726904]\n",
      " [-1.04662081]\n",
      " [-0.92525133]\n",
      " ...\n",
      " [ 1.0407033 ]\n",
      " [-0.92525133]\n",
      " [ 0.86762442]]\n",
      "loss=36288.349224996025\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05409518]\n",
      " [-0.25903465]\n",
      " [-0.15190089]\n",
      " ...\n",
      " [ 0.12989543]\n",
      " [ 0.05409518]\n",
      " [-0.22018784]]\n",
      "t [[ 0.05409518]\n",
      " [-0.25903465]\n",
      " [-0.15190089]\n",
      " ...\n",
      " [ 0.12989543]\n",
      " [ 0.05409518]\n",
      " [-0.22018784]]\n",
      "t [[ 0.09052656]\n",
      " [-0.49007082]\n",
      " [-0.28269714]\n",
      " ...\n",
      " [ 0.21284813]\n",
      " [ 0.09052656]\n",
      " [-0.38234313]]\n",
      "t [[ 0.09052656]\n",
      " [-0.49007082]\n",
      " [-0.28269714]\n",
      " ...\n",
      " [ 0.21284813]\n",
      " [ 0.09052656]\n",
      " [-0.38234313]]\n",
      "Current iteration=2, loss=45556.443867705064\n",
      "t [[ 0.11454405]\n",
      " [-0.69572368]\n",
      " [-0.39746781]\n",
      " ...\n",
      " [ 0.26584685]\n",
      " [ 0.11454405]\n",
      " [-0.50784469]]\n",
      "t [[ 0.11454405]\n",
      " [-0.69572368]\n",
      " [-0.39746781]\n",
      " ...\n",
      " [ 0.26584685]\n",
      " [ 0.11454405]\n",
      " [-0.50784469]]\n",
      "t [[ 0.1297612 ]\n",
      " [-0.87908001]\n",
      " [-0.49975322]\n",
      " ...\n",
      " [ 0.29930539]\n",
      " [ 0.1297612 ]\n",
      " [-0.60960589]]\n",
      "t [[ 0.1297612 ]\n",
      " [-0.87908001]\n",
      " [-0.49975322]\n",
      " ...\n",
      " [ 0.29930539]\n",
      " [ 0.1297612 ]\n",
      " [-0.60960589]]\n",
      "Current iteration=4, loss=41697.69350755517\n",
      "t [[ 0.13864725]\n",
      " [-1.04313657]\n",
      " [-0.59203315]\n",
      " ...\n",
      " [ 0.31970214]\n",
      " [ 0.13864725]\n",
      " [-0.69537156]]\n",
      "t [[ 0.13864725]\n",
      " [-1.04313657]\n",
      " [-0.59203315]\n",
      " ...\n",
      " [ 0.31970214]\n",
      " [ 0.13864725]\n",
      " [-0.69537156]]\n",
      "t [[ 0.14291644]\n",
      " [-1.19058013]\n",
      " [-0.67609036]\n",
      " ...\n",
      " [ 0.33116016]\n",
      " [ 0.14291644]\n",
      " [-0.76986819]]\n",
      "t [[ 0.14291644]\n",
      " [-1.19058013]\n",
      " [-0.67609036]\n",
      " ...\n",
      " [ 0.33116016]\n",
      " [ 0.14291644]\n",
      " [-0.76986819]]\n",
      "Current iteration=6, loss=39121.46258875138\n",
      "t [[ 0.14378405]\n",
      " [-1.32372991]\n",
      " [-0.75324563]\n",
      " ...\n",
      " [ 0.33636077]\n",
      " [ 0.14378405]\n",
      " [-0.8360577 ]]\n",
      "t [[ 0.14378405]\n",
      " [-1.32372991]\n",
      " [-0.75324563]\n",
      " ...\n",
      " [ 0.33636077]\n",
      " [ 0.14378405]\n",
      " [-0.8360577 ]]\n",
      "t [[ 0.14212956]\n",
      " [-1.44455028]\n",
      " [-0.82450622]\n",
      " ...\n",
      " [ 0.33709183]\n",
      " [ 0.14212956]\n",
      " [-0.89585749]]\n",
      "t [[ 0.14212956]\n",
      " [-1.44455028]\n",
      " [-0.82450622]\n",
      " ...\n",
      " [ 0.33709183]\n",
      " [ 0.14212956]\n",
      " [-0.89585749]]\n",
      "Current iteration=8, loss=37289.68179214828\n",
      "t [[ 0.13860136]\n",
      " [-1.55469129]\n",
      " [-0.89066058]\n",
      " ...\n",
      " [ 0.33457754]\n",
      " [ 0.13860136]\n",
      " [-0.95055411]]\n",
      "t [[ 0.13860136]\n",
      " [-1.55469129]\n",
      " [-0.89066058]\n",
      " ...\n",
      " [ 0.33457754]\n",
      " [ 0.13860136]\n",
      " [-0.95055411]]\n",
      "t [[ 0.1336852 ]\n",
      " [-1.65553608]\n",
      " [-0.95233989]\n",
      " ...\n",
      " [ 0.32967808]\n",
      " [ 0.1336852 ]\n",
      " [-1.0010434 ]]\n",
      "loss=35927.012258134695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16335539]\n",
      " [-0.09522964]\n",
      " [-0.15240629]\n",
      " ...\n",
      " [ 0.12799421]\n",
      " [ 0.0538828 ]\n",
      " [-0.21686019]]\n",
      "t [[ 0.16335539]\n",
      " [-0.09522964]\n",
      " [-0.15240629]\n",
      " ...\n",
      " [ 0.12799421]\n",
      " [ 0.0538828 ]\n",
      " [-0.21686019]]\n",
      "t [[ 0.29473919]\n",
      " [-0.21402674]\n",
      " [-0.28348384]\n",
      " ...\n",
      " [ 0.2093797 ]\n",
      " [ 0.09004399]\n",
      " [-0.37572153]]\n",
      "t [[ 0.29473919]\n",
      " [-0.21402674]\n",
      " [-0.28348384]\n",
      " ...\n",
      " [ 0.2093797 ]\n",
      " [ 0.09004399]\n",
      " [-0.37572153]]\n",
      "Current iteration=2, loss=45544.942996440004\n",
      "t [[ 0.40153577]\n",
      " [-0.34038438]\n",
      " [-0.39839063]\n",
      " ...\n",
      " [ 0.26109756]\n",
      " [ 0.11376594]\n",
      " [-0.49811292]]\n",
      "t [[ 0.40153577]\n",
      " [-0.34038438]\n",
      " [-0.39839063]\n",
      " ...\n",
      " [ 0.26109756]\n",
      " [ 0.11376594]\n",
      " [-0.49811292]]\n",
      "t [[ 0.48938241]\n",
      " [-0.46590743]\n",
      " [-0.50071944]\n",
      " ...\n",
      " [ 0.29351714]\n",
      " [ 0.12868082]\n",
      " [-0.59701055]]\n",
      "t [[ 0.48938241]\n",
      " [-0.46590743]\n",
      " [-0.50071944]\n",
      " ...\n",
      " [ 0.29351714]\n",
      " [ 0.12868082]\n",
      " [-0.59701055]]\n",
      "Current iteration=4, loss=41684.695988946696\n",
      "t [[ 0.56243628]\n",
      " [-0.58660459]\n",
      " [-0.59298357]\n",
      " ...\n",
      " [ 0.3130769 ]\n",
      " [ 0.13726809]\n",
      " [-0.68017347]]\n",
      "t [[ 0.56243628]\n",
      " [-0.58660459]\n",
      " [-0.59298357]\n",
      " ...\n",
      " [ 0.3130769 ]\n",
      " [ 0.13726809]\n",
      " [-0.68017347]]\n",
      "t [[ 0.6237813 ]\n",
      " [-0.70081754]\n",
      " [-0.67698723]\n",
      " ...\n",
      " [ 0.3238635 ]\n",
      " [ 0.14124763]\n",
      " [-0.75231818]]\n",
      "t [[ 0.6237813 ]\n",
      " [-0.70081754]\n",
      " [-0.67698723]\n",
      " ...\n",
      " [ 0.3238635 ]\n",
      " [ 0.14124763]\n",
      " [-0.75231818]]\n",
      "Current iteration=6, loss=39108.615040842\n",
      "t [[ 0.67574327]\n",
      " [-0.80807299]\n",
      " [-0.75406502]\n",
      " ...\n",
      " [ 0.32852743]\n",
      " [ 0.14183777]\n",
      " [-0.81638632]]\n",
      "t [[ 0.67574327]\n",
      " [-0.80807299]\n",
      " [-0.75406502]\n",
      " ...\n",
      " [ 0.32852743]\n",
      " [ 0.14183777]\n",
      " [-0.81638632]]\n",
      "t [[ 0.72010465]\n",
      " [-0.90847747]\n",
      " [-0.82523324]\n",
      " ...\n",
      " [ 0.32883142]\n",
      " [ 0.13991952]\n",
      " [-0.8742719 ]]\n",
      "t [[ 0.72010465]\n",
      " [-0.90847747]\n",
      " [-0.82523324]\n",
      " ...\n",
      " [ 0.32883142]\n",
      " [ 0.13991952]\n",
      " [-0.8742719 ]]\n",
      "Current iteration=8, loss=37277.14960905647\n",
      "t [[ 0.75825141]\n",
      " [-1.00240304]\n",
      " [-0.89128627]\n",
      " ...\n",
      " [ 0.32597956]\n",
      " [ 0.13614188]\n",
      " [-0.9272383 ]]\n",
      "t [[ 0.75825141]\n",
      " [-1.00240304]\n",
      " [-0.89128627]\n",
      " ...\n",
      " [ 0.32597956]\n",
      " [ 0.13614188]\n",
      " [-0.9272383 ]]\n",
      "t [[ 0.79127507]\n",
      " [-1.09032581]\n",
      " [-0.9528593 ]\n",
      " ...\n",
      " [ 0.32081597]\n",
      " [ 0.13099071]\n",
      " [-0.97615993]]\n",
      "loss=35914.65277083995\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.16126167]\n",
      " [-0.09116214]\n",
      " [-0.1518355 ]\n",
      " ...\n",
      " [ 0.12996899]\n",
      " [ 0.05487591]\n",
      " [-0.2235384 ]]\n",
      "t [[ 0.16126167]\n",
      " [-0.09116214]\n",
      " [-0.1518355 ]\n",
      " ...\n",
      " [ 0.12996899]\n",
      " [ 0.05487591]\n",
      " [-0.2235384 ]]\n",
      "t [[ 0.29097512]\n",
      " [-0.20599897]\n",
      " [-0.2823444 ]\n",
      " ...\n",
      " [ 0.21207375]\n",
      " [ 0.09181855]\n",
      " [-0.38733756]]\n",
      "t [[ 0.29097512]\n",
      " [-0.20599897]\n",
      " [-0.2823444 ]\n",
      " ...\n",
      " [ 0.21207375]\n",
      " [ 0.09181855]\n",
      " [-0.38733756]]\n",
      "Current iteration=2, loss=45599.48537766008\n",
      "t [[ 0.39644085]\n",
      " [-0.32843497]\n",
      " [-0.3967165 ]\n",
      " ...\n",
      " [ 0.26376749]\n",
      " [ 0.11618212]\n",
      " [-0.51352532]]\n",
      "t [[ 0.39644085]\n",
      " [-0.32843497]\n",
      " [-0.3967165 ]\n",
      " ...\n",
      " [ 0.26376749]\n",
      " [ 0.11618212]\n",
      " [-0.51352532]]\n",
      "t [[ 0.48322942]\n",
      " [-0.45007856]\n",
      " [-0.49855369]\n",
      " ...\n",
      " [ 0.29570555]\n",
      " [ 0.13164169]\n",
      " [-0.61543988]]\n",
      "t [[ 0.48322942]\n",
      " [-0.45007856]\n",
      " [-0.49855369]\n",
      " ...\n",
      " [ 0.29570555]\n",
      " [ 0.13164169]\n",
      " [-0.61543988]]\n",
      "Current iteration=4, loss=41785.377655340846\n",
      "t [[ 0.55544595]\n",
      " [-0.56695943]\n",
      " [-0.5903678 ]\n",
      " ...\n",
      " [ 0.31447923]\n",
      " [ 0.14070219]\n",
      " [-0.70105865]]\n",
      "t [[ 0.55544595]\n",
      " [-0.56695943]\n",
      " [-0.5903678 ]\n",
      " ...\n",
      " [ 0.31447923]\n",
      " [ 0.14070219]\n",
      " [-0.70105865]]\n",
      "t [[ 0.61612953]\n",
      " [-0.6774419 ]\n",
      " [-0.6739582 ]\n",
      " ...\n",
      " [ 0.3242724 ]\n",
      " [ 0.14509948]\n",
      " [-0.77524068]]\n",
      "t [[ 0.61612953]\n",
      " [-0.6774419 ]\n",
      " [-0.6739582 ]\n",
      " ...\n",
      " [ 0.3242724 ]\n",
      " [ 0.14509948]\n",
      " [-0.77524068]]\n",
      "Current iteration=6, loss=39243.90011969231\n",
      "t [[ 0.66756857]\n",
      " [-0.78107027]\n",
      " [-0.75065414]\n",
      " ...\n",
      " [ 0.32780212]\n",
      " [ 0.14606268]\n",
      " [-0.8410253 ]]\n",
      "t [[ 0.66756857]\n",
      " [-0.78107027]\n",
      " [-0.75065414]\n",
      " ...\n",
      " [ 0.32780212]\n",
      " [ 0.14606268]\n",
      " [-0.8410253 ]]\n",
      "t [[ 0.71151563]\n",
      " [-0.87796238]\n",
      " [-0.82146698]\n",
      " ...\n",
      " [ 0.32687754]\n",
      " [ 0.14448053]\n",
      " [-0.90037552]]\n",
      "t [[ 0.71151563]\n",
      " [-0.87796238]\n",
      " [-0.82146698]\n",
      " ...\n",
      " [ 0.32687754]\n",
      " [ 0.14448053]\n",
      " [-0.90037552]]\n",
      "Current iteration=8, loss=37438.60459280245\n",
      "t [[ 0.74933315]\n",
      " [-0.96849619]\n",
      " [-0.88718682]\n",
      " ...\n",
      " [ 0.32273539]\n",
      " [ 0.14100786]\n",
      " [-0.95460474]]\n",
      "t [[ 0.74933315]\n",
      " [-0.96849619]\n",
      " [-0.88718682]\n",
      " ...\n",
      " [ 0.32273539]\n",
      " [ 0.14100786]\n",
      " [-0.95460474]]\n",
      "t [[ 0.78209438]\n",
      " [-1.0531498 ]\n",
      " [-0.94844526]\n",
      " ...\n",
      " [ 0.31624302]\n",
      " [ 0.13613504]\n",
      " [-1.00462443]]\n",
      "loss=36096.386847746224\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16100353]\n",
      " [-0.09577492]\n",
      " [-0.15135858]\n",
      " ...\n",
      " [ 0.23189571]\n",
      " [-0.15135858]\n",
      " [ 0.21720319]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.16100353]\n",
      " [-0.09577492]\n",
      " [-0.15135858]\n",
      " ...\n",
      " [ 0.23189571]\n",
      " [-0.15135858]\n",
      " [ 0.21720319]]\n",
      "t [[ 0.29050395]\n",
      " [-0.21394213]\n",
      " [-0.2815745 ]\n",
      " ...\n",
      " [ 0.41279097]\n",
      " [-0.2815745 ]\n",
      " [ 0.378385  ]]\n",
      "t [[ 0.29050395]\n",
      " [-0.21394213]\n",
      " [-0.2815745 ]\n",
      " ...\n",
      " [ 0.41279097]\n",
      " [-0.2815745 ]\n",
      " [ 0.378385  ]]\n",
      "Current iteration=2, loss=45603.70303687158\n",
      "t [[ 0.39579548]\n",
      " [-0.33897955]\n",
      " [-0.39574774]\n",
      " ...\n",
      " [ 0.55635737]\n",
      " [-0.39574774]\n",
      " [ 0.5005736 ]]\n",
      "t [[ 0.39579548]\n",
      " [-0.33897955]\n",
      " [-0.39574774]\n",
      " ...\n",
      " [ 0.55635737]\n",
      " [-0.39574774]\n",
      " [ 0.5005736 ]]\n",
      "t [[ 0.48237381]\n",
      " [-0.46285208]\n",
      " [-0.49743652]\n",
      " ...\n",
      " [ 0.6722012 ]\n",
      " [-0.49743652]\n",
      " [ 0.59515199]]\n",
      "t [[ 0.48237381]\n",
      " [-0.46285208]\n",
      " [-0.49743652]\n",
      " ...\n",
      " [ 0.6722012 ]\n",
      " [-0.49743652]\n",
      " [ 0.59515199]]\n",
      "Current iteration=4, loss=41781.11822747265\n",
      "t [[ 0.55431351]\n",
      " [-0.58177102]\n",
      " [-0.58912983]\n",
      " ...\n",
      " [ 0.76703788]\n",
      " [-0.58912983]\n",
      " [ 0.66969782]]\n",
      "t [[ 0.55431351]\n",
      " [-0.58177102]\n",
      " [-0.58912983]\n",
      " ...\n",
      " [ 0.76703788]\n",
      " [-0.58912983]\n",
      " [ 0.66969782]]\n",
      "t [[ 0.61465739]\n",
      " [-0.69418148]\n",
      " [-0.67261465]\n",
      " ...\n",
      " [ 0.84564518]\n",
      " [-0.67261465]\n",
      " [ 0.72936115]]\n",
      "t [[ 0.61465739]\n",
      " [-0.69418148]\n",
      " [-0.67261465]\n",
      " ...\n",
      " [ 0.84564518]\n",
      " [-0.67261465]\n",
      " [ 0.72936115]]\n",
      "Current iteration=6, loss=39231.69167280288\n",
      "t [[ 0.66570762]\n",
      " [-0.79966188]\n",
      " [-0.74921372]\n",
      " ...\n",
      " [ 0.91149839]\n",
      " [-0.74921372]\n",
      " [ 0.77773522]]\n",
      "t [[ 0.66570762]\n",
      " [-0.79966188]\n",
      " [-0.74921372]\n",
      " ...\n",
      " [ 0.91149839]\n",
      " [-0.74921372]\n",
      " [ 0.77773522]]\n",
      "t [[ 0.70923058]\n",
      " [-0.89834364]\n",
      " [-0.81993515]\n",
      " ...\n",
      " [ 0.96718088]\n",
      " [-0.81993515]\n",
      " [ 0.81739307]]\n",
      "t [[ 0.70923058]\n",
      " [-0.89834364]\n",
      " [-0.81993515]\n",
      " ...\n",
      " [ 0.96718088]\n",
      " [-0.81993515]\n",
      " [ 0.81739307]]\n",
      "Current iteration=8, loss=37420.05784066068\n",
      "t [[ 0.74660008]\n",
      " [-0.99060956]\n",
      " [-0.88556751]\n",
      " ...\n",
      " [ 1.01465136]\n",
      " [-0.88556751]\n",
      " [ 0.85022149]]\n",
      "t [[ 0.74660008]\n",
      " [-0.99060956]\n",
      " [-0.88556751]\n",
      " ...\n",
      " [ 1.01465136]\n",
      " [-0.88556751]\n",
      " [ 0.85022149]]\n",
      "t [[ 0.77889803]\n",
      " [-1.076939  ]\n",
      " [-0.94674172]\n",
      " ...\n",
      " [ 1.0554212 ]\n",
      " [-0.94674172]\n",
      " [ 0.87763361]]\n",
      "loss=36072.845029377095\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05596053]\n",
      " [-0.26796688]\n",
      " [-0.15713886]\n",
      " ...\n",
      " [ 0.13437458]\n",
      " [ 0.05596053]\n",
      " [-0.22778052]]\n",
      "t [[ 0.05596053]\n",
      " [-0.26796688]\n",
      " [-0.15713886]\n",
      " ...\n",
      " [ 0.13437458]\n",
      " [ 0.05596053]\n",
      " [-0.22778052]]\n",
      "t [[ 0.09302617]\n",
      " [-0.50597927]\n",
      " [-0.29170094]\n",
      " ...\n",
      " [ 0.21853336]\n",
      " [ 0.09302617]\n",
      " [-0.39348408]]\n",
      "t [[ 0.09302617]\n",
      " [-0.50597927]\n",
      " [-0.29170094]\n",
      " ...\n",
      " [ 0.21853336]\n",
      " [ 0.09302617]\n",
      " [-0.39348408]]\n",
      "Current iteration=2, loss=45380.66729295403\n",
      "t [[ 0.11701018]\n",
      " [-0.71694522]\n",
      " [-0.40931214]\n",
      " ...\n",
      " [ 0.27127433]\n",
      " [ 0.11701018]\n",
      " [-0.52077038]]\n",
      "t [[ 0.11701018]\n",
      " [-0.71694522]\n",
      " [-0.40931214]\n",
      " ...\n",
      " [ 0.27127433]\n",
      " [ 0.11701018]\n",
      " [-0.52077038]]\n",
      "t [[ 0.13184147]\n",
      " [-0.90429472]\n",
      " [-0.51382534]\n",
      " ...\n",
      " [ 0.30386081]\n",
      " [ 0.13184147]\n",
      " [-0.62358426]]\n",
      "t [[ 0.13184147]\n",
      " [-0.90429472]\n",
      " [-0.51382534]\n",
      " ...\n",
      " [ 0.30386081]\n",
      " [ 0.13184147]\n",
      " [-0.62358426]]\n",
      "Current iteration=4, loss=41475.10926362185\n",
      "t [[ 0.14016312]\n",
      " [-1.07132902]\n",
      " [-0.60790161]\n",
      " ...\n",
      " [ 0.3231649 ]\n",
      " [ 0.14016312]\n",
      " [-0.71010555]]\n",
      "t [[ 0.14016312]\n",
      " [-1.07132902]\n",
      " [-0.60790161]\n",
      " ...\n",
      " [ 0.3231649 ]\n",
      " [ 0.14016312]\n",
      " [-0.71010555]]\n",
      "t [[ 0.14378715]\n",
      " [-1.22097938]\n",
      " [-0.69343458]\n",
      " ...\n",
      " [ 0.33348625]\n",
      " [ 0.14378715]\n",
      " [-0.78523261]]\n",
      "t [[ 0.14378715]\n",
      " [-1.22097938]\n",
      " [-0.69343458]\n",
      " ...\n",
      " [ 0.33348625]\n",
      " [ 0.14378715]\n",
      " [-0.78523261]]\n",
      "Current iteration=6, loss=38892.44714956466\n",
      "t [[ 0.14398502]\n",
      " [-1.35575242]\n",
      " [-0.77181717]\n",
      " ...\n",
      " [ 0.33758163]\n",
      " [ 0.14398502]\n",
      " [-0.85198881]]\n",
      "t [[ 0.14398502]\n",
      " [-1.35575242]\n",
      " [-0.77181717]\n",
      " ...\n",
      " [ 0.33758163]\n",
      " [ 0.14398502]\n",
      " [-0.85198881]]\n",
      "t [[ 0.14166898]\n",
      " [-1.47775313]\n",
      " [-0.84410647]\n",
      " ...\n",
      " [ 0.33726881]\n",
      " [ 0.14166898]\n",
      " [-0.91230963]]\n",
      "t [[ 0.14166898]\n",
      " [-1.47775313]\n",
      " [-0.84410647]\n",
      " ...\n",
      " [ 0.33726881]\n",
      " [ 0.14166898]\n",
      " [-0.91230963]]\n",
      "Current iteration=8, loss=37068.52827477375\n",
      "t [[ 0.13750659]\n",
      " [-1.58873641]\n",
      " [-0.91112706]\n",
      " ...\n",
      " [ 0.33378182]\n",
      " [ 0.13750659]\n",
      " [-0.96748436]]\n",
      "t [[ 0.13750659]\n",
      " [-1.58873641]\n",
      " [-0.91112706]\n",
      " ...\n",
      " [ 0.33378182]\n",
      " [ 0.13750659]\n",
      " [-0.96748436]]\n",
      "t [[ 0.13199457]\n",
      " [-1.69016363]\n",
      " [-0.97353752]\n",
      " ...\n",
      " [ 0.32798198]\n",
      " [ 0.13199457]\n",
      " [-1.01840738]]\n",
      "loss=35718.60669800975\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16898834]\n",
      " [-0.09851342]\n",
      " [-0.15766168]\n",
      " ...\n",
      " [ 0.1324078 ]\n",
      " [ 0.05574082]\n",
      " [-0.22433813]]\n",
      "t [[ 0.16898834]\n",
      " [-0.09851342]\n",
      " [-0.15766168]\n",
      " ...\n",
      " [ 0.1324078 ]\n",
      " [ 0.05574082]\n",
      " [-0.22433813]]\n",
      "t [[ 0.30377722]\n",
      " [-0.2222347 ]\n",
      " [-0.29250693]\n",
      " ...\n",
      " [ 0.21495699]\n",
      " [ 0.09252496]\n",
      " [-0.38663548]]\n",
      "t [[ 0.30377722]\n",
      " [-0.2222347 ]\n",
      " [-0.29250693]\n",
      " ...\n",
      " [ 0.21495699]\n",
      " [ 0.09252496]\n",
      " [-0.38663548]]\n",
      "Current iteration=2, loss=45369.00173489221\n",
      "t [[ 0.41255003]\n",
      " [-0.35346142]\n",
      " [-0.41024899]\n",
      " ...\n",
      " [ 0.26639286]\n",
      " [ 0.11620153]\n",
      " [-0.51071756]]\n",
      "t [[ 0.41255003]\n",
      " [-0.35346142]\n",
      " [-0.41024899]\n",
      " ...\n",
      " [ 0.26639286]\n",
      " [ 0.11620153]\n",
      " [-0.51071756]]\n",
      "t [[ 0.50146225]\n",
      " [-0.48321222]\n",
      " [-0.51479757]\n",
      " ...\n",
      " [ 0.29792987]\n",
      " [ 0.13071942]\n",
      " [-0.61059514]]\n",
      "t [[ 0.50146225]\n",
      " [-0.48321222]\n",
      " [-0.51479757]\n",
      " ...\n",
      " [ 0.29792987]\n",
      " [ 0.13071942]\n",
      " [-0.61059514]]\n",
      "Current iteration=4, loss=41462.088345781245\n",
      "t [[ 0.57499409]\n",
      " [-0.60739371]\n",
      " [-0.6088494 ]\n",
      " ...\n",
      " [ 0.31639602]\n",
      " [ 0.13873246]\n",
      " [-0.69446027]]\n",
      "t [[ 0.57499409]\n",
      " [-0.60739371]\n",
      " [-0.6088494 ]\n",
      " ...\n",
      " [ 0.31639602]\n",
      " [ 0.13873246]\n",
      " [-0.69446027]]\n",
      "t [[ 0.63643584]\n",
      " [-0.72441701]\n",
      " [-0.69432052]\n",
      " ...\n",
      " [ 0.32605127]\n",
      " [ 0.14205842]\n",
      " [-0.76719778]]\n",
      "t [[ 0.63643584]\n",
      " [-0.72441701]\n",
      " [-0.69432052]\n",
      " ...\n",
      " [ 0.32605127]\n",
      " [ 0.14205842]\n",
      " [-0.76719778]]\n",
      "Current iteration=6, loss=38879.634984361524\n",
      "t [[ 0.68824853]\n",
      " [-0.83392189]\n",
      " [-0.77261806]\n",
      " ...\n",
      " [ 0.32961923]\n",
      " [ 0.14197178]\n",
      " [-0.83180732]]\n",
      "t [[ 0.68824853]\n",
      " [-0.83392189]\n",
      " [-0.77261806]\n",
      " ...\n",
      " [ 0.32961923]\n",
      " [ 0.14197178]\n",
      " [-0.83180732]]\n",
      "t [[ 0.73230507]\n",
      " [-0.93612486]\n",
      " [-0.84480822]\n",
      " ...\n",
      " [ 0.32889098]\n",
      " [ 0.13938622]\n",
      " [-0.89019804]]\n",
      "t [[ 0.73230507]\n",
      " [-0.93612486]\n",
      " [-0.84480822]\n",
      " ...\n",
      " [ 0.32889098]\n",
      " [ 0.13938622]\n",
      " [-0.89019804]]\n",
      "Current iteration=8, loss=37056.03632587605\n",
      "t [[ 0.77005264]\n",
      " [-1.03149098]\n",
      " [-0.91172156]\n",
      " ...\n",
      " [ 0.32507938]\n",
      " [ 0.13496979]\n",
      " [-0.94363375]]\n",
      "t [[ 0.77005264]\n",
      " [-1.03149098]\n",
      " [-0.91172156]\n",
      " ...\n",
      " [ 0.32507938]\n",
      " [ 0.13496979]\n",
      " [-0.94363375]]\n",
      "t [[ 0.80262474]\n",
      " [-1.12057066]\n",
      " [-0.97402052]\n",
      " ...\n",
      " [ 0.31902903]\n",
      " [ 0.12921919]\n",
      " [-0.99298561]]\n",
      "loss=35706.26475211246\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.16682241]\n",
      " [-0.09430566]\n",
      " [-0.15707121]\n",
      " ...\n",
      " [ 0.13445068]\n",
      " [ 0.05676818]\n",
      " [-0.23124662]]\n",
      "t [[ 0.16682241]\n",
      " [-0.09430566]\n",
      " [-0.15707121]\n",
      " ...\n",
      " [ 0.13445068]\n",
      " [ 0.05676818]\n",
      " [-0.23124662]]\n",
      "t [[ 0.29989817]\n",
      " [-0.2139339 ]\n",
      " [-0.29132842]\n",
      " ...\n",
      " [ 0.2177011 ]\n",
      " [ 0.0943534 ]\n",
      " [-0.39859181]]\n",
      "t [[ 0.29989817]\n",
      " [-0.2139339 ]\n",
      " [-0.29132842]\n",
      " ...\n",
      " [ 0.2177011 ]\n",
      " [ 0.0943534 ]\n",
      " [-0.39859181]]\n",
      "Current iteration=2, loss=45425.39165694879\n",
      "t [[ 0.40731772]\n",
      " [-0.34110658]\n",
      " [-0.40851991]\n",
      " ...\n",
      " [ 0.26906212]\n",
      " [ 0.11868418]\n",
      " [-0.52652338]]\n",
      "t [[ 0.40731772]\n",
      " [-0.34110658]\n",
      " [-0.40851991]\n",
      " ...\n",
      " [ 0.26906212]\n",
      " [ 0.11868418]\n",
      " [-0.52652338]]\n",
      "t [[ 0.49516383]\n",
      " [-0.46684842]\n",
      " [-0.51256446]\n",
      " ...\n",
      " [ 0.30005343]\n",
      " [ 0.13375555]\n",
      " [-0.62944184]]\n",
      "t [[ 0.49516383]\n",
      " [-0.46684842]\n",
      " [-0.51256446]\n",
      " ...\n",
      " [ 0.30005343]\n",
      " [ 0.13375555]\n",
      " [-0.62944184]]\n",
      "Current iteration=4, loss=41565.60493094822\n",
      "t [[ 0.56785979]\n",
      " [-0.58709063]\n",
      " [-0.60615644]\n",
      " ...\n",
      " [ 0.31766391]\n",
      " [ 0.14224821]\n",
      " [-0.71577055]]\n",
      "t [[ 0.56785979]\n",
      " [-0.58709063]\n",
      " [-0.60615644]\n",
      " ...\n",
      " [ 0.31766391]\n",
      " [ 0.14224821]\n",
      " [-0.71577055]]\n",
      "t [[ 0.62864714]\n",
      " [-0.70026951]\n",
      " [-0.69120626]\n",
      " ...\n",
      " [ 0.32625546]\n",
      " [ 0.14599663]\n",
      " [-0.79054386]]\n",
      "t [[ 0.62864714]\n",
      " [-0.70026951]\n",
      " [-0.69120626]\n",
      " ...\n",
      " [ 0.32625546]\n",
      " [ 0.14599663]\n",
      " [-0.79054386]]\n",
      "Current iteration=6, loss=39018.09711148596\n",
      "t [[ 0.67994691]\n",
      " [-0.80604346]\n",
      " [-0.76911512]\n",
      " ...\n",
      " [ 0.32862086]\n",
      " [ 0.1462865 ]\n",
      " [-0.85686298]]\n",
      "t [[ 0.67994691]\n",
      " [-0.80604346]\n",
      " [-0.76911512]\n",
      " ...\n",
      " [ 0.32862086]\n",
      " [ 0.1462865 ]\n",
      " [-0.85686298]]\n",
      "t [[ 0.72360037]\n",
      " [-0.90464037]\n",
      " [-0.84094385]\n",
      " ...\n",
      " [ 0.32659882]\n",
      " [ 0.14403954]\n",
      " [-0.91670821]]\n",
      "t [[ 0.72360037]\n",
      " [-0.90464037]\n",
      " [-0.84094385]\n",
      " ...\n",
      " [ 0.32659882]\n",
      " [ 0.14403954]\n",
      " [-0.91670821]]\n",
      "Current iteration=8, loss=37220.72358272434\n",
      "t [[ 0.76103021]\n",
      " [-0.99653078]\n",
      " [-0.90751843]\n",
      " ...\n",
      " [ 0.32143535]\n",
      " [ 0.13992984]\n",
      " [-0.97139461]]\n",
      "t [[ 0.76103021]\n",
      " [-0.99653078]\n",
      " [-0.90751843]\n",
      " ...\n",
      " [ 0.32143535]\n",
      " [ 0.13992984]\n",
      " [-0.97139461]]\n",
      "t [[ 0.79335099]\n",
      " [-1.08226635]\n",
      " [-0.96949754]\n",
      " ...\n",
      " [ 0.31399842]\n",
      " [ 0.13445879]\n",
      " [-1.02183117]]\n",
      "loss=35891.15861978383\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.16655537]\n",
      " [-0.09907751]\n",
      " [-0.15657784]\n",
      " ...\n",
      " [ 0.23989212]\n",
      " [-0.15657784]\n",
      " [ 0.22469295]]\n",
      "t [[ 0.16655537]\n",
      " [-0.09907751]\n",
      " [-0.15657784]\n",
      " ...\n",
      " [ 0.23989212]\n",
      " [-0.15657784]\n",
      " [ 0.22469295]]\n",
      "t [[ 0.29941138]\n",
      " [-0.2221068 ]\n",
      " [-0.29053831]\n",
      " ...\n",
      " [ 0.4252284 ]\n",
      " [-0.29053831]\n",
      " [ 0.38946   ]]\n",
      "t [[ 0.29941138]\n",
      " [-0.2221068 ]\n",
      " [-0.29053831]\n",
      " ...\n",
      " [ 0.4252284 ]\n",
      " [-0.29053831]\n",
      " [ 0.38946   ]]\n",
      "Current iteration=2, loss=45429.42387703841\n",
      "t [[ 0.40664947]\n",
      " [-0.35192393]\n",
      " [-0.4075305 ]\n",
      " ...\n",
      " [ 0.5711502 ]\n",
      " [-0.4075305 ]\n",
      " [ 0.51315471]]\n",
      "t [[ 0.40664947]\n",
      " [-0.35192393]\n",
      " [-0.4075305 ]\n",
      " ...\n",
      " [ 0.5711502 ]\n",
      " [-0.4075305 ]\n",
      " [ 0.51315471]]\n",
      "t [[ 0.49427101]\n",
      " [-0.47993666]\n",
      " [-0.51142661]\n",
      " ...\n",
      " [ 0.68811355]\n",
      " [-0.51142661]\n",
      " [ 0.60813543]]\n",
      "t [[ 0.49427101]\n",
      " [-0.47993666]\n",
      " [-0.51142661]\n",
      " ...\n",
      " [ 0.68811355]\n",
      " [-0.51142661]\n",
      " [ 0.60813543]]\n",
      "Current iteration=4, loss=41560.75233913994\n",
      "t [[ 0.5666704 ]\n",
      " [-0.60226285]\n",
      " [-0.60489726]\n",
      " ...\n",
      " [ 0.78332275]\n",
      " [-0.60489726]\n",
      " [ 0.68249322]]\n",
      "t [[ 0.5666704 ]\n",
      " [-0.60226285]\n",
      " [-0.60489726]\n",
      " ...\n",
      " [ 0.78332275]\n",
      " [-0.60489726]\n",
      " [ 0.68249322]]\n",
      "t [[ 0.62709595]\n",
      " [-0.71741804]\n",
      " [-0.68984036]\n",
      " ...\n",
      " [ 0.86184532]\n",
      " [-0.68984036]\n",
      " [ 0.74165544]]\n",
      "t [[ 0.62709595]\n",
      " [-0.71741804]\n",
      " [-0.68984036]\n",
      " ...\n",
      " [ 0.86184532]\n",
      " [-0.68984036]\n",
      " [ 0.74165544]]\n",
      "Current iteration=6, loss=39005.145928866164\n",
      "t [[ 0.67798438]\n",
      " [-0.8250924 ]\n",
      " [-0.76765073]\n",
      " ...\n",
      " [ 0.92733432]\n",
      " [-0.76765073]\n",
      " [ 0.7893702 ]]\n",
      "t [[ 0.67798438]\n",
      " [-0.8250924 ]\n",
      " [-0.76765073]\n",
      " ...\n",
      " [ 0.92733432]\n",
      " [-0.76765073]\n",
      " [ 0.7893702 ]]\n",
      "t [[ 0.72119183]\n",
      " [-0.92552559]\n",
      " [-0.8393862 ]\n",
      " ...\n",
      " [ 0.98248558]\n",
      " [-0.8393862 ]\n",
      " [ 0.82829982]]\n",
      "t [[ 0.72119183]\n",
      " [-0.92552559]\n",
      " [-0.8393862 ]\n",
      " ...\n",
      " [ 0.98248558]\n",
      " [-0.8393862 ]\n",
      " [ 0.82829982]]\n",
      "Current iteration=8, loss=37201.39140868415\n",
      "t [[ 0.7581528 ]\n",
      " [-1.01919171]\n",
      " [-0.9058714 ]\n",
      " ...\n",
      " [ 1.02933089]\n",
      " [-0.9058714 ]\n",
      " [ 0.86038396]]\n",
      "t [[ 0.7581528 ]\n",
      " [-1.01919171]\n",
      " [-0.9058714 ]\n",
      " ...\n",
      " [ 1.02933089]\n",
      " [-0.9058714 ]\n",
      " [ 0.86038396]]\n",
      "t [[ 0.78999072]\n",
      " [-1.10664307]\n",
      " [-0.96776444]\n",
      " ...\n",
      " [ 1.06942994]\n",
      " [-0.96776444]\n",
      " [ 0.88706748]]\n",
      "loss=35866.841612048695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05782588]\n",
      " [-0.27689911]\n",
      " [-0.16237682]\n",
      " ...\n",
      " [ 0.13885374]\n",
      " [ 0.05782588]\n",
      " [-0.2353732 ]]\n",
      "t [[ 0.05782588]\n",
      " [-0.27689911]\n",
      " [-0.16237682]\n",
      " ...\n",
      " [ 0.13885374]\n",
      " [ 0.05782588]\n",
      " [-0.2353732 ]]\n",
      "t [[ 0.09548514]\n",
      " [-0.52182253]\n",
      " [-0.30065598]\n",
      " ...\n",
      " [ 0.22411041]\n",
      " [ 0.09548514]\n",
      " [-0.40449158]]\n",
      "t [[ 0.09548514]\n",
      " [-0.52182253]\n",
      " [-0.30065598]\n",
      " ...\n",
      " [ 0.22411041]\n",
      " [ 0.09548514]\n",
      " [-0.40449158]]\n",
      "Current iteration=2, loss=45207.36769150634\n",
      "t [[ 0.11939276]\n",
      " [-0.73799271]\n",
      " [-0.42104767]\n",
      " ...\n",
      " [ 0.27649904]\n",
      " [ 0.11939276]\n",
      " [-0.53344949]]\n",
      "t [[ 0.11939276]\n",
      " [-0.73799271]\n",
      " [-0.42104767]\n",
      " ...\n",
      " [ 0.27649904]\n",
      " [ 0.11939276]\n",
      " [-0.53344949]]\n",
      "t [[ 0.13380454]\n",
      " [-0.92920692]\n",
      " [-0.52772931]\n",
      " ...\n",
      " [ 0.30815523]\n",
      " [ 0.13380454]\n",
      " [-0.6372464 ]]\n",
      "t [[ 0.13380454]\n",
      " [-0.92920692]\n",
      " [-0.52772931]\n",
      " ...\n",
      " [ 0.30815523]\n",
      " [ 0.13380454]\n",
      " [-0.6372464 ]]\n",
      "Current iteration=4, loss=41258.20526736646\n",
      "t [[ 0.14153926]\n",
      " [-1.09908772]\n",
      " [-0.62354617]\n",
      " ...\n",
      " [ 0.32633954]\n",
      " [ 0.14153926]\n",
      " [-0.72448574]]\n",
      "t [[ 0.14153926]\n",
      " [-1.09908772]\n",
      " [-0.62354617]\n",
      " ...\n",
      " [ 0.32633954]\n",
      " [ 0.14153926]\n",
      " [-0.72448574]]\n",
      "t [[ 0.14450551]\n",
      " [-1.25082046]\n",
      " [-0.710503  ]\n",
      " ...\n",
      " [ 0.33551809]\n",
      " [ 0.14450551]\n",
      " [-0.80022328]]\n",
      "t [[ 0.14450551]\n",
      " [-1.25082046]\n",
      " [-0.710503  ]\n",
      " ...\n",
      " [ 0.33551809]\n",
      " [ 0.14450551]\n",
      " [-0.80022328]]\n",
      "Current iteration=6, loss=38671.05616644838\n",
      "t [[ 0.14402922]\n",
      " [-1.38710363]\n",
      " [-0.79006451]\n",
      " ...\n",
      " [ 0.33851502]\n",
      " [ 0.14402922]\n",
      " [-0.86753378]]\n",
      "t [[ 0.14402922]\n",
      " [-1.38710363]\n",
      " [-0.79006451]\n",
      " ...\n",
      " [ 0.33851502]\n",
      " [ 0.14402922]\n",
      " [-0.86753378]]\n",
      "t [[ 0.14105379]\n",
      " [-1.5101843 ]\n",
      " [-0.86333722]\n",
      " ...\n",
      " [ 0.33717273]\n",
      " [ 0.14105379]\n",
      " [-0.92836508]]\n",
      "t [[ 0.14105379]\n",
      " [-1.5101843 ]\n",
      " [-0.86333722]\n",
      " ...\n",
      " [ 0.33717273]\n",
      " [ 0.14105379]\n",
      " [-0.92836508]]\n",
      "Current iteration=8, loss=36855.98799178415\n",
      "t [[ 0.13626456]\n",
      " [-1.62192204]\n",
      " [-0.93118161]\n",
      " ...\n",
      " [ 0.33273185]\n",
      " [ 0.13626456]\n",
      " [-0.98400647]]\n",
      "t [[ 0.13626456]\n",
      " [-1.62192204]\n",
      " [-0.93118161]\n",
      " ...\n",
      " [ 0.33273185]\n",
      " [ 0.13626456]\n",
      " [-0.98400647]]\n",
      "t [[ 0.13016809]\n",
      " [-1.7238556 ]\n",
      " [-0.99428339]\n",
      " ...\n",
      " [ 0.32605304]\n",
      " [ 0.13016809]\n",
      " [-1.03534988]]\n",
      "loss=35519.2227041723\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.17462128]\n",
      " [-0.10179721]\n",
      " [-0.16291707]\n",
      " ...\n",
      " [ 0.1368214 ]\n",
      " [ 0.05759885]\n",
      " [-0.23181607]]\n",
      "t [[ 0.17462128]\n",
      " [-0.10179721]\n",
      " [-0.16291707]\n",
      " ...\n",
      " [ 0.1368214 ]\n",
      " [ 0.05759885]\n",
      " [-0.23181607]]\n",
      "t [[ 0.31274178]\n",
      " [-0.23049651]\n",
      " [-0.30148077]\n",
      " ...\n",
      " [ 0.22042685]\n",
      " [ 0.09496518]\n",
      " [-0.3974161 ]]\n",
      "t [[ 0.31274178]\n",
      " [-0.23049651]\n",
      " [-0.30148077]\n",
      " ...\n",
      " [ 0.22042685]\n",
      " [ 0.09496518]\n",
      " [-0.3974161 ]]\n",
      "Current iteration=2, loss=45195.54781420421\n",
      "t [[ 0.42339884]\n",
      " [-0.3665872 ]\n",
      " [-0.42199758]\n",
      " ...\n",
      " [ 0.27148733]\n",
      " [ 0.11855341]\n",
      " [-0.52307693]]\n",
      "t [[ 0.42339884]\n",
      " [-0.3665872 ]\n",
      " [-0.42199758]\n",
      " ...\n",
      " [ 0.27148733]\n",
      " [ 0.11855341]\n",
      " [-0.52307693]]\n",
      "t [[ 0.51328898]\n",
      " [-0.50050278]\n",
      " [-0.52870634]\n",
      " ...\n",
      " [ 0.30208486]\n",
      " [ 0.13264074]\n",
      " [-0.62386691]]\n",
      "t [[ 0.51328898]\n",
      " [-0.50050278]\n",
      " [-0.52870634]\n",
      " ...\n",
      " [ 0.30208486]\n",
      " [ 0.13264074]\n",
      " [-0.62386691]]\n",
      "Current iteration=4, loss=41245.16888403236\n",
      "t [[ 0.58722335]\n",
      " [-0.62807231]\n",
      " [-0.62449005]\n",
      " ...\n",
      " [ 0.3194315 ]\n",
      " [ 0.14005717]\n",
      " [-0.70839916]]\n",
      "t [[ 0.58722335]\n",
      " [-0.62807231]\n",
      " [-0.62449005]\n",
      " ...\n",
      " [ 0.3194315 ]\n",
      " [ 0.14005717]\n",
      " [-0.70839916]]\n",
      "t [[ 0.64870054]\n",
      " [-0.74779697]\n",
      " [-0.71137679]\n",
      " ...\n",
      " [ 0.32795028]\n",
      " [ 0.14271717]\n",
      " [-0.78171207]]\n",
      "t [[ 0.64870054]\n",
      " [-0.74779697]\n",
      " [-0.71137679]\n",
      " ...\n",
      " [ 0.32795028]\n",
      " [ 0.14271717]\n",
      " [-0.78171207]]\n",
      "Current iteration=6, loss=38658.28140441564\n",
      "t [[ 0.70031644]\n",
      " [-0.85944221]\n",
      " [-0.7908458 ]\n",
      " ...\n",
      " [ 0.33042981]\n",
      " [ 0.14194959]\n",
      " [-0.846853  ]]\n",
      "t [[ 0.70031644]\n",
      " [-0.85944221]\n",
      " [-0.7908458 ]\n",
      " ...\n",
      " [ 0.33042981]\n",
      " [ 0.14194959]\n",
      " [-0.846853  ]]\n",
      "t [[ 0.74403296]\n",
      " [-0.96334114]\n",
      " [-0.86401281]\n",
      " ...\n",
      " [ 0.32868425]\n",
      " [ 0.13869917]\n",
      " [-0.90574043]]\n",
      "t [[ 0.74403296]\n",
      " [-0.96334114]\n",
      " [-0.86401281]\n",
      " ...\n",
      " [ 0.32868425]\n",
      " [ 0.13869917]\n",
      " [-0.90574043]]\n",
      "Current iteration=8, loss=36843.53367123207\n",
      "t [[ 0.78135694]\n",
      " [-1.06005456]\n",
      " [-0.93174419]\n",
      " ...\n",
      " [ 0.32393207]\n",
      " [ 0.1336516 ]\n",
      " [-0.95963582]]\n",
      "t [[ 0.78135694]\n",
      " [-1.06005456]\n",
      " [-0.93174419]\n",
      " ...\n",
      " [ 0.32393207]\n",
      " [ 0.1336516 ]\n",
      " [-0.95963582]]\n",
      "t [[ 0.81346218]\n",
      " [-1.15020782]\n",
      " [-0.99472949]\n",
      " ...\n",
      " [ 0.31701657]\n",
      " [ 0.12731331]\n",
      " [-1.00940611]]\n",
      "loss=35506.89284843219\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.17238316]\n",
      " [-0.09744918]\n",
      " [-0.16230692]\n",
      " ...\n",
      " [ 0.13893237]\n",
      " [ 0.05866045]\n",
      " [-0.23895485]]\n",
      "t [[ 0.17238316]\n",
      " [-0.09744918]\n",
      " [-0.16230692]\n",
      " ...\n",
      " [ 0.13893237]\n",
      " [ 0.05866045]\n",
      " [-0.23895485]]\n",
      "t [[ 0.3087487 ]\n",
      " [-0.22192292]\n",
      " [-0.30026322]\n",
      " ...\n",
      " [ 0.22321834]\n",
      " [ 0.09684703]\n",
      " [-0.40970888]]\n",
      "t [[ 0.3087487 ]\n",
      " [-0.22192292]\n",
      " [-0.30026322]\n",
      " ...\n",
      " [ 0.22321834]\n",
      " [ 0.09684703]\n",
      " [-0.40970888]]\n",
      "Current iteration=2, loss=45253.777652642384\n",
      "t [[ 0.41803143]\n",
      " [-0.35382718]\n",
      " [-0.42021384]\n",
      " ...\n",
      " [ 0.2741512 ]\n",
      " [ 0.12110161]\n",
      " [-0.53926876]]\n",
      "t [[ 0.41803143]\n",
      " [-0.35382718]\n",
      " [-0.42021384]\n",
      " ...\n",
      " [ 0.2741512 ]\n",
      " [ 0.12110161]\n",
      " [-0.53926876]]\n",
      "t [[ 0.50684881]\n",
      " [-0.48360457]\n",
      " [-0.52640646]\n",
      " ...\n",
      " [ 0.30413744]\n",
      " [ 0.13575084]\n",
      " [-0.64312077]]\n",
      "t [[ 0.50684881]\n",
      " [-0.48360457]\n",
      " [-0.52640646]\n",
      " ...\n",
      " [ 0.30413744]\n",
      " [ 0.13575084]\n",
      " [-0.64312077]]\n",
      "Current iteration=4, loss=41351.47064742744\n",
      "t [[ 0.5799499 ]\n",
      " [-0.6071127 ]\n",
      " [-0.6217208 ]\n",
      " ...\n",
      " [ 0.32055807]\n",
      " [ 0.14365296]\n",
      " [-0.73012226]]\n",
      "t [[ 0.5799499 ]\n",
      " [-0.6071127 ]\n",
      " [-0.6217208 ]\n",
      " ...\n",
      " [ 0.32055807]\n",
      " [ 0.14365296]\n",
      " [-0.73012226]]\n",
      "t [[ 0.64078058]\n",
      " [-0.7228804 ]\n",
      " [-0.70817848]\n",
      " ...\n",
      " [ 0.32794279]\n",
      " [ 0.14673983]\n",
      " [-0.80546798]]\n",
      "t [[ 0.64078058]\n",
      " [-0.7228804 ]\n",
      " [-0.70817848]\n",
      " ...\n",
      " [ 0.32794279]\n",
      " [ 0.14673983]\n",
      " [-0.80546798]]\n",
      "Current iteration=6, loss=38799.83614738311\n",
      "t [[ 0.69189406]\n",
      " [-0.83069278]\n",
      " [-0.78725219]\n",
      " ...\n",
      " [ 0.32915177]\n",
      " [ 0.14635197]\n",
      " [-0.87231052]]\n",
      "t [[ 0.69189406]\n",
      " [-0.83069278]\n",
      " [-0.78725219]\n",
      " ...\n",
      " [ 0.32915177]\n",
      " [ 0.14635197]\n",
      " [-0.87231052]]\n",
      "t [[ 0.73521901]\n",
      " [-0.93089424]\n",
      " [-0.86005187]\n",
      " ...\n",
      " [ 0.3260479 ]\n",
      " [ 0.14344242]\n",
      " [-0.93264158]]\n",
      "t [[ 0.73521901]\n",
      " [-0.93089424]\n",
      " [-0.86005187]\n",
      " ...\n",
      " [ 0.3260479 ]\n",
      " [ 0.14344242]\n",
      " [-0.93264158]]\n",
      "Current iteration=8, loss=37011.34694669655\n",
      "t [[ 0.77223678]\n",
      " [-1.02405047]\n",
      " [-0.92743905]\n",
      " ...\n",
      " [ 0.31988322]\n",
      " [ 0.13870313]\n",
      " [-0.987775  ]]\n",
      "t [[ 0.77223678]\n",
      " [-1.02405047]\n",
      " [-0.92743905]\n",
      " ...\n",
      " [ 0.31988322]\n",
      " [ 0.13870313]\n",
      " [-0.987775  ]]\n",
      "t [[ 0.80410169]\n",
      " [-1.11078728]\n",
      " [-0.9900993 ]\n",
      " ...\n",
      " [ 0.31152442]\n",
      " [ 0.13264542]\n",
      " [-1.03861622]]\n",
      "loss=35694.82736895585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.17210722]\n",
      " [-0.10238009]\n",
      " [-0.1617971 ]\n",
      " ...\n",
      " [ 0.24788852]\n",
      " [-0.1617971 ]\n",
      " [ 0.23218272]]\n",
      "t [[ 0.17210722]\n",
      " [-0.10238009]\n",
      " [-0.1617971 ]\n",
      " ...\n",
      " [ 0.24788852]\n",
      " [-0.1617971 ]\n",
      " [ 0.23218272]]\n",
      "t [[ 0.30824626]\n",
      " [-0.23032279]\n",
      " [-0.2994533 ]\n",
      " ...\n",
      " [ 0.43754838]\n",
      " [-0.2994533 ]\n",
      " [ 0.40040612]]\n",
      "t [[ 0.30824626]\n",
      " [-0.23032279]\n",
      " [-0.2994533 ]\n",
      " ...\n",
      " [ 0.43754838]\n",
      " [-0.2994533 ]\n",
      " [ 0.40040612]]\n",
      "Current iteration=2, loss=45257.61187773725\n",
      "t [[ 0.4173398 ]\n",
      " [-0.36491315]\n",
      " [-0.41920437]\n",
      " ...\n",
      " [ 0.58569097]\n",
      " [-0.41920437]\n",
      " [ 0.52547321]]\n",
      "t [[ 0.4173398 ]\n",
      " [-0.36491315]\n",
      " [-0.41920437]\n",
      " ...\n",
      " [ 0.58569097]\n",
      " [-0.41920437]\n",
      " [ 0.52547321]]\n",
      "t [[ 0.50591755]\n",
      " [-0.49700342]\n",
      " [-0.52524855]\n",
      " ...\n",
      " [ 0.70365509]\n",
      " [-0.52524855]\n",
      " [ 0.62075066]]\n",
      "t [[ 0.50591755]\n",
      " [-0.49700342]\n",
      " [-0.52524855]\n",
      " ...\n",
      " [ 0.70365509]\n",
      " [-0.52524855]\n",
      " [ 0.62075066]]\n",
      "Current iteration=4, loss=41346.02702038176\n",
      "t [[ 0.57870183]\n",
      " [-0.62264189]\n",
      " [-0.62044098]\n",
      " ...\n",
      " [ 0.79914132]\n",
      " [-0.62044098]\n",
      " [ 0.69484544]]\n",
      "t [[ 0.57870183]\n",
      " [-0.62264189]\n",
      " [-0.62044098]\n",
      " ...\n",
      " [ 0.79914132]\n",
      " [-0.62044098]\n",
      " [ 0.69484544]]\n",
      "t [[ 0.63914839]\n",
      " [-0.74043456]\n",
      " [-0.70679068]\n",
      " ...\n",
      " [ 0.8775061 ]\n",
      " [-0.70679068]\n",
      " [ 0.753457  ]]\n",
      "t [[ 0.63914839]\n",
      " [-0.74043456]\n",
      " [-0.70679068]\n",
      " ...\n",
      " [ 0.8775061 ]\n",
      " [-0.70679068]\n",
      " [ 0.753457  ]]\n",
      "Current iteration=6, loss=38786.15732597865\n",
      "t [[ 0.68982806]\n",
      " [-0.85019571]\n",
      " [-0.78576422]\n",
      " ...\n",
      " [ 0.94257719]\n",
      " [-0.78576422]\n",
      " [ 0.80048251]]\n",
      "t [[ 0.68982806]\n",
      " [-0.85019571]\n",
      " [-0.78576422]\n",
      " ...\n",
      " [ 0.94257719]\n",
      " [-0.78576422]\n",
      " [ 0.80048251]]\n",
      "t [[ 0.73268531]\n",
      " [-0.95227959]\n",
      " [-0.85846874]\n",
      " ...\n",
      " [ 0.99715976]\n",
      " [-0.85846874]\n",
      " [ 0.83866885]]\n",
      "t [[ 0.73268531]\n",
      " [-0.95227959]\n",
      " [-0.85846874]\n",
      " ...\n",
      " [ 0.99715976]\n",
      " [-0.85846874]\n",
      " [ 0.83866885]]\n",
      "Current iteration=8, loss=36991.252430349035\n",
      "t [[ 0.76921373]\n",
      " [-1.0472543 ]\n",
      " [-0.92576463]\n",
      " ...\n",
      " [ 1.04335576]\n",
      " [-0.92576463]\n",
      " [ 0.87000491]]\n",
      "t [[ 0.76921373]\n",
      " [-1.0472543 ]\n",
      " [-0.92576463]\n",
      " ...\n",
      " [ 1.04335576]\n",
      " [-0.92576463]\n",
      " [ 0.87000491]]\n",
      "t [[ 0.80057662]\n",
      " [-1.13574571]\n",
      " [-0.98833702]\n",
      " ...\n",
      " [ 1.08277084]\n",
      " [-0.98833702]\n",
      " [ 0.89596464]]\n",
      "loss=35669.76436261829\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.05969123]\n",
      " [-0.28583134]\n",
      " [-0.16761478]\n",
      " ...\n",
      " [ 0.14333289]\n",
      " [ 0.05969123]\n",
      " [-0.24296589]]\n",
      "t [[ 0.05969123]\n",
      " [-0.28583134]\n",
      " [-0.16761478]\n",
      " ...\n",
      " [ 0.14333289]\n",
      " [ 0.05969123]\n",
      " [-0.24296589]]\n",
      "t [[ 0.09790359]\n",
      " [-0.53760072]\n",
      " [-0.30956238]\n",
      " ...\n",
      " [ 0.22957955]\n",
      " [ 0.09790359]\n",
      " [-0.415366  ]]\n",
      "t [[ 0.09790359]\n",
      " [-0.53760072]\n",
      " [-0.30956238]\n",
      " ...\n",
      " [ 0.22957955]\n",
      " [ 0.09790359]\n",
      " [-0.415366  ]]\n",
      "Current iteration=2, loss=45036.50294707342\n",
      "t [[ 0.12169324]\n",
      " [-0.75886725]\n",
      " [-0.43267586]\n",
      " ...\n",
      " [ 0.28152536]\n",
      " [ 0.12169324]\n",
      " [-0.54588752]]\n",
      "t [[ 0.12169324]\n",
      " [-0.75886725]\n",
      " [-0.43267586]\n",
      " ...\n",
      " [ 0.28152536]\n",
      " [ 0.12169324]\n",
      " [-0.54588752]]\n",
      "t [[ 0.13565388]\n",
      " [-0.95382027]\n",
      " [-0.54146865]\n",
      " ...\n",
      " [ 0.31219837]\n",
      " [ 0.13565388]\n",
      " [-0.65060412]]\n",
      "t [[ 0.13565388]\n",
      " [-0.95382027]\n",
      " [-0.54146865]\n",
      " ...\n",
      " [ 0.31219837]\n",
      " [ 0.13565388]\n",
      " [-0.65060412]]\n",
      "Current iteration=4, loss=41046.78638706243\n",
      "t [[ 0.1427812 ]\n",
      " [-1.12642043]\n",
      " [-0.63897264]\n",
      " ...\n",
      " [ 0.32924044]\n",
      " [ 0.1427812 ]\n",
      " [-0.73852903]]\n",
      "t [[ 0.1427812 ]\n",
      " [-1.12642043]\n",
      " [-0.63897264]\n",
      " ...\n",
      " [ 0.32924044]\n",
      " [ 0.1427812 ]\n",
      " [-0.73852903]]\n",
      "t [[ 0.14507899]\n",
      " [-1.28011631]\n",
      " [-0.72730373]\n",
      " ...\n",
      " [ 0.33727345]\n",
      " [ 0.14507899]\n",
      " [-0.81486041]]\n",
      "t [[ 0.14507899]\n",
      " [-1.28011631]\n",
      " [-0.72730373]\n",
      " ...\n",
      " [ 0.33727345]\n",
      " [ 0.14507899]\n",
      " [-0.81486041]]\n",
      "Current iteration=6, loss=38456.94335165662\n",
      "t [[ 0.14392577]\n",
      " [-1.41780226]\n",
      " [-0.80799796]\n",
      " ...\n",
      " [ 0.33918082]\n",
      " [ 0.14392577]\n",
      " [-0.88271454]]\n",
      "t [[ 0.14392577]\n",
      " [-1.41780226]\n",
      " [-0.80799796]\n",
      " ...\n",
      " [ 0.33918082]\n",
      " [ 0.14392577]\n",
      " [-0.88271454]]\n",
      "t [[ 0.14029442]\n",
      " [-1.54186848]\n",
      " [-0.88221092]\n",
      " ...\n",
      " [ 0.33682455]\n",
      " [ 0.14029442]\n",
      " [-0.94404645]]\n",
      "t [[ 0.14029442]\n",
      " [-1.54186848]\n",
      " [-0.88221092]\n",
      " ...\n",
      " [ 0.33682455]\n",
      " [ 0.14029442]\n",
      " [-0.94404645]]\n",
      "Current iteration=8, loss=36651.60075800115\n",
      "t [[ 0.13488669]\n",
      " [-1.6542788 ]\n",
      " [-0.95083871]\n",
      " ...\n",
      " [ 0.33144906]\n",
      " [ 0.13488669]\n",
      " [-1.00014319]]\n",
      "t [[ 0.13488669]\n",
      " [-1.6542788 ]\n",
      " [-0.95083871]\n",
      " ...\n",
      " [ 0.33144906]\n",
      " [ 0.13488669]\n",
      " [-1.00014319]]\n",
      "t [[ 0.12821794]\n",
      " [-1.75664824]\n",
      " [-1.014594  ]\n",
      " ...\n",
      " [ 0.32391268]\n",
      " [ 0.12821794]\n",
      " [-1.05189353]]\n",
      "loss=35328.32489409494\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18025423]\n",
      " [-0.10508099]\n",
      " [-0.16817245]\n",
      " ...\n",
      " [ 0.14123499]\n",
      " [ 0.05945688]\n",
      " [-0.239294  ]]\n",
      "t [[ 0.18025423]\n",
      " [-0.10508099]\n",
      " [-0.16817245]\n",
      " ...\n",
      " [ 0.14123499]\n",
      " [ 0.05945688]\n",
      " [-0.239294  ]]\n",
      "t [[ 0.32163306]\n",
      " [-0.238812  ]\n",
      " [-0.31040546]\n",
      " ...\n",
      " [ 0.22578955]\n",
      " [ 0.09736475]\n",
      " [-0.40806373]]\n",
      "t [[ 0.32163306]\n",
      " [-0.238812  ]\n",
      " [-0.31040546]\n",
      " ...\n",
      " [ 0.22578955]\n",
      " [ 0.09736475]\n",
      " [-0.40806373]]\n",
      "Current iteration=2, loss=45024.538708757376\n",
      "t [[ 0.43408442]\n",
      " [-0.37975806]\n",
      " [-0.43363791]\n",
      " ...\n",
      " [ 0.27638534]\n",
      " [ 0.12082302]\n",
      " [-0.53519658]]\n",
      "t [[ 0.43408442]\n",
      " [-0.37975806]\n",
      " [-0.43363791]\n",
      " ...\n",
      " [ 0.27638534]\n",
      " [ 0.12082302]\n",
      " [-0.53519658]]\n",
      "t [[ 0.52486819]\n",
      " [-0.51777234]\n",
      " [-0.54244931]\n",
      " ...\n",
      " [ 0.3059918 ]\n",
      " [ 0.13444825]\n",
      " [-0.6368377 ]]\n",
      "t [[ 0.52486819]\n",
      " [-0.51777234]\n",
      " [-0.54244931]\n",
      " ...\n",
      " [ 0.3059918 ]\n",
      " [ 0.13444825]\n",
      " [-0.6368377 ]]\n",
      "Current iteration=4, loss=41033.7416257295\n",
      "t [[ 0.59913361]\n",
      " [-0.64863286]\n",
      " [-0.6399114 ]\n",
      " ...\n",
      " [ 0.32219764]\n",
      " [ 0.14124778]\n",
      " [-0.72200706]]\n",
      "t [[ 0.59913361]\n",
      " [-0.64863286]\n",
      " [-0.6399114 ]\n",
      " ...\n",
      " [ 0.32219764]\n",
      " [ 0.14124778]\n",
      " [-0.72200706]]\n",
      "t [[ 0.66058914]\n",
      " [-0.77095154]\n",
      " [-0.72816421]\n",
      " ...\n",
      " [ 0.32957811]\n",
      " [ 0.14323135]\n",
      " [-0.79588117]]\n",
      "t [[ 0.66058914]\n",
      " [-0.77095154]\n",
      " [-0.72816421]\n",
      " ...\n",
      " [ 0.32957811]\n",
      " [ 0.14323135]\n",
      " [-0.79588117]]\n",
      "Current iteration=6, loss=38444.20735837955\n",
      "t [[ 0.71196476]\n",
      " [-0.88463167]\n",
      " [-0.80875865]\n",
      " ...\n",
      " [ 0.33097879]\n",
      " [ 0.14178035]\n",
      " [-0.8615451 ]]\n",
      "t [[ 0.71196476]\n",
      " [-0.88463167]\n",
      " [-0.80875865]\n",
      " ...\n",
      " [ 0.33097879]\n",
      " [ 0.14178035]\n",
      " [-0.8615451 ]]\n",
      "t [[ 0.75530988]\n",
      " [-0.99012876]\n",
      " [-0.88285952]\n",
      " ...\n",
      " [ 0.32823188]\n",
      " [ 0.13786883]\n",
      " [-0.92092137]]\n",
      "t [[ 0.75530988]\n",
      " [-0.99012876]\n",
      " [-0.88285952]\n",
      " ...\n",
      " [ 0.32823188]\n",
      " [ 0.13786883]\n",
      " [-0.92092137]]\n",
      "Current iteration=8, loss=36639.18113794837\n",
      "t [[ 0.79218924]\n",
      " [-1.08810149]\n",
      " [-0.95136878]\n",
      " ...\n",
      " [ 0.32255868]\n",
      " [ 0.13219877]\n",
      " [-0.97526682]]\n",
      "t [[ 0.79218924]\n",
      " [-1.08810149]\n",
      " [-0.95136878]\n",
      " ...\n",
      " [ 0.32255868]\n",
      " [ 0.13219877]\n",
      " [-0.97526682]]\n",
      "t [[ 0.82381531]\n",
      " [-1.1792504 ]\n",
      " [-1.01500282]\n",
      " ...\n",
      " [ 0.31479957]\n",
      " [ 0.12528524]\n",
      " [-1.0254435 ]]\n",
      "loss=35316.00169337884\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.17794391]\n",
      " [-0.1005927 ]\n",
      " [-0.16754262]\n",
      " ...\n",
      " [ 0.14341406]\n",
      " [ 0.06055273]\n",
      " [-0.24666307]]\n",
      "t [[ 0.17794391]\n",
      " [-0.1005927 ]\n",
      " [-0.16754262]\n",
      " ...\n",
      " [ 0.14341406]\n",
      " [ 0.06055273]\n",
      " [-0.24666307]]\n",
      "t [[ 0.31752691]\n",
      " [-0.22996586]\n",
      " [-0.30914891]\n",
      " ...\n",
      " [ 0.22862578]\n",
      " [ 0.09929956]\n",
      " [-0.42068915]]\n",
      "t [[ 0.31752691]\n",
      " [-0.22996586]\n",
      " [-0.30914891]\n",
      " ...\n",
      " [ 0.22862578]\n",
      " [ 0.09929956]\n",
      " [-0.42068915]]\n",
      "Current iteration=2, loss=45084.6004585434\n",
      "t [[ 0.42858419]\n",
      " [-0.36659313]\n",
      " [-0.43179977]\n",
      " ...\n",
      " [ 0.27903918]\n",
      " [ 0.1234359 ]\n",
      " [-0.55176716]]\n",
      "t [[ 0.42858419]\n",
      " [-0.36659313]\n",
      " [-0.43179977]\n",
      " ...\n",
      " [ 0.27903918]\n",
      " [ 0.1234359 ]\n",
      " [-0.55176716]]\n",
      "t [[ 0.51828987]\n",
      " [-0.50034025]\n",
      " [-0.54008325]\n",
      " ...\n",
      " [ 0.30796744]\n",
      " [ 0.13763108]\n",
      " [-0.65648882]]\n",
      "t [[ 0.51828987]\n",
      " [-0.50034025]\n",
      " [-0.54008325]\n",
      " ...\n",
      " [ 0.30796744]\n",
      " [ 0.13763108]\n",
      " [-0.65648882]]\n",
      "Current iteration=4, loss=41142.77940494337\n",
      "t [[ 0.59172571]\n",
      " [-0.6270181 ]\n",
      " [-0.63706675]\n",
      " ...\n",
      " [ 0.32317636]\n",
      " [ 0.14492206]\n",
      " [-0.74413117]]\n",
      "t [[ 0.59172571]\n",
      " [-0.6270181 ]\n",
      " [-0.63706675]\n",
      " ...\n",
      " [ 0.32317636]\n",
      " [ 0.14492206]\n",
      " [-0.74413117]]\n",
      "t [[ 0.65254335]\n",
      " [-0.74526879]\n",
      " [-0.724883  ]\n",
      " ...\n",
      " [ 0.3293524 ]\n",
      " [ 0.14733665]\n",
      " [-0.82003375]]\n",
      "t [[ 0.65254335]\n",
      " [-0.74526879]\n",
      " [-0.724883  ]\n",
      " ...\n",
      " [ 0.3293524 ]\n",
      " [ 0.14733665]\n",
      " [-0.82003375]]\n",
      "Current iteration=6, loss=38588.773011027064\n",
      "t [[ 0.70342749]\n",
      " [-0.85501606]\n",
      " [-0.80507572]\n",
      " ...\n",
      " [ 0.32941497]\n",
      " [ 0.1462683 ]\n",
      " [-0.88739039]]\n",
      "t [[ 0.70342749]\n",
      " [-0.85501606]\n",
      " [-0.80507572]\n",
      " ...\n",
      " [ 0.32941497]\n",
      " [ 0.1462683 ]\n",
      " [-0.88739039]]\n",
      "t [[ 0.74639271]\n",
      " [-0.95672652]\n",
      " [-0.87880351]\n",
      " ...\n",
      " [ 0.325246  ]\n",
      " [ 0.14269971]\n",
      " [-0.94819873]]\n",
      "t [[ 0.74639271]\n",
      " [-0.95672652]\n",
      " [-0.87880351]\n",
      " ...\n",
      " [ 0.325246  ]\n",
      " [ 0.14269971]\n",
      " [-0.94819873]]\n",
      "Current iteration=8, loss=36810.01877893252\n",
      "t [[ 0.78297738]\n",
      " [-1.05106304]\n",
      " [-0.9469632 ]\n",
      " ...\n",
      " [ 0.31810062]\n",
      " [ 0.13733932]\n",
      " [-1.00376906]]\n",
      "t [[ 0.78297738]\n",
      " [-1.05106304]\n",
      " [-0.9469632 ]\n",
      " ...\n",
      " [ 0.31810062]\n",
      " [ 0.13733932]\n",
      " [-1.00376906]]\n",
      "t [[ 0.81437395]\n",
      " [-1.13872571]\n",
      " [-1.01026705]\n",
      " ...\n",
      " [ 0.30884258]\n",
      " [ 0.13070723]\n",
      " [-1.05500256]]\n",
      "loss=35506.86373460191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.17765907]\n",
      " [-0.10568267]\n",
      " [-0.16701636]\n",
      " ...\n",
      " [ 0.25588492]\n",
      " [-0.16701636]\n",
      " [ 0.23967248]]\n",
      "t [[ 0.17765907]\n",
      " [-0.10568267]\n",
      " [-0.16701636]\n",
      " ...\n",
      " [ 0.25588492]\n",
      " [-0.16701636]\n",
      " [ 0.23967248]]\n",
      "t [[ 0.31700875]\n",
      " [-0.23858995]\n",
      " [-0.30831958]\n",
      " ...\n",
      " [ 0.44975121]\n",
      " [-0.30831958]\n",
      " [ 0.41122371]]\n",
      "t [[ 0.31700875]\n",
      " [-0.23858995]\n",
      " [-0.30831958]\n",
      " ...\n",
      " [ 0.44975121]\n",
      " [-0.30831958]\n",
      " [ 0.41122371]]\n",
      "Current iteration=2, loss=45088.22488132517\n",
      "t [[ 0.42786867]\n",
      " [-0.37794372]\n",
      " [-0.43077083]\n",
      " ...\n",
      " [ 0.59998357]\n",
      " [-0.43077083]\n",
      " [ 0.53753374]]\n",
      "t [[ 0.42786867]\n",
      " [-0.37794372]\n",
      " [-0.43077083]\n",
      " ...\n",
      " [ 0.59998357]\n",
      " [-0.43077083]\n",
      " [ 0.53753374]]\n",
      "t [[ 0.51731895]\n",
      " [-0.51404589]\n",
      " [-0.53890589]\n",
      " ...\n",
      " [ 0.71883519]\n",
      " [-0.53890589]\n",
      " [ 0.63300851]]\n",
      "t [[ 0.51731895]\n",
      " [-0.51404589]\n",
      " [-0.53890589]\n",
      " ...\n",
      " [ 0.71883519]\n",
      " [-0.53890589]\n",
      " [ 0.63300851]]\n",
      "Current iteration=4, loss=41136.74775502001\n",
      "t [[ 0.59041727]\n",
      " [-0.64290094]\n",
      " [-0.63576679]\n",
      " ...\n",
      " [ 0.81450896]\n",
      " [-0.63576679]\n",
      " [ 0.70677151]]\n",
      "t [[ 0.59041727]\n",
      " [-0.64290094]\n",
      " [-0.63576679]\n",
      " ...\n",
      " [ 0.81450896]\n",
      " [-0.63576679]\n",
      " [ 0.70677151]]\n",
      "t [[ 0.65082833]\n",
      " [-0.76322553]\n",
      " [-0.7234737 ]\n",
      " ...\n",
      " [ 0.89264866]\n",
      " [-0.7234737 ]\n",
      " [ 0.76478835]]\n",
      "t [[ 0.65082833]\n",
      " [-0.76322553]\n",
      " [-0.7234737 ]\n",
      " ...\n",
      " [ 0.89264866]\n",
      " [-0.7234737 ]\n",
      " [ 0.76478835]]\n",
      "Current iteration=6, loss=38574.38158832221\n",
      "t [[ 0.7012563 ]\n",
      " [-0.87496981]\n",
      " [-0.80356449]\n",
      " ...\n",
      " [ 0.95725346]\n",
      " [-0.80356449]\n",
      " [ 0.81109923]]\n",
      "t [[ 0.7012563 ]\n",
      " [-0.87496981]\n",
      " [-0.80356449]\n",
      " ...\n",
      " [ 0.95725346]\n",
      " [-0.80356449]\n",
      " [ 0.81109923]]\n",
      "t [[ 0.74373241]\n",
      " [-0.97860828]\n",
      " [-0.87719521]\n",
      " ...\n",
      " [ 1.01123448]\n",
      " [-0.87719521]\n",
      " [ 0.84853083]]\n",
      "t [[ 0.74373241]\n",
      " [-0.97860828]\n",
      " [-0.87719521]\n",
      " ...\n",
      " [ 1.01123448]\n",
      " [-0.87719521]\n",
      " [ 0.84853083]]\n",
      "Current iteration=8, loss=36789.18438096184\n",
      "t [[ 0.77980759]\n",
      " [-1.07480515]\n",
      " [-0.94526171]\n",
      " ...\n",
      " [ 1.05676106]\n",
      " [-0.94526171]\n",
      " [ 0.87911785]]\n",
      "t [[ 0.77980759]\n",
      " [-1.07480515]\n",
      " [-0.94526171]\n",
      " ...\n",
      " [ 1.05676106]\n",
      " [-0.94526171]\n",
      " [ 0.87911785]]\n",
      "t [[ 0.81068341]\n",
      " [-1.16426007]\n",
      " [-1.00847593]\n",
      " ...\n",
      " [ 1.09548237]\n",
      " [-1.00847593]\n",
      " [ 0.90436077]]\n",
      "loss=35481.08293621003\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06155658]\n",
      " [-0.29476357]\n",
      " [-0.17285274]\n",
      " ...\n",
      " [ 0.14781204]\n",
      " [ 0.06155658]\n",
      " [-0.25055857]]\n",
      "t [[ 0.06155658]\n",
      " [-0.29476357]\n",
      " [-0.17285274]\n",
      " ...\n",
      " [ 0.14781204]\n",
      " [ 0.06155658]\n",
      " [-0.25055857]]\n",
      "t [[ 0.10028162]\n",
      " [-0.55331395]\n",
      " [-0.31842027]\n",
      " ...\n",
      " [ 0.23494106]\n",
      " [ 0.10028162]\n",
      " [-0.42610769]]\n",
      "t [[ 0.10028162]\n",
      " [-0.55331395]\n",
      " [-0.31842027]\n",
      " ...\n",
      " [ 0.23494106]\n",
      " [ 0.10028162]\n",
      " [-0.42610769]]\n",
      "Current iteration=2, loss=44868.03155046554\n",
      "t [[ 0.12391306]\n",
      " [-0.77956996]\n",
      " [-0.44419818]\n",
      " ...\n",
      " [ 0.28635761]\n",
      " [ 0.12391306]\n",
      " [-0.55808995]]\n",
      "t [[ 0.12391306]\n",
      " [-0.77956996]\n",
      " [-0.44419818]\n",
      " ...\n",
      " [ 0.28635761]\n",
      " [ 0.12391306]\n",
      " [-0.55808995]]\n",
      "t [[ 0.13739285]\n",
      " [-0.97813842]\n",
      " [-0.5550468 ]\n",
      " ...\n",
      " [ 0.31599966]\n",
      " [ 0.13739285]\n",
      " [-0.66366888]]\n",
      "t [[ 0.13739285]\n",
      " [-0.97813842]\n",
      " [-0.5550468 ]\n",
      " ...\n",
      " [ 0.31599966]\n",
      " [ 0.13739285]\n",
      " [-0.66366888]]\n",
      "Current iteration=4, loss=40840.665940907675\n",
      "t [[ 0.1438943 ]\n",
      " [-1.15333478]\n",
      " [-0.65418664]\n",
      " ...\n",
      " [ 0.33188136]\n",
      " [ 0.1438943 ]\n",
      " [-0.75225152]]\n",
      "t [[ 0.1438943 ]\n",
      " [-1.15333478]\n",
      " [-0.65418664]\n",
      " ...\n",
      " [ 0.33188136]\n",
      " [ 0.1438943 ]\n",
      " [-0.75225152]]\n",
      "t [[ 0.14551471]\n",
      " [-1.30887957]\n",
      " [-0.7438445 ]\n",
      " ...\n",
      " [ 0.338769  ]\n",
      " [ 0.14551471]\n",
      " [-0.82916286]]\n",
      "t [[ 0.14551471]\n",
      " [-1.30887957]\n",
      " [-0.7438445 ]\n",
      " ...\n",
      " [ 0.338769  ]\n",
      " [ 0.14551471]\n",
      " [-0.82916286]]\n",
      "Current iteration=6, loss=38249.78225377886\n",
      "t [[ 0.14368325]\n",
      " [-1.44786646]\n",
      " [-0.82562732]\n",
      " ...\n",
      " [ 0.33959742]\n",
      " [ 0.14368325]\n",
      " [-0.89755132]]\n",
      "t [[ 0.14368325]\n",
      " [-1.44786646]\n",
      " [-0.82562732]\n",
      " ...\n",
      " [ 0.33959742]\n",
      " [ 0.14368325]\n",
      " [-0.89755132]]\n",
      "t [[ 0.13940062]\n",
      " [-1.57282946]\n",
      " [-0.90073935]\n",
      " ...\n",
      " [ 0.33624355]\n",
      " [ 0.13940062]\n",
      " [-0.95937444]]\n",
      "t [[ 0.13940062]\n",
      " [-1.57282946]\n",
      " [-0.90073935]\n",
      " ...\n",
      " [ 0.33624355]\n",
      " [ 0.13940062]\n",
      " [-0.95937444]]\n",
      "Current iteration=8, loss=36454.93754517238\n",
      "t [[ 0.13338364]\n",
      " [-1.68583598]\n",
      " [-0.97011206]\n",
      " ...\n",
      " [ 0.32995297]\n",
      " [ 0.13338364]\n",
      " [-1.01591517]]\n",
      "t [[ 0.13338364]\n",
      " [-1.68583598]\n",
      " [-0.97011206]\n",
      " ...\n",
      " [ 0.32995297]\n",
      " [ 0.13338364]\n",
      " [-1.01591517]]\n",
      "t [[ 0.12615537]\n",
      " [-1.7885761 ]\n",
      " [-1.03448493]\n",
      " ...\n",
      " [ 0.32158029]\n",
      " [ 0.12615537]\n",
      " [-1.06805883]]\n",
      "loss=35145.41833675407\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18588717]\n",
      " [-0.10836477]\n",
      " [-0.17342784]\n",
      " ...\n",
      " [ 0.14564858]\n",
      " [ 0.06131491]\n",
      " [-0.24677194]]\n",
      "t [[ 0.18588717]\n",
      " [-0.10836477]\n",
      " [-0.17342784]\n",
      " ...\n",
      " [ 0.14564858]\n",
      " [ 0.06131491]\n",
      " [-0.24677194]]\n",
      "t [[ 0.33045126]\n",
      " [-0.24718101]\n",
      " [-0.31928113]\n",
      " ...\n",
      " [ 0.23104537]\n",
      " [ 0.09972378]\n",
      " [-0.41857876]]\n",
      "t [[ 0.33045126]\n",
      " [-0.24718101]\n",
      " [-0.31928113]\n",
      " ...\n",
      " [ 0.23104537]\n",
      " [ 0.09972378]\n",
      " [-0.41857876]]\n",
      "Current iteration=2, loss=44855.93251051697\n",
      "t [[ 0.444609  ]\n",
      " [-0.39297039]\n",
      " [-0.44517144]\n",
      " ...\n",
      " [ 0.28109119]\n",
      " [ 0.12301182]\n",
      " [-0.547082  ]]\n",
      "t [[ 0.444609  ]\n",
      " [-0.39297039]\n",
      " [-0.44517144]\n",
      " ...\n",
      " [ 0.28109119]\n",
      " [ 0.12301182]\n",
      " [-0.547082  ]]\n",
      "t [[ 0.53620531]\n",
      " [-0.53501443]\n",
      " [-0.55602997]\n",
      " ...\n",
      " [ 0.30966006]\n",
      " [ 0.13614532]\n",
      " [-0.64951902]]\n",
      "t [[ 0.53620531]\n",
      " [-0.53501443]\n",
      " [-0.55602997]\n",
      " ...\n",
      " [ 0.30966006]\n",
      " [ 0.13614532]\n",
      " [-0.64951902]]\n",
      "Current iteration=4, loss=40827.61911663659\n",
      "t [[ 0.61073414]\n",
      " [-0.66906841]\n",
      " [-0.65511911]\n",
      " ...\n",
      " [ 0.32470806]\n",
      " [ 0.14230966]\n",
      " [-0.73530003]]\n",
      "t [[ 0.61073414]\n",
      " [-0.66906841]\n",
      " [-0.65511911]\n",
      " ...\n",
      " [ 0.32470806]\n",
      " [ 0.14230966]\n",
      " [-0.73530003]]\n",
      "t [[ 0.67211481]\n",
      " [-0.79387573]\n",
      " [-0.74469062]\n",
      " ...\n",
      " [ 0.33095129]\n",
      " [ 0.14360812]\n",
      " [-0.80972388]]\n",
      "t [[ 0.67211481]\n",
      " [-0.79387573]\n",
      " [-0.74469062]\n",
      " ...\n",
      " [ 0.33095129]\n",
      " [ 0.14360812]\n",
      " [-0.80972388]]\n",
      "Current iteration=6, loss=38237.085825713664\n",
      "t [[ 0.72321045]\n",
      " [-0.90948892]\n",
      " [-0.8263665 ]\n",
      " ...\n",
      " [ 0.33128434]\n",
      " [ 0.14147265]\n",
      " [-0.87590366]]\n",
      "t [[ 0.72321045]\n",
      " [-0.90948892]\n",
      " [-0.8263665 ]\n",
      " ...\n",
      " [ 0.33128434]\n",
      " [ 0.14147265]\n",
      " [-0.87590366]]\n",
      "t [[ 0.76615624]\n",
      " [-1.01649101]\n",
      " [-0.90136025]\n",
      " ...\n",
      " [ 0.32755287]\n",
      " [ 0.13690495]\n",
      " [-0.93576124]]\n",
      "t [[ 0.76615624]\n",
      " [-1.01649101]\n",
      " [-0.90136025]\n",
      " ...\n",
      " [ 0.32755287]\n",
      " [ 0.13690495]\n",
      " [-0.93576124]]\n",
      "Current iteration=8, loss=36442.54944732863\n",
      "t [[ 0.80257305]\n",
      " [-1.11564016]\n",
      " [-0.9706091 ]\n",
      " ...\n",
      " [ 0.3209784 ]\n",
      " [ 0.13062195]\n",
      " [-0.99054698]]\n",
      "t [[ 0.80257305]\n",
      " [-1.11564016]\n",
      " [-0.9706091 ]\n",
      " ...\n",
      " [ 0.3209784 ]\n",
      " [ 0.13062195]\n",
      " [-0.99054698]]\n",
      "t [[ 0.83371035]\n",
      " [-1.20771192]\n",
      " [-1.03485615]\n",
      " ...\n",
      " [ 0.31239707]\n",
      " [ 0.12314624]\n",
      " [-1.04111773]]\n",
      "loss=35133.09641835651\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.18350466]\n",
      " [-0.10373623]\n",
      " [-0.17277833]\n",
      " ...\n",
      " [ 0.14789575]\n",
      " [ 0.062445  ]\n",
      " [-0.25437129]]\n",
      "t [[ 0.18350466]\n",
      " [-0.10373623]\n",
      " [-0.17277833]\n",
      " ...\n",
      " [ 0.14789575]\n",
      " [ 0.062445  ]\n",
      " [-0.25437129]]\n",
      "t [[ 0.326233  ]\n",
      " [-0.23806257]\n",
      " [-0.3179856 ]\n",
      " ...\n",
      " [ 0.23392371]\n",
      " [ 0.10171109]\n",
      " [-0.43153303]]\n",
      "t [[ 0.326233  ]\n",
      " [-0.23806257]\n",
      " [-0.3179856 ]\n",
      " ...\n",
      " [ 0.23392371]\n",
      " [ 0.10171109]\n",
      " [-0.43153303]]\n",
      "Current iteration=2, loss=44917.81780374318\n",
      "t [[ 0.43897818]\n",
      " [-0.37940077]\n",
      " [-0.44327918]\n",
      " ...\n",
      " [ 0.2837305 ]\n",
      " [ 0.12568851]\n",
      " [-0.56402424]]\n",
      "t [[ 0.43897818]\n",
      " [-0.37940077]\n",
      " [-0.44327918]\n",
      " ...\n",
      " [ 0.2837305 ]\n",
      " [ 0.12568851]\n",
      " [-0.56402424]]\n",
      "t [[ 0.52949239]\n",
      " [-0.517049  ]\n",
      " [-0.55359832]\n",
      " ...\n",
      " [ 0.31155306]\n",
      " [ 0.13939968]\n",
      " [-0.66955782]]\n",
      "t [[ 0.52949239]\n",
      " [-0.517049  ]\n",
      " [-0.55359832]\n",
      " ...\n",
      " [ 0.31155306]\n",
      " [ 0.13939968]\n",
      " [-0.66955782]]\n",
      "Current iteration=4, loss=40939.3443833488\n",
      "t [[ 0.60319633]\n",
      " [-0.64679995]\n",
      " [-0.65219995]\n",
      " ...\n",
      " [ 0.3255327 ]\n",
      " [ 0.14606091]\n",
      " [-0.7578138 ]]\n",
      "t [[ 0.60319633]\n",
      " [-0.64679995]\n",
      " [-0.65219995]\n",
      " ...\n",
      " [ 0.3255327 ]\n",
      " [ 0.14606091]\n",
      " [-0.7578138 ]]\n",
      "t [[ 0.66394842]\n",
      " [-0.76742975]\n",
      " [-0.74132762]\n",
      " ...\n",
      " [ 0.33050121]\n",
      " [ 0.14779428]\n",
      " [-0.83426055]]\n",
      "t [[ 0.66394842]\n",
      " [-0.76742975]\n",
      " [-0.74132762]\n",
      " ...\n",
      " [ 0.33050121]\n",
      " [ 0.14779428]\n",
      " [-0.83426055]]\n",
      "Current iteration=6, loss=38384.58334182034\n",
      "t [[ 0.71456387]\n",
      " [-0.87901204]\n",
      " [-0.82259554]\n",
      " ...\n",
      " [ 0.32942911]\n",
      " [ 0.14604418]\n",
      " [-0.90212329]]\n",
      "t [[ 0.71456387]\n",
      " [-0.87901204]\n",
      " [-0.82259554]\n",
      " ...\n",
      " [ 0.32942911]\n",
      " [ 0.14604418]\n",
      " [-0.90212329]]\n",
      "t [[ 0.75714157]\n",
      " [-0.98214059]\n",
      " [-0.89721058]\n",
      " ...\n",
      " [ 0.3242126 ]\n",
      " [ 0.14182127]\n",
      " [-0.96340079]]\n",
      "t [[ 0.75714157]\n",
      " [-0.98214059]\n",
      " [-0.89721058]\n",
      " ...\n",
      " [ 0.3242126 ]\n",
      " [ 0.14182127]\n",
      " [-0.96340079]]\n",
      "Current iteration=8, loss=36616.31417041226\n",
      "t [[ 0.79327513]\n",
      " [-1.07757692]\n",
      " [-0.96610459]\n",
      " ...\n",
      " [ 0.31610724]\n",
      " [ 0.13584914]\n",
      " [-1.01939785]]\n",
      "t [[ 0.79327513]\n",
      " [-1.07757692]\n",
      " [-0.96610459]\n",
      " ...\n",
      " [ 0.31610724]\n",
      " [ 0.13584914]\n",
      " [-1.01939785]]\n",
      "t [[ 0.82419357]\n",
      " [-1.16609515]\n",
      " [-1.03001634]\n",
      " ...\n",
      " [ 0.30597246]\n",
      " [ 0.12865559]\n",
      " [-1.07101098]]\n",
      "loss=35326.7784768146\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18321091]\n",
      " [-0.10898526]\n",
      " [-0.17223562]\n",
      " ...\n",
      " [ 0.26388133]\n",
      " [-0.17223562]\n",
      " [ 0.24716225]]\n",
      "t [[ 0.18321091]\n",
      " [-0.10898526]\n",
      " [-0.17223562]\n",
      " ...\n",
      " [ 0.26388133]\n",
      " [-0.17223562]\n",
      " [ 0.24716225]]\n",
      "t [[ 0.32569904]\n",
      " [-0.24690811]\n",
      " [-0.31713726]\n",
      " ...\n",
      " [ 0.46183719]\n",
      " [-0.31713726]\n",
      " [ 0.42191312]]\n",
      "t [[ 0.32569904]\n",
      " [-0.24690811]\n",
      " [-0.31713726]\n",
      " ...\n",
      " [ 0.46183719]\n",
      " [-0.31713726]\n",
      " [ 0.42191312]]\n",
      "Current iteration=2, loss=44921.22134243199\n",
      "t [[ 0.43823821]\n",
      " [-0.39101216]\n",
      " [-0.44223134]\n",
      " ...\n",
      " [ 0.61403186]\n",
      " [-0.44223134]\n",
      " [ 0.54934095]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.43823821]\n",
      " [-0.39101216]\n",
      " [-0.44223134]\n",
      " ...\n",
      " [ 0.61403186]\n",
      " [-0.44223134]\n",
      " [ 0.54934095]]\n",
      "t [[ 0.52848057]\n",
      " [-0.53105784]\n",
      " [-0.55240206]\n",
      " ...\n",
      " [ 0.73366297]\n",
      " [-0.55240206]\n",
      " [ 0.64491952]]\n",
      "t [[ 0.52848057]\n",
      " [-0.53105784]\n",
      " [-0.55240206]\n",
      " ...\n",
      " [ 0.73366297]\n",
      " [-0.55240206]\n",
      " [ 0.64491952]]\n",
      "Current iteration=4, loss=40932.728476254895\n",
      "t [[ 0.60182587]\n",
      " [-0.66303337]\n",
      " [-0.65088032]\n",
      " ...\n",
      " [ 0.82944048]\n",
      " [-0.65088032]\n",
      " [ 0.71828781]]\n",
      "t [[ 0.60182587]\n",
      " [-0.66303337]\n",
      " [-0.65088032]\n",
      " ...\n",
      " [ 0.82944048]\n",
      " [-0.65088032]\n",
      " [ 0.71828781]]\n",
      "t [[ 0.66214885]\n",
      " [-0.78578625]\n",
      " [-0.7398972 ]\n",
      " ...\n",
      " [ 0.90729323]\n",
      " [-0.7398972 ]\n",
      " [ 0.77567089]]\n",
      "t [[ 0.66214885]\n",
      " [-0.78578625]\n",
      " [-0.7398972 ]\n",
      " ...\n",
      " [ 0.90729323]\n",
      " [-0.7398972 ]\n",
      " [ 0.77567089]]\n",
      "Current iteration=6, loss=38369.49423354805\n",
      "t [[ 0.7122859 ]\n",
      " [-0.89941359]\n",
      " [-0.82106135]\n",
      " ...\n",
      " [ 0.97138818]\n",
      " [-0.82106135]\n",
      " [ 0.8212458 ]]\n",
      "t [[ 0.7122859 ]\n",
      " [-0.89941359]\n",
      " [-0.82106135]\n",
      " ...\n",
      " [ 0.97138818]\n",
      " [-0.82106135]\n",
      " [ 0.8212458 ]]\n",
      "t [[ 0.75435335]\n",
      " [-1.00451515]\n",
      " [-0.89557738]\n",
      " ...\n",
      " [ 1.02473904]\n",
      " [-0.89557738]\n",
      " [ 0.85791442]]\n",
      "t [[ 0.75435335]\n",
      " [-1.00451515]\n",
      " [-0.89557738]\n",
      " ...\n",
      " [ 1.02473904]\n",
      " [-0.89557738]\n",
      " [ 0.85791442]]\n",
      "Current iteration=8, loss=36594.76174220145\n",
      "t [[ 0.78995769]\n",
      " [-1.10185273]\n",
      " [-0.96437633]\n",
      " ...\n",
      " [ 1.06957966]\n",
      " [-0.96437633]\n",
      " [ 0.8877539 ]]\n",
      "t [[ 0.78995769]\n",
      " [-1.10185273]\n",
      " [-0.96437633]\n",
      " ...\n",
      " [ 1.06957966]\n",
      " [-0.96437633]\n",
      " [ 0.8877539 ]]\n",
      "t [[ 0.82033705]\n",
      " [-1.19219965]\n",
      " [-1.02819674]\n",
      " ...\n",
      " [ 1.10760047]\n",
      " [-1.02819674]\n",
      " [ 0.91228884]]\n",
      "loss=35300.30714940682\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06342193]\n",
      " [-0.3036958 ]\n",
      " [-0.1780907 ]\n",
      " ...\n",
      " [ 0.15229119]\n",
      " [ 0.06342193]\n",
      " [-0.25815126]]\n",
      "t [[ 0.06342193]\n",
      " [-0.3036958 ]\n",
      " [-0.1780907 ]\n",
      " ...\n",
      " [ 0.15229119]\n",
      " [ 0.06342193]\n",
      " [-0.25815126]]\n",
      "t [[ 0.10261936]\n",
      " [-0.56896233]\n",
      " [-0.32722975]\n",
      " ...\n",
      " [ 0.24019524]\n",
      " [ 0.10261936]\n",
      " [-0.43671704]]\n",
      "t [[ 0.10261936]\n",
      " [-0.56896233]\n",
      " [-0.32722975]\n",
      " ...\n",
      " [ 0.24019524]\n",
      " [ 0.10261936]\n",
      " [-0.43671704]]\n",
      "Current iteration=2, loss=44701.912604221114\n",
      "t [[ 0.12605365]\n",
      " [-0.80010194]\n",
      " [-0.45561606]\n",
      " ...\n",
      " [ 0.2910001 ]\n",
      " [ 0.12605365]\n",
      " [-0.57006219]]\n",
      "t [[ 0.12605365]\n",
      " [-0.80010194]\n",
      " [-0.45561606]\n",
      " ...\n",
      " [ 0.2910001 ]\n",
      " [ 0.12605365]\n",
      " [-0.57006219]]\n",
      "t [[ 0.13902473]\n",
      " [-1.00216501]\n",
      " [-0.56846713]\n",
      " ...\n",
      " [ 0.31956829]\n",
      " [ 0.13902473]\n",
      " [-0.67645181]]\n",
      "t [[ 0.13902473]\n",
      " [-1.00216501]\n",
      " [-0.56846713]\n",
      " ...\n",
      " [ 0.31956829]\n",
      " [ 0.13902473]\n",
      " [-0.67645181]]\n",
      "Current iteration=4, loss=40639.66526311698\n",
      "t [[ 0.14488369]\n",
      " [-1.17983834]\n",
      " [-0.66919357]\n",
      " ...\n",
      " [ 0.33427538]\n",
      " [ 0.14488369]\n",
      " [-0.76566847]]\n",
      "t [[ 0.14488369]\n",
      " [-1.17983834]\n",
      " [-0.66919357]\n",
      " ...\n",
      " [ 0.33427538]\n",
      " [ 0.14488369]\n",
      " [-0.76566847]]\n",
      "t [[ 0.14581944]\n",
      " [-1.33712261]\n",
      " [-0.76013274]\n",
      " ...\n",
      " [ 0.34002043]\n",
      " [ 0.14581944]\n",
      " [-0.8431483 ]]\n",
      "t [[ 0.14581944]\n",
      " [-1.33712261]\n",
      " [-0.76013274]\n",
      " ...\n",
      " [ 0.34002043]\n",
      " [ 0.14581944]\n",
      " [-0.8431483 ]]\n",
      "Current iteration=6, loss=38049.26488459002\n",
      "t [[ 0.14330977]\n",
      " [-1.47731385]\n",
      " [-0.84296192]\n",
      " ...\n",
      " [ 0.33978195]\n",
      " [ 0.14330977]\n",
      " [-0.91206282]]\n",
      "t [[ 0.14330977]\n",
      " [-1.47731385]\n",
      " [-0.84296192]\n",
      " ...\n",
      " [ 0.33978195]\n",
      " [ 0.14330977]\n",
      " [-0.91206282]]\n",
      "t [[ 0.13838153]\n",
      " [-1.60309017]\n",
      " [-0.91893367]\n",
      " ...\n",
      " [ 0.33544746]\n",
      " [ 0.13838153]\n",
      " [-0.97436797]]\n",
      "t [[ 0.13838153]\n",
      " [-1.60309017]\n",
      " [-0.91893367]\n",
      " ...\n",
      " [ 0.33544746]\n",
      " [ 0.13838153]\n",
      " [-0.97436797]]\n",
      "Current iteration=8, loss=36265.59795384726\n",
      "t [[ 0.13176531]\n",
      " [-1.71662166]\n",
      " [-0.98901461]\n",
      " ...\n",
      " [ 0.32826142]\n",
      " [ 0.13176531]\n",
      " [-1.03134125]]\n",
      "t [[ 0.13176531]\n",
      " [-1.71662166]\n",
      " [-0.98901461]\n",
      " ...\n",
      " [ 0.32826142]\n",
      " [ 0.13176531]\n",
      " [-1.03134125]]\n",
      "t [[ 0.1239908 ]\n",
      " [-1.81967208]\n",
      " [-1.05397085]\n",
      " ...\n",
      " [ 0.31907352]\n",
      " [ 0.1239908 ]\n",
      " [-1.0838644 ]]\n",
      "loss=34970.04487319554\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.19152011]\n",
      " [-0.11164855]\n",
      " [-0.17868323]\n",
      " ...\n",
      " [ 0.15006218]\n",
      " [ 0.06317293]\n",
      " [-0.25424988]]\n",
      "t [[ 0.19152011]\n",
      " [-0.11164855]\n",
      " [-0.17868323]\n",
      " ...\n",
      " [ 0.15006218]\n",
      " [ 0.06317293]\n",
      " [-0.25424988]]\n",
      "t [[ 0.33919659]\n",
      " [-0.25560335]\n",
      " [-0.3281079 ]\n",
      " ...\n",
      " [ 0.23619461]\n",
      " [ 0.10204238]\n",
      " [-0.42896155]]\n",
      "t [[ 0.33919659]\n",
      " [-0.25560335]\n",
      " [-0.3281079 ]\n",
      " ...\n",
      " [ 0.23619461]\n",
      " [ 0.10204238]\n",
      " [-0.42896155]]\n",
      "Current iteration=2, loss=44689.68793358778\n",
      "t [[ 0.45497476]\n",
      " [-0.40622064]\n",
      " [-0.45659965]\n",
      " ...\n",
      " [ 0.2856092 ]\n",
      " [ 0.12512126]\n",
      " [-0.55873865]]\n",
      "t [[ 0.45497476]\n",
      " [-0.40622064]\n",
      " [-0.45659965]\n",
      " ...\n",
      " [ 0.2856092 ]\n",
      " [ 0.12512126]\n",
      " [-0.55873865]]\n",
      "t [[ 0.5473057 ]\n",
      " [-0.55222288]\n",
      " [-0.56945174]\n",
      " ...\n",
      " [ 0.31309878]\n",
      " [ 0.13773526]\n",
      " [-0.661922  ]]\n",
      "t [[ 0.5473057 ]\n",
      " [-0.55222288]\n",
      " [-0.56945174]\n",
      " ...\n",
      " [ 0.31309878]\n",
      " [ 0.13773526]\n",
      " [-0.661922  ]]\n",
      "Current iteration=4, loss=40626.62198611691\n",
      "t [[ 0.62203389]\n",
      " [-0.68937261]\n",
      " [-0.67011868]\n",
      " ...\n",
      " [ 0.32697577]\n",
      " [ 0.14324796]\n",
      " [-0.74829336]]\n",
      "t [[ 0.62203389]\n",
      " [-0.68937261]\n",
      " [-0.67011868]\n",
      " ...\n",
      " [ 0.32697577]\n",
      " [ 0.14324796]\n",
      " [-0.74829336]]\n",
      "t [[ 0.68329023]\n",
      " [-0.81656531]\n",
      " [-0.76096351]\n",
      " ...\n",
      " [ 0.33208532]\n",
      " [ 0.14385427]\n",
      " [-0.82325776]]\n",
      "t [[ 0.68329023]\n",
      " [-0.81656531]\n",
      " [-0.76096351]\n",
      " ...\n",
      " [ 0.33208532]\n",
      " [ 0.14385427]\n",
      " [-0.82325776]]\n",
      "Current iteration=6, loss=38036.60832430019\n",
      "t [[ 0.73406966]\n",
      " [-0.93401346]\n",
      " [-0.84367877]\n",
      " ...\n",
      " [ 0.33136336]\n",
      " [ 0.14103462]\n",
      " [-0.88994719]]\n",
      "t [[ 0.73406966]\n",
      " [-0.93401346]\n",
      " [-0.84367877]\n",
      " ...\n",
      " [ 0.33136336]\n",
      " [ 0.14103462]\n",
      " [-0.88994719]]\n",
      "t [[ 0.77659139]\n",
      " [-1.04243197]\n",
      " [-0.91952624]\n",
      " ...\n",
      " [ 0.32666466]\n",
      " [ 0.13581671]\n",
      " [-0.95027866]]\n",
      "t [[ 0.77659139]\n",
      " [-1.04243197]\n",
      " [-0.91952624]\n",
      " ...\n",
      " [ 0.32666466]\n",
      " [ 0.13581671]\n",
      " [-0.95027866]]\n",
      "Current iteration=8, loss=36253.238010275636\n",
      "t [[ 0.81253054]\n",
      " [-1.14267949]\n",
      " [-0.9894782 ]\n",
      " ...\n",
      " [ 0.31920876]\n",
      " [ 0.12893105]\n",
      " [-1.00549471]]\n",
      "t [[ 0.81253054]\n",
      " [-1.14267949]\n",
      " [-0.9894782 ]\n",
      " ...\n",
      " [ 0.31920876]\n",
      " [ 0.12893105]\n",
      " [-1.00549471]]\n",
      "t [[ 0.84317188]\n",
      " [-1.23560607]\n",
      " [-1.05430424]\n",
      " ...\n",
      " [ 0.30982635]\n",
      " [ 0.12090672]\n",
      " [-1.05644689]]\n",
      "loss=34957.71896348053\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.1890654 ]\n",
      " [-0.10687975]\n",
      " [-0.17801404]\n",
      " ...\n",
      " [ 0.15237744]\n",
      " [ 0.06433727]\n",
      " [-0.26207951]]\n",
      "t [[ 0.1890654 ]\n",
      " [-0.10687975]\n",
      " [-0.17801404]\n",
      " ...\n",
      " [ 0.15237744]\n",
      " [ 0.06433727]\n",
      " [-0.26207951]]\n",
      "t [[ 0.33486717]\n",
      " [-0.24621284]\n",
      " [-0.32677344]\n",
      " ...\n",
      " [ 0.23911245]\n",
      " [ 0.10408174]\n",
      " [-0.44224091]]\n",
      "t [[ 0.33486717]\n",
      " [-0.24621284]\n",
      " [-0.32677344]\n",
      " ...\n",
      " [ 0.23911245]\n",
      " [ 0.10408174]\n",
      " [-0.44224091]]\n",
      "Current iteration=2, loss=44753.38805698598\n",
      "t [[ 0.4492156 ]\n",
      " [-0.39224654]\n",
      " [-0.45465356]\n",
      " ...\n",
      " [ 0.28822956]\n",
      " [ 0.12786089]\n",
      " [-0.5760456 ]]\n",
      "t [[ 0.4492156 ]\n",
      " [-0.39224654]\n",
      " [-0.45465356]\n",
      " ...\n",
      " [ 0.28822956]\n",
      " [ 0.12786089]\n",
      " [-0.5760456 ]]\n",
      "t [[ 0.54046163]\n",
      " [-0.53372463]\n",
      " [-0.56695509]\n",
      " ...\n",
      " [ 0.31490362]\n",
      " [ 0.14105995]\n",
      " [-0.68233918]]\n",
      "t [[ 0.54046163]\n",
      " [-0.53372463]\n",
      " [-0.56695509]\n",
      " ...\n",
      " [ 0.31490362]\n",
      " [ 0.14105995]\n",
      " [-0.68233918]]\n",
      "Current iteration=4, loss=40740.98689312955\n",
      "t [[ 0.6143706 ]\n",
      " [-0.66645193]\n",
      " [-0.66712587]\n",
      " ...\n",
      " [ 0.32764041]\n",
      " [ 0.14707473]\n",
      " [-0.77118586]]\n",
      "t [[ 0.6143706 ]\n",
      " [-0.66645193]\n",
      " [-0.66712587]\n",
      " ...\n",
      " [ 0.32764041]\n",
      " [ 0.14707473]\n",
      " [-0.77118586]]\n",
      "t [[ 0.67500824]\n",
      " [-0.78935913]\n",
      " [-0.75751981]\n",
      " ...\n",
      " [ 0.3314051 ]\n",
      " [ 0.14811957]\n",
      " [-0.84816648]]\n",
      "t [[ 0.67500824]\n",
      " [-0.78935913]\n",
      " [-0.75751981]\n",
      " ...\n",
      " [ 0.3314051 ]\n",
      " [ 0.14811957]\n",
      " [-0.84816648]]\n",
      "Current iteration=6, loss=38186.96124568981\n",
      "t [[ 0.7253191 ]\n",
      " [-0.9026803 ]\n",
      " [-0.83982103]\n",
      " ...\n",
      " [ 0.32921151]\n",
      " [ 0.14568781]\n",
      " [-0.91652836]]\n",
      "t [[ 0.7253191 ]\n",
      " [-0.9026803 ]\n",
      " [-0.83982103]\n",
      " ...\n",
      " [ 0.32921151]\n",
      " [ 0.14568781]\n",
      " [-0.91652836]]\n",
      "t [[ 0.76748461]\n",
      " [-1.00714059]\n",
      " [-0.91528428]\n",
      " ...\n",
      " [ 0.32296562]\n",
      " [ 0.14081635]\n",
      " [-0.97826708]]\n",
      "t [[ 0.76748461]\n",
      " [-1.00714059]\n",
      " [-0.91528428]\n",
      " ...\n",
      " [ 0.32296562]\n",
      " [ 0.14081635]\n",
      " [-0.97826708]]\n",
      "Current iteration=8, loss=36429.83667982539\n",
      "t [[ 0.80315186]\n",
      " [-1.10360107]\n",
      " [-0.98487618]\n",
      " ...\n",
      " [ 0.31392111]\n",
      " [ 0.1342426 ]\n",
      " [-1.03468049]]\n",
      "t [[ 0.80315186]\n",
      " [-1.10360107]\n",
      " [-0.98487618]\n",
      " ...\n",
      " [ 0.31392111]\n",
      " [ 0.1342426 ]\n",
      " [-1.03468049]]\n",
      "t [[ 0.83358475]\n",
      " [-1.19290929]\n",
      " [-1.04936188]\n",
      " ...\n",
      " [ 0.30293181]\n",
      " [ 0.12650102]\n",
      " [-1.08666035]]\n",
      "loss=35154.11881395059\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.18876276]\n",
      " [-0.11228784]\n",
      " [-0.17745489]\n",
      " ...\n",
      " [ 0.27187773]\n",
      " [-0.17745489]\n",
      " [ 0.25465201]]\n",
      "t [[ 0.18876276]\n",
      " [-0.11228784]\n",
      " [-0.17745489]\n",
      " ...\n",
      " [ 0.27187773]\n",
      " [-0.17745489]\n",
      " [ 0.25465201]]\n",
      "t [[ 0.33431734]\n",
      " [-0.25527711]\n",
      " [-0.32590647]\n",
      " ...\n",
      " [ 0.47380665]\n",
      " [-0.32590647]\n",
      " [ 0.43247471]]\n",
      "t [[ 0.33431734]\n",
      " [-0.25527711]\n",
      " [-0.32590647]\n",
      " ...\n",
      " [ 0.47380665]\n",
      " [-0.32590647]\n",
      " [ 0.43247471]]\n",
      "Current iteration=2, loss=44756.56033232175\n",
      "t [[ 0.44845059]\n",
      " [-0.40411506]\n",
      " [-0.45358735]\n",
      " ...\n",
      " [ 0.62783965]\n",
      " [-0.45358735]\n",
      " [ 0.56089941]]\n",
      "t [[ 0.44845059]\n",
      " [-0.40411506]\n",
      " [-0.45358735]\n",
      " ...\n",
      " [ 0.62783965]\n",
      " [-0.45358735]\n",
      " [ 0.56089941]]\n",
      "t [[ 0.53940769]\n",
      " [-0.54803335]\n",
      " [-0.56574044]\n",
      " ...\n",
      " [ 0.74814736]\n",
      " [-0.56574044]\n",
      " [ 0.65649395]]\n",
      "t [[ 0.53940769]\n",
      " [-0.54803335]\n",
      " [-0.56574044]\n",
      " ...\n",
      " [ 0.74814736]\n",
      " [-0.56574044]\n",
      " [ 0.65649395]]\n",
      "Current iteration=4, loss=40733.79113210223\n",
      "t [[ 0.61293655]\n",
      " [-0.68303312]\n",
      " [-0.66578698]\n",
      " ...\n",
      " [ 0.84395018]\n",
      " [-0.66578698]\n",
      " [ 0.72941005]]\n",
      "t [[ 0.61293655]\n",
      " [-0.68303312]\n",
      " [-0.66578698]\n",
      " ...\n",
      " [ 0.84395018]\n",
      " [-0.66578698]\n",
      " [ 0.72941005]]\n",
      "t [[ 0.67312249]\n",
      " [-0.80811278]\n",
      " [-0.75606859]\n",
      " ...\n",
      " [ 0.92145913]\n",
      " [-0.75606859]\n",
      " [ 0.78612492]]\n",
      "t [[ 0.67312249]\n",
      " [-0.80811278]\n",
      " [-0.75606859]\n",
      " ...\n",
      " [ 0.92145913]\n",
      " [-0.75606859]\n",
      " [ 0.78612492]]\n",
      "Current iteration=6, loss=38171.18919879812\n",
      "t [[ 0.7229329 ]\n",
      " [-0.9235268 ]\n",
      " [-0.83826416]\n",
      " ...\n",
      " [ 0.9850051 ]\n",
      " [-0.83826416]\n",
      " [ 0.8309462 ]]\n",
      "t [[ 0.7229329 ]\n",
      " [-0.9235268 ]\n",
      " [-0.83826416]\n",
      " ...\n",
      " [ 0.9850051 ]\n",
      " [-0.83826416]\n",
      " [ 0.8309462 ]]\n",
      "t [[ 0.76456735]\n",
      " [-1.03000439]\n",
      " [-0.91362644]\n",
      " ...\n",
      " [ 1.03770101]\n",
      " [-0.91362644]\n",
      " [ 0.86684642]]\n",
      "t [[ 0.76456735]\n",
      " [-1.03000439]\n",
      " [-0.91362644]\n",
      " ...\n",
      " [ 1.03770101]\n",
      " [-0.91362644]\n",
      " [ 0.86684642]]\n",
      "Current iteration=8, loss=36407.587477334004\n",
      "t [[ 0.79968601]\n",
      " [-1.12840601]\n",
      " [-0.98312143]\n",
      " ...\n",
      " [ 1.0818424 ]\n",
      " [-0.98312143]\n",
      " [ 0.89594199]]\n",
      "t [[ 0.79968601]\n",
      " [-1.12840601]\n",
      " [-0.98312143]\n",
      " ...\n",
      " [ 1.0818424 ]\n",
      " [-0.98312143]\n",
      " [ 0.89594199]]\n",
      "t [[ 0.82956192]\n",
      " [-1.21957814]\n",
      " [-1.04751412]\n",
      " ...\n",
      " [ 1.11915865]\n",
      " [-1.04751412]\n",
      " [ 0.91977937]]\n",
      "loss=35126.98331561001\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06528729]\n",
      " [-0.31262803]\n",
      " [-0.18332866]\n",
      " ...\n",
      " [ 0.15677035]\n",
      " [ 0.06528729]\n",
      " [-0.26574394]]\n",
      "t [[ 0.06528729]\n",
      " [-0.31262803]\n",
      " [-0.18332866]\n",
      " ...\n",
      " [ 0.15677035]\n",
      " [ 0.06528729]\n",
      " [-0.26574394]]\n",
      "t [[ 0.10491691]\n",
      " [-0.58454599]\n",
      " [-0.33599094]\n",
      " ...\n",
      " [ 0.24534239]\n",
      " [ 0.10491691]\n",
      " [-0.44719444]]\n",
      "t [[ 0.10491691]\n",
      " [-0.58454599]\n",
      " [-0.33599094]\n",
      " ...\n",
      " [ 0.24534239]\n",
      " [ 0.10491691]\n",
      " [-0.44719444]]\n",
      "Current iteration=2, loss=44538.10582648419\n",
      "t [[ 0.12811643]\n",
      " [-0.82046431]\n",
      " [-0.46693095]\n",
      " ...\n",
      " [ 0.29545712]\n",
      " [ 0.12811643]\n",
      " [-0.58180963]]\n",
      "t [[ 0.12811643]\n",
      " [-0.82046431]\n",
      " [-0.46693095]\n",
      " ...\n",
      " [ 0.29545712]\n",
      " [ 0.12811643]\n",
      " [-0.58180963]]\n",
      "t [[ 0.14055272]\n",
      " [-1.02590365]\n",
      " [-0.58173293]\n",
      " ...\n",
      " [ 0.32291315]\n",
      " [ 0.14055272]\n",
      " [-0.68896364]]\n",
      "t [[ 0.14055272]\n",
      " [-1.02590365]\n",
      " [-0.58173293]\n",
      " ...\n",
      " [ 0.32291315]\n",
      " [ 0.14055272]\n",
      " [-0.68896364]]\n",
      "Current iteration=4, loss=40443.6132931843\n",
      "t [[ 0.14575432]\n",
      " [-1.20593855]\n",
      " [-0.68399867]\n",
      " ...\n",
      " [ 0.336435  ]\n",
      " [ 0.14575432]\n",
      " [-0.77879439]]\n",
      "t [[ 0.14575432]\n",
      " [-1.20593855]\n",
      " [-0.68399867]\n",
      " ...\n",
      " [ 0.336435  ]\n",
      " [ 0.14575432]\n",
      " [-0.77879439]]\n",
      "t [[ 0.14599962]\n",
      " [-1.36485752]\n",
      " [-0.77617554]\n",
      " ...\n",
      " [ 0.34104245]\n",
      " [ 0.14599962]\n",
      " [-0.85683322]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.14599962]\n",
      " [-1.36485752]\n",
      " [-0.77617554]\n",
      " ...\n",
      " [ 0.34104245]\n",
      " [ 0.14599962]\n",
      " [-0.85683322]]\n",
      "Current iteration=6, loss=37855.10045842835\n",
      "t [[ 0.14281299]\n",
      " [-1.50616152]\n",
      " [-0.86001064]\n",
      " ...\n",
      " [ 0.33975028]\n",
      " [ 0.14281299]\n",
      " [-0.92626628]]\n",
      "t [[ 0.14281299]\n",
      " [-1.50616152]\n",
      " [-0.86001064]\n",
      " ...\n",
      " [ 0.33975028]\n",
      " [ 0.14281299]\n",
      " [-0.92626628]]\n",
      "t [[ 0.13724573]\n",
      " [-1.63267271]\n",
      " [-0.93680448]\n",
      " ...\n",
      " [ 0.3344526 ]\n",
      " [ 0.13724573]\n",
      " [-0.98904437]]\n",
      "t [[ 0.13724573]\n",
      " [-1.63267271]\n",
      " [-0.93680448]\n",
      " ...\n",
      " [ 0.3344526 ]\n",
      " [ 0.13724573]\n",
      " [-0.98904437]]\n",
      "Current iteration=8, loss=36083.207923799724\n",
      "t [[ 0.13004092]\n",
      " [-1.74666273]\n",
      " [-1.00755863]\n",
      " ...\n",
      " [ 0.32639072]\n",
      " [ 0.13004092]\n",
      " [-1.04643857]]\n",
      "t [[ 0.13004092]\n",
      " [-1.74666273]\n",
      " [-1.00755863]\n",
      " ...\n",
      " [ 0.32639072]\n",
      " [ 0.13004092]\n",
      " [-1.04643857]]\n",
      "t [[ 0.12173388]\n",
      " [-1.84996756]\n",
      " [-1.07306566]\n",
      " ...\n",
      " [ 0.31640838]\n",
      " [ 0.12173388]\n",
      " [-1.09932719]]\n",
      "loss=34801.7798235685\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.19715306]\n",
      " [-0.11493233]\n",
      " [-0.18393862]\n",
      " ...\n",
      " [ 0.15447577]\n",
      " [ 0.06503096]\n",
      " [-0.26172782]]\n",
      "t [[ 0.19715306]\n",
      " [-0.11493233]\n",
      " [-0.18393862]\n",
      " ...\n",
      " [ 0.15447577]\n",
      " [ 0.06503096]\n",
      " [-0.26172782]]\n",
      "t [[ 0.34786926]\n",
      " [-0.26407884]\n",
      " [-0.33688589]\n",
      " ...\n",
      " [ 0.24123755]\n",
      " [ 0.10432068]\n",
      " [-0.43921249]]\n",
      "t [[ 0.34786926]\n",
      " [-0.26407884]\n",
      " [-0.33688589]\n",
      " ...\n",
      " [ 0.24123755]\n",
      " [ 0.10432068]\n",
      " [-0.43921249]]\n",
      "Current iteration=2, loss=44525.76431801008\n",
      "t [[ 0.46518391]\n",
      " [-0.41950527]\n",
      " [-0.46792399]\n",
      " ...\n",
      " [ 0.28994362]\n",
      " [ 0.12715275]\n",
      " [-0.57017193]]\n",
      "t [[ 0.46518391]\n",
      " [-0.41950527]\n",
      " [-0.46792399]\n",
      " ...\n",
      " [ 0.28994362]\n",
      " [ 0.12715275]\n",
      " [-0.57017193]]\n",
      "t [[ 0.55817457]\n",
      " [-0.56939177]\n",
      " [-0.58271795]\n",
      " ...\n",
      " [ 0.31631682]\n",
      " [ 0.13922127]\n",
      " [-0.67405743]]\n",
      "t [[ 0.55817457]\n",
      " [-0.56939177]\n",
      " [-0.58271795]\n",
      " ...\n",
      " [ 0.31631682]\n",
      " [ 0.13922127]\n",
      " [-0.67405743]]\n",
      "Current iteration=4, loss=40430.57853066841\n",
      "t [[ 0.63304156]\n",
      " [-0.70953964]\n",
      " [-0.68491539]\n",
      " ...\n",
      " [ 0.32901315]\n",
      " [ 0.14406765]\n",
      " [-0.76100152]]\n",
      "t [[ 0.63304156]\n",
      " [-0.70953964]\n",
      " [-0.68491539]\n",
      " ...\n",
      " [ 0.32901315]\n",
      " [ 0.14406765]\n",
      " [-0.76100152]]\n",
      "t [[ 0.69412754]\n",
      " [-0.83901682]\n",
      " [-0.77699004]\n",
      " ...\n",
      " [ 0.33299477]\n",
      " [ 0.14397625]\n",
      " [-0.83649921]]\n",
      "t [[ 0.69412754]\n",
      " [-0.83901682]\n",
      " [-0.77699004]\n",
      " ...\n",
      " [ 0.33299477]\n",
      " [ 0.14397625]\n",
      " [-0.83649921]]\n",
      "Current iteration=6, loss=37842.48364066772\n",
      "t [[ 0.74455783]\n",
      " [-0.95820557]\n",
      " [-0.86070442]\n",
      " ...\n",
      " [ 0.33123151]\n",
      " [ 0.14047393]\n",
      " [-0.90369273]]\n",
      "t [[ 0.74455783]\n",
      " [-0.95820557]\n",
      " [-0.86070442]\n",
      " ...\n",
      " [ 0.33123151]\n",
      " [ 0.14047393]\n",
      " [-0.90369273]]\n",
      "t [[ 0.7866337 ]\n",
      " [-1.06795633]\n",
      " [-0.93736817]\n",
      " ...\n",
      " [ 0.32558332]\n",
      " [ 0.13461268]\n",
      " [-0.96449066]]\n",
      "t [[ 0.7866337 ]\n",
      " [-1.06795633]\n",
      " [-0.93736817]\n",
      " ...\n",
      " [ 0.32558332]\n",
      " [ 0.13461268]\n",
      " [-0.96449066]]\n",
      "Current iteration=8, loss=36070.8726285586\n",
      "t [[ 0.82208265]\n",
      " [-1.1692288 ]\n",
      " [-1.00798841]\n",
      " ...\n",
      " [ 0.31726574]\n",
      " [ 0.12713529]\n",
      " [-1.02012672]]\n",
      "t [[ 0.82208265]\n",
      " [-1.1692288 ]\n",
      " [-1.00798841]\n",
      " ...\n",
      " [ 0.31726574]\n",
      " [ 0.12713529]\n",
      " [-1.02012672]]\n",
      "t [[ 0.85222302]\n",
      " [-1.26294671]\n",
      " [-1.07336107]\n",
      " ...\n",
      " [ 0.30710308]\n",
      " [ 0.11857632]\n",
      " [-1.07144741]]\n",
      "loss=34789.44477655421\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.19462615]\n",
      " [-0.11002327]\n",
      " [-0.18324975]\n",
      " ...\n",
      " [ 0.15685913]\n",
      " [ 0.06622954]\n",
      " [-0.26978773]]\n",
      "t [[ 0.19462615]\n",
      " [-0.11002327]\n",
      " [-0.18324975]\n",
      " ...\n",
      " [ 0.15685913]\n",
      " [ 0.06622954]\n",
      " [-0.26978773]]\n",
      "t [[ 0.34342964]\n",
      " [-0.25441651]\n",
      " [-0.33551253]\n",
      " ...\n",
      " [ 0.24419232]\n",
      " [ 0.10641164]\n",
      " [-0.4528132 ]]\n",
      "t [[ 0.34342964]\n",
      " [-0.25441651]\n",
      " [-0.33551253]\n",
      " ...\n",
      " [ 0.24419232]\n",
      " [ 0.10641164]\n",
      " [-0.4528132 ]]\n",
      "Current iteration=2, loss=44591.27023025162\n",
      "t [[ 0.4592986 ]\n",
      " [-0.40512688]\n",
      " [-0.46592435]\n",
      " ...\n",
      " [ 0.29254071]\n",
      " [ 0.12995451]\n",
      " [-0.58783678]]\n",
      "t [[ 0.4592986 ]\n",
      " [-0.40512688]\n",
      " [-0.46592435]\n",
      " ...\n",
      " [ 0.29254071]\n",
      " [ 0.12995451]\n",
      " [-0.58783678]]\n",
      "t [[ 0.55120276]\n",
      " [-0.55036124]\n",
      " [-0.58015688]\n",
      " ...\n",
      " [ 0.31802817]\n",
      " [ 0.14261515]\n",
      " [-0.69484398]]\n",
      "t [[ 0.55120276]\n",
      " [-0.55036124]\n",
      " [-0.58015688]\n",
      " ...\n",
      " [ 0.31802817]\n",
      " [ 0.14261515]\n",
      " [-0.69484398]]\n",
      "Current iteration=4, loss=40547.53595090987\n",
      "t [[ 0.62525705]\n",
      " [-0.68596827]\n",
      " [-0.68184978]\n",
      " ...\n",
      " [ 0.32951214]\n",
      " [ 0.14796852]\n",
      " [-0.78426225]]\n",
      "t [[ 0.62525705]\n",
      " [-0.68596827]\n",
      " [-0.68184978]\n",
      " ...\n",
      " [ 0.32951214]\n",
      " [ 0.14796852]\n",
      " [-0.78426225]]\n",
      "t [[ 0.68573476]\n",
      " [-0.81105351]\n",
      " [-0.77346671]\n",
      " ...\n",
      " [ 0.33207901]\n",
      " [ 0.14831904]\n",
      " [-0.86176847]]\n",
      "t [[ 0.68573476]\n",
      " [-0.81105351]\n",
      " [-0.77346671]\n",
      " ...\n",
      " [ 0.33207901]\n",
      " [ 0.14831904]\n",
      " [-0.86176847]]\n",
      "Current iteration=6, loss=37995.61802144657\n",
      "t [[ 0.73570834]\n",
      " [-0.92602119]\n",
      " [-0.8567611 ]\n",
      " ...\n",
      " [ 0.32877824]\n",
      " [ 0.14520691]\n",
      " [-0.93062324]]\n",
      "t [[ 0.73570834]\n",
      " [-0.92602119]\n",
      " [-0.8567611 ]\n",
      " ...\n",
      " [ 0.32877824]\n",
      " [ 0.14520691]\n",
      " [-0.93062324]]\n",
      "t [[ 0.7774399 ]\n",
      " [-1.0317313 ]\n",
      " [-0.93303522]\n",
      " ...\n",
      " [ 0.32152154]\n",
      " [ 0.1396936 ]\n",
      " [-0.99281526]]\n",
      "t [[ 0.7774399 ]\n",
      " [-1.0317313 ]\n",
      " [-0.93303522]\n",
      " ...\n",
      " [ 0.32152154]\n",
      " [ 0.1396936 ]\n",
      " [-0.99281526]]\n",
      "Current iteration=8, loss=36250.21604507955\n",
      "t [[ 0.81262817]\n",
      " [-1.12914486]\n",
      " [-1.00329025]\n",
      " ...\n",
      " [ 0.31155864]\n",
      " [ 0.13252903]\n",
      " [-1.04963441]]\n",
      "t [[ 0.81262817]\n",
      " [-1.12914486]\n",
      " [-1.00329025]\n",
      " ...\n",
      " [ 0.31155864]\n",
      " [ 0.13252903]\n",
      " [-1.04963441]]\n",
      "t [[ 0.84257027]\n",
      " [-1.21918195]\n",
      " [-1.06831752]\n",
      " ...\n",
      " [ 0.29973675]\n",
      " [ 0.12425327]\n",
      " [-1.10196782]]\n",
      "loss=34988.46514651415\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.1943146 ]\n",
      " [-0.11559042]\n",
      " [-0.18267415]\n",
      " ...\n",
      " [ 0.27987414]\n",
      " [-0.18267415]\n",
      " [ 0.26214178]]\n",
      "t [[ 0.1943146 ]\n",
      " [-0.11559042]\n",
      " [-0.18267415]\n",
      " ...\n",
      " [ 0.27987414]\n",
      " [-0.18267415]\n",
      " [ 0.26214178]]\n",
      "t [[ 0.34286383]\n",
      " [-0.2636968 ]\n",
      " [-0.33462732]\n",
      " ...\n",
      " [ 0.48565989]\n",
      " [-0.33462732]\n",
      " [ 0.44290884]]\n",
      "t [[ 0.34286383]\n",
      " [-0.2636968 ]\n",
      " [-0.33462732]\n",
      " ...\n",
      " [ 0.48565989]\n",
      " [-0.33462732]\n",
      " [ 0.44290884]]\n",
      "Current iteration=2, loss=44594.201542509065\n",
      "t [[ 0.45850795]\n",
      " [-0.41724904]\n",
      " [-0.4648403 ]\n",
      " ...\n",
      " [ 0.64141076]\n",
      " [-0.4648403 ]\n",
      " [ 0.5722137 ]]\n",
      "t [[ 0.45850795]\n",
      " [-0.41724904]\n",
      " [-0.4648403 ]\n",
      " ...\n",
      " [ 0.64141076]\n",
      " [-0.4648403 ]\n",
      " [ 0.5722137 ]]\n",
      "t [[ 0.55010547]\n",
      " [-0.56496675]\n",
      " [-0.57892433]\n",
      " ...\n",
      " [ 0.76229708]\n",
      " [-0.57892433]\n",
      " [ 0.66774182]]\n",
      "t [[ 0.55010547]\n",
      " [-0.56496675]\n",
      " [-0.57892433]\n",
      " ...\n",
      " [ 0.76229708]\n",
      " [-0.57892433]\n",
      " [ 0.66774182]]\n",
      "Current iteration=4, loss=40539.76527413768\n",
      "t [[ 0.62375789]\n",
      " [-0.70289467]\n",
      " [-0.68049202]\n",
      " ...\n",
      " [ 0.85805184]\n",
      " [-0.68049202]\n",
      " [ 0.74015332]]\n",
      "t [[ 0.62375789]\n",
      " [-0.70289467]\n",
      " [-0.68049202]\n",
      " ...\n",
      " [ 0.85805184]\n",
      " [-0.68049202]\n",
      " [ 0.74015332]]\n",
      "t [[ 0.68376133]\n",
      " [-0.8302019 ]\n",
      " [-0.77199499]\n",
      " ...\n",
      " [ 0.93516478]\n",
      " [-0.77199499]\n",
      " [ 0.79616972]]\n",
      "t [[ 0.68376133]\n",
      " [-0.8302019 ]\n",
      " [-0.77199499]\n",
      " ...\n",
      " [ 0.93516478]\n",
      " [-0.77199499]\n",
      " [ 0.79616972]]\n",
      "Current iteration=6, loss=37979.17757937885\n",
      "t [[ 0.73321261]\n",
      " [-0.94730988]\n",
      " [-0.8551818 ]\n",
      " ...\n",
      " [ 0.99812676]\n",
      " [-0.8551818 ]\n",
      " [ 0.84022299]]\n",
      "t [[ 0.73321261]\n",
      " [-0.94730988]\n",
      " [-0.8551818 ]\n",
      " ...\n",
      " [ 0.99812676]\n",
      " [-0.8551818 ]\n",
      " [ 0.84022299]]\n",
      "t [[ 0.77439259]\n",
      " [-1.05508083]\n",
      " [-0.931353  ]\n",
      " ...\n",
      " [ 1.0501464 ]\n",
      " [-0.931353  ]\n",
      " [ 0.87535189]]\n",
      "t [[ 0.77439259]\n",
      " [-1.05508083]\n",
      " [-0.931353  ]\n",
      " ...\n",
      " [ 1.0501464 ]\n",
      " [-0.931353  ]\n",
      " [ 0.87535189]]\n",
      "Current iteration=8, loss=36227.29074533172\n",
      "t [[ 0.80901329]\n",
      " [-1.15447435]\n",
      " [-1.00150929]\n",
      " ...\n",
      " [ 1.09357821]\n",
      " [-1.00150929]\n",
      " [ 0.90370908]]\n",
      "t [[ 0.80901329]\n",
      " [-1.15447435]\n",
      " [-1.00150929]\n",
      " ...\n",
      " [ 1.09357821]\n",
      " [-1.00150929]\n",
      " [ 0.90370908]]\n",
      "t [[ 0.83838092]\n",
      " [-1.24640934]\n",
      " [-1.06644195]\n",
      " ...\n",
      " [ 1.13018825]\n",
      " [-1.06644195]\n",
      " [ 0.92686065]]\n",
      "loss=34960.69096659441\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06715264]\n",
      " [-0.32156026]\n",
      " [-0.18856663]\n",
      " ...\n",
      " [ 0.1612495 ]\n",
      " [ 0.06715264]\n",
      " [-0.27333663]]\n",
      "t [[ 0.06715264]\n",
      " [-0.32156026]\n",
      " [-0.18856663]\n",
      " ...\n",
      " [ 0.1612495 ]\n",
      " [ 0.06715264]\n",
      " [-0.27333663]]\n",
      "t [[ 0.1071744 ]\n",
      " [-0.60006505]\n",
      " [-0.34470399]\n",
      " ...\n",
      " [ 0.25038281]\n",
      " [ 0.1071744 ]\n",
      " [-0.45754028]]\n",
      "t [[ 0.1071744 ]\n",
      " [-0.60006505]\n",
      " [-0.34470399]\n",
      " ...\n",
      " [ 0.25038281]\n",
      " [ 0.1071744 ]\n",
      " [-0.45754028]]\n",
      "Current iteration=2, loss=44376.57155416108\n",
      "t [[ 0.13010282]\n",
      " [-0.84065821]\n",
      " [-0.47814427]\n",
      " ...\n",
      " [ 0.29973291]\n",
      " [ 0.13010282]\n",
      " [-0.59333759]]\n",
      "t [[ 0.13010282]\n",
      " [-0.84065821]\n",
      " [-0.47814427]\n",
      " ...\n",
      " [ 0.29973291]\n",
      " [ 0.13010282]\n",
      " [-0.59333759]]\n",
      "t [[ 0.14197993]\n",
      " [-1.04935795]\n",
      " [-0.59484741]\n",
      " ...\n",
      " [ 0.32604289]\n",
      " [ 0.14197993]\n",
      " [-0.70121478]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.14197993]\n",
      " [-1.04935795]\n",
      " [-0.59484741]\n",
      " ...\n",
      " [ 0.32604289]\n",
      " [ 0.14197993]\n",
      " [-0.70121478]]\n",
      "Current iteration=4, loss=40252.34618739018\n",
      "t [[ 0.14651095]\n",
      " [-1.23164273]\n",
      " [-0.69860697]\n",
      " ...\n",
      " [ 0.33837209]\n",
      " [ 0.14651095]\n",
      " [-0.79164306]]\n",
      "t [[ 0.14651095]\n",
      " [-1.23164273]\n",
      " [-0.69860697]\n",
      " ...\n",
      " [ 0.33837209]\n",
      " [ 0.14651095]\n",
      " [-0.79164306]]\n",
      "t [[ 0.14606139]\n",
      " [-1.39209612]\n",
      " [-0.79197971]\n",
      " ...\n",
      " [ 0.34184887]\n",
      " [ 0.14606139]\n",
      " [-0.87023304]]\n",
      "t [[ 0.14606139]\n",
      " [-1.39209612]\n",
      " [-0.79197971]\n",
      " ...\n",
      " [ 0.34184887]\n",
      " [ 0.14606139]\n",
      " [-0.87023304]]\n",
      "Current iteration=6, loss=37667.01423352606\n",
      "t [[ 0.14220011]\n",
      " [-1.53442603]\n",
      " [-0.87678196]\n",
      " ...\n",
      " [ 0.33951714]\n",
      " [ 0.14220011]\n",
      " [-0.94017763]]\n",
      "t [[ 0.14220011]\n",
      " [-1.53442603]\n",
      " [-0.87678196]\n",
      " ...\n",
      " [ 0.33951714]\n",
      " [ 0.14220011]\n",
      " [-0.94017763]]\n",
      "t [[ 0.13600125]\n",
      " [-1.66159835]\n",
      " [-0.95436187]\n",
      " ...\n",
      " [ 0.33327399]\n",
      " [ 0.13600125]\n",
      " [-1.00341954]]\n",
      "t [[ 0.13600125]\n",
      " [-1.66159835]\n",
      " [-0.95436187]\n",
      " ...\n",
      " [ 0.33327399]\n",
      " [ 0.13600125]\n",
      " [-1.00341954]]\n",
      "Current iteration=8, loss=35907.417657085025\n",
      "t [[ 0.12821907]\n",
      " [-1.77598494]\n",
      " [-1.02575575]\n",
      " ...\n",
      " [ 0.32435578]\n",
      " [ 0.12821907]\n",
      " [-1.06122278]]\n",
      "t [[ 0.12821907]\n",
      " [-1.77598494]\n",
      " [-1.02575575]\n",
      " ...\n",
      " [ 0.32435578]\n",
      " [ 0.12821907]\n",
      " [-1.06122278]]\n",
      "t [[ 0.11939353]\n",
      " [-1.87949244]\n",
      " [-1.09178252]\n",
      " ...\n",
      " [ 0.31359947]\n",
      " [ 0.11939353]\n",
      " [-1.11446265]]\n",
      "loss=34640.229034741526\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.202786  ]\n",
      " [-0.11821611]\n",
      " [-0.18919401]\n",
      " ...\n",
      " [ 0.15888936]\n",
      " [ 0.06688899]\n",
      " [-0.26920575]]\n",
      "t [[ 0.202786  ]\n",
      " [-0.11821611]\n",
      " [-0.18919401]\n",
      " ...\n",
      " [ 0.15888936]\n",
      " [ 0.06688899]\n",
      " [-0.26920575]]\n",
      "t [[ 0.3564695 ]\n",
      " [-0.2726073 ]\n",
      " [-0.34561524]\n",
      " ...\n",
      " [ 0.24617451]\n",
      " [ 0.1065588 ]\n",
      " [-0.449332  ]]\n",
      "t [[ 0.3564695 ]\n",
      " [-0.2726073 ]\n",
      " [-0.34561524]\n",
      " ...\n",
      " [ 0.24617451]\n",
      " [ 0.1065588 ]\n",
      " [-0.449332  ]]\n",
      "Current iteration=2, loss=44364.121632824244\n",
      "t [[ 0.47523861]\n",
      " [-0.4328208 ]\n",
      " [-0.47914593]\n",
      " ...\n",
      " [ 0.29409867]\n",
      " [ 0.12910773]\n",
      " [-0.58138719]]\n",
      "t [[ 0.47523861]\n",
      " [-0.4328208 ]\n",
      " [-0.47914593]\n",
      " ...\n",
      " [ 0.29409867]\n",
      " [ 0.12910773]\n",
      " [-0.58138719]]\n",
      "t [[ 0.56881705]\n",
      " [-0.58651547]\n",
      " [-0.59583184]\n",
      " ...\n",
      " [ 0.31932276]\n",
      " [ 0.14060648]\n",
      " [-0.68593573]]\n",
      "t [[ 0.56881705]\n",
      " [-0.58651547]\n",
      " [-0.59583184]\n",
      " ...\n",
      " [ 0.31932276]\n",
      " [ 0.14060648]\n",
      " [-0.68593573]]\n",
      "Current iteration=4, loss=40239.32432016853\n",
      "t [[ 0.64376556]\n",
      " [-0.72956423]\n",
      " [-0.69951434]\n",
      " ...\n",
      " [ 0.33083198]\n",
      " [ 0.1447735 ]\n",
      " [-0.77343827]]\n",
      "t [[ 0.64376556]\n",
      " [-0.72956423]\n",
      " [-0.69951434]\n",
      " ...\n",
      " [ 0.33083198]\n",
      " [ 0.1447735 ]\n",
      " [-0.77343827]]\n",
      "t [[ 0.70463843]\n",
      " [-0.86122742]\n",
      " [-0.79277711]\n",
      " ...\n",
      " [ 0.33369329]\n",
      " [ 0.14398022]\n",
      " [-0.84946355]]\n",
      "t [[ 0.70463843]\n",
      " [-0.86122742]\n",
      " [-0.79277711]\n",
      " ...\n",
      " [ 0.33369329]\n",
      " [ 0.14398022]\n",
      " [-0.84946355]]\n",
      "Current iteration=6, loss=37654.43666334627\n",
      "t [[ 0.75468967]\n",
      " [-0.98206618]\n",
      " [-0.87745198]\n",
      " ...\n",
      " [ 0.33090332]\n",
      " [ 0.13979781]\n",
      " [-0.91715601]]\n",
      "t [[ 0.75468967]\n",
      " [-0.98206618]\n",
      " [-0.87745198]\n",
      " ...\n",
      " [ 0.33090332]\n",
      " [ 0.13979781]\n",
      " [-0.91715601]]\n",
      "t [[ 0.79630059]\n",
      " [-1.09306936]\n",
      " [-0.95489619]\n",
      " ...\n",
      " [ 0.3243236 ]\n",
      " [ 0.1333009 ]\n",
      " [-0.97841281]]\n",
      "t [[ 0.79630059]\n",
      " [-1.09306936]\n",
      " [-0.95489619]\n",
      " ...\n",
      " [ 0.3243236 ]\n",
      " [ 0.1333009 ]\n",
      " [-0.97841281]]\n",
      "Current iteration=8, loss=35895.103410051626\n",
      "t [[ 0.83124915]\n",
      " [-1.19529774]\n",
      " [-1.02615142]\n",
      " ...\n",
      " [ 0.31516399]\n",
      " [ 0.12524327]\n",
      " [-1.03445828]]\n",
      "t [[ 0.83124915]\n",
      " [-1.19529774]\n",
      " [-1.02615142]\n",
      " ...\n",
      " [ 0.31516399]\n",
      " [ 0.12524327]\n",
      " [-1.03445828]]\n",
      "t [[ 0.86088551]\n",
      " [-1.28974769]\n",
      " [-1.09203983]\n",
      " ...\n",
      " [ 0.30424156]\n",
      " [ 0.11616396]\n",
      " [-1.08613424]]\n",
      "loss=34627.87985397489\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.2001869 ]\n",
      " [-0.11316679]\n",
      " [-0.18848545]\n",
      " ...\n",
      " [ 0.16134082]\n",
      " [ 0.06812182]\n",
      " [-0.27749595]]\n",
      "t [[ 0.2001869 ]\n",
      " [-0.11316679]\n",
      " [-0.18848545]\n",
      " ...\n",
      " [ 0.16134082]\n",
      " [ 0.06812182]\n",
      " [-0.27749595]]\n",
      "t [[ 0.35192061]\n",
      " [-0.26267338]\n",
      " [-0.34420301]\n",
      " ...\n",
      " [ 0.24916365]\n",
      " [ 0.1087009 ]\n",
      " [-0.46325034]]\n",
      "t [[ 0.35192061]\n",
      " [-0.26267338]\n",
      " [-0.34420301]\n",
      " ...\n",
      " [ 0.24916365]\n",
      " [ 0.1087009 ]\n",
      " [-0.46325034]]\n",
      "Current iteration=2, loss=44431.42398158802\n",
      "t [[ 0.46922932]\n",
      " [-0.41803832]\n",
      " [-0.47709303]\n",
      " ...\n",
      " [ 0.29666829]\n",
      " [ 0.13197078]\n",
      " [-0.59940329]]\n",
      "t [[ 0.46922932]\n",
      " [-0.41803832]\n",
      " [-0.47709303]\n",
      " ...\n",
      " [ 0.29666829]\n",
      " [ 0.13197078]\n",
      " [-0.59940329]]\n",
      "t [[ 0.56172082]\n",
      " [-0.56695318]\n",
      " [-0.59320694]\n",
      " ...\n",
      " [ 0.32093548]\n",
      " [ 0.14406845]\n",
      " [-0.70708292]]\n",
      "t [[ 0.56172082]\n",
      " [-0.56695318]\n",
      " [-0.59320694]\n",
      " ...\n",
      " [ 0.32093548]\n",
      " [ 0.14406845]\n",
      " [-0.70708292]]\n",
      "Current iteration=4, loss=40358.82787849893\n",
      "t [[ 0.63586398]\n",
      " [-0.70534373]\n",
      " [-0.69637677]\n",
      " ...\n",
      " [ 0.33115997]\n",
      " [ 0.1487471 ]\n",
      " [-0.7970571 ]]\n",
      "t [[ 0.63586398]\n",
      " [-0.70534373]\n",
      " [-0.69637677]\n",
      " ...\n",
      " [ 0.33115997]\n",
      " [ 0.1487471 ]\n",
      " [-0.7970571 ]]\n",
      "t [[ 0.69613949]\n",
      " [-0.83251016]\n",
      " [-0.78917516]\n",
      " ...\n",
      " [ 0.33253693]\n",
      " [ 0.14839891]\n",
      " [-0.87508231]]\n",
      "t [[ 0.69613949]\n",
      " [-0.83251016]\n",
      " [-0.78917516]\n",
      " ...\n",
      " [ 0.33253693]\n",
      " [ 0.14839891]\n",
      " [-0.87508231]]\n",
      "Current iteration=6, loss=37810.28099167271\n",
      "t [[ 0.74574607]\n",
      " [-0.94903573]\n",
      " [-0.87342426]\n",
      " ...\n",
      " [ 0.32814421]\n",
      " [ 0.14460878]\n",
      " [-0.94442422]]\n",
      "t [[ 0.74574607]\n",
      " [-0.94903573]\n",
      " [-0.87342426]\n",
      " ...\n",
      " [ 0.32814421]\n",
      " [ 0.14460878]\n",
      " [-0.94442422]]\n",
      "t [[ 0.78702457]\n",
      " [-1.05591801]\n",
      " [-0.9504735 ]\n",
      " ...\n",
      " [ 0.31989554]\n",
      " [ 0.13846114]\n",
      " [-1.00706153]]\n",
      "t [[ 0.78702457]\n",
      " [-1.05591801]\n",
      " [-0.9504735 ]\n",
      " ...\n",
      " [ 0.31989554]\n",
      " [ 0.13846114]\n",
      " [-1.00706153]]\n",
      "Current iteration=8, loss=36077.10610893248\n",
      "t [[ 0.8217235 ]\n",
      " [-1.15421793]\n",
      " [-1.02135842]\n",
      " ...\n",
      " [ 0.30903488]\n",
      " [ 0.13071709]\n",
      " [-1.06427549]]\n",
      "t [[ 0.8217235 ]\n",
      " [-1.15421793]\n",
      " [-1.02135842]\n",
      " ...\n",
      " [ 0.30903488]\n",
      " [ 0.13071709]\n",
      " [-1.06427549]]\n",
      "t [[ 0.85117152]\n",
      " [-1.24492696]\n",
      " [-1.08689642]\n",
      " ...\n",
      " [ 0.29640196]\n",
      " [ 0.12192136]\n",
      " [-1.11694902]]\n",
      "loss=34829.42812125769\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.19986645]\n",
      " [-0.11889301]\n",
      " [-0.18789341]\n",
      " ...\n",
      " [ 0.28787054]\n",
      " [-0.18789341]\n",
      " [ 0.26963154]]\n",
      "t [[ 0.19986645]\n",
      " [-0.11889301]\n",
      " [-0.18789341]\n",
      " ...\n",
      " [ 0.28787054]\n",
      " [-0.18789341]\n",
      " [ 0.26963154]]\n",
      "t [[ 0.35133872]\n",
      " [-0.27216701]\n",
      " [-0.34329995]\n",
      " ...\n",
      " [ 0.49739726]\n",
      " [-0.34329995]\n",
      " [ 0.4532159 ]]\n",
      "t [[ 0.35133872]\n",
      " [-0.27216701]\n",
      " [-0.34329995]\n",
      " ...\n",
      " [ 0.49739726]\n",
      " [-0.34329995]\n",
      " [ 0.4532159 ]]\n",
      "Current iteration=2, loss=44434.105287660066\n",
      "t [[ 0.4684124 ]\n",
      " [-0.43041075]\n",
      " [-0.47599164]\n",
      " ...\n",
      " [ 0.65474896]\n",
      " [-0.47599164]\n",
      " [ 0.58328833]]\n",
      "t [[ 0.4684124 ]\n",
      " [-0.43041075]\n",
      " [-0.47599164]\n",
      " ...\n",
      " [ 0.65474896]\n",
      " [-0.47599164]\n",
      " [ 0.58328833]]\n",
      "t [[ 0.56057898]\n",
      " [-0.58185262]\n",
      " [-0.59195694]\n",
      " ...\n",
      " [ 0.77612062]\n",
      " [-0.59195694]\n",
      " [ 0.67867286]]\n",
      "t [[ 0.56057898]\n",
      " [-0.58185262]\n",
      " [-0.59195694]\n",
      " ...\n",
      " [ 0.77612062]\n",
      " [-0.59195694]\n",
      " [ 0.67867286]]\n",
      "Current iteration=4, loss=40350.487668584654\n",
      "t [[ 0.63429826]\n",
      " [-0.72261297]\n",
      " [-0.69500049]\n",
      " ...\n",
      " [ 0.87175875]\n",
      " [-0.69500049]\n",
      " [ 0.75053209]]\n",
      "t [[ 0.63429826]\n",
      " [-0.72261297]\n",
      " [-0.69500049]\n",
      " ...\n",
      " [ 0.87175875]\n",
      " [-0.69500049]\n",
      " [ 0.75053209]]\n",
      "t [[ 0.69407696]\n",
      " [-0.85205102]\n",
      " [-0.7876832 ]\n",
      " ...\n",
      " [ 0.94842783]\n",
      " [-0.7876832 ]\n",
      " [ 0.80582361]]\n",
      "t [[ 0.69407696]\n",
      " [-0.85205102]\n",
      " [-0.7876832 ]\n",
      " ...\n",
      " [ 0.94842783]\n",
      " [-0.7876832 ]\n",
      " [ 0.80582361]]\n",
      "Current iteration=6, loss=37793.18646952115\n",
      "t [[ 0.74313961]\n",
      " [-0.97076394]\n",
      " [-0.87182276]\n",
      " ...\n",
      " [ 1.01077451]\n",
      " [-0.87182276]\n",
      " [ 0.84909742]]\n",
      "t [[ 0.74313961]\n",
      " [-0.97076394]\n",
      " [-0.87182276]\n",
      " ...\n",
      " [ 1.01077451]\n",
      " [-0.87182276]\n",
      " [ 0.84909742]]\n",
      "t [[ 0.78384637]\n",
      " [-1.0797498 ]\n",
      " [-0.94876712]\n",
      " ...\n",
      " [ 1.06209977]\n",
      " [-0.94876712]\n",
      " [ 0.8834543 ]]\n",
      "t [[ 0.78384637]\n",
      " [-1.0797498 ]\n",
      " [-0.94876712]\n",
      " ...\n",
      " [ 1.06209977]\n",
      " [-0.94876712]\n",
      " [ 0.8834543 ]]\n",
      "Current iteration=8, loss=36053.52482791373\n",
      "t [[ 0.81795912]\n",
      " [-1.18006742]\n",
      " [-1.01955153]\n",
      " ...\n",
      " [ 1.10481427]\n",
      " [-1.01955153]\n",
      " [ 0.91108024]]\n",
      "t [[ 0.81795912]\n",
      " [-1.18006742]\n",
      " [-1.01955153]\n",
      " ...\n",
      " [ 1.10481427]\n",
      " [-1.01955153]\n",
      " [ 0.91108024]]\n",
      "t [[ 0.84681559]\n",
      " [-1.27270705]\n",
      " [-1.08499338]\n",
      " ...\n",
      " [ 1.14071859]\n",
      " [-1.08499338]\n",
      " [ 0.93355893]]\n",
      "loss=34801.03991427561\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.06901799]\n",
      " [-0.33049249]\n",
      " [-0.19380459]\n",
      " ...\n",
      " [ 0.16572865]\n",
      " [ 0.06901799]\n",
      " [-0.28092931]]\n",
      "t [[ 0.06901799]\n",
      " [-0.33049249]\n",
      " [-0.19380459]\n",
      " ...\n",
      " [ 0.16572865]\n",
      " [ 0.06901799]\n",
      " [-0.28092931]]\n",
      "t [[ 0.10939194]\n",
      " [-0.61551963]\n",
      " [-0.353369  ]\n",
      " ...\n",
      " [ 0.25531682]\n",
      " [ 0.10939194]\n",
      " [-0.46775496]]\n",
      "t [[ 0.10939194]\n",
      " [-0.61551963]\n",
      " [-0.353369  ]\n",
      " ...\n",
      " [ 0.25531682]\n",
      " [ 0.10939194]\n",
      " [-0.46775496]]\n",
      "Current iteration=2, loss=44217.270745385984\n",
      "t [[ 0.13201422]\n",
      " [-0.86068474]\n",
      " [-0.48925746]\n",
      " ...\n",
      " [ 0.30383168]\n",
      " [ 0.13201422]\n",
      " [-0.60465135]]\n",
      "t [[ 0.13201422]\n",
      " [-0.86068474]\n",
      " [-0.48925746]\n",
      " ...\n",
      " [ 0.30383168]\n",
      " [ 0.13201422]\n",
      " [-0.60465135]]\n",
      "t [[ 0.14330942]\n",
      " [-1.07253148]\n",
      " [-0.6078137 ]\n",
      " ...\n",
      " [ 0.32896588]\n",
      " [ 0.14330942]\n",
      " [-0.71321531]]\n",
      "t [[ 0.14330942]\n",
      " [-1.07253148]\n",
      " [-0.6078137 ]\n",
      " ...\n",
      " [ 0.32896588]\n",
      " [ 0.14330942]\n",
      " [-0.71321531]]\n",
      "Current iteration=4, loss=40065.70695159934\n",
      "t [[ 0.14715815]\n",
      " [-1.25695814]\n",
      " [-0.71302336]\n",
      " ...\n",
      " [ 0.34009798]\n",
      " [ 0.14715815]\n",
      " [-0.80422751]]\n",
      "t [[ 0.14715815]\n",
      " [-1.25695814]\n",
      " [-0.71302336]\n",
      " ...\n",
      " [ 0.34009798]\n",
      " [ 0.14715815]\n",
      " [-0.80422751]]\n",
      "t [[ 0.1460106 ]\n",
      " [-1.41884996]\n",
      " [-0.80755176]\n",
      " ...\n",
      " [ 0.34245266]\n",
      " [ 0.1460106 ]\n",
      " [-0.88336215]]\n",
      "t [[ 0.1460106 ]\n",
      " [-1.41884996]\n",
      " [-0.80755176]\n",
      " ...\n",
      " [ 0.34245266]\n",
      " [ 0.1460106 ]\n",
      " [-0.88336215]]\n",
      "Current iteration=6, loss=37484.74644578734\n",
      "t [[ 0.14147797]\n",
      " [-1.56212346]\n",
      " [-0.89328394]\n",
      " ...\n",
      " [ 0.33909624]\n",
      " [ 0.14147797]\n",
      " [-0.9538116 ]]\n",
      "t [[ 0.14147797]\n",
      " [-1.56212346]\n",
      " [-0.89328394]\n",
      " ...\n",
      " [ 0.33909624]\n",
      " [ 0.14147797]\n",
      " [-0.9538116 ]]\n",
      "t [[ 0.13465562]\n",
      " [-1.68988759]\n",
      " [-0.9716154 ]\n",
      " ...\n",
      " [ 0.33192546]\n",
      " [ 0.13465562]\n",
      " [-1.01750803]]\n",
      "t [[ 0.13465562]\n",
      " [-1.68988759]\n",
      " [-0.9716154 ]\n",
      " ...\n",
      " [ 0.33192546]\n",
      " [ 0.13465562]\n",
      " [-1.01750803]]\n",
      "Current iteration=8, loss=35737.8997310289\n",
      "t [[ 0.12630776]\n",
      " [-1.80461297]\n",
      " [-1.04361701]\n",
      " ...\n",
      " [ 0.32217027]\n",
      " [ 0.12630776]\n",
      " [-1.07570821]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.12630776]\n",
      " [-1.80461297]\n",
      " [-1.04361701]\n",
      " ...\n",
      " [ 0.32217027]\n",
      " [ 0.12630776]\n",
      " [-1.07570821]]\n",
      "t [[ 0.11697803]\n",
      " [-1.90827522]\n",
      " [-1.11013389]\n",
      " ...\n",
      " [ 0.31066009]\n",
      " [ 0.11697803]\n",
      " [-1.12928493]]\n",
      "loss=34485.026228725794\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.20841895]\n",
      " [-0.12149989]\n",
      " [-0.1944494 ]\n",
      " ...\n",
      " [ 0.16330296]\n",
      " [ 0.06874702]\n",
      " [-0.27668369]]\n",
      "t [[ 0.20841895]\n",
      " [-0.12149989]\n",
      " [-0.1944494 ]\n",
      " ...\n",
      " [ 0.16330296]\n",
      " [ 0.06874702]\n",
      " [-0.27668369]]\n",
      "t [[ 0.36499751]\n",
      " [-0.28118854]\n",
      " [-0.35429606]\n",
      " ...\n",
      " [ 0.25100579]\n",
      " [ 0.10875685]\n",
      " [-0.45932047]]\n",
      "t [[ 0.36499751]\n",
      " [-0.28118854]\n",
      " [-0.35429606]\n",
      " ...\n",
      " [ 0.25100579]\n",
      " [ 0.10875685]\n",
      " [-0.45932047]]\n",
      "Current iteration=2, loss=44204.720478437994\n",
      "t [[ 0.48514101]\n",
      " [-0.44616381]\n",
      " [-0.49026689]\n",
      " ...\n",
      " [ 0.29807857]\n",
      " [ 0.13098759]\n",
      " [-0.59238975]]\n",
      "t [[ 0.48514101]\n",
      " [-0.44616381]\n",
      " [-0.49026689]\n",
      " ...\n",
      " [ 0.29807857]\n",
      " [ 0.13098759]\n",
      " [-0.59238975]]\n",
      "t [[ 0.57923813]\n",
      " [-0.60358859]\n",
      " [-0.6087966 ]\n",
      " ...\n",
      " [ 0.32212493]\n",
      " [ 0.14189396]\n",
      " [-0.69756699]]\n",
      "t [[ 0.57923813]\n",
      " [-0.60358859]\n",
      " [-0.6087966 ]\n",
      " ...\n",
      " [ 0.32212493]\n",
      " [ 0.14189396]\n",
      " [-0.69756699]]\n",
      "Current iteration=4, loss=40052.70182583994\n",
      "t [[ 0.65421403]\n",
      " [-0.74944158]\n",
      " [-0.71392046]\n",
      " ...\n",
      " [ 0.33244349]\n",
      " [ 0.1453701 ]\n",
      " [-0.78561663]]\n",
      "t [[ 0.65421403]\n",
      " [-0.74944158]\n",
      " [-0.71392046]\n",
      " ...\n",
      " [ 0.33244349]\n",
      " [ 0.1453701 ]\n",
      " [-0.78561663]]\n",
      "t [[ 0.71483412]\n",
      " [-0.88319494]\n",
      " [-0.80833127]\n",
      " ...\n",
      " [ 0.33419371]\n",
      " [ 0.14387204]\n",
      " [-0.86216506]]\n",
      "t [[ 0.71483412]\n",
      " [-0.88319494]\n",
      " [-0.80833127]\n",
      " ...\n",
      " [ 0.33419371]\n",
      " [ 0.14387204]\n",
      " [-0.86216506]]\n",
      "Current iteration=6, loss=37472.20730969154\n",
      "t [[ 0.76447924]\n",
      " [-1.00559682]\n",
      " [-0.89392961]\n",
      " ...\n",
      " [ 0.33039229]\n",
      " [ 0.1390131 ]\n",
      " [-0.93035156]]\n",
      "t [[ 0.76447924]\n",
      " [-1.00559682]\n",
      " [-0.89392961]\n",
      " ...\n",
      " [ 0.33039229]\n",
      " [ 0.1390131 ]\n",
      " [-0.93035156]]\n",
      "t [[ 0.80560862]\n",
      " [-1.11777675]\n",
      " [-0.97211994]\n",
      " ...\n",
      " [ 0.32289912]\n",
      " [ 0.1318889 ]\n",
      " [-0.99205938]]\n",
      "t [[ 0.80560862]\n",
      " [-1.11777675]\n",
      " [-0.97211994]\n",
      " ...\n",
      " [ 0.32289912]\n",
      " [ 0.1318889 ]\n",
      " [-0.99205938]]\n",
      "Current iteration=8, loss=35725.60287503002\n",
      "t [[ 0.84004871]\n",
      " [-1.22089618]\n",
      " [-1.04397834]\n",
      " ...\n",
      " [ 0.31291688]\n",
      " [ 0.123263  ]\n",
      " [-1.04850328]]\n",
      "t [[ 0.84004871]\n",
      " [-1.22089618]\n",
      " [-1.04397834]\n",
      " ...\n",
      " [ 0.31291688]\n",
      " [ 0.123263  ]\n",
      " [-1.04850328]]\n",
      "t [[ 0.86917981]\n",
      " [-1.31602284]\n",
      " [-1.11035305]\n",
      " ...\n",
      " [ 0.30125478]\n",
      " [ 0.11367793]\n",
      " [-1.10052104]]\n",
      "loss=34472.65808357359\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.20574765]\n",
      " [-0.11631031]\n",
      " [-0.19372116]\n",
      " ...\n",
      " [ 0.16582251]\n",
      " [ 0.07001409]\n",
      " [-0.28520417]]\n",
      "t [[ 0.20574765]\n",
      " [-0.11631031]\n",
      " [-0.19372116]\n",
      " ...\n",
      " [ 0.16582251]\n",
      " [ 0.07001409]\n",
      " [-0.28520417]]\n",
      "t [[ 0.36034032]\n",
      " [-0.27098326]\n",
      " [-0.35284501]\n",
      " ...\n",
      " [ 0.25402676]\n",
      " [ 0.11094966]\n",
      " [-0.47355274]]\n",
      "t [[ 0.36034032]\n",
      " [-0.27098326]\n",
      " [-0.35284501]\n",
      " ...\n",
      " [ 0.25402676]\n",
      " [ 0.11094966]\n",
      " [-0.47355274]]\n",
      "Current iteration=2, loss=44273.809617228515\n",
      "t [[ 0.47900991]\n",
      " [-0.43097738]\n",
      " [-0.48816103]\n",
      " ...\n",
      " [ 0.30061659]\n",
      " [ 0.13391114]\n",
      " [-0.61075058]]\n",
      "t [[ 0.47900991]\n",
      " [-0.43097738]\n",
      " [-0.48816103]\n",
      " ...\n",
      " [ 0.30061659]\n",
      " [ 0.13391114]\n",
      " [-0.61075058]]\n",
      "t [[ 0.57202073]\n",
      " [-0.58349509]\n",
      " [-0.60610845]\n",
      " ...\n",
      " [ 0.32363407]\n",
      " [ 0.14542291]\n",
      " [-0.71906637]]\n",
      "t [[ 0.57202073]\n",
      " [-0.58349509]\n",
      " [-0.60610845]\n",
      " ...\n",
      " [ 0.32363407]\n",
      " [ 0.14542291]\n",
      " [-0.71906637]]\n",
      "Current iteration=4, loss=40174.70592441565\n",
      "t [[ 0.64619939]\n",
      " [-0.72457356]\n",
      " [-0.71071176]\n",
      " ...\n",
      " [ 0.33259538]\n",
      " [ 0.14941508]\n",
      " [-0.80958382]]\n",
      "t [[ 0.64619939]\n",
      " [-0.72457356]\n",
      " [-0.71071176]\n",
      " ...\n",
      " [ 0.33259538]\n",
      " [ 0.14941508]\n",
      " [-0.80958382]]\n",
      "t [[ 0.70623344]\n",
      " [-0.85372695]\n",
      " [-0.80465171]\n",
      " ...\n",
      " [ 0.332792  ]\n",
      " [ 0.14836506]\n",
      " [-0.88812275]]\n",
      "t [[ 0.70623344]\n",
      " [-0.85372695]\n",
      " [-0.80465171]\n",
      " ...\n",
      " [ 0.332792  ]\n",
      " [ 0.14836506]\n",
      " [-0.88812275]]\n",
      "Current iteration=6, loss=37630.69242821326\n",
      "t [[ 0.7554461 ]\n",
      " [-0.97172551]\n",
      " [-0.8898186 ]\n",
      " ...\n",
      " [ 0.32732327]\n",
      " [ 0.14390033]\n",
      " [-0.95794634]]\n",
      "t [[ 0.7554461 ]\n",
      " [-0.97172551]\n",
      " [-0.8898186 ]\n",
      " ...\n",
      " [ 0.32732327]\n",
      " [ 0.14390033]\n",
      " [-0.95794634]]\n",
      "t [[ 0.79625491]\n",
      " [-1.07970648]\n",
      " [-0.96760869]\n",
      " ...\n",
      " [ 0.31810159]\n",
      " [ 0.13712657]\n",
      " [-1.02102072]]\n",
      "t [[ 0.79625491]\n",
      " [-1.07970648]\n",
      " [-0.96760869]\n",
      " ...\n",
      " [ 0.31810159]\n",
      " [ 0.13712657]\n",
      " [-1.02102072]]\n",
      "Current iteration=8, loss=35910.18293563311\n",
      "t [[ 0.83045625]\n",
      " [-1.17883018]\n",
      " [-1.03909172]\n",
      " ...\n",
      " [ 0.30636358]\n",
      " [ 0.12881489]\n",
      " [-1.07861826]]\n",
      "t [[ 0.83045625]\n",
      " [-1.17883018]\n",
      " [-1.03909172]\n",
      " ...\n",
      " [ 0.30636358]\n",
      " [ 0.12881489]\n",
      " [-1.07861826]]\n",
      "t [[ 0.85940865]\n",
      " [-1.27015807]\n",
      " [-1.10511101]\n",
      " ...\n",
      " [ 0.2929408 ]\n",
      " [ 0.11951365]\n",
      " [-1.13161822]]\n",
      "loss=34676.64599555236\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.2054183 ]\n",
      " [-0.12219559]\n",
      " [-0.19311267]\n",
      " ...\n",
      " [ 0.29586694]\n",
      " [-0.19311267]\n",
      " [ 0.27712131]]\n",
      "t [[ 0.2054183 ]\n",
      " [-0.12219559]\n",
      " [-0.19311267]\n",
      " ...\n",
      " [ 0.29586694]\n",
      " [-0.19311267]\n",
      " [ 0.27712131]]\n",
      "t [[ 0.35974223]\n",
      " [-0.28068756]\n",
      " [-0.35192448]\n",
      " ...\n",
      " [ 0.5090191 ]\n",
      " [-0.35192448]\n",
      " [ 0.46339626]]\n",
      "t [[ 0.35974223]\n",
      " [-0.28068756]\n",
      " [-0.35192448]\n",
      " ...\n",
      " [ 0.5090191 ]\n",
      " [-0.35192448]\n",
      " [ 0.46339626]]\n",
      "Current iteration=2, loss=44276.23250781192\n",
      "t [[ 0.47816606]\n",
      " [-0.4435969 ]\n",
      " [-0.48704277]\n",
      " ...\n",
      " [ 0.66785799]\n",
      " [-0.48704277]\n",
      " [ 0.5941278 ]]\n",
      "t [[ 0.47816606]\n",
      " [-0.4435969 ]\n",
      " [-0.48704277]\n",
      " ...\n",
      " [ 0.66785799]\n",
      " [-0.48704277]\n",
      " [ 0.5941278 ]]\n",
      "t [[ 0.57083315]\n",
      " [-0.59868581]\n",
      " [-0.60484142]\n",
      " ...\n",
      " [ 0.7896263 ]\n",
      " [-0.60484142]\n",
      " [ 0.68929655]]\n",
      "t [[ 0.57083315]\n",
      " [-0.59868581]\n",
      " [-0.60484142]\n",
      " ...\n",
      " [ 0.7896263 ]\n",
      " [-0.60484142]\n",
      " [ 0.68929655]]\n",
      "Current iteration=4, loss=40165.80192876677\n",
      "t [[ 0.64456572]\n",
      " [-0.74218348]\n",
      " [-0.70931727]\n",
      " ...\n",
      " [ 0.88508371]\n",
      " [-0.70931727]\n",
      " [ 0.76056027]]\n",
      "t [[ 0.64456572]\n",
      " [-0.74218348]\n",
      " [-0.70931727]\n",
      " ...\n",
      " [ 0.88508371]\n",
      " [-0.70931727]\n",
      " [ 0.76056027]]\n",
      "t [[ 0.7040805 ]\n",
      " [-0.87365814]\n",
      " [-0.80313976]\n",
      " ...\n",
      " [ 0.96126509]\n",
      " [-0.80313976]\n",
      " [ 0.81510397]]\n",
      "t [[ 0.7040805 ]\n",
      " [-0.87365814]\n",
      " [-0.80313976]\n",
      " ...\n",
      " [ 0.96126509]\n",
      " [-0.80313976]\n",
      " [ 0.81510397]]\n",
      "Current iteration=6, loss=37612.95789572327\n",
      "t [[ 0.75272785]\n",
      " [-0.99389065]\n",
      " [-0.8881951 ]\n",
      " ...\n",
      " [ 1.02296865]\n",
      " [-0.8881951 ]\n",
      " [ 0.85758953]]\n",
      "t [[ 0.75272785]\n",
      " [-0.99389065]\n",
      " [-0.8881951 ]\n",
      " ...\n",
      " [ 1.02296865]\n",
      " [-0.8881951 ]\n",
      " [ 0.85758953]]\n",
      "t [[ 0.79294508]\n",
      " [-1.10401707]\n",
      " [-0.96587839]\n",
      " ...\n",
      " [ 1.07358426]\n",
      " [-0.96587839]\n",
      " [ 0.89117564]]\n",
      "t [[ 0.79294508]\n",
      " [-1.10401707]\n",
      " [-0.96587839]\n",
      " ...\n",
      " [ 1.07358426]\n",
      " [-0.96587839]\n",
      " [ 0.89117564]]\n",
      "Current iteration=8, loss=35885.965246746244\n",
      "t [[ 0.82654203]\n",
      " [-1.20519506]\n",
      " [-1.03725917]\n",
      " ...\n",
      " [ 1.11557612]\n",
      " [-1.03725917]\n",
      " [ 0.91807888]]\n",
      "t [[ 0.82654203]\n",
      " [-1.20519506]\n",
      " [-1.03725917]\n",
      " ...\n",
      " [ 1.11557612]\n",
      " [-1.03725917]\n",
      " [ 0.91807888]]\n",
      "t [[ 0.85488618]\n",
      " [-1.29848502]\n",
      " [-1.10318084]\n",
      " ...\n",
      " [ 1.15077713]\n",
      " [-1.10318084]\n",
      " [ 0.93989856]]\n",
      "loss=34647.66761284361\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07088334]\n",
      " [-0.33942471]\n",
      " [-0.19904255]\n",
      " ...\n",
      " [ 0.17020781]\n",
      " [ 0.07088334]\n",
      " [-0.28852199]]\n",
      "t [[ 0.07088334]\n",
      " [-0.33942471]\n",
      " [-0.19904255]\n",
      " ...\n",
      " [ 0.17020781]\n",
      " [ 0.07088334]\n",
      " [-0.28852199]]\n",
      "t [[ 0.11156967]\n",
      " [-0.63090987]\n",
      " [-0.36198612]\n",
      " ...\n",
      " [ 0.26014474]\n",
      " [ 0.11156967]\n",
      " [-0.47783892]]\n",
      "t [[ 0.11156967]\n",
      " [-0.63090987]\n",
      " [-0.36198612]\n",
      " ...\n",
      " [ 0.26014474]\n",
      " [ 0.11156967]\n",
      " [-0.47783892]]\n",
      "Current iteration=2, loss=44060.16498132691\n",
      "t [[ 0.13385202]\n",
      " [-0.88054504]\n",
      " [-0.50027192]\n",
      " ...\n",
      " [ 0.30775761]\n",
      " [ 0.13385202]\n",
      " [-0.61575613]]\n",
      "t [[ 0.13385202]\n",
      " [-0.88054504]\n",
      " [-0.50027192]\n",
      " ...\n",
      " [ 0.30775761]\n",
      " [ 0.13385202]\n",
      " [-0.61575613]]\n",
      "t [[ 0.14454415]\n",
      " [-1.09542781]\n",
      " [-0.62063488]\n",
      " ...\n",
      " [ 0.33169025]\n",
      " [ 0.14454415]\n",
      " [-0.72497496]]\n",
      "t [[ 0.14454415]\n",
      " [-1.09542781]\n",
      " [-0.62063488]\n",
      " ...\n",
      " [ 0.33169025]\n",
      " [ 0.14454415]\n",
      " [-0.72497496]]\n",
      "Current iteration=4, loss=39883.54509438077\n",
      "t [[ 0.14770032]\n",
      " [-1.28189187]\n",
      " [-0.72725252]\n",
      " ...\n",
      " [ 0.34162345]\n",
      " [ 0.14770032]\n",
      " [-0.81656015]]\n",
      "t [[ 0.14770032]\n",
      " [-1.28189187]\n",
      " [-0.72725252]\n",
      " ...\n",
      " [ 0.34162345]\n",
      " [ 0.14770032]\n",
      " [-0.81656015]]\n",
      "t [[ 0.14585279]\n",
      " [-1.44513032]\n",
      " [-0.82289792]\n",
      " ...\n",
      " [ 0.34286601]\n",
      " [ 0.14585279]\n",
      " [-0.89623399]]\n",
      "t [[ 0.14585279]\n",
      " [-1.44513032]\n",
      " [-0.82289792]\n",
      " ...\n",
      " [ 0.34286601]\n",
      " [ 0.14585279]\n",
      " [-0.89623399]]\n",
      "Current iteration=6, loss=37308.05132647411\n",
      "t [[ 0.140653  ]\n",
      " [-1.58926939]\n",
      " [-0.90952429]\n",
      " ...\n",
      " [ 0.33850028]\n",
      " [ 0.140653  ]\n",
      " [-0.96718177]]\n",
      "t [[ 0.140653  ]\n",
      " [-1.58926939]\n",
      " [-0.90952429]\n",
      " ...\n",
      " [ 0.33850028]\n",
      " [ 0.140653  ]\n",
      " [-0.96718177]]\n",
      "t [[ 0.13321591]\n",
      " [-1.71756018]\n",
      " [-0.98857418]\n",
      " ...\n",
      " [ 0.33041977]\n",
      " [ 0.13321591]\n",
      " [-1.03132323]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.13321591]\n",
      " [-1.71756018]\n",
      " [-0.98857418]\n",
      " ...\n",
      " [ 0.33041977]\n",
      " [ 0.13321591]\n",
      " [-1.03132323]]\n",
      "Current iteration=8, loss=35574.34738122067\n",
      "t [[ 0.12431448]\n",
      " [-1.83257046]\n",
      " [-1.06115288]\n",
      " ...\n",
      " [ 0.31984671]\n",
      " [ 0.12431448]\n",
      " [-1.08990795]]\n",
      "t [[ 0.12431448]\n",
      " [-1.83257046]\n",
      " [-1.06115288]\n",
      " ...\n",
      " [ 0.31984671]\n",
      " [ 0.12431448]\n",
      " [-1.08990795]]\n",
      "t [[ 0.11449509]\n",
      " [-1.93634311]\n",
      " [-1.12813158]\n",
      " ...\n",
      " [ 0.30760238]\n",
      " [ 0.11449509]\n",
      " [-1.14380699]]\n",
      "loss=34335.83061733999\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21405189]\n",
      " [-0.12478367]\n",
      " [-0.19970479]\n",
      " ...\n",
      " [ 0.16771655]\n",
      " [ 0.07060504]\n",
      " [-0.28416163]]\n",
      "t [[ 0.21405189]\n",
      " [-0.12478367]\n",
      " [-0.19970479]\n",
      " ...\n",
      " [ 0.16771655]\n",
      " [ 0.07060504]\n",
      " [-0.28416163]]\n",
      "t [[ 0.37345354]\n",
      " [-0.28982236]\n",
      " [-0.3629285 ]\n",
      " ...\n",
      " [ 0.25573172]\n",
      " [ 0.11091497]\n",
      " [-0.46917833]]\n",
      "t [[ 0.37345354]\n",
      " [-0.28982236]\n",
      " [-0.3629285 ]\n",
      " ...\n",
      " [ 0.25573172]\n",
      " [ 0.11091497]\n",
      " [-0.46917833]]\n",
      "Current iteration=2, loss=44047.522088323596\n",
      "t [[ 0.49489324]\n",
      " [-0.45953089]\n",
      " [-0.50128831]\n",
      " ...\n",
      " [ 0.30188746]\n",
      " [ 0.13279372]\n",
      " [-0.60318484]]\n",
      "t [[ 0.49489324]\n",
      " [-0.45953089]\n",
      " [-0.50128831]\n",
      " ...\n",
      " [ 0.30188746]\n",
      " [ 0.13279372]\n",
      " [-0.60318484]]\n",
      "t [[ 0.5894427 ]\n",
      " [-0.62060601]\n",
      " [-0.62161533]\n",
      " ...\n",
      " [ 0.3247314 ]\n",
      " [ 0.14308668]\n",
      " [-0.70896097]]\n",
      "t [[ 0.5894427 ]\n",
      " [-0.62060601]\n",
      " [-0.62161533]\n",
      " ...\n",
      " [ 0.3247314 ]\n",
      " [ 0.14308668]\n",
      " [-0.70896097]]\n",
      "Current iteration=4, loss=39870.56006893774\n",
      "t [[ 0.66439487]\n",
      " [-0.76916737]\n",
      " [-0.7281385 ]\n",
      " ...\n",
      " [ 0.33385837]\n",
      " [ 0.14586186]\n",
      " [-0.79754896]]\n",
      "t [[ 0.66439487]\n",
      " [-0.76916737]\n",
      " [-0.7281385 ]\n",
      " ...\n",
      " [ 0.33385837]\n",
      " [ 0.14586186]\n",
      " [-0.79754896]]\n",
      "t [[ 0.72472541]\n",
      " [-0.90491775]\n",
      " [-0.82365883]\n",
      " ...\n",
      " [ 0.33450805]\n",
      " [ 0.14365727]\n",
      " [-0.87461708]]\n",
      "t [[ 0.72472541]\n",
      " [-0.90491775]\n",
      " [-0.82365883]\n",
      " ...\n",
      " [ 0.33450805]\n",
      " [ 0.14365727]\n",
      " [-0.87461708]]\n",
      "Current iteration=6, loss=37295.54953749478\n",
      "t [[ 0.77393995]\n",
      " [-1.02879958]\n",
      " [-0.91014507]\n",
      " ...\n",
      " [ 0.32971093]\n",
      " [ 0.13812625]\n",
      " [-0.94329275]]\n",
      "t [[ 0.77393995]\n",
      " [-1.02879958]\n",
      " [-0.91014507]\n",
      " ...\n",
      " [ 0.32971093]\n",
      " [ 0.13812625]\n",
      " [-0.94329275]]\n",
      "t [[ 0.81457351]\n",
      " [-1.14208459]\n",
      " [-0.98904858]\n",
      " ...\n",
      " [ 0.32132238]\n",
      " [ 0.13038377]\n",
      " [-1.00544345]]\n",
      "t [[ 0.81457351]\n",
      " [-1.14208459]\n",
      " [-0.98904858]\n",
      " ...\n",
      " [ 0.32132238]\n",
      " [ 0.13038377]\n",
      " [-1.00544345]]\n",
      "Current iteration=8, loss=35562.06423341926\n",
      "t [[ 0.84849901]\n",
      " [-1.24603416]\n",
      " [-1.0614797 ]\n",
      " ...\n",
      " [ 0.31053669]\n",
      " [ 0.12120195]\n",
      " [-1.06227447]]\n",
      "t [[ 0.84849901]\n",
      " [-1.24603416]\n",
      " [-1.0614797 ]\n",
      " ...\n",
      " [ 0.31053669]\n",
      " [ 0.12120195]\n",
      " [-1.06227447]]\n",
      "t [[ 0.87712517]\n",
      " [-1.34178584]\n",
      " [-1.1283126 ]\n",
      " ...\n",
      " [ 0.29815461]\n",
      " [ 0.11112589]\n",
      " [-1.11462031]]\n",
      "loss=34323.43885463796\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.21130839]\n",
      " [-0.11945383]\n",
      " [-0.19895687]\n",
      " ...\n",
      " [ 0.1703042 ]\n",
      " [ 0.07190636]\n",
      " [-0.29291239]]\n",
      "t [[ 0.21130839]\n",
      " [-0.11945383]\n",
      " [-0.19895687]\n",
      " ...\n",
      " [ 0.1703042 ]\n",
      " [ 0.07190636]\n",
      " [-0.29291239]]\n",
      "t [[ 0.36868897]\n",
      " [-0.27934595]\n",
      " [-0.36143867]\n",
      " ...\n",
      " [ 0.25878201]\n",
      " [ 0.11315804]\n",
      " [-0.48372087]]\n",
      "t [[ 0.36868897]\n",
      " [-0.27934595]\n",
      " [-0.36143867]\n",
      " ...\n",
      " [ 0.25878201]\n",
      " [ 0.11315804]\n",
      " [-0.48372087]]\n",
      "Current iteration=2, loss=44118.3880930243\n",
      "t [[ 0.48864247]\n",
      " [-0.44394067]\n",
      " [-0.49912979]\n",
      " ...\n",
      " [ 0.30438988]\n",
      " [ 0.135777  ]\n",
      " [-0.62188403]]\n",
      "t [[ 0.48864247]\n",
      " [-0.44394067]\n",
      " [-0.49912979]\n",
      " ...\n",
      " [ 0.30438988]\n",
      " [ 0.135777  ]\n",
      " [-0.62188403]]\n",
      "t [[ 0.58210733]\n",
      " [-0.59998186]\n",
      " [-0.61886451]\n",
      " ...\n",
      " [ 0.32613219]\n",
      " [ 0.14668155]\n",
      " [-0.73080434]]\n",
      "t [[ 0.58210733]\n",
      " [-0.59998186]\n",
      " [-0.61886451]\n",
      " ...\n",
      " [ 0.32613219]\n",
      " [ 0.14668155]\n",
      " [-0.73080434]]\n",
      "Current iteration=4, loss=39995.01990683925\n",
      "t [[ 0.65627104]\n",
      " [-0.74365348]\n",
      " [-0.72485947]\n",
      " ...\n",
      " [ 0.33382931]\n",
      " [ 0.14997693]\n",
      " [-0.82185514]]\n",
      "t [[ 0.65627104]\n",
      " [-0.74365348]\n",
      " [-0.72485947]\n",
      " ...\n",
      " [ 0.33382931]\n",
      " [ 0.14997693]\n",
      " [-0.82185514]]\n",
      "t [[ 0.71602724]\n",
      " [-0.87470232]\n",
      " [-0.81990264]\n",
      " ...\n",
      " [ 0.33285656]\n",
      " [ 0.14822313]\n",
      " [-0.90090356]]\n",
      "t [[ 0.71602724]\n",
      " [-0.87470232]\n",
      " [-0.81990264]\n",
      " ...\n",
      " [ 0.33285656]\n",
      " [ 0.14822313]\n",
      " [-0.90090356]]\n",
      "Current iteration=6, loss=37456.60856337327\n",
      "t [[ 0.76482162]\n",
      " [-0.99409267]\n",
      " [-0.90595185]\n",
      " ...\n",
      " [ 0.32632828]\n",
      " [ 0.14308806]\n",
      " [-0.97120347]]\n",
      "t [[ 0.76482162]\n",
      " [-0.99409267]\n",
      " [-0.90595185]\n",
      " ...\n",
      " [ 0.32632828]\n",
      " [ 0.14308806]\n",
      " [-0.97120347]]\n",
      "t [[ 0.80514638]\n",
      " [-1.10310284]\n",
      " [-0.98444993]\n",
      " ...\n",
      " [ 0.31615254]\n",
      " [ 0.13569705]\n",
      " [-1.03470643]]\n",
      "t [[ 0.80514638]\n",
      " [-1.10310284]\n",
      " [-0.98444993]\n",
      " ...\n",
      " [ 0.31615254]\n",
      " [ 0.13569705]\n",
      " [-1.03470643]]\n",
      "Current iteration=8, loss=35749.14309832365\n",
      "t [[ 0.83884382]\n",
      " [-1.20299162]\n",
      " [-1.05650064]\n",
      " ...\n",
      " [ 0.30355736]\n",
      " [ 0.12682997]\n",
      " [-1.09267599]]\n",
      "t [[ 0.83884382]\n",
      " [-1.20299162]\n",
      " [-1.05650064]\n",
      " ...\n",
      " [ 0.30355736]\n",
      " [ 0.12682997]\n",
      " [-1.09267599]]\n",
      "t [[ 0.86730063]\n",
      " [-1.29488894]\n",
      " [-1.12297312]\n",
      " ...\n",
      " [ 0.28936545]\n",
      " [ 0.11703791]\n",
      " [-1.1459885 ]]\n",
      "loss=34529.78226734584\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21097014]\n",
      " [-0.12549818]\n",
      " [-0.19833193]\n",
      " ...\n",
      " [ 0.30386335]\n",
      " [-0.19833193]\n",
      " [ 0.28461107]]\n",
      "t [[ 0.21097014]\n",
      " [-0.12549818]\n",
      " [-0.19833193]\n",
      " ...\n",
      " [ 0.30386335]\n",
      " [-0.19833193]\n",
      " [ 0.28461107]]\n",
      "t [[ 0.36807456]\n",
      " [-0.28925828]\n",
      " [-0.36050105]\n",
      " ...\n",
      " [ 0.52052575]\n",
      " [-0.36050105]\n",
      " [ 0.47345033]]\n",
      "t [[ 0.36807456]\n",
      " [-0.28925828]\n",
      " [-0.36050105]\n",
      " ...\n",
      " [ 0.52052575]\n",
      " [-0.36050105]\n",
      " [ 0.47345033]]\n",
      "Current iteration=2, loss=44120.544769936285\n",
      "t [[ 0.48777102]\n",
      " [-0.45680422]\n",
      " [-0.49799513]\n",
      " ...\n",
      " [ 0.68074158]\n",
      " [-0.49799513]\n",
      " [ 0.60473655]]\n",
      "t [[ 0.48777102]\n",
      " [-0.45680422]\n",
      " [-0.49799513]\n",
      " ...\n",
      " [ 0.68074158]\n",
      " [-0.49799513]\n",
      " [ 0.60473655]]\n",
      "t [[ 0.58087284]\n",
      " [-0.6154614 ]\n",
      " [-0.61758084]\n",
      " ...\n",
      " [ 0.80282221]\n",
      " [-0.61758084]\n",
      " [ 0.69962211]]\n",
      "t [[ 0.58087284]\n",
      " [-0.6154614 ]\n",
      " [-0.61758084]\n",
      " ...\n",
      " [ 0.80282221]\n",
      " [-0.61758084]\n",
      " [ 0.69962211]]\n",
      "Current iteration=4, loss=39985.55816793359\n",
      "t [[ 0.65456811]\n",
      " [-0.7616021 ]\n",
      " [-0.72344706]\n",
      " ...\n",
      " [ 0.89803909]\n",
      " [-0.72344706]\n",
      " [ 0.7702512 ]]\n",
      "t [[ 0.65456811]\n",
      " [-0.7616021 ]\n",
      " [-0.72344706]\n",
      " ...\n",
      " [ 0.89803909]\n",
      " [-0.72344706]\n",
      " [ 0.7702512 ]]\n",
      "t [[ 0.71378267]\n",
      " [-0.89502183]\n",
      " [-0.8183709 ]\n",
      " ...\n",
      " [ 0.97369267]\n",
      " [-0.8183709 ]\n",
      " [ 0.82402733]]\n",
      "t [[ 0.71378267]\n",
      " [-0.89502183]\n",
      " [-0.8183709 ]\n",
      " ...\n",
      " [ 0.97369267]\n",
      " [-0.8183709 ]\n",
      " [ 0.82402733]]\n",
      "Current iteration=6, loss=37438.247834137466\n",
      "t [[ 0.76199063]\n",
      " [-1.01669218]\n",
      " [-0.90430655]\n",
      " ...\n",
      " [ 1.03472843]\n",
      " [-0.90430655]\n",
      " [ 0.86571819]]\n",
      "t [[ 0.76199063]\n",
      " [-1.01669218]\n",
      " [-0.90430655]\n",
      " ...\n",
      " [ 1.03472843]\n",
      " [-0.90430655]\n",
      " [ 0.86571819]]\n",
      "t [[ 0.80170431]\n",
      " [-1.12788875]\n",
      " [-0.98269592]\n",
      " ...\n",
      " [ 1.08462179]\n",
      " [-0.98269592]\n",
      " [ 0.89853651]]\n",
      "t [[ 0.80170431]\n",
      " [-1.12788875]\n",
      " [-0.98269592]\n",
      " ...\n",
      " [ 1.08462179]\n",
      " [-0.98269592]\n",
      " [ 0.89853651]]\n",
      "Current iteration=8, loss=35724.30805081638\n",
      "t [[ 0.83477951]\n",
      " [-1.2298673 ]\n",
      " [-1.0546427 ]\n",
      " ...\n",
      " [ 1.12588778]\n",
      " [-1.0546427 ]\n",
      " [ 0.92472681]]\n",
      "t [[ 0.83477951]\n",
      " [-1.2298673 ]\n",
      " [-1.0546427 ]\n",
      " ...\n",
      " [ 1.12588778]\n",
      " [-1.0546427 ]\n",
      " [ 0.92472681]]\n",
      "t [[ 0.86261177]\n",
      " [-1.32375687]\n",
      " [-1.12101614]\n",
      " ...\n",
      " [ 1.16038959]\n",
      " [-1.12101614]\n",
      " [ 0.94590221]]\n",
      "loss=34500.236786639485\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07274869]\n",
      " [-0.34835694]\n",
      " [-0.20428051]\n",
      " ...\n",
      " [ 0.17468696]\n",
      " [ 0.07274869]\n",
      " [-0.29611468]]\n",
      "t [[ 0.07274869]\n",
      " [-0.34835694]\n",
      " [-0.20428051]\n",
      " ...\n",
      " [ 0.17468696]\n",
      " [ 0.07274869]\n",
      " [-0.29611468]]\n",
      "t [[ 0.1137077 ]\n",
      " [-0.64623589]\n",
      " [-0.37055547]\n",
      " ...\n",
      " [ 0.26486691]\n",
      " [ 0.1137077 ]\n",
      " [-0.48779256]]\n",
      "t [[ 0.1137077 ]\n",
      " [-0.64623589]\n",
      " [-0.37055547]\n",
      " ...\n",
      " [ 0.26486691]\n",
      " [ 0.1137077 ]\n",
      " [-0.48779256]]\n",
      "Current iteration=2, loss=43905.21646735724\n",
      "t [[ 0.13561758]\n",
      " [-0.90024024]\n",
      " [-0.51118905]\n",
      " ...\n",
      " [ 0.31151484]\n",
      " [ 0.13561758]\n",
      " [-0.62665712]]\n",
      "t [[ 0.13561758]\n",
      " [-0.90024024]\n",
      " [-0.51118905]\n",
      " ...\n",
      " [ 0.31151484]\n",
      " [ 0.13561758]\n",
      " [-0.62665712]]\n",
      "t [[ 0.14568699]\n",
      " [-1.11805046]\n",
      " [-0.63331392]\n",
      " ...\n",
      " [ 0.33422387]\n",
      " [ 0.14568699]\n",
      " [-0.73650314]]\n",
      "t [[ 0.14568699]\n",
      " [-1.11805046]\n",
      " [-0.63331392]\n",
      " ...\n",
      " [ 0.33422387]\n",
      " [ 0.14568699]\n",
      " [-0.73650314]]\n",
      "Current iteration=4, loss=39705.71629948238\n",
      "t [[ 0.14814168]\n",
      " [-1.30645096]\n",
      " [-0.74129901]\n",
      " ...\n",
      " [ 0.34295876]\n",
      " [ 0.14814168]\n",
      " [-0.8286527 ]]\n",
      "t [[ 0.14814168]\n",
      " [-1.30645096]\n",
      " [-0.74129901]\n",
      " ...\n",
      " [ 0.34295876]\n",
      " [ 0.14814168]\n",
      " [-0.8286527 ]]\n",
      "t [[ 0.14559327]\n",
      " [-1.47094819]\n",
      " [-0.8380242 ]\n",
      " ...\n",
      " [ 0.34310033]\n",
      " [ 0.14559327]\n",
      " [-0.90886111]]\n",
      "t [[ 0.14559327]\n",
      " [-1.47094819]\n",
      " [-0.8380242 ]\n",
      " ...\n",
      " [ 0.34310033]\n",
      " [ 0.14559327]\n",
      " [-0.90886111]]\n",
      "Current iteration=6, loss=37136.696196123565\n",
      "t [[ 0.13973129]\n",
      " [-1.6158789 ]\n",
      " [-0.92551036]\n",
      " ...\n",
      " [ 0.33774107]\n",
      " [ 0.13973129]\n",
      " [-0.9803007 ]]\n",
      "t [[ 0.13973129]\n",
      " [-1.6158789 ]\n",
      " [-0.92551036]\n",
      " ...\n",
      " [ 0.33774107]\n",
      " [ 0.13973129]\n",
      " [-0.9803007 ]]\n",
      "t [[ 0.13168875]\n",
      " [-1.74463515]\n",
      " [-1.0052469 ]\n",
      " ...\n",
      " [ 0.32876869]\n",
      " [ 0.13168875]\n",
      " [-1.04487742]]\n",
      "t [[ 0.13168875]\n",
      " [-1.74463515]\n",
      " [-1.0052469 ]\n",
      " ...\n",
      " [ 0.32876869]\n",
      " [ 0.13168875]\n",
      " [-1.04487742]]\n",
      "Current iteration=8, loss=35416.47293696778\n",
      "t [[ 0.12224619]\n",
      " [-1.85988007]\n",
      " [-1.07837333]\n",
      " ...\n",
      " [ 0.31739658]\n",
      " [ 0.12224619]\n",
      " [-1.10383405]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.12224619]\n",
      " [-1.85988007]\n",
      " [-1.07837333]\n",
      " ...\n",
      " [ 0.31739658]\n",
      " [ 0.12224619]\n",
      " [-1.10383405]]\n",
      "t [[ 0.11195181]\n",
      " [-1.96372203]\n",
      " [-1.14578685]\n",
      " ...\n",
      " [ 0.30443743]\n",
      " [ 0.11195181]\n",
      " [-1.15804073]]\n",
      "loss=34192.32475298665\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21968484]\n",
      " [-0.12806745]\n",
      " [-0.20496018]\n",
      " ...\n",
      " [ 0.17213014]\n",
      " [ 0.07246307]\n",
      " [-0.29163957]]\n",
      "t [[ 0.21968484]\n",
      " [-0.12806745]\n",
      " [-0.20496018]\n",
      " ...\n",
      " [ 0.17213014]\n",
      " [ 0.07246307]\n",
      " [-0.29163957]]\n",
      "t [[ 0.38183782]\n",
      " [-0.29850856]\n",
      " [-0.37151269]\n",
      " ...\n",
      " [ 0.26035263]\n",
      " [ 0.11303328]\n",
      " [-0.478906  ]]\n",
      "t [[ 0.38183782]\n",
      " [-0.29850856]\n",
      " [-0.37151269]\n",
      " ...\n",
      " [ 0.26035263]\n",
      " [ 0.11303328]\n",
      " [-0.478906  ]]\n",
      "Current iteration=2, loss=43892.48833007799\n",
      "t [[ 0.50449743]\n",
      " [-0.47291871]\n",
      " [-0.51221161]\n",
      " ...\n",
      " [ 0.30552948]\n",
      " [ 0.13452752]\n",
      " [-0.61377768]]\n",
      "t [[ 0.50449743]\n",
      " [-0.47291871]\n",
      " [-0.51221161]\n",
      " ...\n",
      " [ 0.30552948]\n",
      " [ 0.13452752]\n",
      " [-0.61377768]]\n",
      "t [[ 0.59943556]\n",
      " [-0.63756286]\n",
      " [-0.63429105]\n",
      " ...\n",
      " [ 0.32715001]\n",
      " [ 0.14418752]\n",
      " [-0.7201271 ]]\n",
      "t [[ 0.59943556]\n",
      " [-0.63756286]\n",
      " [-0.63429105]\n",
      " ...\n",
      " [ 0.32715001]\n",
      " [ 0.14418752]\n",
      " [-0.7201271 ]]\n",
      "Current iteration=4, loss=39692.75428915722\n",
      "t [[ 0.67431571]\n",
      " [-0.78873774]\n",
      " [-0.74217306]\n",
      " ...\n",
      " [ 0.33508678]\n",
      " [ 0.14625302]\n",
      " [-0.80924695]]\n",
      "t [[ 0.67431571]\n",
      " [-0.78873774]\n",
      " [-0.74217306]\n",
      " ...\n",
      " [ 0.33508678]\n",
      " [ 0.14625302]\n",
      " [-0.80924695]]\n",
      "t [[ 0.73432264]\n",
      " [-0.92639476]\n",
      " [-0.83876584]\n",
      " ...\n",
      " [ 0.33464761]\n",
      " [ 0.14334124]\n",
      " [-0.88683204]]\n",
      "t [[ 0.73432264]\n",
      " [-0.92639476]\n",
      " [-0.83876584]\n",
      " ...\n",
      " [ 0.33464761]\n",
      " [ 0.14334124]\n",
      " [-0.88683204]]\n",
      "Current iteration=6, loss=37124.23043358894\n",
      "t [[ 0.78308462]\n",
      " [-1.05167698]\n",
      " [-0.92610577]\n",
      " ...\n",
      " [ 0.32887088]\n",
      " [ 0.13714334]\n",
      " [-0.95599195]]\n",
      "t [[ 0.78308462]\n",
      " [-1.05167698]\n",
      " [-0.92610577]\n",
      " ...\n",
      " [ 0.32887088]\n",
      " [ 0.13714334]\n",
      " [-0.95599195]]\n",
      "t [[ 0.82321019]\n",
      " [-1.16599927]\n",
      " [-1.00569085]\n",
      " ...\n",
      " [ 0.31960495]\n",
      " [ 0.12879214]\n",
      " [-1.01857702]]\n",
      "t [[ 0.82321019]\n",
      " [-1.16599927]\n",
      " [-1.00569085]\n",
      " ...\n",
      " [ 0.31960495]\n",
      " [ 0.12879214]\n",
      " [-1.01857702]]\n",
      "Current iteration=8, loss=35404.199815307176\n",
      "t [[ 0.85661676]\n",
      " [-1.27072178]\n",
      " [-1.07866552]\n",
      " ...\n",
      " [ 0.30803466]\n",
      " [ 0.11906711]\n",
      " [-1.0757835 ]]\n",
      "t [[ 0.85661676]\n",
      " [-1.27072178]\n",
      " [-1.07866552]\n",
      " ...\n",
      " [ 0.30803466]\n",
      " [ 0.11906711]\n",
      " [-1.0757835 ]]\n",
      "t [[ 0.88473975]\n",
      " [-1.36705025]\n",
      " [-1.14592976]\n",
      " ...\n",
      " [ 0.29495188]\n",
      " [ 0.10851496]\n",
      " [-1.12844351]]\n",
      "loss=34179.90490484054\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.21686914]\n",
      " [-0.12259736]\n",
      " [-0.20419257]\n",
      " ...\n",
      " [ 0.17478589]\n",
      " [ 0.07379863]\n",
      " [-0.30062061]]\n",
      "t [[ 0.21686914]\n",
      " [-0.12259736]\n",
      " [-0.20419257]\n",
      " ...\n",
      " [ 0.17478589]\n",
      " [ 0.07379863]\n",
      " [-0.30062061]]\n",
      "t [[ 0.37696681]\n",
      " [-0.28776126]\n",
      " [-0.36998412]\n",
      " ...\n",
      " [ 0.26342975]\n",
      " [ 0.11532618]\n",
      " [-0.49375517]]\n",
      "t [[ 0.37696681]\n",
      " [-0.28776126]\n",
      " [-0.36998412]\n",
      " ...\n",
      " [ 0.26342975]\n",
      " [ 0.11532618]\n",
      " [-0.49375517]]\n",
      "Current iteration=2, loss=43965.12101522318\n",
      "t [[ 0.4981291 ]\n",
      " [-0.45692483]\n",
      " [-0.51000073]\n",
      " ...\n",
      " [ 0.30799236]\n",
      " [ 0.13756975]\n",
      " [-0.63280899]]\n",
      "t [[ 0.4981291 ]\n",
      " [-0.45692483]\n",
      " [-0.51000073]\n",
      " ...\n",
      " [ 0.30799236]\n",
      " [ 0.13756975]\n",
      " [-0.63280899]]\n",
      "t [[ 0.59198532]\n",
      " [-0.61640861]\n",
      " [-0.63147812]\n",
      " ...\n",
      " [ 0.32843784]\n",
      " [ 0.14784729]\n",
      " [-0.74230649]]\n",
      "t [[ 0.59198532]\n",
      " [-0.61640861]\n",
      " [-0.63147812]\n",
      " ...\n",
      " [ 0.32843784]\n",
      " [ 0.14784729]\n",
      " [-0.74230649]]\n",
      "Current iteration=4, loss=39819.625876920494\n",
      "t [[ 0.66608646]\n",
      " [-0.76257966]\n",
      " [-0.73882448]\n",
      " ...\n",
      " [ 0.33487216]\n",
      " [ 0.15043692]\n",
      " [-0.83388308]]\n",
      "t [[ 0.66608646]\n",
      " [-0.76257966]\n",
      " [-0.73882448]\n",
      " ...\n",
      " [ 0.33487216]\n",
      " [ 0.15043692]\n",
      " [-0.83388308]]\n",
      "t [[ 0.72553106]\n",
      " [-0.89543522]\n",
      " [-0.83493395]\n",
      " ...\n",
      " [ 0.33274217]\n",
      " [ 0.14797847]\n",
      " [-0.9134376 ]]\n",
      "t [[ 0.72553106]\n",
      " [-0.89543522]\n",
      " [-0.83493395]\n",
      " ...\n",
      " [ 0.33274217]\n",
      " [ 0.14797847]\n",
      " [-0.9134376 ]]\n",
      "Current iteration=6, loss=37287.79867884629\n",
      "t [[ 0.77388525]\n",
      " [-1.0161398 ]\n",
      " [-0.92183138]\n",
      " ...\n",
      " [ 0.32517117]\n",
      " [ 0.14217811]\n",
      " [-0.98420844]]\n",
      "t [[ 0.77388525]\n",
      " [-1.0161398 ]\n",
      " [-0.92183138]\n",
      " ...\n",
      " [ 0.32517117]\n",
      " [ 0.14217811]\n",
      " [-0.98420844]]\n",
      "t [[ 0.81371368]\n",
      " [-1.1261135 ]\n",
      " [-1.00100589]\n",
      " ...\n",
      " [ 0.31406027]\n",
      " [ 0.13417928]\n",
      " [-1.04813116]]\n",
      "t [[ 0.81371368]\n",
      " [-1.1261135 ]\n",
      " [-1.00100589]\n",
      " ...\n",
      " [ 0.31406027]\n",
      " [ 0.13417928]\n",
      " [-1.04813116]]\n",
      "Current iteration=8, loss=35593.70211941295\n",
      "t [[ 0.84690265]\n",
      " [-1.22671235]\n",
      " [-1.07359513]\n",
      " ...\n",
      " [ 0.30062776]\n",
      " [ 0.12476939]\n",
      " [-1.10646087]]\n",
      "t [[ 0.84690265]\n",
      " [-1.22671235]\n",
      " [-1.07359513]\n",
      " ...\n",
      " [ 0.30062776]\n",
      " [ 0.12476939]\n",
      " [-1.10646087]]\n",
      "t [[ 0.87486535]\n",
      " [-1.31913302]\n",
      " [-1.14049394]\n",
      " ...\n",
      " [ 0.28568707]\n",
      " [ 0.11450133]\n",
      " [-1.16007186]]\n",
      "loss=34388.52354051628\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.21652199]\n",
      " [-0.12880076]\n",
      " [-0.20355119]\n",
      " ...\n",
      " [ 0.31185975]\n",
      " [-0.20355119]\n",
      " [ 0.29210084]]\n",
      "t [[ 0.21652199]\n",
      " [-0.12880076]\n",
      " [-0.20355119]\n",
      " ...\n",
      " [ 0.31185975]\n",
      " [-0.20355119]\n",
      " [ 0.29210084]]\n",
      "t [[ 0.37633594]\n",
      " [-0.29787899]\n",
      " [-0.36902979]\n",
      " ...\n",
      " [ 0.53191758]\n",
      " [-0.36902979]\n",
      " [ 0.48337851]]\n",
      "t [[ 0.37633594]\n",
      " [-0.29787899]\n",
      " [-0.36902979]\n",
      " ...\n",
      " [ 0.53191758]\n",
      " [-0.36902979]\n",
      " [ 0.48337851]]\n",
      "Current iteration=2, loss=43967.00426888201\n",
      "t [[ 0.49722936]\n",
      " [-0.47002951]\n",
      " [-0.50885011]\n",
      " ...\n",
      " [ 0.69340342]\n",
      " [-0.50885011]\n",
      " [ 0.61511899]]\n",
      "t [[ 0.49722936]\n",
      " [-0.47002951]\n",
      " [-0.50885011]\n",
      " ...\n",
      " [ 0.69340342]\n",
      " [-0.50885011]\n",
      " [ 0.61511899]]\n",
      "t [[ 0.59070278]\n",
      " [-0.63217471]\n",
      " [-0.63017818]\n",
      " ...\n",
      " [ 0.81571624]\n",
      " [-0.63017818]\n",
      " [ 0.70965852]]\n",
      "t [[ 0.59070278]\n",
      " [-0.63217471]\n",
      " [-0.63017818]\n",
      " ...\n",
      " [ 0.81571624]\n",
      " [-0.63017818]\n",
      " [ 0.70965852]]\n",
      "Current iteration=4, loss=39809.61267147521\n",
      "t [[ 0.664313  ]\n",
      " [-0.78086515]\n",
      " [-0.73739441]\n",
      " ...\n",
      " [ 0.91063678]\n",
      " [-0.73739441]\n",
      " [ 0.77961767]]\n",
      "t [[ 0.664313  ]\n",
      " [-0.78086515]\n",
      " [-0.73739441]\n",
      " ...\n",
      " [ 0.91063678]\n",
      " [-0.73739441]\n",
      " [ 0.77961767]]\n",
      "t [[ 0.72319374]\n",
      " [-0.91614114]\n",
      " [-0.83338262]\n",
      " ...\n",
      " [ 0.98572593]\n",
      " [-0.83338262]\n",
      " [ 0.83260936]]\n",
      "t [[ 0.72319374]\n",
      " [-0.91614114]\n",
      " [-0.83338262]\n",
      " ...\n",
      " [ 0.98572593]\n",
      " [-0.83338262]\n",
      " [ 0.83260936]]\n",
      "Current iteration=6, loss=37268.82530432234\n",
      "t [[ 0.77094067]\n",
      " [-1.03917118]\n",
      " [-0.92016446]\n",
      " ...\n",
      " [ 1.04607215]\n",
      " [-0.92016446]\n",
      " [ 0.87350121]]\n",
      "t [[ 0.77094067]\n",
      " [-1.03917118]\n",
      " [-0.92016446]\n",
      " ...\n",
      " [ 1.04607215]\n",
      " [-0.92016446]\n",
      " [ 0.87350121]]\n",
      "t [[ 0.81013887]\n",
      " [-1.15137128]\n",
      " [-0.99922838]\n",
      " ...\n",
      " [ 1.09523302]\n",
      " [-0.99922838]\n",
      " [ 0.90555623]]\n",
      "t [[ 0.81013887]\n",
      " [-1.15137128]\n",
      " [-0.99922838]\n",
      " ...\n",
      " [ 1.09523302]\n",
      " [-0.99922838]\n",
      " [ 0.90555623]]\n",
      "Current iteration=8, loss=35568.26825638566\n",
      "t [[ 0.84268812]\n",
      " [-1.2540942 ]\n",
      " [-1.07171208]\n",
      " ...\n",
      " [ 1.13577189]\n",
      " [-1.07171208]\n",
      " [ 0.93104445]]\n",
      "t [[ 0.84268812]\n",
      " [-1.2540942 ]\n",
      " [-1.07171208]\n",
      " ...\n",
      " [ 1.13577189]\n",
      " [-1.07171208]\n",
      " [ 0.93104445]]\n",
      "t [[ 0.87001033]\n",
      " [-1.34853607]\n",
      " [-1.1385105 ]\n",
      " ...\n",
      " [ 1.1695801 ]\n",
      " [-1.1385105 ]\n",
      " [ 0.95159095]]\n",
      "loss=34358.433293611604\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07461404]\n",
      " [-0.35728917]\n",
      " [-0.20951847]\n",
      " ...\n",
      " [ 0.17916611]\n",
      " [ 0.07461404]\n",
      " [-0.30370736]]\n",
      "t [[ 0.07461404]\n",
      " [-0.35728917]\n",
      " [-0.20951847]\n",
      " ...\n",
      " [ 0.17916611]\n",
      " [ 0.07461404]\n",
      " [-0.30370736]]\n",
      "t [[ 0.11580618]\n",
      " [-0.66149784]\n",
      " [-0.3790772 ]\n",
      " ...\n",
      " [ 0.26948366]\n",
      " [ 0.11580618]\n",
      " [-0.49761633]]\n",
      "t [[ 0.11580618]\n",
      " [-0.66149784]\n",
      " [-0.3790772 ]\n",
      " ...\n",
      " [ 0.26948366]\n",
      " [ 0.11580618]\n",
      " [-0.49761633]]\n",
      "Current iteration=2, loss=43752.388033628726\n",
      "t [[ 0.13731229]\n",
      " [-0.91977147]\n",
      " [-0.52201025]\n",
      " ...\n",
      " [ 0.31510747]\n",
      " [ 0.13731229]\n",
      " [-0.63735943]]\n",
      "t [[ 0.13731229]\n",
      " [-0.91977147]\n",
      " [-0.52201025]\n",
      " ...\n",
      " [ 0.31510747]\n",
      " [ 0.13731229]\n",
      " [-0.63735943]]\n",
      "t [[ 0.14674076]\n",
      " [-1.14040296]\n",
      " [-0.64585374]\n",
      " ...\n",
      " [ 0.33657434]\n",
      " [ 0.14674076]\n",
      " [-0.74780895]]\n",
      "t [[ 0.14674076]\n",
      " [-1.14040296]\n",
      " [-0.64585374]\n",
      " ...\n",
      " [ 0.33657434]\n",
      " [ 0.14674076]\n",
      " [-0.74780895]]\n",
      "Current iteration=4, loss=39532.08211669788\n",
      "t [[ 0.14848631]\n",
      " [-1.33064229]\n",
      " [-0.75516719]\n",
      " ...\n",
      " [ 0.34411364]\n",
      " [ 0.14848631]\n",
      " [-0.84051628]]\n",
      "t [[ 0.14848631]\n",
      " [-1.33064229]\n",
      " [-0.75516719]\n",
      " ...\n",
      " [ 0.34411364]\n",
      " [ 0.14848631]\n",
      " [-0.84051628]]\n",
      "t [[ 0.14523707]\n",
      " [-1.49631433]\n",
      " [-0.85293631]\n",
      " ...\n",
      " [ 0.34316634]\n",
      " [ 0.14523707]\n",
      " [-0.92125523]]\n",
      "t [[ 0.14523707]\n",
      " [-1.49631433]\n",
      " [-0.85293631]\n",
      " ...\n",
      " [ 0.34316634]\n",
      " [ 0.14523707]\n",
      " [-0.92125523]]\n",
      "Current iteration=6, loss=36970.4606278001\n",
      "t [[ 0.13871859]\n",
      " [-1.64196664]\n",
      " [-0.94124919]\n",
      " ...\n",
      " [ 0.3368296 ]\n",
      " [ 0.13871859]\n",
      " [-0.99318002]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.13871859]\n",
      " [-1.64196664]\n",
      " [-0.94124919]\n",
      " ...\n",
      " [ 0.3368296 ]\n",
      " [ 0.13871859]\n",
      " [-0.99318002]]\n",
      "t [[ 0.13008038]\n",
      " [-1.77113081]\n",
      " [-1.02164182]\n",
      " ...\n",
      " [ 0.32698306]\n",
      " [ 0.13008038]\n",
      " [-1.05818193]]\n",
      "t [[ 0.13008038]\n",
      " [-1.77113081]\n",
      " [-1.02164182]\n",
      " ...\n",
      " [ 0.32698306]\n",
      " [ 0.13008038]\n",
      " [-1.05818193]]\n",
      "Current iteration=8, loss=35264.00639374067\n",
      "t [[ 0.12010942]\n",
      " [-1.8865635 ]\n",
      " [-1.09528787]\n",
      " ...\n",
      " [ 0.31483044]\n",
      " [ 0.12010942]\n",
      " [-1.11749757]]\n",
      "t [[ 0.12010942]\n",
      " [-1.8865635 ]\n",
      " [-1.09528787]\n",
      " ...\n",
      " [ 0.31483044]\n",
      " [ 0.12010942]\n",
      " [-1.11749757]]\n",
      "t [[ 0.10935484]\n",
      " [-1.99043675]\n",
      " [-1.16311037]\n",
      " ...\n",
      " [ 0.3011754 ]\n",
      " [ 0.10935484]\n",
      " [-1.17199715]]\n",
      "loss=34054.21258921692\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.22531778]\n",
      " [-0.13135123]\n",
      " [-0.21021557]\n",
      " ...\n",
      " [ 0.17654374]\n",
      " [ 0.0743211 ]\n",
      " [-0.2991175 ]]\n",
      "t [[ 0.22531778]\n",
      " [-0.13135123]\n",
      " [-0.21021557]\n",
      " ...\n",
      " [ 0.17654374]\n",
      " [ 0.0743211 ]\n",
      " [-0.2991175 ]]\n",
      "t [[ 0.39015057]\n",
      " [-0.30724695]\n",
      " [-0.38004877]\n",
      " ...\n",
      " [ 0.26486886]\n",
      " [ 0.11511191]\n",
      " [-0.48850392]]\n",
      "t [[ 0.39015057]\n",
      " [-0.30724695]\n",
      " [-0.38004877]\n",
      " ...\n",
      " [ 0.26486886]\n",
      " [ 0.11511191]\n",
      " [-0.48850392]]\n",
      "Current iteration=2, loss=43739.58170587395\n",
      "t [[ 0.51395567]\n",
      " [-0.48632396]\n",
      " [-0.5230382 ]\n",
      " ...\n",
      " [ 0.30900872]\n",
      " [ 0.13619035]\n",
      " [-0.62417341]]\n",
      "t [[ 0.51395567]\n",
      " [-0.48632396]\n",
      " [-0.5230382 ]\n",
      " ...\n",
      " [ 0.30900872]\n",
      " [ 0.13619035]\n",
      " [-0.62417341]]\n",
      "t [[ 0.60922137]\n",
      " [-0.65445451]\n",
      " [-0.64682672]\n",
      " ...\n",
      " [ 0.32938831]\n",
      " [ 0.14519931]\n",
      " [-0.73107447]]\n",
      "t [[ 0.60922137]\n",
      " [-0.65445451]\n",
      " [-0.64682672]\n",
      " ...\n",
      " [ 0.32938831]\n",
      " [ 0.14519931]\n",
      " [-0.73107447]]\n",
      "Current iteration=4, loss=39519.1456317745\n",
      "t [[ 0.68398395]\n",
      " [-0.80814922]\n",
      " [-0.75602855]\n",
      " ...\n",
      " [ 0.33613838]\n",
      " [ 0.14654767]\n",
      " [-0.82072169]]\n",
      "t [[ 0.68398395]\n",
      " [-0.80814922]\n",
      " [-0.75602855]\n",
      " ...\n",
      " [ 0.33613838]\n",
      " [ 0.14654767]\n",
      " [-0.82072169]]\n",
      "t [[ 0.7436358 ]\n",
      " [-0.94762534]\n",
      " [-0.85365809]\n",
      " ...\n",
      " [ 0.33462296]\n",
      " [ 0.14292897]\n",
      " [-0.89882155]]\n",
      "t [[ 0.7436358 ]\n",
      " [-0.94762534]\n",
      " [-0.85365809]\n",
      " ...\n",
      " [ 0.33462296]\n",
      " [ 0.14292897]\n",
      " [-0.89882155]]\n",
      "Current iteration=6, loss=36958.029372447796\n",
      "t [[ 0.79192551]\n",
      " [-1.07423198]\n",
      " [-0.94181879]\n",
      " ...\n",
      " [ 0.32788294]\n",
      " [ 0.13607015]\n",
      " [-0.96846058]]\n",
      "t [[ 0.79192551]\n",
      " [-1.07423198]\n",
      " [-0.94181879]\n",
      " ...\n",
      " [ 0.32788294]\n",
      " [ 0.13607015]\n",
      " [-0.96846058]]\n",
      "t [[ 0.83153287]\n",
      " [-1.18952744]\n",
      " [-1.02205508]\n",
      " ...\n",
      " [ 0.31775746]\n",
      " [ 0.12712026]\n",
      " [-1.03147111]]\n",
      "t [[ 0.83153287]\n",
      " [-1.18952744]\n",
      " [-1.02205508]\n",
      " ...\n",
      " [ 0.31775746]\n",
      " [ 0.12712026]\n",
      " [-1.03147111]]\n",
      "Current iteration=8, loss=35251.73963912837\n",
      "t [[ 0.86441778]\n",
      " [-1.29496918]\n",
      " [-1.09554534]\n",
      " ...\n",
      " [ 0.30542112]\n",
      " [ 0.11686497]\n",
      " [-1.08904107]]\n",
      "t [[ 0.86441778]\n",
      " [-1.29496918]\n",
      " [-1.09554534]\n",
      " ...\n",
      " [ 0.30542112]\n",
      " [ 0.11686497]\n",
      " [-1.08904107]]\n",
      "t [[ 0.89204063]\n",
      " [-1.39182938]\n",
      " [-1.16321526]\n",
      " ...\n",
      " [ 0.2916565 ]\n",
      " [ 0.10585177]\n",
      " [-1.14200118]]\n",
      "loss=34041.76037762391\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.22242989]\n",
      " [-0.12574088]\n",
      " [-0.20942828]\n",
      " ...\n",
      " [ 0.17926757]\n",
      " [ 0.07569091]\n",
      " [-0.30832883]]\n",
      "t [[ 0.22242989]\n",
      " [-0.12574088]\n",
      " [-0.20942828]\n",
      " ...\n",
      " [ 0.17926757]\n",
      " [ 0.07569091]\n",
      " [-0.30832883]]\n",
      "t [[ 0.38517406]\n",
      " [-0.29622897]\n",
      " [-0.37848152]\n",
      " ...\n",
      " [ 0.26797034]\n",
      " [ 0.11745421]\n",
      " [-0.5036561 ]]\n",
      "t [[ 0.38517406]\n",
      " [-0.29622897]\n",
      " [-0.37848152]\n",
      " ...\n",
      " [ 0.26797034]\n",
      " [ 0.11745421]\n",
      " [-0.5036561 ]]\n",
      "Current iteration=2, loss=43813.97064062707\n",
      "t [[ 0.50747187]\n",
      " [-0.46992654]\n",
      " [-0.52077526]\n",
      " ...\n",
      " [ 0.31142821]\n",
      " [ 0.13929079]\n",
      " [-0.64353074]]\n",
      "t [[ 0.50747187]\n",
      " [-0.46992654]\n",
      " [-0.52077526]\n",
      " ...\n",
      " [ 0.31142821]\n",
      " [ 0.13929079]\n",
      " [-0.64353074]]\n",
      "t [[ 0.60165931]\n",
      " [-0.63277073]\n",
      " [-0.64395225]\n",
      " ...\n",
      " [ 0.33055875]\n",
      " [ 0.14892297]\n",
      " [-0.75358219]]\n",
      "t [[ 0.60165931]\n",
      " [-0.63277073]\n",
      " [-0.64395225]\n",
      " ...\n",
      " [ 0.33055875]\n",
      " [ 0.14892297]\n",
      " [-0.75358219]]\n",
      "Current iteration=4, loss=39648.38580141174\n",
      "t [[ 0.67565291]\n",
      " [-0.78134869]\n",
      " [-0.7526112 ]\n",
      " ...\n",
      " [ 0.33573382]\n",
      " [ 0.15079915]\n",
      " [-0.84567907]]\n",
      "t [[ 0.67565291]\n",
      " [-0.78134869]\n",
      " [-0.7526112 ]\n",
      " ...\n",
      " [ 0.33573382]\n",
      " [ 0.15079915]\n",
      " [-0.84567907]]\n",
      "t [[ 0.7347547 ]\n",
      " [-0.91592511]\n",
      " [-0.84975141]\n",
      " ...\n",
      " [ 0.3324597 ]\n",
      " [ 0.14763617]\n",
      " [-0.92573685]]\n",
      "t [[ 0.7347547 ]\n",
      " [-0.91592511]\n",
      " [-0.84975141]\n",
      " ...\n",
      " [ 0.3324597 ]\n",
      " [ 0.14763617]\n",
      " [-0.92573685]]\n",
      "Current iteration=6, loss=37124.04426521714\n",
      "t [[ 0.78264904]\n",
      " [-1.03786991]\n",
      " [-0.93746423]\n",
      " ...\n",
      " [ 0.32386305]\n",
      " [ 0.14117631]\n",
      " [-0.99697309]]\n",
      "t [[ 0.78264904]\n",
      " [-1.03786991]\n",
      " [-0.93746423]\n",
      " ...\n",
      " [ 0.32386305]\n",
      " [ 0.14117631]\n",
      " [-0.99697309]]\n",
      "t [[ 0.8219708 ]\n",
      " [-1.14874513]\n",
      " [-1.01728484]\n",
      " ...\n",
      " [ 0.31183572]\n",
      " [ 0.13257955]\n",
      " [-1.06130639]]\n",
      "t [[ 0.8219708 ]\n",
      " [-1.14874513]\n",
      " [-1.01728484]\n",
      " ...\n",
      " [ 0.31183572]\n",
      " [ 0.13257955]\n",
      " [-1.06130639]]\n",
      "Current iteration=8, loss=35443.59304825749\n",
      "t [[ 0.85464833]\n",
      " [-1.2500025 ]\n",
      " [-1.09038467]\n",
      " ...\n",
      " [ 0.29758541]\n",
      " [ 0.12263973]\n",
      " [-1.11998407]]\n",
      "t [[ 0.85464833]\n",
      " [-1.2500025 ]\n",
      " [-1.09038467]\n",
      " ...\n",
      " [ 0.29758541]\n",
      " [ 0.12263973]\n",
      " [-1.11998407]]\n",
      "t [[ 0.88211966]\n",
      " [-1.34290358]\n",
      " [-1.15768416]\n",
      " ...\n",
      " [ 0.28191582]\n",
      " [ 0.1119106 ]\n",
      " [-1.17387935]]\n",
      "loss=34252.577599269214\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.22207383]\n",
      " [-0.13210334]\n",
      " [-0.20877045]\n",
      " ...\n",
      " [ 0.31985616]\n",
      " [-0.20877045]\n",
      " [ 0.2995906 ]]\n",
      "t [[ 0.22207383]\n",
      " [-0.13210334]\n",
      " [-0.20877045]\n",
      " ...\n",
      " [ 0.31985616]\n",
      " [-0.20877045]\n",
      " [ 0.2995906 ]]\n",
      "t [[ 0.38452658]\n",
      " [-0.30654951]\n",
      " [-0.37751084]\n",
      " ...\n",
      " [ 0.54319494]\n",
      " [-0.37751084]\n",
      " [ 0.49318121]]\n",
      "t [[ 0.38452658]\n",
      " [-0.30654951]\n",
      " [-0.37751084]\n",
      " ...\n",
      " [ 0.54319494]\n",
      " [-0.37751084]\n",
      " [ 0.49318121]]\n",
      "Current iteration=2, loss=43815.573827721564\n",
      "t [[ 0.50654314]\n",
      " [-0.48326957]\n",
      " [-0.51960911]\n",
      " ...\n",
      " [ 0.70584715]\n",
      " [-0.51960911]\n",
      " [ 0.6252795 ]]\n",
      "t [[ 0.50654314]\n",
      " [-0.48326957]\n",
      " [-0.51960911]\n",
      " ...\n",
      " [ 0.70584715]\n",
      " [-0.51960911]\n",
      " [ 0.6252795 ]]\n",
      "t [[ 0.60032761]\n",
      " [-0.64882131]\n",
      " [-0.64263636]\n",
      " ...\n",
      " [ 0.82831612]\n",
      " [-0.64263636]\n",
      " [ 0.71941451]]\n",
      "t [[ 0.60032761]\n",
      " [-0.64882131]\n",
      " [-0.64263636]\n",
      " ...\n",
      " [ 0.82831612]\n",
      " [-0.64263636]\n",
      " [ 0.71941451]]\n",
      "Current iteration=4, loss=39637.82758755021\n",
      "t [[ 0.67380773]\n",
      " [-0.79996937]\n",
      " [-0.7511637 ]\n",
      " ...\n",
      " [ 0.92288826]\n",
      " [-0.7511637 ]\n",
      " [ 0.78867195]]\n",
      "t [[ 0.67380773]\n",
      " [-0.79996937]\n",
      " [-0.7511637 ]\n",
      " ...\n",
      " [ 0.92288826]\n",
      " [-0.7511637 ]\n",
      " [ 0.78867195]]\n",
      "t [[ 0.73232359]\n",
      " [-0.93701559]\n",
      " [-0.84818067]\n",
      " ...\n",
      " [ 0.99737959]\n",
      " [-0.84818067]\n",
      " [ 0.84086499]]\n",
      "t [[ 0.73232359]\n",
      " [-0.93701559]\n",
      " [-0.84818067]\n",
      " ...\n",
      " [ 0.99737959]\n",
      " [-0.84818067]\n",
      " [ 0.84086499]]\n",
      "Current iteration=6, loss=37104.47153246126\n",
      "t [[ 0.7795901 ]\n",
      " [-1.06133066]\n",
      " [-0.93577587]\n",
      " ...\n",
      " [ 1.05701717]\n",
      " [-0.93577587]\n",
      " [ 0.88095539]]\n",
      "t [[ 0.7795901 ]\n",
      " [-1.06133066]\n",
      " [-0.93577587]\n",
      " ...\n",
      " [ 1.05701717]\n",
      " [-0.93577587]\n",
      " [ 0.88095539]]\n",
      "t [[ 0.81826284]\n",
      " [-1.17447129]\n",
      " [-1.01548405]\n",
      " ...\n",
      " [ 1.10543752]\n",
      " [-1.01548405]\n",
      " [ 0.91225294]]\n",
      "t [[ 0.81826284]\n",
      " [-1.17447129]\n",
      " [-1.01548405]\n",
      " ...\n",
      " [ 1.10543752]\n",
      " [-1.01548405]\n",
      " [ 0.91225294]]\n",
      "Current iteration=8, loss=35417.578424002575\n",
      "t [[ 0.85028355]\n",
      " [-1.27788586]\n",
      " [-1.08847678]\n",
      " ...\n",
      " [ 1.14524976]\n",
      " [-1.08847678]\n",
      " [ 0.93705083]]\n",
      "t [[ 0.85028355]\n",
      " [-1.27788586]\n",
      " [-1.08847678]\n",
      " ...\n",
      " [ 1.14524976]\n",
      " [-1.08847678]\n",
      " [ 0.93705083]]\n",
      "t [[ 0.8770988 ]\n",
      " [-1.37283585]\n",
      " [-1.15567459]\n",
      " ...\n",
      " [ 1.17837133]\n",
      " [-1.15567459]\n",
      " [ 0.95698443]]\n",
      "loss=34221.96419800151\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07647939]\n",
      " [-0.3662214 ]\n",
      " [-0.21475644]\n",
      " ...\n",
      " [ 0.18364526]\n",
      " [ 0.07647939]\n",
      " [-0.31130005]]\n",
      "t [[ 0.07647939]\n",
      " [-0.3662214 ]\n",
      " [-0.21475644]\n",
      " ...\n",
      " [ 0.18364526]\n",
      " [ 0.07647939]\n",
      " [-0.31130005]]\n",
      "t [[ 0.11786523]\n",
      " [-0.67669585]\n",
      " [-0.38755145]\n",
      " ...\n",
      " [ 0.27399535]\n",
      " [ 0.11786523]\n",
      " [-0.50731067]]\n",
      "t [[ 0.11786523]\n",
      " [-0.67669585]\n",
      " [-0.38755145]\n",
      " ...\n",
      " [ 0.27399535]\n",
      " [ 0.11786523]\n",
      " [-0.50731067]]\n",
      "Current iteration=2, loss=43601.64313506841\n",
      "t [[ 0.13893748]\n",
      " [-0.93913986]\n",
      " [-0.53273689]\n",
      " ...\n",
      " [ 0.31853956]\n",
      " [ 0.13893748]\n",
      " [-0.64786815]]\n",
      "t [[ 0.13893748]\n",
      " [-0.93913986]\n",
      " [-0.53273689]\n",
      " ...\n",
      " [ 0.31853956]\n",
      " [ 0.13893748]\n",
      " [-0.64786815]]\n",
      "t [[ 0.1477082 ]\n",
      " [-1.16248878]\n",
      " [-0.65825719]\n",
      " ...\n",
      " [ 0.33874907]\n",
      " [ 0.1477082 ]\n",
      " [-0.75890115]]\n",
      "t [[ 0.1477082 ]\n",
      " [-1.16248878]\n",
      " [-0.65825719]\n",
      " ...\n",
      " [ 0.33874907]\n",
      " [ 0.1477082 ]\n",
      " [-0.75890115]]\n",
      "Current iteration=4, loss=39362.50967017753\n",
      "t [[ 0.1487381 ]\n",
      " [-1.35447266]\n",
      " [-0.76886128]\n",
      " ...\n",
      " [ 0.34509738]\n",
      " [ 0.1487381 ]\n",
      " [-0.85216141]]\n",
      "t [[ 0.1487381 ]\n",
      " [-1.35447266]\n",
      " [-0.76886128]\n",
      " ...\n",
      " [ 0.34509738]\n",
      " [ 0.1487381 ]\n",
      " [-0.85216141]]\n",
      "t [[ 0.144789  ]\n",
      " [-1.52123921]\n",
      " [-0.86763978]\n",
      " ...\n",
      " [ 0.3430741 ]\n",
      " [ 0.144789  ]\n",
      " [-0.93342727]]\n",
      "t [[ 0.144789  ]\n",
      " [-1.52123921]\n",
      " [-0.86763978]\n",
      " ...\n",
      " [ 0.3430741 ]\n",
      " [ 0.144789  ]\n",
      " [-0.93342727]]\n",
      "Current iteration=6, loss=36809.135673481476\n",
      "t [[ 0.13762034]\n",
      " [-1.66754676]\n",
      " [-0.95674748]\n",
      " ...\n",
      " [ 0.33577605]\n",
      " [ 0.13762034]\n",
      " [-1.00583044]]\n",
      "t [[ 0.13762034]\n",
      " [-1.66754676]\n",
      " [-0.95674748]\n",
      " ...\n",
      " [ 0.33577605]\n",
      " [ 0.13762034]\n",
      " [-1.00583044]]\n",
      "t [[ 0.12839667]\n",
      " [-1.79706481]\n",
      " [-1.03776684]\n",
      " ...\n",
      " [ 0.32507293]\n",
      " [ 0.12839667]\n",
      " [-1.07124715]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.12839667]\n",
      " [-1.79706481]\n",
      " [-1.03776684]\n",
      " ...\n",
      " [ 0.32507293]\n",
      " [ 0.12839667]\n",
      " [-1.07124715]]\n",
      "Current iteration=8, loss=35116.69410892404\n",
      "t [[ 0.11791027]\n",
      " [-1.91264156]\n",
      " [-1.11190553]\n",
      " ...\n",
      " [ 0.312158  ]\n",
      " [ 0.11791027]\n",
      " [-1.1309087 ]]\n",
      "t [[ 0.11791027]\n",
      " [-1.91264156]\n",
      " [-1.11190553]\n",
      " ...\n",
      " [ 0.312158  ]\n",
      " [ 0.11791027]\n",
      " [-1.1309087 ]]\n",
      "t [[ 0.10671032]\n",
      " [-2.01651089]\n",
      " [-1.18011232]\n",
      " ...\n",
      " [ 0.29782559]\n",
      " [ 0.10671032]\n",
      " [-1.18568639]]\n",
      "loss=33921.217728031195\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23095073]\n",
      " [-0.13463501]\n",
      " [-0.21547096]\n",
      " ...\n",
      " [ 0.18095733]\n",
      " [ 0.07617913]\n",
      " [-0.30659544]]\n",
      "t [[ 0.23095073]\n",
      " [-0.13463501]\n",
      " [-0.21547096]\n",
      " ...\n",
      " [ 0.18095733]\n",
      " [ 0.07617913]\n",
      " [-0.30659544]]\n",
      "t [[ 0.39839205]\n",
      " [-0.31603732]\n",
      " [-0.38853688]\n",
      " ...\n",
      " [ 0.26928074]\n",
      " [ 0.117151  ]\n",
      " [-0.49797254]]\n",
      "t [[ 0.39839205]\n",
      " [-0.31603732]\n",
      " [-0.38853688]\n",
      " ...\n",
      " [ 0.26928074]\n",
      " [ 0.117151  ]\n",
      " [-0.49797254]]\n",
      "Current iteration=2, loss=43588.76535233288\n",
      "t [[ 0.52327004]\n",
      " [-0.49974339]\n",
      " [-0.53376948]\n",
      " ...\n",
      " [ 0.31232922]\n",
      " [ 0.13778358]\n",
      " [-0.63437713]]\n",
      "t [[ 0.52327004]\n",
      " [-0.49974339]\n",
      " [-0.53376948]\n",
      " ...\n",
      " [ 0.31232922]\n",
      " [ 0.13778358]\n",
      " [-0.63437713]]\n",
      "t [[ 0.61880472]\n",
      " [-0.67127656]\n",
      " [-0.65922521]\n",
      " ...\n",
      " [ 0.33145364]\n",
      " [ 0.14612481]\n",
      " [-0.74181189]]\n",
      "t [[ 0.61880472]\n",
      " [-0.67127656]\n",
      " [-0.65922521]\n",
      " ...\n",
      " [ 0.33145364]\n",
      " [ 0.14612481]\n",
      " [-0.74181189]]\n",
      "Current iteration=4, loss=39349.600852544136\n",
      "t [[ 0.69340675]\n",
      " [-0.82739877]\n",
      " [-0.76970925]\n",
      " ...\n",
      " [ 0.33702234]\n",
      " [ 0.14674971]\n",
      " [-0.83198366]]\n",
      "t [[ 0.69340675]\n",
      " [-0.82739877]\n",
      " [-0.76970925]\n",
      " ...\n",
      " [ 0.33702234]\n",
      " [ 0.14674971]\n",
      " [-0.83198366]]\n",
      "t [[ 0.75267446]\n",
      " [-0.96860931]\n",
      " [-0.86834113]\n",
      " ...\n",
      " [ 0.33444403]\n",
      " [ 0.1424253 ]\n",
      " [-0.91059641]]\n",
      "t [[ 0.75267446]\n",
      " [-0.96860931]\n",
      " [-0.86834113]\n",
      " ...\n",
      " [ 0.33444403]\n",
      " [ 0.1424253 ]\n",
      " [-0.91059641]]\n",
      "Current iteration=6, loss=36796.73723849101\n",
      "t [[ 0.80047432]\n",
      " [-1.09646788]\n",
      " [-0.95729089]\n",
      " ...\n",
      " [ 0.32675713]\n",
      " [ 0.13491212]\n",
      " [-0.98070915]]\n",
      "t [[ 0.80047432]\n",
      " [-1.09646788]\n",
      " [-0.95729089]\n",
      " ...\n",
      " [ 0.32675713]\n",
      " [ 0.13491212]\n",
      " [-0.98070915]]\n",
      "t [[ 0.83955507]\n",
      " [-1.21267591]\n",
      " [-1.03814919]\n",
      " ...\n",
      " [ 0.31578974]\n",
      " [ 0.12537398]\n",
      " [-1.04413585]]\n",
      "t [[ 0.83955507]\n",
      " [-1.21267591]\n",
      " [-1.03814919]\n",
      " ...\n",
      " [ 0.31578974]\n",
      " [ 0.12537398]\n",
      " [-1.04413585]]\n",
      "Current iteration=8, loss=35104.43010373844\n",
      "t [[ 0.87191707]\n",
      " [-1.31878649]\n",
      " [-1.11212825]\n",
      " ...\n",
      " [ 0.30270554]\n",
      " [ 0.11460164]\n",
      " [-1.10205701]]\n",
      "t [[ 0.87191707]\n",
      " [-1.31878649]\n",
      " [-1.11212825]\n",
      " ...\n",
      " [ 0.30270554]\n",
      " [ 0.11460164]\n",
      " [-1.10205701]]\n",
      "t [[ 0.89904397]\n",
      " [-1.41613633]\n",
      " [-1.1801793 ]\n",
      " ...\n",
      " [ 0.28827756]\n",
      " [ 0.10314243]\n",
      " [-1.15530306]]\n",
      "loss=33908.72906688586\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.22799063]\n",
      " [-0.1288844 ]\n",
      " [-0.21466399]\n",
      " ...\n",
      " [ 0.18374926]\n",
      " [ 0.07758318]\n",
      " [-0.31603705]]\n",
      "t [[ 0.22799063]\n",
      " [-0.1288844 ]\n",
      " [-0.21466399]\n",
      " ...\n",
      " [ 0.18374926]\n",
      " [ 0.07758318]\n",
      " [-0.31603705]]\n",
      "t [[ 0.39331097]\n",
      " [-0.30474889]\n",
      " [-0.386931  ]\n",
      " ...\n",
      " [ 0.27240414]\n",
      " [ 0.11954227]\n",
      " [-0.51342415]]\n",
      "t [[ 0.39331097]\n",
      " [-0.30474889]\n",
      " [-0.386931  ]\n",
      " ...\n",
      " [ 0.27240414]\n",
      " [ 0.11954227]\n",
      " [-0.51342415]]\n",
      "Current iteration=2, loss=43664.89987615877\n",
      "t [[ 0.51667285]\n",
      " [-0.48294252]\n",
      " [-0.53145479]\n",
      " ...\n",
      " [ 0.31470159]\n",
      " [ 0.14094147]\n",
      " [-0.65405451]]\n",
      "t [[ 0.51667285]\n",
      " [-0.48294252]\n",
      " [-0.53145479]\n",
      " ...\n",
      " [ 0.31470159]\n",
      " [ 0.14094147]\n",
      " [-0.65405451]]\n",
      "t [[ 0.61113382]\n",
      " [-0.64906384]\n",
      " [-0.65628977]\n",
      " ...\n",
      " [ 0.33250242]\n",
      " [ 0.14991136]\n",
      " [-0.76464045]]\n",
      "t [[ 0.61113382]\n",
      " [-0.64906384]\n",
      " [-0.65628977]\n",
      " ...\n",
      " [ 0.33250242]\n",
      " [ 0.14991136]\n",
      " [-0.76464045]]\n",
      "Current iteration=4, loss=39481.16726358904\n",
      "t [[ 0.68497745]\n",
      " [-0.79995755]\n",
      " [-0.76622389]\n",
      " ...\n",
      " [ 0.33642369]\n",
      " [ 0.1510676 ]\n",
      " [-0.85725389]]\n",
      "t [[ 0.68497745]\n",
      " [-0.79995755]\n",
      " [-0.76622389]\n",
      " ...\n",
      " [ 0.33642369]\n",
      " [ 0.1510676 ]\n",
      " [-0.85725389]]\n",
      "t [[ 0.74370759]\n",
      " [-0.93617186]\n",
      " [-0.86436055]\n",
      " ...\n",
      " [ 0.33201932]\n",
      " [ 0.14720109]\n",
      " [-0.93781249]]\n",
      "t [[ 0.74370759]\n",
      " [-0.93617186]\n",
      " [-0.86436055]\n",
      " ...\n",
      " [ 0.33201932]\n",
      " [ 0.14720109]\n",
      " [-0.93781249]]\n",
      "Current iteration=6, loss=36965.138245626884\n",
      "t [[ 0.79112449]\n",
      " [-1.05928635]\n",
      " [-0.95285712]\n",
      " ...\n",
      " [ 0.3224142 ]\n",
      " [ 0.14008815]\n",
      " [-1.00950836]]\n",
      "t [[ 0.79112449]\n",
      " [-1.05928635]\n",
      " [-0.95285712]\n",
      " ...\n",
      " [ 0.3224142 ]\n",
      " [ 0.14008815]\n",
      " [-1.00950836]]\n",
      "t [[ 0.82993103]\n",
      " [-1.17100458]\n",
      " [-1.03329469]\n",
      " ...\n",
      " [ 0.309489  ]\n",
      " [ 0.13090379]\n",
      " [-1.0742427 ]]\n",
      "t [[ 0.82993103]\n",
      " [-1.17100458]\n",
      " [-1.03329469]\n",
      " ...\n",
      " [ 0.309489  ]\n",
      " [ 0.13090379]\n",
      " [-1.0742427 ]]\n",
      "Current iteration=8, loss=35298.565162323306\n",
      "t [[ 0.86209562]\n",
      " [-1.27287218]\n",
      " [-1.10687831]\n",
      " ...\n",
      " [ 0.29444005]\n",
      " [ 0.12044714]\n",
      " [-1.13325588]]\n",
      "t [[ 0.86209562]\n",
      " [-1.27287218]\n",
      " [-1.10687831]\n",
      " ...\n",
      " [ 0.29444005]\n",
      " [ 0.12044714]\n",
      " [-1.13325588]]\n",
      "t [[ 0.88907947]\n",
      " [-1.36621362]\n",
      " [-1.17455391]\n",
      " ...\n",
      " [ 0.27806105]\n",
      " [ 0.10927195]\n",
      " [-1.18742117]]\n",
      "loss=34121.67166851426\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.22762568]\n",
      " [-0.13540593]\n",
      " [-0.21398972]\n",
      " ...\n",
      " [ 0.32785256]\n",
      " [-0.21398972]\n",
      " [ 0.30708037]]\n",
      "t [[ 0.22762568]\n",
      " [-0.13540593]\n",
      " [-0.21398972]\n",
      " ...\n",
      " [ 0.32785256]\n",
      " [-0.21398972]\n",
      " [ 0.30708037]]\n",
      "t [[ 0.39264671]\n",
      " [-0.31526964]\n",
      " [-0.38594434]\n",
      " ...\n",
      " [ 0.55435821]\n",
      " [-0.38594434]\n",
      " [ 0.50285886]]\n",
      "t [[ 0.39264671]\n",
      " [-0.31526964]\n",
      " [-0.38594434]\n",
      " ...\n",
      " [ 0.55435821]\n",
      " [-0.38594434]\n",
      " [ 0.50285886]]\n",
      "Current iteration=2, loss=43666.21689753272\n",
      "t [[ 0.51571441]\n",
      " [-0.49652128]\n",
      " [-0.53027351]\n",
      " ...\n",
      " [ 0.71807642]\n",
      " [-0.53027351]\n",
      " [ 0.6352224 ]]\n",
      "t [[ 0.51571441]\n",
      " [-0.49652128]\n",
      " [-0.53027351]\n",
      " ...\n",
      " [ 0.71807642]\n",
      " [-0.53027351]\n",
      " [ 0.6352224 ]]\n",
      "t [[ 0.60975186]\n",
      " [-0.66539696]\n",
      " [-0.65495824]\n",
      " ...\n",
      " [ 0.84062933]\n",
      " [-0.65495824]\n",
      " [ 0.72889855]]\n",
      "t [[ 0.60975186]\n",
      " [-0.66539696]\n",
      " [-0.65495824]\n",
      " ...\n",
      " [ 0.84062933]\n",
      " [-0.65495824]\n",
      " [ 0.72889855]]\n",
      "Current iteration=4, loss=39470.07063517169\n",
      "t [[ 0.68305942]\n",
      " [-0.81891186]\n",
      " [-0.76475916]\n",
      " ...\n",
      " [ 0.93480459]\n",
      " [-0.76475916]\n",
      " [ 0.79742581]]\n",
      "t [[ 0.68305942]\n",
      " [-0.81891186]\n",
      " [-0.76475916]\n",
      " ...\n",
      " [ 0.93480459]\n",
      " [-0.76475916]\n",
      " [ 0.79742581]]\n",
      "t [[ 0.74118175]\n",
      " [-0.95764512]\n",
      " [-0.86277056]\n",
      " ...\n",
      " [ 1.00866769]\n",
      " [-0.86277056]\n",
      " [ 0.84880835]]\n",
      "t [[ 0.74118175]\n",
      " [-0.95764512]\n",
      " [-0.86277056]\n",
      " ...\n",
      " [ 1.00866769]\n",
      " [-0.86277056]\n",
      " [ 0.84880835]]\n",
      "Current iteration=6, loss=36944.9791778403\n",
      "t [[ 0.78795055]\n",
      " [-1.08317399]\n",
      " [-0.95114748]\n",
      " ...\n",
      " [ 1.06758002]\n",
      " [-0.95114748]\n",
      " [ 0.88809658]]\n",
      "t [[ 0.78795055]\n",
      " [-1.08317399]\n",
      " [-0.95114748]\n",
      " ...\n",
      " [ 1.06758002]\n",
      " [-0.95114748]\n",
      " [ 0.88809658]]\n",
      "t [[ 0.82608961]\n",
      " [-1.19719561]\n",
      " [-1.03147082]\n",
      " ...\n",
      " [ 1.11525383]\n",
      " [-1.03147082]\n",
      " [ 0.9186437 ]]\n",
      "t [[ 0.82608961]\n",
      " [-1.19719561]\n",
      " [-1.03147082]\n",
      " ...\n",
      " [ 1.11525383]\n",
      " [-1.03147082]\n",
      " [ 0.9186437 ]]\n",
      "Current iteration=8, loss=35271.98735885473\n",
      "t [[ 0.85758062]\n",
      " [-1.30125235]\n",
      " [-1.10494584]\n",
      " ...\n",
      " [ 1.15434147]\n",
      " [-1.10494584]\n",
      " [ 0.94276381]]\n",
      "t [[ 0.85758062]\n",
      " [-1.30125235]\n",
      " [-1.10494584]\n",
      " ...\n",
      " [ 1.15434147]\n",
      " [-1.10494584]\n",
      " [ 0.94276381]]\n",
      "t [[ 0.88389315]\n",
      " [-1.39666922]\n",
      " [-1.17251855]\n",
      " ...\n",
      " [ 1.18678456]\n",
      " [-1.17251855]\n",
      " [ 0.96210095]]\n",
      "loss=34090.556029196465\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.07834474]\n",
      " [-0.37515363]\n",
      " [-0.2199944 ]\n",
      " ...\n",
      " [ 0.18812442]\n",
      " [ 0.07834474]\n",
      " [-0.31889273]]\n",
      "t [[ 0.07834474]\n",
      " [-0.37515363]\n",
      " [-0.2199944 ]\n",
      " ...\n",
      " [ 0.18812442]\n",
      " [ 0.07834474]\n",
      " [-0.31889273]]\n",
      "t [[ 0.11988499]\n",
      " [-0.69183006]\n",
      " [-0.39597835]\n",
      " ...\n",
      " [ 0.27840232]\n",
      " [ 0.11988499]\n",
      " [-0.51687604]]\n",
      "t [[ 0.11988499]\n",
      " [-0.69183006]\n",
      " [-0.39597835]\n",
      " ...\n",
      " [ 0.27840232]\n",
      " [ 0.11988499]\n",
      " [-0.51687604]]\n",
      "Current iteration=2, loss=43452.9458508316\n",
      "t [[ 0.1404945 ]\n",
      " [-0.95834654]\n",
      " [-0.54337034]\n",
      " ...\n",
      " [ 0.32181516]\n",
      " [ 0.1404945 ]\n",
      " [-0.65818827]]\n",
      "t [[ 0.1404945 ]\n",
      " [-0.95834654]\n",
      " [-0.54337034]\n",
      " ...\n",
      " [ 0.32181516]\n",
      " [ 0.1404945 ]\n",
      " [-0.65818827]]\n",
      "t [[ 0.14859197]\n",
      " [-1.18431139]\n",
      " [-0.67052704]\n",
      " ...\n",
      " [ 0.34075517]\n",
      " [ 0.14859197]\n",
      " [-0.76978822]]\n",
      "t [[ 0.14859197]\n",
      " [-1.18431139]\n",
      " [-0.67052704]\n",
      " ...\n",
      " [ 0.34075517]\n",
      " [ 0.14859197]\n",
      " [-0.76978822]]\n",
      "Current iteration=4, loss=39196.87138326137\n",
      "t [[ 0.14890082]\n",
      " [-1.37794875]\n",
      " [-0.78238538]\n",
      " ...\n",
      " [ 0.34591879]\n",
      " [ 0.14890082]\n",
      " [-0.86359805]]\n",
      "t [[ 0.14890082]\n",
      " [-1.37794875]\n",
      " [-0.78238538]\n",
      " ...\n",
      " [ 0.34591879]\n",
      " [ 0.14890082]\n",
      " [-0.86359805]]\n",
      "t [[ 0.14425362]\n",
      " [-1.54573306]\n",
      " [-0.88213986]\n",
      " ...\n",
      " [ 0.34283304]\n",
      " [ 0.14425362]\n",
      " [-0.94538743]]\n",
      "t [[ 0.14425362]\n",
      " [-1.54573306]\n",
      " [-0.88213986]\n",
      " ...\n",
      " [ 0.34283304]\n",
      " [ 0.14425362]\n",
      " [-0.94538743]]\n",
      "Current iteration=6, loss=36652.52314800257\n",
      "t [[ 0.13644168]\n",
      " [-1.692633  ]\n",
      " [-0.97201166]\n",
      " ...\n",
      " [ 0.33458992]\n",
      " [ 0.13644168]\n",
      " [-1.01826191]]\n",
      "t [[ 0.13644168]\n",
      " [-1.692633  ]\n",
      " [-0.97201166]\n",
      " ...\n",
      " [ 0.33458992]\n",
      " [ 0.13644168]\n",
      " [-1.01826191]]\n",
      "t [[ 0.12664311]\n",
      " [-1.82245416]\n",
      " [-1.05362949]\n",
      " ...\n",
      " [ 0.32304755]\n",
      " [ 0.12664311]\n",
      " [-1.0840827 ]]\n",
      "t [[ 0.12664311]\n",
      " [-1.82245416]\n",
      " [-1.05362949]\n",
      " ...\n",
      " [ 0.32304755]\n",
      " [ 0.12664311]\n",
      " [-1.0840827 ]]\n",
      "Current iteration=8, loss=34974.29760875091\n",
      "t [[ 0.11565442]\n",
      " [-1.93813421]\n",
      " [-1.12823494]\n",
      " ...\n",
      " [ 0.30938819]\n",
      " [ 0.11565442]\n",
      " [-1.14407685]]\n",
      "t [[ 0.11565442]\n",
      " [-1.93813421]\n",
      " [-1.12823494]\n",
      " ...\n",
      " [ 0.30938819]\n",
      " [ 0.11565442]\n",
      " [-1.14407685]]\n",
      "t [[ 0.10402397]\n",
      " [-2.04196701]\n",
      " [-1.19680239]\n",
      " ...\n",
      " [ 0.29439654]\n",
      " [ 0.10402397]\n",
      " [-1.19911784]]\n",
      "loss=33793.08183367807\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23658367]\n",
      " [-0.13791879]\n",
      " [-0.22072635]\n",
      " ...\n",
      " [ 0.18537093]\n",
      " [ 0.07803715]\n",
      " [-0.31407338]]\n",
      "t [[ 0.23658367]\n",
      " [-0.13791879]\n",
      " [-0.22072635]\n",
      " ...\n",
      " [ 0.18537093]\n",
      " [ 0.07803715]\n",
      " [-0.31407338]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.40656249]\n",
      " [-0.32487945]\n",
      " [-0.39697718]\n",
      " ...\n",
      " [ 0.27358863]\n",
      " [ 0.11915069]\n",
      " [-0.50731231]]\n",
      "t [[ 0.40656249]\n",
      " [-0.32487945]\n",
      " [-0.39697718]\n",
      " ...\n",
      " [ 0.27358863]\n",
      " [ 0.11915069]\n",
      " [-0.50731231]]\n",
      "Current iteration=2, loss=43440.0030398462\n",
      "t [[ 0.5324426 ]\n",
      " [-0.51317377]\n",
      " [-0.54440683]\n",
      " ...\n",
      " [ 0.315495  ]\n",
      " [ 0.13930854]\n",
      " [-0.64439387]]\n",
      "t [[ 0.5324426 ]\n",
      " [-0.51317377]\n",
      " [-0.54440683]\n",
      " ...\n",
      " [ 0.315495  ]\n",
      " [ 0.13930854]\n",
      " [-0.64439387]]\n",
      "t [[ 0.62819007]\n",
      " [-0.68802485]\n",
      " [-0.67148934]\n",
      " ...\n",
      " [ 0.33335309]\n",
      " [ 0.14696667]\n",
      " [-0.75234782]]\n",
      "t [[ 0.62819007]\n",
      " [-0.68802485]\n",
      " [-0.67148934]\n",
      " ...\n",
      " [ 0.33335309]\n",
      " [ 0.14696667]\n",
      " [-0.75234782]]\n",
      "Current iteration=4, loss=39183.99203940788\n",
      "t [[ 0.70259106]\n",
      " [-0.8464837 ]\n",
      " [-0.78321929]\n",
      " ...\n",
      " [ 0.33774739]\n",
      " [ 0.14686292]\n",
      " [-0.84304277]]\n",
      "t [[ 0.70259106]\n",
      " [-0.8464837 ]\n",
      " [-0.78321929]\n",
      " ...\n",
      " [ 0.33774739]\n",
      " [ 0.14686292]\n",
      " [-0.84304277]]\n",
      "t [[ 0.76144783]\n",
      " [-0.9893469 ]\n",
      " [-0.88282028]\n",
      " ...\n",
      " [ 0.33412012]\n",
      " [ 0.14183479]\n",
      " [-0.92216671]]\n",
      "t [[ 0.76144783]\n",
      " [-0.9893469 ]\n",
      " [-0.88282028]\n",
      " ...\n",
      " [ 0.33412012]\n",
      " [ 0.14183479]\n",
      " [-0.92216671]]\n",
      "Current iteration=6, loss=36640.155706440484\n",
      "t [[ 0.80874225]\n",
      " [-1.1183883 ]\n",
      " [-0.97252854]\n",
      " ...\n",
      " [ 0.32550278]\n",
      " [ 0.13367439]\n",
      " [-0.99274742]]\n",
      "t [[ 0.80874225]\n",
      " [-1.1183883 ]\n",
      " [-0.97252854]\n",
      " ...\n",
      " [ 0.32550278]\n",
      " [ 0.13367439]\n",
      " [-0.99274742]]\n",
      "t [[ 0.84728966]\n",
      " [-1.23545167]\n",
      " [-1.05398077]\n",
      " ...\n",
      " [ 0.31371089]\n",
      " [ 0.12355881]\n",
      " [-1.05658058]]\n",
      "t [[ 0.84728966]\n",
      " [-1.23545167]\n",
      " [-1.05398077]\n",
      " ...\n",
      " [ 0.31371089]\n",
      " [ 0.12355881]\n",
      " [-1.05658058]]\n",
      "Current iteration=8, loss=34962.03279216636\n",
      "t [[ 0.87912882]\n",
      " [-1.34218377]\n",
      " [-1.1284229 ]\n",
      " ...\n",
      " [ 0.29989667]\n",
      " [ 0.1122828 ]\n",
      " [-1.11484039]]\n",
      "t [[ 0.87912882]\n",
      " [-1.34218377]\n",
      " [-1.1284229 ]\n",
      " ...\n",
      " [ 0.29989667]\n",
      " [ 0.1122828 ]\n",
      " [-1.11484039]]\n",
      "t [[ 0.90576499]\n",
      " [-1.43998391]\n",
      " [-1.19683161]\n",
      " ...\n",
      " [ 0.28482336]\n",
      " [ 0.10039268]\n",
      " [-1.16835814]]\n",
      "loss=33780.55282864028\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.23355138]\n",
      " [-0.13202792]\n",
      " [-0.2198997 ]\n",
      " ...\n",
      " [ 0.18823095]\n",
      " [ 0.07947545]\n",
      " [-0.32374527]]\n",
      "t [[ 0.23355138]\n",
      " [-0.13202792]\n",
      " [-0.2198997 ]\n",
      " ...\n",
      " [ 0.18823095]\n",
      " [ 0.07947545]\n",
      " [-0.32374527]]\n",
      "t [[ 0.40137778]\n",
      " [-0.31332079]\n",
      " [-0.39533271]\n",
      " ...\n",
      " [ 0.27673153]\n",
      " [ 0.1215905 ]\n",
      " [-0.52305979]]\n",
      "t [[ 0.40137778]\n",
      " [-0.31332079]\n",
      " [-0.39533271]\n",
      " ...\n",
      " [ 0.27673153]\n",
      " [ 0.1215905 ]\n",
      " [-0.52305979]]\n",
      "Current iteration=2, loss=43517.87227786552\n",
      "t [[ 0.52573405]\n",
      " [-0.49596955]\n",
      " [-0.5420407 ]\n",
      " ...\n",
      " [ 0.31781658]\n",
      " [ 0.14252317]\n",
      " [-0.66438546]]\n",
      "t [[ 0.52573405]\n",
      " [-0.49596955]\n",
      " [-0.5420407 ]\n",
      " ...\n",
      " [ 0.31781658]\n",
      " [ 0.14252317]\n",
      " [-0.66438546]]\n",
      "t [[ 0.62041324]\n",
      " [-0.66528377]\n",
      " [-0.66849348]\n",
      " ...\n",
      " [ 0.33427609]\n",
      " [ 0.15081516]\n",
      " [-0.77548998]]\n",
      "t [[ 0.62041324]\n",
      " [-0.66528377]\n",
      " [-0.66849348]\n",
      " ...\n",
      " [ 0.33427609]\n",
      " [ 0.15081516]\n",
      " [-0.77548998]]\n",
      "Current iteration=4, loss=39317.843181468685\n",
      "t [[ 0.69406689]\n",
      " [-0.81840361]\n",
      " [-0.77966665]\n",
      " ...\n",
      " [ 0.33695072]\n",
      " [ 0.15124605]\n",
      " [-0.86861775]]\n",
      "t [[ 0.69406689]\n",
      " [-0.81840361]\n",
      " [-0.77966665]\n",
      " ...\n",
      " [ 0.33695072]\n",
      " [ 0.15124605]\n",
      " [-0.86861775]]\n",
      "t [[ 0.75239878]\n",
      " [-0.95617573]\n",
      " [-0.87876667]\n",
      " ...\n",
      " [ 0.33143059]\n",
      " [ 0.14667785]\n",
      " [-0.94967496]]\n",
      "t [[ 0.75239878]\n",
      " [-0.95617573]\n",
      " [-0.87876667]\n",
      " ...\n",
      " [ 0.33143059]\n",
      " [ 0.14667785]\n",
      " [-0.94967496]]\n",
      "Current iteration=6, loss=36810.88425783749\n",
      "t [[ 0.79932262]\n",
      " [-1.08039277]\n",
      " [-0.9680165 ]\n",
      " ...\n",
      " [ 0.32083422]\n",
      " [ 0.13891882]\n",
      " [-1.02182437]]\n",
      "t [[ 0.79932262]\n",
      " [-1.08039277]\n",
      " [-0.9680165 ]\n",
      " ...\n",
      " [ 0.32083422]\n",
      " [ 0.13891882]\n",
      " [-1.02182437]]\n",
      "t [[ 0.83760704]\n",
      " [-1.19289885]\n",
      " [-1.04904295]\n",
      " ...\n",
      " [ 0.30702945]\n",
      " [ 0.12915757]\n",
      " [-1.08694981]]\n",
      "t [[ 0.83760704]\n",
      " [-1.19289885]\n",
      " [-1.04904295]\n",
      " ...\n",
      " [ 0.30702945]\n",
      " [ 0.12915757]\n",
      " [-1.08694981]]\n",
      "Current iteration=8, loss=35158.38277959513\n",
      "t [[ 0.86925852]\n",
      " [-1.2953314 ]\n",
      " [-1.12308465]\n",
      " ...\n",
      " [ 0.29120067]\n",
      " [ 0.11819738]\n",
      " [-1.1462858 ]]\n",
      "t [[ 0.86925852]\n",
      " [-1.2953314 ]\n",
      " [-1.12308465]\n",
      " ...\n",
      " [ 0.29120067]\n",
      " [ 0.11819738]\n",
      " [-1.1462858 ]]\n",
      "t [[ 0.89575979]\n",
      " [-1.38907587]\n",
      " [-1.19111288]\n",
      " ...\n",
      " [ 0.27413129]\n",
      " [ 0.10659115]\n",
      " [-1.20070676]]\n",
      "loss=33995.550839997995\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23317752]\n",
      " [-0.13870851]\n",
      " [-0.21920898]\n",
      " ...\n",
      " [ 0.33584896]\n",
      " [-0.21920898]\n",
      " [ 0.31457013]]\n",
      "t [[ 0.23317752]\n",
      " [-0.13870851]\n",
      " [-0.21920898]\n",
      " ...\n",
      " [ 0.33584896]\n",
      " [-0.21920898]\n",
      " [ 0.31457013]]\n",
      "t [[ 0.40069658]\n",
      " [-0.3240392 ]\n",
      " [-0.39433044]\n",
      " ...\n",
      " [ 0.56540776]\n",
      " [-0.39433044]\n",
      " [ 0.51241188]]\n",
      "t [[ 0.40069658]\n",
      " [-0.3240392 ]\n",
      " [-0.39433044]\n",
      " ...\n",
      " [ 0.56540776]\n",
      " [-0.39433044]\n",
      " [ 0.51241188]]\n",
      "Current iteration=2, loss=43518.89755664398\n",
      "t [[ 0.5247452 ]\n",
      " [-0.50978154]\n",
      " [-0.54084468]\n",
      " ...\n",
      " [ 0.73009481]\n",
      " [-0.54084468]\n",
      " [ 0.64495197]]\n",
      "t [[ 0.5247452 ]\n",
      " [-0.50978154]\n",
      " [-0.54084468]\n",
      " ...\n",
      " [ 0.73009481]\n",
      " [-0.54084468]\n",
      " [ 0.64495197]]\n",
      "t [[ 0.61897996]\n",
      " [-0.68189769]\n",
      " [-0.6671466 ]\n",
      " ...\n",
      " [ 0.85266322]\n",
      " [-0.6671466 ]\n",
      " [ 0.73811889]]\n",
      "t [[ 0.61897996]\n",
      " [-0.68189769]\n",
      " [-0.6671466 ]\n",
      " ...\n",
      " [ 0.85266322]\n",
      " [-0.6671466 ]\n",
      " [ 0.73811889]]\n",
      "Current iteration=4, loss=39306.214828820885\n",
      "t [[ 0.69207494]\n",
      " [-0.83769011]\n",
      " [-0.77818489]\n",
      " ...\n",
      " [ 0.94639643]\n",
      " [-0.77818489]\n",
      " [ 0.80589056]]\n",
      "t [[ 0.69207494]\n",
      " [-0.83769011]\n",
      " [-0.77818489]\n",
      " ...\n",
      " [ 0.94639643]\n",
      " [-0.77818489]\n",
      " [ 0.80589056]]\n",
      "t [[ 0.74977733]\n",
      " [-0.97803007]\n",
      " [-0.87715757]\n",
      " ...\n",
      " [ 1.01960368]\n",
      " [-0.87715757]\n",
      " [ 0.85645292]]\n",
      "t [[ 0.74977733]\n",
      " [-0.97803007]\n",
      " [-0.87715757]\n",
      " ...\n",
      " [ 1.01960368]\n",
      " [-0.87715757]\n",
      " [ 0.85645292]]\n",
      "Current iteration=6, loss=36790.151617002106\n",
      "t [[ 0.79603312]\n",
      " [-1.10470482]\n",
      " [-0.96628574]\n",
      " ...\n",
      " [ 1.07777642]\n",
      " [-0.96628574]\n",
      " [ 0.89493975]]\n",
      "t [[ 0.79603312]\n",
      " [-1.10470482]\n",
      " [-0.96628574]\n",
      " ...\n",
      " [ 1.07777642]\n",
      " [-0.96628574]\n",
      " [ 0.89493975]]\n",
      "t [[ 0.83363192]\n",
      " [-1.21955122]\n",
      " [-1.04719622]\n",
      " ...\n",
      " [ 1.12469947]\n",
      " [-1.04719622]\n",
      " [ 0.9247445 ]]\n",
      "t [[ 0.83363192]\n",
      " [-1.21955122]\n",
      " [-1.04719622]\n",
      " ...\n",
      " [ 1.12469947]\n",
      " [-1.04719622]\n",
      " [ 0.9247445 ]]\n",
      "Current iteration=8, loss=35131.258922297864\n",
      "t [[ 0.86459342]\n",
      " [-1.32420369]\n",
      " [-1.12112787]\n",
      " ...\n",
      " [ 1.163066  ]\n",
      " [-1.12112787]\n",
      " [ 0.94820009]]\n",
      "t [[ 0.86459342]\n",
      " [-1.32420369]\n",
      " [-1.12112787]\n",
      " ...\n",
      " [ 1.163066  ]\n",
      " [-1.12112787]\n",
      " [ 0.94820009]]\n",
      "t [[ 0.89040848]\n",
      " [-1.42004889]\n",
      " [-1.18905207]\n",
      " ...\n",
      " [ 1.1948398 ]\n",
      " [-1.18905207]\n",
      " [ 0.9669576 ]]\n",
      "loss=33963.9532065072\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08021009]\n",
      " [-0.38408586]\n",
      " [-0.22523236]\n",
      " ...\n",
      " [ 0.19260357]\n",
      " [ 0.08021009]\n",
      " [-0.32648541]]\n",
      "t [[ 0.08021009]\n",
      " [-0.38408586]\n",
      " [-0.22523236]\n",
      " ...\n",
      " [ 0.19260357]\n",
      " [ 0.08021009]\n",
      " [-0.32648541]]\n",
      "t [[ 0.12186559]\n",
      " [-0.70690062]\n",
      " [-0.40435806]\n",
      " ...\n",
      " [ 0.28270494]\n",
      " [ 0.12186559]\n",
      " [-0.52631289]]\n",
      "t [[ 0.12186559]\n",
      " [-0.70690062]\n",
      " [-0.40435806]\n",
      " ...\n",
      " [ 0.28270494]\n",
      " [ 0.12186559]\n",
      " [-0.52631289]]\n",
      "Current iteration=2, loss=43306.260883237\n",
      "t [[ 0.14198467]\n",
      " [-0.97739265]\n",
      " [-0.55391195]\n",
      " ...\n",
      " [ 0.32493824]\n",
      " [ 0.14198467]\n",
      " [-0.66832478]]\n",
      "t [[ 0.14198467]\n",
      " [-0.97739265]\n",
      " [-0.55391195]\n",
      " ...\n",
      " [ 0.32493824]\n",
      " [ 0.14198467]\n",
      " [-0.66832478]]\n",
      "t [[ 0.14939465]\n",
      " [-1.20587421]\n",
      " [-0.682666  ]\n",
      " ...\n",
      " [ 0.34259955]\n",
      " [ 0.14939465]\n",
      " [-0.78047833]]\n",
      "t [[ 0.14939465]\n",
      " [-1.20587421]\n",
      " [-0.682666  ]\n",
      " ...\n",
      " [ 0.34259955]\n",
      " [ 0.14939465]\n",
      " [-0.78047833]]\n",
      "Current iteration=4, loss=39035.044718934136\n",
      "t [[ 0.14897806]\n",
      " [-1.40107711]\n",
      " [-0.79574342]\n",
      " ...\n",
      " [ 0.34658623]\n",
      " [ 0.14897806]\n",
      " [-0.87483561]]\n",
      "t [[ 0.14897806]\n",
      " [-1.40107711]\n",
      " [-0.79574342]\n",
      " ...\n",
      " [ 0.34658623]\n",
      " [ 0.14897806]\n",
      " [-0.87483561]]\n",
      "t [[ 0.14363529]\n",
      " [-1.56980586]\n",
      " [-0.89644165]\n",
      " ...\n",
      " [ 0.34245201]\n",
      " [ 0.14363529]\n",
      " [-0.95714522]]\n",
      "t [[ 0.14363529]\n",
      " [-1.56980586]\n",
      " [-0.89644165]\n",
      " ...\n",
      " [ 0.34245201]\n",
      " [ 0.14363529]\n",
      " [-0.95714522]]\n",
      "Current iteration=6, loss=36500.43496553681\n",
      "t [[ 0.13518749]\n",
      " [-1.71723867]\n",
      " [-0.98704788]\n",
      " ...\n",
      " [ 0.33328003]\n",
      " [ 0.13518749]\n",
      " [-1.03048362]]\n",
      "t [[ 0.13518749]\n",
      " [-1.71723867]\n",
      " [-0.98704788]\n",
      " ...\n",
      " [ 0.33328003]\n",
      " [ 0.13518749]\n",
      " [-1.03048362]]\n",
      "t [[ 0.12482491]\n",
      " [-1.84731523]\n",
      " [-1.06923697]\n",
      " ...\n",
      " [ 0.32091551]\n",
      " [ 0.12482491]\n",
      " [-1.09669744]]\n",
      "t [[ 0.12482491]\n",
      " [-1.84731523]\n",
      " [-1.06923697]\n",
      " ...\n",
      " [ 0.32091551]\n",
      " [ 0.12482491]\n",
      " [-1.09669744]]\n",
      "Current iteration=8, loss=34836.59249564975\n",
      "t [[ 0.11334721]\n",
      " [-1.96306058]\n",
      " [-1.14428434]\n",
      " ...\n",
      " [ 0.30652925]\n",
      " [ 0.11334721]\n",
      " [-1.15701072]]\n",
      "t [[ 0.11334721]\n",
      " [-1.96306058]\n",
      " [-1.14428434]\n",
      " ...\n",
      " [ 0.30652925]\n",
      " [ 0.11334721]\n",
      " [-1.15701072]]\n",
      "t [[ 0.10130113]\n",
      " [-2.06682665]\n",
      " [-1.21318983]\n",
      " ...\n",
      " [ 0.2908961 ]\n",
      " [ 0.10130113]\n",
      " [-1.21230025]]\n",
      "loss=33669.56319515154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24221662]\n",
      " [-0.14120257]\n",
      " [-0.22598174]\n",
      " ...\n",
      " [ 0.18978452]\n",
      " [ 0.07989518]\n",
      " [-0.32155132]]\n",
      "t [[ 0.24221662]\n",
      " [-0.14120257]\n",
      " [-0.22598174]\n",
      " ...\n",
      " [ 0.18978452]\n",
      " [ 0.07989518]\n",
      " [-0.32155132]]\n",
      "t [[ 0.41466216]\n",
      " [-0.33377313]\n",
      " [-0.40536981]\n",
      " ...\n",
      " [ 0.2777929 ]\n",
      " [ 0.1211111 ]\n",
      " [-0.5165237 ]]\n",
      "t [[ 0.41466216]\n",
      " [-0.33377313]\n",
      " [-0.40536981]\n",
      " ...\n",
      " [ 0.2777929 ]\n",
      " [ 0.1211111 ]\n",
      " [-0.5165237 ]]\n",
      "Current iteration=2, loss=43293.25917137626\n",
      "t [[ 0.5414754 ]\n",
      " [-0.52661194]\n",
      " [-0.55495164]\n",
      " ...\n",
      " [ 0.31851004]\n",
      " [ 0.14076656]\n",
      " [-0.65422863]]\n",
      "t [[ 0.5414754 ]\n",
      " [-0.52661194]\n",
      " [-0.55495164]\n",
      " ...\n",
      " [ 0.31851004]\n",
      " [ 0.14076656]\n",
      " [-0.65422863]]\n",
      "t [[ 0.6373818 ]\n",
      " [-0.70469542]\n",
      " [-0.68362183]\n",
      " ...\n",
      " [ 0.33509352]\n",
      " [ 0.1477275 ]\n",
      " [-0.76269045]]\n",
      "t [[ 0.6373818 ]\n",
      " [-0.70469542]\n",
      " [-0.68362183]\n",
      " ...\n",
      " [ 0.33509352]\n",
      " [ 0.1477275 ]\n",
      " [-0.76269045]]\n",
      "Current iteration=4, loss=39022.19635009084\n",
      "t [[ 0.71154359]\n",
      " [-0.86540168]\n",
      " [-0.79656264]\n",
      " ...\n",
      " [ 0.33832182]\n",
      " [ 0.14689091]\n",
      " [-0.8539084 ]]\n",
      "t [[ 0.71154359]\n",
      " [-0.86540168]\n",
      " [-0.79656264]\n",
      " ...\n",
      " [ 0.33832182]\n",
      " [ 0.14689091]\n",
      " [-0.8539084 ]]\n",
      "t [[ 0.76996478]\n",
      " [-1.00983869]\n",
      " [-0.89710066]\n",
      " ...\n",
      " [ 0.33365995]\n",
      " [ 0.14116182]\n",
      " [-0.93354184]]\n",
      "t [[ 0.76996478]\n",
      " [-1.00983869]\n",
      " [-0.89710066]\n",
      " ...\n",
      " [ 0.33365995]\n",
      " [ 0.14116182]\n",
      " [-0.93354184]]\n",
      "Current iteration=6, loss=36488.09657464461\n",
      "t [[ 0.81674002]\n",
      " [-1.13999711]\n",
      " [-0.98753794]\n",
      " ...\n",
      " [ 0.32412855]\n",
      " [ 0.13236184]\n",
      " [-1.00458436]]\n",
      "t [[ 0.81674002]\n",
      " [-1.13999711]\n",
      " [-0.98753794]\n",
      " ...\n",
      " [ 0.32412855]\n",
      " [ 0.13236184]\n",
      " [-1.00458436]]\n",
      "t [[ 0.85474887]\n",
      " [-1.2578618 ]\n",
      " [-1.06955706]\n",
      " ...\n",
      " [ 0.31152929]\n",
      " [ 0.12167994]\n",
      " [-1.06881389]]\n",
      "t [[ 0.85474887]\n",
      " [-1.2578618 ]\n",
      " [-1.06955706]\n",
      " ...\n",
      " [ 0.31152929]\n",
      " [ 0.12167994]\n",
      " [-1.06881389]]\n",
      "Current iteration=8, loss=34824.32337621156\n",
      "t [[ 0.88606651]\n",
      " [-1.36517101]\n",
      " [-1.14443756]\n",
      " ...\n",
      " [ 0.29700254]\n",
      " [ 0.10991377]\n",
      " [-1.12739958]]\n",
      "t [[ 0.88606651]\n",
      " [-1.36517101]\n",
      " [-1.14443756]\n",
      " ...\n",
      " [ 0.29700254]\n",
      " [ 0.10991377]\n",
      " [-1.12739958]]\n",
      "t [[ 0.91221808]\n",
      " [-1.46338465]\n",
      " [-1.21318146]\n",
      " ...\n",
      " [ 0.28130156]\n",
      " [ 0.09760781]\n",
      " [-1.18117479]]\n",
      "loss=33656.990141780945\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.23911213]\n",
      " [-0.13517144]\n",
      " [-0.2251354 ]\n",
      " ...\n",
      " [ 0.19271264]\n",
      " [ 0.08136772]\n",
      " [-0.33145349]]\n",
      "t [[ 0.23911213]\n",
      " [-0.13517144]\n",
      " [-0.2251354 ]\n",
      " ...\n",
      " [ 0.19271264]\n",
      " [ 0.08136772]\n",
      " [-0.33145349]]\n",
      "t [[ 0.40937474]\n",
      " [-0.32194447]\n",
      " [-0.4036868 ]\n",
      " ...\n",
      " [ 0.2809529 ]\n",
      " [ 0.12359904]\n",
      " [-0.53256351]]\n",
      "t [[ 0.40937474]\n",
      " [-0.32194447]\n",
      " [-0.4036868 ]\n",
      " ...\n",
      " [ 0.2809529 ]\n",
      " [ 0.12359904]\n",
      " [-0.53256351]]\n",
      "Current iteration=2, loss=43372.852049392866\n",
      "t [[ 0.53465751]\n",
      " [-0.50900444]\n",
      " [-0.55253437]\n",
      " ...\n",
      " [ 0.32077724]\n",
      " [ 0.14403724]\n",
      " [-0.6745287 ]]\n",
      "t [[ 0.53465751]\n",
      " [-0.50900444]\n",
      " [-0.55253437]\n",
      " ...\n",
      " [ 0.32077724]\n",
      " [ 0.14403724]\n",
      " [-0.6745287 ]]\n",
      "t [[ 0.62950188]\n",
      " [-0.6814266 ]\n",
      " [-0.68056611]\n",
      " ...\n",
      " [ 0.33588677]\n",
      " [ 0.15163699]\n",
      " [-0.78613918]]\n",
      "t [[ 0.62950188]\n",
      " [-0.6814266 ]\n",
      " [-0.68056611]\n",
      " ...\n",
      " [ 0.33588677]\n",
      " [ 0.15163699]\n",
      " [-0.78613918]]\n",
      "Current iteration=4, loss=39158.29154235245\n",
      "t [[ 0.70292783]\n",
      " [-0.83668459]\n",
      " [-0.79294345]\n",
      " ...\n",
      " [ 0.33732337]\n",
      " [ 0.15133815]\n",
      " [-0.87978033]]\n",
      "t [[ 0.70292783]\n",
      " [-0.83668459]\n",
      " [-0.79294345]\n",
      " ...\n",
      " [ 0.33732337]\n",
      " [ 0.15133815]\n",
      " [-0.87978033]]\n",
      "t [[ 0.76083699]\n",
      " [-0.97593737]\n",
      " [-0.89297484]\n",
      " ...\n",
      " [ 0.33070244]\n",
      " [ 0.14607084]\n",
      " [-0.96133397]]\n",
      "t [[ 0.76083699]\n",
      " [-0.97593737]\n",
      " [-0.89297484]\n",
      " ...\n",
      " [ 0.33070244]\n",
      " [ 0.14607084]\n",
      " [-0.96133397]]\n",
      "Current iteration=6, loss=36661.095989527035\n",
      "t [[ 0.80725398]\n",
      " [-1.1011931 ]\n",
      " [-0.9829485 ]\n",
      " ...\n",
      " [ 0.319132  ]\n",
      " [ 0.13767325]\n",
      " [-1.03393048]]\n",
      "t [[ 0.80725398]\n",
      " [-1.1011931 ]\n",
      " [-0.9829485 ]\n",
      " ...\n",
      " [ 0.319132  ]\n",
      " [ 0.13767325]\n",
      " [-1.03393048]]\n",
      "t [[ 0.84501088]\n",
      " [-1.21443501]\n",
      " [-1.06453684]\n",
      " ...\n",
      " [ 0.30446569]\n",
      " [ 0.12734611]\n",
      " [-1.09943671]]\n",
      "t [[ 0.84501088]\n",
      " [-1.21443501]\n",
      " [-1.06453684]\n",
      " ...\n",
      " [ 0.30446569]\n",
      " [ 0.12734611]\n",
      " [-1.09943671]]\n",
      "Current iteration=8, loss=35022.824171379616\n",
      "t [[ 0.87615029]\n",
      " [-1.31739011]\n",
      " [-1.13901192]\n",
      " ...\n",
      " [ 0.28787552]\n",
      " [ 0.11589582]\n",
      " [-1.1590826 ]]\n",
      "t [[ 0.87615029]\n",
      " [-1.31739011]\n",
      " [-1.13901192]\n",
      " ...\n",
      " [ 0.28787552]\n",
      " [ 0.11589582]\n",
      " [-1.1590826 ]]\n",
      "t [[ 0.9021748 ]\n",
      " [-1.41150277]\n",
      " [-1.20737028]\n",
      " ...\n",
      " [ 0.2701344 ]\n",
      " [ 0.10387358]\n",
      " [-1.21374487]]\n",
      "loss=33873.97664640848\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.23872937]\n",
      " [-0.14201109]\n",
      " [-0.22442824]\n",
      " ...\n",
      " [ 0.34384537]\n",
      " [-0.22442824]\n",
      " [ 0.3220599 ]]\n",
      "t [[ 0.23872937]\n",
      " [-0.14201109]\n",
      " [-0.22442824]\n",
      " ...\n",
      " [ 0.34384537]\n",
      " [-0.22442824]\n",
      " [ 0.3220599 ]]\n",
      "t [[ 0.4086764 ]\n",
      " [-0.33285799]\n",
      " [-0.40266929]\n",
      " ...\n",
      " [ 0.57634399]\n",
      " [-0.40266929]\n",
      " [ 0.52184071]]\n",
      "t [[ 0.4086764 ]\n",
      " [-0.33285799]\n",
      " [-0.40266929]\n",
      " ...\n",
      " [ 0.57634399]\n",
      " [-0.40266929]\n",
      " [ 0.52184071]]\n",
      "Current iteration=2, loss=43373.580509367835\n",
      "t [[ 0.53363752]\n",
      " [-0.5230473 ]\n",
      " [-0.55132397]\n",
      " ...\n",
      " [ 0.74190588]\n",
      " [-0.55132397]\n",
      " [ 0.65447248]]\n",
      "t [[ 0.53363752]\n",
      " [-0.5230473 ]\n",
      " [-0.55132397]\n",
      " ...\n",
      " [ 0.74190588]\n",
      " [-0.55132397]\n",
      " [ 0.65447248]]\n",
      "t [[ 0.62801626]\n",
      " [-0.69831969]\n",
      " [-0.67920413]\n",
      " ...\n",
      " [ 0.86442491]\n",
      " [-0.67920413]\n",
      " [ 0.74708354]]\n",
      "t [[ 0.62801626]\n",
      " [-0.69831969]\n",
      " [-0.67920413]\n",
      " ...\n",
      " [ 0.86442491]\n",
      " [-0.67920413]\n",
      " [ 0.74708354]]\n",
      "Current iteration=4, loss=39146.13821868367\n",
      "t [[ 0.70086097]\n",
      " [-0.85630195]\n",
      " [-0.79144482]\n",
      " ...\n",
      " [ 0.95767404]\n",
      " [-0.79144482]\n",
      " [ 0.814077  ]]\n",
      "t [[ 0.70086097]\n",
      " [-0.85630195]\n",
      " [-0.79144482]\n",
      " ...\n",
      " [ 0.95767404]\n",
      " [-0.79144482]\n",
      " [ 0.814077  ]]\n",
      "t [[ 0.75811914]\n",
      " [-0.99817111]\n",
      " [-0.89134678]\n",
      " ...\n",
      " [ 1.03020042]\n",
      " [-0.89134678]\n",
      " [ 0.86381148]]\n",
      "t [[ 0.75811914]\n",
      " [-0.99817111]\n",
      " [-0.89134678]\n",
      " ...\n",
      " [ 1.03020042]\n",
      " [-0.89134678]\n",
      " [ 0.86381148]]\n",
      "Current iteration=6, loss=36639.802280549855\n",
      "t [[ 0.80384842]\n",
      " [-1.12592708]\n",
      " [-0.98119679]\n",
      " ...\n",
      " [ 1.08762133]\n",
      " [-0.98119679]\n",
      " [ 0.90149905]]\n",
      "t [[ 0.80384842]\n",
      " [-1.12592708]\n",
      " [-0.98119679]\n",
      " ...\n",
      " [ 1.08762133]\n",
      " [-0.98119679]\n",
      " [ 0.90149905]]\n",
      "t [[ 0.84090192]\n",
      " [-1.24154517]\n",
      " [-1.06266745]\n",
      " ...\n",
      " [ 1.13379107]\n",
      " [-1.06266745]\n",
      " [ 0.93057043]]\n",
      "t [[ 0.84090192]\n",
      " [-1.24154517]\n",
      " [-1.06266745]\n",
      " ...\n",
      " [ 1.13379107]\n",
      " [-1.06266745]\n",
      " [ 0.93057043]]\n",
      "Current iteration=8, loss=34995.1709437637\n",
      "t [[ 0.87133528]\n",
      " [-1.3467498 ]\n",
      " [-1.13703111]\n",
      " ...\n",
      " [ 1.17144124]\n",
      " [-1.13703111]\n",
      " [ 0.95337531]]\n",
      "t [[ 0.87133528]\n",
      " [-1.3467498 ]\n",
      " [-1.13703111]\n",
      " ...\n",
      " [ 1.17144124]\n",
      " [-1.13703111]\n",
      " [ 0.95337531]]\n",
      "t [[ 0.89665902]\n",
      " [-1.44298732]\n",
      " [-1.20528437]\n",
      " ...\n",
      " [ 1.20255589]\n",
      " [-1.20528437]\n",
      " [ 0.97157035]]\n",
      "loss=33841.91661207479\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08207544]\n",
      " [-0.39301809]\n",
      " [-0.23047032]\n",
      " ...\n",
      " [ 0.19708272]\n",
      " [ 0.08207544]\n",
      " [-0.3340781 ]]\n",
      "t [[ 0.08207544]\n",
      " [-0.39301809]\n",
      " [-0.23047032]\n",
      " ...\n",
      " [ 0.19708272]\n",
      " [ 0.08207544]\n",
      " [-0.3340781 ]]\n",
      "t [[ 0.12380718]\n",
      " [-0.72190767]\n",
      " [-0.41269072]\n",
      " ...\n",
      " [ 0.28690359]\n",
      " [ 0.12380718]\n",
      " [-0.5356217 ]]\n",
      "t [[ 0.12380718]\n",
      " [-0.72190767]\n",
      " [-0.41269072]\n",
      " ...\n",
      " [ 0.28690359]\n",
      " [ 0.12380718]\n",
      " [-0.5356217 ]]\n",
      "Current iteration=2, loss=43161.55355621166\n",
      "t [[ 0.14340932]\n",
      " [-0.99627931]\n",
      " [-0.56436308]\n",
      " ...\n",
      " [ 0.32791276]\n",
      " [ 0.14340932]\n",
      " [-0.67828256]]\n",
      "t [[ 0.14340932]\n",
      " [-0.99627931]\n",
      " [-0.56436308]\n",
      " ...\n",
      " [ 0.32791276]\n",
      " [ 0.14340932]\n",
      " [-0.67828256]]\n",
      "t [[ 0.15011877]\n",
      " [-1.22718067]\n",
      " [-0.69467669]\n",
      " ...\n",
      " [ 0.34428888]\n",
      " [ 0.15011877]\n",
      " [-0.79097936]]\n",
      "t [[ 0.15011877]\n",
      " [-1.22718067]\n",
      " [-0.69467669]\n",
      " ...\n",
      " [ 0.34428888]\n",
      " [ 0.15011877]\n",
      " [-0.79097936]]\n",
      "Current iteration=4, loss=38876.91193503366\n",
      "t [[ 0.14897331]\n",
      " [-1.42386422]\n",
      " [-0.80893919]\n",
      " ...\n",
      " [ 0.34710765]\n",
      " [ 0.14897331]\n",
      " [-0.885883  ]]\n",
      "t [[ 0.14897331]\n",
      " [-1.42386422]\n",
      " [-0.80893919]\n",
      " ...\n",
      " [ 0.34710765]\n",
      " [ 0.14897331]\n",
      " [-0.885883  ]]\n",
      "t [[ 0.14293816]\n",
      " [-1.59346732]\n",
      " [-0.91054999]\n",
      " ...\n",
      " [ 0.3419393 ]\n",
      " [ 0.14293816]\n",
      " [-0.96870952]]\n",
      "t [[ 0.14293816]\n",
      " [-1.59346732]\n",
      " [-0.91054999]\n",
      " ...\n",
      " [ 0.3419393 ]\n",
      " [ 0.14293816]\n",
      " [-0.96870952]]\n",
      "Current iteration=6, loss=36352.69252409726\n",
      "t [[ 0.13386236]\n",
      " [-1.74137665]\n",
      " [-1.00186202]\n",
      " ...\n",
      " [ 0.33185459]\n",
      " [ 0.13386236]\n",
      " [-1.04250408]]\n",
      "t [[ 0.13386236]\n",
      " [-1.74137665]\n",
      " [-1.00186202]\n",
      " ...\n",
      " [ 0.33185459]\n",
      " [ 0.13386236]\n",
      " [-1.04250408]]\n",
      "t [[ 0.12294692]\n",
      " [-1.8716638 ]\n",
      " [-1.08459618]\n",
      " ...\n",
      " [ 0.31868474]\n",
      " [ 0.12294692]\n",
      " [-1.10909958]]\n",
      "t [[ 0.12294692]\n",
      " [-1.8716638 ]\n",
      " [-1.08459618]\n",
      " ...\n",
      " [ 0.31868474]\n",
      " [ 0.12294692]\n",
      " [-1.10909958]]\n",
      "Current iteration=8, loss=34703.36744642074\n",
      "t [[ 0.11099363]\n",
      " [-1.98743902]\n",
      " [-1.1600616 ]\n",
      " ...\n",
      " [ 0.30358877]\n",
      " [ 0.11099363]\n",
      " [-1.16971839]]\n",
      "t [[ 0.11099363]\n",
      " [-1.98743902]\n",
      " [-1.1600616 ]\n",
      " ...\n",
      " [ 0.30358877]\n",
      " [ 0.11099363]\n",
      " [-1.16971839]]\n",
      "t [[ 0.09854674]\n",
      " [-2.09111041]\n",
      " [-1.22928348]\n",
      " ...\n",
      " [ 0.28733147]\n",
      " [ 0.09854674]\n",
      " [-1.22524174]]\n",
      "loss=33550.43542169759\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24784956]\n",
      " [-0.14448636]\n",
      " [-0.23123713]\n",
      " ...\n",
      " [ 0.19419811]\n",
      " [ 0.08175321]\n",
      " [-0.32902925]]\n",
      "t [[ 0.24784956]\n",
      " [-0.14448636]\n",
      " [-0.23123713]\n",
      " ...\n",
      " [ 0.19419811]\n",
      " [ 0.08175321]\n",
      " [-0.32902925]]\n",
      "t [[ 0.4226913 ]\n",
      " [-0.34271815]\n",
      " [-0.41371492]\n",
      " ...\n",
      " [ 0.28189391]\n",
      " [ 0.1230324 ]\n",
      " [-0.52560719]]\n",
      "t [[ 0.4226913 ]\n",
      " [-0.34271815]\n",
      " [-0.41371492]\n",
      " ...\n",
      " [ 0.28189391]\n",
      " [ 0.1230324 ]\n",
      " [-0.52560719]]\n",
      "Current iteration=2, loss=43148.49878076154\n",
      "t [[ 0.55037045]\n",
      " [-0.54005476]\n",
      " [-0.56540526]\n",
      " ...\n",
      " [ 0.32137826]\n",
      " [ 0.14215898]\n",
      " [-0.66388632]]\n",
      "t [[ 0.55037045]\n",
      " [-0.54005476]\n",
      " [-0.56540526]\n",
      " ...\n",
      " [ 0.32137826]\n",
      " [ 0.14215898]\n",
      " [-0.66388632]]\n",
      "t [[ 0.64638418]\n",
      " [-0.72128456]\n",
      " [-0.69562537]\n",
      " ...\n",
      " [ 0.33668155]\n",
      " [ 0.14840983]\n",
      " [-0.77284766]]\n",
      "t [[ 0.64638418]\n",
      " [-0.72128456]\n",
      " [-0.69562537]\n",
      " ...\n",
      " [ 0.33668155]\n",
      " [ 0.14840983]\n",
      " [-0.77284766]]\n",
      "Current iteration=4, loss=38864.09576469685\n",
      "t [[ 0.72027084]\n",
      " [-0.88415074]\n",
      " [-0.80974314]\n",
      " ...\n",
      " [ 0.33875348]\n",
      " [ 0.14683717]\n",
      " [-0.86458939]]\n",
      "t [[ 0.72027084]\n",
      " [-0.88415074]\n",
      " [-0.80974314]\n",
      " ...\n",
      " [ 0.33875348]\n",
      " [ 0.14683717]\n",
      " [-0.86458939]]\n",
      "t [[ 0.77823382]\n",
      " [-1.0300856 ]\n",
      " [-0.91118718]\n",
      " ...\n",
      " [ 0.3330717 ]\n",
      " [ 0.14041054]\n",
      " [-0.94473054]]\n",
      "t [[ 0.77823382]\n",
      " [-1.0300856 ]\n",
      " [-0.91118718]\n",
      " ...\n",
      " [ 0.3330717 ]\n",
      " [ 0.14041054]\n",
      " [-0.94473054]]\n",
      "Current iteration=6, loss=36340.381146793356\n",
      "t [[ 0.82447787]\n",
      " [-1.16129844]\n",
      " [-1.00232501]\n",
      " ...\n",
      " [ 0.3226425 ]\n",
      " [ 0.13097909]\n",
      " [-1.01622831]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.82447787]\n",
      " [-1.16129844]\n",
      " [-1.00232501]\n",
      " ...\n",
      " [ 0.3226425 ]\n",
      " [ 0.13097909]\n",
      " [-1.01622831]]\n",
      "t [[ 0.86194438]\n",
      " [-1.27991343]\n",
      " [-1.08488498]\n",
      " ...\n",
      " [ 0.3092527 ]\n",
      " [ 0.11974224]\n",
      " [-1.08084373]]\n",
      "t [[ 0.86194438]\n",
      " [-1.27991343]\n",
      " [-1.08488498]\n",
      " ...\n",
      " [ 0.3092527 ]\n",
      " [ 0.11974224]\n",
      " [-1.08084373]]\n",
      "Current iteration=8, loss=34691.09061223772\n",
      "t [[ 0.8927429 ]\n",
      " [-1.38775804]\n",
      " [-1.16018015]\n",
      " ...\n",
      " [ 0.29403055]\n",
      " [ 0.10749953]\n",
      " [-1.13974235]]\n",
      "t [[ 0.8927429 ]\n",
      " [-1.38775804]\n",
      " [-1.16018015]\n",
      " ...\n",
      " [ 0.29403055]\n",
      " [ 0.10749953]\n",
      " [-1.13974235]]\n",
      "t [[ 0.91841682]\n",
      " [-1.48635078]\n",
      " [-1.22923772]\n",
      " ...\n",
      " [ 0.27771917]\n",
      " [ 0.09479276]\n",
      " [-1.19376075]]\n",
      "loss=33537.81480219631\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.24467288]\n",
      " [-0.13831497]\n",
      " [-0.23037111]\n",
      " ...\n",
      " [ 0.19719433]\n",
      " [ 0.08326   ]\n",
      " [-0.33916172]]\n",
      "t [[ 0.24467288]\n",
      " [-0.13831497]\n",
      " [-0.23037111]\n",
      " ...\n",
      " [ 0.19719433]\n",
      " [ 0.08326   ]\n",
      " [-0.33916172]]\n",
      "t [[ 0.4173021 ]\n",
      " [-0.33061969]\n",
      " [-0.41199343]\n",
      " ...\n",
      " [ 0.28506864]\n",
      " [ 0.12556804]\n",
      " [-0.54193582]]\n",
      "t [[ 0.4173021 ]\n",
      " [-0.33061969]\n",
      " [-0.41199343]\n",
      " ...\n",
      " [ 0.28506864]\n",
      " [ 0.12556804]\n",
      " [-0.54193582]]\n",
      "Current iteration=2, loss=43229.804039953466\n",
      "t [[ 0.54344523]\n",
      " [-0.52204405]\n",
      " [-0.56293716]\n",
      " ...\n",
      " [ 0.3235876 ]\n",
      " [ 0.14548499]\n",
      " [-0.6844893 ]]\n",
      "t [[ 0.54344523]\n",
      " [-0.52204405]\n",
      " [-0.56293716]\n",
      " ...\n",
      " [ 0.3235876 ]\n",
      " [ 0.14548499]\n",
      " [-0.6844893 ]]\n",
      "t [[ 0.63840394]\n",
      " [-0.69748862]\n",
      " [-0.69251031]\n",
      " ...\n",
      " [ 0.33734123]\n",
      " [ 0.15237941]\n",
      " [-0.79659614]]\n",
      "t [[ 0.63840394]\n",
      " [-0.69748862]\n",
      " [-0.69251031]\n",
      " ...\n",
      " [ 0.33734123]\n",
      " [ 0.15237941]\n",
      " [-0.79659614]]\n",
      "Current iteration=4, loss=39002.39515276726\n",
      "t [[ 0.71156669]\n",
      " [-0.85479853]\n",
      " [-0.8060581 ]\n",
      " ...\n",
      " [ 0.33754972]\n",
      " [ 0.15134741]\n",
      " [-0.89075074]]\n",
      "t [[ 0.71156669]\n",
      " [-0.85479853]\n",
      " [-0.8060581 ]\n",
      " ...\n",
      " [ 0.33754972]\n",
      " [ 0.15134741]\n",
      " [-0.89075074]]\n",
      "t [[ 0.76903059]\n",
      " [-0.99545774]\n",
      " [-0.90698996]\n",
      " ...\n",
      " [ 0.32984329]\n",
      " [ 0.14538427]\n",
      " [-0.9727986 ]]\n",
      "t [[ 0.76903059]\n",
      " [-0.99545774]\n",
      " [-0.90698996]\n",
      " ...\n",
      " [ 0.32984329]\n",
      " [ 0.14538427]\n",
      " [-0.9727986 ]]\n",
      "Current iteration=6, loss=36515.59656216533\n",
      "t [[ 0.81492865]\n",
      " [-1.12169148]\n",
      " [-0.99765903]\n",
      " ...\n",
      " [ 0.31731583]\n",
      " [ 0.13635609]\n",
      " [-1.04583535]]\n",
      "t [[ 0.81492865]\n",
      " [-1.12169148]\n",
      " [-0.99765903]\n",
      " ...\n",
      " [ 0.31731583]\n",
      " [ 0.13635609]\n",
      " [-1.04583535]]\n",
      "t [[ 0.85215404]\n",
      " [-1.23562021]\n",
      " [-1.07978323]\n",
      " ...\n",
      " [ 0.30180571]\n",
      " [ 0.12547436]\n",
      " [-1.1117117 ]]\n",
      "t [[ 0.85215404]\n",
      " [-1.23562021]\n",
      " [-1.07978323]\n",
      " ...\n",
      " [ 0.30180571]\n",
      " [ 0.12547436]\n",
      " [-1.1117117 ]]\n",
      "Current iteration=8, loss=34891.68056586047\n",
      "t [[ 0.88278352]\n",
      " [-1.33905812]\n",
      " [-1.15466797]\n",
      " ...\n",
      " [ 0.28447223]\n",
      " [ 0.11354752]\n",
      " [-1.1716544 ]]\n",
      "t [[ 0.88278352]\n",
      " [-1.33905812]\n",
      " [-1.15466797]\n",
      " ...\n",
      " [ 0.28447223]\n",
      " [ 0.11354752]\n",
      " [-1.1716544 ]]\n",
      "t [[ 0.90833792]\n",
      " [-1.43350644]\n",
      " [-1.22333494]\n",
      " ...\n",
      " [ 0.2660776 ]\n",
      " [ 0.10112424]\n",
      " [-1.22654366]]\n",
      "loss=33756.72576779485\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24428122]\n",
      " [-0.14531368]\n",
      " [-0.2296475 ]\n",
      " ...\n",
      " [ 0.35184177]\n",
      " [-0.2296475 ]\n",
      " [ 0.32954966]]\n",
      "t [[ 0.24428122]\n",
      " [-0.14531368]\n",
      " [-0.2296475 ]\n",
      " ...\n",
      " [ 0.35184177]\n",
      " [-0.2296475 ]\n",
      " [ 0.32954966]]\n",
      "t [[ 0.41658644]\n",
      " [-0.34172582]\n",
      " [-0.41096103]\n",
      " ...\n",
      " [ 0.58716728]\n",
      " [-0.41096103]\n",
      " [ 0.53114581]]\n",
      "t [[ 0.41658644]\n",
      " [-0.34172582]\n",
      " [-0.41096103]\n",
      " ...\n",
      " [ 0.58716728]\n",
      " [-0.41096103]\n",
      " [ 0.53114581]]\n",
      "Current iteration=2, loss=43230.23108425431\n",
      "t [[ 0.54239337]\n",
      " [-0.53631554]\n",
      " [-0.56171274]\n",
      " ...\n",
      " [ 0.75351317]\n",
      " [-0.56171274]\n",
      " [ 0.66378812]]\n",
      "t [[ 0.54239337]\n",
      " [-0.53631554]\n",
      " [-0.56171274]\n",
      " ...\n",
      " [ 0.75351317]\n",
      " [-0.56171274]\n",
      " [ 0.66378812]]\n",
      "t [[ 0.63686498]\n",
      " [-0.71465941]\n",
      " [-0.69113348]\n",
      " ...\n",
      " [ 0.87592135]\n",
      " [-0.69113348]\n",
      " [ 0.75580028]]\n",
      "t [[ 0.63686498]\n",
      " [-0.71465941]\n",
      " [-0.69113348]\n",
      " ...\n",
      " [ 0.87592135]\n",
      " [-0.69113348]\n",
      " [ 0.75580028]]\n",
      "Current iteration=4, loss=38989.72364564134\n",
      "t [[ 0.70942396]\n",
      " [-0.8747455 ]\n",
      " [-0.80454276]\n",
      " ...\n",
      " [ 0.96864734]\n",
      " [-0.80454276]\n",
      " [ 0.82199553]]\n",
      "t [[ 0.70942396]\n",
      " [-0.8747455 ]\n",
      " [-0.80454276]\n",
      " ...\n",
      " [ 0.96864734]\n",
      " [-0.80454276]\n",
      " [ 0.82199553]]\n",
      "t [[ 0.76621563]\n",
      " [-1.01806923]\n",
      " [-0.90534306]\n",
      " ...\n",
      " [ 1.04047022]\n",
      " [-0.90534306]\n",
      " [ 0.87089618]]\n",
      "t [[ 0.76621563]\n",
      " [-1.01806923]\n",
      " [-0.90534306]\n",
      " ...\n",
      " [ 1.04047022]\n",
      " [-0.90534306]\n",
      " [ 0.87089618]]\n",
      "Current iteration=6, loss=36493.754038072344\n",
      "t [[ 0.81140662]\n",
      " [-1.14684488]\n",
      " [-0.99588652]\n",
      " ...\n",
      " [ 1.09712897]\n",
      " [-0.99588652]\n",
      " [ 0.90778784]]\n",
      "t [[ 0.81140662]\n",
      " [-1.14684488]\n",
      " [-0.99588652]\n",
      " ...\n",
      " [ 1.09712897]\n",
      " [-0.99588652]\n",
      " [ 0.90778784]]\n",
      "t [[ 0.84791116]\n",
      " [-1.26318458]\n",
      " [-1.07789138]\n",
      " ...\n",
      " [ 1.14254437]\n",
      " [-1.07789138]\n",
      " [ 0.93613566]]\n",
      "t [[ 0.84791116]\n",
      " [-1.26318458]\n",
      " [-1.07789138]\n",
      " ...\n",
      " [ 1.14254437]\n",
      " [-1.07789138]\n",
      " [ 0.93613566]]\n",
      "Current iteration=8, loss=34863.514223437276\n",
      "t [[ 0.87781885]\n",
      " [-1.36890045]\n",
      " [-1.1526634 ]\n",
      " ...\n",
      " [ 1.17948411]\n",
      " [-1.1526634 ]\n",
      " [ 0.95830417]]\n",
      "t [[ 0.87781885]\n",
      " [-1.36890045]\n",
      " [-1.1526634 ]\n",
      " ...\n",
      " [ 1.17948411]\n",
      " [-1.1526634 ]\n",
      " [ 0.95830417]]\n",
      "t [[ 0.90265822]\n",
      " [-1.46549663]\n",
      " [-1.22122425]\n",
      " ...\n",
      " [ 1.20995058]\n",
      " [-1.22122425]\n",
      " [ 0.97595412]]\n",
      "loss=33724.22229622849\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0839408 ]\n",
      " [-0.40195032]\n",
      " [-0.23570828]\n",
      " ...\n",
      " [ 0.20156187]\n",
      " [ 0.0839408 ]\n",
      " [-0.34167078]]\n",
      "t [[ 0.0839408 ]\n",
      " [-0.40195032]\n",
      " [-0.23570828]\n",
      " ...\n",
      " [ 0.20156187]\n",
      " [ 0.0839408 ]\n",
      " [-0.34167078]]\n",
      "t [[ 0.1257099 ]\n",
      " [-0.73685137]\n",
      " [-0.42097649]\n",
      " ...\n",
      " [ 0.29099863]\n",
      " [ 0.1257099 ]\n",
      " [-0.54480295]]\n",
      "t [[ 0.1257099 ]\n",
      " [-0.73685137]\n",
      " [-0.42097649]\n",
      " ...\n",
      " [ 0.29099863]\n",
      " [ 0.1257099 ]\n",
      " [-0.54480295]]\n",
      "Current iteration=2, loss=43018.789813270356\n",
      "t [[ 0.14476974]\n",
      " [-1.01500766]\n",
      " [-0.57472506]\n",
      " ...\n",
      " [ 0.33074262]\n",
      " [ 0.14476974]\n",
      " [-0.68806649]]\n",
      "t [[ 0.14476974]\n",
      " [-1.01500766]\n",
      " [-0.57472506]\n",
      " ...\n",
      " [ 0.33074262]\n",
      " [ 0.14476974]\n",
      " [-0.68806649]]\n",
      "t [[ 0.15076679]\n",
      " [-1.24823413]\n",
      " [-0.7065617 ]\n",
      " ...\n",
      " [ 0.34582962]\n",
      " [ 0.15076679]\n",
      " [-0.80129892]]\n",
      "t [[ 0.15076679]\n",
      " [-1.24823413]\n",
      " [-0.7065617 ]\n",
      " ...\n",
      " [ 0.34582962]\n",
      " [ 0.15076679]\n",
      " [-0.80129892]]\n",
      "Current iteration=4, loss=38722.35985337627\n",
      "t [[ 0.14888989]\n",
      " [-1.4463164 ]\n",
      " [-0.82197636]\n",
      " ...\n",
      " [ 0.3474906 ]\n",
      " [ 0.14888989]\n",
      " [-0.89674863]]\n",
      "t [[ 0.14888989]\n",
      " [-1.4463164 ]\n",
      " [-0.82197636]\n",
      " ...\n",
      " [ 0.3474906 ]\n",
      " [ 0.14888989]\n",
      " [-0.89674863]]\n",
      "t [[ 0.14216618]\n",
      " [-1.61672694]\n",
      " [-0.92446957]\n",
      " ...\n",
      " [ 0.34130271]\n",
      " [ 0.14216618]\n",
      " [-0.98008859]]\n",
      "t [[ 0.14216618]\n",
      " [-1.61672694]\n",
      " [-0.92446957]\n",
      " ...\n",
      " [ 0.34130271]\n",
      " [ 0.14216618]\n",
      " [-0.98008859]]\n",
      "Current iteration=6, loss=36209.126133978796\n",
      "t [[ 0.13247065]\n",
      " [-1.76505942]\n",
      " [-1.01645973]\n",
      " ...\n",
      " [ 0.33032123]\n",
      " [ 0.13247065]\n",
      " [-1.05433118]]\n",
      "t [[ 0.13247065]\n",
      " [-1.76505942]\n",
      " [-1.01645973]\n",
      " ...\n",
      " [ 0.33032123]\n",
      " [ 0.13247065]\n",
      " [-1.05433118]]\n",
      "t [[ 0.12101376]\n",
      " [-1.89551508]\n",
      " [-1.0997137 ]\n",
      " ...\n",
      " [ 0.31636259]\n",
      " [ 0.12101376]\n",
      " [-1.12129672]]\n",
      "t [[ 0.12101376]\n",
      " [-1.89551508]\n",
      " [-1.0997137 ]\n",
      " ...\n",
      " [ 0.31636259]\n",
      " [ 0.12101376]\n",
      " [-1.12129672]]\n",
      "Current iteration=8, loss=34574.42329269154\n",
      "t [[ 0.10859834]\n",
      " [-2.01128711]\n",
      " [-1.17557425]\n",
      " ...\n",
      " [ 0.30057376]\n",
      " [ 0.10859834]\n",
      " [-1.18220736]]\n",
      "t [[ 0.10859834]\n",
      " [-2.01128711]\n",
      " [-1.17557425]\n",
      " ...\n",
      " [ 0.30057376]\n",
      " [ 0.10859834]\n",
      " [-1.18220736]]\n",
      "t [[ 0.09576541]\n",
      " [-2.11483795]\n",
      " [-1.24509178]\n",
      " ...\n",
      " [ 0.2837093 ]\n",
      " [ 0.09576541]\n",
      " [-1.23794992]]\n",
      "loss=33435.48625747453\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.2534825 ]\n",
      " [-0.14777014]\n",
      " [-0.23649251]\n",
      " ...\n",
      " [ 0.19861171]\n",
      " [ 0.08361123]\n",
      " [-0.33650719]]\n",
      "t [[ 0.2534825 ]\n",
      " [-0.14777014]\n",
      " [-0.23649251]\n",
      " ...\n",
      " [ 0.19861171]\n",
      " [ 0.08361123]\n",
      " [-0.33650719]]\n",
      "t [[ 0.43065017]\n",
      " [-0.35171429]\n",
      " [-0.42201266]\n",
      " ...\n",
      " [ 0.28589203]\n",
      " [ 0.12491471]\n",
      " [-0.53456326]]\n",
      "t [[ 0.43065017]\n",
      " [-0.35171429]\n",
      " [-0.42201266]\n",
      " ...\n",
      " [ 0.28589203]\n",
      " [ 0.12491471]\n",
      " [-0.53456326]]\n",
      "Current iteration=2, loss=43005.687530553434\n",
      "t [[ 0.55912976]\n",
      " [-0.55349916]\n",
      " [-0.57576904]\n",
      " ...\n",
      " [ 0.32410356]\n",
      " [ 0.14348709]\n",
      " [-0.67337182]]\n",
      "t [[ 0.55912976]\n",
      " [-0.55349916]\n",
      " [-0.57576904]\n",
      " ...\n",
      " [ 0.32410356]\n",
      " [ 0.14348709]\n",
      " [-0.67337182]]\n",
      "t [[ 0.65520137]\n",
      " [-0.73778875]\n",
      " [-0.70750254]\n",
      " ...\n",
      " [ 0.33812358]\n",
      " [ 0.14901612]\n",
      " [-0.78282705]]\n",
      "t [[ 0.65520137]\n",
      " [-0.73778875]\n",
      " [-0.70750254]\n",
      " ...\n",
      " [ 0.33812358]\n",
      " [ 0.14901612]\n",
      " [-0.78282705]]\n",
      "Current iteration=4, loss=38709.576852444516\n",
      "t [[ 0.72877913]\n",
      " [-0.90272921]\n",
      " [-0.82276449]\n",
      " ...\n",
      " [ 0.33904984]\n",
      " [ 0.14670503]\n",
      " [-0.8750941 ]]\n",
      "t [[ 0.72877913]\n",
      " [-0.90272921]\n",
      " [-0.82276449]\n",
      " ...\n",
      " [ 0.33904984]\n",
      " [ 0.14670503]\n",
      " [-0.8750941 ]]\n",
      "t [[ 0.78626317]\n",
      " [-1.05008886]\n",
      " [-0.92508455]\n",
      " ...\n",
      " [ 0.33236304]\n",
      " [ 0.1395849 ]\n",
      " [-0.95574096]]\n",
      "t [[ 0.78626317]\n",
      " [-1.05008886]\n",
      " [-0.92508455]\n",
      " ...\n",
      " [ 0.33236304]\n",
      " [ 0.1395849 ]\n",
      " [-0.95574096]]\n",
      "Current iteration=6, loss=36196.839657899676\n",
      "t [[ 0.83196563]\n",
      " [-1.18229657]\n",
      " [-1.01689542]\n",
      " ...\n",
      " [ 0.32105212]\n",
      " [ 0.12953048]\n",
      " [-1.02768695]]\n",
      "t [[ 0.83196563]\n",
      " [-1.18229657]\n",
      " [-1.01689542]\n",
      " ...\n",
      " [ 0.32105212]\n",
      " [ 0.12953048]\n",
      " [-1.02768695]]\n",
      "t [[ 0.86888731]\n",
      " [-1.30161374]\n",
      " [-1.09997116]\n",
      " ...\n",
      " [ 0.30688833]\n",
      " [ 0.11775032]\n",
      " [-1.09267743]]\n",
      "t [[ 0.86888731]\n",
      " [-1.30161374]\n",
      " [-1.09997116]\n",
      " ...\n",
      " [ 0.30688833]\n",
      " [ 0.11775032]\n",
      " [-1.09267743]]\n",
      "Current iteration=8, loss=34562.135419566395\n",
      "t [[ 0.89917013]\n",
      " [-1.40995459]\n",
      " [-1.17565821]\n",
      " ...\n",
      " [ 0.29098757]\n",
      " [ 0.10504475]\n",
      " [-1.15187587]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.89917013]\n",
      " [-1.40995459]\n",
      " [-1.17565821]\n",
      " ...\n",
      " [ 0.29098757]\n",
      " [ 0.10504475]\n",
      " [-1.15187587]]\n",
      "t [[ 0.92437407]\n",
      " [-1.5088942 ]\n",
      " [-1.24500884]\n",
      " ...\n",
      " [ 0.27408267]\n",
      " [ 0.09195213]\n",
      " [-1.20612329]]\n",
      "loss=33422.8147363282\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.25023362]\n",
      " [-0.14145849]\n",
      " [-0.23560682]\n",
      " ...\n",
      " [ 0.20167602]\n",
      " [ 0.08515227]\n",
      " [-0.34686994]]\n",
      "t [[ 0.25023362]\n",
      " [-0.14145849]\n",
      " [-0.23560682]\n",
      " ...\n",
      " [ 0.20167602]\n",
      " [ 0.08515227]\n",
      " [-0.34686994]]\n",
      "t [[ 0.42516012]\n",
      " [-0.33934625]\n",
      " [-0.42025275]\n",
      " ...\n",
      " [ 0.28907914]\n",
      " [ 0.12749764]\n",
      " [-0.55117723]]\n",
      "t [[ 0.42516012]\n",
      " [-0.33934625]\n",
      " [-0.42025275]\n",
      " ...\n",
      " [ 0.28907914]\n",
      " [ 0.12749764]\n",
      " [-0.55117723]]\n",
      "Current iteration=2, loss=43088.69374182164\n",
      "t [[ 0.55209917]\n",
      " [-0.53508529]\n",
      " [-0.57325043]\n",
      " ...\n",
      " [ 0.32625163]\n",
      " [ 0.14686776]\n",
      " [-0.69427224]]\n",
      "t [[ 0.55209917]\n",
      " [-0.53508529]\n",
      " [-0.57325043]\n",
      " ...\n",
      " [ 0.32625163]\n",
      " [ 0.14686776]\n",
      " [-0.69427224]]\n",
      "t [[ 0.64712353]\n",
      " [-0.71346632]\n",
      " [-0.7043287 ]\n",
      " ...\n",
      " [ 0.33864602]\n",
      " [ 0.15304489]\n",
      " [-0.80686866]]\n",
      "t [[ 0.64712353]\n",
      " [-0.71346632]\n",
      " [-0.7043287 ]\n",
      " ...\n",
      " [ 0.33864602]\n",
      " [ 0.15304489]\n",
      " [-0.80686866]]\n",
      "Current iteration=4, loss=38850.04140290593\n",
      "t [[ 0.71998965]\n",
      " [-0.87274381]\n",
      " [-0.8190143 ]\n",
      " ...\n",
      " [ 0.3376374 ]\n",
      " [ 0.1512772 ]\n",
      " [-0.9015376 ]]\n",
      "t [[ 0.71998965]\n",
      " [-0.87274381]\n",
      " [-0.8190143 ]\n",
      " ...\n",
      " [ 0.3376374 ]\n",
      " [ 0.1512772 ]\n",
      " [-0.9015376 ]]\n",
      "t [[ 0.77698765]\n",
      " [-1.01473812]\n",
      " [-0.92081671]\n",
      " ...\n",
      " [ 0.32886099]\n",
      " [ 0.14462212]\n",
      " [-0.98407728]]\n",
      "t [[ 0.77698765]\n",
      " [-1.01473812]\n",
      " [-0.92081671]\n",
      " ...\n",
      " [ 0.32886099]\n",
      " [ 0.14462212]\n",
      " [-0.98407728]]\n",
      "Current iteration=6, loss=36374.21795928698\n",
      "t [[ 0.82235628]\n",
      " [-1.14189224]\n",
      " [-1.01215374]\n",
      " ...\n",
      " [ 0.31539341]\n",
      " [ 0.13497176]\n",
      " [-1.05754699]]\n",
      "t [[ 0.82235628]\n",
      " [-1.14189224]\n",
      " [-1.01215374]\n",
      " ...\n",
      " [ 0.31539341]\n",
      " [ 0.13497176]\n",
      " [-1.05754699]]\n",
      "t [[ 0.85904747]\n",
      " [-1.25646161]\n",
      " [-1.0947887 ]\n",
      " ...\n",
      " [ 0.29905691]\n",
      " [ 0.12354695]\n",
      " [-1.12378246]]\n",
      "t [[ 0.85904747]\n",
      " [-1.25646161]\n",
      " [-1.0947887 ]\n",
      " ...\n",
      " [ 0.29905691]\n",
      " [ 0.12354695]\n",
      " [-1.12378246]]\n",
      "Current iteration=8, loss=34764.755233811455\n",
      "t [[ 0.88917016]\n",
      " [-1.3603451 ]\n",
      " [-1.17006032]\n",
      " ...\n",
      " [ 0.28099784]\n",
      " [ 0.11115718]\n",
      " [-1.18400874]]\n",
      "t [[ 0.88917016]\n",
      " [-1.3603451 ]\n",
      " [-1.17006032]\n",
      " ...\n",
      " [ 0.28099784]\n",
      " [ 0.11115718]\n",
      " [-1.18400874]]\n",
      "t [[ 0.9142618 ]\n",
      " [-1.45509868]\n",
      " [-1.23901526]\n",
      " ...\n",
      " [ 0.26196752]\n",
      " [ 0.09834778]\n",
      " [-1.23911073]]\n",
      "loss=33643.58885647325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.24983306]\n",
      " [-0.14861626]\n",
      " [-0.23486676]\n",
      " ...\n",
      " [ 0.35983818]\n",
      " [-0.23486676]\n",
      " [ 0.33703943]]\n",
      "t [[ 0.24983306]\n",
      " [-0.14861626]\n",
      " [-0.23486676]\n",
      " ...\n",
      " [ 0.35983818]\n",
      " [-0.23486676]\n",
      " [ 0.33703943]]\n",
      "t [[ 0.42442692]\n",
      " [-0.35064249]\n",
      " [-0.41920581]\n",
      " ...\n",
      " [ 0.59787805]\n",
      " [-0.41920581]\n",
      " [ 0.54032762]]\n",
      "t [[ 0.42442692]\n",
      " [-0.35064249]\n",
      " [-0.41920581]\n",
      " ...\n",
      " [ 0.59787805]\n",
      " [-0.41920581]\n",
      " [ 0.54032762]]\n",
      "Current iteration=2, loss=43088.81523188576\n",
      "t [[ 0.55101471]\n",
      " [-0.5495833 ]\n",
      " [-0.57201232]\n",
      " ...\n",
      " [ 0.76492017]\n",
      " [-0.57201232]\n",
      " [ 0.67290305]]\n",
      "t [[ 0.55101471]\n",
      " [-0.5495833 ]\n",
      " [-0.57201232]\n",
      " ...\n",
      " [ 0.76492017]\n",
      " [-0.57201232]\n",
      " [ 0.67290305]]\n",
      "t [[ 0.64553027]\n",
      " [-0.73091346]\n",
      " [-0.70293723]\n",
      " ...\n",
      " [ 0.88715932]\n",
      " [-0.70293723]\n",
      " [ 0.76427668]]\n",
      "t [[ 0.64553027]\n",
      " [-0.73091346]\n",
      " [-0.70293723]\n",
      " ...\n",
      " [ 0.88715932]\n",
      " [-0.70293723]\n",
      " [ 0.76427668]]\n",
      "Current iteration=4, loss=38836.85851017337\n",
      "t [[ 0.71777017]\n",
      " [-0.89301923]\n",
      " [-0.81748239]\n",
      " ...\n",
      " [ 0.97932585]\n",
      " [-0.81748239]\n",
      " [ 0.8296561 ]]\n",
      "t [[ 0.71777017]\n",
      " [-0.89301923]\n",
      " [-0.81748239]\n",
      " ...\n",
      " [ 0.97932585]\n",
      " [-0.81748239]\n",
      " [ 0.8296561 ]]\n",
      "t [[ 0.77407492]\n",
      " [-1.03772572]\n",
      " [-0.9191511 ]\n",
      " ...\n",
      " [ 1.05042483]\n",
      " [-0.9191511 ]\n",
      " [ 0.87771859]]\n",
      "t [[ 0.77407492]\n",
      " [-1.03772572]\n",
      " [-0.9191511 ]\n",
      " ...\n",
      " [ 1.05042483]\n",
      " [-0.9191511 ]\n",
      " [ 0.87771859]]\n",
      "Current iteration=6, loss=36351.83862710601\n",
      "t [[ 0.81871744]\n",
      " [-1.16746255]\n",
      " [-1.01036057]\n",
      " ...\n",
      " [ 1.1063129 ]\n",
      " [-1.01036057]\n",
      " [ 0.91381877]]\n",
      "t [[ 0.81871744]\n",
      " [-1.16746255]\n",
      " [-1.01036057]\n",
      " ...\n",
      " [ 1.1063129 ]\n",
      " [-1.01036057]\n",
      " [ 0.91381877]]\n",
      "t [[ 0.85467065]\n",
      " [-1.28447657]\n",
      " [-1.09287462]\n",
      " ...\n",
      " [ 1.15097435]\n",
      " [-1.09287462]\n",
      " [ 0.94145355]]\n",
      "t [[ 0.85467065]\n",
      " [-1.28447657]\n",
      " [-1.09287462]\n",
      " ...\n",
      " [ 1.15097435]\n",
      " [-1.09287462]\n",
      " [ 0.94145355]]\n",
      "Current iteration=8, loss=34736.0916171326\n",
      "t [[ 0.88405614]\n",
      " [-1.3906653 ]\n",
      " [-1.16803227]\n",
      " ...\n",
      " [ 1.1872106 ]\n",
      " [-1.16803227]\n",
      " [ 0.96300046]]\n",
      "t [[ 0.88405614]\n",
      " [-1.3906653 ]\n",
      " [-1.16803227]\n",
      " ...\n",
      " [ 1.1872106 ]\n",
      " [-1.16803227]\n",
      " [ 0.96300046]]\n",
      "t [[ 0.9084188 ]\n",
      " [-1.48758864]\n",
      " [-1.23688015]\n",
      " ...\n",
      " [ 1.21704057]\n",
      " [-1.23688015]\n",
      " [ 0.98012288]]\n",
      "loss=33610.66030145165\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08580615]\n",
      " [-0.41088255]\n",
      " [-0.24094625]\n",
      " ...\n",
      " [ 0.20604103]\n",
      " [ 0.08580615]\n",
      " [-0.34926347]]\n",
      "t [[ 0.08580615]\n",
      " [-0.41088255]\n",
      " [-0.24094625]\n",
      " ...\n",
      " [ 0.20604103]\n",
      " [ 0.08580615]\n",
      " [-0.34926347]]\n",
      "t [[ 0.1275739 ]\n",
      " [-0.75173186]\n",
      " [-0.42921552]\n",
      " ...\n",
      " [ 0.29499046]\n",
      " [ 0.1275739 ]\n",
      " [-0.55385712]]\n",
      "t [[ 0.1275739 ]\n",
      " [-0.75173186]\n",
      " [-0.42921552]\n",
      " ...\n",
      " [ 0.29499046]\n",
      " [ 0.1275739 ]\n",
      " [-0.55385712]]\n",
      "Current iteration=2, loss=42877.93621505919\n",
      "t [[ 0.14606721]\n",
      " [-1.03357883]\n",
      " [-0.5849992 ]\n",
      " ...\n",
      " [ 0.3334317 ]\n",
      " [ 0.14606721]\n",
      " [-0.69768135]]\n",
      "t [[ 0.14606721]\n",
      " [-1.03357883]\n",
      " [-0.5849992 ]\n",
      " ...\n",
      " [ 0.3334317 ]\n",
      " [ 0.14606721]\n",
      " [-0.69768135]]\n",
      "t [[ 0.15134107]\n",
      " [-1.26903793]\n",
      " [-0.71832351]\n",
      " ...\n",
      " [ 0.34722798]\n",
      " [ 0.15134107]\n",
      " [-0.8114443 ]]\n",
      "t [[ 0.15134107]\n",
      " [-1.26903793]\n",
      " [-0.71832351]\n",
      " ...\n",
      " [ 0.34722798]\n",
      " [ 0.15134107]\n",
      " [-0.8114443 ]]\n",
      "Current iteration=4, loss=38571.27964199665\n",
      "t [[ 0.14873101]\n",
      " [-1.46843989]\n",
      " [-0.83485848]\n",
      " ...\n",
      " [ 0.34774223]\n",
      " [ 0.14873101]\n",
      " [-0.90744044]]\n",
      "t [[ 0.14873101]\n",
      " [-1.46843989]\n",
      " [-0.83485848]\n",
      " ...\n",
      " [ 0.34774223]\n",
      " [ 0.14873101]\n",
      " [-0.90744044]]\n",
      "t [[ 0.14132312]\n",
      " [-1.63959395]\n",
      " [-0.93820488]\n",
      " ...\n",
      " [ 0.34054952]\n",
      " [ 0.14132312]\n",
      " [-0.99129014]]\n",
      "t [[ 0.14132312]\n",
      " [-1.63959395]\n",
      " [-0.93820488]\n",
      " ...\n",
      " [ 0.34054952]\n",
      " [ 0.14132312]\n",
      " [-0.99129014]]\n",
      "Current iteration=6, loss=36069.574486468046\n",
      "t [[ 0.13101651]\n",
      " [-1.78829909]\n",
      " [-1.03084642]\n",
      " ...\n",
      " [ 0.32868708]\n",
      " [ 0.13101651]\n",
      " [-1.06597224]]\n",
      "t [[ 0.13101651]\n",
      " [-1.78829909]\n",
      " [-1.03084642]\n",
      " ...\n",
      " [ 0.32868708]\n",
      " [ 0.13101651]\n",
      " [-1.06597224]]\n",
      "t [[ 0.11902974]\n",
      " [-1.91888371]\n",
      " [-1.11459586]\n",
      " ...\n",
      " [ 0.31395589]\n",
      " [ 0.11902974]\n",
      " [-1.1332959 ]]\n",
      "t [[ 0.11902974]\n",
      " [-1.91888371]\n",
      " [-1.11459586]\n",
      " ...\n",
      " [ 0.31395589]\n",
      " [ 0.11902974]\n",
      " [-1.1332959 ]]\n",
      "Current iteration=8, loss=34449.57217601515\n",
      "t [[ 0.10616574]\n",
      " [-2.03462177]\n",
      " [-1.1908295 ]\n",
      " ...\n",
      " [ 0.29749072]\n",
      " [ 0.10616574]\n",
      " [-1.19448461]]\n",
      "t [[ 0.10616574]\n",
      " [-2.03462177]\n",
      " [-1.1908295 ]\n",
      " ...\n",
      " [ 0.29749072]\n",
      " [ 0.10616574]\n",
      " [-1.19448461]]\n",
      "t [[ 0.09296144]\n",
      " [-2.1380281 ]\n",
      " [-1.26062281]\n",
      " ...\n",
      " [ 0.28003572]\n",
      " [ 0.09296144]\n",
      " [-1.2504319 ]]\n",
      "loss=33324.51650310999\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.25911545]\n",
      " [-0.15105392]\n",
      " [-0.2417479 ]\n",
      " ...\n",
      " [ 0.2030253 ]\n",
      " [ 0.08546926]\n",
      " [-0.34398513]]\n",
      "t [[ 0.25911545]\n",
      " [-0.15105392]\n",
      " [-0.2417479 ]\n",
      " ...\n",
      " [ 0.2030253 ]\n",
      " [ 0.08546926]\n",
      " [-0.34398513]]\n",
      "t [[ 0.43853904]\n",
      " [-0.36076133]\n",
      " [-0.43026319]\n",
      " ...\n",
      " [ 0.28978766]\n",
      " [ 0.1267582 ]\n",
      " [-0.54339239]]\n",
      "t [[ 0.43853904]\n",
      " [-0.36076133]\n",
      " [-0.43026319]\n",
      " ...\n",
      " [ 0.28978766]\n",
      " [ 0.1267582 ]\n",
      " [-0.54339239]]\n",
      "Current iteration=2, loss=42864.791709412806\n",
      "t [[ 0.56775531]\n",
      " [-0.56694209]\n",
      " [-0.58604432]\n",
      " ...\n",
      " [ 0.3266898 ]\n",
      " [ 0.14475219]\n",
      " [-0.68268996]]\n",
      "t [[ 0.56775531]\n",
      " [-0.56694209]\n",
      " [-0.58604432]\n",
      " ...\n",
      " [ 0.3266898 ]\n",
      " [ 0.14475219]\n",
      " [-0.68268996]]\n",
      "t [[ 0.66383747]\n",
      " [-0.75420468]\n",
      " [-0.71925589]\n",
      " ...\n",
      " [ 0.33942579]\n",
      " [ 0.14954876]\n",
      " [-0.79263593]]\n",
      "t [[ 0.66383747]\n",
      " [-0.75420468]\n",
      " [-0.71925589]\n",
      " ...\n",
      " [ 0.33942579]\n",
      " [ 0.14954876]\n",
      " [-0.79263593]]\n",
      "Current iteration=4, loss=38558.53055172592\n",
      "t [[ 0.73707455]\n",
      " [-0.9211357 ]\n",
      " [-0.83563028]\n",
      " ...\n",
      " [ 0.33921798]\n",
      " [ 0.14649771]\n",
      " [-0.88543044]]\n",
      "t [[ 0.73707455]\n",
      " [-0.9211357 ]\n",
      " [-0.83563028]\n",
      " ...\n",
      " [ 0.33921798]\n",
      " [ 0.14649771]\n",
      " [-0.88543044]]\n",
      "t [[ 0.79406071]\n",
      " [-1.06984995]\n",
      " [-0.9387973 ]\n",
      " ...\n",
      " [ 0.33154114]\n",
      " [ 0.13868869]\n",
      " [-0.96658069]]\n",
      "t [[ 0.79406071]\n",
      " [-1.06984995]\n",
      " [-0.9387973 ]\n",
      " ...\n",
      " [ 0.33154114]\n",
      " [ 0.13868869]\n",
      " [-0.96658069]]\n",
      "Current iteration=6, loss=36057.310740828034\n",
      "t [[ 0.83921268]\n",
      " [-1.20299597]\n",
      " [-1.03125463]\n",
      " ...\n",
      " [ 0.3193644 ]\n",
      " [ 0.12802017]\n",
      " [-1.03896741]]\n",
      "t [[ 0.83921268]\n",
      " [-1.20299597]\n",
      " [-1.03125463]\n",
      " ...\n",
      " [ 0.3193644 ]\n",
      " [ 0.12802017]\n",
      " [-1.03896741]]\n",
      "t [[ 0.87558826]\n",
      " [-1.3229699 ]\n",
      " [-1.11482194]\n",
      " ...\n",
      " [ 0.30444283]\n",
      " [ 0.11570848]\n",
      " [-1.10432179]]\n",
      "t [[ 0.87558826]\n",
      " [-1.3229699 ]\n",
      " [-1.11482194]\n",
      " ...\n",
      " [ 0.30444283]\n",
      " [ 0.11570848]\n",
      " [-1.10432179]]\n",
      "Current iteration=8, loss=34437.27003379026\n",
      "t [[ 0.90535971]\n",
      " [-1.43177019]\n",
      " [-1.19087897]\n",
      " ...\n",
      " [ 0.2878799 ]\n",
      " [ 0.1025538 ]\n",
      " [-1.16380683]]\n",
      "t [[ 0.90535971]\n",
      " [-1.43177019]\n",
      " [-1.19087897]\n",
      " ...\n",
      " [ 0.2878799 ]\n",
      " [ 0.1025538 ]\n",
      " [-1.16380683]]\n",
      "t [[ 0.93010198]\n",
      " [-1.5310265 ]\n",
      " [-1.26050292]\n",
      " ...\n",
      " [ 0.27039799]\n",
      " [ 0.08909019]\n",
      " [-1.21826916]]\n",
      "loss=33311.79092187097\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.25579437]\n",
      " [-0.14460201]\n",
      " [-0.24084252]\n",
      " ...\n",
      " [ 0.20615771]\n",
      " [ 0.08704454]\n",
      " [-0.35457816]]\n",
      "t [[ 0.25579437]\n",
      " [-0.14460201]\n",
      " [-0.24084252]\n",
      " ...\n",
      " [ 0.20615771]\n",
      " [ 0.08704454]\n",
      " [-0.35457816]]\n",
      "t [[ 0.43294905]\n",
      " [-0.3481239 ]\n",
      " [-0.42846492]\n",
      " ...\n",
      " [ 0.29298482]\n",
      " [ 0.12938801]\n",
      " [-0.56028826]]\n",
      "t [[ 0.43294905]\n",
      " [-0.3481239 ]\n",
      " [-0.42846492]\n",
      " ...\n",
      " [ 0.29298482]\n",
      " [ 0.12938801]\n",
      " [-0.56028826]]\n",
      "Current iteration=2, loss=42949.48728737853\n",
      "t [[ 0.56062129]\n",
      " [-0.5481251 ]\n",
      " [-0.58347552]\n",
      " ...\n",
      " [ 0.32877327]\n",
      " [ 0.14818685]\n",
      " [-0.70388246]]\n",
      "t [[ 0.56062129]\n",
      " [-0.5481251 ]\n",
      " [-0.58347552]\n",
      " ...\n",
      " [ 0.32877327]\n",
      " [ 0.14818685]\n",
      " [-0.70388246]]\n",
      "t [[ 0.65566467]\n",
      " [-0.72935642]\n",
      " [-0.71602379]\n",
      " ...\n",
      " [ 0.33980747]\n",
      " [ 0.15363584]\n",
      " [-0.81696425]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.65566467]\n",
      " [-0.72935642]\n",
      " [-0.71602379]\n",
      " ...\n",
      " [ 0.33980747]\n",
      " [ 0.15363584]\n",
      " [-0.81696425]]\n",
      "Current iteration=4, loss=38701.1220447103\n",
      "t [[ 0.72820271]\n",
      " [-0.89051909]\n",
      " [-0.8318156 ]\n",
      " ...\n",
      " [ 0.33759367]\n",
      " [ 0.15113076]\n",
      " [-0.91214906]]\n",
      "t [[ 0.72820271]\n",
      " [-0.89051909]\n",
      " [-0.8318156 ]\n",
      " ...\n",
      " [ 0.33759367]\n",
      " [ 0.15113076]\n",
      " [-0.91214906]]\n",
      "t [[ 0.78471594]\n",
      " [-1.03378003]\n",
      " [-0.9344596 ]\n",
      " ...\n",
      " [ 0.32776293]\n",
      " [ 0.14378821]\n",
      " [-0.9951779 ]]\n",
      "t [[ 0.78471594]\n",
      " [-1.03378003]\n",
      " [-0.9344596 ]\n",
      " ...\n",
      " [ 0.32776293]\n",
      " [ 0.14378821]\n",
      " [-0.9951779 ]]\n",
      "Current iteration=6, loss=36236.80049539655\n",
      "t [[ 0.82954612]\n",
      " [-1.16179987]\n",
      " [-1.02643803]\n",
      " ...\n",
      " [ 0.31337193]\n",
      " [ 0.13352441]\n",
      " [-1.06907284]]\n",
      "t [[ 0.82954612]\n",
      " [-1.16179987]\n",
      " [-1.02643803]\n",
      " ...\n",
      " [ 0.31337193]\n",
      " [ 0.13352441]\n",
      " [-1.06907284]]\n",
      "t [[ 0.86570161]\n",
      " [-1.27696639]\n",
      " [-1.10955958]\n",
      " ...\n",
      " [ 0.29622614]\n",
      " [ 0.12156826]\n",
      " [-1.1356561 ]]\n",
      "t [[ 0.86570161]\n",
      " [-1.27696639]\n",
      " [-1.10955958]\n",
      " ...\n",
      " [ 0.29622614]\n",
      " [ 0.12156826]\n",
      " [-1.1356561 ]]\n",
      "Current iteration=8, loss=34641.86264879927\n",
      "t [[ 0.89532157]\n",
      " [-1.38126052]\n",
      " [-1.18519616]\n",
      " ...\n",
      " [ 0.27745883]\n",
      " [ 0.10872923]\n",
      " [-1.19615264]]\n",
      "t [[ 0.89532157]\n",
      " [-1.38126052]\n",
      " [-1.18519616]\n",
      " ...\n",
      " [ 0.27745883]\n",
      " [ 0.10872923]\n",
      " [-1.19615264]]\n",
      "t [[ 0.91995846]\n",
      " [-1.47629098]\n",
      " [-1.25441931]\n",
      " ...\n",
      " [ 0.25781028]\n",
      " [ 0.09554853]\n",
      " [-1.2514532 ]]\n",
      "loss=33534.36946819996\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.25538491]\n",
      " [-0.15191884]\n",
      " [-0.24008602]\n",
      " ...\n",
      " [ 0.36783458]\n",
      " [-0.24008602]\n",
      " [ 0.34452919]]\n",
      "t [[ 0.25538491]\n",
      " [-0.15191884]\n",
      " [-0.24008602]\n",
      " ...\n",
      " [ 0.36783458]\n",
      " [-0.24008602]\n",
      " [ 0.34452919]]\n",
      "t [[ 0.4321981 ]\n",
      " [-0.35960778]\n",
      " [-0.4274038 ]\n",
      " ...\n",
      " [ 0.60847668]\n",
      " [-0.4274038 ]\n",
      " [ 0.5493866 ]]\n",
      "t [[ 0.4321981 ]\n",
      " [-0.35960778]\n",
      " [-0.4274038 ]\n",
      " ...\n",
      " [ 0.60847668]\n",
      " [-0.4274038 ]\n",
      " [ 0.5493866 ]]\n",
      "Current iteration=2, loss=42949.29952224024\n",
      "t [[ 0.55950352]\n",
      " [-0.56284763]\n",
      " [-0.58222403]\n",
      " ...\n",
      " [ 0.77613034]\n",
      " [-0.58222403]\n",
      " [ 0.6818214 ]]\n",
      "t [[ 0.55950352]\n",
      " [-0.56284763]\n",
      " [-0.58222403]\n",
      " ...\n",
      " [ 0.77613034]\n",
      " [-0.58222403]\n",
      " [ 0.6818214 ]]\n",
      "t [[ 0.65401618]\n",
      " [-0.74707868]\n",
      " [-0.71461788]\n",
      " ...\n",
      " [ 0.89814542]\n",
      " [-0.71461788]\n",
      " [ 0.77252006]]\n",
      "t [[ 0.65401618]\n",
      " [-0.74707868]\n",
      " [-0.71461788]\n",
      " ...\n",
      " [ 0.89814542]\n",
      " [-0.71461788]\n",
      " [ 0.77252006]]\n",
      "Current iteration=4, loss=38687.43455437351\n",
      "t [[ 0.72590565]\n",
      " [-0.91112186]\n",
      " [-0.83026724]\n",
      " ...\n",
      " [ 0.98971879]\n",
      " [-0.83026724]\n",
      " [ 0.83706824]]\n",
      "t [[ 0.72590565]\n",
      " [-0.91112186]\n",
      " [-0.83026724]\n",
      " ...\n",
      " [ 0.98971879]\n",
      " [-0.83026724]\n",
      " [ 0.83706824]]\n",
      "t [[ 0.78170487]\n",
      " [-1.05714214]\n",
      " [-0.93277538]\n",
      " ...\n",
      " [ 1.06007555]\n",
      " [-0.93277538]\n",
      " [ 0.8842897 ]]\n",
      "t [[ 0.78170487]\n",
      " [-1.05714214]\n",
      " [-0.93277538]\n",
      " ...\n",
      " [ 1.06007555]\n",
      " [-0.93277538]\n",
      " [ 0.8842897 ]]\n",
      "Current iteration=6, loss=36213.89612244697\n",
      "t [[ 0.8257902 ]\n",
      " [-1.18778454]\n",
      " [-1.02462436]\n",
      " ...\n",
      " [ 1.11518604]\n",
      " [-1.02462436]\n",
      " [ 0.9196038 ]]\n",
      "t [[ 0.8257902 ]\n",
      " [-1.18778454]\n",
      " [-1.02462436]\n",
      " ...\n",
      " [ 1.11518604]\n",
      " [-1.02462436]\n",
      " [ 0.9196038 ]]\n",
      "t [[ 0.8611909 ]\n",
      " [-1.3054283 ]\n",
      " [-1.10762347]\n",
      " ...\n",
      " [ 1.15909518]\n",
      " [-1.10762347]\n",
      " [ 0.9465367 ]]\n",
      "t [[ 0.8611909 ]\n",
      " [-1.3054283 ]\n",
      " [-1.10762347]\n",
      " ...\n",
      " [ 1.15909518]\n",
      " [-1.10762347]\n",
      " [ 0.9465367 ]]\n",
      "Current iteration=8, loss=34612.717195713514\n",
      "t [[ 0.89005854]\n",
      " [-1.41205383]\n",
      " [-1.1831449 ]\n",
      " ...\n",
      " [ 1.19463586]\n",
      " [-1.1831449 ]\n",
      " [ 0.96747713]]\n",
      "t [[ 0.89005854]\n",
      " [-1.41205383]\n",
      " [-1.1831449 ]\n",
      " ...\n",
      " [ 1.19463586]\n",
      " [-1.1831449 ]\n",
      " [ 0.96747713]]\n",
      "t [[ 0.91395278]\n",
      " [-1.50927485]\n",
      " [-1.2522601 ]\n",
      " ...\n",
      " [ 1.22384162]\n",
      " [-1.2522601 ]\n",
      " [ 0.98408971]]\n",
      "loss=33501.03359271672\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0876715 ]\n",
      " [-0.41981478]\n",
      " [-0.24618421]\n",
      " ...\n",
      " [ 0.21052018]\n",
      " [ 0.0876715 ]\n",
      " [-0.35685615]]\n",
      "t [[ 0.0876715 ]\n",
      " [-0.41981478]\n",
      " [-0.24618421]\n",
      " ...\n",
      " [ 0.21052018]\n",
      " [ 0.0876715 ]\n",
      " [-0.35685615]]\n",
      "t [[ 0.12939933]\n",
      " [-0.76654932]\n",
      " [-0.43740796]\n",
      " ...\n",
      " [ 0.29887947]\n",
      " [ 0.12939933]\n",
      " [-0.56278472]]\n",
      "t [[ 0.12939933]\n",
      " [-0.76654932]\n",
      " [-0.43740796]\n",
      " ...\n",
      " [ 0.29887947]\n",
      " [ 0.12939933]\n",
      " [-0.56278472]]\n",
      "Current iteration=2, loss=42738.959936482184\n",
      "t [[ 0.14730302]\n",
      " [-1.05199395]\n",
      " [-0.59518682]\n",
      " ...\n",
      " [ 0.33598381]\n",
      " [ 0.14730302]\n",
      " [-0.70713189]]\n",
      "t [[ 0.14730302]\n",
      " [-1.05199395]\n",
      " [-0.59518682]\n",
      " ...\n",
      " [ 0.33598381]\n",
      " [ 0.14730302]\n",
      " [-0.70713189]]\n",
      "t [[ 0.15184395]\n",
      " [-1.28959541]\n",
      " [-0.72996458]\n",
      " ...\n",
      " [ 0.34848997]\n",
      " [ 0.15184395]\n",
      " [-0.82142255]]\n",
      "t [[ 0.15184395]\n",
      " [-1.28959541]\n",
      " [-0.72996458]\n",
      " ...\n",
      " [ 0.34848997]\n",
      " [ 0.15184395]\n",
      " [-0.82142255]]\n",
      "Current iteration=4, loss=38423.56660973492\n",
      "t [[ 0.14849973]\n",
      " [-1.49024082]\n",
      " [-0.84758895]\n",
      " ...\n",
      " [ 0.34786933]\n",
      " [ 0.14849973]\n",
      " [-0.91796594]]\n",
      "t [[ 0.14849973]\n",
      " [-1.49024082]\n",
      " [-0.84758895]\n",
      " ...\n",
      " [ 0.34786933]\n",
      " [ 0.14849973]\n",
      " [-0.91796594]]\n",
      "t [[ 0.14041257]\n",
      " [-1.66207737]\n",
      " [-0.95176023]\n",
      " ...\n",
      " [ 0.33968658]\n",
      " [ 0.14041257]\n",
      " [-1.00232137]]\n",
      "t [[ 0.14041257]\n",
      " [-1.66207737]\n",
      " [-0.95176023]\n",
      " ...\n",
      " [ 0.33968658]\n",
      " [ 0.14041257]\n",
      " [-1.00232137]]\n",
      "Current iteration=6, loss=35933.88415949715\n",
      "t [[ 0.12950385]\n",
      " [-1.81110735]\n",
      " [-1.04502728]\n",
      " ...\n",
      " [ 0.32695875]\n",
      " [ 0.12950385]\n",
      " [-1.07743404]]\n",
      "t [[ 0.12950385]\n",
      " [-1.81110735]\n",
      " [-1.04502728]\n",
      " ...\n",
      " [ 0.32695875]\n",
      " [ 0.12950385]\n",
      " [-1.07743404]]\n",
      "t [[ 0.11699895]\n",
      " [-1.94178382]\n",
      " [-1.12924871]\n",
      " ...\n",
      " [ 0.31147098]\n",
      " [ 0.11699895]\n",
      " [-1.14510368]]\n",
      "t [[ 0.11699895]\n",
      " [-1.94178382]\n",
      " [-1.12924871]\n",
      " ...\n",
      " [ 0.31147098]\n",
      " [ 0.11699895]\n",
      " [-1.14510368]]\n",
      "Current iteration=8, loss=34328.6367707679\n",
      "t [[ 0.10369991]\n",
      " [-2.05745918]\n",
      " [-1.20583425]\n",
      " ...\n",
      " [ 0.29434566]\n",
      " [ 0.10369991]\n",
      " [-1.20655667]]\n",
      "t [[ 0.10369991]\n",
      " [-2.05745918]\n",
      " [-1.20583425]\n",
      " ...\n",
      " [ 0.29434566]\n",
      " [ 0.10369991]\n",
      " [-1.20655667]]\n",
      "t [[ 0.09013882]\n",
      " [-2.16069883]\n",
      " [-1.27588429]\n",
      " ...\n",
      " [ 0.27631637]\n",
      " [ 0.09013882]\n",
      " [-1.26269436]]\n",
      "loss=33217.3390332931\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.26474839]\n",
      " [-0.1543377 ]\n",
      " [-0.24700329]\n",
      " ...\n",
      " [ 0.20743889]\n",
      " [ 0.08732729]\n",
      " [-0.35146307]]\n",
      "t [[ 0.26474839]\n",
      " [-0.1543377 ]\n",
      " [-0.24700329]\n",
      " ...\n",
      " [ 0.20743889]\n",
      " [ 0.08732729]\n",
      " [-0.35146307]]\n",
      "t [[ 0.44635817]\n",
      " [-0.36985903]\n",
      " [-0.43846668]\n",
      " ...\n",
      " [ 0.29358117]\n",
      " [ 0.128563  ]\n",
      " [-0.55209508]]\n",
      "t [[ 0.44635817]\n",
      " [-0.36985903]\n",
      " [-0.43846668]\n",
      " ...\n",
      " [ 0.29358117]\n",
      " [ 0.128563  ]\n",
      " [-0.55209508]]\n",
      "Current iteration=2, loss=42725.778229088646\n",
      "t [[ 0.57624905]\n",
      " [-0.58038054]\n",
      " [-0.59623243]\n",
      " ...\n",
      " [ 0.32914078]\n",
      " [ 0.14595556]\n",
      " [-0.69184549]]\n",
      "t [[ 0.57624905]\n",
      " [-0.58038054]\n",
      " [-0.59623243]\n",
      " ...\n",
      " [ 0.32914078]\n",
      " [ 0.14595556]\n",
      " [-0.69184549]]\n",
      "t [[ 0.67229645]\n",
      " [-0.77052923]\n",
      " [-0.73088788]\n",
      " ...\n",
      " [ 0.34059415]\n",
      " [ 0.15001008]\n",
      " [-0.80228134]]\n",
      "t [[ 0.67229645]\n",
      " [-0.77052923]\n",
      " [-0.73088788]\n",
      " ...\n",
      " [ 0.34059415]\n",
      " [ 0.15001008]\n",
      " [-0.80228134]]\n",
      "Current iteration=4, loss=38410.85196270191\n",
      "t [[ 0.74516301]\n",
      " [-0.93936913]\n",
      " [-0.84834395]\n",
      " ...\n",
      " [ 0.3392646 ]\n",
      " [ 0.14621829]\n",
      " [-0.89560583]]\n",
      "t [[ 0.74516301]\n",
      " [-0.93936913]\n",
      " [-0.84834395]\n",
      " ...\n",
      " [ 0.3392646 ]\n",
      " [ 0.14621829]\n",
      " [-0.89560583]]\n",
      "t [[ 0.80163405]\n",
      " [-1.08937061]\n",
      " [-0.95232979]\n",
      " ...\n",
      " [ 0.33061275]\n",
      " [ 0.1377255 ]\n",
      " [-0.97725681]]\n",
      "t [[ 0.80163405]\n",
      " [-1.08937061]\n",
      " [-0.95232979]\n",
      " ...\n",
      " [ 0.33061275]\n",
      " [ 0.1377255 ]\n",
      " [-0.97725681]]\n",
      "Current iteration=6, loss=35921.64093001415\n",
      "t [[ 0.84622804]\n",
      " [-1.22340122]\n",
      " [-1.04540786]\n",
      " ...\n",
      " [ 0.31758581]\n",
      " [ 0.12645206]\n",
      " [-1.05007629]]\n",
      "t [[ 0.84622804]\n",
      " [-1.22340122]\n",
      " [-1.04540786]\n",
      " ...\n",
      " [ 0.31758581]\n",
      " [ 0.12645206]\n",
      " [-1.05007629]]\n",
      "t [[ 0.88205734]\n",
      " [-1.34398907]\n",
      " [-1.1294434 ]\n",
      " ...\n",
      " [ 0.30192239]\n",
      " [ 0.11362081]\n",
      " [-1.11578312]]\n",
      "t [[ 0.88205734]\n",
      " [-1.34398907]\n",
      " [-1.1294434 ]\n",
      " ...\n",
      " [ 0.30192239]\n",
      " [ 0.11362081]\n",
      " [-1.11578312]]\n",
      "Current iteration=8, loss=34316.31722812872\n",
      "t [[ 0.91132257]\n",
      " [-1.45321422]\n",
      " [-1.20584937]\n",
      " ...\n",
      " [ 0.28471339]\n",
      " [ 0.10003076]\n",
      " [-1.17554147]]\n",
      "t [[ 0.91132257]\n",
      " [-1.45321422]\n",
      " [-1.20584937]\n",
      " ...\n",
      " [ 0.28471339]\n",
      " [ 0.10003076]\n",
      " [-1.17554147]]\n",
      "t [[ 0.93561204]\n",
      " [-1.55275894]\n",
      " [-1.27572771]\n",
      " ...\n",
      " [ 0.26667065]\n",
      " [ 0.08621091]\n",
      " [-1.23020473]]\n",
      "loss=33204.55640471365\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.26135512]\n",
      " [-0.14774553]\n",
      " [-0.24607823]\n",
      " ...\n",
      " [ 0.2106394 ]\n",
      " [ 0.08893682]\n",
      " [-0.36228638]]\n",
      "t [[ 0.26135512]\n",
      " [-0.14774553]\n",
      " [-0.24607823]\n",
      " ...\n",
      " [ 0.2106394 ]\n",
      " [ 0.08893682]\n",
      " [-0.36228638]]\n",
      "t [[ 0.44066917]\n",
      " [-0.35695242]\n",
      " [-0.4366301 ]\n",
      " ...\n",
      " [ 0.29678608]\n",
      " [ 0.13123929]\n",
      " [-0.56926943]]\n",
      "t [[ 0.44066917]\n",
      " [-0.35695242]\n",
      " [-0.4366301 ]\n",
      " ...\n",
      " [ 0.29678608]\n",
      " [ 0.13123929]\n",
      " [-0.56926943]]\n",
      "Current iteration=2, loss=42812.15144573647\n",
      "t [[ 0.56901354]\n",
      " [-0.56116047]\n",
      " [-0.59361375]\n",
      " ...\n",
      " [ 0.33115639]\n",
      " [ 0.14944356]\n",
      " [-0.71332485]]\n",
      "t [[ 0.56901354]\n",
      " [-0.56116047]\n",
      " [-0.59361375]\n",
      " ...\n",
      " [ 0.33115639]\n",
      " [ 0.14944356]\n",
      " [-0.71332485]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.66403127]\n",
      " [-0.74515581]\n",
      " [-0.72759804]\n",
      " ...\n",
      " [ 0.34083166]\n",
      " [ 0.15415462]\n",
      " [-0.82689015]]\n",
      "t [[ 0.66403127]\n",
      " [-0.74515581]\n",
      " [-0.72759804]\n",
      " ...\n",
      " [ 0.34083166]\n",
      " [ 0.15415462]\n",
      " [-0.82689015]]\n",
      "Current iteration=4, loss=38555.53298278329\n",
      "t [[ 0.7362117 ]\n",
      " [-0.90812334]\n",
      " [-0.84446545]\n",
      " ...\n",
      " [ 0.33742541]\n",
      " [ 0.15091121]\n",
      " [-0.9225928 ]]\n",
      "t [[ 0.7362117 ]\n",
      " [-0.90812334]\n",
      " [-0.84446545]\n",
      " ...\n",
      " [ 0.33742541]\n",
      " [ 0.15091121]\n",
      " [-0.9225928 ]]\n",
      "t [[ 0.79222292]\n",
      " [-1.05258526]\n",
      " [-0.94792294]\n",
      " ...\n",
      " [ 0.32655603]\n",
      " [ 0.14288617]\n",
      " [-1.00610779]]\n",
      "t [[ 0.79222292]\n",
      " [-1.05258526]\n",
      " [-0.94792294]\n",
      " ...\n",
      " [ 0.32655603]\n",
      " [ 0.14288617]\n",
      " [-1.00610779]]\n",
      "Current iteration=6, loss=36103.19232210803\n",
      "t [[ 0.83650703]\n",
      " [-1.18141897]\n",
      " [-1.0405171 ]\n",
      " ...\n",
      " [ 0.31125804]\n",
      " [ 0.13201802]\n",
      " [-1.08041977]]\n",
      "t [[ 0.83650703]\n",
      " [-1.18141897]\n",
      " [-1.0405171 ]\n",
      " ...\n",
      " [ 0.31125804]\n",
      " [ 0.13201802]\n",
      " [-1.08041977]]\n",
      "t [[ 0.87212643]\n",
      " [-1.29714169]\n",
      " [-1.1241019 ]\n",
      " ...\n",
      " [ 0.29331978]\n",
      " [ 0.1195424 ]\n",
      " [-1.14733923]]\n",
      "t [[ 0.87212643]\n",
      " [-1.29714169]\n",
      " [-1.1241019 ]\n",
      " ...\n",
      " [ 0.29331978]\n",
      " [ 0.1195424 ]\n",
      " [-1.14733923]]\n",
      "Current iteration=8, loss=34522.827715014486\n",
      "t [[ 0.90124853]\n",
      " [-1.40181371]\n",
      " [-1.20008238]\n",
      " ...\n",
      " [ 0.27386124]\n",
      " [ 0.10626782]\n",
      " [-1.20809264]]\n",
      "t [[ 0.90124853]\n",
      " [-1.40181371]\n",
      " [-1.20008238]\n",
      " ...\n",
      " [ 0.27386124]\n",
      " [ 0.10626782]\n",
      " [-1.20809264]]\n",
      "t [[ 0.92543922]\n",
      " [-1.49709448]\n",
      " [-1.26955478]\n",
      " ...\n",
      " [ 0.25361152]\n",
      " [ 0.09273055]\n",
      " [-1.26357775]]\n",
      "loss=33428.883088781695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.26093675]\n",
      " [-0.15522143]\n",
      " [-0.24530528]\n",
      " ...\n",
      " [ 0.37583098]\n",
      " [-0.24530528]\n",
      " [ 0.35201896]]\n",
      "t [[ 0.26093675]\n",
      " [-0.15522143]\n",
      " [-0.24530528]\n",
      " ...\n",
      " [ 0.37583098]\n",
      " [-0.24530528]\n",
      " [ 0.35201896]]\n",
      "t [[ 0.43990024]\n",
      " [-0.36862149]\n",
      " [-0.43555515]\n",
      " ...\n",
      " [ 0.61896361]\n",
      " [-0.43555515]\n",
      " [ 0.55832324]]\n",
      "t [[ 0.43990024]\n",
      " [-0.36862149]\n",
      " [-0.43555515]\n",
      " ...\n",
      " [ 0.61896361]\n",
      " [-0.43555515]\n",
      " [ 0.55832324]]\n",
      "Current iteration=2, loss=42811.651141650524\n",
      "t [[ 0.56786173]\n",
      " [-0.57610564]\n",
      " [-0.59234918]\n",
      " ...\n",
      " [ 0.78714711]\n",
      " [-0.59234918]\n",
      " [ 0.69054724]]\n",
      "t [[ 0.56786173]\n",
      " [-0.57610564]\n",
      " [-0.59234918]\n",
      " ...\n",
      " [ 0.78714711]\n",
      " [-0.59234918]\n",
      " [ 0.69054724]]\n",
      "t [[ 0.66232667]\n",
      " [-0.76315209]\n",
      " [-0.72617787]\n",
      " ...\n",
      " [ 0.90888607]\n",
      " [-0.72617787]\n",
      " [ 0.78053756]]\n",
      "t [[ 0.66232667]\n",
      " [-0.76315209]\n",
      " [-0.72617787]\n",
      " ...\n",
      " [ 0.90888607]\n",
      " [-0.72617787]\n",
      " [ 0.78053756]]\n",
      "Current iteration=4, loss=38541.34765630894\n",
      "t [[ 0.73383629]\n",
      " [-0.92905241]\n",
      " [-0.84290074]\n",
      " ...\n",
      " [ 0.999835  ]\n",
      " [-0.84290074]\n",
      " [ 0.84424111]]\n",
      "t [[ 0.73383629]\n",
      " [-0.92905241]\n",
      " [-0.84290074]\n",
      " ...\n",
      " [ 0.999835  ]\n",
      " [-0.84290074]\n",
      " [ 0.84424111]]\n",
      "t [[ 0.78911299]\n",
      " [-1.07632025]\n",
      " [-0.94622023]\n",
      " ...\n",
      " [ 1.06943315]\n",
      " [-0.94622023]\n",
      " [ 0.89061997]]\n",
      "t [[ 0.78911299]\n",
      " [-1.07632025]\n",
      " [-0.94622023]\n",
      " ...\n",
      " [ 1.06943315]\n",
      " [-0.94622023]\n",
      " [ 0.89061997]]\n",
      "Current iteration=6, loss=36079.77444248475\n",
      "t [[ 0.83263381]\n",
      " [-1.20781545]\n",
      " [-1.03868309]\n",
      " ...\n",
      " [ 1.12376071]\n",
      " [-1.03868309]\n",
      " [ 0.92515425]]\n",
      "t [[ 0.83263381]\n",
      " [-1.20781545]\n",
      " [-1.03868309]\n",
      " ...\n",
      " [ 1.12376071]\n",
      " [-1.03868309]\n",
      " [ 0.92515425]]\n",
      "t [[ 0.86748193]\n",
      " [-1.32604687]\n",
      " [-1.12214398]\n",
      " ...\n",
      " [ 1.16692035]\n",
      " [-1.12214398]\n",
      " [ 0.95139698]]\n",
      "t [[ 0.86748193]\n",
      " [-1.32604687]\n",
      " [-1.12214398]\n",
      " ...\n",
      " [ 1.16692035]\n",
      " [-1.12214398]\n",
      " [ 0.95139698]]\n",
      "Current iteration=8, loss=34493.21547220261\n",
      "t [[ 0.8958369 ]\n",
      " [-1.43307534]\n",
      " [-1.19800819]\n",
      " ...\n",
      " [ 1.2017742 ]\n",
      " [-1.19800819]\n",
      " [ 0.97174636]]\n",
      "t [[ 0.8958369 ]\n",
      " [-1.43307534]\n",
      " [-1.19800819]\n",
      " ...\n",
      " [ 1.2017742 ]\n",
      " [-1.19800819]\n",
      " [ 0.97174636]]\n",
      "t [[ 0.91927156]\n",
      " [-1.53056644]\n",
      " [-1.26737182]\n",
      " ...\n",
      " [ 1.23036861]\n",
      " [-1.26737182]\n",
      " [ 0.98786686]]\n",
      "loss=33395.15708334788\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.08953685]\n",
      " [-0.42874701]\n",
      " [-0.25142217]\n",
      " ...\n",
      " [ 0.21499933]\n",
      " [ 0.08953685]\n",
      " [-0.36444883]]\n",
      "t [[ 0.08953685]\n",
      " [-0.42874701]\n",
      " [-0.25142217]\n",
      " ...\n",
      " [ 0.21499933]\n",
      " [ 0.08953685]\n",
      " [-0.36444883]]\n",
      "t [[ 0.13118633]\n",
      " [-0.78130388]\n",
      " [-0.44555397]\n",
      " ...\n",
      " [ 0.30266605]\n",
      " [ 0.13118633]\n",
      " [-0.57158624]]\n",
      "t [[ 0.13118633]\n",
      " [-0.78130388]\n",
      " [-0.44555397]\n",
      " ...\n",
      " [ 0.30266605]\n",
      " [ 0.13118633]\n",
      " [-0.57158624]]\n",
      "Current iteration=2, loss=42601.82876344152\n",
      "t [[ 0.14847842]\n",
      " [-1.07025415]\n",
      " [-0.6052892 ]\n",
      " ...\n",
      " [ 0.33840276]\n",
      " [ 0.14847842]\n",
      " [-0.7164228 ]]\n",
      "t [[ 0.14847842]\n",
      " [-1.07025415]\n",
      " [-0.6052892 ]\n",
      " ...\n",
      " [ 0.33840276]\n",
      " [ 0.14847842]\n",
      " [-0.7164228 ]]\n",
      "t [[ 0.15227767]\n",
      " [-1.30990985]\n",
      " [-0.74148728]\n",
      " ...\n",
      " [ 0.3496214 ]\n",
      " [ 0.15227767]\n",
      " [-0.83124045]]\n",
      "t [[ 0.15227767]\n",
      " [-1.30990985]\n",
      " [-0.74148728]\n",
      " ...\n",
      " [ 0.3496214 ]\n",
      " [ 0.15227767]\n",
      " [-0.83124045]]\n",
      "Current iteration=4, loss=38279.12001243868\n",
      "t [[ 0.14819902]\n",
      " [-1.51172518]\n",
      " [-0.86017109]\n",
      " ...\n",
      " [ 0.34787833]\n",
      " [ 0.14819902]\n",
      " [-0.92833218]]\n",
      "t [[ 0.14819902]\n",
      " [-1.51172518]\n",
      " [-0.86017109]\n",
      " ...\n",
      " [ 0.34787833]\n",
      " [ 0.14819902]\n",
      " [-0.92833218]]\n",
      "t [[ 0.13943795]\n",
      " [-1.68418599]\n",
      " [-0.96513978]\n",
      " ...\n",
      " [ 0.33872032]\n",
      " [ 0.13943795]\n",
      " [-1.01318899]]\n",
      "t [[ 0.13943795]\n",
      " [-1.68418599]\n",
      " [-0.96513978]\n",
      " ...\n",
      " [ 0.33872032]\n",
      " [ 0.13943795]\n",
      " [-1.01318899]]\n",
      "Current iteration=6, loss=35801.909157238224\n",
      "t [[ 0.12793637]\n",
      " [-1.83349556]\n",
      " [-1.0590073 ]\n",
      " ...\n",
      " [ 0.32514243]\n",
      " [ 0.12793637]\n",
      " [-1.08872289]]\n",
      "t [[ 0.12793637]\n",
      " [-1.83349556]\n",
      " [-1.0590073 ]\n",
      " ...\n",
      " [ 0.32514243]\n",
      " [ 0.12793637]\n",
      " [-1.08872289]]\n",
      "t [[ 0.11492523]\n",
      " [-1.96422902]\n",
      " [-1.14367807]\n",
      " ...\n",
      " [ 0.30891372]\n",
      " [ 0.11492523]\n",
      " [-1.15672615]]\n",
      "t [[ 0.11492523]\n",
      " [-1.96422902]\n",
      " [-1.14367807]\n",
      " ...\n",
      " [ 0.30891372]\n",
      " [ 0.11492523]\n",
      " [-1.15672615]]\n",
      "Current iteration=8, loss=34211.449568715085\n",
      "t [[ 0.1012047 ]\n",
      " [-2.07981491]\n",
      " [-1.22059513]\n",
      " ...\n",
      " [ 0.29114412]\n",
      " [ 0.1012047 ]\n",
      " [-1.21842964]]\n",
      "t [[ 0.1012047 ]\n",
      " [-2.07981491]\n",
      " [-1.22059513]\n",
      " ...\n",
      " [ 0.29114412]\n",
      " [ 0.1012047 ]\n",
      " [-1.21842964]]\n",
      "t [[ 0.08730128]\n",
      " [-2.18286737]\n",
      " [-1.29088363]\n",
      " ...\n",
      " [ 0.27255648]\n",
      " [ 0.08730128]\n",
      " [-1.27474359]]\n",
      "loss=33113.77790075951\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.27038134]\n",
      " [-0.15762148]\n",
      " [-0.25225868]\n",
      " ...\n",
      " [ 0.21185249]\n",
      " [ 0.08918532]\n",
      " [-0.35894101]]\n",
      "t [[ 0.27038134]\n",
      " [-0.15762148]\n",
      " [-0.25225868]\n",
      " ...\n",
      " [ 0.21185249]\n",
      " [ 0.08918532]\n",
      " [-0.35894101]]\n",
      "t [[ 0.45410784]\n",
      " [-0.37900716]\n",
      " [-0.44662328]\n",
      " ...\n",
      " [ 0.29727296]\n",
      " [ 0.13032926]\n",
      " [-0.56067185]]\n",
      "t [[ 0.45410784]\n",
      " [-0.37900716]\n",
      " [-0.44662328]\n",
      " ...\n",
      " [ 0.29727296]\n",
      " [ 0.13032926]\n",
      " [-0.56067185]]\n",
      "Current iteration=2, loss=42588.61462100543\n",
      "t [[ 0.58461294]\n",
      " [-0.59381157]\n",
      " [-0.60633468]\n",
      " ...\n",
      " [ 0.33146028]\n",
      " [ 0.14709845]\n",
      " [-0.70084312]]\n",
      "t [[ 0.58461294]\n",
      " [-0.59381157]\n",
      " [-0.60633468]\n",
      " ...\n",
      " [ 0.33146028]\n",
      " [ 0.14709845]\n",
      " [-0.70084312]]\n",
      "t [[ 0.68058221]\n",
      " [-0.78675949]\n",
      " [-0.74240091]\n",
      " ...\n",
      " [ 0.34163442]\n",
      " [ 0.15040234]\n",
      " [-0.81177005]]\n",
      "t [[ 0.68058221]\n",
      " [-0.78675949]\n",
      " [-0.74240091]\n",
      " ...\n",
      " [ 0.34163442]\n",
      " [ 0.15040234]\n",
      " [-0.81177005]]\n",
      "Current iteration=4, loss=38266.44015168645\n",
      "t [[ 0.75305026]\n",
      " [-0.95742868]\n",
      " [-0.86090883]\n",
      " ...\n",
      " [ 0.33919607]\n",
      " [ 0.14586973]\n",
      " [-0.90562729]]\n",
      "t [[ 0.75305026]\n",
      " [-0.95742868]\n",
      " [-0.86090883]\n",
      " ...\n",
      " [ 0.33919607]\n",
      " [ 0.14586973]\n",
      " [-0.90562729]]\n",
      "t [[ 0.80899051]\n",
      " [-1.1086528 ]\n",
      " [-0.96568619]\n",
      " ...\n",
      " [ 0.32958417]\n",
      " [ 0.13669876]\n",
      " [-0.98777589]]\n",
      "t [[ 0.80899051]\n",
      " [-1.1086528 ]\n",
      " [-0.96568619]\n",
      " ...\n",
      " [ 0.32958417]\n",
      " [ 0.13669876]\n",
      " [-0.98777589]]\n",
      "Current iteration=6, loss=35789.684199339266\n",
      "t [[ 0.85302032]\n",
      " [-1.24351702]\n",
      " [-1.05936013]\n",
      " ...\n",
      " [ 0.31572243]\n",
      " [ 0.12482988]\n",
      " [-1.06101972]]\n",
      "t [[ 0.85302032]\n",
      " [-1.24351702]\n",
      " [-1.05936013]\n",
      " ...\n",
      " [ 0.31572243]\n",
      " [ 0.12482988]\n",
      " [-1.06101972]]\n",
      "t [[ 0.8883042 ]\n",
      " [-1.36467835]\n",
      " [-1.14384139]\n",
      " ...\n",
      " [ 0.29933275]\n",
      " [ 0.11149116]\n",
      " [-1.12706728]]\n",
      "t [[ 0.8883042 ]\n",
      " [-1.36467835]\n",
      " [-1.14384139]\n",
      " ...\n",
      " [ 0.29933275]\n",
      " [ 0.11149116]\n",
      " [-1.12706728]]\n",
      "Current iteration=8, loss=34199.109596662\n",
      "t [[ 0.91706912]\n",
      " [-1.47429586]\n",
      " [-1.22057604]\n",
      " ...\n",
      " [ 0.28149348]\n",
      " [ 0.09747947]\n",
      " [-1.18708562]]\n",
      "t [[ 0.91706912]\n",
      " [-1.47429586]\n",
      " [-1.22057604]\n",
      " ...\n",
      " [ 0.28149348]\n",
      " [ 0.09747947]\n",
      " [-1.18708562]]\n",
      "t [[ 0.94091514]\n",
      " [-1.57410243]\n",
      " [-1.29069064]\n",
      " ...\n",
      " [ 0.26290572]\n",
      " [ 0.08331802]\n",
      " [-1.24193597]]\n",
      "loss=33100.93540244762\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.26691586]\n",
      " [-0.15088905]\n",
      " [-0.25131394]\n",
      " ...\n",
      " [ 0.21512109]\n",
      " [ 0.09082909]\n",
      " [-0.3699946 ]]\n",
      "t [[ 0.26691586]\n",
      " [-0.15088905]\n",
      " [-0.25131394]\n",
      " ...\n",
      " [ 0.21512109]\n",
      " [ 0.09082909]\n",
      " [-0.3699946 ]]\n",
      "t [[ 0.44832074]\n",
      " [-0.36583158]\n",
      " [-0.44474847]\n",
      " ...\n",
      " [ 0.30048335]\n",
      " [ 0.13305163]\n",
      " [-0.57812128]]\n",
      "t [[ 0.44832074]\n",
      " [-0.36583158]\n",
      " [-0.44474847]\n",
      " ...\n",
      " [ 0.30048335]\n",
      " [ 0.13305163]\n",
      " [-0.57812128]]\n",
      "Current iteration=2, loss=42676.65361896606\n",
      "t [[ 0.57727784]\n",
      " [-0.57418843]\n",
      " [-0.60366643]\n",
      " ...\n",
      " [ 0.33340486]\n",
      " [ 0.15063915]\n",
      " [-0.72260423]]\n",
      "t [[ 0.57727784]\n",
      " [-0.57418843]\n",
      " [-0.60366643]\n",
      " ...\n",
      " [ 0.33340486]\n",
      " [ 0.15063915]\n",
      " [-0.72260423]]\n",
      "t [[ 0.67222717]\n",
      " [-0.76086159]\n",
      " [-0.73905385]\n",
      " ...\n",
      " [ 0.34172448]\n",
      " [ 0.1546035 ]\n",
      " [-0.83665331]]\n",
      "t [[ 0.67222717]\n",
      " [-0.76086159]\n",
      " [-0.73905385]\n",
      " ...\n",
      " [ 0.34172448]\n",
      " [ 0.1546035 ]\n",
      " [-0.83665331]]\n",
      "Current iteration=4, loss=38413.17407734639\n",
      "t [[ 0.74402223]\n",
      " [-0.92555575]\n",
      " [-0.85696715]\n",
      " ...\n",
      " [ 0.33713915]\n",
      " [ 0.15062152]\n",
      " [-0.93287606]]\n",
      "t [[ 0.74402223]\n",
      " [-0.92555575]\n",
      " [-0.85696715]\n",
      " ...\n",
      " [ 0.33713915]\n",
      " [ 0.15062152]\n",
      " [-0.93287606]]\n",
      "t [[ 0.7995158 ]\n",
      " [-1.0711558 ]\n",
      " [-0.96121091]\n",
      " ...\n",
      " [ 0.32524678]\n",
      " [ 0.14191946]\n",
      " [-1.01687378]]\n",
      "t [[ 0.7995158 ]\n",
      " [-1.0711558 ]\n",
      " [-0.96121091]\n",
      " ...\n",
      " [ 0.32524678]\n",
      " [ 0.14191946]\n",
      " [-1.01687378]]\n",
      "Current iteration=6, loss=35973.24896845232\n",
      "t [[ 0.84324751]\n",
      " [-1.20075425]\n",
      " [-1.05439593]\n",
      " ...\n",
      " [ 0.309058  ]\n",
      " [ 0.13045632]\n",
      " [-1.0915942 ]]\n",
      "t [[ 0.84324751]\n",
      " [-1.20075425]\n",
      " [-1.05439593]\n",
      " ...\n",
      " [ 0.309058  ]\n",
      " [ 0.13045632]\n",
      " [-1.0915942 ]]\n",
      "t [[ 0.87833142]\n",
      " [-1.31699459]\n",
      " [-1.13842148]\n",
      " ...\n",
      " [ 0.29034371]\n",
      " [ 0.11747326]\n",
      " [-1.158838  ]]\n",
      "t [[ 0.87833142]\n",
      " [-1.31699459]\n",
      " [-1.13842148]\n",
      " ...\n",
      " [ 0.29034371]\n",
      " [ 0.11747326]\n",
      " [-1.158838  ]]\n",
      "Current iteration=8, loss=34407.485056585654\n",
      "t [[ 0.90696128]\n",
      " [-1.42201377]\n",
      " [-1.21472558]\n",
      " ...\n",
      " [ 0.27021062]\n",
      " [ 0.10377682]\n",
      " [-1.21983487]]\n",
      "t [[ 0.90696128]\n",
      " [-1.42201377]\n",
      " [-1.21472558]\n",
      " ...\n",
      " [ 0.27021062]\n",
      " [ 0.10377682]\n",
      " [-1.21983487]]\n",
      "t [[ 0.93071484]\n",
      " [-1.51751999]\n",
      " [-1.28442908]\n",
      " ...\n",
      " [ 0.24937647]\n",
      " [ 0.08989757]\n",
      " [-1.27549064]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=33326.95624651853\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.2664886 ]\n",
      " [-0.15852401]\n",
      " [-0.25052454]\n",
      " ...\n",
      " [ 0.38382739]\n",
      " [-0.25052454]\n",
      " [ 0.35950872]]\n",
      "t [[ 0.2664886 ]\n",
      " [-0.15852401]\n",
      " [-0.25052454]\n",
      " ...\n",
      " [ 0.38382739]\n",
      " [-0.25052454]\n",
      " [ 0.35950872]]\n",
      "t [[ 0.4475336 ]\n",
      " [-0.37768341]\n",
      " [-0.44366002]\n",
      " ...\n",
      " [ 0.62933925]\n",
      " [-0.44366002]\n",
      " [ 0.567138  ]]\n",
      "t [[ 0.4475336 ]\n",
      " [-0.37768341]\n",
      " [-0.44366002]\n",
      " ...\n",
      " [ 0.62933925]\n",
      " [-0.44366002]\n",
      " [ 0.567138  ]]\n",
      "Current iteration=2, loss=42675.8378893777\n",
      "t [[ 0.57609126]\n",
      " [-0.58935449]\n",
      " [-0.60238907]\n",
      " ...\n",
      " [ 0.79797386]\n",
      " [-0.60238907]\n",
      " [ 0.6990846 ]]\n",
      "t [[ 0.57609126]\n",
      " [-0.58935449]\n",
      " [-0.60238907]\n",
      " ...\n",
      " [ 0.79797386]\n",
      " [-0.60238907]\n",
      " [ 0.6990846 ]]\n",
      "t [[ 0.6704656 ]\n",
      " [-0.77913089]\n",
      " [-0.73761958]\n",
      " ...\n",
      " [ 0.91938752]\n",
      " [-0.73761958]\n",
      " [ 0.78833608]]\n",
      "t [[ 0.6704656 ]\n",
      " [-0.77913089]\n",
      " [-0.73761958]\n",
      " ...\n",
      " [ 0.91938752]\n",
      " [-0.73761958]\n",
      " [ 0.78833608]]\n",
      "Current iteration=4, loss=38398.497635993015\n",
      "t [[ 0.74156775]\n",
      " [-0.94681013]\n",
      " [-0.85538621]\n",
      " ...\n",
      " [ 1.00968303]\n",
      " [-0.85538621]\n",
      " [ 0.85118346]]\n",
      "t [[ 0.74156775]\n",
      " [-0.94681013]\n",
      " [-0.85538621]\n",
      " ...\n",
      " [ 1.00968303]\n",
      " [-0.85538621]\n",
      " [ 0.85118346]]\n",
      "t [[ 0.79630655]\n",
      " [-1.09526207]\n",
      " [-0.95948982]\n",
      " ...\n",
      " [ 1.07850799]\n",
      " [-0.95948982]\n",
      " [ 0.89671934]]\n",
      "t [[ 0.79630655]\n",
      " [-1.09526207]\n",
      " [-0.95948982]\n",
      " ...\n",
      " [ 1.07850799]\n",
      " [-0.95948982]\n",
      " [ 0.89671934]]\n",
      "Current iteration=6, loss=35949.32888954136\n",
      "t [[ 0.83925683]\n",
      " [-1.22755994]\n",
      " [-1.05254172]\n",
      " ...\n",
      " [ 1.13204865]\n",
      " [-1.05254172]\n",
      " [ 0.93048084]]\n",
      "t [[ 0.83925683]\n",
      " [-1.22755994]\n",
      " [-1.05254172]\n",
      " ...\n",
      " [ 1.13204865]\n",
      " [-1.05254172]\n",
      " [ 0.93048084]]\n",
      "t [[ 0.87355328]\n",
      " [-1.34633936]\n",
      " [-1.13644196]\n",
      " ...\n",
      " [ 1.17446267]\n",
      " [-1.13644196]\n",
      " [ 0.95604561]]\n",
      "t [[ 0.87355328]\n",
      " [-1.34633936]\n",
      " [-1.13644196]\n",
      " ...\n",
      " [ 1.17446267]\n",
      " [-1.13644196]\n",
      " [ 0.95604561]]\n",
      "Current iteration=8, loss=34377.42069043708\n",
      "t [[ 0.90140149]\n",
      " [-1.45373894]\n",
      " [-1.21262875]\n",
      " ...\n",
      " [ 1.2086392 ]\n",
      " [-1.21262875]\n",
      " [ 0.97581962]]\n",
      "t [[ 0.90140149]\n",
      " [-1.45373894]\n",
      " [-1.21262875]\n",
      " ...\n",
      " [ 1.2086392 ]\n",
      " [-1.21262875]\n",
      " [ 0.97581962]]\n",
      "t [[ 0.92438589]\n",
      " [-1.55147424]\n",
      " [-1.28222269]\n",
      " ...\n",
      " [ 1.23663558]\n",
      " [-1.28222269]\n",
      " [ 0.99146584]]\n",
      "loss=33292.856746788624\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.0914022 ]\n",
      " [-0.43767924]\n",
      " [-0.25666013]\n",
      " ...\n",
      " [ 0.21947849]\n",
      " [ 0.0914022 ]\n",
      " [-0.37204152]]\n",
      "t [[ 0.0914022 ]\n",
      " [-0.43767924]\n",
      " [-0.25666013]\n",
      " ...\n",
      " [ 0.21947849]\n",
      " [ 0.0914022 ]\n",
      " [-0.37204152]]\n",
      "t [[ 0.13293506]\n",
      " [-0.79599572]\n",
      " [-0.45365372]\n",
      " ...\n",
      " [ 0.30635061]\n",
      " [ 0.13293506]\n",
      " [-0.5802622 ]]\n",
      "t [[ 0.13293506]\n",
      " [-0.79599572]\n",
      " [-0.45365372]\n",
      " ...\n",
      " [ 0.30635061]\n",
      " [ 0.13293506]\n",
      " [-0.5802622 ]]\n",
      "Current iteration=2, loss=42466.51108920911\n",
      "t [[ 0.14959466]\n",
      " [-1.08836056]\n",
      " [-0.61530765]\n",
      " ...\n",
      " [ 0.34069227]\n",
      " [ 0.14959466]\n",
      " [-0.72555869]]\n",
      "t [[ 0.14959466]\n",
      " [-1.08836056]\n",
      " [-0.61530765]\n",
      " ...\n",
      " [ 0.34069227]\n",
      " [ 0.14959466]\n",
      " [-0.72555869]]\n",
      "t [[ 0.15264442]\n",
      " [-1.3299845 ]\n",
      " [-0.75289392]\n",
      " ...\n",
      " [ 0.35062786]\n",
      " [ 0.15264442]\n",
      " [-0.84090451]]\n",
      "t [[ 0.15264442]\n",
      " [-1.3299845 ]\n",
      " [-0.75289392]\n",
      " ...\n",
      " [ 0.35062786]\n",
      " [ 0.15264442]\n",
      " [-0.84090451]]\n",
      "Current iteration=4, loss=38137.8428700842\n",
      "t [[ 0.14783172]\n",
      " [-1.5328989 ]\n",
      " [-0.87260807]\n",
      " ...\n",
      " [ 0.34777535]\n",
      " [ 0.14783172]\n",
      " [-0.93854585]]\n",
      "t [[ 0.14783172]\n",
      " [-1.5328989 ]\n",
      " [-0.87260807]\n",
      " ...\n",
      " [ 0.34777535]\n",
      " [ 0.14783172]\n",
      " [-0.93854585]]\n",
      "t [[ 0.13840253]\n",
      " [-1.70592835]\n",
      " [-0.97834754]\n",
      " ...\n",
      " [ 0.33765675]\n",
      " [ 0.13840253]\n",
      " [-1.02389924]]\n",
      "t [[ 0.13840253]\n",
      " [-1.70592835]\n",
      " [-0.97834754]\n",
      " ...\n",
      " [ 0.33765675]\n",
      " [ 0.13840253]\n",
      " [-1.02389924]]\n",
      "Current iteration=6, loss=35673.51048091926\n",
      "t [[ 0.12631759]\n",
      " [-1.8554747 ]\n",
      " [-1.07279128]\n",
      " ...\n",
      " [ 0.32324389]\n",
      " [ 0.12631759]\n",
      " [-1.09984466]]\n",
      "t [[ 0.12631759]\n",
      " [-1.8554747 ]\n",
      " [-1.07279128]\n",
      " ...\n",
      " [ 0.32324389]\n",
      " [ 0.12631759]\n",
      " [-1.09984466]]\n",
      "t [[ 0.11281222]\n",
      " [-1.98623242]\n",
      " [-1.15788952]\n",
      " ...\n",
      " [ 0.3062896 ]\n",
      " [ 0.11281222]\n",
      " [-1.16816899]]\n",
      "t [[ 0.11281222]\n",
      " [-1.98623242]\n",
      " [-1.15788952]\n",
      " ...\n",
      " [ 0.3062896 ]\n",
      " [ 0.11281222]\n",
      " [-1.16816899]]\n",
      "Current iteration=8, loss=34097.852219730936\n",
      "t [[ 0.09868374]\n",
      " [-2.10170389]\n",
      " [-1.23511849]\n",
      " ...\n",
      " [ 0.28789129]\n",
      " [ 0.09868374]\n",
      " [-1.23010925]]\n",
      "t [[ 0.09868374]\n",
      " [-2.10170389]\n",
      " [-1.23511849]\n",
      " ...\n",
      " [ 0.28789129]\n",
      " [ 0.09868374]\n",
      " [-1.23010925]]\n",
      "t [[ 0.08445229]\n",
      " [-2.20455018]\n",
      " [-1.30562795]\n",
      " ...\n",
      " [ 0.26876089]\n",
      " [ 0.08445229]\n",
      " [-1.28658553]]\n",
      "loss=33013.66751809829\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.27601428]\n",
      " [-0.16090526]\n",
      " [-0.25751407]\n",
      " ...\n",
      " [ 0.21626608]\n",
      " [ 0.09104334]\n",
      " [-0.36641894]]\n",
      "t [[ 0.27601428]\n",
      " [-0.16090526]\n",
      " [-0.25751407]\n",
      " ...\n",
      " [ 0.21626608]\n",
      " [ 0.09104334]\n",
      " [-0.36641894]]\n",
      "t [[ 0.46178832]\n",
      " [-0.3882055 ]\n",
      " [-0.45473316]\n",
      " ...\n",
      " [ 0.30086345]\n",
      " [ 0.13205715]\n",
      " [-0.56912321]]\n",
      "t [[ 0.46178832]\n",
      " [-0.3882055 ]\n",
      " [-0.45473316]\n",
      " ...\n",
      " [ 0.30086345]\n",
      " [ 0.13205715]\n",
      " [-0.56912321]]\n",
      "Current iteration=2, loss=42453.26903248193\n",
      "t [[ 0.59284889]\n",
      " [-0.60723225]\n",
      " [-0.61635237]\n",
      " ...\n",
      " [ 0.33365202]\n",
      " [ 0.14818214]\n",
      " [-0.7096875 ]]\n",
      "t [[ 0.59284889]\n",
      " [-0.60723225]\n",
      " [-0.61635237]\n",
      " ...\n",
      " [ 0.33365202]\n",
      " [ 0.14818214]\n",
      " [-0.7096875 ]]\n",
      "t [[ 0.68869856]\n",
      " [-0.80289273]\n",
      " [-0.75379732]\n",
      " ...\n",
      " [ 0.34255214]\n",
      " [ 0.15072773]\n",
      " [-0.82110858]]\n",
      "t [[ 0.68869856]\n",
      " [-0.80289273]\n",
      " [-0.75379732]\n",
      " ...\n",
      " [ 0.34255214]\n",
      " [ 0.15072773]\n",
      " [-0.82110858]]\n",
      "Current iteration=4, loss=38125.197966610285\n",
      "t [[ 0.76074182]\n",
      " [-0.97531377]\n",
      " [-0.87332815]\n",
      " ...\n",
      " [ 0.3390184 ]\n",
      " [ 0.14545489]\n",
      " [-0.91550143]]\n",
      "t [[ 0.76074182]\n",
      " [-0.97531377]\n",
      " [-0.87332815]\n",
      " ...\n",
      " [ 0.3390184 ]\n",
      " [ 0.14545489]\n",
      " [-0.91550143]]\n",
      "t [[ 0.81613712]\n",
      " [-1.12769868]\n",
      " [-0.97887053]\n",
      " ...\n",
      " [ 0.32846133]\n",
      " [ 0.13561173]\n",
      " [-0.99814406]]\n",
      "t [[ 0.81613712]\n",
      " [-1.12769868]\n",
      " [-0.97887053]\n",
      " ...\n",
      " [ 0.32846133]\n",
      " [ 0.13561173]\n",
      " [-0.99814406]]\n",
      "Current iteration=6, loss=35661.30153141354\n",
      "t [[ 0.8595978 ]\n",
      " [-1.26334814]\n",
      " [-1.07311625]\n",
      " ...\n",
      " [ 0.31377987]\n",
      " [ 0.12315713]\n",
      " [-1.07180337]]\n",
      "t [[ 0.8595978 ]\n",
      " [-1.26334814]\n",
      " [-1.07311625]\n",
      " ...\n",
      " [ 0.31377987]\n",
      " [ 0.12315713]\n",
      " [-1.07180337]]\n",
      "t [[ 0.89433805]\n",
      " [-1.3850448 ]\n",
      " [-1.15802151]\n",
      " ...\n",
      " [ 0.29667925]\n",
      " [ 0.10932314]\n",
      " [-1.13817973]]\n",
      "t [[ 0.89433805]\n",
      " [-1.3850448 ]\n",
      " [-1.15802151]\n",
      " ...\n",
      " [ 0.29667925]\n",
      " [ 0.10932314]\n",
      " [-1.13817973]]\n",
      "Current iteration=8, loss=34085.488893903974\n",
      "t [[ 0.92260924]\n",
      " [-1.49502406]\n",
      " [-1.23506536]\n",
      " ...\n",
      " [ 0.27822518]\n",
      " [ 0.09490354]\n",
      " [-1.19844472]]\n",
      "t [[ 0.92260924]\n",
      " [-1.49502406]\n",
      " [-1.23506536]\n",
      " ...\n",
      " [ 0.27822518]\n",
      " [ 0.09490354]\n",
      " [-1.19844472]]\n",
      "t [[ 0.94602159]\n",
      " [-1.59506758]\n",
      " [-1.30539881]\n",
      " ...\n",
      " [ 0.25910789]\n",
      " [ 0.08041495]\n",
      " [-1.25346851]]\n",
      "loss=33000.762485841915\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807905\n",
      "t [[ 0.27247661]\n",
      " [-0.15403258]\n",
      " [-0.25654964]\n",
      " ...\n",
      " [ 0.21960278]\n",
      " [ 0.09272136]\n",
      " [-0.37770282]]\n",
      "t [[ 0.27247661]\n",
      " [-0.15403258]\n",
      " [-0.25654964]\n",
      " ...\n",
      " [ 0.21960278]\n",
      " [ 0.09272136]\n",
      " [-0.37770282]]\n",
      "t [[ 0.45590403]\n",
      " [-0.37476114]\n",
      " [-0.45282017]\n",
      " ...\n",
      " [ 0.30407705]\n",
      " [ 0.13482521]\n",
      " [-0.58684435]]\n",
      "t [[ 0.45590403]\n",
      " [-0.37476114]\n",
      " [-0.45282017]\n",
      " ...\n",
      " [ 0.30407705]\n",
      " [ 0.13482521]\n",
      " [-0.58684435]]\n",
      "Current iteration=2, loss=42542.96183795197\n",
      "t [[ 0.58541606]\n",
      " [-0.58720605]\n",
      " [-0.61363487]\n",
      " ...\n",
      " [ 0.33552248]\n",
      " [ 0.1517749 ]\n",
      " [-0.73172535]]\n",
      "t [[ 0.58541606]\n",
      " [-0.58720605]\n",
      " [-0.61363487]\n",
      " ...\n",
      " [ 0.33552248]\n",
      " [ 0.1517749 ]\n",
      " [-0.73172535]]\n",
      "t [[ 0.6802561 ]\n",
      " [-0.77647106]\n",
      " [-0.75039355]\n",
      " ...\n",
      " [ 0.34249162]\n",
      " [ 0.1549847 ]\n",
      " [-0.84626042]]\n",
      "t [[ 0.6802561 ]\n",
      " [-0.77647106]\n",
      " [-0.75039355]\n",
      " ...\n",
      " [ 0.34249162]\n",
      " [ 0.1549847 ]\n",
      " [-0.84626042]]\n",
      "Current iteration=4, loss=38273.948958510504\n",
      "t [[ 0.75163976]\n",
      " [-0.94281578]\n",
      " [-0.86932392]\n",
      " ...\n",
      " [ 0.33674106]\n",
      " [ 0.15026457]\n",
      " [-0.94300567]]\n",
      "t [[ 0.75163976]\n",
      " [-0.94281578]\n",
      " [-0.86932392]\n",
      " ...\n",
      " [ 0.33674106]\n",
      " [ 0.15026457]\n",
      " [-0.94300567]]\n",
      "t [[ 0.8066015 ]\n",
      " [-1.08949384]\n",
      " [-0.97432752]\n",
      " ...\n",
      " [ 0.32384126]\n",
      " [ 0.14089137]\n",
      " [-1.02748226]]\n",
      "t [[ 0.8066015 ]\n",
      " [-1.08949384]\n",
      " [-0.97432752]\n",
      " ...\n",
      " [ 0.32384126]\n",
      " [ 0.14089137]\n",
      " [-1.02748226]]\n",
      "Current iteration=6, loss=35846.83291258314\n",
      "t [[ 0.84977568]\n",
      " [-1.21981051]\n",
      " [-1.06807932]\n",
      " ...\n",
      " [ 0.30677758]\n",
      " [ 0.12884288]\n",
      " [-1.10260205]]\n",
      "t [[ 0.84977568]\n",
      " [-1.21981051]\n",
      " [-1.06807932]\n",
      " ...\n",
      " [ 0.30677758]\n",
      " [ 0.12884288]\n",
      " [-1.10260205]]\n",
      "t [[ 0.88432568]\n",
      " [-1.33653214]\n",
      " [-1.15252387]\n",
      " ...\n",
      " [ 0.28730344]\n",
      " [ 0.11536451]\n",
      " [-1.17015813]]\n",
      "t [[ 0.88432568]\n",
      " [-1.33653214]\n",
      " [-1.15252387]\n",
      " ...\n",
      " [ 0.28730344]\n",
      " [ 0.11536451]\n",
      " [-1.17015813]]\n",
      "Current iteration=8, loss=34295.67836285497\n",
      "t [[ 0.91246959]\n",
      " [-1.44186962]\n",
      " [-1.2291321 ]\n",
      " ...\n",
      " [ 0.26651215]\n",
      " [ 0.10125988]\n",
      " [-1.23138504]]\n",
      "t [[ 0.91246959]\n",
      " [-1.44186962]\n",
      " [-1.2291321 ]\n",
      " ...\n",
      " [ 0.26651215]\n",
      " [ 0.10125988]\n",
      " [-1.23138504]]\n",
      "t [[ 0.93579548]\n",
      " [-1.537578  ]\n",
      " [-1.29904927]\n",
      " ...\n",
      " [ 0.24510992]\n",
      " [ 0.08705311]\n",
      " [-1.2871978 ]]\n",
      "loss=33228.42570193952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=51940.29082807904\n",
      "t [[ 0.27204044]\n",
      " [-0.16182659]\n",
      " [-0.25574381]\n",
      " ...\n",
      " [ 0.39182379]\n",
      " [-0.25574381]\n",
      " [ 0.36699849]]\n",
      "t [[ 0.27204044]\n",
      " [-0.16182659]\n",
      " [-0.25574381]\n",
      " ...\n",
      " [ 0.39182379]\n",
      " [-0.25574381]\n",
      " [ 0.36699849]]\n",
      "t [[ 0.45509843]\n",
      " [-0.38679333]\n",
      " [-0.45171857]\n",
      " ...\n",
      " [ 0.63960402]\n",
      " [-0.45171857]\n",
      " [ 0.57583137]]\n",
      "t [[ 0.45509843]\n",
      " [-0.38679333]\n",
      " [-0.45171857]\n",
      " ...\n",
      " [ 0.63960402]\n",
      " [-0.45171857]\n",
      " [ 0.57583137]]\n",
      "Current iteration=2, loss=42541.82817382785\n",
      "t [[ 0.58419401]\n",
      " [-0.60259136]\n",
      " [-0.61234498]\n",
      " ...\n",
      " [ 0.80861396]\n",
      " [-0.61234498]\n",
      " [ 0.70743749]]\n",
      "t [[ 0.58419401]\n",
      " [-0.60259136]\n",
      " [-0.61234498]\n",
      " ...\n",
      " [ 0.80861396]\n",
      " [-0.61234498]\n",
      " [ 0.70743749]]\n",
      "t [[ 0.67843674]\n",
      " [-0.79501247]\n",
      " [-0.74894533]\n",
      " ...\n",
      " [ 0.92965586]\n",
      " [-0.74894533]\n",
      " [ 0.79592234]]\n",
      "t [[ 0.67843674]\n",
      " [-0.79501247]\n",
      " [-0.74894533]\n",
      " ...\n",
      " [ 0.92965586]\n",
      " [-0.74894533]\n",
      " [ 0.79592234]]\n",
      "Current iteration=4, loss=38258.78807227571\n",
      "t [[ 0.74910554]\n",
      " [-0.96439454]\n",
      " [-0.86772682]\n",
      " ...\n",
      " [ 1.01927111]\n",
      " [-0.86772682]\n",
      " [ 0.85790371]]\n",
      "t [[ 0.74910554]\n",
      " [-0.96439454]\n",
      " [-0.86772682]\n",
      " ...\n",
      " [ 1.01927111]\n",
      " [-0.86772682]\n",
      " [ 0.85790371]]\n",
      "t [[ 0.80329253]\n",
      " [-1.11396976]\n",
      " [-0.97258813]\n",
      " ...\n",
      " [ 1.08730996]\n",
      " [-0.97258813]\n",
      " [ 0.90259728]]\n",
      "t [[ 0.80329253]\n",
      " [-1.11396976]\n",
      " [-0.97258813]\n",
      " ...\n",
      " [ 1.08730996]\n",
      " [-0.97258813]\n",
      " [ 0.90259728]]\n",
      "Current iteration=6, loss=35822.42172148924\n",
      "t [[ 0.84566745]\n",
      " [-1.24702278]\n",
      " [-1.06620507]\n",
      " ...\n",
      " [ 1.14006107]\n",
      " [-1.06620507]\n",
      " [ 0.93559375]]\n",
      "t [[ 0.84566745]\n",
      " [-1.24702278]\n",
      " [-1.06620507]\n",
      " ...\n",
      " [ 1.14006107]\n",
      " [-1.06620507]\n",
      " [ 0.93559375]]\n",
      "t [[ 0.87941408]\n",
      " [-1.36631276]\n",
      " [-1.15052297]\n",
      " ...\n",
      " [ 1.18173435]\n",
      " [-1.15052297]\n",
      " [ 0.96049318]]\n",
      "t [[ 0.87941408]\n",
      " [-1.36631276]\n",
      " [-1.15052297]\n",
      " ...\n",
      " [ 1.18173435]\n",
      " [-1.15052297]\n",
      " [ 0.96049318]]\n",
      "Current iteration=8, loss=34265.176169749975\n",
      "t [[ 0.9067621 ]\n",
      " [-1.47405353]\n",
      " [-1.2270129 ]\n",
      " ...\n",
      " [ 1.21524372]\n",
      " [-1.2270129 ]\n",
      " [ 0.9797077 ]]\n",
      "t [[ 0.9067621 ]\n",
      " [-1.47405353]\n",
      " [-1.2270129 ]\n",
      " ...\n",
      " [ 1.21524372]\n",
      " [-1.2270129 ]\n",
      " [ 0.9797077 ]]\n",
      "t [[ 0.92930599]\n",
      " [-1.57200877]\n",
      " [-1.29681979]\n",
      " ...\n",
      " [ 1.2426558 ]\n",
      " [-1.29681979]\n",
      " [ 0.99489744]]\n",
      "loss=33193.96880572593\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=69254.41425128598\n",
      "t [[-0.07844999]\n",
      " [ 0.00878458]\n",
      " [ 0.00715734]\n",
      " ...\n",
      " [-0.01540242]\n",
      " [ 0.00708345]\n",
      " [-0.01540242]]\n",
      "t [[-0.07844999]\n",
      " [ 0.00878458]\n",
      " [ 0.00715734]\n",
      " ...\n",
      " [-0.01540242]\n",
      " [ 0.00708345]\n",
      " [-0.01540242]]\n",
      "t [[-0.15518199]\n",
      " [ 0.01752977]\n",
      " [ 0.01417684]\n",
      " ...\n",
      " [-0.03064349]\n",
      " [ 0.01411854]\n",
      " [-0.03064349]]\n",
      "t [[-0.15518199]\n",
      " [ 0.01752977]\n",
      " [ 0.01417684]\n",
      " ...\n",
      " [-0.03064349]\n",
      " [ 0.01411854]\n",
      " [-0.03064349]]\n",
      "Current iteration=2, loss=68468.25370881142\n",
      "t [[-0.2302381 ]\n",
      " [ 0.02623359]\n",
      " [ 0.02106067]\n",
      " ...\n",
      " [-0.04572565]\n",
      " [ 0.0211045 ]\n",
      " [-0.04572565]]\n",
      "t [[-0.2302381 ]\n",
      " [ 0.02623359]\n",
      " [ 0.02106067]\n",
      " ...\n",
      " [-0.04572565]\n",
      " [ 0.0211045 ]\n",
      " [-0.04572565]]\n",
      "t [[-0.30365959]\n",
      " [ 0.03489415]\n",
      " [ 0.02781098]\n",
      " ...\n",
      " [-0.06065134]\n",
      " [ 0.02804059]\n",
      " [-0.06065134]]\n",
      "t [[-0.30365959]\n",
      " [ 0.03489415]\n",
      " [ 0.02781098]\n",
      " ...\n",
      " [-0.06065134]\n",
      " [ 0.02804059]\n",
      " [-0.06065134]]\n",
      "Current iteration=4, loss=67714.10556395467\n",
      "t [[-0.37548689]\n",
      " [ 0.04350965]\n",
      " [ 0.03442993]\n",
      " ...\n",
      " [-0.07542294]\n",
      " [ 0.03492616]\n",
      " [-0.07542294]]\n",
      "t [[-0.37548689]\n",
      " [ 0.04350965]\n",
      " [ 0.03442993]\n",
      " ...\n",
      " [-0.07542294]\n",
      " [ 0.03492616]\n",
      " [-0.07542294]]\n",
      "t [[-0.44575959]\n",
      " [ 0.05207837]\n",
      " [ 0.04091962]\n",
      " ...\n",
      " [-0.09004282]\n",
      " [ 0.04176057]\n",
      " [-0.09004282]]\n",
      "t [[-0.44575959]\n",
      " [ 0.05207837]\n",
      " [ 0.04091962]\n",
      " ...\n",
      " [-0.09004282]\n",
      " [ 0.04176057]\n",
      " [-0.09004282]]\n",
      "Current iteration=6, loss=66990.2146470263\n",
      "t [[-0.51451642]\n",
      " [ 0.06059869]\n",
      " [ 0.04728217]\n",
      " ...\n",
      " [-0.10451331]\n",
      " [ 0.04854325]\n",
      " [-0.10451331]]\n",
      "t [[-0.51451642]\n",
      " [ 0.06059869]\n",
      " [ 0.04728217]\n",
      " ...\n",
      " [-0.10451331]\n",
      " [ 0.04854325]\n",
      " [-0.10451331]]\n",
      "t [[-0.58179527]\n",
      " [ 0.06906906]\n",
      " [ 0.05351966]\n",
      " ...\n",
      " [-0.11883672]\n",
      " [ 0.05527369]\n",
      " [-0.11883672]]\n",
      "t [[-0.58179527]\n",
      " [ 0.06906906]\n",
      " [ 0.05351966]\n",
      " ...\n",
      " [-0.11883672]\n",
      " [ 0.05527369]\n",
      " [-0.11883672]]\n",
      "Current iteration=8, loss=66294.93750006132\n",
      "t [[-0.64763318]\n",
      " [ 0.07748802]\n",
      " [ 0.05963415]\n",
      " ...\n",
      " [-0.1330153 ]\n",
      " [ 0.06195141]\n",
      " [-0.1330153 ]]\n",
      "t [[-0.64763318]\n",
      " [ 0.07748802]\n",
      " [ 0.05963415]\n",
      " ...\n",
      " [-0.1330153 ]\n",
      " [ 0.06195141]\n",
      " [-0.1330153 ]]\n",
      "t [[-0.71206631]\n",
      " [ 0.08585418]\n",
      " [ 0.06562769]\n",
      " ...\n",
      " [-0.14705129]\n",
      " [ 0.06857596]\n",
      " [-0.14705129]]\n",
      "t [[-0.71206631]\n",
      " [ 0.08585418]\n",
      " [ 0.06562769]\n",
      " ...\n",
      " [-0.14705129]\n",
      " [ 0.06857596]\n",
      " [-0.14705129]]\n",
      "Current iteration=10, loss=65626.73574255295\n",
      "t [[-0.77513003]\n",
      " [ 0.09416625]\n",
      " [ 0.07150228]\n",
      " ...\n",
      " [-0.1609469 ]\n",
      " [ 0.07514697]\n",
      " [-0.1609469 ]]\n",
      "t [[-0.77513003]\n",
      " [ 0.09416625]\n",
      " [ 0.07150228]\n",
      " ...\n",
      " [-0.1609469 ]\n",
      " [ 0.07514697]\n",
      " [-0.1609469 ]]\n",
      "t [[-0.83685881]\n",
      " [ 0.102423  ]\n",
      " [ 0.07725994]\n",
      " ...\n",
      " [-0.17470428]\n",
      " [ 0.08166409]\n",
      " [-0.17470428]]\n",
      "t [[-0.83685881]\n",
      " [ 0.102423  ]\n",
      " [ 0.07725994]\n",
      " ...\n",
      " [-0.17470428]\n",
      " [ 0.08166409]\n",
      " [-0.17470428]]\n",
      "Current iteration=12, loss=64984.16950885916\n",
      "t [[-0.89728634]\n",
      " [ 0.11062326]\n",
      " [ 0.08290261]\n",
      " ...\n",
      " [-0.18832558]\n",
      " [ 0.08812701]\n",
      " [-0.18832558]]\n",
      "t [[-0.89728634]\n",
      " [ 0.11062326]\n",
      " [ 0.08290261]\n",
      " ...\n",
      " [-0.18832558]\n",
      " [ 0.08812701]\n",
      " [-0.18832558]]\n",
      "t [[-0.95644545]\n",
      " [ 0.11876596]\n",
      " [ 0.08843226]\n",
      " ...\n",
      " [-0.20181288]\n",
      " [ 0.09453546]\n",
      " [-0.20181288]]\n",
      "t [[-0.95644545]\n",
      " [ 0.11876596]\n",
      " [ 0.08843226]\n",
      " ...\n",
      " [-0.20181288]\n",
      " [ 0.09453546]\n",
      " [-0.20181288]]\n",
      "Current iteration=14, loss=64365.891057945984\n",
      "t [[-1.01436817]\n",
      " [ 0.12685009]\n",
      " [ 0.09385079]\n",
      " ...\n",
      " [-0.21516826]\n",
      " [ 0.10088922]\n",
      " [-0.21516826]]\n",
      "t [[-1.01436817]\n",
      " [ 0.12685009]\n",
      " [ 0.09385079]\n",
      " ...\n",
      " [-0.21516826]\n",
      " [ 0.10088922]\n",
      " [-0.21516826]]\n",
      "t [[-1.07108575]\n",
      " [ 0.1348747 ]\n",
      " [ 0.09916009]\n",
      " ...\n",
      " [-0.22839374]\n",
      " [ 0.10718809]\n",
      " [-0.22839374]]\n",
      "t [[-1.07108575]\n",
      " [ 0.1348747 ]\n",
      " [ 0.09916009]\n",
      " ...\n",
      " [-0.22839374]\n",
      " [ 0.10718809]\n",
      " [-0.22839374]]\n",
      "Current iteration=16, loss=63770.63862613424\n",
      "t [[-1.12662862]\n",
      " [ 0.14283891]\n",
      " [ 0.10436205]\n",
      " ...\n",
      " [-0.24149132]\n",
      " [ 0.11343191]\n",
      " [-0.24149132]]\n",
      "t [[-1.12662862]\n",
      " [ 0.14283891]\n",
      " [ 0.10436205]\n",
      " ...\n",
      " [-0.24149132]\n",
      " [ 0.11343191]\n",
      " [-0.24149132]]\n",
      "t [[-1.18102646]\n",
      " [ 0.15074191]\n",
      " [ 0.10945848]\n",
      " ...\n",
      " [-0.25446297]\n",
      " [ 0.11962055]\n",
      " [-0.25446297]]\n",
      "t [[-1.18102646]\n",
      " [ 0.15074191]\n",
      " [ 0.10945848]\n",
      " ...\n",
      " [-0.25446297]\n",
      " [ 0.11962055]\n",
      " [-0.25446297]]\n",
      "Current iteration=18, loss=63197.23056928423\n",
      "t [[-1.23430816]\n",
      " [ 0.15858294]\n",
      " [ 0.11445122]\n",
      " ...\n",
      " [-0.26731061]\n",
      " [ 0.12575393]\n",
      " [-0.26731061]]\n",
      "t [[-1.23430816]\n",
      " [ 0.15858294]\n",
      " [ 0.11445122]\n",
      " ...\n",
      " [-0.26731061]\n",
      " [ 0.12575393]\n",
      " [-0.26731061]]\n",
      "t [[-1.28650189]\n",
      " [ 0.16636132]\n",
      " [ 0.11934203]\n",
      " ...\n",
      " [-0.28003614]\n",
      " [ 0.13183197]\n",
      " [-0.28003614]]\n",
      "t [[-1.28650189]\n",
      " [ 0.16636132]\n",
      " [ 0.11934203]\n",
      " ...\n",
      " [-0.28003614]\n",
      " [ 0.13183197]\n",
      " [-0.28003614]]\n",
      "Current iteration=20, loss=62644.5598218673\n",
      "t [[-1.33763508]\n",
      " [ 0.17407639]\n",
      " [ 0.12413268]\n",
      " ...\n",
      " [-0.29264142]\n",
      " [ 0.13785465]\n",
      " [-0.29264142]]\n",
      "t [[-1.33763508]\n",
      " [ 0.17407639]\n",
      " [ 0.12413268]\n",
      " ...\n",
      " [-0.29264142]\n",
      " [ 0.13785465]\n",
      " [-0.29264142]]\n",
      "t [[-1.38773443]\n",
      " [ 0.18172759]\n",
      " [ 0.12882489]\n",
      " ...\n",
      " [-0.30512829]\n",
      " [ 0.14382195]\n",
      " [-0.30512829]]\n",
      "t [[-1.38773443]\n",
      " [ 0.18172759]\n",
      " [ 0.12882489]\n",
      " ...\n",
      " [-0.30512829]\n",
      " [ 0.14382195]\n",
      " [-0.30512829]]\n",
      "Current iteration=22, loss=62111.58868593765\n",
      "t [[-1.43682594]\n",
      " [ 0.18931438]\n",
      " [ 0.13342038]\n",
      " ...\n",
      " [-0.31749854]\n",
      " [ 0.1497339 ]\n",
      " [-0.31749854]]\n",
      "t [[-1.43682594]\n",
      " [ 0.18931438]\n",
      " [ 0.13342038]\n",
      " ...\n",
      " [-0.31749854]\n",
      " [ 0.1497339 ]\n",
      " [-0.31749854]]\n",
      "t [[-1.48493493]\n",
      " [ 0.19683629]\n",
      " [ 0.1379208 ]\n",
      " ...\n",
      " [-0.32975394]\n",
      " [ 0.15559053]\n",
      " [-0.32975394]]\n",
      "t [[-1.48493493]\n",
      " [ 0.19683629]\n",
      " [ 0.1379208 ]\n",
      " ...\n",
      " [-0.32975394]\n",
      " [ 0.15559053]\n",
      " [-0.32975394]]\n",
      "Current iteration=24, loss=61597.34395240801\n",
      "t [[-1.53208604]\n",
      " [ 0.20429289]\n",
      " [ 0.14232782]\n",
      " ...\n",
      " [-0.34189623]\n",
      " [ 0.16139192]\n",
      " [-0.34189623]]\n",
      "t [[-1.53208604]\n",
      " [ 0.20429289]\n",
      " [ 0.14232782]\n",
      " ...\n",
      " [-0.34189623]\n",
      " [ 0.16139192]\n",
      " [-0.34189623]]\n",
      "t [[-1.57830326]\n",
      " [ 0.21168379]\n",
      " [ 0.14664304]\n",
      " ...\n",
      " [-0.35392711]\n",
      " [ 0.16713815]\n",
      " [-0.35392711]]\n",
      "t [[-1.57830326]\n",
      " [ 0.21168379]\n",
      " [ 0.14664304]\n",
      " ...\n",
      " [-0.35392711]\n",
      " [ 0.16713815]\n",
      " [-0.35392711]]\n",
      "Current iteration=26, loss=61100.91234948729\n",
      "t [[-1.62360993]\n",
      " [ 0.21900866]\n",
      " [ 0.15086806]\n",
      " ...\n",
      " [-0.36584827]\n",
      " [ 0.17282934]\n",
      " [-0.36584827]]\n",
      "t [[-1.62360993]\n",
      " [ 0.21900866]\n",
      " [ 0.15086806]\n",
      " ...\n",
      " [-0.36584827]\n",
      " [ 0.17282934]\n",
      " [-0.36584827]]\n",
      "t [[-1.66802877]\n",
      " [ 0.22626722]\n",
      " [ 0.15500445]\n",
      " ...\n",
      " [-0.37766133]\n",
      " [ 0.17846561]\n",
      " [-0.37766133]]\n",
      "t [[-1.66802877]\n",
      " [ 0.22626722]\n",
      " [ 0.15500445]\n",
      " ...\n",
      " [-0.37766133]\n",
      " [ 0.17846561]\n",
      " [-0.37766133]]\n",
      "Current iteration=28, loss=60621.43630799641\n",
      "t [[-1.71158187]\n",
      " [ 0.2334592 ]\n",
      " [ 0.15905374]\n",
      " ...\n",
      " [-0.38936791]\n",
      " [ 0.18404712]\n",
      " [-0.38936791]]\n",
      "t [[-1.71158187]\n",
      " [ 0.2334592 ]\n",
      " [ 0.15905374]\n",
      " ...\n",
      " [-0.38936791]\n",
      " [ 0.18404712]\n",
      " [-0.38936791]]\n",
      "t [[-1.75429076]\n",
      " [ 0.24058441]\n",
      " [ 0.16301743]\n",
      " ...\n",
      " [-0.40096961]\n",
      " [ 0.18957404]\n",
      " [-0.40096961]]\n",
      "t [[-1.75429076]\n",
      " [ 0.24058441]\n",
      " [ 0.16301743]\n",
      " ...\n",
      " [-0.40096961]\n",
      " [ 0.18957404]\n",
      " [-0.40096961]]\n",
      "Current iteration=30, loss=60158.11002992276\n",
      "t [[-1.79617636]\n",
      " [ 0.24764267]\n",
      " [ 0.16689702]\n",
      " ...\n",
      " [-0.41246798]\n",
      " [ 0.19504655]\n",
      " [-0.41246798]]\n",
      "t [[-1.79617636]\n",
      " [ 0.24764267]\n",
      " [ 0.16689702]\n",
      " ...\n",
      " [-0.41246798]\n",
      " [ 0.19504655]\n",
      " [-0.41246798]]\n",
      "t [[-1.83725902]\n",
      " [ 0.25463385]\n",
      " [ 0.17069396]\n",
      " ...\n",
      " [-0.42386455]\n",
      " [ 0.20046485]\n",
      " [-0.42386455]]\n",
      "t [[-1.83725902]\n",
      " [ 0.25463385]\n",
      " [ 0.17069396]\n",
      " ...\n",
      " [-0.42386455]\n",
      " [ 0.20046485]\n",
      " [-0.42386455]]\n",
      "Current iteration=32, loss=59710.175844539895\n",
      "t [[-1.87755855]\n",
      " [ 0.26155785]\n",
      " [ 0.17440968]\n",
      " ...\n",
      " [-0.43516081]\n",
      " [ 0.20582917]\n",
      " [-0.43516081]]\n",
      "t [[-1.87755855]\n",
      " [ 0.26155785]\n",
      " [ 0.17440968]\n",
      " ...\n",
      " [-0.43516081]\n",
      " [ 0.20582917]\n",
      " [-0.43516081]]\n",
      "t [[-1.91709423]\n",
      " [ 0.26841461]\n",
      " [ 0.17804559]\n",
      " ...\n",
      " [-0.44635824]\n",
      " [ 0.21113973]\n",
      " [-0.44635824]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.91709423]\n",
      " [ 0.26841461]\n",
      " [ 0.17804559]\n",
      " ...\n",
      " [-0.44635824]\n",
      " [ 0.21113973]\n",
      " [-0.44635824]]\n",
      "Current iteration=34, loss=59276.9208353461\n",
      "t [[-1.95588479]\n",
      " [ 0.27520408]\n",
      " [ 0.18160306]\n",
      " ...\n",
      " [-0.4574583 ]\n",
      " [ 0.21639678]\n",
      " [-0.4574583 ]]\n",
      "t [[-1.95588479]\n",
      " [ 0.27520408]\n",
      " [ 0.18160306]\n",
      " ...\n",
      " [-0.4574583 ]\n",
      " [ 0.21639678]\n",
      " [-0.4574583 ]]\n",
      "t [[-1.99394848]\n",
      " [ 0.28192628]\n",
      " [ 0.18508346]\n",
      " ...\n",
      " [-0.46846238]\n",
      " [ 0.22160059]\n",
      " [-0.46846238]]\n",
      "t [[-1.99394848]\n",
      " [ 0.28192628]\n",
      " [ 0.18508346]\n",
      " ...\n",
      " [-0.46846238]\n",
      " [ 0.22160059]\n",
      " [-0.46846238]]\n",
      "Current iteration=36, loss=58857.67372064233\n",
      "t [[-2.03130303]\n",
      " [ 0.28858123]\n",
      " [ 0.1884881 ]\n",
      " ...\n",
      " [-0.47937191]\n",
      " [ 0.22675141]\n",
      " [-0.47937191]]\n",
      "t [[-2.03130303]\n",
      " [ 0.28858123]\n",
      " [ 0.1884881 ]\n",
      " ...\n",
      " [-0.47937191]\n",
      " [ 0.22675141]\n",
      " [-0.47937191]]\n",
      "t [[-2.0679657 ]\n",
      " [ 0.29516898]\n",
      " [ 0.1918183 ]\n",
      " ...\n",
      " [-0.49018823]\n",
      " [ 0.23184953]\n",
      " [-0.49018823]]\n",
      "t [[-2.0679657 ]\n",
      " [ 0.29516898]\n",
      " [ 0.1918183 ]\n",
      " ...\n",
      " [-0.49018823]\n",
      " [ 0.23184953]\n",
      " [-0.49018823]]\n",
      "Current iteration=38, loss=58451.801970640554\n",
      "t [[-2.10395327]\n",
      " [ 0.30168963]\n",
      " [ 0.19507534]\n",
      " ...\n",
      " [-0.50091269]\n",
      " [ 0.23689525]\n",
      " [-0.50091269]]\n",
      "t [[-2.10395327]\n",
      " [ 0.30168963]\n",
      " [ 0.19507534]\n",
      " ...\n",
      " [-0.50091269]\n",
      " [ 0.23689525]\n",
      " [-0.50091269]]\n",
      "t [[-2.13928207]\n",
      " [ 0.30814326]\n",
      " [ 0.19826047]\n",
      " ...\n",
      " [-0.51154662]\n",
      " [ 0.24188886]\n",
      " [-0.51154662]]\n",
      "t [[-2.13928207]\n",
      " [ 0.30814326]\n",
      " [ 0.19826047]\n",
      " ...\n",
      " [-0.51154662]\n",
      " [ 0.24188886]\n",
      " [-0.51154662]]\n",
      "Current iteration=40, loss=58058.70914436325\n",
      "t [[-2.17396798]\n",
      " [ 0.31453003]\n",
      " [ 0.20137494]\n",
      " ...\n",
      " [-0.52209131]\n",
      " [ 0.24683068]\n",
      " [-0.52209131]]\n",
      "t [[-2.17396798]\n",
      " [ 0.31453003]\n",
      " [ 0.20137494]\n",
      " ...\n",
      " [-0.52209131]\n",
      " [ 0.24683068]\n",
      " [-0.52209131]]\n",
      "t [[-2.20802646]\n",
      " [ 0.32085008]\n",
      " [ 0.20441995]\n",
      " ...\n",
      " [-0.53254803]\n",
      " [ 0.25172103]\n",
      " [-0.53254803]]\n",
      "t [[-2.20802646]\n",
      " [ 0.32085008]\n",
      " [ 0.20441995]\n",
      " ...\n",
      " [-0.53254803]\n",
      " [ 0.25172103]\n",
      " [-0.53254803]]\n",
      "Current iteration=42, loss=57677.832430183305\n",
      "t [[-2.24147254]\n",
      " [ 0.32710359]\n",
      " [ 0.20739668]\n",
      " ...\n",
      " [-0.54291803]\n",
      " [ 0.25656023]\n",
      " [-0.54291803]]\n",
      "t [[-2.24147254]\n",
      " [ 0.32710359]\n",
      " [ 0.20739668]\n",
      " ...\n",
      " [-0.54291803]\n",
      " [ 0.25656023]\n",
      " [-0.54291803]]\n",
      "t [[-2.27432084]\n",
      " [ 0.33329076]\n",
      " [ 0.21030632]\n",
      " ...\n",
      " [-0.55320254]\n",
      " [ 0.26134862]\n",
      " [-0.55320254]]\n",
      "t [[-2.27432084]\n",
      " [ 0.33329076]\n",
      " [ 0.21030632]\n",
      " ...\n",
      " [-0.55320254]\n",
      " [ 0.26134862]\n",
      " [-0.55320254]]\n",
      "Current iteration=44, loss=57308.64037460493\n",
      "t [[-2.30658558]\n",
      " [ 0.33941181]\n",
      " [ 0.21315001]\n",
      " ...\n",
      " [-0.56340275]\n",
      " [ 0.26608654]\n",
      " [-0.56340275]]\n",
      "t [[-2.30658558]\n",
      " [ 0.33941181]\n",
      " [ 0.21315001]\n",
      " ...\n",
      " [-0.56340275]\n",
      " [ 0.26608654]\n",
      " [-0.56340275]]\n",
      "t [[-2.33828059]\n",
      " [ 0.34546697]\n",
      " [ 0.21592886]\n",
      " ...\n",
      " [-0.57351985]\n",
      " [ 0.27077434]\n",
      " [-0.57351985]]\n",
      "t [[-2.33828059]\n",
      " [ 0.34546697]\n",
      " [ 0.21592886]\n",
      " ...\n",
      " [-0.57351985]\n",
      " [ 0.27077434]\n",
      " [-0.57351985]]\n",
      "Current iteration=46, loss=56950.63078470934\n",
      "t [[-2.36941935]\n",
      " [ 0.35145649]\n",
      " [ 0.21864399]\n",
      " ...\n",
      " [-0.583555  ]\n",
      " [ 0.27541237]\n",
      " [-0.583555  ]]\n",
      "t [[-2.36941935]\n",
      " [ 0.35145649]\n",
      " [ 0.21864399]\n",
      " ...\n",
      " [-0.583555  ]\n",
      " [ 0.27541237]\n",
      " [-0.583555  ]]\n",
      "t [[-2.40001495]\n",
      " [ 0.35738066]\n",
      " [ 0.22129647]\n",
      " ...\n",
      " [-0.59350934]\n",
      " [ 0.280001  ]\n",
      " [-0.59350934]]\n",
      "t [[-2.40001495]\n",
      " [ 0.35738066]\n",
      " [ 0.22129647]\n",
      " ...\n",
      " [-0.59350934]\n",
      " [ 0.280001  ]\n",
      " [-0.59350934]]\n",
      "Current iteration=48, loss=56603.32879057492\n",
      "t [[-2.43008013]\n",
      " [ 0.36323975]\n",
      " [ 0.22388738]\n",
      " ...\n",
      " [-0.60338399]\n",
      " [ 0.28454059]\n",
      " [-0.60338399]]\n",
      "t [[-2.43008013]\n",
      " [ 0.36323975]\n",
      " [ 0.22388738]\n",
      " ...\n",
      " [-0.60338399]\n",
      " [ 0.28454059]\n",
      " [-0.60338399]]\n",
      "t [[-2.4596273 ]\n",
      " [ 0.36903406]\n",
      " [ 0.22641775]\n",
      " ...\n",
      " [-0.61318005]\n",
      " [ 0.2890315 ]\n",
      " [-0.61318005]]\n",
      "loss=56266.2850548757\n",
      "Cross validation finished: optimal gamma 0.02\n",
      "logistic regression loss 56433.55177276884\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set1_x.shape[1])\n",
    "gamma_opt = cross_validation(set1_y, set1_x_lr, k_fold, gammas, fonction=4)\n",
    "w_lr1, loss_lr = logistic_regression(set1_y, set1_x, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"logistic regression loss {loss}\".format(loss=loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=40312.053727005376\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00272121]\n",
      " [-0.01162471]\n",
      " [-0.00730655]\n",
      " ...\n",
      " [-0.00529544]\n",
      " [ 0.00114649]\n",
      " [-0.00308696]]\n",
      "t [[ 0.00272121]\n",
      " [-0.01162471]\n",
      " [-0.00730655]\n",
      " ...\n",
      " [-0.00529544]\n",
      " [ 0.00114649]\n",
      " [-0.00308696]]\n",
      "t [[ 0.00540711]\n",
      " [-0.02312815]\n",
      " [-0.01453338]\n",
      " ...\n",
      " [-0.0105242 ]\n",
      " [ 0.00226895]\n",
      " [-0.00615564]]\n",
      "t [[ 0.00540711]\n",
      " [-0.02312815]\n",
      " [-0.01453338]\n",
      " ...\n",
      " [-0.0105242 ]\n",
      " [ 0.00226895]\n",
      " [-0.00615564]]\n",
      "Current iteration=2, loss=40198.00366476075\n",
      "t [[ 0.00805816]\n",
      " [-0.03451193]\n",
      " [-0.0216816 ]\n",
      " ...\n",
      " [-0.01568729]\n",
      " [ 0.00336768]\n",
      " [-0.00920621]]\n",
      "t [[ 0.00805816]\n",
      " [-0.03451193]\n",
      " [-0.0216816 ]\n",
      " ...\n",
      " [-0.01568729]\n",
      " [ 0.00336768]\n",
      " [-0.00920621]]\n",
      "t [[ 0.01067482]\n",
      " [-0.04577763]\n",
      " [-0.02875233]\n",
      " ...\n",
      " [-0.02078566]\n",
      " [ 0.00444299]\n",
      " [-0.01223884]]\n",
      "t [[ 0.01067482]\n",
      " [-0.04577763]\n",
      " [-0.02875233]\n",
      " ...\n",
      " [-0.02078566]\n",
      " [ 0.00444299]\n",
      " [-0.01223884]]\n",
      "Current iteration=4, loss=40087.6535873823\n",
      "t [[ 0.01325756]\n",
      " [-0.05692684]\n",
      " [-0.03574666]\n",
      " ...\n",
      " [-0.02582029]\n",
      " [ 0.0054952 ]\n",
      " [-0.01525369]]\n",
      "t [[ 0.01325756]\n",
      " [-0.05692684]\n",
      " [-0.03574666]\n",
      " ...\n",
      " [-0.02582029]\n",
      " [ 0.0054952 ]\n",
      " [-0.01525369]]\n",
      "t [[ 0.01580683]\n",
      " [-0.0679611 ]\n",
      " [-0.04266567]\n",
      " ...\n",
      " [-0.03079212]\n",
      " [ 0.0065246 ]\n",
      " [-0.01825094]]\n",
      "t [[ 0.01580683]\n",
      " [-0.0679611 ]\n",
      " [-0.04266567]\n",
      " ...\n",
      " [-0.03079212]\n",
      " [ 0.0065246 ]\n",
      " [-0.01825094]]\n",
      "Current iteration=6, loss=39980.82813214355\n",
      "t [[ 0.01832307]\n",
      " [-0.07888196]\n",
      " [-0.04951042]\n",
      " ...\n",
      " [-0.0357021 ]\n",
      " [ 0.0075315 ]\n",
      " [-0.02123075]]\n",
      "t [[ 0.01832307]\n",
      " [-0.07888196]\n",
      " [-0.04951042]\n",
      " ...\n",
      " [-0.0357021 ]\n",
      " [ 0.0075315 ]\n",
      " [-0.02123075]]\n",
      "t [[ 0.02080673]\n",
      " [-0.08969094]\n",
      " [-0.05628198]\n",
      " ...\n",
      " [-0.04055115]\n",
      " [ 0.0085162 ]\n",
      " [-0.02419329]]\n",
      "t [[ 0.02080673]\n",
      " [-0.08969094]\n",
      " [-0.05628198]\n",
      " ...\n",
      " [-0.04055115]\n",
      " [ 0.0085162 ]\n",
      " [-0.02419329]]\n",
      "Current iteration=8, loss=39877.36079180516\n",
      "t [[ 0.02325826]\n",
      " [-0.10038955]\n",
      " [-0.0629814 ]\n",
      " ...\n",
      " [-0.0453402 ]\n",
      " [ 0.00947899]\n",
      " [-0.02713871]]\n",
      "t [[ 0.02325826]\n",
      " [-0.10038955]\n",
      " [-0.0629814 ]\n",
      " ...\n",
      " [-0.0453402 ]\n",
      " [ 0.00947899]\n",
      " [-0.02713871]]\n",
      "t [[ 0.02567809]\n",
      " [-0.11097928]\n",
      " [-0.0696097 ]\n",
      " ...\n",
      " [-0.05007015]\n",
      " [ 0.01042016]\n",
      " [-0.03006718]]\n",
      "loss=39777.09357226104\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00035886]\n",
      " [-0.00220943]\n",
      " [-0.00692679]\n",
      " ...\n",
      " [-0.00531547]\n",
      " [ 0.00103336]\n",
      " [-0.00315337]]\n",
      "t [[ 0.00035886]\n",
      " [-0.00220943]\n",
      " [-0.00692679]\n",
      " ...\n",
      " [-0.00531547]\n",
      " [ 0.00103336]\n",
      " [-0.00315337]]\n",
      "t [[ 0.00070549]\n",
      " [-0.00443756]\n",
      " [-0.01377177]\n",
      " ...\n",
      " [-0.01056283]\n",
      " [ 0.00204325]\n",
      " [-0.00628805]]\n",
      "t [[ 0.00070549]\n",
      " [-0.00443756]\n",
      " [-0.01377177]\n",
      " ...\n",
      " [-0.01056283]\n",
      " [ 0.00204325]\n",
      " [-0.00628805]]\n",
      "Current iteration=2, loss=40196.658833863534\n",
      "t [[ 0.00104011]\n",
      " [-0.00668398]\n",
      " [-0.02053612]\n",
      " ...\n",
      " [-0.0157431 ]\n",
      " [ 0.00302998]\n",
      " [-0.0094042 ]]\n",
      "t [[ 0.00104011]\n",
      " [-0.00668398]\n",
      " [-0.02053612]\n",
      " ...\n",
      " [-0.0157431 ]\n",
      " [ 0.00302998]\n",
      " [-0.0094042 ]]\n",
      "t [[ 0.00136293]\n",
      " [-0.00894827]\n",
      " [-0.02722105]\n",
      " ...\n",
      " [-0.02085728]\n",
      " [ 0.00399386]\n",
      " [-0.01250201]]\n",
      "t [[ 0.00136293]\n",
      " [-0.00894827]\n",
      " [-0.02722105]\n",
      " ...\n",
      " [-0.02085728]\n",
      " [ 0.00399386]\n",
      " [-0.01250201]]\n",
      "Current iteration=4, loss=40085.01560997811\n",
      "t [[ 0.00167414]\n",
      " [-0.01123001]\n",
      " [-0.03382771]\n",
      " ...\n",
      " [-0.02590637]\n",
      " [ 0.0049352 ]\n",
      " [-0.01558165]]\n",
      "t [[ 0.00167414]\n",
      " [-0.01123001]\n",
      " [-0.03382771]\n",
      " ...\n",
      " [-0.02590637]\n",
      " [ 0.0049352 ]\n",
      " [-0.01558165]]\n",
      "t [[ 0.00197396]\n",
      " [-0.0135288 ]\n",
      " [-0.04035728]\n",
      " ...\n",
      " [-0.03089135]\n",
      " [ 0.0058543 ]\n",
      " [-0.01864329]]\n",
      "t [[ 0.00197396]\n",
      " [-0.0135288 ]\n",
      " [-0.04035728]\n",
      " ...\n",
      " [-0.03089135]\n",
      " [ 0.0058543 ]\n",
      " [-0.01864329]]\n",
      "Current iteration=6, loss=39976.94450690044\n",
      "t [[ 0.00226259]\n",
      " [-0.01584424]\n",
      " [-0.04681089]\n",
      " ...\n",
      " [-0.03581319]\n",
      " [ 0.00675147]\n",
      " [-0.0216871 ]]\n",
      "t [[ 0.00226259]\n",
      " [-0.01584424]\n",
      " [-0.04681089]\n",
      " ...\n",
      " [-0.03581319]\n",
      " [ 0.00675147]\n",
      " [-0.0216871 ]]\n",
      "t [[ 0.00254021]\n",
      " [-0.01817594]\n",
      " [-0.05318968]\n",
      " ...\n",
      " [-0.04067285]\n",
      " [ 0.007627  ]\n",
      " [-0.02471325]]\n",
      "t [[ 0.00254021]\n",
      " [-0.01817594]\n",
      " [-0.05318968]\n",
      " ...\n",
      " [-0.04067285]\n",
      " [ 0.007627  ]\n",
      " [-0.02471325]]\n",
      "Current iteration=8, loss=39872.27515785223\n",
      "t [[ 0.00280703]\n",
      " [-0.0205235 ]\n",
      " [-0.05949477]\n",
      " ...\n",
      " [-0.04547127]\n",
      " [ 0.00848119]\n",
      " [-0.02772191]]\n",
      "t [[ 0.00280703]\n",
      " [-0.0205235 ]\n",
      " [-0.05949477]\n",
      " ...\n",
      " [-0.04547127]\n",
      " [ 0.00848119]\n",
      " [-0.02772191]]\n",
      "t [[ 0.00306325]\n",
      " [-0.02288654]\n",
      " [-0.06572726]\n",
      " ...\n",
      " [-0.0502094 ]\n",
      " [ 0.00931433]\n",
      " [-0.03071324]]\n",
      "loss=39770.84601717691\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0003065 ]\n",
      " [-0.00193913]\n",
      " [-0.00700181]\n",
      " ...\n",
      " [-0.00522658]\n",
      " [ 0.00111107]\n",
      " [-0.00295788]]\n",
      "t [[ 0.0003065 ]\n",
      " [-0.00193913]\n",
      " [-0.00700181]\n",
      " ...\n",
      " [-0.00522658]\n",
      " [ 0.00111107]\n",
      " [-0.00295788]]\n",
      "t [[ 0.0006009 ]\n",
      " [-0.00389783]\n",
      " [-0.01392198]\n",
      " ...\n",
      " [-0.01038598]\n",
      " [ 0.00219843]\n",
      " [-0.00589769]]\n",
      "t [[ 0.0006009 ]\n",
      " [-0.00389783]\n",
      " [-0.01392198]\n",
      " ...\n",
      " [-0.01038598]\n",
      " [ 0.00219843]\n",
      " [-0.00589769]]\n",
      "Current iteration=2, loss=40197.86056761726\n",
      "t [[ 0.00088341]\n",
      " [-0.00587567]\n",
      " [-0.02076168]\n",
      " ...\n",
      " [-0.0154792 ]\n",
      " [ 0.00326241]\n",
      " [-0.0088196 ]]\n",
      "t [[ 0.00088341]\n",
      " [-0.00587567]\n",
      " [-0.02076168]\n",
      " ...\n",
      " [-0.0154792 ]\n",
      " [ 0.00326241]\n",
      " [-0.0088196 ]]\n",
      "t [[ 0.00115424]\n",
      " [-0.00787225]\n",
      " [-0.0275221 ]\n",
      " ...\n",
      " [-0.02050722]\n",
      " [ 0.00430331]\n",
      " [-0.01172379]]\n",
      "t [[ 0.00115424]\n",
      " [-0.00787225]\n",
      " [-0.0275221 ]\n",
      " ...\n",
      " [-0.02050722]\n",
      " [ 0.00430331]\n",
      " [-0.01172379]]\n",
      "Current iteration=4, loss=40087.38858871582\n",
      "t [[ 0.00141359]\n",
      " [-0.00988714]\n",
      " [-0.0342044 ]\n",
      " ...\n",
      " [-0.02547104]\n",
      " [ 0.00532143]\n",
      " [-0.01461042]]\n",
      "t [[ 0.00141359]\n",
      " [-0.00988714]\n",
      " [-0.0342044 ]\n",
      " ...\n",
      " [-0.02547104]\n",
      " [ 0.00532143]\n",
      " [-0.01461042]]\n",
      "t [[ 0.00166168]\n",
      " [-0.01191993]\n",
      " [-0.04080973]\n",
      " ...\n",
      " [-0.03037161]\n",
      " [ 0.00631708]\n",
      " [-0.01747967]]\n",
      "t [[ 0.00166168]\n",
      " [-0.01191993]\n",
      " [-0.04080973]\n",
      " ...\n",
      " [-0.03037161]\n",
      " [ 0.00631708]\n",
      " [-0.01747967]]\n",
      "Current iteration=6, loss=39980.460229434175\n",
      "t [[ 0.00189869]\n",
      " [-0.01397024]\n",
      " [-0.04733922]\n",
      " ...\n",
      " [-0.0352099 ]\n",
      " [ 0.00729057]\n",
      " [-0.0203317 ]]\n",
      "t [[ 0.00189869]\n",
      " [-0.01397024]\n",
      " [-0.04733922]\n",
      " ...\n",
      " [-0.0352099 ]\n",
      " [ 0.00729057]\n",
      " [-0.0203317 ]]\n",
      "t [[ 0.00212483]\n",
      " [-0.01603765]\n",
      " [-0.05379399]\n",
      " ...\n",
      " [-0.03998684]\n",
      " [ 0.00824219]\n",
      " [-0.02316668]]\n",
      "t [[ 0.00212483]\n",
      " [-0.01603765]\n",
      " [-0.05379399]\n",
      " ...\n",
      " [-0.03998684]\n",
      " [ 0.00824219]\n",
      " [-0.02316668]]\n",
      "Current iteration=8, loss=39876.906972706725\n",
      "t [[ 0.0023403 ]\n",
      " [-0.01812179]\n",
      " [-0.06017515]\n",
      " ...\n",
      " [-0.04470338]\n",
      " [ 0.00917223]\n",
      " [-0.02598477]]\n",
      "t [[ 0.0023403 ]\n",
      " [-0.01812179]\n",
      " [-0.06017515]\n",
      " ...\n",
      " [-0.04470338]\n",
      " [ 0.00917223]\n",
      " [-0.02598477]]\n",
      "t [[ 0.00254529]\n",
      " [-0.02022225]\n",
      " [-0.06648381]\n",
      " ...\n",
      " [-0.04936042]\n",
      " [ 0.010081  ]\n",
      " [-0.02878614]]\n",
      "loss=39776.568989900385\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00025526]\n",
      " [-0.00205387]\n",
      " [-0.00695561]\n",
      " ...\n",
      " [-0.00306238]\n",
      " [-0.00036882]\n",
      " [-0.00624774]]\n",
      "t [[ 0.00025526]\n",
      " [-0.00205387]\n",
      " [-0.00695561]\n",
      " ...\n",
      " [-0.00306238]\n",
      " [-0.00036882]\n",
      " [-0.00624774]]\n",
      "t [[ 0.00049865]\n",
      " [-0.00412718]\n",
      " [-0.0138289 ]\n",
      " ...\n",
      " [-0.00607199]\n",
      " [-0.0007202 ]\n",
      " [-0.01246479]]\n",
      "t [[ 0.00049865]\n",
      " [-0.00412718]\n",
      " [-0.0138289 ]\n",
      " ...\n",
      " [-0.00607199]\n",
      " [-0.0007202 ]\n",
      " [-0.01246479]]\n",
      "Current iteration=2, loss=40195.82388529231\n",
      "t [[ 0.00073039]\n",
      " [-0.0062195 ]\n",
      " [-0.02062105]\n",
      " ...\n",
      " [-0.00902969]\n",
      " [-0.00105448]\n",
      " [-0.01865138]]\n",
      "t [[ 0.00073039]\n",
      " [-0.0062195 ]\n",
      " [-0.02062105]\n",
      " ...\n",
      " [-0.00902969]\n",
      " [-0.00105448]\n",
      " [-0.01865138]]\n",
      "t [[ 0.00095068]\n",
      " [-0.00833041]\n",
      " [-0.02733328]\n",
      " ...\n",
      " [-0.01193631]\n",
      " [-0.001372  ]\n",
      " [-0.02480777]]\n",
      "t [[ 0.00095068]\n",
      " [-0.00833041]\n",
      " [-0.02733328]\n",
      " ...\n",
      " [-0.01193631]\n",
      " [-0.001372  ]\n",
      " [-0.02480777]]\n",
      "Current iteration=4, loss=40083.39157744605\n",
      "t [[ 0.00115974]\n",
      " [-0.0104595 ]\n",
      " [-0.03396674]\n",
      " ...\n",
      " [-0.01479268]\n",
      " [-0.0016731 ]\n",
      " [-0.03093421]]\n",
      "t [[ 0.00115974]\n",
      " [-0.0104595 ]\n",
      " [-0.03396674]\n",
      " ...\n",
      " [-0.01479268]\n",
      " [-0.0016731 ]\n",
      " [-0.03093421]]\n",
      "t [[ 0.00135776]\n",
      " [-0.01260635]\n",
      " [-0.04052262]\n",
      " ...\n",
      " [-0.01759961]\n",
      " [-0.00195811]\n",
      " [-0.03703097]]\n",
      "t [[ 0.00135776]\n",
      " [-0.01260635]\n",
      " [-0.04052262]\n",
      " ...\n",
      " [-0.01759961]\n",
      " [-0.00195811]\n",
      " [-0.03703097]]\n",
      "Current iteration=6, loss=39974.57497261882\n",
      "t [[ 0.00154494]\n",
      " [-0.01477057]\n",
      " [-0.04700205]\n",
      " ...\n",
      " [-0.02035793]\n",
      " [-0.00222736]\n",
      " [-0.04309827]]\n",
      "t [[ 0.00154494]\n",
      " [-0.01477057]\n",
      " [-0.04700205]\n",
      " ...\n",
      " [-0.02035793]\n",
      " [-0.00222736]\n",
      " [-0.04309827]]\n",
      "t [[ 0.00172149]\n",
      " [-0.01695174]\n",
      " [-0.05340617]\n",
      " ...\n",
      " [-0.02306842]\n",
      " [-0.00248116]\n",
      " [-0.04913637]]\n",
      "t [[ 0.00172149]\n",
      " [-0.01695174]\n",
      " [-0.05340617]\n",
      " ...\n",
      " [-0.02306842]\n",
      " [-0.00248116]\n",
      " [-0.04913637]]\n",
      "Current iteration=8, loss=39869.201521874435\n",
      "t [[ 0.0018876 ]\n",
      " [-0.01914949]\n",
      " [-0.05973611]\n",
      " ...\n",
      " [-0.02573188]\n",
      " [-0.00271985]\n",
      " [-0.05514551]]\n",
      "t [[ 0.0018876 ]\n",
      " [-0.01914949]\n",
      " [-0.05973611]\n",
      " ...\n",
      " [-0.02573188]\n",
      " [-0.00271985]\n",
      " [-0.05514551]]\n",
      "t [[ 0.00204346]\n",
      " [-0.02136341]\n",
      " [-0.06599298]\n",
      " ...\n",
      " [-0.02834907]\n",
      " [-0.00294372]\n",
      " [-0.06112593]]\n",
      "loss=39767.1075941708\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00544243]\n",
      " [-0.02324942]\n",
      " [-0.0146131 ]\n",
      " ...\n",
      " [-0.01059087]\n",
      " [ 0.00229298]\n",
      " [-0.00617392]]\n",
      "t [[ 0.00544243]\n",
      " [-0.02324942]\n",
      " [-0.0146131 ]\n",
      " ...\n",
      " [-0.01059087]\n",
      " [ 0.00229298]\n",
      " [-0.00617392]]\n",
      "t [[ 0.01074359]\n",
      " [-0.04601378]\n",
      " [-0.02890733]\n",
      " ...\n",
      " [-0.02091508]\n",
      " [ 0.00448982]\n",
      " [-0.01227472]]\n",
      "t [[ 0.01074359]\n",
      " [-0.04601378]\n",
      " [-0.02890733]\n",
      " ...\n",
      " [-0.02091508]\n",
      " [ 0.00448982]\n",
      " [-0.01227472]]\n",
      "Current iteration=2, loss=40086.76187560591\n",
      "t [[ 0.01590726]\n",
      " [-0.06830599]\n",
      " [-0.04289167]\n",
      " ...\n",
      " [-0.03098053]\n",
      " [ 0.00659304]\n",
      " [-0.01830376]]\n",
      "t [[ 0.01590726]\n",
      " [-0.06830599]\n",
      " [-0.04289167]\n",
      " ...\n",
      " [-0.03098053]\n",
      " [ 0.00659304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-0.01830376]]\n",
      "t [[ 0.02093711]\n",
      " [-0.09013868]\n",
      " [-0.0565749 ]\n",
      " ...\n",
      " [-0.04079496]\n",
      " [ 0.00860508]\n",
      " [-0.02426241]]\n",
      "t [[ 0.02093711]\n",
      " [-0.09013868]\n",
      " [-0.0565749 ]\n",
      " ...\n",
      " [-0.04079496]\n",
      " [ 0.00860508]\n",
      " [-0.02426241]]\n",
      "Current iteration=4, loss=39875.74092270465\n",
      "t [[ 0.02583675]\n",
      " [-0.11152421]\n",
      " [-0.06996562]\n",
      " ...\n",
      " [-0.05036591]\n",
      " [ 0.01052838]\n",
      " [-0.030152  ]]\n",
      "t [[ 0.02583675]\n",
      " [-0.11152421]\n",
      " [-0.06996562]\n",
      " ...\n",
      " [-0.05036591]\n",
      " [ 0.01052838]\n",
      " [-0.030152  ]]\n",
      "t [[ 0.03060971]\n",
      " [-0.13247467]\n",
      " [-0.08307219]\n",
      " ...\n",
      " [-0.05970074]\n",
      " [ 0.0123653 ]\n",
      " [-0.03597383]]\n",
      "t [[ 0.03060971]\n",
      " [-0.13247467]\n",
      " [-0.08307219]\n",
      " ...\n",
      " [-0.05970074]\n",
      " [ 0.0123653 ]\n",
      " [-0.03597383]]\n",
      "Current iteration=6, loss=39677.66652901402\n",
      "t [[ 0.03525944]\n",
      " [-0.15300186]\n",
      " [-0.0959028 ]\n",
      " ...\n",
      " [-0.06880664]\n",
      " [ 0.01411818]\n",
      " [-0.04172919]]\n",
      "t [[ 0.03525944]\n",
      " [-0.15300186]\n",
      " [-0.0959028 ]\n",
      " ...\n",
      " [-0.06880664]\n",
      " [ 0.01411818]\n",
      " [-0.04172919]]\n",
      "t [[ 0.0397893 ]\n",
      " [-0.17311728]\n",
      " [-0.1084654 ]\n",
      " ...\n",
      " [-0.07769059]\n",
      " [ 0.01578927]\n",
      " [-0.04741934]]\n",
      "t [[ 0.0397893 ]\n",
      " [-0.17311728]\n",
      " [-0.1084654 ]\n",
      " ...\n",
      " [-0.07769059]\n",
      " [ 0.01578927]\n",
      " [-0.04741934]]\n",
      "Current iteration=8, loss=39491.34841084758\n",
      "t [[ 0.04420259]\n",
      " [-0.19283215]\n",
      " [-0.12076777]\n",
      " ...\n",
      " [-0.08635939]\n",
      " [ 0.0173808 ]\n",
      " [-0.0530455 ]]\n",
      "t [[ 0.04420259]\n",
      " [-0.19283215]\n",
      " [-0.12076777]\n",
      " ...\n",
      " [-0.08635939]\n",
      " [ 0.0173808 ]\n",
      " [-0.0530455 ]]\n",
      "t [[ 0.0485025 ]\n",
      " [-0.21215739]\n",
      " [-0.13281743]\n",
      " ...\n",
      " [-0.09481964]\n",
      " [ 0.01889494]\n",
      " [-0.05860889]]\n",
      "loss=39315.718832111714\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00071771]\n",
      " [-0.00441886]\n",
      " [-0.01385358]\n",
      " ...\n",
      " [-0.01063094]\n",
      " [ 0.00206673]\n",
      " [-0.00630675]]\n",
      "t [[ 0.00071771]\n",
      " [-0.00441886]\n",
      " [-0.01385358]\n",
      " ...\n",
      " [-0.01063094]\n",
      " [ 0.00206673]\n",
      " [-0.00630675]]\n",
      "t [[ 0.00138654]\n",
      " [-0.00891254]\n",
      " [-0.02737991]\n",
      " ...\n",
      " [-0.02098945]\n",
      " [ 0.00403957]\n",
      " [-0.0125387 ]]\n",
      "t [[ 0.00138654]\n",
      " [-0.00891254]\n",
      " [-0.02737991]\n",
      " ...\n",
      " [-0.02098945]\n",
      " [ 0.00403957]\n",
      " [-0.0125387 ]]\n",
      "Current iteration=2, loss=40084.11177741806\n",
      "t [[ 0.00200816]\n",
      " [-0.01347765]\n",
      " [-0.04058861]\n",
      " ...\n",
      " [-0.03108371]\n",
      " [ 0.00592103]\n",
      " [-0.01869729]]\n",
      "t [[ 0.00200816]\n",
      " [-0.01347765]\n",
      " [-0.04058861]\n",
      " ...\n",
      " [-0.03108371]\n",
      " [ 0.00592103]\n",
      " [-0.01869729]]\n",
      "t [[ 0.00258424]\n",
      " [-0.01811089]\n",
      " [-0.05348911]\n",
      " ...\n",
      " [-0.04092168]\n",
      " [ 0.0077136 ]\n",
      " [-0.02478391]]\n",
      "t [[ 0.00258424]\n",
      " [-0.01811089]\n",
      " [-0.05348911]\n",
      " ...\n",
      " [-0.04092168]\n",
      " [ 0.0077136 ]\n",
      " [-0.02478391]]\n",
      "Current iteration=4, loss=39870.63480446351\n",
      "t [[ 0.00311637]\n",
      " [-0.02280905]\n",
      " [-0.06609059]\n",
      " ...\n",
      " [-0.05051116]\n",
      " [ 0.0094197 ]\n",
      " [-0.03079993]]\n",
      "t [[ 0.00311637]\n",
      " [-0.02280905]\n",
      " [-0.06609059]\n",
      " ...\n",
      " [-0.05051116]\n",
      " [ 0.0094197 ]\n",
      " [-0.03079993]]\n",
      "t [[ 0.00360613]\n",
      " [-0.027569  ]\n",
      " [-0.07840203]\n",
      " ...\n",
      " [-0.05985975]\n",
      " [ 0.01104171]\n",
      " [-0.0367467 ]]\n",
      "t [[ 0.00360613]\n",
      " [-0.027569  ]\n",
      " [-0.07840203]\n",
      " ...\n",
      " [-0.05985975]\n",
      " [ 0.01104171]\n",
      " [-0.0367467 ]]\n",
      "Current iteration=6, loss=39670.267882845335\n",
      "t [[ 0.00405506]\n",
      " [-0.03238772]\n",
      " [-0.09043218]\n",
      " ...\n",
      " [-0.06897484]\n",
      " [ 0.01258196]\n",
      " [-0.04262554]]\n",
      "t [[ 0.00405506]\n",
      " [-0.03238772]\n",
      " [-0.09043218]\n",
      " ...\n",
      " [-0.06897484]\n",
      " [ 0.01258196]\n",
      " [-0.04262554]]\n",
      "t [[ 0.00446463]\n",
      " [-0.03726226]\n",
      " [-0.10218953]\n",
      " ...\n",
      " [-0.07786365]\n",
      " [ 0.01404271]\n",
      " [-0.04843775]]\n",
      "t [[ 0.00446463]\n",
      " [-0.03726226]\n",
      " [-0.10218953]\n",
      " ...\n",
      " [-0.07786365]\n",
      " [ 0.01404271]\n",
      " [-0.04843775]]\n",
      "Current iteration=8, loss=39481.79494711808\n",
      "t [[ 0.00483631]\n",
      " [-0.04218977]\n",
      " [-0.11368237]\n",
      " ...\n",
      " [-0.08653317]\n",
      " [ 0.01542619]\n",
      " [-0.05418459]]\n",
      "t [[ 0.00483631]\n",
      " [-0.04218977]\n",
      " [-0.11368237]\n",
      " ...\n",
      " [-0.08653317]\n",
      " [ 0.01542619]\n",
      " [-0.05418459]]\n",
      "t [[ 0.00517149]\n",
      " [-0.04716749]\n",
      " [-0.12491874]\n",
      " ...\n",
      " [-0.09499021]\n",
      " [ 0.01673456]\n",
      " [-0.05986731]]\n",
      "loss=39304.126686231066\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00061301]\n",
      " [-0.00387826]\n",
      " [-0.01400362]\n",
      " ...\n",
      " [-0.01045316]\n",
      " [ 0.00222214]\n",
      " [-0.00591575]]\n",
      "t [[ 0.00061301]\n",
      " [-0.00387826]\n",
      " [-0.01400362]\n",
      " ...\n",
      " [-0.01045316]\n",
      " [ 0.00222214]\n",
      " [-0.00591575]]\n",
      "t [[ 0.00117761]\n",
      " [-0.00783479]\n",
      " [-0.02768067]\n",
      " ...\n",
      " [-0.0206376 ]\n",
      " [ 0.00434947]\n",
      " [-0.01175924]]\n",
      "t [[ 0.00117761]\n",
      " [-0.00783479]\n",
      " [-0.02768067]\n",
      " ...\n",
      " [-0.0206376 ]\n",
      " [ 0.00434947]\n",
      " [-0.01175924]]\n",
      "Current iteration=2, loss=40086.492001289604\n",
      "t [[ 0.0016955 ]\n",
      " [-0.0118662 ]\n",
      " [-0.04104068]\n",
      " ...\n",
      " [-0.03056137]\n",
      " [ 0.00638451]\n",
      " [-0.01753183]]\n",
      "t [[ 0.0016955 ]\n",
      " [-0.0118662 ]\n",
      " [-0.04104068]\n",
      " ...\n",
      " [-0.03056137]\n",
      " [ 0.00638451]\n",
      " [-0.01753183]]\n",
      "t [[ 0.00216834]\n",
      " [-0.01596918]\n",
      " [-0.05409299]\n",
      " ...\n",
      " [-0.04023233]\n",
      " [ 0.00832973]\n",
      " [-0.02323492]]\n",
      "t [[ 0.00216834]\n",
      " [-0.01596918]\n",
      " [-0.05409299]\n",
      " ...\n",
      " [-0.04023233]\n",
      " [ 0.00832973]\n",
      " [-0.02323492]]\n",
      "Current iteration=4, loss=39875.279316458145\n",
      "t [[ 0.00259775]\n",
      " [-0.0201405 ]\n",
      " [-0.0668467 ]\n",
      " ...\n",
      " [-0.04965813]\n",
      " [ 0.01018754]\n",
      " [-0.02886983]]\n",
      "t [[ 0.00259775]\n",
      " [-0.0201405 ]\n",
      " [-0.0668467 ]\n",
      " ...\n",
      " [-0.04965813]\n",
      " [ 0.01018754]\n",
      " [-0.02886983]]\n",
      "t [[ 0.00298532]\n",
      " [-0.02437703]\n",
      " [-0.07931071]\n",
      " ...\n",
      " [-0.05884627]\n",
      " [ 0.01196031]\n",
      " [-0.03443789]]\n",
      "t [[ 0.00298532]\n",
      " [-0.02437703]\n",
      " [-0.07931071]\n",
      " ...\n",
      " [-0.05884627]\n",
      " [ 0.01196031]\n",
      " [-0.03443789]]\n",
      "Current iteration=6, loss=39677.075424830764\n",
      "t [[ 0.00333258]\n",
      " [-0.02867572]\n",
      " [-0.09149367]\n",
      " ...\n",
      " [-0.06780403]\n",
      " [ 0.01365038]\n",
      " [-0.0399404 ]]\n",
      "t [[ 0.00333258]\n",
      " [-0.02867572]\n",
      " [-0.09149367]\n",
      " ...\n",
      " [-0.06780403]\n",
      " [ 0.01365038]\n",
      " [-0.0399404 ]]\n",
      "t [[ 0.00364102]\n",
      " [-0.03303362]\n",
      " [-0.10340402]\n",
      " ...\n",
      " [-0.0765385 ]\n",
      " [ 0.01525999]\n",
      " [-0.04537862]]\n",
      "t [[ 0.00364102]\n",
      " [-0.03303362]\n",
      " [-0.10340402]\n",
      " ...\n",
      " [-0.0765385 ]\n",
      " [ 0.01525999]\n",
      " [-0.04537862]]\n",
      "Current iteration=8, loss=39490.6768449918\n",
      "t [[ 0.00391211]\n",
      " [-0.03744786]\n",
      " [-0.11504996]\n",
      " ...\n",
      " [-0.08505659]\n",
      " [ 0.01679137]\n",
      " [-0.0507538 ]]\n",
      "t [[ 0.00391211]\n",
      " [-0.03744786]\n",
      " [-0.11504996]\n",
      " ...\n",
      " [-0.08505659]\n",
      " [ 0.01679137]\n",
      " [-0.0507538 ]]\n",
      "t [[ 0.00414726]\n",
      " [-0.04191566]\n",
      " [-0.12643947]\n",
      " ...\n",
      " [-0.093365  ]\n",
      " [ 0.01824666]\n",
      " [-0.05606716]]\n",
      "loss=39315.00496587191\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00051051]\n",
      " [-0.00410774]\n",
      " [-0.01391122]\n",
      " ...\n",
      " [-0.00612475]\n",
      " [-0.00073765]\n",
      " [-0.01249549]]\n",
      "t [[ 0.00051051]\n",
      " [-0.00410774]\n",
      " [-0.01391122]\n",
      " ...\n",
      " [-0.00612475]\n",
      " [-0.00073765]\n",
      " [-0.01249549]]\n",
      "t [[ 0.00097357]\n",
      " [-0.00829323]\n",
      " [-0.02749315]\n",
      " ...\n",
      " [-0.01203846]\n",
      " [-0.00140552]\n",
      " [-0.02486816]]\n",
      "t [[ 0.00097357]\n",
      " [-0.00829323]\n",
      " [-0.02749315]\n",
      " ...\n",
      " [-0.01203846]\n",
      " [-0.00140552]\n",
      " [-0.02486816]]\n",
      "Current iteration=2, loss=40082.47671227152\n",
      "t [[ 0.00139087]\n",
      " [-0.01255305]\n",
      " [-0.04075543]\n",
      " ...\n",
      " [-0.01774793]\n",
      " [-0.00200638]\n",
      " [-0.03712007]]\n",
      "t [[ 0.00139087]\n",
      " [-0.01255305]\n",
      " [-0.04075543]\n",
      " ...\n",
      " [-0.01774793]\n",
      " [-0.00200638]\n",
      " [-0.03712007]]\n",
      "t [[ 0.00176407]\n",
      " [-0.01688386]\n",
      " [-0.05370754]\n",
      " ...\n",
      " [-0.0232598 ]\n",
      " [-0.00254294]\n",
      " [-0.04925322]]\n",
      "t [[ 0.00176407]\n",
      " [-0.01688386]\n",
      " [-0.05370754]\n",
      " ...\n",
      " [-0.0232598 ]\n",
      " [-0.00254294]\n",
      " [-0.04925322]]\n",
      "Current iteration=4, loss=39867.541258390615\n",
      "t [[ 0.00209477]\n",
      " [-0.02128242]\n",
      " [-0.06635869]\n",
      " ...\n",
      " [-0.02858057]\n",
      " [-0.00301781]\n",
      " [-0.06126962]]\n",
      "t [[ 0.00209477]\n",
      " [-0.02128242]\n",
      " [-0.06635869]\n",
      " ...\n",
      " [-0.02858057]\n",
      " [-0.00301781]\n",
      " [-0.06126962]]\n",
      "t [[ 0.00238455]\n",
      " [-0.02574558]\n",
      " [-0.0787179 ]\n",
      " ...\n",
      " [-0.03371655]\n",
      " [-0.00343354]\n",
      " [-0.07317121]]\n",
      "t [[ 0.00238455]\n",
      " [-0.02574558]\n",
      " [-0.0787179 ]\n",
      " ...\n",
      " [-0.03371655]\n",
      " [-0.00343354]\n",
      " [-0.07317121]]\n",
      "Current iteration=6, loss=39665.875072568364\n",
      "t [[ 0.00263494]\n",
      " [-0.03027027]\n",
      " [-0.09079393]\n",
      " ...\n",
      " [-0.03867389]\n",
      " [-0.0037926 ]\n",
      " [-0.08495993]]\n",
      "t [[ 0.00263494]\n",
      " [-0.03027027]\n",
      " [-0.09079393]\n",
      " ...\n",
      " [-0.03867389]\n",
      " [-0.0037926 ]\n",
      " [-0.08495993]]\n",
      "t [[ 0.00284743]\n",
      " [-0.03485352]\n",
      " [-0.10259533]\n",
      " ...\n",
      " [-0.04345855]\n",
      " [-0.00409738]\n",
      " [-0.09663766]]\n",
      "t [[ 0.00284743]\n",
      " [-0.03485352]\n",
      " [-0.10259533]\n",
      " ...\n",
      " [-0.04345855]\n",
      " [-0.00409738]\n",
      " [-0.09663766]]\n",
      "Current iteration=8, loss=39476.24624524176\n",
      "t [[ 0.00302346]\n",
      " [-0.03949245]\n",
      " [-0.1141304 ]\n",
      " ...\n",
      " [-0.04807635]\n",
      " [-0.00435019]\n",
      " [-0.10820628]]\n",
      "t [[ 0.00302346]\n",
      " [-0.03949245]\n",
      " [-0.1141304 ]\n",
      " ...\n",
      " [-0.04807635]\n",
      " [-0.00435019]\n",
      " [-0.10820628]]\n",
      "t [[ 0.00316445]\n",
      " [-0.04418426]\n",
      " [-0.12540723]\n",
      " ...\n",
      " [-0.05253293]\n",
      " [-0.00455328]\n",
      " [-0.1196676 ]]\n",
      "loss=39297.551034674565\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00816364]\n",
      " [-0.03487413]\n",
      " [-0.02191964]\n",
      " ...\n",
      " [-0.01588631]\n",
      " [ 0.00343947]\n",
      " [-0.00926088]]\n",
      "t [[ 0.00816364]\n",
      " [-0.03487413]\n",
      " [-0.02191964]\n",
      " ...\n",
      " [-0.01588631]\n",
      " [ 0.00343947]\n",
      " [-0.00926088]]\n",
      "t [[ 0.01600946]\n",
      " [-0.06865695]\n",
      " [-0.04312188]\n",
      " ...\n",
      " [-0.03117266]\n",
      " [ 0.00666265]\n",
      " [-0.01835724]]\n",
      "t [[ 0.01600946]\n",
      " [-0.06865695]\n",
      " [-0.04312188]\n",
      " ...\n",
      " [-0.03117266]\n",
      " [ 0.00666265]\n",
      " [-0.01835724]]\n",
      "Current iteration=2, loss=39978.259418688\n",
      "t [[ 0.02355021]\n",
      " [-0.10139218]\n",
      " [-0.06363714]\n",
      " ...\n",
      " [-0.04588584]\n",
      " [ 0.00967803]\n",
      " [-0.02729374]]\n",
      "t [[ 0.02355021]\n",
      " [-0.10139218]\n",
      " [-0.06363714]\n",
      " ...\n",
      " [-0.04588584]\n",
      " [ 0.00967803]\n",
      " [-0.02729374]]\n",
      "t [[ 0.03079822]\n",
      " [-0.13312215]\n",
      " [-0.08349481]\n",
      " ...\n",
      " [-0.06005171]\n",
      " [ 0.01249391]\n",
      " [-0.03607492]]\n",
      "t [[ 0.03079822]\n",
      " [-0.13312215]\n",
      " [-0.08349481]\n",
      " ...\n",
      " [-0.06005171]\n",
      " [ 0.01249391]\n",
      " [-0.03607492]]\n",
      "Current iteration=4, loss=39675.425373273174\n",
      "t [[ 0.03776544]\n",
      " [-0.16388772]\n",
      " [-0.10272323]\n",
      " ...\n",
      " [-0.07369516]\n",
      " [ 0.0151183 ]\n",
      " [-0.04470523]]\n",
      "t [[ 0.03776544]\n",
      " [-0.16388772]\n",
      " [-0.10272323]\n",
      " ...\n",
      " [-0.07369516]\n",
      " [ 0.0151183 ]\n",
      " [-0.04470523]]\n",
      "t [[ 0.04446337]\n",
      " [-0.19372828]\n",
      " [-0.12134968]\n",
      " ...\n",
      " [-0.08684011]\n",
      " [ 0.01755898]\n",
      " [-0.05318894]]\n",
      "t [[ 0.04446337]\n",
      " [-0.19372828]\n",
      " [-0.12134968]\n",
      " ...\n",
      " [-0.08684011]\n",
      " [ 0.01755898]\n",
      " [-0.05318894]]\n",
      "Current iteration=6, loss=39399.338491418115\n",
      "t [[ 0.05090312]\n",
      " [-0.22268169]\n",
      " [-0.13940033]\n",
      " ...\n",
      " [-0.09950951]\n",
      " [ 0.01982343]\n",
      " [-0.06153024]]\n",
      "t [[ 0.05090312]\n",
      " [-0.22268169]\n",
      " [-0.13940033]\n",
      " ...\n",
      " [-0.09950951]\n",
      " [ 0.01982343]\n",
      " [-0.06153024]]\n",
      "t [[ 0.05709533]\n",
      " [-0.25078428]\n",
      " [-0.15690026]\n",
      " ...\n",
      " [-0.11172531]\n",
      " [ 0.02191887]\n",
      " [-0.06973313]]\n",
      "t [[ 0.05709533]\n",
      " [-0.25078428]\n",
      " [-0.15690026]\n",
      " ...\n",
      " [-0.11172531]\n",
      " [ 0.02191887]\n",
      " [-0.06973313]]\n",
      "Current iteration=8, loss=39146.422654069596\n",
      "t [[ 0.06305026]\n",
      " [-0.27807091]\n",
      " [-0.17387347]\n",
      " ...\n",
      " [-0.12350852]\n",
      " [ 0.02385222]\n",
      " [-0.07780152]]\n",
      "t [[ 0.06305026]\n",
      " [-0.27807091]\n",
      " [-0.17387347]\n",
      " ...\n",
      " [-0.12350852]\n",
      " [ 0.02385222]\n",
      " [-0.07780152]]\n",
      "t [[ 0.06877773]\n",
      " [-0.30457492]\n",
      " [-0.1903429 ]\n",
      " ...\n",
      " [-0.13487918]\n",
      " [ 0.02563013]\n",
      " [-0.08573917]]\n",
      "loss=38913.65118838852\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00107657]\n",
      " [-0.00662829]\n",
      " [-0.02078038]\n",
      " ...\n",
      " [-0.01594641]\n",
      " [ 0.00310009]\n",
      " [-0.00946012]]\n",
      "t [[ 0.00107657]\n",
      " [-0.00662829]\n",
      " [-0.02078038]\n",
      " ...\n",
      " [-0.01594641]\n",
      " [ 0.00310009]\n",
      " [-0.00946012]]\n",
      "t [[ 0.00204315]\n",
      " [-0.01342491]\n",
      " [-0.04082447]\n",
      " ...\n",
      " [-0.0312799 ]\n",
      " [ 0.00598895]\n",
      " [-0.01875197]]\n",
      "t [[ 0.00204315]\n",
      " [-0.01342491]\n",
      " [-0.04082447]\n",
      " ...\n",
      " [-0.0312799 ]\n",
      " [ 0.00598895]\n",
      " [-0.01875197]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2, loss=39974.341930955474\n",
      "t [[ 0.00290547]\n",
      " [-0.02037838]\n",
      " [-0.06016491]\n",
      " ...\n",
      " [-0.04602813]\n",
      " [ 0.00867511]\n",
      " [-0.02788038]]\n",
      "t [[ 0.00290547]\n",
      " [-0.02037838]\n",
      " [-0.06016491]\n",
      " ...\n",
      " [-0.04602813]\n",
      " [ 0.00867511]\n",
      " [-0.02788038]]\n",
      "t [[ 0.00366903]\n",
      " [-0.0274777 ]\n",
      " [-0.07883322]\n",
      " ...\n",
      " [-0.06021778]\n",
      " [ 0.01116689]\n",
      " [-0.03685002]]\n",
      "t [[ 0.00366903]\n",
      " [-0.0274777 ]\n",
      " [-0.07883322]\n",
      " ...\n",
      " [-0.06021778]\n",
      " [ 0.01116689]\n",
      " [-0.03685002]]\n",
      "Current iteration=4, loss=39668.000131877474\n",
      "t [[ 0.00433917]\n",
      " [-0.03471231]\n",
      " [-0.09685971]\n",
      " ...\n",
      " [-0.07387455]\n",
      " [ 0.01347233]\n",
      " [-0.04566546]]\n",
      "t [[ 0.00433917]\n",
      " [-0.03471231]\n",
      " [-0.09685971]\n",
      " ...\n",
      " [-0.07387455]\n",
      " [ 0.01347233]\n",
      " [-0.04566546]]\n",
      "t [[ 0.00492097]\n",
      " [-0.04207212]\n",
      " [-0.11427353]\n",
      " ...\n",
      " [-0.0870231 ]\n",
      " [ 0.01559921]\n",
      " [-0.05433114]]\n",
      "t [[ 0.00492097]\n",
      " [-0.04207212]\n",
      " [-0.11427353]\n",
      " ...\n",
      " [-0.0870231 ]\n",
      " [ 0.01559921]\n",
      " [-0.05433114]]\n",
      "Current iteration=6, loss=39388.721290518675\n",
      "t [[ 0.00541934]\n",
      " [-0.04954753]\n",
      " [-0.1311026 ]\n",
      " ...\n",
      " [-0.09968706]\n",
      " [ 0.01755503]\n",
      " [-0.06285132]]\n",
      "t [[ 0.00541934]\n",
      " [-0.04954753]\n",
      " [-0.1311026 ]\n",
      " ...\n",
      " [-0.09968706]\n",
      " [ 0.01755503]\n",
      " [-0.06285132]]\n",
      "t [[ 0.00583893]\n",
      " [-0.05712938]\n",
      " [-0.14737365]\n",
      " ...\n",
      " [-0.11188905]\n",
      " [ 0.01934697]\n",
      " [-0.07123017]]\n",
      "t [[ 0.00583893]\n",
      " [-0.05712938]\n",
      " [-0.14737365]\n",
      " ...\n",
      " [-0.11188905]\n",
      " [ 0.01934697]\n",
      " [-0.07123017]]\n",
      "Current iteration=8, loss=39132.857455462814\n",
      "t [[ 0.0061842 ]\n",
      " [-0.06480898]\n",
      " [-0.16311223]\n",
      " ...\n",
      " [-0.12365069]\n",
      " [ 0.02098197]\n",
      " [-0.07947168]]\n",
      "t [[ 0.0061842 ]\n",
      " [-0.06480898]\n",
      " [-0.16311223]\n",
      " ...\n",
      " [-0.12365069]\n",
      " [ 0.02098197]\n",
      " [-0.07947168]]\n",
      "t [[ 0.0064594 ]\n",
      " [-0.07257807]\n",
      " [-0.17834271]\n",
      " ...\n",
      " [-0.13499261]\n",
      " [ 0.02246665]\n",
      " [-0.08757971]]\n",
      "loss=38897.32783575143\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00091951]\n",
      " [-0.00581738]\n",
      " [-0.02100543]\n",
      " ...\n",
      " [-0.01567974]\n",
      " [ 0.0033332 ]\n",
      " [-0.00887363]]\n",
      "t [[ 0.00091951]\n",
      " [-0.00581738]\n",
      " [-0.02100543]\n",
      " ...\n",
      " [-0.01567974]\n",
      " [ 0.0033332 ]\n",
      " [-0.00887363]]\n",
      "t [[ 0.00173011]\n",
      " [-0.01181088]\n",
      " [-0.04127612]\n",
      " ...\n",
      " [-0.0307549 ]\n",
      " [ 0.00645311]\n",
      " [-0.01758466]]\n",
      "t [[ 0.00173011]\n",
      " [-0.01181088]\n",
      " [-0.04127612]\n",
      " ...\n",
      " [-0.0307549 ]\n",
      " [ 0.00645311]\n",
      " [-0.01758466]]\n",
      "Current iteration=2, loss=39977.87820184317\n",
      "t [[ 0.00243757]\n",
      " [-0.01796898]\n",
      " [-0.06084437]\n",
      " ...\n",
      " [-0.04525274]\n",
      " [ 0.00936825]\n",
      " [-0.02613779]]\n",
      "t [[ 0.00243757]\n",
      " [-0.01796898]\n",
      " [-0.06084437]\n",
      " ...\n",
      " [-0.04525274]\n",
      " [ 0.00936825]\n",
      " [-0.02613779]]\n",
      "t [[ 0.00304743]\n",
      " [-0.02428063]\n",
      " [-0.07974142]\n",
      " ...\n",
      " [-0.05919951]\n",
      " [ 0.0120869 ]\n",
      " [-0.03453762]]\n",
      "t [[ 0.00304743]\n",
      " [-0.02428063]\n",
      " [-0.07974142]\n",
      " ...\n",
      " [-0.05919951]\n",
      " [ 0.0120869 ]\n",
      " [-0.03453762]]\n",
      "Current iteration=4, loss=39674.824727488376\n",
      "t [[ 0.00356504]\n",
      " [-0.03073523]\n",
      " [-0.0979973 ]\n",
      " ...\n",
      " [-0.07262051]\n",
      " [ 0.01461708]\n",
      " [-0.04278862]]\n",
      "t [[ 0.00356504]\n",
      " [-0.03073523]\n",
      " [-0.0979973 ]\n",
      " ...\n",
      " [-0.07262051]\n",
      " [ 0.01461708]\n",
      " [-0.04278862]]\n",
      "t [[ 0.00399553]\n",
      " [-0.03732265]\n",
      " [-0.1156409 ]\n",
      " ...\n",
      " [-0.08554003]\n",
      " [ 0.01696654]\n",
      " [-0.05089513]]\n",
      "t [[ 0.00399553]\n",
      " [-0.03732265]\n",
      " [-0.1156409 ]\n",
      " ...\n",
      " [-0.08554003]\n",
      " [ 0.01696654]\n",
      " [-0.05089513]]\n",
      "Current iteration=6, loss=39398.631773760906\n",
      "t [[ 0.00434381]\n",
      " [-0.04403324]\n",
      " [-0.13269988]\n",
      " ...\n",
      " [-0.09798136]\n",
      " [ 0.01914275]\n",
      " [-0.05886135]]\n",
      "t [[ 0.00434381]\n",
      " [-0.04403324]\n",
      " [-0.13269988]\n",
      " ...\n",
      " [-0.09798136]\n",
      " [ 0.01914275]\n",
      " [-0.05886135]]\n",
      "t [[ 0.00461458]\n",
      " [-0.05085779]\n",
      " [-0.14920076]\n",
      " ...\n",
      " [-0.10996678]\n",
      " [ 0.0211529 ]\n",
      " [-0.06669134]]\n",
      "t [[ 0.00461458]\n",
      " [-0.05085779]\n",
      " [-0.14920076]\n",
      " ...\n",
      " [-0.10996678]\n",
      " [ 0.0211529 ]\n",
      " [-0.06669134]]\n",
      "Current iteration=8, loss=39145.687047774125\n",
      "t [[ 0.00481232]\n",
      " [-0.05778755]\n",
      " [-0.16516888]\n",
      " ...\n",
      " [-0.12151759]\n",
      " [ 0.02300387]\n",
      " [-0.07438902]]\n",
      "t [[ 0.00481232]\n",
      " [-0.05778755]\n",
      " [-0.16516888]\n",
      " ...\n",
      " [-0.12151759]\n",
      " [ 0.02300387]\n",
      " [-0.07438902]]\n",
      "t [[ 0.00494127]\n",
      " [-0.06481421]\n",
      " [-0.18062841]\n",
      " ...\n",
      " [-0.13265414]\n",
      " [ 0.02470229]\n",
      " [-0.08195817]]\n",
      "loss=38912.93728339799\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00076577]\n",
      " [-0.00616161]\n",
      " [-0.02086683]\n",
      " ...\n",
      " [-0.00918713]\n",
      " [-0.00110647]\n",
      " [-0.01874323]]\n",
      "t [[ 0.00076577]\n",
      " [-0.00616161]\n",
      " [-0.02086683]\n",
      " ...\n",
      " [-0.00918713]\n",
      " [-0.00110647]\n",
      " [-0.01874323]]\n",
      "t [[ 0.00142478]\n",
      " [-0.01249814]\n",
      " [-0.04099279]\n",
      " ...\n",
      " [-0.01789943]\n",
      " [-0.00205595]\n",
      " [-0.03721014]]\n",
      "t [[ 0.00142478]\n",
      " [-0.01249814]\n",
      " [-0.04099279]\n",
      " ...\n",
      " [-0.01789943]\n",
      " [-0.00205595]\n",
      " [-0.03721014]]\n",
      "Current iteration=2, loss=39971.940692641736\n",
      "t [[ 0.00198277]\n",
      " [-0.01899801]\n",
      " [-0.06041061]\n",
      " ...\n",
      " [-0.02615997]\n",
      " [-0.00285782]\n",
      " [-0.05540767]]\n",
      "t [[ 0.00198277]\n",
      " [-0.01899801]\n",
      " [-0.06041061]\n",
      " ...\n",
      " [-0.02615997]\n",
      " [-0.00285782]\n",
      " [-0.05540767]]\n",
      "t [[ 0.00244527]\n",
      " [-0.02565009]\n",
      " [-0.07915193]\n",
      " ...\n",
      " [-0.03399096]\n",
      " [-0.00352108]\n",
      " [-0.07334261]]\n",
      "t [[ 0.00244527]\n",
      " [-0.02565009]\n",
      " [-0.07915193]\n",
      " ...\n",
      " [-0.03399096]\n",
      " [-0.00352108]\n",
      " [-0.07334261]]\n",
      "Current iteration=4, loss=39663.579972889886\n",
      "t [[ 0.00281759]\n",
      " [-0.03244373]\n",
      " [-0.09724718]\n",
      " ...\n",
      " [-0.04141375]\n",
      " [-0.00405432]\n",
      " [-0.09102158]]\n",
      "t [[ 0.00281759]\n",
      " [-0.03244373]\n",
      " [-0.09724718]\n",
      " ...\n",
      " [-0.04141375]\n",
      " [-0.00405432]\n",
      " [-0.09102158]]\n",
      "t [[ 0.00310485]\n",
      " [-0.03936872]\n",
      " [-0.11472561]\n",
      " ...\n",
      " [-0.04844881]\n",
      " [-0.00446574]\n",
      " [-0.10845106]]\n",
      "t [[ 0.00310485]\n",
      " [-0.03936872]\n",
      " [-0.11472561]\n",
      " ...\n",
      " [-0.04844881]\n",
      " [-0.00446574]\n",
      " [-0.10845106]]\n",
      "Current iteration=6, loss=39382.60881969628\n",
      "t [[ 0.00331191]\n",
      " [-0.04641536]\n",
      " [-0.13161527]\n",
      " ...\n",
      " [-0.05511573]\n",
      " [-0.00476314]\n",
      " [-0.12563733]]\n",
      "t [[ 0.00331191]\n",
      " [-0.04641536]\n",
      " [-0.13161527]\n",
      " ...\n",
      " [-0.05511573]\n",
      " [-0.00476314]\n",
      " [-0.12563733]]\n",
      "t [[ 0.00344344]\n",
      " [-0.05357438]\n",
      " [-0.14794299]\n",
      " ...\n",
      " [-0.06143322]\n",
      " [-0.00495395]\n",
      " [-0.1425865 ]]\n",
      "t [[ 0.00344344]\n",
      " [-0.05357438]\n",
      " [-0.14794299]\n",
      " ...\n",
      " [-0.06143322]\n",
      " [-0.00495395]\n",
      " [-0.1425865 ]]\n",
      "Current iteration=8, loss=39125.3308788209\n",
      "t [[ 0.0035039 ]\n",
      " [-0.06083697]\n",
      " [-0.16373443]\n",
      " ...\n",
      " [-0.06741914]\n",
      " [-0.00504521]\n",
      " [-0.1593045 ]]\n",
      "t [[ 0.0035039 ]\n",
      " [-0.06083697]\n",
      " [-0.16373443]\n",
      " ...\n",
      " [-0.06741914]\n",
      " [-0.00504521]\n",
      " [-0.1593045 ]]\n",
      "t [[ 0.0034975 ]\n",
      " [-0.06819479]\n",
      " [-0.17901409]\n",
      " ...\n",
      " [-0.07309051]\n",
      " [-0.00504361]\n",
      " [-0.17579708]]\n",
      "loss=38888.62346074794\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01088485]\n",
      " [-0.04649883]\n",
      " [-0.02922619]\n",
      " ...\n",
      " [-0.02118174]\n",
      " [ 0.00458596]\n",
      " [-0.01234784]]\n",
      "t [[ 0.01088485]\n",
      " [-0.04649883]\n",
      " [-0.02922619]\n",
      " ...\n",
      " [-0.02118174]\n",
      " [ 0.00458596]\n",
      " [-0.01234784]]\n",
      "t [[ 0.02120474]\n",
      " [-0.09105773]\n",
      " [-0.05717711]\n",
      " ...\n",
      " [-0.04129698]\n",
      " [ 0.00878743]\n",
      " [-0.02440321]]\n",
      "t [[ 0.02120474]\n",
      " [-0.09105773]\n",
      " [-0.05717711]\n",
      " ...\n",
      " [-0.04129698]\n",
      " [ 0.00878743]\n",
      " [-0.02440321]]\n",
      "Current iteration=2, loss=39872.427842535115\n",
      "t [[ 0.03098998]\n",
      " [-0.13378072]\n",
      " [-0.08392512]\n",
      " ...\n",
      " [-0.06040943]\n",
      " [ 0.01262468]\n",
      " [-0.03617722]]\n",
      "t [[ 0.03098998]\n",
      " [-0.13378072]\n",
      " [-0.08392512]\n",
      " ...\n",
      " [-0.06040943]\n",
      " [ 0.01262468]\n",
      " [-0.03617722]]\n",
      "t [[ 0.04026959]\n",
      " [-0.17476726]\n",
      " [-0.10953926]\n",
      " ...\n",
      " [-0.07857978]\n",
      " [ 0.0161172 ]\n",
      " [-0.04768059]]\n",
      "t [[ 0.04026959]\n",
      " [-0.17476726]\n",
      " [-0.10953926]\n",
      " ...\n",
      " [-0.07857978]\n",
      " [ 0.0161172 ]\n",
      " [-0.04768059]]\n",
      "Current iteration=4, loss=39485.872105825154\n",
      "t [[ 0.04907123]\n",
      " [-0.21411198]\n",
      " [-0.13408509]\n",
      " ...\n",
      " [-0.09586553]\n",
      " [ 0.01928365]\n",
      " [-0.05892362]]\n",
      "t [[ 0.04907123]\n",
      " [-0.21411198]\n",
      " [-0.13408509]\n",
      " ...\n",
      " [-0.09586553]\n",
      " [ 0.01928365]\n",
      " [-0.05892362]]\n",
      "t [[ 0.05742119]\n",
      " [-0.25190463]\n",
      " [-0.15762463]\n",
      " ...\n",
      " [-0.11232105]\n",
      " [ 0.02214178]\n",
      " [-0.06991619]]\n",
      "t [[ 0.05742119]\n",
      " [-0.25190463]\n",
      " [-0.15762463]\n",
      " ...\n",
      " [-0.11232105]\n",
      " [ 0.02214178]\n",
      " [-0.06991619]]\n",
      "Current iteration=6, loss=39142.98452831651\n",
      "t [[ 0.06534439]\n",
      " [-0.28823005]\n",
      " [-0.18021639]\n",
      " ...\n",
      " [-0.12799754]\n",
      " [ 0.02470844]\n",
      " [-0.08066774]]\n",
      "t [[ 0.06534439]\n",
      " [-0.28823005]\n",
      " [-0.18021639]\n",
      " ...\n",
      " [-0.12799754]\n",
      " [ 0.02470844]\n",
      " [-0.08066774]]\n",
      "t [[ 0.0728644 ]\n",
      " [-0.3231683 ]\n",
      " [-0.20191546]\n",
      " ...\n",
      " [-0.14294315]\n",
      " [ 0.02699957]\n",
      " [-0.09118727]]\n",
      "t [[ 0.0728644 ]\n",
      " [-0.3231683 ]\n",
      " [-0.20191546]\n",
      " ...\n",
      " [-0.14294315]\n",
      " [ 0.02699957]\n",
      " [-0.09118727]]\n",
      "Current iteration=8, loss=38836.24041788725\n",
      "t [[ 0.08000347]\n",
      " [-0.35679477]\n",
      " [-0.22277358]\n",
      " ...\n",
      " [-0.15720309]\n",
      " [ 0.02903024]\n",
      " [-0.10148334]]\n",
      "t [[ 0.08000347]\n",
      " [-0.35679477]\n",
      " [-0.22277358]\n",
      " ...\n",
      " [-0.15720309]\n",
      " [ 0.02903024]\n",
      " [-0.10148334]]\n",
      "t [[ 0.08678259]\n",
      " [-0.38918034]\n",
      " [-0.24283931]\n",
      " ...\n",
      " [-0.1708197 ]\n",
      " [ 0.03081463]\n",
      " [-0.11156411]]\n",
      "loss=38559.6349434251\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00143542]\n",
      " [-0.00883772]\n",
      " [-0.02770717]\n",
      " ...\n",
      " [-0.02126188]\n",
      " [ 0.00413345]\n",
      " [-0.01261349]]\n",
      "t [[ 0.00143542]\n",
      " [-0.00883772]\n",
      " [-0.02770717]\n",
      " ...\n",
      " [-0.02126188]\n",
      " [ 0.00413345]\n",
      " [-0.01261349]]\n",
      "t [[ 0.00267534]\n",
      " [-0.01797466]\n",
      " [-0.05410549]\n",
      " ...\n",
      " [-0.04143421]\n",
      " [ 0.00789142]\n",
      " [-0.02492787]]\n",
      "t [[ 0.00267534]\n",
      " [-0.01797466]\n",
      " [-0.05410549]\n",
      " ...\n",
      " [-0.04143421]\n",
      " [ 0.00789142]\n",
      " [-0.02492787]]\n",
      "Current iteration=2, loss=39867.27917824446\n",
      "t [[ 0.00373336]\n",
      " [-0.02738354]\n",
      " [-0.07927263]\n",
      " ...\n",
      " [-0.06058279]\n",
      " [ 0.01129424]\n",
      " [-0.03695458]]\n",
      "t [[ 0.00373336]\n",
      " [-0.02738354]\n",
      " [-0.07927263]\n",
      " ...\n",
      " [-0.06058279]\n",
      " [ 0.01129424]\n",
      " [-0.03695458]]\n",
      "t [[ 0.00462245]\n",
      " [-0.03703857]\n",
      " [-0.10328251]\n",
      " ...\n",
      " [-0.07877024]\n",
      " [ 0.01436149]\n",
      " [-0.04870469]]\n",
      "t [[ 0.00462245]\n",
      " [-0.03703857]\n",
      " [-0.10328251]\n",
      " ...\n",
      " [-0.07877024]\n",
      " [ 0.01436149]\n",
      " [-0.04870469]]\n",
      "Current iteration=4, loss=39476.25800543617\n",
      "t [[ 0.00535488]\n",
      " [-0.04691548]\n",
      " [-0.1262052 ]\n",
      " ...\n",
      " [-0.09605587]\n",
      " [ 0.01711182]\n",
      " [-0.06018882]]\n",
      "t [[ 0.00535488]\n",
      " [-0.04691548]\n",
      " [-0.1262052 ]\n",
      " ...\n",
      " [-0.09605587]\n",
      " [ 0.01711182]\n",
      " [-0.06018882]]\n",
      "t [[ 0.00594224]\n",
      " [-0.05699149]\n",
      " [-0.14810686]\n",
      " ...\n",
      " [-0.11249569]\n",
      " [ 0.01956302]\n",
      " [-0.07141712]]\n",
      "t [[ 0.00594224]\n",
      " [-0.05699149]\n",
      " [-0.14810686]\n",
      " ...\n",
      " [-0.11249569]\n",
      " [ 0.01956302]\n",
      " [-0.07141712]]\n",
      "Current iteration=6, loss=39129.38623643863\n",
      "t [[ 0.0063954 ]\n",
      " [-0.06724534]\n",
      " [-0.16904983]\n",
      " ...\n",
      " [-0.12814243]\n",
      " [ 0.0217319 ]\n",
      " [-0.08239932]]\n",
      "t [[ 0.0063954 ]\n",
      " [-0.06724534]\n",
      " [-0.16904983]\n",
      " ...\n",
      " [-0.12814243]\n",
      " [ 0.0217319 ]\n",
      " [-0.08239932]]\n",
      "t [[ 0.00672455]\n",
      " [-0.07765718]\n",
      " [-0.18909264]\n",
      " ...\n",
      " [-0.14304564]\n",
      " [ 0.02363438]\n",
      " [-0.09314466]]\n",
      "t [[ 0.00672455]\n",
      " [-0.07765718]\n",
      " [-0.18909264]\n",
      " ...\n",
      " [-0.14304564]\n",
      " [ 0.02363438]\n",
      " [-0.09314466]]\n",
      "Current iteration=8, loss=38819.00005902015\n",
      "t [[ 0.00693922]\n",
      " [-0.08820856]\n",
      " [-0.20829023]\n",
      " ...\n",
      " [-0.15725177]\n",
      " [ 0.02528544]\n",
      " [-0.10366196]]\n",
      "t [[ 0.00693922]\n",
      " [-0.08820856]\n",
      " [-0.20829023]\n",
      " ...\n",
      " [-0.15725177]\n",
      " [ 0.02528544]\n",
      " [-0.10366196]]\n",
      "t [[ 0.00704828]\n",
      " [-0.09888232]\n",
      " [-0.22669402]\n",
      " ...\n",
      " [-0.17080434]\n",
      " [ 0.0266992 ]\n",
      " [-0.11395958]]\n",
      "loss=38539.00114637026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00122601]\n",
      " [-0.00775651]\n",
      " [-0.02800725]\n",
      " ...\n",
      " [-0.02090631]\n",
      " [ 0.00444427]\n",
      " [-0.01183151]]\n",
      "t [[ 0.00122601]\n",
      " [-0.00775651]\n",
      " [-0.02800725]\n",
      " ...\n",
      " [-0.02090631]\n",
      " [ 0.00444427]\n",
      " [-0.01183151]]\n",
      "t [[ 0.00225844]\n",
      " [-0.01582608]\n",
      " [-0.05470837]\n",
      " ...\n",
      " [-0.04073793]\n",
      " [ 0.00850939]\n",
      " [-0.02337396]]\n",
      "t [[ 0.00225844]\n",
      " [-0.01582608]\n",
      " [-0.05470837]\n",
      " ...\n",
      " [-0.04073793]\n",
      " [ 0.00850939]\n",
      " [-0.02337396]]\n",
      "Current iteration=2, loss=39871.94984900532\n",
      "t [[ 0.00311098]\n",
      " [-0.02418135]\n",
      " [-0.08018029]\n",
      " ...\n",
      " [-0.05955962]\n",
      " [ 0.01221566]\n",
      " [-0.03463857]]\n",
      "t [[ 0.00311098]\n",
      " [-0.02418135]\n",
      " [-0.08018029]\n",
      " ...\n",
      " [-0.05955962]\n",
      " [ 0.01221566]\n",
      " [-0.03463857]]\n",
      "t [[ 0.00379667]\n",
      " [-0.03279641]\n",
      " [-0.10449624]\n",
      " ...\n",
      " [-0.07743303]\n",
      " [ 0.01558257]\n",
      " [-0.04563617]]\n",
      "t [[ 0.00379667]\n",
      " [-0.03279641]\n",
      " [-0.10449624]\n",
      " ...\n",
      " [-0.07743303]\n",
      " [ 0.01558257]\n",
      " [-0.04563617]]\n",
      "Current iteration=4, loss=39485.18035572746\n",
      "t [[ 0.00432786]\n",
      " [-0.0416469 ]\n",
      " [-0.12772565]\n",
      " ...\n",
      " [-0.09441659]\n",
      " [ 0.01862872]\n",
      " [-0.05637717]]\n",
      "t [[ 0.00432786]\n",
      " [-0.0416469 ]\n",
      " [-0.12772565]\n",
      " ...\n",
      " [-0.09441659]\n",
      " [ 0.01862872]\n",
      " [-0.05637717]]\n",
      "t [[ 0.00471618]\n",
      " [-0.05070994]\n",
      " [-0.14993413]\n",
      " ...\n",
      " [-0.11056545]\n",
      " [ 0.02137184]\n",
      " [-0.06687152]]\n",
      "t [[ 0.00471618]\n",
      " [-0.05070994]\n",
      " [-0.14993413]\n",
      " ...\n",
      " [-0.11056545]\n",
      " [ 0.02137184]\n",
      " [-0.06687152]]\n",
      "Current iteration=6, loss=39142.23998548431\n",
      "t [[ 0.00497257]\n",
      " [-0.05996411]\n",
      " [-0.17118346]\n",
      " ...\n",
      " [-0.12593159]\n",
      " [ 0.0238287 ]\n",
      " [-0.07712874]]\n",
      "t [[ 0.00497257]\n",
      " [-0.05996411]\n",
      " [-0.17118346]\n",
      " ...\n",
      " [-0.12593159]\n",
      " [ 0.0238287 ]\n",
      " [-0.07712874]]\n",
      "t [[ 0.00510726]\n",
      " [-0.06938945]\n",
      " [-0.19153174]\n",
      " ...\n",
      " [-0.14056384]\n",
      " [ 0.02601517]\n",
      " [-0.08715789]]\n",
      "t [[ 0.00510726]\n",
      " [-0.06938945]\n",
      " [-0.19153174]\n",
      " ...\n",
      " [-0.14056384]\n",
      " [ 0.02601517]\n",
      " [-0.08715789]]\n",
      "Current iteration=8, loss=38835.53516562907\n",
      "t [[ 0.00512982]\n",
      " [-0.07896734]\n",
      " [-0.2110335 ]\n",
      " ...\n",
      " [-0.15450803]\n",
      " [ 0.02794621]\n",
      " [-0.0969676 ]]\n",
      "t [[ 0.00512982]\n",
      " [-0.07896734]\n",
      " [-0.2110335 ]\n",
      " ...\n",
      " [-0.15450803]\n",
      " [ 0.02794621]\n",
      " [-0.0969676 ]]\n",
      "t [[ 0.00504917]\n",
      " [-0.08868046]\n",
      " [-0.22973979]\n",
      " ...\n",
      " [-0.16780707]\n",
      " [ 0.02963591]\n",
      " [-0.10656605]]\n",
      "loss=38559.01650789535\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00102102]\n",
      " [-0.00821548]\n",
      " [-0.02782245]\n",
      " ...\n",
      " [-0.01224951]\n",
      " [-0.0014753 ]\n",
      " [-0.02499098]]\n",
      "t [[ 0.00102102]\n",
      " [-0.00821548]\n",
      " [-0.02782245]\n",
      " ...\n",
      " [-0.01224951]\n",
      " [-0.0014753 ]\n",
      " [-0.02499098]]\n",
      "t [[ 0.00185228]\n",
      " [-0.0167419 ]\n",
      " [-0.05432788]\n",
      " ...\n",
      " [-0.02365494]\n",
      " [-0.00267151]\n",
      " [-0.04949074]]\n",
      "t [[ 0.00185228]\n",
      " [-0.0167419 ]\n",
      " [-0.05432788]\n",
      " ...\n",
      " [-0.02365494]\n",
      " [-0.00267151]\n",
      " [-0.04949074]]\n",
      "Current iteration=2, loss=39864.14482887465\n",
      "t [[ 0.00250742]\n",
      " [-0.02555171]\n",
      " [-0.07959422]\n",
      " ...\n",
      " [-0.03427116]\n",
      " [-0.00361096]\n",
      " [-0.07351582]]\n",
      "t [[ 0.00250742]\n",
      " [-0.02555171]\n",
      " [-0.07959422]\n",
      " ...\n",
      " [-0.03427116]\n",
      " [-0.00361096]\n",
      " [-0.07351582]]\n",
      "t [[ 0.00299942]\n",
      " [-0.03461887]\n",
      " [-0.10369567]\n",
      " ...\n",
      " [-0.04415025]\n",
      " [-0.00431465]\n",
      " [-0.09708224]]\n",
      "t [[ 0.00299942]\n",
      " [-0.03461887]\n",
      " [-0.10369567]\n",
      " ...\n",
      " [-0.04415025]\n",
      " [-0.00431465]\n",
      " [-0.09708224]]\n",
      "Current iteration=4, loss=39470.64314480164\n",
      "t [[ 0.00334055]\n",
      " [-0.04391884]\n",
      " [-0.12670257]\n",
      " ...\n",
      " [-0.05334145]\n",
      " [-0.00480235]\n",
      " [-0.12020549]]\n",
      "t [[ 0.00334055]\n",
      " [-0.04391884]\n",
      " [-0.12670257]\n",
      " ...\n",
      " [-0.05334145]\n",
      " [-0.00480235]\n",
      " [-0.12020549]]\n",
      "t [[ 0.00354238]\n",
      " [-0.05342861]\n",
      " [-0.14868136]\n",
      " ...\n",
      " [-0.06189117]\n",
      " [-0.00509252]\n",
      " [-0.14290049]]\n",
      "t [[ 0.00354238]\n",
      " [-0.05342861]\n",
      " [-0.14868136]\n",
      " ...\n",
      " [-0.06189117]\n",
      " [-0.00509252]\n",
      " [-0.14290049]]\n",
      "Current iteration=6, loss=39121.81929408314\n",
      "t [[ 0.00361578]\n",
      " [-0.06312663]\n",
      " [-0.16969463]\n",
      " ...\n",
      " [-0.06984301]\n",
      " [-0.00520243]\n",
      " [-0.16518159]]\n",
      "t [[ 0.00361578]\n",
      " [-0.06312663]\n",
      " [-0.16969463]\n",
      " ...\n",
      " [-0.06984301]\n",
      " [-0.00520243]\n",
      " [-0.16518159]]\n",
      "t [[ 0.0035709 ]\n",
      " [-0.07299281]\n",
      " [-0.1898012 ]\n",
      " ...\n",
      " [-0.07723783]\n",
      " [-0.00514813]\n",
      " [-0.18706257]]\n",
      "t [[ 0.0035709 ]\n",
      " [-0.07299281]\n",
      " [-0.1898012 ]\n",
      " ...\n",
      " [-0.07723783]\n",
      " [-0.00514813]\n",
      " [-0.18706257]]\n",
      "Current iteration=8, loss=38809.90589765168\n",
      "t [[ 0.00341724]\n",
      " [-0.08300845]\n",
      " [-0.20905626]\n",
      " ...\n",
      " [-0.0841139 ]\n",
      " [-0.00494457]\n",
      " [-0.20855661]]\n",
      "t [[ 0.00341724]\n",
      " [-0.08300845]\n",
      " [-0.20905626]\n",
      " ...\n",
      " [-0.0841139 ]\n",
      " [-0.00494457]\n",
      " [-0.20855661]]\n",
      "t [[ 0.00316365]\n",
      " [-0.09315616]\n",
      " [-0.22751151]\n",
      " ...\n",
      " [-0.09050696]\n",
      " [-0.00460564]\n",
      " [-0.22967636]]\n",
      "loss=38528.71968053818\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01360606]\n",
      " [-0.05812354]\n",
      " [-0.03653274]\n",
      " ...\n",
      " [-0.02647718]\n",
      " [ 0.00573246]\n",
      " [-0.0154348 ]]\n",
      "t [[ 0.01360606]\n",
      " [-0.05812354]\n",
      " [-0.03653274]\n",
      " ...\n",
      " [-0.02647718]\n",
      " [ 0.00573246]\n",
      " [-0.0154348 ]]\n",
      "t [[ 0.02632946]\n",
      " [-0.11321623]\n",
      " [-0.07107307]\n",
      " ...\n",
      " [-0.05128811]\n",
      " [ 0.01086421]\n",
      " [-0.03041266]]\n",
      "t [[ 0.02632946]\n",
      " [-0.11321623]\n",
      " [-0.07107307]\n",
      " ...\n",
      " [-0.05128811]\n",
      " [ 0.01086421]\n",
      " [-0.03041266]]\n",
      "Current iteration=2, loss=39769.19925883704\n",
      "t [[ 0.03822962]\n",
      " [-0.16548205]\n",
      " [-0.10376285]\n",
      " ...\n",
      " [-0.07455767]\n",
      " [ 0.01543505]\n",
      " [-0.04495534]]\n",
      "t [[ 0.03822962]\n",
      " [-0.16548205]\n",
      " [-0.10376285]\n",
      " ...\n",
      " [-0.07455767]\n",
      " [ 0.01543505]\n",
      " [-0.04495534]]\n",
      "t [[ 0.04936266]\n",
      " [-0.21511327]\n",
      " [-0.13473548]\n",
      " ...\n",
      " [-0.09640301]\n",
      " [ 0.01948274]\n",
      " [-0.05908366]]\n",
      "t [[ 0.04936266]\n",
      " [-0.21511327]\n",
      " [-0.13473548]\n",
      " ...\n",
      " [-0.09640301]\n",
      " [ 0.01948274]\n",
      " [-0.05908366]]\n",
      "Current iteration=4, loss=39306.301082248436\n",
      "t [[ 0.0597813 ]\n",
      " [-0.26229001]\n",
      " [-0.16411556]\n",
      " ...\n",
      " [-0.11693339]\n",
      " [ 0.02304287]\n",
      " [-0.07281734]]\n",
      "t [[ 0.0597813 ]\n",
      " [-0.26229001]\n",
      " [-0.16411556]\n",
      " ...\n",
      " [-0.11693339]\n",
      " [ 0.02304287]\n",
      " [-0.07281734]]\n",
      "t [[ 0.06953482]\n",
      " [-0.30718017]\n",
      " [-0.19201891]\n",
      " ...\n",
      " [-0.13625018]\n",
      " [ 0.02614875]\n",
      " [-0.08617501]]\n",
      "t [[ 0.06953482]\n",
      " [-0.30718017]\n",
      " [-0.19201891]\n",
      " ...\n",
      " [-0.13625018]\n",
      " [ 0.02614875]\n",
      " [-0.08617501]]\n",
      "Current iteration=6, loss=38906.08768575905\n",
      "t [[ 0.07866917]\n",
      " [-0.34993973]\n",
      " [-0.21855284]\n",
      " ...\n",
      " [-0.15444719]\n",
      " [ 0.02883145]\n",
      " [-0.09917423]]\n",
      "t [[ 0.07866917]\n",
      " [-0.34993973]\n",
      " [-0.21855284]\n",
      " ...\n",
      " [-0.15444719]\n",
      " [ 0.02883145]\n",
      " [-0.09917423]]\n",
      "t [[ 0.08722703]\n",
      " [-0.39071315]\n",
      " [-0.24381644]\n",
      " ...\n",
      " [-0.17161094]\n",
      " [ 0.03111985]\n",
      " [-0.11183149]]\n",
      "t [[ 0.08722703]\n",
      " [-0.39071315]\n",
      " [-0.24381644]\n",
      " ...\n",
      " [-0.17161094]\n",
      " [ 0.03111985]\n",
      " [-0.11183149]]\n",
      "Current iteration=8, loss=38555.53922328925\n",
      "t [[ 0.09524803]\n",
      " [-0.42963403]\n",
      " [-0.26790107]\n",
      " ...\n",
      " [-0.18782109]\n",
      " [ 0.03304072]\n",
      " [-0.12416226]]\n",
      "t [[ 0.09524803]\n",
      " [-0.42963403]\n",
      " [-0.26790107]\n",
      " ...\n",
      " [-0.18782109]\n",
      " [ 0.03304072]\n",
      " [-0.12416226]]\n",
      "t [[ 0.10276887]\n",
      " [-0.46682578]\n",
      " [-0.29089086]\n",
      " ...\n",
      " [-0.20315091]\n",
      " [ 0.03461884]\n",
      " [-0.13618103]]\n",
      "loss=38244.855303733115\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00179428]\n",
      " [-0.01104715]\n",
      " [-0.03463396]\n",
      " ...\n",
      " [-0.02657735]\n",
      " [ 0.00516682]\n",
      " [-0.01576686]]\n",
      "t [[ 0.00179428]\n",
      " [-0.01104715]\n",
      " [-0.03463396]\n",
      " ...\n",
      " [-0.02657735]\n",
      " [ 0.00516682]\n",
      " [-0.01576686]]\n",
      "t [[ 0.00328313]\n",
      " [-0.02256176]\n",
      " [-0.06722306]\n",
      " ...\n",
      " [-0.05145245]\n",
      " [ 0.00974701]\n",
      " [-0.0310664 ]]\n",
      "t [[ 0.00328313]\n",
      " [-0.02256176]\n",
      " [-0.06722306]\n",
      " ...\n",
      " [-0.05145245]\n",
      " [ 0.00974701]\n",
      " [-0.0310664 ]]\n",
      "Current iteration=2, loss=39762.85399066431\n",
      "t [[ 0.00449323]\n",
      " [-0.03449041]\n",
      " [-0.09791952]\n",
      " ...\n",
      " [-0.07475426]\n",
      " [ 0.0137805 ]\n",
      " [-0.04592107]]\n",
      "t [[ 0.00449323]\n",
      " [-0.03449041]\n",
      " [-0.09791952]\n",
      " ...\n",
      " [-0.07475426]\n",
      " [ 0.0137805 ]\n",
      " [-0.04592107]]\n",
      "t [[ 0.00544963]\n",
      " [-0.04678341]\n",
      " [-0.12686607]\n",
      " ...\n",
      " [-0.09660366]\n",
      " [ 0.01730518]\n",
      " [-0.06035232]]\n",
      "t [[ 0.00544963]\n",
      " [-0.04678341]\n",
      " [-0.12686607]\n",
      " ...\n",
      " [-0.09660366]\n",
      " [ 0.01730518]\n",
      " [-0.06035232]]\n",
      "Current iteration=4, loss=39294.61163280877\n",
      "t [[ 0.00617563]\n",
      " [-0.05939485]\n",
      " [-0.15419577]\n",
      " ...\n",
      " [-0.11711324]\n",
      " [ 0.02035665]\n",
      " [-0.07438046]]\n",
      "t [[ 0.00617563]\n",
      " [-0.05939485]\n",
      " [-0.15419577]\n",
      " ...\n",
      " [-0.11711324]\n",
      " [ 0.02035665]\n",
      " [-0.07438046]]\n",
      "t [[ 0.00669277]\n",
      " [-0.07228255]\n",
      " [-0.180032  ]\n",
      " ...\n",
      " [-0.13638744]\n",
      " [ 0.02296819]\n",
      " [-0.08802468]]\n",
      "t [[ 0.00669277]\n",
      " [-0.07228255]\n",
      " [-0.180032  ]\n",
      " ...\n",
      " [-0.13638744]\n",
      " [ 0.02296819]\n",
      " [-0.08802468]]\n",
      "Current iteration=6, loss=38889.69830147816\n",
      "t [[ 0.0070209 ]\n",
      " [-0.08540791]\n",
      " [-0.20448879]\n",
      " ...\n",
      " [-0.15452274]\n",
      " [ 0.02517081]\n",
      " [-0.10130302]]\n",
      "t [[ 0.0070209 ]\n",
      " [-0.08540791]\n",
      " [-0.20448879]\n",
      " ...\n",
      " [-0.15452274]\n",
      " [ 0.02517081]\n",
      " [-0.10130302]]\n",
      "t [[ 0.00717821]\n",
      " [-0.09873569]\n",
      " [-0.22767126]\n",
      " ...\n",
      " [-0.17160806]\n",
      " [ 0.02699322]\n",
      " [-0.11423242]]\n",
      "t [[ 0.00717821]\n",
      " [-0.09873569]\n",
      " [-0.22767126]\n",
      " ...\n",
      " [-0.17160806]\n",
      " [ 0.02699322]\n",
      " [-0.11423242]]\n",
      "Current iteration=8, loss=38534.874255172974\n",
      "t [[ 0.00718134]\n",
      " [-0.11223385]\n",
      " [-0.2496761 ]\n",
      " ...\n",
      " [-0.1877252 ]\n",
      " [ 0.02846204]\n",
      " [-0.12682878]]\n",
      "t [[ 0.00718134]\n",
      " [-0.11223385]\n",
      " [-0.2496761 ]\n",
      " ...\n",
      " [-0.1877252 ]\n",
      " [ 0.02846204]\n",
      " [-0.12682878]]\n",
      "t [[ 0.00704551]\n",
      " [-0.12587326]\n",
      " [-0.27059213]\n",
      " ...\n",
      " [-0.20294929]\n",
      " [ 0.02960184]\n",
      " [-0.13910695]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=38220.20849338645\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00153251]\n",
      " [-0.00969564]\n",
      " [-0.03500906]\n",
      " ...\n",
      " [-0.02613289]\n",
      " [ 0.00555534]\n",
      " [-0.01478939]]\n",
      "t [[ 0.00153251]\n",
      " [-0.00969564]\n",
      " [-0.03500906]\n",
      " ...\n",
      " [-0.02613289]\n",
      " [ 0.00555534]\n",
      " [-0.01478939]]\n",
      "t [[ 0.0027626 ]\n",
      " [-0.01988037]\n",
      " [-0.06797751]\n",
      " ...\n",
      " [-0.05058674]\n",
      " [ 0.01051833]\n",
      " [-0.02912715]]\n",
      "t [[ 0.0027626 ]\n",
      " [-0.01988037]\n",
      " [-0.06797751]\n",
      " ...\n",
      " [-0.05058674]\n",
      " [ 0.01051833]\n",
      " [-0.02912715]]\n",
      "Current iteration=2, loss=39768.63820313081\n",
      "t [[ 0.00371711]\n",
      " [-0.03050059]\n",
      " [-0.0990561 ]\n",
      " ...\n",
      " [-0.07348848]\n",
      " [ 0.01492879]\n",
      " [-0.04303531]]\n",
      "t [[ 0.00371711]\n",
      " [-0.03050059]\n",
      " [-0.0990561 ]\n",
      " ...\n",
      " [-0.07348848]\n",
      " [ 0.01492879]\n",
      " [-0.04303531]]\n",
      "t [[ 0.00442123]\n",
      " [-0.04150642]\n",
      " [-0.12838626]\n",
      " ...\n",
      " [-0.09495712]\n",
      " [ 0.01882447]\n",
      " [-0.05653486]]\n",
      "t [[ 0.00442123]\n",
      " [-0.04150642]\n",
      " [-0.12838626]\n",
      " ...\n",
      " [-0.09495712]\n",
      " [ 0.01882447]\n",
      " [-0.05653486]]\n",
      "Current iteration=4, loss=39305.55764918958\n",
      "t [[ 0.00489837]\n",
      " [-0.05285175]\n",
      " [-0.15609984]\n",
      " ...\n",
      " [-0.11510359]\n",
      " [ 0.02224086]\n",
      " [-0.06964571]]\n",
      "t [[ 0.00489837]\n",
      " [-0.05285175]\n",
      " [-0.15609984]\n",
      " ...\n",
      " [-0.11510359]\n",
      " [ 0.02224086]\n",
      " [-0.06964571]]\n",
      "t [[ 0.0051702 ]\n",
      " [-0.06449413]\n",
      " [-0.1823192 ]\n",
      " ...\n",
      " [-0.13403079]\n",
      " [ 0.02521115]\n",
      " [-0.08238665]]\n",
      "t [[ 0.0051702 ]\n",
      " [-0.06449413]\n",
      " [-0.1823192 ]\n",
      " ...\n",
      " [-0.13403079]\n",
      " [ 0.02521115]\n",
      " [-0.08238665]]\n",
      "Current iteration=6, loss=38905.3595602441\n",
      "t [[ 0.00525665]\n",
      " [-0.07639469]\n",
      " [-0.20715751]\n",
      " ...\n",
      " [-0.15183382]\n",
      " [ 0.02776625]\n",
      " [-0.09477536]]\n",
      "t [[ 0.00525665]\n",
      " [-0.07639469]\n",
      " [-0.20715751]\n",
      " ...\n",
      " [-0.15183382]\n",
      " [ 0.02776625]\n",
      " [-0.09477536]]\n",
      "t [[ 0.00517599]\n",
      " [-0.0885179 ]\n",
      " [-0.23071909]\n",
      " ...\n",
      " [-0.16860039]\n",
      " [ 0.02993485]\n",
      " [-0.1068284 ]]\n",
      "t [[ 0.00517599]\n",
      " [-0.0885179 ]\n",
      " [-0.23071909]\n",
      " ...\n",
      " [-0.16860039]\n",
      " [ 0.02993485]\n",
      " [-0.1068284 ]]\n",
      "Current iteration=8, loss=38554.917260608185\n",
      "t [[ 0.00494494]\n",
      " [-0.10083137]\n",
      " [-0.2531    ]\n",
      " ...\n",
      " [-0.18441121]\n",
      " [ 0.03174351]\n",
      " [-0.11856133]]\n",
      "t [[ 0.00494494]\n",
      " [-0.10083137]\n",
      " [-0.2531    ]\n",
      " ...\n",
      " [-0.18441121]\n",
      " [ 0.03174351]\n",
      " [-0.11856133]]\n",
      "t [[ 0.00457876]\n",
      " [-0.11330563]\n",
      " [-0.27438854]\n",
      " ...\n",
      " [-0.19934045]\n",
      " [ 0.03321681]\n",
      " [-0.12998867]]\n",
      "loss=38244.37062332108\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00127628]\n",
      " [-0.01026935]\n",
      " [-0.03477806]\n",
      " ...\n",
      " [-0.01531188]\n",
      " [-0.00184412]\n",
      " [-0.03123872]]\n",
      "t [[ 0.00127628]\n",
      " [-0.01026935]\n",
      " [-0.03477806]\n",
      " ...\n",
      " [-0.01531188]\n",
      " [-0.00184412]\n",
      " [-0.03123872]]\n",
      "t [[ 0.0022561 ]\n",
      " [-0.02102447]\n",
      " [-0.06749849]\n",
      " ...\n",
      " [-0.02930504]\n",
      " [-0.00325222]\n",
      " [-0.06170998]]\n",
      "t [[ 0.0022561 ]\n",
      " [-0.02102447]\n",
      " [-0.06749849]\n",
      " ...\n",
      " [-0.02930504]\n",
      " [-0.00325222]\n",
      " [-0.06170998]]\n",
      "Current iteration=2, loss=39759.01871856283\n",
      "t [[ 0.00296621]\n",
      " [-0.03221142]\n",
      " [-0.09831404]\n",
      " ...\n",
      " [-0.04208696]\n",
      " [-0.00426797]\n",
      " [-0.09144618]]\n",
      "t [[ 0.00296621]\n",
      " [-0.03221142]\n",
      " [-0.09831404]\n",
      " ...\n",
      " [-0.04208696]\n",
      " [-0.00426797]\n",
      " [-0.09144618]]\n",
      "t [[ 0.00343166]\n",
      " [-0.04378001]\n",
      " [-0.12736797]\n",
      " ...\n",
      " [-0.05375811]\n",
      " [-0.00493183]\n",
      " [-0.12047848]]\n",
      "t [[ 0.00343166]\n",
      " [-0.04378001]\n",
      " [-0.12736797]\n",
      " ...\n",
      " [-0.05375811]\n",
      " [-0.00493183]\n",
      " [-0.12047848]]\n",
      "Current iteration=4, loss=39287.923604882126\n",
      "t [[ 0.00367573]\n",
      " [-0.05568381]\n",
      " [-0.15479386]\n",
      " ...\n",
      " [-0.06441187]\n",
      " [-0.00528109]\n",
      " [-0.14883662]]\n",
      "t [[ 0.00367573]\n",
      " [-0.05568381]\n",
      " [-0.15479386]\n",
      " ...\n",
      " [-0.06441187]\n",
      " [-0.00528109]\n",
      " [-0.14883662]]\n",
      "t [[ 0.00371996]\n",
      " [-0.06788016]\n",
      " [-0.18071564]\n",
      " ...\n",
      " [-0.0741346 ]\n",
      " [-0.00534996]\n",
      " [-0.17654891]]\n",
      "t [[ 0.00371996]\n",
      " [-0.06788016]\n",
      " [-0.18071564]\n",
      " ...\n",
      " [-0.0741346 ]\n",
      " [-0.00534996]\n",
      " [-0.17654891]]\n",
      "Current iteration=6, loss=38880.907589799885\n",
      "t [[ 0.00358411]\n",
      " [-0.08032995]\n",
      " [-0.20524787]\n",
      " ...\n",
      " [-0.0830059 ]\n",
      " [-0.00516971]\n",
      " [-0.20364224]]\n",
      "t [[ 0.00358411]\n",
      " [-0.08032995]\n",
      " [-0.20524787]\n",
      " ...\n",
      " [-0.0830059 ]\n",
      " [-0.00516971]\n",
      " [-0.20364224]]\n",
      "t [[ 0.00328634]\n",
      " [-0.09299746]\n",
      " [-0.22849618]\n",
      " ...\n",
      " [-0.09109892]\n",
      " [-0.00476887]\n",
      " [-0.23014204]]\n",
      "t [[ 0.00328634]\n",
      " [-0.09299746]\n",
      " [-0.22849618]\n",
      " ...\n",
      " [-0.09109892]\n",
      " [-0.00476887]\n",
      " [-0.23014204]]\n",
      "Current iteration=8, loss=38524.54886158138\n",
      "t [[ 0.00284321]\n",
      " [-0.10585016]\n",
      " [-0.25055779]\n",
      " ...\n",
      " [-0.09848079]\n",
      " [-0.00417337]\n",
      " [-0.2560724 ]]\n",
      "t [[ 0.00284321]\n",
      " [-0.10585016]\n",
      " [-0.25055779]\n",
      " ...\n",
      " [-0.09848079]\n",
      " [-0.00417337]\n",
      " [-0.2560724 ]]\n",
      "t [[ 0.00226986]\n",
      " [-0.11885848]\n",
      " [-0.27152202]\n",
      " ...\n",
      " [-0.10521298]\n",
      " [-0.0034068 ]\n",
      " [-0.28145607]]\n",
      "loss=38208.77533275602\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01632728]\n",
      " [-0.06974825]\n",
      " [-0.04383929]\n",
      " ...\n",
      " [-0.03177262]\n",
      " [ 0.00687895]\n",
      " [-0.01852176]]\n",
      "t [[ 0.01632728]\n",
      " [-0.06974825]\n",
      " [-0.04383929]\n",
      " ...\n",
      " [-0.03177262]\n",
      " [ 0.00687895]\n",
      " [-0.01852176]]\n",
      "t [[ 0.03138367]\n",
      " [-0.13513262]\n",
      " [-0.08480987]\n",
      " ...\n",
      " [-0.06114613]\n",
      " [ 0.01289302]\n",
      " [-0.03638558]]\n",
      "t [[ 0.03138367]\n",
      " [-0.13513262]\n",
      " [-0.08480987]\n",
      " ...\n",
      " [-0.06114613]\n",
      " [ 0.01289302]\n",
      " [-0.03638558]]\n",
      "Current iteration=2, loss=39668.50641224014\n",
      "t [[ 0.04527221]\n",
      " [-0.1965068 ]\n",
      " [-0.12315771]\n",
      " ...\n",
      " [-0.08833699]\n",
      " [ 0.01811125]\n",
      " [-0.05362925]]\n",
      "t [[ 0.04527221]\n",
      " [-0.1965068 ]\n",
      " [-0.12315771]\n",
      " ...\n",
      " [-0.08833699]\n",
      " [ 0.01811125]\n",
      " [-0.05362925]]\n",
      "t [[ 0.05808883]\n",
      " [-0.25419931]\n",
      " [-0.15911055]\n",
      " ...\n",
      " [-0.11354512]\n",
      " [ 0.02259828]\n",
      " [-0.07028839]]\n",
      "t [[ 0.05808883]\n",
      " [-0.25419931]\n",
      " [-0.15911055]\n",
      " ...\n",
      " [-0.11354512]\n",
      " [ 0.02259828]\n",
      " [-0.07028839]]\n",
      "Current iteration=4, loss=39135.985724397426\n",
      "t [[ 0.06992224]\n",
      " [-0.30851276]\n",
      " [-0.19287746]\n",
      " ...\n",
      " [-0.13695366]\n",
      " [ 0.02641403]\n",
      " [-0.08639637]]\n",
      "t [[ 0.06992224]\n",
      " [-0.30851276]\n",
      " [-0.19287746]\n",
      " ...\n",
      " [-0.13695366]\n",
      " [ 0.02641403]\n",
      " [-0.08639637]]\n",
      "t [[ 0.08085395]\n",
      " [-0.35972422]\n",
      " [-0.22464923]\n",
      " ...\n",
      " [-0.15872949]\n",
      " [ 0.02961361]\n",
      " [-0.10198422]]\n",
      "t [[ 0.08085395]\n",
      " [-0.35972422]\n",
      " [-0.22464923]\n",
      " ...\n",
      " [-0.15872949]\n",
      " [ 0.02961361]\n",
      " [-0.10198422]]\n",
      "Current iteration=6, loss=38686.441977093564\n",
      "t [[ 0.09095858]\n",
      " [-0.4080863 ]\n",
      " [-0.25459919]\n",
      " ...\n",
      " [-0.17902396]\n",
      " [ 0.03224747]\n",
      " [-0.11708071]]\n",
      "t [[ 0.09095858]\n",
      " [-0.4080863 ]\n",
      " [-0.25459919]\n",
      " ...\n",
      " [-0.17902396]\n",
      " [ 0.03224747]\n",
      " [-0.11708071]]\n",
      "t [[ 0.10030429]\n",
      " [-0.45382875]\n",
      " [-0.28288441]\n",
      " ...\n",
      " [-0.19797404]\n",
      " [ 0.03436165]\n",
      " [-0.13171249]]\n",
      "t [[ 0.10030429]\n",
      " [-0.45382875]\n",
      " [-0.28288441]\n",
      " ...\n",
      " [-0.19797404]\n",
      " [ 0.03436165]\n",
      " [-0.13171249]]\n",
      "Current iteration=8, loss=38299.97017122624\n",
      "t [[ 0.10895322]\n",
      " [-0.49716015]\n",
      " [-0.30964696]\n",
      " ...\n",
      " [-0.21570342]\n",
      " [ 0.03599806]\n",
      " [-0.14590417]]\n",
      "t [[ 0.10895322]\n",
      " [-0.49716015]\n",
      " [-0.30964696]\n",
      " ...\n",
      " [-0.21570342]\n",
      " [ 0.03599806]\n",
      " [-0.14590417]]\n",
      "t [[ 0.11696198]\n",
      " [-0.53826975]\n",
      " [-0.33501525]\n",
      " ...\n",
      " [-0.23232373]\n",
      " [ 0.03719486]\n",
      " [-0.15967851]]\n",
      "loss=37962.42109317938\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00215314]\n",
      " [-0.01325658]\n",
      " [-0.04156075]\n",
      " ...\n",
      " [-0.03189282]\n",
      " [ 0.00620018]\n",
      " [-0.01892024]]\n",
      "t [[ 0.00215314]\n",
      " [-0.01325658]\n",
      " [-0.04156075]\n",
      " ...\n",
      " [-0.03189282]\n",
      " [ 0.00620018]\n",
      " [-0.01892024]]\n",
      "t [[ 0.00386653]\n",
      " [-0.02718619]\n",
      " [-0.08017729]\n",
      " ...\n",
      " [-0.06133471]\n",
      " [ 0.01155575]\n",
      " [-0.03716758]]\n",
      "t [[ 0.00386653]\n",
      " [-0.02718619]\n",
      " [-0.08017729]\n",
      " ...\n",
      " [-0.06133471]\n",
      " [ 0.01155575]\n",
      " [-0.03716758]]\n",
      "Current iteration=2, loss=39660.99750009919\n",
      "t [[ 0.00518647]\n",
      " [-0.04169627]\n",
      " [-0.11611346]\n",
      " ...\n",
      " [-0.08854919]\n",
      " [ 0.01613601]\n",
      " [-0.05478102]]\n",
      "t [[ 0.00518647]\n",
      " [-0.04169627]\n",
      " [-0.11611346]\n",
      " ...\n",
      " [-0.08854919]\n",
      " [ 0.01613601]\n",
      " [-0.05478102]]\n",
      "t [[ 0.00615569]\n",
      " [-0.05670223]\n",
      " [-0.14961285]\n",
      " ...\n",
      " [-0.11374249]\n",
      " [ 0.02000576]\n",
      " [-0.07179728]]\n",
      "t [[ 0.00615569]\n",
      " [-0.05670223]\n",
      " [-0.14961285]\n",
      " ...\n",
      " [-0.11374249]\n",
      " [ 0.02000576]\n",
      " [-0.07179728]]\n",
      "Current iteration=4, loss=39122.319584575\n",
      "t [[ 0.00681319]\n",
      " [-0.07212746]\n",
      " [-0.18089843]\n",
      " ...\n",
      " [-0.13710332]\n",
      " [ 0.0232249 ]\n",
      " [-0.08825069]]\n",
      "t [[ 0.00681319]\n",
      " [-0.07212746]\n",
      " [-0.18089843]\n",
      " ...\n",
      " [-0.13710332]\n",
      " [ 0.0232249 ]\n",
      " [-0.08825069]]\n",
      "t [[ 0.00719433]\n",
      " [-0.08790304]\n",
      " [-0.21017313]\n",
      " ...\n",
      " [-0.15880338]\n",
      " [ 0.02584844]\n",
      " [-0.10417317]]\n",
      "t [[ 0.00719433]\n",
      " [-0.08790304]\n",
      " [-0.21017313]\n",
      " ...\n",
      " [-0.15880338]\n",
      " [ 0.02584844]\n",
      " [-0.10417317]]\n",
      "Current iteration=6, loss=38667.4138982772\n",
      "t [[ 0.00733096]\n",
      " [-0.10396726]\n",
      " [-0.23762085]\n",
      " ...\n",
      " [-0.17899827]\n",
      " [ 0.0279266 ]\n",
      " [-0.11959431]]\n",
      "t [[ 0.00733096]\n",
      " [-0.10396726]\n",
      " [-0.23762085]\n",
      " ...\n",
      " [-0.17899827]\n",
      " [ 0.0279266 ]\n",
      " [-0.11959431]]\n",
      "t [[ 0.00725166]\n",
      " [-0.12026509]\n",
      " [-0.26340778]\n",
      " ...\n",
      " [-0.19782857]\n",
      " [ 0.0295051 ]\n",
      " [-0.13454144]]\n",
      "t [[ 0.00725166]\n",
      " [-0.12026509]\n",
      " [-0.26340778]\n",
      " ...\n",
      " [-0.19782857]\n",
      " [ 0.0295051 ]\n",
      " [-0.13454144]]\n",
      "Current iteration=8, loss=38276.07073934251\n",
      "t [[ 0.00698205]\n",
      " [-0.13674755]\n",
      " [-0.28768385]\n",
      " ...\n",
      " [-0.21542112]\n",
      " [ 0.0306255 ]\n",
      " [-0.14903981]]\n",
      "t [[ 0.00698205]\n",
      " [-0.13674755]\n",
      " [-0.28768385]\n",
      " ...\n",
      " [-0.21542112]\n",
      " [ 0.0306255 ]\n",
      " [-0.14903981]]\n",
      "t [[ 0.00654499]\n",
      " [-0.15337118]\n",
      " [-0.31058419]\n",
      " ...\n",
      " [-0.23189024]\n",
      " [ 0.03132552]\n",
      " [-0.16311275]]\n",
      "loss=37933.98255937103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00183902]\n",
      " [-0.01163477]\n",
      " [-0.04201087]\n",
      " ...\n",
      " [-0.03135947]\n",
      " [ 0.00666641]\n",
      " [-0.01774726]]\n",
      "t [[ 0.00183902]\n",
      " [-0.01163477]\n",
      " [-0.04201087]\n",
      " ...\n",
      " [-0.03135947]\n",
      " [ 0.00666641]\n",
      " [-0.01774726]]\n",
      "t [[ 0.00324262]\n",
      " [-0.02397372]\n",
      " [-0.08108363]\n",
      " ...\n",
      " [-0.06030142]\n",
      " [ 0.01247996]\n",
      " [-0.03484426]]\n",
      "t [[ 0.00324262]\n",
      " [-0.02397372]\n",
      " [-0.08108363]\n",
      " ...\n",
      " [-0.06030142]\n",
      " [ 0.01247996]\n",
      " [-0.03484426]]\n",
      "Current iteration=2, loss=39667.87517585917\n",
      "t [[ 0.00425737]\n",
      " [-0.03692398]\n",
      " [-0.11747961]\n",
      " ...\n",
      " [-0.08704585]\n",
      " [ 0.01750976]\n",
      " [-0.05132917]]\n",
      "t [[ 0.00425737]\n",
      " [-0.03692398]\n",
      " [-0.11747961]\n",
      " ...\n",
      " [-0.08704585]\n",
      " [ 0.01750976]\n",
      " [-0.05132917]]\n",
      "t [[ 0.00492624]\n",
      " [-0.05040065]\n",
      " [-0.15144012]\n",
      " ...\n",
      " [-0.11179585]\n",
      " [ 0.02182037]\n",
      " [-0.06723799]]\n",
      "t [[ 0.00492624]\n",
      " [-0.05040065]\n",
      " [-0.15144012]\n",
      " ...\n",
      " [-0.11179585]\n",
      " [ 0.02182037]\n",
      " [-0.06723799]]\n",
      "Current iteration=4, loss=39135.2225580142\n",
      "t [[ 0.00528843]\n",
      " [-0.06432669]\n",
      " [-0.18318624]\n",
      " ...\n",
      " [-0.13473732]\n",
      " [ 0.02547151]\n",
      " [-0.08260436]]\n",
      "t [[ 0.00528843]\n",
      " [-0.06432669]\n",
      " [-0.18318624]\n",
      " ...\n",
      " [-0.13473732]\n",
      " [ 0.02547151]\n",
      " [-0.08260436]]\n",
      "t [[ 0.00537947]\n",
      " [-0.07863273]\n",
      " [-0.21291927]\n",
      " ...\n",
      " [-0.15603952]\n",
      " [ 0.02851804]\n",
      " [-0.09745952]]\n",
      "t [[ 0.00537947]\n",
      " [-0.07863273]\n",
      " [-0.21291927]\n",
      " ...\n",
      " [-0.15603952]\n",
      " [ 0.02851804]\n",
      " [-0.09745952]]\n",
      "Current iteration=6, loss=38685.766222001446\n",
      "t [[ 0.00523133]\n",
      " [-0.09325654]\n",
      " [-0.24082177]\n",
      " ...\n",
      " [-0.17585589]\n",
      " [ 0.0310101 ]\n",
      " [-0.11183242]]\n",
      "t [[ 0.00523133]\n",
      " [-0.09325654]\n",
      " [-0.24082177]\n",
      " ...\n",
      " [-0.17585589]\n",
      " [ 0.0310101 ]\n",
      " [-0.11183242]]\n",
      "t [[ 0.00487275]\n",
      " [-0.10814249]\n",
      " [-0.26705885]\n",
      " ...\n",
      " [-0.19432516]\n",
      " [ 0.03299335]\n",
      " [-0.12574982]]\n",
      "t [[ 0.00487275]\n",
      " [-0.10814249]\n",
      " [-0.26705885]\n",
      " ...\n",
      " [-0.19432516]\n",
      " [ 0.03299335]\n",
      " [-0.12574982]]\n",
      "Current iteration=8, loss=38299.45643115796\n",
      "t [[ 0.00432942]\n",
      " [-0.123241  ]\n",
      " [-0.29177956]\n",
      " ...\n",
      " [-0.21157255]\n",
      " [ 0.03450933]\n",
      " [-0.1392364 ]]\n",
      "t [[ 0.00432942]\n",
      " [-0.123241  ]\n",
      " [-0.29177956]\n",
      " ...\n",
      " [-0.21157255]\n",
      " [ 0.03450933]\n",
      " [-0.1392364 ]]\n",
      "t [[ 0.00362431]\n",
      " [-0.13850795]\n",
      " [-0.31511837]\n",
      " ...\n",
      " [-0.22771096]\n",
      " [ 0.03559579]\n",
      " [-0.15231493]]\n",
      "loss=37962.07574532175\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00153153]\n",
      " [-0.01232322]\n",
      " [-0.04173367]\n",
      " ...\n",
      " [-0.01837426]\n",
      " [-0.00221295]\n",
      " [-0.03748647]]\n",
      "t [[ 0.00153153]\n",
      " [-0.01232322]\n",
      " [-0.04173367]\n",
      " ...\n",
      " [-0.01837426]\n",
      " [-0.00221295]\n",
      " [-0.03748647]]\n",
      "t [[ 0.00263624]\n",
      " [-0.02534583]\n",
      " [-0.08050474]\n",
      " ...\n",
      " [-0.0348498 ]\n",
      " [-0.00379809]\n",
      " [-0.07386787]]\n",
      "t [[ 0.00263624]\n",
      " [-0.02534583]\n",
      " [-0.08050474]\n",
      " ...\n",
      " [-0.0348498 ]\n",
      " [-0.00379809]\n",
      " [-0.07386787]]\n",
      "Current iteration=2, loss=39656.49262749441\n",
      "t [[ 0.00336052]\n",
      " [-0.03897438]\n",
      " [-0.11657797]\n",
      " ...\n",
      " [-0.04961288]\n",
      " [-0.00483106]\n",
      " [-0.10920047]]\n",
      "t [[ 0.00336052]\n",
      " [-0.03897438]\n",
      " [-0.11657797]\n",
      " ...\n",
      " [-0.04961288]\n",
      " [-0.00483106]\n",
      " [-0.10920047]]\n",
      "t [[ 0.00374708]\n",
      " [-0.0531234 ]\n",
      " [-0.15019786]\n",
      " ...\n",
      " [-0.06283479]\n",
      " [-0.00538067]\n",
      " [-0.14353774]]\n",
      "t [[ 0.00374708]\n",
      " [-0.0531234 ]\n",
      " [-0.15019786]\n",
      " ...\n",
      " [-0.06283479]\n",
      " [-0.00538067]\n",
      " [-0.14353774]]\n",
      "Current iteration=4, loss=39114.67014234568\n",
      "t [[ 0.00383491]\n",
      " [-0.06771541]\n",
      " [-0.18158831]\n",
      " ...\n",
      " [-0.07467185]\n",
      " [-0.0055091 ]\n",
      " [-0.17693012]]\n",
      "t [[ 0.00383491]\n",
      " [-0.06771541]\n",
      " [-0.18158831]\n",
      " ...\n",
      " [-0.07467185]\n",
      " [-0.0055091 ]\n",
      " [-0.17693012]]\n",
      "t [[ 0.00365928]\n",
      " [-0.08268062]\n",
      " [-0.21095318]\n",
      " ...\n",
      " [-0.08526582]\n",
      " [-0.0052722 ]\n",
      " [-0.20942503]]\n",
      "t [[ 0.00365928]\n",
      " [-0.08268062]\n",
      " [-0.21095318]\n",
      " ...\n",
      " [-0.08526582]\n",
      " [-0.0052722 ]\n",
      " [-0.20942503]]\n",
      "Current iteration=6, loss=38657.59930497724\n",
      "t [[ 0.00325193]\n",
      " [-0.09795648]\n",
      " [-0.23847728]\n",
      " ...\n",
      " [-0.09474473]\n",
      " [-0.00471994]\n",
      " [-0.24106686]]\n",
      "t [[ 0.00325193]\n",
      " [-0.09795648]\n",
      " [-0.23847728]\n",
      " ...\n",
      " [-0.09474473]\n",
      " [-0.00471994]\n",
      " [-0.24106686]]\n",
      "t [[ 0.00264134]\n",
      " [-0.11348713]\n",
      " [-0.26432769]\n",
      " ...\n",
      " [-0.10322394]\n",
      " [-0.00389689]\n",
      " [-0.27189711]]\n",
      "t [[ 0.00264134]\n",
      " [-0.11348713]\n",
      " [-0.26432769]\n",
      " ...\n",
      " [-0.10322394]\n",
      " [-0.00389689]\n",
      " [-0.27189711]]\n",
      "Current iteration=8, loss=38264.78805369277\n",
      "t [[ 0.00185298]\n",
      " [-0.1292228 ]\n",
      " [-0.28865522]\n",
      " ...\n",
      " [-0.11080712]\n",
      " [-0.0028427 ]\n",
      " [-0.30195451]]\n",
      "t [[ 0.00185298]\n",
      " [-0.1292228 ]\n",
      " [-0.28865522]\n",
      " ...\n",
      " [-0.11080712]\n",
      " [-0.0028427 ]\n",
      " [-0.30195451]]\n",
      "t [[ 0.00090958]\n",
      " [-0.14511924]\n",
      " [-0.31159584]\n",
      " ...\n",
      " [-0.1175874 ]\n",
      " [-0.00159262]\n",
      " [-0.33127521]]\n",
      "loss=37921.722684952154\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01904849]\n",
      " [-0.08137296]\n",
      " [-0.05114583]\n",
      " ...\n",
      " [-0.03706805]\n",
      " [ 0.00802544]\n",
      " [-0.02160873]]\n",
      "t [[ 0.01904849]\n",
      " [-0.08137296]\n",
      " [-0.05114583]\n",
      " ...\n",
      " [-0.03706805]\n",
      " [ 0.00802544]\n",
      " [-0.02160873]]\n",
      "t [[ 0.03636741]\n",
      " [-0.15680707]\n",
      " [-0.09838763]\n",
      " ...\n",
      " [-0.07087114]\n",
      " [ 0.01487389]\n",
      " [-0.04232201]]\n",
      "t [[ 0.03636741]\n",
      " [-0.15680707]\n",
      " [-0.09838763]\n",
      " ...\n",
      " [-0.07087114]\n",
      " [ 0.01487389]\n",
      " [-0.04232201]]\n",
      "Current iteration=2, loss=39570.28274598911\n",
      "t [[ 0.05212088]\n",
      " [-0.22686575]\n",
      " [-0.14211716]\n",
      " ...\n",
      " [-0.10175393]\n",
      " [ 0.02065542]\n",
      " [-0.06220009]]\n",
      "t [[ 0.05212088]\n",
      " [-0.22686575]\n",
      " [-0.14211716]\n",
      " ...\n",
      " [-0.10175393]\n",
      " [ 0.02065542]\n",
      " [-0.06220009]]\n",
      "t [[ 0.06645946]\n",
      " [-0.29206423]\n",
      " [-0.18269134]\n",
      " ...\n",
      " [-0.13002957]\n",
      " [ 0.02547156]\n",
      " [-0.08129903]]\n",
      "t [[ 0.06645946]\n",
      " [-0.29206423]\n",
      " [-0.18269134]\n",
      " ...\n",
      " [-0.13002957]\n",
      " [ 0.02547156]\n",
      " [-0.08129903]]\n",
      "Current iteration=4, loss=38974.25110300347\n",
      "t [[ 0.07952001]\n",
      " [-0.35286875]\n",
      " [-0.22043186]\n",
      " ...\n",
      " [-0.15597971]\n",
      " [ 0.02941478]\n",
      " [-0.09967053]]\n",
      "t [[ 0.07952001]\n",
      " [-0.35286875]\n",
      " [-0.22043186]\n",
      " ...\n",
      " [-0.15597971]\n",
      " [ 0.02941478]\n",
      " [-0.09967053]]\n",
      "t [[ 0.09142609]\n",
      " [-0.40969825]\n",
      " [-0.25562665]\n",
      " ...\n",
      " [-0.17985594]\n",
      " [ 0.03256859]\n",
      " [-0.11736196]]\n",
      "t [[ 0.09142609]\n",
      " [-0.40969825]\n",
      " [-0.25562665]\n",
      " ...\n",
      " [-0.17985594]\n",
      " [ 0.03256859]\n",
      " [-0.11736196]]\n",
      "Current iteration=6, loss=38482.11997588381\n",
      "t [[ 0.10228879]\n",
      " [-0.46292762]\n",
      " [-0.28853226]\n",
      " ...\n",
      " [-0.20188209]\n",
      " [ 0.03500802]\n",
      " [-0.13441658]]\n",
      "t [[ 0.10228879]\n",
      " [-0.46292762]\n",
      " [-0.28853226]\n",
      " ...\n",
      " [-0.20188209]\n",
      " [ 0.03500802]\n",
      " [-0.13441658]]\n",
      "t [[ 0.11220779]\n",
      " [-0.51289145]\n",
      " [-0.31937667]\n",
      " ...\n",
      " [-0.22225669]\n",
      " [ 0.03680028]\n",
      " [-0.15087383]]\n",
      "t [[ 0.11220779]\n",
      " [-0.51289145]\n",
      " [-0.31937667]\n",
      " ...\n",
      " [-0.22225669]\n",
      " [ 0.03680028]\n",
      " [-0.15087383]]\n",
      "Current iteration=8, loss=38065.94517242433\n",
      "t [[ 0.1212724 ]\n",
      " [-0.55988814]\n",
      " [-0.34836225]\n",
      " ...\n",
      " [-0.24115566]\n",
      " [ 0.03800553]\n",
      " [-0.16676964]]\n",
      "t [[ 0.1212724 ]\n",
      " [-0.55988814]\n",
      " [-0.34836225]\n",
      " ...\n",
      " [-0.24115566]\n",
      " [ 0.03800553]\n",
      " [-0.16676964]]\n",
      "t [[ 0.12956262]\n",
      " [-0.60418372]\n",
      " [-0.3756686 ]\n",
      " ...\n",
      " [-0.25873478]\n",
      " [ 0.0386776 ]\n",
      " [-0.18213679]]\n",
      "loss=37706.935229092975\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00251199]\n",
      " [-0.01546601]\n",
      " [-0.04848754]\n",
      " ...\n",
      " [-0.03720829]\n",
      " [ 0.00723355]\n",
      " [-0.02207361]]\n",
      "t [[ 0.00251199]\n",
      " [-0.01546601]\n",
      " [-0.04848754]\n",
      " ...\n",
      " [-0.03720829]\n",
      " [ 0.00723355]\n",
      " [-0.02207361]]\n",
      "t [[ 0.00442557]\n",
      " [-0.03184791]\n",
      " [-0.09296829]\n",
      " ...\n",
      " [-0.07108109]\n",
      " [ 0.01331769]\n",
      " [-0.04323144]]\n",
      "t [[ 0.00442557]\n",
      " [-0.03184791]\n",
      " [-0.09296829]\n",
      " ...\n",
      " [-0.07108109]\n",
      " [ 0.01331769]\n",
      " [-0.04323144]]\n",
      "Current iteration=2, loss=39561.64156742538\n",
      "t [[ 0.0058145 ]\n",
      " [-0.04899836]\n",
      " [-0.13386244]\n",
      " ...\n",
      " [-0.10197434]\n",
      " [ 0.01836292]\n",
      " [-0.06353563]]\n",
      "t [[ 0.0058145 ]\n",
      " [-0.04899836]\n",
      " [-0.13386244]\n",
      " ...\n",
      " [-0.10197434]\n",
      " [ 0.01836292]\n",
      " [-0.06353563]]\n",
      "t [[ 0.00674569]\n",
      " [-0.06678518]\n",
      " [-0.17155149]\n",
      " ...\n",
      " [-0.13021097]\n",
      " [ 0.02247097]\n",
      " [-0.08304395]]\n",
      "t [[ 0.00674569]\n",
      " [-0.06678518]\n",
      " [-0.17155149]\n",
      " ...\n",
      " [-0.13021097]\n",
      " [ 0.02247097]\n",
      " [-0.08304395]]\n",
      "Current iteration=4, loss=38958.6938653924\n",
      "t [[ 0.00727901]\n",
      " [-0.08509115]\n",
      " [-0.20637818]\n",
      " ...\n",
      " [-0.15608106]\n",
      " [ 0.02573422]\n",
      " [-0.1018096 ]]\n",
      "t [[ 0.00727901]\n",
      " [-0.08509115]\n",
      " [-0.20637818]\n",
      " ...\n",
      " [-0.15608106]\n",
      " [ 0.02573422]\n",
      " [-0.1018096 ]]\n",
      "t [[ 0.00746755]\n",
      " [-0.1038131 ]\n",
      " [-0.23864832]\n",
      " ...\n",
      " [-0.17984332]\n",
      " [ 0.0282359 ]\n",
      " [-0.11988129]]\n",
      "t [[ 0.00746755]\n",
      " [-0.1038131 ]\n",
      " [-0.23864832]\n",
      " ...\n",
      " [-0.17984332]\n",
      " [ 0.0282359 ]\n",
      " [-0.11988129]]\n",
      "Current iteration=6, loss=38460.57625002823\n",
      "t [[ 0.00735812]\n",
      " [-0.12286075]\n",
      " [-0.26863353]\n",
      " ...\n",
      " [-0.20172758]\n",
      " [ 0.03005058]\n",
      " [-0.13730342]]\n",
      "t [[ 0.00735812]\n",
      " [-0.12286075]\n",
      " [-0.26863353]\n",
      " ...\n",
      " [-0.20172758]\n",
      " [ 0.03005058]\n",
      " [-0.13730342]]\n",
      "t [[ 0.00699183]\n",
      " [-0.14215542]\n",
      " [-0.29657447]\n",
      " ...\n",
      " [-0.22193743]\n",
      " [ 0.03124488]\n",
      " [-0.15411646]]\n",
      "t [[ 0.00699183]\n",
      " [-0.14215542]\n",
      " [-0.29657447]\n",
      " ...\n",
      " [-0.22193743]\n",
      " [ 0.03124488]\n",
      " [-0.15411646]]\n",
      "Current iteration=8, loss=38038.96003456318\n",
      "t [[ 0.00640471]\n",
      " [-0.16162869]\n",
      " [-0.32268409]\n",
      " ...\n",
      " [-0.24065301]\n",
      " [ 0.03187825]\n",
      " [-0.17035722]]\n",
      "t [[ 0.00640471]\n",
      " [-0.16162869]\n",
      " [-0.32268409]\n",
      " ...\n",
      " [-0.24065301]\n",
      " [ 0.03187825]\n",
      " [-0.17035722]]\n",
      "t [[ 0.00562828]\n",
      " [-0.18122123]\n",
      " [-0.34715075]\n",
      " ...\n",
      " [-0.25803363]\n",
      " [ 0.03200376]\n",
      " [-0.18605926]]\n",
      "loss=37674.881042323745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00214552]\n",
      " [-0.01357389]\n",
      " [-0.04901268]\n",
      " ...\n",
      " [-0.03658605]\n",
      " [ 0.00777747]\n",
      " [-0.02070514]]\n",
      "t [[ 0.00214552]\n",
      " [-0.01357389]\n",
      " [-0.04901268]\n",
      " ...\n",
      " [-0.03658605]\n",
      " [ 0.00777747]\n",
      " [-0.02070514]]\n",
      "t [[ 0.00369851]\n",
      " [-0.02810608]\n",
      " [-0.09402687]\n",
      " ...\n",
      " [-0.06988206]\n",
      " [ 0.01439433]\n",
      " [-0.0405253 ]]\n",
      "t [[ 0.00369851]\n",
      " [-0.02810608]\n",
      " [-0.09402687]\n",
      " ...\n",
      " [-0.06988206]\n",
      " [ 0.01439433]\n",
      " [-0.0405253 ]]\n",
      "Current iteration=2, loss=39569.59339654575\n",
      "t [[ 0.00473318]\n",
      " [-0.04344876]\n",
      " [-0.13545871]\n",
      " ...\n",
      " [-0.10023839]\n",
      " [ 0.0199607 ]\n",
      " [-0.05952133]]\n",
      "t [[ 0.00473318]\n",
      " [-0.04344876]\n",
      " [-0.13545871]\n",
      " ...\n",
      " [-0.10023839]\n",
      " [ 0.0199607 ]\n",
      " [-0.05952133]]\n",
      "t [[ 0.00531679]\n",
      " [-0.05946919]\n",
      " [-0.17368623]\n",
      " ...\n",
      " [-0.12797307]\n",
      " [ 0.02457796]\n",
      " [-0.07774986]]\n",
      "t [[ 0.00531679]\n",
      " [-0.05946919]\n",
      " [-0.17368623]\n",
      " ...\n",
      " [-0.12797307]\n",
      " [ 0.02457796]\n",
      " [-0.07774986]]\n",
      "Current iteration=4, loss=38973.49364365149\n",
      "t [[ 0.0055095 ]\n",
      " [-0.07604947]\n",
      " [-0.20904931]\n",
      " ...\n",
      " [-0.15337191]\n",
      " [ 0.02833824]\n",
      " [-0.09526298]]\n",
      "t [[ 0.0055095 ]\n",
      " [-0.07604947]\n",
      " [-0.20904931]\n",
      " ...\n",
      " [-0.15337191]\n",
      " [ 0.02833824]\n",
      " [-0.09526298]]\n",
      "t [[ 0.00536466]\n",
      " [-0.09308562]\n",
      " [-0.24185143]\n",
      " ...\n",
      " [-0.17669003]\n",
      " [ 0.03132458]\n",
      " [-0.11210838]]\n",
      "t [[ 0.00536466]\n",
      " [-0.09308562]\n",
      " [-0.24185143]\n",
      " ...\n",
      " [-0.17669003]\n",
      " [ 0.03132458]\n",
      " [-0.11210838]]\n",
      "Current iteration=6, loss=38481.51860399378\n",
      "t [[ 0.00492928]\n",
      " [-0.11048645]\n",
      " [-0.2723624 ]\n",
      " ...\n",
      " [-0.19815418]\n",
      " [ 0.03361144]\n",
      " [-0.1283295 ]]\n",
      "t [[ 0.00492928]\n",
      " [-0.11048645]\n",
      " [-0.2723624 ]\n",
      " ...\n",
      " [-0.19815418]\n",
      " [ 0.03361144]\n",
      " [-0.1283295 ]]\n",
      "t [[ 0.00424464]\n",
      " [-0.12817231]\n",
      " [-0.30082149]\n",
      " ...\n",
      " [-0.21796535]\n",
      " [ 0.03526543]\n",
      " [-0.1439659 ]]\n",
      "t [[ 0.00424464]\n",
      " [-0.12817231]\n",
      " [-0.30082149]\n",
      " ...\n",
      " [-0.21796535]\n",
      " [ 0.03526543]\n",
      " [-0.1439659 ]]\n",
      "Current iteration=8, loss=38065.54647818749\n",
      "t [[ 0.00334689]\n",
      " [-0.14607373]\n",
      " [-0.32744058]\n",
      " ...\n",
      " [-0.23630146]\n",
      " [ 0.03634603]\n",
      " [-0.15905354]]\n",
      "t [[ 0.00334689]\n",
      " [-0.14607373]\n",
      " [-0.32744058]\n",
      " ...\n",
      " [-0.23630146]\n",
      " [ 0.03634603]\n",
      " [-0.15905354]]\n",
      "t [[ 0.00226766]\n",
      " [-0.16413033]\n",
      " [-0.35240727]\n",
      " ...\n",
      " [-0.25331997]\n",
      " [ 0.0369064 ]\n",
      " [-0.17362518]]\n",
      "loss=37706.71793638922\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00178679]\n",
      " [-0.01437709]\n",
      " [-0.04868928]\n",
      " ...\n",
      " [-0.02143664]\n",
      " [-0.00258177]\n",
      " [-0.04373421]]\n",
      "t [[ 0.00178679]\n",
      " [-0.01437709]\n",
      " [-0.04868928]\n",
      " ...\n",
      " [-0.02143664]\n",
      " [-0.00258177]\n",
      " [-0.04373421]]\n",
      "t [[ 0.00299275]\n",
      " [-0.02970594]\n",
      " [-0.09334675]\n",
      " ...\n",
      " [-0.0402893 ]\n",
      " [-0.00430915]\n",
      " [-0.08596446]]\n",
      "t [[ 0.00299275]\n",
      " [-0.02970594]\n",
      " [-0.09334675]\n",
      " ...\n",
      " [-0.0402893 ]\n",
      " [-0.00430915]\n",
      " [-0.08596446]]\n",
      "Current iteration=2, loss=39556.49755830079\n",
      "t [[ 0.00369177]\n",
      " [-0.04583781]\n",
      " [-0.13439404]\n",
      " ...\n",
      " [-0.05685451]\n",
      " [-0.00530245]\n",
      " [-0.12678043]]\n",
      "t [[ 0.00369177]\n",
      " [-0.04583781]\n",
      " [-0.13439404]\n",
      " ...\n",
      " [-0.05685451]\n",
      " [-0.00530245]\n",
      " [-0.12678043]]\n",
      "t [[ 0.00395076]\n",
      " [-0.0626391 ]\n",
      " [-0.17221411]\n",
      " ...\n",
      " [-0.07140038]\n",
      " [-0.00566913]\n",
      " [-0.1662664 ]]\n",
      "t [[ 0.00395076]\n",
      " [-0.0626391 ]\n",
      " [-0.17221411]\n",
      " ...\n",
      " [-0.07140038]\n",
      " [-0.00566913]\n",
      " [-0.1662664 ]]\n",
      "Current iteration=4, loss=38950.18555952627\n",
      "t [[ 0.00382948]\n",
      " [-0.07999122]\n",
      " [-0.20715121]\n",
      " ...\n",
      " [-0.08416682]\n",
      " [-0.00550427]\n",
      " [-0.20450089]]\n",
      "t [[ 0.00382948]\n",
      " [-0.07999122]\n",
      " [-0.20715121]\n",
      " ...\n",
      " [-0.08416682]\n",
      " [-0.00550427]\n",
      " [-0.20450089]]\n",
      "t [[ 0.00338091]\n",
      " [-0.09778962]\n",
      " [-0.23951259]\n",
      " ...\n",
      " [-0.09536709]\n",
      " [-0.00489145]\n",
      " [-0.24155668]]\n",
      "t [[ 0.00338091]\n",
      " [-0.09778962]\n",
      " [-0.23951259]\n",
      " ...\n",
      " [-0.09536709]\n",
      " [-0.00489145]\n",
      " [-0.24155668]]\n",
      "Current iteration=6, loss=38449.91025941297\n",
      "t [[ 0.00265166]\n",
      " [-0.11594271]\n",
      " [-0.26957132]\n",
      " ...\n",
      " [-0.10518988]\n",
      " [-0.00390378]\n",
      " [-0.27750101]]\n",
      "t [[ 0.00265166]\n",
      " [-0.11594271]\n",
      " [-0.26957132]\n",
      " ...\n",
      " [-0.10518988]\n",
      " [-0.00390378]\n",
      " [-0.27750101]]\n",
      "t [[ 0.00168263]\n",
      " [-0.13437052]\n",
      " [-0.29756946]\n",
      " ...\n",
      " [-0.11380167]\n",
      " [-0.00260502]\n",
      " [-0.31239589]]\n",
      "t [[ 0.00168263]\n",
      " [-0.13437052]\n",
      " [-0.29756946]\n",
      " ...\n",
      " [-0.11380167]\n",
      " [-0.00260502]\n",
      " [-0.31239589]]\n",
      "Current iteration=8, loss=38026.941724607124\n",
      "t [[ 0.00050961]\n",
      " [-0.15300342]\n",
      " [-0.32372128]\n",
      " ...\n",
      " [-0.12134916]\n",
      " [-0.00105062]\n",
      " [-0.34629846]]\n",
      "t [[ 0.00050961]\n",
      " [-0.15300342]\n",
      " [-0.32372128]\n",
      " ...\n",
      " [-0.12134916]\n",
      " [-0.00105062]\n",
      " [-0.34629846]]\n",
      "t [[-0.00083613]\n",
      " [-0.17178091]\n",
      " [-0.34821641]\n",
      " ...\n",
      " [-0.12796148]\n",
      " [ 0.00071126]\n",
      " [-0.37926141]]\n",
      "loss=37662.04039436646\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0217697 ]\n",
      " [-0.09299767]\n",
      " [-0.05845238]\n",
      " ...\n",
      " [-0.04236349]\n",
      " [ 0.00917193]\n",
      " [-0.02469569]]\n",
      "t [[ 0.0217697 ]\n",
      " [-0.09299767]\n",
      " [-0.05845238]\n",
      " ...\n",
      " [-0.04236349]\n",
      " [ 0.00917193]\n",
      " [-0.02469569]]\n",
      "t [[ 0.04128075]\n",
      " [-0.17823978]\n",
      " [-0.1118065 ]\n",
      " ...\n",
      " [-0.08046325]\n",
      " [ 0.01680689]\n",
      " [-0.04822197]]\n",
      "t [[ 0.04128075]\n",
      " [-0.17823978]\n",
      " [-0.1118065 ]\n",
      " ...\n",
      " [-0.08046325]\n",
      " [ 0.01680689]\n",
      " [-0.04822197]]\n",
      "Current iteration=2, loss=39474.46246329937\n",
      "t [[ 0.0587788 ]\n",
      " [-0.25656981]\n",
      " [-0.16064877]\n",
      " ...\n",
      " [-0.11481508]\n",
      " [ 0.02306976]\n",
      " [-0.07066906]]\n",
      "t [[ 0.0587788 ]\n",
      " [-0.25656981]\n",
      " [-0.16064877]\n",
      " ...\n",
      " [-0.11481508]\n",
      " [ 0.02306976]\n",
      " [-0.07066906]]\n",
      "t [[ 0.07448577]\n",
      " [-0.32874648]\n",
      " [-0.20550439]\n",
      " ...\n",
      " [-0.14587958]\n",
      " [ 0.02811023]\n",
      " [-0.09211979]]\n",
      "t [[ 0.07448577]\n",
      " [-0.32874648]\n",
      " [-0.20550439]\n",
      " ...\n",
      " [-0.14587958]\n",
      " [ 0.02811023]\n",
      " [-0.09211979]]\n",
      "Current iteration=4, loss=38820.47176943463\n",
      "t [[ 0.08859979]\n",
      " [-0.39544373]\n",
      " [-0.2468378 ]\n",
      " ...\n",
      " [-0.17406308]\n",
      " [ 0.03206221]\n",
      " [-0.11264937]]\n",
      "t [[ 0.08859979]\n",
      " [-0.39544373]\n",
      " [-0.2468378 ]\n",
      " ...\n",
      " [-0.17406308]\n",
      " [ 0.03206221]\n",
      " [-0.11264937]]\n",
      "t [[ 0.10129651]\n",
      " [-0.45725568]\n",
      " [-0.28505646]\n",
      " ...\n",
      " [-0.19972141]\n",
      " [ 0.03504432]\n",
      " [-0.13232564]]\n",
      "t [[ 0.10129651]\n",
      " [-0.45725568]\n",
      " [-0.28505646]\n",
      " ...\n",
      " [-0.19972141]\n",
      " [ 0.03504432]\n",
      " [-0.13232564]]\n",
      "Current iteration=6, loss=38291.4412036011\n",
      "t [[ 0.11273104]\n",
      " [-0.51470387]\n",
      " [-0.32051631]\n",
      " ...\n",
      " [-0.22316476]\n",
      " [ 0.03716115]\n",
      " [-0.15120959]]\n",
      "t [[ 0.11273104]\n",
      " [-0.51470387]\n",
      " [-0.32051631]\n",
      " ...\n",
      " [-0.22316476]\n",
      " [ 0.03716115]\n",
      " [-0.15120959]]\n",
      "t [[ 0.12304006]\n",
      " [-0.56824533]\n",
      " [-0.35352764]\n",
      " ...\n",
      " [-0.24466299]\n",
      " [ 0.03850472]\n",
      " [-0.16935601]]\n",
      "t [[ 0.12304006]\n",
      " [-0.56824533]\n",
      " [-0.35352764]\n",
      " ...\n",
      " [-0.24466299]\n",
      " [ 0.03850472]\n",
      " [-0.16935601]]\n",
      "Current iteration=8, loss=37850.503302894685\n",
      "t [[ 0.13234399]\n",
      " [-0.61828045]\n",
      " [-0.38436079]\n",
      " ...\n",
      " [-0.26445068]\n",
      " [ 0.03915602]\n",
      " [-0.18681417]]\n",
      "t [[ 0.13234399]\n",
      " [-0.61828045]\n",
      " [-0.38436079]\n",
      " ...\n",
      " [-0.26445068]\n",
      " [ 0.03915602]\n",
      " [-0.18681417]]\n",
      "t [[ 0.14074894]\n",
      " [-0.66516033]\n",
      " [-0.41325147]\n",
      " ...\n",
      " [-0.28273177]\n",
      " [ 0.0391864 ]\n",
      " [-0.20362845]]\n",
      "loss=37474.152069790696\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00287085]\n",
      " [-0.01767544]\n",
      " [-0.05541434]\n",
      " ...\n",
      " [-0.04252376]\n",
      " [ 0.00826691]\n",
      " [-0.02522698]]\n",
      "t [[ 0.00287085]\n",
      " [-0.01767544]\n",
      " [-0.05541434]\n",
      " ...\n",
      " [-0.04252376]\n",
      " [ 0.00826691]\n",
      " [-0.02522698]]\n",
      "t [[ 0.00496029]\n",
      " [-0.03654686]\n",
      " [-0.10559622]\n",
      " ...\n",
      " [-0.08069171]\n",
      " [ 0.01503288]\n",
      " [-0.04925801]]\n",
      "t [[ 0.00496029]\n",
      " [-0.03654686]\n",
      " [-0.10559622]\n",
      " ...\n",
      " [-0.08069171]\n",
      " [ 0.01503288]\n",
      " [-0.04925801]]\n",
      "Current iteration=2, loss=39464.718846471354\n",
      "t [[ 0.00637877]\n",
      " [-0.05639388]\n",
      " [-0.15117454]\n",
      " ...\n",
      " [-0.1150365 ]\n",
      " [ 0.02046343]\n",
      " [-0.07218613]]\n",
      "t [[ 0.00637877]\n",
      " [-0.05639388]\n",
      " [-0.15117454]\n",
      " ...\n",
      " [-0.1150365 ]\n",
      " [ 0.02046343]\n",
      " [-0.07218613]]\n",
      "t [[ 0.00722464]\n",
      " [-0.07702257]\n",
      " [-0.19271032]\n",
      " ...\n",
      " [-0.14603303]\n",
      " [ 0.02470848]\n",
      " [-0.09409669]]\n",
      "t [[ 0.00722464]\n",
      " [-0.07702257]\n",
      " [-0.19271032]\n",
      " ...\n",
      " [-0.14603303]\n",
      " [ 0.02470848]\n",
      " [-0.09409669]]\n",
      "Current iteration=4, loss=38803.097614408885\n",
      "t [[ 0.00758408]\n",
      " [-0.09826463]\n",
      " [-0.23069786]\n",
      " ...\n",
      " [-0.17409956]\n",
      " [ 0.02790169]\n",
      " [-0.11506705]]\n",
      "t [[ 0.00758408]\n",
      " [-0.09826463]\n",
      " [-0.23069786]\n",
      " ...\n",
      " [-0.17409956]\n",
      " [ 0.02790169]\n",
      " [-0.11506705]]\n",
      "t [[ 0.00753184]\n",
      " [-0.11997532]\n",
      " [-0.26556927]\n",
      " ...\n",
      " [-0.19960172]\n",
      " [ 0.03016111]\n",
      " [-0.13516691]]\n",
      "t [[ 0.00753184]\n",
      " [-0.11997532]\n",
      " [-0.26556927]\n",
      " ...\n",
      " [-0.19960172]\n",
      " [ 0.03016111]\n",
      " [-0.13516691]]\n",
      "Current iteration=6, loss=38267.482301596516\n",
      "t [[ 0.00713236]\n",
      " [-0.1420309 ]\n",
      " [-0.29770069]\n",
      " ...\n",
      " [-0.22285773]\n",
      " [ 0.03159051]\n",
      " [-0.15445886]]\n",
      "t [[ 0.00713236]\n",
      " [-0.1420309 ]\n",
      " [-0.29770069]\n",
      " ...\n",
      " [-0.22285773]\n",
      " [ 0.03159051]\n",
      " [-0.15445886]]\n",
      "t [[ 0.006441  ]\n",
      " [-0.16432601]\n",
      " [-0.32741882]\n",
      " ...\n",
      " [-0.24414399]\n",
      " [ 0.0322809 ]\n",
      " [-0.17299905]]\n",
      "t [[ 0.006441  ]\n",
      " [-0.16432601]\n",
      " [-0.32741882]\n",
      " ...\n",
      " [-0.24414399]\n",
      " [ 0.0322809 ]\n",
      " [-0.17299905]]\n",
      "Current iteration=8, loss=37820.553595786994\n",
      "t [[ 0.00550527]\n",
      " [-0.1867712 ]\n",
      " [-0.35500729]\n",
      " ...\n",
      " [-0.26370039]\n",
      " [ 0.03231213]\n",
      " [-0.1908379 ]]\n",
      "t [[ 0.00550527]\n",
      " [-0.1867712 ]\n",
      " [-0.35500729]\n",
      " ...\n",
      " [-0.26370039]\n",
      " [ 0.03231213]\n",
      " [-0.1908379 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.00436587]\n",
      " [-0.20929067]\n",
      " [-0.38071244]\n",
      " ...\n",
      " [-0.28173521]\n",
      " [ 0.03175431]\n",
      " [-0.2080208 ]]\n",
      "loss=37438.63263805534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00245202]\n",
      " [-0.01551302]\n",
      " [-0.05601449]\n",
      " ...\n",
      " [-0.04181263]\n",
      " [ 0.00888854]\n",
      " [-0.02366302]]\n",
      "t [[ 0.00245202]\n",
      " [-0.01551302]\n",
      " [-0.05601449]\n",
      " ...\n",
      " [-0.04181263]\n",
      " [ 0.00888854]\n",
      " [-0.02366302]]\n",
      "t [[ 0.00413032]\n",
      " [-0.03227742]\n",
      " [-0.10680736]\n",
      " ...\n",
      " [-0.0793288 ]\n",
      " [ 0.01626149]\n",
      " [-0.04617029]]\n",
      "t [[ 0.00413032]\n",
      " [-0.03227742]\n",
      " [-0.10680736]\n",
      " ...\n",
      " [-0.0793288 ]\n",
      " [ 0.01626149]\n",
      " [-0.04617029]]\n",
      "Current iteration=2, loss=39473.726274351735\n",
      "t [[ 0.00514599]\n",
      " [-0.05007212]\n",
      " [-0.15300141]\n",
      " ...\n",
      " [-0.1130728 ]\n",
      " [ 0.0222838 ]\n",
      " [-0.06761299]]\n",
      "t [[ 0.00514599]\n",
      " [-0.05007212]\n",
      " [-0.15300141]\n",
      " ...\n",
      " [-0.1130728 ]\n",
      " [ 0.0222838 ]\n",
      " [-0.06761299]]\n",
      "t [[ 0.0055979 ]\n",
      " [-0.06870233]\n",
      " [-0.19515264]\n",
      " ...\n",
      " [-0.14351234]\n",
      " [ 0.0271049 ]\n",
      " [-0.08807471]]\n",
      "t [[ 0.0055979 ]\n",
      " [-0.06870233]\n",
      " [-0.19515264]\n",
      " ...\n",
      " [-0.14351234]\n",
      " [ 0.0271049 ]\n",
      " [-0.08807471]]\n",
      "Current iteration=4, loss=38819.73982998158\n",
      "t [[ 0.00557263]\n",
      " [-0.08799865]\n",
      " [-0.23375133]\n",
      " ...\n",
      " [-0.17105969]\n",
      " [ 0.03085808]\n",
      " [-0.10763123]]\n",
      "t [[ 0.00557263]\n",
      " [-0.08799865]\n",
      " [-0.23375133]\n",
      " ...\n",
      " [-0.17105969]\n",
      " [ 0.03085808]\n",
      " [-0.10763123]]\n",
      "t [[ 0.00514529]\n",
      " [-0.10781506]\n",
      " [-0.26922654]\n",
      " ...\n",
      " [-0.19607546]\n",
      " [ 0.03366121]\n",
      " [-0.12635075]]\n",
      "t [[ 0.00514529]\n",
      " [-0.10781506]\n",
      " [-0.26922654]\n",
      " ...\n",
      " [-0.19607546]\n",
      " [ 0.03366121]\n",
      " [-0.12635075]]\n",
      "Current iteration=6, loss=38290.92583290898\n",
      "t [[ 0.00438058]\n",
      " [-0.12802636]\n",
      " [-0.30195209]\n",
      " ...\n",
      " [-0.21887376]\n",
      " [ 0.03561796]\n",
      " [-0.14429446]]\n",
      "t [[ 0.00438058]\n",
      " [-0.12802636]\n",
      " [-0.30195209]\n",
      " ...\n",
      " [-0.21887376]\n",
      " [ 0.03561796]\n",
      " [-0.14429446]]\n",
      "t [[ 0.00333409]\n",
      " [-0.14852566]\n",
      " [-0.33225304]\n",
      " ...\n",
      " [-0.23972759]\n",
      " [ 0.0368194 ]\n",
      " [-0.16151724]]\n",
      "t [[ 0.00333409]\n",
      " [-0.14852566]\n",
      " [-0.33225304]\n",
      " ...\n",
      " [-0.23972759]\n",
      " [ 0.0368194 ]\n",
      " [-0.16151724]]\n",
      "Current iteration=8, loss=37850.215465989444\n",
      "t [[ 0.00205347]\n",
      " [-0.16922191]\n",
      " [-0.36041185]\n",
      " ...\n",
      " [-0.25887401]\n",
      " [ 0.0373455 ]\n",
      " [-0.17806832]]\n",
      "t [[ 0.00205347]\n",
      " [-0.16922191]\n",
      " [-0.36041185]\n",
      " ...\n",
      " [-0.25887401]\n",
      " [ 0.0373455 ]\n",
      " [-0.17806832]]\n",
      "t [[ 0.00057953]\n",
      " [-0.19003771]\n",
      " [-0.38667406]\n",
      " ...\n",
      " [-0.27651896]\n",
      " [ 0.03726658]\n",
      " [-0.19399193]]\n",
      "loss=37474.044544807824\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00204204]\n",
      " [-0.01643097]\n",
      " [-0.05564489]\n",
      " ...\n",
      " [-0.02449902]\n",
      " [-0.0029506 ]\n",
      " [-0.04998196]]\n",
      "t [[ 0.00204204]\n",
      " [-0.01643097]\n",
      " [-0.05564489]\n",
      " ...\n",
      " [-0.02449902]\n",
      " [-0.0029506 ]\n",
      " [-0.04998196]]\n",
      "t [[ 0.00332564]\n",
      " [-0.03410476]\n",
      " [-0.10602466]\n",
      " ...\n",
      " [-0.04562363]\n",
      " [-0.00478543]\n",
      " [-0.09799978]]\n",
      "t [[ 0.00332564]\n",
      " [-0.03410476]\n",
      " [-0.10602466]\n",
      " ...\n",
      " [-0.04562363]\n",
      " [-0.00478543]\n",
      " [-0.09799978]]\n",
      "Current iteration=2, loss=39458.96531457224\n",
      "t [[ 0.00396141]\n",
      " [-0.05279887]\n",
      " [-0.15177033]\n",
      " ...\n",
      " [-0.06381751]\n",
      " [-0.00568439]\n",
      " [-0.14418784]]\n",
      "t [[ 0.00396141]\n",
      " [-0.05279887]\n",
      " [-0.15177033]\n",
      " ...\n",
      " [-0.06381751]\n",
      " [-0.00568439]\n",
      " [-0.14418784]]\n",
      "t [[ 0.00404767]\n",
      " [-0.0723173 ]\n",
      " [-0.19344516]\n",
      " ...\n",
      " [-0.07947464]\n",
      " [-0.00580503]\n",
      " [-0.18867087]]\n",
      "t [[ 0.00404767]\n",
      " [-0.0723173 ]\n",
      " [-0.19344516]\n",
      " ...\n",
      " [-0.07947464]\n",
      " [-0.00580503]\n",
      " [-0.18867087]]\n",
      "Current iteration=4, loss=38793.824356140125\n",
      "t [[ 0.00367042]\n",
      " [-0.09248967]\n",
      " [-0.23154565]\n",
      " ...\n",
      " [-0.09294057]\n",
      " [-0.0052838 ]\n",
      " [-0.23156359]]\n",
      "t [[ 0.00367042]\n",
      " [-0.09248967]\n",
      " [-0.23154565]\n",
      " ...\n",
      " [-0.09294057]\n",
      " [-0.0052838 ]\n",
      " [-0.23156359]]\n",
      "t [[ 0.0029042 ]\n",
      " [-0.11316923]\n",
      " [-0.26650611]\n",
      " ...\n",
      " [-0.10451612]\n",
      " [-0.00423796]\n",
      " [-0.27297082]]\n",
      "t [[ 0.0029042 ]\n",
      " [-0.11316923]\n",
      " [-0.26650611]\n",
      " ...\n",
      " [-0.10451612]\n",
      " [-0.00423796]\n",
      " [-0.27297082]]\n",
      "Current iteration=6, loss=38256.11314565181\n",
      "t [[ 0.00181313]\n",
      " [-0.13423029]\n",
      " [-0.29870478]\n",
      " ...\n",
      " [-0.11446189]\n",
      " [-0.00276778]\n",
      " [-0.3129881 ]]\n",
      "t [[ 0.00181313]\n",
      " [-0.13423029]\n",
      " [-0.29870478]\n",
      " ...\n",
      " [-0.11446189]\n",
      " [-0.00276778]\n",
      " [-0.3129881 ]]\n",
      "t [[ 0.00045225]\n",
      " [-0.15556565]\n",
      " [-0.3284704 ]\n",
      " ...\n",
      " [-0.12300319]\n",
      " [-0.00095867]\n",
      " [-0.35170238]]\n",
      "t [[ 0.00045225]\n",
      " [-0.15556565]\n",
      " [-0.3284704 ]\n",
      " ...\n",
      " [-0.12300319]\n",
      " [-0.00095867]\n",
      " [-0.35170238]]\n",
      "Current iteration=8, loss=37807.977868169175\n",
      "t [[-0.00113132]\n",
      " [-0.1770841 ]\n",
      " [-0.35608849]\n",
      " ...\n",
      " [-0.13033452]\n",
      " [ 0.00111674]\n",
      " [-0.38919282]]\n",
      "t [[-0.00113132]\n",
      " [-0.1770841 ]\n",
      " [-0.35608849]\n",
      " ...\n",
      " [-0.13033452]\n",
      " [ 0.00111674]\n",
      " [-0.38919282]]\n",
      "t [[-0.0028973 ]\n",
      " [-0.19870819]\n",
      " [-0.38180713]\n",
      " ...\n",
      " [-0.1366238 ]\n",
      " [ 0.00339687]\n",
      " [-0.42553155]]\n",
      "loss=37425.3955015285\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.02449091]\n",
      " [-0.10462238]\n",
      " [-0.06575893]\n",
      " ...\n",
      " [-0.04765892]\n",
      " [ 0.01031842]\n",
      " [-0.02778265]]\n",
      "t [[ 0.02449091]\n",
      " [-0.10462238]\n",
      " [-0.06575893]\n",
      " ...\n",
      " [-0.04765892]\n",
      " [ 0.01031842]\n",
      " [-0.02778265]]\n",
      "t [[ 0.04612375]\n",
      " [-0.19943102]\n",
      " [-0.12506664]\n",
      " ...\n",
      " [-0.0899226 ]\n",
      " [ 0.01869208]\n",
      " [-0.05408549]]\n",
      "t [[ 0.04612375]\n",
      " [-0.19943102]\n",
      " [-0.12506664]\n",
      " ...\n",
      " [-0.0899226 ]\n",
      " [ 0.01869208]\n",
      " [-0.05408549]]\n",
      "Current iteration=2, loss=39380.98058438813\n",
      "t [[ 0.06524919]\n",
      " [-0.28563002]\n",
      " [-0.17876016]\n",
      " ...\n",
      " [-0.1275271 ]\n",
      " [ 0.02535645]\n",
      " [-0.07903735]]\n",
      "t [[ 0.06524919]\n",
      " [-0.28563002]\n",
      " [-0.17876016]\n",
      " ...\n",
      " [-0.1275271 ]\n",
      " [ 0.02535645]\n",
      " [-0.07903735]]\n",
      "t [[ 0.08217886]\n",
      " [-0.36428398]\n",
      " [-0.2275759 ]\n",
      " ...\n",
      " [-0.16111799]\n",
      " [ 0.03052186]\n",
      " [-0.10275486]]\n",
      "t [[ 0.08217886]\n",
      " [-0.36428398]\n",
      " [-0.2275759 ]\n",
      " ...\n",
      " [-0.16111799]\n",
      " [ 0.03052186]\n",
      " [-0.10275486]]\n",
      "Current iteration=4, loss=38674.06933617782\n",
      "t [[ 0.09718596]\n",
      " [-0.43632056]\n",
      " [-0.27215225]\n",
      " ...\n",
      " [-0.1912535 ]\n",
      " [ 0.03437286]\n",
      " [-0.12534221]]\n",
      "t [[ 0.09718596]\n",
      " [-0.43632056]\n",
      " [-0.27215225]\n",
      " ...\n",
      " [-0.1912535 ]\n",
      " [ 0.03437286]\n",
      " [-0.12534221]]\n",
      "t [[ 0.11050821]\n",
      " [-0.5025417 ]\n",
      " [-0.3130381 ]\n",
      " ...\n",
      " [-0.21841259]\n",
      " [ 0.0370698 ]\n",
      " [-0.14689191]]\n",
      "t [[ 0.11050821]\n",
      " [-0.5025417 ]\n",
      " [-0.3130381 ]\n",
      " ...\n",
      " [-0.21841259]\n",
      " [ 0.0370698 ]\n",
      " [-0.14689191]]\n",
      "Current iteration=6, loss=38112.942352189566\n",
      "t [[ 0.12235172]\n",
      " [-0.56363808]\n",
      " [-0.35070344]\n",
      " ...\n",
      " [-0.24300443]\n",
      " [ 0.03875147]\n",
      " [-0.16748587]]\n",
      "t [[ 0.12235172]\n",
      " [-0.56363808]\n",
      " [-0.35070344]\n",
      " ...\n",
      " [-0.24300443]\n",
      " [ 0.03875147]\n",
      " [-0.16748587]]\n",
      "t [[ 0.13289498]\n",
      " [-0.62020403]\n",
      " [-0.38555028]\n",
      " ...\n",
      " [-0.26537813]\n",
      " [ 0.03953787]\n",
      " [-0.18719662]]\n",
      "t [[ 0.13289498]\n",
      " [-0.62020403]\n",
      " [-0.38555028]\n",
      " ...\n",
      " [-0.26537813]\n",
      " [ 0.03953787]\n",
      " [-0.18719662]]\n",
      "Current iteration=8, loss=37651.197232688355\n",
      "t [[ 0.14229261]\n",
      " [-0.67275143]\n",
      " [-0.41792258]\n",
      " ...\n",
      " [-0.28583151]\n",
      " [ 0.0395329 ]\n",
      " [-0.20608858]]\n",
      "t [[ 0.14229261]\n",
      " [-0.67275143]\n",
      " [-0.41792258]\n",
      " ...\n",
      " [-0.28583151]\n",
      " [ 0.0395329 ]\n",
      " [-0.20608858]]\n",
      "t [[ 0.15067874]\n",
      " [-0.7217219 ]\n",
      " [-0.44811507]\n",
      " ...\n",
      " [-0.30461885]\n",
      " [ 0.03882676]\n",
      " [-0.22421906]]\n",
      "loss=37260.710441196694\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00322971]\n",
      " [-0.01988487]\n",
      " [-0.06234113]\n",
      " ...\n",
      " [-0.04783923]\n",
      " [ 0.00930027]\n",
      " [-0.02838035]]\n",
      "t [[ 0.00322971]\n",
      " [-0.01988487]\n",
      " [-0.06234113]\n",
      " ...\n",
      " [-0.04783923]\n",
      " [ 0.00930027]\n",
      " [-0.02838035]]\n",
      "t [[ 0.00547072]\n",
      " [-0.04128301]\n",
      " [-0.11806125]\n",
      " ...\n",
      " [-0.09016671]\n",
      " [ 0.01670138]\n",
      " [-0.0552473 ]]\n",
      "t [[ 0.00547072]\n",
      " [-0.04128301]\n",
      " [-0.11806125]\n",
      " ...\n",
      " [-0.09016671]\n",
      " [ 0.01670138]\n",
      " [-0.0552473 ]]\n",
      "Current iteration=2, loss=39370.16284337966\n",
      "t [[ 0.00688072]\n",
      " [-0.06388004]\n",
      " [-0.16805789]\n",
      " ...\n",
      " [-0.12774257]\n",
      " [ 0.02243974]\n",
      " [-0.08073374]]\n",
      "t [[ 0.00688072]\n",
      " [-0.06388004]\n",
      " [-0.16805789]\n",
      " ...\n",
      " [-0.12774257]\n",
      " [ 0.02243974]\n",
      " [-0.08073374]]\n",
      "t [[ 0.00759746]\n",
      " [-0.08740488]\n",
      " [-0.21311722]\n",
      " ...\n",
      " [-0.16123222]\n",
      " [ 0.02672586]\n",
      " [-0.10495979]]\n",
      "t [[ 0.00759746]\n",
      " [-0.08740488]\n",
      " [-0.21311722]\n",
      " ...\n",
      " [-0.16123222]\n",
      " [ 0.02672586]\n",
      " [-0.10495979]]\n",
      "Current iteration=4, loss=38654.94254730242\n",
      "t [[ 0.00773898]\n",
      " [-0.11162751]\n",
      " [-0.25391798]\n",
      " ...\n",
      " [-0.19121001]\n",
      " [ 0.02974379]\n",
      " [-0.12803261]]\n",
      "t [[ 0.00773898]\n",
      " [-0.11162751]\n",
      " [-0.25391798]\n",
      " ...\n",
      " [-0.19121001]\n",
      " [ 0.02974379]\n",
      " [-0.12803261]]\n",
      "t [[ 0.00740542]\n",
      " [-0.1363547 ]\n",
      " [-0.29104134]\n",
      " ...\n",
      " [-0.21816767]\n",
      " [ 0.03165291]\n",
      " [-0.15004717]]\n",
      "t [[ 0.00740542]\n",
      " [-0.1363547 ]\n",
      " [-0.29104134]\n",
      " ...\n",
      " [-0.21816767]\n",
      " [ 0.03165291]\n",
      " [-0.15004717]]\n",
      "Current iteration=6, loss=38086.65164136216\n",
      "t [[ 0.00668117]\n",
      " [-0.16142518]\n",
      " [-0.32498299]\n",
      " ...\n",
      " [-0.24252461]\n",
      " [ 0.03259065]\n",
      " [-0.17108745]]\n",
      "t [[ 0.00668117]\n",
      " [-0.16142518]\n",
      " [-0.32498299]\n",
      " ...\n",
      " [-0.24252461]\n",
      " [ 0.03259065]\n",
      " [-0.17108745]]\n",
      "t [[ 0.00563715]\n",
      " [-0.18670483]\n",
      " [-0.35616511]\n",
      " ...\n",
      " [-0.26463796]\n",
      " [ 0.03267541]\n",
      " [-0.19122773]]\n",
      "t [[ 0.00563715]\n",
      " [-0.18670483]\n",
      " [-0.35616511]\n",
      " ...\n",
      " [-0.26463796]\n",
      " [ 0.03267541]\n",
      " [-0.19122773]]\n",
      "Current iteration=8, loss=37618.38620860166\n",
      "t [[ 0.00433293]\n",
      " [-0.2120825 ]\n",
      " [-0.38494742]\n",
      " ...\n",
      " [-0.2848119 ]\n",
      " [ 0.03200937]\n",
      " [-0.21053389]]\n",
      "t [[ 0.00433293]\n",
      " [-0.2120825 ]\n",
      " [-0.38494742]\n",
      " ...\n",
      " [-0.2848119 ]\n",
      " [ 0.03200937]\n",
      " [-0.21053389]]\n",
      "t [[ 0.00281854]\n",
      " [-0.23746623]\n",
      " [-0.41163676]\n",
      " ...\n",
      " [-0.30330576]\n",
      " [ 0.03068093]\n",
      " [-0.22906449]]\n",
      "loss=37221.8624120399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00275853]\n",
      " [-0.01745215]\n",
      " [-0.0630163 ]\n",
      " ...\n",
      " [-0.04703921]\n",
      " [ 0.00999961]\n",
      " [-0.02662089]]\n",
      "t [[ 0.00275853]\n",
      " [-0.01745215]\n",
      " [-0.0630163 ]\n",
      " ...\n",
      " [-0.04703921]\n",
      " [ 0.00999961]\n",
      " [-0.02662089]]\n",
      "t [[ 0.00453808]\n",
      " [-0.03648769]\n",
      " [-0.11942527]\n",
      " ...\n",
      " [-0.08864175]\n",
      " [ 0.01808149]\n",
      " [-0.05177928]]\n",
      "t [[ 0.00453808]\n",
      " [-0.03648769]\n",
      " [-0.11942527]\n",
      " ...\n",
      " [-0.08864175]\n",
      " [ 0.01808149]\n",
      " [-0.05177928]]\n",
      "Current iteration=2, loss=39380.20805592604\n",
      "t [[ 0.00549726]\n",
      " [-0.05679126]\n",
      " [-0.17011575]\n",
      " ...\n",
      " [-0.12555583]\n",
      " [ 0.02448125]\n",
      " [-0.07560535]]\n",
      "t [[ 0.00549726]\n",
      " [-0.05679126]\n",
      " [-0.17011575]\n",
      " ...\n",
      " [-0.12555583]\n",
      " [ 0.02448125]\n",
      " [-0.07560535]]\n",
      "t [[ 0.0057745 ]\n",
      " [-0.0780905 ]\n",
      " [-0.21586696]\n",
      " ...\n",
      " [-0.15843687]\n",
      " [ 0.0294087 ]\n",
      " [-0.09821678]]\n",
      "t [[ 0.0057745 ]\n",
      " [-0.0780905 ]\n",
      " [-0.21586696]\n",
      " ...\n",
      " [-0.15843687]\n",
      " [ 0.0294087 ]\n",
      " [-0.09821678]]\n",
      "Current iteration=4, loss=38673.377903873625\n",
      "t [[ 0.00548844]\n",
      " [-0.10015372]\n",
      " [-0.25735234]\n",
      " ...\n",
      " [-0.18785109]\n",
      " [ 0.03304747]\n",
      " [-0.11971848]]\n",
      "t [[ 0.00548844]\n",
      " [-0.10015372]\n",
      " [-0.25735234]\n",
      " ...\n",
      " [-0.18785109]\n",
      " [ 0.03304747]\n",
      " [-0.11971848]]\n",
      "t [[ 0.00473964]\n",
      " [-0.12278573]\n",
      " [-0.29514921]\n",
      " ...\n",
      " [-0.21428368]\n",
      " [ 0.03555672]\n",
      " [-0.14020337]]\n",
      "t [[ 0.00473964]\n",
      " [-0.12278573]\n",
      " [-0.29514921]\n",
      " ...\n",
      " [-0.21428368]\n",
      " [ 0.03555672]\n",
      " [-0.14020337]]\n",
      "Current iteration=6, loss=38112.51705563225\n",
      "t [[ 0.00361284]\n",
      " [-0.14582309]\n",
      " [-0.32975053]\n",
      " ...\n",
      " [-0.23814877]\n",
      " [ 0.03707389]\n",
      " [-0.15975352]]\n",
      "t [[ 0.00361284]\n",
      " [-0.14582309]\n",
      " [-0.32975053]\n",
      " ...\n",
      " [-0.23814877]\n",
      " [ 0.03707389]\n",
      " [-0.15975352]]\n",
      "t [[ 0.00217922]\n",
      " [-0.16912943]\n",
      " [-0.36157665]\n",
      " ...\n",
      " [-0.25979925]\n",
      " [ 0.03771754]\n",
      " [-0.17844148]]\n",
      "t [[ 0.00217922]\n",
      " [-0.16912943]\n",
      " [-0.36157665]\n",
      " ...\n",
      " [-0.25979925]\n",
      " [ 0.03771754]\n",
      " [-0.17844148]]\n",
      "Current iteration=8, loss=37651.01001820724\n",
      "t [[ 0.00049853]\n",
      " [-0.19259128]\n",
      " [-0.39098609]\n",
      " ...\n",
      " [-0.27953587]\n",
      " [ 0.03759011]\n",
      " [-0.19633148]]\n",
      "t [[ 0.00049853]\n",
      " [-0.19259128]\n",
      " [-0.39098609]\n",
      " ...\n",
      " [-0.27953587]\n",
      " [ 0.03759011]\n",
      " [-0.19633148]]\n",
      "t [[-0.00137912]\n",
      " [-0.21611442]\n",
      " [-0.41828502]\n",
      " ...\n",
      " [-0.29761518]\n",
      " [ 0.03678036]\n",
      " [-0.2134806 ]]\n",
      "loss=37260.69319535321\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0022973 ]\n",
      " [-0.01848484]\n",
      " [-0.0626005 ]\n",
      " ...\n",
      " [-0.02756139]\n",
      " [-0.00331942]\n",
      " [-0.0562297 ]]\n",
      "t [[ 0.0022973 ]\n",
      " [-0.01848484]\n",
      " [-0.0626005 ]\n",
      " ...\n",
      " [-0.02756139]\n",
      " [-0.00331942]\n",
      " [-0.0562297 ]]\n",
      "t [[ 0.00363496]\n",
      " [-0.03854222]\n",
      " [-0.11853864]\n",
      " ...\n",
      " [-0.0508529 ]\n",
      " [-0.00522697]\n",
      " [-0.10997387]]\n",
      "t [[ 0.00363496]\n",
      " [-0.03854222]\n",
      " [-0.11853864]\n",
      " ...\n",
      " [-0.0508529 ]\n",
      " [-0.00522697]\n",
      " [-0.10997387]]\n",
      "Current iteration=2, loss=39363.828560382724\n",
      "t [[ 0.00417088]\n",
      " [-0.05985476]\n",
      " [-0.16871502]\n",
      " ...\n",
      " [-0.07050756]\n",
      " [-0.00597911]\n",
      " [-0.16142447]]\n",
      "t [[ 0.00417088]\n",
      " [-0.05985476]\n",
      " [-0.16871502]\n",
      " ...\n",
      " [-0.07050756]\n",
      " [-0.00597911]\n",
      " [-0.16142447]]\n",
      "t [[ 0.00404272]\n",
      " [-0.08214838]\n",
      " [-0.21391899]\n",
      " ...\n",
      " [-0.08707704]\n",
      " [-0.00579602]\n",
      " [-0.21075751]]\n",
      "t [[ 0.00404272]\n",
      " [-0.08214838]\n",
      " [-0.21391899]\n",
      " ...\n",
      " [-0.08707704]\n",
      " [-0.00579602]\n",
      " [-0.21075751]]\n",
      "Current iteration=4, loss=38644.99015513612\n",
      "t [[ 0.00336828]\n",
      " [-0.10519013]\n",
      " [-0.25483243]\n",
      " ...\n",
      " [-0.10103526]\n",
      " [-0.00486415]\n",
      " [-0.25813255]]\n",
      "t [[ 0.00336828]\n",
      " [-0.10519013]\n",
      " [-0.25483243]\n",
      " ...\n",
      " [-0.10103526]\n",
      " [-0.00486415]\n",
      " [-0.25813255]]\n",
      "t [[ 0.0022473 ]\n",
      " [-0.12878395]\n",
      " [-0.29203964]\n",
      " ...\n",
      " [-0.11278603]\n",
      " [-0.00334006]\n",
      " [-0.30369341]]\n",
      "t [[ 0.0022473 ]\n",
      " [-0.12878395]\n",
      " [-0.29203964]\n",
      " ...\n",
      " [-0.11278603]\n",
      " [-0.00334006]\n",
      " [-0.30369341]]\n",
      "Current iteration=6, loss=38074.70619013268\n",
      "t [[ 0.00076373]\n",
      " [-0.15276586]\n",
      " [-0.32603924]\n",
      " ...\n",
      " [-0.12267193]\n",
      " [-0.00135454]\n",
      " [-0.34756936]]\n",
      "t [[ 0.00076373]\n",
      " [-0.15276586]\n",
      " [-0.32603924]\n",
      " ...\n",
      " [-0.12267193]\n",
      " [-0.00135454]\n",
      " [-0.34756936]]\n",
      "t [[-0.00101203]\n",
      " [-0.17699921]\n",
      " [-0.35725619]\n",
      " ...\n",
      " [-0.13098302]\n",
      " [ 0.00098359]\n",
      " [-0.38987657]]\n",
      "t [[-0.00101203]\n",
      " [-0.17699921]\n",
      " [-0.35725619]\n",
      " ...\n",
      " [-0.13098302]\n",
      " [ 0.00098359]\n",
      " [-0.38987657]]\n",
      "Current iteration=8, loss=37605.395371701365\n",
      "t [[-0.00302098]\n",
      " [-0.20137043]\n",
      " [-0.38605274]\n",
      " ...\n",
      " [-0.13796481]\n",
      " [ 0.00358398]\n",
      " [-0.43071957]]\n",
      "t [[-0.00302098]\n",
      " [-0.20137043]\n",
      " [-0.38605274]\n",
      " ...\n",
      " [-0.13796481]\n",
      " [ 0.00358398]\n",
      " [-0.43071957]]\n",
      "t [[-0.00521372]\n",
      " [-0.22578534]\n",
      " [-0.41273804]\n",
      " ...\n",
      " [-0.14382514]\n",
      " [ 0.0063718 ]\n",
      " [-0.47019256]]\n",
      "loss=37208.3653508298\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.02721213]\n",
      " [-0.11624708]\n",
      " [-0.07306548]\n",
      " ...\n",
      " [-0.05295436]\n",
      " [ 0.01146491]\n",
      " [-0.03086961]]\n",
      "t [[ 0.02721213]\n",
      " [-0.11624708]\n",
      " [-0.07306548]\n",
      " ...\n",
      " [-0.05295436]\n",
      " [ 0.01146491]\n",
      " [-0.03086961]]\n",
      "t [[ 0.05089649]\n",
      " [-0.22038104]\n",
      " [-0.13816824]\n",
      " ...\n",
      " [-0.09924934]\n",
      " [ 0.0205295 ]\n",
      " [-0.05991259]]\n",
      "t [[ 0.05089649]\n",
      " [-0.22038104]\n",
      " [-0.13816824]\n",
      " ...\n",
      " [-0.09924934]\n",
      " [ 0.0205295 ]\n",
      " [-0.05991259]]\n",
      "Current iteration=2, loss=39289.772999139335\n",
      "t [[ 0.07153528]\n",
      " [-0.31405747]\n",
      " [-0.19645901]\n",
      " ...\n",
      " [-0.13989669]\n",
      " [ 0.02751773]\n",
      " [-0.08730617]]\n",
      "t [[ 0.07153528]\n",
      " [-0.31405747]\n",
      " [-0.19645901]\n",
      " ...\n",
      " [-0.13989669]\n",
      " [ 0.02751773]\n",
      " [-0.08730617]]\n",
      "t [[ 0.08954968]\n",
      " [-0.39871405]\n",
      " [-0.24893157]\n",
      " ...\n",
      " [-0.17576726]\n",
      " [ 0.0327139 ]\n",
      " [-0.11320836]]\n",
      "t [[ 0.08954968]\n",
      " [-0.39871405]\n",
      " [-0.24893157]\n",
      " ...\n",
      " [-0.17576726]\n",
      " [ 0.0327139 ]\n",
      " [-0.11320836]]\n",
      "Current iteration=4, loss=38534.509897244556\n",
      "t [[ 0.10530207]\n",
      " [-0.47557901]\n",
      " [-0.29642999]\n",
      " ...\n",
      " [-0.20759876]\n",
      " [ 0.03636263]\n",
      " [-0.13775809]]\n",
      "t [[ 0.10530207]\n",
      " [-0.47557901]\n",
      " [-0.29642999]\n",
      " ...\n",
      " [-0.20759876]\n",
      " [ 0.03636263]\n",
      " [-0.13775809]]\n",
      "t [[ 0.11910189]\n",
      " [-0.54569335]\n",
      " [-0.33966521]\n",
      " ...\n",
      " [-0.2360111 ]\n",
      " [ 0.03867241]\n",
      " [-0.16107668]]\n",
      "t [[ 0.11910189]\n",
      " [-0.54569335]\n",
      " [-0.33966521]\n",
      " ...\n",
      " [-0.2360111 ]\n",
      " [ 0.03867241]\n",
      " [-0.16107668]]\n",
      "Current iteration=6, loss=37945.349925554816\n",
      "t [[ 0.13121258]\n",
      " [-0.60993679]\n",
      " [-0.3792341 ]\n",
      " ...\n",
      " [-0.26152335]\n",
      " [ 0.03982036]\n",
      " [-0.18326998]]\n",
      "t [[ 0.13121258]\n",
      " [-0.60993679]\n",
      " [-0.3792341 ]\n",
      " ...\n",
      " [-0.26152335]\n",
      " [ 0.03982036]\n",
      " [-0.18326998]]\n",
      "t [[ 0.14185835]\n",
      " [-0.66905299]\n",
      " [-0.41563767]\n",
      " ...\n",
      " [-0.28456982]\n",
      " [ 0.03995707]\n",
      " [-0.2044305 ]]\n",
      "t [[ 0.14185835]\n",
      " [-0.66905299]\n",
      " [-0.41563767]\n",
      " ...\n",
      " [-0.28456982]\n",
      " [ 0.03995707]\n",
      " [-0.2044305 ]]\n",
      "Current iteration=8, loss=37465.998550622\n",
      "t [[ 0.15123033]\n",
      " [-0.72367188]\n",
      " [-0.44929705]\n",
      " ...\n",
      " [-0.30551426]\n",
      " [ 0.03921095]\n",
      " [-0.22463934]]\n",
      "t [[ 0.15123033]\n",
      " [-0.72367188]\n",
      " [-0.44929705]\n",
      " ...\n",
      " [-0.30551426]\n",
      " [ 0.03921095]\n",
      " [-0.22463934]]\n",
      "t [[ 0.15949177]\n",
      " [-0.77432862]\n",
      " [-0.48056713]\n",
      " ...\n",
      " [-0.32466181]\n",
      " [ 0.03769196]\n",
      " [-0.24396796]]\n",
      "loss=37063.92844471734\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00358856]\n",
      " [-0.02209429]\n",
      " [-0.06926792]\n",
      " ...\n",
      " [-0.0531547 ]\n",
      " [ 0.01033364]\n",
      " [-0.03153373]]\n",
      "t [[ 0.00358856]\n",
      " [-0.02209429]\n",
      " [-0.06926792]\n",
      " ...\n",
      " [-0.0531547 ]\n",
      " [ 0.01033364]\n",
      " [-0.03153373]]\n",
      "t [[ 0.00595689]\n",
      " [-0.04605628]\n",
      " [-0.13036357]\n",
      " ...\n",
      " [-0.09950625]\n",
      " [ 0.01832325]\n",
      " [-0.06119937]]\n",
      "t [[ 0.00595689]\n",
      " [-0.04605628]\n",
      " [-0.13036357]\n",
      " ...\n",
      " [-0.09950625]\n",
      " [ 0.01832325]\n",
      " [-0.06119937]]\n",
      "Current iteration=2, loss=39277.9079713463\n",
      "t [[ 0.00732183]\n",
      " [-0.07145402]\n",
      " [-0.18452069]\n",
      " ...\n",
      " [-0.14009945]\n",
      " [ 0.02429409]\n",
      " [-0.08917972]]\n",
      "t [[ 0.00732183]\n",
      " [-0.07145402]\n",
      " [-0.18452069]\n",
      " ...\n",
      " [-0.14009945]\n",
      " [ 0.02429409]\n",
      " [-0.08917972]]\n",
      "t [[ 0.00786896]\n",
      " [-0.09792281]\n",
      " [-0.23279957]\n",
      " ...\n",
      " [-0.17583169]\n",
      " [ 0.02853055]\n",
      " [-0.11563754]]\n",
      "t [[ 0.00786896]\n",
      " [-0.09792281]\n",
      " [-0.23279957]\n",
      " ...\n",
      " [-0.17583169]\n",
      " [ 0.02853055]\n",
      " [-0.11563754]]\n",
      "Current iteration=4, loss=38513.686236373396\n",
      "t [[ 0.00775384]\n",
      " [-0.12516032]\n",
      " [-0.27609666]\n",
      " ...\n",
      " [-0.20746153]\n",
      " [ 0.03127639]\n",
      " [-0.14071559]]\n",
      "t [[ 0.00775384]\n",
      " [-0.12516032]\n",
      " [-0.27609666]\n",
      " ...\n",
      " [-0.20746153]\n",
      " [ 0.03127639]\n",
      " [-0.14071559]]\n",
      "t [[ 0.00710535]\n",
      " [-0.15291863]\n",
      " [-0.3151636 ]\n",
      " ...\n",
      " [-0.23562498]\n",
      " [ 0.0327385 ]\n",
      " [-0.16453839]]\n",
      "t [[ 0.00710535]\n",
      " [-0.15291863]\n",
      " [-0.3151636 ]\n",
      " ...\n",
      " [-0.23562498]\n",
      " [ 0.0327385 ]\n",
      " [-0.16453839]]\n",
      "Current iteration=6, loss=37916.79802846552\n",
      "t [[ 0.00602971]\n",
      " [-0.18099574]\n",
      " [-0.35062854]\n",
      " ...\n",
      " [-0.2608535 ]\n",
      " [ 0.03309194]\n",
      " [-0.18721438]]\n",
      "t [[ 0.00602971]\n",
      " [-0.18099574]\n",
      " [-0.35062854]\n",
      " ...\n",
      " [-0.2608535 ]\n",
      " [ 0.03309194]\n",
      " [-0.18721438]]\n",
      "t [[ 0.00461428]\n",
      " [-0.20922774]\n",
      " [-0.38301621]\n",
      " ...\n",
      " [-0.28359094]\n",
      " [ 0.03248498]\n",
      " [-0.2088382 ]]\n",
      "t [[ 0.00461428]\n",
      " [-0.20922774]\n",
      " [-0.38301621]\n",
      " ...\n",
      " [-0.28359094]\n",
      " [ 0.03248498]\n",
      " [-0.2088382 ]]\n",
      "Current iteration=8, loss=37430.4183023956\n",
      "t [[ 0.00293089]\n",
      " [-0.23748199]\n",
      " [-0.41276557]\n",
      " ...\n",
      " [-0.30420835]\n",
      " [ 0.03104356]\n",
      " [-0.22949276]]\n",
      "t [[ 0.00293089]\n",
      " [-0.23748199]\n",
      " [-0.41276557]\n",
      " ...\n",
      " [-0.30420835]\n",
      " [ 0.03104356]\n",
      " [-0.22949276]]\n",
      "t [[ 0.00103863]\n",
      " [-0.26565154]\n",
      " [-0.44024454]\n",
      " ...\n",
      " [-0.32301646]\n",
      " [ 0.02887513]\n",
      " [-0.249251  ]]\n",
      "loss=37021.881598420325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00306503]\n",
      " [-0.01939128]\n",
      " [-0.07001812]\n",
      " ...\n",
      " [-0.05226578]\n",
      " [ 0.01111068]\n",
      " [-0.02957877]]\n",
      "t [[ 0.00306503]\n",
      " [-0.01939128]\n",
      " [-0.07001812]\n",
      " ...\n",
      " [-0.05226578]\n",
      " [ 0.01111068]\n",
      " [-0.02957877]]\n",
      "t [[ 0.00492182]\n",
      " [-0.04073682]\n",
      " [-0.1318808 ]\n",
      " ...\n",
      " [-0.09782109]\n",
      " [ 0.0198544 ]\n",
      " [-0.05735229]]\n",
      "t [[ 0.00492182]\n",
      " [-0.04073682]\n",
      " [-0.1318808 ]\n",
      " ...\n",
      " [-0.09782109]\n",
      " [ 0.0198544 ]\n",
      " [-0.05735229]]\n",
      "Current iteration=2, loss=39288.97387865269\n",
      "t [[ 0.00578845]\n",
      " [-0.06360338]\n",
      " [-0.18680985]\n",
      " ...\n",
      " [-0.1376943 ]\n",
      " [ 0.02655527]\n",
      " [-0.08349964]]\n",
      "t [[ 0.00578845]\n",
      " [-0.06360338]\n",
      " [-0.18680985]\n",
      " ...\n",
      " [-0.1376943 ]\n",
      " [ 0.02655527]\n",
      " [-0.08349964]]\n",
      "t [[ 0.00585144]\n",
      " [-0.08762437]\n",
      " [-0.23585634]\n",
      " ...\n",
      " [-0.17276946]\n",
      " [ 0.0314968 ]\n",
      " [-0.10818023]]\n",
      "t [[ 0.00585144]\n",
      " [-0.08762437]\n",
      " [-0.23585634]\n",
      " ...\n",
      " [-0.17276946]\n",
      " [ 0.0314968 ]\n",
      " [-0.10818023]]\n",
      "Current iteration=4, loss=38533.86985770139\n",
      "t [[ 0.00526708]\n",
      " [-0.11249504]\n",
      " [-0.27991002]\n",
      " ...\n",
      " [-0.20379457]\n",
      " [ 0.03492223]\n",
      " [-0.13153383]]\n",
      "t [[ 0.00526708]\n",
      " [-0.11249504]\n",
      " [-0.27991002]\n",
      " ...\n",
      " [-0.20379457]\n",
      " [ 0.03492223]\n",
      " [-0.13153383]]\n",
      "t [[ 0.00416482]\n",
      " [-0.13796464]\n",
      " [-0.31971789]\n",
      " ...\n",
      " [-0.23139739]\n",
      " [ 0.0370383 ]\n",
      " [-0.1536822 ]]\n",
      "t [[ 0.00416482]\n",
      " [-0.13796464]\n",
      " [-0.31971789]\n",
      " ...\n",
      " [-0.23139739]\n",
      " [ 0.0370383 ]\n",
      " [-0.1536822 ]]\n",
      "Current iteration=6, loss=37945.01348956119\n",
      "t [[ 0.0026513 ]\n",
      " [-0.16382811]\n",
      " [-0.35590503]\n",
      " ...\n",
      " [-0.25610291]\n",
      " [ 0.0380202 ]\n",
      " [-0.17473127]]\n",
      "t [[ 0.0026513 ]\n",
      " [-0.16382811]\n",
      " [-0.35590503]\n",
      " ...\n",
      " [-0.25610291]\n",
      " [ 0.0380202 ]\n",
      " [-0.17473127]]\n",
      "t [[ 0.00081415]\n",
      " [-0.18991834]\n",
      " [-0.38899428]\n",
      " ...\n",
      " [-0.27834989]\n",
      " [ 0.03801651]\n",
      " [-0.1947734 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.00081415]\n",
      " [-0.18991834]\n",
      " [-0.38899428]\n",
      " ...\n",
      " [-0.27834989]\n",
      " [ 0.03801651]\n",
      " [-0.1947734 ]]\n",
      "Current iteration=8, loss=37465.89905137279\n",
      "t [[-0.00127462]\n",
      " [-0.21609957]\n",
      " [-0.41942352]\n",
      " ...\n",
      " [-0.29850535]\n",
      " [ 0.03715365]\n",
      " [-0.21388939]]\n",
      "t [[-0.00127462]\n",
      " [-0.21609957]\n",
      " [-0.41942352]\n",
      " ...\n",
      " [-0.29850535]\n",
      " [ 0.03715365]\n",
      " [-0.21388939]]\n",
      "t [[-0.00355585]\n",
      " [-0.24226174]\n",
      " [-0.4475602 ]\n",
      " ...\n",
      " [-0.31687684]\n",
      " [ 0.03553962]\n",
      " [-0.23215025]]\n",
      "loss=37063.98396328325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00255255]\n",
      " [-0.02053871]\n",
      " [-0.06955611]\n",
      " ...\n",
      " [-0.03062377]\n",
      " [-0.00368825]\n",
      " [-0.06247745]]\n",
      "t [[ 0.00255255]\n",
      " [-0.02053871]\n",
      " [-0.06955611]\n",
      " ...\n",
      " [-0.03062377]\n",
      " [-0.00368825]\n",
      " [-0.06247745]]\n",
      "t [[ 0.00392073]\n",
      " [-0.04301827]\n",
      " [-0.1308889 ]\n",
      " ...\n",
      " [-0.05597724]\n",
      " [-0.00563379]\n",
      " [-0.12188679]]\n",
      "t [[ 0.00392073]\n",
      " [-0.04301827]\n",
      " [-0.1308889 ]\n",
      " ...\n",
      " [-0.05597724]\n",
      " [-0.00563379]\n",
      " [-0.12188679]]\n",
      "Current iteration=2, loss=39271.020875209695\n",
      "t [[ 0.00432165]\n",
      " [-0.06700261]\n",
      " [-0.18523634]\n",
      " ...\n",
      " [-0.07693039]\n",
      " [-0.00618888]\n",
      " [-0.17849217]]\n",
      "t [[ 0.00432165]\n",
      " [-0.06700261]\n",
      " [-0.18523634]\n",
      " ...\n",
      " [-0.07693039]\n",
      " [-0.00618888]\n",
      " [-0.17849217]]\n",
      "t [[ 0.00394073]\n",
      " [-0.09212292]\n",
      " [-0.2336631 ]\n",
      " ...\n",
      " [-0.09422669]\n",
      " [-0.00564962]\n",
      " [-0.23253262]]\n",
      "t [[ 0.00394073]\n",
      " [-0.09212292]\n",
      " [-0.2336631 ]\n",
      " ...\n",
      " [-0.09422669]\n",
      " [-0.00564962]\n",
      " [-0.23253262]]\n",
      "Current iteration=4, loss=38503.132966089135\n",
      "t [[ 0.00293313]\n",
      " [-0.11807286]\n",
      " [-0.27706998]\n",
      " ...\n",
      " [-0.1084913 ]\n",
      " [-0.00426107]\n",
      " [-0.28422176]]\n",
      "t [[ 0.00293313]\n",
      " [-0.11807286]\n",
      " [-0.27706998]\n",
      " ...\n",
      " [-0.1084913 ]\n",
      " [-0.00426107]\n",
      " [-0.28422176]]\n",
      "t [[ 0.00142719]\n",
      " [-0.14460068]\n",
      " [-0.31621282]\n",
      " ...\n",
      " [-0.12024549]\n",
      " [-0.00222421]\n",
      " [-0.3337494 ]]\n",
      "t [[ 0.00142719]\n",
      " [-0.14460068]\n",
      " [-0.31621282]\n",
      " ...\n",
      " [-0.12024549]\n",
      " [-0.00222421]\n",
      " [-0.3337494 ]]\n",
      "Current iteration=6, loss=37904.38442080155\n",
      "t [[-4.71543613e-04]\n",
      " [-1.71500766e-01]\n",
      " [-3.51723702e-01]\n",
      " ...\n",
      " [-1.29922177e-01]\n",
      " [ 2.97057141e-04]\n",
      " [-3.81283876e-01]]\n",
      "t [[-4.71543613e-04]\n",
      " [-1.71500766e-01]\n",
      " [-3.51723702e-01]\n",
      " ...\n",
      " [-1.29922177e-01]\n",
      " [ 2.97057141e-04]\n",
      " [-3.81283876e-01]]\n",
      "t [[-0.00267646]\n",
      " [-0.19860583]\n",
      " [-0.38413101]\n",
      " ...\n",
      " [-0.13788045]\n",
      " [ 0.00316954]\n",
      " [-0.4269746 ]]\n",
      "t [[-0.00267646]\n",
      " [-0.19860583]\n",
      " [-0.38413101]\n",
      " ...\n",
      " [-0.13788045]\n",
      " [ 0.00316954]\n",
      " [-0.4269746 ]]\n",
      "Current iteration=8, loss=37417.12518618403\n",
      "t [[-0.00511657]\n",
      " [-0.2257801 ]\n",
      " [-0.41387693]\n",
      " ...\n",
      " [-0.14441828]\n",
      " [ 0.00628532]\n",
      " [-0.47095452]]\n",
      "t [[-0.00511657]\n",
      " [-0.2257801 ]\n",
      " [-0.41387693]\n",
      " ...\n",
      " [-0.14441828]\n",
      " [ 0.00628532]\n",
      " [-0.47095452]]\n",
      "t [[-0.00773366]\n",
      " [-0.25291371]\n",
      " [-0.44133236]\n",
      " ...\n",
      " [-0.14978311]\n",
      " [ 0.00955722]\n",
      " [-0.51334224]]\n",
      "loss=37008.224495566246\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.02993334]\n",
      " [-0.12787179]\n",
      " [-0.08037202]\n",
      " ...\n",
      " [-0.05824979]\n",
      " [ 0.0126114 ]\n",
      " [-0.03395657]]\n",
      "t [[ 0.02993334]\n",
      " [-0.12787179]\n",
      " [-0.08037202]\n",
      " ...\n",
      " [-0.05824979]\n",
      " [ 0.0126114 ]\n",
      " [-0.03395657]]\n",
      "t [[ 0.05559906]\n",
      " [-0.24109017]\n",
      " [-0.1511115 ]\n",
      " ...\n",
      " [-0.10844363]\n",
      " [ 0.02231926]\n",
      " [-0.06570332]]\n",
      "t [[ 0.05559906]\n",
      " [-0.24109017]\n",
      " [-0.1511115 ]\n",
      " ...\n",
      " [-0.10844363]\n",
      " [ 0.02231926]\n",
      " [-0.06570332]]\n",
      "Current iteration=2, loss=39200.77651541376\n",
      "t [[ 0.07764031]\n",
      " [-0.34186335]\n",
      " [-0.21375306]\n",
      " ...\n",
      " [-0.15193059]\n",
      " [ 0.02955583]\n",
      " [-0.09547673]]\n",
      "t [[ 0.07764031]\n",
      " [-0.34186335]\n",
      " [-0.21375306]\n",
      " ...\n",
      " [-0.15193059]\n",
      " [ 0.02955583]\n",
      " [-0.09547673]]\n",
      "t [[ 0.09660896]\n",
      " [-0.43207332]\n",
      " [-0.26959665]\n",
      " ...\n",
      " [-0.1898494 ]\n",
      " [ 0.03469365]\n",
      " [-0.12348439]]\n",
      "t [[ 0.09660896]\n",
      " [-0.43207332]\n",
      " [-0.26959665]\n",
      " ...\n",
      " [-0.1898494 ]\n",
      " [ 0.03469365]\n",
      " [-0.12348439]]\n",
      "Current iteration=4, loss=38401.30136467834\n",
      "t [[ 0.11297079]\n",
      " [-0.51329575]\n",
      " [-0.31972354]\n",
      " ...\n",
      " [-0.22314467]\n",
      " [ 0.03804683]\n",
      " [-0.14990575]]\n",
      "t [[ 0.11297079]\n",
      " [-0.51329575]\n",
      " [-0.31972354]\n",
      " ...\n",
      " [-0.22314467]\n",
      " [ 0.03804683]\n",
      " [-0.14990575]]\n",
      "t [[ 0.12711602]\n",
      " [-0.58683973]\n",
      " [-0.36502582]\n",
      " ...\n",
      " [-0.25259363]\n",
      " [ 0.03987795]\n",
      " [-0.17489507]]\n",
      "t [[ 0.12711602]\n",
      " [-0.58683973]\n",
      " [-0.36502582]\n",
      " ...\n",
      " [-0.25259363]\n",
      " [ 0.03987795]\n",
      " [-0.17489507]]\n",
      "Current iteration=6, loss=37787.55555674896\n",
      "t [[ 0.1393709 ]\n",
      " [-0.65379103]\n",
      " [-0.40623783]\n",
      " ...\n",
      " [-0.27883411]\n",
      " [ 0.04040608]\n",
      " [-0.19858498]]\n",
      "t [[ 0.1393709 ]\n",
      " [-0.65379103]\n",
      " [-0.40623783]\n",
      " ...\n",
      " [-0.27883411]\n",
      " [ 0.04040608]\n",
      " [-0.19858498]]\n",
      "t [[ 0.15000849]\n",
      " [-0.71505168]\n",
      " [-0.44396467]\n",
      " ...\n",
      " [-0.30238968]\n",
      " [ 0.03981451]\n",
      " [-0.2210899 ]]\n",
      "t [[ 0.15000849]\n",
      " [-0.71505168]\n",
      " [-0.44396467]\n",
      " ...\n",
      " [-0.30238968]\n",
      " [ 0.03981451]\n",
      " [-0.2210899 ]]\n",
      "Current iteration=8, loss=37293.219842229824\n",
      "t [[ 0.15925783]\n",
      " [-0.77137357]\n",
      " [-0.47870632]\n",
      " ...\n",
      " [-0.32369091]\n",
      " [ 0.03825728]\n",
      " [-0.24250903]]\n",
      "t [[ 0.15925783]\n",
      " [-0.77137357]\n",
      " [-0.47870632]\n",
      " ...\n",
      " [-0.32369091]\n",
      " [ 0.03825728]\n",
      " [-0.24250903]]\n",
      "t [[ 0.16731173]\n",
      " [-0.82338614]\n",
      " [-0.51087727]\n",
      " ...\n",
      " [-0.34309276]\n",
      " [ 0.03586463]\n",
      " [-0.26292891]]\n",
      "loss=36881.64691340677\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00394742]\n",
      " [-0.02430372]\n",
      " [-0.07619471]\n",
      " ...\n",
      " [-0.05847017]\n",
      " [ 0.011367  ]\n",
      " [-0.0346871 ]]\n",
      "t [[ 0.00394742]\n",
      " [-0.02430372]\n",
      " [-0.07619471]\n",
      " ...\n",
      " [-0.05847017]\n",
      " [ 0.011367  ]\n",
      " [-0.0346871 ]]\n",
      "t [[ 0.00641887]\n",
      " [-0.05086662]\n",
      " [-0.14250339]\n",
      " ...\n",
      " [-0.1087105 ]\n",
      " [ 0.01989858]\n",
      " [-0.06711423]]\n",
      "t [[ 0.00641887]\n",
      " [-0.05086662]\n",
      " [-0.14250339]\n",
      " ...\n",
      " [-0.1087105 ]\n",
      " [ 0.01989858]\n",
      " [-0.06711423]]\n",
      "Current iteration=2, loss=39187.88960075628\n",
      "t [[ 0.00770355]\n",
      " [-0.07911301]\n",
      " [-0.20057117]\n",
      " ...\n",
      " [-0.15211411]\n",
      " [ 0.02602872]\n",
      " [-0.09752532]]\n",
      "t [[ 0.00770355]\n",
      " [-0.07911301]\n",
      " [-0.20057117]\n",
      " ...\n",
      " [-0.15211411]\n",
      " [ 0.02602872]\n",
      " [-0.09752532]]\n",
      "t [[ 0.00804387]\n",
      " [-0.10856727]\n",
      " [-0.25178422]\n",
      " ...\n",
      " [-0.18985412]\n",
      " [ 0.03012987]\n",
      " [-0.12613411]]\n",
      "t [[ 0.00804387]\n",
      " [-0.10856727]\n",
      " [-0.25178422]\n",
      " ...\n",
      " [-0.18985412]\n",
      " [ 0.03012987]\n",
      " [-0.12613411]]\n",
      "Current iteration=4, loss=38378.82930888973\n",
      "t [[ 0.0076383 ]\n",
      " [-0.13884451]\n",
      " [-0.29728963]\n",
      " ...\n",
      " [-0.22290124]\n",
      " [ 0.03251473]\n",
      " [-0.15312498]]\n",
      "t [[ 0.0076383 ]\n",
      " [-0.13884451]\n",
      " [-0.29728963]\n",
      " ...\n",
      " [-0.22290124]\n",
      " [ 0.03251473]\n",
      " [-0.15312498]]\n",
      "t [[ 0.0066475 ]\n",
      " [-0.16963675]\n",
      " [-0.33802904]\n",
      " ...\n",
      " [-0.2520523 ]\n",
      " [ 0.03344346]\n",
      " [-0.1786561 ]]\n",
      "t [[ 0.0066475 ]\n",
      " [-0.16963675]\n",
      " [-0.33802904]\n",
      " ...\n",
      " [-0.2520523 ]\n",
      " [ 0.03344346]\n",
      " [-0.1786561 ]]\n",
      "Current iteration=6, loss=37756.803775735025\n",
      "t [[ 0.00520095]\n",
      " [-0.20069912]\n",
      " [-0.37477361]\n",
      " ...\n",
      " [-0.27795971]\n",
      " [ 0.03313221]\n",
      " [-0.20286328]]\n",
      "t [[ 0.00520095]\n",
      " [-0.20069912]\n",
      " [-0.37477361]\n",
      " ...\n",
      " [-0.27795971]\n",
      " [ 0.03313221]\n",
      " [-0.20286328]]\n",
      "t [[ 0.00340281]\n",
      " [-0.23183777]\n",
      " [-0.40815535]\n",
      " ...\n",
      " [-0.30115794]\n",
      " [ 0.03176101]\n",
      " [-0.22586348]]\n",
      "t [[ 0.00340281]\n",
      " [-0.23183777]\n",
      " [-0.40815535]\n",
      " ...\n",
      " [-0.30115794]\n",
      " [ 0.03176101]\n",
      " [-0.22586348]]\n",
      "Current iteration=8, loss=37254.95582472687\n",
      "t [[ 0.00133699]\n",
      " [-0.26289977]\n",
      " [-0.43869345]\n",
      " ...\n",
      " [-0.32208565]\n",
      " [ 0.02948055]\n",
      " [-0.24775804]]\n",
      "t [[ 0.00133699]\n",
      " [-0.26289977]\n",
      " [-0.43869345]\n",
      " ...\n",
      " [-0.32208565]\n",
      " [ 0.02948055]\n",
      " [-0.24775804]]\n",
      "t [[-0.00092891]\n",
      " [-0.29376496]\n",
      " [-0.46681576]\n",
      " ...\n",
      " [-0.34110385]\n",
      " [ 0.02641771]\n",
      " [-0.26863525]]\n",
      "loss=36836.52786542825\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00337153]\n",
      " [-0.0213304 ]\n",
      " [-0.07701993]\n",
      " ...\n",
      " [-0.05749236]\n",
      " [ 0.01222174]\n",
      " [-0.03253665]]\n",
      "t [[ 0.00337153]\n",
      " [-0.0213304 ]\n",
      " [-0.07701993]\n",
      " ...\n",
      " [-0.05749236]\n",
      " [ 0.01222174]\n",
      " [-0.03253665]]\n",
      "t [[ 0.0052816 ]\n",
      " [-0.04502477]\n",
      " [-0.14417414]\n",
      " ...\n",
      " [-0.10686697]\n",
      " [ 0.0215803 ]\n",
      " [-0.06288935]]\n",
      "t [[ 0.0052816 ]\n",
      " [-0.04502477]\n",
      " [-0.14417414]\n",
      " ...\n",
      " [-0.10686697]\n",
      " [ 0.0215803 ]\n",
      " [-0.06288935]]\n",
      "Current iteration=2, loss=39199.959819479125\n",
      "t [[ 0.00602103]\n",
      " [-0.07050563]\n",
      " [-0.20309185]\n",
      " ...\n",
      " [-0.14949504]\n",
      " [ 0.02850809]\n",
      " [-0.09129708]]\n",
      "t [[ 0.00602103]\n",
      " [-0.07050563]\n",
      " [-0.20309185]\n",
      " ...\n",
      " [-0.14949504]\n",
      " [ 0.02850809]\n",
      " [-0.09129708]]\n",
      "t [[ 0.00583345]\n",
      " [-0.09729478]\n",
      " [-0.25514739]\n",
      " ...\n",
      " [-0.18653243]\n",
      " [ 0.03337646]\n",
      " [-0.11796917]]\n",
      "t [[ 0.00583345]\n",
      " [-0.09729478]\n",
      " [-0.25514739]\n",
      " ...\n",
      " [-0.18653243]\n",
      " [ 0.03337646]\n",
      " [-0.11796917]]\n",
      "Current iteration=4, loss=38400.72015103053\n",
      "t [[ 0.00491825]\n",
      " [-0.12500391]\n",
      " [-0.30147968]\n",
      " ...\n",
      " [-0.21893659]\n",
      " [ 0.03649755]\n",
      " [-0.14308607]]\n",
      "t [[ 0.00491825]\n",
      " [-0.12500391]\n",
      " [-0.30147968]\n",
      " ...\n",
      " [-0.21893659]\n",
      " [ 0.03649755]\n",
      " [-0.14308607]]\n",
      "t [[ 0.00343679]\n",
      " [-0.15332107]\n",
      " [-0.343025  ]\n",
      " ...\n",
      " [-0.24749423]\n",
      " [ 0.03813149]\n",
      " [-0.1668024 ]]\n",
      "t [[ 0.00343679]\n",
      " [-0.15332107]\n",
      " [-0.343025  ]\n",
      " ...\n",
      " [-0.24749423]\n",
      " [ 0.03813149]\n",
      " [-0.1668024 ]]\n",
      "Current iteration=6, loss=37787.30324568376\n",
      "t [[ 0.00151902]\n",
      " [-0.18199723]\n",
      " [-0.38055122]\n",
      " ...\n",
      " [-0.27285008]\n",
      " [ 0.03849472]\n",
      " [-0.18925076]]\n",
      "t [[ 0.00151902]\n",
      " [-0.18199723]\n",
      " [-0.38055122]\n",
      " ...\n",
      " [-0.27285008]\n",
      " [ 0.03849472]\n",
      " [-0.18925076]]\n",
      "t [[-0.00073059]\n",
      " [-0.21083431]\n",
      " [-0.41468853]\n",
      " ...\n",
      " [-0.29553271]\n",
      " [ 0.03776783]\n",
      " [-0.21054522]]\n",
      "t [[-0.00073059]\n",
      " [-0.21083431]\n",
      " [-0.41468853]\n",
      " ...\n",
      " [-0.29553271]\n",
      " [ 0.03776783]\n",
      " [-0.21054522]]\n",
      "Current iteration=8, loss=37293.1946889756\n",
      "t [[-0.00322801]\n",
      " [-0.23967518]\n",
      " [-0.44595528]\n",
      " ...\n",
      " [-0.31597619]\n",
      " [ 0.03610223]\n",
      " [-0.23078444]]\n",
      "t [[-0.00322801]\n",
      " [-0.23967518]\n",
      " [-0.44595528]\n",
      " ...\n",
      " [-0.31597619]\n",
      " [ 0.03610223]\n",
      " [-0.23078444]]\n",
      "t [[-0.00590561]\n",
      " [-0.26839567]\n",
      " [-0.47477918]\n",
      " ...\n",
      " [-0.33453797]\n",
      " [ 0.03362558]\n",
      " [-0.25005433]]\n",
      "loss=36881.76116172473\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00280781]\n",
      " [-0.02259258]\n",
      " [-0.07651173]\n",
      " ...\n",
      " [-0.03368615]\n",
      " [-0.00405707]\n",
      " [-0.06872519]]\n",
      "t [[ 0.00280781]\n",
      " [-0.02259258]\n",
      " [-0.07651173]\n",
      " ...\n",
      " [-0.03368615]\n",
      " [-0.00405707]\n",
      " [-0.06872519]]\n",
      "t [[ 0.00418302]\n",
      " [-0.04753284]\n",
      " [-0.14307564]\n",
      " ...\n",
      " [-0.06099679]\n",
      " [-0.00600595]\n",
      " [-0.13373858]]\n",
      "t [[ 0.00418302]\n",
      " [-0.04753284]\n",
      " [-0.14307564]\n",
      " ...\n",
      " [-0.06099679]\n",
      " [-0.00600595]\n",
      " [-0.13373858]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2, loss=39180.47680427156\n",
      "t [[ 0.00441516]\n",
      " [-0.07423959]\n",
      " [-0.20134255]\n",
      " ...\n",
      " [-0.08309174]\n",
      " [-0.00631596]\n",
      " [-0.19539275]]\n",
      "t [[ 0.00441516]\n",
      " [-0.07423959]\n",
      " [-0.20134255]\n",
      " ...\n",
      " [-0.08309174]\n",
      " [-0.00631596]\n",
      " [-0.19539275]]\n",
      "t [[ 0.00374639]\n",
      " [-0.10223173]\n",
      " [-0.25270446]\n",
      " ...\n",
      " [-0.10094227]\n",
      " [-0.00537314]\n",
      " [-0.25400246]]\n",
      "t [[ 0.00374639]\n",
      " [-0.10223173]\n",
      " [-0.25270446]\n",
      " ...\n",
      " [-0.10094227]\n",
      " [-0.00537314]\n",
      " [-0.25400246]]\n",
      "Current iteration=4, loss=38367.74636595817\n",
      "t [[ 0.00237459]\n",
      " [-0.13111905]\n",
      " [-0.29831428]\n",
      " ...\n",
      " [-0.11534739]\n",
      " [-0.00348957]\n",
      " [-0.30984487]]\n",
      "t [[ 0.00237459]\n",
      " [-0.13111905]\n",
      " [-0.29831428]\n",
      " ...\n",
      " [-0.11534739]\n",
      " [-0.00348957]\n",
      " [-0.30984487]]\n",
      "t [[ 0.00045967]\n",
      " [-0.16058856]\n",
      " [-0.33911915]\n",
      " ...\n",
      " [-0.12695881]\n",
      " [-0.00091504]\n",
      " [-0.36316277]]\n",
      "t [[ 0.00045967]\n",
      " [-0.16058856]\n",
      " [-0.33911915]\n",
      " ...\n",
      " [-0.12695881]\n",
      " [-0.00091504]\n",
      " [-0.36316277]]\n",
      "Current iteration=6, loss=37744.01379525784\n",
      "t [[-0.00186985]\n",
      " [-0.19039072]\n",
      " [-0.3758953 ]\n",
      " ...\n",
      " [-0.13630641]\n",
      " [ 0.00215159]\n",
      " [-0.41416866]]\n",
      "t [[-0.00186985]\n",
      " [-0.19039072]\n",
      " [-0.3758953 ]\n",
      " ...\n",
      " [-0.13630641]\n",
      " [ 0.00215159]\n",
      " [-0.41416866]]\n",
      "t [[-0.00451087]\n",
      " [-0.22032736]\n",
      " [-0.40927933]\n",
      " ...\n",
      " [-0.14382081]\n",
      " [ 0.00555264]\n",
      " [-0.46304895]]\n",
      "t [[-0.00451087]\n",
      " [-0.22032736]\n",
      " [-0.40927933]\n",
      " ...\n",
      " [-0.14382081]\n",
      " [ 0.00555264]\n",
      " [-0.46304895]]\n",
      "Current iteration=8, loss=37241.44917978262\n",
      "t [[-0.00738064]\n",
      " [-0.25024155]\n",
      " [-0.43979449]\n",
      " ...\n",
      " [-0.14985232]\n",
      " [ 0.00916352]\n",
      " [-0.50996775]]\n",
      "t [[-0.00738064]\n",
      " [-0.25024155]\n",
      " [-0.43979449]\n",
      " ...\n",
      " [-0.14985232]\n",
      " [ 0.00916352]\n",
      " [-0.50996775]]\n",
      "t [[-0.01041278]\n",
      " [-0.28000947]\n",
      " [-0.46787222]\n",
      " ...\n",
      " [-0.15468634]\n",
      " [ 0.01288617]\n",
      " [-0.55507011]]\n",
      "loss=36822.782594136435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.03265455]\n",
      " [-0.1394965 ]\n",
      " [-0.08767857]\n",
      " ...\n",
      " [-0.06354523]\n",
      " [ 0.01375789]\n",
      " [-0.03704353]]\n",
      "t [[ 0.03265455]\n",
      " [-0.1394965 ]\n",
      " [-0.08767857]\n",
      " ...\n",
      " [-0.06354523]\n",
      " [ 0.01375789]\n",
      " [-0.03704353]]\n",
      "t [[ 0.06023156]\n",
      " [-0.26155875]\n",
      " [-0.16389666]\n",
      " ...\n",
      " [-0.11750568]\n",
      " [ 0.02406141]\n",
      " [-0.07145772]]\n",
      "t [[ 0.06023156]\n",
      " [-0.26155875]\n",
      " [-0.16389666]\n",
      " ...\n",
      " [-0.11750568]\n",
      " [ 0.02406141]\n",
      " [-0.07145772]]\n",
      "Current iteration=2, loss=39113.92890305495\n",
      "t [[ 0.08356756]\n",
      " [-0.36905885]\n",
      " [-0.23065003]\n",
      " ...\n",
      " [-0.16363553]\n",
      " [ 0.03147301]\n",
      " [-0.10355026]]\n",
      "t [[ 0.08356756]\n",
      " [-0.36905885]\n",
      " [-0.23065003]\n",
      " ...\n",
      " [-0.16363553]\n",
      " [ 0.03147301]\n",
      " [-0.10355026]]\n",
      "t [[ 0.10336728]\n",
      " [-0.46439771]\n",
      " [-0.28959582]\n",
      " ...\n",
      " [-0.20338597]\n",
      " [ 0.03646827]\n",
      " [-0.13358694]]\n",
      "t [[ 0.10336728]\n",
      " [-0.46439771]\n",
      " [-0.28959582]\n",
      " ...\n",
      " [-0.20338597]\n",
      " [ 0.03646827]\n",
      " [-0.13358694]]\n",
      "Current iteration=4, loss=38273.99078296925\n",
      "t [[ 0.12021393]\n",
      " [-0.54954429]\n",
      " [-0.34208323]\n",
      " ...\n",
      " [-0.23793511]\n",
      " [ 0.03944012]\n",
      " [-0.16179365]]\n",
      "t [[ 0.12021393]\n",
      " [-0.54954429]\n",
      " [-0.34208323]\n",
      " ...\n",
      " [-0.23793511]\n",
      " [ 0.03944012]\n",
      " [-0.16179365]]\n",
      "t [[ 0.13458688]\n",
      " [-0.62610223]\n",
      " [-0.38920253]\n",
      " ...\n",
      " [-0.26823209]\n",
      " [ 0.04071066]\n",
      " [-0.1883615 ]]\n",
      "t [[ 0.13458688]\n",
      " [-0.62610223]\n",
      " [-0.38920253]\n",
      " ...\n",
      " [-0.26823209]\n",
      " [ 0.04071066]\n",
      " [-0.1883615 ]]\n",
      "Current iteration=6, loss=37638.594037340525\n",
      "t [[ 0.14687983]\n",
      " [-0.69537723]\n",
      " [-0.43183388]\n",
      " ...\n",
      " [-0.29504031]\n",
      " [ 0.04054401]\n",
      " [-0.21345249]]\n",
      "t [[ 0.14687983]\n",
      " [-0.69537723]\n",
      " [-0.43183388]\n",
      " ...\n",
      " [-0.29504031]\n",
      " [ 0.04054401]\n",
      " [-0.21345249]]\n",
      "t [[ 0.15741684]\n",
      " [-0.75843582]\n",
      " [-0.47068952]\n",
      " ...\n",
      " [-0.31897468]\n",
      " [ 0.03915773]\n",
      " [-0.23720471]]\n",
      "t [[ 0.15741684]\n",
      " [-0.75843582]\n",
      " [-0.47068952]\n",
      " ...\n",
      " [-0.31897468]\n",
      " [ 0.03915773]\n",
      " [-0.23720471]]\n",
      "Current iteration=8, loss=37131.45109230439\n",
      "t [[ 0.16646558]\n",
      " [-0.81615334]\n",
      " [-0.50634808]\n",
      " ...\n",
      " [-0.34053188]\n",
      " [ 0.03673219]\n",
      " [-0.25973664]]\n",
      "t [[ 0.16646558]\n",
      " [-0.81615334]\n",
      " [-0.50634808]\n",
      " ...\n",
      " [-0.34053188]\n",
      " [ 0.03673219]\n",
      " [-0.25973664]]\n",
      "t [[ 0.17424814]\n",
      " [-0.86925224]\n",
      " [-0.53928177]\n",
      " ...\n",
      " [-0.36011437]\n",
      " [ 0.03341809]\n",
      " [-0.28115071]]\n",
      "loss=36712.11039347524\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00430627]\n",
      " [-0.02651315]\n",
      " [-0.0831215 ]\n",
      " ...\n",
      " [-0.06378564]\n",
      " [ 0.01240036]\n",
      " [-0.03784047]]\n",
      "t [[ 0.00430627]\n",
      " [-0.02651315]\n",
      " [-0.0831215 ]\n",
      " ...\n",
      " [-0.06378564]\n",
      " [ 0.01240036]\n",
      " [-0.03784047]]\n",
      "t [[ 0.00685669]\n",
      " [-0.05571395]\n",
      " [-0.15448096]\n",
      " ...\n",
      " [-0.11777966]\n",
      " [ 0.02142744]\n",
      " [-0.07299194]]\n",
      "t [[ 0.00685669]\n",
      " [-0.05571395]\n",
      " [-0.15448096]\n",
      " ...\n",
      " [-0.11777966]\n",
      " [ 0.02142744]\n",
      " [-0.07299194]]\n",
      "Current iteration=2, loss=39100.044104770306\n",
      "t [[ 0.00802737]\n",
      " [-0.08685417]\n",
      " [-0.21621761]\n",
      " ...\n",
      " [-0.16379349]\n",
      " [ 0.0276459 ]\n",
      " [-0.10577181]]\n",
      "t [[ 0.00802737]\n",
      " [-0.08685417]\n",
      " [-0.21621761]\n",
      " ...\n",
      " [-0.16379349]\n",
      " [ 0.0276459 ]\n",
      " [-0.10577181]]\n",
      "t [[ 0.00812679]\n",
      " [-0.11932943]\n",
      " [-0.27009741]\n",
      " ...\n",
      " [-0.20332168]\n",
      " [ 0.03153096]\n",
      " [-0.13645366]]\n",
      "t [[ 0.00812679]\n",
      " [-0.11932943]\n",
      " [-0.27009741]\n",
      " ...\n",
      " [-0.20332168]\n",
      " [ 0.03153096]\n",
      " [-0.13645366]]\n",
      "Current iteration=4, loss=38249.91262844893\n",
      "t [[ 0.00740156]\n",
      " [-0.15266248]\n",
      " [-0.31755015]\n",
      " ...\n",
      " [-0.23757421]\n",
      " [ 0.03347335]\n",
      " [-0.16526946]]\n",
      "t [[ 0.00740156]\n",
      " [-0.15266248]\n",
      " [-0.31755015]\n",
      " ...\n",
      " [-0.23757421]\n",
      " [ 0.03347335]\n",
      " [-0.16526946]]\n",
      "t [[ 0.00604669]\n",
      " [-0.1864809 ]\n",
      " [-0.35972477]\n",
      " ...\n",
      " [-0.26752333]\n",
      " [ 0.03379179]\n",
      " [-0.19241508]]\n",
      "t [[ 0.00604669]\n",
      " [-0.1864809 ]\n",
      " [-0.35972477]\n",
      " ...\n",
      " [-0.26752333]\n",
      " [ 0.03379179]\n",
      " [-0.19241508]]\n",
      "Current iteration=6, loss=37605.69700890043\n",
      "t [[ 0.00421582]\n",
      " [-0.22049588]\n",
      " [-0.39754346]\n",
      " ...\n",
      " [-0.2939492 ]\n",
      " [ 0.03274633]\n",
      " [-0.21805629]]\n",
      "t [[ 0.00421582]\n",
      " [-0.22049588]\n",
      " [-0.39754346]\n",
      " ...\n",
      " [-0.2939492 ]\n",
      " [ 0.03274633]\n",
      " [-0.21805629]]\n",
      "t [[ 0.00202992]\n",
      " [-0.25448449]\n",
      " [-0.43174789]\n",
      " ...\n",
      " [-0.3174788 ]\n",
      " [ 0.03055019]\n",
      " [-0.24233415]]\n",
      "t [[ 0.00202992]\n",
      " [-0.25448449]\n",
      " [-0.43174789]\n",
      " ...\n",
      " [-0.3174788 ]\n",
      " [ 0.03055019]\n",
      " [-0.24233415]]\n",
      "Current iteration=8, loss=37090.585060485486\n",
      "t [[-4.15606074e-04]\n",
      " [-2.88275353e-01]\n",
      " [-4.62936652e-01]\n",
      " ...\n",
      " [-3.38617525e-01]\n",
      " [ 2.73793453e-02]\n",
      " [-2.65369598e-01]]\n",
      "t [[-4.15606074e-04]\n",
      " [-2.88275353e-01]\n",
      " [-4.62936652e-01]\n",
      " ...\n",
      " [-3.38617525e-01]\n",
      " [ 2.73793453e-02]\n",
      " [-2.65369598e-01]]\n",
      "t [[-0.00304542]\n",
      " [-0.32173755]\n",
      " [-0.49159476]\n",
      " ...\n",
      " [-0.35777415]\n",
      " [ 0.02338016]\n",
      " [-0.28726707]]\n",
      "loss=36664.04432409243\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00367803]\n",
      " [-0.02326953]\n",
      " [-0.08402174]\n",
      " ...\n",
      " [-0.06271894]\n",
      " [ 0.01333281]\n",
      " [-0.03549452]]\n",
      "t [[ 0.00367803]\n",
      " [-0.02326953]\n",
      " [-0.08402174]\n",
      " ...\n",
      " [-0.06271894]\n",
      " [ 0.01333281]\n",
      " [-0.03549452]]\n",
      "t [[ 0.00561746]\n",
      " [-0.04935144]\n",
      " [-0.15630554]\n",
      " ...\n",
      " [-0.11577959]\n",
      " [ 0.02325926]\n",
      " [-0.06839051]]\n",
      "t [[ 0.00561746]\n",
      " [-0.04935144]\n",
      " [-0.15630554]\n",
      " ...\n",
      " [-0.11577959]\n",
      " [ 0.02325926]\n",
      " [-0.06839051]]\n",
      "Current iteration=2, loss=39113.10293937561\n",
      "t [[ 0.0061965 ]\n",
      " [-0.07749519]\n",
      " [-0.21896995]\n",
      " ...\n",
      " [-0.16096492]\n",
      " [ 0.03034196]\n",
      " [-0.09899891]]\n",
      "t [[ 0.0061965 ]\n",
      " [-0.07749519]\n",
      " [-0.21896995]\n",
      " ...\n",
      " [-0.16096492]\n",
      " [ 0.03034196]\n",
      " [-0.09899891]]\n",
      "t [[ 0.00572514]\n",
      " [-0.10709285]\n",
      " [-0.27376615]\n",
      " ...\n",
      " [-0.19974767]\n",
      " [ 0.0350548 ]\n",
      " [-0.12758765]]\n",
      "t [[ 0.00572514]\n",
      " [-0.10709285]\n",
      " [-0.27376615]\n",
      " ...\n",
      " [-0.19974767]\n",
      " [ 0.0350548 ]\n",
      " [-0.12758765]]\n",
      "Current iteration=4, loss=38273.472954068435\n",
      "t [[ 0.00445116]\n",
      " [-0.13766251]\n",
      " [-0.32211423]\n",
      " ...\n",
      " [-0.2333216 ]\n",
      " [ 0.03778799]\n",
      " [-0.15438369]]\n",
      "t [[ 0.00445116]\n",
      " [-0.13766251]\n",
      " [-0.32211423]\n",
      " ...\n",
      " [-0.2333216 ]\n",
      " [ 0.03778799]\n",
      " [-0.15438369]]\n",
      "t [[ 0.00257039]\n",
      " [-0.16882644]\n",
      " [-0.36515719]\n",
      " ...\n",
      " [-0.26264697]\n",
      " [ 0.03886032]\n",
      " [-0.17957841]]\n",
      "t [[ 0.00257039]\n",
      " [-0.16882644]\n",
      " [-0.36515719]\n",
      " ...\n",
      " [-0.26264697]\n",
      " [ 0.03886032]\n",
      " [-0.17957841]]\n",
      "Current iteration=6, loss=37638.418950783365\n",
      "t [[ 2.36984900e-04]\n",
      " [-2.00290325e-01]\n",
      " [-4.03813843e-01]\n",
      " ...\n",
      " [-2.88495003e-01]\n",
      " [ 3.85324256e-02]\n",
      " [-2.03333585e-01]]\n",
      "t [[ 2.36984900e-04]\n",
      " [-2.00290325e-01]\n",
      " [-4.03813843e-01]\n",
      " ...\n",
      " [-2.88495003e-01]\n",
      " [ 3.85324256e-02]\n",
      " [-2.03333585e-01]]\n",
      "t [[-0.00242778]\n",
      " [-0.23182572]\n",
      " [-0.43882428]\n",
      " ...\n",
      " [-0.311486  ]\n",
      " [ 0.03701836]\n",
      " [-0.22578669]]\n",
      "t [[-0.00242778]\n",
      " [-0.23182572]\n",
      " [-0.43882428]\n",
      " ...\n",
      " [-0.311486  ]\n",
      " [ 0.03701836]\n",
      " [-0.22578669]]\n",
      "Current iteration=8, loss=37131.48782825143\n",
      "t [[-0.00532844]\n",
      " [-0.26325595]\n",
      " [-0.47078661]\n",
      " ...\n",
      " [-0.33212029]\n",
      " [ 0.03449513]\n",
      " [-0.24705542]]\n",
      "t [[-0.00532844]\n",
      " [-0.26325595]\n",
      " [-0.47078661]\n",
      " ...\n",
      " [-0.33212029]\n",
      " [ 0.03449513]\n",
      " [-0.24705542]]\n",
      "t [[-0.00838976]\n",
      " [-0.29444502]\n",
      " [-0.50018614]\n",
      " ...\n",
      " [-0.35080272]\n",
      " [ 0.03111021]\n",
      " [-0.26724125]]\n",
      "loss=36712.27330310799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00306306]\n",
      " [-0.02464645]\n",
      " [-0.08346734]\n",
      " ...\n",
      " [-0.03674852]\n",
      " [-0.0044259 ]\n",
      " [-0.07497294]]\n",
      "t [[ 0.00306306]\n",
      " [-0.02464645]\n",
      " [-0.08346734]\n",
      " ...\n",
      " [-0.03674852]\n",
      " [-0.0044259 ]\n",
      " [-0.07497294]]\n",
      "t [[ 0.00442185]\n",
      " [-0.05208587]\n",
      " [-0.1550991 ]\n",
      " ...\n",
      " [-0.06591169]\n",
      " [-0.00634348]\n",
      " [-0.1455293 ]]\n",
      "t [[ 0.00442185]\n",
      " [-0.05208587]\n",
      " [-0.1550991 ]\n",
      " ...\n",
      " [-0.06591169]\n",
      " [-0.00634348]\n",
      " [-0.1455293 ]]\n",
      "Current iteration=2, loss=39092.13190434284\n",
      "t [[ 0.0044529 ]\n",
      " [-0.08156284]\n",
      " [-0.21704194]\n",
      " ...\n",
      " [-0.08899736]\n",
      " [-0.00636259]\n",
      " [-0.21212808]]\n",
      "t [[ 0.0044529 ]\n",
      " [-0.08156284]\n",
      " [-0.21704194]\n",
      " ...\n",
      " [-0.08899736]\n",
      " [-0.00636259]\n",
      " [-0.21212808]]\n",
      "t [[ 0.00346429]\n",
      " [-0.11246585]\n",
      " [-0.27106944]\n",
      " ...\n",
      " [-0.10724207]\n",
      " [-0.00497373]\n",
      " [-0.27517321]]\n",
      "t [[ 0.00346429]\n",
      " [-0.11246585]\n",
      " [-0.27106944]\n",
      " ...\n",
      " [-0.10724207]\n",
      " [-0.00497373]\n",
      " [-0.27517321]]\n",
      "Current iteration=4, loss=38238.36466175136\n",
      "t [[ 0.00170183]\n",
      " [-0.14431084]\n",
      " [-0.31861889]\n",
      " ...\n",
      " [-0.12164049]\n",
      " [-0.00256393]\n",
      " [-0.33501513]]\n",
      "t [[ 0.00170183]\n",
      " [-0.14431084]\n",
      " [-0.31861889]\n",
      " ...\n",
      " [-0.12164049]\n",
      " [-0.00256393]\n",
      " [-0.33501513]]\n",
      "t [[-0.00064052]\n",
      " [-0.17671897]\n",
      " [-0.36084627]\n",
      " ...\n",
      " [-0.13298612]\n",
      " [ 0.00056453]\n",
      " [-0.39195645]]\n",
      "t [[-0.00064052]\n",
      " [-0.17671897]\n",
      " [-0.36084627]\n",
      " ...\n",
      " [-0.13298612]\n",
      " [ 0.00056453]\n",
      " [-0.39195645]]\n",
      "Current iteration=6, loss=37592.6082143541\n",
      "t [[-0.00341043]\n",
      " [-0.20939555]\n",
      " [-0.39868011]\n",
      " ...\n",
      " [-0.14191056]\n",
      " [ 0.00417689]\n",
      " [-0.44625869]]\n",
      "t [[-0.00341043]\n",
      " [-0.20939555]\n",
      " [-0.39868011]\n",
      " ...\n",
      " [-0.14191056]\n",
      " [ 0.00417689]\n",
      " [-0.44625869]]\n",
      "t [[-0.00648836]\n",
      " [-0.24211223]\n",
      " [-0.43286771]\n",
      " ...\n",
      " [-0.14891672]\n",
      " [ 0.00809161]\n",
      " [-0.49814868]]\n",
      "t [[-0.00648836]\n",
      " [-0.24211223]\n",
      " [-0.43286771]\n",
      " ...\n",
      " [-0.14891672]\n",
      " [ 0.00809161]\n",
      " [-0.49814868]]\n",
      "Current iteration=8, loss=37076.934064013025\n",
      "t [[-0.00978047]\n",
      " [-0.27469273]\n",
      " [-0.46401256]\n",
      " ...\n",
      " [-0.15440574]\n",
      " [ 0.01216896]\n",
      " [-0.54782408]]\n",
      "t [[-0.00978047]\n",
      " [-0.27469273]\n",
      " [-0.46401256]\n",
      " ...\n",
      " [-0.15440574]\n",
      " [ 0.01216896]\n",
      " [-0.54782408]]\n",
      "t [[-0.01321309]\n",
      " [-0.30700164]\n",
      " [-0.49260397]\n",
      " ...\n",
      " [-0.1586982 ]\n",
      " [ 0.01630196]\n",
      " [-0.59545815]]\n",
      "loss=36650.26151211111\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.03537576]\n",
      " [-0.15112121]\n",
      " [-0.09498512]\n",
      " ...\n",
      " [-0.06884067]\n",
      " [ 0.01490438]\n",
      " [-0.04013049]]\n",
      "t [[ 0.03537576]\n",
      " [-0.15112121]\n",
      " [-0.09498512]\n",
      " ...\n",
      " [-0.06884067]\n",
      " [ 0.01490438]\n",
      " [-0.04013049]]\n",
      "t [[ 0.06479408]\n",
      " [-0.28178713]\n",
      " [-0.17652396]\n",
      " ...\n",
      " [-0.12643567]\n",
      " [ 0.02575606]\n",
      " [-0.07717582]]\n",
      "t [[ 0.06479408]\n",
      " [-0.28178713]\n",
      " [-0.17652396]\n",
      " ...\n",
      " [-0.12643567]\n",
      " [ 0.02575606]\n",
      " [-0.07717582]]\n",
      "Current iteration=2, loss=39029.16893366933\n",
      "t [[ 0.08932029]\n",
      " [-0.39565523]\n",
      " [-0.24715771]\n",
      " ...\n",
      " [-0.17501827]\n",
      " [ 0.03327152]\n",
      " [-0.11152798]]\n",
      "t [[ 0.08932029]\n",
      " [-0.39565523]\n",
      " [-0.24715771]\n",
      " ...\n",
      " [-0.17501827]\n",
      " [ 0.03327152]\n",
      " [-0.11152798]]\n",
      "t [[ 0.10983496]\n",
      " [-0.49572229]\n",
      " [-0.30895322]\n",
      " ...\n",
      " [-0.216398  ]\n",
      " [ 0.03804476]\n",
      " [-0.14351998]]\n",
      "t [[ 0.10983496]\n",
      " [-0.49572229]\n",
      " [-0.30895322]\n",
      " ...\n",
      " [-0.216398  ]\n",
      " [ 0.03804476]\n",
      " [-0.14351998]]\n",
      "Current iteration=4, loss=38152.16167008603\n",
      "t [[ 0.1270524 ]\n",
      " [-0.584395  ]\n",
      " [-0.36355715]\n",
      " ...\n",
      " [-0.252012  ]\n",
      " [ 0.0405565 ]\n",
      " [-0.17342997]]\n",
      "t [[ 0.1270524 ]\n",
      " [-0.584395  ]\n",
      " [-0.36355715]\n",
      " ...\n",
      " [-0.252012  ]\n",
      " [ 0.0405565 ]\n",
      " [-0.17342997]]\n",
      "t [[ 0.14154867]\n",
      " [-0.66359487]\n",
      " [-0.41227273]\n",
      " ...\n",
      " [-0.2829938 ]\n",
      " [ 0.04119332]\n",
      " [-0.20148965]]\n",
      "t [[ 0.14154867]\n",
      " [-0.66359487]\n",
      " [-0.41227273]\n",
      " ...\n",
      " [-0.2829938 ]\n",
      " [ 0.04119332]\n",
      " [-0.20148965]]\n",
      "Current iteration=6, loss=37497.6239570009\n",
      "t [[ 0.15378858]\n",
      " [-0.73485826]\n",
      " [-0.456132  ]\n",
      " ...\n",
      " [-0.31023712]\n",
      " [ 0.04026684]\n",
      " [-0.22789279]]\n",
      "t [[ 0.15378858]\n",
      " [-0.73485826]\n",
      " [-0.456132  ]\n",
      " ...\n",
      " [-0.31023712]\n",
      " [ 0.04026684]\n",
      " [-0.22789279]]\n",
      "t [[ 0.16414854]\n",
      " [-0.79941957]\n",
      " [-0.49595535]\n",
      " ...\n",
      " [-0.33444847]\n",
      " [ 0.03803001]\n",
      " [-0.25280265]]\n",
      "t [[ 0.16414854]\n",
      " [-0.79941957]\n",
      " [-0.49595535]\n",
      " ...\n",
      " [-0.33444847]\n",
      " [ 0.03803001]\n",
      " [-0.25280265]]\n",
      "Current iteration=8, loss=36979.50805711144\n",
      "t [[ 0.17293484]\n",
      " [-0.85827692]\n",
      " [-0.53239837]\n",
      " ...\n",
      " [-0.35618842]\n",
      " [ 0.03468991]\n",
      " [-0.27635799]]\n",
      "t [[ 0.17293484]\n",
      " [-0.85827692]\n",
      " [-0.53239837]\n",
      " ...\n",
      " [-0.35618842]\n",
      " [ 0.03468991]\n",
      " [-0.27635799]]\n",
      "t [[ 0.18039804]\n",
      " [-0.91224289]\n",
      " [-0.56598786]\n",
      " ...\n",
      " [-0.37590355]\n",
      " [ 0.03041756]\n",
      " [-0.29867777]]\n",
      "loss=36553.87673561766\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00466513]\n",
      " [-0.02872258]\n",
      " [-0.0900483 ]\n",
      " ...\n",
      " [-0.06910111]\n",
      " [ 0.01343373]\n",
      " [-0.04099384]]\n",
      "t [[ 0.00466513]\n",
      " [-0.02872258]\n",
      " [-0.0900483 ]\n",
      " ...\n",
      " [-0.06910111]\n",
      " [ 0.01343373]\n",
      " [-0.04099384]]\n",
      "t [[ 0.00727041]\n",
      " [-0.0605982 ]\n",
      " [-0.16629652]\n",
      " ...\n",
      " [-0.12671394]\n",
      " [ 0.02290993]\n",
      " [-0.07883253]]\n",
      "t [[ 0.00727041]\n",
      " [-0.0605982 ]\n",
      " [-0.16629652]\n",
      " ...\n",
      " [-0.12671394]\n",
      " [ 0.02290993]\n",
      " [-0.07883253]]\n",
      "Current iteration=2, loss=39014.30890045171\n",
      "t [[ 0.00829476]\n",
      " [-0.0946747 ]\n",
      " [-0.23146826]\n",
      " ...\n",
      " [-0.17514459]\n",
      " [ 0.02914787]\n",
      " [-0.11392045]]\n",
      "t [[ 0.00829476]\n",
      " [-0.0946747 ]\n",
      " [-0.23146826]\n",
      " ...\n",
      " [-0.17514459]\n",
      " [ 0.02914787]\n",
      " [-0.11392045]]\n",
      "t [[ 0.00812219]\n",
      " [-0.13020066]\n",
      " [-0.28776477]\n",
      " ...\n",
      " [-0.21625604]\n",
      " [ 0.03274079]\n",
      " [-0.14660024]]\n",
      "t [[ 0.00812219]\n",
      " [-0.13020066]\n",
      " [-0.28776477]\n",
      " ...\n",
      " [-0.21625604]\n",
      " [ 0.03274079]\n",
      " [-0.14660024]]\n",
      "Current iteration=4, loss=38126.514510116926\n",
      "t [[ 0.00705235]\n",
      " [-0.1665975 ]\n",
      " [-0.3369291 ]\n",
      " ...\n",
      " [-0.25152345]\n",
      " [ 0.0341662 ]\n",
      " [-0.17715744]]\n",
      "t [[ 0.00705235]\n",
      " [-0.1665975 ]\n",
      " [-0.3369291 ]\n",
      " ...\n",
      " [-0.25152345]\n",
      " [ 0.0341662 ]\n",
      " [-0.17715744]]\n",
      "t [[ 0.00531666]\n",
      " [-0.203425  ]\n",
      " [-0.38033229]\n",
      " ...\n",
      " [-0.28210705]\n",
      " [ 0.03380602]\n",
      " [-0.20582937]]\n",
      "t [[ 0.00531666]\n",
      " [-0.203425  ]\n",
      " [-0.38033229]\n",
      " ...\n",
      " [-0.28210705]\n",
      " [ 0.03380602]\n",
      " [-0.20582937]]\n",
      "Current iteration=6, loss=37462.631680073835\n",
      "t [[ 0.00309336]\n",
      " [-0.24035037]\n",
      " [-0.41905306]\n",
      " ...\n",
      " [-0.30891926]\n",
      " [ 0.0319665 ]\n",
      " [-0.23281417]]\n",
      "t [[ 0.00309336]\n",
      " [-0.24035037]\n",
      " [-0.41905306]\n",
      " ...\n",
      " [-0.30891926]\n",
      " [ 0.0319665 ]\n",
      " [-0.23281417]]\n",
      "t [[ 0.00051982]\n",
      " [-0.27712329]\n",
      " [-0.453943  ]\n",
      " ...\n",
      " [-0.33267965]\n",
      " [ 0.02889496]\n",
      " [-0.25827853]]\n",
      "t [[ 0.00051982]\n",
      " [-0.27712329]\n",
      " [-0.453943  ]\n",
      " ...\n",
      " [-0.33267965]\n",
      " [ 0.02889496]\n",
      " [-0.25827853]]\n",
      "Current iteration=8, loss=36936.11988598644\n",
      "t [[-0.00229789]\n",
      " [-0.31355655]\n",
      " [-0.4856775 ]\n",
      " ...\n",
      " [-0.35395796]\n",
      " [ 0.02479289]\n",
      " [-0.28236398]]\n",
      "t [[-0.00229789]\n",
      " [-0.31355655]\n",
      " [-0.4856775 ]\n",
      " ...\n",
      " [-0.35395796]\n",
      " [ 0.02479289]\n",
      " [-0.28236398]]\n",
      "t [[-0.00527778]\n",
      " [-0.34951139]\n",
      " [-0.51479482]\n",
      " ...\n",
      " [-0.37320719]\n",
      " [ 0.01982592]\n",
      " [-0.30519176]]\n",
      "loss=36502.98794660499\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00398454]\n",
      " [-0.02520866]\n",
      " [-0.09102355]\n",
      " ...\n",
      " [-0.06794552]\n",
      " [ 0.01444388]\n",
      " [-0.0384524 ]]\n",
      "t [[ 0.00398454]\n",
      " [-0.02520866]\n",
      " [-0.09102355]\n",
      " ...\n",
      " [-0.06794552]\n",
      " [ 0.01444388]\n",
      " [-0.0384524 ]]\n",
      "t [[ 0.00592945]\n",
      " [-0.05371678]\n",
      " [-0.16827524]\n",
      " ...\n",
      " [-0.12455915]\n",
      " [ 0.02489137]\n",
      " [-0.07385581]]\n",
      "t [[ 0.00592945]\n",
      " [-0.05371678]\n",
      " [-0.16827524]\n",
      " ...\n",
      " [-0.12455915]\n",
      " [ 0.02489137]\n",
      " [-0.07385581]]\n",
      "Current iteration=2, loss=39028.34132350592\n",
      "t [[ 0.00631631]\n",
      " [-0.08456925]\n",
      " [-0.23445231]\n",
      " ...\n",
      " [-0.17211079]\n",
      " [ 0.03205912]\n",
      " [-0.10660639]]\n",
      "t [[ 0.00631631]\n",
      " [-0.08456925]\n",
      " [-0.23445231]\n",
      " ...\n",
      " [-0.17211079]\n",
      " [ 0.03205912]\n",
      " [-0.10660639]]\n",
      "t [[ 0.00553101]\n",
      " [-0.11700989]\n",
      " [-0.29173802]\n",
      " ...\n",
      " [-0.21243652]\n",
      " [ 0.0365388 ]\n",
      " [-0.13703965]]\n",
      "t [[ 0.00553101]\n",
      " [-0.11700989]\n",
      " [-0.29173802]\n",
      " ...\n",
      " [-0.21243652]\n",
      " [ 0.0365388 ]\n",
      " [-0.13703965]]\n",
      "Current iteration=4, loss=38151.70942214012\n",
      "t [[ 0.0038746 ]\n",
      " [-0.15045394]\n",
      " [-0.34186423]\n",
      " ...\n",
      " [-0.24699207]\n",
      " [ 0.03880745]\n",
      " [-0.16543488]]\n",
      "t [[ 0.0038746 ]\n",
      " [-0.15045394]\n",
      " [-0.34186423]\n",
      " ...\n",
      " [-0.24699207]\n",
      " [ 0.03880745]\n",
      " [-0.16543488]]\n",
      "t [[ 0.00157944]\n",
      " [-0.18445425]\n",
      " [-0.38619556]\n",
      " ...\n",
      " [-0.27692373]\n",
      " [ 0.03924732]\n",
      " [-0.19202391]]\n",
      "t [[ 0.00157944]\n",
      " [-0.18445425]\n",
      " [-0.38619556]\n",
      " ...\n",
      " [-0.27692373]\n",
      " [ 0.03924732]\n",
      " [-0.19202391]]\n",
      "Current iteration=6, loss=37497.51805566284\n",
      "t [[-0.00117568]\n",
      " [-0.21867098]\n",
      " [-0.42580749]\n",
      " ...\n",
      " [-0.30313383]\n",
      " [ 0.03816557]\n",
      " [-0.21699997]]\n",
      "t [[-0.00117568]\n",
      " [-0.21867098]\n",
      " [-0.42580749]\n",
      " ...\n",
      " [-0.30313383]\n",
      " [ 0.03816557]\n",
      " [-0.21699997]]\n",
      "t [[-0.00425312]\n",
      " [-0.25284685]\n",
      " [-0.46155038]\n",
      " ...\n",
      " [-0.32633453]\n",
      " [ 0.03581073]\n",
      " [-0.24052541]]\n",
      "t [[-0.00425312]\n",
      " [-0.25284685]\n",
      " [-0.46155038]\n",
      " ...\n",
      " [-0.32633453]\n",
      " [ 0.03581073]\n",
      " [-0.24052541]]\n",
      "Current iteration=8, loss=36979.59592354839\n",
      "t [[-0.00754685]\n",
      " [-0.28678807]\n",
      " [-0.49409966]\n",
      " ...\n",
      " [-0.34709003]\n",
      " [ 0.03238563]\n",
      " [-0.26273785]]\n",
      "t [[-0.00754685]\n",
      " [-0.28678807]\n",
      " [-0.49409966]\n",
      " ...\n",
      " [-0.34709003]\n",
      " [ 0.03238563]\n",
      " [-0.26273785]]\n",
      "t [[-0.01097515]\n",
      " [-0.32034966]\n",
      " [-0.52399445]\n",
      " ...\n",
      " [-0.36584913]\n",
      " [ 0.02805741]\n",
      " [-0.28375501]]\n",
      "loss=36554.08209786218\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00331832]\n",
      " [-0.02670032]\n",
      " [-0.09042295]\n",
      " ...\n",
      " [-0.0398109 ]\n",
      " [-0.00479472]\n",
      " [-0.08122068]]\n",
      "t [[ 0.00331832]\n",
      " [-0.02670032]\n",
      " [-0.09042295]\n",
      " ...\n",
      " [-0.0398109 ]\n",
      " [-0.00479472]\n",
      " [-0.08122068]]\n",
      "t [[ 0.00463729]\n",
      " [-0.05667727]\n",
      " [-0.16695954]\n",
      " ...\n",
      " [-0.07072212]\n",
      " [-0.00664645]\n",
      " [-0.15725902]]\n",
      "t [[ 0.00463729]\n",
      " [-0.05667727]\n",
      " [-0.16695954]\n",
      " ...\n",
      " [-0.07072212]\n",
      " [-0.00664645]\n",
      " [-0.15725902]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2, loss=39005.92278513576\n",
      "t [[ 0.00443634]\n",
      " [-0.0889695 ]\n",
      " [-0.23234281]\n",
      " ...\n",
      " [-0.09465301]\n",
      " [-0.00633105]\n",
      " [-0.22870002]]\n",
      "t [[ 0.00443634]\n",
      " [-0.0889695 ]\n",
      " [-0.23234281]\n",
      " ...\n",
      " [-0.09465301]\n",
      " [-0.00633105]\n",
      " [-0.22870002]]\n",
      "t [[ 0.00309889]\n",
      " [-0.12281656]\n",
      " [-0.28878378]\n",
      " ...\n",
      " [-0.11314392]\n",
      " [-0.00445835]\n",
      " [-0.29605099]]\n",
      "t [[ 0.00309889]\n",
      " [-0.12281656]\n",
      " [-0.28878378]\n",
      " ...\n",
      " [-0.11314392]\n",
      " [-0.00445835]\n",
      " [-0.29605099]]\n",
      "Current iteration=4, loss=38114.56008580873\n",
      "t [[ 0.00092355]\n",
      " [-0.15763123]\n",
      " [-0.33803498]\n",
      " ...\n",
      " [-0.12740583]\n",
      " [-0.00149773]\n",
      " [-0.35974542]]\n",
      "t [[ 0.00092355]\n",
      " [-0.15763123]\n",
      " [-0.33803498]\n",
      " ...\n",
      " [-0.12740583]\n",
      " [-0.00149773]\n",
      " [-0.35974542]]\n",
      "t [[-0.00185972]\n",
      " [-0.19296534]\n",
      " [-0.3814762 ]\n",
      " ...\n",
      " [-0.13838356]\n",
      " [ 0.00219322]\n",
      " [-0.42015242]]\n",
      "t [[-0.00185972]\n",
      " [-0.19296534]\n",
      " [-0.3814762 ]\n",
      " ...\n",
      " [-0.13838356]\n",
      " [ 0.00219322]\n",
      " [-0.42015242]]\n",
      "Current iteration=6, loss=37449.30930303282\n",
      "t [[-0.00507438]\n",
      " [-0.22847882]\n",
      " [-0.4201939 ]\n",
      " ...\n",
      " [-0.14681327]\n",
      " [ 0.00634376]\n",
      " [-0.47758705]]\n",
      "t [[-0.00507438]\n",
      " [-0.22847882]\n",
      " [-0.4201939 ]\n",
      " ...\n",
      " [-0.14681327]\n",
      " [ 0.00634376]\n",
      " [-0.47758705]]\n",
      "t [[-0.00858498]\n",
      " [-0.26391475]\n",
      " [-0.45504642]\n",
      " ...\n",
      " [-0.15326932]\n",
      " [ 0.01074985]\n",
      " [-0.5323196 ]]\n",
      "t [[-0.00858498]\n",
      " [-0.26391475]\n",
      " [-0.45504642]\n",
      " ...\n",
      " [-0.15326932]\n",
      " [ 0.01074985]\n",
      " [-0.5323196 ]]\n",
      "Current iteration=8, loss=36922.37787938795\n",
      "t [[-0.0122875 ]\n",
      " [-0.29907998]\n",
      " [-0.48671493]\n",
      " ...\n",
      " [-0.1582009 ]\n",
      " [ 0.0152587 ]\n",
      " [-0.58458349]]\n",
      "t [[-0.0122875 ]\n",
      " [-0.29907998]\n",
      " [-0.48671493]\n",
      " ...\n",
      " [-0.1582009 ]\n",
      " [ 0.0152587 ]\n",
      " [-0.58458349]]\n",
      "t [[-0.0161021 ]\n",
      " [-0.33383035]\n",
      " [-0.51574268]\n",
      " ...\n",
      " [-0.16196005]\n",
      " [ 0.01975682]\n",
      " [-0.63458162]]\n",
      "loss=36489.20222443329\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.03809698]\n",
      " [-0.16274592]\n",
      " [-0.10229167]\n",
      " ...\n",
      " [-0.0741361 ]\n",
      " [ 0.01605087]\n",
      " [-0.04321745]]\n",
      "t [[ 0.03809698]\n",
      " [-0.16274592]\n",
      " [-0.10229167]\n",
      " ...\n",
      " [-0.0741361 ]\n",
      " [ 0.01605087]\n",
      " [-0.04321745]]\n",
      "t [[ 0.06928674]\n",
      " [-0.30177574]\n",
      " [-0.18899368]\n",
      " ...\n",
      " [-0.13523382]\n",
      " [ 0.0274033 ]\n",
      " [-0.08285767]]\n",
      "t [[ 0.06928674]\n",
      " [-0.30177574]\n",
      " [-0.18899368]\n",
      " ...\n",
      " [-0.13523382]\n",
      " [ 0.0274033 ]\n",
      " [-0.08285767]]\n",
      "Current iteration=2, loss=38946.43641628859\n",
      "t [[ 0.09490179]\n",
      " [-0.42166372]\n",
      " [-0.26328383]\n",
      " ...\n",
      " [-0.18608559]\n",
      " [ 0.03495361]\n",
      " [-0.11941114]]\n",
      "t [[ 0.09490179]\n",
      " [-0.42166372]\n",
      " [-0.26328383]\n",
      " ...\n",
      " [-0.18608559]\n",
      " [ 0.03495361]\n",
      " [-0.11941114]]\n",
      "t [[ 0.11602212]\n",
      " [-0.52608136]\n",
      " [-0.32769237]\n",
      " ...\n",
      " [-0.22890604]\n",
      " [ 0.03942994]\n",
      " [-0.15328736]]\n",
      "t [[ 0.11602212]\n",
      " [-0.52608136]\n",
      " [-0.32769237]\n",
      " ...\n",
      " [-0.22890604]\n",
      " [ 0.03942994]\n",
      " [-0.15328736]]\n",
      "Current iteration=4, loss=38035.431422331116\n",
      "t [[ 0.13350627]\n",
      " [-0.61791516]\n",
      " [-0.38419119]\n",
      " ...\n",
      " [-0.26541534]\n",
      " [ 0.04140939]\n",
      " [-0.18482256]]\n",
      "t [[ 0.13350627]\n",
      " [-0.61791516]\n",
      " [-0.38419119]\n",
      " ...\n",
      " [-0.26541534]\n",
      " [ 0.04140939]\n",
      " [-0.18482256]]\n",
      "t [[ 0.14803358]\n",
      " [-0.69942463]\n",
      " [-0.43430889]\n",
      " ...\n",
      " [-0.29694176]\n",
      " [ 0.0413473 ]\n",
      " [-0.21429252]]\n",
      "t [[ 0.14803358]\n",
      " [-0.69942463]\n",
      " [-0.43430889]\n",
      " ...\n",
      " [-0.29694176]\n",
      " [ 0.0413473 ]\n",
      " [-0.21429252]]\n",
      "Current iteration=6, loss=37363.910771821145\n",
      "t [[ 0.1601427 ]\n",
      " [-0.77238436]\n",
      " [-0.47923311]\n",
      " ...\n",
      " [-0.32451197]\n",
      " [ 0.03960478]\n",
      " [-0.24192488]]\n",
      "t [[ 0.1601427 ]\n",
      " [-0.77238436]\n",
      " [-0.47923311]\n",
      " ...\n",
      " [-0.32451197]\n",
      " [ 0.03960478]\n",
      " [-0.24192488]]\n",
      "t [[ 0.17026296]\n",
      " [-0.83819749]\n",
      " [-0.51989162]\n",
      " ...\n",
      " [-0.3489226 ]\n",
      " [ 0.03647086]\n",
      " [-0.26790942]]\n",
      "t [[ 0.17026296]\n",
      " [-0.83819749]\n",
      " [-0.51989162]\n",
      " ...\n",
      " [-0.3489226 ]\n",
      " [ 0.03647086]\n",
      " [-0.26790942]]\n",
      "Current iteration=8, loss=36836.390490615784\n",
      "t [[ 0.17873861]\n",
      " [-0.89798231]\n",
      " [-0.55701389]\n",
      " ...\n",
      " [-0.3707947 ]\n",
      " [ 0.0321793 ]\n",
      " [-0.29240597]]\n",
      "t [[ 0.17873861]\n",
      " [-0.89798231]\n",
      " [-0.55701389]\n",
      " ...\n",
      " [-0.3707947 ]\n",
      " [ 0.0321793 ]\n",
      " [-0.29240597]]\n",
      "t [[ 0.18584745]\n",
      " [-0.95263737]\n",
      " [-0.59117732]\n",
      " ...\n",
      " [-0.39061472]\n",
      " [ 0.02692118]\n",
      " [-0.31555056]]\n",
      "loss=36405.74836277898\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00502399]\n",
      " [-0.03093201]\n",
      " [-0.09697509]\n",
      " ...\n",
      " [-0.07441658]\n",
      " [ 0.01446709]\n",
      " [-0.04414722]]\n",
      "t [[ 0.00502399]\n",
      " [-0.03093201]\n",
      " [-0.09697509]\n",
      " ...\n",
      " [-0.07441658]\n",
      " [ 0.01446709]\n",
      " [-0.04414722]]\n",
      "t [[ 0.00766008]\n",
      " [-0.06551927]\n",
      " [-0.17795036]\n",
      " ...\n",
      " [-0.13551356]\n",
      " [ 0.02434614]\n",
      " [-0.08463607]]\n",
      "t [[ 0.00766008]\n",
      " [-0.06551927]\n",
      " [-0.17795036]\n",
      " ...\n",
      " [-0.13551356]\n",
      " [ 0.02434614]\n",
      " [-0.08463607]]\n",
      "Current iteration=2, loss=38930.622485550324\n",
      "t [[ 0.0085072 ]\n",
      " [-0.10257177]\n",
      " [-0.24633138]\n",
      " ...\n",
      " [-0.18617438]\n",
      " [ 0.0305369 ]\n",
      " [-0.12197251]]\n",
      "t [[ 0.0085072 ]\n",
      " [-0.10257177]\n",
      " [-0.24633138]\n",
      " ...\n",
      " [-0.18617438]\n",
      " [ 0.0305369 ]\n",
      " [-0.12197251]]\n",
      "t [[ 0.00803443]\n",
      " [-0.14117262]\n",
      " [-0.30481129]\n",
      " ...\n",
      " [-0.22867833]\n",
      " [ 0.03376617]\n",
      " [-0.15657783]]\n",
      "t [[ 0.00803443]\n",
      " [-0.14117262]\n",
      " [-0.30481129]\n",
      " ...\n",
      " [-0.22867833]\n",
      " [ 0.03376617]\n",
      " [-0.15657783]]\n",
      "Current iteration=4, loss=38008.24800785993\n",
      "t [[ 0.00659898]\n",
      " [-0.18063374]\n",
      " [-0.35547499]\n",
      " ...\n",
      " [-0.26479003]\n",
      " [ 0.03460656]\n",
      " [-0.18879698]]\n",
      "t [[ 0.00659898]\n",
      " [-0.18063374]\n",
      " [-0.35547499]\n",
      " ...\n",
      " [-0.26479003]\n",
      " [ 0.03460656]\n",
      " [-0.18879698]]\n",
      "t [[ 0.00447019]\n",
      " [-0.22044492]\n",
      " [-0.39992778]\n",
      " ...\n",
      " [-0.29586792]\n",
      " [ 0.03350724]\n",
      " [-0.21891228]]\n",
      "t [[ 0.00447019]\n",
      " [-0.22044492]\n",
      " [-0.39992778]\n",
      " ...\n",
      " [-0.29586792]\n",
      " [ 0.03350724]\n",
      " [-0.21891228]]\n",
      "Current iteration=6, loss=37326.87013307461\n",
      "t [[ 0.0018509 ]\n",
      " [-0.26023036]\n",
      " [-0.43940793]\n",
      " ...\n",
      " [-0.32295912]\n",
      " [ 0.03082239]\n",
      " [-0.24715637]]\n",
      "t [[ 0.0018509 ]\n",
      " [-0.26023036]\n",
      " [-0.43940793]\n",
      " ...\n",
      " [-0.32295912]\n",
      " [ 0.03082239]\n",
      " [-0.24715637]]\n",
      "t [[-0.00110588]\n",
      " [-0.29971485]\n",
      " [-0.47487519]\n",
      " ...\n",
      " [-0.34687414]\n",
      " [ 0.02683395]\n",
      " [-0.27372289]]\n",
      "t [[-0.00110588]\n",
      " [-0.29971485]\n",
      " [-0.47487519]\n",
      " ...\n",
      " [-0.34687414]\n",
      " [ 0.02683395]\n",
      " [-0.27372289]]\n",
      "Current iteration=8, loss=36790.55922020253\n",
      "t [[-0.00428452]\n",
      " [-0.33869852]\n",
      " [-0.50707772]\n",
      " ...\n",
      " [-0.36824341]\n",
      " [ 0.02176878]\n",
      " [-0.29877478]]\n",
      "t [[-0.00428452]\n",
      " [-0.33869852]\n",
      " [-0.50707772]\n",
      " ...\n",
      " [-0.36824341]\n",
      " [ 0.02176878]\n",
      " [-0.29877478]]\n",
      "t [[-0.00759755]\n",
      " [-0.37703803]\n",
      " [-0.53660213]\n",
      " ...\n",
      " [-0.38755971]\n",
      " [ 0.01581134]\n",
      " [-0.32245059]]\n",
      "loss=36352.16017677187\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00429104]\n",
      " [-0.02714779]\n",
      " [-0.09802536]\n",
      " ...\n",
      " [-0.0731721 ]\n",
      " [ 0.01555495]\n",
      " [-0.04141028]]\n",
      "t [[ 0.00429104]\n",
      " [-0.02714779]\n",
      " [-0.09802536]\n",
      " ...\n",
      " [-0.0731721 ]\n",
      " [ 0.01555495]\n",
      " [-0.04141028]]\n",
      "t [[ 0.00621763]\n",
      " [-0.05812069]\n",
      " [-0.18008352]\n",
      " ...\n",
      " [-0.13320588]\n",
      " [ 0.02647673]\n",
      " [-0.07928531]]\n",
      "t [[ 0.00621763]\n",
      " [-0.05812069]\n",
      " [-0.18008352]\n",
      " ...\n",
      " [-0.13320588]\n",
      " [ 0.02647673]\n",
      " [-0.07928531]]\n",
      "Current iteration=2, loss=38945.614117218545\n",
      "t [[ 0.00638195]\n",
      " [-0.09172496]\n",
      " [-0.24954712]\n",
      " ...\n",
      " [-0.18293953]\n",
      " [ 0.03366182]\n",
      " [-0.11412074]]\n",
      "t [[ 0.00638195]\n",
      " [-0.09172496]\n",
      " [-0.24954712]\n",
      " ...\n",
      " [-0.18293953]\n",
      " [ 0.03366182]\n",
      " [-0.11412074]]\n",
      "t [[ 0.00525543]\n",
      " [-0.12703749]\n",
      " [-0.30908781]\n",
      " ...\n",
      " [-0.22461979]\n",
      " [ 0.03783522]\n",
      " [-0.14632905]]\n",
      "t [[ 0.00525543]\n",
      " [-0.12703749]\n",
      " [-0.30908781]\n",
      " ...\n",
      " [-0.22461979]\n",
      " [ 0.03783522]\n",
      " [-0.14632905]]\n",
      "Current iteration=4, loss=38035.04503874319\n",
      "t [[ 0.00319688]\n",
      " [-0.16336215]\n",
      " [-0.3607779 ]\n",
      " ...\n",
      " [-0.25998851]\n",
      " [ 0.03956922]\n",
      " [-0.17624755]]\n",
      "t [[ 0.00319688]\n",
      " [-0.16336215]\n",
      " [-0.3607779 ]\n",
      " ...\n",
      " [-0.25998851]\n",
      " [ 0.03956922]\n",
      " [-0.17624755]]\n",
      "t [[ 0.00047673]\n",
      " [-0.20017995]\n",
      " [-0.40621598]\n",
      " ...\n",
      " [-0.29038822]\n",
      " [ 0.03931364]\n",
      " [-0.20415189]]\n",
      "t [[ 0.00047673]\n",
      " [-0.20017995]\n",
      " [-0.40621598]\n",
      " ...\n",
      " [-0.29038822]\n",
      " [ 0.03931364]\n",
      " [-0.20415189]]\n",
      "Current iteration=6, loss=37363.86563680974\n",
      "t [[-0.0027016 ]\n",
      " [-0.23710623]\n",
      " [-0.44663736]\n",
      " ...\n",
      " [-0.31685483]\n",
      " [ 0.03742396]\n",
      " [-0.23026886]]\n",
      "t [[-0.0027016 ]\n",
      " [-0.23710623]\n",
      " [-0.44663736]\n",
      " ...\n",
      " [-0.31685483]\n",
      " [ 0.03742396]\n",
      " [-0.23026886]]\n",
      "t [[-0.00618495]\n",
      " [-0.27385725]\n",
      " [-0.48300114]\n",
      " ...\n",
      " [-0.34019077]\n",
      " [ 0.03418375]\n",
      " [-0.25478692]]\n",
      "t [[-0.00618495]\n",
      " [-0.27385725]\n",
      " [-0.48300114]\n",
      " ...\n",
      " [-0.34019077]\n",
      " [ 0.03418375]\n",
      " [-0.25478692]]\n",
      "Current iteration=8, loss=36836.520806562316\n",
      "t [[-0.0098579 ]\n",
      " [-0.31022511]\n",
      " [-0.51605616]\n",
      " ...\n",
      " [-0.36102053]\n",
      " [ 0.02982165]\n",
      " [-0.27786439]]\n",
      "t [[-0.0098579 ]\n",
      " [-0.31022511]\n",
      " [-0.51605616]\n",
      " ...\n",
      " [-0.36102053]\n",
      " [ 0.02982165]\n",
      " [-0.27786439]]\n",
      "t [[-0.01363345]\n",
      " [-0.34605901]\n",
      " [-0.5463905 ]\n",
      " ...\n",
      " [-0.37983246]\n",
      " [ 0.02452405]\n",
      " [-0.29963567]]\n",
      "loss=36405.993405690795\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00357357]\n",
      " [-0.02875419]\n",
      " [-0.09737856]\n",
      " ...\n",
      " [-0.04287328]\n",
      " [-0.00516355]\n",
      " [-0.08746843]]\n",
      "t [[ 0.00357357]\n",
      " [-0.02875419]\n",
      " [-0.09737856]\n",
      " ...\n",
      " [-0.04287328]\n",
      " [-0.00516355]\n",
      " [-0.08746843]]\n",
      "t [[ 0.0048294 ]\n",
      " [-0.06130695]\n",
      " [-0.17865724]\n",
      " ...\n",
      " [-0.07542826]\n",
      " [-0.0069149 ]\n",
      " [-0.16892782]]\n",
      "t [[ 0.0048294 ]\n",
      " [-0.06130695]\n",
      " [-0.17865724]\n",
      " ...\n",
      " [-0.07542826]\n",
      " [-0.0069149 ]\n",
      " [-0.16892782]]\n",
      "Current iteration=2, loss=38921.7871463639\n",
      "t [[ 0.00436693]\n",
      " [-0.09645674]\n",
      " [-0.24725345]\n",
      " ...\n",
      " [-0.10006446]\n",
      " [-0.00622358]\n",
      " [-0.24511045]]\n",
      "t [[ 0.00436693]\n",
      " [-0.09645674]\n",
      " [-0.24725345]\n",
      " ...\n",
      " [-0.10006446]\n",
      " [-0.00622358]\n",
      " [-0.24511045]]\n",
      "t [[ 0.00265454]\n",
      " [-0.13327539]\n",
      " [-0.30587259]\n",
      " ...\n",
      " [-0.11866518]\n",
      " [-0.00383374]\n",
      " [-0.31664181]]\n",
      "t [[ 0.00265454]\n",
      " [-0.13327539]\n",
      " [-0.30587259]\n",
      " ...\n",
      " [-0.11866518]\n",
      " [-0.00383374]\n",
      " [-0.31664181]]\n",
      "Current iteration=4, loss=37995.94006230878\n",
      "t [[ 4.79983240e-05]\n",
      " [-1.71064149e-01]\n",
      " [-3.56611335e-01]\n",
      " ...\n",
      " [-1.32676999e-01]\n",
      " [-3.03866916e-04]\n",
      " [-3.84048218e-01]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 4.79983240e-05]\n",
      " [-1.71064149e-01]\n",
      " [-3.56611335e-01]\n",
      " ...\n",
      " [-1.32676999e-01]\n",
      " [-3.03866916e-04]\n",
      " [-3.84048218e-01]]\n",
      "t [[-0.00318524]\n",
      " [-0.20930309]\n",
      " [-0.40108561]\n",
      " ...\n",
      " [-0.1432035 ]\n",
      " [ 0.00395131]\n",
      " [-0.44777168]]\n",
      "t [[-0.00318524]\n",
      " [-0.20930309]\n",
      " [-0.40108561]\n",
      " ...\n",
      " [-0.1432035 ]\n",
      " [ 0.00395131]\n",
      " [-0.44777168]]\n",
      "Current iteration=6, loss=37313.3687587694\n",
      "t [[-0.00684455]\n",
      " [-0.24760759]\n",
      " [-0.44054294]\n",
      " ...\n",
      " [-0.15108644]\n",
      " [ 0.00862576]\n",
      " [-0.50818495]]\n",
      "t [[-0.00684455]\n",
      " [-0.24760759]\n",
      " [-0.44054294]\n",
      " ...\n",
      " [-0.15108644]\n",
      " [ 0.00862576]\n",
      " [-0.50818495]]\n",
      "t [[-0.01077941]\n",
      " [-0.28569456]\n",
      " [-0.475951  ]\n",
      " ...\n",
      " [-0.15696933]\n",
      " [ 0.01349501]\n",
      " [-0.56560458]]\n",
      "t [[-0.01077941]\n",
      " [-0.28569456]\n",
      " [-0.475951  ]\n",
      " ...\n",
      " [-0.15696933]\n",
      " [ 0.01349501]\n",
      " [-0.56560458]]\n",
      "Current iteration=8, loss=36776.766793623334\n",
      "t [[-0.01487684]\n",
      " [-0.32335703]\n",
      " [-0.50806467]\n",
      " ...\n",
      " [-0.16134552]\n",
      " [ 0.01839576]\n",
      " [-0.62030154]]\n",
      "t [[-0.01487684]\n",
      " [-0.32335703]\n",
      " [-0.50806467]\n",
      " ...\n",
      " [-0.16134552]\n",
      " [ 0.01839576]\n",
      " [-0.62030154]]\n",
      "t [[-0.01905209]\n",
      " [-0.36044534]\n",
      " [-0.53747621]\n",
      " ...\n",
      " [-0.1645941 ]\n",
      " [ 0.02321072]\n",
      " [-0.67250964]]\n",
      "loss=36338.39423013731\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.04081819]\n",
      " [-0.17437063]\n",
      " [-0.10959822]\n",
      " ...\n",
      " [-0.07943154]\n",
      " [ 0.01719737]\n",
      " [-0.04630441]]\n",
      "t [[ 0.04081819]\n",
      " [-0.17437063]\n",
      " [-0.10959822]\n",
      " ...\n",
      " [-0.07943154]\n",
      " [ 0.01719737]\n",
      " [-0.04630441]]\n",
      "t [[ 0.07370967]\n",
      " [-0.32152499]\n",
      " [-0.20130611]\n",
      " ...\n",
      " [-0.14390038]\n",
      " [ 0.02900324]\n",
      " [-0.08850333]]\n",
      "t [[ 0.07370967]\n",
      " [-0.32152499]\n",
      " [-0.20130611]\n",
      " ...\n",
      " [-0.14390038]\n",
      " [ 0.02900324]\n",
      " [-0.08850333]]\n",
      "Current iteration=2, loss=38865.672229042335\n",
      "t [[ 0.10031534]\n",
      " [-0.44709556]\n",
      " [-0.27903615]\n",
      " ...\n",
      " [-0.19684421]\n",
      " [ 0.03652153]\n",
      " [-0.12720096]]\n",
      "t [[ 0.10031534]\n",
      " [-0.44709556]\n",
      " [-0.27903615]\n",
      " ...\n",
      " [-0.19684421]\n",
      " [ 0.03652153]\n",
      " [-0.12720096]]\n",
      "t [[ 0.12193863]\n",
      " [-0.55550832]\n",
      " [-0.3458362 ]\n",
      " ...\n",
      " [-0.24093006]\n",
      " [ 0.04063046]\n",
      " [-0.16289289]]\n",
      "t [[ 0.12193863]\n",
      " [-0.55550832]\n",
      " [-0.3458362 ]\n",
      " ...\n",
      " [-0.24093006]\n",
      " [ 0.04063046]\n",
      " [-0.16289289]]\n",
      "Current iteration=4, loss=37923.44881041585\n",
      "t [[ 0.13959475]\n",
      " [-0.65016905]\n",
      " [-0.40402913]\n",
      " ...\n",
      " [-0.27818328]\n",
      " [ 0.04201156]\n",
      " [-0.19597902]]\n",
      "t [[ 0.13959475]\n",
      " [-0.65016905]\n",
      " [-0.40402913]\n",
      " ...\n",
      " [-0.27818328]\n",
      " [ 0.04201156]\n",
      " [-0.19597902]]\n",
      "t [[ 0.15407187]\n",
      " [-0.73369183]\n",
      " [-0.45537877]\n",
      " ...\n",
      " [-0.31013485]\n",
      " [ 0.04119265]\n",
      " [-0.22678245]]\n",
      "t [[ 0.15407187]\n",
      " [-0.73369183]\n",
      " [-0.45537877]\n",
      " ...\n",
      " [-0.31013485]\n",
      " [ 0.04119265]\n",
      " [-0.22678245]]\n",
      "Current iteration=6, loss=37236.81208075559\n",
      "t [[ 0.16598435]\n",
      " [-0.80809412]\n",
      " [-0.50122998]\n",
      " ...\n",
      " [-0.33794505]\n",
      " [ 0.03858577]\n",
      " [-0.25556662]]\n",
      "t [[ 0.16598435]\n",
      " [-0.80809412]\n",
      " [-0.50122998]\n",
      " ...\n",
      " [-0.33794505]\n",
      " [ 0.03858577]\n",
      " [-0.25556662]]\n",
      "t [[ 0.17581423]\n",
      " [-0.87494638]\n",
      " [-0.54261542]\n",
      " ...\n",
      " [-0.36249767]\n",
      " [ 0.0345163 ]\n",
      " [-0.2825489 ]]\n",
      "t [[ 0.17581423]\n",
      " [-0.87494638]\n",
      " [-0.54261542]\n",
      " ...\n",
      " [-0.36249767]\n",
      " [ 0.0345163 ]\n",
      " [-0.2825489 ]]\n",
      "Current iteration=8, loss=36701.24840468094\n",
      "t [[ 0.18394253]\n",
      " [-0.93548287]\n",
      " [-0.58033425]\n",
      " ...\n",
      " [-0.38446974]\n",
      " [ 0.02924448]\n",
      " [-0.30791089]]\n",
      "t [[ 0.18394253]\n",
      " [-0.93548287]\n",
      " [-0.58033425]\n",
      " ...\n",
      " [-0.38446974]\n",
      " [ 0.02924448]\n",
      " [-0.30791089]]\n",
      "t [[ 0.19067266]\n",
      " [-0.99068288]\n",
      " [-0.61500974]\n",
      " ...\n",
      " [-0.40438262]\n",
      " [ 0.02298093]\n",
      " [-0.3318061 ]]\n",
      "loss=36266.719913450215\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00538284]\n",
      " [-0.03314144]\n",
      " [-0.10390188]\n",
      " ...\n",
      " [-0.07973205]\n",
      " [ 0.01550045]\n",
      " [-0.04730059]]\n",
      "t [[ 0.00538284]\n",
      " [-0.03314144]\n",
      " [-0.10390188]\n",
      " ...\n",
      " [-0.07973205]\n",
      " [ 0.01550045]\n",
      " [-0.04730059]]\n",
      "t [[ 0.00802578]\n",
      " [-0.07047709]\n",
      " [-0.18944279]\n",
      " ...\n",
      " [-0.14417878]\n",
      " [ 0.02573618]\n",
      " [-0.09040259]]\n",
      "t [[ 0.00802578]\n",
      " [-0.07047709]\n",
      " [-0.18944279]\n",
      " ...\n",
      " [-0.14417878]\n",
      " [ 0.02573618]\n",
      " [-0.09040259]]\n",
      "Current iteration=2, loss=38848.924471082966\n",
      "t [[ 0.00866614]\n",
      " [-0.11054259]\n",
      " [-0.26081524]\n",
      " ...\n",
      " [-0.19688981]\n",
      " [ 0.03181525]\n",
      " [-0.12992927]]\n",
      "t [[ 0.00866614]\n",
      " [-0.11054259]\n",
      " [-0.26081524]\n",
      " ...\n",
      " [-0.19688981]\n",
      " [ 0.03181525]\n",
      " [-0.12992927]]\n",
      "t [[ 0.00786774]\n",
      " [-0.1522372 ]\n",
      " [-0.32126129]\n",
      " ...\n",
      " [-0.24060911]\n",
      " [ 0.03461372]\n",
      " [-0.16639033]]\n",
      "t [[ 0.00786774]\n",
      " [-0.1522372 ]\n",
      " [-0.32126129]\n",
      " ...\n",
      " [-0.24060911]\n",
      " [ 0.03461372]\n",
      " [-0.16639033]]\n",
      "Current iteration=4, loss=37894.75830238659\n",
      "t [[ 0.00604929]\n",
      " [-0.19475621]\n",
      " [-0.37323404]\n",
      " ...\n",
      " [-0.27741305]\n",
      " [ 0.0348071 ]\n",
      " [-0.20019587]]\n",
      "t [[ 0.00604929]\n",
      " [-0.19475621]\n",
      " [-0.37323404]\n",
      " ...\n",
      " [-0.27741305]\n",
      " [ 0.0348071 ]\n",
      " [-0.20019587]]\n",
      "t [[ 0.0035191 ]\n",
      " [-0.23751837]\n",
      " [-0.4185824 ]\n",
      " ...\n",
      " [-0.30886614]\n",
      " [ 0.03291521]\n",
      " [-0.23167645]]\n",
      "t [[ 0.0035191 ]\n",
      " [-0.23751837]\n",
      " [-0.4185824 ]\n",
      " ...\n",
      " [-0.30886614]\n",
      " [ 0.03291521]\n",
      " [-0.23167645]]\n",
      "Current iteration=6, loss=37197.7679800568\n",
      "t [[ 0.00050417]\n",
      " [-0.28010678]\n",
      " [-0.45870481]\n",
      " ...\n",
      " [-0.33615061]\n",
      " [ 0.0293414 ]\n",
      " [-0.26110113]]\n",
      "t [[ 0.00050417]\n",
      " [-0.28010678]\n",
      " [-0.45870481]\n",
      " ...\n",
      " [-0.33615061]\n",
      " [ 0.0293414 ]\n",
      " [-0.26110113]]\n",
      "t [[-0.00282794]\n",
      " [-0.32222458]\n",
      " [-0.49466569]\n",
      " ...\n",
      " [-0.36016468]\n",
      " [ 0.02440233]\n",
      " [-0.28869161]]\n",
      "t [[-0.00282794]\n",
      " [-0.32222458]\n",
      " [-0.49466569]\n",
      " ...\n",
      " [-0.36016468]\n",
      " [ 0.02440233]\n",
      " [-0.28869161]]\n",
      "Current iteration=8, loss=36653.0527596096\n",
      "t [[-0.00635335]\n",
      " [-0.36366284]\n",
      " [-0.52728076]\n",
      " ...\n",
      " [-0.38159477]\n",
      " [ 0.01834985]\n",
      " [-0.3146329 ]]\n",
      "t [[-0.00635335]\n",
      " [-0.36366284]\n",
      " [-0.52728076]\n",
      " ...\n",
      " [-0.38159477]\n",
      " [ 0.01834985]\n",
      " [-0.3146329 ]]\n",
      "t [[-0.00998044]\n",
      " [-0.40427725]\n",
      " [-0.55717926]\n",
      " ...\n",
      " [-0.40096832]\n",
      " [ 0.01138668]\n",
      " [-0.3390813 ]]\n",
      "loss=36210.55424163175\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00459754]\n",
      " [-0.02908691]\n",
      " [-0.10502717]\n",
      " ...\n",
      " [-0.07839868]\n",
      " [ 0.01666601]\n",
      " [-0.04436816]]\n",
      "t [[ 0.00459754]\n",
      " [-0.02908691]\n",
      " [-0.10502717]\n",
      " ...\n",
      " [-0.07839868]\n",
      " [ 0.01666601]\n",
      " [-0.04436816]]\n",
      "t [[ 0.00648207]\n",
      " [-0.06256309]\n",
      " [-0.19173067]\n",
      " ...\n",
      " [-0.14172001]\n",
      " [ 0.02801543]\n",
      " [-0.08467904]]\n",
      "t [[ 0.00648207]\n",
      " [-0.06256309]\n",
      " [-0.19173067]\n",
      " ...\n",
      " [-0.14172001]\n",
      " [ 0.02801543]\n",
      " [-0.08467904]]\n",
      "Current iteration=2, loss=38864.86155798717\n",
      "t [[ 0.00639489]\n",
      " [-0.09895953]\n",
      " [-0.26426255]\n",
      " ...\n",
      " [-0.19345798]\n",
      " [ 0.0351523 ]\n",
      " [-0.12154323]]\n",
      "t [[ 0.00639489]\n",
      " [-0.09895953]\n",
      " [-0.26426255]\n",
      " ...\n",
      " [-0.19345798]\n",
      " [ 0.0351523 ]\n",
      " [-0.12154323]]\n",
      "t [[ 0.00490263]\n",
      " [-0.13716746]\n",
      " [-0.32583966]\n",
      " ...\n",
      " [-0.23631778]\n",
      " [ 0.03895069]\n",
      " [-0.15545967]]\n",
      "t [[ 0.00490263]\n",
      " [-0.13716746]\n",
      " [-0.32583966]\n",
      " ...\n",
      " [-0.23631778]\n",
      " [ 0.03895069]\n",
      " [-0.15545967]]\n",
      "Current iteration=4, loss=37923.12705474814\n",
      "t [[ 0.00242589]\n",
      " [-0.17637195]\n",
      " [-0.37890121]\n",
      " ...\n",
      " [-0.27234954]\n",
      " [ 0.04008596]\n",
      " [-0.18682926]]\n",
      "t [[ 0.00242589]\n",
      " [-0.17637195]\n",
      " [-0.37890121]\n",
      " ...\n",
      " [-0.27234954]\n",
      " [ 0.04008596]\n",
      " [-0.18682926]]\n",
      "t [[-0.00072583]\n",
      " [-0.21598082]\n",
      " [-0.42528932]\n",
      " ...\n",
      " [-0.30309994]\n",
      " [ 0.03907906]\n",
      " [-0.21597466]]\n",
      "t [[-0.00072583]\n",
      " [-0.21598082]\n",
      " [-0.42528932]\n",
      " ...\n",
      " [-0.30309994]\n",
      " [ 0.03907906]\n",
      " [-0.21597466]]\n",
      "Current iteration=6, loss=37236.81946052414\n",
      "t [[-0.00432499]\n",
      " [-0.25556629]\n",
      " [-0.46640001]\n",
      " ...\n",
      " [-0.32973894]\n",
      " [ 0.03633508]\n",
      " [-0.243158  ]]\n",
      "t [[-0.00432499]\n",
      " [-0.25556629]\n",
      " [-0.46640001]\n",
      " ...\n",
      " [-0.32973894]\n",
      " [ 0.03633508]\n",
      " [-0.243158  ]]\n",
      "t [[-0.00820402]\n",
      " [-0.29482123]\n",
      " [-0.50329773]\n",
      " ...\n",
      " [-0.35315607]\n",
      " [ 0.03217282]\n",
      " [-0.26859491]]\n",
      "t [[-0.00820402]\n",
      " [-0.29482123]\n",
      " [-0.50329773]\n",
      " ...\n",
      " [-0.35315607]\n",
      " [ 0.03217282]\n",
      " [-0.26859491]]\n",
      "Current iteration=8, loss=36701.41467418821\n",
      "t [[-0.01223946]\n",
      " [-0.33352712]\n",
      " [-0.53679964]\n",
      " ...\n",
      " [-0.37403151]\n",
      " [ 0.02684638]\n",
      " [-0.29246502]]\n",
      "t [[-0.01223946]\n",
      " [-0.33352712]\n",
      " [-0.53679964]\n",
      " ...\n",
      " [-0.37403151]\n",
      " [ 0.02684638]\n",
      " [-0.29246502]]\n",
      "t [[-0.01634043]\n",
      " [-0.37153084]\n",
      " [-0.5675372 ]\n",
      " ...\n",
      " [-0.39288806]\n",
      " [ 0.02056088]\n",
      " [-0.3149198 ]]\n",
      "loss=36267.00473664366\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00382883]\n",
      " [-0.03080806]\n",
      " [-0.10433417]\n",
      " ...\n",
      " [-0.04593565]\n",
      " [-0.00553237]\n",
      " [-0.09371617]]\n",
      "t [[ 0.00382883]\n",
      " [-0.03080806]\n",
      " [-0.10433417]\n",
      " ...\n",
      " [-0.04593565]\n",
      " [-0.00553237]\n",
      " [-0.09371617]]\n",
      "t [[ 0.00499822]\n",
      " [-0.06597482]\n",
      " [-0.19019251]\n",
      " ...\n",
      " [-0.08003029]\n",
      " [-0.00714889]\n",
      " [-0.18053575]]\n",
      "t [[ 0.00499822]\n",
      " [-0.06597482]\n",
      " [-0.19019251]\n",
      " ...\n",
      " [-0.08003029]\n",
      " [-0.00714889]\n",
      " [-0.18053575]]\n",
      "Current iteration=2, loss=38839.66381062587\n",
      "t [[ 0.00424615]\n",
      " [-0.10402172]\n",
      " [-0.26178216]\n",
      " ...\n",
      " [-0.10523743]\n",
      " [-0.00604242]\n",
      " [-0.26136124]]\n",
      "t [[ 0.00424615]\n",
      " [-0.10402172]\n",
      " [-0.26178216]\n",
      " ...\n",
      " [-0.10523743]\n",
      " [-0.00604242]\n",
      " [-0.26136124]]\n",
      "t [[ 0.00213545]\n",
      " [-0.14383412]\n",
      " [-0.32236032]\n",
      " ...\n",
      " [-0.12382275]\n",
      " [-0.00310647]\n",
      " [-0.33695158]]\n",
      "t [[ 0.00213545]\n",
      " [-0.14383412]\n",
      " [-0.32236032]\n",
      " ...\n",
      " [-0.12382275]\n",
      " [-0.00310647]\n",
      " [-0.33695158]]\n",
      "Current iteration=4, loss=37882.14457331037\n",
      "t [[-0.000917  ]\n",
      " [-0.18459436]\n",
      " [-0.37439444]\n",
      " ...\n",
      " [-0.13748592]\n",
      " [ 0.00100546]\n",
      " [-0.40793562]]\n",
      "t [[-0.000917  ]\n",
      " [-0.18459436]\n",
      " [-0.37439444]\n",
      " ...\n",
      " [-0.13748592]\n",
      " [ 0.00100546]\n",
      " [-0.40793562]]\n",
      "t [[-0.00460532]\n",
      " [-0.22570949]\n",
      " [-0.41974612]\n",
      " ...\n",
      " [-0.14749472]\n",
      " [ 0.00582055]\n",
      " [-0.4748343 ]]\n",
      "t [[-0.00460532]\n",
      " [-0.22570949]\n",
      " [-0.41974612]\n",
      " ...\n",
      " [-0.14749472]\n",
      " [ 0.00582055]\n",
      " [-0.4748343 ]]\n",
      "Current iteration=6, loss=37184.13302875486\n",
      "t [[-0.00870535]\n",
      " [-0.26675211]\n",
      " [-0.45982468]\n",
      " ...\n",
      " [-0.15479577]\n",
      " [ 0.01099903]\n",
      " [-0.5380819 ]]\n",
      "t [[-0.00870535]\n",
      " [-0.26675211]\n",
      " [-0.45982468]\n",
      " ...\n",
      " [-0.15479577]\n",
      " [ 0.01099903]\n",
      " [-0.5380819 ]]\n",
      "t [[-0.01305271]\n",
      " [-0.30741606]\n",
      " [-0.49570368]\n",
      " ...\n",
      " [-0.16009809]\n",
      " [ 0.01629858]\n",
      " [-0.59804368]]\n",
      "t [[-0.01305271]\n",
      " [-0.30741606]\n",
      " [-0.49570368]\n",
      " ...\n",
      " [-0.16009809]\n",
      " [ 0.01629858]\n",
      " [-0.59804368]]\n",
      "Current iteration=8, loss=36639.240286746135\n",
      "t [[-0.01752681]\n",
      " [-0.34748414]\n",
      " [-0.52820646]\n",
      " ...\n",
      " [-0.1639344 ]\n",
      " [ 0.02154841]\n",
      " [-0.65502974]]\n",
      "t [[-0.01752681]\n",
      " [-0.34748414]\n",
      " [-0.52820646]\n",
      " ...\n",
      " [-0.1639344 ]\n",
      " [ 0.02154841]\n",
      " [-0.65502974]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.02203944]\n",
      " [-0.38680468]\n",
      " [-0.55796862]\n",
      " ...\n",
      " [-0.16670586]\n",
      " [ 0.02663035]\n",
      " [-0.70930585]]\n",
      "loss=36196.82192645794\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0435394 ]\n",
      " [-0.18599533]\n",
      " [-0.11690476]\n",
      " ...\n",
      " [-0.08472697]\n",
      " [ 0.01834386]\n",
      " [-0.04939137]]\n",
      "t [[ 0.0435394 ]\n",
      " [-0.18599533]\n",
      " [-0.11690476]\n",
      " ...\n",
      " [-0.08472697]\n",
      " [ 0.01834386]\n",
      " [-0.04939137]]\n",
      "t [[ 0.07806299]\n",
      " [-0.34103535]\n",
      " [-0.21346155]\n",
      " ...\n",
      " [-0.15243559]\n",
      " [ 0.03055597]\n",
      " [-0.09411284]]\n",
      "t [[ 0.07806299]\n",
      " [-0.34103535]\n",
      " [-0.21346155]\n",
      " ...\n",
      " [-0.15243559]\n",
      " [ 0.03055597]\n",
      " [-0.09411284]]\n",
      "Current iteration=2, loss=38786.81834698847\n",
      "t [[ 0.10556421]\n",
      " [-0.47196196]\n",
      " [-0.2944224 ]\n",
      " ...\n",
      " [-0.20730087]\n",
      " [ 0.03797753]\n",
      " [-0.13489867]]\n",
      "t [[ 0.10556421]\n",
      " [-0.47196196]\n",
      " [-0.2944224 ]\n",
      " ...\n",
      " [-0.20730087]\n",
      " [ 0.03797753]\n",
      " [-0.13489867]]\n",
      "t [[ 0.12759415]\n",
      " [-0.58403571]\n",
      " [-0.36340701]\n",
      " ...\n",
      " [-0.25248954]\n",
      " [ 0.04165278]\n",
      " [-0.17234028]]\n",
      "t [[ 0.12759415]\n",
      " [-0.58403571]\n",
      " [-0.36340701]\n",
      " ...\n",
      " [-0.25248954]\n",
      " [ 0.04165278]\n",
      " [-0.17234028]]\n",
      "Current iteration=4, loss=37815.89158599448\n",
      "t [[ 0.1453362 ]\n",
      " [-0.68121796]\n",
      " [-0.42311268]\n",
      " ...\n",
      " [-0.29035213]\n",
      " [ 0.0423752 ]\n",
      " [-0.20690664]]\n",
      "t [[ 0.1453362 ]\n",
      " [-0.68121796]\n",
      " [-0.42311268]\n",
      " ...\n",
      " [-0.29035213]\n",
      " [ 0.0423752 ]\n",
      " [-0.20690664]]\n",
      "t [[ 0.159692  ]\n",
      " [-0.7664905 ]\n",
      " [-0.47554577]\n",
      " ...\n",
      " [-0.32262806]\n",
      " [ 0.04074819]\n",
      " [-0.23897116]]\n",
      "t [[ 0.159692  ]\n",
      " [-0.7664905 ]\n",
      " [-0.47554577]\n",
      " ...\n",
      " [-0.32262806]\n",
      " [ 0.04074819]\n",
      " [-0.23897116]]\n",
      "Current iteration=6, loss=37115.76487690607\n",
      "t [[ 0.17135252]\n",
      " [-0.84211539]\n",
      " [-0.52220786]\n",
      " ...\n",
      " [-0.35060994]\n",
      " [ 0.03723563]\n",
      " [-0.26883474]]\n",
      "t [[ 0.17135252]\n",
      " [-0.84211539]\n",
      " [-0.52220786]\n",
      " ...\n",
      " [-0.35060994]\n",
      " [ 0.03723563]\n",
      " [-0.26883474]]\n",
      "t [[ 0.18085168]\n",
      " [-0.90982697]\n",
      " [-0.56423264]\n",
      " ...\n",
      " [-0.3752644 ]\n",
      " [ 0.03219928]\n",
      " [-0.29674329]]\n",
      "t [[ 0.18085168]\n",
      " [-0.90982697]\n",
      " [-0.56423264]\n",
      " ...\n",
      " [-0.3752644 ]\n",
      " [ 0.03219928]\n",
      " [-0.29674329]]\n",
      "Current iteration=8, loss=36573.3548385044\n",
      "t [[ 0.18860561]\n",
      " [-0.97096998]\n",
      " [-0.60248382]\n",
      " ...\n",
      " [-0.39731906]\n",
      " [ 0.02592534]\n",
      " [-0.3229007 ]]\n",
      "t [[ 0.18860561]\n",
      " [-0.97096998]\n",
      " [-0.60248382]\n",
      " ...\n",
      " [-0.39731906]\n",
      " [ 0.02592534]\n",
      " [-0.3229007 ]]\n",
      "t [[ 0.19494137]\n",
      " [-1.0265985 ]\n",
      " [-0.63762529]\n",
      " ...\n",
      " [-0.41732482]\n",
      " [ 0.01864336]\n",
      " [-0.34747829]]\n",
      "loss=36135.938245711295\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0057417 ]\n",
      " [-0.03535087]\n",
      " [-0.11082867]\n",
      " ...\n",
      " [-0.08504752]\n",
      " [ 0.01653382]\n",
      " [-0.05045396]]\n",
      "t [[ 0.0057417 ]\n",
      " [-0.03535087]\n",
      " [-0.11082867]\n",
      " ...\n",
      " [-0.08504752]\n",
      " [ 0.01653382]\n",
      " [-0.05045396]]\n",
      "t [[ 0.00836757]\n",
      " [-0.07547154]\n",
      " [-0.20077411]\n",
      " ...\n",
      " [-0.15270985]\n",
      " [ 0.02708015]\n",
      " [-0.09613215]]\n",
      "t [[ 0.00836757]\n",
      " [-0.07547154]\n",
      " [-0.20077411]\n",
      " ...\n",
      " [-0.15270985]\n",
      " [ 0.02708015]\n",
      " [-0.09613215]]\n",
      "Current iteration=2, loss=38769.15560986976\n",
      "t [[ 0.00877305]\n",
      " [-0.11858437]\n",
      " [-0.27492806]\n",
      " ...\n",
      " [-0.20729783]\n",
      " [ 0.03298515]\n",
      " [-0.137792  ]]\n",
      "t [[ 0.00877305]\n",
      " [-0.11858437]\n",
      " [-0.27492806]\n",
      " ...\n",
      " [-0.20729783]\n",
      " [ 0.03298515]\n",
      " [-0.137792  ]]\n",
      "t [[ 0.0076262 ]\n",
      " [-0.16338656]\n",
      " [-0.33713841]\n",
      " ...\n",
      " [-0.25206838]\n",
      " [ 0.03528988]\n",
      " [-0.17604158]]\n",
      "t [[ 0.0076262 ]\n",
      " [-0.16338656]\n",
      " [-0.33713841]\n",
      " ...\n",
      " [-0.25206838]\n",
      " [ 0.03528988]\n",
      " [-0.17604158]]\n",
      "Current iteration=4, loss=37785.72020890881\n",
      "t [[ 0.00541072]\n",
      " [-0.20895076]\n",
      " [-0.39025022]\n",
      " ...\n",
      " [-0.28942973]\n",
      " [ 0.03477988]\n",
      " [-0.21136158]]\n",
      "t [[ 0.00541072]\n",
      " [-0.20895076]\n",
      " [-0.39025022]\n",
      " ...\n",
      " [-0.28942973]\n",
      " [ 0.03477988]\n",
      " [-0.21136158]]\n",
      "t [[ 0.00247436]\n",
      " [-0.2546248 ]\n",
      " [-0.43636252]\n",
      " ...\n",
      " [-0.32115791]\n",
      " [ 0.03204845]\n",
      " [-0.24413387]]\n",
      "t [[ 0.00247436]\n",
      " [-0.2546248 ]\n",
      " [-0.43636252]\n",
      " ...\n",
      " [-0.32115791]\n",
      " [ 0.03204845]\n",
      " [-0.24413387]]\n",
      "Current iteration=6, loss=37074.76103816781\n",
      "t [[-0.00093252]\n",
      " [-0.29995348]\n",
      " [-0.47703239]\n",
      " ...\n",
      " [-0.34856872]\n",
      " [ 0.0275488 ]\n",
      " [-0.27466556]]\n",
      "t [[-0.00093252]\n",
      " [-0.29995348]\n",
      " [-0.47703239]\n",
      " ...\n",
      " [-0.34856872]\n",
      " [ 0.0275488 ]\n",
      " [-0.27466556]]\n",
      "t [[-0.00462922]\n",
      " [-0.34462214]\n",
      " [-0.51342381]\n",
      " ...\n",
      " [-0.37264351]\n",
      " [ 0.02163216]\n",
      " [-0.30320734]]\n",
      "t [[-0.00462922]\n",
      " [-0.34462214]\n",
      " [-0.51342381]\n",
      " ...\n",
      " [-0.37264351]\n",
      " [ 0.02163216]\n",
      " [-0.30320734]]\n",
      "Current iteration=8, loss=36522.87340171368\n",
      "t [[-0.00848506]\n",
      " [-0.38841679]\n",
      " [-0.54641382]\n",
      " ...\n",
      " [-0.39411909]\n",
      " [ 0.01457474]\n",
      " [-0.32996685]]\n",
      "t [[-0.00848506]\n",
      " [-0.38841679]\n",
      " [-0.54641382]\n",
      " ...\n",
      " [-0.39411909]\n",
      " [ 0.01457474]\n",
      " [-0.32996685]]\n",
      "t [[-0.01240567]\n",
      " [-0.43119592]\n",
      " [-0.57666814]\n",
      " ...\n",
      " [-0.41355207]\n",
      " [ 0.00659681]\n",
      " [-0.35511843]]\n",
      "loss=36077.31502401449\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00490405]\n",
      " [-0.03102604]\n",
      " [-0.11202898]\n",
      " ...\n",
      " [-0.08362525]\n",
      " [ 0.01777708]\n",
      " [-0.04732603]]\n",
      "t [[ 0.00490405]\n",
      " [-0.03102604]\n",
      " [-0.11202898]\n",
      " ...\n",
      " [-0.08362525]\n",
      " [ 0.01777708]\n",
      " [-0.04732603]]\n",
      "t [[ 0.00672282]\n",
      " [-0.06704389]\n",
      " [-0.203217  ]\n",
      " ...\n",
      " [-0.1501018 ]\n",
      " [ 0.02950759]\n",
      " [-0.09003708]]\n",
      "t [[ 0.00672282]\n",
      " [-0.06704389]\n",
      " [-0.203217  ]\n",
      " ...\n",
      " [-0.1501018 ]\n",
      " [ 0.02950759]\n",
      " [-0.09003708]]\n",
      "Current iteration=2, loss=38786.02500345\n",
      "t [[ 0.00635659]\n",
      " [-0.10627016]\n",
      " [-0.27860676]\n",
      " ...\n",
      " [-0.20367299]\n",
      " [ 0.0365328 ]\n",
      " [-0.12887509]]\n",
      "t [[ 0.00635659]\n",
      " [-0.10627016]\n",
      " [-0.27860676]\n",
      " ...\n",
      " [-0.20367299]\n",
      " [ 0.0365328 ]\n",
      " [-0.12887509]]\n",
      "t [[ 0.00447672]\n",
      " [-0.14739188]\n",
      " [-0.34201705]\n",
      " ...\n",
      " [-0.24755021]\n",
      " [ 0.03989161]\n",
      " [-0.16443524]]\n",
      "t [[ 0.00447672]\n",
      " [-0.14739188]\n",
      " [-0.34201705]\n",
      " ...\n",
      " [-0.24755021]\n",
      " [ 0.03989161]\n",
      " [-0.16443524]]\n",
      "Current iteration=4, loss=37815.63204300476\n",
      "t [[ 0.0015691 ]\n",
      " [-0.18946898]\n",
      " [-0.3962779 ]\n",
      " ...\n",
      " [-0.28411191]\n",
      " [ 0.04036976]\n",
      " [-0.19718734]]\n",
      "t [[ 0.0015691 ]\n",
      " [-0.18946898]\n",
      " [-0.3962779 ]\n",
      " ...\n",
      " [-0.28411191]\n",
      " [ 0.04036976]\n",
      " [-0.19718734]]\n",
      "t [[-0.00201725]\n",
      " [-0.23183586]\n",
      " [-0.44348176]\n",
      " ...\n",
      " [-0.31511444]\n",
      " [ 0.03856217]\n",
      " [-0.22750392]]\n",
      "t [[-0.00201725]\n",
      " [-0.23183586]\n",
      " [-0.44348176]\n",
      " ...\n",
      " [-0.31511444]\n",
      " [ 0.03856217]\n",
      " [-0.22750392]]\n",
      "Current iteration=6, loss=37115.81706691708\n",
      "t [[-0.00603152]\n",
      " [-0.27402428]\n",
      " [-0.48518397]\n",
      " ...\n",
      " [-0.34186035]\n",
      " [ 0.03492436]\n",
      " [-0.25568405]]\n",
      "t [[-0.00603152]\n",
      " [-0.27402428]\n",
      " [-0.48518397]\n",
      " ...\n",
      " [-0.34186035]\n",
      " [ 0.03492436]\n",
      " [-0.25568405]]\n",
      "t [[-0.01029318]\n",
      " [-0.31570736]\n",
      " [-0.52254942]\n",
      " ...\n",
      " [-0.36532177]\n",
      " [ 0.02981023]\n",
      " [-0.2819714 ]]\n",
      "t [[-0.01029318]\n",
      " [-0.31570736]\n",
      " [-0.52254942]\n",
      " ...\n",
      " [-0.36532177]\n",
      " [ 0.02981023]\n",
      " [-0.2819714 ]]\n",
      "Current iteration=8, loss=36573.5526820923\n",
      "t [[-0.01467226]\n",
      " [-0.35665988]\n",
      " [-0.5564575 ]\n",
      " ...\n",
      " [-0.38622902]\n",
      " [ 0.02349883]\n",
      " [-0.30656741]]\n",
      "t [[-0.01467226]\n",
      " [-0.35665988]\n",
      " [-0.5564575 ]\n",
      " ...\n",
      " [-0.38622902]\n",
      " [ 0.02349883]\n",
      " [-0.30656741]]\n",
      "t [[-0.01907547]\n",
      " [-0.39673006]\n",
      " [-0.58757693]\n",
      " ...\n",
      " [-0.40513388]\n",
      " [ 0.01621328]\n",
      " [-0.32964087]]\n",
      "loss=36136.265219430155\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00408408]\n",
      " [-0.03286193]\n",
      " [-0.11128978]\n",
      " ...\n",
      " [-0.04899803]\n",
      " [-0.0059012 ]\n",
      " [-0.09996392]]\n",
      "t [[ 0.00408408]\n",
      " [-0.03286193]\n",
      " [-0.11128978]\n",
      " ...\n",
      " [-0.04899803]\n",
      " [-0.0059012 ]\n",
      " [-0.09996392]]\n",
      "t [[ 0.00514384]\n",
      " [-0.07068079]\n",
      " [-0.20156565]\n",
      " ...\n",
      " [-0.08452843]\n",
      " [-0.00734849]\n",
      " [-0.19208292]]\n",
      "t [[ 0.00514384]\n",
      " [-0.07068079]\n",
      " [-0.20156565]\n",
      " ...\n",
      " [-0.08452843]\n",
      " [-0.00734849]\n",
      " [-0.19208292]]\n",
      "Current iteration=2, loss=38759.49275226308\n",
      "t [[ 0.00407544]\n",
      " [-0.11166163]\n",
      " [-0.27593719]\n",
      " ...\n",
      " [-0.11017766]\n",
      " [-0.00578982]\n",
      " [-0.2774543 ]]\n",
      "t [[ 0.00407544]\n",
      " [-0.11166163]\n",
      " [-0.27593719]\n",
      " ...\n",
      " [-0.11017766]\n",
      " [-0.00578982]\n",
      " [-0.2774543 ]]\n",
      "t [[ 0.00154569]\n",
      " [-0.1544848 ]\n",
      " [-0.33827074]\n",
      " ...\n",
      " [-0.12863305]\n",
      " [-0.00228293]\n",
      " [-0.35698615]]\n",
      "t [[ 0.00154569]\n",
      " [-0.1544848 ]\n",
      " [-0.33827074]\n",
      " ...\n",
      " [-0.12863305]\n",
      " [-0.00228293]\n",
      " [-0.35698615]]\n",
      "Current iteration=4, loss=37772.84364409282\n",
      "t [[-0.00196405]\n",
      " [-0.19820746]\n",
      " [-0.39142854]\n",
      " ...\n",
      " [-0.14186293]\n",
      " [ 0.00241868]\n",
      " [-0.43141933]]\n",
      "t [[-0.00196405]\n",
      " [-0.19820746]\n",
      " [-0.39142854]\n",
      " ...\n",
      " [-0.14186293]\n",
      " [ 0.00241868]\n",
      " [-0.43141933]]\n",
      "t [[-0.00610909]\n",
      " [-0.24216354]\n",
      " [-0.43752458]\n",
      " ...\n",
      " [-0.15130264]\n",
      " [ 0.00778406]\n",
      " [-0.50135944]]\n",
      "t [[-0.00610909]\n",
      " [-0.24216354]\n",
      " [-0.43752458]\n",
      " ...\n",
      " [-0.15130264]\n",
      " [ 0.00778406]\n",
      " [-0.50135944]]\n",
      "Current iteration=6, loss=37061.03006522458\n",
      "t [[-0.01064269]\n",
      " [-0.28588554]\n",
      " [-0.4781285 ]\n",
      " ...\n",
      " [-0.15800124]\n",
      " [ 0.01344199]\n",
      " [-0.56730577]]\n",
      "t [[-0.01064269]\n",
      " [-0.28588554]\n",
      " [-0.4781285 ]\n",
      " ...\n",
      " [-0.15800124]\n",
      " [ 0.01344199]\n",
      " [-0.56730577]]\n",
      "t [[-0.01538805]\n",
      " [-0.32904798]\n",
      " [-0.51441466]\n",
      " ...\n",
      " [-0.16272844]\n",
      " [ 0.01913548]\n",
      " [-0.6296744 ]]\n",
      "t [[-0.01538805]\n",
      " [-0.32904798]\n",
      " [-0.51441466]\n",
      " ...\n",
      " [-0.16272844]\n",
      " [ 0.01913548]\n",
      " [-0.6296744 ]]\n",
      "Current iteration=8, loss=36509.06311508655\n",
      "t [[-0.02021859]\n",
      " [-0.37142732]\n",
      " [-0.54726866]\n",
      " ...\n",
      " [-0.16605085]\n",
      " [ 0.02468956]\n",
      " [-0.68881593]]\n",
      "t [[-0.02021859]\n",
      " [-0.37142732]\n",
      " [-0.54726866]\n",
      " ...\n",
      " [-0.16605085]\n",
      " [ 0.02468956]\n",
      " [-0.68881593]]\n",
      "t [[-0.02504409]\n",
      " [-0.41287361]\n",
      " [-0.57736323]\n",
      " ...\n",
      " [-0.16838634]\n",
      " [ 0.0299882 ]\n",
      " [-0.7450289 ]]\n",
      "loss=36063.62374955288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.04626062]\n",
      " [-0.19762004]\n",
      " [-0.12421131]\n",
      " ...\n",
      " [-0.09002241]\n",
      " [ 0.01949035]\n",
      " [-0.05247833]]\n",
      "t [[ 0.04626062]\n",
      " [-0.19762004]\n",
      " [-0.12421131]\n",
      " ...\n",
      " [-0.09002241]\n",
      " [ 0.01949035]\n",
      " [-0.05247833]]\n",
      "t [[ 0.08234684]\n",
      " [-0.36030731]\n",
      " [-0.22546034]\n",
      " ...\n",
      " [-0.16083973]\n",
      " [ 0.03206164]\n",
      " [-0.09968627]]\n",
      "t [[ 0.08234684]\n",
      " [-0.36030731]\n",
      " [-0.22546034]\n",
      " ...\n",
      " [-0.16083973]\n",
      " [ 0.03206164]\n",
      " [-0.09968627]]\n",
      "Current iteration=2, loss=38709.81786626164\n",
      "t [[ 0.11065167]\n",
      " [-0.49627407]\n",
      " [-0.30945027]\n",
      " ...\n",
      " [-0.21746228]\n",
      " [ 0.03932384]\n",
      " [-0.14250552]]\n",
      "t [[ 0.11065167]\n",
      " [-0.49627407]\n",
      " [-0.30945027]\n",
      " ...\n",
      " [-0.21746228]\n",
      " [ 0.03932384]\n",
      " [-0.14250552]]\n",
      "t [[ 0.13299806]\n",
      " [-0.6116952 ]\n",
      " [-0.38042649]\n",
      " ...\n",
      " [-0.26360339]\n",
      " [ 0.04250319]\n",
      " [-0.18163315]]\n",
      "t [[ 0.13299806]\n",
      " [-0.6116952 ]\n",
      " [-0.38042649]\n",
      " ...\n",
      " [-0.26360339]\n",
      " [ 0.04250319]\n",
      " [-0.18163315]]\n",
      "Current iteration=4, loss=37712.464211257\n",
      "t [[ 0.1507482 ]\n",
      " [-0.71112037]\n",
      " [-0.44148154]\n",
      " ...\n",
      " [-0.30195647]\n",
      " [ 0.04251195]\n",
      " [-0.21761243]]\n",
      "t [[ 0.1507482 ]\n",
      " [-0.71112037]\n",
      " [-0.44148154]\n",
      " ...\n",
      " [-0.30195647]\n",
      " [ 0.04251195]\n",
      " [-0.21761243]]\n",
      "t [[ 0.1649207 ]\n",
      " [-0.79790873]\n",
      " [-0.49486908]\n",
      " ...\n",
      " [-0.33447274]\n",
      " [ 0.04003155]\n",
      " [-0.25086978]]\n",
      "t [[ 0.1649207 ]\n",
      " [-0.79790873]\n",
      " [-0.49486908]\n",
      " ...\n",
      " [-0.33447274]\n",
      " [ 0.04003155]\n",
      " [-0.25086978]]\n",
      "Current iteration=6, loss=37000.27454409757\n",
      "t [[ 0.17628328]\n",
      " [-0.87456607]\n",
      " [-0.54224509]\n",
      " ...\n",
      " [-0.36257408]\n",
      " [ 0.03557828]\n",
      " [-0.28174497]]\n",
      "t [[ 0.17628328]\n",
      " [-0.87456607]\n",
      " [-0.54224509]\n",
      " ...\n",
      " [-0.36257408]\n",
      " [ 0.03557828]\n",
      " [-0.28174497]]\n",
      "t [[ 0.18542028]\n",
      " [-0.94298543]\n",
      " [-0.58483906]\n",
      " ...\n",
      " [-0.38730452]\n",
      " [ 0.02954992]\n",
      " [-0.31051324]]\n",
      "t [[ 0.18542028]\n",
      " [-0.94298543]\n",
      " [-0.58483906]\n",
      " ...\n",
      " [-0.38730452]\n",
      " [ 0.02954992]\n",
      " [-0.31051324]]\n",
      "Current iteration=8, loss=36452.083881235674\n",
      "t [[ 0.19278091]\n",
      " [-1.00461546]\n",
      " [-0.62357343]\n",
      " ...\n",
      " [-0.40943619]\n",
      " [ 0.02225802]\n",
      " [-0.33740124]]\n",
      "t [[ 0.19278091]\n",
      " [-1.00461546]\n",
      " [-0.62357343]\n",
      " ...\n",
      " [-0.40943619]\n",
      " [ 0.02225802]\n",
      " [-0.33740124]]\n",
      "t [[ 0.19871372]\n",
      " [-1.06057854]\n",
      " [-0.65914712]\n",
      " ...\n",
      " [-0.42954383]\n",
      " [ 0.01395026]\n",
      " [-0.36259829]]\n",
      "loss=36012.67177605613\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00610056]\n",
      " [-0.0375603 ]\n",
      " [-0.11775546]\n",
      " ...\n",
      " [-0.09036299]\n",
      " [ 0.01756718]\n",
      " [-0.05360733]]\n",
      "t [[ 0.00610056]\n",
      " [-0.0375603 ]\n",
      " [-0.11775546]\n",
      " ...\n",
      " [-0.09036299]\n",
      " [ 0.01756718]\n",
      " [-0.05360733]]\n",
      "t [[ 0.00868552]\n",
      " [-0.08050254]\n",
      " [-0.21194468]\n",
      " ...\n",
      " [-0.16110706]\n",
      " [ 0.02837819]\n",
      " [-0.10182482]]\n",
      "t [[ 0.00868552]\n",
      " [-0.08050254]\n",
      " [-0.21194468]\n",
      " ...\n",
      " [-0.16110706]\n",
      " [ 0.02837819]\n",
      " [-0.10182482]]\n",
      "Current iteration=2, loss=38691.257821199666\n",
      "t [[ 0.00882939]\n",
      " [-0.12669435]\n",
      " [-0.28867803]\n",
      " ...\n",
      " [-0.21740536]\n",
      " [ 0.03404886]\n",
      " [-0.14556198]]\n",
      "t [[ 0.00882939]\n",
      " [-0.12669435]\n",
      " [-0.28867803]\n",
      " ...\n",
      " [-0.21740536]\n",
      " [ 0.03404886]\n",
      " [-0.14556198]]\n",
      "t [[ 0.00731379]\n",
      " [-0.17461309]\n",
      " [-0.35246565]\n",
      " ...\n",
      " [-0.26307557]\n",
      " [ 0.03580089]\n",
      " [-0.18553528]]\n",
      "t [[ 0.00731379]\n",
      " [-0.17461309]\n",
      " [-0.35246565]\n",
      " ...\n",
      " [-0.26307557]\n",
      " [ 0.03580089]\n",
      " [-0.18553528]]\n",
      "Current iteration=4, loss=37680.83581735801\n",
      "t [[ 0.00469031]\n",
      " [-0.22320404]\n",
      " [-0.40656534]\n",
      " ...\n",
      " [-0.30087545]\n",
      " [ 0.03453637]\n",
      " [-0.22230133]]\n",
      "t [[ 0.00469031]\n",
      " [-0.22320404]\n",
      " [-0.40656534]\n",
      " ...\n",
      " [-0.30087545]\n",
      " [ 0.03453637]\n",
      " [-0.22230133]]\n",
      "t [[ 0.00134615]\n",
      " [-0.27174527]\n",
      " [-0.4533301 ]\n",
      " ...\n",
      " [-0.33279564]\n",
      " [ 0.03092428]\n",
      " [-0.25629594]]\n",
      "t [[ 0.00134615]\n",
      " [-0.27174527]\n",
      " [-0.4533301 ]\n",
      " ...\n",
      " [-0.33279564]\n",
      " [ 0.03092428]\n",
      " [-0.25629594]]\n",
      "Current iteration=6, loss=36957.354081025325\n",
      "t [[-0.00244622]\n",
      " [-0.31974695]\n",
      " [-0.49447195]\n",
      " ...\n",
      " [-0.36028214]\n",
      " [ 0.02546797]\n",
      " [-0.28786573]]\n",
      "t [[-0.00244622]\n",
      " [-0.31974695]\n",
      " [-0.49447195]\n",
      " ...\n",
      " [-0.36028214]\n",
      " [ 0.02546797]\n",
      " [-0.28786573]]\n",
      "t [[-0.00649451]\n",
      " [-0.36688099]\n",
      " [-0.53124802]\n",
      " ...\n",
      " [-0.38439366]\n",
      " [ 0.01855273]\n",
      " [-0.31729117]]\n",
      "t [[-0.00649451]\n",
      " [-0.36688099]\n",
      " [-0.53124802]\n",
      " ...\n",
      " [-0.38439366]\n",
      " [ 0.01855273]\n",
      " [-0.31729117]]\n",
      "Current iteration=8, loss=36399.395049219136\n",
      "t [[-0.01066285]\n",
      " [-0.41293264]\n",
      " [-0.56458971]\n",
      " ...\n",
      " [-0.40591119]\n",
      " [ 0.0104784 ]\n",
      " [-0.34480295]]\n",
      "t [[-0.01066285]\n",
      " [-0.41293264]\n",
      " [-0.56458971]\n",
      " ...\n",
      " [-0.40591119]\n",
      " [ 0.0104784 ]\n",
      " [-0.34480295]]\n",
      "t [[-0.01485555]\n",
      " [-0.45776701]\n",
      " [-0.59519273]\n",
      " ...\n",
      " [-0.42541464]\n",
      " [ 0.0014819 ]\n",
      " [-0.37059369]]\n",
      "loss=35951.70838821185\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00521055]\n",
      " [-0.03296517]\n",
      " [-0.1190308 ]\n",
      " ...\n",
      " [-0.08885183]\n",
      " [ 0.01888815]\n",
      " [-0.05028391]]\n",
      "t [[ 0.00521055]\n",
      " [-0.03296517]\n",
      " [-0.1190308 ]\n",
      " ...\n",
      " [-0.08885183]\n",
      " [ 0.01888815]\n",
      " [-0.05028391]]\n",
      "t [[ 0.00693997]\n",
      " [-0.07156298]\n",
      " [-0.21454286]\n",
      " ...\n",
      " [-0.15835153]\n",
      " [ 0.03095333]\n",
      " [-0.09535946]]\n",
      "t [[ 0.00693997]\n",
      " [-0.07156298]\n",
      " [-0.21454286]\n",
      " ...\n",
      " [-0.15835153]\n",
      " [ 0.03095333]\n",
      " [-0.09535946]]\n",
      "Current iteration=2, loss=38709.04695571012\n",
      "t [[ 0.00626851]\n",
      " [-0.11365405]\n",
      " [-0.29258785]\n",
      " ...\n",
      " [-0.21359136]\n",
      " [ 0.03780553]\n",
      " [-0.13611757]]\n",
      "t [[ 0.00626851]\n",
      " [-0.11365405]\n",
      " [-0.29258785]\n",
      " ...\n",
      " [-0.21359136]\n",
      " [ 0.03780553]\n",
      " [-0.13611757]]\n",
      "t [[ 0.00398168]\n",
      " [-0.15770307]\n",
      " [-0.35764281]\n",
      " ...\n",
      " [-0.25833625]\n",
      " [ 0.04066423]\n",
      " [-0.1732594 ]]\n",
      "t [[ 0.00398168]\n",
      " [-0.15770307]\n",
      " [-0.35764281]\n",
      " ...\n",
      " [-0.25833625]\n",
      " [ 0.04066423]\n",
      " [-0.1732594 ]]\n",
      "Current iteration=4, loss=37712.2635808607\n",
      "t [[ 0.00063357]\n",
      " [-0.20263965]\n",
      " [-0.41294959]\n",
      " ...\n",
      " [-0.29531059]\n",
      " [ 0.04043211]\n",
      " [-0.20732878]]\n",
      "t [[ 0.00063357]\n",
      " [-0.20263965]\n",
      " [-0.41294959]\n",
      " ...\n",
      " [-0.29531059]\n",
      " [ 0.04043211]\n",
      " [-0.20732878]]\n",
      "t [[-0.00338732]\n",
      " [-0.24772571]\n",
      " [-0.46085507]\n",
      " ...\n",
      " [-0.32648357]\n",
      " [ 0.03778036]\n",
      " [-0.23875074]]\n",
      "t [[-0.00338732]\n",
      " [-0.24772571]\n",
      " [-0.46085507]\n",
      " ...\n",
      " [-0.32648357]\n",
      " [ 0.03778036]\n",
      " [-0.23875074]]\n",
      "Current iteration=6, loss=37000.364636122395\n",
      "t [[-0.00780819]\n",
      " [-0.29245598]\n",
      " [-0.50307044]\n",
      " ...\n",
      " [-0.35328704]\n",
      " [ 0.0332153 ]\n",
      " [-0.26786263]]\n",
      "t [[-0.00780819]\n",
      " [-0.29245598]\n",
      " [-0.50307044]\n",
      " ...\n",
      " [-0.35328704]\n",
      " [ 0.0332153 ]\n",
      " [-0.26786263]]\n",
      "t [[-0.01243721]\n",
      " [-0.33648807]\n",
      " [-0.54085478]\n",
      " ...\n",
      " [-0.3767701 ]\n",
      " [ 0.0271255 ]\n",
      " [-0.29493686]]\n",
      "t [[-0.01243721]\n",
      " [-0.33648807]\n",
      " [-0.54085478]\n",
      " ...\n",
      " [-0.3767701 ]\n",
      " [ 0.0271255 ]\n",
      " [-0.29493686]]\n",
      "Current iteration=8, loss=36452.31086024466\n",
      "t [[-0.01713956]\n",
      " [-0.37959423]\n",
      " [-0.57514281]\n",
      " ...\n",
      " [-0.39770699]\n",
      " [ 0.01981426]\n",
      " [-0.32019706]]\n",
      "t [[-0.01713956]\n",
      " [-0.37959423]\n",
      " [-0.57514281]\n",
      " ...\n",
      " [-0.39770699]\n",
      " [ 0.01981426]\n",
      " [-0.32019706]]\n",
      "t [[-0.02182104]\n",
      " [-0.42162782]\n",
      " [-0.60663412]\n",
      " ...\n",
      " [-0.41667267]\n",
      " [ 0.01152197]\n",
      " [-0.34382959]]\n",
      "loss=36013.04496644939\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00433934]\n",
      " [-0.0349158 ]\n",
      " [-0.11824539]\n",
      " ...\n",
      " [-0.05206041]\n",
      " [-0.00627002]\n",
      " [-0.10621166]]\n",
      "t [[ 0.00433934]\n",
      " [-0.0349158 ]\n",
      " [-0.11824539]\n",
      " ...\n",
      " [-0.05206041]\n",
      " [-0.00627002]\n",
      " [-0.10621166]]\n",
      "t [[ 0.00526631]\n",
      " [-0.07542475]\n",
      " [-0.21277702]\n",
      " ...\n",
      " [-0.0889229 ]\n",
      " [-0.00751376]\n",
      " [-0.20356939]]\n",
      "t [[ 0.00526631]\n",
      " [-0.07542475]\n",
      " [-0.21277702]\n",
      " ...\n",
      " [-0.0889229 ]\n",
      " [-0.00751376]\n",
      " [-0.20356939]]\n",
      "Current iteration=2, loss=38681.21512236082\n",
      "t [[ 0.00385626]\n",
      " [-0.11937364]\n",
      " [-0.28972677]\n",
      " ...\n",
      " [-0.11489084]\n",
      " [-0.00546799]\n",
      " [-0.2933915 ]]\n",
      "t [[ 0.00385626]\n",
      " [-0.11937364]\n",
      " [-0.28972677]\n",
      " ...\n",
      " [-0.11489084]\n",
      " [-0.00546799]\n",
      " [-0.2933915 ]]\n",
      "t [[ 0.00088921]\n",
      " [-0.16521971]\n",
      " [-0.35362695]\n",
      " ...\n",
      " [-0.13311202]\n",
      " [-0.00136926]\n",
      " [-0.37675121]]\n",
      "t [[ 0.00088921]\n",
      " [-0.16521971]\n",
      " [-0.35362695]\n",
      " ...\n",
      " [-0.13311202]\n",
      " [-0.00136926]\n",
      " [-0.37675121]]\n",
      "Current iteration=4, loss=37667.7349606106\n",
      "t [[-0.00308615]\n",
      " [-0.21188985]\n",
      " [-0.40775571]\n",
      " ...\n",
      " [-0.14583685]\n",
      " [ 0.00392486]\n",
      " [-0.45451065]]\n",
      "t [[-0.00308615]\n",
      " [-0.21188985]\n",
      " [-0.40775571]\n",
      " ...\n",
      " [-0.14583685]\n",
      " [ 0.00392486]\n",
      " [-0.45451065]]\n",
      "t [[-0.00768648]\n",
      " [-0.25864589]\n",
      " [-0.45448338]\n",
      " ...\n",
      " [-0.15466951]\n",
      " [ 0.00982627]\n",
      " [-0.52736541]]\n",
      "t [[-0.00768648]\n",
      " [-0.25864589]\n",
      " [-0.45448338]\n",
      " ...\n",
      " [-0.15466951]\n",
      " [ 0.00982627]\n",
      " [-0.52736541]]\n",
      "Current iteration=6, loss=36943.557913608005\n",
      "t [[-0.01264376]\n",
      " [-0.30498373]\n",
      " [-0.49553629]\n",
      " ...\n",
      " [-0.16075756]\n",
      " [ 0.01593525]\n",
      " [-0.59588289]]\n",
      "t [[-0.01264376]\n",
      " [-0.30498373]\n",
      " [-0.49553629]\n",
      " ...\n",
      " [-0.16075756]\n",
      " [ 0.01593525]\n",
      " [-0.59588289]]\n",
      "t [[-0.01777051]\n",
      " [-0.35056285]\n",
      " [-0.53218326]\n",
      " ...\n",
      " [-0.16492558]\n",
      " [ 0.02198378]\n",
      " [-0.66053187]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.01777051]\n",
      " [-0.35056285]\n",
      " [-0.53218326]\n",
      " ...\n",
      " [-0.16492558]\n",
      " [ 0.02198378]\n",
      " [-0.66053187]]\n",
      "Current iteration=8, loss=36385.60273334914\n",
      "t [[-0.02293584]\n",
      " [-0.39515761]\n",
      " [-0.56536513]\n",
      " ...\n",
      " [-0.16776801]\n",
      " [ 0.02779619]\n",
      " [-0.72170456]]\n",
      "t [[-0.02293584]\n",
      " [-0.39515761]\n",
      " [-0.56536513]\n",
      " ...\n",
      " [-0.16776801]\n",
      " [ 0.02779619]\n",
      " [-0.72170456]]\n",
      "t [[-0.02804906]\n",
      " [-0.43862362]\n",
      " [-0.59578523]\n",
      " ...\n",
      " [-0.16971386]\n",
      " [ 0.03326179]\n",
      " [-0.77973299]]\n",
      "loss=35938.060930938685\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.04898183]\n",
      " [-0.20924475]\n",
      " [-0.13151786]\n",
      " ...\n",
      " [-0.09531785]\n",
      " [ 0.02063684]\n",
      " [-0.05556529]]\n",
      "t [[ 0.04898183]\n",
      " [-0.20924475]\n",
      " [-0.13151786]\n",
      " ...\n",
      " [-0.09531785]\n",
      " [ 0.02063684]\n",
      " [-0.05556529]]\n",
      "t [[ 0.08656137]\n",
      " [-0.37934139]\n",
      " [-0.23730283]\n",
      " ...\n",
      " [-0.16911307]\n",
      " [ 0.03352034]\n",
      " [-0.10522367]]\n",
      "t [[ 0.08656137]\n",
      " [-0.37934139]\n",
      " [-0.23730283]\n",
      " ...\n",
      " [-0.16911307]\n",
      " [ 0.03352034]\n",
      " [-0.10522367]]\n",
      "Current iteration=2, loss=38634.615024711704\n",
      "t [[ 0.11558098]\n",
      " [-0.52004303]\n",
      " [-0.32412743]\n",
      " ...\n",
      " [-0.2273351 ]\n",
      " [ 0.04056269]\n",
      " [-0.15002272]]\n",
      "t [[ 0.11558098]\n",
      " [-0.52004303]\n",
      " [-0.32412743]\n",
      " ...\n",
      " [-0.2273351 ]\n",
      " [ 0.04056269]\n",
      " [-0.15002272]]\n",
      "t [[ 0.13815951]\n",
      " [-0.63851755]\n",
      " [-0.39691569]\n",
      " ...\n",
      " [-0.27428998]\n",
      " [ 0.04318778]\n",
      " [-0.19077506]]\n",
      "t [[ 0.13815951]\n",
      " [-0.63851755]\n",
      " [-0.39691569]\n",
      " ...\n",
      " [-0.27428998]\n",
      " [ 0.04318778]\n",
      " [-0.19077506]]\n",
      "Current iteration=4, loss=37612.89571889124\n",
      "t [[ 0.1558475 ]\n",
      " [-0.73993197]\n",
      " [-0.45917347]\n",
      " ...\n",
      " [-0.31302916]\n",
      " [ 0.04243286]\n",
      " [-0.22810314]]\n",
      "t [[ 0.1558475 ]\n",
      " [-0.73993197]\n",
      " [-0.45917347]\n",
      " ...\n",
      " [-0.31302916]\n",
      " [ 0.04243286]\n",
      " [-0.22810314]]\n",
      "t [[ 0.16978308]\n",
      " [-0.82802903]\n",
      " [-0.51340405]\n",
      " ...\n",
      " [-0.34571675]\n",
      " [ 0.03905929]\n",
      " [-0.26248886]]\n",
      "t [[ 0.16978308]\n",
      " [-0.82802903]\n",
      " [-0.51340405]\n",
      " ...\n",
      " [-0.34571675]\n",
      " [ 0.03905929]\n",
      " [-0.26248886]]\n",
      "Current iteration=6, loss=36889.90538244331\n",
      "t [[ 0.18080999]\n",
      " [-0.90555495]\n",
      " [-0.56141362]\n",
      " ...\n",
      " [-0.3738993 ]\n",
      " [ 0.03363588]\n",
      " [-0.29431208]]\n",
      "t [[ 0.18080999]\n",
      " [-0.90555495]\n",
      " [-0.56141362]\n",
      " ...\n",
      " [-0.3738993 ]\n",
      " [ 0.03363588]\n",
      " [-0.29431208]]\n",
      "t [[ 0.18956106]\n",
      " [-0.97455475]\n",
      " [-0.60452132]\n",
      " ...\n",
      " [-0.39869167]\n",
      " [ 0.02659586]\n",
      " [-0.32387803]]\n",
      "t [[ 0.18956106]\n",
      " [-0.97455475]\n",
      " [-0.60452132]\n",
      " ...\n",
      " [-0.39869167]\n",
      " [ 0.02659586]\n",
      " [-0.32387803]]\n",
      "Current iteration=8, loss=36336.892924175954\n",
      "t [[ 0.19651619]\n",
      " [-1.0365737 ]\n",
      " [-0.64370187]\n",
      " ...\n",
      " [-0.420904  ]\n",
      " [ 0.01827532]\n",
      " [-0.3514364 ]]\n",
      "t [[ 0.19651619]\n",
      " [-1.0365737 ]\n",
      " [-0.64370187]\n",
      " ...\n",
      " [-0.420904  ]\n",
      " [ 0.01827532]\n",
      " [-0.3514364 ]]\n",
      "t [[ 0.20204314]\n",
      " [-1.09279564]\n",
      " [-0.6796835 ]\n",
      " ...\n",
      " [-0.44112901]\n",
      " [ 0.00893925]\n",
      " [-0.37719476]]\n",
      "loss=35896.28687701702\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00645941]\n",
      " [-0.03976973]\n",
      " [-0.12468226]\n",
      " ...\n",
      " [-0.09567846]\n",
      " [ 0.01860055]\n",
      " [-0.05676071]]\n",
      "t [[ 0.00645941]\n",
      " [-0.03976973]\n",
      " [-0.12468226]\n",
      " ...\n",
      " [-0.09567846]\n",
      " [ 0.01860055]\n",
      " [-0.05676071]]\n",
      "t [[ 0.0089797 ]\n",
      " [-0.08556996]\n",
      " [-0.22295485]\n",
      " ...\n",
      " [-0.16937069]\n",
      " [ 0.0296304 ]\n",
      " [-0.10748066]]\n",
      "t [[ 0.0089797 ]\n",
      " [-0.08556996]\n",
      " [-0.22295485]\n",
      " ...\n",
      " [-0.16937069]\n",
      " [ 0.0296304 ]\n",
      " [-0.10748066]]\n",
      "Current iteration=2, loss=38615.17421180854\n",
      "t [[ 0.0088366 ]\n",
      " [-0.13486975]\n",
      " [-0.3020733 ]\n",
      " ...\n",
      " [-0.22721928]\n",
      " [ 0.03500858]\n",
      " [-0.15324047]]\n",
      "t [[ 0.0088366 ]\n",
      " [-0.13486975]\n",
      " [-0.3020733 ]\n",
      " ...\n",
      " [-0.22721928]\n",
      " [ 0.03500858]\n",
      " [-0.15324047]]\n",
      "t [[ 0.00693432]\n",
      " [-0.18590947]\n",
      " [-0.36726528]\n",
      " ...\n",
      " [-0.27364955]\n",
      " [ 0.03615281]\n",
      " [-0.19487511]]\n",
      "t [[ 0.00693432]\n",
      " [-0.18590947]\n",
      " [-0.36726528]\n",
      " ...\n",
      " [-0.27364955]\n",
      " [ 0.03615281]\n",
      " [-0.19487511]]\n",
      "Current iteration=4, loss=37579.83227206428\n",
      "t [[ 0.0038947 ]\n",
      " [-0.23750344]\n",
      " [-0.42221913]\n",
      " ...\n",
      " [-0.31178386]\n",
      " [ 0.03408752]\n",
      " [-0.23302201]]\n",
      "t [[ 0.0038947 ]\n",
      " [-0.23750344]\n",
      " [-0.42221913]\n",
      " ...\n",
      " [-0.31178386]\n",
      " [ 0.03408752]\n",
      " [-0.23302201]]\n",
      "t [[ 1.43865035e-04]\n",
      " [-2.88862346e-01]\n",
      " [-4.69542883e-01]\n",
      " ...\n",
      " [-3.43828180e-01]\n",
      " [ 2.95589461e-02]\n",
      " [-2.68173434e-01]]\n",
      "t [[ 1.43865035e-04]\n",
      " [-2.88862346e-01]\n",
      " [-4.69542883e-01]\n",
      " ...\n",
      " [-3.43828180e-01]\n",
      " [ 2.95589461e-02]\n",
      " [-2.68173434e-01]]\n",
      "Current iteration=6, loss=36845.11117539753\n",
      "t [[-0.00402516]\n",
      " [-0.33946611]\n",
      " [-0.51109795]\n",
      " ...\n",
      " [-0.37135374]\n",
      " [ 0.0231205 ]\n",
      " [-0.30071673]]\n",
      "t [[-0.00402516]\n",
      " [-0.33946611]\n",
      " [-0.51109795]\n",
      " ...\n",
      " [-0.37135374]\n",
      " [ 0.0231205 ]\n",
      " [-0.30071673]]\n",
      "t [[-0.00841026]\n",
      " [-0.38897802]\n",
      " [-0.54822707]\n",
      " ...\n",
      " [-0.39548986]\n",
      " [ 0.0151908 ]\n",
      " [-0.33096274]]\n",
      "t [[-0.00841026]\n",
      " [-0.38897802]\n",
      " [-0.54822707]\n",
      " ...\n",
      " [-0.39548986]\n",
      " [ 0.0151908 ]\n",
      " [-0.33096274]]\n",
      "Current iteration=8, loss=36282.07473311515\n",
      "t [[-0.01287208]\n",
      " [-0.43718705]\n",
      " [-0.58190848]\n",
      " ...\n",
      " [-0.41705495]\n",
      " [ 0.00609241]\n",
      " [-0.35916555]]\n",
      "t [[-0.01287208]\n",
      " [-0.43718705]\n",
      " [-0.58190848]\n",
      " ...\n",
      " [-0.41705495]\n",
      " [ 0.00609241]\n",
      " [-0.35916555]]\n",
      "t [[-0.01731507]\n",
      " [-0.48396881]\n",
      " [-0.61286122]\n",
      " ...\n",
      " [-0.43664627]\n",
      " [-0.003922  ]\n",
      " [-0.38553627]]\n",
      "loss=35833.097630057375\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00551705]\n",
      " [-0.0349043 ]\n",
      " [-0.12603261]\n",
      " ...\n",
      " [-0.09407841]\n",
      " [ 0.01999922]\n",
      " [-0.05324179]]\n",
      "t [[ 0.00551705]\n",
      " [-0.0349043 ]\n",
      " [-0.12603261]\n",
      " ...\n",
      " [-0.09407841]\n",
      " [ 0.01999922]\n",
      " [-0.05324179]]\n",
      "t [[ 0.00713358]\n",
      " [-0.07612025]\n",
      " [-0.22570859]\n",
      " ...\n",
      " [-0.16646949]\n",
      " [ 0.03235276]\n",
      " [-0.10064627]]\n",
      "t [[ 0.00713358]\n",
      " [-0.07612025]\n",
      " [-0.22570859]\n",
      " ...\n",
      " [-0.16646949]\n",
      " [ 0.03235276]\n",
      " [-0.10064627]]\n",
      "Current iteration=2, loss=38633.87108207015\n",
      "t [[ 0.0061321 ]\n",
      " [-0.12110846]\n",
      " [-0.30621391]\n",
      " ...\n",
      " [-0.22321986]\n",
      " [ 0.03897272]\n",
      " [-0.1432719 ]]\n",
      "t [[ 0.0061321 ]\n",
      " [-0.12110846]\n",
      " [-0.30621391]\n",
      " ...\n",
      " [-0.22321986]\n",
      " [ 0.03897272]\n",
      " [-0.1432719 ]]\n",
      "t [[ 0.00342136]\n",
      " [-0.16809362]\n",
      " [-0.37273907]\n",
      " ...\n",
      " [-0.26869452]\n",
      " [ 0.0412746 ]\n",
      " [-0.18193572]]\n",
      "t [[ 0.00342136]\n",
      " [-0.16809362]\n",
      " [-0.37273907]\n",
      " ...\n",
      " [-0.26869452]\n",
      " [ 0.0412746 ]\n",
      " [-0.18193572]]\n",
      "Current iteration=4, loss=37612.75006772074\n",
      "t [[-3.74021774e-04]\n",
      " [-2.15871130e-01]\n",
      " [-4.28955859e-01]\n",
      " ...\n",
      " [-3.05978822e-01]\n",
      " [ 4.02839598e-02]\n",
      " [-2.17260339e-01]]\n",
      "t [[-3.74021774e-04]\n",
      " [-2.15871130e-01]\n",
      " [-4.28955859e-01]\n",
      " ...\n",
      " [-3.05978822e-01]\n",
      " [ 4.02839598e-02]\n",
      " [-2.17260339e-01]]\n",
      "t [[-0.00482661]\n",
      " [-0.2636325 ]\n",
      " [-0.47746687]\n",
      " ...\n",
      " [-0.33725563]\n",
      " [ 0.03674993]\n",
      " [-0.24972563]]\n",
      "t [[-0.00482661]\n",
      " [-0.2636325 ]\n",
      " [-0.47746687]\n",
      " ...\n",
      " [-0.33725563]\n",
      " [ 0.03674993]\n",
      " [-0.24972563]]\n",
      "Current iteration=6, loss=36890.027413510456\n",
      "t [[-0.00964324]\n",
      " [-0.3108396 ]\n",
      " [-0.52013385]\n",
      " ...\n",
      " [-0.36408127]\n",
      " [ 0.03122962]\n",
      " [-0.27970838]]\n",
      "t [[-0.00964324]\n",
      " [-0.3108396 ]\n",
      " [-0.52013385]\n",
      " ...\n",
      " [-0.36408127]\n",
      " [ 0.03122962]\n",
      " [-0.27970838]]\n",
      "t [[-0.0146226 ]\n",
      " [-0.35713922]\n",
      " [-0.55830265]\n",
      " ...\n",
      " [-0.38757509]\n",
      " [ 0.02414564]\n",
      " [-0.30751034]]\n",
      "t [[-0.0146226 ]\n",
      " [-0.35713922]\n",
      " [-0.55830265]\n",
      " ...\n",
      " [-0.38757509]\n",
      " [ 0.02414564]\n",
      " [-0.30751034]]\n",
      "Current iteration=8, loss=36337.14830634132\n",
      "t [[-0.01962681]\n",
      " [-0.40230546]\n",
      " [-0.5929559 ]\n",
      " ...\n",
      " [-0.40854857]\n",
      " [ 0.01582467]\n",
      " [-0.33337759]]\n",
      "t [[-0.01962681]\n",
      " [-0.40230546]\n",
      " [-0.5929559 ]\n",
      " ...\n",
      " [-0.40854857]\n",
      " [ 0.01582467]\n",
      " [-0.33337759]]\n",
      "t [[-0.02456231]\n",
      " [-0.44620063]\n",
      " [-0.62481747]\n",
      " ...\n",
      " [-0.42759389]\n",
      " [ 0.00652348]\n",
      " [-0.35751417]]\n",
      "loss=35896.71153066836\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00459459]\n",
      " [-0.03696967]\n",
      " [-0.12520101]\n",
      " ...\n",
      " [-0.05512279]\n",
      " [-0.00663885]\n",
      " [-0.11245941]]\n",
      "t [[ 0.00459459]\n",
      " [-0.03696967]\n",
      " [-0.12520101]\n",
      " ...\n",
      " [-0.05512279]\n",
      " [-0.00663885]\n",
      " [-0.11245941]]\n",
      "t [[ 0.00536571]\n",
      " [-0.08020658]\n",
      " [-0.22382698]\n",
      " ...\n",
      " [-0.09321392]\n",
      " [-0.00764478]\n",
      " [-0.21499527]]\n",
      "t [[ 0.00536571]\n",
      " [-0.08020658]\n",
      " [-0.22382698]\n",
      " ...\n",
      " [-0.09321392]\n",
      " [-0.00764478]\n",
      " [-0.21499527]]\n",
      "Current iteration=2, loss=38604.77327007189\n",
      "t [[ 0.00359005]\n",
      " [-0.12715499]\n",
      " [-0.30315909]\n",
      " ...\n",
      " [-0.11938265]\n",
      " [-0.00507916]\n",
      " [-0.30917475]]\n",
      "t [[ 0.00359005]\n",
      " [-0.12715499]\n",
      " [-0.30315909]\n",
      " ...\n",
      " [-0.11938265]\n",
      " [-0.00507916]\n",
      " [-0.30917475]]\n",
      "t [[ 1.69841240e-04]\n",
      " [-1.76031423e-01]\n",
      " [-3.68451372e-01]\n",
      " ...\n",
      " [-1.37275123e-01]\n",
      " [-3.71467787e-04]\n",
      " [-3.96252390e-01]]\n",
      "t [[ 1.69841240e-04]\n",
      " [-1.76031423e-01]\n",
      " [-3.68451372e-01]\n",
      " ...\n",
      " [-1.37275123e-01]\n",
      " [-3.71467787e-04]\n",
      " [-3.96252390e-01]]\n",
      "Current iteration=4, loss=37566.54162634849\n",
      "t [[-0.00427669]\n",
      " [-0.22562871]\n",
      " [-0.42341594]\n",
      " ...\n",
      " [-0.14943502]\n",
      " [ 0.00551366]\n",
      " [-0.47722052]]\n",
      "t [[-0.00427669]\n",
      " [-0.22562871]\n",
      " [-0.42341594]\n",
      " ...\n",
      " [-0.14943502]\n",
      " [ 0.00551366]\n",
      " [-0.47722052]]\n",
      "t [[-0.00932817]\n",
      " [-0.27513869]\n",
      " [-0.47068067]\n",
      " ...\n",
      " [-0.15763462]\n",
      " [ 0.01193279]\n",
      " [-0.55286967]]\n",
      "t [[-0.00932817]\n",
      " [-0.27513869]\n",
      " [-0.47068067]\n",
      " ...\n",
      " [-0.15763462]\n",
      " [ 0.01193279]\n",
      " [-0.55286967]]\n",
      "Current iteration=6, loss=36831.27490320828\n",
      "t [[-0.014697  ]\n",
      " [-0.32402498]\n",
      " [-0.51212314]\n",
      " ...\n",
      " [-0.16311461]\n",
      " [ 0.01846138]\n",
      " [-0.62383813]]\n",
      "t [[-0.014697  ]\n",
      " [-0.32402498]\n",
      " [-0.51212314]\n",
      " ...\n",
      " [-0.16311461]\n",
      " [ 0.01846138]\n",
      " [-0.62383813]]\n",
      "t [[-0.02018689]\n",
      " [-0.37193669]\n",
      " [-0.54909902]\n",
      " ...\n",
      " [-0.16674782]\n",
      " [ 0.0248243 ]\n",
      " [-0.690649  ]]\n",
      "t [[-0.02018689]\n",
      " [-0.37193669]\n",
      " [-0.54909902]\n",
      " ...\n",
      " [-0.16674782]\n",
      " [ 0.0248243 ]\n",
      " [-0.690649  ]]\n",
      "Current iteration=8, loss=36268.31110129019\n",
      "t [[-0.02566444]\n",
      " [-0.41865055]\n",
      " [-0.58259687]\n",
      " ...\n",
      " [-0.16915002]\n",
      " [ 0.03084892]\n",
      " [-0.75373702]]\n",
      "t [[-0.02566444]\n",
      " [-0.41865055]\n",
      " [-0.58259687]\n",
      " ...\n",
      " [-0.16915002]\n",
      " [ 0.03084892]\n",
      " [-0.75373702]]\n",
      "t [[-0.03104003]\n",
      " [-0.46403154]\n",
      " [-0.61334395]\n",
      " ...\n",
      " [-0.17075571]\n",
      " [ 0.036433  ]\n",
      " [-0.81346827]]\n",
      "loss=35819.493506258834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05170304]\n",
      " [-0.22086946]\n",
      " [-0.13882441]\n",
      " ...\n",
      " [-0.10061328]\n",
      " [ 0.02178333]\n",
      " [-0.05865225]]\n",
      "t [[ 0.05170304]\n",
      " [-0.22086946]\n",
      " [-0.13882441]\n",
      " ...\n",
      " [-0.10061328]\n",
      " [ 0.02178333]\n",
      " [-0.05865225]]\n",
      "t [[ 0.09070673]\n",
      " [-0.39813814]\n",
      " [-0.24898938]\n",
      " ...\n",
      " [-0.17725593]\n",
      " [ 0.03493223]\n",
      " [-0.11072511]]\n",
      "t [[ 0.09070673]\n",
      " [-0.39813814]\n",
      " [-0.24898938]\n",
      " ...\n",
      " [-0.17725593]\n",
      " [ 0.03493223]\n",
      " [-0.11072511]]\n",
      "Current iteration=2, loss=38561.155219209715\n",
      "t [[ 0.12035538]\n",
      " [-0.54327988]\n",
      " [-0.33846147]\n",
      " ...\n",
      " [-0.23692598]\n",
      " [ 0.04169627]\n",
      " [-0.15745151]]\n",
      "t [[ 0.12035538]\n",
      " [-0.54327988]\n",
      " [-0.33846147]\n",
      " ...\n",
      " [-0.23692598]\n",
      " [ 0.04169627]\n",
      " [-0.15745151]]\n",
      "t [[ 0.14308741]\n",
      " [-0.66453265]\n",
      " [-0.41289503]\n",
      " ...\n",
      " [-0.28456713]\n",
      " [ 0.04371247]\n",
      " [-0.19976947]]\n",
      "t [[ 0.14308741]\n",
      " [-0.66453265]\n",
      " [-0.41289503]\n",
      " ...\n",
      " [-0.28456713]\n",
      " [ 0.04371247]\n",
      " [-0.19976947]]\n",
      "Current iteration=4, loss=37516.93770559045\n",
      "t [[ 0.16065015]\n",
      " [-0.76770583]\n",
      " [-0.47622439]\n",
      " ...\n",
      " [-0.32360144]\n",
      " [ 0.04214847]\n",
      " [-0.23838527]]\n",
      "t [[ 0.16065015]\n",
      " [-0.76770583]\n",
      " [-0.47622439]\n",
      " ...\n",
      " [-0.32360144]\n",
      " [ 0.04214847]\n",
      " [-0.23838527]]\n",
      "t [[ 0.1743027 ]\n",
      " [-0.85692869]\n",
      " [-0.53120232]\n",
      " ...\n",
      " [-0.35640478]\n",
      " [ 0.03784694]\n",
      " [-0.27383844]]\n",
      "t [[ 0.1743027 ]\n",
      " [-0.85692869]\n",
      " [-0.53120232]\n",
      " ...\n",
      " [-0.35640478]\n",
      " [ 0.03784694]\n",
      " [-0.27383844]]\n",
      "Current iteration=6, loss=36784.27246475859\n",
      "t [[ 0.1849635 ]\n",
      " [-0.93518238]\n",
      " [-0.57977958]\n",
      " ...\n",
      " [-0.38464221]\n",
      " [ 0.03142894]\n",
      " [-0.30655   ]]\n",
      "t [[ 0.1849635 ]\n",
      " [-0.93518238]\n",
      " [-0.57977958]\n",
      " ...\n",
      " [-0.38464221]\n",
      " [ 0.03142894]\n",
      " [-0.30655   ]]\n",
      "t [[ 0.19331139]\n",
      " [-1.00465602]\n",
      " [-0.62335778]\n",
      " ...\n",
      " [-0.40949215]\n",
      " [ 0.02336242]\n",
      " [-0.33685564]]\n",
      "t [[ 0.19331139]\n",
      " [-1.00465602]\n",
      " [-0.62335778]\n",
      " ...\n",
      " [-0.40949215]\n",
      " [ 0.02336242]\n",
      " [-0.33685564]]\n",
      "Current iteration=8, loss=36227.308314133596\n",
      "t [[ 0.19985441]\n",
      " [-1.06698351]\n",
      " [-0.66295721]\n",
      " ...\n",
      " [-0.43179585]\n",
      " [ 0.01400706]\n",
      " [-0.36502838]]\n",
      "t [[ 0.19985441]\n",
      " [-1.06698351]\n",
      " [-0.66295721]\n",
      " ...\n",
      " [-0.43179585]\n",
      " [ 0.01400706]\n",
      " [-0.36502838]]\n",
      "t [[ 0.20497713]\n",
      " [-1.12340329]\n",
      " [-0.6993296 ]\n",
      " ...\n",
      " [-0.45215816]\n",
      " [ 0.00364425]\n",
      " [-0.39129416]]\n",
      "loss=35786.22962232554\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00681827]\n",
      " [-0.04197916]\n",
      " [-0.13160905]\n",
      " ...\n",
      " [-0.10099393]\n",
      " [ 0.01963391]\n",
      " [-0.05991408]]\n",
      "t [[ 0.00681827]\n",
      " [-0.04197916]\n",
      " [-0.13160905]\n",
      " ...\n",
      " [-0.10099393]\n",
      " [ 0.01963391]\n",
      " [-0.05991408]]\n",
      "t [[ 0.00925021]\n",
      " [-0.09067369]\n",
      " [-0.23380501]\n",
      " ...\n",
      " [-0.17750107]\n",
      " [ 0.03083694]\n",
      " [-0.11309972]]\n",
      "t [[ 0.00925021]\n",
      " [-0.09067369]\n",
      " [-0.23380501]\n",
      " ...\n",
      " [-0.17750107]\n",
      " [ 0.03083694]\n",
      " [-0.11309972]]\n",
      "Current iteration=2, loss=38540.849093362376\n",
      "t [[ 0.00879611]\n",
      " [-0.14310787]\n",
      " [-0.31512197]\n",
      " ...\n",
      " [-0.23674642]\n",
      " [ 0.03586653]\n",
      " [-0.16082873]]\n",
      "t [[ 0.00879611]\n",
      " [-0.14310787]\n",
      " [-0.31512197]\n",
      " ...\n",
      " [-0.23674642]\n",
      " [ 0.03586653]\n",
      " [-0.16082873]]\n",
      "t [[ 0.0064915 ]\n",
      " [-0.19726861]\n",
      " [-0.38155893]\n",
      " ...\n",
      " [-0.28380862]\n",
      " [ 0.03635151]\n",
      " [-0.2040646 ]]\n",
      "t [[ 0.0064915 ]\n",
      " [-0.19726861]\n",
      " [-0.38155893]\n",
      " ...\n",
      " [-0.28380862]\n",
      " [ 0.03635151]\n",
      " [-0.2040646 ]]\n",
      "Current iteration=4, loss=37482.459693614685\n",
      "t [[ 0.00303018]\n",
      " [-0.25183711]\n",
      " [-0.43724934]\n",
      " ...\n",
      " [-0.3221869 ]\n",
      " [ 0.03344371]\n",
      " [-0.24353028]]\n",
      "t [[ 0.00303018]\n",
      " [-0.25183711]\n",
      " [-0.43724934]\n",
      " ...\n",
      " [-0.3221869 ]\n",
      " [ 0.03344371]\n",
      " [-0.24353028]]\n",
      "t [[-0.0011238 ]\n",
      " [-0.30596002]\n",
      " [-0.48505469]\n",
      " ...\n",
      " [-0.35430107]\n",
      " [ 0.02796766]\n",
      " [-0.27977662]]\n",
      "t [[-0.0011238 ]\n",
      " [-0.30596002]\n",
      " [-0.48505469]\n",
      " ...\n",
      " [-0.35430107]\n",
      " [ 0.02796766]\n",
      " [-0.27977662]]\n",
      "Current iteration=6, loss=36737.64739382295\n",
      "t [[-0.0056587 ]\n",
      " [-0.35909211]\n",
      " [-0.52697858]\n",
      " ...\n",
      " [-0.38184109]\n",
      " [ 0.02052638]\n",
      " [-0.31323273]]\n",
      "t [[-0.0056587 ]\n",
      " [-0.35909211]\n",
      " [-0.52697858]\n",
      " ...\n",
      " [-0.38184109]\n",
      " [ 0.02052638]\n",
      " [-0.31323273]]\n",
      "t [[-0.01036449]\n",
      " [-0.41089319]\n",
      " [-0.56444088]\n",
      " ...\n",
      " [-0.40599932]\n",
      " [ 0.01157089]\n",
      " [-0.34424037]]\n",
      "t [[-0.01036449]\n",
      " [-0.41089319]\n",
      " [-0.56444088]\n",
      " ...\n",
      " [-0.40599932]\n",
      " [ 0.01157089]\n",
      " [-0.34424037]]\n",
      "Current iteration=8, loss=36170.43819852336\n",
      "t [[-0.01510008]\n",
      " [-0.46116055]\n",
      " [-0.59845877]\n",
      " ...\n",
      " [-0.42762456]\n",
      " [ 0.00144545]\n",
      " [-0.37307725]]\n",
      "t [[-0.01510008]\n",
      " [-0.46116055]\n",
      " [-0.59845877]\n",
      " ...\n",
      " [-0.42762456]\n",
      " [ 0.00144545]\n",
      " [-0.37307725]]\n",
      "t [[-0.01977151]\n",
      " [-0.50978419]\n",
      " [-0.62976802]\n",
      " ...\n",
      " [-0.44732546]\n",
      " [-0.00958249]\n",
      " [-0.39997308]]\n",
      "loss=35720.92530677862\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00582355]\n",
      " [-0.03684342]\n",
      " [-0.13303442]\n",
      " ...\n",
      " [-0.09930499]\n",
      " [ 0.02111028]\n",
      " [-0.05619966]]\n",
      "t [[ 0.00582355]\n",
      " [-0.03684342]\n",
      " [-0.13303442]\n",
      " ...\n",
      " [-0.09930499]\n",
      " [ 0.02111028]\n",
      " [-0.05619966]]\n",
      "t [[ 0.00730373]\n",
      " [-0.0807156 ]\n",
      " [-0.23671457]\n",
      " ...\n",
      " [-0.17445597]\n",
      " [ 0.03370601]\n",
      " [-0.10589756]]\n",
      "t [[ 0.00730373]\n",
      " [-0.0807156 ]\n",
      " [-0.23671457]\n",
      " ...\n",
      " [-0.17445597]\n",
      " [ 0.03370601]\n",
      " [-0.10589756]]\n",
      "Current iteration=2, loss=38560.44223238241\n",
      "t [[ 0.00594879]\n",
      " [-0.12863063]\n",
      " [-0.31949296]\n",
      " ...\n",
      " [-0.23256523]\n",
      " [ 0.04003656]\n",
      " [-0.15033933]]\n",
      "t [[ 0.00594879]\n",
      " [-0.12863063]\n",
      " [-0.31949296]\n",
      " ...\n",
      " [-0.23256523]\n",
      " [ 0.04003656]\n",
      " [-0.15033933]]\n",
      "t [[ 0.00279947]\n",
      " [-0.17855634]\n",
      " [-0.38732734]\n",
      " ...\n",
      " [-0.27864307]\n",
      " [ 0.04172859]\n",
      " [-0.19046766]]\n",
      "t [[ 0.00279947]\n",
      " [-0.17855634]\n",
      " [-0.38732734]\n",
      " ...\n",
      " [-0.27864307]\n",
      " [ 0.04172859]\n",
      " [-0.19046766]]\n",
      "Current iteration=4, loss=37516.8426805812\n",
      "t [[-0.00144737]\n",
      " [-0.22915134]\n",
      " [-0.44433429]\n",
      " ...\n",
      " [-0.31614817]\n",
      " [ 0.03993572]\n",
      " [-0.22698848]]\n",
      "t [[-0.00144737]\n",
      " [-0.22915134]\n",
      " [-0.44433429]\n",
      " ...\n",
      " [-0.31614817]\n",
      " [ 0.03993572]\n",
      " [-0.22698848]]\n",
      "t [[-0.0063264 ]\n",
      " [-0.27953979]\n",
      " [-0.49337089]\n",
      " ...\n",
      " [-0.3474757 ]\n",
      " [ 0.03548618]\n",
      " [-0.26043857]]\n",
      "t [[-0.0063264 ]\n",
      " [-0.27953979]\n",
      " [-0.49337089]\n",
      " ...\n",
      " [-0.3474757 ]\n",
      " [ 0.03548618]\n",
      " [-0.26043857]]\n",
      "Current iteration=6, loss=36784.42148996206\n",
      "t [[-0.01152599]\n",
      " [-0.32915561]\n",
      " [-0.53644241]\n",
      " ...\n",
      " [-0.37430005]\n",
      " [ 0.02898747]\n",
      " [-0.2912351 ]]\n",
      "t [[-0.01152599]\n",
      " [-0.32915561]\n",
      " [-0.53644241]\n",
      " ...\n",
      " [-0.37430005]\n",
      " [ 0.02898747]\n",
      " [-0.2912351 ]]\n",
      "t [[-0.01683738]\n",
      " [-0.37763979]\n",
      " [-0.57497313]\n",
      " ...\n",
      " [-0.39780335]\n",
      " [ 0.0208954 ]\n",
      " [-0.3197096 ]]\n",
      "t [[-0.01683738]\n",
      " [-0.37763979]\n",
      " [-0.57497313]\n",
      " ...\n",
      " [-0.39780335]\n",
      " [ 0.0208954 ]\n",
      " [-0.3197096 ]]\n",
      "Current iteration=8, loss=36227.59281367755\n",
      "t [[-0.02212146]\n",
      " [-0.4247728 ]\n",
      " [-0.60998579]\n",
      " ...\n",
      " [-0.41882728]\n",
      " [ 0.01155907]\n",
      " [-0.34613085]]\n",
      "t [[-0.02212146]\n",
      " [-0.4247728 ]\n",
      " [-0.60998579]\n",
      " ...\n",
      " [-0.41882728]\n",
      " [ 0.01155907]\n",
      " [-0.34613085]]\n",
      "t [[-0.0272868 ]\n",
      " [-0.47042967]\n",
      " [-0.64222197]\n",
      " ...\n",
      " [-0.43797533]\n",
      " [ 0.00125073]\n",
      " [-0.37072064]]\n",
      "loss=35786.71172471689\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00484985]\n",
      " [-0.03902354]\n",
      " [-0.13215662]\n",
      " ...\n",
      " [-0.05818516]\n",
      " [-0.00700767]\n",
      " [-0.11870715]]\n",
      "t [[ 0.00484985]\n",
      " [-0.03902354]\n",
      " [-0.13215662]\n",
      " ...\n",
      " [-0.05818516]\n",
      " [-0.00700767]\n",
      " [-0.11870715]]\n",
      "t [[ 0.00544212]\n",
      " [-0.08502617]\n",
      " [-0.23471591]\n",
      " ...\n",
      " [-0.09740175]\n",
      " [-0.00774163]\n",
      " [-0.22636066]]\n",
      "t [[ 0.00544212]\n",
      " [-0.08502617]\n",
      " [-0.23471591]\n",
      " ...\n",
      " [-0.09740175]\n",
      " [-0.00774163]\n",
      " [-0.22636066]]\n",
      "Current iteration=2, loss=38530.11076044866\n",
      "t [[ 0.00327823]\n",
      " [-0.13500291]\n",
      " [-0.31624228]\n",
      " ...\n",
      " [-0.12365871]\n",
      " [-0.0046255 ]\n",
      " [-0.32480594]]\n",
      "t [[ 0.00327823]\n",
      " [-0.13500291]\n",
      " [-0.31624228]\n",
      " ...\n",
      " [-0.12365871]\n",
      " [-0.0046255 ]\n",
      " [-0.32480594]]\n",
      "t [[-0.00060874]\n",
      " [-0.18691273]\n",
      " [-0.38276574]\n",
      " ...\n",
      " [-0.14113734]\n",
      " [ 0.00070468]\n",
      " [-0.41549518]]\n",
      "t [[-0.00060874]\n",
      " [-0.18691273]\n",
      " [-0.38276574]\n",
      " ...\n",
      " [-0.14113734]\n",
      " [ 0.00070468]\n",
      " [-0.41549518]]\n",
      "Current iteration=4, loss=37469.01006154853\n",
      "t [[-0.00552943]\n",
      " [-0.23941192]\n",
      " [-0.4384472 ]\n",
      " ...\n",
      " [-0.15268335]\n",
      " [ 0.00717533]\n",
      " [-0.49955951]]\n",
      "t [[-0.00552943]\n",
      " [-0.23941192]\n",
      " [-0.4384472 ]\n",
      " ...\n",
      " [-0.15268335]\n",
      " [ 0.00717533]\n",
      " [-0.49955951]]\n",
      "t [[-0.01102557]\n",
      " [-0.29162554]\n",
      " [-0.48617069]\n",
      " ...\n",
      " [-0.16023445]\n",
      " [ 0.01409042]\n",
      " [-0.57788891]]\n",
      "t [[-0.01102557]\n",
      " [-0.29162554]\n",
      " [-0.48617069]\n",
      " ...\n",
      " [-0.16023445]\n",
      " [ 0.01409042]\n",
      " [-0.57788891]]\n",
      "Current iteration=6, loss=36723.79123010435\n",
      "t [[-0.01679194]\n",
      " [-0.34298982]\n",
      " [-0.5279578 ]\n",
      " ...\n",
      " [-0.16511784]\n",
      " [ 0.02100474]\n",
      " [-0.65119504]]\n",
      "t [[-0.01679194]\n",
      " [-0.34298982]\n",
      " [-0.5279578 ]\n",
      " ...\n",
      " [-0.16511784]\n",
      " [ 0.02100474]\n",
      " [-0.65119504]]\n",
      "t [[-0.0226255 ]\n",
      " [-0.39314863]\n",
      " [-0.5652426 ]\n",
      " ...\n",
      " [-0.16824722]\n",
      " [ 0.0276404 ]\n",
      " [-0.72005667]]\n",
      "t [[-0.0226255 ]\n",
      " [-0.39314863]\n",
      " [-0.5652426 ]\n",
      " ...\n",
      " [-0.16824722]\n",
      " [ 0.0276404 ]\n",
      " [-0.72005667]]\n",
      "Current iteration=8, loss=36156.71000841391\n",
      "t [[-0.0283922 ]\n",
      " [-0.4418856 ]\n",
      " [-0.59905343]\n",
      " ...\n",
      " [-0.17025299]\n",
      " [ 0.03383153]\n",
      " [-0.78495189]]\n",
      "t [[-0.0283922 ]\n",
      " [-0.4418856 ]\n",
      " [-0.59905343]\n",
      " ...\n",
      " [-0.17025299]\n",
      " [ 0.03383153]\n",
      " [-0.78495189]]\n",
      "t [[-0.034005  ]\n",
      " [-0.48907891]\n",
      " [-0.63013482]\n",
      " ...\n",
      " [-0.17156953]\n",
      " [ 0.03948745]\n",
      " [-0.8462812 ]]\n",
      "loss=35707.36180412342\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05442425]\n",
      " [-0.23249417]\n",
      " [-0.14613095]\n",
      " ...\n",
      " [-0.10590872]\n",
      " [ 0.02292982]\n",
      " [-0.06173921]]\n",
      "t [[ 0.05442425]\n",
      " [-0.23249417]\n",
      " [-0.14613095]\n",
      " ...\n",
      " [-0.10590872]\n",
      " [ 0.02292982]\n",
      " [-0.06173921]]\n",
      "t [[ 0.09478309]\n",
      " [-0.41669814]\n",
      " [-0.26052039]\n",
      " ...\n",
      " [-0.1852686 ]\n",
      " [ 0.03629744]\n",
      " [-0.11619065]]\n",
      "t [[ 0.09478309]\n",
      " [-0.41669814]\n",
      " [-0.26052039]\n",
      " ...\n",
      " [-0.1852686 ]\n",
      " [ 0.03629744]\n",
      " [-0.11619065]]\n",
      "Current iteration=2, loss=38489.38501980485\n",
      "t [[ 0.12497809]\n",
      " [-0.56599561]\n",
      " [-0.35245997]\n",
      " ...\n",
      " [-0.24624148]\n",
      " [ 0.04272677]\n",
      " [-0.1647931 ]]\n",
      "t [[ 0.12497809]\n",
      " [-0.56599561]\n",
      " [-0.35245997]\n",
      " ...\n",
      " [-0.24624148]\n",
      " [ 0.04272677]\n",
      " [-0.1647931 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.14779041]\n",
      " [-0.68976949]\n",
      " [-0.42838432]\n",
      " ...\n",
      " [-0.29445212]\n",
      " [ 0.04408297]\n",
      " [-0.20861975]]\n",
      "t [[ 0.14779041]\n",
      " [-0.68976949]\n",
      " [-0.42838432]\n",
      " ...\n",
      " [-0.29445212]\n",
      " [ 0.04408297]\n",
      " [-0.20861975]]\n",
      "Current iteration=4, loss=37424.362459125834\n",
      "t [[ 0.16517141]\n",
      " [-0.79449246]\n",
      " [-0.49266843]\n",
      " ...\n",
      " [-0.33370297]\n",
      " [ 0.04166882]\n",
      " [-0.24846504]]\n",
      "t [[ 0.16517141]\n",
      " [-0.79449246]\n",
      " [-0.49266843]\n",
      " ...\n",
      " [-0.33370297]\n",
      " [ 0.04166882]\n",
      " [-0.24846504]]\n",
      "t [[ 0.17850169]\n",
      " [-0.88468005]\n",
      " [-0.54831214]\n",
      " ...\n",
      " [-0.36657844]\n",
      " [ 0.03640907]\n",
      " [-0.28492805]]\n",
      "t [[ 0.17850169]\n",
      " [-0.88468005]\n",
      " [-0.54831214]\n",
      " ...\n",
      " [-0.36657844]\n",
      " [ 0.03640907]\n",
      " [-0.28492805]]\n",
      "Current iteration=6, loss=36683.03464580114\n",
      "t [[ 0.18877232]\n",
      " [-0.96354099]\n",
      " [-0.59740368]\n",
      " ...\n",
      " [-0.39485466]\n",
      " [ 0.02897654]\n",
      " [-0.31847179]]\n",
      "t [[ 0.18877232]\n",
      " [-0.96354099]\n",
      " [-0.59740368]\n",
      " ...\n",
      " [-0.39485466]\n",
      " [ 0.02897654]\n",
      " [-0.31847179]]\n",
      "t [[ 0.19670539]\n",
      " [-1.03339948]\n",
      " [-0.64141935]\n",
      " ...\n",
      " [-0.41976565]\n",
      " [ 0.01987287]\n",
      " [-0.34946285]]\n",
      "t [[ 0.19670539]\n",
      " [-1.03339948]\n",
      " [-0.64141935]\n",
      " ...\n",
      " [-0.41976565]\n",
      " [ 0.01987287]\n",
      " [-0.34946285]]\n",
      "Current iteration=8, loss=36122.9137403514\n",
      "t [[ 0.20283429]\n",
      " [-1.09596983]\n",
      " [-0.68141796]\n",
      " ...\n",
      " [-0.44217668]\n",
      " [ 0.00948039]\n",
      " [-0.37819778]]\n",
      "t [[ 0.20283429]\n",
      " [-1.09596983]\n",
      " [-0.68141796]\n",
      " ...\n",
      " [-0.44217668]\n",
      " [ 0.00948039]\n",
      " [-0.37819778]]\n",
      "t [[ 0.20755794]\n",
      " [-1.15253812]\n",
      " [-0.71816915]\n",
      " ...\n",
      " [-0.46269899]\n",
      " [-0.00190412]\n",
      " [-0.40492093]]\n",
      "loss=35682.011592107025\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00717712]\n",
      " [-0.04418859]\n",
      " [-0.13853584]\n",
      " ...\n",
      " [-0.1063094 ]\n",
      " [ 0.02066727]\n",
      " [-0.06306745]]\n",
      "t [[ 0.00717712]\n",
      " [-0.04418859]\n",
      " [-0.13853584]\n",
      " ...\n",
      " [-0.1063094 ]\n",
      " [ 0.02066727]\n",
      " [-0.06306745]]\n",
      "t [[ 0.00949711]\n",
      " [-0.09581361]\n",
      " [-0.24449555]\n",
      " ...\n",
      " [-0.18549851]\n",
      " [ 0.03199792]\n",
      " [-0.11868209]]\n",
      "t [[ 0.00949711]\n",
      " [-0.09581361]\n",
      " [-0.24449555]\n",
      " ...\n",
      " [-0.18549851]\n",
      " [ 0.03199792]\n",
      " [-0.11868209]]\n",
      "Current iteration=2, loss=38468.22799664002\n",
      "t [[ 0.00870933]\n",
      " [-0.15140599]\n",
      " [-0.32783209]\n",
      " ...\n",
      " [-0.24599357]\n",
      " [ 0.03662489]\n",
      " [-0.16832802]]\n",
      "t [[ 0.00870933]\n",
      " [-0.15140599]\n",
      " [-0.32783209]\n",
      " ...\n",
      " [-0.24599357]\n",
      " [ 0.03662489]\n",
      " [-0.16832802]]\n",
      "t [[ 0.00598891]\n",
      " [-0.20868367]\n",
      " [-0.39536755]\n",
      " ...\n",
      " [-0.2935705 ]\n",
      " [ 0.03640267]\n",
      " [-0.21310723]]\n",
      "t [[ 0.00598891]\n",
      " [-0.20868367]\n",
      " [-0.39536755]\n",
      " ...\n",
      " [-0.2935705 ]\n",
      " [ 0.03640267]\n",
      " [-0.21310723]]\n",
      "Current iteration=4, loss=37388.48924234679\n",
      "t [[ 0.00210268]\n",
      " [-0.26619388]\n",
      " [-0.45169176]\n",
      " ...\n",
      " [-0.33211491]\n",
      " [ 0.03261483]\n",
      " [-0.25383252]]\n",
      "t [[ 0.00210268]\n",
      " [-0.26619388]\n",
      " [-0.45169176]\n",
      " ...\n",
      " [-0.33211491]\n",
      " [ 0.03261483]\n",
      " [-0.25383252]]\n",
      "t [[-0.00244882]\n",
      " [-0.3230236 ]\n",
      " [-0.49991566]\n",
      " ...\n",
      " [-0.36425671]\n",
      " [ 0.02616468]\n",
      " [-0.29111524]]\n",
      "t [[-0.00244882]\n",
      " [-0.3230236 ]\n",
      " [-0.49991566]\n",
      " ...\n",
      " [-0.36425671]\n",
      " [ 0.02616468]\n",
      " [-0.29111524]]\n",
      "Current iteration=6, loss=36634.62171597111\n",
      "t [[-0.00733717]\n",
      " [-0.37860811]\n",
      " [-0.54217629]\n",
      " ...\n",
      " [-0.39179684]\n",
      " [ 0.01770411]\n",
      " [-0.3254271 ]]\n",
      "t [[-0.00733717]\n",
      " [-0.37860811]\n",
      " [-0.54217629]\n",
      " ...\n",
      " [-0.39179684]\n",
      " [ 0.01770411]\n",
      " [-0.3254271 ]]\n",
      "t [[-0.01234656]\n",
      " [-0.43260918]\n",
      " [-0.57996142]\n",
      " ...\n",
      " [-0.41598249]\n",
      " [ 0.00771545]\n",
      " [-0.35714117]]\n",
      "t [[-0.01234656]\n",
      " [-0.43260918]\n",
      " [-0.57996142]\n",
      " ...\n",
      " [-0.41598249]\n",
      " [ 0.00771545]\n",
      " [-0.35714117]]\n",
      "Current iteration=8, loss=36064.06826569957\n",
      "t [[-0.01733588]\n",
      " [-0.48483708]\n",
      " [-0.61431914]\n",
      " ...\n",
      " [-0.4376856 ]\n",
      " [-0.00343645]\n",
      " [-0.38655901]]\n",
      "t [[-0.01733588]\n",
      " [-0.48483708]\n",
      " [-0.61431914]\n",
      " ...\n",
      " [-0.4376856 ]\n",
      " [-0.00343645]\n",
      " [-0.38655901]]\n",
      "t [[-0.02221417]\n",
      " [-0.53519992]\n",
      " [-0.64599551]\n",
      " ...\n",
      " [-0.45752043]\n",
      " [-0.01547038]\n",
      " [-0.41392901]]\n",
      "loss=35614.69913850759\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00613006]\n",
      " [-0.03878255]\n",
      " [-0.14003623]\n",
      " ...\n",
      " [-0.10453157]\n",
      " [ 0.02222135]\n",
      " [-0.05915754]]\n",
      "t [[ 0.00613006]\n",
      " [-0.03878255]\n",
      " [-0.14003623]\n",
      " ...\n",
      " [-0.10453157]\n",
      " [ 0.02222135]\n",
      " [-0.05915754]]\n",
      "t [[ 0.0074505 ]\n",
      " [-0.0853489 ]\n",
      " [-0.24756119]\n",
      " ...\n",
      " [-0.1823113 ]\n",
      " [ 0.03501322]\n",
      " [-0.1111134 ]]\n",
      "t [[ 0.0074505 ]\n",
      " [-0.0853489 ]\n",
      " [-0.24756119]\n",
      " ...\n",
      " [-0.1823113 ]\n",
      " [ 0.03501322]\n",
      " [-0.1111134 ]]\n",
      "Current iteration=2, loss=38488.70645319901\n",
      "t [[ 0.00571999]\n",
      " [-0.13621784]\n",
      " [-0.33243296]\n",
      " ...\n",
      " [-0.24163416]\n",
      " [ 0.04099923]\n",
      " [-0.15732109]]\n",
      "t [[ 0.00571999]\n",
      " [-0.13621784]\n",
      " [-0.33243296]\n",
      " ...\n",
      " [-0.24163416]\n",
      " [ 0.04099923]\n",
      " [-0.15732109]]\n",
      "t [[ 0.00211959]\n",
      " [-0.18908431]\n",
      " [-0.40142844]\n",
      " ...\n",
      " [-0.2881994 ]\n",
      " [ 0.04203188]\n",
      " [-0.1988586 ]]\n",
      "t [[ 0.00211959]\n",
      " [-0.18908431]\n",
      " [-0.40142844]\n",
      " ...\n",
      " [-0.2881994 ]\n",
      " [ 0.04203188]\n",
      " [-0.1988586 ]]\n",
      "Current iteration=4, loss=37424.313467274726\n",
      "t [[-0.00258052]\n",
      " [-0.24246887]\n",
      " [-0.45912059]\n",
      " ...\n",
      " [-0.32584863]\n",
      " [ 0.03939731]\n",
      " [-0.23651942]]\n",
      "t [[-0.00258052]\n",
      " [-0.24246887]\n",
      " [-0.45912059]\n",
      " ...\n",
      " [-0.32584863]\n",
      " [ 0.03939731]\n",
      " [-0.23651942]]\n",
      "t [[-0.00787862]\n",
      " [-0.29543245]\n",
      " [-0.50861722]\n",
      " ...\n",
      " [-0.35718573]\n",
      " [ 0.03400344]\n",
      " [-0.27089902]]\n",
      "t [[-0.00787862]\n",
      " [-0.29543245]\n",
      " [-0.50861722]\n",
      " ...\n",
      " [-0.35718573]\n",
      " [ 0.03400344]\n",
      " [-0.27089902]]\n",
      "Current iteration=6, loss=36683.206754367005\n",
      "t [[-0.0134468 ]\n",
      " [-0.34738651]\n",
      " [-0.5520586 ]\n",
      " ...\n",
      " [-0.38399553]\n",
      " [ 0.02650751]\n",
      " [-0.30245575]]\n",
      "t [[-0.0134468 ]\n",
      " [-0.34738651]\n",
      " [-0.5520586 ]\n",
      " ...\n",
      " [-0.38399553]\n",
      " [ 0.02650751]\n",
      " [-0.30245575]]\n",
      "t [[-0.01907098]\n",
      " [-0.39797152]\n",
      " [-0.59093838]\n",
      " ...\n",
      " [-0.40751476]\n",
      " [ 0.01739748]\n",
      " [-0.33155125]]\n",
      "t [[-0.01907098]\n",
      " [-0.39797152]\n",
      " [-0.59093838]\n",
      " ...\n",
      " [-0.40751476]\n",
      " [ 0.01739748]\n",
      " [-0.33155125]]\n",
      "Current iteration=8, loss=36123.22925495669\n",
      "t [[-0.02461267]\n",
      " [-0.44697891]\n",
      " [-0.62631141]\n",
      " ...\n",
      " [-0.42860813]\n",
      " [ 0.00704385]\n",
      " [-0.35847713]]\n",
      "t [[-0.02461267]\n",
      " [-0.44697891]\n",
      " [-0.62631141]\n",
      " ...\n",
      " [-0.42860813]\n",
      " [ 0.00704385]\n",
      " [-0.35847713]]\n",
      "t [[-0.02998405]\n",
      " [-0.49430012]\n",
      " [-0.65893055]\n",
      " ...\n",
      " [-0.44788462]\n",
      " [-0.00426658]\n",
      " [-0.38347301]]\n",
      "loss=35682.55750334991\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0051051 ]\n",
      " [-0.04107741]\n",
      " [-0.13911223]\n",
      " ...\n",
      " [-0.06124754]\n",
      " [-0.0073765 ]\n",
      " [-0.1249549 ]]\n",
      "t [[ 0.0051051 ]\n",
      " [-0.04107741]\n",
      " [-0.13911223]\n",
      " ...\n",
      " [-0.06124754]\n",
      " [-0.0073765 ]\n",
      " [-0.1249549 ]]\n",
      "t [[ 0.00549561]\n",
      " [-0.0898834 ]\n",
      " [-0.24544421]\n",
      " ...\n",
      " [-0.10148664]\n",
      " [-0.00780437]\n",
      " [-0.23766565]]\n",
      "t [[ 0.00549561]\n",
      " [-0.0898834 ]\n",
      " [-0.24544421]\n",
      " ...\n",
      " [-0.10148664]\n",
      " [-0.00780437]\n",
      " [-0.23766565]]\n",
      "Current iteration=2, loss=38457.17238897427\n",
      "t [[ 0.00292221]\n",
      " [-0.14291466]\n",
      " [-0.32898443]\n",
      " ...\n",
      " [-0.12772461]\n",
      " [-0.00410921]\n",
      " [-0.34028694]]\n",
      "t [[ 0.00292221]\n",
      " [-0.14291466]\n",
      " [-0.32898443]\n",
      " ...\n",
      " [-0.12772461]\n",
      " [-0.00410921]\n",
      " [-0.34028694]]\n",
      "t [[-0.00144297]\n",
      " [-0.19785668]\n",
      " [-0.39659113]\n",
      " ...\n",
      " [-0.14471318]\n",
      " [ 0.00185359]\n",
      " [-0.43448497]]\n",
      "t [[-0.00144297]\n",
      " [-0.19785668]\n",
      " [-0.39659113]\n",
      " ...\n",
      " [-0.14471318]\n",
      " [ 0.00185359]\n",
      " [-0.43448497]]\n",
      "Current iteration=4, loss=37374.90804449052\n",
      "t [[-0.00683848]\n",
      " [-0.2532281 ]\n",
      " [-0.45288557]\n",
      " ...\n",
      " [-0.15560641]\n",
      " [ 0.00890067]\n",
      " [-0.5215378 ]]\n",
      "t [[-0.00683848]\n",
      " [-0.2532281 ]\n",
      " [-0.45288557]\n",
      " ...\n",
      " [-0.15560641]\n",
      " [ 0.00890067]\n",
      " [-0.5215378 ]]\n",
      "t [[-0.01277072]\n",
      " [-0.30809135]\n",
      " [-0.50100397]\n",
      " ...\n",
      " [-0.1625029 ]\n",
      " [ 0.01628697]\n",
      " [-0.60243904]]\n",
      "t [[-0.01277072]\n",
      " [-0.30809135]\n",
      " [-0.50100397]\n",
      " ...\n",
      " [-0.1625029 ]\n",
      " [ 0.01628697]\n",
      " [-0.60243904]]\n",
      "Current iteration=6, loss=36620.76174382715\n",
      "t [[-0.01891912]\n",
      " [-0.36186085]\n",
      " [-0.54310326]\n",
      " ...\n",
      " [-0.16680861]\n",
      " [ 0.02355139]\n",
      " [-0.67797588]]\n",
      "t [[-0.01891912]\n",
      " [-0.36186085]\n",
      " [-0.54310326]\n",
      " ...\n",
      " [-0.16680861]\n",
      " [ 0.02355139]\n",
      " [-0.67797588]]\n",
      "t [[-0.02507605]\n",
      " [-0.41418057]\n",
      " [-0.58068665]\n",
      " ...\n",
      " [-0.16947024]\n",
      " [ 0.0304177 ]\n",
      " [-0.7487839 ]]\n",
      "t [[-0.02507605]\n",
      " [-0.41418057]\n",
      " [-0.58068665]\n",
      " ...\n",
      " [-0.16947024]\n",
      " [ 0.0304177 ]\n",
      " [-0.7487839 ]]\n",
      "Current iteration=8, loss=36050.37922003321\n",
      "t [[-0.03110865]\n",
      " [-0.46484565]\n",
      " [-0.61481419]\n",
      " ...\n",
      " [-0.17112597]\n",
      " [ 0.03673064]\n",
      " [-0.81538516]]\n",
      "t [[-0.03110865]\n",
      " [-0.46484565]\n",
      " [-0.61481419]\n",
      " ...\n",
      " [-0.17112597]\n",
      " [ 0.03673064]\n",
      " [-0.81538516]]\n",
      "t [[-0.03693399]\n",
      " [-0.51375124]\n",
      " [-0.64624115]\n",
      " ...\n",
      " [-0.17220459]\n",
      " [ 0.04241402]\n",
      " [-0.87821495]]\n",
      "loss=35601.172085046215\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05714547]\n",
      " [-0.24411888]\n",
      " [-0.1534375 ]\n",
      " ...\n",
      " [-0.11120415]\n",
      " [ 0.02407631]\n",
      " [-0.06482618]]\n",
      "t [[ 0.05714547]\n",
      " [-0.24411888]\n",
      " [-0.1534375 ]\n",
      " ...\n",
      " [-0.11120415]\n",
      " [ 0.02407631]\n",
      " [-0.06482618]]\n",
      "t [[ 0.09879061]\n",
      " [-0.43502199]\n",
      " [-0.27189626]\n",
      " ...\n",
      " [-0.19315142]\n",
      " [ 0.0376161 ]\n",
      " [-0.12162036]]\n",
      "t [[ 0.09879061]\n",
      " [-0.43502199]\n",
      " [-0.27189626]\n",
      " ...\n",
      " [-0.19315142]\n",
      " [ 0.0376161 ]\n",
      " [-0.12162036]]\n",
      "Current iteration=2, loss=38419.25218091634\n",
      "t [[ 0.12945231]\n",
      " [-0.58820112]\n",
      " [-0.36613043]\n",
      " ...\n",
      " [-0.25528817]\n",
      " [ 0.04365637]\n",
      " [-0.1720487 ]]\n",
      "t [[ 0.12945231]\n",
      " [-0.58820112]\n",
      " [-0.36613043]\n",
      " ...\n",
      " [-0.25528817]\n",
      " [ 0.04365637]\n",
      " [-0.1720487 ]]\n",
      "t [[ 0.15227693]\n",
      " [-0.7142562 ]\n",
      " [-0.44340276]\n",
      " ...\n",
      " [-0.30396171]\n",
      " [ 0.04430485]\n",
      " [-0.21732921]]\n",
      "t [[ 0.15227693]\n",
      " [-0.7142562 ]\n",
      " [-0.44340276]\n",
      " ...\n",
      " [-0.30396171]\n",
      " [ 0.04430485]\n",
      " [-0.21732921]]\n",
      "Current iteration=4, loss=37334.961216644704\n",
      "t [[ 0.16942586]\n",
      " [-0.82033994]\n",
      " [-0.50853802]\n",
      " ...\n",
      " [-0.34336194]\n",
      " [ 0.04100347]\n",
      " [-0.25834845]]\n",
      "t [[ 0.16942586]\n",
      " [-0.82033994]\n",
      " [-0.50853802]\n",
      " ...\n",
      " [-0.34336194]\n",
      " [ 0.04100347]\n",
      " [-0.25834845]]\n",
      "t [[ 0.18240081]\n",
      " [-0.9113509 ]\n",
      " [-0.56477855]\n",
      " ...\n",
      " [-0.37627651]\n",
      " [ 0.0347594 ]\n",
      " [-0.29576673]]\n",
      "t [[ 0.18240081]\n",
      " [-0.9113509 ]\n",
      " [-0.56477855]\n",
      " ...\n",
      " [-0.37627651]\n",
      " [ 0.0347594 ]\n",
      " [-0.29576673]]\n",
      "Current iteration=6, loss=36585.88856656143\n",
      "t [[ 0.19226283]\n",
      " [-0.99071626]\n",
      " [-0.6143417 ]\n",
      " ...\n",
      " [-0.4045841 ]\n",
      " [ 0.02629635]\n",
      " [-0.33008979]]\n",
      "t [[ 0.19226283]\n",
      " [-0.99071626]\n",
      " [-0.6143417 ]\n",
      " ...\n",
      " [-0.4045841 ]\n",
      " [ 0.02629635]\n",
      " [-0.33008979]]\n",
      "t [[ 0.19977417]\n",
      " [-1.06088559]\n",
      " [-0.65877013]\n",
      " ...\n",
      " [-0.42956584]\n",
      " [ 0.01614863]\n",
      " [-0.36171541]]\n",
      "t [[ 0.19977417]\n",
      " [-1.06088559]\n",
      " [-0.65877013]\n",
      " ...\n",
      " [-0.42956584]\n",
      " [ 0.01614863]\n",
      " [-0.36171541]]\n",
      "Current iteration=8, loss=36023.340818414596\n",
      "t [[ 0.20549066]\n",
      " [-1.12364514]\n",
      " [-0.69915414]\n",
      " ...\n",
      " [-0.45210389]\n",
      " [ 0.00472007]\n",
      " [-0.39096377]]\n",
      "t [[ 0.20549066]\n",
      " [-1.12364514]\n",
      " [-0.69915414]\n",
      " ...\n",
      " [-0.45210389]\n",
      " [ 0.00472007]\n",
      " [-0.39096377]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.20982317]\n",
      " [-1.18032193]\n",
      " [-0.73627578]\n",
      " ...\n",
      " [-0.47281037]\n",
      " [-0.00767813]\n",
      " [-0.41809775]]\n",
      "loss=35583.19876824266\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00753598]\n",
      " [-0.04639802]\n",
      " [-0.14546263]\n",
      " ...\n",
      " [-0.11162487]\n",
      " [ 0.02170064]\n",
      " [-0.06622082]]\n",
      "t [[ 0.00753598]\n",
      " [-0.04639802]\n",
      " [-0.14546263]\n",
      " ...\n",
      " [-0.11162487]\n",
      " [ 0.02170064]\n",
      " [-0.06622082]]\n",
      "t [[ 0.0097205 ]\n",
      " [-0.1009896 ]\n",
      " [-0.2550269 ]\n",
      " ...\n",
      " [-0.19336337]\n",
      " [ 0.03311352]\n",
      " [-0.12422784]]\n",
      "t [[ 0.0097205 ]\n",
      " [-0.1009896 ]\n",
      " [-0.2550269 ]\n",
      " ...\n",
      " [-0.19336337]\n",
      " [ 0.03311352]\n",
      " [-0.12422784]]\n",
      "Current iteration=2, loss=38397.25768261335\n",
      "t [[ 0.00857767]\n",
      " [-0.15976143]\n",
      " [-0.34021163]\n",
      " ...\n",
      " [-0.25496748]\n",
      " [ 0.03728583]\n",
      " [-0.17573959]]\n",
      "t [[ 0.00857767]\n",
      " [-0.15976143]\n",
      " [-0.34021163]\n",
      " ...\n",
      " [-0.25496748]\n",
      " [ 0.03728583]\n",
      " [-0.17573959]]\n",
      "t [[ 0.00542999]\n",
      " [-0.22014804]\n",
      " [-0.40871143]\n",
      " ...\n",
      " [-0.30295239]\n",
      " [ 0.03631179]\n",
      " [-0.22200638]]\n",
      "t [[ 0.00542999]\n",
      " [-0.22014804]\n",
      " [-0.40871143]\n",
      " ...\n",
      " [-0.30295239]\n",
      " [ 0.03631179]\n",
      " [-0.22200638]]\n",
      "Current iteration=4, loss=37297.711320524766\n",
      "t [[ 0.00111779]\n",
      " [-0.28056323]\n",
      " [-0.4655804 ]\n",
      " ...\n",
      " [-0.34159665]\n",
      " [ 0.03161026]\n",
      " [-0.26393486]]\n",
      "t [[ 0.00111779]\n",
      " [-0.28056323]\n",
      " [-0.4655804 ]\n",
      " ...\n",
      " [-0.34159665]\n",
      " [ 0.03161026]\n",
      " [-0.26393486]]\n",
      "t [[-0.00382374]\n",
      " [-0.34003961]\n",
      " [-0.51417251]\n",
      " ...\n",
      " [-0.37373459]\n",
      " [ 0.02416338]\n",
      " [-0.30219852]]\n",
      "t [[-0.00382374]\n",
      " [-0.34003961]\n",
      " [-0.51417251]\n",
      " ...\n",
      " [-0.37373459]\n",
      " [ 0.02416338]\n",
      " [-0.30219852]]\n",
      "Current iteration=6, loss=36535.73095347062\n",
      "t [[-0.00905187]\n",
      " [-0.39799915]\n",
      " [-0.55674821]\n",
      " ...\n",
      " [-0.40126915]\n",
      " [ 0.01467086]\n",
      " [-0.33731241]]\n",
      "t [[-0.00905187]\n",
      " [-0.39799915]\n",
      " [-0.55674821]\n",
      " ...\n",
      " [-0.40126915]\n",
      " [ 0.01467086]\n",
      " [-0.33731241]]\n",
      "t [[-0.01434708]\n",
      " [-0.45411117]\n",
      " [-0.59485348]\n",
      " ...\n",
      " [-0.42549367]\n",
      " [ 0.0036451 ]\n",
      " [-0.36968117]]\n",
      "t [[-0.01434708]\n",
      " [-0.45411117]\n",
      " [-0.59485348]\n",
      " ...\n",
      " [-0.42549367]\n",
      " [ 0.0036451 ]\n",
      " [-0.36968117]]\n",
      "Current iteration=8, loss=35962.59541524637\n",
      "t [[-0.01957002]\n",
      " [-0.50820358]\n",
      " [-0.62955919]\n",
      " ...\n",
      " [-0.44729599]\n",
      " [-0.00852962]\n",
      " [-0.39963034]]\n",
      "t [[-0.01957002]\n",
      " [-0.50820358]\n",
      " [-0.62955919]\n",
      " ...\n",
      " [-0.44729599]\n",
      " [-0.00852962]\n",
      " [-0.39963034]]\n",
      "t [[-0.02463409]\n",
      " [-0.5602062 ]\n",
      " [-0.6616155 ]\n",
      " ...\n",
      " [-0.4672904 ]\n",
      " [-0.02155936]\n",
      " [-0.42742707]]\n",
      "loss=35513.980999333966\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00643656]\n",
      " [-0.04072168]\n",
      " [-0.14703804]\n",
      " ...\n",
      " [-0.10975815]\n",
      " [ 0.02333242]\n",
      " [-0.06211542]]\n",
      "t [[ 0.00643656]\n",
      " [-0.04072168]\n",
      " [-0.14703804]\n",
      " ...\n",
      " [-0.10975815]\n",
      " [ 0.02333242]\n",
      " [-0.06211542]]\n",
      "t [[ 0.00757399]\n",
      " [-0.09002004]\n",
      " [-0.25824886]\n",
      " ...\n",
      " [-0.19003581]\n",
      " [ 0.03627453]\n",
      " [-0.11629386]]\n",
      "t [[ 0.00757399]\n",
      " [-0.09002004]\n",
      " [-0.25824886]\n",
      " ...\n",
      " [-0.19003581]\n",
      " [ 0.03627453]\n",
      " [-0.11629386]]\n",
      "Current iteration=2, loss=38418.61099890989\n",
      "t [[ 0.00544712]\n",
      " [-0.14386739]\n",
      " [-0.34504184]\n",
      " ...\n",
      " [-0.25043329]\n",
      " [ 0.04186289]\n",
      " [-0.16421839]]\n",
      "t [[ 0.00544712]\n",
      " [-0.14386739]\n",
      " [-0.34504184]\n",
      " ...\n",
      " [-0.25043329]\n",
      " [ 0.04186289]\n",
      " [-0.16421839]]\n",
      "t [[ 0.0013852 ]\n",
      " [-0.19967083]\n",
      " [-0.41506255]\n",
      " ...\n",
      " [-0.29738049]\n",
      " [ 0.04218998]\n",
      " [-0.20711186]]\n",
      "t [[ 0.0013852 ]\n",
      " [-0.19967083]\n",
      " [-0.41506255]\n",
      " ...\n",
      " [-0.29738049]\n",
      " [ 0.04218998]\n",
      " [-0.20711186]]\n",
      "Current iteration=4, loss=37334.953574782885\n",
      "t [[-0.00376785]\n",
      " [-0.25581299]\n",
      " [-0.47334863]\n",
      " ...\n",
      " [-0.33510865]\n",
      " [ 0.03867817]\n",
      " [-0.24585914]]\n",
      "t [[-0.00376785]\n",
      " [-0.25581299]\n",
      " [-0.47334863]\n",
      " ...\n",
      " [-0.33510865]\n",
      " [ 0.03867817]\n",
      " [-0.24585914]]\n",
      "t [[-0.00947582]\n",
      " [-0.31129661]\n",
      " [-0.52325252]\n",
      " ...\n",
      " [-0.36642482]\n",
      " [ 0.03231517]\n",
      " [-0.28111597]]\n",
      "t [[-0.00947582]\n",
      " [-0.31129661]\n",
      " [-0.52325252]\n",
      " ...\n",
      " [-0.36642482]\n",
      " [ 0.03231517]\n",
      " [-0.28111597]]\n",
      "Current iteration=6, loss=36586.080856613684\n",
      "t [[-0.01539694]\n",
      " [-0.36551667]\n",
      " [-0.56703966]\n",
      " ...\n",
      " [-0.39321541]\n",
      " [ 0.02380704]\n",
      " [-0.31338252]]\n",
      "t [[-0.01539694]\n",
      " [-0.36551667]\n",
      " [-0.56703966]\n",
      " ...\n",
      " [-0.39321541]\n",
      " [ 0.02380704]\n",
      " [-0.31338252]]\n",
      "t [[-0.02131406]\n",
      " [-0.41811866]\n",
      " [-0.60626341]\n",
      " ...\n",
      " [-0.41676315]\n",
      " [ 0.01367273]\n",
      " [-0.3430508 ]]\n",
      "t [[-0.02131406]\n",
      " [-0.41811866]\n",
      " [-0.60626341]\n",
      " ...\n",
      " [-0.41676315]\n",
      " [ 0.01367273]\n",
      " [-0.3430508 ]]\n",
      "Current iteration=8, loss=36023.69017754097\n",
      "t [[-0.02709111]\n",
      " [-0.46890953]\n",
      " [-0.64200273]\n",
      " ...\n",
      " [-0.43794853]\n",
      " [ 0.00230306]\n",
      " [-0.3704353 ]]\n",
      "t [[-0.02709111]\n",
      " [-0.46890953]\n",
      " [-0.64200273]\n",
      " ...\n",
      " [-0.43794853]\n",
      " [ 0.00230306]\n",
      " [-0.3704353 ]]\n",
      "t [[-0.03264535]\n",
      " [-0.51780067]\n",
      " [-0.67501561]\n",
      " ...\n",
      " [-0.45738044]\n",
      " [-0.01000163]\n",
      " [-0.39579351]]\n",
      "loss=35583.81493272021\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00536036]\n",
      " [-0.04313128]\n",
      " [-0.14606784]\n",
      " ...\n",
      " [-0.06430992]\n",
      " [-0.00774532]\n",
      " [-0.13120264]]\n",
      "t [[ 0.00536036]\n",
      " [-0.04313128]\n",
      " [-0.14606784]\n",
      " ...\n",
      " [-0.06430992]\n",
      " [-0.00774532]\n",
      " [-0.13120264]]\n",
      "t [[ 0.00552628]\n",
      " [-0.09477814]\n",
      " [-0.2560123 ]\n",
      " ...\n",
      " [-0.10546887]\n",
      " [-0.00783309]\n",
      " [-0.24891035]]\n",
      "t [[ 0.00552628]\n",
      " [-0.09477814]\n",
      " [-0.2560123 ]\n",
      " ...\n",
      " [-0.10546887]\n",
      " [-0.00783309]\n",
      " [-0.24891035]]\n",
      "Current iteration=2, loss=38385.90419298496\n",
      "t [[ 0.0025234 ]\n",
      " [-0.15088753]\n",
      " [-0.34139354]\n",
      " ...\n",
      " [-0.1315859 ]\n",
      " [-0.00353245]\n",
      " [-0.35561966]]\n",
      "t [[ 0.0025234 ]\n",
      " [-0.15088753]\n",
      " [-0.34139354]\n",
      " ...\n",
      " [-0.1315859 ]\n",
      " [-0.00353245]\n",
      " [-0.35561966]]\n",
      "t [[-0.00232941]\n",
      " [-0.20885658]\n",
      " [-0.40994795]\n",
      " ...\n",
      " [-0.14801669]\n",
      " [ 0.00306986]\n",
      " [-0.45322704]]\n",
      "t [[-0.00232941]\n",
      " [-0.20885658]\n",
      " [-0.40994795]\n",
      " ...\n",
      " [-0.14801669]\n",
      " [ 0.00306986]\n",
      " [-0.45322704]]\n",
      "Current iteration=4, loss=37284.02289205933\n",
      "t [[-0.00819829]\n",
      " [-0.26706651]\n",
      " [-0.46676526]\n",
      " ...\n",
      " [-0.15822747]\n",
      " [ 0.010681  ]\n",
      " [-0.54316525]]\n",
      "t [[-0.00819829]\n",
      " [-0.26706651]\n",
      " [-0.46676526]\n",
      " ...\n",
      " [-0.15822747]\n",
      " [ 0.010681  ]\n",
      " [-0.54316525]]\n",
      "t [[-0.01455632]\n",
      " [-0.32452228]\n",
      " [-0.51522758]\n",
      " ...\n",
      " [-0.16447141]\n",
      " [ 0.0185113 ]\n",
      " [-0.62653527]]\n",
      "t [[-0.01455632]\n",
      " [-0.32452228]\n",
      " [-0.51522758]\n",
      " ...\n",
      " [-0.16447141]\n",
      " [ 0.0185113 ]\n",
      " [-0.62653527]]\n",
      "Current iteration=6, loss=36521.8797711972\n",
      "t [[-0.02107002]\n",
      " [-0.38062253]\n",
      " [-0.55761716]\n",
      " ...\n",
      " [-0.16822454]\n",
      " [ 0.02608887]\n",
      " [-0.70420171]]\n",
      "t [[-0.02107002]\n",
      " [-0.38062253]\n",
      " [-0.55761716]\n",
      " ...\n",
      " [-0.16822454]\n",
      " [ 0.02608887]\n",
      " [-0.70420171]]\n",
      "t [[-0.02752947]\n",
      " [-0.43501692]\n",
      " [-0.59549661]\n",
      " ...\n",
      " [-0.17045826]\n",
      " [ 0.03314384]\n",
      " [-0.77685795]]\n",
      "t [[-0.02752947]\n",
      " [-0.43501692]\n",
      " [-0.59549661]\n",
      " ...\n",
      " [-0.17045826]\n",
      " [ 0.03314384]\n",
      " [-0.77685795]]\n",
      "Current iteration=8, loss=35948.94688602626\n",
      "t [[-0.03380484]\n",
      " [-0.4875167 ]\n",
      " [-0.62994949]\n",
      " ...\n",
      " [-0.17181171]\n",
      " [ 0.0395353 ]\n",
      " [-0.84507048]]\n",
      "t [[-0.03380484]\n",
      " [-0.4875167 ]\n",
      " [-0.62994949]\n",
      " ...\n",
      " [-0.17181171]\n",
      " [ 0.0395353 ]\n",
      " [-0.84507048]]\n",
      "t [[-0.03981873]\n",
      " [-0.53803755]\n",
      " [-0.66173559]\n",
      " ...\n",
      " [-0.17270283]\n",
      " [ 0.04520436]\n",
      " [-0.90930966]]\n",
      "loss=35500.48533112435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.05986668]\n",
      " [-0.25574358]\n",
      " [-0.16074405]\n",
      " ...\n",
      " [-0.11649959]\n",
      " [ 0.0252228 ]\n",
      " [-0.06791314]]\n",
      "t [[ 0.05986668]\n",
      " [-0.25574358]\n",
      " [-0.16074405]\n",
      " ...\n",
      " [-0.11649959]\n",
      " [ 0.0252228 ]\n",
      " [-0.06791314]]\n",
      "t [[ 0.10272948]\n",
      " [-0.45311032]\n",
      " [-0.28311741]\n",
      " ...\n",
      " [-0.20090474]\n",
      " [ 0.03888838]\n",
      " [-0.12701433]]\n",
      "t [[ 0.10272948]\n",
      " [-0.45311032]\n",
      " [-0.28311741]\n",
      " ...\n",
      " [-0.20090474]\n",
      " [ 0.03888838]\n",
      " [-0.12701433]]\n",
      "Current iteration=2, loss=38350.70564974434\n",
      "t [[ 0.13378123]\n",
      " [-0.60990722]\n",
      " [-0.37948028]\n",
      " ...\n",
      " [-0.26407251]\n",
      " [ 0.04448721]\n",
      " [-0.17921953]]\n",
      "t [[ 0.13378123]\n",
      " [-0.60990722]\n",
      " [-0.37948028]\n",
      " ...\n",
      " [-0.26407251]\n",
      " [ 0.04448721]\n",
      " [-0.17921953]]\n",
      "t [[ 0.15655513]\n",
      " [-0.73802005]\n",
      " [-0.45796895]\n",
      " ...\n",
      " [-0.31311212]\n",
      " [ 0.04438347]\n",
      " [-0.22590104]]\n",
      "t [[ 0.15655513]\n",
      " [-0.73802005]\n",
      " [-0.45796895]\n",
      " ...\n",
      " [-0.31311212]\n",
      " [ 0.04438347]\n",
      " [-0.22590104]]\n",
      "Current iteration=4, loss=37248.542550149985\n",
      "t [[ 0.17342741]\n",
      " [-0.84529401]\n",
      " [-0.52386395]\n",
      " ...\n",
      " [-0.35260506]\n",
      " [ 0.04016151]\n",
      " [-0.26804126]]\n",
      "t [[ 0.17342741]\n",
      " [-0.84529401]\n",
      " [-0.52386395]\n",
      " ...\n",
      " [-0.35260506]\n",
      " [ 0.04016151]\n",
      " [-0.26804126]]\n",
      "t [[ 0.18601954]\n",
      " [-0.93700469]\n",
      " [-0.58064355]\n",
      " ...\n",
      " [-0.38553513]\n",
      " [ 0.03291077]\n",
      " [-0.30636311]]\n",
      "t [[ 0.18601954]\n",
      " [-0.93700469]\n",
      " [-0.58064355]\n",
      " ...\n",
      " [-0.38553513]\n",
      " [ 0.03291077]\n",
      " [-0.30636311]]\n",
      "Current iteration=6, loss=36492.563515144575\n",
      "t [[ 0.19545943]\n",
      " [-1.0167871 ]\n",
      " [-0.63064484]\n",
      " ...\n",
      " [-0.41387394]\n",
      " [ 0.02340483]\n",
      " [-0.34141564]]\n",
      "t [[ 0.19545943]\n",
      " [-1.0167871 ]\n",
      " [-0.63064484]\n",
      " ...\n",
      " [-0.41387394]\n",
      " [ 0.02340483]\n",
      " [-0.34141564]]\n",
      "t [[ 0.2025461 ]\n",
      " [-1.08720592]\n",
      " [-0.6754681 ]\n",
      " ...\n",
      " [-0.43894097]\n",
      " [ 0.01220942]\n",
      " [-0.37362803]]\n",
      "t [[ 0.2025461 ]\n",
      " [-1.08720592]\n",
      " [-0.6754681 ]\n",
      " ...\n",
      " [-0.43894097]\n",
      " [ 0.01220942]\n",
      " [-0.37362803]]\n",
      "Current iteration=8, loss=35928.26144043405\n",
      "t [[ 2.07854908e-01]\n",
      " [-1.15011082e+00]\n",
      " [-7.16228138e-01]\n",
      " ...\n",
      " [-4.61628181e-01]\n",
      " [-2.51283621e-04]\n",
      " [-4.03344228e-01]]\n",
      "t [[ 2.07854908e-01]\n",
      " [-1.15011082e+00]\n",
      " [-7.16228138e-01]\n",
      " ...\n",
      " [-4.61628181e-01]\n",
      " [-2.51283621e-04]\n",
      " [-4.03344228e-01]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.2118063 ]\n",
      " [-1.20686335]\n",
      " [-0.75371427]\n",
      " ...\n",
      " [-0.48254339]\n",
      " [-0.01365265]\n",
      " [-0.43084564]]\n",
      "loss=35489.4027881978\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00789484]\n",
      " [-0.04860745]\n",
      " [-0.15238942]\n",
      " ...\n",
      " [-0.11694034]\n",
      " [ 0.022734  ]\n",
      " [-0.0693742 ]]\n",
      "t [[ 0.00789484]\n",
      " [-0.04860745]\n",
      " [-0.15238942]\n",
      " ...\n",
      " [-0.11694034]\n",
      " [ 0.022734  ]\n",
      " [-0.0693742 ]]\n",
      "t [[ 0.00992046]\n",
      " [-0.10620152]\n",
      " [-0.26539949]\n",
      " ...\n",
      " [-0.20109599]\n",
      " [ 0.03418386]\n",
      " [-0.12973703]]\n",
      "t [[ 0.00992046]\n",
      " [-0.10620152]\n",
      " [-0.26539949]\n",
      " ...\n",
      " [-0.20109599]\n",
      " [ 0.03418386]\n",
      " [-0.12973703]]\n",
      "Current iteration=2, loss=38327.88615062084\n",
      "t [[ 0.00840253]\n",
      " [-0.16817155]\n",
      " [-0.35226851]\n",
      " ...\n",
      " [-0.26367483]\n",
      " [ 0.0378515 ]\n",
      " [-0.18306469]]\n",
      "t [[ 0.00840253]\n",
      " [-0.16817155]\n",
      " [-0.35226851]\n",
      " ...\n",
      " [-0.26367483]\n",
      " [ 0.0378515 ]\n",
      " [-0.18306469]]\n",
      "t [[ 0.00481807]\n",
      " [-0.23165539]\n",
      " [-0.42161021]\n",
      " ...\n",
      " [-0.3119709 ]\n",
      " [ 0.03608419]\n",
      " [-0.23076535]]\n",
      "t [[ 0.00481807]\n",
      " [-0.23165539]\n",
      " [-0.42161021]\n",
      " ...\n",
      " [-0.3119709 ]\n",
      " [ 0.03608419]\n",
      " [-0.23076535]]\n",
      "Current iteration=4, loss=37209.93390851785\n",
      "t [[ 8.07902708e-05]\n",
      " [-2.94935309e-01]\n",
      " [-4.78947479e-01]\n",
      " ...\n",
      " [-3.50659425e-01]\n",
      " [ 3.04389648e-02]\n",
      " [-2.73843187e-01]]\n",
      "t [[ 8.07902708e-05]\n",
      " [-2.94935309e-01]\n",
      " [-4.78947479e-01]\n",
      " ...\n",
      " [-3.50659425e-01]\n",
      " [ 3.04389648e-02]\n",
      " [-2.73843187e-01]]\n",
      "t [[-0.00524172]\n",
      " [-0.35699574]\n",
      " [-0.52786871]\n",
      " ...\n",
      " [-0.38277144]\n",
      " [ 0.02197632]\n",
      " [-0.31303528]]\n",
      "t [[-0.00524172]\n",
      " [-0.35699574]\n",
      " [-0.52786871]\n",
      " ...\n",
      " [-0.38277144]\n",
      " [ 0.02197632]\n",
      " [-0.31303528]]\n",
      "Current iteration=6, loss=36440.70455366393\n",
      "t [[-0.0107949 ]\n",
      " [-0.41725193]\n",
      " [-0.57074664]\n",
      " ...\n",
      " [-0.41030201]\n",
      " [ 0.01144253]\n",
      " [-0.3489005 ]]\n",
      "t [[-0.0107949 ]\n",
      " [-0.41725193]\n",
      " [-0.57074664]\n",
      " ...\n",
      " [-0.41030201]\n",
      " [ 0.01144253]\n",
      " [-0.3489005 ]]\n",
      "t [[-0.01635775]\n",
      " [-0.47538654]\n",
      " [-0.60917534]\n",
      " ...\n",
      " [-0.43458162]\n",
      " [-0.00062124]\n",
      " [-0.38187536]]\n",
      "t [[-0.01635775]\n",
      " [-0.47538654]\n",
      " [-0.60917534]\n",
      " ...\n",
      " [-0.43458162]\n",
      " [-0.00062124]\n",
      " [-0.38187536]]\n",
      "Current iteration=8, loss=35865.69015663293\n",
      "t [[-0.02179439]\n",
      " [-0.53124963]\n",
      " [-0.64424049]\n",
      " ...\n",
      " [-0.45650682]\n",
      " [-0.0138125 ]\n",
      " [-0.41230943]]\n",
      "t [[-0.02179439]\n",
      " [-0.53124963]\n",
      " [-0.64424049]\n",
      " ...\n",
      " [-0.45650682]\n",
      " [-0.0138125 ]\n",
      " [-0.41230943]]\n",
      "t [[-0.02702383]\n",
      " [-0.58479614]\n",
      " [-0.67669057]\n",
      " ...\n",
      " [-0.47668671]\n",
      " [-0.02782562]\n",
      " [-0.44048868]]\n",
      "loss=35418.37825936684\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00674306]\n",
      " [-0.04266081]\n",
      " [-0.15403985]\n",
      " ...\n",
      " [-0.11498472]\n",
      " [ 0.02444349]\n",
      " [-0.06507329]]\n",
      "t [[ 0.00674306]\n",
      " [-0.04266081]\n",
      " [-0.15403985]\n",
      " ...\n",
      " [-0.11498472]\n",
      " [ 0.02444349]\n",
      " [-0.06507329]]\n",
      "t [[ 0.00767427]\n",
      " [-0.09472888]\n",
      " [-0.26877801]\n",
      " ...\n",
      " [-0.19762986]\n",
      " [ 0.0374901 ]\n",
      " [-0.12143902]]\n",
      "t [[ 0.00767427]\n",
      " [-0.09472888]\n",
      " [-0.26877801]\n",
      " ...\n",
      " [-0.19762986]\n",
      " [ 0.0374901 ]\n",
      " [-0.12143902]]\n",
      "Current iteration=2, loss=38350.10434005553\n",
      "t [[ 0.00513158]\n",
      " [-0.15157661]\n",
      " [-0.35732743]\n",
      " ...\n",
      " [-0.25896921]\n",
      " [ 0.04262966]\n",
      " [-0.17103244]]\n",
      "t [[ 0.00513158]\n",
      " [-0.15157661]\n",
      " [-0.35732743]\n",
      " ...\n",
      " [-0.25896921]\n",
      " [ 0.04262966]\n",
      " [-0.17103244]]\n",
      "t [[ 0.00059963]\n",
      " [-0.21030945]\n",
      " [-0.42824921]\n",
      " ...\n",
      " [-0.30620275]\n",
      " [ 0.04220821]\n",
      " [-0.21523063]]\n",
      "t [[ 0.00059963]\n",
      " [-0.21030945]\n",
      " [-0.42824921]\n",
      " ...\n",
      " [-0.30620275]\n",
      " [ 0.04220821]\n",
      " [-0.21523063]]\n",
      "Current iteration=4, loss=37248.571608259845\n",
      "t [[-0.00500406]\n",
      " [-0.2691736 ]\n",
      " [-0.48705056]\n",
      " ...\n",
      " [-0.34395522]\n",
      " [ 0.03778725]\n",
      " [-0.25501336]]\n",
      "t [[-0.00500406]\n",
      " [-0.2691736 ]\n",
      " [-0.48705056]\n",
      " ...\n",
      " [-0.34395522]\n",
      " [ 0.03778725]\n",
      " [-0.25501336]]\n",
      "t [[-0.01111114]\n",
      " [-0.32711952]\n",
      " [-0.53732028]\n",
      " ...\n",
      " [-0.37522933]\n",
      " [ 0.030434  ]\n",
      " [-0.29109797]]\n",
      "t [[-0.01111114]\n",
      " [-0.32711952]\n",
      " [-0.53732028]\n",
      " ...\n",
      " [-0.37522933]\n",
      " [ 0.030434  ]\n",
      " [-0.29109797]]\n",
      "Current iteration=6, loss=36492.77403937564\n",
      "t [[-0.01736856]\n",
      " [-0.38353218]\n",
      " [-0.58143797]\n",
      " ...\n",
      " [-0.40200329]\n",
      " [ 0.02090213]\n",
      " [-0.32402692]]\n",
      "t [[-0.01736856]\n",
      " [-0.38353218]\n",
      " [-0.58143797]\n",
      " ...\n",
      " [-0.40200329]\n",
      " [ 0.02090213]\n",
      " [-0.32402692]]\n",
      "t [[-0.0235584 ]\n",
      " [-0.43806772]\n",
      " [-0.62100676]\n",
      " ...\n",
      " [-0.42559686]\n",
      " [ 0.00974034]\n",
      " [-0.35422275]]\n",
      "t [[-0.0235584 ]\n",
      " [-0.43806772]\n",
      " [-0.62100676]\n",
      " ...\n",
      " [-0.42559686]\n",
      " [ 0.00974034]\n",
      " [-0.35422275]]\n",
      "Current iteration=8, loss=35928.6481737776\n",
      "t [[-0.02954885]\n",
      " [-0.49055307]\n",
      " [-0.65712174]\n",
      " ...\n",
      " [-0.44689914]\n",
      " [-0.0026414 ]\n",
      " [-0.38202293]]\n",
      "t [[-0.02954885]\n",
      " [-0.49055307]\n",
      " [-0.65712174]\n",
      " ...\n",
      " [-0.44689914]\n",
      " [-0.0026414 ]\n",
      " [-0.38202293]]\n",
      "t [[-0.03526353]\n",
      " [-0.54092297]\n",
      " [-0.6905403 ]\n",
      " ...\n",
      " [-0.46651367]\n",
      " [-0.01593012]\n",
      " [-0.40770277]]\n",
      "loss=35490.095511122614\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00561561]\n",
      " [-0.04518516]\n",
      " [-0.15302345]\n",
      " ...\n",
      " [-0.06737229]\n",
      " [-0.00811415]\n",
      " [-0.13745039]]\n",
      "t [[ 0.00561561]\n",
      " [-0.04518516]\n",
      " [-0.15302345]\n",
      " ...\n",
      " [-0.06737229]\n",
      " [-0.00811415]\n",
      " [-0.13745039]]\n",
      "t [[ 0.00553421]\n",
      " [-0.09971025]\n",
      " [-0.26642063]\n",
      " ...\n",
      " [-0.10934872]\n",
      " [-0.00782788]\n",
      " [-0.26009487]]\n",
      "t [[ 0.00553421]\n",
      " [-0.09971025]\n",
      " [-0.26642063]\n",
      " ...\n",
      " [-0.10934872]\n",
      " [-0.00782788]\n",
      " [-0.26009487]]\n",
      "Current iteration=2, loss=38316.253460175954\n",
      "t [[ 0.00208318]\n",
      " [-0.15891883]\n",
      " [-0.35347757]\n",
      " ...\n",
      " [-0.13524808]\n",
      " [-0.00289736]\n",
      " [-0.37080596]]\n",
      "t [[ 0.00208318]\n",
      " [-0.15891883]\n",
      " [-0.35347757]\n",
      " ...\n",
      " [-0.13524808]\n",
      " [-0.00289736]\n",
      " [-0.37080596]]\n",
      "t [[-0.00326475]\n",
      " [-0.21990597]\n",
      " [-0.42285596]\n",
      " ...\n",
      " [-0.15106144]\n",
      " [ 0.00434828]\n",
      " [-0.47172653]]\n",
      "t [[-0.00326475]\n",
      " [-0.21990597]\n",
      " [-0.42285596]\n",
      " ...\n",
      " [-0.15106144]\n",
      " [ 0.00434828]\n",
      " [-0.47172653]]\n",
      "Current iteration=4, loss=37196.159775066946\n",
      "t [[-0.00960361]\n",
      " [-0.28091707]\n",
      " [-0.48011872]\n",
      " ...\n",
      " [-0.16056853]\n",
      " [ 0.01250816]\n",
      " [-0.56445134]]\n",
      "t [[-0.00960361]\n",
      " [-0.28091707]\n",
      " [-0.48011872]\n",
      " ...\n",
      " [-0.16056853]\n",
      " [ 0.01250816]\n",
      " [-0.56445134]]\n",
      "t [[-0.01637559]\n",
      " [-0.34090564]\n",
      " [-0.52888536]\n",
      " ...\n",
      " [-0.16616916]\n",
      " [ 0.02075317]\n",
      " [-0.65019211]]\n",
      "t [[-0.01637559]\n",
      " [-0.34090564]\n",
      " [-0.52888536]\n",
      " ...\n",
      " [-0.16616916]\n",
      " [ 0.02075317]\n",
      " [-0.65019211]]\n",
      "Current iteration=6, loss=36426.87183143576\n",
      "t [[-0.02323696]\n",
      " [-0.39926106]\n",
      " [-0.57155227]\n",
      " ...\n",
      " [-0.1693998 ]\n",
      " [ 0.02860614]\n",
      " [-0.72989249]]\n",
      "t [[-0.02323696]\n",
      " [-0.39926106]\n",
      " [-0.57155227]\n",
      " ...\n",
      " [-0.1693998 ]\n",
      " [ 0.02860614]\n",
      " [-0.72989249]]\n",
      "t [[-0.0299778 ]\n",
      " [-0.45564434]\n",
      " [-0.60973134]\n",
      " ...\n",
      " [-0.17124808]\n",
      " [ 0.03580831]\n",
      " [-0.80430445]]\n",
      "t [[-0.0299778 ]\n",
      " [-0.45564434]\n",
      " [-0.60973134]\n",
      " ...\n",
      " [-0.17124808]\n",
      " [ 0.03580831]\n",
      " [-0.80430445]]\n",
      "Current iteration=8, loss=35852.08176485558\n",
      "t [[-0.03647315]\n",
      " [-0.50988738]\n",
      " [-0.64452161]\n",
      " ...\n",
      " [-0.17234735]\n",
      " [ 0.04223678]\n",
      " [-0.87403934]]\n",
      "t [[-0.03647315]\n",
      " [-0.50988738]\n",
      " [-0.64452161]\n",
      " ...\n",
      " [-0.17234735]\n",
      " [ 0.04223678]\n",
      " [-0.87403934]]\n",
      "t [[-0.04265247]\n",
      " [-0.56192982]\n",
      " [-0.67668148]\n",
      " ...\n",
      " [-0.17309979]\n",
      " [ 0.04785252]\n",
      " [-0.93960272]]\n",
      "loss=35404.90843436496\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.06258789]\n",
      " [-0.26736829]\n",
      " [-0.1680506 ]\n",
      " ...\n",
      " [-0.12179503]\n",
      " [ 0.02636929]\n",
      " [-0.0710001 ]]\n",
      "t [[ 0.06258789]\n",
      " [-0.26736829]\n",
      " [-0.1680506 ]\n",
      " ...\n",
      " [-0.12179503]\n",
      " [ 0.02636929]\n",
      " [-0.0710001 ]]\n",
      "t [[ 0.10659988]\n",
      " [-0.47096379]\n",
      " [-0.29418429]\n",
      " ...\n",
      " [-0.20852892]\n",
      " [ 0.04011442]\n",
      " [-0.13237261]]\n",
      "t [[ 0.10659988]\n",
      " [-0.47096379]\n",
      " [-0.29418429]\n",
      " ...\n",
      " [-0.20852892]\n",
      " [ 0.04011442]\n",
      " [-0.13237261]]\n",
      "Current iteration=2, loss=38283.695572082135\n",
      "t [[ 0.137968  ]\n",
      " [-0.63112463]\n",
      " [-0.39251689]\n",
      " ...\n",
      " [-0.27260095]\n",
      " [ 0.0452214 ]\n",
      " [-0.18630677]]\n",
      "t [[ 0.137968  ]\n",
      " [-0.63112463]\n",
      " [-0.39251689]\n",
      " ...\n",
      " [-0.27260095]\n",
      " [ 0.0452214 ]\n",
      " [-0.18630677]]\n",
      "t [[ 0.16063296]\n",
      " [-0.76108745]\n",
      " [-0.47210088]\n",
      " ...\n",
      " [-0.32191906]\n",
      " [ 0.04432403]\n",
      " [-0.23433838]]\n",
      "t [[ 0.16063296]\n",
      " [-0.76108745]\n",
      " [-0.47210088]\n",
      " ...\n",
      " [-0.32191906]\n",
      " [ 0.04432403]\n",
      " [-0.23433838]]\n",
      "Current iteration=4, loss=37164.93087393109\n",
      "t [[ 0.17718932]\n",
      " [-0.86939818]\n",
      " [-0.53867548]\n",
      " ...\n",
      " [-0.36145771]\n",
      " [ 0.0391516 ]\n",
      " [-0.277549  ]]\n",
      "t [[ 0.17718932]\n",
      " [-0.86939818]\n",
      " [-0.53867548]\n",
      " ...\n",
      " [-0.36145771]\n",
      " [ 0.0391516 ]\n",
      " [-0.277549  ]]\n",
      "t [[ 0.18937617]\n",
      " [-0.96170082]\n",
      " [-0.59594637]\n",
      " ...\n",
      " [-0.39438791]\n",
      " [ 0.03087531]\n",
      " [-0.31672538]]\n",
      "t [[ 0.18937617]\n",
      " [-0.96170082]\n",
      " [-0.59594637]\n",
      " ...\n",
      " [-0.39438791]\n",
      " [ 0.03087531]\n",
      " [-0.31672538]]\n",
      "Current iteration=6, loss=36402.817023603595\n",
      "t [[ 0.19838465]\n",
      " [-1.04182637]\n",
      " [-0.64636012]\n",
      " ...\n",
      " [-0.42276386]\n",
      " [ 0.02031729]\n",
      " [-0.3524603 ]]\n",
      "t [[ 0.19838465]\n",
      " [-1.04182637]\n",
      " [-0.64636012]\n",
      " ...\n",
      " [-0.42276386]\n",
      " [ 0.02031729]\n",
      " [-0.3524603 ]]\n",
      "t [[ 0.20504706]\n",
      " [-1.11244394]\n",
      " [-0.6915657 ]\n",
      " ...\n",
      " [-0.44793435]\n",
      " [ 0.00807339]\n",
      " [-0.38521454]]\n",
      "t [[ 0.20504706]\n",
      " [-1.11244394]\n",
      " [-0.6915657 ]\n",
      " ...\n",
      " [-0.44793435]\n",
      " [ 0.00807339]\n",
      " [-0.38521454]]\n",
      "Current iteration=8, loss=35837.38154599632\n",
      "t [[ 0.20995532]\n",
      " [-1.17545827]\n",
      " [-0.7326956 ]\n",
      " ...\n",
      " [-0.4707943 ]\n",
      " [-0.005413  ]\n",
      " [-0.41535584]]\n",
      "t [[ 0.20995532]\n",
      " [-1.17545827]\n",
      " [-0.7326956 ]\n",
      " ...\n",
      " [-0.4707943 ]\n",
      " [-0.005413  ]\n",
      " [-0.41535584]]\n",
      "t [[ 0.21353715]\n",
      " [-1.23225943]\n",
      " [-0.77054165]\n",
      " ...\n",
      " [-0.49194235]\n",
      " [-0.01980488]\n",
      " [-0.44318417]]\n",
      "loss=35400.27400438801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00825369]\n",
      " [-0.05081688]\n",
      " [-0.15931622]\n",
      " ...\n",
      " [-0.12225581]\n",
      " [ 0.02376736]\n",
      " [-0.07252757]]\n",
      "t [[ 0.00825369]\n",
      " [-0.05081688]\n",
      " [-0.15931622]\n",
      " ...\n",
      " [-0.12225581]\n",
      " [ 0.02376736]\n",
      " [-0.07252757]]\n",
      "t [[ 0.0100971 ]\n",
      " [-0.11144924]\n",
      " [-0.27561378]\n",
      " ...\n",
      " [-0.20869676]\n",
      " [ 0.03520911]\n",
      " [-0.13520976]]\n",
      "t [[ 0.0100971 ]\n",
      " [-0.11144924]\n",
      " [-0.27561378]\n",
      " ...\n",
      " [-0.20869676]\n",
      " [ 0.03520911]\n",
      " [-0.13520976]]\n",
      "Current iteration=2, loss=38260.06264382892\n",
      "t [[ 0.00818526]\n",
      " [-0.17663371]\n",
      " [-0.36401056]\n",
      " ...\n",
      " [-0.27212224]\n",
      " [ 0.038324  ]\n",
      " [-0.19030455]]\n",
      "t [[ 0.00818526]\n",
      " [-0.17663371]\n",
      " [-0.36401056]\n",
      " ...\n",
      " [-0.27212224]\n",
      " [ 0.038324  ]\n",
      " [-0.19030455]]\n",
      "t [[ 0.00415636]\n",
      " [-0.24319959]\n",
      " [-0.4340829 ]\n",
      " ...\n",
      " [-0.32064215]\n",
      " [ 0.03572503]\n",
      " [-0.23938734]]\n",
      "t [[ 0.00415636]\n",
      " [-0.24319959]\n",
      " [-0.4340829 ]\n",
      " ...\n",
      " [-0.32064215]\n",
      " [ 0.03572503]\n",
      " [-0.23938734]]\n",
      "Current iteration=4, loss=37124.98102912007\n",
      "t [[-0.00100335]\n",
      " [-0.30930084]\n",
      " [-0.49182356]\n",
      " ...\n",
      " [-0.3593291 ]\n",
      " [ 0.02910943]\n",
      " [-0.28356317]]\n",
      "t [[-0.00100335]\n",
      " [-0.30930084]\n",
      " [-0.49182356]\n",
      " ...\n",
      " [-0.3593291 ]\n",
      " [ 0.02910943]\n",
      " [-0.28356317]]\n",
      "t [[-0.00669643]\n",
      " [-0.37388072]\n",
      " [-0.54104475]\n",
      " ...\n",
      " [-0.39140144]\n",
      " [ 0.01961526]\n",
      " [-0.32363385]]\n",
      "t [[-0.00669643]\n",
      " [-0.37388072]\n",
      " [-0.54104475]\n",
      " ...\n",
      " [-0.39140144]\n",
      " [ 0.01961526]\n",
      " [-0.32363385]]\n",
      "Current iteration=6, loss=36349.30015674265\n",
      "t [[-0.01255916]\n",
      " [-0.43635473]\n",
      " [-0.58421939]\n",
      " ...\n",
      " [-0.41893561]\n",
      " [ 0.0080339 ]\n",
      " [-0.36020257]]\n",
      "t [[-0.01255916]\n",
      " [-0.43635473]\n",
      " [-0.58421939]\n",
      " ...\n",
      " [-0.41893561]\n",
      " [ 0.0080339 ]\n",
      " [-0.36020257]]\n",
      "t [[-0.01837127]\n",
      " [-0.49642464]\n",
      " [-0.6229794 ]\n",
      " ...\n",
      " [-0.44329006]\n",
      " [-0.00506614]\n",
      " [-0.3937378 ]]\n",
      "t [[-0.01837127]\n",
      " [-0.49642464]\n",
      " [-0.6229794 ]\n",
      " ...\n",
      " [-0.44329006]\n",
      " [-0.00506614]\n",
      " [-0.3937378 ]]\n",
      "Current iteration=8, loss=35773.05682734818\n",
      "t [[-0.02400203]\n",
      " [-0.5539671 ]\n",
      " [-0.65841752]\n",
      " ...\n",
      " [-0.46536311]\n",
      " [-0.01926542]\n",
      " [-0.42461326]]\n",
      "t [[-0.02400203]\n",
      " [-0.5539671 ]\n",
      " [-0.65841752]\n",
      " ...\n",
      " [-0.46536311]\n",
      " [-0.01926542]\n",
      " [-0.42461326]]\n",
      "t [[-0.02937724]\n",
      " [-0.60896536]\n",
      " [-0.69127521]\n",
      " ...\n",
      " [-0.48575382]\n",
      " [-0.03424764]\n",
      " [-0.45313369]]\n",
      "loss=35327.53692150705\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00704957]\n",
      " [-0.04459993]\n",
      " [-0.16104166]\n",
      " ...\n",
      " [-0.1202113 ]\n",
      " [ 0.02555455]\n",
      " [-0.06803117]]\n",
      "t [[ 0.00704957]\n",
      " [-0.04459993]\n",
      " [-0.16104166]\n",
      " ...\n",
      " [-0.1202113 ]\n",
      " [ 0.02555455]\n",
      " [-0.06803117]]\n",
      "t [[ 0.00775145]\n",
      " [-0.09947529]\n",
      " [-0.27914909]\n",
      " ...\n",
      " [-0.2050938 ]\n",
      " [ 0.03866006]\n",
      " [-0.12654896]]\n",
      "t [[ 0.00775145]\n",
      " [-0.09947529]\n",
      " [-0.27914909]\n",
      " ...\n",
      " [-0.2050938 ]\n",
      " [ 0.03866006]\n",
      " [-0.12654896]]\n",
      "Current iteration=2, loss=38283.13616899905\n",
      "t [[ 0.00477472]\n",
      " [-0.15934287]\n",
      " [-0.36929752]\n",
      " ...\n",
      " [-0.26724844]\n",
      " [ 0.04330167]\n",
      " [-0.17776446]]\n",
      "t [[ 0.00477472]\n",
      " [-0.15934287]\n",
      " [-0.36929752]\n",
      " ...\n",
      " [-0.26724844]\n",
      " [ 0.04330167]\n",
      " [-0.17776446]]\n",
      "t [[-2.33899936e-04]\n",
      " [-2.20993967e-01]\n",
      " [-4.41007344e-01]\n",
      " ...\n",
      " [-3.14682088e-01]\n",
      " [ 4.20917209e-02]\n",
      " [-2.23218042e-01]]\n",
      "t [[-2.33899936e-04]\n",
      " [-2.20993967e-01]\n",
      " [-4.41007344e-01]\n",
      " ...\n",
      " [-3.14682088e-01]\n",
      " [ 4.20917209e-02]\n",
      " [-2.23218042e-01]]\n",
      "Current iteration=4, loss=37164.99211522563\n",
      "t [[-0.00628416]\n",
      " [-0.2825412 ]\n",
      " [-0.50025689]\n",
      " ...\n",
      " [-0.35241393]\n",
      " [ 0.03673312]\n",
      " [-0.2639876 ]]\n",
      "t [[-0.00628416]\n",
      " [-0.2825412 ]\n",
      " [-0.50025689]\n",
      " ...\n",
      " [-0.35241393]\n",
      " [ 0.03673312]\n",
      " [-0.2639876 ]]\n",
      "t [[-0.01277823]\n",
      " [-0.34288953]\n",
      " [-0.55086098]\n",
      " ...\n",
      " [-0.38363312]\n",
      " [ 0.0283718 ]\n",
      " [-0.30085313]]\n",
      "t [[-0.01277823]\n",
      " [-0.34288953]\n",
      " [-0.55086098]\n",
      " ...\n",
      " [-0.38363312]\n",
      " [ 0.0283718 ]\n",
      " [-0.30085313]]\n",
      "Current iteration=6, loss=36403.04471542586\n",
      "t [[-0.01935457]\n",
      " [-0.4014207 ]\n",
      " [-0.59530146]\n",
      " ...\n",
      " [-0.41039902]\n",
      " [ 0.0178077 ]\n",
      " [-0.33439979]]\n",
      "t [[-0.01935457]\n",
      " [-0.4014207 ]\n",
      " [-0.59530146]\n",
      " ...\n",
      " [-0.41039902]\n",
      " [ 0.0178077 ]\n",
      " [-0.33439979]]\n",
      "t [[-0.02579679]\n",
      " [-0.4578072 ]\n",
      " [-0.63522107]\n",
      " ...\n",
      " [-0.43405924]\n",
      " [ 0.00561795]\n",
      " [-0.36508073]]\n",
      "t [[-0.02579679]\n",
      " [-0.4578072 ]\n",
      " [-0.63522107]\n",
      " ...\n",
      " [-0.43405924]\n",
      " [ 0.00561795]\n",
      " [-0.36508073]]\n",
      "Current iteration=8, loss=35837.80967688562\n",
      "t [[-0.0319791 ]\n",
      " [-0.51190032]\n",
      " [-0.67172333]\n",
      " ...\n",
      " [-0.45550458]\n",
      " [-0.00776949]\n",
      " [-0.3932564 ]]\n",
      "t [[-0.0319791 ]\n",
      " [-0.51190032]\n",
      " [-0.67172333]\n",
      " ...\n",
      " [-0.45550458]\n",
      " [-0.00776949]\n",
      " [-0.3932564 ]]\n",
      "t [[-0.03783276]\n",
      " [-0.56366127]\n",
      " [-0.70555967]\n",
      " ...\n",
      " [-0.47532835]\n",
      " [-0.02203004]\n",
      " [-0.41921994]]\n",
      "loss=35401.04928661352\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00587087]\n",
      " [-0.04723903]\n",
      " [-0.15997906]\n",
      " ...\n",
      " [-0.07043467]\n",
      " [-0.00848297]\n",
      " [-0.14369813]]\n",
      "t [[ 0.00587087]\n",
      " [-0.04723903]\n",
      " [-0.15997906]\n",
      " ...\n",
      " [-0.07043467]\n",
      " [-0.00848297]\n",
      " [-0.14369813]]\n",
      "t [[ 0.0055195 ]\n",
      " [-0.1046796 ]\n",
      " [-0.27666965]\n",
      " ...\n",
      " [-0.11312647]\n",
      " [-0.00778883]\n",
      " [-0.27121934]]\n",
      "t [[ 0.0055195 ]\n",
      " [-0.1046796 ]\n",
      " [-0.27666965]\n",
      " ...\n",
      " [-0.11312647]\n",
      " [-0.00778883]\n",
      " [-0.27121934]]\n",
      "Current iteration=2, loss=38248.168734380815\n",
      "t [[ 0.00160291]\n",
      " [-0.16700591]\n",
      " [-0.36524439]\n",
      " ...\n",
      " [-0.13871659]\n",
      " [-0.00220605]\n",
      " [-0.38584772]]\n",
      "t [[ 0.00160291]\n",
      " [-0.16700591]\n",
      " [-0.36524439]\n",
      " ...\n",
      " [-0.13871659]\n",
      " [-0.00220605]\n",
      " [-0.38584772]]\n",
      "t [[-0.0042458 ]\n",
      " [-0.23099861]\n",
      " [-0.43533427]\n",
      " ...\n",
      " [-0.15386059]\n",
      " [ 0.00568379]\n",
      " [-0.4899885 ]]\n",
      "t [[-0.0042458 ]\n",
      " [-0.23099861]\n",
      " [-0.43533427]\n",
      " ...\n",
      " [-0.15386059]\n",
      " [ 0.00568379]\n",
      " [-0.4899885 ]]\n",
      "Current iteration=4, loss=37111.14016257928\n",
      "t [[-0.01104953]\n",
      " [-0.29477029]\n",
      " [-0.49297674]\n",
      " ...\n",
      " [-0.16265045]\n",
      " [ 0.01437447]\n",
      " [-0.58540525]]\n",
      "t [[-0.01104953]\n",
      " [-0.29477029]\n",
      " [-0.49297674]\n",
      " ...\n",
      " [-0.16265045]\n",
      " [ 0.01437447]\n",
      " [-0.58540525]]\n",
      "t [[-0.01822233]\n",
      " [-0.35722982]\n",
      " [-0.54201812]\n",
      " ...\n",
      " [-0.16762315]\n",
      " [ 0.02300322]\n",
      " [-0.67342344]]\n",
      "t [[-0.01822233]\n",
      " [-0.35722982]\n",
      " [-0.54201812]\n",
      " ...\n",
      " [-0.16762315]\n",
      " [ 0.02300322]\n",
      " [-0.67342344]]\n",
      "Current iteration=6, loss=36335.49311570435\n",
      "t [[-0.02541303]\n",
      " [-0.41776418]\n",
      " [-0.58495687]\n",
      " ...\n",
      " [-0.17036541]\n",
      " [ 0.03109344]\n",
      " [-0.75506715]]\n",
      "t [[-0.02541303]\n",
      " [-0.41776418]\n",
      " [-0.58495687]\n",
      " ...\n",
      " [-0.17036541]\n",
      " [ 0.03109344]\n",
      " [-0.75506715]]\n",
      "t [[-0.03241408]\n",
      " [-0.47605149]\n",
      " [-0.62344381]\n",
      " ...\n",
      " [-0.17187239]\n",
      " [ 0.03840223]\n",
      " [-0.83114758]]\n",
      "t [[-0.03241408]\n",
      " [-0.47605149]\n",
      " [-0.62344381]\n",
      " ...\n",
      " [-0.17187239]\n",
      " [ 0.03840223]\n",
      " [-0.83114758]]\n",
      "Current iteration=8, loss=35759.48690468999\n",
      "t [[-0.03910714]\n",
      " [-0.53194871]\n",
      " [-0.65858565]\n",
      " ...\n",
      " [-0.17276511]\n",
      " [ 0.04482821]\n",
      " [-0.90232124]]\n",
      "t [[-0.03910714]\n",
      " [-0.53194871]\n",
      " [-0.65858565]\n",
      " ...\n",
      " [-0.17276511]\n",
      " [ 0.04482821]\n",
      " [-0.90232124]]\n",
      "t [[-0.04542976]\n",
      " [-0.58542263]\n",
      " [-0.69113399]\n",
      " ...\n",
      " [-0.17342549]\n",
      " [ 0.0503546 ]\n",
      " [-0.96912905]]\n",
      "loss=35314.087216708795\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0653091 ]\n",
      " [-0.278993  ]\n",
      " [-0.17535714]\n",
      " ...\n",
      " [-0.12709046]\n",
      " [ 0.02751578]\n",
      " [-0.07408706]]\n",
      "t [[ 0.0653091 ]\n",
      " [-0.278993  ]\n",
      " [-0.17535714]\n",
      " ...\n",
      " [-0.12709046]\n",
      " [ 0.02751578]\n",
      " [-0.07408706]]\n",
      "t [[ 0.110402  ]\n",
      " [-0.48858307]\n",
      " [-0.30509735]\n",
      " ...\n",
      " [-0.21602432]\n",
      " [ 0.0412944 ]\n",
      " [-0.1376953 ]]\n",
      "t [[ 0.110402  ]\n",
      " [-0.48858307]\n",
      " [-0.30509735]\n",
      " ...\n",
      " [-0.21602432]\n",
      " [ 0.0412944 ]\n",
      " [-0.1376953 ]]\n",
      "Current iteration=2, loss=38218.17329570747\n",
      "t [[ 0.14201576]\n",
      " [-0.65186397]\n",
      " [-0.40524756]\n",
      " ...\n",
      " [-0.28087985]\n",
      " [ 0.04586106]\n",
      " [-0.19331162]]\n",
      "t [[ 0.14201576]\n",
      " [-0.65186397]\n",
      " [-0.40524756]\n",
      " ...\n",
      " [-0.28087985]\n",
      " [ 0.04586106]\n",
      " [-0.19331162]]\n",
      "t [[ 0.1645181 ]\n",
      " [-0.78348399]\n",
      " [-0.48581597]\n",
      " ...\n",
      " [-0.33039772]\n",
      " [ 0.04413157]\n",
      " [-0.24264428]]\n",
      "t [[ 0.1645181 ]\n",
      " [-0.78348399]\n",
      " [-0.48581597]\n",
      " ...\n",
      " [-0.33039772]\n",
      " [ 0.04413157]\n",
      " [-0.24264428]]\n",
      "Current iteration=4, loss=37083.96506793646\n",
      "t [[ 0.1807242 ]\n",
      " [-0.89269386]\n",
      " [-0.55300036]\n",
      " ...\n",
      " [-0.36994392]\n",
      " [ 0.037982  ]\n",
      " [-0.28687698]]\n",
      "t [[ 0.1807242 ]\n",
      " [-0.89269386]\n",
      " [-0.55300036]\n",
      " ...\n",
      " [-0.36994392]\n",
      " [ 0.037982  ]\n",
      " [-0.28687698]]\n",
      "t [[ 0.19248782]\n",
      " [-0.98549493]\n",
      " [-0.61072357]\n",
      " ...\n",
      " [-0.40286614]\n",
      " [ 0.02866437]\n",
      " [-0.32686132]]\n",
      "t [[ 0.19248782]\n",
      " [-0.98549493]\n",
      " [-0.61072357]\n",
      " ...\n",
      " [-0.40286614]\n",
      " [ 0.02866437]\n",
      " [-0.32686132]]\n",
      "Current iteration=6, loss=36316.4310961592\n",
      "t [[ 0.20105935]\n",
      " [-1.06590136]\n",
      " [-0.66153067]\n",
      " ...\n",
      " [-0.43129011]\n",
      " [ 0.01704796]\n",
      " [-0.36323415]]\n",
      "t [[ 0.20105935]\n",
      " [-1.06590136]\n",
      " [-0.66153067]\n",
      " ...\n",
      " [-0.43129011]\n",
      " [ 0.01704796]\n",
      " [-0.36323415]]\n",
      "t [[ 0.20730068]\n",
      " [-1.13667582]\n",
      " [-0.70711032]\n",
      " ...\n",
      " [-0.45658487]\n",
      " [ 0.00375734]\n",
      " [-0.39648792]]\n",
      "t [[ 0.20730068]\n",
      " [-1.13667582]\n",
      " [-0.70711032]\n",
      " ...\n",
      " [-0.45658487]\n",
      " [ 0.00375734]\n",
      " [-0.39648792]]\n",
      "Current iteration=8, loss=35750.43603674307\n",
      "t [[ 0.21181738]\n",
      " [-1.19977001]\n",
      " [-0.74860612]\n",
      " ...\n",
      " [-0.47964167]\n",
      " [-0.01074613]\n",
      " [-0.42701423]]\n",
      "t [[ 0.21181738]\n",
      " [-1.19977001]\n",
      " [-0.74860612]\n",
      " ...\n",
      " [-0.47964167]\n",
      " [-0.01074613]\n",
      " [-0.42701423]]\n",
      "t [[ 0.21504227]\n",
      " [-1.25659695]\n",
      " [-0.78680808]\n",
      " ...\n",
      " [-0.50104562]\n",
      " [-0.02611409]\n",
      " [-0.45513156]]\n",
      "loss=35315.495930547295\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00861255]\n",
      " [-0.05302631]\n",
      " [-0.16624301]\n",
      " ...\n",
      " [-0.12757128]\n",
      " [ 0.02480073]\n",
      " [-0.07568094]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.00861255]\n",
      " [-0.05302631]\n",
      " [-0.16624301]\n",
      " ...\n",
      " [-0.12757128]\n",
      " [ 0.02480073]\n",
      " [-0.07568094]]\n",
      "t [[ 0.01025051]\n",
      " [-0.1167326 ]\n",
      " [-0.28567025]\n",
      " ...\n",
      " [-0.21616606]\n",
      " [ 0.03618944]\n",
      " [-0.1406461 ]]\n",
      "t [[ 0.01025051]\n",
      " [-0.1167326 ]\n",
      " [-0.28567025]\n",
      " ...\n",
      " [-0.21616606]\n",
      " [ 0.03618944]\n",
      " [-0.1406461 ]]\n",
      "Current iteration=2, loss=38193.73765217075\n",
      "t [[ 0.00792724]\n",
      " [-0.18514533]\n",
      " [-0.37544554]\n",
      " ...\n",
      " [-0.28031627]\n",
      " [ 0.03870544]\n",
      " [-0.19746038]]\n",
      "t [[ 0.00792724]\n",
      " [-0.18514533]\n",
      " [-0.37544554]\n",
      " ...\n",
      " [-0.28031627]\n",
      " [ 0.03870544]\n",
      " [-0.19746038]]\n",
      "t [[ 0.00344793]\n",
      " [-0.25477474]\n",
      " [-0.44614787]\n",
      " ...\n",
      " [-0.32898169]\n",
      " [ 0.03523926]\n",
      " [-0.24787547]]\n",
      "t [[ 0.00344793]\n",
      " [-0.25477474]\n",
      " [-0.44614787]\n",
      " ...\n",
      " [-0.32898169]\n",
      " [ 0.03523926]\n",
      " [-0.24787547]]\n",
      "Current iteration=4, loss=37042.69133339341\n",
      "t [[-0.00212993]\n",
      " [-0.32365112]\n",
      " [-0.50423761]\n",
      " ...\n",
      " [-0.36763019]\n",
      " [ 0.02762975]\n",
      " [-0.29310024]]\n",
      "t [[-0.00212993]\n",
      " [-0.32365112]\n",
      " [-0.50423761]\n",
      " ...\n",
      " [-0.36763019]\n",
      " [ 0.02762975]\n",
      " [-0.29310024]]\n",
      "t [[-0.00818202]\n",
      " [-0.39068431]\n",
      " [-0.55373827]\n",
      " ...\n",
      " [-0.39965636]\n",
      " [ 0.01709127]\n",
      " [-0.33400221]]\n",
      "t [[-0.00818202]\n",
      " [-0.39068431]\n",
      " [-0.55373827]\n",
      " ...\n",
      " [-0.39965636]\n",
      " [ 0.01709127]\n",
      " [-0.33400221]]\n",
      "Current iteration=6, loss=36261.2997977614\n",
      "t [[-0.01433825]\n",
      " [-0.45529725]\n",
      " [-0.59721018]\n",
      " ...\n",
      " [-0.42720664]\n",
      " [ 0.00445871]\n",
      " [-0.37122918]]\n",
      "t [[-0.01433825]\n",
      " [-0.45529725]\n",
      " [-0.59721018]\n",
      " ...\n",
      " [-0.42720664]\n",
      " [ 0.00445871]\n",
      " [-0.37122918]]\n",
      "t [[-0.02038126]\n",
      " [-0.51721662]\n",
      " [-0.63631275]\n",
      " ...\n",
      " [-0.45165821]\n",
      " [-0.00967358]\n",
      " [-0.40528171]]\n",
      "t [[-0.02038126]\n",
      " [-0.51721662]\n",
      " [-0.63631275]\n",
      " ...\n",
      " [-0.45165821]\n",
      " [-0.00967358]\n",
      " [-0.40528171]]\n",
      "Current iteration=8, loss=35684.42854058517\n",
      "t [[-0.02618707]\n",
      " [-0.57634991]\n",
      " [-0.67213844]\n",
      " ...\n",
      " [-0.4739045 ]\n",
      " [-0.02487042]\n",
      " [-0.43655767]]\n",
      "t [[-0.02618707]\n",
      " [-0.57634991]\n",
      " [-0.67213844]\n",
      " ...\n",
      " [-0.4739045 ]\n",
      " [-0.02487042]\n",
      " [-0.43655767]]\n",
      "t [[-0.03168932]\n",
      " [-0.63271161]\n",
      " [-0.70541683]\n",
      " ...\n",
      " [-0.49453016]\n",
      " [-0.04080592]\n",
      " [-0.46538065]]\n",
      "loss=35241.136133186745\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00735607]\n",
      " [-0.04653906]\n",
      " [-0.16804348]\n",
      " ...\n",
      " [-0.12543788]\n",
      " [ 0.02666562]\n",
      " [-0.07098905]]\n",
      "t [[ 0.00735607]\n",
      " [-0.04653906]\n",
      " [-0.16804348]\n",
      " ...\n",
      " [-0.12543788]\n",
      " [ 0.02666562]\n",
      " [-0.07098905]]\n",
      "t [[ 0.00780561]\n",
      " [-0.10425913]\n",
      " [-0.28936255]\n",
      " ...\n",
      " [-0.21242802]\n",
      " [ 0.03978459]\n",
      " [-0.13162376]]\n",
      "t [[ 0.00780561]\n",
      " [-0.10425913]\n",
      " [-0.28936255]\n",
      " ...\n",
      " [-0.21242802]\n",
      " [ 0.03978459]\n",
      " [-0.13162376]]\n",
      "Current iteration=2, loss=38217.65740314001\n",
      "t [[ 0.00437792]\n",
      " [-0.16716354]\n",
      " [-0.38095979]\n",
      " ...\n",
      " [-0.27527745]\n",
      " [ 0.04388101]\n",
      " [-0.18441563]]\n",
      "t [[ 0.00437792]\n",
      " [-0.16716354]\n",
      " [-0.38095979]\n",
      " ...\n",
      " [-0.27527745]\n",
      " [ 0.04388101]\n",
      " [-0.18441563]]\n",
      "t [[-0.00111229]\n",
      " [-0.23171837]\n",
      " [-0.45335523]\n",
      " ...\n",
      " [-0.32283388]\n",
      " [ 0.0418455 ]\n",
      " [-0.23107714]]\n",
      "t [[-0.00111229]\n",
      " [-0.23171837]\n",
      " [-0.45335523]\n",
      " ...\n",
      " [-0.32283388]\n",
      " [ 0.0418455 ]\n",
      " [-0.23107714]]\n",
      "Current iteration=4, loss=37084.054188620124\n",
      "t [[-0.00760343]\n",
      " [-0.29590687]\n",
      " [-0.51299653]\n",
      " ...\n",
      " [-0.36050906]\n",
      " [ 0.03552389]\n",
      " [-0.27278713]]\n",
      "t [[-0.00760343]\n",
      " [-0.29590687]\n",
      " [-0.51299653]\n",
      " ...\n",
      " [-0.36050906]\n",
      " [ 0.03552389]\n",
      " [-0.27278713]]\n",
      "t [[-0.01447125]\n",
      " [-0.35859598]\n",
      " [-0.56391228]\n",
      " ...\n",
      " [-0.39166767]\n",
      " [ 0.02613971]\n",
      " [-0.31038918]]\n",
      "t [[-0.01447125]\n",
      " [-0.35859598]\n",
      " [-0.56391228]\n",
      " ...\n",
      " [-0.39166767]\n",
      " [ 0.02613971]\n",
      " [-0.31038918]]\n",
      "Current iteration=6, loss=36316.67568378415\n",
      "t [[-0.02134859]\n",
      " [-0.41917133]\n",
      " [-0.608674  ]\n",
      " ...\n",
      " [-0.41843894]\n",
      " [ 0.01453765]\n",
      " [-0.34451135]]\n",
      "t [[-0.02134859]\n",
      " [-0.41917133]\n",
      " [-0.608674  ]\n",
      " ...\n",
      " [-0.41843894]\n",
      " [ 0.01453765]\n",
      " [-0.34451135]]\n",
      "t [[-0.02802293]\n",
      " [-0.47732742]\n",
      " [-0.64895372]\n",
      " ...\n",
      " [-0.44218917]\n",
      " [ 0.00132184]\n",
      " [-0.37563749]]\n",
      "t [[-0.02802293]\n",
      " [-0.47732742]\n",
      " [-0.64895372]\n",
      " ...\n",
      " [-0.44218917]\n",
      " [ 0.00132184]\n",
      " [-0.37563749]]\n",
      "Current iteration=8, loss=35750.909902476415\n",
      "t [[-0.03437617]\n",
      " [-0.53294412]\n",
      " [-0.68585607]\n",
      " ...\n",
      " [-0.46380415]\n",
      " [-0.01306291]\n",
      " [-0.40415102]]\n",
      "t [[-0.03437617]\n",
      " [-0.53294412]\n",
      " [-0.68585607]\n",
      " ...\n",
      " [-0.46380415]\n",
      " [-0.01306291]\n",
      " [-0.40415102]]\n",
      "t [[-0.04034833]\n",
      " [-0.58601201]\n",
      " [-0.72012169]\n",
      " ...\n",
      " [-0.48386254]\n",
      " [-0.02828138]\n",
      " [-0.43036285]]\n",
      "loss=35316.35935254417\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00612613]\n",
      " [-0.0492929 ]\n",
      " [-0.16693467]\n",
      " ...\n",
      " [-0.07349705]\n",
      " [-0.0088518 ]\n",
      " [-0.14994587]]\n",
      "t [[ 0.00612613]\n",
      " [-0.0492929 ]\n",
      " [-0.16693467]\n",
      " ...\n",
      " [-0.07349705]\n",
      " [-0.0088518 ]\n",
      " [-0.14994587]]\n",
      "t [[ 0.00548223]\n",
      " [-0.10968603]\n",
      " [-0.28675984]\n",
      " ...\n",
      " [-0.11680244]\n",
      " [-0.00771603]\n",
      " [-0.28228387]]\n",
      "t [[ 0.00548223]\n",
      " [-0.10968603]\n",
      " [-0.28675984]\n",
      " ...\n",
      " [-0.11680244]\n",
      " [-0.00771603]\n",
      " [-0.28228387]]\n",
      "Current iteration=2, loss=38181.59981881005\n",
      "t [[ 0.00108396]\n",
      " [-0.17514613]\n",
      " [-0.37670181]\n",
      " ...\n",
      " [-0.14199684]\n",
      " [-0.00146065]\n",
      " [-0.4007468 ]]\n",
      "t [[ 0.00108396]\n",
      " [-0.17514613]\n",
      " [-0.37670181]\n",
      " ...\n",
      " [-0.14199684]\n",
      " [-0.00146065]\n",
      " [-0.4007468 ]]\n",
      "t [[-0.00526948]\n",
      " [-0.24212852]\n",
      " [-0.44740138]\n",
      " ...\n",
      " [-0.15642681]\n",
      " [ 0.00707155]\n",
      " [-0.50801788]]\n",
      "t [[-0.00526948]\n",
      " [-0.24212852]\n",
      " [-0.44740138]\n",
      " ...\n",
      " [-0.15642681]\n",
      " [ 0.00707155]\n",
      " [-0.50801788]]\n",
      "Current iteration=4, loss=37028.80038870704\n",
      "t [[-0.01253139]\n",
      " [-0.30861728]\n",
      " [-0.50536849]\n",
      " ...\n",
      " [-0.16449292]\n",
      " [ 0.01627271]\n",
      " [-0.60603582]]\n",
      "t [[-0.01253139]\n",
      " [-0.30861728]\n",
      " [-0.50536849]\n",
      " ...\n",
      " [-0.16449292]\n",
      " [ 0.01627271]\n",
      " [-0.60603582]]\n",
      "t [[-0.0200908 ]\n",
      " [-0.37348423]\n",
      " [-0.55466384]\n",
      " ...\n",
      " [-0.16885842]\n",
      " [ 0.0252529 ]\n",
      " [-0.69624251]]\n",
      "t [[-0.0200908 ]\n",
      " [-0.37348423]\n",
      " [-0.55466384]\n",
      " ...\n",
      " [-0.16885842]\n",
      " [ 0.0252529 ]\n",
      " [-0.69624251]]\n",
      "Current iteration=6, loss=36247.52362134874\n",
      "t [[-0.02759204]\n",
      " [-0.4361211 ]\n",
      " [-0.59787509]\n",
      " ...\n",
      " [-0.17114947]\n",
      " [ 0.03354217]\n",
      " [-0.77974361]]\n",
      "t [[-0.02759204]\n",
      " [-0.4361211 ]\n",
      " [-0.59787509]\n",
      " ...\n",
      " [-0.17114947]\n",
      " [ 0.03354217]\n",
      " [-0.77974361]]\n",
      "t [[-0.03483224]\n",
      " [-0.49622885]\n",
      " [-0.63668163]\n",
      " ...\n",
      " [-0.17236012]\n",
      " [ 0.0409182 ]\n",
      " [-0.85741011]]\n",
      "t [[-0.03483224]\n",
      " [-0.49622885]\n",
      " [-0.63668163]\n",
      " ...\n",
      " [-0.17236012]\n",
      " [ 0.0409182 ]\n",
      " [-0.85741011]]\n",
      "Current iteration=8, loss=35670.89449494463\n",
      "t [[-0.0417014 ]\n",
      " [-0.55369377]\n",
      " [-0.67219034]\n",
      " ...\n",
      " [-0.17309281]\n",
      " [ 0.04730442]\n",
      " [-0.92994387]]\n",
      "t [[-0.0417014 ]\n",
      " [-0.55369377]\n",
      " [-0.67219034]\n",
      " ...\n",
      " [-0.17309281]\n",
      " [ 0.04730442]\n",
      " [-0.92994387]]\n",
      "t [[-0.04814628]\n",
      " [-0.60851274]\n",
      " [-0.70514115]\n",
      " ...\n",
      " [-0.1737051 ]\n",
      " [ 0.05270843]\n",
      " [-0.99792124]]\n",
      "loss=35227.7008536502\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.06803032]\n",
      " [-0.29061771]\n",
      " [-0.18266369]\n",
      " ...\n",
      " [-0.1323859 ]\n",
      " [ 0.02866228]\n",
      " [-0.07717402]]\n",
      "t [[ 0.06803032]\n",
      " [-0.29061771]\n",
      " [-0.18266369]\n",
      " ...\n",
      " [-0.1323859 ]\n",
      " [ 0.02866228]\n",
      " [-0.07717402]]\n",
      "t [[ 0.11413604]\n",
      " [-0.50596889]\n",
      " [-0.31585706]\n",
      " ...\n",
      " [-0.22339134]\n",
      " [ 0.04242848]\n",
      " [-0.14298247]]\n",
      "t [[ 0.11413604]\n",
      " [-0.50596889]\n",
      " [-0.31585706]\n",
      " ...\n",
      " [-0.22339134]\n",
      " [ 0.04242848]\n",
      " [-0.14298247]]\n",
      "Current iteration=2, loss=38154.09137152744\n",
      "t [[ 0.14592762]\n",
      " [-0.67213574]\n",
      " [-0.41767953]\n",
      " ...\n",
      " [-0.28891552]\n",
      " [ 0.04640824]\n",
      " [-0.20023526]]\n",
      "t [[ 0.14592762]\n",
      " [-0.67213574]\n",
      " [-0.41767953]\n",
      " ...\n",
      " [-0.28891552]\n",
      " [ 0.04640824]\n",
      " [-0.20023526]]\n",
      "t [[ 0.16821803]\n",
      " [-0.80523444]\n",
      " [-0.49913109]\n",
      " ...\n",
      " [-0.33856281]\n",
      " [ 0.04381094]\n",
      " [-0.2508217 ]]\n",
      "t [[ 0.16821803]\n",
      " [-0.80523444]\n",
      " [-0.49913109]\n",
      " ...\n",
      " [-0.33856281]\n",
      " [ 0.04381094]\n",
      " [-0.2508217 ]]\n",
      "Current iteration=4, loss=37005.497210620175\n",
      "t [[ 0.1840441 ]\n",
      " [-0.91522039]\n",
      " [-0.56686493]\n",
      " ...\n",
      " [-0.37808649]\n",
      " [ 0.03666057]\n",
      " [-0.29603032]]\n",
      "t [[ 0.1840441 ]\n",
      " [-0.91522039]\n",
      " [-0.56686493]\n",
      " ...\n",
      " [-0.37808649]\n",
      " [ 0.03666057]\n",
      " [-0.29603032]]\n",
      "t [[ 0.19537059]\n",
      " [-1.00843909]\n",
      " [-0.62500926]\n",
      " ...\n",
      " [-0.41099892]\n",
      " [ 0.02628869]\n",
      " [-0.33677837]]\n",
      "t [[ 0.19537059]\n",
      " [-1.00843909]\n",
      " [-0.62500926]\n",
      " ...\n",
      " [-0.41099892]\n",
      " [ 0.02628869]\n",
      " [-0.33677837]]\n",
      "Current iteration=6, loss=36233.208978520044\n",
      "t [[ 0.20350279]\n",
      " [-1.08907421]\n",
      " [-0.67619609]\n",
      " ...\n",
      " [-0.43948578]\n",
      " [ 0.01361012]\n",
      " [-0.37374698]]\n",
      "t [[ 0.20350279]\n",
      " [-1.08907421]\n",
      " [-0.67619609]\n",
      " ...\n",
      " [-0.43948578]\n",
      " [ 0.01361012]\n",
      " [-0.37374698]]\n",
      "t [[ 2.09328494e-01]\n",
      " [-1.15997103e+00]\n",
      " [-7.22144791e-01]\n",
      " ...\n",
      " [-4.64927356e-01]\n",
      " [-7.23260962e-04]\n",
      " [-4.07460383e-01]]\n",
      "t [[ 2.09328494e-01]\n",
      " [-1.15997103e+00]\n",
      " [-7.22144791e-01]\n",
      " ...\n",
      " [-4.64927356e-01]\n",
      " [-7.23260962e-04]\n",
      " [-4.07460383e-01]]\n",
      "Current iteration=8, loss=35667.18461223612\n",
      "t [[ 0.21346406]\n",
      " [-1.22312052]\n",
      " [-0.76400389]\n",
      " ...\n",
      " [-0.48820499]\n",
      " [-0.01623332]\n",
      " [-0.43833399]]\n",
      "t [[ 0.21346406]\n",
      " [-1.22312052]\n",
      " [-0.76400389]\n",
      " ...\n",
      " [-0.48820499]\n",
      " [-0.01623332]\n",
      " [-0.43833399]]\n",
      "t [[ 0.21634535]\n",
      " [-1.2799536 ]\n",
      " [-0.80255774]\n",
      " ...\n",
      " [-0.50988633]\n",
      " [-0.03256139]\n",
      " [-0.46670485]]\n",
      "loss=35234.78075776025\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00897141]\n",
      " [-0.05523574]\n",
      " [-0.1731698 ]\n",
      " ...\n",
      " [-0.13288675]\n",
      " [ 0.02583409]\n",
      " [-0.07883432]]\n",
      "t [[ 0.00897141]\n",
      " [-0.05523574]\n",
      " [-0.1731698 ]\n",
      " ...\n",
      " [-0.13288675]\n",
      " [ 0.02583409]\n",
      " [-0.07883432]]\n",
      "t [[ 0.0103808 ]\n",
      " [-0.12205147]\n",
      " [-0.29556938]\n",
      " ...\n",
      " [-0.22350429]\n",
      " [ 0.03712502]\n",
      " [-0.14604615]]\n",
      "t [[ 0.0103808 ]\n",
      " [-0.12205147]\n",
      " [-0.29556938]\n",
      " ...\n",
      " [-0.22350429]\n",
      " [ 0.03712502]\n",
      " [-0.14604615]]\n",
      "Current iteration=2, loss=38128.86291294734\n",
      "t [[ 0.00762979]\n",
      " [-0.19370385]\n",
      " [-0.38658114]\n",
      " ...\n",
      " [-0.28826341]\n",
      " [ 0.03899788]\n",
      " [-0.20453341]]\n",
      "t [[ 0.00762979]\n",
      " [-0.19370385]\n",
      " [-0.38658114]\n",
      " ...\n",
      " [-0.28826341]\n",
      " [ 0.03899788]\n",
      " [-0.20453341]]\n",
      "t [[ 0.00269578]\n",
      " [-0.26637518]\n",
      " [-0.45782289]\n",
      " ...\n",
      " [-0.33700457]\n",
      " [ 0.03463171]\n",
      " [-0.2562328 ]]\n",
      "t [[ 0.00269578]\n",
      " [-0.26637518]\n",
      " [-0.45782289]\n",
      " ...\n",
      " [-0.33700457]\n",
      " [ 0.03463171]\n",
      " [-0.2562328 ]]\n",
      "Current iteration=4, loss=36962.9168009875\n",
      "t [[-0.00329454]\n",
      " [-0.33797802]\n",
      " [-0.51621707]\n",
      " ...\n",
      " [-0.37558593]\n",
      " [ 0.02600762]\n",
      " [-0.30245963]]\n",
      "t [[-0.00329454]\n",
      " [-0.33797802]\n",
      " [-0.51621707]\n",
      " ...\n",
      " [-0.37558593]\n",
      " [ 0.02600762]\n",
      " [-0.30245963]]\n",
      "t [[-0.00969314]\n",
      " [-0.40739716]\n",
      " [-0.5659843 ]\n",
      " ...\n",
      " [-0.40756574]\n",
      " [ 0.01441474]\n",
      " [-0.34414791]]\n",
      "t [[-0.00969314]\n",
      " [-0.40739716]\n",
      " [-0.5659843 ]\n",
      " ...\n",
      " [-0.40756574]\n",
      " [ 0.01441474]\n",
      " [-0.34414791]]\n",
      "Current iteration=6, loss=36176.50666009449\n",
      "t [[-0.01612641]\n",
      " [-0.47407047]\n",
      " [-0.60975892]\n",
      " ...\n",
      " [-0.43514856]\n",
      " [ 0.00072972]\n",
      " [-0.38199029]]\n",
      "t [[-0.01612641]\n",
      " [-0.47407047]\n",
      " [-0.60975892]\n",
      " ...\n",
      " [-0.43514856]\n",
      " [ 0.00072972]\n",
      " [-0.38199029]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.0223821 ]\n",
      " [-0.53775521]\n",
      " [-0.64921767]\n",
      " ...\n",
      " [-0.45972116]\n",
      " [-0.01442879]\n",
      " [-0.41651949]]\n",
      "t [[-0.0223821 ]\n",
      " [-0.53775521]\n",
      " [-0.64921767]\n",
      " ...\n",
      " [-0.45972116]\n",
      " [-0.01442879]\n",
      " [-0.41651949]]\n",
      "Current iteration=8, loss=35599.56305574476\n",
      "t [[-0.02834451]\n",
      " [-0.59839375]\n",
      " [-0.68544576]\n",
      " ...\n",
      " [-0.48216582]\n",
      " [-0.03061109]\n",
      " [-0.44815754]]\n",
      "t [[-0.02834451]\n",
      " [-0.59839375]\n",
      " [-0.68544576]\n",
      " ...\n",
      " [-0.48216582]\n",
      " [-0.03061109]\n",
      " [-0.44815754]]\n",
      "t [[-0.03395606]\n",
      " [-0.6560345 ]\n",
      " [-0.71915665]\n",
      " ...\n",
      " [-0.50304892]\n",
      " [-0.04748274]\n",
      " [-0.47724684]]\n",
      "loss=35158.88375583205\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00766257]\n",
      " [-0.04847819]\n",
      " [-0.17504529]\n",
      " ...\n",
      " [-0.13066446]\n",
      " [ 0.02777669]\n",
      " [-0.07394693]]\n",
      "t [[ 0.00766257]\n",
      " [-0.04847819]\n",
      " [-0.17504529]\n",
      " ...\n",
      " [-0.13066446]\n",
      " [ 0.02777669]\n",
      " [-0.07394693]]\n",
      "t [[ 0.00783686]\n",
      " [-0.10908025]\n",
      " [-0.29941889]\n",
      " ...\n",
      " [-0.2196329 ]\n",
      " [ 0.04086385]\n",
      " [-0.1366635 ]]\n",
      "t [[ 0.00783686]\n",
      " [-0.10908025]\n",
      " [-0.29941889]\n",
      " ...\n",
      " [-0.2196329 ]\n",
      " [ 0.04086385]\n",
      " [-0.1366635 ]]\n",
      "Current iteration=2, loss=38153.620185845575\n",
      "t [[ 0.00394252]\n",
      " [-0.17503604]\n",
      " [-0.39232188]\n",
      " ...\n",
      " [-0.28306263]\n",
      " [ 0.04436973]\n",
      " [-0.19098714]]\n",
      "t [[ 0.00394252]\n",
      " [-0.17503604]\n",
      " [-0.39232188]\n",
      " ...\n",
      " [-0.28306263]\n",
      " [ 0.04436973]\n",
      " [-0.19098714]]\n",
      "t [[-0.00203254]\n",
      " [-0.2424769 ]\n",
      " [-0.46531057]\n",
      " ...\n",
      " [-0.33067299]\n",
      " [ 0.04147437]\n",
      " [-0.2388109 ]]\n",
      "t [[-0.00203254]\n",
      " [-0.2424769 ]\n",
      " [-0.46531057]\n",
      " ...\n",
      " [-0.33067299]\n",
      " [ 0.04147437]\n",
      " [-0.2388109 ]]\n",
      "Current iteration=4, loss=37005.6101819636\n",
      "t [[-0.00895745]\n",
      " [-0.30926223]\n",
      " [-0.52529687]\n",
      " ...\n",
      " [-0.36826357]\n",
      " [ 0.03416729]\n",
      " [-0.28141705]]\n",
      "t [[-0.00895745]\n",
      " [-0.30926223]\n",
      " [-0.52529687]\n",
      " ...\n",
      " [-0.36826357]\n",
      " [ 0.03416729]\n",
      " [-0.28141705]]\n",
      "t [[-0.01618482]\n",
      " [-0.37422915]\n",
      " [-0.57650926]\n",
      " ...\n",
      " [-0.39936222]\n",
      " [ 0.02374822]\n",
      " [-0.31971346]]\n",
      "t [[-0.01618482]\n",
      " [-0.37422915]\n",
      " [-0.57650926]\n",
      " ...\n",
      " [-0.39936222]\n",
      " [ 0.02374822]\n",
      " [-0.31971346]]\n",
      "Current iteration=6, loss=36233.470892692625\n",
      "t [[-0.0233449 ]\n",
      " [-0.43677446]\n",
      " [-0.62159565]\n",
      " ...\n",
      " [-0.42615623]\n",
      " [ 0.0111049 ]\n",
      " [-0.35437128]]\n",
      "t [[-0.0233449 ]\n",
      " [-0.43677446]\n",
      " [-0.62159565]\n",
      " ...\n",
      " [-0.42615623]\n",
      " [ 0.0111049 ]\n",
      " [-0.35437128]]\n",
      "t [[-0.03023132]\n",
      " [-0.49662033]\n",
      " [-0.66224726]\n",
      " ...\n",
      " [-0.45002145]\n",
      " [-0.00313297]\n",
      " [-0.38590505]]\n",
      "t [[-0.03023132]\n",
      " [-0.49662033]\n",
      " [-0.66224726]\n",
      " ...\n",
      " [-0.45002145]\n",
      " [-0.00313297]\n",
      " [-0.38590505]]\n",
      "Current iteration=8, loss=35667.70871146713\n",
      "t [[-0.03673526]\n",
      " [-0.55367915]\n",
      " [-0.69956289]\n",
      " ...\n",
      " [-0.47183236]\n",
      " [-0.01850489]\n",
      " [-0.41472111]]\n",
      "t [[-0.03673526]\n",
      " [-0.55367915]\n",
      " [-0.69956289]\n",
      " ...\n",
      " [-0.47183236]\n",
      " [-0.01850489]\n",
      " [-0.41472111]]\n",
      "t [[-0.04280653]\n",
      " [-0.60797353]\n",
      " [-0.73426814]\n",
      " ...\n",
      " [-0.49214908]\n",
      " [-0.03466596]\n",
      " [-0.44114813]]\n",
      "loss=35235.73740398974\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00638138]\n",
      " [-0.05134677]\n",
      " [-0.17389029]\n",
      " ...\n",
      " [-0.07655942]\n",
      " [-0.00922062]\n",
      " [-0.15619362]]\n",
      "t [[ 0.00638138]\n",
      " [-0.05134677]\n",
      " [-0.17389029]\n",
      " ...\n",
      " [-0.07655942]\n",
      " [-0.00922062]\n",
      " [-0.15619362]]\n",
      "t [[ 0.00542251]\n",
      " [-0.1147294 ]\n",
      " [-0.29669168]\n",
      " ...\n",
      " [-0.12037695]\n",
      " [-0.00760957]\n",
      " [-0.29328859]]\n",
      "t [[ 0.00542251]\n",
      " [-0.1147294 ]\n",
      " [-0.29669168]\n",
      " ...\n",
      " [-0.12037695]\n",
      " [-0.00760957]\n",
      " [-0.29328859]]\n",
      "Current iteration=2, loss=38116.49777693192\n",
      "t [[ 0.00052766]\n",
      " [-0.1833369 ]\n",
      " [-0.38785754]\n",
      " ...\n",
      " [-0.14509416]\n",
      " [-0.00066322]\n",
      " [-0.41550504]]\n",
      "t [[ 0.00052766]\n",
      " [-0.1833369 ]\n",
      " [-0.38785754]\n",
      " ...\n",
      " [-0.14509416]\n",
      " [-0.00066322]\n",
      " [-0.41550504]]\n",
      "t [[-0.00633283]\n",
      " [-0.25328991]\n",
      " [-0.45907515]\n",
      " ...\n",
      " [-0.15877237]\n",
      " [ 0.00850687]\n",
      " [-0.52581948]]\n",
      "t [[-0.00633283]\n",
      " [-0.25328991]\n",
      " [-0.45907515]\n",
      " ...\n",
      " [-0.15877237]\n",
      " [ 0.00850687]\n",
      " [-0.52581948]]\n",
      "Current iteration=4, loss=36948.99033486298\n",
      "t [[-0.01404482]\n",
      " [-0.32244967]\n",
      " [-0.51732162]\n",
      " ...\n",
      " [-0.16611457]\n",
      " [ 0.01819609]\n",
      " [-0.62635158]]\n",
      "t [[-0.01404482]\n",
      " [-0.32244967]\n",
      " [-0.51732162]\n",
      " ...\n",
      " [-0.16611457]\n",
      " [ 0.01819609]\n",
      " [-0.62635158]]\n",
      "t [[-0.02197573]\n",
      " [-0.38965918]\n",
      " [-0.56685782]\n",
      " ...\n",
      " [-0.16989813]\n",
      " [ 0.02749445]\n",
      " [-0.71866198]]\n",
      "t [[-0.02197573]\n",
      " [-0.38965918]\n",
      " [-0.56685782]\n",
      " ...\n",
      " [-0.16989813]\n",
      " [ 0.02749445]\n",
      " [-0.71866198]]\n",
      "Current iteration=6, loss=36162.76484628793\n",
      "t [[-0.02976843]\n",
      " [-0.45432231]\n",
      " [-0.61034725]\n",
      " ...\n",
      " [-0.17177743]\n",
      " [ 0.03594481]\n",
      " [-0.80393889]]\n",
      "t [[-0.02976843]\n",
      " [-0.45432231]\n",
      " [-0.61034725]\n",
      " ...\n",
      " [-0.17177743]\n",
      " [ 0.03594481]\n",
      " [-0.80393889]]\n",
      "t [[-0.03722704]\n",
      " [-0.51616853]\n",
      " [-0.64948755]\n",
      " ...\n",
      " [-0.17273684]\n",
      " [ 0.04335011]\n",
      " [-0.88311352]]\n",
      "t [[-0.03722704]\n",
      " [-0.51616853]\n",
      " [-0.64948755]\n",
      " ...\n",
      " [-0.17273684]\n",
      " [ 0.04335011]\n",
      " [-0.88311352]]\n",
      "Current iteration=8, loss=35586.061658702514\n",
      "t [[-0.04425146]\n",
      " [-0.57511749]\n",
      " [-0.68537874]\n",
      " ...\n",
      " [-0.17335439]\n",
      " [ 0.0496617 ]\n",
      " [-0.95693323]]\n",
      "t [[-0.04425146]\n",
      " [-0.57511749]\n",
      " [-0.68537874]\n",
      " ...\n",
      " [-0.17335439]\n",
      " [ 0.0496617 ]\n",
      " [-0.95693323]]\n",
      "t [[-0.05079868]\n",
      " [-0.63119883]\n",
      " [-0.71874473]\n",
      " ...\n",
      " [-0.1739596 ]\n",
      " [ 0.05491331]\n",
      " [-1.02600985]]\n",
      "loss=35145.45737766702\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07075153]\n",
      " [-0.30224242]\n",
      " [-0.18997024]\n",
      " ...\n",
      " [-0.13768133]\n",
      " [ 0.02980877]\n",
      " [-0.08026098]]\n",
      "t [[ 0.07075153]\n",
      " [-0.30224242]\n",
      " [-0.18997024]\n",
      " ...\n",
      " [-0.13768133]\n",
      " [ 0.02980877]\n",
      " [-0.08026098]]\n",
      "t [[ 0.11780221]\n",
      " [-0.52312197]\n",
      " [-0.32646393]\n",
      " ...\n",
      " [-0.23063037]\n",
      " [ 0.04351683]\n",
      " [-0.14823422]]\n",
      "t [[ 0.11780221]\n",
      " [-0.52312197]\n",
      " [-0.32646393]\n",
      " ...\n",
      " [-0.23063037]\n",
      " [ 0.04351683]\n",
      " [-0.14823422]]\n",
      "Current iteration=2, loss=38091.40355264431\n",
      "t [[ 0.14970664]\n",
      " [-0.69195034]\n",
      " [-0.42981992]\n",
      " ...\n",
      " [-0.2967142 ]\n",
      " [ 0.04686501]\n",
      " [-0.20707884]]\n",
      "t [[ 0.14970664]\n",
      " [-0.69195034]\n",
      " [-0.42981992]\n",
      " ...\n",
      " [-0.2967142 ]\n",
      " [ 0.04686501]\n",
      " [-0.20707884]]\n",
      "t [[ 0.17173998]\n",
      " [-0.82636279]\n",
      " [-0.51206252]\n",
      " ...\n",
      " [-0.34642854]\n",
      " [ 0.04336685]\n",
      " [-0.25887352]]\n",
      "t [[ 0.17173998]\n",
      " [-0.82636279]\n",
      " [-0.51206252]\n",
      " ...\n",
      " [-0.34642854]\n",
      " [ 0.04336685]\n",
      " [-0.25887352]]\n",
      "Current iteration=4, loss=36929.39141457534\n",
      "t [[ 0.18716047]\n",
      " [-0.9370152 ]\n",
      " [-0.58029419]\n",
      " ...\n",
      " [-0.385907  ]\n",
      " [ 0.03519478]\n",
      " [-0.30501393]]\n",
      "t [[ 0.18716047]\n",
      " [-0.9370152 ]\n",
      " [-0.58029419]\n",
      " ...\n",
      " [-0.385907  ]\n",
      " [ 0.03519478]\n",
      " [-0.30501393]]\n",
      "t [[ 0.19803953]\n",
      " [-1.03058209]\n",
      " [-0.63883524]\n",
      " ...\n",
      " [-0.4188133 ]\n",
      " [ 0.02375833]\n",
      " [-0.34648359]]\n",
      "t [[ 0.19803953]\n",
      " [-1.03058209]\n",
      " [-0.63883524]\n",
      " ...\n",
      " [-0.4188133 ]\n",
      " [ 0.02375833]\n",
      " [-0.34648359]]\n",
      "Current iteration=6, loss=36152.97239056144\n",
      "t [[ 0.20573278]\n",
      " [-1.11140229]\n",
      " [-0.69039269]\n",
      " ...\n",
      " [-0.44738106]\n",
      " [ 0.01001615]\n",
      " [-0.38400808]]\n",
      "t [[ 0.20573278]\n",
      " [-1.11140229]\n",
      " [-0.69039269]\n",
      " ...\n",
      " [-0.44738106]\n",
      " [ 0.01001615]\n",
      " [-0.38400808]]\n",
      "t [[ 0.21115016]\n",
      " [-1.182393  ]\n",
      " [-0.73670781]\n",
      " ...\n",
      " [-0.47299301]\n",
      " [-0.00535407]\n",
      " [-0.41814342]]\n",
      "t [[ 0.21115016]\n",
      " [-1.182393  ]\n",
      " [-0.73670781]\n",
      " ...\n",
      " [-0.47299301]\n",
      " [-0.00535407]\n",
      " [-0.41814342]]\n",
      "Current iteration=8, loss=35587.40834865514\n",
      "t [[ 0.21491605]\n",
      " [-1.24557713]\n",
      " [-0.77892829]\n",
      " ...\n",
      " [-0.49651477]\n",
      " [-0.02185865]\n",
      " [-0.44932884]]\n",
      "t [[ 0.21491605]\n",
      " [-1.24557713]\n",
      " [-0.77892829]\n",
      " ...\n",
      " [-0.49651477]\n",
      " [-0.02185865]\n",
      " [-0.44932884]]\n",
      "t [[ 0.21746752]\n",
      " [-1.30239904]\n",
      " [-0.8178295 ]\n",
      " ...\n",
      " [-0.51849311]\n",
      " [-0.0391296 ]\n",
      " [-0.47791995]]\n",
      "loss=35157.86569914028\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00933026]\n",
      " [-0.05744517]\n",
      " [-0.18009659]\n",
      " ...\n",
      " [-0.13820222]\n",
      " [ 0.02686745]\n",
      " [-0.08198769]]\n",
      "t [[ 0.00933026]\n",
      " [-0.05744517]\n",
      " [-0.18009659]\n",
      " ...\n",
      " [-0.13820222]\n",
      " [ 0.02686745]\n",
      " [-0.08198769]]\n",
      "t [[ 0.01048806]\n",
      " [-0.12740569]\n",
      " [-0.30531168]\n",
      " ...\n",
      " [-0.23071187]\n",
      " [ 0.03801602]\n",
      " [-0.15140998]]\n",
      "t [[ 0.01048806]\n",
      " [-0.12740569]\n",
      " [-0.30531168]\n",
      " ...\n",
      " [-0.23071187]\n",
      " [ 0.03801602]\n",
      " [-0.15140998]]\n",
      "Current iteration=2, loss=38065.39140926892\n",
      "t [[ 0.00729424]\n",
      " [-0.20230673]\n",
      " [-0.39742496]\n",
      " ...\n",
      " [-0.2959701 ]\n",
      " [ 0.03920336]\n",
      " [-0.21152484]]\n",
      "t [[ 0.00729424]\n",
      " [-0.20230673]\n",
      " [-0.39742496]\n",
      " ...\n",
      " [-0.2959701 ]\n",
      " [ 0.03920336]\n",
      " [-0.21152484]]\n",
      "t [[ 0.00190275]\n",
      " [-0.27799545]\n",
      " [-0.46912512]\n",
      " ...\n",
      " [-0.34472535]\n",
      " [ 0.03390701]\n",
      " [-0.26446228]]\n",
      "t [[ 0.00190275]\n",
      " [-0.27799545]\n",
      " [-0.46912512]\n",
      " ...\n",
      " [-0.34472535]\n",
      " [ 0.03390701]\n",
      " [-0.26446228]]\n",
      "Current iteration=4, loss=36885.521547711156\n",
      "t [[-0.00449302]\n",
      " [-0.35227389]\n",
      " [-0.52778792]\n",
      " ...\n",
      " [-0.3832183 ]\n",
      " [ 0.02425035]\n",
      " [-0.31164635]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.00449302]\n",
      " [-0.35227389]\n",
      " [-0.52778792]\n",
      " ...\n",
      " [-0.3832183 ]\n",
      " [ 0.02425035]\n",
      " [-0.31164635]]\n",
      "t [[-0.01122483]\n",
      " [-0.42401081]\n",
      " [-0.57781537]\n",
      " ...\n",
      " [-0.41515699]\n",
      " [ 0.01159545]\n",
      " [-0.35407817]]\n",
      "t [[-0.01122483]\n",
      " [-0.42401081]\n",
      " [-0.57781537]\n",
      " ...\n",
      " [-0.41515699]\n",
      " [ 0.01159545]\n",
      " [-0.35407817]]\n",
      "Current iteration=6, loss=36094.74230008634\n",
      "t [[-0.01791847]\n",
      " [-0.49266656]\n",
      " [-0.62190207]\n",
      " ...\n",
      " [-0.44279184]\n",
      " [-0.00314119]\n",
      " [-0.39249537]]\n",
      "t [[-0.01791847]\n",
      " [-0.49266656]\n",
      " [-0.62190207]\n",
      " ...\n",
      " [-0.44279184]\n",
      " [-0.00314119]\n",
      " [-0.39249537]]\n",
      "t [[-0.02436894]\n",
      " [-0.55803456]\n",
      " [-0.6617321 ]\n",
      " ...\n",
      " [-0.46751029]\n",
      " [-0.01931812]\n",
      " [-0.42746284]]\n",
      "t [[-0.02436894]\n",
      " [-0.55803456]\n",
      " [-0.6617321 ]\n",
      " ...\n",
      " [-0.46751029]\n",
      " [-0.01931812]\n",
      " [-0.42746284]]\n",
      "Current iteration=8, loss=35518.23939108678\n",
      "t [[-0.03047018]\n",
      " [-0.62009585]\n",
      " [-0.698377  ]\n",
      " ...\n",
      " [-0.49017765]\n",
      " [-0.03647241]\n",
      " [-0.45942678]]\n",
      "t [[-0.03047018]\n",
      " [-0.62009585]\n",
      " [-0.698377  ]\n",
      " ...\n",
      " [-0.49017765]\n",
      " [-0.03647241]\n",
      " [-0.45942678]]\n",
      "t [[-0.03617427]\n",
      " [-0.67893515]\n",
      " [-0.73253048]\n",
      " ...\n",
      " [-0.51133866]\n",
      " [-0.05426202]\n",
      " [-0.48874842]]\n",
      "loss=35080.51275186524\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00796907]\n",
      " [-0.05041732]\n",
      " [-0.1820471 ]\n",
      " ...\n",
      " [-0.13589104]\n",
      " [ 0.02888776]\n",
      " [-0.0769048 ]]\n",
      "t [[ 0.00796907]\n",
      " [-0.05041732]\n",
      " [-0.1820471 ]\n",
      " ...\n",
      " [-0.13589104]\n",
      " [ 0.02888776]\n",
      " [-0.0769048 ]]\n",
      "t [[ 0.0078453 ]\n",
      " [-0.11393851]\n",
      " [-0.3093186 ]\n",
      " ...\n",
      " [-0.22670887]\n",
      " [ 0.041898  ]\n",
      " [-0.14166828]]\n",
      "t [[ 0.0078453 ]\n",
      " [-0.11393851]\n",
      " [-0.3093186 ]\n",
      " ...\n",
      " [-0.22670887]\n",
      " [ 0.041898  ]\n",
      " [-0.14166828]]\n",
      "Current iteration=2, loss=38090.977885271874\n",
      "t [[ 0.00346984]\n",
      " [-0.18295781]\n",
      " [-0.40339132]\n",
      " ...\n",
      " [-0.29061033]\n",
      " [ 0.04476987]\n",
      " [-0.19748017]]\n",
      "t [[ 0.00346984]\n",
      " [-0.18295781]\n",
      " [-0.40339132]\n",
      " ...\n",
      " [-0.29061033]\n",
      " [ 0.04476987]\n",
      " [-0.19748017]]\n",
      "t [[-0.00299178]\n",
      " [-0.25326399]\n",
      " [-0.47689046]\n",
      " ...\n",
      " [-0.33821381]\n",
      " [ 0.04098298]\n",
      " [-0.24642218]]\n",
      "t [[-0.00299178]\n",
      " [-0.25326399]\n",
      " [-0.47689046]\n",
      " ...\n",
      " [-0.33821381]\n",
      " [ 0.04098298]\n",
      " [-0.24642218]]\n",
      "Current iteration=4, loss=36929.524529668895\n",
      "t [[-0.01034203]\n",
      " [-0.32259943]\n",
      " [-0.53718389]\n",
      " ...\n",
      " [-0.37569926]\n",
      " [ 0.0326707 ]\n",
      " [-0.28988223]]\n",
      "t [[-0.01034203]\n",
      " [-0.32259943]\n",
      " [-0.53718389]\n",
      " ...\n",
      " [-0.37569926]\n",
      " [ 0.0326707 ]\n",
      " [-0.28988223]]\n",
      "t [[-0.01791399]\n",
      " [-0.3897802 ]\n",
      " [-0.58868451]\n",
      " ...\n",
      " [-0.40674397]\n",
      " [ 0.02120719]\n",
      " [-0.32883297]]\n",
      "t [[-0.01791399]\n",
      " [-0.3897802 ]\n",
      " [-0.58868451]\n",
      " ...\n",
      " [-0.40674397]\n",
      " [ 0.02120719]\n",
      " [-0.32883297]]\n",
      "Current iteration=6, loss=36153.25267029535\n",
      "t [[-0.02533839]\n",
      " [-0.45422171]\n",
      " [-0.63410303]\n",
      " ...\n",
      " [-0.43358112]\n",
      " [ 0.00752147]\n",
      " [-0.36398872]]\n",
      "t [[-0.02533839]\n",
      " [-0.45422171]\n",
      " [-0.63410303]\n",
      " ...\n",
      " [-0.43358112]\n",
      " [ 0.00752147]\n",
      " [-0.36398872]]\n",
      "t [[-0.0324172 ]\n",
      " [-0.51567932]\n",
      " [-0.67513992]\n",
      " ...\n",
      " [-0.4575872 ]\n",
      " [-0.00773262]\n",
      " [-0.39589468]]\n",
      "t [[-0.0324172 ]\n",
      " [-0.51567932]\n",
      " [-0.67513992]\n",
      " ...\n",
      " [-0.4575872 ]\n",
      " [-0.00773262]\n",
      " [-0.39589468]]\n",
      "Current iteration=8, loss=35587.98721468911\n",
      "t [[-0.03905242]\n",
      " [-0.57410169]\n",
      " [-0.71288172]\n",
      " ...\n",
      " [-0.47961951]\n",
      " [-0.02408006]\n",
      " [-0.42498009]]\n",
      "t [[-0.03905242]\n",
      " [-0.57410169]\n",
      " [-0.71288172]\n",
      " ...\n",
      " [-0.47961951]\n",
      " [-0.02408006]\n",
      " [-0.42498009]]\n",
      "t [[-0.04520451]\n",
      " [-0.6295458 ]\n",
      " [-0.74803534]\n",
      " ...\n",
      " [-0.50021622]\n",
      " [-0.04116721]\n",
      " [-0.45159132]]\n",
      "loss=35158.92011479841\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00663664]\n",
      " [-0.05340064]\n",
      " [-0.1808459 ]\n",
      " ...\n",
      " [-0.0796218 ]\n",
      " [-0.00958945]\n",
      " [-0.16244136]]\n",
      "t [[ 0.00663664]\n",
      " [-0.05340064]\n",
      " [-0.1808459 ]\n",
      " ...\n",
      " [-0.0796218 ]\n",
      " [-0.00958945]\n",
      " [-0.16244136]]\n",
      "t [[ 0.00534044]\n",
      " [-0.11980955]\n",
      " [-0.3064657 ]\n",
      " ...\n",
      " [-0.12385032]\n",
      " [-0.00746957]\n",
      " [-0.30423364]]\n",
      "t [[ 0.00534044]\n",
      " [-0.11980955]\n",
      " [-0.3064657 ]\n",
      " ...\n",
      " [-0.12385032]\n",
      " [-0.00746957]\n",
      " [-0.30423364]]\n",
      "Current iteration=2, loss=38052.81493117046\n",
      "t [[-6.46761151e-05]\n",
      " [-1.91575665e-01]\n",
      " [-3.98719222e-01]\n",
      " ...\n",
      " [-1.48013840e-01]\n",
      " [ 1.84164866e-04]\n",
      " [-4.30124293e-01]]\n",
      "t [[-6.46761151e-05]\n",
      " [-1.91575665e-01]\n",
      " [-3.98719222e-01]\n",
      " ...\n",
      " [-1.48013840e-01]\n",
      " [ 1.84164866e-04]\n",
      " [-4.30124293e-01]]\n",
      "t [[-0.00743299]\n",
      " [-0.26447724]\n",
      " [-0.47037287]\n",
      " ...\n",
      " [-0.16090911]\n",
      " [ 0.00998524]\n",
      " [-0.54339801]]\n",
      "t [[-0.00743299]\n",
      " [-0.26447724]\n",
      " [-0.47037287]\n",
      " ...\n",
      " [-0.16090911]\n",
      " [ 0.00998524]\n",
      " [-0.54339801]]\n",
      "Current iteration=4, loss=36871.57222028429\n",
      "t [[-0.01558571]\n",
      " [-0.33625964]\n",
      " [-0.52886232]\n",
      " ...\n",
      " [-0.16753299]\n",
      " [ 0.02013827]\n",
      " [-0.64636076]]\n",
      "t [[-0.01558571]\n",
      " [-0.33625964]\n",
      " [-0.52886232]\n",
      " ...\n",
      " [-0.16753299]\n",
      " [ 0.02013827]\n",
      " [-0.64636076]]\n",
      "t [[-0.02387231]\n",
      " [-0.4057459 ]\n",
      " [-0.57863293]\n",
      " ...\n",
      " [-0.17076367]\n",
      " [ 0.0297208 ]\n",
      " [-0.74069395]]\n",
      "t [[-0.02387231]\n",
      " [-0.4057459 ]\n",
      " [-0.57863293]\n",
      " ...\n",
      " [-0.17076367]\n",
      " [ 0.0297208 ]\n",
      " [-0.74069395]]\n",
      "Current iteration=6, loss=36081.03696227624\n",
      "t [[-0.03193726]\n",
      " [-0.47235953]\n",
      " [-0.62241019]\n",
      " ...\n",
      " [-0.17227228]\n",
      " [ 0.03829483]\n",
      " [-0.82766917]]\n",
      "t [[-0.03193726]\n",
      " [-0.47235953]\n",
      " [-0.62241019]\n",
      " ...\n",
      " [-0.17227228]\n",
      " [ 0.03829483]\n",
      " [-0.82766917]]\n",
      "t [[-0.03959393]\n",
      " [-0.53586408]\n",
      " [-0.66189994]\n",
      " ...\n",
      " [-0.17302512]\n",
      " [ 0.04569306]\n",
      " [-0.90827811]]\n",
      "t [[-0.03959393]\n",
      " [-0.53586408]\n",
      " [-0.66189994]\n",
      " ...\n",
      " [-0.17302512]\n",
      " [ 0.04569306]\n",
      " [-0.90827811]]\n",
      "Current iteration=8, loss=35504.767002165376\n",
      "t [[-0.04675362]\n",
      " [-0.59621634]\n",
      " [-0.69818885]\n",
      " ...\n",
      " [-0.17357034]\n",
      " [ 0.05189758]\n",
      " [-0.9833138 ]]\n",
      "t [[-0.04675362]\n",
      " [-0.59621634]\n",
      " [-0.69818885]\n",
      " ...\n",
      " [-0.17357034]\n",
      " [ 0.05189758]\n",
      " [-0.9833138 ]]\n",
      "t [[-0.05338443]\n",
      " [-0.65348115]\n",
      " [-0.73198103]\n",
      " ...\n",
      " [-0.17420634]\n",
      " [ 0.05696981]\n",
      " [-1.0534235 ]]\n",
      "loss=35067.09001615844\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07347274]\n",
      " [-0.31386713]\n",
      " [-0.19727679]\n",
      " ...\n",
      " [-0.14297677]\n",
      " [ 0.03095526]\n",
      " [-0.08334794]]\n",
      "t [[ 0.07347274]\n",
      " [-0.31386713]\n",
      " [-0.19727679]\n",
      " ...\n",
      " [-0.14297677]\n",
      " [ 0.03095526]\n",
      " [-0.08334794]]\n",
      "t [[ 0.12140073]\n",
      " [-0.54004306]\n",
      " [-0.33691845]\n",
      " ...\n",
      " [-0.23774183]\n",
      " [ 0.04455964]\n",
      " [-0.15345062]]\n",
      "t [[ 0.12140073]\n",
      " [-0.54004306]\n",
      " [-0.33691845]\n",
      " ...\n",
      " [-0.23774183]\n",
      " [ 0.04455964]\n",
      " [-0.15345062]]\n",
      "Current iteration=2, loss=38030.06479150455\n",
      "t [[ 0.15335587]\n",
      " [-0.71131806]\n",
      " [-0.44167583]\n",
      " ...\n",
      " [-0.30428207]\n",
      " [ 0.04723337]\n",
      " [-0.21384353]]\n",
      "t [[ 0.15335587]\n",
      " [-0.71131806]\n",
      " [-0.44167583]\n",
      " ...\n",
      " [-0.30428207]\n",
      " [ 0.04723337]\n",
      " [-0.21384353]]\n",
      "t [[ 0.17509096]\n",
      " [-0.84689225]\n",
      " [-0.52462603]\n",
      " ...\n",
      " [-0.35400865]\n",
      " [ 0.04280386]\n",
      " [-0.26680257]]\n",
      "t [[ 0.17509096]\n",
      " [-0.84689225]\n",
      " [-0.52462603]\n",
      " ...\n",
      " [-0.35400865]\n",
      " [ 0.04280386]\n",
      " [-0.26680257]]\n",
      "Current iteration=4, loss=36855.522758232015\n",
      "t [[ 0.19008424]\n",
      " [-0.95811386]\n",
      " [-0.59331184]\n",
      " ...\n",
      " [-0.39342592]\n",
      " [ 0.03359179]\n",
      " [-0.31383254]]\n",
      "t [[ 0.19008424]\n",
      " [-0.95811386]\n",
      " [-0.59331184]\n",
      " ...\n",
      " [-0.39342592]\n",
      " [ 0.03359179]\n",
      " [-0.31383254]]\n",
      "t [[ 0.20050878]\n",
      " [-1.05196955]\n",
      " [-0.65223114]\n",
      " ...\n",
      " [-0.42633439]\n",
      " [ 0.0210828 ]\n",
      " [-0.35598373]]\n",
      "t [[ 0.20050878]\n",
      " [-1.05196955]\n",
      " [-0.65223114]\n",
      " ...\n",
      " [-0.42633439]\n",
      " [ 0.0210828 ]\n",
      " [-0.35598373]]\n",
      "Current iteration=6, loss=36075.55915555518\n",
      "t [[ 0.20776579]\n",
      " [-1.13293863]\n",
      " [-0.70415376]\n",
      " ...\n",
      " [-0.45500347]\n",
      " [ 0.0062776 ]\n",
      " [-0.39402624]]\n",
      "t [[ 0.20776579]\n",
      " [-1.13293863]\n",
      " [-0.70415376]\n",
      " ...\n",
      " [-0.45500347]\n",
      " [ 0.0062776 ]\n",
      " [-0.39402624]]\n",
      "t [[ 0.2127836 ]\n",
      " [-1.20399964]\n",
      " [-0.75083431]\n",
      " ...\n",
      " [-0.48080974]\n",
      " [-0.01012184]\n",
      " [-0.42854786]]\n",
      "t [[ 0.2127836 ]\n",
      " [-1.20399964]\n",
      " [-0.75083431]\n",
      " ...\n",
      " [-0.48080974]\n",
      " [-0.01012184]\n",
      " [-0.42854786]]\n",
      "Current iteration=8, loss=35510.90687704021\n",
      "t [[ 0.21619204]\n",
      " [-1.26720073]\n",
      " [-0.79341442]\n",
      " ...\n",
      " [-0.50459776]\n",
      " [-0.02760748]\n",
      " [-0.46001166]]\n",
      "t [[ 0.21619204]\n",
      " [-1.26720073]\n",
      " [-0.79341442]\n",
      " ...\n",
      " [-0.50459776]\n",
      " [-0.02760748]\n",
      " [-0.46001166]]\n",
      "t [[ 0.2184276 ]\n",
      " [-1.32399581]\n",
      " [-0.83265765]\n",
      " ...\n",
      " [-0.52689058]\n",
      " [-0.04580302]\n",
      " [-0.48879176]]\n",
      "loss=35084.50997977958\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00968912]\n",
      " [-0.0596546 ]\n",
      " [-0.18702339]\n",
      " ...\n",
      " [-0.14351769]\n",
      " [ 0.02790082]\n",
      " [-0.08514106]]\n",
      "t [[ 0.00968912]\n",
      " [-0.0596546 ]\n",
      " [-0.18702339]\n",
      " ...\n",
      " [-0.14351769]\n",
      " [ 0.02790082]\n",
      " [-0.08514106]]\n",
      "t [[ 0.01057242]\n",
      " [-0.13279511]\n",
      " [-0.31489769]\n",
      " ...\n",
      " [-0.23778923]\n",
      " [ 0.03886262]\n",
      " [-0.15673769]]\n",
      "t [[ 0.01057242]\n",
      " [-0.13279511]\n",
      " [-0.31489769]\n",
      " ...\n",
      " [-0.23778923]\n",
      " [ 0.03886262]\n",
      " [-0.15673769]]\n",
      "Current iteration=2, loss=38003.277366508875\n",
      "t [[ 0.00692189]\n",
      " [-0.21095147]\n",
      " [-0.4079845 ]\n",
      " ...\n",
      " [-0.30344269]\n",
      " [ 0.0393239 ]\n",
      " [-0.21843586]]\n",
      "t [[ 0.00692189]\n",
      " [-0.21095147]\n",
      " [-0.4079845 ]\n",
      " ...\n",
      " [-0.30344269]\n",
      " [ 0.0393239 ]\n",
      " [-0.21843586]]\n",
      "t [[ 0.00107161]\n",
      " [-0.28963032]\n",
      " [-0.48007116]\n",
      " ...\n",
      " [-0.35215809]\n",
      " [ 0.03306965]\n",
      " [-0.27256678]]\n",
      "t [[ 0.00107161]\n",
      " [-0.28963032]\n",
      " [-0.48007116]\n",
      " ...\n",
      " [-0.35215809]\n",
      " [ 0.03306965]\n",
      " [-0.27256678]]\n",
      "Current iteration=4, loss=36810.38073313573\n",
      "t [[-0.00572146]\n",
      " [-0.36653161]\n",
      " [-0.53897478]\n",
      " ...\n",
      " [-0.39054813]\n",
      " [ 0.02236493]\n",
      " [-0.32066523]]\n",
      "t [[-0.00572146]\n",
      " [-0.36653161]\n",
      " [-0.53897478]\n",
      " ...\n",
      " [-0.39054813]\n",
      " [ 0.02236493]\n",
      " [-0.32066523]]\n",
      "t [[-0.01277254]\n",
      " [-0.4405176 ]\n",
      " [-0.58926172]\n",
      " ...\n",
      " [-0.42245557]\n",
      " [ 0.00864257]\n",
      " [-0.36379985]]\n",
      "t [[-0.01277254]\n",
      " [-0.4405176 ]\n",
      " [-0.58926172]\n",
      " ...\n",
      " [-0.42245557]\n",
      " [ 0.00864257]\n",
      " [-0.36379985]]\n",
      "Current iteration=6, loss=36015.84427409717\n",
      "t [[-0.0197098 ]\n",
      " [-0.51107878]\n",
      " [-0.63367287]\n",
      " ...\n",
      " [-0.45016425]\n",
      " [-0.00714295]\n",
      " [-0.40275334]]\n",
      "t [[-0.0197098 ]\n",
      " [-0.51107878]\n",
      " [-0.63367287]\n",
      " ...\n",
      " [-0.45016425]\n",
      " [-0.00714295]\n",
      " [-0.40275334]]\n",
      "t [[-0.02633754]\n",
      " [-0.57805011]\n",
      " [-0.67389004]\n",
      " ...\n",
      " [-0.47505363]\n",
      " [-0.024329  ]\n",
      " [-0.43812276]]\n",
      "t [[-0.02633754]\n",
      " [-0.57805011]\n",
      " [-0.67389004]\n",
      " ...\n",
      " [-0.47505363]\n",
      " [-0.024329  ]\n",
      " [-0.43812276]]\n",
      "Current iteration=8, loss=35440.25503384332\n",
      "t [[-0.03256064]\n",
      " [-0.64145482]\n",
      " [-0.71096523]\n",
      " ...\n",
      " [-0.49796678]\n",
      " [-0.04244062]\n",
      " [-0.47037848]]\n",
      "t [[-0.03256064]\n",
      " [-0.64145482]\n",
      " [-0.71096523]\n",
      " ...\n",
      " [-0.49796678]\n",
      " [-0.04244062]\n",
      " [-0.47037848]]\n",
      "t [[-0.03834152]\n",
      " [-0.70141603]\n",
      " [-0.7455694 ]\n",
      " ...\n",
      " [-0.51942398]\n",
      " [-0.06112914]\n",
      " [-0.49990054]]\n",
      "loss=35005.77820708091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00827558]\n",
      " [-0.05235644]\n",
      " [-0.18904891]\n",
      " ...\n",
      " [-0.14111762]\n",
      " [ 0.02999882]\n",
      " [-0.07986268]]\n",
      "t [[ 0.00827558]\n",
      " [-0.05235644]\n",
      " [-0.18904891]\n",
      " ...\n",
      " [-0.14111762]\n",
      " [ 0.02999882]\n",
      " [-0.07986268]]\n",
      "t [[ 0.00783104]\n",
      " [-0.11883375]\n",
      " [-0.31906219]\n",
      " ...\n",
      " [-0.23365633]\n",
      " [ 0.04288724]\n",
      " [-0.14663817]]\n",
      "t [[ 0.00783104]\n",
      " [-0.11883375]\n",
      " [-0.31906219]\n",
      " ...\n",
      " [-0.23365633]\n",
      " [ 0.04288724]\n",
      " [-0.14663817]]\n",
      "Current iteration=2, loss=38029.68509123857\n",
      "t [[ 0.0029612 ]\n",
      " [-0.19092634]\n",
      " [-0.41417558]\n",
      " ...\n",
      " [-0.29792681]\n",
      " [ 0.04508345]\n",
      " [-0.20389587]]\n",
      "t [[ 0.0029612 ]\n",
      " [-0.19092634]\n",
      " [-0.41417558]\n",
      " ...\n",
      " [-0.29792681]\n",
      " [ 0.04508345]\n",
      " [-0.20389587]]\n",
      "t [[-0.00398724]\n",
      " [-0.26407431]\n",
      " [-0.48811143]\n",
      " ...\n",
      " [-0.34547023]\n",
      " [ 0.04037583]\n",
      " [-0.2539138 ]]\n",
      "t [[-0.00398724]\n",
      " [-0.26407431]\n",
      " [-0.48811143]\n",
      " ...\n",
      " [-0.34547023]\n",
      " [ 0.04037583]\n",
      " [-0.2539138 ]]\n",
      "Current iteration=4, loss=36855.67266553473\n",
      "t [[-0.01175325]\n",
      " [-0.33591111]\n",
      " [-0.54868217]\n",
      " ...\n",
      " [-0.38283671]\n",
      " [ 0.03104113]\n",
      " [-0.29818736]]\n",
      "t [[-0.01175325]\n",
      " [-0.33591111]\n",
      " [-0.54868217]\n",
      " ...\n",
      " [-0.38283671]\n",
      " [ 0.03104113]\n",
      " [-0.29818736]]\n",
      "t [[-0.01965424]\n",
      " [-0.40524108]\n",
      " [-0.60046834]\n",
      " ...\n",
      " [-0.41383812]\n",
      " [ 0.01852589]\n",
      " [-0.33775437]]\n",
      "t [[-0.01965424]\n",
      " [-0.40524108]\n",
      " [-0.60046834]\n",
      " ...\n",
      " [-0.41383812]\n",
      " [ 0.01852589]\n",
      " [-0.33775437]]\n",
      "Current iteration=6, loss=36075.85935510642\n",
      "t [[-0.02732447]\n",
      " [-0.4715058 ]\n",
      " [-0.64622955]\n",
      " ...\n",
      " [-0.44074114]\n",
      " [ 0.00379858]\n",
      " [-0.37337232]]\n",
      "t [[-0.02732447]\n",
      " [-0.4715058 ]\n",
      " [-0.64622955]\n",
      " ...\n",
      " [-0.44074114]\n",
      " [ 0.00379858]\n",
      " [-0.37337232]]\n",
      "t [[-0.03457648]\n",
      " [-0.5344991 ]\n",
      " [-0.68766596]\n",
      " ...\n",
      " [-0.4649142 ]\n",
      " [-0.01246428]\n",
      " [-0.40561702]]\n",
      "t [[-0.03457648]\n",
      " [-0.5344991 ]\n",
      " [-0.68766596]\n",
      " ...\n",
      " [-0.4649142 ]\n",
      " [-0.01246428]\n",
      " [-0.40561702]]\n",
      "Current iteration=8, loss=35511.54497513205\n",
      "t [[-0.04132441]\n",
      " [-0.59420939]\n",
      " [-0.72584603]\n",
      " ...\n",
      " [-0.48719213]\n",
      " [-0.02977433]\n",
      " [-0.43494055]]\n",
      "t [[-0.04132441]\n",
      " [-0.59420939]\n",
      " [-0.72584603]\n",
      " ...\n",
      " [-0.48719213]\n",
      " [-0.02977433]\n",
      " [-0.43494055]]\n",
      "t [[-0.04754015]\n",
      " [-0.65073011]\n",
      " [-0.76145491]\n",
      " ...\n",
      " [-0.50808827]\n",
      " [-0.04777004]\n",
      " [-0.46170694]]\n",
      "loss=35085.66615283182\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00689189]\n",
      " [-0.05545451]\n",
      " [-0.18780151]\n",
      " ...\n",
      " [-0.08268418]\n",
      " [-0.00995827]\n",
      " [-0.16868911]]\n",
      "t [[ 0.00689189]\n",
      " [-0.05545451]\n",
      " [-0.18780151]\n",
      " ...\n",
      " [-0.08268418]\n",
      " [-0.00995827]\n",
      " [-0.16868911]]\n",
      "t [[ 0.00523612]\n",
      " [-0.12492633]\n",
      " [-0.31608242]\n",
      " ...\n",
      " [-0.12722289]\n",
      " [-0.00729612]\n",
      " [-0.31511915]]\n",
      "t [[ 0.00523612]\n",
      " [-0.12492633]\n",
      " [-0.31608242]\n",
      " ...\n",
      " [-0.12722289]\n",
      " [-0.00729612]\n",
      " [-0.31511915]]\n",
      "Current iteration=2, loss=37990.50485959058\n",
      "t [[-0.00069174]\n",
      " [-0.19985988]\n",
      " [-0.4092944 ]\n",
      " ...\n",
      " [-0.15076109]\n",
      " [ 0.00107947]\n",
      " [-0.44460638]]\n",
      "t [[-0.00069174]\n",
      " [-0.19985988]\n",
      " [-0.4092944 ]\n",
      " ...\n",
      " [-0.15076109]\n",
      " [ 0.00107947]\n",
      " [-0.44460638]]\n",
      "t [[-0.00856723]\n",
      " [-0.27568517]\n",
      " [-0.48131124]\n",
      " ...\n",
      " [-0.16284846]\n",
      " [ 0.0115023 ]\n",
      " [-0.56075806]]\n",
      "t [[-0.00856723]\n",
      " [-0.27568517]\n",
      " [-0.48131124]\n",
      " ...\n",
      " [-0.16284846]\n",
      " [ 0.0115023 ]\n",
      " [-0.56075806]]\n",
      "Current iteration=4, loss=36796.41949360429\n",
      "t [[-0.0171502 ]\n",
      " [-0.35003984]\n",
      " [-0.54001538]\n",
      " ...\n",
      " [-0.1687648 ]\n",
      " [ 0.02209329]\n",
      " [-0.6660713 ]]\n",
      "t [[-0.0171502 ]\n",
      " [-0.35003984]\n",
      " [-0.54001538]\n",
      " ...\n",
      " [-0.1687648 ]\n",
      " [ 0.02209329]\n",
      " [-0.6660713 ]]\n",
      "t [[-0.02577608]\n",
      " [-0.42173641]\n",
      " [-0.59001966]\n",
      " ...\n",
      " [-0.17147483]\n",
      " [ 0.03192554]\n",
      " [-0.76235001]]\n",
      "t [[-0.02577608]\n",
      " [-0.42173641]\n",
      " [-0.59001966]\n",
      " ...\n",
      " [-0.17147483]\n",
      " [ 0.03192554]\n",
      " [-0.76235001]]\n",
      "Current iteration=6, loss=36002.176397320254\n",
      "t [[-0.0340941 ]\n",
      " [-0.49022557]\n",
      " [-0.63409751]\n",
      " ...\n",
      " [-0.17265475]\n",
      " [ 0.0405866 ]\n",
      " [-0.85094979]]\n",
      "t [[-0.0340941 ]\n",
      " [-0.49022557]\n",
      " [-0.63409751]\n",
      " ...\n",
      " [-0.17265475]\n",
      " [ 0.0405866 ]\n",
      " [-0.85094979]]\n",
      "t [[-0.04192902]\n",
      " [-0.55531036]\n",
      " [-0.67395322]\n",
      " ...\n",
      " [-0.17324473]\n",
      " [ 0.0479432 ]\n",
      " [-0.93292308]]\n",
      "t [[-0.04192902]\n",
      " [-0.55531036]\n",
      " [-0.67395322]\n",
      " ...\n",
      " [-0.17324473]\n",
      " [ 0.0479432 ]\n",
      " [-0.93292308]]\n",
      "Current iteration=8, loss=35426.80777380367\n",
      "t [[-0.04920492]\n",
      " [-0.61698824]\n",
      " [-0.71065416]\n",
      " ...\n",
      " [-0.17375812]\n",
      " [ 0.05401071]\n",
      " [-1.0091086 ]]\n",
      "t [[-0.04920492]\n",
      " [-0.61698824]\n",
      " [-0.71065416]\n",
      " ...\n",
      " [-0.17375812]\n",
      " [ 0.05401071]\n",
      " [-1.0091086 ]]\n",
      "t [[-0.05590174]\n",
      " [-0.67536135]\n",
      " [-0.74488156]\n",
      " ...\n",
      " [-0.17445951]\n",
      " [ 0.05887949]\n",
      " [-1.08018907]]\n",
      "loss=34992.35417773543\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07619395]\n",
      " [-0.32549184]\n",
      " [-0.20458334]\n",
      " ...\n",
      " [-0.14827221]\n",
      " [ 0.03210175]\n",
      " [-0.0864349 ]]\n",
      "t [[ 0.07619395]\n",
      " [-0.32549184]\n",
      " [-0.20458334]\n",
      " ...\n",
      " [-0.14827221]\n",
      " [ 0.03210175]\n",
      " [-0.0864349 ]]\n",
      "t [[ 0.12493181]\n",
      " [-0.55673296]\n",
      " [-0.34722116]\n",
      " ...\n",
      " [-0.24472613]\n",
      " [ 0.04555708]\n",
      " [-0.15863178]]\n",
      "t [[ 0.12493181]\n",
      " [-0.55673296]\n",
      " [-0.34722116]\n",
      " ...\n",
      " [-0.24472613]\n",
      " [ 0.04555708]\n",
      " [-0.15863178]]\n",
      "Current iteration=2, loss=37970.0312352855\n",
      "t [[ 0.15687833]\n",
      " [-0.73024906]\n",
      " [-0.45325423]\n",
      " ...\n",
      " [-0.31162523]\n",
      " [ 0.04751531]\n",
      " [-0.22053048]]\n",
      "t [[ 0.15687833]\n",
      " [-0.73024906]\n",
      " [-0.45325423]\n",
      " ...\n",
      " [-0.31162523]\n",
      " [ 0.04751531]\n",
      " [-0.22053048]]\n",
      "t [[ 0.17827778]\n",
      " [-0.86684525]\n",
      " [-0.53683685]\n",
      " ...\n",
      " [-0.36131643]\n",
      " [ 0.04212636]\n",
      " [-0.27461156]]\n",
      "t [[ 0.17827778]\n",
      " [-0.86684525]\n",
      " [-0.53683685]\n",
      " ...\n",
      " [-0.36131643]\n",
      " [ 0.04212636]\n",
      " [-0.27461156]]\n",
      "Current iteration=4, loss=36783.7763069949\n",
      "t [[ 0.19282578]\n",
      " [-0.97855019]\n",
      " [-0.60594034]\n",
      " ...\n",
      " [-0.40066261]\n",
      " [ 0.03185838]\n",
      " [-0.3224907 ]]\n",
      "t [[ 0.19282578]\n",
      " [-0.97855019]\n",
      " [-0.60594034]\n",
      " ...\n",
      " [-0.40066261]\n",
      " [ 0.03185838]\n",
      " [-0.3224907 ]]\n",
      "t [[ 0.20279158]\n",
      " [-1.07264423]\n",
      " [-0.66522457]\n",
      " ...\n",
      " [-0.43358551]\n",
      " [ 0.01827107]\n",
      " [-0.36528521]]\n",
      "t [[ 0.20279158]\n",
      " [-1.07264423]\n",
      " [-0.66522457]\n",
      " ...\n",
      " [-0.43358551]\n",
      " [ 0.01827107]\n",
      " [-0.36528521]]\n",
      "Current iteration=6, loss=36000.82116861953\n",
      "t [[ 0.20961702]\n",
      " [-1.15373219]\n",
      " [-0.7175098 ]\n",
      " ...\n",
      " [-0.46237804]\n",
      " [ 0.00240525]\n",
      " [-0.40380977]]\n",
      "t [[ 0.20961702]\n",
      " [-1.15373219]\n",
      " [-0.7175098 ]\n",
      " ...\n",
      " [-0.46237804]\n",
      " [ 0.00240525]\n",
      " [-0.40380977]]\n",
      "t [[ 0.21424518]\n",
      " [-1.22484384]\n",
      " [-0.76455584]\n",
      " ...\n",
      " [-0.48840242]\n",
      " [-0.01501428]\n",
      " [-0.43868392]]\n",
      "t [[ 0.21424518]\n",
      " [-1.22484384]\n",
      " [-0.76455584]\n",
      " ...\n",
      " [-0.48840242]\n",
      " [-0.01501428]\n",
      " [-0.43868392]]\n",
      "Current iteration=8, loss=35437.496045972526\n",
      "t [[ 0.21730884]\n",
      " [-1.28804643]\n",
      " [-0.80749352]\n",
      " ...\n",
      " [-0.51247739]\n",
      " [-0.03346635]\n",
      " [-0.47039455]]\n",
      "t [[ 0.21730884]\n",
      " [-1.28804643]\n",
      " [-0.80749352]\n",
      " ...\n",
      " [-0.51247739]\n",
      " [-0.03346635]\n",
      " [-0.47039455]]\n",
      "t [[ 0.21924242]\n",
      " [-1.34480012]\n",
      " [-0.84707237]\n",
      " ...\n",
      " [-0.5350999 ]\n",
      " [-0.05256731]\n",
      " [-0.49933428]]\n",
      "loss=35014.49233218989\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01004797]\n",
      " [-0.06186402]\n",
      " [-0.19395018]\n",
      " ...\n",
      " [-0.14883316]\n",
      " [ 0.02893418]\n",
      " [-0.08829443]]\n",
      "t [[ 0.01004797]\n",
      " [-0.06186402]\n",
      " [-0.19395018]\n",
      " ...\n",
      " [-0.14883316]\n",
      " [ 0.02893418]\n",
      " [-0.08829443]]\n",
      "t [[ 0.01063397]\n",
      " [-0.13821955]\n",
      " [-0.32432793]\n",
      " ...\n",
      " [-0.24473681]\n",
      " [ 0.039665  ]\n",
      " [-0.16202938]]\n",
      "t [[ 0.01063397]\n",
      " [-0.13821955]\n",
      " [-0.32432793]\n",
      " ...\n",
      " [-0.24473681]\n",
      " [ 0.039665  ]\n",
      " [-0.16202938]]\n",
      "Current iteration=2, loss=37942.476246933446\n",
      "t [[ 0.00651403]\n",
      " [-0.2196356 ]\n",
      " [-0.4182672 ]\n",
      " ...\n",
      " [-0.31068745]\n",
      " [ 0.03936148]\n",
      " [-0.22526764]]\n",
      "t [[ 0.00651403]\n",
      " [-0.2196356 ]\n",
      " [-0.4182672 ]\n",
      " ...\n",
      " [-0.31068745]\n",
      " [ 0.03936148]\n",
      " [-0.22526764]]\n",
      "t [[ 2.05003973e-04]\n",
      " [-3.01274735e-01]\n",
      " [-4.90677016e-01]\n",
      " ...\n",
      " [-3.59316353e-01]\n",
      " [ 3.21239766e-02]\n",
      " [-2.80549117e-01]]\n",
      "t [[ 2.05003973e-04]\n",
      " [-3.01274735e-01]\n",
      " [-4.90677016e-01]\n",
      " ...\n",
      " [-3.59316353e-01]\n",
      " [ 3.21239766e-02]\n",
      " [-2.80549117e-01]]\n",
      "Current iteration=4, loss=36737.379561151414\n",
      "t [[-0.00697618]\n",
      " [-0.3807445 ]\n",
      " [-0.54980094]\n",
      " ...\n",
      " [-0.39759511]\n",
      " [ 0.02035797]\n",
      " [-0.32952091]]\n",
      "t [[-0.00697618]\n",
      " [-0.3807445 ]\n",
      " [-0.54980094]\n",
      " ...\n",
      " [-0.39759511]\n",
      " [ 0.02035797]\n",
      " [-0.32952091]]\n",
      "t [[-0.0143321 ]\n",
      " [-0.45691063]\n",
      " [-0.60035143]\n",
      " ...\n",
      " [-0.42948508]\n",
      " [ 0.00556477]\n",
      " [-0.37331951]]\n",
      "t [[-0.0143321 ]\n",
      " [-0.45691063]\n",
      " [-0.60035143]\n",
      " ...\n",
      " [-0.42948508]\n",
      " [ 0.00556477]\n",
      " [-0.37331951]]\n",
      "Current iteration=6, loss=35939.66410904002\n",
      "t [[-0.02149628]\n",
      " [-0.52930135]\n",
      " [-0.64510163]\n",
      " ...\n",
      " [-0.45729102]\n",
      " [-0.01126527]\n",
      " [-0.41277268]]\n",
      "t [[-0.02149628]\n",
      " [-0.52930135]\n",
      " [-0.64510163]\n",
      " ...\n",
      " [-0.45729102]\n",
      " [-0.01126527]\n",
      " [-0.41277268]]\n",
      "t [[-0.02828427]\n",
      " [-0.59779841]\n",
      " [-0.68572191]\n",
      " ...\n",
      " [-0.48237618]\n",
      " [-0.02944982]\n",
      " [-0.44850965]]\n",
      "t [[-0.02828427]\n",
      " [-0.59779841]\n",
      " [-0.68572191]\n",
      " ...\n",
      " [-0.48237618]\n",
      " [-0.02944982]\n",
      " [-0.44850965]]\n",
      "Current iteration=8, loss=35365.42363187595\n",
      "t [[-0.03461305]\n",
      " [-0.66247042]\n",
      " [-0.72323958]\n",
      " ...\n",
      " [-0.50555664]\n",
      " [-0.0485031 ]\n",
      " [-0.48102494]]\n",
      "t [[-0.03461305]\n",
      " [-0.66247042]\n",
      " [-0.72323958]\n",
      " ...\n",
      " [-0.50555664]\n",
      " [-0.0485031 ]\n",
      " [-0.48102494]]\n",
      "t [[-0.04045598]\n",
      " [-0.72348068]\n",
      " [-0.75830037]\n",
      " ...\n",
      " [-0.52732594]\n",
      " [-0.06807078]\n",
      " [-0.51071739]]\n",
      "loss=34934.45484996673\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00858208]\n",
      " [-0.05429557]\n",
      " [-0.19605072]\n",
      " ...\n",
      " [-0.1463442 ]\n",
      " [ 0.03110989]\n",
      " [-0.08282056]]\n",
      "t [[ 0.00858208]\n",
      " [-0.05429557]\n",
      " [-0.19605072]\n",
      " ...\n",
      " [-0.1463442 ]\n",
      " [ 0.03110989]\n",
      " [-0.08282056]]\n",
      "t [[ 0.00779419]\n",
      " [-0.1237658 ]\n",
      " [-0.32865019]\n",
      " ...\n",
      " [-0.24047572]\n",
      " [ 0.04383173]\n",
      " [-0.15157328]]\n",
      "t [[ 0.00779419]\n",
      " [-0.1237658 ]\n",
      " [-0.32865019]\n",
      " ...\n",
      " [-0.24047572]\n",
      " [ 0.04383173]\n",
      " [-0.15157328]]\n",
      "Current iteration=2, loss=37969.69761031604\n",
      "t [[ 0.00241787]\n",
      " [-0.19893912]\n",
      " [-0.42468201]\n",
      " ...\n",
      " [-0.30501827]\n",
      " [ 0.04531244]\n",
      " [-0.2102354 ]]\n",
      "t [[ 0.00241787]\n",
      " [-0.19893912]\n",
      " [-0.42468201]\n",
      " ...\n",
      " [-0.30501827]\n",
      " [ 0.04531244]\n",
      " [-0.2102354 ]]\n",
      "t [[-0.00501625]\n",
      " [-0.27490271]\n",
      " [-0.49898945]\n",
      " ...\n",
      " [-0.35245567]\n",
      " [ 0.03965728]\n",
      " [-0.26128849]]\n",
      "t [[-0.00501625]\n",
      " [-0.27490271]\n",
      " [-0.49898945]\n",
      " ...\n",
      " [-0.35245567]\n",
      " [ 0.03965728]\n",
      " [-0.26128849]]\n",
      "Current iteration=4, loss=36783.94003256716\n",
      "t [[-0.01318742]\n",
      " [-0.34919038]\n",
      " [-0.55981501]\n",
      " ...\n",
      " [-0.38969545]\n",
      " [ 0.02928525]\n",
      " [-0.30633696]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.01318742]\n",
      " [-0.34919038]\n",
      " [-0.55981501]\n",
      " ...\n",
      " [-0.38969545]\n",
      " [ 0.02928525]\n",
      " [-0.30633696]]\n",
      "t [[-0.02140138]\n",
      " [-0.42060453]\n",
      " [-0.6118889 ]\n",
      " ...\n",
      " [-0.42066809]\n",
      " [ 0.01571305]\n",
      " [-0.34648401]]\n",
      "t [[-0.02140138]\n",
      " [-0.42060453]\n",
      " [-0.6118889 ]\n",
      " ...\n",
      " [-0.42066809]\n",
      " [ 0.01571305]\n",
      " [-0.34648401]]\n",
      "Current iteration=6, loss=36001.143268049644\n",
      "t [[-2.92990688e-02]\n",
      " [-4.88620429e-01]\n",
      " [-6.58005706e-01]\n",
      " ...\n",
      " [-4.47661314e-01]\n",
      " [-5.33032285e-05]\n",
      " [-3.82530269e-01]]\n",
      "t [[-2.92990688e-02]\n",
      " [-4.88620429e-01]\n",
      " [-6.58005706e-01]\n",
      " ...\n",
      " [-4.47661314e-01]\n",
      " [-5.33032285e-05]\n",
      " [-3.82530269e-01]]\n",
      "t [[-0.03670564]\n",
      " [-0.55307552]\n",
      " [-0.69985613]\n",
      " ...\n",
      " [-0.47202725]\n",
      " [-0.01731611]\n",
      " [-0.4150821 ]]\n",
      "t [[-0.03670564]\n",
      " [-0.55307552]\n",
      " [-0.69985613]\n",
      " ...\n",
      " [-0.47202725]\n",
      " [-0.01731611]\n",
      " [-0.4150821 ]]\n",
      "Current iteration=8, loss=35438.197692284535\n",
      "t [[-0.04354861]\n",
      " [-0.61400114]\n",
      " [-0.73848534]\n",
      " ...\n",
      " [-0.49457343]\n",
      " [-0.03557472]\n",
      " [-0.44461433]]\n",
      "t [[-0.04354861]\n",
      " [-0.61400114]\n",
      " [-0.73848534]\n",
      " ...\n",
      " [-0.49457343]\n",
      " [-0.03557472]\n",
      " [-0.44461433]]\n",
      "t [[-0.04981196]\n",
      " [-0.67152896]\n",
      " [-0.77455428]\n",
      " ...\n",
      " [-0.51578603]\n",
      " [-0.05446066]\n",
      " [-0.47150864]]\n",
      "loss=35015.75369462673\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00714715]\n",
      " [-0.05750838]\n",
      " [-0.19475712]\n",
      " ...\n",
      " [-0.08574656]\n",
      " [-0.0103271 ]\n",
      " [-0.17493685]]\n",
      "t [[ 0.00714715]\n",
      " [-0.05750838]\n",
      " [-0.19475712]\n",
      " ...\n",
      " [-0.08574656]\n",
      " [-0.0103271 ]\n",
      " [-0.17493685]]\n",
      "t [[ 0.00510967]\n",
      " [-0.13007956]\n",
      " [-0.32554238]\n",
      " ...\n",
      " [-0.13049501]\n",
      " [-0.00708933]\n",
      " [-0.32594526]]\n",
      "t [[ 0.00510967]\n",
      " [-0.13007956]\n",
      " [-0.32554238]\n",
      " ...\n",
      " [-0.13049501]\n",
      " [-0.00708933]\n",
      " [-0.32594526]]\n",
      "Current iteration=2, loss=37929.522390733335\n",
      "t [[-0.00135226]\n",
      " [-0.20818705]\n",
      " [-0.41959054]\n",
      " ...\n",
      " [-0.15334108]\n",
      " [ 0.00202068]\n",
      " [-0.45895313]]\n",
      "t [[-0.00135226]\n",
      " [-0.20818705]\n",
      " [-0.41959054]\n",
      " ...\n",
      " [-0.15334108]\n",
      " [ 0.00202068]\n",
      " [-0.45895313]]\n",
      "t [[-0.00973291]\n",
      " [-0.28690855]\n",
      " [-0.49190636]\n",
      " ...\n",
      " [-0.16460145]\n",
      " [ 0.01305388]\n",
      " [-0.57790411]]\n",
      "t [[-0.00973291]\n",
      " [-0.28690855]\n",
      " [-0.49190636]\n",
      " ...\n",
      " [-0.16460145]\n",
      " [ 0.01305388]\n",
      " [-0.57790411]]\n",
      "Current iteration=4, loss=36723.41581838319\n",
      "t [[-0.01873466]\n",
      " [-0.36378343]\n",
      " [-0.55080429]\n",
      " ...\n",
      " [-0.16982567]\n",
      " [ 0.02405558]\n",
      " [-0.68549086]]\n",
      "t [[-0.01873466]\n",
      " [-0.36378343]\n",
      " [-0.55080429]\n",
      " ...\n",
      " [-0.16982567]\n",
      " [ 0.02405558]\n",
      " [-0.68549086]]\n",
      "t [[-0.027683  ]\n",
      " [-0.4376235 ]\n",
      " [-0.60104638]\n",
      " ...\n",
      " [-0.17204984]\n",
      " [ 0.03410292]\n",
      " [-0.78364123]]\n",
      "t [[-0.027683  ]\n",
      " [-0.4376235 ]\n",
      " [-0.60104638]\n",
      " ...\n",
      " [-0.17204984]\n",
      " [ 0.03410292]\n",
      " [-0.78364123]]\n",
      "Current iteration=6, loss=35926.033767526\n",
      "t [[-0.03623503]\n",
      " [-0.50791424]\n",
      " [-0.64543985]\n",
      " ...\n",
      " [-0.17294351]\n",
      " [ 0.04281531]\n",
      " [-0.87379536]]\n",
      "t [[-0.03623503]\n",
      " [-0.50791424]\n",
      " [-0.64543985]\n",
      " ...\n",
      " [-0.17294351]\n",
      " [ 0.04281531]\n",
      " [-0.87379536]]\n",
      "t [[-0.04422902]\n",
      " [-0.57450338]\n",
      " [-0.68567821]\n",
      " ...\n",
      " [-0.17341304]\n",
      " [ 0.05009761]\n",
      " [-0.95706658]]\n",
      "t [[-0.04422902]\n",
      " [-0.57450338]\n",
      " [-0.68567821]\n",
      " ...\n",
      " [-0.17341304]\n",
      " [ 0.05009761]\n",
      " [-0.95706658]]\n",
      "Current iteration=8, loss=35351.99751509158\n",
      "t [[-0.051603  ]\n",
      " [-0.63743227]\n",
      " [-0.72280421]\n",
      " ...\n",
      " [-0.1739325 ]\n",
      " [ 0.05600071]\n",
      " [-1.03433936]]\n",
      "t [[-0.051603  ]\n",
      " [-0.63743227]\n",
      " [-0.72280421]\n",
      " ...\n",
      " [-0.1739325 ]\n",
      " [ 0.05600071]\n",
      " [-1.03433936]]\n",
      "t [[-0.0583494 ]\n",
      " [-0.69684219]\n",
      " [-0.75747368]\n",
      " ...\n",
      " [-0.17473061]\n",
      " [ 0.06064481]\n",
      " [-1.10633182]]\n",
      "loss=34921.02494532962\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.07891517]\n",
      " [-0.33711654]\n",
      " [-0.21188988]\n",
      " ...\n",
      " [-0.15356764]\n",
      " [ 0.03324824]\n",
      " [-0.08952186]]\n",
      "t [[ 0.07891517]\n",
      " [-0.33711654]\n",
      " [-0.21188988]\n",
      " ...\n",
      " [-0.15356764]\n",
      " [ 0.03324824]\n",
      " [-0.08952186]]\n",
      "t [[ 0.12839569]\n",
      " [-0.57319245]\n",
      " [-0.35737258]\n",
      " ...\n",
      " [-0.25158373]\n",
      " [ 0.04650936]\n",
      " [-0.16377778]]\n",
      "t [[ 0.12839569]\n",
      " [-0.57319245]\n",
      " [-0.35737258]\n",
      " ...\n",
      " [-0.25158373]\n",
      " [ 0.04650936]\n",
      " [-0.16377778]]\n",
      "Current iteration=2, loss=37911.26021966828\n",
      "t [[ 0.160277  ]\n",
      " [-0.74875338]\n",
      " [-0.46456202]\n",
      " ...\n",
      " [-0.31874972]\n",
      " [ 0.04771281]\n",
      " [-0.22714081]]\n",
      "t [[ 0.160277  ]\n",
      " [-0.74875338]\n",
      " [-0.46456202]\n",
      " ...\n",
      " [-0.31874972]\n",
      " [ 0.04771281]\n",
      " [-0.22714081]]\n",
      "t [[ 0.18130703]\n",
      " [-0.88624351]\n",
      " [-0.5487097 ]\n",
      " ...\n",
      " [-0.36836471]\n",
      " [ 0.04133861]\n",
      " [-0.28230318]]\n",
      "t [[ 0.18130703]\n",
      " [-0.88624351]\n",
      " [-0.5487097 ]\n",
      " ...\n",
      " [-0.36836471]\n",
      " [ 0.04133861]\n",
      " [-0.28230318]]\n",
      "Current iteration=4, loss=36714.04621738909\n",
      "t [[ 0.195395  ]\n",
      " [-0.99835634]\n",
      " [-0.61820101]\n",
      " ...\n",
      " [-0.40763538]\n",
      " [ 0.03000104]\n",
      " [-0.33099276]]\n",
      "t [[ 0.195395  ]\n",
      " [-0.99835634]\n",
      " [-0.61820101]\n",
      " ...\n",
      " [-0.40763538]\n",
      " [ 0.03000104]\n",
      " [-0.33099276]]\n",
      "t [[ 0.20490035]\n",
      " [-1.09264611]\n",
      " [-0.67784122]\n",
      " ...\n",
      " [-0.4405883 ]\n",
      " [ 0.01533156]\n",
      " [-0.37439417]]\n",
      "t [[ 0.20490035]\n",
      " [-1.09264611]\n",
      " [-0.67784122]\n",
      " ...\n",
      " [-0.4405883 ]\n",
      " [ 0.01533156]\n",
      " [-0.37439417]]\n",
      "Current iteration=6, loss=35928.622655239014\n",
      "t [[ 0.21130052]\n",
      " [-1.17382821]\n",
      " [-0.73048875]\n",
      " ...\n",
      " [-0.46952756]\n",
      " [-0.00159083]\n",
      " [-0.41336659]]\n",
      "t [[ 0.21130052]\n",
      " [-1.17382821]\n",
      " [-0.73048875]\n",
      " ...\n",
      " [-0.46952756]\n",
      " [-0.00159083]\n",
      " [-0.41336659]]\n",
      "t [[ 0.21554981]\n",
      " [-1.24497393]\n",
      " [-0.77790086]\n",
      " ...\n",
      " [-0.49579328]\n",
      " [-0.02002001]\n",
      " [-0.44856128]]\n",
      "t [[ 0.21554981]\n",
      " [-1.24497393]\n",
      " [-0.77790086]\n",
      " ...\n",
      " [-0.49579328]\n",
      " [-0.02002001]\n",
      " [-0.44856128]]\n",
      "Current iteration=8, loss=35367.005976190514\n",
      "t [[ 0.21828165]\n",
      " [-1.30816416]\n",
      " [-0.82119346]\n",
      " ...\n",
      " [-0.52017416]\n",
      " [-0.03942288]\n",
      " [-0.48048893]]\n",
      "t [[ 0.21828165]\n",
      " [-1.30816416]\n",
      " [-0.82119346]\n",
      " ...\n",
      " [-0.52017416]\n",
      " [-0.03942288]\n",
      " [-0.48048893]]\n",
      "t [[ 0.219927  ]\n",
      " [-1.36486263]\n",
      " [-0.8611003 ]\n",
      " ...\n",
      " [-0.54313924]\n",
      " [-0.05940938]\n",
      " [-0.50956065]]\n",
      "loss=34947.60889045273\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01040683]\n",
      " [-0.06407345]\n",
      " [-0.20087697]\n",
      " ...\n",
      " [-0.15414863]\n",
      " [ 0.02996755]\n",
      " [-0.09144781]]\n",
      "t [[ 0.01040683]\n",
      " [-0.06407345]\n",
      " [-0.20087697]\n",
      " ...\n",
      " [-0.15414863]\n",
      " [ 0.02996755]\n",
      " [-0.09144781]]\n",
      "t [[ 0.01067283]\n",
      " [-0.14367886]\n",
      " [-0.33360296]\n",
      " ...\n",
      " [-0.25155507]\n",
      " [ 0.04042337]\n",
      " [-0.16728514]]\n",
      "t [[ 0.01067283]\n",
      " [-0.14367886]\n",
      " [-0.33360296]\n",
      " ...\n",
      " [-0.25155507]\n",
      " [ 0.04042337]\n",
      " [-0.16728514]]\n",
      "Current iteration=2, loss=37882.944742664506\n",
      "t [[ 0.00607192]\n",
      " [-0.22835671]\n",
      " [-0.42828039]\n",
      " ...\n",
      " [-0.31771061]\n",
      " [ 0.03931806]\n",
      " [-0.23202136]]\n",
      "t [[ 0.00607192]\n",
      " [-0.22835671]\n",
      " [-0.42828039]\n",
      " ...\n",
      " [-0.31771061]\n",
      " [ 0.03931806]\n",
      " [-0.23202136]]\n",
      "t [[-0.00069451]\n",
      " [-0.31292387]\n",
      " [-0.50095817]\n",
      " ...\n",
      " [-0.36621326]\n",
      " [ 0.03107417]\n",
      " [-0.28841201]]\n",
      "t [[-0.00069451]\n",
      " [-0.31292387]\n",
      " [-0.50095817]\n",
      " ...\n",
      " [-0.36621326]\n",
      " [ 0.03107417]\n",
      " [-0.28841201]]\n",
      "Current iteration=4, loss=36666.4123666331\n",
      "t [[-0.00825371]\n",
      " [-0.39490634]\n",
      " [-0.56028842]\n",
      " ...\n",
      " [-0.40437785]\n",
      " [ 0.01823581]\n",
      " [-0.33821786]]\n",
      "t [[-0.00825371]\n",
      " [-0.39490634]\n",
      " [-0.56028842]\n",
      " ...\n",
      " [-0.40437785]\n",
      " [ 0.01823581]\n",
      " [-0.33821786]]\n",
      "t [[-0.01589968]\n",
      " [-0.47318367]\n",
      " [-0.61111054]\n",
      " ...\n",
      " [-0.4362674 ]\n",
      " [ 0.00237017]\n",
      " [-0.38264339]]\n",
      "t [[-0.01589968]\n",
      " [-0.47318367]\n",
      " [-0.61111054]\n",
      " ...\n",
      " [-0.4362674 ]\n",
      " [ 0.00237017]\n",
      " [-0.38264339]]\n",
      "Current iteration=6, loss=35866.065566021105\n",
      "t [[-0.02327423]\n",
      " [-0.54732943]\n",
      " [-0.65621596]\n",
      " ...\n",
      " [-0.46419509]\n",
      " [-0.01549852]\n",
      " [-0.42256141]]\n",
      "t [[-0.02327423]\n",
      " [-0.54732943]\n",
      " [-0.65621596]\n",
      " ...\n",
      " [-0.46419509]\n",
      " [-0.01549852]\n",
      " [-0.42256141]]\n",
      "t [[-0.03020602]\n",
      " [-0.61727706]\n",
      " [-0.69725497]\n",
      " ...\n",
      " [-0.48950021]\n",
      " [-0.03466982]\n",
      " [-0.45863331]]\n",
      "t [[-0.03020602]\n",
      " [-0.61727706]\n",
      " [-0.69725497]\n",
      " ...\n",
      " [-0.48950021]\n",
      " [-0.03466982]\n",
      " [-0.45863331]]\n",
      "Current iteration=8, loss=35293.57307396843\n",
      "t [[-0.03662513]\n",
      " [-0.68314342]\n",
      " [-0.73522566]\n",
      " ...\n",
      " [-0.51296768]\n",
      " [-0.05464827]\n",
      " [-0.49137773]]\n",
      "t [[-0.03662513]\n",
      " [-0.68314342]\n",
      " [-0.73522566]\n",
      " ...\n",
      " [-0.51296768]\n",
      " [-0.05464827]\n",
      " [-0.49137773]]\n",
      "t [[-0.04251637]\n",
      " [-0.74513357]\n",
      " [-0.77074675]\n",
      " ...\n",
      " [-0.53506262]\n",
      " [-0.07507479]\n",
      " [-0.52121234]]\n",
      "loss=34866.33496254143\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00888858]\n",
      " [-0.0562347 ]\n",
      " [-0.20305253]\n",
      " ...\n",
      " [-0.15157077]\n",
      " [ 0.03222096]\n",
      " [-0.08577843]]\n",
      "t [[ 0.00888858]\n",
      " [-0.0562347 ]\n",
      " [-0.20305253]\n",
      " ...\n",
      " [-0.15157077]\n",
      " [ 0.03222096]\n",
      " [-0.08577843]]\n",
      "t [[ 0.00773486]\n",
      " [-0.12873452]\n",
      " [-0.33808316]\n",
      " ...\n",
      " [-0.24716748]\n",
      " [ 0.04473167]\n",
      " [-0.15647371]]\n",
      "t [[ 0.00773486]\n",
      " [-0.12873452]\n",
      " [-0.33808316]\n",
      " ...\n",
      " [-0.24716748]\n",
      " [ 0.04473167]\n",
      " [-0.15647371]]\n",
      "Current iteration=2, loss=37910.97245927511\n",
      "t [[ 0.00184113]\n",
      " [-0.2069937 ]\n",
      " [-0.43491792]\n",
      " ...\n",
      " [-0.31189083]\n",
      " [ 0.0454588 ]\n",
      " [-0.21649989]]\n",
      "t [[ 0.00184113]\n",
      " [-0.2069937 ]\n",
      " [-0.43491792]\n",
      " ...\n",
      " [-0.31189083]\n",
      " [ 0.0454588 ]\n",
      " [-0.21649989]]\n",
      "t [[-0.00607625]\n",
      " [-0.28574425]\n",
      " [-0.50953993]\n",
      " ...\n",
      " [-0.3591831 ]\n",
      " [ 0.03883154]\n",
      " [-0.26854889]]\n",
      "t [[-0.00607625]\n",
      " [-0.28574425]\n",
      " [-0.50953993]\n",
      " ...\n",
      " [-0.3591831 ]\n",
      " [ 0.03883154]\n",
      " [-0.26854889]]\n",
      "Current iteration=4, loss=36714.22117748904\n",
      "t [[-0.01464106]\n",
      " [-0.3624308 ]\n",
      " [-0.57060443]\n",
      " ...\n",
      " [-0.39629391]\n",
      " [ 0.02740943]\n",
      " [-0.31433535]]\n",
      "t [[-0.01464106]\n",
      " [-0.3624308 ]\n",
      " [-0.57060443]\n",
      " ...\n",
      " [-0.39629391]\n",
      " [ 0.02740943]\n",
      " [-0.31433535]]\n",
      "t [[-0.02315161]\n",
      " [-0.43586399]\n",
      " [-0.62297232]\n",
      " ...\n",
      " [-0.42725557]\n",
      " [ 0.01277691]\n",
      " [-0.35502795]]\n",
      "t [[-0.02315161]\n",
      " [-0.43586399]\n",
      " [-0.62297232]\n",
      " ...\n",
      " [-0.42725557]\n",
      " [ 0.01277691]\n",
      " [-0.35502795]]\n",
      "Current iteration=6, loss=35928.96897621607\n",
      "t [[-0.03125858]\n",
      " [-0.50556024]\n",
      " [-0.66945927]\n",
      " ...\n",
      " [-0.45436439]\n",
      " [-0.00402442]\n",
      " [-0.39147034]]\n",
      "t [[-0.03125858]\n",
      " [-0.50556024]\n",
      " [-0.66945927]\n",
      " ...\n",
      " [-0.45436439]\n",
      " [-0.00402442]\n",
      " [-0.39147034]]\n",
      "t [[-0.03880171]\n",
      " [-0.57140549]\n",
      " [-0.71173794]\n",
      " ...\n",
      " [-0.4789484 ]\n",
      " [-0.02227711]\n",
      " [-0.42429936]]\n",
      "t [[-0.03880171]\n",
      " [-0.57140549]\n",
      " [-0.71173794]\n",
      " ...\n",
      " [-0.4789484 ]\n",
      " [-0.02227711]\n",
      " [-0.42429936]]\n",
      "Current iteration=8, loss=35367.77527587784\n",
      "t [[-0.04572298]\n",
      " [-0.63347686]\n",
      " [-0.75082564]\n",
      " ...\n",
      " [-0.50178363]\n",
      " [-0.04146932]\n",
      " [-0.45401256]]\n",
      "t [[-0.04572298]\n",
      " [-0.63347686]\n",
      " [-0.75082564]\n",
      " ...\n",
      " [-0.50178363]\n",
      " [-0.04146932]\n",
      " [-0.45401256]]\n",
      "t [[-0.05201899]\n",
      " [-0.69194577]\n",
      " [-0.78735732]\n",
      " ...\n",
      " [-0.52332731]\n",
      " [-0.06122646]\n",
      " [-0.48100919]]\n",
      "loss=34948.97833368813\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0074024 ]\n",
      " [-0.05956225]\n",
      " [-0.20171273]\n",
      " ...\n",
      " [-0.08880893]\n",
      " [-0.01069592]\n",
      " [-0.1811846 ]]\n",
      "t [[ 0.0074024 ]\n",
      " [-0.05956225]\n",
      " [-0.20171273]\n",
      " ...\n",
      " [-0.08880893]\n",
      " [-0.01069592]\n",
      " [-0.1811846 ]]\n",
      "t [[ 0.00496119]\n",
      " [-0.13526907]\n",
      " [-0.33484615]\n",
      " ...\n",
      " [-0.13366704]\n",
      " [-0.0068493 ]\n",
      " [-0.33671212]]\n",
      "t [[ 0.00496119]\n",
      " [-0.13526907]\n",
      " [-0.33484615]\n",
      " ...\n",
      " [-0.13366704]\n",
      " [-0.0068493 ]\n",
      " [-0.33671212]]\n",
      "Current iteration=2, loss=37869.82359675603\n",
      "t [[-0.00204496]\n",
      " [-0.21655471]\n",
      " [-0.42961503]\n",
      " ...\n",
      " [-0.15575892]\n",
      " [ 0.0030058 ]\n",
      " [-0.47316632]]\n",
      "t [[-0.00204496]\n",
      " [-0.21655471]\n",
      " [-0.42961503]\n",
      " ...\n",
      " [-0.15575892]\n",
      " [ 0.0030058 ]\n",
      " [-0.47316632]]\n",
      "t [[-0.01092747]\n",
      " [-0.29814246]\n",
      " [-0.50217381]\n",
      " ...\n",
      " [-0.16617871]\n",
      " [ 0.01463593]\n",
      " [-0.59484055]]\n",
      "t [[-0.01092747]\n",
      " [-0.29814246]\n",
      " [-0.50217381]\n",
      " ...\n",
      " [-0.16617871]\n",
      " [ 0.01463593]\n",
      " [-0.59484055]]\n",
      "Current iteration=4, loss=36652.45414573023\n",
      "t [[-0.02033566]\n",
      " [-0.37748399]\n",
      " [-0.56125125]\n",
      " ...\n",
      " [-0.17073039]\n",
      " [ 0.02601994]\n",
      " [-0.70462684]]\n",
      "t [[-0.02033566]\n",
      " [-0.37748399]\n",
      " [-0.56125125]\n",
      " ...\n",
      " [-0.17073039]\n",
      " [ 0.02601994]\n",
      " [-0.70462684]]\n",
      "t [[-0.02958935]\n",
      " [-0.45340065]\n",
      " [-0.61173937]\n",
      " ...\n",
      " [-0.1725055 ]\n",
      " [ 0.03624773]\n",
      " [-0.80457823]]\n",
      "t [[-0.02958935]\n",
      " [-0.45340065]\n",
      " [-0.61173937]\n",
      " ...\n",
      " [-0.1725055 ]\n",
      " [ 0.03624773]\n",
      " [-0.80457823]]\n",
      "Current iteration=6, loss=35852.4721072571\n",
      "t [[-0.03835661]\n",
      " [-0.52542028]\n",
      " [-0.65646513]\n",
      " ...\n",
      " [-0.17315532]\n",
      " [ 0.0449769 ]\n",
      " [-0.89621977]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.03835661]\n",
      " [-0.52542028]\n",
      " [-0.65646513]\n",
      " ...\n",
      " [-0.17315532]\n",
      " [ 0.0449769 ]\n",
      " [-0.89621977]]\n",
      "t [[-0.04649114]\n",
      " [-0.5934402 ]\n",
      " [-0.6971025 ]\n",
      " ...\n",
      " [-0.17354514]\n",
      " [ 0.0521542 ]\n",
      " [-0.98072582]]\n",
      "t [[-0.04649114]\n",
      " [-0.5934402 ]\n",
      " [-0.6971025 ]\n",
      " ...\n",
      " [-0.17354514]\n",
      " [ 0.0521542 ]\n",
      " [-0.98072582]]\n",
      "Current iteration=8, loss=35280.16410809738\n",
      "t [[-0.05394605]\n",
      " [-0.65754859]\n",
      " [-0.73466497]\n",
      " ...\n",
      " [-0.1741059 ]\n",
      " [ 0.05786798]\n",
      " [-1.05902659]]\n",
      "t [[-0.05394605]\n",
      " [-0.65754859]\n",
      " [-0.73466497]\n",
      " ...\n",
      " [-0.1741059 ]\n",
      " [ 0.05786798]\n",
      " [-1.05902659]]\n",
      "t [[-0.06072676]\n",
      " [-0.71792744]\n",
      " [-0.76978111]\n",
      " ...\n",
      " [-0.17502879]\n",
      " [ 0.06226891]\n",
      " [-1.13187555]]\n",
      "loss=34852.894968285575\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.08163638]\n",
      " [-0.34874125]\n",
      " [-0.21919643]\n",
      " ...\n",
      " [-0.15886308]\n",
      " [ 0.03439473]\n",
      " [-0.09260882]]\n",
      "t [[ 0.08163638]\n",
      " [-0.34874125]\n",
      " [-0.21919643]\n",
      " ...\n",
      " [-0.15886308]\n",
      " [ 0.03439473]\n",
      " [-0.09260882]]\n",
      "t [[ 0.13179259]\n",
      " [-0.58942238]\n",
      " [-0.36737327]\n",
      " ...\n",
      " [-0.25831507]\n",
      " [ 0.04741666]\n",
      " [-0.16888872]]\n",
      "t [[ 0.13179259]\n",
      " [-0.58942238]\n",
      " [-0.36737327]\n",
      " ...\n",
      " [-0.25831507]\n",
      " [ 0.04741666]\n",
      " [-0.16888872]]\n",
      "Current iteration=2, loss=37853.71026113633\n",
      "t [[ 0.16355483]\n",
      " [-0.76684097]\n",
      " [-0.47560603]\n",
      " ...\n",
      " [-0.3256615 ]\n",
      " [ 0.04782779]\n",
      " [-0.23367566]]\n",
      "t [[ 0.16355483]\n",
      " [-0.76684097]\n",
      " [-0.47560603]\n",
      " ...\n",
      " [-0.3256615 ]\n",
      " [ 0.04782779]\n",
      " [-0.23367566]]\n",
      "t [[ 0.18418508]\n",
      " [-0.90510804]\n",
      " [-0.5602588 ]\n",
      " ...\n",
      " [-0.37516586]\n",
      " [ 0.04044471]\n",
      " [-0.28987999]]\n",
      "t [[ 0.18418508]\n",
      " [-0.90510804]\n",
      " [-0.5602588 ]\n",
      " ...\n",
      " [-0.37516586]\n",
      " [ 0.04044471]\n",
      " [-0.28987999]]\n",
      "Current iteration=4, loss=36646.234918040944\n",
      "t [[ 0.19780129]\n",
      " [-1.01756282]\n",
      " [-0.63011402]\n",
      " ...\n",
      " [-0.41436157]\n",
      " [ 0.02802596]\n",
      " [-0.33934295]]\n",
      "t [[ 0.19780129]\n",
      " [-1.01756282]\n",
      " [-0.63011402]\n",
      " ...\n",
      " [-0.41436157]\n",
      " [ 0.02802596]\n",
      " [-0.33934295]]\n",
      "t [[ 0.2068467 ]\n",
      " [-1.11201264]\n",
      " [-0.69010503]\n",
      " ...\n",
      " [-0.44736279]\n",
      " [ 0.01227224]\n",
      " [-0.38331646]]\n",
      "t [[ 0.2068467 ]\n",
      " [-1.11201264]\n",
      " [-0.69010503]\n",
      " ...\n",
      " [-0.44736279]\n",
      " [ 0.01227224]\n",
      " [-0.38331646]]\n",
      "Current iteration=6, loss=35858.838677744774\n",
      "t [[ 0.21282927]\n",
      " [-1.19326847]\n",
      " [-0.74311618]\n",
      " ...\n",
      " [-0.4764727 ]\n",
      " [-0.0057012 ]\n",
      " [-0.42270418]]\n",
      "t [[ 0.21282927]\n",
      " [-1.19326847]\n",
      " [-0.74311618]\n",
      " ...\n",
      " [-0.4764727 ]\n",
      " [-0.0057012 ]\n",
      " [-0.42270418]]\n",
      "t [[ 0.21671109]\n",
      " [-1.26443409]\n",
      " [-0.79089504]\n",
      " ...\n",
      " [-0.50300208]\n",
      " [-0.02512847]\n",
      " [-0.45818906]]\n",
      "t [[ 0.21671109]\n",
      " [-1.26443409]\n",
      " [-0.79089504]\n",
      " ...\n",
      " [-0.50300208]\n",
      " [-0.02512847]\n",
      " [-0.45818906]]\n",
      "Current iteration=8, loss=35299.279432776624\n",
      "t [[ 0.21912416]\n",
      " [-1.32759919]\n",
      " [-0.83453903]\n",
      " ...\n",
      " [-0.52770594]\n",
      " [-0.04546565]\n",
      " [-0.49030555]]\n",
      "t [[ 0.21912416]\n",
      " [-1.32759919]\n",
      " [-0.83453903]\n",
      " ...\n",
      " [-0.52770594]\n",
      " [-0.04546565]\n",
      " [-0.49030555]]\n",
      "t [[ 0.22049476]\n",
      " [-1.384229  ]\n",
      " [-0.87476497]\n",
      " ...\n",
      " [-0.55102413]\n",
      " [-0.06631722]\n",
      " [-0.51948324]]\n",
      "loss=34883.67140130469\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01076569]\n",
      " [-0.06628288]\n",
      " [-0.20780376]\n",
      " ...\n",
      " [-0.1594641 ]\n",
      " [ 0.03100091]\n",
      " [-0.09460118]]\n",
      "t [[ 0.01076569]\n",
      " [-0.06628288]\n",
      " [-0.20780376]\n",
      " ...\n",
      " [-0.1594641 ]\n",
      " [ 0.03100091]\n",
      " [-0.09460118]]\n",
      "t [[ 0.01068913]\n",
      " [-0.14917287]\n",
      " [-0.34272337]\n",
      " ...\n",
      " [-0.25824447]\n",
      " [ 0.04113792]\n",
      " [-0.17250508]]\n",
      "t [[ 0.01068913]\n",
      " [-0.14917287]\n",
      " [-0.34272337]\n",
      " ...\n",
      " [-0.25824447]\n",
      " [ 0.04113792]\n",
      " [-0.17250508]]\n",
      "Current iteration=2, loss=37824.640767122575\n",
      "t [[ 0.00559682]\n",
      " [-0.23711237]\n",
      " [-0.43803132]\n",
      " ...\n",
      " [-0.3245183 ]\n",
      " [ 0.03919556]\n",
      " [-0.23869818]]\n",
      "t [[ 0.00559682]\n",
      " [-0.23711237]\n",
      " [-0.43803132]\n",
      " ...\n",
      " [-0.3245183 ]\n",
      " [ 0.03919556]\n",
      " [-0.23869818]]\n",
      "t [[-0.00162447]\n",
      " [-0.32457307]\n",
      " [-0.51092954]\n",
      " ...\n",
      " [-0.37286147]\n",
      " [ 0.02992429]\n",
      " [-0.29615811]]\n",
      "t [[-0.00162447]\n",
      " [-0.32457307]\n",
      " [-0.51092954]\n",
      " ...\n",
      " [-0.37286147]\n",
      " [ 0.02992429]\n",
      " [-0.29615811]]\n",
      "Current iteration=4, loss=36597.381781671786\n",
      "t [[-0.0095508 ]\n",
      " [-0.40901133]\n",
      " [-0.57045806]\n",
      " ...\n",
      " [-0.41091397]\n",
      " [ 0.01600446]\n",
      " [-0.34676036]]\n",
      "t [[-0.0095508 ]\n",
      " [-0.40901133]\n",
      " [-0.57045806]\n",
      " ...\n",
      " [-0.41091397]\n",
      " [ 0.01600446]\n",
      " [-0.34676036]]\n",
      "t [[-0.01747178]\n",
      " [-0.48933117]\n",
      " [-0.62156322]\n",
      " ...\n",
      " [-0.44282278]\n",
      " [-0.00093356]\n",
      " [-0.39177745]]\n",
      "t [[-0.01747178]\n",
      " [-0.48933117]\n",
      " [-0.62156322]\n",
      " ...\n",
      " [-0.44282278]\n",
      " [-0.00093356]\n",
      " [-0.39177745]]\n",
      "Current iteration=6, loss=35794.92315400499\n",
      "t [[-0.0250404 ]\n",
      " [-0.56515899]\n",
      " [-0.66704095]\n",
      " ...\n",
      " [-0.47089726]\n",
      " [-0.01983375]\n",
      " [-0.43212718]]\n",
      "t [[-0.0250404 ]\n",
      " [-0.56515899]\n",
      " [-0.66704095]\n",
      " ...\n",
      " [-0.47089726]\n",
      " [-0.01983375]\n",
      " [-0.43212718]]\n",
      "t [[-0.03210013]\n",
      " [-0.63648451]\n",
      " [-0.70851353]\n",
      " ...\n",
      " [-0.49644548]\n",
      " [-0.03997906]\n",
      " [-0.46850302]]\n",
      "t [[-0.03210013]\n",
      " [-0.63648451]\n",
      " [-0.70851353]\n",
      " ...\n",
      " [-0.49644548]\n",
      " [-0.03997906]\n",
      " [-0.46850302]]\n",
      "Current iteration=8, loss=35224.54388426828\n",
      "t [[-0.0385951 ]\n",
      " [-0.7034755 ]\n",
      " [-0.74694597]\n",
      " ...\n",
      " [-0.52021772]\n",
      " [-0.06086551]\n",
      " [-0.50144779]]\n",
      "t [[-0.0385951 ]\n",
      " [-0.7034755 ]\n",
      " [-0.74694597]\n",
      " ...\n",
      " [-0.52021772]\n",
      " [-0.06086551]\n",
      " [-0.50144779]]\n",
      "t [[-0.04452186]\n",
      " [-0.76637993]\n",
      " [-0.78292879]\n",
      " ...\n",
      " [-0.54264944]\n",
      " [-0.0821301 ]\n",
      " [-0.53139793]]\n",
      "loss=34801.22660220652\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00919509]\n",
      " [-0.05817383]\n",
      " [-0.21005435]\n",
      " ...\n",
      " [-0.15679735]\n",
      " [ 0.03333203]\n",
      " [-0.08873631]]\n",
      "t [[ 0.00919509]\n",
      " [-0.05817383]\n",
      " [-0.21005435]\n",
      " ...\n",
      " [-0.15679735]\n",
      " [ 0.03333203]\n",
      " [-0.08873631]]\n",
      "t [[ 0.00765315]\n",
      " [-0.13373972]\n",
      " [-0.34736164]\n",
      " ...\n",
      " [-0.25373207]\n",
      " [ 0.04558724]\n",
      " [-0.16133954]]\n",
      "t [[ 0.00765315]\n",
      " [-0.13373972]\n",
      " [-0.34736164]\n",
      " ...\n",
      " [-0.25373207]\n",
      " [ 0.04558724]\n",
      " [-0.16133954]]\n",
      "Current iteration=2, loss=37853.46785704256\n",
      "t [[ 0.00123224]\n",
      " [-0.21508766]\n",
      " [-0.44489051]\n",
      " ...\n",
      " [-0.31855055]\n",
      " [ 0.04552446]\n",
      " [-0.22269048]]\n",
      "t [[ 0.00123224]\n",
      " [-0.21508766]\n",
      " [-0.44489051]\n",
      " ...\n",
      " [-0.31855055]\n",
      " [ 0.04552446]\n",
      " [-0.22269048]]\n",
      "t [[-0.00716476]\n",
      " [-0.29659419]\n",
      " [-0.51977778]\n",
      " ...\n",
      " [-0.36566503]\n",
      " [ 0.03790266]\n",
      " [-0.27569759]]\n",
      "t [[-0.00716476]\n",
      " [-0.29659419]\n",
      " [-0.51977778]\n",
      " ...\n",
      " [-0.36566503]\n",
      " [ 0.03790266]\n",
      " [-0.27569759]]\n",
      "Current iteration=4, loss=36646.41892357649\n",
      "t [[-0.0161109 ]\n",
      " [-0.37562637]\n",
      " [-0.58107128]\n",
      " ...\n",
      " [-0.40264953]\n",
      " [ 0.02541973]\n",
      " [-0.3221867 ]]\n",
      "t [[-0.0161109 ]\n",
      " [-0.37562637]\n",
      " [-0.58107128]\n",
      " ...\n",
      " [-0.40264953]\n",
      " [ 0.02541973]\n",
      " [-0.3221867 ]]\n",
      "t [[-0.02490144]\n",
      " [-0.45101356]\n",
      " [-0.63374285]\n",
      " ...\n",
      " [-0.43362064]\n",
      " [ 0.00972522]\n",
      " [-0.36339196]]\n",
      "t [[-0.02490144]\n",
      " [-0.45101356]\n",
      " [-0.63374285]\n",
      " ...\n",
      " [-0.43362064]\n",
      " [ 0.00972522]\n",
      " [-0.36339196]]\n",
      "Current iteration=6, loss=35859.211805672596\n",
      "t [[-0.03319981]\n",
      " [-0.52232071]\n",
      " [-0.68061555]\n",
      " ...\n",
      " [-0.460871  ]\n",
      " [-0.00810567]\n",
      " [-0.4001999 ]]\n",
      "t [[-0.03319981]\n",
      " [-0.52232071]\n",
      " [-0.68061555]\n",
      " ...\n",
      " [-0.460871  ]\n",
      " [-0.00810567]\n",
      " [-0.4001999 ]]\n",
      "t [[-0.04086217]\n",
      " [-0.58948683]\n",
      " [-0.723336  ]\n",
      " ...\n",
      " [-0.48569727]\n",
      " [-0.02733712]\n",
      " [-0.43327775]]\n",
      "t [[-0.04086217]\n",
      " [-0.58948683]\n",
      " [-0.723336  ]\n",
      " ...\n",
      " [-0.48569727]\n",
      " [-0.02733712]\n",
      " [-0.43327775]]\n",
      "Current iteration=8, loss=35300.12023464317\n",
      "t [[-0.04784596]\n",
      " [-0.65263739]\n",
      " [-0.76288983]\n",
      " ...\n",
      " [-0.50884036]\n",
      " [-0.04744717]\n",
      " [-0.46314573]]\n",
      "t [[-0.04784596]\n",
      " [-0.65263739]\n",
      " [-0.76288983]\n",
      " ...\n",
      " [-0.50884036]\n",
      " [-0.04744717]\n",
      " [-0.46314573]]\n",
      "t [[-0.05416074]\n",
      " [-0.71198478]\n",
      " [-0.79988472]\n",
      " ...\n",
      " [-0.53072729]\n",
      " [-0.06805593]\n",
      " [-0.49022063]]\n",
      "loss=34885.15130155764\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00765766]\n",
      " [-0.06161612]\n",
      " [-0.20866834]\n",
      " ...\n",
      " [-0.09187131]\n",
      " [-0.01106475]\n",
      " [-0.18743234]]\n",
      "t [[ 0.00765766]\n",
      " [-0.06161612]\n",
      " [-0.20866834]\n",
      " ...\n",
      " [-0.09187131]\n",
      " [-0.01106475]\n",
      " [-0.18743234]]\n",
      "t [[ 0.0047908 ]\n",
      " [-0.1404947 ]\n",
      " [-0.34399429]\n",
      " ...\n",
      " [-0.13673935]\n",
      " [-0.00657616]\n",
      " [-0.34741989]]\n",
      "t [[ 0.0047908 ]\n",
      " [-0.1404947 ]\n",
      " [-0.34399429]\n",
      " ...\n",
      " [-0.13673935]\n",
      " [-0.00657616]\n",
      " [-0.34741989]]\n",
      "Current iteration=2, loss=37811.36578502557\n",
      "t [[-0.0027686 ]\n",
      " [-0.22496042]\n",
      " [-0.43937513]\n",
      " ...\n",
      " [-0.15801964]\n",
      " [ 0.00403286]\n",
      " [-0.48724776]]\n",
      "t [[-0.0027686 ]\n",
      " [-0.22496042]\n",
      " [-0.43937513]\n",
      " ...\n",
      " [-0.15801964]\n",
      " [ 0.00403286]\n",
      " [-0.48724776]]\n",
      "t [[-0.0121485 ]\n",
      " [-0.30938215]\n",
      " [-0.51212863]\n",
      " ...\n",
      " [-0.16759051]\n",
      " [ 0.01624459]\n",
      " [-0.61157163]]\n",
      "t [[-0.0121485 ]\n",
      " [-0.30938215]\n",
      " [-0.51212863]\n",
      " ...\n",
      " [-0.16759051]\n",
      " [ 0.01624459]\n",
      " [-0.61157163]]\n",
      "Current iteration=4, loss=36583.43586744105\n",
      "t [[-0.02195002]\n",
      " [-0.39113554]\n",
      " [-0.57137727]\n",
      " ...\n",
      " [-0.17149291]\n",
      " [ 0.02798151]\n",
      " [-0.72348636]]\n",
      "t [[-0.02195002]\n",
      " [-0.39113554]\n",
      " [-0.57137727]\n",
      " ...\n",
      " [-0.17149291]\n",
      " [ 0.02798151]\n",
      " [-0.72348636]]\n",
      "t [[-0.03149174]\n",
      " [-0.46906203]\n",
      " [-0.62212304]\n",
      " ...\n",
      " [-0.17285728]\n",
      " [ 0.03835535]\n",
      " [-0.82517114]]\n",
      "t [[-0.03149174]\n",
      " [-0.46906203]\n",
      " [-0.62212304]\n",
      " ...\n",
      " [-0.17285728]\n",
      " [ 0.03835535]\n",
      " [-0.82517114]]\n",
      "Current iteration=6, loss=35781.365353880254\n",
      "t [[-0.04045578]\n",
      " [-0.54273926]\n",
      " [-0.66719875]\n",
      " ...\n",
      " [-0.1733052 ]\n",
      " [ 0.04706799]\n",
      " [-0.91823624]]\n",
      "t [[-0.04045578]\n",
      " [-0.54273926]\n",
      " [-0.66719875]\n",
      " ...\n",
      " [-0.1733052 ]\n",
      " [ 0.04706799]\n",
      " [-0.91823624]]\n",
      "t [[-0.04871307]\n",
      " [-0.6121188 ]\n",
      " [-0.70825073]\n",
      " ...\n",
      " [-0.17365418]\n",
      " [ 0.05411163]\n",
      " [-1.00391711]]\n",
      "t [[-0.04871307]\n",
      " [-0.6121188 ]\n",
      " [-0.70825073]\n",
      " ...\n",
      " [-0.17365418]\n",
      " [ 0.05411163]\n",
      " [-1.00391711]]\n",
      "Current iteration=8, loss=35211.14814393238\n",
      "t [[-0.05623274]\n",
      " [-0.67733827]\n",
      " [-0.74625928]\n",
      " ...\n",
      " [-0.17428865]\n",
      " [ 0.05961364]\n",
      " [-1.08318968]]\n",
      "t [[-0.05623274]\n",
      " [-0.67733827]\n",
      " [-0.74625928]\n",
      " ...\n",
      " [-0.17428865]\n",
      " [ 0.05961364]\n",
      " [-1.08318968]]\n",
      "t [[-0.06303357]\n",
      " [-0.73862161]\n",
      " [-0.78182438]\n",
      " ...\n",
      " [-0.17536119]\n",
      " [ 0.06375552]\n",
      " [-1.1568427 ]]\n",
      "loss=34787.772671074315\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.08435759]\n",
      " [-0.36036596]\n",
      " [-0.22650298]\n",
      " ...\n",
      " [-0.16415851]\n",
      " [ 0.03554122]\n",
      " [-0.09569578]]\n",
      "t [[ 0.08435759]\n",
      " [-0.36036596]\n",
      " [-0.22650298]\n",
      " ...\n",
      " [-0.16415851]\n",
      " [ 0.03554122]\n",
      " [-0.09569578]]\n",
      "t [[ 0.13512277]\n",
      " [-0.60542357]\n",
      " [-0.37722379]\n",
      " ...\n",
      " [-0.26492062]\n",
      " [ 0.04827919]\n",
      " [-0.1739647 ]]\n",
      "t [[ 0.13512277]\n",
      " [-0.60542357]\n",
      " [-0.37722379]\n",
      " ...\n",
      " [-0.26492062]\n",
      " [ 0.04827919]\n",
      " [-0.1739647 ]]\n",
      "Current iteration=2, loss=37797.34104793377\n",
      "t [[ 0.16671475]\n",
      " [-0.78452161]\n",
      " [-0.48639299]\n",
      " ...\n",
      " [-0.33236648]\n",
      " [ 0.04786216]\n",
      " [-0.24013613]]\n",
      "t [[ 0.16671475]\n",
      " [-0.78452161]\n",
      " [-0.48639299]\n",
      " ...\n",
      " [-0.33236648]\n",
      " [ 0.04786216]\n",
      " [-0.24013613]]\n",
      "t [[ 0.18691812]\n",
      " [-0.92345913]\n",
      " [-0.57149789]\n",
      " ...\n",
      " [-0.38173186]\n",
      " [ 0.03944865]\n",
      " [-0.29734454]]\n",
      "t [[ 0.18691812]\n",
      " [-0.92345913]\n",
      " [-0.57149789]\n",
      " ...\n",
      " [-0.38173186]\n",
      " [ 0.03944865]\n",
      " [-0.29734454]]\n",
      "Current iteration=4, loss=36580.25236162371\n",
      "t [[ 0.20005363]\n",
      " [-1.03619865]\n",
      " [-0.64169851]\n",
      " ...\n",
      " [-0.42085756]\n",
      " [ 0.02593903]\n",
      " [-0.34754531]]\n",
      "t [[ 0.20005363]\n",
      " [-1.03619865]\n",
      " [-0.64169851]\n",
      " ...\n",
      " [-0.42085756]\n",
      " [ 0.02593903]\n",
      " [-0.34754531]]\n",
      "t [[ 0.20864153]\n",
      " [-1.13077883]\n",
      " [-0.70203825]\n",
      " ...\n",
      " [-0.45392755]\n",
      " [ 0.00910064]\n",
      " [-0.39205767]]\n",
      "t [[ 0.20864153]\n",
      " [-1.13077883]\n",
      " [-0.70203825]\n",
      " ...\n",
      " [-0.45392755]\n",
      " [ 0.00910064]\n",
      " [-0.39205767]]\n",
      "Current iteration=6, loss=35791.35385369453\n",
      "t [[ 0.21421526]\n",
      " [-1.21209158]\n",
      " [-0.75541551]\n",
      " ...\n",
      " [-0.48323224]\n",
      " [-0.00991707]\n",
      " [-0.43182968]]\n",
      "t [[ 0.21421526]\n",
      " [-1.21209158]\n",
      " [-0.75541551]\n",
      " ...\n",
      " [-0.48323224]\n",
      " [-0.00991707]\n",
      " [-0.43182968]]\n",
      "t [[ 0.21774144]\n",
      " [-1.28326472]\n",
      " [-0.80356153]\n",
      " ...\n",
      " [-0.51004638]\n",
      " [-0.03032986]\n",
      " [-0.46757591]]\n",
      "t [[ 0.21774144]\n",
      " [-1.28326472]\n",
      " [-0.80356153]\n",
      " ...\n",
      " [-0.51004638]\n",
      " [-0.03032986]\n",
      " [-0.46757591]]\n",
      "Current iteration=8, loss=35234.17045511595\n",
      "t [[ 0.21984872]\n",
      " [-1.34639264]\n",
      " [-0.84755235]\n",
      " ...\n",
      " [-0.53508828]\n",
      " [-0.05158415]\n",
      " [-0.49985457]]\n",
      "t [[ 0.21984872]\n",
      " [-1.34639264]\n",
      " [-0.84755235]\n",
      " ...\n",
      " [-0.53508828]\n",
      " [-0.05158415]\n",
      " [-0.49985457]]\n",
      "t [[ 0.2209577 ]\n",
      " [-1.40294053]\n",
      " [-0.88808715]\n",
      " ...\n",
      " [-0.55876782]\n",
      " [-0.07327986]\n",
      " [-0.52911371]]\n",
      "loss=34822.50568934305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01112454]\n",
      " [-0.06849231]\n",
      " [-0.21473055]\n",
      " ...\n",
      " [-0.16477957]\n",
      " [ 0.03203427]\n",
      " [-0.09775455]]\n",
      "t [[ 0.01112454]\n",
      " [-0.06849231]\n",
      " [-0.21473055]\n",
      " ...\n",
      " [-0.16477957]\n",
      " [ 0.03203427]\n",
      " [-0.09775455]]\n",
      "t [[ 0.01068297]\n",
      " [-0.15470138]\n",
      " [-0.35168972]\n",
      " ...\n",
      " [-0.26480549]\n",
      " [ 0.04180884]\n",
      " [-0.17768929]]\n",
      "t [[ 0.01068297]\n",
      " [-0.15470138]\n",
      " [-0.35168972]\n",
      " ...\n",
      " [-0.26480549]\n",
      " [ 0.04180884]\n",
      " [-0.17768929]]\n",
      "Current iteration=2, loss=37767.52344509152\n",
      "t [[ 0.00508995]\n",
      " [-0.24590023]\n",
      " [-0.44752715]\n",
      " ...\n",
      " [-0.33111658]\n",
      " [ 0.03899587]\n",
      " [-0.24529922]]\n",
      "t [[ 0.00508995]\n",
      " [-0.24590023]\n",
      " [-0.44752715]\n",
      " ...\n",
      " [-0.33111658]\n",
      " [ 0.03899587]\n",
      " [-0.24529922]]\n",
      "t [[-0.00258252]\n",
      " [-0.33621789]\n",
      " [-0.52060555]\n",
      " ...\n",
      " [-0.3792732 ]\n",
      " [ 0.02867824]\n",
      " [-0.30379   ]]\n",
      "t [[-0.00258252]\n",
      " [-0.33621789]\n",
      " [-0.52060555]\n",
      " ...\n",
      " [-0.3792732 ]\n",
      " [ 0.02867824]\n",
      " [-0.30379   ]]\n",
      "Current iteration=4, loss=36530.197975167895\n",
      "t [[-0.01086441]\n",
      " [-0.42305408]\n",
      " [-0.58032955]\n",
      " ...\n",
      " [-0.4172201 ]\n",
      " [ 0.01366964]\n",
      " [-0.35515254]]\n",
      "t [[-0.01086441]\n",
      " [-0.42305408]\n",
      " [-0.58032955]\n",
      " ...\n",
      " [-0.4172201 ]\n",
      " [ 0.01366964]\n",
      " [-0.35515254]]\n",
      "t [[-0.01904521]\n",
      " [-0.5053482 ]\n",
      " [-0.63173183]\n",
      " ...\n",
      " [-0.44916997]\n",
      " [-0.0043392 ]\n",
      " [-0.40072738]]\n",
      "t [[-0.01904521]\n",
      " [-0.5053482 ]\n",
      " [-0.63173183]\n",
      " ...\n",
      " [-0.44916997]\n",
      " [-0.0043392 ]\n",
      " [-0.40072738]]\n",
      "Current iteration=6, loss=35726.120856691734\n",
      "t [[-0.02679189]\n",
      " [-0.58278674]\n",
      " [-0.67759942]\n",
      " ...\n",
      " [-0.47741637]\n",
      " [-0.02426258]\n",
      " [-0.44147721]]\n",
      "t [[-0.02679189]\n",
      " [-0.58278674]\n",
      " [-0.67759942]\n",
      " ...\n",
      " [-0.47741637]\n",
      " [-0.02426258]\n",
      " [-0.44147721]]\n",
      "t [[-0.03396438]\n",
      " [-0.65542006]\n",
      " [-0.7195193 ]\n",
      " ...\n",
      " [-0.50322958]\n",
      " [-0.04536833]\n",
      " [-0.47812757]]\n",
      "t [[-0.03396438]\n",
      " [-0.65542006]\n",
      " [-0.7195193 ]\n",
      " ...\n",
      " [-0.50322958]\n",
      " [-0.04536833]\n",
      " [-0.47812757]]\n",
      "Current iteration=8, loss=35158.187871141665\n",
      "t [[-0.04052157]\n",
      " [-0.72346904]\n",
      " [-0.75842028]\n",
      " ...\n",
      " [-0.52732225]\n",
      " [-0.06714502]\n",
      " [-0.51124542]]\n",
      "t [[-0.04052157]\n",
      " [-0.72346904]\n",
      " [-0.75842028]\n",
      " ...\n",
      " [-0.52732225]\n",
      " [-0.06714502]\n",
      " [-0.51124542]]\n",
      "t [[-0.04647201]\n",
      " [-0.7872256 ]\n",
      " [-0.79486402]\n",
      " ...\n",
      " [-0.55009954]\n",
      " [-0.08922661]\n",
      " [-0.541286  ]]\n",
      "loss=34738.95207294455\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00950159]\n",
      " [-0.06011295]\n",
      " [-0.21705616]\n",
      " ...\n",
      " [-0.16202393]\n",
      " [ 0.03444309]\n",
      " [-0.09169419]]\n",
      "t [[ 0.00950159]\n",
      " [-0.06011295]\n",
      " [-0.21705616]\n",
      " ...\n",
      " [-0.16202393]\n",
      " [ 0.03444309]\n",
      " [-0.09169419]]\n",
      "t [[ 0.0075492 ]\n",
      " [-0.13878124]\n",
      " [-0.35648622]\n",
      " ...\n",
      " [-0.26016997]\n",
      " [ 0.04639865]\n",
      " [-0.16617088]]\n",
      "t [[ 0.0075492 ]\n",
      " [-0.13878124]\n",
      " [-0.35648622]\n",
      " ...\n",
      " [-0.26016997]\n",
      " [ 0.04639865]\n",
      " [-0.16617088]]\n",
      "Current iteration=2, loss=37797.14321529836\n",
      "t [[ 0.00059243]\n",
      " [-0.22321859]\n",
      " [-0.45460688]\n",
      " ...\n",
      " [-0.3250034 ]\n",
      " [ 0.0455113 ]\n",
      " [-0.22880827]]\n",
      "t [[ 0.00059243]\n",
      " [-0.22321859]\n",
      " [-0.45460688]\n",
      " ...\n",
      " [-0.3250034 ]\n",
      " [ 0.0455113 ]\n",
      " [-0.22880827]]\n",
      "t [[-0.0082794 ]\n",
      " [-0.30744795]\n",
      " [-0.52971735]\n",
      " ...\n",
      " [-0.37191356]\n",
      " [ 0.03687458]\n",
      " [-0.2827371 ]]\n",
      "t [[-0.0082794 ]\n",
      " [-0.30744795]\n",
      " [-0.52971735]\n",
      " ...\n",
      " [-0.37191356]\n",
      " [ 0.03687458]\n",
      " [-0.2827371 ]]\n",
      "Current iteration=4, loss=36580.443615780714\n",
      "t [[-0.0175939 ]\n",
      " [-0.38877148]\n",
      " [-0.59123527]\n",
      " ...\n",
      " [-0.40877879]\n",
      " [ 0.02332193]\n",
      " [-0.32989503]]\n",
      "t [[-0.0175939 ]\n",
      " [-0.38877148]\n",
      " [-0.59123527]\n",
      " ...\n",
      " [-0.40877879]\n",
      " [ 0.02332193]\n",
      " [-0.32989503]]\n",
      "t [[-0.02664769]\n",
      " [-0.46604795]\n",
      " [-0.64422296]\n",
      " ...\n",
      " [-0.4397819 ]\n",
      " [ 0.00656528]\n",
      " [-0.37158155]]\n",
      "t [[-0.02664769]\n",
      " [-0.46604795]\n",
      " [-0.64422296]\n",
      " ...\n",
      " [-0.4397819 ]\n",
      " [ 0.00656528]\n",
      " [-0.37158155]]\n",
      "Current iteration=6, loss=35791.75656682472\n",
      "t [[-0.03511995]\n",
      " [-0.53889807]\n",
      " [-0.69149754]\n",
      " ...\n",
      " [-0.46719985]\n",
      " [-0.01228852]\n",
      " [-0.40872595]]\n",
      "t [[-0.03511995]\n",
      " [-0.53889807]\n",
      " [-0.69149754]\n",
      " ...\n",
      " [-0.46719985]\n",
      " [-0.01228852]\n",
      " [-0.40872595]]\n",
      "t [[-0.04288495]\n",
      " [-0.6073182 ]\n",
      " [-0.73467229]\n",
      " ...\n",
      " [-0.49229125]\n",
      " [-0.03248669]\n",
      " [-0.44202573]]\n",
      "t [[-0.04288495]\n",
      " [-0.6073182 ]\n",
      " [-0.73467229]\n",
      " ...\n",
      " [-0.49229125]\n",
      " [-0.03248669]\n",
      " [-0.44202573]]\n",
      "Current iteration=8, loss=35235.08632037282\n",
      "t [[-0.0499164 ]\n",
      " [-0.67148437]\n",
      " [-0.77469802]\n",
      " ...\n",
      " [-0.51575892]\n",
      " [-0.05349815]\n",
      " [-0.47202374]]\n",
      "t [[-0.0499164 ]\n",
      " [-0.67148437]\n",
      " [-0.77469802]\n",
      " ...\n",
      " [-0.51575892]\n",
      " [-0.05349815]\n",
      " [-0.47202374]]\n",
      "t [[-0.05623711]\n",
      " [-0.7316509 ]\n",
      " [-0.81215445]\n",
      " ...\n",
      " [-0.53799888]\n",
      " [-0.0749385 ]\n",
      " [-0.49915429]]\n",
      "loss=34824.09793966479\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00791291]\n",
      " [-0.06366999]\n",
      " [-0.21562395]\n",
      " ...\n",
      " [-0.09493369]\n",
      " [-0.01143357]\n",
      " [-0.19368009]]\n",
      "t [[ 0.00791291]\n",
      " [-0.06366999]\n",
      " [-0.21562395]\n",
      " ...\n",
      " [-0.09493369]\n",
      " [-0.01143357]\n",
      " [-0.19368009]]\n",
      "t [[ 0.00459862]\n",
      " [-0.14575626]\n",
      " [-0.3529874 ]\n",
      " ...\n",
      " [-0.13971232]\n",
      " [-0.00627002]\n",
      " [-0.35806871]]\n",
      "t [[ 0.00459862]\n",
      " [-0.14575626]\n",
      " [-0.3529874 ]\n",
      " ...\n",
      " [-0.13971232]\n",
      " [-0.00627002]\n",
      " [-0.35806871]]\n",
      "Current iteration=2, loss=37754.10748830488\n",
      "t [[-0.00352195]\n",
      " [-0.23340178]\n",
      " [-0.44887806]\n",
      " ...\n",
      " [-0.16012822]\n",
      " [ 0.00509991]\n",
      " [-0.5011992 ]]\n",
      "t [[-0.00352195]\n",
      " [-0.23340178]\n",
      " [-0.44887806]\n",
      " ...\n",
      " [-0.16012822]\n",
      " [ 0.00509991]\n",
      " [-0.5011992 ]]\n",
      "t [[-0.01339362]\n",
      " [-0.32062307]\n",
      " [-0.52178532]\n",
      " ...\n",
      " [-0.16884674]\n",
      " [ 0.01787613]\n",
      " [-0.62810152]]\n",
      "t [[-0.01339362]\n",
      " [-0.32062307]\n",
      " [-0.52178532]\n",
      " ...\n",
      " [-0.16884674]\n",
      " [ 0.01787613]\n",
      " [-0.62810152]]\n",
      "Current iteration=4, loss=36516.27004340652\n",
      "t [[-0.02357474]\n",
      " [-0.40473251]\n",
      " [-0.58120221]\n",
      " ...\n",
      " [-0.17212633]\n",
      " [ 0.02993576]\n",
      " [-0.74207632]]\n",
      "t [[-0.02357474]\n",
      " [-0.40473251]\n",
      " [-0.58120221]\n",
      " ...\n",
      " [-0.17212633]\n",
      " [ 0.02993576]\n",
      " [-0.74207632]]\n",
      "t [[-0.03338711]\n",
      " [-0.48460242]\n",
      " [-0.63222   ]\n",
      " ...\n",
      " [-0.17311937]\n",
      " [ 0.04042162]\n",
      " [-0.8454297 ]]\n",
      "t [[-0.03338711]\n",
      " [-0.48460242]\n",
      " [-0.63222   ]\n",
      " ...\n",
      " [-0.17311937]\n",
      " [ 0.04042162]\n",
      " [-0.8454297 ]]\n",
      "Current iteration=6, loss=35712.597049713055\n",
      "t [[-0.04252987]\n",
      " [-0.55986752]\n",
      " [-0.67766381]\n",
      " ...\n",
      " [-0.17340658]\n",
      " [ 0.04908585]\n",
      " [-0.93985737]]\n",
      "t [[-0.04252987]\n",
      " [-0.55986752]\n",
      " [-0.67766381]\n",
      " ...\n",
      " [-0.17340658]\n",
      " [ 0.04908585]\n",
      " [-0.93985737]]\n",
      "t [[-0.05089292]\n",
      " [-0.63053798]\n",
      " [-0.71914491]\n",
      " ...\n",
      " [-0.17375146]\n",
      " [ 0.05596922]\n",
      " [-1.02665592]]\n",
      "t [[-0.05089292]\n",
      " [-0.63053798]\n",
      " [-0.71914491]\n",
      " ...\n",
      " [-0.17375146]\n",
      " [ 0.05596922]\n",
      " [-1.02665592]]\n",
      "Current iteration=8, loss=35144.80155107459\n",
      "t [[-0.05846216]\n",
      " [-0.69680315]\n",
      " [-0.75760717]\n",
      " ...\n",
      " [-0.17448927]\n",
      " [ 0.06123939]\n",
      " [-1.10684696]]\n",
      "t [[-0.05846216]\n",
      " [-0.69680315]\n",
      " [-0.75760717]\n",
      " ...\n",
      " [-0.17448927]\n",
      " [ 0.06123939]\n",
      " [-1.10684696]]\n",
      "t [[-0.06526999]\n",
      " [-0.75892991]\n",
      " [-0.79362129]\n",
      " ...\n",
      " [-0.17573324]\n",
      " [ 0.0651088 ]\n",
      " [-1.18125443]]\n",
      "loss=34725.48071553065\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0870788 ]\n",
      " [-0.37199067]\n",
      " [-0.23380953]\n",
      " ...\n",
      " [-0.16945395]\n",
      " [ 0.03668771]\n",
      " [-0.09878274]]\n",
      "t [[ 0.0870788 ]\n",
      " [-0.37199067]\n",
      " [-0.23380953]\n",
      " ...\n",
      " [-0.16945395]\n",
      " [ 0.03668771]\n",
      " [-0.09878274]]\n",
      "t [[ 0.13838645]\n",
      " [-0.6211969 ]\n",
      " [-0.38692473]\n",
      " ...\n",
      " [-0.27140083]\n",
      " [ 0.04909715]\n",
      " [-0.17900582]]\n",
      "t [[ 0.13838645]\n",
      " [-0.6211969 ]\n",
      " [-0.38692473]\n",
      " ...\n",
      " [-0.27140083]\n",
      " [ 0.04909715]\n",
      " [-0.17900582]]\n",
      "Current iteration=2, loss=37742.113429808334\n",
      "t [[ 0.16975962]\n",
      " [-0.80180499]\n",
      " [-0.49692957]\n",
      " ...\n",
      " [-0.33887047]\n",
      " [ 0.0478178 ]\n",
      " [-0.24652332]]\n",
      "t [[ 0.16975962]\n",
      " [-0.80180499]\n",
      " [-0.49692957]\n",
      " ...\n",
      " [-0.33887047]\n",
      " [ 0.0478178 ]\n",
      " [-0.24652332]]\n",
      "t [[ 0.18951212]\n",
      " [-0.94131642]\n",
      " [-0.58244024]\n",
      " ...\n",
      " [-0.38807425]\n",
      " [ 0.03835427]\n",
      " [-0.30469926]]\n",
      "t [[ 0.18951212]\n",
      " [-0.94131642]\n",
      " [-0.58244024]\n",
      " ...\n",
      " [-0.38807425]\n",
      " [ 0.03835427]\n",
      " [-0.30469926]]\n",
      "Current iteration=4, loss=36516.015342223734\n",
      "t [[ 0.20216053]\n",
      " [-1.05429139]\n",
      " [-0.65297257]\n",
      " ...\n",
      " [-0.42713882]\n",
      " [ 0.02374588]\n",
      " [-0.35560373]]\n",
      "t [[ 0.20216053]\n",
      " [-1.05429139]\n",
      " [-0.65297257]\n",
      " ...\n",
      " [-0.42713882]\n",
      " [ 0.02374588]\n",
      " [-0.35560373]]\n",
      "t [[ 0.21029502]\n",
      " [-1.14897746]\n",
      " [-0.71366157]\n",
      " ...\n",
      " [-0.46029976]\n",
      " [ 0.00582384]\n",
      " [-0.40062316]]\n",
      "t [[ 0.21029502]\n",
      " [-1.14897746]\n",
      " [-0.71366157]\n",
      " ...\n",
      " [-0.46029976]\n",
      " [ 0.00582384]\n",
      " [-0.40062316]]\n",
      "Current iteration=6, loss=35726.06125528158\n",
      "t [[ 0.21546954]\n",
      " [-1.23033318]\n",
      " [-0.76740812]\n",
      " ...\n",
      " [-0.48982316]\n",
      " [-0.01423018]\n",
      " [-0.44074987]]\n",
      "t [[ 0.21546954]\n",
      " [-1.23033318]\n",
      " [-0.76740812]\n",
      " ...\n",
      " [-0.48982316]\n",
      " [-0.01423018]\n",
      " [-0.44074987]]\n",
      "t [[ 0.21865217]\n",
      " [-1.30150276]\n",
      " [-0.81592118]\n",
      " ...\n",
      " [-0.51694175]\n",
      " [-0.03561507]\n",
      " [-0.47673002]]\n",
      "t [[ 0.21865217]\n",
      " [-1.30150276]\n",
      " [-0.81592118]\n",
      " ...\n",
      " [-0.51694175]\n",
      " [-0.03561507]\n",
      " [-0.47673002]]\n",
      "Current iteration=8, loss=35171.543196530365\n",
      "t [[ 0.22046649]\n",
      " [-1.36458184]\n",
      " [-0.86025311]\n",
      " ...\n",
      " [-0.54233471]\n",
      " [-0.05776867]\n",
      " [-0.50914558]]\n",
      "t [[ 0.22046649]\n",
      " [-1.36458184]\n",
      " [-0.86025311]\n",
      " ...\n",
      " [-0.54233471]\n",
      " [-0.05776867]\n",
      " [-0.50914558]]\n",
      "t [[ 0.22132656]\n",
      " [-1.4210346 ]\n",
      " [-0.90108525]\n",
      " ...\n",
      " [-0.5663816 ]\n",
      " [-0.08028722]\n",
      " [-0.53846308]]\n",
      "loss=34763.95032792616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0114834 ]\n",
      " [-0.07070174]\n",
      " [-0.22165735]\n",
      " ...\n",
      " [-0.17009504]\n",
      " [ 0.03306764]\n",
      " [-0.10090792]]\n",
      "t [[ 0.0114834 ]\n",
      " [-0.07070174]\n",
      " [-0.22165735]\n",
      " ...\n",
      " [-0.17009504]\n",
      " [ 0.03306764]\n",
      " [-0.10090792]]\n",
      "t [[ 0.0106545 ]\n",
      " [-0.16026424]\n",
      " [-0.36050263]\n",
      " ...\n",
      " [-0.27123863]\n",
      " [ 0.04243635]\n",
      " [-0.18283789]]\n",
      "t [[ 0.0106545 ]\n",
      " [-0.16026424]\n",
      " [-0.36050263]\n",
      " ...\n",
      " [-0.27123863]\n",
      " [ 0.04243635]\n",
      " [-0.18283789]]\n",
      "Current iteration=2, loss=37711.553101536694\n",
      "t [[ 0.00455253]\n",
      " [-0.25471796]\n",
      " [-0.45677495]\n",
      " ...\n",
      " [-0.33751143]\n",
      " [ 0.03872088]\n",
      " [-0.25182564]]\n",
      "t [[ 0.00455253]\n",
      " [-0.25471796]\n",
      " [-0.45677495]\n",
      " ...\n",
      " [-0.33751143]\n",
      " [ 0.03872088]\n",
      " [-0.25182564]]\n",
      "t [[-0.0035664 ]\n",
      " [-0.34785404]\n",
      " [-0.53000009]\n",
      " ...\n",
      " [-0.38546023]\n",
      " [ 0.0273398 ]\n",
      " [-0.31131019]]\n",
      "t [[-0.0035664 ]\n",
      " [-0.34785404]\n",
      " [-0.53000009]\n",
      " ...\n",
      " [-0.38546023]\n",
      " [ 0.0273398 ]\n",
      " [-0.31131019]]\n",
      "Current iteration=4, loss=36464.777959944724\n",
      "t [[-0.01219166]\n",
      " [-0.43702957]\n",
      " [-0.5899215 ]\n",
      " ...\n",
      " [-0.42331192]\n",
      " [ 0.01123683]\n",
      " [-0.36339839]]\n",
      "t [[-0.01219166]\n",
      " [-0.43702957]\n",
      " [-0.5899215 ]\n",
      " ...\n",
      " [-0.42331192]\n",
      " [ 0.01123683]\n",
      " [-0.36339839]]\n",
      "t [[-0.02061703]\n",
      " [-0.52123036]\n",
      " [-0.6416371 ]\n",
      " ...\n",
      " [-0.45532629]\n",
      " [-0.00783995]\n",
      " [-0.40949862]]\n",
      "t [[-0.02061703]\n",
      " [-0.52123036]\n",
      " [-0.6416371 ]\n",
      " ...\n",
      " [-0.45532629]\n",
      " [-0.00783995]\n",
      " [-0.40949862]]\n",
      "Current iteration=6, loss=35659.55104115463\n",
      "t [[-0.02852619]\n",
      " [-0.60021009]\n",
      " [-0.6879121 ]\n",
      " ...\n",
      " [-0.48376948]\n",
      " [-0.02877722]\n",
      " [-0.45061838]]\n",
      "t [[-0.02852619]\n",
      " [-0.60021009]\n",
      " [-0.6879121 ]\n",
      " ...\n",
      " [-0.48376948]\n",
      " [-0.02877722]\n",
      " [-0.45061838]]\n",
      "t [[-0.03579693]\n",
      " [-0.67408369]\n",
      " [-0.73029163]\n",
      " ...\n",
      " [-0.50986803]\n",
      " [-0.05082912]\n",
      " [-0.48751527]]\n",
      "t [[-0.03579693]\n",
      " [-0.67408369]\n",
      " [-0.73029163]\n",
      " ...\n",
      " [-0.50986803]\n",
      " [-0.05082912]\n",
      " [-0.48751527]]\n",
      "Current iteration=8, loss=35094.3669825312\n",
      "t [[-0.04240354]\n",
      " [-0.74312711]\n",
      " [-0.76966591]\n",
      " ...\n",
      " [-0.53429471]\n",
      " [-0.07347782]\n",
      " [-0.52078037]]\n",
      "t [[-0.04240354]\n",
      " [-0.74312711]\n",
      " [-0.76966591]\n",
      " ...\n",
      " [-0.53429471]\n",
      " [-0.07347782]\n",
      " [-0.52078037]]\n",
      "t [[-0.04836672]\n",
      " [-0.80767691]\n",
      " [-0.80656761]\n",
      " ...\n",
      " [-0.5574241 ]\n",
      " [-0.09635506]\n",
      " [-0.55088773]]\n",
      "loss=34679.34659843994\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00980809]\n",
      " [-0.06205208]\n",
      " [-0.22405797]\n",
      " ...\n",
      " [-0.16725051]\n",
      " [ 0.03555416]\n",
      " [-0.09465206]]\n",
      "t [[ 0.00980809]\n",
      " [-0.06205208]\n",
      " [-0.22405797]\n",
      " ...\n",
      " [-0.16725051]\n",
      " [ 0.03555416]\n",
      " [-0.09465206]]\n",
      "t [[ 0.00742312]\n",
      " [-0.14385889]\n",
      " [-0.36545748]\n",
      " ...\n",
      " [-0.26648165]\n",
      " [ 0.04716609]\n",
      " [-0.17096784]]\n",
      "t [[ 0.00742312]\n",
      " [-0.14385889]\n",
      " [-0.36545748]\n",
      " ...\n",
      " [-0.26648165]\n",
      " [ 0.04716609]\n",
      " [-0.17096784]]\n",
      "Current iteration=2, loss=37741.95912784268\n",
      "t [[-7.70804558e-05]\n",
      " [-2.31384130e-01]\n",
      " [-4.64074052e-01]\n",
      " ...\n",
      " [-3.31255296e-01]\n",
      " [ 4.54212077e-02]\n",
      " [-2.34854361e-01]]\n",
      "t [[-7.70804558e-05]\n",
      " [-2.31384130e-01]\n",
      " [-4.64074052e-01]\n",
      " ...\n",
      " [-3.31255296e-01]\n",
      " [ 4.54212077e-02]\n",
      " [-2.34854361e-01]]\n",
      "t [[-0.00941791]\n",
      " [-0.31830118]\n",
      " [-0.53937254]\n",
      " ...\n",
      " [-0.37794033]\n",
      " [ 0.0357511 ]\n",
      " [-0.28966985]]\n",
      "t [[-0.00941791]\n",
      " [-0.31830118]\n",
      " [-0.53937254]\n",
      " ...\n",
      " [-0.37794033]\n",
      " [ 0.0357511 ]\n",
      " [-0.28966985]]\n",
      "Current iteration=4, loss=36516.21243243765\n",
      "t [[-0.01908718]\n",
      " [-0.40186091]\n",
      " [-0.60111503]\n",
      " ...\n",
      " [-0.41469726]\n",
      " [ 0.02112154]\n",
      " [-0.33746419]]\n",
      "t [[-0.01908718]\n",
      " [-0.40186091]\n",
      " [-0.60111503]\n",
      " ...\n",
      " [-0.41469726]\n",
      " [ 0.02112154]\n",
      " [-0.33746419]]\n",
      "t [[-0.02838747]\n",
      " [-0.48096246]\n",
      " [-0.65443347]\n",
      " ...\n",
      " [-0.44575653]\n",
      " [ 0.00330399]\n",
      " [-0.37960199]]\n",
      "t [[-0.02838747]\n",
      " [-0.48096246]\n",
      " [-0.65443347]\n",
      " ...\n",
      " [-0.44575653]\n",
      " [ 0.00330399]\n",
      " [-0.37960199]]\n",
      "Current iteration=6, loss=35726.49646112175\n",
      "t [[-0.03701655]\n",
      " [-0.55528928]\n",
      " [-0.70212614]\n",
      " ...\n",
      " [-0.47336784]\n",
      " [-0.01656502]\n",
      " [-0.41705512]]\n",
      "t [[-0.03701655]\n",
      " [-0.55528928]\n",
      " [-0.70212614]\n",
      " ...\n",
      " [-0.47336784]\n",
      " [-0.01656502]\n",
      " [-0.41705512]]\n",
      "t [[-0.04486836]\n",
      " [-0.62489898]\n",
      " [-0.74576644]\n",
      " ...\n",
      " [-0.49874574]\n",
      " [-0.03771707]\n",
      " [-0.45055129]]\n",
      "t [[-0.04486836]\n",
      " [-0.62489898]\n",
      " [-0.74576644]\n",
      " ...\n",
      " [-0.49874574]\n",
      " [-0.03771707]\n",
      " [-0.45055129]]\n",
      "Current iteration=8, loss=35172.537379348825\n",
      "t [[-0.05193353]\n",
      " [-0.69002008]\n",
      " [-0.78626788]\n",
      " ...\n",
      " [-0.52255256]\n",
      " [-0.05961293]\n",
      " [-0.48065594]]\n",
      "t [[-0.05193353]\n",
      " [-0.69002008]\n",
      " [-0.78626788]\n",
      " ...\n",
      " [-0.52255256]\n",
      " [-0.05961293]\n",
      " [-0.48065594]]\n",
      "t [[-0.05824832]\n",
      " [-0.75094955]\n",
      " [-0.82418211]\n",
      " ...\n",
      " [-0.54515304]\n",
      " [-0.08186449]\n",
      " [-0.50782084]]\n",
      "loss=34765.65637424866\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00816817]\n",
      " [-0.06572386]\n",
      " [-0.22257957]\n",
      " ...\n",
      " [-0.09799606]\n",
      " [-0.0118024 ]\n",
      " [-0.19992783]]\n",
      "t [[ 0.00816817]\n",
      " [-0.06572386]\n",
      " [-0.22257957]\n",
      " ...\n",
      " [-0.09799606]\n",
      " [-0.0118024 ]\n",
      " [-0.19992783]]\n",
      "t [[ 0.00438476]\n",
      " [-0.15105357]\n",
      " [-0.36182609]\n",
      " ...\n",
      " [-0.14258634]\n",
      " [-0.00593099]\n",
      " [-0.36865875]]\n",
      "t [[ 0.00438476]\n",
      " [-0.15105357]\n",
      " [-0.36182609]\n",
      " ...\n",
      " [-0.14258634]\n",
      " [-0.00593099]\n",
      " [-0.36865875]]\n",
      "Current iteration=2, loss=37698.00845366477\n",
      "t [[-0.00430379]\n",
      " [-0.24187642]\n",
      " [-0.45813091]\n",
      " ...\n",
      " [-0.16208958]\n",
      " [ 0.00620502]\n",
      " [-0.51502242]]\n",
      "t [[-0.00430379]\n",
      " [-0.24187642]\n",
      " [-0.45813091]\n",
      " ...\n",
      " [-0.16208958]\n",
      " [ 0.00620502]\n",
      " [-0.51502242]]\n",
      "t [[-0.0146606 ]\n",
      " [-0.33186084]\n",
      " [-0.53115787]\n",
      " ...\n",
      " [-0.16995692]\n",
      " [ 0.01952696]\n",
      " [-0.6444343 ]]\n",
      "t [[-0.0146606 ]\n",
      " [-0.33186084]\n",
      " [-0.53115787]\n",
      " ...\n",
      " [-0.16995692]\n",
      " [ 0.01952696]\n",
      " [-0.6444343 ]]\n",
      "Current iteration=4, loss=36450.872697409344\n",
      "t [[-0.025207  ]\n",
      " [-0.4182697 ]\n",
      " [-0.59074484]\n",
      " ...\n",
      " [-0.17264302]\n",
      " [ 0.0318785 ]\n",
      " [-0.76040335]]\n",
      "t [[-0.025207  ]\n",
      " [-0.4182697 ]\n",
      " [-0.59074484]\n",
      " ...\n",
      " [-0.17264302]\n",
      " [ 0.0318785 ]\n",
      " [-0.76040335]]\n",
      "t [[-0.03527265]\n",
      " [-0.50001716]\n",
      " [-0.64205118]\n",
      " ...\n",
      " [-0.17330481]\n",
      " [ 0.04244291]\n",
      " [-0.86536323]]\n",
      "t [[-0.03527265]\n",
      " [-0.50001716]\n",
      " [-0.64205118]\n",
      " ...\n",
      " [-0.17330481]\n",
      " [ 0.04244291]\n",
      " [-0.86536323]]\n",
      "Current iteration=6, loss=35646.059229220315\n",
      "t [[-0.04457657]\n",
      " [-0.5768021 ]\n",
      " [-0.68788129]\n",
      " ...\n",
      " [-0.1734714 ]\n",
      " [ 0.05102828]\n",
      " [-0.96109519]]\n",
      "t [[-0.04457657]\n",
      " [-0.5768021 ]\n",
      " [-0.68788129]\n",
      " ...\n",
      " [-0.1734714 ]\n",
      " [ 0.05102828]\n",
      " [-0.96109519]]\n",
      "t [[-0.05302914]\n",
      " [-0.64869728]\n",
      " [-0.72980466]\n",
      " ...\n",
      " [-0.17384673]\n",
      " [ 0.05772684]\n",
      " [-1.04895697]]\n",
      "t [[-0.05302914]\n",
      " [-0.64869728]\n",
      " [-0.72980466]\n",
      " ...\n",
      " [-0.17384673]\n",
      " [ 0.05772684]\n",
      " [-1.04895697]]\n",
      "Current iteration=8, loss=35080.98643462374\n",
      "t [[-0.06063373]\n",
      " [-0.71594574]\n",
      " [-0.76872624]\n",
      " ...\n",
      " [-0.17471466]\n",
      " [ 0.06274744]\n",
      " [-1.13001582]]\n",
      "t [[-0.06063373]\n",
      " [-0.71594574]\n",
      " [-0.76872624]\n",
      " ...\n",
      " [-0.17471466]\n",
      " [ 0.06274744]\n",
      " [-1.13001582]]\n",
      "t [[-0.06743647]\n",
      " [-0.77885808]\n",
      " [-0.80518725]\n",
      " ...\n",
      " [-0.17614894]\n",
      " [ 0.06633327]\n",
      " [-1.20513077]]\n",
      "loss=34665.85466809606\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.08980002]\n",
      " [-0.38361538]\n",
      " [-0.24111607]\n",
      " ...\n",
      " [-0.17474938]\n",
      " [ 0.0378342 ]\n",
      " [-0.1018697 ]]\n",
      "t [[ 0.08980002]\n",
      " [-0.38361538]\n",
      " [-0.24111607]\n",
      " ...\n",
      " [-0.17474938]\n",
      " [ 0.0378342 ]\n",
      " [-0.1018697 ]]\n",
      "t [[ 0.14158391]\n",
      " [-0.63674327]\n",
      " [-0.39647669]\n",
      " ...\n",
      " [-0.27775621]\n",
      " [ 0.04987075]\n",
      " [-0.18401219]]\n",
      "t [[ 0.14158391]\n",
      " [-0.63674327]\n",
      " [-0.39647669]\n",
      " ...\n",
      " [-0.27775621]\n",
      " [ 0.04987075]\n",
      " [-0.18401219]]\n",
      "Current iteration=2, loss=37687.98940665784\n",
      "t [[ 0.17269231]\n",
      " [-0.81870066]\n",
      " [-0.50722231]\n",
      " ...\n",
      " [-0.34517924]\n",
      " [ 0.04769655]\n",
      " [-0.25283831]]\n",
      "t [[ 0.17269231]\n",
      " [-0.81870066]\n",
      " [-0.50722231]\n",
      " ...\n",
      " [-0.34517924]\n",
      " [ 0.04769655]\n",
      " [-0.25283831]]\n",
      "t [[ 0.19197288]\n",
      " [-0.95869886]\n",
      " [-0.59309866]\n",
      " ...\n",
      " [-0.39420417]\n",
      " [ 0.0371653 ]\n",
      " [-0.31194654]]\n",
      "t [[ 0.19197288]\n",
      " [-0.95869886]\n",
      " [-0.59309866]\n",
      " ...\n",
      " [-0.39420417]\n",
      " [ 0.0371653 ]\n",
      " [-0.31194654]]\n",
      "Current iteration=4, loss=36453.44687292146\n",
      "t [[ 0.20413007]\n",
      " [-1.0718672 ]\n",
      " [-0.66395334]\n",
      " ...\n",
      " [-0.43321997]\n",
      " [ 0.02145188]\n",
      " [-0.36352198]]\n",
      "t [[ 0.20413007]\n",
      " [-1.0718672 ]\n",
      " [-0.66395334]\n",
      " ...\n",
      " [-0.43321997]\n",
      " [ 0.02145188]\n",
      " [-0.36352198]]\n",
      "t [[ 0.21181672]\n",
      " [-1.16663918]\n",
      " [-0.72499421]\n",
      " ...\n",
      " [-0.46649528]\n",
      " [ 0.00244854]\n",
      " [-0.40901802]]\n",
      "t [[ 0.21181672]\n",
      " [-1.16663918]\n",
      " [-0.72499421]\n",
      " ...\n",
      " [-0.46649528]\n",
      " [ 0.00244854]\n",
      " [-0.40901802]]\n",
      "Current iteration=6, loss=35662.861463350055\n",
      "t [[ 0.21660233]\n",
      " [-1.24802624]\n",
      " [-0.77911355]\n",
      " ...\n",
      " [-0.49626085]\n",
      " [-0.01863282]\n",
      " [-0.44947118]]\n",
      "t [[ 0.21660233]\n",
      " [-1.24802624]\n",
      " [-0.77911355]\n",
      " ...\n",
      " [-0.49626085]\n",
      " [-0.01863282]\n",
      " [-0.44947118]]\n",
      "t [[ 0.2194536 ]\n",
      " [-1.31918204]\n",
      " [-0.82799277]\n",
      " ...\n",
      " [-0.52370198]\n",
      " [-0.04097565]\n",
      " [-0.48565917]]\n",
      "t [[ 0.2194536 ]\n",
      " [-1.31918204]\n",
      " [-0.82799277]\n",
      " ...\n",
      " [-0.52370198]\n",
      " [-0.04097565]\n",
      " [-0.48565917]]\n",
      "Current iteration=8, loss=35111.27093489687\n",
      "t [[ 0.22098754]\n",
      " [-1.38220079]\n",
      " [-0.87265887]\n",
      " ...\n",
      " [-0.54945694]\n",
      " [-0.06401026]\n",
      " [-0.5181877 ]]\n",
      "t [[ 0.22098754]\n",
      " [-1.38220079]\n",
      " [-0.87265887]\n",
      " ...\n",
      " [-0.54945694]\n",
      " [-0.06401026]\n",
      " [-0.5181877 ]]\n",
      "t [[ 0.22161096]\n",
      " [-1.43854516]\n",
      " [-0.91377558]\n",
      " ...\n",
      " [-0.57387506]\n",
      " [-0.08733007]\n",
      " [-0.54754174]]\n",
      "loss=34707.85547826743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01184225]\n",
      " [-0.07291117]\n",
      " [-0.22858414]\n",
      " ...\n",
      " [-0.17541051]\n",
      " [ 0.034101  ]\n",
      " [-0.1040613 ]]\n",
      "t [[ 0.01184225]\n",
      " [-0.07291117]\n",
      " [-0.22858414]\n",
      " ...\n",
      " [-0.17541051]\n",
      " [ 0.034101  ]\n",
      " [-0.1040613 ]]\n",
      "t [[ 0.01060382]\n",
      " [-0.16586125]\n",
      " [-0.36916271]\n",
      " ...\n",
      " [-0.27754439]\n",
      " [ 0.04302065]\n",
      " [-0.18795098]]\n",
      "t [[ 0.01060382]\n",
      " [-0.16586125]\n",
      " [-0.36916271]\n",
      " ...\n",
      " [-0.27754439]\n",
      " [ 0.04302065]\n",
      " [-0.18795098]]\n",
      "Current iteration=2, loss=37656.69124930094\n",
      "t [[ 0.00398575]\n",
      " [-0.26356324]\n",
      " [-0.46578169]\n",
      " ...\n",
      " [-0.34370878]\n",
      " [ 0.03837241]\n",
      " [-0.25827853]]\n",
      "t [[ 0.00398575]\n",
      " [-0.26356324]\n",
      " [-0.46578169]\n",
      " ...\n",
      " [-0.34370878]\n",
      " [ 0.03837241]\n",
      " [-0.25827853]]\n",
      "t [[-0.00457391]\n",
      " [-0.35947743]\n",
      " [-0.53912657]\n",
      " ...\n",
      " [-0.39143393]\n",
      " [ 0.02591262]\n",
      " [-0.31872111]]\n",
      "t [[-0.00457391]\n",
      " [-0.35947743]\n",
      " [-0.53912657]\n",
      " ...\n",
      " [-0.39143393]\n",
      " [ 0.02591262]\n",
      " [-0.31872111]]\n",
      "Current iteration=4, loss=36401.044961911786\n",
      "t [[-0.01352989]\n",
      " [-0.45093316]\n",
      " [-0.59925146]\n",
      " ...\n",
      " [-0.42920426]\n",
      " [ 0.00871123]\n",
      " [-0.37150171]]\n",
      "t [[-0.01352989]\n",
      " [-0.45093316]\n",
      " [-0.59925146]\n",
      " ...\n",
      " [-0.42920426]\n",
      " [ 0.00871123]\n",
      " [-0.37150171]]\n",
      "t [[-0.0221846 ]\n",
      " [-0.53697382]\n",
      " [-0.65129817]\n",
      " ...\n",
      " [-0.46130773]\n",
      " [-0.01142939]\n",
      " [-0.41809637]]\n",
      "t [[-0.0221846 ]\n",
      " [-0.53697382]\n",
      " [-0.65129817]\n",
      " ...\n",
      " [-0.46130773]\n",
      " [-0.01142939]\n",
      " [-0.41809637]]\n",
      "Current iteration=6, loss=35595.113521375075\n",
      "t [[-0.03024106]\n",
      " [-0.61742704]\n",
      " [-0.69799776]\n",
      " ...\n",
      " [-0.48997199]\n",
      " [-0.03337036]\n",
      " [-0.45955726]]\n",
      "t [[-0.03024106]\n",
      " [-0.61742704]\n",
      " [-0.69799776]\n",
      " ...\n",
      " [-0.48997199]\n",
      " [-0.03337036]\n",
      " [-0.45955726]]\n",
      "t [[-0.03759626]\n",
      " [-0.69247601]\n",
      " [-0.74084774]\n",
      " ...\n",
      " [-0.51637459]\n",
      " [-0.05635351]\n",
      " [-0.49667401]]\n",
      "t [[-0.03759626]\n",
      " [-0.69247601]\n",
      " [-0.74084774]\n",
      " ...\n",
      " [-0.51637459]\n",
      " [-0.05635351]\n",
      " [-0.49667401]]\n",
      "Current iteration=8, loss=35032.95232938189\n",
      "t [[-0.04424032]\n",
      " [-0.76245329]\n",
      " [-0.78069803]\n",
      " ...\n",
      " [-0.54114671]\n",
      " [-0.07985562]\n",
      " [-0.53006187]]\n",
      "t [[-0.04424032]\n",
      " [-0.76245329]\n",
      " [-0.78069803]\n",
      " ...\n",
      " [-0.54114671]\n",
      " [-0.07985562]\n",
      " [-0.53006187]]\n",
      "t [[-0.05020613]\n",
      " [-0.82774057]\n",
      " [-0.8180527 ]\n",
      " ...\n",
      " [-0.56463261]\n",
      " [-0.103507  ]\n",
      " [-0.56021369]]\n",
      "loss=34622.257160478344\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0101146 ]\n",
      " [-0.06399121]\n",
      " [-0.23105978]\n",
      " ...\n",
      " [-0.17247709]\n",
      " [ 0.03666523]\n",
      " [-0.09760994]]\n",
      "t [[ 0.0101146 ]\n",
      " [-0.06399121]\n",
      " [-0.23105978]\n",
      " ...\n",
      " [-0.17247709]\n",
      " [ 0.03666523]\n",
      " [-0.09760994]]\n",
      "t [[ 0.00727504]\n",
      " [-0.1489725 ]\n",
      " [-0.37427602]\n",
      " ...\n",
      " [-0.2726676 ]\n",
      " [ 0.04788978]\n",
      " [-0.17573053]]\n",
      "t [[ 0.00727504]\n",
      " [-0.1489725 ]\n",
      " [-0.37427602]\n",
      " ...\n",
      " [-0.2726676 ]\n",
      " [ 0.04788978]\n",
      " [-0.17573053]]\n",
      "Current iteration=2, loss=37687.877358853\n",
      "t [[-0.00077509]\n",
      " [-0.23958195]\n",
      " [-0.47329897]\n",
      " ...\n",
      " [-0.33731207]\n",
      " [ 0.04525602]\n",
      " [-0.24082985]]\n",
      "t [[-0.00077509]\n",
      " [-0.23958195]\n",
      " [-0.47329897]\n",
      " ...\n",
      " [-0.33731207]\n",
      " [ 0.04525602]\n",
      " [-0.24082985]]\n",
      "t [[-0.01057808]\n",
      " [-0.32914965]\n",
      " [-0.54875672]\n",
      " ...\n",
      " [-0.3837566 ]\n",
      " [ 0.03453588]\n",
      " [-0.29649822]]\n",
      "t [[-0.01057808]\n",
      " [-0.32914965]\n",
      " [-0.54875672]\n",
      " ...\n",
      " [-0.3837566 ]\n",
      " [ 0.03453588]\n",
      " [-0.29649822]]\n",
      "Current iteration=4, loss=36453.648758222065\n",
      "t [[-0.02058806]\n",
      " [-0.41488982]\n",
      " [-0.61072815]\n",
      " ...\n",
      " [-0.42041961]\n",
      " [ 0.0188238 ]\n",
      " [-0.3448979 ]]\n",
      "t [[-0.02058806]\n",
      " [-0.41488982]\n",
      " [-0.61072815]\n",
      " ...\n",
      " [-0.42041961]\n",
      " [ 0.0188238 ]\n",
      " [-0.3448979 ]]\n",
      "t [[-3.01181385e-02]\n",
      " [-4.95752918e-01]\n",
      " [-6.64393658e-01]\n",
      " ...\n",
      " [-4.51560389e-01]\n",
      " [-5.21645852e-05]\n",
      " [-3.87458303e-01]]\n",
      "t [[-3.01181385e-02]\n",
      " [-4.95752918e-01]\n",
      " [-6.64393658e-01]\n",
      " ...\n",
      " [-4.51560389e-01]\n",
      " [-5.21645852e-05]\n",
      " [-3.87458303e-01]]\n",
      "Current iteration=6, loss=35663.3321424219\n",
      "t [[-0.03888746]\n",
      " [-0.57149189]\n",
      " [-0.71252033]\n",
      " ...\n",
      " [-0.47939026]\n",
      " [-0.02092773]\n",
      " [-0.42519374]]\n",
      "t [[-0.03888746]\n",
      " [-0.57149189]\n",
      " [-0.71252033]\n",
      " ...\n",
      " [-0.47939026]\n",
      " [-0.02092773]\n",
      " [-0.42519374]]\n",
      "t [[-0.04681103]\n",
      " [-0.64222919]\n",
      " [-0.75663593]\n",
      " ...\n",
      " [-0.50507435]\n",
      " [-0.04302013]\n",
      " [-0.45886204]]\n",
      "t [[-0.04681103]\n",
      " [-0.64222919]\n",
      " [-0.75663593]\n",
      " ...\n",
      " [-0.50507435]\n",
      " [-0.04302013]\n",
      " [-0.45886204]]\n",
      "Current iteration=8, loss=35112.34637265063\n",
      "t [[-0.05389691]\n",
      " [-0.70824741]\n",
      " [-0.79761494]\n",
      " ...\n",
      " [-0.52923275]\n",
      " [-0.06578292]\n",
      " [-0.48905118]]\n",
      "t [[-0.05389691]\n",
      " [-0.70824741]\n",
      " [-0.79761494]\n",
      " ...\n",
      " [-0.52923275]\n",
      " [-0.06578292]\n",
      " [-0.48905118]]\n",
      "t [[-0.06019486]\n",
      " [-0.7698866 ]\n",
      " [-0.83598125]\n",
      " ...\n",
      " [-0.55219903]\n",
      " [-0.08882501]\n",
      " [-0.51623037]]\n",
      "loss=34709.67635745494\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00842342]\n",
      " [-0.06777773]\n",
      " [-0.22953518]\n",
      " ...\n",
      " [-0.10105844]\n",
      " [-0.01217122]\n",
      " [-0.20617558]]\n",
      "t [[ 0.00842342]\n",
      " [-0.06777773]\n",
      " [-0.22953518]\n",
      " ...\n",
      " [-0.10105844]\n",
      " [-0.01217122]\n",
      " [-0.20617558]]\n",
      "t [[ 0.00414936]\n",
      " [-0.15638644]\n",
      " [-0.37051097]\n",
      " ...\n",
      " [-0.14536179]\n",
      " [-0.0055592 ]\n",
      " [-0.37919017]]\n",
      "t [[ 0.00414936]\n",
      " [-0.15638644]\n",
      " [-0.37051097]\n",
      " ...\n",
      " [-0.14536179]\n",
      " [-0.0055592 ]\n",
      " [-0.37919017]]\n",
      "Current iteration=2, loss=37643.029630246456\n",
      "t [[-0.00511294]\n",
      " [-0.25038202]\n",
      " [-0.4671407 ]\n",
      " ...\n",
      " [-0.16390858]\n",
      " [ 0.00734629]\n",
      " [-0.52871914]]\n",
      "t [[-0.00511294]\n",
      " [-0.25038202]\n",
      " [-0.4671407 ]\n",
      " ...\n",
      " [-0.16390858]\n",
      " [ 0.00734629]\n",
      " [-0.52871914]]\n",
      "t [[-0.01594726]\n",
      " [-0.34309129]\n",
      " [-0.54025979]\n",
      " ...\n",
      " [-0.17093026]\n",
      " [ 0.02119365]\n",
      " [-0.66057391]]\n",
      "t [[-0.01594726]\n",
      " [-0.34309129]\n",
      " [-0.54025979]\n",
      " ...\n",
      " [-0.17093026]\n",
      " [ 0.02119365]\n",
      " [-0.66057391]]\n",
      "Current iteration=4, loss=36387.166175791564\n",
      "t [[-0.02684417]\n",
      " [-0.43174229]\n",
      " [-0.60002287]\n",
      " ...\n",
      " [-0.17305458]\n",
      " [ 0.03380583]\n",
      " [-0.77847387]]\n",
      "t [[-0.02684417]\n",
      " [-0.43174229]\n",
      " [-0.60002287]\n",
      " ...\n",
      " [-0.17305458]\n",
      " [ 0.03380583]\n",
      " [-0.77847387]]\n",
      "t [[-0.03714583]\n",
      " [-0.51530214]\n",
      " [-0.65163597]\n",
      " ...\n",
      " [-0.17342554]\n",
      " [ 0.04441598]\n",
      " [-0.88498065]]\n",
      "t [[-0.03714583]\n",
      " [-0.51530214]\n",
      " [-0.65163597]\n",
      " ...\n",
      " [-0.17342554]\n",
      " [ 0.04441598]\n",
      " [-0.88498065]]\n",
      "Current iteration=6, loss=35581.65146415126\n",
      "t [[-0.04659388]\n",
      " [-0.59354067]\n",
      " [-0.69787021]\n",
      " ...\n",
      " [-0.17351028]\n",
      " [ 0.05289363]\n",
      " [-0.98196114]]\n",
      "t [[-0.04659388]\n",
      " [-0.59354067]\n",
      " [-0.69787021]\n",
      " ...\n",
      " [-0.17351028]\n",
      " [ 0.05289363]\n",
      " [-0.98196114]]\n",
      "t [[-0.05512057]\n",
      " [-0.66659687]\n",
      " [-0.74024746]\n",
      " ...\n",
      " [-0.17394825]\n",
      " [ 0.05938489]\n",
      " [-1.07083422]]\n",
      "t [[-0.05512057]\n",
      " [-0.66659687]\n",
      " [-0.74024746]\n",
      " ...\n",
      " [-0.17394825]\n",
      " [ 0.05938489]\n",
      " [-1.07083422]]\n",
      "Current iteration=8, loss=35019.57408720187\n",
      "t [[-0.06274721]\n",
      " [-0.73476912]\n",
      " [-0.7796319 ]\n",
      " ...\n",
      " [-0.17497034]\n",
      " [ 0.0641404 ]\n",
      " [-1.15271271]]\n",
      "t [[-0.06274721]\n",
      " [-0.73476912]\n",
      " [-0.7796319 ]\n",
      " ...\n",
      " [-0.17497034]\n",
      " [ 0.0641404 ]\n",
      " [-1.15271271]]\n",
      "t [[-0.06953372]\n",
      " [-0.79841225]\n",
      " [-0.8165356 ]\n",
      " ...\n",
      " [-0.17661103]\n",
      " [ 0.06743369]\n",
      " [-1.22849065]]\n",
      "loss=34608.74183459111\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.09252123]\n",
      " [-0.39524009]\n",
      " [-0.24842262]\n",
      " ...\n",
      " [-0.18004482]\n",
      " [ 0.03898069]\n",
      " [-0.10495666]]\n",
      "t [[ 0.09252123]\n",
      " [-0.39524009]\n",
      " [-0.24842262]\n",
      " ...\n",
      " [-0.18004482]\n",
      " [ 0.03898069]\n",
      " [-0.10495666]]\n",
      "t [[ 0.14471539]\n",
      " [-0.65206357]\n",
      " [-0.40588025]\n",
      " ...\n",
      " [-0.28398725]\n",
      " [ 0.0506002 ]\n",
      " [-0.18898391]]\n",
      "t [[ 0.14471539]\n",
      " [-0.65206357]\n",
      " [-0.40588025]\n",
      " ...\n",
      " [-0.28398725]\n",
      " [ 0.0506002 ]\n",
      " [-0.18898391]]\n",
      "Current iteration=2, loss=37634.932116191056\n",
      "t [[ 0.17551564]\n",
      " [-0.83521805]\n",
      " [-0.51727772]\n",
      " ...\n",
      " [-0.35129844]\n",
      " [ 0.04750025]\n",
      " [-0.25908218]]\n",
      "t [[ 0.17551564]\n",
      " [-0.83521805]\n",
      " [-0.51727772]\n",
      " ...\n",
      " [-0.35129844]\n",
      " [ 0.04750025]\n",
      " [-0.25908218]]\n",
      "t [[ 0.194306  ]\n",
      " [-0.9756248 ]\n",
      " [-0.60348551]\n",
      " ...\n",
      " [-0.40013236]\n",
      " [ 0.03588531]\n",
      " [-0.31908871]]\n",
      "t [[ 0.194306  ]\n",
      " [-0.9756248 ]\n",
      " [-0.60348551]\n",
      " ...\n",
      " [-0.40013236]\n",
      " [ 0.03588531]\n",
      " [-0.31908871]]\n",
      "Current iteration=4, loss=36392.47561872305\n",
      "t [[ 0.20596997]\n",
      " [-1.08895091]\n",
      " [-0.67465702]\n",
      " ...\n",
      " [-0.4391148 ]\n",
      " [ 0.01906215]\n",
      " [-0.37130364]]\n",
      "t [[ 0.20596997]\n",
      " [-1.08895091]\n",
      " [-0.67465702]\n",
      " ...\n",
      " [-0.4391148 ]\n",
      " [ 0.01906215]\n",
      " [-0.37130364]]\n",
      "t [[ 2.13215544e-01]\n",
      " [-1.18379263e+00]\n",
      " [-7.36054013e-01]\n",
      " ...\n",
      " [-4.72528772e-01]\n",
      " [-1.01891480e-03]\n",
      " [-4.17247153e-01]]\n",
      "t [[ 2.13215544e-01]\n",
      " [-1.18379263e+00]\n",
      " [-7.36054013e-01]\n",
      " ...\n",
      " [-4.72528772e-01]\n",
      " [-1.01891480e-03]\n",
      " [-4.17247153e-01]]\n",
      "Current iteration=6, loss=35601.661753407745\n",
      "t [[ 0.21762305]\n",
      " [-1.26520117]\n",
      " [-0.79054965]\n",
      " ...\n",
      " [-0.50255917]\n",
      " [-0.02311776]\n",
      " [-0.45799976]]\n",
      "t [[ 0.21762305]\n",
      " [-1.26520117]\n",
      " [-0.79054965]\n",
      " ...\n",
      " [-0.50255917]\n",
      " [-0.02311776]\n",
      " [-0.45799976]]\n",
      "t [[ 0.22015514]\n",
      " [-1.33633354]\n",
      " [-0.83979322]\n",
      " ...\n",
      " [-0.53033925]\n",
      " [-0.04640371]\n",
      " [-0.49437074]]\n",
      "t [[ 0.22015514]\n",
      " [-1.33633354]\n",
      " [-0.83979322]\n",
      " ...\n",
      " [-0.53033925]\n",
      " [-0.04640371]\n",
      " [-0.49437074]]\n",
      "Current iteration=8, loss=35053.235223119205\n",
      "t [[ 0.22142096]\n",
      " [-1.39928048]\n",
      " [-0.88478529]\n",
      " ...\n",
      " [-0.55646508]\n",
      " [-0.07030061]\n",
      " [-0.52698953]]\n",
      "t [[ 0.22142096]\n",
      " [-1.39928048]\n",
      " [-0.88478529]\n",
      " ...\n",
      " [-0.55646508]\n",
      " [-0.07030061]\n",
      " [-0.52698953]]\n",
      "t [[ 0.2218195 ]\n",
      " [-1.45550308]\n",
      " [-0.92617263]\n",
      " ...\n",
      " [-0.58125633]\n",
      " [-0.09439992]\n",
      " [-0.55635955]]\n",
      "loss=34654.08186752643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01220111]\n",
      " [-0.0751206 ]\n",
      " [-0.23551093]\n",
      " ...\n",
      " [-0.18072598]\n",
      " [ 0.03513436]\n",
      " [-0.10721467]]\n",
      "t [[ 0.01220111]\n",
      " [-0.0751206 ]\n",
      " [-0.23551093]\n",
      " ...\n",
      " [-0.18072598]\n",
      " [ 0.03513436]\n",
      " [-0.10721467]]\n",
      "t [[ 0.01053107]\n",
      " [-0.17149223]\n",
      " [-0.37767058]\n",
      " ...\n",
      " [-0.28372328]\n",
      " [ 0.04356197]\n",
      " [-0.19302867]]\n",
      "t [[ 0.01053107]\n",
      " [-0.17149223]\n",
      " [-0.37767058]\n",
      " ...\n",
      " [-0.28372328]\n",
      " [ 0.04356197]\n",
      " [-0.19302867]]\n",
      "Current iteration=2, loss=37602.90057579577\n",
      "t [[ 0.00339078]\n",
      " [-0.27243381]\n",
      " [-0.47455427]\n",
      " ...\n",
      " [-0.34971446]\n",
      " [ 0.03795227]\n",
      " [-0.26465901]]\n",
      "t [[ 0.00339078]\n",
      " [-0.27243381]\n",
      " [-0.47455427]\n",
      " ...\n",
      " [-0.34971446]\n",
      " [ 0.03795227]\n",
      " [-0.26465901]]\n",
      "t [[-0.00560297]\n",
      " [-0.37108413]\n",
      " [-0.54799794]\n",
      " ...\n",
      " [-0.39720526]\n",
      " [ 0.02440024]\n",
      " [-0.32602515]]\n",
      "t [[-0.00560297]\n",
      " [-0.37108413]\n",
      " [-0.54799794]\n",
      " ...\n",
      " [-0.39720526]\n",
      " [ 0.02440024]\n",
      " [-0.32602515]]\n",
      "Current iteration=4, loss=36338.927846178165\n",
      "t [[-0.01487656]\n",
      " [-0.46476055]\n",
      " [-0.60833601]\n",
      " ...\n",
      " [-0.43491108]\n",
      " [ 0.00609779]\n",
      " [-0.3794662 ]]\n",
      "t [[-0.01487656]\n",
      " [-0.46476055]\n",
      " [-0.60833601]\n",
      " ...\n",
      " [-0.43491108]\n",
      " [ 0.00609779]\n",
      " [-0.3794662 ]]\n",
      "t [[-0.0237455 ]\n",
      " [-0.5525752 ]\n",
      " [-0.66073276]\n",
      " ...\n",
      " [-0.46712903]\n",
      " [-0.01510149]\n",
      " [-0.42652559]]\n",
      "t [[-0.0237455 ]\n",
      " [-0.5525752 ]\n",
      " [-0.66073276]\n",
      " ...\n",
      " [-0.46712903]\n",
      " [-0.01510149]\n",
      " [-0.42652559]]\n",
      "Current iteration=6, loss=35532.71475373376\n",
      "t [[-0.03193461]\n",
      " [-0.6344362 ]\n",
      " [-0.7078734 ]\n",
      " ...\n",
      " [-0.4960378 ]\n",
      " [-0.03803519]\n",
      " [-0.46830007]]\n",
      "t [[-0.03193461]\n",
      " [-0.6344362 ]\n",
      " [-0.7078734 ]\n",
      " ...\n",
      " [-0.4960378 ]\n",
      " [-0.03803519]\n",
      " [-0.46830007]]\n",
      "t [[-0.03936116]\n",
      " [-0.71059816]\n",
      " [-0.75120294]\n",
      " ...\n",
      " [-0.52276137]\n",
      " [-0.06193417]\n",
      " [-0.50561129]]\n",
      "t [[-0.03936116]\n",
      " [-0.71059816]\n",
      " [-0.75120294]\n",
      " ...\n",
      " [-0.52276137]\n",
      " [-0.06193417]\n",
      " [-0.50561129]]\n",
      "Current iteration=8, loss=34973.82334629886\n",
      "t [[-0.04603151]\n",
      " [-0.78145161]\n",
      " [-0.79152992]\n",
      " ...\n",
      " [-0.54788829]\n",
      " [-0.08627079]\n",
      " [-0.53909866]]\n",
      "t [[-0.04603151]\n",
      " [-0.78145161]\n",
      " [-0.79152992]\n",
      " ...\n",
      " [-0.54788829]\n",
      " [-0.08627079]\n",
      " [-0.53909866]]\n",
      "t [[-0.05199065]\n",
      " [-0.84742362]\n",
      " [-0.82933067]\n",
      " ...\n",
      " [-0.57173307]\n",
      " [-0.11067469]\n",
      " [-0.56927387]]\n",
      "loss=34567.54147414446\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0104211 ]\n",
      " [-0.06593034]\n",
      " [-0.23806159]\n",
      " ...\n",
      " [-0.17770367]\n",
      " [ 0.0377763 ]\n",
      " [-0.10056782]]\n",
      "t [[ 0.0104211 ]\n",
      " [-0.06593034]\n",
      " [-0.23806159]\n",
      " ...\n",
      " [-0.17770367]\n",
      " [ 0.0377763 ]\n",
      " [-0.10056782]]\n",
      "t [[ 0.00710507]\n",
      " [-0.15412189]\n",
      " [-0.38294247]\n",
      " ...\n",
      " [-0.27872833]\n",
      " [ 0.04856992]\n",
      " [-0.18045904]]\n",
      "t [[ 0.00710507]\n",
      " [-0.15412189]\n",
      " [-0.38294247]\n",
      " ...\n",
      " [-0.27872833]\n",
      " [ 0.04856992]\n",
      " [-0.18045904]]\n",
      "Current iteration=2, loss=37634.86083014487\n",
      "t [[-0.00150042]\n",
      " [-0.24780975]\n",
      " [-0.48228849]\n",
      " ...\n",
      " [-0.34317948]\n",
      " [ 0.04501754]\n",
      " [-0.2467358 ]]\n",
      "t [[-0.00150042]\n",
      " [-0.24780975]\n",
      " [-0.48228849]\n",
      " ...\n",
      " [-0.34317948]\n",
      " [ 0.04501754]\n",
      " [-0.2467358 ]]\n",
      "t [[-0.0117578 ]\n",
      " [-0.33998935]\n",
      " [-0.55788279]\n",
      " ...\n",
      " [-0.38937322]\n",
      " [ 0.03323247]\n",
      " [-0.30322452]]\n",
      "t [[-0.0117578 ]\n",
      " [-0.33998935]\n",
      " [-0.55788279]\n",
      " ...\n",
      " [-0.38937322]\n",
      " [ 0.03323247]\n",
      " [-0.30322452]]\n",
      "Current iteration=4, loss=36392.68161335867\n",
      "t [[-0.02209401]\n",
      " [-0.4278537 ]\n",
      " [-0.62009124]\n",
      " ...\n",
      " [-0.42595969]\n",
      " [ 0.01643373]\n",
      " [-0.35219971]]\n",
      "t [[-0.02209401]\n",
      " [-0.4278537 ]\n",
      " [-0.62009124]\n",
      " ...\n",
      " [-0.42595969]\n",
      " [ 0.01643373]\n",
      " [-0.35219971]]\n",
      "t [[-0.03183733]\n",
      " [-0.51041566]\n",
      " [-0.67412132]\n",
      " ...\n",
      " [-0.45720811]\n",
      " [-0.00349704]\n",
      " [-0.39515531]]\n",
      "t [[-0.03183733]\n",
      " [-0.51041566]\n",
      " [-0.67412132]\n",
      " ...\n",
      " [-0.45720811]\n",
      " [-0.00349704]\n",
      " [-0.39515531]]\n",
      "Current iteration=6, loss=35602.170910210785\n",
      "t [[-0.04073086]\n",
      " [-0.58750407]\n",
      " [-0.7226973 ]\n",
      " ...\n",
      " [-0.48528087]\n",
      " [-0.02536968]\n",
      " [-0.43314783]]\n",
      "t [[-0.04073086]\n",
      " [-0.58750407]\n",
      " [-0.7226973 ]\n",
      " ...\n",
      " [-0.48528087]\n",
      " [-0.02536968]\n",
      " [-0.43314783]]\n",
      "t [[-0.0487119 ]\n",
      " [-0.65930943]\n",
      " [-0.76729634]\n",
      " ...\n",
      " [-0.51128907]\n",
      " [-0.0483883 ]\n",
      " [-0.46696516]]\n",
      "t [[-0.0487119 ]\n",
      " [-0.65930943]\n",
      " [-0.76729634]\n",
      " ...\n",
      " [-0.51128907]\n",
      " [-0.0483883 ]\n",
      " [-0.46696516]]\n",
      "Current iteration=8, loss=35054.39453443545\n",
      "t [[-0.05580636]\n",
      " [-0.72616968]\n",
      " [-0.8087528 ]\n",
      " ...\n",
      " [-0.53580933]\n",
      " [-0.07200015]\n",
      " [-0.49721782]]\n",
      "t [[-0.05580636]\n",
      " [-0.72616968]\n",
      " [-0.8087528 ]\n",
      " ...\n",
      " [-0.53580933]\n",
      " [-0.07200015]\n",
      " [-0.49721782]]\n",
      "t [[-0.06207744]\n",
      " [-0.78846825]\n",
      " [-0.84756362]\n",
      " ...\n",
      " [-0.55914465]\n",
      " [-0.0958119 ]\n",
      " [-0.52439244]]\n",
      "loss=34656.01824591435\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00867868]\n",
      " [-0.0698316 ]\n",
      " [-0.23649079]\n",
      " ...\n",
      " [-0.10412082]\n",
      " [-0.01254005]\n",
      " [-0.21242332]]\n",
      "t [[ 0.00867868]\n",
      " [-0.0698316 ]\n",
      " [-0.23649079]\n",
      " ...\n",
      " [-0.10412082]\n",
      " [-0.01254005]\n",
      " [-0.21242332]]\n",
      "t [[ 0.00389252]\n",
      " [-0.16175468]\n",
      " [-0.37904268]\n",
      " ...\n",
      " [-0.1480391 ]\n",
      " [-0.00515477]\n",
      " [-0.38966314]]\n",
      "t [[ 0.00389252]\n",
      " [-0.16175468]\n",
      " [-0.37904268]\n",
      " ...\n",
      " [-0.1480391 ]\n",
      " [-0.00515477]\n",
      " [-0.38966314]]\n",
      "Current iteration=2, loss=37589.133155991505\n",
      "t [[-0.00594821]\n",
      " [-0.25891626]\n",
      " [-0.47591435]\n",
      " ...\n",
      " [-0.16559001]\n",
      " [ 0.00852184]\n",
      " [-0.54229109]]\n",
      "t [[-0.00594821]\n",
      " [-0.25891626]\n",
      " [-0.47591435]\n",
      " ...\n",
      " [-0.16559001]\n",
      " [ 0.00852184]\n",
      " [-0.54229109]]\n",
      "t [[-0.01725152]\n",
      " [-0.35431039]\n",
      " [-0.5491041 ]\n",
      " ...\n",
      " [-0.17177558]\n",
      " [ 0.0228729 ]\n",
      " [-0.67652422]]\n",
      "t [[-0.01725152]\n",
      " [-0.35431039]\n",
      " [-0.5491041 ]\n",
      " ...\n",
      " [-0.17177558]\n",
      " [ 0.0228729 ]\n",
      " [-0.67652422]]\n",
      "Current iteration=4, loss=36325.07856384701\n",
      "t [[-0.0284838 ]\n",
      " [-0.44514583]\n",
      " [-0.60905304]\n",
      " ...\n",
      " [-0.17337192]\n",
      " [ 0.03571414]\n",
      " [-0.79629407]]\n",
      "t [[-0.0284838 ]\n",
      " [-0.44514583]\n",
      " [-0.60905304]\n",
      " ...\n",
      " [-0.17337192]\n",
      " [ 0.03571414]\n",
      " [-0.79629407]]\n",
      "t [[-0.03900437]\n",
      " [-0.53045376]\n",
      " [-0.66099227]\n",
      " ...\n",
      " [-0.17349245]\n",
      " [ 0.04633805]\n",
      " [-0.90429052]]\n",
      "t [[-0.03900437]\n",
      " [-0.53045376]\n",
      " [-0.66099227]\n",
      " ...\n",
      " [-0.17349245]\n",
      " [ 0.04633805]\n",
      " [-0.90429052]]\n",
      "Current iteration=6, loss=35519.28004328702\n",
      "t [[-0.04858009]\n",
      " [-0.61008148]\n",
      " [-0.70764781]\n",
      " ...\n",
      " [-0.17353259]\n",
      " [ 0.05468071]\n",
      " [-1.00246616]]\n",
      "t [[-0.04858009]\n",
      " [-0.61008148]\n",
      " [-0.70764781]\n",
      " ...\n",
      " [-0.17353259]\n",
      " [ 0.05468071]\n",
      " [-1.00246616]]\n",
      "t [[-0.05716629]\n",
      " [-0.6842375 ]\n",
      " [-0.75048885]\n",
      " ...\n",
      " [-0.17406302]\n",
      " [ 0.0609442 ]\n",
      " [-1.09230097]]\n",
      "t [[-0.05716629]\n",
      " [-0.6842375 ]\n",
      " [-0.75048885]\n",
      " ...\n",
      " [-0.17406302]\n",
      " [ 0.0609442 ]\n",
      " [-1.09230097]]\n",
      "Current iteration=8, loss=34960.44413996303\n",
      "t [[-0.0648026 ]\n",
      " [-0.75327686]\n",
      " [-0.79033764]\n",
      " ...\n",
      " [-0.17526064]\n",
      " [ 0.06542123]\n",
      " [-1.17495328]]\n",
      "t [[-0.0648026 ]\n",
      " [-0.75327686]\n",
      " [-0.79033764]\n",
      " ...\n",
      " [-0.17526064]\n",
      " [ 0.06542123]\n",
      " [-1.17495328]]\n",
      "t [[-0.07156267]\n",
      " [-0.8175989 ]\n",
      " [-0.82767789]\n",
      " ...\n",
      " [-0.1771212 ]\n",
      " [ 0.06841504]\n",
      " [-1.25135202]]\n",
      "loss=34554.00023340741\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.09524244]\n",
      " [-0.40686479]\n",
      " [-0.25572917]\n",
      " ...\n",
      " [-0.18534026]\n",
      " [ 0.04012719]\n",
      " [-0.10804363]]\n",
      "t [[ 0.09524244]\n",
      " [-0.40686479]\n",
      " [-0.25572917]\n",
      " ...\n",
      " [-0.18534026]\n",
      " [ 0.04012719]\n",
      " [-0.10804363]]\n",
      "t [[ 0.14778117]\n",
      " [-0.66715873]\n",
      " [-0.41513606]\n",
      " ...\n",
      " [-0.29009444]\n",
      " [ 0.05128572]\n",
      " [-0.1939211 ]]\n",
      "t [[ 0.14778117]\n",
      " [-0.66715873]\n",
      " [-0.41513606]\n",
      " ...\n",
      " [-0.29009444]\n",
      " [ 0.05128572]\n",
      " [-0.1939211 ]]\n",
      "Current iteration=2, loss=37582.90582070749\n",
      "t [[ 0.17823238]\n",
      " [-0.85136648]\n",
      " [-0.52710219]\n",
      " ...\n",
      " [-0.35723371]\n",
      " [ 0.04723068]\n",
      " [-0.265256  ]]\n",
      "t [[ 0.17823238]\n",
      " [-0.85136648]\n",
      " [-0.52710219]\n",
      " ...\n",
      " [-0.35723371]\n",
      " [ 0.04723068]\n",
      " [-0.265256  ]]\n",
      "t [[ 0.1965169 ]\n",
      " [-0.99211194]\n",
      " [-0.61361271]\n",
      " ...\n",
      " [-0.4058692 ]\n",
      " [ 0.0345178 ]\n",
      " [-0.32612802]]\n",
      "t [[ 0.1965169 ]\n",
      " [-0.99211194]\n",
      " [-0.61361271]\n",
      " ...\n",
      " [-0.4058692 ]\n",
      " [ 0.0345178 ]\n",
      " [-0.32612802]]\n",
      "Current iteration=4, loss=36333.03538031356\n",
      "t [[ 0.20768753]\n",
      " [-1.1055661 ]\n",
      " [-0.68509895]\n",
      " ...\n",
      " [-0.44483631]\n",
      " [ 0.01658159]\n",
      " [-0.37895222]]\n",
      "t [[ 0.20768753]\n",
      " [-1.1055661 ]\n",
      " [-0.68509895]\n",
      " ...\n",
      " [-0.44483631]\n",
      " [ 0.01658159]\n",
      " [-0.37895222]]\n",
      "t [[ 0.21449984]\n",
      " [-1.20046462]\n",
      " [-0.74685755]\n",
      " ...\n",
      " [-0.47841373]\n",
      " [-0.00457256]\n",
      " [-0.42531522]]\n",
      "t [[ 0.21449984]\n",
      " [-1.20046462]\n",
      " [-0.74685755]\n",
      " ...\n",
      " [-0.47841373]\n",
      " [-0.00457256]\n",
      " [-0.42531522]]\n",
      "Current iteration=6, loss=35542.37539429333\n",
      "t [[ 0.2185404 ]\n",
      " [-1.28188609]\n",
      " [-0.8017327 ]\n",
      " ...\n",
      " [-0.50873064]\n",
      " [-0.02767821]\n",
      " [-0.46634147]]\n",
      "t [[ 0.2185404 ]\n",
      " [-1.28188609]\n",
      " [-0.8017327 ]\n",
      " ...\n",
      " [-0.50873064]\n",
      " [-0.02767821]\n",
      " [-0.46634147]]\n",
      "t [[ 0.22076538]\n",
      " [-1.35298564]\n",
      " [-0.85133777]\n",
      " ...\n",
      " [-0.53686427]\n",
      " [-0.05189194]\n",
      " [-0.50287174]]\n",
      "t [[ 0.22076538]\n",
      " [-1.35298564]\n",
      " [-0.85133777]\n",
      " ...\n",
      " [-0.53686427]\n",
      " [-0.05189194]\n",
      " [-0.50287174]]\n",
      "Current iteration=8, loss=34997.32515439944\n",
      "t [[ 0.22177497]\n",
      " [-1.41584915]\n",
      " [-0.89664637]\n",
      " ...\n",
      " [-0.56336786]\n",
      " [-0.07663209]\n",
      " [-0.53555925]]\n",
      "t [[ 0.22177497]\n",
      " [-1.41584915]\n",
      " [-0.89664637]\n",
      " ...\n",
      " [-0.56336786]\n",
      " [-0.07663209]\n",
      " [-0.53555925]]\n",
      "t [[ 0.22195991]\n",
      " [-1.47193651]\n",
      " [-0.93828932]\n",
      " ...\n",
      " [-0.58853227]\n",
      " [-0.10148896]\n",
      " [-0.56492584]]\n",
      "loss=34602.49988301808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01255997]\n",
      " [-0.07733003]\n",
      " [-0.24243772]\n",
      " ...\n",
      " [-0.18604145]\n",
      " [ 0.03616773]\n",
      " [-0.11036804]]\n",
      "t [[ 0.01255997]\n",
      " [-0.07733003]\n",
      " [-0.24243772]\n",
      " ...\n",
      " [-0.18604145]\n",
      " [ 0.03616773]\n",
      " [-0.11036804]]\n",
      "t [[ 0.01043638]\n",
      " [-0.17715698]\n",
      " [-0.38602689]\n",
      " ...\n",
      " [-0.28977583]\n",
      " [ 0.04406051]\n",
      " [-0.19807108]]\n",
      "t [[ 0.01043638]\n",
      " [-0.17715698]\n",
      " [-0.38602689]\n",
      " ...\n",
      " [-0.28977583]\n",
      " [ 0.04406051]\n",
      " [-0.19807108]]\n",
      "Current iteration=2, loss=37550.144928796486\n",
      "t [[ 0.0027688 ]\n",
      " [-0.28132743]\n",
      " [-0.48309948]\n",
      " ...\n",
      " [-0.35553423]\n",
      " [ 0.03746226]\n",
      " [-0.27096817]]\n",
      "t [[ 0.0027688 ]\n",
      " [-0.28132743]\n",
      " [-0.48309948]\n",
      " ...\n",
      " [-0.35553423]\n",
      " [ 0.03746226]\n",
      " [-0.27096817]]\n",
      "t [[-0.00665156]\n",
      " [-0.38267038]\n",
      " [-0.55662664]\n",
      " ...\n",
      " [-0.40278478]\n",
      " [ 0.02280606]\n",
      " [-0.33322461]]\n",
      "t [[-0.00665156]\n",
      " [-0.38267038]\n",
      " [-0.55662664]\n",
      " ...\n",
      " [-0.40278478]\n",
      " [ 0.02280606]\n",
      " [-0.33322461]]\n",
      "Current iteration=4, loss=36278.36059537813\n",
      "t [[-0.01622935]\n",
      " [-0.47850777]\n",
      " [-0.61719078]\n",
      " ...\n",
      " [-0.44044555]\n",
      " [ 0.00340125]\n",
      " [-0.3872954 ]]\n",
      "t [[-0.01622935]\n",
      " [-0.47850777]\n",
      " [-0.61719078]\n",
      " ...\n",
      " [-0.44044555]\n",
      " [ 0.00340125]\n",
      " [-0.3872954 ]]\n",
      "t [[-0.02529756]\n",
      " [-0.56803162]\n",
      " [-0.66995721]\n",
      " ...\n",
      " [-0.47280378]\n",
      " [-0.01885055]\n",
      " [-0.43479104]]\n",
      "t [[-0.02529756]\n",
      " [-0.56803162]\n",
      " [-0.66995721]\n",
      " ...\n",
      " [-0.47280378]\n",
      " [-0.01885055]\n",
      " [-0.43479104]]\n",
      "Current iteration=6, loss=35472.26714487214\n",
      "t [[-0.03360515]\n",
      " [-0.65123665]\n",
      " [-0.71755441]\n",
      " ...\n",
      " [-0.50197939]\n",
      " [-0.04276533]\n",
      " [-0.47685276]]\n",
      "t [[-0.03360515]\n",
      " [-0.65123665]\n",
      " [-0.71755441]\n",
      " ...\n",
      " [-0.50197939]\n",
      " [-0.04276533]\n",
      " [-0.47685276]]\n",
      "t [[-0.04109068]\n",
      " [-0.72845179]\n",
      " [-0.76137082]\n",
      " ...\n",
      " [-0.52903903]\n",
      " [-0.0675643 ]\n",
      " [-0.51433422]]\n",
      "t [[-0.04109068]\n",
      " [-0.72845179]\n",
      " [-0.76137082]\n",
      " ...\n",
      " [-0.52903903]\n",
      " [-0.0675643 ]\n",
      " [-0.51433422]]\n",
      "Current iteration=8, loss=34916.867064691316\n",
      "t [[-0.04777691]\n",
      " [-0.8001265 ]\n",
      " [-0.80217321]\n",
      " ...\n",
      " [-0.55452807]\n",
      " [-0.09271629]\n",
      " [-0.54789906]]\n",
      "t [[-0.04777691]\n",
      " [-0.8001265 ]\n",
      " [-0.80217321]\n",
      " ...\n",
      " [-0.55452807]\n",
      " [-0.09271629]\n",
      " [-0.54789906]]\n",
      "t [[-0.05372086]\n",
      " [-0.86673329]\n",
      " [-0.84041139]\n",
      " ...\n",
      " [-0.57873225]\n",
      " [-0.11785105]\n",
      " [-0.57807774]]\n",
      "loss=34515.06707752927\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0107276 ]\n",
      " [-0.06786947]\n",
      " [-0.2450634 ]\n",
      " ...\n",
      " [-0.18293024]\n",
      " [ 0.03888736]\n",
      " [-0.1035257 ]]\n",
      "t [[ 0.0107276 ]\n",
      " [-0.06786947]\n",
      " [-0.2450634 ]\n",
      " ...\n",
      " [-0.18293024]\n",
      " [ 0.03888736]\n",
      " [-0.1035257 ]]\n",
      "t [[ 0.00691336]\n",
      " [-0.15930686]\n",
      " [-0.39145745]\n",
      " ...\n",
      " [-0.28466435]\n",
      " [ 0.04920672]\n",
      " [-0.18515349]]\n",
      "t [[ 0.00691336]\n",
      " [-0.15930686]\n",
      " [-0.39145745]\n",
      " ...\n",
      " [-0.28466435]\n",
      " [ 0.04920672]\n",
      " [-0.18515349]]\n",
      "Current iteration=2, loss=37582.873607542955\n",
      "t [[-0.00225191]\n",
      " [-0.25606526]\n",
      " [-0.49104935]\n",
      " ...\n",
      " [-0.34886322]\n",
      " [ 0.04470756]\n",
      " [-0.25257329]]\n",
      "t [[-0.00225191]\n",
      " [-0.25606526]\n",
      " [-0.49104935]\n",
      " ...\n",
      " [-0.34886322]\n",
      " [ 0.04470756]\n",
      " [-0.25257329]]\n",
      "t [[-0.01295504]\n",
      " [-0.3508164 ]\n",
      " [-0.56676322]\n",
      " ...\n",
      " [-0.39480064]\n",
      " [ 0.0318443 ]\n",
      " [-0.30985099]]\n",
      "t [[-0.01295504]\n",
      " [-0.3508164 ]\n",
      " [-0.56676322]\n",
      " ...\n",
      " [-0.39480064]\n",
      " [ 0.0318443 ]\n",
      " [-0.30985099]]\n",
      "Current iteration=4, loss=36333.245134449564\n",
      "t [[-0.0236027 ]\n",
      " [-0.4407484 ]\n",
      " [-0.62921997]\n",
      " ...\n",
      " [-0.43133056]\n",
      " [ 0.01395609]\n",
      " [-0.35937309]]\n",
      "t [[-0.0236027 ]\n",
      " [-0.4407484 ]\n",
      " [-0.62921997]\n",
      " ...\n",
      " [-0.43133056]\n",
      " [ 0.01395609]\n",
      " [-0.35937309]]\n",
      "t [[-0.03354288]\n",
      " [-0.52494749]\n",
      " [-0.68363291]\n",
      " ...\n",
      " [-0.46271318]\n",
      " [-0.00702486]\n",
      " [-0.40269759]]\n",
      "t [[-0.03354288]\n",
      " [-0.52494749]\n",
      " [-0.68363291]\n",
      " ...\n",
      " [-0.46271318]\n",
      " [-0.00702486]\n",
      " [-0.40269759]]\n",
      "Current iteration=6, loss=35542.926015185454\n",
      "t [[-0.04254518]\n",
      " [-0.6033245 ]\n",
      " [-0.7326726 ]\n",
      " ...\n",
      " [-0.49105209]\n",
      " [-0.02988434]\n",
      " [-0.4409231 ]]\n",
      "t [[-0.04254518]\n",
      " [-0.6033245 ]\n",
      " [-0.7326726 ]\n",
      " ...\n",
      " [-0.49105209]\n",
      " [-0.02988434]\n",
      " [-0.4409231 ]]\n",
      "t [[-0.05057018]\n",
      " [-0.6761408 ]\n",
      " [-0.77776151]\n",
      " ...\n",
      " [-0.51740043]\n",
      " [-0.05381455]\n",
      " [-0.4748675 ]]\n",
      "t [[-0.05057018]\n",
      " [-0.6761408 ]\n",
      " [-0.77776151]\n",
      " ...\n",
      " [-0.51740043]\n",
      " [-0.05381455]\n",
      " [-0.4748675 ]]\n",
      "Current iteration=8, loss=34998.57064336656\n",
      "t [[-0.05766194]\n",
      " [-0.74379067]\n",
      " [-0.8196934 ]\n",
      " ...\n",
      " [-0.54229078]\n",
      " [-0.07825727]\n",
      " [-0.50516382]]\n",
      "t [[-0.05766194]\n",
      " [-0.74379067]\n",
      " [-0.8196934 ]\n",
      " ...\n",
      " [-0.54229078]\n",
      " [-0.07825727]\n",
      " [-0.50516382]]\n",
      "t [[-0.06389696]\n",
      " [-0.80670095]\n",
      " [-0.85893948]\n",
      " ...\n",
      " [-0.56599646]\n",
      " [-0.10281766]\n",
      " [-0.53231606]]\n",
      "loss=34604.552094334525\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00893393]\n",
      " [-0.07188547]\n",
      " [-0.2434464 ]\n",
      " ...\n",
      " [-0.10718319]\n",
      " [-0.01290887]\n",
      " [-0.21867107]]\n",
      "t [[ 0.00893393]\n",
      " [-0.07188547]\n",
      " [-0.2434464 ]\n",
      " ...\n",
      " [-0.10718319]\n",
      " [-0.01290887]\n",
      " [-0.21867107]]\n",
      "t [[ 0.00361439]\n",
      " [-0.1671581 ]\n",
      " [-0.38742186]\n",
      " ...\n",
      " [-0.15061868]\n",
      " [-0.00471782]\n",
      " [-0.40007784]]\n",
      "t [[ 0.00361439]\n",
      " [-0.1671581 ]\n",
      " [-0.38742186]\n",
      " ...\n",
      " [-0.15061868]\n",
      " [-0.00471782]\n",
      " [-0.40007784]]\n",
      "Current iteration=2, loss=37536.28234344996\n",
      "t [[-0.00680846]\n",
      " [-0.26747688]\n",
      " [-0.4844587 ]\n",
      " ...\n",
      " [-0.16713859]\n",
      " [ 0.00972981]\n",
      " [-0.55573998]]\n",
      "t [[-0.00680846]\n",
      " [-0.26747688]\n",
      " [-0.4844587 ]\n",
      " ...\n",
      " [-0.16713859]\n",
      " [ 0.00972981]\n",
      " [-0.55573998]]\n",
      "t [[-0.01857138]\n",
      " [-0.36551429]\n",
      " [-0.55770335]\n",
      " ...\n",
      " [-0.1725014 ]\n",
      " [ 0.02456155]\n",
      " [-0.692289  ]]\n",
      "t [[-0.01857138]\n",
      " [-0.36551429]\n",
      " [-0.55770335]\n",
      " ...\n",
      " [-0.1725014 ]\n",
      " [ 0.02456155]\n",
      " [-0.692289  ]]\n",
      "Current iteration=4, loss=36264.54315515379\n",
      "t [[-0.0301236 ]\n",
      " [-0.45847618]\n",
      " [-0.61785111]\n",
      " ...\n",
      " [-0.17360529]\n",
      " [ 0.03760009]\n",
      " [-0.81386993]]\n",
      "t [[-0.0301236 ]\n",
      " [-0.45847618]\n",
      " [-0.61785111]\n",
      " ...\n",
      " [-0.17360529]\n",
      " [ 0.03760009]\n",
      " [-0.81386993]]\n",
      "t [[-0.0408462 ]\n",
      " [-0.54546887]\n",
      " [-0.6701366 ]\n",
      " ...\n",
      " [-0.1735155 ]\n",
      " [ 0.04820669]\n",
      " [-0.92330108]]\n",
      "t [[-0.0408462 ]\n",
      " [-0.54546887]\n",
      " [-0.6701366 ]\n",
      " ...\n",
      " [-0.1735155 ]\n",
      " [ 0.04820669]\n",
      " [-0.92330108]]\n",
      "Current iteration=6, loss=35458.85726686935\n",
      "t [[-0.05053376]\n",
      " [-0.6264233 ]\n",
      " [-0.71722967]\n",
      " ...\n",
      " [-0.17354658]\n",
      " [ 0.05638874]\n",
      " [-1.02262071]]\n",
      "t [[-0.05053376]\n",
      " [-0.6264233 ]\n",
      " [-0.71722967]\n",
      " ...\n",
      " [-0.17354658]\n",
      " [ 0.05638874]\n",
      " [-1.02262071]]\n",
      "t [[-0.05916568]\n",
      " [-0.7016204 ]\n",
      " [-0.76054261]\n",
      " ...\n",
      " [-0.17419689]\n",
      " [ 0.06240597]\n",
      " [-1.11336989]]\n",
      "t [[-0.05916568]\n",
      " [-0.7016204 ]\n",
      " [-0.76054261]\n",
      " ...\n",
      " [-0.17419689]\n",
      " [ 0.06240597]\n",
      " [-1.11336989]]\n",
      "Current iteration=8, loss=34903.48382839145\n",
      "t [[-0.06680016]\n",
      " [-0.77147292]\n",
      " [-0.80085526]\n",
      " ...\n",
      " [-0.17558881]\n",
      " [ 0.06659315]\n",
      " [-1.1967524 ]]\n",
      "t [[-0.06680016]\n",
      " [-0.77147292]\n",
      " [-0.80085526]\n",
      " ...\n",
      " [-0.17558881]\n",
      " [ 0.06659315]\n",
      " [-1.1967524 ]]\n",
      "t [[-0.07352442]\n",
      " [-0.83642478]\n",
      " [-0.83862414]\n",
      " ...\n",
      " [-0.17768031]\n",
      " [ 0.0692824 ]\n",
      " [-1.27373191]]\n",
      "loss=34501.49768435747\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.09796366]\n",
      " [-0.4184895 ]\n",
      " [-0.26303572]\n",
      " ...\n",
      " [-0.19063569]\n",
      " [ 0.04127368]\n",
      " [-0.11113059]]\n",
      "t [[ 0.09796366]\n",
      " [-0.4184895 ]\n",
      " [-0.26303572]\n",
      " ...\n",
      " [-0.19063569]\n",
      " [ 0.04127368]\n",
      " [-0.11113059]]\n",
      "t [[ 0.1507815 ]\n",
      " [-0.68202969]\n",
      " [-0.42424472]\n",
      " ...\n",
      " [-0.29607831]\n",
      " [ 0.05192754]\n",
      " [-0.19882386]]\n",
      "t [[ 0.1507815 ]\n",
      " [-0.68202969]\n",
      " [-0.42424472]\n",
      " ...\n",
      " [-0.29607831]\n",
      " [ 0.05192754]\n",
      " [-0.19882386]]\n",
      "Current iteration=2, loss=37531.875893093136\n",
      "t [[ 0.1808453 ]\n",
      " [-0.86715513]\n",
      " [-0.53670203]\n",
      " ...\n",
      " [-0.36299057]\n",
      " [ 0.0468896 ]\n",
      " [-0.2713608 ]]\n",
      "t [[ 0.1808453 ]\n",
      " [-0.86715513]\n",
      " [-0.53670203]\n",
      " ...\n",
      " [-0.36299057]\n",
      " [ 0.0468896 ]\n",
      " [-0.2713608 ]]\n",
      "t [[ 0.19861081]\n",
      " [-1.00817739]\n",
      " [-0.6234918 ]\n",
      " ...\n",
      " [-0.41142468]\n",
      " [ 0.03306612]\n",
      " [-0.33306666]]\n",
      "t [[ 0.19861081]\n",
      " [-1.00817739]\n",
      " [-0.6234918 ]\n",
      " ...\n",
      " [-0.41142468]\n",
      " [ 0.03306612]\n",
      " [-0.33306666]]\n",
      "Current iteration=4, loss=36275.0646244277\n",
      "t [[ 0.2092897 ]\n",
      " [-1.12173514]\n",
      " [-0.69529361]\n",
      " ...\n",
      " [-0.45039674]\n",
      " [ 0.01401486]\n",
      " [-0.38647106]]\n",
      "t [[ 0.2092897 ]\n",
      " [-1.12173514]\n",
      " [-0.69529361]\n",
      " ...\n",
      " [-0.45039674]\n",
      " [ 0.01401486]\n",
      " [-0.38647106]]\n",
      "t [[ 0.21567741]\n",
      " [-1.21668019]\n",
      " [-0.75742016]\n",
      " ...\n",
      " [-0.48416259]\n",
      " [-0.00820673]\n",
      " [-0.43322671]]\n",
      "t [[ 0.21567741]\n",
      " [-1.21668019]\n",
      " [-0.75742016]\n",
      " ...\n",
      " [-0.48416259]\n",
      " [-0.00820673]\n",
      " [-0.43322671]]\n",
      "Current iteration=6, loss=35484.92104295511\n",
      "t [[ 0.21936237]\n",
      " [-1.29810698]\n",
      " [-0.81267754]\n",
      " ...\n",
      " [-0.51478651]\n",
      " [-0.03230783]\n",
      " [-0.47450187]]\n",
      "t [[ 0.21936237]\n",
      " [-1.29810698]\n",
      " [-0.81267754]\n",
      " ...\n",
      " [-0.51478651]\n",
      " [-0.03230783]\n",
      " [-0.47450187]]\n",
      "t [[ 0.22129214]\n",
      " [-1.36916439]\n",
      " [-0.86264012]\n",
      " ...\n",
      " [-0.54328647]\n",
      " [-0.05743351]\n",
      " [-0.51116885]]\n",
      "t [[ 0.22129214]\n",
      " [-1.36916439]\n",
      " [-0.86264012]\n",
      " ...\n",
      " [-0.54328647]\n",
      " [-0.05743351]\n",
      " [-0.51116885]]\n",
      "Current iteration=8, loss=34943.43672213904\n",
      "t [[ 0.222057  ]\n",
      " [-1.43193266]\n",
      " [-0.9082546 ]\n",
      " ...\n",
      " [-0.57017278]\n",
      " [-0.08299759]\n",
      " [-0.54390463]]\n",
      "t [[ 0.222057  ]\n",
      " [-1.43193266]\n",
      " [-0.9082546 ]\n",
      " ...\n",
      " [-0.57017278]\n",
      " [-0.08299759]\n",
      " [-0.54390463]]\n",
      "t [[ 0.22203914]\n",
      " [-1.48787122]\n",
      " [-0.95013718]\n",
      " ...\n",
      " [-0.59570868]\n",
      " [-0.10859004]\n",
      " [-0.57324948]]\n",
      "loss=34552.9887644767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01291882]\n",
      " [-0.07953946]\n",
      " [-0.24936451]\n",
      " ...\n",
      " [-0.19135692]\n",
      " [ 0.03720109]\n",
      " [-0.11352141]]\n",
      "t [[ 0.01291882]\n",
      " [-0.07953946]\n",
      " [-0.24936451]\n",
      " ...\n",
      " [-0.19135692]\n",
      " [ 0.03720109]\n",
      " [-0.11352141]]\n",
      "t [[ 0.01031988]\n",
      " [-0.18285532]\n",
      " [-0.3942323 ]\n",
      " ...\n",
      " [-0.29570258]\n",
      " [ 0.04451651]\n",
      " [-0.20307832]]\n",
      "t [[ 0.01031988]\n",
      " [-0.18285532]\n",
      " [-0.3942323 ]\n",
      " ...\n",
      " [-0.29570258]\n",
      " [ 0.04451651]\n",
      " [-0.20307832]]\n",
      "Current iteration=2, loss=37498.38930144335\n",
      "t [[ 0.00212093]\n",
      " [-0.2902419 ]\n",
      " [-0.49142403]\n",
      " ...\n",
      " [-0.36117378]\n",
      " [ 0.03690413]\n",
      " [-0.27720708]]\n",
      "t [[ 0.00212093]\n",
      " [-0.2902419 ]\n",
      " [-0.49142403]\n",
      " ...\n",
      " [-0.36117378]\n",
      " [ 0.03690413]\n",
      " [-0.27720708]]\n",
      "t [[-0.00771774]\n",
      " [-0.39423257]\n",
      " [-0.5650247 ]\n",
      " ...\n",
      " [-0.40818267]\n",
      " [ 0.02113337]\n",
      " [-0.34032173]]\n",
      "t [[-0.00771774]\n",
      " [-0.39423257]\n",
      " [-0.5650247 ]\n",
      " ...\n",
      " [-0.40818267]\n",
      " [ 0.02113337]\n",
      " [-0.34032173]]\n",
      "Current iteration=4, loss=36219.28183582068\n",
      "t [[-0.01758605]\n",
      " [-0.49217117]\n",
      " [-0.6258305 ]\n",
      " ...\n",
      " [-0.44582005]\n",
      " [ 0.00062611]\n",
      " [-0.39499272]]\n",
      "t [[-0.01758605]\n",
      " [-0.49217117]\n",
      " [-0.6258305 ]\n",
      " ...\n",
      " [-0.44582005]\n",
      " [ 0.00062611]\n",
      " [-0.39499272]]\n",
      "t [[-0.0268388 ]\n",
      " [-0.5833406 ]\n",
      " [-0.67898656]\n",
      " ...\n",
      " [-0.47834446]\n",
      " [-0.02267118]\n",
      " [-0.44289727]]\n",
      "t [[-0.0268388 ]\n",
      " [-0.5833406 ]\n",
      " [-0.67898656]\n",
      " ...\n",
      " [-0.47834446]\n",
      " [-0.02267118]\n",
      " [-0.44289727]]\n",
      "Current iteration=6, loss=35413.68845520807\n",
      "t [[-0.03525127]\n",
      " [-0.66782796]\n",
      " [-0.72705466]\n",
      " ...\n",
      " [-0.507808  ]\n",
      " [-0.04755482]\n",
      " [-0.48522098]]\n",
      "t [[-0.03525127]\n",
      " [-0.66782796]\n",
      " [-0.72705466]\n",
      " ...\n",
      " [-0.507808  ]\n",
      " [-0.04755482]\n",
      " [-0.48522098]]\n",
      "t [[-0.0427841 ]\n",
      " [-0.74603895]\n",
      " [-0.77136342]\n",
      " ...\n",
      " [-0.53521693]\n",
      " [-0.07323757]\n",
      " [-0.52284958]]\n",
      "t [[-0.0427841 ]\n",
      " [-0.74603895]\n",
      " [-0.77136342]\n",
      " ...\n",
      " [-0.53521693]\n",
      " [-0.07323757]\n",
      " [-0.52284958]]\n",
      "Current iteration=8, loss=34861.977478536945\n",
      "t [[-0.04947656]\n",
      " [-0.81848268]\n",
      " [-0.81263805]\n",
      " ...\n",
      " [-0.56107346]\n",
      " [-0.09918564]\n",
      " [-0.55647093]]\n",
      "t [[-0.04947656]\n",
      " [-0.81848268]\n",
      " [-0.81263805]\n",
      " ...\n",
      " [-0.56107346]\n",
      " [-0.09918564]\n",
      " [-0.55647093]]\n",
      "t [[-0.05539748]\n",
      " [-0.88567696]\n",
      " [-0.85130343]\n",
      " ...\n",
      " [-0.58563585]\n",
      " [-0.12502958]\n",
      " [-0.5866343 ]]\n",
      "loss=34464.71051835986\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0110341 ]\n",
      " [-0.06980859]\n",
      " [-0.25206521]\n",
      " ...\n",
      " [-0.18815682]\n",
      " [ 0.03999843]\n",
      " [-0.10648357]]\n",
      "t [[ 0.0110341 ]\n",
      " [-0.06980859]\n",
      " [-0.25206521]\n",
      " ...\n",
      " [-0.18815682]\n",
      " [ 0.03999843]\n",
      " [-0.10648357]]\n",
      "t [[ 0.00670001]\n",
      " [-0.16452722]\n",
      " [-0.3998216 ]\n",
      " ...\n",
      " [-0.29047619]\n",
      " [ 0.04980041]\n",
      " [-0.18981401]]\n",
      "t [[ 0.00670001]\n",
      " [-0.16452722]\n",
      " [-0.3998216 ]\n",
      " ...\n",
      " [-0.29047619]\n",
      " [ 0.04980041]\n",
      " [-0.18981401]]\n",
      "Current iteration=2, loss=37531.880886460516\n",
      "t [[-0.0030284 ]\n",
      " [-0.26434624]\n",
      " [-0.49958824]\n",
      " ...\n",
      " [-0.3543689 ]\n",
      " [ 0.04432784]\n",
      " [-0.25834336]]\n",
      "t [[-0.0030284 ]\n",
      " [-0.26434624]\n",
      " [-0.49958824]\n",
      " ...\n",
      " [-0.3543689 ]\n",
      " [ 0.04432784]\n",
      " [-0.25834336]]\n",
      "t [[-0.01416787]\n",
      " [-0.36162711]\n",
      " [-0.57541   ]\n",
      " ...\n",
      " [-0.40004893]\n",
      " [ 0.03037469]\n",
      " [-0.31637982]]\n",
      "t [[-0.01416787]\n",
      " [-0.36162711]\n",
      " [-0.57541   ]\n",
      " ...\n",
      " [-0.40004893]\n",
      " [ 0.03037469]\n",
      " [-0.31637982]]\n",
      "Current iteration=4, loss=36275.27810261679\n",
      "t [[-0.02511193]\n",
      " [-0.45357007]\n",
      " [-0.63812911]\n",
      " ...\n",
      " [-0.4365445 ]\n",
      " [ 0.01139544]\n",
      " [-0.36642132]]\n",
      "t [[-0.02511193]\n",
      " [-0.45357007]\n",
      " [-0.63812911]\n",
      " ...\n",
      " [-0.4365445 ]\n",
      " [ 0.01139544]\n",
      " [-0.36642132]]\n",
      "t [[-0.03523288]\n",
      " [-0.53934563]\n",
      " [-0.69294361]\n",
      " ...\n",
      " [-0.46808799]\n",
      " [-0.01063015]\n",
      " [-0.41008956]]\n",
      "t [[-0.03523288]\n",
      " [-0.53934563]\n",
      " [-0.69294361]\n",
      " ...\n",
      " [-0.46808799]\n",
      " [-0.01063015]\n",
      " [-0.41008956]]\n",
      "Current iteration=6, loss=35485.51606055785\n",
      "t [[-0.04432908]\n",
      " [-0.61895232]\n",
      " [-0.74246032]\n",
      " ...\n",
      " [-0.49671503]\n",
      " [-0.03446562]\n",
      " [-0.44852502]]\n",
      "t [[-0.04432908]\n",
      " [-0.61895232]\n",
      " [-0.74246032]\n",
      " ...\n",
      " [-0.49671503]\n",
      " [-0.03446562]\n",
      " [-0.44852502]]\n",
      "t [[-0.05238533]\n",
      " [-0.69272483]\n",
      " [-0.78804373]\n",
      " ...\n",
      " [-0.52341767]\n",
      " [-0.05929233]\n",
      " [-0.48257556]]\n",
      "t [[-0.05238533]\n",
      " [-0.69272483]\n",
      " [-0.78804373]\n",
      " ...\n",
      " [-0.52341767]\n",
      " [-0.05929233]\n",
      " [-0.48257556]]\n",
      "Current iteration=8, loss=34944.77038724494\n",
      "t [[-0.05946392]\n",
      " [-0.76111445]\n",
      " [-0.83044719]\n",
      " ...\n",
      " [-0.54868436]\n",
      " [-0.08454747]\n",
      " [-0.5128967 ]]\n",
      "t [[-0.05946392]\n",
      " [-0.76111445]\n",
      " [-0.83044719]\n",
      " ...\n",
      " [-0.54868436]\n",
      " [-0.08454747]\n",
      " [-0.5128967 ]]\n",
      "t [[-0.06565449]\n",
      " [-0.82459132]\n",
      " [-0.87011772]\n",
      " ...\n",
      " [-0.57275994]\n",
      " [-0.10983538]\n",
      " [-0.54000981]]\n",
      "loss=34555.1568463706\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00918919]\n",
      " [-0.07393935]\n",
      " [-0.25040201]\n",
      " ...\n",
      " [-0.11024557]\n",
      " [-0.0132777 ]\n",
      " [-0.22491881]]\n",
      "t [[ 0.00918919]\n",
      " [-0.07393935]\n",
      " [-0.25040201]\n",
      " ...\n",
      " [-0.11024557]\n",
      " [-0.0132777 ]\n",
      " [-0.22491881]]\n",
      "t [[ 0.00331509]\n",
      " [-0.1725965 ]\n",
      " [-0.39564917]\n",
      " ...\n",
      " [-0.15310094]\n",
      " [-0.0042485 ]\n",
      " [-0.41043444]]\n",
      "t [[ 0.00331509]\n",
      " [-0.1725965 ]\n",
      " [-0.39564917]\n",
      " ...\n",
      " [-0.15310094]\n",
      " [-0.0042485 ]\n",
      " [-0.41043444]]\n",
      "Current iteration=2, loss=37484.44166476896\n",
      "t [[-0.00769253]\n",
      " [-0.27606164]\n",
      " [-0.49278048]\n",
      " ...\n",
      " [-0.16855901]\n",
      " [ 0.01096837]\n",
      " [-0.5690675 ]]\n",
      "t [[-0.00769253]\n",
      " [-0.27606164]\n",
      " [-0.49278048]\n",
      " ...\n",
      " [-0.16855901]\n",
      " [ 0.01096837]\n",
      " [-0.5690675 ]]\n",
      "t [[-0.01990492]\n",
      " [-0.3766993 ]\n",
      " [-0.56606964]\n",
      " ...\n",
      " [-0.17311592]\n",
      " [ 0.02625655]\n",
      " [-0.70787193]]\n",
      "t [[-0.01990492]\n",
      " [-0.3766993 ]\n",
      " [-0.56606964]\n",
      " ...\n",
      " [-0.17311592]\n",
      " [ 0.02625655]\n",
      " [-0.70787193]]\n",
      "Current iteration=4, loss=36205.49796941333\n",
      "t [[-0.03176143]\n",
      " [-0.47172953]\n",
      " [-0.62643196]\n",
      " ...\n",
      " [-0.17376426]\n",
      " [ 0.03946063]\n",
      " [-0.83120723]]\n",
      "t [[-0.03176143]\n",
      " [-0.47172953]\n",
      " [-0.62643196]\n",
      " ...\n",
      " [-0.17376426]\n",
      " [ 0.03946063]\n",
      " [-0.83120723]]\n",
      "t [[-0.04266948]\n",
      " [-0.56034476]\n",
      " [-0.67908421]\n",
      " ...\n",
      " [-0.17350371]\n",
      " [ 0.05001985]\n",
      " [-0.94202019]]\n",
      "t [[-0.04266948]\n",
      " [-0.56034476]\n",
      " [-0.67908421]\n",
      " ...\n",
      " [-0.17350371]\n",
      " [ 0.05001985]\n",
      " [-0.94202019]]\n",
      "Current iteration=6, loss=35400.3008386913\n",
      "t [[-0.05245366]\n",
      " [-0.64256538]\n",
      " [-0.72662989]\n",
      " ...\n",
      " [-0.17355948]\n",
      " [ 0.05801732]\n",
      " [-1.04243476]]\n",
      "t [[-0.05245366]\n",
      " [-0.64256538]\n",
      " [-0.72662989]\n",
      " ...\n",
      " [-0.17355948]\n",
      " [ 0.05801732]\n",
      " [-1.04243476]]\n",
      "t [[-0.06111832]\n",
      " [-0.71874726]\n",
      " [-0.77042099]\n",
      " ...\n",
      " [-0.17435468]\n",
      " [ 0.06377175]\n",
      " [-1.13405305]]\n",
      "t [[-0.06111832]\n",
      " [-0.71874726]\n",
      " [-0.77042099]\n",
      " ...\n",
      " [-0.17435468]\n",
      " [ 0.06377175]\n",
      " [-1.13405305]]\n",
      "Current iteration=8, loss=34848.587352552066\n",
      "t [[-0.06874032]\n",
      " [-0.78936159]\n",
      " [-0.81119509]\n",
      " ...\n",
      " [-0.17595721]\n",
      " [ 0.06765962]\n",
      " [-1.21812418]]\n",
      "t [[-0.06874032]\n",
      " [-0.78936159]\n",
      " [-0.81119509]\n",
      " ...\n",
      " [-0.17595721]\n",
      " [ 0.06765962]\n",
      " [-1.21812418]]\n",
      "t [[-0.0754202 ]\n",
      " [-0.85489679]\n",
      " [-0.84938305]\n",
      " ...\n",
      " [-0.17828847]\n",
      " [ 0.0700409 ]\n",
      " [-1.29564648]]\n",
      "loss=34451.11099524453\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10068487]\n",
      " [-0.43011421]\n",
      " [-0.27034227]\n",
      " ...\n",
      " [-0.19593113]\n",
      " [ 0.04242017]\n",
      " [-0.11421755]]\n",
      "t [[ 0.10068487]\n",
      " [-0.43011421]\n",
      " [-0.27034227]\n",
      " ...\n",
      " [-0.19593113]\n",
      " [ 0.04242017]\n",
      " [-0.11421755]]\n",
      "t [[ 0.15371667]\n",
      " [-0.69667743]\n",
      " [-0.4332069 ]\n",
      " ...\n",
      " [-0.30193938]\n",
      " [ 0.05252588]\n",
      " [-0.20369231]]\n",
      "t [[ 0.15371667]\n",
      " [-0.69667743]\n",
      " [-0.4332069 ]\n",
      " ...\n",
      " [-0.30193938]\n",
      " [ 0.05252588]\n",
      " [-0.20369231]]\n",
      "Current iteration=2, loss=37481.80880212378\n",
      "t [[ 0.1833571 ]\n",
      " [-0.88259307]\n",
      " [-0.54608349]\n",
      " ...\n",
      " [-0.36857449]\n",
      " [ 0.04647875]\n",
      " [-0.27739762]]\n",
      "t [[ 0.1833571 ]\n",
      " [-0.88259307]\n",
      " [-0.54608349]\n",
      " ...\n",
      " [-0.36857449]\n",
      " [ 0.04647875]\n",
      " [-0.27739762]]\n",
      "t [[ 0.20059282]\n",
      " [-1.02383768]\n",
      " [-0.63313386]\n",
      " ...\n",
      " [-0.41680842]\n",
      " [ 0.03153354]\n",
      " [-0.33990679]]\n",
      "t [[ 0.20059282]\n",
      " [-1.02383768]\n",
      " [-0.63313386]\n",
      " ...\n",
      " [-0.41680842]\n",
      " [ 0.03153354]\n",
      " [-0.33990679]]\n",
      "Current iteration=4, loss=36218.50605694618\n",
      "t [[ 0.21078307]\n",
      " [-1.13747923]\n",
      " [-0.70525467]\n",
      " ...\n",
      " [-0.45580765]\n",
      " [ 0.01136642]\n",
      " [-0.3938634 ]]\n",
      "t [[ 0.21078307]\n",
      " [-1.13747923]\n",
      " [-0.70525467]\n",
      " ...\n",
      " [-0.45580765]\n",
      " [ 0.01136642]\n",
      " [-0.3938634 ]]\n",
      "t [[ 0.21675555]\n",
      " [-1.23246273]\n",
      " [-0.76775606]\n",
      " ...\n",
      " [-0.4897868 ]\n",
      " [-0.0119161 ]\n",
      " [-0.44098591]]\n",
      "t [[ 0.21675555]\n",
      " [-1.23246273]\n",
      " [-0.76775606]\n",
      " ...\n",
      " [-0.4897868 ]\n",
      " [-0.0119161 ]\n",
      " [-0.44098591]]\n",
      "Current iteration=6, loss=35429.22222119716\n",
      "t [[ 0.22009637]\n",
      " [-1.31388785]\n",
      " [-0.82339769]\n",
      " ...\n",
      " [-0.52073688]\n",
      " [-0.03700066]\n",
      " [-0.48248631]]\n",
      "t [[ 0.22009637]\n",
      " [-1.31388785]\n",
      " [-0.82339769]\n",
      " ...\n",
      " [-0.52073688]\n",
      " [-0.03700066]\n",
      " [-0.48248631]]\n",
      "t [[ 0.22174255]\n",
      " [-1.38489368]\n",
      " [-0.8737126 ]\n",
      " ...\n",
      " [-0.54961412]\n",
      " [-0.06302206]\n",
      " [-0.51926843]]\n",
      "t [[ 0.22174255]\n",
      " [-1.38489368]\n",
      " [-0.8737126 ]\n",
      " ...\n",
      " [-0.54961412]\n",
      " [-0.06302206]\n",
      " [-0.51926843]]\n",
      "Current iteration=8, loss=34891.472258223235\n",
      "t [[ 0.22227376]\n",
      " [-1.44755467]\n",
      " [-0.91962117]\n",
      " ...\n",
      " [-0.57688626]\n",
      " [-0.08939056]\n",
      " [-0.55203306]]\n",
      "t [[ 0.22227376]\n",
      " [-1.44755467]\n",
      " [-0.91962117]\n",
      " ...\n",
      " [-0.57688626]\n",
      " [-0.08939056]\n",
      " [-0.55203306]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.22206342]\n",
      " [-1.50333078]\n",
      " [-0.96172655]\n",
      " ...\n",
      " [-0.6027904 ]\n",
      " [-0.11569656]\n",
      " [-0.58133886]]\n",
      "loss=34505.43587998868\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01327768]\n",
      " [-0.08174889]\n",
      " [-0.25629131]\n",
      " ...\n",
      " [-0.19667239]\n",
      " [ 0.03823445]\n",
      " [-0.11667479]]\n",
      "t [[ 0.01327768]\n",
      " [-0.08174889]\n",
      " [-0.25629131]\n",
      " ...\n",
      " [-0.19667239]\n",
      " [ 0.03823445]\n",
      " [-0.11667479]]\n",
      "t [[ 0.01018171]\n",
      " [-0.18858704]\n",
      " [-0.40228745]\n",
      " ...\n",
      " [-0.30150406]\n",
      " [ 0.04493018]\n",
      " [-0.20805052]]\n",
      "t [[ 0.01018171]\n",
      " [-0.18858704]\n",
      " [-0.40228745]\n",
      " ...\n",
      " [-0.30150406]\n",
      " [ 0.04493018]\n",
      " [-0.20805052]]\n",
      "Current iteration=2, loss=37447.599816544505\n",
      "t [[ 0.0014483 ]\n",
      " [-0.29917503]\n",
      " [-0.49953455]\n",
      " ...\n",
      " [-0.36663872]\n",
      " [ 0.03627959]\n",
      " [-0.28337682]]\n",
      "t [[ 0.0014483 ]\n",
      " [-0.29917503]\n",
      " [-0.49953455]\n",
      " ...\n",
      " [-0.36663872]\n",
      " [ 0.03627959]\n",
      " [-0.28337682]]\n",
      "t [[-0.00879966]\n",
      " [-0.40576726]\n",
      " [-0.57320368]\n",
      " ...\n",
      " [-0.41340875]\n",
      " [ 0.01938537]\n",
      " [-0.34731871]]\n",
      "t [[-0.00879966]\n",
      " [-0.40576726]\n",
      " [-0.57320368]\n",
      " ...\n",
      " [-0.41340875]\n",
      " [ 0.01938537]\n",
      " [-0.34731871]]\n",
      "Current iteration=4, loss=36161.634407407655\n",
      "t [[-0.01894461]\n",
      " [-0.5057474 ]\n",
      " [-0.63426901]\n",
      " ...\n",
      " [-0.45104623]\n",
      " [-0.00222333]\n",
      " [-0.40256145]]\n",
      "t [[-0.01894461]\n",
      " [-0.5057474 ]\n",
      " [-0.63426901]\n",
      " ...\n",
      " [-0.45104623]\n",
      " [-0.00222333]\n",
      " [-0.40256145]]\n",
      "t [[-0.02836747]\n",
      " [-0.59850007]\n",
      " [-0.68783469]\n",
      " ...\n",
      " [-0.48376255]\n",
      " [-0.02655832]\n",
      " [-0.45084862]]\n",
      "t [[-0.02836747]\n",
      " [-0.59850007]\n",
      " [-0.68783469]\n",
      " ...\n",
      " [-0.48376255]\n",
      " [-0.02655832]\n",
      " [-0.45084862]]\n",
      "Current iteration=6, loss=35356.90128384294\n",
      "t [[-0.03687178]\n",
      " [-0.68421009]\n",
      " [-0.73638666]\n",
      " ...\n",
      " [-0.51353369]\n",
      " [-0.05239809]\n",
      " [-0.49341014]]\n",
      "t [[-0.03687178]\n",
      " [-0.68421009]\n",
      " [-0.73638666]\n",
      " ...\n",
      " [-0.51353369]\n",
      " [-0.05239809]\n",
      " [-0.49341014]]\n",
      "t [[-0.04444091]\n",
      " [-0.76336205]\n",
      " [-0.7811914 ]\n",
      " ...\n",
      " [-0.54130325]\n",
      " [-0.07894811]\n",
      " [-0.53116382]]\n",
      "t [[-0.04444091]\n",
      " [-0.76336205]\n",
      " [-0.7811914 ]\n",
      " ...\n",
      " [-0.54130325]\n",
      " [-0.07894811]\n",
      " [-0.53116382]]\n",
      "Current iteration=8, loss=34809.054986810355\n",
      "t [[-0.05113067]\n",
      " [-0.8365251 ]\n",
      " [-0.82293331]\n",
      " ...\n",
      " [-0.56753079]\n",
      " [-0.10567282]\n",
      " [-0.56482178]]\n",
      "t [[-0.05113067]\n",
      " [-0.8365251 ]\n",
      " [-0.82293331]\n",
      " ...\n",
      " [-0.56753079]\n",
      " [-0.10567282]\n",
      " [-0.56482178]]\n",
      "t [[-0.05702139]\n",
      " [-0.90426212]\n",
      " [-0.86201422]\n",
      " ...\n",
      " [-0.59244865]\n",
      " [-0.1322043 ]\n",
      " [-0.59495209]]\n",
      "loss=34416.35662354595\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01134061]\n",
      " [-0.07174772]\n",
      " [-0.25906703]\n",
      " ...\n",
      " [-0.1933834 ]\n",
      " [ 0.0411095 ]\n",
      " [-0.10944145]]\n",
      "t [[ 0.01134061]\n",
      " [-0.07174772]\n",
      " [-0.25906703]\n",
      " ...\n",
      " [-0.1933834 ]\n",
      " [ 0.0411095 ]\n",
      " [-0.10944145]]\n",
      "t [[ 0.00646518]\n",
      " [-0.16978277]\n",
      " [-0.40803558]\n",
      " ...\n",
      " [-0.29616438]\n",
      " [ 0.0503512 ]\n",
      " [-0.19444069]]\n",
      "t [[ 0.00646518]\n",
      " [-0.16978277]\n",
      " [-0.40803558]\n",
      " ...\n",
      " [-0.29616438]\n",
      " [ 0.0503512 ]\n",
      " [-0.19444069]]\n",
      "Current iteration=2, loss=37481.848976781264\n",
      "t [[-0.00382877]\n",
      " [-0.27265049]\n",
      " [-0.50791173]\n",
      " ...\n",
      " [-0.35970207]\n",
      " [ 0.04388011]\n",
      " [-0.26404704]]\n",
      "t [[-0.00382877]\n",
      " [-0.27265049]\n",
      " [-0.50791173]\n",
      " ...\n",
      " [-0.35970207]\n",
      " [ 0.04388011]\n",
      " [-0.26404704]]\n",
      "t [[-0.01539441]\n",
      " [-0.37241794]\n",
      " [-0.58383469]\n",
      " ...\n",
      " [-0.40512782]\n",
      " [ 0.02882684]\n",
      " [-0.32281313]]\n",
      "t [[-0.01539441]\n",
      " [-0.37241794]\n",
      " [-0.58383469]\n",
      " ...\n",
      " [-0.40512782]\n",
      " [ 0.02882684]\n",
      " [-0.32281313]]\n",
      "Current iteration=4, loss=36218.72351497879\n",
      "t [[-0.02661965]\n",
      " [-0.46631517]\n",
      " [-0.64683257]\n",
      " ...\n",
      " [-0.44161307]\n",
      " [ 0.00875612]\n",
      " [-0.37334761]]\n",
      "t [[-0.02661965]\n",
      " [-0.46631517]\n",
      " [-0.64683257]\n",
      " ...\n",
      " [-0.44161307]\n",
      " [ 0.00875612]\n",
      " [-0.37334761]]\n",
      "t [[-0.03690559]\n",
      " [-0.55360774]\n",
      " [-0.7020674 ]\n",
      " ...\n",
      " [-0.47334391]\n",
      " [-0.01430777]\n",
      " [-0.41733543]]\n",
      "t [[-0.03690559]\n",
      " [-0.55360774]\n",
      " [-0.7020674 ]\n",
      " ...\n",
      " [-0.47334391]\n",
      " [-0.01430777]\n",
      " [-0.41733543]]\n",
      "Current iteration=6, loss=35429.864484864025\n",
      "t [[-0.04608146]\n",
      " [-0.63438712]\n",
      " [-0.75207314]\n",
      " ...\n",
      " [-0.50227968]\n",
      " [-0.03910778]\n",
      " [-0.45595879]]\n",
      "t [[-0.04608146]\n",
      " [-0.63438712]\n",
      " [-0.75207314]\n",
      " ...\n",
      " [-0.50227968]\n",
      " [-0.03910778]\n",
      " [-0.45595879]]\n",
      "t [[-0.054157  ]\n",
      " [-0.70906342]\n",
      " [-0.79815391]\n",
      " ...\n",
      " [-0.52934888]\n",
      " [-0.06481556]\n",
      " [-0.49009553]]\n",
      "t [[-0.054157  ]\n",
      " [-0.70906342]\n",
      " [-0.79815391]\n",
      " ...\n",
      " [-0.52934888]\n",
      " [-0.06481556]\n",
      " [-0.49009553]]\n",
      "Current iteration=8, loss=34892.89580481019\n",
      "t [[-0.06121274]\n",
      " [-0.77814537]\n",
      " [-0.84102335]\n",
      " ...\n",
      " [-0.55499624]\n",
      " [-0.09086446]\n",
      " [-0.52042364]]\n",
      "t [[-0.06121274]\n",
      " [-0.77814537]\n",
      " [-0.84102335]\n",
      " ...\n",
      " [-0.55499624]\n",
      " [-0.09086446]\n",
      " [-0.52042364]]\n",
      "t [[-0.06735119]\n",
      " [-0.84214615]\n",
      " [-0.88110614]\n",
      " ...\n",
      " [-0.57943968]\n",
      " [-0.1168587 ]\n",
      " [-0.54748182]]\n",
      "loss=34507.71960864751\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00944444]\n",
      " [-0.07599322]\n",
      " [-0.25735762]\n",
      " ...\n",
      " [-0.11330795]\n",
      " [-0.01364652]\n",
      " [-0.23116656]]\n",
      "t [[ 0.00944444]\n",
      " [-0.07599322]\n",
      " [-0.25735762]\n",
      " ...\n",
      " [-0.11330795]\n",
      " [-0.01364652]\n",
      " [-0.23116656]]\n",
      "t [[ 0.00299476]\n",
      " [-0.17806968]\n",
      " [-0.40372528]\n",
      " ...\n",
      " [-0.15548631]\n",
      " [-0.00374692]\n",
      " [-0.42073312]]\n",
      "t [[ 0.00299476]\n",
      " [-0.17806968]\n",
      " [-0.40372528]\n",
      " ...\n",
      " [-0.15548631]\n",
      " [-0.00374692]\n",
      " [-0.42073312]]\n",
      "Current iteration=2, loss=37433.576735958704\n",
      "t [[-0.00859931]\n",
      " [-0.28466833]\n",
      " [-0.50088635]\n",
      " ...\n",
      " [-0.16985587]\n",
      " [ 0.0122357 ]\n",
      " [-0.58227531]]\n",
      "t [[-0.00859931]\n",
      " [-0.28466833]\n",
      " [-0.50088635]\n",
      " ...\n",
      " [-0.16985587]\n",
      " [ 0.0122357 ]\n",
      " [-0.58227531]]\n",
      "t [[-0.0212503 ]\n",
      " [-0.38786188]\n",
      " [-0.57421463]\n",
      " ...\n",
      " [-0.17362702]\n",
      " [ 0.02795501]\n",
      " [-0.72327659]]\n",
      "t [[-0.0212503 ]\n",
      " [-0.38786188]\n",
      " [-0.57421463]\n",
      " ...\n",
      " [-0.17362702]\n",
      " [ 0.02795501]\n",
      " [-0.72327659]]\n",
      "Current iteration=4, loss=36147.885314696054\n",
      "t [[-0.03339531]\n",
      " [-0.48490236]\n",
      " [-0.63480958]\n",
      " ...\n",
      " [-0.17385784]\n",
      " [ 0.04129293]\n",
      " [-0.84831153]]\n",
      "t [[-0.03339531]\n",
      " [-0.48490236]\n",
      " [-0.63480958]\n",
      " ...\n",
      " [-0.17385784]\n",
      " [ 0.04129293]\n",
      " [-0.84831153]]\n",
      "t [[-0.04447258]\n",
      " [-0.57507912]\n",
      " [-0.68784914]\n",
      " ...\n",
      " [-0.1734653 ]\n",
      " [ 0.05177581]\n",
      " [-0.96045544]]\n",
      "t [[-0.04447258]\n",
      " [-0.57507912]\n",
      " [-0.68784914]\n",
      " ...\n",
      " [-0.1734653 ]\n",
      " [ 0.05177581]\n",
      " [-0.96045544]]\n",
      "Current iteration=6, loss=35343.53334132363\n",
      "t [[-0.05433883]\n",
      " [-0.65850739]\n",
      " [-0.73586115]\n",
      " ...\n",
      " [-0.17357756]\n",
      " [ 0.05956642]\n",
      " [-1.06191788]]\n",
      "t [[-0.05433883]\n",
      " [-0.65850739]\n",
      " [-0.73586115]\n",
      " ...\n",
      " [-0.17357756]\n",
      " [ 0.05956642]\n",
      " [-1.06191788]]\n",
      "t [[-0.06302402]\n",
      " [-0.73562013]\n",
      " [-0.78013481]\n",
      " ...\n",
      " [-0.17454032]\n",
      " [ 0.06504335]\n",
      " [-1.15436196]]\n",
      "t [[-0.06302402]\n",
      " [-0.73562013]\n",
      " [-0.78013481]\n",
      " ...\n",
      " [-0.17454032]\n",
      " [ 0.06504335]\n",
      " [-1.15436196]]\n",
      "Current iteration=8, loss=34795.65531545479\n",
      "t [[-0.07062366]\n",
      " [-0.80694743]\n",
      " [-0.82136613]\n",
      " ...\n",
      " [-0.17636743]\n",
      " [ 0.06862422]\n",
      " [-1.23908211]]\n",
      "t [[-0.07062366]\n",
      " [-0.80694743]\n",
      " [-0.82136613]\n",
      " ...\n",
      " [-0.17636743]\n",
      " [ 0.06862422]\n",
      " [-1.23908211]]\n",
      "t [[-0.07725135]\n",
      " [-0.87302195]\n",
      " [-0.85996215]\n",
      " ...\n",
      " [-0.17894522]\n",
      " [ 0.07069572]\n",
      " [-1.31711109]]\n",
      "loss=34402.72523188589\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10340608]\n",
      " [-0.44173892]\n",
      " [-0.27764881]\n",
      " ...\n",
      " [-0.20122656]\n",
      " [ 0.04356666]\n",
      " [-0.11730451]]\n",
      "t [[ 0.10340608]\n",
      " [-0.44173892]\n",
      " [-0.27764881]\n",
      " ...\n",
      " [-0.20122656]\n",
      " [ 0.04356666]\n",
      " [-0.11730451]]\n",
      "t [[ 0.15658696]\n",
      " [-0.71110292]\n",
      " [-0.44202324]\n",
      " ...\n",
      " [-0.30767818]\n",
      " [ 0.05308097]\n",
      " [-0.20852656]]\n",
      "t [[ 0.15658696]\n",
      " [-0.71110292]\n",
      " [-0.44202324]\n",
      " ...\n",
      " [-0.30767818]\n",
      " [ 0.05308097]\n",
      " [-0.20852656]]\n",
      "Current iteration=2, loss=37432.67209715998\n",
      "t [[ 0.18577047]\n",
      " [-0.89768923]\n",
      " [-0.5552527 ]\n",
      " ...\n",
      " [-0.37399086]\n",
      " [ 0.04599985]\n",
      " [-0.2833675 ]]\n",
      "t [[ 0.18577047]\n",
      " [-0.89768923]\n",
      " [-0.5552527 ]\n",
      " ...\n",
      " [-0.37399086]\n",
      " [ 0.04599985]\n",
      " [-0.2833675 ]]\n",
      "t [[ 0.20246782]\n",
      " [-1.03910876]\n",
      " [-0.64254961]\n",
      " ...\n",
      " [-0.42202969]\n",
      " [ 0.02992319]\n",
      " [-0.34665047]]\n",
      "t [[ 0.20246782]\n",
      " [-1.03910876]\n",
      " [-0.64254961]\n",
      " ...\n",
      " [-0.42202969]\n",
      " [ 0.02992319]\n",
      " [-0.34665047]]\n",
      "Current iteration=4, loss=36163.30623512171\n",
      "t [[ 0.21217391]\n",
      " [-1.1528185 ]\n",
      " [-0.71499505]\n",
      " ...\n",
      " [-0.46107986]\n",
      " [ 0.00864054]\n",
      " [-0.40113234]]\n",
      "t [[ 0.21217391]\n",
      " [-1.1528185 ]\n",
      " [-0.71499505]\n",
      " ...\n",
      " [-0.46107986]\n",
      " [ 0.00864054]\n",
      " [-0.40113234]]\n",
      "t [[ 0.21774106]\n",
      " [-1.24783409]\n",
      " [-0.77787841]\n",
      " ...\n",
      " [-0.49529683]\n",
      " [-0.01569561]\n",
      " [-0.44859693]]\n",
      "t [[ 0.21774106]\n",
      " [-1.24783409]\n",
      " [-0.77787841]\n",
      " ...\n",
      " [-0.49529683]\n",
      " [-0.01569561]\n",
      " [-0.44859693]]\n",
      "Current iteration=6, loss=35375.20686230573\n",
      "t [[ 0.2207492 ]\n",
      " [-1.32925086]\n",
      " [-0.83390546]\n",
      " ...\n",
      " [-0.52659079]\n",
      " [-0.0417511 ]\n",
      " [-0.49029986]]\n",
      "t [[ 0.2207492 ]\n",
      " [-1.32925086]\n",
      " [-0.83390546]\n",
      " ...\n",
      " [-0.52659079]\n",
      " [-0.0417511 ]\n",
      " [-0.49029986]]\n",
      "t [[ 0.22212313]\n",
      " [-1.40019546]\n",
      " [-0.88456633]\n",
      " ...\n",
      " [-0.55585443]\n",
      " [-0.06865167]\n",
      " [-0.52717654]]\n",
      "t [[ 0.22212313]\n",
      " [-1.40019546]\n",
      " [-0.88456633]\n",
      " ...\n",
      " [-0.55585443]\n",
      " [-0.06865167]\n",
      " [-0.52717654]]\n",
      "Current iteration=8, loss=34841.33993659163\n",
      "t [[ 0.22243133]\n",
      " [-1.46273689]\n",
      " [-0.93075609]\n",
      " ...\n",
      " [-0.58351379]\n",
      " [-0.09580491]\n",
      " [-0.55995157]]\n",
      "t [[ 0.22243133]\n",
      " [-1.46273689]\n",
      " [-0.93075609]\n",
      " ...\n",
      " [-0.58351379]\n",
      " [-0.09580491]\n",
      " [-0.55995157]]\n",
      "t [[ 0.22203836]\n",
      " [-1.51833692]\n",
      " [-0.97306673]\n",
      " ...\n",
      " [-0.60978154]\n",
      " [-0.12280244]\n",
      " [-0.589202  ]]\n",
      "loss=34459.73607402473\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01363654]\n",
      " [-0.08395832]\n",
      " [-0.2632181 ]\n",
      " ...\n",
      " [-0.20198786]\n",
      " [ 0.03926782]\n",
      " [-0.11982816]]\n",
      "t [[ 0.01363654]\n",
      " [-0.08395832]\n",
      " [-0.2632181 ]\n",
      " ...\n",
      " [-0.20198786]\n",
      " [ 0.03926782]\n",
      " [-0.11982816]]\n",
      "t [[ 0.01002201]\n",
      " [-0.19435194]\n",
      " [-0.41019304]\n",
      " ...\n",
      " [-0.30718084]\n",
      " [ 0.04530176]\n",
      " [-0.21298778]]\n",
      "t [[ 0.01002201]\n",
      " [-0.19435194]\n",
      " [-0.41019304]\n",
      " ...\n",
      " [-0.30718084]\n",
      " [ 0.04530176]\n",
      " [-0.21298778]]\n",
      "Current iteration=2, loss=37397.74371026848\n",
      "t [[ 0.00075202]\n",
      " [-0.3081247 ]\n",
      " [-0.50743755]\n",
      " ...\n",
      " [-0.37193461]\n",
      " [ 0.03559036]\n",
      " [-0.28947842]]\n",
      "t [[ 0.00075202]\n",
      " [-0.3081247 ]\n",
      " [-0.50743755]\n",
      " ...\n",
      " [-0.37193461]\n",
      " [ 0.03559036]\n",
      " [-0.28947842]]\n",
      "t [[-0.00989554]\n",
      " [-0.41727116]\n",
      " [-0.58117474]\n",
      " ...\n",
      " [-0.41847245]\n",
      " [ 0.01756512]\n",
      " [-0.35421767]]\n",
      "t [[-0.00989554]\n",
      " [-0.41727116]\n",
      " [-0.58117474]\n",
      " ...\n",
      " [-0.41847245]\n",
      " [ 0.01756512]\n",
      " [-0.35421767]]\n",
      "Current iteration=4, loss=36105.364973580436\n",
      "t [[-0.02030314]\n",
      " [-0.51923339]\n",
      " [-0.64251938]\n",
      " ...\n",
      " [-0.45613506]\n",
      " [-0.00514296]\n",
      " [-0.41000477]]\n",
      "t [[-0.02030314]\n",
      " [-0.51923339]\n",
      " [-0.64251938]\n",
      " ...\n",
      " [-0.45613506]\n",
      " [-0.00514296]\n",
      " [-0.41000477]]\n",
      "t [[-0.02988197]\n",
      " [-0.61350832]\n",
      " [-0.69651435]\n",
      " ...\n",
      " [-0.48906855]\n",
      " [-0.03050717]\n",
      " [-0.45864929]]\n",
      "t [[-0.02988197]\n",
      " [-0.61350832]\n",
      " [-0.69651435]\n",
      " ...\n",
      " [-0.48906855]\n",
      " [-0.03050717]\n",
      " [-0.45864929]]\n",
      "Current iteration=6, loss=35301.832622698705\n",
      "t [[-0.03846567]\n",
      " [-0.70038341]\n",
      " [-0.7455617 ]\n",
      " ...\n",
      " [-0.51916547]\n",
      " [-0.05728992]\n",
      " [-0.50142541]]\n",
      "t [[-0.03846567]\n",
      " [-0.70038341]\n",
      " [-0.7455617 ]\n",
      " ...\n",
      " [-0.51916547]\n",
      " [-0.05728992]\n",
      " [-0.50142541]]\n",
      "t [[-0.0460608 ]\n",
      " [-0.78042381]\n",
      " [-0.79086418]\n",
      " ...\n",
      " [-0.54730514]\n",
      " [-0.08469046]\n",
      " [-0.53928309]]\n",
      "t [[-0.0460608 ]\n",
      " [-0.78042381]\n",
      " [-0.79086418]\n",
      " ...\n",
      " [-0.54730514]\n",
      " [-0.08469046]\n",
      " [-0.53928309]]\n",
      "Current iteration=8, loss=34758.005899748656\n",
      "t [[-0.05273957]\n",
      " [-0.85425893]\n",
      " [-0.83306674]\n",
      " ...\n",
      " [-0.57390547]\n",
      " [-0.11217231]\n",
      " [-0.57295875]]\n",
      "t [[-0.05273957]\n",
      " [-0.85425893]\n",
      " [-0.83306674]\n",
      " ...\n",
      " [-0.57390547]\n",
      " [-0.11217231]\n",
      " [-0.57295875]]\n",
      "t [[-0.05859353]\n",
      " [-0.9224963 ]\n",
      " [-0.87255027]\n",
      " ...\n",
      " [-0.59917465]\n",
      " [-0.13936976]\n",
      " [-0.60303925]]\n",
      "loss=34369.89784037156\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01164711]\n",
      " [-0.07368685]\n",
      " [-0.26606884]\n",
      " ...\n",
      " [-0.19860998]\n",
      " [ 0.04222057]\n",
      " [-0.11239933]]\n",
      "t [[ 0.01164711]\n",
      " [-0.07368685]\n",
      " [-0.26606884]\n",
      " ...\n",
      " [-0.19860998]\n",
      " [ 0.04222057]\n",
      " [-0.11239933]]\n",
      "t [[ 0.00620899]\n",
      " [-0.17507332]\n",
      " [-0.41610004]\n",
      " ...\n",
      " [-0.30172947]\n",
      " [ 0.05085933]\n",
      " [-0.19903366]]\n",
      "t [[ 0.00620899]\n",
      " [-0.17507332]\n",
      " [-0.41610004]\n",
      " ...\n",
      " [-0.30172947]\n",
      " [ 0.05085933]\n",
      " [-0.19903366]]\n",
      "Current iteration=2, loss=37432.74528712909\n",
      "t [[-0.00465189]\n",
      " [-0.28097583]\n",
      " [-0.51602634]\n",
      " ...\n",
      " [-0.36486819]\n",
      " [ 0.04336606]\n",
      " [-0.26968537]]\n",
      "t [[-0.00465189]\n",
      " [-0.28097583]\n",
      " [-0.51602634]\n",
      " ...\n",
      " [-0.36486819]\n",
      " [ 0.04336606]\n",
      " [-0.26968537]]\n",
      "t [[-0.01663287]\n",
      " [-0.38318549]\n",
      " [-0.59204843]\n",
      " ...\n",
      " [-0.41004665]\n",
      " [ 0.02720385]\n",
      " [-0.32915299]]\n",
      "t [[-0.01663287]\n",
      " [-0.38318549]\n",
      " [-0.59204843]\n",
      " ...\n",
      " [-0.41004665]\n",
      " [ 0.02720385]\n",
      " [-0.32915299]]\n",
      "Current iteration=4, loss=36163.528195785955\n",
      "t [[-0.02812397]\n",
      " [-0.47898046]\n",
      " [-0.65534343]\n",
      " ...\n",
      " [-0.44654714]\n",
      " [ 0.00604228]\n",
      " [-0.38015503]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.02812397]\n",
      " [-0.47898046]\n",
      " [-0.65534343]\n",
      " ...\n",
      " [-0.44654714]\n",
      " [ 0.00604228]\n",
      " [-0.38015503]]\n",
      "t [[-0.03855947]\n",
      " [-0.56773181]\n",
      " [-0.71101713]\n",
      " ...\n",
      " [-0.4784914 ]\n",
      " [-0.01805282]\n",
      " [-0.42443922]]\n",
      "t [[-0.03855947]\n",
      " [-0.56773181]\n",
      " [-0.71101713]\n",
      " ...\n",
      " [-0.4784914 ]\n",
      " [-0.01805282]\n",
      " [-0.42443922]]\n",
      "Current iteration=6, loss=35375.899114149004\n",
      "t [[-0.04780142]\n",
      " [-0.64962888]\n",
      " [-0.76152251]\n",
      " ...\n",
      " [-0.50775495]\n",
      " [-0.04380547]\n",
      " [-0.46322938]]\n",
      "t [[-0.04780142]\n",
      " [-0.64962888]\n",
      " [-0.76152251]\n",
      " ...\n",
      " [-0.50775495]\n",
      " [-0.04380547]\n",
      " [-0.46322938]]\n",
      "t [[-0.05588503]\n",
      " [-0.72515883]\n",
      " [-0.80810169]\n",
      " ...\n",
      " [-0.53520107]\n",
      " [-0.07037855]\n",
      " [-0.49743329]]\n",
      "t [[-0.05588503]\n",
      " [-0.72515883]\n",
      " [-0.80810169]\n",
      " ...\n",
      " [-0.53520107]\n",
      " [-0.07037855]\n",
      " [-0.49743329]]\n",
      "Current iteration=8, loss=34842.85479181008\n",
      "t [[-0.06290899]\n",
      " [-0.79488801]\n",
      " [-0.85142989]\n",
      " ...\n",
      " [-0.5612317 ]\n",
      " [-0.09720237]\n",
      " [-0.52775145]]\n",
      "t [[-0.06290899]\n",
      " [-0.79488801]\n",
      " [-0.85142989]\n",
      " ...\n",
      " [-0.5612317 ]\n",
      " [-0.09720237]\n",
      " [-0.52775145]]\n",
      "t [[-0.06898832]\n",
      " [-0.85937225]\n",
      " [-0.89191155]\n",
      " ...\n",
      " [-0.58603948]\n",
      " [-0.12388175]\n",
      " [-0.55473982]]\n",
      "loss=34462.134996564426\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0096997 ]\n",
      " [-0.07804709]\n",
      " [-0.26431323]\n",
      " ...\n",
      " [-0.11637033]\n",
      " [-0.01401534]\n",
      " [-0.2374143 ]]\n",
      "t [[ 0.0096997 ]\n",
      " [-0.07804709]\n",
      " [-0.26431323]\n",
      " ...\n",
      " [-0.11637033]\n",
      " [-0.01401534]\n",
      " [-0.2374143 ]]\n",
      "t [[ 0.00265352]\n",
      " [-0.18357742]\n",
      " [-0.41165088]\n",
      " ...\n",
      " [-0.15777525]\n",
      " [-0.00321323]\n",
      " [-0.43097406]]\n",
      "t [[ 0.00265352]\n",
      " [-0.18357742]\n",
      " [-0.41165088]\n",
      " ...\n",
      " [-0.15777525]\n",
      " [-0.00321323]\n",
      " [-0.43097406]]\n",
      "Current iteration=2, loss=37383.65430052473\n",
      "t [[-0.00952768]\n",
      " [-0.29329478]\n",
      " [-0.50878288]\n",
      " ...\n",
      " [-0.17103371]\n",
      " [ 0.01353001]\n",
      " [-0.59536508]]\n",
      "t [[-0.00952768]\n",
      " [-0.29329478]\n",
      " [-0.50878288]\n",
      " ...\n",
      " [-0.17103371]\n",
      " [ 0.01353001]\n",
      " [-0.59536508]]\n",
      "t [[-0.02260576]\n",
      " [-0.39899867]\n",
      " [-0.58214954]\n",
      " ...\n",
      " [-0.17404227]\n",
      " [ 0.02965415]\n",
      " [-0.73850646]]\n",
      "t [[-0.02260576]\n",
      " [-0.39899867]\n",
      " [-0.58214954]\n",
      " ...\n",
      " [-0.17404227]\n",
      " [ 0.02965415]\n",
      " [-0.73850646]]\n",
      "Current iteration=4, loss=36091.65139031074\n",
      "t [[-0.03502338]\n",
      " [-0.49799146]\n",
      " [-0.64299715]\n",
      " ...\n",
      " [-0.17389441]\n",
      " [ 0.04309442]\n",
      " [-0.86518824]]\n",
      "t [[-0.03502338]\n",
      " [-0.49799146]\n",
      " [-0.64299715]\n",
      " ...\n",
      " [-0.17389441]\n",
      " [ 0.04309442]\n",
      " [-0.86518824]]\n",
      "t [[-0.04625403]\n",
      " [-0.58967003]\n",
      " [-0.69644431]\n",
      " ...\n",
      " [-0.17340768]\n",
      " [ 0.05347315]\n",
      " [-0.9786141 ]]\n",
      "t [[-0.04625403]\n",
      " [-0.58967003]\n",
      " [-0.69644431]\n",
      " ...\n",
      " [-0.17340768]\n",
      " [ 0.05347315]\n",
      " [-0.9786141 ]]\n",
      "Current iteration=6, loss=35288.48178208065\n",
      "t [[-0.05618845]\n",
      " [-0.67424941]\n",
      " [-0.74493489]\n",
      " ...\n",
      " [-0.17360628]\n",
      " [ 0.06103629]\n",
      " [-1.08107919]]\n",
      "t [[-0.05618845]\n",
      " [-0.67424941]\n",
      " [-0.74493489]\n",
      " ...\n",
      " [-0.17360628]\n",
      " [ 0.06103629]\n",
      " [-1.08107919]]\n",
      "t [[-0.06488276]\n",
      " [-0.7522414 ]\n",
      " [-0.78969367]\n",
      " ...\n",
      " [-0.17475693]\n",
      " [ 0.06622283]\n",
      " [-1.17430763]]\n",
      "t [[-0.06488276]\n",
      " [-0.7522414 ]\n",
      " [-0.78969367]\n",
      " ...\n",
      " [-0.17475693]\n",
      " [ 0.06622283]\n",
      " [-1.17430763]]\n",
      "Current iteration=8, loss=34744.59422639273\n",
      "t [[-0.07245093]\n",
      " [-0.8242352 ]\n",
      " [-0.83137626]\n",
      " ...\n",
      " [-0.17682037]\n",
      " [ 0.0694907 ]\n",
      " [-1.25963899]]\n",
      "t [[-0.07245093]\n",
      " [-0.8242352 ]\n",
      " [-0.83137626]\n",
      " ...\n",
      " [-0.17682037]\n",
      " [ 0.0694907 ]\n",
      " [-1.25963899]]\n",
      "t [[-0.07901932]\n",
      " [-0.89080738]\n",
      " [-0.87036805]\n",
      " ...\n",
      " [-0.17964958]\n",
      " [ 0.07125201]\n",
      " [-1.33814037]]\n",
      "loss=34356.233060129125\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10612729]\n",
      " [-0.45336363]\n",
      " [-0.28495536]\n",
      " ...\n",
      " [-0.206522  ]\n",
      " [ 0.04471315]\n",
      " [-0.12039147]]\n",
      "t [[ 0.10612729]\n",
      " [-0.45336363]\n",
      " [-0.28495536]\n",
      " ...\n",
      " [-0.206522  ]\n",
      " [ 0.04471315]\n",
      " [-0.12039147]]\n",
      "t [[ 0.15939265]\n",
      " [-0.72530716]\n",
      " [-0.4506944 ]\n",
      " ...\n",
      " [-0.31329526]\n",
      " [ 0.05359304]\n",
      " [-0.21332673]]\n",
      "t [[ 0.15939265]\n",
      " [-0.72530716]\n",
      " [-0.4506944 ]\n",
      " ...\n",
      " [-0.31329526]\n",
      " [ 0.05359304]\n",
      " [-0.21332673]]\n",
      "Current iteration=2, loss=37384.43439231336\n",
      "t [[ 0.18808806]\n",
      " [-0.91245244]\n",
      " [-0.56421575]\n",
      " ...\n",
      " [-0.37924502]\n",
      " [ 0.04545458]\n",
      " [-0.28927143]]\n",
      "t [[ 0.18808806]\n",
      " [-0.91245244]\n",
      " [-0.56421575]\n",
      " ...\n",
      " [-0.37924502]\n",
      " [ 0.04545458]\n",
      " [-0.28927143]]\n",
      "t [[ 0.20424056]\n",
      " [-1.05400604]\n",
      " [-0.65174936]\n",
      " ...\n",
      " [-0.42709744]\n",
      " [ 0.02823813]\n",
      " [-0.35329974]]\n",
      "t [[ 0.20424056]\n",
      " [-1.05400604]\n",
      " [-0.65174936]\n",
      " ...\n",
      " [-0.42709744]\n",
      " [ 0.02823813]\n",
      " [-0.35329974]]\n",
      "Current iteration=4, loss=36109.41521561704\n",
      "t [[ 0.21346815]\n",
      " [-1.16777198]\n",
      " [-0.72452692]\n",
      " ...\n",
      " [-0.46622358]\n",
      " [ 0.00584128]\n",
      " [-0.40828089]]\n",
      "t [[ 0.21346815]\n",
      " [-1.16777198]\n",
      " [-0.72452692]\n",
      " ...\n",
      " [-0.46622358]\n",
      " [ 0.00584128]\n",
      " [-0.40828089]]\n",
      "t [[ 0.2186403 ]\n",
      " [-1.26281467]\n",
      " [-0.78779937]\n",
      " ...\n",
      " [-0.50070232]\n",
      " [-0.01954047]\n",
      " [-0.45606369]]\n",
      "t [[ 0.2186403 ]\n",
      " [-1.26281467]\n",
      " [-0.78779937]\n",
      " ...\n",
      " [-0.50070232]\n",
      " [-0.01954047]\n",
      " [-0.45606369]]\n",
      "Current iteration=6, loss=35322.80691723179\n",
      "t [[ 0.22132714]\n",
      " [-1.34421652]\n",
      " [-0.84421204]\n",
      " ...\n",
      " [-0.53235633]\n",
      " [-0.04655391]\n",
      " [-0.49794738]]\n",
      "t [[ 0.22132714]\n",
      " [-1.34421652]\n",
      " [-0.84421204]\n",
      " ...\n",
      " [-0.53235633]\n",
      " [-0.04655391]\n",
      " [-0.49794738]]\n",
      "t [[ 0.22243983]\n",
      " [-1.41508993]\n",
      " [-0.8952113 ]\n",
      " ...\n",
      " [-0.5620137 ]\n",
      " [-0.07431682]\n",
      " [-0.53489896]]\n",
      "t [[ 0.22243983]\n",
      " [-1.41508993]\n",
      " [-0.8952113 ]\n",
      " ...\n",
      " [-0.5620137 ]\n",
      " [-0.07431682]\n",
      " [-0.53489896]]\n",
      "Current iteration=8, loss=34792.95333152519\n",
      "t [[ 0.22253522]\n",
      " [-1.47749928]\n",
      " [-0.94166838]\n",
      " ...\n",
      " [-0.59006004]\n",
      " [-0.10223499]\n",
      " [-0.56766688]]\n",
      "t [[ 0.22253522]\n",
      " [-1.47749928]\n",
      " [-0.94166838]\n",
      " ...\n",
      " [-0.59006004]\n",
      " [-0.10223499]\n",
      " [-0.56766688]]\n",
      "t [[ 0.221969  ]\n",
      " [-1.53290962]\n",
      " [-0.9841661 ]\n",
      " ...\n",
      " [-0.61668551]\n",
      " [-0.1299021 ]\n",
      " [-0.59684653]]\n",
      "loss=34415.79107816918\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01399539]\n",
      " [-0.08616775]\n",
      " [-0.27014489]\n",
      " ...\n",
      " [-0.20730333]\n",
      " [ 0.04030118]\n",
      " [-0.12298153]]\n",
      "t [[ 0.01399539]\n",
      " [-0.08616775]\n",
      " [-0.27014489]\n",
      " ...\n",
      " [-0.20730333]\n",
      " [ 0.04030118]\n",
      " [-0.12298153]]\n",
      "t [[ 0.0098409 ]\n",
      " [-0.20014982]\n",
      " [-0.41794975]\n",
      " ...\n",
      " [-0.31273348]\n",
      " [ 0.04563149]\n",
      " [-0.21789024]]\n",
      "t [[ 0.0098409 ]\n",
      " [-0.20014982]\n",
      " [-0.41794975]\n",
      " ...\n",
      " [-0.31273348]\n",
      " [ 0.04563149]\n",
      " [-0.21789024]]\n",
      "Current iteration=2, loss=37348.78931530878\n",
      "t [[ 3.31904262e-05]\n",
      " [-3.17088782e-01]\n",
      " [-5.15139504e-01]\n",
      " ...\n",
      " [-3.77066909e-01]\n",
      " [ 3.48381009e-02]\n",
      " [-2.95512945e-01]]\n",
      "t [[ 3.31904262e-05]\n",
      " [-3.17088782e-01]\n",
      " [-5.15139504e-01]\n",
      " ...\n",
      " [-3.77066909e-01]\n",
      " [ 3.48381009e-02]\n",
      " [-2.95512945e-01]]\n",
      "t [[-0.01100367]\n",
      " [-0.42874113]\n",
      " [-0.58894859]\n",
      " ...\n",
      " [-0.42338286]\n",
      " [ 0.01567562]\n",
      " [-0.36102068]]\n",
      "t [[-0.01100367]\n",
      " [-0.42874113]\n",
      " [-0.58894859]\n",
      " ...\n",
      " [-0.42338286]\n",
      " [ 0.01567562]\n",
      " [-0.36102068]]\n",
      "Current iteration=4, loss=36050.42366785055\n",
      "t [[-0.02165985]\n",
      " [-0.53262633]\n",
      " [-0.65059384]\n",
      " ...\n",
      " [-0.46109679]\n",
      " [-0.00812889]\n",
      " [-0.41732571]]\n",
      "t [[-0.02165985]\n",
      " [-0.53262633]\n",
      " [-0.65059384]\n",
      " ...\n",
      " [-0.46109679]\n",
      " [-0.00812889]\n",
      " [-0.41732571]]\n",
      "t [[-0.0313809 ]\n",
      " [-0.62836398]\n",
      " [-0.70503725]\n",
      " ...\n",
      " [-0.4942721 ]\n",
      " [-0.03451323]\n",
      " [-0.46630325]]\n",
      "t [[-0.0313809 ]\n",
      " [-0.62836398]\n",
      " [-0.70503725]\n",
      " ...\n",
      " [-0.4942721 ]\n",
      " [-0.03451323]\n",
      " [-0.46630325]]\n",
      "Current iteration=6, loss=35248.41346951995\n",
      "t [[-0.04003212]\n",
      " [-0.71634859]\n",
      " [-0.75458987]\n",
      " ...\n",
      " [-0.52471137]\n",
      " [-0.06222541]\n",
      " [-0.50927171]]\n",
      "t [[-0.04003212]\n",
      " [-0.71634859]\n",
      " [-0.75458987]\n",
      " ...\n",
      " [-0.52471137]\n",
      " [-0.06222541]\n",
      " [-0.50927171]]\n",
      "t [[-0.0476436 ]\n",
      " [-0.79722721]\n",
      " [-0.80039009]\n",
      " ...\n",
      " [-0.55322881]\n",
      " [-0.09045954]\n",
      " [-0.54721325]]\n",
      "t [[-0.0476436 ]\n",
      " [-0.79722721]\n",
      " [-0.80039009]\n",
      " ...\n",
      " [-0.55322881]\n",
      " [-0.09045954]\n",
      " [-0.54721325]]\n",
      "Current iteration=8, loss=34708.741998628364\n",
      "t [[-0.05430375]\n",
      " [-0.87168948]\n",
      " [-0.84304511]\n",
      " ...\n",
      " [-0.58020209]\n",
      " [-0.11867897]\n",
      " [-0.58088865]]\n",
      "t [[-0.05430375]\n",
      " [-0.87168948]\n",
      " [-0.84304511]\n",
      " ...\n",
      " [-0.58020209]\n",
      " [-0.11867897]\n",
      " [-0.58088865]]\n",
      "t [[-0.06011496]\n",
      " [-0.94038703]\n",
      " [-0.88291726]\n",
      " ...\n",
      " [-0.60581722]\n",
      " [-0.14652093]\n",
      " [-0.61090351]]\n",
      "loss=34325.233640157734\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01195361]\n",
      " [-0.07562598]\n",
      " [-0.27307065]\n",
      " ...\n",
      " [-0.20383656]\n",
      " [ 0.04333164]\n",
      " [-0.1153572 ]]\n",
      "t [[ 0.01195361]\n",
      " [-0.07562598]\n",
      " [-0.27307065]\n",
      " ...\n",
      " [-0.20383656]\n",
      " [ 0.04333164]\n",
      " [-0.1153572 ]]\n",
      "t [[ 0.00593158]\n",
      " [-0.18039867]\n",
      " [-0.42401567]\n",
      " ...\n",
      " [-0.307172  ]\n",
      " [ 0.05132501]\n",
      " [-0.20359304]]\n",
      "t [[ 0.00593158]\n",
      " [-0.18039867]\n",
      " [-0.42401567]\n",
      " ...\n",
      " [-0.307172  ]\n",
      " [ 0.05132501]\n",
      " [-0.20359304]]\n",
      "Current iteration=2, loss=37384.53830860591\n",
      "t [[-0.00549668]\n",
      " [-0.28932012]\n",
      " [-0.52393847]\n",
      " ...\n",
      " [-0.36987268]\n",
      " [ 0.04278739]\n",
      " [-0.27525935]]\n",
      "t [[-0.00549668]\n",
      " [-0.28932012]\n",
      " [-0.52393847]\n",
      " ...\n",
      " [-0.36987268]\n",
      " [ 0.04278739]\n",
      " [-0.27525935]]\n",
      "t [[-0.01788153]\n",
      " [-0.39392652]\n",
      " [-0.60006196]\n",
      " ...\n",
      " [-0.41481442]\n",
      " [ 0.02550872]\n",
      " [-0.33540141]]\n",
      "t [[-0.01788153]\n",
      " [-0.39392652]\n",
      " [-0.60006196]\n",
      " ...\n",
      " [-0.41481442]\n",
      " [ 0.02550872]\n",
      " [-0.33540141]]\n",
      "Current iteration=4, loss=36109.64244382808\n",
      "t [[-0.02962311]\n",
      " [-0.49156296]\n",
      " [-0.66367402]\n",
      " ...\n",
      " [-0.45135691]\n",
      " [ 0.00325786]\n",
      " [-0.38684654]]\n",
      "t [[-0.02962311]\n",
      " [-0.49156296]\n",
      " [-0.66367402]\n",
      " ...\n",
      " [-0.45135691]\n",
      " [ 0.00325786]\n",
      " [-0.38684654]]\n",
      "t [[-0.04019316]\n",
      " [-0.58171623]\n",
      " [-0.71980465]\n",
      " ...\n",
      " [-0.48354   ]\n",
      " [-0.02186073]\n",
      " [-0.43140479]]\n",
      "t [[-0.04019316]\n",
      " [-0.58171623]\n",
      " [-0.71980465]\n",
      " ...\n",
      " [-0.48354   ]\n",
      " [-0.02186073]\n",
      " [-0.43140479]]\n",
      "Current iteration=6, loss=35323.55177318241\n",
      "t [[-0.04948824]\n",
      " [-0.66467791]\n",
      " [-0.77081873]\n",
      " ...\n",
      " [-0.5131488 ]\n",
      " [-0.04855364]\n",
      " [-0.47034153]]\n",
      "t [[-0.04948824]\n",
      " [-0.66467791]\n",
      " [-0.77081873]\n",
      " ...\n",
      " [-0.5131488 ]\n",
      " [-0.04855364]\n",
      " [-0.47034153]]\n",
      "t [[-0.05756942]\n",
      " [-0.74101355]\n",
      " [-0.81789564]\n",
      " ...\n",
      " [-0.54098037]\n",
      " [-0.075976  ]\n",
      " [-0.50459447]]\n",
      "t [[-0.05756942]\n",
      " [-0.74101355]\n",
      " [-0.81789564]\n",
      " ...\n",
      " [-0.54098037]\n",
      " [-0.075976  ]\n",
      " [-0.50459447]]\n",
      "Current iteration=8, loss=34794.560660946765\n",
      "t [[-0.06455335]\n",
      " [-0.8113471 ]\n",
      " [-0.86167388]\n",
      " ...\n",
      " [-0.56739516]\n",
      " [-0.10355579]\n",
      " [-0.53488662]]\n",
      "t [[-0.06455335]\n",
      " [-0.8113471 ]\n",
      " [-0.86167388]\n",
      " ...\n",
      " [-0.56739516]\n",
      " [-0.10355579]\n",
      " [-0.53488662]]\n",
      "t [[-0.07056723]\n",
      " [-0.8762765 ]\n",
      " [-0.90253993]\n",
      " ...\n",
      " [-0.59256251]\n",
      " [-0.13089914]\n",
      " [-0.56179116]]\n",
      "loss=34418.304542630154\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.00995495]\n",
      " [-0.08010096]\n",
      " [-0.27126885]\n",
      " ...\n",
      " [-0.1194327 ]\n",
      " [-0.01438417]\n",
      " [-0.24366205]]\n",
      "t [[ 0.00995495]\n",
      " [-0.08010096]\n",
      " [-0.27126885]\n",
      " ...\n",
      " [-0.1194327 ]\n",
      " [-0.01438417]\n",
      " [-0.24366205]]\n",
      "t [[ 0.00229151]\n",
      " [-0.18911953]\n",
      " [-0.41942666]\n",
      " ...\n",
      " [-0.15996818]\n",
      " [-0.00264755]\n",
      " [-0.44115747]]\n",
      "t [[ 0.00229151]\n",
      " [-0.18911953]\n",
      " [-0.41942666]\n",
      " ...\n",
      " [-0.15996818]\n",
      " [-0.00264755]\n",
      " [-0.44115747]]\n",
      "Current iteration=2, loss=37334.642212550374\n",
      "t [[-0.01047656]\n",
      " [-0.30193884]\n",
      " [-0.51647654]\n",
      " ...\n",
      " [-0.17209704]\n",
      " [ 0.01484953]\n",
      " [-0.60833844]]\n",
      "t [[-0.01047656]\n",
      " [-0.30193884]\n",
      " [-0.51647654]\n",
      " ...\n",
      " [-0.17209704]\n",
      " [ 0.01484953]\n",
      " [-0.60833844]]\n",
      "t [[-0.02396961]\n",
      " [-0.41010644]\n",
      " [-0.5898852 ]\n",
      " ...\n",
      " [-0.17436897]\n",
      " [ 0.0313513 ]\n",
      " [-0.75356495]]\n",
      "t [[-0.02396961]\n",
      " [-0.41010644]\n",
      " [-0.5898852 ]\n",
      " ...\n",
      " [-0.17436897]\n",
      " [ 0.0313513 ]\n",
      " [-0.75356495]]\n",
      "Current iteration=4, loss=36036.74592681246\n",
      "t [[-0.03664393]\n",
      " [-0.51099387]\n",
      " [-0.65100707]\n",
      " ...\n",
      " [-0.17388181]\n",
      " [ 0.04486277]\n",
      " [-0.88184255]]\n",
      "t [[-0.03664393]\n",
      " [-0.51099387]\n",
      " [-0.65100707]\n",
      " ...\n",
      " [-0.17388181]\n",
      " [ 0.04486277]\n",
      " [-0.88184255]]\n",
      "t [[-0.04801256]\n",
      " [-0.60411591]\n",
      " [-0.70488158]\n",
      " ...\n",
      " [-0.17333754]\n",
      " [ 0.05511074]\n",
      " [-0.99650315]]\n",
      "t [[-0.04801256]\n",
      " [-0.60411591]\n",
      " [-0.70488158]\n",
      " ...\n",
      " [-0.17333754]\n",
      " [ 0.05511074]\n",
      " [-0.99650315]]\n",
      "Current iteration=6, loss=35235.07719915636\n",
      "t [[-0.05800193]\n",
      " [-0.68979185]\n",
      " [-0.7538614 ]\n",
      " ...\n",
      " [-0.17365027]\n",
      " [ 0.06242745]\n",
      " [-1.09992747]]\n",
      "t [[-0.05800193]\n",
      " [-0.68979185]\n",
      " [-0.7538614 ]\n",
      " ...\n",
      " [-0.17365027]\n",
      " [ 0.06242745]\n",
      " [-1.09992747]]\n",
      "t [[-0.06669465]\n",
      " [-0.76861372]\n",
      " [-0.79910601]\n",
      " ...\n",
      " [-0.17500691]\n",
      " [ 0.06731243]\n",
      " [-1.19390055]]\n",
      "t [[-0.06669465]\n",
      " [-0.76861372]\n",
      " [-0.79910601]\n",
      " ...\n",
      " [-0.17500691]\n",
      " [ 0.06731243]\n",
      " [-1.19390055]]\n",
      "Current iteration=8, loss=34695.3160586787\n",
      "t [[-0.07422298]\n",
      " [-0.84122986]\n",
      " [-0.84123237]\n",
      " ...\n",
      " [-0.17731639]\n",
      " [ 0.07026287]\n",
      " [-1.27980706]]\n",
      "t [[-0.07422298]\n",
      " [-0.84122986]\n",
      " [-0.84123237]\n",
      " ...\n",
      " [-0.17731639]\n",
      " [ 0.07026287]\n",
      " [-1.27980706]]\n",
      "t [[-0.08072558]\n",
      " [-0.90826019]\n",
      " [-0.88060648]\n",
      " ...\n",
      " [-0.18040023]\n",
      " [ 0.07171484]\n",
      " [-1.35874824]]\n",
      "loss=34311.53415055144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.10884851]\n",
      " [-0.46498834]\n",
      " [-0.29226191]\n",
      " ...\n",
      " [-0.21181744]\n",
      " [ 0.04585964]\n",
      " [-0.12347843]]\n",
      "t [[ 0.10884851]\n",
      " [-0.46498834]\n",
      " [-0.29226191]\n",
      " ...\n",
      " [-0.21181744]\n",
      " [ 0.04585964]\n",
      " [-0.12347843]]\n",
      "t [[ 0.16213403]\n",
      " [-0.73929116]\n",
      " [-0.45922107]\n",
      " ...\n",
      " [-0.31879118]\n",
      " [ 0.05406232]\n",
      " [-0.21809295]]\n",
      "t [[ 0.16213403]\n",
      " [-0.73929116]\n",
      " [-0.45922107]\n",
      " ...\n",
      " [-0.31879118]\n",
      " [ 0.05406232]\n",
      " [-0.21809295]]\n",
      "Current iteration=2, loss=37337.06535015649\n",
      "t [[ 0.19031248]\n",
      " [-0.9268914 ]\n",
      " [-0.57297863]\n",
      " ...\n",
      " [-0.38434222]\n",
      " [ 0.0448446 ]\n",
      " [-0.29511043]]\n",
      "t [[ 0.19031248]\n",
      " [-0.9268914 ]\n",
      " [-0.57297863]\n",
      " ...\n",
      " [-0.38434222]\n",
      " [ 0.0448446 ]\n",
      " [-0.29511043]]\n",
      "t [[ 0.2059156 ]\n",
      " [-1.06854439]\n",
      " [-0.66074307]\n",
      " ...\n",
      " [-0.43202027]\n",
      " [ 0.02648129]\n",
      " [-0.35985656]]\n",
      "t [[ 0.2059156 ]\n",
      " [-1.06854439]\n",
      " [-0.66074307]\n",
      " ...\n",
      " [-0.43202027]\n",
      " [ 0.02648129]\n",
      " [-0.35985656]]\n",
      "Current iteration=4, loss=36056.78623529858\n",
      "t [[ 0.21467143]\n",
      " [-1.18235775]\n",
      " [-0.73386174]\n",
      " ...\n",
      " [-0.47124837]\n",
      " [ 0.00297251]\n",
      " [-0.41531194]]\n",
      "t [[ 0.21467143]\n",
      " [-1.18235775]\n",
      " [-0.73386174]\n",
      " ...\n",
      " [-0.47124837]\n",
      " [ 0.00297251]\n",
      " [-0.41531194]]\n",
      "t [[ 0.21945921]\n",
      " [-1.27742351]\n",
      " [-0.79753018]\n",
      " ...\n",
      " [-0.50601205]\n",
      " [-0.02344618]\n",
      " [-0.46339   ]]\n",
      "t [[ 0.21945921]\n",
      " [-1.27742351]\n",
      " [-0.79753018]\n",
      " ...\n",
      " [-0.50601205]\n",
      " [-0.02344618]\n",
      " [-0.46339   ]]\n",
      "Current iteration=6, loss=35271.958011517316\n",
      "t [[ 0.22183597]\n",
      " [-1.35880378]\n",
      " [-0.85432761]\n",
      " ...\n",
      " [-0.53804069]\n",
      " [-0.05140416]\n",
      " [-0.50543353]]\n",
      "t [[ 0.22183597]\n",
      " [-1.35880378]\n",
      " [-0.85432761]\n",
      " ...\n",
      " [-0.53804069]\n",
      " [-0.05140416]\n",
      " [-0.50543353]]\n",
      "t [[ 0.22269805]\n",
      " [-1.42959568]\n",
      " [-0.90565651]\n",
      " ...\n",
      " [-0.5680974 ]\n",
      " [-0.08001233]\n",
      " [-0.54244122]]\n",
      "t [[ 0.22269805]\n",
      " [-1.42959568]\n",
      " [-0.90565651]\n",
      " ...\n",
      " [-0.5680974 ]\n",
      " [-0.08001233]\n",
      " [-0.54244122]]\n",
      "Current iteration=8, loss=34746.23102210515\n",
      "t [[ 0.22259042]\n",
      " [-1.49186024]\n",
      " [-0.95236612]\n",
      " ...\n",
      " [-0.59652899]\n",
      " [-0.10867558]\n",
      " [-0.57518538]]\n",
      "t [[ 0.22259042]\n",
      " [-1.49186024]\n",
      " [-0.95236612]\n",
      " ...\n",
      " [-0.59652899]\n",
      " [-0.10867558]\n",
      " [-0.57518538]]\n",
      "t [[ 0.22185991]\n",
      " [-1.54706739]\n",
      " [-0.99503229]\n",
      " ...\n",
      " [-0.62350518]\n",
      " [-0.13699039]\n",
      " [-0.60427971]]\n",
      "loss=34373.50897681771\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01435425]\n",
      " [-0.08837718]\n",
      " [-0.27707168]\n",
      " ...\n",
      " [-0.2126188 ]\n",
      " [ 0.04133455]\n",
      " [-0.1261349 ]]\n",
      "t [[ 0.01435425]\n",
      " [-0.08837718]\n",
      " [-0.27707168]\n",
      " ...\n",
      " [-0.2126188 ]\n",
      " [ 0.04133455]\n",
      " [-0.1261349 ]]\n",
      "t [[ 0.00963854]\n",
      " [-0.20598047]\n",
      " [-0.42555828]\n",
      " ...\n",
      " [-0.31816255]\n",
      " [ 0.0459196 ]\n",
      " [-0.22275801]]\n",
      "t [[ 0.00963854]\n",
      " [-0.20598047]\n",
      " [-0.42555828]\n",
      " ...\n",
      " [-0.31816255]\n",
      " [ 0.0459196 ]\n",
      " [-0.22275801]]\n",
      "Current iteration=2, loss=37300.7060435969\n",
      "t [[-0.00070713]\n",
      " [-0.32606519]\n",
      " [-0.52264676]\n",
      " ...\n",
      " [-0.38204102]\n",
      " [ 0.03402446]\n",
      " [-0.30148141]]\n",
      "t [[-0.00070713]\n",
      " [-0.32606519]\n",
      " [-0.52264676]\n",
      " ...\n",
      " [-0.38204102]\n",
      " [ 0.03402446]\n",
      " [-0.30148141]]\n",
      "t [[-0.01212241]\n",
      " [-0.44017415]\n",
      " [-0.59653557]\n",
      " ...\n",
      " [-0.42814872]\n",
      " [ 0.01371973]\n",
      " [-0.36772974]]\n",
      "t [[-0.01212241]\n",
      " [-0.44017415]\n",
      " [-0.59653557]\n",
      " ...\n",
      " [-0.42814872]\n",
      " [ 0.01371973]\n",
      " [-0.36772974]]\n",
      "Current iteration=4, loss=35996.76377374522\n",
      "t [[-0.02301312]\n",
      " [-0.54592371]\n",
      " [-0.65850391]\n",
      " ...\n",
      " [-0.46594108]\n",
      " [-0.01117738]\n",
      " [-0.42452724]]\n",
      "t [[-0.02301312]\n",
      " [-0.54592371]\n",
      " [-0.65850391]\n",
      " ...\n",
      " [-0.46594108]\n",
      " [-0.01117738]\n",
      " [-0.42452724]]\n",
      "t [[-0.03286302]\n",
      " [-0.64306601]\n",
      " [-0.7134141 ]\n",
      " ...\n",
      " [-0.49938198]\n",
      " [-0.03857224]\n",
      " [-0.47381436]]\n",
      "t [[-0.03286302]\n",
      " [-0.64306601]\n",
      " [-0.7134141 ]\n",
      " ...\n",
      " [-0.49938198]\n",
      " [-0.03857224]\n",
      " [-0.47381436]]\n",
      "Current iteration=6, loss=35196.57849091466\n",
      "t [[-0.04157048]\n",
      " [-0.73210662]\n",
      " [-0.76348028]\n",
      " ...\n",
      " [-0.53017852]\n",
      " [-0.0672    ]\n",
      " [-0.51695377]]\n",
      "t [[-0.04157048]\n",
      " [-0.73210662]\n",
      " [-0.76348028]\n",
      " ...\n",
      " [-0.53017852]\n",
      " [-0.0672    ]\n",
      " [-0.51695377]]\n",
      "t [[-0.0491893 ]\n",
      " [-0.81377547]\n",
      " [-0.80977646]\n",
      " ...\n",
      " [-0.55907964]\n",
      " [-0.09625062]\n",
      " [-0.55495992]]\n",
      "t [[-0.0491893 ]\n",
      " [-0.81377547]\n",
      " [-0.80977646]\n",
      " ...\n",
      " [-0.55907964]\n",
      " [-0.09625062]\n",
      " [-0.55495992]]\n",
      "Current iteration=8, loss=34661.1801407282\n",
      "t [[-0.05582376]\n",
      " [-0.88882215]\n",
      " [-0.85287436]\n",
      " ...\n",
      " [-0.58642453]\n",
      " [-0.12518807]\n",
      " [-0.58861797]]\n",
      "t [[-0.05582376]\n",
      " [-0.88882215]\n",
      " [-0.85287436]\n",
      " ...\n",
      " [-0.58642453]\n",
      " [-0.12518807]\n",
      " [-0.58861797]]\n",
      "t [[-0.06158675]\n",
      " [-0.9579418 ]\n",
      " [-0.89312018]\n",
      " ...\n",
      " [-0.61237915]\n",
      " [-0.1536532 ]\n",
      " [-0.61855225]]\n",
      "loss=34282.269976841446\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01226012]\n",
      " [-0.0775651 ]\n",
      " [-0.28007246]\n",
      " ...\n",
      " [-0.20906314]\n",
      " [ 0.0444427 ]\n",
      " [-0.11831508]]\n",
      "t [[ 0.01226012]\n",
      " [-0.0775651 ]\n",
      " [-0.28007246]\n",
      " ...\n",
      " [-0.20906314]\n",
      " [ 0.0444427 ]\n",
      " [-0.11831508]]\n",
      "t [[ 0.00563308]\n",
      " [-0.1857586 ]\n",
      " [-0.43178316]\n",
      " ...\n",
      " [-0.31249254]\n",
      " [ 0.05174849]\n",
      " [-0.20811896]]\n",
      "t [[ 0.00563308]\n",
      " [-0.1857586 ]\n",
      " [-0.43178316]\n",
      " ...\n",
      " [-0.31249254]\n",
      " [ 0.05174849]\n",
      " [-0.20811896]]\n",
      "Current iteration=2, loss=37337.19759807292\n",
      "t [[-0.00636205]\n",
      " [-0.29768123]\n",
      " [-0.53165445]\n",
      " ...\n",
      " [-0.37472086]\n",
      " [ 0.04214573]\n",
      " [-0.28076998]]\n",
      "t [[-0.00636205]\n",
      " [-0.29768123]\n",
      " [-0.53165445]\n",
      " ...\n",
      " [-0.37472086]\n",
      " [ 0.04214573]\n",
      " [-0.28076998]]\n",
      "t [[-0.01913874]\n",
      " [-0.40463794]\n",
      " [-0.60788558]\n",
      " ...\n",
      " [-0.4194398 ]\n",
      " [ 0.02374433]\n",
      " [-0.34156033]]\n",
      "t [[-0.01913874]\n",
      " [-0.40463794]\n",
      " [-0.60788558]\n",
      " ...\n",
      " [-0.4194398 ]\n",
      " [ 0.02374433]\n",
      " [-0.34156033]]\n",
      "Current iteration=4, loss=36057.01971299314\n",
      "t [[-3.11154474e-02]\n",
      " [-5.04059957e-01]\n",
      " [-6.71835885e-01]\n",
      " ...\n",
      " [-4.56051940e-01]\n",
      " [ 4.06638672e-04]\n",
      " [-3.93424987e-01]]\n",
      "t [[-3.11154474e-02]\n",
      " [-5.04059957e-01]\n",
      " [-6.71835885e-01]\n",
      " ...\n",
      " [-4.56051940e-01]\n",
      " [ 4.06638672e-04]\n",
      " [-3.93424987e-01]]\n",
      "t [[-0.04180545]\n",
      " [-0.59555967]\n",
      " [-0.72844078]\n",
      " ...\n",
      " [-0.48849844]\n",
      " [-0.02572713]\n",
      " [-0.43823586]]\n",
      "t [[-0.04180545]\n",
      " [-0.59555967]\n",
      " [-0.72844078]\n",
      " ...\n",
      " [-0.48849844]\n",
      " [-0.02572713]\n",
      " [-0.43823586]]\n",
      "Current iteration=6, loss=35272.75794688814\n",
      "t [[-0.05114136]\n",
      " [-0.67953484]\n",
      " [-0.77997104]\n",
      " ...\n",
      " [-0.5184683 ]\n",
      " [-0.05334759]\n",
      " [-0.47729978]]\n",
      "t [[-0.05114136]\n",
      " [-0.67953484]\n",
      " [-0.77997104]\n",
      " ...\n",
      " [-0.5184683 ]\n",
      " [-0.05334759]\n",
      " [-0.47729978]]\n",
      "t [[-0.05921032]\n",
      " [-0.75663036]\n",
      " [-0.8275433 ]\n",
      " ...\n",
      " [-0.54669206]\n",
      " [-0.08160295]\n",
      " [-0.51158444]]\n",
      "t [[-0.05921032]\n",
      " [-0.75663036]\n",
      " [-0.8275433 ]\n",
      " ...\n",
      " [-0.54669206]\n",
      " [-0.08160295]\n",
      " [-0.51158444]]\n",
      "Current iteration=8, loss=34747.93174731243\n",
      "t [[-0.06614665]\n",
      " [-0.82752751]\n",
      " [-0.87176148]\n",
      " ...\n",
      " [-0.57349039]\n",
      " [-0.10991967]\n",
      " [-0.54183534]]\n",
      "t [[-0.06614665]\n",
      " [-0.82752751]\n",
      " [-0.87176148]\n",
      " ...\n",
      " [-0.57349039]\n",
      " [-0.10991967]\n",
      " [-0.54183534]]\n",
      "t [[-0.0720893 ]\n",
      " [-0.89286575]\n",
      " [-0.91299656]\n",
      " ...\n",
      " [-0.59901137]\n",
      " [-0.13790588]\n",
      " [-0.56864284]]\n",
      "loss=34376.136159712325\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01021021]\n",
      " [-0.08215483]\n",
      " [-0.27822446]\n",
      " ...\n",
      " [-0.12249508]\n",
      " [-0.01475299]\n",
      " [-0.24990979]]\n",
      "t [[ 0.01021021]\n",
      " [-0.08215483]\n",
      " [-0.27822446]\n",
      " ...\n",
      " [-0.12249508]\n",
      " [-0.01475299]\n",
      " [-0.24990979]]\n",
      "t [[ 0.00190888]\n",
      " [-0.19469579]\n",
      " [-0.42705333]\n",
      " ...\n",
      " [-0.16206557]\n",
      " [-0.00205004]\n",
      " [-0.45128352]]\n",
      "t [[ 0.00190888]\n",
      " [-0.19469579]\n",
      " [-0.42705333]\n",
      " ...\n",
      " [-0.16206557]\n",
      " [-0.00205004]\n",
      " [-0.45128352]]\n",
      "Current iteration=2, loss=37286.5094193066\n",
      "t [[-0.01144486]\n",
      " [-0.31059841]\n",
      " [-0.52397374]\n",
      " ...\n",
      " [-0.17305028]\n",
      " [ 0.01619251]\n",
      " [-0.62119702]]\n",
      "t [[-0.01144486]\n",
      " [-0.31059841]\n",
      " [-0.52397374]\n",
      " ...\n",
      " [-0.17305028]\n",
      " [ 0.01619251]\n",
      " [-0.62119702]]\n",
      "t [[-0.02534021]\n",
      " [-0.42118209]\n",
      " [-0.59743199]\n",
      " ...\n",
      " [-0.1746141 ]\n",
      " [ 0.03304392]\n",
      " [-0.76845538]]\n",
      "t [[-0.02534021]\n",
      " [-0.42118209]\n",
      " [-0.59743199]\n",
      " ...\n",
      " [-0.1746141 ]\n",
      " [ 0.03304392]\n",
      " [-0.76845538]]\n",
      "Current iteration=4, loss=35983.12185994231\n",
      "t [[-0.03825538]\n",
      " [-0.52390693]\n",
      " [-0.65885096]\n",
      " ...\n",
      " [-0.17382736]\n",
      " [ 0.04659583]\n",
      " [-0.89827949]]\n",
      "t [[-0.03825538]\n",
      " [-0.52390693]\n",
      " [-0.65885096]\n",
      " ...\n",
      " [-0.17382736]\n",
      " [ 0.04659583]\n",
      " [-0.89827949]]\n",
      "t [[-0.04974704]\n",
      " [-0.61841549]\n",
      " [-0.71317183]\n",
      " ...\n",
      " [-0.1732609 ]\n",
      " [ 0.05668774]\n",
      " [-1.01412931]]\n",
      "t [[-0.04974704]\n",
      " [-0.61841549]\n",
      " [-0.71317183]\n",
      " ...\n",
      " [-0.1732609 ]\n",
      " [ 0.05668774]\n",
      " [-1.01412931]]\n",
      "Current iteration=6, loss=35183.254318925116\n",
      "t [[-0.05977882]\n",
      " [-0.70513542]\n",
      " [-0.76264991]\n",
      " ...\n",
      " [-0.17371351]\n",
      " [ 0.06374069]\n",
      " [-1.11847109]]\n",
      "t [[-0.05977882]\n",
      " [-0.70513542]\n",
      " [-0.76264991]\n",
      " ...\n",
      " [-0.17371351]\n",
      " [ 0.06374069]\n",
      " [-1.11847109]]\n",
      "t [[-0.06845997]\n",
      " [-0.78473999]\n",
      " [-0.8083793 ]\n",
      " ...\n",
      " [-0.17529206]\n",
      " [ 0.06831459]\n",
      " [-1.21315078]]\n",
      "t [[-0.06845997]\n",
      " [-0.78473999]\n",
      " [-0.8083793 ]\n",
      " ...\n",
      " [-0.17529206]\n",
      " [ 0.06831459]\n",
      " [-1.21315078]]\n",
      "Current iteration=8, loss=34647.737853250845\n",
      "t [[-0.07594075]\n",
      " [-0.85793646]\n",
      " [-0.85094047]\n",
      " ...\n",
      " [-0.17785536]\n",
      " [ 0.07094457]\n",
      " [-1.29959802]]\n",
      "t [[-0.07594075]\n",
      " [-0.85793646]\n",
      " [-0.85094047]\n",
      " ...\n",
      " [-0.17785536]\n",
      " [ 0.07094457]\n",
      " [-1.29959802]]\n",
      "t [[-0.08237167]\n",
      " [-0.9253875 ]\n",
      " [-0.89068251]\n",
      " ...\n",
      " [-0.18119552]\n",
      " [ 0.07208923]\n",
      " [-1.37894799]]\n",
      "loss=34268.53463818882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11156972]\n",
      " [-0.47661304]\n",
      " [-0.29956846]\n",
      " ...\n",
      " [-0.21711287]\n",
      " [ 0.04700613]\n",
      " [-0.12656539]]\n",
      "t [[ 0.11156972]\n",
      " [-0.47661304]\n",
      " [-0.29956846]\n",
      " ...\n",
      " [-0.21711287]\n",
      " [ 0.04700613]\n",
      " [-0.12656539]]\n",
      "t [[ 0.1648114 ]\n",
      " [-0.75305594]\n",
      " [-0.46760392]\n",
      " ...\n",
      " [-0.32416648]\n",
      " [ 0.05448907]\n",
      " [-0.22282534]]\n",
      "t [[ 0.1648114 ]\n",
      " [-0.75305594]\n",
      " [-0.46760392]\n",
      " ...\n",
      " [-0.32416648]\n",
      " [ 0.05448907]\n",
      " [-0.22282534]]\n",
      "Current iteration=2, loss=37290.53566504514\n",
      "t [[ 0.19244633]\n",
      " [-0.94101471]\n",
      " [-0.58154723]\n",
      " ...\n",
      " [-0.38928766]\n",
      " [ 0.04417153]\n",
      " [-0.30088546]]\n",
      "t [[ 0.19244633]\n",
      " [-0.94101471]\n",
      " [-0.58154723]\n",
      " ...\n",
      " [-0.38928766]\n",
      " [ 0.04417153]\n",
      " [-0.30088546]]\n",
      "t [[ 0.20749737]\n",
      " [-1.08273814]\n",
      " [-0.66954031]\n",
      " ...\n",
      " [-0.43680643]\n",
      " [ 0.02465554]\n",
      " [-0.36632285]]\n",
      "t [[ 0.20749737]\n",
      " [-1.08273814]\n",
      " [-0.66954031]\n",
      " ...\n",
      " [-0.43680643]\n",
      " [ 0.02465554]\n",
      " [-0.36632285]]\n",
      "Current iteration=4, loss=36005.37542197265\n",
      "t [[ 2.15789071e-01]\n",
      " [-1.19659289e+00]\n",
      " [-7.43010327e-01]\n",
      " ...\n",
      " [-4.76163189e-01]\n",
      " [ 3.79613457e-05]\n",
      " [-4.22228279e-01]]\n",
      "t [[ 2.15789071e-01]\n",
      " [-1.19659289e+00]\n",
      " [-7.43010327e-01]\n",
      " ...\n",
      " [-4.76163189e-01]\n",
      " [ 3.79613457e-05]\n",
      " [-4.22228279e-01]]\n",
      "t [[ 0.2202033 ]\n",
      " [-1.2916784 ]\n",
      " [-0.8070812 ]\n",
      " ...\n",
      " [-0.51123406]\n",
      " [-0.02740846]\n",
      " [-0.47057946]]\n",
      "t [[ 0.2202033 ]\n",
      " [-1.2916784 ]\n",
      " [-0.8070812 ]\n",
      " ...\n",
      " [-0.51123406]\n",
      " [-0.02740846]\n",
      " [-0.47057946]]\n",
      "Current iteration=6, loss=35222.59914545053\n",
      "t [[ 0.22228101]\n",
      " [-1.37303018]\n",
      " [-0.8642614 ]\n",
      " ...\n",
      " [-0.54365028]\n",
      " [-0.05629722]\n",
      " [-0.51276276]]\n",
      "t [[ 0.22228101]\n",
      " [-1.37303018]\n",
      " [-0.8642614 ]\n",
      " ...\n",
      " [-0.54365028]\n",
      " [-0.05629722]\n",
      " [-0.51276276]]\n",
      "t [[ 0.22290275]\n",
      " [-1.44372986]\n",
      " [-0.91591008]\n",
      " ...\n",
      " [-0.57411025]\n",
      " [-0.0857334 ]\n",
      " [-0.5498086 ]]\n",
      "t [[ 0.22290275]\n",
      " [-1.44372986]\n",
      " [-0.91591008]\n",
      " ...\n",
      " [-0.57411025]\n",
      " [-0.0857334 ]\n",
      " [-0.5498086 ]]\n",
      "Current iteration=8, loss=34701.09623592215\n",
      "t [[ 0.22260145]\n",
      " [-1.50583676]\n",
      " [-0.96285665]\n",
      " ...\n",
      " [-0.60292399]\n",
      " [-0.11512182]\n",
      " [-0.58251319]]\n",
      "t [[ 0.22260145]\n",
      " [-1.50583676]\n",
      " [-0.96285665]\n",
      " ...\n",
      " [-0.60292399]\n",
      " [-0.11512182]\n",
      " [-0.58251319]]\n",
      "t [[ 0.22171519]\n",
      " [-1.56082737]\n",
      " [-1.00567224]\n",
      " ...\n",
      " [-0.63024295]\n",
      " [-0.14406256]\n",
      " [-0.61150847]]\n",
      "loss=34332.80372141724\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0147131 ]\n",
      " [-0.09058661]\n",
      " [-0.28399847]\n",
      " ...\n",
      " [-0.21793427]\n",
      " [ 0.04236791]\n",
      " [-0.12928828]]\n",
      "t [[ 0.0147131 ]\n",
      " [-0.09058661]\n",
      " [-0.28399847]\n",
      " ...\n",
      " [-0.21793427]\n",
      " [ 0.04236791]\n",
      " [-0.12928828]]\n",
      "t [[ 0.00941506]\n",
      " [-0.21184369]\n",
      " [-0.43301934]\n",
      " ...\n",
      " [-0.32346864]\n",
      " [ 0.04616632]\n",
      " [-0.22759124]]\n",
      "t [[ 0.00941506]\n",
      " [-0.21184369]\n",
      " [-0.43301934]\n",
      " ...\n",
      " [-0.32346864]\n",
      " [ 0.04616632]\n",
      " [-0.22759124]]\n",
      "Current iteration=2, loss=37253.46436863381\n",
      "t [[-0.00146787]\n",
      " [-0.33505188]\n",
      " [-0.52996559]\n",
      " ...\n",
      " [-0.38686227]\n",
      " [ 0.03315106]\n",
      " [-0.30738482]]\n",
      "t [[-0.00146787]\n",
      " [-0.33505188]\n",
      " [-0.52996559]\n",
      " ...\n",
      " [-0.38686227]\n",
      " [ 0.03315106]\n",
      " [-0.30738482]]\n",
      "t [[-0.01325019]\n",
      " [-0.45156738]\n",
      " [-0.60394561]\n",
      " ...\n",
      " [-0.43277844]\n",
      " [ 0.01170023]\n",
      " [-0.37434682]]\n",
      "t [[-0.01325019]\n",
      " [-0.45156738]\n",
      " [-0.60394561]\n",
      " ...\n",
      " [-0.43277844]\n",
      " [ 0.01170023]\n",
      " [-0.37434682]]\n",
      "Current iteration=4, loss=35944.34143525628\n",
      "t [[-0.0243614 ]\n",
      " [-0.55912322]\n",
      " [-0.66626039]\n",
      " ...\n",
      " [-0.47067696]\n",
      " [-0.01428489]\n",
      " [-0.43161218]]\n",
      "t [[-0.0243614 ]\n",
      " [-0.55912322]\n",
      " [-0.66626039]\n",
      " ...\n",
      " [-0.47067696]\n",
      " [-0.01428489]\n",
      " [-0.43161218]]\n",
      "t [[-0.03432721]\n",
      " [-0.65761365]\n",
      " [-0.72165474]\n",
      " ...\n",
      " [-0.50440621]\n",
      " [-0.04268016]\n",
      " [-0.48118628]]\n",
      "t [[-0.03432721]\n",
      " [-0.65761365]\n",
      " [-0.72165474]\n",
      " ...\n",
      " [-0.50440621]\n",
      " [-0.04268016]\n",
      " [-0.48118628]]\n",
      "Current iteration=6, loss=35146.265727924256\n",
      "t [[-0.04308023]\n",
      " [-0.74765876]\n",
      " [-0.77224103]\n",
      " ...\n",
      " [-0.53557327]\n",
      " [-0.07220939]\n",
      " [-0.52447609]]\n",
      "t [[-0.04308023]\n",
      " [-0.74765876]\n",
      " [-0.77224103]\n",
      " ...\n",
      " [-0.53557327]\n",
      " [-0.07220939]\n",
      " [-0.52447609]]\n",
      "t [[-0.05069798]\n",
      " [-0.83007198]\n",
      " [-0.81902976]\n",
      " ...\n",
      " [-0.56486229]\n",
      " [-0.10205929]\n",
      " [-0.56252844]]\n",
      "t [[-0.05069798]\n",
      " [-0.83007198]\n",
      " [-0.81902976]\n",
      " ...\n",
      " [-0.56486229]\n",
      " [-0.10205929]\n",
      " [-0.56252844]]\n",
      "Current iteration=8, loss=34615.24190274712\n",
      "t [[-0.05730028]\n",
      " [-0.90566243]\n",
      " [-0.86255966]\n",
      " ...\n",
      " [-0.59257611]\n",
      " [-0.1316952 ]\n",
      " [-0.59615292]]\n",
      "t [[-0.05730028]\n",
      " [-0.90566243]\n",
      " [-0.86255966]\n",
      " ...\n",
      " [-0.59257611]\n",
      " [-0.1316952 ]\n",
      " [-0.59615292]]\n",
      "t [[-0.06301005]\n",
      " [-0.97516805]\n",
      " [-0.90316342]\n",
      " ...\n",
      " [-0.61886278]\n",
      " [-0.16076233]\n",
      " [-0.62599252]]\n",
      "loss=34240.91879417167\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01256662]\n",
      " [-0.07950423]\n",
      " [-0.28707427]\n",
      " ...\n",
      " [-0.21428971]\n",
      " [ 0.04555377]\n",
      " [-0.12127296]]\n",
      "t [[ 0.01256662]\n",
      " [-0.07950423]\n",
      " [-0.28707427]\n",
      " ...\n",
      " [-0.21428971]\n",
      " [ 0.04555377]\n",
      " [-0.12127296]]\n",
      "t [[ 0.00531364]\n",
      " [-0.19115292]\n",
      " [-0.43940319]\n",
      " ...\n",
      " [-0.31769166]\n",
      " [ 0.05213   ]\n",
      " [-0.21261153]]\n",
      "t [[ 0.00531364]\n",
      " [-0.19115292]\n",
      " [-0.43940319]\n",
      " ...\n",
      " [-0.31769166]\n",
      " [ 0.05213   ]\n",
      " [-0.21261153]]\n",
      "Current iteration=2, loss=37290.693761042836\n",
      "t [[-0.00724694]\n",
      " [-0.30605709]\n",
      " [-0.53918054]\n",
      " ...\n",
      " [-0.379418  ]\n",
      " [ 0.04144272]\n",
      " [-0.28621824]]\n",
      "t [[-0.00724694]\n",
      " [-0.30605709]\n",
      " [-0.53918054]\n",
      " ...\n",
      " [-0.379418  ]\n",
      " [ 0.04144272]\n",
      " [-0.28621824]]\n",
      "t [[-0.02040293]\n",
      " [-0.41531679]\n",
      " [-0.61552924]\n",
      " ...\n",
      " [-0.4239311 ]\n",
      " [ 0.0219135 ]\n",
      " [-0.34763167]]\n",
      "t [[-0.02040293]\n",
      " [-0.41531679]\n",
      " [-0.61552924]\n",
      " ...\n",
      " [-0.4239311 ]\n",
      " [ 0.0219135 ]\n",
      " [-0.34763167]]\n",
      "Current iteration=4, loss=36005.616323108785\n",
      "t [[-0.03259946]\n",
      " [-0.51646899]\n",
      " [-0.67983989]\n",
      " ...\n",
      " [-0.4606412 ]\n",
      " [-0.00250779]\n",
      " [-0.39989312]]\n",
      "t [[-0.03259946]\n",
      " [-0.51646899]\n",
      " [-0.67983989]\n",
      " ...\n",
      " [-0.4606412 ]\n",
      " [-0.00250779]\n",
      " [-0.39989312]]\n",
      "t [[-0.04339529]\n",
      " [-0.60926113]\n",
      " [-0.73693548]\n",
      " ...\n",
      " [-0.49337468]\n",
      " [-0.02964793]\n",
      " [-0.44493597]]\n",
      "t [[-0.04339529]\n",
      " [-0.60926113]\n",
      " [-0.73693548]\n",
      " ...\n",
      " [-0.49337468]\n",
      " [-0.02964793]\n",
      " [-0.44493597]]\n",
      "Current iteration=6, loss=35223.456484481176\n",
      "t [[-0.05276038]\n",
      " [-0.69420058]\n",
      " [-0.78898775]\n",
      " ...\n",
      " [-0.52371971]\n",
      " [-0.05818287]\n",
      " [-0.48410846]]\n",
      "t [[-0.05276038]\n",
      " [-0.69420058]\n",
      " [-0.78898775]\n",
      " ...\n",
      " [-0.52371971]\n",
      " [-0.05818287]\n",
      " [-0.48410846]]\n",
      "t [[-0.06080799]\n",
      " [-0.77201221]\n",
      " [-0.83705135]\n",
      " ...\n",
      " [-0.55234069]\n",
      " [-0.0872548 ]\n",
      " [-0.51840831]]\n",
      "t [[-0.06080799]\n",
      " [-0.77201221]\n",
      " [-0.83705135]\n",
      " ...\n",
      " [-0.55234069]\n",
      " [-0.0872548 ]\n",
      " [-0.51840831]]\n",
      "Current iteration=8, loss=34702.89105253106\n",
      "t [[-0.06768976]\n",
      " [-0.8434342 ]\n",
      " [-0.88169813]\n",
      " ...\n",
      " [-0.57952054]\n",
      " [-0.11628935]\n",
      " [-0.54860352]]\n",
      "t [[-0.06768976]\n",
      " [-0.8434342 ]\n",
      " [-0.88169813]\n",
      " ...\n",
      " [-0.57952054]\n",
      " [-0.11628935]\n",
      " [-0.54860352]]\n",
      "t [[-0.07355596]\n",
      " [-0.90914684]\n",
      " [-0.92328613]\n",
      " ...\n",
      " [-0.60538821]\n",
      " [-0.14489737]\n",
      " [-0.57530156]]\n",
      "loss=34335.54365285422\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01046546]\n",
      " [-0.0842087 ]\n",
      " [-0.28518007]\n",
      " ...\n",
      " [-0.12555746]\n",
      " [-0.01512182]\n",
      " [-0.25615754]]\n",
      "t [[ 0.01046546]\n",
      " [-0.0842087 ]\n",
      " [-0.28518007]\n",
      " ...\n",
      " [-0.12555746]\n",
      " [-0.01512182]\n",
      " [-0.25615754]]\n",
      "t [[ 0.00150576]\n",
      " [-0.20030598]\n",
      " [-0.43453159]\n",
      " ...\n",
      " [-0.16406786]\n",
      " [-0.00142082]\n",
      " [-0.46135242]]\n",
      "t [[ 0.00150576]\n",
      " [-0.20030598]\n",
      " [-0.43453159]\n",
      " ...\n",
      " [-0.16406786]\n",
      " [-0.00142082]\n",
      " [-0.46135242]]\n",
      "Current iteration=2, loss=37239.22594346066\n",
      "t [[-0.01243154]\n",
      " [-0.31927139]\n",
      " [-0.53128077]\n",
      " ...\n",
      " [-0.17389781]\n",
      " [ 0.01755722]\n",
      " [-0.6339424 ]]\n",
      "t [[-0.01243154]\n",
      " [-0.31927139]\n",
      " [-0.53128077]\n",
      " ...\n",
      " [-0.17389781]\n",
      " [ 0.01755722]\n",
      " [-0.6339424 ]]\n",
      "t [[-0.02671603]\n",
      " [-0.4322227 ]\n",
      " [-0.60479993]\n",
      " ...\n",
      " [-0.17478437]\n",
      " [ 0.0347296 ]\n",
      " [-0.783181  ]]\n",
      "t [[-0.02671603]\n",
      " [-0.4322227 ]\n",
      " [-0.60479993]\n",
      " ...\n",
      " [-0.17478437]\n",
      " [ 0.0347296 ]\n",
      " [-0.783181  ]]\n",
      "Current iteration=4, loss=35930.7350355506\n",
      "t [[-0.03985627]\n",
      " [-0.53672819]\n",
      " [-0.66653973]\n",
      " ...\n",
      " [-0.17373786]\n",
      " [ 0.04829171]\n",
      " [-0.91450393]]\n",
      "t [[-0.03985627]\n",
      " [-0.53672819]\n",
      " [-0.66653973]\n",
      " ...\n",
      " [-0.17373786]\n",
      " [ 0.04829171]\n",
      " [-0.91450393]]\n",
      "t [[-0.0514565 ]\n",
      " [-0.63256783]\n",
      " [-0.72132502]\n",
      " ...\n",
      " [-0.17318312]\n",
      " [ 0.05820351]\n",
      " [-1.03149904]]\n",
      "t [[-0.0514565 ]\n",
      " [-0.63256783]\n",
      " [-0.72132502]\n",
      " ...\n",
      " [-0.17318312]\n",
      " [ 0.05820351]\n",
      " [-1.03149904]]\n",
      "Current iteration=6, loss=35132.95125673851\n",
      "t [[-0.06151879]\n",
      " [-0.72028114]\n",
      " [-0.77130867]\n",
      " ...\n",
      " [-0.17379929]\n",
      " [ 0.06497698]\n",
      " [-1.1367181 ]]\n",
      "t [[-0.06151879]\n",
      " [-0.72028114]\n",
      " [-0.77130867]\n",
      " ...\n",
      " [-0.17379929]\n",
      " [ 0.06497698]\n",
      " [-1.1367181 ]]\n",
      "t [[-0.0701791 ]\n",
      " [-0.80062332]\n",
      " [-0.81752013]\n",
      " ...\n",
      " [-0.17561363]\n",
      " [ 0.06923183]\n",
      " [-1.23206793]]\n",
      "t [[-0.0701791 ]\n",
      " [-0.80062332]\n",
      " [-0.81752013]\n",
      " ...\n",
      " [-0.17561363]\n",
      " [ 0.06923183]\n",
      " [-1.23206793]]\n",
      "Current iteration=8, loss=34601.78136125586\n",
      "t [[-0.07760523]\n",
      " [-0.87436014]\n",
      " [-0.86050583]\n",
      " ...\n",
      " [-0.17843678]\n",
      " [ 0.07153968]\n",
      " [-1.31902301]]\n",
      "t [[-0.07760523]\n",
      " [-0.87436014]\n",
      " [-0.86050583]\n",
      " ...\n",
      " [-0.17843678]\n",
      " [ 0.07153968]\n",
      " [-1.31902301]]\n",
      "t [[-0.08395915]\n",
      " [-0.94219639]\n",
      " [-0.90060056]\n",
      " ...\n",
      " [-0.18203356]\n",
      " [ 0.07238009]\n",
      " [-1.39875228]]\n",
      "loss=34227.146630929805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11429093]\n",
      " [-0.48823775]\n",
      " [-0.306875  ]\n",
      " ...\n",
      " [-0.22240831]\n",
      " [ 0.04815262]\n",
      " [-0.12965235]]\n",
      "t [[ 0.11429093]\n",
      " [-0.48823775]\n",
      " [-0.306875  ]\n",
      " ...\n",
      " [-0.22240831]\n",
      " [ 0.04815262]\n",
      " [-0.12965235]]\n",
      "t [[ 0.16742505]\n",
      " [-0.76660256]\n",
      " [-0.47584365]\n",
      " ...\n",
      " [-0.32942175]\n",
      " [ 0.05487351]\n",
      " [-0.22752401]]\n",
      "t [[ 0.16742505]\n",
      " [-0.76660256]\n",
      " [-0.47584365]\n",
      " ...\n",
      " [-0.32942175]\n",
      " [ 0.05487351]\n",
      " [-0.22752401]]\n",
      "Current iteration=2, loss=37244.8170461148\n",
      "t [[ 0.19449216]\n",
      " [-0.95483084]\n",
      " [-0.58992742]\n",
      " ...\n",
      " [-0.39408645]\n",
      " [ 0.04343699]\n",
      " [-0.30659752]]\n",
      "t [[ 0.19449216]\n",
      " [-0.95483084]\n",
      " [-0.58992742]\n",
      " ...\n",
      " [-0.39408645]\n",
      " [ 0.04343699]\n",
      " [-0.30659752]]\n",
      "t [[ 0.20899015]\n",
      " [-1.09660114]\n",
      " [-0.67815031]\n",
      " ...\n",
      " [-0.44146389]\n",
      " [ 0.02276361]\n",
      " [-0.37270047]]\n",
      "t [[ 0.20899015]\n",
      " [-1.09660114]\n",
      " [-0.67815031]\n",
      " ...\n",
      " [-0.44146389]\n",
      " [ 0.02276361]\n",
      " [-0.37270047]]\n",
      "Current iteration=4, loss=35955.14153247717\n",
      "t [[ 0.21682614]\n",
      " [-1.21049357]\n",
      " [-0.75198283]\n",
      " ...\n",
      " [-0.48097645]\n",
      " [-0.00295884]\n",
      " [-0.42903261]]\n",
      "t [[ 0.21682614]\n",
      " [-1.21049357]\n",
      " [-0.75198283]\n",
      " ...\n",
      " [-0.48097645]\n",
      " [-0.00295884]\n",
      " [-0.42903261]]\n",
      "t [[ 0.22087775]\n",
      " [-1.30559589]\n",
      " [-0.81646197]\n",
      " ...\n",
      " [-0.51637568]\n",
      " [-0.03142326]\n",
      " [-0.47763557]]\n",
      "t [[ 0.22087775]\n",
      " [-1.30559589]\n",
      " [-0.81646197]\n",
      " ...\n",
      " [-0.51637568]\n",
      " [-0.03142326]\n",
      " [-0.47763557]]\n",
      "Current iteration=6, loss=35174.67243104572\n",
      "t [[ 0.22266718]\n",
      " [-1.38691192]\n",
      " [-0.87402182]\n",
      " ...\n",
      " [-0.54919077]\n",
      " [-0.06122874]\n",
      " [-0.51993933]]\n",
      "t [[ 0.22266718]\n",
      " [-1.38691192]\n",
      " [-0.87402182]\n",
      " ...\n",
      " [-0.54919077]\n",
      " [-0.06122874]\n",
      " [-0.51993933]]\n",
      "t [[ 0.22305844]\n",
      " [-1.4575083 ]\n",
      " [-0.92597932]\n",
      " ...\n",
      " [-0.58005633]\n",
      " [-0.09147551]\n",
      " [-0.55700615]]\n",
      "t [[ 0.22305844]\n",
      " [-1.4575083 ]\n",
      " [-0.92597932]\n",
      " ...\n",
      " [-0.58005633]\n",
      " [-0.09147551]\n",
      " [-0.55700615]]\n",
      "Current iteration=8, loss=34657.47652641185\n",
      "t [[ 0.22257242]\n",
      " [-1.51944455]\n",
      " [-0.97314656]\n",
      " ...\n",
      " [-0.60924787]\n",
      " [-0.12156919]\n",
      " [-0.58965614]]\n",
      "t [[ 0.22257242]\n",
      " [-1.51944455]\n",
      " [-0.97314656]\n",
      " ...\n",
      " [-0.60924787]\n",
      " [-0.12156919]\n",
      " [-0.58965614]]\n",
      "t [[ 0.22153857]\n",
      " [-1.57420553]\n",
      " [-1.01609229]\n",
      " ...\n",
      " [-0.63690081]\n",
      " [-0.15111423]\n",
      " [-0.61853943]]\n",
      "loss=34293.59468784271\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01507196]\n",
      " [-0.09279604]\n",
      " [-0.29092527]\n",
      " ...\n",
      " [-0.22324973]\n",
      " [ 0.04340127]\n",
      " [-0.13244165]]\n",
      "t [[ 0.01507196]\n",
      " [-0.09279604]\n",
      " [-0.29092527]\n",
      " ...\n",
      " [-0.22324973]\n",
      " [ 0.04340127]\n",
      " [-0.13244165]]\n",
      "t [[ 0.00917062]\n",
      " [-0.21773925]\n",
      " [-0.44033365]\n",
      " ...\n",
      " [-0.32865233]\n",
      " [ 0.04637191]\n",
      " [-0.23239003]]\n",
      "t [[ 0.00917062]\n",
      " [-0.21773925]\n",
      " [-0.44033365]\n",
      " ...\n",
      " [-0.32865233]\n",
      " [ 0.04637191]\n",
      " [-0.23239003]]\n",
      "Current iteration=2, loss=37207.03580750528\n",
      "t [[-0.00224798]\n",
      " [-0.34404683]\n",
      " [-0.53710221]\n",
      " ...\n",
      " [-0.39153592]\n",
      " [ 0.0322195 ]\n",
      " [-0.31322419]]\n",
      "t [[-0.00224798]\n",
      " [-0.34404683]\n",
      " [-0.53710221]\n",
      " ...\n",
      " [-0.39153592]\n",
      " [ 0.0322195 ]\n",
      " [-0.31322419]]\n",
      "t [[-0.01438552]\n",
      " [-0.46291809]\n",
      " [-0.61118824]\n",
      " ...\n",
      " [-0.43728009]\n",
      " [ 0.00961981]\n",
      " [-0.38087382]]\n",
      "t [[-0.01438552]\n",
      " [-0.46291809]\n",
      " [-0.61118824]\n",
      " ...\n",
      " [-0.43728009]\n",
      " [ 0.00961981]\n",
      " [-0.38087382]]\n",
      "Current iteration=4, loss=35893.115395117915\n",
      "t [[-0.0257033 ]\n",
      " [-0.57222282]\n",
      " [-0.67387337]\n",
      " ...\n",
      " [-0.47531286]\n",
      " [-0.01744803]\n",
      " [-0.43858327]]\n",
      "t [[-0.0257033 ]\n",
      " [-0.57222282]\n",
      " [-0.67387337]\n",
      " ...\n",
      " [-0.47531286]\n",
      " [-0.01744803]\n",
      " [-0.43858327]]\n",
      "t [[-0.03577252]\n",
      " [-0.67200643]\n",
      " [-0.72976813]\n",
      " ...\n",
      " [-0.50935208]\n",
      " [-0.04683321]\n",
      " [-0.48842255]]\n",
      "t [[-0.03577252]\n",
      " [-0.67200643]\n",
      " [-0.72976813]\n",
      " ...\n",
      " [-0.50935208]\n",
      " [-0.04683321]\n",
      " [-0.48842255]]\n",
      "Current iteration=6, loss=35097.41633773945\n",
      "t [[-0.044561  ]\n",
      " [-0.76300649]\n",
      " [-0.7808794 ]\n",
      " ...\n",
      " [-0.54090123]\n",
      " [-0.07724956]\n",
      " [-0.53184299]]\n",
      "t [[-0.044561  ]\n",
      " [-0.76300649]\n",
      " [-0.7808794 ]\n",
      " ...\n",
      " [-0.54090123]\n",
      " [-0.07724956]\n",
      " [-0.53184299]]\n",
      "t [[-0.05216986]\n",
      " [-0.84612028]\n",
      " [-0.82815568]\n",
      " ...\n",
      " [-0.57058077]\n",
      " [-0.10788144]\n",
      " [-0.56992394]]\n",
      "t [[-0.05216986]\n",
      " [-0.84612028]\n",
      " [-0.82815568]\n",
      " ...\n",
      " [-0.57058077]\n",
      " [-0.10788144]\n",
      " [-0.56992394]]\n",
      "Current iteration=8, loss=34570.853257220646\n",
      "t [[-0.05873402]\n",
      " [-0.92221582]\n",
      " [-0.87210557]\n",
      " ...\n",
      " [-0.59865957]\n",
      " [-0.13819632]\n",
      " [-0.60349943]]\n",
      "t [[-0.05873402]\n",
      " [-0.92221582]\n",
      " [-0.87210557]\n",
      " ...\n",
      " [-0.59865957]\n",
      " [-0.13819632]\n",
      " [-0.60349943]]\n",
      "t [[-0.06438602]\n",
      " [-0.99207313]\n",
      " [-0.91305091]\n",
      " ...\n",
      " [-0.62527009]\n",
      " [-0.16784441]\n",
      " [-0.63323105]]\n",
      "loss=34201.0975762084\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01287312]\n",
      " [-0.08144336]\n",
      " [-0.29407608]\n",
      " ...\n",
      " [-0.21951629]\n",
      " [ 0.04666484]\n",
      " [-0.12423084]]\n",
      "t [[ 0.01287312]\n",
      " [-0.08144336]\n",
      " [-0.29407608]\n",
      " ...\n",
      " [-0.21951629]\n",
      " [ 0.04666484]\n",
      " [-0.12423084]]\n",
      "t [[ 0.00497339]\n",
      " [-0.1965814 ]\n",
      " [-0.44687648]\n",
      " ...\n",
      " [-0.32276994]\n",
      " [ 0.05246978]\n",
      " [-0.21707088]]\n",
      "t [[ 0.00497339]\n",
      " [-0.1965814 ]\n",
      " [-0.44687648]\n",
      " ...\n",
      " [-0.32276994]\n",
      " [ 0.05246978]\n",
      " [-0.21707088]]\n",
      "Current iteration=2, loss=37244.998434248155\n",
      "t [[-0.00815028]\n",
      " [-0.31444564]\n",
      " [-0.5465229 ]\n",
      " ...\n",
      " [-0.38396929]\n",
      " [ 0.04067995]\n",
      " [-0.29160512]]\n",
      "t [[-0.00815028]\n",
      " [-0.31444564]\n",
      " [-0.5465229 ]\n",
      " ...\n",
      " [-0.38396929]\n",
      " [ 0.04067995]\n",
      " [-0.29160512]]\n",
      "t [[-0.02167258]\n",
      " [-0.42596025]\n",
      " [-0.62300249]\n",
      " ...\n",
      " [-0.42829635]\n",
      " [ 0.02001892]\n",
      " [-0.35361727]]\n",
      "t [[-0.02167258]\n",
      " [-0.42596025]\n",
      " [-0.62300249]\n",
      " ...\n",
      " [-0.42829635]\n",
      " [ 0.02001892]\n",
      " [-0.35361727]]\n",
      "Current iteration=4, loss=35955.39119842915\n",
      "t [[-0.03407376]\n",
      " [-0.52878784]\n",
      " [-0.68769619]\n",
      " ...\n",
      " [-0.46513308]\n",
      " [-0.005482  ]\n",
      " [-0.4062536 ]]\n",
      "t [[-0.03407376]\n",
      " [-0.52878784]\n",
      " [-0.68769619]\n",
      " ...\n",
      " [-0.46513308]\n",
      " [-0.005482  ]\n",
      " [-0.4062536 ]]\n",
      "t [[-0.04496178]\n",
      " [-0.62281987]\n",
      " [-0.74529782]\n",
      " ...\n",
      " [-0.49817596]\n",
      " [-0.03361923]\n",
      " [-0.45150853]]\n",
      "t [[-0.04496178]\n",
      " [-0.62281987]\n",
      " [-0.74529782]\n",
      " ...\n",
      " [-0.49817596]\n",
      " [-0.03361923]\n",
      " [-0.45150853]]\n",
      "Current iteration=6, loss=35175.58933992675\n",
      "t [[-0.05434503]\n",
      " [-0.70867629]\n",
      " [-0.79787629]\n",
      " ...\n",
      " [-0.52890858]\n",
      " [-0.06305532]\n",
      " [-0.49077172]]\n",
      "t [[-0.05434503]\n",
      " [-0.70867629]\n",
      " [-0.79787629]\n",
      " ...\n",
      " [-0.52890858]\n",
      " [-0.06305532]\n",
      " [-0.49077172]]\n",
      "t [[-0.0623628 ]\n",
      " [-0.78716223]\n",
      " [-0.84642569]\n",
      " ...\n",
      " [-0.55793019]\n",
      " [-0.09292722]\n",
      " [-0.52507098]]\n",
      "t [[-0.0623628 ]\n",
      " [-0.78716223]\n",
      " [-0.84642569]\n",
      " ...\n",
      " [-0.55793019]\n",
      " [-0.09292722]\n",
      " [-0.52507098]]\n",
      "Current iteration=8, loss=34659.36592210011\n",
      "t [[-0.06918363]\n",
      " [-0.85907216]\n",
      " [-0.89148862]\n",
      " ...\n",
      " [-0.58548823]\n",
      " [-0.12266046]\n",
      " [-0.55519681]]\n",
      "t [[-0.06918363]\n",
      " [-0.85907216]\n",
      " [-0.89148862]\n",
      " ...\n",
      " [-0.58548823]\n",
      " [-0.12266046]\n",
      " [-0.55519681]]\n",
      "t [[-0.07496864]\n",
      " [-0.92512652]\n",
      " [-0.9334128 ]\n",
      " ...\n",
      " [-0.6116948 ]\n",
      " [-0.15186935]\n",
      " [-0.58177368]]\n",
      "loss=34296.446274309215\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01072072]\n",
      " [-0.08626257]\n",
      " [-0.29213568]\n",
      " ...\n",
      " [-0.12861983]\n",
      " [-0.01549064]\n",
      " [-0.26240528]]\n",
      "t [[ 0.01072072]\n",
      " [-0.08626257]\n",
      " [-0.29213568]\n",
      " ...\n",
      " [-0.12861983]\n",
      " [-0.01549064]\n",
      " [-0.26240528]]\n",
      "t [[ 0.00108229]\n",
      " [-0.2059499 ]\n",
      " [-0.44186219]\n",
      " ...\n",
      " [-0.16597554]\n",
      " [-0.00076004]\n",
      " [-0.47136437]]\n",
      "t [[ 0.00108229]\n",
      " [-0.2059499 ]\n",
      " [-0.44186219]\n",
      " ...\n",
      " [-0.16597554]\n",
      " [-0.00076004]\n",
      " [-0.47136437]]\n",
      "Current iteration=2, loss=37192.76286495087\n",
      "t [[-0.01343554]\n",
      " [-0.32795573]\n",
      " [-0.53840386]\n",
      " ...\n",
      " [-0.17464394]\n",
      " [ 0.01894196]\n",
      " [-0.64657618]]\n",
      "t [[-0.01343554]\n",
      " [-0.32795573]\n",
      " [-0.53840386]\n",
      " ...\n",
      " [-0.17464394]\n",
      " [ 0.01894196]\n",
      " [-0.64657618]]\n",
      "t [[-0.02809557]\n",
      " [-0.44322544]\n",
      " [-0.61199865]\n",
      " ...\n",
      " [-0.17488624]\n",
      " [ 0.03640603]\n",
      " [-0.79774494]]\n",
      "t [[-0.02809557]\n",
      " [-0.44322544]\n",
      " [-0.61199865]\n",
      " ...\n",
      " [-0.17488624]\n",
      " [ 0.03640603]\n",
      " [-0.79774494]]\n",
      "Current iteration=4, loss=35879.54394279525\n",
      "t [[-0.04144526]\n",
      " [-0.54945546]\n",
      " [-0.67408362]\n",
      " ...\n",
      " [-0.17361961]\n",
      " [ 0.04994869]\n",
      " [-0.93052057]]\n",
      "t [[-0.04144526]\n",
      " [-0.54945546]\n",
      " [-0.67408362]\n",
      " ...\n",
      " [-0.17361961]\n",
      " [ 0.04994869]\n",
      " [-0.93052057]]\n",
      "t [[-0.05314011]\n",
      " [-0.64657225]\n",
      " [-0.72935027]\n",
      " ...\n",
      " [-0.17310898]\n",
      " [ 0.05965769]\n",
      " [-1.04861856]]\n",
      "t [[-0.05314011]\n",
      " [-0.64657225]\n",
      " [-0.72935027]\n",
      " ...\n",
      " [-0.17310898]\n",
      " [ 0.05965769]\n",
      " [-1.04861856]]\n",
      "Current iteration=6, loss=35084.109254698415\n",
      "t [[-0.06322169]\n",
      " [-0.73523025]\n",
      " [-0.77984508]\n",
      " ...\n",
      " [-0.17391038]\n",
      " [ 0.06613752]\n",
      " [-1.15467622]]\n",
      "t [[-0.06322169]\n",
      " [-0.73523025]\n",
      " [-0.77984508]\n",
      " ...\n",
      " [-0.17391038]\n",
      " [ 0.06613752]\n",
      " [-1.15467622]]\n",
      "t [[-0.07185253]\n",
      " [-0.81626697]\n",
      " [-0.82653426]\n",
      " ...\n",
      " [-0.1759724 ]\n",
      " [ 0.07006681]\n",
      " [-1.25066122]]\n",
      "t [[-0.07185253]\n",
      " [-0.81626697]\n",
      " [-0.82653426]\n",
      " ...\n",
      " [-0.1759724 ]\n",
      " [ 0.07006681]\n",
      " [-1.25066122]]\n",
      "Current iteration=8, loss=34557.37272002357\n",
      "t [[-0.07921752]\n",
      " [-0.89050612]\n",
      " [-0.86993307]\n",
      " ...\n",
      " [-0.1790598 ]\n",
      " [ 0.07205207]\n",
      " [-1.33809273]]\n",
      "t [[-0.07921752]\n",
      " [-0.89050612]\n",
      " [-0.86993307]\n",
      " ...\n",
      " [-0.1790598 ]\n",
      " [ 0.07205207]\n",
      " [-1.33809273]]\n",
      "t [[-0.08548959]\n",
      " [-0.95869386]\n",
      " [-0.9103646 ]\n",
      " ...\n",
      " [-0.18291232]\n",
      " [ 0.0725922 ]\n",
      " [-1.41817321]]\n",
      "loss=34187.28776121372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11701214]\n",
      " [-0.49986246]\n",
      " [-0.31418155]\n",
      " ...\n",
      " [-0.22770374]\n",
      " [ 0.04929911]\n",
      " [-0.13273931]]\n",
      "t [[ 0.11701214]\n",
      " [-0.49986246]\n",
      " [-0.31418155]\n",
      " ...\n",
      " [-0.22770374]\n",
      " [ 0.04929911]\n",
      " [-0.13273931]]\n",
      "t [[ 0.1699753 ]\n",
      " [-0.77993207]\n",
      " [-0.48394097]\n",
      " ...\n",
      " [-0.33455755]\n",
      " [ 0.0552159 ]\n",
      " [-0.23218909]]\n",
      "t [[ 0.1699753 ]\n",
      " [-0.77993207]\n",
      " [-0.48394097]\n",
      " ...\n",
      " [-0.33455755]\n",
      " [ 0.0552159 ]\n",
      " [-0.23218909]]\n",
      "Current iteration=2, loss=37199.88220001009\n",
      "t [[ 0.19645248]\n",
      " [-0.96834816]\n",
      " [-0.59812493]\n",
      " ...\n",
      " [-0.39874367]\n",
      " [ 0.04264255]\n",
      " [-0.31224756]]\n",
      "t [[ 0.19645248]\n",
      " [-0.96834816]\n",
      " [-0.59812493]\n",
      " ...\n",
      " [-0.39874367]\n",
      " [ 0.04264255]\n",
      " [-0.31224756]]\n",
      "t [[ 0.21039805]\n",
      " [-1.11014672]\n",
      " [-0.68658195]\n",
      " ...\n",
      " [-0.4460003 ]\n",
      " [ 0.02080819]\n",
      " [-0.37899125]]\n",
      "t [[ 0.21039805]\n",
      " [-1.11014672]\n",
      " [-0.68658195]\n",
      " ...\n",
      " [-0.4460003 ]\n",
      " [ 0.02080819]\n",
      " [-0.37899125]]\n",
      "Current iteration=4, loss=35906.04571575074\n",
      "t [[ 0.21778741]\n",
      " [-1.22407509]\n",
      " [-0.76078881]\n",
      " ...\n",
      " [-0.48569599]\n",
      " [-0.00601451]\n",
      " [-0.43572752]]\n",
      "t [[ 0.21778741]\n",
      " [-1.22407509]\n",
      " [-0.76078881]\n",
      " ...\n",
      " [-0.48569599]\n",
      " [-0.00601451]\n",
      " [-0.43572752]]\n",
      "t [[ 0.22148736]\n",
      " [-1.31919145]\n",
      " [-0.8256813 ]\n",
      " ...\n",
      " [-0.52144354]\n",
      " [-0.03548676]\n",
      " [-0.48456166]]\n",
      "t [[ 0.22148736]\n",
      " [-1.31919145]\n",
      " [-0.8256813 ]\n",
      " ...\n",
      " [-0.52144354]\n",
      " [-0.03548676]\n",
      " [-0.48456166]]\n",
      "Current iteration=6, loss=35128.12286039788\n",
      "t [[ 0.22299898]\n",
      " [-1.40046403]\n",
      " [-0.88361647]\n",
      " ...\n",
      " [-0.55466718]\n",
      " [-0.06619463]\n",
      " [-0.52696732]]\n",
      "t [[ 0.22299898]\n",
      " [-1.40046403]\n",
      " [-0.88361647]\n",
      " ...\n",
      " [-0.55466718]\n",
      " [-0.06619463]\n",
      " [-0.52696732]]\n",
      "t [[ 0.22316926]\n",
      " [-1.47094564]\n",
      " [-0.93587084]\n",
      " ...\n",
      " [-0.58593916]\n",
      " [-0.09723446]\n",
      " [-0.56403871]]\n",
      "t [[ 0.22316926]\n",
      " [-1.47094564]\n",
      " [-0.93587084]\n",
      " ...\n",
      " [-0.58593916]\n",
      " [-0.09723446]\n",
      " [-0.56403871]]\n",
      "Current iteration=8, loss=34615.30347923111\n",
      "t [[ 0.22250706]\n",
      " [-1.53269823]\n",
      " [-0.98324188]\n",
      " ...\n",
      " [-0.61550302]\n",
      " [-0.12801349]\n",
      " [-0.59661984]]\n",
      "t [[ 0.22250706]\n",
      " [-1.53269823]\n",
      " [-0.98324188]\n",
      " ...\n",
      " [-0.61550302]\n",
      " [-0.12801349]\n",
      " [-0.59661984]]\n",
      "t [[ 0.2213334 ]\n",
      " [-1.58721675]\n",
      " [-1.02629832]\n",
      " ...\n",
      " [-0.64348044]\n",
      " [-0.15814138]\n",
      " [-0.62537894]]\n",
      "loss=34255.80627231268\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01543082]\n",
      " [-0.09500547]\n",
      " [-0.29785206]\n",
      " ...\n",
      " [-0.2285652 ]\n",
      " [ 0.04443464]\n",
      " [-0.13559502]]\n",
      "t [[ 0.01543082]\n",
      " [-0.09500547]\n",
      " [-0.29785206]\n",
      " ...\n",
      " [-0.2285652 ]\n",
      " [ 0.04443464]\n",
      " [-0.13559502]]\n",
      "t [[ 0.00890535]\n",
      " [-0.22366695]\n",
      " [-0.44750194]\n",
      " ...\n",
      " [-0.33371422]\n",
      " [ 0.04653661]\n",
      " [-0.23715453]]\n",
      "t [[ 0.00890535]\n",
      " [-0.22366695]\n",
      " [-0.44750194]\n",
      " ...\n",
      " [-0.33371422]\n",
      " [ 0.04653661]\n",
      " [-0.23715453]]\n",
      "Current iteration=2, loss=37161.39290264118\n",
      "t [[-0.00304645]\n",
      " [-0.35304802]\n",
      " [-0.54406271]\n",
      " ...\n",
      " [-0.39606717]\n",
      " [ 0.03123135]\n",
      " [-0.3190005 ]]\n",
      "t [[-0.00304645]\n",
      " [-0.35304802]\n",
      " [-0.54406271]\n",
      " ...\n",
      " [-0.39606717]\n",
      " [ 0.03123135]\n",
      " [-0.3190005 ]]\n",
      "t [[-0.01552694]\n",
      " [-0.47422369]\n",
      " [-0.61827265]\n",
      " ...\n",
      " [-0.44166144]\n",
      " [ 0.00748106]\n",
      " [-0.38731259]]\n",
      "t [[-0.01552694]\n",
      " [-0.47422369]\n",
      " [-0.61827265]\n",
      " ...\n",
      " [-0.44166144]\n",
      " [ 0.00748106]\n",
      " [-0.38731259]]\n",
      "Current iteration=4, loss=35843.046758457946\n",
      "t [[-0.02703752]\n",
      " [-0.58522068]\n",
      " [-0.68135232]\n",
      " ...\n",
      " [-0.47985667]\n",
      " [-0.02066358]\n",
      " [-0.44544315]]\n",
      "t [[-0.02703752]\n",
      " [-0.58522068]\n",
      " [-0.68135232]\n",
      " ...\n",
      " [-0.47985667]\n",
      " [-0.02066358]\n",
      " [-0.44544315]]\n",
      "t [[-0.03719812]\n",
      " [-0.68624411]\n",
      " [-0.73776245]\n",
      " ...\n",
      " [-0.5142262 ]\n",
      " [-0.05102779]\n",
      " [-0.49552655]]\n",
      "t [[-0.03719812]\n",
      " [-0.68624411]\n",
      " [-0.73776245]\n",
      " ...\n",
      " [-0.5142262 ]\n",
      " [-0.05102779]\n",
      " [-0.49552655]]\n",
      "Current iteration=6, loss=35049.974366143615\n",
      "t [[-0.04601253]\n",
      " [-0.77815153]\n",
      " [-0.78940187]\n",
      " ...\n",
      " [-0.54616733]\n",
      " [-0.08231674]\n",
      " [-0.53905863]]\n",
      "t [[-0.04601253]\n",
      " [-0.77815153]\n",
      " [-0.78940187]\n",
      " ...\n",
      " [-0.54616733]\n",
      " [-0.08231674]\n",
      " [-0.53905863]]\n",
      "t [[-0.05360523]\n",
      " [-0.86192403]\n",
      " [-0.83715921]\n",
      " ...\n",
      " [-0.57623852]\n",
      " [-0.11371325]\n",
      " [-0.57715134]]\n",
      "t [[-0.05360523]\n",
      " [-0.86192403]\n",
      " [-0.83715921]\n",
      " ...\n",
      " [-0.57623852]\n",
      " [-0.11371325]\n",
      " [-0.57715134]]\n",
      "Current iteration=8, loss=34527.94427749182\n",
      "t [[-0.06012576]\n",
      " [-0.93848785]\n",
      " [-0.8815161 ]\n",
      " ...\n",
      " [-0.60467726]\n",
      " [-0.14468764]\n",
      " [-0.61066318]]\n",
      "t [[-0.06012576]\n",
      " [-0.93848785]\n",
      " [-0.8815161 ]\n",
      " ...\n",
      " [-0.60467726]\n",
      " [-0.14468764]\n",
      " [-0.61066318]]\n",
      "t [[-0.06571584]\n",
      " [-1.00866429]\n",
      " [-0.92278614]\n",
      " ...\n",
      " [-0.6316027 ]\n",
      " [-0.17489589]\n",
      " [-0.64027426]]\n",
      "loss=34162.72893659102\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01317962]\n",
      " [-0.08338249]\n",
      " [-0.30107789]\n",
      " ...\n",
      " [-0.22474287]\n",
      " [ 0.04777591]\n",
      " [-0.12718871]]\n",
      "t [[ 0.01317962]\n",
      " [-0.08338249]\n",
      " [-0.30107789]\n",
      " ...\n",
      " [-0.22474287]\n",
      " [ 0.04777591]\n",
      " [-0.12718871]]\n",
      "t [[ 0.00461248]\n",
      " [-0.20204384]\n",
      " [-0.45420375]\n",
      " ...\n",
      " [-0.32772796]\n",
      " [ 0.05276806]\n",
      " [-0.22149714]]\n",
      "t [[ 0.00461248]\n",
      " [-0.20204384]\n",
      " [-0.45420375]\n",
      " ...\n",
      " [-0.32772796]\n",
      " [ 0.05276806]\n",
      " [-0.22149714]]\n",
      "Current iteration=2, loss=37200.08426794397\n",
      "t [[-0.00907105]\n",
      " [-0.32284483]\n",
      " [-0.55368762]\n",
      " ...\n",
      " [-0.38837987]\n",
      " [ 0.03985901]\n",
      " [-0.29693158]]\n",
      "t [[-0.00907105]\n",
      " [-0.32284483]\n",
      " [-0.55368762]\n",
      " ...\n",
      " [-0.38837987]\n",
      " [ 0.03985901]\n",
      " [-0.29693158]]\n",
      "t [[-0.02294625]\n",
      " [-0.43656563]\n",
      " [-0.63031451]\n",
      " ...\n",
      " [-0.43254323]\n",
      " [ 0.01806321]\n",
      " [-0.35951893]]\n",
      "t [[-0.02294625]\n",
      " [-0.43656563]\n",
      " [-0.63031451]\n",
      " ...\n",
      " [-0.43254323]\n",
      " [ 0.01806321]\n",
      " [-0.35951893]]\n",
      "Current iteration=4, loss=35906.30563134287\n",
      "t [[-0.03553705]\n",
      " [-0.54101451]\n",
      " [-0.69541432]\n",
      " ...\n",
      " [-0.4695354 ]\n",
      " [-0.00851273]\n",
      " [-0.41250897]]\n",
      "t [[-0.03553705]\n",
      " [-0.54101451]\n",
      " [-0.69541432]\n",
      " ...\n",
      " [-0.4695354 ]\n",
      " [-0.00851273]\n",
      " [-0.41250897]]\n",
      "t [[-0.04650414]\n",
      " [-0.63623543]\n",
      " [-0.75353611]\n",
      " ...\n",
      " [-0.50290884]\n",
      " [-0.03763738]\n",
      " [-0.4579568 ]]\n",
      "t [[-0.04650414]\n",
      " [-0.63623543]\n",
      " [-0.75353611]\n",
      " ...\n",
      " [-0.50290884]\n",
      " [-0.03763738]\n",
      " [-0.4579568 ]]\n",
      "Current iteration=6, loss=35129.10134329674\n",
      "t [[-0.05589515]\n",
      " [-0.72296335]\n",
      " [-0.8066433 ]\n",
      " ...\n",
      " [-0.5340398 ]\n",
      " [-0.06796104]\n",
      " [-0.49729354]]\n",
      "t [[-0.05589515]\n",
      " [-0.72296335]\n",
      " [-0.8066433 ]\n",
      " ...\n",
      " [-0.5340398 ]\n",
      " [-0.06796104]\n",
      " [-0.49729354]]\n",
      "t [[-0.06387522]\n",
      " [-0.80208367]\n",
      " [-0.85567151]\n",
      " ...\n",
      " [-0.56346389]\n",
      " [-0.09861617]\n",
      " [-0.53157715]]\n",
      "t [[-0.06387522]\n",
      " [-0.80208367]\n",
      " [-0.85567151]\n",
      " ...\n",
      " [-0.56346389]\n",
      " [-0.09861617]\n",
      " [-0.53157715]]\n",
      "Current iteration=8, loss=34617.2877514486\n",
      "t [[-0.07062926]\n",
      " [-0.87444646]\n",
      " [-0.90113717]\n",
      " ...\n",
      " [-0.59139565]\n",
      " [-0.12902896]\n",
      " [-0.5616206 ]]\n",
      "t [[-0.07062926]\n",
      " [-0.87444646]\n",
      " [-0.90113717]\n",
      " ...\n",
      " [-0.59139565]\n",
      " [-0.12902896]\n",
      " [-0.5616206 ]]\n",
      "t [[-0.07632881]\n",
      " [-0.94081148]\n",
      " [-0.94338031]\n",
      " ...\n",
      " [-0.6179326 ]\n",
      " [-0.15881791]\n",
      " [-0.5880653 ]]\n",
      "loss=34258.76831723055\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01097597]\n",
      " [-0.08831644]\n",
      " [-0.29909129]\n",
      " ...\n",
      " [-0.13168221]\n",
      " [-0.01585947]\n",
      " [-0.26865303]]\n",
      "t [[ 0.01097597]\n",
      " [-0.08831644]\n",
      " [-0.29909129]\n",
      " ...\n",
      " [-0.13168221]\n",
      " [-0.01585947]\n",
      " [-0.26865303]]\n",
      "t [[ 6.38615878e-04]\n",
      " [-2.11627315e-01]\n",
      " [-4.49045860e-01]\n",
      " ...\n",
      " [-1.67789068e-01]\n",
      " [-6.78432136e-05]\n",
      " [-4.81319568e-01]]\n",
      "t [[ 6.38615878e-04]\n",
      " [-2.11627315e-01]\n",
      " [-4.49045860e-01]\n",
      " ...\n",
      " [-1.67789068e-01]\n",
      " [-6.78432136e-05]\n",
      " [-4.81319568e-01]]\n",
      "Current iteration=2, loss=37147.09230258745\n",
      "t [[-0.01445583]\n",
      " [-0.33664941]\n",
      " [-0.54534916]\n",
      " ...\n",
      " [-0.17529294]\n",
      " [ 0.02034504]\n",
      " [-0.65909993]]\n",
      "t [[-0.01445583]\n",
      " [-0.33664941]\n",
      " [-0.54534916]\n",
      " ...\n",
      " [-0.17529294]\n",
      " [ 0.02034504]\n",
      " [-0.65909993]]\n",
      "t [[-0.02947742]\n",
      " [-0.45418766]\n",
      " [-0.61903738]\n",
      " ...\n",
      " [-0.17492587]\n",
      " [ 0.038071  ]\n",
      " [-0.81215028]]\n",
      "t [[-0.02947742]\n",
      " [-0.45418766]\n",
      " [-0.61903738]\n",
      " ...\n",
      " [-0.17492587]\n",
      " [ 0.038071  ]\n",
      " [-0.81215028]]\n",
      "Current iteration=4, loss=35829.50947312915\n",
      "t [[-0.0430211 ]\n",
      " [-0.56208677]\n",
      " [-0.68149219]\n",
      " ...\n",
      " [-0.17347848]\n",
      " [ 0.05156524]\n",
      " [-0.94633394]]\n",
      "t [[-0.0430211 ]\n",
      " [-0.56208677]\n",
      " [-0.68149219]\n",
      " ...\n",
      " [-0.17347848]\n",
      " [ 0.05156524]\n",
      " [-0.94633394]]\n",
      "t [[-0.05479716]\n",
      " [-0.66042832]\n",
      " [-0.73725587]\n",
      " ...\n",
      " [-0.17304272]\n",
      " [ 0.06105008]\n",
      " [-1.06549384]]\n",
      "t [[-0.05479716]\n",
      " [-0.66042832]\n",
      " [-0.73725587]\n",
      " ...\n",
      " [-0.17304272]\n",
      " [ 0.06105008]\n",
      " [-1.06549384]]\n",
      "Current iteration=6, loss=35036.67245086578\n",
      "t [[-0.06488745]\n",
      " [-0.74998423]\n",
      " [-0.78826573]\n",
      " ...\n",
      " [-0.17404898]\n",
      " [ 0.06722363]\n",
      " [-1.17235286]]\n",
      "t [[-0.06488745]\n",
      " [-0.74998423]\n",
      " [-0.78826573]\n",
      " ...\n",
      " [-0.17404898]\n",
      " [ 0.06722363]\n",
      " [-1.17235286]]\n",
      "t [[-0.07348081]\n",
      " [-0.83167434]\n",
      " [-0.83542681]\n",
      " ...\n",
      " [-0.17636874]\n",
      " [ 0.07082225]\n",
      " [-1.26893946]]\n",
      "t [[-0.07348081]\n",
      " [-0.83167434]\n",
      " [-0.83542681]\n",
      " ...\n",
      " [-0.17636874]\n",
      " [ 0.07082225]\n",
      " [-1.26893946]]\n",
      "Current iteration=8, loss=34514.4421578879\n",
      "t [[-0.08077872]\n",
      " [-0.90637961]\n",
      " [-0.87922624]\n",
      " ...\n",
      " [-0.1797233 ]\n",
      " [ 0.07248558]\n",
      " [-1.35681737]]\n",
      "t [[-0.08077872]\n",
      " [-0.90637961]\n",
      " [-0.87922624]\n",
      " ...\n",
      " [-0.1797233 ]\n",
      " [ 0.07248558]\n",
      " [-1.35681737]]\n",
      "t [[-0.08696456]\n",
      " [-0.97488683]\n",
      " [-0.91997813]\n",
      " ...\n",
      " [-0.18382961]\n",
      " [ 0.07273021]\n",
      " [-1.43722236]]\n",
      "loss=34148.88077646886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.11973336]\n",
      " [-0.51148717]\n",
      " [-0.3214881 ]\n",
      " ...\n",
      " [-0.23299918]\n",
      " [ 0.0504456 ]\n",
      " [-0.13582627]]\n",
      "t [[ 0.11973336]\n",
      " [-0.51148717]\n",
      " [-0.3214881 ]\n",
      " ...\n",
      " [-0.23299918]\n",
      " [ 0.0504456 ]\n",
      " [-0.13582627]]\n",
      "t [[ 0.17246244]\n",
      " [-0.79304554]\n",
      " [-0.49189657]\n",
      " ...\n",
      " [-0.33957448]\n",
      " [ 0.05551649]\n",
      " [-0.23682073]]\n",
      "t [[ 0.17246244]\n",
      " [-0.79304554]\n",
      " [-0.49189657]\n",
      " ...\n",
      " [-0.33957448]\n",
      " [ 0.05551649]\n",
      " [-0.23682073]]\n",
      "Current iteration=2, loss=37155.70481340034\n",
      "t [[ 0.19832979]\n",
      " [-0.9815749 ]\n",
      " [-0.60614546]\n",
      " ...\n",
      " [-0.40326429]\n",
      " [ 0.04178979]\n",
      " [-0.31783653]]\n",
      "t [[ 0.19832979]\n",
      " [-0.9815749 ]\n",
      " [-0.60614546]\n",
      " ...\n",
      " [-0.40326429]\n",
      " [ 0.04178979]\n",
      " [-0.31783653]]\n",
      "t [[ 0.21172505]\n",
      " [-1.12338776]\n",
      " [-0.69484377]\n",
      " ...\n",
      " [-0.45042299]\n",
      " [ 0.01879185]\n",
      " [-0.38519694]]\n",
      "t [[ 0.21172505]\n",
      " [-1.12338776]\n",
      " [-0.69484377]\n",
      " ...\n",
      " [-0.45042299]\n",
      " [ 0.01879185]\n",
      " [-0.38519694]]\n",
      "Current iteration=4, loss=35858.05129869419\n",
      "t [[ 0.21867742]\n",
      " [-1.23735191]\n",
      " [-0.76943722]\n",
      " ...\n",
      " [-0.49032915]\n",
      " [-0.00912582]\n",
      " [-0.44231552]]\n",
      "t [[ 0.21867742]\n",
      " [-1.23735191]\n",
      " [-0.76943722]\n",
      " ...\n",
      " [-0.49032915]\n",
      " [-0.00912582]\n",
      " [-0.44231552]]\n",
      "t [[ 0.22203659]\n",
      " [-1.33247949]\n",
      " [-0.83474726]\n",
      " ...\n",
      " [-0.52644369]\n",
      " [-0.03959533]\n",
      " [-0.49136094]]\n",
      "t [[ 0.22203659]\n",
      " [-1.33247949]\n",
      " [-0.83474726]\n",
      " ...\n",
      " [-0.52644369]\n",
      " [-0.03959533]\n",
      " [-0.49136094]]\n",
      "Current iteration=6, loss=35082.89810077753\n",
      "t [[ 0.22328057]\n",
      " [-1.4137004 ]\n",
      " [-0.89305226]\n",
      " ...\n",
      " [-0.5600839 ]\n",
      " [-0.07119104]\n",
      " [-0.53385067]]\n",
      "t [[ 0.22328057]\n",
      " [-1.4137004 ]\n",
      " [-0.89305226]\n",
      " ...\n",
      " [-0.5600839 ]\n",
      " [-0.07119104]\n",
      " [-0.53385067]]\n",
      "t [[ 0.22323896]\n",
      " [-1.48405545]\n",
      " [-0.94559062]\n",
      " ...\n",
      " [-0.59176173]\n",
      " [-0.10300631]\n",
      " [-0.57091092]]\n",
      "t [[ 0.22323896]\n",
      " [-1.48405545]\n",
      " [-0.94559062]\n",
      " ...\n",
      " [-0.59176173]\n",
      " [-0.10300631]\n",
      " [-0.57091092]]\n",
      "Current iteration=8, loss=34574.512443919535\n",
      "t [[ 0.22240877]\n",
      " [-1.54561136]\n",
      " [-0.99314808]\n",
      " ...\n",
      " [-0.62169142]\n",
      " [-0.13445084]\n",
      " [-0.60340963]]\n",
      "t [[ 0.22240877]\n",
      " [-1.54561136]\n",
      " [-0.99314808]\n",
      " ...\n",
      " [-0.62169142]\n",
      " [-0.13445084]\n",
      " [-0.60340963]]\n",
      "t [[ 0.22110273]\n",
      " [-1.59987493]\n",
      " [-1.03629572]\n",
      " ...\n",
      " [-0.64998327]\n",
      " [-0.16514029]\n",
      " [-0.63203304]]\n",
      "loss=34219.36752189088\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01578967]\n",
      " [-0.0972149 ]\n",
      " [-0.30477885]\n",
      " ...\n",
      " [-0.23388067]\n",
      " [ 0.045468  ]\n",
      " [-0.13874839]]\n",
      "t [[ 0.01578967]\n",
      " [-0.0972149 ]\n",
      " [-0.30477885]\n",
      " ...\n",
      " [-0.23388067]\n",
      " [ 0.045468  ]\n",
      " [-0.13874839]]\n",
      "t [[ 0.0086194 ]\n",
      " [-0.22962657]\n",
      " [-0.45452494]\n",
      " ...\n",
      " [-0.33865493]\n",
      " [ 0.04666067]\n",
      " [-0.24188487]]\n",
      "t [[ 0.0086194 ]\n",
      " [-0.22962657]\n",
      " [-0.45452494]\n",
      " ...\n",
      " [-0.33865493]\n",
      " [ 0.04666067]\n",
      " [-0.24188487]]\n",
      "Current iteration=2, loss=37116.50920337371\n",
      "t [[-0.00386225]\n",
      " [-0.3620535 ]\n",
      " [-0.55085314]\n",
      " ...\n",
      " [-0.40046114]\n",
      " [ 0.03018815]\n",
      " [-0.32471474]]\n",
      "t [[-0.00386225]\n",
      " [-0.3620535 ]\n",
      " [-0.55085314]\n",
      " ...\n",
      " [-0.40046114]\n",
      " [ 0.03018815]\n",
      " [-0.32471474]]\n",
      "t [[-0.01667309]\n",
      " [-0.48548171]\n",
      " [-0.62520763]\n",
      " ...\n",
      " [-0.44592993]\n",
      " [ 0.00528651]\n",
      " [-0.39366494]]\n",
      "t [[-0.01667309]\n",
      " [-0.48548171]\n",
      " [-0.62520763]\n",
      " ...\n",
      " [-0.44592993]\n",
      " [ 0.00528651]\n",
      " [-0.39366494]]\n",
      "Current iteration=4, loss=35794.09877957024\n",
      "t [[-0.02836287]\n",
      " [-0.59811517]\n",
      " [-0.68870606]\n",
      " ...\n",
      " [-0.48431574]\n",
      " [-0.02392845]\n",
      " [-0.45219436]]\n",
      "t [[-0.02836287]\n",
      " [-0.59811517]\n",
      " [-0.68870606]\n",
      " ...\n",
      " [-0.48431574]\n",
      " [-0.02392845]\n",
      " [-0.45219436]]\n",
      "t [[-0.0386033 ]\n",
      " [-0.70032669]\n",
      " [-0.74564516]\n",
      " ...\n",
      " [-0.51903458]\n",
      " [-0.05526053]\n",
      " [-0.50250156]]\n",
      "t [[-0.0386033 ]\n",
      " [-0.70032669]\n",
      " [-0.74564516]\n",
      " ...\n",
      " [-0.51903458]\n",
      " [-0.05526053]\n",
      " [-0.50250156]]\n",
      "Current iteration=6, loss=35003.88654608917\n",
      "t [[-0.04743466]\n",
      " [-0.79309576]\n",
      " [-0.7978142 ]\n",
      " ...\n",
      " [-0.55137592]\n",
      " [-0.08740739]\n",
      " [-0.54612698]]\n",
      "t [[-0.04743466]\n",
      " [-0.79309576]\n",
      " [-0.7978142 ]\n",
      " ...\n",
      " [-0.55137592]\n",
      " [-0.08740739]\n",
      " [-0.54612698]]\n",
      "t [[-0.05500447]\n",
      " [-0.87748701]\n",
      " [-0.84604477]\n",
      " ...\n",
      " [-0.58183847]\n",
      " [-0.11955113]\n",
      " [-0.58421534]]\n",
      "t [[-0.05500447]\n",
      " [-0.87748701]\n",
      " [-0.84604477]\n",
      " ...\n",
      " [-0.58183847]\n",
      " [-0.11955113]\n",
      " [-0.58421534]]\n",
      "Current iteration=8, loss=34486.44886760096\n",
      "t [[-0.06147633]\n",
      " [-0.95448403]\n",
      " [-0.89079476]\n",
      " ...\n",
      " [-0.61063112]\n",
      " [-0.15116567]\n",
      " [-0.61764961]]\n",
      "t [[-0.06147633]\n",
      " [-0.95448403]\n",
      " [-0.89079476]\n",
      " ...\n",
      " [-0.61063112]\n",
      " [-0.15116567]\n",
      " [-0.61764961]]\n",
      "t [[-0.06700071]\n",
      " [-1.02494865]\n",
      " [-0.93237225]\n",
      " ...\n",
      " [-0.637862  ]\n",
      " [-0.18191346]\n",
      " [-0.64712833]]\n",
      "loss=34125.74024266305\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01348613]\n",
      " [-0.08532161]\n",
      " [-0.30807971]\n",
      " ...\n",
      " [-0.22996945]\n",
      " [ 0.04888697]\n",
      " [-0.13014659]]\n",
      "t [[ 0.01348613]\n",
      " [-0.08532161]\n",
      " [-0.30807971]\n",
      " ...\n",
      " [-0.22996945]\n",
      " [ 0.04888697]\n",
      " [-0.13014659]]\n",
      "t [[ 0.00423105]\n",
      " [-0.20754002]\n",
      " [-0.46138571]\n",
      " ...\n",
      " [-0.33256631]\n",
      " [ 0.0530251 ]\n",
      " [-0.22589043]]\n",
      "t [[ 0.00423105]\n",
      " [-0.20754002]\n",
      " [-0.46138571]\n",
      " ...\n",
      " [-0.33256631]\n",
      " [ 0.0530251 ]\n",
      " [-0.22589043]]\n",
      "Current iteration=2, loss=37155.92490799885\n",
      "t [[-0.01000822]\n",
      " [-0.33125268]\n",
      " [-0.56068071]\n",
      " ...\n",
      " [-0.39265479]\n",
      " [ 0.03898145]\n",
      " [-0.30219856]]\n",
      "t [[-0.01000822]\n",
      " [-0.33125268]\n",
      " [-0.56068071]\n",
      " ...\n",
      " [-0.39265479]\n",
      " [ 0.03898145]\n",
      " [-0.30219856]]\n",
      "t [[-0.02422256]\n",
      " [-0.44713039]\n",
      " [-0.63747411]\n",
      " ...\n",
      " [-0.43667912]\n",
      " [ 0.01604891]\n",
      " [-0.36533841]]\n",
      "t [[-0.02422256]\n",
      " [-0.44713039]\n",
      " [-0.63747411]\n",
      " ...\n",
      " [-0.43667912]\n",
      " [ 0.01604891]\n",
      " [-0.36533841]]\n",
      "Current iteration=4, loss=35858.32306907793\n",
      "t [[-0.03698817]\n",
      " [-0.55314722]\n",
      " [-0.70300317]\n",
      " ...\n",
      " [-0.47385547]\n",
      " [-0.01159684]\n",
      " [-0.4186617 ]]\n",
      "t [[-0.03698817]\n",
      " [-0.55314722]\n",
      " [-0.70300317]\n",
      " ...\n",
      " [-0.47385547]\n",
      " [-0.01159684]\n",
      " [-0.4186617 ]]\n",
      "t [[-0.04802171]\n",
      " [-0.64950756]\n",
      " [-0.76165791]\n",
      " ...\n",
      " [-0.50757928]\n",
      " [-0.0416989 ]\n",
      " [-0.46428394]]\n",
      "t [[-0.04802171]\n",
      " [-0.64950756]\n",
      " [-0.76165791]\n",
      " ...\n",
      " [-0.50757928]\n",
      " [-0.0416989 ]\n",
      " [-0.46428394]]\n",
      "Current iteration=6, loss=35083.93999842057\n",
      "t [[-0.05741069]\n",
      " [-0.73706333]\n",
      " [-0.81529469]\n",
      " ...\n",
      " [-0.53911763]\n",
      " [-0.07289632]\n",
      " [-0.50367774]]\n",
      "t [[-0.05741069]\n",
      " [-0.73706333]\n",
      " [-0.81529469]\n",
      " ...\n",
      " [-0.53911763]\n",
      " [-0.07289632]\n",
      " [-0.50367774]]\n",
      "t [[-0.06534577]\n",
      " [-0.81677991]\n",
      " [-0.86479341]\n",
      " ...\n",
      " [-0.56894464]\n",
      " [-0.10431787]\n",
      " [-0.5379313 ]]\n",
      "t [[-0.06534577]\n",
      " [-0.81677991]\n",
      " [-0.86479341]\n",
      " ...\n",
      " [-0.56894464]\n",
      " [-0.10431787]\n",
      " [-0.5379313 ]]\n",
      "Current iteration=8, loss=34576.59171703872\n",
      "t [[-0.07202769]\n",
      " [-0.88956212]\n",
      " [-0.91064753]\n",
      " ...\n",
      " [-0.59724461]\n",
      " [-0.13539108]\n",
      " [-0.56788005]]\n",
      "t [[-0.07202769]\n",
      " [-0.88956212]\n",
      " [-0.91064753]\n",
      " ...\n",
      " [-0.59724461]\n",
      " [-0.13539108]\n",
      " [-0.56788005]]\n",
      "t [[-0.07763791]\n",
      " [-0.9562083 ]\n",
      " [-0.95319206]\n",
      " ...\n",
      " [-0.62410279]\n",
      " [-0.16573943]\n",
      " [-0.59418225]]\n",
      "loss=34222.438744085994\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01123123]\n",
      " [-0.09037031]\n",
      " [-0.3060469 ]\n",
      " ...\n",
      " [-0.13474459]\n",
      " [-0.01622829]\n",
      " [-0.27490077]]\n",
      "t [[ 0.01123123]\n",
      " [-0.09037031]\n",
      " [-0.3060469 ]\n",
      " ...\n",
      " [-0.13474459]\n",
      " [-0.01622829]\n",
      " [-0.27490077]]\n",
      "t [[ 1.74881921e-04]\n",
      " [-2.17338007e-01]\n",
      " [-4.56083339e-01]\n",
      " ...\n",
      " [-1.69508922e-01]\n",
      " [ 6.55620749e-04]\n",
      " [-4.91218225e-01]]\n",
      "t [[ 1.74881921e-04]\n",
      " [-2.17338007e-01]\n",
      " [-4.56083339e-01]\n",
      " ...\n",
      " [-1.69508922e-01]\n",
      " [ 6.55620749e-04]\n",
      " [-4.91218225e-01]]\n",
      "Current iteration=2, loss=37102.187395437155\n",
      "t [[-0.01549141]\n",
      " [-0.34535042]\n",
      " [-0.55212273]\n",
      " ...\n",
      " [-0.17584901]\n",
      " [ 0.0217648 ]\n",
      " [-0.67151519]]\n",
      "t [[-0.01549141]\n",
      " [-0.34535042]\n",
      " [-0.55212273]\n",
      " ...\n",
      " [-0.17584901]\n",
      " [ 0.0217648 ]\n",
      " [-0.67151519]]\n",
      "t [[-0.03086021]\n",
      " [-0.46510681]\n",
      " [-0.62592502]\n",
      " ...\n",
      " [-0.17490918]\n",
      " [ 0.03972244]\n",
      " [-0.82640003]]\n",
      "t [[-0.03086021]\n",
      " [-0.46510681]\n",
      " [-0.62592502]\n",
      " ...\n",
      " [-0.17490918]\n",
      " [ 0.03972244]\n",
      " [-0.82640003]]\n",
      "Current iteration=4, loss=35780.59470279351\n",
      "t [[-0.04458268]\n",
      " [-0.57462038]\n",
      " [-0.68877439]\n",
      " ...\n",
      " [-0.17331986]\n",
      " [ 0.05314002]\n",
      " [-0.96194844]]\n",
      "t [[-0.04458268]\n",
      " [-0.57462038]\n",
      " [-0.68877439]\n",
      " ...\n",
      " [-0.17331986]\n",
      " [ 0.05314002]\n",
      " [-0.96194844]]\n",
      "t [[-0.05642707]\n",
      " [-0.67413587]\n",
      " [-0.74504939]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0623807 ]\n",
      " [-1.08213066]]\n",
      "t [[-0.05642707]\n",
      " [-0.67413587]\n",
      " [-0.74504939]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0623807 ]\n",
      " [-1.08213066]]\n",
      "Current iteration=6, loss=34990.58767520581\n",
      "t [[-0.06651612]\n",
      " [-0.76454472]\n",
      " [-0.7965765 ]\n",
      " ...\n",
      " [-0.17421687]\n",
      " [ 0.0682368 ]\n",
      " [-1.18975514]]\n",
      "t [[-0.06651612]\n",
      " [-0.76454472]\n",
      " [-0.7965765 ]\n",
      " ...\n",
      " [-0.17421687]\n",
      " [ 0.0682368 ]\n",
      " [-1.18975514]]\n",
      "t [[-0.07506459]\n",
      " [-0.84684893]\n",
      " [-0.84420223]\n",
      " ...\n",
      " [-0.17680266]\n",
      " [ 0.07150093]\n",
      " [-1.28691112]]\n",
      "t [[-0.07506459]\n",
      " [-0.84684893]\n",
      " [-0.84420223]\n",
      " ...\n",
      " [-0.17680266]\n",
      " [ 0.07150093]\n",
      " [-1.28691112]]\n",
      "Current iteration=8, loss=34472.92372413839\n",
      "t [[-0.08228998]\n",
      " [-0.92198586]\n",
      " [-0.88838892]\n",
      " ...\n",
      " [-0.18042596]\n",
      " [ 0.07284399]\n",
      " [-1.37520675]]\n",
      "t [[-0.08228998]\n",
      " [-0.92198586]\n",
      " [-0.88838892]\n",
      " ...\n",
      " [-0.18042596]\n",
      " [ 0.07284399]\n",
      " [-1.37520675]]\n",
      "t [[-0.08838564]\n",
      " [-0.99078212]\n",
      " [-0.92944433]\n",
      " ...\n",
      " [-0.18478317]\n",
      " [ 0.07279862]\n",
      " [-1.4559108 ]]\n",
      "loss=34111.85316436017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.12245457]\n",
      " [-0.52311188]\n",
      " [-0.32879465]\n",
      " ...\n",
      " [-0.23829462]\n",
      " [ 0.0515921 ]\n",
      " [-0.13891323]]\n",
      "t [[ 0.12245457]\n",
      " [-0.52311188]\n",
      " [-0.32879465]\n",
      " ...\n",
      " [-0.23829462]\n",
      " [ 0.0515921 ]\n",
      " [-0.13891323]]\n",
      "t [[ 0.1748868 ]\n",
      " [-0.80594405]\n",
      " [-0.4997112 ]\n",
      " ...\n",
      " [-0.34447311]\n",
      " [ 0.05577552]\n",
      " [-0.24141903]]\n",
      "t [[ 0.1748868 ]\n",
      " [-0.80594405]\n",
      " [-0.4997112 ]\n",
      " ...\n",
      " [-0.34447311]\n",
      " [ 0.05577552]\n",
      " [-0.24141903]]\n",
      "Current iteration=2, loss=37112.259535330086\n",
      "t [[ 0.20012654]\n",
      " [-0.99451922]\n",
      " [-0.61399463]\n",
      " ...\n",
      " [-0.40765326]\n",
      " [ 0.04088022]\n",
      " [-0.32336537]]\n",
      "t [[ 0.20012654]\n",
      " [-0.99451922]\n",
      " [-0.61399463]\n",
      " ...\n",
      " [-0.40765326]\n",
      " [ 0.04088022]\n",
      " [-0.32336537]]\n",
      "t [[ 0.212975  ]\n",
      " [-1.13633663]\n",
      " [-0.70294397]\n",
      " ...\n",
      " [-0.45473901]\n",
      " [ 0.01671708]\n",
      " [-0.39131927]]\n",
      "t [[ 0.212975  ]\n",
      " [-1.13633663]\n",
      " [-0.70294397]\n",
      " ...\n",
      " [-0.45473901]\n",
      " [ 0.01671708]\n",
      " [-0.39131927]]\n",
      "Current iteration=4, loss=35811.12359281747\n",
      "t [[ 0.21950044]\n",
      " [-1.25033768]\n",
      " [-0.77793647]\n",
      " ...\n",
      " [-0.49488277]\n",
      " [-0.01228968]\n",
      " [-0.44879902]]\n",
      "t [[ 0.21950044]\n",
      " [-1.25033768]\n",
      " [-0.77793647]\n",
      " ...\n",
      " [-0.49488277]\n",
      " [-0.01228968]\n",
      " [-0.44879902]]\n",
      "t [[ 0.22252963]\n",
      " [-1.34547343]\n",
      " [-0.84366729]\n",
      " ...\n",
      " [-0.53138158]\n",
      " [-0.04374556]\n",
      " [-0.49803651]]\n",
      "t [[ 0.22252963]\n",
      " [-1.34547343]\n",
      " [-0.84366729]\n",
      " ...\n",
      " [-0.53138158]\n",
      " [-0.04374556]\n",
      " [-0.49803651]]\n",
      "Current iteration=6, loss=35038.94831253234\n",
      "t [[ 0.22351579]\n",
      " [-1.42663393]\n",
      " [-0.90233545]\n",
      " ...\n",
      " [-0.56544482]\n",
      " [-0.07621435]\n",
      " [-0.54059313]]\n",
      "t [[ 0.22351579]\n",
      " [-1.42663393]\n",
      " [-0.90233545]\n",
      " ...\n",
      " [-0.56544482]\n",
      " [-0.07621435]\n",
      " [-0.54059313]]\n",
      "t [[ 0.22327101]\n",
      " [-1.49685032]\n",
      " [-0.95514406]\n",
      " ...\n",
      " [-0.59752659]\n",
      " [-0.10878736]\n",
      " [-0.57762722]]\n",
      "t [[ 0.22327101]\n",
      " [-1.49685032]\n",
      " [-0.95514406]\n",
      " ...\n",
      " [-0.59752659]\n",
      " [-0.10878736]\n",
      " [-0.57762722]]\n",
      "Current iteration=8, loss=34535.042287752105\n",
      "t [[ 0.22228065]\n",
      " [-1.55819662]\n",
      " [-1.00287018]\n",
      " ...\n",
      " [-0.62781476]\n",
      " [-0.1408776 ]\n",
      " [-0.61003065]]\n",
      "t [[ 0.22228065]\n",
      " [-1.55819662]\n",
      " [-1.00287018]\n",
      " ...\n",
      " [-0.62781476]\n",
      " [-0.1408776 ]\n",
      " [-0.61003065]]\n",
      "t [[ 0.22084933]\n",
      " [-1.61219314]\n",
      " [-1.04608955]\n",
      " ...\n",
      " [-0.65641048]\n",
      " [-0.17210753]\n",
      " [-0.63850755]]\n",
      "loss=34184.21179614331\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01614853]\n",
      " [-0.09942433]\n",
      " [-0.31170564]\n",
      " ...\n",
      " [-0.23919614]\n",
      " [ 0.04650136]\n",
      " [-0.14190177]]\n",
      "t [[ 0.01614853]\n",
      " [-0.09942433]\n",
      " [-0.31170564]\n",
      " ...\n",
      " [-0.23919614]\n",
      " [ 0.04650136]\n",
      " [-0.14190177]]\n",
      "t [[ 0.00831292]\n",
      " [-0.23561789]\n",
      " [-0.4614034 ]\n",
      " ...\n",
      " [-0.34347505]\n",
      " [ 0.04674433]\n",
      " [-0.24658118]]\n",
      "t [[ 0.00831292]\n",
      " [-0.23561789]\n",
      " [-0.4614034 ]\n",
      " ...\n",
      " [-0.34347505]\n",
      " [ 0.04674433]\n",
      " [-0.24658118]]\n",
      "Current iteration=2, loss=37072.35924734554\n",
      "t [[-0.00469438]\n",
      " [-0.37106133]\n",
      " [-0.55747946]\n",
      " ...\n",
      " [-0.40472289]\n",
      " [ 0.02909143]\n",
      " [-0.33036787]]\n",
      "t [[-0.00469438]\n",
      " [-0.37106133]\n",
      " [-0.55747946]\n",
      " ...\n",
      " [-0.40472289]\n",
      " [ 0.02909143]\n",
      " [-0.33036787]]\n",
      "t [[-0.01782265]\n",
      " [-0.49668982]\n",
      " [-0.63200164]\n",
      " ...\n",
      " [-0.45009271]\n",
      " [ 0.00303856]\n",
      " [-0.39993261]]\n",
      "t [[-0.01782265]\n",
      " [-0.49668982]\n",
      " [-0.63200164]\n",
      " ...\n",
      " [-0.45009271]\n",
      " [ 0.00303856]\n",
      " [-0.39993261]]\n",
      "Current iteration=4, loss=35746.23666974114\n",
      "t [[-0.02967825]\n",
      " [-0.61090489]\n",
      " [-0.69594285]\n",
      " ...\n",
      " [-0.48869693]\n",
      " [-0.02723971]\n",
      " [-0.45883936]]\n",
      "t [[-0.02967825]\n",
      " [-0.61090489]\n",
      " [-0.69594285]\n",
      " ...\n",
      " [-0.48869693]\n",
      " [-0.02723971]\n",
      " [-0.45883936]]\n",
      "t [[-0.03998745]\n",
      " [-0.71425438]\n",
      " [-0.753423  ]\n",
      " ...\n",
      " [-0.52378261]\n",
      " [-0.05952823]\n",
      " [-0.50935068]]\n",
      "t [[-0.03998745]\n",
      " [-0.71425438]\n",
      " [-0.753423  ]\n",
      " ...\n",
      " [-0.52378261]\n",
      " [-0.05952823]\n",
      " [-0.50935068]]\n",
      "Current iteration=6, loss=34959.1021185172\n",
      "t [[-0.04882735]\n",
      " [-0.80784121]\n",
      " [-0.80612152]\n",
      " ...\n",
      " [-0.5565308 ]\n",
      " [-0.0925182 ]\n",
      " [-0.55305185]]\n",
      "t [[-0.04882735]\n",
      " [-0.80784121]\n",
      " [-0.80612152]\n",
      " ...\n",
      " [-0.5565308 ]\n",
      " [-0.0925182 ]\n",
      " [-0.55305185]]\n",
      "t [[-0.05636801]\n",
      " [-0.89281303]\n",
      " [-0.8548162 ]\n",
      " ...\n",
      " [-0.58738312]\n",
      " [-0.12539176]\n",
      " [-0.59112045]]\n",
      "t [[-0.05636801]\n",
      " [-0.89281303]\n",
      " [-0.8548162 ]\n",
      " ...\n",
      " [-0.58738312]\n",
      " [-0.12539176]\n",
      " [-0.59112045]]\n",
      "Current iteration=8, loss=34446.304514100964\n",
      "t [[-0.06278657]\n",
      " [-0.97020986]\n",
      " [-0.89994468]\n",
      " ...\n",
      " [-0.61652278]\n",
      " [-0.15762717]\n",
      " [-0.62446392]]\n",
      "t [[-0.06278657]\n",
      " [-0.97020986]\n",
      " [-0.89994468]\n",
      " ...\n",
      " [-0.61652278]\n",
      " [-0.15762717]\n",
      " [-0.62446392]]\n",
      "t [[-0.06824181]\n",
      " [-1.04093319]\n",
      " [-0.9418121 ]\n",
      " ...\n",
      " [-0.64404917]\n",
      " [-0.1888941 ]\n",
      " [-0.65379914]]\n",
      "loss=34090.06327104874\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01379263]\n",
      " [-0.08726074]\n",
      " [-0.31508152]\n",
      " ...\n",
      " [-0.23519603]\n",
      " [ 0.04999804]\n",
      " [-0.13310447]]\n",
      "t [[ 0.01379263]\n",
      " [-0.08726074]\n",
      " [-0.31508152]\n",
      " ...\n",
      " [-0.23519603]\n",
      " [ 0.04999804]\n",
      " [-0.13310447]]\n",
      "t [[ 0.00382925]\n",
      " [-0.21306971]\n",
      " [-0.4684231 ]\n",
      " ...\n",
      " [-0.33728561]\n",
      " [ 0.05324114]\n",
      " [-0.2302509 ]]\n",
      "t [[ 0.00382925]\n",
      " [-0.21306971]\n",
      " [-0.4684231 ]\n",
      " ...\n",
      " [-0.33728561]\n",
      " [ 0.05324114]\n",
      " [-0.2302509 ]]\n",
      "Current iteration=2, loss=37112.494977825365\n",
      "t [[-0.01096079]\n",
      " [-0.33966719]\n",
      " [-0.56750811]\n",
      " ...\n",
      " [-0.39679907]\n",
      " [ 0.03804879]\n",
      " [-0.307407  ]]\n",
      "t [[-0.01096079]\n",
      " [-0.33966719]\n",
      " [-0.56750811]\n",
      " ...\n",
      " [-0.39679907]\n",
      " [ 0.03804879]\n",
      " [-0.307407  ]]\n",
      "t [[-0.02550017]\n",
      " [-0.45765209]\n",
      " [-0.64448976]\n",
      " ...\n",
      " [-0.44071111]\n",
      " [ 0.01397845]\n",
      " [-0.37107739]]\n",
      "t [[-0.02550017]\n",
      " [-0.45765209]\n",
      " [-0.64448976]\n",
      " ...\n",
      " [-0.44071111]\n",
      " [ 0.01397845]\n",
      " [-0.37107739]]\n",
      "Current iteration=4, loss=35811.40892135943\n",
      "t [[-0.03842603]\n",
      " [-0.5651844 ]\n",
      " [-0.71047105]\n",
      " ...\n",
      " [-0.4781001 ]\n",
      " [-0.01473136]\n",
      " [-0.42471416]]\n",
      "t [[-0.03842603]\n",
      " [-0.5651844 ]\n",
      " [-0.71047105]\n",
      " ...\n",
      " [-0.4781001 ]\n",
      " [-0.01473136]\n",
      " [-0.42471416]]\n",
      "t [[-0.04951394]\n",
      " [-0.66263624]\n",
      " [-0.76967009]\n",
      " ...\n",
      " [-0.51219263]\n",
      " [-0.0458005 ]\n",
      " [-0.47049294]]\n",
      "t [[-0.04951394]\n",
      " [-0.66263624]\n",
      " [-0.76967009]\n",
      " ...\n",
      " [-0.51219263]\n",
      " [-0.0458005 ]\n",
      " [-0.47049294]]\n",
      "Current iteration=6, loss=35040.05530293072\n",
      "t [[-0.05889172]\n",
      " [-0.75097797]\n",
      " [-0.82383576]\n",
      " ...\n",
      " [-0.54414582]\n",
      " [-0.07785773]\n",
      " [-0.50992796]]\n",
      "t [[-0.05889172]\n",
      " [-0.75097797]\n",
      " [-0.82383576]\n",
      " ...\n",
      " [-0.54414582]\n",
      " [-0.07785773]\n",
      " [-0.50992796]]\n",
      "t [[-0.06677506]\n",
      " [-0.8312544 ]\n",
      " [-0.87379543]\n",
      " ...\n",
      " [-0.57437482]\n",
      " [-0.11002877]\n",
      " [-0.54413774]]\n",
      "t [[-0.06677506]\n",
      " [-0.8312544 ]\n",
      " [-0.87379543]\n",
      " ...\n",
      " [-0.57437482]\n",
      " [-0.11002877]\n",
      " [-0.54413774]]\n",
      "Current iteration=8, loss=34537.2165294882\n",
      "t [[-0.07337998]\n",
      " [-0.90442419]\n",
      " [-0.92002304]\n",
      " ...\n",
      " [-0.60303661]\n",
      " [-0.1417433 ]\n",
      " [-0.57398013]]\n",
      "t [[-0.07337998]\n",
      " [-0.90442419]\n",
      " [-0.92002304]\n",
      " ...\n",
      " [-0.60303661]\n",
      " [-0.1417433 ]\n",
      " [-0.57398013]]\n",
      "t [[-0.0788974 ]\n",
      " [-0.97132345]\n",
      " [-0.96285113]\n",
      " ...\n",
      " [-0.63020636]\n",
      " [-0.17263055]\n",
      " [-0.60013009]]\n",
      "loss=34187.39084637695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01148649]\n",
      " [-0.09242418]\n",
      " [-0.31300251]\n",
      " ...\n",
      " [-0.13780696]\n",
      " [-0.01659712]\n",
      " [-0.28114852]]\n",
      "t [[ 0.01148649]\n",
      " [-0.09242418]\n",
      " [-0.31300251]\n",
      " ...\n",
      " [-0.13780696]\n",
      " [-0.01659712]\n",
      " [-0.28114852]]\n",
      "t [[-3.08766096e-04]\n",
      " [-2.23081752e-01]\n",
      " [-4.62975385e-01]\n",
      " ...\n",
      " [-1.71135589e-01]\n",
      " [ 1.41020603e-03]\n",
      " [-5.01060552e-01]]\n",
      "t [[-3.08766096e-04]\n",
      " [-2.23081752e-01]\n",
      " [-4.62975385e-01]\n",
      " ...\n",
      " [-1.71135589e-01]\n",
      " [ 1.41020603e-03]\n",
      " [-5.01060552e-01]]\n",
      "Current iteration=2, loss=37058.02228404287\n",
      "t [[-0.01654126]\n",
      " [-0.35405679]\n",
      " [-0.55873056]\n",
      " ...\n",
      " [-0.17631632]\n",
      " [ 0.02319958]\n",
      " [-0.6838235 ]]\n",
      "t [[-0.01654126]\n",
      " [-0.35405679]\n",
      " [-0.55873056]\n",
      " ...\n",
      " [-0.17631632]\n",
      " [ 0.02319958]\n",
      " [-0.6838235 ]]\n",
      "t [[-0.03224266]\n",
      " [-0.47598048]\n",
      " [-0.63267007]\n",
      " ...\n",
      " [-0.17484183]\n",
      " [ 0.04135835]\n",
      " [-0.8404971 ]]\n",
      "t [[-0.03224266]\n",
      " [-0.47598048]\n",
      " [-0.63267007]\n",
      " ...\n",
      " [-0.17484183]\n",
      " [ 0.04135835]\n",
      " [-0.8404971 ]]\n",
      "Current iteration=4, loss=35732.76469672381\n",
      "t [[-0.04612896]\n",
      " [-0.58705473]\n",
      " [-0.69593857]\n",
      " ...\n",
      " [-0.17314876]\n",
      " [ 0.05467186]\n",
      " [-0.97736832]]\n",
      "t [[-0.04612896]\n",
      " [-0.58705473]\n",
      " [-0.69593857]\n",
      " ...\n",
      " [-0.17314876]\n",
      " [ 0.05467186]\n",
      " [-0.97736832]]\n",
      "t [[-0.05802937]\n",
      " [-0.68769491]\n",
      " [-0.75273771]\n",
      " ...\n",
      " [-0.17294825]\n",
      " [ 0.06364974]\n",
      " [-1.09853456]]\n",
      "t [[-0.05802937]\n",
      " [-0.68769491]\n",
      " [-0.75273771]\n",
      " ...\n",
      " [-0.17294825]\n",
      " [ 0.06364974]\n",
      " [-1.09853456]]\n",
      "Current iteration=6, loss=34945.804268287204\n",
      "t [[-0.06810783]\n",
      " [-0.77891357]\n",
      " [-0.80478259]\n",
      " ...\n",
      " [-0.17441539]\n",
      " [ 0.06917863]\n",
      " [-1.20688991]]\n",
      "t [[-0.06810783]\n",
      " [-0.77891357]\n",
      " [-0.80478259]\n",
      " ...\n",
      " [-0.17441539]\n",
      " [ 0.06917863]\n",
      " [-1.20688991]]\n",
      "t [[-0.07660456]\n",
      " [-0.86179432]\n",
      " [-0.85286447]\n",
      " ...\n",
      " [-0.17727388]\n",
      " [ 0.07210567]\n",
      " [-1.30458435]]\n",
      "t [[-0.07660456]\n",
      " [-0.86179432]\n",
      " [-0.85286447]\n",
      " ...\n",
      " [-0.17727388]\n",
      " [ 0.07210567]\n",
      " [-1.30458435]]\n",
      "Current iteration=8, loss=34432.755041047945\n",
      "t [[-0.08375246]\n",
      " [-0.93733007]\n",
      " [-0.89742427]\n",
      " ...\n",
      " [-0.18116629]\n",
      " [ 0.07313104]\n",
      " [-1.39327024]]\n",
      "t [[-0.08375246]\n",
      " [-0.93733007]\n",
      " [-0.89742427]\n",
      " ...\n",
      " [-0.18116629]\n",
      " [ 0.07313104]\n",
      " [-1.39327024]]\n",
      "t [[-0.08975436]\n",
      " [-1.00638642]\n",
      " [-0.93876605]\n",
      " ...\n",
      " [-0.18577072]\n",
      " [ 0.07280179]\n",
      " [-1.4742491 ]]\n",
      "loss=34076.13680942983\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.12517578]\n",
      " [-0.53473659]\n",
      " [-0.33610119]\n",
      " ...\n",
      " [-0.24359005]\n",
      " [ 0.05273859]\n",
      " [-0.14200019]]\n",
      "t [[ 0.12517578]\n",
      " [-0.53473659]\n",
      " [-0.33610119]\n",
      " ...\n",
      " [-0.24359005]\n",
      " [ 0.05273859]\n",
      " [-0.14200019]]\n",
      "t [[ 0.17724868]\n",
      " [-0.8186287 ]\n",
      " [-0.50738556]\n",
      " ...\n",
      " [-0.34925405]\n",
      " [ 0.05599325]\n",
      " [-0.24598414]]\n",
      "t [[ 0.17724868]\n",
      " [-0.8186287 ]\n",
      " [-0.50738556]\n",
      " ...\n",
      " [-0.34925405]\n",
      " [ 0.05599325]\n",
      " [-0.24598414]]\n",
      "Current iteration=2, loss=37069.52195945039\n",
      "t [[ 0.20184517]\n",
      " [-1.00718914]\n",
      " [-0.62167796]\n",
      " ...\n",
      " [-0.41191545]\n",
      " [ 0.03991538]\n",
      " [-0.32883502]]\n",
      "t [[ 0.20184517]\n",
      " [-1.00718914]\n",
      " [-0.62167796]\n",
      " ...\n",
      " [-0.41191545]\n",
      " [ 0.03991538]\n",
      " [-0.32883502]]\n",
      "t [[ 0.21415158]\n",
      " [-1.14900528]\n",
      " [-0.71089045]\n",
      " ...\n",
      " [-0.45895513]\n",
      " [ 0.01458631]\n",
      " [-0.39735991]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.21415158]\n",
      " [-1.14900528]\n",
      " [-0.71089045]\n",
      " ...\n",
      " [-0.45895513]\n",
      " [ 0.01458631]\n",
      " [-0.39735991]]\n",
      "Current iteration=4, loss=35765.22971982908\n",
      "t [[ 0.22026052]\n",
      " [-1.26304529]\n",
      " [-0.78629444]\n",
      " ...\n",
      " [-0.4993632 ]\n",
      " [-0.01550315]\n",
      " [-0.45518036]]\n",
      "t [[ 0.22026052]\n",
      " [-1.26304529]\n",
      " [-0.78629444]\n",
      " ...\n",
      " [-0.4993632 ]\n",
      " [-0.01550315]\n",
      " [-0.45518036]]\n",
      "t [[ 0.22297033]\n",
      " [-1.35818579]\n",
      " [-0.85244818]\n",
      " ...\n",
      " [-0.53626212]\n",
      " [-0.04793418]\n",
      " [-0.50459133]]\n",
      "t [[ 0.22297033]\n",
      " [-1.35818579]\n",
      " [-0.85244818]\n",
      " ...\n",
      " [-0.53626212]\n",
      " [-0.04793418]\n",
      " [-0.50459133]]\n",
      "Current iteration=6, loss=34996.22598645912\n",
      "t [[ 0.22370815]\n",
      " [-1.43927656]\n",
      " [-0.91147172]\n",
      " ...\n",
      " [-0.5707533 ]\n",
      " [-0.08126116]\n",
      " [-0.54719831]]\n",
      "t [[ 0.22370815]\n",
      " [-1.43927656]\n",
      " [-0.91147172]\n",
      " ...\n",
      " [-0.5707533 ]\n",
      " [-0.08126116]\n",
      " [-0.54719831]]\n",
      "t [[ 0.22326855]\n",
      " [-1.50934196]\n",
      " [-0.96453609]\n",
      " ...\n",
      " [-0.60323592]\n",
      " [-0.11457417]\n",
      " [-0.5841919 ]]\n",
      "t [[ 0.22326855]\n",
      " [-1.50934196]\n",
      " [-0.96453609]\n",
      " ...\n",
      " [-0.60323592]\n",
      " [-0.11457417]\n",
      " [-0.5841919 ]]\n",
      "Current iteration=8, loss=34496.83516921988\n",
      "t [[ 0.2221255 ]\n",
      " [-1.57046586]\n",
      " [-1.01241279]\n",
      " ...\n",
      " [-0.63387442]\n",
      " [-0.14729039]\n",
      " [-0.61648783]]\n",
      "t [[ 0.2221255 ]\n",
      " [-1.57046586]\n",
      " [-1.01241279]\n",
      " ...\n",
      " [-0.63387442]\n",
      " [-0.14729039]\n",
      " [-0.61648783]]\n",
      "t [[ 0.22057572]\n",
      " [-1.62418366]\n",
      " [-1.05568453]\n",
      " ...\n",
      " [-0.6627631 ]\n",
      " [-0.17903996]\n",
      " [-0.64480802]]\n",
      "loss=34150.27645694691\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01650739]\n",
      " [-0.10163376]\n",
      " [-0.31863243]\n",
      " ...\n",
      " [-0.24451161]\n",
      " [ 0.04753473]\n",
      " [-0.14505514]]\n",
      "t [[ 0.01650739]\n",
      " [-0.10163376]\n",
      " [-0.31863243]\n",
      " ...\n",
      " [-0.24451161]\n",
      " [ 0.04753473]\n",
      " [-0.14505514]]\n",
      "t [[ 0.00798607]\n",
      " [-0.24164068]\n",
      " [-0.46813809]\n",
      " ...\n",
      " [-0.34817522]\n",
      " [ 0.04678786]\n",
      " [-0.2512436 ]]\n",
      "t [[ 0.00798607]\n",
      " [-0.24164068]\n",
      " [-0.46813809]\n",
      " ...\n",
      " [-0.34817522]\n",
      " [ 0.04678786]\n",
      " [-0.2512436 ]]\n",
      "Current iteration=2, loss=37028.91854181399\n",
      "t [[-0.00554186]\n",
      " [-0.38006957]\n",
      " [-0.56394755]\n",
      " ...\n",
      " [-0.40885742]\n",
      " [ 0.02794269]\n",
      " [-0.33596084]]\n",
      "t [[-0.00554186]\n",
      " [-0.38006957]\n",
      " [-0.56394755]\n",
      " ...\n",
      " [-0.40885742]\n",
      " [ 0.02794269]\n",
      " [-0.33596084]]\n",
      "t [[-0.01897438]\n",
      " [-0.50784581]\n",
      " [-0.63866278]\n",
      " ...\n",
      " [-0.45415662]\n",
      " [ 0.00073957]\n",
      " [-0.40611732]]\n",
      "t [[-0.01897438]\n",
      " [-0.50784581]\n",
      " [-0.63866278]\n",
      " ...\n",
      " [-0.45415662]\n",
      " [ 0.00073957]\n",
      " [-0.40611732]]\n",
      "Current iteration=4, loss=35699.42742423358\n",
      "t [[-0.03098267]\n",
      " [-0.62358861]\n",
      " [-0.70307036]\n",
      " ...\n",
      " [-0.4930066 ]\n",
      " [-0.03059458]\n",
      " [-0.46538051]]\n",
      "t [[-0.03098267]\n",
      " [-0.62358861]\n",
      " [-0.70307036]\n",
      " ...\n",
      " [-0.4930066 ]\n",
      " [-0.03059458]\n",
      " [-0.46538051]]\n",
      "t [[-0.04135007]\n",
      " [-0.72802758]\n",
      " [-0.76110212]\n",
      " ...\n",
      " [-0.52847517]\n",
      " [-0.06382786]\n",
      " [-0.51607695]]\n",
      "t [[-0.04135007]\n",
      " [-0.72802758]\n",
      " [-0.76110212]\n",
      " ...\n",
      " [-0.52847517]\n",
      " [-0.06382786]\n",
      " [-0.51607695]]\n",
      "Current iteration=6, loss=34915.57267213152\n",
      "t [[-0.05019062]\n",
      " [-0.82239007]\n",
      " [-0.81432837]\n",
      " ...\n",
      " [-0.56163529]\n",
      " [-0.09764603]\n",
      " [-0.55983692]]\n",
      "t [[-0.05019062]\n",
      " [-0.82239007]\n",
      " [-0.81432837]\n",
      " ...\n",
      " [-0.56163529]\n",
      " [-0.09764603]\n",
      " [-0.55983692]]\n",
      "t [[-0.05769634]\n",
      " [-0.90790599]\n",
      " [-0.86347693]\n",
      " ...\n",
      " [-0.59287458]\n",
      " [-0.131232  ]\n",
      " [-0.59787101]]\n",
      "t [[-0.05769634]\n",
      " [-0.90790599]\n",
      " [-0.86347693]\n",
      " ...\n",
      " [-0.59287458]\n",
      " [-0.131232  ]\n",
      " [-0.59787101]]\n",
      "Current iteration=8, loss=34407.45205731731\n",
      "t [[-0.06405737]\n",
      " [-0.98567077]\n",
      " [-0.90896865]\n",
      " ...\n",
      " [-0.62235359]\n",
      " [-0.16406912]\n",
      " [-0.63111113]]\n",
      "t [[-0.06405737]\n",
      " [-0.98567077]\n",
      " [-0.90896865]\n",
      " ...\n",
      " [-0.62235359]\n",
      " [-0.16406912]\n",
      " [-0.63111113]]\n",
      "t [[-0.06944033]\n",
      " [-1.05662478]\n",
      " [-0.95110831]\n",
      " ...\n",
      " [-0.65016519]\n",
      " [-0.19583505]\n",
      " [-0.66029235]]\n",
      "loss=34055.633891689045\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01409913]\n",
      " [-0.08919987]\n",
      " [-0.32208333]\n",
      " ...\n",
      " [-0.24042261]\n",
      " [ 0.05110911]\n",
      " [-0.13606234]]\n",
      "t [[ 0.01409913]\n",
      " [-0.08919987]\n",
      " [-0.32208333]\n",
      " ...\n",
      " [-0.24042261]\n",
      " [ 0.05110911]\n",
      " [-0.13606234]]\n",
      "t [[ 0.00340722]\n",
      " [-0.21863271]\n",
      " [-0.47531667]\n",
      " ...\n",
      " [-0.34188645]\n",
      " [ 0.05341642]\n",
      " [-0.23457867]]\n",
      "t [[ 0.00340722]\n",
      " [-0.21863271]\n",
      " [-0.47531667]\n",
      " ...\n",
      " [-0.34188645]\n",
      " [ 0.05341642]\n",
      " [-0.23457867]]\n",
      "Current iteration=2, loss=37069.77006019396\n",
      "t [[-0.01192775]\n",
      " [-0.34808643]\n",
      " [-0.57417566]\n",
      " ...\n",
      " [-0.40081763]\n",
      " [ 0.03706254]\n",
      " [-0.31255784]]\n",
      "t [[-0.01192775]\n",
      " [-0.34808643]\n",
      " [-0.57417566]\n",
      " ...\n",
      " [-0.40081763]\n",
      " [ 0.03706254]\n",
      " [-0.31255784]]\n",
      "t [[-0.02677785]\n",
      " [-0.46812843]\n",
      " [-0.65136958]\n",
      " ...\n",
      " [-0.444646  ]\n",
      " [ 0.0118542 ]\n",
      " [-0.37673755]]\n",
      "t [[-0.02677785]\n",
      " [-0.46812843]\n",
      " [-0.65136958]\n",
      " ...\n",
      " [-0.444646  ]\n",
      " [ 0.0118542 ]\n",
      " [-0.37673755]]\n",
      "Current iteration=4, loss=35765.53038714416\n",
      "t [[-0.03984965]\n",
      " [-0.57712467]\n",
      " [-0.7178257 ]\n",
      " ...\n",
      " [-0.4822756 ]\n",
      " [-0.01791347]\n",
      " [-0.43066865]]\n",
      "t [[-0.03984965]\n",
      " [-0.57712467]\n",
      " [-0.7178257 ]\n",
      " ...\n",
      " [-0.4822756 ]\n",
      " [-0.01791347]\n",
      " [-0.43066865]]\n",
      "t [[-0.0509804 ]\n",
      " [-0.67562166]\n",
      " [-0.77757889]\n",
      " ...\n",
      " [-0.51675373]\n",
      " [-0.04993909]\n",
      " [-0.47658671]]\n",
      "t [[-0.0509804 ]\n",
      " [-0.67562166]\n",
      " [-0.77757889]\n",
      " ...\n",
      " [-0.51675373]\n",
      " [-0.04993909]\n",
      " [-0.47658671]]\n",
      "Current iteration=6, loss=34997.39958740269\n",
      "t [[-0.06033836]\n",
      " [-0.76470915]\n",
      " [-0.83227119]\n",
      " ...\n",
      " [-0.54912763]\n",
      " [-0.08284199]\n",
      " [-0.51604773]]\n",
      "t [[-0.06033836]\n",
      " [-0.76470915]\n",
      " [-0.83227119]\n",
      " ...\n",
      " [-0.54912763]\n",
      " [-0.08284199]\n",
      " [-0.51604773]]\n",
      "t [[-0.06816375]\n",
      " [-0.84551065]\n",
      " [-0.88268115]\n",
      " ...\n",
      " [-0.57975648]\n",
      " [-0.11574555]\n",
      " [-0.5502006 ]]\n",
      "t [[-0.06816375]\n",
      " [-0.84551065]\n",
      " [-0.88268115]\n",
      " ...\n",
      " [-0.57975648]\n",
      " [-0.11574555]\n",
      " [-0.5502006 ]]\n",
      "Current iteration=8, loss=34499.10420620534\n",
      "t [[-0.07468723]\n",
      " [-0.91903765]\n",
      " [-0.92926666]\n",
      " ...\n",
      " [-0.60877286]\n",
      " [-0.14808236]\n",
      " [-0.57992558]]\n",
      "t [[-0.07468723]\n",
      " [-0.91903765]\n",
      " [-0.92926666]\n",
      " ...\n",
      " [-0.60877286]\n",
      " [-0.14808236]\n",
      " [-0.57992558]]\n",
      "t [[-0.08010871]\n",
      " [-0.98616327]\n",
      " [-0.97236034]\n",
      " ...\n",
      " [-0.63624414]\n",
      " [-0.1794882 ]\n",
      " [-0.60591417]]\n",
      "loss=34153.561932662255\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01174174]\n",
      " [-0.09447805]\n",
      " [-0.31995813]\n",
      " ...\n",
      " [-0.14086934]\n",
      " [-0.01696594]\n",
      " [-0.28739626]]\n",
      "t [[ 0.01174174]\n",
      " [-0.09447805]\n",
      " [-0.31995813]\n",
      " ...\n",
      " [-0.14086934]\n",
      " [-0.01696594]\n",
      " [-0.28739626]]\n",
      "t [[-0.00081218]\n",
      " [-0.22885832]\n",
      " [-0.46972276]\n",
      " ...\n",
      " [-0.17266956]\n",
      " [ 0.00219576]\n",
      " [-0.51084676]]\n",
      "t [[-0.00081218]\n",
      " [-0.22885832]\n",
      " [-0.46972276]\n",
      " ...\n",
      " [-0.17266956]\n",
      " [ 0.00219576]\n",
      " [-0.51084676]]\n",
      "Current iteration=2, loss=37014.57209152647\n",
      "t [[-0.01760441]\n",
      " [-0.36276658]\n",
      " [-0.56517855]\n",
      " ...\n",
      " [-0.17669895]\n",
      " [ 0.02464776]\n",
      " [-0.69602637]]\n",
      "t [[-0.01760441]\n",
      " [-0.36276658]\n",
      " [-0.56517855]\n",
      " ...\n",
      " [-0.17669895]\n",
      " [ 0.02464776]\n",
      " [-0.69602637]]\n",
      "t [[-0.03362353]\n",
      " [-0.48680638]\n",
      " [-0.63928073]\n",
      " ...\n",
      " [-0.17472923]\n",
      " [ 0.04297686]\n",
      " [-0.85444434]]\n",
      "t [[-0.03362353]\n",
      " [-0.48680638]\n",
      " [-0.63928073]\n",
      " ...\n",
      " [-0.17472923]\n",
      " [ 0.04297686]\n",
      " [-0.85444434]]\n",
      "Current iteration=4, loss=35685.98633194591\n",
      "t [[-0.04765901]\n",
      " [-0.59938848]\n",
      " [-0.7029925 ]\n",
      " ...\n",
      " [-0.17296976]\n",
      " [ 0.05615976]\n",
      " [-0.9925977 ]]\n",
      "t [[-0.04765901]\n",
      " [-0.59938848]\n",
      " [-0.7029925 ]\n",
      " ...\n",
      " [-0.17296976]\n",
      " [ 0.05615976]\n",
      " [-0.9925977 ]]\n",
      "t [[-0.05960366]\n",
      " [-0.7011057 ]\n",
      " [-0.76032706]\n",
      " ...\n",
      " [-0.17292612]\n",
      " [ 0.06485756]\n",
      " [-1.11471088]]\n",
      "t [[-0.05960366]\n",
      " [-0.7011057 ]\n",
      " [-0.76032706]\n",
      " ...\n",
      " [-0.17292612]\n",
      " [ 0.06485756]\n",
      " [-1.11471088]]\n",
      "Current iteration=6, loss=34902.27391936733\n",
      "t [[-0.0696628 ]\n",
      " [-0.79309275]\n",
      " [-0.81288866]\n",
      " ...\n",
      " [-0.17464551]\n",
      " [ 0.07005083]\n",
      " [-1.22376374]]\n",
      "t [[-0.0696628 ]\n",
      " [-0.79309275]\n",
      " [-0.81288866]\n",
      " ...\n",
      " [-0.17464551]\n",
      " [ 0.07005083]\n",
      " [-1.22376374]]\n",
      "t [[-0.07810147]\n",
      " [-0.87651418]\n",
      " [-0.86141699]\n",
      " ...\n",
      " [-0.17778186]\n",
      " [ 0.07263928]\n",
      " [-1.32196694]]\n",
      "t [[-0.07810147]\n",
      " [-0.87651418]\n",
      " [-0.86141699]\n",
      " ...\n",
      " [-0.17778186]\n",
      " [ 0.07263928]\n",
      " [-1.32196694]]\n",
      "Current iteration=8, loss=34393.87707544716\n",
      "t [[-0.08516735]\n",
      " [-0.95241742]\n",
      " [-0.90633509]\n",
      " ...\n",
      " [-0.18194266]\n",
      " [ 0.07335039]\n",
      " [-1.41101684]]\n",
      "t [[-0.08516735]\n",
      " [-0.95241742]\n",
      " [-0.90633509]\n",
      " ...\n",
      " [-0.18194266]\n",
      " [ 0.07335039]\n",
      " [-1.41101684]]\n",
      "t [[-0.09107227]\n",
      " [-1.0217063 ]\n",
      " [-0.9479459 ]\n",
      " ...\n",
      " [-0.18678993]\n",
      " [ 0.07274391]\n",
      " [-1.49224743]]\n",
      "loss=34041.667678133905\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.12789699]\n",
      " [-0.5463613 ]\n",
      " [-0.34340774]\n",
      " ...\n",
      " [-0.24888549]\n",
      " [ 0.05388508]\n",
      " [-0.14508715]]\n",
      "t [[ 0.12789699]\n",
      " [-0.5463613 ]\n",
      " [-0.34340774]\n",
      " ...\n",
      " [-0.24888549]\n",
      " [ 0.05388508]\n",
      " [-0.14508715]]\n",
      "t [[ 0.17954841]\n",
      " [-0.83110061]\n",
      " [-0.51492041]\n",
      " ...\n",
      " [-0.35391791]\n",
      " [ 0.05616993]\n",
      " [-0.25051618]]\n",
      "t [[ 0.17954841]\n",
      " [-0.83110061]\n",
      " [-0.51492041]\n",
      " ...\n",
      " [-0.35391791]\n",
      " [ 0.05616993]\n",
      " [-0.25051618]]\n",
      "Current iteration=2, loss=37027.46860617189\n",
      "t [[ 0.20348807]\n",
      " [-1.01959261]\n",
      " [-0.62920094]\n",
      " ...\n",
      " [-0.41605566]\n",
      " [ 0.03889674]\n",
      " [-0.33424638]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.20348807]\n",
      " [-1.01959261]\n",
      " [-0.62920094]\n",
      " ...\n",
      " [-0.41605566]\n",
      " [ 0.03889674]\n",
      " [-0.33424638]]\n",
      "t [[ 0.21525838]\n",
      " [-1.16140519]\n",
      " [-0.71869079]\n",
      " ...\n",
      " [-0.46307782]\n",
      " [ 0.01240186]\n",
      " [-0.40332048]]\n",
      "t [[ 0.21525838]\n",
      " [-1.16140519]\n",
      " [-0.71869079]\n",
      " ...\n",
      " [-0.46307782]\n",
      " [ 0.01240186]\n",
      " [-0.40332048]]\n",
      "Current iteration=4, loss=35720.338454474295\n",
      "t [[ 0.22096148]\n",
      " [-1.27548691]\n",
      " [-0.79451848]\n",
      " ...\n",
      " [-0.50377634]\n",
      " [-0.01876343]\n",
      " [-0.46146179]]\n",
      "t [[ 0.22096148]\n",
      " [-1.27548691]\n",
      " [-0.79451848]\n",
      " ...\n",
      " [-0.50377634]\n",
      " [-0.01876343]\n",
      " [-0.46146179]]\n",
      "t [[ 0.2233623 ]\n",
      " [-1.37062823]\n",
      " [-0.86109618]\n",
      " ...\n",
      " [-0.54108975]\n",
      " [-0.05215813]\n",
      " [-0.51102826]]\n",
      "t [[ 0.2233623 ]\n",
      " [-1.37062823]\n",
      " [-0.86109618]\n",
      " ...\n",
      " [-0.54108975]\n",
      " [-0.05215813]\n",
      " [-0.51102826]]\n",
      "Current iteration=6, loss=34954.685797822545\n",
      "t [[ 0.2238609 ]\n",
      " [-1.45163942]\n",
      " [-0.92046621]\n",
      " ...\n",
      " [-0.57601228]\n",
      " [-0.08632824]\n",
      " [-0.5536697 ]]\n",
      "t [[ 0.2238609 ]\n",
      " [-1.45163942]\n",
      " [-0.92046621]\n",
      " ...\n",
      " [-0.57601228]\n",
      " [-0.08632824]\n",
      " [-0.5536697 ]]\n",
      "t [[ 0.22323449]\n",
      " [-1.52154127]\n",
      " [-0.97377116]\n",
      " ...\n",
      " [-0.60889154]\n",
      " [-0.12036349]\n",
      " [-0.59060906]]\n",
      "t [[ 0.22323449]\n",
      " [-1.52154127]\n",
      " [-0.97377116]\n",
      " ...\n",
      " [-0.60889154]\n",
      " [-0.12036349]\n",
      " [-0.59060906]]\n",
      "Current iteration=8, loss=34459.836328997284\n",
      "t [[ 0.22194591]\n",
      " [-1.58243017]\n",
      " [-1.02178019]\n",
      " ...\n",
      " [-0.63987157]\n",
      " [-0.15368607]\n",
      " [-0.62278591]]\n",
      "t [[ 0.22194591]\n",
      " [-1.58243017]\n",
      " [-1.02178019]\n",
      " ...\n",
      " [-0.63987157]\n",
      " [-0.15368607]\n",
      " [-0.62278591]]\n",
      "t [[ 0.22028417]\n",
      " [-1.63585807]\n",
      " [-1.06508507]\n",
      " ...\n",
      " [-0.669042  ]\n",
      " [-0.18593467]\n",
      " [-0.65093979]]\n",
      "loss=34117.50258380073\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01686624]\n",
      " [-0.10384318]\n",
      " [-0.32555923]\n",
      " ...\n",
      " [-0.24982708]\n",
      " [ 0.04856809]\n",
      " [-0.14820851]]\n",
      "t [[ 0.01686624]\n",
      " [-0.10384318]\n",
      " [-0.32555923]\n",
      " ...\n",
      " [-0.24982708]\n",
      " [ 0.04856809]\n",
      " [-0.14820851]]\n",
      "t [[ 0.00763898]\n",
      " [-0.24769473]\n",
      " [-0.47472975]\n",
      " ...\n",
      " [-0.35275606]\n",
      " [ 0.04679151]\n",
      " [-0.25587227]]\n",
      "t [[ 0.00763898]\n",
      " [-0.24769473]\n",
      " [-0.47472975]\n",
      " ...\n",
      " [-0.35275606]\n",
      " [ 0.04679151]\n",
      " [-0.25587227]]\n",
      "Current iteration=2, loss=36986.163544894735\n",
      "t [[-0.00640372]\n",
      " [-0.38907636]\n",
      " [-0.5702632 ]\n",
      " ...\n",
      " [-0.41286966]\n",
      " [ 0.02674341]\n",
      " [-0.34149459]]\n",
      "t [[-0.00640372]\n",
      " [-0.38907636]\n",
      " [-0.5702632 ]\n",
      " ...\n",
      " [-0.41286966]\n",
      " [ 0.02674341]\n",
      " [-0.34149459]]\n",
      "t [[-0.02012707]\n",
      " [-0.51894758]\n",
      " [-0.64519882]\n",
      " ...\n",
      " [-0.45812823]\n",
      " [-0.0016082 ]\n",
      " [-0.41222071]]\n",
      "t [[-0.02012707]\n",
      " [-0.51894758]\n",
      " [-0.64519882]\n",
      " ...\n",
      " [-0.45812823]\n",
      " [-0.0016082 ]\n",
      " [-0.41222071]]\n",
      "Current iteration=4, loss=35653.639666689945\n",
      "t [[-0.03227522]\n",
      " [-0.63616529]\n",
      " [-0.71009571]\n",
      " ...\n",
      " [-0.49725064]\n",
      " [-0.0339904 ]\n",
      " [-0.47182011]]\n",
      "t [[-0.03227522]\n",
      " [-0.63616529]\n",
      " [-0.71009571]\n",
      " ...\n",
      " [-0.49725064]\n",
      " [-0.0339904 ]\n",
      " [-0.47182011]]\n",
      "t [[-0.04269076]\n",
      " [-0.74164687]\n",
      " [-0.76868804]\n",
      " ...\n",
      " [-0.53311663]\n",
      " [-0.06815658]\n",
      " [-0.52268324]]\n",
      "t [[-0.04269076]\n",
      " [-0.74164687]\n",
      " [-0.76868804]\n",
      " ...\n",
      " [-0.53311663]\n",
      " [-0.06815658]\n",
      " [-0.52268324]]\n",
      "Current iteration=6, loss=34873.25199935249\n",
      "t [[-0.05152456]\n",
      " [-0.83674464]\n",
      " [-0.82243879]\n",
      " ...\n",
      " [-0.56669225]\n",
      " [-0.10278796]\n",
      " [-0.56648569]]\n",
      "t [[-0.05152456]\n",
      " [-0.83674464]\n",
      " [-0.82243879]\n",
      " ...\n",
      " [-0.56669225]\n",
      " [-0.10278796]\n",
      " [-0.56648569]]\n",
      "t [[-0.05899001]\n",
      " [-0.92276978]\n",
      " [-0.87202993]\n",
      " ...\n",
      " [-0.59831463]\n",
      " [-0.13706895]\n",
      " [-0.60447118]]\n",
      "t [[-0.05899001]\n",
      " [-0.92276978]\n",
      " [-0.87202993]\n",
      " ...\n",
      " [-0.59831463]\n",
      " [-0.13706895]\n",
      " [-0.60447118]]\n",
      "Current iteration=8, loss=34369.83547997815\n",
      "t [[-0.06528961]\n",
      " [-1.00087212]\n",
      " [-0.91786916]\n",
      " ...\n",
      " [-0.62812471]\n",
      " [-0.17048873]\n",
      " [-0.63759605]]\n",
      "t [[-0.06528961]\n",
      " [-1.00087212]\n",
      " [-0.91786916]\n",
      " ...\n",
      " [-0.62812471]\n",
      " [-0.17048873]\n",
      " [-0.63759605]]\n",
      "t [[-0.07059743]\n",
      " [-1.0720301 ]\n",
      " [-0.96026328]\n",
      " ...\n",
      " [-0.65621094]\n",
      " [-0.20273376]\n",
      " [-0.66661339]]\n",
      "loss=34022.39177769114\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01440564]\n",
      " [-0.091139  ]\n",
      " [-0.32908514]\n",
      " ...\n",
      " [-0.24564918]\n",
      " [ 0.05222018]\n",
      " [-0.13902022]]\n",
      "t [[ 0.01440564]\n",
      " [-0.091139  ]\n",
      " [-0.32908514]\n",
      " ...\n",
      " [-0.24564918]\n",
      " [ 0.05222018]\n",
      " [-0.13902022]]\n",
      "t [[ 0.00296511]\n",
      " [-0.22422877]\n",
      " [-0.48206716]\n",
      " ...\n",
      " [-0.34636945]\n",
      " [ 0.05355121]\n",
      " [-0.23887387]]\n",
      "t [[ 0.00296511]\n",
      " [-0.22422877]\n",
      " [-0.48206716]\n",
      " ...\n",
      " [-0.34636945]\n",
      " [ 0.05355121]\n",
      " [-0.23887387]]\n",
      "Current iteration=2, loss=37027.72667897431\n",
      "t [[-0.01290814]\n",
      " [-0.35650847]\n",
      " [-0.58068916]\n",
      " ...\n",
      " [-0.40471535]\n",
      " [ 0.0360242 ]\n",
      " [-0.31765198]]\n",
      "t [[-0.01290814]\n",
      " [-0.35650847]\n",
      " [-0.58068916]\n",
      " ...\n",
      " [-0.40471535]\n",
      " [ 0.0360242 ]\n",
      " [-0.31765198]]\n",
      "t [[-0.02805437]\n",
      " [-0.47855723]\n",
      " [-0.65812134]\n",
      " ...\n",
      " [-0.44849027]\n",
      " [ 0.00967844]\n",
      " [-0.38232049]]\n",
      "t [[-0.02805437]\n",
      " [-0.47855723]\n",
      " [-0.65812134]\n",
      " ...\n",
      " [-0.44849027]\n",
      " [ 0.00967844]\n",
      " [-0.38232049]]\n",
      "Current iteration=4, loss=35720.65629871094\n",
      "t [[-0.04125815]\n",
      " [-0.58896685]\n",
      " [-0.72507432]\n",
      " ...\n",
      " [-0.48638784]\n",
      " [-0.02114046]\n",
      " [-0.43652738]]\n",
      "t [[-0.04125815]\n",
      " [-0.58896685]\n",
      " [-0.72507432]\n",
      " ...\n",
      " [-0.48638784]\n",
      " [-0.02114046]\n",
      " [-0.43652738]]\n",
      "t [[-0.05242073]\n",
      " [-0.68846417]\n",
      " [-0.78538996]\n",
      " ...\n",
      " [-0.52126691]\n",
      " [-0.05411174]\n",
      " [-0.48256804]]\n",
      "t [[-0.05242073]\n",
      " [-0.68846417]\n",
      " [-0.78538996]\n",
      " ...\n",
      " [-0.52126691]\n",
      " [-0.05411174]\n",
      " [-0.48256804]]\n",
      "Current iteration=6, loss=34955.92737080339\n",
      "t [[-0.06175083]\n",
      " [-0.77825888]\n",
      " [-0.84060516]\n",
      " ...\n",
      " [-0.55406587]\n",
      " [-0.08784603]\n",
      " [-0.52204041]]\n",
      "t [[-0.06175083]\n",
      " [-0.77825888]\n",
      " [-0.84060516]\n",
      " ...\n",
      " [-0.55406587]\n",
      " [-0.08784603]\n",
      " [-0.52204041]]\n",
      "t [[-0.06951252]\n",
      " [-0.85955221]\n",
      " [-0.89145374]\n",
      " ...\n",
      " [-0.58509128]\n",
      " [-0.12146509]\n",
      " [-0.55612385]]\n",
      "t [[-0.06951252]\n",
      " [-0.85955221]\n",
      " [-0.89145374]\n",
      " ...\n",
      " [-0.58509128]\n",
      " [-0.12146509]\n",
      " [-0.55612385]]\n",
      "Current iteration=8, loss=34462.19986144171\n",
      "t [[-0.07595051]\n",
      " [-0.93340747]\n",
      " [-0.93838108]\n",
      " ...\n",
      " [-0.61445436]\n",
      " [-0.15440519]\n",
      " [-0.58572094]]\n",
      "t [[-0.07595051]\n",
      " [-0.93340747]\n",
      " [-0.93838108]\n",
      " ...\n",
      " [-0.61445436]\n",
      " [-0.15440519]\n",
      " [-0.58572094]]\n",
      "t [[-0.08127325]\n",
      " [-1.00073399]\n",
      " [-0.98172232]\n",
      " ...\n",
      " [-0.6422168 ]\n",
      " [-0.18630952]\n",
      " [-0.61153961]]\n",
      "loss=34120.893042234995\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.011997  ]\n",
      " [-0.09653192]\n",
      " [-0.32691374]\n",
      " ...\n",
      " [-0.14393172]\n",
      " [-0.01733477]\n",
      " [-0.29364401]]\n",
      "t [[ 0.011997  ]\n",
      " [-0.09653192]\n",
      " [-0.32691374]\n",
      " ...\n",
      " [-0.14393172]\n",
      " [-0.01733477]\n",
      " [-0.29364401]]\n",
      "t [[-0.00133521]\n",
      " [-0.23466749]\n",
      " [-0.47632624]\n",
      " ...\n",
      " [-0.17411132]\n",
      " [ 0.00301214]\n",
      " [-0.52057707]]\n",
      "t [[-0.00133521]\n",
      " [-0.23466749]\n",
      " [-0.47632624]\n",
      " ...\n",
      " [-0.17411132]\n",
      " [ 0.00301214]\n",
      " [-0.52057707]]\n",
      "Current iteration=2, loss=36971.812904618135\n",
      "t [[-0.01867987]\n",
      " [-0.37147788]\n",
      " [-0.57147253]\n",
      " ...\n",
      " [-0.17700097]\n",
      " [ 0.02610773]\n",
      " [-0.70812529]]\n",
      "t [[-0.01867987]\n",
      " [-0.37147788]\n",
      " [-0.57147253]\n",
      " ...\n",
      " [-0.17700097]\n",
      " [ 0.02610773]\n",
      " [-0.70812529]]\n",
      "t [[-0.03500164]\n",
      " [-0.49758234]\n",
      " [-0.64576482]\n",
      " ...\n",
      " [-0.17457656]\n",
      " [ 0.0445762 ]\n",
      " [-0.86824452]]\n",
      "t [[-0.03500164]\n",
      " [-0.49758234]\n",
      " [-0.64576482]\n",
      " ...\n",
      " [-0.17457656]\n",
      " [ 0.0445762 ]\n",
      " [-0.86824452]]\n",
      "Current iteration=4, loss=35640.22813869985\n",
      "t [[-0.04917199]\n",
      " [-0.61162047]\n",
      " [-0.70994343]\n",
      " ...\n",
      " [-0.17278706]\n",
      " [ 0.05760285]\n",
      " [-1.00764054]]\n",
      "t [[-0.04917199]\n",
      " [-0.61162047]\n",
      " [-0.70994343]\n",
      " ...\n",
      " [-0.17278706]\n",
      " [ 0.05760285]\n",
      " [-1.00764054]]\n",
      "t [[-0.06114969]\n",
      " [-0.71436864]\n",
      " [-0.76782308]\n",
      " ...\n",
      " [-0.17292407]\n",
      " [ 0.06600463]\n",
      " [-1.13066479]]\n",
      "t [[-0.06114969]\n",
      " [-0.71436864]\n",
      " [-0.76782308]\n",
      " ...\n",
      " [-0.17292407]\n",
      " [ 0.06600463]\n",
      " [-1.13066479]]\n",
      "Current iteration=6, loss=34859.950521018465\n",
      "t [[-0.07118132]\n",
      " [-0.80708437]\n",
      " [-0.8208988 ]\n",
      " ...\n",
      " [-0.17490787]\n",
      " [ 0.07085518]\n",
      " [-1.24038299]]\n",
      "t [[-0.07118132]\n",
      " [-0.80708437]\n",
      " [-0.8208988 ]\n",
      " ...\n",
      " [-0.17490787]\n",
      " [ 0.07085518]\n",
      " [-1.24038299]]\n",
      "t [[-0.07955611]\n",
      " [-0.89101219]\n",
      " [-0.86986282]\n",
      " ...\n",
      " [-0.17832585]\n",
      " [ 0.07310461]\n",
      " [-1.3390664 ]]\n",
      "t [[-0.07955611]\n",
      " [-0.89101219]\n",
      " [-0.86986282]\n",
      " ...\n",
      " [-0.17832585]\n",
      " [ 0.07310461]\n",
      " [-1.3390664 ]]\n",
      "Current iteration=8, loss=34356.23392773602\n",
      "t [[-0.08653583]\n",
      " [-0.96725304]\n",
      " [-0.9151239 ]\n",
      " ...\n",
      " [-0.18275335]\n",
      " [ 0.0735056 ]\n",
      " [-1.42845521]]\n",
      "t [[-0.08653583]\n",
      " [-0.96725304]\n",
      " [-0.9151239 ]\n",
      " ...\n",
      " [-0.18275335]\n",
      " [ 0.0735056 ]\n",
      " [-1.42845521]]\n",
      "t [[-0.09234087]\n",
      " [-1.03674819]\n",
      " [-0.95698631]\n",
      " ...\n",
      " [-0.18783849]\n",
      " [ 0.07262901]\n",
      " [-1.50991552]]\n",
      "loss=34008.38552962703\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.13061821]\n",
      " [-0.557986  ]\n",
      " [-0.35071429]\n",
      " ...\n",
      " [-0.25418092]\n",
      " [ 0.05503157]\n",
      " [-0.14817412]]\n",
      "t [[ 0.13061821]\n",
      " [-0.557986  ]\n",
      " [-0.35071429]\n",
      " ...\n",
      " [-0.25418092]\n",
      " [ 0.05503157]\n",
      " [-0.14817412]]\n",
      "t [[ 0.18178632]\n",
      " [-0.84336088]\n",
      " [-0.52231647]\n",
      " ...\n",
      " [-0.35846529]\n",
      " [ 0.05630582]\n",
      " [-0.2550153 ]]\n",
      "t [[ 0.18178632]\n",
      " [-0.84336088]\n",
      " [-0.52231647]\n",
      " ...\n",
      " [-0.35846529]\n",
      " [ 0.05630582]\n",
      " [-0.2550153 ]]\n",
      "Current iteration=2, loss=36986.07690477786\n",
      "t [[ 0.20505761]\n",
      " [-1.03173745]\n",
      " [-0.63656896]\n",
      " ...\n",
      " [-0.42007864]\n",
      " [ 0.03782577]\n",
      " [-0.33960037]]\n",
      "t [[ 0.20505761]\n",
      " [-1.03173745]\n",
      " [-0.63656896]\n",
      " ...\n",
      " [-0.42007864]\n",
      " [ 0.03782577]\n",
      " [-0.33960037]]\n",
      "t [[ 0.21629881]\n",
      " [-1.17354742]\n",
      " [-0.72635224]\n",
      " ...\n",
      " [-0.46711329]\n",
      " [ 0.010166  ]\n",
      " [-0.40920258]]\n",
      "t [[ 0.21629881]\n",
      " [-1.17354742]\n",
      " [-0.72635224]\n",
      " ...\n",
      " [-0.46711329]\n",
      " [ 0.010166  ]\n",
      " [-0.40920258]]\n",
      "Current iteration=4, loss=35676.420083068595\n",
      "t [[ 0.22160691]\n",
      " [-1.28767399]\n",
      " [-0.80261547]\n",
      " ...\n",
      " [-0.50812769]\n",
      " [-0.02206782]\n",
      " [-0.4676455 ]]\n",
      "t [[ 0.22160691]\n",
      " [-1.28767399]\n",
      " [-0.80261547]\n",
      " ...\n",
      " [-0.50812769]\n",
      " [-0.02206782]\n",
      " [-0.4676455 ]]\n",
      "t [[ 0.2237089 ]\n",
      " [-1.3828116 ]\n",
      " [-0.86961699]\n",
      " ...\n",
      " [-0.54586843]\n",
      " [-0.05641449]\n",
      " [-0.51735006]]\n",
      "t [[ 0.2237089 ]\n",
      " [-1.3828116 ]\n",
      " [-0.86961699]\n",
      " ...\n",
      " [-0.54586843]\n",
      " [-0.05641449]\n",
      " [-0.51735006]]\n",
      "Current iteration=6, loss=34914.28447463349\n",
      "t [[ 0.22397704]\n",
      " [-1.4637328 ]\n",
      " [-0.92932358]\n",
      " ...\n",
      " [-0.58122431]\n",
      " [-0.09141258]\n",
      " [-0.56001064]]\n",
      "t [[ 0.22397704]\n",
      " [-1.4637328 ]\n",
      " [-0.92932358]\n",
      " ...\n",
      " [-0.58122431]\n",
      " [-0.09141258]\n",
      " [-0.56001064]]\n",
      "t [[ 0.22317146]\n",
      " [-1.53345846]\n",
      " [-0.98285337]\n",
      " ...\n",
      " [-0.61449498]\n",
      " [-0.12615228]\n",
      " [-0.59688264]]\n",
      "t [[ 0.22317146]\n",
      " [-1.53345846]\n",
      " [-0.98285337]\n",
      " ...\n",
      " [-0.61449498]\n",
      " [-0.12615228]\n",
      " [-0.59688264]]\n",
      "Current iteration=8, loss=34423.993896594984\n",
      "t [[ 0.22174422]\n",
      " [-1.59410002]\n",
      " [-1.03097631]\n",
      " ...\n",
      " [-0.64580719]\n",
      " [-0.16006173]\n",
      " [-0.62892942]]\n",
      "t [[ 0.22174422]\n",
      " [-1.59410002]\n",
      " [-1.03097631]\n",
      " ...\n",
      " [-0.64580719]\n",
      " [-0.16006173]\n",
      " [-0.62892942]]\n",
      "t [[ 0.21997677]\n",
      " [-1.6472273 ]\n",
      " [-1.07429539]\n",
      " ...\n",
      " [-0.67524795]\n",
      " [-0.19278899]\n",
      " [-0.65690799]]\n",
      "loss=34085.83471228776\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.0172251 ]\n",
      " [-0.10605261]\n",
      " [-0.33248602]\n",
      " ...\n",
      " [-0.25514255]\n",
      " [ 0.04960145]\n",
      " [-0.15136189]]\n",
      "t [[ 0.0172251 ]\n",
      " [-0.10605261]\n",
      " [-0.33248602]\n",
      " ...\n",
      " [-0.25514255]\n",
      " [ 0.04960145]\n",
      " [-0.15136189]]\n",
      "t [[ 0.00727182]\n",
      " [-0.2537798 ]\n",
      " [-0.48117916]\n",
      " ...\n",
      " [-0.3572182 ]\n",
      " [ 0.04675553]\n",
      " [-0.26046732]]\n",
      "t [[ 0.00727182]\n",
      " [-0.2537798 ]\n",
      " [-0.48117916]\n",
      " ...\n",
      " [-0.3572182 ]\n",
      " [ 0.04675553]\n",
      " [-0.26046732]]\n",
      "Current iteration=2, loss=36944.07164678267\n",
      "t [[-0.00727898]\n",
      " [-0.39807981]\n",
      " [-0.57643216]\n",
      " ...\n",
      " [-0.41676449]\n",
      " [ 0.02549504]\n",
      " [-0.34697006]]\n",
      "t [[-0.00727898]\n",
      " [-0.39807981]\n",
      " [-0.57643216]\n",
      " ...\n",
      " [-0.41676449]\n",
      " [ 0.02549504]\n",
      " [-0.34697006]]\n",
      "t [[-0.02127959]\n",
      " [-0.52999316]\n",
      " [-0.65161719]\n",
      " ...\n",
      " [-0.46201379]\n",
      " [-0.00400256]\n",
      " [-0.41824441]]\n",
      "t [[-0.02127959]\n",
      " [-0.52999316]\n",
      " [-0.65161719]\n",
      " ...\n",
      " [-0.46201379]\n",
      " [-0.00400256]\n",
      " [-0.41824441]]\n",
      "Current iteration=4, loss=35608.84350935723\n",
      "t [[-0.03355508]\n",
      " [-0.64863408]\n",
      " [-0.71702554]\n",
      " ...\n",
      " [-0.50143452]\n",
      " [-0.03742462]\n",
      " [-0.47816035]]\n",
      "t [[-0.03355508]\n",
      " [-0.64863408]\n",
      " [-0.71702554]\n",
      " ...\n",
      " [-0.50143452]\n",
      " [-0.03742462]\n",
      " [-0.47816035]]\n",
      "t [[-0.04400919]\n",
      " [-0.755113  ]\n",
      " [-0.77618577]\n",
      " ...\n",
      " [-0.53771088]\n",
      " [-0.0725117 ]\n",
      " [-0.52917236]]\n",
      "t [[-0.04400919]\n",
      " [-0.755113  ]\n",
      " [-0.77618577]\n",
      " ...\n",
      " [-0.53771088]\n",
      " [-0.0725117 ]\n",
      " [-0.52917236]]\n",
      "Current iteration=6, loss=34832.09596611099\n",
      "t [[-0.05282935]\n",
      " [-0.85090732]\n",
      " [-0.83045632]\n",
      " ...\n",
      " [-0.57170417]\n",
      " [-0.10794124]\n",
      " [-0.57300157]]\n",
      "t [[-0.05282935]\n",
      " [-0.85090732]\n",
      " [-0.83045632]\n",
      " ...\n",
      " [-0.57170417]\n",
      " [-0.10794124]\n",
      " [-0.57300157]]\n",
      "t [[-0.06024959]\n",
      " [-0.93740835]\n",
      " [-0.88047785]\n",
      " ...\n",
      " [-0.60370477]\n",
      " [-0.14289986]\n",
      " [-0.61092498]]\n",
      "t [[-0.06024959]\n",
      " [-0.93740835]\n",
      " [-0.88047785]\n",
      " ...\n",
      " [-0.60370477]\n",
      " [-0.14289986]\n",
      " [-0.61092498]]\n",
      "Current iteration=8, loss=34333.40171146786\n",
      "t [[-0.06648418]\n",
      " [-1.01581922]\n",
      " [-0.92664844]\n",
      " ...\n",
      " [-0.63383706]\n",
      " [-0.17688342]\n",
      " [-0.64392328]]\n",
      "t [[-0.06648418]\n",
      " [-1.01581922]\n",
      " [-0.92664844]\n",
      " ...\n",
      " [-0.63383706]\n",
      " [-0.17688342]\n",
      " [-0.64392328]]\n",
      "t [[-0.07171426]\n",
      " [-1.0871557 ]\n",
      " [-0.96927928]\n",
      " ...\n",
      " [-0.66218716]\n",
      " [-0.20958791]\n",
      " [-0.67276745]]\n",
      "loss=33990.28013863641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01471214]\n",
      " [-0.09307812]\n",
      " [-0.33608695]\n",
      " ...\n",
      " [-0.25087576]\n",
      " [ 0.05333124]\n",
      " [-0.1419781 ]]\n",
      "t [[ 0.01471214]\n",
      " [-0.09307812]\n",
      " [-0.33608695]\n",
      " ...\n",
      " [-0.25087576]\n",
      " [ 0.05333124]\n",
      " [-0.1419781 ]]\n",
      "t [[ 0.00250307]\n",
      " [-0.22985769]\n",
      " [-0.48867534]\n",
      " ...\n",
      " [-0.35073524]\n",
      " [ 0.05364575]\n",
      " [-0.24313664]]\n",
      "t [[ 0.00250307]\n",
      " [-0.22985769]\n",
      " [-0.48867534]\n",
      " ...\n",
      " [-0.35073524]\n",
      " [ 0.05364575]\n",
      " [-0.24313664]]\n",
      "Current iteration=2, loss=36986.34228084129\n",
      "t [[-0.01390097]\n",
      " [-0.3649314 ]\n",
      " [-0.58705433]\n",
      " ...\n",
      " [-0.40849705]\n",
      " [ 0.03493522]\n",
      " [-0.32269033]]\n",
      "t [[-0.01390097]\n",
      " [-0.3649314 ]\n",
      " [-0.58705433]\n",
      " ...\n",
      " [-0.40849705]\n",
      " [ 0.03493522]\n",
      " [-0.32269033]]\n",
      "t [[-0.02932861]\n",
      " [-0.48893643]\n",
      " [-0.6647525 ]\n",
      " ...\n",
      " [-0.45225017]\n",
      " [ 0.00745337]\n",
      " [-0.38782778]]\n",
      "t [[-0.02932861]\n",
      " [-0.48893643]\n",
      " [-0.6647525 ]\n",
      " ...\n",
      " [-0.45225017]\n",
      " [ 0.00745337]\n",
      " [-0.38782778]]\n",
      "Current iteration=4, loss=35676.75698152449\n",
      "t [[-0.04265073]\n",
      " [-0.60070992]\n",
      " [-0.7322236 ]\n",
      " ...\n",
      " [-0.49044226]\n",
      " [-0.02440975]\n",
      " [-0.44229248]]\n",
      "t [[-0.04265073]\n",
      " [-0.60070992]\n",
      " [-0.7322236 ]\n",
      " ...\n",
      " [-0.49044226]\n",
      " [-0.02440975]\n",
      " [-0.44229248]]\n",
      "t [[-0.05383468]\n",
      " [-0.70116431]\n",
      " [-0.79310841]\n",
      " ...\n",
      " [-0.52573604]\n",
      " [-0.05831565]\n",
      " [-0.48843962]]\n",
      "t [[-0.05383468]\n",
      " [-0.70116431]\n",
      " [-0.79310841]\n",
      " ...\n",
      " [-0.52573604]\n",
      " [-0.05831565]\n",
      " [-0.48843962]]\n",
      "Current iteration=6, loss=34915.59522989633\n",
      "t [[-0.0631294 ]\n",
      " [-0.79162929]\n",
      " [-0.84884135]\n",
      " ...\n",
      " [-0.55896297]\n",
      " [-0.09286696]\n",
      " [-0.52790925]]\n",
      "t [[-0.0631294 ]\n",
      " [-0.79162929]\n",
      " [-0.84884135]\n",
      " ...\n",
      " [-0.55896297]\n",
      " [-0.09286696]\n",
      " [-0.52790925]]\n",
      "t [[-0.07082211]\n",
      " [-0.87338268]\n",
      " [-0.90011598]\n",
      " ...\n",
      " [-0.59038063]\n",
      " [-0.12718447]\n",
      " [-0.5619113 ]]\n",
      "t [[-0.07082211]\n",
      " [-0.87338268]\n",
      " [-0.90011598]\n",
      " ...\n",
      " [-0.59038063]\n",
      " [-0.12718447]\n",
      " [-0.5619113 ]]\n",
      "Current iteration=8, loss=34426.451511996056\n",
      "t [[-0.07717093]\n",
      " [-0.94753854]\n",
      " [-0.94736871]\n",
      " ...\n",
      " [-0.62008193]\n",
      " [-0.16070897]\n",
      " [-0.5913706 ]]\n",
      "t [[-0.07717093]\n",
      " [-0.94753854]\n",
      " [-0.94736871]\n",
      " ...\n",
      " [-0.62008193]\n",
      " [-0.16070897]\n",
      " [-0.5913706 ]]\n",
      "t [[-0.0823924 ]\n",
      " [-1.01504167]\n",
      " [-0.9909395 ]\n",
      " ...\n",
      " [-0.64812492]\n",
      " [-0.19309188]\n",
      " [-0.61701132]]\n",
      "loss=34089.32868209493\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01225225]\n",
      " [-0.09858579]\n",
      " [-0.33386935]\n",
      " ...\n",
      " [-0.14699409]\n",
      " [-0.01770359]\n",
      " [-0.29989175]]\n",
      "t [[ 0.01225225]\n",
      " [-0.09858579]\n",
      " [-0.33386935]\n",
      " ...\n",
      " [-0.14699409]\n",
      " [-0.01770359]\n",
      " [-0.29989175]]\n",
      "t [[-0.00187772]\n",
      " [-0.24050902]\n",
      " [-0.4827866 ]\n",
      " ...\n",
      " [-0.17546137]\n",
      " [ 0.00385919]\n",
      " [-0.53025171]]\n",
      "t [[-0.00187772]\n",
      " [-0.24050902]\n",
      " [-0.4827866 ]\n",
      " ...\n",
      " [-0.17546137]\n",
      " [ 0.00385919]\n",
      " [-0.53025171]]\n",
      "Current iteration=2, loss=36929.72175465302\n",
      "t [[-0.01976669]\n",
      " [-0.38018877]\n",
      " [-0.57761827]\n",
      " ...\n",
      " [-0.17722637]\n",
      " [ 0.0275779 ]\n",
      " [-0.72012175]]\n",
      "t [[-0.01976669]\n",
      " [-0.38018877]\n",
      " [-0.57761827]\n",
      " ...\n",
      " [-0.17722637]\n",
      " [ 0.0275779 ]\n",
      " [-0.72012175]]\n",
      "t [[-0.03637588]\n",
      " [-0.50830631]\n",
      " [-0.65212986]\n",
      " ...\n",
      " [-0.17438873]\n",
      " [ 0.04615468]\n",
      " [-0.88190035]]\n",
      "t [[-0.03637588]\n",
      " [-0.50830631]\n",
      " [-0.65212986]\n",
      " ...\n",
      " [-0.17438873]\n",
      " [ 0.04615468]\n",
      " [-0.88190035]]\n",
      "Current iteration=4, loss=35595.460157673246\n",
      "t [[-0.05066715]\n",
      " [-0.6237497 ]\n",
      " [-0.71679807]\n",
      " ...\n",
      " [-0.17260451]\n",
      " [ 0.05900046]\n",
      " [-1.0225007 ]]\n",
      "t [[-0.05066715]\n",
      " [-0.6237497 ]\n",
      " [-0.71679807]\n",
      " ...\n",
      " [-0.17260451]\n",
      " [ 0.05900046]\n",
      " [-1.0225007 ]]\n",
      "t [[-0.06266724]\n",
      " [-0.72748431]\n",
      " [-0.77523086]\n",
      " ...\n",
      " [-0.17294415]\n",
      " [ 0.0670916 ]\n",
      " [-1.14640126]]\n",
      "t [[-0.06266724]\n",
      " [-0.72748431]\n",
      " [-0.77523086]\n",
      " ...\n",
      " [-0.17294415]\n",
      " [ 0.0670916 ]\n",
      " [-1.14640126]]\n",
      "Current iteration=6, loss=34818.790037895866\n",
      "t [[-0.07266374]\n",
      " [-0.82089062]\n",
      " [-0.82881662]\n",
      " ...\n",
      " [-0.17520284]\n",
      " [ 0.07159354]\n",
      " [-1.25675374]]\n",
      "t [[-0.07266374]\n",
      " [-0.82089062]\n",
      " [-0.82881662]\n",
      " ...\n",
      " [-0.17520284]\n",
      " [ 0.07159354]\n",
      " [-1.25675374]]\n",
      "t [[-0.08096928]\n",
      " [-0.90529206]\n",
      " [-0.87820464]\n",
      " ...\n",
      " [-0.17890492]\n",
      " [ 0.07350446]\n",
      " [-1.35588996]]\n",
      "t [[-0.08096928]\n",
      " [-0.90529206]\n",
      " [-0.87820464]\n",
      " ...\n",
      " [-0.17890492]\n",
      " [ 0.07350446]\n",
      " [-1.35588996]]\n",
      "Current iteration=8, loss=34319.77263655777\n",
      "t [[-0.08785908]\n",
      " [-0.981842  ]\n",
      " [-0.92379296]\n",
      " ...\n",
      " [-0.1835966 ]\n",
      " [ 0.07360014]\n",
      " [-1.44559363]]\n",
      "t [[-0.08785908]\n",
      " [-0.981842  ]\n",
      " [-0.92379296]\n",
      " ...\n",
      " [-0.1835966 ]\n",
      " [ 0.07360014]\n",
      " [-1.44559363]]\n",
      "t [[-0.09356163]\n",
      " [-1.05151839]\n",
      " [-0.9658895 ]\n",
      " ...\n",
      " [-0.18891413]\n",
      " [ 0.07246096]\n",
      " [-1.5272627 ]]\n",
      "loss=33976.233649941096\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.13333942]\n",
      " [-0.56961071]\n",
      " [-0.35802084]\n",
      " ...\n",
      " [-0.25947636]\n",
      " [ 0.05617806]\n",
      " [-0.15126108]]\n",
      "t [[ 0.13333942]\n",
      " [-0.56961071]\n",
      " [-0.35802084]\n",
      " ...\n",
      " [-0.25947636]\n",
      " [ 0.05617806]\n",
      " [-0.15126108]]\n",
      "t [[ 0.18396272]\n",
      " [-0.85541066]\n",
      " [-0.52957451]\n",
      " ...\n",
      " [-0.36289682]\n",
      " [ 0.05640119]\n",
      " [-0.25948162]]\n",
      "t [[ 0.18396272]\n",
      " [-0.85541066]\n",
      " [-0.52957451]\n",
      " ...\n",
      " [-0.36289682]\n",
      " [ 0.05640119]\n",
      " [-0.25948162]]\n",
      "Current iteration=2, loss=36945.32517553136\n",
      "t [[ 0.20655613]\n",
      " [-1.0436314 ]\n",
      " [-0.64378736]\n",
      " ...\n",
      " [-0.42398908]\n",
      " [ 0.03670393]\n",
      " [-0.34489787]]\n",
      "t [[ 0.20655613]\n",
      " [-1.0436314 ]\n",
      " [-0.64378736]\n",
      " ...\n",
      " [-0.42398908]\n",
      " [ 0.03670393]\n",
      " [-0.34489787]]\n",
      "t [[ 0.2172762 ]\n",
      " [-1.18544259]\n",
      " [-0.73388178]\n",
      " ...\n",
      " [-0.47106747]\n",
      " [ 0.00788089]\n",
      " [-0.41500774]]\n",
      "t [[ 0.2172762 ]\n",
      " [-1.18544259]\n",
      " [-0.73388178]\n",
      " ...\n",
      " [-0.47106747]\n",
      " [ 0.00788089]\n",
      " [-0.41500774]]\n",
      "Current iteration=4, loss=35633.446276297866\n",
      "t [[ 0.22220021]\n",
      " [-1.29961732]\n",
      " [-0.81059183]\n",
      " ...\n",
      " [-0.51242229]\n",
      " [-0.02541378]\n",
      " [-0.47373356]]\n",
      "t [[ 0.22220021]\n",
      " [-1.29961732]\n",
      " [-0.81059183]\n",
      " ...\n",
      " [-0.51242229]\n",
      " [-0.02541378]\n",
      " [-0.47373356]]\n",
      "t [[ 0.22401322]\n",
      " [-1.39474601]\n",
      " [-0.87801583]\n",
      " ...\n",
      " [-0.55060169]\n",
      " [-0.06070051]\n",
      " [-0.52355937]]\n",
      "t [[ 0.22401322]\n",
      " [-1.39474601]\n",
      " [-0.87801583]\n",
      " ...\n",
      " [-0.55060169]\n",
      " [-0.06070051]\n",
      " [-0.52355937]]\n",
      "Current iteration=6, loss=34874.980678171676\n",
      "t [[ 0.22405932]\n",
      " [-1.47556633]\n",
      " [-0.93804809]\n",
      " ...\n",
      " [-0.58639158]\n",
      " [-0.09651132]\n",
      " [-0.56622435]]\n",
      "t [[ 0.22405932]\n",
      " [-1.47556633]\n",
      " [-0.93804809]\n",
      " ...\n",
      " [-0.58639158]\n",
      " [-0.09651132]\n",
      " [-0.56622435]]\n",
      "t [[ 0.2230819 ]\n",
      " [-1.54510306]\n",
      " [-0.99178645]\n",
      " ...\n",
      " [-0.62004753]\n",
      " [-0.1319377 ]\n",
      " [-0.60301645]]\n",
      "t [[ 0.2230819 ]\n",
      " [-1.54510306]\n",
      " [-0.99178645]\n",
      " ...\n",
      " [-0.62004753]\n",
      " [-0.1319377 ]\n",
      " [-0.60301645]]\n",
      "Current iteration=8, loss=34389.258711167226\n",
      "t [[ 0.22152258]\n",
      " [-1.60548524]\n",
      " [-1.04000485]\n",
      " ...\n",
      " [-0.65168207]\n",
      " [-0.16641464]\n",
      " [-0.63492273]]\n",
      "t [[ 0.22152258]\n",
      " [-1.60548524]\n",
      " [-1.04000485]\n",
      " ...\n",
      " [-0.65168207]\n",
      " [-0.16641464]\n",
      " [-0.63492273]]\n",
      "t [[ 0.21965543]\n",
      " [-1.65830173]\n",
      " [-1.08331946]\n",
      " ...\n",
      " [-0.68138164]\n",
      " [-0.19960046]\n",
      " [-0.66271752]]\n",
      "loss=34055.22059358888\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01758395]\n",
      " [-0.10826204]\n",
      " [-0.33941281]\n",
      " ...\n",
      " [-0.26045802]\n",
      " [ 0.05063482]\n",
      " [-0.15451526]]\n",
      "t [[ 0.01758395]\n",
      " [-0.10826204]\n",
      " [-0.33941281]\n",
      " ...\n",
      " [-0.26045802]\n",
      " [ 0.05063482]\n",
      " [-0.15451526]]\n",
      "t [[ 0.00688474]\n",
      " [-0.25989568]\n",
      " [-0.4874871 ]\n",
      " ...\n",
      " [-0.36156229]\n",
      " [ 0.04668019]\n",
      " [-0.26502889]]\n",
      "t [[ 0.00688474]\n",
      " [-0.25989568]\n",
      " [-0.4874871 ]\n",
      " ...\n",
      " [-0.36156229]\n",
      " [ 0.04668019]\n",
      " [-0.26502889]]\n",
      "Current iteration=2, loss=36902.62115098648\n",
      "t [[-0.00816671]\n",
      " [-0.40707808]\n",
      " [-0.58246009]\n",
      " ...\n",
      " [-0.42054672]\n",
      " [ 0.02419904]\n",
      " [-0.35238816]]\n",
      "t [[-0.00816671]\n",
      " [-0.40707808]\n",
      " [-0.58246009]\n",
      " ...\n",
      " [-0.42054672]\n",
      " [ 0.02419904]\n",
      " [-0.35238816]]\n",
      "t [[-0.02243085]\n",
      " [-0.54098069]\n",
      " [-0.65792499]\n",
      " ...\n",
      " [-0.46581931]\n",
      " [-0.00644142]\n",
      " [-0.42418998]]\n",
      "t [[-0.02243085]\n",
      " [-0.54098069]\n",
      " [-0.65792499]\n",
      " ...\n",
      " [-0.46581931]\n",
      " [-0.00644142]\n",
      " [-0.42418998]]\n",
      "Current iteration=4, loss=35565.01042767132\n",
      "t [[-0.03482152]\n",
      " [-0.66099426]\n",
      " [-0.72386598]\n",
      " ...\n",
      " [-0.50556328]\n",
      " [-0.04089483]\n",
      " [-0.48440337]]\n",
      "t [[-0.03482152]\n",
      " [-0.66099426]\n",
      " [-0.72386598]\n",
      " ...\n",
      " [-0.50556328]\n",
      " [-0.04089483]\n",
      " [-0.48440337]]\n",
      "t [[-0.04530513]\n",
      " [-0.76842686]\n",
      " [-0.78359979]\n",
      " ...\n",
      " [-0.54226141]\n",
      " [-0.07689066]\n",
      " [-0.53554697]]\n",
      "t [[-0.04530513]\n",
      " [-0.76842686]\n",
      " [-0.78359979]\n",
      " ...\n",
      " [-0.54226141]\n",
      " [-0.07689066]\n",
      " [-0.53554697]]\n",
      "Current iteration=6, loss=34792.06239351568\n",
      "t [[-0.05410521]\n",
      " [-0.86488058]\n",
      " [-0.83838411]\n",
      " ...\n",
      " [-0.57667318]\n",
      " [-0.11310326]\n",
      " [-0.57938782]]\n",
      "t [[-0.05410521]\n",
      " [-0.86488058]\n",
      " [-0.83838411]\n",
      " ...\n",
      " [-0.57667318]\n",
      " [-0.11310326]\n",
      " [-0.57938782]]\n",
      "t [[-0.06147568]\n",
      " [-0.9518256 ]\n",
      " [-0.88882302]\n",
      " ...\n",
      " [-0.60904623]\n",
      " [-0.14872219]\n",
      " [-0.61723627]]\n",
      "t [[-0.06147568]\n",
      " [-0.9518256 ]\n",
      " [-0.88882302]\n",
      " ...\n",
      " [-0.60904623]\n",
      " [-0.14872219]\n",
      " [-0.61723627]]\n",
      "Current iteration=8, loss=34298.100446214725\n",
      "t [[-0.06764199]\n",
      " [-1.03051728]\n",
      " [-0.93530854]\n",
      " ...\n",
      " [-0.63949145]\n",
      " [-0.18325076]\n",
      " [-0.65009728]]\n",
      "t [[-0.06764199]\n",
      " [-1.03051728]\n",
      " [-0.93530854]\n",
      " ...\n",
      " [-0.63949145]\n",
      " [-0.18325076]\n",
      " [-0.65009728]]\n",
      "t [[-0.07279197]\n",
      " [-1.10200797]\n",
      " [-0.97815841]\n",
      " ...\n",
      " [-0.66809453]\n",
      " [-0.21639534]\n",
      " [-0.67875955]]\n",
      "loss=33959.245475239935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01501864]\n",
      " [-0.09501725]\n",
      " [-0.34308876]\n",
      " ...\n",
      " [-0.25610234]\n",
      " [ 0.05444231]\n",
      " [-0.14493597]]\n",
      "t [[ 0.01501864]\n",
      " [-0.09501725]\n",
      " [-0.34308876]\n",
      " ...\n",
      " [-0.25610234]\n",
      " [ 0.05444231]\n",
      " [-0.14493597]]\n",
      "t [[ 0.00202124]\n",
      " [-0.23551923]\n",
      " [-0.49514196]\n",
      " ...\n",
      " [-0.35498443]\n",
      " [ 0.05370029]\n",
      " [-0.24736713]]\n",
      "t [[ 0.00202124]\n",
      " [-0.23551923]\n",
      " [-0.49514196]\n",
      " ...\n",
      " [-0.35498443]\n",
      " [ 0.05370029]\n",
      " [-0.24736713]]\n",
      "Current iteration=2, loss=36945.595216980626\n",
      "t [[-0.0149053 ]\n",
      " [-0.37335335]\n",
      " [-0.59327679]\n",
      " ...\n",
      " [-0.41216749]\n",
      " [ 0.03379706]\n",
      " [-0.32767379]]\n",
      "t [[-0.0149053 ]\n",
      " [-0.37335335]\n",
      " [-0.59327679]\n",
      " ...\n",
      " [-0.41216749]\n",
      " [ 0.03379706]\n",
      " [-0.32767379]]\n",
      "t [[-0.03059947]\n",
      " [-0.49926409]\n",
      " [-0.6712702 ]\n",
      " ...\n",
      " [-0.45593164]\n",
      " [ 0.00518113]\n",
      " [-0.39326094]]\n",
      "t [[-0.03059947]\n",
      " [-0.49926409]\n",
      " [-0.6712702 ]\n",
      " ...\n",
      " [-0.45593164]\n",
      " [ 0.00518113]\n",
      " [-0.39326094]]\n",
      "Current iteration=4, loss=35633.80412842245\n",
      "t [[-0.04402667]\n",
      " [-0.61235303]\n",
      " [-0.73927974]\n",
      " ...\n",
      " [-0.49444387]\n",
      " [-0.02771888]\n",
      " [-0.44796601]]\n",
      "t [[-0.04402667]\n",
      " [-0.61235303]\n",
      " [-0.73927974]\n",
      " ...\n",
      " [-0.49444387]\n",
      " [-0.02771888]\n",
      " [-0.44796601]]\n",
      "t [[-0.05522206]\n",
      " [-0.71372275]\n",
      " [-0.80073884]\n",
      " ...\n",
      " [-0.53016457]\n",
      " [-0.06254821]\n",
      " [-0.49420401]]\n",
      "t [[-0.05522206]\n",
      " [-0.71372275]\n",
      " [-0.80073884]\n",
      " ...\n",
      " [-0.53016457]\n",
      " [-0.06254821]\n",
      " [-0.49420401]]\n",
      "Current iteration=6, loss=34876.361680625356\n",
      "t [[-0.0644744 ]\n",
      " [-0.80482258]\n",
      " [-0.85698305]\n",
      " ...\n",
      " [-0.563821  ]\n",
      " [-0.09790205]\n",
      " [-0.53365737]]\n",
      "t [[-0.0644744 ]\n",
      " [-0.80482258]\n",
      " [-0.85698305]\n",
      " ...\n",
      " [-0.563821  ]\n",
      " [-0.09790205]\n",
      " [-0.53365737]]\n",
      "t [[-0.07209328]\n",
      " [-0.88700564]\n",
      " [-0.90867038]\n",
      " ...\n",
      " [-0.59562568]\n",
      " [-0.13290093]\n",
      " [-0.56756662]]\n",
      "t [[-0.07209328]\n",
      " [-0.88700564]\n",
      " [-0.90867038]\n",
      " ...\n",
      " [-0.59562568]\n",
      " [-0.13290093]\n",
      " [-0.56756662]]\n",
      "Current iteration=8, loss=34391.80989706628\n",
      "t [[-0.07834959]\n",
      " [-0.96143568]\n",
      " [-0.95623178]\n",
      " ...\n",
      " [-0.62565623]\n",
      " [-0.16699102]\n",
      " [-0.59687876]]\n",
      "t [[-0.07834959]\n",
      " [-0.96143568]\n",
      " [-0.95623178]\n",
      " ...\n",
      " [-0.62565623]\n",
      " [-0.16699102]\n",
      " [-0.59687876]]\n",
      "t [[-0.08346754]\n",
      " [-1.02909227]\n",
      " [-1.0000142 ]\n",
      " ...\n",
      " [-0.653969  ]\n",
      " [-0.19983288]\n",
      " [-0.62233401]]\n",
      "loss=34058.81658510983\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=40312.053727005376\n",
      "t [[ 0.01250751]\n",
      " [-0.10063966]\n",
      " [-0.34082496]\n",
      " ...\n",
      " [-0.15005647]\n",
      " [-0.01807242]\n",
      " [-0.30613949]]\n",
      "t [[ 0.01250751]\n",
      " [-0.10063966]\n",
      " [-0.34082496]\n",
      " ...\n",
      " [-0.15005647]\n",
      " [-0.01807242]\n",
      " [-0.30613949]]\n",
      "t [[-0.00243954]\n",
      " [-0.24638269]\n",
      " [-0.48910464]\n",
      " ...\n",
      " [-0.17672023]\n",
      " [ 0.00473674]\n",
      " [-0.53987089]]\n",
      "t [[-0.00243954]\n",
      " [-0.24638269]\n",
      " [-0.48910464]\n",
      " ...\n",
      " [-0.17672023]\n",
      " [ 0.00473674]\n",
      " [-0.53987089]]\n",
      "Current iteration=2, loss=36888.276598570636\n",
      "t [[-0.02086391]\n",
      " [-0.38889741]\n",
      " [-0.58362146]\n",
      " ...\n",
      " [-0.1773791 ]\n",
      " [ 0.02905669]\n",
      " [-0.73201722]]\n",
      "t [[-0.02086391]\n",
      " [-0.38889741]\n",
      " [-0.58362146]\n",
      " ...\n",
      " [-0.1773791 ]\n",
      " [ 0.02905669]\n",
      " [-0.73201722]]\n",
      "t [[-0.03774518]\n",
      " [-0.51897636]\n",
      " [-0.658383  ]\n",
      " ...\n",
      " [-0.17417047]\n",
      " [ 0.04771073]\n",
      " [-0.89541446]]\n",
      "t [[-0.03774518]\n",
      " [-0.51897636]\n",
      " [-0.658383  ]\n",
      " ...\n",
      " [-0.17417047]\n",
      " [ 0.04771073]\n",
      " [-0.89541446]]\n",
      "Current iteration=4, loss=35551.65381186029\n",
      "t [[-0.05214382]\n",
      " [-0.63577537]\n",
      " [-0.72356265]\n",
      " ...\n",
      " [-0.17242559]\n",
      " [ 0.060352  ]\n",
      " [-1.03718189]]\n",
      "t [[-0.05214382]\n",
      " [-0.63577537]\n",
      " [-0.72356265]\n",
      " ...\n",
      " [-0.17242559]\n",
      " [ 0.060352  ]\n",
      " [-1.03718189]]\n",
      "t [[-0.06415621]\n",
      " [-0.74045345]\n",
      " [-0.78255496]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0681192 ]\n",
      " [-1.1619251 ]]\n",
      "t [[-0.06415621]\n",
      " [-0.74045345]\n",
      " [-0.78255496]\n",
      " ...\n",
      " [-0.17298806]\n",
      " [ 0.0681192 ]\n",
      " [-1.1619251 ]]\n",
      "Current iteration=6, loss=34778.75038762944\n",
      "t [[-0.07411048]\n",
      " [-0.83451382]\n",
      " [-0.83664535]\n",
      " ...\n",
      " [-0.1755305 ]\n",
      " [ 0.07226784]\n",
      " [-1.27288189]]\n",
      "t [[-0.07411048]\n",
      " [-0.83451382]\n",
      " [-0.83664535]\n",
      " ...\n",
      " [-0.1755305 ]\n",
      " [ 0.07226784]\n",
      " [-1.27288189]]\n",
      "t [[-0.08234182]\n",
      " [-0.91935752]\n",
      " [-0.88644483]\n",
      " ...\n",
      " [-0.17951799]\n",
      " [ 0.07384164]\n",
      " [-1.37244456]]\n",
      "t [[-0.08234182]\n",
      " [-0.91935752]\n",
      " [-0.88644483]\n",
      " ...\n",
      " [-0.17951799]\n",
      " [ 0.07384164]\n",
      " [-1.37244456]]\n",
      "Current iteration=8, loss=34284.442997626786\n",
      "t [[-0.08913829]\n",
      " [-0.99618928]\n",
      " [-0.9323443 ]\n",
      " ...\n",
      " [-0.18447056]\n",
      " [ 0.0736374 ]\n",
      " [-1.4624401 ]]\n",
      "t [[-0.08913829]\n",
      " [-0.99618928]\n",
      " [-0.9323443 ]\n",
      " ...\n",
      " [-0.18447056]\n",
      " [ 0.0736374 ]\n",
      " [-1.4624401 ]]\n",
      "t [[-0.094736  ]\n",
      " [-1.06602303]\n",
      " [-0.97465758]\n",
      " ...\n",
      " [-0.19001462]\n",
      " [ 0.07224346]\n",
      " [-1.54429794]]\n",
      "loss=33945.158607453144\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=53749.404969340525\n",
      "t [[-0.03779159]\n",
      " [-0.0029329 ]\n",
      " [ 0.00123944]\n",
      " ...\n",
      " [-0.0029329 ]\n",
      " [ 0.0026682 ]\n",
      " [-0.00153612]]\n",
      "t [[-0.03779159]\n",
      " [-0.0029329 ]\n",
      " [ 0.00123944]\n",
      " ...\n",
      " [-0.0029329 ]\n",
      " [ 0.0026682 ]\n",
      " [-0.00153612]]\n",
      "t [[-0.07493605]\n",
      " [-0.005848  ]\n",
      " [ 0.00247142]\n",
      " ...\n",
      " [-0.005848  ]\n",
      " [ 0.00530351]\n",
      " [-0.00306339]]\n",
      "t [[-0.07493605]\n",
      " [-0.005848  ]\n",
      " [ 0.00247142]\n",
      " ...\n",
      " [-0.005848  ]\n",
      " [ 0.00530351]\n",
      " [-0.00306339]]\n",
      "Current iteration=2, loss=53596.156021112605\n",
      "t [[-0.11144353]\n",
      " [-0.00874549]\n",
      " [ 0.00369592]\n",
      " ...\n",
      " [-0.00874549]\n",
      " [ 0.00790632]\n",
      " [-0.00458191]]\n",
      "t [[-0.11144353]\n",
      " [-0.00874549]\n",
      " [ 0.00369592]\n",
      " ...\n",
      " [-0.00874549]\n",
      " [ 0.00790632]\n",
      " [-0.00458191]]\n",
      "t [[-0.14732413]\n",
      " [-0.01162557]\n",
      " [ 0.00491288]\n",
      " ...\n",
      " [-0.01162557]\n",
      " [ 0.01047702]\n",
      " [-0.00609179]]\n",
      "t [[-0.14732413]\n",
      " [-0.01162557]\n",
      " [ 0.00491288]\n",
      " ...\n",
      " [-0.01162557]\n",
      " [ 0.01047702]\n",
      " [-0.00609179]]\n",
      "Current iteration=4, loss=53447.89623502396\n",
      "t [[-0.18258789]\n",
      " [-0.01448843]\n",
      " [ 0.00612227]\n",
      " ...\n",
      " [-0.01448843]\n",
      " [ 0.01301601]\n",
      " [-0.00759311]]\n",
      "t [[-0.18258789]\n",
      " [-0.01448843]\n",
      " [ 0.00612227]\n",
      " ...\n",
      " [-0.01448843]\n",
      " [ 0.01301601]\n",
      " [-0.00759311]]\n",
      "t [[-0.21724478]\n",
      " [-0.01733424]\n",
      " [ 0.00732407]\n",
      " ...\n",
      " [-0.01733424]\n",
      " [ 0.01552369]\n",
      " [-0.00908598]]\n",
      "t [[-0.21724478]\n",
      " [-0.01733424]\n",
      " [ 0.00732407]\n",
      " ...\n",
      " [-0.01733424]\n",
      " [ 0.01552369]\n",
      " [-0.00908598]]\n",
      "Current iteration=6, loss=53304.38756613169\n",
      "t [[-0.25130467]\n",
      " [-0.02016319]\n",
      " [ 0.00851824]\n",
      " ...\n",
      " [-0.02016319]\n",
      " [ 0.01800043]\n",
      " [-0.0105705 ]]\n",
      "t [[-0.25130467]\n",
      " [-0.02016319]\n",
      " [ 0.00851824]\n",
      " ...\n",
      " [-0.02016319]\n",
      " [ 0.01800043]\n",
      " [-0.0105705 ]]\n",
      "t [[-0.28477736]\n",
      " [-0.02297547]\n",
      " [ 0.00970475]\n",
      " ...\n",
      " [-0.02297547]\n",
      " [ 0.02044661]\n",
      " [-0.01204676]]\n",
      "t [[-0.28477736]\n",
      " [-0.02297547]\n",
      " [ 0.00970475]\n",
      " ...\n",
      " [-0.02297547]\n",
      " [ 0.02044661]\n",
      " [-0.01204676]]\n",
      "Current iteration=8, loss=53165.404086882125\n",
      "t [[-0.31767256]\n",
      " [-0.02577126]\n",
      " [ 0.01088357]\n",
      " ...\n",
      " [-0.02577126]\n",
      " [ 0.02286263]\n",
      " [-0.01351486]]\n",
      "t [[-0.31767256]\n",
      " [-0.02577126]\n",
      " [ 0.01088357]\n",
      " ...\n",
      " [-0.02577126]\n",
      " [ 0.02286263]\n",
      " [-0.01351486]]\n",
      "t [[-0.34999986]\n",
      " [-0.02855072]\n",
      " [ 0.01205468]\n",
      " ...\n",
      " [-0.02855072]\n",
      " [ 0.02524884]\n",
      " [-0.01497489]]\n",
      "t [[-0.34999986]\n",
      " [-0.02855072]\n",
      " [ 0.01205468]\n",
      " ...\n",
      " [-0.02855072]\n",
      " [ 0.02524884]\n",
      " [-0.01497489]]\n",
      "Current iteration=10, loss=53030.731512952625\n",
      "t [[-0.38176878]\n",
      " [-0.03131405]\n",
      " [ 0.01321806]\n",
      " ...\n",
      " [-0.03131405]\n",
      " [ 0.02760563]\n",
      " [-0.01642694]]\n",
      "t [[-0.38176878]\n",
      " [-0.03131405]\n",
      " [ 0.01321806]\n",
      " ...\n",
      " [-0.03131405]\n",
      " [ 0.02760563]\n",
      " [-0.01642694]]\n",
      "t [[-0.41298869]\n",
      " [-0.03406141]\n",
      " [ 0.01437368]\n",
      " ...\n",
      " [-0.03406141]\n",
      " [ 0.02993337]\n",
      " [-0.01787112]]\n",
      "t [[-0.41298869]\n",
      " [-0.03406141]\n",
      " [ 0.01437368]\n",
      " ...\n",
      " [-0.03406141]\n",
      " [ 0.02993337]\n",
      " [-0.01787112]]\n",
      "Current iteration=12, loss=52900.166720809655\n",
      "t [[-0.4436689 ]\n",
      " [-0.03679297]\n",
      " [ 0.01552152]\n",
      " ...\n",
      " [-0.03679297]\n",
      " [ 0.0322324 ]\n",
      " [-0.0193075 ]]\n",
      "t [[-0.4436689 ]\n",
      " [-0.03679297]\n",
      " [ 0.01552152]\n",
      " ...\n",
      " [-0.03679297]\n",
      " [ 0.0322324 ]\n",
      " [-0.0193075 ]]\n",
      "t [[-0.47381856]\n",
      " [-0.03950892]\n",
      " [ 0.01666156]\n",
      " ...\n",
      " [-0.03950892]\n",
      " [ 0.03450311]\n",
      " [-0.02073618]]\n",
      "t [[-0.47381856]\n",
      " [-0.03950892]\n",
      " [ 0.01666156]\n",
      " ...\n",
      " [-0.03950892]\n",
      " [ 0.03450311]\n",
      " [-0.02073618]]\n",
      "Current iteration=14, loss=52773.517262031055\n",
      "t [[-0.50344672]\n",
      " [-0.04220941]\n",
      " [ 0.01779378]\n",
      " ...\n",
      " [-0.04220941]\n",
      " [ 0.03674583]\n",
      " [-0.02215725]]\n",
      "t [[-0.50344672]\n",
      " [-0.04220941]\n",
      " [ 0.01779378]\n",
      " ...\n",
      " [-0.04220941]\n",
      " [ 0.03674583]\n",
      " [-0.02215725]]\n",
      "t [[-0.53256232]\n",
      " [-0.04489461]\n",
      " [ 0.01891818]\n",
      " ...\n",
      " [-0.04489461]\n",
      " [ 0.03896093]\n",
      " [-0.02357079]]\n",
      "t [[-0.53256232]\n",
      " [-0.04489461]\n",
      " [ 0.01891818]\n",
      " ...\n",
      " [-0.04489461]\n",
      " [ 0.03896093]\n",
      " [-0.02357079]]\n",
      "Current iteration=16, loss=52650.60087874284\n",
      "t [[-0.56117416]\n",
      " [-0.04756469]\n",
      " [ 0.02003473]\n",
      " ...\n",
      " [-0.04756469]\n",
      " [ 0.04114875]\n",
      " [-0.02497689]]\n",
      "t [[-0.56117416]\n",
      " [-0.04756469]\n",
      " [ 0.02003473]\n",
      " ...\n",
      " [-0.04756469]\n",
      " [ 0.04114875]\n",
      " [-0.02497689]]\n",
      "t [[-0.58929093]\n",
      " [-0.05021981]\n",
      " [ 0.02114342]\n",
      " ...\n",
      " [-0.05021981]\n",
      " [ 0.04330963]\n",
      " [-0.02637564]]\n",
      "t [[-0.58929093]\n",
      " [-0.05021981]\n",
      " [ 0.02114342]\n",
      " ...\n",
      " [-0.05021981]\n",
      " [ 0.04330963]\n",
      " [-0.02637564]]\n",
      "Current iteration=18, loss=52531.245023867334\n",
      "t [[-0.61692118]\n",
      " [-0.05286014]\n",
      " [ 0.02224424]\n",
      " ...\n",
      " [-0.05286014]\n",
      " [ 0.04544392]\n",
      " [-0.02776713]]\n",
      "t [[-0.61692118]\n",
      " [-0.05286014]\n",
      " [ 0.02224424]\n",
      " ...\n",
      " [-0.05286014]\n",
      " [ 0.04544392]\n",
      " [-0.02776713]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.64407335]\n",
      " [-0.05548582]\n",
      " [ 0.02333718]\n",
      " ...\n",
      " [-0.05548582]\n",
      " [ 0.04755196]\n",
      " [-0.02915142]]\n",
      "t [[-0.64407335]\n",
      " [-0.05548582]\n",
      " [ 0.02333718]\n",
      " ...\n",
      " [-0.05548582]\n",
      " [ 0.04755196]\n",
      " [-0.02915142]]\n",
      "Current iteration=20, loss=52415.286389269786\n",
      "t [[-0.67075574]\n",
      " [-0.05809702]\n",
      " [ 0.02442224]\n",
      " ...\n",
      " [-0.05809702]\n",
      " [ 0.04963406]\n",
      " [-0.03052862]]\n",
      "t [[-0.67075574]\n",
      " [-0.05809702]\n",
      " [ 0.02442224]\n",
      " ...\n",
      " [-0.05809702]\n",
      " [ 0.04963406]\n",
      " [-0.03052862]]\n",
      "t [[-0.69697652]\n",
      " [-0.06069388]\n",
      " [ 0.0254994 ]\n",
      " ...\n",
      " [-0.06069388]\n",
      " [ 0.05169056]\n",
      " [-0.03189879]]\n",
      "t [[-0.69697652]\n",
      " [-0.06069388]\n",
      " [ 0.0254994 ]\n",
      " ...\n",
      " [-0.06069388]\n",
      " [ 0.05169056]\n",
      " [-0.03189879]]\n",
      "Current iteration=22, loss=52302.570444335135\n",
      "t [[-0.72274375]\n",
      " [-0.06327658]\n",
      " [ 0.02656865]\n",
      " ...\n",
      " [-0.06327658]\n",
      " [ 0.05372179]\n",
      " [-0.03326201]]\n",
      "t [[-0.72274375]\n",
      " [-0.06327658]\n",
      " [ 0.02656865]\n",
      " ...\n",
      " [-0.06327658]\n",
      " [ 0.05372179]\n",
      " [-0.03326201]]\n",
      "t [[-0.74806534]\n",
      " [-0.06584525]\n",
      " [ 0.02763   ]\n",
      " ...\n",
      " [-0.06584525]\n",
      " [ 0.05572806]\n",
      " [-0.03461837]]\n",
      "t [[-0.74806534]\n",
      " [-0.06584525]\n",
      " [ 0.02763   ]\n",
      " ...\n",
      " [-0.06584525]\n",
      " [ 0.05572806]\n",
      " [-0.03461837]]\n",
      "Current iteration=24, loss=52192.95098700352\n",
      "t [[-0.77294908]\n",
      " [-0.06840004]\n",
      " [ 0.02868343]\n",
      " ...\n",
      " [-0.06840004]\n",
      " [ 0.05770969]\n",
      " [-0.03596795]]\n",
      "t [[-0.77294908]\n",
      " [-0.06840004]\n",
      " [ 0.02868343]\n",
      " ...\n",
      " [-0.06840004]\n",
      " [ 0.05770969]\n",
      " [-0.03596795]]\n",
      "t [[-0.79740263]\n",
      " [-0.0709411 ]\n",
      " [ 0.02972896]\n",
      " ...\n",
      " [-0.0709411 ]\n",
      " [ 0.05966699]\n",
      " [-0.03731081]]\n",
      "t [[-0.79740263]\n",
      " [-0.0709411 ]\n",
      " [ 0.02972896]\n",
      " ...\n",
      " [-0.0709411 ]\n",
      " [ 0.05966699]\n",
      " [-0.03731081]]\n",
      "Current iteration=26, loss=52086.289708846816\n",
      "t [[-0.82143354]\n",
      " [-0.07346858]\n",
      " [ 0.03076656]\n",
      " ...\n",
      " [-0.07346858]\n",
      " [ 0.06160026]\n",
      " [-0.03864704]]\n",
      "t [[-0.82143354]\n",
      " [-0.07346858]\n",
      " [ 0.03076656]\n",
      " ...\n",
      " [-0.07346858]\n",
      " [ 0.06160026]\n",
      " [-0.03864704]]\n",
      "t [[-0.8450492 ]\n",
      " [-0.07598262]\n",
      " [ 0.03179626]\n",
      " ...\n",
      " [-0.07598262]\n",
      " [ 0.06350981]\n",
      " [-0.03997671]]\n",
      "t [[-0.8450492 ]\n",
      " [-0.07598262]\n",
      " [ 0.03179626]\n",
      " ...\n",
      " [-0.07598262]\n",
      " [ 0.06350981]\n",
      " [-0.03997671]]\n",
      "Current iteration=28, loss=51982.45577537387\n",
      "t [[-0.8682569 ]\n",
      " [-0.07848336]\n",
      " [ 0.03281803]\n",
      " ...\n",
      " [-0.07848336]\n",
      " [ 0.06539594]\n",
      " [-0.04129989]]\n",
      "t [[-0.8682569 ]\n",
      " [-0.07848336]\n",
      " [ 0.03281803]\n",
      " ...\n",
      " [-0.07848336]\n",
      " [ 0.06539594]\n",
      " [-0.04129989]]\n",
      "t [[-0.89106379]\n",
      " [-0.08097094]\n",
      " [ 0.03383189]\n",
      " ...\n",
      " [-0.08097094]\n",
      " [ 0.06725893]\n",
      " [-0.04261666]]\n",
      "t [[-0.89106379]\n",
      " [-0.08097094]\n",
      " [ 0.03383189]\n",
      " ...\n",
      " [-0.08097094]\n",
      " [ 0.06725893]\n",
      " [-0.04261666]]\n",
      "Current iteration=30, loss=51881.325422412716\n",
      "t [[-0.91347692]\n",
      " [-0.08344549]\n",
      " [ 0.03483784]\n",
      " ...\n",
      " [-0.08344549]\n",
      " [ 0.0690991 ]\n",
      " [-0.04392707]]\n",
      "t [[-0.91347692]\n",
      " [-0.08344549]\n",
      " [ 0.03483784]\n",
      " ...\n",
      " [-0.08344549]\n",
      " [ 0.0690991 ]\n",
      " [-0.04392707]]\n",
      "t [[-0.93550319]\n",
      " [-0.08590716]\n",
      " [ 0.03583589]\n",
      " ...\n",
      " [-0.08590716]\n",
      " [ 0.07091671]\n",
      " [-0.04523122]]\n",
      "t [[-0.93550319]\n",
      " [-0.08590716]\n",
      " [ 0.03583589]\n",
      " ...\n",
      " [-0.08590716]\n",
      " [ 0.07091671]\n",
      " [-0.04523122]]\n",
      "Current iteration=32, loss=51782.78156911905\n",
      "t [[-0.95714939]\n",
      " [-0.08835607]\n",
      " [ 0.03682603]\n",
      " ...\n",
      " [-0.08835607]\n",
      " [ 0.07271205]\n",
      " [-0.04652915]]\n",
      "t [[-0.95714939]\n",
      " [-0.08835607]\n",
      " [ 0.03682603]\n",
      " ...\n",
      " [-0.08835607]\n",
      " [ 0.07271205]\n",
      " [-0.04652915]]\n",
      "t [[-0.97842219]\n",
      " [-0.09079236]\n",
      " [ 0.03780827]\n",
      " ...\n",
      " [-0.09079236]\n",
      " [ 0.0744854 ]\n",
      " [-0.04782095]]\n",
      "t [[-0.97842219]\n",
      " [-0.09079236]\n",
      " [ 0.03780827]\n",
      " ...\n",
      " [-0.09079236]\n",
      " [ 0.0744854 ]\n",
      " [-0.04782095]]\n",
      "Current iteration=34, loss=51686.713447916496\n",
      "t [[-0.99932813]\n",
      " [-0.09321616]\n",
      " [ 0.03878262]\n",
      " ...\n",
      " [-0.09321616]\n",
      " [ 0.07623704]\n",
      " [-0.04910668]]\n",
      "t [[-0.99932813]\n",
      " [-0.09321616]\n",
      " [ 0.03878262]\n",
      " ...\n",
      " [-0.09321616]\n",
      " [ 0.07623704]\n",
      " [-0.04910668]]\n",
      "t [[-1.01987367]\n",
      " [-0.0956276 ]\n",
      " [ 0.03974909]\n",
      " ...\n",
      " [-0.0956276 ]\n",
      " [ 0.07796724]\n",
      " [-0.0503864 ]]\n",
      "t [[-1.01987367]\n",
      " [-0.0956276 ]\n",
      " [ 0.03974909]\n",
      " ...\n",
      " [-0.0956276 ]\n",
      " [ 0.07796724]\n",
      " [-0.0503864 ]]\n",
      "Current iteration=36, loss=51593.01625145912\n",
      "t [[-1.0400651 ]\n",
      " [-0.0980268 ]\n",
      " [ 0.04070768]\n",
      " ...\n",
      " [-0.0980268 ]\n",
      " [ 0.07967626]\n",
      " [-0.05166018]]\n",
      "t [[-1.0400651 ]\n",
      " [-0.0980268 ]\n",
      " [ 0.04070768]\n",
      " ...\n",
      " [-0.0980268 ]\n",
      " [ 0.07967626]\n",
      " [-0.05166018]]\n",
      "t [[-1.05990864]\n",
      " [-0.10041389]\n",
      " [ 0.04165841]\n",
      " ...\n",
      " [-0.10041389]\n",
      " [ 0.08136437]\n",
      " [-0.05292808]]\n",
      "t [[-1.05990864]\n",
      " [-0.10041389]\n",
      " [ 0.04165841]\n",
      " ...\n",
      " [-0.10041389]\n",
      " [ 0.08136437]\n",
      " [-0.05292808]]\n",
      "Current iteration=38, loss=51501.590796536795\n",
      "t [[-1.07941039]\n",
      " [-0.10278899]\n",
      " [ 0.04260127]\n",
      " ...\n",
      " [-0.10278899]\n",
      " [ 0.08303183]\n",
      " [-0.05419016]]\n",
      "t [[-1.07941039]\n",
      " [-0.10278899]\n",
      " [ 0.04260127]\n",
      " ...\n",
      " [-0.10278899]\n",
      " [ 0.08303183]\n",
      " [-0.05419016]]\n",
      "t [[-1.09857631]\n",
      " [-0.10515222]\n",
      " [ 0.04353629]\n",
      " ...\n",
      " [-0.10515222]\n",
      " [ 0.08467889]\n",
      " [-0.0554465 ]]\n",
      "t [[-1.09857631]\n",
      " [-0.10515222]\n",
      " [ 0.04353629]\n",
      " ...\n",
      " [-0.10515222]\n",
      " [ 0.08467889]\n",
      " [-0.0554465 ]]\n",
      "Current iteration=40, loss=51412.343204699726\n",
      "t [[-1.11741229]\n",
      " [-0.10750371]\n",
      " [ 0.04446347]\n",
      " ...\n",
      " [-0.10750371]\n",
      " [ 0.08630581]\n",
      " [-0.05669714]]\n",
      "t [[-1.11741229]\n",
      " [-0.10750371]\n",
      " [ 0.04446347]\n",
      " ...\n",
      " [-0.10750371]\n",
      " [ 0.08630581]\n",
      " [-0.05669714]]\n",
      "t [[-1.13592408]\n",
      " [-0.10984357]\n",
      " [ 0.04538283]\n",
      " ...\n",
      " [-0.10984357]\n",
      " [ 0.08791283]\n",
      " [-0.05794215]]\n",
      "t [[-1.13592408]\n",
      " [-0.10984357]\n",
      " [ 0.04538283]\n",
      " ...\n",
      " [-0.10984357]\n",
      " [ 0.08791283]\n",
      " [-0.05794215]]\n",
      "Current iteration=42, loss=51325.184599265616\n",
      "t [[-1.15411736]\n",
      " [-0.11217191]\n",
      " [ 0.04629438]\n",
      " ...\n",
      " [-0.11217191]\n",
      " [ 0.08950021]\n",
      " [-0.05918158]]\n",
      "t [[-1.15411736]\n",
      " [-0.11217191]\n",
      " [ 0.04629438]\n",
      " ...\n",
      " [-0.11217191]\n",
      " [ 0.08950021]\n",
      " [-0.05918158]]\n",
      "t [[-1.17199768]\n",
      " [-0.11448885]\n",
      " [ 0.04719812]\n",
      " ...\n",
      " [-0.11448885]\n",
      " [ 0.09106818]\n",
      " [-0.0604155 ]]\n",
      "t [[-1.17199768]\n",
      " [-0.11448885]\n",
      " [ 0.04719812]\n",
      " ...\n",
      " [-0.11448885]\n",
      " [ 0.09106818]\n",
      " [-0.0604155 ]]\n",
      "Current iteration=44, loss=51240.03081828476\n",
      "t [[-1.18957049]\n",
      " [-0.11679451]\n",
      " [ 0.04809409]\n",
      " ...\n",
      " [-0.11679451]\n",
      " [ 0.09261698]\n",
      " [-0.06164397]]\n",
      "t [[-1.18957049]\n",
      " [-0.11679451]\n",
      " [ 0.04809409]\n",
      " ...\n",
      " [-0.11679451]\n",
      " [ 0.09261698]\n",
      " [-0.06164397]]\n",
      "t [[-1.20684114]\n",
      " [-0.119089  ]\n",
      " [ 0.04898228]\n",
      " ...\n",
      " [-0.119089  ]\n",
      " [ 0.09414685]\n",
      " [-0.06286703]]\n",
      "t [[-1.20684114]\n",
      " [-0.119089  ]\n",
      " [ 0.04898228]\n",
      " ...\n",
      " [-0.119089  ]\n",
      " [ 0.09414685]\n",
      " [-0.06286703]]\n",
      "Current iteration=46, loss=51156.80214296672\n",
      "t [[-1.2238149 ]\n",
      " [-0.12137243]\n",
      " [ 0.04986271]\n",
      " ...\n",
      " [-0.12137243]\n",
      " [ 0.09565802]\n",
      " [-0.06408474]]\n",
      "t [[-1.2238149 ]\n",
      " [-0.12137243]\n",
      " [ 0.04986271]\n",
      " ...\n",
      " [-0.12137243]\n",
      " [ 0.09565802]\n",
      " [-0.06408474]]\n",
      "t [[-1.24049693]\n",
      " [-0.1236449 ]\n",
      " [ 0.05073541]\n",
      " ...\n",
      " [-0.1236449 ]\n",
      " [ 0.09715071]\n",
      " [-0.06529717]]\n",
      "t [[-1.24049693]\n",
      " [-0.1236449 ]\n",
      " [ 0.05073541]\n",
      " ...\n",
      " [-0.1236449 ]\n",
      " [ 0.09715071]\n",
      " [-0.06529717]]\n",
      "Current iteration=48, loss=51075.42304102473\n",
      "t [[-1.25689228]\n",
      " [-0.12590652]\n",
      " [ 0.05160038]\n",
      " ...\n",
      " [-0.12590652]\n",
      " [ 0.09862516]\n",
      " [-0.06650435]]\n",
      "t [[-1.25689228]\n",
      " [-0.12590652]\n",
      " [ 0.05160038]\n",
      " ...\n",
      " [-0.12590652]\n",
      " [ 0.09862516]\n",
      " [-0.06650435]]\n",
      "t [[-1.27300594]\n",
      " [-0.1281574 ]\n",
      " [ 0.05245765]\n",
      " ...\n",
      " [-0.1281574 ]\n",
      " [ 0.10008158]\n",
      " [-0.06770635]]\n",
      "loss=50995.821924356045\n",
      "Cross validation finished: optimal gamma 0.01\n",
      "logistic regression loss 51035.40456105957\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set2_x.shape[1])\n",
    "gamma_opt = cross_validation(set2_y, set2_x, k_fold, gammas, fonction=4)\n",
    "w_lr2, loss_lr = logistic_regression(set2_y,set2_x, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"logistic regression loss {loss}\".format(loss=loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=37710.672358363896\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=37710.672358363896\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=37710.672358363896\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss=37710.67235836389\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.00916416]\n",
      " [-0.00192785]\n",
      " [ 0.0019134 ]\n",
      " ...\n",
      " [-0.00241862]\n",
      " [-0.00300713]\n",
      " [-0.00063217]]\n",
      "t [[-0.00916416]\n",
      " [-0.00192785]\n",
      " [ 0.0019134 ]\n",
      " ...\n",
      " [-0.00241862]\n",
      " [-0.00300713]\n",
      " [-0.00063217]]\n",
      "t [[-0.01826613]\n",
      " [-0.00382856]\n",
      " [ 0.00381913]\n",
      " ...\n",
      " [-0.00482649]\n",
      " [-0.00599834]\n",
      " [-0.00126409]]\n",
      "t [[-0.01826613]\n",
      " [-0.00382856]\n",
      " [ 0.00381913]\n",
      " ...\n",
      " [-0.00482649]\n",
      " [-0.00599834]\n",
      " [-0.00126409]]\n",
      "Current iteration=2, loss=37566.621202691844\n",
      "t [[-0.02730627]\n",
      " [-0.00570255]\n",
      " [ 0.00571716]\n",
      " ...\n",
      " [-0.00722365]\n",
      " [-0.00897374]\n",
      " [-0.00189573]]\n",
      "t [[-0.02730627]\n",
      " [-0.00570255]\n",
      " [ 0.00571716]\n",
      " ...\n",
      " [-0.00722365]\n",
      " [-0.00897374]\n",
      " [-0.00189573]]\n",
      "t [[-0.03628493]\n",
      " [-0.0075502 ]\n",
      " [ 0.00760744]\n",
      " ...\n",
      " [-0.00961016]\n",
      " [-0.01193341]\n",
      " [-0.00252706]]\n",
      "t [[-0.03628493]\n",
      " [-0.0075502 ]\n",
      " [ 0.00760744]\n",
      " ...\n",
      " [-0.00961016]\n",
      " [-0.01193341]\n",
      " [-0.00252706]]\n",
      "Current iteration=4, loss=37425.89485938753\n",
      "t [[-0.04520246]\n",
      " [-0.0093719 ]\n",
      " [ 0.00948993]\n",
      " ...\n",
      " [-0.01198606]\n",
      " [-0.01487744]\n",
      " [-0.00315806]]\n",
      "t [[-0.04520246]\n",
      " [-0.0093719 ]\n",
      " [ 0.00948993]\n",
      " ...\n",
      " [-0.01198606]\n",
      " [-0.01487744]\n",
      " [-0.00315806]]\n",
      "t [[-0.05405921]\n",
      " [-0.01116806]\n",
      " [ 0.01136458]\n",
      " ...\n",
      " [-0.01435141]\n",
      " [-0.01780594]\n",
      " [-0.00378869]]\n",
      "t [[-0.05405921]\n",
      " [-0.01116806]\n",
      " [ 0.01136458]\n",
      " ...\n",
      " [-0.01435141]\n",
      " [-0.01780594]\n",
      " [-0.00378869]]\n",
      "Current iteration=6, loss=37288.41106393095\n",
      "t [[-0.06285555]\n",
      " [-0.01293905]\n",
      " [ 0.01323136]\n",
      " ...\n",
      " [-0.01670624]\n",
      " [-0.02071898]\n",
      " [-0.00441893]]\n",
      "t [[-0.06285555]\n",
      " [-0.01293905]\n",
      " [ 0.01323136]\n",
      " ...\n",
      " [-0.01670624]\n",
      " [-0.02071898]\n",
      " [-0.00441893]]\n",
      "t [[-0.07159181]\n",
      " [-0.01468525]\n",
      " [ 0.01509024]\n",
      " ...\n",
      " [-0.01905062]\n",
      " [-0.02361668]\n",
      " [-0.00504875]]\n",
      "t [[-0.07159181]\n",
      " [-0.01468525]\n",
      " [ 0.01509024]\n",
      " ...\n",
      " [-0.01905062]\n",
      " [-0.02361668]\n",
      " [-0.00504875]]\n",
      "Current iteration=8, loss=37154.089217086024\n",
      "t [[-0.08026836]\n",
      " [-0.01640703]\n",
      " [ 0.01694116]\n",
      " ...\n",
      " [-0.02138458]\n",
      " [-0.02649911]\n",
      " [-0.00567812]]\n",
      "t [[-0.08026836]\n",
      " [-0.01640703]\n",
      " [ 0.01694116]\n",
      " ...\n",
      " [-0.02138458]\n",
      " [-0.02649911]\n",
      " [-0.00567812]]\n",
      "t [[-0.08888555]\n",
      " [-0.01810478]\n",
      " [ 0.01878411]\n",
      " ...\n",
      " [-0.02370818]\n",
      " [-0.02936638]\n",
      " [-0.00630702]]\n",
      "loss=37022.85039036967\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.01076255]\n",
      " [-0.00242466]\n",
      " [ 0.00094711]\n",
      " ...\n",
      " [-0.0023249 ]\n",
      " [-0.00302276]\n",
      " [-0.00062742]]\n",
      "t [[-0.01076255]\n",
      " [-0.00242466]\n",
      " [ 0.00094711]\n",
      " ...\n",
      " [-0.0023249 ]\n",
      " [-0.00302276]\n",
      " [-0.00062742]]\n",
      "t [[-0.02146201]\n",
      " [-0.00484666]\n",
      " [ 0.00190032]\n",
      " ...\n",
      " [-0.0046394 ]\n",
      " [-0.00602948]\n",
      " [-0.0012549 ]]\n",
      "t [[-0.02146201]\n",
      " [-0.00484666]\n",
      " [ 0.00190032]\n",
      " ...\n",
      " [-0.0046394 ]\n",
      " [-0.00602948]\n",
      " [-0.0012549 ]]\n",
      "Current iteration=2, loss=37565.30467557325\n",
      "t [[-0.03209883]\n",
      " [-0.00726588]\n",
      " [ 0.00285944]\n",
      " ...\n",
      " [-0.00694357]\n",
      " [-0.00902024]\n",
      " [-0.00188241]]\n",
      "t [[-0.03209883]\n",
      " [-0.00726588]\n",
      " [ 0.00285944]\n",
      " ...\n",
      " [-0.00694357]\n",
      " [-0.00902024]\n",
      " [-0.00188241]]\n",
      "t [[-0.04267343]\n",
      " [-0.00968221]\n",
      " [ 0.00382428]\n",
      " ...\n",
      " [-0.00923744]\n",
      " [-0.01199514]\n",
      " [-0.00250992]]\n",
      "t [[-0.04267343]\n",
      " [-0.00968221]\n",
      " [ 0.00382428]\n",
      " ...\n",
      " [-0.00923744]\n",
      " [-0.01199514]\n",
      " [-0.00250992]]\n",
      "Current iteration=4, loss=37423.31145178205\n",
      "t [[-0.05318623]\n",
      " [-0.01209556]\n",
      " [ 0.00479465]\n",
      " ...\n",
      " [-0.01152107]\n",
      " [-0.01495428]\n",
      " [-0.00313738]]\n",
      "t [[-0.05318623]\n",
      " [-0.01209556]\n",
      " [ 0.00479465]\n",
      " ...\n",
      " [-0.01152107]\n",
      " [-0.01495428]\n",
      " [-0.00313738]]\n",
      "t [[-0.06363766]\n",
      " [-0.01450582]\n",
      " [ 0.00577037]\n",
      " ...\n",
      " [-0.01379451]\n",
      " [-0.01789774]\n",
      " [-0.00376476]]\n",
      "t [[-0.06363766]\n",
      " [-0.01450582]\n",
      " [ 0.00577037]\n",
      " ...\n",
      " [-0.01379451]\n",
      " [-0.01789774]\n",
      " [-0.00376476]]\n",
      "Current iteration=6, loss=37284.60843991852\n",
      "t [[-0.07402815]\n",
      " [-0.0169129 ]\n",
      " [ 0.00675126]\n",
      " ...\n",
      " [-0.0160578 ]\n",
      " [-0.02082563]\n",
      " [-0.00439204]]\n",
      "t [[-0.07402815]\n",
      " [-0.0169129 ]\n",
      " [ 0.00675126]\n",
      " ...\n",
      " [-0.0160578 ]\n",
      " [-0.02082563]\n",
      " [-0.00439204]]\n",
      "t [[-0.08435811]\n",
      " [-0.0193167 ]\n",
      " [ 0.00773714]\n",
      " ...\n",
      " [-0.018311  ]\n",
      " [-0.02373804]\n",
      " [-0.00501917]]\n",
      "t [[-0.08435811]\n",
      " [-0.0193167 ]\n",
      " [ 0.00773714]\n",
      " ...\n",
      " [-0.018311  ]\n",
      " [-0.02373804]\n",
      " [-0.00501917]]\n",
      "Current iteration=8, loss=37149.11312603085\n",
      "t [[-0.09462798]\n",
      " [-0.02171713]\n",
      " [ 0.00872783]\n",
      " ...\n",
      " [-0.02055415]\n",
      " [-0.02663506]\n",
      " [-0.00564612]]\n",
      "t [[-0.09462798]\n",
      " [-0.02171713]\n",
      " [ 0.00872783]\n",
      " ...\n",
      " [-0.02055415]\n",
      " [-0.02663506]\n",
      " [-0.00564612]]\n",
      "t [[-0.10483816]\n",
      " [-0.02411408]\n",
      " [ 0.00972317]\n",
      " ...\n",
      " [-0.02278731]\n",
      " [-0.02951679]\n",
      " [-0.00627286]]\n",
      "loss=37016.74473355381\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.01079007]\n",
      " [-0.00250078]\n",
      " [ 0.00064991]\n",
      " ...\n",
      " [-0.00229809]\n",
      " [-0.00304052]\n",
      " [-0.00061497]]\n",
      "t [[-0.01079007]\n",
      " [-0.00250078]\n",
      " [ 0.00064991]\n",
      " ...\n",
      " [-0.00229809]\n",
      " [-0.00304052]\n",
      " [-0.00061497]]\n",
      "t [[-0.02151696]\n",
      " [-0.00499809]\n",
      " [ 0.00130926]\n",
      " ...\n",
      " [-0.00458638]\n",
      " [-0.0060649 ]\n",
      " [-0.00123005]]\n",
      "t [[-0.02151696]\n",
      " [-0.00499809]\n",
      " [ 0.00130926]\n",
      " ...\n",
      " [-0.00458638]\n",
      " [-0.0060649 ]\n",
      " [-0.00123005]]\n",
      "Current iteration=2, loss=37563.39646908167\n",
      "t [[-0.0321811 ]\n",
      " [-0.00749183]\n",
      " [ 0.00197783]\n",
      " ...\n",
      " [-0.00686491]\n",
      " [-0.00907324]\n",
      " [-0.0018452 ]]\n",
      "t [[-0.0321811 ]\n",
      " [-0.00749183]\n",
      " [ 0.00197783]\n",
      " ...\n",
      " [-0.00686491]\n",
      " [-0.00907324]\n",
      " [-0.0018452 ]]\n",
      "t [[-0.04278292]\n",
      " [-0.00998191]\n",
      " [ 0.00265539]\n",
      " ...\n",
      " [-0.00913373]\n",
      " [-0.01206562]\n",
      " [-0.00246038]]\n",
      "t [[-0.04278292]\n",
      " [-0.00998191]\n",
      " [ 0.00265539]\n",
      " ...\n",
      " [-0.00913373]\n",
      " [-0.01206562]\n",
      " [-0.00246038]]\n",
      "Current iteration=4, loss=37419.540442480655\n",
      "t [[-0.05332283]\n",
      " [-0.01246824]\n",
      " [ 0.00334174]\n",
      " ...\n",
      " [-0.01139288]\n",
      " [-0.01504216]\n",
      " [-0.00307556]]\n",
      "t [[-0.05332283]\n",
      " [-0.01246824]\n",
      " [ 0.00334174]\n",
      " ...\n",
      " [-0.01139288]\n",
      " [-0.01504216]\n",
      " [-0.00307556]]\n",
      "t [[-0.06380126]\n",
      " [-0.01495073]\n",
      " [ 0.00403665]\n",
      " ...\n",
      " [-0.0136424 ]\n",
      " [-0.01800294]\n",
      " [-0.00369072]]\n",
      "t [[-0.06380126]\n",
      " [-0.01495073]\n",
      " [ 0.00403665]\n",
      " ...\n",
      " [-0.0136424 ]\n",
      " [-0.01800294]\n",
      " [-0.00369072]]\n",
      "Current iteration=6, loss=37279.01899327821\n",
      "t [[-0.07421863]\n",
      " [-0.0174293 ]\n",
      " [ 0.00473993]\n",
      " ...\n",
      " [-0.01588233]\n",
      " [-0.02094805]\n",
      " [-0.00430581]]\n",
      "t [[-0.07421863]\n",
      " [-0.0174293 ]\n",
      " [ 0.00473993]\n",
      " ...\n",
      " [-0.01588233]\n",
      " [-0.02094805]\n",
      " [-0.00430581]]\n",
      "t [[-0.08457537]\n",
      " [-0.01990385]\n",
      " [ 0.00545135]\n",
      " ...\n",
      " [-0.01811272]\n",
      " [-0.0238776 ]\n",
      " [-0.0049208 ]]\n",
      "t [[-0.08457537]\n",
      " [-0.01990385]\n",
      " [ 0.00545135]\n",
      " ...\n",
      " [-0.01811272]\n",
      " [-0.0238776 ]\n",
      " [-0.0049208 ]]\n",
      "Current iteration=8, loss=37141.748577260056\n",
      "t [[-0.0948719 ]\n",
      " [-0.02237431]\n",
      " [ 0.00617073]\n",
      " ...\n",
      " [-0.02033361]\n",
      " [-0.02679167]\n",
      " [-0.00553567]]\n",
      "t [[-0.0948719 ]\n",
      " [-0.02237431]\n",
      " [ 0.00617073]\n",
      " ...\n",
      " [-0.02033361]\n",
      " [-0.02679167]\n",
      " [-0.00553567]]\n",
      "t [[-0.10510863]\n",
      " [-0.0248406 ]\n",
      " [ 0.00689785]\n",
      " ...\n",
      " [-0.02254504]\n",
      " [-0.02969037]\n",
      " [-0.00615039]]\n",
      "loss=37007.6473971183\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.01070497]\n",
      " [-0.00232837]\n",
      " [ 0.00076209]\n",
      " ...\n",
      " [ 0.01199829]\n",
      " [-0.00883439]\n",
      " [ 0.00973201]]\n",
      "t [[-0.01070497]\n",
      " [-0.00232837]\n",
      " [ 0.00076209]\n",
      " ...\n",
      " [ 0.01199829]\n",
      " [-0.00883439]\n",
      " [ 0.00973201]]\n",
      "t [[-0.02134756]\n",
      " [-0.00465497]\n",
      " [ 0.00153252]\n",
      " ...\n",
      " [ 0.0239153 ]\n",
      " [-0.01761283]\n",
      " [ 0.01940319]]\n",
      "t [[-0.02134756]\n",
      " [-0.00465497]\n",
      " [ 0.00153252]\n",
      " ...\n",
      " [ 0.0239153 ]\n",
      " [-0.01761283]\n",
      " [ 0.01940319]]\n",
      "Current iteration=2, loss=37568.57320878067\n",
      "t [[-0.03192822]\n",
      " [-0.00697966]\n",
      " [ 0.00231108]\n",
      " ...\n",
      " [ 0.03575155]\n",
      " [-0.02633564]\n",
      " [ 0.02901391]]\n",
      "t [[-0.03192822]\n",
      " [-0.00697966]\n",
      " [ 0.00231108]\n",
      " ...\n",
      " [ 0.03575155]\n",
      " [-0.02633564]\n",
      " [ 0.02901391]]\n",
      "t [[-0.04244735]\n",
      " [-0.00930234]\n",
      " [ 0.00309755]\n",
      " ...\n",
      " [ 0.04750758]\n",
      " [-0.03500318]\n",
      " [ 0.03856457]]\n",
      "t [[-0.04244735]\n",
      " [-0.00930234]\n",
      " [ 0.00309755]\n",
      " ...\n",
      " [ 0.04750758]\n",
      " [-0.03500318]\n",
      " [ 0.03856457]]\n",
      "Current iteration=4, loss=37429.75190646807\n",
      "t [[-0.05290539]\n",
      " [-0.0116229 ]\n",
      " [ 0.00389173]\n",
      " ...\n",
      " [ 0.05918394]\n",
      " [-0.04361578]\n",
      " [ 0.04805554]]\n",
      "t [[-0.05290539]\n",
      " [-0.0116229 ]\n",
      " [ 0.00389173]\n",
      " ...\n",
      " [ 0.05918394]\n",
      " [-0.04361578]\n",
      " [ 0.04805554]]\n",
      "t [[-0.06330275]\n",
      " [-0.0139412 ]\n",
      " [ 0.00469341]\n",
      " ...\n",
      " [ 0.07078114]\n",
      " [-0.05217379]\n",
      " [ 0.05748721]]\n",
      "t [[-0.06330275]\n",
      " [-0.0139412 ]\n",
      " [ 0.00469341]\n",
      " ...\n",
      " [ 0.07078114]\n",
      " [-0.05217379]\n",
      " [ 0.05748721]]\n",
      "Current iteration=6, loss=37294.12692752312\n",
      "t [[-0.07363984]\n",
      " [-0.01625716]\n",
      " [ 0.00550239]\n",
      " ...\n",
      " [ 0.08229974]\n",
      " [-0.06067754]\n",
      " [ 0.06685996]]\n",
      "t [[-0.07363984]\n",
      " [-0.01625716]\n",
      " [ 0.00550239]\n",
      " ...\n",
      " [ 0.08229974]\n",
      " [-0.06067754]\n",
      " [ 0.06685996]]\n",
      "t [[-0.0839171 ]\n",
      " [-0.01857066]\n",
      " [ 0.00631847]\n",
      " ...\n",
      " [ 0.09374025]\n",
      " [-0.06912739]\n",
      " [ 0.07617418]]\n",
      "t [[-0.0839171 ]\n",
      " [-0.01857066]\n",
      " [ 0.00631847]\n",
      " ...\n",
      " [ 0.09374025]\n",
      " [-0.06912739]\n",
      " [ 0.07617418]]\n",
      "Current iteration=8, loss=37161.61843219875\n",
      "t [[-0.09413494]\n",
      " [-0.02088159]\n",
      " [ 0.00714144]\n",
      " ...\n",
      " [ 0.10510321]\n",
      " [-0.07752366]\n",
      " [ 0.08543023]]\n",
      "t [[-0.09413494]\n",
      " [-0.02088159]\n",
      " [ 0.00714144]\n",
      " ...\n",
      " [ 0.10510321]\n",
      " [-0.07752366]\n",
      " [ 0.08543023]]\n",
      "t [[-0.10429376]\n",
      " [-0.02318986]\n",
      " [ 0.00797112]\n",
      " ...\n",
      " [ 0.11638915]\n",
      " [-0.0858667 ]\n",
      " [ 0.09462851]]\n",
      "loss=37032.14826748679\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.01832832]\n",
      " [-0.0038557 ]\n",
      " [ 0.00382679]\n",
      " ...\n",
      " [-0.00483724]\n",
      " [-0.00601426]\n",
      " [-0.00126433]]\n",
      "t [[-0.01832832]\n",
      " [-0.0038557 ]\n",
      " [ 0.00382679]\n",
      " ...\n",
      " [-0.00483724]\n",
      " [-0.00601426]\n",
      " [-0.00126433]]\n",
      "t [[-0.0364079 ]\n",
      " [-0.00760286]\n",
      " [ 0.00762295]\n",
      " ...\n",
      " [-0.00963147]\n",
      " [-0.01196486]\n",
      " [-0.00252768]]\n",
      "t [[-0.0364079 ]\n",
      " [-0.00760286]\n",
      " [ 0.00762295]\n",
      " ...\n",
      " [-0.00963147]\n",
      " [-0.01196486]\n",
      " [-0.00252768]]\n",
      "Current iteration=2, loss=37425.07908448187\n",
      "t [[-0.05424157]\n",
      " [-0.01124472]\n",
      " [ 0.01138809]\n",
      " ...\n",
      " [-0.01438309]\n",
      " [-0.01785256]\n",
      " [-0.00378979]]\n",
      "t [[-0.05424157]\n",
      " [-0.01124472]\n",
      " [ 0.01138809]\n",
      " ...\n",
      " [-0.01438309]\n",
      " [-0.01785256]\n",
      " [-0.00378979]]\n",
      "t [[-0.07183217]\n",
      " [-0.01478442]\n",
      " [ 0.01512189]\n",
      " ...\n",
      " [-0.01909248]\n",
      " [-0.02367812]\n",
      " [-0.00505044]]\n",
      "t [[-0.07183217]\n",
      " [-0.01478442]\n",
      " [ 0.01512189]\n",
      " ...\n",
      " [-0.01909248]\n",
      " [-0.02367812]\n",
      " [-0.00505044]]\n",
      "Current iteration=4, loss=37152.53715338473\n",
      "t [[-0.08918254]\n",
      " [-0.01822504]\n",
      " [ 0.01882403]\n",
      " ...\n",
      " [-0.02376004]\n",
      " [-0.02944227]\n",
      " [-0.0063094 ]]\n",
      "t [[-0.08918254]\n",
      " [-0.01822504]\n",
      " [ 0.01882403]\n",
      " ...\n",
      " [-0.02376004]\n",
      " [-0.02944227]\n",
      " [-0.0063094 ]]\n",
      "t [[-0.10629554]\n",
      " [-0.02156959]\n",
      " [ 0.02249423]\n",
      " ...\n",
      " [-0.02838616]\n",
      " [-0.03514577]\n",
      " [-0.00756645]]\n",
      "t [[-0.10629554]\n",
      " [-0.02156959]\n",
      " [ 0.02249423]\n",
      " ...\n",
      " [-0.02838616]\n",
      " [-0.03514577]\n",
      " [-0.00756645]]\n",
      "Current iteration=6, loss=36892.40273447211\n",
      "t [[-0.12317401]\n",
      " [-0.024821  ]\n",
      " [ 0.02613225]\n",
      " ...\n",
      " [-0.03297124]\n",
      " [-0.04078936]\n",
      " [-0.0088214 ]]\n",
      "t [[-0.12317401]\n",
      " [-0.024821  ]\n",
      " [ 0.02613225]\n",
      " ...\n",
      " [-0.03297124]\n",
      " [-0.04078936]\n",
      " [-0.0088214 ]]\n",
      "t [[-0.13982081]\n",
      " [-0.02798213]\n",
      " [ 0.02973786]\n",
      " ...\n",
      " [-0.03751567]\n",
      " [-0.04637378]\n",
      " [-0.01007404]]\n",
      "t [[-0.13982081]\n",
      " [-0.02798213]\n",
      " [ 0.02973786]\n",
      " ...\n",
      " [-0.03751567]\n",
      " [-0.04637378]\n",
      " [-0.01007404]]\n",
      "Current iteration=8, loss=36644.0589923739\n",
      "t [[-0.15623879]\n",
      " [-0.03105576]\n",
      " [ 0.03331086]\n",
      " ...\n",
      " [-0.04201985]\n",
      " [-0.05189979]\n",
      " [-0.01132418]]\n",
      "t [[-0.15623879]\n",
      " [-0.03105576]\n",
      " [ 0.03331086]\n",
      " ...\n",
      " [-0.04201985]\n",
      " [-0.05189979]\n",
      " [-0.01132418]]\n",
      "t [[-0.17243081]\n",
      " [-0.03404463]\n",
      " [ 0.0368511 ]\n",
      " ...\n",
      " [-0.04648419]\n",
      " [-0.0573681 ]\n",
      " [-0.01257165]]\n",
      "loss=36406.91600258473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.02152509]\n",
      " [-0.00484933]\n",
      " [ 0.00189421]\n",
      " ...\n",
      " [-0.0046498 ]\n",
      " [-0.00604553]\n",
      " [-0.00125484]]\n",
      "t [[-0.02152509]\n",
      " [-0.00484933]\n",
      " [ 0.00189421]\n",
      " ...\n",
      " [-0.0046498 ]\n",
      " [-0.00604553]\n",
      " [-0.00125484]]\n",
      "t [[-0.04279788]\n",
      " [-0.00968798]\n",
      " [ 0.00381284]\n",
      " ...\n",
      " [-0.00925803]\n",
      " [-0.01202686]\n",
      " [-0.00250993]]\n",
      "t [[-0.04279788]\n",
      " [-0.00968798]\n",
      " [ 0.00381284]\n",
      " ...\n",
      " [-0.00925803]\n",
      " [-0.01202686]\n",
      " [-0.00250993]]\n",
      "Current iteration=2, loss=37422.48363875184\n",
      "t [[-0.0638218 ]\n",
      " [-0.01451509]\n",
      " [ 0.00575433]\n",
      " ...\n",
      " [-0.0138251 ]\n",
      " [-0.01794476]\n",
      " [-0.003765  ]]\n",
      "t [[-0.0638218 ]\n",
      " [-0.01451509]\n",
      " [ 0.00575433]\n",
      " ...\n",
      " [-0.0138251 ]\n",
      " [-0.01794476]\n",
      " [-0.003765  ]]\n",
      "t [[-0.0846003 ]\n",
      " [-0.01932983]\n",
      " [ 0.00771719]\n",
      " ...\n",
      " [-0.0183514 ]\n",
      " [-0.02379999]\n",
      " [-0.00501975]]\n",
      "t [[-0.0846003 ]\n",
      " [-0.01932983]\n",
      " [ 0.00771719]\n",
      " ...\n",
      " [-0.0183514 ]\n",
      " [-0.02379999]\n",
      " [-0.00501975]]\n",
      "Current iteration=4, loss=37147.53886847838\n",
      "t [[-0.10513679]\n",
      " [-0.02413142]\n",
      " [ 0.00969996]\n",
      " ...\n",
      " [-0.02283733]\n",
      " [-0.02959331]\n",
      " [-0.00627391]]\n",
      "t [[-0.10513679]\n",
      " [-0.02413142]\n",
      " [ 0.00969996]\n",
      " ...\n",
      " [-0.02283733]\n",
      " [-0.02959331]\n",
      " [-0.00627391]]\n",
      "t [[-0.12543466]\n",
      " [-0.02891911]\n",
      " [ 0.01170125]\n",
      " ...\n",
      " [-0.02728329]\n",
      " [-0.03532547]\n",
      " [-0.00752724]]\n",
      "t [[-0.12543466]\n",
      " [-0.02891911]\n",
      " [ 0.01170125]\n",
      " ...\n",
      " [-0.02728329]\n",
      " [-0.03532547]\n",
      " [-0.00752724]]\n",
      "Current iteration=6, loss=36885.17895079388\n",
      "t [[-0.14549727]\n",
      " [-0.03369217]\n",
      " [ 0.01371968]\n",
      " ...\n",
      " [-0.03168968]\n",
      " [-0.04099723]\n",
      " [-0.00877947]]\n",
      "t [[-0.14549727]\n",
      " [-0.03369217]\n",
      " [ 0.01371968]\n",
      " ...\n",
      " [-0.03168968]\n",
      " [-0.04099723]\n",
      " [-0.00877947]]\n",
      "t [[-0.16532797]\n",
      " [-0.03844992]\n",
      " [ 0.01575393]\n",
      " ...\n",
      " [-0.03605691]\n",
      " [-0.04660935]\n",
      " [-0.01003037]]\n",
      "t [[-0.16532797]\n",
      " [-0.03844992]\n",
      " [ 0.01575393]\n",
      " ...\n",
      " [-0.03605691]\n",
      " [-0.04660935]\n",
      " [-0.01003037]]\n",
      "Current iteration=8, loss=36634.772847838176\n",
      "t [[-0.18493008]\n",
      " [-0.04319171]\n",
      " [ 0.01780273]\n",
      " ...\n",
      " [-0.04038538]\n",
      " [-0.05216258]\n",
      " [-0.01127971]]\n",
      "t [[-0.18493008]\n",
      " [-0.04319171]\n",
      " [ 0.01780273]\n",
      " ...\n",
      " [-0.04038538]\n",
      " [-0.05216258]\n",
      " [-0.01127971]]\n",
      "t [[-0.20430689]\n",
      " [-0.04791692]\n",
      " [ 0.01986484]\n",
      " ...\n",
      " [-0.04467549]\n",
      " [-0.05765765]\n",
      " [-0.01252728]]\n",
      "loss=36395.71745010131\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.02158014]\n",
      " [-0.00500157]\n",
      " [ 0.00129983]\n",
      " ...\n",
      " [-0.00459617]\n",
      " [-0.00608104]\n",
      " [-0.00122995]]\n",
      "t [[-0.02158014]\n",
      " [-0.00500157]\n",
      " [ 0.00129983]\n",
      " ...\n",
      " [-0.00459617]\n",
      " [-0.00608104]\n",
      " [-0.00122995]]\n",
      "t [[-0.04290758]\n",
      " [-0.00998924]\n",
      " [ 0.0026374 ]\n",
      " ...\n",
      " [-0.00915316]\n",
      " [-0.01209752]\n",
      " [-0.00246031]]\n",
      "t [[-0.04290758]\n",
      " [-0.00998924]\n",
      " [ 0.0026374 ]\n",
      " ...\n",
      " [-0.00915316]\n",
      " [-0.01209752]\n",
      " [-0.00246031]]\n",
      "Current iteration=2, loss=37418.70147090237\n",
      "t [[-0.06398573]\n",
      " [-0.01496225]\n",
      " [ 0.00401095]\n",
      " ...\n",
      " [-0.01367129]\n",
      " [-0.01805022]\n",
      " [-0.00369081]]\n",
      "t [[-0.06398573]\n",
      " [-0.01496225]\n",
      " [ 0.00401095]\n",
      " ...\n",
      " [-0.01367129]\n",
      " [-0.01805022]\n",
      " [-0.00369081]]\n",
      "t [[-0.08481802]\n",
      " [-0.01991989]\n",
      " [ 0.00541874]\n",
      " ...\n",
      " [-0.01815091]\n",
      " [-0.0239399 ]\n",
      " [-0.00492118]]\n",
      "t [[-0.08481802]\n",
      " [-0.01991989]\n",
      " [ 0.00541874]\n",
      " ...\n",
      " [-0.01815091]\n",
      " [-0.0239399 ]\n",
      " [-0.00492118]]\n",
      "Current iteration=4, loss=37140.1530104415\n",
      "t [[-0.10540784]\n",
      " [-0.02486147]\n",
      " [ 0.00685909]\n",
      " ...\n",
      " [-0.02259237]\n",
      " [-0.02976731]\n",
      " [-0.00615117]]\n",
      "t [[-0.10540784]\n",
      " [-0.02486147]\n",
      " [ 0.00685909]\n",
      " ...\n",
      " [-0.02259237]\n",
      " [-0.02976731]\n",
      " [-0.00615117]]\n",
      "t [[-0.12575858]\n",
      " [-0.02978632]\n",
      " [ 0.00833038]\n",
      " ...\n",
      " [-0.026996  ]\n",
      " [-0.03553324]\n",
      " [-0.00738054]]\n",
      "t [[-0.12575858]\n",
      " [-0.02978632]\n",
      " [ 0.00833038]\n",
      " ...\n",
      " [-0.026996  ]\n",
      " [-0.03553324]\n",
      " [-0.00738054]]\n",
      "Current iteration=6, loss=36874.3596291601\n",
      "t [[-0.1458736 ]\n",
      " [-0.03469382]\n",
      " [ 0.00983101]\n",
      " ...\n",
      " [-0.03136216]\n",
      " [-0.04123843]\n",
      " [-0.00860903]]\n",
      "t [[-0.1458736 ]\n",
      " [-0.03469382]\n",
      " [ 0.00983101]\n",
      " ...\n",
      " [-0.03136216]\n",
      " [-0.04123843]\n",
      " [-0.00860903]]\n",
      "t [[-0.16575622]\n",
      " [-0.03958337]\n",
      " [ 0.01135944]\n",
      " ...\n",
      " [-0.03569119]\n",
      " [-0.04688364]\n",
      " [-0.00983643]]\n",
      "t [[-0.16575622]\n",
      " [-0.03958337]\n",
      " [ 0.01135944]\n",
      " ...\n",
      " [-0.03569119]\n",
      " [-0.04688364]\n",
      " [-0.00983643]]\n",
      "Current iteration=8, loss=36620.68220824876\n",
      "t [[-0.18540977]\n",
      " [-0.04445441]\n",
      " [ 0.01291416]\n",
      " ...\n",
      " [-0.03998345]\n",
      " [-0.05246964]\n",
      " [-0.01106252]]\n",
      "t [[-0.18540977]\n",
      " [-0.04445441]\n",
      " [ 0.01291416]\n",
      " ...\n",
      " [-0.03998345]\n",
      " [-0.05246964]\n",
      " [-0.01106252]]\n",
      "t [[-0.20483751]\n",
      " [-0.04930639]\n",
      " [ 0.01449373]\n",
      " ...\n",
      " [-0.04423928]\n",
      " [-0.05799716]\n",
      " [-0.01228708]]\n",
      "loss=36378.50976756395\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.02140993]\n",
      " [-0.00465675]\n",
      " [ 0.00152418]\n",
      " ...\n",
      " [ 0.02399658]\n",
      " [-0.01766878]\n",
      " [ 0.01946402]]\n",
      "t [[-0.02140993]\n",
      " [-0.00465675]\n",
      " [ 0.00152418]\n",
      " ...\n",
      " [ 0.02399658]\n",
      " [-0.01766878]\n",
      " [ 0.01946402]]\n",
      "t [[-0.0425704 ]\n",
      " [-0.00930638]\n",
      " [ 0.00308173]\n",
      " ...\n",
      " [ 0.04766802]\n",
      " [-0.03511374]\n",
      " [ 0.03868471]]\n",
      "t [[-0.0425704 ]\n",
      " [-0.00930638]\n",
      " [ 0.00308173]\n",
      " ...\n",
      " [ 0.04766802]\n",
      " [-0.03511374]\n",
      " [ 0.03868471]]\n",
      "Current iteration=2, loss=37428.94773732644\n",
      "t [[-0.06348481]\n",
      " [-0.01394793]\n",
      " [ 0.00467091]\n",
      " ...\n",
      " [ 0.07101863]\n",
      " [-0.0523376 ]\n",
      " [ 0.05766515]]\n",
      "t [[-0.06348481]\n",
      " [-0.01394793]\n",
      " [ 0.00467091]\n",
      " ...\n",
      " [ 0.07101863]\n",
      " [-0.0523376 ]\n",
      " [ 0.05766515]]\n",
      "t [[-0.08415657]\n",
      " [-0.0185805 ]\n",
      " [ 0.00629007]\n",
      " ...\n",
      " [ 0.0940527 ]\n",
      " [-0.06934312]\n",
      " [ 0.07640843]]\n",
      "t [[-0.08415657]\n",
      " [-0.0185805 ]\n",
      " [ 0.00629007]\n",
      " ...\n",
      " [ 0.0940527 ]\n",
      " [-0.06934312]\n",
      " [ 0.07640843]]\n",
      "Current iteration=4, loss=37160.08883277223\n",
      "t [[-0.10458903]\n",
      " [-0.0232032 ]\n",
      " [ 0.00793758]\n",
      " ...\n",
      " [ 0.11677453]\n",
      " [-0.08613304]\n",
      " [ 0.09491761]]\n",
      "t [[-0.10458903]\n",
      " [-0.0232032 ]\n",
      " [ 0.00793758]\n",
      " ...\n",
      " [ 0.11677453]\n",
      " [-0.08613304]\n",
      " [ 0.09491761]]\n",
      "t [[-0.12478556]\n",
      " [-0.02781518]\n",
      " [ 0.00961185]\n",
      " ...\n",
      " [ 0.13918839]\n",
      " [-0.1027101 ]\n",
      " [ 0.11319577]]\n",
      "t [[-0.12478556]\n",
      " [-0.02781518]\n",
      " [ 0.00961185]\n",
      " ...\n",
      " [ 0.13918839]\n",
      " [-0.1027101 ]\n",
      " [ 0.11319577]]\n",
      "Current iteration=6, loss=36903.45792099681\n",
      "t [[-0.14474948]\n",
      " [-0.03241565]\n",
      " [ 0.01131135]\n",
      " ...\n",
      " [ 0.16129854]\n",
      " [-0.11907703]\n",
      " [ 0.13124596]]\n",
      "t [[-0.14474948]\n",
      " [-0.03241565]\n",
      " [ 0.01131135]\n",
      " ...\n",
      " [ 0.16129854]\n",
      " [-0.11907703]\n",
      " [ 0.13124596]]\n",
      "t [[-0.16448409]\n",
      " [-0.03700383]\n",
      " [ 0.0130346 ]\n",
      " ...\n",
      " [ 0.18310919]\n",
      " [-0.13523656]\n",
      " [ 0.14907123]]\n",
      "t [[-0.16448409]\n",
      " [-0.03700383]\n",
      " [ 0.0130346 ]\n",
      " ...\n",
      " [ 0.18310919]\n",
      " [-0.13523656]\n",
      " [ 0.14907123]]\n",
      "Current iteration=8, loss=36658.44450363789\n",
      "t [[-0.18399267]\n",
      " [-0.04157899]\n",
      " [ 0.01478016]\n",
      " ...\n",
      " [ 0.20462456]\n",
      " [-0.15119139]\n",
      " [ 0.1666746 ]]\n",
      "t [[-0.18399267]\n",
      " [-0.04157899]\n",
      " [ 0.01478016]\n",
      " ...\n",
      " [ 0.20462456]\n",
      " [-0.15119139]\n",
      " [ 0.1666746 ]]\n",
      "t [[-0.20327846]\n",
      " [-0.04614042]\n",
      " [ 0.01654661]\n",
      " ...\n",
      " [ 0.22584882]\n",
      " [-0.16694424]\n",
      " [ 0.18405909]]\n",
      "loss=36424.465141294604\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.02749248]\n",
      " [-0.00578354]\n",
      " [ 0.00574019]\n",
      " ...\n",
      " [-0.00725585]\n",
      " [-0.00902139]\n",
      " [-0.0018965 ]]\n",
      "t [[-0.02749248]\n",
      " [-0.00578354]\n",
      " [ 0.00574019]\n",
      " ...\n",
      " [-0.00725585]\n",
      " [-0.00902139]\n",
      " [-0.0018965 ]]\n",
      "t [[-0.05442532]\n",
      " [-0.0113229 ]\n",
      " [ 0.01141144]\n",
      " ...\n",
      " [-0.01441496]\n",
      " [-0.01789956]\n",
      " [-0.00379077]]\n",
      "t [[-0.05442532]\n",
      " [-0.0113229 ]\n",
      " [ 0.01141144]\n",
      " ...\n",
      " [-0.01441496]\n",
      " [-0.01789956]\n",
      " [-0.00379077]]\n",
      "Current iteration=2, loss=37286.01433074578\n",
      "t [[-0.08080811]\n",
      " [-0.01662898]\n",
      " [ 0.01701251]\n",
      " ...\n",
      " [-0.02147863]\n",
      " [-0.02663706]\n",
      " [-0.00568201]]\n",
      "t [[-0.08080811]\n",
      " [-0.01662898]\n",
      " [ 0.01701251]\n",
      " ...\n",
      " [-0.02147863]\n",
      " [-0.02663706]\n",
      " [-0.00568201]]\n",
      "t [[-0.10665048]\n",
      " [-0.0217123 ]\n",
      " [ 0.02254231]\n",
      " ...\n",
      " [-0.0284482 ]\n",
      " [-0.03523645]\n",
      " [-0.00756942]]\n",
      "t [[-0.10665048]\n",
      " [-0.0217123 ]\n",
      " [ 0.02254231]\n",
      " ...\n",
      " [-0.0284482 ]\n",
      " [-0.03523645]\n",
      " [-0.00756942]]\n",
      "Current iteration=4, loss=36890.17058443642\n",
      "t [[-0.13196212]\n",
      " [-0.02658303]\n",
      " [ 0.02799989]\n",
      " ...\n",
      " [-0.035325  ]\n",
      " [-0.04370026]\n",
      " [-0.0094523 ]]\n",
      "t [[-0.13196212]\n",
      " [-0.02658303]\n",
      " [ 0.02799989]\n",
      " ...\n",
      " [-0.035325  ]\n",
      " [-0.04370026]\n",
      " [-0.0094523 ]]\n",
      "t [[-0.15675268]\n",
      " [-0.03125094]\n",
      " [ 0.03338448]\n",
      " ...\n",
      " [-0.04211038]\n",
      " [-0.05203101]\n",
      " [-0.01132997]]\n",
      "t [[-0.15675268]\n",
      " [-0.03125094]\n",
      " [ 0.03338448]\n",
      " ...\n",
      " [-0.04211038]\n",
      " [-0.05203101]\n",
      " [-0.01132997]]\n",
      "Current iteration=6, loss=36521.017585425\n",
      "t [[-0.18103184]\n",
      " [-0.03572545]\n",
      " [ 0.03869543]\n",
      " ...\n",
      " [-0.0488057 ]\n",
      " [-0.0602312 ]\n",
      " [-0.01320178]]\n",
      "t [[-0.18103184]\n",
      " [-0.03572545]\n",
      " [ 0.03869543]\n",
      " ...\n",
      " [-0.0488057 ]\n",
      " [-0.0602312 ]\n",
      " [-0.01320178]]\n",
      "t [[-0.20480924]\n",
      " [-0.04001562]\n",
      " [ 0.04393222]\n",
      " ...\n",
      " [-0.05541232]\n",
      " [-0.06830332]\n",
      " [-0.01506717]]\n",
      "t [[-0.20480924]\n",
      " [-0.04001562]\n",
      " [ 0.04393222]\n",
      " ...\n",
      " [-0.05541232]\n",
      " [-0.06830332]\n",
      " [-0.01506717]]\n",
      "Current iteration=8, loss=36176.569230234585\n",
      "t [[-0.22809447]\n",
      " [-0.04413014]\n",
      " [ 0.04909449]\n",
      " ...\n",
      " [-0.06193158]\n",
      " [-0.07624982]\n",
      " [-0.01692556]]\n",
      "t [[-0.22809447]\n",
      " [-0.04413014]\n",
      " [ 0.04909449]\n",
      " ...\n",
      " [-0.06193158]\n",
      " [-0.07624982]\n",
      " [-0.01692556]]\n",
      "t [[-0.25089707]\n",
      " [-0.04807735]\n",
      " [ 0.05418196]\n",
      " ...\n",
      " [-0.06836484]\n",
      " [-0.08407314]\n",
      " [-0.01877647]]\n",
      "loss=35854.97453883148\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.03228764]\n",
      " [-0.00727399]\n",
      " [ 0.00284132]\n",
      " ...\n",
      " [-0.00697469]\n",
      " [-0.00906829]\n",
      " [-0.00188226]]\n",
      "t [[-0.03228764]\n",
      " [-0.00727399]\n",
      " [ 0.00284132]\n",
      " ...\n",
      " [-0.00697469]\n",
      " [-0.00906829]\n",
      " [-0.00188226]]\n",
      "t [[-0.06400761]\n",
      " [-0.01452396]\n",
      " [ 0.00573756]\n",
      " ...\n",
      " [-0.01385588]\n",
      " [-0.01799215]\n",
      " [-0.00376509]]\n",
      "t [[-0.06400761]\n",
      " [-0.01452396]\n",
      " [ 0.00573756]\n",
      " ...\n",
      " [-0.01385588]\n",
      " [-0.01799215]\n",
      " [-0.00376509]]\n",
      "Current iteration=2, loss=37282.17680314681\n",
      "t [[-0.09517161]\n",
      " [-0.02174698]\n",
      " [ 0.0086835 ]\n",
      " ...\n",
      " [-0.02064489]\n",
      " [-0.02677416]\n",
      " [-0.00564753]]\n",
      "t [[-0.09517161]\n",
      " [-0.02174698]\n",
      " [ 0.0086835 ]\n",
      " ...\n",
      " [-0.02064489]\n",
      " [-0.02677416]\n",
      " [-0.00564753]]\n",
      "t [[-0.12579126]\n",
      " [-0.02894033]\n",
      " [ 0.01167417]\n",
      " ...\n",
      " [-0.0273431 ]\n",
      " [-0.0354169 ]\n",
      " [-0.00752864]]\n",
      "t [[-0.12579126]\n",
      " [-0.02894033]\n",
      " [ 0.01167417]\n",
      " ...\n",
      " [-0.0273431 ]\n",
      " [-0.0354169 ]\n",
      " [-0.00752864]]\n",
      "Current iteration=4, loss=36882.91573398543\n",
      "t [[-0.15587804]\n",
      " [-0.03610149]\n",
      " [ 0.0147048 ]\n",
      " ...\n",
      " [-0.03395187]\n",
      " [-0.04392295]\n",
      " [-0.00940754]]\n",
      "t [[-0.15587804]\n",
      " [-0.03610149]\n",
      " [ 0.0147048 ]\n",
      " ...\n",
      " [-0.03395187]\n",
      " [-0.04392295]\n",
      " [-0.00940754]]\n",
      "t [[-0.18544335]\n",
      " [-0.04322808]\n",
      " [ 0.01777086]\n",
      " ...\n",
      " [-0.04047255]\n",
      " [-0.05229485]\n",
      " [-0.01128342]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.18544335]\n",
      " [-0.04322808]\n",
      " [ 0.01777086]\n",
      " ...\n",
      " [-0.04047255]\n",
      " [-0.05229485]\n",
      " [-0.01128342]]\n",
      "Current iteration=6, loss=36510.716092043\n",
      "t [[-0.21449839]\n",
      " [-0.05031792]\n",
      " [ 0.02086806]\n",
      " ...\n",
      " [-0.04690651]\n",
      " [-0.06053516]\n",
      " [-0.01315551]]\n",
      "t [[-0.21449839]\n",
      " [-0.05031792]\n",
      " [ 0.02086806]\n",
      " ...\n",
      " [-0.04690651]\n",
      " [-0.06053516]\n",
      " [-0.01315551]]\n",
      "t [[-0.24305426]\n",
      " [-0.05736896]\n",
      " [ 0.02399231]\n",
      " ...\n",
      " [-0.05325513]\n",
      " [-0.06864637]\n",
      " [-0.01502308]]\n",
      "t [[-0.24305426]\n",
      " [-0.05736896]\n",
      " [ 0.02399231]\n",
      " ...\n",
      " [-0.05325513]\n",
      " [-0.06864637]\n",
      " [-0.01502308]]\n",
      "Current iteration=8, loss=36163.547489928445\n",
      "t [[-0.27112185]\n",
      " [-0.06437935]\n",
      " [ 0.02713972]\n",
      " ...\n",
      " [-0.05951975]\n",
      " [-0.07663098]\n",
      " [-0.01688546]]\n",
      "t [[-0.27112185]\n",
      " [-0.06437935]\n",
      " [ 0.02713972]\n",
      " ...\n",
      " [-0.05951975]\n",
      " [-0.07663098]\n",
      " [-0.01688546]]\n",
      "t [[-0.2987119 ]\n",
      " [-0.07134737]\n",
      " [ 0.03030662]\n",
      " ...\n",
      " [-0.06570173]\n",
      " [-0.08449145]\n",
      " [-0.01874204]]\n",
      "loss=35839.519513854\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.03237022]\n",
      " [-0.00750235]\n",
      " [ 0.00194974]\n",
      " ...\n",
      " [-0.00689426]\n",
      " [-0.00912156]\n",
      " [-0.00184492]]\n",
      "t [[-0.03237022]\n",
      " [-0.00750235]\n",
      " [ 0.00194974]\n",
      " ...\n",
      " [-0.00689426]\n",
      " [-0.00912156]\n",
      " [-0.00184492]]\n",
      "t [[-0.06417185]\n",
      " [-0.01497343]\n",
      " [ 0.00398441]\n",
      " ...\n",
      " [-0.01370035]\n",
      " [-0.01809787]\n",
      " [-0.00369077]]\n",
      "t [[-0.06417185]\n",
      " [-0.01497343]\n",
      " [ 0.00398441]\n",
      " ...\n",
      " [-0.01370035]\n",
      " [-0.01809787]\n",
      " [-0.00369077]]\n",
      "Current iteration=2, loss=37276.55451853865\n",
      "t [[-0.09541655]\n",
      " [-0.02241068]\n",
      " [ 0.006098  ]\n",
      " ...\n",
      " [-0.02041942]\n",
      " [-0.02693155]\n",
      " [-0.00553663]]\n",
      "t [[-0.09541655]\n",
      " [-0.02241068]\n",
      " [ 0.006098  ]\n",
      " ...\n",
      " [-0.02041942]\n",
      " [-0.02693155]\n",
      " [-0.00553663]]\n",
      "t [[-0.12611589]\n",
      " [-0.02981173]\n",
      " [ 0.00828475]\n",
      " ...\n",
      " [-0.02705262]\n",
      " [-0.03562518]\n",
      " [-0.00738161]]\n",
      "t [[-0.12611589]\n",
      " [-0.02981173]\n",
      " [ 0.00828475]\n",
      " ...\n",
      " [-0.02705262]\n",
      " [-0.03562518]\n",
      " [-0.00738161]]\n",
      "Current iteration=4, loss=36872.06568008838\n",
      "t [[-0.15628131]\n",
      " [-0.03717434]\n",
      " [ 0.01053916]\n",
      " ...\n",
      " [-0.03360115]\n",
      " [-0.04418136]\n",
      " [-0.00922488]]\n",
      "t [[-0.15628131]\n",
      " [-0.03717434]\n",
      " [ 0.01053916]\n",
      " ...\n",
      " [-0.03360115]\n",
      " [-0.04418136]\n",
      " [-0.00922488]]\n",
      "t [[-0.18592417]\n",
      " [-0.04449646]\n",
      " [ 0.01285593]\n",
      " ...\n",
      " [-0.04006618]\n",
      " [-0.05260265]\n",
      " [-0.01106566]]\n",
      "t [[-0.18592417]\n",
      " [-0.04449646]\n",
      " [ 0.01285593]\n",
      " ...\n",
      " [-0.04006618]\n",
      " [-0.05260265]\n",
      " [-0.01106566]]\n",
      "Current iteration=6, loss=36495.005198398496\n",
      "t [[-0.21505566]\n",
      " [-0.05177618]\n",
      " [ 0.01523   ]\n",
      " ...\n",
      " [-0.04644889]\n",
      " [-0.06089161]\n",
      " [-0.01290321]]\n",
      "t [[-0.21505566]\n",
      " [-0.05177618]\n",
      " [ 0.01523   ]\n",
      " ...\n",
      " [-0.04644889]\n",
      " [-0.06089161]\n",
      " [-0.01290321]]\n",
      "t [[-0.24368685]\n",
      " [-0.05901175]\n",
      " [ 0.01765654]\n",
      " ...\n",
      " [-0.05275049]\n",
      " [-0.06905077]\n",
      " [-0.01473685]]\n",
      "t [[-0.24368685]\n",
      " [-0.05901175]\n",
      " [ 0.01765654]\n",
      " ...\n",
      " [-0.05275049]\n",
      " [-0.06905077]\n",
      " [-0.01473685]]\n",
      "Current iteration=8, loss=36143.316143554024\n",
      "t [[-0.27182864]\n",
      " [-0.06620156]\n",
      " [ 0.02013095]\n",
      " ...\n",
      " [-0.05897216]\n",
      " [-0.07708264]\n",
      " [-0.01656593]]\n",
      "t [[-0.27182864]\n",
      " [-0.06620156]\n",
      " [ 0.02013095]\n",
      " ...\n",
      " [-0.05897216]\n",
      " [-0.07708264]\n",
      " [-0.01656593]]\n",
      "t [[-0.29949174]\n",
      " [-0.07334412]\n",
      " [ 0.02264884]\n",
      " ...\n",
      " [-0.06511509]\n",
      " [-0.08498969]\n",
      " [-0.01838985]]\n",
      "loss=35815.08283817638\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.0321149 ]\n",
      " [-0.00698512]\n",
      " [ 0.00228627]\n",
      " ...\n",
      " [ 0.03599487]\n",
      " [-0.02650318]\n",
      " [ 0.02919603]]\n",
      "t [[-0.0321149 ]\n",
      " [-0.00698512]\n",
      " [ 0.00228627]\n",
      " ...\n",
      " [ 0.03599487]\n",
      " [-0.02650318]\n",
      " [ 0.02919603]]\n",
      "t [[-0.06366852]\n",
      " [-0.01395422]\n",
      " [ 0.00464761]\n",
      " ...\n",
      " [ 0.0712582 ]\n",
      " [-0.05250275]\n",
      " [ 0.05784458]]\n",
      "t [[-0.06366852]\n",
      " [-0.01395422]\n",
      " [ 0.00464761]\n",
      " ...\n",
      " [ 0.0712582 ]\n",
      " [-0.05250275]\n",
      " [ 0.05784458]]\n",
      "Current iteration=2, loss=37291.764549698215\n",
      "t [[-0.09467245]\n",
      " [-0.02090408]\n",
      " [ 0.00707819]\n",
      " ...\n",
      " [ 0.1058046 ]\n",
      " [-0.07800803]\n",
      " [ 0.08595613]]\n",
      "t [[-0.09467245]\n",
      " [-0.02090408]\n",
      " [ 0.00707819]\n",
      " ...\n",
      " [ 0.1058046 ]\n",
      " [-0.07800803]\n",
      " [ 0.08595613]]\n",
      "t [[-0.12513814]\n",
      " [-0.02783166]\n",
      " [ 0.00957245]\n",
      " ...\n",
      " [ 0.13964866]\n",
      " [-0.10302832]\n",
      " [ 0.11354111]]\n",
      "t [[-0.12513814]\n",
      " [-0.02783166]\n",
      " [ 0.00957245]\n",
      " ...\n",
      " [ 0.13964866]\n",
      " [-0.10302832]\n",
      " [ 0.11354111]]\n",
      "Current iteration=4, loss=36901.25851355992\n",
      "t [[-0.15507698]\n",
      " [-0.03473411]\n",
      " [ 0.01212505]\n",
      " ...\n",
      " [ 0.17280484]\n",
      " [-0.12757291]\n",
      " [ 0.14060993]]\n",
      "t [[-0.15507698]\n",
      " [-0.03473411]\n",
      " [ 0.01212505]\n",
      " ...\n",
      " [ 0.17280484]\n",
      " [-0.12757291]\n",
      " [ 0.14060993]]\n",
      "t [[-0.18450018]\n",
      " [-0.04160878]\n",
      " [ 0.01473089]\n",
      " ...\n",
      " [ 0.20528753]\n",
      " [-0.15165105]\n",
      " [ 0.16717292]]\n",
      "t [[-0.18450018]\n",
      " [-0.04160878]\n",
      " [ 0.01473089]\n",
      " ...\n",
      " [ 0.20528753]\n",
      " [-0.15165105]\n",
      " [ 0.16717292]]\n",
      "Current iteration=6, loss=36537.051831080884\n",
      "t [[-0.21341883]\n",
      " [-0.0484532 ]\n",
      " [ 0.01738511]\n",
      " ...\n",
      " [ 0.23711095]\n",
      " [-0.17527194]\n",
      " [ 0.19324034]]\n",
      "t [[-0.21341883]\n",
      " [-0.0484532 ]\n",
      " [ 0.01738511]\n",
      " ...\n",
      " [ 0.23711095]\n",
      " [-0.17527194]\n",
      " [ 0.19324034]]\n",
      "t [[-0.24184388]\n",
      " [-0.05526507]\n",
      " [ 0.02008307]\n",
      " ...\n",
      " [ 0.2682892 ]\n",
      " [-0.19844468]\n",
      " [ 0.21882236]]\n",
      "t [[-0.24184388]\n",
      " [-0.05526507]\n",
      " [ 0.02008307]\n",
      " ...\n",
      " [ 0.2682892 ]\n",
      " [-0.19844468]\n",
      " [ 0.21882236]]\n",
      "Current iteration=8, loss=36197.180346803034\n",
      "t [[-0.26978609]\n",
      " [-0.06204225]\n",
      " [ 0.02282035]\n",
      " ...\n",
      " [ 0.2988362 ]\n",
      " [-0.22117834]\n",
      " [ 0.24392905]]\n",
      "t [[-0.26978609]\n",
      " [-0.06204225]\n",
      " [ 0.02282035]\n",
      " ...\n",
      " [ 0.2988362 ]\n",
      " [-0.22117834]\n",
      " [ 0.24392905]]\n",
      "t [[-0.29725606]\n",
      " [-0.06878278]\n",
      " [ 0.02559276]\n",
      " ...\n",
      " [ 0.32876568]\n",
      " [-0.24348185]\n",
      " [ 0.26857034]]\n",
      "loss=35879.81531646475\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.03665664]\n",
      " [-0.00771139]\n",
      " [ 0.00765358]\n",
      " ...\n",
      " [-0.00967447]\n",
      " [-0.01202852]\n",
      " [-0.00252866]]\n",
      "t [[-0.03665664]\n",
      " [-0.00771139]\n",
      " [ 0.00765358]\n",
      " ...\n",
      " [-0.00967447]\n",
      " [-0.01202852]\n",
      " [-0.00252866]]\n",
      "t [[-0.0723184 ]\n",
      " [-0.0149887 ]\n",
      " [ 0.01518462]\n",
      " ...\n",
      " [-0.01917695]\n",
      " [-0.02380243]\n",
      " [-0.00505338]]\n",
      "t [[-0.0723184 ]\n",
      " [-0.0149887 ]\n",
      " [ 0.01518462]\n",
      " ...\n",
      " [-0.01917695]\n",
      " [-0.02380243]\n",
      " [-0.00505338]]\n",
      "Current iteration=2, loss=37149.3951758371\n",
      "t [[-0.10700814]\n",
      " [-0.02185782]\n",
      " [ 0.02259015]\n",
      " ...\n",
      " [-0.02851059]\n",
      " [-0.03532784]\n",
      " [-0.0075722 ]]\n",
      "t [[-0.10700814]\n",
      " [-0.02185782]\n",
      " [ 0.02259015]\n",
      " ...\n",
      " [-0.02851059]\n",
      " [-0.03532784]\n",
      " [-0.0075722 ]]\n",
      "t [[-0.14074885]\n",
      " [-0.02834346]\n",
      " [ 0.02986773]\n",
      " ...\n",
      " [-0.03767857]\n",
      " [-0.04661079]\n",
      " [-0.01008333]]\n",
      "t [[-0.14074885]\n",
      " [-0.02834346]\n",
      " [ 0.02986773]\n",
      " ...\n",
      " [-0.03767857]\n",
      " [-0.04661079]\n",
      " [-0.01008333]]\n",
      "Current iteration=4, loss=36638.37560727178\n",
      "t [[-0.17356355]\n",
      " [-0.0344691 ]\n",
      " [ 0.03701539]\n",
      " ...\n",
      " [-0.0466841 ]\n",
      " [-0.05765731]\n",
      " [-0.01258515]]\n",
      "t [[-0.17356355]\n",
      " [-0.0344691 ]\n",
      " [ 0.03701539]\n",
      " ...\n",
      " [-0.0466841 ]\n",
      " [-0.05765731]\n",
      " [-0.01258515]]\n",
      "t [[-0.20547525]\n",
      " [-0.04025708]\n",
      " [ 0.04403163]\n",
      " ...\n",
      " [-0.0555304 ]\n",
      " [-0.06847334]\n",
      " [-0.01507616]]\n",
      "t [[-0.20547525]\n",
      " [-0.04025708]\n",
      " [ 0.04403163]\n",
      " ...\n",
      " [-0.0555304 ]\n",
      " [-0.06847334]\n",
      " [-0.01507616]]\n",
      "Current iteration=6, loss=36172.69926565052\n",
      "t [[-0.23650682]\n",
      " [-0.04572854]\n",
      " [ 0.05091537]\n",
      " ...\n",
      " [-0.06422071]\n",
      " [-0.07906477]\n",
      " [-0.017555  ]]\n",
      "t [[-0.23650682]\n",
      " [-0.04572854]\n",
      " [ 0.05091537]\n",
      " ...\n",
      " [-0.06422071]\n",
      " [-0.07906477]\n",
      " [-0.017555  ]]\n",
      "t [[-0.26668099]\n",
      " [-0.05090351]\n",
      " [ 0.05766593]\n",
      " ...\n",
      " [-0.07275825]\n",
      " [-0.08943739]\n",
      " [-0.02002045]]\n",
      "t [[-0.26668099]\n",
      " [-0.05090351]\n",
      " [ 0.05766593]\n",
      " ...\n",
      " [-0.07275825]\n",
      " [-0.08943739]\n",
      " [-0.02002045]]\n",
      "Current iteration=8, loss=35747.88572954113\n",
      "t [[-0.29602024]\n",
      " [-0.05580094]\n",
      " [ 0.06428297]\n",
      " ...\n",
      " [-0.08114621]\n",
      " [-0.09959689]\n",
      " [-0.0224714 ]]\n",
      "t [[-0.29602024]\n",
      " [-0.05580094]\n",
      " [ 0.06428297]\n",
      " ...\n",
      " [-0.08114621]\n",
      " [-0.09959689]\n",
      " [-0.0224714 ]]\n",
      "t [[-0.3245468 ]\n",
      " [-0.06043868]\n",
      " [ 0.0707665 ]\n",
      " ...\n",
      " [-0.08938778]\n",
      " [-0.10954887]\n",
      " [-0.02490685]]\n",
      "loss=35359.873012145326\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.04305018]\n",
      " [-0.00969866]\n",
      " [ 0.00378843]\n",
      " ...\n",
      " [-0.00929959]\n",
      " [-0.01209106]\n",
      " [-0.00250967]]\n",
      "t [[-0.04305018]\n",
      " [-0.00969866]\n",
      " [ 0.00378843]\n",
      " ...\n",
      " [-0.00929959]\n",
      " [-0.01209106]\n",
      " [-0.00250967]]\n",
      "t [[-0.08509123]\n",
      " [-0.01935459]\n",
      " [ 0.00767448]\n",
      " ...\n",
      " [-0.01843295]\n",
      " [-0.02392535]\n",
      " [-0.00502039]]\n",
      "t [[-0.08509123]\n",
      " [-0.01935459]\n",
      " [ 0.00767448]\n",
      " ...\n",
      " [-0.01843295]\n",
      " [-0.02392535]\n",
      " [-0.00502039]]\n",
      "Current iteration=2, loss=37144.351631861195\n",
      "t [[-0.12615103]\n",
      " [-0.02896089]\n",
      " [ 0.01164578]\n",
      " ...\n",
      " [-0.02740329]\n",
      " [-0.03550905]\n",
      " [-0.0075298 ]]\n",
      "t [[-0.12615103]\n",
      " [-0.02896089]\n",
      " [ 0.01164578]\n",
      " ...\n",
      " [-0.02740329]\n",
      " [-0.03550905]\n",
      " [-0.0075298 ]]\n",
      "t [[-0.16625714]\n",
      " [-0.03851125]\n",
      " [ 0.01569069]\n",
      " ...\n",
      " [-0.03621386]\n",
      " [-0.04684829]\n",
      " [-0.01003576]]\n",
      "t [[-0.16625714]\n",
      " [-0.03851125]\n",
      " [ 0.01569069]\n",
      " ...\n",
      " [-0.03621386]\n",
      " [-0.04684829]\n",
      " [-0.01003576]]\n",
      "Current iteration=4, loss=36629.012690212636\n",
      "t [[-0.20543673]\n",
      " [-0.04799994]\n",
      " [ 0.01979829]\n",
      " ...\n",
      " [-0.04486791]\n",
      " [-0.05794918]\n",
      " [-0.0125363 ]]\n",
      "t [[-0.20543673]\n",
      " [-0.04799994]\n",
      " [ 0.01979829]\n",
      " ...\n",
      " [-0.04486791]\n",
      " [-0.05794918]\n",
      " [-0.0125363 ]]\n",
      "t [[-0.24371651]\n",
      " [-0.05742178]\n",
      " [ 0.0239584 ]\n",
      " ...\n",
      " [-0.0533687 ]\n",
      " [-0.06881773]\n",
      " [-0.01502958]]\n",
      "t [[-0.24371651]\n",
      " [-0.05742178]\n",
      " [ 0.0239584 ]\n",
      " ...\n",
      " [-0.0533687 ]\n",
      " [-0.06881773]\n",
      " [-0.01502958]]\n",
      "Current iteration=6, loss=36159.62844656922\n",
      "t [[-0.28112265]\n",
      " [-0.06677211]\n",
      " [ 0.02816152]\n",
      " ...\n",
      " [-0.06171946]\n",
      " [-0.07945992]\n",
      " [-0.01751395]]\n",
      "t [[-0.28112265]\n",
      " [-0.06677211]\n",
      " [ 0.02816152]\n",
      " ...\n",
      " [-0.06171946]\n",
      " [-0.07945992]\n",
      " [-0.01751395]]\n",
      "t [[-0.31768079]\n",
      " [-0.07604677]\n",
      " [ 0.0323988 ]\n",
      " ...\n",
      " [-0.06992342]\n",
      " [-0.08988161]\n",
      " [-0.0199879 ]]\n",
      "t [[-0.31768079]\n",
      " [-0.07604677]\n",
      " [ 0.0323988 ]\n",
      " ...\n",
      " [-0.06992342]\n",
      " [-0.08988161]\n",
      " [-0.0199879 ]]\n",
      "Current iteration=8, loss=35731.62180630765\n",
      "t [[-0.35341595]\n",
      " [-0.08524206]\n",
      " [ 0.03666205]\n",
      " ...\n",
      " [-0.07798378]\n",
      " [-0.10008856]\n",
      " [-0.02245005]]\n",
      "t [[-0.35341595]\n",
      " [-0.08524206]\n",
      " [ 0.03666205]\n",
      " ...\n",
      " [-0.07798378]\n",
      " [-0.10008856]\n",
      " [-0.02245005]]\n",
      "t [[-0.38835256]\n",
      " [-0.09435472]\n",
      " [ 0.04094367]\n",
      " ...\n",
      " [-0.0859037 ]\n",
      " [-0.11008645]\n",
      " [-0.02489916]]\n",
      "loss=35340.848385521094\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.04316029]\n",
      " [-0.01000314]\n",
      " [ 0.00259966]\n",
      " ...\n",
      " [-0.00919235]\n",
      " [-0.01216208]\n",
      " [-0.00245989]]\n",
      "t [[-0.04316029]\n",
      " [-0.01000314]\n",
      " [ 0.00259966]\n",
      " ...\n",
      " [-0.00919235]\n",
      " [-0.01216208]\n",
      " [-0.00245989]]\n",
      "t [[-0.08530982]\n",
      " [-0.01995067]\n",
      " [ 0.00535027]\n",
      " ...\n",
      " [-0.01822795]\n",
      " [-0.02406596]\n",
      " [-0.00492145]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.08530982]\n",
      " [-0.01995067]\n",
      " [ 0.00535027]\n",
      " ...\n",
      " [-0.01822795]\n",
      " [-0.02406596]\n",
      " [-0.00492145]]\n",
      "Current iteration=2, loss=37136.92266859078\n",
      "t [[-0.12647635]\n",
      " [-0.02983655]\n",
      " [ 0.00823761]\n",
      " ...\n",
      " [-0.02710957]\n",
      " [-0.03571784]\n",
      " [-0.00738246]]\n",
      "t [[-0.12647635]\n",
      " [-0.02983655]\n",
      " [ 0.00823761]\n",
      " ...\n",
      " [-0.02710957]\n",
      " [-0.03571784]\n",
      " [-0.00738246]]\n",
      "t [[-0.16668735]\n",
      " [-0.03965524]\n",
      " [ 0.0112482 ]\n",
      " ...\n",
      " [-0.03583998]\n",
      " [-0.04712392]\n",
      " [-0.00984087]]\n",
      "t [[-0.16668735]\n",
      " [-0.03965524]\n",
      " [ 0.0112482 ]\n",
      " ...\n",
      " [-0.03583998]\n",
      " [-0.04712392]\n",
      " [-0.00984087]]\n",
      "Current iteration=4, loss=36614.84359445392\n",
      "t [[-0.20596992]\n",
      " [-0.04940174]\n",
      " [ 0.01436931]\n",
      " ...\n",
      " [-0.04442201]\n",
      " [-0.05829031]\n",
      " [-0.01229481]]\n",
      "t [[-0.20596992]\n",
      " [-0.04940174]\n",
      " [ 0.01436931]\n",
      " ...\n",
      " [-0.04442201]\n",
      " [-0.05829031]\n",
      " [-0.01229481]]\n",
      "t [[-0.24435068]\n",
      " [-0.05907156]\n",
      " [ 0.01758897]\n",
      " ...\n",
      " [-0.05285849]\n",
      " [-0.06922309]\n",
      " [-0.01474252]]\n",
      "t [[-0.24435068]\n",
      " [-0.05907156]\n",
      " [ 0.01758897]\n",
      " ...\n",
      " [-0.05285849]\n",
      " [-0.06922309]\n",
      " [-0.01474252]]\n",
      "Current iteration=6, loss=36139.34344221302\n",
      "t [[-0.2818558 ]\n",
      " [-0.06866067]\n",
      " [ 0.02089592]\n",
      " ...\n",
      " [-0.06115226]\n",
      " [-0.07992827]\n",
      " [-0.01718244]]\n",
      "t [[-0.2818558 ]\n",
      " [-0.06866067]\n",
      " [ 0.02089592]\n",
      " ...\n",
      " [-0.06115226]\n",
      " [-0.07992827]\n",
      " [-0.01718244]]\n",
      "t [[-0.31851089]\n",
      " [-0.07816549]\n",
      " [ 0.02427961]\n",
      " ...\n",
      " [-0.06930616]\n",
      " [-0.09041174]\n",
      " [-0.01961309]]\n",
      "t [[-0.31851089]\n",
      " [-0.07816549]\n",
      " [ 0.02427961]\n",
      " ...\n",
      " [-0.06930616]\n",
      " [-0.09041174]\n",
      " [-0.01961309]]\n",
      "Current iteration=8, loss=35705.78427842395\n",
      "t [[-0.35434096]\n",
      " [-0.08758287]\n",
      " [ 0.02773017]\n",
      " ...\n",
      " [-0.07732301]\n",
      " [-0.10067931]\n",
      " [-0.02203317]]\n",
      "t [[-0.35434096]\n",
      " [-0.08758287]\n",
      " [ 0.02773017]\n",
      " ...\n",
      " [-0.07732301]\n",
      " [-0.10067931]\n",
      " [-0.02203317]]\n",
      "t [[-0.38937043]\n",
      " [-0.09691008]\n",
      " [ 0.03123836]\n",
      " ...\n",
      " [-0.08520562]\n",
      " [-0.1107367 ]\n",
      " [-0.02444146]]\n",
      "loss=35309.96538182514\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.04281986]\n",
      " [-0.00931349]\n",
      " [ 0.00304836]\n",
      " ...\n",
      " [ 0.04799317]\n",
      " [-0.03533757]\n",
      " [ 0.03892804]]\n",
      "t [[-0.04281986]\n",
      " [-0.00931349]\n",
      " [ 0.00304836]\n",
      " ...\n",
      " [ 0.04799317]\n",
      " [-0.03533757]\n",
      " [ 0.03892804]]\n",
      "t [[-0.08464196]\n",
      " [-0.01859851]\n",
      " [ 0.00623016]\n",
      " ...\n",
      " [ 0.09468586]\n",
      " [-0.06977988]\n",
      " [ 0.07688284]]\n",
      "t [[-0.08464196]\n",
      " [-0.01859851]\n",
      " [ 0.00623016]\n",
      " ...\n",
      " [ 0.09468586]\n",
      " [-0.06977988]\n",
      " [ 0.07688284]]\n",
      "Current iteration=2, loss=37156.992166043376\n",
      "t [[-0.12549386]\n",
      " [-0.02784737]\n",
      " [ 0.00953158]\n",
      " ...\n",
      " [ 0.14011294]\n",
      " [-0.10334912]\n",
      " [ 0.11388934]]\n",
      "t [[-0.12549386]\n",
      " [-0.02784737]\n",
      " [ 0.00953158]\n",
      " ...\n",
      " [ 0.14011294]\n",
      " [-0.10334912]\n",
      " [ 0.11388934]]\n",
      "t [[-0.16540282]\n",
      " [-0.03705304]\n",
      " [ 0.0129396 ]\n",
      " ...\n",
      " [ 0.18430905]\n",
      " [-0.13606746]\n",
      " [ 0.1499724 ]]\n",
      "t [[-0.16540282]\n",
      " [-0.03705304]\n",
      " [ 0.0129396 ]\n",
      " ...\n",
      " [ 0.18430905]\n",
      " [-0.13606746]\n",
      " [ 0.1499724 ]]\n",
      "Current iteration=4, loss=36652.84559173399\n",
      "t [[-0.20439565]\n",
      " [-0.04620909]\n",
      " [ 0.01644194]\n",
      " ...\n",
      " [ 0.22730846]\n",
      " [-0.16795692]\n",
      " [ 0.18515667]]\n",
      "t [[-0.20439565]\n",
      " [-0.04620909]\n",
      " [ 0.01644194]\n",
      " ...\n",
      " [ 0.22730846]\n",
      " [-0.16795692]\n",
      " [ 0.18515667]]\n",
      "t [[-0.24249873]\n",
      " [-0.05530966]\n",
      " [ 0.02002707]\n",
      " ...\n",
      " [ 0.26914504]\n",
      " [-0.19903938]\n",
      " [ 0.21946656]]\n",
      "t [[-0.24249873]\n",
      " [-0.05530966]\n",
      " [ 0.02002707]\n",
      " ...\n",
      " [ 0.26914504]\n",
      " [-0.19903938]\n",
      " [ 0.21946656]]\n",
      "Current iteration=6, loss=36193.36924940837\n",
      "t [[-0.27973792]\n",
      " [-0.06434945]\n",
      " [ 0.0236842 ]\n",
      " ...\n",
      " [ 0.30985213]\n",
      " [-0.22933647]\n",
      " [ 0.25292615]]\n",
      "t [[-0.27973792]\n",
      " [-0.06434945]\n",
      " [ 0.0236842 ]\n",
      " ...\n",
      " [ 0.30985213]\n",
      " [-0.22933647]\n",
      " [ 0.25292615]]\n",
      "t [[-0.3161385 ]\n",
      " [-0.07332372]\n",
      " [ 0.02740323]\n",
      " ...\n",
      " [ 0.3494625 ]\n",
      " [-0.25886952]\n",
      " [ 0.28555916]]\n",
      "t [[-0.3161385 ]\n",
      " [-0.07332372]\n",
      " [ 0.02740323]\n",
      " ...\n",
      " [ 0.3494625 ]\n",
      " [-0.25886952]\n",
      " [ 0.28555916]]\n",
      "Current iteration=8, loss=35774.135403034066\n",
      "t [[-0.35172519]\n",
      " [-0.08222819]\n",
      " [ 0.03117472]\n",
      " ...\n",
      " [ 0.38800828]\n",
      " [-0.28765955]\n",
      " [ 0.31738887]]\n",
      "t [[-0.35172519]\n",
      " [-0.08222819]\n",
      " [ 0.03117472]\n",
      " ...\n",
      " [ 0.38800828]\n",
      " [-0.28765955]\n",
      " [ 0.31738887]]\n",
      "t [[-0.38652206]\n",
      " [-0.09105907]\n",
      " [ 0.03498992]\n",
      " ...\n",
      " [ 0.42552093]\n",
      " [-0.3157272 ]\n",
      " [ 0.34843813]]\n",
      "loss=35391.13445360711\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.04582079]\n",
      " [-0.00963924]\n",
      " [ 0.00956698]\n",
      " ...\n",
      " [-0.01209309]\n",
      " [-0.01503564]\n",
      " [-0.00316083]]\n",
      "t [[-0.04582079]\n",
      " [-0.00963924]\n",
      " [ 0.00956698]\n",
      " ...\n",
      " [-0.01209309]\n",
      " [-0.01503564]\n",
      " [-0.00316083]]\n",
      "t [[-0.09008718]\n",
      " [-0.01860026]\n",
      " [ 0.01894249]\n",
      " ...\n",
      " [-0.02391747]\n",
      " [-0.0296735 ]\n",
      " [-0.00631549]]\n",
      "t [[-0.09008718]\n",
      " [-0.01860026]\n",
      " [ 0.01894249]\n",
      " ...\n",
      " [-0.02391747]\n",
      " [-0.0296735 ]\n",
      " [-0.00631549]]\n",
      "Current iteration=2, loss=37015.18978167168\n",
      "t [[-0.13284403]\n",
      " [-0.02693376]\n",
      " [ 0.02812075]\n",
      " ...\n",
      " [-0.03547932]\n",
      " [-0.04392552]\n",
      " [-0.00946018]]\n",
      "t [[-0.13284403]\n",
      " [-0.02693376]\n",
      " [ 0.02812075]\n",
      " ...\n",
      " [-0.03547932]\n",
      " [-0.04392552]\n",
      " [-0.00946018]]\n",
      "t [[-0.17413647]\n",
      " [-0.03468745]\n",
      " [ 0.03709725]\n",
      " ...\n",
      " [-0.04678493]\n",
      " [-0.05780358]\n",
      " [-0.0125915 ]]\n",
      "t [[-0.17413647]\n",
      " [-0.03468745]\n",
      " [ 0.03709725]\n",
      " ...\n",
      " [-0.04678493]\n",
      " [-0.05780358]\n",
      " [-0.0125915 ]]\n",
      "Current iteration=4, loss=36396.74243850645\n",
      "t [[-0.21400964]\n",
      " [-0.04190606]\n",
      " [ 0.04586867]\n",
      " ...\n",
      " [-0.05784062]\n",
      " [-0.07131939]\n",
      " [-0.0157064 ]]\n",
      "t [[-0.21400964]\n",
      " [-0.04190606]\n",
      " [ 0.04586867]\n",
      " ...\n",
      " [-0.05784062]\n",
      " [-0.07131939]\n",
      " [-0.0157064 ]]\n",
      "t [[-0.25250844]\n",
      " [-0.04863144]\n",
      " [ 0.05443274]\n",
      " ...\n",
      " [-0.06865273]\n",
      " [-0.08448453]\n",
      " [-0.01880219]]\n",
      "t [[-0.25250844]\n",
      " [-0.04863144]\n",
      " [ 0.05443274]\n",
      " ...\n",
      " [-0.06865273]\n",
      " [-0.08448453]\n",
      " [-0.01880219]]\n",
      "Current iteration=6, loss=35845.967497600584\n",
      "t [[-0.28967737]\n",
      " [-0.05490264]\n",
      " [ 0.06278818]\n",
      " ...\n",
      " [-0.07922758]\n",
      " [-0.09731033]\n",
      " [-0.02187648]]\n",
      "t [[-0.28967737]\n",
      " [-0.05490264]\n",
      " [ 0.06278818]\n",
      " ...\n",
      " [-0.07922758]\n",
      " [-0.09731033]\n",
      " [-0.02187648]]\n",
      "t [[-0.32556029]\n",
      " [-0.06075603]\n",
      " [ 0.07093459]\n",
      " ...\n",
      " [-0.08957142]\n",
      " [-0.10980788]\n",
      " [-0.02492718]]\n",
      "t [[-0.32556029]\n",
      " [-0.06075603]\n",
      " [ 0.07093459]\n",
      " ...\n",
      " [-0.08957142]\n",
      " [-0.10980788]\n",
      " [-0.02492718]]\n",
      "Current iteration=8, loss=35354.554729358155\n",
      "t [[-0.36020036]\n",
      " [-0.06622539]\n",
      " [ 0.07887231]\n",
      " ...\n",
      " [-0.09969044]\n",
      " [-0.121988  ]\n",
      " [-0.02795247]]\n",
      "t [[-0.36020036]\n",
      " [-0.06622539]\n",
      " [ 0.07887231]\n",
      " ...\n",
      " [-0.09969044]\n",
      " [-0.121988  ]\n",
      " [-0.02795247]]\n",
      "t [[-0.39363985]\n",
      " [-0.07134208]\n",
      " [ 0.0866024 ]\n",
      " ...\n",
      " [-0.10959073]\n",
      " [-0.13386122]\n",
      " [-0.03095078]]\n",
      "loss=34915.18479809637\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.05381273]\n",
      " [-0.01212332]\n",
      " [ 0.00473553]\n",
      " ...\n",
      " [-0.01162449]\n",
      " [-0.01511382]\n",
      " [-0.00313709]]\n",
      "t [[-0.05381273]\n",
      " [-0.01212332]\n",
      " [ 0.00473553]\n",
      " ...\n",
      " [-0.01162449]\n",
      " [-0.01511382]\n",
      " [-0.00313709]]\n",
      "t [[-0.10604878]\n",
      " [-0.02417987]\n",
      " [ 0.0096236 ]\n",
      " ...\n",
      " [-0.02298926]\n",
      " [-0.02982647]\n",
      " [-0.00627581]]\n",
      "t [[-0.10604878]\n",
      " [-0.02417987]\n",
      " [ 0.0096236 ]\n",
      " ...\n",
      " [-0.02298926]\n",
      " [-0.02982647]\n",
      " [-0.00627581]]\n",
      "Current iteration=2, loss=37008.97551679736\n",
      "t [[-0.15676291]\n",
      " [-0.03615615]\n",
      " [ 0.01463997]\n",
      " ...\n",
      " [-0.03410064]\n",
      " [-0.04415006]\n",
      " [-0.00941158]]\n",
      "t [[-0.15676291]\n",
      " [-0.03615615]\n",
      " [ 0.01463997]\n",
      " ...\n",
      " [-0.03410064]\n",
      " [-0.04415006]\n",
      " [-0.00941158]]\n",
      "t [[-0.20600902]\n",
      " [-0.04804014]\n",
      " [ 0.01976227]\n",
      " ...\n",
      " [-0.04496501]\n",
      " [-0.05809662]\n",
      " [-0.01254031]]\n",
      "t [[-0.20600902]\n",
      " [-0.04804014]\n",
      " [ 0.01976227]\n",
      " ...\n",
      " [-0.04496501]\n",
      " [-0.05809662]\n",
      " [-0.01254031]]\n",
      "Current iteration=4, loss=36385.41055814532\n",
      "t [[-0.25383987]\n",
      " [-0.0598212 ]\n",
      " [ 0.02496992]\n",
      " ...\n",
      " [-0.05558877]\n",
      " [-0.07167804]\n",
      " [-0.01565829]]\n",
      "t [[-0.25383987]\n",
      " [-0.0598212 ]\n",
      " [ 0.02496992]\n",
      " ...\n",
      " [-0.05558877]\n",
      " [-0.07167804]\n",
      " [-0.01565829]]\n",
      "t [[-0.30030701]\n",
      " [-0.07149004]\n",
      " [ 0.03024408]\n",
      " ...\n",
      " [-0.06597828]\n",
      " [-0.08490603]\n",
      " [-0.01876222]]\n",
      "t [[-0.30030701]\n",
      " [-0.07149004]\n",
      " [ 0.03024408]\n",
      " ...\n",
      " [-0.06597828]\n",
      " [-0.08490603]\n",
      " [-0.01876222]]\n",
      "Current iteration=6, loss=35830.403566034656\n",
      "t [[-0.34546056]\n",
      " [-0.08303857]\n",
      " [ 0.03556752]\n",
      " ...\n",
      " [-0.07613987]\n",
      " [-0.09779206]\n",
      " [-0.02184917]]\n",
      "t [[-0.34546056]\n",
      " [-0.08303857]\n",
      " [ 0.03556752]\n",
      " ...\n",
      " [-0.07613987]\n",
      " [-0.09779206]\n",
      " [-0.02184917]]\n",
      "t [[-0.38934916]\n",
      " [-0.09445987]\n",
      " [ 0.0409246 ]\n",
      " ...\n",
      " [-0.08607976]\n",
      " [-0.11034738]\n",
      " [-0.02491653]]\n",
      "t [[-0.38934916]\n",
      " [-0.09445987]\n",
      " [ 0.0409246 ]\n",
      " ...\n",
      " [-0.08607976]\n",
      " [-0.11034738]\n",
      " [-0.02491653]]\n",
      "Current iteration=8, loss=35335.47089337609\n",
      "t [[-0.43201989]\n",
      " [-0.10574806]\n",
      " [ 0.04630116]\n",
      " ...\n",
      " [-0.0958041 ]\n",
      " [-0.12258294]\n",
      " [-0.02796203]]\n",
      "t [[-0.43201989]\n",
      " [-0.10574806]\n",
      " [ 0.04630116]\n",
      " ...\n",
      " [-0.0958041 ]\n",
      " [-0.12258294]\n",
      " [-0.02796203]]\n",
      "t [[-0.4735182 ]\n",
      " [-0.11689825]\n",
      " [ 0.05168439]\n",
      " ...\n",
      " [-0.10531892]\n",
      " [-0.13450942]\n",
      " [-0.03098366]]\n",
      "loss=34893.15195733911\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.05395036]\n",
      " [-0.01250392]\n",
      " [ 0.00324957]\n",
      " ...\n",
      " [-0.01149043]\n",
      " [-0.0152026 ]\n",
      " [-0.00307486]]\n",
      "t [[-0.05395036]\n",
      " [-0.01250392]\n",
      " [ 0.00324957]\n",
      " ...\n",
      " [-0.01149043]\n",
      " [-0.0152026 ]\n",
      " [-0.00307486]]\n",
      "t [[-0.10632151]\n",
      " [-0.02492096]\n",
      " [ 0.006735  ]\n",
      " ...\n",
      " [-0.02273597]\n",
      " [-0.03000178]\n",
      " [-0.00615233]]\n",
      "t [[-0.10632151]\n",
      " [-0.02492096]\n",
      " [ 0.006735  ]\n",
      " ...\n",
      " [-0.02273597]\n",
      " [-0.03000178]\n",
      " [-0.00615233]]\n",
      "Current iteration=2, loss=36999.77290099797\n",
      "t [[-0.15716797]\n",
      " [-0.03723925]\n",
      " [ 0.01042841]\n",
      " ...\n",
      " [-0.03374204]\n",
      " [-0.04440974]\n",
      " [-0.00922808]]\n",
      "t [[-0.15716797]\n",
      " [-0.03723925]\n",
      " [ 0.01042841]\n",
      " ...\n",
      " [-0.03374204]\n",
      " [-0.04440974]\n",
      " [-0.00922808]]\n",
      "t [[-0.20654345]\n",
      " [-0.04944829]\n",
      " [ 0.01430386]\n",
      " ...\n",
      " [-0.04451414]\n",
      " [-0.05843858]\n",
      " [-0.0122982 ]]\n",
      "t [[-0.20654345]\n",
      " [-0.04944829]\n",
      " [ 0.01430386]\n",
      " ...\n",
      " [-0.04451414]\n",
      " [-0.05843858]\n",
      " [-0.0122982 ]]\n",
      "Current iteration=4, loss=36368.06211081914\n",
      "t [[-0.25450062]\n",
      " [-0.06153882]\n",
      " [ 0.01833722]\n",
      " ...\n",
      " [-0.05505783]\n",
      " [-0.07210026]\n",
      " [-0.01535916]]\n",
      "t [[-0.25450062]\n",
      " [-0.06153882]\n",
      " [ 0.01833722]\n",
      " ...\n",
      " [-0.05505783]\n",
      " [-0.07210026]\n",
      " [-0.01535916]]\n",
      "t [[-0.30109093]\n",
      " [-0.07350279]\n",
      " [ 0.02250619]\n",
      " ...\n",
      " [-0.06537867]\n",
      " [-0.08540657]\n",
      " [-0.01840781]]\n",
      "t [[-0.30109093]\n",
      " [-0.07350279]\n",
      " [ 0.02250619]\n",
      " ...\n",
      " [-0.06537867]\n",
      " [-0.08540657]\n",
      " [-0.01840781]]\n",
      "Current iteration=6, loss=35805.84168760646\n",
      "t [[-0.34636446]\n",
      " [-0.08533327]\n",
      " [ 0.02679021]\n",
      " ...\n",
      " [-0.07548224]\n",
      " [-0.09836907]\n",
      " [-0.02144132]]\n",
      "t [[-0.34636446]\n",
      " [-0.08533327]\n",
      " [ 0.02679021]\n",
      " ...\n",
      " [-0.07548224]\n",
      " [-0.09836907]\n",
      " [-0.02144132]]\n",
      "t [[-0.39036986]\n",
      " [-0.09702436]\n",
      " [ 0.0311704 ]\n",
      " ...\n",
      " [-0.08537405]\n",
      " [-0.11099907]\n",
      " [-0.0244572 ]]\n",
      "t [[-0.39036986]\n",
      " [-0.09702436]\n",
      " [ 0.0311704 ]\n",
      " ...\n",
      " [-0.08537405]\n",
      " [-0.11099907]\n",
      " [-0.0244572 ]]\n",
      "Current iteration=8, loss=35304.51371610955\n",
      "t [[-0.43315421]\n",
      " [-0.10857117]\n",
      " [ 0.03562942]\n",
      " ...\n",
      " [-0.09505956]\n",
      " [-0.12330761]\n",
      " [-0.02745322]]\n",
      "t [[-0.43315421]\n",
      " [-0.10857117]\n",
      " [ 0.03562942]\n",
      " ...\n",
      " [-0.09505956]\n",
      " [-0.12330761]\n",
      " [-0.02745322]]\n",
      "t [[-0.47476301]\n",
      " [-0.11996966]\n",
      " [ 0.04015148]\n",
      " ...\n",
      " [-0.10454418]\n",
      " [-0.13530542]\n",
      " [-0.03042744]]\n",
      "loss=34856.51501104771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.05352483]\n",
      " [-0.01164187]\n",
      " [ 0.00381046]\n",
      " ...\n",
      " [ 0.05999146]\n",
      " [-0.04417196]\n",
      " [ 0.04866005]]\n",
      "t [[-0.05352483]\n",
      " [-0.01164187]\n",
      " [ 0.00381046]\n",
      " ...\n",
      " [ 0.05999146]\n",
      " [-0.04417196]\n",
      " [ 0.04866005]]\n",
      "t [[-0.10549074]\n",
      " [-0.02323923]\n",
      " [ 0.00782938]\n",
      " ...\n",
      " [ 0.11795105]\n",
      " [-0.08694516]\n",
      " [ 0.0957995 ]]\n",
      "t [[-0.10549074]\n",
      " [-0.02323923]\n",
      " [ 0.00782938]\n",
      " ...\n",
      " [ 0.11795105]\n",
      " [-0.08694516]\n",
      " [ 0.0957995 ]]\n",
      "Current iteration=2, loss=37024.59904093447\n",
      "t [[-0.15595189]\n",
      " [-0.03477707]\n",
      " [ 0.01202976]\n",
      " ...\n",
      " [ 0.17394721]\n",
      " [-0.12836317]\n",
      " [ 0.14146734]]\n",
      "t [[-0.15595189]\n",
      " [-0.03477707]\n",
      " [ 0.01202976]\n",
      " ...\n",
      " [ 0.17394721]\n",
      " [-0.12836317]\n",
      " [ 0.14146734]]\n",
      "t [[-0.20496151]\n",
      " [-0.04624194]\n",
      " [ 0.01638649]\n",
      " ...\n",
      " [ 0.22804773]\n",
      " [-0.16846941]\n",
      " [ 0.18571226]]\n",
      "t [[-0.20496151]\n",
      " [-0.04624194]\n",
      " [ 0.01638649]\n",
      " ...\n",
      " [ 0.22804773]\n",
      " [-0.16846941]\n",
      " [ 0.18571226]]\n",
      "Current iteration=4, loss=36414.44459938089\n",
      "t [[-0.25257173]\n",
      " [-0.05762188]\n",
      " [ 0.02087636]\n",
      " ...\n",
      " [ 0.28031938]\n",
      " [-0.20730692]\n",
      " [ 0.22858234]]\n",
      "t [[-0.25257173]\n",
      " [-0.05762188]\n",
      " [ 0.02087636]\n",
      " ...\n",
      " [ 0.28031938]\n",
      " [-0.20730692]\n",
      " [ 0.22858234]]\n",
      "t [[-0.29883344]\n",
      " [-0.06890632]\n",
      " [ 0.02547795]\n",
      " ...\n",
      " [ 0.33082772]\n",
      " [-0.2449182 ]\n",
      " [ 0.27012493]]\n",
      "t [[-0.29883344]\n",
      " [-0.06890632]\n",
      " [ 0.02547795]\n",
      " ...\n",
      " [ 0.33082772]\n",
      " [-0.2449182 ]\n",
      " [ 0.27012493]]\n",
      "Current iteration=6, loss=35870.947221511284\n",
      "t [[-0.3437961 ]\n",
      " [-0.08008602]\n",
      " [ 0.03017159]\n",
      " ...\n",
      " [ 0.37963687]\n",
      " [-0.28134501]\n",
      " [ 0.31038643]]\n",
      "t [[-0.3437961 ]\n",
      " [-0.08008602]\n",
      " [ 0.03017159]\n",
      " ...\n",
      " [ 0.37963687]\n",
      " [-0.28134501]\n",
      " [ 0.31038643]]\n",
      "t [[-0.38750772]\n",
      " [-0.09115295]\n",
      " [ 0.03493926]\n",
      " ...\n",
      " [ 0.42680933]\n",
      " [-0.31662827]\n",
      " [ 0.34941218]]\n",
      "t [[-0.38750772]\n",
      " [-0.09115295]\n",
      " [ 0.03493926]\n",
      " ...\n",
      " [ 0.42680933]\n",
      " [-0.31662827]\n",
      " [ 0.34941218]]\n",
      "Current iteration=8, loss=35385.89974105927\n",
      "t [[-0.43001473]\n",
      " [-0.10210023]\n",
      " [ 0.03976452]\n",
      " ...\n",
      " [ 0.47240587]\n",
      " [-0.35080797]\n",
      " [ 0.38724635]]\n",
      "t [[-0.43001473]\n",
      " [-0.10210023]\n",
      " [ 0.03976452]\n",
      " ...\n",
      " [ 0.47240587]\n",
      " [-0.35080797]\n",
      " [ 0.38724635]]\n",
      "t [[-0.47136197]\n",
      " [-0.11292201]\n",
      " [ 0.04463239]\n",
      " ...\n",
      " [ 0.51648543]\n",
      " [-0.38392305]\n",
      " [ 0.42393187]]\n",
      "loss=34952.08278609367\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.05498495]\n",
      " [-0.01156709]\n",
      " [ 0.01148037]\n",
      " ...\n",
      " [-0.01451171]\n",
      " [-0.01804277]\n",
      " [-0.003793  ]]\n",
      "t [[-0.05498495]\n",
      " [-0.01156709]\n",
      " [ 0.01148037]\n",
      " ...\n",
      " [-0.01451171]\n",
      " [-0.01804277]\n",
      " [-0.003793  ]]\n",
      "t [[-0.1077317 ]\n",
      " [-0.02215761]\n",
      " [ 0.02268504]\n",
      " ...\n",
      " [-0.0286365 ]\n",
      " [-0.03551277]\n",
      " [-0.0075771 ]]\n",
      "t [[-0.1077317 ]\n",
      " [-0.02215761]\n",
      " [ 0.02268504]\n",
      " ...\n",
      " [-0.0286365 ]\n",
      " [-0.03551277]\n",
      " [-0.0075771 ]]\n",
      "Current iteration=2, loss=36883.36625758257\n",
      "t [[-0.15831817]\n",
      " [-0.03185936]\n",
      " [ 0.03360403]\n",
      " ...\n",
      " [-0.04238516]\n",
      " [-0.05243075]\n",
      " [-0.01134578]]\n",
      "t [[-0.15831817]\n",
      " [-0.03185936]\n",
      " [ 0.03360403]\n",
      " ...\n",
      " [-0.04238516]\n",
      " [-0.05243075]\n",
      " [-0.01134578]]\n",
      "t [[-0.20682271]\n",
      " [-0.04075379]\n",
      " [ 0.04423003]\n",
      " ...\n",
      " [-0.05576863]\n",
      " [-0.06881727]\n",
      " [-0.01509327]]\n",
      "t [[-0.20682271]\n",
      " [-0.04075379]\n",
      " [ 0.04423003]\n",
      " ...\n",
      " [-0.05576863]\n",
      " [-0.06881727]\n",
      " [-0.01509327]]\n",
      "Current iteration=4, loss=36164.871600637365\n",
      "t [[-0.25332347]\n",
      " [-0.04891618]\n",
      " [ 0.05455814]\n",
      " ...\n",
      " [-0.06879793]\n",
      " [-0.08469254]\n",
      " [-0.01881459]]\n",
      "t [[-0.25332347]\n",
      " [-0.04891618]\n",
      " [ 0.05455814]\n",
      " ...\n",
      " [-0.06879793]\n",
      " [-0.08469254]\n",
      " [-0.01881459]]\n",
      "t [[-0.29789785]\n",
      " [-0.0564158 ]\n",
      " [ 0.06458559]\n",
      " ...\n",
      " [-0.08148401]\n",
      " [-0.10007637]\n",
      " [-0.02250543]]\n",
      "t [[-0.29789785]\n",
      " [-0.0564158 ]\n",
      " [ 0.06458559]\n",
      " ...\n",
      " [-0.08148401]\n",
      " [-0.10007637]\n",
      " [-0.02250543]]\n",
      "Current iteration=6, loss=35539.419364725436\n",
      "t [[-0.34062204]\n",
      " [-0.06331623]\n",
      " [ 0.07431149]\n",
      " ...\n",
      " [-0.09383775]\n",
      " [-0.11498803]\n",
      " [-0.02616212]]\n",
      "t [[-0.34062204]\n",
      " [-0.06331623]\n",
      " [ 0.07431149]\n",
      " ...\n",
      " [-0.09383775]\n",
      " [-0.11498803]\n",
      " [-0.02616212]]\n",
      "t [[-0.3815707 ]\n",
      " [-0.06967566]\n",
      " [ 0.08373658]\n",
      " ...\n",
      " [-0.10586986]\n",
      " [-0.12944623]\n",
      " [-0.02978154]]\n",
      "t [[-0.3815707 ]\n",
      " [-0.06967566]\n",
      " [ 0.08373658]\n",
      " ...\n",
      " [-0.10586986]\n",
      " [-0.12944623]\n",
      " [-0.02978154]]\n",
      "Current iteration=8, loss=34993.398375732344\n",
      "t [[-0.42081661]\n",
      " [-0.07554719]\n",
      " [ 0.09286301]\n",
      " ...\n",
      " [-0.11759083]\n",
      " [-0.14346907]\n",
      " [-0.03336108]]\n",
      "t [[-0.42081661]\n",
      " [-0.07554719]\n",
      " [ 0.09286301]\n",
      " ...\n",
      " [-0.11759083]\n",
      " [-0.14346907]\n",
      " [-0.03336108]]\n",
      "t [[-0.45843048]\n",
      " [-0.08097925]\n",
      " [ 0.10169412]\n",
      " ...\n",
      " [-0.12901092]\n",
      " [-0.15707401]\n",
      " [-0.03689858]]\n",
      "loss=34515.1738042725\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.06457527]\n",
      " [-0.01454799]\n",
      " [ 0.00568264]\n",
      " ...\n",
      " [-0.01394939]\n",
      " [-0.01813659]\n",
      " [-0.00376451]]\n",
      "t [[-0.06457527]\n",
      " [-0.01454799]\n",
      " [ 0.00568264]\n",
      " ...\n",
      " [-0.01394939]\n",
      " [-0.01813659]\n",
      " [-0.00376451]]\n",
      "t [[-0.12688031]\n",
      " [-0.02899981]\n",
      " [ 0.0115849 ]\n",
      " ...\n",
      " [-0.0275248 ]\n",
      " [-0.03569553]\n",
      " [-0.00753136]]\n",
      "t [[-0.12688031]\n",
      " [-0.02899981]\n",
      " [ 0.0115849 ]\n",
      " ...\n",
      " [-0.0275248 ]\n",
      " [-0.03569553]\n",
      " [-0.00753136]]\n",
      "Current iteration=2, loss=36876.01579921044\n",
      "t [[-0.18701021]\n",
      " [-0.04333211]\n",
      " [ 0.01766486]\n",
      " ...\n",
      " [-0.04073728]\n",
      " [-0.05269786]\n",
      " [-0.01129267]]\n",
      "t [[-0.18701021]\n",
      " [-0.04333211]\n",
      " [ 0.01766486]\n",
      " ...\n",
      " [-0.04073728]\n",
      " [-0.05269786]\n",
      " [-0.01129267]]\n",
      "t [[-0.24505811]\n",
      " [-0.05752464]\n",
      " [ 0.0238845 ]\n",
      " ...\n",
      " [-0.05359792]\n",
      " [-0.0691644 ]\n",
      " [-0.0150415 ]]\n",
      "t [[-0.24505811]\n",
      " [-0.05752464]\n",
      " [ 0.0238845 ]\n",
      " ...\n",
      " [-0.05359792]\n",
      " [-0.0691644 ]\n",
      " [-0.0150415 ]]\n",
      "Current iteration=4, loss=36151.700888552536\n",
      "t [[-0.30111462]\n",
      " [-0.07156001]\n",
      " [ 0.03020951]\n",
      " ...\n",
      " [-0.06611781]\n",
      " [-0.08511566]\n",
      " [-0.01877175]]\n",
      "t [[-0.30111462]\n",
      " [-0.07156001]\n",
      " [ 0.03020951]\n",
      " ...\n",
      " [-0.06611781]\n",
      " [-0.08511566]\n",
      " [-0.01877175]]\n",
      "t [[-0.3552675 ]\n",
      " [-0.08542354]\n",
      " [ 0.03660913]\n",
      " ...\n",
      " [-0.07830794]\n",
      " [-0.10057168]\n",
      " [-0.02247815]]\n",
      "t [[-0.3552675 ]\n",
      " [-0.08542354]\n",
      " [ 0.03660913]\n",
      " ...\n",
      " [-0.07830794]\n",
      " [-0.10057168]\n",
      " [-0.02247815]]\n",
      "Current iteration=6, loss=35521.60915957639\n",
      "t [[-0.40760141]\n",
      " [-0.09910297]\n",
      " [ 0.04305591]\n",
      " ...\n",
      " [-0.09017915]\n",
      " [-0.11555199]\n",
      " [-0.02615612]]\n",
      "t [[-0.40760141]\n",
      " [-0.09910297]\n",
      " [ 0.04305591]\n",
      " ...\n",
      " [-0.09017915]\n",
      " [-0.11555199]\n",
      " [-0.02615612]]\n",
      "t [[-0.45819768]\n",
      " [-0.1125883 ]\n",
      " [ 0.04952551]\n",
      " ...\n",
      " [-0.10174207]\n",
      " [-0.13007553]\n",
      " [-0.02980174]]\n",
      "t [[-0.45819768]\n",
      " [-0.1125883 ]\n",
      " [ 0.04952551]\n",
      " ...\n",
      " [-0.10174207]\n",
      " [-0.13007553]\n",
      " [-0.02980174]]\n",
      "Current iteration=8, loss=34971.85486206536\n",
      "t [[-0.50713423]\n",
      " [-0.1258715 ]\n",
      " [ 0.05599642]\n",
      " ...\n",
      " [-0.11300706]\n",
      " [-0.14416064]\n",
      " [-0.03341171]]\n",
      "t [[-0.50713423]\n",
      " [-0.1258715 ]\n",
      " [ 0.05599642]\n",
      " ...\n",
      " [-0.11300706]\n",
      " [-0.14416064]\n",
      " [-0.03341171]]\n",
      "t [[-0.55448554]\n",
      " [-0.13894635]\n",
      " [ 0.06244975]\n",
      " ...\n",
      " [-0.12398423]\n",
      " [-0.15782496]\n",
      " [-0.03698323]]\n",
      "loss=34490.58971354734\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.06474043]\n",
      " [-0.01500471]\n",
      " [ 0.00389949]\n",
      " ...\n",
      " [-0.01378852]\n",
      " [-0.01824312]\n",
      " [-0.00368984]]\n",
      "t [[-0.06474043]\n",
      " [-0.01500471]\n",
      " [ 0.00389949]\n",
      " ...\n",
      " [-0.01378852]\n",
      " [-0.01824312]\n",
      " [-0.00368984]]\n",
      "t [[-0.12720698]\n",
      " [-0.02988428]\n",
      " [ 0.00813856]\n",
      " ...\n",
      " [-0.02722442]\n",
      " [-0.03590536]\n",
      " [-0.00738342]]\n",
      "t [[-0.12720698]\n",
      " [-0.02988428]\n",
      " [ 0.00813856]\n",
      " ...\n",
      " [-0.02722442]\n",
      " [-0.03590536]\n",
      " [-0.00738342]]\n",
      "Current iteration=2, loss=36865.072140095464\n",
      "t [[-0.18749435]\n",
      " [-0.04461822]\n",
      " [ 0.01266901]\n",
      " ...\n",
      " [-0.04031715]\n",
      " [-0.0530079 ]\n",
      " [-0.01107329]]\n",
      "t [[-0.18749435]\n",
      " [-0.04461822]\n",
      " [ 0.01266901]\n",
      " ...\n",
      " [-0.04031715]\n",
      " [-0.0530079 ]\n",
      " [-0.01107329]]\n",
      "t [[-0.24569539]\n",
      " [-0.05918882]\n",
      " [ 0.01744657]\n",
      " ...\n",
      " [-0.05307631]\n",
      " [-0.06957169]\n",
      " [-0.01475283]]\n",
      "t [[-0.24569539]\n",
      " [-0.05918882]\n",
      " [ 0.01744657]\n",
      " ...\n",
      " [-0.05307631]\n",
      " [-0.06957169]\n",
      " [-0.01475283]]\n",
      "Current iteration=4, loss=36131.30737498074\n",
      "t [[-0.30190055]\n",
      " [-0.07358098]\n",
      " [ 0.02243089]\n",
      " ...\n",
      " [-0.06551156]\n",
      " [-0.08561737]\n",
      " [-0.01841624]]\n",
      "t [[-0.30190055]\n",
      " [-0.07358098]\n",
      " [ 0.02243089]\n",
      " ...\n",
      " [-0.06551156]\n",
      " [-0.08561737]\n",
      " [-0.01841624]]\n",
      "t [[-0.35619752]\n",
      " [-0.08778206]\n",
      " [ 0.02758534]\n",
      " ...\n",
      " [-0.07763256]\n",
      " [-0.10116511]\n",
      " [-0.02205844]]\n",
      "t [[-0.35619752]\n",
      " [-0.08778206]\n",
      " [ 0.02758534]\n",
      " ...\n",
      " [-0.07763256]\n",
      " [-0.10116511]\n",
      " [-0.02205844]]\n",
      "Current iteration=6, loss=35493.0481820722\n",
      "t [[-0.40867092]\n",
      " [-0.10178165]\n",
      " [ 0.0328768 ]\n",
      " ...\n",
      " [-0.08944891]\n",
      " [-0.11623458]\n",
      " [-0.02567501]]\n",
      "t [[-0.40867092]\n",
      " [-0.10178165]\n",
      " [ 0.0328768 ]\n",
      " ...\n",
      " [-0.08944891]\n",
      " [-0.11623458]\n",
      " [-0.02567501]]\n",
      "t [[-0.45940212]\n",
      " [-0.11557137]\n",
      " [ 0.03827551]\n",
      " ...\n",
      " [-0.10097006]\n",
      " [-0.13084486]\n",
      " [-0.02926219]]\n",
      "t [[-0.45940212]\n",
      " [-0.11557137]\n",
      " [ 0.03827551]\n",
      " ...\n",
      " [-0.10097006]\n",
      " [-0.13084486]\n",
      " [-0.02926219]]\n",
      "Current iteration=8, loss=34936.21983715764\n",
      "t [[-0.50846912]\n",
      " [-0.12914467]\n",
      " [ 0.04375481]\n",
      " ...\n",
      " [-0.1122053 ]\n",
      " [-0.14501439]\n",
      " [-0.03281672]]\n",
      "t [[-0.50846912]\n",
      " [-0.12914467]\n",
      " [ 0.04375481]\n",
      " ...\n",
      " [-0.1122053 ]\n",
      " [-0.14501439]\n",
      " [-0.03281672]]\n",
      "t [[-0.5559465 ]\n",
      " [-0.14249665]\n",
      " [ 0.0492909 ]\n",
      " ...\n",
      " [-0.12316372]\n",
      " [-0.15876096]\n",
      " [-0.03633586]]\n",
      "loss=34448.80992261769\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.06422979]\n",
      " [-0.01397024]\n",
      " [ 0.00457255]\n",
      " ...\n",
      " [ 0.07198975]\n",
      " [-0.05300635]\n",
      " [ 0.05839206]]\n",
      "t [[-0.06422979]\n",
      " [-0.01397024]\n",
      " [ 0.00457255]\n",
      " ...\n",
      " [ 0.07198975]\n",
      " [-0.05300635]\n",
      " [ 0.05839206]]\n",
      "t [[-0.12621493]\n",
      " [-0.02787637]\n",
      " [ 0.00944525]\n",
      " ...\n",
      " [ 0.14105382]\n",
      " [-0.10399863]\n",
      " [ 0.11459461]]\n",
      "t [[-0.12621493]\n",
      " [-0.02787637]\n",
      " [ 0.00944525]\n",
      " ...\n",
      " [ 0.14105382]\n",
      " [-0.10399863]\n",
      " [ 0.11459461]]\n",
      "Current iteration=2, loss=36894.55358297548\n",
      "t [[-0.18604944]\n",
      " [-0.04169242]\n",
      " [ 0.01457136]\n",
      " ...\n",
      " [ 0.20731108]\n",
      " [-0.1530525 ]\n",
      " [ 0.16869278]]\n",
      "t [[-0.18604944]\n",
      " [-0.04169242]\n",
      " [ 0.01457136]\n",
      " ...\n",
      " [ 0.20731108]\n",
      " [-0.1530525 ]\n",
      " [ 0.16869278]]\n",
      "t [[-0.24382531]\n",
      " [-0.05539567]\n",
      " [ 0.01990815]\n",
      " ...\n",
      " [ 0.27087877]\n",
      " [-0.2002432 ]\n",
      " [ 0.22077085]]\n",
      "t [[-0.24382531]\n",
      " [-0.05539567]\n",
      " [ 0.01990815]\n",
      " ...\n",
      " [ 0.27087877]\n",
      " [-0.2002432 ]\n",
      " [ 0.22077085]]\n",
      "Current iteration=4, loss=36185.66047423368\n",
      "t [[-0.29963205]\n",
      " [-0.06896651]\n",
      " [ 0.02541678]\n",
      " ...\n",
      " [ 0.33187184]\n",
      " [-0.24564499]\n",
      " [ 0.27091169]]\n",
      "t [[-0.29963205]\n",
      " [-0.06896651]\n",
      " [ 0.02541678]\n",
      " ...\n",
      " [ 0.33187184]\n",
      " [-0.24564499]\n",
      " [ 0.27091169]]\n",
      "t [[-0.35355628]\n",
      " [-0.08238816]\n",
      " [ 0.03106217]\n",
      " ...\n",
      " [ 0.39040231]\n",
      " [-0.28933073]\n",
      " [ 0.31919635]]\n",
      "t [[-0.35355628]\n",
      " [-0.08238816]\n",
      " [ 0.03106217]\n",
      " ...\n",
      " [ 0.39040231]\n",
      " [-0.28933073]\n",
      " [ 0.31919635]]\n",
      "Current iteration=6, loss=35568.40028336179\n",
      "t [[-0.40568152]\n",
      " [-0.09564645]\n",
      " [ 0.03681274]\n",
      " ...\n",
      " [ 0.44657889]\n",
      " [-0.33137155]\n",
      " [ 0.36570365]]\n",
      "t [[-0.40568152]\n",
      " [-0.09564645]\n",
      " [ 0.03681274]\n",
      " ...\n",
      " [ 0.44657889]\n",
      " [-0.33137155]\n",
      " [ 0.36570365]]\n",
      "t [[-0.45608802]\n",
      " [-0.10872963]\n",
      " [ 0.04264021]\n",
      " ...\n",
      " [ 0.50050662]\n",
      " [-0.37183659]\n",
      " [ 0.41050995]]\n",
      "t [[-0.45608802]\n",
      " [-0.10872963]\n",
      " [ 0.04264021]\n",
      " ...\n",
      " [ 0.50050662]\n",
      " [-0.37183659]\n",
      " [ 0.41050995]]\n",
      "Current iteration=8, loss=35029.338557101255\n",
      "t [[-0.50485262]\n",
      " [-0.12162808]\n",
      " [ 0.04851939]\n",
      " ...\n",
      " [ 0.55228666]\n",
      " [-0.41079278]\n",
      " [ 0.453689  ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.50485262]\n",
      " [-0.12162808]\n",
      " [ 0.04851939]\n",
      " ...\n",
      " [ 0.55228666]\n",
      " [-0.41079278]\n",
      " [ 0.453689  ]]\n",
      "t [[-0.55204877]\n",
      " [-0.13433413]\n",
      " [ 0.05442788]\n",
      " ...\n",
      " [ 0.60201617]\n",
      " [-0.44830474]\n",
      " [ 0.49531173]]\n",
      "loss=34557.006911829456\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.06414911]\n",
      " [-0.01349494]\n",
      " [ 0.01339377]\n",
      " ...\n",
      " [-0.01693032]\n",
      " [-0.0210499 ]\n",
      " [-0.00442516]]\n",
      "t [[-0.06414911]\n",
      " [-0.01349494]\n",
      " [ 0.01339377]\n",
      " ...\n",
      " [-0.01693032]\n",
      " [-0.0210499 ]\n",
      " [-0.00442516]]\n",
      "t [[-0.12525201]\n",
      " [-0.02566077]\n",
      " [ 0.02641228]\n",
      " ...\n",
      " [-0.03333407]\n",
      " [-0.04132026]\n",
      " [-0.00883823]]\n",
      "t [[-0.12525201]\n",
      " [-0.02566077]\n",
      " [ 0.02641228]\n",
      " ...\n",
      " [-0.03333407]\n",
      " [-0.04132026]\n",
      " [-0.00883823]]\n",
      "Current iteration=2, loss=36753.89267978137\n",
      "t [[-0.18343303]\n",
      " [-0.03663718]\n",
      " [ 0.03903974]\n",
      " ...\n",
      " [-0.04922848]\n",
      " [-0.06084419]\n",
      " [-0.01322881]]\n",
      "t [[-0.18343303]\n",
      " [-0.03663718]\n",
      " [ 0.03903974]\n",
      " ...\n",
      " [-0.04922848]\n",
      " [-0.06084419]\n",
      " [-0.01322881]]\n",
      "t [[-0.23881711]\n",
      " [-0.04655193]\n",
      " [ 0.05126531]\n",
      " ...\n",
      " [-0.06463109]\n",
      " [-0.07965438]\n",
      " [-0.01758802]]\n",
      "t [[-0.23881711]\n",
      " [-0.04655193]\n",
      " [ 0.05126531]\n",
      " ...\n",
      " [-0.06463109]\n",
      " [-0.07965438]\n",
      " [-0.01758802]]\n",
      "Current iteration=4, loss=35942.374364500814\n",
      "t [[-0.29152838]\n",
      " [-0.05552121]\n",
      " [ 0.06308252]\n",
      " ...\n",
      " [-0.07955944]\n",
      " [-0.09778283]\n",
      " [-0.02190835]]\n",
      "t [[-0.29152838]\n",
      " [-0.05552121]\n",
      " [ 0.06308252]\n",
      " ...\n",
      " [-0.07955944]\n",
      " [-0.09778283]\n",
      " [-0.02190835]]\n",
      "t [[-0.34168917]\n",
      " [-0.06365021]\n",
      " [ 0.0744887 ]\n",
      " ...\n",
      " [-0.09403093]\n",
      " [-0.11526061]\n",
      " [-0.02618351]]\n",
      "t [[-0.34168917]\n",
      " [-0.06365021]\n",
      " [ 0.0744887 ]\n",
      " ...\n",
      " [-0.09403093]\n",
      " [-0.11526061]\n",
      " [-0.02618351]]\n",
      "Current iteration=6, loss=35251.72944903096\n",
      "t [[-0.38941903]\n",
      " [-0.07103379]\n",
      " [ 0.08548437]\n",
      " ...\n",
      " [-0.10806269]\n",
      " [-0.13211778]\n",
      " [-0.03040832]]\n",
      "t [[-0.38941903]\n",
      " [-0.07103379]\n",
      " [ 0.08548437]\n",
      " ...\n",
      " [-0.10806269]\n",
      " [-0.13211778]\n",
      " [-0.03040832]]\n",
      "t [[-0.43483416]\n",
      " [-0.07775721]\n",
      " [ 0.09607282]\n",
      " ...\n",
      " [-0.12167147]\n",
      " [-0.14838323]\n",
      " [-0.03457857]]\n",
      "t [[-0.43483416]\n",
      " [-0.07775721]\n",
      " [ 0.09607282]\n",
      " ...\n",
      " [-0.12167147]\n",
      " [-0.14838323]\n",
      " [-0.03457857]]\n",
      "Current iteration=8, loss=34661.505208696835\n",
      "t [[-0.47804684]\n",
      " [-0.08389687]\n",
      " [ 0.10625952]\n",
      " ...\n",
      " [-0.13487354]\n",
      " [-0.16408466]\n",
      " [-0.03869089]]\n",
      "t [[-0.47804684]\n",
      " [-0.08389687]\n",
      " [ 0.10625952]\n",
      " ...\n",
      " [-0.13487354]\n",
      " [-0.16408466]\n",
      " [-0.03869089]]\n",
      "t [[-0.51916515]\n",
      " [-0.08952114]\n",
      " [ 0.1160518 ]\n",
      " ...\n",
      " [-0.14768467]\n",
      " [-0.17924851]\n",
      " [-0.04274265]]\n",
      "loss=34154.74603093892\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.07533782]\n",
      " [-0.01697265]\n",
      " [ 0.00662975]\n",
      " ...\n",
      " [-0.01627428]\n",
      " [-0.02115935]\n",
      " [-0.00439193]]\n",
      "t [[-0.07533782]\n",
      " [-0.01697265]\n",
      " [ 0.00662975]\n",
      " ...\n",
      " [-0.01627428]\n",
      " [-0.02115935]\n",
      " [-0.00439193]]\n",
      "t [[-0.14758587]\n",
      " [-0.03381439]\n",
      " [ 0.01355838]\n",
      " ...\n",
      " [-0.0320396 ]\n",
      " [-0.04153252]\n",
      " [-0.00878704]]\n",
      "t [[-0.14758587]\n",
      " [-0.03381439]\n",
      " [ 0.01355838]\n",
      " ...\n",
      " [-0.0320396 ]\n",
      " [-0.04153252]\n",
      " [-0.00878704]]\n",
      "Current iteration=2, loss=36745.43978984299\n",
      "t [[-0.21689594]\n",
      " [-0.05048809]\n",
      " [ 0.02071926]\n",
      " ...\n",
      " [-0.0473136 ]\n",
      " [-0.06115309]\n",
      " [-0.01317283]]\n",
      "t [[-0.21689594]\n",
      " [-0.05048809]\n",
      " [ 0.02071926]\n",
      " ...\n",
      " [-0.0473136 ]\n",
      " [-0.06115309]\n",
      " [-0.01317283]]\n",
      "t [[-0.28341578]\n",
      " [-0.06696243]\n",
      " [ 0.02805303]\n",
      " ...\n",
      " [-0.06211401]\n",
      " [-0.08005417]\n",
      " [-0.01753855]]\n",
      "t [[-0.28341578]\n",
      " [-0.06696243]\n",
      " [ 0.02805303]\n",
      " ...\n",
      " [-0.06211401]\n",
      " [-0.08005417]\n",
      " [-0.01753855]]\n",
      "Current iteration=4, loss=35927.486406943084\n",
      "t [[-0.34728817]\n",
      " [-0.08321139]\n",
      " [ 0.03550725]\n",
      " ...\n",
      " [-0.07645847]\n",
      " [-0.09826818]\n",
      " [-0.02187499]]\n",
      "t [[-0.34728817]\n",
      " [-0.08321139]\n",
      " [ 0.03550725]\n",
      " ...\n",
      " [-0.07645847]\n",
      " [-0.09826818]\n",
      " [-0.02187499]]\n",
      "t [[-0.40865019]\n",
      " [-0.09921379]\n",
      " [ 0.0430359 ]\n",
      " ...\n",
      " [-0.09036436]\n",
      " [-0.1158266 ]\n",
      " [-0.02617439]]\n",
      "t [[-0.40865019]\n",
      " [-0.09921379]\n",
      " [ 0.0430359 ]\n",
      " ...\n",
      " [-0.09036436]\n",
      " [-0.1158266 ]\n",
      " [-0.02617439]]\n",
      "Current iteration=6, loss=35231.893099857974\n",
      "t [[-0.46763278]\n",
      " [-0.11495281]\n",
      " [ 0.05059896]\n",
      " ...\n",
      " [-0.10384872]\n",
      " [-0.13275986]\n",
      " [-0.03043027]]\n",
      "t [[-0.46763278]\n",
      " [-0.11495281]\n",
      " [ 0.05059896]\n",
      " ...\n",
      " [-0.10384872]\n",
      " [-0.13275986]\n",
      " [-0.03043027]]\n",
      "t [[-0.52436049]\n",
      " [-0.13041541]\n",
      " [ 0.05816181]\n",
      " ...\n",
      " [-0.11692809]\n",
      " [-0.14909723]\n",
      " [-0.03463727]]\n",
      "t [[-0.52436049]\n",
      " [-0.13041541]\n",
      " [ 0.05816181]\n",
      " ...\n",
      " [-0.11692809]\n",
      " [-0.14909723]\n",
      " [-0.03463727]]\n",
      "Current iteration=8, loss=34637.808453609585\n",
      "t [[-0.57895141]\n",
      " [-0.14559195]\n",
      " [ 0.06569472]\n",
      " ...\n",
      " [-0.1296185 ]\n",
      " [-0.16486672]\n",
      " [-0.03879102]]\n",
      "t [[-0.57895141]\n",
      " [-0.14559195]\n",
      " [ 0.06569472]\n",
      " ...\n",
      " [-0.1296185 ]\n",
      " [-0.16486672]\n",
      " [-0.03879102]]\n",
      "t [[-0.63151717]\n",
      " [-0.16047568]\n",
      " [ 0.07317239]\n",
      " ...\n",
      " [-0.14193541]\n",
      " [-0.1800951 ]\n",
      " [-0.04288801]]\n",
      "loss=34127.981301874555\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.0755305 ]\n",
      " [-0.01750549]\n",
      " [ 0.0045494 ]\n",
      " ...\n",
      " [-0.01608661]\n",
      " [-0.02128363]\n",
      " [-0.00430481]]\n",
      "t [[-0.0755305 ]\n",
      " [-0.01750549]\n",
      " [ 0.0045494 ]\n",
      " ...\n",
      " [-0.01608661]\n",
      " [-0.02128363]\n",
      " [-0.00430481]]\n",
      "t [[-0.14796629]\n",
      " [-0.03484064]\n",
      " [ 0.00956096]\n",
      " ...\n",
      " [-0.03169331]\n",
      " [-0.0417767 ]\n",
      " [-0.00861472]]\n",
      "t [[-0.14796629]\n",
      " [-0.03484064]\n",
      " [ 0.00956096]\n",
      " ...\n",
      " [-0.03169331]\n",
      " [-0.0417767 ]\n",
      " [-0.00861472]]\n",
      "Current iteration=2, loss=36732.78727524704\n",
      "t [[-0.2174585 ]\n",
      " [-0.05197286]\n",
      " [ 0.01495798]\n",
      " ...\n",
      " [-0.04683522]\n",
      " [-0.06151299]\n",
      " [-0.01291788]]\n",
      "t [[-0.2174585 ]\n",
      " [-0.05197286]\n",
      " [ 0.01495798]\n",
      " ...\n",
      " [-0.04683522]\n",
      " [-0.06151299]\n",
      " [-0.01291788]]\n",
      "t [[-0.28415452]\n",
      " [-0.06887478]\n",
      " [ 0.02067124]\n",
      " ...\n",
      " [-0.06152772]\n",
      " [-0.08052582]\n",
      " [-0.01720406]]\n",
      "t [[-0.28415452]\n",
      " [-0.06887478]\n",
      " [ 0.02067124]\n",
      " ...\n",
      " [-0.06152772]\n",
      " [-0.08052582]\n",
      " [-0.01720406]]\n",
      "Current iteration=4, loss=35904.17678698007\n",
      "t [[-0.34819691]\n",
      " [-0.08552389]\n",
      " [ 0.02663872]\n",
      " ...\n",
      " [-0.07578622]\n",
      " [-0.09884783]\n",
      " [-0.02146444]]\n",
      "t [[-0.34819691]\n",
      " [-0.08552389]\n",
      " [ 0.02663872]\n",
      " ...\n",
      " [-0.07578622]\n",
      " [-0.09884783]\n",
      " [-0.02146444]]\n",
      "t [[-0.40972267]\n",
      " [-0.10190207]\n",
      " [ 0.03280528]\n",
      " ...\n",
      " [-0.08962608]\n",
      " [-0.11651071]\n",
      " [-0.02569156]]\n",
      "t [[-0.40972267]\n",
      " [-0.10190207]\n",
      " [ 0.03280528]\n",
      " ...\n",
      " [-0.08962608]\n",
      " [-0.11651071]\n",
      " [-0.02569156]]\n",
      "Current iteration=6, loss=35199.59220591453\n",
      "t [[-0.4688628 ]\n",
      " [-0.11799517]\n",
      " [ 0.03912212]\n",
      " ...\n",
      " [-0.10306241]\n",
      " [-0.13354512]\n",
      " [-0.02987916]]\n",
      "t [[-0.4688628 ]\n",
      " [-0.11799517]\n",
      " [ 0.03912212]\n",
      " ...\n",
      " [-0.10306241]\n",
      " [-0.13354512]\n",
      " [-0.02987916]]\n",
      "t [[-0.52574195]\n",
      " [-0.13379254]\n",
      " [ 0.04554637]\n",
      " ...\n",
      " [-0.11611004]\n",
      " [-0.14998049]\n",
      " [-0.034022  ]]\n",
      "t [[-0.52574195]\n",
      " [-0.13379254]\n",
      " [ 0.04554637]\n",
      " ...\n",
      " [-0.11611004]\n",
      " [-0.14998049]\n",
      " [-0.034022  ]]\n",
      "Current iteration=8, loss=34597.89610841792\n",
      "t [[-0.58047837]\n",
      " [-0.14928661]\n",
      " [ 0.05204054]\n",
      " ...\n",
      " [-0.1287834 ]\n",
      " [-0.16584506]\n",
      " [-0.03811579]]\n",
      "t [[-0.58047837]\n",
      " [-0.14928661]\n",
      " [ 0.05204054]\n",
      " ...\n",
      " [-0.1287834 ]\n",
      " [-0.16584506]\n",
      " [-0.03811579]]\n",
      "t [[-0.63318389]\n",
      " [-0.16447244]\n",
      " [ 0.05857203]\n",
      " ...\n",
      " [-0.14109649]\n",
      " [-0.18116575]\n",
      " [-0.04215707]]\n",
      "loss=34081.59743691684\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.07493476]\n",
      " [-0.01629861]\n",
      " [ 0.00533464]\n",
      " ...\n",
      " [ 0.08398804]\n",
      " [-0.06184074]\n",
      " [ 0.06812407]]\n",
      "t [[-0.07493476]\n",
      " [-0.01629861]\n",
      " [ 0.00533464]\n",
      " ...\n",
      " [ 0.08398804]\n",
      " [-0.06184074]\n",
      " [ 0.06812407]]\n",
      "t [[-0.14681457]\n",
      " [-0.03250994]\n",
      " [ 0.01107777]\n",
      " ...\n",
      " [ 0.16399425]\n",
      " [-0.12094032]\n",
      " [ 0.13326824]]\n",
      "t [[-0.14681457]\n",
      " [-0.03250994]\n",
      " [ 0.01107777]\n",
      " ...\n",
      " [ 0.16399425]\n",
      " [-0.12094032]\n",
      " [ 0.13326824]]\n",
      "Current iteration=2, loss=36766.824174011985\n",
      "t [[-0.21578947]\n",
      " [-0.04859267]\n",
      " [ 0.01715504]\n",
      " ...\n",
      " [ 0.24020828]\n",
      " [-0.1774195 ]\n",
      " [ 0.19556836]]\n",
      "t [[-0.21578947]\n",
      " [-0.04859267]\n",
      " [ 0.01715504]\n",
      " ...\n",
      " [ 0.24020828]\n",
      " [-0.1774195 ]\n",
      " [ 0.19556836]]\n",
      "t [[-0.28200544]\n",
      " [-0.06451164]\n",
      " [ 0.02349964]\n",
      " ...\n",
      " [ 0.31281643]\n",
      " [-0.23139799]\n",
      " [ 0.25515846]]\n",
      "t [[-0.28200544]\n",
      " [-0.06451164]\n",
      " [ 0.02349964]\n",
      " ...\n",
      " [ 0.31281643]\n",
      " [-0.23139799]\n",
      " [ 0.25515846]]\n",
      "Current iteration=4, loss=35966.108975935225\n",
      "t [[-0.34560345]\n",
      " [-0.08023734]\n",
      " [ 0.03005202]\n",
      " ...\n",
      " [ 0.3820002 ]\n",
      " [-0.28299333]\n",
      " [ 0.31216959]]\n",
      "t [[-0.34560345]\n",
      " [-0.08023734]\n",
      " [ 0.03005202]\n",
      " ...\n",
      " [ 0.3820002 ]\n",
      " [-0.28299333]\n",
      " [ 0.31216959]]\n",
      "t [[-0.40671876]\n",
      " [-0.09574544]\n",
      " [ 0.03675947]\n",
      " ...\n",
      " [ 0.44793515]\n",
      " [-0.33232012]\n",
      " [ 0.36672893]]\n",
      "t [[-0.40671876]\n",
      " [-0.09574544]\n",
      " [ 0.03675947]\n",
      " ...\n",
      " [ 0.44793515]\n",
      " [-0.33232012]\n",
      " [ 0.36672893]]\n",
      "Current iteration=6, loss=35284.42038361575\n",
      "t [[-0.46548058]\n",
      " [-0.11101624]\n",
      " [ 0.04357561]\n",
      " ...\n",
      " [ 0.51079015]\n",
      " [-0.37948932]\n",
      " [ 0.41895917]]\n",
      "t [[-0.46548058]\n",
      " [-0.11101624]\n",
      " [ 0.04357561]\n",
      " ...\n",
      " [ 0.51079015]\n",
      " [-0.37948932]\n",
      " [ 0.41895917]]\n",
      "t [[-0.52201173]\n",
      " [-0.12603418]\n",
      " [ 0.05045988]\n",
      " ...\n",
      " [ 0.57072681]\n",
      " [-0.42460783]\n",
      " [ 0.46897806]]\n",
      "t [[-0.52201173]\n",
      " [-0.12603418]\n",
      " [ 0.05045988]\n",
      " ...\n",
      " [ 0.57072681]\n",
      " [-0.42460783]\n",
      " [ 0.46897806]]\n",
      "Current iteration=8, loss=34701.58193650311\n",
      "t [[-0.57642865]\n",
      " [-0.14078731]\n",
      " [ 0.05737704]\n",
      " ...\n",
      " [ 0.62789924]\n",
      " [-0.4677782 ]\n",
      " [ 0.51689818]]\n",
      "t [[-0.57642865]\n",
      " [-0.14078731]\n",
      " [ 0.05737704]\n",
      " ...\n",
      " [ 0.62789924]\n",
      " [-0.4677782 ]\n",
      " [ 0.51689818]]\n",
      "t [[-0.62884139]\n",
      " [-0.15526683]\n",
      " [ 0.06429661]\n",
      " ...\n",
      " [ 0.6824539 ]\n",
      " [-0.50909844]\n",
      " [ 0.56282674]]\n",
      "loss=34200.88986823073\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.07331327]\n",
      " [-0.01542278]\n",
      " [ 0.01530716]\n",
      " ...\n",
      " [-0.01934894]\n",
      " [-0.02405703]\n",
      " [-0.00505733]]\n",
      "t [[-0.07331327]\n",
      " [-0.01542278]\n",
      " [ 0.01530716]\n",
      " ...\n",
      " [-0.01934894]\n",
      " [-0.02405703]\n",
      " [-0.00505733]]\n",
      "t [[-0.14264815]\n",
      " [-0.02910977]\n",
      " [ 0.03012421]\n",
      " ...\n",
      " [-0.03801018]\n",
      " [-0.04709597]\n",
      " [-0.01009886]]\n",
      "t [[-0.14264815]\n",
      " [-0.02910977]\n",
      " [ 0.03012421]\n",
      " ...\n",
      " [-0.03801018]\n",
      " [-0.04709597]\n",
      " [-0.01009886]]\n",
      "Current iteration=2, loss=36626.73711040034\n",
      "t [[-0.20819114]\n",
      " [-0.04126984]\n",
      " [ 0.04442762]\n",
      " ...\n",
      " [-0.05600967]\n",
      " [-0.0691665 ]\n",
      " [-0.01510911]]\n",
      "t [[-0.20819114]\n",
      " [-0.04126984]\n",
      " [ 0.04442762]\n",
      " ...\n",
      " [-0.05600967]\n",
      " [-0.0691665 ]\n",
      " [-0.01510911]]\n",
      "t [[-0.27012932]\n",
      " [-0.0520912 ]\n",
      " [ 0.05820239]\n",
      " ...\n",
      " [-0.07337373]\n",
      " [-0.09031746]\n",
      " [-0.02007515]]\n",
      "t [[-0.27012932]\n",
      " [-0.0520912 ]\n",
      " [ 0.05820239]\n",
      " ...\n",
      " [-0.07337373]\n",
      " [-0.09031746]\n",
      " [-0.02007515]]\n",
      "Current iteration=4, loss=35728.873096558804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.32864787]\n",
      " [-0.06174228]\n",
      " [ 0.07144078]\n",
      " ...\n",
      " [-0.09012862]\n",
      " [-0.11059633]\n",
      " [-0.02498639]]\n",
      "t [[-0.32864787]\n",
      " [-0.06174228]\n",
      " [ 0.07144078]\n",
      " ...\n",
      " [-0.09012862]\n",
      " [-0.11059633]\n",
      " [-0.02498639]]\n",
      "t [[-0.38392806]\n",
      " [-0.07037291]\n",
      " [ 0.08414123]\n",
      " ...\n",
      " [-0.10630022]\n",
      " [-0.13004893]\n",
      " [-0.02983424]]\n",
      "t [[-0.38392806]\n",
      " [-0.07037291]\n",
      " [ 0.08414123]\n",
      " ...\n",
      " [-0.10630022]\n",
      " [-0.13004893]\n",
      " [-0.02983424]]\n",
      "Current iteration=6, loss=34981.64901498324\n",
      "t [[-0.43614573]\n",
      " [-0.07811578]\n",
      " [ 0.09630725]\n",
      " ...\n",
      " [-0.12191382]\n",
      " [-0.14871915]\n",
      " [-0.03461191]]\n",
      "t [[-0.43614573]\n",
      " [-0.07811578]\n",
      " [ 0.09630725]\n",
      " ...\n",
      " [-0.12191382]\n",
      " [-0.14871915]\n",
      " [-0.03461191]]\n",
      "t [[-0.48547027]\n",
      " [-0.08508795]\n",
      " [ 0.10794646]\n",
      " ...\n",
      " [-0.13699392]\n",
      " [-0.16664883]\n",
      " [-0.0393141 ]]\n",
      "t [[-0.48547027]\n",
      " [-0.08508795]\n",
      " [ 0.10794646]\n",
      " ...\n",
      " [-0.13699392]\n",
      " [-0.16664883]\n",
      " [-0.0393141 ]]\n",
      "Current iteration=8, loss=34356.21717804693\n",
      "t [[-0.53206382]\n",
      " [-0.09139238]\n",
      " [ 0.11906969]\n",
      " ...\n",
      " [-0.15156413]\n",
      " [-0.18387765]\n",
      " [-0.04393682]]\n",
      "t [[-0.53206382]\n",
      " [-0.09139238]\n",
      " [ 0.11906969]\n",
      " ...\n",
      " [-0.15156413]\n",
      " [-0.18387765]\n",
      " [-0.04393682]]\n",
      "t [[-0.5760809 ]\n",
      " [-0.09711942]\n",
      " [ 0.12969028]\n",
      " ...\n",
      " [-0.16564711]\n",
      " [-0.20044315]\n",
      " [-0.04847717]]\n",
      "loss=33829.39463038943\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.08610036]\n",
      " [-0.01939732]\n",
      " [ 0.00757685]\n",
      " ...\n",
      " [-0.01859918]\n",
      " [-0.02418211]\n",
      " [-0.00501935]]\n",
      "t [[-0.08610036]\n",
      " [-0.01939732]\n",
      " [ 0.00757685]\n",
      " ...\n",
      " [-0.01859918]\n",
      " [-0.02418211]\n",
      " [-0.00501935]]\n",
      "t [[-0.16816554]\n",
      " [-0.03862361]\n",
      " [ 0.01554403]\n",
      " ...\n",
      " [-0.03653367]\n",
      " [-0.04733748]\n",
      " [-0.01004285]]\n",
      "t [[-0.16816554]\n",
      " [-0.03862361]\n",
      " [ 0.01554403]\n",
      " ...\n",
      " [-0.03653367]\n",
      " [-0.04733748]\n",
      " [-0.01004285]]\n",
      "Current iteration=2, loss=36617.21478861492\n",
      "t [[-0.24642316]\n",
      " [-0.05762342]\n",
      " [ 0.02380195]\n",
      " ...\n",
      " [-0.05382997]\n",
      " [-0.06951645]\n",
      " [-0.01505187]]\n",
      "t [[-0.24642316]\n",
      " [-0.05762342]\n",
      " [ 0.02380195]\n",
      " ...\n",
      " [-0.05382997]\n",
      " [-0.06951645]\n",
      " [-0.01505187]]\n",
      "t [[-0.32109349]\n",
      " [-0.07635127]\n",
      " [ 0.03226359]\n",
      " ...\n",
      " [-0.07051471]\n",
      " [-0.0907685 ]\n",
      " [-0.02003073]]\n",
      "t [[-0.32109349]\n",
      " [-0.07635127]\n",
      " [ 0.03226359]\n",
      " ...\n",
      " [-0.07051471]\n",
      " [-0.0907685 ]\n",
      " [-0.02003073]]\n",
      "Current iteration=4, loss=35712.38135965998\n",
      "t [[-0.39238768]\n",
      " [-0.09477067]\n",
      " [ 0.04085369]\n",
      " ...\n",
      " [-0.0866142 ]\n",
      " [-0.11114175]\n",
      " [-0.0249664 ]]\n",
      "t [[-0.39238768]\n",
      " [-0.09477067]\n",
      " [ 0.04085369]\n",
      " ...\n",
      " [-0.0866142 ]\n",
      " [-0.11114175]\n",
      " [-0.0249664 ]]\n",
      "t [[-0.46050657]\n",
      " [-0.11285317]\n",
      " [ 0.04950781]\n",
      " ...\n",
      " [-0.10215424]\n",
      " [-0.13068261]\n",
      " [-0.02984824]]\n",
      "t [[-0.46050657]\n",
      " [-0.11285317]\n",
      " [ 0.04950781]\n",
      " ...\n",
      " [-0.10215424]\n",
      " [-0.13068261]\n",
      " [-0.02984824]]\n",
      "Current iteration=6, loss=34959.98245341639\n",
      "t [[-0.5256401 ]\n",
      " [-0.13057731]\n",
      " [ 0.05817125]\n",
      " ...\n",
      " [-0.11715987]\n",
      " [-0.14943551]\n",
      " [-0.03466764]]\n",
      "t [[-0.5256401 ]\n",
      " [-0.13057731]\n",
      " [ 0.05817125]\n",
      " ...\n",
      " [-0.11715987]\n",
      " [-0.14943551]\n",
      " [-0.03466764]]\n",
      "t [[-0.58796702]\n",
      " [-0.14792768]\n",
      " [ 0.06679808]\n",
      " ...\n",
      " [-0.13165525]\n",
      " [-0.16744279]\n",
      " [-0.03941776]]\n",
      "t [[-0.58796702]\n",
      " [-0.14792768]\n",
      " [ 0.06679808]\n",
      " ...\n",
      " [-0.13165525]\n",
      " [-0.16744279]\n",
      " [-0.03941776]]\n",
      "Current iteration=8, loss=34330.62716925712\n",
      "t [[-0.64765494]\n",
      " [-0.16489396]\n",
      " [ 0.07535001]\n",
      " ...\n",
      " [-0.14566355]\n",
      " [-0.18474462]\n",
      " [-0.0440933 ]]\n",
      "t [[-0.64765494]\n",
      " [-0.16489396]\n",
      " [ 0.07535001]\n",
      " ...\n",
      " [-0.14566355]\n",
      " [-0.18474462]\n",
      " [-0.0440933 ]]\n",
      "t [[-0.7048606 ]\n",
      " [-0.18147013]\n",
      " [ 0.08379554]\n",
      " ...\n",
      " [-0.15920691]\n",
      " [-0.20137895]\n",
      " [-0.04869023]]\n",
      "loss=33800.74880661354\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.08632058]\n",
      " [-0.02000627]\n",
      " [ 0.00519931]\n",
      " ...\n",
      " [-0.01838469]\n",
      " [-0.02432415]\n",
      " [-0.00491978]]\n",
      "t [[-0.08632058]\n",
      " [-0.02000627]\n",
      " [ 0.00519931]\n",
      " ...\n",
      " [-0.01838469]\n",
      " [-0.02432415]\n",
      " [-0.00491978]]\n",
      "t [[-0.1685995 ]\n",
      " [-0.03979003]\n",
      " [ 0.01100218]\n",
      " ...\n",
      " [-0.03614264]\n",
      " [-0.04761583]\n",
      " [-0.00984622]]\n",
      "t [[-0.1685995 ]\n",
      " [-0.03979003]\n",
      " [ 0.01100218]\n",
      " ...\n",
      " [-0.03614264]\n",
      " [-0.04761583]\n",
      " [-0.00984622]]\n",
      "Current iteration=2, loss=36602.88518102553\n",
      "t [[-0.24706349]\n",
      " [-0.05930258]\n",
      " [ 0.01729393]\n",
      " ...\n",
      " [-0.05329659]\n",
      " [-0.06992569]\n",
      " [-0.01476166]]\n",
      "t [[-0.24706349]\n",
      " [-0.05930258]\n",
      " [ 0.01729393]\n",
      " ...\n",
      " [-0.05329659]\n",
      " [-0.06992569]\n",
      " [-0.01476166]]\n",
      "t [[-0.32193229]\n",
      " [-0.07850423]\n",
      " [ 0.02397282]\n",
      " ...\n",
      " [-0.06986963]\n",
      " [-0.09130355]\n",
      " [-0.01965115]]\n",
      "t [[-0.32193229]\n",
      " [-0.07850423]\n",
      " [ 0.02397282]\n",
      " ...\n",
      " [-0.06986963]\n",
      " [-0.09130355]\n",
      " [-0.01965115]]\n",
      "Current iteration=4, loss=35686.27936524563\n",
      "t [[-0.39341682]\n",
      " [-0.09736351]\n",
      " [ 0.03094947]\n",
      " ...\n",
      " [-0.08588487]\n",
      " [-0.11179785]\n",
      " [-0.02450222]]\n",
      "t [[-0.39341682]\n",
      " [-0.09736351]\n",
      " [ 0.03094947]\n",
      " ...\n",
      " [-0.08588487]\n",
      " [-0.11179785]\n",
      " [-0.02450222]]\n",
      "t [[-0.46171792]\n",
      " [-0.11585628]\n",
      " [ 0.03814601]\n",
      " ...\n",
      " [-0.10136516]\n",
      " [-0.1314553 ]\n",
      " [-0.02930459]]\n",
      "t [[-0.46171792]\n",
      " [-0.11585628]\n",
      " [ 0.03814601]\n",
      " ...\n",
      " [-0.10136516]\n",
      " [-0.1314553 ]\n",
      " [-0.02930459]]\n",
      "Current iteration=6, loss=34924.18317409357\n",
      "t [[-0.52702563]\n",
      " [-0.13396479]\n",
      " [ 0.04549512]\n",
      " ...\n",
      " [-0.11633285]\n",
      " [-0.15032064]\n",
      " [-0.03404989]]\n",
      "t [[-0.52702563]\n",
      " [-0.13396479]\n",
      " [ 0.04549512]\n",
      " ...\n",
      " [-0.11633285]\n",
      " [-0.15032064]\n",
      " [-0.03404989]]\n",
      "t [[-0.58951895]\n",
      " [-0.1516768 ]\n",
      " [ 0.05293905]\n",
      " ...\n",
      " [-0.13080965]\n",
      " [-0.16843652]\n",
      " [-0.03873143]]\n",
      "t [[-0.58951895]\n",
      " [-0.1516768 ]\n",
      " [ 0.05293905]\n",
      " ...\n",
      " [-0.13080965]\n",
      " [-0.16843652]\n",
      " [-0.03873143]]\n",
      "Current iteration=8, loss=34286.800249821084\n",
      "t [[-0.64936578]\n",
      " [-0.16898476]\n",
      " [ 0.06042858]\n",
      " ...\n",
      " [-0.14481655]\n",
      " [-0.18584334]\n",
      " [-0.04334394]]\n",
      "t [[-0.64936578]\n",
      " [-0.16898476]\n",
      " [ 0.06042858]\n",
      " ...\n",
      " [-0.14481655]\n",
      " [-0.18584334]\n",
      " [-0.04334394]]\n",
      "t [[-0.70672318]\n",
      " [-0.18588503]\n",
      " [ 0.06792206]\n",
      " ...\n",
      " [-0.1583737 ]\n",
      " [-0.20257933]\n",
      " [-0.04788337]]\n",
      "loss=33750.23584064313\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.08563972]\n",
      " [-0.01862699]\n",
      " [ 0.00609673]\n",
      " ...\n",
      " [ 0.09598633]\n",
      " [-0.07067514]\n",
      " [ 0.07785608]]\n",
      "t [[-0.08563972]\n",
      " [-0.01862699]\n",
      " [ 0.00609673]\n",
      " ...\n",
      " [ 0.09598633]\n",
      " [-0.07067514]\n",
      " [ 0.07785608]]\n",
      "t [[-0.16728973]\n",
      " [-0.03713992]\n",
      " [ 0.01272692]\n",
      " ...\n",
      " [ 0.18677242]\n",
      " [-0.13777031]\n",
      " [ 0.15182043]]\n",
      "t [[-0.16728973]\n",
      " [-0.03713992]\n",
      " [ 0.01272692]\n",
      " ...\n",
      " [ 0.18677242]\n",
      " [-0.13777031]\n",
      " [ 0.15182043]]\n",
      "Current iteration=2, loss=36641.3791879198\n",
      "t [[-0.24517504]\n",
      " [-0.05547707]\n",
      " [ 0.01977941]\n",
      " ...\n",
      " [ 0.27264266]\n",
      " [-0.20146664]\n",
      " [ 0.22209684]]\n",
      "t [[-0.24517504]\n",
      " [-0.05547707]\n",
      " [ 0.01977941]\n",
      " ...\n",
      " [ 0.27264266]\n",
      " [-0.20146664]\n",
      " [ 0.22209684]]\n",
      "t [[-0.31951323]\n",
      " [-0.07358728]\n",
      " [ 0.02715611]\n",
      " ...\n",
      " [ 0.35387511]\n",
      " [-0.26194304]\n",
      " [ 0.28888552]]\n",
      "t [[-0.31951323]\n",
      " [-0.07358728]\n",
      " [ 0.02715611]\n",
      " ...\n",
      " [ 0.35387511]\n",
      " [-0.26194304]\n",
      " [ 0.28888552]]\n",
      "Current iteration=4, loss=35755.41700862664\n",
      "t [[-0.39051273]\n",
      " [-0.09142903]\n",
      " [ 0.0347713 ]\n",
      " ...\n",
      " [ 0.43073884]\n",
      " [-0.31937424]\n",
      " [ 0.35238096]]\n",
      "t [[-0.39051273]\n",
      " [-0.09142903]\n",
      " [ 0.0347713 ]\n",
      " ...\n",
      " [ 0.43073884]\n",
      " [-0.31937424]\n",
      " [ 0.35238096]]\n",
      "t [[-0.45837174]\n",
      " [-0.10896933]\n",
      " [ 0.04255074]\n",
      " ...\n",
      " [ 0.50349193]\n",
      " [-0.37392924]\n",
      " [ 0.41277041]]\n",
      "t [[-0.45837174]\n",
      " [-0.10896933]\n",
      " [ 0.04255074]\n",
      " ...\n",
      " [ 0.50349193]\n",
      " [-0.37392924]\n",
      " [ 0.41277041]]\n",
      "Current iteration=6, loss=35017.775887179116\n",
      "t [[-0.52327759]\n",
      " [-0.12618276]\n",
      " [ 0.05043066]\n",
      " ...\n",
      " [ 0.57238032]\n",
      " [-0.42577035]\n",
      " [ 0.47023288]]\n",
      "t [[-0.52327759]\n",
      " [-0.12618276]\n",
      " [ 0.05043066]\n",
      " ...\n",
      " [ 0.57238032]\n",
      " [-0.42577035]\n",
      " [ 0.47023288]]\n",
      "t [[-0.58540652]\n",
      " [-0.14305038]\n",
      " [ 0.05835666]\n",
      " ...\n",
      " [ 0.63763703]\n",
      " [-0.47505243]\n",
      " [ 0.52493849]]\n",
      "t [[-0.58540652]\n",
      " [-0.14305038]\n",
      " [ 0.05835666]\n",
      " ...\n",
      " [ 0.63763703]\n",
      " [-0.47505243]\n",
      " [ 0.52493849]]\n",
      "Current iteration=8, loss=34400.01135683927\n",
      "t [[-0.64492378]\n",
      " [-0.15955882]\n",
      " [ 0.06628277]\n",
      " ...\n",
      " [ 0.69948191]\n",
      " [-0.52192259]\n",
      " [ 0.57704818]]\n",
      "t [[-0.64492378]\n",
      " [-0.15955882]\n",
      " [ 0.06628277]\n",
      " ...\n",
      " [ 0.69948191]\n",
      " [-0.52192259]\n",
      " [ 0.57704818]]\n",
      "t [[-0.70198385]\n",
      " [-0.17569937]\n",
      " [ 0.07417036]\n",
      " ...\n",
      " [ 0.75812168]\n",
      " [-0.56651996]\n",
      " [ 0.62671366]]\n",
      "loss=33879.295404835124\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.08247743]\n",
      " [-0.01735063]\n",
      " [ 0.01722056]\n",
      " ...\n",
      " [-0.02176756]\n",
      " [-0.02706416]\n",
      " [-0.00568949]]\n",
      "t [[-0.08247743]\n",
      " [-0.01735063]\n",
      " [ 0.01722056]\n",
      " ...\n",
      " [-0.02176756]\n",
      " [-0.02706416]\n",
      " [-0.00568949]]\n",
      "t [[-0.1599202 ]\n",
      " [-0.03250464]\n",
      " [ 0.03382083]\n",
      " ...\n",
      " [-0.04266485]\n",
      " [-0.05283993]\n",
      " [-0.011359  ]]\n",
      "t [[-0.1599202 ]\n",
      " [-0.03250464]\n",
      " [ 0.03382083]\n",
      " ...\n",
      " [-0.04266485]\n",
      " [-0.05283993]\n",
      " [-0.011359  ]]\n",
      "Current iteration=2, loss=36501.867616091615\n",
      "t [[-0.23259506]\n",
      " [-0.04575993]\n",
      " [ 0.04976744]\n",
      " ...\n",
      " [-0.06272911]\n",
      " [-0.07739839]\n",
      " [-0.01698649]]\n",
      "t [[-0.23259506]\n",
      " [-0.04575993]\n",
      " [ 0.04976744]\n",
      " ...\n",
      " [-0.06272911]\n",
      " [-0.07739839]\n",
      " [-0.01698649]]\n",
      "t [[-0.30076913]\n",
      " [-0.05738083]\n",
      " [ 0.06504064]\n",
      " ...\n",
      " [-0.08199802]\n",
      " [-0.10080905]\n",
      " [-0.02255408]]\n",
      "t [[-0.30076913]\n",
      " [-0.05738083]\n",
      " [ 0.06504064]\n",
      " ...\n",
      " [-0.08199802]\n",
      " [-0.10080905]\n",
      " [-0.02255408]]\n",
      "Current iteration=4, loss=35524.00151783872\n",
      "t [[-0.3647055 ]\n",
      " [-0.06759989]\n",
      " [ 0.07963214]\n",
      " ...\n",
      " [-0.10050898]\n",
      " [-0.12313912]\n",
      " [-0.02804751]]\n",
      "t [[-0.3647055 ]\n",
      " [-0.06759989]\n",
      " [ 0.07963214]\n",
      " ...\n",
      " [-0.10050898]\n",
      " [-0.12313912]\n",
      " [-0.02804751]]\n",
      "t [[-0.42465992]\n",
      " [-0.07662032]\n",
      " [ 0.09354301]\n",
      " ...\n",
      " [-0.11829862]\n",
      " [-0.14445287]\n",
      " [-0.03345568]]\n",
      "t [[-0.42465992]\n",
      " [-0.07662032]\n",
      " [ 0.09354301]\n",
      " ...\n",
      " [-0.11829862]\n",
      " [-0.14445287]\n",
      " [-0.03345568]]\n",
      "Current iteration=6, loss=34728.00442310856\n",
      "t [[-0.48087842]\n",
      " [-0.08461872]\n",
      " [ 0.10678175]\n",
      " ...\n",
      " [-0.13540243]\n",
      " [-0.16481132]\n",
      " [-0.03877014]]\n",
      "t [[-0.48087842]\n",
      " [-0.08461872]\n",
      " [ 0.10678175]\n",
      " ...\n",
      " [-0.13540243]\n",
      " [-0.16481132]\n",
      " [-0.03877014]]\n",
      "t [[-0.53359575]\n",
      " [-0.09174793]\n",
      " [ 0.11936262]\n",
      " ...\n",
      " [-0.1518545 ]\n",
      " [-0.18427206]\n",
      " [-0.04398467]]\n",
      "t [[-0.53359575]\n",
      " [-0.09174793]\n",
      " [ 0.11936262]\n",
      " ...\n",
      " [-0.1518545 ]\n",
      " [-0.18427206]\n",
      " [-0.04398467]]\n",
      "Current iteration=8, loss=34075.114438634875\n",
      "t [[-0.58303444]\n",
      " [-0.09813983]\n",
      " [ 0.13130418]\n",
      " ...\n",
      " [-0.16768737]\n",
      " [-0.2028892 ]\n",
      " [-0.04909488]]\n",
      "t [[-0.58303444]\n",
      " [-0.09813983]\n",
      " [ 0.13130418]\n",
      " ...\n",
      " [-0.16768737]\n",
      " [-0.2028892 ]\n",
      " [-0.04909488]]\n",
      "t [[-0.62940432]\n",
      " [-0.10390793]\n",
      " [ 0.14262806]\n",
      " ...\n",
      " [-0.18293196]\n",
      " [-0.22071337]\n",
      " [-0.05409789]]\n",
      "loss=33535.14289477459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.09686291]\n",
      " [-0.02182198]\n",
      " [ 0.00852396]\n",
      " ...\n",
      " [-0.02092408]\n",
      " [-0.02720488]\n",
      " [-0.00564677]]\n",
      "t [[-0.09686291]\n",
      " [-0.02182198]\n",
      " [ 0.00852396]\n",
      " ...\n",
      " [-0.02092408]\n",
      " [-0.02720488]\n",
      " [-0.00564677]]\n",
      "t [[-0.18861939]\n",
      " [-0.04342746]\n",
      " [ 0.01754184]\n",
      " ...\n",
      " [-0.04100701]\n",
      " [-0.05311041]\n",
      " [-0.01129879]]\n",
      "t [[-0.18861939]\n",
      " [-0.04342746]\n",
      " [ 0.01754184]\n",
      " ...\n",
      " [-0.04100701]\n",
      " [-0.05311041]\n",
      " [-0.01129879]]\n",
      "Current iteration=2, loss=36491.30810384436\n",
      "t [[-0.27559502]\n",
      " [-0.06473742]\n",
      " [ 0.02691171]\n",
      " ...\n",
      " [-0.0602868 ]\n",
      " [-0.07778863]\n",
      " [-0.01692956]]\n",
      "t [[-0.27559502]\n",
      " [-0.06473742]\n",
      " [ 0.02691171]\n",
      " ...\n",
      " [-0.0602868 ]\n",
      " [-0.07778863]\n",
      " [-0.01692956]]\n",
      "t [[-0.35810279]\n",
      " [-0.08568897]\n",
      " [ 0.03651198]\n",
      " ...\n",
      " [-0.07880148]\n",
      " [-0.10130999]\n",
      " [-0.02251731]]\n",
      "t [[-0.35810279]\n",
      " [-0.08568897]\n",
      " [ 0.03651198]\n",
      " ...\n",
      " [-0.07880148]\n",
      " [-0.10130999]\n",
      " [-0.02251731]]\n",
      "Current iteration=4, loss=35506.011768445285\n",
      "t [[-0.43644015]\n",
      " [-0.1062335 ]\n",
      " [ 0.04623982]\n",
      " ...\n",
      " [-0.0965885 ]\n",
      " [-0.12374255]\n",
      " [-0.02804451]]\n",
      "t [[-0.43644015]\n",
      " [-0.1062335 ]\n",
      " [ 0.04623982]\n",
      " ...\n",
      " [-0.0965885 ]\n",
      " [-0.12374255]\n",
      " [-0.02804451]]\n",
      "t [[-0.5108873 ]\n",
      " [-0.12633484]\n",
      " [ 0.05600934]\n",
      " ...\n",
      " [-0.11368425]\n",
      " [-0.14515141]\n",
      " [-0.03349724]]\n",
      "t [[-0.5108873 ]\n",
      " [-0.12633484]\n",
      " [ 0.05600934]\n",
      " ...\n",
      " [-0.11368425]\n",
      " [-0.14515141]\n",
      " [-0.03349724]]\n",
      "Current iteration=6, loss=34704.681727535564\n",
      "t [[-0.58170642]\n",
      " [-0.14596741]\n",
      " [ 0.06574966]\n",
      " ...\n",
      " [-0.13012379]\n",
      " [-0.16559833]\n",
      " [-0.0388647 ]]\n",
      "t [[-0.58170642]\n",
      " [-0.14596741]\n",
      " [ 0.06574966]\n",
      " ...\n",
      " [-0.13012379]\n",
      " [-0.16559833]\n",
      " [-0.0388647 ]]\n",
      "t [[-0.64914154]\n",
      " [-0.16511445]\n",
      " [ 0.07540296]\n",
      " ...\n",
      " [-0.14594059]\n",
      " [-0.1851416 ]\n",
      " [-0.04413869]]\n",
      "t [[-0.64914154]\n",
      " [-0.16511445]\n",
      " [ 0.07540296]\n",
      " ...\n",
      " [-0.14594059]\n",
      " [-0.1851416 ]\n",
      " [-0.04413869]]\n",
      "Current iteration=8, loss=34047.851206557025\n",
      "t [[-0.71341896]\n",
      " [-0.18376647]\n",
      " [ 0.08492269]\n",
      " ...\n",
      " [-0.16116652]\n",
      " [-0.20383595]\n",
      " [-0.04931319]]\n",
      "t [[-0.71341896]\n",
      " [-0.18376647]\n",
      " [ 0.08492269]\n",
      " ...\n",
      " [-0.16116652]\n",
      " [-0.20383595]\n",
      " [-0.04931319]]\n",
      "t [[-0.77474798]\n",
      " [-0.20191986]\n",
      " [ 0.09427195]\n",
      " ...\n",
      " [-0.17583172]\n",
      " [-0.22173258]\n",
      " [-0.05438393]]\n",
      "loss=33504.8572440479\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.09711065]\n",
      " [-0.02250706]\n",
      " [ 0.00584923]\n",
      " ...\n",
      " [-0.02068278]\n",
      " [-0.02736467]\n",
      " [-0.00553475]]\n",
      "t [[-0.09711065]\n",
      " [-0.02250706]\n",
      " [ 0.00584923]\n",
      " ...\n",
      " [-0.02068278]\n",
      " [-0.02736467]\n",
      " [-0.00553475]]\n",
      "t [[-0.1891067 ]\n",
      " [-0.04473245]\n",
      " [ 0.0124622 ]\n",
      " ...\n",
      " [-0.04057243]\n",
      " [-0.05342275]\n",
      " [-0.01107794]]\n",
      "t [[-0.1891067 ]\n",
      " [-0.04473245]\n",
      " [ 0.0124622 ]\n",
      " ...\n",
      " [-0.04057243]\n",
      " [-0.05342275]\n",
      " [-0.01107794]]\n",
      "Current iteration=2, loss=36475.3327369164\n",
      "t [[-0.27631246]\n",
      " [-0.06660678]\n",
      " [ 0.01967542]\n",
      " ...\n",
      " [-0.05970159]\n",
      " [-0.07824672]\n",
      " [-0.01660442]]\n",
      "t [[-0.27631246]\n",
      " [-0.06660678]\n",
      " [ 0.01967542]\n",
      " ...\n",
      " [-0.05970159]\n",
      " [-0.07824672]\n",
      " [-0.01660442]]\n",
      "t [[-0.35904026]\n",
      " [-0.08807525]\n",
      " [ 0.02734637]\n",
      " ...\n",
      " [-0.07810333]\n",
      " [-0.10190751]\n",
      " [-0.02209344]]\n",
      "t [[-0.35904026]\n",
      " [-0.08807525]\n",
      " [ 0.02734637]\n",
      " ...\n",
      " [-0.07810333]\n",
      " [-0.10190751]\n",
      " [-0.02209344]]\n",
      "Current iteration=4, loss=35477.23601658298\n",
      "t [[-0.43758733]\n",
      " [-0.1090961 ]\n",
      " [ 0.03535233]\n",
      " ...\n",
      " [-0.09581061]\n",
      " [-0.12447364]\n",
      " [-0.02752816]]\n",
      "t [[-0.43758733]\n",
      " [-0.1090961 ]\n",
      " [ 0.03535233]\n",
      " ...\n",
      " [-0.09581061]\n",
      " [-0.12447364]\n",
      " [-0.02752816]]\n",
      "t [[-0.51223395]\n",
      " [-0.12963893]\n",
      " [ 0.0435887 ]\n",
      " ...\n",
      " [-0.11285577]\n",
      " [-0.14601064]\n",
      " [-0.03289512]]\n",
      "t [[-0.51223395]\n",
      " [-0.12963893]\n",
      " [ 0.0435887 ]\n",
      " ...\n",
      " [-0.11285577]\n",
      " [-0.14601064]\n",
      " [-0.03289512]]\n",
      "Current iteration=6, loss=34665.60892498748\n",
      "t [[-0.58324256]\n",
      " [-0.14968301]\n",
      " [ 0.05196722]\n",
      " ...\n",
      " [-0.12927023]\n",
      " [-0.16658072]\n",
      " [-0.03818378]]\n",
      "t [[-0.58324256]\n",
      " [-0.14968301]\n",
      " [ 0.05196722]\n",
      " ...\n",
      " [-0.12927023]\n",
      " [-0.16658072]\n",
      " [-0.03818378]]\n",
      "t [[-0.6508576 ]\n",
      " [-0.1692157 ]\n",
      " [ 0.06041407]\n",
      " ...\n",
      " [-0.14508424]\n",
      " [-0.18624253]\n",
      " [-0.04338604]]\n",
      "t [[-0.6508576 ]\n",
      " [-0.1692157 ]\n",
      " [ 0.06041407]\n",
      " ...\n",
      " [-0.14508424]\n",
      " [-0.18624253]\n",
      " [-0.04338604]]\n",
      "Current iteration=8, loss=34000.43809417226\n",
      "t [[-0.71530584]\n",
      " [-0.18823099]\n",
      " [ 0.06886804]\n",
      " ...\n",
      " [-0.16032677]\n",
      " [-0.20505116]\n",
      " [-0.04849584]]\n",
      "t [[-0.71530584]\n",
      " [-0.18823099]\n",
      " [ 0.06886804]\n",
      " ...\n",
      " [-0.16032677]\n",
      " [-0.20505116]\n",
      " [-0.04849584]]\n",
      "t [[-0.77679704]\n",
      " [-0.20672821]\n",
      " [ 0.07727885]\n",
      " ...\n",
      " [-0.17502541]\n",
      " [-0.22305815]\n",
      " [-0.05350881]]\n",
      "loss=33450.63422207633\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.09634469]\n",
      " [-0.02095536]\n",
      " [ 0.00685882]\n",
      " ...\n",
      " [ 0.10798462]\n",
      " [-0.07950953]\n",
      " [ 0.08758809]]\n",
      "t [[-0.09634469]\n",
      " [-0.02095536]\n",
      " [ 0.00685882]\n",
      " ...\n",
      " [ 0.10798462]\n",
      " [-0.07950953]\n",
      " [ 0.08758809]]\n",
      "t [[-0.18764049]\n",
      " [-0.0417663 ]\n",
      " [ 0.01439269]\n",
      " ...\n",
      " [ 0.20938843]\n",
      " [-0.15448865]\n",
      " [ 0.17025127]]\n",
      "t [[-0.18764049]\n",
      " [-0.0417663 ]\n",
      " [ 0.01439269]\n",
      " ...\n",
      " [ 0.20938843]\n",
      " [-0.15448865]\n",
      " [ 0.17025127]]\n",
      "Current iteration=2, loss=36518.18700894875\n",
      "t [[-0.27420924]\n",
      " [-0.06234486]\n",
      " [ 0.02244311]\n",
      " ...\n",
      " [ 0.30461812]\n",
      " [-0.2251964 ]\n",
      " [ 0.24828107]]\n",
      "t [[-0.27420924]\n",
      " [-0.06234486]\n",
      " [ 0.02244311]\n",
      " ...\n",
      " [ 0.30461812]\n",
      " [-0.2251964 ]\n",
      " [ 0.24828107]]\n",
      "t [[-0.35636007]\n",
      " [-0.08262015]\n",
      " [ 0.0308728 ]\n",
      " ...\n",
      " [ 0.39406933]\n",
      " [-0.29188773]\n",
      " [ 0.32196254]]\n",
      "t [[-0.35636007]\n",
      " [-0.08262015]\n",
      " [ 0.0308728 ]\n",
      " ...\n",
      " [ 0.39406933]\n",
      " [-0.29188773]\n",
      " [ 0.32196254]]\n",
      "Current iteration=4, loss=35553.222856810484\n",
      "t [[-0.43438657]\n",
      " [-0.10253658]\n",
      " [ 0.03956425]\n",
      " ...\n",
      " [ 0.47812201]\n",
      " [-0.35480998]\n",
      " [ 0.39157068]]\n",
      "t [[-0.43438657]\n",
      " [-0.10253658]\n",
      " [ 0.03956425]\n",
      " ...\n",
      " [ 0.47812201]\n",
      " [-0.35480998]\n",
      " [ 0.39157068]]\n",
      "t [[-0.50856518]\n",
      " [-0.12205188]\n",
      " [ 0.04841802]\n",
      " ...\n",
      " [ 0.55713743]\n",
      " [-0.41420054]\n",
      " [ 0.45736796]]\n",
      "t [[-0.50856518]\n",
      " [-0.12205188]\n",
      " [ 0.04841802]\n",
      " ...\n",
      " [ 0.55713743]\n",
      " [-0.41420054]\n",
      " [ 0.45736796]]\n",
      "Current iteration=6, loss=34767.30984178567\n",
      "t [[-0.57915443]\n",
      " [-0.14113516]\n",
      " [ 0.05735077]\n",
      " ...\n",
      " [ 0.63145639]\n",
      " [-0.47028523]\n",
      " [ 0.51960286]]\n",
      "t [[-0.57915443]\n",
      " [-0.14113516]\n",
      " [ 0.05735077]\n",
      " ...\n",
      " [ 0.63145639]\n",
      " [-0.47028523]\n",
      " [ 0.51960286]]\n",
      "t [[-0.64639491]\n",
      " [-0.15976511]\n",
      " [ 0.06629335]\n",
      " ...\n",
      " [ 0.70139846]\n",
      " [-0.52327747]\n",
      " [ 0.57850914]]\n",
      "t [[-0.64639491]\n",
      " [-0.15976511]\n",
      " [ 0.06629335]\n",
      " ...\n",
      " [ 0.70139846]\n",
      " [-0.52327747]\n",
      " [ 0.57850914]]\n",
      "Current iteration=8, loss=34122.24415024564\n",
      "t [[-0.71050969]\n",
      " [-0.17792832]\n",
      " [ 0.07518894]\n",
      " ...\n",
      " [ 0.76726192]\n",
      " [-0.57337782]\n",
      " [ 0.6343056 ]]\n",
      "t [[-0.71050969]\n",
      " [-0.17792832]\n",
      " [ 0.07518894]\n",
      " ...\n",
      " [ 0.76726192]\n",
      " [-0.57337782]\n",
      " [ 0.6343056 ]]\n",
      "t [[-0.77170502]\n",
      " [-0.19561783]\n",
      " [ 0.08399136]\n",
      " ...\n",
      " [ 0.82932419]\n",
      " [-0.62077409]\n",
      " [ 0.68719629]]\n",
      "loss=33588.310700388174\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.09164159]\n",
      " [-0.01927848]\n",
      " [ 0.01913395]\n",
      " ...\n",
      " [-0.02418618]\n",
      " [-0.03007129]\n",
      " [-0.00632166]]\n",
      "t [[-0.09164159]\n",
      " [-0.01927848]\n",
      " [ 0.01913395]\n",
      " ...\n",
      " [-0.02418618]\n",
      " [-0.03007129]\n",
      " [-0.00632166]]\n",
      "t [[-0.17706823]\n",
      " [-0.03584542]\n",
      " [ 0.03750215]\n",
      " ...\n",
      " [-0.0472981 ]\n",
      " [-0.05855216]\n",
      " [-0.01261865]]\n",
      "t [[-0.17706823]\n",
      " [-0.03584542]\n",
      " [ 0.03750215]\n",
      " ...\n",
      " [-0.0472981 ]\n",
      " [-0.05855216]\n",
      " [-0.01261865]]\n",
      "Current iteration=2, loss=36379.2522861621\n",
      "t [[-0.25664743]\n",
      " [-0.0501101 ]\n",
      " [ 0.05505894]\n",
      " ...\n",
      " [-0.06938721]\n",
      " [-0.08554053]\n",
      " [-0.01886079]]\n",
      "t [[-0.25664743]\n",
      " [-0.0501101 ]\n",
      " [ 0.05505894]\n",
      " ...\n",
      " [-0.06938721]\n",
      " [-0.08554053]\n",
      " [-0.01886079]]\n",
      "t [[-0.33074642]\n",
      " [-0.06242994]\n",
      " [ 0.07177951]\n",
      " ...\n",
      " [-0.09050544]\n",
      " [-0.11113176]\n",
      " [-0.02502423]]\n",
      "t [[-0.33074642]\n",
      " [-0.06242994]\n",
      " [ 0.07177951]\n",
      " ...\n",
      " [-0.09050544]\n",
      " [-0.11113176]\n",
      " [-0.02502423]]\n",
      "Current iteration=4, loss=35327.4048816289\n",
      "t [[-0.39972486]\n",
      " [-0.07311392]\n",
      " [ 0.08765611]\n",
      " ...\n",
      " [-0.11070404]\n",
      " [-0.13541727]\n",
      " [-0.03109058]]\n",
      "t [[-0.39972486]\n",
      " [-0.07311392]\n",
      " [ 0.08765611]\n",
      " ...\n",
      " [-0.11070404]\n",
      " [-0.13541727]\n",
      " [-0.03109058]]\n",
      "t [[-0.46392977]\n",
      " [-0.08242703]\n",
      " [ 0.10269446]\n",
      " ...\n",
      " [-0.13003283]\n",
      " [-0.15848383]\n",
      " [-0.03704605]]\n",
      "t [[-0.46392977]\n",
      " [-0.08242703]\n",
      " [ 0.10269446]\n",
      " ...\n",
      " [-0.13003283]\n",
      " [-0.15848383]\n",
      " [-0.03704605]]\n",
      "Current iteration=6, loss=34489.69493983634\n",
      "t [[-0.52369195]\n",
      " [-0.09059522]\n",
      " [ 0.11691059]\n",
      " ...\n",
      " [-0.14853965]\n",
      " [-0.18041298]\n",
      " [-0.04288065]]\n",
      "t [[-0.52369195]\n",
      " [-0.09059522]\n",
      " [ 0.11691059]\n",
      " ...\n",
      " [-0.14853965]\n",
      " [-0.18041298]\n",
      " [-0.04288065]]\n",
      "t [[-0.57932398]\n",
      " [-0.09781023]\n",
      " [ 0.13032811]\n",
      " ...\n",
      " [-0.16627006]\n",
      " [-0.20128093]\n",
      " [-0.04858746]]\n",
      "t [[-0.57932398]\n",
      " [-0.09781023]\n",
      " [ 0.13032811]\n",
      " ...\n",
      " [-0.16627006]\n",
      " [-0.20128093]\n",
      " [-0.04858746]]\n",
      "Current iteration=8, loss=33815.998922317\n",
      "t [[-0.63111908]\n",
      " [-0.10423432]\n",
      " [ 0.14297595]\n",
      " ...\n",
      " [-0.18326714]\n",
      " [-0.22115856]\n",
      " [-0.05416203]]\n",
      "t [[-0.63111908]\n",
      " [-0.10423432]\n",
      " [ 0.14297595]\n",
      " ...\n",
      " [-0.18326714]\n",
      " [-0.22115856]\n",
      " [-0.05416203]]\n",
      "t [[-0.67935088]\n",
      " [-0.11000443]\n",
      " [ 0.15488654]\n",
      " ...\n",
      " [-0.19957144]\n",
      " [-0.24011151]\n",
      " [-0.05960186]]\n",
      "loss=33268.48819180844\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.10762545]\n",
      " [-0.02424665]\n",
      " [ 0.00947106]\n",
      " ...\n",
      " [-0.02324898]\n",
      " [-0.03022764]\n",
      " [-0.00627418]]\n",
      "t [[-0.10762545]\n",
      " [-0.02424665]\n",
      " [ 0.00947106]\n",
      " ...\n",
      " [-0.02324898]\n",
      " [-0.03022764]\n",
      " [-0.00627418]]\n",
      "t [[-0.20894752]\n",
      " [-0.04822593]\n",
      " [ 0.01955179]\n",
      " ...\n",
      " [-0.04545964]\n",
      " [-0.05885134]\n",
      " [-0.01255486]]\n",
      "t [[-0.20894752]\n",
      " [-0.04822593]\n",
      " [ 0.01955179]\n",
      " ...\n",
      " [-0.04545964]\n",
      " [-0.05885134]\n",
      " [-0.01255486]]\n",
      "Current iteration=2, loss=36367.68707097795\n",
      "t [[-0.30441474]\n",
      " [-0.07182943]\n",
      " [ 0.03004732]\n",
      " ...\n",
      " [-0.06668447]\n",
      " [-0.08597035]\n",
      " [-0.01880569]]\n",
      "t [[-0.30441474]\n",
      " [-0.07182943]\n",
      " [ 0.03004732]\n",
      " ...\n",
      " [-0.06668447]\n",
      " [-0.08597035]\n",
      " [-0.01880569]]\n",
      "t [[-0.39445528]\n",
      " [-0.09497345]\n",
      " [ 0.04079409]\n",
      " ...\n",
      " [-0.08697582]\n",
      " [-0.11168125]\n",
      " [-0.02499761]]\n",
      "t [[-0.39445528]\n",
      " [-0.09497345]\n",
      " [ 0.04079409]\n",
      " ...\n",
      " [-0.08697582]\n",
      " [-0.11168125]\n",
      " [-0.02499761]]\n",
      "Current iteration=4, loss=35308.015601059946\n",
      "t [[-0.47947238]\n",
      " [-0.11759585]\n",
      " [ 0.05165699]\n",
      " ...\n",
      " [-0.10638487]\n",
      " [-0.1360767 ]\n",
      " [-0.03110789]]\n",
      "t [[-0.47947238]\n",
      " [-0.11759585]\n",
      " [ 0.05165699]\n",
      " ...\n",
      " [-0.10638487]\n",
      " [-0.1360767 ]\n",
      " [-0.03110789]]\n",
      "t [[-0.55984201]\n",
      " [-0.13965281]\n",
      " [ 0.06252608]\n",
      " ...\n",
      " [-0.12496102]\n",
      " [-0.15924452]\n",
      " [-0.03711914]]\n",
      "t [[-0.55984201]\n",
      " [-0.13965281]\n",
      " [ 0.06252608]\n",
      " ...\n",
      " [-0.12496102]\n",
      " [-0.15924452]\n",
      " [-0.03711914]]\n",
      "Current iteration=6, loss=34464.87050729808\n",
      "t [[-0.63591209]\n",
      " [-0.16111558]\n",
      " [ 0.07331323]\n",
      " ...\n",
      " [-0.1427514 ]\n",
      " [-0.18126725]\n",
      " [-0.04301839]]\n",
      "t [[-0.63591209]\n",
      " [-0.16111558]\n",
      " [ 0.07331323]\n",
      " ...\n",
      " [-0.1427514 ]\n",
      " [-0.18126725]\n",
      " [-0.04301839]]\n",
      "t [[-0.70800287]\n",
      " [-0.18196769]\n",
      " [ 0.08394888]\n",
      " ...\n",
      " [-0.15980065]\n",
      " [-0.202222  ]\n",
      " [-0.0487963 ]]\n",
      "t [[-0.70800287]\n",
      " [-0.18196769]\n",
      " [ 0.08394888]\n",
      " ...\n",
      " [-0.15980065]\n",
      " [-0.202222  ]\n",
      " [-0.0487963 ]]\n",
      "Current iteration=8, loss=33787.24822556776\n",
      "t [[-0.77640804]\n",
      " [-0.20220235]\n",
      " [ 0.09437908]\n",
      " ...\n",
      " [-0.17615078]\n",
      " [-0.22218043]\n",
      " [-0.05444646]]\n",
      "t [[-0.77640804]\n",
      " [-0.20220235]\n",
      " [ 0.09437908]\n",
      " ...\n",
      " [-0.17615078]\n",
      " [-0.22218043]\n",
      " [-0.05444646]]\n",
      "t [[-0.84139623]\n",
      " [-0.22182035]\n",
      " [ 0.10456284]\n",
      " ...\n",
      " [-0.19184122]\n",
      " [-0.24120891]\n",
      " [-0.05996475]]\n",
      "loss=33236.75633032733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.10790072]\n",
      " [-0.02500784]\n",
      " [ 0.00649914]\n",
      " ...\n",
      " [-0.02298086]\n",
      " [-0.03040519]\n",
      " [-0.00614973]]\n",
      "t [[-0.10790072]\n",
      " [-0.02500784]\n",
      " [ 0.00649914]\n",
      " ...\n",
      " [-0.02298086]\n",
      " [-0.03040519]\n",
      " [-0.00614973]]\n",
      "t [[-0.20948798]\n",
      " [-0.04966788]\n",
      " [ 0.01394101]\n",
      " ...\n",
      " [-0.0449827 ]\n",
      " [-0.0591975 ]\n",
      " [-0.01230986]]\n",
      "t [[-0.20948798]\n",
      " [-0.04966788]\n",
      " [ 0.01394101]\n",
      " ...\n",
      " [-0.0449827 ]\n",
      " [-0.0591975 ]\n",
      " [-0.01230986]]\n",
      "Current iteration=2, loss=36350.09684652132\n",
      "t [[-0.30520862]\n",
      " [-0.07388487]\n",
      " [ 0.02210102]\n",
      " ...\n",
      " [-0.06605058]\n",
      " [-0.08647679]\n",
      " [-0.01844595]]\n",
      "t [[-0.30520862]\n",
      " [-0.07388487]\n",
      " [ 0.02210102]\n",
      " ...\n",
      " [-0.06605058]\n",
      " [-0.08647679]\n",
      " [-0.01844595]]\n",
      "t [[-0.39549002]\n",
      " [-0.09758602]\n",
      " [ 0.03078701]\n",
      " ...\n",
      " [-0.08623013]\n",
      " [-0.11234033]\n",
      " [-0.02453025]]\n",
      "t [[-0.39549002]\n",
      " [-0.09758602]\n",
      " [ 0.03078701]\n",
      " ...\n",
      " [-0.08623013]\n",
      " [-0.11234033]\n",
      " [-0.02453025]]\n",
      "Current iteration=4, loss=35276.679719728265\n",
      "t [[-0.48073523]\n",
      " [-0.12071819]\n",
      " [ 0.03983687]\n",
      " ...\n",
      " [-0.10556653]\n",
      " [-0.13688138]\n",
      " [-0.0305409 ]]\n",
      "t [[-0.48073523]\n",
      " [-0.12071819]\n",
      " [ 0.03983687]\n",
      " ...\n",
      " [-0.10556653]\n",
      " [-0.13688138]\n",
      " [-0.0305409 ]]\n",
      "t [[-0.56132046]\n",
      " [-0.14324497]\n",
      " [ 0.04911564]\n",
      " ...\n",
      " [-0.12410382]\n",
      " [-0.16018836]\n",
      " [-0.03646101]]\n",
      "t [[-0.56132046]\n",
      " [-0.14324497]\n",
      " [ 0.04911564]\n",
      " ...\n",
      " [-0.12410382]\n",
      " [-0.16018836]\n",
      " [-0.03646101]]\n",
      "Current iteration=6, loss=34422.73337478017\n",
      "t [[-0.63759409]\n",
      " [-0.16514379]\n",
      " [ 0.05851221]\n",
      " ...\n",
      " [-0.14188442]\n",
      " [-0.18234439]\n",
      " [-0.04227785]]\n",
      "t [[-0.63759409]\n",
      " [-0.16514379]\n",
      " [ 0.05851221]\n",
      " ...\n",
      " [-0.14188442]\n",
      " [-0.18234439]\n",
      " [-0.04227785]]\n",
      "t [[-0.70987698]\n",
      " [-0.18640322]\n",
      " [ 0.06793616]\n",
      " ...\n",
      " [-0.1589488 ]\n",
      " [-0.20342707]\n",
      " [-0.04798212]]\n",
      "t [[-0.70987698]\n",
      " [-0.18640322]\n",
      " [ 0.06793616]\n",
      " ...\n",
      " [-0.1589488 ]\n",
      " [-0.20342707]\n",
      " [-0.04798212]]\n",
      "Current iteration=8, loss=33736.54619912423\n",
      "t [[-0.77846349]\n",
      " [-0.20702073]\n",
      " [ 0.07731468]\n",
      " ...\n",
      " [-0.17533536]\n",
      " [-0.2235085 ]\n",
      " [-0.05356725]]\n",
      "t [[-0.77846349]\n",
      " [-0.20702073]\n",
      " [ 0.07731468]\n",
      " ...\n",
      " [-0.17533536]\n",
      " [-0.2235085 ]\n",
      " [-0.05356725]]\n",
      "t [[-0.84362293]\n",
      " [-0.22700061]\n",
      " [ 0.08658988]\n",
      " ...\n",
      " [-0.19108037]\n",
      " [-0.24265549]\n",
      " [-0.05902892]]\n",
      "loss=33179.19347028269\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.10704965]\n",
      " [-0.02328373]\n",
      " [ 0.00762091]\n",
      " ...\n",
      " [ 0.11998292]\n",
      " [-0.08834392]\n",
      " [ 0.0973201 ]]\n",
      "t [[-0.10704965]\n",
      " [-0.02328373]\n",
      " [ 0.00762091]\n",
      " ...\n",
      " [ 0.11998292]\n",
      " [-0.08834392]\n",
      " [ 0.0973201 ]]\n",
      "t [[-0.20786694]\n",
      " [-0.04638909]\n",
      " [ 0.01607506]\n",
      " ...\n",
      " [ 0.23184238]\n",
      " [-0.1710954 ]\n",
      " [ 0.18856084]]\n",
      "t [[-0.20786694]\n",
      " [-0.04638909]\n",
      " [ 0.01607506]\n",
      " ...\n",
      " [ 0.23184238]\n",
      " [-0.1710954 ]\n",
      " [ 0.18856084]]\n",
      "Current iteration=2, loss=36397.21604960022\n",
      "t [[-0.30289524]\n",
      " [-0.06919528]\n",
      " [ 0.02514475]\n",
      " ...\n",
      " [ 0.33613865]\n",
      " [-0.24861135]\n",
      " [ 0.27412391]]\n",
      "t [[-0.30289524]\n",
      " [-0.06919528]\n",
      " [ 0.02514475]\n",
      " ...\n",
      " [ 0.33613865]\n",
      " [-0.24861135]\n",
      " [ 0.27412391]]\n",
      "t [[-0.39255741]\n",
      " [-0.09160784]\n",
      " [ 0.03464501]\n",
      " ...\n",
      " [ 0.43341367]\n",
      " [-0.32124148]\n",
      " [ 0.35440011]]\n",
      "t [[-0.39255741]\n",
      " [-0.09160784]\n",
      " [ 0.03464501]\n",
      " ...\n",
      " [ 0.43341367]\n",
      " [-0.32124148]\n",
      " [ 0.35440011]]\n",
      "Current iteration=4, loss=35359.17634168137\n",
      "t [[-0.47725139]\n",
      " [-0.11355534]\n",
      " [ 0.04442092]\n",
      " ...\n",
      " [ 0.52418379]\n",
      " [-0.38932278]\n",
      " [ 0.42976353]]\n",
      "t [[-0.47725139]\n",
      " [-0.11355534]\n",
      " [ 0.04442092]\n",
      " ...\n",
      " [ 0.52418379]\n",
      " [-0.38932278]\n",
      " [ 0.42976353]]\n",
      "t [[-0.55734803]\n",
      " [-0.13498598]\n",
      " [ 0.05434447]\n",
      " ...\n",
      " [ 0.60893532]\n",
      " [-0.45317587]\n",
      " [ 0.500568  ]]\n",
      "t [[-0.55734803]\n",
      " [-0.13498598]\n",
      " [ 0.05434447]\n",
      " ...\n",
      " [ 0.60893532]\n",
      " [-0.45317587]\n",
      " [ 0.500568  ]]\n",
      "Current iteration=6, loss=34531.93767168955\n",
      "t [[-0.63319036]\n",
      " [-0.15586428]\n",
      " [ 0.06431119]\n",
      " ...\n",
      " [ 0.68812231]\n",
      " [-0.51310286]\n",
      " [ 0.56714511]]\n",
      "t [[-0.63319036]\n",
      " [-0.15586428]\n",
      " [ 0.06431119]\n",
      " ...\n",
      " [ 0.68812231]\n",
      " [-0.51310286]\n",
      " [ 0.56714511]]\n",
      "t [[-0.70509408]\n",
      " [-0.17616807]\n",
      " [ 0.07423684]\n",
      " ...\n",
      " [ 0.76216596]\n",
      " [-0.56938628]\n",
      " [ 0.62980351]]\n",
      "t [[-0.70509408]\n",
      " [-0.17616807]\n",
      " [ 0.07423684]\n",
      " ...\n",
      " [ 0.76216596]\n",
      " [-0.56938628]\n",
      " [ 0.62980351]]\n",
      "Current iteration=8, loss=33866.116888466684\n",
      "t [[-0.7733486 ]\n",
      " [-0.19588577]\n",
      " [ 0.08405438]\n",
      " ...\n",
      " [ 0.83145512]\n",
      " [-0.62228893]\n",
      " [ 0.688829  ]]\n",
      "t [[-0.7733486 ]\n",
      " [-0.19588577]\n",
      " [ 0.08405438]\n",
      " ...\n",
      " [ 0.83145512]\n",
      " [-0.62228893]\n",
      " [ 0.688829  ]]\n",
      "t [[-0.8382187 ]\n",
      " [-0.21501416]\n",
      " [ 0.09371119]\n",
      " ...\n",
      " [ 0.89634759]\n",
      " [-0.67205426]\n",
      " [ 0.7444853 ]]\n",
      "loss=33324.4903863175\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.10080575]\n",
      " [-0.02120633]\n",
      " [ 0.02104735]\n",
      " ...\n",
      " [-0.0266048 ]\n",
      " [-0.03307842]\n",
      " [-0.00695383]]\n",
      "t [[-0.10080575]\n",
      " [-0.02120633]\n",
      " [ 0.02104735]\n",
      " ...\n",
      " [-0.0266048 ]\n",
      " [-0.03307842]\n",
      " [-0.00695383]]\n",
      "t [[-0.19409231]\n",
      " [-0.03913215]\n",
      " [ 0.04116816]\n",
      " ...\n",
      " [-0.05190994]\n",
      " [-0.06423267]\n",
      " [-0.01387781]]\n",
      "t [[-0.19409231]\n",
      " [-0.03913215]\n",
      " [ 0.04116816]\n",
      " ...\n",
      " [-0.05190994]\n",
      " [-0.06423267]\n",
      " [-0.01387781]]\n",
      "Current iteration=2, loss=36258.859250224574\n",
      "t [[-0.28035093]\n",
      " [-0.054323  ]\n",
      " [ 0.06030188]\n",
      " ...\n",
      " [-0.07598439]\n",
      " [-0.09359367]\n",
      " [-0.02073183]]\n",
      "t [[-0.28035093]\n",
      " [-0.054323  ]\n",
      " [ 0.06030188]\n",
      " ...\n",
      " [-0.07598439]\n",
      " [-0.09359367]\n",
      " [-0.02073183]]\n",
      "t [[-0.36007113]\n",
      " [-0.0672475 ]\n",
      " [ 0.07841853]\n",
      " ...\n",
      " [-0.09889749]\n",
      " [-0.12128816]\n",
      " [-0.02748507]]\n",
      "t [[-0.36007113]\n",
      " [-0.0672475 ]\n",
      " [ 0.07841853]\n",
      " ...\n",
      " [-0.09889749]\n",
      " [-0.12128816]\n",
      " [-0.02748507]]\n",
      "Current iteration=4, loss=35138.74007715169\n",
      "t [[-0.4337295 ]\n",
      " [-0.07830352]\n",
      " [ 0.09551245]\n",
      " ...\n",
      " [-0.12071731]\n",
      " [-0.1474368 ]\n",
      " [-0.03411456]]\n",
      "t [[-0.4337295 ]\n",
      " [-0.07830352]\n",
      " [ 0.09551245]\n",
      " ...\n",
      " [-0.12071731]\n",
      " [-0.1474368 ]\n",
      " [-0.03411456]]\n",
      "t [[-0.50178208]\n",
      " [-0.08782579]\n",
      " [ 0.11159659]\n",
      " ...\n",
      " [-0.14150946]\n",
      " [-0.17215299]\n",
      " [-0.0406038 ]]\n",
      "t [[-0.50178208]\n",
      " [-0.08782579]\n",
      " [ 0.11159659]\n",
      " ...\n",
      " [-0.14150946]\n",
      " [-0.17215299]\n",
      " [-0.0406038 ]]\n",
      "Current iteration=6, loss=34265.690091205855\n",
      "t [[-0.56465954]\n",
      " [-0.09609416]\n",
      " [ 0.1266975 ]\n",
      " ...\n",
      " [-0.16133638]\n",
      " [-0.1955423 ]\n",
      " [-0.04694144]]\n",
      "t [[-0.56465954]\n",
      " [-0.09609416]\n",
      " [ 0.1266975 ]\n",
      " ...\n",
      " [-0.16133638]\n",
      " [-0.1955423 ]\n",
      " [-0.04694144]]\n",
      "t [[-0.62276464]\n",
      " [-0.10334147]\n",
      " [ 0.14085119]\n",
      " ...\n",
      " [-0.18025696]\n",
      " [-0.21770238]\n",
      " [-0.05312023]]\n",
      "t [[-0.62276464]\n",
      " [-0.10334147]\n",
      " [ 0.14085119]\n",
      " ...\n",
      " [-0.18025696]\n",
      " [-0.21770238]\n",
      " [-0.05312023]]\n",
      "Current iteration=8, loss=33576.87745247829\n",
      "t [[-0.67647129]\n",
      " [-0.10976084]\n",
      " [ 0.15409982]\n",
      " ...\n",
      " [-0.1983264 ]\n",
      " [-0.23872309]\n",
      " [-0.05913606]]\n",
      "t [[-0.67647129]\n",
      " [-0.10976084]\n",
      " [ 0.15409982]\n",
      " ...\n",
      " [-0.1983264 ]\n",
      " [-0.23872309]\n",
      " [-0.05913606]]\n",
      "t [[-0.72612469]\n",
      " [-0.11551215]\n",
      " [ 0.16648915]\n",
      " ...\n",
      " [-0.21559616]\n",
      " [-0.25868689]\n",
      " [-0.06498723]]\n",
      "loss=33026.34876228158\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.118388  ]\n",
      " [-0.02667131]\n",
      " [ 0.01041817]\n",
      " ...\n",
      " [-0.02557387]\n",
      " [-0.03325041]\n",
      " [-0.0069016 ]]\n",
      "t [[-0.118388  ]\n",
      " [-0.02667131]\n",
      " [ 0.01041817]\n",
      " ...\n",
      " [-0.02557387]\n",
      " [-0.03325041]\n",
      " [-0.0069016 ]]\n",
      "t [[-0.22915002]\n",
      " [-0.05301903]\n",
      " [ 0.02157387]\n",
      " ...\n",
      " [-0.04989158]\n",
      " [-0.06456029]\n",
      " [-0.01381107]]\n",
      "t [[-0.22915002]\n",
      " [-0.05301903]\n",
      " [ 0.02157387]\n",
      " ...\n",
      " [-0.04989158]\n",
      " [-0.06456029]\n",
      " [-0.01381107]]\n",
      "Current iteration=2, loss=36246.31907081065\n",
      "t [[-0.33288555]\n",
      " [-0.07889877]\n",
      " [ 0.03320755]\n",
      " ...\n",
      " [-0.07302342]\n",
      " [-0.09406232]\n",
      " [-0.02068006]]\n",
      "t [[-0.33288555]\n",
      " [-0.07889877]\n",
      " [ 0.03320755]\n",
      " ...\n",
      " [-0.07302342]\n",
      " [-0.09406232]\n",
      " [-0.02068006]]\n",
      "t [[-0.43016257]\n",
      " [-0.10420266]\n",
      " [ 0.04510591]\n",
      " ...\n",
      " [-0.09503921]\n",
      " [-0.12188492]\n",
      " [-0.02747096]]\n",
      "t [[-0.43016257]\n",
      " [-0.10420266]\n",
      " [ 0.04510591]\n",
      " ...\n",
      " [-0.09503921]\n",
      " [-0.12188492]\n",
      " [-0.02747096]]\n",
      "Current iteration=4, loss=35118.04286562829\n",
      "t [[-0.52151087]\n",
      " [-0.128854  ]\n",
      " [ 0.05709697]\n",
      " ...\n",
      " [-0.11600681]\n",
      " [-0.14815031]\n",
      " [-0.03415526]]\n",
      "t [[-0.52151087]\n",
      " [-0.128854  ]\n",
      " [ 0.05709697]\n",
      " ...\n",
      " [-0.11600681]\n",
      " [-0.14815031]\n",
      " [-0.03415526]]\n",
      "t [[-0.60741917]\n",
      " [-0.15280182]\n",
      " [ 0.06904463]\n",
      " ...\n",
      " [-0.1359911 ]\n",
      " [-0.17297327]\n",
      " [-0.04071194]]\n",
      "t [[-0.60741917]\n",
      " [-0.15280182]\n",
      " [ 0.06904463]\n",
      " ...\n",
      " [-0.1359911 ]\n",
      " [-0.17297327]\n",
      " [-0.04071194]]\n",
      "Current iteration=6, loss=34239.50063593835\n",
      "t [[-0.68833465]\n",
      " [-0.17601577]\n",
      " [ 0.0808431 ]\n",
      " ...\n",
      " [-0.15505345]\n",
      " [-0.19646066]\n",
      " [-0.04712607]]\n",
      "t [[-0.68833465]\n",
      " [-0.17601577]\n",
      " [ 0.0808431 ]\n",
      " ...\n",
      " [-0.15505345]\n",
      " [-0.19646066]\n",
      " [-0.04712607]]\n",
      "t [[-0.76466431]\n",
      " [-0.19848171]\n",
      " [ 0.09241179]\n",
      " ...\n",
      " [-0.17325141]\n",
      " [-0.21871122]\n",
      " [-0.0533875 ]]\n",
      "t [[-0.76466431]\n",
      " [-0.19848171]\n",
      " [ 0.09241179]\n",
      " ...\n",
      " [-0.17325141]\n",
      " [-0.21871122]\n",
      " [-0.0533875 ]]\n",
      "Current iteration=8, loss=33546.7957254163\n",
      "t [[-0.83677724]\n",
      " [-0.22019783]\n",
      " [ 0.10369078]\n",
      " ...\n",
      " [-0.19063869]\n",
      " [-0.23981582]\n",
      " [-0.05948984]]\n",
      "t [[-0.83677724]\n",
      " [-0.22019783]\n",
      " [ 0.10369078]\n",
      " ...\n",
      " [-0.19063869]\n",
      " [-0.23981582]\n",
      " [-0.05948984]]\n",
      "t [[-0.90500749]\n",
      " [-0.24117153]\n",
      " [ 0.11463681]\n",
      " ...\n",
      " [-0.20726519]\n",
      " [-0.25985777]\n",
      " [-0.06542959]]\n",
      "loss=32993.325430170815\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.11869079]\n",
      " [-0.02750863]\n",
      " [ 0.00714906]\n",
      " ...\n",
      " [-0.02527895]\n",
      " [-0.03344571]\n",
      " [-0.0067647 ]]\n",
      "t [[-0.11869079]\n",
      " [-0.02750863]\n",
      " [ 0.00714906]\n",
      " ...\n",
      " [-0.02527895]\n",
      " [-0.03344571]\n",
      " [-0.0067647 ]]\n",
      "t [[-0.22974344]\n",
      " [-0.05459633]\n",
      " [ 0.01543858]\n",
      " ...\n",
      " [-0.04937346]\n",
      " [-0.06494008]\n",
      " [-0.01354199]]\n",
      "t [[-0.22974344]\n",
      " [-0.05459633]\n",
      " [ 0.01543858]\n",
      " ...\n",
      " [-0.04937346]\n",
      " [-0.06494008]\n",
      " [-0.01354199]]\n",
      "Current iteration=2, loss=36227.14445624132\n",
      "t [[-0.33375523]\n",
      " [-0.08113627]\n",
      " [ 0.02456931]\n",
      " ...\n",
      " [-0.07234394]\n",
      " [-0.09461663]\n",
      " [-0.02028605]]\n",
      "t [[-0.33375523]\n",
      " [-0.08113627]\n",
      " [ 0.02456931]\n",
      " ...\n",
      " [-0.07234394]\n",
      " [-0.09461663]\n",
      " [-0.02028605]]\n",
      "t [[-0.4312932 ]\n",
      " [-0.10703477]\n",
      " [ 0.03428996]\n",
      " ...\n",
      " [-0.09425137]\n",
      " [-0.12260466]\n",
      " [-0.02696095]]\n",
      "t [[-0.4312932 ]\n",
      " [-0.10703477]\n",
      " [ 0.03428996]\n",
      " ...\n",
      " [-0.09425137]\n",
      " [-0.12260466]\n",
      " [-0.02696095]]\n",
      "Current iteration=4, loss=35084.25563029238\n",
      "t [[-0.52288708]\n",
      " [-0.13222662]\n",
      " [ 0.04439314]\n",
      " ...\n",
      " [-0.11515577]\n",
      " [-0.14902722]\n",
      " [-0.03353918]]\n",
      "t [[-0.52288708]\n",
      " [-0.13222662]\n",
      " [ 0.04439314]\n",
      " ...\n",
      " [-0.11515577]\n",
      " [-0.14902722]\n",
      " [-0.03353918]]\n",
      "t [[-0.609026  ]\n",
      " [-0.15667011]\n",
      " [ 0.05471026]\n",
      " ...\n",
      " [-0.13511521]\n",
      " [-0.17399988]\n",
      " [-0.04000029]]\n",
      "t [[-0.609026  ]\n",
      " [-0.15667011]\n",
      " [ 0.05471026]\n",
      " ...\n",
      " [-0.13511521]\n",
      " [-0.17399988]\n",
      " [-0.04000029]]\n",
      "Current iteration=6, loss=34194.49369533305\n",
      "t [[-0.6901579 ]\n",
      " [-0.18034242]\n",
      " [ 0.06510617]\n",
      " ...\n",
      " [-0.1541851 ]\n",
      " [-0.1976302 ]\n",
      " [-0.0463295 ]]\n",
      "t [[-0.6901579 ]\n",
      " [-0.18034242]\n",
      " [ 0.06510617]\n",
      " ...\n",
      " [-0.1541851 ]\n",
      " [-0.1976302 ]\n",
      " [-0.0463295 ]]\n",
      "t [[-0.76669062]\n",
      " [-0.20323549]\n",
      " [ 0.07547397]\n",
      " ...\n",
      " [-0.17241788]\n",
      " [-0.22001756]\n",
      " [-0.05251659]]\n",
      "t [[-0.76669062]\n",
      " [-0.20323549]\n",
      " [ 0.07547397]\n",
      " ...\n",
      " [-0.17241788]\n",
      " [-0.22001756]\n",
      " [-0.05251659]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=8, loss=33493.07402044673\n",
      "t [[-0.83899417]\n",
      " [-0.22535249]\n",
      " [ 0.08573028]\n",
      " ...\n",
      " [-0.1898628 ]\n",
      " [-0.24125339]\n",
      " [-0.05855488]]\n",
      "t [[-0.83899417]\n",
      " [-0.22535249]\n",
      " [ 0.08573028]\n",
      " ...\n",
      " [-0.1898628 ]\n",
      " [-0.24125339]\n",
      " [-0.05855488]]\n",
      "t [[-0.90740347]\n",
      " [-0.24670491]\n",
      " [ 0.09581114]\n",
      " ...\n",
      " [-0.20656599]\n",
      " [-0.26142149]\n",
      " [-0.06444048]]\n",
      "loss=32932.750428439205\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.11775462]\n",
      " [-0.02561211]\n",
      " [ 0.008383  ]\n",
      " ...\n",
      " [ 0.13198121]\n",
      " [-0.09717831]\n",
      " [ 0.10705211]]\n",
      "t [[-0.11775462]\n",
      " [-0.02561211]\n",
      " [ 0.008383  ]\n",
      " ...\n",
      " [ 0.13198121]\n",
      " [-0.09717831]\n",
      " [ 0.10705211]]\n",
      "t [[-0.22796918]\n",
      " [-0.05100826]\n",
      " [ 0.017774  ]\n",
      " ...\n",
      " [ 0.2541344 ]\n",
      " [-0.18759065]\n",
      " [ 0.20674922]]\n",
      "t [[-0.22796918]\n",
      " [-0.05100826]\n",
      " [ 0.017774  ]\n",
      " ...\n",
      " [ 0.2541344 ]\n",
      " [-0.18759065]\n",
      " [ 0.20674922]]\n",
      "Current iteration=2, loss=36278.4347680199\n",
      "t [[-0.33123626]\n",
      " [-0.07602757]\n",
      " [ 0.02788295]\n",
      " ...\n",
      " [ 0.3672083 ]\n",
      " [-0.27171406]\n",
      " [ 0.29962832]]\n",
      "t [[-0.33123626]\n",
      " [-0.07602757]\n",
      " [ 0.02788295]\n",
      " ...\n",
      " [ 0.3672083 ]\n",
      " [-0.27171406]\n",
      " [ 0.29962832]]\n",
      "t [[-0.42811673]\n",
      " [-0.10054804]\n",
      " [ 0.03846815]\n",
      " ...\n",
      " [ 0.47192279]\n",
      " [-0.35001376]\n",
      " [ 0.38620885]]\n",
      "t [[-0.42811673]\n",
      " [-0.10054804]\n",
      " [ 0.03846815]\n",
      " ...\n",
      " [ 0.47192279]\n",
      " [-0.35001376]\n",
      " [ 0.38620885]]\n",
      "Current iteration=4, loss=35172.93890520292\n",
      "t [[-0.51913336]\n",
      " [-0.12448097]\n",
      " [ 0.04933178]\n",
      " ...\n",
      " [ 0.56895796]\n",
      " [-0.42293471]\n",
      " [ 0.4669841 ]]\n",
      "t [[-0.51913336]\n",
      " [-0.12448097]\n",
      " [ 0.04933178]\n",
      " ...\n",
      " [ 0.56895796]\n",
      " [-0.42293471]\n",
      " [ 0.4669841 ]]\n",
      "t [[-0.60476811]\n",
      " [-0.14776537]\n",
      " [ 0.06031439]\n",
      " ...\n",
      " [ 0.658948  ]\n",
      " [-0.49089643]\n",
      " [ 0.5424161 ]]\n",
      "t [[-0.60476811]\n",
      " [-0.14776537]\n",
      " [ 0.06031439]\n",
      " ...\n",
      " [ 0.658948  ]\n",
      " [-0.49089643]\n",
      " [ 0.5424161 ]]\n",
      "Current iteration=6, loss=34310.644443139005\n",
      "t [[-0.68546184]\n",
      " [-0.17036258]\n",
      " [ 0.07128942]\n",
      " ...\n",
      " [ 0.74247877]\n",
      " [-0.55429024]\n",
      " [ 0.61293336]]\n",
      "t [[-0.68546184]\n",
      " [-0.17036258]\n",
      " [ 0.07128942]\n",
      " ...\n",
      " [ 0.74247877]\n",
      " [-0.55429024]\n",
      " [ 0.61293336]]\n",
      "t [[-0.76161569]\n",
      " [-0.19225155]\n",
      " [ 0.08215792]\n",
      " ...\n",
      " [ 0.82008778]\n",
      " [-0.6134783 ]\n",
      " [ 0.67893044]]\n",
      "t [[-0.76161569]\n",
      " [-0.19225155]\n",
      " [ 0.08215792]\n",
      " ...\n",
      " [ 0.82008778]\n",
      " [-0.6134783 ]\n",
      " [ 0.67893044]]\n",
      "Current iteration=8, loss=33629.66842486429\n",
      "t [[-0.83359338]\n",
      " [-0.21342483]\n",
      " [ 0.09284381]\n",
      " ...\n",
      " [ 0.89226586]\n",
      " [-0.66879392]\n",
      " [ 0.74076877]]\n",
      "t [[-0.83359338]\n",
      " [-0.21342483]\n",
      " [ 0.09284381]\n",
      " ...\n",
      " [ 0.89226586]\n",
      " [-0.66879392]\n",
      " [ 0.74076877]]\n",
      "t [[-0.90172411]\n",
      " [-0.23388521]\n",
      " [ 0.10328973]\n",
      " ...\n",
      " [ 0.95945979]\n",
      " [-0.7205426 ]\n",
      " [ 0.79877833]]\n",
      "loss=33084.803664051324\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.10996991]\n",
      " [-0.02313418]\n",
      " [ 0.02296075]\n",
      " ...\n",
      " [-0.02902341]\n",
      " [-0.03608555]\n",
      " [-0.00758599]]\n",
      "t [[-0.10996991]\n",
      " [-0.02313418]\n",
      " [ 0.02296075]\n",
      " ...\n",
      " [-0.02902341]\n",
      " [-0.03608555]\n",
      " [-0.00758599]]\n",
      "t [[-0.21099253]\n",
      " [-0.04236488]\n",
      " [ 0.04481888]\n",
      " ...\n",
      " [-0.0565004 ]\n",
      " [-0.0698815 ]\n",
      " [-0.01513648]]\n",
      "t [[-0.21099253]\n",
      " [-0.04236488]\n",
      " [ 0.04481888]\n",
      " ...\n",
      " [-0.0565004 ]\n",
      " [-0.0698815 ]\n",
      " [-0.01513648]]\n",
      "Current iteration=2, loss=36140.65669534802\n",
      "t [[-0.30370826]\n",
      " [-0.05840127]\n",
      " [ 0.06549605]\n",
      " ...\n",
      " [-0.08252106]\n",
      " [-0.10155851]\n",
      " [-0.02259944]]\n",
      "t [[-0.30370826]\n",
      " [-0.05840127]\n",
      " [ 0.06549605]\n",
      " ...\n",
      " [-0.08252106]\n",
      " [-0.10155851]\n",
      " [-0.02259944]]\n",
      "t [[-0.38875326]\n",
      " [-0.07184235]\n",
      " [ 0.08495727]\n",
      " ...\n",
      " [-0.1071757 ]\n",
      " [-0.13128086]\n",
      " [-0.02993607]]\n",
      "t [[-0.38875326]\n",
      " [-0.07184235]\n",
      " [ 0.08495727]\n",
      " ...\n",
      " [-0.1071757 ]\n",
      " [-0.13128086]\n",
      " [-0.02993607]]\n",
      "Current iteration=4, loss=34957.675666415555\n",
      "t [[-0.46674284]\n",
      " [-0.08318719]\n",
      " [ 0.10320113]\n",
      " ...\n",
      " [-0.13055233]\n",
      " [-0.15920367]\n",
      " [-0.03711849]]\n",
      "t [[-0.46674284]\n",
      " [-0.08318719]\n",
      " [ 0.10320113]\n",
      " ...\n",
      " [-0.13055233]\n",
      " [-0.15920367]\n",
      " [-0.03711849]]\n",
      "t [[-0.53826067]\n",
      " [-0.09284757]\n",
      " [ 0.12025089]\n",
      " ...\n",
      " [-0.15273507]\n",
      " [-0.18547132]\n",
      " [-0.04412753]]\n",
      "t [[-0.53826067]\n",
      " [-0.09284757]\n",
      " [ 0.12025089]\n",
      " ...\n",
      " [-0.15273507]\n",
      " [-0.18547132]\n",
      " [-0.04412753]]\n",
      "Current iteration=6, loss=34055.02668844487\n",
      "t [[-0.60385255]\n",
      " [-0.10116084]\n",
      " [ 0.13614703]\n",
      " ...\n",
      " [-0.17380326]\n",
      " [-0.21021691]\n",
      " [-0.05095084]]\n",
      "t [[-0.60385255]\n",
      " [-0.10116084]\n",
      " [ 0.13614703]\n",
      " ...\n",
      " [-0.17380326]\n",
      " [-0.21021691]\n",
      " [-0.05095084]]\n",
      "t [[-0.66402354]\n",
      " [-0.10840212]\n",
      " [ 0.15094128]\n",
      " ...\n",
      " [-0.193831  ]\n",
      " [-0.23356223]\n",
      " [-0.05758125]]\n",
      "t [[-0.66402354]\n",
      " [-0.10840212]\n",
      " [ 0.15094128]\n",
      " ...\n",
      " [-0.193831  ]\n",
      " [-0.23356223]\n",
      " [-0.05758125]]\n",
      "Current iteration=8, loss=33355.944967397256\n",
      "t [[-0.71923756]\n",
      " [-0.11479512]\n",
      " [ 0.164692  ]\n",
      " ...\n",
      " [-0.2128871 ]\n",
      " [-0.25561826]\n",
      " [-0.06401548]]\n",
      "t [[-0.71923756]\n",
      " [-0.11479512]\n",
      " [ 0.164692  ]\n",
      " ...\n",
      " [-0.2128871 ]\n",
      " [-0.25561826]\n",
      " [-0.06401548]]\n",
      "t [[-0.76991843]\n",
      " [-0.12052137]\n",
      " [ 0.17746081]\n",
      " ...\n",
      " [-0.2310352 ]\n",
      " [-0.27648582]\n",
      " [-0.07025307]]\n",
      "loss=32806.01447345293\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.12915055]\n",
      " [-0.02909598]\n",
      " [ 0.01136528]\n",
      " ...\n",
      " [-0.02789877]\n",
      " [-0.03627317]\n",
      " [-0.00752902]]\n",
      "t [[-0.12915055]\n",
      " [-0.02909598]\n",
      " [ 0.01136528]\n",
      " ...\n",
      " [-0.02789877]\n",
      " [-0.03627317]\n",
      " [-0.00752902]]\n",
      "t [[-0.249227  ]\n",
      " [-0.05780673]\n",
      " [ 0.02360807]\n",
      " ...\n",
      " [-0.05430285]\n",
      " [-0.07023729]\n",
      " [-0.0150674 ]]\n",
      "t [[-0.249227  ]\n",
      " [-0.05780673]\n",
      " [ 0.02360807]\n",
      " ...\n",
      " [-0.05430285]\n",
      " [-0.07023729]\n",
      " [-0.0150674 ]]\n",
      "Current iteration=2, loss=36127.17154717836\n",
      "t [[-0.36101078]\n",
      " [-0.08594477]\n",
      " [ 0.0363912 ]\n",
      " ...\n",
      " [-0.07930405]\n",
      " [-0.10206529]\n",
      " [-0.02255244]]\n",
      "t [[-0.36101078]\n",
      " [-0.08594477]\n",
      " [ 0.0363912 ]\n",
      " ...\n",
      " [-0.07930405]\n",
      " [-0.10206529]\n",
      " [-0.02255244]]\n",
      "t [[-0.46523629]\n",
      " [-0.11337468]\n",
      " [ 0.04944352]\n",
      " ...\n",
      " [-0.10299318]\n",
      " [-0.13192363]\n",
      " [-0.0299367 ]]\n",
      "t [[-0.46523629]\n",
      " [-0.11337468]\n",
      " [ 0.04944352]\n",
      " ...\n",
      " [-0.10299318]\n",
      " [-0.13192363]\n",
      " [-0.0299367 ]]\n",
      "Current iteration=4, loss=34935.755636322916\n",
      "t [[-0.5625818 ]\n",
      " [-0.14000455]\n",
      " [ 0.06255195]\n",
      " ...\n",
      " [-0.1254578 ]\n",
      " [-0.15996941]\n",
      " [-0.03718537]]\n",
      "t [[-0.5625818 ]\n",
      " [-0.14000455]\n",
      " [ 0.06255195]\n",
      " ...\n",
      " [-0.1254578 ]\n",
      " [-0.15996941]\n",
      " [-0.03718537]]\n",
      "t [[-0.65366602]\n",
      " [-0.16577737]\n",
      " [ 0.07555262]\n",
      " ...\n",
      " [-0.14678093]\n",
      " [-0.18634878]\n",
      " [-0.04427384]]\n",
      "t [[-0.65366602]\n",
      " [-0.16577737]\n",
      " [ 0.07555262]\n",
      " ...\n",
      " [-0.14678093]\n",
      " [-0.18634878]\n",
      " [-0.04427384]]\n",
      "Current iteration=6, loss=34027.59307062189\n",
      "t [[-0.73904872]\n",
      " [-0.19066329]\n",
      " [ 0.08832235]\n",
      " ...\n",
      " [-0.16704035]\n",
      " [-0.21119638]\n",
      " [-0.05118547]]\n",
      "t [[-0.73904872]\n",
      " [-0.19066329]\n",
      " [ 0.08832235]\n",
      " ...\n",
      " [-0.16704035]\n",
      " [-0.21119638]\n",
      " [-0.05118547]]\n",
      "t [[-0.81923356]\n",
      " [-0.21465295]\n",
      " [ 0.10077087]\n",
      " ...\n",
      " [-0.18630833]\n",
      " [-0.23463539]\n",
      " [-0.05790979]]\n",
      "t [[-0.81923356]\n",
      " [-0.21465295]\n",
      " [ 0.10077087]\n",
      " ...\n",
      " [-0.18630833]\n",
      " [-0.23463539]\n",
      " [-0.05790979]]\n",
      "Current iteration=8, loss=33324.66360365178\n",
      "t [[-0.89467234]\n",
      " [-0.23775205]\n",
      " [ 0.11283419]\n",
      " ...\n",
      " [-0.20465164]\n",
      " [-0.25677797]\n",
      " [-0.06444093]]\n",
      "t [[-0.89467234]\n",
      " [-0.23775205]\n",
      " [ 0.11283419]\n",
      " ...\n",
      " [-0.20465164]\n",
      " [-0.25677797]\n",
      " [-0.06444093]]\n",
      "t [[-0.96576966]\n",
      " [-0.25997702]\n",
      " [ 0.12446894]\n",
      " ...\n",
      " [-0.22213182]\n",
      " [-0.27772597]\n",
      " [-0.07077649]]\n",
      "loss=32771.82275536839\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.12948086]\n",
      " [-0.03000941]\n",
      " [ 0.00779897]\n",
      " ...\n",
      " [-0.02757704]\n",
      " [-0.03648623]\n",
      " [-0.00737967]]\n",
      "t [[-0.12948086]\n",
      " [-0.03000941]\n",
      " [ 0.00779897]\n",
      " ...\n",
      " [-0.02757704]\n",
      " [-0.03648623]\n",
      " [-0.00737967]]\n",
      "t [[-0.24987319]\n",
      " [-0.05951777]\n",
      " [ 0.01695489]\n",
      " ...\n",
      " [-0.05374472]\n",
      " [-0.07065054]\n",
      " [-0.01477433]]\n",
      "t [[-0.24987319]\n",
      " [-0.05951777]\n",
      " [ 0.01695489]\n",
      " ...\n",
      " [-0.05374472]\n",
      " [-0.07065054]\n",
      " [-0.01477433]]\n",
      "Current iteration=2, loss=36106.44257342122\n",
      "t [[-0.36195559]\n",
      " [-0.08836037]\n",
      " [ 0.02707883]\n",
      " ...\n",
      " [-0.07858202]\n",
      " [-0.10266699]\n",
      " [-0.02212454]]\n",
      "t [[-0.36195559]\n",
      " [-0.08836037]\n",
      " [ 0.02707883]\n",
      " ...\n",
      " [-0.07858202]\n",
      " [-0.10266699]\n",
      " [-0.02212454]]\n",
      "t [[-0.46646143]\n",
      " [-0.11641984]\n",
      " [ 0.03785053]\n",
      " ...\n",
      " [-0.10216837]\n",
      " [-0.13270317]\n",
      " [-0.02938491]]\n",
      "t [[-0.46646143]\n",
      " [-0.11641984]\n",
      " [ 0.03785053]\n",
      " ...\n",
      " [-0.10216837]\n",
      " [-0.13270317]\n",
      " [-0.02938491]]\n",
      "Current iteration=4, loss=34899.62111488309\n",
      "t [[-0.56406906]\n",
      " [-0.14361852]\n",
      " [ 0.04901158]\n",
      " ...\n",
      " [-0.12458144]\n",
      " [-0.16091725]\n",
      " [-0.03652183]]\n",
      "t [[-0.56406906]\n",
      " [-0.14361852]\n",
      " [ 0.04901158]\n",
      " ...\n",
      " [-0.12458144]\n",
      " [-0.16091725]\n",
      " [-0.03652183]]\n",
      "t [[-0.6553979 ]\n",
      " [-0.16991069]\n",
      " [ 0.06035711]\n",
      " ...\n",
      " [-0.14589574]\n",
      " [-0.18745638]\n",
      " [-0.04351121]]\n",
      "t [[-0.6553979 ]\n",
      " [-0.16991069]\n",
      " [ 0.06035711]\n",
      " ...\n",
      " [-0.14589574]\n",
      " [-0.18745638]\n",
      " [-0.04351121]]\n",
      "Current iteration=6, loss=33979.89715194425\n",
      "t [[-0.74100876]\n",
      " [-0.19527545]\n",
      " [ 0.07172737]\n",
      " ...\n",
      " [-0.16618176]\n",
      " [-0.21245608]\n",
      " [-0.0503365 ]]\n",
      "t [[-0.74100876]\n",
      " [-0.19527545]\n",
      " [ 0.07172737]\n",
      " ...\n",
      " [-0.16618176]\n",
      " [-0.21245608]\n",
      " [-0.0503365 ]]\n",
      "t [[-0.82140648]\n",
      " [-0.21971057]\n",
      " [ 0.08299981]\n",
      " ...\n",
      " [-0.18550558]\n",
      " [-0.23604031]\n",
      " [-0.05698694]]\n",
      "t [[-0.82140648]\n",
      " [-0.21971057]\n",
      " [ 0.08299981]\n",
      " ...\n",
      " [-0.18550558]\n",
      " [-0.23604031]\n",
      " [-0.05698694]]\n",
      "Current iteration=8, loss=33268.16624106257\n",
      "t [[-0.89704401]\n",
      " [-0.24322745]\n",
      " [ 0.09408209]\n",
      " ...\n",
      " [-0.20392873]\n",
      " [-0.25832188]\n",
      " [-0.06345621]]\n",
      "t [[-0.89704401]\n",
      " [-0.24322745]\n",
      " [ 0.09408209]\n",
      " ...\n",
      " [-0.20392873]\n",
      " [-0.25832188]\n",
      " [-0.06345621]]\n",
      "t [[-0.96832706]\n",
      " [-0.26584711]\n",
      " [ 0.10490615]\n",
      " ...\n",
      " [-0.22150832]\n",
      " [-0.27940324]\n",
      " [-0.0697413 ]]\n",
      "loss=32708.52632167009\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.12845958]\n",
      " [-0.02794048]\n",
      " [ 0.00914509]\n",
      " ...\n",
      " [ 0.1439795 ]\n",
      " [-0.1060127 ]\n",
      " [ 0.11678412]]\n",
      "t [[-0.12845958]\n",
      " [-0.02794048]\n",
      " [ 0.00914509]\n",
      " ...\n",
      " [ 0.1439795 ]\n",
      " [-0.1060127 ]\n",
      " [ 0.11678412]]\n",
      "t [[-0.24794731]\n",
      " [-0.0556238 ]\n",
      " [ 0.01948951]\n",
      " ...\n",
      " [ 0.27626462]\n",
      " [-0.20397449]\n",
      " [ 0.22481653]]\n",
      "t [[-0.24794731]\n",
      " [-0.0556238 ]\n",
      " [ 0.01948951]\n",
      " ...\n",
      " [ 0.27626462]\n",
      " [-0.20397449]\n",
      " [ 0.22481653]]\n",
      "Current iteration=2, loss=36161.811684888424\n",
      "t [[-0.35923557]\n",
      " [-0.08284098]\n",
      " [ 0.0306563 ]\n",
      " ...\n",
      " [ 0.39783118]\n",
      " [-0.29450718]\n",
      " [ 0.32479726]]\n",
      "t [[-0.35923557]\n",
      " [-0.08284098]\n",
      " [ 0.0306563 ]\n",
      " ...\n",
      " [ 0.39783118]\n",
      " [-0.29450718]\n",
      " [ 0.32479726]]\n",
      "t [[-0.46304951]\n",
      " [-0.10943851]\n",
      " [ 0.04233774]\n",
      " ...\n",
      " [ 0.50961135]\n",
      " [-0.37821408]\n",
      " [ 0.41739944]]\n",
      "t [[-0.46304951]\n",
      " [-0.10943851]\n",
      " [ 0.04233774]\n",
      " ...\n",
      " [ 0.50961135]\n",
      " [-0.37821408]\n",
      " [ 0.41739944]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4, loss=34994.183629159954\n",
      "t [[-0.56005829]\n",
      " [-0.13530948]\n",
      " [ 0.05428774]\n",
      " ...\n",
      " [ 0.61247792]\n",
      " [-0.45566768]\n",
      " [ 0.50325675]]\n",
      "t [[-0.56005829]\n",
      " [-0.13530948]\n",
      " [ 0.05428774]\n",
      " ...\n",
      " [ 0.61247792]\n",
      " [-0.45566768]\n",
      " [ 0.50325675]]\n",
      "t [[-0.65087201]\n",
      " [-0.16038457]\n",
      " [ 0.06631315]\n",
      " ...\n",
      " [ 0.70723646]\n",
      " [-0.5274026 ]\n",
      " [ 0.58295687]]\n",
      "t [[-0.65087201]\n",
      " [-0.16038457]\n",
      " [ 0.06631315]\n",
      " ...\n",
      " [ 0.70723646]\n",
      " [-0.5274026 ]\n",
      " [ 0.58295687]]\n",
      "Current iteration=6, loss=34102.481826412215\n",
      "t [[-0.73604243]\n",
      " [-0.18462394]\n",
      " [ 0.07826508]\n",
      " ...\n",
      " [ 0.79462302]\n",
      " [-0.59391237]\n",
      " [ 0.65703894]]\n",
      "t [[-0.73604243]\n",
      " [-0.18462394]\n",
      " [ 0.07826508]\n",
      " ...\n",
      " [ 0.79462302]\n",
      " [-0.59391237]\n",
      " [ 0.65703894]]\n",
      "t [[-0.81606592]\n",
      " [-0.20801015]\n",
      " [ 0.0900309 ]\n",
      " ...\n",
      " [ 0.87530546]\n",
      " [-0.65564891]\n",
      " [ 0.72599402]]\n",
      "t [[-0.81606592]\n",
      " [-0.20801015]\n",
      " [ 0.0900309 ]\n",
      " ...\n",
      " [ 0.87530546]\n",
      " [-0.65564891]\n",
      " [ 0.72599402]]\n",
      "Current iteration=8, loss=33411.12313145529\n",
      "t [[-0.89138769]\n",
      " [-0.23054241]\n",
      " [ 0.10152724]\n",
      " ...\n",
      " [ 0.94988703]\n",
      " [-0.71302371]\n",
      " [ 0.79026713]]\n",
      "t [[-0.89138769]\n",
      " [-0.23054241]\n",
      " [ 0.10152724]\n",
      " ...\n",
      " [ 0.94988703]\n",
      " [-0.71302371]\n",
      " [ 0.79026713]]\n",
      "t [[-0.96240645]\n",
      " [-0.25223193]\n",
      " [ 0.1126941 ]\n",
      " ...\n",
      " [ 1.01891103]\n",
      " [-0.76641005]\n",
      " [ 0.85026033]]\n",
      "loss=32866.5855142383\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.11913406]\n",
      " [-0.02506203]\n",
      " [ 0.02487414]\n",
      " ...\n",
      " [-0.03144203]\n",
      " [-0.03909268]\n",
      " [-0.00821816]]\n",
      "t [[-0.11913406]\n",
      " [-0.02506203]\n",
      " [ 0.02487414]\n",
      " ...\n",
      " [-0.03144203]\n",
      " [-0.03909268]\n",
      " [-0.00821816]]\n",
      "t [[-0.227769  ]\n",
      " [-0.04554365]\n",
      " [ 0.04845431]\n",
      " ...\n",
      " [-0.06106949]\n",
      " [-0.07549867]\n",
      " [-0.01639466]]\n",
      "t [[-0.227769  ]\n",
      " [-0.04554365]\n",
      " [ 0.04845431]\n",
      " ...\n",
      " [-0.06106949]\n",
      " [-0.07549867]\n",
      " [-0.01639466]]\n",
      "Current iteration=2, loss=36024.612882692\n",
      "t [[-0.3267222 ]\n",
      " [-0.0623476 ]\n",
      " [ 0.07064121]\n",
      " ...\n",
      " [-0.08899766]\n",
      " [-0.10943579]\n",
      " [-0.02446346]]\n",
      "t [[-0.3267222 ]\n",
      " [-0.0623476 ]\n",
      " [ 0.07064121]\n",
      " ...\n",
      " [-0.08899766]\n",
      " [-0.10943579]\n",
      " [-0.02446346]]\n",
      "t [[-0.41680287]\n",
      " [-0.07622315]\n",
      " [ 0.09139542]\n",
      " ...\n",
      " [-0.11534159]\n",
      " [-0.14111247]\n",
      " [-0.03237676]]\n",
      "t [[-0.41680287]\n",
      " [-0.07622315]\n",
      " [ 0.09139542]\n",
      " ...\n",
      " [-0.11534159]\n",
      " [-0.14111247]\n",
      " [-0.03237676]]\n",
      "Current iteration=4, loss=34783.891861312746\n",
      "t [[-0.49878818]\n",
      " [-0.08778273]\n",
      " [ 0.1107224 ]\n",
      " ...\n",
      " [-0.14021259]\n",
      " [-0.17072379]\n",
      " [-0.04010149]]\n",
      "t [[-0.49878818]\n",
      " [-0.08778273]\n",
      " [ 0.1107224 ]\n",
      " ...\n",
      " [-0.14021259]\n",
      " [-0.17072379]\n",
      " [-0.04010149]]\n",
      "t [[-0.57340864]\n",
      " [-0.09752153]\n",
      " [ 0.12865935]\n",
      " ...\n",
      " [-0.16371611]\n",
      " [-0.19844957]\n",
      " [-0.04761605]]\n",
      "t [[-0.57340864]\n",
      " [-0.09752153]\n",
      " [ 0.12865935]\n",
      " ...\n",
      " [-0.16371611]\n",
      " [-0.19844957]\n",
      " [-0.04761605]]\n",
      "Current iteration=6, loss=33856.80563419402\n",
      "t [[-0.64134037]\n",
      " [-0.10583709]\n",
      " [ 0.1452645 ]\n",
      " ...\n",
      " [-0.18595066]\n",
      " [-0.22445384]\n",
      " [-0.05490748]]\n",
      "t [[-0.64134037]\n",
      " [-0.10583709]\n",
      " [ 0.1452645 ]\n",
      " ...\n",
      " [-0.18595066]\n",
      " [-0.22445384]\n",
      " [-0.05490748]]\n",
      "t [[-0.70320242]\n",
      " [-0.11304704]\n",
      " [ 0.16060879]\n",
      " ...\n",
      " [-0.2070074 ]\n",
      " [-0.24888519]\n",
      " [-0.06196925]]\n",
      "t [[-0.70320242]\n",
      " [-0.11304704]\n",
      " [ 0.16060879]\n",
      " ...\n",
      " [-0.2070074 ]\n",
      " [-0.24888519]\n",
      " [-0.06196925]]\n",
      "Current iteration=8, loss=33151.56825338103\n",
      "t [[-0.75955728]\n",
      " [-0.11940452]\n",
      " [ 0.17476977]\n",
      " ...\n",
      " [-0.22697023]\n",
      " [-0.27187762]\n",
      " [-0.06879938]]\n",
      "t [[-0.75955728]\n",
      " [-0.11940452]\n",
      " [ 0.17476977]\n",
      " ...\n",
      " [-0.22697023]\n",
      " [-0.27187762]\n",
      " [-0.06879938]]\n",
      "t [[-0.81091351]\n",
      " [-0.12511091]\n",
      " [ 0.18782731]\n",
      " ...\n",
      " [-0.24591602]\n",
      " [-0.2935517 ]\n",
      " [-0.07539918]]\n",
      "loss=32605.102043183088\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.13991309]\n",
      " [-0.03152064]\n",
      " [ 0.01231238]\n",
      " ...\n",
      " [-0.03022367]\n",
      " [-0.03929594]\n",
      " [-0.00815644]]\n",
      "t [[-0.13991309]\n",
      " [-0.03152064]\n",
      " [ 0.01231238]\n",
      " ...\n",
      " [-0.03022367]\n",
      " [-0.03929594]\n",
      " [-0.00815644]]\n",
      "t [[-0.2691786 ]\n",
      " [-0.06258903]\n",
      " [ 0.02565436]\n",
      " ...\n",
      " [-0.05869346]\n",
      " [-0.07588235]\n",
      " [-0.01632386]]\n",
      "t [[-0.2691786 ]\n",
      " [-0.06258903]\n",
      " [ 0.02565436]\n",
      " ...\n",
      " [-0.05869346]\n",
      " [-0.07588235]\n",
      " [-0.01632386]]\n",
      "Current iteration=2, loss=36010.21202410754\n",
      "t [[-0.38879377]\n",
      " [-0.09296676]\n",
      " [ 0.03959702]\n",
      " ...\n",
      " [-0.08552681]\n",
      " [-0.10998   ]\n",
      " [-0.02442265]]\n",
      "t [[-0.38879377]\n",
      " [-0.09296676]\n",
      " [ 0.03959702]\n",
      " ...\n",
      " [-0.08552681]\n",
      " [-0.10998   ]\n",
      " [-0.02442265]]\n",
      "t [[-0.49968807]\n",
      " [-0.12248764]\n",
      " [ 0.05380311]\n",
      " ...\n",
      " [-0.11083924]\n",
      " [-0.14180002]\n",
      " [-0.03239422]]\n",
      "t [[-0.49968807]\n",
      " [-0.12248764]\n",
      " [ 0.05380311]\n",
      " ...\n",
      " [-0.11083924]\n",
      " [-0.14180002]\n",
      " [-0.03239422]]\n",
      "Current iteration=4, loss=34760.82801783197\n",
      "t [[-0.60271092]\n",
      " [-0.15104439]\n",
      " [ 0.06801451]\n",
      " ...\n",
      " [-0.1347413 ]\n",
      " [-0.17153999]\n",
      " [-0.04019711]]\n",
      "t [[-0.60271092]\n",
      " [-0.15104439]\n",
      " [ 0.06801451]\n",
      " ...\n",
      " [-0.1347413 ]\n",
      " [-0.17153999]\n",
      " [-0.04019711]]\n",
      "t [[-0.69862849]\n",
      " [-0.17857568]\n",
      " [ 0.08203867]\n",
      " ...\n",
      " [-0.15733685]\n",
      " [-0.1993819 ]\n",
      " [-0.04780323]]\n",
      "t [[-0.69862849]\n",
      " [-0.17857568]\n",
      " [ 0.08203867]\n",
      " ...\n",
      " [-0.15733685]\n",
      " [-0.1993819 ]\n",
      " [-0.04780323]]\n",
      "Current iteration=6, loss=33828.23452537687\n",
      "t [[-0.78812593]\n",
      " [-0.20505468]\n",
      " [ 0.09573591]\n",
      " ...\n",
      " [-0.17872226]\n",
      " [-0.22549166]\n",
      " [-0.0551947 ]]\n",
      "t [[-0.78812593]\n",
      " [-0.20505468]\n",
      " [ 0.09573591]\n",
      " ...\n",
      " [-0.17872226]\n",
      " [-0.22549166]\n",
      " [-0.0551947 ]]\n",
      "t [[-0.87181282]\n",
      " [-0.23047969]\n",
      " [ 0.10900823]\n",
      " ...\n",
      " [-0.19898623]\n",
      " [-0.25001949]\n",
      " [-0.06236123]]\n",
      "t [[-0.87181282]\n",
      " [-0.23047969]\n",
      " [ 0.10900823]\n",
      " ...\n",
      " [-0.19898623]\n",
      " [-0.25001949]\n",
      " [-0.06236123]]\n",
      "Current iteration=8, loss=33119.19729988219\n",
      "t [[-0.95023008]\n",
      " [-0.25486663]\n",
      " [ 0.12178989]\n",
      " ...\n",
      " [-0.21820998]\n",
      " [-0.2731008 ]\n",
      " [-0.06929806]]\n",
      "t [[-0.95023008]\n",
      " [-0.25486663]\n",
      " [ 0.12178989]\n",
      " ...\n",
      " [-0.21820998]\n",
      " [-0.2731008 ]\n",
      " [-0.06929806]]\n",
      "t [[-1.02385723]\n",
      " [-0.27824337]\n",
      " [ 0.13403988]\n",
      " ...\n",
      " [-0.23646768]\n",
      " [-0.29485735]\n",
      " [-0.0760044 ]]\n",
      "loss=32569.83929409923\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.14027093]\n",
      " [-0.0325102 ]\n",
      " [ 0.00844889]\n",
      " ...\n",
      " [-0.02987512]\n",
      " [-0.03952675]\n",
      " [-0.00799465]]\n",
      "t [[-0.14027093]\n",
      " [-0.0325102 ]\n",
      " [ 0.00844889]\n",
      " ...\n",
      " [-0.02987512]\n",
      " [-0.03952675]\n",
      " [-0.00799465]]\n",
      "t [[-0.26987737]\n",
      " [-0.06443221]\n",
      " [ 0.01848991]\n",
      " ...\n",
      " [-0.05809651]\n",
      " [-0.07632889]\n",
      " [-0.01600688]]\n",
      "t [[-0.26987737]\n",
      " [-0.06443221]\n",
      " [ 0.01848991]\n",
      " ...\n",
      " [-0.05809651]\n",
      " [-0.07632889]\n",
      " [-0.01600688]]\n",
      "Current iteration=2, loss=35987.95828393971\n",
      "t [[-0.38981305]\n",
      " [-0.09555659]\n",
      " [ 0.02962814]\n",
      " ...\n",
      " [-0.08476523]\n",
      " [-0.11062861]\n",
      " [-0.0239612 ]]\n",
      "t [[-0.38981305]\n",
      " [-0.09555659]\n",
      " [ 0.02962814]\n",
      " ...\n",
      " [-0.08476523]\n",
      " [-0.11062861]\n",
      " [-0.0239612 ]]\n",
      "t [[-0.50100634]\n",
      " [-0.12573961]\n",
      " [ 0.04146415]\n",
      " ...\n",
      " [-0.1099825 ]\n",
      " [-0.14263851]\n",
      " [-0.03180154]]\n",
      "t [[-0.50100634]\n",
      " [-0.12573961]\n",
      " [ 0.04146415]\n",
      " ...\n",
      " [-0.1099825 ]\n",
      " [-0.14263851]\n",
      " [-0.03180154]]\n",
      "Current iteration=4, loss=34722.44572202941\n",
      "t [[-0.60430698]\n",
      " [-0.1548913 ]\n",
      " [ 0.0536831 ]\n",
      " ...\n",
      " [-0.13384664]\n",
      " [-0.1725575 ]\n",
      " [-0.03948774]]\n",
      "t [[-0.60430698]\n",
      " [-0.1548913 ]\n",
      " [ 0.0536831 ]\n",
      " ...\n",
      " [-0.13384664]\n",
      " [-0.1725575 ]\n",
      " [-0.03948774]]\n",
      "t [[-0.70048217]\n",
      " [-0.18296375]\n",
      " [ 0.06604182]\n",
      " ...\n",
      " [-0.15645114]\n",
      " [-0.20056881]\n",
      " [-0.04699221]]\n",
      "t [[-0.70048217]\n",
      " [-0.18296375]\n",
      " [ 0.06604182]\n",
      " ...\n",
      " [-0.15645114]\n",
      " [-0.20056881]\n",
      " [-0.04699221]]\n",
      "Current iteration=6, loss=33778.01771647487\n",
      "t [[-0.79021848]\n",
      " [-0.20994058]\n",
      " [ 0.07835615]\n",
      " ...\n",
      " [-0.17788362]\n",
      " [-0.22683941]\n",
      " [-0.05429694]]\n",
      "t [[-0.79021848]\n",
      " [-0.20994058]\n",
      " [ 0.07835615]\n",
      " ...\n",
      " [-0.17788362]\n",
      " [-0.22683941]\n",
      " [-0.05429694]]\n",
      "t [[-0.874127  ]\n",
      " [-0.23582822]\n",
      " [ 0.09048935]\n",
      " ...\n",
      " [-0.1982255 ]\n",
      " [-0.25152045]\n",
      " [-0.06139118]]\n",
      "t [[-0.874127  ]\n",
      " [-0.23582822]\n",
      " [ 0.09048935]\n",
      " ...\n",
      " [-0.1982255 ]\n",
      " [-0.25152045]\n",
      " [-0.06139118]]\n",
      "Current iteration=8, loss=33060.14567358797\n",
      "t [[-0.9527501 ]\n",
      " [-0.26064901]\n",
      " [ 0.10234223]\n",
      " ...\n",
      " [-0.21755193]\n",
      " [-0.27474812]\n",
      " [-0.06826945]]\n",
      "t [[-0.9527501 ]\n",
      " [-0.26064901]\n",
      " [ 0.10234223]\n",
      " ...\n",
      " [-0.21755193]\n",
      " [-0.27474812]\n",
      " [-0.06826945]]\n",
      "t [[-1.02656865]\n",
      " [-0.28443585]\n",
      " [ 0.113845  ]\n",
      " ...\n",
      " [-0.23593203]\n",
      " [-0.29664484]\n",
      " [-0.07493011]]\n",
      "loss=32504.07996989281\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.13916455]\n",
      " [-0.03026886]\n",
      " [ 0.00990718]\n",
      " ...\n",
      " [ 0.15597779]\n",
      " [-0.1148471 ]\n",
      " [ 0.12651613]]\n",
      "t [[-0.13916455]\n",
      " [-0.03026886]\n",
      " [ 0.00990718]\n",
      " ...\n",
      " [ 0.15597779]\n",
      " [-0.1148471 ]\n",
      " [ 0.12651613]]\n",
      "t [[-0.26780146]\n",
      " [-0.0602357 ]\n",
      " [ 0.02122155]\n",
      " ...\n",
      " [ 0.29823319]\n",
      " [-0.22024701]\n",
      " [ 0.24276287]]\n",
      "t [[-0.26780146]\n",
      " [-0.0602357 ]\n",
      " [ 0.02122155]\n",
      " ...\n",
      " [ 0.29823319]\n",
      " [-0.22024701]\n",
      " [ 0.24276287]]\n",
      "Current iteration=2, loss=36047.31539979537\n",
      "t [[-0.38689647]\n",
      " [-0.08963474]\n",
      " [ 0.03346342]\n",
      " ...\n",
      " [ 0.42801146]\n",
      " [-0.31699339]\n",
      " [ 0.34963377]]\n",
      "t [[-0.38689647]\n",
      " [-0.08963474]\n",
      " [ 0.03346342]\n",
      " ...\n",
      " [ 0.42801146]\n",
      " [-0.31699339]\n",
      " [ 0.34963377]]\n",
      "t [[-0.4973672 ]\n",
      " [-0.11827713]\n",
      " [ 0.04624936]\n",
      " ...\n",
      " [ 0.54649402]\n",
      " [-0.40585198]\n",
      " [ 0.44798253]]\n",
      "t [[-0.4973672 ]\n",
      " [-0.11827713]\n",
      " [ 0.04624936]\n",
      " ...\n",
      " [ 0.54649402]\n",
      " [-0.40585198]\n",
      " [ 0.44798253]]\n",
      "Current iteration=4, loss=34822.595196251845\n",
      "t [[-0.60005159]\n",
      " [-0.14603721]\n",
      " [ 0.05928012]\n",
      " ...\n",
      " [ 0.65477669]\n",
      " [-0.48754339]\n",
      " [ 0.53860559]]\n",
      "t [[-0.60005159]\n",
      " [-0.14603721]\n",
      " [ 0.05928012]\n",
      " ...\n",
      " [ 0.65477669]\n",
      " [-0.48754339]\n",
      " [ 0.53860559]]\n",
      "t [[-0.69570501]\n",
      " [-0.17283889]\n",
      " [ 0.07232721]\n",
      " ...\n",
      " [ 0.75386019]\n",
      " [-0.5627339 ]\n",
      " [ 0.62223386]]\n",
      "t [[-0.69570501]\n",
      " [-0.17283889]\n",
      " [ 0.07232721]\n",
      " ...\n",
      " [ 0.75386019]\n",
      " [-0.5627339 ]\n",
      " [ 0.62223386]]\n",
      "Current iteration=6, loss=33906.56485988242\n",
      "t [[-0.78500274]\n",
      " [-0.19864364]\n",
      " [ 0.08521984]\n",
      " ...\n",
      " [ 0.84464878]\n",
      " [-0.63203212]\n",
      " [ 0.69953068]]\n",
      "t [[-0.78500274]\n",
      " [-0.19864364]\n",
      " [ 0.08521984]\n",
      " ...\n",
      " [ 0.84464878]\n",
      " [-0.63203212]\n",
      " [ 0.69953068]]\n",
      "t [[-0.86854549]\n",
      " [-0.22344049]\n",
      " [ 0.09783339]\n",
      " ...\n",
      " [ 0.92795386]\n",
      " [-0.69598935]\n",
      " [ 0.77109359]]\n",
      "t [[-0.86854549]\n",
      " [-0.22344049]\n",
      " [ 0.09783339]\n",
      " ...\n",
      " [ 0.92795386]\n",
      " [-0.69598935]\n",
      " [ 0.77109359]]\n",
      "Current iteration=8, loss=33208.87470789662\n",
      "t [[-0.94686629]\n",
      " [-0.2472381 ]\n",
      " [ 0.11007945]\n",
      " ...\n",
      " [ 1.0045004 ]\n",
      " [-0.75510226]\n",
      " [ 0.83745833]]\n",
      "t [[-0.94686629]\n",
      " [-0.2472381 ]\n",
      " [ 0.11007945]\n",
      " ...\n",
      " [ 1.0045004 ]\n",
      " [-0.75510226]\n",
      " [ 0.83745833]]\n",
      "t [[-1.02043771]\n",
      " [-0.2700586 ]\n",
      " [ 0.12189779]\n",
      " ...\n",
      " [ 1.07493458]\n",
      " [-0.80981678]\n",
      " [ 0.89910403]]\n",
      "loss=32667.492443387415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.12829822]\n",
      " [-0.02698987]\n",
      " [ 0.02678754]\n",
      " ...\n",
      " [-0.03386065]\n",
      " [-0.04209981]\n",
      " [-0.00885032]]\n",
      "t [[-0.12829822]\n",
      " [-0.02698987]\n",
      " [ 0.02678754]\n",
      " ...\n",
      " [-0.03386065]\n",
      " [-0.04209981]\n",
      " [-0.00885032]]\n",
      "t [[-0.24442182]\n",
      " [-0.04866852]\n",
      " [ 0.05207444]\n",
      " ...\n",
      " [-0.06561724]\n",
      " [-0.0810842 ]\n",
      " [-0.01765235]]\n",
      "t [[-0.24442182]\n",
      " [-0.04866852]\n",
      " [ 0.05207444]\n",
      " ...\n",
      " [-0.06561724]\n",
      " [-0.0810842 ]\n",
      " [-0.01765235]]\n",
      "Current iteration=2, loss=35910.69616361191\n",
      "t [[-0.34939554]\n",
      " [-0.06616464]\n",
      " [ 0.07573715]\n",
      " ...\n",
      " [-0.09541463]\n",
      " [-0.11722627]\n",
      " [-0.02632371]]\n",
      "t [[-0.34939554]\n",
      " [-0.06616464]\n",
      " [ 0.07573715]\n",
      " ...\n",
      " [-0.09541463]\n",
      " [-0.11722627]\n",
      " [-0.02632371]]\n",
      "t [[-0.44423001]\n",
      " [-0.08039841]\n",
      " [ 0.0977327 ]\n",
      " ...\n",
      " [-0.1233967 ]\n",
      " [-0.1507856 ]\n",
      " [-0.03480664]]\n",
      "t [[-0.44423001]\n",
      " [-0.08039841]\n",
      " [ 0.0977327 ]\n",
      " ...\n",
      " [-0.1233967 ]\n",
      " [-0.1507856 ]\n",
      " [-0.03480664]]\n",
      "Current iteration=4, loss=34617.08044780061\n",
      "t [[-0.52988862]\n",
      " [-0.09210722]\n",
      " [ 0.11807669]\n",
      " ...\n",
      " [-0.14970156]\n",
      " [-0.182003  ]\n",
      " [-0.04306274]]\n",
      "t [[-0.52988862]\n",
      " [-0.09210722]\n",
      " [ 0.11807669]\n",
      " ...\n",
      " [-0.14970156]\n",
      " [-0.182003  ]\n",
      " [-0.04306274]]\n",
      "t [[-0.60726824]\n",
      " [-0.10187516]\n",
      " [ 0.1368244 ]\n",
      " ...\n",
      " [-0.17445889]\n",
      " [-0.21109821]\n",
      " [-0.05106828]]\n",
      "t [[-0.60726824]\n",
      " [-0.10187516]\n",
      " [ 0.1368244 ]\n",
      " ...\n",
      " [-0.17445889]\n",
      " [-0.21109821]\n",
      " [-0.05106828]]\n",
      "Current iteration=6, loss=33670.18860002869\n",
      "t [[-0.67719033]\n",
      " [-0.11016153]\n",
      " [ 0.15405588]\n",
      " ...\n",
      " [-0.19778864]\n",
      " [-0.23826956]\n",
      " [-0.05881026]]\n",
      "t [[-0.67719033]\n",
      " [-0.11016153]\n",
      " [ 0.15405588]\n",
      " ...\n",
      " [-0.19778864]\n",
      " [-0.23826956]\n",
      " [-0.05881026]]\n",
      "t [[-0.7403989 ]\n",
      " [-0.11732583]\n",
      " [ 0.16986489]\n",
      " ...\n",
      " [-0.21980082]\n",
      " [-0.26369487]\n",
      " [-0.06628331]]\n",
      "t [[-0.7403989 ]\n",
      " [-0.11732583]\n",
      " [ 0.16986489]\n",
      " ...\n",
      " [-0.21980082]\n",
      " [-0.26369487]\n",
      " [-0.06628331]]\n",
      "Current iteration=8, loss=32962.27045636184\n",
      "t [[-0.7975628 ]\n",
      " [-0.1236488 ]\n",
      " [ 0.18435112]\n",
      " ...\n",
      " [-0.24059575]\n",
      " [-0.28753291]\n",
      " [-0.07348738]]\n",
      "t [[-0.7975628 ]\n",
      " [-0.1236488 ]\n",
      " [ 0.18435112]\n",
      " ...\n",
      " [-0.24059575]\n",
      " [-0.28753291]\n",
      " [-0.07348738]]\n",
      "t [[-0.84928045]\n",
      " [-0.12934941]\n",
      " [ 0.19761487]\n",
      " ...\n",
      " [-0.26026461]\n",
      " [-0.30992521]\n",
      " [-0.08042596]]\n",
      "loss=32421.51485965908\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.15067564]\n",
      " [-0.03394531]\n",
      " [ 0.01325949]\n",
      " ...\n",
      " [-0.03254857]\n",
      " [-0.0423187 ]\n",
      " [-0.00878386]]\n",
      "t [[-0.15067564]\n",
      " [-0.03394531]\n",
      " [ 0.01325949]\n",
      " ...\n",
      " [-0.03254857]\n",
      " [-0.0423187 ]\n",
      " [-0.00878386]]\n",
      "t [[-0.28900492]\n",
      " [-0.06736592]\n",
      " [ 0.02771273]\n",
      " ...\n",
      " [-0.06306345]\n",
      " [-0.08149552]\n",
      " [-0.01758046]]\n",
      "t [[-0.28900492]\n",
      " [-0.06736592]\n",
      " [ 0.02771273]\n",
      " ...\n",
      " [-0.06306345]\n",
      " [-0.08149552]\n",
      " [-0.01758046]]\n",
      "Current iteration=2, loss=35895.408122409404\n",
      "t [[-0.41623792]\n",
      " [-0.09996407]\n",
      " [ 0.04282381]\n",
      " ...\n",
      " [-0.09169214]\n",
      " [-0.1178072 ]\n",
      " [-0.02629046]]\n",
      "t [[-0.41623792]\n",
      " [-0.09996407]\n",
      " [ 0.04282381]\n",
      " ...\n",
      " [-0.09169214]\n",
      " [-0.1178072 ]\n",
      " [-0.02629046]]\n",
      "t [[-0.53352945]\n",
      " [-0.13153976]\n",
      " [ 0.05818097]\n",
      " ...\n",
      " [-0.11857892]\n",
      " [-0.15151674]\n",
      " [-0.03484292]]\n",
      "t [[-0.53352945]\n",
      " [-0.13153976]\n",
      " [ 0.05818097]\n",
      " ...\n",
      " [-0.11857892]\n",
      " [-0.15151674]\n",
      " [-0.03484292]]\n",
      "Current iteration=4, loss=34592.946055784196\n",
      "t [[-0.64192357]\n",
      " [-0.16197074]\n",
      " [ 0.07347763]\n",
      " ...\n",
      " [-0.14386075]\n",
      " [-0.18286796]\n",
      " [-0.04318942]]\n",
      "t [[-0.64192357]\n",
      " [-0.16197074]\n",
      " [ 0.07347763]\n",
      " ...\n",
      " [-0.14386075]\n",
      " [-0.18286796]\n",
      " [-0.04318942]]\n",
      "t [[-0.74235112]\n",
      " [-0.1911936 ]\n",
      " [ 0.08849233]\n",
      " ...\n",
      " [-0.16766507]\n",
      " [-0.21208322]\n",
      " [-0.05129871]]\n",
      "t [[-0.74235112]\n",
      " [-0.1911936 ]\n",
      " [ 0.08849233]\n",
      " ...\n",
      " [-0.16766507]\n",
      " [-0.21208322]\n",
      " [-0.05129871]]\n",
      "Current iteration=6, loss=33640.57399383147\n",
      "t [[-0.83563491]\n",
      " [-0.21918761]\n",
      " [ 0.10307045]\n",
      " ...\n",
      " [-0.19010902]\n",
      " [-0.23936312]\n",
      " [-0.05915218]]\n",
      "t [[-0.83563491]\n",
      " [-0.21918761]\n",
      " [ 0.10307045]\n",
      " ...\n",
      " [-0.19010902]\n",
      " [-0.23936312]\n",
      " [-0.05915218]]\n",
      "t [[-0.92249887]\n",
      " [-0.24596181]\n",
      " [ 0.11710866]\n",
      " ...\n",
      " [-0.21129937]\n",
      " [-0.26488737]\n",
      " [-0.06674035]]\n",
      "t [[-0.92249887]\n",
      " [-0.24596181]\n",
      " [ 0.11710866]\n",
      " ...\n",
      " [-0.21129937]\n",
      " [-0.26488737]\n",
      " [-0.06674035]]\n",
      "Current iteration=8, loss=32928.901789371346\n",
      "t [[-1.0035786 ]\n",
      " [-0.27154521]\n",
      " [ 0.13054222]\n",
      " ...\n",
      " [-0.23133306]\n",
      " [-0.28881636]\n",
      " [-0.07406017]]\n",
      "t [[-1.0035786 ]\n",
      " [-0.27154521]\n",
      " [ 0.13054222]\n",
      " ...\n",
      " [-0.23133306]\n",
      " [-0.28881636]\n",
      " [-0.07406017]]\n",
      "t [[-1.07943205]\n",
      " [-0.29597953]\n",
      " [ 0.14333508]\n",
      " ...\n",
      " [-0.25029786]\n",
      " [-0.31129297]\n",
      " [-0.08111298]]\n",
      "loss=32385.257559876478\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.15106101]\n",
      " [-0.03501098]\n",
      " [ 0.0090988 ]\n",
      " ...\n",
      " [-0.03217321]\n",
      " [-0.04256727]\n",
      " [-0.00860962]]\n",
      "t [[-0.15106101]\n",
      " [-0.03501098]\n",
      " [ 0.0090988 ]\n",
      " ...\n",
      " [-0.03217321]\n",
      " [-0.04256727]\n",
      " [-0.00860962]]\n",
      "t [[-0.28975609]\n",
      " [-0.06933962]\n",
      " [ 0.02004362]\n",
      " ...\n",
      " [-0.06242886]\n",
      " [-0.08197516]\n",
      " [-0.01723964]]\n",
      "t [[-0.28975609]\n",
      " [-0.06933962]\n",
      " [ 0.02004362]\n",
      " ...\n",
      " [-0.06242886]\n",
      " [-0.08197516]\n",
      " [-0.01723964]]\n",
      "Current iteration=2, loss=35871.658769231006\n",
      "t [[-0.41733103]\n",
      " [-0.10272436]\n",
      " [ 0.0322158 ]\n",
      " ...\n",
      " [-0.09089394]\n",
      " [-0.11850226]\n",
      " [-0.02579584]]\n",
      "t [[-0.41733103]\n",
      " [-0.10272436]\n",
      " [ 0.0322158 ]\n",
      " ...\n",
      " [-0.09089394]\n",
      " [-0.11850226]\n",
      " [-0.02579584]]\n",
      "t [[-0.53493952]\n",
      " [-0.13499255]\n",
      " [ 0.04512635]\n",
      " ...\n",
      " [-0.11769512]\n",
      " [-0.15241335]\n",
      " [-0.03421027]]\n",
      "t [[-0.53493952]\n",
      " [-0.13499255]\n",
      " [ 0.04512635]\n",
      " ...\n",
      " [-0.11769512]\n",
      " [-0.15241335]\n",
      " [-0.03421027]]\n",
      "Current iteration=4, loss=34552.411097269345\n",
      "t [[-0.64362621]\n",
      " [-0.16604265]\n",
      " [ 0.05839901]\n",
      " ...\n",
      " [-0.14295446]\n",
      " [-0.18395393]\n",
      " [-0.04243591]]\n",
      "t [[-0.64362621]\n",
      " [-0.16604265]\n",
      " [ 0.05839901]\n",
      " ...\n",
      " [-0.14295446]\n",
      " [-0.18395393]\n",
      " [-0.04243591]]\n",
      "t [[-0.74432344]\n",
      " [-0.19582691]\n",
      " [ 0.07175106]\n",
      " ...\n",
      " [-0.16678702]\n",
      " [-0.21334784]\n",
      " [-0.05044188]]\n",
      "t [[-0.74432344]\n",
      " [-0.19582691]\n",
      " [ 0.07175106]\n",
      " ...\n",
      " [-0.16678702]\n",
      " [-0.21334784]\n",
      " [-0.05044188]]\n",
      "Current iteration=6, loss=33587.9925517065\n",
      "t [[-0.83785584]\n",
      " [-0.22433653]\n",
      " [ 0.08497479]\n",
      " ...\n",
      " [-0.18929967]\n",
      " [-0.24079693]\n",
      " [-0.05820924]]\n",
      "t [[-0.83785584]\n",
      " [-0.22433653]\n",
      " [ 0.08497479]\n",
      " ...\n",
      " [-0.18929967]\n",
      " [-0.24079693]\n",
      " [-0.05820924]]\n",
      "t [[-0.92494921]\n",
      " [-0.25158967]\n",
      " [ 0.09792133]\n",
      " ...\n",
      " [-0.21059077]\n",
      " [-0.266482  ]\n",
      " [-0.06572776]]\n",
      "t [[-0.92494921]\n",
      " [-0.25158967]\n",
      " [ 0.09792133]\n",
      " ...\n",
      " [-0.21059077]\n",
      " [-0.266482  ]\n",
      " [-0.06572776]]\n",
      "Current iteration=8, loss=32867.49701354785\n",
      "t [[-1.00624091]\n",
      " [-0.2776224 ]\n",
      " [ 0.11048721]\n",
      " ...\n",
      " [-0.23075034]\n",
      " [-0.29056434]\n",
      " [-0.07299337]]\n",
      "t [[-1.00624091]\n",
      " [-0.2776224 ]\n",
      " [ 0.11048721]\n",
      " ...\n",
      " [-0.23075034]\n",
      " [-0.29056434]\n",
      " [-0.07299337]]\n",
      "t [[-1.08229051]\n",
      " [-0.30248189]\n",
      " [ 0.1226035 ]\n",
      " ...\n",
      " [-0.24986052]\n",
      " [-0.31318758]\n",
      " [-0.0800063 ]]\n",
      "loss=32317.2658889363\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.14986951]\n",
      " [-0.03259723]\n",
      " [ 0.01066927]\n",
      " ...\n",
      " [ 0.16797608]\n",
      " [-0.12368149]\n",
      " [ 0.13624814]]\n",
      "t [[-0.14986951]\n",
      " [-0.03259723]\n",
      " [ 0.01066927]\n",
      " ...\n",
      " [ 0.16797608]\n",
      " [-0.12368149]\n",
      " [ 0.13624814]]\n",
      "t [[-0.28753176]\n",
      " [-0.06484396]\n",
      " [ 0.0229701 ]\n",
      " ...\n",
      " [ 0.32004026]\n",
      " [-0.23640832]\n",
      " [ 0.26058836]]\n",
      "t [[-0.28753176]\n",
      " [-0.06484396]\n",
      " [ 0.0229701 ]\n",
      " ...\n",
      " [ 0.32004026]\n",
      " [-0.23640832]\n",
      " [ 0.26058836]]\n",
      "Current iteration=2, loss=35934.91460708398\n",
      "t [[-0.41422232]\n",
      " [-0.0964081 ]\n",
      " [ 0.03630292]\n",
      " ...\n",
      " [ 0.45775337]\n",
      " [-0.33917538]\n",
      " [ 0.37414092]]\n",
      "t [[-0.41422232]\n",
      " [-0.0964081 ]\n",
      " [ 0.03630292]\n",
      " ...\n",
      " [ 0.45775337]\n",
      " [-0.33917538]\n",
      " [ 0.37414092]]\n",
      "t [[-0.53108122]\n",
      " [-0.12706184]\n",
      " [ 0.05019875]\n",
      " ...\n",
      " [ 0.58258542]\n",
      " [-0.43293697]\n",
      " [ 0.4779688 ]]\n",
      "t [[-0.53108122]\n",
      " [-0.12706184]\n",
      " [ 0.05019875]\n",
      " ...\n",
      " [ 0.58258542]\n",
      " [-0.43293697]\n",
      " [ 0.4779688 ]]\n",
      "Current iteration=4, loss=34657.86980004088\n",
      "t [[-0.63913825]\n",
      " [-0.15666083]\n",
      " [ 0.06430071]\n",
      " ...\n",
      " [ 0.69588675]\n",
      " [-0.51858326]\n",
      " [ 0.57305434]]\n",
      "t [[-0.63913825]\n",
      " [-0.15666083]\n",
      " [ 0.06430071]\n",
      " ...\n",
      " [ 0.69588675]\n",
      " [-0.51858326]\n",
      " [ 0.57305434]]\n",
      "t [[-0.73931103]\n",
      " [-0.18512434]\n",
      " [ 0.07834404]\n",
      " ...\n",
      " [ 0.79887707]\n",
      " [-0.59692895]\n",
      " [ 0.66028951]]\n",
      "t [[-0.73931103]\n",
      " [-0.18512434]\n",
      " [ 0.07834404]\n",
      " ...\n",
      " [ 0.79887707]\n",
      " [-0.59692895]\n",
      " [ 0.66028951]]\n",
      "Current iteration=6, loss=33722.06860352628\n",
      "t [[-0.83241043]\n",
      " [-0.21241817]\n",
      " [ 0.09213727]\n",
      " ...\n",
      " [ 0.89264618]\n",
      " [-0.66871022]\n",
      " [ 0.74047488]]\n",
      "t [[-0.83241043]\n",
      " [-0.21241817]\n",
      " [ 0.09213727]\n",
      " ...\n",
      " [ 0.89264618]\n",
      " [-0.66871022]\n",
      " [ 0.74047488]]\n",
      "t [[-0.9191498 ]\n",
      " [-0.238541  ]\n",
      " [ 0.10554598]\n",
      " ...\n",
      " [ 0.97816118]\n",
      " [-0.73458675]\n",
      " [ 0.81432371]]\n",
      "t [[-0.9191498 ]\n",
      " [-0.238541  ]\n",
      " [ 0.10554598]\n",
      " ...\n",
      " [ 0.97816118]\n",
      " [-0.73458675]\n",
      " [ 0.81432371]]\n",
      "Current iteration=8, loss=33021.4708114047\n",
      "t [[-1.00015549]\n",
      " [-0.26351382]\n",
      " [ 0.11847942]\n",
      " ...\n",
      " [ 1.05627692]\n",
      " [-0.79514664]\n",
      " [ 0.88246885]]\n",
      "t [[-1.00015549]\n",
      " [-0.26351382]\n",
      " [ 0.11847942]\n",
      " ...\n",
      " [ 1.05627692]\n",
      " [-0.79514664]\n",
      " [ 0.88246885]]\n",
      "t [[-1.07597742]\n",
      " [-0.2873722 ]\n",
      " [ 0.13087978]\n",
      " ...\n",
      " [ 1.1277477 ]\n",
      " [-0.85091262]\n",
      " [ 0.94547056]]\n",
      "loss=32485.46284510595\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.13746238]\n",
      " [-0.02891772]\n",
      " [ 0.02870093]\n",
      " ...\n",
      " [-0.03627927]\n",
      " [-0.04510693]\n",
      " [-0.00948249]]\n",
      "t [[-0.13746238]\n",
      " [-0.02891772]\n",
      " [ 0.02870093]\n",
      " ...\n",
      " [-0.03627927]\n",
      " [-0.04510693]\n",
      " [-0.00948249]]\n",
      "t [[-0.26095111]\n",
      " [-0.05173954]\n",
      " [ 0.05567929]\n",
      " ...\n",
      " [-0.07014367]\n",
      " [-0.08663814]\n",
      " [-0.01890956]]\n",
      "t [[-0.26095111]\n",
      " [-0.05173954]\n",
      " [ 0.05567929]\n",
      " ...\n",
      " [-0.07014367]\n",
      " [-0.08663814]\n",
      " [-0.01890956]]\n",
      "Current iteration=2, loss=35798.874995224396\n",
      "t [[-0.37173111]\n",
      " [-0.06985508]\n",
      " [ 0.08078366]\n",
      " ...\n",
      " [-0.10177242]\n",
      " [-0.1249307 ]\n",
      " [-0.02818004]]\n",
      "t [[-0.37173111]\n",
      " [-0.06985508]\n",
      " [ 0.08078366]\n",
      " ...\n",
      " [-0.10177242]\n",
      " [-0.1249307 ]\n",
      " [-0.02818004]]\n",
      "t [[-0.47104478]\n",
      " [-0.08437647]\n",
      " [ 0.10396892]\n",
      " ...\n",
      " [-0.13134257]\n",
      " [-0.16030285]\n",
      " [-0.03722527]]\n",
      "t [[-0.47104478]\n",
      " [-0.08437647]\n",
      " [ 0.10396892]\n",
      " ...\n",
      " [-0.13134257]\n",
      " [-0.16030285]\n",
      " [-0.03722527]]\n",
      "Current iteration=4, loss=34456.9446637019\n",
      "t [[-0.56006701]\n",
      " [-0.09617705]\n",
      " [ 0.12526467]\n",
      " ...\n",
      " [-0.15902271]\n",
      " [-0.19304706]\n",
      " [-0.0460015 ]]\n",
      "t [[-0.56006701]\n",
      " [-0.09617705]\n",
      " [ 0.12526467]\n",
      " ...\n",
      " [-0.15902271]\n",
      " [-0.19304706]\n",
      " [-0.0460015 ]]\n",
      "t [[-0.63988083]\n",
      " [-0.10593426]\n",
      " [ 0.14474884]\n",
      " ...\n",
      " [-0.18496961]\n",
      " [-0.22342745]\n",
      " [-0.05448332]]\n",
      "t [[-0.63988083]\n",
      " [-0.10593426]\n",
      " [ 0.14474884]\n",
      " ...\n",
      " [-0.18496961]\n",
      " [-0.22342745]\n",
      " [-0.05448332]]\n",
      "Current iteration=6, loss=33494.394649375354\n",
      "t [[-0.71146759]\n",
      " [-0.1141697 ]\n",
      " [ 0.16252766]\n",
      " ...\n",
      " [-0.20932696]\n",
      " [-0.25167991]\n",
      " [-0.06265831]]\n",
      "t [[-0.71146759]\n",
      " [-0.1141697 ]\n",
      " [ 0.16252766]\n",
      " ...\n",
      " [-0.20932696]\n",
      " [-0.25167991]\n",
      " [-0.06265831]]\n",
      "t [[-0.77570649]\n",
      " [-0.12128327]\n",
      " [ 0.17872134]\n",
      " ...\n",
      " [-0.23222532]\n",
      " [-0.27801376]\n",
      " [-0.07052291]]\n",
      "t [[-0.77570649]\n",
      " [-0.12128327]\n",
      " [ 0.17872134]\n",
      " ...\n",
      " [-0.23222532]\n",
      " [-0.27801376]\n",
      " [-0.07052291]]\n",
      "Current iteration=8, loss=32786.71653806421\n",
      "t [[-0.83337951]\n",
      " [-0.12758095]\n",
      " [ 0.19345448]\n",
      " ...\n",
      " [-0.25378267]\n",
      " [-0.30261409]\n",
      " [-0.07807952]]\n",
      "t [[-0.83337951]\n",
      " [-0.12758095]\n",
      " [ 0.19345448]\n",
      " ...\n",
      " [-0.25378267]\n",
      " [-0.30261409]\n",
      " [-0.07807952]]\n",
      "t [[-0.88517918]\n",
      " [-0.13329661]\n",
      " [ 0.20684984]\n",
      " ...\n",
      " [-0.27410547]\n",
      " [-0.32564437]\n",
      " [-0.08533433]]\n",
      "loss=32253.407272907076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.16143818]\n",
      " [-0.03636997]\n",
      " [ 0.0142066 ]\n",
      " ...\n",
      " [-0.03487346]\n",
      " [-0.04534146]\n",
      " [-0.00941128]]\n",
      "t [[-0.16143818]\n",
      " [-0.03636997]\n",
      " [ 0.0142066 ]\n",
      " ...\n",
      " [-0.03487346]\n",
      " [-0.04534146]\n",
      " [-0.00941128]]\n",
      "t [[-0.30870613]\n",
      " [-0.07213739]\n",
      " [ 0.02978317]\n",
      " ...\n",
      " [-0.06741283]\n",
      " [-0.08707681]\n",
      " [-0.01883719]]\n",
      "t [[-0.30870613]\n",
      " [-0.07213739]\n",
      " [ 0.02978317]\n",
      " ...\n",
      " [-0.06741283]\n",
      " [-0.08707681]\n",
      " [-0.01883719]]\n",
      "Current iteration=2, loss=35782.727575707635\n",
      "t [[-0.44334666]\n",
      " [-0.10693605]\n",
      " [ 0.04607033]\n",
      " ...\n",
      " [-0.09780048]\n",
      " [-0.12554767]\n",
      " [-0.02815568]]\n",
      "t [[-0.44334666]\n",
      " [-0.10693605]\n",
      " [ 0.04607033]\n",
      " ...\n",
      " [-0.09780048]\n",
      " [-0.12554767]\n",
      " [-0.02815568]]\n",
      "t [[-0.56677196]\n",
      " [-0.14052936]\n",
      " [ 0.06257348]\n",
      " ...\n",
      " [-0.12621376]\n",
      " [-0.16107641]\n",
      " [-0.03728224]]\n",
      "t [[-0.56677196]\n",
      " [-0.14052936]\n",
      " [ 0.06257348]\n",
      " ...\n",
      " [-0.12621376]\n",
      " [-0.16107641]\n",
      " [-0.03728224]]\n",
      "Current iteration=4, loss=34431.80759996647\n",
      "t [[-0.6802446 ]\n",
      " [-0.1727811 ]\n",
      " [ 0.07893469]\n",
      " ...\n",
      " [-0.15281954]\n",
      " [-0.19395914]\n",
      " [-0.04616135]]\n",
      "t [[-0.6802446 ]\n",
      " [-0.1727811 ]\n",
      " [ 0.07893469]\n",
      " ...\n",
      " [-0.15281954]\n",
      " [-0.19395914]\n",
      " [-0.04616135]]\n",
      "t [[-0.78487705]\n",
      " [-0.20362865]\n",
      " [ 0.09490405]\n",
      " ...\n",
      " [-0.17777167]\n",
      " [-0.22446309]\n",
      " [-0.05475903]]\n",
      "t [[-0.78487705]\n",
      " [-0.20362865]\n",
      " [ 0.09490405]\n",
      " ...\n",
      " [-0.17777167]\n",
      " [-0.22446309]\n",
      " [-0.05475903]]\n",
      "Current iteration=6, loss=33463.81922709945\n",
      "t [[-0.88164134]\n",
      " [-0.23306078]\n",
      " [ 0.11031421]\n",
      " ...\n",
      " [-0.20121013]\n",
      " [-0.25282679]\n",
      " [-0.06305659]]\n",
      "t [[-0.88164134]\n",
      " [-0.23306078]\n",
      " [ 0.11031421]\n",
      " ...\n",
      " [-0.20121013]\n",
      " [-0.25282679]\n",
      " [-0.06305659]]\n",
      "t [[-0.97138327]\n",
      " [-0.26110063]\n",
      " [ 0.12505938]\n",
      " ...\n",
      " [-0.22326139]\n",
      " [-0.27926176]\n",
      " [-0.07104607]]\n",
      "t [[-0.97138327]\n",
      " [-0.26110063]\n",
      " [ 0.12505938]\n",
      " ...\n",
      " [-0.22326139]\n",
      " [-0.27926176]\n",
      " [-0.07104607]]\n",
      "Current iteration=8, loss=32752.426587606977\n",
      "t [[-1.05483784]\n",
      " [-0.28779317]\n",
      " [ 0.13907878]\n",
      " ...\n",
      " [-0.24403927]\n",
      " [-0.30395491]\n",
      " [-0.07872672]]\n",
      "t [[-1.05483784]\n",
      " [-0.28779317]\n",
      " [ 0.13907878]\n",
      " ...\n",
      " [-0.24403927]\n",
      " [-0.30395491]\n",
      " [-0.07872672]]\n",
      "t [[-1.13264419]\n",
      " [-0.31319627]\n",
      " [ 0.15234406]\n",
      " ...\n",
      " [-0.26364596]\n",
      " [-0.32707123]\n",
      " [-0.08610254]]\n",
      "loss=32216.215001749806\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.16185108]\n",
      " [-0.03751177]\n",
      " [ 0.00974872]\n",
      " ...\n",
      " [-0.0344713 ]\n",
      " [-0.04560779]\n",
      " [-0.00922459]]\n",
      "t [[-0.16185108]\n",
      " [-0.03751177]\n",
      " [ 0.00974872]\n",
      " ...\n",
      " [-0.0344713 ]\n",
      " [-0.04560779]\n",
      " [-0.00922459]]\n",
      "t [[-0.30950951]\n",
      " [-0.07424001]\n",
      " [ 0.021616  ]\n",
      " ...\n",
      " [-0.06674177]\n",
      " [-0.08758939]\n",
      " [-0.0184726 ]]\n",
      "t [[-0.30950951]\n",
      " [-0.07424001]\n",
      " [ 0.021616  ]\n",
      " ...\n",
      " [-0.06674177]\n",
      " [-0.08758939]\n",
      " [-0.0184726 ]]\n",
      "Current iteration=2, loss=35757.5113227269\n",
      "t [[-0.44451296]\n",
      " [-0.10986308]\n",
      " [ 0.03484038]\n",
      " ...\n",
      " [-0.09696855]\n",
      " [-0.12628871]\n",
      " [-0.02762828]]\n",
      "t [[-0.44451296]\n",
      " [-0.10986308]\n",
      " [ 0.03484038]\n",
      " ...\n",
      " [-0.09696855]\n",
      " [-0.12628871]\n",
      " [-0.02762828]]\n",
      "t [[-0.56827248]\n",
      " [-0.14417723]\n",
      " [ 0.04883276]\n",
      " ...\n",
      " [-0.12530758]\n",
      " [-0.16203034]\n",
      " [-0.03661055]]\n",
      "t [[-0.56827248]\n",
      " [-0.14417723]\n",
      " [ 0.04883276]\n",
      " ...\n",
      " [-0.12530758]\n",
      " [-0.16203034]\n",
      " [-0.03661055]]\n",
      "Current iteration=4, loss=34389.21084942178\n",
      "t [[-0.68205166]\n",
      " [-0.17707055]\n",
      " [ 0.06315109]\n",
      " ...\n",
      " [-0.15190796]\n",
      " [-0.1951124 ]\n",
      " [-0.0453654 ]]\n",
      "t [[-0.68205166]\n",
      " [-0.17707055]\n",
      " [ 0.06315109]\n",
      " ...\n",
      " [-0.15190796]\n",
      " [-0.1951124 ]\n",
      " [-0.0453654 ]]\n",
      "t [[-0.78696495]\n",
      " [-0.20849839]\n",
      " [ 0.07747251]\n",
      " ...\n",
      " [-0.17690892]\n",
      " [-0.22580388]\n",
      " [-0.053859  ]]\n",
      "t [[-0.78696495]\n",
      " [-0.20849839]\n",
      " [ 0.07747251]\n",
      " ...\n",
      " [-0.17690892]\n",
      " [-0.22580388]\n",
      " [-0.053859  ]]\n",
      "Current iteration=6, loss=33409.01844495715\n",
      "t [[-0.88398667]\n",
      " [-0.23846295]\n",
      " [ 0.09156738]\n",
      " ...\n",
      " [-0.20043862]\n",
      " [-0.25434478]\n",
      " [-0.06207207]]\n",
      "t [[-0.88398667]\n",
      " [-0.23846295]\n",
      " [ 0.09156738]\n",
      " ...\n",
      " [-0.20043862]\n",
      " [-0.25434478]\n",
      " [-0.06207207]]\n",
      "t [[-0.97396488]\n",
      " [-0.26699743]\n",
      " [ 0.10527731]\n",
      " ...\n",
      " [-0.22261396]\n",
      " [-0.28094784]\n",
      " [-0.06999552]]\n",
      "t [[-0.97396488]\n",
      " [-0.26699743]\n",
      " [ 0.10527731]\n",
      " ...\n",
      " [-0.22261396]\n",
      " [-0.28094784]\n",
      " [-0.06999552]]\n",
      "Current iteration=8, loss=32688.851612016446\n",
      "t [[-1.05763669]\n",
      " [-0.2941544 ]\n",
      " [ 0.11849742]\n",
      " ...\n",
      " [-0.24354103]\n",
      " [-0.30580099]\n",
      " [-0.07762728]]\n",
      "t [[-1.05763669]\n",
      " [-0.2941544 ]\n",
      " [ 0.11849742]\n",
      " ...\n",
      " [-0.24354103]\n",
      " [-0.30580099]\n",
      " [-0.07762728]]\n",
      "t [[-1.13564308]\n",
      " [-0.31999757]\n",
      " [ 0.13116244]\n",
      " ...\n",
      " [-0.26331586]\n",
      " [-0.32907005]\n",
      " [-0.08496992]]\n",
      "loss=32146.197126242354\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.16057448]\n",
      " [-0.0349256 ]\n",
      " [ 0.01143137]\n",
      " ...\n",
      " [ 0.17997437]\n",
      " [-0.13251588]\n",
      " [ 0.14598015]]\n",
      "t [[-0.16057448]\n",
      " [-0.0349256 ]\n",
      " [ 0.01143137]\n",
      " ...\n",
      " [ 0.17997437]\n",
      " [-0.13251588]\n",
      " [ 0.14598015]]\n",
      "t [[-0.30713834]\n",
      " [-0.06944854]\n",
      " [ 0.02473513]\n",
      " ...\n",
      " [ 0.34168601]\n",
      " [-0.25245852]\n",
      " [ 0.27829313]]\n",
      "t [[-0.30713834]\n",
      " [-0.06944854]\n",
      " [ 0.02473513]\n",
      " ...\n",
      " [ 0.34168601]\n",
      " [-0.25245852]\n",
      " [ 0.27829313]]\n",
      "Current iteration=2, loss=35824.57811115588\n",
      "t [[-0.44121651]\n",
      " [-0.10316031]\n",
      " [ 0.0391734 ]\n",
      " ...\n",
      " [ 0.48706116]\n",
      " [-0.3610559 ]\n",
      " [ 0.39832181]]\n",
      "t [[-0.44121651]\n",
      " [-0.10316031]\n",
      " [ 0.0391734 ]\n",
      " ...\n",
      " [ 0.48706116]\n",
      " [-0.3610559 ]\n",
      " [ 0.39832181]]\n",
      "t [[-0.56420294]\n",
      " [-0.13579068]\n",
      " [ 0.05418172]\n",
      " ...\n",
      " [ 0.61790015]\n",
      " [-0.45947856]\n",
      " [ 0.50736886]]\n",
      "t [[-0.56420294]\n",
      " [-0.13579068]\n",
      " [ 0.05418172]\n",
      " ...\n",
      " [ 0.61790015]\n",
      " [-0.45947856]\n",
      " [ 0.50736886]]\n",
      "Current iteration=4, loss=34499.715010245105\n",
      "t [[-0.67734279]\n",
      " [-0.16717732]\n",
      " [ 0.06934169]\n",
      " ...\n",
      " [ 0.73584008]\n",
      " [-0.54880843]\n",
      " [ 0.60662641]]\n",
      "t [[-0.67734279]\n",
      " [-0.16717732]\n",
      " [ 0.06934169]\n",
      " ...\n",
      " [ 0.73584008]\n",
      " [-0.54880843]\n",
      " [ 0.60662641]]\n",
      "t [[-0.78173259]\n",
      " [-0.19723761]\n",
      " [ 0.08435211]\n",
      " ...\n",
      " [ 0.84234333]\n",
      " [-0.63002536]\n",
      " [ 0.69716511]]\n",
      "t [[-0.78173259]\n",
      " [-0.19723761]\n",
      " [ 0.08435211]\n",
      " ...\n",
      " [ 0.84234333]\n",
      " [-0.63002536]\n",
      " [ 0.69716511]]\n",
      "Current iteration=6, loss=33548.2247529124\n",
      "t [[-0.87833021]\n",
      " [-0.22594516]\n",
      " [ 0.09900269]\n",
      " ...\n",
      " [ 0.93870175]\n",
      " [-0.70400516]\n",
      " [ 0.77993524]]\n",
      "t [[-0.87833021]\n",
      " [-0.22594516]\n",
      " [ 0.09900269]\n",
      " ...\n",
      " [ 0.93870175]\n",
      " [-0.70400516]\n",
      " [ 0.77993524]]\n",
      "t [[-0.96796906]\n",
      " [-0.25331165]\n",
      " [ 0.11315199]\n",
      " ...\n",
      " [ 1.02604914]\n",
      " [-0.77152417]\n",
      " [ 0.85577431]]\n",
      "t [[-0.96796906]\n",
      " [-0.25331165]\n",
      " [ 0.11315199]\n",
      " ...\n",
      " [ 1.02604914]\n",
      " [-0.77152417]\n",
      " [ 0.85577431]]\n",
      "Current iteration=8, loss=32847.59865806893\n",
      "t [[-1.05137349]\n",
      " [-0.27937343]\n",
      " [ 0.12670986]\n",
      " ...\n",
      " [ 1.10537721]\n",
      " [-0.83326735]\n",
      " [ 0.92541767]]\n",
      "t [[-1.05137349]\n",
      " [-0.27937343]\n",
      " [ 0.12670986]\n",
      " ...\n",
      " [ 1.10537721]\n",
      " [-0.83326735]\n",
      " [ 0.92541767]]\n",
      "t [[-1.12917352]\n",
      " [-0.30418183]\n",
      " [ 0.13962379]\n",
      " ...\n",
      " [ 1.17755247]\n",
      " [-0.88983755]\n",
      " [ 0.98951008]]\n",
      "loss=32318.681821126604\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.14662654]\n",
      " [-0.03084557]\n",
      " [ 0.03061433]\n",
      " ...\n",
      " [-0.03869788]\n",
      " [-0.04811406]\n",
      " [-0.01011466]]\n",
      "t [[-0.14662654]\n",
      " [-0.03084557]\n",
      " [ 0.03061433]\n",
      " ...\n",
      " [-0.03869788]\n",
      " [-0.04811406]\n",
      " [-0.01011466]]\n",
      "t [[-0.27735697]\n",
      " [-0.05475678]\n",
      " [ 0.05926886]\n",
      " ...\n",
      " [-0.07464882]\n",
      " [-0.0921605 ]\n",
      " [-0.02016628]]\n",
      "t [[-0.27735697]\n",
      " [-0.05475678]\n",
      " [ 0.05926886]\n",
      " ...\n",
      " [-0.07464882]\n",
      " [-0.0921605 ]\n",
      " [-0.02016628]]\n",
      "Current iteration=2, loss=35689.11795542378\n",
      "t [[-0.39373177]\n",
      " [-0.0734216 ]\n",
      " [ 0.08578053]\n",
      " ...\n",
      " [-0.10807149]\n",
      " [-0.13254984]\n",
      " [-0.03003229]]\n",
      "t [[-0.39373177]\n",
      " [-0.0734216 ]\n",
      " [ 0.08578053]\n",
      " ...\n",
      " [-0.10807149]\n",
      " [-0.13254984]\n",
      " [-0.03003229]]\n",
      "t [[-0.49725723]\n",
      " [-0.08816549]\n",
      " [ 0.11010396]\n",
      " ...\n",
      " [-0.13918075]\n",
      " [-0.16966681]\n",
      " [-0.03963221]]\n",
      "t [[-0.49725723]\n",
      " [-0.08816549]\n",
      " [ 0.11010396]\n",
      " ...\n",
      " [-0.13918075]\n",
      " [-0.16966681]\n",
      " [-0.03963221]]\n",
      "Current iteration=4, loss=34303.1990363024\n",
      "t [[-0.58934595]\n",
      " [-0.10000793]\n",
      " [ 0.13228719]\n",
      " ...\n",
      " [-0.16817942]\n",
      " [-0.20386162]\n",
      " [-0.04891709]]\n",
      "t [[-0.58934595]\n",
      " [-0.10000793]\n",
      " [ 0.13228719]\n",
      " ...\n",
      " [-0.16817942]\n",
      " [-0.20386162]\n",
      " [-0.04891709]]\n",
      "t [[-0.67128683]\n",
      " [-0.10972304]\n",
      " [ 0.15243584]\n",
      " ...\n",
      " [-0.19525432]\n",
      " [-0.23544721]\n",
      " [-0.05786042]]\n",
      "t [[-0.67128683]\n",
      " [-0.10972304]\n",
      " [ 0.15243584]\n",
      " ...\n",
      " [-0.19525432]\n",
      " [-0.23544721]\n",
      " [-0.05786042]]\n",
      "Current iteration=6, loss=33328.69686517179\n",
      "t [[-0.74423513]\n",
      " [-0.11789429]\n",
      " [ 0.1706868 ]\n",
      " ...\n",
      " [-0.22057507]\n",
      " [-0.26470014]\n",
      " [-0.066451  ]]\n",
      "t [[-0.74423513]\n",
      " [-0.11789429]\n",
      " [ 0.1706868 ]\n",
      " ...\n",
      " [-0.22057507]\n",
      " [-0.26470014]\n",
      " [-0.066451  ]]\n",
      "t [[-0.80921456]\n",
      " [-0.12495976]\n",
      " [ 0.18719033]\n",
      " ...\n",
      " [-0.24429436]\n",
      " [-0.2918633 ]\n",
      " [-0.07468779]]\n",
      "t [[-0.80921456]\n",
      " [-0.12495976]\n",
      " [ 0.18719033]\n",
      " ...\n",
      " [-0.24429436]\n",
      " [-0.2918633 ]\n",
      " [-0.07468779]]\n",
      "Current iteration=8, loss=32623.699765538535\n",
      "t [[-0.86712608]\n",
      " [-0.13124783]\n",
      " [ 0.20209851]\n",
      " ...\n",
      " [-0.26654907]\n",
      " [-0.31714943]\n",
      " [-0.0825762 ]]\n",
      "t [[-0.86712608]\n",
      " [-0.13124783]\n",
      " [ 0.20209851]\n",
      " ...\n",
      " [-0.26654907]\n",
      " [-0.31714943]\n",
      " [-0.0825762 ]]\n",
      "t [[-0.9187596 ]\n",
      " [-0.1370044 ]\n",
      " [ 0.21555831]\n",
      " ...\n",
      " [-0.28746171]\n",
      " [-0.34074476]\n",
      " [-0.09012555]]\n",
      "loss=32099.153086982093\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.17220073]\n",
      " [-0.03879464]\n",
      " [ 0.0151537 ]\n",
      " ...\n",
      " [-0.03719836]\n",
      " [-0.04836423]\n",
      " [-0.01003869]]\n",
      "t [[-0.17220073]\n",
      " [-0.03879464]\n",
      " [ 0.0151537 ]\n",
      " ...\n",
      " [-0.03719836]\n",
      " [-0.04836423]\n",
      " [-0.01003869]]\n",
      "t [[-0.32828237]\n",
      " [-0.07690342]\n",
      " [ 0.03186563]\n",
      " ...\n",
      " [-0.07174163]\n",
      " [-0.09262628]\n",
      " [-0.02009404]]\n",
      "t [[-0.32828237]\n",
      " [-0.07690342]\n",
      " [ 0.03186563]\n",
      " ...\n",
      " [-0.07174163]\n",
      " [-0.09262628]\n",
      " [-0.02009404]]\n",
      "Current iteration=2, loss=35672.1382458907\n",
      "t [[-0.47012347]\n",
      " [-0.11388205]\n",
      " [ 0.04933538]\n",
      " ...\n",
      " [-0.10385229]\n",
      " [-0.13320217]\n",
      " [-0.03001811]]\n",
      "t [[-0.47012347]\n",
      " [-0.11388205]\n",
      " [ 0.04933538]\n",
      " ...\n",
      " [-0.10385229]\n",
      " [-0.13320217]\n",
      " [-0.03001811]]\n",
      "t [[-0.59942704]\n",
      " [-0.14945484]\n",
      " [ 0.06697715]\n",
      " ...\n",
      " [-0.13374527]\n",
      " [-0.17048166]\n",
      " [-0.03971162]]\n",
      "t [[-0.59942704]\n",
      " [-0.14945484]\n",
      " [ 0.06697715]\n",
      " ...\n",
      " [-0.13374527]\n",
      " [-0.17048166]\n",
      " [-0.03971162]]\n",
      "Current iteration=4, loss=34277.12212677521\n",
      "t [[-0.71769835]\n",
      " [-0.18347326]\n",
      " [ 0.08437946]\n",
      " ...\n",
      " [-0.16162103]\n",
      " [-0.20481925]\n",
      " [-0.04911202]]\n",
      "t [[-0.71769835]\n",
      " [-0.18347326]\n",
      " [ 0.08437946]\n",
      " ...\n",
      " [-0.16162103]\n",
      " [-0.20481925]\n",
      " [-0.04911202]]\n",
      "t [[-0.826248  ]\n",
      " [-0.21587888]\n",
      " [ 0.10126515]\n",
      " ...\n",
      " [-0.18766255]\n",
      " [-0.23653154]\n",
      " [-0.05818311]]\n",
      "t [[-0.826248  ]\n",
      " [-0.21587888]\n",
      " [ 0.10126515]\n",
      " ...\n",
      " [-0.18766255]\n",
      " [-0.23653154]\n",
      " [-0.05818311]]\n",
      "Current iteration=6, loss=33297.23322682093\n",
      "t [[-0.92620794]\n",
      " [-0.24667378]\n",
      " [ 0.1174569 ]\n",
      " ...\n",
      " [-0.21203478]\n",
      " [-0.2658981 ]\n",
      " [-0.06690692]]\n",
      "t [[-0.92620794]\n",
      " [-0.24667378]\n",
      " [ 0.1174569 ]\n",
      " ...\n",
      " [-0.21203478]\n",
      " [-0.2658981 ]\n",
      " [-0.06690692]]\n",
      "t [[-1.01855251]\n",
      " [-0.27589866]\n",
      " [ 0.13284974]\n",
      " ...\n",
      " [-0.23488536]\n",
      " [-0.29316432]\n",
      " [-0.07527769]]\n",
      "t [[-1.01855251]\n",
      " [-0.27589866]\n",
      " [ 0.13284974]\n",
      " ...\n",
      " [-0.23488536]\n",
      " [-0.29316432]\n",
      " [-0.07527769]]\n",
      "Current iteration=8, loss=32588.551848493298\n",
      "t [[-1.10411999]\n",
      " [-0.30361725]\n",
      " [ 0.14739001]\n",
      " ...\n",
      " [-0.25634603]\n",
      " [-0.31854499]\n",
      " [-0.08329761]]\n",
      "t [[-1.10411999]\n",
      " [-0.30361725]\n",
      " [ 0.14739001]\n",
      " ...\n",
      " [-0.25634603]\n",
      " [-0.31854499]\n",
      " [-0.08329761]]\n",
      "t [[-1.18363275]\n",
      " [-0.32990578]\n",
      " [ 0.16105985]\n",
      " ...\n",
      " [-0.27653422]\n",
      " [-0.34222801]\n",
      " [-0.09097387]]\n",
      "loss=32061.07177338074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.17264115]\n",
      " [-0.04001255]\n",
      " [ 0.01039863]\n",
      " ...\n",
      " [-0.03676938]\n",
      " [-0.04864831]\n",
      " [-0.00983956]]\n",
      "t [[-0.17264115]\n",
      " [-0.04001255]\n",
      " [ 0.01039863]\n",
      " ...\n",
      " [-0.03676938]\n",
      " [-0.04864831]\n",
      " [-0.00983956]]\n",
      "t [[-0.32913778]\n",
      " [-0.07913336]\n",
      " [ 0.02320699]\n",
      " ...\n",
      " [-0.07103528]\n",
      " [-0.09317162]\n",
      " [-0.01970577]]\n",
      "t [[-0.32913778]\n",
      " [-0.07913336]\n",
      " [ 0.02320699]\n",
      " ...\n",
      " [-0.07103528]\n",
      " [-0.09317162]\n",
      " [-0.01970577]]\n",
      "Current iteration=2, loss=35645.48336570989\n",
      "t [[-0.4713623 ]\n",
      " [-0.11697218]\n",
      " [ 0.03750042]\n",
      " ...\n",
      " [-0.10298947]\n",
      " [-0.13398875]\n",
      " [-0.02945831]]\n",
      "t [[-0.4713623 ]\n",
      " [-0.11697218]\n",
      " [ 0.03750042]\n",
      " ...\n",
      " [-0.10298947]\n",
      " [-0.13398875]\n",
      " [-0.02945831]]\n",
      "t [[-0.6010167 ]\n",
      " [-0.15329228]\n",
      " [ 0.05257914]\n",
      " ...\n",
      " [-0.13282127]\n",
      " [-0.17149213]\n",
      " [-0.03900186]]\n",
      "t [[-0.6010167 ]\n",
      " [-0.15329228]\n",
      " [ 0.05257914]\n",
      " ...\n",
      " [-0.13282127]\n",
      " [-0.17149213]\n",
      " [-0.03900186]]\n",
      "Current iteration=4, loss=34232.55037466479\n",
      "t [[-0.71960771]\n",
      " [-0.18797323]\n",
      " [ 0.06793152]\n",
      " ...\n",
      " [-0.16071015]\n",
      " [-0.20603868]\n",
      " [-0.04827534]]\n",
      "t [[-0.71960771]\n",
      " [-0.18797323]\n",
      " [ 0.06793152]\n",
      " ...\n",
      " [-0.16071015]\n",
      " [-0.20603868]\n",
      " [-0.04827534]]\n",
      "t [[-0.8284485 ]\n",
      " [-0.22097693]\n",
      " [ 0.0831948 ]\n",
      " ...\n",
      " [-0.18682221]\n",
      " [-0.23794703]\n",
      " [-0.0572425 ]]\n",
      "t [[-0.8284485 ]\n",
      " [-0.22097693]\n",
      " [ 0.0831948 ]\n",
      " ...\n",
      " [-0.18682221]\n",
      " [-0.23794703]\n",
      " [-0.0572425 ]]\n",
      "Current iteration=6, loss=33240.34825315929\n",
      "t [[-0.92867384]\n",
      " [-0.25232034]\n",
      " [ 0.0981197 ]\n",
      " ...\n",
      " [-0.21130891]\n",
      " [-0.26749846]\n",
      " [-0.06588435]]\n",
      "t [[-0.92867384]\n",
      " [-0.25232034]\n",
      " [ 0.0981197 ]\n",
      " ...\n",
      " [-0.21130891]\n",
      " [-0.26749846]\n",
      " [-0.06588435]]\n",
      "t [[-1.02126074]\n",
      " [-0.28205506]\n",
      " [ 0.11254137]\n",
      " ...\n",
      " [-0.23430717]\n",
      " [-0.29493972]\n",
      " [-0.07419366]]\n",
      "t [[-1.02126074]\n",
      " [-0.28205506]\n",
      " [ 0.11254137]\n",
      " ...\n",
      " [-0.23430717]\n",
      " [-0.29493972]\n",
      " [-0.07419366]]\n",
      "Current iteration=8, loss=32522.97335521284\n",
      "t [[-1.10704993]\n",
      " [-0.31025299]\n",
      " [ 0.12635666]\n",
      " ...\n",
      " [-0.25594027]\n",
      " [-0.32048674]\n",
      " [-0.08217088]]\n",
      "t [[-1.10704993]\n",
      " [-0.31025299]\n",
      " [ 0.12635666]\n",
      " ...\n",
      " [-0.25594027]\n",
      " [-0.32048674]\n",
      " [-0.08217088]]\n",
      "t [[-1.18676585]\n",
      " [-0.33699643]\n",
      " [ 0.13950684]\n",
      " ...\n",
      " [-0.27631895]\n",
      " [-0.34432833]\n",
      " [-0.08982144]]\n",
      "loss=31989.212527452095\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.17127944]\n",
      " [-0.03725398]\n",
      " [ 0.01219346]\n",
      " ...\n",
      " [ 0.19197267]\n",
      " [-0.14135027]\n",
      " [ 0.15571216]]\n",
      "t [[-0.17127944]\n",
      " [-0.03725398]\n",
      " [ 0.01219346]\n",
      " ...\n",
      " [ 0.19197267]\n",
      " [-0.14135027]\n",
      " [ 0.15571216]]\n",
      "t [[-0.32662135]\n",
      " [-0.07404945]\n",
      " [ 0.02651661]\n",
      " ...\n",
      " [ 0.36317063]\n",
      " [-0.26839774]\n",
      " [ 0.29587733]]\n",
      "t [[-0.32662135]\n",
      " [-0.07404945]\n",
      " [ 0.02651661]\n",
      " ...\n",
      " [ 0.36317063]\n",
      " [-0.26839774]\n",
      " [ 0.29587733]]\n",
      "Current iteration=2, loss=35716.27484122724\n",
      "t [[-0.46788246]\n",
      " [-0.10989062]\n",
      " [ 0.04207347]\n",
      " ...\n",
      " [ 0.51593915]\n",
      " [-0.38263772]\n",
      " [ 0.42217958]]\n",
      "t [[-0.46788246]\n",
      " [-0.10989062]\n",
      " [ 0.04207347]\n",
      " ...\n",
      " [ 0.51593915]\n",
      " [-0.38263772]\n",
      " [ 0.42217958]]\n",
      "t [[-0.59674364]\n",
      " [-0.14446178]\n",
      " [ 0.05819421]\n",
      " ...\n",
      " [ 0.6524527 ]\n",
      " [-0.48548624]\n",
      " [ 0.53619331]]\n",
      "t [[-0.59674364]\n",
      " [-0.14446178]\n",
      " [ 0.05819421]\n",
      " ...\n",
      " [ 0.6524527 ]\n",
      " [-0.48548624]\n",
      " [ 0.53619331]]\n",
      "Current iteration=4, loss=34347.84959949114\n",
      "t [[-0.71468919]\n",
      " [-0.17758397]\n",
      " [ 0.07439567]\n",
      " ...\n",
      " [ 0.77466808]\n",
      " [-0.57823972]\n",
      " [ 0.63934478]]\n",
      "t [[-0.71468919]\n",
      " [-0.17758397]\n",
      " [ 0.07439567]\n",
      " ...\n",
      " [ 0.77466808]\n",
      " [-0.57823972]\n",
      " [ 0.63934478]]\n",
      "t [[-0.82301082]\n",
      " [-0.20917603]\n",
      " [ 0.09034083]\n",
      " ...\n",
      " [ 0.88431352]\n",
      " [-0.66205976]\n",
      " [ 0.73290073]]\n",
      "t [[-0.82301082]\n",
      " [-0.20917603]\n",
      " [ 0.09034083]\n",
      " ...\n",
      " [ 0.88431352]\n",
      " [-0.66205976]\n",
      " [ 0.73290073]]\n",
      "Current iteration=6, loss=33384.31827020744\n",
      "t [[-0.92282391]\n",
      " [-0.23922322]\n",
      " [ 0.10580308]\n",
      " ...\n",
      " [ 0.98289844]\n",
      " [-0.73797323]\n",
      " [ 0.81797295]]\n",
      "t [[-0.92282391]\n",
      " [-0.23922322]\n",
      " [ 0.10580308]\n",
      " ...\n",
      " [ 0.98289844]\n",
      " [-0.73797323]\n",
      " [ 0.81797295]]\n",
      "t [[-1.01508854]\n",
      " [-0.2677538 ]\n",
      " [ 0.12063717]\n",
      " ...\n",
      " [ 1.07173315]\n",
      " [-0.80688068]\n",
      " [ 0.89553076]]\n",
      "t [[-1.01508854]\n",
      " [-0.2677538 ]\n",
      " [ 0.12063717]\n",
      " ...\n",
      " [ 1.07173315]\n",
      " [-0.80688068]\n",
      " [ 0.89553076]]\n",
      "Current iteration=8, loss=32686.07167230371\n",
      "t [[-1.1006309 ]\n",
      " [-0.29482233]\n",
      " [ 0.13475679]\n",
      " ...\n",
      " [ 1.15195206]\n",
      " [-0.8695685 ]\n",
      " [ 0.96641667]]\n",
      "t [[-1.1006309 ]\n",
      " [-0.29482233]\n",
      " [ 0.13475679]\n",
      " ...\n",
      " [ 1.15195206]\n",
      " [-0.8695685 ]\n",
      " [ 0.96641667]]\n",
      "t [[-1.18016316]\n",
      " [-0.32049824]\n",
      " [ 0.1481176 ]\n",
      " ...\n",
      " [ 1.22453685]\n",
      " [-0.92672227]\n",
      " [ 1.03136244]]\n",
      "loss=32165.550172256266\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.1557907 ]\n",
      " [-0.03277342]\n",
      " [ 0.03252772]\n",
      " ...\n",
      " [-0.0411165 ]\n",
      " [-0.05112119]\n",
      " [-0.01074682]]\n",
      "t [[-0.1557907 ]\n",
      " [-0.03277342]\n",
      " [ 0.03252772]\n",
      " ...\n",
      " [-0.0411165 ]\n",
      " [-0.05112119]\n",
      " [-0.01074682]]\n",
      "t [[-0.29363956]\n",
      " [-0.0577203 ]\n",
      " [ 0.06284316]\n",
      " ...\n",
      " [-0.0791327 ]\n",
      " [-0.09765134]\n",
      " [-0.02142251]]\n",
      "t [[-0.29363956]\n",
      " [-0.0577203 ]\n",
      " [ 0.06284316]\n",
      " ...\n",
      " [-0.0791327 ]\n",
      " [-0.09765134]\n",
      " [-0.02142251]]\n",
      "Current iteration=2, loss=35581.393757342346\n",
      "t [[-0.41540044]\n",
      " [-0.07686687]\n",
      " [ 0.09072757]\n",
      " ...\n",
      " [-0.1143123 ]\n",
      " [-0.14008446]\n",
      " [-0.03188028]]\n",
      "t [[-0.41540044]\n",
      " [-0.07686687]\n",
      " [ 0.09072757]\n",
      " ...\n",
      " [-0.1143123 ]\n",
      " [-0.14008446]\n",
      " [-0.03188028]]\n",
      "t [[-0.52287743]\n",
      " [-0.09177343]\n",
      " [ 0.11613777]\n",
      " ...\n",
      " [-0.14691278]\n",
      " [-0.17888006]\n",
      " [-0.04202706]]\n",
      "t [[-0.52287743]\n",
      " [-0.09177343]\n",
      " [ 0.11613777]\n",
      " ...\n",
      " [-0.14691278]\n",
      " [-0.17888006]\n",
      " [-0.04202706]]\n",
      "Current iteration=4, loss=34155.569185528686\n",
      "t [[-0.61774772]\n",
      " [-0.10361487]\n",
      " [ 0.13914531]\n",
      " ...\n",
      " [-0.17717506]\n",
      " [-0.21445226]\n",
      " [-0.05180892]]\n",
      "t [[-0.61774772]\n",
      " [-0.10361487]\n",
      " [ 0.13914531]\n",
      " ...\n",
      " [-0.17717506]\n",
      " [-0.21445226]\n",
      " [-0.05180892]]\n",
      "t [[-0.70152568]\n",
      " [-0.11326417]\n",
      " [ 0.15988887]\n",
      " ...\n",
      " [-0.20531894]\n",
      " [-0.24716716]\n",
      " [-0.06119891]]\n",
      "t [[-0.70152568]\n",
      " [-0.11326417]\n",
      " [ 0.15988887]\n",
      " ...\n",
      " [-0.20531894]\n",
      " [-0.24716716]\n",
      " [-0.06119891]]\n",
      "Current iteration=6, loss=33172.41902875848\n",
      "t [[-0.7755537 ]\n",
      " [-0.1213653 ]\n",
      " [ 0.17854063]\n",
      " ...\n",
      " [-0.23154207]\n",
      " [-0.27734491]\n",
      " [-0.07018787]]\n",
      "t [[-0.7755537 ]\n",
      " [-0.1213653 ]\n",
      " [ 0.17854063]\n",
      " ...\n",
      " [-0.23154207]\n",
      " [-0.27734491]\n",
      " [-0.07018787]]\n",
      "t [[-0.84100843]\n",
      " [-0.12839169]\n",
      " [ 0.19528432]\n",
      " ...\n",
      " [-0.25602085]\n",
      " [-0.30526387]\n",
      " [-0.07877794]]\n",
      "t [[-0.84100843]\n",
      " [-0.12839169]\n",
      " [ 0.19528432]\n",
      " ...\n",
      " [-0.25602085]\n",
      " [-0.30526387]\n",
      " [-0.07877794]]\n",
      "Current iteration=8, loss=32472.129266277967\n",
      "t [[-0.89891464]\n",
      " [-0.13469087]\n",
      " [ 0.21030189]\n",
      " ...\n",
      " [-0.2789121 ]\n",
      " [-0.33116556]\n",
      " [-0.08697809]]\n",
      "t [[-0.89891464]\n",
      " [-0.13469087]\n",
      " [ 0.21030189]\n",
      " ...\n",
      " [-0.2789121 ]\n",
      " [-0.33116556]\n",
      " [-0.08697809]]\n",
      "t [[-0.95016205]\n",
      " [-0.14051781]\n",
      " [ 0.22376595]\n",
      " ...\n",
      " [-0.30035515]\n",
      " [-0.35525961]\n",
      " [-0.09480121]]\n",
      "loss=31957.317904428925\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.18296327]\n",
      " [-0.0412193 ]\n",
      " [ 0.01610081]\n",
      " ...\n",
      " [-0.03952326]\n",
      " [-0.05138699]\n",
      " [-0.01066611]]\n",
      "t [[-0.18296327]\n",
      " [-0.0412193 ]\n",
      " [ 0.01610081]\n",
      " ...\n",
      " [-0.03952326]\n",
      " [-0.05138699]\n",
      " [-0.01066611]]\n",
      "t [[-0.34773381]\n",
      " [-0.081664  ]\n",
      " [ 0.03396012]\n",
      " ...\n",
      " [-0.07604988]\n",
      " [-0.09814394]\n",
      " [-0.02135103]]\n",
      "t [[-0.34773381]\n",
      " [-0.081664  ]\n",
      " [ 0.03396012]\n",
      " ...\n",
      " [-0.07604988]\n",
      " [-0.09814394]\n",
      " [-0.02135103]]\n",
      "Current iteration=2, loss=35563.608137982286\n",
      "t [[-0.49657183]\n",
      " [-0.1208014 ]\n",
      " [ 0.05261775]\n",
      " ...\n",
      " [-0.10984803]\n",
      " [-0.1407715 ]\n",
      " [-0.03187754]]\n",
      "t [[-0.49657183]\n",
      " [-0.1208014 ]\n",
      " [ 0.05261775]\n",
      " ...\n",
      " [-0.10984803]\n",
      " [-0.1407715 ]\n",
      " [-0.03187754]]\n",
      "t [[-0.63150604]\n",
      " [-0.15831468]\n",
      " [ 0.07138859]\n",
      " ...\n",
      " [-0.14117499]\n",
      " [-0.17973511]\n",
      " [-0.04213054]]\n",
      "t [[-0.63150604]\n",
      " [-0.15831468]\n",
      " [ 0.07138859]\n",
      " ...\n",
      " [-0.14117499]\n",
      " [-0.17973511]\n",
      " [-0.04213054]]\n",
      "Current iteration=4, loss=34128.610526912744\n",
      "t [[-0.75430864]\n",
      " [-0.19404528]\n",
      " [ 0.08980609]\n",
      " ...\n",
      " [-0.17026851]\n",
      " [-0.21545394]\n",
      " [-0.0520406 ]]\n",
      "t [[-0.75430864]\n",
      " [-0.19404528]\n",
      " [ 0.08980609]\n",
      " ...\n",
      " [-0.17026851]\n",
      " [-0.21545394]\n",
      " [-0.0520406 ]]\n",
      "t [[-0.86650423]\n",
      " [-0.22794291]\n",
      " [ 0.10756774]\n",
      " ...\n",
      " [-0.1973435 ]\n",
      " [-0.24829833]\n",
      " [-0.06157003]]\n",
      "t [[-0.86650423]\n",
      " [-0.22794291]\n",
      " [ 0.10756774]\n",
      " ...\n",
      " [-0.1973435 ]\n",
      " [-0.24829833]\n",
      " [-0.06157003]]\n",
      "Current iteration=6, loss=33140.1308000587\n",
      "t [[-0.96939457]\n",
      " [-0.26002703]\n",
      " [ 0.1244896 ]\n",
      " ...\n",
      " [-0.22259183]\n",
      " [-0.27859184]\n",
      " [-0.07070236]]\n",
      "t [[-0.96939457]\n",
      " [-0.26002703]\n",
      " [ 0.1244896 ]\n",
      " ...\n",
      " [-0.22259183]\n",
      " [-0.27859184]\n",
      " [-0.07070236]]\n",
      "t [[-1.06408827]\n",
      " [-0.29035941]\n",
      " [ 0.14047106]\n",
      " ...\n",
      " [-0.24618373]\n",
      " [-0.30661563]\n",
      " [-0.07943481]]\n",
      "t [[-1.06408827]\n",
      " [-0.29035941]\n",
      " [ 0.14047106]\n",
      " ...\n",
      " [-0.24618373]\n",
      " [-0.30661563]\n",
      " [-0.07943481]]\n",
      "Current iteration=8, loss=32436.175581896845\n",
      "t [[-1.15152999]\n",
      " [-0.3190253 ]\n",
      " [ 0.15546887]\n",
      " ...\n",
      " [-0.26826987]\n",
      " [-0.33261349]\n",
      " [-0.08777312]]\n",
      "t [[-1.15152999]\n",
      " [-0.3190253 ]\n",
      " [ 0.15546887]\n",
      " ...\n",
      " [-0.26826987]\n",
      " [-0.33261349]\n",
      " [-0.08777312]]\n",
      "t [[-1.23252673]\n",
      " [-0.34612126]\n",
      " [ 0.16947837]\n",
      " ...\n",
      " [-0.28898356]\n",
      " [-0.35679686]\n",
      " [-0.09572813]]\n",
      "loss=31918.382485299844\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.18343122]\n",
      " [-0.04251333]\n",
      " [ 0.01104854]\n",
      " ...\n",
      " [-0.03906747]\n",
      " [-0.05168883]\n",
      " [-0.01045454]]\n",
      "t [[-0.18343122]\n",
      " [-0.04251333]\n",
      " [ 0.01104854]\n",
      " ...\n",
      " [-0.03906747]\n",
      " [-0.05168883]\n",
      " [-0.01045454]]\n",
      "t [[-0.34864107]\n",
      " [-0.08401966]\n",
      " [ 0.02481659]\n",
      " ...\n",
      " [-0.0753094 ]\n",
      " [-0.09872187]\n",
      " [-0.02093915]]\n",
      "t [[-0.34864107]\n",
      " [-0.08401966]\n",
      " [ 0.02481659]\n",
      " ...\n",
      " [-0.0753094 ]\n",
      " [-0.09872187]\n",
      " [-0.02093915]]\n",
      "Current iteration=2, loss=35535.54246256978\n",
      "t [[-0.49788257]\n",
      " [-0.12405109]\n",
      " [ 0.04019449]\n",
      " ...\n",
      " [-0.10895712]\n",
      " [-0.14160316]\n",
      " [-0.03128575]]\n",
      "t [[-0.49788257]\n",
      " [-0.12405109]\n",
      " [ 0.04019449]\n",
      " ...\n",
      " [-0.10895712]\n",
      " [-0.14160316]\n",
      " [-0.03128575]]\n",
      "t [[-0.63318353]\n",
      " [-0.16233642]\n",
      " [ 0.05636135]\n",
      " ...\n",
      " [-0.14023754]\n",
      " [-0.18080136]\n",
      " [-0.04138369]]\n",
      "t [[-0.63318353]\n",
      " [-0.16233642]\n",
      " [ 0.05636135]\n",
      " ...\n",
      " [-0.14023754]\n",
      " [-0.18080136]\n",
      " [-0.04138369]]\n",
      "Current iteration=4, loss=34082.146644605236\n",
      "t [[-0.75631823]\n",
      " [-0.19874917]\n",
      " [ 0.07273291]\n",
      " ...\n",
      " [-0.16936404]\n",
      " [-0.21673846]\n",
      " [-0.05116495]]\n",
      "t [[-0.75631823]\n",
      " [-0.19874917]\n",
      " [ 0.07273291]\n",
      " ...\n",
      " [-0.16936404]\n",
      " [-0.21673846]\n",
      " [-0.05116495]]\n",
      "t [[-0.86881446]\n",
      " [-0.23326176]\n",
      " [ 0.0889075 ]\n",
      " ...\n",
      " [-0.19653219]\n",
      " [-0.24978714]\n",
      " [-0.06059144]]\n",
      "t [[-0.86881446]\n",
      " [-0.23326176]\n",
      " [ 0.0889075 ]\n",
      " ...\n",
      " [-0.19653219]\n",
      " [-0.24978714]\n",
      " [-0.06059144]]\n",
      "Current iteration=6, loss=33081.28740786116\n",
      "t [[-0.97197736]\n",
      " [-0.26590991]\n",
      " [ 0.10461911]\n",
      " ...\n",
      " [-0.2219187 ]\n",
      " [-0.28027288]\n",
      " [-0.06964525]]\n",
      "t [[-0.97197736]\n",
      " [-0.26590991]\n",
      " [ 0.10461911]\n",
      " ...\n",
      " [-0.2219187 ]\n",
      " [-0.28027288]\n",
      " [-0.06964525]]\n",
      "t [[-1.06691868]\n",
      " [-0.29676708]\n",
      " [ 0.11969989]\n",
      " ...\n",
      " [-0.245682  ]\n",
      " [-0.30847838]\n",
      " [-0.07832165]]\n",
      "t [[-1.06691868]\n",
      " [-0.29676708]\n",
      " [ 0.11969989]\n",
      " ...\n",
      " [-0.245682  ]\n",
      " [-0.30847838]\n",
      " [-0.07832165]]\n",
      "Current iteration=8, loss=32368.745679541338\n",
      "t [[-1.15458582]\n",
      " [-0.3259271 ]\n",
      " [ 0.13405176]\n",
      " ...\n",
      " [-0.26796355]\n",
      " [-0.33464864]\n",
      " [-0.08662425]]\n",
      "t [[-1.15458582]\n",
      " [-0.3259271 ]\n",
      " [ 0.13405176]\n",
      " ...\n",
      " [-0.26796355]\n",
      " [-0.33464864]\n",
      " [-0.08662425]]\n",
      "t [[-1.23578811]\n",
      " [-0.35349286]\n",
      " [ 0.14762542]\n",
      " ...\n",
      " [-0.28888952]\n",
      " [-0.35899611]\n",
      " [-0.09456174]]\n",
      "loss=31844.848053480062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.18198441]\n",
      " [-0.03958235]\n",
      " [ 0.01295555]\n",
      " ...\n",
      " [ 0.20397096]\n",
      " [-0.15018466]\n",
      " [ 0.16544417]]\n",
      "t [[-0.18198441]\n",
      " [-0.03958235]\n",
      " [ 0.01295555]\n",
      " ...\n",
      " [ 0.20397096]\n",
      " [-0.15018466]\n",
      " [ 0.16544417]]\n",
      "t [[-0.34598096]\n",
      " [-0.07864665]\n",
      " [ 0.02831452]\n",
      " ...\n",
      " [ 0.38449431]\n",
      " [-0.2842261 ]\n",
      " [ 0.31334111]]\n",
      "t [[-0.34598096]\n",
      " [-0.07864665]\n",
      " [ 0.02831452]\n",
      " ...\n",
      " [ 0.38449431]\n",
      " [-0.2842261 ]\n",
      " [ 0.31334111]]\n",
      "Current iteration=2, loss=35609.97386552954\n",
      "t [[-0.49422363]\n",
      " [-0.1165983 ]\n",
      " [ 0.04500175]\n",
      " ...\n",
      " [ 0.54439168]\n",
      " [-0.40392364]\n",
      " [ 0.44571739]]\n",
      "t [[-0.49422363]\n",
      " [-0.1165983 ]\n",
      " [ 0.04500175]\n",
      " ...\n",
      " [ 0.54439168]\n",
      " [-0.40392364]\n",
      " [ 0.44571739]]\n",
      "t [[-0.62871452]\n",
      " [-0.15307337]\n",
      " [ 0.06223229]\n",
      " ...\n",
      " [ 0.68625751]\n",
      " [-0.51096944]\n",
      " [ 0.56445267]]\n",
      "t [[-0.62871452]\n",
      " [-0.15307337]\n",
      " [ 0.06223229]\n",
      " ...\n",
      " [ 0.68625751]\n",
      " [-0.51096944]\n",
      " [ 0.56445267]]\n",
      "Current iteration=4, loss=34202.00333723286\n",
      "t [[-0.75120095]\n",
      " [-0.18787837]\n",
      " [ 0.07945569]\n",
      " ...\n",
      " [ 0.81240154]\n",
      " [-0.60689758]\n",
      " [ 0.67123201]]\n",
      "t [[-0.75120095]\n",
      " [-0.18787837]\n",
      " [ 0.07945569]\n",
      " ...\n",
      " [ 0.81240154]\n",
      " [-0.60689758]\n",
      " [ 0.67123201]]\n",
      "t [[-0.86318541]\n",
      " [-0.22093754]\n",
      " [ 0.09630051]\n",
      " ...\n",
      " [ 0.92484045]\n",
      " [-0.69306771]\n",
      " [ 0.76753522]]\n",
      "t [[-0.86318541]\n",
      " [-0.22093754]\n",
      " [ 0.09630051]\n",
      " ...\n",
      " [ 0.92484045]\n",
      " [-0.69306771]\n",
      " [ 0.76753522]]\n",
      "Current iteration=6, loss=33229.68407616399\n",
      "t [[-0.96595053]\n",
      " [-0.2522519 ]\n",
      " [ 0.11252693]\n",
      " ...\n",
      " [ 1.02531566]\n",
      " [-0.77066854]\n",
      " [ 0.85464662]]\n",
      "t [[-0.96595053]\n",
      " [-0.2522519 ]\n",
      " [ 0.11252693]\n",
      " ...\n",
      " [ 1.02531566]\n",
      " [-0.77066854]\n",
      " [ 0.85464662]]\n",
      "t [[-1.06058874]\n",
      " [-0.28186992]\n",
      " [ 0.1279895 ]\n",
      " ...\n",
      " [ 1.11532258]\n",
      " [-0.84073146]\n",
      " [ 0.93367407]]\n",
      "t [[-1.06058874]\n",
      " [-0.28186992]\n",
      " [ 0.1279895 ]\n",
      " ...\n",
      " [ 1.11532258]\n",
      " [-0.84073146]\n",
      " [ 0.93367407]]\n",
      "Current iteration=8, loss=32535.817207662818\n",
      "t [[-1.14803116]\n",
      " [-0.30986724]\n",
      " [ 0.14260909]\n",
      " ...\n",
      " [ 1.19614291]\n",
      " [-0.90414815]\n",
      " [ 1.00557096]]\n",
      "t [[-1.14803116]\n",
      " [-0.30986724]\n",
      " [ 0.14260909]\n",
      " ...\n",
      " [ 1.19614291]\n",
      " [-0.90414815]\n",
      " [ 1.00557096]]\n",
      "t [[-1.22907353]\n",
      " [-0.33633342]\n",
      " [ 0.15635246]\n",
      " ...\n",
      " [ 1.26887558]\n",
      " [-0.96168875]\n",
      " [ 1.07115786]]\n",
      "loss=32024.65720073156\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.16495486]\n",
      " [-0.03470127]\n",
      " [ 0.03444112]\n",
      " ...\n",
      " [-0.04353512]\n",
      " [-0.05412832]\n",
      " [-0.01137899]]\n",
      "t [[-0.16495486]\n",
      " [-0.03470127]\n",
      " [ 0.03444112]\n",
      " ...\n",
      " [-0.04353512]\n",
      " [-0.05412832]\n",
      " [-0.01137899]]\n",
      "t [[-0.309799  ]\n",
      " [-0.06063017]\n",
      " [ 0.06640219]\n",
      " ...\n",
      " [-0.08359536]\n",
      " [-0.10311068]\n",
      " [-0.02267826]]\n",
      "t [[-0.309799  ]\n",
      " [-0.06063017]\n",
      " [ 0.06640219]\n",
      " ...\n",
      " [-0.08359536]\n",
      " [-0.10311068]\n",
      " [-0.02267826]]\n",
      "Current iteration=2, loss=35475.67126324929\n",
      "t [[-0.43674001]\n",
      " [-0.08019357]\n",
      " [ 0.0956246 ]\n",
      " ...\n",
      " [-0.12049532]\n",
      " [-0.14753534]\n",
      " [-0.03372387]]\n",
      "t [[-0.43674001]\n",
      " [-0.08019357]\n",
      " [ 0.0956246 ]\n",
      " ...\n",
      " [-0.12049532]\n",
      " [-0.14753534]\n",
      " [-0.03372387]]\n",
      "t [[-0.54791538]\n",
      " [-0.09520808]\n",
      " [ 0.12207036]\n",
      " ...\n",
      " [-0.15454019]\n",
      " [-0.18794517]\n",
      " [-0.04440943]]\n",
      "t [[-0.54791538]\n",
      " [-0.09520808]\n",
      " [ 0.12207036]\n",
      " ...\n",
      " [-0.15454019]\n",
      " [-0.18794517]\n",
      " [-0.04440943]]\n",
      "Current iteration=4, loss=34013.791598068885\n",
      "t [[-0.6452943 ]\n",
      " [-0.10701222]\n",
      " [ 0.14584023]\n",
      " ...\n",
      " [-0.18601294]\n",
      " [-0.22482443]\n",
      " [-0.05467643]]\n",
      "t [[-0.6452943 ]\n",
      " [-0.10701222]\n",
      " [ 0.14584023]\n",
      " ...\n",
      " [-0.18601294]\n",
      " [-0.22482443]\n",
      " [-0.05467643]]\n",
      "t [[-0.73063579]\n",
      " [-0.11657886]\n",
      " [ 0.16711166]\n",
      " ...\n",
      " [-0.21516921]\n",
      " [-0.25859665]\n",
      " [-0.06449828]]\n",
      "t [[-0.73063579]\n",
      " [-0.11657886]\n",
      " [ 0.16711166]\n",
      " ...\n",
      " [-0.21516921]\n",
      " [-0.25859665]\n",
      " [-0.06449828]]\n",
      "Current iteration=6, loss=33024.93238547326\n",
      "t [[-0.80548185]\n",
      " [-0.12461025]\n",
      " [ 0.18609673]\n",
      " ...\n",
      " [-0.24223677]\n",
      " [-0.28962829]\n",
      " [-0.07386867]]\n",
      "t [[-0.80548185]\n",
      " [-0.12461025]\n",
      " [ 0.18609673]\n",
      " ...\n",
      " [-0.24223677]\n",
      " [-0.28962829]\n",
      " [-0.07386867]]\n",
      "t [[-0.87116946]\n",
      " [-0.13161175]\n",
      " [ 0.20301595]\n",
      " ...\n",
      " [-0.26741713]\n",
      " [-0.31823485]\n",
      " [-0.08279358]]\n",
      "t [[-0.87116946]\n",
      " [-0.13161175]\n",
      " [ 0.20301595]\n",
      " ...\n",
      " [-0.26741713]\n",
      " [-0.31823485]\n",
      " [-0.08279358]]\n",
      "Current iteration=8, loss=32331.018641064897\n",
      "t [[-0.9288511 ]\n",
      " [-0.13794665]\n",
      " [ 0.21808317]\n",
      " ...\n",
      " [-0.29088806]\n",
      " [-0.34468759]\n",
      " [-0.09128611]]\n",
      "t [[-0.9288511 ]\n",
      " [-0.13794665]\n",
      " [ 0.21808317]\n",
      " ...\n",
      " [-0.29088806]\n",
      " [-0.34468759]\n",
      " [-0.09128611]]\n",
      "t [[-0.97951784]\n",
      " [-0.14387587]\n",
      " [ 0.23149782]\n",
      " ...\n",
      " [-0.31280634]\n",
      " [-0.36922002]\n",
      " [-0.09936312]]\n",
      "loss=31826.63494390339\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.19372582]\n",
      " [-0.04364397]\n",
      " [ 0.01704792]\n",
      " ...\n",
      " [-0.04184816]\n",
      " [-0.05440976]\n",
      " [-0.01129353]]\n",
      "t [[-0.19372582]\n",
      " [-0.04364397]\n",
      " [ 0.01704792]\n",
      " ...\n",
      " [-0.04184816]\n",
      " [-0.05440976]\n",
      " [-0.01129353]]\n",
      "t [[-0.36706061]\n",
      " [-0.08641912]\n",
      " [ 0.03606659]\n",
      " ...\n",
      " [-0.08033761]\n",
      " [-0.10362985]\n",
      " [-0.02260816]]\n",
      "t [[-0.36706061]\n",
      " [-0.08641912]\n",
      " [ 0.03606659]\n",
      " ...\n",
      " [-0.08033761]\n",
      " [-0.10362985]\n",
      " [-0.02260816]]\n",
      "Current iteration=2, loss=35457.105414424484\n",
      "t [[-0.52269526]\n",
      " [-0.12769347]\n",
      " [ 0.05591622]\n",
      " ...\n",
      " [-0.11578816]\n",
      " [-0.14825643]\n",
      " [-0.03373378]]\n",
      "t [[-0.52269526]\n",
      " [-0.12769347]\n",
      " [ 0.05591622]\n",
      " ...\n",
      " [-0.11578816]\n",
      " [-0.14825643]\n",
      " [-0.03373378]]\n",
      "t [[-0.66302021]\n",
      " [-0.16710747]\n",
      " [ 0.07580452]\n",
      " ...\n",
      " [-0.14850444]\n",
      " [-0.18883934]\n",
      " [-0.04453852]]\n",
      "t [[-0.66302021]\n",
      " [-0.16710747]\n",
      " [ 0.07580452]\n",
      " ...\n",
      " [-0.14850444]\n",
      " [-0.18883934]\n",
      " [-0.04453852]]\n",
      "Current iteration=4, loss=33986.004863887254\n",
      "t [[-0.79009872]\n",
      " [-0.2044955 ]\n",
      " [ 0.0952091 ]\n",
      " ...\n",
      " [-0.17876524]\n",
      " [-0.22586873]\n",
      " [-0.05494637]]\n",
      "t [[-0.79009872]\n",
      " [-0.2044955 ]\n",
      " [ 0.0952091 ]\n",
      " ...\n",
      " [-0.17876524]\n",
      " [-0.22586873]\n",
      " [-0.05494637]]\n",
      "t [[-0.90568459]\n",
      " [-0.23981985]\n",
      " [ 0.11380473]\n",
      " ...\n",
      " [-0.20682013]\n",
      " [-0.25977292]\n",
      " [-0.064919  ]]\n",
      "t [[-0.90568459]\n",
      " [-0.23981985]\n",
      " [ 0.11380473]\n",
      " ...\n",
      " [-0.20682013]\n",
      " [-0.25977292]\n",
      " [-0.064919  ]]\n",
      "Current iteration=6, loss=32991.87521140855\n",
      "t [[-1.01125829]\n",
      " [-0.27312167]\n",
      " [ 0.13140462]\n",
      " ...\n",
      " [-0.23288982]\n",
      " [-0.29092224]\n",
      " [-0.07444232]]\n",
      "t [[-1.01125829]\n",
      " [-0.27312167]\n",
      " [ 0.13140462]\n",
      " ...\n",
      " [-0.23288982]\n",
      " [-0.29092224]\n",
      " [-0.07444232]]\n",
      "t [[-1.10806762]\n",
      " [-0.30448727]\n",
      " [ 0.14791639]\n",
      " ...\n",
      " [-0.25716845]\n",
      " [-0.31963526]\n",
      " [-0.0835173 ]]\n",
      "t [[-1.10806762]\n",
      " [-0.30448727]\n",
      " [ 0.14791639]\n",
      " ...\n",
      " [-0.25716845]\n",
      " [-0.31963526]\n",
      " [-0.0835173 ]]\n",
      "Current iteration=8, loss=32294.301976302813\n",
      "t [[-1.19716597]\n",
      " [-0.33402604]\n",
      " [ 0.16331045]\n",
      " ...\n",
      " [-0.27982648]\n",
      " [-0.34618574]\n",
      " [-0.09215382]]\n",
      "t [[-1.19716597]\n",
      " [-0.33402604]\n",
      " [ 0.16331045]\n",
      " ...\n",
      " [-0.27982648]\n",
      " [-0.34618574]\n",
      " [-0.09215382]]\n",
      "t [[-1.27944576]\n",
      " [-0.36185665]\n",
      " [ 0.17759804]\n",
      " ...\n",
      " [-0.30101369]\n",
      " [-0.37080913]\n",
      " [-0.1003668 ]]\n",
      "loss=31786.871537996565\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.19422129]\n",
      " [-0.04501412]\n",
      " [ 0.01169846]\n",
      " ...\n",
      " [-0.04136556]\n",
      " [-0.05472935]\n",
      " [-0.01106951]]\n",
      "t [[-0.19422129]\n",
      " [-0.04501412]\n",
      " [ 0.01169846]\n",
      " ...\n",
      " [-0.04136556]\n",
      " [-0.05472935]\n",
      " [-0.01106951]]\n",
      "t [[-0.36801955]\n",
      " [-0.08889889]\n",
      " [ 0.02644474]\n",
      " ...\n",
      " [-0.07956418]\n",
      " [-0.10424019]\n",
      " [-0.02217274]]\n",
      "t [[-0.36801955]\n",
      " [-0.08889889]\n",
      " [ 0.02644474]\n",
      " ...\n",
      " [-0.07956418]\n",
      " [-0.10424019]\n",
      " [-0.02217274]]\n",
      "Current iteration=2, loss=35427.656335458734\n",
      "t [[-0.52407729]\n",
      " [-0.13109925]\n",
      " [ 0.04292115]\n",
      " ...\n",
      " [-0.11487189]\n",
      " [-0.14913274]\n",
      " [-0.03311041]]\n",
      "t [[-0.52407729]\n",
      " [-0.13109925]\n",
      " [ 0.04292115]\n",
      " ...\n",
      " [-0.11487189]\n",
      " [-0.14913274]\n",
      " [-0.03311041]]\n",
      "t [[-0.66478426]\n",
      " [-0.17130845]\n",
      " [ 0.0601754 ]\n",
      " ...\n",
      " [-0.14755777]\n",
      " [-0.18996064]\n",
      " [-0.04375557]]\n",
      "t [[-0.66478426]\n",
      " [-0.17130845]\n",
      " [ 0.0601754 ]\n",
      " ...\n",
      " [-0.14755777]\n",
      " [-0.18996064]\n",
      " [-0.04375557]]\n",
      "Current iteration=4, loss=33937.72796406389\n",
      "t [[-0.79220652]\n",
      " [-0.20939711]\n",
      " [ 0.07754827]\n",
      " ...\n",
      " [-0.17787255]\n",
      " [-0.2272173 ]\n",
      " [-0.0540335 ]]\n",
      "t [[-0.79220652]\n",
      " [-0.20939711]\n",
      " [ 0.07754827]\n",
      " ...\n",
      " [-0.17787255]\n",
      " [-0.2272173 ]\n",
      " [-0.0540335 ]]\n",
      "t [[-0.90810176]\n",
      " [-0.24535258]\n",
      " [ 0.09460104]\n",
      " ...\n",
      " [-0.20604397]\n",
      " [-0.26133371]\n",
      " [-0.06390503]]\n",
      "t [[-0.90810176]\n",
      " [-0.24535258]\n",
      " [ 0.09460104]\n",
      " ...\n",
      " [-0.20604397]\n",
      " [-0.26133371]\n",
      " [-0.06390503]]\n",
      "Current iteration=6, loss=32931.19051689083\n",
      "t [[-1.01395444]\n",
      " [-0.27923354]\n",
      " [ 0.11105442]\n",
      " ...\n",
      " [-0.23227589]\n",
      " [-0.29268235]\n",
      " [-0.07335411]]\n",
      "t [[-1.01395444]\n",
      " [-0.27923354]\n",
      " [ 0.11105442]\n",
      " ...\n",
      " [-0.23227589]\n",
      " [-0.29268235]\n",
      " [-0.07335411]]\n",
      "t [[-1.11101595]\n",
      " [-0.31113871]\n",
      " [ 0.12674133]\n",
      " ...\n",
      " [-0.25674956]\n",
      " [-0.32158347]\n",
      " [-0.08237924]]\n",
      "t [[-1.11101595]\n",
      " [-0.31113871]\n",
      " [ 0.12674133]\n",
      " ...\n",
      " [-0.25674956]\n",
      " [-0.32158347]\n",
      " [-0.08237924]]\n",
      "Current iteration=8, loss=32225.15970908165\n",
      "t [[-1.20034273]\n",
      " [-0.34118641]\n",
      " [ 0.14157219]\n",
      " ...\n",
      " [-0.27962558]\n",
      " [-0.34831215]\n",
      " [-0.09098774]]\n",
      "t [[-1.20034273]\n",
      " [-0.34118641]\n",
      " [ 0.14157219]\n",
      " ...\n",
      " [-0.27962558]\n",
      " [-0.34831215]\n",
      " [-0.09098774]]\n",
      "t [[-1.28282979]\n",
      " [-0.36950181]\n",
      " [ 0.15551002]\n",
      " ...\n",
      " [-0.30104621]\n",
      " [-0.3731049 ]\n",
      " [-0.09919201]]\n",
      "loss=31711.811739345423\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.19268937]\n",
      " [-0.04191072]\n",
      " [ 0.01371764]\n",
      " ...\n",
      " [ 0.21596925]\n",
      " [-0.15901906]\n",
      " [ 0.17517618]]\n",
      "t [[-0.19268937]\n",
      " [-0.04191072]\n",
      " [ 0.01371764]\n",
      " ...\n",
      " [ 0.21596925]\n",
      " [-0.15901906]\n",
      " [ 0.17517618]]\n",
      "t [[-0.36521734]\n",
      " [-0.08324014]\n",
      " [ 0.03012882]\n",
      " ...\n",
      " [ 0.40565726]\n",
      " [-0.29994374]\n",
      " [ 0.33068461]]\n",
      "t [[-0.36521734]\n",
      " [-0.08324014]\n",
      " [ 0.03012882]\n",
      " ...\n",
      " [ 0.40565726]\n",
      " [-0.29994374]\n",
      " [ 0.33068461]]\n",
      "Current iteration=2, loss=35505.64440495041\n",
      "t [[-0.52024349]\n",
      " [-0.12328259]\n",
      " [ 0.04795685]\n",
      " ...\n",
      " [ 0.57242314]\n",
      " [-0.42491648]\n",
      " [ 0.46893843]]\n",
      "t [[-0.52024349]\n",
      " [-0.12328259]\n",
      " [ 0.04795685]\n",
      " ...\n",
      " [ 0.57242314]\n",
      " [-0.42491648]\n",
      " [ 0.46893843]]\n",
      "t [[-0.66012668]\n",
      " [-0.16162377]\n",
      " [ 0.06629213]\n",
      " ...\n",
      " [ 0.71932887]\n",
      " [-0.53593754]\n",
      " [ 0.59215741]]\n",
      "t [[-0.66012668]\n",
      " [-0.16162377]\n",
      " [ 0.06629213]\n",
      " ...\n",
      " [ 0.71932887]\n",
      " [-0.53593754]\n",
      " [ 0.59215741]]\n",
      "Current iteration=4, loss=34061.91675611053\n",
      "t [[-0.786901  ]\n",
      " [-0.19805841]\n",
      " [ 0.08451517]\n",
      " ...\n",
      " [ 0.84907062]\n",
      " [-0.63480213]\n",
      " [ 0.70231024]]\n",
      "t [[-0.786901  ]\n",
      " [-0.19805841]\n",
      " [ 0.08451517]\n",
      " ...\n",
      " [ 0.84907062]\n",
      " [-0.63480213]\n",
      " [ 0.70231024]]\n",
      "t [[-0.90229464]\n",
      " [-0.23252059]\n",
      " [ 0.1022223 ]\n",
      " ...\n",
      " [ 0.96397525]\n",
      " [-0.72308376]\n",
      " [ 0.80110624]]\n",
      "t [[-0.90229464]\n",
      " [-0.23252059]\n",
      " [ 0.1022223 ]\n",
      " ...\n",
      " [ 0.96397525]\n",
      " [-0.72308376]\n",
      " [ 0.80110624]]\n",
      "Current iteration=6, loss=33083.70383634558\n",
      "t [[-1.00776631]\n",
      " [-0.26503153]\n",
      " [ 0.11916417]\n",
      " ...\n",
      " [ 1.06602939]\n",
      " [-0.80214299]\n",
      " [ 0.89001241]]\n",
      "t [[-1.00776631]\n",
      " [-0.26503153]\n",
      " [ 0.11916417]\n",
      " ...\n",
      " [ 1.06602939]\n",
      " [-0.80214299]\n",
      " [ 0.89001241]]\n",
      "t [[-1.10454562]\n",
      " [-0.29566347]\n",
      " [ 0.13519895]\n",
      " ...\n",
      " [ 1.15692093]\n",
      " [-0.87314794]\n",
      " [ 0.97028099]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.10454562]\n",
      " [-0.29566347]\n",
      " [ 0.13519895]\n",
      " ...\n",
      " [ 1.15692093]\n",
      " [-0.87314794]\n",
      " [ 0.97028099]]\n",
      "Current iteration=8, loss=32395.865324676637\n",
      "t [[-1.193671  ]\n",
      " [-0.32451586]\n",
      " [ 0.15025819]\n",
      " ...\n",
      " [ 1.2380824 ]\n",
      " [-0.93709857]\n",
      " [ 1.04297923]]\n",
      "t [[-1.193671  ]\n",
      " [-0.32451586]\n",
      " [ 0.15025819]\n",
      " ...\n",
      " [ 1.2380824 ]\n",
      " [-0.93709857]\n",
      " [ 1.04297923]]\n",
      "t [[-1.27602264]\n",
      " [-0.35170024]\n",
      " [ 0.16432258]\n",
      " ...\n",
      " [ 1.31073111]\n",
      " [-0.99485079]\n",
      " [ 1.10901761]]\n",
      "loss=31894.75694062635\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.17411902]\n",
      " [-0.03662911]\n",
      " [ 0.03635451]\n",
      " ...\n",
      " [-0.04595374]\n",
      " [-0.05713545]\n",
      " [-0.01201115]]\n",
      "t [[-0.17411902]\n",
      " [-0.03662911]\n",
      " [ 0.03635451]\n",
      " ...\n",
      " [-0.04595374]\n",
      " [-0.05713545]\n",
      " [-0.01201115]]\n",
      "t [[-0.32583545]\n",
      " [-0.06348647]\n",
      " [ 0.06994595]\n",
      " ...\n",
      " [-0.08803682]\n",
      " [-0.10853856]\n",
      " [-0.02393353]]\n",
      "t [[-0.32583545]\n",
      " [-0.06348647]\n",
      " [ 0.06994595]\n",
      " ...\n",
      " [-0.08803682]\n",
      " [-0.10853856]\n",
      " [-0.02393353]]\n",
      "Current iteration=2, loss=35371.91949788494\n",
      "t [[-0.45775345]\n",
      " [-0.08340437]\n",
      " [ 0.10047144]\n",
      " ...\n",
      " [-0.12662102]\n",
      " [-0.15490327]\n",
      " [-0.0355629 ]]\n",
      "t [[-0.45775345]\n",
      " [-0.08340437]\n",
      " [ 0.10047144]\n",
      " ...\n",
      " [-0.12662102]\n",
      " [-0.15490327]\n",
      " [-0.0355629 ]]\n",
      "t [[-0.57238108]\n",
      " [-0.09847702]\n",
      " [ 0.12790181]\n",
      " ...\n",
      " [-0.16206453]\n",
      " [-0.19686469]\n",
      " [-0.04677896]]\n",
      "t [[-0.57238108]\n",
      " [-0.09847702]\n",
      " [ 0.12790181]\n",
      " ...\n",
      " [-0.16206453]\n",
      " [-0.19686469]\n",
      " [-0.04677896]]\n",
      "Current iteration=4, loss=33877.61337736877\n",
      "t [[-0.6720073 ]\n",
      " [-0.11021364]\n",
      " [ 0.15237333]\n",
      " ...\n",
      " [-0.19469631]\n",
      " [-0.23498349]\n",
      " [-0.05751914]]\n",
      "t [[-0.6720073 ]\n",
      " [-0.11021364]\n",
      " [ 0.15237333]\n",
      " ...\n",
      " [-0.19469631]\n",
      " [-0.23498349]\n",
      " [-0.05751914]]\n",
      "t [[-0.75865453]\n",
      " [-0.11968693]\n",
      " [ 0.1741082 ]\n",
      " ...\n",
      " [-0.22481073]\n",
      " [-0.26974475]\n",
      " [-0.06775811]]\n",
      "t [[-0.75865453]\n",
      " [-0.11968693]\n",
      " [ 0.1741082 ]\n",
      " ...\n",
      " [-0.22481073]\n",
      " [-0.26974475]\n",
      " [-0.06775811]]\n",
      "Current iteration=6, loss=32885.65252314512\n",
      "t [[-0.83407592]\n",
      " [-0.12765433]\n",
      " [ 0.19336295]\n",
      " ...\n",
      " [-0.25266765]\n",
      " [-0.30156378]\n",
      " [-0.07749325]]\n",
      "t [[-0.83407592]\n",
      " [-0.12765433]\n",
      " [ 0.19336295]\n",
      " ...\n",
      " [-0.25266765]\n",
      " [-0.30156378]\n",
      " [-0.07749325]]\n",
      "t [[-0.89977516]\n",
      " [-0.13464935]\n",
      " [ 0.2103979 ]\n",
      " ...\n",
      " [-0.27849501]\n",
      " [-0.33079466]\n",
      " [-0.08673511]]\n",
      "t [[-0.89977516]\n",
      " [-0.13464935]\n",
      " [ 0.2103979 ]\n",
      " ...\n",
      " [-0.27849501]\n",
      " [-0.33079466]\n",
      " [-0.08673511]]\n",
      "Current iteration=8, loss=32199.475599436606\n",
      "t [[-0.95703538]\n",
      " [-0.14104742]\n",
      " [ 0.22546063]\n",
      " ...\n",
      " [-0.3024924 ]\n",
      " [-0.35773918]\n",
      " [-0.09550138]]\n",
      "t [[-0.95703538]\n",
      " [-0.14104742]\n",
      " [ 0.22546063]\n",
      " ...\n",
      " [-0.3024924 ]\n",
      " [-0.35773918]\n",
      " [-0.09550138]]\n",
      "t [[-1.00694979]\n",
      " [-0.14711234]\n",
      " [ 0.23877821]\n",
      " ...\n",
      " [-0.32483469]\n",
      " [-0.38265504]\n",
      " [-0.10381325]]\n",
      "loss=31705.98395084906\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.20448836]\n",
      " [-0.04606863]\n",
      " [ 0.01799502]\n",
      " ...\n",
      " [-0.04417305]\n",
      " [-0.05743252]\n",
      " [-0.01192095]]\n",
      "t [[-0.20448836]\n",
      " [-0.04606863]\n",
      " [ 0.01799502]\n",
      " ...\n",
      " [-0.04417305]\n",
      " [-0.05743252]\n",
      " [-0.01192095]]\n",
      "t [[-0.38626295]\n",
      " [-0.09116877]\n",
      " [ 0.03818503]\n",
      " ...\n",
      " [-0.08460484]\n",
      " [-0.10908403]\n",
      " [-0.02386541]]\n",
      "t [[-0.38626295]\n",
      " [-0.09116877]\n",
      " [ 0.03818503]\n",
      " ...\n",
      " [-0.08460484]\n",
      " [-0.10908403]\n",
      " [-0.02386541]]\n",
      "Current iteration=2, loss=35352.59840877098\n",
      "t [[-0.54849733]\n",
      " [-0.13455761]\n",
      " [ 0.0592296 ]\n",
      " ...\n",
      " [-0.12167317]\n",
      " [-0.15565776]\n",
      " [-0.03558664]]\n",
      "t [[-0.54849733]\n",
      " [-0.13455761]\n",
      " [ 0.0592296 ]\n",
      " ...\n",
      " [-0.12167317]\n",
      " [-0.15565776]\n",
      " [-0.03558664]]\n",
      "t [[-0.69398067]\n",
      " [-0.17583185]\n",
      " [ 0.08022178]\n",
      " ...\n",
      " [-0.15573512]\n",
      " [-0.19779696]\n",
      " [-0.04693507]]\n",
      "t [[-0.69398067]\n",
      " [-0.17583185]\n",
      " [ 0.08022178]\n",
      " ...\n",
      " [-0.15573512]\n",
      " [-0.19779696]\n",
      " [-0.04693507]]\n",
      "Current iteration=4, loss=33849.04810841471\n",
      "t [[-0.8250913 ]\n",
      " [-0.2148225 ]\n",
      " [ 0.10058335]\n",
      " ...\n",
      " [-0.18711441]\n",
      " [-0.23606904]\n",
      " [-0.05782867]]\n",
      "t [[-0.8250913 ]\n",
      " [-0.2148225 ]\n",
      " [ 0.10058335]\n",
      " ...\n",
      " [-0.18711441]\n",
      " [-0.23606904]\n",
      " [-0.05782867]]\n",
      "t [[-0.9438265 ]\n",
      " [-0.25150928]\n",
      " [ 0.1199697 ]\n",
      " ...\n",
      " [-0.2160979 ]\n",
      " [-0.27096449]\n",
      " [-0.06822937]]\n",
      "t [[-0.9438265 ]\n",
      " [-0.25150928]\n",
      " [ 0.1199697 ]\n",
      " ...\n",
      " [-0.2160979 ]\n",
      " [-0.27096449]\n",
      " [-0.06822937]]\n",
      "Current iteration=6, loss=32851.87495817509\n",
      "t [[-1.05185349]\n",
      " [-0.28595946]\n",
      " [ 0.13819543]\n",
      " ...\n",
      " [-0.24293697]\n",
      " [-0.30290292]\n",
      " [-0.07812641]]\n",
      "t [[-1.05185349]\n",
      " [-0.28595946]\n",
      " [ 0.13819543]\n",
      " ...\n",
      " [-0.24293697]\n",
      " [-0.30290292]\n",
      " [-0.07812641]]\n",
      "t [[-1.15056328]\n",
      " [-0.31828734]\n",
      " [ 0.15518029]\n",
      " ...\n",
      " [-0.26785088]\n",
      " [-0.33224181]\n",
      " [-0.08752525]]\n",
      "t [[-1.15056328]\n",
      " [-0.31828734]\n",
      " [ 0.15518029]\n",
      " ...\n",
      " [-0.26785088]\n",
      " [-0.33224181]\n",
      " [-0.08752525]]\n",
      "Current iteration=8, loss=32162.030785447987\n",
      "t [[-1.24111967]\n",
      " [-0.34862885]\n",
      " [ 0.17091172]\n",
      " ...\n",
      " [-0.29103069]\n",
      " [-0.35928561]\n",
      " [-0.09644051]]\n",
      "t [[-1.24111967]\n",
      " [-0.34862885]\n",
      " [ 0.17091172]\n",
      " ...\n",
      " [-0.29103069]\n",
      " [-0.35928561]\n",
      " [-0.09644051]]\n",
      "t [[-1.32450086]\n",
      " [-0.37712633]\n",
      " [ 0.18541934]\n",
      " ...\n",
      " [-0.31264316]\n",
      " [-0.38429411]\n",
      " [-0.1048916 ]]\n",
      "loss=31665.411636266013\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.20501137]\n",
      " [-0.0475149 ]\n",
      " [ 0.01234837]\n",
      " ...\n",
      " [-0.04366364]\n",
      " [-0.05776986]\n",
      " [-0.01168448]]\n",
      "t [[-0.20501137]\n",
      " [-0.0475149 ]\n",
      " [ 0.01234837]\n",
      " ...\n",
      " [-0.04366364]\n",
      " [-0.05776986]\n",
      " [-0.01168448]]\n",
      "t [[-0.38727341]\n",
      " [-0.09377104]\n",
      " [ 0.02809141]\n",
      " ...\n",
      " [-0.08379964]\n",
      " [-0.10972662]\n",
      " [-0.02340654]]\n",
      "t [[-0.38727341]\n",
      " [-0.09377104]\n",
      " [ 0.02809141]\n",
      " ...\n",
      " [-0.08379964]\n",
      " [-0.10972662]\n",
      " [-0.02340654]]\n",
      "Current iteration=2, loss=35321.7928783415\n",
      "t [[-0.54995001]\n",
      " [-0.13811609]\n",
      " [ 0.04567898]\n",
      " ...\n",
      " [-0.12073422]\n",
      " [-0.15657829]\n",
      " [-0.0349321 ]]\n",
      "t [[-0.54995001]\n",
      " [-0.13811609]\n",
      " [ 0.04567898]\n",
      " ...\n",
      " [-0.12073422]\n",
      " [-0.15657829]\n",
      " [-0.0349321 ]]\n",
      "t [[-0.69583002]\n",
      " [-0.18020726]\n",
      " [ 0.06401739]\n",
      " ...\n",
      " [-0.1547833 ]\n",
      " [-0.19897258]\n",
      " [-0.04611703]]\n",
      "t [[-0.69583002]\n",
      " [-0.18020726]\n",
      " [ 0.06401739]\n",
      " ...\n",
      " [-0.1547833 ]\n",
      " [-0.19897258]\n",
      " [-0.04611703]]\n",
      "Current iteration=4, loss=33799.03370382751\n",
      "t [[-0.82729533]\n",
      " [-0.21991603]\n",
      " [ 0.08237101]\n",
      " ...\n",
      " [-0.1862386 ]\n",
      " [-0.23748065]\n",
      " [-0.05688034]]\n",
      "t [[-0.82729533]\n",
      " [-0.21991603]\n",
      " [ 0.08237101]\n",
      " ...\n",
      " [-0.1862386 ]\n",
      " [-0.23748065]\n",
      " [-0.05688034]]\n",
      "t [[-0.94634792]\n",
      " [-0.25724951]\n",
      " [ 0.10026667]\n",
      " ...\n",
      " [-0.21536257]\n",
      " [-0.27259599]\n",
      " [-0.06718258]]\n",
      "t [[-0.94634792]\n",
      " [-0.25724951]\n",
      " [ 0.10026667]\n",
      " ...\n",
      " [-0.21536257]\n",
      " [-0.27259599]\n",
      " [-0.06718258]]\n",
      "Current iteration=6, loss=32789.45808959147\n",
      "t [[-1.05465961]\n",
      " [-0.29229367]\n",
      " [ 0.11741576]\n",
      " ...\n",
      " [-0.24238809]\n",
      " [-0.30474057]\n",
      " [-0.07701045]]\n",
      "t [[-1.05465961]\n",
      " [-0.29229367]\n",
      " [ 0.11741576]\n",
      " ...\n",
      " [-0.24238809]\n",
      " [-0.30474057]\n",
      " [-0.07701045]]\n",
      "t [[-1.15362545]\n",
      " [-0.32517583]\n",
      " [ 0.13365599]\n",
      " ...\n",
      " [-0.26752048]\n",
      " [-0.33427369]\n",
      " [-0.0863664 ]]\n",
      "t [[-1.15362545]\n",
      " [-0.32517583]\n",
      " [ 0.13365599]\n",
      " ...\n",
      " [-0.26752048]\n",
      " [-0.33427369]\n",
      " [-0.0863664 ]]\n",
      "Current iteration=8, loss=32091.30347459492\n",
      "t [[-1.24441266]\n",
      " [-0.35604113]\n",
      " [ 0.14890974]\n",
      " ...\n",
      " [-0.29094035]\n",
      " [-0.36150123]\n",
      " [-0.09526197]]\n",
      "t [[-1.24441266]\n",
      " [-0.35604113]\n",
      " [ 0.14890974]\n",
      " ...\n",
      " [-0.29094035]\n",
      " [-0.36150123]\n",
      " [-0.09526197]]\n",
      "t [[-1.32800219]\n",
      " [-0.38503855]\n",
      " [ 0.16315512]\n",
      " ...\n",
      " [-0.31280662]\n",
      " [-0.3866841 ]\n",
      " [-0.10371366]]\n",
      "loss=31588.961888215897\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.20339434]\n",
      " [-0.0442391 ]\n",
      " [ 0.01447973]\n",
      " ...\n",
      " [ 0.22796754]\n",
      " [-0.16785345]\n",
      " [ 0.18490819]]\n",
      "t [[-0.20339434]\n",
      " [-0.0442391 ]\n",
      " [ 0.01447973]\n",
      " ...\n",
      " [ 0.22796754]\n",
      " [-0.16785345]\n",
      " [ 0.18490819]]\n",
      "t [[-0.38433065]\n",
      " [-0.0878299 ]\n",
      " [ 0.03195947]\n",
      " ...\n",
      " [ 0.4266597 ]\n",
      " [-0.3155508 ]\n",
      " [ 0.34790802]]\n",
      "t [[-0.38433065]\n",
      " [-0.0878299 ]\n",
      " [ 0.03195947]\n",
      " ...\n",
      " [ 0.4266597 ]\n",
      " [-0.3155508 ]\n",
      " [ 0.34790802]]\n",
      "Current iteration=2, loss=35403.25584611073\n",
      "t [[-0.54594555]\n",
      " [-0.12994278]\n",
      " [ 0.05093739]\n",
      " ...\n",
      " [ 0.60003793]\n",
      " [-0.44561907]\n",
      " [ 0.49184593]]\n",
      "t [[-0.54594555]\n",
      " [-0.12994278]\n",
      " [ 0.05093739]\n",
      " ...\n",
      " [ 0.60003793]\n",
      " [-0.44561907]\n",
      " [ 0.49184593]]\n",
      "t [[-0.69099109]\n",
      " [-0.1701114 ]\n",
      " [ 0.07037001]\n",
      " ...\n",
      " [ 0.75168098]\n",
      " [-0.56039986]\n",
      " [ 0.61931789]]\n",
      "t [[-0.69099109]\n",
      " [-0.1701114 ]\n",
      " [ 0.07037001]\n",
      " ...\n",
      " [ 0.75168098]\n",
      " [-0.56039986]\n",
      " [ 0.61931789]]\n",
      "Current iteration=4, loss=33927.34089558681\n",
      "t [[-0.82181172]\n",
      " [-0.20812224]\n",
      " [ 0.08956794]\n",
      " ...\n",
      " [ 0.88470481]\n",
      " [-0.66197305]\n",
      " [ 0.7326011 ]]\n",
      "t [[-0.82181172]\n",
      " [-0.20812224]\n",
      " [ 0.08956794]\n",
      " ...\n",
      " [ 0.88470481]\n",
      " [-0.66197305]\n",
      " [ 0.7326011 ]]\n",
      "t [[-0.94037541]\n",
      " [-0.24392417]\n",
      " [ 0.10809816]\n",
      " ...\n",
      " [ 1.0017673 ]\n",
      " [-0.75214136]\n",
      " [ 0.83365016]]\n",
      "t [[-0.94037541]\n",
      " [-0.24392417]\n",
      " [ 0.10809816]\n",
      " ...\n",
      " [ 1.0017673 ]\n",
      " [-0.75214136]\n",
      " [ 0.83365016]]\n",
      "Current iteration=6, loss=32945.80286587268\n",
      "t [[-1.04832483]\n",
      " [-0.27756313]\n",
      " [ 0.12570596]\n",
      " ...\n",
      " [ 1.10511224]\n",
      " [-0.83244639]\n",
      " [ 0.92412404]]\n",
      "t [[-1.04832483]\n",
      " [-0.27756313]\n",
      " [ 0.12570596]\n",
      " ...\n",
      " [ 1.10511224]\n",
      " [-0.83244639]\n",
      " [ 0.92412404]]\n",
      "t [[-1.14703086]\n",
      " [-0.30913874]\n",
      " [ 0.14225725]\n",
      " ...\n",
      " [ 1.19662616]\n",
      " [-0.90419793]\n",
      " [ 1.00542426]]\n",
      "t [[-1.14703086]\n",
      " [-0.30913874]\n",
      " [ 0.14225725]\n",
      " ...\n",
      " [ 1.19662616]\n",
      " [-0.90419793]\n",
      " [ 1.00542426]]\n",
      "Current iteration=8, loss=32265.338586113845\n",
      "t [[-1.23764092]\n",
      " [-0.33877671]\n",
      " [ 0.15769769]\n",
      " ...\n",
      " [ 1.27789491]\n",
      " [-0.96850657]\n",
      " [ 1.07873415]]\n",
      "t [[-1.23764092]\n",
      " [-0.33877671]\n",
      " [ 0.15769769]\n",
      " ...\n",
      " [ 1.27789491]\n",
      " [-0.96850657]\n",
      " [ 1.07873415]]\n",
      "t [[-1.32112   ]\n",
      " [-0.36661222]\n",
      " [ 0.17202467]\n",
      " ...\n",
      " [ 1.35025452]\n",
      " [-1.02631456]\n",
      " [ 1.14505462]]\n",
      "loss=31774.747435764657\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.18328318]\n",
      " [-0.03855696]\n",
      " [ 0.03826791]\n",
      " ...\n",
      " [-0.04837235]\n",
      " [-0.06014258]\n",
      " [-0.01264332]]\n",
      "t [[-0.18328318]\n",
      " [-0.03855696]\n",
      " [ 0.03826791]\n",
      " ...\n",
      " [-0.04837235]\n",
      " [-0.06014258]\n",
      " [-0.01264332]]\n",
      "t [[-0.34174905]\n",
      " [-0.06628928]\n",
      " [ 0.07347446]\n",
      " ...\n",
      " [-0.09245711]\n",
      " [-0.11393504]\n",
      " [-0.02518832]]\n",
      "t [[-0.34174905]\n",
      " [-0.06628928]\n",
      " [ 0.07347446]\n",
      " ...\n",
      " [-0.09245711]\n",
      " [-0.11393504]\n",
      " [-0.02518832]]\n",
      "Current iteration=2, loss=35270.107661228234\n",
      "t [[-0.47844371]\n",
      " [-0.08650194]\n",
      " [ 0.1052679 ]\n",
      " ...\n",
      " [-0.13268989]\n",
      " [-0.16218904]\n",
      " [-0.03739722]]\n",
      "t [[-0.47844371]\n",
      " [-0.08650194]\n",
      " [ 0.1052679 ]\n",
      " ...\n",
      " [-0.13268989]\n",
      " [-0.16218904]\n",
      " [-0.03739722]]\n",
      "t [[-0.59628444]\n",
      " [-0.10158765]\n",
      " [ 0.13363228]\n",
      " ...\n",
      " [-0.1694873 ]\n",
      " [-0.20564113]\n",
      " [-0.04913529]]\n",
      "t [[-0.59628444]\n",
      " [-0.10158765]\n",
      " [ 0.13363228]\n",
      " ...\n",
      " [-0.1694873 ]\n",
      " [-0.20564113]\n",
      " [-0.04913529]]\n",
      "Current iteration=4, loss=33746.79197400174\n",
      "t [[-0.69790797]\n",
      " [-0.11323219]\n",
      " [ 0.15874616]\n",
      " ...\n",
      " [-0.20322836]\n",
      " [-0.2449347 ]\n",
      " [-0.06033662]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.69790797]\n",
      " [-0.11323219]\n",
      " [ 0.15874616]\n",
      " ...\n",
      " [-0.20322836]\n",
      " [-0.2449347 ]\n",
      " [-0.06033662]]\n",
      "t [[-0.78561826]\n",
      " [-0.12260686]\n",
      " [ 0.18088266]\n",
      " ...\n",
      " [-0.23424895]\n",
      " [-0.28062026]\n",
      " [-0.07097809]]\n",
      "t [[-0.78561826]\n",
      " [-0.12260686]\n",
      " [ 0.18088266]\n",
      " ...\n",
      " [-0.23424895]\n",
      " [-0.28062026]\n",
      " [-0.07097809]]\n",
      "Current iteration=6, loss=32754.036381989034\n",
      "t [[-0.86139011]\n",
      " [-0.13052058]\n",
      " [ 0.20034725]\n",
      " ...\n",
      " [-0.26284287]\n",
      " [-0.31316431]\n",
      " [-0.08106164]]\n",
      "t [[-0.86139011]\n",
      " [-0.13052058]\n",
      " [ 0.20034725]\n",
      " ...\n",
      " [-0.26284287]\n",
      " [-0.31316431]\n",
      " [-0.08106164]]\n",
      "t [[-0.92689931]\n",
      " [-0.13753087]\n",
      " [ 0.21744284]\n",
      " ...\n",
      " [-0.28926576]\n",
      " [-0.34296079]\n",
      " [-0.09060306]]\n",
      "t [[-0.92689931]\n",
      " [-0.13753087]\n",
      " [ 0.21744284]\n",
      " ...\n",
      " [-0.28926576]\n",
      " [-0.34296079]\n",
      " [-0.09060306]]\n",
      "Current iteration=8, loss=32076.69256511932\n",
      "t [[-0.98356173]\n",
      " [-0.14402158]\n",
      " [ 0.2324522 ]\n",
      " ...\n",
      " [-0.31373983]\n",
      " [-0.3703426 ]\n",
      " [-0.09962514]]\n",
      "t [[-0.98356173]\n",
      " [-0.14402158]\n",
      " [ 0.2324522 ]\n",
      " ...\n",
      " [-0.31373983]\n",
      " [-0.3703426 ]\n",
      " [-0.09962514]]\n",
      "t [[-1.03257274]\n",
      " [-0.15025642]\n",
      " [ 0.2456306 ]\n",
      " ...\n",
      " [-0.33645847]\n",
      " [-0.39559183]\n",
      " [-0.10815371]]\n",
      "loss=31594.37283803208\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.21525091]\n",
      " [-0.0484933 ]\n",
      " [ 0.01894213]\n",
      " ...\n",
      " [-0.04649795]\n",
      " [-0.06045528]\n",
      " [-0.01254837]]\n",
      "t [[-0.21525091]\n",
      " [-0.0484933 ]\n",
      " [ 0.01894213]\n",
      " ...\n",
      " [-0.04649795]\n",
      " [-0.06045528]\n",
      " [-0.01254837]]\n",
      "t [[-0.40534104]\n",
      " [-0.09591292]\n",
      " [ 0.04031541]\n",
      " ...\n",
      " [-0.08885162]\n",
      " [-0.11450654]\n",
      " [-0.0251228 ]]\n",
      "t [[-0.40534104]\n",
      " [-0.09591292]\n",
      " [ 0.04031541]\n",
      " ...\n",
      " [-0.08885162]\n",
      " [-0.11450654]\n",
      " [-0.0251228 ]]\n",
      "Current iteration=2, loss=35250.05563878862\n",
      "t [[-0.57398158]\n",
      " [-0.1413932 ]\n",
      " [ 0.06255669]\n",
      " ...\n",
      " [-0.12750351]\n",
      " [-0.16297629]\n",
      " [-0.03743592]]\n",
      "t [[-0.57398158]\n",
      " [-0.1413932 ]\n",
      " [ 0.06255669]\n",
      " ...\n",
      " [-0.12750351]\n",
      " [-0.16297629]\n",
      " [-0.03743592]]\n",
      "t [[-0.72439843]\n",
      " [-0.18448659]\n",
      " [ 0.08463731]\n",
      " ...\n",
      " [-0.16286855]\n",
      " [-0.20661049]\n",
      " [-0.04931976]]\n",
      "t [[-0.72439843]\n",
      " [-0.18448659]\n",
      " [ 0.08463731]\n",
      " ...\n",
      " [-0.16286855]\n",
      " [-0.20661049]\n",
      " [-0.04931976]]\n",
      "Current iteration=4, loss=33717.49385336034\n",
      "t [[-0.8593085 ]\n",
      " [-0.2250251 ]\n",
      " [ 0.10592407]\n",
      " ...\n",
      " [-0.19531914]\n",
      " [-0.24606017]\n",
      " [-0.06068689]]\n",
      "t [[-0.8593085 ]\n",
      " [-0.2250251 ]\n",
      " [ 0.10592407]\n",
      " ...\n",
      " [-0.19531914]\n",
      " [-0.24606017]\n",
      " [-0.06068689]]\n",
      "t [[-0.980966  ]\n",
      " [-0.26301118]\n",
      " [ 0.12605696]\n",
      " ...\n",
      " [-0.2251821 ]\n",
      " [-0.28188192]\n",
      " [-0.07150059]]\n",
      "t [[-0.980966  ]\n",
      " [-0.26301118]\n",
      " [ 0.12605696]\n",
      " ...\n",
      " [-0.2251821 ]\n",
      " [-0.28188192]\n",
      " [-0.07150059]]\n",
      "Current iteration=6, loss=32719.580686614827\n",
      "t [[-1.09123195]\n",
      " [-0.29854271]\n",
      " [ 0.1448565 ]\n",
      " ...\n",
      " [-0.25274118]\n",
      " [-0.31454696]\n",
      " [-0.08175437]]\n",
      "t [[-1.09123195]\n",
      " [-0.29854271]\n",
      " [ 0.1448565 ]\n",
      " ...\n",
      " [-0.25274118]\n",
      " [-0.31454696]\n",
      " [-0.08175437]]\n",
      "t [[-1.19164384]\n",
      " [-0.33176529]\n",
      " [ 0.16225871]\n",
      " ...\n",
      " [-0.27824188]\n",
      " [-0.34445293]\n",
      " [-0.09145896]]\n",
      "t [[-1.19164384]\n",
      " [-0.33176529]\n",
      " [ 0.16225871]\n",
      " ...\n",
      " [-0.27824188]\n",
      " [-0.34445293]\n",
      " [-0.09145896]]\n",
      "Current iteration=8, loss=32038.547720764574\n",
      "t [[-1.28347696]\n",
      " [-0.36284358]\n",
      " [ 0.17827121]\n",
      " ...\n",
      " [-0.30189658]\n",
      " [-0.37193557]\n",
      " [-0.10063422]]\n",
      "t [[-1.28347696]\n",
      " [-0.36284358]\n",
      " [ 0.17827121]\n",
      " ...\n",
      " [-0.30189658]\n",
      " [-0.37193557]\n",
      " [-0.10063422]]\n",
      "t [[-1.36779512]\n",
      " [-0.39194495]\n",
      " [ 0.19294444]\n",
      " ...\n",
      " [-0.32388945]\n",
      " [-0.3972792 ]\n",
      " [-0.10930443]]\n",
      "loss=31553.005105485914\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.21580144]\n",
      " [-0.05001569]\n",
      " [ 0.01299829]\n",
      " ...\n",
      " [-0.04596173]\n",
      " [-0.06081038]\n",
      " [-0.01229945]]\n",
      "t [[-0.21580144]\n",
      " [-0.05001569]\n",
      " [ 0.01299829]\n",
      " ...\n",
      " [-0.04596173]\n",
      " [-0.06081038]\n",
      " [-0.01229945]]\n",
      "t [[-0.40640284]\n",
      " [-0.09863609]\n",
      " [ 0.02975657]\n",
      " ...\n",
      " [-0.0880158 ]\n",
      " [-0.1151812 ]\n",
      " [-0.02464054]]\n",
      "t [[-0.40640284]\n",
      " [-0.09863609]\n",
      " [ 0.02975657]\n",
      " ...\n",
      " [-0.0880158 ]\n",
      " [-0.1151812 ]\n",
      " [-0.02464054]]\n",
      "Current iteration=2, loss=35217.920170438745\n",
      "t [[-0.57550431]\n",
      " [-0.14510108]\n",
      " [ 0.04846654]\n",
      " ...\n",
      " [-0.12654453]\n",
      " [-0.16394063]\n",
      " [-0.03675064]]\n",
      "t [[-0.57550431]\n",
      " [-0.14510108]\n",
      " [ 0.04846654]\n",
      " ...\n",
      " [-0.12654453]\n",
      " [-0.16394063]\n",
      " [-0.03675064]]\n",
      "t [[-0.72633185]\n",
      " [-0.18903181]\n",
      " [ 0.06788355]\n",
      " ...\n",
      " [-0.16191549]\n",
      " [-0.20783976]\n",
      " [-0.04846765]]\n",
      "t [[-0.72633185]\n",
      " [-0.18903181]\n",
      " [ 0.06788355]\n",
      " ...\n",
      " [-0.16191549]\n",
      " [-0.20783976]\n",
      " [-0.04846765]]\n",
      "Current iteration=4, loss=33665.81401314828\n",
      "t [[-0.86160683]\n",
      " [-0.23030512]\n",
      " [ 0.08719495]\n",
      " ...\n",
      " [-0.19446503]\n",
      " [-0.24753387]\n",
      " [-0.05970486]]\n",
      "t [[-0.86160683]\n",
      " [-0.23030512]\n",
      " [ 0.08719495]\n",
      " ...\n",
      " [-0.19446503]\n",
      " [-0.24753387]\n",
      " [-0.05970486]]\n",
      "t [[-0.98358904]\n",
      " [-0.26895302]\n",
      " [ 0.10589642]\n",
      " ...\n",
      " [-0.22449286]\n",
      " [-0.28358292]\n",
      " [-0.07042354]]\n",
      "t [[-0.98358904]\n",
      " [-0.26895302]\n",
      " [ 0.10589642]\n",
      " ...\n",
      " [-0.22449286]\n",
      " [-0.28358292]\n",
      " [-0.07042354]]\n",
      "Current iteration=6, loss=32655.53340441594\n",
      "t [[-1.09414475]\n",
      " [-0.30509321]\n",
      " [ 0.12369449]\n",
      " ...\n",
      " [-0.25226263]\n",
      " [-0.31646069]\n",
      " [-0.08061397]]\n",
      "t [[-1.09414475]\n",
      " [-0.30509321]\n",
      " [ 0.12369449]\n",
      " ...\n",
      " [-0.25226263]\n",
      " [-0.31646069]\n",
      " [-0.08061397]]\n",
      "t [[-1.19481595]\n",
      " [-0.33888481]\n",
      " [ 0.14043582]\n",
      " ...\n",
      " [-0.27800494]\n",
      " [-0.34656679]\n",
      " [-0.09028327]]\n",
      "t [[-1.19481595]\n",
      " [-0.33888481]\n",
      " [ 0.14043582]\n",
      " ...\n",
      " [-0.27800494]\n",
      " [-0.34656679]\n",
      " [-0.09028327]]\n",
      "Current iteration=8, loss=31966.352155386427\n",
      "t [[-1.28688165]\n",
      " [-0.37050188]\n",
      " [ 0.15605822]\n",
      " ...\n",
      " [-0.30192117]\n",
      " [-0.37423847]\n",
      " [-0.09944773]]\n",
      "t [[-1.28688165]\n",
      " [-0.37050188]\n",
      " [ 0.15605822]\n",
      " ...\n",
      " [-0.30192117]\n",
      " [-0.37423847]\n",
      " [-0.09944773]]\n",
      "t [[-1.37140865]\n",
      " [-0.40011849]\n",
      " [ 0.17055751]\n",
      " ...\n",
      " [-0.32418741]\n",
      " [-0.39976123]\n",
      " [-0.10812831]]\n",
      "loss=31475.288114436855\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.2140993 ]\n",
      " [-0.04656747]\n",
      " [ 0.01524182]\n",
      " ...\n",
      " [ 0.23996583]\n",
      " [-0.17668784]\n",
      " [ 0.1946402 ]]\n",
      "t [[-0.2140993 ]\n",
      " [-0.04656747]\n",
      " [ 0.01524182]\n",
      " ...\n",
      " [ 0.23996583]\n",
      " [-0.17668784]\n",
      " [ 0.1946402 ]]\n",
      "t [[-0.4033211 ]\n",
      " [-0.0924159 ]\n",
      " [ 0.03380644]\n",
      " ...\n",
      " [ 0.44750186]\n",
      " [-0.33104744]\n",
      " [ 0.3650115 ]]\n",
      "t [[-0.4033211 ]\n",
      " [-0.0924159 ]\n",
      " [ 0.03380644]\n",
      " ...\n",
      " [ 0.44750186]\n",
      " [-0.33104744]\n",
      " [ 0.3650115 ]]\n",
      "Current iteration=2, loss=35302.77775387729\n",
      "t [[-0.57133331]\n",
      " [-0.13657814]\n",
      " [ 0.05394199]\n",
      " ...\n",
      " [ 0.62724049]\n",
      " [-0.46603428]\n",
      " [ 0.51444311]]\n",
      "t [[-0.57133331]\n",
      " [-0.13657814]\n",
      " [ 0.05394199]\n",
      " ...\n",
      " [ 0.62724049]\n",
      " [-0.46603428]\n",
      " [ 0.51444311]]\n",
      "t [[-0.72131862]\n",
      " [-0.17853476]\n",
      " [ 0.07446236]\n",
      " ...\n",
      " [ 0.78332789]\n",
      " [-0.58436566]\n",
      " [ 0.64594439]]\n",
      "t [[-0.72131862]\n",
      " [-0.17853476]\n",
      " [ 0.07446236]\n",
      " ...\n",
      " [ 0.78332789]\n",
      " [-0.58436566]\n",
      " [ 0.64594439]]\n",
      "Current iteration=4, loss=33798.03702725709\n",
      "t [[-0.85595491]\n",
      " [-0.21806829]\n",
      " [ 0.09460819]\n",
      " ...\n",
      " [ 0.91933293]\n",
      " [-0.68842968]\n",
      " [ 0.76212579]]\n",
      "t [[-0.85595491]\n",
      " [-0.21806829]\n",
      " [ 0.09460819]\n",
      " ...\n",
      " [ 0.91933293]\n",
      " [-0.68842968]\n",
      " [ 0.76212579]]\n",
      "t [[-0.97746321]\n",
      " [-0.25514771]\n",
      " [ 0.11392081]\n",
      " ...\n",
      " [ 1.03826428]\n",
      " [-0.78027294]\n",
      " [ 0.86520219]]\n",
      "t [[-0.97746321]\n",
      " [-0.25514771]\n",
      " [ 0.11392081]\n",
      " ...\n",
      " [ 1.03826428]\n",
      " [-0.78027294]\n",
      " [ 0.86520219]]\n",
      "Current iteration=6, loss=32815.44716957206\n",
      "t [[-1.08767715]\n",
      " [-0.28984838]\n",
      " [ 0.13214471]\n",
      " ...\n",
      " [ 1.14263356]\n",
      " [-0.86162642]\n",
      " [ 0.95703288]]\n",
      "t [[-1.08767715]\n",
      " [-0.28984838]\n",
      " [ 0.13214471]\n",
      " ...\n",
      " [ 1.14263356]\n",
      " [-0.86162642]\n",
      " [ 0.95703288]]\n",
      "t [[-1.18811208]\n",
      " [-0.32230068]\n",
      " [ 0.14915772]\n",
      " ...\n",
      " [ 1.2345309 ]\n",
      " [-0.9339458 ]\n",
      " [ 1.03917276]]\n",
      "t [[-1.18811208]\n",
      " [-0.32230068]\n",
      " [ 0.14915772]\n",
      " ...\n",
      " [ 1.2345309 ]\n",
      " [-0.9339458 ]\n",
      " [ 1.03917276]]\n",
      "Current iteration=8, loss=32143.442814080183\n",
      "t [[-1.28002558]\n",
      " [-0.3526589 ]\n",
      " [ 0.16492312]\n",
      " ...\n",
      " [ 1.31569706]\n",
      " [-0.99845381]\n",
      " [ 1.1129227 ]]\n",
      "t [[-1.28002558]\n",
      " [-0.3526589 ]\n",
      " [ 0.16492312]\n",
      " ...\n",
      " [ 1.31569706]\n",
      " [-0.99845381]\n",
      " [ 1.1129227 ]]\n",
      "t [[-1.36446736]\n",
      " [-0.38108324]\n",
      " [ 0.17945753]\n",
      " ...\n",
      " [ 1.38758634]\n",
      " [-1.05617913]\n",
      " [ 1.17937408]]\n",
      "loss=31663.652703828506\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.19244733]\n",
      " [-0.04048481]\n",
      " [ 0.0401813 ]\n",
      " ...\n",
      " [-0.05079097]\n",
      " [-0.06314971]\n",
      " [-0.01327549]]\n",
      "t [[-0.19244733]\n",
      " [-0.04048481]\n",
      " [ 0.0401813 ]\n",
      " ...\n",
      " [-0.05079097]\n",
      " [-0.06314971]\n",
      " [-0.01327549]]\n",
      "t [[-0.35753998]\n",
      " [-0.06903867]\n",
      " [ 0.07698772]\n",
      " ...\n",
      " [-0.09685629]\n",
      " [-0.11930014]\n",
      " [-0.02644262]]\n",
      "t [[-0.35753998]\n",
      " [-0.06903867]\n",
      " [ 0.07698772]\n",
      " ...\n",
      " [-0.09685629]\n",
      " [-0.11930014]\n",
      " [-0.02644262]]\n",
      "Current iteration=2, loss=35170.20514069736\n",
      "t [[-0.49881379]\n",
      " [-0.08948893]\n",
      " [ 0.11001384]\n",
      " ...\n",
      " [-0.13870241]\n",
      " [-0.16939343]\n",
      " [-0.03922667]]\n",
      "t [[-0.49881379]\n",
      " [-0.08948893]\n",
      " [ 0.11001384]\n",
      " ...\n",
      " [-0.13870241]\n",
      " [-0.16939343]\n",
      " [-0.03922667]]\n",
      "t [[-0.61963532]\n",
      " [-0.10454717]\n",
      " [ 0.13926196]\n",
      " ...\n",
      " [-0.17681003]\n",
      " [-0.21427702]\n",
      " [-0.0514781 ]]\n",
      "t [[-0.61963532]\n",
      " [-0.10454717]\n",
      " [ 0.13926196]\n",
      " ...\n",
      " [-0.17681003]\n",
      " [-0.21427702]\n",
      " [-0.0514781 ]]\n",
      "Current iteration=4, loss=33621.094900485674\n",
      "t [[-0.7230172 ]\n",
      " [-0.11608027]\n",
      " [ 0.16496036]\n",
      " ...\n",
      " [-0.21161224]\n",
      " [-0.25468318]\n",
      " [-0.06312848]]\n",
      "t [[-0.7230172 ]\n",
      " [-0.11608027]\n",
      " [ 0.16496036]\n",
      " ...\n",
      " [-0.21161224]\n",
      " [-0.25468318]\n",
      " [-0.06312848]]\n",
      "t [[-0.81156223]\n",
      " [-0.12535589]\n",
      " [ 0.18743939]\n",
      " ...\n",
      " [-0.24348915]\n",
      " [-0.29123168]\n",
      " [-0.07415797]]\n",
      "t [[-0.81156223]\n",
      " [-0.12535589]\n",
      " [ 0.18743939]\n",
      " ...\n",
      " [-0.24348915]\n",
      " [-0.29123168]\n",
      " [-0.07415797]]\n",
      "Current iteration=6, loss=32629.579408117806\n",
      "t [[-0.88747646]\n",
      " [-0.13323008]\n",
      " [ 0.20705773]\n",
      " ...\n",
      " [-0.2727703 ]\n",
      " [-0.32444227]\n",
      " [-0.08457396]]\n",
      "t [[-0.88747646]\n",
      " [-0.13323008]\n",
      " [ 0.20705773]\n",
      " ...\n",
      " [-0.2727703 ]\n",
      " [-0.32444227]\n",
      " [-0.08457396]]\n",
      "t [[-0.9526121 ]\n",
      " [-0.14027997]\n",
      " [ 0.22416335]\n",
      " ...\n",
      " [-0.29974016]\n",
      " [-0.35474985]\n",
      " [-0.09439811]]\n",
      "t [[-0.9526121 ]\n",
      " [-0.14027997]\n",
      " [ 0.22416335]\n",
      " ...\n",
      " [-0.29974016]\n",
      " [-0.35474985]\n",
      " [-0.09439811]]\n",
      "Current iteration=8, loss=31961.938188451248\n",
      "t [[-1.00851898]\n",
      " [-0.14689414]\n",
      " [ 0.23907536]\n",
      " ...\n",
      " [-0.32464427]\n",
      " [-0.38251885]\n",
      " [-0.10365877]]\n",
      "t [[-1.00851898]\n",
      " [-0.14689414]\n",
      " [ 0.23907536]\n",
      " ...\n",
      " [-0.32464427]\n",
      " [-0.38251885]\n",
      " [-0.10365877]]\n",
      "t [[-1.05649409]\n",
      " [-0.1533333 ]\n",
      " [ 0.25207755]\n",
      " ...\n",
      " [-0.34769499]\n",
      " [-0.40805583]\n",
      " [-0.11238667]]\n",
      "loss=31490.92171952595\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.22601345]\n",
      " [-0.05091796]\n",
      " [ 0.01988924]\n",
      " ...\n",
      " [-0.04882285]\n",
      " [-0.06347805]\n",
      " [-0.01317579]]\n",
      "t [[-0.22601345]\n",
      " [-0.05091796]\n",
      " [ 0.01988924]\n",
      " ...\n",
      " [-0.04882285]\n",
      " [-0.06347805]\n",
      " [-0.01317579]]\n",
      "t [[-0.42429507]\n",
      " [-0.10065157]\n",
      " [ 0.04245769]\n",
      " ...\n",
      " [-0.09307797]\n",
      " [-0.11989742]\n",
      " [-0.02638032]]\n",
      "t [[-0.42429507]\n",
      " [-0.10065157]\n",
      " [ 0.04245769]\n",
      " ...\n",
      " [-0.09307797]\n",
      " [-0.11989742]\n",
      " [-0.02638032]]\n",
      "Current iteration=2, loss=35149.44581896778\n",
      "t [[-0.59915159]\n",
      " [-0.14819961]\n",
      " [ 0.06589631]\n",
      " ...\n",
      " [-0.13327968]\n",
      " [-0.17021283]\n",
      " [-0.03928143]]\n",
      "t [[-0.59915159]\n",
      " [-0.14819961]\n",
      " [ 0.06589631]\n",
      " ...\n",
      " [-0.13327968]\n",
      " [-0.17021283]\n",
      " [-0.03928143]]\n",
      "t [[-0.75428437]\n",
      " [-0.19307053]\n",
      " [ 0.08904816]\n",
      " ...\n",
      " [-0.16990621]\n",
      " [-0.2152825 ]\n",
      " [-0.05169214]]\n",
      "t [[-0.75428437]\n",
      " [-0.19307053]\n",
      " [ 0.08904816]\n",
      " ...\n",
      " [-0.16990621]\n",
      " [-0.2152825 ]\n",
      " [-0.05169214]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4, loss=33591.10601340679\n",
      "t [[-0.89277185]\n",
      " [-0.23510236]\n",
      " [ 0.1112268 ]\n",
      " ...\n",
      " [-0.20338251]\n",
      " [-0.25584732]\n",
      " [-0.0635205 ]]\n",
      "t [[-0.89277185]\n",
      " [-0.23510236]\n",
      " [ 0.1112268 ]\n",
      " ...\n",
      " [-0.20338251]\n",
      " [-0.25584732]\n",
      " [-0.0635205 ]]\n",
      "t [[-1.01713773]\n",
      " [-0.27432592]\n",
      " [ 0.1320614 ]\n",
      " ...\n",
      " [-0.23407786]\n",
      " [-0.29253379]\n",
      " [-0.07473223]]\n",
      "t [[-1.01713773]\n",
      " [-0.27432592]\n",
      " [ 0.1320614 ]\n",
      " ...\n",
      " [-0.23407786]\n",
      " [-0.29253379]\n",
      " [-0.07473223]]\n",
      "Current iteration=6, loss=32594.482260864766\n",
      "t [[-1.12944294]\n",
      " [-0.3108742 ]\n",
      " [ 0.15138328]\n",
      " ...\n",
      " [-0.26231005]\n",
      " [-0.32586686]\n",
      " [-0.08532612]]\n",
      "t [[-1.12944294]\n",
      " [-0.3108742 ]\n",
      " [ 0.15138328]\n",
      " ...\n",
      " [-0.26231005]\n",
      " [-0.32586686]\n",
      " [-0.08532612]]\n",
      "t [[-1.23137398]\n",
      " [-0.34492726]\n",
      " [ 0.16914877]\n",
      " ...\n",
      " [-0.28835182]\n",
      " [-0.35628537]\n",
      " [-0.09531887]]\n",
      "t [[-1.23137398]\n",
      " [-0.34492726]\n",
      " [ 0.16914877]\n",
      " ...\n",
      " [-0.28835182]\n",
      " [-0.35628537]\n",
      " [-0.09531887]]\n",
      "Current iteration=8, loss=31923.115781708624\n",
      "t [[-1.32431814]\n",
      " [-0.37668044]\n",
      " [ 0.18538885]\n",
      " ...\n",
      " [-0.3124375 ]\n",
      " [-0.38415679]\n",
      " [-0.10473612]]\n",
      "t [[-1.32431814]\n",
      " [-0.37668044]\n",
      " [ 0.18538885]\n",
      " ...\n",
      " [-0.3124375 ]\n",
      " [-0.38415679]\n",
      " [-0.10473612]]\n",
      "t [[-1.40942439]\n",
      " [-0.40632722]\n",
      " [ 0.20017694]\n",
      " ...\n",
      " [-0.33476905]\n",
      " [-0.40979001]\n",
      " [-0.1136073 ]]\n",
      "loss=31448.767660157006\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.22659151]\n",
      " [-0.05251647]\n",
      " [ 0.0136482 ]\n",
      " ...\n",
      " [-0.04825982]\n",
      " [-0.0638509 ]\n",
      " [-0.01291443]]\n",
      "t [[-0.22659151]\n",
      " [-0.05251647]\n",
      " [ 0.0136482 ]\n",
      " ...\n",
      " [-0.04825982]\n",
      " [-0.0638509 ]\n",
      " [-0.01291443]]\n",
      "t [[-0.42540806]\n",
      " [-0.10349404]\n",
      " [ 0.03144016]\n",
      " ...\n",
      " [-0.09221271]\n",
      " [-0.12060398]\n",
      " [-0.02587475]]\n",
      "t [[-0.42540806]\n",
      " [-0.10349404]\n",
      " [ 0.03144016]\n",
      " ...\n",
      " [-0.09221271]\n",
      " [-0.12060398]\n",
      " [-0.02587475]]\n",
      "Current iteration=2, loss=35116.00648906399\n",
      "t [[-0.60074376]\n",
      " [-0.15205365]\n",
      " [ 0.05128243]\n",
      " ...\n",
      " [-0.13230324]\n",
      " [-0.17122055]\n",
      " [-0.03856585]]\n",
      "t [[-0.60074376]\n",
      " [-0.15205365]\n",
      " [ 0.05128243]\n",
      " ...\n",
      " [-0.13230324]\n",
      " [-0.17122055]\n",
      " [-0.03856585]]\n",
      "t [[-0.75630063]\n",
      " [-0.19778116]\n",
      " [ 0.07177024]\n",
      " ...\n",
      " [-0.1689557 ]\n",
      " [-0.21656473]\n",
      " [-0.05080702]]\n",
      "t [[-0.75630063]\n",
      " [-0.19778116]\n",
      " [ 0.07177024]\n",
      " ...\n",
      " [-0.1689557 ]\n",
      " [-0.21656473]\n",
      " [-0.05080702]]\n",
      "Current iteration=4, loss=33537.82951630776\n",
      "t [[-0.89516261]\n",
      " [-0.24056378]\n",
      " [ 0.09201425]\n",
      " ...\n",
      " [-0.20255464]\n",
      " [-0.25738219]\n",
      " [-0.06250655]]\n",
      "t [[-0.89516261]\n",
      " [-0.24056378]\n",
      " [ 0.09201425]\n",
      " ...\n",
      " [-0.20255464]\n",
      " [-0.25738219]\n",
      " [-0.06250655]]\n",
      "t [[-1.01985987]\n",
      " [-0.28046397]\n",
      " [ 0.11148307]\n",
      " ...\n",
      " [-0.23343958]\n",
      " [-0.29430313]\n",
      " [-0.07362743]]\n",
      "t [[-1.01985987]\n",
      " [-0.28046397]\n",
      " [ 0.11148307]\n",
      " ...\n",
      " [-0.23343958]\n",
      " [-0.29430313]\n",
      " [-0.07362743]]\n",
      "Current iteration=6, loss=32528.89953106414\n",
      "t [[-1.13245928]\n",
      " [-0.31763552]\n",
      " [ 0.12988311]\n",
      " ...\n",
      " [-0.26190661]\n",
      " [-0.32785528]\n",
      " [-0.0841645 ]]\n",
      "t [[-1.13245928]\n",
      " [-0.31763552]\n",
      " [ 0.12988311]\n",
      " ...\n",
      " [-0.26190661]\n",
      " [-0.32785528]\n",
      " [-0.0841645 ]]\n",
      "t [[-1.23465228]\n",
      " [-0.3522724 ]\n",
      " [ 0.14707427]\n",
      " ...\n",
      " [-0.28821268]\n",
      " [-0.3584796 ]\n",
      " [-0.09413017]]\n",
      "t [[-1.23465228]\n",
      " [-0.3522724 ]\n",
      " [ 0.14707427]\n",
      " ...\n",
      " [-0.28821268]\n",
      " [-0.3584796 ]\n",
      " [-0.09413017]]\n",
      "Current iteration=8, loss=31849.559275171865\n",
      "t [[-1.32783021]\n",
      " [-0.3845795 ]\n",
      " [ 0.16301318]\n",
      " ...\n",
      " [-0.31258069]\n",
      " [-0.38654513]\n",
      " [-0.10354599]]\n",
      "t [[-1.32783021]\n",
      " [-0.3845795 ]\n",
      " [ 0.16301318]\n",
      " ...\n",
      " [-0.31258069]\n",
      " [-0.38654513]\n",
      " [-0.10354599]]\n",
      "t [[-1.41314521]\n",
      " [-0.414757  ]\n",
      " [ 0.17771586]\n",
      " ...\n",
      " [-0.33520432]\n",
      " [-0.41236201]\n",
      " [-0.11243768]]\n",
      "loss=31369.894879411993\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.22480427]\n",
      " [-0.04889584]\n",
      " [ 0.01600391]\n",
      " ...\n",
      " [ 0.25196412]\n",
      " [-0.18552223]\n",
      " [ 0.20437221]]\n",
      "t [[-0.22480427]\n",
      " [-0.04889584]\n",
      " [ 0.01600391]\n",
      " ...\n",
      " [ 0.25196412]\n",
      " [-0.18552223]\n",
      " [ 0.20437221]]\n",
      "t [[-0.42218888]\n",
      " [-0.09699812]\n",
      " [ 0.03566969]\n",
      " ...\n",
      " [ 0.468184  ]\n",
      " [-0.34643381]\n",
      " [ 0.38199524]]\n",
      "t [[-0.42218888]\n",
      " [-0.09699812]\n",
      " [ 0.03566969]\n",
      " ...\n",
      " [ 0.468184  ]\n",
      " [-0.34643381]\n",
      " [ 0.38199524]]\n",
      "Current iteration=2, loss=35204.179883310906\n",
      "t [[-0.59641032]\n",
      " [-0.14318795]\n",
      " [ 0.0569693 ]\n",
      " ...\n",
      " [ 0.65403527]\n",
      " [-0.48616498]\n",
      " [ 0.53673323]]\n",
      "t [[-0.59641032]\n",
      " [-0.14318795]\n",
      " [ 0.0569693 ]\n",
      " ...\n",
      " [ 0.65403527]\n",
      " [-0.48616498]\n",
      " [ 0.53673323]]\n",
      "t [[-0.75111997]\n",
      " [-0.18689248]\n",
      " [ 0.07856572]\n",
      " ...\n",
      " [ 0.81428348]\n",
      " [-0.60784408]\n",
      " [ 0.67204708]]\n",
      "t [[-0.75111997]\n",
      " [-0.18689248]\n",
      " [ 0.07856572]\n",
      " ...\n",
      " [ 0.81428348]\n",
      " [-0.60784408]\n",
      " [ 0.67204708]]\n",
      "Current iteration=4, loss=33673.77636580302\n",
      "t [[-0.8893518 ]\n",
      " [-0.22789523]\n",
      " [ 0.09963049]\n",
      " ...\n",
      " [ 0.95298313]\n",
      " [-0.71419091]\n",
      " [ 0.79090498]]\n",
      "t [[-0.8893518 ]\n",
      " [-0.22789523]\n",
      " [ 0.09963049]\n",
      " ...\n",
      " [ 0.95298313]\n",
      " [-0.71419091]\n",
      " [ 0.79090498]]\n",
      "t [[-1.0135922 ]\n",
      " [-0.26619109]\n",
      " [ 0.11968367]\n",
      " ...\n",
      " [ 1.0735122 ]\n",
      " [-0.80750987]\n",
      " [ 0.89579629]]\n",
      "t [[-1.0135922 ]\n",
      " [-0.26619109]\n",
      " [ 0.11968367]\n",
      " ...\n",
      " [ 1.0735122 ]\n",
      " [-0.80750987]\n",
      " [ 0.89579629]]\n",
      "Current iteration=6, loss=32692.140628395275\n",
      "t [[-1.12587182]\n",
      " [-0.30188947]\n",
      " [ 0.13847386]\n",
      " ...\n",
      " [ 1.17865957]\n",
      " [-0.88972878]\n",
      " [ 0.988788  ]]\n",
      "t [[-1.12587182]\n",
      " [-0.30188947]\n",
      " [ 0.13847386]\n",
      " ...\n",
      " [ 1.17865957]\n",
      " [-0.88972878]\n",
      " [ 0.988788  ]]\n",
      "t [[-1.22785306]\n",
      " [-0.33515479]\n",
      " [ 0.15589511]\n",
      " ...\n",
      " [ 1.27072279]\n",
      " [-0.96245258]\n",
      " [ 1.07159168]]\n",
      "t [[-1.22785306]\n",
      " [-0.33515479]\n",
      " [ 0.15589511]\n",
      " ...\n",
      " [ 1.27072279]\n",
      " [-0.96245258]\n",
      " [ 1.07159168]]\n",
      "Current iteration=8, loss=32029.458744209987\n",
      "t [[-1.3209042 ]\n",
      " [-0.36617198]\n",
      " [ 0.17193164]\n",
      " ...\n",
      " [ 1.35159819]\n",
      " [-1.02701709]\n",
      " [ 1.14562658]]\n",
      "t [[-1.3209042 ]\n",
      " [-0.36617198]\n",
      " [ 0.17193164]\n",
      " ...\n",
      " [ 1.35159819]\n",
      " [-1.02701709]\n",
      " [ 1.14562658]]\n",
      "t [[-1.40615928]\n",
      " [-0.39512736]\n",
      " [ 0.18662176]\n",
      " ...\n",
      " [ 1.42285737]\n",
      " [-1.08453697]\n",
      " [ 1.21207405]]\n",
      "loss=31560.607053444925\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.20161149]\n",
      " [-0.04241266]\n",
      " [ 0.0420947 ]\n",
      " ...\n",
      " [-0.05320959]\n",
      " [-0.06615684]\n",
      " [-0.01390765]]\n",
      "t [[-0.20161149]\n",
      " [-0.04241266]\n",
      " [ 0.0420947 ]\n",
      " ...\n",
      " [-0.05320959]\n",
      " [-0.06615684]\n",
      " [-0.01390765]]\n",
      "t [[-0.3732084 ]\n",
      " [-0.07173474]\n",
      " [ 0.08048574]\n",
      " ...\n",
      " [-0.10123437]\n",
      " [-0.12463392]\n",
      " [-0.02769645]]\n",
      "t [[-0.3732084 ]\n",
      " [-0.07173474]\n",
      " [ 0.08048574]\n",
      " ...\n",
      " [-0.10123437]\n",
      " [-0.12463392]\n",
      " [-0.02769645]]\n",
      "Current iteration=2, loss=35072.18152278497\n",
      "t [[-0.51886667]\n",
      " [-0.09236799]\n",
      " [ 0.11470909]\n",
      " ...\n",
      " [-0.14465906]\n",
      " [-0.17651725]\n",
      " [-0.04105111]]\n",
      "t [[-0.51886667]\n",
      " [-0.09236799]\n",
      " [ 0.11470909]\n",
      " ...\n",
      " [-0.14465906]\n",
      " [-0.17651725]\n",
      " [-0.04105111]]\n",
      "t [[-0.64244351]\n",
      " [-0.10736258]\n",
      " [ 0.14479112]\n",
      " ...\n",
      " [-0.18403421]\n",
      " [-0.22277482]\n",
      " [-0.05380708]]\n",
      "t [[-0.64244351]\n",
      " [-0.10736258]\n",
      " [ 0.14479112]\n",
      " ...\n",
      " [-0.18403421]\n",
      " [-0.22277482]\n",
      " [-0.05380708]]\n",
      "Current iteration=4, loss=33500.299434206005\n",
      "t [[-0.74735545]\n",
      " [-0.11876969]\n",
      " [ 0.17101775]\n",
      " ...\n",
      " [-0.21985102]\n",
      " [-0.26423396]\n",
      " [-0.06589441]]\n",
      "t [[-0.74735545]\n",
      " [-0.11876969]\n",
      " [ 0.17101775]\n",
      " ...\n",
      " [-0.21985102]\n",
      " [-0.26423396]\n",
      " [-0.06589441]]\n",
      "t [[-0.83652069]\n",
      " [-0.12795008]\n",
      " [ 0.19378288]\n",
      " ...\n",
      " [-0.25253646]\n",
      " [-0.30158721]\n",
      " [-0.07729761]]\n",
      "t [[-0.83652069]\n",
      " [-0.12795008]\n",
      " [ 0.19378288]\n",
      " ...\n",
      " [-0.25253646]\n",
      " [-0.30158721]\n",
      " [-0.07729761]]\n",
      "Current iteration=6, loss=32511.812857827077\n",
      "t [[-0.91238494]\n",
      " [-0.13580206]\n",
      " [ 0.21350251]\n",
      " ...\n",
      " [-0.28245751]\n",
      " [-0.33540954]\n",
      " [-0.08803044]]\n",
      "t [[-0.91238494]\n",
      " [-0.13580206]\n",
      " [ 0.21350251]\n",
      " ...\n",
      " [-0.28245751]\n",
      " [-0.33540954]\n",
      " [-0.08803044]]\n",
      "t [[-0.97698027]\n",
      " [-0.14291783]\n",
      " [ 0.23057181]\n",
      " ...\n",
      " [-0.30992852]\n",
      " [-0.36617762]\n",
      " [-0.09812105]]\n",
      "t [[-0.97698027]\n",
      " [-0.14291783]\n",
      " [ 0.23057181]\n",
      " ...\n",
      " [-0.30992852]\n",
      " [-0.36617762]\n",
      " [-0.09812105]]\n",
      "Current iteration=8, loss=31854.549697711875\n",
      "t [[-1.03199088]\n",
      " [-0.14968704]\n",
      " [ 0.24534709]\n",
      " ...\n",
      " [-0.33521897]\n",
      " [-0.39428771]\n",
      " [-0.10760372]]\n",
      "t [[-1.03199088]\n",
      " [-0.14968704]\n",
      " [ 0.24534709]\n",
      " ...\n",
      " [-0.33521897]\n",
      " [-0.39428771]\n",
      " [-0.10760372]]\n",
      "t [[-1.07881427]\n",
      " [-0.15636468]\n",
      " [ 0.25814066]\n",
      " ...\n",
      " [-0.35856056]\n",
      " [-0.42007083]\n",
      " [-0.11651438]]\n",
      "loss=31394.84903311285\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.236776  ]\n",
      " [-0.05334263]\n",
      " [ 0.02083634]\n",
      " ...\n",
      " [-0.05114775]\n",
      " [-0.06650081]\n",
      " [-0.0138032 ]]\n",
      "t [[-0.236776  ]\n",
      " [-0.05334263]\n",
      " [ 0.02083634]\n",
      " ...\n",
      " [-0.05114775]\n",
      " [-0.06650081]\n",
      " [-0.0138032 ]]\n",
      "t [[-0.44312525]\n",
      " [-0.1053847 ]\n",
      " [ 0.04461185]\n",
      " ...\n",
      " [-0.09728393]\n",
      " [-0.12525671]\n",
      " [-0.02763797]]\n",
      "t [[-0.44312525]\n",
      " [-0.1053847 ]\n",
      " [ 0.04461185]\n",
      " ...\n",
      " [-0.09728393]\n",
      " [-0.12525671]\n",
      " [-0.02763797]]\n",
      "Current iteration=2, loss=35050.73787244322\n",
      "t [[-0.62401097]\n",
      " [-0.15497622]\n",
      " [ 0.06924728]\n",
      " ...\n",
      " [-0.13900216]\n",
      " [-0.17736818]\n",
      " [-0.04112298]]\n",
      "t [[-0.62401097]\n",
      " [-0.15497622]\n",
      " [ 0.06924728]\n",
      " ...\n",
      " [-0.13900216]\n",
      " [-0.17736818]\n",
      " [-0.04112298]]\n",
      "t [[-0.78364919]\n",
      " [-0.20158258]\n",
      " [ 0.09345152]\n",
      " ...\n",
      " [-0.17684958]\n",
      " [-0.22381547]\n",
      " [-0.05405183]]\n",
      "t [[-0.78364919]\n",
      " [-0.20158258]\n",
      " [ 0.09345152]\n",
      " ...\n",
      " [-0.17684958]\n",
      " [-0.22381547]\n",
      " [-0.05405183]]\n",
      "Current iteration=4, loss=33469.65851320074\n",
      "t [[-0.92550231]\n",
      " [-0.24505357]\n",
      " [ 0.11648743]\n",
      " ...\n",
      " [-0.21130752]\n",
      " [-0.26543556]\n",
      " [-0.06632902]]\n",
      "t [[-0.92550231]\n",
      " [-0.24505357]\n",
      " [ 0.11648743]\n",
      " ...\n",
      " [-0.21130752]\n",
      " [-0.26543556]\n",
      " [-0.06632902]]\n",
      "t [[-1.052375  ]\n",
      " [-0.28545421]\n",
      " [ 0.13797854]\n",
      " ...\n",
      " [-0.24279018]\n",
      " [-0.30292842]\n",
      " [-0.07792396]]\n",
      "t [[-1.052375  ]\n",
      " [-0.28545421]\n",
      " [ 0.13797854]\n",
      " ...\n",
      " [-0.24279018]\n",
      " [-0.30292842]\n",
      " [-0.07792396]]\n",
      "Current iteration=6, loss=32476.105991062665\n",
      "t [[-1.16653338]\n",
      " [-0.32295713]\n",
      " [ 0.15777203]\n",
      " ...\n",
      " [-0.27165089]\n",
      " [-0.33687461]\n",
      " [-0.08884169]]\n",
      "t [[-1.16653338]\n",
      " [-0.32295713]\n",
      " [ 0.15777203]\n",
      " ...\n",
      " [-0.27165089]\n",
      " [-0.33687461]\n",
      " [-0.08884169]]\n",
      "t [[-1.26981474]\n",
      " [-0.35777976]\n",
      " [ 0.17584868]\n",
      " ...\n",
      " [-0.29819059]\n",
      " [-0.36775506]\n",
      " [-0.09910559]]\n",
      "t [[-1.26981474]\n",
      " [-0.35777976]\n",
      " [ 0.17584868]\n",
      " ...\n",
      " [-0.29819059]\n",
      " [-0.36775506]\n",
      " [-0.09910559]]\n",
      "Current iteration=8, loss=31815.067451519764\n",
      "t [[-1.36371844]\n",
      " [-0.39014984]\n",
      " [ 0.19226573]\n",
      " ...\n",
      " [-0.32266611]\n",
      " [-0.39596923]\n",
      " [-0.1087475 ]]\n",
      "t [[-1.36371844]\n",
      " [-0.39014984]\n",
      " [ 0.19226573]\n",
      " ...\n",
      " [-0.32266611]\n",
      " [-0.39596923]\n",
      " [-0.1087475 ]]\n",
      "t [[-1.44947778]\n",
      " [-0.42028782]\n",
      " [ 0.20712159]\n",
      " ...\n",
      " [-0.34529749]\n",
      " [-0.42185054]\n",
      " [-0.11780233]]\n",
      "loss=31351.914308832354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.23738158]\n",
      " [-0.05501726]\n",
      " [ 0.01429812]\n",
      " ...\n",
      " [-0.0505579 ]\n",
      " [-0.06689142]\n",
      " [-0.0135294 ]]\n",
      "t [[-0.23738158]\n",
      " [-0.05501726]\n",
      " [ 0.01429812]\n",
      " ...\n",
      " [-0.0505579 ]\n",
      " [-0.06689142]\n",
      " [-0.0135294 ]]\n",
      "t [[-0.44428926]\n",
      " [-0.10834487]\n",
      " [ 0.03314215]\n",
      " ...\n",
      " [-0.09639039]\n",
      " [-0.12599501]\n",
      " [-0.02710918]]\n",
      "t [[-0.44428926]\n",
      " [-0.10834487]\n",
      " [ 0.03314215]\n",
      " ...\n",
      " [-0.09639039]\n",
      " [-0.12599501]\n",
      " [-0.02710918]]\n",
      "Current iteration=2, loss=35016.02032185588\n",
      "t [[-0.62567197]\n",
      " [-0.15897329]\n",
      " [ 0.05412523]\n",
      " ...\n",
      " [-0.13801079]\n",
      " [-0.17841888]\n",
      " [-0.04037755]]\n",
      "t [[-0.62567197]\n",
      " [-0.15897329]\n",
      " [ 0.05412523]\n",
      " ...\n",
      " [-0.13801079]\n",
      " [-0.17841888]\n",
      " [-0.04037755]]\n",
      "t [[-0.7857471 ]\n",
      " [-0.20645443]\n",
      " [ 0.07567394]\n",
      " ...\n",
      " [-0.17590524]\n",
      " [-0.22515001]\n",
      " [-0.05313474]]\n",
      "t [[-0.7857471 ]\n",
      " [-0.20645443]\n",
      " [ 0.07567394]\n",
      " ...\n",
      " [-0.17590524]\n",
      " [-0.22515001]\n",
      " [-0.05313474]]\n",
      "Current iteration=4, loss=33414.850997115325\n",
      "t [[-0.92798367]\n",
      " [-0.25069161]\n",
      " [ 0.09682347]\n",
      " ...\n",
      " [-0.21051016]\n",
      " [-0.26703071]\n",
      " [-0.06528491]]\n",
      "t [[-0.92798367]\n",
      " [-0.25069161]\n",
      " [ 0.09682347]\n",
      " ...\n",
      " [-0.21051016]\n",
      " [-0.26703071]\n",
      " [-0.06528491]]\n",
      "t [[-1.0551938 ]\n",
      " [-0.29178349]\n",
      " [ 0.11702005]\n",
      " ...\n",
      " [-0.24220732]\n",
      " [-0.30476498]\n",
      " [-0.0767939 ]]\n",
      "t [[-1.0551938 ]\n",
      " [-0.29178349]\n",
      " [ 0.11702005]\n",
      " ...\n",
      " [-0.24220732]\n",
      " [-0.30476498]\n",
      " [-0.0767939 ]]\n",
      "Current iteration=6, loss=32409.07651405449\n",
      "t [[-1.16965021]\n",
      " [-0.32992429]\n",
      " [ 0.13597511]\n",
      " ...\n",
      " [-0.27132683]\n",
      " [-0.3389364 ]\n",
      " [-0.08766197]]\n",
      "t [[-1.16965021]\n",
      " [-0.32992429]\n",
      " [ 0.13597511]\n",
      " ...\n",
      " [-0.27132683]\n",
      " [-0.3389364 ]\n",
      " [-0.08766197]]\n",
      "t [[-1.27319563]\n",
      " [-0.36534565]\n",
      " [ 0.1535661 ]\n",
      " ...\n",
      " [-0.29815304]\n",
      " [-0.3700281 ]\n",
      " [-0.09790754]]\n",
      "t [[-1.27319563]\n",
      " [-0.36534565]\n",
      " [ 0.1535661 ]\n",
      " ...\n",
      " [-0.29815304]\n",
      " [-0.3700281 ]\n",
      " [-0.09790754]]\n",
      "Current iteration=8, loss=31740.2487783194\n",
      "t [[-1.36733375]\n",
      " [-0.39828495]\n",
      " [ 0.16977172]\n",
      " ...\n",
      " [-0.32293096]\n",
      " [-0.39844124]\n",
      " [-0.10755783]]\n",
      "t [[-1.36733375]\n",
      " [-0.39828495]\n",
      " [ 0.16977172]\n",
      " ...\n",
      " [-0.32293096]\n",
      " [-0.39844124]\n",
      " [-0.10755783]]\n",
      "t [[-1.45330121]\n",
      " [-0.42896934]\n",
      " [ 0.18463045]\n",
      " ...\n",
      " [-0.34587223]\n",
      " [-0.42451051]\n",
      " [-0.11664362]]\n",
      "loss=31271.987198607472\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.23550923]\n",
      " [-0.05122422]\n",
      " [ 0.016766  ]\n",
      " ...\n",
      " [ 0.26396241]\n",
      " [-0.19435662]\n",
      " [ 0.21410422]]\n",
      "t [[-0.23550923]\n",
      " [-0.05122422]\n",
      " [ 0.016766  ]\n",
      " ...\n",
      " [ 0.26396241]\n",
      " [-0.19435662]\n",
      " [ 0.21410422]]\n",
      "t [[-0.4409342 ]\n",
      " [-0.10157655]\n",
      " [ 0.03754917]\n",
      " ...\n",
      " [ 0.48870636]\n",
      " [-0.36171008]\n",
      " [ 0.39885944]]\n",
      "t [[-0.4409342 ]\n",
      " [-0.10157655]\n",
      " [ 0.03754917]\n",
      " ...\n",
      " [ 0.48870636]\n",
      " [-0.36171008]\n",
      " [ 0.39885944]]\n",
      "Current iteration=2, loss=35107.43219105129\n",
      "t [[-0.62118012]\n",
      " [-0.14977151]\n",
      " [ 0.06001796]\n",
      " ...\n",
      " [ 0.68042674]\n",
      " [-0.50601405]\n",
      " [ 0.55871954]]\n",
      "t [[-0.62118012]\n",
      " [-0.14977151]\n",
      " [ 0.06001796]\n",
      " ...\n",
      " [ 0.68042674]\n",
      " [-0.50601405]\n",
      " [ 0.55871954]]\n",
      "t [[-0.78040572]\n",
      " [-0.19518323]\n",
      " [ 0.08267673]\n",
      " ...\n",
      " [ 0.8445615 ]\n",
      " [-0.63084422]\n",
      " [ 0.69763603]]\n",
      "t [[-0.78040572]\n",
      " [-0.19518323]\n",
      " [ 0.08267673]\n",
      " ...\n",
      " [ 0.8445615 ]\n",
      " [-0.63084422]\n",
      " [ 0.69763603]]\n",
      "Current iteration=4, loss=33554.3397691437\n",
      "t [[-0.92202305]\n",
      " [-0.23760198]\n",
      " [ 0.10462978]\n",
      " ...\n",
      " [ 0.98568285]\n",
      " [-0.73927522]\n",
      " [ 0.81895888]]\n",
      "t [[-0.92202305]\n",
      " [-0.23760198]\n",
      " [ 0.10462978]\n",
      " ...\n",
      " [ 0.98568285]\n",
      " [-0.73927522]\n",
      " [ 0.81895888]]\n",
      "t [[-1.0487952 ]\n",
      " [-0.27705455]\n",
      " [ 0.12538085]\n",
      " ...\n",
      " [ 1.10755543]\n",
      " [-0.83388246]\n",
      " [ 0.92546524]]\n",
      "t [[-1.0487952 ]\n",
      " [-0.27705455]\n",
      " [ 0.12538085]\n",
      " ...\n",
      " [ 1.10755543]\n",
      " [-0.83388246]\n",
      " [ 0.92546524]]\n",
      "Current iteration=6, loss=32575.422338156986\n",
      "t [[-1.16295509]\n",
      " [-0.31368906]\n",
      " [ 0.14468785]\n",
      " ...\n",
      " [ 1.21325343]\n",
      " [-0.91679719]\n",
      " [ 1.01943631]]\n",
      "t [[-1.16295509]\n",
      " [-0.31368906]\n",
      " [ 0.14468785]\n",
      " ...\n",
      " [ 1.21325343]\n",
      " [-0.91679719]\n",
      " [ 1.01943631]]\n",
      "t [[-1.26631398]\n",
      " [-0.34770704]\n",
      " [ 0.16246539]\n",
      " ...\n",
      " [ 1.30528465]\n",
      " [-0.98977616]\n",
      " [ 1.10274276]]\n",
      "t [[-1.26631398]\n",
      " [-0.34770704]\n",
      " [ 0.16246539]\n",
      " ...\n",
      " [ 1.30528465]\n",
      " [-0.98977616]\n",
      " [ 1.10274276]]\n",
      "Current iteration=8, loss=31922.73450796955\n",
      "t [[-1.360351  ]\n",
      " [-0.3793258 ]\n",
      " [ 0.17872182]\n",
      " ...\n",
      " [ 1.38570087]\n",
      " [-1.05426864]\n",
      " [ 1.1769225 ]]\n",
      "t [[-1.360351  ]\n",
      " [-0.3793258 ]\n",
      " [ 0.17872182]\n",
      " ...\n",
      " [ 1.38570087]\n",
      " [-1.05426864]\n",
      " [ 1.1769225 ]]\n",
      "t [[-1.44628377]\n",
      " [-0.40875873]\n",
      " [ 0.19351938]\n",
      " ...\n",
      " [ 1.45618939]\n",
      " [-1.11147441]\n",
      " [ 1.24324595]]\n",
      "loss=31464.841453033732\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.21077565]\n",
      " [-0.04434051]\n",
      " [ 0.0440081 ]\n",
      " ...\n",
      " [-0.05562821]\n",
      " [-0.06916397]\n",
      " [-0.01453982]]\n",
      "t [[-0.21077565]\n",
      " [-0.04434051]\n",
      " [ 0.0440081 ]\n",
      " ...\n",
      " [-0.05562821]\n",
      " [-0.06916397]\n",
      " [-0.01453982]]\n",
      "t [[-0.3887545 ]\n",
      " [-0.07437757]\n",
      " [ 0.08396852]\n",
      " ...\n",
      " [-0.1055914 ]\n",
      " [-0.12993643]\n",
      " [-0.02894981]]\n",
      "t [[-0.3887545 ]\n",
      " [-0.07437757]\n",
      " [ 0.08396852]\n",
      " ...\n",
      " [-0.1055914 ]\n",
      " [-0.12993643]\n",
      " [-0.02894981]]\n",
      "Current iteration=2, loss=34976.00660413042\n",
      "t [[-0.53860538]\n",
      " [-0.09514176]\n",
      " [ 0.1193535 ]\n",
      " ...\n",
      " [-0.15056033]\n",
      " [-0.18356129]\n",
      " [-0.04287039]]\n",
      "t [[-0.53860538]\n",
      " [-0.09514176]\n",
      " [ 0.1193535 ]\n",
      " ...\n",
      " [-0.15056033]\n",
      " [-0.18356129]\n",
      " [-0.04287039]]\n",
      "t [[-0.66471874]\n",
      " [-0.11004067]\n",
      " [ 0.1502201 ]\n",
      " ...\n",
      " [-0.19116133]\n",
      " [-0.23113699]\n",
      " [-0.05612195]]\n",
      "t [[-0.66471874]\n",
      " [-0.11004067]\n",
      " [ 0.1502201 ]\n",
      " ...\n",
      " [-0.19116133]\n",
      " [-0.23113699]\n",
      " [-0.05612195]]\n",
      "Current iteration=4, loss=33384.19231170906\n",
      "t [[-0.7709428 ]\n",
      " [-0.12131166]\n",
      " [ 0.17692023]\n",
      " ...\n",
      " [-0.22794771]\n",
      " [-0.27359193]\n",
      " [-0.06863411]]\n",
      "t [[-0.7709428 ]\n",
      " [-0.12131166]\n",
      " [ 0.17692023]\n",
      " ...\n",
      " [-0.22794771]\n",
      " [-0.27359193]\n",
      " [-0.06863411]]\n",
      "t [[-0.8605268 ]\n",
      " [-0.13040436]\n",
      " [ 0.19991774]\n",
      " ...\n",
      " [-0.26139586]\n",
      " [-0.31169482]\n",
      " [-0.08039693]]\n",
      "t [[-0.8605268 ]\n",
      " [-0.13040436]\n",
      " [ 0.19991774]\n",
      " ...\n",
      " [-0.26139586]\n",
      " [-0.31169482]\n",
      " [-0.08039693]]\n",
      "Current iteration=6, loss=32400.301255801194\n",
      "t [[-0.9361635 ]\n",
      " [-0.13825407]\n",
      " [ 0.21968973]\n",
      " ...\n",
      " [-0.29191179]\n",
      " [-0.34607747]\n",
      " [-0.09143139]]\n",
      "t [[-0.9361635 ]\n",
      " [-0.13825407]\n",
      " [ 0.21968973]\n",
      " ...\n",
      " [-0.29191179]\n",
      " [-0.34607747]\n",
      " [-0.09143139]]\n",
      "t [[-1.00006725]\n",
      " [-0.14546342]\n",
      " [ 0.23668043]\n",
      " ...\n",
      " [-0.31984067]\n",
      " [-0.37725905]\n",
      " [-0.10177273]]\n",
      "t [[-1.00006725]\n",
      " [-0.14546342]\n",
      " [ 0.23668043]\n",
      " ...\n",
      " [-0.31984067]\n",
      " [-0.37725905]\n",
      " [-0.10177273]]\n",
      "Current iteration=8, loss=31753.92601986581\n",
      "t [[-1.05405636]\n",
      " [-0.15241958]\n",
      " [ 0.25128382]\n",
      " ...\n",
      " [-0.3454765 ]\n",
      " [-0.40566784]\n",
      " [-0.11146152]]\n",
      "t [[-1.05405636]\n",
      " [-0.15241958]\n",
      " [ 0.25128382]\n",
      " ...\n",
      " [-0.3454765 ]\n",
      " [-0.40566784]\n",
      " [-0.11146152]]\n",
      "t [[-1.0996272 ]\n",
      " [-0.15936921]\n",
      " [ 0.2638406 ]\n",
      " ...\n",
      " [-0.36907061]\n",
      " [-0.43165913]\n",
      " [-0.1205391 ]]\n",
      "loss=31305.45947865683\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.24753854]\n",
      " [-0.05576729]\n",
      " [ 0.02178345]\n",
      " ...\n",
      " [-0.05347264]\n",
      " [-0.06952358]\n",
      " [-0.01443062]]\n",
      "t [[-0.24753854]\n",
      " [-0.05576729]\n",
      " [ 0.02178345]\n",
      " ...\n",
      " [-0.05347264]\n",
      " [-0.06952358]\n",
      " [-0.01443062]]\n",
      "t [[-0.46183182]\n",
      " [-0.11011229]\n",
      " [ 0.04677787]\n",
      " ...\n",
      " [-0.10146954]\n",
      " [-0.13058446]\n",
      " [-0.02889576]]\n",
      "t [[-0.46183182]\n",
      " [-0.11011229]\n",
      " [ 0.04677787]\n",
      " ...\n",
      " [-0.10146954]\n",
      " [-0.13058446]\n",
      " [-0.02889576]]\n",
      "Current iteration=2, loss=34953.90094232875\n",
      "t [[-0.6485633 ]\n",
      " [-0.16172244]\n",
      " [ 0.07260843]\n",
      " ...\n",
      " [-0.14467142]\n",
      " [-0.18444315]\n",
      " [-0.04296039]]\n",
      "t [[-0.6485633 ]\n",
      " [-0.16172244]\n",
      " [ 0.07260843]\n",
      " ...\n",
      " [-0.14467142]\n",
      " [-0.18444315]\n",
      " [-0.04296039]]\n",
      "t [[-0.81250348]\n",
      " [-0.21002175]\n",
      " [ 0.09784464]\n",
      " ...\n",
      " [-0.18370013]\n",
      " [-0.2322119 ]\n",
      " [-0.05639844]]\n",
      "t [[-0.81250348]\n",
      " [-0.21002175]\n",
      " [ 0.09784464]\n",
      " ...\n",
      " [-0.18370013]\n",
      " [-0.2322119 ]\n",
      " [-0.05639844]]\n",
      "Current iteration=4, loss=33352.93496731539\n",
      "t [[-0.95752026]\n",
      " [-0.25487819]\n",
      " [ 0.12170213]\n",
      " ...\n",
      " [-0.21909711]\n",
      " [-0.27482985]\n",
      " [-0.06911203]]\n",
      "t [[-0.95752026]\n",
      " [-0.25487819]\n",
      " [ 0.12170213]\n",
      " ...\n",
      " [-0.21909711]\n",
      " [-0.27482985]\n",
      " [-0.06911203]]\n",
      "t [[-1.08670984]\n",
      " [-0.29639707]\n",
      " [ 0.14380442]\n",
      " ...\n",
      " [-0.25132386]\n",
      " [-0.31307382]\n",
      " [-0.08107553]]\n",
      "t [[-1.08670984]\n",
      " [-0.29639707]\n",
      " [ 0.14380442]\n",
      " ...\n",
      " [-0.25132386]\n",
      " [-0.31307382]\n",
      " [-0.08107553]]\n",
      "Current iteration=6, loss=32364.012023137013\n",
      "t [[-1.20254788]\n",
      " [-0.33479503]\n",
      " [ 0.16401983]\n",
      " ...\n",
      " [-0.2807707 ]\n",
      " [-0.34758167]\n",
      " [-0.09230122]]\n",
      "t [[-1.20254788]\n",
      " [-0.33479503]\n",
      " [ 0.16401983]\n",
      " ...\n",
      " [-0.2807707 ]\n",
      " [-0.34758167]\n",
      " [-0.09230122]]\n",
      "t [[-1.30702369]\n",
      " [-0.37032956]\n",
      " [ 0.18235754]\n",
      " ...\n",
      " [-0.30776762]\n",
      " [-0.37887708]\n",
      " [-0.10281981]]\n",
      "t [[-1.30702369]\n",
      " [-0.37032956]\n",
      " [ 0.18235754]\n",
      " ...\n",
      " [-0.30776762]\n",
      " [-0.37887708]\n",
      " [-0.10281981]]\n",
      "Current iteration=8, loss=31713.797685101825\n",
      "t [[-1.40174832]\n",
      " [-0.40326228]\n",
      " [ 0.19890389]\n",
      " ...\n",
      " [-0.33259441]\n",
      " [-0.40739166]\n",
      " [-0.11266976]]\n",
      "t [[-1.40174832]\n",
      " [-0.40326228]\n",
      " [ 0.19890389]\n",
      " ...\n",
      " [-0.33259441]\n",
      " [-0.40739166]\n",
      " [-0.11266976]]\n",
      "t [[-1.4880383 ]\n",
      " [-0.43384126]\n",
      " [ 0.21378407]\n",
      " ...\n",
      " [-0.35548946]\n",
      " [-0.43348323]\n",
      " [-0.12189171]]\n",
      "loss=31261.747114155114\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.24817165]\n",
      " [-0.05751804]\n",
      " [ 0.01494803]\n",
      " ...\n",
      " [-0.05285599]\n",
      " [-0.06993194]\n",
      " [-0.01414437]]\n",
      "t [[-0.24817165]\n",
      " [-0.05751804]\n",
      " [ 0.01494803]\n",
      " ...\n",
      " [-0.05285599]\n",
      " [-0.06993194]\n",
      " [-0.01414437]]\n",
      "t [[-0.46304669]\n",
      " [-0.11318855]\n",
      " [ 0.03486249]\n",
      " ...\n",
      " [-0.10054889]\n",
      " [-0.13135433]\n",
      " [-0.02834381]]\n",
      "t [[-0.46304669]\n",
      " [-0.11318855]\n",
      " [ 0.03486249]\n",
      " ...\n",
      " [-0.10054889]\n",
      " [-0.13135433]\n",
      " [-0.02834381]]\n",
      "Current iteration=2, loss=34917.93037840872\n",
      "t [[-0.65029255]\n",
      " [-0.16585946]\n",
      " [ 0.05699355]\n",
      " ...\n",
      " [-0.14366762]\n",
      " [-0.18553644]\n",
      " [-0.04218557]]\n",
      "t [[-0.65029255]\n",
      " [-0.16585946]\n",
      " [ 0.05699355]\n",
      " ...\n",
      " [-0.14366762]\n",
      " [-0.18553644]\n",
      " [-0.04218557]]\n",
      "t [[-0.81468186]\n",
      " [-0.21505083]\n",
      " [ 0.07959123]\n",
      " ...\n",
      " [-0.18276544]\n",
      " [-0.23359812]\n",
      " [-0.05545044]]\n",
      "t [[-0.81468186]\n",
      " [-0.21505083]\n",
      " [ 0.07959123]\n",
      " ...\n",
      " [-0.18276544]\n",
      " [-0.23359812]\n",
      " [-0.05545044]]\n",
      "Current iteration=4, loss=33296.65907478423\n",
      "t [[-0.96009043]\n",
      " [-0.26068843]\n",
      " [ 0.10161749]\n",
      " ...\n",
      " [-0.21833429]\n",
      " [-0.27648442]\n",
      " [-0.06803953]]\n",
      "t [[-0.96009043]\n",
      " [-0.26068843]\n",
      " [ 0.10161749]\n",
      " ...\n",
      " [-0.21833429]\n",
      " [-0.27648442]\n",
      " [-0.06803953]]\n",
      "t [[-1.08962291]\n",
      " [-0.30291301]\n",
      " [ 0.12250148]\n",
      " ...\n",
      " [-0.25080055]\n",
      " [-0.31497655]\n",
      " [-0.07992265]]\n",
      "t [[-1.08962291]\n",
      " [-0.30291301]\n",
      " [ 0.12250148]\n",
      " ...\n",
      " [-0.25080055]\n",
      " [-0.31497655]\n",
      " [-0.07992265]]\n",
      "Current iteration=6, loss=32295.61872045014\n",
      "t [[-1.20576227]\n",
      " [-0.34196351]\n",
      " [ 0.14196497]\n",
      " ...\n",
      " [-0.28052987]\n",
      " [-0.34971556]\n",
      " [-0.09110645]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.20576227]\n",
      " [-0.34196351]\n",
      " [ 0.14196497]\n",
      " ...\n",
      " [-0.28052987]\n",
      " [-0.34971556]\n",
      " [-0.09110645]]\n",
      "t [[-1.31050371]\n",
      " [-0.37811184]\n",
      " [ 0.15990725]\n",
      " ...\n",
      " [-0.30783492]\n",
      " [-0.38122747]\n",
      " [-0.10161595]]\n",
      "t [[-1.31050371]\n",
      " [-0.37811184]\n",
      " [ 0.15990725]\n",
      " ...\n",
      " [-0.30783492]\n",
      " [-0.38122747]\n",
      " [-0.10161595]]\n",
      "Current iteration=8, loss=31637.80791186841\n",
      "t [[-1.40546287]\n",
      " [-0.41162927]\n",
      " [ 0.17633221]\n",
      " ...\n",
      " [-0.33298341]\n",
      " [-0.40994566]\n",
      " [-0.11148444]]\n",
      "t [[-1.40546287]\n",
      " [-0.41162927]\n",
      " [ 0.17633221]\n",
      " ...\n",
      " [-0.33298341]\n",
      " [-0.40994566]\n",
      " [-0.11148444]]\n",
      "t [[-1.49195982]\n",
      " [-0.44277051]\n",
      " [ 0.19130293]\n",
      " ...\n",
      " [-0.35620525]\n",
      " [-0.43622926]\n",
      " [-0.12074804]]\n",
      "loss=31180.85823322541\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.2462142 ]\n",
      " [-0.05355259]\n",
      " [ 0.01752809]\n",
      " ...\n",
      " [ 0.27596071]\n",
      " [-0.20319102]\n",
      " [ 0.22383623]]\n",
      "t [[-0.2462142 ]\n",
      " [-0.05355259]\n",
      " [ 0.01752809]\n",
      " ...\n",
      " [ 0.27596071]\n",
      " [-0.20319102]\n",
      " [ 0.22383623]]\n",
      "t [[-0.45955728]\n",
      " [-0.10615115]\n",
      " [ 0.03944486]\n",
      " ...\n",
      " [ 0.50906921]\n",
      " [-0.37687643]\n",
      " [ 0.4156043 ]]\n",
      "t [[-0.45955728]\n",
      " [-0.10615115]\n",
      " [ 0.03944486]\n",
      " ...\n",
      " [ 0.50906921]\n",
      " [-0.37687643]\n",
      " [ 0.4156043 ]]\n",
      "Current iteration=2, loss=35012.50484614237\n",
      "t [[-0.64564626]\n",
      " [-0.15632812]\n",
      " [ 0.06308661]\n",
      " ...\n",
      " [ 0.70641937]\n",
      " [-0.52558439]\n",
      " [ 0.58040533]]\n",
      "t [[-0.64564626]\n",
      " [-0.15632812]\n",
      " [ 0.06308661]\n",
      " ...\n",
      " [ 0.70641937]\n",
      " [-0.52558439]\n",
      " [ 0.58040533]]\n",
      "t [[-0.8091863 ]\n",
      " [-0.20340581]\n",
      " [ 0.08679217]\n",
      " ...\n",
      " [ 0.8741755 ]\n",
      " [-0.65337505]\n",
      " [ 0.72272118]]\n",
      "t [[-0.8091863 ]\n",
      " [-0.20340581]\n",
      " [ 0.08679217]\n",
      " ...\n",
      " [ 0.8741755 ]\n",
      " [-0.65337505]\n",
      " [ 0.72272118]]\n",
      "Current iteration=4, loss=33439.51743094478\n",
      "t [[-0.95398874]\n",
      " [-0.24718767]\n",
      " [ 0.10960133]\n",
      " ...\n",
      " [ 1.01745884]\n",
      " [-0.76370068]\n",
      " [ 0.84630719]]\n",
      "t [[-0.95398874]\n",
      " [-0.24718767]\n",
      " [ 0.10960133]\n",
      " ...\n",
      " [ 1.01745884]\n",
      " [-0.76370068]\n",
      " [ 0.84630719]]\n",
      "t [[-1.08310377]\n",
      " [-0.28773869]\n",
      " [ 0.13100707]\n",
      " ...\n",
      " [ 1.14043669]\n",
      " [-0.85942004]\n",
      " [ 0.95424066]]\n",
      "t [[-1.08310377]\n",
      " [-0.28773869]\n",
      " [ 0.13100707]\n",
      " ...\n",
      " [ 1.14043669]\n",
      " [-0.85942004]\n",
      " [ 0.95424066]]\n",
      "Current iteration=6, loss=32464.864102850683\n",
      "t [[-1.19897093]\n",
      " [-0.32525021]\n",
      " [ 0.15078203]\n",
      " ...\n",
      " [ 1.24647543]\n",
      " [-0.94287347]\n",
      " [ 1.04902256]]\n",
      "t [[-1.19897093]\n",
      " [-0.32525021]\n",
      " [ 0.15078203]\n",
      " ...\n",
      " [ 1.24647543]\n",
      " [-0.94287347]\n",
      " [ 1.04902256]]\n",
      "t [[-1.30355162]\n",
      " [-0.35996369]\n",
      " [ 0.16886565]\n",
      " ...\n",
      " [ 1.33829483]\n",
      " [-1.01597144]\n",
      " [ 1.13268442]]\n",
      "t [[-1.30355162]\n",
      " [-0.35996369]\n",
      " [ 0.16886565]\n",
      " ...\n",
      " [ 1.33829483]\n",
      " [-1.01597144]\n",
      " [ 1.13268442]]\n",
      "Current iteration=8, loss=31822.678873302648\n",
      "t [[-1.39843551]\n",
      " [-0.39213037]\n",
      " [ 0.18529348]\n",
      " ...\n",
      " [ 1.41810133]\n",
      " [-1.08027642]\n",
      " [ 1.20688253]]\n",
      "t [[-1.39843551]\n",
      " [-0.39213037]\n",
      " [ 0.18529348]\n",
      " ...\n",
      " [ 1.41810133]\n",
      " [-1.08027642]\n",
      " [ 1.20688253]]\n",
      "t [[-1.48492278]\n",
      " [-0.42199136]\n",
      " [ 0.20015369]\n",
      " ...\n",
      " [ 1.48769588]\n",
      " [-1.1370721 ]\n",
      " [ 1.2729751 ]]\n",
      "loss=31375.671682966116\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.21993981]\n",
      " [-0.04626835]\n",
      " [ 0.04592149]\n",
      " ...\n",
      " [-0.05804683]\n",
      " [-0.07217109]\n",
      " [-0.01517199]]\n",
      "t [[-0.21993981]\n",
      " [-0.04626835]\n",
      " [ 0.04592149]\n",
      " ...\n",
      " [-0.05804683]\n",
      " [-0.07217109]\n",
      " [-0.01517199]]\n",
      "t [[-0.40417845]\n",
      " [-0.07696725]\n",
      " [ 0.08743608]\n",
      " ...\n",
      " [-0.10992742]\n",
      " [-0.13520772]\n",
      " [-0.03020268]]\n",
      "t [[-0.40417845]\n",
      " [-0.07696725]\n",
      " [ 0.08743608]\n",
      " ...\n",
      " [-0.10992742]\n",
      " [-0.13520772]\n",
      " [-0.03020268]]\n",
      "Current iteration=2, loss=34881.65040203325\n",
      "t [[-0.55803294]\n",
      " [-0.09781285]\n",
      " [ 0.12394694]\n",
      " ...\n",
      " [-0.15640672]\n",
      " [-0.19052636]\n",
      " [-0.04468436]]\n",
      "t [[-0.55803294]\n",
      " [-0.09781285]\n",
      " [ 0.12394694]\n",
      " ...\n",
      " [-0.15640672]\n",
      " [-0.19052636]\n",
      " [-0.04468436]]\n",
      "t [[-0.68647062]\n",
      " [-0.11258806]\n",
      " [ 0.15554927]\n",
      " ...\n",
      " [-0.19819286]\n",
      " [-0.23936595]\n",
      " [-0.05842244]]\n",
      "t [[-0.68647062]\n",
      " [-0.11258806]\n",
      " [ 0.15554927]\n",
      " ...\n",
      " [-0.19819286]\n",
      " [-0.23936595]\n",
      " [-0.05842244]]\n",
      "Current iteration=4, loss=33272.56941725589\n",
      "t [[-0.79379892]\n",
      " [-0.12371682]\n",
      " [ 0.18266982]\n",
      " ...\n",
      " [-0.23590524]\n",
      " [-0.28276189]\n",
      " [-0.07134736]]\n",
      "t [[-0.79379892]\n",
      " [-0.12371682]\n",
      " [ 0.18266982]\n",
      " ...\n",
      " [-0.23590524]\n",
      " [-0.28276189]\n",
      " [-0.07134736]]\n",
      "t [[-0.8836127 ]\n",
      " [-0.13273262]\n",
      " [ 0.20584866]\n",
      " ...\n",
      " [-0.27007216]\n",
      " [-0.32156218]\n",
      " [-0.08345593]]\n",
      "t [[-0.8836127 ]\n",
      " [-0.13273262]\n",
      " [ 0.20584866]\n",
      " ...\n",
      " [-0.27007216]\n",
      " [-0.32156218]\n",
      " [-0.08345593]]\n",
      "Current iteration=6, loss=32294.640007267037\n",
      "t [[-0.95885812]\n",
      " [-0.1406021 ]\n",
      " [ 0.22562751]\n",
      " ...\n",
      " [-0.30114013]\n",
      " [-0.35645694]\n",
      " [-0.09477721]]\n",
      "t [[-0.95885812]\n",
      " [-0.1406021 ]\n",
      " [ 0.22562751]\n",
      " ...\n",
      " [-0.30114013]\n",
      " [-0.35645694]\n",
      " [-0.09477721]]\n",
      "t [[-1.02193332]\n",
      " [-0.1479337 ]\n",
      " [ 0.24250112]\n",
      " ...\n",
      " [-0.32948603]\n",
      " [-0.38800838]\n",
      " [-0.10535408]]\n",
      "t [[-1.02193332]\n",
      " [-0.1479337 ]\n",
      " [ 0.24250112]\n",
      " ...\n",
      " [-0.32948603]\n",
      " [-0.38800838]\n",
      " [-0.10535408]]\n",
      "Current iteration=8, loss=31659.521602349185\n",
      "t [[-1.0747898 ]\n",
      " [-0.15510865]\n",
      " [ 0.25690142]\n",
      " ...\n",
      " [-0.35542884]\n",
      " [-0.41667681]\n",
      " [-0.11523372]]\n",
      "t [[-1.0747898 ]\n",
      " [-0.15510865]\n",
      " [ 0.25690142]\n",
      " ...\n",
      " [-0.35542884]\n",
      " [-0.41667681]\n",
      " [-0.11523372]]\n",
      "t [[-1.11902076]\n",
      " [-0.16236288]\n",
      " [ 0.26919704]\n",
      " ...\n",
      " [-0.37923974]\n",
      " [-0.44284163]\n",
      " [-0.12446309]]\n",
      "loss=31222.13353175082\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.25830109]\n",
      " [-0.05819196]\n",
      " [ 0.02273056]\n",
      " ...\n",
      " [-0.05579754]\n",
      " [-0.07254634]\n",
      " [-0.01505804]]\n",
      "t [[-0.25830109]\n",
      " [-0.05819196]\n",
      " [ 0.02273056]\n",
      " ...\n",
      " [-0.05579754]\n",
      " [-0.07254634]\n",
      " [-0.01505804]]\n",
      "t [[-0.48041498]\n",
      " [-0.11483432]\n",
      " [ 0.04895569]\n",
      " ...\n",
      " [-0.10563484]\n",
      " [-0.13588074]\n",
      " [-0.03015369]]\n",
      "t [[-0.48041498]\n",
      " [-0.11483432]\n",
      " [ 0.04895569]\n",
      " ...\n",
      " [-0.10563484]\n",
      " [-0.13588074]\n",
      " [-0.03015369]]\n",
      "Current iteration=2, loss=34858.90440247027\n",
      "t [[-0.6728122 ]\n",
      " [-0.16843766]\n",
      " [ 0.0759786 ]\n",
      " ...\n",
      " [-0.15028797]\n",
      " [-0.19143856]\n",
      " [-0.04479348]]\n",
      "t [[-0.6728122 ]\n",
      " [-0.16843766]\n",
      " [ 0.0759786 ]\n",
      " ...\n",
      " [-0.15028797]\n",
      " [-0.19143856]\n",
      " [-0.04479348]]\n",
      "t [[-0.84085762]\n",
      " [-0.21838715]\n",
      " [ 0.10222493]\n",
      " ...\n",
      " [-0.1904593 ]\n",
      " [-0.24047423]\n",
      " [-0.05873161]]\n",
      "t [[-0.84085762]\n",
      " [-0.21838715]\n",
      " [ 0.10222493]\n",
      " ...\n",
      " [-0.1904593 ]\n",
      " [-0.24047423]\n",
      " [-0.05873161]]\n",
      "Current iteration=4, loss=33240.728354974824\n",
      "t [[-0.98884548]\n",
      " [-0.26457591]\n",
      " [ 0.12686738]\n",
      " ...\n",
      " [-0.22675416]\n",
      " [-0.28403502]\n",
      " [-0.07186916]]\n",
      "t [[-0.98884548]\n",
      " [-0.26457591]\n",
      " [ 0.12686738]\n",
      " ...\n",
      " [-0.22675416]\n",
      " [-0.28403502]\n",
      " [-0.07186916]]\n",
      "t [[-1.12017298]\n",
      " [-0.30715581]\n",
      " [ 0.14953562]\n",
      " ...\n",
      " [-0.25968357]\n",
      " [-0.32297776]\n",
      " [-0.08418677]]\n",
      "t [[-1.12017298]\n",
      " [-0.30715581]\n",
      " [ 0.14953562]\n",
      " ...\n",
      " [-0.25968357]\n",
      " [-0.32297776]\n",
      " [-0.08418677]]\n",
      "Current iteration=6, loss=32257.791889627377\n",
      "t [[-1.2375289 ]\n",
      " [-0.34639173]\n",
      " [ 0.17012442]\n",
      " ...\n",
      " [-0.28967623]\n",
      " [-0.35799902]\n",
      " [-0.09570494]]\n",
      "t [[-1.2375289 ]\n",
      " [-0.34639173]\n",
      " [ 0.17012442]\n",
      " ...\n",
      " [-0.28967623]\n",
      " [-0.35799902]\n",
      " [-0.09570494]]\n",
      "t [[-1.34305518]\n",
      " [-0.38258366]\n",
      " [ 0.1886753 ]\n",
      " ...\n",
      " [-0.31709191]\n",
      " [-0.38966578]\n",
      " [-0.10646232]]\n",
      "t [[-1.34305518]\n",
      " [-0.38258366]\n",
      " [ 0.1886753 ]\n",
      " ...\n",
      " [-0.31709191]\n",
      " [-0.38966578]\n",
      " [-0.10646232]]\n",
      "Current iteration=8, loss=31618.757617342773\n",
      "t [[-1.43847382]\n",
      " [-0.41602829]\n",
      " [ 0.20530625]\n",
      " ...\n",
      " [-0.34223381]\n",
      " [-0.41844183]\n",
      " [-0.11650436]]\n",
      "t [[-1.43847382]\n",
      " [-0.41602829]\n",
      " [ 0.20530625]\n",
      " ...\n",
      " [-0.34223381]\n",
      " [-0.41844183]\n",
      " [-0.11650436]]\n",
      "t [[-1.52518329]\n",
      " [-0.44700179]\n",
      " [ 0.2201708 ]\n",
      " ...\n",
      " [-0.36535883]\n",
      " [-0.44470913]\n",
      " [-0.12587763]]\n",
      "loss=31177.644560120505\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.25896173]\n",
      " [-0.06001882]\n",
      " [ 0.01559794]\n",
      " ...\n",
      " [-0.05515407]\n",
      " [-0.07297246]\n",
      " [-0.01475935]]\n",
      "t [[-0.25896173]\n",
      " [-0.06001882]\n",
      " [ 0.01559794]\n",
      " ...\n",
      " [-0.05515407]\n",
      " [-0.07297246]\n",
      " [-0.01475935]]\n",
      "t [[-0.48168058]\n",
      " [-0.11802507]\n",
      " [ 0.03660114]\n",
      " ...\n",
      " [-0.10468823]\n",
      " [-0.13668201]\n",
      " [-0.02957864]]\n",
      "t [[-0.48168058]\n",
      " [-0.11802507]\n",
      " [ 0.03660114]\n",
      " ...\n",
      " [-0.10468823]\n",
      " [-0.13668201]\n",
      " [-0.02957864]]\n",
      "Current iteration=2, loss=34821.705601306356\n",
      "t [[-0.6746091 ]\n",
      " [-0.17271165]\n",
      " [ 0.059886  ]\n",
      " ...\n",
      " [-0.14927415]\n",
      " [-0.19257404]\n",
      " [-0.04398972]]\n",
      "t [[-0.6746091 ]\n",
      " [-0.17271165]\n",
      " [ 0.059886  ]\n",
      " ...\n",
      " [-0.14927415]\n",
      " [-0.19257404]\n",
      " [-0.04398972]]\n",
      "t [[-0.84311532]\n",
      " [-0.22356965]\n",
      " [ 0.08351885]\n",
      " ...\n",
      " [-0.18953761]\n",
      " [-0.24191151]\n",
      " [-0.05775377]]\n",
      "t [[-0.84311532]\n",
      " [-0.22356965]\n",
      " [ 0.08351885]\n",
      " ...\n",
      " [-0.18953761]\n",
      " [-0.24191151]\n",
      " [-0.05775377]]\n",
      "Current iteration=4, loss=33183.04387422585\n",
      "t [[-0.99150273]\n",
      " [-0.27055419]\n",
      " [ 0.10639155]\n",
      " ...\n",
      " [-0.22602966]\n",
      " [-0.2857482 ]\n",
      " [-0.07077002]]\n",
      "t [[-0.99150273]\n",
      " [-0.27055419]\n",
      " [ 0.10639155]\n",
      " ...\n",
      " [-0.22602966]\n",
      " [-0.2857482 ]\n",
      " [-0.07077002]]\n",
      "t [[-1.12317802]\n",
      " [-0.31385422]\n",
      " [ 0.12792205]\n",
      " ...\n",
      " [-0.2592236 ]\n",
      " [-0.32494563]\n",
      " [-0.08301349]]\n",
      "t [[-1.12317802]\n",
      " [-0.31385422]\n",
      " [ 0.12792205]\n",
      " ...\n",
      " [-0.2592236 ]\n",
      " [-0.32494563]\n",
      " [-0.08301349]]\n",
      "Current iteration=6, loss=32188.11235123285\n",
      "t [[-1.24083802]\n",
      " [-0.35375743]\n",
      " [ 0.14784796]\n",
      " ...\n",
      " [-0.28952205]\n",
      " [-0.36020379]\n",
      " [-0.09449808]]\n",
      "t [[-1.24083802]\n",
      " [-0.35375743]\n",
      " [ 0.14784796]\n",
      " ...\n",
      " [-0.28952205]\n",
      " [-0.36020379]\n",
      " [-0.09449808]]\n",
      "t [[-1.34663099]\n",
      " [-0.39057839]\n",
      " [ 0.16609468]\n",
      " ...\n",
      " [-0.31726687]\n",
      " [-0.39209211]\n",
      " [-0.10525603]]\n",
      "t [[-1.34663099]\n",
      " [-0.39057839]\n",
      " [ 0.16609468]\n",
      " ...\n",
      " [-0.31726687]\n",
      " [-0.39209211]\n",
      " [-0.10525603]]\n",
      "Current iteration=8, loss=31541.6808403275\n",
      "t [[-1.44228378]\n",
      " [-0.4246234 ]\n",
      " [ 0.1826942 ]\n",
      " ...\n",
      " [-0.34274895]\n",
      " [-0.42107619]\n",
      " [-0.11532708]]\n",
      "t [[-1.44228378]\n",
      " [-0.4246234 ]\n",
      " [ 0.1826942 ]\n",
      " ...\n",
      " [-0.34274895]\n",
      " [-0.42107619]\n",
      " [-0.11532708]]\n",
      "t [[-1.52919855]\n",
      " [-0.45617518]\n",
      " [ 0.19773603]\n",
      " ...\n",
      " [-0.36621674]\n",
      " [-0.44753938]\n",
      " [-0.12475291]]\n",
      "loss=31095.878514160315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.25691916]\n",
      " [-0.05588096]\n",
      " [ 0.01829019]\n",
      " ...\n",
      " [ 0.287959  ]\n",
      " [-0.21202541]\n",
      " [ 0.23356824]]\n",
      "t [[-0.25691916]\n",
      " [-0.05588096]\n",
      " [ 0.01829019]\n",
      " ...\n",
      " [ 0.287959  ]\n",
      " [-0.21202541]\n",
      " [ 0.23356824]]\n",
      "t [[-0.47805834]\n",
      " [-0.11072192]\n",
      " [ 0.0413567 ]\n",
      " ...\n",
      " [ 0.52927284]\n",
      " [-0.39193304]\n",
      " [ 0.43223004]]\n",
      "t [[-0.47805834]\n",
      " [-0.11072192]\n",
      " [ 0.0413567 ]\n",
      " ...\n",
      " [ 0.52927284]\n",
      " [-0.39193304]\n",
      " [ 0.43223004]]\n",
      "Current iteration=2, loss=34919.368240302014\n",
      "t [[-0.6698123 ]\n",
      " [-0.16285709]\n",
      " [ 0.06617393]\n",
      " ...\n",
      " [ 0.73201766]\n",
      " [-0.5448789 ]\n",
      " [ 0.60179387]]\n",
      "t [[-0.6698123 ]\n",
      " [-0.16285709]\n",
      " [ 0.06617393]\n",
      " ...\n",
      " [ 0.73201766]\n",
      " [-0.5448789 ]\n",
      " [ 0.60179387]]\n",
      "t [[-0.83747196]\n",
      " [-0.2115591 ]\n",
      " [ 0.09090894]\n",
      " ...\n",
      " [ 0.90313885]\n",
      " [-0.67544543]\n",
      " [ 0.74731234]]\n",
      "t [[-0.83747196]\n",
      " [-0.2115591 ]\n",
      " [ 0.09090894]\n",
      " ...\n",
      " [ 0.90313885]\n",
      " [-0.67544543]\n",
      " [ 0.74731234]]\n",
      "Current iteration=4, loss=33329.1085682744\n",
      "t [[-0.98526837]\n",
      " [-0.25665168]\n",
      " [ 0.11454072]\n",
      " ...\n",
      " [ 1.04833716]\n",
      " [-0.78748492]\n",
      " [ 0.8729691 ]]\n",
      "t [[-0.98526837]\n",
      " [-0.25665168]\n",
      " [ 0.11454072]\n",
      " ...\n",
      " [ 1.04833716]\n",
      " [-0.78748492]\n",
      " [ 0.8729691 ]]\n",
      "t [[-1.11654821]\n",
      " [-0.29824442]\n",
      " [ 0.13655763]\n",
      " ...\n",
      " [ 1.17219716]\n",
      " [-0.88415089]\n",
      " [ 0.98215301]]\n",
      "t [[-1.11654821]\n",
      " [-0.29824442]\n",
      " [ 0.13655763]\n",
      " ...\n",
      " [ 1.17219716]\n",
      " [-0.88415089]\n",
      " [ 0.98215301]]\n",
      "Current iteration=6, loss=32360.0680818663\n",
      "t [[-1.23396122]\n",
      " [-0.33657632]\n",
      " [ 0.15675255]\n",
      " ...\n",
      " [ 1.27838303]\n",
      " [-0.96799763]\n",
      " [ 1.0775895 ]]\n",
      "t [[-1.23396122]\n",
      " [-0.33657632]\n",
      " [ 0.15675255]\n",
      " ...\n",
      " [ 1.27838303]\n",
      " [-0.96799763]\n",
      " [ 1.0775895 ]]\n",
      "t [[-1.3396196 ]\n",
      " [-0.37193132]\n",
      " [ 0.17509396]\n",
      " ...\n",
      " [ 1.3698274 ]\n",
      " [-1.04109046]\n",
      " [ 1.16147199]]\n",
      "t [[-1.3396196 ]\n",
      " [-0.37193132]\n",
      " [ 0.17509396]\n",
      " ...\n",
      " [ 1.3698274 ]\n",
      " [-1.04109046]\n",
      " [ 1.16147199]]\n",
      "Current iteration=8, loss=31728.755175399063\n",
      "t [[-1.43522293]\n",
      " [-0.40459581]\n",
      " [ 0.19164747]\n",
      " ...\n",
      " [ 1.44888989]\n",
      " [-1.10510437]\n",
      " [ 1.23557438]]\n",
      "t [[-1.43522293]\n",
      " [-0.40459581]\n",
      " [ 0.19164747]\n",
      " ...\n",
      " [ 1.44888989]\n",
      " [-1.10510437]\n",
      " [ 1.23557438]]\n",
      "t [[-1.52215273]\n",
      " [-0.43483912]\n",
      " [ 0.20652895]\n",
      " ...\n",
      " [ 1.51748264]\n",
      " [-1.16140539]\n",
      " [ 1.30134115]]\n",
      "loss=31292.488034238668\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.22910397]\n",
      " [-0.0481962 ]\n",
      " [ 0.04783489]\n",
      " ...\n",
      " [-0.06046544]\n",
      " [-0.07517822]\n",
      " [-0.01580415]]\n",
      "t [[-0.22910397]\n",
      " [-0.0481962 ]\n",
      " [ 0.04783489]\n",
      " ...\n",
      " [-0.06046544]\n",
      " [-0.07517822]\n",
      " [-0.01580415]]\n",
      "t [[-0.41948046]\n",
      " [-0.07950389]\n",
      " [ 0.09088842]\n",
      " ...\n",
      " [-0.11424248]\n",
      " [-0.14044784]\n",
      " [-0.03145509]]\n",
      "t [[-0.41948046]\n",
      " [-0.07950389]\n",
      " [ 0.09088842]\n",
      " ...\n",
      " [-0.11424248]\n",
      " [-0.14044784]\n",
      " [-0.03145509]]\n",
      "Current iteration=2, loss=34789.08316441326\n",
      "t [[-0.57715238]\n",
      " [-0.10038388]\n",
      " [ 0.12848929]\n",
      " ...\n",
      " [-0.16219871]\n",
      " [-0.19741327]\n",
      " [-0.04649288]]\n",
      "t [[-0.57715238]\n",
      " [-0.10038388]\n",
      " [ 0.12848929]\n",
      " ...\n",
      " [-0.16219871]\n",
      " [-0.19741327]\n",
      " [-0.04649288]]\n",
      "t [[-0.70770868]\n",
      " [-0.11501113]\n",
      " [ 0.16077909]\n",
      " ...\n",
      " [-0.20513026]\n",
      " [-0.24746409]\n",
      " [-0.0607083 ]]\n",
      "t [[-0.70770868]\n",
      " [-0.11501113]\n",
      " [ 0.16077909]\n",
      " ...\n",
      " [-0.20513026]\n",
      " [-0.24746409]\n",
      " [-0.0607083 ]]\n",
      "Current iteration=4, loss=33165.23546817643\n",
      "t [[-0.81594304]\n",
      " [-0.12599527]\n",
      " [ 0.18826862]\n",
      " ...\n",
      " [-0.2437265 ]\n",
      " [-0.2917485 ]\n",
      " [-0.07403395]]\n",
      "t [[-0.81594304]\n",
      " [-0.12599527]\n",
      " [ 0.18826862]\n",
      " ...\n",
      " [-0.2437265 ]\n",
      " [-0.2917485 ]\n",
      " [-0.07403395]]\n",
      "t [[-0.90580953]\n",
      " [-0.13494776]\n",
      " [ 0.21158039]\n",
      " ...\n",
      " [-0.27857004]\n",
      " [-0.33119668]\n",
      " [-0.08647464]]\n",
      "t [[-0.90580953]\n",
      " [-0.13494776]\n",
      " [ 0.21158039]\n",
      " ...\n",
      " [-0.27857004]\n",
      " [-0.33119668]\n",
      " [-0.08647464]]\n",
      "Current iteration=6, loss=32194.45316174208\n",
      "t [[-0.98051286]\n",
      " [-0.14286073]\n",
      " [ 0.23132391]\n",
      " ...\n",
      " [-0.31014928]\n",
      " [-0.36655834]\n",
      " [-0.09806836]]\n",
      "t [[-0.98051286]\n",
      " [-0.14286073]\n",
      " [ 0.23132391]\n",
      " ...\n",
      " [-0.31014928]\n",
      " [-0.36655834]\n",
      " [-0.09806836]]\n",
      "t [[-1.04263573]\n",
      " [-0.15034381]\n",
      " [ 0.24804554]\n",
      " ...\n",
      " [-0.3388736 ]\n",
      " [-0.3984391 ]\n",
      " [-0.10886608]]\n",
      "t [[-1.04263573]\n",
      " [-0.15034381]\n",
      " [ 0.24804554]\n",
      " ...\n",
      " [-0.3388736 ]\n",
      " [-0.3984391 ]\n",
      " [-0.10886608]]\n",
      "Current iteration=8, loss=31570.840870304804\n",
      "t [[-1.09426131]\n",
      " [-0.15776902]\n",
      " [ 0.26221515]\n",
      " ...\n",
      " [-0.36508735]\n",
      " [-0.42733122]\n",
      " [-0.11892193]]\n",
      "t [[-1.09426131]\n",
      " [-0.15776902]\n",
      " [ 0.26221515]\n",
      " ...\n",
      " [-0.36508735]\n",
      " [-0.42733122]\n",
      " [-0.11892193]]\n",
      "t [[-1.13707718]\n",
      " [-0.16535937]\n",
      " [ 0.27422871]\n",
      " ...\n",
      " [-0.38908176]\n",
      " [-0.45363792]\n",
      " [-0.12828862]]\n",
      "loss=31144.318321675746\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.26906364]\n",
      " [-0.06061662]\n",
      " [ 0.02367766]\n",
      " ...\n",
      " [-0.05812244]\n",
      " [-0.07556911]\n",
      " [-0.01568546]]\n",
      "t [[-0.26906364]\n",
      " [-0.06061662]\n",
      " [ 0.02367766]\n",
      " ...\n",
      " [-0.05812244]\n",
      " [-0.07556911]\n",
      " [-0.01568546]]\n",
      "t [[-0.498875  ]\n",
      " [-0.11955077]\n",
      " [ 0.0511453 ]\n",
      " ...\n",
      " [-0.10977987]\n",
      " [-0.14114558]\n",
      " [-0.03141174]]\n",
      "t [[-0.498875  ]\n",
      " [-0.11955077]\n",
      " [ 0.0511453 ]\n",
      " ...\n",
      " [-0.10977987]\n",
      " [-0.14114558]\n",
      " [-0.03141174]]\n",
      "Current iteration=2, loss=34765.717867623236\n",
      "t [[-0.69676127]\n",
      " [-0.17512129]\n",
      " [ 0.07935664]\n",
      " ...\n",
      " [-0.15585228]\n",
      " [-0.19835521]\n",
      " [-0.04662207]]\n",
      "t [[-0.69676127]\n",
      " [-0.17512129]\n",
      " [ 0.07935664]\n",
      " ...\n",
      " [-0.15585228]\n",
      " [-0.19835521]\n",
      " [-0.04662207]]\n",
      "t [[-0.86872187]\n",
      " [-0.22667793]\n",
      " [ 0.10658988]\n",
      " ...\n",
      " [-0.19712852]\n",
      " [-0.24860488]\n",
      " [-0.06105101]]\n",
      "t [[-0.86872187]\n",
      " [-0.22667793]\n",
      " [ 0.10658988]\n",
      " ...\n",
      " [-0.19712852]\n",
      " [-0.24860488]\n",
      " [-0.06105101]]\n",
      "Current iteration=4, loss=33132.840692121696\n",
      "t [[-1.01949719]\n",
      " [-0.27414657]\n",
      " [ 0.13197993]\n",
      " ...\n",
      " [-0.23428145]\n",
      " [-0.2930558 ]\n",
      " [-0.0746001 ]]\n",
      "t [[-1.01949719]\n",
      " [-0.27414657]\n",
      " [ 0.13197993]\n",
      " ...\n",
      " [-0.23428145]\n",
      " [-0.2930558 ]\n",
      " [-0.0746001 ]]\n",
      "t [[-1.15279394]\n",
      " [-0.31773198]\n",
      " [ 0.15516918]\n",
      " ...\n",
      " [-0.26787385]\n",
      " [-0.3326477 ]\n",
      " [-0.08725761]]\n",
      "t [[-1.15279394]\n",
      " [-0.31773198]\n",
      " [ 0.15516918]\n",
      " ...\n",
      " [-0.26787385]\n",
      " [-0.3326477 ]\n",
      " [-0.08725761]]\n",
      "Current iteration=6, loss=32157.066218536376\n",
      "t [[-1.27151683]\n",
      " [-0.35775128]\n",
      " [ 0.17608418]\n",
      " ...\n",
      " [-0.29837396]\n",
      " [-0.36813715]\n",
      " [-0.0990532 ]]\n",
      "t [[-1.27151683]\n",
      " [-0.35775128]\n",
      " [ 0.17608418]\n",
      " ...\n",
      " [-0.29837396]\n",
      " [-0.36813715]\n",
      " [-0.0990532 ]]\n",
      "t [[-1.37796052]\n",
      " [-0.39454916]\n",
      " [ 0.19480258]\n",
      " ...\n",
      " [-0.32617208]\n",
      " [-0.40013477]\n",
      " [-0.11003401]]\n",
      "t [[-1.37796052]\n",
      " [-0.39454916]\n",
      " [ 0.19480258]\n",
      " ...\n",
      " [-0.32617208]\n",
      " [-0.40013477]\n",
      " [-0.11003401]]\n",
      "Current iteration=8, loss=31529.448923431555\n",
      "t [[-1.47395691]\n",
      " [-0.42845836]\n",
      " [ 0.21147639]\n",
      " ...\n",
      " [-0.35159514]\n",
      " [-0.42913643]\n",
      " [-0.12025282]]\n",
      "t [[-1.47395691]\n",
      " [-0.42845836]\n",
      " [ 0.21147639]\n",
      " ...\n",
      " [-0.35159514]\n",
      " [-0.42913643]\n",
      " [-0.12025282]]\n",
      "t [[-1.56098493]\n",
      " [-0.45978336]\n",
      " [ 0.22628881]\n",
      " ...\n",
      " [-0.37491871]\n",
      " [-0.45554797]\n",
      " [-0.12976232]]\n",
      "loss=31099.0523097599\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.2697518 ]\n",
      " [-0.06251961]\n",
      " [ 0.01624786]\n",
      " ...\n",
      " [-0.05745216]\n",
      " [-0.07601298]\n",
      " [-0.01537432]]\n",
      "t [[-0.2697518 ]\n",
      " [-0.06251961]\n",
      " [ 0.01624786]\n",
      " ...\n",
      " [-0.05745216]\n",
      " [-0.07601298]\n",
      " [-0.01537432]]\n",
      "t [[-0.50019117]\n",
      " [-0.12285442]\n",
      " [ 0.03835804]\n",
      " ...\n",
      " [-0.10880845]\n",
      " [-0.14197809]\n",
      " [-0.03081369]]\n",
      "t [[-0.50019117]\n",
      " [-0.12285442]\n",
      " [ 0.03835804]\n",
      " ...\n",
      " [-0.10880845]\n",
      " [-0.14197809]\n",
      " [-0.03081369]]\n",
      "Current iteration=2, loss=34727.31517656498\n",
      "t [[-0.69862525]\n",
      " [-0.17952934]\n",
      " [ 0.06280118]\n",
      " ...\n",
      " [-0.15483083]\n",
      " [-0.1995325 ]\n",
      " [-0.04578984]]\n",
      "t [[-0.69862525]\n",
      " [-0.17952934]\n",
      " [ 0.06280118]\n",
      " ...\n",
      " [-0.15483083]\n",
      " [-0.1995325 ]\n",
      " [-0.04578984]]\n",
      "t [[-0.87105776]\n",
      " [-0.23201024]\n",
      " [ 0.08745364]\n",
      " ...\n",
      " [-0.19622305]\n",
      " [-0.25009264]\n",
      " [-0.06004442]]\n",
      "t [[-0.87105776]\n",
      " [-0.23201024]\n",
      " [ 0.08745364]\n",
      " ...\n",
      " [-0.19622305]\n",
      " [-0.25009264]\n",
      " [-0.06004442]]\n",
      "Current iteration=4, loss=33073.804693427395\n",
      "t [[-1.0222398 ]\n",
      " [-0.28028903]\n",
      " [ 0.11114119]\n",
      " ...\n",
      " [-0.23359885]\n",
      " [-0.29482679]\n",
      " [-0.07347606]]\n",
      "t [[-1.0222398 ]\n",
      " [-0.28028903]\n",
      " [ 0.11114119]\n",
      " ...\n",
      " [-0.23359885]\n",
      " [-0.29482679]\n",
      " [-0.07347606]]\n",
      "t [[-1.15588872]\n",
      " [-0.32460899]\n",
      " [ 0.13327702]\n",
      " ...\n",
      " [-0.26748067]\n",
      " [-0.33467974]\n",
      " [-0.08606628]]\n",
      "t [[-1.15588872]\n",
      " [-0.32460899]\n",
      " [ 0.13327702]\n",
      " ...\n",
      " [-0.26748067]\n",
      " [-0.33467974]\n",
      " [-0.08606628]]\n",
      "Current iteration=6, loss=32086.173113371187\n",
      "t [[-1.27491795]\n",
      " [-0.36531048]\n",
      " [ 0.15362018]\n",
      " ...\n",
      " [-0.29830946]\n",
      " [-0.37041164]\n",
      " [-0.0978371 ]]\n",
      "t [[-1.27491795]\n",
      " [-0.36531048]\n",
      " [ 0.15362018]\n",
      " ...\n",
      " [-0.29830946]\n",
      " [-0.37041164]\n",
      " [-0.0978371 ]]\n",
      "t [[-1.3816289 ]\n",
      " [-0.4027528 ]\n",
      " [ 0.17212628]\n",
      " ...\n",
      " [-0.32645706]\n",
      " [-0.40263567]\n",
      " [-0.10882853]]\n",
      "t [[-1.3816289 ]\n",
      " [-0.4027528 ]\n",
      " [ 0.17212628]\n",
      " ...\n",
      " [-0.32645706]\n",
      " [-0.40263567]\n",
      " [-0.10882853]]\n",
      "Current iteration=8, loss=31451.362923526136\n",
      "t [[-1.47785857]\n",
      " [-0.43727821]\n",
      " [ 0.18885818]\n",
      " ...\n",
      " [-0.35223796]\n",
      " [-0.43184959]\n",
      " [-0.11908709]]\n",
      "t [[-1.47785857]\n",
      " [-0.43727821]\n",
      " [ 0.18885818]\n",
      " ...\n",
      " [-0.35223796]\n",
      " [-0.43184959]\n",
      " [-0.11908709]]\n",
      "t [[-1.56508973]\n",
      " [-0.46919769]\n",
      " [ 0.20393345]\n",
      " ...\n",
      " [-0.37591937]\n",
      " [-0.45846066]\n",
      " [-0.12866019]]\n",
      "loss=31016.48657756839\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.26762413]\n",
      " [-0.05820934]\n",
      " [ 0.01905228]\n",
      " ...\n",
      " [ 0.29995729]\n",
      " [-0.2208598 ]\n",
      " [ 0.24330025]]\n",
      "t [[-0.26762413]\n",
      " [-0.05820934]\n",
      " [ 0.01905228]\n",
      " ...\n",
      " [ 0.29995729]\n",
      " [-0.2208598 ]\n",
      " [ 0.24330025]]\n",
      "t [[-0.49643762]\n",
      " [-0.11528881]\n",
      " [ 0.04328464]\n",
      " ...\n",
      " [ 0.54931755]\n",
      " [-0.40688009]\n",
      " [ 0.44873688]]\n",
      "t [[-0.49643762]\n",
      " [-0.11528881]\n",
      " [ 0.04328464]\n",
      " ...\n",
      " [ 0.54931755]\n",
      " [-0.40688009]\n",
      " [ 0.44873688]]\n",
      "Current iteration=2, loss=34827.992997641864\n",
      "t [[-0.6936818 ]\n",
      " [-0.16935774]\n",
      " [ 0.06927857]\n",
      " ...\n",
      " [ 0.75722611]\n",
      " [-0.5639005 ]\n",
      " [ 0.62288846]]\n",
      "t [[-0.6936818 ]\n",
      " [-0.16935774]\n",
      " [ 0.06927857]\n",
      " ...\n",
      " [ 0.75722611]\n",
      " [-0.5639005 ]\n",
      " [ 0.62288846]]\n",
      "t [[-0.86527279]\n",
      " [-0.21964208]\n",
      " [ 0.09502405]\n",
      " ...\n",
      " [ 0.93146476]\n",
      " [-0.69706415]\n",
      " [ 0.77141918]]\n",
      "t [[-0.86527279]\n",
      " [-0.21964208]\n",
      " [ 0.09502405]\n",
      " ...\n",
      " [ 0.93146476]\n",
      " [-0.69706415]\n",
      " [ 0.77141918]]\n",
      "Current iteration=4, loss=33222.921106847774\n",
      "t [[-1.01588088]\n",
      " [-0.26599354]\n",
      " [ 0.11944389]\n",
      " ...\n",
      " [ 1.07834317]\n",
      " [-0.81064515]\n",
      " [ 0.89896331]]\n",
      "t [[-1.01588088]\n",
      " [-0.26599354]\n",
      " [ 0.11944389]\n",
      " ...\n",
      " [ 1.07834317]\n",
      " [-0.81064515]\n",
      " [ 0.89896331]]\n",
      "t [[-1.14915762]\n",
      " [-0.30857293]\n",
      " [ 0.14202837]\n",
      " ...\n",
      " [ 1.20287647]\n",
      " [-0.90810233]\n",
      " [ 1.00923167]]\n",
      "t [[-1.14915762]\n",
      " [-0.30857293]\n",
      " [ 0.14202837]\n",
      " ...\n",
      " [ 1.20287647]\n",
      " [-0.90810233]\n",
      " [ 1.00923167]]\n",
      "Current iteration=6, loss=32260.664588213258\n",
      "t [[-1.26796576]\n",
      " [-0.34767108]\n",
      " [ 0.1625963 ]\n",
      " ...\n",
      " [ 1.30903106]\n",
      " [-0.99220791]\n",
      " [ 1.10517793]]\n",
      "t [[-1.26796576]\n",
      " [-0.34767108]\n",
      " [ 0.1625963 ]\n",
      " ...\n",
      " [ 1.30903106]\n",
      " [-0.99220791]\n",
      " [ 1.10517793]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.37456854]\n",
      " [-0.38361666]\n",
      " [ 0.18114924]\n",
      " ...\n",
      " [ 1.39995244]\n",
      " [-1.06518257]\n",
      " [ 1.18915784]]\n",
      "t [[-1.37456854]\n",
      " [-0.38361666]\n",
      " [ 0.18114924]\n",
      " ...\n",
      " [ 1.39995244]\n",
      " [-1.06518257]\n",
      " [ 1.18915784]]\n",
      "Current iteration=8, loss=31640.47587243109\n",
      "t [[-1.47077442]\n",
      " [-0.41673224]\n",
      " [ 0.1977855 ]\n",
      " ...\n",
      " [ 1.47815136]\n",
      " [-1.12881268]\n",
      " [ 1.26306174]]\n",
      "t [[-1.47077442]\n",
      " [-0.41673224]\n",
      " [ 0.1977855 ]\n",
      " ...\n",
      " [ 1.47815136]\n",
      " [-1.12881268]\n",
      " [ 1.26306174]]\n",
      "t [[-1.55804493]\n",
      " [-0.4473156 ]\n",
      " [ 0.21265024]\n",
      " ...\n",
      " [ 1.54564841]\n",
      " [-1.18454479]\n",
      " [ 1.32841852]]\n",
      "loss=31214.746346356394\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.23826813]\n",
      " [-0.05012405]\n",
      " [ 0.04974828]\n",
      " ...\n",
      " [-0.06288406]\n",
      " [-0.07818535]\n",
      " [-0.01643632]]\n",
      "t [[-0.23826813]\n",
      " [-0.05012405]\n",
      " [ 0.04974828]\n",
      " ...\n",
      " [-0.06288406]\n",
      " [-0.07818535]\n",
      " [-0.01643632]]\n",
      "t [[-0.43466073]\n",
      " [-0.08198759]\n",
      " [ 0.09432555]\n",
      " ...\n",
      " [-0.11853662]\n",
      " [-0.14565685]\n",
      " [-0.03270702]]\n",
      "t [[-0.43466073]\n",
      " [-0.08198759]\n",
      " [ 0.09432555]\n",
      " ...\n",
      " [-0.11853662]\n",
      " [-0.14565685]\n",
      " [-0.03270702]]\n",
      "Current iteration=2, loss=34698.27537922329\n",
      "t [[-0.59596675]\n",
      " [-0.10285744]\n",
      " [ 0.1329804 ]\n",
      " ...\n",
      " [-0.16793681]\n",
      " [-0.20422281]\n",
      " [-0.04829582]]\n",
      "t [[-0.59596675]\n",
      " [-0.10285744]\n",
      " [ 0.1329804 ]\n",
      " ...\n",
      " [-0.16793681]\n",
      " [-0.20422281]\n",
      " [-0.04829582]]\n",
      "t [[-0.72844236]\n",
      " [-0.11731611]\n",
      " [ 0.16591003]\n",
      " ...\n",
      " [-0.21197496]\n",
      " [-0.25543378]\n",
      " [-0.06297928]]\n",
      "t [[-0.72844236]\n",
      " [-0.11731611]\n",
      " [ 0.16591003]\n",
      " ...\n",
      " [-0.21197496]\n",
      " [-0.25543378]\n",
      " [-0.06297928]]\n",
      "Current iteration=4, loss=33062.00369923901\n",
      "t [[-0.837394  ]\n",
      " [-0.12815658]\n",
      " [ 0.19371882]\n",
      " ...\n",
      " [-0.2514143 ]\n",
      " [-0.30055631]\n",
      " [-0.07669373]]\n",
      "t [[-0.837394  ]\n",
      " [-0.12815658]\n",
      " [ 0.19371882]\n",
      " ...\n",
      " [-0.2514143 ]\n",
      " [-0.30055631]\n",
      " [-0.07669373]]\n",
      "t [[-0.92714737]\n",
      " [-0.13706177]\n",
      " [ 0.21711774]\n",
      " ...\n",
      " [-0.28689402]\n",
      " [-0.34060549]\n",
      " [-0.08945318]]\n",
      "t [[-0.92714737]\n",
      " [-0.13706177]\n",
      " [ 0.21711774]\n",
      " ...\n",
      " [-0.28689402]\n",
      " [-0.34060549]\n",
      " [-0.08945318]]\n",
      "Current iteration=6, loss=32099.391324251555\n",
      "t [[-1.00116997]\n",
      " [-0.14504322]\n",
      " [ 0.23678688]\n",
      " ...\n",
      " [-0.31894571]\n",
      " [-0.37639165]\n",
      " [-0.10130533]]\n",
      "t [[-1.00116997]\n",
      " [-0.14504322]\n",
      " [ 0.23678688]\n",
      " ...\n",
      " [-0.31894571]\n",
      " [-0.37639165]\n",
      " [-0.10130533]]\n",
      "t [[-1.0622289 ]\n",
      " [-0.15270725]\n",
      " [ 0.25332503]\n",
      " ...\n",
      " [-0.34801196]\n",
      " [-0.40856404]\n",
      " [-0.11230976]]\n",
      "t [[-1.0622289 ]\n",
      " [-0.15270725]\n",
      " [ 0.25332503]\n",
      " ...\n",
      " [-0.34801196]\n",
      " [-0.40856404]\n",
      " [-0.11230976]]\n",
      "Current iteration=8, loss=31487.433257473793\n",
      "t [[-1.11253698]\n",
      " [-0.1604136 ]\n",
      " [ 0.26723967]\n",
      " ...\n",
      " [-0.37446286]\n",
      " [-0.43764672]\n",
      " [-0.12252775]]\n",
      "t [[-1.11253698]\n",
      " [-0.1604136 ]\n",
      " [ 0.26723967]\n",
      " ...\n",
      " [-0.37446286]\n",
      " [-0.43764672]\n",
      " [-0.12252775]]\n",
      "t [[-1.15387345]\n",
      " [-0.16837031]\n",
      " [ 0.27895339]\n",
      " ...\n",
      " [-0.39860977]\n",
      " [-0.4640664 ]\n",
      " [-0.13201793]]\n",
      "loss=31071.519689851448\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.27982618]\n",
      " [-0.06304128]\n",
      " [ 0.02462477]\n",
      " ...\n",
      " [-0.06044734]\n",
      " [-0.07859187]\n",
      " [-0.01631288]]\n",
      "t [[-0.27982618]\n",
      " [-0.06304128]\n",
      " [ 0.02462477]\n",
      " ...\n",
      " [-0.06044734]\n",
      " [-0.07859187]\n",
      " [-0.01631288]]\n",
      "t [[-0.51721212]\n",
      " [-0.12426162]\n",
      " [ 0.05334666]\n",
      " ...\n",
      " [-0.11390467]\n",
      " [-0.14637905]\n",
      " [-0.03266994]]\n",
      "t [[-0.51721212]\n",
      " [-0.12426162]\n",
      " [ 0.05334666]\n",
      " ...\n",
      " [-0.11390467]\n",
      " [-0.14637905]\n",
      " [-0.03266994]]\n",
      "Current iteration=2, loss=34674.31120306112\n",
      "t [[-0.72041411]\n",
      " [-0.18177276]\n",
      " [ 0.08274142]\n",
      " ...\n",
      " [-0.16136485]\n",
      " [-0.20519393]\n",
      " [-0.04844598]]\n",
      "t [[-0.72041411]\n",
      " [-0.18177276]\n",
      " [ 0.08274142]\n",
      " ...\n",
      " [-0.16136485]\n",
      " [-0.20519393]\n",
      " [-0.04844598]]\n",
      "t [[-0.89610629]\n",
      " [-0.23489335]\n",
      " [ 0.1109371 ]\n",
      " ...\n",
      " [-0.2037092 ]\n",
      " [-0.25660625]\n",
      " [-0.06335632]]\n",
      "t [[-0.89610629]\n",
      " [-0.23489335]\n",
      " [ 0.1109371 ]\n",
      " ...\n",
      " [-0.2037092 ]\n",
      " [-0.25660625]\n",
      " [-0.06335632]]\n",
      "Current iteration=4, loss=33029.0827030716\n",
      "t [[-1.04949402]\n",
      " [-0.28359022]\n",
      " [ 0.13703682]\n",
      " ...\n",
      " [-0.24168174]\n",
      " [-0.30189679]\n",
      " [-0.07730456]]\n",
      "t [[-1.04949402]\n",
      " [-0.28359022]\n",
      " [ 0.13703682]\n",
      " ...\n",
      " [-0.24168174]\n",
      " [-0.30189679]\n",
      " [-0.07730456]]\n",
      "t [[-1.18460106]\n",
      " [-0.32812736]\n",
      " [ 0.16070256]\n",
      " ...\n",
      " [-0.27589904]\n",
      " [-0.34209087]\n",
      " [-0.09028802]]\n",
      "t [[-1.18460106]\n",
      " [-0.32812736]\n",
      " [ 0.16070256]\n",
      " ...\n",
      " [-0.27589904]\n",
      " [-0.34209087]\n",
      " [-0.09028802]]\n",
      "Current iteration=6, loss=32061.482595474856\n",
      "t [[-1.3045501 ]\n",
      " [-0.36887794]\n",
      " [ 0.18189806]\n",
      " ...\n",
      " [-0.3068701 ]\n",
      " [-0.37800611]\n",
      " [-0.10234637]]\n",
      "t [[-1.3045501 ]\n",
      " [-0.36887794]\n",
      " [ 0.18189806]\n",
      " ...\n",
      " [-0.3068701 ]\n",
      " [-0.37800611]\n",
      " [-0.10234637]]\n",
      "t [[-1.41178819]\n",
      " [-0.40623327]\n",
      " [ 0.20074062]\n",
      " ...\n",
      " [-0.33501633]\n",
      " [-0.41029698]\n",
      " [-0.11353581]]\n",
      "t [[-1.41178819]\n",
      " [-0.40623327]\n",
      " [ 0.20074062]\n",
      " ...\n",
      " [-0.33501633]\n",
      " [-0.41029698]\n",
      " [-0.11353581]]\n",
      "Current iteration=8, loss=31445.41876693678\n",
      "t [[-1.50825577]\n",
      " [-0.44056284]\n",
      " [ 0.21741846]\n",
      " ...\n",
      " [-0.36068868]\n",
      " [-0.43949124]\n",
      " [-0.1239167 ]]\n",
      "t [[-1.50825577]\n",
      " [-0.44056284]\n",
      " [ 0.21741846]\n",
      " ...\n",
      " [-0.36068868]\n",
      " [-0.43949124]\n",
      " [-0.1239167 ]]\n",
      "t [[-1.59551062]\n",
      " [-0.47219957]\n",
      " [ 0.23214555]\n",
      " ...\n",
      " [-0.38418151]\n",
      " [-0.46601827]\n",
      " [-0.13354801]]\n",
      "loss=31025.47516468099\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.28054187]\n",
      " [-0.06502039]\n",
      " [ 0.01689777]\n",
      " ...\n",
      " [-0.05975025]\n",
      " [-0.0790535 ]\n",
      " [-0.01598929]]\n",
      "t [[-0.28054187]\n",
      " [-0.06502039]\n",
      " [ 0.01689777]\n",
      " ...\n",
      " [-0.05975025]\n",
      " [-0.0790535 ]\n",
      " [-0.01598929]]\n",
      "t [[-0.51857872]\n",
      " [-0.12767658]\n",
      " [ 0.04013314]\n",
      " ...\n",
      " [-0.1129096 ]\n",
      " [-0.14724263]\n",
      " [-0.03204895]]\n",
      "t [[-0.51857872]\n",
      " [-0.12767658]\n",
      " [ 0.04013314]\n",
      " ...\n",
      " [-0.1129096 ]\n",
      " [-0.14724263]\n",
      " [-0.03204895]]\n",
      "Current iteration=2, loss=34634.72854349241\n",
      "t [[-0.72234461]\n",
      " [-0.18631204]\n",
      " [ 0.06573775]\n",
      " ...\n",
      " [-0.1603381 ]\n",
      " [-0.20641266]\n",
      " [-0.04758576]]\n",
      "t [[-0.72234461]\n",
      " [-0.18631204]\n",
      " [ 0.06573775]\n",
      " ...\n",
      " [-0.1603381 ]\n",
      " [-0.20641266]\n",
      " [-0.04758576]]\n",
      "t [[-0.89851925]\n",
      " [-0.24037204]\n",
      " [ 0.09139254]\n",
      " ...\n",
      " [-0.20282303]\n",
      " [-0.2581439 ]\n",
      " [-0.06232206]]\n",
      "t [[-0.89851925]\n",
      " [-0.24037204]\n",
      " [ 0.09139254]\n",
      " ...\n",
      " [-0.20282303]\n",
      " [-0.2581439 ]\n",
      " [-0.06232206]]\n",
      "Current iteration=4, loss=32968.749670229445\n",
      "t [[-1.05232035]\n",
      " [-0.28989325]\n",
      " [ 0.11586229]\n",
      " ...\n",
      " [-0.24104436]\n",
      " [-0.30372483]\n",
      " [-0.07615736]]\n",
      "t [[-1.05232035]\n",
      " [-0.28989325]\n",
      " [ 0.11586229]\n",
      " ...\n",
      " [-0.24104436]\n",
      " [-0.30372483]\n",
      " [-0.07615736]]\n",
      "t [[-1.18778341]\n",
      " [-0.33517943]\n",
      " [ 0.13856218]\n",
      " ...\n",
      " [-0.27557584]\n",
      " [-0.34418613]\n",
      " [-0.08908094]]\n",
      "t [[-1.18778341]\n",
      " [-0.33517943]\n",
      " [ 0.13856218]\n",
      " ...\n",
      " [-0.27557584]\n",
      " [-0.34418613]\n",
      " [-0.08908094]]\n",
      "Current iteration=6, loss=31989.44404782708\n",
      "t [[-1.30804057]\n",
      " [-0.37662726]\n",
      " [ 0.15927839]\n",
      " ...\n",
      " [-0.30689798]\n",
      " [-0.38034919]\n",
      " [-0.10112381]]\n",
      "t [[-1.30804057]\n",
      " [-0.37662726]\n",
      " [ 0.15927839]\n",
      " ...\n",
      " [-0.30689798]\n",
      " [-0.38034919]\n",
      " [-0.10112381]]\n",
      "t [[-1.41554603]\n",
      " [-0.4146426 ]\n",
      " [ 0.17800075]\n",
      " ...\n",
      " [-0.33541331]\n",
      " [-0.41287115]\n",
      " [-0.11233424]]\n",
      "t [[-1.41554603]\n",
      " [-0.4146426 ]\n",
      " [ 0.17800075]\n",
      " ...\n",
      " [-0.33541331]\n",
      " [-0.41287115]\n",
      " [-0.11233424]]\n",
      "Current iteration=8, loss=31366.395592069624\n",
      "t [[-1.51224555]\n",
      " [-0.4496044 ]\n",
      " [ 0.19482546]\n",
      " ...\n",
      " [-0.36146033]\n",
      " [-0.4422817 ]\n",
      " [-0.12276581]]\n",
      "t [[-1.51224555]\n",
      " [-0.4496044 ]\n",
      " [ 0.19482546]\n",
      " ...\n",
      " [-0.36146033]\n",
      " [-0.4422817 ]\n",
      " [-0.12276581]]\n",
      "t [[-1.5997009 ]\n",
      " [-0.48185194]\n",
      " [ 0.2098996 ]\n",
      " ...\n",
      " [-0.38532515]\n",
      " [-0.46901169]\n",
      " [-0.13247189]]\n",
      "loss=30942.18082018487\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.27832909]\n",
      " [-0.06053771]\n",
      " [ 0.01981437]\n",
      " ...\n",
      " [ 0.31195558]\n",
      " [-0.22969419]\n",
      " [ 0.25303226]]\n",
      "t [[-0.27832909]\n",
      " [-0.06053771]\n",
      " [ 0.01981437]\n",
      " ...\n",
      " [ 0.31195558]\n",
      " [-0.22969419]\n",
      " [ 0.25303226]]\n",
      "t [[-0.51469538]\n",
      " [-0.11985182]\n",
      " [ 0.04522864]\n",
      " ...\n",
      " [ 0.56920362]\n",
      " [-0.4217178 ]\n",
      " [ 0.46512505]]\n",
      "t [[-0.51469538]\n",
      " [-0.11985182]\n",
      " [ 0.04522864]\n",
      " ...\n",
      " [ 0.56920362]\n",
      " [-0.4217178 ]\n",
      " [ 0.46512505]]\n",
      "Current iteration=2, loss=34738.34998384387\n",
      "t [[-0.71725832]\n",
      " [-0.1758294 ]\n",
      " [ 0.07239922]\n",
      " ...\n",
      " [ 0.7820492 ]\n",
      " [-0.58265209]\n",
      " [ 0.64369237]]\n",
      "t [[-0.71725832]\n",
      " [-0.1758294 ]\n",
      " [ 0.07239922]\n",
      " ...\n",
      " [ 0.7820492 ]\n",
      " [-0.58265209]\n",
      " [ 0.64369237]]\n",
      "t [[-0.89259874]\n",
      " [-0.22765378]\n",
      " [ 0.09913464]\n",
      " ...\n",
      " [ 0.95916622]\n",
      " [-0.71823984]\n",
      " [ 0.79505126]]\n",
      "t [[-0.89259874]\n",
      " [-0.22765378]\n",
      " [ 0.09913464]\n",
      " ...\n",
      " [ 0.95916622]\n",
      " [-0.71823984]\n",
      " [ 0.79505126]]\n",
      "Current iteration=4, loss=33120.77136598219\n",
      "t [[-1.04584464]\n",
      " [-0.27521302]\n",
      " [ 0.12430703]\n",
      " ...\n",
      " [ 1.10750154]\n",
      " [-0.83319815]\n",
      " [ 0.92430802]]\n",
      "t [[-1.04584464]\n",
      " [-0.27521302]\n",
      " [ 0.12430703]\n",
      " ...\n",
      " [ 1.10750154]\n",
      " [-0.83319815]\n",
      " [ 0.92430802]]\n",
      "t [[-1.18095991]\n",
      " [-0.31872568]\n",
      " [ 0.14741564]\n",
      " ...\n",
      " [ 1.23251276]\n",
      " [-0.93130074]\n",
      " [ 1.03550491]]\n",
      "t [[-1.18095991]\n",
      " [-0.31872568]\n",
      " [ 0.14741564]\n",
      " ...\n",
      " [ 1.23251276]\n",
      " [-0.93130074]\n",
      " [ 1.03550491]]\n",
      "Current iteration=6, loss=32166.310033216425\n",
      "t [[-1.30102246]\n",
      " [-0.35853841]\n",
      " [ 0.16831083]\n",
      " ...\n",
      " [ 1.33847179]\n",
      " [-1.01554089]\n",
      " [ 1.13182679]]\n",
      "t [[-1.30102246]\n",
      " [-0.35853841]\n",
      " [ 0.16831083]\n",
      " ...\n",
      " [ 1.33847179]\n",
      " [-1.01554089]\n",
      " [ 1.13182679]]\n",
      "t [[-1.40844625]\n",
      " [-0.39502657]\n",
      " [ 0.18703117]\n",
      " ...\n",
      " [ 1.42873621]\n",
      " [-1.08829457]\n",
      " [ 1.21579157]]\n",
      "t [[-1.40844625]\n",
      " [-0.39502657]\n",
      " [ 0.18703117]\n",
      " ...\n",
      " [ 1.42873621]\n",
      " [-1.08829457]\n",
      " [ 1.21579157]]\n",
      "Current iteration=8, loss=31557.39766508282\n",
      "t [[-1.50514745]\n",
      " [-0.42854969]\n",
      " [ 0.20371008]\n",
      " ...\n",
      " [ 1.50596542]\n",
      " [-1.15145802]\n",
      " [ 1.2894045 ]]\n",
      "t [[-1.50514745]\n",
      " [-0.42854969]\n",
      " [ 0.20371008]\n",
      " ...\n",
      " [ 1.50596542]\n",
      " [-1.15145802]\n",
      " [ 1.2894045 ]]\n",
      "t [[-1.59266599]\n",
      " [-0.4594341 ]\n",
      " [ 0.21852332]\n",
      " ...\n",
      " [ 1.57228535]\n",
      " [-1.20655621]\n",
      " [ 1.35427683]]\n",
      "loss=31141.9602039329\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.24743229]\n",
      " [-0.0520519 ]\n",
      " [ 0.05166168]\n",
      " ...\n",
      " [-0.06530268]\n",
      " [-0.08119248]\n",
      " [-0.01706848]]\n",
      "t [[-0.24743229]\n",
      " [-0.0520519 ]\n",
      " [ 0.05166168]\n",
      " ...\n",
      " [-0.06530268]\n",
      " [-0.08119248]\n",
      " [-0.01706848]]\n",
      "t [[-0.44971948]\n",
      " [-0.08441845]\n",
      " [ 0.09774749]\n",
      " ...\n",
      " [-0.12280989]\n",
      " [-0.15083479]\n",
      " [-0.03395849]]\n",
      "t [[-0.44971948]\n",
      " [-0.08441845]\n",
      " [ 0.09774749]\n",
      " ...\n",
      " [-0.12280989]\n",
      " [-0.15083479]\n",
      " [-0.03395849]]\n",
      "Current iteration=2, loss=34609.197783322554\n",
      "t [[-0.61447909]\n",
      " [-0.1052361 ]\n",
      " [ 0.13742018]\n",
      " ...\n",
      " [-0.17362151]\n",
      " [-0.2109558 ]\n",
      " [-0.05009304]]\n",
      "t [[-0.61447909]\n",
      " [-0.1052361 ]\n",
      " [ 0.13742018]\n",
      " ...\n",
      " [-0.17362151]\n",
      " [-0.2109558 ]\n",
      " [-0.05009304]]\n",
      "t [[-0.74868099]\n",
      " [-0.11950901]\n",
      " [ 0.17094265]\n",
      " ...\n",
      " [-0.21872837]\n",
      " [-0.26327733]\n",
      " [-0.06523519]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.74868099]\n",
      " [-0.11950901]\n",
      " [ 0.17094265]\n",
      " ...\n",
      " [-0.21872837]\n",
      " [-0.26327733]\n",
      " [-0.06523519]]\n",
      "Current iteration=4, loss=32962.6955479505\n",
      "t [[-0.8581702 ]\n",
      " [-0.13020981]\n",
      " [ 0.1990227 ]\n",
      " ...\n",
      " [-0.25897138]\n",
      " [-0.30918977]\n",
      " [-0.07932657]]\n",
      "t [[-0.8581702 ]\n",
      " [-0.13020981]\n",
      " [ 0.1990227 ]\n",
      " ...\n",
      " [-0.25897138]\n",
      " [-0.30918977]\n",
      " [-0.07932657]]\n",
      "t [[-0.94765536]\n",
      " [-0.13908573]\n",
      " [ 0.22246555]\n",
      " ...\n",
      " [-0.29504848]\n",
      " [-0.34979551]\n",
      " [-0.09239168]]\n",
      "t [[-0.94765536]\n",
      " [-0.13908573]\n",
      " [ 0.22246555]\n",
      " ...\n",
      " [-0.29504848]\n",
      " [-0.34979551]\n",
      " [-0.09239168]]\n",
      "Current iteration=6, loss=32009.129708615437\n",
      "t [[-1.02086989]\n",
      " [-0.14716161]\n",
      " [ 0.24202429]\n",
      " ...\n",
      " [-0.32753567]\n",
      " [-0.38596637]\n",
      " [-0.10448869]]\n",
      "t [[-1.02086989]\n",
      " [-0.14716161]\n",
      " [ 0.24202429]\n",
      " ...\n",
      " [-0.32753567]\n",
      " [-0.38596637]\n",
      " [-0.10448869]]\n",
      "t [[-1.08076448]\n",
      " [-0.15503606]\n",
      " [ 0.25835058]\n",
      " ...\n",
      " [-0.35690936]\n",
      " [-0.41839538]\n",
      " [-0.11568617]]\n",
      "t [[-1.08076448]\n",
      " [-0.15503606]\n",
      " [ 0.25835058]\n",
      " ...\n",
      " [-0.35690936]\n",
      " [-0.41839538]\n",
      " [-0.11568617]]\n",
      "Current iteration=8, loss=31408.888753314182\n",
      "t [[-1.12967911]\n",
      " [-0.16305364]\n",
      " [ 0.27198902]\n",
      " ...\n",
      " [-0.38356567]\n",
      " [-0.4476381 ]\n",
      " [-0.12605278]]\n",
      "t [[-1.12967911]\n",
      " [-0.16305364]\n",
      " [ 0.27198902]\n",
      " ...\n",
      " [-0.38356567]\n",
      " [-0.4476381 ]\n",
      " [-0.12605278]]\n",
      "t [[-1.16948168]\n",
      " [-0.17140557]\n",
      " [ 0.28338794]\n",
      " ...\n",
      " [-0.40783619]\n",
      " [-0.47414435]\n",
      " [-0.13565321]]\n",
      "loss=31003.29526928773\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.29058873]\n",
      " [-0.06546595]\n",
      " [ 0.02557188]\n",
      " ...\n",
      " [-0.06277223]\n",
      " [-0.08161463]\n",
      " [-0.0169403 ]]\n",
      "t [[-0.29058873]\n",
      " [-0.06546595]\n",
      " [ 0.02557188]\n",
      " ...\n",
      " [-0.06277223]\n",
      " [-0.08161463]\n",
      " [-0.0169403 ]]\n",
      "t [[-0.5354266 ]\n",
      " [-0.12896687]\n",
      " [ 0.05555972]\n",
      " ...\n",
      " [-0.11800928]\n",
      " [-0.15158121]\n",
      " [-0.03392826]]\n",
      "t [[-0.5354266 ]\n",
      " [-0.12896687]\n",
      " [ 0.05555972]\n",
      " ...\n",
      " [-0.11800928]\n",
      " [-0.15158121]\n",
      " [-0.03392826]]\n",
      "Current iteration=2, loss=34584.65453362371\n",
      "t [[-0.74377434]\n",
      " [-0.18839149]\n",
      " [ 0.08613181]\n",
      " ...\n",
      " [-0.16682617]\n",
      " [-0.21195553]\n",
      " [-0.05026503]]\n",
      "t [[-0.74377434]\n",
      " [-0.18839149]\n",
      " [ 0.08613181]\n",
      " ...\n",
      " [-0.16682617]\n",
      " [-0.21195553]\n",
      " [-0.05026503]]\n",
      "t [[-0.92302077]\n",
      " [-0.24303275]\n",
      " [ 0.11526428]\n",
      " ...\n",
      " [-0.21020274]\n",
      " [-0.26448068]\n",
      " [-0.06564724]]\n",
      "t [[-0.92302077]\n",
      " [-0.24303275]\n",
      " [ 0.11526428]\n",
      " ...\n",
      " [-0.21020274]\n",
      " [-0.26448068]\n",
      " [-0.06564724]]\n",
      "Current iteration=4, loss=32929.273493687666\n",
      "t [[-1.07885407]\n",
      " [-0.29290704]\n",
      " [ 0.14203532]\n",
      " ...\n",
      " [-0.24895769]\n",
      " [-0.31056246]\n",
      " [-0.07998233]]\n",
      "t [[-1.07885407]\n",
      " [-0.29290704]\n",
      " [ 0.14203532]\n",
      " ...\n",
      " [-0.24895769]\n",
      " [-0.31056246]\n",
      " [-0.07998233]]\n",
      "t [[-1.21562155]\n",
      " [-0.33834389]\n",
      " [ 0.16613365]\n",
      " ...\n",
      " [-0.28376339]\n",
      " [-0.35131423]\n",
      " [-0.09327803]]\n",
      "t [[-1.21562155]\n",
      " [-0.33834389]\n",
      " [ 0.16613365]\n",
      " ...\n",
      " [-0.28376339]\n",
      " [-0.35131423]\n",
      " [-0.09327803]]\n",
      "Current iteration=6, loss=31970.713573125064\n",
      "t [[-1.3366653 ]\n",
      " [-0.37977611]\n",
      " [ 0.18756549]\n",
      " ...\n",
      " [-0.31517064]\n",
      " [-0.3876155 ]\n",
      " [-0.10558492]]\n",
      "t [[-1.3366653 ]\n",
      " [-0.37977611]\n",
      " [ 0.18756549]\n",
      " ...\n",
      " [-0.31517064]\n",
      " [-0.3876155 ]\n",
      " [-0.10558492]]\n",
      "t [[-1.44458398]\n",
      " [-0.41764321]\n",
      " [ 0.20649122]\n",
      " ...\n",
      " [-0.34363252]\n",
      " [-0.42016468]\n",
      " [-0.11696869]]\n",
      "t [[-1.44458398]\n",
      " [-0.41764321]\n",
      " [ 0.20649122]\n",
      " ...\n",
      " [-0.34363252]\n",
      " [-0.42016468]\n",
      " [-0.11696869]]\n",
      "Current iteration=8, loss=31366.255276132168\n",
      "t [[-1.54142505]\n",
      " [-0.45235195]\n",
      " [ 0.22313711]\n",
      " ...\n",
      " [-0.36952424]\n",
      " [-0.44952116]\n",
      " [-0.12749756]]\n",
      "t [[-1.54142505]\n",
      " [-0.45235195]\n",
      " [ 0.22313711]\n",
      " ...\n",
      " [-0.36952424]\n",
      " [-0.44952116]\n",
      " [-0.12749756]]\n",
      "t [[-1.62882337]\n",
      " [-0.4842636 ]\n",
      " [ 0.23774883]\n",
      " ...\n",
      " [-0.39315899]\n",
      " [-0.47613743]\n",
      " [-0.13723691]]\n",
      "loss=30956.47006311413\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.29133194]\n",
      " [-0.06752118]\n",
      " [ 0.01754769]\n",
      " ...\n",
      " [-0.06204833]\n",
      " [-0.08209402]\n",
      " [-0.01660426]]\n",
      "t [[-0.29133194]\n",
      " [-0.06752118]\n",
      " [ 0.01754769]\n",
      " ...\n",
      " [-0.06204833]\n",
      " [-0.08209402]\n",
      " [-0.01660426]]\n",
      "t [[-0.53684349]\n",
      " [-0.13249152]\n",
      " [ 0.0419264 ]\n",
      " ...\n",
      " [-0.11699172]\n",
      " [-0.15247569]\n",
      " [-0.03328441]]\n",
      "t [[-0.53684349]\n",
      " [-0.13249152]\n",
      " [ 0.0419264 ]\n",
      " ...\n",
      " [-0.11699172]\n",
      " [-0.15247569]\n",
      " [-0.03328441]]\n",
      "Current iteration=2, loss=34543.91540397172\n",
      "t [[-0.74577079]\n",
      " [-0.19305927]\n",
      " [ 0.06869434]\n",
      " ...\n",
      " [-0.1657964 ]\n",
      " [-0.21321534]\n",
      " [-0.04937731]]\n",
      "t [[-0.74577079]\n",
      " [-0.19305927]\n",
      " [ 0.06869434]\n",
      " ...\n",
      " [-0.1657964 ]\n",
      " [-0.21321534]\n",
      " [-0.04937731]]\n",
      "t [[-0.92550972]\n",
      " [-0.24865456]\n",
      " [ 0.09533265]\n",
      " ...\n",
      " [-0.20933882]\n",
      " [-0.26606767]\n",
      " [-0.06458641]]\n",
      "t [[-0.92550972]\n",
      " [-0.24865456]\n",
      " [ 0.09533265]\n",
      " ...\n",
      " [-0.20933882]\n",
      " [-0.26606767]\n",
      " [-0.06458641]]\n",
      "Current iteration=4, loss=32867.69545050066\n",
      "t [[-1.08176249]\n",
      " [-0.29936729]\n",
      " [ 0.120551  ]\n",
      " ...\n",
      " [-0.24836867]\n",
      " [-0.31244681]\n",
      " [-0.07881369]]\n",
      "t [[-1.08176249]\n",
      " [-0.29936729]\n",
      " [ 0.120551  ]\n",
      " ...\n",
      " [-0.24836867]\n",
      " [-0.31244681]\n",
      " [-0.07881369]]\n",
      "t [[-1.21888936]\n",
      " [-0.34556778]\n",
      " [ 0.14377379]\n",
      " ...\n",
      " [-0.28351305]\n",
      " [-0.35347181]\n",
      " [-0.09205747]]\n",
      "t [[-1.21888936]\n",
      " [-0.34556778]\n",
      " [ 0.14377379]\n",
      " ...\n",
      " [-0.28351305]\n",
      " [-0.35347181]\n",
      " [-0.09205747]]\n",
      "Current iteration=6, loss=31897.593507459973\n",
      "t [[-1.34024255]\n",
      " [-0.38771247]\n",
      " [ 0.16482002]\n",
      " ...\n",
      " [-0.31529324]\n",
      " [-0.3900261 ]\n",
      " [-0.10435856]]\n",
      "t [[-1.34024255]\n",
      " [-0.38771247]\n",
      " [ 0.16482002]\n",
      " ...\n",
      " [-0.31529324]\n",
      " [-0.3900261 ]\n",
      " [-0.10435856]]\n",
      "t [[-1.44842826]\n",
      " [-0.42625535]\n",
      " [ 0.1837175 ]\n",
      " ...\n",
      " [-0.34414313]\n",
      " [-0.42281085]\n",
      " [-0.11577398]]\n",
      "t [[-1.44842826]\n",
      " [-0.42625535]\n",
      " [ 0.1837175 ]\n",
      " ...\n",
      " [-0.34414313]\n",
      " [-0.42281085]\n",
      " [-0.11577398]]\n",
      "Current iteration=8, loss=31286.361759756175\n",
      "t [[-1.54549949]\n",
      " [-0.46161247]\n",
      " [ 0.20059809]\n",
      " ...\n",
      " [-0.3704255 ]\n",
      " [-0.45238745]\n",
      " [-0.12636463]]\n",
      "t [[-1.54549949]\n",
      " [-0.46161247]\n",
      " [ 0.20059809]\n",
      " ...\n",
      " [-0.3704255 ]\n",
      " [-0.45238745]\n",
      " [-0.12636463]]\n",
      "t [[-1.63309518]\n",
      " [-0.49415141]\n",
      " [ 0.21563955]\n",
      " ...\n",
      " [-0.39444548]\n",
      " [-0.4792099 ]\n",
      " [-0.13618998]]\n",
      "loss=30872.51240824534\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.28903406]\n",
      " [-0.06286608]\n",
      " [ 0.02057646]\n",
      " ...\n",
      " [ 0.32395387]\n",
      " [-0.23852858]\n",
      " [ 0.26276427]]\n",
      "t [[-0.28903406]\n",
      " [-0.06286608]\n",
      " [ 0.02057646]\n",
      " ...\n",
      " [ 0.32395387]\n",
      " [-0.23852858]\n",
      " [ 0.26276427]]\n",
      "t [[-0.53283185]\n",
      " [-0.12441091]\n",
      " [ 0.04718865]\n",
      " ...\n",
      " [ 0.58893139]\n",
      " [-0.43644635]\n",
      " [ 0.48139478]]\n",
      "t [[-0.53283185]\n",
      " [-0.12441091]\n",
      " [ 0.04718865]\n",
      " ...\n",
      " [ 0.58893139]\n",
      " [-0.43644635]\n",
      " [ 0.48139478]]\n",
      "Current iteration=2, loss=34650.41031480176\n",
      "t [[-0.74054541]\n",
      " [-0.18227142]\n",
      " [ 0.07553458]\n",
      " ...\n",
      " [ 0.80649143]\n",
      " [-0.6011366 ]\n",
      " [ 0.66420891]]\n",
      "t [[-0.74054541]\n",
      " [-0.18227142]\n",
      " [ 0.07553458]\n",
      " ...\n",
      " [ 0.80649143]\n",
      " [-0.6011366 ]\n",
      " [ 0.66420891]]\n",
      "t [[-0.91945954]\n",
      " [-0.23559336]\n",
      " [ 0.10323795]\n",
      " ...\n",
      " [ 0.98625601]\n",
      " [-0.73898105]\n",
      " [ 0.81821795]]\n",
      "t [[-0.91945954]\n",
      " [-0.23559336]\n",
      " [ 0.10323795]\n",
      " ...\n",
      " [ 0.98625601]\n",
      " [-0.73898105]\n",
      " [ 0.81821795]]\n",
      "Current iteration=4, loss=33022.483745089565\n",
      "t [[-1.07517746]\n",
      " [-0.28431004]\n",
      " [ 0.12912667]\n",
      " ...\n",
      " [ 1.13583626]\n",
      " [-0.85516028]\n",
      " [ 0.94902096]]\n",
      "t [[-1.07517746]\n",
      " [-0.28431004]\n",
      " [ 0.12912667]\n",
      " ...\n",
      " [ 1.13583626]\n",
      " [-0.85516028]\n",
      " [ 0.94902096]]\n",
      "t [[-1.21198191]\n",
      " [-0.32870433]\n",
      " [ 0.15271626]\n",
      " ...\n",
      " [ 1.26114273]\n",
      " [-0.95377153]\n",
      " [ 1.06099997]]\n",
      "t [[-1.21198191]\n",
      " [-0.32870433]\n",
      " [ 0.15271626]\n",
      " ...\n",
      " [ 1.26114273]\n",
      " [-0.95377153]\n",
      " [ 1.06099997]]\n",
      "Current iteration=6, loss=32076.685011989328\n",
      "t [[-1.33316738]\n",
      " [-0.3691824 ]\n",
      " [ 0.17389432]\n",
      " ...\n",
      " [ 1.36675508]\n",
      " [-1.03803151]\n",
      " [ 1.15757325]]\n",
      "t [[-1.33316738]\n",
      " [-0.3691824 ]\n",
      " [ 0.17389432]\n",
      " ...\n",
      " [ 1.36675508]\n",
      " [-1.03803151]\n",
      " [ 1.15757325]]\n",
      "t [[-1.44129795]\n",
      " [-0.406168  ]\n",
      " [ 0.19274009]\n",
      " ...\n",
      " [ 1.45624145]\n",
      " [-1.11047084]\n",
      " [ 1.24142018]]\n",
      "t [[-1.44129795]\n",
      " [-0.406168  ]\n",
      " [ 0.19274009]\n",
      " ...\n",
      " [ 1.45624145]\n",
      " [-1.11047084]\n",
      " [ 1.24142018]]\n",
      "Current iteration=8, loss=31479.117123158587\n",
      "t [[-1.53839601]\n",
      " [-0.44005811]\n",
      " [ 0.20942431]\n",
      " ...\n",
      " [ 1.53240697]\n",
      " [-1.17309376]\n",
      " [ 1.31465901]]\n",
      "t [[-1.53839601]\n",
      " [-0.44005811]\n",
      " [ 0.20942431]\n",
      " ...\n",
      " [ 1.53240697]\n",
      " [-1.17309376]\n",
      " [ 1.31465901]]\n",
      "t [[-1.62607822]\n",
      " [-0.47120754]\n",
      " [ 0.22415445]\n",
      " ...\n",
      " [ 1.5974796 ]\n",
      " [-1.22750143]\n",
      " [ 1.37898118]]\n",
      "loss=31073.694135488768\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.25659645]\n",
      " [-0.05397975]\n",
      " [ 0.05357507]\n",
      " ...\n",
      " [-0.0677213 ]\n",
      " [-0.08419961]\n",
      " [-0.01770065]]\n",
      "t [[-0.25659645]\n",
      " [-0.05397975]\n",
      " [ 0.05357507]\n",
      " ...\n",
      " [-0.0677213 ]\n",
      " [-0.08419961]\n",
      " [-0.01770065]]\n",
      "t [[-0.46465691]\n",
      " [-0.08679657]\n",
      " [ 0.10115423]\n",
      " ...\n",
      " [-0.12706233]\n",
      " [-0.15598174]\n",
      " [-0.03520948]]\n",
      "t [[-0.46465691]\n",
      " [-0.08679657]\n",
      " [ 0.10115423]\n",
      " ...\n",
      " [-0.12706233]\n",
      " [-0.15598174]\n",
      " [-0.03520948]]\n",
      "Current iteration=2, loss=34521.82137081869\n",
      "t [[-0.63269245]\n",
      " [-0.10752242]\n",
      " [ 0.14180851]\n",
      " ...\n",
      " [-0.17925331]\n",
      " [-0.21761303]\n",
      " [-0.0518844 ]]\n",
      "t [[-0.63269245]\n",
      " [-0.10752242]\n",
      " [ 0.14180851]\n",
      " ...\n",
      " [-0.17925331]\n",
      " [-0.21761303]\n",
      " [-0.0518844 ]]\n",
      "t [[-0.7684338 ]\n",
      " [-0.12159565]\n",
      " [ 0.17587753]\n",
      " ...\n",
      " [-0.2253919 ]\n",
      " [-0.27099705]\n",
      " [-0.06747582]]\n",
      "t [[-0.7684338 ]\n",
      " [-0.12159565]\n",
      " [ 0.17587753]\n",
      " ...\n",
      " [-0.2253919 ]\n",
      " [-0.27099705]\n",
      " [-0.06747582]]\n",
      "Current iteration=4, loss=32867.14034242901\n",
      "t [[-0.87828961]\n",
      " [-0.13216354]\n",
      " [ 0.20418259]\n",
      " ...\n",
      " [-0.26640043]\n",
      " [-0.31765319]\n",
      " [-0.0819324 ]]\n",
      "t [[-0.87828961]\n",
      " [-0.13216354]\n",
      " [ 0.20418259]\n",
      " ...\n",
      " [-0.26640043]\n",
      " [-0.31765319]\n",
      " [-0.0819324 ]]\n",
      "t [[-0.96736166]\n",
      " [-0.14102996]\n",
      " [ 0.22762864]\n",
      " ...\n",
      " [-0.30303766]\n",
      " [-0.35877338]\n",
      " [-0.09529035]]\n",
      "t [[-0.96736166]\n",
      " [-0.14102996]\n",
      " [ 0.22762864]\n",
      " ...\n",
      " [-0.30303766]\n",
      " [-0.35877338]\n",
      " [-0.09529035]]\n",
      "Current iteration=6, loss=31923.36632652949\n",
      "t [[-1.03965139]\n",
      " [-0.14922686]\n",
      " [ 0.24704385]\n",
      " ...\n",
      " [-0.33592515]\n",
      " [-0.39529164]\n",
      " [-0.10761903]]\n",
      "t [[-1.03965139]\n",
      " [-0.14922686]\n",
      " [ 0.24704385]\n",
      " ...\n",
      " [-0.33592515]\n",
      " [-0.39529164]\n",
      " [-0.10761903]]\n",
      "t [[-1.09829157]\n",
      " [-0.15734094]\n",
      " [ 0.26313286]\n",
      " ...\n",
      " [-0.36557368]\n",
      " [-0.42794469]\n",
      " [-0.11899638]]\n",
      "t [[-1.09829157]\n",
      " [-0.15734094]\n",
      " [ 0.26313286]\n",
      " ...\n",
      " [-0.36557368]\n",
      " [-0.42794469]\n",
      " [-0.11899638]]\n",
      "Current iteration=8, loss=31334.83391351944\n",
      "t [[-1.14574645]\n",
      " [-0.16569891]\n",
      " [ 0.27647667]\n",
      " ...\n",
      " [-0.3924056 ]\n",
      " [-0.45731933]\n",
      " [-0.12949864]]\n",
      "t [[-1.14574645]\n",
      " [-0.16569891]\n",
      " [ 0.27647667]\n",
      " ...\n",
      " [-0.3924056 ]\n",
      " [-0.45731933]\n",
      " [-0.12949864]]\n",
      "t [[-1.18396944]\n",
      " [-0.17447348]\n",
      " [ 0.28754832]\n",
      " ...\n",
      " [-0.4167728 ]\n",
      " [-0.48388802]\n",
      " [-0.13919665]]\n",
      "loss=30939.248447074402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.30135127]\n",
      " [-0.06789061]\n",
      " [ 0.02651898]\n",
      " ...\n",
      " [-0.06509713]\n",
      " [-0.0846374 ]\n",
      " [-0.01756771]]\n",
      "t [[-0.30135127]\n",
      " [-0.06789061]\n",
      " [ 0.02651898]\n",
      " ...\n",
      " [-0.06509713]\n",
      " [-0.0846374 ]\n",
      " [-0.01756771]]\n",
      "t [[-0.5535187 ]\n",
      " [-0.13366647]\n",
      " [ 0.05778446]\n",
      " ...\n",
      " [-0.12209375]\n",
      " [-0.15675211]\n",
      " [-0.03518673]]\n",
      "t [[-0.5535187 ]\n",
      " [-0.13366647]\n",
      " [ 0.05778446]\n",
      " ...\n",
      " [-0.12209375]\n",
      " [-0.15675211]\n",
      " [-0.03518673]]\n",
      "Current iteration=2, loss=34496.7182522134\n",
      "t [[-0.76684556]\n",
      " [-0.19497693]\n",
      " [ 0.08952668]\n",
      " ...\n",
      " [-0.17223673]\n",
      " [-0.21864083]\n",
      " [-0.05207905]]\n",
      "t [[-0.76684556]\n",
      " [-0.19497693]\n",
      " [ 0.08952668]\n",
      " ...\n",
      " [-0.17223673]\n",
      " [-0.21864083]\n",
      " [-0.05207905]]\n",
      "t [[-0.94947504]\n",
      " [-0.25109553]\n",
      " [ 0.11956924]\n",
      " ...\n",
      " [-0.2166105 ]\n",
      " [-0.27223049]\n",
      " [-0.06792351]]\n",
      "t [[-0.94947504]\n",
      " [-0.25109553]\n",
      " [ 0.11956924]\n",
      " ...\n",
      " [-0.2166105 ]\n",
      " [-0.27223049]\n",
      " [-0.06792351]]\n",
      "Current iteration=4, loss=32833.24022772568\n",
      "t [[-1.10759485]\n",
      " [-0.30209735]\n",
      " [ 0.14697296]\n",
      " ...\n",
      " [-0.25611191]\n",
      " [-0.31905719]\n",
      " [-0.08263321]]\n",
      "t [[-1.10759485]\n",
      " [-0.30209735]\n",
      " [ 0.14697296]\n",
      " ...\n",
      " [-0.25611191]\n",
      " [-0.31905719]\n",
      " [-0.08263321]]\n",
      "t [[-1.24588149]\n",
      " [-0.34838373]\n",
      " [ 0.1714607 ]\n",
      " ...\n",
      " [-0.29147099]\n",
      " [-0.3603245 ]\n",
      " [-0.09622774]]\n",
      "t [[-1.24588149]\n",
      " [-0.34838373]\n",
      " [ 0.1714607 ]\n",
      " ...\n",
      " [-0.29147099]\n",
      " [-0.3603245 ]\n",
      " [-0.09622774]]\n",
      "Current iteration=6, loss=31884.45482121393\n",
      "t [[-1.36789722]\n",
      " [-0.39045032]\n",
      " [ 0.19308639]\n",
      " ...\n",
      " [-0.32328133]\n",
      " [-0.39697452]\n",
      " [-0.10876935]]\n",
      "t [[-1.36789722]\n",
      " [-0.39045032]\n",
      " [ 0.19308639]\n",
      " ...\n",
      " [-0.32328133]\n",
      " [-0.39697452]\n",
      " [-0.10876935]]\n",
      "t [[-1.4763912 ]\n",
      " [-0.42878621]\n",
      " [ 0.21205659]\n",
      " ...\n",
      " [-0.35202815]\n",
      " [-0.42974955]\n",
      " [-0.12033367]]\n",
      "t [[-1.4763912 ]\n",
      " [-0.42878621]\n",
      " [ 0.21205659]\n",
      " ...\n",
      " [-0.35202815]\n",
      " [-0.42974955]\n",
      " [-0.12033367]]\n",
      "Current iteration=8, loss=31291.58349396417\n",
      "t [[-1.57351616]\n",
      " [-0.46383568]\n",
      " [ 0.22863734]\n",
      " ...\n",
      " [-0.37811113]\n",
      " [-0.45924025]\n",
      " [-0.13099698]]\n",
      "t [[-1.57351616]\n",
      " [-0.46383568]\n",
      " [ 0.22863734]\n",
      " ...\n",
      " [-0.37811113]\n",
      " [-0.45924025]\n",
      " [-0.13099698]]\n",
      "t [[-1.66098216]\n",
      " [-0.49598822]\n",
      " [ 0.24310669]\n",
      " ...\n",
      " [-0.40186229]\n",
      " [-0.48592178]\n",
      " [-0.14083119]]\n",
      "loss=30891.639975364007\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.30212201]\n",
      " [-0.07002196]\n",
      " [ 0.0181976 ]\n",
      " ...\n",
      " [-0.06434642]\n",
      " [-0.08513454]\n",
      " [-0.01721924]]\n",
      "t [[-0.30212201]\n",
      " [-0.07002196]\n",
      " [ 0.0181976 ]\n",
      " ...\n",
      " [-0.06434642]\n",
      " [-0.08513454]\n",
      " [-0.01721924]]\n",
      "t [[-0.55498576]\n",
      " [-0.13729923]\n",
      " [ 0.04373775]\n",
      " ...\n",
      " [-0.12105485]\n",
      " [-0.15767734]\n",
      " [-0.03452009]]\n",
      "t [[-0.55498576]\n",
      " [-0.13729923]\n",
      " [ 0.04373775]\n",
      " ...\n",
      " [-0.12105485]\n",
      " [-0.15767734]\n",
      " [-0.03452009]]\n",
      "Current iteration=2, loss=34454.84573117901\n",
      "t [[-0.7689074 ]\n",
      " [-0.19977053]\n",
      " [ 0.0716696 ]\n",
      " ...\n",
      " [-0.17120617]\n",
      " [-0.21994135]\n",
      " [-0.05116432]]\n",
      "t [[-0.7689074 ]\n",
      " [-0.19977053]\n",
      " [ 0.0716696 ]\n",
      " ...\n",
      " [-0.17120617]\n",
      " [-0.21994135]\n",
      " [-0.05116432]]\n",
      "t [[-0.95203889]\n",
      " [-0.25685737]\n",
      " [ 0.09927114]\n",
      " ...\n",
      " [-0.21577166]\n",
      " [-0.27386628]\n",
      " [-0.0668372 ]]\n",
      "t [[-0.95203889]\n",
      " [-0.25685737]\n",
      " [ 0.09927114]\n",
      " ...\n",
      " [-0.21577166]\n",
      " [-0.27386628]\n",
      " [-0.0668372 ]]\n",
      "Current iteration=4, loss=32770.46685941522\n",
      "t [[-1.11058378]\n",
      " [-0.30871171]\n",
      " [ 0.12520377]\n",
      " ...\n",
      " [-0.25557417]\n",
      " [-0.32099715]\n",
      " [-0.08144484]]\n",
      "t [[-1.11058378]\n",
      " [-0.30871171]\n",
      " [ 0.12520377]\n",
      " ...\n",
      " [-0.25557417]\n",
      " [-0.32099715]\n",
      " [-0.08144484]]\n",
      "t [[-1.24923272]\n",
      " [-0.35577645]\n",
      " [ 0.14890859]\n",
      " ...\n",
      " [-0.29129613]\n",
      " [-0.36254353]\n",
      " [-0.0949959 ]]\n",
      "t [[-1.24923272]\n",
      " [-0.35577645]\n",
      " [ 0.14890859]\n",
      " ...\n",
      " [-0.29129613]\n",
      " [-0.36254353]\n",
      " [-0.0949959 ]]\n",
      "Current iteration=6, loss=31810.31327792254\n",
      "t [[-1.37155878]\n",
      " [-0.39857093]\n",
      " [ 0.17024305]\n",
      " ...\n",
      " [-0.3235007 ]\n",
      " [-0.3994516 ]\n",
      " [-0.10754178]]\n",
      "t [[-1.37155878]\n",
      " [-0.39857093]\n",
      " [ 0.17024305]\n",
      " ...\n",
      " [-0.3235007 ]\n",
      " [-0.3994516 ]\n",
      " [-0.10754178]]\n",
      "t [[-1.48031901]\n",
      " [-0.43759851]\n",
      " [ 0.18927657]\n",
      " ...\n",
      " [-0.3526537 ]\n",
      " [-0.4324665 ]\n",
      " [-0.11914865]]\n",
      "t [[-1.48031901]\n",
      " [-0.43759851]\n",
      " [ 0.18927657]\n",
      " ...\n",
      " [-0.3526537 ]\n",
      " [-0.4324665 ]\n",
      " [-0.11914865]]\n",
      "Current iteration=8, loss=31210.881717323144\n",
      "t [[-1.57767188]\n",
      " [-0.47331269]\n",
      " [ 0.20617868]\n",
      " ...\n",
      " [-0.37914246]\n",
      " [-0.46218096]\n",
      " [-0.12988497]]\n",
      "t [[-1.57767188]\n",
      " [-0.47331269]\n",
      " [ 0.20617868]\n",
      " ...\n",
      " [-0.37914246]\n",
      " [-0.46218096]\n",
      " [-0.12988497]]\n",
      "t [[-1.66533168]\n",
      " [-0.50610911]\n",
      " [ 0.22115882]\n",
      " ...\n",
      " [-0.40329121]\n",
      " [-0.48907169]\n",
      " [-0.13981645]]\n",
      "loss=30807.079096553018\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.29973902]\n",
      " [-0.06519446]\n",
      " [ 0.02133855]\n",
      " ...\n",
      " [ 0.33595216]\n",
      " [-0.24736298]\n",
      " [ 0.27249628]]\n",
      "t [[-0.29973902]\n",
      " [-0.06519446]\n",
      " [ 0.02133855]\n",
      " ...\n",
      " [ 0.33595216]\n",
      " [-0.24736298]\n",
      " [ 0.27249628]]\n",
      "t [[-0.55084732]\n",
      " [-0.12896605]\n",
      " [ 0.04916462]\n",
      " ...\n",
      " [ 0.60850118]\n",
      " [-0.45106597]\n",
      " [ 0.49754633]]\n",
      "t [[-0.55084732]\n",
      " [-0.12896605]\n",
      " [ 0.04916462]\n",
      " ...\n",
      " [ 0.60850118]\n",
      " [-0.45106597]\n",
      " [ 0.49754633]]\n",
      "Current iteration=2, loss=34564.145364735494\n",
      "t [[-0.76354664]\n",
      " [-0.18868314]\n",
      " [ 0.07868335]\n",
      " ...\n",
      " [ 0.83055729]\n",
      " [-0.61935693]\n",
      " [ 0.68444135]]\n",
      "t [[-0.76354664]\n",
      " [-0.18868314]\n",
      " [ 0.07868335]\n",
      " ...\n",
      " [ 0.83055729]\n",
      " [-0.61935693]\n",
      " [ 0.68444135]]\n",
      "t [[-0.94586477]\n",
      " [-0.24346004]\n",
      " [ 0.10733134]\n",
      " ...\n",
      " [ 1.01274673]\n",
      " [-0.75929619]\n",
      " [ 0.84092853]]\n",
      "t [[-0.94586477]\n",
      " [-0.24346004]\n",
      " [ 0.10733134]\n",
      " ...\n",
      " [ 1.01274673]\n",
      " [-0.75929619]\n",
      " [ 0.84092853]]\n",
      "Current iteration=4, loss=32927.890413265886\n",
      "t [[-1.10389662]\n",
      " [-0.29328468]\n",
      " [ 0.13389957]\n",
      " ...\n",
      " [ 1.16337066]\n",
      " [-0.87654747]\n",
      " [ 0.97311933]]\n",
      "t [[-1.10389662]\n",
      " [-0.29328468]\n",
      " [ 0.13389957]\n",
      " ...\n",
      " [ 1.16337066]\n",
      " [-0.87654747]\n",
      " [ 0.97311933]]\n",
      "t [[-1.24224933]\n",
      " [-0.33851075]\n",
      " [ 0.15792749]\n",
      " ...\n",
      " [ 1.28880169]\n",
      " [-0.97553923]\n",
      " [ 1.08574306]]\n",
      "t [[-1.24224933]\n",
      " [-0.33851075]\n",
      " [ 0.15792749]\n",
      " ...\n",
      " [ 1.28880169]\n",
      " [-0.97553923]\n",
      " [ 1.08574306]]\n",
      "Current iteration=6, loss=31991.49252320533\n",
      "t [[-1.36443487]\n",
      " [-0.37960731]\n",
      " [ 0.17934545]\n",
      " ...\n",
      " [ 1.39392846]\n",
      " [-1.05971318]\n",
      " [ 1.18245279]]\n",
      "t [[-1.36443487]\n",
      " [-0.37960731]\n",
      " [ 0.17934545]\n",
      " ...\n",
      " [ 1.39392846]\n",
      " [-1.05971318]\n",
      " [ 1.18245279]]\n",
      "t [[-1.47316639]\n",
      " [-0.4170479 ]\n",
      " [ 0.19827692]\n",
      " ...\n",
      " [ 1.48252753]\n",
      " [-1.13175349]\n",
      " [ 1.26608818]]\n",
      "t [[-1.47316639]\n",
      " [-0.4170479 ]\n",
      " [ 0.19827692]\n",
      " ...\n",
      " [ 1.48252753]\n",
      " [-1.13175349]\n",
      " [ 1.26608818]]\n",
      "Current iteration=8, loss=31405.266767202716\n",
      "t [[-1.57057086]\n",
      " [-0.45126726]\n",
      " [ 0.21493184]\n",
      " ...\n",
      " [ 1.55754644]\n",
      " [-1.1937702 ]\n",
      " [ 1.33887838]]\n",
      "t [[-1.57057086]\n",
      " [-0.45126726]\n",
      " [ 0.21493184]\n",
      " ...\n",
      " [ 1.55754644]\n",
      " [-1.1937702 ]\n",
      " [ 1.33887838]]\n",
      "t [[-1.65833995]\n",
      " [-0.48264847]\n",
      " [ 0.2295503 ]\n",
      " ...\n",
      " [ 1.62131174]\n",
      " [-1.24743827]\n",
      " [ 1.40259259]]\n",
      "loss=31009.55767910525\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.2657606 ]\n",
      " [-0.05590759]\n",
      " [ 0.05548847]\n",
      " ...\n",
      " [-0.07013991]\n",
      " [-0.08720674]\n",
      " [-0.01833282]]\n",
      "t [[-0.2657606 ]\n",
      " [-0.05590759]\n",
      " [ 0.05548847]\n",
      " ...\n",
      " [-0.07013991]\n",
      " [-0.08720674]\n",
      " [-0.01833282]]\n",
      "t [[-0.47947325]\n",
      " [-0.08912208]\n",
      " [ 0.1045458 ]\n",
      " ...\n",
      " [-0.13129399]\n",
      " [-0.16109776]\n",
      " [-0.03646001]]\n",
      "t [[-0.47947325]\n",
      " [-0.08912208]\n",
      " [ 0.1045458 ]\n",
      " ...\n",
      " [-0.13129399]\n",
      " [-0.16109776]\n",
      " [-0.03646001]]\n",
      "Current iteration=2, loss=34436.11740088768\n",
      "t [[-0.65060987]\n",
      " [-0.10971893]\n",
      " [ 0.1461453 ]\n",
      " ...\n",
      " [-0.1848327 ]\n",
      " [-0.22419533]\n",
      " [-0.05366977]]\n",
      "t [[-0.65060987]\n",
      " [-0.10971893]\n",
      " [ 0.1461453 ]\n",
      " ...\n",
      " [-0.1848327 ]\n",
      " [-0.22419533]\n",
      " [-0.05366977]]\n",
      "t [[-0.78770989]\n",
      " [-0.12358166]\n",
      " [ 0.18071531]\n",
      " ...\n",
      " [-0.23196693]\n",
      " [-0.2785952 ]\n",
      " [-0.06970099]]\n",
      "t [[-0.78770989]\n",
      " [-0.12358166]\n",
      " [ 0.18071531]\n",
      " ...\n",
      " [-0.23196693]\n",
      " [-0.2785952 ]\n",
      " [-0.06970099]]\n",
      "Current iteration=4, loss=32775.17499324278\n",
      "t [[-0.89776981]\n",
      " [-0.13402586]\n",
      " [ 0.20920087]\n",
      " ...\n",
      " [-0.27370405]\n",
      " [-0.32595079]\n",
      " [-0.08451115]]\n",
      "t [[-0.89776981]\n",
      " [-0.13402586]\n",
      " [ 0.20920087]\n",
      " ...\n",
      " [-0.27370405]\n",
      " [-0.32595079]\n",
      " [-0.08451115]]\n",
      "t [[-0.98629347]\n",
      " [-0.14290398]\n",
      " [ 0.23261187]\n",
      " ...\n",
      " [-0.31086568]\n",
      " [-0.36754553]\n",
      " [-0.09814941]]\n",
      "t [[-0.98629347]\n",
      " [-0.14290398]\n",
      " [ 0.23261187]\n",
      " ...\n",
      " [-0.31086568]\n",
      " [-0.36754553]\n",
      " [-0.09814941]]\n",
      "Current iteration=6, loss=31841.820305607867\n",
      "t [[-1.05755155]\n",
      " [-0.15124891]\n",
      " [ 0.25185315]\n",
      " ...\n",
      " [-0.34411993]\n",
      " [-0.40437616]\n",
      " [-0.11069698]]\n",
      "t [[-1.05755155]\n",
      " [-0.15124891]\n",
      " [ 0.25185315]\n",
      " ...\n",
      " [-0.34411993]\n",
      " [-0.40437616]\n",
      " [-0.11069698]]\n",
      "t [[-1.11485679]\n",
      " [-0.15963142]\n",
      " [ 0.26768217]\n",
      " ...\n",
      " [-0.37401245]\n",
      " [-0.437223  ]\n",
      " [-0.12224147]]\n",
      "t [[-1.11485679]\n",
      " [-0.15963142]\n",
      " [ 0.26768217]\n",
      " ...\n",
      " [-0.37401245]\n",
      " [-0.437223  ]\n",
      " [-0.12224147]]\n",
      "Current iteration=8, loss=31264.928285728434\n",
      "t [[-1.16079445]\n",
      " [-0.16835786]\n",
      " [ 0.28071544]\n",
      " ...\n",
      " [-0.40099201]\n",
      " [-0.46670362]\n",
      " [-0.13286691]]\n",
      "t [[-1.16079445]\n",
      " [-0.16835786]\n",
      " [ 0.28071544]\n",
      " ...\n",
      " [-0.40099201]\n",
      " [-0.46670362]\n",
      " [-0.13286691]]\n",
      "t [[-1.19740006]\n",
      " [-0.17758101]\n",
      " [ 0.29144966]\n",
      " ...\n",
      " [-0.42543077]\n",
      " [-0.49331269]\n",
      " [-0.14265036]]\n",
      "loss=30879.02309082505\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.31211382]\n",
      " [-0.07031528]\n",
      " [ 0.02746609]\n",
      " ...\n",
      " [-0.06742203]\n",
      " [-0.08766016]\n",
      " [-0.01819513]]\n",
      "t [[-0.31211382]\n",
      " [-0.07031528]\n",
      " [ 0.02746609]\n",
      " ...\n",
      " [-0.06742203]\n",
      " [-0.08766016]\n",
      " [-0.01819513]]\n",
      "t [[-0.57148871]\n",
      " [-0.13836042]\n",
      " [ 0.06002083]\n",
      " ...\n",
      " [-0.12615813]\n",
      " [-0.16189181]\n",
      " [-0.03644533]]\n",
      "t [[-0.57148871]\n",
      " [-0.13836042]\n",
      " [ 0.06002083]\n",
      " ...\n",
      " [-0.12615813]\n",
      " [-0.16189181]\n",
      " [-0.03644533]]\n",
      "Current iteration=2, loss=34410.47302775032\n",
      "t [[-0.78963135]\n",
      " [-0.20152853]\n",
      " [ 0.09292495]\n",
      " ...\n",
      " [-0.17759703]\n",
      " [-0.22525064]\n",
      " [-0.05388788]]\n",
      "t [[-0.78963135]\n",
      " [-0.20152853]\n",
      " [ 0.09292495]\n",
      " ...\n",
      " [-0.17759703]\n",
      " [-0.22525064]\n",
      " [-0.05388788]]\n",
      "t [[-0.97547862]\n",
      " [-0.25908117]\n",
      " [ 0.12384989]\n",
      " ...\n",
      " [-0.22293384]\n",
      " [-0.27985797]\n",
      " [-0.07018485]]\n",
      "t [[-0.97547862]\n",
      " [-0.25908117]\n",
      " [ 0.12384989]\n",
      " ...\n",
      " [-0.22293384]\n",
      " [-0.27985797]\n",
      " [-0.07018485]]\n",
      "Current iteration=4, loss=32740.817807744323\n",
      "t [[-1.13573335]\n",
      " [-0.31116163]\n",
      " [ 0.15184749]\n",
      " ...\n",
      " [-0.26314694]\n",
      " [-0.32738522]\n",
      " [-0.08525707]]\n",
      "t [[-1.13573335]\n",
      " [-0.31116163]\n",
      " [ 0.15184749]\n",
      " ...\n",
      " [-0.26314694]\n",
      " [-0.32738522]\n",
      " [-0.08525707]]\n",
      "t [[-1.27540593]\n",
      " [-0.35824914]\n",
      " [ 0.17668232]\n",
      " ...\n",
      " [-0.29902578]\n",
      " [-0.36912817]\n",
      " [-0.09913728]]\n",
      "t [[-1.27540593]\n",
      " [-0.35824914]\n",
      " [ 0.17668232]\n",
      " ...\n",
      " [-0.29902578]\n",
      " [-0.36912817]\n",
      " [-0.09913728]]\n",
      "Current iteration=6, loss=31802.423409673494\n",
      "t [[-1.39827902]\n",
      " [-0.40090519]\n",
      " [ 0.19846104]\n",
      " ...\n",
      " [-0.33120768]\n",
      " [-0.40609197]\n",
      " [-0.11190021]]\n",
      "t [[-1.39827902]\n",
      " [-0.40090519]\n",
      " [ 0.19846104]\n",
      " ...\n",
      " [-0.33120768]\n",
      " [-0.40609197]\n",
      " [-0.11190021]]\n",
      "t [[-1.50725084]\n",
      " [-0.43966942]\n",
      " [ 0.21743936]\n",
      " ...\n",
      " [-0.36021041]\n",
      " [-0.43906267]\n",
      " [-0.1236318 ]]\n",
      "t [[-1.50725084]\n",
      " [-0.43966942]\n",
      " [ 0.21743936]\n",
      " ...\n",
      " [-0.36021041]\n",
      " [-0.43906267]\n",
      " [-0.1236318 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=8, loss=31221.061751940924\n",
      "t [[-1.60457745]\n",
      " [-0.47502384]\n",
      " [ 0.23392447]\n",
      " ...\n",
      " [-0.38645823]\n",
      " [-0.4686618 ]\n",
      " [-0.13441654]]\n",
      "t [[-1.60457745]\n",
      " [-0.47502384]\n",
      " [ 0.23392447]\n",
      " ...\n",
      " [-0.38645823]\n",
      " [-0.4686618 ]\n",
      " [-0.13441654]]\n",
      "t [[-1.69204226]\n",
      " [-0.50738578]\n",
      " [ 0.24822734]\n",
      " ...\n",
      " [-0.410302  ]\n",
      " [-0.49538671]\n",
      " [-0.14433301]]\n",
      "loss=30830.62857502242\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.31291209]\n",
      " [-0.07252275]\n",
      " [ 0.01884752]\n",
      " ...\n",
      " [-0.06664451]\n",
      " [-0.08817506]\n",
      " [-0.01783421]]\n",
      "t [[-0.31291209]\n",
      " [-0.07252275]\n",
      " [ 0.01884752]\n",
      " ...\n",
      " [-0.06664451]\n",
      " [-0.08817506]\n",
      " [-0.01783421]]\n",
      "t [[-0.57300581]\n",
      " [-0.14209969]\n",
      " [ 0.04556713]\n",
      " ...\n",
      " [-0.12509904]\n",
      " [-0.16284762]\n",
      " [-0.03575597]]\n",
      "t [[-0.57300581]\n",
      " [-0.14209969]\n",
      " [ 0.04556713]\n",
      " ...\n",
      " [-0.12509904]\n",
      " [-0.16284762]\n",
      " [-0.03575597]]\n",
      "Current iteration=2, loss=34367.48977774509\n",
      "t [[-0.79175803]\n",
      " [-0.20644537]\n",
      " [ 0.07466221]\n",
      " ...\n",
      " [-0.17656785]\n",
      " [-0.22659153]\n",
      " [-0.05294665]]\n",
      "t [[-0.79175803]\n",
      " [-0.20644537]\n",
      " [ 0.07466221]\n",
      " ...\n",
      " [-0.17656785]\n",
      " [-0.22659153]\n",
      " [-0.05294665]]\n",
      "t [[-0.97811632]\n",
      " [-0.26498013]\n",
      " [ 0.10320533]\n",
      " ...\n",
      " [-0.22212279]\n",
      " [-0.28154204]\n",
      " [-0.06907417]]\n",
      "t [[-0.97811632]\n",
      " [-0.26498013]\n",
      " [ 0.10320533]\n",
      " ...\n",
      " [-0.22212279]\n",
      " [-0.28154204]\n",
      " [-0.06907417]]\n",
      "Current iteration=4, loss=32676.89657727475\n",
      "t [[-1.13880126]\n",
      " [-0.31792721]\n",
      " [ 0.1298173 ]\n",
      " ...\n",
      " [-0.26266321]\n",
      " [-0.3293801 ]\n",
      " [-0.08405063]]\n",
      "t [[-1.13880126]\n",
      " [-0.31792721]\n",
      " [ 0.1298173 ]\n",
      " ...\n",
      " [-0.26266321]\n",
      " [-0.3293801 ]\n",
      " [-0.08405063]]\n",
      "t [[-1.27883859]\n",
      " [-0.36580798]\n",
      " [ 0.15396371]\n",
      " ...\n",
      " [-0.29892878]\n",
      " [-0.3714078 ]\n",
      " [-0.09789631]]\n",
      "t [[-1.27883859]\n",
      " [-0.36580798]\n",
      " [ 0.15396371]\n",
      " ...\n",
      " [-0.29892878]\n",
      " [-0.3714078 ]\n",
      " [-0.09789631]]\n",
      "Current iteration=6, loss=31727.316834104142\n",
      "t [[-1.40202247]\n",
      " [-0.40920748]\n",
      " [ 0.175546  ]\n",
      " ...\n",
      " [-0.3315256 ]\n",
      " [-0.40863452]\n",
      " [-0.11067392]]\n",
      "t [[-1.40202247]\n",
      " [-0.40920748]\n",
      " [ 0.175546  ]\n",
      " ...\n",
      " [-0.3315256 ]\n",
      " [-0.40863452]\n",
      " [-0.11067392]]\n",
      "t [[-1.51125934]\n",
      " [-0.44867951]\n",
      " [ 0.19467854]\n",
      " ...\n",
      " [-0.3609519 ]\n",
      " [-0.44184922]\n",
      " [-0.12245914]]\n",
      "t [[-1.51125934]\n",
      " [-0.44867951]\n",
      " [ 0.19467854]\n",
      " ...\n",
      " [-0.3609519 ]\n",
      " [-0.44184922]\n",
      " [-0.12245914]]\n",
      "Current iteration=8, loss=31139.609456874772\n",
      "t [[-1.6088112 ]\n",
      " [-0.48471508]\n",
      " [ 0.21157037]\n",
      " ...\n",
      " [-0.38761979]\n",
      " [-0.47167556]\n",
      " [-0.13332824]]\n",
      "t [[-1.6088112 ]\n",
      " [-0.48471508]\n",
      " [ 0.21157037]\n",
      " ...\n",
      " [-0.38761979]\n",
      " [-0.47167556]\n",
      " [-0.13332824]]\n",
      "t [[-1.69646576]\n",
      " [-0.51773759]\n",
      " [ 0.22646335]\n",
      " ...\n",
      " [-0.41187265]\n",
      " [-0.49861248]\n",
      " [-0.14335324]]\n",
      "loss=30745.519834064853\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.31044399]\n",
      " [-0.06752283]\n",
      " [ 0.02210064]\n",
      " ...\n",
      " [ 0.34795046]\n",
      " [-0.25619737]\n",
      " [ 0.28222829]]\n",
      "t [[-0.31044399]\n",
      " [-0.06752283]\n",
      " [ 0.02210064]\n",
      " ...\n",
      " [ 0.34795046]\n",
      " [-0.25619737]\n",
      " [ 0.28222829]]\n",
      "t [[-0.56874205]\n",
      " [-0.13351722]\n",
      " [ 0.0511565 ]\n",
      " ...\n",
      " [ 0.62791331]\n",
      " [-0.46557688]\n",
      " [ 0.51357995]]\n",
      "t [[-0.56874205]\n",
      " [-0.13351722]\n",
      " [ 0.0511565 ]\n",
      " ...\n",
      " [ 0.62791331]\n",
      " [-0.46557688]\n",
      " [ 0.51357995]]\n",
      "Current iteration=2, loss=34479.52677378854\n",
      "t [[-0.78626555]\n",
      " [-0.19506392]\n",
      " [ 0.08184426]\n",
      " ...\n",
      " [ 0.85425127]\n",
      " [-0.63731599]\n",
      " [ 0.70439299]]\n",
      "t [[-0.78626555]\n",
      " [-0.19506392]\n",
      " [ 0.08184426]\n",
      " ...\n",
      " [ 0.85425127]\n",
      " [-0.63731599]\n",
      " [ 0.70439299]]\n",
      "t [[-0.97182385]\n",
      " [-0.25125312]\n",
      " [ 0.11141228]\n",
      " ...\n",
      " [ 1.03865077]\n",
      " [-0.77919355]\n",
      " [ 0.86319208]]\n",
      "t [[-0.97182385]\n",
      " [-0.25125312]\n",
      " [ 0.11141228]\n",
      " ...\n",
      " [ 1.03865077]\n",
      " [-0.77919355]\n",
      " [ 0.86319208]]\n",
      "Current iteration=4, loss=32836.831003294225\n",
      "t [[-1.13201887]\n",
      " [-0.30213719]\n",
      " [ 0.13862279]\n",
      " ...\n",
      " [ 1.19012738]\n",
      " [-0.89737523]\n",
      " [ 0.99661989]]\n",
      "t [[-1.13201887]\n",
      " [-0.30213719]\n",
      " [ 0.13862279]\n",
      " ...\n",
      " [ 1.19012738]\n",
      " [-0.89737523]\n",
      " [ 0.99661989]]\n",
      "t [[-1.27178687]\n",
      " [-0.34814697]\n",
      " [ 0.16304697]\n",
      " ...\n",
      " [ 1.31552361]\n",
      " [-0.99662747]\n",
      " [ 1.10975941]]\n",
      "t [[-1.27178687]\n",
      " [-0.34814697]\n",
      " [ 0.16304697]\n",
      " ...\n",
      " [ 1.31552361]\n",
      " [-0.99662747]\n",
      " [ 1.10975941]]\n",
      "Current iteration=6, loss=31910.456316204072\n",
      "t [[-1.39485763]\n",
      " [-0.38981751]\n",
      " [ 0.18466343]\n",
      " ...\n",
      " [ 1.42003728]\n",
      " [-1.08061786]\n",
      " [ 1.20649927]]\n",
      "t [[-1.39485763]\n",
      " [-0.38981751]\n",
      " [ 0.18466343]\n",
      " ...\n",
      " [ 1.42003728]\n",
      " [-1.08061786]\n",
      " [ 1.20649927]]\n",
      "t [[-1.50409203]\n",
      " [-0.42767323]\n",
      " [ 0.20364305]\n",
      " ...\n",
      " [ 1.50765066]\n",
      " [-1.15218248]\n",
      " [ 1.28983779]]\n",
      "t [[-1.50409203]\n",
      " [-0.42767323]\n",
      " [ 0.20364305]\n",
      " ...\n",
      " [ 1.50765066]\n",
      " [-1.15218248]\n",
      " [ 1.28983779]]\n",
      "Current iteration=8, loss=31335.51155768476\n",
      "t [[-1.60171983]\n",
      " [-0.46218673]\n",
      " [ 0.22023674]\n",
      " ...\n",
      " [ 1.58145013]\n",
      " [-1.21353477]\n",
      " [ 1.36211261]]\n",
      "t [[-1.60171983]\n",
      " [-0.46218673]\n",
      " [ 0.22023674]\n",
      " ...\n",
      " [ 1.58145013]\n",
      " [-1.21353477]\n",
      " [ 1.36211261]]\n",
      "t [[-1.68950586]\n",
      " [-0.49376904]\n",
      " [ 0.23471783]\n",
      " ...\n",
      " [ 1.64385715]\n",
      " [-1.26642098]\n",
      " [ 1.42516821]]\n",
      "loss=30949.200198129656\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.27492476]\n",
      " [-0.05783544]\n",
      " [ 0.05740186]\n",
      " ...\n",
      " [-0.07255853]\n",
      " [-0.09021387]\n",
      " [-0.01896498]]\n",
      "t [[-0.27492476]\n",
      " [-0.05783544]\n",
      " [ 0.05740186]\n",
      " ...\n",
      " [-0.07255853]\n",
      " [-0.09021387]\n",
      " [-0.01896498]]\n",
      "t [[-0.49416875]\n",
      " [-0.09139509]\n",
      " [ 0.1079222 ]\n",
      " ...\n",
      " [-0.13550492]\n",
      " [-0.1661829 ]\n",
      " [-0.03771008]]\n",
      "t [[-0.49416875]\n",
      " [-0.09139509]\n",
      " [ 0.1079222 ]\n",
      " ...\n",
      " [-0.13550492]\n",
      " [-0.1661829 ]\n",
      " [-0.03771008]]\n",
      "Current iteration=2, loss=34352.05740508206\n",
      "t [[-0.66823441]\n",
      " [-0.11182816]\n",
      " [ 0.15043046]\n",
      " ...\n",
      " [-0.19036018]\n",
      " [-0.23070348]\n",
      " [-0.05544902]]\n",
      "t [[-0.66823441]\n",
      " [-0.11182816]\n",
      " [ 0.15043046]\n",
      " ...\n",
      " [-0.19036018]\n",
      " [-0.23070348]\n",
      " [-0.05544902]]\n",
      "t [[-0.80651826]\n",
      " [-0.12547248]\n",
      " [ 0.18545667]\n",
      " ...\n",
      " [-0.2384548 ]\n",
      " [-0.28607399]\n",
      " [-0.07191052]]\n",
      "t [[-0.80651826]\n",
      " [-0.12547248]\n",
      " [ 0.18545667]\n",
      " ...\n",
      " [-0.2384548 ]\n",
      " [-0.28607399]\n",
      " [-0.07191052]]\n",
      "Current iteration=4, loss=32686.643690383884\n",
      "t [[-0.91662794]\n",
      " [-0.13580446]\n",
      " [ 0.21407999]\n",
      " ...\n",
      " [-0.2808848 ]\n",
      " [-0.33408665]\n",
      " [-0.08706279]]\n",
      "t [[-0.91662794]\n",
      " [-0.13580446]\n",
      " [ 0.21407999]\n",
      " ...\n",
      " [-0.2808848 ]\n",
      " [-0.33408665]\n",
      " [-0.08706279]]\n",
      "t [[-1.00447708]\n",
      " [-0.14471662]\n",
      " [ 0.23742004]\n",
      " ...\n",
      " [-0.31853651]\n",
      " [-0.37611814]\n",
      " [-0.10096912]]\n",
      "t [[-1.00447708]\n",
      " [-0.14471662]\n",
      " [ 0.23742004]\n",
      " ...\n",
      " [-0.31853651]\n",
      " [-0.37611814]\n",
      " [-0.10096912]]\n",
      "Current iteration=6, loss=31764.23032924602\n",
      "t [[-1.07460592]\n",
      " [-0.15323677]\n",
      " [ 0.25645959]\n",
      " ...\n",
      " [-0.35212556]\n",
      " [-0.4132283 ]\n",
      " [-0.11372317]]\n",
      "t [[-1.07460592]\n",
      " [-0.15323677]\n",
      " [ 0.25645959]\n",
      " ...\n",
      " [-0.35212556]\n",
      " [-0.4132283 ]\n",
      " [-0.11372317]]\n",
      "t [[-1.13050443]\n",
      " [-0.16191593]\n",
      " [ 0.27200847]\n",
      " ...\n",
      " [-0.38223289]\n",
      " [-0.44624076]\n",
      " [-0.12542255]]\n",
      "t [[-1.13050443]\n",
      " [-0.16191593]\n",
      " [ 0.27200847]\n",
      " ...\n",
      " [-0.38223289]\n",
      " [-0.44624076]\n",
      " [-0.12542255]]\n",
      "Current iteration=8, loss=31198.86120670448\n",
      "t [[-1.17487538]\n",
      " [-0.1710378 ]\n",
      " [ 0.28471761]\n",
      " ...\n",
      " [-0.40933384]\n",
      " [-0.47580346]\n",
      " [-0.13615917]]\n",
      "t [[-1.17487538]\n",
      " [-0.1710378 ]\n",
      " [ 0.28471761]\n",
      " ...\n",
      " [-0.40933384]\n",
      " [-0.47580346]\n",
      " [-0.13615917]]\n",
      "t [[-1.20983298]\n",
      " [-0.18073394]\n",
      " [ 0.29510622]\n",
      " ...\n",
      " [-0.43382076]\n",
      " [-0.50243275]\n",
      " [-0.14601643]]\n",
      "loss=30822.298936422885\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.32287636]\n",
      " [-0.07273994]\n",
      " [ 0.02841319]\n",
      " ...\n",
      " [-0.06974693]\n",
      " [-0.09068293]\n",
      " [-0.01882255]]\n",
      "t [[-0.32287636]\n",
      " [-0.07273994]\n",
      " [ 0.02841319]\n",
      " ...\n",
      " [-0.06974693]\n",
      " [-0.09068293]\n",
      " [-0.01882255]]\n",
      "t [[-0.58933692]\n",
      " [-0.14304869]\n",
      " [ 0.06226879]\n",
      " ...\n",
      " [-0.13020247]\n",
      " [-0.16700039]\n",
      " [-0.03770407]]\n",
      "t [[-0.58933692]\n",
      " [-0.14304869]\n",
      " [ 0.06226879]\n",
      " ...\n",
      " [-0.13020247]\n",
      " [-0.16700039]\n",
      " [-0.03770407]]\n",
      "Current iteration=2, loss=34325.889812596346\n",
      "t [[-0.81213529]\n",
      " [-0.20804576]\n",
      " [ 0.09632551]\n",
      " ...\n",
      " [-0.18290755]\n",
      " [-0.23178578]\n",
      " [-0.05569134]]\n",
      "t [[-0.81213529]\n",
      " [-0.20804576]\n",
      " [ 0.09632551]\n",
      " ...\n",
      " [-0.18290755]\n",
      " [-0.23178578]\n",
      " [-0.05569134]]\n",
      "t [[-1.00104087]\n",
      " [-0.26698922]\n",
      " [ 0.12810424]\n",
      " ...\n",
      " [-0.22917409]\n",
      " [-0.28736538]\n",
      " [-0.07243105]]\n",
      "t [[-1.00104087]\n",
      " [-0.26698922]\n",
      " [ 0.12810424]\n",
      " ...\n",
      " [-0.22917409]\n",
      " [-0.28736538]\n",
      " [-0.07243105]]\n",
      "Current iteration=4, loss=32651.848561744388\n",
      "t [[-1.16328603]\n",
      " [-0.32010049]\n",
      " [ 0.1566569 ]\n",
      " ...\n",
      " [-0.27006524]\n",
      " [-0.33555069]\n",
      " [-0.08785377]]\n",
      "t [[-1.16328603]\n",
      " [-0.32010049]\n",
      " [ 0.1566569 ]\n",
      " ...\n",
      " [-0.27006524]\n",
      " [-0.33555069]\n",
      " [-0.08785377]]\n",
      "t [[-1.30421891]\n",
      " [-0.36794256]\n",
      " [ 0.18179742]\n",
      " ...\n",
      " [-0.3064316 ]\n",
      " [-0.37773147]\n",
      " [-0.10200683]]\n",
      "t [[-1.30421891]\n",
      " [-0.36794256]\n",
      " [ 0.18179742]\n",
      " ...\n",
      " [-0.3064316 ]\n",
      " [-0.37773147]\n",
      " [-0.10200683]]\n",
      "Current iteration=6, loss=31724.35621740172\n",
      "t [[-1.42784227]\n",
      " [-0.41114536]\n",
      " [ 0.20369012]\n",
      " ...\n",
      " [-0.33895501]\n",
      " [-0.41497626]\n",
      " [-0.1149781 ]]\n",
      "t [[-1.42784227]\n",
      " [-0.41114536]\n",
      " [ 0.20369012]\n",
      " ...\n",
      " [-0.33895501]\n",
      " [-0.41497626]\n",
      " [-0.1149781 ]]\n",
      "t [[-1.53720167]\n",
      " [-0.45029996]\n",
      " [ 0.22264247]\n",
      " ...\n",
      " [-0.36818615]\n",
      " [-0.44811459]\n",
      " [-0.12686413]]\n",
      "t [[-1.53720167]\n",
      " [-0.45029996]\n",
      " [ 0.22264247]\n",
      " ...\n",
      " [-0.36818615]\n",
      " [-0.44811459]\n",
      " [-0.12686413]]\n",
      "Current iteration=8, loss=31154.378422935304\n",
      "t [[-1.63465448]\n",
      " [-0.48592597]\n",
      " [ 0.23900407]\n",
      " ...\n",
      " [-0.39457399]\n",
      " [-0.47779839]\n",
      " [-0.13775782]]\n",
      "t [[-1.63465448]\n",
      " [-0.48592597]\n",
      " [ 0.23900407]\n",
      " ...\n",
      " [-0.39457399]\n",
      " [-0.47779839]\n",
      " [-0.13775782]]\n",
      "t [[-1.72205549]\n",
      " [-0.51846816]\n",
      " [ 0.25311905]\n",
      " ...\n",
      " [-0.41848814]\n",
      " [-0.50454668]\n",
      " [-0.14774447]]\n",
      "loss=30773.11558120353\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.32370216]\n",
      " [-0.07502353]\n",
      " [ 0.01949743]\n",
      " ...\n",
      " [-0.06894259]\n",
      " [-0.09121558]\n",
      " [-0.01844918]]\n",
      "t [[-0.32370216]\n",
      " [-0.07502353]\n",
      " [ 0.01949743]\n",
      " ...\n",
      " [-0.06894259]\n",
      " [-0.09121558]\n",
      " [-0.01844918]]\n",
      "t [[-0.59090393]\n",
      " [-0.14689288]\n",
      " [ 0.0474145 ]\n",
      " ...\n",
      " [-0.12912432]\n",
      " [-0.16798663]\n",
      " [-0.03699206]]\n",
      "t [[-0.59090393]\n",
      " [-0.14689288]\n",
      " [ 0.0474145 ]\n",
      " ...\n",
      " [-0.12912432]\n",
      " [-0.16798663]\n",
      " [-0.03699206]]\n",
      "Current iteration=2, loss=34281.818083372375\n",
      "t [[-0.81432628]\n",
      " [-0.21308332]\n",
      " [ 0.07767085]\n",
      " ...\n",
      " [-0.18188188]\n",
      " [-0.23316669]\n",
      " [-0.05472412]]\n",
      "t [[-0.81432628]\n",
      " [-0.21308332]\n",
      " [ 0.07767085]\n",
      " ...\n",
      " [-0.18188188]\n",
      " [-0.23316669]\n",
      " [-0.05472412]]\n",
      "t [[-1.00375138]\n",
      " [-0.27302253]\n",
      " [ 0.10713264]\n",
      " ...\n",
      " [-0.22839341]\n",
      " [-0.28909722]\n",
      " [-0.07129709]]\n",
      "t [[-1.00375138]\n",
      " [-0.27302253]\n",
      " [ 0.10713264]\n",
      " ...\n",
      " [-0.22839341]\n",
      " [-0.28909722]\n",
      " [-0.07129709]]\n",
      "Current iteration=4, loss=32586.824821078397\n",
      "t [[-1.1664314 ]\n",
      " [-0.3270146 ]\n",
      " [ 0.13438857]\n",
      " ...\n",
      " [-0.26963807]\n",
      " [-0.33759984]\n",
      " [-0.08663094]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.1664314 ]\n",
      " [-0.3270146 ]\n",
      " [ 0.13438857]\n",
      " ...\n",
      " [-0.26963807]\n",
      " [-0.33759984]\n",
      " [-0.08663094]]\n",
      "t [[-1.30773107]\n",
      " [-0.37566499]\n",
      " [ 0.1589367 ]\n",
      " ...\n",
      " [-0.30641461]\n",
      " [-0.38007089]\n",
      " [-0.10075884]]\n",
      "t [[-1.30773107]\n",
      " [-0.37566499]\n",
      " [ 0.1589367 ]\n",
      " ...\n",
      " [-0.30641461]\n",
      " [-0.38007089]\n",
      " [-0.10075884]]\n",
      "Current iteration=6, loss=31648.33772440016\n",
      "t [[-1.43166526]\n",
      " [-0.41962702]\n",
      " [ 0.18072783]\n",
      " ...\n",
      " [-0.33937299]\n",
      " [-0.4175833 ]\n",
      " [-0.11375547]]\n",
      "t [[-1.43166526]\n",
      " [-0.41962702]\n",
      " [ 0.18072783]\n",
      " ...\n",
      " [-0.33937299]\n",
      " [-0.4175833 ]\n",
      " [-0.11375547]]\n",
      "t [[-1.54128812]\n",
      " [-0.45950566]\n",
      " [ 0.19992447]\n",
      " ...\n",
      " [-0.36904434]\n",
      " [-0.45096959]\n",
      " [-0.1257064 ]]\n",
      "t [[-1.54128812]\n",
      " [-0.45950566]\n",
      " [ 0.19992447]\n",
      " ...\n",
      " [-0.36904434]\n",
      " [-0.45096959]\n",
      " [-0.1257064 ]]\n",
      "Current iteration=8, loss=31072.229381154866\n",
      "t [[-1.63896308]\n",
      " [-0.49582937]\n",
      " [ 0.21677669]\n",
      " ...\n",
      " [-0.39586572]\n",
      " [-0.48088386]\n",
      " [-0.13669584]]\n",
      "t [[-1.63896308]\n",
      " [-0.49582937]\n",
      " [ 0.21677669]\n",
      " ...\n",
      " [-0.39586572]\n",
      " [-0.48088386]\n",
      " [-0.13669584]]\n",
      "t [[-1.72654933]\n",
      " [-0.52904894]\n",
      " [ 0.23155933]\n",
      " ...\n",
      " [-0.4201996 ]\n",
      " [-0.50784678]\n",
      " [-0.14680228]]\n",
      "loss=30687.510049592718\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.32114895]\n",
      " [-0.0698512 ]\n",
      " [ 0.02286273]\n",
      " ...\n",
      " [ 0.35994875]\n",
      " [-0.26503176]\n",
      " [ 0.2919603 ]]\n",
      "t [[-0.32114895]\n",
      " [-0.0698512 ]\n",
      " [ 0.02286273]\n",
      " ...\n",
      " [ 0.35994875]\n",
      " [-0.26503176]\n",
      " [ 0.2919603 ]]\n",
      "t [[-0.58651632]\n",
      " [-0.13806438]\n",
      " [ 0.05316423]\n",
      " ...\n",
      " [ 0.64716816]\n",
      " [-0.4799793 ]\n",
      " [ 0.52949592]]\n",
      "t [[-0.58651632]\n",
      " [-0.13806438]\n",
      " [ 0.05316423]\n",
      " ...\n",
      " [ 0.64716816]\n",
      " [-0.4799793 ]\n",
      " [ 0.52949592]]\n",
      "Current iteration=2, loss=34396.526455118394\n",
      "t [[-0.80870566]\n",
      " [-0.20141315]\n",
      " [ 0.08501602]\n",
      " ...\n",
      " [ 0.87757783]\n",
      " [-0.6550167 ]\n",
      " [ 0.72406709]]\n",
      "t [[-0.80870566]\n",
      " [-0.20141315]\n",
      " [ 0.08501602]\n",
      " ...\n",
      " [ 0.87757783]\n",
      " [-0.6550167 ]\n",
      " [ 0.72406709]]\n",
      "t [[-0.99734597]\n",
      " [-0.25897199]\n",
      " [ 0.11547836]\n",
      " ...\n",
      " [ 1.06398027]\n",
      " [-0.7986813 ]\n",
      " [ 0.88501755]]\n",
      "t [[-0.99734597]\n",
      " [-0.25897199]\n",
      " [ 0.11547836]\n",
      " ...\n",
      " [ 1.06398027]\n",
      " [-0.7986813 ]\n",
      " [ 0.88501755]]\n",
      "Current iteration=4, loss=32749.152311159545\n",
      "t [[-1.15956041]\n",
      " [-0.31086795]\n",
      " [ 0.14329362]\n",
      " ...\n",
      " [ 1.21612844]\n",
      " [-0.91765869]\n",
      " [ 1.01953892]]\n",
      "t [[-1.15956041]\n",
      " [-0.31086795]\n",
      " [ 0.14329362]\n",
      " ...\n",
      " [ 1.21612844]\n",
      " [-0.91765869]\n",
      " [ 1.01953892]]\n",
      "t [[-1.30061822]\n",
      " [-0.35761519]\n",
      " [ 0.16807274]\n",
      " ...\n",
      " [ 1.34134114]\n",
      " [-1.01705905]\n",
      " [ 1.13307332]]\n",
      "t [[-1.30061822]\n",
      " [-0.35761519]\n",
      " [ 0.16807274]\n",
      " ...\n",
      " [ 1.34134114]\n",
      " [-1.01705905]\n",
      " [ 1.13307332]]\n",
      "Current iteration=6, loss=31833.319358220804\n",
      "t [[-1.42446678]\n",
      " [-0.39981745]\n",
      " [ 0.18984786]\n",
      " ...\n",
      " [ 1.44512477]\n",
      " [-1.10077605]\n",
      " [ 1.22974503]]\n",
      "t [[-1.42446678]\n",
      " [-0.39981745]\n",
      " [ 0.18984786]\n",
      " ...\n",
      " [ 1.44512477]\n",
      " [-1.10077605]\n",
      " [ 1.22974503]]\n",
      "t [[-1.53411316]\n",
      " [-0.43805088]\n",
      " [ 0.20884035]\n",
      " ...\n",
      " [ 1.53166411]\n",
      " [-1.1717957 ]\n",
      " [ 1.31270904]]\n",
      "t [[-1.53411316]\n",
      " [-0.43805088]\n",
      " [ 0.20884035]\n",
      " ...\n",
      " [ 1.53166411]\n",
      " [-1.1717957 ]\n",
      " [ 1.31270904]]\n",
      "Current iteration=8, loss=31269.545748769342\n",
      "t [[-1.63188794]\n",
      " [-0.47282587]\n",
      " [ 0.22534345]\n",
      " ...\n",
      " [ 1.60418044]\n",
      " [-1.23243221]\n",
      " [ 1.38440885]]\n",
      "t [[-1.63188794]\n",
      " [-0.47282587]\n",
      " [ 0.22534345]\n",
      " ...\n",
      " [ 1.60418044]\n",
      " [-1.23243221]\n",
      " [ 1.38440885]]\n",
      "t [[-1.71962723]\n",
      " [-0.50458094]\n",
      " [ 0.23966425]\n",
      " ...\n",
      " [ 1.66518645]\n",
      " [-1.28450045]\n",
      " [ 1.44676169]]\n",
      "loss=30892.30634625644\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.28408892]\n",
      " [-0.05976329]\n",
      " [ 0.05931526]\n",
      " ...\n",
      " [-0.07497715]\n",
      " [-0.093221  ]\n",
      " [-0.01959715]]\n",
      "t [[-0.28408892]\n",
      " [-0.05976329]\n",
      " [ 0.05931526]\n",
      " ...\n",
      " [-0.07497715]\n",
      " [-0.093221  ]\n",
      " [-0.01959715]]\n",
      "t [[-0.50874364]\n",
      " [-0.09361572]\n",
      " [ 0.11128344]\n",
      " ...\n",
      " [-0.13969518]\n",
      " [-0.17123723]\n",
      " [-0.03895969]]\n",
      "t [[-0.50874364]\n",
      " [-0.09361572]\n",
      " [ 0.11128344]\n",
      " ...\n",
      " [-0.13969518]\n",
      " [-0.17123723]\n",
      " [-0.03895969]]\n",
      "Current iteration=2, loss=34269.61319413807\n",
      "t [[-0.68556911]\n",
      " [-0.11385261]\n",
      " [ 0.15466391]\n",
      " ...\n",
      " [-0.19583624]\n",
      " [-0.23713829]\n",
      " [-0.05722202]]\n",
      "t [[-0.68556911]\n",
      " [-0.11385261]\n",
      " [ 0.15466391]\n",
      " ...\n",
      " [-0.19583624]\n",
      " [-0.23713829]\n",
      " [-0.05722202]]\n",
      "t [[-0.82486778]\n",
      " [-0.12727337]\n",
      " [ 0.19010235]\n",
      " ...\n",
      " [-0.24485687]\n",
      " [-0.29343563]\n",
      " [-0.07410427]]\n",
      "t [[-0.82486778]\n",
      " [-0.12727337]\n",
      " [ 0.19010235]\n",
      " ...\n",
      " [-0.24485687]\n",
      " [-0.29343563]\n",
      " [-0.07410427]]\n",
      "Current iteration=4, loss=32601.397606345465\n",
      "t [[-0.93488072]\n",
      " [-0.13750655]\n",
      " [ 0.21882242]\n",
      " ...\n",
      " [-0.28794517]\n",
      " [-0.34206476]\n",
      " [-0.08958732]]\n",
      "t [[-0.93488072]\n",
      " [-0.13750655]\n",
      " [ 0.21882242]\n",
      " ...\n",
      " [-0.28794517]\n",
      " [-0.34206476]\n",
      " [-0.08958732]]\n",
      "t [[-1.0219379 ]\n",
      " [-0.14647603]\n",
      " [ 0.24205794]\n",
      " ...\n",
      " [-0.32605399]\n",
      " [-0.3844972 ]\n",
      " [-0.10374976]]\n",
      "t [[-1.0219379 ]\n",
      " [-0.14647603]\n",
      " [ 0.24205794]\n",
      " ...\n",
      " [-0.32605399]\n",
      " [-0.3844972 ]\n",
      " [-0.10374976]]\n",
      "Current iteration=6, loss=31690.353191048205\n",
      "t [[-1.09084851]\n",
      " [-0.15519859]\n",
      " [ 0.26087043]\n",
      " ...\n",
      " [-0.3599474 ]\n",
      " [-0.42185605]\n",
      " [-0.11669827]]\n",
      "t [[-1.09084851]\n",
      " [-0.15519859]\n",
      " [ 0.26087043]\n",
      " ...\n",
      " [-0.3599474 ]\n",
      " [-0.42185605]\n",
      " [-0.11669827]]\n",
      "t [[-1.14527659]\n",
      " [-0.16420192]\n",
      " [ 0.27612134]\n",
      " ...\n",
      " [-0.39024195]\n",
      " [-0.45500794]\n",
      " [-0.1285407 ]]\n",
      "t [[-1.14527659]\n",
      " [-0.16420192]\n",
      " [ 0.27612134]\n",
      " ...\n",
      " [-0.39024195]\n",
      " [-0.45500794]\n",
      " [-0.1285407 ]]\n",
      "Current iteration=8, loss=31136.348931523375\n",
      "t [[-1.18803863]\n",
      " [-0.17374501]\n",
      " [ 0.28849486]\n",
      " ...\n",
      " [-0.41743961]\n",
      " [-0.48463068]\n",
      " [-0.13937696]]\n",
      "t [[-1.18803863]\n",
      " [-0.17374501]\n",
      " [ 0.28849486]\n",
      " ...\n",
      " [-0.41743961]\n",
      " [-0.48463068]\n",
      " [-0.13937696]]\n",
      "t [[-1.22132394]\n",
      " [-0.18393703]\n",
      " [ 0.29853151]\n",
      " ...\n",
      " [-0.44195286]\n",
      " [-0.51126178]\n",
      " [-0.14929689]]\n",
      "loss=30768.78754865573\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.33363891]\n",
      " [-0.07516461]\n",
      " [ 0.0293603 ]\n",
      " ...\n",
      " [-0.07207182]\n",
      " [-0.09370569]\n",
      " [-0.01944997]]\n",
      "t [[-0.33363891]\n",
      " [-0.07516461]\n",
      " [ 0.0293603 ]\n",
      " ...\n",
      " [-0.07207182]\n",
      " [-0.09370569]\n",
      " [-0.01944997]]\n",
      "t [[-0.60706362]\n",
      " [-0.14773126]\n",
      " [ 0.06452831]\n",
      " ...\n",
      " [-0.13422682]\n",
      " [-0.1720779 ]\n",
      " [-0.03896295]]\n",
      "t [[-0.60706362]\n",
      " [-0.14773126]\n",
      " [ 0.06452831]\n",
      " ...\n",
      " [-0.13422682]\n",
      " [-0.1720779 ]\n",
      " [-0.03896295]]\n",
      "Current iteration=2, loss=34242.939849459784\n",
      "t [[-0.83436097]\n",
      " [-0.21452808]\n",
      " [ 0.09972729]\n",
      " ...\n",
      " [-0.18816878]\n",
      " [-0.23824706]\n",
      " [-0.05748928]]\n",
      "t [[-0.83436097]\n",
      " [-0.21452808]\n",
      " [ 0.09972729]\n",
      " ...\n",
      " [-0.18816878]\n",
      " [-0.23824706]\n",
      " [-0.05748928]]\n",
      "t [[-1.02617095]\n",
      " [-0.27481931]\n",
      " [ 0.1323304 ]\n",
      " ...\n",
      " [-0.23533255]\n",
      " [-0.29475491]\n",
      " [-0.07466187]]\n",
      "t [[-1.02617095]\n",
      " [-0.27481931]\n",
      " [ 0.1323304 ]\n",
      " ...\n",
      " [-0.23533255]\n",
      " [-0.29475491]\n",
      " [-0.07466187]]\n",
      "Current iteration=4, loss=32566.181936494606\n",
      "t [[-1.19026882]\n",
      " [-0.32891463]\n",
      " [ 0.16139936]\n",
      " ...\n",
      " [-0.27686923]\n",
      " [-0.34355762]\n",
      " [-0.09042325]]\n",
      "t [[-1.19026882]\n",
      " [-0.32891463]\n",
      " [ 0.16139936]\n",
      " ...\n",
      " [-0.27686923]\n",
      " [-0.34355762]\n",
      " [-0.09042325]]\n",
      "t [[-1.33234349]\n",
      " [-0.37746648]\n",
      " [ 0.18680521]\n",
      " ...\n",
      " [-0.31369214]\n",
      " [-0.38614043]\n",
      " [-0.10483661]]\n",
      "t [[-1.33234349]\n",
      " [-0.37746648]\n",
      " [ 0.18680521]\n",
      " ...\n",
      " [-0.31369214]\n",
      " [-0.38614043]\n",
      " [-0.10483661]]\n",
      "Current iteration=6, loss=31650.00845896236\n",
      "t [[-1.45661704]\n",
      " [-0.42117554]\n",
      " [ 0.20877459]\n",
      " ...\n",
      " [-0.34652842]\n",
      " [-0.42363545]\n",
      " [-0.11800363]]\n",
      "t [[-1.45661704]\n",
      " [-0.42117554]\n",
      " [ 0.20877459]\n",
      " ...\n",
      " [-0.34652842]\n",
      " [-0.42363545]\n",
      " [-0.11800363]]\n",
      "t [[-1.56628043]\n",
      " [-0.46068483]\n",
      " [ 0.22766916]\n",
      " ...\n",
      " [-0.37596197]\n",
      " [-0.45691535]\n",
      " [-0.13003173]]\n",
      "t [[-1.56628043]\n",
      " [-0.46068483]\n",
      " [ 0.22766916]\n",
      " ...\n",
      " [-0.37596197]\n",
      " [-0.45691535]\n",
      " [-0.13003173]]\n",
      "Current iteration=8, loss=31091.24901235034\n",
      "t [[-1.66379021]\n",
      " [-0.49655133]\n",
      " [ 0.24388187]\n",
      " ...\n",
      " [-0.40246648]\n",
      " [-0.48666193]\n",
      " [-0.14102238]]\n",
      "t [[-1.66379021]\n",
      " [-0.49655133]\n",
      " [ 0.24388187]\n",
      " ...\n",
      " [-0.40246648]\n",
      " [-0.48666193]\n",
      " [-0.14102238]]\n",
      "t [[-1.75107052]\n",
      " [-0.52924681]\n",
      " [ 0.25779013]\n",
      " ...\n",
      " [-0.42643027]\n",
      " [-0.51341535]\n",
      " [-0.15106765]]\n",
      "loss=30718.812681680556\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.33449223]\n",
      " [-0.07752432]\n",
      " [ 0.02014734]\n",
      " ...\n",
      " [-0.07124068]\n",
      " [-0.09425609]\n",
      " [-0.01906415]]\n",
      "t [[-0.33449223]\n",
      " [-0.07752432]\n",
      " [ 0.02014734]\n",
      " ...\n",
      " [-0.07124068]\n",
      " [-0.09425609]\n",
      " [-0.01906415]]\n",
      "t [[-0.60868043]\n",
      " [-0.15167877]\n",
      " [ 0.04927979]\n",
      " ...\n",
      " [-0.13313075]\n",
      " [-0.1730944 ]\n",
      " [-0.03822837]]\n",
      "t [[-0.60868043]\n",
      " [-0.15167877]\n",
      " [ 0.04927979]\n",
      " ...\n",
      " [-0.13313075]\n",
      " [-0.1730944 ]\n",
      " [-0.03822837]]\n",
      "Current iteration=2, loss=34197.8014819187\n",
      "t [[-0.83661573]\n",
      " [-0.21968394]\n",
      " [ 0.08069422]\n",
      " ...\n",
      " [-0.18714871]\n",
      " [-0.23966766]\n",
      " [-0.05649658]]\n",
      "t [[-0.83661573]\n",
      " [-0.21968394]\n",
      " [ 0.08069422]\n",
      " ...\n",
      " [-0.18714871]\n",
      " [-0.23966766]\n",
      " [-0.05649658]]\n",
      "t [[-1.02895326]\n",
      " [-0.28098437]\n",
      " [ 0.1110506 ]\n",
      " ...\n",
      " [-0.23458472]\n",
      " [-0.29653403]\n",
      " [-0.07350575]]\n",
      "t [[-1.02895326]\n",
      " [-0.28098437]\n",
      " [ 0.1110506 ]\n",
      " ...\n",
      " [-0.23458472]\n",
      " [-0.29653403]\n",
      " [-0.07350575]]\n",
      "Current iteration=4, loss=32500.09903283257\n",
      "t [[-1.19349019]\n",
      " [-0.33597479]\n",
      " [ 0.13891479]\n",
      " ...\n",
      " [-0.276501  ]\n",
      " [-0.3456604 ]\n",
      " [-0.08918566]]\n",
      "t [[-1.19349019]\n",
      " [-0.33597479]\n",
      " [ 0.13891479]\n",
      " ...\n",
      " [-0.276501  ]\n",
      " [-0.3456604 ]\n",
      " [-0.08918566]]\n",
      "t [[-1.33593326]\n",
      " [-0.38535021]\n",
      " [ 0.16382544]\n",
      " ...\n",
      " [-0.3137571 ]\n",
      " [-0.38853886]\n",
      " [-0.10358364]]\n",
      "t [[-1.33593326]\n",
      " [-0.38535021]\n",
      " [ 0.16382544]\n",
      " ...\n",
      " [-0.3137571 ]\n",
      " [-0.38853886]\n",
      " [-0.10358364]]\n",
      "Current iteration=6, loss=31573.128075004362\n",
      "t [[-1.4605173 ]\n",
      " [-0.42983444]\n",
      " [ 0.18578796]\n",
      " ...\n",
      " [-0.34704774]\n",
      " [-0.42630604]\n",
      " [-0.11678696]]\n",
      "t [[-1.4605173 ]\n",
      " [-0.42983444]\n",
      " [ 0.18578796]\n",
      " ...\n",
      " [-0.34704774]\n",
      " [-0.42630604]\n",
      " [-0.11678696]]\n",
      "t [[-1.57044214]\n",
      " [-0.47008415]\n",
      " [ 0.20501585]\n",
      " ...\n",
      " [-0.37693736]\n",
      " [-0.45983767]\n",
      " [-0.12889136]]\n",
      "t [[-1.57044214]\n",
      " [-0.47008415]\n",
      " [ 0.20501585]\n",
      " ...\n",
      " [-0.37693736]\n",
      " [-0.45983767]\n",
      " [-0.12889136]]\n",
      "Current iteration=8, loss=31008.453356376944\n",
      "t [[-1.66817055]\n",
      " [-0.50666502]\n",
      " [ 0.22180154]\n",
      " ...\n",
      " [-0.40388806]\n",
      " [-0.48981779]\n",
      " [-0.13998918]]\n",
      "t [[-1.66817055]\n",
      " [-0.50666502]\n",
      " [ 0.22180154]\n",
      " ...\n",
      " [-0.40388806]\n",
      " [-0.48981779]\n",
      " [-0.13998918]]\n",
      "t [[-1.75563115]\n",
      " [-0.54005475]\n",
      " [ 0.2364532 ]\n",
      " ...\n",
      " [-0.4282814 ]\n",
      " [-0.51678826]\n",
      " [-0.15016547]]\n",
      "loss=30632.757526103553\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.33185392]\n",
      " [-0.07217958]\n",
      " [ 0.02362482]\n",
      " ...\n",
      " [ 0.37194704]\n",
      " [-0.27386615]\n",
      " [ 0.30169231]]\n",
      "t [[-0.33185392]\n",
      " [-0.07217958]\n",
      " [ 0.02362482]\n",
      " ...\n",
      " [ 0.37194704]\n",
      " [-0.27386615]\n",
      " [ 0.30169231]]\n",
      "t [[-0.60417043]\n",
      " [-0.14260752]\n",
      " [ 0.05518776]\n",
      " ...\n",
      " [ 0.66626607]\n",
      " [-0.49427347]\n",
      " [ 0.54529449]]\n",
      "t [[-0.60417043]\n",
      " [-0.14260752]\n",
      " [ 0.05518776]\n",
      " ...\n",
      " [ 0.66626607]\n",
      " [-0.49427347]\n",
      " [ 0.54529449]]\n",
      "Current iteration=2, loss=34315.11660149104\n",
      "t [[-0.83087051]\n",
      " [-0.20773022]\n",
      " [ 0.0881974 ]\n",
      " ...\n",
      " [ 0.90054144]\n",
      " [-0.67246196]\n",
      " [ 0.74346692]]\n",
      "t [[-0.83087051]\n",
      " [-0.20773022]\n",
      " [ 0.0881974 ]\n",
      " ...\n",
      " [ 0.90054144]\n",
      " [-0.67246196]\n",
      " [ 0.74346692]]\n",
      "t [[-1.02244019]\n",
      " [-0.26661611]\n",
      " [ 0.11952726]\n",
      " ...\n",
      " [ 1.08874721]\n",
      " [-0.81776747]\n",
      " [ 0.90641375]]\n",
      "t [[-1.02244019]\n",
      " [-0.26661611]\n",
      " [ 0.11952726]\n",
      " ...\n",
      " [ 1.08874721]\n",
      " [-0.81776747]\n",
      " [ 0.90641375]]\n",
      "Current iteration=4, loss=32664.708001978892\n",
      "t [[-1.18653696]\n",
      " [-0.31947749]\n",
      " [ 0.14790958]\n",
      " ...\n",
      " [ 1.2413952 ]\n",
      " [-0.93741254]\n",
      " [ 1.04189221]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.18653696]\n",
      " [-0.31947749]\n",
      " [ 0.14790958]\n",
      " ...\n",
      " [ 1.2413952 ]\n",
      " [-0.93741254]\n",
      " [ 1.04189221]]\n",
      "t [[-1.32876612]\n",
      " [-0.36691772]\n",
      " [ 0.17300316]\n",
      " ...\n",
      " [ 1.3662857 ]\n",
      " [-1.03685594]\n",
      " [ 1.15570816]]\n",
      "t [[-1.32876612]\n",
      " [-0.36691772]\n",
      " [ 0.17300316]\n",
      " ...\n",
      " [ 1.3662857 ]\n",
      " [-1.03685594]\n",
      " [ 1.15570816]]\n",
      "Current iteration=6, loss=31759.842414458522\n",
      "t [[-1.45329202]\n",
      " [-0.40961165]\n",
      " [ 0.19489879]\n",
      " ...\n",
      " [ 1.46923221]\n",
      " [-1.12021695]\n",
      " [ 1.25222095]]\n",
      "t [[-1.45329202]\n",
      " [-0.40961165]\n",
      " [ 0.19489879]\n",
      " ...\n",
      " [ 1.46923221]\n",
      " [-1.12021695]\n",
      " [ 1.25222095]]\n",
      "t [[-1.56326607]\n",
      " [-0.4481877 ]\n",
      " [ 0.21387102]\n",
      " ...\n",
      " [ 1.55461833]\n",
      " [-1.19062917]\n",
      " [ 1.33473989]]\n",
      "t [[-1.56326607]\n",
      " [-0.4481877 ]\n",
      " [ 0.21387102]\n",
      " ...\n",
      " [ 1.55461833]\n",
      " [-1.19062917]\n",
      " [ 1.33473989]]\n",
      "Current iteration=8, loss=31207.090067912046\n",
      "t [[-1.66111766]\n",
      " [-0.4831938 ]\n",
      " [ 0.2302567 ]\n",
      " ...\n",
      " [ 1.62579619]\n",
      " [-1.25050472]\n",
      " [ 1.40581159]]\n",
      "t [[-1.66111766]\n",
      " [-0.4831938 ]\n",
      " [ 0.2302567 ]\n",
      " ...\n",
      " [ 1.62579619]\n",
      " [-1.25050472]\n",
      " [ 1.40581159]]\n",
      "t [[-1.74875223]\n",
      " [-0.51509548]\n",
      " [ 0.24439689]\n",
      " ...\n",
      " [ 1.68536584]\n",
      " [-1.30172444]\n",
      " [ 1.46742336]]\n",
      "loss=30838.59209526967\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.29325308]\n",
      " [-0.06169114]\n",
      " [ 0.06122865]\n",
      " ...\n",
      " [-0.07739577]\n",
      " [-0.09622813]\n",
      " [-0.02022931]]\n",
      "t [[-0.29325308]\n",
      " [-0.06169114]\n",
      " [ 0.06122865]\n",
      " ...\n",
      " [-0.07739577]\n",
      " [-0.09622813]\n",
      " [-0.02022931]]\n",
      "t [[-0.52319817]\n",
      " [-0.09578409]\n",
      " [ 0.11462954]\n",
      " ...\n",
      " [-0.14386483]\n",
      " [-0.17626082]\n",
      " [-0.04020883]]\n",
      "t [[-0.52319817]\n",
      " [-0.09578409]\n",
      " [ 0.11462954]\n",
      " ...\n",
      " [-0.14386483]\n",
      " [-0.17626082]\n",
      " [-0.04020883]]\n",
      "Current iteration=2, loss=34188.75686429255\n",
      "t [[-0.70261701]\n",
      " [-0.11579473]\n",
      " [ 0.15884557]\n",
      " ...\n",
      " [-0.20126139]\n",
      " [-0.24350057]\n",
      " [-0.05898864]]\n",
      "t [[-0.70261701]\n",
      " [-0.11579473]\n",
      " [ 0.15884557]\n",
      " ...\n",
      " [-0.20126139]\n",
      " [-0.24350057]\n",
      " [-0.05898864]]\n",
      "t [[-0.84276723]\n",
      " [-0.12898942]\n",
      " [ 0.19465309]\n",
      " ...\n",
      " [-0.25117444]\n",
      " [-0.30068225]\n",
      " [-0.07628211]]\n",
      "t [[-0.84276723]\n",
      " [-0.12898942]\n",
      " [ 0.19465309]\n",
      " ...\n",
      " [-0.25117444]\n",
      " [-0.30068225]\n",
      " [-0.07628211]]\n",
      "Current iteration=4, loss=32519.29460609055\n",
      "t [[-0.95254446]\n",
      " [-0.13913898]\n",
      " [ 0.22343068]\n",
      " ...\n",
      " [-0.29488756]\n",
      " [-0.349889  ]\n",
      " [-0.09208477]]\n",
      "t [[-0.95254446]\n",
      " [-0.13913898]\n",
      " [ 0.22343068]\n",
      " ...\n",
      " [-0.29488756]\n",
      " [-0.349889  ]\n",
      " [-0.09208477]]\n",
      "t [[-1.03870044]\n",
      " [-0.14818976]\n",
      " [ 0.24653031]\n",
      " ...\n",
      " [-0.33342187]\n",
      " [-0.39268846]\n",
      " [-0.10649166]]\n",
      "t [[-1.03870044]\n",
      " [-0.14818976]\n",
      " [ 0.24653031]\n",
      " ...\n",
      " [-0.33342187]\n",
      " [-0.39268846]\n",
      " [-0.10649166]]\n",
      "Current iteration=6, loss=31619.962456595862\n",
      "t [[-1.10631188]\n",
      " [-0.15714178]\n",
      " [ 0.26509272]\n",
      " ...\n",
      " [-0.36759059]\n",
      " [-0.43026705]\n",
      " [-0.11962297]]\n",
      "t [[-1.10631188]\n",
      " [-0.15714178]\n",
      " [ 0.26509272]\n",
      " ...\n",
      " [-0.36759059]\n",
      " [-0.43026705]\n",
      " [-0.11962297]]\n",
      "t [[-1.15921326]\n",
      " [-0.166496  ]\n",
      " [ 0.28003001]\n",
      " ...\n",
      " [-0.39804625]\n",
      " [-0.46353402]\n",
      " [-0.131597  ]]\n",
      "t [[-1.15921326]\n",
      " [-0.166496  ]\n",
      " [ 0.28003001]\n",
      " ...\n",
      " [-0.39804625]\n",
      " [-0.46353402]\n",
      " [-0.131597  ]]\n",
      "Current iteration=8, loss=31077.13205929153\n",
      "t [[-1.20033081]\n",
      " [-0.17648484]\n",
      " [ 0.29205833]\n",
      " ...\n",
      " [-0.42531745]\n",
      " [-0.49319648]\n",
      " [-0.14252181]]\n",
      "t [[-1.20033081]\n",
      " [-0.17648484]\n",
      " [ 0.29205833]\n",
      " ...\n",
      " [-0.42531745]\n",
      " [-0.49319648]\n",
      " [-0.14252181]]\n",
      "t [[-1.23192532]\n",
      " [-0.18719413]\n",
      " [ 0.30173827]\n",
      " ...\n",
      " [-0.44983672]\n",
      " [-0.51981255]\n",
      " [-0.1524937 ]]\n",
      "loss=30718.228778614437\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.34440145]\n",
      " [-0.07758927]\n",
      " [ 0.03030741]\n",
      " ...\n",
      " [-0.07439672]\n",
      " [-0.09672846]\n",
      " [-0.02007739]]\n",
      "t [[-0.34440145]\n",
      " [-0.07758927]\n",
      " [ 0.03030741]\n",
      " ...\n",
      " [-0.07439672]\n",
      " [-0.09672846]\n",
      " [-0.02007739]]\n",
      "t [[-0.62466912]\n",
      " [-0.15240811]\n",
      " [ 0.06679934]\n",
      " ...\n",
      " [-0.13823122]\n",
      " [-0.17712442]\n",
      " [-0.04022196]]\n",
      "t [[-0.62466912]\n",
      " [-0.15240811]\n",
      " [ 0.06679934]\n",
      " ...\n",
      " [-0.13823122]\n",
      " [-0.17712442]\n",
      " [-0.04022196]]\n",
      "Current iteration=2, loss=34161.59467779266\n",
      "t [[-0.85631192]\n",
      " [-0.22097499]\n",
      " [ 0.10312923]\n",
      " ...\n",
      " [-0.19338122]\n",
      " [-0.24463528]\n",
      " [-0.05928152]]\n",
      "t [[-0.85631192]\n",
      " [-0.22097499]\n",
      " [ 0.10312923]\n",
      " ...\n",
      " [-0.19338122]\n",
      " [-0.24463528]\n",
      " [-0.05928152]]\n",
      "t [[-1.05087785]\n",
      " [-0.28257112]\n",
      " [ 0.13652657]\n",
      " ...\n",
      " [-0.24141052]\n",
      " [-0.30202875]\n",
      " [-0.07687711]]\n",
      "t [[-1.05087785]\n",
      " [-0.28257112]\n",
      " [ 0.13652657]\n",
      " ...\n",
      " [-0.24141052]\n",
      " [-0.30202875]\n",
      " [-0.07687711]]\n",
      "Current iteration=4, loss=32483.674198318524\n",
      "t [[-1.21669717]\n",
      " [-0.33760489]\n",
      " [ 0.16607326]\n",
      " ...\n",
      " [-0.28356125]\n",
      " [-0.35140994]\n",
      " [-0.09296545]]\n",
      "t [[-1.21669717]\n",
      " [-0.33760489]\n",
      " [ 0.16607326]\n",
      " ...\n",
      " [-0.28356125]\n",
      " [-0.35140994]\n",
      " [-0.09296545]]\n",
      "t [[-1.3598018 ]\n",
      " [-0.38682352]\n",
      " [ 0.19170517]\n",
      " ...\n",
      " [-0.32081098]\n",
      " [-0.39436086]\n",
      " [-0.10762688]]\n",
      "t [[-1.3598018 ]\n",
      " [-0.38682352]\n",
      " [ 0.19170517]\n",
      " ...\n",
      " [-0.32081098]\n",
      " [-0.39436086]\n",
      " [-0.10762688]]\n",
      "Current iteration=6, loss=31579.15232163601\n",
      "t [[-1.48463199]\n",
      " [-0.43100044]\n",
      " [ 0.21371571]\n",
      " ...\n",
      " [-0.35393284]\n",
      " [-0.43207725]\n",
      " [-0.12097743]]\n",
      "t [[-1.48463199]\n",
      " [-0.43100044]\n",
      " [ 0.21371571]\n",
      " ...\n",
      " [-0.35393284]\n",
      " [-0.43207725]\n",
      " [-0.12097743]]\n",
      "t [[-1.59452195]\n",
      " [-0.47083092]\n",
      " [ 0.2325229 ]\n",
      " ...\n",
      " [-0.38354416]\n",
      " [-0.46547448]\n",
      " [-0.13313569]]\n",
      "t [[-1.59452195]\n",
      " [-0.47083092]\n",
      " [ 0.2325229 ]\n",
      " ...\n",
      " [-0.38354416]\n",
      " [-0.46547448]\n",
      " [-0.13313569]]\n",
      "Current iteration=8, loss=31031.413551240847\n",
      "t [[-1.69202515]\n",
      " [-0.50690895]\n",
      " [ 0.24856374]\n",
      " ...\n",
      " [-0.4101434 ]\n",
      " [-0.49526367]\n",
      " [-0.14421175]]\n",
      "t [[-1.69202515]\n",
      " [-0.50690895]\n",
      " [ 0.24856374]\n",
      " ...\n",
      " [-0.4101434 ]\n",
      " [-0.49526367]\n",
      " [-0.14421175]]\n",
      "t [[-1.77913307]\n",
      " [-0.53973275]\n",
      " [ 0.26224889]\n",
      " ...\n",
      " [-0.43413747]\n",
      " [-0.52200556]\n",
      " [-0.15430457]]\n",
      "loss=30667.459959401815\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.3452823 ]\n",
      " [-0.0800251 ]\n",
      " [ 0.02079726]\n",
      " ...\n",
      " [-0.07353877]\n",
      " [-0.09729661]\n",
      " [-0.01967913]]\n",
      "t [[-0.3452823 ]\n",
      " [-0.0800251 ]\n",
      " [ 0.02079726]\n",
      " ...\n",
      " [-0.07353877]\n",
      " [-0.09729661]\n",
      " [-0.01967913]]\n",
      "t [[-0.62633561]\n",
      " [-0.15645735]\n",
      " [ 0.05116293]\n",
      " ...\n",
      " [-0.13711838]\n",
      " [-0.17817103]\n",
      " [-0.03946488]]\n",
      "t [[-0.62633561]\n",
      " [-0.15645735]\n",
      " [ 0.05116293]\n",
      " ...\n",
      " [-0.13711838]\n",
      " [-0.17817103]\n",
      " [-0.03946488]]\n",
      "Current iteration=2, loss=34115.41110796033\n",
      "t [[-0.85862993]\n",
      " [-0.22624679]\n",
      " [ 0.08373102]\n",
      " ...\n",
      " [-0.19236876]\n",
      " [-0.24609525]\n",
      " [-0.05826388]]\n",
      "t [[-0.85862993]\n",
      " [-0.22624679]\n",
      " [ 0.08373102]\n",
      " ...\n",
      " [-0.19236876]\n",
      " [-0.24609525]\n",
      " [-0.05826388]]\n",
      "t [[-1.05373094]\n",
      " [-0.28886548]\n",
      " [ 0.11495683]\n",
      " ...\n",
      " [-0.2406979 ]\n",
      " [-0.30385467]\n",
      " [-0.07569993]]\n",
      "t [[-1.05373094]\n",
      " [-0.28886548]\n",
      " [ 0.11495683]\n",
      " ...\n",
      " [-0.2406979 ]\n",
      " [-0.30385467]\n",
      " [-0.07569993]]\n",
      "Current iteration=4, loss=32416.57357540247\n",
      "t [[-1.21999309]\n",
      " [-0.34480878]\n",
      " [ 0.14339341]\n",
      " ...\n",
      " [-0.28325417]\n",
      " [-0.35356572]\n",
      " [-0.09171473]]\n",
      "t [[-1.21999309]\n",
      " [-0.34480878]\n",
      " [ 0.14339341]\n",
      " ...\n",
      " [-0.28325417]\n",
      " [-0.35356572]\n",
      " [-0.09171473]]\n",
      "t [[-1.36346734]\n",
      " [-0.39486644]\n",
      " [ 0.16862816]\n",
      " ...\n",
      " [-0.32095961]\n",
      " [-0.39681753]\n",
      " [-0.10637091]]\n",
      "t [[-1.36346734]\n",
      " [-0.39486644]\n",
      " [ 0.16862816]\n",
      " ...\n",
      " [-0.32095961]\n",
      " [-0.39681753]\n",
      " [-0.10637091]]\n",
      "Current iteration=6, loss=31501.457206490584\n",
      "t [[-1.4886073 ]\n",
      " [-0.43983464]\n",
      " [ 0.19072615]\n",
      " ...\n",
      " [-0.35455455]\n",
      " [-0.43481047]\n",
      " [-0.11976893]]\n",
      "t [[-1.4886073 ]\n",
      " [-0.43983464]\n",
      " [ 0.19072615]\n",
      " ...\n",
      " [-0.35455455]\n",
      " [-0.43481047]\n",
      " [-0.11976893]]\n",
      "t [[-1.59875631]\n",
      " [-0.48042203]\n",
      " [ 0.2099545 ]\n",
      " ...\n",
      " [-0.38463704]\n",
      " [-0.46846304]\n",
      " [-0.13201499]]\n",
      "t [[-1.59875631]\n",
      " [-0.48042203]\n",
      " [ 0.2099545 ]\n",
      " ...\n",
      " [-0.38463704]\n",
      " [-0.46846304]\n",
      " [-0.13201499]]\n",
      "Current iteration=8, loss=30948.01807155579\n",
      "t [[-1.6964742 ]\n",
      " [-0.5172312 ]\n",
      " [ 0.22664909]\n",
      " ...\n",
      " [-0.41169432]\n",
      " [-0.49848866]\n",
      " [-0.14320965]]\n",
      "t [[-1.6964742 ]\n",
      " [-0.5172312 ]\n",
      " [ 0.22664909]\n",
      " ...\n",
      " [-0.41169432]\n",
      " [-0.49848866]\n",
      " [-0.14320965]]\n",
      "t [[-1.78375703]\n",
      " [-0.55076619]\n",
      " [ 0.24115154]\n",
      " ...\n",
      " [-0.43612695]\n",
      " [-0.52544983]\n",
      " [-0.15344466]]\n",
      "loss=30580.998784924857\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.34255888]\n",
      " [-0.07450795]\n",
      " [ 0.02438691]\n",
      " ...\n",
      " [ 0.38394533]\n",
      " [-0.28270054]\n",
      " [ 0.31142432]]\n",
      "t [[-0.34255888]\n",
      " [-0.07450795]\n",
      " [ 0.02438691]\n",
      " ...\n",
      " [ 0.38394533]\n",
      " [-0.28270054]\n",
      " [ 0.31142432]]\n",
      "t [[-0.62170468]\n",
      " [-0.1471466 ]\n",
      " [ 0.05722702]\n",
      " ...\n",
      " [ 0.68520741]\n",
      " [-0.50845962]\n",
      " [ 0.56097596]]\n",
      "t [[-0.62170468]\n",
      " [-0.1471466 ]\n",
      " [ 0.05722702]\n",
      " ...\n",
      " [ 0.68520741]\n",
      " [-0.50845962]\n",
      " [ 0.56097596]]\n",
      "Current iteration=2, loss=34235.26969139117\n",
      "t [[-0.85276359]\n",
      " [-0.21401451]\n",
      " [ 0.09138715]\n",
      " ...\n",
      " [ 0.92314653]\n",
      " [-0.68965465]\n",
      " [ 0.76259574]]\n",
      "t [[-0.85276359]\n",
      " [-0.21401451]\n",
      " [ 0.09138715]\n",
      " ...\n",
      " [ 0.92314653]\n",
      " [-0.68965465]\n",
      " [ 0.76259574]]\n",
      "t [[-1.04711535]\n",
      " [-0.27418501]\n",
      " [ 0.12355679]\n",
      " ...\n",
      " [ 1.11296331]\n",
      " [-0.83645996]\n",
      " [ 0.92738931]]\n",
      "t [[-1.04711535]\n",
      " [-0.27418501]\n",
      " [ 0.12355679]\n",
      " ...\n",
      " [ 1.11296331]\n",
      " [-0.83645996]\n",
      " [ 0.92738931]]\n",
      "Current iteration=4, loss=32583.358323076623\n",
      "t [[-1.21296373]\n",
      " [-0.32796645]\n",
      " [ 0.15246843]\n",
      " ...\n",
      " [ 1.2659484 ]\n",
      " [-0.95665107]\n",
      " [ 1.06369513]]\n",
      "t [[-1.21296373]\n",
      " [-0.32796645]\n",
      " [ 0.15246843]\n",
      " ...\n",
      " [ 1.2659484 ]\n",
      " [-0.95665107]\n",
      " [ 1.06369513]]\n",
      "t [[-1.35625239]\n",
      " [-0.37605698]\n",
      " [ 0.17783692]\n",
      " ...\n",
      " [ 1.39038749]\n",
      " [-1.05603929]\n",
      " [ 1.17768642]]\n",
      "t [[-1.35625239]\n",
      " [-0.37605698]\n",
      " [ 0.17783692]\n",
      " ...\n",
      " [ 1.39038749]\n",
      " [-1.05603929]\n",
      " [ 1.17768642]]\n",
      "Current iteration=6, loss=31689.802733789864\n",
      "t [[-1.48136164]\n",
      " [-0.41920463]\n",
      " [ 0.19981656]\n",
      " ...\n",
      " [ 1.49239898]\n",
      " [-1.13896846]\n",
      " [ 1.27395652]]\n",
      "t [[-1.48136164]\n",
      " [-0.41920463]\n",
      " [ 0.19981656]\n",
      " ...\n",
      " [ 1.49239898]\n",
      " [-1.13896846]\n",
      " [ 1.27395652]]\n",
      "t [[-1.59158517]\n",
      " [-0.45809041]\n",
      " [ 0.21873761]\n",
      " ...\n",
      " [ 1.57656118]\n",
      " [-1.20871706]\n",
      " [ 1.35596638]]\n",
      "t [[-1.59158517]\n",
      " [-0.45809041]\n",
      " [ 0.21873761]\n",
      " ...\n",
      " [ 1.57656118]\n",
      " [-1.20871706]\n",
      " [ 1.35596638]]\n",
      "Current iteration=8, loss=31147.88918645685\n",
      "t [[-1.68944906]\n",
      " [-0.49329939]\n",
      " [ 0.23498143]\n",
      " ...\n",
      " [ 1.64635281]\n",
      " [-1.26779218]\n",
      " [ 1.42636281]]\n",
      "t [[-1.68944906]\n",
      " [-0.49329939]\n",
      " [ 0.23498143]\n",
      " ...\n",
      " [ 1.64635281]\n",
      " [-1.26779218]\n",
      " [ 1.42636281]]\n",
      "t [[-1.77692617]\n",
      " [-0.5253235 ]\n",
      " [ 0.24892316]\n",
      " ...\n",
      " [ 1.70445742]\n",
      " [-1.31813783]\n",
      " [ 1.48720052]]\n",
      "loss=30787.801250778426\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.30241724]\n",
      " [-0.06361899]\n",
      " [ 0.06314205]\n",
      " ...\n",
      " [-0.07981439]\n",
      " [-0.09923526]\n",
      " [-0.02086148]]\n",
      "t [[-0.30241724]\n",
      " [-0.06361899]\n",
      " [ 0.06314205]\n",
      " ...\n",
      " [-0.07981439]\n",
      " [-0.09923526]\n",
      " [-0.02086148]]\n",
      "t [[-0.53753259]\n",
      " [-0.09790032]\n",
      " [ 0.1179605 ]\n",
      " ...\n",
      " [-0.1480139 ]\n",
      " [-0.18125374]\n",
      " [-0.04145752]]\n",
      "t [[-0.53753259]\n",
      " [-0.09790032]\n",
      " [ 0.1179605 ]\n",
      " ...\n",
      " [-0.1480139 ]\n",
      " [-0.18125374]\n",
      " [-0.04145752]]\n",
      "Current iteration=2, loss=34109.46080312247\n",
      "t [[-0.71938114]\n",
      " [-0.11765699]\n",
      " [ 0.16297539]\n",
      " ...\n",
      " [-0.20663611]\n",
      " [-0.24979111]\n",
      " [-0.06074878]]\n",
      "t [[-0.71938114]\n",
      " [-0.11765699]\n",
      " [ 0.16297539]\n",
      " ...\n",
      " [-0.20663611]\n",
      " [-0.24979111]\n",
      " [-0.06074878]]\n",
      "t [[-0.86022522]\n",
      " [-0.13062551]\n",
      " [ 0.19910971]\n",
      " ...\n",
      " [-0.2574088 ]\n",
      " [-0.30781598]\n",
      " [-0.0784439 ]]\n",
      "t [[-0.86022522]\n",
      " [-0.13062551]\n",
      " [ 0.19910971]\n",
      " ...\n",
      " [-0.2574088 ]\n",
      " [-0.30781598]\n",
      " [-0.0784439 ]]\n",
      "Current iteration=4, loss=32440.198964542775\n",
      "t [[-0.96963509]\n",
      " [-0.14070817]\n",
      " [ 0.22790731]\n",
      " ...\n",
      " [-0.30171436]\n",
      " [-0.35756314]\n",
      " [-0.09455519]]\n",
      "t [[-0.96963509]\n",
      " [-0.14070817]\n",
      " [ 0.22790731]\n",
      " ...\n",
      " [-0.30171436]\n",
      " [-0.35756314]\n",
      " [-0.09455519]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.05478839]\n",
      " [-0.14986478]\n",
      " [ 0.25084186]\n",
      " ...\n",
      " [-0.34064374]\n",
      " [-0.40069747]\n",
      " [-0.10919515]]\n",
      "t [[-1.05478839]\n",
      " [-0.14986478]\n",
      " [ 0.25084186]\n",
      " ...\n",
      " [-0.34064374]\n",
      " [-0.40069747]\n",
      " [-0.10919515]]\n",
      "Current iteration=6, loss=31552.847225475543\n",
      "t [[-1.12102723]\n",
      " [-0.15907299]\n",
      " [ 0.26913334]\n",
      " ...\n",
      " [-0.37506011]\n",
      " [-0.43846865]\n",
      " [-0.12249796]]\n",
      "t [[-1.12102723]\n",
      " [-0.15907299]\n",
      " [ 0.26913334]\n",
      " ...\n",
      " [-0.37506011]\n",
      " [-0.43846865]\n",
      " [-0.12249796]]\n",
      "t [[-1.17235246]\n",
      " [-0.16880395]\n",
      " [ 0.28374336]\n",
      " ...\n",
      " [-0.40565217]\n",
      " [-0.47182802]\n",
      " [-0.13459255]]\n",
      "t [[-1.17235246]\n",
      " [-0.16880395]\n",
      " [ 0.28374336]\n",
      " ...\n",
      " [-0.40565217]\n",
      " [-0.47182802]\n",
      " [-0.13459255]]\n",
      "Current iteration=8, loss=31020.973223591332\n",
      "t [[-1.21179594]\n",
      " [-0.17926185]\n",
      " [ 0.29541863]\n",
      " ...\n",
      " [-0.43297516]\n",
      " [-0.50151147]\n",
      " [-0.14559521]]\n",
      "t [[-1.21179594]\n",
      " [-0.17926185]\n",
      " [ 0.29541863]\n",
      " ...\n",
      " [-0.43297516]\n",
      " [-0.50151147]\n",
      " [-0.14559521]]\n",
      "t [[-1.24168631]\n",
      " [-0.19050831]\n",
      " [ 0.30473855]\n",
      " ...\n",
      " [-0.45748149]\n",
      " [-0.52809715]\n",
      " [-0.15560881]]\n",
      "loss=30670.38765230807\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.355164  ]\n",
      " [-0.08001394]\n",
      " [ 0.03125451]\n",
      " ...\n",
      " [-0.07672162]\n",
      " [-0.09975122]\n",
      " [-0.02070481]]\n",
      "t [[-0.355164  ]\n",
      " [-0.08001394]\n",
      " [ 0.03125451]\n",
      " ...\n",
      " [-0.07672162]\n",
      " [-0.09975122]\n",
      " [-0.02070481]]\n",
      "t [[-0.64215372]\n",
      " [-0.15707921]\n",
      " [ 0.06908183]\n",
      " ...\n",
      " [-0.14221574]\n",
      " [-0.18214001]\n",
      " [-0.04148112]]\n",
      "t [[-0.64215372]\n",
      " [-0.15707921]\n",
      " [ 0.06908183]\n",
      " ...\n",
      " [-0.14221574]\n",
      " [-0.18214001]\n",
      " [-0.04148112]]\n",
      "Current iteration=2, loss=34081.8261396932\n",
      "t [[-0.8779917 ]\n",
      " [-0.22738597]\n",
      " [ 0.10653026]\n",
      " ...\n",
      " [-0.19854535]\n",
      " [-0.25095126]\n",
      " [-0.06106792]]\n",
      "t [[-0.8779917 ]\n",
      " [-0.22738597]\n",
      " [ 0.10653026]\n",
      " ...\n",
      " [-0.19854535]\n",
      " [-0.25095126]\n",
      " [-0.06106792]]\n",
      "t [[-1.07517036]\n",
      " [-0.29024441]\n",
      " [ 0.14069105]\n",
      " ...\n",
      " [-0.24740926]\n",
      " [-0.30918904]\n",
      " [-0.07907658]]\n",
      "t [[-1.07517036]\n",
      " [-0.29024441]\n",
      " [ 0.14069105]\n",
      " ...\n",
      " [-0.24740926]\n",
      " [-0.30918904]\n",
      " [-0.07907658]]\n",
      "Current iteration=4, loss=32404.188141954583\n",
      "t [[-1.24258601]\n",
      " [-0.34617217]\n",
      " [ 0.17067717]\n",
      " ...\n",
      " [-0.29014359]\n",
      " [-0.35911144]\n",
      " [-0.09548035]]\n",
      "t [[-1.24258601]\n",
      " [-0.34617217]\n",
      " [ 0.17067717]\n",
      " ...\n",
      " [-0.29014359]\n",
      " [-0.35911144]\n",
      " [-0.09548035]]\n",
      "t [[-1.38661509]\n",
      " [-0.39601637]\n",
      " [ 0.19649702]\n",
      " ...\n",
      " [-0.32779159]\n",
      " [-0.40239835]\n",
      " [-0.11037792]]\n",
      "t [[-1.38661509]\n",
      " [-0.39601637]\n",
      " [ 0.19649702]\n",
      " ...\n",
      " [-0.32779159]\n",
      " [-0.40239835]\n",
      " [-0.11037792]]\n",
      "Current iteration=6, loss=31511.575705412113\n",
      "t [[-1.51191445]\n",
      " [-0.44062477]\n",
      " [ 0.218515  ]\n",
      " ...\n",
      " [-0.36117298]\n",
      " [-0.44030905]\n",
      " [-0.12390018]]\n",
      "t [[-1.51191445]\n",
      " [-0.44062477]\n",
      " [ 0.218515  ]\n",
      " ...\n",
      " [-0.36117298]\n",
      " [-0.44030905]\n",
      " [-0.12390018]]\n",
      "t [[-1.62195923]\n",
      " [-0.48074497]\n",
      " [ 0.23720733]\n",
      " ...\n",
      " [-0.39093876]\n",
      " [-0.47380108]\n",
      " [-0.13617708]]\n",
      "t [[-1.62195923]\n",
      " [-0.48074497]\n",
      " [ 0.23720733]\n",
      " ...\n",
      " [-0.39093876]\n",
      " [-0.47380108]\n",
      " [-0.13617708]]\n",
      "Current iteration=8, loss=30974.634258800477\n",
      "t [[-1.71939756]\n",
      " [-0.51700754]\n",
      " [ 0.25305564]\n",
      " ...\n",
      " [-0.4176121 ]\n",
      " [-0.5036143 ]\n",
      " [-0.14732745]]\n",
      "t [[-1.71939756]\n",
      " [-0.51700754]\n",
      " [ 0.25305564]\n",
      " ...\n",
      " [-0.4176121 ]\n",
      " [-0.5036143 ]\n",
      " [-0.14732745]]\n",
      "t [[-1.80628617]\n",
      " [-0.54993652]\n",
      " [ 0.26650357]\n",
      " ...\n",
      " [-0.44161836]\n",
      " [-0.53032945]\n",
      " [-0.1574572 ]]\n",
      "loss=30618.822755691923\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.35607237]\n",
      " [-0.08252588]\n",
      " [ 0.02144717]\n",
      " ...\n",
      " [-0.07583685]\n",
      " [-0.10033713]\n",
      " [-0.0202941 ]]\n",
      "t [[-0.35607237]\n",
      " [-0.08252588]\n",
      " [ 0.02144717]\n",
      " ...\n",
      " [-0.07583685]\n",
      " [-0.10033713]\n",
      " [-0.0202941 ]]\n",
      "t [[-0.64386981]\n",
      " [-0.1612286 ]\n",
      " [ 0.05306387]\n",
      " ...\n",
      " [-0.14108726]\n",
      " [-0.18321658]\n",
      " [-0.0407016 ]]\n",
      "t [[-0.64386981]\n",
      " [-0.1612286 ]\n",
      " [ 0.05306387]\n",
      " ...\n",
      " [-0.14108726]\n",
      " [-0.18321658]\n",
      " [-0.0407016 ]]\n",
      "Current iteration=2, loss=34034.61840284746\n",
      "t [[-0.88037243]\n",
      " [-0.23277145]\n",
      " [ 0.08677999]\n",
      " ...\n",
      " [-0.1975425 ]\n",
      " [-0.25245028]\n",
      " [-0.06002586]]\n",
      "t [[-0.88037243]\n",
      " [-0.23277145]\n",
      " [ 0.08677999]\n",
      " ...\n",
      " [-0.1975425 ]\n",
      " [-0.25245028]\n",
      " [-0.06002586]]\n",
      "t [[-1.07809326]\n",
      " [-0.29666575]\n",
      " [ 0.11884909]\n",
      " ...\n",
      " [-0.24673409]\n",
      " [-0.3110613 ]\n",
      " [-0.07787945]]\n",
      "t [[-1.07809326]\n",
      " [-0.29666575]\n",
      " [ 0.11884909]\n",
      " ...\n",
      " [-0.24673409]\n",
      " [-0.3110613 ]\n",
      " [-0.07787945]]\n",
      "Current iteration=4, loss=32336.109436540555\n",
      "t [[-1.24595507]\n",
      " [-0.35351766]\n",
      " [ 0.14782209]\n",
      " ...\n",
      " [-0.2898997 ]\n",
      " [-0.36131964]\n",
      " [-0.09421809]]\n",
      "t [[-1.24595507]\n",
      " [-0.35351766]\n",
      " [ 0.14782209]\n",
      " ...\n",
      " [-0.2898997 ]\n",
      " [-0.36131964]\n",
      " [-0.09421809]]\n",
      "t [[-1.39035462]\n",
      " [-0.40421653]\n",
      " [ 0.17334339]\n",
      " ...\n",
      " [-0.32802543]\n",
      " [-0.40491253]\n",
      " [-0.10912087]]\n",
      "t [[-1.39035462]\n",
      " [-0.40421653]\n",
      " [ 0.17334339]\n",
      " ...\n",
      " [-0.32802543]\n",
      " [-0.40491253]\n",
      " [-0.10912087]]\n",
      "Current iteration=6, loss=31433.110355130997\n",
      "t [[-1.51596265]\n",
      " [-0.44963247]\n",
      " [ 0.19554252]\n",
      " ...\n",
      " [-0.36189794]\n",
      " [-0.443104  ]\n",
      " [-0.12270196]]\n",
      "t [[-1.51596265]\n",
      " [-0.44963247]\n",
      " [ 0.19554252]\n",
      " ...\n",
      " [-0.36189794]\n",
      " [-0.443104  ]\n",
      " [-0.12270196]]\n",
      "t [[-1.62626371]\n",
      " [-0.49052621]\n",
      " [ 0.21474258]\n",
      " ...\n",
      " [-0.39214922]\n",
      " [-0.47685482]\n",
      " [-0.13507823]]\n",
      "t [[-1.62626371]\n",
      " [-0.49052621]\n",
      " [ 0.21474258]\n",
      " ...\n",
      " [-0.39214922]\n",
      " [-0.47685482]\n",
      " [-0.13507823]]\n",
      "Current iteration=8, loss=30890.682671178838\n",
      "t [[-1.72391236]\n",
      " [-0.52753676]\n",
      " [ 0.23132371]\n",
      " ...\n",
      " [-0.41929166]\n",
      " [-0.50690717]\n",
      " [-0.14635863]]\n",
      "t [[-1.72391236]\n",
      " [-0.52753676]\n",
      " [ 0.23132371]\n",
      " ...\n",
      " [-0.41929166]\n",
      " [-0.50690717]\n",
      " [-0.14635863]]\n",
      "t [[-1.81097006]\n",
      " [-0.56119394]\n",
      " [ 0.24566103]\n",
      " ...\n",
      " [-0.44374474]\n",
      " [-0.53384363]\n",
      " [-0.15664167]]\n",
      "loss=30531.995912180777\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.35326385]\n",
      " [-0.07683632]\n",
      " [ 0.025149  ]\n",
      " ...\n",
      " [ 0.39594362]\n",
      " [-0.29153494]\n",
      " [ 0.32115633]]\n",
      "t [[-0.35326385]\n",
      " [-0.07683632]\n",
      " [ 0.025149  ]\n",
      " ...\n",
      " [ 0.39594362]\n",
      " [-0.29153494]\n",
      " [ 0.32115633]]\n",
      "t [[-0.63911938]\n",
      " [-0.15168159]\n",
      " [ 0.05928196]\n",
      " ...\n",
      " [ 0.70399258]\n",
      " [-0.52253802]\n",
      " [ 0.57654061]]\n",
      "t [[-0.63911938]\n",
      " [-0.15168159]\n",
      " [ 0.05928196]\n",
      " ...\n",
      " [ 0.70399258]\n",
      " [-0.52253802]\n",
      " [ 0.57654061]]\n",
      "Current iteration=2, loss=34156.958494659804\n",
      "t [[-0.87438841]\n",
      " [-0.22026544]\n",
      " [ 0.09458404]\n",
      " ...\n",
      " [ 0.94539754]\n",
      " [-0.70659766]\n",
      " [ 0.78145679]]\n",
      "t [[-0.87438841]\n",
      " [-0.22026544]\n",
      " [ 0.09458404]\n",
      " ...\n",
      " [ 0.94539754]\n",
      " [-0.70659766]\n",
      " [ 0.78145679]]\n",
      "t [[-1.07138013]\n",
      " [-0.28167831]\n",
      " [ 0.12756485]\n",
      " ...\n",
      " [ 1.13664011]\n",
      " [-0.85476656]\n",
      " [ 0.94795271]]\n",
      "t [[-1.07138013]\n",
      " [-0.28167831]\n",
      " [ 0.12756485]\n",
      " ...\n",
      " [ 1.13664011]\n",
      " [-0.85476656]\n",
      " [ 0.94795271]]\n",
      "Current iteration=4, loss=32504.969824781343\n",
      "t [[-1.23885544]\n",
      " [-0.33633558]\n",
      " [ 0.15696815]\n",
      " ...\n",
      " [ 1.28980817]\n",
      " [-0.97538821]\n",
      " [ 1.08496258]]\n",
      "t [[-1.23885544]\n",
      " [-0.33633558]\n",
      " [ 0.15696815]\n",
      " ...\n",
      " [ 1.28980817]\n",
      " [-0.97538821]\n",
      " [ 1.08496258]]\n",
      "t [[-1.38309799]\n",
      " [-0.3850355 ]\n",
      " [ 0.18257299]\n",
      " ...\n",
      " [ 1.41367556]\n",
      " [-1.0746295 ]\n",
      " [ 1.19902976]]\n",
      "t [[-1.38309799]\n",
      " [-0.3850355 ]\n",
      " [ 0.18257299]\n",
      " ...\n",
      " [ 1.41367556]\n",
      " [-1.0746295 ]\n",
      " [ 1.19902976]]\n",
      "Current iteration=6, loss=31622.99283304477\n",
      "t [[-1.50870259]\n",
      " [-0.42860097]\n",
      " [ 0.20460187]\n",
      " ...\n",
      " [ 1.51466265]\n",
      " [-1.15705728]\n",
      " [ 1.29497992]]\n",
      "t [[-1.50870259]\n",
      " [-0.42860097]\n",
      " [ 0.20460187]\n",
      " ...\n",
      " [ 1.51466265]\n",
      " [-1.15705728]\n",
      " [ 1.29497992]]\n",
      "t [[-1.61910309]\n",
      " [-0.46776563]\n",
      " [ 0.22344292]\n",
      " ...\n",
      " [ 1.59753802]\n",
      " [-1.22609186]\n",
      " [ 1.37642272]]\n",
      "t [[-1.61910309]\n",
      " [-0.46776563]\n",
      " [ 0.22344292]\n",
      " ...\n",
      " [ 1.59753802]\n",
      " [-1.22609186]\n",
      " [ 1.37642272]]\n",
      "Current iteration=8, loss=31091.70945003303\n",
      "t [[-1.71692002]\n",
      " [-0.50315123]\n",
      " [ 0.23952277]\n",
      " ...\n",
      " [ 1.66590259]\n",
      " [-1.28433223]\n",
      " [ 1.44610219]]\n",
      "t [[-1.71692002]\n",
      " [-0.50315123]\n",
      " [ 0.23952277]\n",
      " ...\n",
      " [ 1.66590259]\n",
      " [-1.28433223]\n",
      " [ 1.44610219]]\n",
      "t [[-1.80419166]\n",
      " [-0.53527546]\n",
      " [ 0.25325054]\n",
      " ...\n",
      " [ 1.72251951]\n",
      " [-1.33378282]\n",
      " [ 1.50613765]]\n",
      "loss=30739.702391643717\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.3115814 ]\n",
      " [-0.06554683]\n",
      " [ 0.06505545]\n",
      " ...\n",
      " [-0.082233  ]\n",
      " [-0.10224238]\n",
      " [-0.02149365]]\n",
      "t [[-0.3115814 ]\n",
      " [-0.06554683]\n",
      " [ 0.06505545]\n",
      " ...\n",
      " [-0.082233  ]\n",
      " [-0.10224238]\n",
      " [-0.02149365]]\n",
      "t [[-0.55174718]\n",
      " [-0.09996455]\n",
      " [ 0.12127635]\n",
      " ...\n",
      " [-0.15214247]\n",
      " [-0.18621605]\n",
      " [-0.04270576]]\n",
      "t [[-0.55174718]\n",
      " [-0.09996455]\n",
      " [ 0.12127635]\n",
      " ...\n",
      " [-0.15214247]\n",
      " [-0.18621605]\n",
      " [-0.04270576]]\n",
      "Current iteration=2, loss=34031.69769491846\n",
      "t [[-0.73586454]\n",
      " [-0.11944181]\n",
      " [ 0.16705329]\n",
      " ...\n",
      " [-0.21196089]\n",
      " [-0.2560107 ]\n",
      " [-0.06250229]]\n",
      "t [[-0.73586454]\n",
      " [-0.11944181]\n",
      " [ 0.16705329]\n",
      " ...\n",
      " [-0.21196089]\n",
      " [-0.2560107 ]\n",
      " [-0.06250229]]\n",
      "t [[-0.87725028]\n",
      " [-0.13218638]\n",
      " [ 0.20347304]\n",
      " ...\n",
      " [-0.26356123]\n",
      " [-0.3148389 ]\n",
      " [-0.08058953]]\n",
      "t [[-0.87725028]\n",
      " [-0.13218638]\n",
      " [ 0.20347304]\n",
      " ...\n",
      " [-0.26356123]\n",
      " [-0.3148389 ]\n",
      " [-0.08058953]]\n",
      "Current iteration=4, loss=32363.9810920882\n",
      "t [[-0.9861681 ]\n",
      " [-0.14222021]\n",
      " [ 0.23225488]\n",
      " ...\n",
      " [-0.30842785]\n",
      " [-0.36509084]\n",
      " [-0.09699863]]\n",
      "t [[-0.9861681 ]\n",
      " [-0.14222021]\n",
      " [ 0.23225488]\n",
      " ...\n",
      " [-0.30842785]\n",
      " [-0.36509084]\n",
      " [-0.09699863]]\n",
      "t [[-1.07022461]\n",
      " [-0.1515075 ]\n",
      " [ 0.25499722]\n",
      " ...\n",
      " [-0.34772311]\n",
      " [-0.40852958]\n",
      " [-0.11186058]]\n",
      "t [[-1.07022461]\n",
      " [-0.1515075 ]\n",
      " [ 0.25499722]\n",
      " ...\n",
      " [-0.34772311]\n",
      " [-0.40852958]\n",
      " [-0.11186058]]\n",
      "Current iteration=6, loss=31488.810986706496\n",
      "t [[-1.1350244 ]\n",
      " [-0.16099826]\n",
      " [ 0.27299897]\n",
      " ...\n",
      " [-0.38236074]\n",
      " [-0.44646785]\n",
      " [-0.12532392]]\n",
      "t [[-1.1350244 ]\n",
      " [-0.16099826]\n",
      " [ 0.27299897]\n",
      " ...\n",
      " [-0.38236074]\n",
      " [-0.44646785]\n",
      " [-0.12532392]]\n",
      "t [[-1.18473033]\n",
      " [-0.17113089]\n",
      " [ 0.28726992]\n",
      " ...\n",
      " [-0.41306582]\n",
      " [-0.47989856]\n",
      " [-0.13752841]]\n",
      "t [[-1.18473033]\n",
      " [-0.17113089]\n",
      " [ 0.28726992]\n",
      " ...\n",
      " [-0.41306582]\n",
      " [-0.47989856]\n",
      " [-0.13752841]]\n",
      "Current iteration=8, loss=30967.655019211266\n",
      "t [[-1.22247563]\n",
      " [-0.18207987]\n",
      " [ 0.29858582]\n",
      " ...\n",
      " [-0.44042017]\n",
      " [-0.50958571]\n",
      " [-0.14859862]]\n",
      "t [[-1.22247563]\n",
      " [-0.18207987]\n",
      " [ 0.29858582]\n",
      " ...\n",
      " [-0.44042017]\n",
      " [-0.50958571]\n",
      " [-0.14859862]]\n",
      "t [[-1.25065314]\n",
      " [-0.19388193]\n",
      " [ 0.30754369]\n",
      " ...\n",
      " [-0.46489591]\n",
      " [-0.53612697]\n",
      " [-0.15864408]]\n",
      "loss=30625.051634039643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.36592654]\n",
      " [-0.0824386 ]\n",
      " [ 0.03220162]\n",
      " ...\n",
      " [-0.07904652]\n",
      " [-0.10277398]\n",
      " [-0.02133222]]\n",
      "t [[-0.36592654]\n",
      " [-0.0824386 ]\n",
      " [ 0.03220162]\n",
      " ...\n",
      " [-0.07904652]\n",
      " [-0.10277398]\n",
      " [-0.02133222]]\n",
      "t [[-0.65951776]\n",
      " [-0.16174454]\n",
      " [ 0.07137574]\n",
      " ...\n",
      " [-0.14618042]\n",
      " [-0.18712475]\n",
      " [-0.04274041]]\n",
      "t [[-0.65951776]\n",
      " [-0.16174454]\n",
      " [ 0.07137574]\n",
      " ...\n",
      " [-0.14618042]\n",
      " [-0.18712475]\n",
      " [-0.04274041]]\n",
      "Current iteration=2, loss=34003.606385326595\n",
      "t [[-0.89940381]\n",
      " [-0.23376054]\n",
      " [ 0.10992937]\n",
      " ...\n",
      " [-0.20366165]\n",
      " [-0.25719581]\n",
      " [-0.06284831]]\n",
      "t [[-0.89940381]\n",
      " [-0.23376054]\n",
      " [ 0.10992937]\n",
      " ...\n",
      " [-0.20366165]\n",
      " [-0.25719581]\n",
      " [-0.06284831]]\n",
      "t [[-1.09905711]\n",
      " [-0.297839  ]\n",
      " [ 0.14482222]\n",
      " ...\n",
      " [-0.25333   ]\n",
      " [-0.31623788]\n",
      " [-0.08126013]]\n",
      "t [[-1.09905711]\n",
      " [-0.297839  ]\n",
      " [ 0.14482222]\n",
      " ...\n",
      " [-0.25333   ]\n",
      " [-0.31623788]\n",
      " [-0.08126013]]\n",
      "Current iteration=4, loss=32327.59280795876\n",
      "t [[-1.26794981]\n",
      " [-0.35461751]\n",
      " [ 0.17520982]\n",
      " ...\n",
      " [-0.29661847]\n",
      " [-0.36666583]\n",
      " [-0.09796796]]\n",
      "t [[-1.26794981]\n",
      " [-0.35461751]\n",
      " [ 0.17520982]\n",
      " ...\n",
      " [-0.29661847]\n",
      " [-0.36666583]\n",
      " [-0.09796796]]\n",
      "t [[-1.41280375]\n",
      " [-0.40504776]\n",
      " [ 0.2011807 ]\n",
      " ...\n",
      " [-0.33463732]\n",
      " [-0.41025832]\n",
      " [-0.11309002]]\n",
      "t [[-1.41280375]\n",
      " [-0.40504776]\n",
      " [ 0.2011807 ]\n",
      " ...\n",
      " [-0.33463732]\n",
      " [-0.41025832]\n",
      " [-0.11309002]]\n",
      "Current iteration=6, loss=31447.081058768985\n",
      "t [[-1.53849048]\n",
      " [-0.45005319]\n",
      " [ 0.22317416]\n",
      " ...\n",
      " [-0.36825341]\n",
      " [-0.44833792]\n",
      " [-0.12677253]]\n",
      "t [[-1.53849048]\n",
      " [-0.45005319]\n",
      " [ 0.22317416]\n",
      " ...\n",
      " [-0.36825341]\n",
      " [-0.44833792]\n",
      " [-0.12677253]]\n",
      "t [[-1.64862363]\n",
      " [-0.4904336 ]\n",
      " [ 0.24172627]\n",
      " ...\n",
      " [-0.39815156]\n",
      " [-0.48190381]\n",
      " [-0.13915696]]\n",
      "t [[-1.64862363]\n",
      " [-0.4904336 ]\n",
      " [ 0.24172627]\n",
      " ...\n",
      " [-0.39815156]\n",
      " [-0.48190381]\n",
      " [-0.13915696]]\n",
      "Current iteration=8, loss=30920.69344510231\n",
      "t [[-1.74594358]\n",
      " [-0.52685556]\n",
      " [ 0.25736355]\n",
      " ...\n",
      " [-0.42487961]\n",
      " [-0.51172393]\n",
      " [-0.15037098]]\n",
      "t [[-1.74594358]\n",
      " [-0.52685556]\n",
      " [ 0.25736355]\n",
      " ...\n",
      " [-0.42487961]\n",
      " [-0.51172393]\n",
      " [-0.15037098]]\n",
      "t [[-1.83257034]\n",
      " [-0.55986827]\n",
      " [ 0.27056229]\n",
      " ...\n",
      " [-0.4488812 ]\n",
      " [-0.53839849]\n",
      " [-0.16052747]]\n",
      "loss=30572.688912739624\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.36686245]\n",
      " [-0.08502667]\n",
      " [ 0.02209709]\n",
      " ...\n",
      " [-0.07813494]\n",
      " [-0.10337765]\n",
      " [-0.02090907]]\n",
      "t [[-0.36686245]\n",
      " [-0.08502667]\n",
      " [ 0.02209709]\n",
      " ...\n",
      " [-0.07813494]\n",
      " [-0.10337765]\n",
      " [-0.02090907]]\n",
      "t [[-0.66128333]\n",
      " [-0.16599249]\n",
      " [ 0.05498253]\n",
      " ...\n",
      " [-0.14503743]\n",
      " [-0.18823112]\n",
      " [-0.04193853]]\n",
      "t [[-0.66128333]\n",
      " [-0.16599249]\n",
      " [ 0.05498253]\n",
      " ...\n",
      " [-0.14503743]\n",
      " [-0.18823112]\n",
      " [-0.04193853]]\n",
      "Current iteration=2, loss=33955.39512026512\n",
      "t [[-0.90184675]\n",
      " [-0.2392575 ]\n",
      " [ 0.08983986]\n",
      " ...\n",
      " [-0.20267034]\n",
      " [-0.25873356]\n",
      " [-0.06178238]]\n",
      "t [[-0.90184675]\n",
      " [-0.2392575 ]\n",
      " [ 0.08983986]\n",
      " ...\n",
      " [-0.20267034]\n",
      " [-0.25873356]\n",
      " [-0.06178238]]\n",
      "t [[-1.10204883]\n",
      " [-0.30438514]\n",
      " [ 0.12272521]\n",
      " ...\n",
      " [-0.25269444]\n",
      " [-0.31815603]\n",
      " [-0.08004415]]\n",
      "t [[-1.10204883]\n",
      " [-0.30438514]\n",
      " [ 0.12272521]\n",
      " ...\n",
      " [-0.25269444]\n",
      " [-0.31815603]\n",
      " [-0.08004415]]\n",
      "Current iteration=4, loss=32258.573941579492\n",
      "t [[-1.27139064]\n",
      " [-0.36210262]\n",
      " [ 0.15219871]\n",
      " ...\n",
      " [-0.29643967]\n",
      " [-0.36892586]\n",
      " [-0.09669572]]\n",
      "t [[-1.27139064]\n",
      " [-0.36210262]\n",
      " [ 0.15219871]\n",
      " ...\n",
      " [-0.29643967]\n",
      " [-0.36892586]\n",
      " [-0.09669572]]\n",
      "t [[-1.41661551]\n",
      " [-0.41340339]\n",
      " [ 0.17796996]\n",
      " ...\n",
      " [-0.33495772]\n",
      " [-0.41282929]\n",
      " [-0.11183377]]\n",
      "t [[-1.41661551]\n",
      " [-0.41340339]\n",
      " [ 0.17796996]\n",
      " ...\n",
      " [-0.33495772]\n",
      " [-0.41282929]\n",
      " [-0.11183377]]\n",
      "Current iteration=6, loss=31367.887491656817\n",
      "t [[-1.54260947]\n",
      " [-0.45923276]\n",
      " [ 0.2002375 ]\n",
      " ...\n",
      " [-0.36908228]\n",
      " [-0.45119374]\n",
      " [-0.12558663]]\n",
      "t [[-1.54260947]\n",
      " [-0.45923276]\n",
      " [ 0.2002375 ]\n",
      " ...\n",
      " [-0.36908228]\n",
      " [-0.45119374]\n",
      " [-0.12558663]]\n",
      "t [[-1.65299572]\n",
      " [-0.50040343]\n",
      " [ 0.2193825 ]\n",
      " ...\n",
      " [-0.39947952]\n",
      " [-0.48502168]\n",
      " [-0.13808204]]\n",
      "t [[-1.65299572]\n",
      " [-0.50040343]\n",
      " [ 0.2193825 ]\n",
      " ...\n",
      " [-0.39947952]\n",
      " [-0.48502168]\n",
      " [-0.13808204]]\n",
      "Current iteration=8, loss=30836.226631604317\n",
      "t [[-1.75052125]\n",
      " [-0.53759028]\n",
      " [ 0.23583   ]\n",
      " ...\n",
      " [-0.42668697]\n",
      " [-0.51508347]\n",
      " [-0.14943747]]\n",
      "t [[-1.75052125]\n",
      " [-0.53759028]\n",
      " [ 0.23583   ]\n",
      " ...\n",
      " [-0.42668697]\n",
      " [-0.51508347]\n",
      " [-0.14943747]]\n",
      "t [[-1.83731083]\n",
      " [-0.57134825]\n",
      " [ 0.24998837]\n",
      " ...\n",
      " [-0.45114286]\n",
      " [-0.54198117]\n",
      " [-0.15975829]]\n",
      "loss=30485.53376923647\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.36396881]\n",
      " [-0.0791647 ]\n",
      " [ 0.0259111 ]\n",
      " ...\n",
      " [ 0.40794191]\n",
      " [-0.30036933]\n",
      " [ 0.33088834]]\n",
      "t [[-0.36396881]\n",
      " [-0.0791647 ]\n",
      " [ 0.0259111 ]\n",
      " ...\n",
      " [ 0.40794191]\n",
      " [-0.30036933]\n",
      " [ 0.33088834]]\n",
      "t [[-0.65641484]\n",
      " [-0.15621246]\n",
      " [ 0.06135252]\n",
      " ...\n",
      " [ 0.72262196]\n",
      " [-0.53650891]\n",
      " [ 0.59198875]]\n",
      "t [[-0.65641484]\n",
      " [-0.15621246]\n",
      " [ 0.06135252]\n",
      " ...\n",
      " [ 0.72262196]\n",
      " [-0.53650891]\n",
      " [ 0.59198875]]\n",
      "Current iteration=2, loss=34080.15607767231\n",
      "t [[-0.89574843]\n",
      " [-0.22648243]\n",
      " [ 0.09778686]\n",
      " ...\n",
      " [ 0.96729886]\n",
      " [-0.72329387]\n",
      " [ 0.80005331]]\n",
      "t [[-0.89574843]\n",
      " [-0.22648243]\n",
      " [ 0.09778686]\n",
      " ...\n",
      " [ 0.96729886]\n",
      " [-0.72329387]\n",
      " [ 0.80005331]]\n",
      "t [[-1.09524302]\n",
      " [-0.28909567]\n",
      " [ 0.13154942]\n",
      " ...\n",
      " [ 1.15978891]\n",
      " [-0.8726949 ]\n",
      " [ 0.96811229]]\n",
      "t [[-1.09524302]\n",
      " [-0.28909567]\n",
      " [ 0.13154942]\n",
      " ...\n",
      " [ 1.15978891]\n",
      " [-0.8726949 ]\n",
      " [ 0.96811229]]\n",
      "Current iteration=4, loss=32429.41508938645\n",
      "t [[-1.26422634]\n",
      " [-0.34458574]\n",
      " [ 0.16140689]\n",
      " ...\n",
      " [ 1.31299406]\n",
      " [-0.99363748]\n",
      " [ 1.10570905]]\n",
      "t [[-1.26422634]\n",
      " [-0.34458574]\n",
      " [ 0.16140689]\n",
      " ...\n",
      " [ 1.31299406]\n",
      " [-0.99363748]\n",
      " [ 1.10570905]]\n",
      "t [[-1.40932302]\n",
      " [-0.39385588]\n",
      " [ 0.18721062]\n",
      " ...\n",
      " [ 1.43617787]\n",
      " [-1.09264624]\n",
      " [ 1.21975902]]\n",
      "t [[-1.40932302]\n",
      " [-0.39385588]\n",
      " [ 0.18721062]\n",
      " ...\n",
      " [ 1.43617787]\n",
      " [-1.09264624]\n",
      " [ 1.21975902]]\n",
      "Current iteration=6, loss=31559.219373081378\n",
      "t [[-1.53534063]\n",
      " [-0.43780519]\n",
      " [ 0.20925566]\n",
      " ...\n",
      " [ 1.5360591 ]\n",
      " [-1.1745089 ]\n",
      " [ 1.31531806]]\n",
      "t [[-1.53534063]\n",
      " [-0.43780519]\n",
      " [ 0.20925566]\n",
      " ...\n",
      " [ 1.5360591 ]\n",
      " [-1.1745089 ]\n",
      " [ 1.31531806]]\n",
      "t [[-1.64585081]\n",
      " [-0.47721985]\n",
      " [ 0.22798997]\n",
      " ...\n",
      " [ 1.61759193]\n",
      " [-1.24278443]\n",
      " [ 1.39614142]]\n",
      "t [[-1.64585081]\n",
      " [-0.47721985]\n",
      " [ 0.22798997]\n",
      " ...\n",
      " [ 1.61759193]\n",
      " [-1.24278443]\n",
      " [ 1.39614142]]\n",
      "Current iteration=8, loss=31038.336840855773\n",
      "t [[-1.74356629]\n",
      " [-0.51275765]\n",
      " [ 0.24388596]\n",
      " ...\n",
      " [ 1.68449489]\n",
      " [-1.30016047]\n",
      " [ 1.46506719]]\n",
      "t [[-1.74356629]\n",
      " [-0.51275765]\n",
      " [ 0.24388596]\n",
      " ...\n",
      " [ 1.68449489]\n",
      " [-1.30016047]\n",
      " [ 1.46506719]]\n",
      "t [[-1.83058884]\n",
      " [-0.54496137]\n",
      " [ 0.25738646]\n",
      " ...\n",
      " [ 1.73960686]\n",
      " [-1.3486991 ]\n",
      " [ 1.5242766 ]]\n",
      "loss=30694.086177703055\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.32074556]\n",
      " [-0.06747468]\n",
      " [ 0.06696884]\n",
      " ...\n",
      " [-0.08465162]\n",
      " [-0.10524951]\n",
      " [-0.02212581]]\n",
      "t [[-0.32074556]\n",
      " [-0.06747468]\n",
      " [ 0.06696884]\n",
      " ...\n",
      " [-0.08465162]\n",
      " [-0.10524951]\n",
      " [-0.02212581]]\n",
      "t [[-0.5658422 ]\n",
      " [-0.10197692]\n",
      " [ 0.12457709]\n",
      " ...\n",
      " [-0.1562506 ]\n",
      " [-0.19114784]\n",
      " [-0.04395355]]\n",
      "t [[-0.5658422 ]\n",
      " [-0.10197692]\n",
      " [ 0.12457709]\n",
      " ...\n",
      " [-0.1562506 ]\n",
      " [-0.19114784]\n",
      " [-0.04395355]]\n",
      "Current iteration=2, loss=33955.44052560579\n",
      "t [[-0.7520702 ]\n",
      " [-0.1211516 ]\n",
      " [ 0.17107924]\n",
      " ...\n",
      " [-0.21723623]\n",
      " [-0.26216014]\n",
      " [-0.06424907]]\n",
      "t [[-0.7520702 ]\n",
      " [-0.1211516 ]\n",
      " [ 0.17107924]\n",
      " ...\n",
      " [-0.21723623]\n",
      " [-0.26216014]\n",
      " [-0.06424907]]\n",
      "t [[-0.89385079]\n",
      " [-0.13367658]\n",
      " [ 0.20774395]\n",
      " ...\n",
      " [-0.26963298]\n",
      " [-0.32175304]\n",
      " [-0.08271892]]\n",
      "t [[-0.89385079]\n",
      " [-0.13367658]\n",
      " [ 0.20774395]\n",
      " ...\n",
      " [-0.26963298]\n",
      " [-0.32175304]\n",
      " [-0.08271892]]\n",
      "Current iteration=4, loss=32290.517268454867\n",
      "t [[-1.00215862]\n",
      " [-0.1436808 ]\n",
      " [ 0.23647596]\n",
      " ...\n",
      " [-0.31503029]\n",
      " [-0.37247568]\n",
      " [-0.09941519]]\n",
      "t [[-1.00215862]\n",
      " [-0.1436808 ]\n",
      " [ 0.23647596]\n",
      " ...\n",
      " [-0.31503029]\n",
      " [-0.37247568]\n",
      " [-0.09941519]]\n",
      "t [[-1.08503117]\n",
      " [-0.15312386]\n",
      " [ 0.25900095]\n",
      " ...\n",
      " [-0.35466337]\n",
      " [-0.41618996]\n",
      " [-0.11448832]]\n",
      "t [[-1.08503117]\n",
      " [-0.15312386]\n",
      " [ 0.25900095]\n",
      " ...\n",
      " [-0.35466337]\n",
      " [-0.41618996]\n",
      " [-0.11448832]]\n",
      "Current iteration=6, loss=31427.670560986055\n",
      "t [[-1.14833198]\n",
      " [-0.162923  ]\n",
      " [ 0.27669612]\n",
      " ...\n",
      " [-0.3894971 ]\n",
      " [-0.45427138]\n",
      " [-0.12810157]]\n",
      "t [[-1.14833198]\n",
      " [-0.162923  ]\n",
      " [ 0.27669612]\n",
      " ...\n",
      " [-0.3894971 ]\n",
      " [-0.45427138]\n",
      " [-0.12810157]]\n",
      "t [[-1.19638124]\n",
      " [-0.17348126]\n",
      " [ 0.29061785]\n",
      " ...\n",
      " [-0.42029308]\n",
      " [-0.48775382]\n",
      " [-0.14040563]]\n",
      "t [[-1.19638124]\n",
      " [-0.17348126]\n",
      " [ 0.29061785]\n",
      " ...\n",
      " [-0.42029308]\n",
      " [-0.48775382]\n",
      " [-0.14040563]]\n",
      "Current iteration=8, loss=30916.978139769686\n",
      "t [[-1.23240924]\n",
      " [-0.18494209]\n",
      " [ 0.30156951]\n",
      " ...\n",
      " [-0.44765962]\n",
      " [-0.51742876]\n",
      " [-0.15153348]]\n",
      "t [[-1.23240924]\n",
      " [-0.18494209]\n",
      " [ 0.30156951]\n",
      " ...\n",
      " [-0.44765962]\n",
      " [-0.51742876]\n",
      " [-0.15153348]]\n",
      "t [[-1.25886933]\n",
      " [-0.19731677]\n",
      " [ 0.31016441]\n",
      " ...\n",
      " [-0.47208833]\n",
      " [-0.54391279]\n",
      " [-0.16160133]]\n",
      "loss=30582.028215887047\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.37668909]\n",
      " [-0.08486327]\n",
      " [ 0.03314873]\n",
      " ...\n",
      " [-0.08137141]\n",
      " [-0.10579675]\n",
      " [-0.02195964]]\n",
      "t [[-0.37668909]\n",
      " [-0.08486327]\n",
      " [ 0.03314873]\n",
      " ...\n",
      " [-0.08137141]\n",
      " [-0.10579675]\n",
      " [-0.02195964]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-0.67676157]\n",
      " [-0.16640408]\n",
      " [ 0.07368103]\n",
      " ...\n",
      " [-0.15012532]\n",
      " [-0.19207871]\n",
      " [-0.04399985]]\n",
      "t [[-0.67676157]\n",
      " [-0.16640408]\n",
      " [ 0.07368103]\n",
      " ...\n",
      " [-0.15012532]\n",
      " [-0.19207871]\n",
      " [-0.04399985]]\n",
      "Current iteration=2, loss=33926.907877877435\n",
      "t [[-0.92055177]\n",
      " [-0.24009822]\n",
      " [ 0.1133255 ]\n",
      " ...\n",
      " [-0.20873062]\n",
      " [-0.26336971]\n",
      " [-0.06462255]]\n",
      "t [[-0.92055177]\n",
      " [-0.24009822]\n",
      " [ 0.1133255 ]\n",
      " ...\n",
      " [-0.20873062]\n",
      " [-0.26336971]\n",
      " [-0.06462255]]\n",
      "t [[-1.12254651]\n",
      " [-0.30535476]\n",
      " [ 0.14891856]\n",
      " ...\n",
      " [-0.25917398]\n",
      " [-0.32317732]\n",
      " [-0.08342758]]\n",
      "t [[-1.12254651]\n",
      " [-0.30535476]\n",
      " [ 0.14891856]\n",
      " ...\n",
      " [-0.25917398]\n",
      " [-0.32317732]\n",
      " [-0.08342758]]\n",
      "Current iteration=4, loss=32253.76320899484\n",
      "t [[-1.29280256]\n",
      " [-0.36294199]\n",
      " [ 0.17967013]\n",
      " ...\n",
      " [-0.30298806]\n",
      " [-0.37407671]\n",
      " [-0.10042832]]\n",
      "t [[-1.29280256]\n",
      " [-0.36294199]\n",
      " [ 0.17967013]\n",
      " ...\n",
      " [-0.30298806]\n",
      " [-0.37407671]\n",
      " [-0.10042832]]\n",
      "t [[-1.43838735]\n",
      " [-0.41392049]\n",
      " [ 0.20575636]\n",
      " ...\n",
      " [-0.3413514 ]\n",
      " [-0.41794596]\n",
      " [-0.11576353]]\n",
      "t [[-1.43838735]\n",
      " [-0.41392049]\n",
      " [ 0.20575636]\n",
      " ...\n",
      " [-0.3413514 ]\n",
      " [-0.41794596]\n",
      " [-0.11576353]]\n",
      "Current iteration=6, loss=31385.484303396577\n",
      "t [[-1.56438496]\n",
      " [-0.45929034]\n",
      " [ 0.22769512]\n",
      " ...\n",
      " [-0.37517852]\n",
      " [-0.45617064]\n",
      " [-0.12959518]]\n",
      "t [[-1.56438496]\n",
      " [-0.45929034]\n",
      " [ 0.22769512]\n",
      " ...\n",
      " [-0.37517852]\n",
      " [-0.45617064]\n",
      " [-0.12959518]]\n",
      "t [[-1.6745449 ]\n",
      " [-0.49990327]\n",
      " [ 0.24608364]\n",
      " ...\n",
      " [-0.40518813]\n",
      " [-0.48979091]\n",
      " [-0.14207641]]\n",
      "t [[-1.6745449 ]\n",
      " [-0.49990327]\n",
      " [ 0.24608364]\n",
      " ...\n",
      " [-0.40518813]\n",
      " [-0.48979091]\n",
      " [-0.14207641]]\n",
      "Current iteration=8, loss=30869.3916281323\n",
      "t [[-1.77169743]\n",
      " [-0.53646115]\n",
      " [ 0.26149351]\n",
      " ...\n",
      " [-0.43195265]\n",
      " [-0.51960219]\n",
      " [-0.15334379]]\n",
      "t [[-1.77169743]\n",
      " [-0.53646115]\n",
      " [ 0.26149351]\n",
      " ...\n",
      " [-0.43195265]\n",
      " [-0.51960219]\n",
      " [-0.15334379]]\n",
      "t [[-1.85802377]\n",
      " [-0.56953772]\n",
      " [ 0.27443309]\n",
      " ...\n",
      " [-0.45593381]\n",
      " [-0.54622349]\n",
      " [-0.16351727]]\n",
      "loss=30528.8663459411\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.37765252]\n",
      " [-0.08752745]\n",
      " [ 0.022747  ]\n",
      " ...\n",
      " [-0.08043303]\n",
      " [-0.10641817]\n",
      " [-0.02152405]]\n",
      "t [[-0.37765252]\n",
      " [-0.08752745]\n",
      " [ 0.022747  ]\n",
      " ...\n",
      " [-0.08043303]\n",
      " [-0.10641817]\n",
      " [-0.02152405]]\n",
      "t [[-0.67857653]\n",
      " [-0.17074899]\n",
      " [ 0.05691886]\n",
      " ...\n",
      " [-0.14896895]\n",
      " [-0.19321473]\n",
      " [-0.04317568]]\n",
      "t [[-0.67857653]\n",
      " [-0.17074899]\n",
      " [ 0.05691886]\n",
      " ...\n",
      " [-0.14896895]\n",
      " [-0.19321473]\n",
      " [-0.04317568]]\n",
      "Current iteration=2, loss=33877.71333131388\n",
      "t [[-0.92305641]\n",
      " [-0.24570455]\n",
      " [ 0.09290939]\n",
      " ...\n",
      " [-0.20775272]\n",
      " [-0.26494589]\n",
      " [-0.06353329]]\n",
      "t [[-0.92305641]\n",
      " [-0.24570455]\n",
      " [ 0.09290939]\n",
      " ...\n",
      " [-0.20775272]\n",
      " [-0.26494589]\n",
      " [-0.06353329]]\n",
      "t [[-1.12560611]\n",
      " [-0.31202367]\n",
      " [ 0.12658314]\n",
      " ...\n",
      " [-0.25858006]\n",
      " [-0.32514094]\n",
      " [-0.08219386]]\n",
      "t [[-1.12560611]\n",
      " [-0.31202367]\n",
      " [ 0.12658314]\n",
      " ...\n",
      " [-0.25858006]\n",
      " [-0.32514094]\n",
      " [-0.08219386]]\n",
      "Current iteration=4, loss=32183.840475149602\n",
      "t [[-1.29631381]\n",
      " [-0.3705649 ]\n",
      " [ 0.15652135]\n",
      " ...\n",
      " [-0.3028761 ]\n",
      " [-0.37638801]\n",
      " [-0.09914763]]\n",
      "t [[-1.29631381]\n",
      " [-0.3705649 ]\n",
      " [ 0.15652135]\n",
      " ...\n",
      " [-0.3028761 ]\n",
      " [-0.37638801]\n",
      " [-0.09914763]]\n",
      "t [[-1.44226965]\n",
      " [-0.42242995]\n",
      " [ 0.18250694]\n",
      " ...\n",
      " [-0.34175956]\n",
      " [-0.42057302]\n",
      " [-0.11450989]]\n",
      "t [[-1.44226965]\n",
      " [-0.42242995]\n",
      " [ 0.18250694]\n",
      " ...\n",
      " [-0.34175956]\n",
      " [-0.42057302]\n",
      " [-0.11450989]]\n",
      "Current iteration=6, loss=31305.602230483077\n",
      "t [[-1.56857268]\n",
      " [-0.46864027]\n",
      " [ 0.20481177]\n",
      " ...\n",
      " [-0.3761118 ]\n",
      " [-0.45908647]\n",
      " [-0.12842354]]\n",
      "t [[-1.56857268]\n",
      " [-0.46864027]\n",
      " [ 0.20481177]\n",
      " ...\n",
      " [-0.3761118 ]\n",
      " [-0.45908647]\n",
      " [-0.12842354]]\n",
      "t [[-1.67898217]\n",
      " [-0.51006026]\n",
      " [ 0.22387688]\n",
      " ...\n",
      " [-0.40663334]\n",
      " [-0.4929719 ]\n",
      " [-0.14102739]]\n",
      "t [[-1.67898217]\n",
      " [-0.51006026]\n",
      " [ 0.22387688]\n",
      " ...\n",
      " [-0.40663334]\n",
      " [-0.4929719 ]\n",
      " [-0.14102739]]\n",
      "Current iteration=8, loss=30784.4478547883\n",
      "t [[-1.77633512]\n",
      " [-0.5474    ]\n",
      " [ 0.24017263]\n",
      " ...\n",
      " [-0.43388681]\n",
      " [-0.52302719]\n",
      " [-0.15244752]]\n",
      "t [[-1.77633512]\n",
      " [-0.5474    ]\n",
      " [ 0.24017263]\n",
      " ...\n",
      " [-0.43388681]\n",
      " [-0.52302719]\n",
      " [-0.15244752]]\n",
      "t [[-1.86281758]\n",
      " [-0.58123894]\n",
      " [ 0.2541403 ]\n",
      " ...\n",
      " [-0.45832905]\n",
      " [-0.54987329]\n",
      " [-0.16279625]]\n",
      "loss=30441.41753702816\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.37467378]\n",
      " [-0.08149307]\n",
      " [ 0.02667319]\n",
      " ...\n",
      " [ 0.41994021]\n",
      " [-0.30920372]\n",
      " [ 0.34062035]]\n",
      "t [[-0.37467378]\n",
      " [-0.08149307]\n",
      " [ 0.02667319]\n",
      " ...\n",
      " [ 0.41994021]\n",
      " [-0.30920372]\n",
      " [ 0.34062035]]\n",
      "t [[-0.67359138]\n",
      " [-0.16073918]\n",
      " [ 0.06343864]\n",
      " ...\n",
      " [ 0.74109595]\n",
      " [-0.55037256]\n",
      " [ 0.60732067]]\n",
      "t [[-0.67359138]\n",
      " [-0.16073918]\n",
      " [ 0.06343864]\n",
      " ...\n",
      " [ 0.74109595]\n",
      " [-0.55037256]\n",
      " [ 0.60732067]]\n",
      "Current iteration=2, loss=34004.83580806949\n",
      "t [[-0.91684711]\n",
      " [-0.23266492]\n",
      " [ 0.10099442]\n",
      " ...\n",
      " [ 0.98885488]\n",
      " [-0.73974614]\n",
      " [ 0.81838851]]\n",
      "t [[-0.91684711]\n",
      " [-0.23266492]\n",
      " [ 0.10099442]\n",
      " ...\n",
      " [ 0.98885488]\n",
      " [-0.73974614]\n",
      " [ 0.81838851]]\n",
      "t [[-1.11871232]\n",
      " [-0.29643685]\n",
      " [ 0.13550862]\n",
      " ...\n",
      " [ 1.1824208 ]\n",
      " [-0.8902525 ]\n",
      " [ 0.98787621]]\n",
      "t [[-1.11871232]\n",
      " [-0.29643685]\n",
      " [ 0.13550862]\n",
      " ...\n",
      " [ 1.1824208 ]\n",
      " [-0.8902525 ]\n",
      " [ 0.98787621]]\n",
      "Current iteration=4, loss=32356.572468598366\n",
      "t [[-1.28909025]\n",
      " [-0.35271787]\n",
      " [ 0.16578303]\n",
      " ...\n",
      " [ 1.335525  ]\n",
      " [-1.01141203]\n",
      " [ 1.12594859]]\n",
      "t [[-1.28909025]\n",
      " [-0.35271787]\n",
      " [ 0.16578303]\n",
      " ...\n",
      " [ 1.335525  ]\n",
      " [-1.01141203]\n",
      " [ 1.12594859]]\n",
      "t [[-1.4349468 ]\n",
      " [-0.40252076]\n",
      " [ 0.19174931]\n",
      " ...\n",
      " [ 1.45792127]\n",
      " [-1.11010845]\n",
      " [ 1.23989423]]\n",
      "t [[-1.4349468 ]\n",
      " [-0.40252076]\n",
      " [ 0.19174931]\n",
      " ...\n",
      " [ 1.45792127]\n",
      " [-1.11010845]\n",
      " [ 1.23989423]]\n",
      "Current iteration=6, loss=31498.302120128665\n",
      "t [[-1.5613003 ]\n",
      " [-0.44682183]\n",
      " [ 0.21377915]\n",
      " ...\n",
      " [ 1.55662261]\n",
      " [-1.19134776]\n",
      " [ 1.33499668]]\n",
      "t [[-1.5613003 ]\n",
      " [-0.44682183]\n",
      " [ 0.21377915]\n",
      " ...\n",
      " [ 1.55662261]\n",
      " [-1.19134776]\n",
      " [ 1.33499668]]\n",
      "t [[-1.67185774]\n",
      " [-0.48645943]\n",
      " [ 0.23238201]\n",
      " ...\n",
      " [ 1.63676376]\n",
      " [-1.25882411]\n",
      " [ 1.41515337]]\n",
      "t [[-1.67185774]\n",
      " [-0.48645943]\n",
      " [ 0.23238201]\n",
      " ...\n",
      " [ 1.63676376]\n",
      " [-1.25882411]\n",
      " [ 1.41515337]]\n",
      "Current iteration=8, loss=30987.575147032618\n",
      "t [[-1.76942175]\n",
      " [-0.52212671]\n",
      " [ 0.24807637]\n",
      " ...\n",
      " [ 1.70217634]\n",
      " [-1.31531058]\n",
      " [ 1.48329328]]\n",
      "t [[-1.76942175]\n",
      " [-0.52212671]\n",
      " [ 0.24807637]\n",
      " ...\n",
      " [ 1.70217634]\n",
      " [-1.31531058]\n",
      " [ 1.48329328]]\n",
      "t [[-1.85615558]\n",
      " [-0.55439083]\n",
      " [ 0.26133834]\n",
      " ...\n",
      " [ 1.75577099]\n",
      " [-1.36292405]\n",
      " [ 1.54165679]]\n",
      "loss=30650.76297804372\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.32990972]\n",
      " [-0.06940253]\n",
      " [ 0.06888224]\n",
      " ...\n",
      " [-0.08707024]\n",
      " [-0.10825664]\n",
      " [-0.02275798]]\n",
      "t [[-0.32990972]\n",
      " [-0.06940253]\n",
      " [ 0.06888224]\n",
      " ...\n",
      " [-0.08707024]\n",
      " [-0.10825664]\n",
      " [-0.02275798]]\n",
      "t [[-0.57981793]\n",
      " [-0.10393755]\n",
      " [ 0.12786273]\n",
      " ...\n",
      " [-0.16033833]\n",
      " [-0.19604918]\n",
      " [-0.04520088]]\n",
      "t [[-0.57981793]\n",
      " [-0.10393755]\n",
      " [ 0.12786273]\n",
      " ...\n",
      " [-0.16033833]\n",
      " [-0.19604918]\n",
      " [-0.04520088]]\n",
      "Current iteration=2, loss=33880.66258722537\n",
      "t [[-0.76800116]\n",
      " [-0.12278873]\n",
      " [ 0.1750532 ]\n",
      " ...\n",
      " [-0.2224626 ]\n",
      " [-0.26824022]\n",
      " [-0.065989  ]]\n",
      "t [[-0.76800116]\n",
      " [-0.12278873]\n",
      " [ 0.1750532 ]\n",
      " ...\n",
      " [-0.2224626 ]\n",
      " [-0.26824022]\n",
      " [-0.065989  ]]\n",
      "t [[-0.91003501]\n",
      " [-0.13510049]\n",
      " [ 0.21192335]\n",
      " ...\n",
      " [-0.27562526]\n",
      " [-0.32856042]\n",
      " [-0.08483197]]\n",
      "t [[-0.91003501]\n",
      " [-0.13510049]\n",
      " [ 0.21192335]\n",
      " ...\n",
      " [-0.27562526]\n",
      " [-0.32856042]\n",
      " [-0.08483197]]\n",
      "Current iteration=4, loss=32219.689385229634\n",
      "t [[-1.01762137]\n",
      " [-0.14509534]\n",
      " [ 0.24057314]\n",
      " ...\n",
      " [-0.32152385]\n",
      " [-0.37972111]\n",
      " [-0.10180497]]\n",
      "t [[-1.01762137]\n",
      " [-0.14509534]\n",
      " [ 0.24057314]\n",
      " ...\n",
      " [-0.32152385]\n",
      " [-0.37972111]\n",
      " [-0.10180497]]\n",
      "t [[-1.09922936]\n",
      " [-0.1547193 ]\n",
      " [ 0.26285755]\n",
      " ...\n",
      " [-0.36146779]\n",
      " [-0.42368359]\n",
      " [-0.11707876]]\n",
      "t [[-1.09922936]\n",
      " [-0.1547193 ]\n",
      " [ 0.26285755]\n",
      " ...\n",
      " [-0.36146779]\n",
      " [-0.42368359]\n",
      " [-0.11707876]]\n",
      "Current iteration=6, loss=31369.255123484807\n",
      "t [[-1.16097734]\n",
      " [-0.16485208]\n",
      " [ 0.28023106]\n",
      " ...\n",
      " [-0.39647364]\n",
      " [-0.4618857 ]\n",
      " [-0.13083161]]\n",
      "t [[-1.16097734]\n",
      " [-0.16485208]\n",
      " [ 0.28023106]\n",
      " ...\n",
      " [-0.39647364]\n",
      " [-0.4618857 ]\n",
      " [-0.13083161]]\n",
      "t [[-1.20733789]\n",
      " [-0.17585895]\n",
      " [ 0.29379501]\n",
      " ...\n",
      " [-0.42733959]\n",
      " [-0.49540163]\n",
      " [-0.14322527]]\n",
      "t [[-1.20733789]\n",
      " [-0.17585895]\n",
      " [ 0.29379501]\n",
      " ...\n",
      " [-0.42733959]\n",
      " [-0.49540163]\n",
      " [-0.14322527]]\n",
      "Current iteration=8, loss=30868.759703595366\n",
      "t [[-1.24163395]\n",
      " [-0.18785115]\n",
      " [ 0.30437879]\n",
      " ...\n",
      " [-0.45470031]\n",
      " [-0.52504969]\n",
      " [-0.15440119]]\n",
      "t [[-1.24163395]\n",
      " [-0.18785115]\n",
      " [ 0.30437879]\n",
      " ...\n",
      " [-0.45470031]\n",
      " [-0.52504969]\n",
      " [-0.15440119]]\n",
      "t [[-1.26637581]\n",
      " [-0.20081406]\n",
      " [ 0.31261082]\n",
      " ...\n",
      " [-0.4790667 ]\n",
      " [-0.55146482]\n",
      " [-0.16448236]]\n",
      "loss=30541.142791326645\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.38745164]\n",
      " [-0.08728793]\n",
      " [ 0.03409583]\n",
      " ...\n",
      " [-0.08369631]\n",
      " [-0.10881951]\n",
      " [-0.02258706]]\n",
      "t [[-0.38745164]\n",
      " [-0.08728793]\n",
      " [ 0.03409583]\n",
      " ...\n",
      " [-0.08369631]\n",
      " [-0.10881951]\n",
      " [-0.02258706]]\n",
      "t [[-0.69388549]\n",
      " [-0.1710578 ]\n",
      " [ 0.07599765]\n",
      " ...\n",
      " [-0.1540505 ]\n",
      " [-0.19700197]\n",
      " [-0.04525943]]\n",
      "t [[-0.69388549]\n",
      " [-0.1710578 ]\n",
      " [ 0.07599765]\n",
      " ...\n",
      " [-0.1540505 ]\n",
      " [-0.19700197]\n",
      " [-0.04525943]]\n",
      "Current iteration=2, loss=33851.70339804748\n",
      "t [[-0.94143905]\n",
      " [-0.24639853]\n",
      " [ 0.11671767]\n",
      " ...\n",
      " [-0.21375273]\n",
      " [-0.26947378]\n",
      " [-0.06639048]]\n",
      "t [[-0.94143905]\n",
      " [-0.24639853]\n",
      " [ 0.11671767]\n",
      " ...\n",
      " [-0.21375273]\n",
      " [-0.26947378]\n",
      " [-0.06639048]]\n",
      "t [[-1.14564683]\n",
      " [-0.31279164]\n",
      " [ 0.15297861]\n",
      " ...\n",
      " [-0.26494238]\n",
      " [-0.33000941]\n",
      " [-0.08557882]]\n",
      "t [[-1.14564683]\n",
      " [-0.31279164]\n",
      " [ 0.15297861]\n",
      " ...\n",
      " [-0.26494238]\n",
      " [-0.33000941]\n",
      " [-0.08557882]]\n",
      "Current iteration=4, loss=32182.580065248672\n",
      "t [[-1.31715782]\n",
      " [-0.37114678]\n",
      " [ 0.18405716]\n",
      " ...\n",
      " [-0.30925446]\n",
      " [-0.38134757]\n",
      " [-0.10286146]]\n",
      "t [[-1.31715782]\n",
      " [-0.37114678]\n",
      " [ 0.18405716]\n",
      " ...\n",
      " [-0.30925446]\n",
      " [-0.38134757]\n",
      " [-0.10286146]]\n",
      "t [[-1.4633847 ]\n",
      " [-0.42263738]\n",
      " [ 0.21022434]\n",
      " ...\n",
      " [-0.34793697]\n",
      " [-0.4254663 ]\n",
      " [-0.11839878]]\n",
      "t [[-1.4633847 ]\n",
      " [-0.42263738]\n",
      " [ 0.21022434]\n",
      " ...\n",
      " [-0.34793697]\n",
      " [-0.4254663 ]\n",
      " [-0.11839878]]\n",
      "Current iteration=6, loss=31326.613841360588\n",
      "t [[-1.58962163]\n",
      " [-0.46834082]\n",
      " [ 0.23207994]\n",
      " ...\n",
      " [-0.38195253]\n",
      " [-0.4638137 ]\n",
      " [-0.13236881]]\n",
      "t [[-1.58962163]\n",
      " [-0.46834082]\n",
      " [ 0.23207994]\n",
      " ...\n",
      " [-0.38195253]\n",
      " [-0.4638137 ]\n",
      " [-0.13236881]]\n",
      "t [[-1.6997513 ]\n",
      " [-0.50916027]\n",
      " [ 0.2502835 ]\n",
      " ...\n",
      " [-0.4120538 ]\n",
      " [-0.49747026]\n",
      " [-0.14493648]]\n",
      "t [[-1.6997513 ]\n",
      " [-0.50916027]\n",
      " [ 0.2502835 ]\n",
      " ...\n",
      " [-0.4120538 ]\n",
      " [-0.49747026]\n",
      " [-0.14493648]]\n",
      "Current iteration=8, loss=30820.54584199332\n",
      "t [[-1.79669146]\n",
      " [-0.54583219]\n",
      " [ 0.26545152]\n",
      " ...\n",
      " [-0.43883765]\n",
      " [-0.52725817]\n",
      " [-0.15624732]]\n",
      "t [[-1.79669146]\n",
      " [-0.54583219]\n",
      " [ 0.26545152]\n",
      " ...\n",
      " [-0.43883765]\n",
      " [-0.52725817]\n",
      " [-0.15624732]]\n",
      "t [[-1.88268248]\n",
      " [-0.57895418]\n",
      " [ 0.27812383]\n",
      " ...\n",
      " [-0.46278368]\n",
      " [-0.5538147 ]\n",
      " [-0.16642841]]\n",
      "loss=30487.180903495733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.38844259]\n",
      " [-0.09002824]\n",
      " [ 0.02339692]\n",
      " ...\n",
      " [-0.08273111]\n",
      " [-0.10945869]\n",
      " [-0.02213902]]\n",
      "t [[-0.38844259]\n",
      " [-0.09002824]\n",
      " [ 0.02339692]\n",
      " ...\n",
      " [-0.08273111]\n",
      " [-0.10945869]\n",
      " [-0.02213902]]\n",
      "t [[-0.69574975]\n",
      " [-0.1754981 ]\n",
      " [ 0.05887278]\n",
      " ...\n",
      " [-0.15288188]\n",
      " [-0.19816748]\n",
      " [-0.04441303]]\n",
      "t [[-0.69574975]\n",
      " [-0.1754981 ]\n",
      " [ 0.05887278]\n",
      " ...\n",
      " [-0.15288188]\n",
      " [-0.19816748]\n",
      " [-0.04441303]]\n",
      "Current iteration=2, loss=33801.54542912399\n",
      "t [[-0.94400488]\n",
      " [-0.25211219]\n",
      " [ 0.09598736]\n",
      " ...\n",
      " [-0.21279009]\n",
      " [-0.2710881 ]\n",
      " [-0.06527845]]\n",
      "t [[-0.94400488]\n",
      " [-0.25211219]\n",
      " [ 0.09598736]\n",
      " ...\n",
      " [-0.21279009]\n",
      " [-0.2710881 ]\n",
      " [-0.06527845]]\n",
      "t [[-1.14877336]\n",
      " [-0.31958139]\n",
      " [ 0.13042092]\n",
      " ...\n",
      " [-0.26439206]\n",
      " [-0.33201806]\n",
      " [-0.08432844]]\n",
      "t [[-1.14877336]\n",
      " [-0.31958139]\n",
      " [ 0.13042092]\n",
      " ...\n",
      " [-0.26439206]\n",
      " [-0.33201806]\n",
      " [-0.08432844]]\n",
      "Current iteration=4, loss=32111.788212167692\n",
      "t [[-1.32073817]\n",
      " [-0.3789058 ]\n",
      " [ 0.16078827]\n",
      " ...\n",
      " [-0.30921096]\n",
      " [-0.3837096 ]\n",
      " [-0.10157385]]\n",
      "t [[-1.32073817]\n",
      " [-0.3789058 ]\n",
      " [ 0.16078827]\n",
      " ...\n",
      " [-0.30921096]\n",
      " [-0.3837096 ]\n",
      " [-0.10157385]]\n",
      "t [[-1.46733588]\n",
      " [-0.43129916]\n",
      " [ 0.18695365]\n",
      " ...\n",
      " [-0.34843393]\n",
      " [-0.42814877]\n",
      " [-0.11714951]]\n",
      "t [[-1.46733588]\n",
      " [-0.43129916]\n",
      " [ 0.18695365]\n",
      " ...\n",
      " [-0.34843393]\n",
      " [-0.42814877]\n",
      " [-0.11714951]]\n",
      "Current iteration=6, loss=31246.080822767246\n",
      "t [[-1.59387606]\n",
      " [-0.47785972]\n",
      " [ 0.20926626]\n",
      " ...\n",
      " [-0.38299057]\n",
      " [-0.46678871]\n",
      " [-0.13121329]]\n",
      "t [[-1.59387606]\n",
      " [-0.47785972]\n",
      " [ 0.20926626]\n",
      " ...\n",
      " [-0.38299057]\n",
      " [-0.46678871]\n",
      " [-0.13121329]]\n",
      "t [[-1.70425138]\n",
      " [-0.5195031 ]\n",
      " [ 0.22822857]\n",
      " ...\n",
      " [-0.41361586]\n",
      " [-0.50071337]\n",
      " [-0.14391522]]\n",
      "t [[-1.70425138]\n",
      " [-0.5195031 ]\n",
      " [ 0.22822857]\n",
      " ...\n",
      " [-0.41361586]\n",
      " [-0.50071337]\n",
      " [-0.14391522]]\n",
      "Current iteration=8, loss=30735.160955837073\n",
      "t [[-1.80138638]\n",
      " [-0.55697392]\n",
      " [ 0.24435642]\n",
      " ...\n",
      " [-0.44089748]\n",
      " [-0.53074746]\n",
      " [-0.1553901 ]]\n",
      "t [[-1.80138638]\n",
      " [-0.55697392]\n",
      " [ 0.24435642]\n",
      " ...\n",
      " [-0.44089748]\n",
      " [-0.53074746]\n",
      " [-0.1553901 ]]\n",
      "t [[-1.88752642]\n",
      " [-0.5908754 ]\n",
      " [ 0.2581235 ]\n",
      " ...\n",
      " [-0.46531068]\n",
      " [-0.55753026]\n",
      " [-0.16575727]]\n",
      "loss=30399.470551094128\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.38537874]\n",
      " [-0.08382144]\n",
      " [ 0.02743528]\n",
      " ...\n",
      " [ 0.4319385 ]\n",
      " [-0.31803811]\n",
      " [ 0.35035236]]\n",
      "t [[-0.38537874]\n",
      " [-0.08382144]\n",
      " [ 0.02743528]\n",
      " ...\n",
      " [ 0.4319385 ]\n",
      " [-0.31803811]\n",
      " [ 0.35035236]]\n",
      "t [[-0.69064935]\n",
      " [-0.16526171]\n",
      " [ 0.06554025]\n",
      " ...\n",
      " [ 0.75941497]\n",
      " [-0.56412924]\n",
      " [ 0.6225367 ]]\n",
      "t [[-0.69064935]\n",
      " [-0.16526171]\n",
      " [ 0.06554025]\n",
      " ...\n",
      " [ 0.75941497]\n",
      " [-0.56412924]\n",
      " [ 0.6225367 ]]\n",
      "Current iteration=2, loss=33930.97135905463\n",
      "t [[-0.93768788]\n",
      " [-0.23881236]\n",
      " [ 0.10420553]\n",
      " ...\n",
      " [ 1.01006997]\n",
      " [-0.7559573 ]\n",
      " [ 0.83646558]]\n",
      "t [[-0.93768788]\n",
      " [-0.23881236]\n",
      " [ 0.10420553]\n",
      " ...\n",
      " [ 1.01006997]\n",
      " [-0.7559573 ]\n",
      " [ 0.83646558]]\n",
      "t [[-1.14179617]\n",
      " [-0.30370165]\n",
      " [ 0.13944064]\n",
      " ...\n",
      " [ 1.20454665]\n",
      " [-0.90744673]\n",
      " [ 1.0072525 ]]\n",
      "t [[-1.14179617]\n",
      " [-0.30370165]\n",
      " [ 0.13944064]\n",
      " ...\n",
      " [ 1.20454665]\n",
      " [-0.90744673]\n",
      " [ 1.0072525 ]]\n",
      "Current iteration=4, loss=32286.325829694415\n",
      "t [[-1.31346051]\n",
      " [-0.36073303]\n",
      " [ 0.17009511]\n",
      " ...\n",
      " [ 1.3574194 ]\n",
      " [-1.02872464]\n",
      " [ 1.14569484]]\n",
      "t [[-1.31346051]\n",
      " [-0.36073303]\n",
      " [ 0.17009511]\n",
      " ...\n",
      " [ 1.3574194 ]\n",
      " [-1.02872464]\n",
      " [ 1.14569484]]\n",
      "t [[-1.45998787]\n",
      " [-0.41103286]\n",
      " [ 0.19618876]\n",
      " ...\n",
      " [ 1.47893162]\n",
      " [-1.12703438]\n",
      " [ 1.25945472]]\n",
      "t [[-1.45998787]\n",
      " [-0.41103286]\n",
      " [ 0.19618876]\n",
      " ...\n",
      " [ 1.47893162]\n",
      " [-1.12703438]\n",
      " [ 1.25945472]]\n",
      "Current iteration=6, loss=31440.07298621256\n",
      "t [[-1.58660507]\n",
      " [-0.45565536]\n",
      " [ 0.21817375]\n",
      " ...\n",
      " [ 1.5763859 ]\n",
      " [-1.2075972 ]\n",
      " [ 1.35404038]]\n",
      "t [[-1.58660507]\n",
      " [-0.45565536]\n",
      " [ 0.21817375]\n",
      " ...\n",
      " [ 1.5763859 ]\n",
      " [-1.2075972 ]\n",
      " [ 1.35404038]]\n",
      "t [[-1.69715184]\n",
      " [-0.49549057]\n",
      " [ 0.2366224 ]\n",
      " ...\n",
      " [ 1.65509235]\n",
      " [-1.27423882]\n",
      " [ 1.43348793]]\n",
      "t [[-1.69715184]\n",
      " [-0.49549057]\n",
      " [ 0.2366224 ]\n",
      " ...\n",
      " [ 1.65509235]\n",
      " [-1.27423882]\n",
      " [ 1.43348793]]\n",
      "Current iteration=8, loss=30939.244316681325\n",
      "t [[-1.79451844]\n",
      " [-0.53126618]\n",
      " [ 0.25209938]\n",
      " ...\n",
      " [ 1.71899099]\n",
      " [-1.32981439]\n",
      " [ 1.50081399]]\n",
      "t [[-1.79451844]\n",
      " [-0.53126618]\n",
      " [ 0.25209938]\n",
      " ...\n",
      " [ 1.71899099]\n",
      " [-1.32981439]\n",
      " [ 1.50081399]]\n",
      "t [[-1.88092761]\n",
      " [-0.56357305]\n",
      " [ 0.26511349]\n",
      " ...\n",
      " [ 1.77106035]\n",
      " [-1.37649287]\n",
      " [ 1.55831533]]\n",
      "loss=30609.56077863396\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.33907388]\n",
      " [-0.07133038]\n",
      " [ 0.07079563]\n",
      " ...\n",
      " [-0.08948886]\n",
      " [-0.11126377]\n",
      " [-0.02339014]]\n",
      "t [[-0.33907388]\n",
      " [-0.07133038]\n",
      " [ 0.07079563]\n",
      " ...\n",
      " [-0.08948886]\n",
      " [-0.11126377]\n",
      " [-0.02339014]]\n",
      "t [[-0.59367465]\n",
      " [-0.10584659]\n",
      " [ 0.1311333 ]\n",
      " ...\n",
      " [-0.16440574]\n",
      " [-0.20092014]\n",
      " [-0.04644777]]\n",
      "t [[-0.59367465]\n",
      " [-0.10584659]\n",
      " [ 0.1311333 ]\n",
      " ...\n",
      " [-0.16440574]\n",
      " [-0.20092014]\n",
      " [-0.04644777]]\n",
      "Current iteration=2, loss=33807.337481988376\n",
      "t [[-0.78366038]\n",
      " [-0.12435556]\n",
      " [ 0.17897512]\n",
      " ...\n",
      " [-0.22764051]\n",
      " [-0.27425173]\n",
      " [-0.06772196]]\n",
      "t [[-0.78366038]\n",
      " [-0.12435556]\n",
      " [ 0.17897512]\n",
      " ...\n",
      " [-0.22764051]\n",
      " [-0.27425173]\n",
      " [-0.06772196]]\n",
      "t [[-0.92581107]\n",
      " [-0.13646234]\n",
      " [ 0.21601217]\n",
      " ...\n",
      " [-0.2815393 ]\n",
      " [-0.33526301]\n",
      " [-0.08692862]]\n",
      "t [[-0.92581107]\n",
      " [-0.13646234]\n",
      " [ 0.21601217]\n",
      " ...\n",
      " [-0.2815393 ]\n",
      " [-0.33526301]\n",
      " [-0.08692862]]\n",
      "Current iteration=4, loss=32151.38469717876\n",
      "t [[-1.0325707 ]\n",
      " [-0.14646887]\n",
      " [ 0.24454902]\n",
      " ...\n",
      " [-0.32791067]\n",
      " [-0.38683051]\n",
      " [-0.1041681 ]]\n",
      "t [[-1.0325707 ]\n",
      " [-0.14646887]\n",
      " [ 0.24454902]\n",
      " ...\n",
      " [-0.32791067]\n",
      " [-0.38683051]\n",
      " [-0.1041681 ]]\n",
      "t [[-1.11283975]\n",
      " [-0.15629885]\n",
      " [ 0.26657144]\n",
      " ...\n",
      " [-0.36813955]\n",
      " [-0.43101527]\n",
      " [-0.11963228]]\n",
      "t [[-1.11283975]\n",
      " [-0.15629885]\n",
      " [ 0.26657144]\n",
      " ...\n",
      " [-0.36813955]\n",
      " [-0.43101527]\n",
      " [-0.11963228]]\n",
      "Current iteration=6, loss=31313.405301260682\n",
      "t [[-1.17298668]\n",
      " [-0.16678991]\n",
      " [ 0.28360991]\n",
      " ...\n",
      " [-0.40329465]\n",
      " [-0.46931698]\n",
      " [-0.13351473]]\n",
      "t [[-1.17298668]\n",
      " [-0.16678991]\n",
      " [ 0.28360991]\n",
      " ...\n",
      " [-0.40329465]\n",
      " [-0.46931698]\n",
      " [-0.13351473]]\n",
      "t [[-1.21763137]\n",
      " [-0.17826731]\n",
      " [ 0.29680892]\n",
      " ...\n",
      " [-0.43421077]\n",
      " [-0.50284943]\n",
      " [-0.14598834]]\n",
      "t [[-1.21763137]\n",
      " [-0.17826731]\n",
      " [ 0.29680892]\n",
      " ...\n",
      " [-0.43421077]\n",
      " [-0.50284943]\n",
      " [-0.14598834]]\n",
      "Current iteration=8, loss=30822.831747702985\n",
      "t [[-1.25018496]\n",
      " [-0.19080917]\n",
      " [ 0.30702233]\n",
      " ...\n",
      " [-0.46154879]\n",
      " [-0.53245711]\n",
      " [-0.15720311]]\n",
      "t [[-1.25018496]\n",
      " [-0.19080917]\n",
      " [ 0.30702233]\n",
      " ...\n",
      " [-0.46154879]\n",
      " [-0.53245711]\n",
      " [-0.15720311]]\n",
      "t [[-1.27321113]\n",
      " [-0.2043746 ]\n",
      " [ 0.31489246]\n",
      " ...\n",
      " [-0.48583862]\n",
      " [-0.55879269]\n",
      " [-0.16728887]]\n",
      "loss=30502.23677677487\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.39821418]\n",
      " [-0.0897126 ]\n",
      " [ 0.03504294]\n",
      " ...\n",
      " [-0.08602121]\n",
      " [-0.11184228]\n",
      " [-0.02321448]]\n",
      "t [[-0.39821418]\n",
      " [-0.0897126 ]\n",
      " [ 0.03504294]\n",
      " ...\n",
      " [-0.08602121]\n",
      " [-0.11184228]\n",
      " [-0.02321448]]\n",
      "t [[-0.71088985]\n",
      " [-0.17570568]\n",
      " [ 0.07832555]\n",
      " ...\n",
      " [-0.15795602]\n",
      " [-0.20189461]\n",
      " [-0.04651915]]\n",
      "t [[-0.71088985]\n",
      " [-0.17570568]\n",
      " [ 0.07832555]\n",
      " ...\n",
      " [-0.15795602]\n",
      " [-0.20189461]\n",
      " [-0.04651915]]\n",
      "Current iteration=2, loss=33777.96604811289\n",
      "t [[-0.96206911]\n",
      " [-0.25266102]\n",
      " [ 0.12010487]\n",
      " ...\n",
      " [-0.21872846]\n",
      " [-0.2755088 ]\n",
      " [-0.06815195]]\n",
      "t [[-0.96206911]\n",
      " [-0.25266102]\n",
      " [ 0.12010487]\n",
      " ...\n",
      " [-0.21872846]\n",
      " [-0.2755088 ]\n",
      " [-0.06815195]]\n",
      "t [[-1.16836612]\n",
      " [-0.32014963]\n",
      " [ 0.15700102]\n",
      " ...\n",
      " [-0.27063639]\n",
      " [-0.33673612]\n",
      " [-0.0877137 ]]\n",
      "t [[-1.16836612]\n",
      " [-0.32014963]\n",
      " [ 0.15700102]\n",
      " ...\n",
      " [-0.27063639]\n",
      " [-0.33673612]\n",
      " [-0.0877137 ]]\n",
      "Current iteration=4, loss=32113.929549109584\n",
      "t [[-1.3410287 ]\n",
      " [-0.37923313]\n",
      " [ 0.18837013]\n",
      " ...\n",
      " [-0.31541973]\n",
      " [-0.38848181]\n",
      " [-0.10526747]]\n",
      "t [[-1.3410287 ]\n",
      " [-0.37923313]\n",
      " [ 0.18837013]\n",
      " ...\n",
      " [-0.31541973]\n",
      " [-0.38848181]\n",
      " [-0.10526747]]\n",
      "t [[-1.48781386]\n",
      " [-0.43120129]\n",
      " [ 0.21458515]\n",
      " ...\n",
      " [-0.35439708]\n",
      " [-0.43282419]\n",
      " [-0.12099614]]\n",
      "t [[-1.48781386]\n",
      " [-0.43120129]\n",
      " [ 0.21458515]\n",
      " ...\n",
      " [-0.35439708]\n",
      " [-0.43282419]\n",
      " [-0.12099614]]\n",
      "Current iteration=6, loss=31270.309638568047\n",
      "t [[-1.61422317]\n",
      " [-0.47720916]\n",
      " [ 0.23633086]\n",
      " ...\n",
      " [-0.38857952]\n",
      " [-0.47127333]\n",
      " [-0.13509412]]\n",
      "t [[-1.61422317]\n",
      " [-0.47720916]\n",
      " [ 0.23633086]\n",
      " ...\n",
      " [-0.38857952]\n",
      " [-0.47127333]\n",
      " [-0.13509412]]\n",
      "t [[-1.72426973]\n",
      " [-0.51821073]\n",
      " [ 0.25432993]\n",
      " ...\n",
      " [-0.4187537 ]\n",
      " [-0.50494935]\n",
      " [-0.1477382 ]]\n",
      "t [[-1.72426973]\n",
      " [-0.51821073]\n",
      " [ 0.25432993]\n",
      " ...\n",
      " [-0.4187537 ]\n",
      " [-0.50494935]\n",
      " [-0.1477382 ]]\n",
      "Current iteration=8, loss=30773.98811570521\n",
      "t [[-1.82095634]\n",
      " [-0.55497629]\n",
      " [ 0.26924356]\n",
      " ...\n",
      " [-0.44554076]\n",
      " [-0.53470053]\n",
      " [-0.15908297]]\n",
      "t [[-1.82095634]\n",
      " [-0.55497629]\n",
      " [ 0.26924356]\n",
      " ...\n",
      " [-0.44554076]\n",
      " [-0.53470053]\n",
      " [-0.15908297]]\n",
      "t [[-1.90658051]\n",
      " [-0.58812656]\n",
      " [ 0.28164225]\n",
      " ...\n",
      " [-0.46943796]\n",
      " [-0.56118183]\n",
      " [-0.16926269]]\n",
      "loss=30447.474476500953\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.39923266]\n",
      " [-0.09252902]\n",
      " [ 0.02404683]\n",
      " ...\n",
      " [-0.0850292 ]\n",
      " [-0.11249921]\n",
      " [-0.02275399]]\n",
      "t [[-0.39923266]\n",
      " [-0.09252902]\n",
      " [ 0.02404683]\n",
      " ...\n",
      " [-0.0850292 ]\n",
      " [-0.11249921]\n",
      " [-0.02275399]]\n",
      "t [[-0.71280335]\n",
      " [-0.18023977]\n",
      " [ 0.06084422]\n",
      " ...\n",
      " [-0.15677626]\n",
      " [-0.20308947]\n",
      " [-0.0456506 ]]\n",
      "t [[-0.71280335]\n",
      " [-0.18023977]\n",
      " [ 0.06084422]\n",
      " ...\n",
      " [-0.15677626]\n",
      " [-0.20308947]\n",
      " [-0.0456506 ]]\n",
      "Current iteration=2, loss=33726.86413301793\n",
      "t [[-0.96469564]\n",
      " [-0.25848006]\n",
      " [ 0.09907255]\n",
      " ...\n",
      " [-0.21778287]\n",
      " [-0.27716097]\n",
      " [-0.06701771]]\n",
      "t [[-0.96469564]\n",
      " [-0.25848006]\n",
      " [ 0.09907255]\n",
      " ...\n",
      " [-0.21778287]\n",
      " [-0.27716097]\n",
      " [-0.06701771]]\n",
      "t [[-1.17155865]\n",
      " [-0.32705842]\n",
      " [ 0.1342367 ]\n",
      " ...\n",
      " [-0.2701315 ]\n",
      " [-0.33878941]\n",
      " [-0.08644777]]\n",
      "t [[-1.17155865]\n",
      " [-0.32705842]\n",
      " [ 0.1342367 ]\n",
      " ...\n",
      " [-0.2701315 ]\n",
      " [-0.33878941]\n",
      " [-0.08644777]]\n",
      "Current iteration=4, loss=32042.3018582491\n",
      "t [[-1.34467685]\n",
      " [-0.38712669]\n",
      " [ 0.1649979 ]\n",
      " ...\n",
      " [-0.31544617]\n",
      " [-0.39089404]\n",
      " [-0.10397441]]\n",
      "t [[-1.34467685]\n",
      " [-0.38712669]\n",
      " [ 0.1649979 ]\n",
      " ...\n",
      " [-0.31544617]\n",
      " [-0.39089404]\n",
      " [-0.10397441]]\n",
      "t [[-1.49183229]\n",
      " [-0.44001399]\n",
      " [ 0.19130963]\n",
      " ...\n",
      " [-0.35498372]\n",
      " [-0.4355614 ]\n",
      " [-0.11975294]]\n",
      "t [[-1.49183229]\n",
      " [-0.44001399]\n",
      " [ 0.19130963]\n",
      " ...\n",
      " [-0.35498372]\n",
      " [-0.4355614 ]\n",
      " [-0.11975294]]\n",
      "Current iteration=6, loss=31189.1612270427\n",
      "t [[-1.61854235]\n",
      " [-0.48689573]\n",
      " [ 0.21360212]\n",
      " ...\n",
      " [-0.38972251]\n",
      " [-0.47430671]\n",
      " [-0.13395649]]\n",
      "t [[-1.61854235]\n",
      " [-0.48689573]\n",
      " [ 0.21360212]\n",
      " ...\n",
      " [-0.38972251]\n",
      " [-0.47430671]\n",
      " [-0.13395649]]\n",
      "t [[-1.72883028]\n",
      " [-0.52873817]\n",
      " [ 0.23244054]\n",
      " ...\n",
      " [-0.42043209]\n",
      " [-0.5082536 ]\n",
      " [-0.14674647]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.72883028]\n",
      " [-0.52873817]\n",
      " [ 0.23244054]\n",
      " ...\n",
      " [-0.42043209]\n",
      " [-0.5082536 ]\n",
      " [-0.14674647]]\n",
      "Current iteration=8, loss=30688.19572347676\n",
      "t [[-1.82570577]\n",
      " [-0.5663197 ]\n",
      " [ 0.24838622]\n",
      " ...\n",
      " [-0.44772504]\n",
      " [-0.53825297]\n",
      " [-0.15826648]]\n",
      "t [[-1.82570577]\n",
      " [-0.5663197 ]\n",
      " [ 0.24838622]\n",
      " ...\n",
      " [-0.44772504]\n",
      " [-0.53825297]\n",
      " [-0.15826648]]\n",
      "t [[-1.91147142]\n",
      " [-0.60026665]\n",
      " [ 0.26194461]\n",
      " ...\n",
      " [-0.47209482]\n",
      " [-0.56496181]\n",
      " [-0.168643  ]]\n",
      "loss=30359.532390063527\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.39608371]\n",
      " [-0.08614982]\n",
      " [ 0.02819737]\n",
      " ...\n",
      " [ 0.44393679]\n",
      " [-0.3268725 ]\n",
      " [ 0.36008437]]\n",
      "t [[-0.39608371]\n",
      " [-0.08614982]\n",
      " [ 0.02819737]\n",
      " ...\n",
      " [ 0.44393679]\n",
      " [-0.3268725 ]\n",
      " [ 0.36008437]]\n",
      "t [[-0.70758909]\n",
      " [-0.16978003]\n",
      " [ 0.06765729]\n",
      " ...\n",
      " [ 0.77757944]\n",
      " [-0.57777922]\n",
      " [ 0.63763715]]\n",
      "t [[-0.70758909]\n",
      " [-0.16978003]\n",
      " [ 0.06765729]\n",
      " ...\n",
      " [ 0.77757944]\n",
      " [-0.57777922]\n",
      " [ 0.63763715]]\n",
      "Current iteration=2, loss=33858.53671327036\n",
      "t [[-0.95827415]\n",
      " [-0.2449242 ]\n",
      " [ 0.10741904]\n",
      " ...\n",
      " [ 1.03094845]\n",
      " [-0.77193021]\n",
      " [ 0.8542877 ]]\n",
      "t [[-0.95827415]\n",
      " [-0.2449242 ]\n",
      " [ 0.10741904]\n",
      " ...\n",
      " [ 1.03094845]\n",
      " [-0.77193021]\n",
      " [ 0.8542877 ]]\n",
      "t [[-1.16450252]\n",
      " [-0.31088994]\n",
      " [ 0.14334377]\n",
      " ...\n",
      " [ 1.22617713]\n",
      " [-0.92428486]\n",
      " [ 1.026249  ]]\n",
      "t [[-1.16450252]\n",
      " [-0.31088994]\n",
      " [ 0.14334377]\n",
      " ...\n",
      " [ 1.22617713]\n",
      " [-0.92428486]\n",
      " [ 1.026249  ]]\n",
      "Current iteration=4, loss=32218.564310523667\n",
      "t [[-1.33735004]\n",
      " [-0.36863231]\n",
      " [ 0.17434185]\n",
      " ...\n",
      " [ 1.37869507]\n",
      " [-1.04558774]\n",
      " [ 1.16496104]]\n",
      "t [[-1.33735004]\n",
      " [-0.36863231]\n",
      " [ 0.17434185]\n",
      " ...\n",
      " [ 1.37869507]\n",
      " [-1.04558774]\n",
      " [ 1.16496104]]\n",
      "t [[-1.48446405]\n",
      " [-0.41939491]\n",
      " [ 0.2005289 ]\n",
      " ...\n",
      " [ 1.49923378]\n",
      " [-1.14344163]\n",
      " [ 1.27845906]]\n",
      "t [[-1.48446405]\n",
      " [-0.41939491]\n",
      " [ 0.2005289 ]\n",
      " ...\n",
      " [ 1.49923378]\n",
      " [-1.14344163]\n",
      " [ 1.27845906]]\n",
      "Current iteration=6, loss=31384.37514281951\n",
      "t [[-1.61127733]\n",
      " [-0.46431023]\n",
      " [ 0.22244106]\n",
      " ...\n",
      " [ 1.59538025]\n",
      " [-1.22327959]\n",
      " [ 1.37247269]]\n",
      "t [[-1.61127733]\n",
      " [-0.46431023]\n",
      " [ 0.22244106]\n",
      " ...\n",
      " [ 1.59538025]\n",
      " [-1.22327959]\n",
      " [ 1.37247269]]\n",
      "t [[-1.72175972]\n",
      " [-0.5043193 ]\n",
      " [ 0.24071467]\n",
      " ...\n",
      " [ 1.67261457]\n",
      " [-1.2890551 ]\n",
      " [ 1.45117305]]\n",
      "t [[-1.72175972]\n",
      " [-0.5043193 ]\n",
      " [ 0.24071467]\n",
      " ...\n",
      " [ 1.67261457]\n",
      " [-1.2890551 ]\n",
      " [ 1.45117305]]\n",
      "Current iteration=8, loss=30893.178977092248\n",
      "t [[-1.81888673]\n",
      " [-0.54018359]\n",
      " [ 0.25596042]\n",
      " ...\n",
      " [ 1.73498049]\n",
      " [-1.34370208]\n",
      " [ 1.5176611 ]]\n",
      "t [[-1.81888673]\n",
      " [-0.54018359]\n",
      " [ 0.25596042]\n",
      " ...\n",
      " [ 1.73498049]\n",
      " [-1.34370208]\n",
      " [ 1.5176611 ]]\n",
      "t [[-1.90493867]\n",
      " [-0.57251686]\n",
      " [ 0.26871915]\n",
      " ...\n",
      " [ 1.78552057]\n",
      " [-1.38943874]\n",
      " [ 1.57428727]]\n",
      "loss=30570.323333745007\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.34823803]\n",
      " [-0.07325823]\n",
      " [ 0.07270903]\n",
      " ...\n",
      " [-0.09190747]\n",
      " [-0.1142709 ]\n",
      " [-0.02402231]]\n",
      "t [[-0.34823803]\n",
      " [-0.07325823]\n",
      " [ 0.07270903]\n",
      " ...\n",
      " [-0.09190747]\n",
      " [-0.1142709 ]\n",
      " [-0.02402231]]\n",
      "t [[-0.60741266]\n",
      " [-0.10770418]\n",
      " [ 0.13438881]\n",
      " ...\n",
      " [-0.16845289]\n",
      " [-0.2057608 ]\n",
      " [-0.04769422]]\n",
      "t [[-0.60741266]\n",
      " [-0.10770418]\n",
      " [ 0.13438881]\n",
      " ...\n",
      " [-0.16845289]\n",
      " [-0.2057608 ]\n",
      " [-0.04769422]]\n",
      "Current iteration=2, loss=33735.43912591775\n",
      "t [[-0.79905087]\n",
      " [-0.12585441]\n",
      " [ 0.18284499]\n",
      " ...\n",
      " [-0.23277042]\n",
      " [-0.28019543]\n",
      " [-0.06944785]]\n",
      "t [[-0.79905087]\n",
      " [-0.12585441]\n",
      " [ 0.18284499]\n",
      " ...\n",
      " [-0.23277042]\n",
      " [-0.28019543]\n",
      " [-0.06944785]]\n",
      "t [[-0.94118699]\n",
      " [-0.13776619]\n",
      " [ 0.22001137]\n",
      " ...\n",
      " [-0.28737626]\n",
      " [-0.34186273]\n",
      " [-0.08900879]]\n",
      "t [[-0.94118699]\n",
      " [-0.13776619]\n",
      " [ 0.22001137]\n",
      " ...\n",
      " [-0.28737626]\n",
      " [-0.34186273]\n",
      " [-0.08900879]]\n",
      "Current iteration=4, loss=32085.49558245899\n",
      "t [[-1.04702059]\n",
      " [-0.14780617]\n",
      " [ 0.24840619]\n",
      " ...\n",
      " [-0.33419282]\n",
      " [-0.39380715]\n",
      " [-0.1065047 ]]\n",
      "t [[-1.04702059]\n",
      " [-0.14780617]\n",
      " [ 0.24840619]\n",
      " ...\n",
      " [-0.33419282]\n",
      " [-0.39380715]\n",
      " [-0.1065047 ]]\n",
      "t [[-1.12588217]\n",
      " [-0.15786713]\n",
      " [ 0.27014695]\n",
      " ...\n",
      " [-0.37468175]\n",
      " [-0.43818965]\n",
      " [-0.12214929]]\n",
      "t [[-1.12588217]\n",
      " [-0.15786713]\n",
      " [ 0.27014695]\n",
      " ...\n",
      " [-0.37468175]\n",
      " [-0.43818965]\n",
      " [-0.12214929]]\n",
      "Current iteration=6, loss=31259.97233970787\n",
      "t [[-1.18438511]\n",
      " [-0.16874038]\n",
      " [ 0.28683856]\n",
      " ...\n",
      " [-0.40996428]\n",
      " [-0.47657116]\n",
      " [-0.13615163]]\n",
      "t [[-1.18438511]\n",
      " [-0.16874038]\n",
      " [ 0.28683856]\n",
      " ...\n",
      " [-0.40996428]\n",
      " [-0.47657116]\n",
      " [-0.13615163]]\n",
      "t [[-1.22729128]\n",
      " [-0.18070925]\n",
      " [ 0.29966676]\n",
      " ...\n",
      " [-0.44091185]\n",
      " [-0.51010436]\n",
      " [-0.14869586]]\n",
      "t [[-1.22729128]\n",
      " [-0.18070925]\n",
      " [ 0.29966676]\n",
      " ...\n",
      " [-0.44091185]\n",
      " [-0.51010436]\n",
      " [-0.14869586]]\n",
      "Current iteration=8, loss=30779.039871918703\n",
      "t [[-1.2580956 ]\n",
      " [-0.19381784]\n",
      " [ 0.30950833]\n",
      " ...\n",
      " [-0.46821131]\n",
      " [-0.53965921]\n",
      " [-0.15994057]]\n",
      "t [[-1.2580956 ]\n",
      " [-0.19381784]\n",
      " [ 0.30950833]\n",
      " ...\n",
      " [-0.46821131]\n",
      " [-0.53965921]\n",
      " [-0.15994057]]\n",
      "t [[-1.2794116 ]\n",
      " [-0.20799877]\n",
      " [ 0.31701832]\n",
      " ...\n",
      " [-0.49241135]\n",
      " [-0.56590557]\n",
      " [-0.17002255]]\n",
      "loss=30465.165949744238\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.40897673]\n",
      " [-0.09213726]\n",
      " [ 0.03599005]\n",
      " ...\n",
      " [-0.08834611]\n",
      " [-0.11486504]\n",
      " [-0.0238419 ]]\n",
      "t [[-0.40897673]\n",
      " [-0.09213726]\n",
      " [ 0.03599005]\n",
      " ...\n",
      " [-0.08834611]\n",
      " [-0.11486504]\n",
      " [-0.0238419 ]]\n",
      "t [[-0.72777503]\n",
      " [-0.18034768]\n",
      " [ 0.08066468]\n",
      " ...\n",
      " [-0.16184194]\n",
      " [-0.2067567 ]\n",
      " [-0.04777902]]\n",
      "t [[-0.72777503]\n",
      " [-0.18034768]\n",
      " [ 0.08066468]\n",
      " ...\n",
      " [-0.16184194]\n",
      " [-0.2067567 ]\n",
      " [-0.04777902]]\n",
      "Current iteration=2, loss=33705.669255555025\n",
      "t [[-0.98244538]\n",
      " [-0.25888524]\n",
      " [ 0.12348611]\n",
      " ...\n",
      " [-0.22365829]\n",
      " [-0.28147557]\n",
      " [-0.06990681]]\n",
      "t [[-0.98244538]\n",
      " [-0.25888524]\n",
      " [ 0.12348611]\n",
      " ...\n",
      " [-0.22365829]\n",
      " [-0.28147557]\n",
      " [-0.06990681]]\n",
      "t [[-1.19071228]\n",
      " [-0.32742876]\n",
      " [ 0.16098452]\n",
      " ...\n",
      " [-0.27625714]\n",
      " [-0.34335942]\n",
      " [-0.08983212]]\n",
      "t [[-1.19071228]\n",
      " [-0.32742876]\n",
      " [ 0.16098452]\n",
      " ...\n",
      " [-0.27625714]\n",
      " [-0.34335942]\n",
      " [-0.08983212]]\n",
      "Current iteration=4, loss=32047.7030391815\n",
      "t [[-1.36442789]\n",
      " [-0.38720232]\n",
      " [ 0.19260837]\n",
      " ...\n",
      " [-0.32148587]\n",
      " [-0.39548274]\n",
      " [-0.10764643]]\n",
      "t [[-1.36442789]\n",
      " [-0.38720232]\n",
      " [ 0.19260837]\n",
      " ...\n",
      " [-0.32148587]\n",
      " [-0.39548274]\n",
      " [-0.10764643]]\n",
      "t [[-1.51169218]\n",
      " [-0.43961508]\n",
      " [ 0.21883943]\n",
      " ...\n",
      " [-0.36073466]\n",
      " [-0.44002428]\n",
      " [-0.123556  ]]\n",
      "t [[-1.51169218]\n",
      " [-0.43961508]\n",
      " [ 0.21883943]\n",
      " ...\n",
      " [-0.36073466]\n",
      " [-0.44002428]\n",
      " [-0.123556  ]]\n",
      "Current iteration=6, loss=31216.422378765463\n",
      "t [[-1.63821125]\n",
      " [-0.48589981]\n",
      " [ 0.2404502 ]\n",
      " ...\n",
      " [-0.39506342]\n",
      " [-0.4785555 ]\n",
      " [-0.1377718 ]]\n",
      "t [[-1.63821125]\n",
      " [-0.48589981]\n",
      " [ 0.2404502 ]\n",
      " ...\n",
      " [-0.39506342]\n",
      " [-0.4785555 ]\n",
      " [-0.1377718 ]]\n",
      "t [[-1.74812576]\n",
      " [-0.5270606 ]\n",
      " [ 0.25822708]\n",
      " ...\n",
      " [-0.42529275]\n",
      " [-0.51223534]\n",
      " [-0.15048262]]\n",
      "t [[-1.74812576]\n",
      " [-0.5270606 ]\n",
      " [ 0.25822708]\n",
      " ...\n",
      " [-0.42529275]\n",
      " [-0.51223534]\n",
      " [-0.15048262]]\n",
      "Current iteration=8, loss=30729.56410430263\n",
      "t [[-1.84452113]\n",
      " [-0.56390075]\n",
      " [ 0.27287555]\n",
      " ...\n",
      " [-0.45206788]\n",
      " [-0.54193752]\n",
      " [-0.16185211]]\n",
      "t [[-1.84452113]\n",
      " [-0.56390075]\n",
      " [ 0.27287555]\n",
      " ...\n",
      " [-0.45206788]\n",
      " [-0.54193752]\n",
      " [-0.16185211]]\n",
      "t [[-1.92975002]\n",
      " [-0.59706341]\n",
      " [ 0.28499587]\n",
      " ...\n",
      " [-0.47590348]\n",
      " [-0.56833404]\n",
      " [-0.17202184]]\n",
      "loss=30409.603327804532\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.41002273]\n",
      " [-0.09502981]\n",
      " [ 0.02469674]\n",
      " ...\n",
      " [-0.08732728]\n",
      " [-0.11553973]\n",
      " [-0.02336896]]\n",
      "t [[-0.41002273]\n",
      " [-0.09502981]\n",
      " [ 0.02469674]\n",
      " ...\n",
      " [-0.08732728]\n",
      " [-0.11553973]\n",
      " [-0.02336896]]\n",
      "t [[-0.72973768]\n",
      " [-0.184974  ]\n",
      " [ 0.06283311]\n",
      " ...\n",
      " [-0.16065216]\n",
      " [-0.20798075]\n",
      " [-0.04688838]]\n",
      "t [[-0.72973768]\n",
      " [-0.184974  ]\n",
      " [ 0.06283311]\n",
      " ...\n",
      " [-0.16065216]\n",
      " [-0.20798075]\n",
      " [-0.04688838]]\n",
      "Current iteration=2, loss=33653.64249223531\n",
      "t [[-0.98513211]\n",
      " [-0.26480777]\n",
      " [ 0.10216377]\n",
      " ...\n",
      " [-0.22273149]\n",
      " [-0.2831653 ]\n",
      " [-0.06875094]]\n",
      "t [[-0.98513211]\n",
      " [-0.26480777]\n",
      " [ 0.10216377]\n",
      " ...\n",
      " [-0.22273149]\n",
      " [-0.2831653 ]\n",
      " [-0.06875094]]\n",
      "t [[-1.1939699 ]\n",
      " [-0.33445494]\n",
      " [ 0.13802869]\n",
      " ...\n",
      " [-0.27579945]\n",
      " [-0.34545694]\n",
      " [-0.08855174]]\n",
      "t [[-1.1939699 ]\n",
      " [-0.33445494]\n",
      " [ 0.13802869]\n",
      " ...\n",
      " [-0.27579945]\n",
      " [-0.34545694]\n",
      " [-0.08855174]]\n",
      "Current iteration=4, loss=31975.271399610938\n",
      "t [[-1.36814258]\n",
      " [-0.395229  ]\n",
      " [ 0.16914887]\n",
      " ...\n",
      " [-0.3215836 ]\n",
      " [-0.39794466]\n",
      " [-0.10634938]]\n",
      "t [[-1.36814258]\n",
      " [-0.395229  ]\n",
      " [ 0.16914887]\n",
      " ...\n",
      " [-0.3215836 ]\n",
      " [-0.39794466]\n",
      " [-0.10634938]]\n",
      "t [[-1.51577627]\n",
      " [-0.44857741]\n",
      " [ 0.19557463]\n",
      " ...\n",
      " [-0.36141175]\n",
      " [-0.44281559]\n",
      " [-0.12232049]]\n",
      "t [[-1.51577627]\n",
      " [-0.44857741]\n",
      " [ 0.19557463]\n",
      " ...\n",
      " [-0.36141175]\n",
      " [-0.44281559]\n",
      " [-0.12232049]]\n",
      "Current iteration=6, loss=31134.692251545428\n",
      "t [[-1.64259325]\n",
      " [-0.49575286]\n",
      " [ 0.21782067]\n",
      " ...\n",
      " [-0.39631142]\n",
      " [-0.48164645]\n",
      " [-0.13665376]]\n",
      "t [[-1.64259325]\n",
      " [-0.49575286]\n",
      " [ 0.21782067]\n",
      " ...\n",
      " [-0.39631142]\n",
      " [-0.48164645]\n",
      " [-0.13665376]]\n",
      "t [[-1.75274449]\n",
      " [-0.53777152]\n",
      " [ 0.2365159 ]\n",
      " ...\n",
      " [-0.42708683]\n",
      " [-0.51559978]\n",
      " [-0.14952208]]\n",
      "t [[-1.75274449]\n",
      " [-0.53777152]\n",
      " [ 0.2365159 ]\n",
      " ...\n",
      " [-0.42708683]\n",
      " [-0.51559978]\n",
      " [-0.14952208]]\n",
      "Current iteration=8, loss=30643.395734853955\n",
      "t [[-1.84932237]\n",
      " [-0.57544473]\n",
      " [ 0.25226694]\n",
      " ...\n",
      " [-0.45437528]\n",
      " [-0.54555196]\n",
      " [-0.16107794]]\n",
      "t [[-1.84932237]\n",
      " [-0.57544473]\n",
      " [ 0.25226694]\n",
      " ...\n",
      " [-0.45437528]\n",
      " [-0.54555196]\n",
      " [-0.16107794]]\n",
      "t [[-1.93468479]\n",
      " [-0.60942127]\n",
      " [ 0.26561019]\n",
      " ...\n",
      " [-0.47868822]\n",
      " [-0.57217713]\n",
      " [-0.17145507]]\n",
      "loss=30321.457185447936\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.40678867]\n",
      " [-0.08847819]\n",
      " [ 0.02895946]\n",
      " ...\n",
      " [ 0.45593508]\n",
      " [-0.3357069 ]\n",
      " [ 0.36981638]]\n",
      "t [[-0.40678867]\n",
      " [-0.08847819]\n",
      " [ 0.02895946]\n",
      " ...\n",
      " [ 0.45593508]\n",
      " [-0.3357069 ]\n",
      " [ 0.36981638]]\n",
      "t [[-0.72441094]\n",
      " [-0.17429409]\n",
      " [ 0.06978969]\n",
      " ...\n",
      " [ 0.7955898 ]\n",
      " [-0.59132279]\n",
      " [ 0.65262235]]\n",
      "t [[-0.72441094]\n",
      " [-0.17429409]\n",
      " [ 0.06978969]\n",
      " ...\n",
      " [ 0.7955898 ]\n",
      " [-0.59132279]\n",
      " [ 0.65262235]]\n",
      "Current iteration=2, loss=33787.50616626859\n",
      "t [[-0.97860931]\n",
      " [-0.25099992]\n",
      " [ 0.11063379]\n",
      " ...\n",
      " [ 1.05149462]\n",
      " [-0.78766767]\n",
      " [ 0.87185804]]\n",
      "t [[-0.97860931]\n",
      " [-0.25099992]\n",
      " [ 0.11063379]\n",
      " ...\n",
      " [ 1.05149462]\n",
      " [-0.78766767]\n",
      " [ 0.87185804]]\n",
      "t [[-1.18683914]\n",
      " [-0.31800167]\n",
      " [ 0.14721639]\n",
      " ...\n",
      " [ 1.24732268]\n",
      " [-0.94077398]\n",
      " [ 1.04487343]]\n",
      "t [[-1.18683914]\n",
      " [-0.31800167]\n",
      " [ 0.14721639]\n",
      " ...\n",
      " [ 1.24732268]\n",
      " [-0.94077398]\n",
      " [ 1.04487343]]\n",
      "Current iteration=4, loss=32153.182083409003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.36077136]\n",
      " [-0.37641692]\n",
      " [ 0.17852211]\n",
      " ...\n",
      " [ 1.39936934]\n",
      " [-1.06201342]\n",
      " [ 1.18376003]]\n",
      "t [[-1.36077136]\n",
      " [-0.37641692]\n",
      " [ 0.17852211]\n",
      " ...\n",
      " [ 1.39936934]\n",
      " [-1.06201342]\n",
      " [ 1.18376003]]\n",
      "t [[-1.50839247]\n",
      " [-0.42760969]\n",
      " [ 0.20476984]\n",
      " ...\n",
      " [ 1.51885168]\n",
      " [-1.15934714]\n",
      " [ 1.29692514]]\n",
      "t [[-1.50839247]\n",
      " [-0.42760969]\n",
      " [ 0.20476984]\n",
      " ...\n",
      " [ 1.51885168]\n",
      " [-1.15934714]\n",
      " [ 1.29692514]]\n",
      "Current iteration=6, loss=31331.062202300895\n",
      "t [[-1.63533848]\n",
      " [-0.4727908 ]\n",
      " [ 0.22658287]\n",
      " ...\n",
      " [ 1.61363555]\n",
      " [-1.2384163 ]\n",
      " [ 1.39031613]]\n",
      "t [[-1.63533848]\n",
      " [-0.4727908 ]\n",
      " [ 0.22658287]\n",
      " ...\n",
      " [ 1.61363555]\n",
      " [-1.2384163 ]\n",
      " [ 1.39031613]]\n",
      "t [[-1.74570667]\n",
      " [-0.51295151]\n",
      " [ 0.24466241]\n",
      " ...\n",
      " [ 1.68936549]\n",
      " [-1.30329823]\n",
      " [ 1.46823531]]\n",
      "t [[-1.74570667]\n",
      " [-0.51295151]\n",
      " [ 0.24466241]\n",
      " ...\n",
      " [ 1.68936549]\n",
      " [-1.30329823]\n",
      " [ 1.46823531]]\n",
      "Current iteration=8, loss=30849.22710134092\n",
      "t [[-1.84255543]\n",
      " [-0.54888615]\n",
      " [ 0.25966492]\n",
      " ...\n",
      " [ 1.75018426]\n",
      " [-1.3570022 ]\n",
      " [ 1.53386469]]\n",
      "t [[-1.84255543]\n",
      " [-0.54888615]\n",
      " [ 0.25966492]\n",
      " ...\n",
      " [ 1.75018426]\n",
      " [-1.3570022 ]\n",
      " [ 1.53386469]]\n",
      "t [[-1.92822067]\n",
      " [-0.5812307 ]\n",
      " [ 0.27216241]\n",
      " ...\n",
      " [ 1.79919463]\n",
      " [-1.40179296]\n",
      " [ 1.58960566]]\n",
      "loss=30532.90853042197\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.35740219]\n",
      " [-0.07518608]\n",
      " [ 0.07462242]\n",
      " ...\n",
      " [-0.09432609]\n",
      " [-0.11727803]\n",
      " [-0.02465448]]\n",
      "t [[-0.35740219]\n",
      " [-0.07518608]\n",
      " [ 0.07462242]\n",
      " ...\n",
      " [-0.09432609]\n",
      " [-0.11727803]\n",
      " [-0.02465448]]\n",
      "t [[-0.62103226]\n",
      " [-0.10951046]\n",
      " [ 0.13762926]\n",
      " ...\n",
      " [-0.17247984]\n",
      " [-0.21057124]\n",
      " [-0.04894023]]\n",
      "t [[-0.62103226]\n",
      " [-0.10951046]\n",
      " [ 0.13762926]\n",
      " ...\n",
      " [-0.17247984]\n",
      " [-0.21057124]\n",
      " [-0.04894023]]\n",
      "Current iteration=2, loss=33664.9417520906\n",
      "t [[-0.81417558]\n",
      " [-0.12728759]\n",
      " [ 0.18666278]\n",
      " ...\n",
      " [-0.23785281]\n",
      " [-0.28607211]\n",
      " [-0.07116655]]\n",
      "t [[-0.81417558]\n",
      " [-0.12728759]\n",
      " [ 0.18666278]\n",
      " ...\n",
      " [-0.23785281]\n",
      " [-0.28607211]\n",
      " [-0.07116655]]\n",
      "t [[-0.95617064]\n",
      " [-0.13901595]\n",
      " [ 0.22392196]\n",
      " ...\n",
      " [-0.2931373 ]\n",
      " [-0.34836148]\n",
      " [-0.09107245]]\n",
      "t [[-0.95617064]\n",
      " [-0.13901595]\n",
      " [ 0.22392196]\n",
      " ...\n",
      " [-0.2931373 ]\n",
      " [-0.34836148]\n",
      " [-0.09107245]]\n",
      "Current iteration=4, loss=32021.91931173749\n",
      "t [[-1.06098464]\n",
      " [-0.1491117 ]\n",
      " [ 0.25214724]\n",
      " ...\n",
      " [-0.34037232]\n",
      " [-0.40065421]\n",
      " [-0.10881492]]\n",
      "t [[-1.06098464]\n",
      " [-0.1491117 ]\n",
      " [ 0.25214724]\n",
      " ...\n",
      " [-0.34037232]\n",
      " [-0.40065421]\n",
      " [-0.10881492]]\n",
      "t [[-1.13837577]\n",
      " [-0.15942837]\n",
      " [ 0.27358834]\n",
      " ...\n",
      " [-0.38109737]\n",
      " [-0.44521118]\n",
      " [-0.1246302 ]]\n",
      "t [[-1.13837577]\n",
      " [-0.15942837]\n",
      " [ 0.27358834]\n",
      " ...\n",
      " [-0.38109737]\n",
      " [-0.44521118]\n",
      " [-0.1246302 ]]\n",
      "Current iteration=6, loss=31208.8173328063\n",
      "t [[-1.19519667]\n",
      " [-0.17070702]\n",
      " [ 0.28992272]\n",
      " ...\n",
      " [-0.41648653]\n",
      " [-0.48365392]\n",
      " [-0.13874301]]\n",
      "t [[-1.19519667]\n",
      " [-0.17070702]\n",
      " [ 0.28992272]\n",
      " ...\n",
      " [-0.41648653]\n",
      " [-0.48365392]\n",
      " [-0.13874301]]\n",
      "t [[-1.23634579]\n",
      " [-0.18318723]\n",
      " [ 0.30237542]\n",
      " ...\n",
      " [-0.44744784]\n",
      " [-0.5171732 ]\n",
      " [-0.15134881]]\n",
      "t [[-1.23634579]\n",
      " [-0.18318723]\n",
      " [ 0.30237542]\n",
      " ...\n",
      " [-0.44744784]\n",
      " [-0.5171732 ]\n",
      " [-0.15134881]]\n",
      "Current iteration=8, loss=30737.2420171903\n",
      "t [[-1.26539742]\n",
      " [-0.19687843]\n",
      " [ 0.31184461]\n",
      " ...\n",
      " [-0.47469388]\n",
      " [-0.54666378]\n",
      " [-0.16261488]]\n",
      "t [[-1.26539742]\n",
      " [-0.19687843]\n",
      " [ 0.31184461]\n",
      " ...\n",
      " [-0.47469388]\n",
      " [-0.54666378]\n",
      " [-0.16261488]]\n",
      "t [[-1.28501147]\n",
      " [-0.2116866 ]\n",
      " [ 0.31899691]\n",
      " ...\n",
      " [-0.49879183]\n",
      " [-0.57281211]\n",
      " [-0.17268504]]\n",
      "loss=30429.79897653236\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.41973927]\n",
      " [-0.09456193]\n",
      " [ 0.03693715]\n",
      " ...\n",
      " [-0.090671  ]\n",
      " [-0.11788781]\n",
      " [-0.02446932]]\n",
      "t [[-0.41973927]\n",
      " [-0.09456193]\n",
      " [ 0.03693715]\n",
      " ...\n",
      " [-0.090671  ]\n",
      " [-0.11788781]\n",
      " [-0.02446932]]\n",
      "t [[-0.74454139]\n",
      " [-0.1849838 ]\n",
      " [ 0.08301499]\n",
      " ...\n",
      " [-0.16570832]\n",
      " [-0.21158833]\n",
      " [-0.04903902]]\n",
      "t [[-0.74454139]\n",
      " [-0.1849838 ]\n",
      " [ 0.08301499]\n",
      " ...\n",
      " [-0.16570832]\n",
      " [-0.21158833]\n",
      " [-0.04903902]]\n",
      "Current iteration=2, loss=33634.78677627913\n",
      "t [[-1.00257127]\n",
      " [-0.26507076]\n",
      " [ 0.12686044]\n",
      " ...\n",
      " [-0.2285427 ]\n",
      " [-0.28737487]\n",
      " [-0.07165494]]\n",
      "t [[-1.00257127]\n",
      " [-0.26507076]\n",
      " [ 0.12686044]\n",
      " ...\n",
      " [-0.2285427 ]\n",
      " [-0.28737487]\n",
      " [-0.07165494]]\n",
      "t [[-1.21269301]\n",
      " [-0.33462916]\n",
      " [ 0.1649279 ]\n",
      " ...\n",
      " [-0.28180579]\n",
      " [-0.34988121]\n",
      " [-0.09193399]]\n",
      "t [[-1.21269301]\n",
      " [-0.33462916]\n",
      " [ 0.1649279 ]\n",
      " ...\n",
      " [-0.28180579]\n",
      " [-0.34988121]\n",
      " [-0.09193399]]\n",
      "Current iteration=4, loss=31983.796883618277\n",
      "t [[-1.38736767]\n",
      " [-0.39505573]\n",
      " [ 0.19677138]\n",
      " ...\n",
      " [-0.32745481]\n",
      " [-0.40235357]\n",
      " [-0.10999846]]\n",
      "t [[-1.38736767]\n",
      " [-0.39505573]\n",
      " [ 0.19677138]\n",
      " ...\n",
      " [-0.32745481]\n",
      " [-0.40235357]\n",
      " [-0.10999846]]\n",
      "t [[-1.53503634]\n",
      " [-0.44788161]\n",
      " [ 0.22298799]\n",
      " ...\n",
      " [-0.36695257]\n",
      " [-0.44707109]\n",
      " [-0.12607873]]\n",
      "t [[-1.53503634]\n",
      " [-0.44788161]\n",
      " [ 0.22298799]\n",
      " ...\n",
      " [-0.36695257]\n",
      " [-0.44707109]\n",
      " [-0.12607873]]\n",
      "Current iteration=6, loss=31164.812682670214\n",
      "t [[-1.66160658]\n",
      " [-0.49441716]\n",
      " [ 0.24444041]\n",
      " ...\n",
      " [-0.40140803]\n",
      " [-0.48566592]\n",
      " [-0.14040254]]\n",
      "t [[-1.66160658]\n",
      " [-0.49441716]\n",
      " [ 0.24444041]\n",
      " ...\n",
      " [-0.40140803]\n",
      " [-0.48566592]\n",
      " [-0.14040254]]\n",
      "t [[-1.77134375]\n",
      " [-0.53571568]\n",
      " [ 0.26197916]\n",
      " ...\n",
      " [-0.4316757 ]\n",
      " [-0.51933508]\n",
      " [-0.15317072]]\n",
      "t [[-1.77134375]\n",
      " [-0.53571568]\n",
      " [ 0.26197916]\n",
      " ...\n",
      " [-0.4316757 ]\n",
      " [-0.51933508]\n",
      " [-0.15317072]]\n",
      "Current iteration=8, loss=30687.131855964697\n",
      "t [[-1.86741342]\n",
      " [-0.57261262]\n",
      " [ 0.27635336]\n",
      " ...\n",
      " [-0.45842467]\n",
      " [-0.54897695]\n",
      " [-0.16455609]]\n",
      "t [[-1.86741342]\n",
      " [-0.57261262]\n",
      " [ 0.27635336]\n",
      " ...\n",
      " [-0.45842467]\n",
      " [-0.54897695]\n",
      " [-0.16455609]]\n",
      "t [[-1.95222141]\n",
      " [-0.6057729 ]\n",
      " [ 0.28819206]\n",
      " ...\n",
      " [-0.48218675]\n",
      " [-0.57528004]\n",
      " [-0.17470756]]\n",
      "loss=30373.436612170386\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.4208128 ]\n",
      " [-0.09753059]\n",
      " [ 0.02534666]\n",
      " ...\n",
      " [-0.08962537]\n",
      " [-0.11858025]\n",
      " [-0.02398394]]\n",
      "t [[-0.4208128 ]\n",
      " [-0.09753059]\n",
      " [ 0.02534666]\n",
      " ...\n",
      " [-0.08962537]\n",
      " [-0.11858025]\n",
      " [-0.02398394]]\n",
      "t [[-0.74655311]\n",
      " [-0.18970075]\n",
      " [ 0.06483938]\n",
      " ...\n",
      " [-0.16450963]\n",
      " [-0.21284143]\n",
      " [-0.04812637]]\n",
      "t [[-0.74655311]\n",
      " [-0.18970075]\n",
      " [ 0.06483938]\n",
      " ...\n",
      " [-0.16450963]\n",
      " [-0.21284143]\n",
      " [-0.04812637]]\n",
      "Current iteration=2, loss=33581.85388923554\n",
      "t [[-1.0053177 ]\n",
      " [-0.27109497]\n",
      " [ 0.10525984]\n",
      " ...\n",
      " [-0.22763638]\n",
      " [-0.28910189]\n",
      " [-0.07047801]]\n",
      "t [[-1.0053177 ]\n",
      " [-0.27109497]\n",
      " [ 0.10525984]\n",
      " ...\n",
      " [-0.22763638]\n",
      " [-0.28910189]\n",
      " [-0.07047801]]\n",
      "t [[-1.21601481]\n",
      " [-0.34177114]\n",
      " [ 0.14179522]\n",
      " ...\n",
      " [-0.28139695]\n",
      " [-0.35202258]\n",
      " [-0.09064023]]\n",
      "t [[-1.21601481]\n",
      " [-0.34177114]\n",
      " [ 0.14179522]\n",
      " ...\n",
      " [-0.28139695]\n",
      " [-0.35202258]\n",
      " [-0.09064023]]\n",
      "Current iteration=4, loss=31910.59186246484\n",
      "t [[-1.39114766]\n",
      " [-0.40321417]\n",
      " [ 0.17323992]\n",
      " ...\n",
      " [-0.32762509]\n",
      " [-0.40486468]\n",
      " [-0.10869884]]\n",
      "t [[-1.39114766]\n",
      " [-0.40321417]\n",
      " [ 0.17323992]\n",
      " ...\n",
      " [-0.32762509]\n",
      " [-0.40486468]\n",
      " [-0.10869884]]\n",
      "t [[-1.53918452]\n",
      " [-0.45699239]\n",
      " [ 0.19974857]\n",
      " ...\n",
      " [-0.36772072]\n",
      " [-0.44991587]\n",
      " [-0.12485251]]\n",
      "t [[-1.53918452]\n",
      " [-0.45699239]\n",
      " [ 0.19974857]\n",
      " ...\n",
      " [-0.36772072]\n",
      " [-0.44991587]\n",
      " [-0.12485251]]\n",
      "Current iteration=6, loss=31082.53276273056\n",
      "t [[-1.66604952]\n",
      " [-0.50443558]\n",
      " [ 0.22192342]\n",
      " ...\n",
      " [-0.40276099]\n",
      " [-0.48881367]\n",
      " [-0.13930572]]\n",
      "t [[-1.66604952]\n",
      " [-0.50443558]\n",
      " [ 0.22192342]\n",
      " ...\n",
      " [-0.40276099]\n",
      " [-0.48881367]\n",
      " [-0.13930572]]\n",
      "t [[-1.77601839]\n",
      " [-0.546609  ]\n",
      " [ 0.24045788]\n",
      " ...\n",
      " [-0.43358471]\n",
      " [-0.52275876]\n",
      " [-0.15224295]]\n",
      "t [[-1.77601839]\n",
      " [-0.546609  ]\n",
      " [ 0.24045788]\n",
      " ...\n",
      " [-0.43358471]\n",
      " [-0.52275876]\n",
      " [-0.15224295]]\n",
      "Current iteration=8, loss=30600.617108148454\n",
      "t [[-1.87226383]\n",
      " [-0.58435615]\n",
      " [ 0.25600347]\n",
      " ...\n",
      " [-0.46085378]\n",
      " [-0.55265228]\n",
      " [-0.16382572]]\n",
      "t [[-1.87226383]\n",
      " [-0.58435615]\n",
      " [ 0.25600347]\n",
      " ...\n",
      " [-0.46085378]\n",
      " [-0.55265228]\n",
      " [-0.16382572]]\n",
      "t [[-1.95719699]\n",
      " [-0.61834753]\n",
      " [ 0.26912668]\n",
      " ...\n",
      " [-0.48509734]\n",
      " [-0.57918496]\n",
      " [-0.17419507]]\n",
      "loss=30285.11212494184\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.41749364]\n",
      " [-0.09080657]\n",
      " [ 0.02972155]\n",
      " ...\n",
      " [ 0.46793337]\n",
      " [-0.34454129]\n",
      " [ 0.37954839]]\n",
      "t [[-0.41749364]\n",
      " [-0.09080657]\n",
      " [ 0.02972155]\n",
      " ...\n",
      " [ 0.46793337]\n",
      " [-0.34454129]\n",
      " [ 0.37954839]]\n",
      "t [[-0.74111527]\n",
      " [-0.17880387]\n",
      " [ 0.07193739]\n",
      " ...\n",
      " [ 0.81344649]\n",
      " [-0.60476024]\n",
      " [ 0.66749265]]\n",
      "t [[-0.74111527]\n",
      " [-0.17880387]\n",
      " [ 0.07193739]\n",
      " ...\n",
      " [ 0.81344649]\n",
      " [-0.60476024]\n",
      " [ 0.66749265]]\n",
      "Current iteration=2, loss=33717.85432958734\n",
      "t [[-0.99869672]\n",
      " [-0.257039  ]\n",
      " [ 0.11384865]\n",
      " ...\n",
      " [ 1.07171278]\n",
      " [-0.80317249]\n",
      " [ 0.88917974]]\n",
      "t [[-0.99869672]\n",
      " [-0.257039  ]\n",
      " [ 0.11384865]\n",
      " ...\n",
      " [ 1.07171278]\n",
      " [-0.80317249]\n",
      " [ 0.88917974]]\n",
      "t [[-1.20881362]\n",
      " [-0.32503681]\n",
      " [ 0.15105696]\n",
      " ...\n",
      " [ 1.26799355]\n",
      " [-0.9569211 ]\n",
      " [ 1.06313334]]\n",
      "t [[-1.20881362]\n",
      " [-0.32503681]\n",
      " [ 0.15105696]\n",
      " ...\n",
      " [ 1.26799355]\n",
      " [-0.9569211 ]\n",
      " [ 1.06313334]]\n",
      "Current iteration=4, loss=32090.07812794345\n",
      "t [[-1.38373658]\n",
      " [-0.38408809]\n",
      " [ 0.18263493]\n",
      " ...\n",
      " [ 1.41945897]\n",
      " [-1.07801339]\n",
      " [ 1.20210428]]\n",
      "t [[-1.38373658]\n",
      " [-0.38408809]\n",
      " [ 0.18263493]\n",
      " ...\n",
      " [ 1.41945897]\n",
      " [-1.07801339]\n",
      " [ 1.20210428]]\n",
      "t [[-1.53178957]\n",
      " [-0.43567999]\n",
      " [ 0.20891187]\n",
      " ...\n",
      " [ 1.53780834]\n",
      " [-1.17476727]\n",
      " [ 1.3148702 ]]\n",
      "t [[-1.53178957]\n",
      " [-0.43567999]\n",
      " [ 0.20891187]\n",
      " ...\n",
      " [ 1.53780834]\n",
      " [-1.17476727]\n",
      " [ 1.3148702 ]]\n",
      "Current iteration=6, loss=31279.99746187117\n",
      "t [[-1.65880901]\n",
      " [-0.4811014 ]\n",
      " [ 0.2306011 ]\n",
      " ...\n",
      " [ 1.63118039]\n",
      " [-1.25302784]\n",
      " [ 1.40759224]]\n",
      "t [[-1.65880901]\n",
      " [-0.4811014 ]\n",
      " [ 0.2306011 ]\n",
      " ...\n",
      " [ 1.63118039]\n",
      " [-1.25302784]\n",
      " [ 1.40759224]]\n",
      "t [[-1.7690168 ]\n",
      " [-0.52139292]\n",
      " [ 0.24846931]\n",
      " ...\n",
      " [ 1.70537846]\n",
      " [-1.31699227]\n",
      " [ 1.48470002]]\n",
      "t [[-1.7690168 ]\n",
      " [-0.52139292]\n",
      " [ 0.24846931]\n",
      " ...\n",
      " [ 1.70537846]\n",
      " [-1.31699227]\n",
      " [ 1.48470002]]\n",
      "Current iteration=8, loss=30807.248806697076\n",
      "t [[-1.86555187]\n",
      " [-0.55738086]\n",
      " [ 0.26321828]\n",
      " ...\n",
      " [ 1.76463961]\n",
      " [-1.36974183]\n",
      " [ 1.54945329]]\n",
      "t [[-1.86555187]\n",
      " [-0.55738086]\n",
      " [ 0.26321828]\n",
      " ...\n",
      " [ 1.76463961]\n",
      " [-1.36974183]\n",
      " [ 1.54945329]]\n",
      "t [[-1.95080382]\n",
      " [-0.58972265]\n",
      " [ 0.27545024]\n",
      " ...\n",
      " [ 1.81212305]\n",
      " [-1.41358509]\n",
      " [ 1.6043017 ]]\n",
      "loss=30497.186939400086\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.36656635]\n",
      " [-0.07711392]\n",
      " [ 0.07653582]\n",
      " ...\n",
      " [-0.09674471]\n",
      " [-0.12028516]\n",
      " [-0.02528664]]\n",
      "t [[-0.36656635]\n",
      " [-0.07711392]\n",
      " [ 0.07653582]\n",
      " ...\n",
      " [-0.09674471]\n",
      " [-0.12028516]\n",
      " [-0.02528664]]\n",
      "t [[-0.63453375]\n",
      " [-0.1112656 ]\n",
      " [ 0.14085469]\n",
      " ...\n",
      " [-0.17648665]\n",
      " [-0.21535155]\n",
      " [-0.05018579]]\n",
      "t [[-0.63453375]\n",
      " [-0.1112656 ]\n",
      " [ 0.14085469]\n",
      " ...\n",
      " [-0.17648665]\n",
      " [-0.21535155]\n",
      " [-0.05018579]]\n",
      "Current iteration=2, loss=33595.81991349464\n",
      "t [[-0.82903747]\n",
      " [-0.12865737]\n",
      " [ 0.19042847]\n",
      " ...\n",
      " [-0.24288817]\n",
      " [-0.29188255]\n",
      " [-0.07287795]]\n",
      "t [[-0.82903747]\n",
      " [-0.12865737]\n",
      " [ 0.19042847]\n",
      " ...\n",
      " [-0.24288817]\n",
      " [-0.29188255]\n",
      " [-0.07287795]]\n",
      "t [[-0.97076978]\n",
      " [-0.14021537]\n",
      " [ 0.22774493]\n",
      " ...\n",
      " [-0.29882358]\n",
      " [-0.35476113]\n",
      " [-0.09311955]]\n",
      "t [[-0.97076978]\n",
      " [-0.14021537]\n",
      " [ 0.22774493]\n",
      " ...\n",
      " [-0.29882358]\n",
      " [-0.35476113]\n",
      " [-0.09311955]]\n",
      "Current iteration=4, loss=31960.55782618086\n",
      "t [[-1.0744761 ]\n",
      " [-0.15038965]\n",
      " [ 0.25577476]\n",
      " ...\n",
      " [-0.34645114]\n",
      " [-0.40737478]\n",
      " [-0.11109893]]\n",
      "t [[-1.0744761 ]\n",
      " [-0.15038965]\n",
      " [ 0.25577476]\n",
      " ...\n",
      " [-0.34645114]\n",
      " [-0.40737478]\n",
      " [-0.11109893]]\n",
      "t [[-1.15033901]\n",
      " [-0.16098647]\n",
      " [ 0.27689976]\n",
      " ...\n",
      " [-0.3873893 ]\n",
      " [-0.45208419]\n",
      " [-0.12707542]]\n",
      "t [[-1.15033901]\n",
      " [-0.16098647]\n",
      " [ 0.27689976]\n",
      " ...\n",
      " [-0.3873893 ]\n",
      " [-0.45208419]\n",
      " [-0.12707542]]\n",
      "Current iteration=6, loss=31159.81051228191\n",
      "t [[-1.20544439]\n",
      " [-0.17269294]\n",
      " [ 0.2928679 ]\n",
      " ...\n",
      " [-0.42286529]\n",
      " [-0.49057071]\n",
      " [-0.14128954]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.20544439]\n",
      " [-0.17269294]\n",
      " [ 0.2928679 ]\n",
      " ...\n",
      " [-0.42286529]\n",
      " [-0.49057071]\n",
      " [-0.14128954]]\n",
      "t [[-1.24482174]\n",
      " [-0.18570335]\n",
      " [ 0.3049415 ]\n",
      " ...\n",
      " [-0.45382357]\n",
      " [-0.52406244]\n",
      " [-0.15394816]]\n",
      "t [[-1.24482174]\n",
      " [-0.18570335]\n",
      " [ 0.3049415 ]\n",
      " ...\n",
      " [-0.45382357]\n",
      " [-0.52406244]\n",
      " [-0.15394816]]\n",
      "Current iteration=8, loss=30697.307363881042\n",
      "t [[-1.27212031]\n",
      " [-0.19999189]\n",
      " [ 0.31403858]\n",
      " ...\n",
      " [-0.48100227]\n",
      " [-0.55347823]\n",
      " [-0.16522731]]\n",
      "t [[-1.27212031]\n",
      " [-0.19999189]\n",
      " [ 0.31403858]\n",
      " ...\n",
      " [-0.48100227]\n",
      " [-0.55347823]\n",
      " [-0.16522731]]\n",
      "t [[-1.29004301]\n",
      " [-0.21543784]\n",
      " [ 0.32083622]\n",
      " ...\n",
      " [-0.50498672]\n",
      " [-0.57952055]\n",
      " [-0.17527792]]\n",
      "loss=30396.016105986022\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.43050182]\n",
      " [-0.09698659]\n",
      " [ 0.03788426]\n",
      " ...\n",
      " [-0.0929959 ]\n",
      " [-0.12091057]\n",
      " [-0.02509673]]\n",
      "t [[-0.43050182]\n",
      " [-0.09698659]\n",
      " [ 0.03788426]\n",
      " ...\n",
      " [-0.0929959 ]\n",
      " [-0.12091057]\n",
      " [-0.02509673]]\n",
      "t [[-0.76118929]\n",
      " [-0.18961399]\n",
      " [ 0.08537643]\n",
      " ...\n",
      " [-0.16955522]\n",
      " [-0.21638957]\n",
      " [-0.05029918]]\n",
      "t [[-0.76118929]\n",
      " [-0.18961399]\n",
      " [ 0.08537643]\n",
      " ...\n",
      " [-0.16955522]\n",
      " [-0.21638957]\n",
      " [-0.05029918]]\n",
      "Current iteration=2, loss=33565.29269743532\n",
      "t [[-1.02245015]\n",
      " [-0.27121715]\n",
      " [ 0.13022689]\n",
      " ...\n",
      " [-0.23338214]\n",
      " [-0.29320747]\n",
      " [-0.07339617]]\n",
      "t [[-1.02245015]\n",
      " [-0.27121715]\n",
      " [ 0.13022689]\n",
      " ...\n",
      " [-0.23338214]\n",
      " [-0.29320747]\n",
      " [-0.07339617]]\n",
      "t [[-1.23431584]\n",
      " [-0.34175095]\n",
      " [ 0.16883004]\n",
      " ...\n",
      " [-0.28728343]\n",
      " [-0.35630338]\n",
      " [-0.09401922]]\n",
      "t [[-1.23431584]\n",
      " [-0.34175095]\n",
      " [ 0.16883004]\n",
      " ...\n",
      " [-0.28728343]\n",
      " [-0.35630338]\n",
      " [-0.09401922]]\n",
      "Current iteration=4, loss=31922.112172719986\n",
      "t [[-1.40985993]\n",
      " [-0.40279473]\n",
      " [ 0.20085876]\n",
      " ...\n",
      " [-0.33332846]\n",
      " [-0.40909742]\n",
      " [-0.11232369]]\n",
      "t [[-1.40985993]\n",
      " [-0.40279473]\n",
      " [ 0.20085876]\n",
      " ...\n",
      " [-0.33332846]\n",
      " [-0.40909742]\n",
      " [-0.11232369]]\n",
      "t [[-1.55786235]\n",
      " [-0.45600376]\n",
      " [ 0.22703176]\n",
      " ...\n",
      " [-0.37305357]\n",
      " [-0.45396897]\n",
      " [-0.12856473]]\n",
      "t [[-1.55786235]\n",
      " [-0.45600376]\n",
      " [ 0.22703176]\n",
      " ...\n",
      " [-0.37305357]\n",
      " [-0.45396897]\n",
      " [-0.12856473]]\n",
      "Current iteration=6, loss=31115.350387199647\n",
      "t [[-1.68442897]\n",
      " [-0.50276551]\n",
      " [ 0.24830403]\n",
      " ...\n",
      " [-0.40761701]\n",
      " [-0.4926101 ]\n",
      " [-0.14298704]]\n",
      "t [[-1.68442897]\n",
      " [-0.50276551]\n",
      " [ 0.24830403]\n",
      " ...\n",
      " [-0.40761701]\n",
      " [-0.4926101 ]\n",
      " [-0.14298704]]\n",
      "t [[-1.79394688]\n",
      " [-0.54418158]\n",
      " [ 0.26559034]\n",
      " ...\n",
      " [-0.4379071 ]\n",
      " [-0.52625508]\n",
      " [-0.15580352]]\n",
      "t [[-1.79394688]\n",
      " [-0.54418158]\n",
      " [ 0.26559034]\n",
      " ...\n",
      " [-0.4379071 ]\n",
      " [-0.52625508]\n",
      " [-0.15580352]]\n",
      "Current iteration=8, loss=30646.56070071727\n",
      "t [[-1.88965939]\n",
      " [-0.5811187 ]\n",
      " [ 0.27968274]\n",
      " ...\n",
      " [-0.46461654]\n",
      " [-0.55582628]\n",
      " [-0.16719622]]\n",
      "t [[-1.88965939]\n",
      " [-0.5811187 ]\n",
      " [ 0.27968274]\n",
      " ...\n",
      " [-0.46461654]\n",
      " [-0.55582628]\n",
      " [-0.16719622]]\n",
      "t [[-1.97402351]\n",
      " [-0.61426285]\n",
      " [ 0.29123796]\n",
      " ...\n",
      " [-0.48829401]\n",
      " [-0.58202809]\n",
      " [-0.17732148]]\n",
      "loss=30338.85506399797\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.43160288]\n",
      " [-0.10003137]\n",
      " [ 0.02599657]\n",
      " ...\n",
      " [-0.09192346]\n",
      " [-0.12162077]\n",
      " [-0.02459891]]\n",
      "t [[-0.43160288]\n",
      " [-0.10003137]\n",
      " [ 0.02599657]\n",
      " ...\n",
      " [-0.09192346]\n",
      " [-0.12162077]\n",
      " [-0.02459891]]\n",
      "t [[-0.76325004]\n",
      " [-0.19442   ]\n",
      " [ 0.06686294]\n",
      " ...\n",
      " [-0.16834874]\n",
      " [-0.21767159]\n",
      " [-0.04936457]]\n",
      "t [[-0.76325004]\n",
      " [-0.19442   ]\n",
      " [ 0.06686294]\n",
      " ...\n",
      " [-0.16834874]\n",
      " [-0.21767159]\n",
      " [-0.04936457]]\n",
      "Current iteration=2, loss=33511.47204259256\n",
      "t [[-1.0252558 ]\n",
      " [-0.27734131]\n",
      " [ 0.10835959]\n",
      " ...\n",
      " [-0.23249796]\n",
      " [-0.29497152]\n",
      " [-0.07219877]]\n",
      "t [[-1.0252558 ]\n",
      " [-0.27734131]\n",
      " [ 0.10835959]\n",
      " ...\n",
      " [-0.23249796]\n",
      " [-0.29497152]\n",
      " [-0.07219877]]\n",
      "t [[-1.23770095]\n",
      " [-0.34900728]\n",
      " [ 0.14553471]\n",
      " ...\n",
      " [-0.28692501]\n",
      " [-0.35848823]\n",
      " [-0.09271316]]\n",
      "t [[-1.23770095]\n",
      " [-0.34900728]\n",
      " [ 0.14553471]\n",
      " ...\n",
      " [-0.28692501]\n",
      " [-0.35848823]\n",
      " [-0.09271316]]\n",
      "Current iteration=4, loss=31848.163081837072\n",
      "t [[-1.413704  ]\n",
      " [-0.41108372]\n",
      " [ 0.17726999]\n",
      " ...\n",
      " [-0.3335724 ]\n",
      " [-0.41165722]\n",
      " [-0.11102288]]\n",
      "t [[-1.413704  ]\n",
      " [-0.41108372]\n",
      " [ 0.17726999]\n",
      " ...\n",
      " [-0.3335724 ]\n",
      " [-0.41165722]\n",
      " [-0.11102288]]\n",
      "t [[-1.56207311]\n",
      " [-0.46526189]\n",
      " [ 0.20383156]\n",
      " ...\n",
      " [-0.3739133 ]\n",
      " [-0.4568666 ]\n",
      " [-0.12734934]]\n",
      "t [[-1.56207311]\n",
      " [-0.46526189]\n",
      " [ 0.20383156]\n",
      " ...\n",
      " [-0.3739133 ]\n",
      " [-0.4568666 ]\n",
      " [-0.12734934]]\n",
      "Current iteration=6, loss=31032.550954847295\n",
      "t [[-1.68893099]\n",
      " [-0.51294826]\n",
      " [ 0.22591201]\n",
      " ...\n",
      " [-0.40907477]\n",
      " [-0.49581387]\n",
      " [-0.14191297]]\n",
      "t [[-1.68893099]\n",
      " [-0.51294826]\n",
      " [ 0.22591201]\n",
      " ...\n",
      " [-0.40907477]\n",
      " [-0.49581387]\n",
      " [-0.14191297]]\n",
      "t [[-1.79867524]\n",
      " [-0.5552563 ]\n",
      " [ 0.24426978]\n",
      " ...\n",
      " [-0.43993021]\n",
      " [-0.52973707]\n",
      " [-0.15491   ]]\n",
      "t [[-1.79867524]\n",
      " [-0.5552563 ]\n",
      " [ 0.24426978]\n",
      " ...\n",
      " [-0.43993021]\n",
      " [-0.52973707]\n",
      " [-0.15491   ]]\n",
      "Current iteration=8, loss=30559.72737832091\n",
      "t [[-1.89455637]\n",
      " [-0.59306079]\n",
      " [ 0.25960073]\n",
      " ...\n",
      " [-0.46716588]\n",
      " [-0.5595614 ]\n",
      " [-0.16651104]]\n",
      "t [[-1.89455637]\n",
      " [-0.59306079]\n",
      " [ 0.25960073]\n",
      " ...\n",
      " [-0.46716588]\n",
      " [-0.5595614 ]\n",
      " [-0.16651104]]\n",
      "t [[-1.97903689]\n",
      " [-0.62705329]\n",
      " [ 0.27250041]\n",
      " ...\n",
      " [-0.49132837]\n",
      " [-0.58599356]\n",
      " [-0.17686454]]\n",
      "loss=30250.376125176674\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.4281986 ]\n",
      " [-0.09313494]\n",
      " [ 0.03048364]\n",
      " ...\n",
      " [ 0.47993166]\n",
      " [-0.35337568]\n",
      " [ 0.3892804 ]]\n",
      "t [[-0.4281986 ]\n",
      " [-0.09313494]\n",
      " [ 0.03048364]\n",
      " ...\n",
      " [ 0.47993166]\n",
      " [-0.35337568]\n",
      " [ 0.3892804 ]]\n",
      "t [[-0.75770243]\n",
      " [-0.18330932]\n",
      " [ 0.07410031]\n",
      " ...\n",
      " [ 0.83114997]\n",
      " [-0.61809186]\n",
      " [ 0.68224839]]\n",
      "t [[-0.75770243]\n",
      " [-0.18330932]\n",
      " [ 0.07410031]\n",
      " ...\n",
      " [ 0.83114997]\n",
      " [-0.61809186]\n",
      " [ 0.68224839]]\n",
      "Current iteration=2, loss=33649.55613344801\n",
      "t [[-1.01853972]\n",
      " [-0.26304094]\n",
      " [ 0.1170625 ]\n",
      " ...\n",
      " [ 1.09160715]\n",
      " [-0.81844744]\n",
      " [ 0.9062559 ]]\n",
      "t [[-1.01853972]\n",
      " [-0.26304094]\n",
      " [ 0.1170625 ]\n",
      " ...\n",
      " [ 1.09160715]\n",
      " [-0.81844744]\n",
      " [ 0.9062559 ]]\n",
      "t [[-1.2304334 ]\n",
      " [-0.33199543]\n",
      " [ 0.15486404]\n",
      " ...\n",
      " [ 1.28819976]\n",
      " [-0.97273305]\n",
      " [ 1.08103614]]\n",
      "t [[-1.2304334 ]\n",
      " [-0.33199543]\n",
      " [ 0.15486404]\n",
      " ...\n",
      " [ 1.28819976]\n",
      " [-0.97273305]\n",
      " [ 1.08103614]]\n",
      "Current iteration=4, loss=32029.15601262008\n",
      "t [[-1.40625741]\n",
      " [-0.39164715]\n",
      " [ 0.18667949]\n",
      " ...\n",
      " [ 1.43898023]\n",
      " [-1.09359907]\n",
      " [ 1.2200059 ]]\n",
      "t [[-1.40625741]\n",
      " [-0.39164715]\n",
      " [ 0.18667949]\n",
      " ...\n",
      " [ 1.43898023]\n",
      " [-1.09359907]\n",
      " [ 1.2200059 ]]\n",
      "t [[-1.55467118]\n",
      " [-0.44360858]\n",
      " [ 0.21295543]\n",
      " ...\n",
      " [ 1.55612591]\n",
      " [-1.18971775]\n",
      " [ 1.33231082]]\n",
      "t [[-1.55467118]\n",
      " [-0.44360858]\n",
      " [ 0.21295543]\n",
      " ...\n",
      " [ 1.55612591]\n",
      " [-1.18971775]\n",
      " [ 1.33231082]]\n",
      "Current iteration=6, loss=31231.053205396747\n",
      "t [[-1.6817085 ]\n",
      " [-0.48924626]\n",
      " [ 0.23449779]\n",
      " ...\n",
      " [ 1.64804209]\n",
      " [-1.26713382]\n",
      " [ 1.42432165]]\n",
      "t [[-1.6817085 ]\n",
      " [-0.48924626]\n",
      " [ 0.23449779]\n",
      " ...\n",
      " [ 1.64804209]\n",
      " [-1.26713382]\n",
      " [ 1.42432165]]\n",
      "t [[-1.79171307]\n",
      " [-0.52964907]\n",
      " [ 0.25213913]\n",
      " ...\n",
      " [ 1.72068519]\n",
      " [-1.33016015]\n",
      " [ 1.5005913 ]]\n",
      "t [[-1.79171307]\n",
      " [-0.52964907]\n",
      " [ 0.25213913]\n",
      " ...\n",
      " [ 1.72068519]\n",
      " [-1.33016015]\n",
      " [ 1.5005913 ]]\n",
      "Current iteration=8, loss=30767.115270903545\n",
      "t [[-1.88790202]\n",
      " [-0.56567441]\n",
      " [ 0.26662588]\n",
      " ...\n",
      " [ 1.77838188]\n",
      " [-1.38194665]\n",
      " [ 1.56445397]]\n",
      "t [[-1.88790202]\n",
      " [-0.56567441]\n",
      " [ 0.26662588]\n",
      " ...\n",
      " [ 1.77838188]\n",
      " [-1.38194665]\n",
      " [ 1.56445397]]\n",
      "t [[-1.9727167 ]\n",
      " [-0.59800045]\n",
      " [ 0.27858945]\n",
      " ...\n",
      " [ 1.82434408]\n",
      " [-1.42484303]\n",
      " [ 1.6184049 ]]\n",
      "loss=30463.040529417936\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.37573051]\n",
      " [-0.07904177]\n",
      " [ 0.07844921]\n",
      " ...\n",
      " [-0.09916333]\n",
      " [-0.12329229]\n",
      " [-0.02591881]]\n",
      "t [[-0.37573051]\n",
      " [-0.07904177]\n",
      " [ 0.07844921]\n",
      " ...\n",
      " [-0.09916333]\n",
      " [-0.12329229]\n",
      " [-0.02591881]]\n",
      "t [[-0.64791745]\n",
      " [-0.11296973]\n",
      " [ 0.14406509]\n",
      " ...\n",
      " [-0.1804734 ]\n",
      " [-0.2201018 ]\n",
      " [-0.05143093]]\n",
      "t [[-0.64791745]\n",
      " [-0.11296973]\n",
      " [ 0.14406509]\n",
      " ...\n",
      " [-0.1804734 ]\n",
      " [-0.2201018 ]\n",
      " [-0.05143093]]\n",
      "Current iteration=2, loss=33528.04848551308\n",
      "t [[-0.84363947]\n",
      " [-0.12996599]\n",
      " [ 0.19414208]\n",
      " ...\n",
      " [-0.24787696]\n",
      " [-0.29762749]\n",
      " [-0.07458196]]\n",
      "t [[-0.84363947]\n",
      " [-0.12996599]\n",
      " [ 0.19414208]\n",
      " ...\n",
      " [-0.24787696]\n",
      " [-0.29762749]\n",
      " [-0.07458196]]\n",
      "t [[-0.98499204]\n",
      " [-0.14136806]\n",
      " [ 0.23148134]\n",
      " ...\n",
      " [-0.30443619]\n",
      " [-0.3610635 ]\n",
      " [-0.09515007]]\n",
      "t [[-0.98499204]\n",
      " [-0.14136806]\n",
      " [ 0.23148134]\n",
      " ...\n",
      " [-0.30443619]\n",
      " [-0.3610635 ]\n",
      " [-0.09515007]]\n",
      "Current iteration=4, loss=31901.31752422508\n",
      "t [[-1.08750787]\n",
      " [-0.15164397]\n",
      " [ 0.25929131]\n",
      " ...\n",
      " [-0.35243121]\n",
      " [-0.41397187]\n",
      " [-0.1133569 ]]\n",
      "t [[-1.08750787]\n",
      " [-0.15164397]\n",
      " [ 0.25929131]\n",
      " ...\n",
      " [-0.35243121]\n",
      " [-0.41397187]\n",
      " [-0.1133569 ]]\n",
      "t [[-1.16178974]\n",
      " [-0.16254498]\n",
      " [ 0.28008529]\n",
      " ...\n",
      " [-0.39356037]\n",
      " [-0.45881284]\n",
      " [-0.12948536]]\n",
      "t [[-1.16178974]\n",
      " [-0.16254498]\n",
      " [ 0.28008529]\n",
      " ...\n",
      " [-0.39356037]\n",
      " [-0.45881284]\n",
      " [-0.12948536]]\n",
      "Current iteration=6, loss=31112.83059112441\n",
      "t [[-1.21515034]\n",
      " [-0.17470091]\n",
      " [ 0.29567942]\n",
      " ...\n",
      " [-0.42910427]\n",
      " [-0.49732678]\n",
      " [-0.14379191]]\n",
      "t [[-1.21515034]\n",
      " [-0.17470091]\n",
      " [ 0.29567942]\n",
      " ...\n",
      " [-0.42910427]\n",
      " [-0.49732678]\n",
      " [-0.14379191]]\n",
      "t [[-1.25274467]\n",
      " [-0.18825937]\n",
      " [ 0.3073713 ]\n",
      " ...\n",
      " [-0.46004371]\n",
      " [-0.53077829]\n",
      " [-0.15649485]]\n",
      "t [[-1.25274467]\n",
      " [-0.18825937]\n",
      " [ 0.3073713 ]\n",
      " ...\n",
      " [-0.46004371]\n",
      " [-0.53077829]\n",
      " [-0.15649485]]\n",
      "Current iteration=8, loss=30659.11533741564\n",
      "t [[-1.2782926 ]\n",
      " [-0.20315885]\n",
      " [ 0.31609725]\n",
      " ...\n",
      " [-0.48714202]\n",
      " [-0.56010962]\n",
      " [-0.16777909]]\n",
      "t [[-1.2782926 ]\n",
      " [-0.20315885]\n",
      " [ 0.31609725]\n",
      " ...\n",
      " [-0.48714202]\n",
      " [-0.56010962]\n",
      " [-0.16777909]]\n",
      "t [[-1.29453672]\n",
      " [-0.21925195]\n",
      " [ 0.32254382]\n",
      " ...\n",
      " [-0.51100236]\n",
      " [-0.58603868]\n",
      " [-0.17780273]]\n",
      "loss=30363.708008995374\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.44126436]\n",
      " [-0.09941126]\n",
      " [ 0.03883137]\n",
      " ...\n",
      " [-0.0953208 ]\n",
      " [-0.12393333]\n",
      " [-0.02572415]]\n",
      "t [[-0.44126436]\n",
      " [-0.09941126]\n",
      " [ 0.03883137]\n",
      " ...\n",
      " [-0.0953208 ]\n",
      " [-0.12393333]\n",
      " [-0.02572415]]\n",
      "t [[-0.77771912]\n",
      " [-0.19423823]\n",
      " [ 0.08774895]\n",
      " ...\n",
      " [-0.17338271]\n",
      " [-0.22116053]\n",
      " [-0.05155948]]\n",
      "t [[-0.77771912]\n",
      " [-0.19423823]\n",
      " [ 0.08774895]\n",
      " ...\n",
      " [-0.17338271]\n",
      " [-0.22116053]\n",
      " [-0.05155948]]\n",
      "Current iteration=2, loss=33497.161439856136\n",
      "t [[-1.04208538]\n",
      " [-0.27732399]\n",
      " [ 0.13358453]\n",
      " ...\n",
      " [-0.23817709]\n",
      " [-0.29897416]\n",
      " [-0.07513038]]\n",
      "t [[-1.04208538]\n",
      " [-0.27732399]\n",
      " [ 0.13358453]\n",
      " ...\n",
      " [-0.23817709]\n",
      " [-0.29897416]\n",
      " [-0.07513038]]\n",
      "t [[-1.25558815]\n",
      " [-0.34879435]\n",
      " [ 0.17268989]\n",
      " ...\n",
      " [-0.29269114]\n",
      " [-0.36262777]\n",
      " [-0.09608774]]\n",
      "t [[-1.25558815]\n",
      " [-0.34879435]\n",
      " [ 0.17268989]\n",
      " ...\n",
      " [-0.29269114]\n",
      " [-0.36262777]\n",
      " [-0.09608774]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4, loss=31862.55452067843\n",
      "t [[-1.43191616]\n",
      " [-0.41042078]\n",
      " [ 0.20487022]\n",
      " ...\n",
      " [-0.33910865]\n",
      " [-0.41571732]\n",
      " [-0.11462225]]\n",
      "t [[-1.43191616]\n",
      " [-0.41042078]\n",
      " [ 0.20487022]\n",
      " ...\n",
      " [-0.33910865]\n",
      " [-0.41571732]\n",
      " [-0.11462225]]\n",
      "t [[-1.58018563]\n",
      " [-0.46398439]\n",
      " [ 0.23097177]\n",
      " ...\n",
      " [-0.37904033]\n",
      " [-0.46072212]\n",
      " [-0.13101441]]\n",
      "t [[-1.58018563]\n",
      " [-0.46398439]\n",
      " [ 0.23097177]\n",
      " ...\n",
      " [-0.37904033]\n",
      " [-0.46072212]\n",
      " [-0.13101441]]\n",
      "Current iteration=6, loss=31067.913880114582\n",
      "t [[-1.70669739]\n",
      " [-0.51094909]\n",
      " [ 0.25204365]\n",
      " ...\n",
      " [-0.41369391]\n",
      " [-0.49939331]\n",
      " [-0.14552598]]\n",
      "t [[-1.70669739]\n",
      " [-0.51094909]\n",
      " [ 0.25204365]\n",
      " ...\n",
      " [-0.41369391]\n",
      " [-0.49939331]\n",
      " [-0.14552598]]\n",
      "t [[-1.81595727]\n",
      " [-0.55246375]\n",
      " [ 0.26906483]\n",
      " ...\n",
      " [-0.44399133]\n",
      " [-0.53300159]\n",
      " [-0.15838198]]\n",
      "t [[-1.81595727]\n",
      " [-0.55246375]\n",
      " [ 0.26906483]\n",
      " ...\n",
      " [-0.44399133]\n",
      " [-0.53300159]\n",
      " [-0.15838198]]\n",
      "Current iteration=8, loss=30607.730247856278\n",
      "t [[-1.91128391]\n",
      " [-0.5894255 ]\n",
      " [ 0.28286938]\n",
      " ...\n",
      " [-0.4706487 ]\n",
      " [-0.56249259]\n",
      " [-0.16977378]]\n",
      "t [[-1.91128391]\n",
      " [-0.5894255 ]\n",
      " [ 0.28286938]\n",
      " ...\n",
      " [-0.4706487 ]\n",
      " [-0.56249259]\n",
      " [-0.16977378]]\n",
      "t [[-1.99518358]\n",
      " [-0.62254076]\n",
      " [ 0.29414052]\n",
      " ...\n",
      " [-0.49423122]\n",
      " [-0.58858602]\n",
      " [-0.17986521]]\n",
      "loss=30305.74983200083\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.44239295]\n",
      " [-0.10253216]\n",
      " [ 0.02664649]\n",
      " ...\n",
      " [-0.09422154]\n",
      " [-0.12466129]\n",
      " [-0.02521388]]\n",
      "t [[-0.44239295]\n",
      " [-0.10253216]\n",
      " [ 0.02664649]\n",
      " ...\n",
      " [-0.09422154]\n",
      " [-0.12466129]\n",
      " [-0.02521388]]\n",
      "t [[-0.77982883]\n",
      " [-0.19913173]\n",
      " [ 0.06890374]\n",
      " ...\n",
      " [-0.17216954]\n",
      " [-0.22247131]\n",
      " [-0.05060299]]\n",
      "t [[-0.77982883]\n",
      " [-0.19913173]\n",
      " [ 0.06890374]\n",
      " ...\n",
      " [-0.17216954]\n",
      " [-0.22247131]\n",
      " [-0.05060299]]\n",
      "Current iteration=2, loss=33442.47100949681\n",
      "t [[-1.04494977]\n",
      " [-0.28354646]\n",
      " [ 0.11146189]\n",
      " ...\n",
      " [-0.23731665]\n",
      " [-0.30077497]\n",
      " [-0.07391309]]\n",
      "t [[-1.04494977]\n",
      " [-0.28354646]\n",
      " [ 0.11146189]\n",
      " ...\n",
      " [-0.23731665]\n",
      " [-0.30077497]\n",
      " [-0.07391309]]\n",
      "t [[-1.25903569]\n",
      " [-0.35616368]\n",
      " [ 0.14924563]\n",
      " ...\n",
      " [-0.29238464]\n",
      " [-0.36485573]\n",
      " [-0.09477046]]\n",
      "t [[-1.25903569]\n",
      " [-0.35616368]\n",
      " [ 0.14924563]\n",
      " ...\n",
      " [-0.29238464]\n",
      " [-0.36485573]\n",
      " [-0.09477046]]\n",
      "Current iteration=4, loss=31787.889479705365\n",
      "t [[-1.43582313]\n",
      " [-0.41883918]\n",
      " [ 0.18123813]\n",
      " ...\n",
      " [-0.33942729]\n",
      " [-0.41832534]\n",
      " [-0.11332162]]\n",
      "t [[-1.43582313]\n",
      " [-0.41883918]\n",
      " [ 0.18123813]\n",
      " ...\n",
      " [-0.33942729]\n",
      " [-0.41832534]\n",
      " [-0.11332162]]\n",
      "t [[-1.58445748]\n",
      " [-0.47338883]\n",
      " [ 0.20782385]\n",
      " ...\n",
      " [-0.37999204]\n",
      " [-0.46367199]\n",
      " [-0.12981131]]\n",
      "t [[-1.58445748]\n",
      " [-0.47338883]\n",
      " [ 0.20782385]\n",
      " ...\n",
      " [-0.37999204]\n",
      " [-0.46367199]\n",
      " [-0.12981131]]\n",
      "Current iteration=6, loss=30984.62367579985\n",
      "t [[-1.71125667]\n",
      " [-0.52129518]\n",
      " [ 0.22978822]\n",
      " ...\n",
      " [-0.41525618]\n",
      " [-0.50265235]\n",
      " [-0.14447614]]\n",
      "t [[-1.71125667]\n",
      " [-0.52129518]\n",
      " [ 0.22978822]\n",
      " ...\n",
      " [-0.41525618]\n",
      " [-0.50265235]\n",
      " [-0.14447614]]\n",
      "t [[-1.82073716]\n",
      " [-0.56371891]\n",
      " [ 0.24795497]\n",
      " ...\n",
      " [-0.44612761]\n",
      " [-0.53654098]\n",
      " [-0.15752412]]\n",
      "t [[-1.82073716]\n",
      " [-0.56371891]\n",
      " [ 0.24795497]\n",
      " ...\n",
      " [-0.44612761]\n",
      " [-0.53654098]\n",
      " [-0.15752412]]\n",
      "Current iteration=8, loss=30520.604482953066\n",
      "t [[-1.91622489]\n",
      " [-0.60156523]\n",
      " [ 0.26306359]\n",
      " ...\n",
      " [-0.47331672]\n",
      " [-0.56628643]\n",
      " [-0.16913508]]\n",
      "t [[-1.91622489]\n",
      " [-0.60156523]\n",
      " [ 0.26306359]\n",
      " ...\n",
      " [-0.47331672]\n",
      " [-0.56628643]\n",
      " [-0.16913508]]\n",
      "t [[-2.0002318 ]\n",
      " [-0.6355461 ]\n",
      " [ 0.27573759]\n",
      " ...\n",
      " [-0.49738724]\n",
      " [-0.5926108 ]\n",
      " [-0.17946501]]\n",
      "loss=30217.13865308153\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.43890357]\n",
      " [-0.09546331]\n",
      " [ 0.03124573]\n",
      " ...\n",
      " [ 0.49192995]\n",
      " [-0.36221007]\n",
      " [ 0.39901241]]\n",
      "t [[-0.43890357]\n",
      " [-0.09546331]\n",
      " [ 0.03124573]\n",
      " ...\n",
      " [ 0.49192995]\n",
      " [-0.36221007]\n",
      " [ 0.39901241]]\n",
      "t [[-0.77417281]\n",
      " [-0.18781042]\n",
      " [ 0.07627838]\n",
      " ...\n",
      " [ 0.84870069]\n",
      " [-0.63131796]\n",
      " [ 0.69688991]]\n",
      "t [[-0.77417281]\n",
      " [-0.18781042]\n",
      " [ 0.07627838]\n",
      " ...\n",
      " [ 0.84870069]\n",
      " [-0.63131796]\n",
      " [ 0.69688991]]\n",
      "Current iteration=2, loss=33582.586829087406\n",
      "t [[-1.03814161]\n",
      " [-0.26900525]\n",
      " [ 0.12027424]\n",
      " ...\n",
      " [ 1.11118197]\n",
      " [-0.8334953 ]\n",
      " [ 0.92308962]]\n",
      "t [[-1.03814161]\n",
      " [-0.26900525]\n",
      " [ 0.12027424]\n",
      " ...\n",
      " [ 1.11118197]\n",
      " [-0.8334953 ]\n",
      " [ 0.92308962]]\n",
      "t [[-1.25170573]\n",
      " [-0.33887761]\n",
      " [ 0.15863628]\n",
      " ...\n",
      " [ 1.30795114]\n",
      " [-0.98821659]\n",
      " [ 1.09858906]]\n",
      "t [[-1.25170573]\n",
      " [-0.33887761]\n",
      " [ 0.15863628]\n",
      " ...\n",
      " [ 1.30795114]\n",
      " [-0.98821659]\n",
      " [ 1.09858906]]\n",
      "Current iteration=4, loss=31970.323685188767\n",
      "t [[-1.42834519]\n",
      " [-0.39909544]\n",
      " [ 0.19065508]\n",
      " ...\n",
      " [ 1.45794892]\n",
      " [-1.10878154]\n",
      " [ 1.23747661]]\n",
      "t [[-1.42834519]\n",
      " [-0.39909544]\n",
      " [ 0.19065508]\n",
      " ...\n",
      " [ 1.45794892]\n",
      " [-1.10878154]\n",
      " [ 1.23747661]]\n",
      "t [[-1.57705251]\n",
      " [-0.45139826]\n",
      " [ 0.21690111]\n",
      " ...\n",
      " [ 1.57382572]\n",
      " [-1.20421379]\n",
      " [ 1.34926299]]\n",
      "t [[-1.57705251]\n",
      " [-0.45139826]\n",
      " [ 0.21690111]\n",
      " ...\n",
      " [ 1.57382572]\n",
      " [-1.20421379]\n",
      " [ 1.34926299]]\n",
      "Current iteration=6, loss=31184.11005850511\n",
      "t [[-1.70405568]\n",
      " [-0.49722953]\n",
      " [ 0.2382751 ]\n",
      " ...\n",
      " [ 1.66424682]\n",
      " [-1.28075304]\n",
      " [ 1.4405241 ]]\n",
      "t [[-1.70405568]\n",
      " [-0.49722953]\n",
      " [ 0.2382751 ]\n",
      " ...\n",
      " [ 1.66424682]\n",
      " [-1.28075304]\n",
      " [ 1.4405241 ]]\n",
      "t [[-1.81381738]\n",
      " [-0.53772535]\n",
      " [ 0.25567564]\n",
      " ...\n",
      " [ 1.7353159 ]\n",
      " [-1.3428237 ]\n",
      " [ 1.51593212]]\n",
      "t [[-1.81381738]\n",
      " [-0.53772535]\n",
      " [ 0.25567564]\n",
      " ...\n",
      " [ 1.7353159 ]\n",
      " [-1.3428237 ]\n",
      " [ 1.51593212]]\n",
      "Current iteration=8, loss=30728.707753936775\n",
      "t [[-1.90963055]\n",
      " [-0.57377328]\n",
      " [ 0.26989303]\n",
      " ...\n",
      " [ 1.79144457]\n",
      " [-1.393641  ]\n",
      " [ 1.57889241]]\n",
      "t [[-1.90963055]\n",
      " [-0.57377328]\n",
      " [ 0.26989303]\n",
      " ...\n",
      " [ 1.79144457]\n",
      " [-1.393641  ]\n",
      " [ 1.57889241]]\n",
      "t [[-1.99398642]\n",
      " [-0.60607152]\n",
      " [ 0.28158668]\n",
      " ...\n",
      " [ 1.83589381]\n",
      " [-1.43559313]\n",
      " [ 1.63194315]]\n",
      "loss=30430.361524932097\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.38489467]\n",
      " [-0.08096962]\n",
      " [ 0.08036261]\n",
      " ...\n",
      " [-0.10158194]\n",
      " [-0.12629942]\n",
      " [-0.02655097]]\n",
      "t [[-0.38489467]\n",
      " [-0.08096962]\n",
      " [ 0.08036261]\n",
      " ...\n",
      " [-0.10158194]\n",
      " [-0.12629942]\n",
      " [-0.02655097]]\n",
      "t [[-0.66118367]\n",
      " [-0.11462301]\n",
      " [ 0.1472605 ]\n",
      " ...\n",
      " [-0.18444015]\n",
      " [-0.22482209]\n",
      " [-0.05267563]]\n",
      "t [[-0.66118367]\n",
      " [-0.11462301]\n",
      " [ 0.1472605 ]\n",
      " ...\n",
      " [-0.18444015]\n",
      " [-0.22482209]\n",
      " [-0.05267563]]\n",
      "Current iteration=2, loss=33461.602668051135\n",
      "t [[-0.8579845 ]\n",
      " [-0.13121569]\n",
      " [ 0.19780359]\n",
      " ...\n",
      " [-0.25281965]\n",
      " [-0.30330772]\n",
      " [-0.07627847]]\n",
      "t [[-0.8579845 ]\n",
      " [-0.13121569]\n",
      " [ 0.19780359]\n",
      " ...\n",
      " [-0.25281965]\n",
      " [-0.30330772]\n",
      " [-0.07627847]]\n",
      "t [[-0.99884491]\n",
      " [-0.14247747]\n",
      " [ 0.23513224]\n",
      " ...\n",
      " [-0.30997624]\n",
      " [-0.36727038]\n",
      " [-0.09716398]]\n",
      "t [[-0.99884491]\n",
      " [-0.14247747]\n",
      " [ 0.23513224]\n",
      " ...\n",
      " [-0.30997624]\n",
      " [-0.36727038]\n",
      " [-0.09716398]]\n",
      "Current iteration=4, loss=31844.109056997317\n",
      "t [[-1.10009251]\n",
      " [-0.15287835]\n",
      " [ 0.26269946]\n",
      " ...\n",
      " [-0.35831438]\n",
      " [-0.42044841]\n",
      " [-0.11558901]]\n",
      "t [[-1.10009251]\n",
      " [-0.15287835]\n",
      " [ 0.26269946]\n",
      " ...\n",
      " [-0.35831438]\n",
      " [-0.42044841]\n",
      " [-0.11558901]]\n",
      "t [[-1.17274515]\n",
      " [-0.16410718]\n",
      " [ 0.28314889]\n",
      " ...\n",
      " [-0.3996133 ]\n",
      " [-0.46540115]\n",
      " [-0.13186045]]\n",
      "t [[-1.17274515]\n",
      " [-0.16410718]\n",
      " [ 0.28314889]\n",
      " ...\n",
      " [-0.3996133 ]\n",
      " [-0.46540115]\n",
      " [-0.13186045]]\n",
      "Current iteration=6, loss=31067.764157232144\n",
      "t [[-1.22433567]\n",
      " [-0.1767334 ]\n",
      " [ 0.29836242]\n",
      " ...\n",
      " [-0.43520711]\n",
      " [-0.50392716]\n",
      " [-0.14625079]]\n",
      "t [[-1.22433567]\n",
      " [-0.1767334 ]\n",
      " [ 0.29836242]\n",
      " ...\n",
      " [-0.43520711]\n",
      " [-0.50392716]\n",
      " [-0.14625079]]\n",
      "t [[-1.26013894]\n",
      " [-0.19085673]\n",
      " [ 0.30967084]\n",
      " ...\n",
      " [-0.46611274]\n",
      " [-0.53732669]\n",
      " [-0.15898983]]\n",
      "t [[-1.26013894]\n",
      " [-0.19085673]\n",
      " [ 0.30967084]\n",
      " ...\n",
      " [-0.46611274]\n",
      " [-0.53732669]\n",
      " [-0.15898983]]\n",
      "Current iteration=8, loss=30622.554710042103\n",
      "t [[-1.28394115]\n",
      " [-0.20637965]\n",
      " [ 0.31802733]\n",
      " ...\n",
      " [-0.49311843]\n",
      " [-0.56656468]\n",
      " [-0.17027143]]\n",
      "t [[-1.28394115]\n",
      " [-0.20637965]\n",
      " [ 0.31802733]\n",
      " ...\n",
      " [-0.49311843]\n",
      " [-0.56656468]\n",
      " [-0.17027143]]\n",
      "t [[-1.29852136]\n",
      " [-0.22312819]\n",
      " [ 0.32412685]\n",
      " ...\n",
      " [-0.51684484]\n",
      " [-0.59237394]\n",
      " [-0.180261  ]]\n",
      "loss=30332.77474605017\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.45202691]\n",
      " [-0.10183592]\n",
      " [ 0.03977847]\n",
      " ...\n",
      " [-0.0976457 ]\n",
      " [-0.1269561 ]\n",
      " [-0.02635157]]\n",
      "t [[-0.45202691]\n",
      " [-0.10183592]\n",
      " [ 0.03977847]\n",
      " ...\n",
      " [-0.0976457 ]\n",
      " [-0.1269561 ]\n",
      " [-0.02635157]]\n",
      "t [[-0.79413127]\n",
      " [-0.1988565 ]\n",
      " [ 0.0901325 ]\n",
      " ...\n",
      " [-0.17719085]\n",
      " [-0.22590127]\n",
      " [-0.05281993]]\n",
      "t [[-0.79413127]\n",
      " [-0.1988565 ]\n",
      " [ 0.0901325 ]\n",
      " ...\n",
      " [-0.17719085]\n",
      " [-0.22590127]\n",
      " [-0.05281993]]\n",
      "Current iteration=2, loss=33430.36776012533\n",
      "t [[-1.06148028]\n",
      " [-0.2833909 ]\n",
      " [ 0.13693242]\n",
      " ...\n",
      " [-0.24292802]\n",
      " [-0.3046757 ]\n",
      " [-0.07685743]]\n",
      "t [[-1.06148028]\n",
      " [-0.2833909 ]\n",
      " [ 0.13693242]\n",
      " ...\n",
      " [-0.24292802]\n",
      " [-0.3046757 ]\n",
      " [-0.07685743]]\n",
      "t [[-1.27651712]\n",
      " [-0.3557596 ]\n",
      " [ 0.17650648]\n",
      " ...\n",
      " [-0.29803   ]\n",
      " [-0.36885618]\n",
      " [-0.09813949]]\n",
      "t [[-1.27651712]\n",
      " [-0.3557596 ]\n",
      " [ 0.17650648]\n",
      " ...\n",
      " [-0.29803   ]\n",
      " [-0.36885618]\n",
      " [-0.09813949]]\n",
      "Current iteration=4, loss=31805.03385632064\n",
      "t [[-1.4535475 ]\n",
      " [-0.41793537]\n",
      " [ 0.20880559]\n",
      " ...\n",
      " [-0.34479719]\n",
      " [-0.42221622]\n",
      " [-0.1168943 ]]\n",
      "t [[-1.4535475 ]\n",
      " [-0.41793537]\n",
      " [ 0.20880559]\n",
      " ...\n",
      " [-0.34479719]\n",
      " [-0.42221622]\n",
      " [-0.1168943 ]]\n",
      "t [[-1.60202101]\n",
      " [-0.47182633]\n",
      " [ 0.23480916]\n",
      " ...\n",
      " [-0.38491546]\n",
      " [-0.46733458]\n",
      " [-0.13342819]]\n",
      "t [[-1.60202101]\n",
      " [-0.47182633]\n",
      " [ 0.23480916]\n",
      " ...\n",
      " [-0.38491546]\n",
      " [-0.46733458]\n",
      " [-0.13342819]]\n",
      "Current iteration=6, loss=31022.389485731976\n",
      "t [[-1.72842997]\n",
      " [-0.518972  ]\n",
      " [ 0.25566194]\n",
      " ...\n",
      " [-0.41964213]\n",
      " [-0.50602062]\n",
      " [-0.14802003]]\n",
      "t [[-1.72842997]\n",
      " [-0.518972  ]\n",
      " [ 0.25566194]\n",
      " ...\n",
      " [-0.41964213]\n",
      " [-0.50602062]\n",
      " [-0.14802003]]\n",
      "t [[-1.83739601]\n",
      " [-0.56056744]\n",
      " [ 0.27240682]\n",
      " ...\n",
      " [-0.44993263]\n",
      " [-0.53958057]\n",
      " [-0.16090707]]\n",
      "t [[-1.83739601]\n",
      " [-0.56056744]\n",
      " [ 0.27240682]\n",
      " ...\n",
      " [-0.44993263]\n",
      " [-0.53958057]\n",
      " [-0.16090707]]\n",
      "Current iteration=8, loss=30570.52948066623\n",
      "t [[-1.93231065]\n",
      " [-0.59753932]\n",
      " [ 0.28591884]\n",
      " ...\n",
      " [-0.47652615]\n",
      " [-0.56898265]\n",
      " [-0.17229002]]\n",
      "t [[-1.93231065]\n",
      " [-0.59753932]\n",
      " [ 0.28591884]\n",
      " ...\n",
      " [-0.47652615]\n",
      " [-0.56898265]\n",
      " [-0.17229002]]\n",
      "t [[-2.01572752]\n",
      " [-0.63061379]\n",
      " [ 0.2969065 ]\n",
      " ...\n",
      " [-0.50000411]\n",
      " [-0.59496127]\n",
      " [-0.18234031]]\n",
      "loss=30274.021442966674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.45318302]\n",
      " [-0.10503294]\n",
      " [ 0.0272964 ]\n",
      " ...\n",
      " [-0.09651963]\n",
      " [-0.12770181]\n",
      " [-0.02582885]]\n",
      "t [[-0.45318302]\n",
      " [-0.10503294]\n",
      " [ 0.0272964 ]\n",
      " ...\n",
      " [-0.09651963]\n",
      " [-0.12770181]\n",
      " [-0.02582885]]\n",
      "t [[-0.7962899 ]\n",
      " [-0.20383591]\n",
      " [ 0.07096168]\n",
      " ...\n",
      " [-0.17597209]\n",
      " [-0.22724068]\n",
      " [-0.05184162]]\n",
      "t [[-0.7962899 ]\n",
      " [-0.20383591]\n",
      " [ 0.07096168]\n",
      " ...\n",
      " [-0.17597209]\n",
      " [-0.22724068]\n",
      " [-0.05184162]]\n",
      "Current iteration=2, loss=33374.825187879025\n",
      "t [[-1.06440293]\n",
      " [-0.28971009]\n",
      " [ 0.1145656 ]\n",
      " ...\n",
      " [-0.24209287]\n",
      " [-0.30651302]\n",
      " [-0.07562086]]\n",
      "t [[-1.06440293]\n",
      " [-0.28971009]\n",
      " [ 0.1145656 ]\n",
      " ...\n",
      " [-0.24209287]\n",
      " [-0.30651302]\n",
      " [-0.07562086]]\n",
      "t [[-1.28002622]\n",
      " [-0.36324065]\n",
      " [ 0.15292658]\n",
      " ...\n",
      " [-0.29777683]\n",
      " [-0.3711269 ]\n",
      " [-0.09681205]]\n",
      "t [[-1.28002622]\n",
      " [-0.36324065]\n",
      " [ 0.15292658]\n",
      " ...\n",
      " [-0.29777683]\n",
      " [-0.3711269 ]\n",
      " [-0.09681205]]\n",
      "Current iteration=4, loss=31729.679852299207\n",
      "t [[-1.45751619]\n",
      " [-0.42648212]\n",
      " [ 0.18514354]\n",
      " ...\n",
      " [-0.34519144]\n",
      " [-0.424872  ]\n",
      " [-0.11559517]]\n",
      "t [[-1.45751619]\n",
      " [-0.42648212]\n",
      " [ 0.18514354]\n",
      " ...\n",
      " [-0.34519144]\n",
      " [-0.424872  ]\n",
      " [-0.11559517]]\n",
      "t [[-1.60635248]\n",
      " [-0.48137613]\n",
      " [ 0.21172584]\n",
      " ...\n",
      " [-0.38595943]\n",
      " [-0.47033609]\n",
      " [-0.13223879]]\n",
      "t [[-1.60635248]\n",
      " [-0.48137613]\n",
      " [ 0.21172584]\n",
      " ...\n",
      " [-0.38595943]\n",
      " [-0.47033609]\n",
      " [-0.13223879]]\n",
      "Current iteration=6, loss=30938.63580486878\n",
      "t [[-1.73304473]\n",
      " [-0.52948053]\n",
      " [ 0.23355392]\n",
      " ...\n",
      " [-0.42130855]\n",
      " [-0.50933418]\n",
      " [-0.14699583]]\n",
      "t [[-1.73304473]\n",
      " [-0.52948053]\n",
      " [ 0.23355392]\n",
      " ...\n",
      " [-0.42130855]\n",
      " [-0.50933418]\n",
      " [-0.14699583]]\n",
      "t [[-1.84222528]\n",
      " [-0.57200217]\n",
      " [ 0.25151686]\n",
      " ...\n",
      " [-0.45218108]\n",
      " [-0.54317645]\n",
      " [-0.16008619]]\n",
      "t [[-1.84222528]\n",
      " [-0.57200217]\n",
      " [ 0.25151686]\n",
      " ...\n",
      " [-0.45218108]\n",
      " [-0.54317645]\n",
      " [-0.16008619]]\n",
      "Current iteration=8, loss=30483.13584658946\n",
      "t [[-1.9372931 ]\n",
      " [-0.60987582]\n",
      " [ 0.26639689]\n",
      " ...\n",
      " [-0.47931123]\n",
      " [-0.57283412]\n",
      " [-0.17169902]]\n",
      "t [[-1.9372931 ]\n",
      " [-0.60987582]\n",
      " [ 0.26639689]\n",
      " ...\n",
      " [-0.47931123]\n",
      " [-0.57283412]\n",
      " [-0.17169902]]\n",
      "t [[-2.02080765]\n",
      " [-0.64383317]\n",
      " [ 0.27884427]\n",
      " ...\n",
      " [-0.50327963]\n",
      " [-0.59904412]\n",
      " [-0.18199794]]\n",
      "loss=30185.298677759507\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.44960853]\n",
      " [-0.09779169]\n",
      " [ 0.03200782]\n",
      " ...\n",
      " [ 0.50392825]\n",
      " [-0.37104446]\n",
      " [ 0.40874442]]\n",
      "t [[-0.44960853]\n",
      " [-0.09779169]\n",
      " [ 0.03200782]\n",
      " ...\n",
      " [ 0.50392825]\n",
      " [-0.37104446]\n",
      " [ 0.40874442]]\n",
      "t [[-0.79052679]\n",
      " [-0.19230714]\n",
      " [ 0.07847153]\n",
      " ...\n",
      " [ 0.86609914]\n",
      " [-0.64443885]\n",
      " [ 0.71141758]]\n",
      "t [[-0.79052679]\n",
      " [-0.19230714]\n",
      " [ 0.07847153]\n",
      " ...\n",
      " [ 0.86609914]\n",
      " [-0.64443885]\n",
      " [ 0.71141758]]\n",
      "Current iteration=2, loss=33516.92199073757\n",
      "t [[-1.05750567]\n",
      " [-0.27493145]\n",
      " [ 0.12348278]\n",
      " ...\n",
      " [ 1.1304414 ]\n",
      " [-0.8483188 ]\n",
      " [ 0.93968398]]\n",
      "t [[-1.05750567]\n",
      " [-0.27493145]\n",
      " [ 0.12348278]\n",
      " ...\n",
      " [ 1.1304414 ]\n",
      " [-0.8483188 ]\n",
      " [ 0.93968398]]\n",
      "t [[-1.27263769]\n",
      " [-0.34568351]\n",
      " [ 0.16237239]\n",
      " ...\n",
      " [ 1.3272573 ]\n",
      " [-1.00337829]\n",
      " [ 1.11579922]]\n",
      "t [[-1.27263769]\n",
      " [-0.34568351]\n",
      " [ 0.16237239]\n",
      " ...\n",
      " [ 1.3272573 ]\n",
      " [-1.00337829]\n",
      " [ 1.11579922]]\n",
      "Current iteration=4, loss=31913.493271595777\n",
      "t [[-1.4500109 ]\n",
      " [-0.40643436]\n",
      " [ 0.19456114]\n",
      " ...\n",
      " [ 1.47638035]\n",
      " [-1.12357155]\n",
      " [ 1.25452782]]\n",
      "t [[-1.4500109 ]\n",
      " [-0.40643436]\n",
      " [ 0.19456114]\n",
      " ...\n",
      " [ 1.47638035]\n",
      " [-1.12357155]\n",
      " [ 1.25452782]]\n",
      "t [[-1.5989482 ]\n",
      " [-0.45905182]\n",
      " [ 0.22074961]\n",
      " ...\n",
      " [ 1.5909283 ]\n",
      " [-1.21827001]\n",
      " [ 1.36574211]]\n",
      "t [[-1.5989482 ]\n",
      " [-0.45905182]\n",
      " [ 0.22074961]\n",
      " ...\n",
      " [ 1.5909283 ]\n",
      " [-1.21827001]\n",
      " [ 1.36574211]]\n",
      "Current iteration=6, loss=31139.056392864317\n",
      "t [[-1.72586851]\n",
      " [-0.50505528]\n",
      " [ 0.24193527]\n",
      " ...\n",
      " [ 1.67981961]\n",
      " [-1.29390349]\n",
      " [ 1.45621851]]\n",
      "t [[-1.72586851]\n",
      " [-0.50505528]\n",
      " [ 0.24193527]\n",
      " ...\n",
      " [ 1.67981961]\n",
      " [-1.29390349]\n",
      " [ 1.45621851]]\n",
      "t [[-1.8353506 ]\n",
      " [-0.54562698]\n",
      " [ 0.25908264]\n",
      " ...\n",
      " [ 1.74929933]\n",
      " [-1.35500377]\n",
      " [ 1.53074441]]\n",
      "t [[-1.8353506 ]\n",
      " [-0.54562698]\n",
      " [ 0.25908264]\n",
      " ...\n",
      " [ 1.74929933]\n",
      " [-1.35500377]\n",
      " [ 1.53074441]]\n",
      "Current iteration=8, loss=30691.91671422644\n",
      "t [[-1.93076093]\n",
      " [-0.58168368]\n",
      " [ 0.27302496]\n",
      " ...\n",
      " [ 1.80385943]\n",
      " [-1.40484801]\n",
      " [ 1.592793  ]]\n",
      "t [[-1.93076093]\n",
      " [-0.58168368]\n",
      " [ 0.27302496]\n",
      " ...\n",
      " [ 1.80385943]\n",
      " [-1.40484801]\n",
      " [ 1.592793  ]]\n",
      "t [[-2.01463869]\n",
      " [-0.61394294]\n",
      " [ 0.28444841]\n",
      " ...\n",
      " [ 1.84680632]\n",
      " [-1.44586035]\n",
      " [ 1.64494285]]\n",
      "loss=30399.051389865013\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.39405883]\n",
      " [-0.08289747]\n",
      " [ 0.082276  ]\n",
      " ...\n",
      " [-0.10400056]\n",
      " [-0.12930654]\n",
      " [-0.02718314]]\n",
      "t [[-0.39405883]\n",
      " [-0.08289747]\n",
      " [ 0.082276  ]\n",
      " ...\n",
      " [-0.10400056]\n",
      " [-0.12930654]\n",
      " [-0.02718314]]\n",
      "t [[-0.67433274]\n",
      " [-0.11622561]\n",
      " [ 0.15044093]\n",
      " ...\n",
      " [-0.18838697]\n",
      " [-0.22951251]\n",
      " [-0.05391991]]\n",
      "t [[-0.67433274]\n",
      " [-0.11622561]\n",
      " [ 0.15044093]\n",
      " ...\n",
      " [-0.18838697]\n",
      " [-0.22951251]\n",
      " [-0.05391991]]\n",
      "Current iteration=2, loss=33396.4579873184\n",
      "t [[-0.87207547]\n",
      " [-0.13240865]\n",
      " [ 0.20141302]\n",
      " ...\n",
      " [-0.2577167 ]\n",
      " [-0.30892397]\n",
      " [-0.07796737]]\n",
      "t [[-0.87207547]\n",
      " [-0.13240865]\n",
      " [ 0.20141302]\n",
      " ...\n",
      " [-0.2577167 ]\n",
      " [-0.30892397]\n",
      " [-0.07796737]]\n",
      "t [[-1.01233578]\n",
      " [-0.14354693]\n",
      " [ 0.23869871]\n",
      " ...\n",
      " [-0.3154448 ]\n",
      " [-0.37338351]\n",
      " [-0.09916128]]\n",
      "t [[-1.01233578]\n",
      " [-0.14354693]\n",
      " [ 0.23869871]\n",
      " ...\n",
      " [-0.3154448 ]\n",
      " [-0.37338351]\n",
      " [-0.09916128]]\n",
      "Current iteration=4, loss=31788.847132227165\n",
      "t [[-1.11224224]\n",
      " [-0.15409624]\n",
      " [ 0.26600173]\n",
      " ...\n",
      " [-0.36410249]\n",
      " [-0.42680722]\n",
      " [-0.11779544]]\n",
      "t [[-1.11224224]\n",
      " [-0.15409624]\n",
      " [ 0.26600173]\n",
      " ...\n",
      " [-0.36410249]\n",
      " [-0.42680722]\n",
      " [-0.11779544]]\n",
      "t [[-1.18322184]\n",
      " [-0.16567604]\n",
      " [ 0.28609445]\n",
      " ...\n",
      " [-0.40555075]\n",
      " [-0.471853  ]\n",
      " [-0.13420111]]\n",
      "t [[-1.18322184]\n",
      " [-0.16567604]\n",
      " [ 0.28609445]\n",
      " ...\n",
      " [-0.40555075]\n",
      " [-0.471853  ]\n",
      " [-0.13420111]]\n",
      "Current iteration=6, loss=31024.505113262865\n",
      "t [[-1.23302063]\n",
      " [-0.17879256]\n",
      " [ 0.30092186]\n",
      " ...\n",
      " [-0.44117729]\n",
      " [-0.51037668]\n",
      " [-0.14866682]]\n",
      "t [[-1.23302063]\n",
      " [-0.17879256]\n",
      " [ 0.30092186]\n",
      " ...\n",
      " [-0.44117729]\n",
      " [-0.51037668]\n",
      " [-0.14866682]]\n",
      "t [[-1.26702773]\n",
      " [-0.1934966 ]\n",
      " [ 0.31184587]\n",
      " ...\n",
      " [-0.47203498]\n",
      " [-0.54371331]\n",
      " [-0.161434  ]]\n",
      "t [[-1.26702773]\n",
      " [-0.1934966 ]\n",
      " [ 0.31184587]\n",
      " ...\n",
      " [-0.47203498]\n",
      " [-0.54371331]\n",
      " [-0.161434  ]]\n",
      "Current iteration=8, loss=30587.522788711085\n",
      "t [[-1.28909144]\n",
      " [-0.20965441]\n",
      " [ 0.31983513]\n",
      " ...\n",
      " [-0.49893662]\n",
      " [-0.57284983]\n",
      " [-0.17270552]]\n",
      "t [[-1.28909144]\n",
      " [-0.20965441]\n",
      " [ 0.31983513]\n",
      " ...\n",
      " [-0.49893662]\n",
      " [-0.57284983]\n",
      " [-0.17270552]]\n",
      "t [[-1.3020241 ]\n",
      " [-0.22706561]\n",
      " [ 0.32559205]\n",
      " ...\n",
      " [-0.52252001]\n",
      " [-0.59853335]\n",
      " [-0.18265417]]\n",
      "loss=30303.124847492567\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.46278945]\n",
      " [-0.10426059]\n",
      " [ 0.04072558]\n",
      " ...\n",
      " [-0.09997059]\n",
      " [-0.12997886]\n",
      " [-0.02697899]]\n",
      "t [[-0.46278945]\n",
      " [-0.10426059]\n",
      " [ 0.04072558]\n",
      " ...\n",
      " [-0.09997059]\n",
      " [-0.12997886]\n",
      " [-0.02697899]]\n",
      "t [[-0.81042613]\n",
      " [-0.20346876]\n",
      " [ 0.09252701]\n",
      " ...\n",
      " [-0.18097971]\n",
      " [-0.2306119 ]\n",
      " [-0.05408053]]\n",
      "t [[-0.81042613]\n",
      " [-0.20346876]\n",
      " [ 0.09252701]\n",
      " ...\n",
      " [-0.18097971]\n",
      " [-0.2306119 ]\n",
      " [-0.05408053]]\n",
      "Current iteration=2, loss=33364.8867522919\n",
      "t [[-1.08063815]\n",
      " [-0.28941746]\n",
      " [ 0.14026967]\n",
      " ...\n",
      " [-0.24763537]\n",
      " [-0.31031285]\n",
      " [-0.07857719]]\n",
      "t [[-1.08063815]\n",
      " [-0.28941746]\n",
      " [ 0.14026967]\n",
      " ...\n",
      " [-0.24763537]\n",
      " [-0.31031285]\n",
      " [-0.07857719]]\n",
      "t [[-1.29710977]\n",
      " [-0.36264698]\n",
      " [ 0.18027888]\n",
      " ...\n",
      " [-0.30330104]\n",
      " [-0.37499039]\n",
      " [-0.10017443]]\n",
      "t [[-1.29710977]\n",
      " [-0.36264698]\n",
      " [ 0.18027888]\n",
      " ...\n",
      " [-0.30330104]\n",
      " [-0.37499039]\n",
      " [-0.10017443]]\n",
      "Current iteration=4, loss=31749.46422266603\n",
      "t [[-1.47476474]\n",
      " [-0.42533999]\n",
      " [ 0.2126648 ]\n",
      " ...\n",
      " [-0.35039581]\n",
      " [-0.42859697]\n",
      " [-0.11914   ]]\n",
      "t [[-1.47476474]\n",
      " [-0.42533999]\n",
      " [ 0.2126648 ]\n",
      " ...\n",
      " [-0.35039581]\n",
      " [-0.42859697]\n",
      " [-0.11914   ]]\n",
      "t [[-1.62338275]\n",
      " [-0.4795324 ]\n",
      " [ 0.23854516]\n",
      " ...\n",
      " [-0.39068146]\n",
      " [-0.47381026]\n",
      " [-0.13580646]]\n",
      "t [[-1.62338275]\n",
      " [-0.4795324 ]\n",
      " [ 0.23854516]\n",
      " ...\n",
      " [-0.39068146]\n",
      " [-0.47381026]\n",
      " [-0.13580646]]\n",
      "Current iteration=6, loss=30978.670897683383\n",
      "t [[-1.74964411]\n",
      " [-0.52683829]\n",
      " [ 0.25916161]\n",
      " ...\n",
      " [-0.42546498]\n",
      " [-0.51249687]\n",
      " [-0.15046987]]\n",
      "t [[-1.74964411]\n",
      " [-0.52683829]\n",
      " [ 0.25916161]\n",
      " ...\n",
      " [-0.42546498]\n",
      " [-0.51249687]\n",
      " [-0.15046987]]\n",
      "t [[-1.85828322]\n",
      " [-0.56849777]\n",
      " [ 0.27562045]\n",
      " ...\n",
      " [-0.45573507]\n",
      " [-0.54599773]\n",
      " [-0.16337972]]\n",
      "t [[-1.85828322]\n",
      " [-0.56849777]\n",
      " [ 0.27562045]\n",
      " ...\n",
      " [-0.45573507]\n",
      " [-0.54599773]\n",
      " [-0.16337972]]\n",
      "Current iteration=8, loss=30534.85593827365\n",
      "t [[-1.95276212]\n",
      " [-0.60546619]\n",
      " [ 0.28883659]\n",
      " ...\n",
      " [-0.48225367]\n",
      " [-0.57530288]\n",
      " [-0.17474615]]\n",
      "t [[-1.95276212]\n",
      " [-0.60546619]\n",
      " [ 0.28883659]\n",
      " ...\n",
      " [-0.48225367]\n",
      " [-0.57530288]\n",
      " [-0.17474615]]\n",
      "t [[-2.03567988]\n",
      " [-0.63848881]\n",
      " [ 0.29954242]\n",
      " ...\n",
      " [-0.50561814]\n",
      " [-0.60116092]\n",
      " [-0.18474831]]\n",
      "loss=30243.578879060915\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.46397309]\n",
      " [-0.10753373]\n",
      " [ 0.02794632]\n",
      " ...\n",
      " [-0.09881772]\n",
      " [-0.13074232]\n",
      " [-0.02644383]]\n",
      "t [[-0.46397309]\n",
      " [-0.10753373]\n",
      " [ 0.02794632]\n",
      " ...\n",
      " [-0.09881772]\n",
      " [-0.13074232]\n",
      " [-0.02644383]]\n",
      "t [[-0.81263364]\n",
      " [-0.20853252]\n",
      " [ 0.07303668]\n",
      " ...\n",
      " [-0.17975646]\n",
      " [-0.23197979]\n",
      " [-0.05308046]]\n",
      "t [[-0.81263364]\n",
      " [-0.20853252]\n",
      " [ 0.07303668]\n",
      " ...\n",
      " [-0.17975646]\n",
      " [-0.23197979]\n",
      " [-0.05308046]]\n",
      "Current iteration=2, loss=33308.509318170945\n",
      "t [[-1.08361858]\n",
      " [-0.29583188]\n",
      " [ 0.11766961]\n",
      " ...\n",
      " [-0.24682703]\n",
      " [-0.31218644]\n",
      " [-0.07732193]]\n",
      "t [[-1.08361858]\n",
      " [-0.29583188]\n",
      " [ 0.11766961]\n",
      " ...\n",
      " [-0.24682703]\n",
      " [-0.31218644]\n",
      " [-0.07732193]]\n",
      "t [[-1.30067958]\n",
      " [-0.3702386 ]\n",
      " [ 0.15657621]\n",
      " ...\n",
      " [-0.30310253]\n",
      " [-0.37730353]\n",
      " [-0.09883789]]\n",
      "t [[-1.30067958]\n",
      " [-0.3702386 ]\n",
      " [ 0.15657621]\n",
      " ...\n",
      " [-0.30310253]\n",
      " [-0.37730353]\n",
      " [-0.09883789]]\n",
      "Current iteration=4, loss=31673.447166377577\n",
      "t [[-1.478794  ]\n",
      " [-0.43401412]\n",
      " [ 0.18898552]\n",
      " ...\n",
      " [-0.3508665 ]\n",
      " [-0.43130005]\n",
      " [-0.11784367]]\n",
      "t [[-1.478794  ]\n",
      " [-0.43401412]\n",
      " [ 0.18898552]\n",
      " ...\n",
      " [-0.3508665 ]\n",
      " [-0.43130005]\n",
      " [-0.11784367]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[-1.6277724 ]\n",
      " [-0.48922668]\n",
      " [ 0.21553806]\n",
      " ...\n",
      " [-0.39181791]\n",
      " [-0.47686284]\n",
      " [-0.13463215]]\n",
      "t [[-1.6277724 ]\n",
      " [-0.48922668]\n",
      " [ 0.21553806]\n",
      " ...\n",
      " [-0.39181791]\n",
      " [-0.47686284]\n",
      " [-0.13463215]]\n",
      "Current iteration=6, loss=30894.479678194286\n",
      "t [[-1.75431259]\n",
      " [-0.53750839]\n",
      " [ 0.23721109]\n",
      " ...\n",
      " [-0.42723511]\n",
      " [-0.51586424]\n",
      " [-0.14947264]]\n",
      "t [[-1.75431259]\n",
      " [-0.53750839]\n",
      " [ 0.23721109]\n",
      " ...\n",
      " [-0.42723511]\n",
      " [-0.51586424]\n",
      " [-0.14947264]]\n",
      "t [[-1.86315976]\n",
      " [-0.58011122]\n",
      " [ 0.25495889]\n",
      " ...\n",
      " [-0.45809461]\n",
      " [-0.54964921]\n",
      " [-0.16259707]]\n",
      "t [[-1.86315976]\n",
      " [-0.58011122]\n",
      " [ 0.25495889]\n",
      " ...\n",
      " [-0.45809461]\n",
      " [-0.54964921]\n",
      " [-0.16259707]]\n",
      "Current iteration=8, loss=30447.21755327614\n",
      "t [[-1.95778353]\n",
      " [-0.61799863]\n",
      " [ 0.2696054 ]\n",
      " ...\n",
      " [-0.48515418]\n",
      " [-0.57921095]\n",
      " [-0.17420399]]\n",
      "t [[-1.95778353]\n",
      " [-0.61799863]\n",
      " [ 0.2696054 ]\n",
      " ...\n",
      " [-0.48515418]\n",
      " [-0.57921095]\n",
      " [-0.17420399]]\n",
      "t [[-2.04078904]\n",
      " [-0.65192142]\n",
      " [ 0.28182634]\n",
      " ...\n",
      " [-0.50901098]\n",
      " [-0.60530063]\n",
      " [-0.18446478]]\n",
      "loss=30154.763737158413\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.4603135 ]\n",
      " [-0.10012006]\n",
      " [ 0.03276992]\n",
      " ...\n",
      " [ 0.51592654]\n",
      " [-0.37987885]\n",
      " [ 0.41847644]]\n",
      "t [[-0.4603135 ]\n",
      " [-0.10012006]\n",
      " [ 0.03276992]\n",
      " ...\n",
      " [ 0.51592654]\n",
      " [-0.37987885]\n",
      " [ 0.41847644]]\n",
      "t [[-0.80676474]\n",
      " [-0.19679942]\n",
      " [ 0.0806797 ]\n",
      " ...\n",
      " [ 0.88334579]\n",
      " [-0.65745483]\n",
      " [ 0.72583176]]\n",
      "t [[-0.80676474]\n",
      " [-0.19679942]\n",
      " [ 0.0806797 ]\n",
      " ...\n",
      " [ 0.88334579]\n",
      " [-0.65745483]\n",
      " [ 0.72583176]]\n",
      "Current iteration=2, loss=33452.537517267534\n",
      "t [[-1.07663515]\n",
      " [-0.28081908]\n",
      " [ 0.12668706]\n",
      " ...\n",
      " [ 1.1493896 ]\n",
      " [-0.86292067]\n",
      " [ 0.95604201]]\n",
      "t [[-1.07663515]\n",
      " [-0.28081908]\n",
      " [ 0.12668706]\n",
      " ...\n",
      " [ 1.1493896 ]\n",
      " [-0.86292067]\n",
      " [ 0.95604201]]\n",
      "t [[-1.29323621]\n",
      " [-0.35241334]\n",
      " [ 0.16607116]\n",
      " ...\n",
      " [ 1.34612767]\n",
      " [-1.01822464]\n",
      " [ 1.13267357]]\n",
      "t [[-1.29323621]\n",
      " [-0.35241334]\n",
      " [ 0.16607116]\n",
      " ...\n",
      " [ 1.34612767]\n",
      " [-1.01822464]\n",
      " [ 1.13267357]]\n",
      "Current iteration=4, loss=31858.580883331237\n",
      "t [[-1.47126516]\n",
      " [-0.41366537]\n",
      " [ 0.19839723]\n",
      " ...\n",
      " [ 1.49428935]\n",
      " [-1.13797958]\n",
      " [ 1.27117057]]\n",
      "t [[-1.47126516]\n",
      " [-0.41366537]\n",
      " [ 0.19839723]\n",
      " ...\n",
      " [ 1.49428935]\n",
      " [-1.13797958]\n",
      " [ 1.27117057]]\n",
      "t [[-1.62037232]\n",
      " [-0.46657201]\n",
      " [ 0.22450178]\n",
      " ...\n",
      " [ 1.60745342]\n",
      " [-1.23190053]\n",
      " [ 1.381763  ]]\n",
      "t [[-1.62037232]\n",
      " [-0.46657201]\n",
      " [ 0.22450178]\n",
      " ...\n",
      " [ 1.60745342]\n",
      " [-1.23190053]\n",
      " [ 1.381763  ]]\n",
      "Current iteration=6, loss=31095.787775787292\n",
      "t [[-1.74716418]\n",
      " [-0.5127275 ]\n",
      " [ 0.24548064]\n",
      " ...\n",
      " [ 1.6947844 ]\n",
      " [-1.30660245]\n",
      " [ 1.47142302]]\n",
      "t [[-1.74716418]\n",
      " [-0.5127275 ]\n",
      " [ 0.24548064]\n",
      " ...\n",
      " [ 1.6947844 ]\n",
      " [-1.30660245]\n",
      " [ 1.47142302]]\n",
      "t [[-1.8563327 ]\n",
      " [-0.553359  ]\n",
      " [ 0.26236396]\n",
      " ...\n",
      " [ 1.76266289]\n",
      " [-1.3667202 ]\n",
      " [ 1.54504907]]\n",
      "t [[-1.8563327 ]\n",
      " [-0.553359  ]\n",
      " [ 0.26236396]\n",
      " ...\n",
      " [ 1.76266289]\n",
      " [-1.3667202 ]\n",
      " [ 1.54504907]]\n",
      "Current iteration=8, loss=30656.641009524243\n",
      "t [[-1.95131548]\n",
      " [-0.58941158]\n",
      " [ 0.27602684]\n",
      " ...\n",
      " [ 1.81565659]\n",
      " [-1.41558963]\n",
      " [ 1.60617892]]\n",
      "t [[-1.95131548]\n",
      " [-0.58941158]\n",
      " [ 0.27602684]\n",
      " ...\n",
      " [ 1.81565659]\n",
      " [-1.41558963]\n",
      " [ 1.60617892]]\n",
      "t [[-2.03469792]\n",
      " [-0.6216215 ]\n",
      " [ 0.28718093]\n",
      " ...\n",
      " [ 1.85711383]\n",
      " [-1.45566827]\n",
      " [ 1.65742897]]\n",
      "loss=30369.019922277996\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.40322299]\n",
      " [-0.08482532]\n",
      " [ 0.0841894 ]\n",
      " ...\n",
      " [-0.10641918]\n",
      " [-0.13231367]\n",
      " [-0.02781531]]\n",
      "t [[-0.40322299]\n",
      " [-0.08482532]\n",
      " [ 0.0841894 ]\n",
      " ...\n",
      " [-0.10641918]\n",
      " [-0.13231367]\n",
      " [-0.02781531]]\n",
      "t [[-0.687365  ]\n",
      " [-0.11777768]\n",
      " [ 0.15360639]\n",
      " ...\n",
      " [-0.19231393]\n",
      " [-0.23417313]\n",
      " [-0.05516376]]\n",
      "t [[-0.687365  ]\n",
      " [-0.11777768]\n",
      " [ 0.15360639]\n",
      " ...\n",
      " [-0.19231393]\n",
      " [-0.23417313]\n",
      " [-0.05516376]]\n",
      "Current iteration=2, loss=33332.5902972802\n",
      "t [[-0.88591524]\n",
      " [-0.13354703]\n",
      " [ 0.20497039]\n",
      " ...\n",
      " [-0.26256858]\n",
      " [-0.314477  ]\n",
      " [-0.07964857]]\n",
      "t [[-0.88591524]\n",
      " [-0.13354703]\n",
      " [ 0.20497039]\n",
      " ...\n",
      " [-0.26256858]\n",
      " [-0.314477  ]\n",
      " [-0.07964857]]\n",
      "t [[-1.02547191]\n",
      " [-0.14457963]\n",
      " [ 0.24218187]\n",
      " ...\n",
      " [-0.32084291]\n",
      " [-0.37940464]\n",
      " [-0.10114197]]\n",
      "t [[-1.02547191]\n",
      " [-0.14457963]\n",
      " [ 0.24218187]\n",
      " ...\n",
      " [-0.32084291]\n",
      " [-0.37940464]\n",
      " [-0.10114197]]\n",
      "Current iteration=4, loss=31735.45032645822\n",
      "t [[-1.12396895]\n",
      " [-0.15530088]\n",
      " [ 0.26920065]\n",
      " ...\n",
      " [-0.36979732]\n",
      " [-0.43305108]\n",
      " [-0.1199764 ]]\n",
      "t [[-1.12396895]\n",
      " [-0.15530088]\n",
      " [ 0.26920065]\n",
      " ...\n",
      " [-0.36979732]\n",
      " [-0.43305108]\n",
      " [-0.1199764 ]]\n",
      "t [[-1.19323583]\n",
      " [-0.16725429]\n",
      " [ 0.28892575]\n",
      " ...\n",
      " [-0.41137528]\n",
      " [-0.47817215]\n",
      " [-0.13650774]]\n",
      "t [[-1.19323583]\n",
      " [-0.16725429]\n",
      " [ 0.28892575]\n",
      " ...\n",
      " [-0.41137528]\n",
      " [-0.47817215]\n",
      " [-0.13650774]]\n",
      "Current iteration=6, loss=30982.954159061468\n",
      "t [[-1.24122466]\n",
      " [-0.1808803 ]\n",
      " [ 0.30336249]\n",
      " ...\n",
      " [-0.4470182 ]\n",
      " [-0.51667998]\n",
      " [-0.15104067]]\n",
      "t [[-1.24122466]\n",
      " [-0.1808803 ]\n",
      " [ 0.30336249]\n",
      " ...\n",
      " [-0.4470182 ]\n",
      " [-0.51667998]\n",
      " [-0.15104067]]\n",
      "t [[-1.27343318]\n",
      " [-0.19617992]\n",
      " [ 0.31390189]\n",
      " ...\n",
      " [-0.4778146 ]\n",
      " [-0.54994358]\n",
      " [-0.16382825]]\n",
      "t [[-1.27343318]\n",
      " [-0.19617992]\n",
      " [ 0.31390189]\n",
      " ...\n",
      " [-0.4778146 ]\n",
      " [-0.54994358]\n",
      " [-0.16382825]]\n",
      "Current iteration=8, loss=30553.924680173353\n",
      "t [[-1.29376765]\n",
      " [-0.21298302]\n",
      " [ 0.32152667]\n",
      " ...\n",
      " [-0.5046015 ]\n",
      " [-0.57897117]\n",
      " [-0.17508249]]\n",
      "t [[-1.29376765]\n",
      " [-0.21298302]\n",
      " [ 0.32152667]\n",
      " ...\n",
      " [-0.5046015 ]\n",
      " [-0.57897117]\n",
      " [-0.17508249]]\n",
      "t [[-1.30507064]\n",
      " [-0.2310631 ]\n",
      " [ 0.32694578]\n",
      " ...\n",
      " [-0.52803346]\n",
      " [-0.60452365]\n",
      " [-0.18498369]]\n",
      "loss=30274.674493086695\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.473552  ]\n",
      " [-0.10668525]\n",
      " [ 0.04167269]\n",
      " ...\n",
      " [-0.10229549]\n",
      " [-0.13300163]\n",
      " [-0.02760641]]\n",
      "t [[-0.473552  ]\n",
      " [-0.10668525]\n",
      " [ 0.04167269]\n",
      " ...\n",
      " [-0.10229549]\n",
      " [-0.13300163]\n",
      " [-0.02760641]]\n",
      "t [[-0.82660411]\n",
      " [-0.208075  ]\n",
      " [ 0.09493244]\n",
      " ...\n",
      " [-0.18474936]\n",
      " [-0.2352925 ]\n",
      " [-0.05534127]]\n",
      "t [[-0.82660411]\n",
      " [-0.208075  ]\n",
      " [ 0.09493244]\n",
      " ...\n",
      " [-0.18474936]\n",
      " [-0.2352925 ]\n",
      " [-0.05534127]]\n",
      "Current iteration=2, loss=33300.693849243966\n",
      "t [[-1.09956225]\n",
      " [-0.29540332]\n",
      " [ 0.14359536]\n",
      " ...\n",
      " [-0.2522996 ]\n",
      " [-0.31588638]\n",
      " [-0.08028952]]\n",
      "t [[-1.09956225]\n",
      " [-0.29540332]\n",
      " [ 0.14359536]\n",
      " ...\n",
      " [-0.2522996 ]\n",
      " [-0.31588638]\n",
      " [-0.08028952]]\n",
      "t [[-1.31737295]\n",
      " [-0.36945683]\n",
      " [ 0.18400626]\n",
      " ...\n",
      " [-0.3085053 ]\n",
      " [-0.38103214]\n",
      " [-0.10219253]]\n",
      "t [[-1.31737295]\n",
      " [-0.36945683]\n",
      " [ 0.18400626]\n",
      " ...\n",
      " [-0.3085053 ]\n",
      " [-0.38103214]\n",
      " [-0.10219253]]\n",
      "Current iteration=4, loss=31695.76358508699\n",
      "t [[-1.49557829]\n",
      " [-0.43263619]\n",
      " [ 0.21644786]\n",
      " ...\n",
      " [-0.35590623]\n",
      " [-0.43486237]\n",
      " [-0.12135953]]\n",
      "t [[-1.49557829]\n",
      " [-0.43263619]\n",
      " [ 0.21644786]\n",
      " ...\n",
      " [-0.35590623]\n",
      " [-0.43486237]\n",
      " [-0.12135953]]\n",
      "t [[-1.64428457]\n",
      " [-0.48710542]\n",
      " [ 0.24218108]\n",
      " ...\n",
      " [-0.39634079]\n",
      " [-0.48015295]\n",
      " [-0.13814966]]\n",
      "t [[-1.64428457]\n",
      " [-0.48710542]\n",
      " [ 0.24218108]\n",
      " ...\n",
      " [-0.39634079]\n",
      " [-0.48015295]\n",
      " [-0.13814966]]\n",
      "Current iteration=6, loss=30936.658654999785\n",
      "t [[-1.77035646]\n",
      " [-0.5345519 ]\n",
      " [ 0.26254541]\n",
      " ...\n",
      " [-0.43116566]\n",
      " [-0.51882675]\n",
      " [-0.15287617]]\n",
      "t [[-1.77035646]\n",
      " [-0.5345519 ]\n",
      " [ 0.26254541]\n",
      " ...\n",
      " [-0.43116566]\n",
      " [-0.51882675]\n",
      " [-0.15287617]]\n",
      "t [[-1.87863813]\n",
      " [-0.57625967]\n",
      " [ 0.27870986]\n",
      " ...\n",
      " [-0.46140257]\n",
      " [-0.55225852]\n",
      " [-0.16580084]]\n",
      "t [[-1.87863813]\n",
      " [-0.57625967]\n",
      " [ 0.27870986]\n",
      " ...\n",
      " [-0.46140257]\n",
      " [-0.55225852]\n",
      " [-0.16580084]]\n",
      "Current iteration=8, loss=30500.614975596392\n",
      "t [[-1.97265973]\n",
      " [-0.61321192]\n",
      " [ 0.29162796]\n",
      " ...\n",
      " [-0.48783589]\n",
      " [-0.58145943]\n",
      " [-0.17714338]]\n",
      "t [[-1.97265973]\n",
      " [-0.61321192]\n",
      " [ 0.29162796]\n",
      " ...\n",
      " [-0.48783589]\n",
      " [-0.58145943]\n",
      " [-0.17714338]]\n",
      "t [[-2.05506397]\n",
      " [-0.6461724 ]\n",
      " [ 0.30205461]\n",
      " ...\n",
      " [-0.51107857]\n",
      " [-0.60719169]\n",
      " [-0.18709068]]\n",
      "loss=30214.338755147855\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.47476316]\n",
      " [-0.11003451]\n",
      " [ 0.02859623]\n",
      " ...\n",
      " [-0.1011158 ]\n",
      " [-0.13378284]\n",
      " [-0.0270588 ]]\n",
      "t [[-0.47476316]\n",
      " [-0.11003451]\n",
      " [ 0.02859623]\n",
      " ...\n",
      " [-0.1011158 ]\n",
      " [-0.13378284]\n",
      " [-0.0270588 ]]\n",
      "t [[-0.82886045]\n",
      " [-0.21322153]\n",
      " [ 0.07512868]\n",
      " ...\n",
      " [-0.18352271]\n",
      " [-0.23668873]\n",
      " [-0.05431952]]\n",
      "t [[-0.82886045]\n",
      " [-0.21322153]\n",
      " [ 0.07512868]\n",
      " ...\n",
      " [-0.18352271]\n",
      " [-0.23668873]\n",
      " [-0.05431952]]\n",
      "Current iteration=2, loss=33243.498484717085\n",
      "t [[-1.10259998]\n",
      " [-0.30191154]\n",
      " [ 0.12077282]\n",
      " ...\n",
      " [-0.25151956]\n",
      " [-0.31779599]\n",
      " [-0.07901619]]\n",
      "t [[-1.10259998]\n",
      " [-0.30191154]\n",
      " [ 0.12077282]\n",
      " ...\n",
      " [-0.25151956]\n",
      " [-0.31779599]\n",
      " [-0.07901619]]\n",
      "t [[-1.32100264]\n",
      " [-0.37715792]\n",
      " [ 0.16019326]\n",
      " ...\n",
      " [-0.30836268]\n",
      " [-0.38338735]\n",
      " [-0.10084792]]\n",
      "t [[-1.32100264]\n",
      " [-0.37715792]\n",
      " [ 0.16019326]\n",
      " ...\n",
      " [-0.30836268]\n",
      " [-0.38338735]\n",
      " [-0.10084792]]\n",
      "Current iteration=4, loss=31619.10836427089\n",
      "t [[-1.49966701]\n",
      " [-0.4414368 ]\n",
      " [ 0.19276351]\n",
      " ...\n",
      " [-0.3564541 ]\n",
      " [-0.43761231]\n",
      " [-0.12006726]]\n",
      "t [[-1.49966701]\n",
      " [-0.4414368 ]\n",
      " [ 0.19276351]\n",
      " ...\n",
      " [-0.3564541 ]\n",
      " [-0.43761231]\n",
      " [-0.12006726]]\n",
      "t [[-1.648731  ]\n",
      " [-0.49694332]\n",
      " [ 0.21926116]\n",
      " ...\n",
      " [-0.39756982]\n",
      " [-0.48325602]\n",
      " [-0.13699174]]\n",
      "t [[-1.648731  ]\n",
      " [-0.49694332]\n",
      " [ 0.21926116]\n",
      " ...\n",
      " [-0.39756982]\n",
      " [-0.48325602]\n",
      " [-0.13699174]]\n",
      "Current iteration=6, loss=30852.0545582353\n",
      "t [[-1.77507692]\n",
      " [-0.54538276]\n",
      " [ 0.2407618 ]\n",
      " ...\n",
      " [-0.43303897]\n",
      " [-0.52224719]\n",
      " [-0.15190718]]\n",
      "t [[-1.77507692]\n",
      " [-0.54538276]\n",
      " [ 0.2407618 ]\n",
      " ...\n",
      " [-0.43303897]\n",
      " [-0.52224719]\n",
      " [-0.15190718]]\n",
      "t [[-1.88355984]\n",
      " [-0.58805105]\n",
      " [ 0.25828452]\n",
      " ...\n",
      " [-0.46387206]\n",
      " [-0.55596474]\n",
      " [-0.1650576 ]]\n",
      "t [[-1.88355984]\n",
      " [-0.58805105]\n",
      " [ 0.25828452]\n",
      " ...\n",
      " [-0.46387206]\n",
      " [-0.55596474]\n",
      " [-0.1650576 ]]\n",
      "Current iteration=8, loss=30412.75359813198\n",
      "t [[-1.97771765]\n",
      " [-0.62593953]\n",
      " [ 0.27269384]\n",
      " ...\n",
      " [-0.49085013]\n",
      " [-0.58542306]\n",
      " [-0.17665111]]\n",
      "t [[-1.97771765]\n",
      " [-0.62593953]\n",
      " [ 0.27269384]\n",
      " ...\n",
      " [-0.49085013]\n",
      " [-0.58542306]\n",
      " [-0.17665111]]\n",
      "t [[-2.06019932]\n",
      " [-0.65981746]\n",
      " [ 0.28468956]\n",
      " ...\n",
      " [-0.51458653]\n",
      " [-0.61138707]\n",
      " [-0.18686695]]\n",
      "loss=30125.44910585404\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.47101846]\n",
      " [-0.10244843]\n",
      " [ 0.03353201]\n",
      " ...\n",
      " [ 0.52792483]\n",
      " [-0.38871325]\n",
      " [ 0.42820845]]\n",
      "t [[-0.47101846]\n",
      " [-0.10244843]\n",
      " [ 0.03353201]\n",
      " ...\n",
      " [ 0.52792483]\n",
      " [-0.38871325]\n",
      " [ 0.42820845]]\n",
      "t [[-0.82288708]\n",
      " [-0.20128724]\n",
      " [ 0.08290279]\n",
      " ...\n",
      " [ 0.90044113]\n",
      " [-0.67036625]\n",
      " [ 0.74013283]]\n",
      "t [[-0.82288708]\n",
      " [-0.20128724]\n",
      " [ 0.08290279]\n",
      " ...\n",
      " [ 0.90044113]\n",
      " [-0.67036625]\n",
      " [ 0.74013283]]\n",
      "Current iteration=2, loss=33389.40963350069\n",
      "t [[-1.09553327]\n",
      " [-0.28666768]\n",
      " [ 0.12988602]\n",
      " ...\n",
      " [ 1.16803067]\n",
      " [-0.87730362]\n",
      " [ 0.97216673]]\n",
      "t [[-1.09553327]\n",
      " [-0.28666768]\n",
      " [ 0.12988602]\n",
      " ...\n",
      " [ 1.16803067]\n",
      " [-0.87730362]\n",
      " [ 0.97216673]]\n",
      "t [[-1.31350804]\n",
      " [-0.35906735]\n",
      " [ 0.16973148]\n",
      " ...\n",
      " [ 1.36457146]\n",
      " [-1.03276198]\n",
      " [ 1.14921892]]\n",
      "t [[-1.31350804]\n",
      " [-0.35906735]\n",
      " [ 0.16973148]\n",
      " ...\n",
      " [ 1.36457146]\n",
      " [-1.03276198]\n",
      " [ 1.14921892]]\n",
      "Current iteration=4, loss=31805.506432984825\n",
      "t [[-1.49211827]\n",
      " [-0.42078993]\n",
      " [ 0.20216302]\n",
      " ...\n",
      " [ 1.51169034]\n",
      " [-1.15201577]\n",
      " [ 1.2874156 ]]\n",
      "t [[-1.49211827]\n",
      " [-0.42078993]\n",
      " [ 0.20216302]\n",
      " ...\n",
      " [ 1.51169034]\n",
      " [-1.15201577]\n",
      " [ 1.2874156 ]]\n",
      "t [[-1.64133843]\n",
      " [-0.47396158]\n",
      " [ 0.22815856]\n",
      " ...\n",
      " [ 1.62342011]\n",
      " [-1.24511897]\n",
      " [ 1.39733999]]\n",
      "t [[-1.64133843]\n",
      " [-0.47396158]\n",
      " [ 0.22815856]\n",
      " ...\n",
      " [ 1.62342011]\n",
      " [-1.24511897]\n",
      " [ 1.39733999]]\n",
      "Current iteration=6, loss=31054.206461604095\n",
      "t [[-1.76795916]\n",
      " [-0.52025009]\n",
      " [ 0.24891358]\n",
      " ...\n",
      " [ 1.70916416]\n",
      " [-1.31886645]\n",
      " [ 1.486155  ]]\n",
      "t [[-1.76795916]\n",
      " [-0.52025009]\n",
      " [ 0.24891358]\n",
      " ...\n",
      " [ 1.70916416]\n",
      " [-1.31886645]\n",
      " [ 1.486155  ]]\n",
      "t [[-1.87678269]\n",
      " [-0.56092631]\n",
      " [ 0.2655234 ]\n",
      " ...\n",
      " [ 1.77543269]\n",
      " [-1.37799195]\n",
      " [ 1.55886603]]\n",
      "t [[-1.87678269]\n",
      " [-0.56092631]\n",
      " [ 0.2655234 ]\n",
      " ...\n",
      " [ 1.77543269]\n",
      " [-1.37799195]\n",
      " [ 1.55886603]]\n",
      "Current iteration=8, loss=30622.787173688695\n",
      "t [[-1.97131549]\n",
      " [-0.59696275]\n",
      " [ 0.27890375]\n",
      " ...\n",
      " [ 1.82686465]\n",
      " [-1.42588672]\n",
      " [ 1.61907218]]\n",
      "t [[-1.97131549]\n",
      " [-0.59696275]\n",
      " [ 0.27890375]\n",
      " ...\n",
      " [ 1.82686465]\n",
      " [-1.42588672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 1.61907218]]\n",
      "t [[-2.05418728]\n",
      " [-0.62911369]\n",
      " [ 0.28979034]\n",
      " ...\n",
      " [ 1.86684681]\n",
      " [-1.4650392 ]\n",
      " [ 1.66942519]]\n",
      "loss=30340.184446810148\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.41238715]\n",
      " [-0.08675316]\n",
      " [ 0.0861028 ]\n",
      " ...\n",
      " [-0.1088378 ]\n",
      " [-0.1353208 ]\n",
      " [-0.02844747]]\n",
      "t [[-0.41238715]\n",
      " [-0.08675316]\n",
      " [ 0.0861028 ]\n",
      " ...\n",
      " [-0.1088378 ]\n",
      " [-0.1353208 ]\n",
      " [-0.02844747]]\n",
      "t [[-0.70028078]\n",
      " [-0.11927939]\n",
      " [ 0.15675691]\n",
      " ...\n",
      " [-0.19622111]\n",
      " [-0.23880405]\n",
      " [-0.05640718]]\n",
      "t [[-0.70028078]\n",
      " [-0.11927939]\n",
      " [ 0.15675691]\n",
      " ...\n",
      " [-0.19622111]\n",
      " [-0.23880405]\n",
      " [-0.05640718]]\n",
      "Current iteration=2, loss=33269.975780791865\n",
      "t [[-0.89950669]\n",
      " [-0.13463299]\n",
      " [ 0.2084757 ]\n",
      " ...\n",
      " [-0.26737574]\n",
      " [-0.31996756]\n",
      " [-0.08132197]]\n",
      "t [[-0.89950669]\n",
      " [-0.13463299]\n",
      " [ 0.2084757 ]\n",
      " ...\n",
      " [-0.26737574]\n",
      " [-0.31996756]\n",
      " [-0.08132197]]\n",
      "t [[-1.03826043]\n",
      " [-0.14557861]\n",
      " [ 0.24558282]\n",
      " ...\n",
      " [-0.32617163]\n",
      " [-0.38533543]\n",
      " [-0.10310605]]\n",
      "t [[-1.03826043]\n",
      " [-0.14557861]\n",
      " [ 0.24558282]\n",
      " ...\n",
      " [-0.32617163]\n",
      " [-0.38533543]\n",
      " [-0.10310605]]\n",
      "Current iteration=4, loss=31683.840905348945\n",
      "t [[-1.13528422]\n",
      " [-0.15649529]\n",
      " [ 0.27229872]\n",
      " ...\n",
      " [-0.37540059]\n",
      " [-0.43918265]\n",
      " [-0.12213208]]\n",
      "t [[-1.13528422]\n",
      " [-0.15649529]\n",
      " [ 0.27229872]\n",
      " ...\n",
      " [-0.37540059]\n",
      " [-0.43918265]\n",
      " [-0.12213208]]\n",
      "t [[-1.20280259]\n",
      " [-0.16884438]\n",
      " [ 0.29164646]\n",
      " ...\n",
      " [-0.4170894 ]\n",
      " [-0.48436221]\n",
      " [-0.13878078]]\n",
      "t [[-1.20280259]\n",
      " [-0.16884438]\n",
      " [ 0.29164646]\n",
      " ...\n",
      " [-0.4170894 ]\n",
      " [-0.48436221]\n",
      " [-0.13878078]]\n",
      "Current iteration=6, loss=30943.01831331223\n",
      "t [[-1.24896638]\n",
      " [-0.18299825]\n",
      " [ 0.30568892]\n",
      " ...\n",
      " [-0.45273312]\n",
      " [-0.52284153]\n",
      " [-0.15337297]]\n",
      "t [[-1.24896638]\n",
      " [-0.18299825]\n",
      " [ 0.30568892]\n",
      " ...\n",
      " [-0.45273312]\n",
      " [-0.52284153]\n",
      " [-0.15337297]]\n",
      "t [[-1.27937637]\n",
      " [-0.19890736]\n",
      " [ 0.31584416]\n",
      " ...\n",
      " [-0.48345563]\n",
      " [-0.5560227 ]\n",
      " [-0.16617345]]\n",
      "t [[-1.27937637]\n",
      " [-0.19890736]\n",
      " [ 0.31584416]\n",
      " ...\n",
      " [-0.48345563]\n",
      " [-0.5560227 ]\n",
      " [-0.16617345]]\n",
      "Current iteration=8, loss=30521.672625370193\n",
      "t [[-1.29799273]\n",
      " [-0.21636519]\n",
      " [ 0.32310768]\n",
      " ...\n",
      " [-0.51011779]\n",
      " [-0.58493455]\n",
      " [-0.17740348]]\n",
      "t [[-1.29799273]\n",
      " [-0.21636519]\n",
      " [ 0.32310768]\n",
      " ...\n",
      " [-0.51011779]\n",
      " [-0.58493455]\n",
      " [-0.17740348]]\n",
      "t [[-1.30768527]\n",
      " [-0.23511939]\n",
      " [ 0.32819406]\n",
      " ...\n",
      " [-0.53339054]\n",
      " [-0.61035119]\n",
      " [-0.18725094]]\n",
      "loss=30247.34677923602\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.48431454]\n",
      " [-0.10910992]\n",
      " [ 0.04261979]\n",
      " ...\n",
      " [-0.10462039]\n",
      " [-0.13602439]\n",
      " [-0.02823383]]\n",
      "t [[-0.48431454]\n",
      " [-0.10910992]\n",
      " [ 0.04261979]\n",
      " ...\n",
      " [-0.10462039]\n",
      " [-0.13602439]\n",
      " [-0.02823383]]\n",
      "t [[-0.84266562]\n",
      " [-0.21267518]\n",
      " [ 0.09734873]\n",
      " ...\n",
      " [-0.18849986]\n",
      " [-0.23994317]\n",
      " [-0.05660217]]\n",
      "t [[-0.84266562]\n",
      " [-0.21267518]\n",
      " [ 0.09734873]\n",
      " ...\n",
      " [-0.18849986]\n",
      " [-0.23994317]\n",
      " [-0.05660217]]\n",
      "Current iteration=2, loss=33237.76482375628\n",
      "t [[-1.11825581]\n",
      " [-0.30134809]\n",
      " [ 0.14690863]\n",
      " ...\n",
      " [-0.25692118]\n",
      " [-0.32139704]\n",
      " [-0.0819943 ]]\n",
      "t [[-1.11825581]\n",
      " [-0.30134809]\n",
      " [ 0.14690863]\n",
      " ...\n",
      " [-0.25692118]\n",
      " [-0.32139704]\n",
      " [-0.0819943 ]]\n",
      "t [[-1.33731338]\n",
      " [-0.37618952]\n",
      " [ 0.18768784]\n",
      " ...\n",
      " [-0.31364376]\n",
      " [-0.38698312]\n",
      " [-0.10419375]]\n",
      "t [[-1.33731338]\n",
      " [-0.37618952]\n",
      " [ 0.18768784]\n",
      " ...\n",
      " [-0.31364376]\n",
      " [-0.38698312]\n",
      " [-0.10419375]]\n",
      "Current iteration=4, loss=31643.853647842145\n",
      "t [[-1.51599826]\n",
      " [-0.43982553]\n",
      " [ 0.22015489]\n",
      " ...\n",
      " [-0.36133011]\n",
      " [-0.44101512]\n",
      " [-0.12355308]]\n",
      "t [[-1.51599826]\n",
      " [-0.43982553]\n",
      " [ 0.22015489]\n",
      " ...\n",
      " [-0.36133011]\n",
      " [-0.44101512]\n",
      " [-0.12355308]]\n",
      "t [[-1.66473968]\n",
      " [-0.49454812]\n",
      " [ 0.24571832]\n",
      " ...\n",
      " [-0.40189581]\n",
      " [-0.48636629]\n",
      " [-0.1404582 ]]\n",
      "t [[-1.66473968]\n",
      " [-0.49454812]\n",
      " [ 0.24571832]\n",
      " ...\n",
      " [-0.40189581]\n",
      " [-0.48636629]\n",
      " [-0.1404582 ]]\n",
      "Current iteration=6, loss=30896.259658089486\n",
      "t [[-1.79058298]\n",
      " [-0.54211665]\n",
      " [ 0.26581609]\n",
      " ...\n",
      " [-0.43674726]\n",
      " [-0.52501475]\n",
      " [-0.15523957]]\n",
      "t [[-1.79058298]\n",
      " [-0.54211665]\n",
      " [ 0.26581609]\n",
      " ...\n",
      " [-0.43674726]\n",
      " [-0.52501475]\n",
      " [-0.15523957]]\n",
      "t [[-1.89847908]\n",
      " [-0.58385793]\n",
      " [ 0.28167912]\n",
      " ...\n",
      " [-0.46693891]\n",
      " [-0.55836818]\n",
      " [-0.16817135]]\n",
      "t [[-1.89847908]\n",
      " [-0.58385793]\n",
      " [ 0.28167912]\n",
      " ...\n",
      " [-0.46693891]\n",
      " [-0.55836818]\n",
      " [-0.16817135]]\n",
      "Current iteration=8, loss=30467.719093343767\n",
      "t [[-1.99202391]\n",
      " [-0.6207821 ]\n",
      " [ 0.29429816]\n",
      " ...\n",
      " [-0.49327724]\n",
      " [-0.58745817]\n",
      " [-0.17948286]]\n",
      "t [[-1.99202391]\n",
      " [-0.6207821 ]\n",
      " [ 0.29429816]\n",
      " ...\n",
      " [-0.49327724]\n",
      " [-0.58745817]\n",
      " [-0.17948286]]\n",
      "t [[-2.07390196]\n",
      " [-0.65367083]\n",
      " [ 0.30444917]\n",
      " ...\n",
      " [-0.51639041]\n",
      " [-0.61305998]\n",
      " [-0.18936888]]\n",
      "loss=30186.22458434066\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.48555324]\n",
      " [-0.1125353 ]\n",
      " [ 0.02924615]\n",
      " ...\n",
      " [-0.10341389]\n",
      " [-0.13682336]\n",
      " [-0.02767377]]\n",
      "t [[-0.48555324]\n",
      " [-0.1125353 ]\n",
      " [ 0.02924615]\n",
      " ...\n",
      " [-0.10341389]\n",
      " [-0.13682336]\n",
      " [-0.02767377]]\n",
      "t [[-0.84497077]\n",
      " [-0.21790291]\n",
      " [ 0.07723758]\n",
      " ...\n",
      " [-0.1872709 ]\n",
      " [-0.2413676 ]\n",
      " [-0.0555588 ]]\n",
      "t [[-0.84497077]\n",
      " [-0.21790291]\n",
      " [ 0.07723758]\n",
      " ...\n",
      " [-0.1872709 ]\n",
      " [-0.2413676 ]\n",
      " [-0.0555588 ]]\n",
      "Current iteration=2, loss=33179.76811685274\n",
      "t [[-1.12135036]\n",
      " [-0.30794876]\n",
      " [ 0.12387416]\n",
      " ...\n",
      " [-0.25617084]\n",
      " [-0.32334243]\n",
      " [-0.08070352]]\n",
      "t [[-1.12135036]\n",
      " [-0.30794876]\n",
      " [ 0.12387416]\n",
      " ...\n",
      " [-0.25617084]\n",
      " [-0.32334243]\n",
      " [-0.08070352]]\n",
      "t [[-1.34100211]\n",
      " [-0.38399909]\n",
      " [ 0.16377653]\n",
      " ...\n",
      " [-0.31355823]\n",
      " [-0.38938009]\n",
      " [-0.10284212]]\n",
      "t [[-1.34100211]\n",
      " [-0.38399909]\n",
      " [ 0.16377653]\n",
      " ...\n",
      " [-0.31355823]\n",
      " [-0.38938009]\n",
      " [-0.10284212]]\n",
      "Current iteration=4, loss=31566.5841774523\n",
      "t [[-1.52014533]\n",
      " [-0.44875178]\n",
      " [ 0.19647707]\n",
      " ...\n",
      " [-0.36195579]\n",
      " [-0.44381147]\n",
      " [-0.1222661 ]]\n",
      "t [[-1.52014533]\n",
      " [-0.44875178]\n",
      " [ 0.19647707]\n",
      " ...\n",
      " [-0.36195579]\n",
      " [-0.44381147]\n",
      " [-0.1222661 ]]\n",
      "t [[-1.6692415 ]\n",
      " [-0.50452888]\n",
      " [ 0.22289589]\n",
      " ...\n",
      " [-0.40321745]\n",
      " [-0.48951927]\n",
      " [-0.13931793]]\n",
      "t [[-1.6692415 ]\n",
      " [-0.50452888]\n",
      " [ 0.22289589]\n",
      " ...\n",
      " [-0.40321745]\n",
      " [-0.48951927]\n",
      " [-0.13931793]]\n",
      "Current iteration=6, loss=30811.26614370902\n",
      "t [[-1.79535373]\n",
      " [-0.55310751]\n",
      " [ 0.24420818]\n",
      " ...\n",
      " [-0.43872315]\n",
      " [-0.52848756]\n",
      " [-0.15430004]]\n",
      "t [[-1.79535373]\n",
      " [-0.55310751]\n",
      " [ 0.24420818]\n",
      " ...\n",
      " [-0.43872315]\n",
      " [-0.52848756]\n",
      " [-0.15430004]]\n",
      "t [[-1.90344392]\n",
      " [-0.59582647]\n",
      " [ 0.26149721]\n",
      " ...\n",
      " [-0.46951718]\n",
      " [-0.56212826]\n",
      " [-0.16746863]]\n",
      "t [[-1.90344392]\n",
      " [-0.59582647]\n",
      " [ 0.26149721]\n",
      " ...\n",
      " [-0.46951718]\n",
      " [-0.56212826]\n",
      " [-0.16746863]]\n",
      "Current iteration=8, loss=30379.655209798366\n",
      "t [[-1.9971159 ]\n",
      " [-0.63370412]\n",
      " [ 0.27566684]\n",
      " ...\n",
      " [-0.49640349]\n",
      " [-0.59147633]\n",
      " [-0.17904148]]\n",
      "t [[-1.9971159 ]\n",
      " [-0.63370412]\n",
      " [ 0.27566684]\n",
      " ...\n",
      " [-0.49640349]\n",
      " [-0.59147633]\n",
      " [-0.17904148]]\n",
      "t [[-2.0790607 ]\n",
      " [-0.6675276 ]\n",
      " [ 0.2874395 ]\n",
      " ...\n",
      " [-0.52001128]\n",
      " [-0.61730984]\n",
      " [-0.18920581]]\n",
      "loss=30097.27705202274\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.48172343]\n",
      " [-0.10477681]\n",
      " [ 0.0342941 ]\n",
      " ...\n",
      " [ 0.53992312]\n",
      " [-0.39754764]\n",
      " [ 0.43794046]]\n",
      "t [[-0.48172343]\n",
      " [-0.10477681]\n",
      " [ 0.0342941 ]\n",
      " ...\n",
      " [ 0.53992312]\n",
      " [-0.39754764]\n",
      " [ 0.43794046]]\n",
      "t [[-0.8388942 ]\n",
      " [-0.20577056]\n",
      " [ 0.08514075]\n",
      " ...\n",
      " [ 0.91738567]\n",
      " [-0.68317341]\n",
      " [ 0.75432116]]\n",
      "t [[-0.8388942 ]\n",
      " [-0.20577056]\n",
      " [ 0.08514075]\n",
      " ...\n",
      " [ 0.91738567]\n",
      " [-0.68317341]\n",
      " [ 0.75432116]]\n",
      "Current iteration=2, loss=33327.51489122084\n",
      "t [[-1.11420322]\n",
      " [-0.2924768 ]\n",
      " [ 0.13307862]\n",
      " ...\n",
      " [ 1.1863687 ]\n",
      " [-0.89147032]\n",
      " [ 0.98806114]]\n",
      "t [[-1.11420322]\n",
      " [-0.2924768 ]\n",
      " [ 0.13307862]\n",
      " ...\n",
      " [ 1.1863687 ]\n",
      " [-0.89147032]\n",
      " [ 0.98806114]]\n",
      "t [[-1.33345979]\n",
      " [-0.36564583]\n",
      " [ 0.1733523 ]\n",
      " ...\n",
      " [ 1.38259769]\n",
      " [-1.04699654]\n",
      " [ 1.16544193]]\n",
      "t [[-1.33345979]\n",
      " [-0.36564583]\n",
      " [ 0.1733523 ]\n",
      " ...\n",
      " [ 1.38259769]\n",
      " [-1.04699654]\n",
      " [ 1.16544193]]\n",
      "Current iteration=4, loss=31754.1934577907\n",
      "t [[-1.51258018]\n",
      " [-0.42780955]\n",
      " [ 0.20585826]\n",
      " ...\n",
      " [ 1.52859728]\n",
      " [-1.16569001]\n",
      " [ 1.30327331]]\n",
      "t [[-1.51258018]\n",
      " [-0.42780955]\n",
      " [ 0.20585826]\n",
      " ...\n",
      " [ 1.52859728]\n",
      " [-1.16569001]\n",
      " [ 1.30327331]]\n",
      "t [[-1.66185959]\n",
      " [-0.48122325]\n",
      " [ 0.23172097]\n",
      " ...\n",
      " [ 1.63884673]\n",
      " [-1.25793847]\n",
      " [ 1.41248684]]\n",
      "t [[-1.66185959]\n",
      " [-0.48122325]\n",
      " [ 0.23172097]\n",
      " ...\n",
      " [ 1.63884673]\n",
      " [-1.25793847]\n",
      " [ 1.41248684]]\n",
      "Current iteration=6, loss=31014.22092151612\n",
      "t [[-1.78826926]\n",
      " [-0.52762684]\n",
      " [ 0.25223655]\n",
      " ...\n",
      " [ 1.72298085]\n",
      " [-1.33071137]\n",
      " [ 1.50043113]]\n",
      "t [[-1.78826926]\n",
      " [-0.52762684]\n",
      " [ 0.25223655]\n",
      " ...\n",
      " [ 1.72298085]\n",
      " [-1.33071137]\n",
      " [ 1.50043113]]\n",
      "t [[-1.8967188 ]\n",
      " [-0.56833363]\n",
      " [ 0.26856475]\n",
      " ...\n",
      " [ 1.78763364]\n",
      " [-1.3888371 ]\n",
      " [ 1.57221434]]\n",
      "t [[-1.8967188 ]\n",
      " [-0.56833363]\n",
      " [ 0.26856475]\n",
      " ...\n",
      " [ 1.78763364]\n",
      " [-1.3888371 ]\n",
      " [ 1.57221434]]\n",
      "Current iteration=8, loss=30590.268761606614\n",
      "t [[-1.99078123]\n",
      " [-0.6043427 ]\n",
      " [ 0.28166066]\n",
      " ...\n",
      " [ 1.83751078]\n",
      " [-1.43575911]\n",
      " [ 1.63149375]]\n",
      "t [[-1.99078123]\n",
      " [-0.6043427 ]\n",
      " [ 0.28166066]\n",
      " ...\n",
      " [ 1.83751078]\n",
      " [-1.43575911]\n",
      " [ 1.63149375]]\n",
      "t [[-2.07312883]\n",
      " [-0.63642572]\n",
      " [ 0.29228256]\n",
      " ...\n",
      " [ 1.87603408]\n",
      " [-1.47399432]\n",
      " [ 1.68095395]]\n",
      "loss=30312.469093405467\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.4215513 ]\n",
      " [-0.08868101]\n",
      " [ 0.08801619]\n",
      " ...\n",
      " [-0.11125642]\n",
      " [-0.13832793]\n",
      " [-0.02907964]]\n",
      "t [[-0.4215513 ]\n",
      " [-0.08868101]\n",
      " [ 0.08801619]\n",
      " ...\n",
      " [-0.11125642]\n",
      " [-0.13832793]\n",
      " [-0.02907964]]\n",
      "t [[-0.71308043]\n",
      " [-0.1207309 ]\n",
      " [ 0.1598925 ]\n",
      " ...\n",
      " [-0.20010858]\n",
      " [-0.24340537]\n",
      " [-0.0576502 ]]\n",
      "t [[-0.71308043]\n",
      " [-0.1207309 ]\n",
      " [ 0.1598925 ]\n",
      " ...\n",
      " [-0.20010858]\n",
      " [-0.24340537]\n",
      " [-0.0576502 ]]\n",
      "Current iteration=2, loss=33208.59095042895\n",
      "t [[-0.91285266]\n",
      " [-0.13566862]\n",
      " [ 0.21192899]\n",
      " ...\n",
      " [-0.27213864]\n",
      " [-0.32539637]\n",
      " [-0.08298748]]\n",
      "t [[-0.91285266]\n",
      " [-0.13566862]\n",
      " [ 0.21192899]\n",
      " ...\n",
      " [-0.27213864]\n",
      " [-0.32539637]\n",
      " [-0.08298748]]\n",
      "t [[-1.05070835]\n",
      " [-0.14654679]\n",
      " [ 0.24890272]\n",
      " ...\n",
      " [-0.33143194]\n",
      " [-0.39117754]\n",
      " [-0.10505355]]\n",
      "t [[-1.05070835]\n",
      " [-0.14654679]\n",
      " [ 0.24890272]\n",
      " ...\n",
      " [-0.33143194]\n",
      " [-0.39117754]\n",
      " [-0.10505355]]\n",
      "Current iteration=4, loss=31633.944651835103\n",
      "t [[-1.1461993 ]\n",
      " [-0.15768231]\n",
      " [ 0.27529839]\n",
      " ...\n",
      " [-0.38091401]\n",
      " [-0.44520455]\n",
      " [-0.12426271]]\n",
      "t [[-1.1461993 ]\n",
      " [-0.15768231]\n",
      " [ 0.27529839]\n",
      " ...\n",
      " [-0.38091401]\n",
      " [-0.44520455]\n",
      " [-0.12426271]]\n",
      "t [[-1.21193704]\n",
      " [-0.17044859]\n",
      " [ 0.29426019]\n",
      " ...\n",
      " [-0.42269553]\n",
      " [-0.49042668]\n",
      " [-0.14102064]]\n",
      "t [[-1.21193704]\n",
      " [-0.17044859]\n",
      " [ 0.29426019]\n",
      " ...\n",
      " [-0.42269553]\n",
      " [-0.49042668]\n",
      " [-0.14102064]]\n",
      "Current iteration=6, loss=30904.610471320884\n",
      "t [[-1.25626364]\n",
      " [-0.18514786]\n",
      " [ 0.30790557]\n",
      " ...\n",
      " [-0.45832521]\n",
      " [-0.52886562]\n",
      " [-0.15566434]]\n",
      "t [[-1.25626364]\n",
      " [-0.18514786]\n",
      " [ 0.30790557]\n",
      " ...\n",
      " [-0.45832521]\n",
      " [-0.52886562]\n",
      " [-0.15566434]]\n",
      "t [[-1.28487743]\n",
      " [-0.20167943]\n",
      " [ 0.3176777 ]\n",
      " ...\n",
      " [-0.48896196]\n",
      " [-0.56195566]\n",
      " [-0.16847045]]\n",
      "t [[-1.28487743]\n",
      " [-0.20167943]\n",
      " [ 0.3176777 ]\n",
      " ...\n",
      " [-0.48896196]\n",
      " [-0.56195566]\n",
      " [-0.16847045]]\n",
      "Current iteration=8, loss=30490.685396055935\n",
      "t [[-1.3017885 ]\n",
      " [-0.21980046]\n",
      " [ 0.32458356]\n",
      " ...\n",
      " [-0.51549004]\n",
      " [-0.59074555]\n",
      " [-0.17966957]]\n",
      "t [[-1.3017885 ]\n",
      " [-0.21980046]\n",
      " [ 0.32458356]\n",
      " ...\n",
      " [-0.51549004]\n",
      " [-0.59074555]\n",
      " [-0.17966957]]\n",
      "t [[-1.30989096]\n",
      " [-0.23923312]\n",
      " [ 0.32934257]\n",
      " ...\n",
      " [-0.53859642]\n",
      " [-0.61602207]\n",
      " [-0.18945729]]\n",
      "loss=30221.07106365891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.49507709]\n",
      " [-0.11153458]\n",
      " [ 0.0435669 ]\n",
      " ...\n",
      " [-0.10694529]\n",
      " [-0.13904716]\n",
      " [-0.02886124]]\n",
      "t [[-0.49507709]\n",
      " [-0.11153458]\n",
      " [ 0.0435669 ]\n",
      " ...\n",
      " [-0.10694529]\n",
      " [-0.13904716]\n",
      " [-0.02886124]]\n",
      "t [[-0.85861107]\n",
      " [-0.21726927]\n",
      " [ 0.09977582]\n",
      " ...\n",
      " [-0.19223129]\n",
      " [-0.24456399]\n",
      " [-0.05786322]]\n",
      "t [[-0.85861107]\n",
      " [-0.21726927]\n",
      " [ 0.09977582]\n",
      " ...\n",
      " [-0.19223129]\n",
      " [-0.24456399]\n",
      " [-0.05786322]]\n",
      "Current iteration=2, loss=33176.07578922541\n",
      "t [[-1.13672204]\n",
      " [-0.30725143]\n",
      " [ 0.1502086 ]\n",
      " ...\n",
      " [-0.26150053]\n",
      " [-0.32684556]\n",
      " [-0.08369139]]\n",
      "t [[-1.13672204]\n",
      " [-0.30725143]\n",
      " [ 0.1502086 ]\n",
      " ...\n",
      " [-0.26150053]\n",
      " [-0.32684556]\n",
      " [-0.08369139]]\n",
      "t [[-1.35693758]\n",
      " [-0.38284546]\n",
      " [ 0.19132291]\n",
      " ...\n",
      " [-0.31871741]\n",
      " [-0.39284501]\n",
      " [-0.10617808]]\n",
      "t [[-1.35693758]\n",
      " [-0.38284546]\n",
      " [ 0.19132291]\n",
      " ...\n",
      " [-0.31871741]\n",
      " [-0.39284501]\n",
      " [-0.10617808]]\n",
      "Current iteration=4, loss=31593.659678735225\n",
      "t [[-1.53603444]\n",
      " [-0.44690959]\n",
      " [ 0.22378607]\n",
      " ...\n",
      " [-0.36666905]\n",
      " [-0.44705784]\n",
      " [-0.12572084]]\n",
      "t [[-1.53603444]\n",
      " [-0.44690959]\n",
      " [ 0.22378607]\n",
      " ...\n",
      " [-0.36666905]\n",
      " [-0.44705784]\n",
      " [-0.12572084]]\n",
      "t [[-1.68476082]\n",
      " [-0.50186325]\n",
      " [ 0.24915832]\n",
      " ...\n",
      " [-0.40734883]\n",
      " [-0.4924538 ]\n",
      " [-0.14273249]]\n",
      "t [[-1.68476082]\n",
      " [-0.50186325]\n",
      " [ 0.24915832]\n",
      " ...\n",
      " [-0.40734883]\n",
      " [-0.4924538 ]\n",
      " [-0.14273249]]\n",
      "Current iteration=6, loss=30857.38672144372\n",
      "t [[-1.810339  ]\n",
      " [-0.54953631]\n",
      " [ 0.26897645]\n",
      " ...\n",
      " [-0.44221277]\n",
      " [-0.53106519]\n",
      " [-0.15756073]]\n",
      "t [[-1.810339  ]\n",
      " [-0.54953631]\n",
      " [ 0.26897645]\n",
      " ...\n",
      " [-0.44221277]\n",
      " [-0.53106519]\n",
      " [-0.15756073]]\n",
      "t [[-1.91782365]\n",
      " [-0.59129714]\n",
      " [ 0.28453228]\n",
      " ...\n",
      " [-0.47234775]\n",
      " [-0.56433171]\n",
      " [-0.17049212]]\n",
      "t [[-1.91782365]\n",
      " [-0.59129714]\n",
      " [ 0.28453228]\n",
      " ...\n",
      " [-0.47234775]\n",
      " [-0.56433171]\n",
      " [-0.17049212]]\n",
      "Current iteration=8, loss=30436.08733090327\n",
      "t [[-2.01087412]\n",
      " [-0.62818209]\n",
      " [ 0.29685229]\n",
      " ...\n",
      " [-0.49858198]\n",
      " [-0.59330468]\n",
      " [-0.18176573]]\n",
      "t [[-2.01087412]\n",
      " [-0.62818209]\n",
      " [ 0.29685229]\n",
      " ...\n",
      " [-0.49858198]\n",
      " [-0.59330468]\n",
      " [-0.18176573]]\n",
      "t [[-2.09221492]\n",
      " [-0.66099015]\n",
      " [ 0.30673201]\n",
      " ...\n",
      " [-0.52155849]\n",
      " [-0.61877188]\n",
      " [-0.19158432]]\n",
      "loss=30159.16612148856\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.49634331]\n",
      " [-0.11503608]\n",
      " [ 0.02989606]\n",
      " ...\n",
      " [-0.10571198]\n",
      " [-0.13986388]\n",
      " [-0.02828874]]\n",
      "t [[-0.49634331]\n",
      " [-0.11503608]\n",
      " [ 0.02989606]\n",
      " ...\n",
      " [-0.10571198]\n",
      " [-0.13986388]\n",
      " [-0.02828874]]\n",
      "t [[-0.86096501]\n",
      " [-0.22257665]\n",
      " [ 0.0793633 ]\n",
      " ...\n",
      " [-0.19100111]\n",
      " [-0.2460165 ]\n",
      " [-0.05679829]]\n",
      "t [[-0.86096501]\n",
      " [-0.22257665]\n",
      " [ 0.0793633 ]\n",
      " ...\n",
      " [-0.19100111]\n",
      " [-0.2460165 ]\n",
      " [-0.05679829]]\n",
      "Current iteration=2, loss=33117.29398966185\n",
      "t [[-1.13987293]\n",
      " [-0.31394326]\n",
      " [ 0.12697257]\n",
      " ...\n",
      " [-0.26078129]\n",
      " [-0.32882651]\n",
      " [-0.08238379]]\n",
      "t [[-1.13987293]\n",
      " [-0.31394326]\n",
      " [ 0.12697257]\n",
      " ...\n",
      " [-0.26078129]\n",
      " [-0.32882651]\n",
      " [-0.08238379]]\n",
      "t [[-1.36068454]\n",
      " [-0.39076258]\n",
      " [ 0.16732491]\n",
      " ...\n",
      " [-0.31869007]\n",
      " [-0.39528341]\n",
      " [-0.10482046]]\n",
      "t [[-1.36068454]\n",
      " [-0.39076258]\n",
      " [ 0.16732491]\n",
      " ...\n",
      " [-0.31869007]\n",
      " [-0.39528341]\n",
      " [-0.10482046]]\n",
      "Current iteration=4, loss=31515.798948387404\n",
      "t [[-1.54023877]\n",
      " [-0.45596071]\n",
      " [ 0.20012584]\n",
      " ...\n",
      " [-0.36737313]\n",
      " [-0.44990019]\n",
      " [-0.12444034]]\n",
      "t [[-1.54023877]\n",
      " [-0.45596071]\n",
      " [ 0.20012584]\n",
      " ...\n",
      " [-0.36737313]\n",
      " [-0.44990019]\n",
      " [-0.12444034]]\n",
      "t [[-1.68931666]\n",
      " [-0.51198611]\n",
      " [ 0.22644309]\n",
      " ...\n",
      " [-0.40876302]\n",
      " [-0.49565614]\n",
      " [-0.14161109]]\n",
      "t [[-1.68931666]\n",
      " [-0.51198611]\n",
      " [ 0.22644309]\n",
      " ...\n",
      " [-0.40876302]\n",
      " [-0.49565614]\n",
      " [-0.14161109]]\n",
      "Current iteration=6, loss=30772.026116789948\n",
      "t [[-1.81515835]\n",
      " [-0.56068642]\n",
      " [ 0.2475524 ]\n",
      " ...\n",
      " [-0.44429058]\n",
      " [-0.53458965]\n",
      " [-0.15665179]]\n",
      "t [[-1.81515835]\n",
      " [-0.56068642]\n",
      " [ 0.2475524 ]\n",
      " ...\n",
      " [-0.44429058]\n",
      " [-0.53458965]\n",
      " [-0.15665179]]\n",
      "t [[-1.92282956]\n",
      " [-0.60344213]\n",
      " [ 0.26460043]\n",
      " ...\n",
      " [-0.47503356]\n",
      " [-0.5681448 ]\n",
      " [-0.16983096]]\n",
      "t [[-1.92282956]\n",
      " [-0.60344213]\n",
      " [ 0.26460043]\n",
      " ...\n",
      " [-0.47503356]\n",
      " [-0.5681448 ]\n",
      " [-0.16983096]]\n",
      "Current iteration=8, loss=30347.84023650767\n",
      "t [[-2.01599778]\n",
      " [-0.64129783]\n",
      " [ 0.27852895]\n",
      " ...\n",
      " [-0.50181848]\n",
      " [-0.59737636]\n",
      " [-0.18137617]]\n",
      "t [[-2.01599778]\n",
      " [-0.64129783]\n",
      " [ 0.27852895]\n",
      " ...\n",
      " [-0.50181848]\n",
      " [-0.59737636]\n",
      " [-0.18137617]]\n",
      "t [[-2.09739429]\n",
      " [-0.67505793]\n",
      " [ 0.29008157]\n",
      " ...\n",
      " [-0.52529007]\n",
      " [-0.62307507]\n",
      " [-0.19148271]]\n",
      "loss=30070.176173196738\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.49242839]\n",
      " [-0.10710518]\n",
      " [ 0.03505619]\n",
      " ...\n",
      " [ 0.55192141]\n",
      " [-0.40638203]\n",
      " [ 0.44767247]]\n",
      "t [[-0.49242839]\n",
      " [-0.10710518]\n",
      " [ 0.03505619]\n",
      " ...\n",
      " [ 0.55192141]\n",
      " [-0.40638203]\n",
      " [ 0.44767247]]\n",
      "t [[-0.85478651]\n",
      " [-0.21024934]\n",
      " [ 0.08739349]\n",
      " ...\n",
      " [ 0.93417992]\n",
      " [-0.69587666]\n",
      " [ 0.76839716]]\n",
      "t [[-0.85478651]\n",
      " [-0.21024934]\n",
      " [ 0.08739349]\n",
      " ...\n",
      " [ 0.93417992]\n",
      " [-0.69587666]\n",
      " [ 0.76839716]]\n",
      "Current iteration=2, loss=33266.83016988057\n",
      "t [[-1.13264816]\n",
      " [-0.29824603]\n",
      " [ 0.13626384]\n",
      " ...\n",
      " [ 1.20440772]\n",
      " [-0.90542344]\n",
      " [ 1.0037282 ]]\n",
      "t [[-1.13264816]\n",
      " [-0.29824603]\n",
      " [ 0.13626384]\n",
      " ...\n",
      " [ 1.20440772]\n",
      " [-0.90542344]\n",
      " [ 1.0037282 ]]\n",
      "t [[-1.35309788]\n",
      " [-0.37214912]\n",
      " [ 0.17693263]\n",
      " ...\n",
      " [ 1.40021521]\n",
      " [-1.06093444]\n",
      " [ 1.18134915]]\n",
      "t [[-1.35309788]\n",
      " [-0.37214912]\n",
      " [ 0.17693263]\n",
      " ...\n",
      " [ 1.40021521]\n",
      " [-1.06093444]\n",
      " [ 1.18134915]]\n",
      "Current iteration=4, loss=31704.568950927805\n",
      "t [[-1.53266054]\n",
      " [-0.43472575]\n",
      " [ 0.20948285]\n",
      " ...\n",
      " [ 1.54502373]\n",
      " [-1.17901188]\n",
      " [ 1.31875381]]\n",
      "t [[-1.53266054]\n",
      " [-0.43472575]\n",
      " [ 0.20948285]\n",
      " ...\n",
      " [ 1.54502373]\n",
      " [-1.17901188]\n",
      " [ 1.31875381]]\n",
      "t [[-1.68194836]\n",
      " [-0.48835971]\n",
      " [ 0.23519014]\n",
      " ...\n",
      " [ 1.65375093]\n",
      " [-1.27037168]\n",
      " [ 1.42721686]]\n",
      "t [[-1.68194836]\n",
      " [-0.48835971]\n",
      " [ 0.23519014]\n",
      " ...\n",
      " [ 1.65375093]\n",
      " [-1.27037168]\n",
      " [ 1.42721686]]\n",
      "Current iteration=6, loss=30975.745408900475\n",
      "t [[-1.80810964]\n",
      " [-0.53486145]\n",
      " [ 0.25545202]\n",
      " ...\n",
      " [ 1.73625555]\n",
      " [-1.34215242]\n",
      " [ 1.51426738]]\n",
      "t [[-1.80810964]\n",
      " [-0.53486145]\n",
      " [ 0.25545202]\n",
      " ...\n",
      " [ 1.73625555]\n",
      " [-1.34215242]\n",
      " [ 1.51426738]]\n",
      "t [[-1.91615843]\n",
      " [-0.57558555]\n",
      " [ 0.2714918 ]\n",
      " ...\n",
      " [ 1.79928949]\n",
      " [-1.39927291]\n",
      " [ 1.58511219]]\n",
      "t [[-1.91615843]\n",
      " [-0.57558555]\n",
      " [ 0.2714918 ]\n",
      " ...\n",
      " [ 1.79928949]\n",
      " [-1.39927291]\n",
      " [ 1.58511219]]\n",
      "Current iteration=8, loss=30559.00575531905\n",
      "t [[-2.00973202]\n",
      " [-0.61155674]\n",
      " [ 0.28430244]\n",
      " ...\n",
      " [ 1.84762078]\n",
      " [-1.44522565]\n",
      " [ 1.64346356]]\n",
      "t [[-2.00973202]\n",
      " [-0.61155674]\n",
      " [ 0.28430244]\n",
      " ...\n",
      " [ 1.84762078]\n",
      " [-1.44522565]\n",
      " [ 1.64346356]]\n",
      "t [[-2.09154351]\n",
      " [-0.64356356]\n",
      " [ 0.29466332]\n",
      " ...\n",
      " [ 1.88470293]\n",
      " [-1.48255364]\n",
      " [ 1.69203653]]\n",
      "loss=30285.804152302146\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.43071546]\n",
      " [-0.09060886]\n",
      " [ 0.08992959]\n",
      " ...\n",
      " [-0.11367503]\n",
      " [-0.14133506]\n",
      " [-0.0297118 ]]\n",
      "t [[-0.43071546]\n",
      " [-0.09060886]\n",
      " [ 0.08992959]\n",
      " ...\n",
      " [-0.11367503]\n",
      " [-0.14133506]\n",
      " [-0.0297118 ]]\n",
      "t [[-0.72576431]\n",
      " [-0.12213238]\n",
      " [ 0.16301319]\n",
      " ...\n",
      " [-0.20397641]\n",
      " [-0.24797718]\n",
      " [-0.05889279]]\n",
      "t [[-0.72576431]\n",
      " [-0.12213238]\n",
      " [ 0.16301319]\n",
      " ...\n",
      " [-0.20397641]\n",
      " [-0.24797718]\n",
      " [-0.05889279]]\n",
      "Current iteration=2, loss=33148.4126490269\n",
      "t [[-0.92595596]\n",
      " [-0.13665603]\n",
      " [ 0.2153303 ]\n",
      " ...\n",
      " [-0.27685771]\n",
      " [-0.33076418]\n",
      " [-0.084645  ]]\n",
      "t [[-0.92595596]\n",
      " [-0.13665603]\n",
      " [ 0.2153303 ]\n",
      " ...\n",
      " [-0.27685771]\n",
      " [-0.33076418]\n",
      " [-0.084645  ]]\n",
      "t [[-1.06282255]\n",
      " [-0.14748697]\n",
      " [ 0.2521427 ]\n",
      " ...\n",
      " [-0.33662485]\n",
      " [-0.39693258]\n",
      " [-0.10698448]]\n",
      "t [[-1.06282255]\n",
      " [-0.14748697]\n",
      " [ 0.2521427 ]\n",
      " ...\n",
      " [-0.33662485]\n",
      " [-0.39693258]\n",
      " [-0.10698448]]\n",
      "Current iteration=4, loss=31585.69070191385\n",
      "t [[-1.15672516]\n",
      " [-0.15886456]\n",
      " [ 0.27820211]\n",
      " ...\n",
      " [-0.38633921]\n",
      " [-0.45111931]\n",
      " [-0.12636849]]\n",
      "t [[-1.15672516]\n",
      " [-0.15886456]\n",
      " [ 0.27820211]\n",
      " ...\n",
      " [-0.38633921]\n",
      " [-0.45111931]\n",
      " [-0.12636849]]\n",
      "t [[-1.22065358]\n",
      " [-0.17206895]\n",
      " [ 0.29677042]\n",
      " ...\n",
      " [-0.42819604]\n",
      " [-0.49636894]\n",
      " [-0.14322773]]\n",
      "t [[-1.22065358]\n",
      " [-0.17206895]\n",
      " [ 0.29677042]\n",
      " ...\n",
      " [-0.42819604]\n",
      " [-0.49636894]\n",
      " [-0.14322773]]\n",
      "Current iteration=6, loss=30867.64899607432\n",
      "t [[-1.26313357]\n",
      " [-0.18733034]\n",
      " [ 0.31001669]\n",
      " ...\n",
      " [-0.46379755]\n",
      " [-0.53475637]\n",
      " [-0.15791541]]\n",
      "t [[-1.26313357]\n",
      " [-0.18733034]\n",
      " [ 0.31001669]\n",
      " ...\n",
      " [-0.46379755]\n",
      " [-0.53475637]\n",
      " [-0.15791541]]\n",
      "t [[-1.28995556]\n",
      " [-0.20449644]\n",
      " [ 0.3194073 ]\n",
      " ...\n",
      " [-0.49433733]\n",
      " [-0.56774723]\n",
      " [-0.17072009]]\n",
      "t [[-1.28995556]\n",
      " [-0.20449644]\n",
      " [ 0.3194073 ]\n",
      " ...\n",
      " [-0.49433733]\n",
      " [-0.56774723]\n",
      " [-0.17072009]]\n",
      "Current iteration=8, loss=30460.887747357618\n",
      "t [[-1.30517566]\n",
      " [-0.22328822]\n",
      " [ 0.32595948]\n",
      " ...\n",
      " [-0.52072263]\n",
      " [-0.59640948]\n",
      " [-0.18188184]]\n",
      "t [[-1.30517566]\n",
      " [-0.22328822]\n",
      " [ 0.32595948]\n",
      " ...\n",
      " [-0.52072263]\n",
      " [-0.59640948]\n",
      " [-0.18188184]]\n",
      "t [[-1.31170948]\n",
      " [-0.2434028 ]\n",
      " [ 0.33039667]\n",
      " ...\n",
      " [-0.54365603]\n",
      " [-0.62154207]\n",
      " [-0.19160406]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=30195.782378610827\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.50583964]\n",
      " [-0.11395925]\n",
      " [ 0.04451401]\n",
      " ...\n",
      " [-0.10927018]\n",
      " [-0.14206992]\n",
      " [-0.02948866]]\n",
      "t [[-0.50583964]\n",
      " [-0.11395925]\n",
      " [ 0.04451401]\n",
      " ...\n",
      " [-0.10927018]\n",
      " [-0.14206992]\n",
      " [-0.02948866]]\n",
      "t [[-0.8744409 ]\n",
      " [-0.22185725]\n",
      " [ 0.10221366]\n",
      " ...\n",
      " [-0.19594371]\n",
      " [-0.24915507]\n",
      " [-0.05912442]]\n",
      "t [[-0.8744409 ]\n",
      " [-0.22185725]\n",
      " [ 0.10221366]\n",
      " ...\n",
      " [-0.19594371]\n",
      " [-0.24915507]\n",
      " [-0.05912442]]\n",
      "Current iteration=2, loss=33115.60320010645\n",
      "t [[-1.15496409]\n",
      " [-0.31311298]\n",
      " [ 0.15349441]\n",
      " ...\n",
      " [-0.26603811]\n",
      " [-0.33223269]\n",
      " [-0.08538069]]\n",
      "t [[-1.15496409]\n",
      " [-0.31311298]\n",
      " [ 0.15349441]\n",
      " ...\n",
      " [-0.26603811]\n",
      " [-0.33223269]\n",
      " [-0.08538069]]\n",
      "t [[-1.37625192]\n",
      " [-0.38942509]\n",
      " [ 0.19491082]\n",
      " ...\n",
      " [-0.32372721]\n",
      " [-0.39861943]\n",
      " [-0.10814553]]\n",
      "t [[-1.37625192]\n",
      " [-0.38942509]\n",
      " [ 0.19491082]\n",
      " ...\n",
      " [-0.32372721]\n",
      " [-0.39861943]\n",
      " [-0.10814553]]\n",
      "Current iteration=4, loss=31545.110341641805\n",
      "t [[-1.55569628]\n",
      " [-0.45388996]\n",
      " [ 0.22734168]\n",
      " ...\n",
      " [-0.37192463]\n",
      " [-0.45299309]\n",
      " [-0.12786301]]\n",
      "t [[-1.55569628]\n",
      " [-0.45388996]\n",
      " [ 0.22734168]\n",
      " ...\n",
      " [-0.37192463]\n",
      " [-0.45299309]\n",
      " [-0.12786301]]\n",
      "t [[-1.70436023]\n",
      " [-0.5090535 ]\n",
      " [ 0.25250258]\n",
      " ...\n",
      " [-0.41270208]\n",
      " [-0.49841888]\n",
      " [-0.14497295]]\n",
      "t [[-1.70436023]\n",
      " [-0.5090535 ]\n",
      " [ 0.25250258]\n",
      " ...\n",
      " [-0.41270208]\n",
      " [-0.49841888]\n",
      " [-0.14497295]]\n",
      "Current iteration=6, loss=30819.958160153532\n",
      "t [[-1.82963922]\n",
      " [-0.5568145 ]\n",
      " [ 0.27202929]\n",
      " ...\n",
      " [-0.44756508]\n",
      " [-0.5369822 ]\n",
      " [-0.15984027]]\n",
      "t [[-1.82963922]\n",
      " [-0.5568145 ]\n",
      " [ 0.27202929]\n",
      " ...\n",
      " [-0.44756508]\n",
      " [-0.5369822 ]\n",
      " [-0.15984027]]\n",
      "t [[-1.93668861]\n",
      " [-0.59858178]\n",
      " [ 0.28727329]\n",
      " ...\n",
      " [-0.47763261]\n",
      " [-0.5701539 ]\n",
      " [-0.17276401]]\n",
      "t [[-1.93668861]\n",
      " [-0.59858178]\n",
      " [ 0.28727329]\n",
      " ...\n",
      " [-0.47763261]\n",
      " [-0.5701539 ]\n",
      " [-0.17276401]]\n",
      "Current iteration=8, loss=30405.644715729868\n",
      "t [[-2.02922893]\n",
      " [-0.63541707]\n",
      " [ 0.29929529]\n",
      " ...\n",
      " [-0.50375421]\n",
      " [-0.59900431]\n",
      " [-0.18399309]]\n",
      "t [[-2.02922893]\n",
      " [-0.63541707]\n",
      " [ 0.29929529]\n",
      " ...\n",
      " [-0.50375421]\n",
      " [-0.59900431]\n",
      " [-0.18399309]]\n",
      "t [[-2.11002291]\n",
      " [-0.66813613]\n",
      " [ 0.3089088 ]\n",
      " ...\n",
      " [-0.52658744]\n",
      " [-0.62433319]\n",
      " [-0.19373836]]\n",
      "loss=30133.098775605955\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.50713338]\n",
      " [-0.11753686]\n",
      " [ 0.03054597]\n",
      " ...\n",
      " [-0.10801006]\n",
      " [-0.1429044 ]\n",
      " [-0.02890372]]\n",
      "t [[-0.50713338]\n",
      " [-0.11753686]\n",
      " [ 0.03054597]\n",
      " ...\n",
      " [-0.10801006]\n",
      " [-0.1429044 ]\n",
      " [-0.02890372]]\n",
      "t [[-0.87684361]\n",
      " [-0.22724271]\n",
      " [ 0.08150576]\n",
      " ...\n",
      " [-0.19471339]\n",
      " [-0.25063552]\n",
      " [-0.058038  ]]\n",
      "t [[-0.87684361]\n",
      " [-0.22724271]\n",
      " [ 0.08150576]\n",
      " ...\n",
      " [-0.19471339]\n",
      " [-0.25063552]\n",
      " [-0.058038  ]]\n",
      "Current iteration=2, loss=33056.052224429586\n",
      "t [[-1.15817086]\n",
      " [-0.31989478]\n",
      " [ 0.13006699]\n",
      " ...\n",
      " [-0.26535131]\n",
      " [-0.33424898]\n",
      " [-0.0840569 ]]\n",
      "t [[-1.15817086]\n",
      " [-0.31989478]\n",
      " [ 0.13006699]\n",
      " ...\n",
      " [-0.26535131]\n",
      " [-0.33424898]\n",
      " [-0.0840569 ]]\n",
      "t [[-1.38005631]\n",
      " [-0.39744891]\n",
      " [ 0.17083735]\n",
      " ...\n",
      " [-0.3237591 ]\n",
      " [-0.40109896]\n",
      " [-0.10678292]]\n",
      "t [[-1.38005631]\n",
      " [-0.39744891]\n",
      " [ 0.17083735]\n",
      " ...\n",
      " [-0.3237591 ]\n",
      " [-0.40109896]\n",
      " [-0.10678292]]\n",
      "Current iteration=4, loss=31466.680460399595\n",
      "t [[-1.55995681]\n",
      " [-0.46306522]\n",
      " [ 0.20370957]\n",
      " ...\n",
      " [-0.37270759]\n",
      " [-0.45588101]\n",
      " [-0.12659015]]\n",
      "t [[-1.55995681]\n",
      " [-0.46306522]\n",
      " [ 0.20370957]\n",
      " ...\n",
      " [-0.37270759]\n",
      " [-0.45588101]\n",
      " [-0.12659015]]\n",
      "t [[-1.70896876]\n",
      " [-0.51931777]\n",
      " [ 0.22990372]\n",
      " ...\n",
      " [-0.41420871]\n",
      " [-0.50167004]\n",
      " [-0.14387159]]\n",
      "t [[-1.70896876]\n",
      " [-0.51931777]\n",
      " [ 0.22990372]\n",
      " ...\n",
      " [-0.41420871]\n",
      " [-0.50167004]\n",
      " [-0.14387159]]\n",
      "Current iteration=6, loss=30734.251724601498\n",
      "t [[-1.8345055 ]\n",
      " [-0.56812318]\n",
      " [ 0.25079669]\n",
      " ...\n",
      " [-0.44974408]\n",
      " [-0.54055764]\n",
      " [-0.15896302]]\n",
      "t [[-1.8345055 ]\n",
      " [-0.56812318]\n",
      " [ 0.25079669]\n",
      " ...\n",
      " [-0.44974408]\n",
      " [-0.54055764]\n",
      " [-0.15896302]]\n",
      "t [[-1.94173359]\n",
      " [-0.61090253]\n",
      " [ 0.26759762]\n",
      " ...\n",
      " [-0.48042468]\n",
      " [-0.57401915]\n",
      " [-0.1721454 ]]\n",
      "t [[-1.94173359]\n",
      " [-0.61090253]\n",
      " [ 0.26759762]\n",
      " ...\n",
      " [-0.48042468]\n",
      " [-0.57401915]\n",
      " [-0.1721454 ]]\n",
      "Current iteration=8, loss=30317.232589303298\n",
      "t [[-2.03438187]\n",
      " [-0.64872584]\n",
      " [ 0.28128465]\n",
      " ...\n",
      " [-0.5070992 ]\n",
      " [-0.60312852]\n",
      " [-0.18365623]]\n",
      "t [[-2.03438187]\n",
      " [-0.64872584]\n",
      " [ 0.28128465]\n",
      " ...\n",
      " [-0.5070992 ]\n",
      " [-0.60312852]\n",
      " [-0.18365623]]\n",
      "t [[-2.11522017]\n",
      " [-0.68241423]\n",
      " [ 0.29262101]\n",
      " ...\n",
      " [-0.5304275 ]\n",
      " [-0.62868856]\n",
      " [-0.19369898]]\n",
      "loss=30044.080801707103\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.50313336]\n",
      " [-0.10943355]\n",
      " [ 0.03581828]\n",
      " ...\n",
      " [ 0.5639197 ]\n",
      " [-0.41521642]\n",
      " [ 0.45740448]]\n",
      "t [[-0.50313336]\n",
      " [-0.10943355]\n",
      " [ 0.03581828]\n",
      " ...\n",
      " [ 0.5639197 ]\n",
      " [-0.41521642]\n",
      " [ 0.45740448]]\n",
      "t [[-0.87056443]\n",
      " [-0.21472355]\n",
      " [ 0.08966093]\n",
      " ...\n",
      " [ 0.95082439]\n",
      " [-0.70847633]\n",
      " [ 0.7823612 ]]\n",
      "t [[-0.87056443]\n",
      " [-0.21472355]\n",
      " [ 0.08966093]\n",
      " ...\n",
      " [ 0.95082439]\n",
      " [-0.70847633]\n",
      " [ 0.7823612 ]]\n",
      "Current iteration=2, loss=33207.332677024955\n",
      "t [[-1.15087121]\n",
      " [-0.30397494]\n",
      " [ 0.13944068]\n",
      " ...\n",
      " [ 1.22215173]\n",
      " [-0.91916561]\n",
      " [ 1.01917085]]\n",
      "t [[-1.15087121]\n",
      " [-0.30397494]\n",
      " [ 0.13944068]\n",
      " ...\n",
      " [ 1.22215173]\n",
      " [-0.91916561]\n",
      " [ 1.01917085]]\n",
      "t [[-1.37242861]\n",
      " [-0.3785776 ]\n",
      " [ 0.18047157]\n",
      " ...\n",
      " [ 1.41743265]\n",
      " [-1.07458164]\n",
      " [ 1.19694695]]\n",
      "t [[-1.37242861]\n",
      " [-0.3785776 ]\n",
      " [ 0.18047157]\n",
      " ...\n",
      " [ 1.41743265]\n",
      " [-1.07458164]\n",
      " [ 1.19694695]]\n",
      "Current iteration=4, loss=31656.563200330613\n",
      "t [[-1.55236869]\n",
      " [-0.44154009]\n",
      " [ 0.21303673]\n",
      " ...\n",
      " [ 1.56098282]\n",
      " [-1.19199071]\n",
      " [ 1.3338669 ]]\n",
      "t [[-1.55236869]\n",
      " [-0.44154009]\n",
      " [ 0.21303673]\n",
      " ...\n",
      " [ 1.56098282]\n",
      " [-1.19199071]\n",
      " [ 1.3338669 ]]\n",
      "t [[-1.70161686]\n",
      " [-0.49537361]\n",
      " [ 0.23856727]\n",
      " ...\n",
      " [ 1.66814974]\n",
      " [-1.28243083]\n",
      " [ 1.44154286]]\n",
      "t [[-1.70161686]\n",
      " [-0.49537361]\n",
      " [ 0.23856727]\n",
      " ...\n",
      " [ 1.66814974]\n",
      " [-1.28243083]\n",
      " [ 1.44154286]]\n",
      "Current iteration=6, loss=30938.69955727029\n",
      "t [[-1.82749485]\n",
      " [-0.54195753]\n",
      " [ 0.25856251]\n",
      " ...\n",
      " [ 1.74900844]\n",
      " [-1.35320421]\n",
      " [ 1.5276791 ]]\n",
      "t [[-1.82749485]\n",
      " [-0.54195753]\n",
      " [ 0.25856251]\n",
      " ...\n",
      " [ 1.74900844]\n",
      " [-1.35320421]\n",
      " [ 1.5276791 ]]\n",
      "t [[-1.93511822]\n",
      " [-0.58268646]\n",
      " [ 0.27430826]\n",
      " ...\n",
      " [ 1.81042291]\n",
      " [-1.40931589]\n",
      " [ 1.59757697]]\n",
      "t [[-1.93511822]\n",
      " [-0.58268646]\n",
      " [ 0.27430826]\n",
      " ...\n",
      " [ 1.81042291]\n",
      " [-1.40931589]\n",
      " [ 1.59757697]]\n",
      "Current iteration=8, loss=30528.92402516991\n",
      "t [[-2.0281863 ]\n",
      " [-0.61861   ]\n",
      " [ 0.28683386]\n",
      " ...\n",
      " [ 1.85721917]\n",
      " [-1.45430429]\n",
      " [ 1.65500059]]\n",
      "t [[-2.0281863 ]\n",
      " [-0.61861   ]\n",
      " [ 0.28683386]\n",
      " ...\n",
      " [ 1.85721917]\n",
      " [-1.45430429]\n",
      " [ 1.65500059]]\n",
      "t [[-2.1094513 ]\n",
      " [-0.65053289]\n",
      " [ 0.29693816]\n",
      " ...\n",
      " [ 1.89287921]\n",
      " [-1.49073617]\n",
      " [ 1.70269312]]\n",
      "loss=30260.12549651535\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.43987962]\n",
      " [-0.09253671]\n",
      " [ 0.09184298]\n",
      " ...\n",
      " [-0.11609365]\n",
      " [-0.14434219]\n",
      " [-0.03034397]]\n",
      "t [[-0.43987962]\n",
      " [-0.09253671]\n",
      " [ 0.09184298]\n",
      " ...\n",
      " [-0.11609365]\n",
      " [-0.14434219]\n",
      " [-0.03034397]]\n",
      "t [[-0.73833276]\n",
      " [-0.12348401]\n",
      " [ 0.16611899]\n",
      " ...\n",
      " [-0.20782468]\n",
      " [-0.25251956]\n",
      " [-0.06013498]]\n",
      "t [[-0.73833276]\n",
      " [-0.12348401]\n",
      " [ 0.16611899]\n",
      " ...\n",
      " [-0.20782468]\n",
      " [-0.25251956]\n",
      " [-0.06013498]]\n",
      "Current iteration=2, loss=33089.418049942775\n",
      "t [[-0.93881939]\n",
      " [-0.13759725]\n",
      " [ 0.21867966]\n",
      " ...\n",
      " [-0.28153341]\n",
      " [-0.3360717 ]\n",
      " [-0.08629445]]\n",
      "t [[-0.93881939]\n",
      " [-0.13759725]\n",
      " [ 0.21867966]\n",
      " ...\n",
      " [-0.28153341]\n",
      " [-0.3360717 ]\n",
      " [-0.08629445]]\n",
      "t [[-1.07460982]\n",
      " [-0.14840181]\n",
      " [ 0.25530394]\n",
      " ...\n",
      " [-0.34175132]\n",
      " [-0.40260216]\n",
      " [-0.10889888]]\n",
      "t [[-1.07460982]\n",
      " [-0.14840181]\n",
      " [ 0.25530394]\n",
      " ...\n",
      " [-0.34175132]\n",
      " [-0.40260216]\n",
      " [-0.10889888]]\n",
      "Current iteration=4, loss=31539.01138780029\n",
      "t [[-1.16687246]\n",
      " [-0.16004453]\n",
      " [ 0.28101229]\n",
      " ...\n",
      " [-0.39167781]\n",
      " [-0.4569294 ]\n",
      " [-0.12844964]]\n",
      "t [[-1.16687246]\n",
      " [-0.16004453]\n",
      " [ 0.28101229]\n",
      " ...\n",
      " [-0.39167781]\n",
      " [-0.4569294 ]\n",
      " [-0.12844964]]\n",
      "t [[-1.2289661 ]\n",
      " [-0.17370731]\n",
      " [ 0.29918053]\n",
      " ...\n",
      " [-0.43359322]\n",
      " [-0.50219226]\n",
      " [-0.14540245]]\n",
      "t [[-1.2289661 ]\n",
      " [-0.17370731]\n",
      " [ 0.29918053]\n",
      " ...\n",
      " [-0.43359322]\n",
      " [-0.50219226]\n",
      " [-0.14540245]]\n",
      "Current iteration=6, loss=30832.057339950607\n",
      "t [[-1.26959261]\n",
      " [-0.18954674]\n",
      " [ 0.31202639]\n",
      " ...\n",
      " [-0.46915312]\n",
      " [-0.54051777]\n",
      " [-0.16012677]]\n",
      "t [[-1.26959261]\n",
      " [-0.18954674]\n",
      " [ 0.31202639]\n",
      " ...\n",
      " [-0.46915312]\n",
      " [-0.54051777]\n",
      " [-0.16012677]]\n",
      "t [[-1.29462909]\n",
      " [-0.20735855]\n",
      " [ 0.32103753]\n",
      " ...\n",
      " [-0.49958536]\n",
      " [-0.573402  ]\n",
      " [-0.17292318]]\n",
      "t [[-1.29462909]\n",
      " [-0.20735855]\n",
      " [ 0.32103753]\n",
      " ...\n",
      " [-0.49958536]\n",
      " [-0.573402  ]\n",
      " [-0.17292318]]\n",
      "Current iteration=8, loss=30432.209920656293\n",
      "t [[-1.30817391]\n",
      " [-0.22682775]\n",
      " [ 0.32724033]\n",
      " ...\n",
      " [-0.52581976]\n",
      " [-0.60193144]\n",
      " [-0.18404133]]\n",
      "t [[-1.30817391]\n",
      " [-0.22682775]\n",
      " [ 0.32724033]\n",
      " ...\n",
      " [-0.52581976]\n",
      " [-0.60193144]\n",
      " [-0.18404133]]\n",
      "t [[-1.3131614 ]\n",
      " [-0.24762689]\n",
      " [ 0.33136146]\n",
      " ...\n",
      " [-0.54857411]\n",
      " [-0.6269167 ]\n",
      " [-0.19369255]]\n",
      "loss=30171.420904849037\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.51660218]\n",
      " [-0.11638391]\n",
      " [ 0.04546111]\n",
      " ...\n",
      " [-0.11159508]\n",
      " [-0.14509268]\n",
      " [-0.03011608]]\n",
      "t [[-0.51660218]\n",
      " [-0.11638391]\n",
      " [ 0.04546111]\n",
      " ...\n",
      " [-0.11159508]\n",
      " [-0.14509268]\n",
      " [-0.03011608]]\n",
      "t [[-0.89015552]\n",
      " [-0.22643909]\n",
      " [ 0.10466219]\n",
      " ...\n",
      " [-0.19963721]\n",
      " [-0.25371649]\n",
      " [-0.06038578]]\n",
      "t [[-0.89015552]\n",
      " [-0.22643909]\n",
      " [ 0.10466219]\n",
      " ...\n",
      " [-0.19963721]\n",
      " [-0.25371649]\n",
      " [-0.06038578]]\n",
      "Current iteration=2, loss=33056.3238520647\n",
      "t [[-1.1729851 ]\n",
      " [-0.3189324 ]\n",
      " [ 0.15676524]\n",
      " ...\n",
      " [-0.27053436]\n",
      " [-0.33755916]\n",
      " [-0.08706206]]\n",
      "t [[-1.1729851 ]\n",
      " [-0.3189324 ]\n",
      " [ 0.15676524]\n",
      " ...\n",
      " [-0.27053436]\n",
      " [-0.33755916]\n",
      " [-0.08706206]]\n",
      "t [[-1.39526264]\n",
      " [-0.39592891]\n",
      " [ 0.19845097]\n",
      " ...\n",
      " [-0.3286741 ]\n",
      " [-0.40430799]\n",
      " [-0.11009609]]\n",
      "t [[-1.39526264]\n",
      " [-0.39592891]\n",
      " [ 0.19845097]\n",
      " ...\n",
      " [-0.3286741 ]\n",
      " [-0.40430799]\n",
      " [-0.11009609]]\n",
      "Current iteration=4, loss=31498.137536637463\n",
      "t [[-1.57499297]\n",
      " [-0.46076825]\n",
      " [ 0.23082205]\n",
      " ...\n",
      " [-0.3770984 ]\n",
      " [-0.45882335]\n",
      " [-0.1299798 ]]\n",
      "t [[-1.57499297]\n",
      " [-0.46076825]\n",
      " [ 0.23082205]\n",
      " ...\n",
      " [-0.3770984 ]\n",
      " [-0.45882335]\n",
      " [-0.1299798 ]]\n",
      "t [[-1.72354974]\n",
      " [-0.51612152]\n",
      " [ 0.25575266]\n",
      " ...\n",
      " [-0.41795774]\n",
      " [-0.50426482]\n",
      " [-0.14718001]]\n",
      "t [[-1.72354974]\n",
      " [-0.51612152]\n",
      " [ 0.25575266]\n",
      " ...\n",
      " [-0.41795774]\n",
      " [-0.50426482]\n",
      " [-0.14718001]]\n",
      "Current iteration=6, loss=30783.89740755462\n",
      "t [[-1.84849773]\n",
      " [-0.56395478]\n",
      " [ 0.27497739]\n",
      " ...\n",
      " [-0.452807  ]\n",
      " [-0.5427698 ]\n",
      " [-0.16207882]]\n",
      "t [[-1.84849773]\n",
      " [-0.56395478]\n",
      " [ 0.27497739]\n",
      " ...\n",
      " [-0.452807  ]\n",
      " [-0.5427698 ]\n",
      " [-0.16207882]]\n",
      "t [[-1.95509005]\n",
      " [-0.60571616]\n",
      " [ 0.28990609]\n",
      " ...\n",
      " [-0.48279689]\n",
      " [-0.57583936]\n",
      " [-0.17498788]]\n",
      "t [[-1.95509005]\n",
      " [-0.60571616]\n",
      " [ 0.28990609]\n",
      " ...\n",
      " [-0.48279689]\n",
      " [-0.57583936]\n",
      " [-0.17498788]]\n",
      "Current iteration=8, loss=30376.32176354566\n",
      "t [[-2.04710606]\n",
      " [-0.642492  ]\n",
      " [ 0.30163198]\n",
      " ...\n",
      " [-0.50879789]\n",
      " [-0.60456217]\n",
      " [-0.18616605]]\n",
      "t [[-2.04710606]\n",
      " [-0.642492  ]\n",
      " [ 0.30163198]\n",
      " ...\n",
      " [-0.50879789]\n",
      " [-0.60456217]\n",
      " [-0.18616605]]\n",
      "t [[-2.12734502]\n",
      " [-0.6751143 ]\n",
      " [ 0.31098502]\n",
      " ...\n",
      " [-0.53148171]\n",
      " [-0.62974945]\n",
      " [-0.19583236]]\n",
      "loss=30107.963083367962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.51792345]\n",
      " [-0.12003765]\n",
      " [ 0.03119589]\n",
      " ...\n",
      " [-0.11030815]\n",
      " [-0.14594492]\n",
      " [-0.02951869]]\n",
      "t [[-0.51792345]\n",
      " [-0.12003765]\n",
      " [ 0.03119589]\n",
      " ...\n",
      " [-0.11030815]\n",
      " [-0.14594492]\n",
      " [-0.02951869]]\n",
      "t [[-0.892607  ]\n",
      " [-0.23190107]\n",
      " [ 0.08366486]\n",
      " ...\n",
      " [-0.19840782]\n",
      " [-0.25522475]\n",
      " [-0.05927793]]\n",
      "t [[-0.892607  ]\n",
      " [-0.23190107]\n",
      " [ 0.08366486]\n",
      " ...\n",
      " [-0.19840782]\n",
      " [-0.25522475]\n",
      " [-0.05927793]]\n",
      "Current iteration=2, loss=32996.01928880297\n",
      "t [[-1.17624728]\n",
      " [-0.32580304]\n",
      " [ 0.1331564 ]\n",
      " ...\n",
      " [-0.26988129]\n",
      " [-0.33961058]\n",
      " [-0.08572271]]\n",
      "t [[-1.17624728]\n",
      " [-0.32580304]\n",
      " [ 0.1331564 ]\n",
      " ...\n",
      " [-0.26988129]\n",
      " [-0.33961058]\n",
      " [-0.08572271]]\n",
      "t [[-1.39912366]\n",
      " [-0.40405864]\n",
      " [ 0.17431289]\n",
      " ...\n",
      " [-0.32876619]\n",
      " [-0.40682835]\n",
      " [-0.1087295 ]]\n",
      "t [[-1.39912366]\n",
      " [-0.40405864]\n",
      " [ 0.17431289]\n",
      " ...\n",
      " [-0.32876619]\n",
      " [-0.40682835]\n",
      " [-0.1087295 ]]\n",
      "Current iteration=4, loss=31419.159775279688\n",
      "t [[-1.57930865]\n",
      " [-0.47006698]\n",
      " [ 0.2072281 ]\n",
      " ...\n",
      " [-0.37796066]\n",
      " [-0.46175643]\n",
      " [-0.12871572]]\n",
      "t [[-1.57930865]\n",
      " [-0.47006698]\n",
      " [ 0.2072281 ]\n",
      " ...\n",
      " [-0.37796066]\n",
      " [-0.46175643]\n",
      " [-0.12871572]]\n",
      "t [[-1.72820964]\n",
      " [-0.52652653]\n",
      " [ 0.23327878]\n",
      " ...\n",
      " [-0.41955662]\n",
      " [-0.50756426]\n",
      " [-0.1460998 ]]\n",
      "t [[-1.72820964]\n",
      " [-0.52652653]\n",
      " [ 0.23327878]\n",
      " ...\n",
      " [-0.41955662]\n",
      " [-0.50756426]\n",
      " [-0.1460998 ]]\n",
      "Current iteration=6, loss=30697.865392270724\n",
      "t [[-1.85340932]\n",
      " [-0.57542136]\n",
      " [ 0.25394334]\n",
      " ...\n",
      " [-0.45508643]\n",
      " [-0.54639553]\n",
      " [-0.1612343 ]]\n",
      "t [[-1.85340932]\n",
      " [-0.57542136]\n",
      " [ 0.25394334]\n",
      " ...\n",
      " [-0.45508643]\n",
      " [-0.54639553]\n",
      " [-0.1612343 ]]\n",
      "t [[-1.9601721 ]\n",
      " [-0.61821201]\n",
      " [ 0.27049218]\n",
      " ...\n",
      " [-0.48569392]\n",
      " [-0.57975594]\n",
      " [-0.17441274]]\n",
      "t [[-1.9601721 ]\n",
      " [-0.61821201]\n",
      " [ 0.27049218]\n",
      " ...\n",
      " [-0.48569392]\n",
      " [-0.57975594]\n",
      " [-0.17441274]]\n",
      "Current iteration=8, loss=30287.761736646265\n",
      "t [[-2.05228593]\n",
      " [-0.65599315]\n",
      " [ 0.28393829]\n",
      " ...\n",
      " [-0.51224957]\n",
      " [-0.60873792]\n",
      " [-0.18588268]]\n",
      "t [[-2.05228593]\n",
      " [-0.65599315]\n",
      " [ 0.28393829]\n",
      " ...\n",
      " [-0.51224957]\n",
      " [-0.60873792]\n",
      " [-0.18588268]]\n",
      "t [[-2.13255747]\n",
      " [-0.68960207]\n",
      " [ 0.29506289]\n",
      " ...\n",
      " [-0.53542803]\n",
      " [-0.63415587]\n",
      " [-0.19585591]]\n",
      "loss=30018.930471853433\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.51383832]\n",
      " [-0.11176193]\n",
      " [ 0.03658037]\n",
      " ...\n",
      " [ 0.575918  ]\n",
      " [-0.42405081]\n",
      " [ 0.46713649]]\n",
      "t [[-0.51383832]\n",
      " [-0.11176193]\n",
      " [ 0.03658037]\n",
      " ...\n",
      " [ 0.575918  ]\n",
      " [-0.42405081]\n",
      " [ 0.46713649]]\n",
      "t [[-0.88622838]\n",
      " [-0.21919315]\n",
      " [ 0.09194299]\n",
      " ...\n",
      " [ 0.96731961]\n",
      " [-0.72097277]\n",
      " [ 0.79621369]]\n",
      "t [[-0.88622838]\n",
      " [-0.21919315]\n",
      " [ 0.09194299]\n",
      " ...\n",
      " [ 0.96731961]\n",
      " [-0.72097277]\n",
      " [ 0.79621369]]\n",
      "Current iteration=2, loss=33148.99994844332\n",
      "t [[-1.16887548]\n",
      " [-0.30966313]\n",
      " [ 0.14260815]\n",
      " ...\n",
      " [ 1.2396047 ]\n",
      " [-0.93269943]\n",
      " [ 1.03439199]]\n",
      "t [[-1.16887548]\n",
      " [-0.30966313]\n",
      " [ 0.14260815]\n",
      " ...\n",
      " [ 1.2396047 ]\n",
      " [-0.93269943]\n",
      " [ 1.03439199]]\n",
      "t [[-1.39145812]\n",
      " [-0.38493171]\n",
      " [ 0.18396829]\n",
      " ...\n",
      " [ 1.43425847]\n",
      " [-1.08794402]\n",
      " [ 1.2122416 ]]\n",
      "t [[-1.39145812]\n",
      " [-0.38493171]\n",
      " [ 0.18396829]\n",
      " ...\n",
      " [ 1.43425847]\n",
      " [-1.08794402]\n",
      " [ 1.2122416 ]]\n",
      "Current iteration=4, loss=31610.10963475804\n",
      "t [[-1.57171368]\n",
      " [-0.44825412]\n",
      " [ 0.21651995]\n",
      " ...\n",
      " [ 1.57648732]\n",
      " [-1.20463555]\n",
      " [ 1.3486221 ]]\n",
      "t [[-1.57171368]\n",
      " [-0.44825412]\n",
      " [ 0.21651995]\n",
      " ...\n",
      " [ 1.57648732]\n",
      " [-1.20463555]\n",
      " [ 1.3486221 ]]\n",
      "t [[-1.72087677]\n",
      " [-0.50226758]\n",
      " [ 0.24185363]\n",
      " ...\n",
      " [ 1.68205958]\n",
      " [-1.2941277 ]\n",
      " [ 1.45547721]]\n",
      "t [[-1.72087677]\n",
      " [-0.50226758]\n",
      " [ 0.24185363]\n",
      " ...\n",
      " [ 1.68205958]\n",
      " [-1.2941277 ]\n",
      " [ 1.45547721]]\n",
      "Current iteration=6, loss=30903.008008316552\n",
      "t [[-1.84643886]\n",
      " [-0.54891861]\n",
      " [ 0.26157056]\n",
      " ...\n",
      " [ 1.7612589 ]\n",
      " [-1.36388075]\n",
      " [ 1.54068101]]\n",
      "t [[-1.84643886]\n",
      " [-0.54891861]\n",
      " [ 0.26157056]\n",
      " ...\n",
      " [ 1.7612589 ]\n",
      " [-1.36388075]\n",
      " [ 1.54068101]]\n",
      "t [[-1.95361412]\n",
      " [-0.58964065]\n",
      " [ 0.27701785]\n",
      " ...\n",
      " [ 1.82105554]\n",
      " [-1.41898179]\n",
      " [ 1.60962528]]\n",
      "t [[-1.95361412]\n",
      " [-0.58964065]\n",
      " [ 0.27701785]\n",
      " ...\n",
      " [ 1.82105554]\n",
      " [-1.41898179]\n",
      " [ 1.60962528]]\n",
      "Current iteration=8, loss=30499.95484046152\n",
      "t [[-2.0461617 ]\n",
      " [-0.62550738]\n",
      " [ 0.28925955]\n",
      " ...\n",
      " [ 1.86632928]\n",
      " [-1.46301211]\n",
      " [ 1.66612293]]\n",
      "t [[-2.0461617 ]\n",
      " [-0.62550738]\n",
      " [ 0.28925955]\n",
      " ...\n",
      " [ 1.86632928]\n",
      " [-1.46301211]\n",
      " [ 1.66612293]]\n",
      "t [[-2.12687119]\n",
      " [-0.65733919]\n",
      " [ 0.29911243]\n",
      " ...\n",
      " [ 1.90058739]\n",
      " [-1.49855994]\n",
      " [ 1.71294288]]\n",
      "loss=30235.374064131785\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.44904378]\n",
      " [-0.09446456]\n",
      " [ 0.09375638]\n",
      " ...\n",
      " [-0.11851227]\n",
      " [-0.14734932]\n",
      " [-0.03097614]]\n",
      "t [[-0.44904378]\n",
      " [-0.09446456]\n",
      " [ 0.09375638]\n",
      " ...\n",
      " [-0.11851227]\n",
      " [-0.14734932]\n",
      " [-0.03097614]]\n",
      "t [[-0.75078615]\n",
      " [-0.12478595]\n",
      " [ 0.16920993]\n",
      " ...\n",
      " [-0.21165347]\n",
      " [-0.25703263]\n",
      " [-0.06137676]]\n",
      "t [[-0.75078615]\n",
      " [-0.12478595]\n",
      " [ 0.16920993]\n",
      " ...\n",
      " [-0.21165347]\n",
      " [-0.25703263]\n",
      " [-0.06137676]]\n",
      "Current iteration=2, loss=33031.58465705208\n",
      "t [[-0.95144573]\n",
      " [-0.13849432]\n",
      " [ 0.22197712]\n",
      " ...\n",
      " [-0.28616618]\n",
      " [-0.34131966]\n",
      " [-0.08793572]]\n",
      "t [[-0.95144573]\n",
      " [-0.13849432]\n",
      " [ 0.22197712]\n",
      " ...\n",
      " [-0.28616618]\n",
      " [-0.34131966]\n",
      " [-0.08793572]]\n",
      "t [[-1.08607682]\n",
      " [-0.14929388]\n",
      " [ 0.25838762]\n",
      " ...\n",
      " [-0.3468123 ]\n",
      " [-0.40818781]\n",
      " [-0.1107968 ]]\n",
      "t [[-1.08607682]\n",
      " [-0.14929388]\n",
      " [ 0.25838762]\n",
      " ...\n",
      " [-0.3468123 ]\n",
      " [-0.40818781]\n",
      " [-0.1107968 ]]\n",
      "Current iteration=4, loss=31493.842088201403\n",
      "t [[-1.17665158]\n",
      " [-0.16122449]\n",
      " [ 0.2837313 ]\n",
      " ...\n",
      " [-0.39693138]\n",
      " [-0.4626372 ]\n",
      " [-0.13050638]]\n",
      "t [[-1.17665158]\n",
      " [-0.16122449]\n",
      " [ 0.2837313 ]\n",
      " ...\n",
      " [-0.39693138]\n",
      " [-0.4626372 ]\n",
      " [-0.13050638]]\n",
      "t [[-1.23688803]\n",
      " [-0.17536533]\n",
      " [ 0.30149384]\n",
      " ...\n",
      " [-0.43888931]\n",
      " [-0.50789979]\n",
      " [-0.14754523]]\n",
      "t [[-1.23688803]\n",
      " [-0.17536533]\n",
      " [ 0.30149384]\n",
      " ...\n",
      " [-0.43888931]\n",
      " [-0.50789979]\n",
      " [-0.14754523]]\n",
      "Current iteration=6, loss=30797.76369466028\n",
      "t [[-1.27565651]\n",
      " [-0.19179791]\n",
      " [ 0.3139386 ]\n",
      " ...\n",
      " [-0.47439481]\n",
      " [-0.54615363]\n",
      " [-0.16229903]]\n",
      "t [[-1.27565651]\n",
      " [-0.19179791]\n",
      " [ 0.3139386 ]\n",
      " ...\n",
      " [-0.47439481]\n",
      " [-0.54615363]\n",
      " [-0.16229903]]\n",
      "t [[-1.29891552]\n",
      " [-0.21026576]\n",
      " [ 0.32257277]\n",
      " ...\n",
      " [-0.50470955]\n",
      " [-0.57892434]\n",
      " [-0.17508051]]\n",
      "t [[-1.29891552]\n",
      " [-0.21026576]\n",
      " [ 0.32257277]\n",
      " ...\n",
      " [-0.50470955]\n",
      " [-0.57892434]\n",
      " [-0.17508051]]\n",
      "Current iteration=8, loss=30404.587191777107\n",
      "t [[-1.31080197]\n",
      " [-0.2304182 ]\n",
      " [ 0.32843074]\n",
      " ...\n",
      " [-0.5307855 ]\n",
      " [-0.6073163 ]\n",
      " [-0.18614907]]\n",
      "t [[-1.31080197]\n",
      " [-0.2304182 ]\n",
      " [ 0.32843074]\n",
      " ...\n",
      " [-0.5307855 ]\n",
      " [-0.6073163 ]\n",
      " [-0.18614907]]\n",
      "t [[-1.31426624]\n",
      " [-0.25190374]\n",
      " [ 0.33224173]\n",
      " ...\n",
      " [-0.55335523]\n",
      " [-0.63215125]\n",
      " [-0.19572402]]\n",
      "loss=30147.931499495167\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.52736473]\n",
      " [-0.11880857]\n",
      " [ 0.04640822]\n",
      " ...\n",
      " [-0.11391998]\n",
      " [-0.14811545]\n",
      " [-0.0307435 ]]\n",
      "t [[-0.52736473]\n",
      " [-0.11880857]\n",
      " [ 0.04640822]\n",
      " ...\n",
      " [-0.11391998]\n",
      " [-0.14811545]\n",
      " [-0.0307435 ]]\n",
      "t [[-0.90575539]\n",
      " [-0.23101476]\n",
      " [ 0.10712134]\n",
      " ...\n",
      " [-0.20331185]\n",
      " [-0.25824837]\n",
      " [-0.06164729]]\n",
      "t [[-0.90575539]\n",
      " [-0.23101476]\n",
      " [ 0.10712134]\n",
      " ...\n",
      " [-0.20331185]\n",
      " [-0.25824837]\n",
      " [-0.06164729]]\n",
      "Current iteration=2, loss=32998.214881855514\n",
      "t [[-1.19078818]\n",
      " [-0.32470938]\n",
      " [ 0.16002024]\n",
      " ...\n",
      " [-0.2749897 ]\n",
      " [-0.3428257 ]\n",
      " [-0.08873539]]\n",
      "t [[-1.19078818]\n",
      " [-0.32470938]\n",
      " [ 0.16002024]\n",
      " ...\n",
      " [-0.2749897 ]\n",
      " [-0.3428257 ]\n",
      " [-0.08873539]]\n",
      "t [[-1.41397582]\n",
      " [-0.40235742]\n",
      " [ 0.20194282]\n",
      " ...\n",
      " [-0.33355899]\n",
      " [-0.40991226]\n",
      " [-0.11202978]]\n",
      "t [[-1.41397582]\n",
      " [-0.40235742]\n",
      " [ 0.20194282]\n",
      " ...\n",
      " [-0.33355899]\n",
      " [-0.40991226]\n",
      " [-0.11202978]]\n",
      "Current iteration=4, loss=31452.67624745624\n",
      "t [[-1.59393339]\n",
      " [-0.46754607]\n",
      " [ 0.23422759]\n",
      " ...\n",
      " [-0.38219185]\n",
      " [-0.46455103]\n",
      " [-0.13207143]]\n",
      "t [[-1.59393339]\n",
      " [-0.46754607]\n",
      " [ 0.23422759]\n",
      " ...\n",
      " [-0.38219185]\n",
      " [-0.46455103]\n",
      " [-0.13207143]]\n",
      "t [[-1.74234074]\n",
      " [-0.5230699 ]\n",
      " [ 0.25891017]\n",
      " ...\n",
      " [-0.42311792]\n",
      " [-0.50999481]\n",
      " [-0.14935406]]\n",
      "t [[-1.74234074]\n",
      " [-0.5230699 ]\n",
      " [ 0.25891017]\n",
      " ...\n",
      " [-0.42311792]\n",
      " [-0.50999481]\n",
      " [-0.14935406]]\n",
      "Current iteration=6, loss=30749.13266153089\n",
      "t [[-1.86692811]\n",
      " [-0.57096059]\n",
      " [ 0.27782357]\n",
      " ...\n",
      " [-0.45794125]\n",
      " [-0.54843183]\n",
      " [-0.164277  ]]\n",
      "t [[-1.86692811]\n",
      " [-0.57096059]\n",
      " [ 0.27782357]\n",
      " ...\n",
      " [-0.45794125]\n",
      " [-0.54843183]\n",
      " [-0.164277  ]]\n",
      "t [[-1.97304333]\n",
      " [-0.61270444]\n",
      " [ 0.2924345 ]\n",
      " ...\n",
      " [-0.48784389]\n",
      " [-0.58139249]\n",
      " [-0.17716454]]\n",
      "t [[-1.97304333]\n",
      " [-0.61270444]\n",
      " [ 0.2924345 ]\n",
      " ...\n",
      " [-0.48784389]\n",
      " [-0.58139249]\n",
      " [-0.17716454]]\n",
      "Current iteration=8, loss=30348.054024270503\n",
      "t [[-2.06452242]\n",
      " [-0.64941166]\n",
      " [ 0.30386705]\n",
      " ...\n",
      " [-0.51371683]\n",
      " [-0.60998314]\n",
      " [-0.18828565]]\n",
      "t [[-2.06452242]\n",
      " [-0.64941166]\n",
      " [ 0.30386705]\n",
      " ...\n",
      " [-0.51371683]\n",
      " [-0.60998314]\n",
      " [-0.18828565]]\n",
      "t [[-2.14419944]\n",
      " [-0.68192997]\n",
      " [ 0.31296595]\n",
      " ...\n",
      " [-0.53624557]\n",
      " [-0.63502592]\n",
      " [-0.19786763]]\n",
      "loss=30083.704236769318\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.672358363896\n",
      "t [[-0.52871352]\n",
      " [-0.12253843]\n",
      " [ 0.0318458 ]\n",
      " ...\n",
      " [-0.11260624]\n",
      " [-0.14898544]\n",
      " [-0.03013366]]\n",
      "t [[-0.52871352]\n",
      " [-0.12253843]\n",
      " [ 0.0318458 ]\n",
      " ...\n",
      " [-0.11260624]\n",
      " [-0.14898544]\n",
      " [-0.03013366]]\n",
      "t [[-0.90825564]\n",
      " [-0.2365517 ]\n",
      " [ 0.08584054]\n",
      " ...\n",
      " [-0.20208446]\n",
      " [-0.25978431]\n",
      " [-0.06051807]]\n",
      "t [[-0.90825564]\n",
      " [-0.2365517 ]\n",
      " [ 0.08584054]\n",
      " ...\n",
      " [-0.20208446]\n",
      " [-0.25978431]\n",
      " [-0.06051807]]\n",
      "Current iteration=2, loss=32937.171996673525\n",
      "t [[-1.19410528]\n",
      " [-0.33166779]\n",
      " [ 0.13623978]\n",
      " ...\n",
      " [-0.27437163]\n",
      " [-0.34491203]\n",
      " [-0.08738113]]\n",
      "t [[-1.19410528]\n",
      " [-0.33166779]\n",
      " [ 0.13623978]\n",
      " ...\n",
      " [-0.27437163]\n",
      " [-0.34491203]\n",
      " [-0.08738113]]\n",
      "t [[-1.41789268]\n",
      " [-0.41059235]\n",
      " [ 0.17775062]\n",
      " ...\n",
      " [-0.33371219]\n",
      " [-0.41247314]\n",
      " [-0.11066021]]\n",
      "t [[-1.41789268]\n",
      " [-0.41059235]\n",
      " [ 0.17775062]\n",
      " ...\n",
      " [-0.33371219]\n",
      " [-0.41247314]\n",
      " [-0.11066021]]\n",
      "Current iteration=4, loss=31373.171078363463\n",
      "t [[-1.5983032 ]\n",
      " [-0.47696763]\n",
      " [ 0.21068136]\n",
      " ...\n",
      " [-0.38313376]\n",
      " [-0.46752888]\n",
      " [-0.13081722]]\n",
      "t [[-1.5983032 ]\n",
      " [-0.47696763]\n",
      " [ 0.21068136]\n",
      " ...\n",
      " [-0.38313376]\n",
      " [-0.46752888]\n",
      " [-0.13081722]]\n",
      "t [[-1.7470507 ]\n",
      " [-0.53361502]\n",
      " [ 0.23656937]\n",
      " ...\n",
      " [-0.42480879]\n",
      " [-0.51334199]\n",
      " [-0.14829608]]\n",
      "t [[-1.7470507 ]\n",
      " [-0.53361502]\n",
      " [ 0.23656937]\n",
      " ...\n",
      " [-0.42480879]\n",
      " [-0.51334199]\n",
      " [-0.14829608]]\n",
      "Current iteration=6, loss=30662.79436503592\n",
      "t [[-1.87188337]\n",
      " [-0.58258444]\n",
      " [ 0.25699463]\n",
      " ...\n",
      " [-0.46032028]\n",
      " [-0.55210716]\n",
      " [-0.16346619]]\n",
      "t [[-1.87188337]\n",
      " [-0.58258444]\n",
      " [ 0.25699463]\n",
      " ...\n",
      " [-0.46032028]\n",
      " [-0.55210716]\n",
      " [-0.16346619]]\n",
      "t [[-1.97816049]\n",
      " [-0.62537477]\n",
      " [ 0.27328752]\n",
      " ...\n",
      " [-0.49084452]\n",
      " [-0.58535957]\n",
      " [-0.17663375]]\n",
      "t [[-1.97816049]\n",
      " [-0.62537477]\n",
      " [ 0.27328752]\n",
      " ...\n",
      " [-0.49084452]\n",
      " [-0.58535957]\n",
      " [-0.17663375]]\n",
      "Current iteration=8, loss=30259.36224526545\n",
      "t [[-2.0697269 ]\n",
      " [-0.66310458]\n",
      " [ 0.28649414]\n",
      " ...\n",
      " [-0.51727339]\n",
      " [-0.61420946]\n",
      " [-0.18805653]]\n",
      "t [[-2.0697269 ]\n",
      " [-0.66310458]\n",
      " [ 0.28649414]\n",
      " ...\n",
      " [-0.51727339]\n",
      " [-0.61420946]\n",
      " [-0.18805653]]\n",
      "t [[-2.14942444]\n",
      " [-0.69662679]\n",
      " [ 0.29741211]\n",
      " ...\n",
      " [-0.54029593]\n",
      " [-0.63948229]\n",
      " [-0.19795474]]\n",
      "loss=29994.669441822123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=37710.67235836389\n",
      "t [[-0.52454329]\n",
      " [-0.1140903 ]\n",
      " [ 0.03734246]\n",
      " ...\n",
      " [ 0.58791629]\n",
      " [-0.43288521]\n",
      " [ 0.4768685 ]]\n",
      "t [[-0.52454329]\n",
      " [-0.1140903 ]\n",
      " [ 0.03734246]\n",
      " ...\n",
      " [ 0.58791629]\n",
      " [-0.43288521]\n",
      " [ 0.4768685 ]]\n",
      "t [[-0.9017788 ]\n",
      " [-0.22365809]\n",
      " [ 0.0942396 ]\n",
      " ...\n",
      " [ 0.98366611]\n",
      " [-0.73336634]\n",
      " [ 0.80995503]]\n",
      "t [[-0.9017788 ]\n",
      " [-0.22365809]\n",
      " [ 0.0942396 ]\n",
      " ...\n",
      " [ 0.98366611]\n",
      " [-0.73336634]\n",
      " [ 0.80995503]]\n",
      "Current iteration=2, loss=33091.80984806211\n",
      "t [[-1.18666401]\n",
      " [-0.3153102 ]\n",
      " [ 0.14576527]\n",
      " ...\n",
      " [ 1.25677055]\n",
      " [-0.94602751]\n",
      " [ 1.04939452]]\n",
      "t [[-1.18666401]\n",
      " [-0.3153102 ]\n",
      " [ 0.14576527]\n",
      " ...\n",
      " [ 1.25677055]\n",
      " [-0.94602751]\n",
      " [ 1.04939452]]\n",
      "t [[-1.41019239]\n",
      " [-0.39121189]\n",
      " [ 0.18742199]\n",
      " ...\n",
      " [ 1.45070095]\n",
      " [-1.10102734]\n",
      " [ 1.22723921]]\n",
      "t [[-1.41019239]\n",
      " [-0.39121189]\n",
      " [ 0.18742199]\n",
      " ...\n",
      " [ 1.45070095]\n",
      " [-1.10102734]\n",
      " [ 1.22723921]]\n",
      "Current iteration=4, loss=31565.14467686377\n",
      "t [[-1.59070428]\n",
      " [-0.45486943]\n",
      " [ 0.21993266]\n",
      " ...\n",
      " [ 1.59154959]\n",
      " [-1.2169552 ]\n",
      " [ 1.36302866]]\n",
      "t [[-1.59070428]\n",
      " [-0.45486943]\n",
      " [ 0.21993266]\n",
      " ...\n",
      " [ 1.59154959]\n",
      " [-1.2169552 ]\n",
      " [ 1.36302866]]\n",
      "t [[-1.73973934]\n",
      " [-0.50904419]\n",
      " [ 0.24505054]\n",
      " ...\n",
      " [ 1.69549628]\n",
      " [-1.30547369]\n",
      " [ 1.46903184]]\n",
      "t [[-1.73973934]\n",
      " [-0.50904419]\n",
      " [ 0.24505054]\n",
      " ...\n",
      " [ 1.69549628]\n",
      " [-1.30547369]\n",
      " [ 1.46903184]]\n",
      "Current iteration=6, loss=30868.600067662555\n",
      "t [[-1.86495509]\n",
      " [-0.55574809]\n",
      " [ 0.26447872]\n",
      " ...\n",
      " [ 1.77302548]\n",
      " [-1.37419551]\n",
      " [ 1.55328726]]\n",
      "t [[-1.86495509]\n",
      " [-0.55574809]\n",
      " [ 0.26447872]\n",
      " ...\n",
      " [ 1.77302548]\n",
      " [-1.37419551]\n",
      " [ 1.55328726]]\n",
      "t [[-1.9716614 ]\n",
      " [-0.59645224]\n",
      " [ 0.27962421]\n",
      " ...\n",
      " [ 1.83120805]\n",
      " [-1.42828568]\n",
      " [ 1.62127303]]\n",
      "t [[-1.9716614 ]\n",
      " [-0.59645224]\n",
      " [ 0.27962421]\n",
      " ...\n",
      " [ 1.83120805]\n",
      " [-1.42828568]\n",
      " [ 1.62127303]]\n",
      "Current iteration=8, loss=30472.03442469212\n",
      "t [[-2.06367503]\n",
      " [-0.63225359]\n",
      " [ 0.29158406]\n",
      " ...\n",
      " [ 1.87497331]\n",
      " [-1.47136537]\n",
      " [ 1.67684781]]\n",
      "t [[-2.06367503]\n",
      " [-0.63225359]\n",
      " [ 0.29158406]\n",
      " ...\n",
      " [ 1.87497331]\n",
      " [-1.47136537]\n",
      " [ 1.67684781]]\n",
      "t [[-2.14382129]\n",
      " [-0.66398769]\n",
      " [ 0.30119128]\n",
      " ...\n",
      " [ 1.90785068]\n",
      " [-1.50604203]\n",
      " [ 1.72280403]]\n",
      "loss=30211.495393679103\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "t [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current iteration=0, loss=50282.97591936022\n",
      "t [[ 0.01162302]\n",
      " [ 0.00822455]\n",
      " [-0.00369114]\n",
      " ...\n",
      " [-0.01204012]\n",
      " [-0.04089296]\n",
      " [-0.01797669]]\n",
      "t [[ 0.01162302]\n",
      " [ 0.00822455]\n",
      " [-0.00369114]\n",
      " ...\n",
      " [-0.01204012]\n",
      " [-0.04089296]\n",
      " [-0.01797669]]\n",
      "t [[ 0.02245269]\n",
      " [ 0.01615377]\n",
      " [-0.0071666 ]\n",
      " ...\n",
      " [-0.02382514]\n",
      " [-0.08080481]\n",
      " [-0.03555959]]\n",
      "t [[ 0.02245269]\n",
      " [ 0.01615377]\n",
      " [-0.0071666 ]\n",
      " ...\n",
      " [-0.02382514]\n",
      " [-0.08080481]\n",
      " [-0.03555959]]\n",
      "Current iteration=2, loss=49531.42700004802\n",
      "t [[ 0.03254914]\n",
      " [ 0.02379694]\n",
      " [-0.01043334]\n",
      " ...\n",
      " [-0.03536115]\n",
      " [-0.11976695]\n",
      " [-0.05275875]]\n",
      "t [[ 0.03254914]\n",
      " [ 0.02379694]\n",
      " [-0.01043334]\n",
      " ...\n",
      " [-0.03536115]\n",
      " [-0.11976695]\n",
      " [-0.05275875]]\n",
      "t [[ 0.04196903]\n",
      " [ 0.03116323]\n",
      " [-0.01349822]\n",
      " ...\n",
      " [-0.04665426]\n",
      " [-0.15781009]\n",
      " [-0.06958414]]\n",
      "t [[ 0.04196903]\n",
      " [ 0.03116323]\n",
      " [-0.01349822]\n",
      " ...\n",
      " [-0.04665426]\n",
      " [-0.15781009]\n",
      " [-0.06958414]]\n",
      "Current iteration=4, loss=48847.346174606195\n",
      "t [[ 0.05076571]\n",
      " [ 0.03826167]\n",
      " [-0.01636802]\n",
      " ...\n",
      " [-0.05771049]\n",
      " [-0.19496415]\n",
      " [-0.08604567]]\n",
      "t [[ 0.05076571]\n",
      " [ 0.03826167]\n",
      " [-0.01636802]\n",
      " ...\n",
      " [-0.05771049]\n",
      " [-0.19496415]\n",
      " [-0.08604567]]\n",
      "t [[ 0.05898922]\n",
      " [ 0.04510108]\n",
      " [-0.01904939]\n",
      " ...\n",
      " [-0.06853583]\n",
      " [-0.23125825]\n",
      " [-0.10215311]]\n",
      "t [[ 0.05898922]\n",
      " [ 0.04510108]\n",
      " [-0.01904939]\n",
      " ...\n",
      " [-0.06853583]\n",
      " [-0.23125825]\n",
      " [-0.10215311]]\n",
      "Current iteration=6, loss=48224.10339101126\n",
      "t [[ 0.06668641]\n",
      " [ 0.05169012]\n",
      " [-0.02154885]\n",
      " ...\n",
      " [-0.07913618]\n",
      " [-0.26672065]\n",
      " [-0.11791611]]\n",
      "t [[ 0.06668641]\n",
      " [ 0.05169012]\n",
      " [-0.02154885]\n",
      " ...\n",
      " [-0.07913618]\n",
      " [-0.26672065]\n",
      " [-0.11791611]]\n",
      "t [[ 0.07390107]\n",
      " [ 0.05803724]\n",
      " [-0.02387278]\n",
      " ...\n",
      " [-0.08951736]\n",
      " [-0.30137877]\n",
      " [-0.13334416]]\n",
      "t [[ 0.07390107]\n",
      " [ 0.05803724]\n",
      " [-0.02387278]\n",
      " ...\n",
      " [-0.08951736]\n",
      " [-0.30137877]\n",
      " [-0.13334416]]\n",
      "Current iteration=8, loss=47655.65938934026\n",
      "t [[ 0.08067404]\n",
      " [ 0.06415066]\n",
      " [-0.02602741]\n",
      " ...\n",
      " [-0.09968509]\n",
      " [-0.33525912]\n",
      " [-0.1484466 ]]\n",
      "t [[ 0.08067404]\n",
      " [ 0.06415066]\n",
      " [-0.02602741]\n",
      " ...\n",
      " [-0.09968509]\n",
      " [-0.33525912]\n",
      " [-0.1484466 ]]\n",
      "t [[ 0.08704331]\n",
      " [ 0.07003842]\n",
      " [-0.0280188 ]\n",
      " ...\n",
      " [-0.10964498]\n",
      " [-0.36838736]\n",
      " [-0.16323257]]\n",
      "t [[ 0.08704331]\n",
      " [ 0.07003842]\n",
      " [-0.0280188 ]\n",
      " ...\n",
      " [-0.10964498]\n",
      " [-0.36838736]\n",
      " [-0.16323257]]\n",
      "Current iteration=10, loss=47136.54344736417\n",
      "t [[ 0.0930442 ]\n",
      " [ 0.0757083 ]\n",
      " [-0.02985286]\n",
      " ...\n",
      " [-0.11940254]\n",
      " [-0.40078825]\n",
      " [-0.17771105]]\n",
      "t [[ 0.0930442 ]\n",
      " [ 0.0757083 ]\n",
      " [-0.02985286]\n",
      " ...\n",
      " [-0.11940254]\n",
      " [-0.40078825]\n",
      " [-0.17771105]]\n",
      "t [[ 0.09870948]\n",
      " [ 0.08116786]\n",
      " [-0.03153533]\n",
      " ...\n",
      " [-0.12896314]\n",
      " [-0.43248566]\n",
      " [-0.1918908 ]]\n",
      "t [[ 0.09870948]\n",
      " [ 0.08116786]\n",
      " [-0.03153533]\n",
      " ...\n",
      " [-0.12896314]\n",
      " [-0.43248566]\n",
      " [-0.1918908 ]]\n",
      "Current iteration=12, loss=46661.82138086053\n",
      "t [[ 0.10406947]\n",
      " [ 0.08642444]\n",
      " [-0.03307176]\n",
      " ...\n",
      " [-0.13833205]\n",
      " [-0.46350262]\n",
      " [-0.20578039]]\n",
      "t [[ 0.10406947]\n",
      " [ 0.08642444]\n",
      " [-0.03307176]\n",
      " ...\n",
      " [-0.13833205]\n",
      " [-0.46350262]\n",
      " [-0.20578039]]\n",
      "t [[ 0.10915224]\n",
      " [ 0.09148514]\n",
      " [-0.03446756]\n",
      " ...\n",
      " [-0.14751439]\n",
      " [-0.4938613 ]\n",
      " [-0.21938816]]\n",
      "t [[ 0.10915224]\n",
      " [ 0.09148514]\n",
      " [-0.03446756]\n",
      " ...\n",
      " [-0.14751439]\n",
      " [-0.4938613 ]\n",
      " [-0.21938816]]\n",
      "Current iteration=14, loss=46227.05776723411\n",
      "t [[ 0.1139837 ]\n",
      " [ 0.09635685]\n",
      " [-0.03572795]\n",
      " ...\n",
      " [-0.15651518]\n",
      " [-0.52358302]\n",
      " [-0.23272228]]\n",
      "t [[ 0.1139837 ]\n",
      " [ 0.09635685]\n",
      " [-0.03572795]\n",
      " ...\n",
      " [-0.15651518]\n",
      " [-0.52358302]\n",
      " [-0.23272228]]\n",
      "t [[ 0.11858773]\n",
      " [ 0.10104621]\n",
      " [-0.03685798]\n",
      " ...\n",
      " [-0.16533929]\n",
      " [-0.55268831]\n",
      " [-0.24579066]]\n",
      "t [[ 0.11858773]\n",
      " [ 0.10104621]\n",
      " [-0.03685798]\n",
      " ...\n",
      " [-0.16533929]\n",
      " [-0.55268831]\n",
      " [-0.24579066]]\n",
      "Current iteration=16, loss=45828.27542681225\n",
      "t [[ 0.12298636]\n",
      " [ 0.10555964]\n",
      " [-0.03786253]\n",
      " ...\n",
      " [-0.17399145]\n",
      " [-0.58119689]\n",
      " [-0.25860102]]\n",
      "t [[ 0.12298636]\n",
      " [ 0.10555964]\n",
      " [-0.03786253]\n",
      " ...\n",
      " [-0.17399145]\n",
      " [-0.58119689]\n",
      " [-0.25860102]]\n",
      "t [[ 0.12719981]\n",
      " [ 0.10990334]\n",
      " [-0.03874632]\n",
      " ...\n",
      " [-0.1824763 ]\n",
      " [-0.60912773]\n",
      " [-0.27116088]]\n",
      "t [[ 0.12719981]\n",
      " [ 0.10990334]\n",
      " [-0.03874632]\n",
      " ...\n",
      " [-0.1824763 ]\n",
      " [-0.60912773]\n",
      " [-0.27116088]]\n",
      "Current iteration=18, loss=45461.91437219555\n",
      "t [[ 0.13124669]\n",
      " [ 0.11408331]\n",
      " [-0.03951391]\n",
      " ...\n",
      " [-0.1907983 ]\n",
      " [-0.63649904]\n",
      " [-0.28347751]]\n",
      "t [[ 0.13124669]\n",
      " [ 0.11408331]\n",
      " [-0.03951391]\n",
      " ...\n",
      " [-0.1907983 ]\n",
      " [-0.63649904]\n",
      " [-0.28347751]]\n",
      "t [[ 0.13514405]\n",
      " [ 0.11810532]\n",
      " [-0.04016968]\n",
      " ...\n",
      " [-0.19896182]\n",
      " [-0.66332833]\n",
      " [-0.295558  ]]\n",
      "t [[ 0.13514405]\n",
      " [ 0.11810532]\n",
      " [-0.04016968]\n",
      " ...\n",
      " [-0.19896182]\n",
      " [-0.66332833]\n",
      " [-0.295558  ]]\n",
      "Current iteration=20, loss=45124.79175465771\n",
      "t [[ 0.13890754]\n",
      " [ 0.12197494]\n",
      " [-0.04071787]\n",
      " ...\n",
      " [-0.20697109]\n",
      " [-0.6896324 ]\n",
      " [-0.30740922]]\n",
      "t [[ 0.13890754]\n",
      " [ 0.12197494]\n",
      " [-0.04071787]\n",
      " ...\n",
      " [-0.20697109]\n",
      " [-0.6896324 ]\n",
      " [-0.30740922]]\n",
      "t [[ 0.14255146]\n",
      " [ 0.12569753]\n",
      " [-0.04116256]\n",
      " ...\n",
      " [-0.2148302 ]\n",
      " [-0.7154274 ]\n",
      " [-0.31903782]]\n",
      "t [[ 0.14255146]\n",
      " [ 0.12569753]\n",
      " [-0.04116256]\n",
      " ...\n",
      " [-0.2148302 ]\n",
      " [-0.7154274 ]\n",
      " [-0.31903782]]\n",
      "Current iteration=22, loss=44814.06380042439\n",
      "t [[ 0.1460889 ]\n",
      " [ 0.12927828]\n",
      " [-0.04150768]\n",
      " ...\n",
      " [-0.22254314]\n",
      " [-0.74072882]\n",
      " [-0.33045028]]\n",
      "t [[ 0.1460889 ]\n",
      " [ 0.12927828]\n",
      " [-0.04150768]\n",
      " ...\n",
      " [-0.22254314]\n",
      " [-0.74072882]\n",
      " [-0.33045028]]\n",
      "t [[ 0.1495318 ]\n",
      " [ 0.13272215]\n",
      " [-0.04175701]\n",
      " ...\n",
      " [-0.23011377]\n",
      " [-0.76555155]\n",
      " [-0.34165285]]\n",
      "t [[ 0.1495318 ]\n",
      " [ 0.13272215]\n",
      " [-0.04175701]\n",
      " ...\n",
      " [-0.23011377]\n",
      " [-0.76555155]\n",
      " [-0.34165285]]\n",
      "Current iteration=24, loss=44527.190324977775\n",
      "t [[ 0.15289106]\n",
      " [ 0.13603394]\n",
      " [-0.04191421]\n",
      " ...\n",
      " [-0.23754584]\n",
      " [-0.7899099 ]\n",
      " [-0.3526516 ]]\n",
      "t [[ 0.15289106]\n",
      " [ 0.13603394]\n",
      " [-0.04191421]\n",
      " ...\n",
      " [-0.23754584]\n",
      " [-0.7899099 ]\n",
      " [-0.3526516 ]]\n",
      "t [[ 0.1561766 ]\n",
      " [ 0.13921829]\n",
      " [-0.04198277]\n",
      " ...\n",
      " [-0.24484296]\n",
      " [-0.81381759]\n",
      " [-0.36345241]]\n",
      "t [[ 0.1561766 ]\n",
      " [ 0.13921829]\n",
      " [-0.04198277]\n",
      " ...\n",
      " [-0.24484296]\n",
      " [-0.81381759]\n",
      " [-0.36345241]]\n",
      "Current iteration=26, loss=44261.90211948826\n",
      "t [[ 0.15939743]\n",
      " [ 0.14227963]\n",
      " [-0.04196608]\n",
      " ...\n",
      " [-0.25200866]\n",
      " [-0.83728782]\n",
      " [-0.37406096]]\n",
      "t [[ 0.15939743]\n",
      " [ 0.14227963]\n",
      " [-0.04196608]\n",
      " ...\n",
      " [-0.25200866]\n",
      " [-0.83728782]\n",
      " [-0.37406096]]\n",
      "t [[ 0.16256175]\n",
      " [ 0.14522225]\n",
      " [-0.04186737]\n",
      " ...\n",
      " [-0.25904634]\n",
      " [-0.86033325]\n",
      " [-0.38448277]]\n",
      "t [[ 0.16256175]\n",
      " [ 0.14522225]\n",
      " [-0.04186737]\n",
      " ...\n",
      " [-0.25904634]\n",
      " [-0.86033325]\n",
      " [-0.38448277]]\n",
      "Current iteration=28, loss=44016.17129785088\n",
      "t [[ 0.16567699]\n",
      " [ 0.14805027]\n",
      " [-0.04168979]\n",
      " ...\n",
      " [-0.26595931]\n",
      " [-0.88296606]\n",
      " [-0.39472318]]\n",
      "t [[ 0.16567699]\n",
      " [ 0.14805027]\n",
      " [-0.04168979]\n",
      " ...\n",
      " [-0.26595931]\n",
      " [-0.88296606]\n",
      " [-0.39472318]]\n",
      "t [[ 0.16874988]\n",
      " [ 0.15076767]\n",
      " [-0.04143632]\n",
      " ...\n",
      " [-0.27275075]\n",
      " [-0.90519795]\n",
      " [-0.40478733]]\n",
      "t [[ 0.16874988]\n",
      " [ 0.15076767]\n",
      " [-0.04143632]\n",
      " ...\n",
      " [-0.27275075]\n",
      " [-0.90519795]\n",
      " [-0.40478733]]\n",
      "Current iteration=30, loss=43788.184555103566\n",
      "t [[ 0.17178648]\n",
      " [ 0.15337825]\n",
      " [-0.04110986]\n",
      " ...\n",
      " [-0.27942377]\n",
      " [-0.92704018]\n",
      " [-0.41468023]]\n",
      "t [[ 0.17178648]\n",
      " [ 0.15337825]\n",
      " [-0.04110986]\n",
      " ...\n",
      " [-0.27942377]\n",
      " [-0.92704018]\n",
      " [-0.41468023]]\n",
      "t [[ 0.17479228]\n",
      " [ 0.15588569]\n",
      " [-0.04071319]\n",
      " ...\n",
      " [-0.28598137]\n",
      " [-0.94850355]\n",
      " [-0.42440672]]\n",
      "t [[ 0.17479228]\n",
      " [ 0.15588569]\n",
      " [-0.04071319]\n",
      " ...\n",
      " [-0.28598137]\n",
      " [-0.94850355]\n",
      " [-0.42440672]]\n",
      "Current iteration=32, loss=43576.31920078256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t [[ 0.17777221]\n",
      " [ 0.15829354]\n",
      " [-0.04024898]\n",
      " ...\n",
      " [-0.29242645]\n",
      " [-0.96959847]\n",
      " [-0.43397146]]\n",
      "t [[ 0.17777221]\n",
      " [ 0.15829354]\n",
      " [-0.04024898]\n",
      " ...\n",
      " [-0.29242645]\n",
      " [-0.96959847]\n",
      " [-0.43397146]]\n",
      "t [[ 0.18073071]\n",
      " [ 0.1606052 ]\n",
      " [-0.03971981]\n",
      " ...\n",
      " [-0.29876182]\n",
      " [-0.99033494]\n",
      " [-0.44337898]]\n",
      "t [[ 0.18073071]\n",
      " [ 0.1606052 ]\n",
      " [-0.03971981]\n",
      " ...\n",
      " [-0.29876182]\n",
      " [-0.99033494]\n",
      " [-0.44337898]]\n",
      "Current iteration=34, loss=43379.121779971436\n",
      "t [[ 0.18367174]\n",
      " [ 0.16282394]\n",
      " [-0.03912814]\n",
      " ...\n",
      " [-0.30499022]\n",
      " [-1.0107226 ]\n",
      " [-0.45263367]]\n",
      "t [[ 0.18367174]\n",
      " [ 0.16282394]\n",
      " [-0.03912814]\n",
      " ...\n",
      " [-0.30499022]\n",
      " [-1.0107226 ]\n",
      " [-0.45263367]]\n",
      "t [[ 0.18659885]\n",
      " [ 0.16495293]\n",
      " [-0.03847635]\n",
      " ...\n",
      " [-0.31111429]\n",
      " [-1.03077071]\n",
      " [-0.46173975]]\n",
      "t [[ 0.18659885]\n",
      " [ 0.16495293]\n",
      " [-0.03847635]\n",
      " ...\n",
      " [-0.31111429]\n",
      " [-1.03077071]\n",
      " [-0.46173975]]\n",
      "Current iteration=36, loss=43195.28906952118\n",
      "t [[ 0.18951519]\n",
      " [ 0.1669952 ]\n",
      " [-0.03776673]\n",
      " ...\n",
      " [-0.31713658]\n",
      " [-1.0504882 ]\n",
      " [-0.47070132]]\n",
      "t [[ 0.18951519]\n",
      " [ 0.1669952 ]\n",
      " [-0.03776673]\n",
      " ...\n",
      " [-0.31713658]\n",
      " [-1.0504882 ]\n",
      " [-0.47070132]]\n",
      "t [[ 0.19242356]\n",
      " [ 0.16895368]\n",
      " [-0.03700149]\n",
      " ...\n",
      " [-0.32305957]\n",
      " [-1.06988365]\n",
      " [-0.47952235]]\n",
      "t [[ 0.19242356]\n",
      " [ 0.16895368]\n",
      " [-0.03700149]\n",
      " ...\n",
      " [-0.32305957]\n",
      " [-1.06988365]\n",
      " [-0.47952235]]\n",
      "Current iteration=38, loss=43023.65122895838\n",
      "t [[ 0.19532645]\n",
      " [ 0.17083119]\n",
      " [-0.03618273]\n",
      " ...\n",
      " [-0.32888567]\n",
      " [-1.08896535]\n",
      " [-0.48820667]]\n",
      "t [[ 0.19532645]\n",
      " [ 0.17083119]\n",
      " [-0.03618273]\n",
      " ...\n",
      " [-0.32888567]\n",
      " [-1.08896535]\n",
      " [-0.48820667]]\n",
      "t [[ 0.19822604]\n",
      " [ 0.17263045]\n",
      " [-0.03531248]\n",
      " ...\n",
      " [-0.33461721]\n",
      " [-1.10774128]\n",
      " [-0.49675799]]\n",
      "t [[ 0.19822604]\n",
      " [ 0.17263045]\n",
      " [-0.03531248]\n",
      " ...\n",
      " [-0.33461721]\n",
      " [-1.10774128]\n",
      " [-0.49675799]]\n",
      "Current iteration=40, loss=42863.1568889534\n",
      "t [[ 0.20112422]\n",
      " [ 0.17435405]\n",
      " [-0.03439272]\n",
      " ...\n",
      " [-0.34025644]\n",
      " [-1.12621913]\n",
      " [-0.5051799 ]]\n",
      "t [[ 0.20112422]\n",
      " [ 0.17435405]\n",
      " [-0.03439272]\n",
      " ...\n",
      " [-0.34025644]\n",
      " [-1.12621913]\n",
      " [-0.5051799 ]]\n",
      "t [[ 0.20402265]\n",
      " [ 0.17600453]\n",
      " [-0.03342532]\n",
      " ...\n",
      " [-0.34580555]\n",
      " [-1.14440632]\n",
      " [-0.51347587]]\n",
      "t [[ 0.20402265]\n",
      " [ 0.17600453]\n",
      " [-0.03342532]\n",
      " ...\n",
      " [-0.34580555]\n",
      " [-1.14440632]\n",
      " [-0.51347587]]\n",
      "Current iteration=42, loss=42712.8599705751\n",
      "t [[ 0.20692278]\n",
      " [ 0.17758431]\n",
      " [-0.0324121 ]\n",
      " ...\n",
      " [-0.35126666]\n",
      " [-1.16231   ]\n",
      " [-0.52164927]]\n",
      "t [[ 0.20692278]\n",
      " [ 0.17758431]\n",
      " [-0.0324121 ]\n",
      " ...\n",
      " [-0.35126666]\n",
      " [-1.16231   ]\n",
      " [-0.52164927]]\n",
      "t [[ 0.20982582]\n",
      " [ 0.17909573]\n",
      " [-0.0313548 ]\n",
      " ...\n",
      " [-0.35664182]\n",
      " [-1.17993708]\n",
      " [-0.52970335]]\n",
      "t [[ 0.20982582]\n",
      " [ 0.17909573]\n",
      " [-0.0313548 ]\n",
      " ...\n",
      " [-0.35664182]\n",
      " [-1.17993708]\n",
      " [-0.52970335]]\n",
      "Current iteration=44, loss=42571.90804290779\n",
      "t [[ 0.21273281]\n",
      " [ 0.18054104]\n",
      " [-0.03025509]\n",
      " ...\n",
      " [-0.36193303]\n",
      " [-1.19729423]\n",
      " [-0.53764125]]\n",
      "t [[ 0.21273281]\n",
      " [ 0.18054104]\n",
      " [-0.03025509]\n",
      " ...\n",
      " [-0.36193303]\n",
      " [-1.19729423]\n",
      " [-0.53764125]]\n",
      "t [[ 0.21564462]\n",
      " [ 0.18192241]\n",
      " [-0.0291146 ]\n",
      " ...\n",
      " [-0.36714223]\n",
      " [-1.21438788]\n",
      " [-0.54546602]]\n",
      "t [[ 0.21564462]\n",
      " [ 0.18192241]\n",
      " [-0.0291146 ]\n",
      " ...\n",
      " [-0.36714223]\n",
      " [-1.21438788]\n",
      " [-0.54546602]]\n",
      "Current iteration=46, loss=42439.532042881205\n",
      "t [[ 0.21856196]\n",
      " [ 0.18324194]\n",
      " [-0.02793489]\n",
      " ...\n",
      " [-0.37227128]\n",
      " [-1.23122424]\n",
      " [-0.55318062]]\n",
      "t [[ 0.21856196]\n",
      " [ 0.18324194]\n",
      " [-0.02793489]\n",
      " ...\n",
      " [-0.37227128]\n",
      " [-1.23122424]\n",
      " [-0.55318062]]\n",
      "t [[ 0.22148539]\n",
      " [ 0.18450165]\n",
      " [-0.02671744]\n",
      " ...\n",
      " [-0.37732201]\n",
      " [-1.24780933]\n",
      " [-0.5607879 ]]\n",
      "t [[ 0.22148539]\n",
      " [ 0.18450165]\n",
      " [-0.02671744]\n",
      " ...\n",
      " [-0.37732201]\n",
      " [-1.24780933]\n",
      " [-0.5607879 ]]\n",
      "Current iteration=48, loss=42315.03719800057\n",
      "t [[ 0.22441535]\n",
      " [ 0.18570347]\n",
      " [-0.02546371]\n",
      " ...\n",
      " [-0.38229618]\n",
      " [-1.26414895]\n",
      " [-0.56829062]]\n",
      "t [[ 0.22441535]\n",
      " [ 0.18570347]\n",
      " [-0.02546371]\n",
      " ...\n",
      " [-0.38229618]\n",
      " [-1.26414895]\n",
      " [-0.56829062]]\n",
      "t [[ 0.22735217]\n",
      " [ 0.1868493 ]\n",
      " [-0.02417508]\n",
      " ...\n",
      " [-0.38719551]\n",
      " [-1.28024872]\n",
      " [-0.57569147]]\n",
      "loss=42197.79500918358\n",
      "Cross validation finished: optimal gamma 0.04\n",
      "logistic regression loss 42255.54641354369\n",
      "w [[-0.08324485]\n",
      " [ 0.00386312]\n",
      " [-0.07383862]\n",
      " [ 0.0697467 ]\n",
      " [ 0.10482291]\n",
      " [ 0.18331767]\n",
      " [ 0.06951333]\n",
      " [-0.15732276]\n",
      " [-0.00724361]\n",
      " [-0.13103539]\n",
      " [ 0.0035129 ]\n",
      " [-0.10237477]\n",
      " [ 0.16506005]\n",
      " [ 0.18395978]\n",
      " [ 0.13117077]\n",
      " [-0.00687802]\n",
      " [-0.00117682]\n",
      " [-0.01537501]\n",
      " [-0.00647807]\n",
      " [ 0.00344156]\n",
      " [ 0.05936375]\n",
      " [ 0.00754796]\n",
      " [-0.03563932]\n",
      " [-0.03579419]\n",
      " [ 0.00364734]\n",
      " [ 0.00338975]\n",
      " [-0.03579419]\n",
      " [ 0.00165711]\n",
      " [-0.00452427]\n",
      " [-0.03579419]]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(set3_x.shape[1])\n",
    "gamma_opt = cross_validation(set3_y, set3_x, k_fold, gammas, fonction=4)\n",
    "w_lr3, loss_lr = logistic_regression(set3_y, set3_x, initial_w, max_iters, gamma_opt)\n",
    "print(\"Cross validation finished: optimal gamma {g}\".format(g=gamma_opt))\n",
    "print(\"logistic regression loss {loss}\".format(loss=loss_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "OUTPUT_PATH = '../data/pred_ls.csv' \n",
    "tX_test_ls = cut(std(filtering_with_mean(tX_test)),[15,18,20])\n",
    "tX_test_ls = build_poly(tX_test_ls, degree_ls)\n",
    "y_pred = predict_labels(w_ls, tX_test_ls)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_set_ls.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test , tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = cut(tX_test, [15,18,20])\n",
    "\n",
    "set1_x, _, set1_ids, set2_x, _, set2_ids, set3_x, _, set3_ids = separate_sets(tX_test, y_test, ids_test, 22-3)\n",
    "\n",
    "def filtering_test_ls (set_x, degree_rr):\n",
    "    print(set_x.shape)\n",
    "    set_x = outliers(set_x, -999)\n",
    "    print(set_x.shape)\n",
    "    set_x = std(filtering_with_mean(set_x))\n",
    "    set_x = build_poly(set_x, degree_rr)\n",
    "    return set_x\n",
    "\n",
    "\n",
    "set1_x = filtering_test_ls(set1_x, degree_set1_ls)\n",
    "set2_x = filtering_test_ls(set2_x, degree_set2_ls)\n",
    "set3_x = filtering_test_ls(set3_x, degree_set3_ls)\n",
    "\n",
    "y_pred1 = predict_labels(w_set1_ls, set1_x)\n",
    "y_pred2 = predict_labels(w_set2_ls, set2_x)\n",
    "y_pred3 = predict_labels(w_set3_ls, set3_x)\n",
    "\n",
    "y_pred_ls, ids_ls = concatenate_sets(y_pred1, set1_ids, y_pred2, set2_ids, y_pred3, set3_ids)\n",
    "create_csv_submission(ids_ls, y_pred_ls, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_set_rr.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test , tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "set1_x, _, set1_ids, set2_x, _, set2_ids, set3_x, _, set3_ids = separate_sets(tX_test, y_test, ids_test)\n",
    "\n",
    "def filtering_test_rr (set_x, degree_rr):\n",
    "    set_x = outliers(set_x, -999)\n",
    "    set_x = build_poly(set_x, degree_rr)\n",
    "    return set_x\n",
    "\n",
    "set1_x = filtering_test_rr(set1_x, degree_rr_set1)\n",
    "set2_x = filtering_test_rr(set2_x, degree_rr_set2)\n",
    "set3_x = filtering_test_rr(set3_x, degree_rr_set3)\n",
    "y_pred1 = predict_labels(w_rr_set1, set1_x)\n",
    "y_pred2 = predict_labels(w_rr_set2, set2_x)\n",
    "y_pred3 = predict_labels(w_rr_set3, set3_x)\n",
    "\n",
    "y_pred_rr, ids_rr = concatenate_sets(y_pred1, set1_ids, y_pred2, set2_ids, y_pred3, set3_ids)\n",
    "create_csv_submission(ids_rr, y_pred_rr, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = filtering_with_mean(test_set1_x)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_gd1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_gd2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_gd3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = filtering_with_mean(test_set1_x)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_gd1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_gd2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_gd3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regressions submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers ratio for each feature [0.26054480387588036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.09834148900979822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "outliers ratio for each feature [0.0, 0.060335344108509326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "tX_test_log = log_distribution(tX_test, to_log)\n",
    "test_set1_x, _, test_set1_ids, test_set2_x, _, test_set2_ids, test_set3_x, _, test_set3_ids = separate_sets(tX_test_log, _, ids_test)\n",
    "\n",
    "test_set1_x = outliers(test_set1_x, -999)\n",
    "test_set1_x = np.c_[np.ones((test_set1_x .shape[0], 1)), test_set1_x ]\n",
    "test_set1_x = filtering_with_mean(test_set1_x)\n",
    "test_set1_x = std(test_set1_x)\n",
    "\n",
    "test_set2_x = np.c_[np.ones((test_set2_x .shape[0], 1)), test_set2_x ]\n",
    "test_set2_x = outliers(test_set2_x, -999)\n",
    "test_set2_x = filtering_with_mean(test_set2_x)\n",
    "test_set2_x = std(test_set2_x)\n",
    "\n",
    "test_set3_x = np.c_[np.ones((test_set3_x .shape[0], 1)), test_set3_x ]\n",
    "test_set3_x = outliers(test_set3_x, -999)\n",
    "test_set3_x = filtering_with_mean(test_set3_x)\n",
    "test_set3_x = std(test_set3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/pred_lr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred1 = predict_labels(w_lr1, test_set1_x)\n",
    "y_pred2 = predict_labels(w_lr2, test_set2_x)\n",
    "y_pred3 = predict_labels(w_lr3, test_set3_x)\n",
    "y_pred = np.concatenate((y_pred1, y_pred2, y_pred3), axis=0)\n",
    "ids_test = np.concatenate((test_set1_ids, test_set2_ids, test_set3_ids), axis=0)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
